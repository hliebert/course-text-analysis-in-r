Indicator and Stratification Methods for Missing Explanatory Variables in Multiple Linear
Regression
Author(s): Michael P. Jones
Source: Journal of the American Statistical Association, Vol. 91, No. 433 (Mar., 1996), pp.
222-230
Published by: Taylor & Francis, Ltd. on behalf of the American Statistical Association
Stable URL: https://www.jstor.org/stable/2291399
Accessed: 08-03-2019 22:06 UTC
REFERENCES
Linked references are available on JSTOR for this article:
https://www.jstor.org/stable/2291399?seq=1&cid=pdf-reference#references_tab_contents
You may need to log in to JSTOR to access the linked references.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

American Statistical Association, Taylor & Francis, Ltd. are collaborating with JSTOR to
digitize, preserve and extend access to Journal of the American Statistical Association

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

Indicator and Stratification Methods for Missing

Explanatory Variables in Multiple Linear Regression
Michael P. JONES

The statistical literature and folklore contain many methods for handling missing explanatory variable data in multiple linear
regression. One such approach is to incorporate into the regression model an indicator variable for whether an explanatory variable
is observed. Another approach is to stratify the model based on the range of values for an explanatory variable, with a separate
stratum for those individuals in which the explanatory variable is missing. For a least squares regression analysis using either
of these two missing-data approaches, the exact biases of the estimators for the regression coefficients and the residual variance

are derived and reported. The complete-case analysis, in which individuals with any missing data are omitted, is also investigated
theoretically and is found to be free of bias in many situations, though often wasteful of information. A numerical evaluation
of the bias of two missing-indicator methods and the complete-case analysis is reported. The missing-indicator methods show
unacceptably large biases in practical situations and are not advisable in general.
KEY WORDS: Epidemiology; Incomplete data; Missing data; Psychology.

ogy, sample survey research, and business and economics.

1. INTRODUCTION

In the missing-indicator method, an indicator of whether

It is quite common in practice that a statistician planning

an explanatory variable is missing is worked into the re-

on performing a regression analysis finds that the explana-

gression model (1). In particular, if Q2 is a binary indicator

tory variable information is incomplete on some subjects.

of whether X2 is observed, then (1) is modified by replac-

There can be several mechanisms at work that produce the

ing X2 by X2Q2 and by adding 1 - Q2 as another predic-

incompleteness. The statistician, of course, wishes to ap-

tor. This procedure has been suggested by Anderson et al.

ply a strategy that comes as close as possible to the true

(1983), Chow (1979), Cohen and Cohen (1975), and Miet-

regression had data not been missing. For the purpose of

tinen (1985). Cohen and Cohen (1975, p. 274) argued that

this article, it is sufficient to assume that the true regression

such methodology uses all the available information, includ-

model is

ing the presence or absence of values on the explanatory

Y? = o + lXii + 2X2i + i1,...,n, (1)

variable. Thus one avoids both "the risk of nonrepresenta-

where the ei are independent error terms with mean zero

tiveness in dropping subjects if data are missing nonran-

domly" and as well lower power from reduced sample size

and common variance a2. We also assume throughout that

"even if data are missing randomly." In the context of lo-

Y and X1 are always measured but X2 may be missing.
The most commonly used method for handling such data

gistic regression, Chow (1979) proposed a variation on this

is the complete-case analysis, so called because any sub-

method in which the interaction term X1Q2 is also added

ject with missing data is removed from the analysis. This

as a predictor to the model. A special case of the missing-

approach is valid when X2 is not missing as a function of

indicator method is of interest. In a one-way analysis-of-

either Y or e and is useful when the vast majority of the

variance problem, a common method of handling observa-

cases are complete. But when a fair proportion of the data

tions with unknown group identification is to create a sep-

are missing, the complete-case method is very wasteful of

arate group for them. This "missing" group is created by

information. A multitude of alternative missing-data tech-

adding a 1 - Q term to the regression model and replacing

niques have been devised to reclaim as much of the avail-

the true dummy group indicators Xj by the observed group

able data as possible. Review articles by Afifi and Elashoff

indicators XjQ.

(1967), Anderson, Basilevsky, and Hum (1983), and Little

A close relative to the missing-indicator methods is the

(1992) have summarized many of the missing-explanatory
variable regression methods. There are, however, a couple
of classes of missing-data methods in common use in various disciplines that have not yet been investigated for their
validity.
The two classes of missing-data methods to be studied
here fall under the headings of missing-indicator methods

class of stratification methods. Suppose that X2 in model

(1) can assume only k distinct values. In this class of procedures, the analysis is stratified into k + 1 submodels, one
for each group of subjects with a distinct value of X2 and

then a (k + 1)th stratum for those with unknown X2. This
is commonly done when X2 is a confounder and X1 is the
predictor of interest.

and stratification methods. These methods have been pro-

These classes of missing-data procedures for regression

posed for use in the areas of behavioral sciences, epidemiol-

have been proposed for use regardless of the missing-data
mechanism. These methods' appeal is that they incorporate

the observed "missingness" into the model. A basic check

Michael P. Jones is Associate Professor, Department of Preventive
Medicine and Department of Statistics and Actuarial Science, University

of Iowa, Iowa City, IA 52242. This work was supported in part by National Cancer Institute Grant CA55212. The author thanks Jon Lemke for
helpful discussion and the referees and associate editor for suggestions
that improved the recommendations for what to do in practice.

? 1996 American Statistical Association
Journal of the American Statistical Association

March 1996, Vol. 91, No. 433, Theory and Methods
222

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

Jones:

Methods

for

Missing

on the validity of such modeling is whether the methods
produce biased estimation if no information about the re-

Explanatory

Variables

223

The first term on the right is e'Qe, and the second term

is easily seen to be e'H,e, where H, = X,(X'X,)-'X'.

gression of Y on (X1, X2) is contained within the "miss-

Hence RSS = '(Q - Hc)e. By theorem 1.7 of Seber

ingness" aspect of the data. The goal of this article is to

(1977) and the assumption that Q is independent of e,

investigate whether these procedures are biased when the

true regression model is given in (1) and X2 can be missing

E(RSS) = E[e/(Q - Hc)e] = tr[(Q - Hc)Var(e)]
+ [(Ee)'(Q-Hc) (E)] = 2[tr Q - tr Hc]

as a function of X1 or X2 but not as a function of either Y

= 2 (SQi - 3)

or 6. These missing-explanatory variable methodologies are
quite general in that they can be implemented for any type
of regression: least squares, generalized linear models, Cox
proportional hazards regression, and others. Here we treat

since tr[Xc(X'Xc)-1X'] = tr[(X' Xc)-X'Xc] = 3.

According to Theorem 2.1, for the method of complete-

least squares, because we can then derive exact biases of

case analysis to yield unbiased estimates of the regression

the estimators for the regression coefficients and variances.

coefficients and error variance, the missing-data mechanism

It is reasonable to conjecture that if parameter estimators

can depend on the values of the covariates (i.e., Qi can

are biased for least squares, then they are probably biased
for the other regressions as well.

be a function of Xli and X2j) but not on the error term.
Two common ways in which the missing-data mechanism

The complete-case method is reviewed in Section 2,

may depend on the error term are through the existence of

which will help to set the stage for the other methods. The

an omitted covariate and through mismeasured covariates.

missing-indicator methods are investigated in Section 3,

Suppose in (1) that ei = yX3& + Ei, where E* has the usual

with a special section looking at the missing-group method.

properties of an error term and X3 is the omitted covariate.

In Section 4 the stratification methods are considered. An

For example, in a forward stepwise regression procedure,

evaluation of the magnitude of bias is carried out in Sec-

the second stage of analysis may only include X1 and X2.

tion 5 for the purpose of recommending procedures to use

If X3 is orthogonal to (1, X1, X2) and there is no missing

in practice. Some concluding comments are contained in
Section 6.

data (Q = In), then by (3), estimation is unbiased. But if
data are missing as a function of either X1, X2, or X3, then

2. REVIEW OF THE COMPLETE-CASE METHOD

the bias is
/ EX3iQi

Before studying the missing-indicator and stratification
methods for handling missing data, it will be helpful to

(XI QX)-1 XliX3iQi ,(4)

review a standard missing-data technique-the complete-

Y ZX2iX3iQi }

case analysis, so called because any individuals with missing data are excluded from the analysis. The true model is

which could be quite different from the zero vector.

The other case involves mismeasured covariates. Suppose
given by (1). Let Qi be 1 if the ith individual has complete

data and be zero otherwise. Define Q = diag(Qi, , Qn)

that X1i is a mismeasurement of the true covariate Sli so

that in (1), Ei = j 1(Sli - X1i) + e*, where e* has the usual

Then the complete-case model is

YiQi = OfcQi + OfCX1iQi + Oc2X2iQi + EiQii
i =1, .. .,n (2)
or, equivalently, as QY = QXOc + Qe. The following theorem summarizes the properties of the complete-case least
squares analysis.

Theorem 2.1. The least squares estimator for model (2)
is

properties of an error term. Then, regardless of the type of

missing-data pattern (including no missing data at all), the
estimation of /3 can be biased. The resultant bias is given

by (4) with i1 replacing ay and Sli - Xli replacing X3i.
3. MISSING-INDICATOR METHODS

As defined in Section 1, these methods modify the original model by adding a missingness indicator and possibly
interactions between this indicator and the covariates. In the

first two methods discussed, the true model is of the form
(1), and only X2 is liable to be missing. The third method
If Q is independent of 6, then, conditional
on Q,
involves adding
a missing indicator to simple linear regres-

oc = ,/ + (X'QX)-1X'Qe. (3)

E(RSS) = E(QY - QXOc)'(QY-QX0C)

- a2(ZQ3)I(y-Q '

= ol2 ( Qi-3 3)

Proof. Let Xc = QX and Yc = QY. Then &c
= (X/Xc)-1'Xyc = (X'QX)-1X'QY = (X'QX)-1

sion. The two-sample problem with missing-group information is considered as a special case.
3.1 Missing-indicator Methods I and 11

The true model is assumed to be given by (1); however,

X2 is not always observed. Let Q2j equal 1 when X2i is

X'Q(Xj3 + e) = j3 + (X'QX)1X'Qe, giving the first result. Next, recall the standard result
RS=(Yc - Xcj)'(Yc - Xc3)
- (OY - )X'c X( (Y - 3).

observed and zero when it is missing. The first missing-

indicator method, as described by Anderson et al. (1983, p.
456), Chow (1979), Cohen and Cohen (1975, secs. 7.4.3 and
9.3.5), and Mietinnen (1985, sec. 18.2.5), is

~Yo + 7ylXl% + 7y2X2iQ2i + 7y3( - Q2i) + ei. (5

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

224

Journal

of

the

American

Statistical

Association,

March

1996

The original model (1) is modified by replacing X2i by

Theorem 3.2. Assume that e is independent of (Xl, X2,
X2iQ2i and adding the missing indicator 1 - Q2i. ThisQ2). Then, conditional on (Xl, X2, Q2), the expected

model is commonly used to test whether the data are miss-

least squares estimators for model (10) are E(00)
ing at random by checking if y3 is significantly different= i0, E(01) = f1, E(02) = 2, E(03) = o + 323 X2Ill,

from zero. Furthermore, regardless of the test result, the

and
estimates of yo, -Yi, and -y2 are used. A slight modification

of (5) that is mathematically easier to work with is

Yi = 0oQ2i + 0lXli + 02X2iQ2i + 03(1 - Q2i) + ei. (6)

E(04) = 31 + /323m2Ixx where dmI1 and are

the least squares intercept and slope estimators from the regression of X2 on X1 for those missing X2. Furthermore,
conditional on (X1, X2, Q2), the expected mean squared error for model (10) is

The least squares estimators of ('Yo, 7Y1, 72) are identical to
those of (00, 01, 02). The least squares estimators are biased

1X)]3 2X1 )3,
a22+ [RSSm(X2

for (fi, 131, 32), as stated in the following theorem.
Theorem 3.1. If E is independent of (X1, X2, Q2), then,

where RSSm(X2lXl) is the residual sum of squares f

conditional on (X1, X2, Q2), the expected least squares es-

regressing X2 on X1 for the subset of individuals miss-

timators for model (6) are

ing X2.

E(0O) = 30 + fi2PmSlnFo F
E(01) = fl + 32PmS Fi,

Note that (0o, 01, 02) are unbiased estimators of
(f0, f1,/32), whereas (03, 04) are biased for (do, /1); if X1
and X2 are uncorrelated among those missing X2, then

dxm xx = 0, in which case 04 is unbiased for f1. Rewriting

(10) for those with X2 information and those without gives
the subset models (7) and (9), which contain no common
E(02) = 32(1-P.SlrF2),
regression coefficients. As such, model (7) is the completecaseSm
analysis,
which, by Theorem 2.1, allows unbiased eswhere Pm = 1 - Q2, the proportion missing X2;
is
the sample covariance of X1 and X2 for those missing X2;
timation, whereas model (9) is incorrectly specified if Y is
and (Fo, F1, F2) are functions of the means, variances, and
truly modeled by (1). Overestimation of cr2 is also not surcovariances of X1 and X2 and are defined in the proof.
prising because of the assumption that var(ei) = a2 in (10)
Note that the estimators are unbiased if either Pm = 0 or
regardless of whether Q2i is zero or 1. In particular, as seen
and

S12 = 0. The proof is given in the Appendix.

by comparing (7) and (9), the contribution to the residual
By writing down model (6) for the subsets with and with- sum of squares for those missing X2 will be larger than for
out X2 information, one can gain an intuitive understanding
those with X2, unless f2 0 O. The residual sum of squares
why this model produces biased estimators. For those with
(and hence the bias for a2) is largest when X1 and X2 are
X2 information (Q2i =1), model (6) is

Yi = 00 + 0lXli + 02X2i = ei, (7)

uncorrelated for those subjects missing X2.

Because model (9) excludes the predictor X2, it will be
helpful to review the effect of omitting a predictor variable

whereas for those missing X2 (Q2i 0 O), it isbefore proving Theorem 3.2. That proof is found in the

Yi = 03 + 0lXli + ei (8)
The intercept terms are different, but the X1 coefficients

are required to be the same, regardless of the adjustment
value of X2. This causes the bias. A simple way around
this constraint is to add another term to the model, which

allows the X1 coefficients for the subsets to differ, so that
for those subjects missing X2,

Yi = 03 + 04Xli + ei- (9)
This new model is described next.

Appendix. The following lemma is from section 6.1.1 of

Seber (1977).
Lemma 3.1. Suppose that the true model is given by

Y = X,3 + Zy + E, where E(E) = 0, E(E'e) = a2in_ and 3
and -y are vectors. Furthermore, suppose that Z is omitted
from the model fit. Then

E(3) = (X'X)-1X'(X)3 + Z-y) =,3 + (X'X)-1X'Z-y.
Given H = X(X'X)-1X' and MSE = Y'(Ln - H)Y/
(n - p), where p = dim(X), then

The second-missing indicator method is to use the
model

E(MSE) = a2 + >y Z'(L2-H)Z 2.
rn-p

Yi = 0oQ2i + 01XliQ2i + 02X2iQ2i

+ 03(1 - Q2i) + 04Xli(l - Q2i) + ei. (10) The estimated variance of /3 is MSE(X'X)-1, which is
This generalizes the first missing-indicator method by split-larger on average than a2(X'X)-1.

ting the Xli term of model (6) into two predictor variables
with corresponding coefficients 0 and 04. The subset models are given by (7) and (9). Model (10) is only a slight vari-

3.2 Missing-lndicator Method III and the
Missing-Group Method

ation on that proposed by Chow (1979). The modification

Cohen and Cohen (1975, chap. 7) described a missing-

was made to facilitate the proof of the following theorem.

indicator method for simple linear regression similar to

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

Jones:

Methods

for

Missing

Explanatory

those described earlier. Assuming that the true model is

Y, = 30 + i1ZI + ei,
they recommended the data analyst use the model

Variables

where

225

mj

is

The proof is given in the Appendix. This MSE bias would

affect the standard t test for the difference between the two
nonmissing groups

Y- = ao + 71[ZiQi+ c(1-Qi)] + t2(1-Qi) +ei7 (1

t = (Y1-Y22) MSE (-+ I)

where c is an arbitrary constant and Qi takes the value 1

when Zi is measured and zero when Zi is missing.
Cohen and Cohen (1975) correctly noted that the esti-

The complete-case analysis, which does not include the
missing group, uses a smaller, unbiased estimator of a2,
but
mation of 7Y0 and 71 is independent of the choice of theits t test is based on fewer degrees of freedom. Natuconstant c.
rally, any comparison of t tests based on these two methTheorem 3.3. Assume that E is independent of (Z, Q).

Then, conditional on (Z, Q), the expected least squares es-

timators for model (11) are E(o) = 3o0,E(-1) =1 and

ods depends on the values of MI, M2, n,u 2, and a0. Two
examples illustrate this. Suppose that the two groups are
of equal size, each with half their observations missing, so
that ml = m2 = n/4 and a-2 - 1,0 a = 2. The MSE bias

EY2) 31(Zm c), where Zm is the average of the Z's
of the missing-group method is roughly 0.5, which would
among those missing Z. Moreover, 'yo and ii are exactly
produce a less powerful test than the complete-case analy-

the least squares estimators from the complete-case analy-

sis for the usual a levels at moderate degrees of freedom.
On the other hand, suppose that one group has no missing
(MSE) from fitting (11) is a2 + /2 MSE*, where MSE* is
observations, perhaps ml = 0. Then the MSE bias is zero,
the mean squared error from fitting the model
and the missing-group method produces the larger critical
region. In practice, though, ml and M2 are unknown, and
(I - Qi) (Zi - c)
caution would suggest using the complete-case method.
sis. Conditional on (Z, Q), the expected mean squared error

- r1 + 771{ZiQi + c(l -Qi)} + 72(l - Qi) + ei.

4. STRATIFICATION METHODS

The proof of Theorem 3.3 is given in the Appendix.
This missing-indicator methods can be further illustrated

Another class of methods for handling missing data fall
under the category of stratification methods because they
in analysis of variance, group membership is unknown for
involve stratifying the data set into smaller pieces for analsome individuals. One method from the statistical folklore
ysis. Once again, suppose that the true model for Y is linfor dealing with this type of missing data is to form anotherear in the predictor variables X1 and X2, given by (1), and
group and then to perform the analysis of variance on the
that X1 is always observed but X2 is sometimes missing.

by a special case, called the missing-group method. Often

augmented set of groups. Cohen and Cohen (1975, chap. 7)

Furthermore, suppose that X2 can only take on values in

recommended this approach. To investigate possible bias in
this procedure, the two-sample problem is considered here

{cl, ... , Ck}, with at least two observations per category.
There are several possible scenarios for X1 and X2. Con-

for simplicity. The true model is assumed to be

sider here the case in which X1 is the predictor of interest

and X2 is an important confounder but f2 is considered a
nuisance parameter. For example, X1 might be a treatment

Yi = ,o + aoZi + ei,

indicator or dosage level, and X2 the age category or sex of
where Zi assumes the value zero for group 1 and 1 for group
a subject. In general a stratification method would control
2 and ei has mean zero and variance a2. Again let Qi be 1 if
for the confounder X2 by creating k strata corresponding to
Zi is observed and zero if not. The third missing-indicator
the k subsets {{iX2i= cj}, j 1,.. . , k} and then by modmethod, described previously, uses the model
eling Y in terms of X1 in each stratum assuming a constant
f1 across strata. In such a stratified setting, the true model
Y t + aZiQi+0(l- Q +ei, (12)

(1) for subjects in stratum j is Yi -(io + 32cj) + /31XIi

where the arbitrary constant c of the ?third-missing
indicator
e. Data analysts sometimes
modify. this method for
model is chosen here to be zero. An artificial group 3 is
missing-data problems by adding a stratum to include all

thereby created to consist of those subjects with missing-

group identification. The three group means are [,up + a,
and [ + Q.
Corollary 3.1. Assume that E is independent of (Z, Q).
Then, conditional on (Z, Q), the expected least squares es-

those with unobserved X2 information. In particular, define

the stratum indicator I1j to be 1 when X2i is observed to
be cj and zero otherwise for j =1,... ., k, and let Ik+1,i be
1 if X2i is missing and zero otherwise. The resulting model

is

timators for model (12) are E(/i) = ,o IE(a") a a0, and

k+1

Yi = E, -yoAi + -Ylxli + ei* (13),

E(O) = a0 times the proportion of group 2 subjects among
those with missing-group identification. But conditional on
(Z, Q), the expected MSE from this analysis is
2

mlm2

a0"-

2

a0,

j=1

Theorem 4.1. The least squares estimators for model
(13) are

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

226 Journal of the American Statistical Association, March 1996

YOj = O + (cj-Xl(j)PmS12/Sl1)/2 + EIjii

(j =1...,k),

is the least squares slope e
of X2 on X1 in stratum k + 1. Conditional on (X1, X2, Q),
the expected MSE's for these models are

E[MSE(14)] = 92 + 022[RSSm(X2 lX)]/(n - 2(k + 1))
'YO,k+l =3O + (X2(k+1) - Xl(k+l)PmS12/Sll)/2

+ EIk+l,iEi,

and

and

E[MSE(15)] = 92 +, 22[RSSm(X2|X1)]/(n - k - 3),
1 3i =01 + (PmSl2/S1I)/32 + ZXiiEi,

where XI(j) is the average X1 within the jth stratum, Pm
is the proportion of individuals in stratum k + 1 (unknown
X2), Slmf is the sample covariance of X1 and X2 for those

missing X2, S1= n- Z Z Iji(X1I -X1(j))2, and Ei is the
error term from the true model (1).
The proof is somewhat tedious but straightforward and
is omitted here. Of particular interest is that even if Ei is

where RSSm(X21X1) is the residual sum of squares after

regressing X2 on X1 for those missing X2 (stratum k + 1).
The proof is given in the Appendix.
5. EVALUATION OF THE MAGNITUDE OF BIAS

Proper assessment of whether the missing-covariate meth-

ods should be used in practice requires an evaluation of
the magnitude of these methods' biases for /3 and u2. The

independent of Xli and the Iji's, -^yj is a biased estimator
focus here is on missing-indicator methods I and II, modof i1 unless Pm = 0 or Sm = 0. The flavor of this resulteled
is in (6) and (10). The ultimate question of interest is

reminiscent of the first missing-indicator method of Section

3, because the fl1 coefficient is assumed to be the same in
the "missing-data" stratum (in which X2 may be heterogeneous and thus Sm nonzero) as in the other strata (in which

whether either of these methods should be preferred over

the complete-case analysis, based on model (2).
Once again, the true model is assumed to be (1), where
X2 may be missing for some individuals. The bias in es-

X2 is homogeneous). Adding a different ,1 parameter for
timating /3 = (o, 01, /32) by missing-indicator method I
(MIM I) is given in Theorem 3.1. The biases in estimating

the "missing-data" stratum gets around this constraint. Two

alternative stratification methods that allow more flexibility ,3 and a2 by missing-indicator method II (MIM II) are given
in Theorem 3.2. Although the bias in estimating a2 by MIM
I is not derived herein, it is at least as large as that for MIM
k+1

through additional stratum-specific parameters are

Yi > E (YOj + 'YljXli)Iji + ei (14)
j=1

II, because the MIM I model (6) is restricted relative to the

MIM II model (10). The evaluation study consists of two
parts. In Section 5.1 no particular distribution for (X1, X2)
is assumed, but the measurement indicator Q2 is assumed

and

to be independent of X1, X2, and e. That is, X2 is missing

k

completely at random. In Section 5.2 (Xl, X2) are assumed

Yi Z (YOj + Ylxli)Iji

to be bivariate Bernoulli and Q2 may be missing as a func-

j=1

+ (-YO,k+l + -Yl,k+lXli)Ik+1,i + ei. (15)

Theorem 4.2. Assume that Ei is independent of Xli

tion of X1 and/or X2. This allows a more complete study
by pattern of missingness. Bias is evaluated by asymptotic
theory and computer simulation.

5.1 General Covariate Distribution
and the Iji's. The least squares estimators (IOj, lj) for
model (14) and (iOj, -I) for model (15) are unbiased for In this part of the evaluation study, the covariates X1
(io + f22cj, 31) for j 1,... ,k. In stratum k + 1, conand X2 are assumed to have variances a, and a2 and correditional on (XI, X2, Q),(Yo,k? Y1,k+l) are unbiased for lation P12 Furthermore, Q2 is assumed to be independent

(/O + /2 (X2(k+1)- flX2Ix (k+1)) 431 + /2x0 2 1x1), where
of X1,X2, and e. Let Pm = P(Q2 = 0) be the proportion
missing. Replacing terms in the bias expressions in TheoTable 1. Asymptotic Biases of 31, 32, and or2
MIMI

MIMI

MIMII

rem 3.1 by their almost sure limits, the asymptotic biases
in MIM I for estimating /31 and /32 become

0Y2 P12 bias(31) bias(32) bias(6r2)
1

0

0

0

.50

.5 .29,32 -.1 4,2 .37 22
.9

2

0

.76/32

0

0

-.68/32

.10

/2

2.00322

.5 .57/32 -.14/32 1 .50322
.9 1.51/32 -.68/32 .38/32

5

0

0

0

1

as Pm92Pi2

biasn(/1) 02 -T[l - P12(l Pm)1 (16)

23

2.50/32

.5 1 .43/32 -.14/32 9.37/32
.9 3.78/32 -.68/32 2.378 2

and
2

biasn(/32) -,B2 [1p2 (17)

As expected, these biases are zero when

Pm = 0 or P12 = 0. When Pm i 0, bias(/3

-32 (Jf2/Ji1) when P12 =-1 to /32 (J2/Ji1) when P12 = 1.

Obviously, the larger the ratio a2/ffi, the larger this bias

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

Jones:

Methods

for

Missing

Explanatory

Variables

227

Table 2. Averages and Standard Deviations Over Simulated Data Sets

Analysis Ave(31) SD(/31) Ave(se(/31)) Ave(/32) SD(/32) Ave(se(/32)) Ave (&2)
P12 =0
All

data

1.002

.072

Complete-case

1.006

MIM

I

.212

MIM

II

1.001
1.006

.071

.100

2.003

.101

.215

1.999

.100

.307

.078

.082

.035

1.999

.035

.050

.051

.051

.991
.988

.154

9.115

1.999

.050

.154

9.115

2.001

.041

.041

1.002

P12= .5

All

data

.996

Complete-case

MIM

i

MIM

II

.994

2.128

.115

.247

.994

.118

.214

.115

.313

2.002

.060

.059

.995

1.717

.095

.152

7.858

2.002

.060

.156

7.038

becomes. When Pm 4 0, the range of bias(32) is from zero
when P12 = 0 to -,32 when pl2 = 1.

j, k = 1, 2 be the joint frequency function of (X1, X2). Also

The regression-coefficient estimators for MIM II are the

quency function of Q2. As before, Q2 is independent of e.

let qjk = P(Q2 = 1XI = j, X2 = k) be the conditional fre-

There are four possible patterns of missing X2 data:

same as those of the complete-case analysis and thus are unbiased. The MIM II estimator of a2 is not unbiased. In fact,

P1. X2 missing completely at random: P(Q2 = ll

from Theorem 3.2, the MSE from MIM II can be shown

X2) = P(Q2 =1)

to converge almost surely to a2 + i23pm 2 (1 -_P2). This

P2. X2 missing as a function of X1: P(Q2 = 11X1,X2)

bias is very large when ac is large and P12 is zero. Table 1

= P(Q2 = 1IX1)
P3. X2 missing as a function of X2: P(Q2 = 1X1, X2)

summarizes the MIM I asymptotic bias in (31, ,/2) and the
MIM II asymptotic bias in a 2 when Pm = .5 and ac = 1.
As already mentioned, the MIM I estimate of a2 is greater
than or equal to that of MIM II. Settings that produce the
largest (smallest) bias in the regression coefficient estima-

= P(Q2 = 11X2)

P4. X2 missing as a function of X1 and X2.

First, the bias of the MIM I regression coefficient esti-

tion produce the smallest (largest) bias in residual variance

mators is considered. Using Theorem 3.1, the asymptotic

estimation.

biases are zero if the limiting covariance between X1 and

X2 for subjects with X2 missing is zero; that is, if am = 0.
It can be shown that am = 0 if and only if

It is also of interest to consider the standard errors of
the MIM regression coefficient estimators and how they
compare to those of the complete-case analysis. Using the

GAUSS matrix language (Aptech Systems, Inc. 1991), sim-

PooPiI (1 - qoo)_( - lqll)

ulation was used to answer this question. Two bivariate nor-

Plopol (1 - qlo)(1 - qol)

mal covariates were generated with zero means; ar = 1

and a 2 = 4; P12 = 0 in the first simulation and .5 in the

Let A represent the first term and B the second. Then A

= 2, a2 = 1. Also, Pm = .5. The results in Table 2 are
based on 500 simulated data sets, each with n = 200. The
standard deviations of the simulated estimators are not com-

data patterns P1-P3, B = 1, but in P4, B : 1. Hence
even if the covariates are independent, MIM I will give

= 1 if and only if X1 and X2 are independent. In missingsecond. In the assumed true model (1), /3o = 1,131 = 1, /32

biased estimates if X2 is missing as a function of both X1
and X2. Two pattern P4 examples illustrate this bias. First,
suppose that X2 is missing whenever X1 :$ X2; that is,

parable to the averages of the simulated standard errors for

the missing-indicator methods. The MSE's of 01 and /2 for
MIM I and MIM II far exceed those of the complete-case

qoo = qll= 1,qol = qlo= 0. Then biasn(:3) -

and biasn(32) ,B+ . Second, suppose that X2 is mis

analysis.

whenever X1 = X2; that is, qoo = qll = 0, qol = q= 1

Then biasn(,31) a /32 and biasn(32) /2

5.2 Various Patterns of Missingness

Finally, the properties of the missing-indicator methods'

In the previous section X2 are missing completely at

estimators can be compared to those of the complete-case

random. Now the various possible patterns of X2 miss-

analysis through computer simulation. The analysis of all

ing and their effect on bias are investigated for bivariate

data is given as the baseline standard of comparison. As

Bernoulli covariates. Let Pjk = P(X1 = j, X2 = k) for

before, 500 simulated data sets of size n = 200 each were

Table 3. Simulation Results When X2 is Missing Completely at Random

Analysis Ave (31) SD(/31) Ave(se(3,)) Ave(/2) SD(/2) Ave(se(32)) Ave (82)
All

data

.997

Complete-case
MIM

I

MIM

II

.996

.139
1.004

.174

1.004

.142
.198

.174

.198

2.001

.202

1.995

.249

.145

1.994

1.994

.142

.201

.201

.202

.248

.201

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

.249

.994
.998

1.503
1.503

228 Journal of the American Statistical Association, March 1996
Table 4. Simulation Results When X2 is Missing as a Function of Xl and X2

Analysis Ave(f31) SD(,i>) Ave(se(j)) Ave0(/32) SD(/32) Ave(se(/32)) Ave(6-2)
All data .990 .135 .142 2.006 .131 .142 .996
Complete-case .989 .167 .174 2.008 .169 .174 .996
MIM I .443 .159 .162 2.195 .178 .186 1.177
MIM II .989 .167 .175 2.008 .169 .175 .996

variances, and the (1)
correlation with
between X1 and X2. 30
Various
generated for model
=
patterns
of
missing
data
were
investigated
in
Section
5.2
and Pjk .25. Note that XI and X2
ar
3 summarizes the results when X2 is missing completely

for the case of binary covariates. As discussed there, MIM
I is not advised as a general method. MIM II produces the
results when X2 is missing as a function of X1 and X2; in
same regression parameter estimates as the complete-case
particular, X2 is present whenever X1 = X2 but is missing
analysis, but often with considerably larger standard errors.
In Section 3.2 the true model was assumed to contain
with probability .5 whenever X1 54 X2.
only
a single explanatory variable. MIM III, modeled in
In the setting of Table 3, MIM I is unbiased for /31 and
(11),
was
found to have biases given in Theorem 3.3, and
has a lower estimated standard error of ,31 on average than
for the special case of the so-called missing-group method
does the complete-case analysis. This advantage disappears
(12), biases were given in Corollary 3.1. The missing-group
when X1 and X2 are correlated or when X2 is missing as
method
overestimates the residual variance. Hence the ada function of both X1 and X2, as shown in Table 4 where
dition
of
a missing group can weaken the power of the t test
the bias in estimating /1 is unacceptable. Knowledge of
that
compares
two of the nonmissing groups. On the other
why X2 is missing and of the correlation among covariates
hand, the complete-case analysis uses an unbiased estimawhen X2 is missing is essential when using MIM I for
of the residual variance, but its t test is based on fewer
MIM I overestimates the standard error of f2 Use of tor
MIM
degrees
of freedom. As discussed at the end of Section 3,
I is generally ill-advised. But some special cases in which
the
complete-case
analysis is generally preferable.
a covariate is missing by design due to cost considerations
Of
those
researchers
suggesting the missing-data methshould be investigated. MIM II shows no advantage over
ods
studied
in
this
article,
Cohen and Cohen (1975) recogthe complete-case analysis.
nized that the residual variance may be overestimated, as
they stated that the power of the regression analysis may
6. DISCUSSION
be weakened if one truly knows that the data are missing at
random. By "missing at random" they mean that X2 is not
This article has investigated the possible bias in the estimissing as a function of (Y, X1, X2, 6). This corresponds to
mators of the regression coefficients and residual variance
derived from the missing-indicator and stratification meth- the " missing completely at random" definition given by Litat random and P(Q2 = 1) = .5. Table 4 summarizes the

ods of handling missing data. In particular, the true regres- tle and Rubin (1987). In general, Cohen and Cohen (1975)
recommended against such an assumption; however, they
sion relationship between Y and (XI, X2) is assumed to
did not recognize that MIM I will produce biased regression
be given in (1), in which X2 can be missing as a function
coefficient estimators, even when the assumption is correct.
of X1 and/or X2 but not as a function of E. Hence, given

X1 and X2, missingness of X2 is conditionally indepen-

APPENDIX: PROOFS OF THEOREMS

dent of Y. The missing-data methods studied in this article

include (a) complete-case analysis, modeled in (2), with biases in Theorem 2.1; (b) missing-indicator method (MIM)
I (6) with biases in Theorem 3.1; (c) MIM 11 (10) with biases in Theorem 3.1; (d) stratification method I (13), with
biases in Theorem 4.1; and (e) stratification methods II and
III, modeled in (14) and (15), with biases given in Theorem 4.2. In summary, the complete-case analysis is valid so
long as the covariates are independent of E; they need not
be missing completely at random. MIM I and stratification
method I produce biased regression parameter estimators.
MIM's II and III and stratification methods II and III give
the same regression estimators as the complete-case analysis and thus are unbiased. But they each overestimate the
residual variance. The magnitude of these biases was stud-

With the exception of Theorem 2. 1, the proofs of the the
are given in this Appendix.

Proof of Theorem 3.1
This proof is very lengthy and tedious, so only a sketch of it is

given. Define Y' =(Y1,..., I,),/ ='(E1, * n

U Xl21 X21
_

1 Xl. X2,

and

Q21 Xll X21Q21 1-CQ21

ied further in Section 5 via asymptotic theory and computer
simulation. In Section 5.1, where X2 is missing completely

( Q2n Xln X2nQ2n 1-Q2n /

at random, the biases in estimating ,1, 2, and 2 by MIM
I were seen to be arbitrarily large, depending on the per-

centage of missing data, the value of /32, the ratio of X

Next, some definitions are necessary. Define QC2 = Q2i/n.
j, k C {1,2}, let

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

Jones:

Methods

for

Missing

X = ZQ2iXji/nQ2,

Explanatory

Variables

229

Proof of Theorem 3.3
The data analyst-assumed models for the subsets of the data

X = E (1 - Q2i)Xji/n(- Q2),

with and without measured Zi values are

Y =yo+yiZ?+ei (Qi=1)

Sik = Q2i (Xj - )(Xki - Q2,

and

Yi = Yo +Ylc+Y2 +ei (Qi = O).

Sjk Q2i)(Xji (-fX )(Xki -CXk ) -Q2),

The least squares estimators 'yo and 'y, are derived completely
from the complete-case subset of the data, while

and

r2= S12 2

These terms represent the sample means, variances, and covariances for those with and without X2 information. Because

0 = (X XI)1)'X'Y = (XI XI))'XI(X/3 + ?),

= - -O - Y1C,

where ym is the average Y value in the missing-data subset. Conditional on Z and Q, EYm = 3o + ?1 Zm, and be-

cause (yo, y,) are unbiased complete-case estimators, E-2 =
+ f31Zm - p3o - /31c = f31(Zm - c). These results can also be
derived from the usual straightforward but tedious solution of the
normal equations.

and because, by assumption,

Let X be the n x 3 matrix whose ith row is (1, ZiQi + c(l

E[(XIXI) 'X/EIX, Q] = (XI XI)<'X/E(E) = 0,

- Qi), 1 - Qi) and let H = X(X'X)-'X'. Then
E(RSS) = E[Y'(I-H)Y]

the proof consists of showing that (X'X1)-'1X1X is given as

= E[E'(I - H)c] + E[(/3ol + 31Z)'

in Theorem 3.1 with

x (I-H)(fol?+l1Z)].

Fo = (S12X2C -S22Xl)1D,

By theorem 1.7 of Seber (1977), E[c'(I - H)c] = a2(n -3).

F1 = S221D,

Because I - H is a projection matrix, the last term of E(RSS) is
the RSS from regressing 3ol + /1Z onto the columns of X. To

F2 = S12ID,

n vector with ith element (1 - Qi) (Zi - c). Then one can easily
show that 3o +? 1 Z = X,8* +?31 Z*. Because X is orthogonal to

see the rest of the result, let f3* = (3o,1, 0)' and let Z* be an

and

I - H,
where D = (1 - Q2)S1lmlS2C2 + Q2SlS2C2[1 - (r12)2]. The expec-

tation of 03 iS /o ? /2 times a very messy expression, which is

(Aol + /1Z)'(I - H)(:3ol +? 31Z) = 323Z*'(I-H - Z*

omitted here.

which is '32 times the RSS from regressing Z* on the colum

Proof of Theorem 3.2

of X.

As already stated, MIM II, modeled by (10), can be rewritten
Proof of Corollary 3.1
as submodels (7) and (9) for the subsets of individuals with and
The least squares estimators follow directly from Theorem 3.3.
without X2 information. For the purpose of finding the bias in
Here
c = 0. With regards to the E(MSE), defined in Theorem
the predictor coefficients, it is sufficient to consider the submodels
separately, because the coefficients are different between the two 3.3, 7o = 71 = 0 and r72 = z = m2/(ml + m2). The rest
follows after noting that the MSE* of Theorem 3.3 is EQi(Zi
submodels. Model (7) is in fact a complete-case analysis for that
- Z m)2 /(n - 3).
subset, and thus by Theorem 2.1, E(Oo) = 3o,E(01) = /1, and
E(02) = 32. Model (9) underfits the true model by omitting X2.
Proof of Theorem 4.2
By Lemma 3.1,

The true model for subjects in stratum j is

- ) 1 + (X*/X*)3)

Yi = (30 +,32Cj) +,3lXli + Ei, j =1.,k.
Model (14) is considered first. This model allows separate

where X* is the n x 2 design matrix (1, Xi). The latter term is

(-yoj, -ylj) for each stratum and hence the least squares esti-

/32 times the least squares intercept and slope estimators from themators of them are unbiased for j = 1, . .. , k. Estimation of
regression of X2 on Xi.

(-YO,k+l, -Yl,k+l) is the omitted covariate problem, and the reModel (10) assumes the var(ei) = a2, regardless of whether an
sult follows directly from Lemma 3.1. The expected RSS for
individual possesses or is missing X2 information. The RSS for
the complete-case strata is (n -E Ik+?,i - 2k)o2, whereas that
model (10) can be split into two parts, one for each submodel.
of the missing-data stratum, according to Lemma 3.1, is
Because submodel (7) is a complete-case analysis for that subset,
(Z Ik+1,i -2) 2 +?3p2RSSm(X21Xl).
then by Theorem 2.1, the RSS for (7) in the subset with X2 data
is unbiased for [(Z Q2i) - 3]T2. Because submodel (9) omits the
Adding these sums of squares and dividing by n - 2(k + 1) yields
predictor X2, then by Lemma 3.1, the RSS for (9) in the subset
the expected MSE for (14).
missing X2 is [(n - Z Q2i) - 2]o2 + /32XX(I - H*)X2, where
Model (15) is considered next. Estimation of {1o1,... ,-YOk,y }
H* = X*(X*/X*)-1X*/. Note that X'2(In - H*)X2 is the RSS
is equivalent to estimation in a complete-case analysis that,
from regressing X2 on X, in the subset missing X2. Hence the
RSS for the entire data set is unbiased for

by Theorem 2.1, produces unbiased estimators. Estimation of

(7yo,k?1, y1,k?1) iS the same as under model (14). The verification of the expected MSE parallels that of model (14), except that

(n - 5)a2 ? /32RSSm (X2 IX,).-

the number of parameters is k ? 3.

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

230 Journal of the American Statistical Association, March 1996

REFERENCES
Afifi, A. A., and Elashoff, R. M. (1967), "Missing Observations in Multivariate Statistics II: Point Estimation in Simple Linear Regression,"
Journal of the American Statistical Association, 62, 10-29.

Anderson, A. B., Basilevsky, A., and Hum, D. P. J. (1983), "Missing Data:
A Review of the Literature," in Handbook of Survey Research, eds.
P. H. Rossi, J. D. Wright, and A. Anderson, New York: Academic Press,

in the Presence of Missing Values," in Proceedings of Business and
Economics Section, American Statistical Association, pp. 417-420.

Cohen, J., and Cohen, P. (1975), Applied Multiple Regression Correlation
Analysis for the Behavioral Sciences, New York: John Wiley.

Little, R. J. A. (1992), "Regression With Missing X's: A Review," Journal
of the American Statistical Association, 87, 1227-1237.

Little, R. J. A., and Rubin, D. B. (1987), Statistical Analysis With Missing
Data, New York: John Wiley.

pp. 415-492.

Aptech Systems, Inc. (1991), GAUSS (Version 2.2), Maple Valley, WA:

Miettinen, 0. S. (1985), Theoretical Epidemiology: Principles of Occurrence Research in Medicine, New York: John Wiley.

Author.

Chow, W. K. (1979), "A Look at Various Estimators in Logistic Models

Seber, G. A. F. (1977), Linear Regression Analysis, New York: John Wiley.

This content downloaded from 206.253.207.235 on Fri, 08 Mar 2019 22:06:52 UTC
All use subject to https://about.jstor.org/terms

