Econometrica, Vol. 72, No. 6 (November, 2004), 1845â€“1857

NOTES AND COMMENTS
CONFIDENCE INTERVALS FOR PARTIALLY IDENTIFIED PARAMETERS
BY GUIDO W. IMBENS AND CHARLES F. MANSKI1
Recently a growing body of research has studied inference in settings where parameters of interest are partially identified. In many cases the parameter is real-valued
and the identification region is an interval whose lower and upper bounds may be estimated from sample data. For this case confidence intervals (CIs) have been proposed
that cover the entire identification region with fixed probability. Here, we introduce a
conceptually different type of confidence interval. Rather than cover the entire identification region with fixed probability, we propose CIs that asymptotically cover the
true value of the parameter with this probability. However, the exact coverage probabilities of the simplest version of our new CIs do not converge to their nominal values
uniformly across different values for the width of the identification region. To avoid
the problems associated with this, we modify the proposed CI to ensure that its exact
coverage probabilities do converge uniformly to their nominal values. We motivate this
modified CI through exact results for the Gaussian case.
KEYWORDS: Bounds, identification regions, confidence intervals, uniform convergence.

1. INTRODUCTION
IN THE LAST DECADE a growing body of research has studied inference in settings
where parameters of interest are partially identified (see Manski (2003) for an overview
of this literature). In many cases, where the parameter is real-valued, the identification
region is an interval whose lower and upper bounds may be estimated from sample
data. Confidence intervals (CIs) may be constructed to take account of the sampling
variation in these estimates. Early on, Manski, Sandefur, McLanahan, and Powers
(1992) computed separate confidence intervals for the lower and upper bounds. Subsequently, Horowitz and Manski (2000) developed CIs that asymptotically cover the
entire identification region with fixed probability. Recently Chernozhukov, Hong, and
Tamer (2003) extended this approach to settings with vector-valued parameters defined
through minimization problems.
Here, we introduce a conceptually different type of confidence interval. Rather than
cover the entire identification region with fixed probability Î±, we propose CIs that asymptotically cover the true value of the parameter with this probability. The key insight
is that when the identification region has positive width, the true parameter can be close
to at most one of the regionâ€™s boundaries. Suppose that the true value is close to the
upper bound of the identification region. Then, asymptotically the probability that the
estimate for the lower bound exceeds the true value can be ignored when making inference on the true parameter. This allows the researcher to allocate the entire probability
1
We thank Joerg Stoye for excellent research assistance, Michael Jansson, Francesca Molinari,
Tom Rothenberg, a coeditor, and two anonymous referees for comments, and the NSF for support through Grants SES 0136789 (Imbens) and SES 0314312 (Manski). Imbens also acknowledges financial support from the Giannini Foundation and the Agricultural Experimental Station
at UC Berkeley.

1845

1846

G. IMBENS AND C. MANSKI

of making an error, 1 âˆ’ Î±, to values above the upper-bound point estimate. We do not
know whether the true parameter is close to the upper or lower bound, so one-sided
intervals with confidence level Î± are constructed around both bounds.
To illustrate the nature of our CIs for partially identified parameters, we construct
CIs for the mean of a bounded random variable when some data are missing and the
distribution of missing data is unrestricted. We show that our CIs for the parameter are
proper subsets of the corresponding CIs for the identification region, with the difference in width related to the difference in critical values for one- and two-sided tests.
However, the exact coverage probabilities of the simplest version of our new CIs do
not converge to their nominal values uniformly across different values for the width of
the identification region. A consequence is that confidence intervals can be wider when
the parameter is point-identified than when it is set-identified. To avoid this anomaly,
we modify the proposed CI to ensure that its exact coverage probabilities do converge
uniformly to their nominal values. We motivate this modified CI through exact results
for the Gaussian case.
2. CONFIDENCE INTERVALS FOR PARAMETERS AND IDENTIFICATION REGIONS
Many problems of partial identification have the following abstract structure. Let
(â„¦ A P) be a specified probability space, and let P be a space of probability distributions on (â„¦ A). The distribution P is not known, but a random sample of size N
is available, with empirical distribution PN . Let Î» be a quantity that is known only to
belong to a specified set Î›. Let f (Â· Â·) : P Ã— Î› â†’ R be a specified real-valued function.
The object of interest is the real parameter Î¸ = f (P Î»). Then the identification region
for f (P Î») is the set {f (P Î» ) Î» âˆˆ Î›}. Suppose that Î»l (P) = argminÎ» âˆˆÎ› f (P Î» ) and
Î»u (P) = argmaxÎ» âˆˆÎ› f (P Î» ) exist for all P âˆˆ P . We focus on the class of problems in
which the identification region is the closed interval [f (P Î»l (P)) f (P Î»u(P))]. Manski
(2003) describes various problems in this class.
It is natural to estimate the identification region [f (P Î»l (P)) f (P Î»u(P))] by
its sample analog [f (PN  Î»l (PN )) f (PN  Î»u (PN ))], which is consistent under standard regularity conditions. It is also natural to construct confidence intervals for
[f (P Î»l (P)) f (P Î»u(P))] of the form [f (PN  Î»l (PN )) âˆ’ CN0  f (PN  Î»u (PN )) + CN1 ],
where (CN0  CN1 ) are specified nonnegative numbers that may depend on the sample data. Horowitz and Manski (2000) proposed CIs of this form and showed how
(CN0  CN1 ) may be chosen to achieve a specified asymptotic probability of coverage
of the identification region. Chernozhukov, Hong, and Tamer (2003) study confidence
sets with the same property in more general settings. In this paper, we study the use
of these intervals as CIs for the partially identified parameter f (P Î»). Our most basic
finding is Lemma 1:
LEMMA 1: Let CN0 â‰¥ 0, CN1 â‰¥ 0, Î» âˆˆ Î›, and P âˆˆ P . The probability that the
interval [f (PN  Î»l (PN )) âˆ’ CN0  f (PN  Î»u (PN )) + CN1 ] covers the parameter f (P Î»)
is at least as large as the probability that it covers the entire identification region
[f (P Î»l (P)) f (P Î»u(P))].
All proofs are given in the Appendix.
An implication of the lemma is that researchers face a substantive choice whether
to report intervals that cover the entire identification region or intervals that cover the

PARTIALLY IDENTIFIED PARAMETERS

1847

true parameter value with some fixed probability. Although both intervals generally
converge to the identification region as N â†’ âˆ their difference typically is of the
order Op (N âˆ’1/2 ) Which CI is of interest depends on the application.
3. MEANS WITH MISSING DATA AND KNOWN PROPENSITY SCORE
In this section we construct CIs for the mean of a bounded random variable when
some data are missing and the distribution of missing data is unrestricted. Let (Y W )
be a pair of random variables, where Y takes values in the bounded set Y and W is
binary with values {0 1}; without loss of generality, let the smallest and largest elements
of Y be 0 and 1, respectively. The researcher has a random sample of (Wi  Yi Â· Wi ),
i = 1     N, so Wi is always observed and Yi is only observed if Wi = 1. Define Âµ =
E[Y |W = 1], Î» = E[Y |W = 0], Ïƒ 2 = V(Y |W = 1), and p = E[W ], with 0 < p â‰¤ 1. We
assume initially that p is known. This will be relaxed in Section 4. Let F(y) be the
conditional distribution function of Y given W = 1, an element of the set of distribution
functions F with variance Ïƒ 2 â‰¤ Ïƒ 2 â‰¤ Ïƒ 2 , for some positive and finite Ïƒ 2 and Ïƒ 2 . The
distribution of Y given W = 0 is unknown; hence, Î» âˆˆ [0 1]. The parameter of interest
is Î¸ = E[Y ] = Âµ Â· p + Î» Â· (1 âˆ’ p). The identification region for Î¸ is the closed interval
[Î¸l  Î¸u ] = [Âµ Â· p Âµ Â· p + 1 âˆ’ p].
With p known, the only unknown determinant of the interval boundaries is the
conditional mean Âµ. This parameter can be estimated by its sample analog ÂµÌ‚ =
N
N
i=1 Wi Â· Yi /N1 (where N1 =
i=1 Wi ), and the identification region can be consistently
estimated by [Î¸Ì‚l  Î¸Ì‚u ] = [ÂµÌ‚ Â· p ÂµÌ‚ Â· p + 1 âˆ’ p] The first step towards constructing
CIs
âˆš
is to consider inference for Âµ. Using standard large sample results, we have N(ÂµÌ‚ âˆ’
N
d
Âµ) â†’ N (0 Ïƒ 2 /p) A consistent estimator for Ïƒ 2 is ÏƒÌ‚ 2 = i=1 Wi Â· (Yi âˆ’ ÂµÌ‚)2 /(N1 âˆ’ 1).
Hence, the standard 100 Â· Î±% confidence interval for Âµ is


ÏƒÌ‚
ÏƒÌ‚
Âµ
(1)
 ÂµÌ‚ + z(Î±+1)/2 Â· 

CI Î± = ÂµÌ‚ âˆ’ z(Î±+1)/2 Â· 
pÂ·N
pÂ·N
âˆš
 zÏ„
where zÏ„ is the Ï„ quantile of the standard normal distribution: Î¦(zÏ„ ) = âˆ’âˆ
(1/ 2Ï€ ) Ã—
2
eâˆ’y /2 dy = Ï„. In the point identified case with p = 1 we have Î¸ = Î¸l = Î¸u = Âµ, and thus
in that case CI ÂµÎ± is also the appropriate CI for Î¸ and [Î¸l  Î¸u ].
Now consider symmetric CIs for the identification region [Î¸l  Î¸u ] and the parameter Î¸. The CI for [Î¸l  Î¸u ] substitutes the lower (upper) confidence bound for Âµ into the
lower (upper) bound for the identification region:





ÏƒÌ‚
ÏƒÌ‚
CI Î±[Î¸l Î¸u ] = ÂµÌ‚ âˆ’ z(Î±+1)/2 Â· 
(2)
Â· p ÂµÌ‚ + z(Î±+1)/2 Â· 
Â·p+1âˆ’p 
pÂ·N
pÂ·N
Note that as p â†’ 1, CIÎ±[Î¸l Î¸u ] â†’ CIÎ±Âµ . The CI for Î¸ adjusts the critical values to obtain
the appropriate coverage for Î¸:





ÏƒÌ‚
ÏƒÌ‚
Î¸


CI Î± = ÂµÌ‚ âˆ’ zÎ± Â·
(3)
Â· p ÂµÌ‚ + zÎ± Â·
Â·p+1âˆ’p 
pÂ·N
pÂ·N

Note that this is a proper interval only if 2zÎ± ÏƒÌ‚/ pN > âˆ’(1 âˆ’ p)/p, which is always
true if Î± â‰¥ 5 and will be true with probability arbitrarily close to one for N large if

1848

G. IMBENS AND C. MANSKI

Î± < 5. One can modify the interval if this condition is not satisfied without affecting
the asymptotic properties. The following lemma describes the large sample properties
of these intervals.
LEMMA 2: For any p0 > 0,
(i) infF âˆˆF p0 â‰¤pâ‰¤1 limNâ†’âˆ Pr([Î¸l  Î¸u ] âŠ‚ CIÎ±[Î¸l Î¸u ] ) = Î±;
(ii) infF âˆˆF Î»âˆˆÎ›p0 â‰¤p<1 limNâ†’âˆ Pr(Î¸ âˆˆ CIÎ±Î¸ ) = Î±.
Although the confidence interval CIÎ±Î¸ has in large samples the appropriate confidence level for all values of p, Î», and F , it has an unattractive feature. The issue is that
for any N one can find a value of p and Î» such that the coverage is arbitrarily close to
100 Â· (2Î± âˆ’ 1)%, rather than the nominal 100 Â· Î±%. To see this, we consider
 an example
with Y |W = 1 normal with mean Âµ and known variance Ïƒ 2 . Let pÌ‚ = i Wi /N. The
exact coverage probability of CI Î¸Î± for Î¸, conditional on pÌ‚, at Î» = 0 (so Î¸ = Âµ Â· p) is
 




pÌ‚
pÌ‚
(1 âˆ’ p)
Î¸
Pr(Î¸ âˆˆ CI Î± ) = Î¦ zÎ± Â·
âˆ’ Î¦ âˆ’zÎ± Â·
âˆ’ N pÌ‚ Â·

p
p
Ïƒp
For any fixed p âˆˆ (0 1), this coverage probability approaches Î± with probability one as
N â†’ âˆ. However, for any fixed N < âˆ, the coverage probability approaches 2Î± âˆ’ 1
with probability one as p â†’ 1. This example shows that the asymptotic coverage result in Lemma 2 is very delicate.âˆšOne can also see this by considering the width of
âˆš
to 2zÎ± Â· ÏƒÌ‚ p/ N + 1 âˆ’ p. As p â†’ 1, for fixed N, this width coninterval CI Î¸Î± equalâˆš
verges to 2zÎ± Â· ÏƒÌ‚/ N. This is strictly less than the width of CI ÂµÎ± , which is the standard interval for Î¸ for the point-identified case with p = 1. It is counterintuitive that
the CI for Î¸ should be shorter when the parameter is partially identified than when it
is point-identified. The anomaly arises because the coverage of CI Î¸Î± does not converge
uniformly in (F Î» p) and, in particular, not uniformly in p.
We propose here a modification of CI Î¸Î± whose coverage probability does converge
uniformly in (F Î» p) To motivate the modification, again consider the case where
Y |W = 1 âˆ¼ N (0 Ïƒ 2 ) with known Ïƒ 2 . The conditional coverage rate for symmetric intervals of the form [Î¸Ì‚l âˆ’ D Î¸Ì‚u + D] is
Pr(Î¸Ì‚l âˆ’ D â‰¤ Î¸ â‰¤ Î¸Ì‚u + D|pÌ‚)




D + (1 âˆ’ Î») Â· (1 âˆ’ p)
D + Î» Â· (1 âˆ’ p)
=Î¦
N pÌ‚ Â·
âˆ’ Î¦ âˆ’ N pÌ‚ Â·

Ïƒp
Ïƒp
To get the coverage rate to be at least Î± for all values of Î», one needs to choose D to
solve:




D + (1 âˆ’ p)
D
Î¦ N pÌ‚ Â·
âˆ’ Î¦ âˆ’ N pÌ‚ Â·
= Î±
Ïƒp
Ïƒp

To facilitate comparison with the previous CI, let CN = D N pÌ‚/(pÏƒ) so that CN solves


1âˆ’p
Î¦ CN + N pÌ‚ Â·
âˆ’ Î¦(âˆ’CN ) = Î±
Ïƒp

1849

PARTIALLY IDENTIFIED PARAMETERS

with the corresponding confidence interval


pÏƒ
pÏƒ
Î¸
 ÂµÌ‚ Â· p + (1 âˆ’ p) + CN 
CI Î± = ÂµÌ‚ Â· p âˆ’ CN 

N pÌ‚
N pÌ‚
For any fixed 0 < p < 1, limNâ†’âˆ CN = zÎ± , which would give us the interval CI Î¸Î± back.
For fixed N, as p â†’ 1, the interval estimate now converges to CI ÂµÎ± with no discontinuity
at p = 1. For 0 < p < 1, the confidence interval is strictly wider than the interval for
p = 1.
For the general case with unknown distribution for Y |W = 1, we construct a CI by
replacing Ïƒ by ÏƒÌ‚ and pÌ‚ by p:
(4)




Î¸
CI Î± = ÂµÌ‚ âˆ’ CN Â· ÏƒÌ‚/ p Â· N Â· p ÂµÌ‚ + CN Â· ÏƒÌ‚/ p Â· N Â· p + 1 âˆ’ p 

where CN satisfies


âˆš
1âˆ’p
Î¦ CN + N Â· âˆš
(5)
âˆ’ Î¦(âˆ’CN ) = Î±
ÏƒÌ‚ p
Lemma 3 shows that the new interval has a coverage rate that converges uniformly in
(F Î» p):
LEMMA 3: For any p0 > 0,
lim

inf

Nâ†’âˆ F âˆˆF Î»âˆˆÎ›p0 â‰¤pâ‰¤1

Pr(Î¸ âˆˆ CI Î± ) = Î±
Î¸

[Î¸l Î¸u ]
It is interesting to compare
, CI Î¸Î± , and CI Î± in terms of the
 the three intervals CI Î±
constants that multiply ÏƒÌ‚/ p Â· N, the standard error of ÂµÌ‚. The form of the intervals
is the same for all three cases and the width of the intervals is strictly increasing in
this constant, so we can compare the widths by directly comparing these constants. For
l Î¸u ]
CI [Î¸
, the constant is z(Î±+1)/2 , which solves Î¦(C) âˆ’ Î¦(âˆ’C) = Î± For CI Î¸Î± , the constant
Î±
Î¸
is zÎ± , which solves Î¦(âˆ) âˆ’ Î¦(âˆ’C) = Î± and which is strictly smaller. For CI Î± , the
constant is CN which solves


âˆš
1âˆ’p
Î¦ C+ NÂ· âˆš
âˆ’ Î¦(âˆ’C) = Î±
ÏƒÌ‚ p
Î¸

Unless p = 1, this is strictly between the first two constants so CI Î¸Î± âŠ‚ CI Î± âŠ‚ CI Î±[Î¸l Î¸u ] .
Î¸
If the parameter is point identified (p = 1), then CN = z(Î±+1)/2 and CI Î¸Î± âŠ‚ CI Î± =
l Î¸u ]
= CI ÂµÎ± .
CI [Î¸
Î±
4. THE GENERAL CASE
Here we develop a confidence interval that converges uniformly in more general
settings, including ones in which the width of the identification region is a nuisance

1850

G. IMBENS AND C. MANSKI

parameter that must be estimated. We use the same structure and notation as in Section 2. Define Î¸l = f (P Î»l (P)), Î¸u = f (P Î»u (P)), and âˆ† = Î¸u âˆ’ Î¸l , and let Î¸Ì‚l , Î¸Ì‚u , and
âˆ†Ë† = Î¸Ì‚u âˆ’ Î¸Ì‚l be estimators for Î¸l , Î¸u , and âˆ†. Then the identification region [Î¸l  Î¸u ] is
naturally estimated by its sample analog [Î¸Ì‚l  Î¸Ì‚u ].
We consider the following set of assumptions:
ASSUMPTION 1: (i) There are estimators for the lower and upper bound Î¸Ì‚l and Î¸Ì‚u that
satisfy:
âˆš
N



Î¸Ì‚l âˆ’ Î¸l
Î¸Ì‚u âˆ’ Î¸u



  
0
Ïƒl2
âˆ’â†’ N

0
ÏÏƒl Ïƒu
d

ÏÏƒl Ïƒu
Ïƒu2




uniformly in P âˆˆ P , and there are estimators ÏƒÌ‚l2 , ÏƒÌ‚u2 , and ÏÌ‚ for Ïƒl2 , Ïƒu2 , and Ï that converge
to their population values uniformly in P âˆˆ P . (Ï may be equal to one in absolute value, as
in the case where the width of the identification region is known.)
(ii) For all P âˆˆ P , Ïƒ 2 â‰¤ Ïƒl2  Ïƒu2 â‰¤ Ïƒ 2 for some positive and finite Ïƒ 2 and Ïƒ 2 , and
Î¸u âˆ’ Î¸l â‰¤ âˆ† < âˆ.
âˆš
(iii) For all  > 0, there are Î½ > 0, K, and N0 such that N â‰¥ N0 implies Pr( N|âˆ†Ë† âˆ’
âˆ†| > Kâˆ†Î½ ) <  uniformly in P âˆˆ P .
Given Assumption 1 we construct the confidence interval as:
(6)

âˆš
âˆš 
Î¸
CI Î± = Î¸Ì‚l âˆ’ C N Â· ÏƒÌ‚l / N Î¸Ì‚u + C N Â· ÏƒÌ‚u / N 

where C N satisfies
(7)


âˆš
Î¦ CN + N Â·


âˆ†Ë†
âˆ’ Î¦(âˆ’C N ) = Î±
max(ÏƒÌ‚l  ÏƒÌ‚u )

The following lemma gives the general uniform coverage result.
LEMMA 4: Suppose Assumption 1 holds. Then
lim

inf

Nâ†’âˆ PâˆˆPÎ»âˆˆÎ›

Î¸

Pr(Î¸ âˆˆ CI Î± ) â‰¥ Î±

Next we return to the missing data problem of Section 3. We allow for an unknown p
(assuming p is bounded away from zero) and show that this problem fits the assumption
sufficient for the application of Lemma 4. Because the conditional variance of Y given
W = 1 is bounded and bounded away from zero, Assumption 1(ii) is satisfied. The
N
lower bound can be estimated by Î¸Ì‚l = (1/N) i=1 Wi Â· Yi  The upper bound can be
N
estimated by Î¸Ì‚u = (1/N) i=1 (Wi Â· Yi + 1 âˆ’ Wi ). Both estimators are asymptotically

PARTIALLY IDENTIFIED PARAMETERS

1851

âˆš
âˆš
d
d
normal, with N(Î¸Ì‚l âˆ’ Î¸l ) â†’ N (0 Ïƒl2 ) and N(Î¸Ì‚u âˆ’ Î¸u ) â†’ N (0 Ïƒu2 ) where Ïƒl2 = Ïƒ 2 Â·
2
2
2
2
p + Âµ Â· p Â· (1 âˆ’ p) and Ïƒu = Ïƒ Â· p + Âµ Â· p Â· (1 âˆ’ p) + p Â· (1 âˆ’ p) âˆ’ 2 Â· Âµ Â· p Â· (1 âˆ’ p). Since
the convergence is also uniform in P, Assumption 1(i) is satisfied. Finally, consider
Assumption 1(iii). Let Î½ = 1/2, and N0 = 1. In the missing data case âˆ†Ë† = 1 âˆ’ pÌ‚. The
variance of âˆ†Ë† is âˆ†(1 âˆ’âˆšâˆ†)/N. Hence, E[N Â· (âˆ†Ë† âˆ’ âˆ†)2 ] â‰¤ âˆ†. Now apply Chebyshevâ€™s
inequality, with K = 1/ , so that
âˆš
Pr N|âˆ†Ë† âˆ’ âˆ†| > K Â· âˆ†Î½ = Pr N(âˆ†Ë† âˆ’ âˆ†)2 > K 2 Â· âˆ†2Î½
< E[N Â· (âˆ†Ë† âˆ’ âˆ†)2 ]/(K 2 âˆ†2Î½ )
â‰¤ âˆ†/(K 2 âˆ†2Î½ ) = 1/K 2 = 
Hence Assumption 1 is satisfied, and Lemma 4 can be used to construct a CI which is
Î¸
equivalent to that obtained by substituting pÌ‚ for p in CI Î± given in (4).
Dept. of Economics, and Dept. of Agricultural and Resource Economics, 661 Evans
Hall, University of California at Berkeley, Berkeley, CA 94720-3880, U.S.A., and NBER;
imbens@econ.berkeley.edu; http://elsa.berkeley.edu/users/imbens/
and
Dept. of Economics and Institute for Policy Research, Northwestern University,
2001 Sheridan Rd., Evanston, IL 60208-2600, U.S.A.; cfmanski@northwestern.edu;
http://www.faculty.econ.northwestern.edu/faculty/manski/.
Manuscript received May, 2003; final revision received March, 2004.
APPENDIX
PROOF OF LEMMA 1: Define the following two events:


A1 = f (P Î») âˆˆ f (PN  Î»l (PN )) âˆ’ CN0  f (PN  Î»u (PN )) + CN1 


A2 = f (P Î»l (P)) f (P Î»u (P)) âŠ‚ f (PN  Î»l (PN )) âˆ’ CN0  f (PN  Î»u (PN )) + CN1 

Because f (P Î») âˆˆ [f (P Î»l (P)) f (P Î»u (P))], it follows that A2 implies A1 and that the coverage probability for the set (equal to the probability of the event A2 ) is less than the coverage
Q.E.D.
probability for the parameter (equal to the probability of the set A1 ).
PROOF OF LEMMA 2: For the first part, fix F and p. Then
Pr [Î¸l  Î¸u ] âŠ‚ CI Î±[Î¸l Î¸u ]



ÏƒÌ‚
Â· p and
= Pr Î¸l â‰¥ ÂµÌ‚ âˆ’ z(Î±+1)/2 Â· 
pÂ·N



ÏƒÌ‚
Î¸u â‰¤ ÂµÌ‚ + z(Î±+1)/2 Â· 
Â·p+1âˆ’p
pÂ·N



ÏƒÌ‚
= 1 âˆ’ Pr Î¸l < ÂµÌ‚ âˆ’ z(Î±+1)/2 Â· 
Â· p or
pÂ·N



ÏƒÌ‚
Î¸u > ÂµÌ‚ + z(Î±+1)/2 Â· 
Â·p+1âˆ’p
pÂ·N

1852

G. IMBENS AND C. MANSKI
 


ÏƒÌ‚
Â·p
= 1 âˆ’ Pr Âµ Â· p < ÂµÌ‚ âˆ’ z(Î±+1)/2 Â· 
pÂ·N




ÏƒÌ‚
âˆ’ Pr Âµ Â· p + 1 âˆ’ p > ÂµÌ‚ + z(Î±+1)/2 Â· 
Â·p+1âˆ’p
pÂ·N




ÏƒÌ‚
ÏƒÌ‚
âˆ’ Pr Âµ > ÂµÌ‚ + z(Î±+1)/2 Â· 

= 1 âˆ’ Pr Âµ < ÂµÌ‚ âˆ’ z(Î±+1)/2 Â· 
pÂ·N
pÂ·N

which converges to 1 âˆ’ (1 âˆ’ Î±)/2 âˆ’ (1 âˆ’ Î±)/2 = Î± as N gets large. For the second part consider
the three possibilities for Î»: Î» = 0, Î» = 1, and 0 < Î» < 1. If Î» = 0,
we have Î¸ = Âµ Â· p. Hence, the
coverage probability of CIÎ±Î¸ is, for N large enough so that 2zÎ± ÏƒÌ‚/ pN > âˆ’(1 âˆ’ p)/p,





ÏƒÌ‚
ÏƒÌ‚
Pr(Î¸ âˆˆ CI Î¸Î± ) = Pr ÂµÌ‚ âˆ’ zÎ± Â· 
Â· p â‰¤ Âµ Â· p â‰¤ ÂµÌ‚ + zÎ± Â· 
Â·p+1âˆ’p
pÂ·N
pÂ·N



ÏƒÌ‚
= 1 âˆ’ Pr ÂµÌ‚ âˆ’ zÎ± Â· 
Â·p>ÂµÂ·p
pÂ·N




ÏƒÌ‚
âˆ’ Pr Âµ Â· p > ÂµÌ‚ + zÎ± Â· 
Â·p+1âˆ’p 
pÂ·N
The second term converges to 1 âˆ’ Î±. The third term converges to zero, which implies the coverage rate is Î±. A similar argument applies when Î» = 1. When Î» âˆˆ (0 1) the coverage probability
converges to one.
Q.E.D.
Before presenting a proof of Lemma 3 we present a number of preliminary results.
LEMMA 5 (Uniform Central Limit Theorem, Berryâ€“Esseen): Suppose X1  X2     are indeN
pendent and identically distributed random variables with c.d.f. F âˆˆ F . Let XÌ„N = i=1 Xi /N,
2
2
2
2
2
Âµ(F) = EF [X], Ïƒ (F) = EF [(X âˆ’ Âµ) ], and let 0 < Ïƒ â‰¤ Ïƒ (F) â‰¤ ÏƒÌ„ < âˆ, and EF [|X 3 |] < âˆ
for all F âˆˆ F . Then
 


âˆš  XÌ„N âˆ’ Âµ 


Pr
 âˆ’â†’ 0
sup
N
<
a
âˆ’
Î¦(a)


Ïƒ
âˆ’âˆ<a<âˆFâˆˆF
See, e.g., Shorack (2000). Next, we show that we can use this to construct confidence intervals
for sample means with asymptotically uniform convergence even with estimated variances.
LEMMA 6: Under the same conditions as in Lemma 5,


ÏƒÌ‚
ÏƒÌ‚
inf Pr XÌ„N âˆ’ z(Î±+1)/2 Â· âˆš â‰¤ Âµ â‰¤ XÌ„N + z(Î±+1)/2 Â· âˆš
âˆ’â†’ Î±
FâˆˆF
N
N
PROOF OF LEMMA 6: First,


ÏƒÌ‚
ÏƒÌ‚
inf Pr XÌ„N âˆ’ z(Î±+1)/2 Â· âˆš â‰¤ Âµ â‰¤ XÌ„N + z(Î±+1)/2 Â· âˆš
FâˆˆF
N
N


âˆš
XÌ„N âˆ’ Âµ
â‰¤ z(Î±+1)/2 
= inf Pr âˆ’z(Î±+1)/2 â‰¤ N Â·
FâˆˆF
ÏƒÌ‚
Hence it will suffice to show that
 




âˆš


XÌ„N âˆ’ Âµ
Pr
 âˆ’â†’ 0
N
<
a
âˆ’
Î¦(a)
sup


ÏƒÌ‚
âˆ’âˆ<a<âˆFâˆˆF

PARTIALLY IDENTIFIED PARAMETERS

1853

By the triangle inequality:
 




âˆš


XÌ„N âˆ’ Âµ
Pr

N
<
a
âˆ’
Î¦(a)


ÏƒÌ‚

 


  

âˆš  XÌ„N âˆ’ Âµ 


ÏƒÌ‚
ÏƒÌ‚  
ÏƒÌ‚

N
<a
âˆ’Î¦ a
+
Î¦
a
âˆ’
Î¦(a)
â‰¤ Pr



Ïƒ
Ïƒ
Ïƒ
Ïƒ
By Lemma 5 the first term converges to zero, and by uniform convergence of ÏƒÌ‚ to Ïƒ the second
one converges to zero.
Q.E.D.
PROOF OF LEMMA 3: First we prove that the asymptotic coverage probability is greater than
or equal to Î±. For fixed Î» the coverage probability is




Pr ÂµÌ‚ âˆ’ CN Â· ÏƒÌ‚/ p Â· N Â· p â‰¤ Âµ Â· p + Î» Â· (1 âˆ’ p) â‰¤ ÂµÌ‚ + CN Â· ÏƒÌ‚/ p Â· N Â· p + 1 âˆ’ p

Î» Â· (1 âˆ’ p)
ÏƒÌ‚ âˆš
= Pr âˆ’CN âˆ’ N Â·
âˆš
Ïƒ
ÏƒÂ· p
â‰¤

âˆš

NÂ·


ÏƒÌ‚ âˆš
Âµ âˆ’ ÂµÌ‚
(1 âˆ’ Î») Â· (1 âˆ’ p)

âˆš â‰¤ CN + N Â·
âˆš
Ïƒ/ p
Ïƒ
ÏƒÂ· p

For any  > 0, there almost surely exists an N0 such that for N > N0 , |(ÏƒÌ‚ âˆ’ Ïƒ)/Ïƒ| < , so that
 > 1 âˆ’ ÏƒÌ‚/Ïƒ. Therefore for N â‰¥ N0 ,


Î» Â· (1 âˆ’ p) âˆš
Âµ âˆ’ ÂµÌ‚
(1 âˆ’ Î») Â· (1 âˆ’ p)
ÏƒÌ‚ âˆš
ÏƒÌ‚ âˆš
N
Â·
N
Â·
Pr âˆ’CN âˆ’ N Â·
â‰¤
â‰¤
C
+
âˆš
âˆš
âˆš
N
Ïƒ
ÏƒÂ· p
Ïƒ/ p
Ïƒ
ÏƒÂ· p

âˆš
Î» Â· (1 âˆ’ p)
â‰¥ Pr âˆ’CN (1 âˆ’ ) âˆ’ N Â·
âˆš
ÏƒÂ· p

âˆš
âˆš
Âµ âˆ’ ÂµÌ‚
(1 âˆ’ Î») Â· (1 âˆ’ p)
â‰¤ N Â· âˆš â‰¤ CN (1 âˆ’ ) + N Â·

âˆš
Ïƒ/ p
ÏƒÂ· p
For N large enough this can be made arbitrarily close to




âˆš
âˆš
(1 âˆ’ Î») Â· (1 âˆ’ p)
Î» Â· (1 âˆ’ p)
(1
âˆ’
)
âˆ’
N
Â·
Î¦ CN (1 âˆ’ ) + N Â·
âˆ’
Î¦
âˆ’C
âˆš
âˆš
N
ÏƒÂ· p
ÏƒÂ· p




âˆš
âˆš
(1 âˆ’ Î») Â· (1 âˆ’ p)
Î» Â· (1 âˆ’ p)
= Î¦ CN + N Â·
âˆ’ Î¦ âˆ’CN âˆ’ N Â·
+ 2CN Ï†(Ï‰)
âˆš
âˆš
ÏƒÂ· p
ÏƒÂ· p
for some Ï‰. Because CN â‰¤ z(Î±+1)/2 (see definition of CN ), and since Ï†(Â·) is bounded, the last term
can be made arbitrarily small by choosing  small. The sum of the first two terms has a negative
second derivative with respect to Î», and so it is minimized at Î» = 0 or Î» = 1. By the definition of
CN it follows that at those values for Î» the value of the sum is Î±. Hence, for any Î½ > 0, for N large
enough, we have




Pr ÂµÌ‚ âˆ’ CN Â· ÏƒÌ‚/ p Â· N Â· p â‰¤ Âµ Â· p + Î» Â· (1 âˆ’ p) â‰¤ ÂµÌ‚ + CN Â· ÏƒÌ‚/ p Â· N Â· p + 1 âˆ’ p
â‰¥ Î± âˆ’ Î½
To prove equality, note that at p = 1 the CI is identical to CIÎ±Âµ , so in that case the asymptotic
coverage rate is equal to Î±.
Q.E.D.

1854

G. IMBENS AND C. MANSKI

Before proving Lemma 4 we establish a couple of preliminary results. Define CÌ†N and CÌˆN by


âˆš
âˆ†
âˆ’ Î¦(âˆ’CÌ†N ) = Î± and
Î¦ CÌ†N + N Â·
max(Ïƒl  Ïƒu )


âˆš
âˆ†Ë†
Î¦ CÌˆN + N Â·
âˆ’ Î¦(âˆ’CÌˆN ) = Î±
max(Ïƒl  Ïƒu )
Ë† while CÌ†N is a sequence of constants.
Note that C N and CÌˆN are stochastic (as they depend on âˆ†),
Next we give two results without proof that show that one can ignore estimation error in
Ïƒl and Ïƒu .
LEMMA 7: Suppose Assumption 1 holds. Then, uniformly in P âˆˆ P ,
|C N âˆ’ CÌˆN | âˆ’â†’ 0
LEMMA 8: For all  > 0, there is an N0 such that for N â‰¥ N0 , uniformly in P âˆˆ P and Î» âˆˆ Î›,
âˆš
âˆš

Pr Î¸Ì‚l âˆ’ C N Â· ÏƒÌ‚l / N â‰¤ Î¸ â‰¤ Î¸Ì‚u + C N Â· ÏƒÌ‚u / N
âˆš
âˆš 
âˆ’ Pr Î¸Ì‚l âˆ’ CÌˆN Â· Ïƒl / N â‰¤ Î¸ â‰¤ Î¸Ì‚u + CÌˆN Â· Ïƒu / N  < 
The next two lemmas account for the effects of estimation error in âˆ†.
LEMMA 9: For any Î·  > 0, there is an N0 such that for N â‰¥ N0 , uniformly in P âˆˆ P ,
 


âˆš
âˆ†
Pr Î¦ CÌˆN + N Â·
âˆ’ Î¦(âˆ’CÌˆN ) < Î± âˆ’ Î· < 
max(Ïƒl  Ïƒu )
âˆš
Ë† max(Ïƒl  Ïƒu )) âˆ’ Î¦(âˆ’CÌˆN ) = Î±, we
PROOF OF LEMMA 9: Because CÌˆN satisfies Î¦(CÌˆN + N âˆ†/
only need to prove that
 




âˆš
âˆš
âˆ†Ë†
âˆ†
Pr Î¦ CÌˆN + N Â·
âˆ’ Î¦ CÌˆN + N Â·
> Î· < 
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )
By Assumption 1(iii) there are Î½, K, and N0 such that with Î´ = Î½/5 and N â‰¥ max(N0  K 1/Î´ ),
âˆš
âˆš
Pr N|âˆ†Ë† âˆ’ âˆ†| > N Î´ âˆ†Î½ â‰¤ Pr N|âˆ†Ë† âˆ’ âˆ†| > Kâˆ†Î½ < 
Then:

(8)


âˆš
Î¦ CÌˆN + N Â·




âˆš
âˆ†Ë†
âˆ†
âˆ’ Î¦ CÌˆN + N Â·
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )
 



Ë†
âˆš
âˆš
âˆ†
âˆ†
= 1{âˆ†Ë† â‰¤ âˆ†} Â· Î¦ CÌˆN + N Â·
âˆ’ Î¦ CÌˆN + N Â·
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )
âˆš


Î´
Î½
+ 1 âˆ†Ë† > âˆ† N|âˆ†Ë† âˆ’ âˆ†| â‰¤ N âˆ†
 



âˆš
âˆš
âˆ†Ë†
âˆ†
Ã— Î¦ CÌˆN + N Â·
âˆ’ Î¦ CÌˆN + N Â·
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )
âˆš


+ 1 âˆ†Ë† > âˆ† N|âˆ†Ë† âˆ’ âˆ†| > N Î´ âˆ†Î½
 



âˆš
âˆš
âˆ†Ë†
âˆ†
Ã— Î¦ CÌˆN + N Â·
âˆ’ Î¦ CÌˆN + N Â·
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )

1855

PARTIALLY IDENTIFIED PARAMETERS
(9)

(10)

âˆš


â‰¤ 1 âˆ†Ë† > âˆ† N|âˆ†Ë† âˆ’ âˆ†| â‰¤ N Î´ âˆ†Î½



 
âˆš
âˆš
âˆ†Ë†
âˆ†
âˆ’ Î¦ CÌˆN + N Â·
Ã— Î¦ CÌˆN + N Â·
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )
âˆš

+ 1 N|âˆ†Ë† âˆ’ âˆ†| > N Î´ âˆ†Î½ 

using the fact that (8) is nonpositive. The expectation of (10) is less than . By a mean value
theorem (9) is, for some Î³ âˆˆ [0 1], equal to
âˆš


1 âˆ†Ë† > âˆ† N|âˆ†Ë† âˆ’ âˆ†| â‰¤ N Î´ âˆ†Î½

 âˆš
âˆš
âˆš
âˆ†Ë† âˆ’ âˆ†
âˆ†Ë† âˆ’ âˆ†
âˆ†
Ã—Ï† CÌˆN + N Â·
+Î³Â· N Â·
Â· NÂ·

max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )
Because the product is zero unless âˆ†Ë† > âˆ†, and CÌˆN  âˆ† â‰¥ 0, this can be bounded from above by


âˆš
âˆš
âˆš


âˆ†
âˆ†Ë† âˆ’ âˆ†
1 âˆ†Ë† > âˆ† N|âˆ†Ë† âˆ’ âˆ†| â‰¤ N Î´ âˆ†Î½ Â· Ï†
NÂ·
(11)
Â· NÂ·
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )


âˆš
âˆš


âˆ†
N Î´ âˆ†Î½
NÂ·
Â·
â‰¤ 1 âˆ†Ë† > âˆ† N|âˆ†Ë† âˆ’ âˆ†| â‰¤ N Î´ âˆ†Î½ Â· Ï†
max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )


2Î´
Î½
âˆš
âˆ†
N âˆ†
â‰¤ N âˆ’Î´ Â· Ï†
NÂ·
Â·

max(Ïƒl  Ïƒu )
max(Ïƒl  Ïƒu )
Maximizing this over âˆ† gives

âˆš
N Î´âˆ’Î½ exp(âˆ’Î½/2)Î½Î½/2 max(Ïƒl  Ïƒu )Î½âˆ’1 / 2Ï€

Given that Î´ < Î½, this can be bounded arbitrarily close to zero uniformly in P âˆˆ P .

Q.E.D.

LEMMA 10: For any Î·  > 0, there is an N0 such that for N â‰¥ N0 , uniformly in P âˆˆ P ,
Pr(CÌˆN < CÌ†N âˆ’ Î·) < 
PROOF: Let Ï† = Ï†(z(Î±+1)/2 ). Note that CÌˆË™N and CÌ†N are positive and less than z(Î±+1)/2 , and thus
Ï†(CÌˆN ) â‰¥ Ï† and Ï†(CÌ†N ) â‰¥ Ï†. Using Lemma 9 there is an N0 such that for N â‰¥ N0
 


âˆš
âˆ†
Pr Î¦ CÌˆN + N Â·
âˆ’ Î¦(âˆ’CÌˆN ) < Î± âˆ’ Î· Â· Ï† < 
max(Ïƒl  Ïƒu )
uniformly in P âˆˆ P . Since

âˆš
Î¦ CÌˆN + N Â·

âˆ†
max(Ïƒl  Ïƒu )


âˆš
â‡’ Î¦ CÌˆN + N Â·


âˆ’ Î¦(âˆ’CÌˆN ) > Î± âˆ’ Î· Â· Ï†

âˆ†
max(Ïƒl  Ïƒu )




âˆš
âˆ’ Î¦ CÌ†N + N Â·

âˆ†
max(Ïƒl  Ïƒu )

âˆ’ Î¦(âˆ’CÌˆN ) + Î¦(âˆ’CÌ†N ) > âˆ’Î· Â· Ï†
 

âˆš
âˆ†
â‡’ Ï† CÌˆN + N Â·
+ Î³ Â· (CÌ†N âˆ’ CÌˆN )
max(Ïƒl  Ïƒu )

+ Ï†(CÌ†N + Î³ Â· (CÌˆN âˆ’ CÌ†N )) Â· (CÌˆN âˆ’ CÌ†N ) > âˆ’Î· Â· Ï†



1856

G. IMBENS AND C. MANSKI
â‡’ Ï† Â· (CÌˆN âˆ’ CÌ†N ) > âˆ’Î· Â· Ï†
â‡’ CÌˆN âˆ’ CÌ†N > âˆ’Î·

for some Î³ âˆˆ [0 1] by the mean value theorem, and thus with probability CÌˆN âˆ’ CÌ†N > âˆ’Î· with
probability at least 1 âˆ’ .
Q.E.D.
Note that Lemma 10 does not imply that |C N âˆ’ CÌ†N | converges to zero uniformly. This is not
necessarily true unless we are willing to rule out values of âˆ† close to zero, which is exactly the
point-identified area with which we are concerned.
PROOF OF LEMMA 4: We will prove that for any positive , for N sufficiently large,
âˆš
âˆš
Pr Î¸Ì‚l âˆ’ C N Â· ÏƒÌ‚l / N â‰¤ Î¸ â‰¤ Î¸Ì‚u + C N Â· ÏƒÌ‚u / N â‰¥ Î± âˆ’ 
uniformly in P âˆˆ P . We will prove this for Î¸ = Î¸u . The proof for Î¸ = Î¸l is analogous, and by
joint normality of the estimators for the upper and lower boundary the coverage probability is
minimized at the boundary of the identification region.
For arbitrary positive âˆš
1 , 2 , and 3 , choose N large enough so that the following conditions
âˆš
are satisfied (i), supz | Pr( N(Î¸Ì‚l âˆ’ Î¸l )/Ïƒl â‰¤ z) âˆ’ Î¦(z)| â‰¤ 1 , (ii), supz | Pr( N(Î¸Ì‚u âˆ’ Î¸u )/Ïƒu â‰¤
z) âˆ’ Î¦(z)| â‰¤ 1 , and (iii), Pr(CÌˆN âˆ’ CÌ†N < âˆ’2 ) < 3 . Existence of such an N follows for conditions
(i) and (ii) from Assumption 4.2, and for condition (iii) from Lemma 10. Define the following
events:
âˆš
âˆš
E1 â‰¡ Î¸Ì‚l âˆ’ C N Â· ÏƒÌ‚l / N â‰¤ Î¸u â‰¤ Î¸Ì‚u + C N Â· ÏƒÌ‚u / N
âˆš
âˆš
E2 â‰¡ Î¸Ì‚l âˆ’ CÌˆN Â· Ïƒl / N â‰¤ Î¸u â‰¤ Î¸Ì‚u + CÌˆN Â· Ïƒu / N
âˆš
âˆš
E3 â‰¡ Î¸Ì‚l âˆ’ (CÌ†N âˆ’ 2 ) Â· Ïƒl / N â‰¤ Î¸u â‰¤ Î¸Ì‚u + (CÌ†N âˆ’ 2 ) Â· Ïƒu / N
âˆš
âˆš
E4 â‰¡ Î¸Ì‚l âˆ’ CÌ†N Â· Ïƒl / N â‰¤ Î¸u â‰¤ Î¸Ì‚u + CÌ†N Â· Ïƒu / N
E5 â‰¡ CÌˆN âˆ’ CÌ†N > âˆ’2 

and E5c â‰¡ CÌˆN âˆ’ CÌ†N â‰¤ âˆ’2 

Note that (E5 âˆ© E3 ) â‡’ E2 and thus (E5 âˆ© E3 ) â‡’ (E2 âˆ© E3 ). Define also
âˆš
P3 â‰¡ Î¦ CÌ†N âˆ’ 2 + N Â· âˆ†/Ïƒl âˆ’ Î¦(âˆ’CÌ†N + 2 )
and
P4 â‰¡ Î¦ CÌ†N +

âˆš

N Â· âˆ†/Ïƒl âˆ’ Î¦(âˆ’CÌ†N ) = Î±

By conditions (i) and (ii), |P3 âˆ’ Pr(E3 )| â‰¤ 21 and |P4 âˆ’ Pr(E4 )| â‰¤ 21 . Also, |P3 âˆ’ P4 | â‰¤ 22 Ï†Ì„, and
by (iii), Pr(E6c ) < 3 . By Lemma 8 it follows that for any 4 > 0 we can choose N large enough so
that |Pr(E1 ) âˆ’ Pr(E2 )| < 4 . Then, by elementary set theory
Pr(E1 ) â‰¥ Pr(E2 ) âˆ’ 4 â‰¥ Pr(E2 âˆ© E3 ) âˆ’ 4 â‰¥ Pr(E5 âˆ© E3 ) âˆ’ 4 â‰¥ Pr(E3 ) âˆ’ Pr(E5c ) âˆ’ 4
â‰¥ P3 âˆ’ 21 âˆ’ 3 âˆ’ 4 â‰¥ P4 âˆ’ 21 âˆ’ 3 âˆ’ 22 Ï†Ì„ âˆ’ 4 = Î± âˆ’ 21 âˆ’ 3 âˆ’ 22 Ï†Ì„ âˆ’ 4 
Since 1      4 were chosen arbitrarily, one can make Pr(E1 ) > Î± âˆ’  for any  > 0.

Q.E.D.

PARTIALLY IDENTIFIED PARAMETERS

1857

REFERENCES
CHERNOZHUKOV, V., H. HONG, AND E. TAMER (2003): â€œParameter Set Inference in a Class of
Econometric Models,â€ Unpublished Manuscript, Department of Economics, Princeton University.
HOROWITZ, J., AND C. MANSKI (2000): â€œNonparametric Analysis of Randomized Experiments
with Missing Covariate and Outcome Data,â€ Journal of the American Statistical Association, 95,
77â€“84.
MANSKI, C. (2003): Partial Identification of Probability Distributions. New York: Springer-Verlag.
MANSKI, C., G. SANDEFUR, S. MCLANAHAN, AND D. POWERS (1992): â€œAlternative Estimates of
the Effect of Family Structure During Adolescence on High School Graduation,â€ Journal of the
American Statistical Association, 87, 25â€“37.
SHORACK, G. (2000): Probability for Statisticians. New York: Springer-Verlag.

