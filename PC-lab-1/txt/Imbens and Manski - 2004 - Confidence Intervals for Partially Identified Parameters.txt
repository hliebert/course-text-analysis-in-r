Econometrica, Vol. 72, No. 6 (November, 2004), 1845–1857

NOTES AND COMMENTS
CONFIDENCE INTERVALS FOR PARTIALLY IDENTIFIED PARAMETERS
BY GUIDO W. IMBENS AND CHARLES F. MANSKI1
Recently a growing body of research has studied inference in settings where parameters of interest are partially identified. In many cases the parameter is real-valued
and the identification region is an interval whose lower and upper bounds may be estimated from sample data. For this case confidence intervals (CIs) have been proposed
that cover the entire identification region with fixed probability. Here, we introduce a
conceptually different type of confidence interval. Rather than cover the entire identification region with fixed probability, we propose CIs that asymptotically cover the
true value of the parameter with this probability. However, the exact coverage probabilities of the simplest version of our new CIs do not converge to their nominal values
uniformly across different values for the width of the identification region. To avoid
the problems associated with this, we modify the proposed CI to ensure that its exact
coverage probabilities do converge uniformly to their nominal values. We motivate this
modified CI through exact results for the Gaussian case.
KEYWORDS: Bounds, identification regions, confidence intervals, uniform convergence.

1. INTRODUCTION
IN THE LAST DECADE a growing body of research has studied inference in settings
where parameters of interest are partially identified (see Manski (2003) for an overview
of this literature). In many cases, where the parameter is real-valued, the identification
region is an interval whose lower and upper bounds may be estimated from sample
data. Confidence intervals (CIs) may be constructed to take account of the sampling
variation in these estimates. Early on, Manski, Sandefur, McLanahan, and Powers
(1992) computed separate confidence intervals for the lower and upper bounds. Subsequently, Horowitz and Manski (2000) developed CIs that asymptotically cover the
entire identification region with fixed probability. Recently Chernozhukov, Hong, and
Tamer (2003) extended this approach to settings with vector-valued parameters defined
through minimization problems.
Here, we introduce a conceptually different type of confidence interval. Rather than
cover the entire identification region with fixed probability α, we propose CIs that asymptotically cover the true value of the parameter with this probability. The key insight
is that when the identification region has positive width, the true parameter can be close
to at most one of the region’s boundaries. Suppose that the true value is close to the
upper bound of the identification region. Then, asymptotically the probability that the
estimate for the lower bound exceeds the true value can be ignored when making inference on the true parameter. This allows the researcher to allocate the entire probability
1
We thank Joerg Stoye for excellent research assistance, Michael Jansson, Francesca Molinari,
Tom Rothenberg, a coeditor, and two anonymous referees for comments, and the NSF for support through Grants SES 0136789 (Imbens) and SES 0314312 (Manski). Imbens also acknowledges financial support from the Giannini Foundation and the Agricultural Experimental Station
at UC Berkeley.

1845

1846

G. IMBENS AND C. MANSKI

of making an error, 1 − α, to values above the upper-bound point estimate. We do not
know whether the true parameter is close to the upper or lower bound, so one-sided
intervals with confidence level α are constructed around both bounds.
To illustrate the nature of our CIs for partially identified parameters, we construct
CIs for the mean of a bounded random variable when some data are missing and the
distribution of missing data is unrestricted. We show that our CIs for the parameter are
proper subsets of the corresponding CIs for the identification region, with the difference in width related to the difference in critical values for one- and two-sided tests.
However, the exact coverage probabilities of the simplest version of our new CIs do
not converge to their nominal values uniformly across different values for the width of
the identification region. A consequence is that confidence intervals can be wider when
the parameter is point-identified than when it is set-identified. To avoid this anomaly,
we modify the proposed CI to ensure that its exact coverage probabilities do converge
uniformly to their nominal values. We motivate this modified CI through exact results
for the Gaussian case.
2. CONFIDENCE INTERVALS FOR PARAMETERS AND IDENTIFICATION REGIONS
Many problems of partial identification have the following abstract structure. Let
(Ω A P) be a specified probability space, and let P be a space of probability distributions on (Ω A). The distribution P is not known, but a random sample of size N
is available, with empirical distribution PN . Let λ be a quantity that is known only to
belong to a specified set Λ. Let f (· ·) : P × Λ → R be a specified real-valued function.
The object of interest is the real parameter θ = f (P λ). Then the identification region
for f (P λ) is the set {f (P λ ) λ ∈ Λ}. Suppose that λl (P) = argminλ ∈Λ f (P λ ) and
λu (P) = argmaxλ ∈Λ f (P λ ) exist for all P ∈ P . We focus on the class of problems in
which the identification region is the closed interval [f (P λl (P)) f (P λu(P))]. Manski
(2003) describes various problems in this class.
It is natural to estimate the identification region [f (P λl (P)) f (P λu(P))] by
its sample analog [f (PN  λl (PN )) f (PN  λu (PN ))], which is consistent under standard regularity conditions. It is also natural to construct confidence intervals for
[f (P λl (P)) f (P λu(P))] of the form [f (PN  λl (PN )) − CN0  f (PN  λu (PN )) + CN1 ],
where (CN0  CN1 ) are specified nonnegative numbers that may depend on the sample data. Horowitz and Manski (2000) proposed CIs of this form and showed how
(CN0  CN1 ) may be chosen to achieve a specified asymptotic probability of coverage
of the identification region. Chernozhukov, Hong, and Tamer (2003) study confidence
sets with the same property in more general settings. In this paper, we study the use
of these intervals as CIs for the partially identified parameter f (P λ). Our most basic
finding is Lemma 1:
LEMMA 1: Let CN0 ≥ 0, CN1 ≥ 0, λ ∈ Λ, and P ∈ P . The probability that the
interval [f (PN  λl (PN )) − CN0  f (PN  λu (PN )) + CN1 ] covers the parameter f (P λ)
is at least as large as the probability that it covers the entire identification region
[f (P λl (P)) f (P λu(P))].
All proofs are given in the Appendix.
An implication of the lemma is that researchers face a substantive choice whether
to report intervals that cover the entire identification region or intervals that cover the

PARTIALLY IDENTIFIED PARAMETERS

1847

true parameter value with some fixed probability. Although both intervals generally
converge to the identification region as N → ∞ their difference typically is of the
order Op (N −1/2 ) Which CI is of interest depends on the application.
3. MEANS WITH MISSING DATA AND KNOWN PROPENSITY SCORE
In this section we construct CIs for the mean of a bounded random variable when
some data are missing and the distribution of missing data is unrestricted. Let (Y W )
be a pair of random variables, where Y takes values in the bounded set Y and W is
binary with values {0 1}; without loss of generality, let the smallest and largest elements
of Y be 0 and 1, respectively. The researcher has a random sample of (Wi  Yi · Wi ),
i = 1     N, so Wi is always observed and Yi is only observed if Wi = 1. Define µ =
E[Y |W = 1], λ = E[Y |W = 0], σ 2 = V(Y |W = 1), and p = E[W ], with 0 < p ≤ 1. We
assume initially that p is known. This will be relaxed in Section 4. Let F(y) be the
conditional distribution function of Y given W = 1, an element of the set of distribution
functions F with variance σ 2 ≤ σ 2 ≤ σ 2 , for some positive and finite σ 2 and σ 2 . The
distribution of Y given W = 0 is unknown; hence, λ ∈ [0 1]. The parameter of interest
is θ = E[Y ] = µ · p + λ · (1 − p). The identification region for θ is the closed interval
[θl  θu ] = [µ · p µ · p + 1 − p].
With p known, the only unknown determinant of the interval boundaries is the
conditional mean µ. This parameter can be estimated by its sample analog µ̂ =
N
N
i=1 Wi · Yi /N1 (where N1 =
i=1 Wi ), and the identification region can be consistently
estimated by [θ̂l  θ̂u ] = [µ̂ · p µ̂ · p + 1 − p] The first step towards constructing
CIs
√
is to consider inference for µ. Using standard large sample results, we have N(µ̂ −
N
d
µ) → N (0 σ 2 /p) A consistent estimator for σ 2 is σ̂ 2 = i=1 Wi · (Yi − µ̂)2 /(N1 − 1).
Hence, the standard 100 · α% confidence interval for µ is


σ̂
σ̂
µ
(1)
 µ̂ + z(α+1)/2 · 

CI α = µ̂ − z(α+1)/2 · 
p·N
p·N
√
 zτ
where zτ is the τ quantile of the standard normal distribution: Φ(zτ ) = −∞
(1/ 2π ) ×
2
e−y /2 dy = τ. In the point identified case with p = 1 we have θ = θl = θu = µ, and thus
in that case CI µα is also the appropriate CI for θ and [θl  θu ].
Now consider symmetric CIs for the identification region [θl  θu ] and the parameter θ. The CI for [θl  θu ] substitutes the lower (upper) confidence bound for µ into the
lower (upper) bound for the identification region:





σ̂
σ̂
CI α[θl θu ] = µ̂ − z(α+1)/2 · 
(2)
· p µ̂ + z(α+1)/2 · 
·p+1−p 
p·N
p·N
Note that as p → 1, CIα[θl θu ] → CIαµ . The CI for θ adjusts the critical values to obtain
the appropriate coverage for θ:





σ̂
σ̂
θ


CI α = µ̂ − zα ·
(3)
· p µ̂ + zα ·
·p+1−p 
p·N
p·N

Note that this is a proper interval only if 2zα σ̂/ pN > −(1 − p)/p, which is always
true if α ≥ 5 and will be true with probability arbitrarily close to one for N large if

1848

G. IMBENS AND C. MANSKI

α < 5. One can modify the interval if this condition is not satisfied without affecting
the asymptotic properties. The following lemma describes the large sample properties
of these intervals.
LEMMA 2: For any p0 > 0,
(i) infF ∈F p0 ≤p≤1 limN→∞ Pr([θl  θu ] ⊂ CIα[θl θu ] ) = α;
(ii) infF ∈F λ∈Λp0 ≤p<1 limN→∞ Pr(θ ∈ CIαθ ) = α.
Although the confidence interval CIαθ has in large samples the appropriate confidence level for all values of p, λ, and F , it has an unattractive feature. The issue is that
for any N one can find a value of p and λ such that the coverage is arbitrarily close to
100 · (2α − 1)%, rather than the nominal 100 · α%. To see this, we consider
 an example
with Y |W = 1 normal with mean µ and known variance σ 2 . Let p̂ = i Wi /N. The
exact coverage probability of CI θα for θ, conditional on p̂, at λ = 0 (so θ = µ · p) is
 




p̂
p̂
(1 − p)
θ
Pr(θ ∈ CI α ) = Φ zα ·
− Φ −zα ·
− N p̂ ·

p
p
σp
For any fixed p ∈ (0 1), this coverage probability approaches α with probability one as
N → ∞. However, for any fixed N < ∞, the coverage probability approaches 2α − 1
with probability one as p → 1. This example shows that the asymptotic coverage result in Lemma 2 is very delicate.√One can also see this by considering the width of
√
to 2zα · σ̂ p/ N + 1 − p. As p → 1, for fixed N, this width coninterval CI θα equal√
verges to 2zα · σ̂/ N. This is strictly less than the width of CI µα , which is the standard interval for θ for the point-identified case with p = 1. It is counterintuitive that
the CI for θ should be shorter when the parameter is partially identified than when it
is point-identified. The anomaly arises because the coverage of CI θα does not converge
uniformly in (F λ p) and, in particular, not uniformly in p.
We propose here a modification of CI θα whose coverage probability does converge
uniformly in (F λ p) To motivate the modification, again consider the case where
Y |W = 1 ∼ N (0 σ 2 ) with known σ 2 . The conditional coverage rate for symmetric intervals of the form [θ̂l − D θ̂u + D] is
Pr(θ̂l − D ≤ θ ≤ θ̂u + D|p̂)




D + (1 − λ) · (1 − p)
D + λ · (1 − p)
=Φ
N p̂ ·
− Φ − N p̂ ·

σp
σp
To get the coverage rate to be at least α for all values of λ, one needs to choose D to
solve:




D + (1 − p)
D
Φ N p̂ ·
− Φ − N p̂ ·
= α
σp
σp

To facilitate comparison with the previous CI, let CN = D N p̂/(pσ) so that CN solves


1−p
Φ CN + N p̂ ·
− Φ(−CN ) = α
σp

1849

PARTIALLY IDENTIFIED PARAMETERS

with the corresponding confidence interval


pσ
pσ
θ
 µ̂ · p + (1 − p) + CN 
CI α = µ̂ · p − CN 

N p̂
N p̂
For any fixed 0 < p < 1, limN→∞ CN = zα , which would give us the interval CI θα back.
For fixed N, as p → 1, the interval estimate now converges to CI µα with no discontinuity
at p = 1. For 0 < p < 1, the confidence interval is strictly wider than the interval for
p = 1.
For the general case with unknown distribution for Y |W = 1, we construct a CI by
replacing σ by σ̂ and p̂ by p:
(4)




θ
CI α = µ̂ − CN · σ̂/ p · N · p µ̂ + CN · σ̂/ p · N · p + 1 − p 

where CN satisfies


√
1−p
Φ CN + N · √
(5)
− Φ(−CN ) = α
σ̂ p
Lemma 3 shows that the new interval has a coverage rate that converges uniformly in
(F λ p):
LEMMA 3: For any p0 > 0,
lim

inf

N→∞ F ∈F λ∈Λp0 ≤p≤1

Pr(θ ∈ CI α ) = α
θ

[θl θu ]
It is interesting to compare
, CI θα , and CI α in terms of the
 the three intervals CI α
constants that multiply σ̂/ p · N, the standard error of µ̂. The form of the intervals
is the same for all three cases and the width of the intervals is strictly increasing in
this constant, so we can compare the widths by directly comparing these constants. For
l θu ]
CI [θ
, the constant is z(α+1)/2 , which solves Φ(C) − Φ(−C) = α For CI θα , the constant
α
θ
is zα , which solves Φ(∞) − Φ(−C) = α and which is strictly smaller. For CI α , the
constant is CN which solves


√
1−p
Φ C+ N· √
− Φ(−C) = α
σ̂ p
θ

Unless p = 1, this is strictly between the first two constants so CI θα ⊂ CI α ⊂ CI α[θl θu ] .
θ
If the parameter is point identified (p = 1), then CN = z(α+1)/2 and CI θα ⊂ CI α =
l θu ]
= CI µα .
CI [θ
α
4. THE GENERAL CASE
Here we develop a confidence interval that converges uniformly in more general
settings, including ones in which the width of the identification region is a nuisance

1850

G. IMBENS AND C. MANSKI

parameter that must be estimated. We use the same structure and notation as in Section 2. Define θl = f (P λl (P)), θu = f (P λu (P)), and ∆ = θu − θl , and let θ̂l , θ̂u , and
∆ˆ = θ̂u − θ̂l be estimators for θl , θu , and ∆. Then the identification region [θl  θu ] is
naturally estimated by its sample analog [θ̂l  θ̂u ].
We consider the following set of assumptions:
ASSUMPTION 1: (i) There are estimators for the lower and upper bound θ̂l and θ̂u that
satisfy:
√
N



θ̂l − θl
θ̂u − θu



  
0
σl2
−→ N

0
ρσl σu
d

ρσl σu
σu2




uniformly in P ∈ P , and there are estimators σ̂l2 , σ̂u2 , and ρ̂ for σl2 , σu2 , and ρ that converge
to their population values uniformly in P ∈ P . (ρ may be equal to one in absolute value, as
in the case where the width of the identification region is known.)
(ii) For all P ∈ P , σ 2 ≤ σl2  σu2 ≤ σ 2 for some positive and finite σ 2 and σ 2 , and
θu − θl ≤ ∆ < ∞.
√
(iii) For all  > 0, there are ν > 0, K, and N0 such that N ≥ N0 implies Pr( N|∆ˆ −
∆| > K∆ν ) <  uniformly in P ∈ P .
Given Assumption 1 we construct the confidence interval as:
(6)

√
√ 
θ
CI α = θ̂l − C N · σ̂l / N θ̂u + C N · σ̂u / N 

where C N satisfies
(7)


√
Φ CN + N ·


∆ˆ
− Φ(−C N ) = α
max(σ̂l  σ̂u )

The following lemma gives the general uniform coverage result.
LEMMA 4: Suppose Assumption 1 holds. Then
lim

inf

N→∞ P∈Pλ∈Λ

θ

Pr(θ ∈ CI α ) ≥ α

Next we return to the missing data problem of Section 3. We allow for an unknown p
(assuming p is bounded away from zero) and show that this problem fits the assumption
sufficient for the application of Lemma 4. Because the conditional variance of Y given
W = 1 is bounded and bounded away from zero, Assumption 1(ii) is satisfied. The
N
lower bound can be estimated by θ̂l = (1/N) i=1 Wi · Yi  The upper bound can be
N
estimated by θ̂u = (1/N) i=1 (Wi · Yi + 1 − Wi ). Both estimators are asymptotically

PARTIALLY IDENTIFIED PARAMETERS

1851

√
√
d
d
normal, with N(θ̂l − θl ) → N (0 σl2 ) and N(θ̂u − θu ) → N (0 σu2 ) where σl2 = σ 2 ·
2
2
2
2
p + µ · p · (1 − p) and σu = σ · p + µ · p · (1 − p) + p · (1 − p) − 2 · µ · p · (1 − p). Since
the convergence is also uniform in P, Assumption 1(i) is satisfied. Finally, consider
Assumption 1(iii). Let ν = 1/2, and N0 = 1. In the missing data case ∆ˆ = 1 − p̂. The
variance of ∆ˆ is ∆(1 −√∆)/N. Hence, E[N · (∆ˆ − ∆)2 ] ≤ ∆. Now apply Chebyshev’s
inequality, with K = 1/ , so that
√
Pr N|∆ˆ − ∆| > K · ∆ν = Pr N(∆ˆ − ∆)2 > K 2 · ∆2ν
< E[N · (∆ˆ − ∆)2 ]/(K 2 ∆2ν )
≤ ∆/(K 2 ∆2ν ) = 1/K 2 = 
Hence Assumption 1 is satisfied, and Lemma 4 can be used to construct a CI which is
θ
equivalent to that obtained by substituting p̂ for p in CI α given in (4).
Dept. of Economics, and Dept. of Agricultural and Resource Economics, 661 Evans
Hall, University of California at Berkeley, Berkeley, CA 94720-3880, U.S.A., and NBER;
imbens@econ.berkeley.edu; http://elsa.berkeley.edu/users/imbens/
and
Dept. of Economics and Institute for Policy Research, Northwestern University,
2001 Sheridan Rd., Evanston, IL 60208-2600, U.S.A.; cfmanski@northwestern.edu;
http://www.faculty.econ.northwestern.edu/faculty/manski/.
Manuscript received May, 2003; final revision received March, 2004.
APPENDIX
PROOF OF LEMMA 1: Define the following two events:


A1 = f (P λ) ∈ f (PN  λl (PN )) − CN0  f (PN  λu (PN )) + CN1 


A2 = f (P λl (P)) f (P λu (P)) ⊂ f (PN  λl (PN )) − CN0  f (PN  λu (PN )) + CN1 

Because f (P λ) ∈ [f (P λl (P)) f (P λu (P))], it follows that A2 implies A1 and that the coverage probability for the set (equal to the probability of the event A2 ) is less than the coverage
Q.E.D.
probability for the parameter (equal to the probability of the set A1 ).
PROOF OF LEMMA 2: For the first part, fix F and p. Then
Pr [θl  θu ] ⊂ CI α[θl θu ]



σ̂
· p and
= Pr θl ≥ µ̂ − z(α+1)/2 · 
p·N



σ̂
θu ≤ µ̂ + z(α+1)/2 · 
·p+1−p
p·N



σ̂
= 1 − Pr θl < µ̂ − z(α+1)/2 · 
· p or
p·N



σ̂
θu > µ̂ + z(α+1)/2 · 
·p+1−p
p·N

1852

G. IMBENS AND C. MANSKI
 


σ̂
·p
= 1 − Pr µ · p < µ̂ − z(α+1)/2 · 
p·N




σ̂
− Pr µ · p + 1 − p > µ̂ + z(α+1)/2 · 
·p+1−p
p·N




σ̂
σ̂
− Pr µ > µ̂ + z(α+1)/2 · 

= 1 − Pr µ < µ̂ − z(α+1)/2 · 
p·N
p·N

which converges to 1 − (1 − α)/2 − (1 − α)/2 = α as N gets large. For the second part consider
the three possibilities for λ: λ = 0, λ = 1, and 0 < λ < 1. If λ = 0,
we have θ = µ · p. Hence, the
coverage probability of CIαθ is, for N large enough so that 2zα σ̂/ pN > −(1 − p)/p,





σ̂
σ̂
Pr(θ ∈ CI θα ) = Pr µ̂ − zα · 
· p ≤ µ · p ≤ µ̂ + zα · 
·p+1−p
p·N
p·N



σ̂
= 1 − Pr µ̂ − zα · 
·p>µ·p
p·N




σ̂
− Pr µ · p > µ̂ + zα · 
·p+1−p 
p·N
The second term converges to 1 − α. The third term converges to zero, which implies the coverage rate is α. A similar argument applies when λ = 1. When λ ∈ (0 1) the coverage probability
converges to one.
Q.E.D.
Before presenting a proof of Lemma 3 we present a number of preliminary results.
LEMMA 5 (Uniform Central Limit Theorem, Berry–Esseen): Suppose X1  X2     are indeN
pendent and identically distributed random variables with c.d.f. F ∈ F . Let X̄N = i=1 Xi /N,
2
2
2
2
2
µ(F) = EF [X], σ (F) = EF [(X − µ) ], and let 0 < σ ≤ σ (F) ≤ σ̄ < ∞, and EF [|X 3 |] < ∞
for all F ∈ F . Then
 


√  X̄N − µ 


Pr
 −→ 0
sup
N
<
a
−
Φ(a)


σ
−∞<a<∞F∈F
See, e.g., Shorack (2000). Next, we show that we can use this to construct confidence intervals
for sample means with asymptotically uniform convergence even with estimated variances.
LEMMA 6: Under the same conditions as in Lemma 5,


σ̂
σ̂
inf Pr X̄N − z(α+1)/2 · √ ≤ µ ≤ X̄N + z(α+1)/2 · √
−→ α
F∈F
N
N
PROOF OF LEMMA 6: First,


σ̂
σ̂
inf Pr X̄N − z(α+1)/2 · √ ≤ µ ≤ X̄N + z(α+1)/2 · √
F∈F
N
N


√
X̄N − µ
≤ z(α+1)/2 
= inf Pr −z(α+1)/2 ≤ N ·
F∈F
σ̂
Hence it will suffice to show that
 




√


X̄N − µ
Pr
 −→ 0
N
<
a
−
Φ(a)
sup


σ̂
−∞<a<∞F∈F

PARTIALLY IDENTIFIED PARAMETERS

1853

By the triangle inequality:
 




√


X̄N − µ
Pr

N
<
a
−
Φ(a)


σ̂

 


  

√  X̄N − µ 


σ̂
σ̂  
σ̂

N
<a
−Φ a
+
Φ
a
−
Φ(a)
≤ Pr



σ
σ
σ
σ
By Lemma 5 the first term converges to zero, and by uniform convergence of σ̂ to σ the second
one converges to zero.
Q.E.D.
PROOF OF LEMMA 3: First we prove that the asymptotic coverage probability is greater than
or equal to α. For fixed λ the coverage probability is




Pr µ̂ − CN · σ̂/ p · N · p ≤ µ · p + λ · (1 − p) ≤ µ̂ + CN · σ̂/ p · N · p + 1 − p

λ · (1 − p)
σ̂ √
= Pr −CN − N ·
√
σ
σ· p
≤

√

N·


σ̂ √
µ − µ̂
(1 − λ) · (1 − p)

√ ≤ CN + N ·
√
σ/ p
σ
σ· p

For any  > 0, there almost surely exists an N0 such that for N > N0 , |(σ̂ − σ)/σ| < , so that
 > 1 − σ̂/σ. Therefore for N ≥ N0 ,


λ · (1 − p) √
µ − µ̂
(1 − λ) · (1 − p)
σ̂ √
σ̂ √
N
·
N
·
Pr −CN − N ·
≤
≤
C
+
√
√
√
N
σ
σ· p
σ/ p
σ
σ· p

√
λ · (1 − p)
≥ Pr −CN (1 − ) − N ·
√
σ· p

√
√
µ − µ̂
(1 − λ) · (1 − p)
≤ N · √ ≤ CN (1 − ) + N ·

√
σ/ p
σ· p
For N large enough this can be made arbitrarily close to




√
√
(1 − λ) · (1 − p)
λ · (1 − p)
(1
−
)
−
N
·
Φ CN (1 − ) + N ·
−
Φ
−C
√
√
N
σ· p
σ· p




√
√
(1 − λ) · (1 − p)
λ · (1 − p)
= Φ CN + N ·
− Φ −CN − N ·
+ 2CN φ(ω)
√
√
σ· p
σ· p
for some ω. Because CN ≤ z(α+1)/2 (see definition of CN ), and since φ(·) is bounded, the last term
can be made arbitrarily small by choosing  small. The sum of the first two terms has a negative
second derivative with respect to λ, and so it is minimized at λ = 0 or λ = 1. By the definition of
CN it follows that at those values for λ the value of the sum is α. Hence, for any ν > 0, for N large
enough, we have




Pr µ̂ − CN · σ̂/ p · N · p ≤ µ · p + λ · (1 − p) ≤ µ̂ + CN · σ̂/ p · N · p + 1 − p
≥ α − ν
To prove equality, note that at p = 1 the CI is identical to CIαµ , so in that case the asymptotic
coverage rate is equal to α.
Q.E.D.

1854

G. IMBENS AND C. MANSKI

Before proving Lemma 4 we establish a couple of preliminary results. Define C̆N and C̈N by


√
∆
− Φ(−C̆N ) = α and
Φ C̆N + N ·
max(σl  σu )


√
∆ˆ
Φ C̈N + N ·
− Φ(−C̈N ) = α
max(σl  σu )
ˆ while C̆N is a sequence of constants.
Note that C N and C̈N are stochastic (as they depend on ∆),
Next we give two results without proof that show that one can ignore estimation error in
σl and σu .
LEMMA 7: Suppose Assumption 1 holds. Then, uniformly in P ∈ P ,
|C N − C̈N | −→ 0
LEMMA 8: For all  > 0, there is an N0 such that for N ≥ N0 , uniformly in P ∈ P and λ ∈ Λ,
√
√

Pr θ̂l − C N · σ̂l / N ≤ θ ≤ θ̂u + C N · σ̂u / N
√
√ 
− Pr θ̂l − C̈N · σl / N ≤ θ ≤ θ̂u + C̈N · σu / N  < 
The next two lemmas account for the effects of estimation error in ∆.
LEMMA 9: For any η  > 0, there is an N0 such that for N ≥ N0 , uniformly in P ∈ P ,
 


√
∆
Pr Φ C̈N + N ·
− Φ(−C̈N ) < α − η < 
max(σl  σu )
√
ˆ max(σl  σu )) − Φ(−C̈N ) = α, we
PROOF OF LEMMA 9: Because C̈N satisfies Φ(C̈N + N ∆/
only need to prove that
 




√
√
∆ˆ
∆
Pr Φ C̈N + N ·
− Φ C̈N + N ·
> η < 
max(σl  σu )
max(σl  σu )
By Assumption 1(iii) there are ν, K, and N0 such that with δ = ν/5 and N ≥ max(N0  K 1/δ ),
√
√
Pr N|∆ˆ − ∆| > N δ ∆ν ≤ Pr N|∆ˆ − ∆| > K∆ν < 
Then:

(8)


√
Φ C̈N + N ·




√
∆ˆ
∆
− Φ C̈N + N ·
max(σl  σu )
max(σl  σu )
 



ˆ
√
√
∆
∆
= 1{∆ˆ ≤ ∆} · Φ C̈N + N ·
− Φ C̈N + N ·
max(σl  σu )
max(σl  σu )
√


δ
ν
+ 1 ∆ˆ > ∆ N|∆ˆ − ∆| ≤ N ∆
 



√
√
∆ˆ
∆
× Φ C̈N + N ·
− Φ C̈N + N ·
max(σl  σu )
max(σl  σu )
√


+ 1 ∆ˆ > ∆ N|∆ˆ − ∆| > N δ ∆ν
 



√
√
∆ˆ
∆
× Φ C̈N + N ·
− Φ C̈N + N ·
max(σl  σu )
max(σl  σu )

1855

PARTIALLY IDENTIFIED PARAMETERS
(9)

(10)

√


≤ 1 ∆ˆ > ∆ N|∆ˆ − ∆| ≤ N δ ∆ν



 
√
√
∆ˆ
∆
− Φ C̈N + N ·
× Φ C̈N + N ·
max(σl  σu )
max(σl  σu )
√

+ 1 N|∆ˆ − ∆| > N δ ∆ν 

using the fact that (8) is nonpositive. The expectation of (10) is less than . By a mean value
theorem (9) is, for some γ ∈ [0 1], equal to
√


1 ∆ˆ > ∆ N|∆ˆ − ∆| ≤ N δ ∆ν

 √
√
√
∆ˆ − ∆
∆ˆ − ∆
∆
×φ C̈N + N ·
+γ· N ·
· N·

max(σl  σu )
max(σl  σu )
max(σl  σu )
Because the product is zero unless ∆ˆ > ∆, and C̈N  ∆ ≥ 0, this can be bounded from above by


√
√
√


∆
∆ˆ − ∆
1 ∆ˆ > ∆ N|∆ˆ − ∆| ≤ N δ ∆ν · φ
N·
(11)
· N·
max(σl  σu )
max(σl  σu )


√
√


∆
N δ ∆ν
N·
·
≤ 1 ∆ˆ > ∆ N|∆ˆ − ∆| ≤ N δ ∆ν · φ
max(σl  σu )
max(σl  σu )


2δ
ν
√
∆
N ∆
≤ N −δ · φ
N·
·

max(σl  σu )
max(σl  σu )
Maximizing this over ∆ gives

√
N δ−ν exp(−ν/2)νν/2 max(σl  σu )ν−1 / 2π

Given that δ < ν, this can be bounded arbitrarily close to zero uniformly in P ∈ P .

Q.E.D.

LEMMA 10: For any η  > 0, there is an N0 such that for N ≥ N0 , uniformly in P ∈ P ,
Pr(C̈N < C̆N − η) < 
PROOF: Let φ = φ(z(α+1)/2 ). Note that C̈˙N and C̆N are positive and less than z(α+1)/2 , and thus
φ(C̈N ) ≥ φ and φ(C̆N ) ≥ φ. Using Lemma 9 there is an N0 such that for N ≥ N0
 


√
∆
Pr Φ C̈N + N ·
− Φ(−C̈N ) < α − η · φ < 
max(σl  σu )
uniformly in P ∈ P . Since

√
Φ C̈N + N ·

∆
max(σl  σu )


√
⇒ Φ C̈N + N ·


− Φ(−C̈N ) > α − η · φ

∆
max(σl  σu )




√
− Φ C̆N + N ·

∆
max(σl  σu )

− Φ(−C̈N ) + Φ(−C̆N ) > −η · φ
 

√
∆
⇒ φ C̈N + N ·
+ γ · (C̆N − C̈N )
max(σl  σu )

+ φ(C̆N + γ · (C̈N − C̆N )) · (C̈N − C̆N ) > −η · φ



1856

G. IMBENS AND C. MANSKI
⇒ φ · (C̈N − C̆N ) > −η · φ
⇒ C̈N − C̆N > −η

for some γ ∈ [0 1] by the mean value theorem, and thus with probability C̈N − C̆N > −η with
probability at least 1 − .
Q.E.D.
Note that Lemma 10 does not imply that |C N − C̆N | converges to zero uniformly. This is not
necessarily true unless we are willing to rule out values of ∆ close to zero, which is exactly the
point-identified area with which we are concerned.
PROOF OF LEMMA 4: We will prove that for any positive , for N sufficiently large,
√
√
Pr θ̂l − C N · σ̂l / N ≤ θ ≤ θ̂u + C N · σ̂u / N ≥ α − 
uniformly in P ∈ P . We will prove this for θ = θu . The proof for θ = θl is analogous, and by
joint normality of the estimators for the upper and lower boundary the coverage probability is
minimized at the boundary of the identification region.
For arbitrary positive √
1 , 2 , and 3 , choose N large enough so that the following conditions
√
are satisfied (i), supz | Pr( N(θ̂l − θl )/σl ≤ z) − Φ(z)| ≤ 1 , (ii), supz | Pr( N(θ̂u − θu )/σu ≤
z) − Φ(z)| ≤ 1 , and (iii), Pr(C̈N − C̆N < −2 ) < 3 . Existence of such an N follows for conditions
(i) and (ii) from Assumption 4.2, and for condition (iii) from Lemma 10. Define the following
events:
√
√
E1 ≡ θ̂l − C N · σ̂l / N ≤ θu ≤ θ̂u + C N · σ̂u / N
√
√
E2 ≡ θ̂l − C̈N · σl / N ≤ θu ≤ θ̂u + C̈N · σu / N
√
√
E3 ≡ θ̂l − (C̆N − 2 ) · σl / N ≤ θu ≤ θ̂u + (C̆N − 2 ) · σu / N
√
√
E4 ≡ θ̂l − C̆N · σl / N ≤ θu ≤ θ̂u + C̆N · σu / N
E5 ≡ C̈N − C̆N > −2 

and E5c ≡ C̈N − C̆N ≤ −2 

Note that (E5 ∩ E3 ) ⇒ E2 and thus (E5 ∩ E3 ) ⇒ (E2 ∩ E3 ). Define also
√
P3 ≡ Φ C̆N − 2 + N · ∆/σl − Φ(−C̆N + 2 )
and
P4 ≡ Φ C̆N +

√

N · ∆/σl − Φ(−C̆N ) = α

By conditions (i) and (ii), |P3 − Pr(E3 )| ≤ 21 and |P4 − Pr(E4 )| ≤ 21 . Also, |P3 − P4 | ≤ 22 φ̄, and
by (iii), Pr(E6c ) < 3 . By Lemma 8 it follows that for any 4 > 0 we can choose N large enough so
that |Pr(E1 ) − Pr(E2 )| < 4 . Then, by elementary set theory
Pr(E1 ) ≥ Pr(E2 ) − 4 ≥ Pr(E2 ∩ E3 ) − 4 ≥ Pr(E5 ∩ E3 ) − 4 ≥ Pr(E3 ) − Pr(E5c ) − 4
≥ P3 − 21 − 3 − 4 ≥ P4 − 21 − 3 − 22 φ̄ − 4 = α − 21 − 3 − 22 φ̄ − 4 
Since 1      4 were chosen arbitrarily, one can make Pr(E1 ) > α −  for any  > 0.

Q.E.D.

PARTIALLY IDENTIFIED PARAMETERS

1857

REFERENCES
CHERNOZHUKOV, V., H. HONG, AND E. TAMER (2003): “Parameter Set Inference in a Class of
Econometric Models,” Unpublished Manuscript, Department of Economics, Princeton University.
HOROWITZ, J., AND C. MANSKI (2000): “Nonparametric Analysis of Randomized Experiments
with Missing Covariate and Outcome Data,” Journal of the American Statistical Association, 95,
77–84.
MANSKI, C. (2003): Partial Identification of Probability Distributions. New York: Springer-Verlag.
MANSKI, C., G. SANDEFUR, S. MCLANAHAN, AND D. POWERS (1992): “Alternative Estimates of
the Effect of Family Structure During Adolescence on High School Graduation,” Journal of the
American Statistical Association, 87, 25–37.
SHORACK, G. (2000): Probability for Statisticians. New York: Springer-Verlag.

