Instructor’s Solutions Manual
for

Econometric Analysis of Cross Section and Panel Data,
second edition
by Jeffrey M. Wooldridge
2011
The MIT Press

© 2011 Massachusetts Institute of Technology

All rights reserved. No part of this book may be reproduced in any form by any electronic or
mechanical means (including photocopying, recording, or information storage and retrieval)
without permission in writing from the publisher.

Contents
Preface............................................................................................................2
Solutions to Chapter 2 Problems....................................................................4
Solutions to Chapter 3 Problems....................................................................11
Solutions to Chapter 4 Problems....................................................................15
Solutions to Chapter 5 Problems....................................................................38
Solutions to Chapter 6 Problems....................................................................57
Solutions to Chapter 7 Problems....................................................................80
Solutions to Chapter 8 Problems....................................................................104
Solutions to Chapter 9 Problems....................................................................126
Solutions to Chapter 10 Problems..................................................................151
Solutions to Chapter 11 Problems..................................................................207
Solutions to Chapter 12 Problems..................................................................242
Solutions to Chapter 13 Problems..................................................................270
Solutions to Chapter 14 Problems..................................................................295
Solutions to Chapter 15 Problems..................................................................304
Solutions to Chapter 16 Problems..................................................................341
Solutions to Chapter 17 Problems..................................................................358
Solutions to Chapter 18 Problems..................................................................406
Solutions to Chapter 19 Problems..................................................................445
Solutions to Chapter 20 Problems..................................................................465
Solutions to Chapter 21 Problems..................................................................484
Solutions to Chapter 22 Problems..................................................................538

1

Preface
This manual contains the solutions to all of the problems in the second edition of my MIT
Press book, Econometric Analysis of Cross Section and Panel Data. In addition to the
problems printed in the text, I have included some “bonus problems” along with their
solutions. Several of these problems I left out due to space constraints and others occured to
me since the book was published. I have a collection of other problems, with solutions, that I
have used over the past 10 years for problem sets, takehome exams, and in class exams. I am
happy to provide these to instructors who have adopted the book for a course.
I solved the empirical examples using various versions of Stata, ranging from 8.0 through
11.0. I have included the Stata commands and output directly in the text. No doubt there are
Stata users and users of other software packages who will, at least in some cases, see more
efficient or more elegant ways to compute estimates and test statistics.
Some of the solutions are fairly long. In addition to filling in all or most of the algebraic
steps, I have tried to offer commentary about why a particular problem is interesting, why I
solved the problem the way I did, or which conclusions would change if we varied some of the
assumptions. Several of the problems offer what appear to be novel solutions to situations that
can arise in actual empirical work.
My progress in finishing this manual was slowed by a health problem in spring and
summer of 2010. Fortunately, several graduate students came to my aid by either working
through some problems or organizing the overall effort. I would like to thank Do Won Kwak,
Cuicui Lu, Myoung-Jin Keay, Shenwu Sheng, Iraj Rahmani, and Monthien Satimanon for their
able assistance.
I would appreciate learning about any mistakes in the solutions and also receiving

2

suggestions for how to make the answers more transparent. Of course I will gladly entertain
suggestions for how the text can be improved, too. I can be reached via email at
wooldri1@msu.edu.

3

Solutions to Chapter 2 Problems
2.1. a. Simple partial differentiation gives
∂Ey|x 1 , x 2 
 1  4x2
∂x 1
and
∂Ey|x 1 , x 2 
  2  2 3 x 2   4 x 1
∂x 2
b. By definition, Eu|x 1 x 2   0. Because x 22 and x 1 x 2 are functions of x 1 , x 2 , it does not
matter whether or not we also condition on them: Eu|x 1 , x 2, x 22 , x 1 x 2   0.
c. All we can say about Varu|x 1, x 2  is that it is nonnegative for all x 1 and x 2 :
Eu|x 1 , x 2   0 in no way restricts Varu|x 1 , x 2 .
2.2. a. Because ∂Ey|x/∂x   1  2 2 x − µ, the marginal effect of x on Ey|x is a linear
function of x. If  2 is negative then the marginal effect is less than  1 when x is above its mean.
If, for example,  1  0 and  2  0, the marginal effect will eventually be negative for x far
enough above . (Whether the values for x such that ∂Ey|x/∂x  0 represents an interesting
segment of the population is a different matter.)
b. Because ∂Ey|x/∂x is a function of x, we take the expectation of ∂Ey|x/∂x over the
distribution of x: E∂Ey|x/∂x  E 1  2 2 x −    1  2 2 Ex −    1 .
c. One way to do this part is to apply Property LP.5 from Appendix 2A. We have
Ly|1,x  LEy|x   0   1 Lx − |1, x   2 Lx −  2 |1, x
  0   1 x −    2  0   1 x,
because Lx − |1, x  x −  and  0   1 x is the linear projection of x −  2 on x. By
assumption, x −  2 and x are uncorrelated, and so  1  0. It follows that

4

Ly|x   0 −  1    2  0    1 x
2.3. a. y   0   1 x 1   2 x 2   3 x 1 x 2  u, where u has a zero mean given x 1 and x 2 :
Eu|x 1 , x 2   0. We can say nothing further about u.
b. ∂Ey|x 1 , x 2 /∂x 1   1   3 x 2 . Because Ex 2   0,  1  E∂Ey|x 1 , x 2 /∂x 1 , that is,  1 is
the average partial effect of x 1 on Ey|x 1 , x 2 /∂x 1 . Similarly,  2 E∂Ey|x 1 , x 2 /∂x 2 .
c. If x 1 and x 2 are independent with zero mean then Ex 1 x 2   Ex 1 Ex 2   0. Further,
the covariance between x 1 x 2 and x 1 is Ex 1 x 2  x 1   Ex 21 x 2   Ex 21 Ex 2  (by
independence)  0. A similar argument shows that the covariance between x 1 x 2 and x 2 is zero.
But then the linear projection of x 1 x 2 onto 1, x 1 , x 2  is identically zero. Now just use the law
of iterated projections (Property LP.5 in Appendix 2A):
Ly|1, x 1 , x 2   L 0   1 x 1   2 x 2   3 x 1 x 2 |1, x 1 , x 2 
  0   1 x 1   2 x 2   3 Lx 1 x 2 |1, x 1 , x 2 
 0  1x1  2x2.
d. Equation (2.47) is more useful because it allows us to compute the partial effects of x 1
and x 2 at any values of x 1 and x 2 . Under the assumptions we have made, the linear projection
in (2.48) does have as its slope coefficients on x 1 and x 2 the partial effects at the population
average values of x 1 and x 2 – zero in both cases – but it does not allow us to obtain the partial
effects at any other values of x 1 and x 2 . Incidentally, the main conclusions of this problem go
through if we allow x 1 and x 2 to have nonzero population means.
2.4. By assumption,
Eu|x, v   0  x   1 v
for some scalars  0 ,  1 and a column vector . Now, it suffices to show that  0  0 and   0.
One way to do this is to use LP.7 in Appendix 2A, and in particular, equation (2.56). This says
5

that  0 ,  ′  ′ can be obtained by first projecting 1, x onto v, and obtaining the population
residual, r. Then, project u onto r. Now, since v has zero mean and is uncorrelated with x, the
first step projection does nothing: r  1, x. Thus, projecting u onto r is just projecting u onto
1, x. Since u has zero mean and is uncorrelated with x, this projection is identically zero,
which means that  0  0 and   0.
2.5. By definition and the zero conditional mean assumptions, Varu 1 |x, z  Vary|x, z
and Varu 2 |x  Vary|x. By assumption, these are constant and necessarily equal to
 21 ≡ Varu 1  and  22 ≡ Varu 2 , respectively. But then Property CV.4 implies that  22 ≥  21 .
This simple conclusion means that, when error variances are constant, the error variance falls
as more explanatory variables are conditioned on.
2.6. a. By linearity of the linear projection,
Lq|1, x  Lq ∗ |1, x  Le|1, x  Lq ∗ |1, x,
where the last inequality follows because Le|1, x  0 when Ee  0 and Ex ′ e  0.
Therefore, the parameters in the linear projection of q onto 1,x are the same as the linear
projection of q ∗ onto 1,x. This fact is useful for studying equations with measurement error
in the explained or explanatory variables.
b. r  q − Lq|1, x  q ∗  e − Lq|1, x  q ∗  e − Lq ∗ |1, x (from part a)
 q ∗ − Lq ∗ |1, x  e  r ∗  e.
2.7. Write the equation in error form as
y  gx  z u
Eu|x, z  0.
Take the expected value of the first equation conditional only on x:

6

Ey|x  gx  Ez|x
and subtract this from the first equation to get
y − Ey|x  z − Ez|x  u
or
ỹ  z̃   u
Because z̃ is a function of x, z, Eu|z̃   0 [since Eu|x¸z  0], and so Eỹ |z̃   z̃ .
This basic result is fundamental in the literature on estimating partial linear models. First,
one estimates Ey|x and Ez|x using very flexible methods (typically, nonparametric
methods). Then, after obtaining residuals of the form ỹ i ≡ y i − Êy i |x i  and z̃ i ≡ z i − Êz i |x i , 
is estimated from an OLS regression ỹ i on z̃ i , i  1, . . . , N. Under general conditions, this kind
of nonparametric partialling-out procedure leads to a N -consistent, asymptotically normal
estimator of . See Robinson (1988) and Powell (1994).
In the case where Ey|x and the elements of Ez|x are approximated as linear functions of
a common set of functions, say h 1 x, . . . , h Q x, the partialling out is equivalent to
estimating a linear model
y   0   1 h 1 x . . .  Q h Q x  x  error
by OLS.
2.8. a. By exponentiation we can write y  expgx  u  expgx expu. It follows
that
Ey|x  expgxEexpu|x  expgxax
Using the product rule gives

7

∂Ey|x
∂gx
∂ax

expgxax  expgx
∂x j
∂x j
∂x j
∂gx
∂ax

Ey|x  Ey|x
 1
∂x j
∂x j
ax
Therefore,
xj
xj
∂Ey|x
∂gx
∂ax


 xj 

∂x j
∂x j
∂x j
Ey|x
ax
We can establish this relationship more simply by assuming Ey|x  0 for all x and using
equation (2.10).
b. Write z j ≡ logx j  so x j  expz j . Then, using the chain rule,
∂gx
∂gx
∂gx ∂x j
∂gx
∂gx




 expz j  
 xj
∂z
∂z
∂x
∂x
∂x j
∂ logx j 
j
j
j
j
c. From logy  gx  u and Eu|x  0 we have Elogy|x  gx. Therefore, using
(2.11), the elasticity would be simply
∂gx
∂gx

 xj
∂x j
∂ logx j 
which, compared with the definition based on Ey|x, omits the elasticity of ax with respect
to x j .
2.9. This is easily shown by using iterated expectations:
Ex ′ y  EEx ′ y|x  Ex ′ Ey|x  Ex ′ x
Therefore,
  Ex ′ x −1 Ex ′ y  Ex ′ x −1 Ex ′ x
and the latter equation is the vector of parameters in the linear projection of x on x.
2.10. a. As given in the hint, we can always write

8

Ey|x, s  1 − s   0 x  s   1 x
Now condition only on s and use iterated expectations:
Ey|s  EEy|x, s|s  E1 − s   0 x  s   1 x|s
 1 − sE 0 x|s  sE 1 x|s
Therefore,
Ey|s  1  E 1 x|s  1
Ey|s  0  E 0 x|s  0
and so, by adding and subtracting E 0 x|s  1, we get
Ey|s  1 − Ey|s  0  E 1 x|s  1 − E 0 x|s  0
 E 1 x|s  1 − E 0 x|s  1  E 0 x|s  1 − E 0 x|s  0
b. Use part a and linearity of the conditional means:
Ey|s  1 − Ey|s  0  Ex|s  1 1 − Ex|s  1 0   Ex|s  1 0 − Ex|s  0 0 
 Ex|s  1   1 −  0   Ex|s  1 − Ex|s  0   0
This decomposition attributes the difference in the unconditional means,
Ey|s  1 − Ey|s  0, to two pieces. The first part is due to differences in the regression
parameters,  1 −  0 – where we evaluate the difference at the average of the covariates from
the s  1 subpopulation. The second part is due to a difference in means of the covariates from
the two subpopulations – where we apply the regression coefficients from the s  0
subpopulation. If, for example, the two regression functions are the same – that is,  1   0 –
then any difference in the subpopulation means Ey|s  0 and Ey|s  1 is due to a
difference in averages of the covariates across the subpopulations. If the covariate means are
the same – that is, Ex|s  1  Ex|s  0 – then Ey|s  0 and Ey|s  0 can still differ if

9

 1 ≠  0 . In many applications, both pieces in Ey|s  1 − Ey|s  0 are present.
Incidentally, the approach in this problem is not the only interesting way to decompose
Ey|s  1 − Ey|s  0. See, for example, T.E. Elder, J.H. Goddeeris, and S.J. Haider,
“Unexplained Gaps and Oaxaca–Blinder Decompositions,” Labour Economics, 2010.

10

Solutions to Chapter 3 Problems
3.1. To prove Lemma 3.1, we must show that for all   0, there exists b    and an
p

integer N  such that P|x N |≥ b    , all N ≥ N  . We use the following fact: since x N  a, for
any   0 there exists an integer N  such that P|x N − a| 1   for all N ≥ N  . [The existence
of N  is implied by Definition 3.3(1).] But |x N | |x N − a  a|≤ |x N − a||a| (by the triangle
inequality), and so |x N |−|a|≤ |x N − a|. It follows that P|x N |−|a| 1 ≤ P|x N − a| 1. Therefore,
in Definition 3.3(3) we can take b  ≡ |a|1 (irrespective of the value of ) and then the
existence of N  follows from Definition 3.3(1).
′

3.2. Each element of the K  1 vector Z N x N is the sum of J terms of the form Z Nji x Nj .
Because Z Nji  o p 1 and x Nj  O p 1, each term in the sum is o p 1 from Lemma 3.2(4). By
Lemma 3.2(1), the sum of o p 1 terms is o p 1.
p

3.3. This follows immediately from Lemma 3.1 because gx N   gc.
3.4. Both parts follow from the continuous mapping theorem and basic properties of the
normal distribution.
a. The function defined by gz  A ′ z is clearly continuous. Further, if z ~ Normal0, V
then A ′ z ~ Normal0, A ′ VA. By the continuous mapping theorem,
d

A ′ z N → A ′ z ~ Normal0, A ′ VA.
b. Because V is nonsingular, the function gz  z ′ V −1 z is continuous. But if
d

z ~ Normal0, V, z ′ V −1 z ~  2K . So z ′N V −1 z N → z ′ V −1 z ~  2K .
3.5. a. Because Varȳ N    2 /N, Var N ȳ N −   N 2 /N   2 .
a

b. By the CLT, N ȳ N −  ~ Normal0,  2 , and so Avar N ȳ N −    2 .
c. We obtain Avarȳ N  by dividing Avar N ȳ N −  by N. Therefore, Avarȳ N    2 /N.

11

As expected, this coincides with the actual variance of ȳ N .
d. The asymptotic standard deviation of ȳ N is the square root of its asymptotic variance, or
/ N .
e. To obtain the asymptotic standard error of ȳ N , we need a consistent estimator of .
N

Typically, the unbiased estimator of  2 is used: ̂ 2  N − 1 −1 ∑ i1 y i − ȳ N  2 , and then ̂ is
the positive square root. The asymptotic standard error of ȳ N is simply ̂ / N .
3.6. From Definition 3.4, we need to show that for any 0 ≤ c  1/2, N c ̂ N −   o p 1.
But
N c ̂ N −   N c−1/2 N ̂ N −   N c−1/2  O p 1.
Because c  1/2, N c−1/2  o1, and so N c ̂ N −   o1  O p 1  o p 1.
3.7. a. For   0 the natural logarithm is a continuous function, and so
plimlog̂   logplim̂   log  .
b. We use the delta method to find Avar N ̂ − . In the scalar case, if ̂  g̂  then
Avar N ̂ −   dg/d 2 Avar N ̂ − . When g  log  – which is, of course,
continuously differentiable – Avar N ̂ −   1/ 2 Avar N ̂ − .
c. In the scalar case, the asymptotic standard error of ̂ is generally |dg̂ /d|se̂ .
Therefore, for g  log , se̂   se̂ /̂ . When ̂  4 and
se̂   2, ̂  log4 ≈ 1. 39 and se̂   1/2.
d. The asymptotic t statistic for testing H 0 :   1 is ̂ − 1/se̂   3/2  1. 5.
e. Because   log, the null of interest can also be stated as H 0 :   0. The t statistic
based on ̂ is about 1. 39/. 5  2. 78. This leads to a very strong rejection of H 0 , whereas the t
statistic based on ̂ is, at best, marginally significant. The lesson is that, using the Wald test,

12

we can change the outcome of hypotheses tests by using nonlinear transformations.
3.8 a. This follows by Slutsky’s Theorem since the function g 1 ,  2  ≡  1 / 2 is continuous
at all points in  2 where  2 ≠ 0: plim̂ 1 /̂ 2   plim̂ 1 /plim̂ 2    1 / 2 .
b. To find Avar̂  we need to find ∇  g, where g 1 ,  2    1 / 2 . But
′
∇  g  1/ 2 , − 1 / 22 , and so Avar̂   1/ 2 −  1 / 22 Avar̂1/ 2 −  1 / 22  .
′
c. If ̂  −1. 5, . 5 then ∇  g̂  2, 6. Therefore,

Avar̂   2, 6Avar̂2, 6 ′  66. 4. Taking the square root gives se̂  ≈ 8. 15.
3.9. By the delta method,
Avar N ̂ −   GV 1 G ′ , Avar N ̂ −   GV 2 G ′ ,
where G    g is Q  P. Therefore,
Avar N ̂ −  − Avar N ̂ −   GV 2 − V 1 G ′ .
By assumption, V 2 − V 1 is positive semi-definite, and therefore GV 2 − V 1 G ′ is p.s.d.
This complete the proof.
3.10. By assumption,  2  Ew 2i   Varw i   . Because of the i.i.d. assumption,
Varx N   N −1/2  2 N 2   2 .
Now, Chebyshev’s inequality gives that for any b   0,
P|x N |≥ b   ≤

VarX N 
2

b 2
b 2

Therefore, in the definition of O p 1, for any   0 choose b   /  and N   1 and then
P|x N |≥ b   ≤  for all N ≥ N  .
N

3.11. a. Let x N  N −1 ∑ i1 w i −  i  so that

13

N

N

i1

i1

Varx N   N −2 ∑ Varw i   N −2 ∑  2i
By Chebyshev’s inequality, for any   0,
N

N −2 ∑ i1  2i
Varx N 

P|x N |  ≤
2
2
N

It follows that P|x N |  → 0 as N →  if N −2 ∑ i1  2i → 0 as N → .
b. If  2i  b   for all i – that is, the sequence of variances is bounded – then
N

N −2 ∑  2i ≤ b/N → 0 as N → .
i1

Thus, uniformly bounded variances is sufficient for i.n.i.d. sequences to satisfy the WLLN.

14

Solutions to Chapter 4 Problems
4.1. a. Exponentiating equation (4.49) gives
wage  exp 0   1 married   2 educ  z  u
 expu exp 0   1 married   2 educ  z.
Therefore,
Ewage|x  Eexpu|x exp 0   1 married   2 educ  z,
where x denotes all explanatory variables. Now, if u and x are independent
thenEexpu|x Eexpu   0 , say. Therefore
Ewage|x   0 exp 0   1 married   2 educ  z.
If we set married  1 and married  0 in this expecation (keeping all else equal) and find the
proportionate increase we get
 0 exp 0   1   2 educ  z −  0 exp 0   2 educ  z
 exp 1  − 1.
 0 exp 0   2 educ  z
Thus, the percentage difference is 100  exp 1  − 1.
b. Since  1  100  exp 1  − 1  g 1 , we need the derivative of g with respect to  1 :
dg/d 1  100  exp 1 . The asymptotic standard error of ̂ 1 using the delta method is
obtained as the absolute value of dĝ/d 1 times se̂ 1 :
se̂ 1   100  exp̂ 1   se̂ 1 .
c. We can evaluate the conditional expectation in part a at two levels of education, say
educ 0 and educ 1 , all else fixed. The proportionate change in expected wage from educ 0 to
educ 1 is
exp 2 educ 1  − exp 2 educ 0 / exp 2 educ 0   exp 2 educ 1 − educ 0  − 1  exp 2 Δeduc − 1.

15

Using the same arguments in part b, ̂ 2  100  exp 2 Δeduc − 1 and
se̂ 2   100  |Δeduc|exp̂ 2 Δeducse̂ 2 .
d. For the estimated version of equation (4.29), ̂ 1 . 199, se̂ 1  . 039, ̂ 2 . 065, and
se̂ 2  . 006. Therefore, ̂ 1  22. 01 and se̂ 1   4. 76. For ̂ 2 we set Δeduc  4. Then
̂ 2  29. 7 and se̂ 2   3. 11.
4.2. a. For each i we have, by OLS.2, Eu i |X  0. By independence across i and Property
CE.5, Eu i |X  Eu i |x i  because u i , x i  is independent of the explanatory variables for all
other observations. Letting U be the N  1 vector of all errors, this implies EU|X  0. But
̂    X ′ X −1 X ′ U and so
E̂|X    X ′ X −1 X ′ EU|X    X ′ X −1 X ′  0  .
b. From the expression for ̂ in part a we have
Var̂|X  VarX ′ X −1 X ′ U|X  X ′ X −1 X ′ VarU|XXX ′ X −1 .
Now, because EU|X  0, VarU|X  EUU ′ |X. For the diagonal terms,
Eu 2i |X  Eu 2i |x i   Varu i |x i    2 , where the least equality is the homoskedasticity
assumption. For the covariance terms, we must show that Eu i u h |X  0 for all
i ≠ h, i, h  1, . . . , N. Again using Property CE.5, Eu i u h |X  Eu i u h |x i , x h  and
Eu i |x i , u h , x h   Eu i |x i   0. But then Eu i u h |x i , u h , x h   Eu i |x i , u h , x h u h  0. It follows
immediately by iterated expectations that conditioning on the smaller set also yields a zero
conditional mean: Eu i u h |x i , x h   0. This completes the proof.
4.3. a. Not in general. The conditional variance can always be written as
Varu|x  Eu 2 |x − Eu|x 2 ; if Eu|x ≠ 0, then Eu 2 |x ≠ Varu|x.
b. It could be that Ex ′ u  0, in which case OLS is consistent, and Varu|x is constant.

16

But, generally, the usual standard errors would not be valid unless Eu|x  0 because it is
Eu 2 |x that should be constant.
4.4. For each i, û i  y i − x i ̂  u i − x i ̂ − , and so
û 2i  u 2i − 2u i x i ̂ −   x i ̂ −  2 . Therefore, we can write
N

N

N

N

i1

i1

i1

i1

N −1 ∑ û 21 x ′i x i  N −1 ∑ u 21 x ′i x i − 2N −1 ∑u i x i ̂ − x ′i x i  N −1 ∑x i ̂ −  2 x ′i x i .
Dropping the " −2" , the second term can be written as the sum of K terms of the form
N

N

i1

i1

N ∑u i x ij ̂ j −  j x ′i x i  ̂ j −  j N −1 ∑u i x ij x ′i x i  o p 1  O p 1,
−1

N
where we have used ̂ j −  j  o p 1 and N −1 ∑ i1 u i x ij x ′i x i  O p 1 whenever

E|u i x ij x ih x ik |   for all j, h, and k (as would just be assumed). Similarly, the third term can
be written as the sum of K 2 terms of the form
N

̂ j −  j ̂ h −  h N −1 ∑x ij x ih x ′i x i  o p 1  o p 1  O p 1  o p 1,
i1

N

where we have used N −1 ∑ i1 x ij x ih x ′i x i  O p 1 whenever E|x ij x ih x ik x im |   for all j, h,
N

N

k, and m. We have shown that N −1 ∑ i1 û 2i x ′i x i  N −1 ∑ i1 u 2i x ′i x i  o p 1, which is what we
wanted to show.
4.5. Write equation (4.50) as Ey|w  w, where w  x, z. Since Vary|w   2 , it
′
follows by Theorem 4.2 that Avar N ̂ −  is  2 Ew ′ w −1 , where ̂  ̂ , ̂ ′ . Importantly,

because Ex ′ z  0, Ew ′ w is block diagonal, with upper block Ex ′ x and lower block Ez 2 .
Inverting Ew ′ w and focusing on the upper K  K block gives
Avar N ̂ −    2 Ex ′ x −1 .

17

Next, we need to find Avar N ̃ − . It is helpful to write y  x  v where v  z  u
and u ≡ y − Ey|x, z. Because Ex ′ z  0 and Ex ′ u  0, Ex ′ v  0. Further,
Ev 2 |x   2 Ez 2 |x Eu 2 |x 2Ezu|x   2 Ez 2 |x   2 , where we use
Ezu|x, z  zEu|x, z  0 and Eu 2 |x, z  Vary|x, z   2 . Unless Ez 2 |x is constant, the
equation y  x  v generally violates the homoskedasticity assumption OLS.3. So, without
further assumptions,
Avar N ̃ −   Ex ′ x −1 Ev 2 x ′ xEx ′ x −1 .
Now we can show Avar N ̃ −  − Avar N ̂ −  is positive semi-definite by writing
Avar N ̃ −  − Avar N ̂ −   Ex ′ x −1 Ev 2 x ′ xEx ′ x −1 −  2 Ex ′ x −1
 Ex ′ x −1 Ev 2 x ′ xEx ′ x −1 −  2 Ex ′ x −1 Ex ′ xEx ′ x −1
 Ex ′ x −1 Ev 2 x ′ x −  2 Ex ′ xEx ′ x −1
Because Ex ′ x −1 is positive definite, it suffices to show that Ev 2 x ′ x −  2 Ex ′ x is p.s.d.
To this end, let hx ≡ Ez 2 |x. Then by the law of iterated expectations,
Ev 2 x ′ x EEv 2 |xx ′ x   2 Ehxx ′ x   2 Ex ′ x. Therefore,
Ev 2 x ′ x −  2 Ex ′ x   2 Ehxx ′ x, which, when  ≠ 0, is actually a positive definite matrix
except by fluke. In particular, if Ez 2 |x  Ez 2    2  0 (in which case y  x  v satisfies
the homoskedasticity assumption OLS.3), Ev 2 x ′ x −  2 Ex ′ x   2  2 Ex ′ x, which is
positive definite.
4.6. Because nonwhite is determined at birth, we do not have to worry about nonwhite
being determined simultaneously with any kind of response variable. Measurement error is
certainly a possibility, as a binary indicator for being Caucasian is a very crude way to measure
race. Still, many studies hope to isolate systematic differences between those classified as
white versus other races, in which case a binary indicator might be a good proxy. Of course, it

18

is always possible that people are misclassified in survey data. But an important point is that
measurement error in nonwhite would not follow the classical errors-in-variables assumption.
For example, if the issue is simply recording the incorrect entry, then the true indicator,
nonwhite ∗ , is also binary. Then, there are four possible outcomes: nonwhite ∗  1 and
nonwhite  1; nonwhite ∗  0 and nonwhite  1; nonwhite ∗  1 and
nonwhite  0; nonwhite ∗  0 and nonwhite  0. In the first and last cases, no error is made.
Generally, it makes no sense to write nonwhite  nonwhite ∗  e, where e is a mean−zero
measurement error that is independent of nonwhite ∗ .
Probably in applications that seek to estimate a race effect, we would be most concerned
about omitted variables. While race is determined at birth, it is not independent of other factors
that generally affect economic and social outcomes. For example, we would want to include
family income and wealth in an equation to test for discrimination in loan applications. If we
cannot, and race is correlated with income and wealth, then an attempt to test for
discrimination can fail. Many other applications could suffer from endogeneity caused by
omitted variables. In looking at crime rates by race, we also need to control for family
background characteristics.
4.7. a. One important omitted factor in u is family income: students that come from
wealthier families tend to do better in school, other things equal. Family income and PC
ownership are positively correlated because the probability of owning a PC increases with
family income. Another factor in u is quality of high school. This may also be correlated with
PC: a student who had more exposure with computers in high school may be more likely to
own a computer.
b. ̂ 3 is likely to have an upward bias because of the positive correlation between u and PC,

19

but it is not clear cut because of the other explanatory variables in the equation. If we write the
linear projection
u   0   1 hsGPA   2 SAT   3 PC  r
then the bias is upward if  3 is greater than zero. This measures the partial correlation between
u (say, family income) and PC, and it is likely to be positive.
c. If data on family income can be collected then it can be included in the equation. If
family income is not available sometimes level of parents’ education is. Another possibility is
to use average house value in each student’s home zip code, as zip code is often part of school
records. Proxies for high school quality might be faculty−student ratios, expenditure per
student, average teacher salary, and so on.
4.8. a. ∂Ey|x 1 , x 2 /∂x 1   1   3 x 2 . Taking the expected value of this equation with
respect to the distribution of x 2 gives  1 ≡  1   3  2 . Similarly,
∂Ey|x 1 , x 2 /∂x 2   2   3 x 1  2 4 x 2 , and its expected value is  2 ≡  2   3  1  2 4  2 .
b. One way to write Ey|x 1 , x 2  is
Ey|x 1 , x 2    0   1 x 1   2 x 2   3 x 1 −  1 x 2 −  2    4 x 2 −  2  2 ,
where  0   0   3  1  2 −  4  22 (as can be verified by matching the intercepts in the two
equations).
c. Regress y i on 1, x i1 , x i2 , x i1 −  1 x i2 −  2 , x i2 −  2  2 , i  1, 2, . . . , N. If we do not
know  1 and  2 , we can estimate these using the sample averages, x̄ 1 and x̄ 2 .
d. The following Stata session can be used to answer this part:
. sum educ exper
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------educ |
935
13.46845
2.196654
9
18
exper |
935
11.56364
4.374586
1
23

20

. gen educ0exper0  (educ - 13.47)*(exper - 11.56)
. gen exper0sq  (exper - 11.56)^2
. reg lwage educ exper

educ0exper0 exper0sq

Source |
SS
df
MS
------------------------------------------Model | 22.7093743
4 5.67734357
Residual | 142.946909
930 .153706354
------------------------------------------Total | 165.656283
934 .177362188

Number of obs
F( 4,
930)
Prob  F
R-squared
Adj R-squared
Root MSE








935
36.94
0.0000
0.1371
0.1334
.39205

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.0837981
.0069787
12.01
0.000
.0701022
.097494
exper |
.0223954
.0034481
6.49
0.000
.0156284
.0291624
educ0exper0 |
.0045485
.0017652
2.58
0.010
.0010843
.0080127
exper0sq |
.0009943
.000653
1.52
0.128
-.0002872
.0022758
_cons |
5.392285
.1207342
44.66
0.000
5.155342
5.629228
----------------------------------------------------------------------------. gen educexper  educ*exper
. gen expersq  exper^2
. reg lwage educ exper

educexper expersq

Source |
SS
df
MS
------------------------------------------Model | 22.7093743
4 5.67734357
Residual | 142.946909
930 .153706354
------------------------------------------Total | 165.656283
934 .177362188

Number of obs
F( 4,
930)
Prob  F
R-squared
Adj R-squared
Root MSE








935
36.94
0.0000
0.1371
0.1334
.39205

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.0312176
.0193142
1.62
0.106
-.0066869
.0691221
exper | -.0618608
.0331851
-1.86
0.063
-.1269872
.0032656
educexper |
.0045485
.0017652
2.58
0.010
.0010843
.0080127
expersq |
.0009943
.000653
1.52
0.128
-.0002872
.0022758
_cons |
6.233415
.3044512
20.47
0.000
5.635924
6.830906
-----------------------------------------------------------------------------

In the equation where educ and exper are both demeaned before creating the interaction
and the squared terms, the coefficients on educ and exper seem reasonable. For example, the
coefficient on educ means that, at the average level of experience, the return to another year of
education is about 8.4%. As experience increases above its average value, the return to

21

education also increases (by . 45 percentage points for each year of experience above 11. 56).
In the model containing educ  exper and exper 2 , the coefficient on educ is the return to
education when exper  0 – not an especially interesting segment of the population, and
certainly not representative of the men in the sample.(Notice that the standard error of ̂ educ in
the second regression is almost three times the standard error in the first regression. This
difference illustrates that we can estimate the marginal effect at the average values of the
covariates much more precisely than at extreme values of the covariates.) The coefficient on
exper in the first regression is the return to another year of experience at the average values of
both educ and exper. So, for a man with about 13.5 years of education and 11.6 years of
experience, another year of experience is estimated to be worth about 2.2%. In the second
regression, where educ and exper are not first demeaned, the coefficient on exper is the return
to the first year of experience for a man with no schooling. This is not an interesting part of the
U.S. population, and, in a sample where the lowest completed grade is ninth, we have no hope
of estimating such an effect, anyway. The negative, large coefficient on exper in the second
regression is puzzling only when we forget what it actually estimates. Note that the standard
error on ̂ exper in the second regression is about 10 times as large as the standard error in the
first regression.
4.9. a. Just subtract logy −1  from both sides and define Δ logy  logy − logy −1 :
Δ logy   0  x   1 − 1 logy −1   u.
Clearly, the intercept and slope estimates on x will be the same. The coefficient on logy −1 
becomes  1 − 1.
b. For simplicity, let w  logy and w −1  logy −1 . Then the population slope coefficient
in a simple regression is always  1  Covw −1 , w/Varw −1 . By assumption,
22

Varw  Varw −1 , which means we can write  1  Covw −1 , w/ w −1  w , where
 w −1  sdw −1  and  w  sdw. But Corrw −1 , w  Covw −1 , w/ w −1  w , and since a
correlation coefficient is always between −1 and 1, the result follows.
4.10. Write the linear projection of x ∗K onto the other explanatory variables as
x ∗K   0   1 x 1   2 x 2 …  K−1 x K−1  r ∗K . Now, because x K  x ∗K  e K ,
Lx K |1, x 1 , … , x K−1   Lx ∗K |1, x 1 , … , x K−1   Le K |1, x 1 , … , x K−1 
 Lx ∗K |1, x 1 , … , x K−1 
because e K has zero mean and is uncorrelated with x 1 , … , x K−1 [and so
Le K |1, x 1 , … , x K−1   0]. But the linear projection error r K is
r K ≡ x K − Lx K |1, x 1 , … , x K−1   x ∗K − Lx ∗K |1, x 1 , … , x K−1   e K  r ∗K  e K .
Now we can use the two-step projection formula: the coefficient on x K in Ly|1, x 1 , … , x K  is
the coefficient in Ly|r K , say  1 . But
 1  Covr K , y/Varr K    K Covr ∗K , x ∗K /Varr K 
since e K is uncorrelated with x 1 , … , x K−1 , x ∗K , and v by assumption and r ∗K is uncorrelated with
x 1 , … , x K−1 , by definition. Now Covr ∗K , x ∗K   Varr ∗K  and Varr K   Varr ∗K   Vare K 
[because Covr ∗K , e K   0]. Therefore  1 is given by equation (4.47), which is what we wanted
to show.
4.11. Here is some Stata output obtained to answer this question:
. reg lwage exper tenure married south urban black educ iq kww
Source |
SS
df
MS
------------------------------------------Model | 44.0967944
9 4.89964382
Residual | 121.559489
925 .131415664
------------------------------------------Total | 165.656283
934 .177362188

Number of obs
F( 9,
925)
Prob  F
R-squared
Adj R-squared
Root MSE








935
37.28
0.0000
0.2662
0.2591
.36251

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval

23

---------------------------------------------------------------------------exper |
.0127522
.0032308
3.95
0.000
.0064117
.0190927
tenure |
.0109248
.0024457
4.47
0.000
.006125
.0157246
married |
.1921449
.0389094
4.94
0.000
.1157839
.2685059
south | -.0820295
.0262222
-3.13
0.002
-.1334913
-.0305676
urban |
.1758226
.0269095
6.53
0.000
.1230118
.2286334
black | -.1303995
.0399014
-3.27
0.001
-.2087073
-.0520917
educ |
.0498375
.007262
6.86
0.000
.0355856
.0640893
iq |
.0031183
.0010128
3.08
0.002
.0011306
.0051059
kww |
.003826
.0018521
2.07
0.039
.0001911
.0074608
_cons |
5.175644
.127776
40.51
0.000
4.924879
5.426408
----------------------------------------------------------------------------. test iq kww
( 1)
( 2)

iq  0
kww  0
F(

2,
925) 
Prob  F 

8.59
0.0002

a. The estimated return to education using both IQ and KWW as proxies for ability is about
5%. When we used no proxy the estimated return was about 6.5%, and with only IQ as a proxy
it was about 5.4%. Thus, we have an even lower estimated return to education, but it is still
practically nontrivial and statistically very significant.
b. We can see from the t statistics that these variables are going to be jointly significant.
The F test verifies this, with p-value  .0002.
c. The wage differential between nonblacks and blacks does not disappear. Blacks are
estimated to earn about 13% less than nonblacks, holding other factors in the regression fixed.
d. Adding the interaction terms described in the problem gives the following results:
. sum iq kww
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------iq |
935
101.2824
15.05264
50
145
kww |
935
35.74439
7.638788
12
56
. gen educiq0  educ*(iq - 100)
. gen educkww0  educ*(kww - 35.74)
. reg lwage exper tenure married south urban black educ iq kww educiq0 educkww0
Source |

SS

df

MS

24

Number of obs 

935

------------------------------------------Model | 45.1916886
11 4.10833533
Residual | 120.464595
923 .130514187
------------------------------------------Total | 165.656283
934 .177362188

F( 11,
923)
Prob  F
R-squared
Adj R-squared
Root MSE







31.48
0.0000
0.2728
0.2641
.36127

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------exper |
.0121544
.0032358
3.76
0.000
.005804
.0185047
tenure |
.0107206
.0024383
4.40
0.000
.0059353
.015506
married |
.1978269
.0388272
5.10
0.000
.1216271
.2740267
south | -.0807609
.0261374
-3.09
0.002
-.1320565
-.0294652
urban |
.178431
.026871
6.64
0.000
.1256957
.2311664
black | -.1381481
.0399615
-3.46
0.001
-.2165741
-.0597221
educ |
.0452316
.0076472
5.91
0.000
.0302235
.0602396
iq |
.0048228
.0057333
0.84
0.400
-.006429
.0160745
kww | -.0248007
.0107382
-2.31
0.021
-.0458749
-.0037266
educiq0 | -.0001138
.0004228
-0.27
0.788
-.0009436
.0007161
educkww0 |
.002161
.0007957
2.72
0.007
.0005994
.0037227
_cons |
6.080005
.5610875
10.84
0.000
4.978849
7.18116
----------------------------------------------------------------------------. test
( 1)
( 2)

educiq0 educkww0
educiq0  0
educkww0  0
F(

2,
923) 
Prob  F 

4.19
0.0154

The interaction educkww0 is statistically significant, and the two interactions are jointly
significant at the 2% signifiance level. The estimated return to education at the average values
of IQ and KWW (in the population and sample, respectively) is somewhat smaller now: about
4. 5%. Further, as KWW increases above its mean, the return to education increases. For
example, if KWW is about one standard deviation (7. 64) above its mean, the return to
education is about . 045 . 00227. 6 . 06172, or about 6. 2%. So “knowledge of the world of
work” interacts positively with education levels.
4.12. Here is the Stata output when union is added to both equations:
. reg lscrap grant union if d88
Source |
SS
df
MS
------------------------------------------Model | 4.59902319
2 2.29951159
Residual | 100.763637
51 1.97575759

25

Number of obs 
F( 2,
51) 
Prob  F

R-squared


54
1.16
0.3204
0.0436

------------------------------------------Total |
105.36266
53 1.98797472

Adj R-squared 
Root MSE


0.0061
1.4056

----------------------------------------------------------------------------lscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------grant | -.0276192
.4043649
-0.07
0.946
-.8394156
.7841772
union |
.6222888
.4096347
1.52
0.135
-.2000873
1.444665
_cons |
.2307292
.2648551
0.87
0.388
-.3009896
.762448
----------------------------------------------------------------------------. reg lscrap grant union lscrap_1 if d88
Source |
SS
df
MS
------------------------------------------Model | 92.7289733
3 30.9096578
Residual | 12.6336868
50 .252673735
------------------------------------------Total |
105.36266
53 1.98797472

Number of obs
F( 3,
50)
Prob  F
R-squared
Adj R-squared
Root MSE








54
122.33
0.0000
0.8801
0.8729
.50267

----------------------------------------------------------------------------lscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------grant | -.2851103
.1452619
-1.96
0.055
-.5768775
.0066568
union |
.2580653
.1477832
1.75
0.087
-.0387659
.5548965
lscrap_1 |
.8210298
.043962
18.68
0.000
.7327295
.90933
_cons | -.0477754
.0958824
-0.50
0.620
-.2403608
.14481
-----------------------------------------------------------------------------

The basic story does not change: initially, the grant is estimated to have essentially no
effect, but adding logscrap −1  gives the grant a strong effect that is marginally statistically
significant. Interestingly, unionized firms are estimated to have larger scrap rates; over 25%
more in the second equation. The effect is significant at the 10% level.
4.13. a. Using the 90 counties for 1987 gives
. reg lcrmrte lprbarr lprbconv lprbpris lavgsen if d87
Source |
SS
df
MS
------------------------------------------Model | 11.1549601
4 2.78874002
Residual | 15.6447379
85
.18405574
------------------------------------------Total |
26.799698
89 .301120202

Number of obs
F( 4,
85)
Prob  F
R-squared
Adj R-squared
Root MSE








90
15.15
0.0000
0.4162
0.3888
.42902

----------------------------------------------------------------------------lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.7239696
.1153163
-6.28
0.000
-.9532493
-.4946898
lprbconv | -.4725112
.0831078
-5.69
0.000
-.6377519
-.3072706
lprbpris |
.1596698
.2064441
0.77
0.441
-.2507964
.570136
lavgsen |
.0764213
.1634732
0.47
0.641
-.2486073
.4014499

26

_cons | -4.867922
.4315307
-11.28
0.000
-5.725921
-4.009923
-----------------------------------------------------------------------------

Because of the log-log functional form, all coefficients are elasticities. The elasticities of
crime with respect to the arrest and conviction probabilities are the sign we expect, and both
are practically and statistically significant. The elasticities with respect to the probability of
serving a prison term and the average sentence length are positive but are statistically
insignificant.
b. To add the previous year’s crime rate we first generate the first lag of lcrmrte:
. xtset county year
panel variable:
time variable:
delta:

county (strongly balanced)
year, 81 to 87
1 unit

. gen lcrmrte_1  L.lcrmrte
(90 missing values generated)
. reg lcrmrte lprbarr lprbconv lprbpris lavgsen lcrmrte_1 if d87
Source |
SS
df
MS
------------------------------------------Model | 23.3549731
5 4.67099462
Residual |
3.4447249
84
.04100863
------------------------------------------Total |
26.799698
89 .301120202

Number of obs
F( 5,
84)
Prob  F
R-squared
Adj R-squared
Root MSE








90
113.90
0.0000
0.8715
0.8638
.20251

----------------------------------------------------------------------------lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.1850424
.0627624
-2.95
0.004
-.3098523
-.0602325
lprbconv | -.0386768
.0465999
-0.83
0.409
-.1313457
.0539921
lprbpris | -.1266874
.0988505
-1.28
0.204
-.3232625
.0698876
lavgsen | -.1520228
.0782915
-1.94
0.056
-.3077141
.0036684
lcrmrte_1 |
.7798129
.0452114
17.25
0.000
.6899051
.8697208
_cons | -.7666256
.3130986
-2.45
0.016
-1.389257
-.1439946
-----------------------------------------------------------------------------

There are some notable changes in the coefficients on the original variables. The
elasticities with respect to prbarr and prbconv are much smaller now, but still have signs
predicted by a deterrent-effect story. The conviction probability is no longer statistically
significant. Adding the lagged crime rate changes the signs of the elasticities with respect to
prbpris and avgsen, and the latter is almost statistically significant at the 5% level against a

27

two-sided alternative (p-value  .056). Not surprisingly, the elasticity with respect to the
lagged crime rate is large and very statistically significant. (The elasticity is also statistically
less than unity.)
c. Adding the logs of the nine wage variables gives the following:
. reg lcrmrte lprbarr lprbconv lprbpris lavgsen lcrmrte_1 lwcon- lwloc if d87
Source |
SS
df
MS
------------------------------------------Model | 23.8798774
14 1.70570553
Residual | 2.91982063
75 .038930942
------------------------------------------Total |
26.799698
89 .301120202

Number of obs
F( 14,
75)
Prob  F
R-squared
Adj R-squared
Root MSE








90
43.81
0.0000
0.8911
0.8707
.19731

----------------------------------------------------------------------------lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.1725122
.0659533
-2.62
0.011
-.3038978
-.0411265
lprbconv | -.0683639
.049728
-1.37
0.173
-.1674273
.0306994
lprbpris | -.2155553
.1024014
-2.11
0.039
-.4195493
-.0115614
lavgsen | -.1960546
.0844647
-2.32
0.023
-.364317
-.0277923
lcrmrte_1 |
.7453414
.0530331
14.05
0.000
.6396942
.8509887
lwcon | -.2850008
.1775178
-1.61
0.113
-.6386344
.0686327
lwtuc |
.0641312
.134327
0.48
0.634
-.2034619
.3317244
lwtrd |
.253707
.2317449
1.09
0.277
-.2079525
.7153665
lwfir | -.0835258
.1964974
-0.43
0.672
-.4749687
.3079171
lwser |
.1127542
.0847427
1.33
0.187
-.0560619
.2815703
lwmfg |
.0987371
.1186099
0.83
0.408
-.1375459
.3350201
lwfed |
.3361278
.2453134
1.37
0.175
-.1525615
.8248172
lwsta |
.0395089
.2072112
0.19
0.849
-.3732769
.4522947
lwloc | -.0369855
.3291546
-0.11
0.911
-.6926951
.6187241
_cons | -3.792525
1.957472
-1.94
0.056
-7.692009
.1069593
----------------------------------------------------------------------------. testparm lwcon-lwloc
(
(
(
(
(
(
(
(
(

1)
2)
3)
4)
5)
6)
7)
8)
9)

lwcon
lwtuc
lwtrd
lwfir
lwser
lwmfg
lwfed
lwsta
lwloc
F(











0
0
0
0
0
0
0
0
0

9,
75) 
Prob  F 

1.50
0.1643

The nine wage variables are jointly insignificant even at the 15% level. Plus, the elasticities

28

are not consistently positive or negative. The two largest elasticities – which also have the
largest absolute t statistics – have the opposite sign. These are with respect to the wage in
construction ( −. 285) and the wage for federal employees (. 336).
d. The following Stata output gives the heteroskedasiticity-robust F statistic:
. qui reg lcrmrte lprbarr lprbconv lprbpris lavgsen lcrmrte_1 lwcon- lwloc if
. testparm lwcon-lwloc
(
(
(
(
(
(
(
(
(

1)
2)
3)
4)
5)
6)
7)
8)
9)

lwcon
lwtuc
lwtrd
lwfir
lwser
lwmfg
lwfed
lwsta
lwloc
F(











0
0
0
0
0
0
0
0
0

9,
75) 
Prob  F 

2.19
0.0319

Therefore, we would reject the null at the 5% signifiance level. But we might hesitate to
rely on asymptotic theory – which the heteroskedasticity-robust test requires – with N  90
and K  15 parameters to estimate. (This heteroskedasticity-robust F statistic is the
heteroskedasticity-robust Wald statistic divided by the number of restrictions being tested,
which is nine in this example. The division by the number of restrictions turns the asymptotic
chi-square statistic into one can be treated as having roughly an F distribution.)
4.14. a. Before doing the regression, it is helpful to know some summary statistics for the
variables of primary interest:
. sum stndfnl atndrte
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------stndfnl |
680
.0296589
.9894611 -3.308824
2.783613
atndrte |
680
81.70956
17.04699
6.25
100

Because the final exam score has been standardized, it has close to a zero mean and its

29

standard deviation is close to one. The values are not closer to zero and one, respectively,
because the standardization was done with a larger data set that included students with missing
values on other key variables. It might make sense to redefine the standardized test score using
the mean and standard deviation in the sample of 680, but the effect should be minor.
The regression that controls only for year in school in addition to attendance rate is as
follows:
. reg stndfnl atndrte frosh soph
Source |
SS
df
MS
------------------------------------------Model | 19.3023776
3 6.43412588
Residual |
645.46119
676 .954824246
------------------------------------------Total | 664.763568
679 .979033237

Number of obs
F( 3,
676)
Prob  F
R-squared
Adj R-squared
Root MSE








680
6.74
0.0002
0.0290
0.0247
.97715

----------------------------------------------------------------------------stndfnl |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------atndrte |
.0081634
.0022031
3.71
0.000
.0038376
.0124892
frosh | -.2898943
.1157244
-2.51
0.012
-.5171168
-.0626719
soph | -.1184456
.0990267
-1.20
0.232
-.3128824
.0759913
_cons | -.5017308
.196314
-2.56
0.011
-.8871893
-.1162724
-----------------------------------------------------------------------------

If atndrte increases by 10 percentage points (say, from 75 to 85), the standardized test
score is estimated to increase by about . 082 standard deviations.
b. Certainly there is a potential for self-selection. The better students may also be the ones
attending lecture more regularly. So the positive effect of the attendance rate simply might
capture the fact that better students tend to do better on exams. It is unlikely that controlling
just for year in college (frosh and soph) solves the endogeneity of atndrete.
c. Adding priGPA and ACT gives
reg stndfnl atndrte frosh soph priGPA ACT
Source |
SS
df
MS
------------------------------------------Model | 136.801957
5 27.3603913
Residual | 527.961611
674 .783325833
-------------------------------------------

30

Number of obs
F( 5,
674)
Prob  F
R-squared
Adj R-squared







680
34.93
0.0000
0.2058
0.1999

Total |

664.763568

679

.979033237

Root MSE



.88506

----------------------------------------------------------------------------stndfnl |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------atndrte |
.0052248
.0023844
2.19
0.029
.000543
.0099065
frosh | -.0494692
.1078903
-0.46
0.647
-.2613108
.1623724
soph | -.1596475
.0897716
-1.78
0.076
-.3359132
.0166181
priGPA |
.4265845
.0819203
5.21
0.000
.2657348
.5874343
ACT |
.0844119
.0111677
7.56
0.000
.0624843
.1063395
_cons | -3.297342
.308831
-10.68
0.000
-3.903729
-2.690956
-----------------------------------------------------------------------------

The effect of atndrte has fallen, which is what we expect if we think better, smarter
students also attend lectures more frequently. The estimate now is that a 10 percentage point
increase in atndrte increases the standardized test score by . 052 standard deviations; the effect
is statistically significant at the usual 5% level against a two-sided alternative, but the t statistic
is much lower than in part a. The strong positive effects of prior GPA and ACT score are also
expected.
d. Controlling for priGPA and ACT causes the sophomore effect (relative to students in
year three and beyond) to get slightly larger in magnitude and more statistically significant.
These data are for a course taught in the second term, so each frosh student does have a prior
GPA – his or her GPA for the first semester in college. Adding priGPA in particular causes the
“freshman effect” to essentially disappear. This is not too surprising because the average prior
GPA for first-year students is notably less than the overall average priGPA.
e. Here is the Stata session for adding squares in the proxy variables. Because we are not
interested in the effects of the proxies, we do not demean them before creating the squared
terms:
. gen priGPAsq  priGPA^2
. gen ACTsq  ACT^2
. reg stndfnl atndrte frosh soph priGPA ACT
Source |
SS
df
MS
-------------------------------------------

31

priGPAsq ACTsq
Number of obs 
F( 7,
672) 

680
28.94

Model | 153.974309
7 21.9963299
Residual | 510.789259
672 .760103064
------------------------------------------Total | 664.763568
679 .979033237

Prob  F
R-squared
Adj R-squared
Root MSE






0.0000
0.2316
0.2236
.87184

----------------------------------------------------------------------------stndfnl |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------atndrte |
.0062317
.0023583
2.64
0.008
.0016011
.0108623
frosh | -.1053368
.1069747
-0.98
0.325
-.3153817
.1047081
soph | -.1807289
.0886354
-2.04
0.042
-.3547647
-.0066932
priGPA |
-1.52614
.4739715
-3.22
0.001
-2.456783
-.5954966
ACT | -.1124331
.098172
-1.15
0.253
-.3051938
.0803276
priGPAsq |
.3682176
.0889847
4.14
0.000
.1934961
.5429391
ACTsq |
.0041821
.0021689
1.93
0.054
-.0000766
.0084408
_cons |
1.384812
1.239361
1.12
0.264
-1.048674
3.818298
-----------------------------------------------------------------------------

Adding the squared terms – one of which is very significant, the other of which is
marginally significant – actually increases the attendance rate effect. And it does so while
slightly reducing the standard error on atndrte, resulting in a t statistic that is notably more
significant than in part c.
f. Adding the squared attendance rate is not warranted, as it is very insignificant:
. gen atndrtesq  atndrte^2
. reg stndfnl atndrte frosh soph priGPA ACT
Source |
SS
df
MS
------------------------------------------Model | 153.975323
8 19.2469154
Residual | 510.788245
671 .761234344
------------------------------------------Total | 664.763568
679 .979033237

priGPAsq ACTsq

atndrtesq

Number of obs
F( 8,
671)
Prob  F
R-squared
Adj R-squared
Root MSE








680
25.28
0.0000
0.2316
0.2225
.87249

----------------------------------------------------------------------------stndfnl |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------atndrte |
.0058425
.0109203
0.54
0.593
-.0155996
.0272847
frosh | -.1053656
.1070572
-0.98
0.325
-.3155729
.1048418
soph | -.1808403
.0887539
-2.04
0.042
-.355109
-.0065716
priGPA | -1.524803
.475737
-3.21
0.001
-2.458915
-.5906902
ACT | -.1123423
.0982764
-1.14
0.253
-.3053087
.080624
priGPAsq |
.3679124
.0894427
4.11
0.000
.192291
.5435337
ACTsq |
.0041802
.0021712
1.93
0.055
-.0000829
.0084433
atndrtesq |
2.87e-06
.0000787
0.04
0.971
-.0001517
.0001574
_cons |
1.394292
1.267186
1.10
0.272
-1.093835
3.88242
-----------------------------------------------------------------------------

The very large increase in the standard error on atndrte suggest that atndrte and atndrte 2

32

are highly collinear. In fact, their sample correlation is about . 983. Importantly, the coefficient
on atndrte now has an uninteresting interpretation: it measures the partial effect of atndrte
starting from atndrte  0. The lowest attendance rate in the sample is 6.25, with the vast
majority of students (94.3%) attending 50 percent or more of the lectures. If the quadratic term
were significant, we might want to center atndrte about its mean or median before creating the
square. Or, a more sophisticated functional form might be called for. It may be better to define
several intervals for atndrrte and include dummy variables for those intervals.
4.15. a. Because each x j has finite second moment, Varx  . Since Varu  ,
Covx, u is well−defined. But each x j is uncorrelated with u, so Covx, u  0. Therefore,
Vary  Varx  Varu or  2y  Varx   2u .
b. This is nonsense when we view x i as a random draw along with y i . The statement
“Varu i    2  Vary i  for all i” assumes that the regressors are nonrandom (or   0, which
is not a very interesting case). This is another example of how the assumption of nonrandom
regressors can lead to counterintuitive conclusions. Suppose that an element of the error term,
say z, which is uncorrelated with each x j , suddenly becomes observed. When we add z to the
regressor list, the error changes, and so does the error variance. In the vast majority of
economic applications, it makes no sense to think we have access to the entire set of factors
that one would ever want to control for, and so we should allow for error variances to change
across different sets of explanatory variables that we might use for the same response variable.
We avoid trouble by focusing on joint distributions in the population.
c. Write R 2  1 − SSR/SST  1 − SSR/N/SST/N. Therefore,
plimR 2   1 − plimSSR/N/SST/N  1 − plimSSR/N/plimSST/N  1 −  2u / 2y 
where we use the fact that SSR/N is a consistent estimator of  2u and SST/N is a consistent

33

estimator of  2y .
d. The derivation in part c assumed nothing about Varu|x. The population R-squared
depends on only the unconditional variances of u and y. Therefore, regardless of the nature of
heteroskedasticity in Varu|x, the usual R−squared consistently estimates the population
R-squared. Neither R-squared nor the adjusted R-squared has desirable finite-sample
properties, such as unbiasedness, so the only analysis we can given in any generality involves
asymptotics. The statement in the problem is simply wrong.
4.16. a. The proof is fairly similar to that for random sampling. First, note that the
p

N

assumptions N −1 ∑ i1 x ′i x i − Ex ′i x i  → 0 – which is how the WLLN is stated for i.n.i.d.
N

sequences – and N −1 ∑ i1 Ex ′i x i  → A – which is not crucial but is pretty harmless and
simplifies the proof – imply
N

N −1 ∑ x ′i x i → A
p

i1

N

In addition, Ex ′i u i   0 and the assumption that N −1 ∑ i1 x ′i u i satisfies the law of large
numbers imply
N

N −1 ∑ x ′i u i → 0.
p

i1

We are also given that A is positive definite, which means X ′ X/N is invertible with probability
p

approaching one and X ′ X/N −1 → A −1 . Therefore,

34

−1

N

plim N→ ̂    plim

N

−1

∑

x ′i x i

N

N

i1

   plim

N

∑

∑ x ′i u i
i1

−1

N

−1

−1

x ′i x i

i1

N

plim N −1 ∑ x ′i u i
i1

   A −1  0  .
d

N

N

b. Because N −1/2 ∑ i1 x ′i u i → Normal0, B, the sequence N −1/2 ∑ i1 x ′i u i is O p 1. We
already used in part a that
−1

N

N −1 ∑ x ′i x i

− A −1  o p 1

i1

Now, as in the i.i.d. case, write
N ̂ −  

−1

N

N

−1

∑

N

x ′i x i

N

−1/2

i1

i1

−1

N



N

−1

∑

∑ x ′i u i

x ′i x i

N

−A

−1

N

−1/2

i1

∑
i1

N

x ′i u i

A

−1

N

−1/2

∑ x ′i u i
i1

N

 o p 1  O p 1  A

−1

N

−1/2

∑ x ′i u i
i1

d

→ Normal0, A −1 BA −1 
N

d

where we use the assumption N −1/2 ∑ i1 x ′i u i → Normal0, B. The asymptotic variance of
N ̂ −  has the usual sandwich form, A −1 BA −1 .
c. We already know that
N

N −1 ∑ x ′i x i → A.
p

i1

Further, by the WLLN and the assumption that B N → B,

35

N

N −1 ∑ u 2i x ′i x i → B
p

i1

The hard part – just as with the i.i.d. case – is to show that replacing the u i with the OLS
residuals, û i , does not affect consistency. Nevertheless, under general assumptions it follows
that
N

N

−1

∑ û 2i x ′i x i → B
p

i1

Naturally, we can use the same degrees-of-freedom adjustment as in the i.i.d. case: replace N −1
with N − K −1 .
d. The point of this exercise is that we are led to exactly the same heteroskedasticity-robust
estimator whether we assume i.i.d. observations or i.n.i.d. observations. In particular, even if
unconditional variances are constant – as they must be in the i.i.d. case – we still might need
heteroskedasticity-robust standard errors. In the i.n.i.d. case, the robust variance matrix
estimator allows for changing unconditional variances as well as conditional variances that
depend on x i .
4.17. We know that, in general,
Avar N ̂ −   Ex ′ x −1 Eu 2 x ′ xEx ′ x −1 .
Now we just apply iterated expecations to the matrix in the middle:
Eu 2 x ′ x  EEu 2 x ′ x|x  EEu 2 |xx ′ x  Ehxx ′ x
4.18. a. This is a fairly common misconception – or at least misstatement. Recall that the
distribution of any random draw, u i , is the population distribution of u. But, of course, the
population distribution of u is what it is; it does not change with the sample size. In fact, it has

36

nothing to do with the sample size. Therefore, the random draws on u i have the same
distribution regardless of N. A correct statement is that the standardized average of the errors,
N

N −1/2 ∑ i1 u i 

N ū approaches normality as N → . This is a much different statement. (In
N

regression analysis, we use the fact that N −1/2 ∑ i1 x ′i u i generally converges to a multivariate
N

normal distribution, which implies the convergence of N −1/2 ∑ i1 u i to normality when x i
contains unity.)
b. It is tempting but incorrect to think that a single squared OLS residual can consistently
estimate a conditional mean, Eu 2i |x i  ≡ hx i , but there is no sense in which this statement is
true. It is not even clear what we would mean by it, but we can make some headway by writing
p

û 2i  u 2i − 2u i x i ̂ −   x i ̂ −  2 . Now, we can conclude û 2i − u 2i → 0 and N →  because
p
̂ → . But remember u 2i  hx i   v i where Ev i |x i   0. There is no sense in which u 2i is a

consistent estimator of hx i ; they do not even depend on the sample size N.
It was the view that we needed û 2i to be a good estimate of Eu 2i |x i  that possibly held up
progress on heteroskedasticity-consistent covariance matrices. Fortunately, all we need to
consistent estmate is the population mean
B  Eu 2 x ′ x,
for which the obvious consistent (and unbiased) estimator is
N

N

−1

∑ u 2i x ′i x i .
i1

The rest is demonstrating the replacing the implicit  with ̂ preserves consistency (not
unbiasedness). As we know, this requires some tricky algebra with o p 1 and O p 1, but the
work is not too onerous.

37

Solutions to Chapter 5 Problems
′
5.1. Define x 1 ≡ z 1 , y 2  and x 2 ≡ v̂ 2 , and let ̂ ≡ ̂ 1 , ̂ 1  ′ be OLS estimator from (5.52),
′
where ̂ 1  ̂ 1 , ̂ 1 . Using the hint, ̂ 1 can also be obtained by partitioned regression:

(i) Regress x 1 onto v̂ 2 and save the residuals, say ẍ 1 .
(ii) Regress y 1 onto ẍ 1 .
But when we regress z 1 onto v̂ 2 the residuals are just z 1 because v̂ 2 is orthogonal in sample
N

to z. (More precisely, ∑ i1 z ′i1 v̂ i2  0.) Further, because we can write y 2  ŷ 2  v̂ 2 , where ŷ 2
and v̂ 2 are orthogonal in sample, the residuals from regressing y 2 onto v̂ 2 are simply the first
stage fitted values, ŷ 2 . In other words, ẍ 1  z 1 , ŷ 2 . But the 2SLS estimator of  1 is obtained
exactly from the OLS regression y 1 on z 1 , ŷ 2 .
5.2. a. Unobserved factors that tend to make an individual healthier also tend to make that
person exercise more. For example, if health is a cardiovascular measure, people with a history
of heart problems are probably less likely to exercise. Unobserved factors such as prior health
or family history are contained in u 1 , and so we are worried about correlation between exercise
and u 1 . Self-selection into exercising predicts that the benefits of exercising will be, on
average, overestimated. Ideally, the amount of exercise could be randomized across a sample
of people, but this can be difficult.
b. If people do not systematically choose the location of their homes and jobs relative to
health clubs based on unobserved health characteristics, then it is reasonable to believe that
disthome and distwork are uncorrelated with u 1 . But the location of health clubs is not
necessarily exogenous. Clubs may tend to be built near neighborhoods where residents have
higher income and wealth, on average, and these factors can certainly affect overall health. It

38

may make sense to choose residents from neighborhoods with very similar characteristics but
where one neighborhood is located near a health club.
c. The reduced form for exercise is
exercise   0   1 age   2 weight   3 height
  4 male   5 work   6 disthome   7 distwork  u 1 ,
For identification we need at least one of  6 and  7 to be different from zero. this
assumption can fail if the amount that people exercise is not systematically related to distances
to the nearest health club.
d. An F test of H 0 :  6  0,  7  0 is the simplest way to test the identification assumption
in part c. As usual, it would be a good idea to compute a heteroskedasticity-robust version.
5.3. a. There may be unobserved health factors correlated with smoking behavior that
affect infant birth weight. For example, women who smoke during pregnancy may, on average,
drink more coffee or alcohol, or eat less nutritious meals.
b. Basic economics says that packs should be negatively correlated with cigarette price,
although the correlation might be small (especially because price is aggregated at the state
level). At first glance it seems that cigarette price should be exogenous in equation (5.54), but
we must be a little careful. One component of cigarette price is the state tax on cigarettes.
States that have lower taxes on cigarettes may also have lower quality of health care, on
average. Quality of health care is in u, and so maybe cigarette price fails the exogeneity
requirement for an IV.
c. OLS is followed by 2SLS (IV, in this case):
. reg lbwght male parity lfaminc packs
Source |
SS
df
MS
------------------------------------------Model | 1.76664363
4 .441660908

39

Number of obs 
F( 4, 1383) 
Prob  F


1388
12.55
0.0000

Residual |
48.65369 1383 .035179819
------------------------------------------Total | 50.4203336 1387 .036352079

R-squared

Adj R-squared 
Root MSE


0.0350
0.0322
.18756

----------------------------------------------------------------------------lbwght |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------male |
.0262407
.0100894
2.60
0.009
.0064486
.0460328
parity |
.0147292
.0056646
2.60
0.009
.0036171
.0258414
lfaminc |
.0180498
.0055837
3.23
0.001
.0070964
.0290032
packs | -.0837281
.0171209
-4.89
0.000
-.1173139
-.0501423
_cons |
4.675618
.0218813
213.68
0.000
4.632694
4.718542
----------------------------------------------------------------------------. ivreg lbwght male parity lfaminc (packs  cigprice)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | -91.350027
4 -22.8375067
Residual | 141.770361 1383 .102509299
------------------------------------------Total | 50.4203336 1387 .036352079

Number of obs
F( 4, 1383)
Prob  F
R-squared
Adj R-squared
Root MSE








1388
2.39
0.0490
.32017

----------------------------------------------------------------------------lbwght |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------packs |
.7971063
1.086275
0.73
0.463
-1.333819
2.928031
male |
.0298205
.017779
1.68
0.094
-.0050562
.0646972
parity | -.0012391
.0219322
-0.06
0.955
-.044263
.0417848
lfaminc |
.063646
.0570128
1.12
0.264
-.0481949
.1754869
_cons |
4.467861
.2588289
17.26
0.000
3.960122
4.975601
----------------------------------------------------------------------------Instrumented: packs
Instruments:
male parity lfaminc cigprice
-----------------------------------------------------------------------------

The difference between OLS and IV in the estimated effect of packs on bwght is huge.
With the OLS estimate, one more pack of cigarettes is estimated to reduce bwght by about
8.4%, and is statistically significant. The IV estimate has the opposite sign, is huge in
magnitude, and is not statistically significant. The sign and size of the smoking effect are not
realistic.
d. We can see the problem with IV by estimating the reduced form for packs.
. reg packs male parity lfaminc cigprice
Source |
SS
df
MS
-------------------------------------------

40

Number of obs 
F( 4, 1383) 

1388
10.86

Model | 3.76705108
4
.94176277
Residual | 119.929078 1383 .086716615
------------------------------------------Total | 123.696129 1387 .089182501

Prob  F
R-squared
Adj R-squared
Root MSE






0.0000
0.0305
0.0276
.29448

----------------------------------------------------------------------------packs |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------male | -.0047261
.0158539
-0.30
0.766
-.0358264
.0263742
parity |
.0181491
.0088802
2.04
0.041
.0007291
.0355692
lfaminc | -.0526374
.0086991
-6.05
0.000
-.0697023
-.0355724
cigprice |
.000777
.0007763
1.00
0.317
-.0007459
.0022999
_cons |
.1374075
.1040005
1.32
0.187
-.0666084
.3414234
-----------------------------------------------------------------------------

The reduced form estimates show that cigprice does not significantly affect packs. In fact,
the coefficient on cigprice does not have the sign we expect. Thus, cigprice fails as an IV for
packs because cigprice is not partially correlated with packs with a sensible sign for the
correlation. This is separate from the problem that cigprice may not truly be exogenous in the
birth weight equation.
5.4. a. Here are the OLS results:
. reg lwage educ exper expersq black south smsa reg661-reg668 smsa66
Source |
SS
df
MS
------------------------------------------Model | 177.695591
15 11.8463727
Residual | 414.946054 2994 .138592536
------------------------------------------Total | 592.641645 3009 .196956346

Number of obs
F( 15, 2994)
Prob  F
R-squared
Adj R-squared
Root MSE








3010
85.48
0.0000
0.2998
0.2963
.37228

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.0746933
.0034983
21.35
0.000
.0678339
.0815527
exper |
.084832
.0066242
12.81
0.000
.0718435
.0978205
expersq |
-.002287
.0003166
-7.22
0.000
-.0029079
-.0016662
black | -.1990123
.0182483
-10.91
0.000
-.2347927
-.1632318
south |
-.147955
.0259799
-5.69
0.000
-.1988952
-.0970148
smsa |
.1363845
.0201005
6.79
0.000
.0969724
.1757967
reg661 | -.1185698
.0388301
-3.05
0.002
-.194706
-.0424335
reg662 | -.0222026
.0282575
-0.79
0.432
-.0776088
.0332036
reg663 |
.0259703
.0273644
0.95
0.343
-.0276846
.0796251
reg664 | -.0634942
.0356803
-1.78
0.075
-.1334546
.0064662
reg665 |
.0094551
.0361174
0.26
0.794
-.0613623
.0802725
reg666 |
.0219476
.0400984
0.55
0.584
-.0566755
.1005708
reg667 | -.0005887
.0393793
-0.01
0.988
-.077802
.0766245
reg668 | -.1750058
.0463394
-3.78
0.000
-.265866
-.0841456
smsa66 |
.0262417
.0194477
1.35
0.177
-.0118905
.0643739
_cons |
4.739377
.0715282
66.26
0.000
4.599127
4.879626

41

-----------------------------------------------------------------------------

The estimated return to education is about 7.5%, with a very large t statistic. These
reproduce the estimates from Table 2, Column (2) in Card (1995).
b. The reduced form for educ is
. reg educ exper expersq black south smsa reg661-reg668 smsa66 nearc4
Source |
SS
df
MS
------------------------------------------Model | 10287.6179
15 685.841194
Residual | 11274.4622 2994 3.76568542
------------------------------------------Total | 21562.0801 3009 7.16586243

Number of obs
F( 15, 2994)
Prob  F
R-squared
Adj R-squared
Root MSE








3010
182.13
0.0000
0.4771
0.4745
1.9405

----------------------------------------------------------------------------educ |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------exper | -.4125334
.0336996
-12.24
0.000
-.4786101
-.3464566
expersq |
.0008686
.0016504
0.53
0.599
-.0023674
.0041046
black | -.9355287
.0937348
-9.98
0.000
-1.11932
-.7517377
south | -.0516126
.1354284
-0.38
0.703
-.3171548
.2139296
smsa |
.4021825
.1048112
3.84
0.000
.1966732
.6076918
reg661 |
-.210271
.2024568
-1.04
0.299
-.6072395
.1866975
reg662 | -.2889073
.1473395
-1.96
0.050
-.5778042
-.0000105
reg663 | -.2382099
.1426357
-1.67
0.095
-.5178838
.0414639
reg664 |
-.093089
.1859827
-0.50
0.617
-.4577559
.2715779
reg665 | -.4828875
.1881872
-2.57
0.010
-.8518767
-.1138982
reg666 | -.5130857
.2096352
-2.45
0.014
-.9241293
-.1020421
reg667 | -.4270887
.2056208
-2.08
0.038
-.8302611
-.0239163
reg668 |
.3136204
.2416739
1.30
0.194
-.1602434
.7874841
smsa66 |
.0254805
.1057692
0.24
0.810
-.1819071
.2328682
nearc4 |
.3198989
.0878638
3.64
0.000
.1476194
.4921785
_cons |
16.84852
.2111222
79.80
0.000
16.43456
17.26248
-----------------------------------------------------------------------------

The important coefficient is on nearc4. Statistically, educ and nearc4 are partially
correlated, and in a way that makes sense: holding other factors in the reduced form fixed,
someone living near a four-year college at age 16 has, on average, almost one-third a year
more education than a person not near a four-year college at age 16. This is not trivial a effect,
so nearc4 passes the requirement that it is partially correlated with educ.
c. Here are the IV estimates:
. ivreg lwage exper expersq black south smsa reg661-reg668 smsa66 (educ  nearc4

42

Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | 141.146813
15 9.40978752
Residual | 451.494832 2994 .150799877
------------------------------------------Total | 592.641645 3009 .196956346

Number of obs
F( 15, 2994)
Prob  F
R-squared
Adj R-squared
Root MSE








3010
51.01
0.0000
0.2382
0.2343
.38833

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.1315038
.0549637
2.39
0.017
.0237335
.2392742
exper |
.1082711
.0236586
4.58
0.000
.0618824
.1546598
expersq | -.0023349
.0003335
-7.00
0.000
-.0029888
-.001681
black | -.1467757
.0538999
-2.72
0.007
-.2524603
-.0410912
south | -.1446715
.0272846
-5.30
0.000
-.19817
-.091173
smsa |
.1118083
.031662
3.53
0.000
.0497269
.1738898
reg661 | -.1078142
.0418137
-2.58
0.010
-.1898007
-.0258278
reg662 | -.0070465
.0329073
-0.21
0.830
-.0715696
.0574767
reg663 |
.0404445
.0317806
1.27
0.203
-.0218694
.1027585
reg664 | -.0579172
.0376059
-1.54
0.124
-.1316532
.0158189
reg665 |
.0384577
.0469387
0.82
0.413
-.0535777
.130493
reg666 |
.0550887
.0526597
1.05
0.296
-.0481642
.1583416
reg667 |
.026758
.0488287
0.55
0.584
-.0689832
.1224992
reg668 | -.1908912
.0507113
-3.76
0.000
-.2903238
-.0914586
smsa66 |
.0185311
.0216086
0.86
0.391
-.0238381
.0609003
_cons |
3.773965
.934947
4.04
0.000
1.940762
5.607169
----------------------------------------------------------------------------Instrumented: educ
Instruments:
exper expersq black south smsa reg661 reg662 reg663 reg664
reg665 reg666 reg667 reg668 smsa66 nearc4
-----------------------------------------------------------------------------

The estimated return to education has increased to about 13.2%, but notice how wide the
95% confidence interval is: 2.4% to 23.9%. By contrast, the OLS confidence interval is about
6.8% to 8.2%, which is much tighter. Of course, OLS could be inconsistent, in which case a
tighter CI is of little value. But the estimated return to education is higher with IV, something
that seems a bit counterintuitive.
One possible explanation is that educ suffers from classical errors-in-variables. Therefore,
while OLS would tend to overestimate the return to schooling because of omitted “ability,”
classical measurement error in educ leads to an attenuation bias. Measurement error may help
explain why the IV estimate is larger, but it is not entirely convincing. It seems unlikely that
educ satisfies the CEV assumptions. For example, if we think the measurement error is due to

43

truncation – people are asked about highest grade completed, not actual years of schooling –
then educ is always less than or equal to educ ∗ . And the measurement error could not be
independent of educ ∗ . If we think the mismeasurement is due to is unobserved quality of
schooling, it seems likely that quality of schooling – part of the measurement error – is
positively correlated with actual amount of schooling. This, too, violates the CEV assumptions.
Another possibility for the much higher IV estimate comes out of the recent treatment effect
literature, which is covered in Section 21.4. Of course, we must also remember that the point
estimates – particularly the IV estimate – are subject to substantial sampling variation. At this
point, we do not even know of OLS and IV are statistically different from each other. See
Problem 6.1.
d. When nearc2 is added to the reduced form of educ it has a coefficient (standard error) of
.123 (.077), compared with .321 (.089) for nearc4. Therefore, nearc4 has a much stronger
ceteris paribus relationship with educ; nearc2 is only marginally statistically significant once
nearc4 has been included. The joint F test gives F  7. 89 with p-value  . 004.
The 2SLS estimate of the return to education becomes about 15.7%, with 95% CI given by
5.4% to 26%. The CI is still very wide.
5.5. Under the null hypothesis that qand z 2 are uncorrelated, z 1 and z 2 are exogenous in
(5.55) because each is uncorrelated with u 1 . Unfortunately, y 2 is correlated with u 1 , and so the
regression of y 1 on z 1 , y 2 , z 2 does not produce a consistent estimator of 0 on z 2 even when
Ez ′2 q  0. We could find that ̂ 1 from this regression is statistically different from zero even
when q and z 2 are uncorrelated – in which case we would incorrectly conclude that z 2 is not a
valid IV candidate. Or, we might fail to reject H 0 :  1  0 when z 2 and q are correlated – in

44

which case we incorrectly conclude that the elements in z 2 are valid as instruments.
The point of this exercise is that one cannot simply add instrumental variable candidates in
the structural equation and then test for significance of these variables using OLS estimation.
This is the sense in which identification cannot be tested: we cannot test whether all of the IV
candidates are uncorrelated with q. With a single endogenous variable, we must take a stand
that at least one element of z 2 is uncorrelated with q.
5.6. a. By definition, the reduced form is the linear projection
Lq 1 |1, x, q 2    0  x 1   2 q 2 ,
and we want to show that  1  0 when q 2 is uncorrelated with x. Now, because q 2 is a linear
function of q and a 2 , and a 2 is uncorrelated with x, q 2 is uncorrelated with x if and only if q is
uncorrelated with x. Assuming then that q and x are uncorrelated, q 1 is also uncorrelated with
x. A basic fact about linear projections is that, because q 1 and q 2 are each uncorrelated with
the vector x,  1  0. This claim follows from Property LP.7:  1 can be obtained by first
projecting x on 1, q 2 and obtaining the population residuals, say r . Then, project q 1 onto r. But
because x and q 2 are orthogonal, r  x −  x . Projecting q 1 on x −  x  just gives the zero
vector because Ex −  x  ′ q 1   0. Therefore,  1  0.
b. If q 2 and x are correlated then  1 ≠ 0, and x appears in the reduced form for q 1 . It is not
realistic to assume that q 2 and x are uncorrelated. Under the multiple indicator assumptions,
assuming x and q 2 are uncorrelated is the same as assuming q and x are uncorrelated. If we
believe q and x are uncorrelated then there is no need to collect indicators on q to consistently
estimate : we could simply put q into the error term and estimate  from an OLS regression of
y on 1, x. (Of course, if q and x are uncorrelated we could, in general, gain efficiency for

45

estimating  by including q as an extra regressor.)
5.7. a. If we plug q  1/ 1 q 1 − 1/ 1 a 1 into equation (5.45) we get
y   0   1 x 1 . . .  K x K   1 q 1  v −  1 a 1 ,

(5.56)

where  1 ≡ 1/ 1  . Now, because the z h are redundant in (5.45), they are uncorrelated with
the structural error, v (by definition of redundancy). Further, we have assumed that the z h are
uncorrelated with a 1 . Since each x j is also uncorrelated with v −  1 a 1 we can estimate (5.56)
by 2SLS using instruments 1, x 1 , … , x K , z 1 , z 2 , … , z M  to get consistent of the  j and  1 .
Given all of the zero correlation assumptions, what we need for identification is that at
least one of the z h appears in the reduced form for q 1 . More formally, in the linear projection
q 1   0   1 x 1 . . .  K x K   K1 z 1 . . .  KM z M  r 1 ,
at least one of  K1 , … ,  KM must be different from zero.
b. We need family background variables to be redundant in the logwage equation once
ability (and other factors, such as educ and exper), have been controlled for. The idea here is
that family background may influence ability but should have no partial effect on logwage
once ability has been accounted for. For the rank condition to hold, we need family
background variables to be correlated with the indicator, q 1 say IQ, once the x j have been
netted out. This is likely to be true if we think that family background and ability are
(partially) correlated.
c. Applying the procedure to the data set in NLS80.RAW gives the following results:
. ivreg lwage exper tenure educ married south urban black (iq  meduc feduc sibs
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | 19.6029198
8 2.45036497

46

Number of obs 
F( 8,
713) 
Prob  F


722
25.81
0.0000

Residual | 107.208996
713 .150363248
------------------------------------------Total | 126.811916
721 .175883378

R-squared

Adj R-squared 
Root MSE


0.1546
0.1451
.38777

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------iq |
.0154368
.0077077
2.00
0.046
.0003044
.0305692
exper |
.0162185
.0040076
4.05
0.000
.0083503
.0240867
tenure |
.0076754
.0030956
2.48
0.013
.0015979
.0137529
educ |
.0161809
.0261982
0.62
0.537
-.035254
.0676158
married |
.1901012
.0467592
4.07
0.000
.0982991
.2819033
south |
-.047992
.0367425
-1.31
0.192
-.1201284
.0241444
urban |
.1869376
.0327986
5.70
0.000
.1225442
.2513311
black |
.0400269
.1138678
0.35
0.725
-.1835294
.2635832
_cons |
4.471616
.468913
9.54
0.000
3.551
5.392231
----------------------------------------------------------------------------Instrumented: iq
Instruments:
exper tenure educ married south urban black meduc feduc sibs
----------------------------------------------------------------------------. ivreg lwage exper tenure educ married south urban black (kww  meduc feduc
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model |
19.820304
8
2.477538
Residual | 106.991612
713 .150058361
------------------------------------------Total | 126.811916
721 .175883378

Number of obs
F( 8,
713)
Prob  F
R-squared
Adj R-squared
Root MSE








722
25.70
0.0000
0.1563
0.1468
.38737

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------kww |
.0249441
.0150576
1.66
0.098
-.0046184
.0545067
exper |
.0068682
.0067471
1.02
0.309
-.0063783
.0201147
tenure |
.0051145
.0037739
1.36
0.176
-.0022947
.0125238
educ |
.0260808
.0255051
1.02
0.307
-.0239933
.0761549
married |
.1605273
.0529759
3.03
0.003
.0565198
.2645347
south |
-.091887
.0322147
-2.85
0.004
-.1551341
-.0286399
urban |
.1484003
.0411598
3.61
0.000
.0675914
.2292093
black | -.0424452
.0893695
-0.47
0.635
-.2179041
.1330137
_cons |
5.217818
.1627592
32.06
0.000
4.898273
5.537362
----------------------------------------------------------------------------Instrumented: kww
Instruments:
exper tenure educ married south urban black meduc feduc sibs
-----------------------------------------------------------------------------

Even though there are 935 men in the sample, only 722 are used for the estimation because
data are missing on meduc and feduc.
The return to education is estimated to be small and insignificant whether IQ or KWW used
is used as the indicator. This could be because family background variables do not satisfy the

47

appropriate redundancy condition, or they might be correlated with a 1 . (In both first-stage
regressions, the F statistic for joint significance of meduc,feduc and sibs have p-values below
.002, so it seems the family background variables have some partial correlation with the ability
indicators.)
5.8. a. Plug in the indicator q 1 for q and the measurement x K for x ∗K , being sure to keep
track of the errors:
y   0   1 x 1 …  K x K   1 q 1  v −  K e K   1 a 1 ,
≡  0   1 x 1 …  K x K   1 q 1  u
where  1  1/ 1 Now, if the variables z 1 , … , z M are redundant in the structural equation (so
they are uncorrelated with v), and uncorrelated with the measurement error e K and the
indicator error a 1 we can use these as IVs for x K and q 1 in 2SLS. We need M ≥ 2 because we
have two explanatory variables, x q and q 1 , that are possibly correlated with the composite
error u.
b. The Stata results are:
. ivreg lwage exper tenure married south urban black (educ iq  kww meduc feduc
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | -.295429993
8 -.036928749
Residual | 127.107346
713 .178271172
------------------------------------------Total | 126.811916
721 .175883378

Number of obs
F( 8,
713)
Prob  F
R-squared
Adj R-squared
Root MSE








722
18.74
0.0000
.42222

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.1646904
.1132659
1.45
0.146
-.0576843
.3870651
iq | -.0102736
.0200124
-0.51
0.608
-.0495638
.0290166
exper |
.0313987
.0122537
2.56
0.011
.007341
.0554564
tenure |
.0070476
.0033717
2.09
0.037
.0004279
.0136672
married |
.2133365
.0535285
3.99
0.000
.1082442
.3184289
south | -.0941667
.0506389
-1.86
0.063
-.1935859
.0052525
urban |
.1680721
.0384337
4.37
0.000
.0926152
.2435289
black | -.2345713
.2247568
-1.04
0.297
-.6758356
.2066929

48

_cons |
4.932962
.4870124
10.13
0.000
3.976812
5.889112
----------------------------------------------------------------------------Instrumented: educ iq
Instruments:
exper tenure married south urban black kww meduc feduc sibs
-----------------------------------------------------------------------------

The estimated return to education is very large, but imprecisely estimated. The 95%
confidence interval is very wide, and easily includes zero. Interestingly, the coefficient on iq is
actually negative, and not statistically different from zero. The large IV estimate of the return
to education and the insignificant ability indicator lend some support to the idea that omitted
ability is less of a problem than schooling measurement error in the standard logwage model
estimated by OLS. But the evidence is not very convincing given the very wide confidence
interval for the educ coefficient.
5.9. Define  4   4 −  3 , so that  4   3   4 . Plugging this expression into the equation
and rearranging gives
2

logwage   0   1 exper   2 exper   3 twoyr  fouryr   4 fouryr  u
2

  0   1 exper   2 exper   3 totcoll   4 fouryr  u,
where totcoll  twoyr  fouryr. Now, just estimate the latter equation by 2SLS using exper,
exper 2 , dist2yr and dist4yr as the full set of instruments. We can use the t statistic on ̂ 4 to
test H 0 :  4  0 against H 1 :  4  0.
5.10. a. For ̂ 1 , the lower right hand element in the general formula (5.24) with x  1, x
and z  1, z is
 2 Covz, x 2 /Varz.
Alternatively, you can derive this formula directly by writing
N ̂ 1 −  1  

N

N

−1

∑z i − z̄x i − x̄ 
i1

49

−1

N

N −1/2 ∑z i − z̄u i
i1

Now,  2zx  Covz, x 2 / 2z  2x , so simple algebra shows that the asymptotic variance is
 2 / 2zx  2x . The asymptotic variance for the OLS estimator is  2 / 2x . Thus, the difference is the
presence of  2zx in the denominator of the IV asymptotic variance.
b. Naturally, as the error variance  2 increases so does the the asymptotic variance of the
IV estimator. More variance in x in the population is better for estimating  1 : as  2x increases
the asymptotic variance decreases. These effects are identical to the findings for OLS. A larger
correlation between z and x reduces the asymptotic variance of the IV estimator. As  zx → 0
the asymptotic variance increases without bound. This illustrates why an instrument that is
only weakly correlated with x can lead to very imprecise IV estimators.
5.11. Following the hint, let y 02 be the linear projection of y 2 on z 2 , let a 2 be the projection
error, and assume that  2 is known. (The results on generated regressors in Section 6.1.1 show
that the argument carries over to the case when  2 is estimated.) Plugging in y 2  y 02  a 2 gives
y 1  z 1  1   1 y 02   1 a 2  u 1 .
Effectively, we regress y 1 on z 1 , y 02 . The key consistency condition is that each explanatory is
orthogonal to the composite error,  1 a 2  u 1 . By assumption, Ez ′1 u 1   0. Further,
Ey 02 a 2   0 by construction. The problem is that, in general, Ez ′1 a 2  ≠ 0 because z 1 was not
included in the linear projection for y 2 . Therefore, OLS will be inconsistent for all parameters
in general. Contrast this conclusion with 2SLS when y ∗2 is the projection on z 1 and z 2 :
y 2  y ∗2  r 2  z 2  r 2
′

Ez r 2   0
The second step regression (assuming that  2 is known) is
y 1  z 1  1   1 y ∗2   1 r 2  u 1 .

50

By construction r 2 is uncorrelated with z, and so Ez ′1 r 2   0 and Ey ∗2 r 2   0.
The lesson is that one must be very careful if manually carrying out 2SLS by explicitly
doing the first- and second- stage regressions: all exogenous variables must be included in the
first stage.
5.12. This problem is essentially proven by the hint. Given the description of , the only
way the K columns of  can be linearly dependent is if the last column can be written as a
linear combination of the first K − 1 columns. This is true if and only if each  j is zero. Thus,
if at least one  j is different from zero, rank  K.
5.13. a. In a simple regression model with a single IV, the IV estimate of the slope can be
written as
̂ 1 

N

N

∑z i − z̄y i − ȳ 

∑z i − z̄x i − x̄ 

i1
N



i1

N

∑ z i y i − ȳ 

∑ z i x i − x̄ 

i1

.

i1

Now the numerator can be written as
N
N
N
N
z i y i − ȳ   ∑ i1 z i y i − ∑ i1 z i ȳ  N 1 ȳ 1 − N 1 ȳ  N 1 ȳ 1 − ȳ  where N 1  ∑ i1 z i is
∑ i1

the number of observations in the sample with z i  1 and ȳ 1 is the average of the y i over the
observations with z i  1. Next, write ȳ as a weighted average:
ȳ  N 0 /Nȳ 0  N 1 /Nȳ
where the notation should be clear. Straightforward algebra shows that
ȳ 1 − ȳ  N − N 1 /Nȳ − N 0 /Nȳ 0
 N 0 /Nȳ 1 − ȳ 0 .
Therefore, the numerator of the IV estimate is N 0 N 1 /Nȳ 1 − ȳ 0 . The same argument shows

51

that the denominator is N 0 N 1 /Nx̄ 1 − x 0 . Taking the ratio proves the result.
b. If x is also binary – representing some “treatment” – x̄ 1 is the fraction of observations
receiving treatment when z i  1 and x̄ 0 is the fraction receiving treatment when z i  0.
Suppose x i  1 if person i participates in a job training program, and let z i  1 if person i is
eligible for participation in the program. Then x̄ 1 is the fraction of people participating in the
program out of those made eligible, and x̄ 0 is the fraction of people participating who are not
eligible. (When eligibility is necessary for participation, x̄ 0  0.) Generally, x̄ 1 − x̄ 0 is the
difference in participation rates when z  1 and z  0. So the difference in the mean response
between the z  1 and z  0 groups gets divided by the difference in participation rates across
the two groups.
5.14. a. Taking the linear projection of (5.1) under the assumption that
x 1 , … , x K−1 , z i , … , z M  are uncorrelated with u gives
Ly|z   0   1 x 1 …  K−1 x K−1   K Lx K |z  Lu|z
  0   1 x 1 …  K−1 x K−1   K x ∗K
because Lu|z  0.
b. By the law of iterated projections,
Ly|1, x 1 , . . . , x K−1 , x ∗K    0   1 x 1 …  K−1 x K−1   K x ∗K .
Consistency of OLS for the  j from the regression y on 1, x 1 , . . . , x K−1 , x ∗K follows immediately
from our treatment of OLS from Chapter 4: OLS consistently estimates the parameters in a
linear projection provided there is not perfect collinearity in 1, x 1 , . . . , x K−1 , x ∗K .
c. I should have said explicitly to assume Ez ′ z is nonsingular – that is, 2SLS.2a holds.
Then, x ∗K is not a perfect linear combination of x 1 , . . . , x K−1  if and only if at least one element

52

of z 1 , … , z M has nonzero coefficient in Lx K |1, x 1 , . . . x K−1 , z 1 , . . . , z M . In the model with a
single endogenous explanatory variable, we know this condition is equivalent to Assumption
2SLS.2b, the standard rank condition.
5.15. In Lx|z  z we can write


 11

0

 12 I K 2

,

where I K 2 is the K 2  K 2 identity matrix, 0 is the L 1  K 2 zero matrix,  11 is L 1  K 1 , and  12
is K 2  K 1 . As in Problem 5.12, the rank condition holds if and only if rank   K.
a. If for some x j , the vector z 1 does not appear in Lx j |z, then  11 has a column which is
entirely zeros. Then that column of  can be written as a linear combination of the last K 2
columns of  – because any K 2  1 vector in  12 can be written as a linear combination of the
columns of I K 2 . This implies rank  K. Therefore, a necessary condition for the rank
condition is that no columns of  11 be exactly zero, which means that at least one z h must
appear in the reduced form of each x j , j  1, … , K 1 .
b. Suppose K 1  2 and L 1  2, where z 1 appears in the reduced form from both x 1 and x 2 ,
but z 2 appears in neither reduced form. Then the 2  2 matrix  11 has zeros in its second row,
which means that the second row of  is all zeros. In that case, it cannot have rank K.
Intuitively, while we began with two instruments, only one of them turned out to be partially
correlated with x 1 and x 2 .
c. Without loss of generality, assume that z j appears in the reduced form for x j ; we can
simply reorder the elements of z 1 to ensure this is the case. Then  11 is a K 1  K 1 diagonal
matrix with nonzero diagonal elements. Looking at

53



 11

0

 12 I K 2

we see that if  11 is diagonal with all nonzero diagonals then  is lower triangular with all
diagonal elements nonzero. Therefore, rank   K.
5.16. a. The discussion below equation (5.24) implies directly that
Avar N ̃ −    2u /Varw ∗ 
because there are no other explanatory variables – exogenous or endogenous – in the equation.
Remember, the expression
 2u Ex ∗′ x ∗  −1
has the same form as that for OLS but with x ∗ replacing x. So any algebra derived for OLS can
be applied to 2SLS.
b. We can write
v  u − h,
so if Eg ′ u  0 and Eg ′ h  0 then
Eg ′ v  Eg ′ u − Eg ′ h  0.
c. For the hint here to be entirely correct, I should have stated that Ew  0. As we will
see, when w has a nonzero mean, r̆ differs from w ∗ by an additive constant [which, of course,
implies Varr̆  Varw ∗ ].
Again using the discussion following equation (5.24),
Avar N ̂ −    2v /Varr̆,
̆ on 1, h, and w
̆ are the
where  2v  Varv, r̆ is the population residual from the regression w

54

population fitted values from the linear projection of w on g, h.
Because Eg ′ h  0, we can write
̆  g 1  h 2
w
where
 1  Eg ′ g −1 Eg ′ w
 2  Eh ′ h −1 Eh ′ w.
Note that
w ∗  Lw|g  g 1 .
Next,
̆ |1, h  Lg 1  h 2 |1, h  Lg|1, h 1  h 2
Lw
 Lg|1 1  h 2
because Eh  0 and Eg ′ h  0 are assumed. Now Lg|1  Eg, and so
̆ |1, h   1  h 2
Lw
where  1   g  1 . Therefore,
̆ − Lw
̆ |1, h  g 1  h 2  −  1  h 2   − 1  g 1
r̆  w
 − 1  w ∗
It follows that Varr̆  Varw ∗  and so we have shown
Avar N ̂ −    2v /Varw ∗ ,
d. Because Eh ′ v  0 by definition, we have
 2u  Varh   2u   ′  h    2v ≥  2v ,
with strict inequality if  h is positive definite and  ≠ 0 (and even in some cases where

55

 h ≡ Varh is not positive definite). This means that, asymptotically, we generally get a
smaller asymptotic variance for estimate  by including exogenous variables that are
uncorrelated with the instruments g:
 2u
 2v
−
Varw ∗ 
Varw ∗ 
′h

≥ 0.
Varw ∗ 

Avar N ̃ −  − Avar N ̂ −  

56

Solutions to Chapter 6 Problems
6.1. a. Here is abbreviated Stata output for testing the null hypothesis that educ is
exogenous:
. use card
. qui reg educ nearc4 nearc2 exper expersq black south smsa reg661-reg668
smsa66
. predict v2hat, resid
. reg lwage educ exper expersq black south smsa reg661-reg668 smsa66 v2hat
Source |
SS
df
MS
------------------------------------------Model | 178.100803
16 11.1313002
Residual | 414.540842 2993 .138503455
------------------------------------------Total | 592.641645 3009 .196956346

Number of obs
F( 16, 2993)
Prob  F
R-squared
Adj R-squared
Root MSE








3010
80.37
0.0000
0.3005
0.2968
.37216

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.1570594
.0482814
3.25
0.001
.0623912
.2517275
exper |
.1188149
.0209423
5.67
0.000
.0777521
.1598776
expersq | -.0023565
.0003191
-7.38
0.000
-.0029822
-.0017308
black | -.1232778
.0478882
-2.57
0.010
-.2171749
-.0293806
south | -.1431945
.0261202
-5.48
0.000
-.1944098
-.0919791
smsa |
.100753
.0289435
3.48
0.001
.0440018
.1575042
reg661 |
-.102976
.0398738
-2.58
0.010
-.1811588
-.0247932
reg662 | -.0002286
.0310325
-0.01
0.994
-.0610759
.0606186
reg663 |
.0469556
.0299809
1.57
0.117
-.0118296
.1057408
reg664 | -.0554084
.0359807
-1.54
0.124
-.1259578
.0151411
reg665 |
.0515041
.0436804
1.18
0.238
-.0341426
.1371509
reg666 |
.0699968
.0489487
1.43
0.153
-.0259797
.1659734
reg667 |
.0390596
.0456842
0.85
0.393
-.050516
.1286352
reg668 | -.1980371
.0482417
-4.11
0.000
-.2926273
-.1034468
smsa66 |
.0150626
.0205106
0.73
0.463
-.0251538
.0552789
v2hat | -.0828005
.0484086
-1.71
0.087
-.177718
.0121169
_cons |
3.339687
.821434
4.07
0.000
1.729054
4.950319
-----------------------------------------------------------------------------

The t statistic on v̂ 2 is −1. 71, which is not significant at the 5% level against a two-sided
alternative. The negative correlation between u 1 and educ is essentially the same finding that
the 2SLS estimated return to education is larger than the OLS estimate. In any case, I would
call this marginal evidence that educ is endogenous. The quandary is that the OLS and 2SLS

57

point estimates are quite different.
b. To test the single over indentifying restriction we obtain the 2SLS residuals:
. qui reg lwage educ exper expersq black south smsa reg661-reg668 smsa66
(nearc4 nearc2 exper expersq black south smsa reg661-reg668 smsa66)
. predict uhat1, resid
. qui reg u1hat exper expersq black south smsa reg661-reg668 smsa66 nearc4
nearc2
. di e(r2)
.00041467
. di 3010*e(r2)
1.2481535
. di chiprob(1,3010*e(r2))
.26390545

The test statistic is the sample size times the R-squared from this regression, or about 1. 25.
The p-value, obtained from  21 distribution, is about .264, so the instruments pass the over
identification test.
6.2. We first obtain the reduced form residuals, v̂ 21 and v̂ 22 , for educ and IQ, respectively.
The regression output is suppressed:
. qui reg educ exper tenure married south urban black kww meduc feduc sibs
. predict v21hat, resid
(213 missing values generated)
. qui reg iq exper tenure married south urban black kww meduc feduc sibs
. predict v22hat, resid
(213 missing values generated)
. qui reg lwage exper tenure married south urban black educ iq v21hat v22hat
. test v21hat v22hat
( 1)
( 2)

v21hat  0
v22hat  0
F(

2,
711) 
Prob  F 

4.20
0.0153

The p-value of the joint F test, which is justified asymptotically, is . 0153. Therefore, the

58

test finds fairly strong evidence for endogeneity of at least one of educ and IQ, although this
conclusion relies on the instruments being truly exogenous. If you look back at Problem 5.8,
this IV solution did not seem to work very well. So we still do not know what should be treated
as exogenous in this method.
6.3. a. We need prices to satisfy two requirements. First, calories and protein must be
partially correlated with prices of food. While this is easy to test separately by estimating the
two reduced forms, the rank condition could still be violated. (Problem 5.15c contains a
sufficient condition for the rank condition to hold.) In addition, we must also assume prices are
exogenous in the productivity equation. Ideally, prices vary because of things like
transportation costs that are not systematically related to regional variations in individual
productivity. A potential problem is that prices reflect food quality and that features of the
food other than calories and protein appear in the disturbance u 1 .
b. Since there are two endogenous explanatory variables we need at least two prices.
c. We would first estimate the two reduced forms for calories and protein by regressing
each on a constant, exper, exper 2 , educ, and the M prices, p 1 , ..., p M We obtain the residuals,
v̂ 21 and v̂ 22 . Then we would run the regression logproduc on 1,exper, exper 2 , educ, v̂ 21 , v̂ 22
and do a joint significance test on v̂ 21 and v̂ 22 . We could use a standard F test or use a
heteroskedasticity-robust test.
6.4.a. Since y  x  q  v it follows that
Ey|x  x  Eq|x  Ev|x  x  x  x   ≡ x.
Since Ey|x is linear in x there is no functional form misspecification in this conditional
expectation. Therefore, no functional form test will detect correlation between q and x, no
matter how strong it is:  can be anything.
59

b. Since Ev|x, q  0, Varv|x, q  Ev 2 |x, q   2v  Ev 2 |x  Varv|x. Therefore,
Vary|x  Varq  v|x  Varq|x  Varv|x  2Eqv|x, where we use
Covq, v|x  Eqv|x because Ev|x  0. Now
Eqv|x  EEqv|x, q|x  EqEv|x, q|x  Eq  0|x  0.
Therefore, Vary|x  Varq|x  Varv|x   2q   2v , so that y is conditionally
homoskedastic. But if Ey|x  x and Vary|x is constant, a test for heteroskedasticity will
always have a limiting chi-square distribution. It will have no power for detecting omitted
variables.
c. Since Eu 2 |x  Varu|x  Eu|x 2 and Varu|x is constant, Eu 2 |x is constant if and
only if Eu|x 2 is constant. If Eu|x ≠ Eu then Eu|x is not constant, so Eu|x 2
generally will be a function of x. So Eu 2 |x depends on x, which means that u 2 can be
correlated with functions of x, say hx. It follows that regression tests of the form (6.36) can
be expected, at least in some cases, to detect “heteroskedasticity”. (If the goal is to determine
when heteroskedasticity-robust inference is called for, the regression-based tests do the right
thing.)
6.5. a. For simplicity, absorb the intercept in x, so y  x  u, Eu|x  0, Varu|x   2 .
In these tests, ̂ 2 is implicitly SSR/N – there is no degrees of freedom adjustment. (In any case,
the df adjustment makes no difference asymptotically.) So û 2i − ̂ 2 has a zero sample average,
which means that
N

N

−1/2

∑h i −  h 
i1

′

N

û 2i

− ̂ 2   N −1/2 ∑ h ′i û 2i − ̂ 2 .
i1

′
N
Next, N −1/2 ∑ i1 h i −  h   O p 1 by the central limit theorem and ̂ 2 −  2  o p 1. So

60

′

N

N −1/2 ∑ i1 h i −  h  ̂ 2 −  2   O p 1  o p 1  o p 1. Therefore, so far we have
N

N

−1/2

∑

N

h ′i û 2i

− ̂   N
2

−1/2

i1

∑h i −  h  ′ û 2i − ̂ 2   o p 1.
i1

We are done with this part if we show
N

N

N −1/2 ∑ i1 h i −  h  ′ û 2i  N −1/2 ∑ i1 h i −  h  ′ û 2i  o p 1. Now, as in Problem 4.4, we can
write û 2i  u 2i − 2u i x i ̂ −   x i ̂ −  2 , so
N

N

N −1/2 ∑h i −  h  ′ û 2i  N −1/2 ∑h i −  h  ′ û 2i
i1

(6.62)

i1

N

−2 N

−1/2

∑h i −  h  ′ x i

̂ − 

i1

N



′
N −1/2 ∑h i −  h  x i ⊗ x i  vec̂ − ̂ −  ′ ,

i1

where the expression for the third term follows from
x i ̂ −  2  x i ̂ − ̂ −  ′ x ′i  x i ⊗ x i   vec̂ − ̂ −  ′ . Dropping the “−2” the
N

second term can be written as N −1 ∑ i1 u i h i −  h  ′ x i

N ̂ −   o p 1  O p 1 because

N ̂ −   O p 1 and, under Eu i |x i   0, Eu i h i −  h  ′ x i   0; the law-of-large-numbers
implies that the sample average is o p 1. The third term can be written as
N
N −1/2 N −1 ∑ i1 h i −  h  ′ x i ⊗ x i  vec N ̂ −  N ̂ −  ′   N −1/2  O p 1  O p 1,

where we again use the fact that sample averages are O p 1 by the law of large numbers and
vec N ̂ −  N ̂ −  ′   O p 1. We have shown that the last two terms in (6.62) are
o p 1, which proves part a.
N

b. By part a, the asymptotic variance of N −1/2 ∑ i1 h ′i û 2i −  2  is
Varh i −  h  ′ u 2i −  2  Eu 2i −  2  2 h i −  h  ′ h i −  h . Now

61

u 2i −  2  2  u 4i − 2u 2i  2   4 . Under the null, Eu 2i |x i   Varu i |x i    2 [since Eu i |x i   0
is assumed] and therefore, when we add (6.37), Eu 2i −  2  2 |x i    2 −  4 ≡  2 . A standard
iterated expectations argument gives
Eu 2i −  2  2 h i −  h  ′ h i −  h   EEu 2i −  2  2 h i −  h  ′ h i −  h |x i   EEu 2i −  2  2 |
which is what we wanted to show. (Whether we carry out the calculation for a random draw i
or for random variables representing the population is a matter of taste.)
c. From part b and Lemma 3.8, the following statistic has an asymptotic  2Q distribution:
N

N

−1/2

N

∑

û 2i

i1

− ̂ 2 h i  2 Eh i −  h  ′ h i −  h  −1 N −1/2 ∑ h ′i û 2i − ̂ 2  .
i1

N
Using again the fact that ∑ i1 û 2i − ̂ 2   0, we can replace h i with h i − h̄ in the two vectors

forming the quadratic form. Then, again by Lemma 3.8, we can replace the matrix in the
quadratic form with a consistent estimator, which is
N

̂2



N

−1

∑h i − h̄  ′ h i − h̄ 

,

i1

N

where ̂ 2  N −1 ∑ i1 û 2i − ̂ 2  2 . The computable statistic, after simple algebra, can be written
as
N

̂ −2



∑

û 2i

−  h i − h̄ 
̂2

i1

N

∑h i − h̄  h i − h̄ 
′

i1

−1

N

∑h i − h̄  û 2i − ̂ 2 
′

.

i1

Now ̂ 2 is just the total sum of squares of the û 2i divided by N. The numerator of the statistic is
simply the explained sum of squares from the regression û 2i on 1, h i , i  1, … , N. Therefore,
the test statistic is N times the usual (centered) R-squared from the regression û 2i on
1, h 1 , i  1, … , N, or NR 2c .

62

′

d. Without assumption (6.37) we need to estimate Eu 2i −  2  2 h i −  h  h i −  h 
generally. Hopefully, the approach is by now pretty clear. We replace the population
expected value with the sample average and replace any unknown parameters – ,  2 , and  h
in this case – with their consistent estimators (under H 0 ). So a generally consistent estimator
N

′

of Avar N −1/2 ∑ i1 h i û 2i − ̂ 2  is
N

N

−1

∑û 2i − ̂ 2  2 h i − h̄  h i − h̄ ,
′

i1

and the test statistic robust to heterokurtosis can be written as
N

∑

û 2i

−  h i − h̄ 
̂2

i1

−1

N

∑

û 2i

−   h i − h̄  h i − h̄ 
̂2 2

′

i1

N



∑h i − h̄  û 2i − ̂ 2 
′

,

i1

which is easily seen to be the explained sum of squares from the regression of 1 on
û 2i − ̂ 2 h i − h̄ , i  1, … , N (without an intercept). Since the total sum of squares, without
demeaning, of unity is simply N, the statistic is equivalent to N − SSR 0 , where SSR 0 is the sum
of squared residuals.
6.6. Here is my Stata session using the data NLS80.RAW:
. qui reg lwage exper tenure married south urban black educ
. predict lwageh
(option xb assumed; fitted values)
. gen lwagehsq  lwageh^2
. predict uhat, resid
. gen uhatsq  uhat^2
. reg uhatsq lwageh lwagehsq
Source |

SS

df

MS

63

Number of obs 

935

------------------------------------------Model | .288948987
2 .144474493
Residual | 55.3447136
932
.05938274
------------------------------------------Total | 55.6336626
934 .059564949

F( 2,
932)
Prob  F
R-squared
Adj R-squared
Root MSE







2.43
0.0883
0.0052
0.0031
.24369

----------------------------------------------------------------------------uhatsq |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lwageh |
3.027285
1.880375
1.61
0.108
-.6629745
6.717544
lwagehsq | -.2280088
.1390444
-1.64
0.101
-.5008853
.0448677
_cons | -9.901227
6.353656
-1.56
0.119
-22.37036
2.567902
-----------------------------------------------------------------------------

An asymptotically valid test for heteroskedasticity is just the F statistic for joint
significance of ŷ and ŷ 2 , and this yields p − value . 088 (although this version maitains
Assumption (6.37) under the null, along with homoskedasticity). Thus, there is only modest
evidence of heteroskedasticity. It could be ignored or heteroskedasticity-robust standard errors
and test statistics can be used.
6.7. a. The simple regression results are:
. use hprice
. reg lprice ldist if y81
Source |
SS
df
MS
------------------------------------------Model | 3.86426989
1 3.86426989
Residual | 17.5730845
140 .125522032
------------------------------------------Total | 21.4373543
141 .152037974

Number of obs
F( 1,
140)
Prob  F
R-squared
Adj R-squared
Root MSE








142
30.79
0.0000
0.1803
0.1744
.35429

----------------------------------------------------------------------------lprice |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------ldist |
.3648752
.0657613
5.55
0.000
.2348615
.4948889
_cons |
8.047158
.6462419
12.45
0.000
6.769503
9.324813
-----------------------------------------------------------------------------

This regression suggests a strong link between housing price and distance from the
incinerator (as distance increases, so does housing price). The elasticity is . 365 and the t
statistic is 5. 55. However, this is not a good causal regression: the incinerator may have been
put near homes with lower values to begin with. If so, we would expect the positive

64

relationship found in the simple regression even if the new incinerator had no effect on
housing prices.
b. The parameter  3 should be positive: after the incinerator is built a house should be
worth relatively more the farther it is from the incinerator. Here is the Stata session:
. gen y81ldist  y81*ldist
. reg lprice y81 ldist y81ldist
Source |
SS
df
MS
------------------------------------------Model | 24.3172548
3 8.10575159
Residual | 37.1217306
317 .117103251
------------------------------------------Total | 61.4389853
320 .191996829

Number of obs
F( 3,
317)
Prob  F
R-squared
Adj R-squared
Root MSE








321
69.22
0.0000
0.3958
0.3901
.3422

----------------------------------------------------------------------------lprice |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y81 | -.0113101
.8050622
-0.01
0.989
-1.59525
1.57263
ldist |
.316689
.0515323
6.15
0.000
.2153005
.4180775
y81ldist |
.0481862
.0817929
0.59
0.556
-.1127394
.2091117
_cons |
8.058468
.5084358
15.85
0.000
7.058133
9.058803
-----------------------------------------------------------------------------

The coefficient on ldist reveals the shortcoming of the regression in part a. This coefficient
measures the relationship between lprice and ldist in 1978, before the incinerator was even
being rumored. The effect of the incinerator is given by the coefficient on the interaction,
y81ldist. While the direction of the effect is as expected, it is not especially large, and it is
statistically insignificant, anyway. Therefore, at this point, we cannot reject the null hypothesis
that building the incinerator had no effect on housing prices.
c. Adding the variables listed in the problem gives
. reg lprice y81 ldist y81ldist lintst lintstsq larea lland age agesq rooms
baths
Source |
SS
df
MS
------------------------------------------Model | 48.7611143
11 4.43282858
Residual |
12.677871
309 .041028709
------------------------------------------Total | 61.4389853
320 .191996829

65

Number of obs
F( 11,
309)
Prob  F
R-squared
Adj R-squared
Root MSE








321
108.04
0.0000
0.7937
0.7863
.20256

----------------------------------------------------------------------------lprice |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y81 |
-.229847
.4877198
-0.47
0.638
-1.189519
.7298249
ldist |
.0866424
.0517205
1.68
0.095
-.0151265
.1884113
y81ldist |
.0617759
.0495705
1.25
0.214
-.0357625
.1593143
lintst |
.9633332
.3262647
2.95
0.003
.3213517
1.605315
lintstsq | -.0591504
.0187723
-3.15
0.002
-.096088
-.0222128
larea |
.3548562
.0512328
6.93
0.000
.2540468
.4556655
lland |
.109999
.0248165
4.43
0.000
.0611683
.1588297
age | -.0073939
.0014108
-5.24
0.000
-.0101699
-.0046178
agesq |
.0000315
8.69e-06
3.63
0.000
.0000144
.0000486
rooms |
.0469214
.0171015
2.74
0.006
.0132713
.0805715
baths |
.0958867
.027479
3.49
0.001
.041817
.1499564
_cons |
2.305525
1.774032
1.30
0.195
-1.185185
5.796236
-----------------------------------------------------------------------------

The incinerator effect is now larger (the elasticity is about . 062) and the t statistic is larger,
but the p-value for the interaction term is still fairly large, . 214. Against a one-sided
alternative, the p-value is . 107, so it is almost significant at the 10% level. Still, using these
two years of data and controlling for the listed factors, the evidence that housing prices were
adversely affected by the new incinerator is somewhat weak.
6.8. a. The following is my Stata session:
. use fertil1
. gen agesq  age^2
. reg kids educ age agesq black east northcen west farm othrural town smcity
y74-y84
Source |
SS
df
MS
------------------------------------------Model | 399.610888
17 23.5065228
Residual | 2685.89841 1111 2.41755033
------------------------------------------Total |
3085.5093 1128 2.73538059

Number of obs
F( 17, 1111)
Prob  F
R-squared
Adj R-squared
Root MSE








1129
9.72
0.0000
0.1295
0.1162
1.5548

----------------------------------------------------------------------------kids |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ | -.1284268
.0183486
-7.00
0.000
-.1644286
-.092425
age |
.5321346
.1383863
3.85
0.000
.2606065
.8036626
agesq |
-.005804
.0015643
-3.71
0.000
-.0088733
-.0027347
black |
1.075658
.1735356
6.20
0.000
.7351631
1.416152
east |
.217324
.1327878
1.64
0.102
-.0432192
.4778672
northcen |
.363114
.1208969
3.00
0.003
.125902
.6003261
west |
.1976032
.1669134
1.18
0.237
-.1298978
.5251041
farm | -.0525575
.14719
-0.36
0.721
-.3413592
.2362443

66

othrural | -.1628537
.175442
-0.93
0.353
-.5070887
.1813814
town |
.0843532
.124531
0.68
0.498
-.1599893
.3286957
smcity |
.2118791
.160296
1.32
0.187
-.1026379
.5263961
y74 |
.2681825
.172716
1.55
0.121
-.0707039
.6070689
y76 | -.0973795
.1790456
-0.54
0.587
-.448685
.2539261
y78 | -.0686665
.1816837
-0.38
0.706
-.4251483
.2878154
y80 | -.0713053
.1827707
-0.39
0.697
-.42992
.2873093
y82 | -.5224842
.1724361
-3.03
0.003
-.8608214
-.184147
y84 | -.5451661
.1745162
-3.12
0.002
-.8875846
-.2027477
_cons | -7.742457
3.051767
-2.54
0.011
-13.73033
-1.754579
-----------------------------------------------------------------------------

The estimate says that a women with about eight more years of education has about one
fewer child (gotten from . 1288  1. 024), other factors fixed. The coefficient is very
statistically significant. Also, there has been a notable secular decline in fertility over this
period: on average, with other factors held fixed, a women in 1984 had about half a child less
. 545 than a similar woman in 1972, the base year. The effect is also statistically significant
with p-value  . 002.
b. Estimating the reduced form for educ gives
. reg educ age agesq black east northcen west farm othrural town smcity
y74-y84 meduc feduc
Source |
SS
df
MS
------------------------------------------Model | 2256.26171
18 125.347873
Residual | 5606.85432 1110 5.05122011
------------------------------------------Total | 7863.11603 1128 6.97084755

Number of obs
F( 18, 1110)
Prob  F
R-squared
Adj R-squared
Root MSE








1129
24.82
0.0000
0.2869
0.2754
2.2475

----------------------------------------------------------------------------educ |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------age | -.2243687
.2000013
-1.12
0.262
-.616792
.1680546
agesq |
.0025664
.0022605
1.14
0.256
-.001869
.0070018
black |
.3667819
.2522869
1.45
0.146
-.1282311
.861795
east |
.2488042
.1920135
1.30
0.195
-.1279462
.6255546
northcen |
.0913945
.1757744
0.52
0.603
-.2534931
.4362821
west |
.1010676
.2422408
0.42
0.677
-.3742339
.5763691
farm | -.3792615
.2143864
-1.77
0.077
-.7999099
.0413869
othrural |
-.560814
.2551196
-2.20
0.028
-1.061385
-.060243
town |
.0616337
.1807832
0.34
0.733
-.2930816
.416349
smcity |
.0806634
.2317387
0.35
0.728
-.3740319
.5353588
y74 |
.0060993
.249827
0.02
0.981
-.4840872
.4962858
y76 |
.1239104
.2587922
0.48
0.632
-.3838667
.6316874
y78 |
.2077861
.2627738
0.79
0.429
-.3078033
.7233755
y80 |
.3828911
.2642433
1.45
0.148
-.1355816
.9013638
y82 |
.5820401
.2492372
2.34
0.020
.0930108
1.071069

67

y84 |
.4250429
.2529006
1.68
0.093
-.0711741
.92126
meduc |
.1723015
.0221964
7.76
0.000
.1287499
.2158531
feduc |
.2074188
.0254604
8.15
0.000
.1574629
.2573747
_cons |
13.63334
4.396773
3.10
0.002
5.006421
22.26027
----------------------------------------------------------------------------. test meduc feduc
( 1)
( 2)

meduc  0
feduc  0
F(

2, 1110) 
Prob  F 

155.79
0.0000

The joint F test shows that educ is significantly partially correlated with meduc and feduc;
the t statistics also show this clearly. If we make the test robust to heteroskedasticity of
unknown form, the F statistic drops to 131. 37 but the p-value is still zero to four decimal
places.
To test the null that educ is exogenous, we need to reduced form residuals and then include
them in the OLS regression. I suppress the output here:
. predict v2hat, resid
. reg kids educ age agesq black east northcen west farm othrural town smcity
y74-y84 v2hat
Source |
SS
df
MS
------------------------------------------Model | 400.802376
18 22.2667987
Residual | 2684.70692 1110 2.41865489
------------------------------------------Total |
3085.5093 1128 2.73538059

Number of obs
F( 18, 1110)
Prob  F
R-squared
Adj R-squared
Root MSE








1129
9.21
0.0000
0.1299
0.1158
1.5552

----------------------------------------------------------------------------kids |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ | -.1527395
.0392012
-3.90
0.000
-.2296562
-.0758227
age |
.5235536
.1389568
3.77
0.000
.2509059
.7962013
agesq |
-.005716
.0015697
-3.64
0.000
-.0087959
-.0026362
black |
1.072952
.173618
6.18
0.000
.7322958
1.413609
east |
.2285554
.1337787
1.71
0.088
-.0339322
.491043
northcen |
.3744188
.1219925
3.07
0.002
.1350569
.6137807
west |
.2076398
.1675628
1.24
0.216
-.1211357
.5364153
farm | -.0770015
.1512869
-0.51
0.611
-.373842
.2198389
othrural | -.1952451
.1814491
-1.08
0.282
-.5512671
.1607769
town |
.08181
.1246122
0.66
0.512
-.162692
.3263119
smcity |
.2124996
.160335
1.33
0.185
-.1020943
.5270936
y74 |
.2721292
.172847
1.57
0.116
-.0670145
.6112729
y76 | -.0945483
.1791319
-0.53
0.598
-.4460236
.2569269

68

y78 | -.0572543
.1824512
-0.31
0.754
-.4152424
.3007337
y80 |
-.053248
.1846139
-0.29
0.773
-.4154795
.3089836
y82 | -.4962149
.1764897
-2.81
0.005
-.842506
-.1499238
y84 | -.5213604
.1778207
-2.93
0.003
-.8702631
-.1724578
v2hat |
.0311374
.0443634
0.70
0.483
-.0559081
.1181829
_cons | -7.241244
3.134883
-2.31
0.021
-13.39221
-1.09028
-----------------------------------------------------------------------------

The t statistic on v2hat is . 702, so there is little evidence that educ is endogenous in the
equation. Still, we can see if 2SLS produces very different estimates:
. ivreg kids age agesq black east northcen west farm othrural town smcity
y74-y84 (educ  meduc feduc)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model |
395.36632
17 23.2568424
Residual | 2690.14298 1111 2.42137082
------------------------------------------Total |
3085.5093 1128 2.73538059

Number of obs
F( 17, 1111)
Prob  F
R-squared
Adj R-squared
Root MSE








1129
7.72
0.0000
0.1281
0.1148
1.5561

----------------------------------------------------------------------------kids |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ | -.1527395
.0392232
-3.89
0.000
-.2296993
-.0757796
age |
.5235536
.1390348
3.77
0.000
.2507532
.796354
agesq |
-.005716
.0015705
-3.64
0.000
-.0087976
-.0026345
black |
1.072952
.1737155
6.18
0.000
.732105
1.4138
east |
.2285554
.1338537
1.71
0.088
-.0340792
.4911901
northcen |
.3744188
.122061
3.07
0.002
.1349228
.6139148
west |
.2076398
.1676568
1.24
0.216
-.1213199
.5365995
farm | -.0770015
.1513718
-0.51
0.611
-.3740083
.2200053
othrural | -.1952451
.181551
-1.08
0.282
-.5514666
.1609764
town |
.08181
.1246821
0.66
0.512
-.162829
.3264489
smcity |
.2124996
.160425
1.32
0.186
-.1022706
.5272698
y74 |
.2721292
.172944
1.57
0.116
-.0672045
.6114629
y76 | -.0945483
.1792324
-0.53
0.598
-.4462205
.2571239
y78 | -.0572543
.1825536
-0.31
0.754
-.415443
.3009343
y80 |
-.053248
.1847175
-0.29
0.773
-.4156825
.3091865
y82 | -.4962149
.1765888
-2.81
0.005
-.8427
-.1497297
y84 | -.5213604
.1779205
-2.93
0.003
-.8704586
-.1722623
_cons | -7.241244
3.136642
-2.31
0.021
-13.39565
-1.086834
----------------------------------------------------------------------------Instrumented: educ
Instruments:
age agesq black east northcen west farm othrural town smcity
y74 y76 y78 y80 y82 y84 meduc feduc
-----------------------------------------------------------------------------

The estimated coefficient on educ is larger in magnitude than before, but the test for
endogeneity shows that we can reasonably attribute the difference between OLS and 2SLS to
sampling error.
69

c. Since there is little evidence that educ is endogenous, we could just use OLS. I did it
both ways. First, I just added interactions y74  educ, y76  educ, … , y84  educ to the model in
part a and used OLS. Some of the interactions, particularly in the last two years, are
marginally significant and negative, showing that the effect of education has become stronger
over time. But the joint F test for the interaction terms yields p − value . 180, and so we do
not reject the model without the interactions. Still, the possibility that the link between fertility
and education has become stronger over time is deserves attention, especially using more
recent data.
To estimate the full model by 2SLS, I obtained instruments by interacting all year dummies
with both meduc and feduc. The Stata command is then
. ivreg kids age agesq black east northcen west farm othrural town smcity y74
(educ y74educ-y84educ  meduc feduc y74meduc-y84feduc )
. test y74educ y76educ y78educ y80educ y82educ y84educ

Qualitatively, the results are similar to the OLS estimates. The p − value for the joint F test
on the interactions is . 205 – again, this has asymptotic justification under Assumption 2SLS.3,
the homoskedasticity assumption – so again there is no strong evidence favoring including of
the interactions of year dummies and education.
6.9. a. The Stata results are
. use injury
. reg ldurat afchnge highearn afhigh male married head-construc if ky
Source |
SS
df
MS
------------------------------------------Model | 358.441793
14 25.6029852
Residual | 8341.41206 5334 1.56381928
------------------------------------------Total | 8699.85385 5348 1.62674904

Number of obs
F( 14, 5334)
Prob  F
R-squared
Adj R-squared
Root MSE








5349
16.37
0.0000
0.0412
0.0387
1.2505

----------------------------------------------------------------------------ldurat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------afchnge |
.0106274
.0449167
0.24
0.813
-.0774276
.0986824

70

highearn |
.1757598
.0517462
3.40
0.001
.0743161
.2772035
afhigh |
.2308768
.0695248
3.32
0.001
.0945798
.3671738
male | -.0979407
.0445498
-2.20
0.028
-.1852766
-.0106049
married |
.1220995
.0391228
3.12
0.002
.0454027
.1987962
head | -.5139003
.1292776
-3.98
0.000
-.7673372
-.2604634
neck |
.2699126
.1614899
1.67
0.095
-.0466737
.5864988
upextr |
-.178539
.1011794
-1.76
0.078
-.376892
.0198141
trunk |
.1264514
.1090163
1.16
0.246
-.0872651
.340168
lowback | -.0085967
.1015267
-0.08
0.933
-.2076305
.1904371
lowextr | -.1202911
.1023262
-1.18
0.240
-.3208922
.0803101
occdis |
.2727118
.210769
1.29
0.196
-.1404816
.6859052
manuf | -.1606709
.0409038
-3.93
0.000
-.2408591
-.0804827
construc |
.1101967
.0518063
2.13
0.033
.0086352
.2117581
_cons |
1.245922
.1061677
11.74
0.000
1.03779
1.454054
-----------------------------------------------------------------------------

The estimated coefficient on the interaction term is actually higher now – . 231 – than in
equation (6.54), and it has a large5 t statistic (3.32 compare with 2.78). Adding the other
explanatory variables only slightly increased the standard error on the interaction term.
b. The small R-squared, on the order of 4.1%, or 3.9% if we used the adjusted R-squared,
means that we do not explain much of the variation in time on workers compensation using the
variables included in the regression. This is often the case in the social sciences: it is very
difficult to include the multitude of factors that can affect something like durat. The low
R-squared means that making predictions of logdurat would be very difficult given the
factors we have included in the regression: the variation in the unobservables pretty much
swamps the explained variation. However, the low R-squared does not mean we have a biased
or inconsistent estimator of the effect of the policy change. Provided the Kentucky policy
change provides a good natural experiment, the OLS estimator is consistent. With over 5,000
observations, we can get a reasonably precise estimate of the effect, although the 95%
confidence interval is pretty wide.
c. Using the data for Michigan to estimate the basic regression gives
. reg ldurat afchnge highearn afhigh if mi
Source |
SS
df
MS
------------------------------------------Model | 34.3850177
3 11.4616726

71

Number of obs 
F( 3, 1520) 
Prob  F


1524
6.05
0.0004

Residual | 2879.96981 1520 1.89471698
------------------------------------------Total | 2914.35483 1523 1.91356194

R-squared

Adj R-squared 
Root MSE


0.0118
0.0098
1.3765

----------------------------------------------------------------------------ldurat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------afchnge |
.0973808
.0847879
1.15
0.251
-.0689329
.2636945
highearn |
.1691388
.1055676
1.60
0.109
-.0379348
.3762124
afhigh |
.1919906
.1541699
1.25
0.213
-.1104176
.4943988
_cons |
1.412737
.0567172
24.91
0.000
1.301485
1.523989
-----------------------------------------------------------------------------

The coefficient on the interaction term, .192, is remarkably similar to that for Kentucky.
Unfortunately, because of the many fewer observations, the t statistic is insignificant at the
10% level against a one-sided alternative. Asymptotic theory roughly predicts that the standard
error for Michigan will be about 5, 626/1, 524 1/2 ≈ 1. 92 larger than that for Kentucky
(assuming the same error variance and same fraction of observations in the different groups).
In fact, the ratio of standard errors is about 2.23. The difference precision in the KY and MI
cases shows the importance of a large sample size for this kind of policy analysis.
6.10. a. As suggested by the hint, we can write N ̂ −   N −1/2 ∑ i1 A −1 z ′i u i , where
N

A ≡ Ez ′ z, plus a term we can ignore by the asymptotic equivalence lemma. Further,
N

N x̄ −   N −1/2 ∑ i1 x i − . When we stack these two representations, we see that the
asymptotic covariance between N ̂ −  and N x̄ −  is
EA −1 z ′i u i x i −   A −1 Eu i z ′i x i − . Because Eu i |x i   0, the standard iterated
expectations argument shows that Eu i z ′i x i −   0 because z i is a function of x i . This
completes the proof.
b. While the delta method leads to the same place, it is not needed because of linearity of ̂
in the data. We can write ̂ 1  ̂ 1  ̂ 3 x̄ 2  ̂ 1  ̂ 3  2  ̂ 3 x̄ 2 −  2  ≡ ̃ 1  ̂ 3 x̄ 2 −  2 , and
so N ̂ 1 −  1  

N ̃ 1 −  1   ̂ 3  N x̄ 2 −  2 . Now

̂ 3  N x̄ 2 −  2    3  N x̄ 2 −  2   o p 1 because ̂ 3 −  3  o p 1 and

72

N x̄ 2 −  2   O p 1. So we have
N ̂ 1 −  1  

N ̃ 1 −  1    3  N x̄ 2 −  2   o p 1.

By part a, we know that N ̂ −  and N x̄ 2 −  2  are asymptotically jointly normal and
asymptotically independent (uncorrelated). Because N ̃ 1 −  1  is just a deterministic linear
combination of

N ̂ −  it follows that N ̃ 1 −  1  and N x̄ 2 −  2  are asymptotically

uncorrelated. Therefore,.
Avar N ̂ 1 −  1   Avar N ̃ 1 −  1    23 Avar N x̄ 2 −  2 
 Avar N ̃ 1 −  1    23  22 ,
where  22  Varx 2 . Therefore, by the convention introduced in Section 3.5, we write
Avar̂ 1   Avar̃ 1    23  22 /N,
which is what we wanted to show.
c. As stated in the hint, the standard error we get from the regression in Problem 4.8d is
really se̃ 1 , as it does not account for the sampling variation in x̄ 2 . So
se̂ 1   se̃ 1  2  ̂ 23 ̂ 22 /N 1/2  se̃ 1  2  ̂ 23 sex̄ 2  2  1/2
since sex̄ 2    2 / N .
d. The standard error reported for the education variable in Problem 4.8d, se̂ 1 , is about
.00698, the coefficient on the interaction term ̂ 3  is about .00455, and the sample standard
deviation of exper is about 4.375. Plugging these numbers into the formula from part c gives
se̂ 1   . 00698 2  . 00455 2 4. 375 2 /935 1/2 ≈. 00701. For practical purposes, this is not
much bigger than .00698: the effect of accounting for estimation of the population mean of
exper is very modest.
6.11. The following is Stata output for answering the first three parts:

73

. use cps78_85
. reg lwage y85 educ y85educ exper expersq union female y85fem
Source |
SS
df
MS
------------------------------------------Model | 135.992074
8 16.9990092
Residual | 183.099094 1075 .170324738
------------------------------------------Total | 319.091167 1083
.29463635

Number of obs
F( 8, 1075)
Prob  F
R-squared
Adj R-squared
Root MSE








1084
99.80
0.0000
0.4262
0.4219
.4127

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y85 |
.1178062
.1237817
0.95
0.341
-.125075
.3606874
educ |
.0747209
.0066764
11.19
0.000
.0616206
.0878212
y85educ |
.0184605
.0093542
1.97
0.049
.000106
.036815
exper |
.0295843
.0035673
8.29
0.000
.0225846
.036584
expersq | -.0003994
.0000775
-5.15
0.000
-.0005516
-.0002473
union |
.2021319
.0302945
6.67
0.000
.1426888
.2615749
female | -.3167086
.0366215
-8.65
0.000
-.3885663
-.244851
y85fem |
.085052
.051309
1.66
0.098
-.0156251
.185729
_cons |
.4589329
.0934485
4.91
0.000
.2755707
.642295
-----------------------------------------------------------------------------

a. The return to another year of education increased by about . 0185, or 1. 85 percentage
points, between 1978 and 1985. The t statistic on y85educ is 1. 97, which is marginally
significant at the 5% level against a two-sided alternative.
b. The coefficient on y85fem is positive and shows that the estimated gender gap declined
by about 8.5 percentage points. It is still very large, with the gender difference in lwage in
1985 estimated at about −. 232. The t statistic on y85fem is only significant at about the 10%
level against a two-sided alternative. Still, this is suggestive of some closing of wage
differentials between women and men at given levels of education and workforce experience.
c. Only the coefficient on y85 changes if wages are measured in 1978 dollars. In fact, you
can check that when 1978 wages are used, the coefficient on y85 becomes about
−. 383 . 118 − log1. 65 ≈. 118 −. 501.
d. To answer this question, I just took the squared OLS residuals and regressed those on the
year dummy, y85. The coefficient is about . 042 with a standard error of about . 022, which
74

gives a t statistic of about 1. 91. So there is some evidence that the variance of the unexplained
part of log wages (or even log real wages) has increased over time.
e. As the equation is written in the problem, the coefficient  0 is the growth in nominal
wages for a male with no years of education! For a male with 12 years of education, we want
 0 ≡  0  12 1 .
Many packages have simple commands that deliver standard errors and tests for linear
combinations. But a general way to obtain the standard error for ̂ 0 ≡ ̂ 0  12̂ 1 is to replace
y85  educ with y85  educ − 12 and reestimate the equation. Simple algebra shows that, in
the new equation,  0 is the coefficient on educ. In Stata we have
. gen y85educ_12  y85*(educ - 12)
. reg lwage y85 educ y85educ_12 exper expersq union female y85fem
Source |
SS
df
MS
------------------------------------------Model | 135.992074
8 16.9990092
Residual | 183.099094 1075 .170324738
------------------------------------------Total | 319.091167 1083
.29463635

Number of obs
F( 8, 1075)
Prob  F
R-squared
Adj R-squared
Root MSE








1084
99.80
0.0000
0.4262
0.4219
.4127

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y85 |
.3393326
.0340099
9.98
0.000
.2725993
.4060659
educ |
.0747209
.0066764
11.19
0.000
.0616206
.0878212
y85educ_12 |
.0184605
.0093542
1.97
0.049
.000106
.036815
exper |
.0295843
.0035673
8.29
0.000
.0225846
.036584
expersq | -.0003994
.0000775
-5.15
0.000
-.0005516
-.0002473
union |
.2021319
.0302945
6.67
0.000
.1426888
.2615749
female | -.3167086
.0366215
-8.65
0.000
-.3885663
-.244851
y85fem |
.085052
.051309
1.66
0.098
-.0156251
.185729
_cons |
.4589329
.0934485
4.91
0.000
.2755707
.642295
-----------------------------------------------------------------------------

So the growth in nominal wages for a man with educ  12 is about . 339, or 33. 9%. [We
could use the more accurate estimate, . 404, obtained from exp. 339 − 1 . 404.] The 95%
confidence interval goes from about 27. 3 to 40. 6.
Stata users can verify that the command

75

. lincom y85  12*y85educ

after estimation of the original equation delivers the same estimate and inference.
′

′

6.12. Under the assumptions listed, Ex u  0, Ez u  0, and the rank conditions hold
for OLS and 2SLS, so we can write
N

N −1/2 ∑ x ∗′
N ̂ 2SLS −   A −1
∗
i ui

 o p 1,

i1

N

N ̂ OLS −   A −1 N −1/2 ∑ x ′i u i

 o p 1

i1

′

′

where A  Ex i x i , A ∗  Ex ∗i x ∗i , and x ∗i  z i . Further, because of the homoskedasticity
2 ∗′
∗′
2
2
assumptions, Eu 2i x ′i x i    2 A, Eu 2i x ∗′
i x i    A ∗ , and Eu i x i x i    Ex i x i . But we

know from Chapter 5 that Ex ∗′
i x i   A ∗ . Next, we can stack the above equations to obtain
that OLS and 2SLS, when appropriately centered and scaled, are jointly asymptotically normal
with variance-covariance matrix
V1

C

′

V2

C

,

where V 1  Avar N ̂ 2SLS − , V 2  Avar N ̂ OLS − , and
−1
2 ∗′
C  A −1
  2 A −1 . Therefore, we can write the asymptotic variance matrix of
∗ Eu i x i x i A

both estimators as


2

A −1
A −1
∗
A −1 A −1

.

Now, the asymptotic variance of any linear combination is easy to obtain. In particular, the
asymptotic variance of N ̂ 2SLS −  − N ̂ OLS −  is simply
−1
2 −1
 2 A −1
− A −1 − A −1    2 A −1
∗ A
∗ −  A , which is the difference in the asymptotic

76

variances, as we wanted to show.
6.13. This is a simple application of the law of iterated expectations. The statement of the
problem should add the requirement  1 ≠ 0. By the LIE,
Eu 1 |z  EEu 1 |z, v 2 |z  E 1 v 2 |z   1 Ev 2 |z
and so if Eu 1 |z  0 then Ev 2 |z  0, too.
6.14. a. First, y 2 is a function of z, v 2 , and so, from the structural equation,
Ey 1 |z, v 2   z 1  1  gy 2  1  Eu 1 |z, v 2   z 1  1  gy 2  1  Eu 1 |v 2 ,
where
Eu 1 |z, v 2   Eu 1 |v 2 
follows because u 1 , v 2  is independent of z. (Note that, in general, it is not enough to assume
that u 1 and v 2 are separately independent of z; joint independence is needed.)
b. If Eu 1 |v 2    1 v 2 then, under the previous assumptions,
Ey 1 |z, v 2   z 1  1  gy 2  1   1 v 2 .
Therefore, in the first step, we would run OLS of y i2 on z i , i  1, … , N, and obtain the OLS
residuals, v̂ i2 . In the second step, we would regress y i1 on z i1 , gy i2 , v̂ i2 , i  1, … , N. By the
usual two-step estimation results, all coefficients are N -consistent and asymptotically normal
for the corresponding population parameter. The interesting thing about this method is that, if
G 1  1 we have more than one endogenous explanatory variable – g 1 y 1 , … , g G 1 y 2  – but
adding a single regressor, v̂ i2 , cleans up the endogeneity. This occurs because all endogenous
regressors are a function of y 2 , and we have assumed y 2 is an additive function of z and an
independent error, which pretty much restricts y 2 to be continuous. (We can easily replace the

77

linear function z 2 with known nonlinear functions of z.)
As specific examples, the second stage regression might be
y i1 or z i1 , y i2 , y 2i2 , y 3i2 , v̂ i2 , i  1, … N
or
y i1 or z i1 , 1a 1  y i2 ≤ a 2 , … , 1a m−1  y i2 ≤ a m , 1y i2  a M , v̂ i2 , i  1, … , N.
In the latter case, dummies for whether y i2 falls into one of the intervals
−, a 1 , a 1 , a 2 , … , a M−1 , a M , a M ,  appear in the structural model.
c. If  1  0, no adjustment is needed to the asymptotic variance, so we can use the usual t
statistic on v̂ i2 as a test of endogeneity of y 2 , where the null is exogeneity: H 0 :  1  0.
Actually, nothing guarantees that Vary 1 |z, v 2  does not depend on v 2 – and, under weaker
assumptions, it could also depend on z – so there is a good case for making the test robust to
heteroskedasticity.
d. The estimating equation becomes
Ey 1 |z, v 2   z 1  1  gy 2  1   1 v 2   1 v 22 −  22 
and now, to implement a two-step control function procedure, we obtain ̂ 22 , the usual OLS
error variance estimate, along with ̂ 2 . The residuals are constructed as before,
v̂ i2  y i2 − z i ̂ 2 . The second-step regression is now
y i1 on z i1 , gy i2 , v̂ i2 , v̂ 2i2 − ̂ 22 , i  1, … , N
Now we can use a heteroskedasticity-robust Wald test of joint significance of v̂ i2 and
v̂ 2i2 − ̂ 22 . Under the null H 0 :  1  0,  1  0, we do not have to adjust the statistic for the
first-step estimation.
e. We would use traditional 2SLS, where we need at least one IV for each g j y 2 . Methods
78

for coming up with such IVs are discussed in Section 9.5. Briefly, they will be nonlinear
functions of z, which is why Eu 1 |z  0 should be assumed. Generally, we add enough
nonlinear functions, say hz, to the original instrument list z. So, do 2SLS of y 1 on z 1 , g 2
using IVs z, hz. 2SLS will be more robust than the method described in part b because the
reduced form for y 2 is not restricted in any way, and we need not assume u 1 is independent of
z.
6.15. a. Because y 2  z 2  v 2 , we can find Ey 1 |z, v 2  or Ey 1 |z, y 2 ; they are the same.
Now
Ey 1 |z, v 2   z 1  1  gz 1 , y 2  1  gz 1 , y 2 Ev 1 |z, v 2   Eu 1 |z, v 2 
 z 1  1  gz 1 , y 2  1  gz 1 , y 2 v 2  1   1 v 2
b. The first step is to regress y i2 on z i and get the residuals, v̂ i2 . Second, run the regression
y i1 on z i1 , gz i1 , y i2 , gz i1 , y i2 v̂ i2 , v̂ i2
which means that v̂ i2 appears by itself and interacted with all elements of gz i1 , y i2 .
c. The null is H 0 :  1  0,  1  0, which means we can compute a
heteroskedasticity-robust Wald test of joint significance of gz i1 , y i2 v̂ i2 and v̂ i2 .
d. For the specific model give, the second-step regression is
y i1 on z i1 , y i2 , y 2i2 , z i1 y i2 , y i2 v̂ i2 , v̂ i2 , i  1, . . . , N.
In other words, v̂ i2 appears by itself and interacted with y i2 , as in Garen (1984).

79

Solutions to Chapter 7 Problems
7.1. Write (with probability approaching one)
−1

N

̂   

N

−1

∑

X ′i X i

N

N

−1

i1

∑ X ′i u i

.

i1

From Assumption SOLS. 2, the weak law of large numbers, and Slutsky’s Theorem,
N

plim N −1 ∑ X ′i X i

−1

 A −1 .

i1

N

Further, under SOLS.1, the WLLN implies that plim N −1 ∑ i1 X ′i u i
N

plim̂    plim N −1 ∑ X ′i X i

−1

 0. Thus,

N

 plim N

i1

−1

∑ X ′i u i

   A −1  0  .

i1

7.2. a. Under SOLS. 1 and SOLS.2, Theorem 7.2 implies that Avar̂ OLS   A −1 BA −1 /N,
where A  EX ′i X i  and B  EX i u i u ′i X i . But we have assumed that
EX i u i u ′i X i   EX ′i X i , which proves the assertion. Effectively, this is what we can expect
for the asymptotic variance of OLS under the system version of homoskedasticity. [Note that
Assumption SGLS. 3 and EX i u i u ′i X i   EX ′i X i  are not the same, but both are implied by
condition (7.53). There are other cases where they reduce to the same assumption, such as in a
SUR model when  is diagonal.]
b. The estimator in (7.28) is always valid. An estimator that uses the structure of
̂  N −1 ∑ N û i û ′i , where the û i are
Avar ̂ SOLS obtained in part a is obtained as follows. Let 
i1
the G  1 system OLS residuals. Then
Avar ̂ SOLS

N



∑ X ′i X i

−1

i1

N

∑ X ′i ̂X i
i1

80

N

∑ X ′i X i
i1

−1

is a valid estimator provided the homoskedasticity assumption holds.
c. Using the hint and dropping the division by N on the right hand side, we have
Avar̂ FGLS  −1 − Avar̂ SOLS  −1  EX ′i  −1 X i  − EX ′i X i EX ′i X i  −1 EX ′i X i .
Define Z i ≡  −1/2 X i and W i ≡  1/2 X i . Then the difference can be written as
EZ ′i Z i  − EZ ′i W i EW ′i W i  −1 EW ′i Z i .
Now, define R i ≡ Z i − W i , where  ≡ EW ′i W i  −1 EW ′i Z i ; R i is the G  K matrix of
population residuals from the linear projection of Z i on W i . Straightforward multiplication
shows that
EZ ′i Z i  − EZ ′i W i EW ′i W i  −1 EW ′i Z i   ER ′i R i ,
which is necessarily positive semi-definite. We have shown that if (7.53) holds along with
SGLS.1 and the rank conditions for SGLS and SOLS, then FGLS is more efficient than OLS.
d. If    2 I G ,
Avar N ̂ SOLS −   EX ′i X i  −1 EX ′i X i EX ′i X i  −1   2 EX ′i X i  −1 and
Avar N ̂ SOLS −   EX ′i  −1 X i  −1  EX ′i  2 I G  −1 X i  −1   2 EX ′i X i  −1 .
e. This statement is true provided we consider only asymptotic efficiency under the
assumption that SGLS.1 holds. In other words, under SGLS.1, the standard rank conditions,
and Eu i u ′i |X i   , there is nothing to lose asymptotically by using FGLS. Of course, SOLS
is more robust in that it only requires SOLS.1 for consistency (and asymptotic normality).
Small sample properties are another issue because it is difficult to characterize the exact
properties of FGLS under general conditions.
7.3. a. Since OLS equation-by-equation is the same as GLS when  is diagonal, it suffices
to show that the GLS estimators for different equations are asymptotically uncorrelated. This

81

follows if the asymptotic variance matrix is block diagonal (see Section 3.5), where the
blocking is by the parameter vector for each equation. To establish block diagonality, we use
the result from Theorem 7.4: under SGLS. 1, SGLS.2, and SGLS.3,
Avar N ̂ −   EX ′i  −1 X i  −1 .
Now, we can use the special form of X i for SUR (see Example 7.1), the fact that  −1 is
diagonal, and SGLS.3. In the SUR model with diagonal , SGLS.3 implies that
Eu 2ig x ′ig x ig    2g Ex ′ig x ig  for all g  1, . . . , G, and
Eu ig u ih x ′ig x ih   Eu ig u ih Ex ′ig x ih   0, all g ≠ h.
Therefore, we have

EX ′i  −1 X i 



′
 −2
1 Ex i1 x il 

0

0

0



0

0

0

′
 −2
G Ex iG x iG 

.

When this matrix is inverted, it is also block diagonal. This shows that Avar N ̂ −  is
block diagonal, and therefore the N ̂ g −  g  are asymptotically uncorrelated.
b. To test any linear hypothesis, we can either construct the Wald Statistic or we can use
the weighted sum of squared residuals form of the statistic as in (7.56) or (7.57). For the
restricted SSR we must estimate the model with the restriction  1   2 imposed. See Problem
7.6 for one way to impose general linear restrictions.
c. Actually, for the conclusion to hold about asymptotic equivalence, we need to assume
SGLS.1 along with SOLS.2 and SGLS.2. When  is diagonal in a SUR system, system OLS
and GLS are the same. Under SGLS.1 and SGLS.2, GLS and FGLS are asymptotically
equivalent (regardless of the structure of ) whether or not SGLS.3 holds. Now if

82

̂ SOLS  ̂ GLS and N ̂ FGLS − ̂ GLS

 o p 1, then N ̂ SOLS − ̂ FGLS

 o p 1. Thus, when

 is diagonal, OLS and FGLS are asymptotically equivalent under the exogeneity assumption
̂ is estimated in an unrestricted fashion and even if the system
SGLS.1, even if 
homoskedasticity assumption SGLS.3 does not hold.
If only SOLS.1 holds, we cannot conclude N ̂ FGLS − ̂ GLS

 o p 1, and so

N ̂ SOLS − ̂ FGLS is not generally o p 1. It is true that FGLS is still consistent under
SOLS.1 because its plim is
−1

′
 −2
1 Ex i1 x il 

0

0

′
 −2
1 Ex i1 u i1 

0



0



0

0

′
 −2
G Ex iG x iG 

′
 −2
G Ex iG u iG 

and Ex ′ig u ig   0, g  1, . . . , G.
7.4. To make the notation align with the text, use  to denote the SOLS estimator, and let
̂ . Then it suffices to
u i denote the G  1 vector of SOLS residuals that are used in obtaining 
show that
N

N

−1/2

∑

N

u i u ′i

N

−1/2

i1

∑ u i u i  o p 1,

(7.82)

i1

and this follows if, when we sum across N and divide by N, the last three terms in (7.42) are
o p 1. Since the third term is the transpose of the second it suffices to consider only the second
and fourth terms. Now
N

′
N −1/2 ∑ vec u i  −  X ′i
i1

N

 N −1/2 ∑X i ⊗ u i    − 
i1

N



N

−1

∑X i ⊗ u i 
i1

83

N  − 

 o p 1  O p 1  o p 1.

Also,
N

N −1/2 ∑ vec X i ̂ − 


′
̂ −  X ′i

N

N −1 ∑X i ⊗ X i  vec



i1

N  − 

N  − 

′

/ N

i−1

 O p 1  O p 1  N −1/2  o p 1.
N
N
Together, these imply N −1/2 ∑ i1 u i u ′i  N −1/2 ∑ i1 u i u i  o p 1 and so

N

−1/2

N

N

i1

i1

∑u i u ′i −   N −1/2 ∑u i u i −   o p 1.

7.5. This is easy with the hint. Note that
̂


−1

−1

N

∑

⊗

̂ ⊗


x ′i x i

i1

−1

N

∑

x ′i x i

.

i1

Therefore,

̂ 

̂ ⊗


N

∑ x ′i x i

N
x ′i y i1
∑ i1

−1

̂ −1 ⊗ I K


N
x ′i x i
∑ i1



0





i1

N

∑ x ′i y iG
0
N
x ′i x i
∑ i1

−1

∑ x ′i x i



0

N
x ′i y i1
∑ i1

0



N
x ′i y i2
∑ i1

0



0



0



0

N
x ′i x i
∑ i1

−1

N
i1

∑ x ′i y iG



N
x ′i y iG
∑ i1

where  g is the OLS estimator for equation g.
7.6. The model for a random draw from the population is y i  X i   u i , which can be
written as
y i  X i1  1  X i2  2  u i ,

84

N
x ′i y i1
∑ i1



i1

N
i1

−1

IG ⊗

−1

 1

2


 G

.

where the partition of X i is defined in the problem. Now, if  1  R −1
1 r − R 2  2 , we just plug
this into the previous equation:
y i  X i1  1  X i2  2  u i  X i1 R −1
1 r − R 2  2   X i2  2  u i
 X i1 R −1
1 r  X i2 − X i1 R 2  2  u i .
Bringing X i1 R −1
1 r to the left hand side gives
y i − X i1 R −1
1 r  X i2 − X i1 R 2  2  u i .
̃
If we define ỹ i  y i − X i1 R −1
1 r and X i2 ≡ X i2 − X i1 R 2 , then we get the desired equation:
̃ i2   u i .
ỹ i  X
2
̃ i2 are functions of the data for observation i and the known matrices
(Note that ỹ i and X
R 1 , R 2 , and the known vector r.)
This general result is very convenient for computing the weighted SSR form of the F
̂ denote the estimate of  based on estimation of the
statistic (under SGL.3). Let 
̂  N −1 ∑ u i u ′i where u i are the system OLS residuals.
unconstrained system; typically, 
i1
N

̃ i2   u i by FGLS
Using this matrix, we estimate y i  X i1  1  X i2  2  u i and then ỹ i  X
2
̂ . Let û i denote the FGLS residuals from the unrestricted model and let ũ i  ỹ − X
̃ i2 ̃
using 
i
2
denote the restricted FGLS residuals, where ̃ 2 is the FGLS estimator from the restricted
estimation. Then the F statistic computed from (7.57) has an approximate F Q,NG−K distribution
under H 0 (assuming SGLS.1, SGLS.2, and SGLS.3 hold).
7.7. a. First, the diagonal elements of  are easily found because
Eu 2it  EEu 2it |x it    2t by iterated expectations. Now, consider Eu it u is , and take s  t
without loss of generality. Under Eu it |x it , u i,t−1 , . . .   0, Eu it |u is   0 because u is is a subset
of the larger conditioning set. Applying LIE again we have

85

Eu it u is   EEu it u is |u is   EEu it |u is u is   0.
So
 21

0



0

0

 22

0





0



0

0



0

 2T



.

b. The GLS estimator is
−1

N

∗

 ≡

∑

N

∑ X ′i  −1 y i

X ′i  −1 X i

i1
N



i1

−1

T

∑∑

′
 −2
t x it x it

i1 t1

N

T

∑ ∑  −2t x ′it y it

,

i1 t1

which is a weighted least squares estimator with every observation for time period t weighted
by  −2
t , the inverse of the variance.
c. If, say, y it   0   1 y i,t−1  u it , then y it is clearly correlated with u it , which says that
x i,t1  y it is correlated with u it . Thus, SGLS.1 cannot hold. Generally, SGLS.1 fails to hold
whenever there is feedback from y it to x is , s  t. Nevertheless, because  −1 is diagonal,
T

X ′i  −1 u i  ∑ t1 x ′it  −2
t u it , and so
T

EX ′i  −1 u i 

′
− ∑  −2
t Ex it u it   0,
t1

where we use Ex ′it u it   0 under Eu it |x it , u i,t−1 , . . .   0. It follows that the GLS estimator is
GLS is consistent in this case without SGLS.1.
′

−2 ′
−2 ′
′
d. First, since  −1 is diagonal, X ′i  −1   −2
1 x il ,  2 x i2 , … ,  T x iT  ‚ and so

86

T

EX ′i  −1 u i u ′i  −1 X i  

T

∑ ∑  −2t  −2s Eu it u is x ′it x is .
t1 s1

First consider the terms for s ≠ t. Under Eu it |x it , u i,t−1 , . . .   0, Eu it |x it , u is , x is   0 for
s  t, and so by the LIE, Eu it u is x ′it x is   0, all t ≠ s. Next, for each t,
Eu 2it x ′it x it   EEu 2it x ′it x it |x it   EEu 2it |x it x ′it x it 
 E 2t x ′it x it    2t Ex ′it x it , t  1, 2, … , T.
It follows that
T

EX ′i  −1 u i u ′i  −1 X i  

∑  −2t Ex ′it x it   EX ′i  −1 X i .
t1

e. First, run pooled OLS across all i and t and let u it denote the pooled OLS residuals.
Then, for each t, define
N

̂ 2t

N

−1

∑ u 2it
i1

(We might replace N with N − K as a degree-of-freedom adjustment.) By standard arguments,
p

̂ 2t   2t as N  .
f. What we need to show is that replacing the  2t with the ̂ 2t does not affect the
N -asymptotic distribution of the FGLS estimator. We know this generally under SGLS.1, but
we have relaxed that assumption. To show it holds in the current setting we need to show
N

N

−1

T

∑∑

N

′
̂ −2
t x it x it

N

−1

i1 t1
N

N

−1/2

T

∑ ∑  −2t x ′it x it  o p 1
i1 t1

T

N

T

∑ ∑ ̂ −2t x ′it u it  N −1/2 ∑ ∑  −2t x ′it u it  o p 1.
i1 t1

i1 t1

The first follows from the consistency of each ̂ 2t using standard arguments we have used

87

before. The second requirement follows from
N

T

N

T

′
−1/2
N −1/2 ∑ ∑ ̂ −2
∑ ∑  −2t x ′it u it 
t x it u it − N
i1 t1

i1 t1

T

∑
t1

N

−2
N −1/2 ∑ x ′it u it ̂ −2
t − t 
i1

T



∑ O p 1  o p 1  o p 1
t1

N

because N −1/2 ∑ i1 x ′it u it satisfies the CLT under Under Eu it |x it , u i,t−1 , . . .   0 and second
moment assumptions.
So now we know all inference is as if we are applying pooled OLS to
y it / t   x it / t   e it , t  1, 2, … , T
where this equation satisfies POLS.1, POLS.2, and POLS.3. Thus, we can use the usual
statistics – standard errors, confidence intervals, t and F statistics – from the regression
y it /̂ t   x it /̂ t , t  1, . . . , T; i  1, . . . , N.
For F testing, note that the ̂ 2t should be obtained using the pooled OLS residuals for the
unrestricted model.
g. If  2t   2 for all t  1, … , T, inference is very easy because with weighted least
squares method reduces to pooled OLS. Thus, we can use the standard errors and test statistics
reported by a standard OLS regression pooled across i and t.
7.8. Here is some Stata output:
. use fringe
. gen hrvac  vacdays/annhrs
. gen hrsick  sicklve/annhrs
. gen hrins  insur/annhrs
. gen hrpens  pension/annhrs
. sureg (hrearn hrvac hrsick hrins hrpens  educ exper expersq tenure

88

tenuresq union south nrtheast nrthcen married white male), corr
Seemingly unrelated regression
---------------------------------------------------------------------Equation
Obs Parms
RMSE
"R-sq"
chi2
P
---------------------------------------------------------------------hrearn
616
12
4.3089
0.2051
158.93
0.0000
hrvac
616
12
.1389899
0.3550
339.01
0.0000
hrsick
616
12
.056924
0.2695
227.23
0.0000
hrins
616
12
.1573797
0.3891
392.27
0.0000
hrpens
616
12
.2500388
0.3413
319.16
0.0000
-------------------------------------------------------------------------------------------------------------------------------------------------|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------hrearn
|
educ |
.4588139
.068393
6.71
0.000
.3247662
.5928617
exper | -.0758428
.0567371
-1.34
0.181
-.1870455
.0353598
expersq |
.0039945
.0011655
3.43
0.001
.0017102
.0062787
tenure |
.1100846
.0829207
1.33
0.184
-.052437
.2726062
tenuresq | -.0050706
.0032422
-1.56
0.118
-.0114252
.0012839
union |
.8079933
.4034789
2.00
0.045
.0171892
1.598797
south | -.4566222
.5458508
-0.84
0.403
-1.52647
.6132258
nrtheast | -1.150759
.5993283
-1.92
0.055
-2.32542
.0239032
nrthcen | -.6362663
.5501462
-1.16
0.247
-1.714533
.4420005
married |
.6423882
.4133664
1.55
0.120
-.167795
1.452571
white |
1.140891
.6054474
1.88
0.060
-.0457639
2.327546
male |
1.784702
.3937853
4.53
0.000
1.012897
2.556507
_cons | -2.632127
1.215291
-2.17
0.030
-5.014054
-.2501997
---------------------------------------------------------------------------hrvac
|
educ |
.0201829
.0022061
9.15
0.000
.015859
.0245068
exper |
.0066493
.0018301
3.63
0.000
.0030623
.0102363
expersq | -.0001492
.0000376
-3.97
0.000
-.0002229
-.0000755
tenure |
.012386
.0026747
4.63
0.000
.0071436
.0176284
tenuresq | -.0002155
.0001046
-2.06
0.039
-.0004205
-.0000106
union |
.0637464
.0130148
4.90
0.000
.0382378
.0892549
south | -.0179005
.0176072
-1.02
0.309
-.05241
.016609
nrtheast | -.0169824
.0193322
-0.88
0.380
-.0548728
.0209081
nrthcen |
.0002511
.0177458
0.01
0.989
-.03453
.0350321
married |
.0227586
.0133337
1.71
0.088
-.0033751
.0488923
white |
.0084869
.0195296
0.43
0.664
-.0297905
.0467642
male |
.0569525
.0127021
4.48
0.000
.0320568
.0818482
_cons | -.1842348
.039201
-4.70
0.000
-.2610674
-.1074022
---------------------------------------------------------------------------hrsick
|
educ |
.0096054
.0009035
10.63
0.000
.0078346
.0113763
exper |
.002145
.0007495
2.86
0.004
.0006759
.0036141
expersq | -.0000383
.0000154
-2.48
0.013
-.0000684
-8.08e-06
tenure |
.0050021
.0010954
4.57
0.000
.002855
.0071491
tenuresq | -.0001391
.0000428
-3.25
0.001
-.0002231
-.0000552
union | -.0046655
.0053303
-0.88
0.381
-.0151127
.0057816
south |
-.011942
.0072111
-1.66
0.098
-.0260755
.0021916
nrtheast | -.0026651
.0079176
-0.34
0.736
-.0181833
.0128531
nrthcen | -.0222014
.0072679
-3.05
0.002
-.0364462
-.0079567
married |
.0038338
.0054609
0.70
0.483
-.0068694
.014537
white |
.0038635
.0079984
0.48
0.629
-.0118132
.0195401
male |
.0042538
.0052022
0.82
0.414
-.0059423
.01445

89

_cons | -.0937606
.016055
-5.84
0.000
-.1252278
-.0622935
---------------------------------------------------------------------------hrins
|
educ |
.0080042
.002498
3.20
0.001
.0031082
.0129002
exper |
.0054052
.0020723
2.61
0.009
.0013436
.0094668
expersq | -.0001266
.0000426
-2.97
0.003
-.00021
-.0000431
tenure |
.0116978
.0030286
3.86
0.000
.0057618
.0176338
tenuresq | -.0002466
.0001184
-2.08
0.037
-.0004787
-.0000146
union |
.1441536
.0147368
9.78
0.000
.11527
.1730372
south |
.0196786
.0199368
0.99
0.324
-.0193969
.0587541
nrtheast | -.0052563
.0218901
-0.24
0.810
-.0481601
.0376474
nrthcen |
.0242515
.0200937
1.21
0.227
-.0151315
.0636345
married |
.0365441
.0150979
2.42
0.016
.0069527
.0661355
white |
.0378883
.0221136
1.71
0.087
-.0054535
.0812301
male |
.1120058
.0143827
7.79
0.000
.0838161
.1401955
_cons | -.1180824
.0443877
-2.66
0.008
-.2050807
-.0310841
---------------------------------------------------------------------------hrpens
|
educ |
.0390226
.0039687
9.83
0.000
.031244
.0468012
exper |
.0083791
.0032924
2.55
0.011
.0019262
.0148321
expersq | -.0001595
.0000676
-2.36
0.018
-.0002921
-.000027
tenure |
.0243758
.0048118
5.07
0.000
.0149449
.0338067
tenuresq | -.0005597
.0001881
-2.97
0.003
-.0009284
-.0001909
union |
.1621404
.0234133
6.93
0.000
.1162513
.2080296
south | -.0130816
.0316749
-0.41
0.680
-.0751632
.049
nrtheast | -.0323117
.0347781
-0.93
0.353
-.1004755
.0358521
nrthcen | -.0408177
.0319241
-1.28
0.201
-.1033878
.0217525
married | -.0051755
.023987
-0.22
0.829
-.0521892
.0418381
white |
.0395839
.0351332
1.13
0.260
-.0292758
.1084437
male |
.0952459
.0228508
4.17
0.000
.0504592
.1400325
_cons | -.4928338
.0705215
-6.99
0.000
-.6310534
-.3546143
----------------------------------------------------------------------------Correlation matrix of residuals:
hrearn
hrvac
hrsick
hrins
hrpens

hrearn
1.0000
0.2719
0.2541
0.2609
0.2786

hrvac

hrsick

hrins

hrpens

1.0000
0.5762
0.6701
0.7070

1.0000
0.2922
0.4569

1.0000
0.6345

1.0000

Breusch-Pagan test of independence: chi2(10) 
. test married
(
(
(
(
(

1)
2)
3)
4)
5)

[hrearn]married  0
[hrvac]married  0
[hrsick]married  0
[hrins]married  0
[hrpens]married  0
chi2( 5) 
Prob  chi2 

14.48
0.0128

. lincom [hrpens]educ - [hrins]educ
( 1)

- [hrins]educ  [hrpens]educ  0

90

1393.265, Pr  0.0000

----------------------------------------------------------------------------|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------(1) |
.0310184
.0030676
10.11
0.000
.025006
.0370308
-----------------------------------------------------------------------------

The first test shows that there is some evidence that marital status affects at least one of the
five forms of compensation. In fact, it has the largest economic effect on hourly earnings:
. 642, but its t statistic is only about 1. 54. The most statistically significant effect is on hrins:
. 037 with t  2. 42. It is marginally significant and positive for hrvac as well.
The lincom command tests whether another year of education has the same effect on
hrpens and hrins. The t statistic is 10. 11 and the p-value is effectively zero. The estimate in the
hrpens equation (with standard error) is . 039 (. 004) while the estimate in the hrins equation is
. 008 (. 003). Thus, each is positive and statistically significant, and they are significantly
different from one another.
All of the standard errors and statistics reported above assume that SGLS.3 holds, so that
there can be no system heteroskedasticity. This is unlikely to hold in this example.
7.9. The Stata session follows, including a test for serial correlation before computing the
fully robust standard errors:
. use jtrain1
. xtset fcode year
panel variable:
time variable:
delta:

fcode (strongly balanced)
year, 1987 to 1989
1 unit

. reg lscrap d89 grant grant_1 lscrap_1 if year ! 1987
Source |
SS
df
MS
------------------------------------------Model | 186.376973
4 46.5942432
Residual | 31.2296502
103 .303200488
------------------------------------------Total | 217.606623
107 2.03370676

Number of obs
F( 4,
103)
Prob  F
R-squared
Adj R-squared
Root MSE








108
153.67
0.0000
0.8565
0.8509
.55064

----------------------------------------------------------------------------lscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
----------------------------------------------------------------------------

91

d89 | -.1153893
.1199127
-0.96
0.338
-.3532078
.1224292
grant | -.1723924
.1257443
-1.37
0.173
-.4217765
.0769918
grant_1 | -.1073226
.1610378
-0.67
0.507
-.426703
.2120579
lscrap_1 |
.8808216
.0357963
24.61
0.000
.809828
.9518152
_cons | -.0371354
.0883283
-0.42
0.675
-.2123137
.138043
-----------------------------------------------------------------------------

The estimated effect of grant, and its lag, are now the expected sign (if we think the job
training program should reduce the scrap rate), but neither is strongly statistically significant.
The variable grant would be if we use a 10% significance level and a one-sided test. The
results are certainly different from when we omit the lag of logscrap.
Now test for AR1 serial correlation:
. gen uhat_1  l.uhat
(417 missing values generated)
. reg lscrap grant grant_1 lscrap_1 uhat_1 if d89
Source |
SS
df
MS
------------------------------------------Model | 94.4746525
4 23.6186631
Residual | 15.7530202
49 .321490208
------------------------------------------Total | 110.227673
53 2.07976741

Number of obs
F( 4,
49)
Prob  F
R-squared
Adj R-squared
Root MSE








54
73.47
0.0000
0.8571
0.8454
.567

----------------------------------------------------------------------------lscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------grant |
.0165089
.215732
0.08
0.939
-.4170208
.4500385
grant_1 | -.0276544
.1746251
-0.16
0.875
-.3785767
.3232679
lscrap_1 |
.9204706
.0571831
16.10
0.000
.8055569
1.035384
uhat_1 |
.2790328
.1576739
1.77
0.083
-.0378247
.5958904
_cons |
-.232525
.1146314
-2.03
0.048
-.4628854
-.0021646
-----------------------------------------------------------------------------

The estimate of  is about . 28, and it is marginally significant with t  1. 77. (Note we are
relying on asymptotics with N  54.) One could probably make a case for ignoring the serial
correlation. But it is easy enough to obtain the serial-correlation and heteroskedasticity-robust
standard errors:
. reg lscrap d89 grant grant_1 lscrap_1 if year ! 1987, robust cluster(fcode
Number of obs 
F( 4,
53) 
Prob  F

R-squared


Linear regression

92

108
77.24
0.0000
0.8565

Root MSE



.55064

(Std. Err. adjusted for 54 clusters in fcode
----------------------------------------------------------------------------|
Robust
lscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d89 | -.1153893
.1145118
-1.01
0.318
-.3450708
.1142922
grant | -.1723924
.1188807
-1.45
0.153
-.4108369
.0660522
grant_1 | -.1073226
.1790052
-0.60
0.551
-.4663616
.2517165
lscrap_1 |
.8808216
.0645344
13.65
0.000
.7513821
1.010261
_cons | -.0371354
.0893147
-0.42
0.679
-.216278
.1420073
-----------------------------------------------------------------------------

The robust standard errors for grant and grant −1 are actually smaller than the usual ones,
but each is still not statistically significant at the 5% level against a one-sided alternative. In
addition, they are not jointly significant, as the p-value is about . 33:
. test grant grant_1
( 1)
( 2)

grant  0
grant_1  0
F(

2,
53) 
Prob  F 

1.14
0.3266

7.10. The Stata results are:
. use gpa
. reg trmgpa spring cumgpa crsgpa frstsem season sat verbmath hsperc hssize
black female
Source |
SS
df
MS
------------------------------------------Model | 218.156689
11 19.8324263
Residual | 202.140267
720 .280750371
------------------------------------------Total | 420.296956
731 .574961636

Number of obs
F( 11,
720)
Prob  F
R-squared
Adj R-squared
Root MSE








732
70.64
0.0000
0.5191
0.5117
.52986

----------------------------------------------------------------------------trmgpa |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------spring | -.0121568
.0464813
-0.26
0.794
-.1034118
.0790983
cumgpa |
.3146158
.0404916
7.77
0.000
.2351201
.3941115
crsgpa |
.9840371
.0960343
10.25
0.000
.7954964
1.172578
frstsem |
.7691192
.1204162
6.39
0.000
.5327104
1.005528
season | -.0462625
.0470985
-0.98
0.326
-.1387292
.0462042
sat |
.0014097
.0001464
9.63
0.000
.0011223
.0016972
verbmath |
-.112616
.1306157
-0.86
0.389
-.3690491
.1438171
hsperc | -.0066014
.0010195
-6.48
0.000
-.0086029
-.0045998
hssize | -.0000576
.0000994
-0.58
0.562
-.0002527
.0001375
black | -.2312855
.0543347
-4.26
0.000
-.3379589
-.1246122
female |
.2855528
.0509641
5.60
0.000
.1854967
.3856089

93

_cons | -2.067599
.3381007
-6.12
0.000
-2.731381
-1.403818
----------------------------------------------------------------------------. reg trmgpa spring cumgpa crsgpa frstsem season sat verbmath hsperc hssize
black female, robust cluster(id)
Linear regression

Number of obs
F( 11,
365)
Prob  F
R-squared
Root MSE







732
71.31
0.0000
0.5191
.52986

(Std. Err. adjusted for 366 clusters in id
----------------------------------------------------------------------------|
Robust
trmgpa |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------spring | -.0121568
.0395519
-0.31
0.759
-.089935
.0656215
cumgpa |
.3146158
.0514364
6.12
0.000
.2134669
.4157647
crsgpa |
.9840371
.09182
10.72
0.000
.8034745
1.1646
frstsem |
.7691192
.1437178
5.35
0.000
.4865003
1.051738
season | -.0462625
.0431631
-1.07
0.285
-.131142
.038617
sat |
.0014097
.0001743
8.09
0.000
.001067
.0017525
verbmath |
-.112616
.1495196
-0.75
0.452
-.4066441
.1814121
hsperc | -.0066014
.0011954
-5.52
0.000
-.0089522
-.0042506
hssize | -.0000576
.0001066
-0.54
0.589
-.0002673
.0001521
black | -.2312855
.0695278
-3.33
0.001
-.368011
-.0945601
female |
.2855528
.0511767
5.58
0.000
.1849146
.386191
_cons | -2.067599
.3327336
-6.21
0.000
-2.721915
-1.413284
-----------------------------------------------------------------------------

Some of the fully robust standard errors are actually smaller than the corresponding
nonrobust standard error, although the one on cumgpa is quite a bit larger, and drops the t
statistic from 10. 25 to 6. 12. No variable that was statistically significant based on the usual t
statistic becomes statistically insignificant, although the length of some confidence intervals
change. The t statistics for the key variable, season, are similarly and show season is not
statistically significant.
7.11. a. The following Stata output should be self-explanatory. There is clearly strong
positive serial correlation in the errors of the static model (̂ . 792, t ̂  28. 84) and the fully
robust standard errors are much larger than the nonrobust ones. Not, for example, that the t
statistic on the log of the conviction probability, lprbconv goes from −20. 69 to −7. 75.
. use cornwell

94

. xtset county year
panel variable:
time variable:
delta:

county (strongly balanced)
year, 81 to 87
1 unit

. reg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87
Source |
SS
df
MS
------------------------------------------Model | 117.644669
11 10.6949699
Residual |
88.735673
618 .143585231
------------------------------------------Total | 206.380342
629 .328108652

Number of obs
F( 11,
618)
Prob  F
R-squared
Adj R-squared
Root MSE








630
74.49
0.0000
0.5700
0.5624
.37893

----------------------------------------------------------------------------lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.7195033
.0367657
-19.57
0.000
-.7917042
-.6473024
lprbconv | -.5456589
.0263683
-20.69
0.000
-.5974413
-.4938765
lprbpris |
.2475521
.0672268
3.68
0.000
.1155314
.3795728
lavgsen | -.0867575
.0579205
-1.50
0.135
-.2005023
.0269872
lpolpc |
.3659886
.0300252
12.19
0.000
.3070248
.4249525
d82 |
.0051371
.057931
0.09
0.929
-.1086284
.1189026
d83 |
-.043503
.0576243
-0.75
0.451
-.1566662
.0696601
d84 | -.1087542
.057923
-1.88
0.061
-.222504
.0049957
d85 | -.0780454
.0583244
-1.34
0.181
-.1925835
.0364928
d86 | -.0420791
.0578218
-0.73
0.467
-.15563
.0714719
d87 | -.0270426
.056899
-0.48
0.635
-.1387815
.0846963
_cons | -2.082293
.2516253
-8.28
0.000
-2.576438
-1.588149
----------------------------------------------------------------------------. predict uhat, resid
. gen uhat_1  l.uhat
(90 missing values generated)
. reg uhat uhat_1
Source |
SS
df
MS
------------------------------------------Model | 46.6680407
1 46.6680407
Residual | 30.1968286
538 .056127934
------------------------------------------Total | 76.8648693
539 .142606437

Number of obs
F( 1,
538)
Prob  F
R-squared
Adj R-squared
Root MSE








540
831.46
0.0000
0.6071
0.6064
.23691

----------------------------------------------------------------------------uhat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------uhat_1 |
.7918085
.02746
28.84
0.000
.7378666
.8457504
_cons |
1.74e-10
.0101951
0.00
1.000
-.0200271
.0200271
----------------------------------------------------------------------------. reg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87, cluster(county
Number of obs 
F( 11,
89) 
Prob  F

R-squared

Root MSE


Linear regression

95

630
37.19
0.0000
0.5700
.37893

(Std. Err. adjusted for 90 clusters in county
----------------------------------------------------------------------------|
Robust
lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.7195033
.1095979
-6.56
0.000
-.9372719
-.5017347
lprbconv | -.5456589
.0704368
-7.75
0.000
-.6856152
-.4057025
lprbpris |
.2475521
.1088453
2.27
0.025
.0312787
.4638255
lavgsen | -.0867575
.1130321
-0.77
0.445
-.3113499
.1378348
lpolpc |
.3659886
.121078
3.02
0.003
.1254092
.6065681
d82 |
.0051371
.0367296
0.14
0.889
-.0678439
.0781181
d83 |
-.043503
.033643
-1.29
0.199
-.1103509
.0233448
d84 | -.1087542
.0391758
-2.78
0.007
-.1865956
-.0309127
d85 | -.0780454
.0385625
-2.02
0.046
-.1546683
-.0014224
d86 | -.0420791
.0428788
-0.98
0.329
-.1272783
.0431201
d87 | -.0270426
.0381447
-0.71
0.480
-.1028353
.0487502
_cons | -2.082293
.8647054
-2.41
0.018
-3.800445
-.3641423
----------------------------------------------------------------------------. drop uhat uhat_1

b. We lose the first year, 1981, when we add the lag of logcrmrte:
. gen lcrmrte_1  l.lcrmrte
(90 missing values generated)
. reg lcrmrte lcrmrte_1 lprbarr lprbconv lprbpris lavgsen lpolpc d83-d87
Source |
SS
df
MS
------------------------------------------Model | 163.287174
11 14.8442885
Residual | 16.8670945
528 .031945255
------------------------------------------Total | 180.154268
539 .334237975

Number of obs
F( 11,
528)
Prob  F
R-squared
Adj R-squared
Root MSE








540
464.68
0.0000
0.9064
0.9044
.17873

----------------------------------------------------------------------------lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lcrmrte_1 |
.8263047
.0190806
43.31
0.000
.7888214
.8637879
lprbarr | -.1668349
.0229405
-7.27
0.000
-.2119007
-.1217691
lprbconv | -.1285118
.0165096
-7.78
0.000
-.1609444
-.0960793
lprbpris | -.0107492
.0345003
-0.31
0.755
-.078524
.0570255
lavgsen | -.1152298
.030387
-3.79
0.000
-.174924
-.0555355
lpolpc |
.101492
.0164261
6.18
0.000
.0692234
.1337606
d83 | -.0649438
.0267299
-2.43
0.015
-.1174537
-.0124338
d84 | -.0536882
.0267623
-2.01
0.045
-.1062619
-.0011145
d85 | -.0085982
.0268172
-0.32
0.749
-.0612797
.0440833
d86 |
.0420159
.026896
1.56
0.119
-.0108203
.0948522
d87 |
.0671272
.0271816
2.47
0.014
.0137298
.1205245
_cons | -.0304828
.1324195
-0.23
0.818
-.2906166
.229651
-----------------------------------------------------------------------------

Not surprisingly, the coefficient on the lagged crime rate is very large and statistically
significant. Further, including it makes all other coefficients much smaller in magnitude. The
96

variable logprbpris now has a negative sign, although it is insignificant. Adding the lagged
crime rate does not change the positive coefficient on the size of the police force: it is smaller
but now even more statistically significant.
c. There is little evidence of serial correlation in the model with a lagged dependent
variable. The coefficient on û t−1 is small and statistically insignificant:
. predict uhat, resid
(90 missing values generated)
. gen uhat_1  l.uhat
(180 missing values generated)
. reg lcrmrte lcrmrte_1 lprbarr lprbconv lprbpris lavgsen lpolpc d84-d87
uhat_1
Source |
SS
df
MS
------------------------------------------Model | 138.488359
11 12.5898508
Residual | 14.8729012
438 .033956395
------------------------------------------Total |
153.36126
449 .341561826

Number of obs
F( 11,
438)
Prob  F
R-squared
Adj R-squared
Root MSE








450
370.77
0.0000
0.9030
0.9006
.18427

----------------------------------------------------------------------------lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lcrmrte_1 |
.829714
.0248121
33.44
0.000
.7809485
.8784796
lprbarr | -.1576381
.0278786
-5.65
0.000
-.2124305
-.1028457
lprbconv | -.1293032
.0191735
-6.74
0.000
-.1669868
-.0916197
lprbpris | -.0040031
.0395191
-0.10
0.919
-.0816738
.0736675
lavgsen | -.1241479
.034481
-3.60
0.000
-.1919166
-.0563791
lpolpc |
.1107055
.0187613
5.90
0.000
.0738323
.1475788
d84 |
.0103772
.0277393
0.37
0.709
-.0441415
.0648959
d85 |
.0557956
.0277577
2.01
0.045
.0012407
.1103505
d86 |
.107831
.0277087
3.89
0.000
.0533724
.1622895
d87 |
.1333345
.0279635
4.77
0.000
.0783751
.1882938
uhat_1 | -.0592978
.0601101
-0.99
0.324
-.177438
.0588423
_cons |
.0126059
.1524765
0.08
0.934
-.2870706
.3122823
-----------------------------------------------------------------------------

d. None of the logwage variables is statistically significant, and the magnitudes are
pretty small in all cases. The p-value for the joint test, made fully robust, is . 33, which means
the logwage variables are jointly insignificant, too. (Plus, the different signs on the wage
variables is hard to explain, except to conclude that each is estimated with substantial sampling

97

error.)
. reg lcrmrte lcrmrte_1 lprbarr lprbconv lprbpris lavgsen lpolpc d83-d87
lwcon-lwloc, cluster(county)
Number of obs 
F( 20,
89) 
Prob  F

R-squared

Root MSE


Linear regression

540
398.63
0.0000
0.9077
.17895

(Std. Err. adjusted for 90 clusters in county
----------------------------------------------------------------------------|
Robust
lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lcrmrte_1 |
.8087768
.0406432
19.90
0.000
.7280195
.889534
lprbarr | -.1746053
.0495539
-3.52
0.001
-.2730678
-.0761428
lprbconv | -.1337714
.0289031
-4.63
0.000
-.1912012
-.0763415
lprbpris | -.0195318
.0404094
-0.48
0.630
-.0998243
.0607608
lavgsen | -.1108926
.0455404
-2.44
0.017
-.2013804
-.0204049
lpolpc |
.1050704
.0575404
1.83
0.071
-.0092612
.219402
d83 | -.0729231
.0293628
-2.48
0.015
-.1312664
-.0145799
d84 | -.0652494
.0226239
-2.88
0.005
-.1102026
-.0202962
d85 | -.0258059
.0413435
-0.62
0.534
-.1079545
.0563428
d86 |
.0263763
.0393741
0.67
0.505
-.0518591
.1046118
d87 |
.0465632
.0441727
1.05
0.295
-.041207
.1343334
lwcon | -.0283133
.0272813
-1.04
0.302
-.0825207
.025894
lwtuc | -.0034567
.0208431
-0.17
0.869
-.0448715
.0379582
lwtrd |
.0121236
.0496718
0.24
0.808
-.0865733
.1108205
lwfir |
.0296003
.0184296
1.61
0.112
-.0070189
.0662195
lwser |
.012903
.0269695
0.48
0.634
-.0406847
.0664908
lwmfg | -.0409046
.0508117
-0.81
0.423
-.1418664
.0600573
lwfed |
.1070534
.0760639
1.41
0.163
-.044084
.2581908
lwsta | -.0903894
.0548237
-1.65
0.103
-.199323
.0185442
lwloc |
.0961124
.1355681
0.71
0.480
-.1732585
.3654833
_cons | -.6438061
.7958054
-0.81
0.421
-2.225055
.9374423
----------------------------------------------------------------------------. testparm lwcon-lwloc
(
(
(
(
(
(
(
(
(

1)
2)
3)
4)
5)
6)
7)
8)
9)

lwcon
lwtuc
lwtrd
lwfir
lwser
lwmfg
lwfed
lwsta
lwloc
F(











0
0
0
0
0
0
0
0
0

9,
89) 
Prob  F 

1.15
0.3338

7.12. Wealth at the beginning of the year cannot be strictly exogenous in a savings

98

equation: if saving increases unexpectedly this year – so that the disturbance in year t is
positive – beginning of year wealth is higher next year. This is analogous to Example 7.8,
where cumulative grade point average at the start of the semester cannot be strictly exogenous
in an equation to explain current-term GPA.
7.13. a. The Stata output is below. Married men are estimated to have a scoring average
about 1. 2 points higher, and assists are . 42 higher. The coefficient in the rebounds equation is
−. 24, but it is not statistically significant. The coefficient in the assists equation is significant
at the 5% level against a two-sided alternative (p-value  . 048).
. use nbasal
. sureg (points rebounds assists 

age exper expersq coll guard forward black

Seemingly unrelated regression
---------------------------------------------------------------------Equation
Obs Parms
RMSE
"R-sq"
chi2
P
---------------------------------------------------------------------points
282
8
5.352116
0.1750
59.80
0.0000
rebounds
282
8
2.375338
0.3123
128.07
0.0000
assists
282
8
1.64516
0.3727
167.51
0.0000
-------------------------------------------------------------------------------------------------------------------------------------------------|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------points
|
age | -1.214936
.27656
-4.39
0.000
-1.756984
-.6728889
exper |
2.261943
.3759275
6.02
0.000
1.525138
2.998747
expersq | -.0649631
.0204961
-3.17
0.002
-.1051347
-.0247915
coll | -1.011535
.4026169
-2.51
0.012
-1.800649
-.2224201
guard |
1.997013
.9380542
2.13
0.033
.1584603
3.835565
forward |
1.348821
.9390513
1.44
0.151
-.4916863
3.189327
black |
1.476842
.8171698
1.81
0.071
-.1247815
3.078465
marr |
1.236043
.696663
1.77
0.076
-.1293911
2.601478
_cons |
34.8283
6.771391
5.14
0.000
21.55662
48.09998
---------------------------------------------------------------------------rebounds
|
age | -.2818077
.1227409
-2.30
0.022
-.5223754
-.04124
exper |
.830967
.1668415
4.98
0.000
.5039637
1.15797
expersq | -.0344878
.0090964
-3.79
0.000
-.0523165
-.0166591
coll | -.3689707
.1786866
-2.06
0.039
-.71919
-.0187514
guard | -2.727081
.4163206
-6.55
0.000
-3.543054
-1.911107
forward |
.0896382
.4167631
0.22
0.830
-.7272024
.9064789
black |
1.003824
.3626705
2.77
0.006
.2930028
1.714645
marr | -.2406585
.309188
-0.78
0.436
-.8466559
.3653389
_cons |
10.87601
3.005231
3.62
0.000
4.985864
16.76615

99

---------------------------------------------------------------------------assists
|
age | -.3013925
.0850104
-3.55
0.000
-.4680097
-.1347752
exper |
.6633331
.1155545
5.74
0.000
.4368506
.8898157
expersq | -.0222961
.0063002
-3.54
0.000
-.0346442
-.009948
coll | -.1894703
.1237584
-1.53
0.126
-.4320323
.0530916
guard |
2.478626
.2883437
8.60
0.000
1.913482
3.043769
forward |
.4804238
.2886502
1.66
0.096
-.0853202
1.046168
black | -.1528242
.2511857
-0.61
0.543
-.645139
.3394907
marr |
.4236511
.2141437
1.98
0.048
.0039371
.843365
_cons |
7.501437
2.081423
3.60
0.000
3.421922
11.58095
-----------------------------------------------------------------------------

b. The Stata test command gives
. test marr
( 1)
( 2)
( 3)

[points]marr  0
[rebounds]marr  0
[assists]marr  0
chi2( 3) 
Prob  chi2 

12.02
0.0073

The rejection is very strong, presumably coming mainly from the points and assists
equations. Rather than thinking being married causes a basketball player to be more
productive, it could be that the more productive players – at least when it comes to points and
assists – are more likely to be married.
̂ and let  be the estimator that uses 
̂ . Because
7.14. Let ̂ be the estimator that uses 
SGLS.1 to SGLS.3 hold,
Avar N ̂ −   EX ′i  −1 X i  −1 .
Further, we know from the general result for FGLS,
Avar N  −   EX ′i  −1 X i  −1 EX ′i  −1 u i u ′i  −1 X i EX ′i  −1 X i  −1 .
Now, because Eu i u ′i |X i   , it follows that
EX ′i  −1 u i u ′i  −1 X i   EX ′i  −1  −1 X i 
by a simple iterated expectations argument. So, we have to show that

100

EX ′i  −1 X i  −1 EX ′i  −1  −1 X i EX ′i  −1 X i  −1 − EX ′i  −1 X i  −1
is positive semi-definite. We use the standard trick of showing that
EX ′i  −1 X i  − EX ′i  −1 X i EX ′i  −1  −1 X i  −1 EX ′i  −1 X i 
is positive semi-definite. To this end, define Z i ≡  −1/2 X i and W i ≡  −1/2  −1 X i . Then
straightforward algebra shows that the difference above can be written as
EZ ′i Z i  − EZ ′i W i EW ′i W i  −1 EW ′i Z i  which is easily seen to be ER ′i R i , where R i is the
G  K matrix of population residuals from the regression of Z i on W i : R i  Z i − W i  where
  EW ′i W i  −1 EW ′i Z i . Matrices of the form ER ′i R i  are always positive semi-definite
because for a nonrandom vector a, a ′ ER ′i R i a EaR i  ′ aR i  ≥ 0.
′
7.15. Let ̂  ̂ , ̂ ′  ′ be the FGLS estimator from the full model. Then, because SGLS.1

through SGLS.3 hold, we know
Avar N ̂ −   EW ′i  −1 W i  −1
where W i  X i , Z i . Using partitioned matrix multiplication,
EW ′i  −1 W i 

EX ′i  −1 X i  EX ′i  −1 Z i 



EZ ′i  −1 X i  EZ ′i  −1 Z i 

.

Further, because EX i ⊗ Z i   0, it follows that EX ′i  −1 Z i   0. Therefore, EW ′i  −1 W i  is
block diagonal and is equal to
EX ′i  −1 X i 

0

0

EZ ′i  −1 Z i 

Inverting this matrix gives

101

Avar N ̂ −  

EX ′i  −1 X i  −1

0

0

EZ ′i  −1 Z i  −1

and Avar N ̂ −  is the upper left hand block:
Avar N ̂ −   EX ′i  −1 X i  −1 .
Now let ̃ be the FGLS estimator from y i  X i   v i . We know this estimator is consistent
for  because v i  Z i   u i , and so
EX i ⊗ v i   0
because EX i ⊗ Z i   0 and Eu i |X i , Z i   0. Now, FGLS of y i  X i   v i using a consistent
estimator of   Ev i v i  generally has asymptotic variance
EX ′i  −1 X i  −1 EX ′i  −1 v i v ′i  −1 X i EX ′i  −1 X i  −1
Let r i  Z i  so that we can write
v i v ′i  r i  u i r i  u i  ′  r i r ′i  r i u ′i  u ′i r i  u i u ′i .
Now Er i u ′i |X i   0 because Eu i |X i , Z i   0 and r i is a function of Z i . Therefore,
Ev i v ′i |X i   Er i r ′i |X i   Eu i u ′i |X i   Er i r ′i |X i   .
Using iterated expectations,
EX ′i  −1 v i v ′i  −1 X i   EEX ′i  −1 v i v ′i  −1 X i |X i   EX ′i  −1 Ev i v ′i |X i  −1 X i 
 EX ′i  −1 Er i r ′i |X i  −1 X i   EX ′i  −1  −1 X i 
 EX ′i  −1 r i r ′i  −1 X i   EX ′i  −1  −1 X i 
We have shown that
−1
−1
′ −1
′ −1
′ −1
Avar N ̃ −   A −1
2 EX i  r i r i  X i   EX i   X i A 2
−1
−1
−1
−1
′ −1
′ −1
′ −1
 A −1
2 EX i  r i r i  X i A 2  A 2 EX i   X i A 2
−1
−1
′ −1
≡ C 2  A −1
2 EX i   X i A 2

102

−1
′ −1
′ −1
where A 2 ≡ EX ′i  −1 X i  and C 2 ≡ A −1
2 EX i  r i r i  X i A 2 . Note that C 2 is positive

semi-definite.
Now, Problem 7.14 established that
−1
−1
−1
′ −1
A −1
2 EX i   X i A 2 − A 1

is positive semi-definite. Therefore,
−1
−1
−1
′ −1
Avar N ̃ −  − Avar N ̂ −   C 2  A −1
2 EX i   X i A 2 − A 1 

and each matrix is positive-semi-definite. We have shown the result.
Interestingly, the proof shows that the asymptotic inefficiency of ̃ has two sources. First,
we have omitted variables that are uncorrelated with X i . The second piece is due to using the
wrong variance matrix, . If we could effectively use  in obtaining the estimator with Z i
omitted – which we can in principle if we observe Z i – then the only source of inefficiency
would be due to omitting Z i (as happens in the single-equation case).

103

Solutions to Chapter 8 Problems
8.1. Letting Qb denote the objective function in equation (8.27), it follows from
multivariable calculus that
∂Qb ′
 −2
∂b

′

N

∑

Z ′i X i

N

Ŵ

i1

∑ Z ′i y i − X i b

.

i1

Evaluating the derivative at the solution ̂ gives
′

N

∑
i1

Z ′i X i

N

Ŵ

∑ Z ′i y i − X i ̂

 0.

i1

In terms of full data matrices, we can write, after simple algebra,
′
′
X ′ ZŴZ X̂  X ′ ZŴZ Y.

Solving for ̂ gives (8.28).
8.2. a. We can apply general GMM theory to obtain consistency and N asymptotic
normality of the 3SLS estimator (GMM version). The four assumptions given in the problem
N
̂ Zi
are sufficient for SIV.1 to SIV.3, where Ŵ  N −1 ∑ i1 Z ′i 

−1

and

̂   ≡ Eu i u ′i , something that holds
W ≡ EZ ′i Z i  −1  plimŴ. (This assumes plim 
quite generally.) However, without SIV.5, 3SLS is not necessarily an asymptotically efficient
GMM estimator.
b. The asymptotic variance of the 3SLS estimator is given in equation (8.29) with the
choice of W in part a:
Avar N ̂ 3SLS −   C ′ WC −1 C ′ WWCC ′ WC −1 ,
where  ≡ EZ ′i u i u ′i Z i , as in the text. (Note this expression collapses to C ′ WC −1 when

104

  W −1 , as happens under SIV.5.)
c. A consistent estimator of Avar N ̂ 3SLS −  is given in equation (8.31) with
̂ ≡ N −1 ∑ N Z ′i û i û ′i Z i  and û i  y − X i ̂

i
3SLS the 3SLS residuals:
i1
N

X ′ Z/NŴZ X/N −1 X ′ Z/NŴ N −1 ∑ Z ′i û i û ′i Z i ŴZ ′ X/NX ′ Z/NŴZ X/N −1 .
′

′

i1

The estimator of Avar̂ 3SLS  is simply this expression divided by N. Even though the
formula looks complicated, it can be programmed fairly easily in a matrix-based language. Of
course, if we doubt SIV.5 in the first place, we would probably use the more general minimum
chi-square estimator, as it is asymptotically more efficient. (If we were going to obtain the
robust variance matrix estimate for 3SLS anyway, it is no harder to obtain the minimum
chi-square estimate and its asymptotic variance estimate.)
8.3. First, we can always write x as its linear projection plus an error: x  x ∗  e, where
x ∗  z and Ez ′ e  0. Therefore, Ez ′ x  Ez ′ x ∗ , which verifies the first part of the hint.
To verify the second step, let h ≡ hz, and write the linear projection as
Ly|z, h  z 1  h 2
where  1 is M  K and  2 is Q  K. Then we must show that  2  0. But, from the two-step
projection theorem (see Property LP.7 in Chapter 2),
 2  Es ′ s −1 Es ′ r, where s ≡ h − Lh|z and r ≡ x − Lx|z.
Now, by the assumption that Ex|z  Lx|z, r is also equal to x − Ex|z. Therefore,
Er|z  0, and so r is uncorrelated with all functions of z. But s is simply a function of z since
h ≡ hz. Therefore, Es ′ r  0, and this shows that  2  0.
8.4.a. For the system in (8.12), we show that, for each g, rank Ez, h ′ x g   rank Ez ′ x g 

105

for any function hhz. Now, by Problem 8.3, Lx g |z, h  Lx g |z  z 1 when Ex g |z is
linear in z and h is any function of z. As in Problem 8.3, Ez ′ x g   Ez ′ x ∗g   Ez ′ z 1 . Also,
if we let e g x g − x ∗g , then Eh ′ e g   0, and so Ez, h ′ x g   Ez, h ′ x ∗g   Ez, h ′ z 1 . But
rank Ez, h ′ z  rank Ez ′ z, which means that rank Ez, h ′ z 1  rank Ez ′ z 1 . We
have shown that rank Ez, h ′ x g   rank Ez ′ x g , which means adding h to the instrument list
does not help satisfy the rank condition.
b. If Ex g |z is nonlinear in z, then Lx g |z, h will generally depend on h. This can
certainly help in satisfying the rank condition. For example, if K g  M (the dimension of z)
then the order condition fails for equating g using instruments z. But we can add nonlinear
functions of z to the instrument list that are partially correlated with x g and satisfy the order
and rank condition. We use this fact in Section 9.5.
8.5. This follows directly from the hint. Straightforward matrix algebra shows that
C ′  −1 C−C ′ WCC ′ WWC −1 C ′ WC can be written as
C ′  −1/2 I L − DD ′ D −1 D ′  −1/2 C,
where D ≡  1/2 WC. Since this is a matrix quadratic form in the L  L symmetric, idempotent
matrix I L − DD ′ D −1 D ′ , it is necessarily itself positive semi-definite.
8.6. a. First,  −1 u i   11 u i1   12 u i2 ,  12 u i1   22 u i2  ′ . Therefore,
Z ′i  −1 u i 



z ′i1

0

0

z ′i2

 11 u i1   12 u i2 ,  12 u i1   22 u i2  ′

z ′i1  11 u i1   12 u i2 
z ′i2  12 u i1   22 u i2 

.

The expected value of this vector depends on Ez ′i1 u i1 , Ez ′i1 u i2 , Ez ′i2 u i1 , and Ez ′i2 u i2 . If

106

Ez ′i1 u i2  ≠ 0 or Ez ′i2 u i1  ≠ 0 then EZ ′i  −1 u i  ≠ 0 except by fluke. In fact, if Ez ′i1 u i1   0,
Ez ′i2 u i2   0, and  12 ≠ 0 then EZ ′i  −1 u i  ≠ 0 if Ez ′i1 u i2  ≠ 0 or Ez ′i2 u i1  ≠ 0.
b. When  12  0,  12  0, in which case Ez ′i1 u i1   0 and Ez ′i2 u i2   0 imply
EZ ′i  −1 u i   0.
c. If the same instruments are valid in each equation – so Ez ′i u i1   Ez ′i u i2   0 – then
EZ ′i  −1 u i   0 without restrictions on .
̂ is diagonal and z i has the form in (8.15), ∑ N Z ′i 
̂ Z i  Z ′ I N ⊗ 
̂ Z is a
8.7. When 
i1
N

block diagonal matrix with g th block ̂ 2g ∑ i1 z ′ig z ig

≡ ̂ 2g z ′g z, where Z g denotes the N  L g

observation matrix of instruments for the g th equation. Further, Z ′ X is block diagonal with g th
block Z ′g X g . Using these facts, it is now straightforward to show that the 3SLS estimator
consists of X ′g Z g Z ′g Z g  −1 Z ′g X g  −1 X ′g Z g Z ′g Z g  −1 Z ′g Y g stacked from g  1, … , G. This is just
the system 2SLS estimator or, equivalently, 2SLS equation-by-equation.
8.8. a. With Z 1  z ′i1 , z ′i2 , … , z iT  ′ and X i  x ′i1 , x ′i2 , … , x ′iT  ′ ,
T

Z ′i Z i



∑

T

z ′it z it ,

Z ′i X i



t1

∑

T

z ′it x it ,

and

Z ′i y i

t1



∑ z ′it y it .
t1

Summing over all i gives
N

′

ZZ

T

∑∑

N

z ′it z it ,

i1 t1

T

b. rank E ∑ t1 z ′it x it

′

ZX 

T

∑∑

N

z ′it x it ,

i1 t1

′

and Z Y 

T

∑ ∑ z ′it y it .
i1 t1

 K.

c. Let û i be the T  1 vector of pooled 2SLS residuals, û i  y i − X i ̂. Then we just use
̂  N −1 ∑ N Z ′i û i û ′i Z i , cancelling N everywhere:
(8.31) with Ŵ  Z ′ Z/N −1 and 
i1

107

N

X ′ ZZ ′ Z −1 Z ′ X −1 X ′ ZZ ′ Z −1

∑ Z ′i û i û ′i Z i

 Z ′ Z −1 Z ′ XX ′ ZZ ′ Z −1 Z ′ X −1 . (8.67)

i1

d. Using reasoning almost identical to Problem 7.7, (8.65) implies that, for s  t,
Eu it u is z ′it z is   EEu it u is z ′it z is |z it , u is , z is 
 EEu it |z ′it , u is , z is u is , z ′it , z is 
 E0  u is z ′it , z is   0
because Eu it |z it , u is , z is   0 for s  t. A similar argument works for t  s. So for all t ≠ s,
Eu it u is z ′it , z is   0.
Similarly, (8.66) and iterated expectations implies that
Eu 2it z ′it z it   EEu 2it z ′it z it |z it 
 EEu 2it |z it z ′it z it    2 Ez ′it z it , t  1, … T.
Together, these results imply that
T

Varz ′i u i 



2

∑ Ez ′it z it .
t1

N

T

A consistent estimator of this matrix is ̂ 2 Z ′ Z/N, where ̂ 2  1/NT ∑ i1 ∑ t1 û 2it , by the
usual law-of-large-numbers arguments. A degrees of freedom adjustment replaces NT with
N
NT − K. Replacing ∑ i1 Z ′i û i û ′i Z i in (8.67) with ̂ 2 Z ′ Z [since ̂ 2 Z ′ Z/N can play the role of

̂ under the maintained assumptions] and cancelling gives the estimated asymptotic variance

of ̂ as
̂ 2 X ′ ZZ ′ Z −1 Z ′ X −1 .
This is exactly the variance estimator that would be computed from the pooled 2SLS
estimation. This means that the usual 2SLS standard errors and test statistics are

108

asymptotically valid.
e. If the unconditional variance changes across t, the simplest approach is to weight the
variables in each time period by 1/̂ t , where ̂ 2t is a consistent estimator of  2t  Varu it . A
consistent estimator of ̂ 2t is
N

̂ 2t

N

−1

∑ û 2it .
i1

Now, apply pooled 2SLS to the equation
y it /̂ t   x it /̂ t   error it
using instruments z it /̂ t . The usual statistics from this procedure are asymptotically valid: it
can be shown that it has the same N -asymptotic distribution as a if we knew the  2t . This
estimator is a generalized instrumental variables (GIV) estimator except it is consistent under
the contemporaneous exogeneity assumption only. It turns out to be identical to the GMM
N

T

estimator that uses weighting matrix N −1 ∑ i1 ∑ t1 ̂ 2t z ′it z it

−1

– the optimal weighting

matrix under the assumptions in the problem. See Im, Ahn, Schmidt, and Wooldridge (1999,
Section 2) for discussion of a more general result.
8.9 The optimal instruments are given in Theorem 8.5, with G  1:
z ∗i  z i  −1 Ex i |z i , z i   Eu 2i |z i .
If Eu 2i |z i    2 and Ex i |z i   z i , then the optimal instruments are  −2 z i . The constant
multiple  −2 clearly has no effect on the optimal IV estimator, so the optimal instruments are
z i . These are the optimal IVs underlying 2SLS, except that  is replaced with its
N -consistent OLS estimator. The 2SLS estimator has the same asymptotic variance whether
̂ is used, and so 2SLS is asymptotically efficient.
 or 

109

If Eu|x  0 and Eu 2 |x   2 , the optimal instruments are  −2 Ex|x   −2 x, and this
leads to the OLS estimator.
8.10.a. Write u it  u i , t−1  e it , and plug into y it  x it   u it to get
y it  x it   u i , t−1  e it , t  2, … , T.
Under the assumption
Eu it |z it , u it , t−1 , x i , t−1 , z i,t−1 , x i , t−2 , … , u i1 , x i1 , z i1   0
the previous assumption satisfies the dynamic completeness assumption when   0. If we
assume that Eu 2it |z it , u i , t−1  is constant under H 0 , then it satisfies the requisite
homoskedasticity assumption as well. As shown in Problem 8.8, pooled 2SLS estimation of
this equation using instruments z it , u i , t−1  results in valid test statistics.
Now we apply the results from Section 6.1.3: when   0, replacing u i , t−1 with the initial
2SLS residuals û i , t−1 has no effect as N gets large, provided that (8.68) holds. Thus, we can
estimate
y it  x it   û i , t−1  error it , t  2, … , T,
by pooled 2SLS using instruments z it , û i , t−1 , and obtain the usual t statistic for ̂ .
b. If Eu 2it |z it , u i , t−1  is not constant, we can use the usual heteroskedasticity-robust t
statistic from pooled 2SLS for ̂ . This allows for dynamic forms of heteroskedasticity, such as
ARCH and GARCH, as well as static forms of heteroskedasticity.
8.11. a. This is a simple application of Theorem 8.5 when G  1. Without the i subscript,
x 1  z 1 , y 2  and so Ex 1 |z  z 1 , Ey 2 |z. Further, z Varu 1 |z  21 . It follows that the
optimal instruments are 1/ 21 z i , Ey 2 |z. Dropping the division by  21 clearly does not affect
the optimal instruments.

110

(8.68)

b. If y 2 is binary then Ey 2 |z  Py 2  1|z  Fz, and so the optimal IVs are z 1 , Fz.
8.12. a. As long as EZ ′ u  0 holds the estimator is consistent. After all, it is a GMM
estimator with a particular weighting matrix that satisfies all of the GMM regularity
conditions.
b. Unless the optimal weighting matrix Ŵ consistently estimates VarZ ′i u i  −1 , the statistic
fails to be asymptotically chi-square.
̂ and 
̂ converge to the same constant matrix   , there is no difference in
c. Since 
asymptotic efficiency (at least using the usual N -asymptotic distribution).
8.13. a. The optimal instrumental variable is
z ∗  Eu 21 |z −1 Ez 1 , y 2 , z 1 y 2 |z   21  −1 z 1 , Ey 2 |z, z 1 Ey 2 |z   21  −1 z 1 , z 2 , z 1 z 2 .
b. The coefficients  2 can be estimated by running an OLS of y 2 on z. Since the inverse of
the variance is a scalar that does not depend on z, it cancels out in the IV estimation. Thus we
can operationalize the optimal IV estimator by using z 1 , z̂ 2 , z 1 z̂ 2  as the IVs. The
estimator as the same N -asymptotic distribution as if we knew  2 .
8.14. a. With y it2  z it  2  v it2 and Ez ′it u it1   0, t  1, . . . , T maintained, Ey ′it2 u it1   0 is
the same as Ey ′it2 u it1   0. We can always write the linear projection of u it1 onto v it2 as
u it1  v it2  1  e it1
Ev ′it2 e it1   0, t  1, . . . , T
where we assume that the coefficients  1 do not change over time. Thus, we can write the
extended equation
y it1   t1  z it1  1  y it2  2  v it2  1  e it1 , t  1, . . . , T
Now the control function procedure is clear. (1) Estimate the reduced form y it2  z it  2  v it2

111

by pooled OLS (equation-by-equation if necessary when y it2 is a vector) and obtain the
residuals, v̂ it2 . (2) Run the pooled OLS regression
y it1 on 1, d2 t , ..., dT t , z it1 , y it2 , v̂ it2 , t  1, . . . , T; i  1, . . . , N.
and use a fully robust Wald test of H 0 :  1  0. The test has G 1 degrees of freedom in the
chi-square distribution, or one can use an F approximation by dividing the chi-square statistic
by G 1 .
b. Extending the discussion in the text around equation (6.32), partition z it2  g it2 , h it2 
where g it2 is 1  G 1 (the same dimension as y it1 ) and h it2 is 1  Q 1 . Obtain the fitted values ŷ it2
from the first-stage regressions. Then, obtain the residuals, r̂ it2 from the pooled OLS regression
h it2 on z it1 , ŷ it2 , t  1, . . . , T; i  1, . . . , N.
Let û it1 be the P2SLS residuals. Then run the pooled OLS regression
û it1 on r̂ it2 , t  1, . . . , T; i  1, . . . , N,
and test the r̂ it2 for joint significance. A fully robust Wald test is most appropriate, and its
limiting distribution under the null that all elements of z it are exogenous is  2Q 1 .
8.15. a. The coefficient shows that a higher fare reduces passenger demand for flights. The
estimated elasticity is −. 565, which is fairly large. Even the fully robust 95% confidence
interval is pretty narrow, from −. 696 to −. 434. Incidentally, the standard error that is robust
only to heteroskedasticity and not serial correlation is about . 0364, which is actually slightly
smaller than the usual OLS standard error. So it is important to use the fully robust version.
.
. use airfare
. xtset id year
panel variable:
time variable:
delta:

id (strongly balanced)
year, 1997 to 2000
1 unit

112

. reg lpassen y98 y99 y00 lfare ldist ldistsq
Source |
SS
df
MS
------------------------------------------Model | 230.557732
6 38.4262887
Residual | 3360.12968 4589 .732213921
------------------------------------------Total | 3590.68741 4595 .781433605

Number of obs
F( 6, 4589)
Prob  F
R-squared
Adj R-squared
Root MSE








4596
52.48
0.0000
0.0642
0.0630
.85569

----------------------------------------------------------------------------lpassen |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y98 |
.0321212
.0357118
0.90
0.368
-.0378911
.1021335
y99 |
.081651
.035724
2.29
0.022
.0116148
.1516873
y00 |
.1380369
.0358761
3.85
0.000
.0677024
.2083713
lfare | -.5647711
.0369644
-15.28
0.000
-.6372392
-.4923031
ldist |
-1.54939
.3265076
-4.75
0.000
-2.189502
-.9092778
ldistsq |
.1227088
.0247935
4.95
0.000
.0741017
.171316
_cons |
13.65144
1.094166
12.48
0.000
11.50635
15.79653
----------------------------------------------------------------------------. reg lpassen y98 y99 y00 lfare ldist ldistsq, cluster(id)
Linear regression

Number of obs
F( 6, 1148)
Prob  F
R-squared
Root MSE







4596
34.95
0.0000
0.0642
.85569

(Std. Err. adjusted for 1149 clusters in id
----------------------------------------------------------------------------|
Robust
lpassen |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y98 |
.0321212
.0050262
6.39
0.000
.0222597
.0419827
y99 |
.081651
.0073679
11.08
0.000
.0671949
.0961072
y00 |
.1380369
.0104857
13.16
0.000
.1174636
.1586101
lfare | -.5647711
.0667107
-8.47
0.000
-.6956597
-.4338826
ldist |
-1.54939
.69818
-2.22
0.027
-2.919242
-.179538
ldistsq |
.1227088
.0524034
2.34
0.019
.0198916
.2255261
_cons |
13.65144
2.316661
5.89
0.000
9.106074
18.1968
-----------------------------------------------------------------------------

b. I use the test that allows the explanatory variables to be non-strictly exogenous. The
estimate of  is essentially one. In a pure time series context we would have to worry how this
amount of persistence in the errors affects inference. Here, inference is standard because it is
with fixed T and N → . But the “unit root” in u it : t  1, . . . , T is of some concern because
it calls into question whether there is a meaningful relationship between passenger demand and
airfares. If the error term rarely returns to its mean (which we can take to be zero), in what

113

sense is do movements in airfare over time cause movements in passenger demand?
. predict uhat, resid
. gen uhat_1  l.uhat
(1149 missing values generated)
. reg lpassen y99 y00 lfare ldist ldistsq uhat_1, robust
Linear regression

Number of obs
F( 6, 3440)
Prob  F
R-squared
Root MSE


3447
 7168.14
 0.0000
 0.9647

.1684

----------------------------------------------------------------------------|
Robust
lpassen |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y99 |
.0502195
.0065875
7.62
0.000
.0373036
.0631354
y00 |
.1105098
.0072252
15.30
0.000
.0963437
.124676
lfare |
-.628955
.0095767
-65.68
0.000
-.6477315
-.6101784
ldist | -1.549142
.0726222
-21.33
0.000
-1.691528
-1.406755
ldistsq |
.1269054
.0055092
23.04
0.000
.1161037
.1377071
uhat_1 |
1.005428
.0062555
160.73
0.000
.9931627
1.017693
_cons |
13.81801
.2389316
57.83
0.000
13.34955
14.28647
-----------------------------------------------------------------------------

c. The coefficient on concen it is . 360 and the t-statistic that accounts for heteroskedasticity
and serial correlation is 6. 15. Therefore, the partial correlation between lfare and concen is
enough to implement an IV procedure.
. reg lfare y98 y99 y00 ldist ldistsq concen, cluster(id)
Linear regression

Number of obs
F( 6, 1148)
Prob  F
R-squared
Root MSE







4596
205.63
0.0000
0.4062
.33651

(Std. Err. adjusted for 1149 clusters in id
----------------------------------------------------------------------------|
Robust
lfare |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y98 |
.0211244
.0041474
5.09
0.000
.0129871
.0292617
y99 |
.0378496
.0051795
7.31
0.000
.0276872
.048012
y00 |
.09987
.0056469
17.69
0.000
.0887906
.1109493
ldist | -.9016004
.2719464
-3.32
0.001
-1.435168
-.3680328
ldistsq |
.1030196
.0201602
5.11
0.000
.0634647
.1425745
concen |
.3601203
.058556
6.15
0.000
.2452315
.4750092
_cons |
6.209258
.9117551
6.81
0.000
4.420364
7.998151
-----------------------------------------------------------------------------

114

d. The IV estimates are given below. The estimated elasticity is huge, −1. 78. This seems
very large. The fully robust standard error is about twice as large as the usual OLS standard
error, and the fully robust 95% confidence interval is −2. 71 to −. 84, which is very wide, but it
excludes the point estimate from pooled OLS (−. 5. 65).
. ivreg lpassen y98 y99 y00 ldist ldistsq (lfareconcen)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | -556.334915
6 -92.7224858
Residual | 4147.02233 4589 .903687586
------------------------------------------Total | 3590.68741 4595 .781433605

Number of obs
F( 6, 4589)
Prob  F
R-squared
Adj R-squared
Root MSE








4596
20.45
0.0000
.95062

----------------------------------------------------------------------------lpassen |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lfare | -1.776549
.2358788
-7.53
0.000
-2.238985
-1.314113
y98 |
.0616171
.0400745
1.54
0.124
-.0169481
.1401824
y99 |
.1241675
.0405153
3.06
0.002
.044738
.2035971
y00 |
.2542695
.0456607
5.57
0.000
.1647525
.3437865
ldist | -2.498972
.4058371
-6.16
0.000
-3.294607
-1.703336
ldistsq |
.2314932
.0345468
6.70
0.000
.1637648
.2992216
_cons |
21.21249
1.891586
11.21
0.000
17.50407
24.9209
----------------------------------------------------------------------------Instrumented: lfare
Instruments:
y98 y99 y00 ldist ldistsq concen
----------------------------------------------------------------------------. ivreg lpassen y98 y99 y00 ldist ldistsq (lfareconcen), cluster(id)
Instrumental variables (2SLS) regression

Number of obs
F( 6, 1148)
Prob  F
R-squared
Root MSE







4596
28.02
0.0000
.95062

(Std. Err. adjusted for 1149 clusters in id
----------------------------------------------------------------------------|
Robust
lpassen |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lfare | -1.776549
.4753368
-3.74
0.000
-2.709175
-.8439226
y98 |
.0616171
.0131531
4.68
0.000
.0358103
.0874239
y99 |
.1241675
.0183335
6.77
0.000
.0881967
.1601384
y00 |
.2542695
.0458027
5.55
0.000
.164403
.3441359
ldist | -2.498972
.831401
-3.01
0.003
-4.130207
-.8677356
ldistsq |
.2314932
.0705247
3.28
0.001
.0931215
.3698649
_cons |
21.21249
3.860659
5.49
0.000
13.63775
28.78722

115

----------------------------------------------------------------------------Instrumented: lfare
Instruments:
y98 y99 y00 ldist ldistsq concen
-----------------------------------------------------------------------------

e. To compute the asymptotic standard error of N ̂ 1,P2SLS − ̂ 1,POLS  using the traditional
Hausman approach, we have to maintain enough assumptions so that POLS is relatively
efficient under the null. Letting
w it  1, y98 t , y99 t , y00 t , lfare it , ldist i , ldist 2i , concen it 
we would have to assume, under H 0 ,
Ew ′it u it1   0, t  1, . . . , T
Eu 2it1 |w it    2 , t  1, . . . , T
Eu it1 u ir1 |w it , w ir   0, r ≠ t.
The first assumption must be maintained under the null for the test to make sense. The second
assumption – homoskedasticity – can never be guaranteed, and so it is always a good idea to
make tests robust to heteroskedaticity. The current application is a static equation, and so the
assumption of no serial correlation is especially strong. In fact, from part b we already have
good evidence that there is substantial serial correlation in the errors (although this test
maintains contemporaneous exogeneity of lfare it , along with the distance variables).
f. The Stata commands are given below. The fully robust t statistic on v̂ it2 is 2. 92, which is
a strong rejection of the null that lfare it is (contemporaneousl) exogenous – assuming that
concen it is contemporaneously exogenous.
. qui reg lfare y98 y99 y00 ldist ldistsq concen
. predict v2hat, resid
. reg lpassen y98 y99 y00 lfare ldist ldistsq v2hat, cluster(id)
Linear regression

Number of obs
F( 7, 1148)
Prob  F
R-squared

116






4596
31.50
0.0000
0.0711

Root MSE



.85265

(Std. Err. adjusted for 1149 clusters in id
----------------------------------------------------------------------------|
Robust
lpassen |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------y98 |
.0616171
.0112127
5.50
0.000
.0396175
.0836167
y99 |
.1241675
.0160906
7.72
0.000
.0925973
.1557378
y00 |
.2542695
.040158
6.33
0.000
.1754782
.3330608
lfare | -1.776549
.4197937
-4.23
0.000
-2.600198
-.9528999
ldist | -2.498972
.767078
-3.26
0.001
-4.004004
-.9939395
ldistsq |
.2314932
.0640361
3.62
0.000
.1058524
.3571341
v2hat |
1.249653
.4273322
2.92
0.004
.4112137
2.088093
_cons |
21.21249
3.46901
6.11
0.000
14.40618
28.0188
-----------------------------------------------------------------------------

117

8.16 (Bonus Question). Consider the GIV estimator with incorrect restrictions imposed on
p
̂ in place of  with 
̂ →
the estimator of . That is, in (8.47) use 
 ≠ .

a. If Assumption GIV.1 holds, that is EZ i ⊗ u i   0, argue that the GIV estimator is still
consistent under an appropriate rank condition (and state the rank condition).
̂ is
b. Argue that, under the assumptions of part a, the GIV estimator that uses 
N −asymptotically equivalent to the (infeasible) GIV estimator that uses .
̂ but want to guard against inappropriate inference, what would
c. If you insist on using 
you do?
Solution
a. From equation (8.47), and applying the law of large numbers, the key orthogonality
condition for consistency is
EZ ′i  −1 u i   0
p
̂ →
. But if Assumption GIV.1 holds, any linear combination of Z i is uncorrelated
because 

with u i , including  −1 Z i . There are two parts to the rank condition, with the first being the
most important:
rank EZ ′i  −1 Z i   L
rank EZ ′i  −1 X i   K
b. This follows the same line of reasoning that we used for FGLS in Chapter 7 can be used.
First, using the same trick with the Kronecker product,
N

N

−1

∑
i1

̂ −1 Z i
Z ′i 

N

N

−1

∑ Z ′i  −1 Z i  o p 1 → EZ ′i  −1 Z i 
p

i1

N

N

i1

i1

p
̂ −1 X i  N −1 ∑ Z ′i  −1 X i  o p 1 →
EZ ′i  −1 X i 
N −1 ∑ Z ′i 

118

Second,
N

N

N

i1

i1

i1

̂ −1 u i − N −1/2 ∑ Z ′i  −1 u i  N −1/2 ∑u i ⊗ Z i  ′ vec
̂ −1 −  −1 
N −1/2 ∑ Z ′i 
 O p 1  o p 1  o p 1.
Combining these asymptotic equivalances shows that replacing  with the consistent estimator
̂ does not affect the N -limiting distribution of the GIV estimator.

c. Use a full robust asymptotic variance matrix estimator. Write
−1
̂ Â −1
Avar N ̂ −   Â B

where
N

Â

N

−1

∑

̂
X ′i 

−1

N

Zi

N

−1

i1

̂ 
B

N

N

−1

∑

̂
Z ′i 

Zi

N

N

−1

i1

̂ −1 Z i
X ′i 

i1

N

N

−1

∑

N

−1

∑

∑ Z ′i ̂ −1 X i
i1

−1

̂ −1 Z i
Z ′i 

N

N

i1

N



∑

−1
−1

−1

∑ Z ′i ̂ −1 û i û ′i ̂ −1 Z i
i1

−1

̂ −1 Z i
Z ′i 

N

N

i1

−1

∑ Z ′i ̂ −1 X i
i1

where û i  y i − X i ̂ are the GIV residuals. This asymptotic variance matrix estimator allows
Eu i u ′i  ≠  as well as system heteroskedasticity, that is, Eu i u ′i |Z i  ≠ Eu i u ′i . Of course, we
−1
̂ Â −1 /N, whereby all of the divisions by N disappear.
get Avar̂ as Â B

8.17 (Bonus Question). Consider a panel data model with contemporaneously exogenous
instruments z it :
y it1  x it   u it ,
Ez ′it u it   0, t  1, . . . , T,
where x it is 1  K and z it is 1  L for all t, L ≥ K.

119

a. If we maintain the assumptions
ASSUMPTION P2SLS.1: Ez ′it u it   0, t  1, . . . , T
T

T

ASSUMPTION P2SLS.2: (a) rank ∑ t1 Ez ′it z it   L; (b) rank ∑ t1 Ez ′it x it   K,
argue that the pooled 2SLS (P2SLS) estimator is generally consistent (as always with T
fixed, N → , and random sampling across i.
b. Explain how to estimate the asymptotic variance matrix of the P2SLS estimator under
the assumptions in part a.
c. Suppose we add the assumption
ASSUMPTION P2SLS.3: (a) Eu 2it z ′it z it    2 Ez ′it z it , t  1, . . . , T; (b) Eu it u ir z ′it z is   0,
t ≠ r.
Argue that the usual 2SLS variance matrix estimator that assumes homoskedasticity and
ignores the time series component is valid.
d. What would you do if Assumption P2SLS.3(b) holds but not necessarily P2SLS.3(a)?
Solution
a. Using the general formula for the S2SLS estimator, we can write the P2SLS estimator
(with probability approaching one) as
N

T

N −1 ∑ ∑ x ′it z it

̂ 

i1 t1
N



N

−1

N

N −1 ∑ ∑ z ′it z it
N

x ′it z it

N

−1

i1 t1
N

 

T

i1 t1



N

−1

N

∑∑
i1 t1

i1 t1

z ′it z it

N

N

−1

x ′it z it

T

−1

i1 t1

N

N

−1

i1 t1

120

∑ ∑ z ′it y it
N

z ′it z it

T

N −1 ∑ ∑ z ′it x it
i1 t1

−1

T

∑∑

T

i1 t1

N −1 ∑ ∑ z ′it z it

T

−1

T

N −1 ∑ ∑ z ′it x it
−1

T

∑∑

N

i1 t1

N −1 ∑ ∑ x ′it z it
N

−1

i1 t1

T

∑∑

T

N

N

−1

T

∑ ∑ z ′it u it
i1 t1

−1

Notice how the law of large numbers implies
N

T

N −1 ∑ ∑ z ′it z it →
p

i1 t1
N

N

−1

T

∑∑

p
z ′it x it →

i1 t1

T

∑ Ez ′it z it 
t1
T

∑ Ez ′it x it 
t1

and the rank condition states that these matrices of ranks L and K, respectively. Therefore, the
plim can pass through all inverses. We also apply the WLLN and Assumption P2SLS.1 to get
N

N

−1

T

∑∑

p
z ′it u it →

i1 t1

T

∑ Ez ′it u it   0.
t1

p
Now we just pass the plim through using Slutsky’s Theorem to get ̂ → .

b. We have
Avar N ̂ −   A −1 BA −1
where
T

A

∑
t1

∑

∑ Ex ′it z it 
t1

∑ Ez ′it z it 
t1

−1

∑
t1

Ez ′it z it 

T

∑ Ez ′it x it 
t1

T

T



Ez ′it z it 

t1

T

B

−1

T

Ex ′it z it 

T

−1

T

∑ ∑ Eu it u ir z ′it z ir 
t1 r1

∑ Ez ′it x it 
t1

We can consistently estimate each of these matrices:

121

T

N

Â

N

−1

T

N

∑∑

x ′it z it

N

−1

i1 t1

̂ 
B

N

N

−1

N

∑∑

x ′it z it

N

−1

i1 t1

N

N

N

−1

T

∑∑

z ′it z it

−1

T

∑∑

N

∑ ∑ z ′it x it
N

N

−1

−1

i1 t1

T

T

∑ ∑ ∑ û it û ir z ′it z ir
i1 t1 r1

N

z ′it z it

T

i1 t1

−1

i1 t1

N



∑∑

z ′it z it

i1 t1

T

−1

−1

T

T

∑ ∑ z ′it x it
i1 t1

where û it  y it − x it ̂ are the P2SLS residuals.
c. With Assumption P2SLS.3,
T

T

∑∑

T

Eu it u ir z ′it z ir 



t1 r1

∑

T

Eu 2it z ′it z it 



t1

2

∑ Ez ′it z it ,
t1

where the first equality follows from Eu it u ir z ′it z is   0, t ≠ r, and the second follows from
Eu 2it z ′it z it    2 Ez ′it z it , t  1, . . . , T. Therefore,
T

B

2

∑

−1

T

Ex ′it z it 

t1

∑

Ez ′it z it 

t1

T

∑ Ez ′it x it 

  2 A,

t1

and so
Avar N ̂ −    2 A −1
When we use Â from part b and a consistent estimator of  2 (with optional but standard
degrees-of-freedom adjustment),
N

̂ 2 

1
NT − K

then we get

122

T

∑ ∑ û 2it ,
i1 t1

N

T

N

∑ ∑ x ′it z it

Avar̂  ̂ 2

−1

T

∑ ∑ z ′it z it

i1 t1

N

−1

T

∑ ∑ z ′it x it

i1 t1

,

i1 t1

which is exactly the standard formula for 2SLS treating the panel data set as one long cross
section.
d. We need to make the variance matrix robust to heteroskedasticity only. So
̂ 
B

N

N

−1

T

∑∑

N

x ′it z it

N

−1

i1 t1



N

∑∑

z ′it z it

N

N

−1

i1 t1

N

−1

−1

T

−1

T

∑∑

z ′it z it

N

i1 t1

∑ ∑ û 2it z ′it z it
i1 t1

N

−1

T

T

∑ ∑ z ′it x it

.

i1 t1

The resulting Avar̂ is exactly what would be computed by treating the panel data set as one
long cross section with inference robust to heteroskedasticity.
8.18 (Bonus Question). Consider the panel data model
y it  x it   u it , t  1, . . . , T,
where x it is a 1  K vector and the instruments at time t are z it , a 1  L vector for all t. Suppose
the instruments are strictly exogenous in the sense that
Eu it |z i1 , z i2 , . . . , z iT   Eu it |z i   0, t  1, . . . , T.
Assume that Eu i u ′i |z i   Eu i u ′i   , where z i is the vector all all exogenous variables in all
time periods. Further, assume that  has the AR(1) form:

   2e

1



 2   T−1



1

   T−2

2



 









1



 T−1  T−2  

1

123

≡  2e 

where u it  u i,t−1  e it , t  1, . . . , T.
a. If Z ′i  z ′i1 , . . . , z ′iT , find the matrix of transformed instruments,  −1/2 Z i .
b. Describe how to implement the GIV estimator as a particular pooled 2SLS estimation
when  has the AR(1) structure.
c. If you think the AR(1) model might be incorrect, or the system homoskedasticity
assumption does not hold, propose a simple method for obtaining valid standard errors and test
statistics.
Solution
From Section 7.8.6, we know that when  has the AR(1) structure given above,
1 −  2  1/2 z i1
 −1/2 Z i 

z i2 − z i1

z iT − z i,T−1

so that, for t ≥ 2, the transformation results in quasi-difference. For t  1, the transformation
ensures that the transformed errors will have common varianec for all t  1, . . . , T.
b. We need to estimate , so we would use pooled 2SLS to get residuals, say u it . Then,
estimate  from the pooled OLS regression
u it on u i,t−1 , t  2, . . . , T; i  1, . . . , N.
The GIV transformed equation is
1 −  2  1/2 y i1  1 −  2  1/2 x i1   1 −  2  1/2 u i1
y it − y i,t−1  x it − x i,t−1   u it − u i,t−1 , t  2, . . . , T.
The GIV estimator is obtained by replacing  with ̂ and estimating
ỹ it  x̃ it   error it , t  1, . . . , T; i  1, . . . , N

124

using IVs z̃ it , where
z̃ i1  1 − ̂ 2  1/2 z i1
z̃ it  z it − ̂ z i,t−1 , t  2, . . . , T
and where similar definitions hold for ỹ it and x̃ it . As always, the estimation of  has no effect
on the N -asymptotic distribution under the strict exogeneity assumption on the IVs. The
usual P2SLS statistics from the estimation on the transformed variables are asymptotically
valid.
c. If we have misspecified Varu i |z i  then we should make the P2SLS inference from part b
fully robust – to heteroskedasticity and serial correlation. In other words, the transformed
errors
e i1  1 −  2  1/2 u i1
e it  u it − u i,t−1 , t  2, . . . , T
will have serial correlation if the AR(1) model is incorrect, and such errors can always have
heteroskedsticity if u it  does. We know that the GIV estimator that uses an incorrect variance
structure is still consistent and N -asymptotically normal. We might get a more efficient
estimator assuming a simple AR(1) structure than using P2SLS on the original: accounting for
the serial correlation at all might be better than ignoring it in estimation. This is the same
motivation underlying the generalized estimation equations literature when the explanatory
variables are strictly exogenous.

125

Solutions to Chapter 9 Problems
9.1. a. No. What causal inference could one draw from this? We may be interested in the
tradeoff between wages and benefits, but then either of these can be taken as the dependent
variable and estimation of either equation would be by OLS. Of course, if we have omitted
some important factors, or have a measurement error problem, OLS could be inconsistent for
estimating the tradeoff. But there is no simultaneity problem: wages and benefits are jointly
determined, but there is no sense in which an equation for wage and another for benefits satisfy
the autonomy requirement.
b. Yes. We can certainly think of an exogenous change in law enforcement expenditures
causing a reduction in crime, and we are certainly interested in such counterfactuals. If we
could do the appropriate experiment, where expenditures are assigned randomly across cities,
then we could estimate the crime equation by OLS. The simultaneous equations model
recognizes that cities choose law enforcement expenditures in part based on what they expect
the crime rate to be. An SEM is a convenient way to allow expenditures to depend on
unobservables (to the econometrician) that affect crime.
c. No. These are both choice variables of the firm, and the parameters in a two-equation
system modeling one in terms of the other, and vice versa, have no economic meaning. If we
want to know how a change in the price of foreign technology affects foreign technology (FT)
purchases, why would we want to hold fixed R&D spending? Clearly FT purchases and R&D
spending are simultaneously chosen, but we should use a two-equation SUR setup where
neither is an explanatory variable in the other’s equation.
d. Yes. We can be interested in the causal effect of alcohol consumption on productivity,
and therefore on wage. One’s hourly wage is determined by productivity, and other factors;

126

alcohol consumption is determined by individual choice, where one factor is income.
e. No. These are choice variables by the same household. It makes no sense to think about
how exogenous changes in one would affect the other. Further, suppose that we look at the
effects of changes in local property tax rates. We would not want to hold fixed family saving
and then measure the effect of changing property taxes on housing expenditures. When the
property tax changes, a family will generally adjust expenditure in all categories. A SUR
system with property tax as an explanatory variable is the appropriate strategy.
f. No. These are both chosen by the firm, presumably to maximize profits. It makes no
sense to hold advertising expenditures fixed while looking at how other variables affect price
markup.
g. Yes. The outcome variables – quantity demanded and advertising expenditures – are
determined by different economic agents. It makes sense to model quantity demanded as a
function of advertising expenditures – reflecting that more exposure to the public can affect
demand – and at the same time recognize that how much a firm spends on advertising can be
determined by how much of the product it can sell.
h. Yes. The rate of HIV infection is determined by many factors, with condom usage being
one. We can easily imagine being interested in the effects of making condoms more available
on the incidence of HIV. The second equation, which models demand for condoms as a
function of HIV incidence, captures the idea that more people might use condoms as the risk of
HIV infection increases. Each equation stands on its own.
9.2. a. Write the system as
1

− 1

y1

− 2

1

y2



127

z 1  1  u 1
z 2  2  u 2

.

Unique solutions for y 1 and y 2 exist only if the matrix premultiplying y 1 , y 2  ′ is nonsingular.
But its determinant is 1 −  1  2 , so a necessary and sufficient condition for the reduced forms
to exist is  1  2 ≠ 1.
b. The rank condition holds for the first equation if and only if z 2 contains an element not
in z 1 and the coefficient in  2 on that variable is not zero. Similarly, the rank condition
holds for the second equation if and only if z 1 contains an element not in z 2 and the
coefficient in  1 on that variable is not zero.
9.3. a. We can apply part b of Problem 9.2. First, the only variable excluded from the
support equation is the variable mremarr; since the support equation contains one endogenous
variable, this equation is identified if and only if  21 ≠ 0. This ensures that there is an
exogenous variable shifting the mother’s reaction function that does not also shift the father’s
reaction function.
The visits equation is identified if and only if at least one of finc and fremarr actually
appears in the support equation; that is, we need  11 ≠ 0 or  13 ≠ 0.
b. Each equation can be estimated by 2SLS using instruments
1, finc, fremarr, dist, mremarr.
c. First, obtain the reduced form for visits :
visits   20   21 finc   22 fremarr   23 dist   24 mremarr  v 2 .
Estimate this equation by OLS, and save the residuals, v̂ 2 . Then, run the OLS regression
support on 1, visits, finc, fremarr, dist, v̂ 2
and do a (heteroskedasticity-robust) t test that the coefficient on v̂ 2 is zero. If this test rejects
we conclude that visits is in fact endogenous in the support equation.

128

d. There is one overidentifying restriction in the visits equation, assuming that  11 and  12
are both different from zero. Assuming homoskedasticity of u 2 , the easiest way to test the
overidentifying restriction is to first estimate the visits equation by 2SLS. as in part b. Let û 2
be the 2SLS residuals. Then, run the auxiliary regression
û 2 on 1, finc, fremarr, dist, mremarr;
the sample size times the usual R-squared from this regression is distributed asymptotically as
 21 under the null hypothesis that all instruments are exogenous.
A heteroskedasticity-robust test is also easy to obtain. Let support denote the fitted values
from the reduced form regression for support. Next, regress finc (or fremarr) on
support, mremarr, dist, and save the residuals, say r̂ 1 . Then, run the simple regression (without
intercept) of û 2 on r̂ 1 and use the heteroskedasticity-robust t statistic on r̂ 1 . (Note that no
intercept is needed in this final regression, but including one is harmless.)
9.4. a. Because the third equation contains no right hand side endogenous variables, a
reduced form exists for the system if and only if the first two equations can be solved for y 1
and y 2 as functions of y 3 , z 1 , z 2 , z 3 , u 1 , and u 2 . But this is equivalent to asking when the system
1 − 12

y1

1 − 22

y2



c1
c2

has a unique solution in y 1 and y 2 . This matrix is nonsingular if and only if  12 ≠  22 . This
implies that the 3  3 matrix Γ in the general SEM notation is nonsingular.
b. The third equation satisfies the rank condition because it includes no right-hand-side
endogenous variables. The first equation fails the order condition because there are no
excluded exogenous variables in it, but there is one included endogenous variable. This means

129

it fails the rank condition also. The second equation is just identified according to the order
condition because it contains two endogenous variables and also excludes two exogenous
variables. To examine the rank condition, write the second equation as y 2  z 2  u 2  0,
where  2  −1,  22 ,  23  ′ and  2   21 , 0, 0 ′ . Write  2  −1,  22 ,  23 ,  21 ,  22 ,  23  ′ as the
vector of parameters for the second equation with only the normalization  21  −1 imposed.
Then, the restrictions  22  0 and  23  0 can be written as R 2  2  0, where
0 0 0 0 1 0

R2 

.

0 0 0 0 0 1

Now letting B be the 6  3 matrix of all parameters, and imposing all exclusion restrictions in
the system,
R2B 

 12 0  32
 13 0  33

.

The rank condition requires this matrix have rank equal to two. Provided the vector  32 ,  33  ′
is not a multiple of  12 ,  13  ′ , or  12  33 ≠  13  32 , the rank condition is satisfied.
9.5. a. Let  1 denote the 7  1 vector of parameters in the first equation with only the
normalization restriction imposed:
 ′1  −1,  12 ,  13 ,  11 ,  12 ,  13 ,  14 .
The restrictions  12  0 and  13   14  1 are obtained by choosing
R1 

0 0 0 0 1 0 0
1 0 0 0 0 1 1

.

Because R 1 has two rows, and G − 1  2, the order condition is satisfied. Now we need to
check the rank condition. Letting B denote the 7  3 matrix of all structural parameters with
130

only the three normalizations, straightforward matrix multiplication gives

R1B 

 12

 22

 32

 13   14 − 1  23   24 −  21  33   34 −  31

.

By definition of the constraints on the first equation, the first column of R 1 B is zero. Next, we
use the constraints in the remainder of the system to get the expression for R 1 B with all
information imposed. But  23  0,  22  0,  23  0,  24  0,  31  0, and  32  0, and so R 1 B
becomes

R1B 

0

 32

0

0 − 21  33   34 −  31

.

Identification requires  21 ≠ 0 and  32 ≠ 0.
b. It is easy to see how to estimate the first equation under the given assumptions. Set
 14  1 −  13 and plug into the equation. After simple algebra we get
y 1 − z 4   12 y 2   13 y 3   11 z 1   13 z 3 − z 4   u 1 .
This equation can be estimated by 2SLS using instruments z 1 , z 2 , z 3 , z 4 . Note that, if we just
count instruments, there are just enough instruments to estimate this equation.
9.6. a. If  13  0 then the two equations constitute a linear SEM. In that case, the first
equation is identified if and only if  23 ≠ 0 and the second equation is identified if and only if
 12 ≠ 0.
b. If we plug the second equation into the first we obtain
1 −  12  21 −  13  21 z 1 y 1   10  12  20    12  21   13  20   11 z 1
  13  21 z 21   12 z 2   12  23 z 3   13  23 z 1 z 3  u 1   12   13 z 1 u 2 .
This can be solved for y 1 provided 1 −  12  21 −  13  21 z 1  ≠ 0. Given the solution for y 1 , we

131

can use the second equation to get y 2 . Note that both are nonlinear in z 1 unless  13  0.
c. Since Eu 1 |z  Eu 2 |z  0, we can use part (b) to get
Ey 1 |z 1 , z 2 , z 3    10   12  20    12  21   13  20   11 z 1
  13  21 z 21   12 z 2   12  23 z 3   13  23 z 1 z 3 /1 −  12  21 −  13  21 z 1 .
Again, this is a nonlinear function of the exogenous variables appearing in the system unless
 13  0. If  21  0, Ey 1 |z 1 , z 2 , z 3  becomes linear in z 2 and quadratic in z 1 and z 3 .
d. If  13  0, we saw in part a that the first equation is identified. If we include  13 y 2 z 1 in
the model, we need at least one instrument for it. But regardless of the value of  13 , terms z 21
and z 1 z 3 – as well as many other nonlinear functions of z – are partially correlated with y 2 z 1 . In
other words, the linear projection of y 2 z 1 onto 1, z 1 , z 2 , z 3 , z 21 and z 1 z 3 will – except by fluke –
depend on at least one of the last two terms. In any case, we can test this using OLS with y i2 z i1
as the dependent variarbale and a heteroskedasticity-robust test of two exclusion restrictions.
Identification of the second equation is no problem, as z 3 is always available as an IV for y 2 .
To enhance efficiency when  13 ≠ 0, we could add z 21 and z 1 z 3 (say) to the instrument list.
e. We could use IVs 1, z 1 , z 2 , z 3 , z 21 , z 1 z 3  in estimating the equation
y 1   10   12 y 2   13 y 2 z 1   11 z 1   12 z 2  u 1
by 2SLS, which implies a single overidentifying restriction. We can add other IVs – z 22 , z 23 ,
z 1 z 2 , and z 2 z 3 seem natural – or even reciprocals, such as 1/z 1 (or 1/1  |z 1 | if z 1 can equal
zero).
f. We can use the instruments in part e for both equations. With a large sample size we
might expand the list of IVs as discussed in part e.
g. Technically, the parameters in the first equation can be consistently estimated if  13 ≠ 0
because Ey 2 |z is a nonlinear function of z, and so z 21 , z 1 z 2 , and other nonlinear functions
132

would generally be partially correlated with y 2 and y 2 z 1 . But, if  13  0 also, Ey 2 |z is linear
in z 1 and z 2 , and additional nonlinear functions are not partially correlated with y 2 ; thus, there
is no instrument for y 2 . Since the equation is not identified when  13  0 (and  23  0,
H 0 :  13  0 cannot be tested.
9.7. a. Because alcohol and educ are endogenous in the first equation, we need at least two
elements in z 2 , z 3  that are not also in z 1 . Ideally, we have a least one such element in z 2
and at least one such element in z 3 .
b. Let z denote all nonredundant exogenous variables in the system. Then use these as
instruments in a 2SLS analysis.
c. The matrix of instruments for each i is
zi
z1 

0

0

0 z i , educ i  0
0

0

.

zi

d. z 3  z. That is, we should not make any exclusion restrictions in the reduced form for
educ.
9.8. a. I interact nearc4 with experience and its quadratic, and the race indicator. The Stata
output follows.
. use card
. gen educsq  educ^2
. gen nearc4exper  nearc4*exper
. gen nearc4expersq  nearc4*expersq
. gen nearc4black  nearc4*black
. reg educsq exper expersq black south smsa reg661-reg668 smsa66 nearc4
nearc4exper nearc4expersq nearc4black, robust
Number of obs 

Linear regression

133

3010

F( 18, 2991)
Prob  F
R-squared
Root MSE






233.34
0.0000
0.4505
52.172

----------------------------------------------------------------------------|
Robust
educsq |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------exper | -18.01791
1.229128
-14.66
0.000
-20.42793
-15.60789
expersq |
.3700966
.058167
6.36
0.000
.2560452
.4841479
black | -21.04009
3.569591
-5.89
0.000
-28.03919
-14.04098
south | -.5738389
3.973465
-0.14
0.885
-8.36484
7.217162
smsa |
10.38892
3.036816
3.42
0.001
4.434463
16.34338
reg661 | -6.175308
5.574484
-1.11
0.268
-17.10552
4.754903
reg662 | -6.092379
4.254714
-1.43
0.152
-14.43484
2.250083
reg663 | -6.193772
4.010618
-1.54
0.123
-14.05762
1.670077
reg664 | -3.413348
5.069994
-0.67
0.501
-13.35438
6.527681
reg665 | -12.31649
5.439968
-2.26
0.024
-22.98295
-1.650031
reg666 | -13.27102
5.693005
-2.33
0.020
-24.43362
-2.10842
reg667 | -10.83381
5.814901
-1.86
0.063
-22.23542
.567801
reg668 |
8.427749
6.627727
1.27
0.204
-4.567616
21.42312
smsa66 | -.4621454
3.058084
-0.15
0.880
-6.458307
5.534016
nearc4 | -12.25914
7.012394
-1.75
0.081
-26.00874
1.490464
nearc4exper |
4.192304
1.55785
2.69
0.007
1.137738
7.24687
nearc4expe~q | -.1623635
.0753242
-2.16
0.031
-.310056
-.014671
nearc4black | -4.789202
4.247869
-1.13
0.260
-13.11824
3.53984
_cons |
307.212
6.617862
46.42
0.000
294.2359
320.188
----------------------------------------------------------------------------. test nearc4exper nearc4expersq nearc4black
( 1)
( 2)
( 3)

nearc4exper  0
nearc4expersq  0
nearc4black  0
F(

3, 2991) 
Prob  F 

3.72
0.0110

. ivreg lwage exper expersq black south smsa reg661-reg668 smsa66
(educ educsq  nearc4 nearc4exper nearc4expersq nearc4black)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | 116.731381
16 7.29571132
Residual | 475.910264 2993 .159007773
------------------------------------------Total | 592.641645 3009 .196956346

Number of obs
F( 16, 2993)
Prob  F
R-squared
Adj R-squared
Root MSE








3010
45.92
0.0000
0.1970
0.1927
.39876

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.3161298
.1457578
2.17
0.030
.0303342
.6019254
educsq | -.0066592
.0058401
-1.14
0.254
-.0181103
.0047918
exper |
.0840117
.0361077
2.33
0.020
.0132132
.1548101
expersq | -.0007825
.0014221
-0.55
0.582
-.0035709
.0020058
black | -.1360751
.0455727
-2.99
0.003
-.2254322
-.0467181

134

south |
-.141488
.0279775
-5.06
0.000
-.1963451
-.0866308
smsa |
.1072011
.0290324
3.69
0.000
.0502755
.1641267
reg661 | -.1098848
.0428194
-2.57
0.010
-.1938432
-.0259264
reg662 |
.0036271
.0325364
0.11
0.911
-.0601688
.0674231
reg663 |
.0428246
.0315082
1.36
0.174
-.0189554
.1046045
reg664 | -.0639842
.0391843
-1.63
0.103
-.1408151
.0128468
reg665 |
.0480365
.0445934
1.08
0.281
-.0394003
.1354734
reg666 |
.0672512
.0498043
1.35
0.177
-.0304028
.1649052
reg667 |
.0347783
.0471451
0.74
0.461
-.0576617
.1272183
reg668 | -.1933844
.0512395
-3.77
0.000
-.2938526
-.0929161
smsa66 |
.0089666
.0222745
0.40
0.687
-.0347083
.0526414
_cons |
2.610889
.9706341
2.69
0.007
.7077116
4.514067
----------------------------------------------------------------------------Instrumented: educ educsq
Instruments:
exper expersq black south smsa reg661 reg662 reg663 reg664
reg665 reg666 reg667 reg668 smsa66 nearc4 nearc4exper
nearc4expersq nearc4black
-----------------------------------------------------------------------------

The heteroskedasticity-robust Wald test, reported in the form of an F statistic, shows that
the three interaction terms are partially correlated with educ 2 : the p-value  . 011. (Whether the
partial correlation is strong enough is a reasonable concern.)
The 2SLS estimate of  educ 2 is −. 0067 with t  −1. 14. Without stronger evidence, we can
safely leave educ 2 out of the wage equation
b. If Eu 2 |z  0, as we would typically assume, than any function of z is uncorrelated with
u 1 , including interactions of the form black  z j for any exogenous variable z j . Such
interactions are likely to be correlated with black  educ if z j is correlated with educ.
c. The 2SLS estimates, first using black  nearc4 as the IV for black  educ and then using
black  educ as the IV, are given by the Stata output. The heteroskedasticity-robust standard
errors are computed. The standard error using black  educ as the IV is much smaller than the
standard error using black  nearc4 as the IV. (The point estimate is also substantially higher.)
. ivreg lwage exper expersq black south smsa reg661-reg668 smsa66
(educ blackeduc  nearc4 nearc4black), robust
Instrumental variables (2SLS) regression

135

Number of obs
F( 16, 2993)
Prob  F
R-squared
Root MSE







3010
52.35
0.0000
0.2435
.38702

----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.1273557
.0561622
2.27
0.023
.0172352
.2374762
blackeduc |
.0109036
.0399278
0.27
0.785
-.0673851
.0891923
exper |
.1059116
.0249463
4.25
0.000
.0569979
.1548253
expersq | -.0022406
.0004902
-4.57
0.000
-.0032017
-.0012794
black |
-.282765
.5012131
-0.56
0.573
-1.265522
.6999922
south | -.1424762
.0298942
-4.77
0.000
-.2010914
-.083861
smsa |
.1111555
.0310592
3.58
0.000
.050256
.1720551
reg661 | -.1103479
.0418554
-2.64
0.008
-.1924161
-.0282797
reg662 | -.0081783
.0339196
-0.24
0.809
-.0746863
.0583298
reg663 |
.0382413
.0335008
1.14
0.254
-.0274456
.1039283
reg664 | -.0600379
.0398032
-1.51
0.132
-.1380824
.0180066
reg665 |
.0337805
.0519109
0.65
0.515
-.0680042
.1355652
reg666 |
.0498975
.0559569
0.89
0.373
-.0598204
.1596155
reg667 |
.0216942
.0528376
0.41
0.681
-.0819075
.1252959
reg668 | -.1908353
.0506182
-3.77
0.000
-.2900853
-.0915853
smsa66 |
.0180009
.0205709
0.88
0.382
-.0223337
.0583356
_cons |
3.84499
.9545666
4.03
0.000
1.973317
5.716663
----------------------------------------------------------------------------Instrumented: educ blackeduc
Instruments:
exper expersq black south smsa reg661 reg662 reg663 reg664
reg665 reg666 reg667 reg668 smsa66 nearc4 nearc4black
----------------------------------------------------------------------------. ivreg lwage exper expersq black south smsa reg661-reg668 smsa66
(educ blackeduc  nearc4 blackeduchat), robust
Instrumental variables (2SLS) regression

Number of obs
F( 16, 2993)
Prob  F
R-squared
Root MSE







3010
52.52
0.0000
0.2501
.38535

----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.1178141
.0554036
2.13
0.034
.0091811
.226447
blackeduc |
.035984
.0105707
3.40
0.001
.0152573
.0567106
exper |
.1004843
.0241951
4.15
0.000
.0530436
.147925
expersq | -.0020235
.0003597
-5.63
0.000
-.0027288
-.0013183
black | -.5955669
.1587782
-3.75
0.000
-.9068923
-.2842415
south | -.1374265
.0294259
-4.67
0.000
-.1951236
-.0797294
smsa |
.1096541
.0306748
3.57
0.000
.0495083
.1697998
reg661 | -.1161759
.0409317
-2.84
0.005
-.196433
-.0359189
reg662 | -.0107817
.0335743
-0.32
0.748
-.0766127
.0550494
reg663 |
.0331736
.0326007
1.02
0.309
-.0307484
.0970955
reg664 |
-.064916
.0388398
-1.67
0.095
-.1410715
.0112395
reg665 |
.023022
.0505787
0.46
0.649
-.0761506
.1221946
reg666 |
.0379568
.0534653
0.71
0.478
-.0668757
.1427892
reg667 |
.0100466
.0513629
0.20
0.845
-.0906637
.1107568
reg668 | -.1907066
.0502527
-3.79
0.000
-.2892399
-.0921733
smsa66 |
.0167814
.0203639
0.82
0.410
-.0231472
.0567101
_cons |
4.00836
.9416251
4.26
0.000
2.162062
5.854658
-----------------------------------------------------------------------------

136

Instrumented:
Instruments:

educ blackeduc
exper expersq black south smsa reg661 reg662 reg663 reg664
reg665 reg666 reg667 reg668 smsa66 nearc4 blackeduchat
-----------------------------------------------------------------------------

d. Suppose Eeduc|z  z 2 and Varu 1 |z   21 . Then by Theorem 8.5, the optimal IVs for
educ and black  educ are
−2
 −2
1 Eeduc|z   1 z 2
−2
−2
 −2
1 Eblack  educ|z   1 black  Eeduc|z   1 black  z 2 .

We can drop the constant  −2
1 , and so the optimal IVs can be taken to be
z 1 , z 2 , black  z 2 .
When we operationalize this procedure, we can use
z i1 , z i ̂ 2 , black i  z i ̂ 2   z i1 , educ i , black i  educ i 
as the optimal IVs. Nothing is lost asymptotically by including black i  educ i in the reduced
form for educ i along with z i . So using 2SLS with IVs z i , black i  educ i  produces the
asymptotically efficient IV estimator.
9.9. a. The Stata output for 3SLS estimation of (9.28) and (9.29), along with the output for
2SLS on each equation, is given below. For coefficients that are statistically significant, the
3SLS and 2SLS are reasonably close. For coefficients estimated imprecisely, there are some
differences between 2SLS and 3SLS, but these are not unexpected. Generally, the 3SLS
standard errors are smaller (but recall that none of the standard errors are robust to
heteroskedasticity).
. reg3 (hours lwage educ age kidslt6 kidsge6 nwifeinc)
(lwage hours educ exper expersq)
Three-stage least-squares regression
---------------------------------------------------------------------Equation
Obs Parms
RMSE
"R-sq"
chi2
P
---------------------------------------------------------------------hours
428
6
1368.362
-2.1145
34.54
0.0000

137

lwage
428
4
.6892584
0.0895
79.87
0.0000
-------------------------------------------------------------------------------------------------------------------------------------------------|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------hours
|
lwage |
1676.933
431.169
3.89
0.000
831.8577
2522.009
educ | -205.0267
51.84729
-3.95
0.000
-306.6455
-103.4078
age | -12.28121
8.261529
-1.49
0.137
-28.47351
3.911094
kidslt6 | -200.5673
134.2685
-1.49
0.135
-463.7287
62.59414
kidsge6 | -48.63986
35.95137
-1.35
0.176
-119.1032
21.82352
nwifeinc |
.3678943
3.451518
0.11
0.915
-6.396957
7.132745
_cons |
2504.799
535.8919
4.67
0.000
1454.47
3555.128
---------------------------------------------------------------------------lwage
|
hours |
.000201
.0002109
0.95
0.340
-.0002123
.0006143
educ |
.1129699
.0151452
7.46
0.000
.0832858
.1426539
exper |
.0208906
.0142782
1.46
0.143
-.0070942
.0488753
expersq | -.0002943
.0002614
-1.13
0.260
-.0008066
.000218
_cons | -.7051103
.3045904
-2.31
0.021
-1.302097
-.1081241
----------------------------------------------------------------------------Endogenous variables: hours lwage
Exogenous variables:
educ age kidslt6 kidsge6 nwifeinc exper expersq
----------------------------------------------------------------------------. ivreg hours educ age kidslt6 kidsge6 nwifeinc (lwage  exper expersq)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | -456272250
6
-76045375
Residual |
713583270
421 1694972.14
------------------------------------------Total |
257311020
427
602601.92

Number of obs
F( 6,
421)
Prob  F
R-squared
Adj R-squared
Root MSE








428
3.41
0.0027
1301.

----------------------------------------------------------------------------hours |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lwage |
1544.819
480.7387
3.21
0.001
599.8713
2489.766
educ |
-177.449
58.1426
-3.05
0.002
-291.7349
-63.16302
age | -10.78409
9.577347
-1.13
0.261
-29.60946
8.041289
kidslt6 | -210.8339
176.934
-1.19
0.234
-558.6179
136.9501
kidsge6 | -47.55708
56.91786
-0.84
0.404
-159.4357
64.3215
nwifeinc | -9.249121
6.481116
-1.43
0.154
-21.9885
3.490256
_cons |
2432.198
594.1719
4.09
0.000
1264.285
3600.111
----------------------------------------------------------------------------Instrumented: lwage
Instruments:
educ age kidslt6 kidsge6 nwifeinc exper expersq
----------------------------------------------------------------------------. ivreg lwage educ exper expersq (hours  age kidslt6 kidsge6 nwifeinc)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | 24.8336445
4 6.20841113

138

Number of obs 
F( 4,
423) 
Prob  F


428
18.80
0.0000

Residual | 198.493796
423 .469252474
------------------------------------------Total | 223.327441
427 .523015084

R-squared

Adj R-squared 
Root MSE


0.1112
0.1028
.68502

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------hours |
.0001608
.0002154
0.75
0.456
-.0002626
.0005842
educ |
.1111175
.0153319
7.25
0.000
.0809814
.1412536
exper |
.032646
.018061
1.81
0.071
-.0028545
.0681465
expersq | -.0006765
.0004426
-1.53
0.127
-.0015466
.0001935
_cons |
-.69279
.3066002
-2.26
0.024
-1.29544
-.0901403
----------------------------------------------------------------------------Instrumented: hours
Instruments:
educ exper expersq age kidslt6 kidsge6 nwifeinc
-----------------------------------------------------------------------------

More efficient GMM estimators can be obtained, along with robust standard errors. The
weighting matrix is the optimal one that allows for system heteroskedasticity. The (valid)
standard errors from the GMM estimation are quite a bit larger than the usual 3SLS standard
errors. The coefficient estimates change some, but the magnitudes are similar and all
qualitative conclusions hold.
. gmm (hours - {b1}*lwage - {b2}*educ - {b3}*age - {b4}*kidslt6
- {b5}*kidsge6 - {b6}*nwifeinc - {b7})
(lwage - {b8}*hour s - {b9}*educ - {b10}*age - {b11}*exper
- {b12}*expersq - {b13}),
instruments(educ age kidslt6 kidsge6 nwifeinc exper
expersq) winitial(identity)
Step 1
Iteration 0:
Iteration 1:
Iteration 2:

GMM criterion Q(b) 
GMM criterion Q(b) 
GMM criterion Q(b) 

1.339e11
350.92722
350.92722

Step 2
Iteration 0:
Iteration 1:
Iteration 2:

GMM criterion Q(b) 
GMM criterion Q(b) 
GMM criterion Q(b) 

.08810078
.00317682
.00317682

GMM estimation
Number of parameters  13
Number of moments
 16
Initial weight matrix: Identity
GMM weight matrix:
Robust

Number of obs



428

----------------------------------------------------------------------------|
Robust
|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------/b1 |
1606.63
618.7605
2.60
0.009
393.882
2819.379

139

/b2 | -179.4037
69.66358
-2.58
0.010
-315.9418
-42.86564
/b3 | -10.29206
10.88031
-0.95
0.344
-31.61708
11.03295
/b4 | -238.2421
212.7398
-1.12
0.263
-655.2044
178.7202
/b5 | -46.23438
59.7293
-0.77
0.439
-163.3017
70.83289
/b6 | -10.39225
5.576519
-1.86
0.062
-21.32203
.5375285
/b7 |
2385.333
630.2282
3.78
0.000
1150.108
3620.558
/b8 |
.0002704
.0003012
0.90
0.369
-.00032
.0008608
/b9 |
.1153863
.0158272
7.29
0.000
.0843655
.146407
/b10 |
.0031491
.00654
0.48
0.630
-.0096691
.0159673
/b11 |
.0336343
.0246234
1.37
0.172
-.0146266
.0818952
/b12 | -.0008599
.0006453
-1.33
0.183
-.0021247
.000405
/b13 | -.9910496
.5850041
-1.69
0.090
-2.137637
.1555373
----------------------------------------------------------------------------Instruments for equation 1: educ age kidslt6 kidsge6 nwifeinc exper expersq
_cons
Instruments for equation 2: educ age kidslt6 kidsge6 nwifeinc exper expersq
_cons

b. Using  as coefficients on endogenous variables, the three-equation system can be
expressed as
hours   12 lwage   13 educ   11   12 age   13 kidslt6   14 kidsge6   15 nwifeinc  u 1
2

lwage   21 hours   23 educ   21   12 age   22 exper   23 exper  u 2
educ   31   32 age   33 kidslt6   34 kidsge6   35 nwifeinc   36 exper   37 exper

2

  38 motheduc   39 fatheduc   3,10 huseduc  u 3
The IVs for the first equation are all appearing in the third equation, plus educ. For the second
and third equations, the valid IVs are all variables appearing in the third equation (which is
also a reduced form for educ).
The following Stata output produces the GMM estimates using the optimal weighting
matrix, along with the valid standard errors.
. gmm (hours - {b1}*lwage - {b2}*educ - {b3}*age - {b4}*kidslt6 {b5}*kidsge6 - {b6}*nwifeinc - {b7})
(lwage - {b8}*hours - {b9}*educ - {b10}*age - {b11}*exper
- {b12}*expersq - {b13})
(educ - {b14}*age - {b15}*kidslt6 - {b16}*kidsge6 - {b17}*nwifeinc
- {b18}*exper - {b19}*expersq - {b20}*motheduc - {b21}*fatheduc
- {b22}*huseduc - {b23}),
instruments(age kidslt6 kidsge6 nwifeinc exper expersq
motheduc fatheduc huseduc)
instruments(1: educ) winitial(identity)
Step 1
Iteration 0:
Iteration 1:

GMM criterion Q(b) 
GMM criterion Q(b) 

1.344e11
116344.22

140

Iteration 2:

GMM criterion Q(b) 

116344.22

Step 2
Iteration 0:
Iteration 1:
Iteration 2:

GMM criterion Q(b) 
GMM criterion Q(b) 
GMM criterion Q(b) 

.22854932
.0195166
.0195166

GMM estimation
Number of parameters  23
Number of moments
 31
Initial weight matrix: Identity
GMM weight matrix:
Robust

Number of obs



428

----------------------------------------------------------------------------|
Robust
|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------/b1 |
1195.924
383.1797
3.12
0.002
444.9052
1946.942
/b2 | -140.7006
46.54044
-3.02
0.003
-231.9181
-49.48298
/b3 | -9.160326
8.687135
-1.05
0.292
-26.1868
7.866146
/b4 | -298.5054
169.207
-1.76
0.078
-630.1449
33.13422
/b5 | -72.19858
43.44575
-1.66
0.097
-157.3507
12.95353
/b6 | -6.128558
3.9233
-1.56
0.118
-13.81809
1.560969
/b7 |
2297.594
511.7623
4.49
0.000
1294.559
3300.63
/b8 |
.0002455
.0002852
0.86
0.389
-.0003134
.0008044
/b9 |
.1011445
.0233025
4.34
0.000
.0554724
.1468167
/b10 |
.0007376
.0061579
0.12
0.905
-.0113317
.0128069
/b11 |
.031704
.0194377
1.63
0.103
-.0063933
.0698013
/b12 |
-.000695
.0003921
-1.77
0.076
-.0014635
.0000735
/b13 | -.6841501
.6119956
-1.12
0.264
-1.883639
.5153392
/b14 | -.0061529
.0135359
-0.45
0.649
-.0326827
.0203769
/b15 |
.5229541
.209776
2.49
0.013
.1118006
.9341075
/b16 | -.1128699
.070775
-1.59
0.111
-.2515864
.0258465
/b17 |
.0273278
.0093602
2.92
0.004
.0089821
.0456735
/b18 |
.0284576
.0333103
0.85
0.393
-.0368295
.0937447
/b19 | -.0001177
.0010755
-0.11
0.913
-.0022256
.0019902
/b20 |
.1226775
.0302991
4.05
0.000
.0632923
.1820627
/b21 |
.0976753
.0284494
3.43
0.001
.0419155
.153435
/b22 |
.3352722
.0357583
9.38
0.000
.2651873
.4053572
/b23 |
5.858645
.7413707
7.90
0.000
4.405585
7.311704
----------------------------------------------------------------------------Instruments for equation 1: educ age kidslt6 kidsge6 nwifeinc exper expersq
motheduc fatheduc huseduc _cons
Instruments for equation 2: age kidslt6 kidsge6 nwifeinc exper expersq
motheduc fatheduc huseduc _cons
Instruments for equation 3: age kidslt6 kidsge6 nwifeinc exper expersq
motheduc fatheduc huseduc _cons

9.10. a. No. 2SLS estimation of the first equation uses all nonredundant elements of z 1 and
z 2 – call these z – in the first stage regression for y 2 . Therefore, the exclusion restrictions in the
second equation are not imposed.

141

b. No, except in the extremely rare case where the covariance between the structural errors
is estimated to be zero. (If we impose a zero covariance, then the 2SLS estimates and 3SLS
estimates will be the same.) Effectively, each equation – including the second – is
overidentified.
c. This just follows from the first two parts. Because 2SLS puts no restrictions on the
reduced form for y 2 , whereas 3SLS assumes only z 2 appears in the reduced form for y 2 , 2SLS
will be more robust for estimating the parameters in the first equation.
9.11. a. Because z 2 and z 3 are both omitted from the first equation, we just need  22 ≠ 0 or
 23 ≠ 0. The second equation is identified if and only if  11 ≠ 0.
b. After substitution and straightforward algebra, it can be seen that  11   11 /1 −  12  21 .
c. We can estimate the system by 3SLS; for the second equation , this is identical to 2SLS
since it is just identified. Or, we could just use 2SLS on each equation. Given ̂ 11 , ̂ 12 , and ̂ 21 ,
we would form ̂ 11  ̂ 11 /1 − ̂ 12 ̂ 21 .
d. Whether we estimate the parameters by 2SLS or 3SLS, we will generally inconsistently
estimate  11 and  12 . (We are estimating the second equation by 2SLS so we will still
consistently estimate  21 provided we have not misspecified this equation.) So our estimate of
 11  ∂Ey 2 |z/∂z 1 will be inconsistent in any case.
e. We can just estimate the reduced form Ey 2 |z 1 , z 2 , z 3  by ordinary least squares.
f. Consistency of OLS for  11 does not hinge on the validity of the exclusion restrictions in
the structural model, whereas using an SEM does. Of course, if the SEM is correctly specified,
we obtain a more efficient estimator of the reduced form parameters by imposing the
restrictions in estimating  11 .
9.12. a. Generally, Ey 22 |z  Vary 2 |z  Ey 2 |z 2 ; when  13  0 and u 1 and u 2 are

142

homoskedastic, Vary 2 |z is constant, say  22 . (This is easily seen from the reduced form for y 2 ,
which is linear when  13  0.) Therefore, Ey 22 |z   22   20  z 2  2 .
b. We do not really need to use part a; in fact, it turns out to be a red herring for this
problem. Since  13  0,
Ey 1 |z   10   12 Ey 2 |z  z 1  1  Eu 1 |z
  10   12 Ey 2 |z  z 1  1 .
c. When  13  0, any nonlinear function of z, including  20  z 2  2 , has zero coefficient in
Ey 1 |z   10   12  20  z 2   z 1  1 . Plus, if  13  0, then the parameters  20 and  2 are
consistently estimated from the first stage regression y i2 on 1, z 1 , i  1, … , N. Therefore, the
regression y i1 on 1, ̂ 20  z i ̂ 2 , ̂ 20  z i ̂ 2  2 , z i1 , i  1, … , N consistently estimates  10 ,
 12 , 0, and  1 , respectively. But this is just the regression y i1 on 1, ŷ i2 , ŷ i2  2 , z i1 , i  1, … , N.
d. Because Eu 1 |z  0 and Varu 1 |z   21 , we can immediately apply Theorem 8.5 to
conclude that the optimal IVS for estimating the first equation are 1, Ey 2 |z, Ey 22 |z, z 1 / 21 ,
and we can drop the division by  21 . But, if  13  0, then Ey 2 |z is linear in z and, from part a,
Ey 22 |z   22  Ey 2 |z 2 . So the optimal IVs are a linear combination of
1, Ey 2 |z, Ey 2 |z 2 , z 1 , which means they are a linear combination of 1, z, Ey 2 |z 2 . We
never do worse asymptotically by using more IVs, so we can use 1, z, Ey 2 |z 2  as an
optimal set. Why would we use this larger set instead of 1, Ey 2 |z, Ey 2 |z 2 , z 1 ? For one,
the larger set will generally yield overidentifying restrictions. In addition, if  13 ≠ 0, we will
generally be better off using more instruments: z rather than only Ly 2 |1, z.
e. The estimates below are similar to those reported in Section 9.5.2, where we just added
educ 2 , age 2 , and nwifeinc 2 to the IV list and using 2SLS with lwage  logwage and
lwagesq  logwage 2 as endogenous explanatory variables. In particular, the coefficient on

143

lwagesq is still statistically insignificant. The standard errors reported here are robust to
heteroskedasticity (unlike in the text).
. gen lwagesq  lwage^2
. qui reg lwage educ age kidslt6 kidsge6 nwifeinc exper expersq
. predict lwagehat
(option xb assumed; fitted values)
. gen lwagehatsq  lwagehat^2
. ivreg hours (lwage lwagesq  exper expersq lwagehatsq)
educ age kidslt6 kidsge6 nwifeinc, robust
Instrumental variables (2SLS) regression

Number of obs
F( 7,
420)
Prob  F
R-squared
Root MSE







428
2.88
0.0059
1177.

----------------------------------------------------------------------------|
Robust
hours |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lwage |
1846.902
856.1346
2.16
0.032
164.0599
3529.745
lwagesq |
-373.16
401.9586
-0.93
0.354
-1163.261
416.9412
educ | -103.2347
71.99564
-1.43
0.152
-244.7513
38.28199
age | -9.425115
8.848798
-1.07
0.287
-26.81856
7.968333
kidslt6 | -187.0236
177.1703
-1.06
0.292
-535.2747
161.2274
kidsge6 | -55.70163
45.86915
-1.21
0.225
-145.8633
34.46007
nwifeinc |
-7.5979
4.491138
-1.69
0.091
-16.42581
1.230009
_cons |
1775.847
881.2631
2.02
0.045
43.61095
3508.082
----------------------------------------------------------------------------Instrumented: lwage lwagesq
Instruments:
educ age kidslt6 kidsge6 nwifeinc exper expersq lwagehatsq
----------------------------------------------------------------------------. gen educsq  educ^2
. gen agesq  age^2
. gen nwifeincsq  nwifeinc^2
. ivreg hours (lwage lwagesq  exper expersq educsq agesq nwifeincsq)
educ age kidslt6 kidsge6 nwifeinc, robust
Instrumental variables (2SLS) regression

Number of obs
F( 7,
420)
Prob  F
R-squared
Root MSE







428
3.53
0.0011
1161

----------------------------------------------------------------------------|
Robust

144

hours |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lwage |
1873.62
792.3005
2.36
0.018
316.2521
3430.989
lwagesq | -437.2911
313.7658
-1.39
0.164
-1054.038
179.4559
educ |
-87.8511
50.01226
-1.76
0.080
-186.1566
10.45442
age | -9.142303
8.512639
-1.07
0.283
-25.87499
7.590381
kidslt6 | -185.0554
179.9036
-1.03
0.304
-538.6789
168.5682
kidsge6 | -58.18949
44.6767
-1.30
0.193
-146.0073
29.6283
nwifeinc | -7.233422
4.037903
-1.79
0.074
-15.17044
.7035957
_cons |
1657.926
671.9361
2.47
0.014
337.1489
2978.702
----------------------------------------------------------------------------Instrumented: lwage lwagesq
Instruments:
educ age kidslt6 kidsge6 nwifeinc exper expersq educsq agesq
nwifeincsq
-----------------------------------------------------------------------------

9.13. a. The first equation is identified if, and only if,  22 ≠ 0 (the rank condition).
b. Here is the Stata output:
. use openness
. reg open lpcinc lland, robust
Linear regression

Number of obs
F( 2,
111)
Prob  F
R-squared
Root MSE







114
22.22
0.0000
0.4487
17.796

----------------------------------------------------------------------------|
Robust
open |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lpcinc |
.5464812
1.436115
0.38
0.704
-2.299276
3.392238
lland | -7.567103
1.141798
-6.63
0.000
-9.829652
-5.304554
_cons |
117.0845
18.24808
6.42
0.000
80.92473
153.2443
-----------------------------------------------------------------------------

With t  −6. 63, we can conclude that is shows that logland is very statistically
significant in the reduced form for open. The negative coefficient implies that smaller
countries are more “open.”
c. Here is the Stata output. First 2SLS, the OLS, both with heteroskedasticity-robust
standard errors.
. ivreg inf (open  lland) lpcinc, robust
Instrumental variables (2SLS) regression

145

Number of obs 
F( 2,
111) 
Prob  F


114
2.53
0.0844

R-squared
Root MSE




0.0309
23.836

----------------------------------------------------------------------------|
Robust
inf |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------open | -.3374871
.1524489
-2.21
0.029
-.6395748
-.0353994
lpcinc |
.3758247
1.378542
0.27
0.786
-2.355848
3.107497
_cons |
26.89934
10.9199
2.46
0.015
5.260821
48.53785
----------------------------------------------------------------------------Instrumented: open
Instruments:
lpcinc lland
----------------------------------------------------------------------------. reg inf open lpcinc, robust
Linear regression

Number of obs
F( 2,
111)
Prob  F
R-squared
Root MSE







114
3.84
0.0243
0.0453
23.658

----------------------------------------------------------------------------|
Robust
inf |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------open | -.2150695
.0794571
-2.71
0.008
-.3725191
-.0576199
lpcinc |
.0175683
1.278747
0.01
0.989
-2.516354
2.55149
_cons |
25.10403
9.99078
2.51
0.013
5.306636
44.90143
-----------------------------------------------------------------------------

The IV estimate is larger in magnitude – by more than 50% – but its standard error is
almost twice as large as the OLS standard error. There is some but not overwhelming evidence
that open is actually endogenous. The variable-addition Hausman test, made robust to
heteroskedasticity, has t  1. 48. With N  114, we might not expect very strong evidence.
d. If we add  13 open 2 to the equation, we need an IV for it. Since logland is partially
correlated with open, logland 2 is a natural candidate. A regression of open 2 on
logland, logland 2 , and logpcinc gives a heteroskedasticity-robust t statistic on
logland 2 of about 2. This is borderline, but we will go ahead. The Stata output for 2SLS is
. ivreg inf (open opensq  lland llandsq) lpcinc, robust
Instrumental variables (2SLS) regression

146

Number of obs 
F( 3,
110) 
Prob  F


114
1.44
0.2350

R-squared
Root MSE




24.

----------------------------------------------------------------------------|
Robust
inf |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------open | -1.198637
.7139934
-1.68
0.096
-2.613604
.2163303
opensq |
.0075781
.0053779
1.41
0.162
-.0030796
.0182358
lpcinc |
.5066092
1.490845
0.34
0.735
-2.447896
3.461114
_cons |
43.17124
18.37223
2.35
0.021
6.761785
79.5807
----------------------------------------------------------------------------Instrumented: open opensq
Instruments:
lpcinc lland llandsq
-----------------------------------------------------------------------------

The squared term indicates that the impact of open on inf diminishes; the estimate would be
significant at about the 8. 1% level against a one-sided alternative.
e. Here is the Stata output for implementing the method described in the problem:
. qui reg open lpcinc lland
. predict openhat
(option xb assumed; fitted values)
. gen openhatsq  openhat^2
. reg inf openhat openhatsq lpcinc, robust
Linear regression

Number of obs
F( 3,
110)
Prob  F
R-squared
Root MSE







114
2.52
0.0620
0.0575
23.612

----------------------------------------------------------------------------|
Robust
inf |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------openhat | -.8648092
.5762007
-1.50
0.136
-2.006704
.2770854
openhatsq |
.0060502
.0050906
1.19
0.237
-.0040383
.0161387
lpcinc |
.0412172
1.293368
0.03
0.975
-2.521935
2.604369
_cons |
39.17831
15.99614
2.45
0.016
7.477717
70.8789
-----------------------------------------------------------------------------

Qualitatively, the results are similar to the appropriate IV method from part d, but the
coefficient on openhat is quite a bit smaller in magnitude using the “forbidden regression.” If
 13  0, Eopen|lpcinc, lland is linear, and Varopen|lpcinc, lland is constant then, as shown

147

in Problem 9.12, both methods are consistent. But the forbidden regression implemented in this
part is unnecessary, less robust, and we cannot trust the standard errors, anyway.
Incidentally, using openhatsq as an IV, rather than a regressor, gives very similar estimates
to using llandsq as an IV for opensq.
9.14. a. Agree. In equation (9.13), the reduced form variance matrix,   Ev ′ v is always
identified. Now the structural variance matrix can be written as   Γ ′ Γ, so if Γ and  are
both identified, so is .
b. Disagree. In many cases a linear version of the model is not identified because there are
not enough instruments. In that case, identification of the more general model hinges on the
model actually having nonlinearities, a tenuous situation.
c. Disagree. Eu|z  0 implies that z h is uncorrelated with u g for all h  1, . . . , L and
g  1, . . . , G. This kind of orthogonality, along with the rank condition, is sufficient for the
GMM, traditional, or GIV versions of 3SLS to be consistent. We need not restrict Varu|z for
consistency, and robust inference is easily obtained.
d. Disagree. Even true SEMs can have other problems that cause endogeneity, name,
omitted variables and measurement error. In these cases, some variables may be valid
instruments in one equation but not other equations.
e. Disagree. Control function approaches generally require more assumptions in order to be
consistent. Take equations (9.70) and (9.71) as an example. The CF approach proposed there
basically requires assumption (9.72), which can be very restrictive, especially if y 2 or y 3
exhibit discreteness. By contrast, we can use an IV approach that specifies z and nonlinear
functions of z as instruments in directly estimating (9.70) (by 2SLS or GMM). When they are
consistent, we might expect CF approaches to be more efficient asymptotically.

148

9.15. a. The model with  13  0,  11  0, and  12  0 is
y 1  z 1  1   11 y 2   12 y 3  u 3
and we maintain that this equation is identified. Necessary is that L ≥ L 1  2, as is assumed in
the problem. The rank condition is more complicated, and we assume it holds. In other words,
we assume we have enough relevant instruments for y 2 and y 3 . If (9.71) also holds, and say
Eu 2 |z  Eu 3 |z  0, then
Ey 2 y 3 |z  z 2 z 3   Eu 2 u 3 |z
Ey 2 z 1 |z  z 2 z 1
Ey 3 z 1 |z  z 3 z 1
which means that squares and cross products in z are natural instruments for the interaction
terms. Note that there are plenty of these interaction terms, and they are likely to provide
enough variation because the linear version of the model is identified. For example, if z L has
zero coefficient in the reduced form for y 3 , then z L z 1 appears in Ey 2 z 1 |z but not in Ey 3 z 1 |z.
Further, suppose Eu 2 u 3 |z is actually constant. then squares of elements in z 2 , where
z  z 1 , z 2 , appear in Ey 2 y 3 |z but not the other expectations. If Eu 2 u 3 |z is not constant, it
would cancel out with those squares only by fluke. Further, even if we only take (9.71) to be
linear projections, that would only helps our cause because if these are not conditional
expectations then even more nonlinear functions of z would be useful as instruments.
b. There are no overidentification restrictions. We have the same number of instruments as
explanatory variables. That is one drawback to using fitted values as IVs: there are no
ovidentification restrictions to test.
c. There are L − L 1 − 2 overidentification restrictions. The L 1 subvector z 1 acts as its own
instruments, and we need two more elements of z to instrument for y 2 and y 3 . The rest of the

149

explanatory variables are taking care of by the interaction terms.
d. We can get many overidentification restrictions by using as IVs the nonredundant
elements of z i , z i ⊗ z i  – that is, the levels, squares, and cross products of z i . For example, if
L  5, L 1  2, and z and z 1 include a constant, then there are 5  4  6  15 instruments and
2  2  2  2  8 explanatory variables.
e. To solve this problem, we also need to assume Eu 2 u 3 |z is constant, and to explicitly
state that z 1 includes a constant. By Theorem 8.5, the optimal IVs are then given in part a,
along with z 1 , because Varu 1 |z   21 . If Eu 2 u 3 |z is constant then
z 2 z 3   Eu 2 u 3 |z  z 2 z 3   constant. We operationalize the IVs by replacing  2
and  3 with the estimators from the first-stage regressions.
The estimators from parts c and d would also be asymptotically efficient because linear
combinations of the instruments in those two parts are the optimal instruments.
Asymptotically, it does not hurt to use redundant instruments; asymptotically, the optimal
linear combination will be picked out via the first-stage regression.

150

Solutions to Chapter 10 Problems
10.1. a. Because investment is likely to be affected by macroeconomic factors, it is
important to allow for these by including separate time intercepts; this is done by using T − 1
time period dummies.
b. Putting the unobserved effect c i in the equation is a simple way to account for
time-constant features of a county that affect investment and might also be correlated with the
tax variable. Something like “average” county economic climate, which affects investment,
could easily be correlated with tax rates because tax rates are, at least to a certain extent,
selected by state and local officials. If only a cross section were available, we would have to
find an instrument for the tax variable that is uncorrelated with c i and correlated with the tax
rate. This is often a difficult task.
c. Standard investment theories suggest that, ceteris paribus, larger marginal tax rates
decrease investment.
d. I would start with a fixed effects analysis to allow arbitrary correlation between all
time-varying explanatory variables and c i . (Actually, doing pooled OLS is a useful initial
exercise; these results can be compared with those from an FE analysis). Such an analysis
assumes strict exogeneity of z it , tax it , and disaster it in the sense that these are uncorrelated with
the errors u is for all t and s.
I have no strong intuition for the likely serial correlation properties of the u it . These
might have little serial correlation because we have allowed for c i , in which case I would use
standard fixed effects. However, it seems more likely that the u it are positively autocorrelated,
in which case I might use first differencing instead. In either case, I would compute the fully
robust standard errors along with the usual ones. In either case we can test for serial correlation

151

in u it .
e. If tax it and disaster it do not have lagged effects on investment, then the only possible
violation of the strict exogeneity assumption is if future values of these variables are correlated
with u it . It seems reasonable not to worry whether future natural disasters are determined by
past investment. On the other hand, state officials might look at the levels of past investment in
determining future tax policy, especially if there is a target level of tax revenue the officials are
trying to achieve. This could be similar to setting property tax rates: sometimes property tax
rates are set depending on recent housing values because a larger base means a smaller rate can
achieve the same amount of revenue. Given that we allow tax it to be correlated with c i ,
feedback might not be much of a problem. But it cannot be ruled out ahead of time.
10.2. a.  2 ,  2 , and  can be consistently estimated (assuming all elements of z it are
time-varying). The first period intercept,  1 , and the coefficient on female 1 ,  1 , cannot be
estimated.
b. Everything else equal,  2 measures the growth in wage for men over the period. This is
because, if we set female i  0 and z i1  z i2 , the change in log wage is, on average,  2 set
d2 1  0 and d2 2  1. We can think of this as being the growth in wage rates (for males) due
to aggregate factors in the economy. The parameter  2 measures the difference in wage growth
rates between women and men, all else equal. If  2  0 then, for men and women with the
same characteristics, average wage growth is the same.
c. Write
logwage i1    1  z i1    1 female i  c i  u i1
logwage i2    1   2  z i2    1 female i   2 female  c i  u i2 ,
where I have used the fact that d2 1  0 and d2 2  1. Subtracting the first equation from the

152

second gives
Δ logwage i    2  Δz i    2 female i  Δu i .
This equation shows explicitly that the growth in wages depends on Δz i and gender. If
z i1  z i2 then Δz i  0, and the growth in wage for men is  2 and that for women is  2   2 ,
just as above. This shows that we can allow for c i and still test for a gender differential in the
growth of wages. But we cannot say anything about the wage differential between men and
women for a given year.
10.3. a. Let x i  x i1  x i2 /2, y i  y i1  y i2 /2, ẍ i1  x i1 − x i , ẍ i2  x i2 − x i , and
similarly for ÿ i1 and ÿ i2 . For T  2 the fixed effects estimator can be written as

 FE 

−1

N

∑

ẍ ′i1 ẍ i1



ẍ ′i2 ẍ i2 

i1

N

∑ẍ ′i1 y i1  ẍ ′i2 y i2 
i1

Now, by simple algebra,
ẍ i1  x i1 − x i2 /2  −Δx i /2
ẍ i2  x i2 − x i1 /2  Δx i /2
ÿ i1  y i1 − y i2 /2  −Δy i /2
ÿ i2  y i2 − y i1 /2  Δy i /2
Therefore,
ẍ ′i1 ẍ i1  ẍ ′i2 ẍ i2  Δx ′i Δx i /4  Δx ′i Δx i /4  Δx ′i Δx i /2
ẍ ′i1 ÿ i1  ẍ ′i2 ÿ i2  Δx ′i Δy i /4  Δx ′i Δy i /4  Δx ′i Δy i /2
and so

153

.


 FE 

−1

N

∑

N

∑ Δx ′i Δy i /2

Δx ′i Δx i /2

i1

i1

−1

N



∑

N


  FD .

∑ Δx ′i Δy i

Δx ′i Δx i

i1

i1



b. Let û i1  ÿ i1 − ẍ i1  FE and û i2  ÿ i2 − ẍ i2  FE be the fixed effects residuals for the two


time periods for cross section observation i. Since  FE   FD , and using the representations
above, we have


û i1  −Δy i /2 − −Δx i /2  FD  −Δy i − Δx i  FD /2 ≡ −ê i /2


û i2  Δy i /2 − Δx i /2  FD  Δy i − Δx i  FD /2 ≡ ê i /2,

where ê i ≡ Δy i − Δx i  FD are the first difference residuals, i  1, 2, … , N. Therefore,
N

∑

N

û 2i1



û 2i2 

i1

 1/2 ∑ ê 2i .
i1

This shows that the sum of squared residuals from the fixed effects regression is exactly one
half the sum of squared residuals from the first difference regression. Since we know the
variance estimate for fixed effects is the SSR divided by N − K (when T  2), and the variance
estimate for first difference is the SSR divided by N − K, the error variance from fixed effects
is always half the size as the error variance for first difference estimation, that is, ̂ 2u  ̂ 2e /2
(contrary to what the problem asks you to show). What I wanted you to show is that the


variance matrix estimates of  FE and  FD are identical. This is easy since the variance matrix
estimate for fixed effects is
−1

N

̂ 2u

∑

ẍ ′i1 ẍ i1

i1



ẍ ′i2 ẍ i2 

−1

N



̂ 2e /2

∑
i1

154

Δx ′i Δx i /2

−1

N



̂ 2e

∑
i1

Δx ′i Δx i

,

which is the variance matrix estimator for FD estimator. Thus, the standard errors, and the fact
all other test statistics (F statistics) will be numerically identical using the two approaches.
10.4. a. Including the aggregate time effect, d2 t , can be very important. Without it, we
must assume that any change in average y over the two time periods is due to the program, and
not to external factors. For example, if y it is the unemployment rate for city i at time t, and
prog it denotes a job creation program, we want to be sure that we account for the fact that the
general economy may have worsened or improved over the period. If d2 t is omitted, and
 2  0 (an improving economy, since unemployment has fallen), we might attribute a decrease
in unemployment to the job creation program, when in fact it had nothing to do with it. For
general T, each time period should have its own intercept (otherwise the analysis is not entirely
convincing).
b. The presence of c i allows program participation to be correlated with unobserved
individual heterogeneity, something crucial in contexts where the experimental group is not
randomly assigned. Two examples are when individuals “self-select” into the program and
when program administrators target specific groups that may benefit more or less from the
program.
c. If we first difference the equation, use the fact that prog i1  0 for all i, d2 1  0, and
d2 2  1, we get
y i2  y i1   2   1 prog i2  u i2 − u i1 ,
or
Δy i   2   1 prog i2  Δu i .
Now, the FE (and FD) estimates of  2 and  1 are just the OLS estimators from this equation

155

(on cross section data). From basic two-variable regression with a dummy independent
variable, ̂ 2 is the average value of Δy over the group with prog 12  0 – that is, the control
group. Also, ̂ 2 and ̂ 1 is the average value of Δy over the group with prog i1  1 – that is, the
treatment group. Thus, as asserted, we have
̂ 2  Δy control , ̂ 1  Δy treat − Δy control .
If we did not include the d2 t , ̂ 1  Δy treat , the average change of the treated group. The
demonstrates the claim in part b that without the aggregate time effect any change in the
average value of y for the treated group is attributed to the program. Differencing and
averaging over the treated group allows program participation to depend on time-constant
unobservables affecting the level of y, but that does not account for external factors that affect
y for everyone.
d. In general, for T time periods we have
y it   1   2 d2 t   3 d3 t …  T dT t   1 prog it  c i  u it ;
that is, we have separate year intercepts, an unobserved effect c i , and the program indicator.
e. First, the model from part d is more flexible because it allows any sequence of program
participation. Equation (10.89), when extended to T  2, applies only when treatment is
ongoing. In addition, (10.89) is restrictive in terms of aggregate time effects: it assumes that
any aggregate time effects correspond to the start of the program only. It is better to use the
unobserved effects model from part d, and estimate it using either FE or FD.
10.5. a. Write v i v ′i  c 2i j T j ′T  u i u ′i  j T c i u ′i   c i u ′i j ′T . Under RE.1, Eu i |x i , c i   0,
which implies that Ec i u ′i |x i   0 by iterated expectations. Under RE.3a, Eu i u ′i |x i , c i    2u I T ,
which implies that Eu i u ′i |x i    2u I T (again, by iterated expectations). Therefore,

156

Ev i v ′i |x i   Ec 2i |x i j T j ′T  Eu i u ′i |x i   hx i j T j ′T   2u I T ,
where hx i  ≡ Varc i |x i   Ec 2i |x i  (by RE.1b). This shows that the conditional variance
matrix of v i given x i has the same covariance for all t ≠ s, hx i , and the same variance for all
t, hx i    2u . Therefore, while the variances and covariances depend on x i , they do not depend
on time.
b. The RE estimator is still consistent and N −asymptotically normal without Assumption
RE.3b, but the usual random effects variance estimator of Avar̂ RE  is no longer valid because
Ev i v ′i |x i  does not have the form (10.30) (because it depends on x i ). The robust variance
matrix estimator given in (7.52) should be used in obtaining standard errors and Wald
statistics.
10.6. a. By stacking the formulas for the FD and FE estimators, and using standard
asymptotic arguments, we have, under FE. 1 and the rank conditions,
N ̂ − 

N

G

−1

N

−1

∑ si

 o p 1,

i1

where G is the 2K  2K block diagonal matrix with blocks A 1 and A 2 , respectively, and s i is
the 2K  1 vector
si ≡

ΔX ′i Δu i
̈ ′i ü i
X

.


b. Let Δû i denote the T − 1  1 vector of FD residuals, and let ü i denote the T  1 vector
̂  N −1 ∑ N ŝ i ŝ ′i , and
of FE residuals. Plugging these into the formula for s i givens ŝ i . Let D
i1
define Ĝ by replacing A 1 and A 2 with their obvious consistent estimators. Then
−1
̂ Ĝ −1 is a consistent estimator of Avar N ̂ − .
Avar N ̂ −   Ĝ D

157

c. Let R be the K  2K partitioned matrix R  I K ∣ −I K . Then the null hypothesis
imposed by the Hausman test is H 0 : R  0. We can form a Wald-type statistic,
−1
′
̂ Ĝ −1 R ′  −1 R̂.
H  R̂ RĜ D

Under FE.1 and the rank conditions for FD and FE, H has a limiting x 2K distribution. The
statistic requires no particular second moment assumptions of the kind in FE.3. Note that
R̂  ̂ FD − ̂ FE .
10.7. a. The random effects estimates are given below. The coefficient on season is −. 044,
which means that being in season is estimated to reduce an athlete’s term GPA by . 044 points.
The nonrobust t statistic is only −1. 12.
. use gpa
. xtset id term
panel variable:
time variable:
delta:

id (strongly balanced)
term, 8808 to 8901, but with gaps
1 unit

. xtreg trmgpa spring crsgpa frstsem season sat verbmath hsperc hssize black
Random-effects GLS regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.2067
between  0.5390
overall  0.4785




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(10)
Prob  chi2




732
366
2.
512.77
0.0000

----------------------------------------------------------------------------trmgpa |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------spring | -.0606536
.0371605
-1.63
0.103
-.1334868
.0121797
crsgpa |
1.082365
.0930877
11.63
0.000
.8999166
1.264814
frstsem |
.0029948
.0599542
0.05
0.960
-.1145132
.1205028
season | -.0440992
.0392381
-1.12
0.261
-.1210044
.032806
sat |
.0017052
.0001771
9.63
0.000
.0013582
.0020523
verbmath |
-.15752
.16351
-0.96
0.335
-.4779937
.1629538
hsperc | -.0084622
.0012426
-6.81
0.000
-.0108977
-.0060268
hssize | -.0000775
.0001248
-0.62
0.534
-.000322
.000167
black | -.2348189
.0681573
-3.45
0.001
-.3684048
-.1012331
female |
.358153
.0612948
5.84
0.000
.2380173
.4782886
_cons |
-1.73492
.3566599
-4.86
0.000
-2.43396
-1.035879
----------------------------------------------------------------------------

158

sigma_u | .37185442
sigma_e | .40882825
rho |
.4527451
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

b. Below are the fixed effects estimates with nonrobust standard errors. The time-constant
variables have been dropped. The coefficient on season is now larger in magnitude, −. 057, and
it is more statistically significant with t  −1. 37.
. xtreg trmgpa spring crsgpa frstsem season, fe
Fixed-effects (within) regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.2069
between  0.0333
overall  0.0613

corr(u_i, Xb)




732
366

Obs per group: min 
avg 
max 
F(4,362)
Prob  F

 -0.0893




2.
23.61
0.0000

----------------------------------------------------------------------------trmgpa |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------spring | -.0657817
.0391404
-1.68
0.094
-.1427528
.0111895
crsgpa |
1.140688
.1186538
9.61
0.000
.9073505
1.374025
frstsem |
.0128523
.0688364
0.19
0.852
-.1225172
.1482218
season | -.0566454
.0414748
-1.37
0.173
-.1382072
.0249165
_cons | -.7708055
.3305004
-2.33
0.020
-1.420747
-.1208636
---------------------------------------------------------------------------sigma_u | .67913296
sigma_e | .40882825
rho | .73400603
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(365, 362) 
5.40
Prob  F  0.0000

c. The following Stata output gives the nonrobust and fully robust regression-based
Hausman test. Whether we test the three time averages, crsgpabar, frstsembar, and seasonbar,
or just seasonbar, the p-value is large (. 068 for the joint nonrobust test, . 337 for the single
nonrobust test). And the findings do not depend on using a robust test: the p-values are a little
smaller but not close to being significant.
For comparision, the traditional way of computing the Hausman statistic – directly forming
the quadratic form in the FE and RE estimates is included at the end, computed two different
ways. The first uses the difference in the estimated variance matrices, and the value of the
159

statistic, 1. 81, is very close to the nonrobust, regression-based statistic, 1. 83. But the degrees
of freedom reported by Stata are incorrect: it should be three, not four. Thus, the p-value
reported by Stata using the hausman command is too large. This will be the case whenever
aggregate time variables – most commonly, time period dummies – are included among the
coefficients to test.
If we impose that the RE estimate of the variance  2u is used to estimate both the FE and
RE asymptotic variances, Stata then recognizes that the variance matrix has rank three rather
than rank four. (The same is true if we use the FE estimate of  2u in both places.) The p-value
in this case agrees very closely with that for the nonrobust, regression-based test (and both
statistics are 1. 83 rounded to two decimal places.)
. egen crsgpabar  mean(crsgpa), by(id)
. egen frstsembar  mean(frstsem), by(id)
. egen seasonbar  mean(season), by(id)
. xtreg trmgpa spring crsgpa frstsem season sat verbmath hsperc hssize black
female crsgpabar frstsembar seasonbar, re
Random-effects GLS regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.2069
between  0.5408
overall  0.4802




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(13)
Prob  chi2




732
366
2.
513.77
0.0000

----------------------------------------------------------------------------trmgpa |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------spring | -.0657817
.0391404
-1.68
0.093
-.1424954
.0109321
crsgpa |
1.140688
.1186538
9.61
0.000
.9081308
1.373245
frstsem |
.0128523
.0688364
0.19
0.852
-.1220646
.1477692
season | -.0566454
.0414748
-1.37
0.172
-.1379345
.0246438
sat |
.0016681
.0001804
9.24
0.000
.0013145
.0020218
verbmath | -.1316461
.1654748
-0.80
0.426
-.4559708
.1926785
hsperc | -.0084655
.0012554
-6.74
0.000
-.0109259
-.006005
hssize | -.0000783
.000125
-0.63
0.531
-.0003232
.0001666
black | -.2447934
.0686106
-3.57
0.000
-.3792676
-.1103192
female |
.3357016
.0711808
4.72
0.000
.1961898
.4752134

160

crsgpabar | -.1861551
.2011254
-0.93
0.355
-.5803537
.2080434
frstsembar |
-.078244
.1461014
-0.54
0.592
-.3645975
.2081095
seasonbar |
.1243006
.1293555
0.96
0.337
-.1292315
.3778326
_cons | -1.423761
.5183296
-2.75
0.006
-2.439668
-.4078539
---------------------------------------------------------------------------sigma_u | .37185442
sigma_e | .40882825
rho |
.4527451
(fraction of variance due to u_i)
----------------------------------------------------------------------------. test crsgpabar frstsembar seasonbar
( 1)
( 2)
( 3)

crsgpabar  0
frstsembar  0
seasonbar  0
chi2( 3) 
Prob  chi2 

. test
( 1)

1.83
0.6084

seasonbar
seasonbar  0
chi2( 1) 
Prob  chi2 

0.92
0.3366

. xtreg trmgpa spring crsgpa frstsem season sat verbmath hsperc hssize black
female crsgpabar frstsembar seasonbar , re cluster(id)
Random-effects GLS regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.2069
between  0.5408
overall  0.4802




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(13)
Prob  chi2




732
366
2.
629.75
0.0000

(Std. Err. adjusted for 366 clusters in id
----------------------------------------------------------------------------|
Robust
trmgpa |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------spring | -.0657817
.0394865
-1.67
0.096
-.1431737
.0116104
crsgpa |
1.140688
.1317893
8.66
0.000
.8823856
1.39899
frstsem |
.0128523
.0684334
0.19
0.851
-.1212746
.1469793
season | -.0566454
.0411639
-1.38
0.169
-.1373251
.0240344
sat |
.0016681
.0001848
9.03
0.000
.0013059
.0020304
verbmath | -.1316461
.166478
-0.79
0.429
-.4579371
.1946448
hsperc | -.0084655
.0013131
-6.45
0.000
-.0110391
-.0058918
hssize | -.0000783
.0001172
-0.67
0.504
-.000308
.0001514
black | -.2447934
.075569
-3.24
0.001
-.392906
-.0966808
female |
.3357016
.067753
4.95
0.000
.2029081
.4684951
crsgpabar | -.1861551
.1956503
-0.95
0.341
-.5696227
.1973125
frstsembar |
-.078244
.1465886
-0.53
0.594
-.3655525
.2090644
seasonbar |
.1243006
.1342238
0.93
0.354
-.1387732
.3873743
_cons | -1.423761
.4571037
-3.11
0.002
-2.319668
-.5278545

161

---------------------------------------------------------------------------sigma_u | .37185442
sigma_e | .40882825
rho |
.4527451
(fraction of variance due to u_i)
----------------------------------------------------------------------------. test crsgpabar frstsembar seasonbar
( 1)
( 2)
( 3)

crsgpabar  0
frstsembar  0
seasonbar  0
chi2( 3) 
Prob  chi2 

. test
( 1)

1.95
0.5829

seasonbar
seasonbar  0
chi2( 1) 
Prob  chi2 

0.86
0.3544

. * The traditional Hausman test that incorrectly includes the coefficients
. * on "spring" (the time dummy) among those being tested.
. qui xtreg trmgpa spring crsgpa frstsem season, fe
. estimates store fe
. qui xtreg trmgpa spring crsgpa frstsem season sat verbmath hsperc hssize
black female, re
. estimates store re
. hausman fe re
---- Coefficients ---|
(b)
(B)
(b-B)
sqrt(diag(V_b-V_B))
|
fe
re
Difference
S.E.
---------------------------------------------------------------------------spring |
-.0657817
-.0606536
-.0051281
.012291
crsgpa |
1.140688
1.082365
.0583227
.0735758
frstsem |
.0128523
.0029948
.0098575
.0338223
season |
-.0566454
-.0440992
-.0125462
.0134363
----------------------------------------------------------------------------b  consistent under Ho and Ha; obtained from xtreg
B  inconsistent under Ha, efficient under Ho; obtained from xtreg
Test:

Ho:

difference in coefficients not systematic
chi2(4)  (b-B)’[(V_b-V_B)^(-1)](b-B)

1.81
Probchi2 
0.7702

. qui xtreg trmgpa spring crsgpa frstsem season, fe
. estimates store fe
. qui xtreg trmgpa spring crsgpa frstsem season sat verbmath hsperc hssize black

162

. estimates store re
. hausman fe re, sigmamore
Note: the rank of the differenced variance matrix (3) does not equal the
number of coefficients being tested (4); be sure
this is what you expect, or there may be problems computing the test.
Examine the output of your estimators for anything unexpected and
possibly consider scaling your variables so that the coefficients are
on a similar scale.
---- Coefficients ---|
(b)
(B)
(b-B)
sqrt(diag(V_b-V_B))
|
fe
re
Difference
S.E.
---------------------------------------------------------------------------spring |
-.0657817
-.0606536
-.0051281
.0121895
crsgpa |
1.140688
1.082365
.0583227
.0734205
frstsem |
.0128523
.0029948
.0098575
.0337085
season |
-.0566454
-.0440992
-.0125462
.013332
----------------------------------------------------------------------------b  consistent under Ho and Ha; obtained from xtreg
B  inconsistent under Ha, efficient under Ho; obtained from xtreg
Test:

Ho:

difference in coefficients not systematic
chi2(3)  (b-B)’[(V_b-V_B)^(-1)](b-B)

1.83
Probchi2 
0.6077
(V_b-V_B is not positive definite)

10.8. a. The Stata output is below. The coefficients on the lagged “clear-up” percentages
are very close in magnitude. For example, if the first lag is 10 percentage points higher, the
crime rate is estimated to fall by about 18.5 percent, a very large effect. The estimate of  in
the AR(1) serial correlation test is . 574 and t  5. 82, so there is very strong evidence of serial
correlation.
. use norway
. xtset district year, delta(6)
panel variable: district (strongly balanced)
time variable: year, 72 to 78
delta: 6 units
. reg lcrime d78 clrprc_1 clrprc_2
Source |
SS
df
MS
------------------------------------------Model | 18.7948264
3 6.26494214
Residual | 21.1114968
102 .206975459
-------------------------------------------

163

Number of obs
F( 3,
102)
Prob  F
R-squared
Adj R-squared







106
30.27
0.0000
0.4710
0.4554

Total |

39.9063233

105

.380060222

Root MSE



.45495

----------------------------------------------------------------------------lcrime |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d78 | -.0547246
.0944947
-0.58
0.564
-.2421544
.1327051
clrprc_1 | -.0184955
.0053035
-3.49
0.001
-.0290149
-.007976
clrprc_2 | -.0173881
.0054376
-3.20
0.002
-.0281735
-.0066026
_cons |
4.18122
.1878879
22.25
0.000
3.808545
4.553894
----------------------------------------------------------------------------. predict vhat, resid
. gen vhat_1  l.vhat
(53 missing values generated)
. reg vhat vhat_1
Source |
SS
df
MS
------------------------------------------Model |
3.8092697
1
3.8092697
Residual | 5.73894345
51 .112528303
------------------------------------------Total | 9.54821315
52 .183619484

Number of obs
F( 1,
51)
Prob  F
R-squared
Adj R-squared
Root MSE








53
33.85
0.0000
0.3990
0.3872
.33545

----------------------------------------------------------------------------vhat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------vhat_1 |
.5739582
.0986485
5.82
0.000
.3759132
.7720033
_cons | -3.01e-09
.0460779
-0.00
1.000
-.0925053
.0925053
-----------------------------------------------------------------------------

b. The fixed effects estimates are given below. The coefficient on clrprc_1 falls
dramatically in magnitude, and becomes statistically insignificant. The coefficient on clrprc_2
falls somewhat but is still practically large and statistically significant.
To obtain the heteroskedasticity-robust standard error for FE, we must use the FD
estimation (which is the same as FE because T  2) in order to make the calculation simple.
Stock and Watson (2008, Econometrica) show that just applying the usual
heteroskedasticity-robust standard error using pooled regression on the time-demeaned data
does not produce valid standard errors. The reason is simple: as shown in the text, the time
demeaning induces serial correlation in the errors. Of course, one can always use the fully
robust standard errors, which allow for any kind of serial correlation in the original errors and

164

any kind of heteroskedasticity. In this example, obtaining the heteroskedasticity-robust
standard errors has little effect on inference.
. xtreg lcrime d78 clrprc_1 clrprc_2, fe
Fixed-effects (within) regression
Group variable: district
R-sq:

Number of obs
Number of groups

within  0.4209
between  0.4798
overall  0.4234

corr(u_i, Xb)




106
53

Obs per group: min 
avg 
max 
F(3,50)
Prob  F

 0.3645




2.
12.12
0.0000

----------------------------------------------------------------------------lcrime |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d78 |
.0856556
.0637825
1.34
0.185
-.0424553
.2137665
clrprc_1 | -.0040475
.0047199
-0.86
0.395
-.0135276
.0054326
clrprc_2 | -.0131966
.0051946
-2.54
0.014
-.0236302
-.0027629
_cons |
3.350995
.2324736
14.41
0.000
2.884058
3.817932
---------------------------------------------------------------------------sigma_u | .47140473
sigma_e |
.2436645
rho | .78915666
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(52, 50) 
5.88
Prob  F  0.0000
. reg clcrime cclrprc_1 cclrprc_2, robust
Number of obs 
F( 2,
50) 
Prob  F

R-squared

Root MSE


Linear regression

53
4.78
0.0126
0.1933
.34459

----------------------------------------------------------------------------|
Robust
clcrime |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cclrprc_1 | -.0040475
.0042659
-0.95
0.347
-.0126158
.0045207
cclrprc_2 | -.0131966
.0047286
-2.79
0.007
-.0226942
-.003699
_cons |
.0856556
.0554876
1.54
0.129
-.0257945
.1971057
-----------------------------------------------------------------------------

c. I use the FD regression to easily allow for heteroskedasticity. The two-sided p-value is
. 183. Because we do not reject H 0 :  1   2 at even the 15% level, we might justify
estimating a model with  1   2 (and the pooled OLS results suggest it, too). The variable
avgclr is the average of clrprc_1and clrprc_2, and so we can use it as the only explanatory

165

variable. Imposing the restriction gives a large estimated effect – a 10 percentage point
increase in the average clear-up rate decreases crime by about 16. 7% – and the
heteroskedasticity-robust t statistic is −2. 89.
. qui reg clcrime cclrprc_1 cclrprc_2, robust
. lincom cclrprc_1 - cclrprc_2
( 1)

cclrprc_1 - cclrprc_2  0

----------------------------------------------------------------------------clcrime |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------(1) |
.009149
.0067729
1.35
0.183
-.0044548
.0227529
----------------------------------------------------------------------------. reg clcrime cavgclr, robust
Number of obs 
F( 1,
51) 
Prob  F

R-squared

Root MSE


Linear regression

53
8.38
0.0056
0.1747
.34511

----------------------------------------------------------------------------|
Robust
clcrime |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cavgclr | -.0166511
.0057529
-2.89
0.006
-.0282006
-.0051016
_cons |
.0993289
.0554764
1.79
0.079
-.0120446
.2107024
----------------------------------------------------------------------------. reg clcrime cavgclr
Source |
SS
df
MS
------------------------------------------Model | 1.28607105
1 1.28607105
Residual | 6.07411496
51 .119100293
------------------------------------------Total | 7.36018601
52 .141542039

Number of obs
F( 1,
51)
Prob  F
R-squared
Adj R-squared
Root MSE








53
10.80
0.0018
0.1747
0.1586
.34511

----------------------------------------------------------------------------clcrime |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cavgclr | -.0166511
.0050672
-3.29
0.002
-.0268239
-.0064783
_cons |
.0993289
.0625916
1.59
0.119
-.0263289
.2249867
-----------------------------------------------------------------------------

10.9. a. The RE and FE estimates, with fully robust standard errors for each, are given
below. The variable-addition Hausman test is obtained by adding time averages of all variables

166

except the year dummies; all other explanatory variables change across i and t. For
comparison, the traditional nonrobust Hausman statistic is computed. This version uses the RE
estimate of  2u in estimating both the RE and FE asymptotic variances and properly computes
the degrees of freedom (which is five for this application).
While there are differences in the RE and FE estimates, the signs are the same and the
magnitudes are similar. The fully robust Hausman test gives a strong statistical rejection of the
RE assumption that county heterogeneity is uncorrelated with the criminal justice variables.
Therefore, for magnitudes, we should prefer the FE estimates. (Remember, though, that both
RE and FE maintain strict exogeneity conditional on the heterogeneity.) The nonrobust
Hausman test gives a substantially larger statistic, 78. 79 compared with 60. 53, but the
conclusion is the same.
. xtreg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87, re
cluster(county)
Random-effects GLS regression
Group variable: county
R-sq:

Number of obs
Number of groups

within  0.4287
between  0.4533
overall  0.4454




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(11)
Prob  chi2




630
90
7.
156.83
0.0000

(Std. Err. adjusted for 90 clusters in county
----------------------------------------------------------------------------|
Robust
lcrmrte |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.4252097
.0629147
-6.76
0.000
-.5485202
-.3018993
lprbconv | -.3271464
.0499587
-6.55
0.000
-.4250636
-.2292292
lprbpris | -.1793507
.0457547
-3.92
0.000
-.2690283
-.0896731
lavgsen | -.0083696
.0322377
-0.26
0.795
-.0715543
.0548152
lpolpc |
.4294148
.0878659
4.89
0.000
.2572007
.6016288
d82 |
.0137442
.0164857
0.83
0.404
-.0185671
.0460556
d83 |
-.075388
.0194832
-3.87
0.000
-.1135743
-.0372017
d84 | -.1130975
.0217025
-5.21
0.000
-.1556335
-.0705614
d85 | -.1057261
.0254587
-4.15
0.000
-.1556242
-.0558279
d86 | -.0795307
.0239141
-3.33
0.001
-.1264014
-.0326599
d87 | -.0424581
.0246408
-1.72
0.085
-.0907531
.005837
_cons | -1.672632
.5678872
-2.95
0.003
-2.785671
-.5595939

167

---------------------------------------------------------------------------sigma_u | .30032934
sigma_e | .13871215
rho | .82418424
(fraction of variance due to u_i)
----------------------------------------------------------------------------. xtreg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87, fe
cluster(county)
Fixed-effects (within) regression
Group variable: county
R-sq:

Number of obs
Number of groups

within  0.4342
between  0.4066
overall  0.4042

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(11,89)
Prob  F

 0.2068




630
90
7.
11.49
0.0000

(Std. Err. adjusted for 90 clusters in county
----------------------------------------------------------------------------|
Robust
lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.3597944
.0594678
-6.05
0.000
-.4779557
-.2416332
lprbconv | -.2858733
.051522
-5.55
0.000
-.3882464
-.1835001
lprbpris | -.1827812
.0452811
-4.04
0.000
-.2727538
-.0928085
lavgsen | -.0044879
.0333499
-0.13
0.893
-.0707535
.0617777
lpolpc |
.4241142
.0849052
5.00
0.000
.2554095
.592819
d82 |
.0125802
.0160066
0.79
0.434
-.0192246
.044385
d83 | -.0792813
.0195639
-4.05
0.000
-.1181544
-.0404081
d84 | -.1177281
.0217118
-5.42
0.000
-.160869
-.0745872
d85 | -.1119561
.0256583
-4.36
0.000
-.1629386
-.0609736
d86 | -.0818268
.0236276
-3.46
0.001
-.1287745
-.0348792
d87 | -.0404704
.0241765
-1.67
0.098
-.0885087
.0075678
_cons | -1.604135
.5102062
-3.14
0.002
-2.617904
-.5903664
---------------------------------------------------------------------------sigma_u | .43487416
sigma_e | .13871215
rho | .90765322
(fraction of variance due to u_i)
----------------------------------------------------------------------------. egen lprbatb  mean(lprbarr), by(county)
. egen lprbctb  mean(lprbconv), by(county)
. egen lprbptb  mean(lprbpris), by(county)
. egen lavgtb  mean(lavgsen), by(county)
. egen lpoltb  mean(lpolpc), by(county)
. qui xtreg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87
lprbctb lprbptb lavgtb lpoltb, re cluster(county)
. test
( 1)
( 2)

lprbatb lprbctb lprbptb lavgtb lpoltb
lprbatb  0
lprbctb  0

168

lprbatb

( 3)
( 4)
( 5)

lprbptb  0
lavgtb  0
lpoltb  0
chi2( 5) 
Prob  chi2 

60.53
0.0000

. qui xtreg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87, fe
. estimates store fe
. qui xtreg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87, re
. estimates store re
. hausman fe re, sigmamore
Note: the rank of the differenced variance matrix (3) does not equal the
number of coefficients being tested (4); be sure
this is what you expect, or there may be problems computing the test.
Examine the output of your estimators for anything unexpected and
possibly consider scaling your variables so that the coefficients are
on a similar scale.
---- Coefficients ---|
(b)
(B)
(b-B)
sqrt(diag(V_b-V_B))
|
fe
re
Difference
S.E.
---------------------------------------------------------------------------lprbarr |
-.3597944
-.4252097
.0654153
.0133827
lprbconv |
-.2858733
-.3271464
.0412731
.0084853
lprbpris |
-.1827812
-.1793507
-.0034305
.0065028
lavgsen |
-.0044879
-.0083696
.0038816
.0037031
lpolpc |
.4241142
.4294148
-.0053005
.0103217
d82 |
.0125802
.0137442
-.001164
.0010763
d83 |
-.0792813
-.075388
-.0038933
.0008668
d84 |
-.1177281
-.1130975
-.0046306
.0013163
d85 |
-.1119561
-.1057261
-.00623
.0014304
d86 |
-.0818268
-.0795307
-.0022962
.0007719
d87 |
-.0404704
-.0424581
.0019876
.001219
----------------------------------------------------------------------------b  consistent under Ho and Ha; obtained from xtreg
B  inconsistent under Ha, efficient under Ho; obtained from xtreg
Test:

Ho:

difference in coefficients not systematic
chi2(5)  (b-B)’[(V_b-V_B)^(-1)](b-B)

78.79
Probchi2 
0.0000
(V_b-V_B is not positive definite)

b. Below is the Stata output for fixed effects with the nine wage variables; the inference is
fully robust. The log wage variables are jointly significant at the 1.2% significance level.
Unfortunately, one of the most significant wage variables, lwtuc, has a positive, statistically
169

significant coefficient. It is difficult to give that a causal interpretation. The coefficient on the
manufacturing wage implies that a ceteris paribus 10% increase in manufacturing wage
reduces the crime rate by about three percent.
. xtreg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87 lwcon lwtuc
lwtrd lwfir lwser lwmfg lwfed lwsta lwloc, fe cluster(county)
Fixed-effects (within) regression
Group variable: county
R-sq:

Number of obs
Number of groups

within  0.4575
between  0.2518
overall  0.2687

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(20,89)
Prob  F

 0.0804




630
90
7.
13.39
0.0000

(Std. Err. adjusted for 90 clusters in county
----------------------------------------------------------------------------|
Robust
lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.3563515
.0615202
-5.79
0.000
-.4785909
-.2341121
lprbconv | -.2859539
.0507647
-5.63
0.000
-.3868224
-.1850855
lprbpris | -.1751355
.0457628
-3.83
0.000
-.2660652
-.0842058
lavgsen | -.0028739
.0333994
-0.09
0.932
-.0692379
.06349
lpolpc |
.4229
.0822172
5.14
0.000
.2595362
.5862639
d82 |
.0188915
.0221161
0.85
0.395
-.0250527
.0628357
d83 |
-.055286
.0306016
-1.81
0.074
-.1160906
.0055187
d84 | -.0615162
.0406029
-1.52
0.133
-.1421934
.0191609
d85 | -.0397115
.0603405
-0.66
0.512
-.1596068
.0801837
d86 | -.0001133
.0720231
-0.00
0.999
-.1432217
.1429952
d87 |
.0537042
.0847749
0.63
0.528
-.1147418
.2221501
lwcon | -.0345448
.0253345
-1.36
0.176
-.0848839
.0157943
lwtuc |
.0459747
.0161107
2.85
0.005
.013963
.0779864
lwtrd | -.0201766
.0313131
-0.64
0.521
-.0823951
.0420418
lwfir | -.0035445
.0130093
-0.27
0.786
-.0293937
.0223047
lwser |
.0101264
.0202248
0.50
0.618
-.0300599
.0503128
lwmfg | -.3005691
.1063746
-2.83
0.006
-.5119331
-.089205
lwfed | -.3331226
.2245785
-1.48
0.142
-.7793554
.1131101
lwsta |
.0215209
.1051755
0.20
0.838
-.1874606
.2305023
lwloc |
.1810215
.1629903
1.11
0.270
-.1428368
.5048797
_cons |
.8931726
1.479937
0.60
0.548
-2.04743
3.833775
---------------------------------------------------------------------------sigma_u | .47756823
sigma_e | .13700505
rho | .92395784
(fraction of variance due to u_i)
----------------------------------------------------------------------------. testparm lwcon-lwloc
( 1)
( 2)
( 3)

lwcon  0
lwtuc  0
lwtrd  0

170

(
(
(
(
(
(

4)
5)
6)
7)
8)
9)

lwfir
lwser
lwmfg
lwfed
lwsta
lwloc
F(








0
0
0
0
0
0

9,
89) 
Prob  F 

2.54
0.0121

c. First, we need to compute the changes in log wages. Then, we just use pooled OLS.
Rather than difference the year dummies we just include dummies for 1983 through 1987.
Both the usual and full robust standard errors are computed and compared with those from FE.
The nonrobust FD and FE standard errors are similar, and often very different from the
comparable robust standard errors. In fact, in many cases the robust standard errors are double
or more than the nonrobust ones, although some nonrobust ones are actually smaller. The wage
variables generally have much smaller coefficients when FD is used, but they are still jointly
significant using a robust test.
. gen clwcon  lwcon - lwcon[_n-1] if year  81
(90 missing values generated)
. gen clwtuc  lwtuc - lwtuc[_n-1] if year  81
(90 missing values generated)
. gen clwtrd  lwtrd - lwtrd[_n-1] if year  81
(90 missing values generated)
. gen clwfir  lwfir - lwfir[_n-1] if year  81
(90 missing values generated)
. gen clwser  lwser - lwser[_n-1] if year  81
(90 missing values generated)
. gen clwmfg  lwmfg - lwmfg[_n-1] if year  81
(90 missing values generated)
. gen clwfed  lwfed - lwfed[_n-1] if year  81
(90 missing values generated)
. gen clwsta  lwsta - lwsta[_n-1] if year  81
(90 missing values generated)
. gen clwloc  lwloc - lwloc[_n-1] if year  81
(90 missing values generated)

171

. reg clcrmrte clprbarr clprbcon clprbpri clavgsen clpolpc clwcon-clwloc
d83-d87
Source |
SS
df
MS
------------------------------------------Model | 9.86742162
19
.51933798
Residual | 12.3293822
520
.02371035
------------------------------------------Total | 22.1968038
539 .041181454

Number of obs
F( 19,
520)
Prob  F
R-squared
Adj R-squared
Root MSE








540
21.90
0.0000
0.4445
0.4242
.15398

----------------------------------------------------------------------------clcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------clprbarr | -.3230993
.0300195
-10.76
0.000
-.3820737
-.2641248
clprbcon | -.2402885
.0182474
-13.17
0.000
-.2761362
-.2044407
clprbpri | -.1693859
.02617
-6.47
0.000
-.2207978
-.117974
clavgsen | -.0156167
.0224126
-0.70
0.486
-.0596469
.0284136
clpolpc |
.3977221
.026987
14.74
0.000
.3447051
.450739
clwcon | -.0442368
.0304142
-1.45
0.146
-.1039865
.015513
clwtuc |
.0253997
.0142093
1.79
0.074
-.002515
.0533144
clwtrd | -.0290309
.0307907
-0.94
0.346
-.0895203
.0314586
clwfir |
.009122
.0212318
0.43
0.668
-.0325886
.0508326
clwser |
.0219549
.0144342
1.52
0.129
-.0064016
.0503113
clwmfg | -.1402482
.1019317
-1.38
0.169
-.3404967
.0600003
clwfed |
.0174221
.1716065
0.10
0.919
-.319705
.3545493
clwsta | -.0517891
.0957109
-0.54
0.589
-.2398166
.1362385
clwloc | -.0305153
.1021028
-0.30
0.765
-.2311
.1700694
d83 | -.1108653
.0268105
-4.14
0.000
-.1635355
-.0581951
d84 | -.0374103
.024533
-1.52
0.128
-.0856063
.0107856
d85 | -.0005856
.024078
-0.02
0.981
-.0478877
.0467164
d86 |
.0314757
.0245099
1.28
0.200
-.0166749
.0796262
d87 |
.0388632
.0247819
1.57
0.117
-.0098218
.0875482
_cons |
.0198522
.0206974
0.96
0.338
-.0208086
.060513
----------------------------------------------------------------------------. reg clcrmrte clprbarr clprbcon clprbpri clavgsen clpolpc clwcon-clwloc
d83-d87, cluster(county)
Number of obs 
F( 19,
89) 
Prob  F

R-squared

Root MSE


Linear regression

540
13.66
0.0000
0.4445
.15398

(Std. Err. adjusted for 90 clusters in county
----------------------------------------------------------------------------|
Robust
clcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------clprbarr | -.3230993
.0584771
-5.53
0.000
-.4392919
-.2069066
clprbcon | -.2402885
.0403223
-5.96
0.000
-.320408
-.1601689
clprbpri | -.1693859
.0459288
-3.69
0.000
-.2606455
-.0781263
clavgsen | -.0156167
.0267541
-0.58
0.561
-.0687765
.0375432
clpolpc |
.3977221
.1038642
3.83
0.000
.1913461
.604098
clwcon | -.0442368
.0165835
-2.67
0.009
-.0771879
-.0112856
clwtuc |
.0253997
.0123845
2.05
0.043
.000792
.0500075
clwtrd | -.0290309
.0180398
-1.61
0.111
-.0648755
.0068138
clwfir |
.009122
.006921
1.32
0.191
-.0046299
.0228739
clwser |
.0219549
.0180754
1.21
0.228
-.0139606
.0578703

172

clwmfg | -.1402482
.1190279
-1.18
0.242
-.3767541
.0962578
clwfed |
.0174221
.1326
0.13
0.896
-.2460511
.2808954
clwsta | -.0517891
.0674058
-0.77
0.444
-.185723
.0821449
clwloc | -.0305153
.1269012
-0.24
0.811
-.2826652
.2216346
d83 | -.1108653
.0270368
-4.10
0.000
-.1645868
-.0571437
d84 | -.0374103
.0237018
-1.58
0.118
-.0845052
.0096845
d85 | -.0005856
.0256369
-0.02
0.982
-.0515257
.0503544
d86 |
.0314757
.0214193
1.47
0.145
-.011084
.0740353
d87 |
.0388632
.0263357
1.48
0.144
-.0134653
.0911917
_cons |
.0198522
.0180545
1.10
0.274
-.0160217
.0557261
----------------------------------------------------------------------------. test clwcon clwtuc clwtrd clwfir clwser clwmfg clwfed clwsta clwloc
(
(
(
(
(
(
(
(
(

1)
2)
3)
4)
5)
6)
7)
8)
9)

clwcon
clwtuc
clwtrd
clwfir
clwser
clwmfg
clwfed
clwsta
clwloc
F(











0
0
0
0
0
0
0
0
0

9,
89) 
Prob  F 

2.38
0.0184

. xtreg lcrmrte lprbarr lprbconv lprbpris lavgsen lpolpc d82-d87
lwtrd lwfir lwser lwmfg lwfed lwsta lwloc, fe
Fixed-effects (within) regression
Group variable: county
R-sq:

Number of obs
Number of groups

within  0.4575
between  0.2518
overall  0.2687

corr(u_i, Xb)

lwcon lwtuc



Obs per group: min 
avg 
max 
F(20,520)
Prob  F

 0.0804




630
90
7.
21.92
0.0000

----------------------------------------------------------------------------lcrmrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lprbarr | -.3563515
.0321591
-11.08
0.000
-.4195292
-.2931738
lprbconv | -.2859539
.0210513
-13.58
0.000
-.3273099
-.2445979
lprbpris | -.1751355
.0323403
-5.42
0.000
-.2386693
-.1116017
lavgsen | -.0028739
.0262108
-0.11
0.913
-.054366
.0486181
lpolpc |
.4229
.0263942
16.02
0.000
.3710476
.4747524
d82 |
.0188915
.0251244
0.75
0.452
-.0304662
.0682492
d83 |
-.055286
.0330287
-1.67
0.095
-.1201721
.0096001
d84 | -.0615162
.0410805
-1.50
0.135
-.1422204
.0191879
d85 | -.0397115
.0561635
-0.71
0.480
-.1500468
.0706237
d86 | -.0001133
.0680124
-0.00
0.999
-.1337262
.1334996
d87 |
.0537042
.0798953
0.67
0.502
-.1032532
.2106615
lwcon | -.0345448
.0391616
-0.88
0.378
-.1114792
.0423896
lwtuc |
.0459747
.019034
2.42
0.016
.0085817
.0833677
lwtrd | -.0201766
.0406073
-0.50
0.619
-.0999511
.0595979
lwfir | -.0035445
.028333
-0.13
0.900
-.0592058
.0521168
lwser |
.0101264
.0191915
0.53
0.598
-.027576
.0478289

173

lwmfg | -.3005691
.1094068
-2.75
0.006
-.5155028
-.0856354
lwfed | -.3331226
.176448
-1.89
0.060
-.6797612
.013516
lwsta |
.0215209
.1130648
0.19
0.849
-.2005991
.2436409
lwloc |
.1810215
.1180643
1.53
0.126
-.0509203
.4129632
_cons |
.8931726
1.424067
0.63
0.531
-1.90446
3.690805
---------------------------------------------------------------------------sigma_u | .47756823
sigma_e | .13700505
rho | .92395784
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(89, 520) 
39.12
Prob  F  0.0000

d. There is strong evidence of negative serial correlation in the FD equation, suggesting
that if the idiosyncratic errors follow an AR(1) process, the coefficient is less than unity.
. qui reg clcrmrte clprbarr clprbcon clprbpri clavgsen clpolpc clwcon-clwloc
d83-d87
. predict ehat, resid
(90 missing values generated)
. gen ehat_1  l.ehat
(180 missing values generated)
. reg ehat ehat_1
Source |
SS
df
MS
------------------------------------------Model | .490534556
1 .490534556
Residual | 10.3219221
448 .023040005
------------------------------------------Total | 10.8124566
449 .024081195

Number of obs
F( 1,
448)
Prob  F
R-squared
Adj R-squared
Root MSE








450
21.29
0.0000
0.0454
0.0432
.15179

----------------------------------------------------------------------------ehat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------ehat_1 |
-.222258
.0481686
-4.61
0.000
-.3169225
-.1275936
_cons |
5.97e-10
.0071554
0.00
1.000
-.0140624
.0140624
-----------------------------------------------------------------------------

10.10. a. To allow for different intercepts in the original model we can include a year
dummy for 1993 in the FD equation. (The three years of data are 1987, 1990, and 1993.) There
is no evidence of serial correlation in the FD errors, e it  u it − u i,t−1 , as the coefficient on ê i,t−1
is puny and so is its t statistic. It appears that a random walk for u it : t  1, 2, 3 is a
reasonably characterization, although concluding this with T  3 is tenuous.
. use murder
. xtset id year

174

panel variable:
time variable:
delta:

id (strongly balanced)
year, 87 to 93, but with gaps
1 unit

. reg cmrdrte d93 cexec cunem
Source |
SS
df
MS
------------------------------------------Model | 46.7620386
3 15.5873462
Residual | 1812.28688
98 18.4927233
------------------------------------------Total | 1859.04892
101
18.406425

Number of obs
F( 3,
98)
Prob  F
R-squared
Adj R-squared
Root MSE


102

0.84
 0.4736
 0.0252
 -0.0047
 4.3003

----------------------------------------------------------------------------cmrdrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d93 | -1.296717
1.016118
-1.28
0.205
-3.313171
.7197368
cexec | -.1150682
.1473871
-0.78
0.437
-.407553
.1774167
cunem |
.1630854
.3079049
0.53
0.598
-.447942
.7741127
_cons |
1.51099
.6608967
2.29
0.024
.1994622
2.822518
----------------------------------------------------------------------------. predict ehat, resid
(51 missing values generated)
. gen ehat_1  l.ehat
(102 missing values generated)
. reg ehat ehat_1
Source |
SS
df
MS
------------------------------------------Model | .075953071
1 .075953071
Residual | 58.3045094
49 1.18988795
------------------------------------------Total | 58.3804625
50 1.16760925

Number of obs
F( 1,
49)
Prob  F
R-squared
Adj R-squared
Root MSE


51

0.06
 0.8016
 0.0013
 -0.0191
 1.0908

----------------------------------------------------------------------------ehat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------ehat_1 |
.0065807
.0260465
0.25
0.802
-.0457618
.0589231
_cons | -9.10e-10
.1527453
-0.00
1.000
-.3069532
.3069532
-----------------------------------------------------------------------------

b. To make all of the FE and FD estimates comparable, the year dummies are differenced
along with the other variables in the FD estimation, and no constant is included. (The
R-squared for the FD equation is computed using the usual total sum of squares, but the FE and
FD R-squareds are not directly comparable.) The FE and FD coefficient estimates are similar
but, especially for the execution variable, the FD standard error is much smaller. Because these
are fully robust it is sensible to compare them. Because we found no serial correlation in the

175

FD errors, it makes sense that the FD estimator is more efficient than FE (whose idiosyncratic
errors appear to follow a random walk).
. gen cd90  d90 - d90[_n-1] if year  87
(51 missing values generated)
. gen cd93  d93 - d93[_n-1] if year  87
(51 missing values generated)
. reg cmrdrte cd90 cd93 cexec cunem, nocons tsscons cluster(id)
Number of obs 
F( 4,
50) 
Prob  F

R-squared

Root MSE


Linear regression

102
2.95
0.0291
0.0252
4.3003

(Std. Err. adjusted for 51 clusters in id
----------------------------------------------------------------------------|
Robust
cmrdrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cd90 |
1.51099
1.056408
1.43
0.159
-.6108675
3.632848
cd93 |
1.725264
.8603626
2.01
0.050
-.0028256
3.453353
cexec | -.1150682
.0386021
-2.98
0.004
-.1926027
-.0375337
cunem |
.1630854
.2998749
0.54
0.589
-.439231
.7654018
----------------------------------------------------------------------------. xtreg mrdrte d90 d93 exec unem, fe cluster(id)
Fixed-effects (within) regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.0734
between  0.0037
overall  0.0108

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(4,50)
Prob  F

 0.0010




153
51
3.
1.80
0.1443

(Std. Err. adjusted for 51 clusters in id
----------------------------------------------------------------------------|
Robust
mrdrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d90 |
1.556215
1.119004
1.39
0.170
-.6913706
3.8038
d93 |
1.733242
.8685105
2.00
0.051
-.0112126
3.477697
exec | -.1383231
.0805733
-1.72
0.092
-.3001593
.0235132
unem |
.2213158
.374899
0.59
0.558
-.5316909
.9743225
_cons |
5.822104
2.814864
2.07
0.044
.1682823
11.47593
---------------------------------------------------------------------------sigma_u | 8.7527226
sigma_e | 3.5214244
rho | .86068589
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

176

c. The explanatory variable exec it might fail strict exogeneity if states increase future
executions in response to current positive shocks to the murder rate. Given the relatively short
stretch of time, feedback from murder rates to future executions may not be much of a concern,
as the judicial process in capital cases tends to move slowly. (Of course, if it were sped up
because of an increase in murder rates, that could violate strict exogeneity.) With a longer time
series we could add exec i,t1 (and even values from further in the future) and estimate the
equation by FE or FD, testing exec i,t1 for statistical significance.
10.11. a. The key coefficient is  1 . Because AFDC participation gives women access to
better nutrition and prenatal care, we hope that AFDC participation causes the percent of
low-weight births to fall. This only makes sense witih a ceteris paribus thought experiment,
holding fixed economic and other variables, such as demographic variables and quality of
other kinds of health care. A reasonable expectation is  2  0: more physicians means
relatively fewer low-weight births. The variable bedspc is another proxy for health-care
availability, and we expect  3  0. Higher per capita income should lead to lower lowbrth, too
 4  0. The effect of population on a per capita variable is ambiguous, especially because it
is total population and not population density.
b. The Stata output follows. Both the usual and fully robust standard errors are computed.
The standard errors robust to serial correlation (and heteroskedasticity) are, as expected,
somewhat larger. (If you test for AR(1) serial correlation in the composite error, v it it is very
strong. In fact, the estimated  is slightly above one). Only the per capita income variable is
statistically significant. The estimate implies that a 10 percent rise in per capita income is
associated with a .25 percentage point fall in the percent of low-weight births.
. reg lowbrth d90 afdcprc lphypc lbedspc lpcinc lpopul

177

Source |
SS
df
MS
------------------------------------------Model | 33.7710894
6
5.6285149
Residual | 100.834005
93 1.08423661
------------------------------------------Total | 134.605095
99 1.35964742

Number of obs
F( 6,
93)
Prob  F
R-squared
Adj R-squared
Root MSE








100
5.19
0.0001
0.2509
0.2026
1.0413

----------------------------------------------------------------------------lowbrth |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d90 |
.5797136
.2761244
2.10
0.038
.0313853
1.128042
afdcprc |
.0955932
.0921802
1.04
0.302
-.0874584
.2786448
lphypc |
.3080648
.71546
0.43
0.668
-1.112697
1.728827
lbedspc |
.2790041
.5130275
0.54
0.588
-.7397668
1.297775
lpcinc | -2.494685
.9783021
-2.55
0.012
-4.4374
-.5519711
lpopul |
.739284
.7023191
1.05
0.295
-.6553826
2.133951
_cons |
26.57786
7.158022
3.71
0.000
12.36344
40.79227
----------------------------------------------------------------------------. reg lowbrth d90 afdcprc lphypc lbedspc lpcinc lpopul, cluster(state)
Number of obs 
F( 6,
49) 
Prob  F

R-squared

Root MSE


Linear regression

100
4.73
0.0007
0.2509
1.0413

(Std. Err. adjusted for 50 clusters in state
----------------------------------------------------------------------------|
Robust
lowbrth |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d90 |
.5797136
.2214303
2.62
0.012
.1347327
1.024694
afdcprc |
.0955932
.1199883
0.80
0.429
-.1455324
.3367188
lphypc |
.3080648
.9063342
0.34
0.735
-1.513282
2.129411
lbedspc |
.2790041
.7853754
0.36
0.724
-1.299267
1.857275
lpcinc | -2.494685
1.203901
-2.07
0.044
-4.914014
-.0753567
lpopul |
.739284
.9041915
0.82
0.418
-1.077757
2.556325
_cons |
26.57786
9.29106
2.86
0.006
7.906773
45.24894
-----------------------------------------------------------------------------

c. The FD (equivalently, FE) estimates are given below. The heteroskedasticity-robust
standard error for the AFDC variable is actually smaller. In any case, removing the state
unobserved effect changes the sign on the AFDC participation variable, and it is marginally
statistically significant. Oddly, physicians-per-capita now has a positive, significant effect on
percent of low-weight births. The hospital beds-per-capita variable has the expected negative
effect.

178

. reg clowbrth cafdcprc clphypc clbedspc clpcinc clpopul
Source |
SS
df
MS
------------------------------------------Model | .861531934
5 .172306387
Residual | 3.00026764
44 .068187901
------------------------------------------Total | 3.86179958
49 .078812236

Number of obs
F( 5,
44)
Prob  F
R-squared
Adj R-squared
Root MSE








50
2.53
0.0428
0.2231
0.1348
.26113

----------------------------------------------------------------------------clowbrth |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cafdcprc | -.1760763
.0903733
-1.95
0.058
-.3582116
.006059
clphypc |
5.894509
2.816689
2.09
0.042
.2178452
11.57117
clbedspc | -1.576195
.8852111
-1.78
0.082
-3.360221
.2078308
clpcinc | -.8455268
1.356773
-0.62
0.536
-3.579924
1.88887
clpopul |
3.441116
2.872175
1.20
0.237
-2.347372
9.229604
_cons |
.1060158
.3090664
0.34
0.733
-.5168667
.7288983
----------------------------------------------------------------------------. reg clowbrth cafdcprc clphypc clbedspc clpcinc clpopul, robust
Number of obs 
F( 5,
44) 
Prob  F

R-squared

Root MSE


Linear regression

50
1.97
0.1024
0.2231
.26113

----------------------------------------------------------------------------|
Robust
clowbrth |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cafdcprc | -.1760763
.0767568
-2.29
0.027
-.3307695
-.021383
clphypc |
5.894509
3.098646
1.90
0.064
-.3504018
12.13942
clbedspc | -1.576195
1.236188
-1.28
0.209
-4.067567
.9151775
clpcinc | -.8455268
1.484034
-0.57
0.572
-3.8364
2.145346
clpopul |
3.441116
2.687705
1.28
0.207
-1.975596
8.857829
_cons |
.1060158
.3675668
0.29
0.774
-.6347664
.8467981
-----------------------------------------------------------------------------

d. Adding a quadratic in afdcprc yields a diminishing impact of AFDC participation. The
turning point in the quadratic is at about afdcprc  6. 4, and only four states have AFDC
participation rates above 6.4 percent. So, the largest effect is at low AFDC participation rates
and the effect is negative until afdcprc  6. 4. It is not clear this makes sense: if AFDC
participation increases then more women in living in poverty get better prenatal care for their
children. But the quadratic is not statistically significant at the usual levels and we could safely
drop it.
179

. reg clowbrth cafdcprc cafdcpsq clphypc clbedspc clpcinc clpopul, robust
Number of obs 
F( 6,
43) 
Prob  F

R-squared

Root MSE


Linear regression

50
2.07
0.0762
0.2499
.25956

----------------------------------------------------------------------------|
Robust
clowbrth |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cafdcprc | -.5035049
.2612029
-1.93
0.061
-1.030271
.023261
cafdcpsq |
.0396094
.0317531
1.25
0.219
-.0244267
.1036456
clphypc |
6.620885
3.448026
1.92
0.061
-.332723
13.57449
clbedspc | -1.407963
1.344117
-1.05
0.301
-4.118634
1.302707
clpcinc | -.9987865
1.541609
-0.65
0.521
-4.107738
2.110165
clpopul |
4.429026
2.925156
1.51
0.137
-1.470113
10.32817
_cons |
.1245915
.386679
0.32
0.749
-.655221
.9044041
----------------------------------------------------------------------------. di abs(_b[cafdcprc]/(2*_b[cafdcpsq]))
6.3558685
. sum afdcprc if d90
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------afdcprc |
50
4.162976
1.317277
1.688183
7.358795
. count if afdcprc  6.4 & d90
4

10.12. a. Even if c i is uncorrelated with x it for all t, the usual OLS standard errors do not
account for the serial correlation in v it  c i  u it . You can see that the fully robust standard
errors are substantially larger than the usual ones, in some cases more than double.
. use wagepan
. reg lwage educ black hisp exper expersq married union d81-d87, cluster(nr)
Linear regression

Number of obs
F( 14,
544)
Prob  F
R-squared
Root MSE







4360
47.10
0.0000
0.1893
.48033

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
----------------------------------------------------------------------------

180

educ |
.0913498
.0110822
8.24
0.000
.0695807
.1131189
black | -.1392342
.0505238
-2.76
0.006
-.2384798
-.0399887
hisp |
.0160195
.0390781
0.41
0.682
-.060743
.092782
exper |
.0672345
.0195958
3.43
0.001
.0287417
.1057273
expersq | -.0024117
.0010252
-2.35
0.019
-.0044255
-.0003979
married |
.1082529
.026034
4.16
0.000
.0571135
.1593924
union |
.1824613
.0274435
6.65
0.000
.1285531
.2363695
d81 |
.05832
.028228
2.07
0.039
.0028707
.1137693
d82 |
.0627744
.0369735
1.70
0.090
-.0098538
.1354027
d83 |
.0620117
.046248
1.34
0.181
-.0288348
.1528583
d84 |
.0904672
.057988
1.56
0.119
-.0234407
.204375
d85 |
.1092463
.0668474
1.63
0.103
-.0220644
.240557
d86 |
.1419596
.0762348
1.86
0.063
-.007791
.2917102
d87 |
.1738334
.0852056
2.04
0.042
.0064611
.3412057
_cons |
.0920558
.1609365
0.57
0.568
-.2240773
.4081888
----------------------------------------------------------------------------. reg lwage educ black hisp exper expersq married union d81-d87
Source |
SS
df
MS
------------------------------------------Model | 234.048277
14 16.7177341
Residual | 1002.48136 4345 .230720682
------------------------------------------Total | 1236.52964 4359 .283672779

Number of obs
F( 14, 4345)
Prob  F
R-squared
Adj R-squared
Root MSE








4360
72.46
0.0000
0.1893
0.1867
.48033

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.0913498
.0052374
17.44
0.000
.0810819
.1016177
black | -.1392342
.0235796
-5.90
0.000
-.1854622
-.0930062
hisp |
.0160195
.0207971
0.77
0.441
-.0247535
.0567925
exper |
.0672345
.0136948
4.91
0.000
.0403856
.0940834
expersq | -.0024117
.00082
-2.94
0.003
-.0040192
-.0008042
married |
.1082529
.0156894
6.90
0.000
.0774937
.1390122
union |
.1824613
.0171568
10.63
0.000
.1488253
.2160973
d81 |
.05832
.0303536
1.92
0.055
-.0011886
.1178286
d82 |
.0627744
.0332141
1.89
0.059
-.0023421
.1278909
d83 |
.0620117
.0366601
1.69
0.091
-.0098608
.1338843
d84 |
.0904672
.0400907
2.26
0.024
.011869
.1690654
d85 |
.1092463
.0433525
2.52
0.012
.0242533
.1942393
d86 |
.1419596
.046423
3.06
0.002
.0509469
.2329723
d87 |
.1738334
.049433
3.52
0.000
.0769194
.2707474
_cons |
.0920558
.0782701
1.18
0.240
-.0613935
.2455051
-----------------------------------------------------------------------------

b. The random effects estimates on the time-constant variables are similar to the pooled
OLS estimates. The coefficients on the quadratic in experience for RE show an initially
stronger effect of experience, but with the slope diminishing more rapidly. There are important
differences in the variables that chance across individual and time; they are notably lower for
random effects. The random effects marriage premium is about 6.4%, while the pooled OLS

181

estimate is about 10.8%. For union status, the random effects estimate is 10.6% compared with
a pooled OLS estimate of 18.2%.
Note that the RE standard errors for the coefficients on the time-constant explanatory
variables are similar to the fully robust POLS standard errors. However, the RE standard errors
for married and union are substantially smaller than the robust POLS standard errors,
suggestive of the relative efficiency of RE. To be fair, we should compute the fully robust
standard errors for RE. As shown below, these are somewhat larger than the usual RE standard
errors, but for the married and union still not nearly as large as the robust standard errors for
POLS. An important conclusion is that, even though RE might not be the asymptotically
efficient FGLS estimator, it appears to be more efficient than POLS, at least for the
time-varying explanatory variables.
. xtreg lwage educ black hisp exper expersq married union d81-d87, re
Random-effects GLS regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1799
between  0.1860
overall  0.1830




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(14)
Prob  chi2




4360
545
8.
957.77
0.0000

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.0918763
.0106597
8.62
0.000
.0709836
.1127689
black | -.1393767
.0477228
-2.92
0.003
-.2329117
-.0458417
hisp |
.0217317
.0426063
0.51
0.610
-.0617751
.1052385
exper |
.1057545
.0153668
6.88
0.000
.0756361
.1358729
expersq | -.0047239
.0006895
-6.85
0.000
-.0060753
-.0033726
married |
.063986
.0167742
3.81
0.000
.0311091
.0968629
union |
.1061344
.0178539
5.94
0.000
.0711415
.1411273
d81 |
.040462
.0246946
1.64
0.101
-.0079385
.0888626
d82 |
.0309212
.0323416
0.96
0.339
-.0324672
.0943096
d83 |
.0202806
.041582
0.49
0.626
-.0612186
.1017798
d84 |
.0431187
.0513163
0.84
0.401
-.0574595
.1436969
d85 |
.0578155
.0612323
0.94
0.345
-.0621977
.1778286
d86 |
.0919476
.0712293
1.29
0.197
-.0476592
.2315544
d87 |
.1349289
.0813135
1.66
0.097
-.0244427
.2943005
_cons |
.0235864
.1506683
0.16
0.876
-.271718
.3188907

182

---------------------------------------------------------------------------sigma_u | .32460315
sigma_e | .35099001
rho | .46100216
(fraction of variance due to u_i)
----------------------------------------------------------------------------. xtreg lwage educ black hisp exper expersq married union d81-d87, re
cluster(nr)
Random-effects GLS regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1799
between  0.1860
overall  0.1830




4360
545

Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(14)
Prob  chi2




8.
610.97
0.0000

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.0918763
.0111455
8.24
0.000
.0700315
.1137211
black | -.1393767
.0509251
-2.74
0.006
-.2391882
-.0395653
hisp |
.0217317
.0399157
0.54
0.586
-.0565015
.099965
exper |
.1057545
.016379
6.46
0.000
.0736522
.1378568
expersq | -.0047239
.0007917
-5.97
0.000
-.0062756
-.0031723
married |
.063986
.0189722
3.37
0.001
.0268013
.1011708
union |
.1061344
.020844
5.09
0.000
.065281
.1469879
d81 |
.040462
.0275684
1.47
0.142
-.0135711
.0944951
d82 |
.0309212
.0350705
0.88
0.378
-.0378158
.0996581
d83 |
.0202806
.043861
0.46
0.644
-.0656853
.1062466
d84 |
.0431187
.0555848
0.78
0.438
-.0658254
.1520628
d85 |
.0578155
.0645584
0.90
0.370
-.0687167
.1843476
d86 |
.0919476
.0747028
1.23
0.218
-.0544671
.2383623
d87 |
.1349289
.0848618
1.59
0.112
-.0313971
.3012549
_cons |
.0235864
.1599577
0.15
0.883
-.289925
.3370977
---------------------------------------------------------------------------sigma_u | .32460315
sigma_e | .35099001
rho | .46100216
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

c. The variable exper it is redundant because everyone in the sample works every year, so
exper i,t1  exper it  1, t  1, … , 7, for all i. The effects of the initial levels of experience,
exper i1 , cannot be distinguished from c i because we are allowing exper i1 to be correlated with
c i . Then, because each experience variable follows the same linear time trend, the effects
cannot be separated from the aggregate time effects (year dummies).

183

TheFE estimates follow. The marriage and union premiums fall even more, although both
are still statistically significant and economically relevant. The fully robust standard errors are
somewhat larger than the usual FE standard errors.
. xtreg lwage expersq married union d81-d87, fe
Fixed-effects (within) regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1806
between  0.0286
overall  0.0888

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(10,3805)
Prob  F

 -0.1222




4360
545
8.
83.85
0.0000

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------expersq | -.0051855
.0007044
-7.36
0.000
-.0065666
-.0038044
married |
.0466804
.0183104
2.55
0.011
.0107811
.0825796
union |
.0800019
.0193103
4.14
0.000
.0421423
.1178614
d81 |
.1511912
.0219489
6.89
0.000
.1081584
.194224
d82 |
.2529709
.0244185
10.36
0.000
.2050963
.3008454
d83 |
.3544437
.0292419
12.12
0.000
.2971125
.4117749
d84 |
.4901148
.0362266
13.53
0.000
.4190894
.5611402
d85 |
.6174823
.0452435
13.65
0.000
.5287784
.7061861
d86 |
.7654966
.0561277
13.64
0.000
.6554532
.8755399
d87 |
.9250249
.0687731
13.45
0.000
.7901893
1.059861
_cons |
1.426019
.0183415
77.75
0.000
1.390058
1.461979
---------------------------------------------------------------------------sigma_u | .39176195
sigma_e | .35099001
rho | .55472817
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(544, 3805) 
9.16
Prob  F  0.0000
. xtreg lwage expersq married union d81-d87, fe cluster(nr)
Fixed-effects (within) regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1806
between  0.0286
overall  0.0888

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(10,544)
Prob  F

 -0.1222




4360
545
8.
46.59
0.0000

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
----------------------------------------------------------------------------

184

expersq | -.0051855
.0008102
-6.40
0.000
-.0067771
-.0035939
married |
.0466804
.0210038
2.22
0.027
.0054218
.0879389
union |
.0800019
.0227431
3.52
0.000
.0353268
.1246769
d81 |
.1511912
.0255648
5.91
0.000
.1009733
.2014091
d82 |
.2529709
.0286624
8.83
0.000
.1966684
.3092733
d83 |
.3544437
.0348608
10.17
0.000
.2859655
.422922
d84 |
.4901148
.0454581
10.78
0.000
.4008199
.5794097
d85 |
.6174823
.0568088
10.87
0.000
.5058908
.7290737
d86 |
.7654966
.071244
10.74
0.000
.6255495
.9054436
d87 |
.9250249
.0840563
11.00
0.000
.7599103
1.09014
_cons |
1.426019
.0209824
67.96
0.000
1.384802
1.467235
---------------------------------------------------------------------------sigma_u | .39176195
sigma_e | .35099001
rho | .55472817
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

d. The following Stata session adds the year dummy-education interaction terms. There is
no evidence that the return to education has changed over time for the population represented
by these men. The p-value for the joint robust test is about . 89.
. gen d81educ  d81*educ
. gen d82educ  d82*educ
. gen d83educ  d83*educ
. gen d84educ  d84*educ
. gen d85educ  d85*educ
. gen d86educ  d86*educ
. gen d87educ  d87*educ
. xtreg lwage expersq married union d81-d87 d81educ-d87educ, fe cluster(nr)
Fixed-effects (within) regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1814
between  0.0211
overall  0.0784

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(17,544)
Prob  F

 -0.1732




4360
545
8.
28.33
0.0000

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------expersq | -.0060437
.0010323
-5.85
0.000
-.0080715
-.0040159
married |
.0474337
.0210293
2.26
0.024
.006125
.0887423

185

union |
.0789759
.022762
3.47
0.001
.0342638
.123688
d81 |
.0984201
.1463954
0.67
0.502
-.1891495
.3859897
d82 |
.2472016
.1490668
1.66
0.098
-.0456155
.5400186
d83 |
.408813
.1716953
2.38
0.018
.071546
.74608
d84 |
.6399247
.1873708
3.42
0.001
.2718659
1.007984
d85 |
.7729397
.2090195
3.70
0.000
.3623554
1.183524
d86 |
.9699322
.2463734
3.94
0.000
.4859724
1.453892
d87 |
1.188777
.2580167
4.61
0.000
.6819456
1.695608
d81educ |
.0049906
.0122858
0.41
0.685
-.0191429
.0291241
d82educ |
.001651
.012194
0.14
0.892
-.0223021
.025604
d83educ | -.0026621
.0136788
-0.19
0.846
-.0295319
.0242076
d84educ | -.0098257
.0146869
-0.67
0.504
-.0386757
.0190243
d85educ | -.0092145
.0151166
-0.61
0.542
-.0389085
.0204796
d86educ | -.0121382
.0168594
-0.72
0.472
-.0452558
.0209794
d87educ | -.0157892
.0163557
-0.97
0.335
-.0479172
.0163389
_cons |
1.436283
.0227125
63.24
0.000
1.391668
1.480897
---------------------------------------------------------------------------sigma_u | .39876325
sigma_e |
.3511451
rho | .56324361
(fraction of variance due to u_i)
----------------------------------------------------------------------------. testparm d81educ-d87educ
(
(
(
(
(
(
(

1)
2)
3)
4)
5)
6)
7)

d81educ
d82educ
d83educ
d84educ
d85educ
d86educ
d87educ
F(









0
0
0
0
0
0
0

7,
544) 
Prob  F 

0.43
0.8851

e. First, I created the lead variable, and then included it in the FE estimation with fully
robust inference. As you can see, unionp1 is statistically significant with p-value  . 029, and
its coefficient is not small. It seems union it fails the strict exogeneity assumption, and we
possibly should use an IV approach as described in Chapter 11. (However, coming up with
instruments is not trivial.)
. gen unionp1  union[_n1] if year  1987
(545 missing values generated)
. xtreg lwage expersq married union unionp1 d81-d86, fe cluster(nr)
Fixed-effects (within) regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1474
between  0.0305




Obs per group: min 
avg 

186

3815
545
7.

overall  0.0744
corr(u_i, Xb)

max 
F(10,544)
Prob  F

 -0.1262




31.29
0.0000

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------expersq | -.0054448
.0009786
-5.56
0.000
-.0073671
-.0035226
married |
.0448778
.0235662
1.90
0.057
-.0014141
.0911697
union |
.0763554
.0236392
3.23
0.001
.0299202
.1227906
unionp1 |
.0497356
.0227497
2.19
0.029
.0050477
.0944236
d81 |
.1528275
.0257236
5.94
0.000
.1022977
.2033573
d82 |
.2576486
.0304975
8.45
0.000
.1977413
.3175558
d83 |
.3618296
.0384587
9.41
0.000
.2862839
.4373754
d84 |
.5023642
.0517471
9.71
0.000
.4007155
.6040129
d85 |
.6342402
.065288
9.71
0.000
.5059928
.7624876
d86 |
.7841312
.0826431
9.49
0.000
.6217924
.9464699
_cons |
1.417924
.0225168
62.97
0.000
1.373693
1.462154
---------------------------------------------------------------------------sigma_u | .39716048
sigma_e | .35740734
rho |
.5525375
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

f. The Stata output shows that the union premium for Hispanic men is well below that of
non-black men: about 13 percentage points lower. The difference is statistically significant,
too. The estimated union “premium” is actually about −3. 5% for Hispanics, although it is not
statistically different from zero. The estimated wage premium for black men is about 7.1
percentage points higher than the base group, but the difference is not statistically significant.
. gen black_union  black*union
. gen hisp_union  hisp*union
. xtreg lwage expersq married union black_union hisp_union d81-d87, fe
Fixed-effects (within) regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1830
between  0.0267
overall  0.0871

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(12,3803)
Prob  F

 -0.1360




4360
545
8.
70.99
0.0000

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
----------------------------------------------------------------------------

187

expersq |
-.005308
.0007048
-7.53
0.000
-.0066898
-.0039262
married |
.0461639
.0182922
2.52
0.012
.0103004
.0820275
union |
.0957205
.0244326
3.92
0.000
.0478183
.1436227
black_union |
.0714378
.0532042
1.34
0.179
-.0328737
.1757492
hisp_union | -.1302478
.0485409
-2.68
0.007
-.2254166
-.0350791
d81 |
.1507003
.0219236
6.87
0.000
.1077172
.1936833
d82 |
.2545937
.0243936
10.44
0.000
.206768
.3024194
d83 |
.3576139
.029227
12.24
0.000
.3003119
.414916
d84 |
.4947141
.0362132
13.66
0.000
.423715
.5657132
d85 |
.6236823
.0452345
13.79
0.000
.5349961
.7123686
d86 |
.7750896
.0561524
13.80
0.000
.664998
.8851813
d87 |
.9344805
.0687783
13.59
0.000
.7996347
1.069326
_cons |
1.42681
.0183207
77.88
0.000
1.390891
1.462729
---------------------------------------------------------------------------sigma_u |
.393505
sigma_e | .35056318
rho | .55752062
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(544, 3803) 
9.17
Prob  F  0.0000
. xtreg lwage expersq married union black_union hisp_union d81-d87, fe
cluster(nr)
Fixed-effects (within) regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1830
between  0.0267
overall  0.0871

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(12,544)
Prob  F

 -0.1360




4360
545
8.
40.16
0.0000

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------expersq |
-.005308
.0008095
-6.56
0.000
-.0068982
-.0037178
married |
.0461639
.0209641
2.20
0.028
.0049834
.0873444
union |
.0957205
.0304494
3.14
0.002
.0359077
.1555333
black_union |
.0714378
.0600866
1.19
0.235
-.0465925
.189468
hisp_union | -.1302478
.0493283
-2.64
0.009
-.2271451
-.0333505
d81 |
.1507003
.025519
5.91
0.000
.1005725
.2008281
d82 |
.2545937
.0286062
8.90
0.000
.1984014
.3107859
d83 |
.3576139
.0348463
10.26
0.000
.2891641
.4260638
d84 |
.4947141
.0453929
10.90
0.000
.4055473
.5838809
d85 |
.6236823
.056721
11.00
0.000
.5122633
.7351014
d86 |
.7750896
.071142
10.89
0.000
.635343
.9148363
d87 |
.9344805
.0838788
11.14
0.000
.7697145
1.099247
_cons |
1.42681
.0209431
68.13
0.000
1.385671
1.467949
---------------------------------------------------------------------------sigma_u |
.393505
sigma_e | .35056318
rho | .55752062
(fraction of variance due to u_i)
----------------------------------------------------------------------------. lincom union  hisp_union

188

( 1)

union  hisp_union  0

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------(1) | -.0345273
.0388508
-0.89
0.375
-.1108432
.0417886
-----------------------------------------------------------------------------

g. We reject the null hypothesis that union it : t  1, . . . , T is strictly exogenous even
when the union premium is allowed to differ by race and ethnicity.
. xtreg lwage expersq married union black_union hisp_union d81-d86 unionp1, fe
cluster(nr)
Fixed-effects (within) regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1497
between  0.0293
overall  0.0735

corr(u_i, Xb)




3815
545

Obs per group: min 
avg 
max 
F(12,544)
Prob  F

 -0.1386




7.
27.21
0.0000

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------expersq | -.0055477
.0009833
-5.64
0.000
-.0074793
-.0036162
married |
.044659
.0235905
1.89
0.059
-.0016807
.0909986
union |
.0886305
.031931
2.78
0.006
.0259075
.1513536
black_union |
.0849246
.0627531
1.35
0.177
-.0383434
.2081926
hisp_union | -.1179177
.0525974
-2.24
0.025
-.2212365
-.0145988
d81 |
.1522945
.0256633
5.93
0.000
.1018831
.2027058
d82 |
.2589966
.0304368
8.51
0.000
.1992085
.3187846
d83 |
.3643699
.0385023
9.46
0.000
.2887385
.4400013
d84 |
.506142
.0518005
9.77
0.000
.4043885
.6078955
d85 |
.6393639
.0654104
9.77
0.000
.5108761
.7678517
d86 |
.7921705
.0828197
9.57
0.000
.6294849
.954856
unionp1 |
.0502016
.0227235
2.21
0.028
.0055649
.0948382
_cons |
1.418021
.022508
63.00
0.000
1.373808
1.462234
---------------------------------------------------------------------------sigma_u | .39854263
sigma_e | .35703614
rho |
.5547681
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

10.13. a. Showing that this procedure is consistent with fixed T as N   requires some
algebra. First, in the sum of squared residuals, we can “concentrate out” the a i by finding â i b
as a function of x i , y i  and b, substituting back into the sum of squared residuals, and then

189

minimizing with respect to b only. Straightforward algebra gives the first order conditions for
each i as
T

∑y it − a i − x it b/h it  0
t1

which implies
T


a i b  w i

T

∑ y it /h it

− wi

t1

T

where w i ≡ ∑ t1 1/h it 

−1

∑x it /h it 

b ≡ ȳ wi − x̄ wi b,

t1
T

 0 and ȳ wi ≡ w i ∑ t1 y it /h it , and a similar definition holds for

x̄ wi . Note that ȳ wi and x̄ wi are weighted averages with weights w i /h it , t  1, 2, . . . , T. If h it equals
−w
the same constant for all t, y −w
i and x i are simply weighted averages. If h it equals the same

constant for all t, ȳ wi and x̄ wi are the usual time averages.

Now we can plug each â i b into the SSR to get the problem solved by  FEWLS :
N

min
b

K

T

∑ ∑y it − ȳ wi  − x it − x̄ wi b 2 /h it .
i1 t1

This is just a pooled weighted least squares regression of y it − ȳ wi  on x it − x̄ wi  with weights
1/h it . Equivalently, define ỹ it ≡ y it − ȳ wi / h it , x̃ it ≡ x it − x̄ wi / h it , all
t  1, … , T, i  1, … , N. Then ̂ can be expressed in usual pooled OLS form:
̂ FEWLS 

N

−1

T

∑∑

x̃ ′it x̃ it

i1 t1

N

T

∑ ∑ x̃ ′it ỹ it
i1 t1

Note carefully how the initial y it are weighted by 1/h it to obtain y −w
i , but where the usual
1/ h it weighting shows up in the sum of squared residuals on the time-demeaned data (where
the demeaning is a weighted average). Given (10.90), we can easily study the asymptotic

190

(10.90)

T
w
w
N   properties of ̂. First, we can write ȳ wi  x −w
i  c i  ū i where ū i ≡ w i ∑ t1 u it /h it .

Subtracting this equation from y it  x it   c i  u it for all t gives ỹ it  x̃ it  ũ it , where
ũ it ≡ u it − ū wi / h it . When we plug this in for ỹ it in (10.90) and divide by N in the appropriate
places we get
̂ FEWLS   

N

T

N

N −1 ∑ ∑ x̃ ′it x̃ it

T

N −1 ∑ ∑ x̃ ′it u it / h it

i1 t1

.

(10.91)

i1 t1

From this last equation we can immediately read off the consistency of ̂ FEWLS regardless of
whether Varu it |x i , h i , c i    2u h it . Why? We assumed that Eu it |x i , h i , c i   0, which means u it
is uncorrelated with any function of x i , h i , including x̃ it . Therefore, Ex̃ ′it u it   0, t  1, … , T
under Eu it |x i , h i , c i   0 with any restrictions on the conditional second moments of
T

u it : t  1, . . . , T. As long as we assume that ∑ t1 Ex̃ ′it x̃ it  has rank K, we can apply the
consistency result for pooled OLS to conclude plim̂ FEWLS   . (We can even show that
E̂ FEWLS |X, H  , that is, the FEWLS estimator is conditionally unbiased.)
b. It is clear from (10.91) that ̂ FEWLS is N -asymptotically normal under mild assumptions
because we can write
N ̂ FEWLS −  

N

N

−1

T

∑∑

N

x̃ ′it x̃ it

i1 t1

N

−1/2

∑ ∑ x̃ ′it u it /
i1 t1

The asymptotic variance is generally
Avar N ̂ FEWLS −   A −1 BA −1 ,
where

191

T

h it

T

A≡

∑ Ex̃ ′it x̃ it 
t1

T

∑

B≡E

′

T

x̃ ′it u it /

∑

h it

t1

x̃ ′it u it /

h it

.

t1

Iif we assume that Covu it , u is |x i , h i , c i   Eu it u is |x i , h i , c i   0, t ≠ s then by a standard
iterated expectations argument,
x̃ ′it u it / h it

E

x̃ ′is u is / h is

′

E

u it u is x̃ ′it x̃ is / h it h is

 0, t ≠ s.

Further, given the variance assumption Varu it |x i , h i , c i   Eu 2it |x i , h i , c i    2u h it , iterated
expectations implies
E

x̃ ′it u it / h it

x̃ ′it u it / h it

′

 Eu 2it x̃ ′it x̃ it /h it    2u Ex̃ ′it x̃ it /h it .

It follows then that
B   2u A
and so
Avar N ̂ FEWLS −    2u A −1 .
c. The same subtleties that arise in estimating  2u for the usual fixed effects estimator crop
up here as well. Assume the zero conditional covariance assumption and correct variance
specification in part b. Then the residuals from the pooled OLS regression
ỹ it on x̃ it , t  1, … , T, i  1, … , N,

(10.92)



say ü it , are estimating ü it  u it − ū wi / h it in the sense that we obtain ü it from ü it by replacing
 with ̂ FEWLS ). Now
Eü 2it   Eü 2it /h it  − 2Eu it ū wi /h it   Eū wi  2 /h it    2u − 2 2u Ew i /h it    2u Ew i /h it ,
192

where the law of iterated expectations is applied several times, and Eū wi  2 |x i , h i    2u w i has
been used. Therefore, Eü 2it    2u 1 − Ew i /h it , t  1, … , T, and so
T

∑

T

Eü 2it 



 2u T

t1

− Ew i  ∑1/h it    2u T − 1.
t1

This contains the usual result for the within transformation as a special case. A consistent
estimator of  2u is SSR/NT − 1 − K, where SSR is the usual sum of squared residuals from
(10.92), and the subtraction of K is optional as a degrees-of-freedom adjustment. The estimator
of Avar̂ FEWLS  is then
N

̂ 2u

−1

T

∑∑

x̃ ′it x̃ it

.

i1 t1

d. If we want to allow serial correlation in the u it , or allow Varu it |x i , h i , c i  ≠  2u h it , then
we can just apply the robust formula for the pooled OLS regression (10.92). See equation
(7.77) in the text.
10.14. a. Because Eh i |z i   0, z i  and h i are uncorrelated, so
Varc i   Varz i    2h   ′ Varz i    2h ≥  2h . Assuming that Varz i  is positive definite –
which we must to satisfy the RE rank condition – strict inequality holds whenever  ≠ 0. Of
course  ′ Varz i   0 is possible even if Varz i  is not positive definite.
b. If we estimate the model by fixed effects, the associated estimate of the variance of the
unobserved effect is  2c . If we estimate the model by random effects (with, of course, z i
included), the variance component is  2h . This makes intuitive sense: with random effects we
are able to explicitly control for time-constant variances, and so the z i are effectively taken out
of c i with h i as the remainder.

193

c. Using equation (10.81), we obtain  h and  c as follows.
 h  1 − 1/1  T 2h / 2u 

1/2

 c  1 − 1/1  T 2c / 2u  1/2
so
 c −  h  1/1  T 2h / 2u 

1/2

− 1/1  T 2c / 2u  1/2 .

Therefore,  c −  h ≥ 0 iff 1/1  T 2h / 2u  ≥ 1/1  T 2c / 2u  (because 1  T 2h / 2u  and
1  T 2c / 2u  are positive). We conclude that
 c −  h ≥ 0 iff T 2c / 2u  ≥ T 2h / 2u 
which holds because we already showed that  2c ≥  2h (often with strict inequality).
d. If we use FE to estimate the heterogeneity variance then we are estimating  2c , which
means  c is effectively the quasi-time demeaning parameter used in subsequent RE estimation.
If we use POLS, then we estimate  2h , which then delivers the appropriate quasi-time
demeaning parameter  h . Thus, we should use POLS, not FE, as the initial estimator for RE
estimation when the model includes time-constant variables.
e. Because Problem 7.15 contains a more general result, a separate proof is not provided
here. One need not have an RE structure and, as mentioned in the test, we do not need
homoskedasticity of Varc i |z i , either.
10.15. a. Because v i is independent of x i , v̄ i is also independent of x i . Therefore,
Ev it |x i , v i   Ev it | v i , t  1, . . . , T.
Because we assume linearity, we know Ev it |v̄ i  is the linear projection (with intercept zero
because Ev it   0 for all t):

194

Ev it |v̄ i  

Covv̄ i , v it 
 v̄ i .
Var v i 

Now
T

−1

Covv̄ i , v it   T Cov

T

∑ v ir , v it

T

Varv it   ∑ Covv ir , v it 

−1

r1

T

−1

 2c



 2u 

r≠t

 T −

1 2c 



 2c



 2u /T

Also, because v̄ i  c i  ū i , and u it : t  1, . . . , T is serially uncorrelated with constant
variance, and each u it is uncorrelated with c i ,
Var v i   Varc i   Varū i    2c   2u /T  Cov v i , v it 
We have shown that the slope in the population regression of v it on v̄ i is unity, and so
Ev it |v̄ i   v̄ i .
b. Because ȳ i  x̄ i   v̄ i , Ev it |x i , v̄ i   Ev it |x i , ȳ i , and so
Ey it |x i , ȳ i   Ex it   v it |x i , ȳ i   x it   Ev it |x i , ȳ i   x it   v̄ i
 x it   ȳ i − x̄ i .
c. We can rewrite equation in part b as follows.
y it  x it   ȳ i − x̄ i   r it
Er it |x i , ȳ i   0
To impose the coefficient of unity on ȳ i and the common vector on x it and x̄ i , write
y it − ȳ i  x it − x̄ i   r it , t  1, . . . , T,
which we can estimate consistently by POLS because Er it |x i   0 for all t.
d. The RE estimator is not based on Ey it |x i , y i   Ey it |x it , ȳ i , x̄ i , but on
Ey it |x i   Ey it |x it . We now see that the FE estimator can be given an interpretion as an

195

estimator that “controls for” ȳ i , along with x̄ i (even though it does not need to under the RE
assumptions).
e. Under Assumptions RE1 to RE3 we can derive
Lv it |x i , v̄ i   Lv it |v̄ i , t  1, . . . , T
without further assumptions. First, we know that the LP has the form
Lv it |x i , v̄ i   x i  t   t v̄ i ≡ w i  t
where
 t  Ew ′i w i  −1 Ew ′i v it 
But Ew ′i w i  is block diagonal because Ex ′i v̄ i   0. Further, Ex ′i v it   0, and so
0

t 

t

It is easy to see that
t 

Ev̄ i v it 
Covv̄ i , v it 

 1,
2
Varv̄ i 
Ev̄ i 

10.16. a. By independence between v i1 , v i2 , . . . , v iT , v i,T1  and x i1 , x i2 , . . . , x iT , x i,T1 , it
follows immediately that
Ev i,T1 |x i , x i,T1 , v i1 , . . . , v iT   Ev i,T1 |v i1 , . . . , v iT .
Because we are assuming that all conditional expectations involving v it  are linear, we know
Ev i,T1 |v i1 , . . . , v iT  is a linear function of v i1 , . . . , v iT . The tricky part is to show that
Ev i,T1 |v i1 , . . . , v iT   Ev i,T1 |v̄ i . Intuitively it makes sense that the elements in v i1 , . . . , v iT 
should get equal weight under the RE variance-covariance structure. One way to verify that
each element gets the same weight, and to determine that common weight, is to note that the
196

vector in the LP, say  T , satisfies
 2c   2u
 2c

 2c



 2c

 2c   2u 

 2c



T 







 2c

 2c



 2c  2c   2u

 2c


.

 2c

If we hypothesis that  T   T j ′T , where j T is the T  1 vector of ones, then
T 2c   2u  T   2c
so
 2c
T 2c   2u 

T 

This must be the unique solution because the RE variance matrix is assumed to be nonsingular
( 2u  0). So we have shown
Ev i,T1 |x i , x i,T1 , v i1 , . . . , v iT  

 2c
T 2c   2u 

v i1  v i2 . . . v iT 



T 2c
T 2c   2u 

v̄ i

 2c
  2u /T

v̄ i



 2c

which is what the problem asked to show.
b. This is straightforward given part a:
Ey i,T1 |x i , x i,T1 , v i1 , . . . , v iT   x i,T1   Ev i,T1 |x i , x i,T1 , v i1 , . . . , v iT 
 x i,T1   Ev i,T1 |v̄ i   x i,T1  

 2c
 2c   2u /T

v̄ i

c. If we condition only on the history of covariates, and not the past composite errors, we
get

197

Ey i,T1 |x i , x i,T1   x i,T1   Ev i,T1 |x i , x i,T1   x i,T1   0  x i,T1 

because we are assuming RE.1 for all T  1 time periods.
d. Forecast errors are given by:
y i,T1 − Ey i,T1 |x i , x i,T1 , v i1 , . . . , v iT   x i,T1   v i,T1  − x i,T1  −
 v i,T1 −

 2c
 2c   2u /T

 2c
 2c   2u /T

v̄ i

v̄ i

Let    2c / 2c   2u /T. Then the variance of the forecast error is
Varv i,T1 − v̄ i   Varv i,T1    2 Varv̄ i  − 2Covv i,T1 , v̄ i 
  2c   2u − 2 2c    2  2c   2u /T
 2c  2c
 2c  2c
  2c   2u 
−
2
 2c   2u /T
 2c   2u /T
  2c   2u −  2c .
If we use Ey i,T1 |x i , x i,T1  to forecast y i,T1 the forecast error is simply
v i,T1  y i,T1 − Ey i,T1 |x i , x i,T1 , which has variance  2c   2u . Because  2c ≥ 0 and  ≥ 0,
Varv i,T1 − v̄ i  ≤ Varv i,T1 
with strict inequality when  2c  0. Of course, all we have really shown is that
VarEv i,T1 |v̄ i  ≤ Varv i,T1 , which we already know from general properties of conditional
expectations. Using more information in a conditional mean results in a smaller prediction
error variance.
e. We can use N cross section observations and the first T time periods to estimate the
parameters by random effects. Let ̂ RE be the RE estimator, and let
̂ 

̂ 2c
.
̂ 2c  ̂ 2u /T

198

Then

y i,T1  x i,T1 ̂ RE  ̂ v i
where
T

vi  1
T

∑ v̂ it and v̂ it  y it − x it ̂ RE .
t1

10.17. a. By the usual averaging across time, the quasi-time-demeaned equation can be
written, for each time period, as
y it − ȳ i  1 −   d t −  d   w it − w i   v it − v̄ i 
 1 −   1 −  d   d t − d   w it − w i   v it − v̄ 
which is what we wanted to show by letting   1 −   1 −  d .
b. The first part – the N -asymptotic representation of the RE estimator – is just the usual
linear representation of a pooled OLS estimator laid out in Chapter 7. It also follows from the
discussion of random effects in Section 10.7.2. For the second part, write
v it − v̄ i  c i  u it  − c i  ū i   1 − c i  u it − ū i , and plug in:
T

T

t1

t1

T

T

∑d t − d v it − v̄ i   ∑d t − d 1 − c i  ∑d t − d u it − ∑d t − d ū i
t1

t1

T

T

T

t1

t1

t1

 1 − c i ∑d t − d   ∑d t − d u it − ū i  ∑d t − d 
T



∑d t − d u it
t1

T

because ∑ t1 d t − d   0.
c. Actually, there is nothing to do here. This is just the usual first-order representation of
the fixed effects estimator, which follows from the general pooled OLS results.

199

d. From part b we can write
N
T

A 1 N   RE −  N −1/2 ∑ ∑ g ′it v it − v̄ i 

N

 o p 1 ≡

i1 t1

T

N −1/2 ∑ ∑ r it

 o p 1,

i1 t1

where r it  d t − d u it , w it − w i v it − v̄ i  ′ . From part c we can write
A2

N
T

−1/2
N   FE −  N
∑ ∑ h ′it u it

N

 o p 1 ≡

i1 t1

N

−1/2

T

∑ ∑ s it

 o p 1,

i1 t1

where s it  d t − d u it , w it − w i u it  ′ . But the first R elements of r it and s it are d t − d  ′ u it ,
which implies that
N
T


A 1 N   RE −  − A 2 N   FE −   N −1/2 ∑ ∑
i1 t1

0
w it − w i  ′ e it − w it − w i  ′ u it

 o p 1,

where “0” is R  1 and e it  v it − v it . The second part of the vector is M  1 and generally
satisfies the central limit theorem. Under standard rank assumptions
Varw it − w i  ′ e it − w it − w i  ′ u it  has rank M
e. If there were no w it , part d would imply that the limiting distribution of the difference
between RE and FE is degenerate. In other words, we cannot compute the Hausman test
comparing the FE and RE estimators if the only time-varying covariates are aggregates. (In
fact, the FE and RE estimates are numerically identical in this case.) More generally, the
variance-covariance matrix of the difference has rank M, not M  R (whether or not we assume
RE.3 under H 0 ). A properly computed Hausman test will have only M degrees-of-freedom, not
M  R. The regression-based test from equation (10.88) forces one to get the
degrees-of-freedom corret, as there is obviously no value in adding d , a vector of constants, to
the regression.

200

10.18. a. The Stata results are given below. All estimates are numerically identical.
. reg lwage d81-d87
Source |
SS
df
MS
------------------------------------------Model | 92.9668229
7 13.2809747
Residual | 1143.56282 4352 .262767192
------------------------------------------Total | 1236.52964 4359 .283672779

Number of obs
F( 7, 4352)
Prob  F
R-squared
Adj R-squared
Root MSE








4360
50.54
0.0000
0.0752
0.0737
.51261

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
.1193902
.0310529
3.84
0.000
.0585107
.1802697
d82 |
.1781901
.0310529
5.74
0.000
.1173106
.2390696
d83 |
.2257865
.0310529
7.27
0.000
.1649069
.286666
d84 |
.2968181
.0310529
9.56
0.000
.2359386
.3576976
d85 |
.3459333
.0310529
11.14
0.000
.2850538
.4068128
d86 |
.4062418
.0310529
13.08
0.000
.3453623
.4671213
d87 |
.4730023
.0310529
15.23
0.000
.4121228
.5338818
_cons |
1.393477
.0219577
63.46
0.000
1.350429
1.436525
----------------------------------------------------------------------------. xtreg lwage d81-d87, re
Random-effects GLS regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.0000
between  0.0000
overall  0.0752




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(7)
Prob  chi2




4360
545
8.
738.94
0.0000

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
.1193902
.021487
5.56
0.000
.0772765
.1615039
d82 |
.1781901
.021487
8.29
0.000
.1360764
.2203038
d83 |
.2257865
.021487
10.51
0.000
.1836728
.2679001
d84 |
.2968181
.021487
13.81
0.000
.2547044
.3389318
d85 |
.3459333
.021487
16.10
0.000
.3038196
.388047
d86 |
.4062418
.021487
18.91
0.000
.3641281
.4483555
d87 |
.4730023
.021487
22.01
0.000
.4308886
.515116
_cons |
1.393477
.0219577
63.46
0.000
1.350441
1.436513
---------------------------------------------------------------------------sigma_u | .37007665
sigma_e | .35469771
rho | .52120938
(fraction of variance due to u_i)
----------------------------------------------------------------------------. xtreg lwage d81-d87, fe
Fixed-effects (within) regression
Group variable: nr

Number of obs
Number of groups

201




4360
545

R-sq:

within  0.1625
between 
.
overall  0.0752

corr(u_i, Xb)

Obs per group: min 
avg 
max 
F(7,3808)
Prob  F

 0.0000

8.




105.56
0.0000

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
.1193902
.021487
5.56
0.000
.0772631
.1615173
d82 |
.1781901
.021487
8.29
0.000
.136063
.2203172
d83 |
.2257865
.021487
10.51
0.000
.1836594
.2679135
d84 |
.2968181
.021487
13.81
0.000
.254691
.3389452
d85 |
.3459333
.021487
16.10
0.000
.3038063
.3880604
d86 |
.4062418
.021487
18.91
0.000
.3641147
.4483688
d87 |
.4730023
.021487
22.01
0.000
.4308753
.5151294
_cons |
1.393477
.0151936
91.71
0.000
1.363689
1.423265
---------------------------------------------------------------------------sigma_u | .39074676
sigma_e | .35469771
rho | .54824631
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(544, 3808) 
9.71
Prob  F  0.0000
. reg d.(lwage d81-d87), nocons
Source |
SS
df
MS
------------------------------------------Model | 19.3631642
7 2.76616631
Residual | 749.249837 3808 .196756785
------------------------------------------Total | 768.613001 3815 .201471298

Number of obs
F( 7, 3808)
Prob  F
R-squared
Adj R-squared
Root MSE








3815
14.06
0.0000
0.0252
0.0234
.44357

----------------------------------------------------------------------------D.lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
D1. |
.1193902
.0190006
6.28
0.000
.0821379
.1566425
|
d82 |
D1. |
.1781901
.0268709
6.63
0.000
.1255074
.2308728
|
d83 |
D1. |
.2257865
.03291
6.86
0.000
.1612636
.2903093
|
d84 |
D1. |
.2968181
.0380011
7.81
0.000
.2223136
.3713226
|
d85 |
D1. |
.3459333
.0424866
8.14
0.000
.2626347
.4292319
|
d86 |
D1. |
.4062418
.0465417
8.73
0.000
.3149927
.4974908
|
d87 |
D1. |
.4730023
.0502708
9.41
0.000
.3744421
.5715626
-----------------------------------------------------------------------------

202

b. The Stata output follows. The POLS and RE estimates are identical on the year dummies
and the three time-constant variables. This is a general result: if the model includes only
aggregate time effects and individual-specific covariates that have no time variation, POLS 
RE (and, in particular, there is no efficiency gain in using RE).
When FE is used, of course the time-constant variables drop out. The estimates on the year
dummies are the same as POLS and RE. (Recall that the “constant” reported by FE is the
average of the estimated heterogeneity terms. When POLS and RE include time-constant
variables the FE “constant” does not equal the intercept from POLS/RE.)
. reg lwage d81-d87 educ black hisp
Source |
SS
df
MS
------------------------------------------Model | 179.091659
10 17.9091659
Residual | 1057.43798 4349 .243145087
------------------------------------------Total | 1236.52964 4359 .283672779

Number of obs
F( 10, 4349)
Prob  F
R-squared
Adj R-squared
Root MSE








4360
73.66
0.0000
0.1448
0.1429
.4931

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
.1193902
.029871
4.00
0.000
.0608279
.1779526
d82 |
.1781901
.029871
5.97
0.000
.1196277
.2367524
d83 |
.2257865
.029871
7.56
0.000
.1672241
.2843488
d84 |
.2968181
.029871
9.94
0.000
.2382557
.3553804
d85 |
.3459333
.029871
11.58
0.000
.287371
.4044957
d86 |
.4062418
.029871
13.60
0.000
.3476794
.4648041
d87 |
.4730023
.029871
15.83
0.000
.41444
.5315647
educ |
.0770943
.0043766
17.62
0.000
.0685139
.0856747
black | -.1225637
.0237021
-5.17
0.000
-.1690319
-.0760955
hisp |
.024623
.0213056
1.16
0.248
-.0171468
.0663928
_cons |
.4966384
.0566686
8.76
0.000
.3855391
.6077377
----------------------------------------------------------------------------. xtreg lwage d81-d87 educ black hisp, re
Random-effects GLS regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1625
between  0.1296
overall  0.1448




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(10)
Prob  chi2

203




4360
545
8.
819.51
0.0000

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
.1193902
.021487
5.56
0.000
.0772765
.1615039
d82 |
.1781901
.021487
8.29
0.000
.1360764
.2203038
d83 |
.2257865
.021487
10.51
0.000
.1836728
.2679001
d84 |
.2968181
.021487
13.81
0.000
.2547044
.3389318
d85 |
.3459333
.021487
16.10
0.000
.3038196
.388047
d86 |
.4062418
.021487
18.91
0.000
.3641281
.4483555
d87 |
.4730023
.021487
22.01
0.000
.4308886
.515116
educ |
.0770943
.009177
8.40
0.000
.0591076
.0950809
black | -.1225637
.0496994
-2.47
0.014
-.2199728
-.0251546
hisp |
.024623
.0446744
0.55
0.582
-.0629371
.1121831
_cons |
.4966384
.1122718
4.42
0.000
.2765897
.7166871
---------------------------------------------------------------------------sigma_u | .34337144
sigma_e | .35469771
rho | .48377912
(fraction of variance due to u_i)
----------------------------------------------------------------------------. xtreg lwage d81-d87 educ black hisp, fe
note: educ omitted because of collinearity
note: black omitted because of collinearity
note: hisp omitted because of collinearity
Fixed-effects (within) regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1625
between 
.
overall  0.0752

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(7,3808)
Prob  F

 0.0000




4360
545
8.
105.56
0.0000

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
.1193902
.021487
5.56
0.000
.0772631
.1615173
d82 |
.1781901
.021487
8.29
0.000
.136063
.2203172
d83 |
.2257865
.021487
10.51
0.000
.1836594
.2679135
d84 |
.2968181
.021487
13.81
0.000
.254691
.3389452
d85 |
.3459333
.021487
16.10
0.000
.3038063
.3880604
d86 |
.4062418
.021487
18.91
0.000
.3641147
.4483688
d87 |
.4730023
.021487
22.01
0.000
.4308753
.5151294
educ | (omitted)
black | (omitted)
hisp | (omitted)
_cons |
1.393477
.0151936
91.71
0.000
1.363689
1.423265
---------------------------------------------------------------------------sigma_u | .39074676
sigma_e | .35469771
rho | .54824631
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(544, 3808) 
8.45
Prob  F  0.0000

204

c. The reported standard errors for POLS and RE are not the same. The POLS standard
errors assume, in addition to homoskedasticity, no serial correlation in the composite error – in
other words, that there is no unobserved heterogeneity. At least the RE standard errors allow
for the standard RE structure, which means constant variance and correlations that are the
same across all pairs t, s. This may be too restrictive, but it is less restrictive than the usual
OLS standard errors.
d. The fully robust POLS standard errors – that allow any kind of serial correlation and
heteroskedasticity – are reported below. We prefer these to the usual RE standard errors
because, as noted in part c, the usual RE standard errors impose a special kind of serial
correlation. Notice that the fully robust POLS standard errors are not uniformly larger than the
usual RE standard errors.
. reg lwage d81-d87 educ black hisp, cluster(nr)
Linear regression

Number of obs
F( 10,
544)
Prob  F
R-squared
Root MSE







4360
49.41
0.0000
0.1448
.4931

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
.1193902
.0244086
4.89
0.000
.0714435
.1673369
d82 |
.1781901
.0241987
7.36
0.000
.1306558
.2257243
d83 |
.2257865
.0243796
9.26
0.000
.1778968
.2736761
d84 |
.2968181
.0271485
10.93
0.000
.2434894
.3501468
d85 |
.3459333
.0263181
13.14
0.000
.2942358
.3976309
d86 |
.4062418
.0273064
14.88
0.000
.3526029
.4598807
d87 |
.4730023
.025996
18.20
0.000
.4219374
.5240672
educ |
.0770943
.0090198
8.55
0.000
.0593763
.0948122
black | -.1225637
.0532662
-2.30
0.022
-.2271964
-.017931
hisp |
.024623
.0411235
0.60
0.550
-.0561573
.1054033
_cons |
.4966384
.1097474
4.53
0.000
.2810579
.7122189
-----------------------------------------------------------------------------

e. The fully robust standard errors for RE are given below. They are numerically identical

205

to the fully robust POLS standard errors. Because we really have only one estimator – remeber,
POLS  RE in this setup – there is one asymptotic variance. While there could be different
ways to estimate that asymptotic variance, in this case the estimators are the same, and that is
appealing because it means inference does not rely on the particular pre-programmed
command.
. xtreg lwage d81-d87 educ black hisp, re cluster(nr)
Random-effects GLS regression
Group variable: nr
R-sq:

Number of obs
Number of groups

within  0.1625
between  0.1296
overall  0.1448




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(10)
Prob  chi2




4360
545
8.
494.13
0.0000

(Std. Err. adjusted for 545 clusters in nr
----------------------------------------------------------------------------|
Robust
lwage |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------d81 |
.1193902
.0244086
4.89
0.000
.0715502
.1672302
d82 |
.1781901
.0241987
7.36
0.000
.1307616
.2256186
d83 |
.2257865
.0243796
9.26
0.000
.1780033
.2735696
d84 |
.2968181
.0271485
10.93
0.000
.2436081
.3500281
d85 |
.3459333
.0263181
13.14
0.000
.2943508
.3975159
d86 |
.4062418
.0273064
14.88
0.000
.3527222
.4597613
d87 |
.4730023
.025996
18.20
0.000
.422051
.5239536
educ |
.0770943
.0090198
8.55
0.000
.0594157
.0947728
black | -.1225637
.0532662
-2.30
0.021
-.2269636
-.0181638
hisp |
.024623
.0411235
0.60
0.549
-.0559775
.1052236
_cons |
.4966384
.1097474
4.53
0.000
.2815375
.7117392
---------------------------------------------------------------------------sigma_u | .34337144
sigma_e | .35469771
rho | .48377912
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

206

Solutions to Chapter 11 Problems
11.1. a. It is important to remember that, any time we put a variable in a regression model
(whether we are using cross section or panel data), we are controlling for the effects of that
variable on the dependent variable. The whole point of regression analysis is that it allows the
explanatory variables to be correlated while estimating ceteris paribus effect of each
explanatory variable. Thus, the inclusion of y i,t−1 in the equation allows prog it to be correlated
with y i,t−1 , and also recognizes that, due to inertia, y it is often strongly related to y i,t−1 .
An assumption that implies pooled OLS is consistent is
Eu it |z i , x it , y i,t−1 , prog it   0, all t,
which is implied by but is weaker than dynamic completeness. Without additional
assumptions, the pooled OLS standard errors and test statistics need to be adjusted for
heteroskedasticity and serial correlation (although the latter will not be present under dynamic
completeness).
When y i,t−1 is added to a regression model in an astructural way, we can think of the goal as
being to estimate
Ey it |z i , x it , y i,t−1 , prog it ,
which means that we are controlling for differences in the lagged response when gauging the
effect of the program. Of course, we might not have the conditional mean correctly specified;
we may be simply estimating a linear projection.
b. As we discussed in Section 7.8.2, this statement is incorrect. Provided our interest is in
Ey it |z i , x it , y i,t−1 , prog it , we are not especially concerned about serial correlation in the implied
errors,

207

u it ≡ y it − Ey it |z i , x it , y i,t−1 , prog it .
Nor does serial correlation cause inconsistency in the OLS estimators.
c. Such a model is the standard unobserved effects model:
y it   t  x it    1 prog it  c i  u it , t  1, 2, … , T,
where the  t are the time effects (that can be treated as parameters). We would probably
assume that x it , prog it  is strictly exogenous; the weakest form of strict exogeneity is that
x it , prog it  is uncorrelated with u is for all t and s. Then we could estimate the equation by
fixed effects or first differencing. If the u it are serially uncorrelated, FE is preferred. We could
also do a GLS analysis after the fixed effects or first-differencing transformations, but we
should have a large N.
d. A model that incorporates features from parts a and c is
y it   t  x it    1 prog it   1 y i,t−1  c i  u it , t  1, … , T.
Now, program participation can depend on unobserved city heterogeneity as well as on lagged
y it (we assume that y i0 is observed). Fixed effects and first-differencing are both inconsistent
and N   with fixed T.
Assuming that Eu it |x i , prog i , y i,t−1 , y i,t−2 , … , y i0   0, a consistent procedure is obtained by
first differencing, to get
y it  Δx it    1 Δprog it   1 Δy i,t−1  Δu it , t  2, … , T.
At time t and Δx it , Δprog it can be used as their own instruments, along with y i,t−j for j ≥ 2.
Either pooled 2SLS or a GMM procedure can be used. Past and future values of x it can also be
used as instruments because x it  is strictly exogenous.
11.2. a. OLS estimation on the first-differenced equation is inconsistent (for all parameters)
208

if CovΔw i , Δu i  ≠ 0. Because w it is correlated with u it , for all t we cannot assume that Δw i
and Δu i are uncorrelated.
b. Because u it is uncorrelated with z i1 , z i2 , for t  1, 2, Δu i  u i2 − u i1 is uncorrelated with
z i1 and z i2 , and so z i1 , z i2  are exogenous in the equation
Δy i  Δz i   Δw i  Δu i
The linear projection of Δw 1 on z i1 , z i2  can be written as
′

Δw i  z i1  1  z i2  2  r i , Ez it r i   0, t  1, 2.
The question is whether the rank condition holds. Rewrite this linear projection in terms of Δz i
and, say, z i1 as
Δw i  z i1  1 −  2   z i2 − z i1  2  r i  z i1  1  Δz i  2  r i ,
where  1 ≡  1 −  2 . If  1  0, that is  1   2 , then the reduced from of Δw i depends only on
Δz i . Because Δz i appears in the equation for Δy i , there are no instruments for Δw i . Thus, the
change in w it must depend on the level of z it , and not just on the change in z it .
c. With T ≥ 2 time periods we can write the differenced equation as
Δy it  Δz it   Δw it  Δu it , t  2, … , T.
Now, under the assumption that w is is uncorrelated with u it for s  t, we have natural
instruments for Δw it . At time t, Δu it depends on u it and u 1,t−1 . Thus, valid instruments at time t
in the FD equation are w i,t−2 , … , w i1 . We need T ≥ 3 for an IV procedure to work. With T  3
we have the cross sectional equation
Δy i3  Δz i3   Δw i3  Δu i3
and we can instrument for Δw i3 with w i1 (and possibly z ir from earlier time periods).

209

With T ≥ 4, we can implement an IV estimator by using the simple pooled IV estimator
described Section 11.4. Or, we can use the more efficient GMM procedure. Write the T − 1
time periods as
Δy i  ΔZ i   Δw i  Δu i ,
where each data vector or matrix has T − 1 rows. The matrix that includes all possible
instruments of observation i (with T − 1 rows) is
0

0

0

0

 0

0

0

0 z i w i1

0



0

0

 0

0

0

0 0

0

z i w i2 w i1

0

 0

0

0







 

  







0 0

0

0

0

z i w i,T−2  w il

zi 0

0

0

0

0

.

Putting in the levels of all z it for instruments in each time period is perhaps using too many
overidentifying restrictions. The dimension could be reduced substantially by using only
z it , z i,t−1  at period t rather than z i . Further, periods for t ≥ 3 one would use only w i,t−2 and
w i,t−3 as the IVs.
d. Generally, the IV estimator applied to the time-demeaned equation is inconsistent. This
is because w i,t−j is generally correlated with ü it , as the latter depends on the idiosyncratic errors
in all time periods.
11.3. Writing y it  x it  c i  u it − r it , the fixed effects estimator ̂ FE can be written as
N



N

−1

T

∑ ∑x it − x̄ i 
i1 t1

2

N

T

N −1 ∑ ∑x it − x̄ i u it − ū i − r it − r̄ i  .
i1 t1

Now x it − x̄ i  x ∗it − x̄ ∗i   r it − r̄ i . Then, because Er it |x ∗i , c i   0 for all t, x ∗it − x̄ ∗i  and
r it − r̄ i  are uncorrelated, and so

210

Varx it − x̄ i   Varx ∗it − x̄ ∗i   Varr it − r̄ i , all t.
Similarly, under (11.42), x it − x̄ i  and u it − ū i  are uncorrelated for all t. Now
Ex it − x̄ i r it − r̄ i  Ex ∗it − x̄ ∗i   r it − r̄ i r it − r̄ i   Varr it − r̄ i . By the law of large
numbers and the assumption of constant variances across t,
N

N

−1

T

T

∑ ∑x it − x̄ i   ∑ Varx it − x̄ i   TVarx ∗it − x̄ ∗i   Varr it − r̄ i 
p

i1 t1

t1

and
N

N

−1

T

∑ ∑x it − x̄ i u it − ū i − r it − r̄ i 

p

→ −T  Varr it − r̄ i .

i1 t1

Therefore,
plim̂ FE   − 

Varx ∗it

Varr it − r̄ i 
− x̄ ∗i   Varr it − r̄ i 

  1−

Varx ∗it

Varr it − r̄ i 
− x̄ ∗i   Varr it − r̄ i 

11.4. a. For each i we can average across t and rearrange to get
c i  ȳ i − x̄ i  − ū i .
Because Eū i   0,  c ≡ Ec i   Eȳ i − x̄ i . By the law of large numbers,
N

N

i1

i1

N −1 ∑ c i  N −1 ∑ȳ i − x̄ i  →  c .
p

Now replace  with ̂ FE and call the estimator ̂ c :
N

N

̂ c  N ∑ȳ i − x̄ i ̂ FE   N −1 ∑ȳ i − x̄ i  −
−1

i1

i1

N

N

−1

N

N

N

−1

∑ x̄ i
i1

∑ c i  O p 1  o p 1  N ∑ c i  o p 1 →  c ,
−1

i1

i1

211

p

̂ FE − 

.

N

where we use N −1 ∑ i1 x̄ i  O p 1 (by the law of large numbers – see Lemma 3.2) and
̂ FE −   o p 1.
b. There is more than one way to estimate  g , but a simple approach is to first difference,
giving
Δy it  g i  x it   Δu it , t  2, … , T.
Then we can estimate  by fixed effects on the first differences (using t  2, … , T, and then
apply the estimator from part a to the first differenced data. This means we just replace ȳ i with
Δy i and x̄ i with Δx i everywhere (and the time averages are based on T − 1, not T, time periods).
11.5. a. Ev i |z i , x i   Z i Ea i |z i , x i  −   Eu i |z i , x i   Z i  −   0  0. Next,
Varv i |z i , x i   Z i Vara i |z i , x i Z ′i  Varu i |z i , x i   Cova i , u i |z i , x i   Covu i , a i |z i , x i 
 Z i Vara i |z i , x i Z ′i  Varu i |z i , x i 
because a i and u i are uncorrelated, conditional on z i , x i , by Assumption FE.1 ′ and the usual
iterated expectations argument,
Varv i |z i , x i   Z i Z ′i   2u I T .
Therefore, under the assumptions given, which shows that the conditional variance depends on
z i . Unlike in the standard random effects model, there is conditional heteroskedasticity.
b. If we use the usual RE analysis, we are applying FGLS to the equation
y i  Z i   X i   v i , where v i  Z i a i −   u i . From part a, we know that Ev i |x i , z i   0,
and so the usual RE estimator is consistent (as N   for fixed T) and N -asymptotically
normal, provided the rank condition, Assumption RE.2, holds. (Remember, a feasible GLS
̂ will be consistent provided 
̂ converges in probability to a nonsingular
analysis with any 
̂ , or even that
matrix as N  . It need not be the case that Varv i |x i , z i   plim

212

̂ .
Varv i   plim
From part a, we know that Varv i |x i , z i  depends on z i unless we restrict almost all
elements of  to be zero (all but those corresponding to the constant in z it ). Therefore, the
usual random effects inference – that is, based on the usual RE variance matrix estimator – will
be invalid.
c. We can easily make the RE analysis fully robust to an arbitrary Varv i |x i , z i , as in
equation (7.52). Naturally, we expand the set of explanatory variables to z it , x it , and we
estimate  along with .
11.6. No. Assumption (11.42) maintains strict exogeneity of w ∗it  in (11.41), and strict
exogeneity clearly fails when w ∗it  y ∗i,t−1 .
11.7. When  t  /T for all t, we can rearrange (11.6) to get
y it  x it     x̄ i   r it , t  1, 2, … , T.
Let ̂ (along with ̂) denote the pooled OLS estimator from this equation. By standard results
on partitioned regression [for example, Davidson and MacKinnon (1993, Section 1.4)], ̂ can
be obtained by the following two-step procedure:
(i) Regress x it on x̄ i across all t and i, and save the 1  K vectors of residuals, say ĝ it ,
t  1, … , T; i  1, … , N.
(ii) Regress y it on ĝ it across all t and i. The OLS vector on ĝ it is ̂.
We want to show that ̂ is the FE estimator. Given that the FE estimator can be obtained by
pooled OLS of y it on x it − x̄ i , it suffices to show that ĝ it  x it − x̄ i for all t and i. But,
N

ĝ it  x it − x̄ i

−1

T

∑∑

x̄ ′i x̄ i

i1 t1

N

T

∑ ∑ x̄ ′i x̄ it
i1 t1

213

and
N

T

∑ ∑ x̄ ′i x̄ it 
i1 t1

N

T

i1

t1

∑ x̄ ′i ∑ x it 

N

∑ Tx̄ ′i x̄ it 
i1

N

T

∑ ∑ x̄ ′i x̄ i .
i1 t1

It follows that ĝ it  x it − x̄ i I K  x it − x̄ i . This completes the proof.
11.8. a. This is just a special case of Problem 8.8, where we now apply the results to the FD
equation and account for the loss of the first time period. The rank condition is
T

rank ∑ t2 Ez ′it Δx it 

 K.

b. Again, Problem 8.8 provides the answer. Letting e it  Δu it , t ≥ 2, two sufficient
conditions are Vare it |z it    2e , t  2, … , T and Ee it |z it , e i,t−1 , … , z i2 , e i2   0, t  2, … , T.
c. As in the case of pooled OLS after first differencing, this is only useful (and can only be
implemented) when T ≥ 3. First, estimate equation (11.100) by pooled 2SLS and obtain the
residuals, e it , t  2, … , T, i  1, … , N. Then, estimate the augmented equation,
Δy it  Δx it   ê i,t−1  error it , t  3, … , T
by pooled 2SLS, using IVs z it , ê i,t−1 . If we strengthen the condition from part b to
Ee it |z it , Δx i,t−1 , e i,t−1 , … , z i2 , Δx i2 , e i2   0
then, under H 0 , the usual t statistic on ê i,t−1 is distributed as asymptotically standard normal,
provided we add a dynamic homoskedasticity assumption. See Problem 8.10 for verification in
a general IV setting.
11.9. a. We can apply Problem 8.8.b because we are applying pooled 2SLS – this time to
the time-demeaned equation. Therefore, the rank condition is
T

rank

∑ Ez̈ it ẍ it 
′

t1

214

 K.

The rank condition clearly fails if x it contains any time-constant explanatory variables (across
T

all i, as usual). The condition rank ∑ t1 Ez̈ it z̈ it 
′

 L also should be assumed, and this rules

out time-constant instruments (and perfectly collinear instruments). If the rank condition holds,
T

we can always redefine z it so that ∑ t2 Ez̈ it z̈ it  has full rank.
′

b. We can apply the results on GMM estimation in Chapter 8. In particular, in equation
′
′̈
̈ i , W  EZ
̈ ′Z
̈ −1
̈ ′i X
(8.25), take C  EZ
i i  ,   EZ i ü i ü i Z i .

̈ ′i u i , where Q is the T  T
̈ ′i ü i  Q Z i  ′ Q u i   Z ′ Q u i  Z
A key point is that Z
T
T
T
i T
̈ i    2u I T
time-demeaning matrix defined in Chapter 10. Under Assumption FEIV.3, Eu i u ′i |Z
′

̈ Z
̈
(by the usual iterated expectations argument), and so    2u EZ
i i . If we plug these choices
of C, W, and ∧ into (8.29) and simplify, we obtain
̈ ′Z
̈
̈ ′ ̈ −1 ̈ ′ ̈ −1
Avar N ̂ −    2u EX
i i EZ i Z i  EZ i X i  .
c. The argument is very similar to the case of the fixed effects estimator. First, we already

T
showed in Chapter 10 that ∑ t1 Eü 2it   T − 1 2u . If ü it  ÿ it − ẍ it ̂ are the pooled 2SLS
2
N
residuals applied to the time-demeaned data, then NT − 1 −1 ∑ i1 ∑ t1 ü it is a consistent
estimator of  2u . Typically, NT − 1 would be replaced by NT − 1 − K as a degrees of
freedom adjustment.
d. From Problem 5.1 – which is purely algebraic, and so applies directly to pooled 2SLS,
even with lots of dummy variables – the 2SLS estimates, including ̂, can be obtained as
follows. First, run the regression x it on d1 i , … , dN i , z it across all t and i, and obtain the

residuals, say r̈ it . Second, obtain ĉ 1 , … , ĉ N , ̂ from pooled regression y it on d1 i , … , dN i , x it ,


r̈ it . Now, by algebra of partial regression, ̂ and the coefficient on r̈ it , say ̂, from this last
regression can be obtained by first partialling out the dummy variables, d1 i , … , dN i . As we
215

know from Chapter 10, this partialling out is equivalent to time demeaning all variables.

Therefore, ̂ and ̂ can be obtained form the pooled regression ÿ it on ẍ it , r̈ it , where we use the

fact that the time average of r̈ it for each i is identically zero.
Now consider the 2SLS estimator of  from
ÿ it  ẍ it   ü it

(11.102)

using IVs z̈ it . Again appealing to Problem 5.1, the pooled 2SLS estimator can be obtained from

regressing ẍ it on z̈ it and saving the residuals, say s̈ it , and then running the OLS regression ÿ it

on ẍ it , s̈ it . By partial regression and the fact that regressing on d1 i , … , dN i results in time


demeaning, s̈ it  r̈ it for all i and t. This proves that the 2SLS estimates of  from (11.102) and
y it  c 1 d1 i  c 2 d2 i . . . c N dN i  x it   u it
are identical.
e. By writing down the first order condition for the 2SLS estimates from (11.103) (with dn i
as their own instruments, and x̂ it as the IVs for x it ), it is easy to show that ĉ i  ȳ i − x̄ i ̂, where
̂ is the FE2SLS estimator. Therefore, the 2SLS residuals from (11.103) are computed as
y it − ĉ i − x it ̂  y it − ȳ i − x̄ i ̂ − x it ̂  y it − ȳ i  − x it − x̄ i ̂  ÿ it − ẍ it ̂,
which are exactly the 2SLS residuals from (11.102). Because the N dummy variables are
explicitly included in (11.103), the degrees of freedom in estimating  2u from part c are
properly calculated.
The general, messy estimator in equation (8.31) should be used, where X and Z are
̈ /N −1 , 
̈ i ̂, and
̈ and Z
̈ , respectively, Ŵ  Z
̈ ′Z
ü i  ÿ i −X
replaced with X
′ ̈
̂  N −1 ∑ N Z
̈ ′i 

ü
ü iZi.
i
i1

216

(11.103)

11.10. Let ã i , i  1, … , N, and ̃ be the OLS estimates from the pooled OLS regression
(11.101). By partial regression, ̃ can be obtained by first regressing y it on d1 i z it , d2 i z it , … ,
dN 1 z it and obtaining the residuals, ÿ it , and likewise for ẍ it . Then, we regress ÿ it on ẍ it ,
t  1, … , T; i  1, … , N. But regressing on d1 i z it , d2 i z it , … , dN i z it across all t and i is the
same as regressing on z it , t  1, … , T, for each cross section observation, i. Therefore, we can
write
ÿ it  y it − z it Z ′i Z i  −1 Z ′i y i 
ÿi  Miyi
where M i  I T − Z i Z ′i Z i  −1 Z ′i . A similar expression holds for ẍ it . We have shown that
regression (11.101) is identical to the pooled OLS regression ÿ it on ẍ it ,
t  1, … , T, i  1, … , N. The residuals from the two regressions are exactly the same by the
two-step projection result. The regression in (11.101) results in NT − NJ − K  NT − J − K
degrees of freedom, which is exactly what we need in (11.76).
11.11. Differencing twice and using the resulting cross section is easily done in most
statistical packages. Alternatively, Equivalently, use FE on the FD equation (which is the same
as FD on the FD equation).I can use fixed effects on the first differences
The Stata output follows. The estimates from the random growth model are pretty bad: the
estimates on the grant variables are of the “wrong” sign, and they are very imprecise.
The joint F test for the 53 different firm intercepts (when we treat the heterogeneity as
estimable parameters) is significant at the 5% level(p-value  .033), which does suggest a
random growth model is appropriate. (But remember, this statistic is only valid under
restrictive assumptions.) It is hard to know what to make of the poor estimates, but it does cast
doubt on the standard unobserved effects model without a random growth term.

217

. xtreg clscrap d89 cgrant cgrant_1, fe
Fixed-effects (within) regression
Group variable: fcode
R-sq:

Number of obs
Number of groups

within  0.0577
between  0.0476
overall  0.0050

corr(u_i, Xb)




108
54

Obs per group: min 
avg 
max 
F(3,51)
Prob  F

 -0.4011

2.




1.04
0.3826

----------------------------------------------------------------------------clscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------d89 | -.2377384
.1407362
-1.69
0.097
-.5202783
.0448014
cgrant |
.1564748
.2632934
0.59
0.555
-.3721088
.6850584
cgrant_1 |
.6099015
.6343411
0.96
0.341
-.6635913
1.883394
_cons | -.2240491
.114748
-1.95
0.056
-.4544153
.0063171
---------------------------------------------------------------------------sigma_u | .50956703
sigma_e | .49757778
rho | .51190251
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(53, 51) 
1.67
Prob  F  0.0334

11.12. a. Using only the changes from 1990 to 1993 and estimating the first-differenced
equation by OLS gives:
. reg cmrdrte cexec cunem if d93
Source |
SS
df
MS
------------------------------------------Model |
6.8879023
2 3.44395115
Residual | 55.8724857
48 1.16401012
------------------------------------------Total |
62.760388
50 1.25520776

Number of obs
F( 2,
48)
Prob  F
R-squared
Adj R-squared
Root MSE








51
2.96
0.0614
0.1097
0.0727
1.0789

----------------------------------------------------------------------------cmrdrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cexec | -.1038396
.0434139
-2.39
0.021
-.1911292
-.01655
cunem | -.0665914
.1586859
-0.42
0.677
-.3856509
.252468
_cons |
.4132665
.2093848
1.97
0.054
-.0077298
.8342628
-----------------------------------------------------------------------------

The coefficient on cexec means that one more execution reduces the murder rate by about
.10, and the effect is statistically significant.
b. If executions in the future respond to changes in the past murder rate, then exec may not
be strictly exogenous. If executions more than three years ago have a partial effect on the

218

murder rate, this would also violate strict exogeneity because, effectively, we do not have
enough lags. In principle, we could handle the latter problem by collecting more data and
including more lags.
If we assume that only exec it appears in the equation at time t, so that current and past
executions are uncorrelated with u it , then we can difference away c i and apply IV:
Δmrdrte it   0   1 Δexec it   2 Δunem it  Δu it .
A valid IV for Δexec it is Δexec i,t−1 because, by assumption, exec i,t−1 and exec i,t−2 are both
uncorrelated with u it and u i,t−1 . This results in a cross section IV estimation.
c. To test the rank condition, we regress Δexec it on 1, Δexec i,t−1 , Δunem it for 1993, and do a
t test on Δexec i,t−1 :
. reg cexec cexec_1 cunem if d93
Source |
SS
df
MS
------------------------------------------Model | 281.429488
2 140.714744
Residual | 336.217571
48 7.00453273
------------------------------------------Total | 617.647059
50 12.3529412

Number of obs
F( 2,
48)
Prob  F
R-squared
Adj R-squared
Root MSE








51
20.09
0.0000
0.4556
0.4330
2.6466

----------------------------------------------------------------------------cexec |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cexec_1 |
-1.08241
.1707822
-6.34
0.000
-1.42579
-.7390289
cunem |
.0400493
.3892505
0.10
0.918
-.7425912
.8226898
_cons |
.3139609
.5116532
0.61
0.542
-.7147868
1.342709
-----------------------------------------------------------------------------

Interestingly, there is a one-for-one negative relationship between the change in lagged
executions and the change in current executions. Certainly the rank condition passes.
The IV estimates are below:
. reg cmrdrte cexec cunem (cexec_1 cunem) if d93
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | 6.87925253
2 3.43962627

219

Number of obs 
F( 2,
48) 
Prob  F


51
1.31
0.2796

Residual | 55.8811355
48 1.16419032
------------------------------------------Total |
62.760388
50 1.25520776

R-squared

Adj R-squared 
Root MSE


0.1096
0.0725
1.079

----------------------------------------------------------------------------cmrdrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cexec | -.1000972
.0643241
-1.56
0.126
-.2294293
.029235
cunem | -.0667262
.1587074
-0.42
0.676
-.3858289
.2523764
_cons |
.410966
.2114237
1.94
0.058
-.0141298
.8360617
-----------------------------------------------------------------------------

The point estimate on Δexec is essentially the same as the OLS estimate, but, of course, the
IV standard error is larger. We can justify the POLS estimator on the FD equation (as the null
of exogeneity of Δexec would not be rejected).
d. The following Stata command gives the results without Texas:
. reg cmrdrte cexec cunem if (d931 & state! "TX")
Source |
SS
df
MS
------------------------------------------Model | .755191109
2 .377595555
Residual | 55.7000012
47 1.18510641
------------------------------------------Total | 56.4551923
49 1.15214678

Number of obs
F( 2,
47)
Prob  F
R-squared
Adj R-squared
Root MSE


50

0.32
 0.7287
 0.0134
 -0.0286
 1.0886

----------------------------------------------------------------------------cmrdrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cexec |
-.067471
.104913
-0.64
0.523
-.2785288
.1435868
cunem | -.0700316
.1603712
-0.44
0.664
-.3926569
.2525936
_cons |
.4125226
.2112827
1.95
0.057
-.0125233
.8375686
----------------------------------------------------------------------------Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | -1.65785462
2 -.828927308
Residual | 58.1130469
47 1.23644781
------------------------------------------Total | 56.4551923
49 1.15214678

Number of obs
F( 2,
47)
Prob  F
R-squared
Adj R-squared
Root MSE








50
0.11
0.8939
1.112

----------------------------------------------------------------------------cmrdrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cexec |
.082233
.804114
0.10
0.919
-1.535436
1.699902
cunem | -.0826635
.1770735
-0.47
0.643
-.4388895
.2735624
_cons |
.3939505
.2373797
1.66
0.104
-.0835958
.8714968
-----------------------------------------------------------------------------

The OLS estimate is smaller in magnitude and not statistically significant, while the IV
220

estimate actually changes sign (but, statistically, is not different from zero). Clearly, including
Texas in the estimation has a big impact. It is easy to see why this is the case by listing the
change in the murder rates and executions for Texas along with the averages for all states:
. list cmrdrte cexec if (d931 & state "TX")
-------------------
|
cmrdrte
cexec |
|-------------------|
132. | -2.200001
23 |
-------------------
. sum cmrdrte cexec if d931
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------cmrdrte |
51
.2862745
1.120361 -2.200001
3.099998
cexec |
51
.6470588
3.514675
-3
23

Texas has the largest drop in the murder rate from 1990 to 1993, and also the largest
increase in the number of executions. This does not necessarily mean Texas is an outlier, but it
clearly is an influential observation. And it is clear why including Texas makes for a fairly
strong deterrent effect.
11.13. a. The following Stata output estimates the reduced form for Δ logpris and tests
joint significance of final1 and final2, and also tests equality of the coefficients on final1 and
final2. The latter is actually not very interesting Technically, because we do not reject, we
could reduce our instrument to final1  final2, but we could always look ex post for restrictions
on the parameters in a reduced form.
. use prison
. xtset state year
panel variable:
time variable:
delta:

state (strongly balanced)
year, 80 to 93
1 unit

. reg gpris final1 final2 gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17
cag18_24 cag25_34 y81-y93
Source |
SS
df
MS
-------------------------------------------

221

Number of obs 
F( 24,
689) 

714
5.15

Model | .481041472
24 .020043395
Residual | 2.68006631
689 .003889791
------------------------------------------Total | 3.16110778
713 .004433531

Prob  F
R-squared
Adj R-squared
Root MSE






0.0000
0.1522
0.1226
.06237

----------------------------------------------------------------------------gpris |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------final1 |
-.077488
.0259556
-2.99
0.003
-.1284496
-.0265265
final2 | -.0529558
.0184078
-2.88
0.004
-.0890979
-.0168136
gpolpc | -.0286921
.0440058
-0.65
0.515
-.1150937
.0577094
gincpc |
.2095521
.1313169
1.60
0.111
-.0482772
.4673815
cunem |
.1616595
.3111688
0.52
0.604
-.4492935
.7726124
cblack | -.0044763
.0262118
-0.17
0.864
-.055941
.0469883
cmetro | -1.418389
.7860435
-1.80
0.072
-2.961717
.1249393
cag0_14 |
2.617307
1.582611
1.65
0.099
-.4900126
5.724627
cag15_17 | -1.608738
3.755564
-0.43
0.669
-8.982461
5.764986
cag18_24 |
.9533678
1.731188
0.55
0.582
-2.445669
4.352405
cag25_34 | -1.031684
1.763248
-0.59
0.559
-4.493667
2.4303
y81 |
.0124113
.013763
0.90
0.367
-.0146111
.0394337
y82 |
.0773503
.0156924
4.93
0.000
.0465396
.108161
y83 |
.0767785
.0153929
4.99
0.000
.0465559
.1070011
y84 |
.0289763
.0176504
1.64
0.101
-.0056787
.0636314
y85 |
.0279051
.0164176
1.70
0.090
-.0043295
.0601397
y86 |
.0541489
.0179305
3.02
0.003
.018944
.0893539
y87 |
.0312716
.0171317
1.83
0.068
-.002365
.0649082
y88 |
.019245
.0170725
1.13
0.260
-.0142754
.0527654
y89 |
.0184651
.0172867
1.07
0.286
-.0154759
.052406
y90 |
.0635926
.0165775
3.84
0.000
.0310442
.0961411
y91 |
.0263719
.0168913
1.56
0.119
-.0067927
.0595366
y92 |
.0190481
.0179372
1.06
0.289
-.0161701
.0542663
y93 |
.0134109
.0189757
0.71
0.480
-.0238461
.050668
_cons |
.0272013
.0170478
1.60
0.111
-.0062705
.0606731
----------------------------------------------------------------------------. test final1 final2
( 1)
( 2)

final1  0
final2  0
F(

2,
689) 
Prob  F 

8.56
0.0002

. test final1  final2
( 1)

final1 - final2  0
F(

1,
689) 
Prob  F 

0.60
0.4401

Jointly, final1 and final2 are pretty significant. Next, test for serial correlation in
a it ≡ Δv it :
. predict ahat, resid
. gen ahat_1  l.ahat

222

(51 missing values generated)
. reg ahat ahat_1
Source |
SS
df
MS
------------------------------------------Model | .051681199
1 .051681199
Residual | 2.38322468
661 .003605484
------------------------------------------Total | 2.43490588
662 .003678106

Number of obs
F( 1,
661)
Prob  F
R-squared
Adj R-squared
Root MSE








663
14.33
0.0002
0.0212
0.0197
.06005

----------------------------------------------------------------------------ahat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------ahat_1 |
.1426247
.0376713
3.79
0.000
.0686549
.2165945
_cons |
4.24e-11
.002332
0.00
1.000
-.004579
.004579
-----------------------------------------------------------------------------

There is strong evidence of positive serial correlation, although the estimated size of the
AR(1) coefficient, . 143, is not especially large. Still, a fully robust variance matrix should be
used for the joint significance test of final1 and final2. These two IVs are much more
significant when the robust variance matrix is used:
. qui reg gpris final1 final2 gpolpc gincpc cunem cblack cmetro cag0_14
cag15_17 cag18_24 cag25_34 y81-y93, cluster(state)
. test final1 final2
( 1)
( 2)

final1  0
final2  0
F(

2,
50) 
Prob  F 

18.82
0.000

b. First, we do pooled 2SLS to obtain the 2SLS residuals, ê it . Then we add the lagged
residual to the equation, and use it as its own IV:
. ivreg gcriv gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17 cag18_24
cag25_34 y81-y93 (gpris  final1 final2)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | -.696961613
23 -.030302679
Residual | 6.28846843
690 .009113722
------------------------------------------Total | 5.59150682
713 .007842226

Number of obs
F( 23,
690)
Prob  F
R-squared
Adj R-squared
Root MSE








714
6.08
0.0000
.09547

----------------------------------------------------------------------------gcriv |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval

223

---------------------------------------------------------------------------gpris | -1.031956
.3699628
-2.79
0.005
-1.758344
-.3055684
gpolpc |
.035315
.0674989
0.52
0.601
-.0972128
.1678428
gincpc |
.9101992
.2143266
4.25
0.000
.4893885
1.33101
cunem |
.5236958
.4785632
1.09
0.274
-.415919
1.46331
cblack | -.0158476
.0401044
-0.40
0.693
-.0945889
.0628937
cmetro |
-.591517
1.298252
-0.46
0.649
-3.140516
1.957482
cag0_14 |
3.379384
2.634893
1.28
0.200
-1.793985
8.552753
cag15_17 |
3.549945
5.766302
0.62
0.538
-7.771659
14.87155
cag18_24 |
3.358348
2.680839
1.25
0.211
-1.905233
8.621929
cag25_34 |
2.319993
2.706345
0.86
0.392
-2.993667
7.633652
y81 | -.0560732
.0217346
-2.58
0.010
-.0987471
-.0133992
y82 |
.0284616
.0384773
0.74
0.460
-.047085
.1040082
y83 |
.024703
.0373965
0.66
0.509
-.0487216
.0981276
y84 |
.0128703
.0293337
0.44
0.661
-.0447236
.0704643
y85 |
.0354026
.0275023
1.29
0.198
-.0185956
.0894008
y86 |
.0921857
.0343884
2.68
0.008
.0246672
.1597042
y87 |
.004771
.0290145
0.16
0.869
-.0521964
.0617383
y88 |
.0532706
.0273221
1.95
0.052
-.0003738
.106915
y89 |
.0430862
.0275204
1.57
0.118
-.0109476
.0971201
y90 |
.1442652
.0354625
4.07
0.000
.0746379
.2138925
y91 |
.0618481
.0276502
2.24
0.026
.0075595
.1161366
y92 |
.0266574
.0285333
0.93
0.350
-.0293651
.0826799
y93 |
.0222739
.0296099
0.75
0.452
-.0358624
.0804103
_cons |
.0148377
.0275197
0.54
0.590
-.0391948
.0688702
----------------------------------------------------------------------------Instrumented: gpris
Instruments:
gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17 cag18_24
cag25_34 y81 y82 y83 y84 y85 y86 y87 y88 y89 y90 y91 y92 y93
final1 final2
----------------------------------------------------------------------------. predict ehat, resid
. gen ehat_1  l.ehat
(51 missing values generated)
. ivreg gcriv gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17 cag18_24
cag25_34 y81-y93 ehat_1 (gpris  final1 final2)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | -.815873465
23 -.035472759
Residual | 5.90425699
639 .009239839
------------------------------------------Total | 5.08838353
662
.00768638

Number of obs
F( 23,
639)
Prob  F
R-squared
Adj R-squared
Root MSE








663
5.14
0.0000
.09612

----------------------------------------------------------------------------gcriv |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------gpris | -1.084446
.4071905
-2.66
0.008
-1.884039
-.2848525
gpolpc |
.0179121
.0719595
0.25
0.804
-.1233935
.1592176
gincpc |
.7492611
.2421405
3.09
0.002
.2737738
1.224748
cunem |
.1979701
.515973
0.38
0.701
-.8152375
1.211178
cblack | -.0102865
.0424589
-0.24
0.809
-.0936622
.0730893
cmetro | -.5272326
1.357715
-0.39
0.698
-3.193354
2.138889
cag0_14 |
3.284496
3.045539
1.08
0.281
-2.695979
9.26497

224

cag15_17 |
.066451
6.105497
0.01
0.991
-11.92281
12.05571
cag18_24 |
3.094998
2.830038
1.09
0.275
-2.462301
8.652297
cag25_34 |
2.716353
2.799581
0.97
0.332
-2.781137
8.213843
y81 | -.0782703
.0350721
-2.23
0.026
-.1471409
-.0093998
y82 |
.0090276
.0225246
0.40
0.689
-.0352036
.0532588
y83 | (dropped)
y84 | -.0113602
.0314408
-0.36
0.718
-.0731
.0503796
y85 |
.015744
.0309473
0.51
0.611
-.0450267
.0765148
y86 |
.0752485
.027649
2.72
0.007
.0209547
.1295424
y87 | -.0205808
.0282106
-0.73
0.466
-.0759774
.0348159
y88 |
.0265964
.0315542
0.84
0.400
-.0353661
.0885589
y89 |
.0182293
.0327158
0.56
0.578
-.0460142
.0824727
y90 |
.1275351
.0235386
5.42
0.000
.0813126
.1737575
y91 |
.0435859
.0315328
1.38
0.167
-.0183346
.1055064
y92 |
.0121958
.0354112
0.34
0.731
-.0573406
.0817321
y93 |
.0016107
.0365807
0.04
0.965
-.0702221
.0734435
ehat_1 |
.0763754
.0456451
1.67
0.095
-.0132571
.166008
_cons |
.0441747
.0477902
0.92
0.356
-.0496701
.1380195
----------------------------------------------------------------------------Instrumented: gpris
Instruments:
gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17 cag18_24
cag25_34 y81 y82 y83 y84 y85 y86 y87 y88 y89 y90 y91 y92 y93
ehat_1 final1 final2
-----------------------------------------------------------------------------

There is only marginal evidence of positive serial correlation, and it is practically small,
anyway ̂ . 076.
c. Adding a state effect to the change (FD) equation changes very little. In this example,
there seems to be little need for a random growth model. The estimated prison effect becomes
a little smaller in magnitude, −. 959. Here is the Stata output:
. xtivreg gcriv gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17 cag18_24
cag25_34 y81-y93 (gpris  final1 final2), fe
Fixed-effects (within) IV regression
Group variable: state
R-sq:

within 
.
between  0.0001
overall  0.1298

corr(u_i, Xb)




714
51

Obs per group: min 
avg 
max 

14
14.
14




179.24
0.0000

Number of obs
Number of groups

Wald chi2(23)
Prob  chi2

 -0.2529

----------------------------------------------------------------------------gcriv |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------gpris | -.9592287
.3950366
-2.43
0.015
-1.733486
-.1849713
gpolpc |
.04445
.0664696
0.67
0.504
-.0858281
.1747281
gincpc |
1.027161
.2157944
4.76
0.000
.6042122
1.450111
cunem |
.6560942
.4698359
1.40
0.163
-.2647672
1.576956
cblack |
.0706601
.1496426
0.47
0.637
-.2226339
.3639542

225

cmetro |
3.229287
4.683812
0.69
0.491
-5.950815
12.40939
cag0_14 |
1.14119
2.749679
0.42
0.678
-4.248082
6.530462
cag15_17 |
1.402606
6.330461
0.22
0.825
-11.00487
13.81008
cag18_24 |
1.169114
2.866042
0.41
0.683
-4.448225
6.786453
cag25_34 | -2.089449
3.383237
-0.62
0.537
-8.720471
4.541574
y81 | -.0590819
.0230252
-2.57
0.010
-.1042104
-.0139534
y82 |
.0033116
.0388056
0.09
0.932
-.0727459
.0793691
y83 |
.0080099
.0378644
0.21
0.832
-.066203
.0822228
y84 | -.0019285
.0293861
-0.07
0.948
-.0595243
.0556672
y85 |
.0220412
.0276807
0.80
0.426
-.032212
.0762945
y86 |
.075621
.0338898
2.23
0.026
.0091981
.1420438
y87 | -.0124835
.0294198
-0.42
0.671
-.0701453
.0451783
y88 |
.0329977
.0286125
1.15
0.249
-.0230817
.0890771
y89 |
.018718
.0292666
0.64
0.522
-.0386434
.0760794
y90 |
.1157811
.0354143
3.27
0.001
.0463703
.1851919
y91 |
.0378784
.0290414
1.30
0.192
-.0190417
.0947984
y92 | -.0006633
.0305014
-0.02
0.983
-.0604449
.0591184
y93 | -.0007561
.0317733
-0.02
0.981
-.0630306
.0615184
_cons |
.0014574
.0296182
0.05
0.961
-.0565932
.0595079
---------------------------------------------------------------------------sigma_u | .03039696
sigma_e |
.0924926
rho | .09747751
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(50,640) 
0.69
Prob  F
 0.9459
----------------------------------------------------------------------------Instrumented:
gpris
Instruments:
gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17 cag18_24
cag25_34 y81 y82 y83 y84 y85 y86 y87 y88 y89
y90 y91 y92 y93 final1 final2
-----------------------------------------------------------------------------

d. When we use the property crime rate, the estimated elasticity with respect to prison size
is substantially smaller, but still negative and marginally significant:
. ivreg gcrip gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17 cag18_24
cag25_34 y81-y93 (gpris  final1 final2)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | 1.07170564
23 .046595897
Residual |
1.5490539
690 .002245006
------------------------------------------Total | 2.62075954
713
.00367568

Number of obs
F( 23,
690)
Prob  F
R-squared
Adj R-squared
Root MSE








714
22.83
0.0000
0.4089
0.3892
.04738

----------------------------------------------------------------------------gcrip |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------gpris | -.3285567
.1836195
-1.79
0.074
-.6890768
.0319633
gpolpc |
.014567
.033501
0.43
0.664
-.051209
.0803431
gincpc |
.0560822
.1063744
0.53
0.598
-.1527741
.2649385
cunem |
.8583588
.2375199
3.61
0.000
.3920102
1.324707
cblack | -.0507462
.0199046
-2.55
0.011
-.089827
-.0116654
cmetro |
.0404892
.6443472
0.06
0.950
-1.224627
1.305606

226

cag0_14 |
1.890526
1.307747
1.45
0.149
-.6771151
4.458167
cag15_17 |
5.699448
2.861925
1.99
0.047
.0803221
11.31857
cag18_24 |
1.712283
1.330551
1.29
0.199
-.9001312
4.324698
cag25_34 |
2.027833
1.34321
1.51
0.132
-.6094366
4.665102
y81 | -.0771684
.0107873
-7.15
0.000
-.0983483
-.0559886
y82 | -.0980884
.019097
-5.14
0.000
-.1355836
-.0605932
y83 | -.1093989
.0185606
-5.89
0.000
-.1458409
-.0729569
y84 | -.0810119
.0145589
-5.56
0.000
-.1095968
-.0524269
y85 |
-.031369
.0136499
-2.30
0.022
-.0581693
-.0045687
y86 | -.0169451
.0170676
-0.99
0.321
-.0504558
.0165656
y87 | -.0310865
.0144005
-2.16
0.031
-.0593605
-.0028125
y88 | -.0437643
.0135605
-3.23
0.001
-.0703891
-.0171396
y89 | -.0359254
.0136589
-2.63
0.009
-.0627434
-.0091074
y90 | -.0298029
.0176007
-1.69
0.091
-.0643603
.0047544
y91 | -.0505269
.0137233
-3.68
0.000
-.0774713
-.0235824
y92 | -.1024579
.0141616
-7.23
0.000
-.1302629
-.0746529
y93 | -.0867254
.014696
-5.90
0.000
-.1155796
-.0578712
_cons |
.0857682
.0136586
6.28
0.000
.0589509
.1125856
----------------------------------------------------------------------------Instrumented: gpris
Instruments:
gpolpc gincpc cunem cblack cmetro cag0_14 cag15_17 cag18_24
cag25_34 y81 y82 y83 y84 y85 y86 y87 y88 y89 y90 y91 y92 y93
final1 final2
-----------------------------------------------------------------------------

The test for serial correlation yields a coefficient on ê i,t−1 of −. 024 t  −. 52, and so we
conclude that serial correlation is not an issue.
11.14. a. The fixed effects estimate of the first-difference equations are given below. We
have included year dummies without differencing them, since we are not interested in the time
effects in the original model:
. use ezunem
. xtset city year
panel variable:
time variable:
delta:

city (strongly balanced)
year, 1980 to 1988
1 unit

. gen cezt  d.ezt
(22 missing values generated)
. xtreg guclms cez cezt d82-d88, fe
Fixed-effects (within) regression
Group variable: city
R-sq:

Number of obs
Number of groups

within  0.6406
between  0.0094
overall  0.6205

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(9,145)
Prob  F

 -0.0546

227




176
22
8.
28.71
0.0000

----------------------------------------------------------------------------guclms |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cez |
.1937324
.3448663
0.56
0.575
-.4878818
.8753467
cezt | -.0783638
.0679161
-1.15
0.250
-.2125972
.0558697
d82 |
.7787595
.0675022
11.54
0.000
.6453442
.9121748
d83 | -.0331192
.0675022
-0.49
0.624
-.1665345
.1002961
d84 | -.0127177
.0713773
-0.18
0.859
-.153792
.1283566
d85 |
.3616479
.0762138
4.75
0.000
.2110144
.5122814
d86 |
.3277739
.0742264
4.42
0.000
.1810684
.4744793
d87 |
.089568
.0742264
1.21
0.230
-.0571375
.2362735
d88 |
.0185673
.0742264
0.25
0.803
-.1281381
.1652728
_cons | -.3216319
.0477312
-6.74
0.000
-.4159708
-.2272931
---------------------------------------------------------------------------sigma_u | .05880562
sigma_e | .22387933
rho | .06454083
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(21, 145) 
0.49
Prob  F  0.9712
. test cez cezt
( 1)
( 2)

cez  0
cezt  0
F(

2,
145) 
Prob  F 

3.22
0.0428

The coefficient ̂ 2  −. 078 gives the difference in annual growth rate due to EZ
designation. It is not significant at the usual 5% level. Note that this formulation does not give
the coefficient ̂ 1 a simple interpretation because zone designation happened either at t  5 (if
in 1984) or t  6 (if in 1985). A better formulation centers the linear trend at the time of
designation before constructing the interactions:
. egen nyrsez sum(ez), by(city)
. gen ezt0  0 if ~ez
(46 missing values generated)
. replace ezt0  ez*(t-5) if nyrsez  5
(30 real changes made)
. replace ezt0  ez*(t-6) if nyrsez  4
(16 real changes made)
. gen cezt0  ezt0 - ezt0[_n-1] if year  1980
(22 missing values generated)
. xtreg guclms cez cezt0 d82-d88, fe
Fixed-effects (within) regression

Number of obs

228



176

Group variable: city
R-sq:

Number of groups

within  0.6406
between  0.0025
overall  0.6185

corr(u_i, Xb)



22

Obs per group: min 
avg 
max 
F(9,145)
Prob  F

 -0.0630




8.
28.72
0.0000

----------------------------------------------------------------------------guclms |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cez | -.2341545
.0924022
-2.53
0.012
-.4167837
-.0515252
cezt0 |
-.082805
.0715745
-1.16
0.249
-.224269
.058659
d82 |
.7787595
.0675005
11.54
0.000
.6453475
.9121716
d83 | -.0331192
.0675005
-0.49
0.624
-.1665312
.1002928
d84 | -.0028809
.0720513
-0.04
0.968
-.1452874
.1395256
d85 |
.355169
.0740177
4.80
0.000
.208876
.5014621
d86 |
.3297926
.0749318
4.40
0.000
.181693
.4778922
d87 |
.0915867
.0749318
1.22
0.224
-.0565129
.2396864
d88 |
.0205861
.0749318
0.27
0.784
-.1275135
.1686857
_cons | -.3216319
.0477301
-6.74
0.000
-.4159685
-.2272953
---------------------------------------------------------------------------sigma_u | .06091433
sigma_e | .22387389
rho | .06893091
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(21, 145) 
0.50
Prob  F  0.9681

Now the coefficient on cez is the estimated effect of the EZ in the first year of designation,
and that gets added to −. 083  years since initial designation. This is easier to read.
b. Setting  1  0 gives a within R-squared of about .640, compared with that of the original
model in Example 11.4 of about .637. The difference is minor, and we would probably go with
the simpler, basic model in Example 11.4. With more years of data, the trend effect in part a
might become significant.
c. Because the general model contains c i  g i t, we cannot distinguish the effects of a
time-constant variable, w i , or its interaction with a linear time trend – at least if we stay in a
fixed effects framework. If we assume c i and g i are uncorrelated with ez it we could include w i
and w i t.
d. Yes. Provided e it : t  1, . . . , T has the kind of variation that it does in this data set,
w i ez it is linearly independent from other covariates included in the model. Therefore, we can

229

estimate . If we add h i ez it to the model, where h i is additional unobserved heterogeneity, then
 would not be identified (again, allowing h i to be correlated with ez it ).
11.15. a. We would have to assume that grant it is uncorrelated with the idiosyncratic
errors, u is , for all t and s. One way to think of this assumption is that while grant designation
may depend on firm heterogeneity c i , it is not related to idiosyncratic fluctuations in any time
period. Further, one must assume the grants have an effect on scrap rates only through their
effects on job training – the standard assumption for an instrument.
b. The following simple regression shows that Δhrsemp it and Δgrant it are highly positively
correlated, as expected:
. reg chrsemp cgrant if d88
Source |
SS
df
MS
------------------------------------------Model | 18117.5987
1 18117.5987
Residual | 28077.3319
123 228.270991
------------------------------------------Total | 46194.9306
124 372.539763

Number of obs
F( 1,
123)
Prob  F
R-squared
Adj R-squared
Root MSE








125
79.37
0.0000
0.3922
0.3873
15.109

----------------------------------------------------------------------------chrsemp |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cgrant |
27.87793
3.129216
8.91
0.000
21.68384
34.07202
_cons |
.5093234
1.558337
0.33
0.744
-2.57531
3.593956
-----------------------------------------------------------------------------

Unfortunately, this is on a bigger sample than we can use to estimate the scrap rate
equation, because the scrap rate is missing for so many firms. Restricted to that sample, we get:
. reg chrsemp cgrant if d88 & clscrap ~ .
Source |
SS
df
MS
------------------------------------------Model | 6316.65458
1 6316.65458
Residual | 12217.3517
43 284.124457
------------------------------------------Total | 18534.0062
44 421.227414

Number of obs
F( 1,
43)
Prob  F
R-squared
Adj R-squared
Root MSE








45
22.23
0.0000
0.3408
0.3255
16.856

----------------------------------------------------------------------------chrsemp |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------cgrant |
24.43691
5.182712
4.72
0.000
13.98498
34.88885
_cons |
1.580598
3.185483
0.50
0.622
-4.84354
8.004737

230

-----------------------------------------------------------------------------

So there is still a pretty strong relationship, but we will be using IV on a small sample
(N  45).
c. The IV estimate is:
. ivreg clscrap (chrsemp  cgrant) if d88
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | .274951237
1 .274951237
Residual | 17.0148885
43 .395695081
------------------------------------------Total | 17.2898397
44 .392950903

Number of obs
F( 1,
43)
Prob  F
R-squared
Adj R-squared
Root MSE


45

3.20
 0.0808
 0.0159
 -0.0070
 .62904

----------------------------------------------------------------------------clscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------chrsemp | -.0141532
.0079147
-1.79
0.081
-.0301148
.0018084
_cons | -.0326684
.1269512
-0.26
0.798
-.2886898
.223353
----------------------------------------------------------------------------Instrumented: chrsemp
Instruments:
cgrant
-----------------------------------------------------------------------------

The estimate says that 10 more hours training per employee would lower the average scrap
rate by about 14.2 percent, which is a large economic effect. It is marginally statistically
significant (assuming we can trust the asymptotic distribution theory for IV with 45
observations).
d. The OLS estimates is only about −. 0076 – about half of the IV estimate – with
t  −1. 68.
. reg clscrap chrsemp if d88
Source |
SS
df
MS
------------------------------------------Model | 1.07071245
1 1.07071245
Residual | 16.2191273
43 .377189007
------------------------------------------Total | 17.2898397
44 .392950903

Number of obs
F( 1,
43)
Prob  F
R-squared
Adj R-squared
Root MSE








45
2.84
0.0993
0.0619
0.0401
.61416

----------------------------------------------------------------------------clscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------chrsemp | -.0076007
.0045112
-1.68
0.099
-.0166984
.0014971

231

_cons | -.1035161
.103736
-1.00
0.324
-.3127197
.1056875
-----------------------------------------------------------------------------

e. Any effect pretty much disappears using two years of differences (even though you can
verify the rank condition easily holds):
. ivreg clscrap d89 (chrsemp  cgrant)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | .538688387
2 .269344194
Residual | 33.2077492
88 .377360787
------------------------------------------Total | 33.7464376
90 .374960418

Number of obs
F( 2,
88)
Prob  F
R-squared
Adj R-squared
Root MSE


91

0.90
 0.4087
 0.0160
 -0.0064

.6143

----------------------------------------------------------------------------clscrap |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------chrsemp | -.0028567
.0030577
-0.93
0.353
-.0089332
.0032198
d89 | -.1387379
.1296916
-1.07
0.288
-.3964728
.1189969
_cons | -.1548094
.0973592
-1.59
0.115
-.3482902
.0386715
----------------------------------------------------------------------------Instrumented: chrsemp
Instruments:
d89 cgrant
-----------------------------------------------------------------------------

11.16. a. Just use fixed effects, or first differencing. Of course w i gets eliminated by either
transformation.
b. Take the expectation of the structural equation conditional on w i , x i , r i  :
Ey it |w i , x i , r i   w i  x it   Ec i |w i , x i , r i   Eu it |w i , x i , r i 
 w i  x it    0   i r i  x̄ i  2 .
c. Provided a standard rank condition holds for the explanatory variables,  is identified
because it appears in a conditional expectation containing observable variables: Ey it |w i , x i , r i .
The pooled OLS estimation
y it on 1, w i , x it , r i , x̄ i , t  1, … , T; i  1, … , N
consistently estimates all parameters.
d. Following the hint, we can write

232

y it   0  w i  x it    i r i  x̄ i  2  a i  u it , t  1, … , T,

(11.104)

where a i  c i − Ec i |w i , x i , r i . Under the assumptions given, the composite error,
v it ≡ a i  u it , is easily shown to have variance-covariance matrix that has the random effect
form. In particular, Varv it |w i , x i , r i    2a   2u and Covv it , v is |w i , x i , r i    2a . [The
arguments for obtaining these expressions should be familiar. For example, since a i is a
function of c i , x i , and r i , we can replace c i with a i in all of the assumptions concerning the first
and second moments of u it : t  1, … , T. Therefore,
Ea i u it |w i , x i , a i , r i   a i Eu it |w i , x i , a i , r i   0
and so, by iterated expectations,
Cova i , u it |w i , x i , r i   Ea i u it |w i , x i , r i   0.]
We conclude that Varv i |w i , x i , r i   Varv i  has the random effects form, and so we should
just apply the usual random effects estimator to (11.104). This is asymptotically more efficient
than the pooled OLS estimator.
11.17. To obtain (11.81), we used (11.80) and the representation
N ̈′
N ̂ FE −   A −1 N −1/2 ∑ i1 X
i u i  o p 1. Simple algebra and standard properties of

O p 1.and o p 1.give
N

N ̂ −   N

−1/2

∑

Z ′i Z i  −1 Z ′i y i

N

− x i  −  −

i1

N

−1

∑Z i Z i  −1 Z i X i
′

′

N ̂ FE − 

i1

N

N

i1

i1

̈ i u i  o p 1
 N −1/2 ∑s i −  − CA −1 N −1/2 ∑ X
′

where C ≡EZ ′i Z i  −1 Z ′i X i  and s i ≡ Z ′i Z i  −1 Z ′i y i − x i . By definition, Es i   . By
combining terms in the sum we have

233

N

̈ i u i   o p 1,
N ̂ −   N −1/2 ∑s i −  − CA −1 X
′

i1

which implies by the central limit theorem and the asymptotic equivalence lemma that
N ̂ −  is asymptotically normal with zero mean and variance Er i r ′i , where
′

̈ i u i . If we replace , C, A, and  with their consistent estimators, we get
r i ≡ s i −  − CA −1 X

exactly (11.81) because the ü i are the T  1 FE residuals.
11.18. a. Using equation (8.47) we have
N

∑ X ′i ̂

̂ REIV 

−1

N

Zi

i1
N



∑
i1

̂ −1 Z i
X ′i 

∑ Z ′i ̂

−1
−1

Zi

i1
N

∑

N

∑ Z ′i ̂

−1
−1

Xi

i1

−1

̂ −1 Z i
Z ′i 

i1

N

∑ Z ′i ̂ −1 y i
i1

̂ has the RE form (and is probably estimated from the pooled 2SLS residuals).
where 
By arguments very similar to that for FGLS, we can show
N

N ̂ REIV −   A −1 C ′ D −1 N −1/2 ∑ Z ′i  −1 u i

 o p 1

i1

where
̂
  plim
C  EZ ′i  −1 X i 
D  EZ ′i  −1 Z i 
A  C ′ D −1 C
̂ is not generally consistent for Ev i v ′i . It follows
Note that this formulation recognizes that 
that
d
N ̂ REIV −  → Normal0, A −1 BA −1 

234

where
B  C ′ D −1 EZ ′i  −1 u i u ′i  −1 Z i D −1 C
b. Consistent estimators of A and B are
′ −1
̂ Ĉ
ÂĈD
N

̂ −1 û i û ′i 
̂ −1 Z i D
̂ −1 N −1 ∑ Z ′i 
̂ −1 Ĉ
̂  Ĉ′D
B
i1

where
N

ĈN

−1

∑ Z ′i ̂ −1 X i
i1
N

̂ −1 Z i
̂  N −1 ∑ Z ′i 
D
i1

û i  y i − X i ̂ REIV
11.19. a. Below is the Stata output. The concen variable is positive and statistically significant
using both RE and FE estimation of the reduced form, and using fully robust (that is, to any
serial correlation and heteroskedasticity) standard errors. The coefficient is somewhat larger
for RE compared with FE, and its standard error is somewhat smaller for RE. We conclude that
concen is suitably partially correlated with lfare in order to apply REIV and FEIV.
. xtreg lfare concen ldist ldistsq y98 y99 y00, re cluster(id)
Random-effects GLS regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.1348
between  0.4176
overall  0.4030




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(7)
Prob  chi2

4596
1149
4.

 386792.48

0.0000

(Std. Err. adjusted for 1149 clusters in id
----------------------------------------------------------------------------|
Robust
lfare |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval

235

---------------------------------------------------------------------------concen |
.2089935
.0422459
4.95
0.000
.126193
.2917939
ldist | -.8520921
.2720902
-3.13
0.002
-1.385379
-.3188051
ldistsq |
.0974604
.0201417
4.84
0.000
.0579833
.1369375
y98 |
.0224743
.0041461
5.42
0.000
.014348
.0306005
y99 |
.0366898
.0051318
7.15
0.000
.0266317
.046748
y00 |
.098212
.0055241
17.78
0.000
.0873849
.109039
_cons |
6.222005
.9144067
6.80
0.000
4.429801
8.014209
---------------------------------------------------------------------------sigma_u | .31933841
sigma_e | .10651186
rho | .89988885
(fraction of variance due to u_i)
----------------------------------------------------------------------------. xtreg lfare concen y98 y99 y00, fe cluster(id)
Fixed-effects (within) regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.1352
between  0.0576
overall  0.0083

corr(u_i, Xb)




4596
1149

Obs per group: min 
avg 
max 
F(4,1148)
Prob  F

 -0.2033




4.
120.06
0.0000

(Std. Err. adjusted for 1149 clusters in id
----------------------------------------------------------------------------|
Robust
lfare |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------concen |
.168859
.0494587
3.41
0.001
.0718194
.2658985
y98 |
.0228328
.004163
5.48
0.000
.0146649
.0310007
y99 |
.0363819
.0051275
7.10
0.000
.0263215
.0464422
y00 |
.0977717
.0055054
17.76
0.000
.0869698
.1085735
_cons |
4.953331
.0296765
166.91
0.000
4.895104
5.011557
---------------------------------------------------------------------------sigma_u | .43389176
sigma_e | .10651186
rho | .94316439
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

b. The REIV estimates without ldist and ldistsq from Stata are given below. For
comparison, the REIV estimates in Example 11.1 are also reported. Dropping the distance
variables changes the estimated elasticity to −. 654, which is notably larger in magnitude than
−. 508. This is a good example of how relevant time-constant variables – when they are
available – should be controlled for in an RE analysis.
. xtivreg lpassen y98 y99 y00 (lfareconcen), re
G2SLS random-effects IV regression
Group variable: id

Number of obs
Number of groups

236




4596
1149

R-sq:

within  0.4327
between  0.0487
overall  0.0578

corr(u_i, X)

Obs per group: min 
avg 
max 
Wald chi2(4)
Prob  chi2

 0 (assumed)




4.
219.33
0.0000

----------------------------------------------------------------------------lpassen |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lfare | -.6540984
.4019123
-1.63
0.104
-1.441832
.1336351
y98 |
.0342955
.011701
2.93
0.003
.011362
.057229
y99 |
.0847852
.0154938
5.47
0.000
.0544178
.1151525
y00 |
.146605
.0390819
3.75
0.000
.070006
.2232041
_cons |
9.28363
2.032528
4.57
0.000
5.299949
13.26731
---------------------------------------------------------------------------sigma_u | .91384976
sigma_e | .16964171
rho |
.9666879
(fraction of variance due to u_i)
----------------------------------------------------------------------------Instrumented:
lfare
Instruments:
y98 y99 y00 concen
----------------------------------------------------------------------------. xtivreg lpassen ldist ldistsq y98 y99 y00 (lfareconcen), re
G2SLS random-effects IV regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.4075
between  0.0542
overall  0.0641

corr(u_i, X)




Obs per group: min 
avg 
max 
Wald chi2(6)
Prob  chi2

 0 (assumed)




4596
1149
4.
231.10
0.0000

----------------------------------------------------------------------------lpassen |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lfare | -.5078762
.229698
-2.21
0.027
-.958076
-.0576763
ldist | -1.504806
.6933147
-2.17
0.030
-2.863678
-.1459338
ldistsq |
.1176013
.0546255
2.15
0.031
.0105373
.2246652
y98 |
.0307363
.0086054
3.57
0.000
.0138699
.0476027
y99 |
.0796548
.01038
7.67
0.000
.0593104
.0999992
y00 |
.1325795
.0229831
5.77
0.000
.0875335
.1776255
_cons |
13.29643
2.626949
5.06
0.000
8.147709
18.44516
---------------------------------------------------------------------------sigma_u | .94920686
sigma_e | .16964171
rho | .96904799
(fraction of variance due to u_i)
----------------------------------------------------------------------------Instrumented:
lfare
Instruments:
ldist ldistsq y98 y99 y00 concen
-----------------------------------------------------------------------------

c. Now we have three endogenous variables: lfare, ldist −  1   lfare, and

237

ldist 2 −  2   lfare. We can use
concen, ldist −  1   concen, and ldist 2 −  2   concen
as instruments. In other words, we add the interactions ldist −  1   concen and
ldist 2 −  2   concen as extra IVs to account for the endogenous interactions in the structural
model.
In practice, we replace  1 and  2 with the sample averages.
d. The Stata output below provides the estimates. Something interesting happens here. The
REIV and FEIV estimates of the coefficient on lfare now much closer to each other, and much
larger in magnitude then the estimates in Table 11.1. In particular, the estimated elasticity at
the mean of ldist and ldistsq is about −1 for REIV and FEIV. Interestingly, the REIV and FEIV
estimates with the interactions are close to the RE and FE estimates without the interactions.
. egen

mu_ldist  mean(ldist)

. gen dmldist  ldist-mu_ldist
. egen

mu_ldistsq  mean(ldistsq)

. gen dmldistsq  ldistsq-mu_ldistsq
. gen ldist_lfare  dmldist*lfare
. gen ldistsq_lfare  dmldistsq*lfare
. gen ldist_concen  dmldist*concen
. gen ldistsq_concen  dmldistsq*concen
. xtivreg lpassen ldist ldistsq y98 y99 y00 (lfare ldist_lfare ldistsq_lfare
 concen ldist_concen ldistsq_concen), re
G2SLS random-effects IV regression
Group variable: id
R-sq:

Number of obs
Number of groups

within  0.1319
between  0.0006
overall  0.0016

corr(u_i, X)




Obs per group: min 
avg 
max 
Wald chi2(8)
Prob  chi2

 0 (assumed)

238




4596
1149
4.
180.72
0.0000

----------------------------------------------------------------------------lpassen |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lfare | -1.048873
.3250545
-3.23
0.001
-1.685969
-.4117783
ldist_lfare |
29.63707
7.957828
3.72
0.000
14.04001
45.23413
ldistsq_lf~e | -2.330287
.638173
-3.65
0.000
-3.581083
-1.079491
ldist | -157.8477
42.76284
-3.69
0.000
-241.6613
-74.03409
ldistsq |
12.45005
3.437782
3.62
0.000
5.712121
19.18798
y98 |
.0319578
.0105546
3.03
0.002
.0112713
.0526444
y99 |
.080002
.0127579
6.27
0.000
.0549969
.1050071
y00 |
.1570325
.026578
5.91
0.000
.1049406
.2091244
_cons |
504.8691
131.4462
3.84
0.000
247.2392
762.499
---------------------------------------------------------------------------sigma_u | 1.3686882
sigma_e | .19436268
rho | .98023276
(fraction of variance due to u_i)
----------------------------------------------------------------------------Instrumented:
lfare ldist_lfare ldistsq_lfare
Instruments:
ldist ldistsq y98 y99 y00 concen ldist_concen ldistsq_concen
----------------------------------------------------------------------------. xtivreg lpassen y98 y99 y00 (lfare ldist_lfare ldistsq_lfare
 concen ldist_concen ldistsq_concen), fe
Fixed-effects (within) IV regression
Group variable: id
R-sq:

Number of obs
Number of groups

within 
.
between  0.0016
overall  0.0016

corr(u_i, Xb)




4596
1149

Obs per group: min 
avg 
max 
Wald chi2(6)
Prob  chi2

 -0.9913




4.
4.40e06
0.0000

----------------------------------------------------------------------------lpassen |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lfare | -1.011863
.3214187
-3.15
0.002
-1.641832
-.3818937
ldist_lfare |
24.11579
6.951145
3.47
0.001
10.4918
37.73979
ldistsq_lf~e | -1.905021
.5593222
-3.41
0.001
-3.001273
-.8087699
y98 |
.0322146
.0102786
3.13
0.002
.0120689
.0523603
y99 |
.080772
.0123315
6.55
0.000
.0566026
.1049414
y00 |
.155485
.0260008
5.98
0.000
.1045244
.2064456
_cons |
11.33584
1.694321
6.69
0.000
8.015032
14.65665
---------------------------------------------------------------------------sigma_u | 6.6845875
sigma_e | .19436268
rho | .99915529
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(1148,3441) 
72.37
Prob  F
 0.0000
----------------------------------------------------------------------------Instrumented:
lfare ldist_lfare ldistsq_lfare
Instruments:
ldist ldistsq y98 y99 y00 concen ldist_concen ldistsq_concen
-----------------------------------------------------------------------------

e. We can use the command xtivreg2, a user-written program for Stata. The 95%

239

confidence interval for  1 is −2. 408, . 385, which includes zero. The fully robust joint test of
the two interaction terms gives p-value  . 101, so we might be justified in dropping them. The
robust standard error
. xtivreg2 lpassen y98 y99 y00 (lfare ldist_lfare ldistsq_lfare
 concen ldist_concen ldistsq_concen), fe cluster(id)
FIXED EFFECTS ESTIMATION
-----------------------Number of groups 
1149

Obs per group: min 
avg 
max 

4.

IV (2SLS) estimation
-------------------Estimates efficient for homoskedasticity only
Statistics robust to heteroskedasticity and clustering on id
Number of clusters (id)  1149
Total (centered) SS
Total (uncentered) SS
Residual SS





128.0991685
128.0991685
129.9901441

Number of obs
F( 6, 1148)
Prob  F
Centered R2
Uncentered R2
Root MSE








4596
14.90
0.0000
-0.0148
-0.0148
.1942

----------------------------------------------------------------------------|
Robust
lpassen |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lfare | -1.011863
.7124916
-1.42
0.156
-2.408321
.384595
ldist_lfare |
24.11579
11.26762
2.14
0.032
2.03166
46.19993
ldistsq_lf~e | -1.905021
.8941964
-2.13
0.033
-3.657614
-.1524285
y98 |
.0322146
.0167737
1.92
0.055
-.0006613
.0650905
y99 |
.080772
.0261059
3.09
0.002
.0296053
.1319387
y00 |
.155485
.0625692
2.49
0.013
.0328515
.2781184
----------------------------------------------------------------------------Instrumented:
lfare ldist_lfare ldistsq_lfare
Included instruments: y98 y99 y00
Excluded instruments: concen ldist_concen ldistsq_concen
----------------------------------------------------------------------------. test ldist_lfare ldistsq_lfare
( 1)
( 2)

ldist_lfare  0
ldistsq_lfare  0
chi2( 2) 
Prob  chi2 

4.59
0.1008

f. In general, the estimated elasticities can be obtained from

240

lpassen
 ̂ 1  ̂ 1 ldist − ̂ 1   ̂ 2 ldist 2 − ̂ 2 
lfare
for any value of ldist. Calculations are given below. For dist  500 the estimated elasticity is
about . 047 with a very small t statistic. For dist  1, 500, the estimated elasticity is −1. 77 with
fully robust t  −1. 55. So the magnitude of the elasticity increases substantially as the route
distance increase, but the estimates contain substantial noise.
. sum ldist ldistsq if y00
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------ldist |
1149
6.696482
.6595331
4.553877
7.909857
ldistsq |
1149
45.27747
8.729749
20.73779
62.56583
. di log(500) -.4818739

6.696482

. di (log(500))^2 - 45.27747
-6.6561162
. lincom lfare - .4818739*ldist_lfare - 6.6561162*ldistsq_lfare
( 1)

lfare - .4818739*ldist_lfare - 6.656116*ldistsq_lfare  0

----------------------------------------------------------------------------lpassen |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------(1) |
.0474081
.7405447
0.06
0.949
-1.404033
1.498849
----------------------------------------------------------------------------. di log(1500) .61673839

6.696482

. di (log(1500))^2 - 45.27747
8.2057224
. lincom lfare .61673839*ldist_lfare 8.2057224*ldistsq_lfare
( 1)

lfare  .6167384*ldist_lfare  8.205722*ldistsq_lfare  0

----------------------------------------------------------------------------lpassen |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------(1) | -1.770802
1.142114
-1.55
0.121
-4.009305
.4677006
-----------------------------------------------------------------------------

241

Solutions to Chapter 12 Problems
12.1. a. Take the conditional expectation of equation (12.4) with respect to x, and use
Eu|x  0:
Ey − mx,  2 |x  Eu 2 |x  2mx,  o  − mx, Eu|x  Emx,  o  − mx,  2 |x
 Eu 2 |x  0  mx,  o  − mx,  2
 Eu 2 |x  mx,  o  − mx,  2 .
The first term does not depend on  and the second term is clearly minimized at    o for any
x. Therefore, the parameters of a correctly specified conditional mean function minimize the
squared error conditional on any value of x.
b. Part a shows that
Ey − mx,  o  2 |x ≤ Ey − mx,  2 |x, all  ∈ Θ, all x ∈ X.
If we take the expected value of both sides – with respect the the distribution of x, of course –
an apply iterated expectations, we conclude
Ey − mx,  o  2  ≤ Ey − mx,  2 , all  ∈ Θ.
In other words, if we know  o solves the population minimization problem conditional on any
x, then it also solves the unconditional population problem. Of course, conditional on a
particular value of x,  o would usually not be the unique solution. (For example, in the linear
case mx,   x, any  such as that x o −   0 sets mx,  o  − mx,  to zero.)
Uniqueness of  o as a population minimizer is realistic only after we integrate out x to obtain
Ey − mx,  2 .
12.2. a. Since u  y − Ey|x,
Vary|x  Varu|x  Eu 2 |x

242

because Eu|x  0. So Eu 2 |x  exp o  x o .
b. If we knew the u i  y i − mx i ,  o , then we could do a nonlinear regression of u 2i on
exp  x and just use the asymptotic theory for nonlinear regression. The NLS estimators of
 and  would then solve
N

min
,

∑u 2i − exp  x i  2 .
i1

The problem is that  o is unknown. When we replace  o with its NLS estimator, ̂ – that is we
replace u 2i with û 2i , the squared NLS residuals – we are solving the problem
N

min
,

∑y i − mx i , ̂ 2 − exp  x i  2 .
i1

This objective function has the form of a two-step M-estimator in Section 12.4. Since ̂ is
generally consistent for  o , the two-step M-estimator is generally consistent for  o and  o
(under weak regularity and identification conditions). In fact, N -consistency of ̂ and ̂
holds very generally.
c. We now estimate  o by solving
N

min


∑y i − mx i ,  2 / exp̂  x i ̂ ,
i1

where ̂ and ̂ are from part b. The general theory of WNLS under WNLS.1 to WNLS.3 can
be applied.
d. Using the definition of v, write u 2  exp o  x o v 2 . Taking logs gives
logu 2    o  x o  logv 2 . Now, if v is independent of x, so is logv 2 . Therefore,
Elogu 2 |x   o  x o Elogv 2 |x   o  x o   o , where  o ≡Elogv 2 . So, if we

243

could observe the u i , and OLS regression of logu 2i  on 1, x i would be consistent for
 o   o ,  o ; in fact, it would be unbiased. By two-step estimation theory, consistency still
holds if u i is replaced with û i , by essentially the same argument in part b. So, if mx,  is
linear in , we can carry out a weighted NLS procedure without ever doing nonlinear
estimation.
e. If we have misspecified the variance function – or, for example, we use the approach in
part d but v is not independent of x – then we should use a fully robust variance-covariance
matrix in equation (12.60) with ĥ i  exp̂  x i ̂.
12.3. a. The approximate elasticity is
∂ logÊy|z/∂ logz 1   ∂̂ 1  ̂ 2 logz 1   ̂ 3 z 2 /∂ logz 1   ̂ 2 .
b. This is approximated by 100  ∂ logÊy|z/∂z 2  100  ̂ 3 .
c. Since ∂Êy|z/∂z 2  exp̂ 1  ̂ 2 logz 1   ̂ 3 z 2  ̂ 4 z 22   ̂ 3  2̂ 4 z 2 , the estimated
turning point is ẑ ∗2  ̂ 3 /−2̂ 4 . This is a consistent estimator of z ∗2 ≡  3 /−2 4 .
d. Since ∇  mx,   expx 1  1  x 2  2 x, the gradient of the mean function evaluated under
the null is
̃ i  expx i1 ̃ 1 x i ≡ m
̃ ixi,
∇m
where ̃ 1 is the restricted NLS estimator. From regression (12.72), we can compute the usual
̃ i x i1 , m
̃ i x i2 , i  1, … , N, where ũ i  y i − m
̃ i.
LM statistic as NR 2u from the regression ũ i on m
̃ i x i2 on m
̃ i x i1 and obtain the 1  K 2 residuals, r̃ i . Then we
For the robust test, we first regress m
compute the statistic as in regression (12.75).
N

12.4. a. Write the objective function as 1/2 ∑ i1 y i − mx i ,  2 /hx i , ̂ . The objective
function, for any value of , is

244

qw i , ;   1/2y i − mx i ,  2 /hx i , .
Taking the gradient with respect to  gives
∇  qw i , ;   −∇  mx i , y i − mx i , /hx i , 
 −∇  mx i , u i /hx i , .
Taking the transpose gives us the score with respect to  for any  and any .
b. This follows because, under WNLS.1, u i ≡ u i  o  has a zero mean given x i :
Es i  o ; |x i   −∇  mx i ,  o  ′ Eu i |x i /hx i ,   0;
the value of  plays no role.
c. First, the Jacobian of s i  o ;  with respect to  is
∇  s i  o ;   ∇  mx i ,  o  ′ u i ∇  hx i , /hx i ,  2 . Everything but u i is a function only of x i , so
E∇  s i  o ; |x i   ∇  mx i ,  o  ′ Eu i |x i ∇  hx i , /hx i ,  2  0.
It follows by the LIE that the unconditional expectation is zero, too. In other words, we have
shown that the key condition (12.37) holds (and we did not rely on Assumption WNLS.3).
d. We would just use equation (12.60), which can be written as
Avar̂ 

N

∑ ∇  m ′i ∇  m i
i1

−1

N

∑ u 2i ∇  m ′i ∇  m i
i1

N

∑ ∇  m ′i ∇  m i

−1

,

i1

 i ≡ ∇m
̂ i /ĥ 1/2
where u i ≡ û i /ĥ 1/2
i and ∇  m
i are the standardized residuals and gradient,
respectively.
e. Under Assumption WNLS.3 (along with WNLS.1),
Vary i |x i   Eu 2i |x i    2o hx i ,  o ,
and ̂ is N -consistent for  o . This ensures that the asymptotic variance of N ̂ −  o  does

245

not depend on that of N ̂ −  o . Further,
A o  E∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o 
B o  Es i  o ;  o s i  o ;  o  ′   Eu 2i ∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o  2 .
By iterated expectations and WNLS.3,
Eu 2i ∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o  2   Eu 2i ∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o  2 
 EEu 2i ∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o  2 |x i 
 EEu 2i |x i ∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o  2 
 E 2o hx i ,  o ∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o  2 
  2o E∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o 
  2o A o .
Therefore,
Avar N ̂ −  o    2o A −1
o
an a consistent estimator is
N

̂2



′
N ∑ ∇  mx i , ̂ ∇  mx i , ̂/hx i , ̂ 

−1

−1

i1

Dividing this expression by N to get Avar̂ delivers (12.59).
12.5. a. We need the gradient of mx i ,  evaluated under the null hypothesis. By the chain
rule,
∇  mx,   gx   1 x 2   2 x 3   x  2 1 x 2 x  3 2 x 2 x,
∇  mx,   gx   1 x 2   2 x 3   x 2 , x 3 
The gradients with  1   2  0 are
∇  mx, , 0  gx  x
∇  mx, , 0  gx  x 2 , x 3 .

246

Let ̃ denote the NLS estimator with  1   2  0 imposed. Then ∇  mx i , ̃  gx i ̃x i
and ∇  mx i , ̃  gx i ̃x i ̃ , x i ̃ . Therefore, the usual LM statistic can be obtained as
2

3

2
NR 2u from the regression ũ i on g̃ i x i , g̃ i  x i ̃ , g̃ i  x i ̃ 3 , where g̃ i ≡ gx i ̃. If G is the

identity function, g ≡ 1, and the auxiliary regression is
2
ũ i on x i , x i ̃ , x i ̃ 3 ,

which is a versino of RESET.
b. The VAT version of the test is obtained as follows. As with the LM test, first estimate
the model under the null and obtain the NLS estimator, ̃, as before. Then estimate the
2
3
auxiliary model with x i ̃ and x i ̃ as explanatory variables. In other words, act as if the

mean function is
2
3
Gx i    1 x i ̃   2 x i ̃ 

and estimate  1 and  2 along with . A joint Wald test, made robust to heteroskedasticity if
necessary, of H 0 :  1  0,  2  0 is asymptotically equivalent (has the same asymptotic size
and asymptotic power against local alternatives) to the LM test. Given the way modern
software works, this often affords some computational simplification (albeit modest). When
G is the identify function, this variable addition approach gives the RESET test in its
traditional form.
One danger in using the VAT is that it is tempting to use the second-step estimates of  1 ,
 1 , and even  as generally valid estimators. But they are not. If the null is false, ̃ is
inconsistent for  (because ̃ is imposed with  1  0,  2  0) and so the added variables are
not correct under the alternative. The VAT should be used only for testing purposes (just like
the LM statistic).

247

12.6. a. The pooled NLS estimator of  o solves
N

min


T

∑ ∑y it − mx it ,  2 /2,
i1 t1

and so, to put this into the standard M-estimation framework, we can take the objective
T

function for a random draw i to be q i  ≡ qw i ,  ≡ ∑ t1 y it − mx it ,  2 /2. The score for
T

random draw i is s i   ∇  q i   − ∑ t1 ∇  mx it ,  ′ u it . Without further assumptions, a
consistent estimator of B o is
N

̂  N −1 ∑ s i ̂s ̂ ′
B
i
i1

where ̂ is the pooled NLS estimator. The Hessian for observation i, which can be computed as
the Jacobian of the score, can be written as
T

T

t1

t1

H i   ∇  s i   − ∑ ∇ 2 mx it , u it   ∑ ∇  mx it ,  ′ ∇  mx it , .
When we plug in  o and use the fact that Eu it |x it   0, all t  1, … , T, then
T

A o ≡ EH i  o   − ∑

T

E∇ 2 mx it ,  o u it 

t1

 ∑ E∇  mx it ,  o  ′ ∇  mx it ,  o 
t1

T



∑ E∇  mx it ,  o  ′ ∇  mx it ,  o 
t1

because E∇ 2 mx it ,  o u it   0, t  1, … , T by iterated expectations. By the usual law of large
numbers argument,
N

T

N

Â ≡ N ∑ ∑ ∇  mx it , ̂ ′ ∇  mx it , ̂ ≡ N −1 ∑ Â i
−1

i1 t1

i1

is a consistent estimator of A o . Then, we just use the usual sandwich formula in equation
248

(12.49).
T

b. As in the hint we show that B o   2o A o . First, write s i   ∑ t1 s it , where
s it  ≡ −∇  mx it , u it . Under dynamic completeness of the mean, these scores are serially
uncorrelated across t (when evaluated at  o , of course). The argument is very similar to the
linear regression case from Chapter 7.
Let r  t for concreteness. Then
Es it  o s ir  o  ′ |x it , x ir , u ir   Eu it |x it , x ir , u ir u ir ∇  mx it ,  o  ′ ∇  mx ir ,  o   0
because Eu it |x it , u i,t−1 , , x i,t−1 , … ,   0 and r  t. Now apply the LIE to conclude
T

Es it  o s ir  o  ′   0. So we have shown that B o  ∑ t1 Es it  o s it  o  . But for each t,
′

apply iterated expectations:
′

Es it  o s it  o    Eu 2it ∇  mx it ,  o  ′ ∇  mx it ,  o 
 EEu 2it |x it ∇  mx it ,  o  ′ ∇  mx it ,  o 
  2o E∇  mx it ,  o  ′ ∇  mx it ,  o 
where the last equality follows because Eu 2it |x it    2o . It follows that
T

Bo 

∑ Es it  o s it  o   
′

T

 2o

t1

∑ E∇  mx it ,  o  ′ ∇  mx it ,  o 

  2o A o .

t1

Next, the usual two-step estimation argument – see Lemma 12.1 – shows that
N

T

T

NT −1 ∑ ∑ û 2it → T −1 ∑ Eu 2it    2o as N  .
i1 t1

p

t1

The degrees of freedom correction – putting NT − P in place of NT – does not affect
consistency. The variance matrix obtained by ignoring the time dimension and assuming
homoskedasticity is simply

249

N

T

∑ ∑ ∇  mx it , ̂ ∇  mx it , ̂

−1

′

̂ 2

,

i1 t1

and we just showed that N times this matrix is a consistent estimator of Avar N ̂ −  o .
c. As we just saw in part b, B o   2o A o , which means by slightly extending the argument
before (12.69) we can use an extension of the LM statistic there. Namely,
N

∑ s̃ i

LM 

′

̃
 M
̃2

−1

i1

N

∑ s̃ i

.

i1

It is convenient to choose
̃ 
M

N

T

∑ ∑ ∇  mx it , ̃ ′ ∇  mx it , ̃,
i1 t1

′ ′
where ̃  ̃ , ̄ . So the LM statistic can be written as
N

T

∑ ∑ ũ it ∇  mx it , ̃

̃ −2



i1 t1

N

T

∑ ∑ ∇  mx it , ̃ ′ ∇  mx it , ̃
i1 t1

−1

N

T

∑ ∑ ∇  mx it , ̃ ′ ũ it
i1 t1

where we take
N

T

̃ 2  NT −1 ∑ ∑ ũ 2it .
i1 t1

(It is common not to use the degrees of freedom adjustment when estimating  2o under the
null.). Finally, the LM statistic can be written as

LM 

N
T
ũ it ∇  mx it , ̃
∑ i1
∑ t1

 NT 

N
T
∇  mx it , ̃ ′ ∇  mx it , ̃
∑ i1
∑ t1
N
T
NT −1 ∑ i1 ∑ t1 ũ 2it

N
T
ũ it ∇  mx it , ̃
∑ i1
∑ t1

−1

N
T
∇  mx it , ̃ ′ ∇  mx it , ̃
∑ i1
∑ t1
N
T
ũ 2it
∑ i1
∑ t1

 NTR 2u
250

N
T
∇  mx it , ̃ ′ ũ it
∑ i1
∑ t1

−1

N
T
∇  mx it , ̃ ′ ũ it
∑ i1
∑ t1

because the numerator is the explained sum of squares from the pooled OLS regression
ũ it on ∇  mx it , ̃, t  1, . . . , T; i  1, . . . , N
and the numerator is the (uncentered) total sum of squares.
12.7. a. For each i and g, define u ig ≡ y ig − mx ig ,  og , so that Eu ig |x i   0, g  1, … , G.
Further, let u i be the G  1 vector containing the u ig . Then Eu i u ′i |x i   Eu i u ′i    o . Let u i
be the vector of nonlinear least squares residuals for each observation i. That is, compute the
NLS estimates for each equation g and collect the residuals. Then, by standard arguments
(apply Lemma 12.1), a consistent estimator of  o is
N

̂ ≡ N −1 ∑ u i u ′i

i1

because each NLS estimator, ̂ g is consistent for  og as N  .
b. This part involves several steps, and I will sketch how each one goes. First, let  be the
vector of distinct elements of  – the nuisance parameters in the context of two-step
M-estimation. Then, the score for observation i is
sw i , ;   −∇  mx i ,  ′  −1 u i 
 −u i  ⊗ ∇  mx i ,  ′ vec −1 
where mx i ,  is the G  1 vector of conditional mean functions. With this expression we can
verify condition (12.37), even though the actual derivatives are complicated. It is clear that
∇  sw i , ;  is a linear combination of u i , where the linear combination is a function of x i
(and the parameter values). Therefore, because Eu i |x i   0, E∇  sw i ,  o ; |x i   0 for any ,
that is, any . Its unconditional expectation is zero, too, which verifies (12.37). This shows
that we do not have to adjust for the first-stage estimation of  o . (Note: This problem assumes

251

that Varu i |x i    o , but it is clear that (12.37) holds without any assumption about
̂ , to converge to its limit at the usual N rate.)
Varu i |x i . We just need the estimator we use, 
Next we obtain B o ≡Es i  o ;  o s i  o ;  o  ′ :
′ −1
Es i  o ;  o s i  o ;  o  ′   E∇  m i  o  ′  −1
o u i u i  o ∇  m i  o 
′ −1
 EE∇m i  o  ′  −1
o u i u i  o ∇  m i  o |x i 
−1
′
 E∇  m i  o  ′  −1
o Eu i u i |x i  o ∇  m i  o 
−1
 E∇  m i  o  ′  −1
o  o  o ∇  m i  o 

 E∇  m i  o  ′  −1
o ∇  m i  o .
Next, we have to derive A o ≡EH i  o ;  o , and show that B o  A o . The Hessian itself is
complicated, but its expected value is not. The Jacobian of s i ;  with respect to  can be
written
H i ;   ∇  mx i ,  ′  −1 ∇  mx i ,   I P ⊗ u i  ′ Fx i , ; ,
where Fx i , ;  is a GP  P matrix, where P is the total number of parameters, that involves
Jacobians of the rows  −1 ∇  m i  with respect to . The key is that Fx i , ;  depends on x i ,
not on y i . So,
′
EH i  o ;  o |x i   ∇  m i  o  ′  −1
o ∇  m i  o   I P ⊗ Eu i |x i  Fx i ,  o ;  o 

 ∇  m i  o  ′  −1
o ∇  m i  o .
Now iterated expectations gives A o E∇  m i  o  ′  −1
o ∇  m i  o . We have verified (12.37)
and also that A o  B o . Therefore, from Theorem 12.3,
′ −1
−1
Avar N ̂ −  o   A −1
o  E∇  m i  o   o ∇  m i  o  .

c. As usual, we replace expectations with sample averages and unknown parameters, and
divide the result by N to get Avar̂:

252

Avar̂ 

N

−1

̂ ∇  m i ̂
N −1 ∑ ∇  m i ̂ ′ 
−1

/N

i1

N



∑ ∇  m i ̂ ′ ̂

−1
−1

∇  m i ̂

.

i1

̂ can be based on the multivariate NLS residuals or can be updated after the
The estimate 
nonlinear SUR estimates have been obtained.
d. First, note that ∇  m i  o  is a block-diagonal matrix that has G rows, with blocks
∇  g m ig  og , a 1  P g vector. (I assume that there are no cross-equation restrictions imposed in
the nonlinear SUR estimation.) If  o is diagonal, so is its inverse. Standard matrix
multiplication shows that

∇  m i  o  ′  −1
o ∇  m i  o  

o′
o
 −2
o1 ∇  1 m i1 ∇  1 m i1

0



0

0

o′
o
 −2
o2 ∇  2 m i2 ∇  1 m i2











0

0



0

o′
o
 −2
oG ∇  G m iG ∇  G m iG

Taking expectations and inverting the result shows that
o
−1
Avar N ̂ g −  og    2og E∇  g m o′
ig ∇  g m ig  , g  1, … , G. (Note also that the nonlinear

SUR estimators are asymptotically uncorrelated across equations.) These asymptotic variances
are easily seen to be the same as those for nonlinear least squares on each equation.
e. I cannot see a nonlinear analog of Theorem 7.7. The first hint given in Problem 7.5 does
not extend readily to nonlinear models, even when the same regressors appear in each
equation. The key is that X i is replaced with ∇  mx i ,  o . While this G  P matrix has a
block-diagonal form, as described in part d, the blocks are not the same even when the same
regressors appear in each equation. In the linear case, ∇  g m g x i ,  og   x i for all g. But, unless

253

.

 og is the same in all equations – a very restrictive assumption – ∇  g m g x i ,  og  varies across g.
For example, if m g x i ,  og   expx i  og  then ∇  g m g x i ,  og   expx i  og x i , and the gradients
differ across g.
12.8. As stated in the hint, we can use (12.37) and a modfied version of (12.76),
N

−1/2

N

N

i1

i1

∑ s i ̃; ̂  N −1/2 ∑ s i  o ; ̂  A o N ̃ −  o   o p 1,

N
−1/2
s i ̃; ̂  o p 1; this is just standard algebra. Under
to show N ̃ − ̂  A −1
∑ i1
o N

(12.37),
N

−1/2

N

N

i1

i1

∑ s i ̃; ̂  N −1/2 ∑ s i ̃;  o   o p 1,

by a similar mean value expansion used for the unconstrained two-step M-estimator:
N

−1/2

N

N

i1

i1

∑ s i ̃; ̂  N −1/2 ∑ s i ̃;  o   E∇  s i  o ;  o  N ̂ −  o   o p 1,

and use E∇  s i  o ;  o   0. Now, the second-order Taylor expansion gives
N

N

i1

i1

∑ qwi , ̃; ̂ − ∑ qwi , ̂; ̂ 

N

∑ s i ̂; ̂  1/2̃ − ̂ ′
i1

′
 1/2̃ − ̂

N

∑ Ḧ i

N

∑ Ḧ i

̃ − ̂

i1

̃ − ̂.

i1

Therefore,
N

N

i1

i1

2 ∑ qw i , ̃; ̂ − ∑ qw i , ̂; ̂

  N ̃ − ̂ ′ A o  N ̃ − ̂  o p 1
N



N

−1/2

∑ s̃ i
i1

254

N

A −1
o

N

−1/2

∑ s̃ i
i1

 o p 1,

where s̃ i  s i ̃;  o . Again, this shows the asymptotic equivalence of the QLR and LM
statistics. To complete the problem, we should verify that the LM statistic is not affected by ̂
N
N
either, but that follows from N −1/2 ∑ i1 s i ̃; ̂  N −1/2 ∑ i1 s i ̃;  o   o p 1.

12.9. a. We cannot say anything in general about Medy|x because
Medy|x  mx,  o   Medu|x
and Medu|x could be a general function of x.
b. If u and x are independent, then Eu|x and Medu|x are both constants, say  and .
Then Ey|x − Medy|x  mx,  o    − mx,  o      − , which does not depend on
x.
c. When u and x are independent, the partial effects of x j on the conditional mean and
conditional median are the same, and there is no ambiguity about what is “the effect of x j on
y,” at least when only the mean and median are under consideration. In this case, we could
interpret large differences between LAD and NLS as perhaps indicating an outlier problem.
But it could be just that u and x are not independent, and so the function mx,  o  cannot be
both the mean and the median (or differ from each of these by a constant).
12.10. The conditional mean function is mx i , n i ,   n i px i , . So we would, as usual,
N

minimize the sum of squared residuals, ∑ i1 y i − n i px i ,  2 with respect to . This gives the
NLS estimator, say ̆. Define the weights as ĥ i ≡ n i px i , ̆1 − px i , ̆. Then the weighted
N

NLS estimator minimizes ∑ i1 y i − n i px i ,  2 /ĥ i .
12.11. a. The key to the derivation is to verify condition (12.37), which is similar to
Problem 12.7. In fact, this contains Problem 12.7 as a special case. In particular, write the
score (with respect to ) for observation i as

255

sw i , ;   −∇  mx i ,  ′ Wx i ,  −1 u i 
 −u i  ⊗ ∇  mx i ,  ′ vecW i  −1 .
The Jacobina of sw i , ;  with respect to  is generally complicated, but it is clear that
∇  sw i , ;  is a linear combination of u i , where the linear combination is a function of x i
(and the parameter values). Therefore, because Eu i |x i   0, E∇  sw i ,  o ; |x i   0 for any ,
which verifies (12.37). Notice that we do not need to assume Varu i |x i   Wx i ,  o  for some
o.
Without assuming (12.96) there are no simplications for
B o ≡ Es i  o ;  ∗ s i  o ;  ∗  ′ 
 E∇  m i  o  ′ W i  ∗  −1 u i u ′i W i  ∗  −1 ∇  m i  o 
where  ∗  plim̂. A consistent estimator of B o is
N

̂  N −1 ∑ ∇  m i ̂ ′ W i ̂ −1 û i û ′i W i ̂ −1 ∇  m i ̂
B
i1

where ̂ is the WMNLS estimator.
We also need to consistently estimate A o ≡EH i  o ;  o . Again, the argument is similar to
that in Problem 12.7, and uses that the mean function is correctly specified. We can write the
Hessian as
H i ;   ∇  m i  ′ W i  −1 ∇  m i   I P ⊗ u i  ′ Fx i , ; ,
where Fx i , ;  is a GP  P matrix, where P is the total number of parameters, that involves
Jacobians of the rows W i  −1 ∇  m i  with respect to . Therefore,
EH i  o ;  ∗ |x i   ∇  m i  o  ′ W i  ∗  −1 ∇  m i  o   I P ⊗ Eu i |x i  ′ Fx i ,  o ;  ∗ 
 ∇  m i  o  ′ W i  ∗  −1 ∇  m i  o ,

256

and so A o E∇  m i  o  ′ W i  ∗  −1 ∇  m i  o . A consistent estimator of A o is
N

Â  N −1 ∑ ∇  m i ̂ ′ W i ̂ −1 ∇  m i ̂.
i1

When we form
−1
̂ Â −1 /N,
Avar̂  Â B

simple algebra shows this expression is the same as (12.98).
b. If we assume (12.96) then
B o  EE∇  m i  o  ′ W i  o  −1 u i u ′i W i  o  −1 ∇  m i  o |x i 
 E∇  m i  o  ′ W i  o  −1 Eu i u ′i |x i W i  o  −1 ∇  m i  o |x i 
 E∇  m i  o  ′ W i  o  −1 W i  o W i  o  −1 ∇  m i  o 
 Ao
and so
Avar N ̂ −  o   A −1
o .
c. We can apply Problem 12.8 once we have properly chosen the objective function to
ensure B o  A o when (12.96) holds. That objective function, with nuisance parameters , is
qw i , ;   1/2y i − mx i ,  ′ W i  −1 y i − mx i , 
The division by two ensures
Es i  o ;  o s i  o ;  o  ′   EH i  o ;  o 
Now, letting ̃ and ̂ be the restricted and unrestricted estimators, respectively – where both
use ̂ as the nuisance parameter estimator – the QLR statistic is

257

N

N

∑ qwi , ̃; ̂ − ∑ qwi , ̂; ̂

QLR  2

i1



i1

N

N

i1

i1

∑ ũ ′i Ŵ −1i ũ i − ∑ û ′i Ŵ −1i û i

where Ŵ i ≡ W i ̂, ũ i ≡ y i − mx i , ̃, and û i ≡ y i − mx i , ̂. Under H 0 and standard
d

regularity conditions, QLR →  2Q , where Q is the number of restrictions.
And F-type statistic is obtained as
−1

F

−1

N
N
ũ ′i Ŵ i ũ i − ∑ i1 û ′i Ŵ i û i
∑ i1
−1

N
û ′i Ŵ i û i
∑ i1

Q

NG − P

which can be treated as an approximate F Q,NG−P random variable. Note that under (12.96)
Eu ′i W i  o  −1 u i   EEu ′i W i  o  −1 u i |x i 
 Etr EW i  o  −1 u i u ′i |x i   G
because Eu i u ′i |x i   W i  o . Therefore,
N

NG

−1

∑ û ′i Ŵ −1i û i → 1
p

i1

and using NG − P is a degrees-of-freedom adjustment.
12.12. a. We can appeal to equation (12.41) and the discussion that follows about the
scores for the two problems being uncorrelated. We have
s i ;   −∇  mx i , vw i , ,  ′ y i − mx i , vw i , , 
and we know, because Ey i |x i , w i   mx i , vw i ,  o ,  o ,
Es i  o ;  o |x i , w i   0.
As usual, this means any function of x i , w i  is uncorrelated with s i  o ;  o , including rw i ,  o .

258

It follows that
D o  B o  F o Erw i ,  o rw i ,  o  ′ F ′o
where
B o  Es i  o ;  o s i  o ;  o  ′ 
F o  E∇  s i  o ;  o .
the matrix F o Erw i ,  o rw i ,  o  ′ F ′o is at least p.s.d., and so D o − B o is p.s.d. The asymptotic
variance of the two-step estimator of  o (standardized by N  is
−1
A −1
o DoAo

and that of the estimator where  o is known is
−1
A −1
o BoAo .

b. To estimate A o under correct specification of the mean it is convenient to use
A o  E∇  mx i , vw i ,  o ,  o  ′ ∇  mx i , vw i ,  o ,  o 
and so
N

Â  N −1 ∑ ∇  mx i , vw i , ̂, ̂ ′ ∇  mx i , vw i , ̂, ̂
i1

Further,
N

̂  N −1 ∑ s i ̂; ̂s i ̂; ̂ ′ .
B
i1

It remains to consistently estimate F o . But by the product and chain rules,
∇  s i ;   −∇  ∇  mx i , vw i , ,  ′ y i − mx i , vw i , , 
 ∇  mx i , vw i , ,  ′ ∇ v mx i , vw i , , ∇  vw i , .
When we plug in  o ,  o  the first term has zero mean because the conditional mean is
259

correctly specified – much like the argument for the Hessian. Therefore,
F o  E∇  s i  o ;  o 
 E∇  mx i , vw i ,  o ,  o  ′ ∇ v mx i , vw i ,  o ,  o ∇  vw i ,  o 
and
N

F̂  N −1 ∑ ∇  mx i , vw i , ̂, ̂ ′ ∇ v mx i , vw i , ̂, ̂∇  vw i , ̂
i1

is consistent for F o . Finally, let
N

Ĉ  N −1 ∑ r i ̂r i ̂ ′ .
i1

Then
−1
̂  F̂ ĈF̂ ′ Â −1
Avar N ̂ −  o   Â B
−1
̂ Â −1 .
which, numerically, will always be larger (in the matrix sense) than Â B

12.13. a. Strict exogeneity is not needed because the population objective function is
T

1/2 ∑ Ey it − mx it ,  2 /hx it ,  ∗ ,
t1

and  o minimizes this function provided
Ey it |x it   mx it ,  o , t  1, . . . , T.
We do not need
Ey it |x i1 , x i2 , . . . , x iT   mx it ,  o .
The proof of the claim that  o is a minimizer of the population objective function could use
the score – assuming that mx t ,  is continuously differentiable and  o ∈ intΘ – and follow
Problem 12.4. But we can show directly that, for each t  1, . . . , T,
260

Ey it − mx it ,  o  2 /hx it ,  ∗  ≤ Ey it − mx it ,  2 /hx it ,  ∗ ,  ∈ Θ
and then inequality clearly holds when we sum over t. Identification requires that strict
inequality holds for  ≠  o when we sum across t.
To establish the above inequality, we follow Problem 12.1. Applied to a given t, we have
Ey it − mx it ,  o  2 |x it  ≤ Ey it − mx it ,  2 |x it 
for any x it . Because hx it ,  ∗   0 the inquality continues to hold if we divide each side by
hx it ,  ∗ . Further, because hx it ,  ∗  is a function of x it , we can bring it inside both conditional
expectations:
E

y it − mx it ,  o  2
x it
hx it ,  ∗ 

≤E

y it − mx it ,  2
x it
hx it ,  ∗ 

and then take the expected value with respect to x it on both sides.
b. For each t we can use the same argument for WNLS on a single cross section to show
E∇  s it  o ;   0
for any , where
s it ;   −∇  mx it ,  ′ u it /hx it , 
Because
T

s i ;  

∑ s it ; 
t1

it follows that condition (12.37) holds, so we can ignore the estimation of  ∗ in obtaining
Avar N ̂ −  o . But then we just need to estimate

261

B o  Es i  o ;  ∗ s i  o ;  ∗  ′ 
T

Ao 

∑ EH it  o ;  ∗  
t1

T

∑ Eu 2it ∇  mx it ,  o  ′ ∇  mx it ,  o /hx it ,  2 
t1

where u it  y it − mx it ,  o . Consistent estimators are
N

N

̂  N −1 ∑ s i ̂; ̂s i ̂; ̂ ′   N −1 ∑
B
i1
N

 N −1 ∑
i1

i1

T

∑ û it ∇  mx it , ̂ ′ /hx it , ̂
t1

T

∑ s it ̂; ̂
t1

T

∑ s it ̂; ̂

′

t1

T

∑ û it ∇  mx it , ̂/hx it , ̂
t1

and
N

T

Â  N −1 ∑ ∑ û 2it ∇  mx it , ̂ ′ ∇  mx it , ̂/hx it , ̂ 2 .
i1 t1

̂ includes terms involving û it û ir for t ≠ r, thereby allowing for serial correlation.
Notice how B
Further, terms involving û 2it /hx it , ̂ 2 mean we are not assuming the variance function is
correctly specified.
c. For any  we can write
s it  o ; s ir  o ;  ′  u it u ir ∇  mx it ,  o  ′ ∇  mx ir ,  o /hx it , hx ir , .
Take r  t. Then, by dynamic completeness – that is, Ey it |x it , y i,t−1 , x i,t−1 , . . . , y i1 , x i1   0 –
Eu it |u ir , x it , x ir   0,and so
Es it  o ; |u ir , x it , x ir   0.
Therefore,
Es it  o ; s ir  o ;  ′ |u ir , x it , x ir   0
and so Es it  o ; s ir  o ;  ′   0. It follows that we need not estimate the terms

262

Es it  o ; s ir  o ;  ′ , and so a consistent estimator of B o is
N

T

N −1 ∑ ∑ û 2it ∇  mx it , ̂ ′ ∇  mx it , ̂/hx it , ̂ 2 ,
i1 t1

and we need not change Â because the conditional mean for each t is assumed to be correctly
specified.
Remember that, because our analysis is for fixed T and N → , and we are using the usual
N -limiting distribution, there is nothing wrong with using the fully robust form even under
dynamic completeness. There is a sense that imposing zero correlation in the scores when they
are uncorrelated leads to better finite-sample inference, but that is difficult to establish in any
generality.
̂ we can use either of the estimators in parts b or c.
d. Again, we can keep Â the same. For B
But if we want to use both dynamic completeness and a correctly specified conditional
̂ even further.
variance, we can simplify B
T

Bo 

∑ Eu 2it ∇  mx it ,  o  ′ ∇  mx it ,  o /hx it ,  o  2 
t1
T



∑ EEu 2it ∇  mx it ,  o  ′ ∇  mx it ,  o /hx it ,  o  2 |x it 
t1
T



∑ E 2o hx it ,  o ∇  mx it ,  o  ′ ∇  mx it ,  o /hx it ,  o  2 
t1

T



 2o

∑ E∇  mx it ,  o  ′ ∇  mx it ,  o /hx it ,  o 
t1

So
Avar N ̂ −  o   ̂ 2 Â
where

263

−1

  2o A o .

N

T

̂ 2  NT − P −1 ∑ ∑ û 2it /hx it , ̂
i1 t1

is easily shown to be consistent for  2o : by iterated expectations,
Eu 2it /hx it ,  o    2o , t  1, . . . , T.
12.14. Write the score evaluated at  o as
s i  o   −x ′i 1y i − x i  o ≥ 0 − 1 − 1y i − x i  o  0
 −x ′i 1u i ≥ 0 − 1 − 1u i  0
where u i ≡ y i − x i  o . Therefore,
s i  o s i  o  ′  1u i ≥ 0 − 1 − 1u i  0 2 x ′i x i
  2 1u i ≥ 0  1 −  2 1u i  0x ′i x i
where this expression uses the hint that 1u i ≥ 0  1u i  0  0 and the square of an indicator
function is just itself.
Now take the expectation conditional on x i :
Es i  o s i  o  ′ |x i    2 E1u i ≥ 0|x i   1 −  2 E1u i  0|x i x ′i x i
  2 1 −   1 −  2 x ′i x i
 1 − x ′i x i ,
where we use the fact that E1u i  0|x i  Py i  x i  o |x i    – see the discussion below
equation (12.110). Now apply iterated expectations to get (12.115).
12.15. a. ̂ (approximately) solves the first order condition
N

T

∑ ∑ −x ′it 1y it − x it ̂ ≥ 0 − 1 − 1y it − x it ̂  0
i1 i1

so the score function for time period t is

264

≡ 0,

s it   −x ′it 1y it − x it  ≥ 0 − 1 − 1y it − x it   0
and the score for random draw i is
T

s i  

∑ s it .
i1

b. We have to show that the scores s it  o  : t  1, . . . , T in part a are serially
uncorrelated. Now
s it  o   −x ′it 1u it ≥ 0 − 1 − 1u it  0
and, under dynamic completeness of the quantile,
E1u it ≥ 0 − 1 − 1u it  0|x it , y i,t−1 , . . . , y i1 , x i1 
 E1u it ≥ 0 − 1 − 1u it  0|x it 
 1 −  − 1 −   0.
Therefore,
Es it  o |x it , y i,t−1 , . . . , y i1 , x i1   −x ′it E1u it ≥ 0 − 1 − 1u it  0|x it , y i,t−1 , . . . , y i1 , x i1   0,
and it follows that if r  t, s ir  o  is uncorrelated with s it  o . Therefore,
T

Bo 

T

∑ Es it  o s it  o    1 −  ∑ Ex ′it x it 
′

t1

t1

and a consistent estimator is
N

T

̂  1 − N −1 ∑ ∑ x ′it x it
B
i1 t1

c. This follows in a way similar to the to the cross section case. Now
T

a 

∑ Es it 
i1

265

and we need its Jacobian. We use Es it   EEs it |x it  for each t, and then, just as in
Section 12.10.2,
∇  Es it |x it   f u t x it  −  o |x it x ′it x it
Then
T

Ao 

∑ E∇  Es it  o |x it 
t1

T



∑ Ef u 0|x it x ′it x it .
t

t1

12.16. a. Because Medy 2 |z  z 2 , we would use LAD.
b. From
y1  z11  1y2  u1
we have
Medy 1 |y 2 , z  z 1  1   1 y 2  Medu 1 |y 2 , z
 z11  1y2  1v2
 z 1  1   1 y 2   1 y 2 − z 2 
We can use a control function approach but based on LAD. So, in the first stage, estimate  2
by LAD and compute, for each i,
v̂ i2  y i2 − z i ̂ 2 .
Then use LAD in the second stage. Using dummy arguments of optimization,
N

min

d 1 ,a 1 ,r 1

∑|y i1 − z i1 d 1 − a 1 y i2 − r 1 v̂ i2 |
i1

to get ̂ 1 , ̂ 1 , and ̂ 1 . These estimators are generally consistent by the two-step estimation
result discussed in Section 12.4.1.
c. It is natural to use the LAD t statistic for ̂ 1 from the control function procedure in part

266

b. We know from Chapter 6 that if we were using OLS in both stages then we could ignore the
first-stage estimation of  2 under H 0 :  1  0. That seems very likely the case here, too, but it
does not follow from the results presented in the text (which assume smooth objective
functions with nonsingular expected Hessians).
d. As mentioned in part c, an analytical calculation requires an extended set of tools, such
as those in Newey and McFadden (1994). A computationally intensive solution is to bootstrap
the two-step estimation method (being sure to recompute ̂ 2 with every bootstrap sample in
order to account for its sampling distribution).
12.17. a. We use a mean value expansion, similar to the delta method from Chapter 3 but
now allowing for the randomness of w i . By a mean value expansion, we can write
N

N

−1/2

N

∑ gwi , ̂  N −1/2 ∑ gwi ,  o  
i1

i1

N

N

−1

∑ G̈ i

N ̂ −  o ,

i1

̈ i is the M  P Jacobian of gw ,  evaluated at mean values between  o and ̂. Now,
where G
i
a
−1
̂
because N ̂ −  o  ~ Normal0, A −1
o B o A o , it follows that N  −  o   O p 1. Further, by
p
N ̈
Lemma 12.1, N −1 ∑ i1 G
i E∇  gw,  o  ≡ G o (the mean values all converge in probability

to  o ). Therefore,
N

N

−1

∑ G̈ i

N ̂ −  o   G o N ̂ −  o   o p 1,

i1

and so
N

−1/2

N

N

i1

i1

∑ gwi , ̂  N −1/2 ∑ gwi ,  o   G o N ̂ −  o   o p 1.

Because N ̂ −  o   −N −1/2 ∑ i1 A −1
o s i  o   o p 1, we can write
N

267

N

N

i1

i1

N ̂  N −1/2 ∑ gw i , ̂  N −1/2 ∑gw i ,  o  − G o A −1
o s i  o   o p 1
or, subtracting N  o from both sides,
N

N ̂ −  o   N −1/2 ∑gw i ,  o  −  o − G o A −1
o s i  o   o p 1.
i1

Now
−1
Egw i ,  o  −  o − G o A −1
o s i  o   Egw i ,  o  −  o − G o A o Es i  o 

  o −  o  0.
Therefore, by the CLT for i.i.d. sequences,
a
N ̂ −  o  ~ Normal0, D o 

where
D o  Varg i −  o − G o A −1
o s i ,
where hopefully the shorthand is clear. This differs from the usual delta method result because
the randomness in g i  g i  o  must be accounted for.
b. We assume we have Â consistent for A o . By the usual arguments,
N
Ĝ  N −1 ∑ i1 ∇  gw i , ̂ is consistent for G o . Then
N

̂  N −1 ∑ĝ − ̂ − ĜÂ −1 ŝ i ĝ − ̂ − ĜÂ −1 ŝ i  ′
D
i
i
i1

is consistent for D o , where the “^” denotes evaluation at ̂.
c. Using the shorthand notation, if Es i |x i   0 then g i is uncorrelated with s i because the
premise of the problem is that g i is a function of x i . Thefore, g i −  o  is uncorrelated with
G o A −1
o s i , which means

268

D o  Varg i −  o − G o A −1
o si
 Varg i −  o   VarG o A −1
o si
−1 ′
 Varg i   G o A −1
o BoAo Go
 Varg   G o Avar N ̂ −  o G ′o ,
i

which is what we wanted to show.

269

Solutions to Chapter 13 Problems
13.1. No. We know that  o solves
max Elog fy i |x i ; ,
Θ

where the expectation is over the joint distribution of x i , y i . Therefore, because exp is an
increasing function,  o also maximizes expElog fy i |x i ;  over Θ. The problem is that the
expectation and the exponential function cannot be interchanged:
Efy i |x i ;  ≠ expElog fy i |x i ; . In fact, Jensen’s inequality tells us that
Efy i |x i ;   expElog fy i |x i ; 
13.2. a. Because
fy|x i   2 2o  −1/2 exp−y − mx i ,  o  2 /2 2o ,
it follows that for observation i the log likelihood is
ℓ i ,  2   − 1 log2 − 1 log 2  − 1 2 y i − mx i ,  2 .
2
2
2
N

Only the last of these terms depends on . Further, for any  2  0, maximizing ∑ i1 ℓ i ,  2 
with respect to  is the same as minimizing
N

∑y i − mx i ,  2 ,
i1

which means the MLE ̂ is the NLS estimator.
b. First,
∇  ℓ i ,  2   ∇  mx i , y i − mx i , / 2 ;
note that ∇  mx i ,  is 1  P. Next,

270

∂ℓ i ,  2 
 − 1 2  1 4 y i − mx i ,  2 .
∂ 2
2
2
For notational simplicity, define the residual function u i  ≡ y i − mx i , . Then the score is

s i  

∇  m i  ′ u i / 2
− 21 2 

1
2 4

,

u i  2

where ∇  m i  ≡ ∇  mx i , .
Define the errors as u i ≡ u i  o , so that Eu i |x i   0 and Eu 2i |x i   Vary i |x i    2o .
Then, since ∇  m i  o  is a function of x i , it is easily seen that Es i  o |x i   0. Note that we
only use the fact that Ey i |x i   mx i ,  o  and Vary i |x i    2o in showing this. In other words,
only the first two conditional moments of y i need to be correctly specified; nothing else about
the normal distribution is used.
The equation used to obtain ̂ 2 is
N

∑
i1

− 1 2  1 4 y i − mx i , ̂ 2
2̂
2̂

 0,

where ̂ is the nonlinear least squares estimator. Solving gives
N

̂2

 N

−1

∑ û 2i ,
i1

where û i ≡ y i − mx i , ̂. Thus, the MLE of  2 is the sum of squared residuals divided by N. In
practice, N is often replaced with N − P as a degrees-of-freedom adjustment, but this makes no
difference as N  .
c. The derivations are a bit tedious but fairly straightforward:

271

H i  

−∇  m i  ′ ∇  m i / 2  ∇ 2 m i u i / 2 −∇  m i  ′ u i / 4
−∇  m i u i / 4

1
2 4

−

1
6

u i  2

,

where ∇ 2 m i  is the P  P Hessian of m i .
d. From part c and Eu i |x i   0, the off-diagonal blocks are zero. Further,
E∇  m i  o  ′ ∇  m i  o / 2o − ∇ 2 m i  o u i / 2o |x i   ∇  m i  o  ′ ∇  m i  o / 2o
Because , Eu 2i |x i    2o ,
E

1 u2 − 1 xi
i
 6o
2 4o

 1o − 1 4  1 4 .
2 o
2 o

Therefore,

− EH i  o |x i  

∇  m i  o  ′ ∇  m i  o / 2o

0

0

1
2 4o

(13.99)

where we again use Eu i |x i   0 and Eu 2i |x i    2o .
e. To show that −EH i  o |x i  equals Es i  o s i  o  ′ |x i , we need to know that, with u i
defined as above, Eu 3i |x i   0, which can be used, along with the zero mean and constant
conditional variance, to show

′

Es i  o s i  o  |x i  

∇  m i  o  ′ ∇  m i  o / 2o
0

0
E

− 21 2 
o

1
2 4o

u 2i

Further, Eu 4i |x i   3 4o , and so
E

− 1 2  1 4 u 2i
2 o
2 o

2



4
2
1  3 o − 2 o  1 .
4 8o
4 6o
4 4o
2 4o

Thus, we have shown −EH i  o |x i  Es i  o s i  o  ′ |x i .

272

2

.

f. From general MLE, we know that Avar N ̂ −  o  is the P  P upper left hand block of
EA i  o  −1 , where A i  o  is the matrix in (13.99). Because this matrix is block diagonal, it
is easily seen that
Avar N ̂ −  o    2o E∇  m i  o  ′ ∇  m i  o  −1 ,
and this is consistently estimated by
−1

N

̂2



N

−1

∑

̂ ′i ∇  m
̂i
∇m

,

(13.100)

i1

which means that Avar̂ is (13.100) divided by N, or
Avar̂  ̂ 2

−1

N

∑

̂ ′i ∇  m
̂i
∇m

.

i1

̂ i  x i , and we obtain exactly the asymptotic variance estimator for
If the model is linear, ∇  m
the OLS estimator under homoskedasticity.
13.3. a. The conditional log-likelihood for observation i is
ℓ i   y i logGx i ,   1 − y i  log1 − Gx i , .
b. The derivation for the probit case in Example 13.1 extends immediately:
s i   y i ∇  Gx i ,  ′ /Gx i ,  − 1 − y i ∇  Gx i ,  ′ /1 − Gx i , 
 ∇  Gx i ,  ′ y i − Gx i , /Gx i , 1 − Gx i , .
If we plug in  o for  and take the expectation conditional on x i we get Es i  o |x i   0
because Ey i − Gx i ,  o |x i   0, and the functions multiplying y i − Gx i ,  o  depend only on
xi.
c. We need to evaluate the score and the expected Hessian with respect to the full set of
parameters, but then evaluate these at the restricted estimates. Now,
273

(13.101)

∇  Gx i , , 0  xx, x 2 , x 3 ,
a 1  K  2 vector. Let ̃ denote the probit estimates of , obtained under the null. The score
for observation i, evaluated under the null estimates, is the K  2  1 vector
s i ̃  ∇  Gx i , ̃, 0 ′ y i − x i ̃/x i ̃1 − x i ̃
 x i ̃z̃ ′i y i − x i ̃/x i ̃1 − x i ̃,
where z̃ i ≡ x i , x i ̃ 2 , x i ̃ 3 . The negative of the expected Hessian, evaluated under the null,
is the K  2  K  2 matrix
Ax i , ̃  x i ̃ 2 z̃ ′i z̃ i /x i ̃1 − x i ̃.
These can be plugged into the second expression in equation (13.36) to obtain a nonnegative,
well-behaved LM statistic. Simple algebra shows that the statistic can be computed as N times
the explained sum of squares from the regression
ũi
̃ i 1 − 
̃ i


on

̃ i  x i
̃ i 1 − 
̃ i


,

̃ i  x i ̃ 2
,
̃ i 1 − 
̃ i


̃ i  x i ̃ 3
, i  1, . . . , N
̃ i 1 − 
̃ i


̃ i . Under H o , LM is distributed
where “~” denotes evaluation at ̃, 0 and ũ i  y i − 
asymptotically as  22 .
d. The variable addition version of the test is to estimate, in a second step, a probit model
with response probability of the form
x i    1 x i ̃ 2   2 x i ̃ 3 
and the compute a Wald test of H 0 :  1   2  0. As we discussed in Problem 12.5 in a
related context, this is to be used only as a test. The estimates of  1 and  2 obtained by
inserting ̃ into the square and quadratic are generally inconsistent if at least one of  1 and  2
is different from zero.

274

13.4. If the density of y given x is correctly specified then Esw,  o |x  0. But then
Eax,  o sw,  o |x  ax,  o Esw,  o |x  0
which, of course implies an unconditional expectation of zero. The only restriction on ax,  o 
would be to ensure the expected value is well defined (but this is usually just assumed, not
verified).
g

13.5. a. Because s i  o   G o  ′  −1 s i  o ,
g

g

Es i  o s i  o  ′ |x i   EG o  ′  −1 s i  o s i  o  ′ G o  −1 |x i 
 G o  −1 Es i  o s i  o  ′ |x i G o  −1
 G o  ′  −1 A i  o G o  −1 .
where the last equality follows from the conditional information matrix equality.
b. In part a, we just replace  o with ̃ and  o with ̃ :
g
̃ ′−1 Ã i G
̃ −1 .
Ã i  G̃ ′  −1 A i ̃G̃ −1 ≡ G

c. The expected Hessian form of the statistic is given in the second part of equation (13.36),
g

g

but where it depends on s̃ i and Ã i :
N

LM g 

∑

′
g
s̃ i

i1

∑ G̃ ′−1 s̃ i

∑
′

N

i1
N



∑ s̃ i



∑ s̃ i
i1

N

∑ s̃ gi
i1

∑ G̃ ′−1 Ã i G̃ −1

′

̃ −1 G
̃
G

N

∑ Ãi

N

∑ Ãi

−1

i1

N

∑ G̃ ′−1 s̃ i
i1

−1

̃ ′G
̃ −1
G

i1

′

−1

i1

i1
N

−1
g
Ãi

i1

N



N

N

∑ s̃ i
i1

N

∑ s̃ i

 LM.

i1

13.6. a. No, for two reasons. First, just specifying a distribution of y it given x it says

275

nothing, in general, about the distribution of y it given x i ≡ x il , … , x iT . We could assume
these two are the same, which is the strict exogeneity assumption. But, even under strict
exogeneity, we would have to specify something about joint distributions (perhaps via
conditional distributions) involving different time periods. We could assume independence
(conditional on x i ) or make a dynamic completeness assumption. Either way, without
substantially more assumptions, we cannot derive the distribution of y i given x i .
b. This is given in a more general case in equation (18.69) in Chapter 18. It can be derived
easily from Example 13.2, which gives ℓ i  for the cross section case:
T

T

t1

t1

∑y it x it  − expx it  ≡ ∑ ℓ it .

ℓ i  

Taking the gradient and transposing gives
T

s i  

∑ x ′it y it − expx it  ≡
t1

T

∑ s i .
t1

c. First, we need the Hessian for each i, which is easily obtained as ∇  s i  :
T

H i   − ∑ expx it x ′it x it ,
t1

which, in this example, does not depend on the y it (see Problem 13.12 for the notion of a
canonical link function). In particular, A it  o   −EH it  o |x it   −H it  o . Therefore,
N

ÂN

−1

T

∑ ∑ expx it ̂x ′it x it ,
i1 t1

where ̂ is the partial MLE. Further,

276

N

̂  N −1 ∑ s i ̂s i ̂ ′ ,
B
i1

and then Avar̂ is estimated as
N

T

∑∑

−1

expx it ̂x ′it x it

i1 t1

N

∑ s i ̂s i ̂

N

T

∑∑

′

i1

−1

expx it ̂x ′it x it

i1 t1

d. If Ey it |x it , y i,t−1, x i,t−1 , . . . , y i1 , x i1   Ey it |x it  then
Es it  o |x it , y it,t−1 , x i,t−1 , …   x ′it Ey it |x it , y it,t−1 , x i,t−1 , …  − expx it  o   0.
As usual, this finding implies that s it  o  and s ir  o  are uncorrelated, t ≠ r. Therefore,
T

Bo 

∑E

T

s it  o s it  o 

′

t1



∑ Eu 2it x ′it x it ,
t1

where u it ≡ y it − Ey it |x it   y it − expx it  o . Now, by the Poisson assumption,
Eu 2it |x it   Vary it |x it   expx it  o . By iterated expectations,
T

Bo 

∑ Eexpx it  o x ′it x it   A o .
t1

(We have really just verified the conditional information matrix equality for each t in the
special case of Poisson regression with an exponential mean function.) herefore, we can
estimate Avar̂ as
N

T

∑ ∑ expx it ̂x ′it x it

−1

,

i1 t1

which is exactly what we get by using pooled Poisson estimation and ignoring the time
dimension.
13.7. a. The joint density is simply gy 1 |y 2 , x;  o   hy 2 |x;  o . The log-likelihood for

277

observation i is
ℓ i  ≡ loggy i1 |y i2 , x i ;  o   loghy i2 |x i ;  o ,
and we would use this in a standard MLE analysis (conditional on x i ).
b. First, we know that, for all y i2 , x i ,  o minimizes Eℓ i1 |y i2 , x i . Because r i2 is a
function of y i2 , x i ,
Er i2 ℓ i1 |y i2 , x i   r i2 Eℓ i1 |y i2 , x i ;
because r 12 ≥ 0,  o maximizes Er i2 ℓ i1 |y i2 , x i  for all y i2 , x i , and therefore  o maximizes
Er i2 ℓ i1  by iterated expectations. Similarly,  o maximizes Eℓ i1 , and so it follows that
 o maximizes Er i2 ℓ i1   ℓ i2 . For identification, we have to assume or verify uniqueness.
c. The score is
s i   r i2 s i1   s i2 ,
where s i1   ∇  ℓ i1  ′ and s i2  ≡ ∇  ℓ i2  ′ . Therefore,
Es i  o s i  o  ′   Er i2 s i  o s i  o  ′   Es i2  o s i2  o  ′ 
 Er i2 s i1  o s i2  o  ′   Er i2 s i2  o s i1  o  ′ .
Now by the usual conditional MLE theory, Es i  o |y i2 , x i   0 and, since r i2 and s i2  are
functions of y i2 , x i , it follows that Er i2 s i1  o s i2  o  ′ |y i2 , x i   0, and so its transpose also
has zero conditional expectation. As usual, this implies zero unconditional expectation. We
have shown
Es i  o s i  o  ′   Er i2 s i1  o s i1  o  ′   Es i2  o s i2  o  ′ .
Now, by the unconditional information matrix equality for the density hy 2 |x; ,
Es i2  o s i2  o  ′   −EH i2  o ,

278

where H i2  o   ∇  s i2 . Further, by the conditional IM equality for the density
gy 1 |y 2 , x; ,
Es i1  o s i1  o  ′ |y i2 , x i   −EH i1  o |y i2 , x i ,
where H i1  o   ∇  s i1 . Since r i2 is a function of y i2 , x i , we can put r i2 inside both
expectations in (13.102). Then, by iterated expectations,
Er i2 s i  o s i  o  ′   −Er i2 H i1  o .
Combining all the pieces, we have shown that
Es i  o s i  o  ′   −Er i2 H i1  o  − EH i2  o 
 −Er i2 ∇  s i1   ∇  s i2 
 −E∇ 2 ℓ i  ≡ −EH i .
So we have verified that an unconditional IM equality holds, which means we can estimate the
asymptotic variance of N ̂ −  o  by estimating −EH i  −1 .
d. From part c, one consistent estimator of Avar N ̂ −  o  is
N

N −1 ∑r i2 Ĥ i1  Ĥ i2 ,
i1

where the notation should be obvious. In some cases it may be simpler to use an expected
Hessian form for each piece, which we can obtain by looking for consistent estimators of
−Er i2 H i1  o  and −EH i2  o . By definition, A i2  o  ≡ −EH i2  o |x i , and so
EA i2  o  ≡ −EH i2  o . By the usual law of large numbers argument,
N

N

−1

∑ Â i2 → −EH i2  o .
p

i1

Similarly, since A i1  o  ≡ −EH i1  o |y i2 , x i , and r i2 is a function of y i2 , x i , it follows that

279

(13.102)

N

Er i2 A i1  o   −Er i2 H i1  o . Under general regularity conditions, N −1 ∑ i1 r i2 Â i1
consistently estimates −Er i2 H i1  o . This completes what we needed to show.
Interestingly, even though we do not have a true conditional maximum likelihood problem,
we can still use the conditional expectations of the Hessians – but conditioned on different sets
of variables, y i2 , x i  in one case, and x i in the other – to consistently estimate the asymptotic
variance of the partial MLE.
e. (Bonus Question) Show that if we were able to use the entire random sample, the
resulting conditional MLE would be more efficient than the partial MLE based on the selected
sample.
Solution
We use a standard fact about positive definite matrices: if A and B are P  P positive
definite matrices, then A − B is p.s.d. if and only if B −1 − A −1 is p.s.d. Now, as we showed in
part d, the asymptotic variance of the partial MLE is Er i2 A i1  o   A i2  o  −1 . If we could
use the entire random sample for both terms, the asymptotic variance would be
EA i1  o   A i2  o  −1 . But
EA i1  o   A i2  o  − Er i2 A i1  o   A i2  o   E1 − r i2 A i1  o ,
which is p.s.d. because A i1  o  is p.s.d. and 1 − r i2 ≥ 0. Intuitively, the larger is Pr i2  1 the
smaller is the efficiency difference.
13.8. a. This is similar to Problem 12.12 for nonlinear regression; here we are specifying a
full conditional distribution. We can use the results in Section 12.4.2:
−1
′
Avar N ̂ −  o   A −1
o B o  F o C o F o A o
−1
′ −1
 A −1
o  Ao FoCoFoAo

280

where C o Er i  o r i  o  ′ . We also use the information matrix equality, A o  B o , where
A o  Es i  o ;  o s i  o ;  o   −EH i  o ;  o 
and
s i ;   ∇  logfy i |x i , gw i , ;  ′
H i  o ;  o   ∇  s i ; .
To use the formula we need to characterize F o . First,
∇  s i ;   ∇  ∇  logfy i |x i , gw i , ;  ′ ,
which generally requires using the chain rule to compute. Write
ky, x, g;   ∇  logfy i |x i , gw i , ;  ′ .
Then
∇  s i ;   ∇ g ky i , x i , gw i , , ∇  gw i , 
and
F o  E∇ g ky i , x i , gw i ,  o ,  o ∇  gw i ,  o 
b. Generally,
′ −1
−1
−1
Avar N ̂ −  o   Â o  Â o F̂ o Ĉ o F̂ o Â o

where Â o is one of the various choices for estimating the information matrix, evaluated at ̂
and ̂,
N

ĈN

−1

∑ rwi , ̂rwi , ̂ ′
i1

and

281

N

F̂  N −1 ∑ ∇ g ky i , x i , gw i , ̂, ̂∇  gw i , ̂.
i1

c. It applies directly where the scalar  o plays the role of  o . The score for this problem
(with respect to ) is
x i   g i 
s i ;  
x i   g i 1 − x i   g i 

x ′i
g i 

y i − x i   g i ,

where g i   h i − z i . The full Jacobian of s i ;  with respect to  is complicated, but it is
easy to see it has the form
∇  s i ;   Lx i , z i , h i ; , y i − x i   g i 


x i   g i 
x i   g i 1 − x i   g i 

x ′i
g i 

z i x i   g i 

When evaluated at the true values  o and  o , the the first term has zero expectation conditional
on x i , z i , h i  because
Ey i |x i , z i , h i   x i  o   o g i  o 
So F o can be estimated by plugging in the estimators and averaging the second term across i.
d. When  o  0, the second term in ∇  s i  o ;  o  is zero, and so
E∇  s i  o ;  o |x i , z i , h i   0,
which means condition (12.37) holds and F o  0. This implies, from part a,
Avar N ̂ −  o   A −1
o .
e. Because  o is an element of  o , for testing H 0 :  o  0 we can ignore the fact that  o
has been estimated in the first stage. In other words, when we run probit of y i on x i , ĝ i ,

282

i  1, . . . , N, where ĝ i  h i − z i ̂, we can use a standard probit t statistic on g i .
13.9. a. Under the Markov assumption, the joint density of y i0 , … , y iT  is given by
f T y T |y T−1   f T−1 y T−1 |y T−2 f 1 y i |y 0   f 0 y 0 ,
so we would need to model f 0 y 0  to obtain a model of the joint density.
b. The log likelihood
T

ℓ i  

∑ logf t y it |y i,t−1 ; 
t1

is the conditional log-likelihood for the density of y i1 , … , y iT  given y i0 , and so the usual
theory of conditional maximum likelihood applies. In practice, this is MLE pooled across i
and t.
c. Because we have the density of y i1 , … , y iT  given y i0 , we can use any of the three
asymptotic variance estimators implied by the information matrix equality. However, we can
also use the simplifications due to dynamic completeness of each conditional density. Let
s it   ∇  logfy it |y i,t−1 ; , H it   ∇  s it  and A it  o   −EH it |y i,t−1 , t  1, … T.
Then Avar N ̂ −  o  is consistently estimated using the inverse of any of the three matrices
in equation (13.50). If we have a canned package that computes a particular MLE, we can just
use any of the usual asymptotic variance estimates obtained from the pooled MLE.
13.10. a. Because of conditional independence, the joint density [conditional on x, c] is
the product of the marginal densities [conditional on x, c]:
fy 1 , y 2 , … , y G |x, c   Gg1 f g y g |x, c.
b. Let gy i , … , y G |x be the joint density of y i given x i  x . Then
gy i , … , y G |x 

 fy i , y 2, … , y G |x, chc|xdc.
283

c. The density gy i , … , y G |x is now
gy i , … , y G |x;  o ,  o  

 fy i , y 2, … , y G |x, c;  o hc|x;  o dc
G



  fy g |x, c;  go hc|x;  o dc,
g1

and so the log likelihood for observation i is
loggy i1 , … , y G |x i ;  o ,  o 
G

 log

  fy ig |x i , c;  go hc|x i ;  o dc

.

g1

d. This setup has some features in common with a linear SUR model, although here the
correlation across equations is assumed to come through a single common component, c.
Because of computational issues with general nonlinear models – especially if G is large and
some of the models are for qualitative response – one probably needs to restrict the cross
equation correlation somehow.
13.11. a. For each t ≥ 1, the density of y it given y i,t−1  y i,t−1 , y i,t−2  y t−2 , , … , y i0  y 0 and
c i  c is
f t y t |y t−1 , c  2 2e  −1/2 exp−y t − y t−1 − c 2 /2 2e .
Therefore, the density of y i1 , … , y iT  given y i0  y o and c i  c is obtained by the product of
these densities:
 Tt1 2 2e  −1/2 exp−y t − y t−1 − c 2 /2 2e .
If we plug in the data for observation i and take the log we get

284

T

∑−1/2 log 2e  − y it − y i,t−1 − c i  2 /2 2e 
t1

T



−T/2 log 2e 

− ∑y it − y i,t−1 − c i  2 /2 2e ,
t1

where we have dropped the term that does not depend on the parameters.
It is not a good idea to “estimate” the c i along with the  and  2e , as the incidental
parameters problem causes inconsistency – severe in some cases – in the estimator of .
b. If we write c i   0   1 y i0  a i , under the maintained assumption, then the density of
y i1 , … , y iT  given y i0  y 0 , a i  a is
T

2 2e  −1/2 exp−y t − y t−1 −  0 −  1 y 0 − a 2 /2 2e .
t1

Now, to get the density condition on y i0  y 0 only, we integrate this density over the density of
a i given y i0  y 0 . But a i and y i0 are independent, and a i ~ Normal0,  2a . So the density of
y i1 , … , y iT  given y i0  y o is


T

 − 2 2e  −1/2 exp−y t − y t−1 −  0 −  1 y 0 − a 2 /2 2e 

 −1
a a/ a da.

t1

If we now plug in the data y i0 , y i1 , … , y iT  for each i and take the log we get a conditional
log-likelihood (conditional on y i0 ) for each i. We can estimate the parameters by maximizing
the sum of the log-likelihoods across i.
c. As before, we can replace c i with  o   1 y i0  a i . Then, the density of y it given
y i,t−1 , … , y i1 , y i0 , a i  is
Normaly i,t−1   0   1 y i0  a i   0   1 y i0  a i y i,t−1 ,  2e ,
t  1, … , T. Using the same argument as in part b, we just integrate out a i to get the density
285

of y i1 , … , y iT  given y i0  y 0 :


T

 − 2 2e  −1/2 exp−y t − y t−1 −  0 −  1 y i0 − a −  0   1 y i0  ay t−1  2 /2 2e 

 −1
a a/ a da.

t1

Numerically, this could be a difficult MLE problem to solve. Assuming we can get the MLEs,
we would estimate   Ec i  as ̂  ̂ ̂ 0  ̂ 1 ȳ 0 , where ȳ 0 is the cross-sectional average of
the initial observation.
d. The log likelihood for observation i, now conditional on y i0 , z i , is the log of


T

 − 2 2e  −1/2 exp−y it − y i,t−1 − z it  −  0 −  1 y i0 − z̄ i − a 2 /2 2e 

 −1
a a/ a da.

t1

The assumption that we can put in the time average, z̄ i , to account for correlation between c i
and y i0 , z i , may be too strong. It may be better to put in the full vector z i , although this leads
to many more parameters to estimate.
13.12. a. The first order conditions can be written as
N

∑ x ij y i − mx i , ̂

 0, j  1, . . . , K.

i1

If, say, the first element of x i is unity, x i1 ≡ 1, then the first entry of the FOC is
N

∑y i − mx i , ̂

0

i1

or
N

∑ û i  0.
i1

b. For the Bernoulli QLL with mean function x  expx/1  expx it is easily

286

seen that the FOC is
N

∑ x ′i y i − x i ̂

 0, j  1, . . . , K.

i1

and so the canonical mean function is the logistic function. Therefore, g   −1 , and to
find  −1  we need to solve for z as a function of  in
  expz/1  expz  1/exp−z  1. So
1 − 1  1 − 
exp−z  

or
expz 


.
1 − 

Now just take the log to get z  log/1 − .
c. Generally, the FOC for the Poisson QMLE has the form
N

∑ ∇  mx i , ̂ ′ y i − mx i , ̂/mx i , ̂  0
i1

and, with mx,   expx, we get
N

∑ expx i ̂x ′i y i − expx i ̂/ expx i ̂  0,
i1

or
N

∑ x ′i y i − expx i ̂

 0.

i1

So mz  expz is the canonical mean function and its inverse is g  log.
d. This is a true statement. The score for observation i has the form

287

s i   x ′i y i − mx i , ,
and therefore the Hessian, −x ′i ∇  mx i ,  – which has the form −rx i , x ′i x i for some function
r  0 – does not depend on y i . If  ∗ is the plim of ̂ whether or not the mean is correctly
specified, then a consistent estimator of −EHx i ,  ∗  is
N

N −1 ∑ rx i , ̂x i x i  0.
′

i1

By contrast, with any other mean (link) function, the Hessian depends on y i , and the estimators
based on EHx i ,  o |x i  under the assumption the mean is correctly specified are generally
inconsistent.
13.13. In fact, there is nothing special about the QMLE setup for this problem: the
conclusion holds for M-estimation. It is instructive to see the general argument.
To prove the result for general M-estimation (whether a minimization or maximization
problem), use a mean value expansion and multiply through by N −1/2 :
N

N

i1

i1

N −1/2 ∑ qw i , ̂  N −1/2 ∑ qw i ,  ∗  

N

N −1 ∑ s i ̈

′

N ̂ −  ∗ 

i1

where s i  is the P  1 score and ̈ is on the line segment between ̂ and  ∗ . By Lemma 12.1,
N

N

−1

∑ s i ̈ → Es i  ∗ 
p

i1

p

because ̈ →  ∗ . Under the regularity conditions in Theorem 12.3, Es i  ∗   0, and so
N −1 ∑

N
i1

p
s i ̈ → 0. We also know N ̂ −  ∗   O p 1, and so

288

N

N

N

i1

i1

i1

N −1/2 ∑ qw i , ̂  N −1/2 ∑ qw i ,  ∗   o p 1  O p 1  N −1/2 ∑ qw i ,  ∗   o p 1.

289

13.14 (Bonus Question). Let fy t |x t ;  : t  1, … , T be a sequence of correctly specified
densities for y it given x it . That is, assume that there is  o ∈ intΘ such that fy t |x t ;  o  is the
density of y it given x it  x t . Also assume that x it : t  1, 2, … , T is strictly exogenous for
each t: Dy it |x i1 , … , x iT   Dy it |x it .
a. It is true that, under the standard regularity conditions for partial MLE, that
Es it  o |x i1 , … , x iT   0, where s it  o   ∇  log f t y it |x it ;  ′ ?
b. Under the assumptions given, is s it  o  : t  1, … , T necessarily serially uncorrelated?
c. Let c i be “unobserved heterogeneity” for cross section unit i, and assume that, for each t,
Dy it |z i1 , … , z iT , c i   Dy it |z it , c i 
In other words, z it : t  1, … , T is strictly exogenous conditional on c i . Further, assume that
Dc i |z i1 , … , z iT   Dc i |z̄ i ,
where z̄ i  T −1 z i1 . . . z iT  is the vector of time averages. Assuming that well-behaved,
correctly-specified conditional densities are available, how do we choose x it to make part a
applicable?
Solution
a. This is true because, by the general theory for partial MLE, we know that
Es it  o |x it   0, t  1, … , T. But if Dy it |x i1 , … , x iT   Dy it |x it  then, for any function
m t y it , x it , Em t y it , x it |x i1 , … , x iT  Em t y it , x it |x it , including the score function.
b. No. Strict exogeneity and complete dynamic specification of the conditional density are
entirely different. Saying that Dy it |x i1 , … , x iT  does not depend on x is , s ≠ t, says nothing
about whether y ir , r  t, appears in Dy it |x it , y i,t−1 , x i,t−1 , . . . , y i1 , x i1 . Of course it is possible (if
unlikely) for the score to be serially uncorrelated without complete dynamic specification, but

290

that is still a separate issue from strict exogeneity.
c. We take x it  z it , z̄ i , t  1, … , T. If g t y t |z t , c;  is correctly specified for the density of
y it given z it  z t , c i  c, and hc|z̄ ;  is correctly specified for the density of c i given z̄ i  z̄ ,
then the density of y it given z i is obtained as
f t y t |z i ;  o  

C g t y t |z it , c;  o hc|z̄ i ;  o vdc

and this clearly depends only on z it , z̄ i . In other words, under the assumptions given,
Dy it |z i1 , … , z iT   Dy it |z it , z̄ i , t  1, … , T
which implies
Dy it |x i1 , … , x iT   Dy it |x it , t  1, … , T.
Incidentally, we have not eliminated the serial dependence in y it  after only conditioning on
z it , z̄ i : the part of c i not explained by z̄ i affects y it in each time period.
13.15 (Bonus Question). Consider the problem of estimating quantiles in a parametric
context. In particular, write
y   o  x o  u
Du|x  Normal0,  2o exp2x o 
This means that  o  x o  Ey|x  Medy|x.
a. For 0    1 let   be the  th quantile in the standard normal distribution. (So, for
example,  .95  1. 645.) Find Quant  y|x in terms of   and all of the parameters. When is
Quant  y|x a linear function of x?
b. Given a random sample of size N, how would you estimate Quant  y|x for a given ?
c. Suppose we do not assume normality but use the weaker assumption that u/ o expx o 

291

is independent of x. Can we consistently estimate Quant  y|x in this case?
Solution
a. First note that
Quant  y|x   o  x o  Quant  u|x.
Let r  u/ o expx o , so that r is independent of x with a Normal0, 1 distribution. Because
u has a strictly increasing cdf conditional on its quantile q  x is the unique value such that
Pu ≤ q  x|x  ,
or
P

q  x
u
≤
x
 o expx o 
 o expx o 



or
P r≤

q  x
x
 o expx o 

 .

Because r is independent of x, its  th quantile conditonal on x is   . Therefore, we must have
q  x
 
 o expx o 
or
q  x     o expx o .
So we have derived
Quant  y|x   o  x o     o expx o .
Quant  y|x is linear in x for  . 5 because then    0. It is also linear in x for any  if
 o  0, in which case it can be written as

292

Quant  y|x   o  x o     o .
Of course if  o  0 then u and x are independent, and the quantile functions for different  are
parallel lines with different intercepts,  o     o .
b. Because we have specified
Dy|x  Normal o  x o ,  2o exp2x o 
we can use maximum likelihood to estimate all parameters, given a random sample of size N.
Then
Quant  y|x  ̂  x̂    ̂ expx̂.
c. The quantile function still has the form
Quant  y|x   o  x o     o expx o 
but we must treat   as an unknown parameter because we do not know the distribution of r.
Note that the distribution of r may be asymmetric. The key restriction is that Dr|x does not
depend on x.
We know   is the  th quantile of the random variable r. If we observed r i : i  1, . . . , N
we could estimate   as the  th sample quantile of the r i . Instead, we can use the standardized
residuals
r̂ i 

y i − ̂ − x i ̂
ûi

̂ expx i ̂
̂ expx i ̂

and compute ̂  as the  th sample quantile of r̂ i : i  1, . . . , N. Because ̂  solves the
problem

293

N

min
h

∑ c  h  − r̂ i ,
i1

where c   is the “check” function defined in Section 12.10, we can conclude ̂  is generally
consistent using the consistency result for two-step M-estimators in Section 12.4. Of course,
we have to have consistent estimators of the other parameters. From the results of Gourieroux,
Monfort, and Trognon (1984a), the normal QMLE is generally consistent for  o ,  o ,  o , and  o
even if normality does not hold. (As usual, we would need to use a sandwich covariance matrix
estimator for inference on these parameters.) Obtaining a valid standard error for ̂  , and then
getting the joint variance-covariance matrix of all parameter estimators, is challenging.
Probably the nonparametric bootstrap is valid.

294

Solutions to Chapter 14 Problems
14.1. a. The simplest way to estimate (14.35) is by 2SLS, using instruments x 1 , x 2 .
Nonlinear functions of these can be added to the instrument list, and they would generally
improve efficiency if  2 ≠ 1. If Eu 22 |x   22 , 2SLS using the given list of instruments is the
efficient, single equation GMM estimator. If there is heteroskedasticity an optimal weighting
matrix that allows heteroskedasticity of unknown form should be used. Finally, one could try
to use the optimal instruments derived in section 14.4.3. Even under homoskedasticity, these
are difficult, if not impossible, it find analytically if  2 ≠ 1.
With y 2 ≥ 0, equation (14.35) is suspect as a structural equation because it is a linear
model, and generally there are outcomes where x 2  2   3 y 1  u 2  0.
b. No. If  1  0 the parameter  2 does not appear in the model. Of course, if we knew
 1  0, we would consistently estimate  1 by OLS.
c. We can see this by obtaining Ey 1 |x:


Ey 1 |x  x 1  1   1 Ey 2 2 |x  Eu 1 |x


 x 1  1   1 Ey 2 2 |x.


Now, when  2 ≠ 1, Ey 2 2 |x ≠ Ey 2 |x  2 , so we cannot write
Ey 1 |x  x 1  1   1 x 2   2 ;
in fact, we cannot find Ey 1 |x without more assumptions. While the regression y 2 on x 2
consistently estimates  2 , the two-step NLS estimator of y i1 on x i1 , x i ̂ 2   2 will not be
consistent for  1 and  2 . (This is an example of a “forbidden regression,” which we discussed
in Chapter 9.) When  2  1 and we impose this in estimation, we obtain the usual 2SLS
estimator.

295

14.2. a. When  1  1, we obtain the level-level model, hours  − 1  z 1  1   1 wage  u 1 .
Using the hint, let  1  0 to get hours  z 1  1   1 logwage  u 1 .
b. We cannot use a standard t test after estimating the full model (say, by nonlinear 2SLS),
because  1 cannot be estimated under H 0 . The score test and QLR test also fail because of lack
of identification under H 0 . What we can do is fix a value for  1 – essentially our best guess –
and then use a t test on wage  1 − 1/ 1 after linear 2SLS estimation (or GMM more
generally). This need not be a very good test for detecting  1 ≠ 0 if our guess for  1 is not
close to the actual value. There is a growing literature on testing hypotheses when parameters
are not identified under the null.
c. If Varu 1 |z   21 , use nonlinear 2SLS, where we would use z and functions of z as IVs.
If we are not willing to assume homoskedasticity, GMM is generally more efficient.
d. The residual function is r  hours − z 1  1 −  1 wage  1 − 1/ 1 , where
   ′1 ,  1 ,  1  ′ . Using the hint the gradient is
∇  r  −z 1 , −wage  1 − 1/ 1 ,  1 wage  1 − 1 −  1 wage  1 logwage/ 21 .
The score is just the transpose.
e. Estimate  1 and  1 by 2SLS, or use the GMM estimator that accounts for
heteroskedasticity, under the restriction  1  1. Suppose the instruments are z i , a 1  L vector.
This is just linear estimation because the model is linear under H 0 . Then, taking Z i  z i , and
r i ̃  hours i − z i1 ̃ 1 − ̃ 1 wage i − 1
∇  r i ̃  −z i1 , −wage i − 1, ̃ 1 wage i − 1 − wage i logwage i ,
use the score statistic in equation (14.32).
14.3. Let Z ∗i be the G  G matrix of optimal instruments in (14.57), where we suppress its

296

dependence on x i . Let Z i be the G  L matrix that is a function of x i and let  o the probability
limit of the weighting matrix. Then the asymptotic variance of the GMM estimator has the
form (14.10) with G o EZ ′i R o x i . So, in (14.48) take A ≡ G ′o  o G o and
sw i  ≡ G ′o  o Z ′i rw i ,  o . The optimal score function is s ∗ w i  ≡ R o x i  ′  o x i  −1 rw i ,  o .
Now we can verify (14.51) with   1:
Esw 1 s ∗ w 1  ′   G ′o  o EZ ′i rw i ,  o rw i ,  o  ′  o x i  −1 R o x i 
 G ′o  o EZ ′i Erw i ,  o rw i ,  o  ′ |x i  o x i  −1 R o x i 
 G ′o  o EZ ′i  o x i  o x i  −1 R o x i   G ′o  o G o  A.
14.4. a. The residual function for the conditional mean model Ey i |x  mx i ,  o  is
r i  ≡ y i − mx i , . Then  o x i  in (14.55) is just a scalar,  o x i   Vary i |x i  ≡  o x i .
Under WNLS.3,  o x i    2o hx i ,  o  for a known function h. Further,
R o x i  ≡E∇  r i  o |x i   −∇  mx i ,  o , and so the optimal instruments are
∇  mx i ,  o / o x i . The asymptotic variance of the efficient IV estimator is obtained from
(14.60):
E∇  mx i ,  o  ′  o x i  −1 ∇  mx i ,  o  −1   2o E∇  mx i ,  o  ′ ∇  mx i ,  o /hx i ,  o  −1 ,
which is the asymptotic variance of the WNLS estimator under WNLS.1, WNLS.2, and
WNLS.3.
b. If Vary i |x i    2o then NLS achieves the efficiency bound, as it seen by setting
hx,  o  ≡ 1 in part a.
c. Now let r i1  ≡ u i  ≡ y i − mx i ,  and r i2 ,  2   y i − mx i ,  2 −  2 . Let r i 
denote the 2  1 vector obtained by stacking the two residual functions. Then the moment
conditions can be written as

297

Er i  o |x i   0,
where  o   ′o ,  2o  ′ . To obtain the efficient IVs, we first need E∇  r i  o |x i . But
−∇  m i 

∇  r i  

0

.

−2∇  m i u i  −1

Evaluating at  o and using Eu i  o |x i   0 gives
R ∘ x i  ≡ ∇  r i  

−∇  m i 

0

0

−1

.

We also need
′

 o x i   Er i  o r i  o  |x i  

 2o

Eu 3i |x i 

Eu 3i |x i  Eu 4i |x i  −  4o

where u i ≡ y i − mx i ,  o . The optimal IVs are  o x i  −1 R o x i . If Eu 3i |x i   0, as occurs
under conditional symmetry of u i , then the asymptotic variance matrix of the optimal IV
estimator is block diagonal, and for ̂ it is the same as NLS. In other words, adding the moment
condition for the homoskedasticity assumption does not improve efficiency over NLS under
symmetry, even if Eu 4i |x i  is not constant. But there is something subtle here. the NLS
estimator is efficient in the class of estimators that only uses information on the first two
conditional moments. If we use the information Eu 3i |x i   0 then, in general, we could do
better. But, of course, such an estimator would be less robust than NLS.
If, in addition, Eu 4i |x i  is constant, then the usual estimator of  2o based on the sum of
squared NLS residuals is efficient (among estimators that only use the first two conditional
moments, but it happens that Eu 3i |x i   0 and Eu 4i |x i  is constant).
14.5. We can write the unrestricted linear projection as
298

y it   t0  x i  t  v it , t  1, 2, 3
where  t is 1  3K  1, and then  is the 3  9K  1 vector obtained by stacking the  t . Let
  ,  ′1 ,  ′2 ,  ′3 ,  ′  ′ . With the restrictions imposed on the  t we have
 t0  , t  1, 2, 3,  1   1   ′ ,  ′2 ,  ′3  ′
 2   ′1 ,  2   ′ ,  ′3  ′ ,  3   ′1 ,  ′2 ,  3   ′  ′
Therefore, we can write   H for the 3  9K  1  4K matrix H defined by
1 0

0

0

0 IK

0

0 IK

0 0 IK

H

0

0

0

0 0

0 IK

0

1 0

0

0

0

0 IK

0

0

0

0 0 IK

0 IK

0 0

0 IK

0

1 0

0

0

0

0 IK

0

0

0

0 0 IK

0

0

0 0

.

0 IK IK

14.6. By this hint, it suffices to show that
Avar N ̂ −  o  −1 − Avar N ̃ −  o  −1
−1
−1
′ −1
′
is p.s.d. This difference is H ′o  −1
o H o − H o  o H o  H o  o −  o H o . This is positive
−1
semi-definite if  −1
o −  o is p.s.d., which again holds by the hint because  o −  ∘ is assumed

to be p.s.d.
14.7. With h  H, the minimization problem becomes

299

−1
minP ̂ − H ′ ̂ ̂ − H,


where it is assumed that no restrictions are placed on . The first order condition is easily seen
to be
−1
−1
−1
− 2H ′ ̂ ̂ − H  0 or H ′ ̂ H̂  H ′ ̂ ̂.
−1
Therefore, assuming H ′ ̂ H is nonsingular – which occurs w.p.a.1. when H ′  −1
o H – is
−1
−1
nonsingular – we have ̂  H ′ ̂ H −1 H ′ ̂ ̂.

14.8. From the efficiency discussion about maximum likelihood in Section 14.4.2, it is no
less asymptotically efficient to use the density of y i0 , y i1 , … , y iT  than to use the conditional
distribution y i1 , … , y iT  given y i0 . The cost of the asymptotic efficiency is that if we
misspecify f 0 y 0 ; , then the unconditional MLE will generally be inconsistent for  o . The
MLE that conditions on y i0 is consistent provided we have the densities f t y t |y t−1 ;  correctly
specified, t ≥ 1. As f t y t |y t−1 ;  is the density of interest, we are usually willing to put more
effort into testing our specification of it.
14.9. We have to verify equations (14.49) and (14.50) for the random effects and fixed
effects estimators with . The choices of s i1 , s i2 (with added i subscripts for clarity), A 1 , and A 2
are given in the hint. Now, from Chapter 10, we know that Er i r ′i |x i    2u I T under RE.1,
RE.2, and RE.3, where r i  v i − j T v̄ i . Therefore,
 ′i r i r ′i X
 i    2u EX
 ′i X
 i  ≡  2u A 1
Es i1 s ′il   EX
by the usual iterated expectations argument. This means that, in (14.49),  ≡  2u . Now, we just
 i and, as described in the hint,
̈ ′i u i r ′i X
need to verify (14.50) for this choice of . But s i2 s ′i1  X
̈ ′i v i − j v̄ i   X
̈ ′i v i  X
̈ ′i c i j T  u i   X
̈ ′i u i .
̈ ′i r i  X
X
T

300

 i and so
̈ ′i r i r ′i X
Therefore, s i2 s ′i1  X
 i   2u X
i
̈ ′i Er i r ′i |x i X
̈ ′i X
Es i2 s ′i1 |x i   X
It follows that
 i   2u EX
 i
̈ ′i Er i r ′i |x i X
̈ ′i X
Es i2 s ′i1   X
i  X
̈ ′i X i − j x̄ i   X
̈ ′i X i  X
̈ ′i X
̈ i , and so Es i2 s ′i1    2u EX
̈ ′i X
̈ i , and this
̈ ′i X
Finally, X
T
verifies (14.50) with    2u .
14.10. a. For each t we have
Ev it |x i   E t c i  u it |x i   E t c i |x i   Eu it |x i 
  t Ec i |x i   Eu it |x i 
 t  0  0  0
because Ec i |x i   0 and Eu it |x i   0 (because Eu it |x i , c i   0).
b. Under the assumptions – which are the same as Assumptions RE.1 and RE.3 – we know
that
Varu it    2u , t  1, . . . , T
Covc i , u it   0, t  1, . . . , T
Covu it , u is   0, t ≠ s.
Therefore,
Varv it   Var t c i  u it    2t  2c  2 t Covc i , u it    2u
  2t  2c   2u
and, for t ≠ s,
Covv it , v is   Cov t c i  u it ,  s c i  u is 
 Cov t c i ,  s c i   Cov t c i , u is   Cov s c i , u it   Covu it , u is 
  t  s Covc i , c i    t  s  2c

301

c. The usual RE estimator treats the  t as constant (which can then be normalized to be
unity). In other words, it uses a misspecified model for   Varv i . As we discussed in
Chapter 10, the RE estimator is still consistent and N -asymptotically normal, and we can
conduct inference using a robust variance matrix estimator.
A more efficient estimator is, of course, FGLS with the correct form of the
variance-covariance matrix. Write the T time periods for draw i as
yi  Xi  vi
Ev i |x i   0
where the t th row of X i is x it . The T  T variance-covariance matrix (which is also the
conditional on x i ) is
 2c   2u
Varv i  

 2  2c

 2  2c



 22  2c   2u 





 T  2c

 T  2  2c

 T  2c
 2  T  2c





,

  2T  2c   2u

where we impose the normalization  1  1. The GLS estimator is
̂ GLS 

−1

N

∑

X ′i  −1 X i

i1

N

∑ X ′i  −1 y i

.

i1

Of course, we would have to estimate  2c ,  2u , and  2 , . . . ,  T . One way to approach estimation
of the variance-covariance parameters is to write
v it v is   t  s  2c  d ts  2u  r its
Er its   0
for all t, s  1, 2, . . . , T, where d ts is a dummy variable equal to one if t  s, and zero otherwise.
Then we can estimate the parameters by pooled NLS after replacing v it v is with v it v is , where the

302

v it are perhaps the RE residuals (or they could be the POLS residuals). Note that  1 ≡ 1 is
̂ and then use FGLS.
imposed. Then we can form 
14.11. First estimate initial parameters  o from a set of linear reduced-form equations:
yi  Xio  ui
where  o is K  1 and unrestricted. Then estimate  o by, say, system OLS. Or, if we assumed
Varu i |x i    o then FGLS would be no less asymptotically efficient.
Given ̂ and ̂ consistent for
 o  Avar N ̂ −  o ,
the CMD estimator of  o , ̂, solves
−1
min ̂ − g ′ ̂ ̂ − g,
∈Θ

which is algebraically equivalent to a weighted multivariate nonlinear least squares problem
where ̂ plays the role of the K  1 vector of “dependent variables.” As discussed in the case
where g is linear, the asymptotic analysis of the CMD estimator is different from the
standard WMNLS problem: here K is fixed.
After estimation, Avar̂ is estimated as
′ −1
−1
Ĝ ̂ Ĝ /N

where Ĝ ≡ ∇  g̂.

303

Solutions to Chapter 15 Problems
15.1. a. Because the regressors are all orthogonal by construction – that is, dk i  dm i  0
for k ≠ m, and all i – the coefficient on dm i is obtained from the regression y i on
dm i , i  1, … , N. But this is easily seen to be the fraction of ones in the sample falling into
category m (because it is the average of y i over the observations from category m). Therefore,
the fitted value for any i is the cell frequency for the appropriate category. These frequencies
are all necessarily in [0,1].
b. The fitted values for each category will be the same. If we drop d1 i but add an overall
intercept, the overall intercept is the cell frequency for the first category, and the coefficient on
dm i becomes the difference in cell frequencies between category m and category one (the base
category), m  2, … , M.
15.2. a. First, because utility is increasing in both c and q, the budget constraint is binding
at the optimum: c i  p i q i  m i . Plugging c  m i − p i q into the utility function reduces the
problem to
max m i − p i q  a i log1  q.
q≥0

Define utility as a function of q, as
s i q ≡ m i − p i q  a i log1  q.
Then, for all q ≥ 0,
ds i q  −p  a i .
i
1q
dq
The optimal solution is q i  0 if the marginal utility of charitable giving at q  0 is
nonpositive, that is, if

304

ds i 0  −p  a ≤ 0 or a ≤ p .
i
i
i
i
dq
(This can also be obtained by solving the Kuhn-Tucker conditions.) Thus, for this utility
function, a i can be interpreted as the reservation price above which no charitable contribution
will be made; in other words, we have the corner solution q i  0 whenever the price of
charitable giving is too high relative to the marginal utility of charitable giving. On the other
hand, if a i  p i then an interior solution exists q i  0 and necessarily solves the first order
condition
ds i q   −p  a i ≡ 0
i
1  qi
dq i
or
1  q i  a i /p i .
b. By definition of y i , y i  1 if and only if a i /p i  1 or loga i /p i   0. If
a i  expz i   v i , the condition for y i  1 is equivalent to z i   v i − log p i  0. Therefore,
Py i  1|z i , m i , p i   Py i  1|z i , p i 
 Pz i   v i − log p i  0|z i , p i   Pv i /  −z i   log p i /
 1 − G−z i   log p i /  Gz i  − log p i ,
where the last equality follows by symmetry of the distribution of v i /.
15.3. a. If Py i  1|z 1 , z 2   z 1  1   1 z 2   2 z 22  then
∂Py  1|z 1 , z 2 
  1  2 2 z 2   z 1  1   1 z 2   2 z 22 ;
∂z 2
for given z, the partial effect is estimated as
̂ 1  2̂ 2 z 2   z 1 ̂ 1  ̂ 1 z 2  ̂ 2 z 22 ,

305

where, or course, the estimates are the probit estimates.
b. In the model
Py i  1|z i , z 2 , d 1   z 1  1   1 z 2   2 d 1   3 z 2 d 1 ,
the partial effect of z 2 is
∂Py  1|z 1 , z 2 , d 1 
  1   3 d 1   z 1  1   1 z 2   2 d 1   3 z 2 d 1 .
∂z 2
The effect of d 1 is measured as the difference in the probabilities at d 1  1 and d 1  0 :
Py  1|z, d 1  1 − Py  1|z, d 1  0  z 1  1   2   1   3 z 2  − z 1  1   1 z 2 .
Again, to estimate these effects at given z and – in the first case, d 1 – we just replace the
parameters with their probit estimates, and use average or other interesting values of z.
c. If the estimated partial effect is for particular values of z 1 , z 2 , d 1 , for example,
̂ 1  ̂ 3 d o1   z o1  1  ̂ 1 z o2  ̂ 2 d o1  ̂ 3 z o2 d o1 ,
then we can apply the delta method from Chapter 3 (and referred to in Part III). Thus, we
would require the full variance matrix of the probit estimates as well as the gradient of the
expression of interest, such as  1  2 2 z 2   z 1  1   1 z 2   2 z 22 , with respect to all probit
parameters. Alternatively, the bootstrap would be simply but require a bit more computation.
If we are interested in the average partial effect (APE) of d 1 going from zero to one then
we estimate it as
N

N

−1

∑z i1 ̂ 1  ̂ 1  ̂ 3 z i2  ̂ 2  − z i1 ̂ 1  ̂ 1 z i2 ,
i1

that is, we estimate the effect for each unit i and then average these across all i. If we want a
standard error for this, we would use the extension of the delta method worked out in Problem

306

12.17 – to account for the averaging as well as estimation of the parameters. The bootstrap can
be used, too.
d. (Bonus Question) For a fixed value of z 2 , say z o2 , how would you estimate the average
partial effect of d 1 on the response probability?
Solution
Now we average out only with respect to z i1 :
N

N

−1

∑z i1 ̂ 1  ̂ 1  ̂ 3 z o2  ̂ 2  − z i1 ̂ 1  ̂ 1 z o2 .
i1

We can then vary z o2 to see how the effect of changing d 1 from zero to one varies with z o2 .
Again, we can use Problem 12.17 to obtain an asymptotic standard error.
15.4. This is the kind of (nonsense) statement that arises out of failure to distinguish
between the underlying latent variable model and the model for Py  1|x. To compare the
LPM and probit on equal footing, we must recognize that the LPM assumes Py  1|x  x
while the probit model assumes that Py  1|x  x. So the substantive difference is
purely in the functional forms for the response probabilities. And the probit functional form
has some attractive properties compared with the linear model: x is always between zero
and one, and the marginal effect of any x j is diminishing after some point. The LPM and probit
models are both approximations to the true response probability, and the LPM has some
deficencies for describing the partial effects over a broad range of the covariates.
If one insists on focusing on normality of the latent error in the probit case then one must
compare that assumption with with the the corresponding assumption for the LPM. If we
specify a latent variable as y ∗  x  e then the LPM is obtained when e has a uniform
distribution over −,  for some constant 0    . For most purposes, this is much less
307

plausible than the normality underlying probit.
15.5. a. If Py  1|z, q  z 1  1   1 z 2 q then
∂Py  1|z, q
  1 q  z 1  1   1 z 2 q,
∂z 2
assuming that z 2 is not functionally related to z 1 .
b. Write y ∗  z 1  1  r, where r   1 z 2 q  e, and e is independent of z, q with a standard
normal distribution. Because q is assumed independent of z, q|z ~ Normal0,  21 z 22  1; this
follows because Er|z   1 z 2 Eq|z  Ee|z  0. Also,
Varr|z   21 z 22 Varq|z  Vare|z  2 1 z 2 Covq, e|z   21 z 22  1
because Covq, e|z  0 by independence between e and z, q. Thus, r/  21 z 22  1 has a
standard normal distribution independent of z. It follows that
Py  1|z   z 1  1 /  21 z 22  1 .
c. Because Py  1|z depends only on  21 , this is what we can estimate along with  1 . (For
example,  1  −2 and  1  2 give exactly the same model for Py  1|z.) This is why we
define  1   21 . Testing H 0 :  1  0 is most easily done using the score or LM test because,
under H 0 , we have a standard probit model.
Let ̂ 1 denote the probit estimates under the null that  1  0. Define  i  z i1 ̂ 1 ,
̂ i  z i1 ̂ 1 , û i  y i − 
̂ i , and ũ i ≡ û i / 
̂ i 1 − 
̂ i  (the standardized residuals). The

gradient of the mean function in (15.97) with respect to  1 , evaluated under the null estimates,
is simply ̂ i z i1 . The only other quantity needed is the gradient with respect to  1 evaluated at
the null estimates. But the partial derivative of (15.97) with respect to  1 is, for each i,

308

(15.97)

− z i1  1 z 2i2 /2 1 z 2i2  1

−3/2

 z i1  1 /  21 z 22  1 .

When we evaluate this at  1  0 and ̂ 1 we get −z i1 ̂ 1 z 2i2 /2̂ i . Then, the score statistic can
be obtained as NR 2u from the regression
ũ i on

̂ i z i1
̂ i 1 − 
̂ i


,

z i1 ̂ 1 z 2i2 ̂ i
;
̂ i 1 − 
̂ i


a

under H 0 , NR 2u ~  21 .
d. The model can be estimated by MLE using the formulation with  1 in place of  21 . It is
not a standard probit estimation but a kind of “heteroskedastic probit.”
15.6. a. What we would like to know is that, if we exogenously change the number of
cigarettes that someone smokes per day, what effect would this have on the probability of
missing work over a three-month period? In other words, we want to infer causality, not just
find a correlation between missing work and cigarette smoking.
b. Since people choose whether and how much to smoke, we certainly cannot treat the data
as coming from the experiment we have in mind in part a. (That is, we cannot randomly assign
people a daily cigarette consumption.) It is possible that smokers are less healthy to begin with,
or have other attributes that cause them to miss work more often. Or, it could go the other way:
cigarette consumption may be related to personality traits that make people harder workers. In
any case, cigs might be correlated with the unobservables in the equation.
c. If we start with the model
Py  1|z, cigs, q 1   z i1  1   1 cigs  q 1 ,
but ignore q 1 when it is correlated with cigs, we will not consistently estimate anything of
interest, whether the model is linear or nonlinear. Thus, we would not be estimating a causal

309

(15.98)

effect. If q 1 is independent of cigs, the probit ignoring q 1 does estimate the average partial
effect of another cigarette.
d. No. There are many people in the working population who do not smoke. Thus, the
distribution (conditional or unconditional) of cigs piles up at zero. Also, since cigs takes on
integer values, it cannot be normally distributed. But it is really the pile up at zero that is the
most serious issue.
e. Use the Rivers-Vuong test. Obtain the residuals, r̂ 2 , from the regression cigs on z. Then,
estimate the probit of y on z 1 , cigs, r̂ 2 and use a standard t test on r̂ 2 . This does not rely on
normality of r 2 (or cigs). It does, of course, rely on the probit model being correct for y under
H0.
f. Assuming people will not immediately move out of their state of residence when the state
implements no smoking laws in the workplace, and that state of residence is roughly
independent of general health in the population, a dummy indicator for whether the person
works in a state with a new law can be treated as exogenous and excluded from (15.98). (These
situations are often called “natural experiments.”) Further, cigs is likely to be correlated with
the state law indicator because since people will not be able to smoke as much as they
otherwise would. Thus, it seems to be a reasonable instrument for cigs.
15.7. a. The LPM estimates, with the usual and heteroskedasticity-robust standard errors,
are given below. Interesting, the robust standard errors on the non-demographic variables are
often notably smaller than the usual standard errors. The statistical significance of the OLS
coefficients is the same using either set of standard errors.
When pcnv goes from . 25 to . 75, the estimated probability of arrest falls by about . 077, or
7. 7 percentage points.

310

. use grogger
. gen arr86  0
. replace arr86  1 if narr86  0
(755 real changes made)
. reg arr86 pcnv avgsen tottime ptime86 inc86 black hispan born60
Source |
SS
df
MS
------------------------------------------Model | 44.9720916
8 5.62151145
Residual | 500.844422 2716 .184405163
------------------------------------------Total | 545.816514 2724
.20037317

Number of obs
F( 8, 2716)
Prob  F
R-squared
Adj R-squared
Root MSE








2725
30.48
0.0000
0.0824
0.0797
.42942

----------------------------------------------------------------------------arr86 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------pcnv | -.1543802
.0209336
-7.37
0.000
-.1954275
-.1133329
avgsen |
.0035024
.0063417
0.55
0.581
-.0089326
.0159374
tottime | -.0020613
.0048884
-0.42
0.673
-.0116466
.007524
ptime86 | -.0215953
.0044679
-4.83
0.000
-.0303561
-.0128344
inc86 | -.0012248
.000127
-9.65
0.000
-.0014738
-.0009759
black |
.1617183
.0235044
6.88
0.000
.1156299
.2078066
hispan |
.0892586
.0205592
4.34
0.000
.0489454
.1295718
born60 |
.0028698
.0171986
0.17
0.867
-.0308539
.0365936
_cons |
.3609831
.0160927
22.43
0.000
.329428
.3925382
----------------------------------------------------------------------------. reg arr86 pcnv avgsen tottime ptime86 inc86 black hispan born60, robust
Linear regression

Number of obs
F( 8, 2716)
Prob  F
R-squared
Root MSE







2725
37.59
0.0000
0.0824
.42942

----------------------------------------------------------------------------|
Robust
arr86 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------pcnv | -.1543802
.018964
-8.14
0.000
-.1915656
-.1171948
avgsen |
.0035024
.0058876
0.59
0.552
-.0080423
.0150471
tottime | -.0020613
.0042256
-0.49
0.626
-.010347
.0062244
ptime86 | -.0215953
.0027532
-7.84
0.000
-.0269938
-.0161967
inc86 | -.0012248
.0001141
-10.73
0.000
-.0014487
-.001001
black |
.1617183
.0255279
6.33
0.000
.1116622
.2117743
hispan |
.0892586
.0210689
4.24
0.000
.0479459
.1305714
born60 |
.0028698
.0171596
0.17
0.867
-.0307774
.036517
_cons |
.3609831
.0167081
21.61
0.000
.3282214
.3937449
----------------------------------------------------------------------------. di .5*_b[pcnv]
-.0771901

b. The robust statistic and its p-value are gotten by using the “test” command after
311

appending “robust” to the regression command. The p-values are virtually identical.
. qui reg arr86 pcnv avgsen tottime ptime86 inc86 black hispan born60
. test avgsen tottime
( 1)
( 2)

avgsen  0
tottime  0
F(

2, 2716) 
Prob  F 

0.18
0.8360

. qui reg arr86 pcnv avgsen tottime ptime86 inc86 black hispan born60, robust
. test avgsen tottime
( 1)
( 2)

avgsen  0
tottime  0
F(

2, 2716) 
Prob  F 

0.18
0.8320

c. The probit model estimates follow.
. probit arr86 pcnv avgsen tottime ptime86 inc86 black hispan born60
Probit regression

Number of obs
LR chi2(8)
Prob  chi2
Pseudo R2

Log likelihood  -1483.6406






2725
249.09
0.0000
0.0774

----------------------------------------------------------------------------arr86 |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------pcnv | -.5529248
.0720778
-7.67
0.000
-.6941947
-.4116549
avgsen |
.0127395
.0212318
0.60
0.548
-.028874
.0543531
tottime | -.0076486
.0168844
-0.45
0.651
-.0407414
.0254442
ptime86 | -.0812017
.017963
-4.52
0.000
-.1164085
-.0459949
inc86 | -.0046346
.0004777
-9.70
0.000
-.0055709
-.0036983
black |
.4666076
.0719687
6.48
0.000
.3255516
.6076635
hispan |
.2911005
.0654027
4.45
0.000
.1629135
.4192875
born60 |
.0112074
.0556843
0.20
0.840
-.0979318
.1203466
_cons | -.3138331
.0512999
-6.12
0.000
-.4143791
-.213287
-----------------------------------------------------------------------------

Now, we must compute the difference in the normal cdf at the two different values of pcnv,
black  1, hispan  0, born60  1, and at the average values of remaining variables.
. sum avgsen tottime ptime86 inc86
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------avgsen |
2725
.6322936
3.508031
0
59.2
tottime |
2725
.8387523
4.607019
0
63.4

312

ptime86 |
inc86 |

2725
2725

.387156
54.96705

1.950051
66.62721

0
0

12
541

. di normal(_b[_cons]  _b[pcnv]*.75  _b[avgsen]*.6322936
 _b[tottime]*.8387523  _b[ptime86]* .387156  _b[inc86]*54.96705
 _b[black]  _b[born60])
- normal(_b[_cons]  _b[pcnv]*.25  _b[avgsen]*.6322936
 _b[tottime]*.8387523  _b[ptime86]* .387156  _b[inc86]* 54.96705
 _b[black]  _b[born60])
-.10166064

This last command shows that the probability falls by about .102, which is somewhat larger
than the effect obtained from the LPM.
d. To obtain the percent correctly predicted for each outcome, we first generate the
predicted values of arr86 as described on page 465:
. predict PHIhat
(option pr assumed; Pr(arr86))
. gen arr86t  PHIhat  .5
. tab arr86t arr86
|
arr86
arr86t |
0
1 |
Total
------------------------------------------0 |
1,903
677 |
2,580
1 |
67
78 |
145
------------------------------------------Total |
1,970
755 |
2,725
. di 1903/1970
.96598985
. di 78/755
.10331126
. di (1903  78)/2725
.72697248

For men who were not arrested, the probit predicts correctly about 96.6% of the time.
Unfortunately, for the men who were arrested, the probit is correct only about 10.3% of the
time. The overall percent correctly predicted is pretty high – 72.7% – but we cannot very well
predict the outcome we would most like to predict.
e. Adding the quadratic terms gives

313

. probit arr86 pcnv avgsen tottime ptime86 inc86 black hispan born60
pcnvsq pt86sq inc86sq
Probit regression

Number of obs
LR chi2(11)
Prob  chi2
Pseudo R2

Log likelihood  -1439.8005






2725
336.77
0.0000
0.1047

----------------------------------------------------------------------------arr86 |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------pcnv |
.2167615
.2604937
0.83
0.405
-.2937968
.7273198
avgsen |
.0139969
.0244972
0.57
0.568
-.0340166
.0620105
tottime | -.0178158
.0199703
-0.89
0.372
-.056957
.0213253
ptime86 |
.7449712
.1438485
5.18
0.000
.4630333
1.026909
inc86 | -.0058786
.0009851
-5.97
0.000
-.0078094
-.0039478
black |
.4368131
.0733798
5.95
0.000
.2929913
.580635
hispan |
.2663945
.067082
3.97
0.000
.1349163
.3978727
born60 | -.0145223
.0566913
-0.26
0.798
-.1256351
.0965905
pcnvsq | -.8570512
.2714575
-3.16
0.002
-1.389098
-.3250042
pt86sq | -.1035031
.0224234
-4.62
0.000
-.1474522
-.059554
inc86sq |
8.75e-06
4.28e-06
2.04
0.041
3.63e-07
.0000171
_cons |
-.337362
.0562665
-6.00
0.000
-.4476423
-.2270817
----------------------------------------------------------------------------Note: 51 failures and 0 successes completely determined.
. test pcnvsq pt86sq inc86sq
( 1)
( 2)
( 3)

pcnvsq  0
pt86sq  0
inc86sq  0
chi2( 3) 
Prob  chi2 

38.54
0.0000

The quadratics are individually and jointly significant. The quadratic in pcnv means that, at
low levels of pcnv, there is actually a positive relationship between probability of arrest and
pcnv, which does not make much sense. The turning point is easily found as
. 217/2. 857 ≈. 127, and there are many cases – 1,265 – where pcnv is less than . 127.
. sum pcnv
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------pcnv |
2725
.3577872
.395192
0
1
. count if pcnv  .127
1265

15.8. a. The following Stata session answers this part. The difference in estimated
probabilities of smoking at 16 and 12 years of education is about −. 080. In other words, for

314

non-white women at the average family income, women with 16 years of education are, on
average, about eight percentage points less likely to smoke.
. use bwght
. gen smokes  cigs  0
. tab smokes
smokes |
Freq.
Percent
Cum.
----------------------------------------------0 |
1,176
84.73
84.73
1 |
212
15.27
100.00
----------------------------------------------Total |
1,388
100.00
. probit smokes motheduc white lfaminc
Probit regression

Number of obs
LR chi2(3)
Prob  chi2
Pseudo R2

Log likelihood  -546.76991






1387
92.67
0.0000
0.0781

----------------------------------------------------------------------------smokes |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------motheduc | -.1450599
.0207899
-6.98
0.000
-.1858074
-.1043124
white |
.1896765
.1098805
1.73
0.084
-.0256853
.4050383
lfaminc | -.1669109
.0498894
-3.35
0.001
-.2646923
-.0691296
_cons |
1.126276
.2504611
4.50
0.000
.6353817
1.617171
----------------------------------------------------------------------------. sum faminc
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------faminc |
1388
29.02666
18.73928
.5
65
. di normal(_b[_cons]  _b[motheduc]*16  _b[lfaminc]*log(29.02666))
- normal(_b[_cons]  _b[motheduc]*12  _b[lfaminc]*log(29.02666))
-.08020112

b. The variance faminc is probably not exogenous because, at a minium, income is likely
correlated with quality of health care. It might also be correlated with unobserved cultural
factors that are correlated with smoking.
c. The reduced form equation for lfaminc is estimated below. As expected, fatheduc has a
positive partial effect on lfaminc, and the relationship is statistically significant. We need the

315

residuals from this regression for part d. We lose 196 observations due to missing data on
fatheduc, and one observation has already been lost due to a missing value for motheduc.
. reg lfaminc motheduc white fatheduc
Source |
SS
df
MS
------------------------------------------Model | 140.936735
3 46.9789115
Residual | 467.690904 1187 .394010871
------------------------------------------Total | 608.627639 1190 .511451797

Number of obs
F( 3, 1187)
Prob  F
R-squared
Adj R-squared
Root MSE








1191
119.23
0.0000
0.2316
0.2296
.6277

----------------------------------------------------------------------------lfaminc |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------motheduc |
.0709044
.0098338
7.21
0.000
.0516109
.090198
white |
.3452115
.050418
6.85
0.000
.2462931
.4441298
fatheduc |
.0616625
.008708
7.08
0.000
.0445777
.0787473
_cons |
1.241413
.1103648
11.25
0.000
1.024881
1.457945
----------------------------------------------------------------------------. predict v2hat, resid
(197 missing values generated)

d. To test the null of exogeneity, we estimate the probit that includes v̂ 2 :
. probit smokes motheduc white lfaminc v2hat
Iteration
Iteration
Iteration
Iteration
Iteration

0:
1:
2:
3:
4:

log
log
log
log
log

likelihood
likelihood
likelihood
likelihood
likelihood







-471.77574
-432.90303
-432.0639
-432.06242
-432.06242

Probit regression

Number of obs
LR chi2(4)
Prob  chi2
Pseudo R2

Log likelihood  -432.06242






1191
79.43
0.0000
0.0842

----------------------------------------------------------------------------smokes |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------motheduc | -.0826247
.0465204
-1.78
0.076
-.173803
.0085536
white |
.4611075
.1965245
2.35
0.019
.0759265
.8462886
lfaminc | -.7622559
.3652949
-2.09
0.037
-1.478221
-.046291
v2hat |
.6107298
.3708071
1.65
0.100
-.1160387
1.337498
_cons |
1.98796
.5996374
3.32
0.001
.8126927
3.163228
-----------------------------------------------------------------------------

There is not strong evidence of endogeneity, but the sign of the coefficient on v2hat is what
we expect: unobservables that lead to higher income are positively correlated with unobserved

316

factors affecting birth weight. There is a further problem in that using this test presumes
fatheduc can be omitted from the birth weight equation. Remember, the test can be interpreted
as a test for endogeneity of lfaminc only when we maintain that fatheduc is exogenous.
Because of the potential endogeneity of this is perhaps not a very good example, but it
shows you how to mechanically carry out the tests.
Incidentally, the probit coefficients on lfaminc are very different depending on whether we
treat it as exogenous or not. This is true even if we use the same samples, as the Stata output
below shows. The APE is probably quite different, too. It is hard to know what to do in such
cases (which are all too common).
. probit smokes motheduc white lfaminc if v2hat ! .
Probit regression

Number of obs
LR chi2(3)
Prob  chi2
Pseudo R2

Log likelihood  -433.41656






1191
76.72
0.0000
0.0813

----------------------------------------------------------------------------smokes |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------motheduc | -.1497589
.0225634
-6.64
0.000
-.1939823
-.1055355
white |
.2323285
.137875
1.69
0.092
-.0379015
.5025584
lfaminc | -.1719479
.0687396
-2.50
0.012
-.306675
-.0372207
_cons |
1.133026
.2990124
3.79
0.000
.5469727
1.71908
-----------------------------------------------------------------------------

15.9. a. Let Py  1|x  x, where x 1  1. Then for each i,
ℓ i   y i logx i   1 − y i  log1 − x i ,
which is only well-defined for 0  x i   1.
b. For any possible estimate ̂, the log-likelihood function is well-defined only if
0  x i ̂  1 for all i  1, … , N. Therefore, during the iterations to obtain the MLE, this
condition must be checked. It may be impossible to find an estimate that satisfies these
inequalities for every observation, especially if N is large.

317

c. This follows from the KLIC, and the discussion of Vuong’s model selection statistic in
Section 13.11.2: the true density of y given x – evaluated at the true values, of course –
maximizes the KLIC. Because the MLEs are consistent for the unknown parameters,
asymptotically the true density will produce the highest average log-likelihood function. So,
just as we can use an R-squared to choose among different functional forms for Ey|x, we can
use values of the log-likelihood to choose among different models for Py  1|x when y is
binary.
15.10. a. There are several possibilities. One is to define p̂ i  P̂ y  1|x i  – the estimated
response probabilities – and obtain the square of the correlation between y i and p̂ i . For the
LPM, this is just the usual R-squared. For the general index model, Gx i ̂ is the estimate of
Ey|x i , and so it makes sense to compute an analogous goodness-of-fit measure. This
measure is always between zero and one.
An alternative is to use the sum of squared residuals form. While this produces the same
R-squared measure for the linear model, it does not for nonlinear models.
b. The Stata output below gives the square of the correlation between y i and the fitted
probabilities for the LPM and probit. The LPM R-squared is about .106 and that for probit is
higher, about .115. So probit is preferred based on this goodness-of-fit measure, although the
improvement is not overwhelming. (It is about an 8.5% increase in the R-squared.)
. reg arr86 pcnv avgsen tottime ptime86 inc86 black hispan born60
pcnvsq pt86sq inc86sq
Source |
SS
df
MS
------------------------------------------Model | 57.8976285
11 5.26342077
Residual | 487.918885 2713 .179844779
------------------------------------------Total | 545.816514 2724
.20037317

Number of obs
F( 11, 2713)
Prob  F
R-squared
Adj R-squared
Root MSE








2725
29.27
0.0000
0.1061
0.1025
.42408

----------------------------------------------------------------------------arr86 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval

318

---------------------------------------------------------------------------pcnv |
.075977
.0803402
0.95
0.344
-.0815573
.2335112
avgsen |
.0012998
.0062692
0.21
0.836
-.0109932
.0135927
tottime | -.0022213
.0048287
-0.46
0.646
-.0116897
.007247
ptime86 |
.1321786
.0230021
5.75
0.000
.0870752
.177282
inc86 | -.0018505
.0002737
-6.76
0.000
-.0023872
-.0013139
black |
.1447942
.0233225
6.21
0.000
.0990627
.1905258
hispan |
.0803938
.0204959
3.92
0.000
.0402047
.1205829
born60 | -.0062993
.0170252
-0.37
0.711
-.039683
.0270843
pcnvsq | -.2456865
.0812584
-3.02
0.003
-.4050211
-.0863519
pt86sq | -.0139981
.0020109
-6.96
0.000
-.0179411
-.0100551
inc86sq |
3.31e-06
1.09e-06
3.03
0.002
1.17e-06
5.45e-06
_cons |
.363352
.0175536
20.70
0.000
.3289323
.3977718
----------------------------------------------------------------------------. probit arr86 pcnv avgsen tottime ptime86 inc86 black hispan born60
pcnvsq pt86sq inc86sq
Probit regression

Number of obs
LR chi2(11)
Prob  chi2
Pseudo R2

Log likelihood  -1439.8005






2725
336.77
0.0000
0.1047

----------------------------------------------------------------------------arr86 |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------pcnv |
.2167615
.2604937
0.83
0.405
-.2937968
.7273198
avgsen |
.0139969
.0244972
0.57
0.568
-.0340166
.0620105
tottime | -.0178158
.0199703
-0.89
0.372
-.056957
.0213253
ptime86 |
.7449712
.1438486
5.18
0.000
.4630332
1.026909
inc86 | -.0058786
.0009851
-5.97
0.000
-.0078094
-.0039478
black |
.4368131
.0733798
5.95
0.000
.2929913
.580635
hispan |
.2663945
.067082
3.97
0.000
.1349163
.3978727
born60 | -.0145223
.0566913
-0.26
0.798
-.1256351
.0965905
pcnvsq | -.8570512
.2714575
-3.16
0.002
-1.389098
-.3250042
pt86sq | -.1035031
.0224234
-4.62
0.000
-.1474522
-.059554
inc86sq |
8.75e-06
4.28e-06
2.04
0.041
3.63e-07
.0000171
_cons |
-.337362
.0562665
-6.00
0.000
-.4476423
-.2270817
----------------------------------------------------------------------------Note: 51 failures and 0 successes completely determined.
. predict PHIhat
(option pr assumed; Pr(arr86))
. corr PHIhat arr86
(obs2725)
|
PHIhat
arr86
------------------------------PHIhat |
1.0000
arr86 |
0.3396
1.0000
. di .3396^2
.11532816

15.11. We really need to make two assumptions. The first is a conditional independence

319

assumption: given x i  x i1 , … , x iT , y i1 , … , y iT  are independent. This allows us to write
fy i1 , … , y iT |x i   f 1 y 1 |x i f T y T |x i ,
that is, the joint density (conditional on x i ) is the product of the marginal densities (each
conditional on x i ). The second assumption is a strict exogeneity assumption:
Dy iT |x i   Dy iT |x it , t  1, … , T. When we add the standard assumption for pooled probit –
that Dy iT |x it  follows a probit model – then
T

fy 1 , … , y T |x i  

Gx it  y 1 − Gx it  1−y ,
t

t

t1

and so pooled probit is conditional MLE.
15.12. We can extend the T  2 case used to obtain equation (15.81):
Py i1  1|x i , c i , n i  1  Py i1  1, n i  1|x i , c i /Pn i  1|x i , c i 
 Py i1  1, y i2  0, y i3  0|x i , c i /Py i1  1, y i2  0, y i3  0|x i , c i 
 Py i1  0, y i2  1, y i3  0|x i , c i   Py i1  0, y i2  0, y i3  1|x i , c i 
Now, we just use the conditional independence assumption (across t) and the logistic
functional form:
Py i1  1, y i2  0, y i3  0|x i , c i   x i1   c i 1 − x i2   c i   1 − x i3   c i 
Py i1  0, y i2  1, y i3  0|x i , c i   1 − x i1   c i x i2   c i   1 − x i3   c i 
and
Py i1  0, y i2  0, y i3  1|x i , c i   1 − x i1   c i   1 − x i1   c i x i3   c i .
Now, the term
1/1  expx i1   c i   1  expx i2   c i   1  expx i3   c i 
appears multiplicatively in both the numerator and denominator, and so it disappears.

320

Therefore,
Py i1  1|x i , c i , n i  1  expx i1   c i /expx i1   c i   expx i2   c i   expx i3   c i 
 expx i1 /expx i1   expx i2   expx i3 .
Also,
Py i2  1|x i , c i , n i  1  expx i2 /expx i1   expx i2   expx i3 
and
Py i3  1|x i , c i , n i  1  expx i3 /expx i1   expx i2   expx i3 .
Incidentally, a consistent estimator of  is obtained using only the n i  1 observations and
applying conditional logit, as described in Chapter 16. This approach would be inefficient
because it does not use the n i  2 observations.
A similar argument can be used for the three possible configurations with n i  2, which
leads to the log-likelihood conditional on x i , n i , where c i has dropped out. For example,
Py i1  1, y i2  1|x i , c i , n i  2 

expx i1  x i2 
expx i1  x i2   expx i1  x i3   expx i2  x i3 

15.13. a. If there are no covariates, there is no point in using any method other than a
straight comparison of means – in particular, the difference-in-differences approach described
in Section 6.5.2. The estimated probabilities for the treatment and control groups, both before
and after the policy change, will be identical to the sample proportions regardless of the model
we use.
b. Let d2 be a binary indicator for the second time period, and let dB be an indicator for the
treatment group. Then a probit model to evaluate the treatment effect is
Py  1|x   0   1 d2   2 dB   3 d2  dB  x,

321

where x is a vector of covariates. We would estimate all parameters from a probit of y on
1, d2, dB, d2  dB, and x using all observations. Once we have the estimates, we need to
compute the “difference-in-differences” estimate, which requires either plugging in a value for
x, say x̄ , or averaging the differences across x i . In the former case, we have
̂ PAE ≡ ̂ 0  ̂ 1  ̂ 2  ̂ 3  x̄ ̂ − ̂ 0  ̂ 2  x̄ ̂
− ̂ 0  ̂ 1  x̄ ̂ − ̂ 0  x̄ ̂,
and in the latter we have
N

̂ APE ≡ N

−1

∑̂ 0  ̂ 1  ̂ 2  ̂ 3  x i ̂ − ̂ 0  ̂ 2  x i ̂
i1

− ̂ 0  ̂ 1  x i ̂ − ̂ 0  x i ̂.
Probably ̂ APE is preferred as it averages each of the estimated “treatment effects” – see
Chapter 21 – across all units.
c. We would have to use the delta method to obtain a valid standard error for either ̂ PAE or
̂ APE , with the latter using the extension in Problem 12.17.
15.14. a. First plug in for y 2 from (15.40):
y 1  1z 1  1  y 2 z 2  1  u 1  0  1z 1  1  z 2  v 2 z 2  1  u 1  0
 1z 1  1  z 2 z 2  1  u 1  v 2 z 2  1  0
Given the assumptions, u 1  v 2 z 2  1 has a mean zero normal distribution conditional on z. Its
variance is
Varu 1  v 2 z 2  1 |z  1  2 1 z 2  1   22 z 2  1  2
where  1 Covv 2 , u 1  and  22 Varv 2 . So we can write

322

Py 1  1|z  1 − 



−z 1  1  z 2 z 2  1 
1  2 1 z 2  1   22 z 2  1  2

z 1  1  z 2 z 2  1
1  2 1 z 2  1   22 z 2  1  2

which is a heteroskedastic-probit model (but not with exponential heteroskedasticity in the
latent error).
b. This two-step procedure is inconsistent because the response probability Py 1  1|z
does not have the usual probit form
z 1  1  z 2 z 2  1 .
Under the assumptions given, the first-stage estimation of  2 is not the problem: OLS is
consistent. It is the misspecified functional form in the second stage that causes the problem.
c. A control function method works nicely here. Scaled coefficients are easily estimated
and then  1 and  1 can be recovered using the same approach in Section 15.7.2. In addition,
average partial effects are easily estimated after control function estimation.
Under (15.40), independence, and bivariate normality, we can write u 1 as in equation
(15.42) and then substitute:
y 1  1z 1  1  y 2 z 2  1   1 v 2  e 1  0
e 1 |z, y 2 , v 2 ~ Normal0, 1 −  21 
Following the same argument in Section 15.7.2 we have
Py 1  1|z, y 2 , v 2   z 1  1  y 2 z 2  1   1 v 2 
where  1   1 /1 −  21  1/2 ,  1   1 /1 −  21  1/2 , and  1   1 /1 −  21  1/2 . Therefore, the
following two-step CF method – which extends Procedure 15.1 – consistently estimates the

323

scaled parameters: (i) Regress y 2 on z and obtain the OLS residuals, v̂ 2 . (ii) Run a probit of y 1
on z 1 , y 2 z 2 , and v̂ 2 .
Letting  1   ′1 ,  ′1  ′ and  1   1 /1 −  21 , we use exactly the same unscaling of the
parameters as before. Namely,
 1   1 /1   21  22  1/2
where  22  Varv 2 . The estimator in equation (15.45) can still be used.
The approach to estimating the APEs follows directly from the estimator of the average
structural function in equation (15.47). Allowing for the interactions,
n

ASFz 1 , y 2   N

−1

∑ z 1 ̂ 1  y 2 z 2 ̂ 1  ̂ 1 v̂ i2 .
i1

As usual, we can take derivatives or changes with respect to the elements of z 1 , y 2  to obtain
estimated APEs.
15.15. a. This example falls into the situation described below equation (12.41). Namely,
the scores from the two optimization problems are uncorrelated. This follows because the first
problem – OLS regression of y i2 on z i – depends only on the random draws z i , y i2 . In the
second stage, we are estimating a model for fy 1 |y 2 , z. Letting s i  1 ;  2  denote the score for
the second-step MLE – with respect to  1 – s i  1 ;  2  is uncorrelated with any function of
z i , y i2  because Es i  1 ;  2 |z i , y i2   0. (I do not use a separate notation for the true values of
the parameters.)
So that we can apply the results from Section 12.4.2 directly, we set the problem up as a
minimization problem. Then, from the usual score formula for the probit model, we have

324

w i1  2  ′ w i1  2  1 y i1 − w i1  2  1 
s i  1 ;  2   −
w i1  2  1 1 − w i1  2  1 
where w i1  2   x i1 , v i2  2  and v i2  2   y i2 − z i  2 . The expected Hessian (that is, the
expected Jacobian of s i  1 ;  2  with respect to  1 ) has the usual form for binary response with a
correctly specified response probability:
A1  E

w i1  2  ′ w i1  2 w i1  2  1  2
w i1  2  1 1 − w i1  2  1 

Next, we need F  E∇  2 s i  1 ;  2 . Like the Hessian in the usual binary response model,
the Jacobian ∇  2 s i  1 ;  2  is complicated. But its expectation is not. Using the fact that
Ey i1 − w i1  2  1 |z i , y i2   0 it is easy to show
FE

w i1  2  ′ w i1  2  1 
 ∇  2 w i1  2  1 
w i1  2  1 1 − w i1  2  1 

 − 1 E

w i1  2  ′ z i w i1  2  1 
w i1  2  1 1 − w i1  2  1 

Finally, we need a first-order representation for the OLS estimator, ̂ 2 :
N

−1/2
N ̂ 2 −  2   A −1
∑ z ′i v i2  o p 1,
2 N
i1

where A 2 ≡ Ez ′i z i . It follows that the matrix in the middle of the sandwich is
′
D  Vars i  1 ;  2   FA −1
2 z i v i2 
−1 ′
′
 Vars i  1 ;  2   FA −1
2 Varz i v i2 A 2 F
′
 A 1   22 FA −1
2 F

because s i  1 ;  2  and z ′i v i2 are uncorrelated, Vars i  1 ;  2   A 1 by the information matrix
equality, and Ev 2i2 |z i    22 under homoskedasticity for v 2 . (The results that follow do not rely
in any crucial way on Ev 2i2 |z i    22 ; we could just drop that and use the more general

325

formula.) Therefore,
−1 ′
−1
2
Avar N ̂ 1 −  1   A −1
1 A 1   2 FA 2 F A 1
−1 ′ −1
2 −1
 A −1
1   2 A 1 FA 2 F A 1 .

It is easy to construct consistent estimators of each part using sample averages and plugging in
the consistent estimators.
b. If we ignore estimation of  2 we act as if Avar N ̂ 1 −  1  is just A −1
1 , the inverse of
the information matrix from the second stage problem. But the correct matrix differs from A −1
1
−1 ′ −1
by  22 A −1
1 FA 2 F A 1 , which is positive semi-definite (and usual positive definite if  1 ≠ 0.

c. In Problem 12.17 we can take gw i ,   w i1  2  1  1 , but we have to be careful in
choosing the “score” with respect to . The same argument as in Problem 12.17 gives us
N

−1/2

N

N

i1

i1

∑ gwi , ̂  N −1/2 ∑ gwi ,   G N ̂ −   o p 1

where G ≡ E∇  gw i , . For this application,
∇  gw i ,   w i1  2  1 I K 1 1  w i1  2  1  1 w i1  2 |w i1  2  1  ′1 ∇  2 w i1  2  ′ 
where K 1 is the dimension of x 1 and ∇  2 w i1  2  ′  0|−z ′i . To get a representation for
N ̂ −  we stack the first-order representations obtained in part a:
N

N ̂ −   N −1/2 ∑

−1 ′
−A −1
1 s i  1 ;  2   FA 2 z i v i2 
′
A −1
2 z i v i2

i1

 o p 1

N

≡ N −1/2 ∑ e i   o p 1.
i1

Then, from Problem 12.17,
C  Avar N ̂ 1 −  1   Vargw i ,  −  1 − Ge i 

326

d. A consistent estimator of the asymptotic variance in part c is
N

Ĉ  N −1 ∑ĝ i − ̂ 1 − Ĝê i ĝ i − ̂ 1 − Ĝê i  ′
i1

where ĝ i  ŵ i1 ̂ 1 ̂ 1 ,
N

ĜN

−1

∑ŵ i1 ̂ 1 ̂ 1 I K 1  ŵ i1 ̂ 1 ̂ 1 ŵ i1 |ŵ i1 ̂ 1 ̂ ′1 ∇  wi1 ̂ 2  ′ 
1

2

i1

and
−1

−1

−Â 1 ŝ i  F̂ Â 2 z ′i v̂ i2 

êi 

−1

Â 2 z ′i v̂ i2

.

The score ŝ i and Hessian Â 1 are estimated as usual for a probit model (but with minus signs)
and
N

F̂  N −1 ∑
i1

− ′1 ŵ i1 z i ŵ i1 ̂ 1 
.
ŵ i1 ̂ 1 1 − ŵ i1 ̂ 1 

15.16. a. The response probability is
px  1 − 1  expx −
and, using the chain rule,
 j expx
∂px
  j expx1  expx −−1 
∂x K
1  expx 1
Of course, we get the logit partial effect as a special case when   1.
b. The log likeihood has the usual form for a binary response. Let
Gx,   1 − 1  expx − , so 1 − Gx,   1  expx − . Without making the
distinction between generic and “true” values,

327

ℓ i ,   −1 − y i  log1  expx i   y i log1 − 1  expx i  − .
c. The Stata output is given below. Given the estimated value of , ̂  413, 553, the model
does not seem well determined. (Remember, the logit model imposes   1.) The logit
estimates are included for comparison. The ̂ j are are all the same sign and of roughly the
same statistical significance across the two models. The t statistic for H 0 : log  0 is very
small, about . 02.
. scobit inlf nwifeinc educ exper expersq age kidslt6 kidsge6
Skewed logistic regression

Number of obs
Zero outcomes
Nonzero outcomes

Log likelihood  -399.5222





753
325
428

----------------------------------------------------------------------------inlf |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------nwifeinc | -.0148532
.0056874
-2.61
0.009
-.0260003
-.0037061
educ |
.1512102
.0277346
5.45
0.000
.0968514
.2055689
exper |
.139092
.020757
6.70
0.000
.0984091
.1797749
expersq |
-.002257
.0006377
-3.54
0.000
-.0035069
-.0010072
age | -.0587203
.0089444
-6.57
0.000
-.076251
-.0411897
kidslt6 | -.9977924
.1426425
-7.00
0.000
-1.277367
-.7182183
kidsge6 |
.0257666
.045345
0.57
0.570
-.0631079
.1146411
_cons | -13.09326
666.1339
-0.02
0.984
-1318.692
1292.505
---------------------------------------------------------------------------/lnalpha |
12.93254
666.1327
0.02
0.985
-1292.663
1318.529
---------------------------------------------------------------------------alpha |
413553.1
2.75e08
0
----------------------------------------------------------------------------Likelihood-ratio test of alpha1:
chi2(1) 
4.49
Prob  chi2  0.0342
Note: likelihood-ratio tests are recommended for inference with scobit models
. logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6
Logistic regression

Number of obs
LR chi2(7)
Prob  chi2
Pseudo R2

Log likelihood  -401.76515






753
226.22
0.0000
0.2197

----------------------------------------------------------------------------inlf |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------nwifeinc | -.0213452
.0084214
-2.53
0.011
-.0378509
-.0048394
educ |
.2211704
.0434396
5.09
0.000
.1360303
.3063105
exper |
.2058695
.0320569
6.42
0.000
.1430391
.2686999
expersq | -.0031541
.0010161
-3.10
0.002
-.0051456
-.0011626
age | -.0880244
.014573
-6.04
0.000
-.116587
-.0594618
kidslt6 | -1.443354
.2035849
-7.09
0.000
-1.842373
-1.044335

328

kidsge6 |
.0601122
.0747897
0.80
0.422
-.086473
.2066974
_cons |
.4254524
.8603697
0.49
0.621
-1.260841
2.111746
-----------------------------------------------------------------------------

d. The likelihood ratio statistic for H 0 :   1, reported in the Stata output, is 4. 49 with
p−value . 034. So this statistic rejects the logit model, although it is not an overwhelming
rejection. Its p−value is certainly much smaller than the Wald test (t test) for H 0 : log  0.
e. Given the bizarre value for ̂ and the modest gain in fit, the skewed logit model does not
seem worth the effort. Plus, the Stata output below shows that the correlations of the fitted
probabilities and inlf are very similar across the two models (. 5196 for skewed logit, . 5179 for
logit). The average partial effects are similar, too. For nwifeinc, the APE for skewed logit is
about −. 0041 for skewed logit and about −. 0038 for logit. For kidslt6, the APEs are −. 274
(skewed logit) and −. 258 (logit). It is likely these differences can be attributed to sampling
error.
. qui scobit inlf nwifeinc educ exper expersq age kidslt6 kidsge6
. predict phat_skewlog
(option pr assumed; Pr(inlf))
. predict xbh_sklog, xb
. gen scale_sklog  e(alpha)*exp(xbh_sklog)/((1  exp(xbh_sklog))^(1  e(alpha
. sum scale_sklog
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------scale_sklog |
753
.2741413
.0891063
.0098302
.3678786
. predict phat_skewlog
(option pr assumed; Pr(inlf))
. qui logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6
. predict phat_log
(option pr assumed; Pr(inlf))
. predict xbh_log, xb
. gen scale_log  exp(xbh_log)/((1  exp(xbh_log))^2 )
. sum scale_log

329

Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------scale_log |
753
.1785796
.0617942
.0085973
.25
. corr phat_skewlog inlf
(obs753)
| phat_s~g
inlf
------------------------------phat_skewlog |
1.0000
inlf |
0.5196
1.0000
. corr phat_log inlf
(obs753)
| phat_log
inlf
------------------------------phat_log |
1.0000
inlf |
0.5179
1.0000
. di .2741413* (-.0148532)
-.00407188
. di .1785796*(-.0213452)
-.00381182
. di .2741413* (-.9977924)
-.27353611
. di .1785796*(-1.443354)
-.25775358

15.17. a. We obtain the joint density by the product rule, since we have independence
conditional on x, c:
fy 1 , … , y G |x, c;  o   f 1 y 1 |x, c;  1o f 2 y 1 |x, c;  2o f G y G |x, c;  Go .
b. The density of y 1 , … , y G  given x is obtained by integrating out with respect to the
distribution of c given x:
gy 1 , … , y G |x;  o  



 −

G

 f g y g |x, c;  go  hc|x;  o dc,
g1

where c is a dummy argument of integration. Because c appears in each Dy g |x, c, y 1 , … , y G
are dependent without conditioning on c.
c. The log-likelihood for each i is

330

log



 −

G

 f g y ig |x i , c;  g  hc|x i ; dc .
g1

As expected, this depends only on the observed data, x i , y i1 , … , y iG , and the unknown
parameters.
15.18. a. The probability is the same as if we assume (15.73), that is,
Py it  1|x i , a i     x it   x̄ i   a i , t  1, 2, … , T.
The fact that a i given x i is heteroskedastic has no bearing on the distribution conditional on
x i , a i . Only when we “integrate out” a i does Da i |x i  matter.
b. Let g t y|x i , a i ;     x it   x̄ i   a i  y t 1 −   x it   x̄ i   a i  1−y t . Then, by
the product and integration rules,


 −

fy 1 , … , y T |x;  

T

 g t y t |x i , a;  ha|x i ; da ,
t1

where h|x i ,  is the Normal0,  2a expx̄ i  density. We get the log-likelihood by plugging in
the y it and taking the natural log. For each i, the log likelihood depends on x i , y i  and the
parameters  and ; a i does not appear.
c. To estimate the APEs we can estimate the average structural function, which in this case
is
ASFx o   E c i   x o   c i 
 E x i ,a i    x o   x̄ i   a i 
 E x i E  x o   x̄ i   a i |x i 
To compute E  x o   x̄ i   a i |x̄ i  we use a similar trick as before. It is the same as
computing
E1  x o   x̄ i   a i  u it  0|x i 

331

where
a i  u it |x i ~ Normal0, 1   2a expx̄ i .
because u it is independent of a i , x i  with a standard normal distribution. Now
E1  x o   x̄ i   a i  u it  0|x i   Pa i  u it  −  x o   x̄ i |x i 
P

−  x o   x̄ i 
a i  u it

xi
1   2a expx̄ i  1/2
1   2a expx̄ i  1/2



  x o   x̄ i 
1   2a expx̄ i  1/2

.

(Notice this only depends on x̄ i , not on x i . We could relax that assumption.)
The ASF is therefore,
  x o   x̄ i 
1   2a expx̄ i  1/2

ASFx o   E x̄ i 

and a consistent estimator is obtained by using a sample average and plugging in the maximum
likelihood estimators:
N

o

ASFx   N

−1

∑
i1

̂  x o ̂  x̄ i ̂

1  ̂ 2a expx̄ i ̂ 1/2

.

Now take derivatives and changes with respect to x o (a placeholder).
15.19. a. The Stata output is below. We need to assume first-order dynamics for the usual
standard errors and test statistics to be valid.
. tab year
81 to 87 |
Freq.
Percent
Cum.
----------------------------------------------81 |
1,738
14.29
14.29
82 |
1,738
14.29
28.57
83 |
1,738
14.29
42.86
84 |
1,738
14.29
57.14
85 |
1,738
14.29
71.43
86 |
1,738
14.29
85.71
87 |
1,738
14.29
100.00
-----------------------------------------------

332

Total |

12,166

100.00

. tab black if year  87
1 if black |
Freq.
Percent
Cum.
----------------------------------------------0 |
1,065
61.28
61.28
1 |
673
38.72
100.00
----------------------------------------------Total |
1,738
100.00
. xtset id year
panel variable:
time variable:
delta:

id (strongly balanced)
year, 81 to 87
1 unit

. gen employ_1  l.employ
(1738 missing values generated)
. probit employ employ_1 if black
Probit regression

Number of obs
LR chi2(1)
Prob  chi2
Pseudo R2

Log likelihood  -2248.0349






4038
1091.27
0.0000
0.1953

----------------------------------------------------------------------------employ |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------employ_1 |
1.389433
.0437182
31.78
0.000
1.303747
1.475119
_cons | -.5396127
.0281709
-19.15
0.000
-.5948268
-.4843987
-----------------------------------------------------------------------------

b. After estimating the previous model, the Stata calculations are below. The difference in
employment probabilities this year, based on employment status last year, is about . 508.
. di normal(_b[_cons])
.29473206
. di normal(_b[_cons]
.80228758

 _b[employ_1])

. di normal(_b[_cons]
.50755552

 _b[employ_1]) - normal(_b[_cons])

c. With year dummies, the story is very similar. The estimated state dependence for 1987 is
about . 472.
. probit employ employ_1 y83-y87 if black
Probit regression

Number of obs
LR chi2(6)
Prob  chi2
Pseudo R2

Log likelihood  -2215.1795

333






4038
1156.98
0.0000
0.2071

----------------------------------------------------------------------------employ |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------employ_1 |
1.321349
.0453568
29.13
0.000
1.232452
1.410247
y83 |
.3427664
.0749844
4.57
0.000
.1957997
.4897331
y84 |
.4586078
.0755742
6.07
0.000
.3104851
.6067305
y85 |
.5200576
.0767271
6.78
0.000
.3696753
.6704399
y86 |
.3936516
.0774704
5.08
0.000
.2418125
.5454907
y87 |
.5292136
.0773031
6.85
0.000
.3777023
.680725
_cons | -.8850412
.0556042
-15.92
0.000
-.9940233
-.776059
----------------------------------------------------------------------------. di normal(_b[_cons]
.4718734

 _b[y87]  _b[employ_1]) - normal(_b[_cons]  _b[y87]

d. Below gives one way in Stata to estimate the dynamic unobserved effects model.
Compared with not allowing for heterogeneity as in part c, the coefficient on employ −1 has
fallen: from about 1. 321 to about . 899. In addition, the coefficient on the initial condition is
. 566 and it is very statistically significant. But we cannot know how much the amount of state
dependence has changed without computing an average partial effect.
. gen employ81  employ if y81
(10428 missing values generated)
. replace employ81  employ[_n-1] if y82
(1738 real changes made)
. replace employ81  employ[_n-2] if y83
(1738 real changes made)
. replace employ81  employ[_n-3] if y84
(1738 real changes made)
. replace employ81  employ[_n-4] if y85
(1738 real changes made)
. replace employ81  employ[_n-5] if y86
(1738 real changes made)
. replace employ81  employ[_n-6] if y87
(1738 real changes made)
. xtprobit employ employ_1 employ81 y83-y87 if black, re
Random-effects probit regression
Group variable: id

Number of obs
Number of groups




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian

334

4038
673
6
6.

Log likelihood

Wald chi2(7)
Prob  chi2

 -2176.3738




677.59
0.0000

----------------------------------------------------------------------------employ |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------employ_1 |
.8987806
.0677058
13.27
0.000
.7660797
1.031482
employ81 |
.5662897
.0884941
6.40
0.000
.3928444
.739735
y83 |
.4339911
.0804064
5.40
0.000
.2763974
.5915847
y84 |
.6563094
.0841199
7.80
0.000
.4914374
.8211814
y85 |
.7919805
.0887167
8.93
0.000
.618099
.965862
y86 |
.6896344
.090158
7.65
0.000
.5129279
.8663409
y87 |
.8382018
.091054
9.21
0.000
.6597393
1.016664
_cons | -1.005103
.0660945
-15.21
0.000
-1.134646
-.8755602
---------------------------------------------------------------------------/lnsig2u | -1.178731
.1995372
-1.569817
-.7876454
---------------------------------------------------------------------------sigma_u |
.5546791
.0553396
.4561615
.6744736
rho |
.2352804
.0359014
.1722425
.3126745
----------------------------------------------------------------------------Likelihood-ratio test of rho0: chibar2(01) 
47.90 Prob  chibar2  0.000

e. There is still plenty of evidence of state dependence because of the very statistically
significant coefficient on employ −1 (t  13. 27). The coefficient still seems quite large, but we
still need to compute the APE.
The positive coefficient on employ 81 shows that that c i and employ i,81 are positively
correlated. The estimate of  2a is . 5546791 2 , or ̂ 2a ≈. 308.
f. The average state dependence, where we average out the distribution of c i , is estimated
as
N

N

−1

∑
i1



̂  ̂ 87  ̂  ̂ y i0 

1/2
1  ̂ 2a 

−

̂  ̂ 87  ̂ y i0 

1/2
1  ̂ 2a 

where ̂ is the coefficient on y −1  employ −1 , y i0  employ i,1981 , and, in this case, the
averaging is done across the black men in the sample. The Stata calculations below (done after
the calculations in part d) show the estimated state dependence is about .283, which is much
lower than the estimate of . 472 from part c (where we ignored heterogeneity). Bootstrapping is
a convenient way to obtain a standard error, as was done in Example 15.6.
. gen stdep  normal((_b[_cons]  _b[employ_1]  _b[employ81]*employ81

335

 _b[y87])/sqrt(1  e(sigma_u)^2))
- normal((_b[_cons]  _b[employ81]*employ81  _b[y87])
/sqrt(1  e(sigma_u)^2)) if black & y87
(11493 missing values generated)
. sum stdep
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------stdep |
673
.283111
.0257298
.2353074
.2969392

336

15.20. (Bonus Question) Estimate the CRE probit model report in Table 15.3 using the
generalized estimation equation (GEE) approach described in Section 12.9.2, using an
exchangeable correlation structure.
a. How do the point estimates compare with the pooled probit estimates in Column (3) of
Table 15.3?
b. Does it appear that the GEE approach improves on the efficiency of pooled probit?
Explain.
Solution:
a. The Stata output for pooled probit and GEE is given below. The pooled probit estimates
replicate the numbers in Table 15.3.
. probit lfp kids lhinc kidsbar lhincbar educ black age agesq per2-per5,
cluster(id)
Iteration
Iteration
Iteration
Iteration

0:
1:
2:
3:

log
log
log
log

pseudolikelihood
pseudolikelihood
pseudolikelihood
pseudolikelihood






-17709.021
-16521.245
-16516.437
-16516.436

Probit regression

Number of obs
Wald chi2(12)
Prob  chi2
Pseudo R2

Log pseudolikelihood  -16516.436






28315
538.09
0.0000
0.0673

(Std. Err. adjusted for 5663 clusters in id
----------------------------------------------------------------------------|
Robust
lfp |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------kids | -.1173749
.0269743
-4.35
0.000
-.1702435
-.0645064
lhinc | -.0288098
.014344
-2.01
0.045
-.0569234
-.0006961
kidsbar | -.0856913
.0311857
-2.75
0.006
-.146814
-.0245685
lhincbar | -.2501781
.0352907
-7.09
0.000
-.3193466
-.1810097
educ |
.0841338
.0067302
12.50
0.000
.0709428
.0973248
black |
.2030668
.0663945
3.06
0.002
.0729359
.3331976
age |
.1516424
.0124831
12.15
0.000
.127176
.1761089
agesq | -.0020672
.0001553
-13.31
0.000
-.0023717
-.0017628
per2 | -.0135701
.0103752
-1.31
0.191
-.0339051
.0067648
per3 | -.0331991
.0127197
-2.61
0.009
-.0581293
-.008269
per4 | -.0390317
.0136244
-2.86
0.004
-.0657351
-.0123284
per5 | -.0552425
.0146067
-3.78
0.000
-.0838711
-.0266139
_cons | -.7260562
.2836985
-2.56
0.010
-1.282095
-.1700173
-----------------------------------------------------------------------------

337

. xtgee lfp kids lhinc kidsbar lhincbar educ black age agesq per2-per5,
fam(binomial) link(probit) corr(exch) robust
GEE population-averaged model
Group variable:
id
Link:
probit
Family:
binomial
Correlation:
exchangeable
Scale parameter:

1

Number of obs
Number of groups
Obs per group: min
avg
max
Wald chi2(12)
Prob  chi2









28315
5663
5.
536.66
0.0000

(Std. Err. adjusted for clustering on id
----------------------------------------------------------------------------|
Semirobust
lfp |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------kids | -.1125361
.0281366
-4.00
0.000
-.1676828
-.0573894
lhinc | -.0276543
.014799
-1.87
0.062
-.0566598
.0013511
kidsbar | -.0892543
.0323884
-2.76
0.006
-.1527344
-.0257742
lhincbar |
-.252001
.0360377
-6.99
0.000
-.3226337
-.1813684
educ |
.0841304
.0066834
12.59
0.000
.0710312
.0972296
black |
.205611
.0668779
3.07
0.002
.0745328
.3366893
age |
.152809
.0125434
12.18
0.000
.1282245
.1773936
agesq | -.0020781
.0001565
-13.28
0.000
-.0023847
-.0017714
per2 | -.0134259
.0103607
-1.30
0.195
-.0337324
.0068807
per3 | -.0329993
.0126967
-2.60
0.009
-.0578845
-.0081141
per4 | -.0384026
.0136212
-2.82
0.005
-.0650997
-.0117056
per5 |
-.05451
.0146135
-3.73
0.000
-.083152
-.025868
_cons | -.7532503
.285216
-2.64
0.008
-1.312263
-.1942373
-----------------------------------------------------------------------------

b. Surprisingly, and disappointingly, the GEE approach does not improve the precision of
the estimators. In fact, the robust standard errors for GEE are actually slightly above those for
pooled probit. This finding is particular puzzling because there is substantial serial correlation
in the standardized residuals, written generally after pooled probit estimation as
r̂ it ≡

̂ w
y it − x it ̂  
̄ i ̂
,
̂ w
̂ w
x it ̂  
̄ i ̂1 − x it ̂  
̄ i ̂ 1/2

where w
̄ i is the time average of variables that change across i and t (kids it and lhinc it in this
application). The first-order correlation in the r̂ it : t  2, . . . , T; i  1, . . . , N is about . 83.
. qui probit lfp kids lhinc kidsbar lhincbar educ black age agesq per2-per5
. predict phat
(option pr assumed; Pr(lfp))

338

. gen rh  (lfp - phat)/sqrt(phat*(1 - phat))
. gen rh_1  l.rh
(5663 missing values generated)
. corr rh rh_1
(obs22652)
|
rh
rh_1
------------------------------rh |
1.0000
rh_1 |
0.8315
1.0000

c. This is not an answer to a particular question, but serves as an errata for the estimates on
Column (4) of Table 15.3. Those estimates were obtained using a version of Stata earlier than
9.0. Using Stata 11.0, a higher value of the log likelihood is found, and the point estimates are
different. Note that the estimated value of , which is the pairwise correlation between any of
the two composite errors a i  e it , is very large: . 95. The estimated scale factor for the
coefficients, about . 233, is substantially below that in Table 15.3, but the coefficients reported
below are substantially higher. I have deleted the details of the numerical iterations.
. xtprobit lfp kids lhinc kidsbar lhincbar educ black age agesq per2-per5, re
Random-effects probit regression
Group variable: id

Number of obs
Number of groups

Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian

Log likelihood




Wald chi2(12)
Prob  chi2

 -8609.9002




28315
5663
5
5.
623.40
0.0000

----------------------------------------------------------------------------lfp |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------kids | -.3970102
.0701298
-5.66
0.000
-.534462
-.2595584
lhinc | -.1003399
.0469979
-2.13
0.033
-.1924541
-.0082258
kidsbar | -.4085664
.0898875
-4.55
0.000
-.5847428
-.2323901
lhincbar | -.8941069
.1199703
-7.45
0.000
-1.129244
-.6589695
educ |
.3189079
.024327
13.11
0.000
.2712279
.366588
black |
.6388784
.1903525
3.36
0.001
.2657945
1.011962
age |
.7282057
.0445623
16.34
0.000
.6408651
.8155462
agesq | -.0098358
.0005747
-17.11
0.000
-.0109623
-.0087094
per2 | -.0451653
.0499429
-0.90
0.366
-.1430516
.052721
per3 | -.1247056
.0501522
-2.49
0.013
-.2230022
-.026409
per4 | -.1356834
.0500679
-2.71
0.007
-.2338147
-.0375522
per5 |
-.200357
.049539
-4.04
0.000
-.2974515
-.1032624
_cons | -5.359375
1.000514
-5.36
0.000
-7.320346
-3.398404

339

---------------------------------------------------------------------------/lnsig2u |
2.947234
.0435842
2.861811
3.032657
---------------------------------------------------------------------------sigma_u |
4.364995
.0951224
4.182484
4.55547
rho |
.9501326
.002065
.945926
.9540279
----------------------------------------------------------------------------Likelihood-ratio test of rho0: chibar2(01)  1.6e04 Prob  chibar2  0.000
. * Scale factor for coefficients:
. di 1/sqrt(1  e(sigma_u)^2)
.22331011

340

Solutions to Chapter 16 Problems
16.1. a. The Stata ouput below contains the estimates for 1981 and, for completeness, 1987.
Certainly some magnitudes are fairly different. For example, education has a much larger
effect in the latter time period. Also, the effect of experience on the log-odds ratios are quite
different.
. mlogit status educ exper expersq black if y81, base(0)
Multinomial logistic regression

Number of obs
LR chi2(8)
Prob  chi2
Pseudo R2

Log likelihood  -1502.9396






1737
720.39
0.0000
0.1933

----------------------------------------------------------------------------status |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------0
| (base outcome)
---------------------------------------------------------------------------1
|
educ |
-.47558
.0466559
-10.19
0.000
-.5670238
-.3841361
exper |
3.016025
.4513224
6.68
0.000
2.131449
3.900601
expersq | -.5953032
.2690175
-2.21
0.027
-1.122568
-.0680386
black |
.8649358
.1302512
6.64
0.000
.6096481
1.120224
_cons |
4.138761
.5276112
7.84
0.000
3.104662
5.17286
---------------------------------------------------------------------------2
|
educ | -.1019564
.0495931
-2.06
0.040
-.1991571
-.0047558
exper |
4.101794
.4359451
9.41
0.000
3.247357
4.956231
expersq | -.7069626
.2628842
-2.69
0.007
-1.222206
-.1917191
black |
.0208189
.1436123
0.14
0.885
-.2606561
.3022938
_cons | -.0313456
.5828582
-0.05
0.957
-1.173727
1.111035
----------------------------------------------------------------------------. mlogit status educ exper expersq black if y87, base(0)
Multinomial logistic regression

Number of obs
LR chi2(8)
Prob  chi2
Pseudo R2

Log likelihood  -907.85723






1717
583.72
0.0000
0.2433

----------------------------------------------------------------------------status |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------0
| (base outcome)
---------------------------------------------------------------------------1
|
educ | -.6736313
.0698999
-9.64
0.000
-.8106325
-.53663
exper | -.1062149
.173282
-0.61
0.540
-.4458414
.2334116
expersq | -.0125152
.0252291
-0.50
0.620
-.0619633
.036933
black |
.8130166
.3027231
2.69
0.007
.2196902
1.406343

341

_cons |
10.27787
1.133336
9.07
0.000
8.056578
12.49917
---------------------------------------------------------------------------2
|
educ | -.3146573
.0651096
-4.83
0.000
-.4422699
-.1870448
exper |
.8487367
.1569856
5.41
0.000
.5410507
1.156423
expersq | -.0773003
.0229217
-3.37
0.001
-.1222261
-.0323746
black |
.3113612
.2815339
1.11
0.269
-.240435
.8631574
_cons |
5.543798
1.086409
5.10
0.000
3.414475
7.673121
-----------------------------------------------------------------------------

b. Just adding year dummies is probably not sufficient, given the findings in part a, but the
results are below. Because the model is static and we have panel data, we should use inference
robust to arbitrary serial dependence. In this application, the robust standard errors are
typically larger but the difference is not huge.
. mlogit status educ exper expersq black y82-y87, base(0)
Multinomial logistic regression

Number of obs
LR chi2(20)
Prob  chi2
Pseudo R2

Log likelihood  -8842.6383






12108
6409.72
0.0000
0.2660

----------------------------------------------------------------------------status |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------0
| (base outcome)
---------------------------------------------------------------------------1
|
educ | -.5473739
.0189537
-28.88
0.000
-.5845225
-.5102253
exper |
.769957
.0633149
12.16
0.000
.6458621
.8940519
expersq | -.1153749
.0107134
-10.77
0.000
-.1363729
-.094377
black |
.8773806
.0656223
13.37
0.000
.7487633
1.005998
y82 |
.9871298
.0928663
10.63
0.000
.8051152
1.169144
y83 |
1.383591
.1035337
13.36
0.000
1.180669
1.586514
y84 |
1.587213
.115548
13.74
0.000
1.360743
1.813683
y85 |
2.052594
.1307157
15.70
0.000
1.796396
2.308792
y86 |
2.652847
.1513588
17.53
0.000
2.356189
2.949505
y87 |
2.727265
.1701085
16.03
0.000
2.393858
3.060671
_cons |
5.151552
.2282352
22.57
0.000
4.704219
5.598885
---------------------------------------------------------------------------2
|
educ | -.2555556
.0182414
-14.01
0.000
-.291308
-.2198032
exper |
1.823821
.058522
31.16
0.000
1.70912
1.938522
expersq |
-.195654
.0095781
-20.43
0.000
-.2144267
-.1768813
black |
.33846
.0649312
5.21
0.000
.2111972
.4657227
y82 |
.5624964
.0936881
6.00
0.000
.3788712
.7461217
y83 |
1.225732
.0998516
12.28
0.000
1.030027
1.421438
y84 |
1.42652
.1095939
13.02
0.000
1.21172
1.64132
y85 |
1.662994
.1243071
13.38
0.000
1.419357
1.906632
y86 |
2.029585
.1447257
14.02
0.000
1.745928
2.313242
y87 |
1.995639
.1622294
12.30
0.000
1.677675
2.313603
_cons |
1.858323
.225749
8.23
0.000
1.415863
2.300783
-----------------------------------------------------------------------------

342

. mlogit status educ exper expersq black y82-y87, base(0) cluster(id)
Multinomial logistic regression

Number of obs
Wald chi2(20)
Prob  chi2
Pseudo R2

Log pseudolikelihood  -8842.6383






12108
2742.09
0.0000
0.2660

(Std. Err. adjusted for 1738 clusters in id
----------------------------------------------------------------------------|
Robust
status |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------0
| (base outcome)
---------------------------------------------------------------------------1
|
educ | -.5473739
.0200999
-27.23
0.000
-.586769
-.5079789
exper |
.769957
.0776371
9.92
0.000
.617791
.922123
expersq | -.1153749
.0106075
-10.88
0.000
-.1361653
-.0945846
black |
.8773806
.0855443
10.26
0.000
.7097169
1.045044
y82 |
.9871298
.0760747
12.98
0.000
.8380261
1.136234
y83 |
1.383591
.0888752
15.57
0.000
1.209399
1.557784
y84 |
1.587213
.1050477
15.11
0.000
1.381323
1.793103
y85 |
2.052594
.1275644
16.09
0.000
1.802572
2.302615
y86 |
2.652847
.1526831
17.37
0.000
2.353593
2.9521
y87 |
2.727265
.1666166
16.37
0.000
2.400702
3.053827
_cons |
5.151552
.2523957
20.41
0.000
4.656866
5.646238
---------------------------------------------------------------------------2
|
educ | -.2555556
.0177679
-14.38
0.000
-.29038
-.2207312
exper |
1.823821
.0731396
24.94
0.000
1.68047
1.967172
expersq |
-.195654
.010131
-19.31
0.000
-.2155104
-.1757976
black |
.33846
.0783575
4.32
0.000
.1848821
.4920378
y82 |
.5624964
.0796845
7.06
0.000
.4063177
.7186751
y83 |
1.225732
.0897086
13.66
0.000
1.049907
1.401558
y84 |
1.42652
.1027116
13.89
0.000
1.225209
1.627831
y85 |
1.662994
.124454
13.36
0.000
1.419069
1.90692
y86 |
2.029585
.1526669
13.29
0.000
1.730363
2.328807
y87 |
1.995639
.1636634
12.19
0.000
1.674865
2.316414
_cons |
1.858323
.2257666
8.23
0.000
1.415829
2.300817
-----------------------------------------------------------------------------

c. The time dummies have very large t statistics, and the robust joint test gives a  212 value
of 624. 28, which implies a zero p-value to many decimal places.
d. After obtaining the estimates from part c, the following commands produce the change
in the estimated employment probabilities. It is about . 021 for 1981, and about . 058 for 1987.
. di exp([2]_cons  [2]educ*16  [2]exper*5  [2]expersq*25  [2]black)
/(1  exp([1]_cons  [1]educ*16  [1]exper*5  [1]expersq*25  [1]black)
 exp([2]_cons  [2]educ*16  [2]exper*5  [2]expersq*25  [2]black))
.89820453

343

. di exp([2]_cons  [2]educ*12  [2]exper*5  [2]expersq*25  [2]black)
/(1  exp([1]_cons  [1]educ*12  [1]exper*5  [1]expersq*25  [1]black)
 exp([2]_cons  [2]educ*12  [2]exper*5  [2]expersq*25  [2]black))
.91903414
. di .91903414 - .89820453
.02082961
. di exp([2]_cons  [2]educ*12  [2]exper*5  [2]expersq*25  [2]black  [2]y87
/(1  exp([1]_cons  [1]educ*12  [1]exper*5  [1]expersq*25  [1]black
 [1]y87)
 exp([2]_cons  [2]educ*12  [2]exper*5  [2]expersq*25  [2]black
 [2]y8))
.89646574
. di exp([2]_cons  [2]educ*16  [2]exper*5  [2]expersq*25  [2]black  [2]y87
/(1  exp([1]_cons  [1]educ*16  [1]exper*5  [1]expersq*25  [1]black
 [1]y87)
 exp([2]_cons  [2]educ*16  [2]exper*5  [2]expersq*25  [2]black
 [2]y87))
.95454392
. di .95454392 - .89646574
.05807818

16.2. a. The following Stata output contains the linear regression results. Because pctstck is
discrete (taking on the values 0, 50, and 100), it seems likely that heteroskedasticity is present
in a linear model. In fact, the robust standard errors are not very different from the usual ones.
. use pension
. tab pctstck
0mstbnds,5 |
0mixed,100 |
mststcks |
Freq.
Percent
Cum.
----------------------------------------------0 |
78
34.51
34.51
50 |
85
37.61
72.12
100 |
63
27.88
100.00
----------------------------------------------Total |
226
100.00
. reg pctstck choice age educ female black married finc25-finc101 wealth89
prftshr, robust
Linear regression

Number of obs
F( 14,
179)
Prob  F
R-squared
Root MSE







194
2.15
0.0113
0.0998
39.134

----------------------------------------------------------------------------|
Robust

344

pctstck |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------choice |
12.04773
5.994437
2.01
0.046
.2188713
23.87658
age | -1.625967
.8327895
-1.95
0.052
-3.269315
.0173813
educ |
.7538685
1.172328
0.64
0.521
-1.559493
3.06723
female |
1.302856
7.148595
0.18
0.856
-12.80351
15.40922
black |
3.967391
8.974971
0.44
0.659
-13.74297
21.67775
married |
3.303436
8.369616
0.39
0.694
-13.21237
19.81924
finc25 | -18.18567
16.00485
-1.14
0.257
-49.76813
13.39679
finc35 | -3.925374
15.86275
-0.25
0.805
-35.22742
27.37668
finc50 | -8.128784
15.3762
-0.53
0.598
-38.47072
22.21315
finc75 | -17.57921
16.6797
-1.05
0.293
-50.49335
15.33493
finc100 |
-6.74559
16.7482
-0.40
0.688
-39.7949
26.30372
finc101 | -28.34407
16.57814
-1.71
0.089
-61.05781
4.369672
wealth89 | -.0026918
.0114136
-0.24
0.814
-.0252142
.0198307
prftshr |
15.80791
8.107663
1.95
0.053
-.1909844
31.80681
_cons |
134.1161
58.87288
2.28
0.024
17.9419
250.2902
-----------------------------------------------------------------------------

b. With relatively few husband-wife pairs – 23 in this application – we do not expect big
differences in standard errors, and we do not see them. On the key variable, choice, the
cluster-robust standard error is only slightly larger. (Incidentally, this part really should not
come until Chapter 20.)
. reg pctstck choice age educ female black married finc25-finc101 wealth89
prftshr, cluster(id)
Linear regression

Number of obs
F( 14,
170)
Prob  F
R-squared
Root MSE







194
2.12
0.0128
0.0998
39.134

(Std. Err. adjusted for 171 clusters in id
----------------------------------------------------------------------------|
Robust
pctstck |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------choice |
12.04773
6.184085
1.95
0.053
-.1597617
24.25521
age | -1.625967
.8192942
-1.98
0.049
-3.243267
-.0086663
educ |
.7538685
1.1803
0.64
0.524
-1.576064
3.083801
female |
1.302856
7.000538
0.19
0.853
-12.51632
15.12203
black |
3.967391
8.711611
0.46
0.649
-13.22948
21.16426
married |
3.303436
8.624168
0.38
0.702
-13.72082
20.32769
finc25 | -18.18567
16.82939
-1.08
0.281
-51.40716
15.03583
finc35 | -3.925374
16.17574
-0.24
0.809
-35.85656
28.00581
finc50 | -8.128784
15.91447
-0.51
0.610
-39.54421
23.28665
finc75 | -17.57921
17.2789
-1.02
0.310
-51.68804
16.52963
finc100 |
-6.74559
17.24617
-0.39
0.696
-40.78983
27.29865
finc101 | -28.34407
17.10783
-1.66
0.099
-62.1152
5.42707
wealth89 | -.0026918
.0119309
-0.23
0.822
-.0262435
.02086
prftshr |
15.80791
8.356266
1.89
0.060
-.6874979
32.30332
_cons |
134.1161
58.1316
2.31
0.022
19.36333
248.8688

345

----------------------------------------------------------------------------. di _b[_cons]  _b[age]*60  _b[educ]*12  _b[female]  _b[finc50]  _b[wealth89
38.374791
. di _b[_cons]  _b[age]*60  _b[educ]*12  _b[female]  _b[finc50]  _b[wealth89
50.422517

For later use, the predicted pctstck for the person described in the problem, with choice  0
is about 38.37. With choice, it is roughly 50.42.
c. The ordered probit estimates follow, including commands that provide the predictions
for pctstck with and without choice:
. oprobit pctstck choice age educ female black married finc25-finc101 wealth89
prftshr
Iteration
Iteration
Iteration
Iteration

0:
1:
2:
3:

log
log
log
log

likelihood
likelihood
likelihood
likelihood

 -212.37031
 -202.0094
 -201.9865
 -201.9865

Ordered probit regression
Log likelihood 

Number of obs
LR chi2(14)
Prob  chi2
Pseudo R2

-201.9865






194
20.77
0.1077
0.0489

----------------------------------------------------------------------------pctstck |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------choice |
.371171
.1841121
2.02
0.044
.010318
.7320241
age | -.0500516
.0226063
-2.21
0.027
-.0943591
-.005744
educ |
.0261382
.0352561
0.74
0.458
-.0429626
.0952389
female |
.0455642
.206004
0.22
0.825
-.3581963
.4493246
black |
.0933923
.2820403
0.33
0.741
-.4593965
.6461811
married |
.0935981
.2332114
0.40
0.688
-.3634878
.550684
finc25 | -.5784299
.423162
-1.37
0.172
-1.407812
.2509524
finc35 | -.1346721
.4305242
-0.31
0.754
-.9784841
.7091399
finc50 | -.2620401
.4265936
-0.61
0.539
-1.098148
.5740681
finc75 | -.5662312
.4780035
-1.18
0.236
-1.503101
.3706385
finc100 | -.2278963
.4685942
-0.49
0.627
-1.146324
.6905316
finc101 | -.8641109
.5291111
-1.63
0.102
-1.90115
.1729279
wealth89 | -.0000956
.0003737
-0.26
0.798
-.0008279
.0006368
prftshr |
.4817182
.2161233
2.23
0.026
.0581243
.905312
---------------------------------------------------------------------------/cut1 | -3.087373
1.623765
-6.269894
.0951479
/cut2 | -2.053553
1.618611
-5.225972
1.118865
----------------------------------------------------------------------------. di b[age]*60  _b[educ]*12  _b[female]  _b[finc50]  _b[wealth89]*150
-2.9202491
. di normal(_b[/cut2]  2.9202491) - normal(_b[/cut1] 2.9202491)
.37330935

346

. di 1 - normal(_b[/cut2]  2.9202491)
.19305438
. di 50*.37330935  100*.19305438
37.970906
. di _b[age]*60  _b[educ]*12  _b[female]  _b[finc50]  _b[wealth89]*150
 _b[choice]
-2.5490781
. di normal(_b[/cut2]  2.5490781) - normal(_b[/cut1]  2.5490781)
.39469838
. di 1 - normal(_b[/cut2]  2.5490781)
.31011489
. di 50*.39469838  100*.31011489
50.746408
. di 50.75 - 37.97
12.78

Using ordered probit, the effect of having choice for this person is about 12.8 percentage
points more invested in the stock market, which is pretty similar to the 12.1 points obtained
with the linear model.
d. We can compute an R-squared for the ordered probit model by using the squared
correlation between the predicted pctstck i and the actual. The following Stata session does this,
after using the oprobit command. The squared correlation for ordered probit is about . 097,
which is actually slightly below the linear model R-squared, . 098. The correlation between the
fitted values for the linear and OP models is very high: . 998.
. qui oprobit pctstck choice age educ female black married finc25-finc101 wealth89
. predict p1hat p2hat p3hat
(option pr assumed; predicted probabilities)
(32 missing values generated)
. sum p1hat p2hat p3hat
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------p1hat |
194
.331408
.1327901
.0685269
.8053644
p2hat |
194
.3701685
.0321855
.1655734
.3947809
p3hat |
194
.2984236
.1245914
.0290621
.6747374
. gen pctstck_op  50*p2hat  100*p3hat

347

(32 missing values generated)
. corr pctstck pctstck_op
(obs194)
| pctstck pctstc~p
------------------------------pctstck |
1.0000
pctstck_op |
0.3119
1.0000
. di .312^2
.097344
. qui reg pctstck choice age educ female black married finc25-finc101 wealth89
. predict pctstck_lin
(option xb assumed; fitted values)
(32 missing values generated)
. corr pctstck_lin pctstck_op
(obs194)
| pctstc~n pctstc~p
------------------------------pctstck_lin |
1.0000
pctstck_op |
0.9980
1.0000

16.3. a. We can derive the response probabilities from the latent variable formulation in
(16.21) and the rule in (16.22).
exp−x 1 y ∗  exp−x 1 x  exp−x 1 e
 exp−x 1 x  a
where
a|x ~ Normal0, 1.
Now  j  y ∗ ≤  j1 if and only if exp−x 1  j  exp−x 1 y ∗ ≤ exp−x 1  j1 , and so
Py  j|x  Pexp−x 1  j  exp−x 1 x  a ≤ exp−x 1  j1 |x
 Pexp−x 1  j − exp−x 1 x  a ≤ exp−x 1  j1 − exp−x 1 x|x
 exp−x 1  j1 − x − exp−x 1  j − x.
A similar argument holds at j  0 and j  J. Therefore, as described in the text, the response
probabilities for the heteroskedastic ordered probit are of the same form as usual ordered probit

348

but with  j − x everywhere replaced with exp−x 1  j − x.
b. We can obtain a useful VAT by applying the score statistic – just as in the binary probit
case. The score of the log likelihood with respect to  ′ ,  ′  ′ , evaluated at   0, is easily seen
to just be the usual score for ordered probit. For 0  j  J, the score of the response
probability with respect to  evaluated at   0 is
− 1y i  j

x ′i1  j1 − x i  j1 − x i  −  j − x i  j − x i 
.
 j1 − x i  −  j − x i 

For j  0 and j  J we have
− 1y i  0

x ′i1  1 − x i  1 − x i 
 1 − x i 

1y i  J

x ′i1  J − x i  J − x i 
1 −  J − x i 

It is easily seen that these are identical to the scores that would be obtained by adding
− x i1  j − x i 
as a set of explanatory variables to the usual OP model and testing their joint significance. In
practice, we would replace the  j and  with the MLEs from the original OP problem.
d. The ASF can be written as
ASFx  E e i 1 1 − x  e i ≤  2 − x
 P 1 − x  e i ≤  2 − x
 F e  2 − x − F e  1 − x
where F e  is the cdf of e i . We do not know F e because it depends on the distribution of x 1 :
we have specified De i |x i   De i |x i1 , not De i .
e. From iterated expectations we can write
ASFx  E x i1 E1 1 − x  e i ≤  2 − x|x i1 

349

and the conditional expectation is a conditional probability:
E1 1 − x  e i ≤  2 − x|x i1   P 1 − x  e i ≤  2 − x|x i1 
 Pexp−x i1  1 − x  a i ≤ exp−x i1  2 − x|x i1 
 exp−x i1  2 − x − exp−x i1  1 − x.
Therefore,
ASFx  E x i1 exp−x i1  2 − x − exp−x i1  1 − x.
By the law of large numbers, a consistent estimator is
N

N

−1

∑exp−x i1  2 − x

− exp−x i1  1 − x

i1

and, by Lemma 12.1, consistency is preserved if we insert the (consistent) MLES:
N

ASFx  N −1 ∑exp−x i1 ̂̂ 2 − x̂ − exp−x i1 ̂̂ 1 − x̂.
i1

The APEs are estimated by taking derivatives or changes with respect to elements of x in
ASFx.
16.4. a. The results of the ordered probit estimation using invest as the response variable are
given below. Every statistic is identical to when pctstck is used as the response variable. This is
as it should be, as only the order of the outcomes matter – not the magnitudes.
. gen invest  0 if pctstck  0
(148 missing values generated)
. replace invest  1 if pctstck  50
(85 real changes made)
. replace invest  2 if pctstck  100
(63 real changes made)
. oprobit invest choice age educ female black married finc25-finc101 wealth89
Ordered probit regression

Number of obs
LR chi2(14)
Prob  chi2

350





194
20.77
0.1077

Log likelihood 

-201.9865

Pseudo R2



0.0489

----------------------------------------------------------------------------invest |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------choice |
.371171
.1841121
2.02
0.044
.010318
.7320241
age | -.0500516
.0226063
-2.21
0.027
-.0943591
-.005744
educ |
.0261382
.0352561
0.74
0.458
-.0429626
.0952389
female |
.0455642
.206004
0.22
0.825
-.3581963
.4493246
black |
.0933923
.2820403
0.33
0.741
-.4593965
.6461811
married |
.0935981
.2332114
0.40
0.688
-.3634878
.550684
finc25 | -.5784299
.423162
-1.37
0.172
-1.407812
.2509524
finc35 | -.1346721
.4305242
-0.31
0.754
-.9784841
.7091399
finc50 | -.2620401
.4265936
-0.61
0.539
-1.098148
.5740681
finc75 | -.5662312
.4780035
-1.18
0.236
-1.503101
.3706385
finc100 | -.2278963
.4685942
-0.49
0.627
-1.146324
.6905316
finc101 | -.8641109
.5291111
-1.63
0.102
-1.90115
.1729279
wealth89 | -.0000956
.0003737
-0.26
0.798
-.0008279
.0006368
prftshr |
.4817182
.2161233
2.23
0.026
.0581243
.905312
---------------------------------------------------------------------------/cut1 | -3.087373
1.623765
-6.269894
.0951479
/cut2 | -2.053553
1.618611
-5.225972
1.118865
-----------------------------------------------------------------------------

b. One quantity that would change is the estimated expected value, something pretty
obvious because of the rescaling. In particular,
Êinvest|x  P̂ invest  1|x  2  P̂ invest  2|x
whereas
Êpctstck|x  50  P̂ pctstck  50|x  100  P̂ pctstck  100|x
 50  P̂ invest  1|x  100  P̂ invest  2|x
 50  Êinvest|x.
Because pctstck  50  invest, Epctstck|x  50  Einvest|x.
16.5. a. We have
Dy 2 |z  Normalz 2 , expz 2 
which means we should use maximum likelihood to estimate  2 and  2 . In fact, ̂ 2 is
asymptotically equivalent to a weighted least squares estimator using weights exp−z i  2 .
b. By assumption, u 1 , e 2  is independent of z and so Du 1 |e 2 , z  Du 1 |e 2 . Because

351

u 1 , e 2  is bivariate normal with zero mean, we can always write
u1  1e2  e1
where
e 1 |e 2 ~ Normal0,  21 
where  21   21 −  21 , where  21  Varu 1 . This is necessarily the distribution conditional on z,
too.
We can write
y 2  z 2  expz 2 /2e 2 ,
which shows that y 2 is a function of z, e 2 . Therefore, e 1 is independent of y 2 , too.
c. We can use the latent variable formulation in equation (16.30) and insert u 1   1 e 2  e 1 :
y ∗1  z 1  1   1 y 2   1 e 2  e 1
e 1 |z, y 2 ~ Normal0,  21 
To obtain an error with a unit variance, we divide by  1 :
y ∗1 / 1   z 1  1 / 1    1 / 1 y 2   1 / 1 e 2  e 1 / 1 
and then the cut parameters also get divided by  1 . For example,  j  y ∗1 ≤  j1 if and only if
 j / 1   y ∗1 / 1  ≤  j1 / 1 
Therefore, if we run ordered probit of
y 1 on z 1 , y 2 , e 2
we consistently estimate all parameters multiplied by 1/ 1 . Of course we do not observe e 2 , but
we can replace it with estimates because e 2  exp−z 2 /2v 2 .
The two-step approach is to estimate  2 and  2 by the MLE from part a. Then create

352

v̂ i2  y i2 − z i ̂ 2
ê i2  exp−z i ̂ /2v̂ i2
2

In the second step, estimate the scaled coefficients by OP of
y i1 on z i1 , y i2 , ê i2 .
Let ̂ j , j  1, 2, . . . , J, ̂ 1 , ̂ 1 , and ̂ 1 be the scaled coefficients.
Incidentally, a simple test of the null that y 2 is exogenous is the usual MLE t statistic for
̂ 1 .
d. We can obtain the ASF by averaging out e 2 in response probabilities of the form
 ,j1 − z 1  1 −  1 y 2 −  1 e 2  −  j − z 1  1 −  1 y 2 −  1 e 2 
(for 0  j  J). A consistent estimator of the ASF is
N

ASFz 1 , y 2   N −1 ∑̂ ,j1 − z 1 ̂ 1 − ̂ 1 y 2 − ̂ 1 ê i2  − ̂ j − z 1 ̂ 1 − ̂ 1 y 2 − ̂ 1 ê i2 .
i1

and, as usual, we can compute derivatives or changes with respect to the elements of z 1 , y 2 .
e. Now the normal MLE is just applied to
logy 2 |z ~ Normalz 2 , expz 2 
and v̂ i2  logy i2  − z i ̂ 2 .
f. Without a distributional assumption for Du 2 |e 2 , allowing for endogeneity is tricky. We
would still assume that u 1 , e 2  is independent of z. We could just assume we can write
u 2   1 e 2  e 1 where
De 1 |e 2 , z ~ Normal0,  21 .
Then the two-step method from part c, with ASF estimated as in part d, applies but where ̂ 2

353

and ̂ 2 are obtained from a suitable estimation procedure. It could be a several step procedure
or, more conveniently, a single step based on the normal quasi-MLE. That is, we act as if
Dy 2 |z  Normalexpz 2 , expz 2 
even though it cannot be literally true. As the results of Gourieroux, Monfort, and Trognon
(1984a) show, this estimator is generall consistent and N -asymptotically normal. Then
v̂ i2  y i2 − expz i ̂ 2 
ê i2  exp−z i ̂ /2v̂ i2
2

and the steps in part c can be followed.
A way to make the method more flexible is to add polynomials in ê i2 to the second-stage
OP. For example, if we just add a square, the ASF would be estimated as
N

ASFz 1 , y 2   N

−1

∑̂ ,j1 − z 1 ̂ 1 − ̂ 1 y 2 − ̂ 1 ê i2 − ̂ 1 ê 2i2 
i1

− ̂ j − z 1 ̂ 1 − ̂ 1 y 2 − ̂ 1 ê i2 − ̂ 1 ê 2i2 .
where ̂ 1 is the estimate on the quadratic term.
16.6. This problem is similar to that treated in Papke and Wooldridge (2008) for a binary or
fractional response variable. Using the expression for c i1 we can write
y ∗it1  z it1  1   1 y it2   1  z̄ i  1  a i1  u it1
≡ z it1  1   1 y it2   1  z̄ i  1  v it1
where v it1 ≡ a i1  u it1 . Now we need to make some joint distributional assumptions concerning
v it1 and v it2 , where
y it2  z it  2   2  z̄ i  2  v it2
Given the marginal normal distributions assumed in the problem, it is a small step to assuming

354

v it1   1 v it2  e it1
where
De it1 |v it2 , z i   Normal0,  21 .
We could allow  1 , and even  21 , to depend on t.
Now, we can write a control function equation (in latent variable form) as
y ∗it1  z it1  1   1 y it2   1  z̄ i  1   1 v it2  e it1 ;
given the conditional normality assumption for e it1 , and so using pooled probit of
y it1 on z it1 , y it2 , 1, z̄ i , v it2 , t  1, . . . , T; i  1, . . . , N
consistently estimates all parameters – including the cut parameters – multiplied by 1/ 1 . The
two-step method is then (1) Estimate  2 ,  2 , and  2 by pooled OLS of
y it2 on z it , 1, z̄ i , t  1, . . . , T; i  1, . . . , N.
This is equivalent to fixed effects estimation of  2 . Obtain the residuals, v̂ it2 . (2) Do pooled OP
of
y it1 on z it1 , y it2 , 1, z̄ i , v̂ it2 , t  1, . . . , T; i  1, . . . , N
to obtain ̂ g1 , ̂ g1 , and so on. A simple extension is to interact v̂ it2 with time dummies to allow
the regression of u it1 on v it2 to change over time.
b. Define a dummy variable w ij  1y i1  j. Then
w ij  1 j  y ∗i1 ≤  j1 
 1 j  z it1  1   1 y it2  c i1  u it1 ≤  j1 .
The ASF for w ij is obtained by computing the expected value of the right hand side with
respect to the unobservable c i1  u it1 at specific values z t1 , y t2 :

355

ASFz t1 , y t2   E r it1 1 j  z t1  1   1 y t2  r it1 ≤  j1 
where r it1 ≡ c i1  u it1 . Note that r it1   1  z̄ i  1  v it1 and so we can compute the ASF by
taking the average over z̄ i , v it1 :
ASFz t1 , y t2   E z̄ i ,v it1  1 j  z t1  1   1 y t2   1  z̄ i  1  v it1 ≤  j1 
 E z̄ i ,v it2 ,e it1  1 j  z t1  1   1 y t2   1  z̄ i  1   1 v it2  e it1 ≤  j1 
Now we can apply iterated expectations. First find E|z̄ i , v it2  and then average out z̄ i , v it2 .
Now
E1 j  z t1  1   1 y t2   1  z̄ i  1   1 v it2  e it1 ≤  j1 |z̄ i , v it2 
  g,j1 − z t1  g1 −  g1 y t2 −  g1 − z̄ i  g1 −  g1 v it2 
−  g,j − z t1  g1 −  g1 y t2 −  g1 − z̄ i  g1 −  g1 v it2 
where “g” denotes divided by  1 . We use the fact that De it1 |v it2 , z i   Normal0,  21 . It follows
now by iterated expectations that
ASFz t1 , y t2   E z̄ i ,v it2   g,j1 − z t1  g1 −  g1 y t2 −  g1 − z̄ i  g1 −  g1 v it2 
−  g,j − z t1  g1 −  g1 y t2 −  g1 − z̄ i  g1 −  g1 v it2 .
c. To estimate the ASF, we plug in estimates and use a sample average:
n

ASFz t1 , y t2   N

−1

∑̂ g,j1 − z t1 ̂ g1 − ̂ g1 y t2 − ̂ g1 − z̄ i ̂ g1 − ̂ g1 v̂ it2 
i1

̂ g1 − z̄ i ̂ g1 − ̂ g1 v̂ it2 .
− ̂ g,j − z t1 ̂ g1 − ̂ g1 y t2 − 
As usual, the estimated APEs are derivatives or changes with respect to z t1 , y t2 . To get valid
standard errors, we can use Problem 12.17 or the panel bootstrap – where both estimation steps
are carried out with each resampling.
d. The two-step control function procedure does not require any assumptions about the
relationship between u it1 and v ir2 for t ≠ r. In other words, while adding v it2 as a control
356

function renders y it2 contemporaneously exogenous in the estimating equation – e it1 is
independent of y it2 (and z i  – y it2  is not generally strictly exogenous. An important
implication is that we should not apply a method such as generalized estimating equations in
the second stage.
A method that would render y it2  strictly exogenous would be to project v it1 on the entire
history v ir2 , r  1, . . . , T. There are assumptions under which the projection depends only on
v it2 and the time average, v̄ i2 . So, we could write
v it1   1 v it2   1 v̄ i2  e it1
and assume e it1 is independent of v i2  v i12 , . . . , v iT2  ′ . Then e it1 would be uncorrelated
independent of z i , y i2  (under the other maintained assumptions). So, at each time period,

v̂ it2 , v̄ i2  can be added to the ordered probit – that is, we apply the Mundlak device to the
reduced form residuals. In addition to pooled OP, one could use a GEE-like procedure in the
second stage.
More flexibility would be gotten by using the more general Chamberlain formulation:
v it1  v ′i2  t1  e it1
where  t1 is T  1 for each t. Then in each time period include v̂ ′i2 as a set of regressors
interacted with time-period dummies.

357

Solutions to Chapter 17 Problems
17.1. a. No. Because log1  0 and log is strictly increasing,
Plog1  y  0  Py  0  0. Of course, log1  y increases much more slowly than y,
and so one could use log1  y to reduce the influence of “unusually” large observations y i in
linear regression. Also, remembering that the type I Tobit can be obtained from a latent
variable model, the transformation log1  y might make the normality and homoskedasticity
assumptions in the latent variable formulation more plausible.
b. We can just use ordinary least squares. OLS will be consistent for  (and even
conditionally unbiased). Our inference should be made robust to heteroskedasticity because the
restriction r ≥ −x needs to hold, meaning r cannot be independent of x (unless we restrict the
range of r or x somewhat arbitrarily).
c. Exponentiate and subtract one to get
y  expx  r − 1  y  expx expr − 1
Now take the expectation conditional on x:
Ey|x  expxEexpr|x − 1.
If we assume r is independent of x then Eexpr|x Eexpr  , and so
Ey|x   expx − 1.
d. Because  Eexpr, an unbiased and consistent estimator of  would be
N

N −1 ∑ expr i ,
i1

if we observed the random sample of errors, r i : i  1, 2, . . . , N. Instead, we follow Duan’s
(1983) “smearing” approach (which is really just a method of moments approach) and replace

358

the errors with the OLS residuals, r̂ i , from the regression log1  y i  on x i . Then a consistent
estimator of  is
N

̂  N

−1

∑ expr̂ i ,
i1

which is guaranteed to be greater than one by Jensen’s inequality. Note  is also greater than
unity by Jensen’s:
  Eexpr  expEr  exp0  1.
e. The estimated conditional mean function is simply
Êy|x  ̂ expx̂ − 1.
It is not guaranteed to be nonnegative because the estimates ̂ and ̂ have not been chosen to
ensure nonnegativity. It is possible that, for some vectors x, ̂ expx̂  1.
f. The Stata output follows. The estimated  is ̂  17. 18, which is much higher than unity.
None of the fitted values are negative; they range from about . 061 to 45, 202. The largest
prediction is almost 10 times above the largest observed hours in the data set, and the average
of the fitted values, 3, 166, is much too high: the average of actual hours is 740. 6. Therefore,
for predicting hours, using log1  hours in a linear regression is not very appealing.
. gen lhoursp1  log(1  hours)
. reg lhoursp1 nwifeinc educ exper expersq age kidslt6 kidsge6, robust
Linear regression

Number of obs
F( 7,
745)
Prob  F
R-squared
Root MSE







753
73.12
0.0000
0.2950
2.9367

----------------------------------------------------------------------------|
Robust
lhoursp1 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------nwifeinc | -.0228321
.0098273
-2.32
0.020
-.0421247
-.0035395

359

educ |
.2271644
.0507032
4.48
0.000
.1276262
.3267027
exper |
.2968677
.0407256
7.29
0.000
.2169171
.3768182
expersq | -.0043383
.0013579
-3.19
0.001
-.007004
-.0016726
age |
-.122754
.0163732
-7.50
0.000
-.1548971
-.0906109
kidslt6 | -1.991432
.2110337
-9.44
0.000
-2.405724
-1.577141
kidsge6 |
.0372724
.0917873
0.41
0.685
-.1429201
.2174649
_cons |
4.833966
1.050092
4.60
0.000
2.772473
6.895458
----------------------------------------------------------------------------. predict xbhat
(option xb assumed; fitted values)
. predict rhat, resid
. gen exprhat  exp(rhat)
. sum exprhat
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------exprhat |
753
17.17622
69.2013
.0012194
1045.183
. gen hourshat  17.17622*exp(xbhat) - 1
. sum hours hourshat
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------hours |
753
740.5764
871.3142
0
4950
hourshat |
753
3166.422
5164.107
.061139
45202.41

g. The R-squared is computed in the Stata output that follows. It is about .159, which is
substantially below not only the Tobit R-squared, .275, but also the linear regression
R-squared, .266. For this data set, using log1  y in a linear regression does not work well.
. corr hours hourshat
(obs753)
|
hours hourshat
------------------------------hours |
1.0000
hourshat |
0.3984
1.0000
. di .3984^2
.15872256

h. Under the null of independence between r i and x i , we should find no significant
relationship between r 2 and any function of x. Yet the F (that is, modified Wald) statistic for
heteroskedasticity has a p-value of zero to more than four decimal places. Clearly, r i is not

360

independent of x i .
. gen rhatsq  rhat^2
. gen xbhatsq  xbhat^2
. reg rhatsq xbhat xbhatsq
Source |
SS
df
MS
------------------------------------------Model | 5304.11081
2 2652.05541
Residual |
53062.278
750
70.749704
------------------------------------------Total | 58366.3888
752 77.6148787

Number of obs
F( 2,
750)
Prob  F
R-squared
Adj R-squared
Root MSE








753
37.49
0.0000
0.0909
0.0885
8.4113

----------------------------------------------------------------------------rhatsq |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------xbhat |
3.840246
.4952455
7.75
0.000
2.868013
4.812478
xbhatsq |
-.571263
.0665162
-8.59
0.000
-.7018431
-.4406829
_cons |
4.286994
.9089565
4.72
0.000
2.502592
6.071395
-----------------------------------------------------------------------------

17.2. a. No. The two-limit Tobit only makes sense if there is a corner at both endpoints.
With Py  0  0 the two-limit model becomes a one-limit model at unity, which means the
model does not imply a zero density for y  0. The estimates will be identical to the Tobit
model with an upper corner at unity.
b. Over the range 0, 1, w ≡ − logy takes values in 0, , with
Pw  0  Py  1  0. Assuming that y is continuous on 0, 1, w is continuous over
0, . So w is nonnegative, has a pile up at zero, and is continuously distributed over strictly
positive values. A type I Tobit model makes logical sense.
c. It takes some work, but it is tractable. We can write y  exp−w but we cannot just pass
the expected value through the exponential function. One way to proceed is to write
w  max0, x  u where u|x ~ Normal0,  2  so y  exp− max0, x  u. Then, using
exp0  1 and splitting the integral over u  −x and u ≥ −x,

361

Ey|x 




 − exp− max0, x  u1/u/du
−x



 − 1/u/du   −x exp−x  u1/u/du

 −x/  exp−x1 − −x/



 −x exp−u1/u/du

 −x/  exp−x1 − −x/exp−1x  1/
 −x/  exp−x − 1x/x  1/
Although it is not obvious, this conditional mean function is bounded between zero and one.
17.3. a. Because y  a 1 if and only if y ∗ ≤ a 1 we have
Py  a 1 |x  Py ∗ ≤ a 1 |x  Px  u ≤ a 1 |x
 Pu/ ≤ a 1 − x/|x
 a 1 − x/.
Similarly,
Py  a 2 |x  Py ∗  a 2 |x  Px  u ≥ a 2 |x
 Pu/ ≥ a 2 − x/  1 − a 2 − x/
 −a 2 − x/.
Next, for a 1  y  a 2 , Py ≤ y|x  Py ∗ ≤ y|x  y − x i /. Taking the derivative of
this cdf with respect to y gives the pdf of y conditional on x for values y strictly between a 1
and a 2 : 1/y − x/.
b. Because y  y ∗ when a 1  y ∗  a 2 , Ey ∗ |x, a 1  y i  a 2   Ey ∗ |x, a 1  y ∗  a 2 . But
y ∗  x  u and a 1  y ∗  a 2 if and only if a 1 − x  u  a 2 − x. Therefore, using the hint,
Ey ∗ |x, a 1  y ∗  a 2   x  Eu|x, a 1 − x  u  a 2 − x
 x  Eu/|x, a 1 − x/  u/  a 2 − x/
a 1 − x/ − a 2 − x/
 x 
a 2 − x/ − a 1 − x/
 Ey|x, a 1  y  a 2 .

362

Now, we can easily get Ey|x by using the following:
Ey|x  a 1 Py  a 1 |x  Ey|x, a 1  y  a 2   Pa 1  y  a 2 |x  a 2 Py 2  a 2 |x
 a 1 a 1 − x/
 Ey|x, a 1  y  a 2   a 2 − x/ − a 1 − x/
 a 2 x − a 2 /
 a 1 a 1 − x/  x  a 2 − x/ − a 1 − x/
 a 1 − x/ − a 2 − x/
 a 2 x − a 2 /.
c. From part b it is clear that Ey ∗ |x, a 1  y ∗  a 2  ≠ x, and so it would be a fluke if OLS
on the restricted sample consistently estimated . The linear regression of y i on x i using only
those y i such that a 1  y i  a 2 consistently estimates the linear projection of y ∗ on x in the
subpopulation for which a 1  y ∗  a 2 . Generally, there is no reason to think that this will have
any simple relationship to the parameter vector . [In some restrictive cases, the regression on
the restricted subsample could consistently estimate  up to a common scale coefficient.]
d. We get log-likelihood immediately from part a:
ℓ i   1y i  a 1  loga 1 − x i /
 1y i  a 2  logx i  − a 2 /
 1a 1  y i  a 2  log1/y i − x i /.
Note how the indicator function selects out the appropriate density for each of the three
possible cases: at the left endpoint, at the right endpoint, or strictly between the endpoints.
e. After obtaining the maximum likelihood estimates ̂ and ̂ 2 , just plug these into the
formulas in part b. The expressions can be evaluated at interesting values of x.
f. We can show this by brute-force differentiation of the expression in part b for Ey|x. As
a shorthand, write

363

 1 ≡ a 1 − x/,  2 ≡ a 2 − x/  x − a 2 /,
 1 ≡ a 1 − x/, and  2 ≡ a 2 − x/
Then
∂Ey|x
 −a 1 / 1  j  a 2 / 2  j
∂x j
  2 −  1  j  x/ 1 −  2  j
 a 1 − x/ 1  j − a 2 − x/ 2  j
where the first two parts are the derivatives of the first and third terms, respectively, in Ey|x,
and the last two lines are obtained from differentiating the second term in Ey|x. Careful
inspection shows that all terms cancel except  2 −  1  j , which is the expression we wanted
to be left with.
The scale factor,


a 2 − x


−

a 1 − x


is simply the probability that a standard normal random variable falls in the interval
a 1 − x/, a 2 − x/, which is necessarily between zero and one.
g. The partial effects on Ey|x are given in f. These are estimated as


a 2 − x̂
̂

−

a 1 − x̂
̂

where the estimates are the MLEs. We could evaluate these partial effects at, say, x̄ to
estimate the PEA (partial effect at the average). Or, we can estimate the scale factor for the
APE of continuous x j as
N

̂ ≡ N

−1

∑
i1



a 2 − x i ̂
̂

364

−

a 1 − x i ̂
̂

.

Particularly for the APE, the scaled Tobit coefficients can be compared with the OLS
coefficients (the ̂ j ). Generally, we expect
̂ j ≈ ̂  ̂ j ,
where 0  ̂  1. Of course, this approximation need not be very good in a particular
application often it is. It does not make sense to directly compare the magnitude of ̂ j with that
of ̂ j . By the way, note that ̂ appears in the partial effects along with the ̂ j .
17.4. The Stata outpus is below. The heteroskedasticity-robust standard error on grant is
quite a bit bigger, but the robust t statistic is above four. (Interestingly, the
heteroskedasticity-robust standard error for union is substantially smaller than the usual
standard error.) The coefficient on grant implies that a firm receiving a job training grant in
1988 is estimated to provide about 27. 2 more hours of job training per worker, holding firm
size and union status fixed. This effect is very large considering the average hours of annual
training over all 127 firms is about 16.
. use jtrain1
. des hrsemp grant
storage display
value
variable name
type
format
label
variable label
----------------------------------------------------------------------------hrsemp
float %9.0g
tothrs/totrain
grant
byte
%9.0g
 1 if received grant
. reg hrsemp grant lemploy union if d88
Source |
SS
df
MS
------------------------------------------Model | 23232.2579
3 7744.08598
Residual | 65346.8909
123 531.275536
------------------------------------------Total | 88579.1488
126 703.009118

Number of obs
F( 3,
123)
Prob  F
R-squared
Adj R-squared
Root MSE








127
14.58
0.0000
0.2623
0.2443
23.049

----------------------------------------------------------------------------hrsemp |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------grant |
27.17647
4.769283
5.70
0.000
17.73597
36.61698
lemploy | -5.511867
2.012923
-2.74
0.007
-9.496324
-1.527409

365

union | -8.924901
5.392118
-1.66
0.100
-19.59827
1.748465
_cons |
30.76978
7.345811
4.19
0.000
16.2292
45.31037
----------------------------------------------------------------------------. reg hrsemp grant lemploy union if d88, robust
Linear regression

Number of obs
F( 3,
123)
Prob  F
R-squared
Root MSE







127
7.40
0.0001
0.2623
23.049

----------------------------------------------------------------------------|
Robust
hrsemp |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------grant |
27.17647
6.525922
4.16
0.000
14.25881
40.09414
lemploy | -5.511867
2.17685
-2.53
0.013
-9.820807
-1.202926
union | -8.924901
3.181306
-2.81
0.006
-15.2221
-2.627702
_cons |
30.76978
8.558935
3.60
0.000
13.8279
47.71167
-----------------------------------------------------------------------------

b. The Tobit results are below. Out of 127 firms in 1988, 38 provide no job training.
. count if hrsemp  0 & d88
38
. tobit hrsemp grant lemploy union if d88, ll(0)
Tobit regression

Number of obs
LR chi2(3)
Prob  chi2
Pseudo R2

Log likelihood  -451.88026






127
37.46
0.0000
0.0398

----------------------------------------------------------------------------hrsemp |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------grant |
36.34335
6.121823
5.94
0.000
24.22655
48.46016
lemploy | -4.928542
2.656817
-1.86
0.066
-10.18713
.330044
union | -12.63617
7.286913
-1.73
0.085
-27.05901
1.786677
_cons |
20.32933
9.769517
2.08
0.040
.9927198
39.66594
---------------------------------------------------------------------------/sigma |
28.70726
2.229537
24.29438
33.12014
----------------------------------------------------------------------------Obs. summary:
38 left-censored observations at hrsemp0
89
uncensored observations
0 right-censored observations

The language “left censored at hrsemp  0” is misleading for corner solution applications,
but it does tell us that 38 of the 127 firms have hrsemp  0. The estimate of  is ̂  28. 71.
To get the effect of grant on Ehrsemp|grant, employ, union, hrsemp  0, we must
compute the inverse Mills ratio with grant  1 and grant  0. We set

366

employ  employ  60. 87 and union  1. Below is the Stata session.
. gen xb1  _b[_cons]  _b[grant] _b[lemploy]*log(60.87)  _b[union]
. gen xb0  _b[_cons] _b[lemploy]*log(60.87)  _b[union]
. gen prob1  normal(xb1/_b[/sigma])
. gen prob0  normal(xb0/_b[/sigma])
. gen imr1 

normalden(xb1/_b[/sigma])/prob1

. gen imr0 

normalden(xb0/_b[/sigma])/prob0

. gen cm1  xb1  _b[/sigma]*imr1
. gen cm0  xb0  _b[/sigma]*imr0
. gen dcm  cm1 - cm0
. list dcm in 1
----------
|
dcm |
|----------|
1. | 15.09413 |
----------
. gen um1  prob1*cm1
. gen um0  prob0*cm0
. gen dum  um1 - um0
. list dum in 1
----------
|
dum |
|----------|
1. | 20.81422 |
----------

For firms already doing some job training, the grant is estimated to increase training by
about 15.1 hours per employee. When we add in the effects of firms that go from no training to
positive hours, the expected change is about 20.8 hours at union  1 and the average value of
employ in the sample. This is somewhat less than the OLS estimate we obtained earlier, 27.2.
The estimated APE is on the unconditional mean is computed below as 26.2, which is
pretty close to the OLS estimate of 27.2. Bootstrapping can be used to obtain a valid standard
367

error.
. predict xb, xb
(31 missing values generated)
. sum xb if d88
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------xb |
146
9.095312
16.80602 -18.41981
48.7405
. replace xb  . if ~d88
(294 real changes made, 294 to missing)
. replace xb  . if hrsemp  . | lemploy  . | union  .
(19 real changes made, 19 to missing)
. sum xb
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------xb |
127
9.265182
17.20874 -18.41981
48.7405
. gen xb0  xb - _b[grant]*grant
(344 missing values generated)
. gen xb1  xb0  _b[grant]
(344 missing values generated)
. gen prob0  normal(xb0/_b[/sigma])
(344 missing values generated)
. gen prob1  normal(xb1/_b[/sigma])
(344 missing values generated)
. gen imr0  normalden(xb0/_b[/sigma])/prob0
(344 missing values generated)
. gen imr1  normalden(xb1/_b[/sigma])/prob1
(344 missing values generated)
. gen cm0  xb0  _b[/sigma]*imr0
(344 missing values generated)
. gen cm1  xb1  _b[/sigma]*imr1
(344 missing values generated)
. gen um0  prob0*cm0
(344 missing values generated)
. gen um1  prob1*cm1
(344 missing values generated)
. gen pe  um1 - um0
(344 missing values generated)
. sum pe

368

Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------pe |
127
26.23
3.272887
16.88082
30.56553

c. They are jointly significant at the 1.5% level, as the Stata “test” command shows.
. test lemploy union
( 1)
( 2)

[model]lemploy  0
[model]union  0
F(

2,
124) 
Prob  F 

4.34
0.0151

d. For the Tobit model, I use the square of the correlation between y i  hrsemp i and
Êy i |x i  as an R-squared that can be compared with the linear model R-squared. After the
tobit command in Stata, Êy i |x i  can be gotten using the ystar option for predicted values.
(Unfortunately, Stata’s naming convention conflicts with the notation used in the text, as y ∗ is
used to denote the unerlying latent variable, not the actual outcome.)
. predict hrsemph if d88 & hrsemp ! ., ystar(0,.)
(344 missing values generated)
. corr hrsemp hrsemph
(obs127)
|
hrsemp hrsemph
------------------------------hrsemp |
1.0000
hrsemph |
0.5206
1.0000
. di (.5206)^2
.27102436

This R-squared is slightly above that for the linear model (.262), and so the Tobit does
provide a better fit. And remember, the Tobit estimates are not chosen to maximize an
R-squared, so the improvement in fit is effectively better.
17.5. a. The results from OLS estimation of the linear model are given below.
. use fringe
. reg hrbens exper age educ tenure married male white nrtheast nrthcen south
union, robust
Number of obs 

Linear regression

369

616

F( 11,
604)
Prob  F
R-squared
Root MSE






36.02
0.0000
0.3718
.53183

----------------------------------------------------------------------------|
Robust
hrbens |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------exper |
.0029862
.0042485
0.70
0.482
-.0053574
.0113298
age | -.0022495
.0041519
-0.54
0.588
-.0104034
.0059043
educ |
.082204
.0085122
9.66
0.000
.065487
.0989211
tenure |
.0281931
.0037053
7.61
0.000
.0209164
.0354699
married |
.0899016
.0499158
1.80
0.072
-.0081281
.1879312
male |
.251898
.0496953
5.07
0.000
.1543015
.3494946
white |
.098923
.0721337
1.37
0.171
-.0427402
.2405862
nrtheast | -.0834306
.0723545
-1.15
0.249
-.2255277
.0586664
nrthcen | -.0492621
.0626967
-0.79
0.432
-.1723922
.073868
south | -.0284978
.0653108
-0.44
0.663
-.1567617
.0997662
union |
.3768401
.0535136
7.04
0.000
.2717448
.4819354
_cons | -.6999244
.1803555
-3.88
0.000
-1.054125
-.3457242
-----------------------------------------------------------------------------

b. The Tobit estimates recognizing the corner at zero are
. tobit hrbens exper age educ tenure married male white nrtheast nrthcen south
union, ll(0)
Tobit regression

Number of obs
LR chi2(11)
Prob  chi2
Pseudo R2

Log likelihood  -519.66616






616
283.86
0.0000
0.2145

----------------------------------------------------------------------------hrbens |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------exper |
.0040631
.0046627
0.87
0.384
-.0050939
.0132201
age | -.0025859
.0044362
-0.58
0.560
-.0112981
.0061263
educ |
.0869168
.0088168
9.86
0.000
.0696015
.1042321
tenure |
.0287099
.0037237
7.71
0.000
.021397
.0360227
married |
.1027574
.0538339
1.91
0.057
-.0029666
.2084814
male |
.2556765
.0551672
4.63
0.000
.1473341
.364019
white |
.0994408
.078604
1.27
0.206
-.054929
.2538106
nrtheast | -.0778461
.0775035
-1.00
0.316
-.2300547
.0743625
nrthcen | -.0489422
.0713965
-0.69
0.493
-.1891572
.0912729
south | -.0246854
.0709243
-0.35
0.728
-.1639731
.1146022
union |
.4033519
.0522697
7.72
0.000
.3006999
.506004
_cons | -.8137158
.1880725
-4.33
0.000
-1.18307
-.4443616
---------------------------------------------------------------------------/sigma |
.5551027
.0165773
.5225467
.5876588
----------------------------------------------------------------------------Obs. summary:
41 left-censored observations at hrbens0
575
uncensored observations
0 right-censored observations

The Tobit and OLS estimates are similar because only 41 of 616 observations, or about

370

6.7% of the sample, have hrbens  0. As expected, the Tobit estimates are all slightly larger in
magnitude; this reflects that the scale factor is always less than unity.
c. Here is what happens when exper 2 and tenure 2 are included:
. tobit hrbens exper age educ tenure married male white nrtheast nrthcen south
union expersq tenuresq, ll(0)
Tobit regression

Number of obs
LR chi2(13)
Prob  chi2
Pseudo R2

Log likelihood  -503.62108






616
315.95
0.0000
0.2388

----------------------------------------------------------------------------hrbens |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------exper |
.0306652
.0085253
3.60
0.000
.0139224
.047408
age | -.0040294
.0043428
-0.93
0.354
-.0125583
.0044995
educ |
.0802587
.0086957
9.23
0.000
.0631812
.0973362
tenure |
.0581357
.0104947
5.54
0.000
.037525
.0787463
married |
.0714831
.0528969
1.35
0.177
-.0324014
.1753675
male |
.2562597
.0539178
4.75
0.000
.1503703
.3621491
white |
.0906783
.0768576
1.18
0.239
-.0602628
.2416193
nrtheast | -.0480194
.0760238
-0.63
0.528
-.197323
.1012841
nrthcen |
-.033717
.0698213
-0.48
0.629
-.1708394
.1034053
south |
-.017479
.0693418
-0.25
0.801
-.1536597
.1187017
union |
.3874497
.051105
7.58
0.000
.2870843
.4878151
expersq | -.0005524
.0001487
-3.71
0.000
-.0008445
-.0002604
tenuresq | -.0013291
.0004098
-3.24
0.001
-.002134
-.0005242
_cons | -.9436572
.1853532
-5.09
0.000
-1.307673
-.5796409
---------------------------------------------------------------------------/sigma |
.5418171
.0161572
.5100859
.5735484
----------------------------------------------------------------------------Obs. summary:
41 left-censored observations at hrbens0
575
uncensored observations
0 right-censored observations
. test expersq tenuresq
( 1)
( 2)

[model]expersq  0
[model]tenuresq  0
F(

2,
603) 
Prob  F 

16.34
0.0000

Both squared terms are very statistically significant as well as jointly significant. What is
not clear is whether their presence would change the estimated partial effects in important
ways.
d. There are nine industries, and we use ind1 as the base industry:

371

. tobit hrbens exper age educ tenure married male white nrtheast nrthcen south
union expersq tenuresq ind2-ind9, ll(0)
Tobit regression

Number of obs
LR chi2(21)
Prob  chi2
Pseudo R2

Log likelihood  -467.09766






616
388.99
0.0000
0.2940

----------------------------------------------------------------------------hrbens |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------exper |
.0267869
.0081297
3.29
0.001
.0108205
.0427534
age | -.0034182
.0041306
-0.83
0.408
-.0115306
.0046942
educ |
.0789402
.0088598
8.91
0.000
.06154
.0963403
tenure |
.053115
.0099413
5.34
0.000
.0335907
.0726393
married |
.0547462
.0501776
1.09
0.276
-.0438005
.1532928
male |
.2411059
.0556864
4.33
0.000
.1317401
.3504717
white |
.1188029
.0735678
1.61
0.107
-.0256812
.2632871
nrtheast | -.1016799
.0721422
-1.41
0.159
-.2433643
.0400045
nrthcen | -.0724782
.0667174
-1.09
0.278
-.2035085
.0585521
south | -.0379854
.0655859
-0.58
0.563
-.1667934
.0908226
union |
.3143174
.0506381
6.21
0.000
.2148662
.4137686
expersq | -.0004405
.0001417
-3.11
0.002
-.0007188
-.0001623
tenuresq | -.0013026
.0003863
-3.37
0.001
-.0020613
-.000544
ind2 | -.3731778
.3742017
-1.00
0.319
-1.108095
.3617389
ind3 | -.0963657
.368639
-0.26
0.794
-.8203575
.6276261
ind4 | -.2351539
.3716415
-0.63
0.527
-.9650425
.4947348
ind5 |
.0209362
.373072
0.06
0.955
-.7117618
.7536342
ind6 | -.5083107
.3682535
-1.38
0.168
-1.231545
.214924
ind7 |
.0033643
.3739442
0.01
0.993
-.7310468
.7377754
ind8 | -.6107854
.376006
-1.62
0.105
-1.349246
.127675
ind9 | -.3257878
.3669437
-0.89
0.375
-1.04645
.3948746
_cons | -.5750527
.4137824
-1.39
0.165
-1.387704
.2375989
---------------------------------------------------------------------------/sigma |
.5099298
.0151907
.4800959
.5397637
----------------------------------------------------------------------------Obs. summary:
41 left-censored observations at hrbens0
575
uncensored observations
0 right-censored observations
. testparm ind2-ind9
(
(
(
(
(
(
(
(

1)
2)
3)
4)
5)
6)
7)
8)

[model]ind2
[model]ind3
[model]ind4
[model]ind5
[model]ind6
[model]ind7
[model]ind8
[model]ind9
F(










0
0
0
0
0
0
0
0

8,
595) 
Prob  F 

9.66
0.0000

Each industry dummy variable is individually insignificant at even the 10% level, but the
joint Wald test says that they are jointly very significant. This is somewhat unusual for dummy

372

variables that are necessarily othogonal (so that there is not a multicollinearity problem among
them). The likelihood ratio statistic is LR  2503. 621 − 467. 098  73. 046, which is roughly
comparable with Q  F  8  9. 66  77. 28. The p-values in both cases are essentially zero.
Several estimates on the industry dummies are economically significant, with a worker in,
say, industry eight earning about 61 cents less per hour in benefits than a comparable worker in
industry one. [In this example, with so few observations at zero, it is roughly legitimate to use
the parameter estimates as the partial effects.]
17.6. a. First, we can write u 1   1 v 2  e 1 , where  1  Covv 2 , u 1 , and we use the fact
that Varv 2   1. Also,  21   21   21 where  21  Vare 1 . The distribution of y 1 given z, v 2 
can be written as gy 1 |z 1  1   1 y 2   1 v 2 ,  21 −  21 , where y 1 is the generic argument. Next,
we need the density of v 2 given z, y 2 , which is given in equations (15.55) and (15.56). To
obtain the density of y 1 given z, y 2 , we can apply Property CD.3 in Appendix 2A. The
density of v 2 |z, y 2  1 is v 2 /z 2  for v 2  −z 2 . So the density of y 1 given z, y 2  1
is
1
z 2 



 −z

gy 1 |z 1  1   1 y 2   1 v 2 ,  21 −  21 v 2 dv 2
2

(where v 2 is just the dummy argument in the integration) and the density given z, y 2  0 is
1
1 − z 2 

−z 2

 −

gy 1 |z 1  1   1 y 2   1 v 2 ,  21 −  21 v 2 dv 2 .

b. We need to combine the density obtained from part a – called it
fy 1 |y 2 , z;  1 ,  1 ,  1 ,  21 ,  2 , and let hy 2 |z;  2  be the probit density of y 2 given z. Actually, it is
easier to work with  21   21 −  21 . Then the log-likelihood for observation i is

373

logfy i1 |y i2 , z i ;  1 ,  1 ,  1 ,  21 ,  2   loghy i2 |z i ;  2 
 y i2 log

1
z i  2 

 1 − y i2  log



 −z 

gy i1 |z i1  1   1 y i2   1 v 2 ,  21 v 2 dv 2

i 2

1
1 − z i  2 

−z i  2

 −

gy i1 |z i1  1   1 y i2   1 v 2 ,  21 v 2 dv 2

 y i2 logz i  2   1 − y i2  log1 − z i  2 
which simplifies to
ℓ i   y i2 log



 −z 

gy i1 |z i1  1   1 y i2   1 v 2 ,  21 v 2 dv 2

i 2

 1 − y i2  log

−z i  2

 −

gy i1 |z i1  1   1 y i2   1 v 2 ,  21 v 2 dv 2 .

If  1  0, the log likelihood becomes
ℓ i   y i2 loggy i1 |z i1  1   1 y i2 ,  21 z i  2 
 1 − y i2  loggy i1 |z i1  1   1 y i2 ,  21 1 − z i  2 
 loggy i1 |z i1  1   1 y i2 ,  21   y i2 logz i  2   1 − y i2  log1 − z i  2 ,
which is two separate log-likelihoods, one the standard Tobit for y i1 give z i1 , y i2  and the
second for probit of y i2 given z i .
c. As in the probit case (Section 15.7.3), this is another example of a forbidden regression.
There is no way that Ey 1 |z has the Tobit form with z 1 and z 2   Ey 2 |z as the
explanatory variables. In fact, because y 1  max0, z 1  1   1 y 2  u 1 , Ey 1 |z has no simple
form – although it could be computed in principle.
d. As given in the hint, it is easiest to work with the parameterization in terms of  21 , as
shown in part b. Passing the derivative through the integral gives

374



 −z  v 2 g 1 y i1 |z i1  1   1 y i2   1 v 2 ,  21 v 2 dv 2
∂ℓ i 
 y i2 i 2
∂ 1

gy i1 |z i1  1   1 y i2   1 v 2 ,  21 v 2 dv 2
−z i  2

−z 

 1 − y i2 

 −i 2 v 2 g 1 y i1 |z i1  1   1 y i2   1 v 2 ,  21 v 2 dv 2
−z 

 −i 2 gy i1 |z i1  1   1 y i2   1 v 2 ,  21 v 2 dv 2

.

where g 1 denotes the first derivative. When we set  1  0 the first term becomes
y i2

g 1 y i1 |z i1  1   1 y i2 ,  21  
gy i1 |z i1  1   1 y i2 ,  21  


−z i  2

−z i  2

v 2 v 2 dv 2
v 2 dv 2

g 1 y i1 |z i1  1   1 y i2 ,  21 
z i  2 
 y i2

2
1 − −z i  2 
gy i1 |z i1  1   1 y i2 ,  1 


g 1 y i1 |z i1  1   1 y i2 ,  21 
 y i2 z i  2 
gy i1 |z i1  1   1 y i2 ,  21 


where a  a/a is the inverse Mills ratio and we use the fact that  vvdv  a
a

for any a ∈ R. Similarly, using 

a
−

vvdv  −a, the second term is

g 1 y i1 |z i1  1   1 y i2 ,  21 
 −1 − y i2 −z i  2 
gy i1 |z i1  1   1 y i2 ,  21 
and so the partial derivative evaluated at  1  0 is
g 1 y i1 |z i1  1   1 y i2 ,  21 
y i2 z i  2  − 1 − y i2 −z i  2 
gy i1 |z i1  1   1 y i2 ,  21 
≡

g 1 y i1 |z i1  1   1 y i2 ,  21 
gry i2 , z i  2 
gy i1 |z i1  1   1 y i2 ,  21 

where gry i2 , z i  2  ≡ y i2 z i  2  − 1 − y i2 −z i  2  is the generalized residual. The key is that
this is the same partial derivative we would obtain by simply adding gr i2 ≡ gry i2 , z i  2  as an
explanatory variable and giving it a coefficient, say  1 . In other words, form the artificial
model

375

y i1  max0, z i1  1   1 y i2   1 gr i2  e i1 
e i1 |z i1 , y i2 , gr i2 ~ Normal0,  21 .
Of course, under the give assumptions this “model” cannot be true when  1 ≠ 0. But if we act
as if it is true and compute the score for testing H 0 :  1  0, we get exactly the score derived
above. So we are led to a simple variable addition test. In the first stage estimate probit of y i2
on z i to get ̂ 2 . Construct the generalized residuals,
gr i2  y i2 z i ̂ 2  − 1 − y i2 −z i ̂ 2 .
In the second step, estimate a Tobit model of y i1 on z i1 , y i2 , gr i2 and use a t test for the
coefficient ̂ 1 on gr i2 . Under the null, the statistic has an asymptotic Normal0, 1 distribution,
with no need to adjust the standard error for estimation of  2 .
Incidentally, while adding the generalized residual – which acts as a kind of control
function – does not generally solve the endogeneity of y 2 under the assumptions of this
problem, it might be a decent approximation. It is likely to do well when  1 is “close” to zero
(although we then must wonder how much of a problem endogeneity is in the first case). There
is some evidence that it can work well as an approximation more general, where focus would
be on average partial effects. Putting in flexible functions of gr i2 – such as low-order
polynomials – can help even more.
If we simply assert that Dy 1 |z 1 , y 2 , gr 2  follows the Tobit model given above then adding
gr i2 does produce consistent estimators of all parameters and average partial effects (by
averaging out gr i2 in the partial effect formulas for the standard Tobit). This idea is
nontraditional but is in the spirit of viewing all models simply as approximations.
17.7. Let s  1y  0 and use Property CV.3 about conditional variances (see Appendix

376

2.A.2):
Vary|x  EVary|x, s|x  VarEy|x, s|x
Now because y  s  w ∗ ,
Ey|x, s  s  Ew ∗ |x, s  s  Ew ∗ |x  s  expx
Vary|x, s  s 2  Varw ∗ |x, s  s  Varw ∗ |x  s   2 expx 2
and so
Vary|x  Es   2 exp2x|x  Vars  expx|x
 Ps  1|x 2 exp2x  Vars|x exp2x
  2 x exp2x  x1 − x exp2x
17.8. a. For model (1) simply use ordinary least squares. Under the conditional mean
assumption, we could use a weighted least squares procedure if we suspect heteroskedasticity,
as we might, and have a particular form in mind. However, we should probably not think of the
linear model as a model of Ey|x; rather, it is simply the linear projection. If we use a WLS
procedure, we are effectively estimating a linear predictor in weighted variables.
For model (2) we could use nonlinear regression, or weighted nonlinear regression. The
latter is attractive because of probable heteroskedasticity in Vary|x. We might use a variance
function proportional to expx or expx 2 , or a quadratic in the mean function:
Vary|x   0   1 expx   2 expx 2 which contains the previous two as a special case.
We can estimate the  j from the OLS regression û 21 on 1, ŷ i , and ŷ 2i , where the hatted quantities
are from a first stage NLS estimation. The fitted values are the estimated conditinal variances
(and we might have to worry about whether they are all strictly positive). Other attractive
options are Poisson regression – see Chapter 18 for a description of its robustness properties
for estimating Ey|x – or regression using the Exponential quasi-log-likelihood (see Chapter

377

18).
Naturally, for model (3) we would use MLE.
b. We can compute an R-squared type measure any time we directly model Ey|x or we
have an implied model for Ey|x (such as in the Tobit case). In each case, we obtain the fitted
values, Êy i |x i , i  1, … , N. Once we have fitted values we can obtain the squared correlation
between y i and Êy i |x i . These can be compared across different models and even estimation
methods. Alternatively, one can use a sum-of-squared residuals form:
N
y i − Êy i |x i  2
∑ i1
.
R  1−
N
y i − ȳ  2
∑ i1
2

In the linear regression case with an intercept, the two ways of computing R-squared are
identical, but the equivalance does not hold in general. In fact, the SSR version of R-squared
can be negative in some cases. One can always compute an “adjusted” R-squared, too:
N

̄2

R  1−

N − P −1 ∑ i1 y i − Êy i |x i  2
N

N − 1 −1 ∑ i1 y i − ȳ  2

where P is the number of estimated parameters in the mean function.
c. This is clear from equation (17.20). If y i  0 for i  1, … , N, then only the second term
in the log likelihood appears. But that is just the log likelihood for the classical linear
regression model where y i |x i ~ Normalx i ,  2 . It is well known that the MLE of  in this
case is the OLS estimator.
It may seem a bit odd, but if we truly believe the population follows a Tobit model – and
just happen to obtain a sample where y i  0 for all i – then the appropriate estimate of Ey|x
is gotten from (17.14), where we plug in the usual OLS estimators for  and  2 . Estimates of
Ey|x computed in this way would ensure that fitted values in the sample are all positive, even
378

though x i ̂ could be negative for some i.
d. If y  0 in the population, a Tobit model makes no sense because Py  0  0 for a
Tobit model. Instead, we could assume Elogy|x  x, or, equivalently, logy  x  v,
Ev|x  0. If we make the stronger assumption that v is independent of x, then
Ey|x    expx, where  ≡ Eexpv  1. After estimating  from the OLS regression
logy i  on x i , i  1, … , N, we can estimate  using Duan’s (1983) estimator, as in Problem
17.1:
N

̂  N

−1

∑ expv̂ i ,
i1

where the v̂ i are the OLS residuals.
17.9. a. A two-limit Tobit model, of the kind analyzed in Problem 17.3, is appropriate, with
a 1  0, a 2  10.
b. The lower limit at zero is logically necessary considering the kind of response: the
smallest percentage of one’s income that can be invested in a pension plan is zero. On the
other hand, the upper limit of 10 is an arbitrary corner imposed by law. One can imagine that
some people at the corner y  10 would choose y  10 if they could. So, we can think of an
underlying variable, which would be the percentage invested in the absence of any restrictions.
Then, there would be no upper bound required (since we would not have to worry about 100
percent of income being invested in a pension plan).
17.10. A more general version of this problem is done in Problem 17.3, part f: set a 1  0
and let a 2  .
17.11. No. OLS always consistently estimates the parameters of a linear projection
provided the second moments of y and the x j are finite and Varx has full rank K – regardless
379

of the nature of y or x (discrete, continuous, some mixture). The fact that we can always
consistently estimate a linear projection by OLS is why linear regression analysis is always a
reasonable step for discrete outcomes (provided there is no data censoring problem of the type
we discuss in Chapter 19). As discussed in Chapters 15 and 17, the linear regression
coefficients often are close to estimated average partial effects from more complicated models.
See Problem 17.4 part b for an example.
17.12. a. 248 out of 660, or about 37.6%, have ecolbs i  0. The positive responses range
from .333 to a high of 42, but with focal points at integer values, especially one pound and two
pounds. Therefore, a Tobit model cannot literally be true, but it can still lead to good estimates
of the conditional mean and partial effects.
b. The linear model results are given below:
. use apple
. gen lecoprc  log(ecoprc)
. gen lregprc  log(regprc)
. gen lfaminc  log(faminc)
. reg ecolbs lecoprc lregprc lfaminc educ hhsize num5_17
Source |
SS
df
MS
------------------------------------------Model | 155.149478
6 25.8582463
Residual | 4048.98735
653 6.20059318
------------------------------------------Total | 4204.13682
659
6.3795703

Number of obs
F( 6,
653)
Prob  F
R-squared
Adj R-squared
Root MSE








660
4.17
0.0004
0.0369
0.0281
2.4901

----------------------------------------------------------------------------ecolbs |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lecoprc |
-2.56959
.5865181
-4.38
0.000
-3.721279
-1.417901
lregprc |
2.204184
.5903005
3.73
0.000
1.045068
3.3633
lfaminc |
.203861
.155967
1.31
0.192
-.1023964
.5101184
educ |
.0251628
.0457977
0.55
0.583
-.0647657
.1150913
hhsize |
.0015866
.0886932
0.02
0.986
-.1725717
.1757449
num5_17 |
.1111276
.1343768
0.83
0.409
-.1527351
.3749903
_cons |
.7307278
.7610805
0.96
0.337
-.7637326
2.225188
-----------------------------------------------------------------------------

380

The price coefficients are of the expected sign: there is a negative own price effect, and a
positive price effect for the substitute good, regular apples. The coefficient on logecoprc
implies that a 10% increase in ecoprc leads to a fall in estimated demand of about . 26 lbs. At
the mean value of ecolbs, about 1. 47 lbs, this is an estimated own price elasticity of
−2. 57/1. 47  −1. 75, which is very large in magnitude.
c. The test for heteroskedasticity is given below. The F statistic, which is asymptotically
valid as a test for heteroskedasticity, gives a pretty large p-value, . 362, so this test does not
find much evidence of heteroskedasticity.
. predict ecolbsh
(option xb assumed; fitted values)
. gen ecolbshsq  ecolbsh^2
. predict uh, resid
. gen uhsq  uh^2
. reg uhsq

ecolbsh ecolbshsq

Source |
SS
df
MS
------------------------------------------Model | 8923.31842
2 4461.65921
Residual | 2880416.28
657 4384.19525
------------------------------------------Total |
2889339.6
659 4384.43034

Number of obs
F( 2,
657)
Prob  F
R-squared
Adj R-squared
Root MSE








660
1.02
0.3620
0.0031
0.0001
66.213

----------------------------------------------------------------------------uhsq |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------ecolbsh |
32.61476
31.09945
1.05
0.295
-28.45153
93.68105
ecolbshsq |
-8.9604
10.32346
-0.87
0.386
-29.23136
11.31056
_cons | -20.36486
21.92073
-0.93
0.353
-63.40798
22.67827
-----------------------------------------------------------------------------

d. The fitted values were already gotten from part c. The summary statistics are
. sum ecolbs ecolbsh
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------ecolbs |
660
1.47399
2.525781
0
42
ecolbsh |
660
1.47399
.485213
.2251952
2.598743
. count if ecolbs  2.6
541

381

. di 541/660
.81969697

The smallest fitted value is . 225, and so none are negative. The largest fitted value is only
about 2. 6, but about 82 percent of the observations have ecolbs i below 2. 6. Generally, it is
difficult to find models that will track such a wide range in actual outcomes. Further, one
might suspect the largest value, 42, is a mistake or an outlier. (The estimates with this one
observation dropped give a similar story, but the price coefficients shrink in magnitude.)
e. The Tobit results are given below. The signs are the same as for the linear model, with
the price and income variables being more statistically significant for Tobit. We know that the
coefficients need to be scaled down in order to obtain the partial effects. That the Tobit
coefficient on logecoprc is about double the OLS estimate is not surprising, and we need to
compute a scale factor. The scale factor for the APEs (of continuous explanatory variables) is
about . 547. If we multiply each Tobit coefficient by . 547, we get fairly close to the OLS
estimates.
. tobit ecolbs lecoprc lregprc lfaminc educ hhsize num5_17, ll(0)
Tobit regression

Number of obs
LR chi2(6)
Prob  chi2
Pseudo R2

Log likelihood  -1265.7088






660
50.79
0.0000
0.0197

----------------------------------------------------------------------------ecolbs |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lecoprc | -5.238074
.8748606
-5.99
0.000
-6.955949
-3.5202
lregprc |
4.261536
.8890055
4.79
0.000
2.515887
6.007185
lfaminc |
.4149175
.2363235
1.76
0.080
-.0491269
.8789619
educ |
.1005481
.068439
1.47
0.142
-.0338386
.2349348
hhsize |
.0330173
.1325415
0.25
0.803
-.2272409
.2932756
num5_17 |
.2260429
.1970926
1.15
0.252
-.1609678
.6130535
_cons | -1.917668
1.160126
-1.65
0.099
-4.195689
.3603525
---------------------------------------------------------------------------/sigma |
3.445719
.1268015
3.196732
3.694706
----------------------------------------------------------------------------Obs. summary:
248 left-censored observations at ecolbs0
412
uncensored observations
0 right-censored observations

382

. predict xbh, xb
. gen prob  normal(xbh/_b[/sigma])
. sum prob
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------prob |
660
.5472506
.1152633
.2610003
.8191264

f. This question is a bit ambiguous. I will evaluate the partial effect at the mean value of
ecoprc, regprc, and faminc, and then take the log, rather than averaging the logs. The scale
factor for the APEs is given in part e: . 547. The scale factor for the partial effects at the mean
is . 539, which is fairly close. The PAE of lecoprc is about −2. 82, which is somewhat bigger in
magnitude than the OLS estimate, −2. 57.
To get the estimated elasticity, we need to estimate E ecolbs|x at the mean values of the
covariates; we get about 1. 55. So the estimated elasticity at the mean values of the covariates
is about −2. 82/1. 55 ≈ −1. 82. This is slightly larger in in magnitude than that computed for the
linear model, −1. 75.
. sum ecoprc regprc faminc educ hhsize num5_17
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------ecoprc |
660
1.081515
.295573
.59
1.59
regprc |
660
.8827273
.2444687
.59
1.19
faminc |
660
53.40909
35.74122
5
250
educ |
660
14.38182
2.274014
8
20
hhsize |
660
2.940909
1.526049
1
9
--------------------------------------------------------------------num5_17 |
660
.6212121
.994143
0
6
. di normal((_b[_cons]  _b[lecoprc]*log( 1.081515 )
 _b[lregprc]*log(.8827273)  _b[lfaminc]*log(53.40909)
 _b[educ]*14.38182  _b[hhsize]*2.940909)/_b[/sigma])
.53860761
. di .53860761*_b[lecoprc]
-2.8212668
. di _b[_cons]  _b[lecoprc]*log( 1.081515 )
 _b[lregprc]*log(.8827273)  _b[lfaminc]*log(53.40909)
 _b[educ]* 14.38182  _b[hhsize]*2.940909
.33398136

383

. di normalden((_b[_cons]  _b[lecoprc]*log( 1.081515 )
 _b[lregprc]* log(.8827273)  _b[lfaminc]*log(53.40909)
 _b[educ]* 14.38182  _b[hhsize]*2.940909)/_b[/sigma])
.3970727
. di .33398136*.53860761  _b[/sigma]*.3970727
1.5480857
. di -2.82/1.55
-1.8193548

g. Dropping logregprc greatly reduces the magnitude of the coefficient on logecoprc:
from −5. 24 to −1. 82. A standard omitted variable analysis is a linear context suggests a
positive correlation between lecoprc and lregprc. In fact, they are very highly positively
correlated, with a correlation of about . 82. This high correlation was built in as part of the
experimental design.
. tobit ecolbs lecoprc lfaminc educ hhsize num5_17, ll(0)
Tobit regression

Number of obs
LR chi2(5)
Prob  chi2
Pseudo R2

Log likelihood  -1277.3043






660
27.60
0.0000
0.0107

----------------------------------------------------------------------------ecolbs |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lecoprc | -1.822712
.5044411
-3.61
0.000
-2.813229
-.8321952
lfaminc |
.3931692
.2395441
1.64
0.101
-.0771978
.8635362
educ |
.1169085
.0692025
1.69
0.092
-.0189769
.252794
hhsize |
.0222283
.1340901
0.17
0.868
-.2410699
.2855266
num5_17 |
.2474529
.1996317
1.24
0.216
-.1445424
.6394483
_cons | -2.873156
1.161745
-2.47
0.014
-5.154351
-.5919615
---------------------------------------------------------------------------/sigma |
3.499092
.1291121
3.245569
3.752616
----------------------------------------------------------------------------Obs. summary:
248 left-censored observations at ecolbs0
412
uncensored observations
0 right-censored observations
. corr lecoprc lregprc
(obs660)
| lecoprc lregprc
------------------------------lecoprc |
1.0000
lregprc |
0.8205
1.0000

h. In fact, the Tobit model with prices in level form, rather than logarithms, fits a bit better

384

(log-likelihood  −1, 263. 37 versus −1, 265. 71).
. tobit ecolbs ecoprc regprc lfaminc educ hhsize num5_17, ll(0)
Tobit regression

Number of obs
LR chi2(6)
Prob  chi2
Pseudo R2

Log likelihood  -1263.3702






660
55.47
0.0000
0.0215

----------------------------------------------------------------------------ecolbs |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------ecoprc | -5.649516
.887358
-6.37
0.000
-7.391931
-3.907102
regprc |
5.575299
1.063999
5.24
0.000
3.486032
7.664566
lfaminc |
.4195658
.2354371
1.78
0.075
-.0427381
.8818696
educ |
.1002944
.0681569
1.47
0.142
-.0335384
.2341271
hhsize |
.0264861
.1320183
0.20
0.841
-.2327448
.2857171
num5_17 |
.2351291
.1963111
1.20
0.231
-.1503469
.6206051
_cons | -1.632596
1.314633
-1.24
0.215
-4.214007
.9488146
---------------------------------------------------------------------------/sigma |
3.431504
.1262031
3.183692
3.679316
----------------------------------------------------------------------------Obs. summary:
248 left-censored observations at ecolbs0
412
uncensored observations
0 right-censored observations

17.13. This extension has no practical effect on how we estimate an unobserved effects
Tobit or probit model, or how we estimate a variety of unobserved effects panel data models
with conditional normal heterogeneity. We simply have
T

ci  − T

−1

∑ t

  x̄ i   a i ≡   x̄ i   a i ,

t1

T

where  ≡ −T −1 ∑ t1  t . Of course, any aggregate time dummies explicitly get swept out of
x̄ i but they would usually be included in the equation.
An interesting follow-up question is: What if we standardize each x it by its cross-sectional
mean and variance at time t, and assume c i is related to the mean and variance of the
standardized vectors? In other words, let z it ≡ x it −  t  −1/2
, t  1, … , T, for each random
t
draw i from the population, where  t ≡ Varx it . Then, we might assume
c i |x i ~ Normal  z̄ i ,  2a 

385

(where, again, z it would not contain aggregate time dummies). This is the kind of scenario that
is handled by Chamberlain’s more general assumption concerning the relationship between c i
T

and x i : c i    ∑ t1 x ir  r  a i , where  r   −1/2
/T, r  1, 2, … , T. Alternatively, one could
r
estimate  t and  t for each t using the cross section observations x it : i  1, 2, … , N. The
̂ t , are consistent and
usual sample means and sample variance matrices, say ̂ t and 
̂ −1/2
N -asymptotically normal. Then, form ẑ it ≡ x it − ̂ t 
, and proceed with the usual Tobit
t
T

(or probit) unobserved effects analysis that includes the time averages ẑ i  T −1 ∑ t1 ẑ it . This is
̂t
a simple two-step estimation method, but accounting for the sample variation in ̂ t and 
analytically would be cumbersome. The panel bootstrap is an attractive alternative. Or, it may
̂ t , in which case one might ignore
be possible to use a much larger sample to obtain ̂ t and 
the sampling error in the first-stage estimates.
17.14. a. Because heteroskedasticity is only in the distribution of a i given x i , the density of
y it given x i , a i  is the same as that implied by (17.75) and (17.76), namely,
y it |x i , a i ~ Tobit  x it   x̄ i   a i ,  2u .
b. Let fy t |x i , a i ;  denote the Tobit density of y it |x i , a i implied by part a, where  contains
, , , and  2u . Then, under (17.78),
T

fy 1 , … , y T |x i , a i ;  

 f t y t |x i , a i ; .
t1

Therefore, to obtain fy 1 , … , y T |x i ; , we integrate out a i :
fy 1 , … , y T |x i ;  



T

 −  f t y t |x i , a i ; ha|x i ; ,  2a da,
t1

where ha|x i ; ,  2a  denotes the normal density with mean zero and variance  2a expx̄ i . The

386

(17.91)

log-likelihood is obtained by plugging the y it into (17.91) and taking the log.
c. The starting point is still equation (17.79), but the calculation of
Em  x t   x̄ i   a i ,  2u |x i  is complicated by the heteroskedasticity in Vara i |x̄ i .
Nevertheless, essentially the same argument used on page 542 shows that
Em  x t   x̄ i   a i ,  2u |x i   m  x t   x̄ i ,  2a expx̄ i    2u .
Given the MLEs, we can estimate the APEs from the average structural function:
N

ASFx t   N

−1

∑ m̂  x t ̂  x̄ i ̂, ̂ 2a expx̄ i ̂  ̂ 2u ;
i1

we would compute changes or derivatives with respect to the elements of x t . Incidentally, if we
drop assumption (17.78), we could use a pooled heteroskedastic Tobit procedure, and still
consistently estimate the APEs.
17.15. a. The Stata output is given below. The value of the log-likelihood is −17, 599. 96.
. use cps91
. tobit hours nwifeinc educ exper expersq age kidlt6 kidge6, ll(0)
Tobit regression

Number of obs
LR chi2(7)
Prob  chi2
Pseudo R2

Log likelihood  -17599.958






5634
645.55
0.0000
0.0180

----------------------------------------------------------------------------hours |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------nwifeinc | -.2444726
.0165886
-14.74
0.000
-.2769926
-.2119525
educ | -6.064707
22.73817
-0.27
0.790
-50.64029
38.51087
exper | -8.234015
22.74967
-0.36
0.717
-52.83214
36.36411
expersq | -.0178206
.0041379
-4.31
0.000
-.0259325
-.0097087
age |
8.53901
22.73703
0.38
0.707
-36.03435
53.11237
kidlt6 |
-14.0809
1.21084
-11.63
0.000
-16.45461
-11.70719
kidge6 | -1.593786
1.09917
-1.45
0.147
-3.748583
.5610116
_cons | -56.32579
136.3411
-0.41
0.680
-323.6069
210.9553
---------------------------------------------------------------------------/sigma |
28.90194
.3998526
28.11807
29.6858
----------------------------------------------------------------------------Obs. summary:
2348 left-censored observations at hours0
3286
uncensored observations
0 right-censored observations

387

b. The lognormal hurdle model – which has eight more parameters than the Tobit model –
does fit better in this application. The log likelihood – which properly account for the fact that
the linear regression for loghours i  is to be viewed as MLE for Dhours|x, hours  0 – is
about −16, 987. 50. The contribution of the probit is about −3, 538. 41 and the contribution of
the lognormal distribution conditional on hours  0 is −13, 449. 09. The log likelihood for the
Tobit is −17, 599. 96.
. probit inlf nwifeinc educ exper expersq age kidlt6 kidge6
Probit regression

Number of obs
LR chi2(7)
Prob  chi2
Pseudo R2

Log likelihood  -3538.4086






5634
576.67
0.0000
0.0753

----------------------------------------------------------------------------inlf |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------nwifeinc | -.0091475
.0006759
-13.53
0.000
-.0104722
-.0078227
educ | -.0626136
.9045369
-0.07
0.945
-1.835473
1.710246
exper |
-.157161
.9050879
-0.17
0.862
-1.931101
1.616779
expersq | -.0005574
.0001713
-3.25
0.001
-.000893
-.0002217
age |
.1631286
.9044966
0.18
0.857
-1.609652
1.935909
kidlt6 | -.4810832
.051688
-9.31
0.000
-.5823897
-.3797767
kidge6 |
.0409155
.0471194
0.87
0.385
-.0514367
.1332678
_cons | -1.489209
5.422855
-0.27
0.784
-12.11781
9.139393
----------------------------------------------------------------------------. gen lhours  log(hours)
(2348 missing values generated)
. glm lhours nwifeinc educ exper expersq age kidlt6 kidge6
Iteration 0:

log likelihood  -1954.9002

Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

632.3133243
632.3133243

Variance function: V(u)  1
Link function
: g(u)  u
Log likelihood







3286
3278
.1928961
.1928961
.1928961

[Gaussian]
[Identity]
AIC
BIC

 -1954.900228

 1.194705
 -25911.05

----------------------------------------------------------------------------|
OIM
lhours |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval

388

---------------------------------------------------------------------------nwifeinc | -.0018706
.0003327
-5.62
0.000
-.0025227
-.0012185
educ | -.2022625
.4403476
-0.46
0.646
-1.065328
.660803
exper | -.2074679
.4405366
-0.47
0.638
-1.070904
.655968
expersq | -.0001549
.0000812
-1.91
0.057
-.000314
4.33e-06
age |
.2112264
.4403162
0.48
0.631
-.6517775
1.07423
kidlt6 | -.1944414
.0222299
-8.75
0.000
-.2380113
-.1508715
kidge6 | -.1256763
.0199962
-6.29
0.000
-.1648681
-.0864845
_cons |
2.252439
2.640541
0.85
0.394
-2.922926
7.427805
----------------------------------------------------------------------------. sum lhours
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------lhours |
3286
3.497927
.4467688
0
4.787492
. di 3286*r(mean)
11494.189
. di -1954.9002 - 11494.189
-13449.089
. di -3538.4086 - 13449.089
-16987.498

c. The ET2T model is given below. Again, to properly compare its log likelihood, we must
N

subtract ∑ i11 loghours i  to obtain the final log likelihood. As must be the case, the ET2T
model fits better than the lognormal hurdle model, but the improvement is very slight. In fact,
the estimate of  is very small – about . 018 – and not statistically different from zero. The
likelihood ratio statistic gives the same result, producing p-value  . 862. Fortunately the
estimated coefficients are very similar across the two approaches, as we would hope with ̂ so
close to zero.
These findings are very different from what we found using the data in MROZ.RAW – see
Table 17.2. There, the estimate of  is an implausible −. 972. Without an exclusion restriction
(in either application) it is hard to be confident of the results. But with the current data set, we
are led to the lognormal hurdle model with all explanatory variables in the selection and
amount equations.
. heckman lhours nwifeinc educ exper expersq age kidlt6 kidge6,
select(inlf  nwifeinc educ exper expersq age kidlt6 kidge6)

389

Heckman selection model
(regression model with sample selection)

Log likelihood  -5493.294

Number of obs
Censored obs
Uncensored obs





5634
2348
3286

Wald chi2(7)
Prob  chi2




93.35
0.0000

----------------------------------------------------------------------------|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lhours
|
nwifeinc |
-.001911
.0003968
-4.82
0.000
-.0026886
-.0011333
educ | -.2023362
.4398277
-0.46
0.645
-1.064383
.6597103
exper | -.2079298
.4400233
-0.47
0.637
-1.07036
.6545001
expersq | -.0001578
.0000827
-1.91
0.056
-.0003198
4.21e-06
age |
.2117355
.4398047
0.48
0.630
-.6502658
1.073737
kidlt6 | -.1964925
.0247921
-7.93
0.000
-.245084
-.1479009
kidge6 | -.1255154
.0199914
-6.28
0.000
-.1646978
-.086333
_cons |
2.240756
2.63817
0.85
0.396
-2.929962
7.411474
---------------------------------------------------------------------------inlf
|
nwifeinc | -.0091462
.000676
-13.53
0.000
-.0104711
-.0078214
educ | -.0628333
.9045172
-0.07
0.945
-1.835654
1.709988
exper | -.1574369
.9050687
-0.17
0.862
-1.931339
1.616465
expersq | -.0005568
.0001713
-3.25
0.001
-.0008925
-.0002211
age |
.1633826
.9044773
0.18
0.857
-1.60936
1.936125
kidlt6 | -.4810173
.0516912
-9.31
0.000
-.5823302
-.3797043
kidge6 |
.0410785
.0471309
0.87
0.383
-.0512964
.1334534
_cons | -1.491111
5.422743
-0.27
0.783
-12.11949
9.13727
---------------------------------------------------------------------------/athrho |
.0178479
.0959507
0.19
0.852
-.1702121
.2059078
/lnsigma | -.8239347
.0123732
-66.59
0.000
-.8481857
-.7996837
---------------------------------------------------------------------------rho |
.017846
.0959202
-.1685871
.2030463
sigma |
.4387021
.0054281
.4281911
.4494711
lambda |
.0078291
.0420881
-.074662
.0903202
----------------------------------------------------------------------------LR test of indep. eqns. (rho  0):
chi2(1) 
0.03
Prob  chi2  0.8615
----------------------------------------------------------------------------. di -5493.294 - 11494.189
-16987.483
. di 2*(16987.498 - 16987.483)
.03

d. The estimates for the amount part of the truncated normal hurdel model are given below.
Because the participation equation is still the probit model we estimated earlier, we can
compare the log likelihood for the truncated normal regression to that from the lognormal
estimation in part b. The former is −12, 445. 76 and we already computed the latter as
−13, 449. 09. Thus, in this example the TNH model fits substantially better than the LH model.
390

The full log likelihood for the TNH model is −15, 984. 17 and this is much larger than that for
the Tobit (a special case), −17, 599. 96.
. truncreg hours nwifeinc educ exper expersq age kidlt6 kidge6, ll(0)
(note: 2348 obs. truncated)
Fitting full model:
lower 
upper 
Log likelihood 
Limit:

Number of obs 
3286
Wald chi2(7)  132.08
Prob  chi2
 0.0000

0
inf
-12445.76

----------------------------------------------------------------------------hours |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------nwifeinc | -.0439736
.0081584
-5.39
0.000
-.0599638
-.0279835
educ | -9.183178
11.12374
-0.83
0.409
-30.98531
12.61896
exper | -9.426741
11.12822
-0.85
0.397
-31.23765
12.38417
expersq | -.0024584
.0019888
-1.24
0.216
-.0063564
.0014396
age |
9.470886
11.12299
0.85
0.395
-12.32978
31.27155
kidlt6 | -4.779305
.5444546
-8.78
0.000
-5.846417
-3.712194
kidge6 | -3.370223
.4896076
-6.88
0.000
-4.329837
-2.41061
_cons | -21.34309
66.70579
-0.32
0.749
-152.084
109.3979
---------------------------------------------------------------------------/sigma |
10.72244
.1347352
79.58
0.000
10.45836
10.98651
----------------------------------------------------------------------------. di -3538.4086 - 12445.76
-15984.169

17.16. a. Write c i    x̄ i   a i and substitute to get
y ∗it  x it     x̄ i   a i  u it .
Now, conditional on x i , a i , y it follows a standard two-limit Tobit model. Therefore, the
density is
fy t |x i , a i ;   q 1 − x it  −  − x̄ i  − a i / u  1y t q 1 
  −1
̄ i   a i / u  1q 1 y t q 2 
u y t − x it     x
 −q 2  x it     x̄ i   a i / u  1y t q 2 
Byt the conditional independence assumption, the joint density of y i1 , y i2 , . . . , y iT  conditional
on x i , a i  is

391

T

 fy t |x i , a i ; .
t1

Now we integrate out a i to get the joint density of y i1 , y i2 , . . . , y iT  given x i :


 −

T

 fy t |x i , a;   −1
a a/ a da.
t1

Now the log likelihood for a random draw i is just
ℓ i  ≡ log

T



 −

 fy it |x i , a;   −1
a a/ a da
t1

where  is the vector of all parameters, including  2a . As usual, we sum across all i to get the
log likelihood for the entire cross section.
b. This is no different from any of the other CRE models that we have covered. We can
easily find Ec i  and Varc i  from c i    x̄ i   a i because Ea i |x i   0 and Vara i |x i    2a .
In fact,
Ec i   E  x̄ i     Ex̄ i 
and so a consistent estimator of Ec i  is
N

̂ 
̂ c  

N

−1

∑ x̄ i

̂

i1

̂ and ̂ are the MLEs.
where 
Next,
Varc i   Var  x̄ i   Vara i 
  ′ Varx̄ i    2a ,
where we use the fact that a i and x̄ i are uncorrelated. So a consistent estimator is

392

N

′
̂ 2c  ̂ N −1 ∑x̄ i − x̄  ′ x̄ i − x̄  ̂  ̂ 2a
i1

N

where x̄  N −1 ∑ i1 x̄ i .
c. We can get the average structural function by slightly modifying equation (17.66). First,
the conditional mean is
Ey it |x it , c i   q 1 q 1 − x it  − c i / u 
 −q 2  x it   c i / u  − q 1 − x it  − c i / u   gq 1 , q 2 , x it   c i ,  2u 
 q 2 −q 2  x it   c i / u 
where
gq 1 , q 2 , z,  2  ≡ z 

q 2 − z/ u  − q 1 − z/ u 
.
q 2 − z/ u  − q 1 − z/ u 

The ASF is obtained as a function of x t by averaging out c i . But we can use iterated
expectations, as usual, by first conditioning on x̄ i and then averaging out x̄ i :
E c i mx t , c i   E x̄ i Emx t , c i |x̄ i 
where mx t , c  Ey it |x it  x t , c i  c. Using the same argument from the one-limit CRE
Tobit model,
hx t , x̄ i  ≡ Emx t , c i |x̄ i   q 1 q 1 − x t  −  − x̄ i / v 
 −q 2  x t     x̄ i / v  − q 1 − x t  −  − x̄ i / v 
 gq 1 , q 2 , x t     x̄ i ,  2v 
 q 2 −q 2  x t     x̄ i / v 
where  2v   2a   2u . The ASF is consistently estimated as
N

ASFx t   N −1 ∑ ĥx t , x̄ i 
i1

393

where ĥ,  denotes plugging in the MLEs of all estimates. Now take derivatives and changes
with respect to x t .
d. Without assumption (17.78), we can just use a pooled two-limit Tobit analysis to
estimate , , , and  2v . As in the standard Tobit case, we cannot separately estimate  2a and
 2u . But the APEs are still identified as they depend only on  2v , as shown in part c.
17.17. a. Plug in the expressions for c i1 and c i2 to get
y it1  max0,  1 y it2  z it1  1   1  z̄ i  1  a i1  u it1 
y it2  z it  2   2  z̄ i  2  a i2  u it2
or
y it1  max0,  1 y it2  z it1  1   1  z̄ i  1  v it1 
y it2  z it  2   2  z̄ i  2  v it2
where v it  a i1  u it1 and v it2  a i2  u it2 . Given the assumptions on Du it1 , u it2 |z i , a i  and
Da i1 , a i2 |z i , it follows that Dv it1 , v it2 |z i  Dv it1 , v it2  is bivariate normal with mean zero.
Therefore, we can write
v it1   1 v it2  e it1
De it1 |z i , v it2   Ee it1   Normal0,  2e 1 .
It follows we can write
y it1  max0,  1 y it2  z it1  1   1  z̄ i  1   1 v it2  e it1 
De it1 |z i , y i,t2 , v it2   Normal0,  2e 1 
and now a pooled two-step method is immediate. First, obtain the residuals v̂ it2 from the pooled
regression
y it2 on z it , 1, z̄ i , t  1, . . . , T; i  1, . . . , N.

394

Then use pooled Tobit of
y it1 on y it2 , z it1 , 1, z̄ i , v̂ it2
to estimate  1 ,  1 ,  1 ,  1 ,  1 , and  2e 1 .
Incidentally, the statement of the problem said that y it2  will not be strictly exogenous in
the estimable equation. While that is true of the previously proposed solution, in other
approaches y it2  can be rendered strictly exogenous. Here is one possibility. Let
v i2  v i12 , . . . , v iT2  be the entire history on the reduced form errors. Then, given the previous
assumptions, Dv it1 |z i , v i2  Dv it1 |v i2 . Because v it1  a i1  u it1 , it is reasonable to assume a
Chamberlain-Mundlak representation, for example,
v it1   1 v it2   1 v̄ i2  e it1
where now e it1 is independent of z i , v i2  and therefore of z i , v i2 , y i2 , where
y i2  y i12 , . . . , y iT2 . This means that in the equation
y it1  max0,  1 y it2  z it1  1   1  z̄ i  1   1 v it2   1 v̄ i2  e it1 ,
y ir2 , z ir , v ir2 : r  1, . . . , T is strictly exogenous with respect to e it1 . The CF approach changes

in that we add v̄ i2 as an additional explanatory variable (along with v̂ it2 ) in using pooled Tobit.
Because of strict exogeneity, approaches that attempt to exploit the serial dependence in the
scores are now possible.
b. As usual, the two-step nature of the estimation needs to be accounted for by using either
the delta method or the panel bootstrap. In using the delta method, the serial dependence in the
scores should be accounted for. It is automatically accounted for with the panel bootstrap
because the cross section units are resampled.
c. We have used this approach several times. Let mz,  2  denote the unconditional mean

395

function for the standard Tobit model. Then
ASFy t2 , z t1   E z̄ i ,v it2  m 1 y t2  z t1  1   1  z̄ i  1   1 v it2 ,  2e1 
and so a consistent estimator is
N

̂ 1  z̄ i ̂ 1  ̂ 1 v̂ it2 , ̂ 2e1 .
ASFy t2 , z t1   N −1 ∑ m̂ 1 y t2  z t1 ̂ 1  
i1

As usual, the estimated APEs are obtained by taking derivatives or changes with respect to
y t2 , z t1 .
17.18. a. Once we assume z is exogenous in the structural equation – and Eu 1 |z  0
ensures exogeneity – then we only need the rank condition. The assumption that Ez ′ z is
nonsingular is not usually restrictive. The important condition with a single endogenous
explanatory variable is
Ly 2 |z ≠ Ly 2 |z 1 ,
so there is at least one element of z not in z 1 that explains variation in y 2 .
b. We can draw on the optimal instrument variables results from Section 8.6. The condition
Eu 1 |z  0 ensures that any function of z is a valid instrumental variable candidate, and also
implies that Eu 21 |z  Varu 1 |z. Because Eu 21 |z is constant, from Theorem 8.5 the optimal
IVs are
Ey 2 |z, z 1 .
If we think Dy 2 |z follows a standard Tobit then we should obtain Ey 2 |z from the Tobit
model. Recall that if
Dy 2 |z  Tobitz 2 ,  22 
then
396

Ey 2 |z  z 2 / 2 z 2   2 z 2 / 2 
Therefore, if we run Tobit in a first stage, we get
̂ i2 ≡ Êy i2 |z i   z i ̂ 2 /̂ 2 z i ̂ 2  ̂ 2 z i ̂ 2 /̂ 2 
m
̂ i2 , z i1  in the equation
and then use IVs m
y i1   1 y i2  z i1  1  u i1
by IV. This approach just identifies the parameters. We can get overidentification (if we have
enough elements of z i ) by using all of z i in place of z i1 .
Provided we maintain Eu 1 |z  0, using Tobit fitted values as instruments is no less (and
no more) robust than using 2SLS. As mentioned previously, any function of z i is valid as a
potential instrument. Even if the Tobit model is incorrect, we know the quasi-MLEs converge
very generally. Call the plims  ∗2 and  ∗2 and define
m ∗i2 ≡ z i  ∗2 / ∗2 z i  ∗2   ∗2 z i  ∗2 / ∗2 ,
which is just a function of z i . Ruling out perfect collinearity in m ∗i2 , z i1 , the rank condition is
Ly i2 |m ∗i2 , z i1  ≠ Ly i2 |z i1 ,
which simply means that m ∗i2 should have some partial correlation with z i1 , something we
would expect quite generally if z i2 is partially correlated with y i2 .
̂ i2 as an instrument for y i2 is preferred to using it as a regressor in place of m
̂ i2 . If
Using m
̂ i2 as a regressor then we are effectively assuming
we use m
Ey 2 |z  z 2 / 2 z 2   2 z 2 / 2  (and that we have consistent estimators of the
parameters in this mean). Generally, the estimates of  1 and  1 would be inconsistent of the
̂ i2 as an instrument, the reduced implicit reduced
Tobit model for y 2 is misspecified. When m

397

form for the IV estimation is
Ly 2 |m ∗i2 , z i1    2 m ∗i2  z i1  1
and we do not need  2  1 and  1  0, as the plug-in-regressor method essentially does.
c. We can write
y1  z11  1y2  1v2  e1
Ee 1 |z, y 2 , v 2   0
It follows that
Ey 1 |z, y 2   z 1  1   1 y 2   1 Ev 2 |z, y 2 
and we can compute Ev 2 |z, y 2  given that Dy 2 |z  Tobitz 2 ,  22 . In fact, as shown in Vella
(1993, International Economic Review),
Ev 2 |z, y 2   1y 2  0v 2 − 1y 2  0 2

z 2 / 2 
1 − z 2 / 2 

 1y 2  0v 2 − 1y 2  0 2 −z 2 / 2 
where   / is the inverse Mills ratio. (This is an example of a generalized
residual.)
Given the Tobit MLEs, we can easily construct
Êv i2 |z i , y i2   1y i2  0v̂ i2 − 1y i2  0̂ 2 −z i ̂ 2 /̂ 2 
in a first stage, and then in a second stage run the OLS regression
y i1 on z i1 , y i2 , 1y i2  0v̂ i2 − 1y i2  0̂ 2 −z i ̂ 2 /̂ 2 
to consistently estimate  1 ,  1 , and  1 .
Because the CF approach is based on Ey 1 |z, y 2 , nothing important changes if we start
with

398

y 1  g 1 z 1 , y 2  1  u 1 .
The same reasoning as before gets us to
Ey 1 |z, y 2   g 1 z 1 , y 2  1   1 1y 2  0v 2 − 1y 2  0 2 −z 2 / 2 
and so adding the same CF as before works for consistently estimating  1 . Of course, the
interpretation of  1 depends on the nature of the functions in g 1 z 1 , y 2 .
d. The 2SLS estimator that effectively ignores the nature of y 2 is simple and fully robust –
assuming we have at least one valid instrument for y 2 . Standard errors (robust to
heteroskedasticity) are easy to obtain. Its primary drawback is that it may be (asymptotically)
inefficient compared with the other methods. An additional shortcoming is that if we use
general functions g 1 z 1 , y 2  we need to decide on instruments for any function that includes y 2 .
[Remember we are generally not allowed to plug in a fitted value to obtain g 1 z i1 , ŷ i2  and then
regress y i1 on g 1 z i1 , ŷ i2 .]
The method of using the Tobit fitted value as the IV for y 2 is just as robust as 2SLS
estimator yet it exploits the corner solution nature of y 2 . It need not be more (asymptotically)
efficient than 2SLS, but it could be even if the Tobit model for y 2 is misspecified [or Varu 1 |z
is homoskedastic, or both]. That we have estimated the instruments in a first stage can be
ignored in the N -asymptotic distribution of the IV estimator. Like the 2SLS estimator,
having general functions g 1 z 1 , y 2  means we would have to obtain IVs for all endogenous
functions. This is almost always possible but is not always obvious.
The CF method is simple to compute but the standard errors generally have to account for
the two-step estimation unless  1  0. (The CF method provides a simple test of the null that
y 2 is endogenous: just use a heteroskedasticity-robust t statistic for ̂ 1 .) Another drawback to

399

the CF method is that it is derived assuming the Tobit model for y 2 holds. Generally, it is
inconsistent if the Tobit model fails (just like using Tobit fitted values as regressors rather than
instruments). An advantage of the CF method is that, as discussed in part c, it is easily applied
for general functions g 1 z 1 , y 2 . In such cases, the CF method is likely to be more efficient
asymptotically than 2SLS or the IV method described in part b.
e. If we assume joint normality of u 1 , v 2  (and independence from z) then MLE becomes
attractive. It will give the asymptotically efficient estimators and there is no two-step
estimation issue to deal with (as in the CF case). The log likelihood is a bit tricky to obtain
because it depends on Dy 1 |y 2 , z. We already know that Dy 2 |z follows a Tobit. We also
know Dy 1 |v 2 , z follows a classical linear regression model with mean z 1  1   1 y 2   1 v 2 and
variance  2e 1 . For y 2  0, Dy 1 |y 2 , z Dy 1 |v 2 , z. For y 2  0, we have to integrate over
v 2 ≤ −z 2 , just like in Problem 17.6. When we have to two densities, we use, for each i,
logf 1 y i1 |y i2 , z i ;   logf 2 y i2 |z i ;  2 ,  22 
as the log likelihood.
17.19. a. Because of the conditional independence assumption we have
G

fy 1 , . . . , y G |x, c;  o    f g y g |x, c;  go 
g1

for dummy arguments y 1 , . . . , y G .
b. To obtain the density of y 1 , . . . , y G  given x we integrate out c:
gy 1 , . . . , y G |x;  o ,  o  



G

 f g y g |x, c;  go  hc|x;  o dc
g1

where, in general, the integral is a multiple integral. Also, we have indicated c as a continuous
random vector but it need not be.
400

c. The log likelihood for a random draw i is simply
ℓ i   log



G

 f g y ig |x i , c;  g  hc|x i ; dc
g1

where  contains all parameters.
17.20. a. The Stata output is given below. The signs of the coefficients are generally what
we expect: lagged hours has a positive coefficient, as does initial hours in 1980. Thus,
unobserved heterogeneity that positively affects hours worked in 1980 also positively affects
hours contemporaneously. The variables nwifeinc, ch0_2, and ch3_5 all have negative and
statistically significant coefficients. The one slight puzzle is that the older children variable has
a positive and just statistically significant coefficient.
use \mitbook1_2e\statafiles\psid80_92, clear
tsset id year
* Lagged dependent variable:
bysort id (year): gen hours_1  L.hours
* Put initial condition in years 81-92:
by id: gen hours80  hours[1]
* Create exogenous variables for years 81-92:
forv i81/92 {
by id: gen nwifeinc‘i’  nwifeinc[‘i’-80]
}
forv i81/92 {
by id: gen ch0_2_‘i’  ch0_2[‘i’-80]
}
forv i81/92 {
by id: gen ch3_5_‘i’  ch3_5[‘i’-80]
}
forv i81/92 {
by id: gen ch6_17_‘i’  ch6_17[‘i’-80]
}
forv i81/92 {
by id: gen marr‘i’  marr[‘i’-80]
}
xttobit hours hours_1 hours80 nwifeinc nwifeinc81-nwifeinc92
ch0_2 ch0_2_81-ch0_2_92 ch3_5 ch3_5_81-ch3_5_92
ch6_17 ch6_17_81-ch6_17_92 marr marr81-marr92 y82-y92, ll(0) re
note:
note:
note:
note:
note:

marr86
marr89
marr90
marr91
marr92

omitted
omitted
omitted
omitted
omitted

because
because
because
because
because

of
of
of
of
of

collinearity
collinearity
collinearity
collinearity
collinearity

401

Random-effects tobit regression
Group variable: id
Random effects u_i ~Gaussian

Log likelihood




10776
898

Obs per group: min 
avg 
max 

12
12.
12




7997.27
0.0000

Number of obs
Number of groups

Wald chi2(73)
Prob  chi2

 -62882.574

----------------------------------------------------------------------------hours |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------hours_1 |
.7292676
.0119746
60.90
0.000
.7057978
.7527375
hours80 |
.2943114
.0181831
16.19
0.000
.2586731
.3299496
nwifeinc | -1.286033
.3221528
-3.99
0.000
-1.917441
-.6546256
nwifeinc81 |
.6329715
1.08601
0.58
0.560
-1.49557
2.761513
nwifeinc82 |
.1812886
1.914677
0.09
0.925
-3.57141
3.933987
nwifeinc83 | -.6567582
1.822493
-0.36
0.719
-4.228778
2.915262
nwifeinc84 | -.9568491
1.344172
-0.71
0.477
-3.591379
1.677681
nwifeinc85 | -1.169828
1.186202
-0.99
0.324
-3.494742
1.155085
nwifeinc86 |
.437133
1.142004
0.38
0.702
-1.801153
2.675419
nwifeinc87 |
-2.53217
1.067478
-2.37
0.018
-4.624388
-.4399525
nwifeinc88 | -.8224415
.6884551
-1.19
0.232
-2.171789
.5269057
nwifeinc89 |
1.325135
.792212
1.67
0.094
-.2275717
2.877842
nwifeinc90 |
.0811052
.5898146
0.14
0.891
-1.07491
1.237121
nwifeinc91 |
1.550942
.861745
1.80
0.072
-.1380467
3.239932
nwifeinc92 | -.6307469
.7778347
-0.81
0.417
-2.155275
.893781
ch0_2 | -146.0974
21.04471
-6.94
0.000
-187.3443
-104.8506
ch0_2_81 |
170.7185
93.39678
1.83
0.068
-12.33577
353.7729
ch0_2_82 |
89.33674
97.13967
0.92
0.358
-101.0535
279.727
ch0_2_83 |
86.23234
100.9511
0.85
0.393
-111.6281
284.0928
ch0_2_84 | -68.59569
100.3924
-0.68
0.494
-265.3612
128.1699
ch0_2_85 | -11.17728
96.67042
-0.12
0.908
-200.6478
178.2933
ch0_2_86 |
83.53639
108.6359
0.77
0.442
-129.386
296.4588
ch0_2_87 | -77.82159
110.9494
-0.70
0.483
-295.2784
139.6352
ch0_2_88 | -84.03353
141.8912
-0.59
0.554
-362.1352
194.0681
ch0_2_89 |
48.15522
211.0668
0.23
0.820
-365.528
461.8385
ch0_2_90 |
17.49295
102.1897
0.17
0.864
-182.7952
217.7811
ch0_2_91 |
123.7578
96.82641
1.28
0.201
-66.01852
313.534
ch0_2_92 | -48.17428
84.53394
-0.57
0.569
-213.8578
117.5092
ch3_5 | -80.13216
17.85232
-4.49
0.000
-115.1221
-45.14226
ch3_5_81 |
39.44899
62.21243
0.63
0.526
-82.48514
161.3831
ch3_5_82 |
102.3494
72.50917
1.41
0.158
-39.766
244.4647
ch3_5_83 | -38.86165
74.09378
-0.52
0.600
-184.0828
106.3595
ch3_5_84 | -101.8966
94.20263
-1.08
0.279
-286.5304
82.73714
ch3_5_85 | -4.967801
99.28115
-0.05
0.960
-199.5553
189.6197
ch3_5_86 | -25.96859
100.6704
-0.26
0.796
-223.279
171.3418
ch3_5_87 |
5.59682
98.3939
0.06
0.955
-187.2517
198.4453
ch3_5_88 |
46.38591
93.80288
0.49
0.621
-137.4644
230.2362
ch3_5_89 | -95.69263
129.6341
-0.74
0.460
-349.7709
158.3856
ch3_5_90 |
43.70922
129.4244
0.34
0.736
-209.9579
297.3763
ch3_5_91 |
147.7391
143.4973
1.03
0.303
-133.5105
428.9886
ch3_5_92 | -166.5773
214.5918
-0.78
0.438
-587.1694
254.0149
ch6_17 |
22.18895
10.08538
2.20
0.028
2.421976
41.95593
ch6_17_81 |
4.64258
37.68437
0.12
0.902
-69.21744
78.5026
ch6_17_82 |
64.27872
55.13925
1.17
0.244
-43.79223
172.3497
ch6_17_83 | -66.82245
57.25136
-1.17
0.243
-179.033
45.38815

402

ch6_17_84 |
1.173452
56.00241
0.02
0.983
-108.5893
110.9362
ch6_17_85 |
6.738214
54.27217
0.12
0.901
-99.63328
113.1097
ch6_17_86 |
85.64549
57.28103
1.50
0.135
-26.62327
197.9142
ch6_17_87 | -65.96152
62.6244
-1.05
0.292
-188.7031
56.78006
ch6_17_88 |
19.1112
56.21565
0.34
0.734
-91.06945
129.2918
ch6_17_89 |
4.85883
61.37184
0.08
0.937
-115.4278
125.1454
ch6_17_90 |
16.18911
60.09357
0.27
0.788
-101.5921
133.9703
ch6_17_91 | -21.25498
55.55783
-0.38
0.702
-130.1463
87.63636
ch6_17_92 | -7.632119
53.88032
-0.14
0.887
-113.2356
97.97137
marr | -199.1315
144.719
-1.38
0.169
-482.7755
84.51247
marr81 |
127.5178
356.477
0.36
0.721
-571.1642
826.1998
marr82 | -13.59679
491.133
-0.03
0.978
-976.1997
949.0062
marr83 | -507.5586
434.9469
-1.17
0.243
-1360.039
344.9217
marr84 |
1318.284
564.6247
2.33
0.020
211.6404
2424.928
marr85 | -326.1983
585.7084
-0.56
0.578
-1474.166
821.769
marr86 | (omitted)
marr87 |
131.824
331.72
0.40
0.691
-518.3353
781.9832
marr88 | -491.7295
306.7196
-1.60
0.109
-1092.889
109.4299
marr89 | (omitted)
marr90 | (omitted)
marr91 | (omitted)
marr92 | (omitted)
y82 | -32.79071
25.88734
-1.27
0.205
-83.52896
17.94755
y83 |
20.40184
25.84829
0.79
0.430
-30.25988
71.06355
y84 |
105.7757
25.7722
4.10
0.000
55.2631
156.2883
y85 |
26.36698
25.95325
1.02
0.310
-24.50046
77.23441
y86 |
26.82807
25.99402
1.03
0.302
-24.11928
77.77542
y87 | -.1477861
26.16878
-0.01
0.995
-51.43764
51.14207
y88 |
21.84475
26.28302
0.83
0.406
-29.66903
73.35853
y89 |
33.76287
26.39745
1.28
0.201
-17.97518
85.50092
y90 |
30.54594
26.52445
1.15
0.249
-21.44102
82.5329
y91 |
29.17601
26.64107
1.10
0.273
-23.03953
81.39155
y92 | -27.66915
26.97277
-1.03
0.305
-80.53481
25.19651
_cons | -165.6397
47.85094
-3.46
0.001
-259.4258
-71.85356
---------------------------------------------------------------------------/sigma_u |
310.4876
12.44431
24.95
0.000
286.0972
334.878
/sigma_e |
508.4561
4.327479
117.49
0.000
499.9744
516.9378
---------------------------------------------------------------------------rho |
.2716099
.0164159
.2404141
.3046996
----------------------------------------------------------------------------Observation summary:

2835 left-censored observations
7941
uncensored observations
0 right-censored observations

b. The Stata commands below produce the scale factor for the APE of a continuous
explanatory variable, evaluated at hours t−1  0. All other variables are averaged out, and the
scale factor is for 1992. The APE for nwifeinc in 1992 is about −. 742. Because nwifeinc is in
$1,000s, the coefficient implies that a $10,000 increase in other sources of income decreases
estimated annual hours by about 7.4. This is a small economic effect given that the average

403

hours in 1992 is about 1,155, and a $10,000 increase is fairly large.
. predict xbh, xb
(898 missing values generated)
. gen xbh_h0  xbh - _b[hours_1]*hours_1
(898 missing values generated)
. gen scale  normal(xbh_h0/sqrt(_b[/sigma_u]^2  _b[/sigma_e]^2))
(898 missing values generated)
. sum scale if y92
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------scale |
898
.5769774
.1692471
.0649065
.9402357
. di .5769774*_b[nwifeinc]
-.74201219
. sum hours nwifeinc if y92
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------hours |
898
1155.318
899.656
0
3916
nwifeinc |
898
43.57829
44.2727 -7.249999
601.504

c. Because ch0_2 is a discrete variable, we compute the difference in the conditional mean
function and then average. The APE in 1992 in moving from zero to one small children is
about −116. 47, which means average annual hours fall by about 116.5 hours.
. gen xbh_c0  xbh - _b[ch0_2]*ch0_2
(898 missing values generated)
. gen xbh_c1  xbh_c0  _b[ch0_2]
(898 missing values generated)
. gen mean0  normal(xbh_c0/sqrt(_b[/sigma_u]^2  _b[/sigma_e]^2))*xbh_c0  sqrt
 en(xbh_c0/sqrt(_b[/sigma_u]^2  _b[/sigma_e]^2))
(898 missing values generated)
. gen mean1  normal(xbh_c1/sqrt(_b[/sigma_u]^2  _b[/sigma_e]^2))*xbh_c1  sqrt
 en(xbh_c1/sqrt(_b[/sigma_u]^2  _b[/sigma_e]^2))
(898 missing values generated)
. gen diff  mean1 - mean0
(898 missing values generated)
. sum diff if y92
Variable |
Obs
Mean
Std. Dev.
Min
Max
---------------------------------------------------------------------

404

diff |

898

-116.4689

38.50869

405

-146.0974

-7.479618

Solutions to Chapter 18 Problems
18.1. a. This is a simple problem in univariate calculus. Write q ≡  o log −  for
  0. Then dq/d ≡  o / − 1, so    o uniquely sets the derivative to zero. The second
derivative of q is − o  −2  0 for all   0, so the sufficient second order condition for a
maximum is satisfied.
b. For the exponential case, q ≡ Eℓ i   − o / − log. The first order condition is
 o  −2 −  −1  0, which is uniquely solved by    o . The second derivative is −2 o  −3   −2 ,
−2
−2
which, when evaluated at  o , gives −2 −2
o   o  − o  0.

18.2. When mx,   expx, we have s i ̂  expx i ̂x ′i û i / expx i ̂  x ′i û i , where
û i  y i − expx i ̂. Further, the Hessian H i  does not depend on y i , and
A i ̂  −H i ̂  expx i ̂

2 ′
x i x i / expx i ̂

 expx i ̂x ′i x i .

Therefore, we can write equation (18.14) as
N

∑ expx i ̂x ′i x i
i1

−1

N

∑ û 2i x ′i x i
i1

N

∑ expx i ̂x ′i x i

−1

.

i1

18.3. a. The Stata output is below. Neither the price nor income variable is significant at
any reasonable significance level, although the coefficient estimates are the expected sign. It
does not matter whether we use the usual or robust standard errors. The two variables are
jointly insignificant, too, using the usual and heteroskedasticity-robust tests (p-values  .490,
.344, respectively).
. use smoke
. reg cigs lcigpric lincome restaurn white educ age agesq
Source |
SS
df
MS
------------------------------------------Model | 8029.43631
7 1147.06233

406

Number of obs 
F( 7,
799) 
Prob  F


807
6.38
0.0000

Residual | 143724.246
799 179.880158
------------------------------------------Total | 151753.683
806 188.280003

R-squared

Adj R-squared 
Root MSE


0.0529
0.0446
13.412

----------------------------------------------------------------------------cigs |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lcigpric | -.8509044
5.782321
-0.15
0.883
-12.20124
10.49943
lincome |
.8690144
.7287636
1.19
0.233
-.561503
2.299532
restaurn | -2.865621
1.117406
-2.56
0.011
-5.059019
-.6722234
white | -.5592363
1.459461
-0.38
0.702
-3.424067
2.305594
educ | -.5017533
.1671677
-3.00
0.003
-.829893
-.1736135
age |
.7745021
.1605158
4.83
0.000
.4594197
1.089585
agesq | -.0090686
.0017481
-5.19
0.000
-.0124999
-.0056373
_cons | -2.682435
24.22073
-0.11
0.912
-50.22621
44.86134
----------------------------------------------------------------------------. test lcigpric lincome
( 1)
( 2)

lcigpric  0
lincome  0
F(

2,
799) 
Prob  F 

0.71
0.4899

. reg cigs lcigpric lincome restaurn white educ age agesq, robust
Linear regression

Number of obs
F( 7,
799)
Prob  F
R-squared
Root MSE







807
9.38
0.0000
0.0529
13.412

----------------------------------------------------------------------------|
Robust
cigs |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lcigpric | -.8509044
6.054396
-0.14
0.888
-12.7353
11.0335
lincome |
.8690144
.597972
1.45
0.147
-.3047672
2.042796
restaurn | -2.865621
1.017275
-2.82
0.005
-4.862469
-.868774
white | -.5592363
1.378283
-0.41
0.685
-3.26472
2.146247
educ | -.5017533
.1624097
-3.09
0.002
-.8205533
-.1829532
age |
.7745021
.1380317
5.61
0.000
.5035545
1.04545
agesq | -.0090686
.0014589
-6.22
0.000
-.0119324
-.0062048
_cons | -2.682435
25.90194
-0.10
0.918
-53.52632
48.16145
----------------------------------------------------------------------------. test lcigpric lincome
( 1)
( 2)

lcigpric  0
lincome  0
F(

2,
799) 
Prob  F 

1.07
0.3441

b. While the price variable is still highly insignificant (p-value  .46), the income variable,

407

based on the usual Poisson standard errors, is very significant: t  5. 11. Both estimates are
elasticities: the estimate price elasticity is −. 106 and the estimated income elasticity is . 104.
Incidentally, if you drop restaurn – a binary indicator for restaurant smoking restrictions at the
state level – then lcigpric becomes much more significant (using the MLE standard errors). In
this data set, both cigpric and restaurn vary only at the state level, and , not surprisingly, they
are significantly correlated. (States that have restaurant smoking restrictions also have higher
average cigarette prices, on the order of 2.9%.)
. poisson cigs lcigpric lincome restaurn white educ age agesq
Poisson regression
Log likelihood 

Number of obs
LR chi2(7)
Prob  chi2
Pseudo R2

-8111.519






807
1068.70
0.0000
0.0618

----------------------------------------------------------------------------cigs |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lcigpric | -.1059607
.1433932
-0.74
0.460
-.3870061
.1750847
lincome |
.1037275
.0202811
5.11
0.000
.0639772
.1434779
restaurn | -.3636059
.0312231
-11.65
0.000
-.4248021
-.3024098
white | -.0552012
.0374207
-1.48
0.140
-.1285444
.0181421
educ | -.0594225
.0042564
-13.96
0.000
-.0677648
-.0510802
age |
.1142571
.0049694
22.99
0.000
.1045172
.1239969
agesq | -.0013708
.000057
-24.07
0.000
-.0014825
-.0012592
_cons |
.3964494
.6139626
0.65
0.518
-.8068952
1.599794
-----------------------------------------------------------------------------

c. The GLM estimate of  is about ̂  4. 51. This means all of the Poisson standard errors
should be multiplied by this factor, as is done using the glm command in Stata, with the
sca(x2) option. The t statistic on lcigpric is now very small (−. 16), and that on lincome falls
to 1. 13 – much more in line with the linear model t statistic (1.19 with the usual standard
errors). Clearly, using the maximum likelihood standard errors is very misleading in this
example. With the GLM standard errors, the restaurant restriction variable, education, and the
age variables are still significant. (There is no race effect, conditional on the other covariates.)
. glm cigs lcigpric lincome restaurn white educ age agesq, family(poisson)
sca(x2)

408

Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

14752.46933
16232.70987

Variance function: V(u)  u
Link function
: g(u)  ln(u)
Log likelihood







18.46367
20.31628




20.12272
9404.504

807
799

[Poisson]
[Log]
AIC
BIC

 -8111.519022

----------------------------------------------------------------------------|
OIM
cigs |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lcigpric | -.1059607
.6463244
-0.16
0.870
-1.372733
1.160812
lincome |
.1037275
.0914144
1.13
0.257
-.0754414
.2828965
restaurn | -.3636059
.1407338
-2.58
0.010
-.6394391
-.0877728
white | -.0552011
.1686685
-0.33
0.743
-.3857854
.2753831
educ | -.0594225
.0191849
-3.10
0.002
-.0970243
-.0218208
age |
.1142571
.0223989
5.10
0.000
.0703561
.158158
agesq | -.0013708
.0002567
-5.34
0.000
-.001874
-.0008677
_cons |
.3964493
2.76735
0.14
0.886
-5.027457
5.820355
----------------------------------------------------------------------------(Standard errors scaled using square root of Pearson X2-based dispersion.)
. di sqrt(20.31628)
4.5073584

d. The usual LR statistic is about LR  2  8125. 291 − 8111. 519  27. 54, which is a
very large value in a  22 distribution (p-value ≈ 0). The QLR statistic divides the usual LR
statistic by ̂ 2  20. 32, so QLR  1. 36 (p-value ≈. 51. As expected, the QLR statistic shows
that the variables are jointly insignificant, while the LR statistic shows strong statistical
significance.
. poisson cigs restaurn white educ age agesq
Iteration 0:
Iteration 1:
Iteration 2:

log likelihood  -8125.618
log likelihood  -8125.2907
log likelihood  -8125.2906

Poisson regression

Number of obs
LR chi2(5)
Prob  chi2
Pseudo R2

Log likelihood  -8125.2906






807
1041.16
0.0000
0.0602

----------------------------------------------------------------------------cigs |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------restaurn | -.3545336
.0308796
-11.48
0.000
-.4150564
-.2940107

409

white | -.0618025
.037371
-1.65
0.098
-.1350483
.0114433
educ | -.0532166
.0040652
-13.09
0.000
-.0611842
-.0452489
age |
.1211174
.0048175
25.14
0.000
.1116754
.1305594
agesq | -.0014458
.0000553
-26.14
0.000
-.0015543
-.0013374
_cons |
.7617484
.1095991
6.95
0.000
.5469381
.9765587
----------------------------------------------------------------------------. di 2*(8125.291 - 8111.519)
27.544
. di 27.54/20.32
1.355315
. di chi2tail(2,1.36)
.50661699

e. Using the robust standard errors does not change any conclusions; in fact, most
explanatory variables become slightly more significant than when we use the GLM standard
errors. In this example, it is the adjustment by ̂  1 that makes the most difference. Having
fully robust standard errors has no additional effect once we account for the severe
overdispersion.
. glm cigs lcigpric lincome restaurn white educ age agesq, family(poisson)
robust
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

14752.46933
16232.70987

Variance function: V(u)  u
Link function
: g(u)  ln(u)

[Poisson]
[Log]

Log pseudolikelihood  -8111.519022

AIC
BIC







18.46367
20.31628




20.12272
9404.504

807
799

----------------------------------------------------------------------------|
Robust
cigs |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lcigpric | -.1059607
.6681827
-0.16
0.874
-1.415575
1.203653
lincome |
.1037275
.083299
1.25
0.213
-.0595355
.2669906
restaurn | -.3636059
.140366
-2.59
0.010
-.6387182
-.0884937
white | -.0552011
.1632959
-0.34
0.735
-.3752553
.264853
educ | -.0594225
.0192058
-3.09
0.002
-.0970653
-.0217798
age |
.1142571
.0212322
5.38
0.000
.0726427
.1558715
agesq | -.0013708
.0002446
-5.60
0.000
-.0018503
-.0008914
_cons |
.3964493
2.97704
0.13
0.894
-5.438442
6.23134
-----------------------------------------------------------------------------

410

f. We simply compute the turning point for the quadratic:
̂ age /−2̂ age 2  . 1143/2. 00137 ≈ 41. 72, or at about 42 years of age.
g. A double-hurdle model – which separates the initial decision to smoke at all from the
decision of how much to smoke – seems like a good idea. Variables such as level of education,
income, and age could have very different effects on the decision to smoke versus how much
to smoke. It is certainly worth investigating. One approach is to model Dy|x, y ≥ 1 as, say, a
truncated Poisson distribution, and then to model Py  0|x as a logit or probit (with
parameters free to vary from the truncated Poisson distribution).
18.4. In the notation of Section 14.5.3, rw i ,   rw i ,   y i − mx i , , and so
R o x i   −∇  mx i ,  o . Further,  o x i   Vary i − mx i ,  o |x i   Vary i |x i    2o mx i ,  o 
under the GLM assumption. From equation (14.60), the asymptotic variance lower bound is
′

ER o x i  ′  o x i  −1 R o x i  −1   2o E∇  mx i ,  o  ∇  mx i ,  o /mx i ,  o ,
which is the same asymptotic variance for the Poisson QMLE under the GLM assumption.
18.5. a. We just use iterated expectations:
Ey it |x i   EEy it |x i , c i |x i   Ec i expx it |x i , c i 
 Ec i |x i  expx it 
 exp  x̄ i  expx it   exp  x it   x̄ i .
b. We are explicitly testing H 0 :   0, but we are maintaining full independence of c i and
x i under H 0 . We have enough assumptions to derive Vary i |x i , the T  T conditional variance
matrix of y i given x i under H 0 . First,
Vary it |x i   EVary it |x i , c i |x i   VarEy it |x i , c i |x i 
 Ec i expx it |x i   Varc i expx it |x i 
 exp  x it    2 expx it  2 ,

411

where  2 ≡ Varc i  and we have used Ec i |x i   exp under H 0 . A similar, general
expression holds for conditional covariances:
Covy it , y ir |x i   ECovy it , y ir |x i , c i |x i   CovEy it |x i , c i , Ey ir |x i , c i |x i 
 0  Covc i expx it , c i expx ir |x i 
  2 expx it  expx ir .
So, under H 0 , Vary i |x i  depends on , , and  2 , all of which we can estimate. It is natural to
use a score test – actually, its variable addition counterpart – to test H 0 :   0. First, obtain
consistent estimators ̆ , ̆ by, say, pooled Poisson QMLE. Let y̆ it  exp̆  x it ̆ and
ŭ it  y it − y̆ it . A consistent estimator of  2 can be obtained from a simple pooled regression,
through the origin, of
ŭ 2it − y̆ it on exp2x it ̆, t  1, … , T; i  1, … , N.
Let ̃ 2 be the coefficient on exp2x it ̆. It is consistent for  2 because, under H 0 ,
Eu 2it |x i   exp  x it    2 expx it  2 ,
where u it ≡ y it − Ey it |x it . We could also use the many covariance terms in estimating  2
because Eu it u ir |x i    2 expx it  expx ir , t ≠ r. So for all t, r  1, . . . , T, we can write
u it u ir − d tr exp  x it    2 expx it  expx ir   v itr
where Ev itr |x i   0 and d tr  1t  r is a dummy variable. The pooled regression would be
ŭ it ŭ ir − d tr y̆ it on expx it  expx ir 
Next, we construct the T  T weighting matrix for observation i, as in Section 18.7.3. The
matrix W i ̃  Wx i , ̃ has diagonal elements
exp̆  x it ̆  ̃ 2 exp2x it ̆, t  1, … , T
and off-diagonal elements
412

̃ 2 expx it ̃ expx ir ̃, t ≠ r.
Using this weighting matrix in a MWNLS estimation problem we can simply add the time
averages, x̄ i , as an additional set of explanatory variables, and test their joint signficiance. This
is the VAT version of the score test.
In practice, we might want a robust form of the test that does not require
Vary i |x i   Wx i ,  under H 0 , where Wx i ,  is the matrix described above. We can just
use the fully robust variance matrix reported at the bottom of page 761.
Using modern software that supports MWNLS a simpler approach is to estimate the model
under the alternative and obtain a Wald test of H 0 :   0, where it is valid to act as if
Varc i |x i    2 because this is true under the null. This would differ from the score approach
in that  2 would be estimated using a first stage where  is also estimated. A fully robust Wald
test is easy to obtain if we have any doubts about the variance-covariance structure.
Incidentally, this variance-covariance structure is different from the one used in the GEE
literature for Poisson regression. With GEE and an exchangeable correlation structure, the
nominal variance would be
Vary it |x i   exp  x it   x̄ i 
and the nominal covariances
Covy it , y ir |x i    expx it  expx ir  .
c. If we assume (18.83), (18.84) and c i  a i exp  x̄ i  where a i |x i ~ Gamma, , then
testing involves estimation of a Poisson panel data model under random effects assumptions.
Under these assumptions, we have

413

y it |x i , a i ~Poissona i exp  x it   x̄ i 
y it , y ir are independent conditional on x i , a i 
a i |x i ~ Gamma, .
In other words, the full set of random effect Poisson assumptions holds, but where the mean
function in the Poisson distribution is a i exp  x it   x̄ i . In practice, we just add the
(nonredundant elements of) x̄ i in each time period, along with a constant and x it , and carry out
a random effects Poisson analysis. We can test H 0 :   0 using the LR, Wald, or score
approaches. Any of these would be asymptotically efficient. None is robust to misspecification
of the Poisson distribution or the conditional independence assumption because we have used a
full distribution for y i given x i in the MLE analysis.
18.6. a. We know from Problem 12.6 that pooled nonlinear least squares consistently
estimates  o when  o appears in correctly specified conditional means for each t. Because
̈ it  depends on x i we should show
m
̈ it  o , t  1, … , T,
Eÿ it |x i   m
as suggested in the hint. To this end, write
y it  c i  m it x i ,  o   u it , Eu it |x i , c i   0, t  1, … , T.
Then subtracting off time averages gives
̈ it  o   ü it ,
ÿ it  m
T

ü it ≡ u it − T

−1

∑ u ir .
r1

Because Eü it |x i   0, t  1, … , T, consistency follows generally by Problem 12.6. We do
have to make an assumption that ensures that  o is identified, which restricts the way that
time-constant variables can appear in mx it , . (For example, additive time-constant variables
414

get swept away by the time demeaning.)
b. By the general theory of M-estimation, or by adapting either Problem 12.6 or 12.7, we
can show
N

T

−1/2
N ̂ −  o   A −1
∑ ∑ ∇  m̈ it  o  ′ ü it  o p 1,
o N
i1 t1

where
T

Ao  T

−1

∑E

′

̈ it  o  ∇  m
̈ it  o 
∇m

r1

is P  P and P is the dimension of . (As part of the identification assumption, we would
assume that A o is nonsingular.) As in the linear case, we can write, for each t,
T

T

∑ ∇  m̈ it  o  ü it  ∑ ∇  m̈ it  o  ′ u it .
′

t1

t1

Further, Vary i |x i , c i    2o I T is the same as Eu i u ′i |x i , c i    2o I T , which implies
Eu 2it |x i    2o
Eu it u ir |x i   0, t ≠ r
Therefore, by the usual iterated expectations argument,
′

′

̈ it  o  ∇  m
̈ it  o    2o E ∇  m
̈ it  o  ∇  m
̈ it  o  , t  1, … , T
E u 2it ∇  m
and
′

̈ it  o  ∇  m
̈ it  o   0, t ≠ r.
E u it u ir ∇  m
It follows that
T

Var

∑ ∇  m̈ it  o  u it
t1

′

T



 2o

∑E
t1

415

′

̈ it  o  ∇  m
̈ it  o 
∇m

.

Therefore, under the given assumptions,

Avar

−1

T

∑E

N ̂ −  o    2o

′

̈ it  o  ∇  m
̈ it  o 
∇m

.

t1

As in the linear case, the tricky part is in estimating  2o . We can apply virtually the same

̈ it ̂ for all i and t. Then a consistent estimator of  2o is
argument. Let ü it  ÿ it − m
N

1
NT − 1 − P

̂ 2 

T

2

∑ ∑ ü it ,
i1 t1

where the subtraction of P is not needed but is often used as an adjustment for estimation of
 o . Estimation of A o gives
N

ÂN

−1

T

∑ ∑ ∇  m̈ it ̂ ′ ∇  m̈ it ̂.
i1 t1

Then,
Avar̂  ̂ 2

N

T

∑∑

−1

̈ it ̂ ′ ∇  m
̈ it ̂
∇m

.

i1 t1

c. A fully robust variance matrix estimator uses Â and
N

T

T


̂  N −1 ∑ ∑ ∑ 
̈ it ̂ ′ ∇  m
̈ it ̂,
ü it ü ir ∇  m
B
i1 t1 r1

which allows for arbitrary heteroskedasticity and serial correlation in u it : t  1, … , T. Then
−1
̂ Â −1 /N, as usual.
Avar̂  Â B

Remember that the estimator of A o relies on correct specification of the conditional mean;
̈ it  o , which is implied by the model we started with,
in its weakest form, Eÿ it |x i   m
Ey it |x i , c i   c i  mx it ,  o . If we want to allow the model to be misspecified we should use

416


̈ it ̂ ′ ∇  m
̈ it ̂ − ü it ∇ 2 m
̈ it ̂ in place of ∇  m
̈ it ̂ ′ ∇  m
̈ it ̂.
the full Hessian, ∇  m
d. This is easy following the hint. For each i and given , ĉ i  is just the intercept in the
simple regression of y it on 1, m it , t  1, … , T. Therefore, ĉ i   ȳ i − m
̄ i . Therefore, we
can write problem (18.105), after concentrating out the c i , as
N

min


T

∑ ∑y it − ȳ i − m̄ i  − m i 
i1 t1

N

2

 min


T

∑ ∑ÿ it − m̈ it  2 ,
i1 t1

which is what we wanted to show. Note by treating the c i as N parameters to estimate and
using a standardi degrees-of-freedom adjustment, solving (18.105) does yield the estimate ̂ 2
from part b when we use the sum of squared residuals over the degrees of freedom,
NT − N − P  NT − 1 − P.
18.7. a. First, for each t, the density of y it given x i  x, c i  c is
y

fy t |x, c;  o   exp−c  mx t ,  o c  mx t ,  o  t /y t !, y t  0, 1, 2, …
Multiplying these together gives the joint density of y i1 , … , y iT  given x i  x, c i  c.
Taking the log, plugging in the observed data for observation i, and dropping the factorial term
gives
T

∑−c i mx it ,   y it logc i   logmx it , .
t1

b. Taking the derivative of ℓ i c i ,  with respect to c i , setting the result to zero, and
rearranging gives
T

n i /c i  

∑ mx it , .
t1

Letting c i  denote the solution as a function of , we have c i   n i /M i , where

417

T

M i  ≡ ∑ r1 mx ir , . The second order sufficient condition for a maximum is easily seen
to hold.
c. Plugging the solution from part b into ℓ i c i ,  gives
T

ℓ i c i ,   −n i /M i M i   ∑ y it logn i /M i   logmx it , 
t1

T

 −n i  n i logn i   ∑ y it logmx it , /M i 
t1

T



∑ y it logp t x it ,   n 1 − 1 logn i ,
t1

because p t x it ,  ≡ mx it , /M i ; see equation (18.89).
N

d. From part c it follows that if we maximize ∑ i1 ℓ i c i ,  with respect to c i , … , c N  –
N

that is, we concentrate out these parameters – we get exactly ∑ i1 ℓ i c i , . Except for the
N

term ∑ i1 n i − 1 logn i  – which does not depend on  – this is exactly the conditional
log-likelihood for the conditional multinomial distribution obtained in Section 18.7.4.
Therefore, this is another case where treating the c i as parameters to be estimated leads us to a
N -consistent, asymptotically normal estimator of  o .
18.8. a. Generally, there is no simple way to recover Ey|x from Elogy/1 − y|x. In
particular, if Ew|x  x, it is not true that Ey|x  expx/1  expx. In other words,
we cannot simply “undo” the log-odds transformation any more than we can undo any
nonlinear transformation when trying to recover conditional means.
If we make stronger assumptions, we can recover Ey|x from Ew|x. Suppose we write
w  x  v and assume that v is independent of x. Assume for simplicity that v is continuous
with density g. Then

418

Ey|x  x o  



 − expx o   v/1  expx o   vgv.

If we parameterize g – say, g;  – and we have a consistent estimator of , then
Êy|x  x o  



 − expx o ̂  v/1  expx o ̂  vgv; ̂ ,

where ̂ could be the OLS estimator from regressing w i on x i or the maximum likelihood
estimator based on Dw|x. If Dv|x is assumed to be normal then OLS is MLE. Even if we
specify g;  to be a mean-zero normal distribution, obtaining the integral is cumbersome.
There is a simpler approach that is also more robust. If we just maintain that v and x are
independent then, by the law of large numbers, Ey|x  x o  for a given vector x o is
consistently estimated by
N

N −1 ∑ expx o   v i /1  expx o   v i ,
i1

where we can think of drawing random samples x i , v i  : i  1, 2, . . . , N. Because we cannot
observe v i , and we do not know , we operationalize this formula by replacing  with ̂,
including computing residuals v̂ i  w i − x i ̂ , i  1, … , N. Then
N

o

Êy|x  x   N

−1

∑ expx o ̂  v̂ i /1  expx o ̂  v̂ i .
i1

This is an example of Duan’s (1983) “smearing estimate.” This estimator is consistent under
the assumptions given – which do not require a full distribution, but do include independence
between v and x. Obtaining analytical standard errors can be done by following Probem 12.17.
Bootstrapping is also valid. Unfortunately, this approach does not work if y can take on the
boundary values zero or one.

419

The above integral for Ey|x  x o  can be written as Ey|x  x o   rx o  and so, if v and
x are independent, then ∂Ey|x/∂x j /∂Ey|x/∂x h    j / h : for continuous explanatory
variables, the ratio of the partial effects equals the ratio of the parameters in the linear model
for w  logy/1 − y.
b. The functional form Ey|x  expx/1  expx, and that implied by the
assumptions in part a, are generally incompatible. However, as mentioned above,
independence between v and x in logy/1 − y  x  v implies that  j / h is the ratio of the
partial effects of continuous explanatory variables x j and x h . In the fractional logit model, the
ratio of partial effects is  j / h . Therefore, it can make sense to compare ratios of coefficients
on continuous explanatory variables across the two procedures. But the magnitudes themselves
are not generally comparable.
c. Because we have a full distribution of y given x, we should use maximum likelihood,
just as described in Section 17.7.
d. The functional form for Ey|x – as a function of the parameters  and  2 – is given in
equation (17.66) where we set a 1  0, a 2  1, with the obvious change in notation:
Ey|x  1 − x/ − −x/x  1 − x/ − −x/
 −1 − x/
 1 − x/ − −x/x  1 − x/ − −x/
 1 − 1 − x/.
This gives yet a different functional form. Nevertheless, it is easily seen from equation (17.67)
that
∂Ey|x/∂x j /∂Ey|x/∂x h    j / h ,
so that ratios of the coefficients on continuous variables can be compared with those from part

420

b.
e. Because part b only specified a conditional mean, it does not make much sense to
compare the Bernoulli quasi-log-likelihood with the Tobit log-likelihood. If we are mainly
interested in Ey|x – which part b essentially maintains – it makes sense to base comparisons
on goodness-of-fit for Ey|x. For each approach, we can compute a squared correlation
between the y i and the Êy i |x i , where the conditional expectations are estimated using each
approach. Or, we can use a sum-of-squared residuals version (and possibly adjust for
degrees-of-freedom because the Tobit model has an extra mean parameter, ).
f. We would not expect to get similar answers for the full sample – which includes
observations with y i  0 – and the subsample that excludes y i  0 (unless the fraction of
excluded observations is small). Clearly, we cannot have both
Ey|x  expx/1  expx and Ey|x, y  0  expx/1  expx. Moreover, there
is no reason to expect the best fits to yield roughly the same parameter estimates.
g. Because we have assumed Ey|x, y  0  expx/1  expx, we consistently
estimate  using the sample for which 0  y i  1, provided we use the Bernoulli QMLE (or
NLS or weighted NLS). There is no bias from excluding the y i  0 observations because we
have specified the mean for the subpopulation with y  0. (We discuss sample selection issues
in Chapter 19.)
h. We would use a two-part model. Let ̂ be the Bernoulli QMLE from part g, using
observations for which y i  0. To estimate , we run a binary response model using the binary
variable r i  1y i  0. Then Pr i  1|x i   Gx i . Probably we would use a probit or logit
model. Then,

421

Êy i |x i   P̂ y i  0|x i   Êy i |x i , y i  0
 Gx i ̂   expx i ̂/1  expx i ̂.
18.9. a. The Stata output follows. I first convert the dependent variable to be in 0, 1,
rather than 0, 100; this is needed to estimate a fractional response model.
The coefficient on ACT means that five more points on the ACT test, other things equal, is
associated with a lower attendance rate of about . 0175 . 085, or 8. 5 percentage points. For
priGPA, another point on the GPA (a large change) is associated with an attendance rate
roughly 18.2 percentage points higher.
Twelve of the fitted values are bigger than one. This is not surprising because almost 10
percent of the students have perfect attendance rates.
. use attend
. sum atndrte
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------atndrte |
680
81.70956
17.04699
6.25
100
. replace atndrte  atndrte/100
(680 real changes made)
. reg atndrte ACT priGPA frosh soph
Source |
SS
df
MS
------------------------------------------Model | 5.95396289
4 1.48849072
Residual | 13.7777696
675 .020411511
------------------------------------------Total | 19.7317325
679 .029059989

Number of obs
F( 4,
675)
Prob  F
R-squared
Adj R-squared
Root MSE








680
72.92
0.0000
0.3017
0.2976
.14287

----------------------------------------------------------------------------atndrte |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------ACT | -.0169202
.001681
-10.07
0.000
-.0202207
-.0136196
priGPA |
.1820163
.0112156
16.23
0.000
.1599947
.2040379
frosh |
.0517097
.0173019
2.99
0.003
.0177377
.0856818
soph |
.0110085
.014485
0.76
0.448
-.0174327
.0394496
_cons |
.7087769
.0417257
16.99
0.000
.6268492
.7907046
----------------------------------------------------------------------------. predict atndrteh_lin
(option xb assumed; fitted values)

422

. sum

atndrteh_lin

Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------atndrteh_lin |
680
.8170956
.0936415
.4846666
1.086443
. count if atndrteh_lin  1
12
. count if atndrte  1
66

b. The GLM standard errors are given in the output. Note that ̂ ≈. 0161. In other words,
the usual MLE standard errors, obtained, say, from the expected Hessian of the
quasi-log-likelihood, are much too large. The standard errors that account for  2  1 are given
by the GLM output. (If you omit the sca(x2) option in the glm command, you get the usual
MLE standard errors.)
. glm atndrte ACT priGPA frosh soph, family(binomial) link(logit) sca(x2)
note: atndrte has noninteger values
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

87.81698799
85.57283238

Variance function: V(u)  u*(1-u/1)
Link function
: g(u)  ln(u/(1-u))
Log likelihood







680
675
.1300992
.1267746

[Binomial]
[Logit]
AIC
BIC

 -223.6493665

 .6724981
 -4314.596

----------------------------------------------------------------------------|
OIM
atndrte |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------ACT | -.1113802
.0113217
-9.84
0.000
-.1335703
-.0891901
priGPA |
1.244375
.0771321
16.13
0.000
1.093199
1.395552
frosh |
.3899318
.113436
3.44
0.001
.1676013
.6122622
soph |
.0928127
.0944066
0.98
0.326
-.0922209
.2778463
_cons |
.7621699
.2859966
2.66
0.008
.201627
1.322713
----------------------------------------------------------------------------(Standard errors scaled using square root of Pearson X2-based dispersion.)
. di (.1268)^2
.01607824

c. Because the coefficient on ACT is negative, we know that an increase in ACT score,

423

holding year and prior GPA fixed, actually reduces predicted attendance rate. The calculation
below shows that for priGPA − 3. 0 and frosh  soph  0, when ACT increases from 25 to 30,
the estimated fall in atndrte is about .087, or 8.7 percentage points. This is very similar to the
estimate using the linear model – 8.5 percentage points – which is the same for any values of
the explanatory variables.
. di exp(_b[_cons]  _b[ACT]*30  _b[priGPA]*3)/(1  exp(_b[_cons]  _b[ACT]*
 _b[priGPA]*3)) - exp(_b[_cons]  _b[ACT]*25  _b[priGPA]*3)
/(1  exp(_b[_cons]  _b[ACT]*25  _b[priGPA]*3))
-.08671822

d. The R-squared for the linear model is about .302. For the logistic functional form, I
computed the squared correlation between atndrte i and Êatndrte 1 |x i . This R-squared is about
.328, and so the logistic functional form does fit better than the linear model. And, remember
that the parameters in the logistic functional form are not chosen to maximize an R-squared;
the linear model coefficients are chosen to maximize R-squared given the set of explanatory
variables.
. predict atndrteh_log
(option mu assumed; predicted mean atndrte)
. corr atndrte atndrteh_log
(obs680)
| atndrte atndrt~g
------------------------------atndrte |
1.0000
atndrteh_log |
0.5725
1.0000
. di .5725^2
.32775625

18.10. a. The pooled Poisson estimates, with the usual pooled standard errors that assume a
unit variance-mean ratio and dynamic completeness of the conditional mean, are given below.
Using these nonrobust standard errors, all lags except the first are significantly different from
zero.

424

. use patent
. poisson patents y77-y81 lrnd lrnd_1 lrnd_2 lrnd_3 lrnd_4
Poisson regression

Number of obs
LR chi2(10)
Prob  chi2
Pseudo R2

Log likelihood  -12194.868






1356
68767.04
0.0000
0.7382

----------------------------------------------------------------------------patents |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------y77 | -.0732934
.0190128
-3.85
0.000
-.1105578
-.0360291
y78 |
-.227293
.0196925
-11.54
0.000
-.2658896
-.1886965
y79 |
-.36251
.0196912
-18.41
0.000
-.4011041
-.3239159
y80 | -.7066175
.0211325
-33.44
0.000
-.7480365
-.6651985
y81 | -2.115567
.0331249
-63.87
0.000
-2.18049
-2.050643
lrnd |
.4406223
.0425948
10.34
0.000
.357138
.5241066
lrnd_1 |
.0767312
.0635969
1.21
0.228
-.0479165
.2013788
lrnd_2 |
.2452529
.0622048
3.94
0.000
.1233337
.3671721
lrnd_3 | -.1557527
.0630881
-2.47
0.014
-.2794031
-.0321023
lrnd_4 |
.1619174
.0469008
3.45
0.001
.0699936
.2538412
_cons |
1.157326
.0191835
60.33
0.000
1.119727
1.194925
-----------------------------------------------------------------------------

b. The standard errors computed in part a can be wrong for at least two reasons. The first is
that the conditional variance, Vary it |x it , may not equal the conditional mean, Ey it |x it , where
x it contains the current and lagged R&D spending variables. The second is that the mean may
not be dynamically complete in the sense that
Ey it |x it  ≠ Ey it |x it , y i,t−1 , x i,t−1 , … .
A failure of dynamic completeness generally leads to serial correlation in the implied error
terms, and cause the score of the partial quasi-log-likelihood function to be serially correlated.
A third reason the standard errors might not be valid is they use the expected Hessian form
of the asymptotic variance. This form is incorrect of the conditional mean is misspecified.
c. The estimates below give ̂ ≈ 4. 14, which shows that, even if we assume a constant
variance-mean ratio and dynamic completeness of the conditional mean, we need to multiply
all Poisson standard errors by just over four.
Now only the contemporaneous R&D variable is significant; none of the lags has a t

425

statistic above one.
. glm patents y77-y81 lrnd lrnd_1 lrnd_2 lrnd_3 lrnd_4, family(poisson)
link(log) sca(x2)
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

20618.28952
23082.45413

Variance function: V(u)  u
Link function
: g(u)  ln(u)
Log likelihood







15.32958
17.16168




18.00276
10917.75

1356
1345

[Poisson]
[Log]
AIC
BIC

 -12194.86797

----------------------------------------------------------------------------|
OIM
patents |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------y77 | -.0732934
.0787636
-0.93
0.352
-.2276672
.0810803
y78 |
-.227293
.0815794
-2.79
0.005
-.3871858
-.0674003
y79 |
-.36251
.0815742
-4.44
0.000
-.5223925
-.2026275
y80 | -.7066175
.087545
-8.07
0.000
-.8782025
-.5350325
y81 | -2.115567
.1372256
-15.42
0.000
-2.384524
-1.846609
lrnd |
.4406223
.176456
2.50
0.013
.0947748
.7864698
lrnd_1 |
.0767312
.2634608
0.29
0.771
-.4396424
.5931048
lrnd_2 |
.2452529
.2576938
0.95
0.341
-.2598177
.7503235
lrnd_3 | -.1557527
.2613529
-0.60
0.551
-.6679949
.3564895
lrnd_4 |
.1619174
.1942941
0.83
0.405
-.2188921
.5427269
_cons |
1.157326
.0794708
14.56
0.000
1.001566
1.313086
----------------------------------------------------------------------------(Standard errors scaled using square root of Pearson X2-based dispersion.)
. di sqrt(17.16)
4.142463

d. The QLR statistic is just the usual LR statistic divided by ̂ 2  17. 17. The value of the
unrestricted log-likelihood is ℒ ur  −12, 194. 87. The value of the restricted log-likelihood
(without any of the lags), using the same set of years in estimation (1976 to 1981), is
ℒ r  −12, 252. 37. Therefore,
QLR  2  12, 252. 37 − 12, 194. 87/17. 17  6. 70.
With four degrees of freedom in a chi-square distribution, this leads to p-value  .153. The lags
are jointly insignificant at the usual 5% level. The usual LR statistic is 115, which (incorrectly)

426

implies very strong statistical significance for the lags.
e. The Stata results are blow. With the fully robust standard errors, the contemporaneous
term and the second lag are marginally significantly. The robust Wald test for the exclusion of
the four lags gives p-value .494. The fully robust standard errors are clearly smaller than the
Poisson MLE standard errors, but they are actually smaller in some cases than the GLM
standard errors from part c. The four lags are joint insignificant.
. glm patents y77-y81 lrnd lrnd_1 lrnd_2 lrnd_3 lrnd_4, family(poisson)
link(log) robust cluster(cusip)
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

20618.28952
23082.45413

Variance function: V(u)  u
Link function
: g(u)  ln(u)

[Poisson]
[Log]

Log pseudolikelihood  -12194.86797

AIC
BIC







15.32958
17.16168




18.00276
10917.75

1356
1345

(Std. Err. adjusted for 226 clusters in cusip
----------------------------------------------------------------------------|
Robust
patents |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------y77 | -.0732934
.0317955
-2.31
0.021
-.1356115
-.0109754
y78 |
-.227293
.0499251
-4.55
0.000
-.3251445
-.1294416
y79 |
-.36251
.0681543
-5.32
0.000
-.49609
-.22893
y80 | -.7066175
.0667816
-10.58
0.000
-.837507
-.575728
y81 | -2.115567
.1140381
-18.55
0.000
-2.339077
-1.892056
lrnd |
.4406223
.2409156
1.83
0.067
-.0315637
.9128083
lrnd_1 |
.0767312
.1228435
0.62
0.532
-.1640376
.3175
lrnd_2 |
.2452529
.1411443
1.74
0.082
-.0313848
.5218906
lrnd_3 | -.1557527
.2160959
-0.72
0.471
-.579293
.2677875
lrnd_4 |
.1619174
.2679931
0.60
0.546
-.3633395
.6871743
_cons |
1.157326
.2061445
5.61
0.000
.7532903
1.561362
----------------------------------------------------------------------------. test lrnd_1 lrnd_2 lrnd_3 lrnd_4
(
(
(
(

1)
2)
3)
4)

[patents]lrnd_1
[patents]lrnd_2
[patents]lrnd_3
[patents]lrnd_4
chi2( 4) 
Prob  chi2 






0
0
0
0
3.40
0.4937

427

f. The estimated long run elasticity is about . 441 . 077 . 245 −. 156 . 162 . 769. The
lincom command in Stata provides a simple way to obtain a fully robust standard error. Its
fully robust standard error is about . 072, which gives a 95% confidence interval from about
.627 to .910. As is often the case in distributed lag models, we cannot estimate the lag
distribution very precisely but we can get a fairly precise estimate of the long run effect.
. lincom
( 1)

lrnd 

lrnd_1 

lrnd_2 

lrnd_3 

lrnd_4

[patents]lrnd  [patents]lrnd_1  [patents]lrnd_2  [patents]lrnd_3
 [patents]lrnd_4  0

----------------------------------------------------------------------------patents |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------(1) |
.768771
.0722693
10.64
0.000
.6271258
.9104162
-----------------------------------------------------------------------------

g. The fixed effects Poisson estimates are given below. The contemporaneous spending
term and second lag have much smaller effects now, while lags three and four become larger
and even statistically significant – but with the third lag still having a large, negative
coefficient. When we use the fully robust standard errors, only the second lag is statistically
significant at conventional levels, although the third and fourth lags are close.
The estimated long-run elasticity is now only . 261 and it is, at best, marginally significant
with t  1. 60.
. xtpqml patents y77-y81 lrnd lrnd_1 lrnd_2 lrnd_3 lrnd_4, fe
note: 8 groups (48 obs) dropped because of all zero outcomes
Conditional fixed-effects Poisson regression
Group variable: cusip

Number of obs
Number of groups




Obs per group: min 
avg 
max 
Log likelihood

Wald chi2(10)
Prob  chi2

 -2423.7694




1308
218
6.
3002.51
0.0000

----------------------------------------------------------------------------patents |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
----------------------------------------------------------------------------

428

y77 | -.0210069
.0204558
-1.03
0.304
-.0610995
.0190856
y78 |
-.108368
.0251005
-4.32
0.000
-.157564
-.059172
y79 | -.1721306
.0306902
-5.61
0.000
-.2322822
-.1119789
y80 | -.4468227
.039243
-11.39
0.000
-.5237375
-.3699079
y81 | -1.797958
.0547882
-32.82
0.000
-1.905341
-1.690575
lrnd |
.0492403
.0558275
0.88
0.378
-.0601795
.15866
lrnd_1 |
.0512096
.0666844
0.77
0.443
-.0794894
.1819086
lrnd_2 |
.130944
.0662164
1.98
0.048
.0011622
.2607259
lrnd_3 | -.1909907
.0714669
-2.67
0.008
-.3310632
-.0509182
lrnd_4 |
.2201799
.0703992
3.13
0.002
.0821999
.3581599
----------------------------------------------------------------------------Calculating Robust Standard Errors...
----------------------------------------------------------------------------patents |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------patents
|
y77 | -.0210069
.026186
-0.80
0.422
-.0723306
.0303168
y78 |
-.108368
.055447
-1.95
0.051
-.2170422
.0003062
y79 | -.1721306
.071949
-2.39
0.017
-.313148
-.0311131
y80 | -.4468227
.0829316
-5.39
0.000
-.6093657
-.2842797
y81 | -1.797958
.1380887
-13.02
0.000
-2.068607
-1.527309
lrnd |
.0492403
.0868099
0.57
0.571
-.120904
.2193845
lrnd_1 |
.0512096
.0600491
0.85
0.394
-.0664845
.1689038
lrnd_2 |
.130944
.0592739
2.21
0.027
.0147694
.2471187
lrnd_3 | -.1909907
.1066283
-1.79
0.073
-.3999783
.0179968
lrnd_4 |
.2201799
.1431273
1.54
0.124
-.0603446
.5007043
----------------------------------------------------------------------------Wald chi2(10) 
366.83
Prob  chi2 
0.0000
. lincom
( 1)

lrnd 

lrnd_1 

lrnd_2 

lrnd_3 

lrnd_4

[patents]lrnd  [patents]lrnd_1  [patents]lrnd_2  [patents]lrnd_3
 [patents]lrnd_4  0

----------------------------------------------------------------------------patents |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------(1) |
.2605831
.1632377
1.60
0.110
-.059357
.5805231
-----------------------------------------------------------------------------

18.11. a. For each t, the density is
y

f t y t |x i , c i   exp−c i m it m itt /y t !, y t  0, 1, 2, …
Under the conditional independence assumption, the joint density of y i1 , … , y iT  given x i , c i 
is

429

T

fy 1 , … , y T |x i , c i  

exp−c i m it c i m it  y /y t !
t

t1

T

 m yit /y t !



t

c si exp−c i M i ,

t1

where M i ≡ m i1 … m iT and s  y 1 … y T , for all nonnegative integers y t : t  1, … , T.
b. To obtain the density of y i1 , … , y iT  given x i – say gy i1 , … , y iT |x i  – we integrate out
with respect to the distribution of c i (because c i is independent of x i ). Therefore,
T

gy 1 , … , y T |x i  



 m yit /y t !  0 c s exp−c i M i   /Γc −1 exp−cdc.
t

t1

Next, we follow the hint, noting that the general Gamma,  density has the form
hc    /Γc −1 exp−c. Now




 0 c s exp−cM i   /Γc −1 exp−cdc   0   /Γc s−1 exp−M i  cdc
   /Γ Γs  /M i   s



0

M i   s /Γs   c s−1 exp−M i  cdc,

and the integrand is easily seen to be the Gammas  , M i   density, and so it integrates to
unity. Therefore, we have shown
T

gy 1 , … , y T |x i  

 m yit /y t !
t

  /Γ Γs  /M i   s

t1

for all nonnegative integers y t : t  1, … , T.
18.12. a. First, the density of y it given x i , c i  is
it
f t y t |x i , c i   1/c i  m it /Γm it y m
t

−1

exp−1/c i y t , y t  0.

Following the hint, the density of the sum, s i , given x i , c i  is

430

gs|x i , c i   1/c i  M i /ΓM i s M i −1 exp−1/c i s, s  0,
where M i  m i1 … m iT . Therefore, the density of y i1 , . . . , y iT  given s i  s, x i , c i  is
m i1 −1

1/c i  m i1 /Γm i1 y 1

m

exp−1/c i y t   1/c i  m i,T−1 /Γm i,T−1 y T−1i,T−1

−1

exp−1/c i y T−1 

 1/c i  m iT /Γm iT s − y 1 −… −y T−1  m iT  exp−1/c i s − y 1 −… −y T−1 /gs|x i , c i 
T

 1/c i 

−1

 Γm it 

Mi

 y mt −1
it

t1

T

 ΓM i 

 Γm it 
t1

T

exp−1/c i s/1/c i  M i /ΓM i s M i −1 exp−1/c i s

t1

−1

T

 y mt −1
it

/s M i −1 ,

t1

which is what we wanted to show. Note how c i has dropped out of the density.
b. The conditional log-likelihood for observation i is
T

ℓ; y i1 , . . . , y iT , x i   logΓM i  − ∑ logΓm it 
t1

T

 ∑m it  − 1 logy it  − M i  − 1 logy i1   y iT ,
t1

T

where m it   m t x i ,  and M i  ∑ t1 m it . We can sum across all i and maximize the
resulting log-likelihood with respect to  to obtain the fixed effects gamma estimator. The
asymptotic theory is standard, provided the regression functions are smooth functions of  and
depend on the covariates in such a way that  o is identified.
18.13. a. Plug in the data, a genereric value , and take the natural log:
ℓ i   x i   logy i expx i  − 1.
Notice that this is not a member of the linear exponential family (because it is logy i , not y i ,
that appears).
b. The gradient is

431

∇  ℓ i   x i  logy i x i expx i 
and taking the transpose gives
s i   x ′i  logy i x ′i expx i 
 x ′i 1  logy i  expx i .
c. Because 0  y i  1, logy i   0 for all i. Therefore, we know Elogy i |x i   0 for any
outcome x i .
d. We use part c:
Es i  o |x i   x ′i 1  Elogy i |x i  expx i  o 
 x ′i 1 − exp−x i  o  expx i  o   0.
e. Using s i   x ′i 1  logy i  expx i , the Hessian is
H i   ∇  s i   x ′i x i logy i  expx i 
and so
EH i  o |x i   x ′i x i Elogy i |x i  expx i  o 
 x ′i x i Elogy i |x i  expx i  o 
 −x ′i x i exp−x i  o  expx i  o   −x ′i x i ,
and so
− EH i  o |x i   x ′i x i .
f. Given part e, the formula based on the expected Hessian is easiest:
Avar N ̂ −  o   Ex ′i x i  −1
and so
Avar N ̂ −  o  

N

N −1 ∑ x ′i x i
i1

432

−1

 X ′ X/N −1 .

g. From part d, we see that the key condition for Fisher consistency, that is, to make
Es i  o |x i   0, is that
Elogy i |x i   − exp−x i  o .
In other words, the implied model for Elogy i |x i  must be correct. Unfortunately, having
Ey i |x i  correctly specified, that is, Ey i |x i   expx i  o /1  expx i  o , generally says
nothing about Elogy i |x i .
h. We could use the Bernoulli QMLE to estimate the parameters in
Ey i |x i   expx i  o /1  expx i  o  directly, without extra assumptions about Dy i |x i .
18.14. a. The Stata output is given below with the three sets of standard errors asked for in
the problem. The inference starts from the least robust and ends with the most robust.
The difference in standard errors is striking. The standard errors that effectively maintain a
binomial distribution – at least its first two moments – lead to huge t statistics. When we allow
for a scale factor – the so-called GLM variance assumption, (18.34) – the standard errors
increase by at least a factor of 20. The fully robust standard errors, which allow unrestricte
Varpartic i |employ i , x i  are still larger – more than three times the standard errors produced
under the GLM assumption. It seems pretty clear the binomial distribution does not hold in this
application and that the actual conditional variance is not proportional to the nominal variance
in the binomial distribution. It is pretty clear that we should use the fully robust standard
errors, which lead to much more modest (but still quite significant) t statistics.
. glm partic mrate ltotemp age agesq sole, fam(bin employ) link(logit)
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

2199795.239
2021563.356

433







4075
4069
540.6231
496.8207

Variance function: V(u)  u*(1-u/employ)
Link function
: g(u)  ln(u/(employ-u))
Log likelihood

[Binomial]
[Logit]
AIC
BIC

 -1108397.213




544.0016
2165971

----------------------------------------------------------------------------|
OIM
partic |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------mrate |
.9871354
.0033855
291.58
0.000
.9805
.9937708
ltotemp | -.1386562
.000531 -261.10
0.000
-.139697
-.1376153
age |
.0718575
.0001669
430.63
0.000
.0715305
.0721846
agesq | -.0005512
2.82e-06 -195.47
0.000
-.0005567
-.0005457
sole |
.3419834
.003443
99.33
0.000
.3352353
.3487315
_cons |
1.442014
.0053821
267.93
0.000
1.431465
1.452563
----------------------------------------------------------------------------. glm partic mrate ltotemp age agesq sole, fam(bin employ) link(logit) sca(x2
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

2199795.239
2021563.356

Variance function: V(u)  u*(1-u/employ)
Link function
: g(u)  ln(u/(employ-u))
Log likelihood

540.6231
496.8207




544.0016
2165971

4075
4069

[Binomial]
[Logit]
AIC
BIC

 -1108397.213







----------------------------------------------------------------------------|
OIM
partic |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------mrate |
.9871354
.0754604
13.08
0.000
.8392358
1.135035
ltotemp | -.1386562
.0118368
-11.71
0.000
-.1618559
-.1154565
age |
.0718575
.0037193
19.32
0.000
.0645678
.0791472
agesq | -.0005512
.0000629
-8.77
0.000
-.0006744
-.000428
sole |
.3419834
.0767418
4.46
0.000
.1915723
.4923945
_cons |
1.442014
.1199639
12.02
0.000
1.206889
1.677139
----------------------------------------------------------------------------(Standard errors scaled using square root of Pearson X2-based dispersion)
. glm partic mrate ltotemp age agesq sole, fam(bin employ) link(logit) robust
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

2199795.239
2021563.356

Variance function: V(u)  u*(1-u/employ)
Link function
: g(u)  ln(u/(employ-u))

[Binomial]
[Logit]

Log pseudolikelihood  -1108397.213

AIC
BIC

434







540.6231
496.8207




544.0016
2165971

4075
4069

----------------------------------------------------------------------------|
Robust
partic |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------mrate |
.9871354
.2622177
3.76
0.000
.4731982
1.501073
ltotemp | -.1386562
.0546138
-2.54
0.011
-.2456972
-.0316151
age |
.0718575
.0142656
5.04
0.000
.0438974
.0998176
agesq | -.0005512
.0001746
-3.16
0.002
-.0008934
-.000209
sole |
.3419834
.1145195
2.99
0.003
.1175294
.5664375
_cons |
1.442014
.4368904
3.30
0.001
.5857248
2.298303
-----------------------------------------------------------------------------

b. The fractional logit results for prate are given below – with the same kinds of standard
errors in part a. In this case the usual MLE standard errors that are too large: they treat  2  1
in (18.58), which is true in the binary case but not generally. With a factional variable,  2  1.
In fact, the estimate for this data set is ̂ 2 . 214.
The GLM and fully robust standard errors are much closer now, with the fully robust ones
typically (but not always) being slightly larger.
. glm prate mrate ltotemp age agesq sole, fam(bin) link(logit)
note: prate has noninteger values
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

883.051611
871.5810654

Variance function: V(u)  u*(1-u/1)
Link function
: g(u)  ln(u/(1-u))
Log likelihood







4075
4069
.2170193
.2142003

[Binomial]
[Logit]
AIC
BIC

 -1287.919784

 .6350527
 -32941.02

----------------------------------------------------------------------------|
OIM
prate |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------mrate |
1.147984
.1468736
7.82
0.000
.8601167
1.43585
ltotemp | -.2075898
.0290032
-7.16
0.000
-.264435
-.1507446
age |
.0481773
.0145566
3.31
0.001
.0196469
.0767077
agesq | -.0004519
.0004301
-1.05
0.293
-.0012948
.000391
sole |
.1652908
.10408
1.59
0.112
-.0387022
.3692838
_cons |
2.355715
.2299685
10.24
0.000
1.904985
2.806445
----------------------------------------------------------------------------. glm prate mrate ltotemp age agesq sole, fam(bin) link(logit) sca(x2)
note: prate has noninteger values
Generalized linear models

No. of obs

435



4075

Optimization

: ML

Deviance
Pearson




Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

883.051611
871.5810654

Variance function: V(u)  u*(1-u/1)
Link function
: g(u)  ln(u/(1-u))
Log likelihood






4069
.2170193
.2142003

[Binomial]
[Logit]
AIC
BIC

 -1287.919784

 .6350527
 -32941.02

----------------------------------------------------------------------------|
OIM
prate |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------mrate |
1.147984
.0679757
16.89
0.000
1.014754
1.281213
ltotemp | -.2075898
.0134232
-15.47
0.000
-.2338988
-.1812808
age |
.0481773
.006737
7.15
0.000
.0349729
.0613817
agesq | -.0004519
.000199
-2.27
0.023
-.000842
-.0000618
sole |
.1652908
.0481701
3.43
0.001
.0708792
.2597024
_cons |
2.355715
.1064335
22.13
0.000
2.147109
2.564321
----------------------------------------------------------------------------(Standard errors scaled using square root of Pearson X2-based dispersion)
. glm prate mrate ltotemp age agesq sole, fam(bin) link(logit) robust
note: prate has noninteger values
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

883.051611
871.5810654

Variance function: V(u)  u*(1-u/1)
Link function
: g(u)  ln(u/(1-u))

[Binomial]
[Logit]

Log pseudolikelihood  -1287.919784

AIC
BIC







4075
4069
.2170193
.2142003

 .6350527
 -32941.02

----------------------------------------------------------------------------|
Robust
prate |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------mrate |
1.147984
.0747331
15.36
0.000
1.001509
1.294458
ltotemp | -.2075898
.0141209
-14.70
0.000
-.2352662
-.1799134
age |
.0481773
.0061543
7.83
0.000
.036115
.0602396
agesq | -.0004519
.0001764
-2.56
0.010
-.0007976
-.0001063
sole |
.1652908
.0505915
3.27
0.001
.0661334
.2644483
_cons |
2.355715
.1066441
22.09
0.000
2.146696
2.564734
-----------------------------------------------------------------------------

c. It makes sense to compare the coefficients in parts a and b because both approaches
could be estimating the same conditional mean function for prate i  partic i /employ i .
Generally, the binomial approach starts with
436

Ey i |x i , n i   n i x i .
If we divide both sides by n i we get
Ey i |x i , n i 
 x i 
ni
or
y
E nii x i , n i

 x i 

which, of course, implies
y
E nii x i

 x i 

In other words, the fractional variable w i ≡ y i /n i follows a fractional response model with a
logistic response function. So if we start with Ey i |x i , n i   n i x i  then both methods
consistently estimate .
d. The Stata output is given below. Because we want the APE on prate, we compute
N

̂ mrate N −1 ∑
i1

expx i ̂
2
1  expx i ̂

for both set of estimates. For the binomial QMLE the estimate is about . 147. For the Bernoulli
QMLE, the estimate is about . 130. Incidentally, the linear regression estimate – coefficient on
mrate – is about . 106, so quite a bit below the other two.
. qui glm partic mrate ltotemp age agesq sole, fam(bin employ) link(logit)
. predict xb_bin, xb
. gen sca_bin  exp(xb_bin)/((1  exp(xb_bin))^2)
. sum sca_bin
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------sca_bin |
4075
.1492273
.0602467
.0091082
.2499969

437

. di .1492273*_b[mrate]
.14730755
. qui glm prate mrate ltotemp age agesq sole, fam(bin) link(logit)
. predict xb_ber, xb
. gen sca_ber  exp(xb_ber)/((1  exp(xb_ber))^2)
. di sca_ber*_b[mrate]
.13000441

e. The Stata output is given below. The APE is about . 038. If we use the linear model, we
would get . 106. 25 ≈. 027, so somewhat less.
. qui glm prate mrate ltotemp age agesq sole, fam(bin) link(logit)
. gen xb_p50  xb_ber - _b[mrate]*mrate  _b[mrate]*.5
. gen xb_p25  xb_ber - _b[mrate]*mrate  _b[mrate]*.25
. gen phat_p50  exp(xb_p50 )/(1  exp(xb_p50 ))
. gen phat_p25  exp(xb_p25 )/(1  exp(xb_p25 ))
. gen diff  phat_p50 - phat_p25
. sum diff
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------diff |
4075
.0375776
.0107886
.0116919
.0689509

18.15. a. We can just use the usual fixed effects or first-differencing estimators. If we
define w i ≡ logy it /1 − y it  then we have
w it  x it   c i  u it
Eu it |x i , c i   0, t  1, . . . , T,
which means the key strict exogeneity assumption on x it : t  1, . . . , T holds. Of course, we
could use a GLS version of FE or FD, or use Chamberlain’s approach.
b. Because logy it /1 − y it   x it   v it ,
y it /1 − y it   expx it   v it 

438

and so
1 − y it 
1

y it
expx it   v it 
or
1  expx it   v it 
1
1
y it  expx it   v it   1  expx it   v it 
which implies
y it 

expx it   v it 
.
1  expx it   v it 

The ASF is defined, for each t, as
ASF t x t  



 −

expx t   v
g t vdv
1  expx t   v

where g t  is the density of v it . (Of course, allowing this density to be discrete changes the
integral to a sum.) We can also write
ASF t x t   E v it

expx t   v it 
1  expx t   v it 

,

that is, we fix the covariates at values x t and average across the distribution of the
unobservables, v it .
c. The ASF cannot be estimated without further assumptions because we cannot estimate
the expected value of expx t   v it /1  expx t   v it  for given x t without further
assumptions.
d. By the law of iterated expectations, we have

439

expx t   v it 
x̄ i
1  expx t   v it 
expx t     x̄ i   r it 
E
x̄ i
1  expx t     x̄ i   r it 

ASF t x t   E x̄ i E
E x̄ i

.

With r it is independent of x i – and we can assume Ea i   0 due to the presence of  – then
we can consistently estimate , , and  by pooled OLS:
w it  on x it , 1, x̄ i , t  1, . . . , T; i  1, . . . , N.
(Recall this produces the FE estimator of .) Further, by independence,
E

expx t     x̄ i   r it 
x̄ i
1  expx t     x̄ i   r it 





 −

expx t     x̄ i   r
f t rdr
1  expx t     x̄ i   r

For fixed x̄ i  x̄ , we can consistently estimate this expression as
N

N

−1

∑
i1

̂  x̄ ̂  r̂ it 
expx t ̂  
,
̂  x̄ ̂  r̂ it 
1  expx t ̂  

̂ − x̄ i ̂ are the pooled OLS residuals. To get the ASF, we need to
where r̂ it ≡ w it − x it ̂ − 
further average out over the distribution of x̄ i , which gives
N

ASF t x t   N

−1

∑
i1

̂  x̄ i ̂  r̂ it 
expx t ̂  
.
̂  x̄ i ̂  r̂ it 
1  expx t ̂  

We use this as usual: take derivatives and changes with respect to the elements of x t .
18.16. (Bonus Question) Consider a panel data mode for y it ≥ 0 with multiplicative
heterogeneity and a multiplicative idiosyncratic error:
y it  c i expx it r it .
If we assume x it : t  1, . . . , T is strictly exogenous then we can estimate  using the fixed
effects Poisson QMLE. Instead, assume we have instruments, z it : t  1, 2, . . . , T that satisfy

440

a sequential exogeneity assumption:
Er it |z it , . . . , z i1 , c i   Er it   1,
where setting the expected value to unity is a normalization. (As usual, x it should probably
include a full set of time period dummies.)
a. Show that we can write
y i,t1
y it
−
 c i r it − r i,t1  ≡ e i,t1
expx it 
expx i,t1 
where
Ee it |z it , . . . , z i1   0, t  1, . . . , T − 1.
b. Part a implies that we can use the moment conditions
E

y i,t1
y it
−
z it , . . . , z i1
expx it 
expx i,t1 

 0, t  1, . . . , T − 1

to estimate . Explain why using these moments directly can cause computational problems.
(Hint: Suppose For example, if x itj  0 for some j and all i and t. What would happen if  j is
made larger and larger?)
c. Define the average of the population means across time as
T

x ≡ T

−1

∑ Ex ir .
r1

Show that if you multiply the moment conditions in part b by exp x , the resulting moment
conditions are
E

y i,t1
y it
−
z it , . . . , z i1
expx it −  x 
expx i,t1 −  x 

 0.

[See Windmeijer (2002, Economics Letters).] How does this help with the computational

441

problem in part b?
d. What would you use in place of  x given that  x is unknown?
e. Suppose that x it : t  1, 2, . . . , T is sequentially exogenenous, so that we can take
z it  x it . Show that
E y it −

y i,t1
x it , . . . , x i1
expx i,t1 − x it 

 0, t  1, . . . , T − 1.

In other words, we can write moment conditions in terms of the first difference of the
explanatory variables.
Solution
a. From y it  c i expx it r it for all t  1, . . . , T we have
y it
 c i r it
expx it 
y i,t1
 c i r i,t1 ,
expx i,t1 
and subtracting the first equation from the second gives
y i,t1
y it
−
 c i r it − r i,t1 .
expx it 
expx i,t1 
Now
Ec i r it − r i,t1 |z it , . . . , z i1 , c i   c i Er it |z it , . . . , z i1 , c i 
− Er i,t1 |z it , . . . , z i1 , c i 
 c i 1 − 1  0
b. Suppose x it1  0 for all i and t. Then  1 x it1 →  as  j → , which means
exp 1 x it1   2 x it2 . . .  K x itK  → 
for all i and t, for any values of  2 , . . . ,  K . Then

442

y i,t1
y it
−
→0
expx it 
expx i,t1 
as  1 → , and so the residual function can be made closer and closer to zero by increasing  1
without bound.
c. Multiplying the original moment conditions by the exp x  clearly does not does not
change that they still hold:
y i,t1
y it
−
z it , . . . , z i1
expx it 
expx i,t1 

exp x E

 0.

The left hand side is simply
exp x y i,t1
exp x y it
y i,t1
y it
−

−
.
expx it 
expx i,t1 
expx it −  x 
expx i,t1 −  x 
Using these new moment conditions does not lead to the problem discussed in part b because
the deviated covariates, x it −  x , can take on both negative and positive values.
d. We would use the sample counterpart,
T

x̄ ≡ T

−1

∑
r1

N

N

−1

∑ x ir
i1

N

 NT

−1

T

∑ ∑ x ir .
i1 r1

In the sample, the deviated variables, x it − x̄ , will always take on positive and negative values.
Technically we should account for the estimation error in x̄ but it likely has a minor effect.
The sample moments we would like to make close to zero have the form
N

T−1

∑ ∑ g ′it
i1 t1

y i,t1
y it
−
expx it − x̄ 
expx i,t1 − x̄ 

where g it ≡ g t z it , . . . , z i1  is a function of the instruments up through time t. Or, stack the
moments over time rather than sum them up to enhance efficiency. In either case, we would

443

use GMM with an optimal weighting matrix to set the sample moments as close to zero as
possible.
e. If we can take z it  x it then we know from part a that
Ec i r it − r i,t1 |x it , . . . , x i1 , c i .
That means any function of x it , . . . , x i1  can multiply the moment conditions and we are still
left with a zero conditional mean. In particular,
E expx it 

y i,t1
y it
−
expx it 
expx i,t1 

x it , . . . , x i1

 0, t  1, . . . , T − 1

and simple algebra shows
expx it 

y i,t1
y it
−
expx it 
expx i,t1 

444

 y it −

y i,t1
.
expx i,t1 − x it 

Solutions to Chapter 19 Problems
19.1. If r i is the same for any random draw i, then it is nonrandom. From equation (19.9),
we can write
Pw i  1|x i    1   2 x i2 . . .  K x iK − logr/
  1 − logr/   2 /x i2 . . .  K /x iK ,
where it is helpful to separate the intercept from the slopes. From this equation, it is clear that
probit of w i on 1, x i2 , . . . , x iK  consistently estimates  1 − logr/,  2 /, ...,  K /. Let
 ∗1   1 − logr/ and define  ∗j   j /, j  1, . . . , K. Unfortunately, we cannot recover the
original parameters because, for example,  1   ∗1  logr, and we do not know . Although
 ∗1 is identified, and logr is known, we can not recover the scaled intercept
 ∗1 ≡  1 /   ∗1  logr/. Of course, we directly estimate the scaled slopes,  j /, and so we
can estimate the direction of the effects on Ey|x. But we cannot estimate the original
intercepts or slopes. Assuming  h ≠ 0, we can estimate  j / h for j ≠ h, which means we can
estimate the relative effects. Unlike in the case where the r i vary, we cannot estimate the
magnitudes of the partial effects on Ey|x.
19.2. a. It sufficies to find the density of logw i  conditional on x i ; of course we arrive at
the same place for the MLEs if we work with Dw i |x i . Now
logw i   maxlogf, logy i 
and logy i   x i   u i , where
Du i |x i   Normal0,  2 .
̃ i  logw i , ̃f  logf, and ỹ i  logy i , so that Dỹ i |x i   Normalx i ,  2 . Now
Let w

445

̃f
xi  ui
≤

 xi

̃ i  ̃f|x i   Pỹ i ≤ ̃f|x i   P
Pw
P

u i ≤ ̃f − x i  x
i





̃f − x i 


̃  ̃f is simply the conditional density for ỹ i , that is,
The conditional density for w
̃ − xi
w
1
.
 

̃ i conditional on x i can be written as
Therefore, the density for w
̃ − xi
w
1
 


1w̃ f̃

̃f − x i 



1w̃ f̃

.

It follows that the log likelihood for a random draw i is
̃ i  ̃f log
1w

̃ i − xi
w
1
 


̃ i  ̃f log 
 1w

̃f − x i 


.

Notice that when ̃f  0 we get the same log likelihood as for the Type I Tobit model for corner
solutions, which we covered in Chapter 17.
b. Because u is independent of x with a Normal0,  2  distribution,
Eexpu|x  Eexpu  exp 2 /2,
where the second inequality follows from the moments of a lognormal distribution. Therefore,
Ey|x  expxEexpu|x  expx exp 2 /2
 expx   2 /2.
After using the MLE on the censored data to obtain ̂ and ̂ 2 , we can use
Êy|x  expx̂  ̂ 2 /2.
c. It is hard to see why Ew|x would be of much interest. In most cases the floor, f, is

446

arbitrary, and so it is unclear why we would be interested in how the mean of the censored
variable changes with the x j . One could imagine that, if f is a minimum wage and w i represents
the observed wage for worker i, one might be interested to know how a change in a policy
variable affects observed wage, on average.
19.3. a. The two-limit Tobit model from Section 17.7 could be used with limits at 0 and 10.
b. The lower bound of zero reflects the fact that pension contribution cannot be a negative
percentage of income. But the upper bound of 10 percent is imposed by law, and is essentially
arbitrary. If we defined a variable as the desired percentage put into the pension plan, then it
could range from 0 to 100. So the upper bound of 10 can be viewed as a data censoring
problem because some individuals presumably would contribute y  10 if the limit were
raised. But it depends on the purpose of the study: to estimate the effects within the current
institutional setting or to estimate effects on pension contributions in the absense of
constraints.
c. From Problem 17.3 part b, with a 1  0, we have
Ey|x  x  a 2 − x/ − −x/
 x/ − a 2 − x/  a 2 x − a 2 /.
Taking the derivative of this function with respect to a 2 gives
∂Ey|x/∂a 2  x/  a 2 − x/  a 2 − x/  a 2 − x/
 x − a 2 / − a 2 /x − a 2 /
 x − a 2 /.
We can plug in a 2  10 to obtain the approximate effect of increasing the cap from 10 to 11.
For a given value of x, we would compute x̂ − 10/̂ , where ̂ and ̂ are the MLEs. We
might evaluate this expression at the sample average of x or at other interesting values (such as

447

across gender or race).
d. If y i  10 for i  1, … , N, ̂ and ̂ are just the usual type I Tobit estimates with lower
limit at zero: there are no observations that contribute to the third piece in the log likelihood.
19.4. a. If you are interested in the effects of things like age of the building and
neighborhood demographics on fire damage, given that a fire has occurred, then there is no
problem. We simply need a random sample of buildings that actually caught on fire. You
might want to supplement this with an analysis of the probability that buildings catch fire,
given building and neighborhood characteristics. But then a two-part (hurdle) model is
appropriate.
b. The issue in this case is a bit subtle because it depends on the population of interest.
One possibility is, at a given point in time, to define the population of interest to be workers
currently enrolled in a 401(k) plan. Then using a random sample of workers already in a 401(k)
plan is appropriate. But workers currently enrolled in a plan may not represent those that may
be enrolled in the future. In fact, we might think of being interested in a scenario where all
workers are enrolled. It makes sense to think about the sensitivity of contributions to the match
rate for the population of all workers. Of course, in general, using a random sample of those
already enrolled leads to a sample selection problem for estimating the parameters for the
larger population – much like the problem of estimating a wage offer equation (except that, in
addition to not observing contributions, we would not observe a match rate for those not
enrolled).
19.5. Because IQ and KWW are both indicators of abil we can write
IQ   1 abil  a i , KWW   1 abil  a i ,
where  1 ,  2  0. For simplicity, I set the intercepts to zero, as this does not affect the
448

conclusions of the problem. The structural equation is logwage  z 1  1  abil  v. Now,
given the selection mechanism described in Example 19.4 (IQ is observed if IQ  r ≥ 0), we
can assume that
Ev|z 1 , abil, IQ, KWW, r  0,
which is the standard ignorability assumption with the added assumption that v is unrelated to r
in the conditional mean sense. To see what else we need, write abil in terms of IQ and a 1 and
plug into the structural equation to get
−1
logwage  z 1  1   −1
1 IQ  v   1 a 1 .

Now, we want to use KWW as an instrument for IQ in this equation, and use 2SLS on the
selected sample. The full set of instruments is z 1 , KWW. From Theorem 19.1 we need the
error u  v   −1
1 a 1 to satisfy Eu|z 1 , KWW, s  0. Now, because s is a function of IQ and r,
from (19.124) we have Ev|z 1 , KWW, s  0. To ensure Ea 1 |z 1 , KWW, s  0 we can assume
Ea 1 |z 1 , KWW, r  0 or, equivalently, Ea 1 |z 1 , a 2 , r  0. The symmetrical assumption on a 2
is Ea 2 |z 1 , a 1 , r  0. Loosely, in addition to the errors in the indicator equations being
uncorrelated, they are also uncorrelated with the selection error. But for all of this to work we
need to make zero conditional mean assumptions.
19.6. This is essentially given in equation (19.45), but were we allow the truncation points
to depend on x i . Let y i given x i have density fy|x 1 , , , where  is the vector indexing
Ey i |x i  and  is another set of parameters (often a single variance parameter). Then the
density of y i given x i , s i  1, when s i  1a 1 x i   y i  a 2 x i , is
py|x i , s i  1 

fy|x i ; , 
, a 1 x i   y  a 2 x i .
Fa 2 x i |x i ; ,  − Fa 1 x i |x i ; , 

449

(19.124)

In the Hausman and Wise (1977) study, y i  logincome i , a 1 x i   −, and a 2 x i  was a
function of family size (which determines the official poverty level).
19.7. a. If Eu 1 |v 2    1 v 2   2 v 22 − 1 then, because u 1 , v 2  is independent of x,
Ey 1 |x, v 2   x 1  1  Eu 1 |v 2   x 1  1   1 v 2   2 v 22 − 1.
Now, using iterated expectations (since y 2 is a function of x, v 2 ), we have
Ey 1 |x, y 2   x 1  1   1 Ev 2 |x, y 2    2 Ev 22 |x, y 2  − 1
 x 1  1   1 Ev 2 |x, y 2    2 Varv 2 |x, y 2   Ev 2 |x, y 2  2 − 1.
We only need these expressions for y 2  1. Using Ev 2 |v 2  −x 2   x 2  and
Varv 2 |v 2  −x 2   1 − x 2 x 2   x 2 ,
we have
Ey 1 |x, y 2  1  x 1  1   1 Ev 2 |v 2  −x 2    2 Varv 2 |v 2  −x 2 
 x 1  1   1 x 2    2 1 − x 2 x 2   x 2   x 2  2 − 1
 x 1  1   1 x 2  −  2 x 2 x 2 .
b. Now, we obtain x i ̂ 2 and ̂ i2 after first-stage probit and then run the regression
y i1 on x i1 , ̂ i2 , ̂ i2  x i ̂ 2 
using the selected sample. We get consistent estimators of  1 ,  1 , and − 2 .
c. A standard F test of joint significance of ̂ i2 and ̂ i2  x i ̂ 2  (two restrictions) in the
regression from part b is a valid test, assuming homoskedasticity in the population structural
model. As usual, the null is no sample selection bias.
19.8. If we replace y 2 with ŷ 2 , we need to see what happens when y 2  z 2  v 2 is plugged
into the structural mode:

450

y 1  z 1  1   1  z 2  v 2   u 1

(19.125)

 z 1  1   1  z 2   u 1   1 v 2 .
So, the procedure is to replace  2 in (19.125) its N -consistent estimator, ̂ 2 . The key is to
note that the error term in (19.125) is u 1   1 v 2 . If the selection correction is going to work
whn the fitted value is plugged in for y 2 , we need the expected value of u 1   1 v 2 given z, v 3 
to be linear in v 3 (in particular, it cannot depend on z). Then we can write
Ey 1 |z, v 3   z 1  1   1  z 2    1 v 3 ,
where Eu 1   1 v 2 |v 3    1 v 3 by normality. Conditioning on y 3  1 gives
Ey 1 |z, y 3  1  z 1  1   1  z 2    1 z 3 .

(19.126)

A sufficient condition for (19.126) is that u 1 , v 2 , v 3  is independent of z with a trivariate
normal distribution. We can get by with less than this, but the nature of v 2 is restricted. If we
use the IV approach – rather than plugging in fitted values – we need assume nothing about v 2 ;
y 2  z 2  v 2 is just a linear projection.
As a practical matter, if we cannot write y 2  z 2  v 2 , where v 2 is independent of z and
approximately normal, then the OLS alternative will not be consistent. Thus, equations where
y 2 is binary, or is some other variable that exhibits nonnormality, cannot be consistently
estimated using the OLS procedure. This is why 2SLS is generally preferred.
19.9. Here is the Stata session I used to implement Procedure 19.4, although the standard
errors in the second step are not adjusted to account for the first-stage Tobit estimation. Still,
v̂ 3 is not statistically significant, and adding it is not really necessary in this application.
. tobit hours exper expersq age kidslt6 kidsge6 nwifeinc motheduc fatheduc
huseduc, ll(0)
Tobit regression

Number of obs
LR chi2(9)
Prob  chi2

451





753
261.82
0.0000

Log likelihood  -3823.9826

Pseudo R2



0.0331

----------------------------------------------------------------------------hours |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------exper |
136.9463
17.27271
7.93
0.000
103.0373
170.8554
expersq | -1.947776
.5388933
-3.61
0.000
-3.005708
-.8898433
age | -54.78118
7.568762
-7.24
0.000
-69.63985
-39.9225
kidslt6 | -864.3263
111.6246
-7.74
0.000
-1083.463
-645.1896
kidsge6 | -24.68934
38.77122
-0.64
0.524
-100.8034
51.42468
nwifeinc | -5.312478
4.572888
-1.16
0.246
-14.28978
3.664822
motheduc |
24.28791
16.74349
1.45
0.147
-8.582209
57.15803
fatheduc |
6.566355
16.00859
0.41
0.682
-24.86103
37.99374
huseduc |
3.129767
17.46452
0.18
0.858
-31.15583
37.41537
_cons |
1548.141
437.1192
3.54
0.000
690.0075
2406.275
---------------------------------------------------------------------------/sigma |
1126.282
41.77533
1044.271
1208.294
----------------------------------------------------------------------------Obs. summary:
325 left-censored observations at hours0
428
uncensored observations
0 right-censored observations
. predict zd3hat
(option xb assumed; fitted values)
. sum zd3hat
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------zd3hat |
753
302.7538
814.8662 -2486.756
1933.23
. gen v3hat  hours - zd3hat if hours  0
(325 missing values generated)
. ivreg lwage exper expersq v3hat (educ  age kidslt6 kidsge6 nwifeinc
motheduc fatheduc huseduc)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | 34.6676357
4 8.66690893
Residual | 188.659805
423 .446004268
------------------------------------------Total | 223.327441
427 .523015084

Number of obs
F( 4,
423)
Prob  F
R-squared
Adj R-squared
Root MSE








428
9.97
0.0000
0.1552
0.1472
.66784

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.085618
.0213955
4.00
0.000
.0435633
.1276726
exper |
.0378509
.0137757
2.75
0.006
.0107734
.0649283
expersq | -.0007453
.0004036
-1.85
0.065
-.0015386
.0000479
v3hat | -.0000515
.0000412
-1.25
0.211
-.0001325
.0000294
_cons | -.1786154
.2925231
-0.61
0.542
-.7535954
.3963645
----------------------------------------------------------------------------Instrumented: educ
Instruments:
exper expersq v3hat age kidslt6 kidsge6 nwifeinc motheduc
fatheduc huseduc
-----------------------------------------------------------------------------

452

If we just use 2SLS on the selected sample without including v̂ 3 , and the IVs for educ are
motheduc, fatheduc, and huseduc, then the estimated return to education is about 8.0%:
. ivreg lwage exper expersq (educ  motheduc fatheduc huseduc)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | 33.3927368
3 11.1309123
Residual | 189.934704
424 .447959208
------------------------------------------Total | 223.327441
427 .523015084

Number of obs
F( 3,
424)
Prob  F
R-squared
Adj R-squared
Root MSE








428
11.52
0.0000
0.1495
0.1435
.6693

----------------------------------------------------------------------------lwage |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------educ |
.0803918
.021774
3.69
0.000
.0375934
.1231901
exper |
.0430973
.0132649
3.25
0.001
.0170242
.0691704
expersq | -.0008628
.0003962
-2.18
0.030
-.0016415
-.0000841
_cons | -.1868572
.2853959
-0.65
0.513
-.7478242
.3741097
----------------------------------------------------------------------------Instrumented: educ
Instruments:
exper expersq motheduc fatheduc huseduc
-----------------------------------------------------------------------------

19.10. a. Substitute the reduced forms for y 1 and y 2 into the third equation:
y 3  max0,  31 z 1    32 z 2   z 3  3  v 3 
≡ max0, z 3  v 3 ,
where v 3 ≡ u 3   31 v 1   32 v 2 . Under the assumptions given, v 3 is independent of z and
normally distributed. Thus, if we knew  1 and  2 , we could consistently estimate  31 ,  32 , and
 3 from a Tobit of y 3 on z 1 , z 2 , and z 3 . From the usual argument, consistent estimators are
obtained by using initial consistent estimators of  1 and  2 . Estimation of  2 is simple: just
use OLS using the entire sample. Estimation of  1 follows exactly as in Procedure 19.3 using
the system
y 1  z 1  v 1
y 3  max0, z 3  v 3 ,

453

where y 1 is observed only when y 3  0.
Given ̂ 1 and ̂ 2 , form z i ̂ 1 and z i ̂ 2 for each observation i in the sample. Then, obtain
̂ 31 , ̂ 32 , and ̂ 3 from the Tobit
y i3 on z i ̂ 1 , z i ̂ 2 , z i3
using all observations.
For identification, z 1 , z 2 , z 3  can contain no exact linear dependencies. Necessary is that
there must be at least two elements in z not also in z 3 .
Obtaining the correct asymptotic variance matrix is complicated. It is most easily done in a
generalized method of moments framework. Alternatively, it is easy to use bootstrap
resampling on both steps of the estimation procedure.
b. This is not very different from part a. The only difference is that  2 must be estimated
using Procedure 19.3. Then follow the steps from part a.
c. We need to estimate the variance of u 3 ,  23 , and then use the standard formula for the
mean of a Tobit model. This gives the ASF as a function of y 2 , y 3 , z 3  and the parameters
 31 ,  32 ,  3 ,  23 .
19.11. a. This follows from the usual iterated expectations argument, because Z i is a
function of x i :
Es i Z ′i rw i ,  o   EEs i Z ′i rw i ,  o |x i , s i 
 Es i Z ′i rw i ,  o |x i , s i   0
because Erw i ,  o |x i , s i   0.
b. We modify equation (14.24) from Chapter 14 to allow for selection:

454

′

N

min
Θ

∑

s i Z ′i r i 

−1

N

N

−1

i1

∑

s i Z ′i Z i

i1

N

∑ s i Z ′i r i 

.

i1

For consistency, we would have to assume that rank Es i Z ′i Z i   L – which means, that in the
selected sample, the instrument matrix is not perfectly collinear – and we have to assume that
 o is the unique solution to Es i Z ′i rw i ,  o   0. For N -asymptotic normality, we would
also have to assume that rank Es i Z ′i ∇  rw i ,  o   P, the dimension of . None of the
conditions can be true unless Ps i  1  0, that is, we observe a randomly drawn observation
with positive probability. But Ps i  1  0 is not nearly sufficient, as we might not have
identification in the selected population even if we have identification in the full population.
(For example, we might have an instrument that varies sufficiently in the full population but
not in the s  1 subpopulation.)
c. Let  denote the (system) nonlinear 2SLS estimator on the selected sample. For the
minimum chi-square estimator, we would compute
N

̂  N −1 ∑ s i Z ′i r i r  ′ Z i

i
i1

and then solve
′

N

min
Θ

∑

s i Z ′i r i 

̂ −1


i1

N

∑ s i Z ′i r i 

.

i1

19.12. a. Take the expected value of (19.56) conditional on z, y 3  :
Ey 1 |z, y 3   z 1  1   1 Ey 2 |z, y 3   Eu 1 |z, y 3 
 z 1  1   1 Ey 2 |z, y 3 
because Eu 1 |z, y 3   0 follows from Eu 1 |z, v 3   0.

455

b. Now take the expected value of (19.56) conditional on z, v 3 :
Ey 1 |z, v 3   z 1  1   1 Ey 2 |z, v 3   Eu 1 |z, v 3 
 z 1  1   1 z 2  Ev 2 |z, v 3 
 z 1  1   1 z 2   2 v 3 .
Therefore,
Ey 1 |z, y 3   z 1  1   1 z 2   2 Ev 3 |z, y 3 ,
and when y 3  1 we get the usual inverse Mill’s ratio: Ev 3 |z, y 3  1  z 3 . So
Ey 1 |z, y 3  1  z 1  1   1 z 2   2 z 3 .
c. We can view it as a three-step estimation method. The first step is to obtain ̂ 3 from
probit of y i3 on z i , using all of the observations. Then, we can estimate  2 and  2 from standard
Heckit applied to y i2 using the selection sample. (My initial thought was that the two steps in
the Heckit method are treated as one, as it could be carried out by partial MLE.) Given ̂ 3 , ̂ 2 ,
and ̂ 2 , the final stage is the OLS regression
y i1 on z i1 , z i ̂ 2  ̂ 2 z i ̂ 3 
using the s i1  1 sample. Note that the final regressor, z i ̂ 3 , is simply our estimate of
Ey 2 |z, y 3  1. Intuitively, if there is one relevant element in z i not in z i1 , then
Ey i2 |z i , y i3  1 has sufficient variation apart from z i1 to identify  1 and  1 . However, I did
overlook one issue when I wrote this problem: we cannot get a very good estimate of  2 , or  2
for that matter, in the preliminary Heckit unless we can set an element of  2 equal to zero. In
other words, we would really need an exclusion restriction in the reduced form of y 2 in order to
get a good Heckit estimate of  2 . Thus, this procedure seems no better – and perhaps even
worse – than Procedure 19.2, even when we assume Eu 1 |z, v 3   0.

456

If y 2 is always observed, then we can estimate  2 by a first-stage OLS regression, and we
could then estimate  2 precisely, also, without resorting to an exclusion restriction in the
reduced form of y 2 .
d. Unlike Procedure 19.2, the method in part c does not work if Eu 1 |z, y 3  ≠ 0. Therefore,
there is little to recommend it.
e. If Eu 1 |z, y 2 , y 3   0, we would just use OLS on the selected sample: y i1 on z i , y i2 .
19.13. a. There is no sample selection problem because, by definition, you have specified
the distribution of y given x and y  0. We only need to obtain a random sample from the
subpopulation with y  0.
b. Again, there is no sample selection bias because we have specified the conditional
expectation for the population of interest. If we have a random sample from that population,
NLS is generally consistent and N -asymptotically normal.
c. We would use a standard probit model. Let w  1y  0. Then w given x follows a
probit model with Pw  1|x  x.
d. Ey|x  Py  0|x  Ey|x, y  0  x  expx. So we would plug in the NLS
estimator of  and the probit estimator of .
e. By definition, there is no sample selection problem when you specify the conditional
distribution – conditional means – for the second part. As discussed in Section 17.6.3,
confusion can arise when two part models are specified with unobservables that may be
correlated, as in equation (17.50):
y  s  expx  u,
s  1x  v  0,
so that s  0  y  0. As shown in Section 17.6.3, if u and v are correlated then estimation of

457

 does use methods that are closely related to the Heckman sample selection correction. But 
does not tell us what we need to know because both Ey|x and Ey|x, y  0 are much more
complicated than in the truncated normal or lognormal hurdle cases. See Section 17.6.3 for
further discussion.
19.14. a. Write u   0 1 − s   1 s  e where, by assumption, Ee|z, s  0. Plugging this
expression for u into (19.30) gives
y   1   2 x 2 . . .  K x K   0 1 − s   1 s  e
Ee|z, s  0.
Using the selected sample and applying IV corresponds to multiplying the equation through by
s, and then applying 2SLS. We have
s  y   1 s   2 s  x 2  . . .  K s  x K    1 s  s  e
  1   1 s   2 s  x 2  . . .  K s  x K   s  e,
where we use s1 − s  0 and s 2  1. Because Es  e|z, s  0, it follows that, under the rank
conditions in Theorem 19.1, 2SLS applied to the selected sample consistently estimates
 1   1 ,  2 , … ,  K .
b. This is not so much a “show” question as it is just recognizing a basic property of
conditional expectations: if u, s is independent of z, then Eu|z, s  Eu|s. Because we are
willing to assume something like independence between u and z (or, at least, a zero conditional
mean), the important assumption would be independence between s and z. But if the mean of
the unobservable, u, changes with s, why would we assume that the mean of the exogenous
observables, Ez|s, does not? Even Ez|s  Ez is a strong assumption, let alone full
indepdence between z and s.
19.15. a. We cannot use censored Tobit because that requires observing x when whatever

458

the value of y. Instead, we can use truncated Tobit: we use the distribution of y given x and
y  0. If we observed x always then using the truncated Normal regression model would be
inefficient, but censored Tobit for Dy|x implies truncated Tobit for Dy|x, y  0.
b. Because we have assumed y given x follows a standard Tobit, Ey|x is the parametric
function
Ey|x  x/x  x/.
Therefore, even though we never observe some elements of x when y  0, we can still estimate
Ey|x because we can estimate  and  and we have an expression for Ey|x that (we assume)
holds for all x. To estimate  and  2 We do have to assume that x varies enough in the
subpopulation where y  0, namely, rank Ex ′ x|y  0  K. In the case where an element of x
is a derived price, we need sufficient price variation for the population that consumes some of
the good.
19.16. a. To obtain the expected value of
y1  z11  1y2  u1
conditional on z, r 2 , v 2 , use the fact that y 2 is a function of z, v 2 , and use independence of
u 1 , v 2  and z:
Ey 1 |z, r 2 , v 2   z 1  1   1 y 2  Eu 1 |z, r 2 , v 2 
 z 1  1   1 y 2  Eu 1 |v 2 .
Now use the linearity assumption Eu 1 |v 2    1 v 2 to get
Ey 1 |z, r 2 , v 2   z 1  1   1 y 2   1 v 2 .
b. With s 2  1y 2  w 2 , s 2 is clearly a function of z, r 2 , v 2 , and so s 2 is redundant in
Ey 1 |z, r 2 , v 2 , s 2 :

459

Ey 1 |z, r 2 , v 2 , s 2   Ey 1 |z, r 2 , v 2   z 1  1   1 y 2   1 v 2 .
c. Because of part b, if we could observe v i2 whenever s i2  1 we could consistently
estimate  1 ,  1 , and  1 by running the regression
y i1 on z i1 , y i2 , v i2 if s i2  1.
Naturally, we can replace v i2  y i2 − z i  2 with v̂ i2 ≡ y i2 − z i ̂ 2 for a consistent estimator ̂ 2 of
 2 . That estimator should be from a censored normal regression using
w i2  minr i2 , z i  2  v i2 
and then defining
v̂ i2 ≡ y i2 − z i ̂ 2 if y i2  r i2 .
Then run the regression
y i1 on z i1 , y i2 , v̂ i2 if s i2  1.
We can use the delta method to obtain valid standard errors, or bootstrap both steps of the
procedure. A simple test of
19.17. a. The assumption is that, conditional on x i , c i , u it is independent of the entire
history of censoring values, r i1 , r i2 , . . . , r iT . This a kind of strict exogeneity assumption on the
censoring, which rules out the censoring values being related to current or past shocks to y. It
does allow censoring to be arbitrarily correlated with heterogeneity c i .
b. Subsititue for y it to get
w it  1x it   c i  u it  r it 
and then substitue for c i to get

460

w it  1x it     x̄ i   r̄ i  a i  u it  r it 
 1a i  u it   r it − x it     x̄ i   r̄ i 
1

r − x it     x̄ i   r̄ i 
a i  u it 
 it
2
2 1/2
 2a   2u  1/2
 a   u 

.

Now use the fact that Da i  u it |x i , r i  ~ Normal0,  2a   2u :
Pw it  1|x i , r i   1 − 


r it − x it     x̄ i   r̄ i 
 2a   2u  1/2

x it     x̄ i   r̄ i − r it
 2a   2u  1/2

≡ x it  au   au  x̄ i  au   au r̄ i   au r it 
where  au  / 2a   2u  1/2 ,  au  / 2a   2u  1/2 ,  au  / 2a   2u  1/2 , and
 au  −1/ 2a   2u .
c. From part b, we can estimate all of the scaled coefficients, including  au , by pooled
probit, provided x it  and r it  have time variation for at last some units. But
  − au / au
and so we just use
̂  −̂ au /̂ au .
d. The pooled estimation from part c only allows us to estimate  2a   2u and the unscaled
parameters. If we add the assumption that u it : t  1, 2, . . . , T are independent then
Cova i  u it , a i  u is   Vara i    2a for all t ≠ s. We can use a slight modification of
correlated random effects probit, which takes the idiosyncratic error to have unit variance. To
this end, write

461

w it  1x it     x̄ i   r̄ i  a i  u it  r it 
x it     x̄ i   r̄ i − r it 
a i  u it 
1

u
u
≡1

x it     x̄ i   r̄ i − r it 
 g i  e it
u

where e it  u it / u and g i  a i / u . This shows that if we apply the CRE probit model to w it on
x it , 1, x̄ i , r̄ i , r it  we consistentlye estimate  u  / u ,  u  / u ,  u  / u , and  u  −1/ u as
the coefficients and
Varg i    2a / 2u
as the heterogeneity variance. Thus, we can recover the original unscaled coefficients,
 2u  1/ 2u , and
 2a   2g / 2u .
19.18. a. Conditional on y  0, y follows a truncated normal distribution. So truncated
normal regression would consistently estimate  and  2 .
b. Because we are claiming that Dy|x follows a type I Tobit in the population, we use the
expected value derived from that assumption. Namely,
Ey|x  x/x  x/.
and then we compute derivatives and changes with respect to x j , as usual.
This differs from a hurdle model because we do not have a separate model for Py  0|x;
we assume this is also governed by the Tobit model, so Py  0|x  1 − x/.
c. We could not estimate a hurdle model in this case because we have no data when y  0.
We have not sampled from that part of the population, and so we cannot estimate a model for
Ps  1|x where s  1y  0.

462

19.19. a. First, if r i  0 the observation contains no information for estimating the
distribution Dy i |x i because then Pw i  0|x i   1 regardless of Dy i |x i . So what follows is
only relevant for r i ∈ 1, 2, 3, . . . .
For 0 ≤ w  r i ,
Pw i  w|x i   Py i  w|x i   f y w|x i .
Next, w i  r i if and only if y i ≥ r i , and so
Pw i  r i |x i   1 − Py i  r i |x i   1 − Py i ≤ r i − 1|x i 
 1 − F y r i − 1|x i .
We can write the conditional density of w i as
f w w|x i , r i   f y w|x i  1wr i  1 − F y r i − 1|x i  1wr i  , w  0, . . . , r i .
b. In the Poisson case with an exponential mean, the conditional density of y i is
f y y|x i ;  

exp− expx i expx i  y
y!

and the cdf is
y

F y y|x i   exp− expx i 

∑
h0

expx i  h
h!

Now just plug this into the general formula in part a.
c. Maximum likelihood estimators based on censored data are generally not robust to
misspecification of the underlying population – even when that distribution is in the linear
exponential family. (The log likelihood for the censored variable is not in the linear
exponential family; even if it were, Ew i |x i  depends on the underlying distribution.) Just like
censored regression with a normal distribution is not robust for estimating the mean parameters

463

under nonnormality, neither is censored regression with a Poisson distribution. One way to see
this is to write down the score for the general case and observe that just having Ey i |x i 
correctly specified will not imply that the score has zero expectation.
d. As we know from earlier chapters, nonlinear least squares, Poisson QMLE, and other
QMLEs in the LEF are robust for estimating the mean parameters. Thus, if there were no data
censoring, we could use the Poisson QMLE to estimate . With data censoring, Ew i |x i 
always depends on the underlying population distribution. Thus, in general we need to specify
Dy i |x i  even if we are primarily interested in Ey i |x i .
e. Because data censoring requires us to have Dy i |x i  correctly specified, a strong case can
be made for specifying flexible models for Dy i |x i  – even if we are primarily interested in
Dy i |x i . For example, if we use a NegBin I or NegBin II model, these at least include the
Poisson as (limiting) special cases. So, if we are pretty sure the underlying population has
overdispersion, we can use one of these distributions in accounting for the right censoring.
Ideally we would have a distribution that allows underdispersion, too.

464

Solutions to Chapter 20 Problems
20.1. a. Just use calculus and set the derivative to zero:
N0

∑ p −1j w i − ̂ w   0
i

i1

or
N0

∑

N0

p −1
ji w i



i1

∑

N0

̂w
p −1
ji 

∑ p −1j



i

i1

̂ w .

i1

Solving for ̂ w gives
−1 N 0

N0

∑

̂ w 

∑

p −1
ji

i1

N0

p −1
ji w i



i1

∑ v ji w i
i1

where
−1

N0

∑

v ji 

p −1
jh

p −1
ji .

ih

b. From equation (20.7),
Ps i  1|z i , w i   Ps i  1|z i   p 1 z i1 . . . p J z iJ
which implies
Es i |j i , w i   p j i
because the stratum for observation i is j i if and only if z ij  1. Now
N

E N

−1

∑s i /p j w i
i

N

N

i1

−1

∑ Es i /p j w i 
i

i1

and, by iterated expectations,

465

Es i /p j i w i   EEs i /p j i w i |j i , w i 
 EEs i |j i , w i /p j i w i 
 Ep j i /p j i w i   Ew i    o .
This shows E̃ w    o .
c. Notice that ̃ w depends on N, the number of times we sampled the population, including
when we did not record the observation. By contrast, to obtain ̂ w we need only need
information on the sampling weights and data on the units actually kept. Therefore, in addition
to knowing the sampling probabilities, ̃ w requires the extra information that we know how
many observations were discarded by the VP sampling scheme.
20.2. Write the log likelihood for all N observations as
N

J

∑ ∑ z ih s i logp h   1 − s i  log1 − p h .
i1 h1

For a given j ∈ 1, 2, . . . , J, take the derivative with respect to p j , and set the result to zero:
N

∑ z ij
i1

s i − 1 − s i 
p̂ j
1 − p̂ j 

≡0

or, by obtaining a common denominator,
N

∑ z ij
i1

1 − p̂ j s i − p̂ j 1 − s i 
p̂ j 1 − p̂ j 

≡ 0.

Of course, the problem only makes sense for interior solutions 0  p̂ j  1 so the first order
condition is equivalent to
N

∑1 − p̂ j z ij s i − p̂ j z ij 1 − s i 
i1

Simple algebra gives

466

≡ 0.

N

∑ z ij s i 
i1

N

∑ p̂ j z ij
i1

or
N
z ij s i
∑ i1
Mj
≡
.
p̂ j 
N
Nj
∑ i1 z ij

20.3. a. To be specific, consider the case of variable probility sampling, where the
probability weights are
pz i   p 1 z i1 . . . p J z iJ  Ps i  1|z i , w i 
where w i  x i , y i . We can write the IPW nonliner least squares objective function as
N

min
∈Θ

∑
i1

s i y − mx ,  2 ,
i
i
pz i 

which is for form useful for studying asymptotic properties. (For the asymptotic distribution
theory, we divide the objective function by two to make the notation easier.)
b. For VP sampling, we have already assumed that each p j  0, and, because we can write
−1
|s i /pz i |≤ maxp −1
1 , . . . , pJ 

it follows that the regularity conditions sufficient for consistency of NLS on a random sample
are also sufficient for NLS on a VP sample: the objective function is still continuous and the
moment conditions do not need to be changed because
s i y − mx ,  2 ≤ maxp −1 , . . . , p −1 y − mx ,  2 .
i
i
i
i
J
1
pz i 
Further, we know generally that
E

s i y − mx ,  2
i
i
pz i 

467

 Ey i − mx i ,  2 

and so if  o uniquely minimizes the right hand side, it uniquely minizes the left hand side, too.
c. The theory in Section 19.8 can be applied directly. In particular, we can use equation
(19.90) because the probabilities are known, not estimated. In the formula,
A o  EHw i ,  o   EAx i ,  o 
 E∇  mx i ,  o  ′ ∇  mx i ,  o 
and
si
∇  qx i ,  o  ′ ∇  qx i ,  o 
pz i  2

Bo  E

s i u 2i
∇ mx i ,  o  ′ ∇  mx i ,  o 
2 
pz i 

E

where u i  y i − mx i ,  o . We can consistently estimate A o as
N

ÂN

−1

∑
i1

s i ∇ mx , ̂  ′ ∇ mx , ̂ 

i w

i w
pz i 

and B o as
N

s i û 2i
∇ mx i , ̂ w  ′ ∇  mx i , ̂ w 
2 
pz i 

̂  N −1 ∑
B
i1

where û i  y i − mx i , ̂ w  are the residuals. Then
−1
̂ Â −1 /N
Avar̂ w   Â B

(which does not actually require knowing N, as it cancels everywhere).
d. The formula does not generally simplify because Eu 2i |x i , z i  might depend on z i even if
Varu i |x i    2o . [In fact, we do not even assume that Eu i |x i , z i   0 in this problem because
the stratification may be endogenous.]
e. If mx,  is misspecified we must use a more general estimator for A ∗ based on

468

A ∗  EHw i ,  ∗   E∇  mx i ,  ∗  ′ ∇  mx i ,  ∗  − u ∗i ∇ 2 mx i ,  ∗ 
where  ∗ is the pseudo-true value of  that solves the population minimization problem and
u ∗i  y i − mx i ,  ∗ . Our estimator of A ∗ is
N

ÂN

−1

∑
i1

si
∇  mx i , ̂ w  ′ ∇  mx i , ̂ w  − û i ∇ 2 mx i , ̂ w 
pz i 

.

The estimator of B ∗ can be the same as in part c.
20.4. First, we can write the unweighted objective function as
J

N

−1

Nj

Nj

J

∑ ∑ qwij ,   ∑

N j /NN −1
j

j1 i1

j1

i1

Nj

J



∑ Hj
j1

∑ qwij , 

N −1
j

∑ qwij , 

,

i1

Nj

as suggested in the hint. Further, by the same argument as on page 860, N −1
j ∑ i1 qw ij , 
converges (uniformly) to Eqw, |w ∈ W j   Eqw, |x ∈ X j , where we use the fact that
the strata are determined by the conditioning variables and given by X 1 , ..., X J . Therefore, if
̄ j as N →  the unweighted objective function converges uniformly to
Hj → H
̄ J Eqw, |x ∈ X J 
̄ 1 Eqw, |x ∈ X 1  . . . H
H
Given that  o solves (20.15) for each x, we can also show  o minimizes Eqw, |x ∈ X j 
over Θ for each j: by iterated expectations (since the indicator 1x ∈ X j  is a function of x),
Eqw, |x ∈ X j   EEqw, |x|x ∈ X j ,
and if  o minimizes Eqw, |x, it must also minimize EEqw, |x|x ∈ X j . Therefore,  o
is one minimizer of (20.97) over Θ. Now we just have to show it is the unique minimizer if it
̄ j  0,  o need not be the unique
uniquely minimizes Eqw, . Without the assumption H

469

(20.97)

̄ j is strictly positive, let s j  1x ∈ X j .
minimizer of (20.97). To show uniqueness when each H
Then we can write, for any ,
J

Eqw,  − Eqw,  o  

∑ Q j Eqw, |s j  − Eqw,  o |s j ,
j1

where the Q j are the population frequencies. By assumption, the left hand side is strictly
positive when  ≠  o , which means, because Q j  0 for all j, Eqw, |s j  − Eqw,  o |s j 
must be strictly positive for at least one j; we already know that each difference is nonnegative.
̄ j  0, j  1, … , J, implies that (20.97) is uniquely minimized
This, along with the fact that H
at  o .
20.5. a. The Stata output is given below. The variables with “bar” added on denote the
district-level averages. Note that we can still use xtreg even though this is a cluster sample,
not a panel data set. An alternative for obtaining the FE estimates is the areg command in
Stata. The pooled OLS and FE estimates are identical on all explanatory variables. The pooled
OLS standard errors reported below are almost certainly incorrect because the assume no
within-district correlation in the unobservables.
. reg lavgsal bs lstaff lenroll lunch bsbar lstaffbar lenrollbar lunchbar
Source |
SS
df
MS
------------------------------------------Model | 49.9510474
8 6.24388093
Residual | 50.2303314 1839 .027313938
------------------------------------------Total | 100.181379 1847 .054240054

Number of obs
F( 8, 1839)
Prob  F
R-squared
Adj R-squared
Root MSE








1848
228.60
0.0000
0.4986
0.4964
.16527

----------------------------------------------------------------------------lavgsal |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------bs | -.4948449
.2199466
-2.25
0.025
-.9262162
-.0634736
lstaff | -.6218901
.0277027
-22.45
0.000
-.6762221
-.5675581
lenroll | -.0515063
.0155411
-3.31
0.001
-.0819865
-.0210262
lunch |
.0005138
.0003452
1.49
0.137
-.0001632
.0011908
bsbar |
.441438
.2630336
1.68
0.093
-.074438
.9573139
lstaffbar | -.1493942
.0370985
-4.03
0.000
-.2221538
-.0766346
lenrollbar |
.0315714
.0184565
1.71
0.087
-.0046266
.0677694

470

lunchbar | -.0016765
.0003903
-4.30
0.000
-.0024419
-.000911
_cons |
13.98544
.141118
99.10
0.000
13.70867
14.26221
----------------------------------------------------------------------------. xtreg lavgsal bs lstaff lenroll lunch, fe
Fixed-effects (within) regression
Group variable: distid
R-sq:

within  0.5486
between  0.3544
overall  0.4567

corr(u_i, Xb)




1848
537

Obs per group: min 
avg 
max 

3.
162




397.05
0.0000

Number of obs
Number of groups

F(4,1307)
Prob  F

 0.1433

----------------------------------------------------------------------------lavgsal |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------bs | -.4948449
.133039
-3.72
0.000
-.7558382
-.2338515
lstaff | -.6218901
.0167565
-37.11
0.000
-.6547627
-.5890175
lenroll | -.0515063
.0094004
-5.48
0.000
-.0699478
-.0330648
lunch |
.0005138
.0002088
2.46
0.014
.0001042
.0009234
_cons |
13.61783
.1133406
120.15
0.000
13.39548
13.84018
---------------------------------------------------------------------------sigma_u | .15491886
sigma_e | .09996638
rho | .70602068
(fraction of variance due to u_i)
----------------------------------------------------------------------------F test that all u_i0:
F(536, 1307) 
7.24
Prob  F  0.0000

b. The RE estimates are given below, with and without cluster-robust standard errors. Also,
the cluster-robust standard errors for FE are provided. The fully robust standard errors are
bigger than the nonrobust ones, suggesting there might be additional within-district correlation
even after accounting for an additive districte effect.
. xtreg lavgsal bs lstaff lenroll lunch
re
Random-effects GLS regression
Group variable: distid
R-sq:

bsbar lstaffbar lenrollbar lunchbar,



1848
537

Obs per group: min 
avg 
max 

3.
162




1943.89
0.0000

Number of obs
Number of groups

within  0.5486
between  0.4006
overall  0.4831

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(8)
Prob  chi2

----------------------------------------------------------------------------lavgsal |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
----------------------------------------------------------------------------

471

bs | -.4948449
.1334822
-3.71
0.000
-.7564652
-.2332245
lstaff | -.6218901
.0168123
-36.99
0.000
-.6548417
-.5889385
lenroll | -.0515063
.0094317
-5.46
0.000
-.0699921
-.0330205
lunch |
.0005138
.0002095
2.45
0.014
.0001032
.0009244
bsbar |
.2998553
.2437798
1.23
0.219
-.1779443
.777655
lstaffbar | -.0255493
.0418946
-0.61
0.542
-.1076613
.0565627
lenrollbar |
.0657286
.0157977
4.16
0.000
.0347657
.0966914
lunchbar | -.0007259
.0004022
-1.80
0.071
-.0015143
.0000625
_cons |
13.22003
.2136208
61.89
0.000
12.80135
13.63872
---------------------------------------------------------------------------sigma_u | .12627558
sigma_e | .09996638
rho | .61473634
(fraction of variance due to u_i)
----------------------------------------------------------------------------. xtreg lavgsal bs lstaff lenroll lunch
re cluster(distid)
Random-effects GLS regression
Group variable: distid
R-sq:

bsbar lstaffbar lenrollbar lunchbar,



1848
537

Obs per group: min 
avg 
max 

3.
162




556.49
0.0000

Number of obs
Number of groups

within  0.5486
between  0.4006
overall  0.4831

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(8)
Prob  chi2

(Std. Err. adjusted for 537 clusters in distid
----------------------------------------------------------------------------|
Robust
lavgsal |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------bs | -.4948449
.1939422
-2.55
0.011
-.8749646
-.1147252
lstaff | -.6218901
.0432281
-14.39
0.000
-.7066157
-.5371645
lenroll | -.0515063
.013103
-3.93
0.000
-.0771876
-.025825
lunch |
.0005138
.000213
2.41
0.016
.0000964
.0009312
bsbar |
.2998553
.3031961
0.99
0.323
-.2943981
.8941087
lstaffbar | -.0255493
.0651932
-0.39
0.695
-.1533256
.1022269
lenrollbar |
.0657286
.020655
3.18
0.001
.0252455
.1062116
lunchbar | -.0007259
.0004378
-1.66
0.097
-.0015839
.0001322
_cons |
13.22003
.2556139
51.72
0.000
12.71904
13.72103
---------------------------------------------------------------------------sigma_u | .12627558
sigma_e | .09996638
rho | .61473634
(fraction of variance due to u_i)
----------------------------------------------------------------------------. xtreg lavgsal bs lstaff lenroll lunch, fe cluster(distid)
Fixed-effects (within) regression
Group variable: distid
R-sq:

within  0.5486
between  0.3544
overall  0.4567

corr(u_i, Xb)




1848
537

Obs per group: min 
avg 
max 

3.
162




57.84
0.0000

Number of obs
Number of groups

F(4,536)
Prob  F

 0.1433

472

(Std. Err. adjusted for 537 clusters in distid
----------------------------------------------------------------------------|
Robust
lavgsal |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------bs | -.4948449
.1937316
-2.55
0.011
-.8754112
-.1142785
lstaff | -.6218901
.0431812
-14.40
0.000
-.7067152
-.5370649
lenroll | -.0515063
.0130887
-3.94
0.000
-.0772178
-.0257948
lunch |
.0005138
.0002127
2.42
0.016
.0000959
.0009317
_cons |
13.61783
.2413169
56.43
0.000
13.14379
14.09187
---------------------------------------------------------------------------sigma_u | .15491886
sigma_e | .09996638
rho | .70602068
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

c. The robust Wald test for joint significance of the four district-level averages gives a
strong rejection of the null, with p-value  .0004. Therefore, we conclude that at least some of
the variables are correlated with unobserved district effects.
. qui xtreg lavgsal bs lstaff lenroll lunch bsbar lstaffbar lenrollbar lunchbar
re cluster(distid)
. test
(
(
(
(

1)
2)
3)
4)

bsbar lstaffbar lenrollbar lunchbar
bsbar  0
lstaffbar  0
lenrollbar  0
lunchbar  0
chi2( 4) 
Prob  chi2 

20.70
0.0004

20.6. a. Only three schools in the sample have reported benefits/salary ratios of at least . 5.
The highest of these is about . 66.
. count if bs  .5
3
. list distid bs if bs  .5
-------------------
| distid
bs |
|-------------------|
68. |
9030
.6594882 |
1127. | 63160
.5747756 |
1670. | 82040
.5022581 |
-------------------

473

b. The magnitude of the coefficient on bs falls somewhat and the cluster-robust standard
error increases substantially from about . 194 to . 245, likely due to the reduction in variation of
bs within the three districts listed in part a. There is still some evidence of a salary-benefits
tradeoff.
. xtreg lavgsal bs lstaff lenroll lunch if bs  .5, fe cluster(distid)
Fixed-effects (within) regression
Group variable: distid
R-sq:

within  0.5474
between  0.3552
overall  0.4567

corr(u_i, Xb)




1845
537

Obs per group: min 
avg 
max 

3.
162




58.06
0.0000

Number of obs
Number of groups

F(4,536)
Prob  F

 0.1452

(Std. Err. adjusted for 537 clusters in distid
----------------------------------------------------------------------------|
Robust
lavgsal |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------bs | -.4560107
.245449
-1.86
0.064
-.9381705
.0261492
lstaff | -.6226836
.0431074
-14.44
0.000
-.7073637
-.5380035
lenroll | -.0518125
.0131213
-3.95
0.000
-.077588
-.026037
lunch |
.0005151
.0002157
2.39
0.017
.0000913
.0009389
_cons |
13.6096
.2466242
55.18
0.000
13.12513
14.09407
---------------------------------------------------------------------------sigma_u | .15486353
sigma_e | .10003476
rho | .70558835
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

c. The LAD estimates below give a point estimate that indicates a tradeoff but it is smaller
in magnitude than in part a or part b. The standard error, which is not robust to cluster
correlation, implies t  −1. 22. Therefore, using LAD, there is little evidence of a tradeoff.
. qreg lavgsal bs lstaff lenroll lunch

bsbar lstaffbar lenrollbar lunchbar

Median regression
Raw sum of deviations 334.3106 (about 10.482654)
Min sum of deviations 234.7491

Number of obs 

1848



0.2978

Pseudo R2

----------------------------------------------------------------------------lavgsal |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------bs | -.3066784
.2511169
-1.22
0.222
-.7991826
.1858258
lstaff | -.6555687
.0313058
-20.94
0.000
-.7169673
-.59417
lenroll | -.0635032
.017727
-3.58
0.000
-.0982703
-.028736

474

lunch |
.0005538
.0003954
1.40
0.162
-.0002217
.0013293
bsbar |
.3679283
.3003398
1.23
0.221
-.2211145
.9569712
lstaffbar | -.1374073
.0421226
-3.26
0.001
-.2200204
-.0547941
lenrollbar |
.0075581
.0210143
0.36
0.719
-.0336564
.0487726
lunchbar | -.0014894
.0004477
-3.33
0.001
-.0023675
-.0006113
_cons |
14.23874
.1612496
88.30
0.000
13.92249
14.55499
-----------------------------------------------------------------------------

20.7. a. Out of 1,683 schools, 922 have all five years of data. The fewest number of years is
three. Note that the tab command gives includes many more observations than schools
because there are multiple years per school.
. xtsum math4
Variable
|
Mean
Std. Dev.
Min
Max |
Observations
--------------------------------------------------------------------------math4
overall | 63.57726
20.19047
2.9
100 |
N 
7150
between |
16.08074
11.75
98.94 |
n 
1683
within |
12.37335
13.71059
122.3439 | T-bar  4.24837
. egen tobs  sum(1), by(schid)
. count if tobs  5 & y98
922
. tab tobs
tobs |
Freq.
Percent
Cum.
----------------------------------------------3 |
1,512
21.15
21.15
4 |
1,028
14.38
35.52
5 |
4,610
64.48
100.00
----------------------------------------------Total |
7,150
100.00

b. The pooled OLS estimates, with all time averages included, and the fixed effects
estimates – with so-called “school fixed effects” – are given below. Variables with a “b” at the
end are the within-school time averages. As expected, they are identical, including the
coefficients on the year dummies.
The coefficient on lunchb is −. 426, and its fully robust t statistic is −11. 76. Therefore, the
average poverty level over the available years has a very large effect on the math pass rate: a
ten percentage point increase in the average poverty rate predicts a pass rate that is about 4.3
percentage points lower.

475

. reg math4 lavgrexp lunch lenrol y95 y96 y97 y98 lavgrexpb lunchb lenrolb
y95b y96b y97b y98b, cluster(distid)
Linear regression

Number of obs
F( 14,
466)
Prob  F
R-squared
Root MSE







7150
182.55
0.0000
0.4147
15.462

(Std. Err. adjusted for 467 clusters in distid
----------------------------------------------------------------------------|
Robust
math4 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lavgrexp |
6.288376
3.13387
2.01
0.045
.1301085
12.44664
lunch | -.0215072
.0399402
-0.54
0.590
-.0999924
.056978
lenrol | -2.038461
2.099636
-0.97
0.332
-6.164387
2.087466
y95 |
11.6192
.7213934
16.11
0.000
10.20162
13.03679
y96 |
13.05561
.9331425
13.99
0.000
11.22192
14.8893
y97 |
10.14771
.9581113
10.59
0.000
8.264957
12.03046
y98 |
23.41404
1.027817
22.78
0.000
21.39431
25.43377
lavgrexpb |
2.7178
4.04162
0.67
0.502
-5.224258
10.65986
lunchb | -.4256461
.0361912
-11.76
0.000
-.4967642
-.3545279
lenrolb |
.2880016
2.17219
0.13
0.895
-3.9805
4.556503
y95b |
21.26329
15.95857
1.33
0.183
-10.09639
52.62297
y96b |
15.69885
6.523566
2.41
0.016
2.879602
28.5181
y97b |
20.66597
15.71006
1.32
0.189
-10.20536
51.5373
y98b | -8.501184
18.89568
-0.45
0.653
-45.63248
28.63011
_cons | -6.616139
25.07553
-0.26
0.792
-55.89125
42.65897
----------------------------------------------------------------------------. xtreg math4 lavgrexp lunch lenrol y95 y96 y97 y98, fe cluster(distid)
Fixed-effects (within) regression
Group variable: schid
R-sq:

Number of obs
Number of groups

within  0.3602
between  0.0292
overall  0.1514

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(7,466)
Prob  F

 0.0073




7150
1683
4.
259.90
0.0000

(Std. Err. adjusted for 467 clusters in distid
----------------------------------------------------------------------------|
Robust
math4 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lavgrexp |
6.288376
3.132334
2.01
0.045
.1331271
12.44363
lunch | -.0215072
.0399206
-0.54
0.590
-.0999539
.0569395
lenrol | -2.038461
2.098607
-0.97
0.332
-6.162365
2.085443
y95 |
11.6192
.7210398
16.11
0.000
10.20231
13.0361
y96 |
13.05561
.9326851
14.00
0.000
11.22282
14.8884
y97 |
10.14771
.9576417
10.60
0.000
8.26588
12.02954
y98 |
23.41404
1.027313
22.79
0.000
21.3953
25.43278
_cons |
11.84422
32.68429
0.36
0.717
-52.38262
76.07107
---------------------------------------------------------------------------sigma_u |
15.84958
sigma_e | 11.325028

476

rho | .66200804
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

c. The RE estimates are given below, and they are identical to the FE estimates. The RE
coefficients on the time averaegs are not identical to those for POLS. In particular, on lunchb,
the RE coefficient is −. 415, just slightly smaller in magnitude than the POLS estimate. It has a
slightly smaller fully robust t statistic (in absolute value).
. xtreg math4 lavgrexp lunch lenrol y95 y96 y97 y98 lavgrexpb lunchb lenrolb
y95b y96b y97b y98b, re cluster(distid)
Random-effects GLS regression
Group variable: schid
R-sq:

Number of obs
Number of groups

within  0.3602
between  0.4366
overall  0.4146




7150
1683

Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(14)
Prob  chi2




4.
2532.10
0.0000

(Std. Err. adjusted for 467 clusters in distid
----------------------------------------------------------------------------|
Robust
math4 |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lavgrexp |
6.288376
3.13387
2.01
0.045
.1461029
12.43065
lunch | -.0215072
.0399402
-0.54
0.590
-.0997886
.0567741
lenrol | -2.038461
2.099636
-0.97
0.332
-6.153671
2.07675
y95 |
11.6192
.7213934
16.11
0.000
10.2053
13.03311
y96 |
13.05561
.9331425
13.99
0.000
11.22668
14.88453
y97 |
10.14771
.9581113
10.59
0.000
8.269847
12.02557
y98 |
23.41404
1.027817
22.78
0.000
21.39956
25.42852
lavgrexpb |
2.569862
3.99586
0.64
0.520
-5.261881
10.4016
lunchb | -.4153413
.0363218
-11.44
0.000
-.4865308
-.3441518
lenrolb |
.3829623
2.157847
0.18
0.859
-3.84634
4.612264
y95b |
18.96418
15.24131
1.24
0.213
-10.90824
48.83659
y96b |
16.16473
6.628049
2.44
0.015
3.173993
29.15547
y97b |
17.50964
15.42539
1.14
0.256
-12.72357
47.74285
y98b | -9.420143
18.25294
-0.52
0.606
-45.19524
26.35495
_cons | -5.159784
24.08649
-0.21
0.830
-52.36844
42.04887
---------------------------------------------------------------------------sigma_u | 10.702446
sigma_e | 11.325028
rho | .47175866
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

d. When we drop the time averages of the year dummies the RE estimates are slightly
different from the FE estimates. That is because we must now recognize that, with an
unbalanced panel, the time averages of the year dummies are no longer constant. With a
477

balanced panel, the time average are 1/T in each case. Now, the average is either zero – if the
unit does not appear in the appropriate year – or 1/T i where T i is the total number of years for
unit (school) i. For example, the list command below shows that the school with identifier
number 557 has data for the years 1994, 1997, and 1998. Therefore, y95b and y96b are both
zero, while y97b and y98b are both 1/3. With an unbalanced panel, we should include the time
averages of the year dummies. In effect, this is allowing certain forms of sample selection to be
correlated with the unobserved school heterogeneity.
. xtreg math4 lavgrexp lunch lenrol y95 y96 y97 y98 lavgrexpb lunchb lenrolb,
re cluster(distid)
Random-effects GLS regression
Group variable: schid
R-sq:

Number of obs
Number of groups

within  0.3602
between  0.4291
overall  0.4105




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(10)
Prob  chi2




7150
1683
4.
2073.48
0.0000

(Std. Err. adjusted for 467 clusters in distid
----------------------------------------------------------------------------|
Robust
math4 |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lavgrexp |
6.222429
3.121881
1.99
0.046
.1036546
12.3412
lunch | -.0209812
.0402425
-0.52
0.602
-.099855
.0578926
lenrol |
-2.06064
2.070938
-1.00
0.320
-6.119604
1.998325
y95 |
11.78595
.7084874
16.64
0.000
10.39734
13.17456
y96 |
13.16626
.91466
14.39
0.000
11.37356
14.95896
y97 |
10.21612
.9441691
10.82
0.000
8.365579
12.06665
y98 |
23.46409
1.055457
22.23
0.000
21.39544
25.53275
lavgrexpb |
2.417603
3.887099
0.62
0.534
-5.20097
10.03618
lunchb | -.4088571
.0365525
-11.19
0.000
-.4804986
-.3372155
lenrolb |
.7979708
2.109349
0.38
0.705
-3.336278
4.93222
_cons |
2.619295
24.78096
0.11
0.916
-45.95049
51.18908
---------------------------------------------------------------------------sigma_u | 10.702446
sigma_e | 11.325028
rho | .47175866
(fraction of variance due to u_i)
----------------------------------------------------------------------------. list schid year

y95b y96b y97b y98b if schid  557

----------------------------------------------------
| schid
year
y95b
y96b
y97b
y98b |
|----------------------------------------------------|

478

740. |
557
1994
0
0
.33333333
.33333333 |
741. |
557
1997
0
0
.33333333
.33333333 |
742. |
557
1998
0
0
.33333333
.33333333 |
----------------------------------------------------

e. The FE estimates without the year dummies are given below. The coefficient on the
spending variable is more than seven times larger than when the year dummies are included.
The estimate without the year dummies is very misleading. During this period in Michigan,
spending was increasing and, at the same time, the definition of a passing score was changed
so that more students passed the exam. Thus, without controlling for time dummies, most of
the relationship between pass rates and spending is spurious.
. xtreg math4 lavgrexp lunch lenrol, fe cluster(distid)
Fixed-effects (within) regression
Group variable: schid
R-sq:

Number of obs
Number of groups

within  0.1632
between  0.0001
overall  0.0233

corr(u_i, Xb)




7150
1683

Obs per group: min 
avg 
max 
F(3,466)
Prob  F

 -0.3272




4.
136.54
0.0000

(Std. Err. adjusted for 467 clusters in distid
----------------------------------------------------------------------------|
Robust
math4 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lavgrexp |
45.00103
2.452645
18.35
0.000
40.18141
49.82064
lunch |
.0179948
.0377204
0.48
0.634
-.0561284
.092118
lenrol | -2.372125
3.403866
-0.70
0.486
-9.060952
4.316701
_cons | -294.8467
32.11083
-9.18
0.000
-357.9467
-231.7468
---------------------------------------------------------------------------sigma_u | 17.573721
sigma_e |
12.9465
rho | .64820501
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

f. The POLS and RE estimates, without the time averages, are given below. The spending
effects are larger than FE and the effect of the lunch variable are much larger. If we do not
remove the school effect – of which a large component is demographics that do not change
over time – then the poverty measure lunch becomes very important. From the POLS/RE

479

estimates with the time averages included, it is really the average poverty level over several
years that has the most predictive power. Of course, the lunch variable does not vary across
time nearly as much as it does across school. Therefore, using FE, it is difficult to separate the
effect of lunch it from c i .
. reg math4 lavgrexp lunch lenrol y95 y96 y97 y98, cluster(distid)
Linear regression

Number of obs
F( 7,
466)
Prob  F
R-squared
Root MSE







7150
256.84
0.0000
0.4029
15.609

(Std. Err. adjusted for 467 clusters in distid
----------------------------------------------------------------------------|
Robust
math4 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lavgrexp |
8.628338
2.488897
3.47
0.001
3.737487
13.51919
lunch | -.4255479
.0391249
-10.88
0.000
-.5024309
-.3486648
lenrol | -1.294046
1.149539
-1.13
0.261
-3.552969
.9648762
y95 |
12.09916
.8909378
13.58
0.000
10.34841
13.84992
y96 |
13.06982
1.128072
11.59
0.000
10.85308
15.28655
y97 |
10.29535
1.114853
9.23
0.000
8.104584
12.48611
y98 |
23.57121
1.29055
18.26
0.000
21.03519
26.10723
_cons |
2.758117
23.09242
0.12
0.905
-42.62005
48.13628
----------------------------------------------------------------------------. xtreg math4 lavgrexp lunch lenrol y95 y96 y97 y98, re cluster(distid)
Random-effects GLS regression
Group variable: schid
R-sq:

Number of obs
Number of groups

within  0.3455
between  0.4288
overall  0.4016




Obs per group: min 
avg 
max 

Random effects u_i ~Gaussian
corr(u_i, X)
 0 (assumed)

Wald chi2(7)
Prob  chi2




7150
1683
4.
1886.18
0.0000

(Std. Err. adjusted for 467 clusters in distid
----------------------------------------------------------------------------|
Robust
math4 |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------lavgrexp |
7.838068
2.157833
3.63
0.000
3.608793
12.06734
lunch | -.3785643
.0400361
-9.46
0.000
-.4570336
-.3000949
lenrol | -1.391074
.9449022
-1.47
0.141
-3.243048
.4609008
y95 |
11.66598
.7704663
15.14
0.000
10.1559
13.17607
y96 |
12.88762
.9420724
13.68
0.000
11.04119
14.73404
y97 |
10.18776
.896855
11.36
0.000
8.429958
11.94557
y98 |
23.53236
1.029968
22.85
0.000
21.51366
25.55106

480

_cons |
8.166742
20.06401
0.41
0.684
-31.158
47.49148
---------------------------------------------------------------------------sigma_u | 10.702446
sigma_e | 11.325028
rho | .47175866
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

g. It seems pretty clear we need to go with the FE estimate and its standard error robust to
serial correlation within school and cluster correlation within district. Removing a school
effect most likely gives us the least biased estimator of school spending. Clustering at the
district level, rather than just at the school level, increases the standard error to 3. 13 from
about 2. 43, and so it seems prudent to use the standard error clustered at the district level.
. xtreg math4 lavgrexp lunch lenrol y95 y96 y97 y98, fe cluster(schid)
Fixed-effects (within) regression
Group variable: schid
R-sq:

Number of obs
Number of groups

within  0.3602
between  0.0292
overall  0.1514

corr(u_i, Xb)




Obs per group: min 
avg 
max 
F(7,1682)
Prob  F

 0.0073




7150
1683
4.
431.08
0.0000

(Std. Err. adjusted for 1683 clusters in schid
----------------------------------------------------------------------------|
Robust
math4 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------lavgrexp |
6.288376
2.431317
2.59
0.010
1.519651
11.0571
lunch | -.0215072
.0390732
-0.55
0.582
-.0981445
.05513
lenrol | -2.038461
1.789094
-1.14
0.255
-5.547545
1.470623
y95 |
11.6192
.5358469
21.68
0.000
10.56821
12.6702
y96 |
13.05561
.6910815
18.89
0.000
11.70014
14.41108
y97 |
10.14771
.7326314
13.85
0.000
8.710745
11.58468
y98 |
23.41404
.7669553
30.53
0.000
21.90975
24.91833
_cons |
11.84422
25.16643
0.47
0.638
-37.51659
61.20503
---------------------------------------------------------------------------sigma_u |
15.84958
sigma_e | 11.325028
rho | .66200804
(fraction of variance due to u_i)
-----------------------------------------------------------------------------

20.8. a. The information contained in x g , Z g , c g  and x g , Z g , a g  is the same, and so if we
substitute for c g we have

481

Ey gm |x g , Z g , c g     x g   z gm   c g 
   x g   z gm    g  z̄ g  g  a g 
 Ey gm |x g , Z g , a g .
b. Mechanically, we can get Ey gm |x g , Z g , a g   Ey gm |x g , Z g , a g  from


 − 1  x g   z gm    g  z̄ g  g  a g  u  0udu
where  is the standard normal distribution. If we want Ey gm |x g , Z g  then we integrate out
a g with respect to the Normal0,  2g  distribution. Just as in the probit case this the same as
computing
E1  x g   z gm    g  z̄ g  g  a g  u gm  0|x g , Z g 
where a g  u gm  is Normal0, 1   2g  and independent of x g , Z g . Therefore,
Ey gm |x g , Z g   

  x g   z gm    g  z̄ g  g 
1   2g  1/2

.

Notice that  can just be absorbed into  g .
c. Under the asymptotic scheme where G →  and the M g are fixed, there is an upper
bound, say M, with M g ≤ M for all g. If we see relatively few group sizes – and lots of data per
group size – we can allow the parameters to be different for each M g , with an appropriate
normalization. For example, we can have
Ey gm |x g , Z g   

 M g  x g   z gm   z̄ g  M g 
1   2M g  1/2

.

where  2M g is set to zero for one value, such as  2M  0. We can easily estimate all of the
parameters using the quasi-log likelihood associated with a “heteroskedastic probit,” where we
include in the heteroskedasticity function dummy variables for all but one outcome on M g .

482

And, of course, we include an intercept and dummy variables in the index as well as z̄ g and
interactions with the group-size dummies.
d. If we use the Bernoulli QMLE with the mean function discussed in part c, we need to be
sure that the inference is robust both to the true distribution not being Bernoulli and the
within-cluster correlation.

483

Solutions to Chapter 21 Problems
21.1. a. We use equation (21.5). First, because we have a random sample from the
treatment and control groups, Eȳ 1   Ey|w  1 and Eȳ 0   Ey|w  0. Therefore, by
equation (21.5),
Eȳ 1 − ȳ 0   Ey 0 |w  1 − Ey 0 |w  0   att .
It follows that the bias term for estimating  att is given by the first term.
b. If Ey 0 |w  1  Ey 0 |w  0, those who participate in the program would have had
lower average earnings without training than those who chose not to participate. This is a form
of self-selection, and, on average, leads to an underestimate of the impact of the program.
21.2. Let k ≡ w − pxy/px1 − px. Then we know from equation (21.21) that
Ek|x   1 x −  0 x   ate x.
Define a dummy variable as d ≡ 1x ∈ R. Then, by iterated expectations and the fact that d is
a function of x,
Ey 1 − y 0 |d  EEy 1 − y 0 |x, d|d  EEy 1 − y 0 |x|d
 E ate x|d  EEk|x|d  Ek|d
It follows that  ate,R ≡  ate,R  Ey 1 − y 0 |x ∈ R  Ey 1 − y 0 |d  1  Ek|d  1. Now use
the simple relationship
Ed  k  Pd  1Ek|d  1
and so
Ek|d  1 

Ed  k
Ed  k

.
Pd  1
Px ∈ R

If we know the propensity score, a consistent estimator of Ed  k would be

484

N

N −1 ∑ 1x i ∈ Rk i ,
i1

and a consistent estimator of Px ∈ R is just the fraction of observations with x i ∈ R, call
this N R /N. Combining these two estimators and using the expression for k i gives
N

̃ ate,R 

N −1
R

∑ 1x i ∈ Rk i ,
i1

which is simply the average of k i over the subset of observations with x i ∈ R.
21.3. a. The simple regression estimate is ̂ ate . 128, which means that those participating
in the job training program are about . 128 more likely of being unemployed after completing
the progam. Further, its heteroskedasticity-robust t statistic is about four. This appears to be a
case of self-selection into training: those who would have a higher chance of being
unemployed are also more likely to participate in job training.
. reg unem78 train, robust
Linear regression

Number of obs
F( 1, 2673)
Prob  F
R-squared
Root MSE







2675
15.90
0.0001
0.0098
.32779

----------------------------------------------------------------------------|
Robust
unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------train |
.1283838
.0321964
3.99
0.000
.0652514
.1915162
_cons |
.1148594
.0063922
17.97
0.000
.1023252
.1273936
-----------------------------------------------------------------------------

b. Adding the controls listed in the problem changes the picture considerable. The estimate
of  ate is now −. 199, so participating in the job training program is estimated to reduce the
unemployment probability by about . 20. The 95% confidence interval for  ate is
−. 288, −. 111, which clearly excludes zero.

485

. reg unem78 train age educ black hisp married re74 re75 unem75 unem74, robust
Linear regression

Number of obs
F( 10, 2664)
Prob  F
R-squared
Root MSE







2675
64.36
0.0000
0.3141
.27327

----------------------------------------------------------------------------|
Robust
unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------train | -.1993525
.045185
-4.41
0.000
-.2879538
-.1107512
age |
.0028579
.0006397
4.47
0.000
.0016036
.0041123
educ |
.0002969
.0020983
0.14
0.887
-.0038176
.0044114
black | -.0179975
.0122695
-1.47
0.143
-.0420563
.0060613
hisp | -.0625543
.0250947
-2.49
0.013
-.1117613
-.0133474
married | -.0136721
.0173229
-0.79
0.430
-.0476399
.0202957
re74 |
.0008451
.001004
0.84
0.400
-.0011236
.0028138
re75 | -.0042097
.0010084
-4.17
0.000
-.006187
-.0022325
unem75 |
.2994134
.0395227
7.58
0.000
.2219151
.3769118
unem74 |
.2385391
.0419072
5.69
0.000
.1563652
.3207131
_cons |
.0433446
.0358278
1.21
0.226
-.0269085
.1135978
-----------------------------------------------------------------------------

c. After running the regressions for the untrained and trained groups separately, we obtain a
fitted value (fitted probability) in each state for all 2,675 men in the sample. For each i we
estimate the treatment effect conditional on x as
̂ x i   ̂ 1  x i ̂ 1  − ̂ 0  x i ̂ 0 .
Then
N

̂ ate  N

−1

∑ ̂ x i 
i1
N

̂ x i 
̂ att  N −1
1 ∑ train i  
i1

We get ̂ ate  −. 203, which is very close to the estimate when we assume  1   0 . The
estimate of  att is somewhat larger in magnitude: ̂ att  −. 270.
. reg unem78 age educ black hisp married re74 re75 unem75 unem74 if ~train
Source |
SS
df
MS
------------------------------------------Model | 100.075847
9 11.1195386

486

Number of obs 
F( 9, 2480) 
Prob  F


2490
180.15
0.0000

Residual | 153.074354 2480
.06172353
------------------------------------------Total | 253.150201 2489 .101707594

R-squared

Adj R-squared 
Root MSE


0.3953
0.3931
.24844

----------------------------------------------------------------------------unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------age |
.0021732
.0005579
3.90
0.000
.0010791
.0032673
educ | -.0014064
.0019407
-0.72
0.469
-.005212
.0023992
black | -.0173876
.0125968
-1.38
0.168
-.0420889
.0073136
hisp | -.0517355
.0285084
-1.81
0.070
-.1076382
.0041672
married | -.0149914
.0151672
-0.99
0.323
-.0447332
.0147503
re74 |
.0014736
.0007966
1.85
0.064
-.0000884
.0030356
re75 | -.0035097
.0007814
-4.49
0.000
-.0050419
-.0019774
unem75 |
.3435381
.0257242
13.35
0.000
.293095
.3939813
unem74 |
.3363692
.0275345
12.22
0.000
.2823763
.3903622
_cons |
.0500675
.0349642
1.43
0.152
-.0184946
.1186296
----------------------------------------------------------------------------. predict unem78_0
(option xb assumed; fitted values)
. reg unem78 age educ black hisp married re74 re75 unem75 unem74 if train
Source |
SS
df
MS
------------------------------------------Model | 2.71236085
9 .301373428
Residual | 31.3416932
175
.17909539
------------------------------------------Total | 34.0540541
184 .185076381

Number of obs
F( 9,
175)
Prob  F
R-squared
Adj R-squared
Root MSE








185
1.68
0.0962
0.0796
0.0323
.4232

----------------------------------------------------------------------------unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------age | -.0022981
.0046702
-0.49
0.623
-.0115153
.0069192
educ |
-.008484
.0158595
-0.53
0.593
-.0397845
.0228166
black |
.1374346
.1067107
1.29
0.199
-.073171
.3480401
hisp | -.1412636
.1655747
-0.85
0.395
-.468044
.1855168
married | -.0761776
.0855254
-0.89
0.374
-.2449717
.0926165
re74 | -.0019756
.0098056
-0.20
0.841
-.0213281
.017377
re75 |
-.010362
.014196
-0.73
0.466
-.0383794
.0176553
unem75 |
.1822138
.1020566
1.79
0.076
-.0192063
.3836338
unem74 |
-.233911
.1194775
-1.96
0.052
-.4697132
.0018912
_cons |
.3735869
.2407415
1.55
0.123
-.1015435
.8487174
----------------------------------------------------------------------------. predict unem78_1
(option xb assumed; fitted values)
. gen te  unem78_1 - unem78_0
. sum te
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------te |
2675
-.2031515
.2448774
-1.5703
.3241221
. sum te if train

487

Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------te |
185
-.2698234
.309953 -.7017545
.3241221

d. Using the subsample of men who were unemployed in 1974, 1975, or both gives
̂ ate  −. 625 and ̂ att  −. 194. The estimate of  ate is much larger in magnitude than on the
full sample and ̂ att is somewhat smaller.
. keep if unem74 | unem75
(2240 observations deleted)
. reg unem78 age educ black hisp married re74 re75 if ~train
Source |
SS
df
MS
------------------------------------------Model | 17.2414134
7 2.46305906
Residual |
56.020176
294 .190544816
------------------------------------------Total | 73.2615894
301 .243393985

Number of obs
F( 7,
294)
Prob  F
R-squared
Adj R-squared
Root MSE








302
12.93
0.0000
0.2353
0.2171
.43651

----------------------------------------------------------------------------unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------age |
.0121428
.0025998
4.67
0.000
.0070262
.0172594
educ | -.0000954
.0090746
-0.01
0.992
-.0179548
.017764
black | -.0713435
.0713164
-1.00
0.318
-.2116989
.0690119
hisp | -.1965901
.1220144
-1.61
0.108
-.4367224
.0435422
married |
.0610631
.075997
0.80
0.422
-.088504
.2106302
re74 | -.0094196
.0029031
-3.24
0.001
-.0151331
-.0037061
re75 | -.0190763
.0029208
-6.53
0.000
-.0248247
-.013328
_cons |
.1819096
.1810088
1.00
0.316
-.1743278
.5381469
----------------------------------------------------------------------------. predict unem78_0
(option xb assumed; fitted values)
. reg unem78 age educ black hisp married re74 re75 if train
Source |
SS
df
MS
------------------------------------------Model | 2.33329022
7 .333327175
Residual | 21.9674617
125 .175739693
------------------------------------------Total | 24.3007519
132 .184096605

Number of obs
F( 7,
125)
Prob  F
R-squared
Adj R-squared
Root MSE








133
1.90
0.0754
0.0960
0.0454
.41921

----------------------------------------------------------------------------unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------age | -.0058054
.0049952
-1.16
0.247
-.0156914
.0040807
educ | -.0267626
.0175847
-1.52
0.131
-.0615649
.0080397
black |
.1754782
.1201604
1.46
0.147
-.0623342
.4132906
hisp | -.1106474
.2078183
-0.53
0.595
-.5219455
.3006508
married | -.1606594
.1015391
-1.58
0.116
-.3616179
.040299
re74 | -.0150277
.066169
-0.23
0.821
-.1459844
.1159289
re75 | -.0269891
.0282243
-0.96
0.341
-.0828484
.0288702

488

_cons |
.5632464
.253897
2.22
0.028
.0607527
1.06574
----------------------------------------------------------------------------. predict unem78_1
(option xb assumed; fitted values)
. gen te  unem78_1 - unem78_0
. sum te
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------te |
435
-.625014
.3867973
-1.62662
.1450891
. sum te if train
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------te |
133
-.1935882
.2039181 -.7526801
.1450891

e. We use the entire set of data for this part. The logit model for train is estimated below.
Of the 2,675 observations, 78 failures are completely determined. This means that the overlap
assumption fails because for some values of x the probability of being in the training group is
zero. If we are interested in the ATE then our only recourse is to redefine the population so
that each unit has a nonzero chance of being in the treated group (and a nonzero chance of
being in the control group, which is not a problem in this example).
. logit train age educ black hisp married re74 re75 unem74 unem75
Logistic regression

Number of obs
LR chi2(9)
Prob  chi2
Pseudo R2

Log likelihood  -209.38931






2675
926.52
0.0000
0.6887

----------------------------------------------------------------------------train |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------age | -.1109206
.0177106
-6.26
0.000
-.1456327
-.0762084
educ | -.1008807
.0561133
-1.80
0.072
-.2108608
.0090994
black |
2.650097
.3605668
7.35
0.000
1.943399
3.356795
hisp |
2.247747
.5908963
3.80
0.000
1.089611
3.405882
married | -1.560628
.2817913
-5.54
0.000
-2.112928
-1.008327
re74 |
.0201797
.0313149
0.64
0.519
-.0411963
.0815557
re75 | -.2743162
.0477066
-5.75
0.000
-.3678194
-.1808129
unem74 |
3.272456
.4887585
6.70
0.000
2.314507
4.230405
unem75 | -1.371405
.4545789
-3.02
0.003
-2.262363
-.4804465
_cons |
1.794543
.979261
1.83
0.067
-.1247735
3.713859
----------------------------------------------------------------------------Note: 78 failures and 0 successes completely determined.

489

f. The State session is below. The IPW estimate is ̂ ate,psw  −. 132. The standard error that
adjusts for the first-step estimation is about . 0504. If we do not take advantage of the smaller
asymptotic variance due to estimating the propensity score, the standard error is . 0580, which
is about 15% larger. The estimate of  att is similar, about −. 124.
If we assume a constant treatment effect in using regression adjustment,
̂ ate,reg  ̂ att,reg  −. 235 and its standard error is . 0509. Interestingly, this is very close to the
standard error for ̂ ate,psw , but the estimate is much larger in magnitude, leading to a large t
statistic. Unfortunately, it appears separate regression are warranted, and this changes ̂ ate,reg to
−. 119 (although ̂ att,reg  −. 294). The standard error for ̂ ate,reg that does not even account for
the randomness in the sample averages is quite large, . 0911, and so ̂ ate,reg is barely statistically
different from zero at the 10% level if we use a one-sided alternative. The IPW estimator
appears to be more efficient for this application. (It could have something to do with using
linear regression adjustment rather than, say, probit or logit.) The joint test of the interaction
terms shows separate regressions are warranted.
. keep if avgre  15
(1513 observations deleted)
. logit train age educ black hisp married re74 re75 unem74 unem75
Logistic regression

Number of obs
LR chi2(9)
Prob  chi2
Pseudo R2

Log likelihood  -180.28028






1162
641.37
0.0000
0.6401

----------------------------------------------------------------------------train |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------age | -.1155512
.0187215
-6.17
0.000
-.1522447
-.0788577
educ | -.1049275
.0591078
-1.78
0.076
-.2207766
.0109217
black |
2.608068
.3772016
6.91
0.000
1.868767
3.34737
hisp |
2.395905
.6292337
3.81
0.000
1.162629
3.62918
married | -1.631159
.3038189
-5.37
0.000
-2.226633
-1.035685
re74 | -.0290672
.04281
-0.68
0.497
-.1129732
.0548387
re75 | -.3794923
.0682029
-5.56
0.000
-.5131676
-.245817
unem74 |
3.009282
.5221746
5.76
0.000
1.985839
4.032726

490

unem75 | -1.751808
.4995608
-3.51
0.000
-2.730929
-.7726867
_cons |
2.695208
1.053604
2.56
0.011
.6301819
4.760234
----------------------------------------------------------------------------. predict phat
(option pr assumed; Pr(train))
. tab train
1 if in |
job |
training |
Freq.
Percent
Cum.
----------------------------------------------0 |
982
84.51
84.51
1 |
180
15.49
100.00
----------------------------------------------Total |
1,162
100.00
. sum train
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------train |
1162
.1549053
.3619702
0
1
. gen rhohat  r(mean)
. gen kate  ((train - phat)*unem78)/(phat*(1 - phat))
. gen katt  ((train - phat)*unem78)/(rhohat*(1 - phat))
. sum kate katt
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------kate |
1162
-.1319506
1.977683 -16.62496
56.51032
katt |
1162
-.1243131
4.922131 -100.8678
6.455555
. * Get the correct standard error for the ATE estimate.
. gen uh  train - phat
. gen ageuh  age*uh
. gen

educuh  educ*uh

. gen

blackuh  black*uh

. gen

hispuh  hisp*uh

. gen
. gen

marrieduh  married*uh
re74uh  re74*uh

. gen re75uh  re75*uh
. gen unem74uh  unem74*uh
. gen unem75uh  unem75*uh

491

. reg kate uh ageuh educuh blackuh hispuh marrieduh re74uh re75uh unem74uh
unem75uh
Source |
SS
df
MS
------------------------------------------Model | 1138.33705
10 113.833705
Residual | 3402.59957 1151 2.95621161
------------------------------------------Total | 4540.93661 1161 3.91122878

Number of obs
F( 10, 1151)
Prob  F
R-squared
Adj R-squared
Root MSE








1162
38.51
0.0000
0.2507
0.2442
1.7194

----------------------------------------------------------------------------kate |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------uh |
3.525428
1.973887
1.79
0.074
-.3473914
7.398247
ageuh |
.0016821
.0350983
0.05
0.962
-.0671816
.0705458
educuh |
.2945194
.1191697
2.47
0.014
.0607052
.5283336
blackuh | -3.176048
.6611273
-4.80
0.000
-4.473198
-1.878898
hispuh | -5.475508
1.012662
-5.41
0.000
-7.462378
-3.488638
marrieduh |
4.005544
.5475872
7.31
0.000
2.931163
5.079926
re74uh |
.3468368
.075946
4.57
0.000
.1978287
.495845
re75uh | -.8364872
.1060216
-7.89
0.000
-1.044504
-.62847
unem74uh | -2.607257
.818097
-3.19
0.001
-4.212386
-1.002129
unem75uh |
.2278527
.796608
0.29
0.775
-1.335114
1.790819
_cons | -.1319506
.0504388
-2.62
0.009
-.2309129
-.0329883
----------------------------------------------------------------------------. di e(rmse)/sqrt(e(N))
.05043879
. di -.1320/.0504
-2.6190476
. reg kate
Source |
SS
df
MS
------------------------------------------Model |
0
0
.
Residual | 4540.93661 1161 3.91122878
------------------------------------------Total | 4540.93661 1161 3.91122878

Number of obs
F( 0, 1161)
Prob  F
R-squared
Adj R-squared
Root MSE








1162
0.00
0.0000
0.0000
1.9777

----------------------------------------------------------------------------kate |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------_cons | -.1319506
.0580168
-2.27
0.023
-.24578
-.0181211
----------------------------------------------------------------------------. reg unem78 train, robust
Linear regression

Number of obs
F( 1, 1160)
Prob  F
R-squared
Root MSE







1162
0.18
0.6734
0.0002
.4259

----------------------------------------------------------------------------|
Robust
[95% Conf. Interval
unem78 |
Coef.
Std. Err.
t
P|t|
----------------------------------------------------------------------------

492

train |
.0147658
.0350282
0.42
0.673
-.0539599
.0834915
_cons |
.2352342
.0135467
17.36
0.000
.2086555
.2618129
----------------------------------------------------------------------------. reg unem78 train age educ black hisp married re74 re75 unem74 unem75, robust
Linear regression

Number of obs
F( 10, 1151)
Prob  F
R-squared
Root MSE







1162
61.27
0.0000
0.3312
.34968

----------------------------------------------------------------------------|
Robust
unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------train | -.2349689
.0509218
-4.61
0.000
-.3348787
-.135059
age |
.0059358
.0012367
4.80
0.000
.0035094
.0083622
educ |
.0022623
.0042076
0.54
0.591
-.005993
.0105177
black | -.0202408
.022745
-0.89
0.374
-.0648671
.0243855
hisp |
-.100478
.0399462
-2.52
0.012
-.1788536
-.0221024
married | -.0352163
.0272463
-1.29
0.196
-.0886743
.0182417
re74 | -.0010355
.002876
-0.36
0.719
-.0066783
.0046073
re75 | -.0177354
.0024155
-7.34
0.000
-.0224746
-.0129961
unem74 |
.2220472
.051956
4.27
0.000
.1201081
.3239863
unem75 |
.1439644
.048573
2.96
0.003
.0486629
.2392658
_cons |
.1103197
.0759773
1.45
0.147
-.0387499
.2593893
----------------------------------------------------------------------------. reg unem78 age educ black hisp married re74 re75 unem74 unem75 if ~train
Source |
SS
df
MS
------------------------------------------Model |
78.510332
9 8.72337022
Residual | 98.1505642
972 .100977947
------------------------------------------Total | 176.660896
981 .180082463

Number of obs
F( 9,
972)
Prob  F
R-squared
Adj R-squared
Root MSE








982
86.39
0.0000
0.4444
0.4393
.31777

----------------------------------------------------------------------------unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------age |
.0050777
.0011449
4.43
0.000
.0028309
.0073245
educ | -.0002579
.0039421
-0.07
0.948
-.0079938
.0074781
black | -.0146538
.0238818
-0.61
0.540
-.0615196
.0322119
hisp | -.0862098
.0524883
-1.64
0.101
-.1892132
.0167936
married | -.0424904
.0262258
-1.62
0.106
-.093956
.0089752
re74 |
.0022784
.0024006
0.95
0.343
-.0024325
.0069892
re75 | -.0143134
.0025479
-5.62
0.000
-.0193134
-.0093133
unem74 |
.3521536
.0435278
8.09
0.000
.2667344
.4375729
unem75 |
.1965244
.0423339
4.64
0.000
.1134481
.2796007
_cons |
.0770668
.0757616
1.02
0.309
-.0716084
.2257419
----------------------------------------------------------------------------. predict unem78_0
(option xb assumed; fitted values)
. reg unem78 age educ black hisp married re74 re75 unem74 unem75 if train
Source |

SS

df

MS

493

Number of obs 

180

------------------------------------------Model | 2.58861704
9 .287624115
Residual |
31.161383
170 .183302253
------------------------------------------Total |
33.75
179 .188547486

F( 9,
170)
Prob  F
R-squared
Adj R-squared
Root MSE







1.57
0.1281
0.0767
0.0278
.42814

----------------------------------------------------------------------------unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------age |
-.002544
.0047571
-0.53
0.593
-.0119347
.0068466
educ | -.0086994
.0162153
-0.54
0.592
-.0407086
.0233098
black |
.1402344
.108018
1.30
0.196
-.072995
.3534638
hisp | -.1480334
.1683835
-0.88
0.381
-.4804252
.1843585
married | -.0713415
.0879005
-0.81
0.418
-.2448585
.1021756
re74 |
.0073134
.0145599
0.50
0.616
-.021428
.0360549
re75 | -.0064075
.0214837
-0.30
0.766
-.0488166
.0360016
unem74 | -.1885821
.1321929
-1.43
0.156
-.4495331
.0723688
unem75 |
.1935779
.1115475
1.74
0.084
-.0266186
.4137745
_cons |
.3229791
.2550769
1.27
0.207
-.1805469
.8265051
----------------------------------------------------------------------------. predict unem78_1
(option xb assumed; fitted values)
. gen te  unem78_1 - unem78_0
. sum te
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------te |
1162
-.1193285
.3326819 -.9173806
.3599507
. sum te if train
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------te |
180
-.2941826
.2835388
-.728443
.2494144
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

egen mage  mean(age)
gen trainage  train*(age - mage)
egen meduc  mean(educ)
gen traineduc  train*(educ - meduc)
egen mblack  mean(black)
gen trainblack  train*(black - mblack)
egen mhisp  mean(hisp)
gen trainhisp  train*(hisp - mhisp)
egen mmarried  mean(married)
gen trainmarried  train*(married - mmarried)
egen mre74  mean(re74)
gen trainre74  train*(re74 - mre74)
egen mre75  mean(re75)
gen trainre75  train*(re75 - mre75)
egen munem74  mean(unem74)
gen trainunem74  train*(unem74 - munem74)
egen munem75  mean(unem75)
gen trainunem75  train*(unem75 - munem75)

. reg unem78 train age educ black hisp married re74 re75 unem74 unem75
trainage traineduc trainblack trainhisp trainmarried trainre74

494

trainre75 trainunem74 trainunem75, robust
Linear regression

Number of obs
F( 19, 1142)
Prob  F
R-squared
Root MSE







1162
42.62
0.0000
0.3855
.3365

----------------------------------------------------------------------------|
Robust
unem78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------train | -.1193284
.0910893
-1.31
0.190
-.2980497
.0593928
age |
.0050777
.0012214
4.16
0.000
.0026812
.0074742
educ | -.0002579
.0041835
-0.06
0.951
-.008466
.0079503
black | -.0146538
.0231254
-0.63
0.526
-.0600269
.0307192
hisp | -.0862098
.0424936
-2.03
0.043
-.169584
-.0028356
married | -.0424904
.0277233
-1.53
0.126
-.0968847
.0119039
re74 |
.0022784
.0028885
0.79
0.430
-.003389
.0079457
re75 | -.0143134
.0025811
-5.55
0.000
-.0193776
-.0092491
unem74 |
.3521536
.0566374
6.22
0.000
.2410286
.4632787
unem75 |
.1965244
.0570442
3.45
0.001
.0846013
.3084475
trainage | -.0076217
.0050195
-1.52
0.129
-.0174702
.0022267
traineduc | -.0084415
.0152788
-0.55
0.581
-.0384192
.0215361
trainblack |
.1548883
.0871611
1.78
0.076
-.0161256
.3259022
trainhisp | -.0618236
.0975772
-0.63
0.526
-.2532742
.1296271
trainmarried | -.0288511
.0822415
-0.35
0.726
-.1902126
.1325104
trainre74 |
.0050351
.0166047
0.30
0.762
-.027544
.0376142
trainre75 |
.0079059
.0185161
0.43
0.669
-.0284236
.0442353
trainunem74 | -.5407358
.1357835
-3.98
0.000
-.807149
-.2743226
trainunem75 | -.0029465
.0975097
-0.03
0.976
-.1942647
.1883717
_cons |
.0770668
.0760186
1.01
0.311
-.0720851
.2262186
----------------------------------------------------------------------------. test
(
(
(
(
(
(
(
(
(

1)
2)
3)
4)
5)
6)
7)
8)
9)

trainage traineduc trainblack trainhisp trainmarried trainre74
trainre75 trainunem74 trainunem75
trainage  0
traineduc  0
trainblack  0
trainhisp  0
trainmarried  0
trainre74  0
trainre75  0
trainunem74  0
trainunem75  0
F(

9, 1142) 
Prob  F 

8.61
0.0000

21.4. The integral is equivalent to


 − x z  aada.
0

1

2

495

Because da/da  −aa, the antiderivative of aa is simply −a. Now
m

m
 − x z  aada  −a −
x z   −m  − 0  x 1  z 2 
0

1

0

2

1

2

 −m   0  x 1  z 2 
where we use the symmetry of . As m → , m → 0. Therefore,


 − x z  aada   0  x 1  z 2 .
0

1

2

21.5. The Stata output to answer all parts follows.
a. The first two Stata commands are used to obtain the probit fitted values, called PHIhat.
b. The IV estimate of  is −43. 27 and its standard error is huge, 585. 78. Clearly we can
learn nothing of value from this estimate.
̂ i on the x i : the
c. The collinearity suspected in part b is confirmed by regressing 
̂ i that cannot be
R-squared is .9989, which means there is almost no separate variation in 
explained by x i .
d. This example illustrates why trying to achieve identification off of a nonlinearity can be
fraught with problems. In cases with larger sample sizes the estimates may seem more
reasonable, but we are only able to compute estimates at all because of the presumed
functional form for Pw|x. A good general rule is that if a linear IV approach does not identify
 then we should not hope to learn anything useful by introducing nonlinearity in Pw|x.
. probit train age educ black hisp married re74 re75
Probit regression

Number of obs
LR chi2(7)
Prob  chi2
Pseudo R2

Log likelihood  -297.80166






445
8.60
0.2829
0.0142

----------------------------------------------------------------------------train |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------age |
.0066826
.0087391
0.76
0.444
-.0104458
.0238109
educ |
.0387341
.0341574
1.13
0.257
-.0282132
.1056815

496

black | -.2216642
.2242952
-0.99
0.323
-.6612747
.2179463
hisp | -.5753033
.3062908
-1.88
0.060
-1.175622
.0250157
married |
.0900855
.1703412
0.53
0.597
-.2437771
.4239482
re74 | -.0138226
.0155792
-0.89
0.375
-.0443572
.016712
re75 |
.028755
.0267469
1.08
0.282
-.0236679
.0811779
_cons | -.5715372
.475416
-1.20
0.229
-1.503335
.3602609
----------------------------------------------------------------------------. predict PHIhat
(option pr assumed; Pr(train))
. ivreg re78 age educ black hisp married re74 re75 (train  PHIhat)
Instrumental variables (2SLS) regression
Source |
SS
df
MS
------------------------------------------Model | -213187.422
8 -26648.4277
Residual | 232713.078
436 533.745593
------------------------------------------Total | 19525.6566
444 43.9767041

Number of obs
F( 8,
436)
Prob  F
R-squared
Adj R-squared
Root MSE








445
0.18
0.9934
23.103

----------------------------------------------------------------------------re78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------train | -43.26513
585.7793
-0.07
0.941
-1194.567
1108.037
age |
.1717735
1.520127
0.11
0.910
-2.815914
3.159461
educ |
1.067645
8.646843
0.12
0.902
-15.92703
18.06232
black | -6.114187
51.56931
-0.12
0.906
-107.4695
95.24116
hisp | -9.523185
126.287
-0.08
0.940
-257.7302
238.6838
married |
1.432202
20.72909
0.07
0.945
-39.30917
42.17357
re74 | -.1443703
2.973787
-0.05
0.961
-5.98911
5.70037
re75 |
.5327602
6.2896
0.08
0.933
-11.82894
12.89447
_cons |
13.30468
165.517
0.08
0.936
-312.0058
338.6151
----------------------------------------------------------------------------Instrumented: train
Instruments:
age educ black hisp married re74 re75 PHIhat
----------------------------------------------------------------------------. reg PHIhat age educ black hisp married re74 re75
Source |
SS
df
MS
------------------------------------------Model | 2.04859095
7 .292655851
Residual | .002314965
437 5.2974e-06
------------------------------------------Total | 2.05090592
444 .004619157

Number of obs
F( 7,
437)
Prob  F
R-squared
Adj R-squared
Root MSE


445
55245.15
 0.0000
 0.9989
 0.9989

.0023

----------------------------------------------------------------------------PHIhat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------age |
.0025883
.0000158
163.53
0.000
.0025572
.0026194
educ |
.0146708
.000062
236.45
0.000
.0145488
.0147927
black | -.0875955
.0004094 -213.96
0.000
-.0884002
-.0867909
hisp | -.2156441
.0005445 -396.02
0.000
-.2167143
-.2145739
married |
.0351309
.0003107
113.08
0.000
.0345203
.0357415
re74 | -.0051274
.0000271 -189.22
0.000
-.0051807
-.0050742
re75 |
.0108521
.0000474
228.89
0.000
.0107589
.0109453
_cons |
.2823687
.0008635
326.99
0.000
.2806715
.2840659

497

-----------------------------------------------------------------------------

21.6. As in Procedure 21.1, the IV estimator is consistent whether or not Gx, z;  is
correctly specified for Pw  1|x, z. The OLS estimator from y i on 1, Ĝ i , x i , Ĝ i  x i − x̄ ,
i  1, … , N generally requires the model for Pw  1|x, z to be correctly specified. This can
be seen by writing
Ey|x, z    Ew|x, z  x 0  Ew|x, z  x − ,
which is the estimating equation underlying the OLS regression on probit fitted values and the
interactions. If Ew|x, z  Pw  1|x, z ≠ Gx, z;  for all  then plugging in Ĝ i generally
produces inconsistent estimators.
Even if Gx, z;  is correctly specified, the standard errors for the two-step OLS estimator
are harder to obtain. One one must use the material on generated regressors in Chapter 6 or
apply the bootstrap.
21.7. a. There are several options. To estimate the mean parameters, we can use Poisson
regression (especially in the case where w is a count variable) or gamma regression (if w is
nonnegative and continuous). Of course, we can use NLS, too (which is also a QMLE in the
LEF).
As stated in the hint, if we define r  w − Ew|x then
Er 2 |x  Varw|x  exp 0  x 1 . Therefore, if we observed, r 2 , we could use it as the
dependent variable in, say, a gamma or negative binomial QMLE. In practice, we use
r̂ i  w i − exp̂ 0  x i ̂ 1 , the residuals from estimating the mean parameters.
b. By the law of large numbers,
N

N

−1

∑
i1

w i − x i y i
x i 

498

p

→ .

By the usual argument, we can replace x i  and x i  with consistent estimators; more
precisely, in a parametric context replace the unknown parameters with consistent estimators.
In the case of exponential mean and variance functions,
N

̂  N −1 ∑
i1

w i − exp̂ 0  x i ̂ 1 y i
exp̂ 0  x i ̂ 1 

N

≡N

−1

∑
i1

̂ x i y i
w i − 
̂ x i 

.

We can use Problem 12.17 to get a standard error for ̂ or use the bootstrap.
c. I use Poisson regression to estimate the mean parameters and then gamm regression to
estimate the variance parameters. The resulting estimate of  is about . 102; the standard error
is not reported. If we ignore estimation of the parameters in Ew|x and Varw|x then the
standard error is about .050.
There is not much reason to compute a standard error for ̂ because the standard regression
adjustment estimate, ̃ , is very close, and provides a valid standard error. Namely, running the
regression
re78 i on 1, mostrn i , age i , educ i , black i , hisp i , married i , re74 i , re75
gives ̃ . 103 (se . 038). With random assignment to the job training program it is perhaps
not too surprising to see the methods give similar estimates. In fact, the simple regression
estimate is . 112 (se . 038).
. glm mostrn age educ black hisp married re74 re75, fam(poiss) link(log)
robust
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

6136.777616
5296.290311

Variance function: V(u)  u
Link function
: g(u)  ln(u)

[Poisson]
[Log]

Log pseudolikelihood  -3504.968642

AIC
BIC

499







14.04297
12.11966




15.78862
3471.919

445
437

----------------------------------------------------------------------------|
Robust
mostrn |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------age |
.0037486
.0081575
0.46
0.646
-.0122398
.0197369
educ |
.0448349
.0344975
1.30
0.194
-.0227789
.1124487
black | -.1809126
.1906097
-0.95
0.343
-.5545006
.1926755
hisp | -.4907343
.3198765
-1.53
0.125
-1.117681
.1362121
married |
.0824876
.1620227
0.51
0.611
-.2350709
.4000462
re74 | -.0048997
.0140984
-0.35
0.728
-.0325322
.0227327
re75 |
.0388417
.0197161
1.97
0.049
.0001989
.0774846
_cons |
1.605453
.4551975
3.53
0.000
.7132821
2.497624
----------------------------------------------------------------------------. predict mostrnh
(option mu assumed; predicted mean mostrn)
. gen rh  mostrn - mostrnh
. gen rhsq  rh^2
. glm rhsq age educ black hisp married re74 re75, fam(gamma) link(log) robust
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

251.4433046
301.0457257

Variance function: V(u)  u^2
Link function
: g(u)  ln(u)

[Gamma]
[Log]

Log pseudolikelihood  -2442.743803

AIC
BIC







445
437
.6888918
.5753851
.6888918

 11.01458
 -2413.415

----------------------------------------------------------------------------|
Robust
rhsq |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------age | -.0020813
.0054739
-0.38
0.704
-.01281
.0086475
educ |
.0206816
.0254745
0.81
0.417
-.0292474
.0706107
black | -.0424931
.1083251
-0.39
0.695
-.2548063
.1698202
hisp | -.2107907
.2173269
-0.97
0.332
-.6367437
.2151623
married |
.0391702
.0937328
0.42
0.676
-.1445427
.2228831
re74 |
.0094508
.0107463
0.88
0.379
-.0116116
.0305132
re75 |
.051212
.0190328
2.69
0.007
.0139084
.0885156
_cons |
4.288161
.3201553
13.39
0.000
3.660668
4.915654
----------------------------------------------------------------------------. predict omegah
(option mu assumed; predicted mean rhsq)
. sum omegah
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------omegah |
445
91.62743
28.73968
60.9556
369.0591

500

. gen kh  ( mostrn -

mostrnh)*re78/omegah

. sum kh
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------kh |
445
.1024405
1.047671 -3.030556
10.28323
. reg kh
Source |
SS
df
MS
------------------------------------------Model |
0
0
.
Residual |
487.34086
444 1.09761455
------------------------------------------Total |
487.34086
444 1.09761455

Number of obs
F( 0,
444)
Prob  F
R-squared
Adj R-squared
Root MSE








445
0.00
0.0000
0.0000
1.0477

----------------------------------------------------------------------------kh |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------_cons |
.1024405
.0496644
2.06
0.040
.0048341
.200047
----------------------------------------------------------------------------. reg re78 mostrn age educ black hisp married re74 re75, robust
Linear regression

Number of obs
F( 8,
436)
Prob  F
R-squared
Root MSE







445
3.09
0.0021
0.0613
6.4838

----------------------------------------------------------------------------|
Robust
re78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------mostrn |
.102825
.0380686
2.70
0.007
.0280043
.1776458
age |
.0570883
.0399249
1.43
0.153
-.0213808
.1355575
educ |
.3980183
.1548109
2.57
0.010
.09375
.7022867
black | -2.150926
1.007271
-2.14
0.033
-4.130637
-.1712163
hisp |
.1712523
1.365153
0.13
0.900
-2.511846
2.85435
married |
-.154993
.8733899
-0.18
0.859
-1.871571
1.561585
re74 |
.0788359
.1071444
0.74
0.462
-.1317478
.2894197
re75 |
.0305561
.1266573
0.24
0.809
-.2183787
.2794909
_cons |
.6004532
2.366495
0.25
0.800
-4.050703
5.25161
----------------------------------------------------------------------------. reg re78 mostrn, robust
Linear regression

Number of obs
F( 1,
443)
Prob  F
R-squared
Root MSE







445
8.66
0.0034
0.0269
6.5491

----------------------------------------------------------------------------|
Robust
re78 |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------mostrn |
.1126397
.0382802
2.94
0.003
.0374063
.1878731

501

_cons |
4.434831
.3358041
13.21
0.000
3.774864
5.094798
-----------------------------------------------------------------------------

d. Because Ew|x follows a logistic regression model we can use fractional logit (that is,
maximize the Bernoulli QMLE). After we have estimated the mean parameters  0 and  1 , we
form the fitted values and residuals
ŵ i  ̂ 0  x i ̂ 1 
r̂ i  w i − ŵ i
and then estimate  0 ,  1 , and  2 from the OLS regression
r̂ 2i on 1, ŵ i , ŵ 2i
to get the variance estimates
̂ i  ̂ 0  ̂ 1 ŵ i  ̂ 2 ŵ 2i .
Because the ̂ i are fitted values from a linear regression, nothing guarantees ̂ i  0 for all i,
something we need for the method in part b to make sense. To avoid this problem, we might
use Varw|x  exp 0   1 Ew|x   2 Ew|x 2  instead, and use the gamma QMLE with
the squared residuals as the dependent variable.
e. The Stata code carries out the procedure from part d, except that, because 13 estimated
variances were not positive, the exponential variance function was used instead, with a gamma
QMLE. below produces the estimate ̂ . 689. The regression coefficient is not too different:
̃ . 644 (se . 235)
. use attend
. gen ACTsq  ACT^2
. gen ACTcu  ACT^3
. gen priGPAsq  priGPA^2
. gen priGPAcu  priGPA^3
. sum atndrte

502

Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------atndrte |
680
81.70956
17.04699
6.25
100
. replace atndrte  atndrte/100
(680 real changes made)
. glm atndrte priGPA priGPAsq priGPAcu ACT ACTsq ACTcu frosh soph, fam(bin)
link(logit) robust
note: atndrte has noninteger values
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

87.0709545
85.07495268

Variance function: V(u)  u*(1-u/1)
Link function
: g(u)  ln(u/(1-u))

[Binomial]
[Logit]

Log pseudolikelihood  -223.2763498

AIC
BIC







680
671
.129763
.1267883

 .6831657
 -4289.253

----------------------------------------------------------------------------|
Robust
atndrte |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------priGPA | -3.371154
2.195517
-1.54
0.125
-7.67429
.9319806
priGPAsq |
1.886443
.8972586
2.10
0.036
.1278489
3.645038
priGPAcu | -.2454989
.118004
-2.08
0.037
-.4767825
-.0142153
ACT |
.5538998
.6744028
0.82
0.411
-.7679054
1.875705
ACTsq | -.0280986
.0304868
-0.92
0.357
-.0878516
.0316544
ACTcu |
.0003858
.0004505
0.86
0.392
-.000497
.0012687
frosh |
.3939498
.1155299
3.41
0.001
.1675154
.6203841
soph |
.0941678
.1006569
0.94
0.350
-.1031161
.2914517
_cons | -.7731446
5.13392
-0.15
0.880
-10.83544
9.289154
----------------------------------------------------------------------------. predict atndrteh
(option mu assumed; predicted mean atndrte)
. gen rh  atndrte - atndrteh
. gen rhsq  rh^2
. gen atndrtehsq  atndrteh^2
. reg rhsq atndrteh atndrtehsq
Source |
SS
df
MS
------------------------------------------Model | .098172929
2 .049086465
Residual | .894850267
677 .001321788
------------------------------------------Total | .993023196
679 .001462479

Number of obs
F( 2,
677)
Prob  F
R-squared
Adj R-squared
Root MSE








680
37.14
0.0000
0.0989
0.0962
.03636

----------------------------------------------------------------------------rhsq |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval

503

---------------------------------------------------------------------------atndrteh |
.1604177
.1514883
1.06
0.290
-.1370257
.457861
atndrtehsq | -.1854786
.0994129
-1.87
0.063
-.3806733
.0097161
_cons |
.0137235
.0571515
0.24
0.810
-.0984919
.1259389
----------------------------------------------------------------------------. predict omegah
(option xb assumed; fitted values)
. sum omegah
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------omegah |
680
.0192213
.0120243 -.0049489
.0483992
. count if omegah  0
13
. drop omegah
. glm rhsq atndrteh atndrtehsq, fam(gamma) link(log)
Generalized linear models
Optimization
: ML
Deviance
Pearson




No. of obs
Residual df
Scale parameter
(1/df) Deviance
(1/df) Pearson

1657.02942
2560.125871

Variance function: V(u)  u^2
Link function
: g(u)  ln(u)
Log likelihood









680
677
3.781574
2.447606
3.781574

[Gamma]
[Log]
AIC
BIC

2144.628097

 -6.298906
 -2758.427

----------------------------------------------------------------------------|
OIM
rhsq |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------atndrteh |
21.19375
8.816922
2.40
0.016
3.912901
38.4746
atndrtehsq | -17.96346
5.764534
-3.12
0.002
-29.26174
-6.665185
_cons | -9.308977
3.33455
-2.79
0.005
-15.84458
-2.77338
----------------------------------------------------------------------------. predict omegah
(option mu assumed; predicted mean rhsq)
. gen kh  rh*stndfnl/omegah
. sum kh
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------kh |
680
.6890428
8.318917 -47.34537
41.90279
. reg stndfnl atndrte
robust

priGPA priGPAsq priGPAcu ACT ACTsq ACTcu frosh soph,
Number of obs 
F( 9,
670) 

Linear regression

504

680
31.01

Prob  F
R-squared
Root MSE





0.0000
0.2356
.8709

----------------------------------------------------------------------------|
Robust
stndfnl |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------atndrte |
.6444118
.2345274
2.75
0.006
.1839147
1.104909
priGPA |
1.987666
2.651676
0.75
0.454
-3.21893
7.194262
priGPAsq | -1.055604
1.01697
-1.04
0.300
-3.052436
.9412284
priGPAcu |
.1842414
.1262839
1.46
0.145
-.0637183
.4322011
ACT |
.3059699
.7236971
0.42
0.673
-1.115017
1.726957
ACTsq | -.0141693
.0319499
-0.44
0.658
-.0769033
.0485648
ACTcu |
.0002633
.0004629
0.57
0.570
-.0006456
.0011722
frosh | -.1138172
.1035799
-1.10
0.272
-.3171976
.0895631
soph | -.1863224
.0870459
-2.14
0.033
-.3572381
-.0154067
_cons | -4.501184
5.629291
-0.80
0.424
-15.55436
6.55199
-----------------------------------------------------------------------------

21.8. a. From (21.129) and (21.130), we are assuming
a   0  x  u
Eu|x, z  0
and so we can write
y   0  w  x  u  e
≡  0  w  x  r
where Er|x, z  Eu|x, z  Ee|x, z  0. Therefore, we need to add the usual rank condition:
z must appear with nonzero coefficient vector in the linear projection of w on 1, x, z. More
precisely, if
Lw|1, x, z   0  x 1  z 2
then  2 ≠ 0.
b. If
w  max0,  0  x 1  z 2  v,
Dv|x, z ~ Normal0,  2 ,
then

505

Ew|x, z  q/  q    q/,
where q ≡  0  x 1  z 2 [see equation (17.14)]. Because Ew|x, z is a function of x, z and
we have
y   0  w  x  r
Er|x, z  0,
we can use q/  q    q/ as a valid instrument for w. (Remember, any function
of x, z is uncorrelated with r provided the second moments exist.) Because we do not know 
or , we replace them with estimators. In other words, use q i ̂/̂   q i ̂  ̂  q i ̂/̂  as the
IV for w i , where ̂ and ̂ are the estimates from an initial Tobit MLE.
c. By equation (14.57), the optimal IV for w is
Ew|x, z/Varr|x, z.
But Ee|a, x, z  0 implies that e and a are uncorrelated, conditional on x, z. Therefore,
Varu  e|x, z  Varu|x, z  Vare|x, z
  2a   2e ≡  2r .
Therefore, Varr|x, z is constant, and Ew|x, z can serve as the optimal IV for w. As usual,
we replace the parameters in Ew|x, z with N -consistent estimators. The results in Chapter 6
on generated instruments can be used to show that the resulting IV estimator has the same
N -asymptotic distribution as if we know  and .
d. An alternative method would be to run the OLS regression
y i on 1, ŵ i , x i , i  1, … , N
where
ŵ i ≡ q i ̂/̂   q i ̂  ̂  q i ̂/̂ 

506

are the estimated conditional means. While this “plug-in” approach may produce estimates
similar to the IV approach, it is less preferred for the same reasons we covered for the probit
case. First, using the ŵ i as regressors rather than instruments is less robust: using them as
regressors essentially requires the Tobit model to be correctly specified for w given x, z.
Second, valid standard errors are harder to get using ŵ i as a regressor as opposed to an IV.
Third, the plug-in procedure does not appear to be optimal within an interesting class of
estimators. (By contrast, we know that the IV estimator is optimal in the class of IV estimators
under the assumptions given for part c.)
e. Estimate y i   0  x i   w i  w i  x i − x̄   error i by IV, using instruments,
1, x i , ŵ i , ŵ i  x i − x̄  as instruments, where ŵ i are the Tobit fitted values. This would be
generally inefficient, as the error, r, is not necessarily homoskedastic. Another drawback is
that this would not generate overidentifying restrictions.
21.9. a. A histogram of the estimated propensity score for the untreated (train  0) and
treated (train  1) cases is given below. There is a clear problem with overlap, as can be seen
by studying the histogram for the control group: over 80% of units have propensity scores that
are zero or practically zero. This means there are values of x where px  0 or is barely
distinguishable from zero.
The large differences in the histograms for the control and treatment groups spells trouble.
Because px is just a particular function of x, ideally its distribution would be similar across
the control and treatment groups, and this is clearly not the case. The problems this causes is
easily reasoned when thinking of matching on the propensity score. We need to find both
control and treated units with similar values of px, but the histograms make it clear that there
are very few in the control group with p̂ x i  . 5, whereas this is where the bulk of the

507

observations lie for the treated group.
For comparison, the same histograms are plotted using the experimental data in
JTRAIN2.RAW. Now the histograms are virtually indistinguishable and neither has mass at
zero or one.

508

. use jtrain3
. logit train age educ black hisp married re74 re75
Logistic regression

Number of obs
LR chi2(7)
Prob  chi2
Pseudo R2

Log likelihood  -236.23799






2675
872.82
0.0000
0.6488

----------------------------------------------------------------------------train |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------age | -.0840291
.014761
-5.69
0.000
-.1129601
-.055098
educ | -.0624764
.0513973
-1.22
0.224
-.1632134
.0382605
black |
2.242955
.3176941
7.06
0.000
1.620286
2.865624
hisp |
2.094338
.5584561
3.75
0.000
.9997841
3.188892
married | -1.588358
.2602448
-6.10
0.000
-2.098428
-1.078287
re74 |
-.117043
.0293604
-3.99
0.000
-.1745882
-.0594977
re75 | -.2577589
.0394991
-6.53
0.000
-.3351758
-.1803421
_cons |
2.302714
.9112559
2.53
0.012
.5166853
4.088743
----------------------------------------------------------------------------Note: 158 failures and 0 successes completely determined.
. predict phat
(option pr assumed; Pr(train))
. histogram phat, fraction by(train)

509

train = 0

train = 1

0

.2

.4

.6

.8

Fraction

0

.2

.4

.6

.8

1

0

P(train|x)

510

.2

.4

.6

.8

1

. use jtrain2
. logit train age educ black hisp married re74 re75
Logistic regression

Number of obs
LR chi2(7)
Prob  chi2
Pseudo R2

Log likelihood  -297.80826






445
8.58
0.2840
0.0142

----------------------------------------------------------------------------train |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------age |
.0107155
.014017
0.76
0.445
-.0167572
.0381882
educ |
.0628366
.0558026
1.13
0.260
-.0465346
.1722077
black | -.3553063
.3577202
-0.99
0.321
-1.056425
.3458123
hisp | -.9322569
.5001292
-1.86
0.062
-1.912492
.0479784
married |
.1440193
.2734583
0.53
0.598
-.3919492
.6799878
re74 | -.0221324
.0252097
-0.88
0.380
-.0715425
.0272777
re75 |
.0459029
.0429705
1.07
0.285
-.0383177
.1301235
_cons | -.9237055
.7693924
-1.20
0.230
-2.431687
.5842759
----------------------------------------------------------------------------. predict phat
(option pr assumed; Pr(train))
. histogram phat, fraction by(train)

511

train = 1

.1

.2

train = 0

0

Fraction

.2

.4

.6

.8

.2

P(train|x)

512

.4

.6

.8

b. Using the sample restricted to avgre ≤ 15 helps a little in that there now seem to be at
least some untreated units in bins with p̂ x i  . 3. But there are not many. The pile-up at near
zero for the control group is still present. The two histograms still look very different from the
experimental data in JTRAIN2.RAW.

513

.6
.4
.2
0

Fraction

train = 1

.8

train = 0

0

.2

.4

.6

.8

1

0

P(train|x)

514

.2

.4

.6

.8

1

c. (Bonus Part) Suppose that using all 2,675 observations that, after estimating the logit
model for train, we drop all data with p̂ x i  . 05. We then reestimate the logit model using the
remaining observations. How many observations are left? Obtain the resulting histograms as in
part a and part b.
Solution
The Stata session is given below. Only 422 observations are left after dropping those with
p̂ x i  . 05. The histograms look much better in terms of overlap: for the most part, it appears
that for p̂ x i  within a given bin, there a both treated and untreated observations. But the skew
of the distributions is completely different (and not too suprising).

515

. use jtrain3
. qui logit train age educ black hisp married re74 re75
. predict phat
(option pr assumed; Pr(train))
. drop if phat  .05
(2253 observations deleted)
. drop phat
. qui logit train age educ black hisp married re74 re75
. predict phat
(option pr assumed; Pr(train))
. histogram phat, fraction by(train)

516

.1

.2

.3

train = 1

0

Fraction

train = 0

0

.5

1

0

P(train|x)

517

.5

1

21.10. a. This is just problem in the asymptotic theory of simple regression with a binary
explanatory variable. With y i   0  w i  v i0 we have that w i is independent of v i0 , and so
there is no heteroskedasticity. It follows that (see Theorem 4.2)
Avar

Varv i0 
Varv i0 

Varw i 
1 − 

N ̂ −  

because Pw i  1  . This means, by definition,
Varv i0 
.
N1 − 

Avar̃  

b. By definition of the linear projection we can write
y i0   0  x i  0  u i0
Eu i0   0, Ex ′i u i0   0.
Now we just plug this into y i  y i0  w i :
y i   0  x i  0  u i0  w i   0  w i  x i  0  u i0 .
The problem says to assume that w i is independent of y i0 , x i  and so w i is actually independent
of x i , u i0  [because u i0 is a function of y i0 , x i ].
c. Let z i ≡ w i , x i  be the set of nonconstant regressors and let   ,  ′0  ′ . Then, as we
showed in Chapter 4, if ̂ is the OLS estimator under random sampling,
N

N ̂ −   Varz i  N
−1

−1/2

∑z i −  z  ′ u i0  o p 1.
i1

Given that Covx i , w i   0, we we restrict attention to the first element, N ̂ − , we get
N

N ̂ −   Varw i  N
−1

−1/2

∑w i −  ′ u i0  o p 1.
i1

Therefore, using independence between u i0 and w i ,

518

Avar

N ̂ −  

Varw i − u i0 
Varu i0 

2
1 − 
Varw i 

and so
Avar̂  

Varu i0 
.
N1 − 

d. Because y i0   0  v i0 , Varv i0   Vary i0 . Using the linear projection representation
y i0   0  x i  0  u i0 ,
Vary i0   Varx i  0   Varu i0 
and, assuming that Varx i  has full rank, Varx i  0   0 whenever  0 ≠ 0. So
Varu i0   Vary i0   Varv i0 .
It follows by comparing Avar̂  and Avar̃  that Avar̂   Avar̃  whenever  0 ≠ 0, that
is, whenever x i is correlated with y i0 .
e. Even though ̂ is asymptotically more efficient than ̃ , ̂ is generally biased if
Ey i0 |x i  ≠  0  x i  0 . [If Ey i0 |x i    0  x i  0 then ̂ would be conditionally unbiased, that is,
E̂ |W, X  .] The difference-in-means estimator ̃ is unbiased conditional on W because
Ey i |W  Ey i |w i    0  w i .
21.11. Suppose that we allow full slope, as well as intercept, heterogeneity in a linear
representation of two counterfactual outcomes,
y i0  a i0  x i b i0
y i1  a i1  x i b i1
Assume that the vector x i , z i  is independent of a i0 , b i0 , a i1 , b i1  – which makes, as we will
see, z i instrumental variables candidtates in a control function or correction function setting.
a. Because x i is independent of b ig , g  0, 1,
519

Ey ig   Ea ig   Ex i b ig    g  Ex i Eb ig 
  g   g , g  0, 1.
b. From part a,
 0   0   0
 1   1   1
and so
   1 −  0    1 −  0    1 −  0   .
Also,
y ig   g  x i  g  c ig  x i f ig , g  0, 1
and so
y i  1 − w i y i0  w i y i1
 1 − w i  0  x i  0  c i0  x i f i0   w i  1  x i  1  c i1  x i f i1 
  0   1 −  0 w i  x i  0  w i x i  1 −  0 
 c i0  w i c i1 − c i0   x i f i0  w i x i f i1 − f i0 
≡  0   1 −  0 w i  x i  0  w i x i   c i0  w i e i  x i f i0  w i x i d i .
Next, substitute
 0   0 −  0
 1 −  0   − 
to get
y i   0 −  0   − w i  x i  0  w i x i   c i0  w i e i  x i f i0  w i x i d i
  0  w i  x i −  0  w i x i −   c i0  x i f i0  w i e i  w i x i d i ,
which is what we wanted to show.
c. So that there is no notational conflict, write Ed i |a i , x i , z i   a i and keep  to index the
520

binary response model. Now take the expectation of (21.149) conditional on a i , x i , z i , using
the fact that w i is a function of a i , x i , z i :
Ey i |a i , x i , z i    0  w i  x i −  0  w i x i − 
 Ec i0 |a i , x i , z i   x i Ef i0 |a i , x i , z i 
 w i Ee i |a i , x i , z i   w i x i Ed i |a i , x i , z i 
  0  w i  x i −  0  w i x i − 
  0 a i  a i x i  0  w i a i  w i a i x i 
d. Just use iterated expectations along with
Ea i |w i , q i   hw i , q i  ≡ w i q i  − 1 − w i −q i .
So
Ey i |w i , x i , z i    0  w i  x i −  0  w i x i − 
  0 hw i , q i   hw i , q i x i  0  w i hw i , q i   w i hw i , q i x i 
e. Given Ey i |w i , x i , z i , the CF method is straightforward. In the first step, estimate probit
of w i on q i to get ̂, and then compute ĥ i ≡ hw i , q i ̂. Then run the OLS regression
y i on 1, w i , x i − x̄ , w i x i − x̄ , ĥ i , ĥ i x i , w i ĥ i , w i ĥ i x i , i  1, . . . , N.
We replace  with the sample average, x̄ . The coefficient on w i is ̂ .
Compared with the regression in equation (21.85), we have included the interactions ĥ i x
and w i ĥ i x i . These account for the random coefficients in the counterfactual equations.
f. Of course we could work through the delta method to obtain a valid asymptotic standard
error for ̂ , but bootstrapping both steps in the procedure provides a simple alternative.
g. We just compute Ey ig |x for g  0, 1:

521

Ey ig |x i   Ea ig  x i b ig |x i 
 Ea ig |x i   x i Eb ig |x i 
 g  xig
where the last equality holds by the independence assumption. Therefore,
 ate x   1 −  0   x 1 −  0 
  −   x    x − .
So
̂ ate x  ̂  x − x̄ ̂.
21.12. a. The terms c i0 and x i f i0 have zero means conditional on x i , z i  by the
independence of all heterogeneity terms – a i0 , b i0 , a i1 , b i1  – and x i , z i . Remember,
c i0  a i0 −  0 and f i0  b i0 −  0 .
b. The correction functions in this case are Ew i e i |x i , z i  and
Ew i x i d i |x i , z i   x i Ew i d i |x i , z i . Now we just use the formula in equation (21.80) because
Ee i |a i , x i , z i   a i and Ed i |a i , x i , z i   a i . Therefore,
Ew i e i |x i , z i   q i 
Ew i x i d i |x i , z i   x i q i   q i x i .
c. From part b we can write
y i   0  w i  x i −  0  w i x i −   q i   q i x i   c i0  x i f i0  r i
where
r i  w i e i − Ew i e i |x i , z i   w i x i d i − Ew i x i d i |x i , z i 
and so Er i |x i , z i   0. The estimating equation, after the first-stage probit to get ̂, is
y i   0  w i  x i − x̄  0  w i x i − x̄   q i ̂  q i ̂x i   error i

522

which we can estimate using IV with instruments, say,
̂ i , x i − x̄ , 
̂ i  x i − x̄ , ̂ i , ̂ i  x i 
1, 
d. Under the null hypothesis,   0 and   0. The conditions sufficient to ignore the
first-stage estimator for IV estimators in Chapter 6 hold here, so we can use a standard Wald
test (perhaps made robust to heteroskedasticity) to test joint significance of the K  1 variables
̂ i , ̂ i  x i . Remember, these are acting as their own instruments in the estimation.
21.13. Fractional probit or logit are natural, or some other model that keeps the fitted
values in the unit interval. For the treatment rule w i  1x i ≥ c, let G̂ 0  ̂ 0 x be the
estimated fractional response model using the data with x i  c and let G̂ 1  ̂ 1 x be the
estimated model using the data with x i ≥ c. Probably the Bernoulli QMLE would be used.
Then similar to equation (21.104),
̂ c  G̂ 1  ̂ 1 c − G̂ 0  ̂ 0 c.
The delta method or bootstrapping can be used to obtain valid inference for
 c  G 1   1 c − G 0   0 c.
21.14. a. Just take the expected value of y it g  a itg  x it  g :
 gt  Ea itg   Ex it  g   tg   t  g , g  0, 1.
Therefore, for each t,
 t,ate   t1 −  t0    t  1 −  0 .
b. Use  t0   t0 −  t  0 and  t1 −  t0    t −  t  1 −  0  and plug into the equation for
y it :

523

y it  1 − w it  t0  x it  0  c it0   w it  t1  x it  1  c it1 
  t0  w it  t1 −  t0   x it  0  w it x it  1 −  0   c it0  w it c it1 − c it0 
  t0 −  t  0   w it  t −  t  1 −  0  
 x it  0  w it x it  1 −  0   c it0  w it c it1 − c it0 
  t0   t w it  x it −  t  0  w it x it −  t   c it0  w it e it
where  ≡  1 −  0 and e it ≡ c it1 − c it0 .
c. Plugging in for c it0 and e it gives
y it   t0   t w it  x it −  t  0  w it x it −  t 
 x̄ i −  x̄  1  z̄ i −  z̄  2  r it0  w it x̄ i −  x̄  1  z̄ i −  z̄  2  v it 
  t0   t w it  x it −  t  0  w it x it −  t 
 x̄ i −  x̄  1  z̄ i −  z̄  2  w it x̄ i −  x̄  1  w it z̄ i −  z̄  2
 r it0  w it v it
d. As usual, we first condition on q it , x i , z i :
Ey it |q it , x i , z i    t0   t w it  x it −  t  0  w it x it −  t 
 x̄ i −  x̄  1  z̄ i −  z̄  2  w it x̄ i −  x̄  1  w it z̄ i −  z̄  2
 Er it0 |q it , x i , z i   w it Ev it |q it , x i , z i 
  t0   t w it  x it −  t  0  w it x it −  t 
 x̄ i −  x̄  1  z̄ i −  z̄  2  w it x̄ i −  x̄  1  w it z̄ i −  z̄  2
  0 q it  w it q it .
Now condition on w it , x i , z i  using iterated expectations:
Ey it |w it , x i , z i    t0   t w it  x it −  t  0  w it x it −  t 
 x̄ i −  x̄  1  z̄ i −  z̄  2  w it x̄ i −  x̄  1  w it z̄ i −  z̄  2
  0 hw it , g it   w i hw it , g it ,
where
hw it , g it   w it g it  − 1 − w it −g it 

524

and g it    0  x it  1  z it  2  x̄ i  3  z̄ i  4 .
e. In the first step we can use pooled probit of w it on 1, x it , z it , x̄ i , z̄ i to obtain ̂ and
hw it , g it ̂. Then we can use pooled OLS in a second step:
y it on 1, d2 t , ..., dT t , w it , d2 t w it , ..., dT t w it , x it − x̄ t , w it  x it − x̄ t ,
x̄ i − x̄ , z̄ i − z̄ , w it  x̄ i − x̄ , w it  z̄ i − z̄ , ĥ it , w it  ĥ it
where dr t is a time dummy for period r, ĥ it ≡ hw it , g it ̂, and overbars denote sample
averages.
Note that in the conditional expectation underlying the CF method, Ey it |w it , x i , z i ,
w it : t  1, . . . , T is not guaranteed to be strictly exogenous. Therefore, we cannot use
GLS-type methods without making extra assumptions.
21.15. a. The Stata output is given below. There are 5,735 observations, and 2,184 received
a right-heart catheterization.
. tab rhc
1 if |
received |
right heart |
catheteriza |
tion |
Freq.
Percent
Cum.
----------------------------------------------0 |
3,551
61.92
61.92
1 |
2,184
38.08
100.00
----------------------------------------------Total |
5,735
100.00

b. The Stata output, using simple regression and heteroskedasticity-robust standard errors,
is given below. According to the estimate, people receiving an RHR have a . 051 higher
probability of dying. The estimate has a robust t statistic of 3. 95, so it is statistically different
from zero (and practically large – in the “wrong” direction).
. reg death rhc, robust
Number of obs 
F( 1, 5733) 

Linear regression

525

5735
15.56

Prob  F
R-squared
Root MSE





0.0001
0.0027
.47673

----------------------------------------------------------------------------|
Robust
death |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------rhc |
.0507212
.0128566
3.95
0.000
.0255174
.0759249
_cons |
.6296818
.0081049
77.69
0.000
.6137931
.6455705
-----------------------------------------------------------------------------

c. The Stata code and results are given below. We still obtain counterintuitive results:
̂ ate,reg . 078, with a bootstrapped standard error of . 013, and ̂ att,reg . 066 (se . 014). Thus,
both estimates are statistically different from zero. we can either conclude that the controls we
include do not make treatment assignment ignorable or that the RHC actually increases the
probability of death.
clear all
capture program drop ateboot
program ateboot, eclass
* Estimate logit on treatment and control groups separately
tempvar touse
gen byte ‘touse’  1
xi: logit death i.female i.race i.income i.cat1 i.cat2 i.ninsclas age if rhc
predict d1hat
xi: logit death i.female i.race i.income i.cat1 i.cat2 i.ninsclas age if ~rhc
predict d0hat
gen diff  d1hat - d0hat
sum diff
scalar ate  r(mean)
sum diff if rhc
scalar att  r(mean)
matrix b  (ate, att)
matrix colnames b  ate att
ereturn post b , esample(‘touse’)
ereturn display
drop d1hat d0hat diff _I*
end
use catheter
bootstrap _b[ate] _b[att], reps(1000) seed(123): ateboot
program drop ateboot
do catheter_reg
. use catheter

526

. bootstrap _b[ate] _b[att], reps(1000) seed(123): ateboot
(running ateboot on estimation sample)
Bootstrap replications (1000)
------- 1 ------ 2 ------ 3 ------ 4 ------ 5
..................................................

50

..................................................

1000

Bootstrap results
command:
_bs_1:
_bs_2:




Number of obs
Replications

5735
1000

ateboot
_b[ate]
_b[att]

----------------------------------------------------------------------------|
Observed
Bootstrap
Normal-based
|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------_bs_1 |
.0776176
.0129611
5.99
0.000
.0522143
.1030208
_bs_2 |
.0656444
.01366
4.81
0.000
.0388713
.0924175
----------------------------------------------------------------------------.
. program drop ateboot
end of do-file

d. The average p̂ i for the treated group is about . 445 and it ranges from about . 085 to . 737.
For the control group, the numbers are . 341, . 044, and . 738. Though the mean propensity
score is somewhat higher for the treated group, the ranges are comparable. The two histograms
show that for both groups the probabilities stay away from the extremes of zero and one, and
for every bin representing intervals of the estimated propensity score, there are several
individuals in the control and treatment groups. Overlap appears to be good.
. use catheter
. xi: logit rhc i.female i.race i.income i.cat1 i.cat2 i.ninsclas age
i.female
_Ifemale_0-1
(naturally coded; _Ifemale_0 omitted)
i.race
_Irace_0-2
(naturally coded; _Irace_0 omitted)
i.income
_Iincome_0-3
(naturally coded; _Iincome_0 omitted)
i.cat1
_Icat1_1-9
(naturally coded; _Icat1_1 omitted)
i.cat2
_Icat2_1-7
(naturally coded; _Icat2_1 omitted)
i.ninsclas
_Ininsclas_1-6
(naturally coded; _Ininsclas_1 omitted)
Logistic regression

Number of obs
LR chi2(26)
Prob  chi2
Pseudo R2

Log likelihood  -3497.9617

527






5735
625.48
0.0000
0.0821

----------------------------------------------------------------------------rhc |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------_Ifemale_1 |
.1630495
.0587579
2.77
0.006
.0478861
.2782129
_Irace_1 |
.0424279
.0827591
0.51
0.608
-.1197771
.2046328
_Irace_2 |
.0393684
.1361998
0.29
0.773
-.2275784
.3063151
_Iincome_1 |
.0443619
.0762726
0.58
0.561
-.1051296
.1938535
_Iincome_2 |
.151793
.0892757
1.70
0.089
-.0231841
.3267701
_Iincome_3 |
.1579471
.1140752
1.38
0.166
-.0656361
.3815303
_Icat1_2 |
.498032
.107388
4.64
0.000
.2875553
.7085086
_Icat1_3 | -1.226306
.1495545
-8.20
0.000
-1.519428
-.9331849
_Icat1_4 | -.7173791
.1714465
-4.18
0.000
-1.053408
-.3813501
_Icat1_5 | -1.002513
1.085305
-0.92
0.356
-3.129671
1.124645
_Icat1_6 | -.6941957
.1260198
-5.51
0.000
-.94119
-.4472013
_Icat1_7 | -1.258815
.4833701
-2.60
0.009
-2.206203
-.3114273
_Icat1_8 | -.2076635
.1177652
-1.76
0.078
-.438479
.0231519
_Icat1_9 |
1.003787
.0768436
13.06
0.000
.8531766
1.154398
_Icat2_2 |
.9804654
1.465085
0.67
0.503
-1.891048
3.851979
_Icat2_3 | -.4141065
.4428411
-0.94
0.350
-1.282059
.4538461
_Icat2_4 | -.8864827
.8454718
-1.05
0.294
-2.543577
.7706116
_Icat2_5 |
-.195389
.3933026
-0.50
0.619
-.966248
.57547
_Icat2_6 |
1.034498
.369503
2.80
0.005
.3102859
1.758711
_Icat2_7 |
.1415088
.3649828
0.39
0.698
-.5738443
.8568619
_Ininsclas_2 |
.1849583
.1216214
1.52
0.128
-.0534153
.4233318
_Ininsclas_3 |
.1082916
.152243
0.71
0.477
-.1900992
.4066824
_Ininsclas_4 |
.5216726
.1495659
3.49
0.000
.2285288
.8148164
_Ininsclas_5 |
.468176
.1122184
4.17
0.000
.248232
.6881199
_Ininsclas_6 |
.3742273
.1249122
3.00
0.003
.1294038
.6190508
age |
.0006419
.002252
0.29
0.776
-.0037719
.0050557
_cons |
-1.36677
.3834979
-3.56
0.000
-2.118412
-.6151284
----------------------------------------------------------------------------. predict phat
(option pr assumed; Pr(rhc))
. sum phat if rhc
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------phat |
2184
.4449029
.1421669
.0851523
.7369323
. sum phat if ~rhc
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------phat |
3551
.3414058
.1517016
.0435625
.7379614

528

.02

.04

.06

.08

rhc = 1

0

Fraction

rhc = 0

0

.2

.4

.6

.8

0

P(rhc = 1|x)

529

.2

.4

.6

.8

e. The estimates using propensity score weighting are very similar to the
regression-adjustment estimates using logit models. The PSW estimates are ̂ ate.psw . 072
(se . 013) and ̂ att.psw . 063 (se . 014). Unfortunately, out of the 1,000 bootstrap replications
that I ran to obtain the standard errors, only 219 produced usable results because the estimated
propensity score for at least some of the draws was identically zero or identically one (when
the covariates perfectly classify rhc). But there is clearly no evidence, based on these and the
regression-adjustment estimates, that RHC reduces the probability of death. It appears that
RHC is being applied to patients based on variables are not included in the data set that are
associated with both mortality and a doctor recommending RHC.
. do catheter_psw
clear all
capture program drop ateboot
program ateboot, rclass
* Estimate propensity score
xi: logit rhc i.female i.race i.income i.cat1 i.cat2 i.ninsclas age
predict phat
gen kiate  (rhc - phat)*death/(phat*(1 - phat))
sum kiate
return scalar atew  r(mean)
sum rhc
scalar rho  r(mean)
gen kiatt  (rhc - phat)*death/(1 - phat)
sum kiatt
return scalar attw  r(mean)/rho
drop phat kiate kiatt _I*
end
use catheter
bootstrap r(atew) r(attw), reps(1000) seed(123): ateboot
program drop ateboot
.
. use catheter
.
. bootstrap r(atew) r(attw), reps(1000) seed(123): ateboot

530

(running ateboot on estimation sample)
Bootstrap results
command:
_bs_1:
_bs_2:

Number of obs
Replications




5735
219

ateboot
r(atew)
r(attw)

----------------------------------------------------------------------------|
Observed
Bootstrap
Normal-based
|
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------_bs_1 |
.071762
.0131705
5.45
0.000
.0459483
.0975758
_bs_2 |
.0629458
.0140839
4.47
0.000
.035342
.0905497
----------------------------------------------------------------------------Note: one or more parameters could not be estimated in 781 bootstrap
replicates; standard-error estimates include only complete replications
. program drop ateboot
end of do-file

21.16. Use the data in REGDISC.RAW to answer this question. These are simulated data
of a fuzzy regression discontinuity design with forcing variable x. The discontinuity is at
x  5.
a. Exactly half of the observations have x i ≥ 5, but 58.1% (1,162 out of 2,000) of the
observations are in the treatment group.
. sum z w
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------z |
2000
.5
.500125
0
1
w |
2000
.581
.4935188
0
1
. tab w
1 if |
treated |
Freq.
Percent
Cum.
----------------------------------------------0 |
838
41.90
41.90
1 |
1,162
58.10
100.00
----------------------------------------------Total |
2,000
100.00

b. The Stata output is given below. The graphs of the estimated probabilities (LPM and
logit) are fairly similar, although the LPM estimates a larger jump in the treatment

531

probabilities: for the LPM it is about . 271 and for the logit it is about . 192. In fact, the logit
estimate is closer to the true jump in the propensity score at x  5, which is about . 186. [The
treatment probability was generated from the probit model
Pw  1|x  . 1 . 5  1x  5 . 3  x − 5, and so the jump in the probability of
treatment at x  5 is . 6 − . 1 ≈. 186.]

. reg w x if ~z
Source |
SS
df
MS
------------------------------------------Model | 17.5177744
1 17.5177744
Residual | 180.953226
998 .181315857
------------------------------------------Total |
198.471
999
.19866967

Number of obs
F( 1,
998)
Prob  F
R-squared
Adj R-squared
Root MSE








1000
96.61
0.0000
0.0883
0.0874
.42581

----------------------------------------------------------------------------w |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------x |
.0916522
.0093244
9.83
0.000
.0733545
.1099499
_cons |
.043984
.0269105
1.63
0.102
-.0088237
.0967917
----------------------------------------------------------------------------. predict wh0_lpm
(option xb assumed; fitted values)
. gen wh0_lpm_5  _b[_cons]  _b[x]*5 in 1
(1999 missing values generated)
. reg w x if z
Source |
SS
df
MS
------------------------------------------Model |
4.4914493
1
4.4914493
Residual | 94.1875507
998 .094376303
------------------------------------------Total |
98.679
999 .098777778

Number of obs
F( 1,
998)
Prob  F
R-squared
Adj R-squared
Root MSE








1000
47.59
0.0000
0.0455
0.0446
.30721

----------------------------------------------------------------------------w |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------x |
.0464084
.0067272
6.90
0.000
.0332073
.0596095
_cons |
.5408787
.0513891
10.53
0.000
.4400356
.6417218
----------------------------------------------------------------------------. predict wh1_lpm
(option xb assumed; fitted values)
. gen wh1_lpm_5  _b[_cons]  _b[x]*5 in 1

532

(1999 missing values generated)
. gen jump_lpm  wh1_lpm_5 - wh0_lpm_5
(1999 missing values generated)
. gen what  what0 if ~z
(1000 missing values generated)
. replace what  what1 if z
(1000 real changes made)
. sum jump_lpm
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------jump_lpm |
1
.2706757
.
.2706757
.2706757
. qui logit w x if ~z
. predict wh0_logit
(option pr assumed; Pr(w))
. gen wh0_logit_5  invlogit(_b[_cons]  _b[x]*5) in 1
(1999 missing values generated)
. qui logit w x if z
. predict wh1_logit
(option pr assumed; Pr(w))
. gen wh1_logit_5  invlogit(_b[_cons]  _b[x]*5) in 1
(1999 missing values generated)
. gen jump_logit  wh1_logit_5 - wh0_logit_5
(1999 missing values generated)
. sum jump_logit
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------jump_logit |
1
.1922199
.
.1922199
.1922199
. gen wh_logit  wh0_logit if ~z
(1000 missing values generated)
. replace wh_logit  wh1_logit if z
(1000 real changes made)

533

. twoway (line wh_lpm x, sort) (line wh_logit x, sort)

0

.2

.4

.6

.8

1

Estimated Propensity Scores

0

1

2

3

4

5

6

7

8

9

10
x

LPM

534

Logit

c. We already computed the jump for the LPM estimates of the propensity score in part b.
Now we need to estimate the jump in Ey|x at x  5. Using the linear model, this turns out to
be about . 531. From equaton (21.107) the estimate of  c is the ratio of the jumps, which is
about 1. 96. In fact, the true effect is two, so this estimate is very close for this set of data. The
data on y were generated as y i  1  2w i  x i /4  u i where u i is independent of x i and
treatment with a Normal0, . 36 distribution.
. reg y x if ~z
Source |
SS
df
MS
------------------------------------------Model | 409.736838
1 409.736838
Residual | 1109.52773
998 1.11175123
------------------------------------------Total | 1519.26457
999 1.52078535

Number of obs
F( 1,
998)
Prob  F
R-squared
Adj R-squared
Root MSE








1000
368.55
0.0000
0.2697
0.2690
1.0544

----------------------------------------------------------------------------y |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------x |
.4432576
.0230891
19.20
0.000
.3979488
.4885664
_cons |
1.066599
.0666359
16.01
0.000
.9358364
1.197361
----------------------------------------------------------------------------. gen yh0_5  _b[_cons]  _b[x]*5 in 1
(1999 missing values generated)
. reg y x if z
Source |
SS
df
MS
------------------------------------------Model | 230.759468
1 230.759468
Residual | 742.020178
998 .743507193
------------------------------------------Total | 972.779646
999 .973753399

Number of obs
F( 1,
998)
Prob  F
R-squared
Adj R-squared
Root MSE








1000
310.37
0.0000
0.2372
0.2365
.86227

----------------------------------------------------------------------------y |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------x |
.3326468
.0188819
17.62
0.000
.295594
.3696997
_cons |
2.151037
.1442388
14.91
0.000
1.86799
2.434083
----------------------------------------------------------------------------. gen yh1_5  _b[_cons]  _b[x]*5 in 1
(1999 missing values generated)
. gen jumpy  yh1_5 - yh0_5
(1999 missing values generated)
. sum jumpy

535

Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------jumpy |
1
.531384
.
.531384
.531384
. gen ate5  jumpy/jump_lpm
(1999 missing values generated)
. sum ate5
Variable |
Obs
Mean
Std. Dev.
Min
Max
--------------------------------------------------------------------ate5 |
1
1.963176
.
1.963176
1.963176

d. As is claimed in the text, the IV estimate from (21.108) is the same as the estimate from
(21.107), with a very slight rounding error in the sixth digit after the decimal point. The
heteroskedasticity-robust standard error is about . 205. (The nonrobust standard error is about
. 197.)
Because the true equation for y is linear in x – with an expected jump at x  5 – the IV
estimator (and hence the estimate from part c) is consistent even though w i follows a logit
model rather than an LPM.
. ivreg y x_5 zx_5 (w  z), robust
Instrumental variables (2SLS) regression

Number of obs
F( 3, 1996)
Prob  F
R-squared
Root MSE


2000
 3588.42
 0.0000
 0.8722

.5959

----------------------------------------------------------------------------|
Robust
y |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------w |
1.963177
.2046892
9.59
0.000
1.56175
2.364604
x_5 |
.263328
.0295197
8.92
0.000
.2054354
.3212206
zx_5 | -.0217891
.0214587
-1.02
0.310
-.0638729
.0202947
_cons |
2.29689
.1352279
16.99
0.000
2.031688
2.562093
----------------------------------------------------------------------------Instrumented: w
Instruments:
x_5 zx_5 z
-----------------------------------------------------------------------------

e. Using only the data with 3  x i  7 results in 800 observations, rather than 2,000. The
estimate is substantially smaller than two but, more importantly, its standard error has

536

increased to about . 327 from . 205. Nevertheless, the 95% confidence interval for  c easily
contains the true value  c  2.
. ivreg y x_5 zx_5 (w  z) if x  3 & x  7, robust
Instrumental variables (2SLS) regression

Number of obs
F( 3,
796)
Prob  F
R-squared
Root MSE







800
351.50
0.0000
0.7662
.61919

----------------------------------------------------------------------------|
Robust
y |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------w |
1.775465
.3267695
5.43
0.000
1.134033
2.416897
x_5 |
.3471895
.0726118
4.78
0.000
.2046563
.4897226
zx_5 | -.0991082
.0772654
-1.28
0.200
-.2507762
.0525599
_cons |
2.442008
.2182725
11.19
0.000
2.01355
2.870466
----------------------------------------------------------------------------Instrumented: w
Instruments:
x_5 zx_5 z
-----------------------------------------------------------------------------

537

Solutions to Chapter 22 Problems
22.1. a. In Stata, there are two possibilities for estimating a lognormal duration model: the
cnreg command (where we use the log of the duration as a response), and the streg
command (where we specify “lognormal” as the distribution). The streg command is more
flexible (and I use it in the next problem), but here I give the cnreg output. The value of the
log likelihood is −1, 597. 06.
. use recid
. cnreg ldurat workprg priors tserved felon alcohol drugs black married educ
age, censored(cens)
Censored-normal regression
Log likelihood 

Number of obs
LR chi2(10)
Prob  chi2
Pseudo R2

-1597.059






1445
166.74
0.0000
0.0496

----------------------------------------------------------------------------ldurat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------workprg | -.0625715
.1200369
-0.52
0.602
-.2980382
.1728951
priors | -.1372529
.0214587
-6.40
0.000
-.1793466
-.0951592
tserved | -.0193305
.0029779
-6.49
0.000
-.0251721
-.013489
felon |
.4439947
.1450865
3.06
0.002
.1593903
.7285991
alcohol | -.6349092
.1442166
-4.40
0.000
-.9178072
-.3520113
drugs | -.2981602
.1327355
-2.25
0.025
-.5585367
-.0377837
black | -.5427179
.1174428
-4.62
0.000
-.7730958
-.31234
married |
.3406837
.1398431
2.44
0.015
.066365
.6150024
educ |
.0229196
.0253974
0.90
0.367
-.0269004
.0727395
age |
.0039103
.0006062
6.45
0.000
.0027211
.0050994
_cons |
4.099386
.347535
11.80
0.000
3.417655
4.781117
---------------------------------------------------------------------------/sigma |
1.81047
.0623022
1.688257
1.932683
----------------------------------------------------------------------------Observation summary:
0 left-censored observations
552
uncensored observations
893 right-censored observations

b. I graphed the hazard at the stated values of covariates using the Stata commands below.
The estimated hazard initially increases, until about t ∗  4. 6, where it reaches the value of
. 0116 (roughly). It then falls, until it hits about about . 005 at t  81. It may make sense that
there are startup costs to becoming involved in crime upon release, so that the instantaneous

538

probability of recidivism initially increases (for about four and one-half months). After that,
the hazard falls monotonically, although it does not become zero at the largest observed
duration, 81 months.
. di  _b[_cons]  _b[felon]  _b[alcohol]  _b[drugs]  _b[priors]* 1.431834
 _b[tserved]*19.18201 _b[educ]* 9.702422 _b[age]* 345.436
4.616118
. clear
. range t .1 81 5000
obs was 0, now 5000
. gen hazard  (normalden((log(t) - 4.62)/1.81)/
(1 - normal((log(t) - 4.62)/1.81)))/(1.81*t)
egen maxhazard  max(hazard)
. list t hazard if hazard  maxhazard
--------------------
|
t
hazard |
|--------------------|
277. | 4.566573
.011625 |
--------------------
. twoway (line hazard t)

539

0 4.6
20
40

540
60
80
t

.002

.004

.006

.008

.01

.012

hazard

c. Using the only the uncensored in a linear regression analysis provides very different
estimates. For example, the alcohol and drugs coefficients are much smaller in magnitude,
with the latter actually changing sign and becoming very insignificant.
. reg ldurat workprg priors tserved felon alcohol drugs black married educ age
Source |
SS
df
MS
------------------------------------------Model | 33.7647818
10 3.37647818
Residual | 442.796158
541 .818477187
------------------------------------------Total |
476.56094
551 .864901888

Number of obs
F( 10,
541)
Prob  F
R-squared
Adj R-squared
Root MSE








552
4.13
0.0000
0.0709
0.0537
.9047

----------------------------------------------------------------------------ldurat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------workprg |
.0923415
.0827407
1.12
0.265
-.0701909
.254874
priors | -.0483627
.0140418
-3.44
0.001
-.0759459
-.0207795
tserved | -.0067761
.001938
-3.50
0.001
-.010583
-.0029692
felon |
.1187173
.103206
1.15
0.251
-.0840163
.3214509
alcohol | -.2180496
.0970583
-2.25
0.025
-.408707
-.0273923
drugs |
.0177737
.0891098
0.20
0.842
-.1572699
.1928172
black | -.0008505
.0822071
-0.01
0.992
-.1623348
.1606338
married |
.2388998
.0987305
2.42
0.016
.0449577
.432842
educ | -.0194548
.0189254
-1.03
0.304
-.0566312
.0177215
age |
.0005345
.0004228
1.26
0.207
-.000296
.0013651
_cons |
3.001025
.2438418
12.31
0.000
2.522032
3.480017
-----------------------------------------------------------------------------

d. Treating the censored durations as if they are uncensored also gives very different
estimates from the censored regression. Again, the estimated alcohol and drug effects are
attenuated toward zero, although not as much as when we drop all of the censored
observations. In any case, we should use censored regression analysis.
. reg ldurat workprg priors tserved felon alcohol drugs black married educ age
Source |
SS
df
MS
------------------------------------------Model | 134.350088
10 13.4350088
Residual | 1101.29155 1434 .767985737
------------------------------------------Total | 1235.64163 1444 .855707503

Number of obs
F( 10, 1434)
Prob  F
R-squared
Adj R-squared
Root MSE








1445
17.49
0.0000
0.1087
0.1025
.87635

----------------------------------------------------------------------------ldurat |
Coef.
Std. Err.
t
P|t|
[95% Conf. Interval
---------------------------------------------------------------------------workprg |
.008758
.0489457
0.18
0.858
-.0872548
.1047709

541

priors | -.0590636
.0091717
-6.44
0.000
-.077055
-.0410722
tserved | -.0094002
.0013006
-7.23
0.000
-.0119516
-.0068488
felon |
.1785428
.0584077
3.06
0.002
.0639691
.2931165
alcohol | -.2628009
.0598092
-4.39
0.000
-.3801238
-.1454779
drugs | -.0907441
.0549372
-1.65
0.099
-.19851
.0170217
black | -.1791014
.0474354
-3.78
0.000
-.2721516
-.0860511
married |
.1344326
.0554341
2.43
0.015
.025692
.2431732
educ |
.0053914
.0099256
0.54
0.587
-.0140789
.0248618
age |
.0013258
.0002249
5.90
0.000
.0008847
.0017669
_cons |
3.569168
.137962
25.87
0.000
3.298539
3.839797
-----------------------------------------------------------------------------

22.2. a. For this question, I use the streg command. The nohr option means that the ̂ j
that estimate the  j iin equation (22.25) are reported, rather than exp̂ j . Whether or not a
release was “supervised” has no discernible effect on the hazard, whereas, not surprisingly, a
history of rules violation while in prison does increase the recidivism hazard.
. use recid.dta
. gen failed  ~cens
. stset durat, failure(failed)
failure event:
obs. time interval:
exit on or before:

failed ! 0 & failed  .
(0, durat]
failure

----------------------------------------------------------------------------1445 total obs.
0 exclusions
----------------------------------------------------------------------------1445 obs. remaining, representing
552 failures in single record/single failure data
80013 total analysis time at risk, at risk from t 
0
earliest observed entry t 
0
last observed exit t 
81
. streg super rules workprg priors tserved felon alcohol drugs black married
educ age, dist(weibull) nohr
failure _d:
analysis time _t:

failed
durat

Weibull regression -- log relative-hazard form
No. of subjects 
No. of failures 
Time at risk


1445
552
80013



-1630.517

Log likelihood

Number of obs



1445

LR chi2(12)
Prob  chi2




170.51
0.0000

-----------------------------------------------------------------------------

542

_t |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------super | -.0078523
.0979703
-0.08
0.936
-.1998705
.184166
rules |
.0386963
.0166936
2.32
0.020
.0059774
.0714151
workprg |
.1039345
.0914158
1.14
0.256
-.0752371
.2831061
priors |
.086349
.0136871
6.31
0.000
.0595227
.1131752
tserved |
.0116506
.001933
6.03
0.000
.0078621
.0154392
felon | -.3111997
.1074569
-2.90
0.004
-.5218114
-.1005879
alcohol |
.4510744
.1059953
4.26
0.000
.2433275
.6588214
drugs |
.2623752
.0982732
2.67
0.008
.0697632
.4549872
black |
.458454
.0884443
5.18
0.000
.2851063
.6318016
married | -.1563693
.10941
-1.43
0.153
-.3708088
.0580703
educ | -.0246717
.019442
-1.27
0.204
-.0627772
.0134339
age | -.0035167
.0005306
-6.63
0.000
-.0045567
-.0024767
_cons | -3.466394
.3105515
-11.16
0.000
-4.075064
-2.857724
---------------------------------------------------------------------------/ln_p | -.2142514
.038881
-5.51
0.000
-.2904567
-.1380461
---------------------------------------------------------------------------p |
.8071455
.0313826
.7479219
.8710585
1/p |
1.238934
.0481709
1.148028
1.337038
-----------------------------------------------------------------------------

b. The lognormal estimates are given below. The estimated coefficients on super and rules
are consistent with the Weibull results because a decrease in x shifts up the hazard in the
lognormal case.
. streg super rules workprg priors tserved felon alcohol drugs black married
educ age, dist(lognormal)
failure _d:
analysis time _t:

failed
durat

Lognormal regression -- accelerated failure-time form
No. of subjects 
No. of failures 
Time at risk


1445
552
80013



-1594.1683

Log likelihood

Number of obs



1445

LR chi2(12)
Prob  chi2




172.52
0.0000

----------------------------------------------------------------------------_t |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------super |
.0328411
.1280452
0.26
0.798
-.2181229
.2838052
rules | -.0644316
.0276338
-2.33
0.020
-.1185929
-.0102703
workprg | -.0883445
.1208173
-0.73
0.465
-.325142
.148453
priors | -.1341294
.0215358
-6.23
0.000
-.1763388
-.0919199
tserved | -.0156015
.0033872
-4.61
0.000
-.0222403
-.0089628
felon |
.4345115
.1460924
2.97
0.003
.1481757
.7208473
alcohol | -.6415683
.1439736
-4.46
0.000
-.9237515
-.3593851
drugs | -.2785464
.1326321
-2.10
0.036
-.5385007
-.0185922
black |
-.549173
.1172236
-4.68
0.000
-.7789271
-.3194189
married |
.3308228
.1399244
2.36
0.018
.056576
.6050695
educ |
.0234116
.0253302
0.92
0.355
-.0262347
.073058
age |
.0036626
.0006117
5.99
0.000
.0024637
.0048614

543

_cons |
4.173851
.3580214
11.66
0.000
3.472142
4.87556
---------------------------------------------------------------------------/ln_sig |
.5904906
.034399
17.17
0.000
.5230698
.6579114
---------------------------------------------------------------------------sigma |
1.804874
.0620858
1.687199
1.930755
-----------------------------------------------------------------------------

c. The coefficient from the lognormal model directly estimates the proportional effect of
rules violations on the duration. So, one more rules violation reduces the estimated expected
duration by about 6.4%. To obtain the comparable Weibull estimate, we need
−̂ rules /̂  −. 0387/. 807 ≈ −. 048, or about a 4.8% reduction for each rules violation – a bit
smaller than the lognormal estimate.
22.3. a. If all durations in the sample are censored, d i  0 for all i, and so the log-likelihood
N

N

is ∑ i1 log1 − Ft i |x i ;   ∑ i1 log1 − Fc i |x i ; .
b. For the Weibull case, Ft|x i ;   1 − exp− expx i t  , and so the log-likelihood is
N

− ∑ i1 expx i c i .
c. Without covariates, the Weibull log-likelihood with all observations censored is
N

N

− exp ∑ i1 c i . Because c i  0, we can choose any   0 so that ∑ i1 c i  0. But then, for
any   0, the log-likelihood is maximized by minimizing exp across . But as
 → −, exp → 0. Therefore, plugging any value  into the log-likelihood will lead to 
getting more and more negative without bound. So no two real numbers for  and  maximize
the log likelihood.
d. It is not possible to estimate duration models from flow data when all durations are right
censored.
e. To have all durations censored in a large sample we would have to have Pt ∗i  c i  very
close to one. But if Pt ∗i  t  0 for all t  0 and c i  b  0,
Pt ∗i  c i  ≤ Pt ∗i  b  1 − Pt ∗i ≤ b,

544

and Pt ∗i ≤ b  0. So with large samples we should not expect to find that all durations have
been censored.
22.4. a. The binary response d i is equal to one if the observation is uncensored. Because t ∗i
is independent of c i conditional on x i ,
Pd i  1|x i , c i   Pt ∗i ≤ c i |x i   Fc i |x i ; .
Therefore, the log-likelihood is
N

∑d i logFc i |x i ; 

 1 − d i  log1 − Fc i |x i ; ,

i1

which is just of the usual binary response form.
b. When the distribution is Weibull and x i  1, we have (from Problem 22.3c),
Fc i |  1 − exp− expc i , and so the log-likelihood is
N

ℒ,  

∑d i log1 − exp− expc i 

 1 − d i  logexp− expc i .

i1

If c i  c  0 for all i, we have
N

ℒ,  

∑d i log1 − exp− expc  

 1 − d i  logexp− expc  .

i1

If we define  ≡ exp− expc   then 0    1, and the log-likelihood can be written as
N

∑d i log  1 − d i  log1 − .
i1

In other words, the log-likelihood is the same for all combinations of  and  that give the
same value of . While we can consistently estimate  – the fraction of uncensored
observations is the maximum likelihood estimator – we cannot recover estimates of  and . In

545

other words,  and  are not identified.
c. In the log-normal case, the log-likelihood is based on
Plogt ∗i  ≤ logc i |x i   1/ logc i  − 1/x i , because
logt ∗i |x i , c i  ~ Normalx i ,  2 . The log-likelihood is
N

ℒ,   
2

∑ ℓ i ,  2 
i1
N



∑d i log1/ logc i  − 1/x i 

 1 − d i  log1 − 1/ logc i  − 1/x i .

i1

Even though x i1 ≡ 1, 1/ is identified as the coefficient on logc i , provided c i varies. Then, of
course, we can identify  because / is generally identified from the probit log-likelihood.
[We would need to assume the usual condition that rank Ex ′i x i   K.] If c i  c for all i then
the intercept effectively becomes 1/ logc −  1 /; along with  j /, j  2, … , K, these are
the only parameters we can identify. We cannot separately identify  or . This is the same
situation we faced in Section 19.2.1.
22.5. a. We have
Pt ∗i ≤ t|x i , a i , c i , s i  1  Pt ∗i ≤ t|x i , t ∗i  b − a i 
 Pt ∗i ≤ t, t ∗i  b − a i |x i /Pt ∗i  b − a i |x i   Pb − a i  t ∗i ≤ t|x i /Pt ∗i  b − a i |x i 
 Ft|x i  − Fb − a i |x i /1 − Fb − a i |x i 
where we use the fact that t  b − a i .
b. The derivative of the cdf in part a, with respect to t, is simply ft|x i /1 − Fb − a i |x i .
c. Because s i  1t ∗i  b − a i  and t i  c i if and only if t ∗i ≥ c i , we have
Pt i  c i |x i , a i , c i , s i  1  Pt ∗i ≥ c i |x i , t ∗i  b − a i 
 Pt ∗i ≥ c i |x i /Pt ∗i ≥ b − a i |x i 
 1 − Fc i |x i /1 − Fb − a i |x i 

546

where the third equality follows because c i  b − a i .
d. Parts b and c show that the density of the censored variable, t i , conditional on
x i , a i , c i , s i  1, can be written as
ft|x i  1tc i  1 − Fc i |x i  1tc i 
.
1 − Fb − a i |x i 
Showing the dependence on the parameters, plugging in t i , noting that d i  1t i  c i , and
taking the log gives
d i logft|x i ;   1 − d i  log1 − Fc i |x i ;  − log1 − Fb − a i |x i ; .
Summing across all N observations gives equation (22.30).
22.6. In what follows, we initially suppress dependence on the parameters.
a. Because s i  1t ∗i ≥ b − a i ,
Pa i ≤ a, s i  1|x i   Pa i ≤ a, t ∗i ≥ b − a i |x i 


a



 0  b−u qu, v|x i dvdu,

where q, |x i  denotes the joint density of a i , t ∗i  given x i . By conditional independence,
qa, t|x i   ka|x i ft|x i , and so the double integral is
a



 0  b−u fv|x i dv
because 


b−u

ku|x i du 

a

 0 1 − Fb − u|x i ku|x i du

fv|x i dv  1 − Fb − u|x i .

b. From the hint, we first compute Es i  1|a i , x i   Pt ∗i ≥ b − a i |x i   1 − Fb − a i |x i .
Next, we compute the expected value of this with respect to the distribution Da i |x i , which is
simply equation (22.32).
c. The conditional cdf is obtained by dividing the answer from part a by the answer from
part b. The density is just the derivative of the resulting expression with respect to a; by the
547

fundamental theorem of calculus, the derivative is (22.31).
d. When b  1 and ka|x i   1, all 0  a  1, the numerator of (22.31) is just
1

1 − F1 − a|x i . The denominator is simply  1 − F1 − u|x i du.
0

e. In the Weibull case, 1 − F1 − a|x i   exp− expx i 1 − a   and the denominator is
1

 0 exp− expx i 1 − u  du. This integral cannot be solved in closed form unless   1.
22.7. a. For notational simplicity, the parameters in the densities are suppressed. Then, by
equation (22.22) and Da i |c i , x i   Da i |x i , the density of a i , t ∗i  given c i , x i  does not
depend on c i and is given by ka|x i ft|x i  for 0  a  b and 0  t  . This is also the
conditional density of a i , t i  given c i , x i  for t  c i , that is, for values of t corresponding to
being uncensored. For t  c i , the density is ka|x i 1 − Fc i |x i  by the usual right censoring
argument. Now, the probability of observing the random draw a i , c i , x i , t i , conditional on x i ,
is Pt ∗i ≥ b − a i , x i , which is exactly (22.32). From the standard result for densities for
truncated distributions, the density of a i , t i  given c i , d i , x i  and s i  1 is
ka|x i ft|x i  d i 1 − Fc i |x i  1−d i  /Ps i  1|x i ,
for all combinations a, t such as that s i  1. Putting in the observed data, inserting the
parameters, and taking the log gives (22.56).
b. We have the usual trade-off between robustness and efficiency. Using the log likelihood
(20.56) results in more efficient estimators provided we have the two densities correctly
specified; (20.30) requires us to only specify f|x i .
22.8. a. Again I suppress the parameters in the densities. Let z i ≡ t ∗i − b, so that, by
assumption, a i and z i are independent conditional on x i . The conditional density of z i is simply
gz|x i  ≡ fz  b|x i , z  −b. By the usual convolution formula for the density of a sum of

548

independent random variables,
hr|x i  

b

b

 0 ku|x i gr − u|x i du   0 ku|x i fr  b − u|x i du,

where fr  b − u|x i   0 if r  b − u ≤ 0. When r  0, r  b − u  0 for all 0 ≤ u ≤ b, and so
we need not modify the formula.
b. As usual for right censoring, Pr i  q|x i   Pr ∗i  q|x i   1 − Hq|x i .
c. The argument is now essentially the same as the first stock sampling argument treated in
Section 22.3.3, except that we cannot condition on a i . Instead, for 0  r  q,
Pr i ≤ r|x i , s i  1  Hr|x i  − H0|x i /Ps i  1|x i  – and so the density for 0  r  q is
hr|x i /Ps i  1|x i  – and Pr i  q|x i , s i  1  1 − Hq|x i /Ps i  1|x i , where Ps i  1|x i 
is given by (22.32).
d. With b  1 and a uniform distribution for a i , the log-likelihood reduces to
d i loghr i |x i ; ,   1 − d i  log1 − Hr i |x i ; ,  − log

1

 0 1 − F1 − u|x i ; du

.

22.9. a. Let  be the value for type B people. Then we must have
  1 −   1
or   1 − /1 − .
b. The cdf conditional on x, v is Ft|x, v; ,   1 − exp−v expxt  . Therefore, the cdf
conditional on x is obtained by averaging out v:
Gt|x; , , ,   1 − exp− expxt  
 1 − 1 − exp−1 − /1 −  expxt  .
c. The density function is just the derivative with respect to t:

549

gt|x; , , ,    expxt −1 exp− expxt  
 1 −  expxt −1 exp−1 − /1 −  expxt  
If none of the durations are censore, the log-likelihood for each observation i is obtained by
taking the log of this density in plugging in x i , t i . If we have right censored data with
censoring values c i , the log likelihood takes on the usual form:
d i loggt i |x i ; , , ,   1 − d i  logGt i |x i ; , , , 
where d i is the dummy variable equal to one of the observation is not censored. The log
likelihood for the entire sample is a very smooth function of all parameters. As a
computational device, it might be better to replace  with, say, exp/1  exp for
−     and similarly for .
22.10. a. If PT  a m−1   0 then PT  a m   0 because a m  a m−1 in which case the
equality is trivial. So assume that PT  a m−1   0. Then, by definition of conditional
probability,
PT  a m |T  a m−1   PT  a m , T  a m−1 /PT  a m−1 
 PT  a m /PT  a m−1 
since the events T  a m , T  a m−1  and T  a m  are identical when a m  a m−1 .
Rearranging the equality gives the result.
b. We can use induction to obtain an algebraically simple proof. First, equation (22.48)
holds trivially when m  1: PT  a 1   PT  a 1 |T  0 because PT  0  1. Now assume
that (22.48) holds for any m ≥ 1. We show it holds for m  1. By part a,
PT  a m1   PT  a m1 |T  a m PT  a m 
m

 PT  a m1 |T  a m   PT  a r |T  a r−1 
r1

550

m

because PT  a m    r1 PT  a r |T  a r−1  by the induction hypothesis. It follows that
m1

PT  a m1  

 PT  a r |T  a r−1 
r1

and this completes the proof.
22.11. a. The estimates from the log-logistic model with gamma-distributed hazard are
given below. For comparison purposes, the Stata output used to produce Table 22.2 follows.
The log likelihood for the log-logistic model is −1, 587. 92 and that for the Weibull model
(both with gamma heterogeneity) is −1, 584. 92. The Weibull model fits somewhat better. (A
Vuong model selection statistic could be computed to see if the fit is statistically better.)
. streg

workprg priors tserved felon alcohol drugs black married educ age,
d(loglogistic) fr(gamma)

failure _d:
analysis time _t:

failed
durat

Loglogistic regression -- accelerated failure-time form
Gamma frailty
No. of subjects 
No. of failures 
Time at risk


1445
552
80013



-1587.9185

Log likelihood

Number of obs



1445

LR chi2(10)
Prob  chi2




132.67
0.0000

----------------------------------------------------------------------------_t |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------workprg |
.0098501
.11888
0.08
0.934
-.2231504
.2428505
priors | -.1488187
.0231592
-6.43
0.000
-.19421
-.1034275
tserved | -.0190707
.0036147
-5.28
0.000
-.0261553
-.0119861
felon |
.4219072
.1502377
2.81
0.005
.1274467
.7163677
alcohol | -.6597875
.147537
-4.47
0.000
-.9489547
-.3706203
drugs | -.2156168
.131043
-1.65
0.100
-.4724563
.0412227
black | -.4355534
.1209008
-3.60
0.000
-.6725147
-.1985921
married |
.4345344
.1353415
3.21
0.001
.16927
.6997988
educ |
.0243336
.0265813
0.92
0.360
-.0277648
.0764321
age |
.0032531
.000633
5.14
0.000
.0020124
.0044937
_cons |
3.34599
.3570848
9.37
0.000
2.646117
4.045864
---------------------------------------------------------------------------/ln_gam | -.3648587
.0716481
-5.09
0.000
-.5052864
-.2244309
/ln_the |
.8176437
.1998151
4.09
0.000
.4260133
1.209274
---------------------------------------------------------------------------gamma |
.6942948
.0497449
.6033327
.7989708

551

theta |
2.265156
.4526125
1.531141
3.351051
----------------------------------------------------------------------------Likelihood-ratio test of theta0: chibar2(01) 
46.05 Probchibar2  0.000
. streg

workprg priors tserved felon alcohol drugs black married educ age,
d(weibull) fr(gamma) nohr

failure _d:
analysis time _t:

nocens
durat

Weibull regression -- log relative-hazard form
Gamma frailty
No. of subjects 
No. of failures 
Time at risk


1445
552
80013



-1584.9172

Log likelihood

Number of obs



1445

LR chi2(10)
Prob  chi2




143.82
0.0000

----------------------------------------------------------------------------_t |
Coef.
Std. Err.
z
P|z|
[95% Conf. Interval
---------------------------------------------------------------------------workprg |
.0073827
.2038775
0.04
0.971
-.3922099
.4069753
priors |
.2431142
.0421543
5.77
0.000
.1604933
.3257352
tserved |
.0349363
.0070177
4.98
0.000
.0211818
.0486908
felon | -.7909533
.2666084
-2.97
0.003
-1.313496
-.2684104
alcohol |
1.173558
.2805222
4.18
0.000
.6237451
1.723372
drugs |
.2847665
.2233072
1.28
0.202
-.1529074
.7224405
black |
.7715762
.2038289
3.79
0.000
.372079
1.171073
married | -.8057042
.2578214
-3.13
0.002
-1.311025
-.3003834
educ | -.0271193
.044901
-0.60
0.546
-.1151237
.060885
age | -.0052162
.0009974
-5.23
0.000
-.0071711
-.0032613
_cons | -5.393658
.720245
-7.49
0.000
-6.805312
-3.982004
---------------------------------------------------------------------------/ln_p |
.5352553
.0951206
5.63
0.000
.3488225
.7216882
/ln_the |
1.790243
.1788498
10.01
0.000
1.439703
2.140782
---------------------------------------------------------------------------p |
1.707884
.1624549
1.417398
2.057904
1/p |
.5855198
.055695
.4859312
.7055184
theta |
5.990906
1.071472
4.219445
8.506084
----------------------------------------------------------------------------Likelihood-ratio test of theta0: chibar2(01) 
96.23 Probchibar2  0.000

b. The conditional hazard is plotted below. Its shape is very different from the Weibull
case, which is always upward sloping. (See Figure 22.3.) The hazard for the log-logistic model
with gamma heterogeneity has its maximum value near 30 weeks, and it falls off gradually
after that.

552

. stcurve, haz alpha1

.01
.005

Hazard

.015

Conditional Hazard (Log-logistic plus Gamma)

0

20

30

40
Months Until Arrest

553

60

80

c. The unconditional hazard is plotted below. While it also has a hump shape, the
maximum value of the unconditional hazard is around 12 – which is well below that for the
conditional hazard.
. stcurve, haz
(option unconditional assumed)

.006
.004

Hazard

.008

.01

Unconditional Hazard (Log-logistic plus Gamma)

0

12

20

40
Months Until Arrest

554

60

80

There is a final point worth making about this example that builds on the discussion in
Section 22.3.4. Below is the graph in Figure 22.4, reproduced for convenience. It is the
unconditional hazard for the Weibull model with gamma heterogeneity. While it differs
somewhat from the unconditional hazard for the log-logistic model with gamma heterogeneity,
it is practically very similar. Both hazards have sharp increases until about 12 months, and then
fall off to zero more gradually. In other words, when we study features of the distribution
Dt ∗i |x i  – which is what we can generally hope to identify – we get pretty similar findings. Yet
the hazards based on Dt ∗i |x i , v i  are very different. Recall that the hazard for Dt ∗i |x i , v i  in the
Weibull case is of the proportional hazard form while that for the log-logistic is not (which is
apparent when studying the plots of the conditional hazards). Given that the models fit the data
roughly equally well, and that they give similar shapes for the hazard of Dt ∗i |x i , it seems that
trying to decide whether the conditional hazard has a hump shape, as in part b, or is strictly
increasing, as in Figure 22.3, seems pretty hopeless.

555

.008
.006
.004

Hazard

.01

.012

Unconditional Hazard (Weibull plus Gamma)

0

12

20

40
Months Until Arrest

556

60

80

Incidentally, if we use a gamma distribution for Dt ∗i |x i , v i  with gamma heterogeneity, we
obtain an unconditional hazard very similar to the Weibull and log-logistic cases. The
conditional hazard is similar to the log-logistic case, and the log likelihood value is −1, 585. 09.

557

