Inference on Regressions with Interval Data on a Regressor or Outcome
Author(s): Charles F. Manski and Elie Tamer
Reviewed work(s):
Source: Econometrica, Vol. 70, No. 2 (Mar., 2002), pp. 519-546
Published by: The Econometric Society
Stable URL: http://www.jstor.org/stable/2692281 .
Accessed: 13/01/2013 09:57
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at .
http://www.jstor.org/page/info/about/policies/terms.jsp

.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact support@jstor.org.

.

The Econometric Society is collaborating with JSTOR to digitize, preserve and extend access to Econometrica.

http://www.jstor.org

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

Econometrica,

Vol. 70, No. 2 (March, 2002), 519-546

INFERENCE ON REGRESSIONS WITH INTERVAL DATA
ON A REGRESSOR OR OUTCOME
BY CHARLES F. MANSKI AND ELIE TAMER'

This paper examines inference on regressions when interval data are available on
one variable, the other variables being measured precisely. Let a population be characterized by a distribution P(y, x, v, v0, vl), where y E R', x E Rk, and the real variables
(v, v0, vl) satisfy v0 < v < vl. Let a random sample be drawn from P and the realizations of (y, x, v0, vl) be observed, but not those of v. The problem of interest may be
to infer E(ylx, v) or E(vlx). This analysis maintains Interval (I), Monotonicity (M), and
Mean Independence (MI) assumptions: (I) P(v0 < v < v,) = 1; (M) E(ylx, v) is monotone
in v; (MI) E(ylx, v, v0, v,) = E(ylx, v). No restrictions are imposed on the distribution
of the unobserved values of v within the observed intervals [v0, vj]. It is found that the
IMMI Assumptions alone imply simple nonparametric bounds on E(ylx, v) and E(vlx).
These assumptions invoked when y is binaiy and combined with a semiparametric binary
regression model yield an identification region for the parameters that may be estimated
consistently by a modified maximum scoie (MMS) method. The IMMI assumptions combined with a parametric model for E(ylx, v) or E(vlx) yield an identification region that
may be estimated consistently by a modified minimum-distance (MMD) method. Monte
Carlo methods are used to characterize the finite-sample performance of these estimators.
Empirical case studies are performed using interval wealth data in the Health and Retirement Study and interval income data in the Current Population Survey.
KEYWORDS:

Identification, interval data, regression.

1. INTRODUCTION

only interval data on variables that can, in principle, be measured more precisely. The interval data on wealth in the Health
and Retirement Study (HRS) provide a ready illustration (Juster and Suzman
(1995)). Let v denote a person's wealth. Under the HRS questionnaire protocol, a respondent is asked to report v. If he does not comply, the respondent is
then asked to report if wealth falls within a sequence of brackets. The HRS thus
yields a wealth interval [vo, v1] for each respondent. The interval is degenerate
when a respondent provides a point value of wealth, is an informative interval of
positive width when the respondent answers the subsequent bracket questions,
RESEARCHERS

OFTEN HAVE

and is the uninformative interval [-oo, oc] otherwise.
1 The research of Charles F. Manski was supported in part by NSF Grant SES-0001436. The
research of Elie Tamer was supported in part by an Alfred P. Sloan Doctoral Research Fellowship.
We have benefited from the opportunity to present this research in seminars at Harvard-MIT and at
the University of Virginia, as well as at the Winter 2000 annual meeting of the Econometric Society.
We have also benefited from the comments of two anonymous reviewers.

519

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

520

C. F. MANSKI AND E. TAMER

This paper examines inference on regressions when interval data are available
on one variable, the other variables being measured precisely. Let a population
be characterized by a probability distribution P(y, x, v, v0, v1). Here y c R1, x c
Rk, and the real variables (v, v0, v1) satisfy v0 < v < v1. Let a random sample be
drawn from P. Suppose that the realizations of (y, x, v0, v1) are observed, but
not those of v. The problem of interest may be to infer E(ylx, v) or, perhaps,
E(vlx). Observe that interval measurement of v represents a generalization of
the familiar problem of variable censoring, wherein v sometimes is measured
precisely and otherwise is not observed at all. In censoring problems, sometimes
vo = v, = v and otherwise [v0, v1] is the logical range of v.
Inference on E(ylx, v) poses a more complex problem than does inference on
E(vlx), so we focus mainly on the former case. Empirical studies confronting
interval regressor data often modify the problem of interest. A common practice
is to estimate E(ylx, v0, v1), which is identified by the available data, rather than
E(ylx, v), which may not be identified. Suppose that D distinct intervals [v0, v1]
appear in the data. Then researchers may estimate E(ylx, d), d = 1, . . . , D.
Another practice is to impute a value of v to each member of the population,
say v, and then estimate E(ylx, v). Often v1is chosen to be the midpoint of
the interval [v0, v1]. Researchers estimating E(ylx, d) or E(ylx, v-) may interpret their findings as evidence on E(ylx, v), but this interpretation is typically
heuristic rather than formal. Hsiao (1983) critiques these approaches as applied
to linear regression models.
We are concerned with inference on E(ylx, v) itself. Interval measurement of
v implies that the data alone do not identify E(ylx, v), but there is much scope
for inference by combining the data with various assumptions on the distribution
P(y, x, v, v0, v1). At one pole, some authors have assumed P(y, vlx, v0, v1) to lie
within a specified finite-dimensional family of distributions (e.g., Hsiao (1983)).
Such assumptions identify E(ylx, v) and enable classical maximum likelihood
estimation. At another pole, Horowitz and Manski (1998, 2000) have recently
performed a worst-case analysis of regressor-censoring problems. They maintain
no distributional assumptions and find that the data alone yield an informative
but complex nonparametric bound on E(ylx, v).
The present analysis begins with three basic nonparametric assumptions on
P(y, X, V, vo, v1):
Interval, Monotonicity, and Mean Independence (IMMI) Assumptions:
INTERVAL (I): P(v0 < v < v)

= 1.

(M): E(ylx, v) exists and is weakly increasing in v.
INDEPENDENCE (MI): E(ylx, v, v0, v1) = E(ylx, v).

MONOTONICITY
MEAN

Assumption I formalizes the presumption that the unobserved values of v lie in
the observed intervals [v0, v1]. Assumption M imposes a weak, easily understood,
shape restriction on E(ylx, v). Assumption MI asserts that observation of [v0, v1]

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

INTERVAL DATA

521

would be superfluous for prediction of y if v were observed. Assumption MI may
appear innocuous, but we feel that it warrants particular scrutiny in applications.
In the case of regressor-censoring, for example, Assumption MI holds if data on
v are missing at random but may not hold if censoring is endogenous. Observe
that the IMMI assumptions collectively place no restrictions on the distribution
of the unobserved values of v within the observed intervals [v0, v1].
Our analysis spans nonparametric, semiparametric, and parametric specifications of the regressions E(ylx, v) and E(vlx). Section 2 examines the implications of the IMMI Assumptions in the absence of other information. We find that
these assumptions imply simple nonparametric bounds on E(ylx, v) and E(vlx).
These bounds can be estimated consistently under weak regularity conditions.
Section 3 studies inference on E(ylx, v) when y is binary and the semiparametric binary regression model of Manski (1975, 1985) holds. We find that the
quantile independence assumption asserted in this model restricts the model
parameters to a convex set of observationally equivalent values. This identification region is a singleton if the support of (x, v0, v1) is sufficiently rich. We introduce a modified maximum score (MMS) estimator and prove that the set-valued
estimate converges almost surely to a subset of the identification region under
weak regularity conditions. The estimate converges to the identification region
under stronger regularity conditions.
Section 4 studies inference on parametric models for E(ylx, v) and E(vlx).
Focusing on E(ylx, v), we characterize the identification region in abstraction
and analyze its structure when the model has a monotone-index form. We then
show that the identification region solves a minimum-distance problem, and we
propose a sample analog of this problem as an estimator. As with the MMS
estimator, the set-valued modified minimum-distance (MMD) estimate converges
almost surely to a subset of the identification region under weak regularity conditions and converges to the identification region under stronger regularity conditions. We also briefly examine some alternatives to minimum distance estimation.
We sketch a class of modified method-of-moments estimators that converge to
supersets of the identification region. We consider least squares and maximum
likelihood estimators that treat the unobserved values of v as incidental parameters; in general, these are not consistent. Finally, we show how MMD estimation
may be applied to the problem of inference on E(vlx).
This paper does not develop asymptotic distribution theory for the estimators introduced in Sections 2 through 4. We do, however, present numerical and
empirical evidence to complement the theoretical analysis of identification and
consistency. Section 5 uses Monte Carlo methods to characterize the identification regions associated with the estimators and to assess their finite-sample performance. Section 6 presents empirical case studies using the HRS wealth data
and income data in the Current Population Survey (CPS).
Section 7 gives conclusions. An Appendix collects the proofs of the propositions and lemmas.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

522

C. F. MANSIZ AND E. TAMER
2. NONPARAMETRIC

INFERENCE

To begin, we determine the implications of the IMMI Assumptions in the
absence of other information. These assumptions imply the following sharp
bounds on E(ylx, v) and E(vlx):2
PROPOSITION

1: Let the IMMIAssumptions hold. Let V c R1. Then

sup E(ylx, v0, v1) < E(ylx, v = V) < inf E(ylx, v0, v1).

(1)

V1

> v

vvo

Let Assumption I hold. Then
E(v0Ix) < E(vlx) < E(v1lx).

(2)

In the absence of other information, these bounds are shaip.
The bounds (1) and (2) may be estimated consistently under weak regularity
conditions. Let a random sample of N observations of (y, x, v0, vl) be available. Let P(y, x, v0, v1) be sufficiently regular that there exist consistent estimates of [E(ylx, v0, v1), E(volx), E(vllx)], with the estimate of E(ylx, v0, vl)
uniformly consistent on the support of (x, v0, v1). Let EN(YIX, VO,V1),EN(VOIX),
and EN(V1lx) denote such estimates. Then consistent estimates of the bounds
are [supvl<vEN (YIX, vo, vl), infv>JvEN (YIX, v, v1)] and [EN(VOIX), EN (Vl IX)].
Observe that bound (1) reduces to a point if v sometimes is measured precisely. Suppose that the support of (x, v0, v1) contains the point (x, v0 = v, = V).
Then the lower and upper bounds in (1) coincide, their common value being
E(y Ix, v0 = v, = V). Hence E(y Ix, v = V) is identified.
Observe that the IMMI Assumptions form a testable hypothesis. Suppose that
the lower bound in (1) exceeds the upper bound. Then at least one of the IMMI
Assumptions must be incorrect. This suggests the operational test: Reject the
IMMI Assumptions if there exists a V c R1 such that supv<v EN(y Ix, v0, v1)inf,v0> EN(YIX, VO,V1) > JN, where JN > 0 is a suitably chosen critical value.
Bound (2) is a straightforward extension of the nonparametric outcomecensoring bound introduced in Manski (1989). This bound reduces to a point in
the limit case where v is measured precisely with probability one. The bound
is informative if the intervals [v0, v1] are bounded with probability one and are
proper subsets of the logical range of v with positive probability.
3.

INFERENCE ON A SEMIPARAMETRIC

BINARY REGRESSION

MODEL

We next study inference on the parameters of a semiparametric binary regression model. In this section, y is a binary outcome whose value is determined by
2
In the statement of Proposition 1 and elsewhere in this paper, it should be understood that the
regressions E(ylx, vo, vl), E(vojx), and E(vl x) are to be evaluated only on the supports of their
conditioning variables.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

523

INTERVAL DATA
I

_
. ....

M {d
..

_~Y= .

- deIpreiY=OY1
.. . . ........
- xP- bv,

- xP-6bv
FIGURE

---tsS~

- xf3-6gv
1

the familiar threshold-crossing condition y = 1[x(3 +8v + E > 0], where (/(3,8) are
parametersand e varies across the population. Thus
(3)

E(ylx, V)=P(Y =lIx, V)=P(X,(3+NV-+ e > OIx, V).

Interval measurement of v makes the threshold-crossing model incomplete.
Whereas each value of (x, v, e) predicts a unique value for the outcome y, some
values of (x, v0, v1, e) do not predict a unique outcome. Figure 1, which assumes
that 8 > 0, shows the problem.
In what follows, we maintain these assumptions:
Binaty Regression (SBR) Assumptions:
Semiparoarnetric
ASSUMPTION SBR-1: Fora specified acz (0, 1), qO(E Ix, v)
ASSUMPTION SBR-2: P(EIx, v, v0, v) = P(E Ix,v).
ASSUMPTION SBR-3: 8 > 0.

=

0. P(E < OIx, v) = az.

the
quantile-independence
condition underlying
the analSBR-1lis.............
/~Assumption
~~~~~~~~~~~~~~~~~~~~~~~~~~~..
.]
.............
.
ysis of Manski (1985). If the vector x contains a constant component, the part
of SBR-1 setting the o-quantile to zero simply normalizes location. Assumption
SBR-2 implies Assumption MI; SBR-2 and equation (3) yield
(4)

P(y=lIx,v,v0,v1)==P(x/3?8v?E>OIx,v,v0,v1)
= P(x/3+8v+E>

OIx,v) = P(y

=

lix, v).

Assumption SBR-3 asserts a form of monotonicity but does not imply Assumption M. If Assumption SBR-2 were strengthened to assert that P(E Ix, v, v0, v1) =
P(E Ix),then SBR-3 would imply Assumption M. Our analysis, however, does not
require this strengthened form of SBR-2.
In Section 3.1 we study identification of the parameters (/(3,8) under Assumpv
tions SBR-1 through SBR-3. The threshold-crossing condition is invariant to the
scale of the parameters, so we set 8 = 1 as a normalization. This done, the focus
of attention becomes identification of ,(3.
3.1. Identification
We begin by recalling the inequality relating /(3to P(y = lJx, v) implied by
Assumption SBR-1, and its consequence for identification if v were observed (see
Manski (1985)). Under SBR-1,

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

524

C. F. MANSKI AND E. TAMER

Let b c Rk. If v were observed, (5) would imply that : is identified relative to b
(i.e., observationally distinct from b) if and only if
(6)

P[(x, v): xb+v < 0 < x/3+vUx/3+v

< 0 < xb+v] >0.

Now consider the identification problem with (vo, v1) observed rather than v.
Proposition 2 gives the basic finding.
PROPOSITION 2: Let Assumption I and Assumptions SBR-1 through SBR-3
hold. Let b c Rk. Let

(7)

T(b) _ [(x, vo, vj): xb+v, < 0 < x/34-?voUx/3+v, < 0 < xb+vo].

Then /3 is identified relative to b if and only if P[T(b)] > 0.
Proposition 2 implies that the set of parameter values observationally indistinguishable from /3 is
(8)

B*-{b

c

Rk:

P[T(b)] = 0}.

The Corollary below characterizes this set, giving sufficient conditions for identification of the signs of elements of /3 and for identification of /3 itself.
COROLLARY:
(a) B* is nonempty and convex.
(b) Let k c (1,..., K) and X-k
Assume that
(Xl,... ,Xk1,
XK).
Xk+l,...
v1)
has
unbounded
a.e.
v1).
Then
support,
P(xklx-k, vo,
(X-k, vo,
Pk > 0 X= (bk >
O,Vb c B*) and Pk < 0, X (bk < 0, Vb C B*).
(c) Assume that there exists no proper linear subspace of RK havingprobabilityone
under P(x). Assume that P(ao < vo < v1 < a,Ix) > Ofor all (ao, al) E R2 such that
aO < a,, a. e. x. Then B* =
(d) Assume that there exists no proper linear subspace of RK+1 having probability
one under P(x, v). Assume that P(vo = v1) > 0. Assume that there exists a k E
(1, ... , K) such that P(xklx_k, vO= v1) has support R1, a.e. (X-k, vO= v1). Then
B* = /3.

3.2. A Modified Maximum Score Estimator
We now introduce a tractable estimator of the set B*. This estimator, which
modifies the maximum score method to cope with interval data, is obtained in
three steps. We first present a lemma showing that the elements of B* are the
solutions to a certain maximization problem. We next propose a sample analog
of this maximization problem as an estimator for B*. We then slightly weaken
the estimator to encompass approximate solutions of the sample maximization
problem. The main finding of this section is Proposition 3, which analyzes the
almost sure convergence of the estimate.
Lemma 1 forms the basis for the estimator. We adopt the convention
sgn(0) = -1 here.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

525

INTERVAL DATA
LEMMA 1: Let A(X, vo, v1) -[P(y
the problem
max S(b, A),
(9)

=

lIx, vo, v1) > 1 - a]. Every b E B* solves

beRk

where
S(b,A) _f[P(y=

1jx,vo0,v)-(1-a)]

x{A(x, vo, v1) sgn(xb + v1) + [l-A(x,

vo, v1)]

*sgn(xb+ vo)} dP(x, vo, v1).
No b , B* solves this problem if P[(x, vo, v1): P(y

=

lx, vo, v1) = (1-a)]

= 0.

Lemma 1 suggests estimation of B* by the sample analog of (9). Let a random sample (yi, xi, vOi,v1i), i = 1,... , N, be available. Let PN(Y = l1x VO,
V1)
be a consistent estimate of P(y = lix, vo, v1). Let AN(X, VO,V1) 1[PN(Y =
lIx, vo, v1) > 1 - a]. Let B c RK be the parameter space. The analogy principle
suggests the sample maximization problem
(10)
max SN(b, AN),
beB

where B c RK is the parameter space and
SN(b, AN)N

1 N
-

-(1-

a)]

x{AN (Xi, voi, v1i) . sgn(xib + v1i)+ [1 -

AN(Xi, voi, VIM)]

.sgn(xib + voi)}.
This estimator extends the maximum score method to settings with interval data
on v. If v were measured precisely, we would have v = vo = v1 and (10) would
reduce to the maximum score estimator
(11)

1
max -

N

l[yi- (1- a)]sgn(xib + vi).

bENi=l

Proposition 3 analyzes the almost sure convergence of a slightly weakened form
of estimator (10), one in which the estimate BN includes all feasible parameter
values that approximately maximize SN(., AN). Formally, the estimate is
(12)
BN - [b E B: SN(b, AN) > maxSN(c,
AN) - EN],
cEB

for a specified EN > 0. The reason for weakening (10) in this manner is that
B* may be set-valued, which implies a need for care when defining and proving
convergence.
Let p(BN, B*) measure the distance from BN to B* and let p(B*, BN) measure
the distance from B* to BN, as follows:
(13a)
p(BN,B*)~ sup inf IbN- b*I
bNEBN b*EB*

(13b)

p(B*,RN)_

sup inf IbN-b*I
b*eB* bNEBN

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

526

C. F. MANSKI AND E. TAMER

The function p(., ) defined in (13) is an ordinary metric when applied to sets containing one element each; if BN = b,2and B* = b*, then p(BN, B*) = p(B*, BN) =
IbN - b*I. In general, however, p(., ) is asymmetric in its arguments. The quantity p(BN, B*) is small if every element of BN is close to some element of B*,
while p(B*, BN) is small if every element of B* is close to some element of BN.
Proposition 3 gives conditions implying that p(BN, B*) and p(B*, BN) almost
surely converge to zero as N -- oc. Given some regularity on P(x, v), it is relatively easy to show that p(BN, B*) as 0- It suffices for PN(Y = llx, vo, v1)
to be a consistent estimate of P(y = llx, vo, v1) and for the analyst to use a
sequence {EN} that converges to zero. It is a more delicate matter to show that
p(B*, BN) >a.s. 0- We prove here that p(B*, BN) _>a.S 0 if {EN} convergesto zero
no faster than SN(., AN) converges to S(., A).
PROPOSITION 3: Let Assumption I and Assumptions SBR-1 through SBR-3
hold. Let the parameterspace B be compact, with /3 c B. Assume that P[(x, vo, vl):
P(y = 1ix, v0, vl) = (1 - a)] = 0. Assume that for each b c B, the probability
measure of xb + v is dominated by Lebesgue measure, with bounded density. Let
N -> o. Suppose that PN(Y = lix, VO, V1) _>a.s P(y = llx, v0, v1), a.e. (x. v0, v1).
Let EN >a.s. 0?

(a) Thenp(BN, B*)
(b) Let

{SUpbeB

a.s.0

ISN(b, AN) -

S(b, A)I}/EN _>a.s. 0. Then p(B*,

BN)

>a.s.

0?

Part (a) of Proposition 3 suffices to show that the MMS estimator is consistent
when /3 is identified. Part (b) comes into play only when the identification region
B* is set-valued.3
4.

INFERENCE ON PARAMETRIC

REGRESSION

MODELS

We now turn to the problem of inference on parametric regression models.
Sections 4.1 through 4.4 examine inference on E(ylx, v) when it is known that
(14)

E(ylx, v) = f(x, v, y),

where f (.,, ) is a specified function and y is a parameter vector known to lie
in a finite-dimensional parameter space C. We maintain Assumptions I and MI
and we enforce Assumption M through the following:
ParametricRegressionAssumption:
ASSUMPTION

PR: For each value of x and each c c C, f (x, v, c) is weakly

increasing in v.
3 The rate of uniform convergence of SN(, AN) to S(., A) is unknown at present. Determination
of this rate is complex for two reasons. One is that SN(, AN) is a step function on B. The other is
that AN is a step-function transformation of the nonparametric estimate PN(y = llx, vO,v,).

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

527

INTERVAL DATA

The analysis below applies to all functions f satisfying Assumption PR. For example, some components of x may be instrumental variables in the sense of mean
independence; then the function f (., v, y) is specified so as not to vary with these
x components. More specific findings are achievable if f has a simple algebraic
structure. Accordingly, we sometimes assume that f has the monotone-index
form
f (x, v, y) = F(x/8 + 8v),
(15)
where F(.): R1 -> R1 is a strictly increasing function, ,3 c RK, and 8 > 0; thus
y = (,3, ) here.
Section 4.1 analyzes identification of the model parameters. Section 4.2 introduces a modified minimum-distance estimator that follows directly from the
findings on identification. We characterize the set of observationally equivalent
parameter values as the solutions to a class of extremum problems and propose a
sample analog of one such problem as an estimator. Section 4.3 sketches a class
of modified method-of-moments estimators that also follow from the findings on
identification. Section 4.4 considers estimators that treat the unobserved values
of v as incidental parameters.
In Section 4.5 we adapt the findings to inference on E(vlx).
4.1. Identification
Proposition 4 gives the basic finding on identification of parametric regression
models.
PROPOSITION

(16)

V(c)

4: Let Assumptions I, MI, and PR hold. Let c c C. Let
[(x, vo, v1): f(x, v1, c)
<E(yIx,vo,vl)UE(yIx,vo,vl)

<f(x,vo,c)].

Then y is identified relative to c if and only if P[V(c)] > 0.
Proposition 4 implies that the set of parameter values observationally equivalent to y is
(17)

C* _ {c C C: P[V(c)]

= 0}.

We are not able to constructively characterize C* in general, but we are able to
do so if f has the monotone-index form. The Corollary below characterizes this
set, giving a sufficient condition for identification of y.
COROLLARY:

Let f have the monotone-indexform. Then:

(a) C* is nonempty and convex.
(b) Let k c (1, ... , K). Assume that P(xk IXk, vo, vl) has unbounded support,
a.e. (X-k, vO,v1) . Then Pk is identified.
(c) Assume that there exists no proper linear subspace of Rk+l having probability
one under P(x, v). Assume that P(vo = v1) > 0. Then C* = y.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

528

C. F. MANSKI AND E. TAMER

4.2. A Modified Minimum-Distance Estimator

We present a lemma showing that the elements of C* are the solutions to
a large class of minimization problems. We next suggest the sample analogs of
these problems as estimators for C*. We then focus on a slightly weakened form
of one such estimator and show that it is consistent.
Here is the lemma that forms the basis for the estimator.
LEMMA 2: Let -71(x,vo, v1) _ E(y x, vo, v1), g1(c, x, vo, v1)
1[f (x, v1, c) <
rj(x, vo, v1)], and go(c, x, vo, vl)
1[-r(x, vo, vl) < f(x, vo, c)]. Let w(.): R2 ->
[0, ox] be any function such that w(s, t) = 0 if s = t and w(s, t) > 0 othetwise. Then
evety c E C* solves the problem

min Q(c, 71)

(18)

ceC

where
Q(c,-q)=_gl(C,X,vo,vl)w[f(x,vl,c),-q(x,vo,vl)I
+gO(c, x, vo, VJw[f

No c

'

(x,vo,

C),(x,

vo, vj] dP(x,vo

vl).

C* solves this problem.

Lemma 2 suggests estimation of C* by a sample analog of (18). Let a random
sample (yi, xi, voi, v1i), i = 1,... , N, be available. Let T1N(X,vo, vl) be a consistent estimate of qr(x,vo, v1). Let gNl(C, x, vO, v1) 1[f (x, v1, c) < T1N(X,vO,V1)]
and define gNO(C, x, vO,v1) analogously. The analogy principle suggests the sample minimization problem
(19)

minQNQ(C,
7qN)
cCE

where

1N
QN(C, 7qN)- N

E9NlC,

xi, VOi, VIi)W[f

(Xi, Vli,

+ 9NO(C, Xi, VOi, V1JW If (xi, VOi,c),

c), qNN(Xi,VOi,

71'N
(Xi,

VIM]

VOi, V I I

The estimator (19) extends the minimum distance method to settings with interval data on v. Let w(s, t) = (s _ t)2 and suppose that v is measured precisely, so
v = vO= v1. Then (19) reduces to the minimum distance estimator
(20)

1

N

min N [f

(xi, vi, c)

-

N(xi,

v)]

Proposition 5 analyzes the almost sure convergence of a slightly weakened
form of estimator (19), in which the estimate CN includes all feasible parameter
values that approximately minimize QN(', ?N). As with the MMS estimator, the

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

529

INTERVAL DATA

reason for weakening (19) in this manner is that C* may be set-valued. Formally,
the estimate is
(21)

CN

[C E C: QN(C, TiN)

<mindcC

QN(d,

TN)

? EN],

for a specified EN > 0. We restrict attention to the case in which w(s, t) = (s - t)2.
Proposition 5 gives conditions implying that P(CN, C*) and p(C*, CN) almost
surely converge to zero as N -? oo. The structure of this proposition parallels that
of Proposition 3. Maintaining modest regularity conditions, Part (a) shows that
P(CN, C*) >a.s. 0 if TqN(X, vO, v1) is a consistent estimate of rq(x,vo, vl) and if the
analyst uses a sequence {EN} that converges to zero. This suffices to prove that
the MMD estimator is consistent when y is point identified. Part (b) shows that
P(C*, CN) >a.s. 0 if {EN} converges to zero no faster than QN(, vqN) converges
to Q(., rq).4
PROPOSITION 5: Let Assumptions I, MI, and PR hold. Let the parameter space
C be compact, with y E C. Let w(s, t) = (s - t)2. For each value of (x, v), let
f (x, v, ) be a continuous function on C. Assume there exists an integrablefunction
0: Rk+2 -? R1 that dominates

h(c, x, vo, vl)=

g(c,

x, vo, vl)[f(x,

vl, c)-(x,

vo, v)]2

+g0(C, x, v0, v1)[f(x, v0, c)- 7(x, v0,
that is, Ih(c, x, vo, vl) < +(x, vo, v1) for all (c, x, vo, v1), and f +(x, vo, vl) dP is
finite. Let N -> oo. Suppose that TlN(X, vO,v1) _>a.s. q(x, vo, v1), a.e. (x, vo, vl). Let
EN -a.s.

O?

(a) ThenP(CN, C*) a.s. 0
(b) Let {sup,c I QN(C, TN)

-

Q(C, 0)

}/EN

a.s.

0. Then p(C*,

CN)

a.s.

0

It would clearly be of interest to go beyond Proposition 5 to develop asymptotic
distribution theory for the MMD estimator. This appears to be a formidable
challenge in the general case when y is not point identified. Indeed one would
first need to generalize conventional notions of rate of convergence and limiting
distributions to cover problems in which set-valued estimates converge to setvalued estimands.
The situation when parameters are identified appears to depend critically on
how identification is achieved. Consider the Corollary to Proposition 4, which
gives two sufficient conditions for identification of parameters of monotone-index
models. Part (b) of the Corollary showed that fk is identified "at infinity" if
Xk has unbounded support. We conjecture that the rate of convergence of the
MMD estimate Of fk depends on the tail behavior of the distribution of Xk in
4 We have been able to show that QN (-, fN)
converges to Q(., -q) at rate N if 7qN is a kernel
estimate with appropriate properties and if sufficiently strong regularity conditions are maintained.
The statement and proof of this result are long and not especially enlightening, so we do not include
this material here.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

530

C. F. MANSKI AND E. TAMER

this case. Part (c) of the Corollary showed that the entire parameter vector y is
identified if v is sometimes measured precisely; that is, if P(vO = v1) > 0. In this
case, least squares estimation on the subsample where v is measured precisely
has the classical asymptotic properties of N rate of convergence and a limiting
normal distribution. We conjecture that an appropriately smoothed version of
the MMD estimator behaves similarly.
4.3. Modified Method-of-MomentsEstimators
The modified minimum-distance method is not the only approach to estimation that follows from Proposition 4. The proposition may also be applied to
generate a class of modified method-of-moments (MMM) estimators. We sketch
this approach here, but do not study it in depth.
MMM estimation is based on this lemma:
LEMMA 3: Let J be a positive integer.Let w(.): RK?2 -? (0, oo)J be a measurable
w(x, v0, v1)[yfinction mapping (x, vo, v1) into a positive J-vector.Let z1(c)
f (x, vj, c)] dP(x, vo, v1) for j = 0, 1. Let A be a J x J positive definite matrix. Then
every c E C* solves the problem

f

(22)

min{1[,A1(c) > O].A1(c) + [AO(c) < O].AO(c)}
cec

xA {1[A1(c) > 0] .A1(c) + 1[,AO(c)< 0] .AO(c)}.
Here 1[,A1(c)> 0] is the J x J diagonal matrix whose (j, j) element equals one
if the jth component of Al (c) is positive and equals zero otherwise. The matrix
1[,AO(c)< 0] is defined analogously.
Lemma 3 suggests estimation by the sample analog of (22), namely

(23)

min{lI
[AN1(C)
ceC

> 0] * AN1 (C) + 1LANO(C)

x A { 1 AN1 (C)

< 0]

.

ANO(C)}'

> 0] * ANI (C) + 1[ANO (C) < 0] * ANO (C) }

where ANJ(c) _ (1/N) Ei=1...,N W(Xi, VOi, Vli)[Yi - f (Xi, Vji, C)] j = 0, 1. The
estimator (23) extends the method of moments to settings with interval data on v.
If v were measured precisely, we would have v = vo = v1 and (23) would reduce
to the method of moments estimator
(24)

minN
ceC

where AN(C)

N(c)
=ANl(C)

A

'AN(c),

= ANO(C)-

MMM estimation differs from MMD estimation in positive and negative
respects. An advantageous feature of the MMM approach is that the estimator minimizes a continuous function of unconditional sample moments, whereas
the MMD distance method requires nonparametric estimation of E(ylx, vo, v1)

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

531

INTERVAL DATA

en route to estimation of C*. This advantage must be weighed against an unappealing feature of MMM estimation. That is, the estimator does not use the full
identifying power of Proposition 4.
Let C** denote the solutions to problem (22). Lemma 3 shows that C* c C-*,
but not that C* = C**. In general, C* is a proper subset of C**, implying that
problem (22) does not exploit the full identifying power of Proposition 4. How
large C** is relative to C* depends on the function w(.).
It may be possible to develop a variation of MMM that asymptotically does
exploit the full power of Proposition 4. The idea, borrowed from the literature
on conventional method-of-moments estimation, would be to replace the fixed
J-vector w(.) with an appropriate sequence [wN(.), N = 1, . . . , oo] of nested vectors of increasing lengths [JN, N = 1, . . . , oo]. Whereas the conventional reason
for use of such a sequence is to achieve asymptotic efficiency of estimation of a
point-identified parameter, here the objective would be to achieve maximal identification of a possibly interval-identified parameter.
4.4. Estimators Treatingv as Incidental Parameters
To close this examination of estimators for parametric regression models, we
consider methods that treat the unobserved values of v as incidental parameters. If v were observed, one would be able to estimate y using a least squares
criterion. With only interval data on v available, one might consider treating
the unobserved values of v as parameters and solving the extended least square
problem

(25)

f[y

mcCimn
,.
N

viE[vio, vii

-

-(xi,

Vi, C)]2,

i=1

or some weighted version thereof. If y is binary, one might also consider the
maximum likelihood estimator
(26)

mcCaCx
.

Yilog[f(xi, vi, c)] + (1 - yi) log[1 -f

E

-

N

ViE Vio, Vil]

(xi, vi, c)].

i=1

It is complex to analyze these estimators applied to general parametric regression models, but simple to do so when y is binary and f has the monotone-index
form. Then the least squares and maximum likelihood estimators become
(27)

bmB,md>O

iViE[Vio,
Vii] i,

, N

N

yi-

INi

(28)

-

bmEaBxO
viE[vio,

vil i1,

.

N

N

F(xib + dvi)]2,

=1

Ey

log[F(xib + dvi)] + (1 - yi) log[1 - F(xib + dvi)].

i=1

Inspection of (27) and (28) shows that, given any value of b and any d > 0, both
criterion functions are optimized by setting vi = v1i if yi = 1 and vi = voi otherwise.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

532

C. F. MANSKI AND E. TAMER

Thus, these criterion functions have the concentrated forms
l

(29)

bmmBd>ON

N

2+l2

Lyi[l - F(xib + dv1i)]2 (1- yi) F(xib + dvoi)2,
i=1
lN

(30)

max
bcB,d>N

-

yi log[F(xib + dv1i)]+ (1 - yi) log[1 - F(xib + dvoi)].

Unfortunately, solution of each concentrated extremum problem generically
yields an inconsistent estimate of (/3, 8), as shown in Lemma 4.
LEMMA 4: Let y be binary and assume the monotone-index model. Then the
concentrated least squares and maximum likelihood estimators (29) and (30) are
genericallyinconsistent.

Thus here, as in various other estimation problems where the number of incidental parameters grows with sample size, maximum likelihood and least squares
methods do not enjoy their good classical properties. See Lancaster (2000) for a
review of the literature.
4.5. IntervalMeasurementof Outcomes
In this section we examine inference on E(vlx) when it is known that
(31)

E(vlx) = f'(x, y),

where f'(., ) is a specified function and y is a parameter vector known to lie
in a finite-dimensional parameter space C. The analysis closely parallels that of
Sections 4.1 and 4.2, SOwe may be brief.
Proposition 6 gives the basic finding on identification.
PROPOSITION

(32)

6: Let Assumption I and equation (31) hold. Let c

E

C. Let

V'(c) _ [x: f'(x, c) < E(vOlX)UE(v1 Jx) < f'(x, c)].

Then y is identified relative to c if and only if P[V'(c)] > 0.
Proposition 6 is analogous to Proposition 4; hence we do not provide a separate
proof.
Arguments akin to those yielding the MMD estimator of Section 4.2 yield
an MMD estimator in the present case as well. Let EN(vOlx) and EN(V1lx) be
consistent estimates of E(volx) and E(v1lx). Then the estimator is

(33)

cec

C) < EN (Vo Ix)]['(Xi,
C)-IEN

L 1[f'(Xi,
+ 1[ f'(xi, c) > EN(Vl Xi)][f '(Xi, c)

(Vo

x)]2

-

-

EN(Vl Xi)]2.

A consistency theorem analogous to Proposition 5 can be developed here. We
omit the formalities.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

533

INTERVAL DATA

5.

MONTE CARLO EVIDENCE ON THE FINITE-SAMPLE
BEHAVIOR OF THE ESTIMATORS

This section presents Monte Carlo evidence on the finite-sample behavior of
the MMS and MMD estimators for E(ylx, v). Section 5.1 describes the design.
Section 5.2 presents the findings.
5.1. Design
We use linear and binary response models to generate outcome data, as
follows:
LINEAR MODEL (LM): y = yo + y,1v+ V2X+ e.
BINARY RESPONSE MODEL (BRM): y = 1[yo + y1V + y2X + e

>

0].

We specify a value for the parameter vector y -(yo, yl, y2) and a distribution
for the random vector (v, x, e). The experiments reported here set (yo, yl, y2) =
(1, 1, -1) and make (v, x, e) statistically independent. We set e - N(0, 1) and
specify two alternative distributions for (v, x). One group of experiments has
v - N(O, 2) and x - N(1, 4), while the other has v - U[-2, 3] and x - U[0, 5].
We generate interval data on v by rounding v down and up to the nearest
integer. That is, we set v0 int(v) and v1 _int(v) + 1. For example, if v = 1.4,
then v0 = 1 and v, = 2. Integer rounding is a common phenomenon, so we think
this element of the design is particularlyrelevant to practice. The intervals [v0, v1]
yielded by integer rounding all have width one. This is 1/ 2 of the standard
deviation of v under the normal design and 1/5 of the width of the support of v
under the uniform design.
In the experiments assuming normal distributions, P(x(v0, v1) has unbounded
support; hence the Corollary to Proposition 4 implies that y2 is point identified
when a parametric model is asserted. This is not the case in the experiments
assuming uniform distributions. The rounding process for generating v0 and v1
from v does not satisfy the sufficient conditions for point identification given in
the Corollaries to Propositions 2 and 4. Thus, with the exception of y2 in the
normal design, we do not have reason to expect that the parameters are point
identified.
The remaining element of the design is sample size. We report findings for
samples of size N = 200 and N = 800. In each case, we draw 100 independent
pseudo-samples and report various features of the empirical distributions of the
estimates. We also draw a single sample of size N = 20,000 and report the estimates obtained. These large-sample estimates should approximate the identification regions of the parameters.
We use throughout a multivariate version of the Naradaya-Watson kernel
smoother to estimate the regression E(ylx, v0, v1) nonparametrically. This done,
the MMS estimates are obtained using a modified version of the MSCORE program described in Manski and Thompson (1986). A scale normalization is needed
for MMS estimation, so we set yi = 1. The MMD estimates are obtained using
the method of simulated annealing.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

534

C. F. MANSKI AND E. TAMER

5.2. Findings
Tables I and II report the findings under the normal and uniform designs
respectively. Each ently in these tables has two rows. We display in brackets the
pseudo-sample means of the interval parameter estimates. Below these means we
display in parentheses 95 percent confidence regions for the estimates, computed
as follows. For k = 0, 1, 2, let {[L/rj, Uk;], j = 1, . . . , 100} be the pseudo-sample
interval estimates of parameter Yk in a given design using a specified estimation
method. We find the shortest interval that covers 95 of these 100 interval estimates and report this as the confidence region.
Consider first the findings for the single sample of size 20,000. Sampling imprecision should be a minor concern in a sample of such large size, so the interval estimates should approximate the identification regions for the parameters.
Observe that, under the normal design of Table I, the MMD estimates for 'Y2
are extremely narrow intervals containing the true value -1; the intervals are
[-.99, -1.00] for the linear model and [-1.01, -.99] for the binary response
model. This reflects the point identification of Y2 shown in the Corollary to
Proposition 4.
It is evident that the parameters are otherwise not point identified. Nevertheless, the interval estimates indicate that all of the identification regions are informative to a greater or lesser degree. All of the intervals contain the associated
true parameter values. Under the normal design, every interval is narrow, the
widest being [.94, 1.08] in the case of the MMS estimate of 'Yl Under the uniform
TABLE I
MONTE CARLO ANALYSIS-NORMAL

DESIGN

MMS
BRM

MMD
LM

BRM

N = 200
Yo

Yi
72

[.46, 2.38]
(.03, 3.77)
1
[-2.11, -.68]
(-3.33, -.39)

[190, 1.08]
(.73, 1.25)
[.97, 1.07]
(.88, 1.19)
[-1.03, -.95]
(-1.09, -.88)

[.81, 1.48]
(.36, 2.17)
[.93, 1.53]
(.58, 3.09)
[-1.45, -.90]
(-2.49.-.57)

[.91, 1.03]
(.80, 1.14)
[.96, 1.02]
(.91, 1.09)
[-1.02, -.99]
(-1.04, -.94)

[.83, 1.21]
(.54, 1.56)
[.87, 1.15]
(.69, 1.43)
[-1.16, -.88]
(-1.46, -.69)

N = 800
Yo

7,
72

[.99, 1.94]
(.48, 2.58)
1
[-1.45, -.82]
(-1.96, -.58)

N = 20000
YO

7,
72

[.94, 1.08]
1
[-1.03,-.97]

[.93, 1.04]
[.95,1.03]
[-.99, -1.00]

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

[.95, 1.07]
[.96,1.06]
[-1.01, -.99]

535

INTERVAL DATA
TABLE II
MONTE CARLO ANALYSIS-UNIFORM

DESIGN

MMS
BRM

N = 200
YO
Yi
72

N = 800
Yo
Yi
72

N = 20000
Yo
Yi
72

[.152, 2.91]
(.01, 4.68)
1

MMD
LM

BRM

[179, 1.08]
(48, 1.38)
[.98, 1.11]
(.87, 1.23)
[-1.03,-.93]
(-1.14, -.82)

[183, 1.22]
(.35, 2.09)
[1.05, 1.34]
(.73, 1.91)
[-1.21,-.96]
(-1.77, -.68)

[-1.74, -.701]
(-2.17, -0.5)

[.84, 1.01]
(.61, 1.22)
[.99, 1.068]
(.92, 1.17)
[-1.01, -.84]
(-1.08, -.87)

[.80, .94]
(.53, 1.35)
[.95, 1.07]
(.81, 1.29)
[-1.01, -.90]
(-1.18, -.75)

[.56, 1.47]
1
[-1.42, -.71]

[.58, 1.31]
[.97, 1.27]
[-1.14, -.84]

[.68, 1.24]
[.85, 1.14]
[-1.09, -.84]

[-2.24,-.62]
(-3.34, -.35)
[.32, 2.34]
(.02,3.28)
1

design, with its less informative bounded distribution of (v, x), the intervals are
correspondingly less informative. Here the narrowest interval is [-1.09, -.84] for
the MMD-BRM estimate of Y2 and the widest is [.56,1.47] for the MMS estimate of yo.
Now consider the findings for samples of sizes 200 and 800. The confidence
regions show the combined effects of incomplete identification and sampling
imprecision. Two patterns warrant comment. First compare the MMS and MMD
estimates of the binary response model. The MMS method presumes weaker
prior information than does the MMD method, so it is reasonable to expect
wider confidence regions in the former case than the latter. This is what we find.
Second compare the MMD estimates of the linear and binary response models.
We observe the outcome YO+ Ylv + y2x +e in the former case, but only the
indicator function 1[yO+ yIv + y2x+ e > 0] in the latter. Hence it is reasonable
to expect narrower confidence regions in the former case than the latter. This is
what we find.
The remaining question concerns the effect of sample size on the estimates. It
is clear that, for the two designs examined here, samples of size 200 yield reasonably informative MMD estimates and that samples of size 800 yield reasonably informative MMS estimates. Our findings do not give clear indications of
the rates of convergence of these interval estimates. To shed light on this matter
would require a more extensive set of Monte Carlo experiments than those that
we have performed.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

536

C. F. MANSKI AND E. TAMER

6.

ILLUSTRATIVE EMPIRICAL APPLICATIONS

This section presents two illustrative empirical applications of the estimators.
In Section 6.1 we use data from the Health and Retirement Study to estimate
regressions of smoking behavior on wealth and other covariates. In Section 6.2
we use data from the Current Population Survey to estimate regressions of home
ownership on income and other covariates. Some of the HRS wealth data are
point values and some are intervals of varying widths. The CPS income data
are all intervals of fixed widths. Thus, the two applications illustrate use of the
MMS and MMD estimators in settings with different patterns of interval data
measurement.

6.1. Smoking and Wealthin the HRS
In our application using HRS data, y = 1 if a person reports that he or she
is a nonsmoker, and y = 0 if a smoker. The interval-measured covariate v is a
person's wealth. The other covariates x are age, years of schooling, and an index
of body-mass (weight divided by height squared).
The data concern 6898 respondents to the 1994 wave of the HRS. Respondents were asked to provide point values for their debt and for their holdings of
various assets: housing, vehicles, farm/business equity, IRAs, stocks, bonds, bank
accounts, and certificate of deposits. The sum of debt and these assets constitutes wealth. Respondents who did not know or who otherwise refused to report
a point value for some component of wealth were asked whether its value lies
within a sequence of brackets. The outcome was that point measurements of
wealth were obtained from 4216 respondents, informative interval measurements
from 2041, and uninformative intervals from the remaining 641. When the data
obtained from a respondent did not reveal a lower or upper bound on wealth,
we set these bounds at -109 dollars or 109 dollars respectively.
Table III presents descriptive statistics for the data on (y, x, vo, v1). The table
also describes wealth imputations included in the HRS public use file. These

TABLE III
DESCRIPTIVE

Mean
Median
St Dev
Minimum
Maximum

STATISTICS

Point Wealth,
with Imputation

Min Wealth
(vo)

731.63
111.93
2219.57
-4565.23
15230

-88686.34
67.45
284627.88
-1000000
15230

FOR THE HRS DATAa

Max Wealth
(v1)

99363.99
131
298829.63
-3883
1000000

Smoking

Age in
Years

School
Years

.801
1
.212
0
1

62.85
62
4.96
87
23

12.23
12
3.23
0
17

Body
Mass

28.49
26.52
13.71
14.91
194.25

aWealth is in thousands of dollars. Smoking = 0 if the respondent smokes, 1 otherwise. Body Mass is weight in lbs multiplied by
703 divided by height in inches squared.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

537

INTERVAL DATA
TABLE IV
MAXIMUM

LIKELIHOOD

ESTIMATES

OF LOGIT

Using Imputations (N = 6898)
Estimate

Confidence Region

SMOKING

MODEL

ON HRS DATA

Without Imputations (N = 4216)
Estimate

Confidence Region

(1.05, 1.89)

Wealth

0.0452

(.0179, .0726)

1.47

Age

0.0425

(.0313, .0538)

0.041

(.026,.056)

Schooling

0.083

(.067, .1)

0.053

(.031, .076)

Body Mass

0.036

(.026, .047)

0.064

Constant

-3.6

(-4.43,

-2.77)

-4.17

(.05, .078)
(- 5.26, -3.09)

imputations are point estimates of wealth that lie within the interval measurements available.5
Tables IV and V present point and interval regression estimates that maintain alternative assumptions and use alternative estimation methods. Table IV
presents two maximum likelihood (ML) point estimates of a logit binary response
model, accompanied by delta-method confidence regions. One estimate uses all
6898 observations and applies the HRS-provided wealth imputations when point
wealth data are not available. This ML estimate is consistent if the logit model is
correct and the HRS imputations somehow succeed in measuring wealth without
error. The other estimate uses only the 4216 observations where point measurements of wealth were obtained. This ML estimate is consistent if the logit model
is correct and assumption MI holds.
Both of the estimates in Table IV indicate that the probability of being a
nonsmoker increases with wealth, age, schooling, and body mass. Use or nonuse
of the HRS wealth imputations affects the magnitudes of the age, schooling, and
body mass parameter estimates only moderately. However, the magnitude of the
wealth parameter estimate changes strikingly with use of the imputations. The
wealth estimate is 1.47 with confidence region (1.05, 1.89) when imputations are
not used, but shrinks to .0452 with confidence region (.0179, .0726) when the
imputations are used. This empirical finding should serve as a warning against
casual use of the HRS wealth imputations.

5The HRS website (www.umnich.edu/hirswww)provides this information about the imputation
method: "Various forms of missing data imputation have been used for the HRS and AHEAD study
depending on the type of data and to some extent on wave. For economic quantities such as the
value of the various asset types and incomes we have based our imputations on respondent responses
to a series of bracket questions which were asked when a respondent could or would not provide a
precise number. These bracket questions greatly improve our ability to impute reasonable values for
missing data cases (see Juster and Smith 1998 and/or Heeringa, Hill and Howell, 1995). While the
imputations based on the bracket data in early waves also included some controls for demographic
factors (see Connor and Heeringa, 1997), these controls have been abandoned in later waves in favor
of simple random within-bracket selections from the donor pool of precise responses."

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

538

C. F. MANSKI AND E. TAMER
TABLE V
SMOKING REGRESSIONS ON HRS DATA

Wealth
Age
Schooling
Body Mass
Constant

Logit ML without
Imputations (Table IV)

Logit MMD

MMS

Point Estimate
(Confidence Region)

Interval Estimate
(Confidence Region)

Interval Estimate
(Confidence Region)

1.47
(1.05, 1.89)
.041
(.026,.056)
.053
(.031,.076)
.064
(.05,.078)
-4.17
(-5.26, -3.09)

[1.21, 1.23]
(.89, 1.62)
[.0412,.042]
(.0352,.053)
[.033,.034]
(.017,.058)
[.067, .0671]
(.055,.0785)
[-4.28, -3.96]
(-5.58, -2.86)

[1.67, 1.68]
(.27, 4.6)
[.044,.050]
(.014,.081)
[.019,.024]
(.003,.054)
[.066, .067]
(.029,.095)
-4

Table V reports MMD and MMS interval estimates, accompanied by bootstrap
confidence regions.6 The MMD and MMS estimates use all 6898 observations
but do not, of course, use the HRS wealth imputations. Both estimates presume
that the probability of not smoking increases with wealth.
The majority of the HRS wealth data are point measurements, so part (c) of
the Corollary to Proposition 4 ostensibly holds. Supposing that the logit model
is correct and that the IMMI assumptions hold, the MMD interval estimate
should therefore converge to the point y as N -? oo. The HRS data do not
formally meet the unbounded support conditions for point convergence of MMS
estimates, but (x, v) have sufficiently rich support that we anticipate obtaining
relatively narrow intervals for the MMS estimates as well. To achieve the scale
normalization required in MMS estimation, we set the coefficient on the constant
equal to -4. This value falls within the corresponding MMD interval estimate,
namely [-4.28, -3.96], and so facilitates comparison of the MMS and MMD
estimates.
Examination of Table V shows that the MMD and MMS interval estimates
are, as expected, narrow in width. The MMD wealth, age, schooling, and body
mass parameter estimates are essentially points, as are the MMS wealth and body
mass estimates. The other intervals have discernible width, but they still are quite
narrow.
6
In this application of the bootstrap, we treat the sample data as if they were the population
and carry out a Monte Carlo simulation in which the data are sampled randomly N times with
replacement. Let [L, U] denote the identification region for a parameter and let [LN, UN] be the
interval estimate obtained by the MMS or MMD method, as the case may be. Each bootstrap sample
is used to compute a bootstrap estimate of [L, U], denoted [L* , UN]. By repeating the Monte Carlo
simulation, the distribution of [L* , UN] conditional on the actual sample data can be estimated with
arbitrary accuracy. The estimated distribution is used to find z*Nsuch that P* (L*7-z* < LN, UN <
UN+Z*) = .95, where P* is the probability measure induced by bootstrap sampling conditional on
the actual sample data. The bootstrap 95% confidence region is [LN - Z* UN+ ZN].

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

539

INTERVAL DATA

We find that the MMD and MMS estimates are rather similar to one another
in magnitude, and that both are similar to the logit ML estimates obtained without use of imputations. The estimates of the wealth, age, body mass, and constant
parameters vary across the three methods only in their second significant digits.
The estimates of the schooling parameters vary a bit more, but these too are
reasonably close to one another. The confidence intervals of the ML and MMD
estimates are quite similar in width, while those of the MMS estimates are considerably wider. When comparing the ML and MMD confidence intervals, one
should keep in mind that the former method applies an asymptotically efficient
estimator to the subsample of size 4216 with precise measurement of v, while
the latter method applies an estimator of as yet unknown efficiency to the full
sample of size 6898.
Perhaps the main message that emerges from consideration of the full set of
estimates in Tables IV and V is that use of the HRS wealth imputations yields
a very different perspective on the variation of smoking with wealth than one
obtains when the imputations are not used. Our findings call into question the
advisability of using the wealth imputations.
6.2. Home Ownershipand Income in the CPS
In our application using CPS data, y = 1 if a person rents his residence, and
y = 0 if he owns his home. The interval-measured covariate v is family income.
The other covariates x are years of schooling and age. The data concern the
14327 married white males of ages 30 through 50 who responded to the income
questions in the March 1995 wave of the CPS. Respondents were asked to report
their total family income for 1994 within 40 intervals of width 2500 dollars each,
namely [0, 2499],.. ., [97500, 99999]. The residual interval was [100000, oo). Thus
all of the income data are intervals in this application.
Table VI presents descriptive statistics for the data on (y, x, v0, v1). Table VII
presents logit ML, logit MMD, and MMS estimates of the regression of home
ownership on income. As in Section 6.1, the estimates are accompanied by deltamethod or bootstrap confidence regions. To compute the ML estimates, we
restricted attention to persons with income below $100,000 and used the midpoint of the reported income interval as a point imputation of income; the intervals have width $2500, so the maximum measurement error in these imputations

TABLE VI
DESCRIPTIVE

Rent
Family Income ($)
Age
Years Schooling

STATISTICS

FOR THE CPS DATA

Mean

Std Deviation

Min

Max

.205
48717.7
39.49
13.1

.403
22845.43
5.84
2.69

0
1250
30
4

1
98750
50
19

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

540

C. F. MANSKI AND E. TAMER
TABLE VII
RENTAL

Family Income
Age
Years Schooling
Constant

REGRESSIONS

ON CPS DATA

Logit ML

Logit MMD

MMS

Point Estimate
(Confidence Region)

Interval Estimate
(Confidence Region)

InitervalEstimate
(Confidence Region)

-.0286
(-.0311, -.0262)
-.0482
(-.0562, -.0402)
-.0739
(-.0918, -.0562)
4.71
(3.96, 5.47)

[-.019, -.017]
(-.0218, -.009)
[-.0431, -.03]
(-.0501,-.0228)
[-.047, -.034]
(-.060, -.028)
[2.97, 3.11]
(2.64, 3.51)

[-.034, -.0157]
(-.051, -.0095)
[-.0304,-.022]
(-.108,-.017)
[-.049, -.041]
(-.18, -.036)
3.0

is $1250. The MMD and MMS estimates presume that the probability of renting one's residence decreases weakly with income. To normalize the scale of the
MMS estimates, we set the coefficient on the constant equal to 3. This value falls
within the corresponding MMD interval estimate, which is [2.97, 3.11].
Examination of Table VII shows that the three estimation methods yield the
same qualitative conclusions about home ownership patterns in the population,
but not the same quantitative conclusions. All three estimates indicate that the
probability of renting one's residence decreases with income, age, and years of
schooling. The MMD and MMS methods yield reasonably narrow interval estimates that are similar in magnitude to one another. However, the point estimates obtained by maximum likelihood using interval midpoint imputations are
systematically larger in magnitude than the MMD and MMS estimates. Indeed,
the ML estimates for three of the four parameters lie well outside the corresponding MMD confidence intervals. Thus here, as in the HRS application, use
of imputations to cope with interval data yields a different empirical perspective
than one obtains when imputations are not used.

7.

CONCLUSION

We have examined inference on the regressions E(ylx, v) and E(vlx) when
interval data are available on v, the other variables being measured precisely. Our
analysis, based on the IMMI assumptions, placed no restrictions on the location
of v within the available interval data. We began with nonparametric bounds
on the regressions, next determined identification regions for semiparametric
and parametric models, and then developed consistent estimators. The Monte
Carlo and empirical findings indicate that the MMS and MMD methods are
well-behaved in practice.
Looking beyond the problem of regression with interval data, we can see that
the inferential approaches developed here may be applied to other problems in
which the available information implies inequalities on regressions. An example
is the incomplete parametric model of (two person, two action) games studied in

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

541

INTERVAL DATA

Tamer (1999). When the game does not admit a unique pure strategy equilibrium,
the model implies upper and lower probabilities for each of the four possible
outcomes. Tamer (1999) provided sufficient conditions for the model parameters
to be identified. The MMD estimator introduced in the present paper can be
used to estimate the parameters.
Dept. of Economics and Institute for Policy Research, Northwestern University, Evanston, IL 60208, U.S.A.; cfmanski@northwestern.edu;wwwfaculty.econ.
northwestern.edu/faculty/manski
and
Dept. of Economics, Princeton University,Princeton, NJ 08544, US.A.; tamer
@princeton.edu; www.princeton.
edul-tamer
ManuscriptreceivedJanuary,2000; final revision receivedAugust, 2000.

APPENDIX:

PROOFS

OF PROPOSITIONS

AND LEMMAS

PROOF OF PROPOSITION 1: Consider (1). The law of iterated expectations and Assumption MI

yield
(Al)

E(ylx, vo,vl)

f E(ylx, v, vo,vl) dP(vlx, vo,vl)

f E(ylx, v) dP(vlx, Vo,VI).

Assumptions I and M imply that for all constants V0 < Vl,

E(ylx, v = VO)< f E(ylx, v) dP(vlx, vo = Vo, v, = VI) < E(ylx, v = Vi).

(A2)
Hence,

E(ylx, v= V0) < E(ylx, v0=V0, v, = Vj) < E(ylx, v= V).

(A3)

To prove the lower bound on E(ylx, v = V), take any V1< V. It follows from (A3) and from Assumption M that E(ylx, v0 = V0, v1 = VI) < E(ylx, v = V). Hence the bound holds. To prove sharpness,
view the bound as a function of V. This function is weakly increasing in V, so Assumption M holds.
The proof of the sharp upper bound uses analogous reasoning.
Now consider (2). By Assumption I, the smallest possible value of v is v0 and the largest is vl.
The result follows immediately.
Q.E.D.
PROOF OF PROPOSITION 2: Observation of (y, x, v0, vl) identifies P(y = lIx, v0, vl). By (4),

(A4)

P(y=llx,vo,VI)=fP(y=lix,v,vo,vl)dP(vlx,vo,vl)=fP(y=llx,v)

dP(vlx, vo,vl).

We decompose the (K + 2)-dimensional space of values of (x, v0, vl) into three mutually exclusive and
exhaustive subsets and apply (A4) to each in turn. The subsets are the values of (x, v0, vl) satisfying
the conditions (a) 0 < x/ + v0; (b) x/3 + v, < 0; and (c) x/3 + vo < 0 < x/ + v, respectively.
(a) 0 < xP + v0 0 < xP + v
P(y = lIx, v) > 1-oa, by (5). Hence P(y = lIx, v0, vl) > 1-a,

by (A4).
(b) xfl+v

<0

xP?+v< 0=P(y=lIx,v)

< 1-oa, by (5). Hence P(y=lIx,vo,vl)

by (A4).

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

< 1-a,

542

C. F. MANSKI AND E. TAMER

(c) If x3 + v0 < 0 < x3 + vl, then x/ + v may be negative or positive. So 0 < P(y = 1ix, v) < 1, by
(5). Hence 0 < P(y = lix, v0, vl) < 1, by (A4).
Now repeat (a), (b), and (c) with b replacing /3. We draw contradictory conclusions on P(y=
lIx, v0, vl) if and only if xb+v, < 0 < x +?vo or x +?v, < 0 < xb+vo. Hence /3 is identified relative
to b if and only if P[T(b)] > 0.
Q.E.D.
PROOF OF COROLLARY TO PROPOSITION 2: (a) The set B* is nonempty because /3 E B*. To
prove convexity, observe that the condition P[T(b)] = 0, identifying b as a member of B*, holds if
and only if
'0<

P(xb+vl

xP+v0)

<0<

=P(xP+v1

=0.

xb+v0)

Let b' and b" be distinct elements of B*. Then
P(xb'+v,

<0<xP+vo)=P(xb"+v,

P(xp1+v,

<0<

xb'+vo)

<0<xp+vo)=0,
< 0< xb"+vo)

=P(xp/+v,

=0.

Now consider b,,,_ ab' + (1 - a)b", where a E (0, 1). It follows from the above that
< 0 < x: + vo) = P(xp + v, < 0 < xba + vo) = 0.

+
P(xb,,,v?

Hence b,,, E B*.
(b) It suffices to consider the case
that b E B*, with bk < 0. Then

/3k

> 0, as the argument when

Pk

< 0 is analogous. Suppose

P[T(b)] > P(x/ + v, < 0 < xb + vo)
< -(X-k-k

=P[xk

+ Vl)/k

flXk

< -(X-kb-k

+ vo)/bk]

> O0

The final strict inequality holds because, by assumption, P(xklx_k,
v0, vl) has unbounded support,
a.e. (X_k, v0, vI). But P[T(b)] > 0 implies that b g B*, contradicting the supposition. Hence bk ' 0,
all b E B*.
(c) Suppose that b E B*, with b + P. By assumption, no linear subspace of x has probability one.
Hence either P(xb < xp) > 0, or P(x/ < xb) > 0, or both hold. It suffices to consider the case
P(x/ < xb) > 0, as the argument when P(xb < xp) > 0 is analogous. In this case,
P[T(b)]

> P(x/

+ v, < 0 < xb + vo) = P(-xb

< vo < v, <-x:)

>

0,

the final strict inequality holding due to the support condition assumed for (v0, vl). This contradicts
the supposition that b E B*.
(d) By assumption,

P(v0 = vl) > 0. Within

the subpopulation

with

(v0 = vl),

the assump-

tions imposed guarantee identification of /3 up to scale by Manski (1985, Lemma 2) and Manski (1988, Proposition 2, Corollary 2). We have normalized scale by setting 8 = 1. Hence /3 is
identified.
Q.E.D.
PROOFOF LEMMA1: The proof to Proposition 2 enables us to rewrite the set T(b) as follows:
T(b)=[(x,vo,vl):

< 1-oa}

{O<xb+vo0P(y=llx,vo,vj)

U{xb+v

< o0fP(y

-. llx, vo,vl) > 1-a}]

Define f (b, x, vo, vl) to be the integrand of S(b, A). On the complement of T(b), we have
0 < xb+vo

{0 < xb+vf

nP(y

= llx, vo,vl)

> 1-a}

f (b, x, vo, vl) = IP(y = lIx, vo, vl) -(1-

a)I,

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

543

INTERVAL DATA
xb+vi < 0

= lx, vo,vvil) < 1-al}

{xb+vo < OnP(y

ff(b, x, vo, vl) = IP(y = lIx, vo, vl) -(1-

< 0<

xb +vo

xb +v,

f (b, x, vo, vl) =[P(y

a)I,

= 1 x, vo, vl) - (1-

a)]

[2A(x,

vo, vl) -1]

= IP(y = lIx, vo, vl) - (1 - a)I,

xb + vo < O< xb + v,

f (b, X, vo, Vl) = [P(y = l|IX, vo, vl) -(1 - a)]
= IP(y=lIx,vo,vl)-(1-

= IP(y = llx,vo,v)
Thus f(b,x,vo,vI)
- a)J.
-(1
T(b), f(b, x, vo, vl) = -lP(y = llx, vo, vl) -(1- a)I.
Consider b E Bo. Then P[T(b)] = 0. Hence
ff(b,

x, vo, vl) dP(x, vo, vl) =

f IP(y

= lix,

a)I.

Similar reasoning

vo, vl) -(1-

[2A(X, vo, vl) -1]

shows

that

on

a) dP(x, vo, vl).

Now consider b X Bo. Then P[T(b)] > 0. Hence
ff(b,

f

x, vo, vl) dP(x, vo, vI) < lP(y

= lix, vo, v)

-(1-

a) dP(x, vo, vl).

This shows that all b E Bo solve problem (10). Moreover, if P[(x, v0, vl): P(y = llx, v0, vl) =
(1-oa)] = 0, then -lP(y = lix, v0, vl) - (1-oa)l < 0 a.e. on T(b). Hence the above inequality is strict.
Hence b X Bo does not solve (10).
Q.E.D.
PROOF OF PROPOSITION 3: (a) We show here that the sample objective function SN(, AN) converges uniformly on B to the population objective function S(., A), which is continuous on B and is
maximized by B*. These properties, combined with the assumption that EN >a.s. 0, imply the result.
First consider the population problem solved by B*. Our Lemma 1 shows that B* solves the
problem { maXbEB S(b, A)}. Proposition 3 assumes that xb + v has bounded Lebesgue density and that
B is compact. These regularity conditions enable application of Lemmas 5, 7, and 8 of Manski (1988,
Section 7.3) to show that the population objective function S(., A) is continuous on B.
Now consider the sample objective function SN(, AN)- We want to show that SN(, AN) -a.s. S (, A)
uniformly on B. It is easy to see that SN(b, AN) >a,s. S(b, A) for each b E B. This pointwise convergence holds because the integrand of S(., ) is uniformly bounded and because, by assumption,
P[(x, v0, vl): P(y = ljx, vo0,v) = (1 - a)] = 0 and PN(y = llx, vo, vI) a.s. P(Y = llx, vo, v), a.e.
(X,

Vo, V1).

Given pointwise convergence, a sufficient condition for uniform convergence is that the sequence
{SN(, AN)} be strongly stochastically equicontinuous (see Davidson (1994, Sec. 21.3-21.4)). Observe
that
ISN(a, A,,)-SN (C, A,,)| < ISN(a, A,,)-SN

(a, A)I + ISN(a, A) -SN (C, A)I

+ ISN(c, A)-SN(c, A,)I
for any a E B and c E B. The first and third terms on the right side almost surely converge to zero
v a) -a.s. P(Y =
because P[(x, v0, vl): P(y = llx, v0, vl) = (1 - a)] = 0 and because PN(Y = lix, VO,
llx, v0, vl), a.e. (x, v0, vl). As for the second term on the right side, the proof to Lemma 6 of Manski
(1988, Section 7.3) shows that

lim lim
SO0 -N

sup

ISN(a, A) -SN

(c,

A)I1=

a.s.?-

?? (a, c)cBxBs.t. |c-al<S

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

544

C. F. MANSKI AND E. TAMER

Hence { SN(., AN)} is strongly stochastically equicontinuous.
Let S_
(b) Let SNm
maXbEB SN (b, AN) and SN
max,EB S(b, A) =
infb,EB* SN (b,AN)
infbEB*S(b, A), where the last equality holds because S(., A) is maximized on B*. Let AN
SUPbEBISN(b, AN) - S(b, A)I.
By (12), EN > SNm.SN* =X B* C BN =X p(B*, BN) = 0. Observe that
SN

-SN*

=

-SN)

(SNmax-S*)+(S*

< ISNnlax-S*I +IS*

EN > 2AN =X p(B*, BN) = 0. We have assumed that AN/EN-+
sufficiently large, a.s.
Hence

SNI < 2AN.
as.0

Hence

EN > 2AN for N

Q.E.D.

PROOF OF PROPOSITION 4: This proposition is an immediate consequence of inequality (A3)
in the proof of Proposition 1. For a parametric regression model, (A3) becomes f(x, vo, y) <
E(yIx, vo, vl) < f(x, vl, y). It follows that c is observationally equivalent to y if and only if
Q.E.D.
vo, vl) < f (x, vl, c), a.e. (x. Vo, vI).
f (x, vo, c) < E(yIx,
PROOF OF COROLLARY TO PROPOSITION 4: (a) The set C* is nonempty because y E C*. To
= 0, identifying c = (b, d) as a member of C*,

prove convexity, observe that the condition P[V(c)]
holds if and only if

P[F(xb + dvo) < E(ylx, vo, vl) < F(xb + dvl)] = P[xb + dvo < s(x, vo, vl) < xb + dv,] = 1,
where s(x, v0, vl)

F-' [E(ylx, v0, vI)]. Let c' and c" be distinct elements of C*. Then

P[xb'+d'vo <s(x,vovl)
Now consider c

=

<s(x,vo,vl)

<xb'+d'v,]=P[xb"+d"vo

<xb"+d"v1]=1.

ac' + (1 - a)c", where a E (0, 1). It follows from the above that

P[xb,,, + dVo < s(x, vo, vl) < xb,,, + davl] = 1.
Hence ca E C*.

(b) Suppose that c E C*, with bk #
P[V(c)]

> P[E(yIx,

P[(3k

Then

vo, v1) < F(xb+dvo)]

> P[F(xp
=

Pk.

+ 8vl)
-

< F(xb

+ dvo)]

bk)Xk < X-k (b-k - P-k)

= P(xp
+ dvo -

+ 8vl < xb + dvo)
V1] > 0.

The final strict inequality holds because, by assumption, P(xk IX_k, v0, vl) has unbounded support,
a.e. (X_k,v0, v1). But P[V(c)] > 0 implies that c , C*, contradicting the supposition. Hence bk = Pk,
all c E C*.
(c) Consider the subpopulation with (v0 = vl). By assumption, P(v0 = vl) > 0. Hence c E C* must
satisfy the equality F(xb+ dv) = E(ylx, vo, vl) or, equivalently, xb+dv = s(x, vo, vl), a.e. (v0 = v,).
The support condition on P(x, v) implies that (,B,8) is the only parameter value that satisfies the
Q.E.D.
equality almost everywhere (v0 = vI). Hence y is identified.
PROOF OF LEMMA 2: LetccC0.
v0, vl)=g0(c,x,
Thengl(c,x,
the objective function in (18) equals zero. Let c , C0. Then

v0, vl)=0,

a.e. (x v0, vl). Hence

g1(C,X, vo, v)W[f (x, VI,C), Nq(x,vo, VM)]
+g0(c, x, v0, v)w[f (x, v0, c), *(x, v0, vI)] > 0
for all (x, v0, vl) E V(c). P[V(c)] > 0 for c , C0. Hence the objective function is positive.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

Q.E.D.

545

DATA

INTERVAL

PROOF OF PROPOSITION
5: (a) We show here that the sample objective function QN(, NN) converges uniformly on C to the population objective function Q(.,
71), which is continuous on C and is
minimized by C*. These properties, combined with the assumption that EN as 0, imply the result.
First consider the population problem solved by C*. Our Lemma 2 shows that C* solves the
problem {minCECQ(c, 71)}.Observe that the function h(., x, vo, vl) is continuous on C and has been
assumed to be dominated. These regularity conditions enable application of Lemma 2 of Manski
(1988, Section 7.2) to show that the population objective function Q(.,
71) is continuous on C.
Now consider the sample objective function QN(- NqN). We want to show that QN(*,7 N)
S
Q(.,71) uniformly on C. It is easy to see that QN(C, NN) >a.s. Q(c, -q) for each c E C. This pointwise
convergence holds because h(., x, vo, vl) is continuous and dominated and because r1N
(X, VO,vI) a.s.

71(x, v0, vl), a.e. (x, vo, vl).

Given pointwise convergence, a sufficient condition for uniform convergence is that the sequence
m)}be strongly stochastically equicontinuous (see Davidson (1994)). Observe that

{ QNQ(',

IQN(a,

X,n) - QN(C,

in)I < IQN(a,

X,n) - QN (a,

N)I + IQN(a, _)

+IQN(C,ri)-QN(C,

- QN(C,

r0)I

in) I

for any a E C and c E C. The first and third terms on the right side almost surely converge to zero
because QN("*) is continuous and NqN(X, v0, vI) >a.s. N(x, VO, V1), a.e. (x, v0, vl). As for the second
term on the right side, the proof to Lemma 3 of Manski (1988, Section 7.2) shows that
lim lim
8-0 N-o

IQN(a,A)-QN(c,A)I1 =a.s.0?

sup
(a,c)eCxCs.t.Ic-aI<6

Hence {QN(' 'N) } is strongly stochastically equicontinuous.
(b) Let QNmin -SmincUC QN(CNN) and QN suPCC*QN(C, NN). Let Q* minCEc Q(c, -)
supcc* Q(c, -q), where the last equality holds because Q(.,iq) is minimized on C*. Let AN
SUPccCIQN(C, NN) - Q(C, i,)I.
By (21), EN -> QN- QNmin =X C* C CN =X P(C*, CN) =O. Observe that
QN-

=

QNrnin

(Q*N

Q*) + (Q* -

< IQ*N -

QNmin)

Q*I+ IQ*-

QNminI

< 2AN.

EN > 2AN=X P(C*, CN) =O. We have assumed that AN/EN > as.0. Hence EN > 2AN for N
sufficiently large, a.s.
Q.E.D.

Hence

PROOF OF LEMMA 3: c E C* if and only if f(x,

v0, vl) < f(x,

v0, c) < E(ylx,

vl, c), a.e. (x, v0, vl).

For any positive-valued function w(.),
f(x,

v0, c) < E(ylx,

v0, vl)

X

w(x, v0, vJ)E[y-f(x,
W(X,
f

=

Ao(c)

Vo,
>

V1)[y-f(x,

v0, c)I(x, v0, vI)] > 0
vo,

c)]dP(x,

vo, vl)

0.

Similar reasoning shows that Al (c) < 0. Hence the objective function in (22) equals zero on C*. This
objective function is necessarily nonnegative, so C* solves (22).
Q.E.D.
PROOF OF LEMMA4: As N -+ oo, the least squares and maximum likelihood estimates converge
almost surely to the parameter values that solve the following asymptotic forms of problems (29)
and (30):

min E{y[1-F(xb

bcB,d>

+ dvJ)]2+ (1-y)F(xb

l

max E{y .log[F(xb

bcB,d>0

+dvl)] +(1-

+ dvO)2},

y)log[l -F(xb

+dv0)}.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

546

C. F. MANSKI AND E. TAMER

Assuming that the solutions to these asymptotic problems are interior to the parameter space, they
solve the corresponding first-order conditions. To show that the estimators are generically inconsistent, it suffices to set d equal to its true value 8, let x be scalar, and show that the derivatives with
respect to b of the asymptotic criterion functions generally do not equal zero when evaluated at /3.
The derivative at /Bof the asymptotic least square criterion function is
E[2F

f1 (F1 - 1)x + 2(1 - F,)Fo fox],

where FV= F(xf + v), Fj = F(x1 + vj), and fj = [dF(s)/ds](s = x: + vj), j = 0, 1. The derivative at
/3 of the asymptotic maximum likelihood criterion function is
E[Fv j

(1-F)

foxj.

Both derivatives equal zero if P(vO = v, = v) = 1, but they do not generally equal zero
otherwise.
Q.E.D.

REFERENCES
DAVIDSON, J. (1994): Stochastic Limit Theoiy. New York: Oxford University Press.
HOROWITZ, J., AND C. MANSKI (1998): "Censoring of Outcomes and Regressors Due to Sulvey

Nonresponse: Identification and Estimation Using Weights and Imputations," Journal of Econometrics, 84, 37-58.
(2000): "Nonparametric Analysis of Randomized Experiments with Missing Covariate and
Outcome Data," Joiurnalof the American StatisticalAssociation, 95, 77-84.
HSIAO, C. (1983): "Regression Analysis with a Categorized Explanatory Variable," in Studies
in Econometrics, Time Series, and Multivariate Statistics, ed. by S. Karlin, T Amemiya, and
L. Goodman. New York: Academic Press.
JUSTER, T., AND R. SUZMAN (1995): "An Overview of the Health and Retirement Study," Journal
of Human Resources, 30, S7-S56.
LANCASTER, T. (2000): "The Incidental Parameter Problem Since 1948," Journal of Econometrics,
95, 391-413.
MANSKI, C. (1975): "Maximum Score Estimation of the Stochastic Utility Model of Choice," Journal
of Econometrics, 3, 205-228.
(1985): "Semiparametric Analysis of Discrete Response: Asymptotic Properties of the Maximum Score Estimator," Journal of Econometrics, 27, 313-333.
(1988): Analog Estimation Methods in Econometrics. New York: Chapman and Hall.
(1989): "Anatomy of the Selection Problem," Journal of Human Resources, 24, 343-360.
MANSKI, C., AND S. THOMPSON (1986): "Operational Characteristics of Maximum Score Estimation," Journal of Econometrics, 32, 85-108.
TAMER, E. (1999): "Incomplete Simultaneous Discrete Response Models with Multiple Equilibria,"
Department of Economics, Princeton University.

This content downloaded on Sun, 13 Jan 2013 09:57:09 AM
All use subject to JSTOR Terms and Conditions

