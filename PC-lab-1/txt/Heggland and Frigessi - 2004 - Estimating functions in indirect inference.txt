Estimating Functions in Indirect Inference
Author(s): Knut Heggland and Arnoldo Frigessi
Source: Journal of the Royal Statistical Society. Series B (Statistical Methodology), Vol. 66, No.
2 (2004), pp. 447-462
Published by: Wiley for the Royal Statistical Society
Stable URL: http://www.jstor.org/stable/3647536 .
Accessed: 10/05/2013 04:31
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at .
http://www.jstor.org/page/info/about/policies/terms.jsp

.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact support@jstor.org.

.

Wiley and Royal Statistical Society are collaborating with JSTOR to digitize, preserve and extend access to
Journal of the Royal Statistical Society. Series B (Statistical Methodology).

http://www.jstor.org

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

J. R. Statist.Soc. B (2004)

66, Part2, pp. 447-462

Estimatingfunctions in indirect inference
KnutHegglandand ArnoldoFrigessi
Universityof Oslo and NorwegianComputingCentre,Oslo, Norway
[ReceivedJune2002. FinalrevisionSeptember2003]
Summary.Thereare modelsforwhichthe evaluationof the likelihoodis infeasiblein practice.
Forthese modelsthe Metropolis-Hastings
cannotbe easilycomputed.
acceptanceprobability
Thisis the case, forinstance,whenonlydeparturetimesfroma G/G/1queueareobservedand
are required.Indirectinferenceis a methodto
inferenceon the arrivalandservicedistributions
functiondoes nothavean analyticalclosed
estimatea parameter0 in modelswhose likelihood
model
form,butfromwhichrandomsamplescan be drawnforfixedvaluesof 0. Firstan auxiliary
is chosen whose parameter/ can be directlyestimated.Next,the parametersin the auxiliary
modelare estimatedforthe originaldata, leadingto an estimate/3.The parameter/ is also
estimatedby usingseveralsampleddatasets, simulatedfromthe originalmodelfordifferent
valuesof the originalparameter0. Finally,the parameter0 whichleads to the best matchto /
is chosen as the indirectinferenceestimate.We analysewhichpropertiesan auxiliarymodel
shouldhaveto give satisfactoryindirectinference.We lookat the situationwherethe dataare
summarizedin a vectorstatisticT, and the auxiliarymodelis chosen so that inferenceon /
is drawnfromT only.Underappropriate
assumptionsthe asymptoticcovariancematrixof the
to the asymptoticcovariancematrixof T andcomponentwise
indirectestimatorsis proportional
to the squareof the derivative,withrespectto 9, of the expectedvalue
inverselyproportional
of T. We discuss how these resultscan be used in selectinggood estimatingfunctions.We
applyourfindingsto the queuingproblem.
statisticalmodels;
variance;Estimating
GIG/1queue;Implicit
functions;
Keywords:Asymptotic
data
Indirect
Transactional
inference;Simulatedlikelihood;

1. Introduction
Indirect inference methods (Gourieroux et al., 1993) have been developed to estimate a vector
of parameters 0 in models that can be sampled from, but whose likelihood function does not
have a closed analytical form. First an auxiliary model is chosen whose parameters 3 can be
estimated. Then, the parameters of the auxiliary model are estimated by using the original data,
leading to an estimate 3. The parameters are also estimated by using data sets that are simulated
from the original model for various values of the original parameter 0, leading to estimates 3(0).
Finally, 0 is chosen so that /(0) matches 3 best. What properties should the auxiliary model
have to lead to satisfactory estimation of 0? Gallant and Tauchen (1996) suggested to adopt as
the auxiliary model a general model which contains the true model. Gourieroux et al. (1993)
preferredto use auxiliary models that are somehow close to the true model. Intuition suggests
that, the more similar the auxiliary model is to the true model, the better indirect inferences perform. We investigate the precise meaning of such similarity and suggest guidelines for choosing
between different auxiliary models.
Consider a data set yN = (yl,...,yN)'

that is assumed to be generated by a stationary

stochastic process with an unknown parameter 0. Let the generating process be known
Addressfor correspondence:Arnoldo Frigessi,Section of MedicalStatistics,Universityof Oslo, PO Box 1122
Blindern,N-0317 Oslo, Norway.

E-mail:arnoldo.frigessi@basalmed.uio.no

? 2004 RoyalStatisticalSociety

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

1369-7412/04/66447

448

K.Hegglandand A. Frigessi

except for the parameter value 0, so that it is possible to generate samples for given 0. More
function, possibly depending on 0, and let x be a set of
precisely, let G be an ZUN-valued
random variables with known joint density function, also possibly depending on 0, such that
YN= G(0, x). It can be difficult to evaluate the likelihood if the inverse set {x :YN = G(0, x)}
is complex and infeasibly time consuming to compute, as when it is the union of 2N subsets. Then, even for moderately large N the likelihood cannot be computed in practice, and
algorithms to compute the maximum likelihood estimate cannot be applied. This includes
Markov chain Monte Carlo methods, since the acceptance probability requires evaluating the
likelihood.
Implicit statistical models (using the terminology of Diggle and Gratton (1984)) have been
developed and applied in econometrics, finance and population dynamics (Gourieroux and
Monfort, 1996; MacKinnon and Smith, 1998; Corduas, 1997; Mealli and Rampichini, 1999;
Calzolari et al., 2001; Schweder et al., 1999; Pastorello et al., 2000), though Billio et al. (1998)
showed that often it is possible to bypass the intractability of the likelihood by using hidden
variables and Markov chain Monte Carlo sampling. We present a problem where augmenting with hidden variables does not help and indirect inference is, as far as we know, the only
possibility. Recently attention has been given to transactional data in queuing systems which
include only individual service start and termination times, as might be available from automatic bank teller machines or telecommunication network nodes. These incomplete data make
inference more complex; see Bhat et al. (1997), Luh (1999) and Jones (1999) for example. In
Section 5 we consider a GIG/1 queue where only departure times are available. The evaluation of the likelihood function is intractable, even though the model is easy to simulate from.
In Section 2 we describe indirect inference and focus on the choice of the auxiliary model.
In Section 3 we illustrate indirect inference with a simple example, the X2-distribution, and
investigate the effect of the choice of the auxiliary models on parameter estimation. Different models give the same results when inference depends on the data only through the same
statistic. Using an auxiliary estimating function with a parameter / of higher dimension than
0 can give poor estimates, contrary to Gallant and Tauchen (1996). The asymptotic theory
that is presented in Section 4 and the X2-example indicate that it is best to select an auxiliary estimating function based on a statistic T which has a small variance and an expected
value with a large derivative with respect to 0. In Section 5, we apply our findings to inference from departure times of a GIGI1 queue. We suggest the use of certain plots to select the
statistics on which to base the auxiliary estimating function. Finally, Section 6 contains some
discussion.

2. A generalclass of indirectinferencemethods
We start with a general estimating function QN (yN; 0), which depends on an auxiliary parameter vector /3.The estimating function can be a simpler likelihood model for the data. We compute
the estimator of/3 by maximizing the estimating function
(1)
N arg max{ QN(yN;/3)}.
We assume that QN converges almost surely as N - o0oto a deterministic limit Qe (0, P) which
depends on 0, the unknown parameter of the data-generating process. Let
b(0) = arg max{ Q~(0, /3)}

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

IndirectInference

449

be the binding function and assume that it is injective. The intuition is that, as N grows to 00,
the data YN carry increasingly better information about 0 into the estimating function. If the
binding function b was known and one to one, a consistent estimator of 0 would be the solution
The idea is to replace the unknown function b(.) by an estimate based on
ON of
fiN =b(ON).
simulations. First we sample S independent data sets
ZSN(0)

= (Z
(0), . . . , ZS(0))',

S,

S= 1,...,

(2)

from the data-generating process for a given 0. Denote the whole simulated data by zS(O)
=
as
Then
we
estimate
the
from
these
simulations
parameter 0
(Zl(0),...,
Z(0))'.

= argmax

sSN(O)

QN{z~z(O);/3}.

(3)

As N -+ oc, this is equivalent to
fsN(O) =

-1

s

arg max[QNf{zs(0); /}]
SE

(4)

S=1

for every S; see Gourieroux et al. (1993). In both cases, /SN(') is a consistent functional estimator of b(.) as N -- o0. The next step is to calibrate 0. For a symmetric positive definite matrix
QN, our estimate of 0 will be
OSN(QN) =

arg min[{/3N

-

OsN(0)}

QN{(3N

-

SN(0)}],

(5)

minimizing the distance between the estimate that is based on the real data and the estimate that
can be replaced in practice
is based on the simulated data, using the metric defined by 2N.
FUN
is
from
that
estimated
the
data.
a
matrix
by
An alternative approach is to use
OSN = arg max[QN

{yN; PSN(0)}],

(6)

which is consistent and asymptotically normal, but not as efficient as estimate (5), based on
the best possible SQNwhich depends on 0. When the dimensions of 0 and 3 are the same, the
asymptotic properties of all OSN(QN) are the same regardless of the choice of QN and are also
the same as those of OSN.See Gourieroux et al. (1993) for details.
Clearly, the quality of the estimator depends strongly on the estimating function QN. A
possible choice for QN is proportional to the log-likelihood function of an auxiliary model
lN
QN(YN; /3)=N

log{ fa(yt;

) },

(7)

t=1

where IIN= fa (yt; /) approximates the likelihood of the data, whereas /3can be estimated. The
number N of points in the simulated data sets (2) and in the original data set YN should be
and
the same. The estimate of 0 is based on minimizing the distance between
By
SNu(0)
/UN.
in
N
we
minimize
bias
the
estimates
of
same
the
error
due
to
finite
the
sample
keeping
/3. If
/3N actually is a consistent estimator of 0, the procedure that is outlined above can be used to
correct for finite sample bias; see Kuk (1995) for the case of generalized linear mixed models.
The correction is similar to use of the bootstrap; see Gourieroux and Monfort (1996). Calzolari
et al. (1998) incorporate control variates to reduce the variance in indirect inference.
There are other estimation methods based on simulation. The simulated method of moments (McFadden, 1989; Duffie and Singleton, 1993; Pakes and Pollard, 1989), which was

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

450

K.Hegglandand A. Frigessi

invented before other approaches, is a special case of indirect inference, when QN (YN; ) =
where mi is the ith moment. Simulated maximum likelihood is described
1{fli mi((yN)}2,
in Gourieroux and Monfort (1996). The idea is to look for a set of latent variables u and an
unbiased simulator f (yt, u, 0) of f(yt; 0) such that E{f(yt, u; O)Iyt}= f(yt; 0). The simulated
maximum likelihood estimator of 0 is

argmaxE log{ S
0

f (yt,u ;0))

s=1

t=l
where us,s= 1,...,S, t = 1,..., N, are independent drawings from the known distribution of u.
Diggle and Gratton (1984) and Hazelton (1995) also used simulations from the true model for
different values of 0 and estimated the true likelihood function through nonparametric density
estimation. Fermanian and Salani6 (2001) proved asymptotic efficiency as the bandwidth goes
to O.

3. Which estimating function?
Let the data YN be independent samples from a x2-distribution with parameter 0= 3 degrees
of freedom. We create 500 data sets of size N = 100. For each such data set, there are S = 50
simulated data sets of size N = 100 generated from the X2-distribution for several values of 0,
ZN(0), 0=0.60, 0.62,..., 5.40, s = 1,..., S. To ensure that the variation for different 0 in the
simulated data is only due to 0, the same random numbers are used for each value of 0 for the
50 repetitions. We compare four auxiliary models:
(a)
(b)
(c)
(d)

gamma (two parameters);
X2 (one parameter);

normal (two parameters);
exponential (one parameter);

and we use the estimating function (7). In addition, we apply the moment matching approach:
(e) match the mean of the original and simulated data,
QN(YN; 3) = -

3-

Yi
- Ni= 21

;

(8a)

(f) match the mean of the logarithm of the original and simulated data, the latter being a
sufficient statistic,
QN(YN; ) = -

P --

E log(yi)
i=1

.

(8b)

Table 1 shows the means, standard deviations and root-mean-squared errors (RMSEs) over the
500 independent experiments of the six different estimates of 0. For comparison, the first row
shows the results that were obtained by maximum likelihood on the correct model.
We see that the means are all very similar. The standard deviations for gamma, X2and matching the mean of the logarithms are also similarly small. Using the exponential or matching the
mean is identical and a little worse. The normal auxiliary model is the worst. A normal auxiliary
model with fixed variance gives the same results as the exponential auxiliary model. Thus choosing an auxiliary model with more parameters than the data-generating model may give poorer

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

IndirectInference

451

Table 1. Means, standard deviations and RMSEs of estimates
of 0 withdifferentauxiliarymodels
Model

Mean

Maximumlikelihood
(a) Gamma
(b) X2
(c) Normal
(d) Exponential
(e) Mean
(f) Mean logarithmic

3.000
2.996
3.038
3.020
3.025
3.025
3.047

Standarddeviation RMSE
0.217
0.215
0.219
0.331
0.251
0.251
0.219

0.216
0.215
0.222
0.331
0.252
0.252
0.224

results. The estimating function (7), with fa being the exponential density, depends on the data
only through their average, as happens when using the estimating function (8). Although the
metrics are different the result is exactly the same. Let TN indicate a vector of statistics of the
data YN, such that
QN(YN, P)= f{TN(YN), 13}.

(9)

Assume for simplicity that S= 1. Then N (0) = arg maxo(f[TN{zN (0) }, ]) and 0N =
argmaxo[f{TN(yN);,
N(0)}] is the estimator for 0. Assume that TN, 0 and 0 have the same
dimension and there is a value of 0 such that TN{ZN(0)} = TN(YN). Then, any reasonable function f that is used to measure the matching would lead to the same result.
Indirect inference is successful if we can choose an auxiliary statistic TN{ZN(0)} which is sensitive to changes in the parameter of interest 0 but is robust with respect to random variations
of the samples zs~(0) for a given such 0, i.e. when we simulate data z' (0) for different values
of 0 and use the same random numbers TN{ZN(O)} should vary in 0. When 0 is kept fixed and
different random numbers are used for simulation, we would like the statistic to remain stable. More precisely, varo{TN(ZN (0) } should be componentwise small, and aEo[TN{ZN (0) }]/8
componentwise large, at least locally around the true value of 0. Indirect inference must be
able to identify the true 0 by means of samples of TN{ZN(0)}. If the variance of TN{ZN(0)} is
large compared with the derivative of its expectation, it will be more difficult to detect genuine
changes in 0 through the variability of TN.
In the X2-example,we illustrate these criteria by using the three natural statistics
I

N

-T(1
N

T3=

exp

N'

Zn(0),

In=1

Nn=1
E log1{zn(0) ,

approximately on the same scale. We estimated the variance of the statistics from the 500 data
experiments. The three variances are 0.062, 0.079 and 0.044 respectively.The expected value of
the x2-distribution is 0, and the variance is 20. Therefore
T(1)(0) =

-1 N

(0)

n=l
EZn

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

452

K.Hegglandand A. Frigessi

for s = 1,..., 50 is approximately linear in 0, when the same random numbers are used for the
various values of 0. T(2)(0) goes approximately as /(20). Since T(2) in addition has a larger
variance than T('), it should not be preferred.
Plotting Tsj (0) for s = 1,...,50 as a function of 0 e {0.60, 0.62,..., 5.40} and 1= 1,2, 3, we
find that the trajectories of T(1) grow linearly, those of T(2) as a square root and the trajectories
of T(3)are approximately linear at least for 0 > 1.5; see Heggland and Frigessi (2003) for details.
The ranges of the distribution of all three statistics increase in 0. T(2) varies less: for the largest value of 0 that was used, 0 = 5.4, T(2) is between 2.5 and 4.0; when 0 = 3, it is between 1.8
and 3.2. This confirms that T(2) is not a good auxiliary statistic. We estimate the derivative of
Eo(T(')) in 0= 3.0, as
50

50

(1/50)
30E(T(1))10=33

Ts(l)(3.02) - (1/50) 1 Ts()(2.98)

s=1

3.02- 2.98

s=1

1=1,2, 3,

and we obtain 0.99, 0.41 and 0.96 respectively.T(2)has the smallest derivative. T(3)has a slightly
smaller derivative than T(1) but a much smaller variance, which is consistent with the fact that
estimators that are based on this statistic seem to have a better performance.

4. Asymptotics
We now look at the asymptotic properties of the indirect inference estimator. Following
Gouri6roux et al. (1993), we can show that, as N -+ oc and under certain assumptions, the
quality of the indirect inference estimator OSNgiven in equation (5) depends on the gradient
of the expectation of the chosen statistics and on the covariance matrix. For completeness we
summarize the result here.
Theorem 1. Let TN{ZN(0)} be a vector of auxiliary statistics. Assume that the expectation
and covariance matrix have limits
lim (Eo[TN{zN(0)}]),
p(0) := N-oo
E(0) := N-limco(varo[N1/2TN

N (0)(O}]).

Assume that the chosen estimating function depends on the data only through TN, so that
there is some function f for which QN(ZN, /) = f[TN{ZN(0)}, /3]. Assume that this expression converges almost surely to the limit f {p(0), /}. Let 00 be the true value that is used to
generate the data and
o0= arg max[ff{((00), 3}].
Let OSN(QN) be the indirect estimator (5). Let QN converge almost surely to a limit Q as
N -+ oc. Then, under further regularity conditions that are stated in Appendix A, there is a
positive definite matrix W(S, Q) such that
N1/2 {OSN(QN) - O0}
-+--J{0,

in distribution as N -> oc for any fixed S. If
W(S, Q), then

"N

W(S, Q)}

is chosen so that its limit 0 =
-*

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

minimizes

IndirectInference

453

a2f

lm(Oo) a/3'
1+
(1?-1 a'l
amp
S[02f
{(
{(Oo),

W(S,*)=

[2f
x

-10
02 (Oo),/3

0)

{1(00),30

o}

{fI(0

2ff '
ao3
am, -(00)0-}f(0o)).

}(10)

When TN,3 and 0 have the same dimension, then
(
(11)
)
(o)
(8)
W(S,*)=(S)-(
independent of Q*. The matrix a/l'(0o)/0 has (i, j)th element 8ai(0)/ 0j computed in 00.
Similarly, entry (k,l) in 2f (P(0), 3}/a/ a is a2f{ u(O), 0}/1fk dl1.
=1 +

For a proof, see Appendix A.
The asymptotic covariance (11) is independent of the specific chosen estimating function f.
Most interestingly, it is proportional to (00O),the asymptotic covariance matrix of the chosen
statistic, and inversely proportional to the elements of apl'(0o)/a0, which are derivatives of the
limit of the expected value of the statistic.
When TN is a sufficient statistic for the true model and dim(TN) = dim(/) = dim(0), equation (11) is equal to the asymptotic variance of the maximum likelihood estimator, except for
the factor 1 + 1/S. Apart from this factor, the indirect inference estimator is efficient when the
auxiliary model is the true model or depends on the data only through the sufficient statistic
of the true model. When the dimensions of TN,13and 0 are not the same, equation (10) holds.
Again ap'(00)/a0 and E(00) are present, though in a more intricate way.
and E(0), simulating from
To compare different auxiliary statistics we can estimate apu'
(0)/00of
the true model in some value 0 which represents a first estimate
0, although an iterative
scheme (estimate 00, find the best statistic, estimate 00by using this, and so on) might be better.
In the X2-example in Section 3, we compare the asymptotic variance of indirect estimators
based on the statistics
T(1)=

__

T(4)

T

lN

E

Zn,

Nn=l
1N
N n=1

Z"

E (zn- ())2
N n=1

Since E(zi) =0, var(zi) =20, E(z ) - 0(0 + 2)(0 + 4) and E(z1) = 0(0 + 2)(0 + 4)(0 + 6), it
follows that
T
lim {Eo(T
))}=O,
lim { Eo(T4) } = 02 + 20,

Nli oo

lim
N-l oo{

} = 20,
Eo(TNS)

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

454

K.Hegglandand A. Frigessi
lim (var(N1/2T~))} = 20,

N-+oo

lim {var(N1/2T4))} = 80(0 + 2)(0 + 3),

N--+ oo

lim (var(N1/2T5))} = 480.

N--+ oo

We obtain for the indirect inference estimator of 0 based on T(1), T(4) and T(5) respectively
W1)(S) = (I + 120,
W(4)(S)=

(

1+1

S)

1\ 20 (0 + 2) (0 + 3)
(0+1)2
(

W(5)(S)-= (1+)

1)2

+

120.

We see that W(5)(S) > W(4)(S)> W(1)(S)for all 0 > 0. An indirect inference estimator based on
T(5) has an asymptotic variance that is six times larger than an estimator based on T(1). Here
estimators that are based on the mean are far better than estimators that are based on the
standard deviation.

5. Inferencefromdeparturetimes of a GIG/1queue
GIG/G/is a single-server, first come first serve queue with general distributions of the interarrival
and service times. Assume that only departure times are observed and that the distributions of
the arrival and service processes are known up to parameters. We show first that the likelihood
is intractable, in the sense that its evaluation requires a number of steps that are exponential in
the dimension N of the data.
Let Y, denote the interdeparture time corresponding to the nth customer. Let W, be the
corresponding interarrival time and U, the service time, independent of each other and with
finite expectation and variance. Let E(W) > E(U). The interarrivaland the service times have a
known parametric density
N
WN,

f(u,w)(UN,

0)

=

H
n=1

fu

0=

(un, Ou) fw(Wn, Ow),

Given the interdeparture times YN= (l, Y2,
...
departure time process { Y~} it holds that

,

YN) we perform inference on 0. For the intern

{

n

Un+

i=1

'r
Wi -

n-1

if E Wi<
Yi,
i=1
i=1

Un

=

(OuOw).

n-1

n

-'Yi
i=1

if

i=1

n-1

Wi >

(12)

i=1 Yi.

It is clear that {Y,, n = 1,2,... } is not in general a Markov process, since Yndepends on all the
past and the correlation structure is complex.
We need to evaluate repeatedly the likelihood
=
fO(YN)

J

fo(YNIuN,

WN) f(u,w)(uN

WN,

WN,0) dUN

dWN,

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

(13)

IndirectInference

455

which can be written as a 2N-dimensional integral, for a given data set YNand different parameter vectors 0. Here fo(YN IUN, WN) is a delta function, since YN given UN and

WN

is deterministic.

Hence all possible combinations of UNand WNneed to be identified, which can give rise to the
observed data. We construct a binary tree with root representing yl = ul . The next level has

two possibilities: Y2= u2 if Wl + w2 < U1; otherwise Y2= u2 - ul 1+ Wl + w2. We proceed like this
to construct a tree with 2N leaves. Each path from the root to a leaf describes a collection of

equalities and inequalities, which, if jointly solvable, represent one feasible choice of (UN,WN)
that is coherent with the data. At every level of the tree a new w-variable is added, so that the
intersection of all conditions along a path is in general not empty. (It can be empty in special
cases, e.g. when the interarrival density has finite support.) An exponential number of steps
(in N) must be performed to compute integral (13), which is in practice not feasible even for
moderately large N.
Billio et al. (1998) suggested an alternative approach to compute maximum likelihood estimates, called the simulated likelihood ratio method. Maximizing the likelihood fo(YN) is equivalent to maximizing the conditional expectation E[{ fo(YN, h)/f (YN,h) } YN] for a fixed value
0, where h is a set of latent variablesand the expectation is with respect to the conditional density
of h given YN and 0. Given samples h,... .,hs from this conditional density, the expectation
can be approximated by
is
hs)
E- {fO(YN,hS)/ f(yN,h
S s=1
If the full joint model fo(YN, h) can be evaluated, then there is a Markov chain Monte Carlo
algorithm that produces the required samples h1,..., hs, based only on the evaluation of ratios
f6(yN, h)/f6(yN, h). But the density fo(YN, UN, WN) of departure times cannot be evaluated in
practice.
Another approach to compute the likelihood is to condition on latent variables V,,representing the unobserved waiting times. It holds that
if V,,n-1+ Un-1 - W, > 0,
if V,_1 + U,,-1 - W,, 0.
(14)
The variables { Un1 - Wn}are independent and so is Un_1- W, of Vn_1.{ V,1}is a Markov chain
and it is possible to write an explicit expression of the density of the vector V,,. Conditioned on
the waiting times, the interdeparture time process satisfies

0Vn-1
S=

+ U,,n-1- W,

Unif
U,

Y=Un+ W,- Un-1- Vn-1_l

V, > 0,

if V,= 0.

Even conditioned on {V,n},the Y,2sare not a Markov chain. In expression (15) Wnand Un-1 are
not independent of Vn.Trying to condition on UN and WN leads to similar problems.
It might be possible to introduce different, less natural, latent variables leading to polynomial
ways to evaluate the likelihood, though we conjecture that the problem is in general NP hard; see
Garey and Johnson (1979). Fearnhead (2003) discussed special cases, including GIM/1 queues,
for which a polynomial algorithm can be designed. In Heggland and Frigessi (2003) we describe
a further approach, which is not based on equation (13), leading to an exponential algorithm.
Indirect inference does not require evaluating the likelihood. For specificity, assume that the
service times are uniformly distributed in the interval [O1,02] and that the interarrivaltimes are
exponentially distributed with parameter 03. We experiment with a data set consisting of N =
100 consecutive interdeparture times that are generated from the stable queue with 01=0.3,
02 = 0.9 and 03 = 1. We have created 50 independent replicas of the data, which we call the

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

456

K.Hegglandand A. Frigessi

original data. With 50 data sets we can obtain a reasonable idea of the distribution of the
various indirect inference estimators. For a fair comparison of the auxiliary models, the same
simulated data z'v(0) are used in all experiments. In our setting, indirect inference is based
on a set of statistics that can summarize the unknown data-generating distribution. Since the
mean service time is smaller than the mean interarrivaltime, the arrival and departure processes
should be similar. Hence we choose the mean of the interdeparture times as one such statistic.
The minimum interdeparturetime cannot be less than the minimum service time. As the service
is uniformly distributed, we take the minimum of the observed interdepartures.We analyse the
expected values and variances of these statistics by means of a graph that visualizes their variability. Let TN {ZN(0)} be any statistic of the simulated interdeparture times, where 0 = (01, 02,03).
We vary each parameter in turn while keeping the others fixed. Since we know the true value
(0.3, 0.9, 1.0), we simplify the exercise and assign these to the fixed parameters. In a realistic
setting some preliminary estimation must be performed (and possibly repeated iteratively). Say
03 is varied in the range [0.8, 1.3]. For each such value we sample zN(0.3, 0.9, 03) using the same
random numbers and compute the statistics TN{ZN(0) }. This produces a plot of TN{ZN(0)} as a
function of 03. The process is then repeated, say 50 times, each time with new random numbers,
to investigate the effect of the variability between different simulated data sets. We begin with
= 0.9, = 1),s= 1,...,50. Fig. 1(a) shows these 50
i(01,02 =0.9,03=- ) = (1/N)
03
N=Iz?(01,02
curves, as functions of 01. Similarly, Figs 1(b) and 1(c) show the variability of i as functions of
02 and 03. Changing the value of 01 or 02 does not affect -, whereas there appears to be a linear
dependence between 03 and Z. Next we consider the statistic Zmin= mini=l,... Nf{z(01, 02, 03)),
the minimum of the data generated. This is plotted in Figs l(d)-1(f) as functions of each parameter in turn. Changing the value of 02 and 03 does not affect the minimum, whereas there is a
linear dependence with 01. (Note the discontinuities of some curves, which would be smooth if
N were sufficiently large.) This indicates that the minimum and the mean of ZN could lead to
estimates of 01 and 03. Neither seems to carry information on 02. We have tried other statistics,
incuding the median Zmedof ZN. Figs l(g)-l(i) show that the median increases with 02, but its
variance is too large compared with the derivative of the expectation.
To find a statistic for the upper limit of the uniform distribution, we choose an auxiliary
model with parameters whose maximum likelihood estimates could act as a statistic. The joint
density c(y) of a single interdeparturetime from a steady state MIG/1 queue with independent
exponential interarrivalscan be computed; see Gross and Harris (1998), page 234. For uniform
service times this is
0
c(y)=

11I
02-01
2

ify<_01,
if 01 <
Y< 02,

1i-p
- p
(Y 01)
02-01 exp{_-03-1
[exp{-03-

(y-

2)} -exp{-03-1

(y -

1)}],

(16)

if 02 < Y-

We assume the auxiliary model with independent departure times each with marginal density
(16). We denote its three parameters as / = (31, /32, 33) where [Hi, /2] is the support of the
uniform density and /33is the exponential parameter. It is easy to see
t the maximum likelihood estimator of P1is the minimum of the dep ture times zmin. Let /2 denote the maximum
likelihood imator of/32. Figs 1(j)-l(1) show /2 (0) plotted against O1,82and 03, and for most
data sets 32 (0) seems linear in 02, whe~
rs 01 and 03 appear to have little effect.
We choose the statistic T = {2, zmin, /2 (ZN)}. An alternative approach is by means of the
. These two statistics are quite similar
auxiliary model (16), which does not use z but/3
and
This
leads
to
similar
(Heggland
Frigessi, 2003).
very
performances of the two variants.

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

IN

IN

IN

CD
- ---

?

Co

6

co

0

0.20

0.25

0.30
01

0.35

0.80

0.40

0.85

0.90

0.95

1.00

0.8

0.9

82

(b)

(a)

0

o

020

025

0.30

035

040

0.25

0.30

0.35

0.40

00

0

0.20

0

d085

090

0.80

0.85

0.90

095

100

0.95

1.00

08

09

0)

e1

02

(d)

(e)

0.8

0.9

0.8

0.9

-00

o)0

(1)
OD

0.20

0.25

0.30
01

0.35

0.40

(D
0

0.80

0.85

0.90

(j)

0.95

1.00

82

(h)

(d) Zmin versus81;(e) zmin versus 02; (f) Zmin versus 83; (
Fig. 1. (a) z versus01; (b) z versus 02; (C)Z versus03;ML
ML
ML versus
;; (
versus2 V2;()2Versus
(k) 2 versus
83
(i) Zmed versus 83;() 42 versus

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

458

K.Hegglandand A. Frigessi

Next we decide the estimating function (9) to use in equation (1). The asymptotic variance is
independent of the estimating function when the dimension of the statistic and of the parameter
are the same, as in this case. So we chose f(TN, /3) such that
3N = argmax[f {TN(YN), 3}] = TN(YN).
We use equation (5) to calculate OSN(QN), and we take QN as the inverse of the covariance
matrix of the statistic T, estimated from the original 50 data sets. Asymptotically the choice of
tN is uninfluential. In a real case, when only one data set is available, this would not be possible,
and we would first estimate the parameters 0 using the identity matrix as QN and then simulate
several data sets to estimate the covariance matrix of /3. See Gourieroux et al. (1993) for further
details on choosing 2N.
The optimization and estimation are performed as follows. First, we simulate data z"N(0)
from the G/G/1 queue on the grid of 0-values (0.20, 0.24,..., 0.40) x (0.80, 0.84,..., 1.04) x
For each grid point, a total of S = 50 data sets are generated. When
(0.80,0.88,...,1.28).
simulating for different values of 0 through a function z'?(0) = G(0, X), then only 0 is varied
while the random variables X are kept fixed. The estimate O(s=50),(N=100)of 0 for each data
set is found by using equation (5), where the maximization is over all 0-values of the grid. It
is in principle possible to perform a Newton-Raphson maximization or simulated annealing
instead of a plain grid-based search. In fact we refine the grid search, fitting by least squares a
local quadratic approximation, as also suggested by Diggle and Gratton (1984). The quadratic
surface is maximized to obtain the final indirect inference estimate for 0. See Heggland and
Frigessi (2003) for more details. This procedure gives us 50 values of the estimate O(S=50),(N=100)The estimated mean, standard deviation and RMSE, (1/50) E {((i - 0)2}1/2, can be calculated.
ML
In Table 2 we present results for the statistic T= {, Zmin, Z2 (ZN)}, which are very good.
The estimate of 03 does not improve when using the log-likelihood with independent marginals (16) i quation (7). There /33 appears instead of -.Although overall the statistic T=
{Z, Zmin, /2
(ZN)} seems best, we can obtain better estimates of 03 (in terms of small RMSE) by
as
using auxiliary model a mixture of two normal distributions, with five-dimensional parameter
/3= (Ail, ul,1P2, a2, p), two parametersfor each of the distributions and the mixture parameter p.
Interdeparturetimes are assumed independent. This model has no formal relationship to the true
model. We see from Table 2 that the estimate of 03 is better. We have tried a uniform-exponential
mixture as the auxiliary model, with independent interdeparturetimes. The auxiliary parameter
is four dimensional. This model performs less well, with RMSE 0.043, 0.076, 0.099 for the three
parameters. It is somewhat surprising that this model is worse than the mixture of normals,
Table2. Means,standarddeviationsand RMSEsof the parameterestimates,(a) using
T, (b) using an auxiliarymodelbased on independentmarginals(16) and (c) usinga
mixtureof normals
Means

Parameter

01
02
03

Standard deviations

RMSEs

(a)

(b)

(c)

(a)

(b)

(c)

(a)

(b)

(c)

0.301
0.886

0.301
0.883

0.302
0.868

0.011
0.040

0.011
0.041

0.035
0.060

0.011
0.042

0.011
0.043

0.035
0.068

0.982

1.005

0.987

0.100

0.110

0.091

0.100

0.109

0.091

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

IndirectInference

459

since the uniform and exponential components are more natural. Part of the interdeparture
times are uniformly distributed, whereas the rest follow some convolution of exponential and
uniform densities.
We conclude with a remark on the effects of a discontinuous likelihood function within this
framework. Consider the uniform-exponential mixture model and denote by a and b the parameters of the uniform component. These should be reasonable estimates for 01 and 02. When
we change 02 in the true model, this should be reflected in the estimates of b. Unfortunately, this
might not be necessarily so for small N. Consider the maximum likelihood estimate of b. By
construction, this will always be at one of the data points. Recall that we simulate data sets from
the true model by using the same random numbers, just changing the values of the parameters.
Say that we keep 01 and 03 constant, but change 02. Then the value of a given point in the data
set changes smoothly with 02. However, the maximum likelihood estimate of b may change
abruptly:even a small change in 02 may lead to a jump in the estimate of b. This discontinuity
of the estimated binding function makes indirect inference difficult. When N is sufficiently large
this problem is less serious, since data points will be closer to each other and, more generally,
the binding function is continuous. Furthermore the effect will also be reduced, but not solved,
by averagingthe estimates of the auxiliary parameters over several (we take 50) simulated data
sets. Note that this problem does not arise when using the normal mixture model, because there
the likelihood function is continuous, and hence the auxiliary parameters are smooth functions
of the true parameters.

6. Discussion
Indirect inference methods are useful tools for parameter estimation in models with intractable
likelihood functions. When indirect inference is based on a statistic, the asymptotic variance of
the indirect inference estimator is proportional to the asymptotic variance of this statistic, and
inversely proportional to the derivative of the expected value of this statistic, with respect to
the parameters. Thus the choice of estimating function should be guided by these criteria. The
effect on the quality of the estimate is significant. Selecting the right statistic is a difficult task,
and graphical methods are our best tool to select a statistic that is sensitive to changes in the
parameter 0, but not sensitive to random variations in samples that are generated with the same
0. Since numerical optimization is needed, the statistic should vary smoothly with 0. For small
sample sizes this may be a critical point.
What often makes likelihood functions difficult to derive is the complex dependence structure of the data. When it is possible to compute marginal distributions, and when these include
all the parameters of interest, then the product of these marginals can be maximized instead
of the full likelihood, thus ignoring dependence. This often leads to biased and non-efficient
estimators. An example is discretely observed diffusions; see Bibby and Sorensen (2001) and
Sorensen (2001) for example. In spatial Gibbs models, parameters of interest are usually present
in the interaction terms and the pseudolikelihood (Besag, 1986) can be maximized instead, again
leading to suboptimal estimates. In all these cases indirect inference based on these estimating
functions could be applied to correct the estimators for bias. One of the estimating functions
that is used for inference for the departure process from a G/G/1 queue is based on this idea
and performs well. Ignoring the dependence and maximizing the product of the marginals, we
obtain RMSEs that are equal to 0.015, 0.045 and 0.105 for O1,02 and 03 respectively.Comparing
this with Table 2 we see that this is similar to using indirect inference with this as an auxiliary
model. The main difference is in the estimates of O1,where the RMSE has increased as a result
of an increased bias.

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

K.Hegglandand A. Frigessi

460

Indirectinference methods havebeen considered in a frequentist setting. It would be very interesting to extend them towards a Bayesian context. Assume that prior information about 0 can be
summarizedin a prior density 7r(0).Let OsN(0) be computed following equations (3) or (4). Then
instead of using equation (6) we suggest computing argmaxo[QN{YN;/sN(O)} + log{rr(0)}] as
a Bayesian point estimator for 0. Similarly, equation (5) could be penalized.
Very recently, Genton and Ronchetti (2003) considered model misspecification and robustness of the indirect inference estimator. If the data YN do not follow exactly the model that is
assumed for simulation, how is indirectinference performing?They investigated local robustness
properties of the estimator and derived influence functions. This allowed them to design indirect
inference procedures that are stable in the presence of small deviations from the model assumed.

Acknowledgements
We are grateful to Magne Aldrin, who suggested the GIG/I example, and Ingrid Glad, Mette
Langaas and Bent Natvig for many interesting and fruitful discussions around indirect inference. This research was supported by the European Union network ERB-FMRX-CT96-0095
and by the Norwegian Research Council through programmes 121144/420 and BeMatA.

AppendixA
Forcompletenesswe givea versionof the mainresultin Gouri'rouxet al. (1993),whichrelatesthe asymptotic propertiesof OSN to derivativesof the bindingfunction.Assumethat the estimatingfunction QN
satisfies limNoo[Q N{zN(O0), 3}] = Q00(O0,0), dependingon the trueparameter 00.Let Qo be continuous
in / andwitha uniquemaximum/o = argmaxi{ Q, (00,/) }. Then N, is a consistentestimatorof /3o.Let
the bindingfunction
b(0) = arg max{ Q,(O, 3)}

b: Rk

R',

wherek and1 are the dimensionsof 0 and / respectively,be injectiveand let /3o= b(00).The k x l matrix
a2Q/a0 a/3'has (i, j)-entriesof the forma2Q,(0, )/lai, a•. Let the matrixab(0o)/a0'=
ab(O)/a0'16o=o
be of full columnrank.Assumethatthe truemodelis sufficientlyregularthat
N1/2 S aQN{Zs(00),3O}

N1/2aQN(yN,/0)

a/3

S =1

ao

is asymptotically
normalwithzeromeanandsomefiniteasymptoticcovariancematrixlimNo
Let the limits

{varo0(N) }.

lim (varo0[N1/2aQN{ZN(0),
o := N-+oo
O/30}/03])
and
Jo := N--oo
lim [-_2 QN{ZN(O0), /O}/03 a13 = -a2 QooQ(0o,
/o0)/O83a'
exist. Then, as N -+ o, the indirect estimator is consistent and asymptotically normal
N'/2 {N(

N)
- -

0}
--+

A/{0,

W(S, Q)},

where

W(S,)=

(?1)
1+

{b'
IS

a8b

ao
8o'(Oo)j
----(Oo)-

-b'

a

b
a (Oo)0JolloJol-&7(Oo)
8a'

r b'

ab

80O
aeo'
-(Oo)O(Oo)

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

-1

Indirect
Inference

461

Minimizing W(S,Q) with respect to 2 leads to the optimal weight function Q• = JoloJJo. With this choice
the asymptotic covariance matrix is equal to

w=

(

2

1)
1+W {

a

(00,00)}-1

Q2

(00o,o00)'i•
aa310
8O'
ap

S

When the dimensions of 0 and 0 are the same, we have W(S,Q) = Ws* V, and

WQ
= 1+

a

I 0O
oa/3'
l3(o)

,' )
O(00,

In theorem 1 derivatives of the binding function and the asymptotic estimating function are substituted
with quantities which relate to the statistics chosen.

A. 1. Proofof theorem 1
Since differentiation with respect to 3 and 0' gives

a2
a0l

ao

a

a2
Qoo(Oo,
ao
/3o)=f0)

(0(),/3 o} a -'(0o),

am,'

the first-orderexpansion of af/3aparound pL(0)is
af(TN, /3)

a3

~

/3}
{af (0O),

a2f/{(0),

a/

a3
,+..

a/3au'

{

TN -

P

(O)

},

which holds asymptotically in N. We can now derive expressions for Io and Jo as functions of pL(0)and
lo= limrn
[ (varooNT/2 aQN
N-+o
(
I
ao

N

(o)

= lim (varoo[N/2

TN (0

=

o}

lim[
N--oo

aa2f

{(00),
apl
La2f
a•p

])o

am {[(Oo),3o}

{N,/2 TN(0))
varoo

aa2f

}

afI {{p(0o),
o/3}
a1 au'
E(0o)amao'{bL(0),/3o
and

Jo=

a2f

• (•Oo),

00}.

The optimal choice of Q is

*=

f

'

(Oo

a apa'

Iaa3,p'
' 001o}I

{p(o0),/3o}IE(Oo0)
aaand the asymptotic variance of the indirect inference estimator is
W(S, Q*)=

1 +?
x

•)

a2f

(0o(0)

(Oo),o}
a•,ap•,
a-{

/3,{/(0o),/3o}
-1

2f,

a' a,,

(

}'

pa2f
a'a

o),

{/P(0o),/o} E(0o)
-1

}o)
{t(O)
' o} -O-(Oo)

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

462

K. Hegglandand A. Frigessi

If TN,/ and 0 all have the same dimension, we have

(1+}'

W

S+

i

2f/

})

(o0)
{)(•o),/3o}/

assuming that the matrices a2f{It(0),

0

2f

(.--1

o [(9o)
ar(einr

1

o}1
-

30}/a3 a8p'and ap'(90)/aO are invertible.

References
Besag,J. (1986) On the statisticalanalysisof dirtypictures(with discussion). R. Statist. Soc. B, 48, 259-302.
J. systems.In Frontiersin Queueing
Bhat, U. N., Miller,G. K. and Rao, S. S. (1997) Statisticalanalysisof queueing
(ed. J. H. Dshalalow),ch. 13, pp. 351-394. Boca Raton:CRC Press.
Bibby, B. M. and Sorensen,M. (2001) Simplifiedestimatingfunctionsfor diffusion models with a high-dimensional parameter.Scand.J Statist., 28, 99-112.
di C"a
Billio, M., Monfort,A. and Robert,C. P. (1998) The simulatedlikelihoodratio (SLR) method. UniversitY"
nj .nec. com/billio98simulated.html.)
Foscari,Venice.(Availablefromhttp: //citeseer.
Calzolari,G., Di Iorio, E and Fiorentini,G. (1998) Controlvariatesfor variancereductionin indirectinference:
interestratemodels in continuoustime. Econometr.J, 1, 100-112.
Calzolari,G., Di Iorio, F and Fiorentini, G. (2001) Indirectestimation and variancereductionusing control
variates. Metron, 59, 39-53.

Corduas,M. (1997) Indirectinferencefor fractionaltime seriesmodels.J Statist. Comput.Simuln,59, 221-232.
Diggle, P. J. and Gratton, R. J. (1984) Monte Carlo methods of inferencefor implicit statisticalmodels (with
discussion). J R. Statist. Soc. B, 46, 193-227.

Duffie, D. and Singleton,K. J. (1993) Simulatedmomentsestimationof Markovmodels of asset prices.Econometrica, 61, 929-952.

Fearnhead,P. (2003) Exact filteringfor partially-observedqueues. Manuscript.Universityof Lancaster,Lancaster.
Fermanian,J. D. and Salani&,B. (2001) A nonparametricsimulatedmaximumlikelihood estimation method.
Preprint.Centrede Rechercheen Economieet Statistique,Paris.
Gallant, A. R. and Tauchen,G. (1996)Whichmomentsto match?Econometr.Theory,12, 657-681.
Garey, M. R. and Johnson, D. S. (1979) Computers and Intractability. a Guide to the Theory of NP-completeness.

New York:Freeman.
Genton, M. G. and Ronchetti,E. (2003) Robustindirectinference.J Am. Statist. Ass., 98, 67-76.
EconometricMethods.Oxford:OxfordUniversityPress.
Gourieroux,C. and Monfort,A. (1996)Simulation-based
Gourieroux,C., Monfort,A. and Renault,E. (1993) Indirectinference.J Appl.Econometr.,8, s85-s118.
Gross, D. and Harris,C. M. (1998) Fundamentalsof QueueingTheory.New York:Wiley.
Hazelton,M. (1995)ImprovedMonte Carloinferenceformodelswith additiveerror.Statist. Comput.,5, 343-350.
Heggland,K. and Frigessi,A. (2003)Estimatingfunctionsin indirectinference.ResearchReport933. Norwegian
ComputingCenter,Oslo.
Jones,L. K. (1999) Inferringbalkingbehaviorfrom transactionaldata. OpsRes., 47, 778-784.
Kuk, A. Y C. (1995) Asymptoticallyunbiasedestimationin generalizedlinearmodels with randomeffects.J R.
Statist. Soc. B, 57, 395-407.

Luh, H. (1999) Derivationof the n-stepinterdeparturetime distributionin gi/g/1 queueingsystems.Eur.J Oper.
Res., 118, 194-212.
MacKinnon, J. G. and Smith, A. A. (1998) Approximatebias correction in econometrics.J Econometr.,85,
205-230.
McFadden,D. (1989)A methodof simulatedmomentsfor estimationof discreteresponsemodelswithoutnumerical integration.Econometrica,57, 995-1026.
Mealli, E and Rampichini,C. (1999) Estimatingbinarymultilevelmodels throughindirectinference.Comput.
Statist. Data Anal., 29, 313-324.

Pakes,A. and Pollard,D. (1989) Simulationand the asymptoticsof optimizationestimators.Econometrica,57,
1027-1057.
Pastorello,S., Renault,E. and Touzi,N. (2000) Statisticalinferencefor random-varianceoption pricing. Bus.
Econ. Statist., 18, 358-367.
J.
Schweder,T., Skaug, H. J., Langaas,M. and Dimakos, X. K. (1999) Simulatedlikelihoodmethods for complex
double-platformline transectsurveys.Biometrics,55, 678-687.
Sorensen,H. (2001)Discretelyobserveddiffusions:approximationof the continuous-timescorefunction.Scand
J Statist., 28, 113-122.

This content downloaded from 130.82.1.40 on Fri, 10 May 2013 04:31:29 AM
All use subject to JSTOR Terms and Conditions

