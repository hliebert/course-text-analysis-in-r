Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

Advance Access publication November 11, 2014

Political Analysis (2015) 23:180–196
doi:10.1093/pan/mpu017

Using the Predicted Responses from List Experiments as
Explanatory Variables in Regression Models
Kosuke Imai
Department of Politics, Princeton University, Princeton, NJ 08544
e-mail: kimai@princeton.edu (corresponding author)
Bethany Park
Department of Politics, Princeton University, Princeton, NJ 08544
email: bapark@princeton.edu
Kenneth F. Greene
Department of Government, University of Texas, Austin
email: kgreene@austin.utexas.edu
Edited by R. Michael Alvarez

The list experiment, also known as the item count technique, is becoming increasingly popular as a survey
methodology for eliciting truthful responses to sensitive questions. Recently, multivariate regression techniques
have been developed to predict the unobserved response to sensitive questions using respondent characteristics. Nevertheless, no method exists for using this predicted response as an explanatory variable in another
regression model. We address this gap by first improving the performance of a naive two-step estimator.
Despite its simplicity, this improved two-step estimator can only be applied to linear models and is statistically
inefficient. We therefore develop a maximum likelihood estimator that is fully efficient and applicable to a wide
range of models. We use a simulation study to evaluate the empirical performance of the proposed methods.
We also apply them to the Mexico 2012 Panel Study and examine whether vote-buying is associated with
increased turnout and candidate approval. The proposed methods are implemented in open-source software.

1

Introduction

In the social sciences, the list experiment, also known as the item count technique or unmatched
count technique, is becoming increasingly popular as an indirect questioning method to elicit
truthful answers to sensitive survey questions. List experiments have been used in various disciplines
to measure behaviors as varied as employee theft and drug use (e.g., Wimbush and Dalton 1997;
Biemer and Brown 2005). In political science, list experiments have been used to measure socially
undesirable attitudes such as prejudice concerning race, religion, and gender (e.g., Kuklinski, Cobb,
and Gilens 1997; Kane, Craig, and Wald 2004; Streb et al. 2008), and illicit conduct such as voter
fraud and vote-buying (e.g., Corstange 2012b; Gonzalez-Ocantos et al. 2012; Ahlquist, Mayer, and
Jackman 2013). List experiments have also been used to study seemingly less sensitive matters that
are still prone to measurement error, such as turnout and attitudes toward voting rights (e.g.,
Corstange 2009; Holbrook and Krosnick 2010). When successfully applied, list experiments can
enhance measurement validity by reducing social desirability bias that may result when survey
respondents are directly asked sensitive questions (e.g., Tourangeau and Yan 2007).
In recent years, scholars have also made several methodological advances that widen the applicability of list experiments. First, multivariate regression models have been developed to help

Authors’ note: The proposed methods are implemented via the open-source software list: Statistical Methods
for the Item Count Technique and List Experiments, which is available for download at the Comprehensive R
Archive Network (http://cran.r-project.org/package¼list). Supplementary materials for this article are available on the
Political Analysis Web site. The replication archive is available as Imai, Park, and Greene (2014). We thank Adam Glynn
and anonymous reviewers for helpful comments.
ß The Author 2014. Published by Oxford University Press on behalf of the Society for Political Methodology.
All rights reserved. For Permissions, please email: journals.permissions@oup.com

180

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

Using the Predicted Responses from List Experiments

181

researchers identify the respondent characteristics that are associated with certain answers to sensitive questions (Corstange 2009; Imai 2011; Blair and Imai 2012). Second, statistical methods have
been developed for testing whether the key assumptions of list experiments are violated and for
reducing the potential biases that arise from violation of such assumptions (Blair and Imai 2012).
Third, methodological recommendations have been made to improve the design and validity of list
experiments (Blair and Imai 2012; Glynn 2013; Blair, Imai, and Lyall 2014).
Despite these advances, no method currently exists for using the predicted responses to sensitive
items as an explanatory variable in another regression model. And yet, there are many instances in
which researchers wish to use responses to sensitive questions to predict behaviors or opinions. In
such cases, the central feature of list experiments becomes a key obstacle. Although the anonymity
aﬀorded to respondents enhances measurement validity, researchers do not directly observe their
responses to sensitive questions and thus cannot use such responses as predictors in regression
models. For example, Stokes (2005) and Stokes et al. (2013) argue that vote-selling aﬀects candidate
approval, whereas Nichter (2008) argues that it increases turnout. Other work has shown that list
experiments appear to improve the measurement of vote-selling behavior (Gonzalez-Ocantos et al.
2012). Nevertheless, no existing method allows researchers to use this information about voteselling in a regression model to predict candidate approval and turnout.
We ﬁll this gap in the methodological literature by ﬁrst improving the performance of a naive
two-step estimator, which we show is potentially biased and is too statistically ineﬃcient to be
useful in practice (Section 3.3). The improved two-step estimator we propose is relatively easy to
implement and is computationally stable. However, this estimator is not fully eﬃcient and, more
importantly, it can only be applied to linear models. To overcome these problems, we propose the
maximum likelihood (ML) estimator that is statistically eﬃcient and is applicable to a wide range of
models (Section 3.4). Both of our estimators build upon the general likelihood framework proposed
by Imai (2011) and Blair and Imai (2012). The proposed methods are implemented in the opensource software list: Statistical Methods for the Item Count Technique and List
Experiments, which oﬀers a comprehensive set of statistical methods for analyzing list experiments (Blair, Imai, and Park 2014).
We conduct a simulation study to assess the empirical performance of the proposed methods
(Section 4). We ﬁnd that especially in a small sample the one-step estimator, though computationally less stable than the improved two-step estimator, has less bias and is much more eﬃcient than
the two-step estimators. This improvement in statistical properties is particularly notable regarding
the coeﬃcient for the sensitive item. Both estimators signiﬁcantly outperform the naive two-step
estimator in terms of both bias and eﬃciency.
Finally, we apply the proposed methods to the Mexico 2012 Panel Study (Section 2), focusing on
the association between vote-selling and both turnout and candidate approval. This application
motivates the development of our new methodology. We ﬁrst show that the direct question leads to
severe under-reporting of vote-selling (around 6%) when compared to the list experiment (around
20%). These diﬀerences in measurement are reﬂected in the relationship between vote-selling and
candidate approval. Whereas a standard regression model using answers to the direct question
shows that vote-selling is associated with lower candidate approval, our proposed methodology
based on the list experiment implies that vote-sellers hold higher approval ratings. We also demonstrate that vote-selling is associated with lower turnout, even among party supporters that the
literature identiﬁes as prime targets for turnout buying. This ﬁnding suggests that parties may be
targeting voters who are unlikely to turn out in the ﬁrst place. This analysis illustrates that our
methodology can allow analysts to evaluate hypotheses about the eﬀects of sensitive attitudes or
behaviors that are well measured with list experiments.
2

Empirical Application: The Mexico 2012 Panel Study

In this section, we describe our motivating empirical application: the association between voteselling and both turnout and candidate approval in Mexico’s 2012 general elections. After discussing substantive arguments that drive our subsequent empirical analysis, we introduce the survey
instrument and conduct an initial descriptive analysis.

182

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

2.1

Kosuke Imai et al.

Vote-Selling, Turnout, and Candidate Approval

As scholarly interest in clientelism has burgeoned in political science, a primary task has been to
understand the eﬃcacy of vote-buying as a campaign strategy in elections around the world (e.g.,
Brusco, Nazareno, and Stokes 2004; Calvo and Murillo 2004; Stokes 2005; Penfold Becerra 2007;
Magaloni, Diaz-Cayeros, and Estévez 2007; Stokes et al. 2013). At the heart of this inquiry is a
concern over how vote-buying aﬀects both candidate approval and turnout, yet appropriate data
have been scarce and the proposed associations have been evaluated mainly with bivariate
relationships.
The most recent debate in the literature about vote-buying has focused on which voters receive
selective beneﬁts. Theoretical work establishes that machines should target swing voters (Lindbeck
and Weibull 1987; Stokes 2005). Consistent with this claim, Stokes et al. (2013) show that voters
who receive selective beneﬁts in Argentina, Mexico, Venezuela, and India hold higher evaluations
of the patron’s party. However, they also ﬁnd that brokers over-target loyalists who already hold
positive opinions of the party, indicating that vote-buying may only work for narrow slices of the
electorate. In contrast, Nichter (2008) argues that parties use selective beneﬁts to increase turnout
among unmobilized supporters. Such supporters may also be more likely to respond to selective
beneﬁts (Cox and McCubbins 1986).1 Thus, the current literature largely agrees that loyal voters
receive the most largesse but disagrees about whether selective beneﬁts mainly help increase
approval among swing voters or increase turnout among loyalists.
Although these core claims are theoretically compelling, empirical work has been hampered by
two problems, which our methodology addresses. First, most works rely on surveys that ask
directly whether respondents sold their vote (e.g., Cornelius 2004; Greene 2007; Diaz-Cayeros,
_
Estévez, and Magaloni 2009; Kramon 2009; Carreras and Irepoğlu
2013). The illegality of such
exchanges in many countries, including Mexico, likely makes respondents reticent to admit to voteselling. List experiments oﬀer one solution to this under-reporting problem by allowing respondents
to communicate truthful information without openly admitting to illicit behaviors. In their study of
vote-buying in Nicaragua, Gonzalez-Ocantos et al. (2012) show that only 2% of respondents
admitted they sold their vote when asked directly, whereas almost 25% did so when queried
through the list experiment. We follow the same measurement strategy by using a list experiment
on vote-selling.
Furthermore, even if list experiments are successfully implemented, researchers cannot simply
use this measure to predict turnout or voter opinions. The reason is that we do not observe for each
respondent whether they report selling their votes. The very feature of list experiments to provide a
degree of anonymity to respondents makes it impossible to estimate standard regression models in a
straightforward fashion. Although multivariate regression models have been developed to identify
respondent characteristics that are associated with certain answers to sensitive questions (Imai
2011; Blair and Imai 2012), simply taking the predicted probabilities from these models and
using them as an explanatory variable in the outcome regression, as shown later, fail to recover
the true coeﬃcient and result in the underestimation of standard error. To the best of our knowledge, no method exists to appropriately use the responses from list experiments as explanatory
variables in regression models to predict other outcomes, such as (in this case) turnout or opinions
about the candidates. The goal of our article is to ﬁll this methodological gap.
2.2

The List Experiment

We measure vote-selling and turnout using a list experiment embedded in the Mexico 2012 Panel
Study, designed and ﬁelded by Greene et al. (2012). Allegations of vote-buying featured prominently in the presidential election of that year. Enrique Peña Nieto of the formerly dominant
Institutional Revolutionary Party (PRI)—a party with a long history of electoral clientelism
(Cornelius and Craig 1991)—won 39% of the vote, besting Andrés Manuel López Obrador from
1

But see Dixit and Londregan (1996), who provide a general framework that generates two sets of conditions: those
under which parties will favor their core voters and those under which parties will target swing voters.

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

Using the Predicted Responses from List Experiments

183

a leftist coalition led by his Party of the Democratic Revolution (PRD) with 32%, and Joseﬁna
Vázquez Mota of the incumbent conservative National Action Party (PAN) with 26%. López
Obrador claimed that Peña Nieto won through a massive vote-buying scheme (see Greene [2014]
for details).
The 2012 panel study interviewed a nationally representative sample of ordinary citizens with a
valid voter registration card, before and after the election. All interviews were face-to-face. In this
article, we focus on the data from the postelection survey with a total of 1150 respondents who
answered the list experiment questions. Each respondent was randomized either to the control or to
the treatment group. The script for the control group was read aloud, as follows:
I am going to read you a list of three activities that appear on this
card and I want you to tell me how many of these activities you have
done in recent weeks. Please don’t tell me which ones, just HOW MANY.
The three activities are . . . [SHOW CARD AND READ]
(a) See television news that mentions a candidate
(b) Attend a campaign event
(c) Talk about politics with other people
Respondents assigned to the treatment group were read aloud the following script that includes an
additional sensitive item about vote-selling (item c below):
I am going to read you a list of four activities that appear on this
card and I want you to tell me how many of these activities you have
done in recent weeks. Please don’t tell me which ones, just HOW MANY.
The four activities are . . . [SHOW CARD AND READ]
(a)
(b)
(c)
(d)

See television news that mentions a candidate
Attend a campaign event
Exchange your vote for a gift, favor, or access to a service
Talk about politics with other people

We emphasize that the sensitive item speciﬁcally asks respondents whether they “exchanged”
their vote, not whether they “received” a beneﬁt. We believe that this wording better measures voteselling by eliminating exchanges that respondents did not deem worth their vote. Campaigns routinely deliver token beneﬁts to voters such as pins, stickers, key chains, and the like. If the data
scored the receipt of beneﬁts that voters consider to be tokens as vote-selling, we would likely
underestimate the eﬀect of vote-selling on outcomes of interest.
Table 1 summarizes the data from this list experiment and the direct question approach.
Estimating the proportion of vote-sellers using the list experiment is straightforward. Because
the two subsamples were selected at random, the mean number of nonsensitive activities that
respondents aﬃrmed should be equal across the two lists, implying that any diﬀerence in means
is attributable to vote-selling. An obtrusive measure of vote-selling was also asked later in the
survey; however, it is important to note that this question asked whether the respondent was
oﬀered a gift, service, or other beneﬁt in exchange for their vote, not whether they accepted such
an oﬀer. The lower bar for a positive response to the obtrusive measure should at least narrow the
gap between it and the list experiment. Nevertheless, the diﬀerence in results from the two measurement approaches is striking. The obtrusive measure indicates that just 5.5% of respondents
received a beneﬁt, whereas the list experiment reveals that a whopping 19.4% sold their vote. The
conﬁdence intervals are [4.2, 6.8] for the obtrusive measure and [18.1, 20.7] for the list experiment
estimate, respectively.
The validity of the estimate from the list experiment relies upon the assumption that respondents
do not lie about the sensitive item and that adding the sensitive item does not change responses to
the control item list (Imai 2011; Blair and Imai 2012). Although these assumptions are not directly
testable, Blair and Imai (2012) propose a statistical test that can be used to detect certain violations.

184

Kosuke Imai et al.
Table 1 Summary of the responses to list experiment and direct questioning about vote-selling

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

List experiment
Control group

Direct question

Treatment group

Response value

Frequency

Proportion

Frequency

Proportion

0
1
2
3
4
Non-response
Total

33
256
197
73

5.7%
44.5
34.3
12.7

16
575

2.8%
100

39
208
197
89
28
14
575

6.8%
36.1
34.3
15.5
4.9
2.4%
100

Response
value

Frequency

Proportion

No
Yes

1087
63

94.5%
5.5

Non-response
Total

0
1150

0%
100

According to the simple difference-in-means estimation, the list experiment indicates that 19.4% (with the 95% confidence interval of [18.1,
20.7] of respondents sold their votes, which is much higher than the corresponding figure (5.5% with [4.2, 6.8]) from the direct question.

This generates a p-value of 0.27, indicating no clear violation of the required assumptions. Like any
statistical test, this result may be due to the lack of statistical power. Given this promising initial
result, however, we proceed to develop a new statistical methodology, which allows researchers to
use the responses from list experiments as predictors in regression models.
3

The Proposed Methodology

In this section, we propose methods to use the responses from list experiments as predictors in an
outcome regression model. We begin by brieﬂy reviewing the multivariate regression analysis of list
experiments proposed by Imai (2011) and Blair and Imai (2012). We then propose two new estimators. First, we describe a two-step estimator, which uses the estimates from the aforementioned
multivariate regression model and improves the performance of a naive two-step estimator. Despite
its simplicity, the major limitation of this approach is that it cannot be generalized beyond simple
linear regression models. The estimator is also not statistically eﬃcient, often resulting in large
standard errors. To overcome these problems, we propose a general ML estimator that can be
computed in a single step. Although this estimator is more computationally challenging, it is fully
eﬃcient and is applicable to a wide range of statistical models.
3.1

The Setup

Suppose that we have a simple random sample of N respondents from a population. In the simplest
list experiment, each respondent is randomly assigned to either the treatment group, Ti ¼ 1, or the
control group, Ti ¼ 0. The respondents in the control group are asked to report the number of
aﬃrmative items from a list of J control items. Often, these control items are not sensitive, but that
is not a requirement. In contrast, the respondents in the treatment group are asked to count the
number of aﬃrmative items in the list of J þ 1 items, containing the same J control items and one
sensitive item of interest. Let Yi represent the reported count for this list experiment. For the
respondents in the control group, Yi takes an integer between 0 and J. For those in the treatment
group, Yi is an integer ranging from 0 to J þ 1. Finally, we observe a vector of K covariates for each
respondent, which is denoted by a column vector Xi 2 X , where X is the support of Xi.
The goal of this article is to develop new methods for estimating an outcome regression model,
whose predictors include the latent (i.e., unobserved) response to the sensitive item from a list
experiment. To formalize this, we follow the notation used in Imai (2011) and Blair and Imai
(2012) and denote the latent response to the j th item in the list as Zij . Without a loss of generality,
we assume that the sensitive item of interest appears at the end of the list, with Zi;Jþ1 representing
the latent response to the sensitive item. In practice, the order of items in list experiments is often
randomized to minimize the ordering eﬀect, and the proposed methodology is applicable regardless

185

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

Using the Predicted Responses from List Experiments

P
of a particular ordering of items. Finally, we use Yi ¼ Jj¼1 Zij to denote the number of aﬃrmative
responses to the J control items, which is observed for the respondents in the control group but may
be unknown for those in the treatment group. Thus, the relationship between the observed outcome
and these (possibly) latent variables is given by
Yi ¼ Yi þ Ti Zi;Jþ1 :

ð1Þ

Throughout this article, as mentioned in Section 2.2, we assume no design eﬀects and no liars; these
assumptions are formally described in Imai (2011) and Blair and Imai (2012).
Under this setup, researchers may be interested in using the latent responses from list experiments as predictors in regression models for an outcome variable Vi 2 V, where V is the support of
Vi. For example, one of the simplest such models is the following linear regression model:
Vi ¼  þ > Xi þ Zi;Jþ1 þ Yi þ i ;

ð2Þ

where the usual exogeneity assumption is made with respect to both observed and unobserved
variables, that is, Eði j Xi ; Zi;Jþ1 ; Yi Þ ¼ 0. Under this model, the main parameter of interest is the
coeﬃcient for the latent response to the sensitive item, that is, . Beyond this linear regression
model, researchers may be interested in estimating nonlinear models such as logistic regression,
where the responses from list experiments are used as predictors. The problem, of course, is that we
do not directly observe Zi;Jþ1 and Yi for many respondents. The goal of this article is to show how
to estimate these outcome regression models in a principled manner.
3.2

The Multivariate Regression Models for List Experiments: A Review

Next, we brieﬂy review the multivariate regression models developed by Imai (2011) and Blair and
Imai (2012) to analyze list experiments. These models form the basis of our new methodology
introduced in Sections 3.3 and 3.4. Imai (2011) and Blair and Imai (2012) propose a class of the
multivariate regression models that are represented by the following two submodels:
g ðxÞ ¼ Pr ðZi;Jþ1 ¼ 1 j Xi ¼ x; Þ

ð3Þ

h ðy j x; zÞ ¼ Pr ðYi ¼ y j Xi ¼ x; Zi;Jþ1 ¼ z; Þ

ð4Þ

for z ¼ 0, 1, y ¼ 0; 1; . . .; J, and x 2 X , where  and are vectors of unknown parameters.
The ﬁrst submodel given in equation (3) models the probability of an aﬃrmative response to the
sensitive item, whereas the second submodel given in equation (4) represents a model for the
response to the J control items given the response to the sensitive item. Both models include a
vector of covariates Xi. A common submodel for the sensitive item is logistic regression, that
is, g ðxÞ ¼ exp ð0 þ x> 1 Þ=f1 þ exp ð0 þ x> 1 Þg and  ¼ ð0 ; 1 Þ. Similarly, a binomial model
  with

a logistic link is often used as the submodel for the control items, that is, h ðy j x; zÞ ¼

J
y

pðx; zÞy

f1  pðx; zÞgJy ;
where
pðx; zÞ ¼ exp ð 0 þ 1 z þ x> 2 Þ=f1 þ exp ð 0 þ 1 z þ x> 2 Þg
and
¼ ð 0 ; 1 ; 2 Þ.
Imai (2011) and Blair and Imai (2012) show how to obtain consistent estimates of  and using
nonlinear least squares and ML estimation methods. In particular, they develop an expectationmaximization (EM) algorithm for computing fully eﬃcient ML estimates.
3.3

The Two-Step Estimator

We ﬁrst consider two-step estimators using the multivariate regression model described in Section
3.2 as the ﬁrst step. The main advantage of this estimator is its simplicity. However, the estimator is
only applicable when the outcome model is linear. In addition, the simplicity of the estimator comes
at the cost of statistical eﬃciency, as we demonstrate through a simulation study in Section 4. We
show that our proposed two-step estimator improves the performance of a more naive two-step
estimator.

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

186

Kosuke Imai et al.

We consider the linear outcome regression model given in equation (2). Note that the treatment
variable Ti is not included as a regressor in this model because the randomization makes it independent of ðXi ; Zi;Jþ1 ; Yi Þ. The inclusion of Ti, although possible, does not aﬀect large sample
statistical properties of our proposed methodology under the standard assumptions of list experiments. Moreover, it is also possible not to include Yi as a regressor. Here, we simply include it for
completeness. All of our theoretical results apply to the model without this variable.
3.3.1

The naive two-step estimator

We begin by the examination of a naive two-step estimator, which is an approach applied researchers may consider. In this approach, we ﬁrst ﬁt the multivariate regression models, such as
the one described in Section 3.2. Using the ﬁtted model, we then compute, for each respondent, the
predicted probability of aﬃrmatively answering the sensitive item as a function of respondents’

characteristics. We denote this quantity by Z^ i;Jþ1 ¼ g^ ðXi Þ (see equation (3)). Similarly, if Yi is
included in the outcome regression model, we may also estimate the expected response to the
P



control items, that is, Y^ i ¼ Jy¼1 yfh ^ ðy j Xi ; 1ÞZ^ i;Jþ1 þ h ^ ðy j Xi ; 0Þð1  Z^ i;Jþ1 Þg. In the second
step, we include these predicted values in the outcome regression. For example, we would ﬁt the
linear regression model given in equation (2) by replacing Zi;Jþ1 and Yi with their predicted values,


Z^
and Y^ , respectively.
i;Jþ1

i

Unfortunately, this naive two-step estimator suﬀers from several problems. First, the estimator
will be biased if the outcome regression is nonlinear. Second, the usual standard error will be
underestimated because it will not take into account the ﬁrst-stage estimation uncertainty, which
is typically large. Third, and perhaps most importantly, even if the outcome regression model is
linear so that the estimator is unbiased, the naive two-step estimator is too statistically ineﬃcient to


be useful in practice. The reason is that the predicted values, Z^ i;Jþ1 and Y^ i , are highly correlated
with other regressors in the model, Xi. In fact, if a simple linear regression is used in the ﬁrst stage,
the model cannot be ﬁtted, due to perfect collinearity. One way to circumvent this problem is to
exclude some covariates from the second-stage regression by treating them as instrumental variables. However, the assumption of such exclusion restriction is often diﬃcult to justify.
3.3.2

The new two-step estimator

We improve the naive two-step estimator by utilizing all the information from the list experiment.
As stated above, we focus on the case of linear outcome regressions. The naive two-step estimator is
based on the conditional expectation of the outcome given Xi. Here, we condition on all the
information from the list experiment, that is, (Yi, Ti), as well as the characteristics of respondents,
Xi. This yields the following regression equation:
EðVi j Xi ; Ti ; Yi Þ ¼  þ > Xi þ Pr ðZi;Jþ1 ¼ 1 j Xi ; Ti ; Yi Þ þ fYi  Ti Pr ðZi;Jþ1 ¼ 1 j Xi ; Ti ; Yi Þg;
ð5Þ
where we used the relationship given in equation (1) as well as the fact that the exogeneity assumption of the original regression model given in equation (2), that is, Eði j Zi;Jþ1 ; Yi ; Xi Þ ¼ 0, implies
Eði j Xi ; Ti ; Yi Þ ¼ EfEði j Zi;Jþ1 ; Yi ; Ti ; Xi Þ j Xi ; Ti ; Yi g ¼ EfEði j Zi;Jþ1 ; Yi ; Xi Þ j Xi ; Ti ; Yi g ¼ 0.
Furthermore, using Bayes’s rule, the unknown probability in equation (5) can be written as
PrðZi;Jþ1 ¼ 1 j Xi ; Ti ; Yi Þ ¼

h ðYi  Ti j 1; Xi Þg ðXi Þ
:
h ðYi  Ti j 1; Xi Þg ðXi Þ þ h ðYi j 0; Xi Þf1  g ðXi Þg

ð6Þ

Under this approach, the linear regression model given in equation (5) can be ﬁtted via the
following two-step procedure. First, we estimate the unknown parameters ð; Þ from the multivariate regression models g ðxÞ and h ðy j x; zÞ given in equations (3) and (4) using the estimation

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

Using the Predicted Responses from List Experiments

187

methods described in Imai (2011) and Blair and Imai (2012). Replace g ðxÞ and h ðy j x; zÞ in
equation (5) with their estimates, g^ ðxÞ and h ^ ðy j x; zÞ, and then ﬁt this outcome linear regression
model. We emphasize that the estimate of the probability given in equation (6) is readily available
from the E-step of the EM algorithm developed by Imai (2011) and Blair and Imai (2012). The
standard errors that incorporate the ﬁrst-stage estimation uncertainty can be calculated using either
the bootstrap resampling method or analytically based on the (straightforward but tedious) method
of moments framework (Imai 2011).

3.4

The General Full ML Estimator

Although the two-step estimator developed above improves the naive estimator, it still suﬀers from
the fact that the estimator is only applicable when the outcome model is linear. In addition, the
estimator is not yet statistically fully eﬃcient because the ﬁrst step does not use the information
contained in the outcome regression. Statistical eﬃciency is an important consideration when
analyzing list experiments because indirect questioning techniques such as list experiments reduce
bias at the cost of losing statistical eﬃciency.
To address this limitation, we develop the general full ML estimator. This one-step
estimator incorporates all the information from the data in the likelihood framework and therefore
is fully eﬃcient. The estimator is also general in that it can be extended to a wide range of outcome
models.
Consider the general outcome model deﬁned as follows:
f ðVi j Xi ; Yi ; Zi;Jþ1 Þ:

ð7Þ

For example, if the normal linear regression model is assumed, we will have


1
1


>

 2
f ðVi j Xi ; Yi ; Zi;Jþ1 Þ ¼ pﬃﬃﬃﬃﬃﬃ exp  2 ðVi     Xi  Zi;Jþ1  Yi Þ ;
2
2

ð8Þ

where  ¼ f; ; ; ; g. Given this outcome model, we can write the observed-data full likelihood
function as
Lobs ð; ;
¼

j fTi ; Vi ; Xi ; Yi gni¼1 Þ

n 
Y
1fY ¼0gTi
f ðVi j Xi ; Yi ; 0Þh ðYi j Xi ; 0Þf1  g ðXi Þg i
i¼1


1fY ¼Jþ1gTi
 f ðVi j Xi ; Yi  1; 1Þh ðYi  1 j Xi ; 1Þg ðXi Þ i

1f1Yi JgTi
 f ðVi j Xi ; Yi  1; 1Þh ðYi  1 j Xi ; 1Þg ðXi Þ þ f ðVi j Xi ; Yi ; 0Þh ðYi j Xi ; 0Þf1  g ðXi Þg

1Ti
 f ðVi j Xi ; Yi ; 1Þh ðYi j Xi ; 1Þg ðXi Þ þ f ðVi j Xi ; Yi ; 0Þh ðYi j Xi ; 0Þf1  g ðXi Þg
:
ð9Þ
Unfortunately, maximizing the logarithm of this observed-data likelihood function is diﬃcult due
to its complicated mixture structure. However, as done in the multivariate regression models for the
list experiments described in Section 3.2, the EM algorithm can be developed to solve this optimization problem.
To do this, ﬁrst we derive the complete-data likelihood function, which takes the following much
simpler form (when compared to the observed-data likelihood function given in equation (9)),
Lcom ð; ;
¼

n
Y

j fTi ; Vi ; Xi ; Yi ; Zi;Jþ1 gni¼1 Þ

f ðVi j Xi ; Yi  Ti ; 1Þh ðYi  Ti j Xi ; 1Þg ðXi Þ

Zi;Jþ1

i¼1


1Zi;Jþ1
 f ðVi j Xi ; Yi ; 0Þh ðYi j Xi ; 0Þf1  g ðXi Þg
:

ð10Þ

188

Kosuke Imai et al.

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

In each iteration, the M-step of the EM algorithm maximizes the conditional expectation of the
complete-data log-likelihood function, which is given here:
n
X
i¼1

~ i ; Yi ; Ti ; Vi Þ  log g ðXi Þ þ log f ðVi j Xi ; Yi  Ti ; 1Þ þ log h ðYi  Ti j Xi ; 1Þ
wðX


þ ð1  wi Þ log f1  g ðXi Þg þ log f ðVi j Xi ; Yi ; 0Þ þ log h ðYi j Xi ; 0Þ ;

ð11Þ

~ i ; Yi ; Ti ; Vi Þ is given by the following E-step,
where the weight wðX
Pr ðZi;Jþ1 ¼ 1 j Ti ¼ t; Vi ¼ v; Xi ¼ x; Yi ¼ yÞ
¼

f ðv j x; y  t; 1Þh ðy  t j x; 1Þg ðxÞ
:
f ðv j x; y  t; 1Þh ðy  t j x; 1Þg ðxÞ þ f ðv j x; y; 0Þh ðy j x; 0Þf1  g ðxÞg

ð12Þ

In the algorithm, this weight is evaluated using the values of the parameters ð; ; Þ obtained at the
previous iteration. Notice that this weight conditions on the outcome variable Vi, whereas
the weight used in the two-step estimator given in equation (6) does not. This diﬀerence is the
source of relative eﬃciency gain for this one-step estimator. Our EM algorithm starts after suitable
starting values are selected for all parameters, and it is iterated until convergence. The standard
errors are calculated analytically by computing the sample average of the cross-product of score
vectors. The detailed expression of this analytical asymptotic variance is provided in Supplementary
Appendix A.1.
Despite its attractive properties and ﬂexibility, the proposed ML estimator has a disadvantage
over the two-step estimator in that it is more diﬃcult to compute. The likelihood function exhibits a
complicated mixture structure, and as such the EM algorithm may end up in a local maximum.
Moreover, adding too many predictors may lead to the computational instability. A possible
solution is to develop a Markov chain Monte Carlo algorithm, but we leave this to future research.
4

A Simulation Study

We now conduct a simulation study to compare the two estimators proposed above, as well as the
naive two-step estimator, using a linear regression outcome model (so that all three estimators can
be compared on an equal footing). We sample a single covariate Xi from a univariate normal
distribution with mean 1 and variance 1. For the sensitive model g ðXi Þ, we use logistic regression
with an intercept and Xi, where the true values of the coeﬃcients are 0.5 and 1, respectively. For the
responses to the control items, we use binomial logistic regression h ðYi j Xi ; Zi;Jþ1 Þ, where the true
values of the coeﬃcients for an intercept Xi and the latent variable Zi;Jþ1 are both equal to 1.
Finally, the outcome model f ðVi j Xi ; Yi ; Zi;Jþ1 Þ is linear regression, where the true values for the
coeﬃcients, corresponding to an intercept, Xi, Yi , and Zi;Jþ1 , are equal to (–1, 1, 1, 0.5).
We examine three sample sizes: 1000, 1500, and 2500. For each sample size, we independently
simulate a total of 100,000 data sets using the data-generating process speciﬁed here. We ﬁt the
naive two-step, one-step, and two-step estimators to each simulated data set and then calculate
absolute median deviation and root mean squared error (RMSE) across these simulated data sets.
We use the absolute median deviation rather than bias as a measure of systematic error for each
estimator because the naive estimator is extremely variable and its bias cannot be estimated reliably
in a small sample size ever over 100,000 simulated data sets.
Figure 1 summarizes the results of this simulation study. Each row represents a parameter of the
outcome model. Note that RMSE is plotted using log (RMSE þ 0.01) for a large variance of the
naive two-step estimator though the axis labels are on the original scale. The dashed lines display
the results from the two-step estimator, whereas the solid lines represent those of the one-step
estimator. The dash-dotted lines represent the naive two-step estimator, which is consistently the
worst performer, in terms of both absolute median deviation and RMSE. Although, as expected,
both of our proposed estimators recover the truth as the sample size increases, the one-step estimator outperforms the two-step estimator in small sample sizes on both metrics. What is

189

Using the Predicted Responses from List Experiments

Root Mean Square Error
Naive two−step

0.25
0.05

0.02

Two−step
One−step

Two−step
(dashed line)
One−step
(solid line)

0

0.00

Intercept

0.04

1

Naive two−step

1500

2000

2500

1000

1500

2000

2500

1000

1500

2000

2500

1000

1500

2000

2500

1000

1500

2000

2500

1000

1500

2000

2500

1000

1500

2000

2500

1000

1500

2000

2500

0.05

0.02

0.25

0.04

1

0.06

1000

0

0.00

0.05

0.02

0.25

0.04

1

0.06

0

0.00

0.05

0.02

0.25

0.04

1

0.06

0

0.00

Covariate
Control Items
Sensitive Item

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

0.06

Absolute Median Deviation

Sample Size

Sample Size

Fig. 1 A simulation study comparing the performance of the one-step estimator (solid lines), the two-step
estimator (dashed lines), and the naive two-step estimator (dash-dotted lines). The sample size varies from
1000, 1500, to 2500. Although both proposed estimators significantly outperform the naive two-step estimator, the one-step estimator outperforms the proposed two-step estimator in terms of absolute median
deviation and RMSE especially for the coefficient of the sensitive item. The RMSE is transformed before
plotting; the plot shows log (RMSE þ 0.01), but the y-axis labels are on the original scale.

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

190

Kosuke Imai et al.

interesting, however, is that the performance diﬀerence between the two estimators is the greatest
for the coeﬃcient of the sensitive item, which is the main quantity of interest here. When the sample
size is 1000 (which is a typical sample size in social science research), we observe that the median
deviation and RMSE of the one-step estimator are less than half of those of the two-step estimator.
This clearly demonstrates the relative advantages of the one-step estimator, which is not only more
ﬂexible but also less biased and more eﬃcient.
5

Empirical Analysis

In this section, we apply the proposed one-step estimator to the Mexico 2012 Panel Study described
in Section 2. We assess the association between vote-selling and two core outcomes in the clientelism literature: turnout and approval for the winning presidential candidate, Enrique Peña Nieto.
5.1
5.1.1

Effects of Vote-Selling on Turnout
Data and model

We begin by introducing our measure of turnout. As an alternative to self-reported turnout that
often generates an overestimate (e.g., Burden 2000), enumerators in the 2012 Mexico Panel Study
asked respondents to show their voter registration card. Voters who showed cards that were
marked appropriately by poll workers were scored as having voted, whereas those whose cards
were unmarked were scored as nonvoters. Some respondents did not present cards when asked; we
code these as nonvoters. Whereas self-reported turnout was 84.9% in the sample, the alternative
approach measured turnout at 60.2%, coming much closer to the oﬃcial turnout of 64.5%.
In probing the plausibility of our new methodology, we provide a basic model of turnout that
includes the variables most often suggested by researchers in the ﬁeld. We include a set of standard
demographic variables: wealth, age, gender, and education (Wolﬁnger and Rosenstone 1980; Blais
2000). Citizens that are more engaged in politics are generally more likely to vote, so we also include
a measure of political interest as well as an enumerator-scored variable capturing whether or not
the respondent displayed political propaganda on the outside of their home.2 By the same token,
citizens that are disillusioned with politics are more likely to stay home, so we include a binary
measure of whether respondents believed the elections were clean or not (Domı́nguez and McCann
1995). All of these covariates are also used for the model that predicts vote-selling.3
In addition to these standard variables, we include measures of strong partisanship for the three
main parties to operationalize claims in the vote-buying literature, described in Section 2.1.
Although such operationalization is a nontrivial task, this measure is one way to assess the
argument of Nichter (2008) that bosses seeking higher turnout target selective beneﬁts to supporters. In this case, the inclusion of strong partisanship allows us to explore the argument of
Cox and McCubbins (1986) that core supporters respond to selective beneﬁts more than other
potential voters.4
5.1.2

Empirical findings

The estimated coeﬃcients from the model described above are presented in Supplementary
Appendix A.2. Here, we focus on the presentation of substantive results by calculating the
average predicted probabilities from the ﬁtted model.
2

We note that respondents who display such propaganda may be displaying their partisanship (see Corstange 2012a) or
as a way to seek patronage (Nichter and Palmer-Rubin 2015).
3
Unlike the candidate approval model discussed in Section 5.2, we do not include regional indicator variables, as their
inclusion appears to worsen the model fit when the outcome variable is turnout.
4
The question used to measure partisanship is a typical one used in Mexican polls and surveys: “Generally, do you
consider yourself to be a PANista, PRIista or PRDista? Do you think of yourself as a strong or weak [PANista/
PRIista/PRDista]?” Respondents who identified strongly with any of the three major parties were coded as party
supporters. All parties may engage in vote-buying, but the minor parties are less likely to be able to do so effectively.

191

Using the Predicted Responses from List Experiments

Difference
(Non−supporters −
Supporters)

0.6
0.4
0.2
0.0
−0.6 −0.4 −0.2

Non−supporters
Supporters

Vote−Selling and Turnout
Estimated Effect of Vote−Selling on Turnout

0.6
0.4
0.2
0.0

Predicted Probability of Vote−Selling

Overall

−0.6 −0.4 −0.2

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

Party Support and Vote−Selling

Non−supporters
Overall

Supporters
Difference
(Non−supporters −
Supporters)

Fig. 2 The predicted probability of vote-selling and the estimated effect of vote-selling on turnout from the
list experiment. The left panel displays the predicted probabilities of selling one’s vote with 95% confidence
intervals for the overall population, and for supporters and nonsupporters of the three main parties. There
is no statistically significant difference between party supporters and nonparty supporters. The right panel
displays the estimated effect of vote-selling on turnout with 95% confidence intervals for the same three
groups of voters. Among supporters and nonsupporters, those who sold their vote are less likely to turn out
to vote. Again, there is no statistically significant difference between party supporters and nonsupporters.

The left panel of Fig. 2 shows the predicted probability of vote-selling. We ﬁt two models
separately: the ﬁrst to a subsample of strong partisans and the second to the remaining sample.
Our models predict that about 33.2% of respondents (with a 95% conﬁdence interval of [24.7,
42.9]) sold their votes and that there is no statistically signiﬁcant diﬀerence between strong partisans
and others. Although there are many ways to measure the concept of a “core” voter, our ﬁndings
indicate that Mexico’s parties probably do not target selective beneﬁts to strong partisans more
than others, a ﬁnding that is inconsistent with a core-voter model such as Cox and McCubbins
(1986) or Nichter’s assertion that parties should buy the turnout of their loyal supporters. However,
it is consistent with other accounts that emphasize parties’ targeting of various constituencies, either
due to error (Stokes et al. 2013) or as part of a strategy (Magaloni, Diaz-Cayeros, and Estévez 2007;
Gans-Morse, Mazzuca, and Nichter 2014).
The right panel of Fig. 2 presents the results from the outcome model regression on turnout.
Vote-selling appears to decrease the probability of turnout by about 31.7 percentage points, on
average, regardless of partisanship (with the 95% conﬁdence interval of [19.8, 59.3] and [1.6, 47.0]
for strong partisans and others, respectively). Among both subsamples of party supporters and
nonsupporters, we ﬁnd a relatively strong negative correlation between vote-selling and turnout.
We caution that due to their small sample sizes the analyses of subsamples are less robust. For
example, the model ﬁtted to the subsample of party supporters produces large standard errors for
some parameters in the sensitive-item model. This could be an indication for poor model ﬁt given a
smaller sample size. Dropping these variables (i.e., urban and propaganda indicators) does not
substantively change the results, reducing the estimated probability of vote-selling somewhat from
34.8% to 24.9% (95% conﬁdence interval of [11.4%, 46.4%]). However, the estimated eﬀect of
vote-selling on turnout among party supporters under this alternative model speciﬁcation is
reduced to 5.5 percentage points with a 95% conﬁdence interval of [–38.6%, 20.5%].
At ﬁrst glance, these ﬁndings would seem to undermine the contention that vote-buying enhances
turnout, and instead support the much more disturbing idea that bosses buy abstention (Cox and
Kousser 1981). Though possible, ﬁeld reports do not support the claim of a massive abstention-buying
scheme in Mexico. Rather, if bosses direct beneﬁts to voters that are the least likely to turn out, and
their eﬀorts are not always successful, then vote-sellers may still be less likely to turn out, on average,
than nonsellers. This discrepancy can exist even if payoﬀs do increase the probability of turnout.
We note that straightforward behavioral ﬁndings like these have not been discussed in the
literature. Instead, existing studies have mainly focused on broad theoretical relationships
between vote-buying and turnout. Tools such as ours have a potential to move literatures
forward not only by assessing empirical relationships but by uncovering logical yet untheorized

192

Kosuke Imai et al.

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

ones. As Fig. 3 demonstrates, only by using the list experiment responses as a predictor—rather
than the responses to the direct question—can we detect an eﬀect of vote-selling on turnout.
5.2
5.2.1

Effects of Vote-Selling on Candidate Approval
Data and model

As discussed in Section 2.1, vote-selling may also be associated with candidate approval. In
Mexico’s 2012 election, Peña Nieto’s campaign was accused of trading selective beneﬁts for political
support. Such payoﬀs could be associated with higher approval ratings of the winning candidate if
voters responded to the beneﬁts. It is also possible that he targeted his party’s supporters, who
naturally hold him in higher esteem. However, although we ﬁnd an association between vote-selling
and approval of Peña Nieto, we ﬁnd no evidence that party-speciﬁc support is strongly associated
with vote-selling.
A limitation of the list experiment in the Mexico 2012 Panel Study is that it does not identify
which party or candidate bought the vote of the respondent. Mexico’s 2012 elections featured
simultaneous contests for president, Congress, and a host of state and local oﬃces. As a result,
party brokers could have oﬀered to buy citizens’ votes in exchange for their support of the entire
political party, not just the presidential candidate. In addition, respondents could have had downballot candidates in mind when responding to the list experiment. Nevertheless, presidential
coattails, the national campaigns’ control over advertising, and a dearth of split-ticket voting
lead us to conjecture that voters considered the presidential elections when responding to the list
experiment. In the follow-up to the direct vote-buying question, asking respondents which party
attempted to buy their support, many respondents said they received a payoﬀ from a party or
presidential campaign; none mentioned down-ballot candidates.
In order to explore the relationship between vote-selling and candidate approval, we ﬁt a linear
outcome regression model predicting the respondents’ opinion of Peña Nieto, measured on a scale
from 1 to 11, where 1 represents a least favorable opinion and 11 indicates a most favorable
opinion. This model includes the same set of variables as the turnout model, with a few additions.
First, we control for residence in the country’s four main geographic regions (North, Central,
South, and the Metropolitan area of Mexico City), because political support and vote-buying
strategies may vary by geographic region in Mexico. We also adjust for identiﬁcation with the
three main political parties, as separate indicators this time. This allows us to see whether those
identifying with Peña Nieto’s party are more likely to report selling their vote, and to adjust for
partisan identiﬁcation when predicting opinions about the candidate.
5.2.2

Empirical findings

We ﬁt the model described above using the responses to the direct question as well as the predicted
responses from the list experiment. The estimated coeﬃcients of these models are presented in
Supplementary Appendix A.3. Interestingly, the results from these two models are substantively
diﬀerent. The left panel of Fig. 4 presents the estimates based on the direct question. In this plot, we
observe that the estimated diﬀerence between vote-sellers and nonsellers is approximately 0.82
(with the 95% conﬁdence interval of [–1.50, 0.16]), suggesting that vote-selling may decrease one’s
approval rating for Peña Nieto. However, the conclusion based on the list experiment is the exact
opposite. In the right panel of the same ﬁgure, we see that although the estimated support remains
at a similar level, the diﬀerence between vote-sellers and nonsellers is estimated to be positive and
around 0.71 points (with the 95% conﬁdence interval of [–0.04, 1.47]). That is, as one would expect,
vote-sellers appear to assess Peña Nieto somewhat more highly.
The diﬀerent (and opposing) estimates generated by the two models indicate the real costs of
incorrectly measuring vote-selling. If we were to take the direct question model at face value, we
would conclude that vote-selling does not work as bosses intend and in fact backﬁres, contradicting
the theoretical and empirical claims made by the multiple authors we reviewed in Section 2. We do
not necessarily believe this conclusion is warranted. Rather, measurement error in the direct

193

Using the Predicted Responses from List Experiments

Vote Sellers Non−Sellers
Difference
(Vote sellers −
Non−sellers)

0.0

0.5

1.0

Overall

−0.5

0.0

0.5

Difference
(Vote sellers −
Non−sellers)

Predicted Probability of Turnout

Vote Sellers Non−Sellers

−0.5

Predicted Probability of Turnout

Overall

List Experiment

Fig. 3 Predicted turnout and the estimated effect of vote-selling on turnout from the direct question and
the list experiment. The figure displays the predicted turnout, among those who reported they had sold their
vote and those who did not, as measured by the direct question (left panel) and the list experiment (right
panel). The vertical bars represent 95% confidence intervals. According to the direct question, vote-sellers
are not measurably less likely to turn out than nonvote-sellers. However, the list experiment yields a
different conclusion: vote-selling has a negative impact of around 31.7 percentage points on the predicted
probability of turnout.

List Experiment
10

10

Direct Question

Non−Sellers

6

8

Vote Sellers

4

4

0
−2

2
0
−2

Difference
(Vote sellers −
Non−sellers)

2

Vote Sellers

Predicted Approval, Rating

Overall

Non−Sellers

6

8

Overall
Predicted Approval, Rating

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

1.0

Direct Question

Difference
(Vote sellers −
Non−sellers)

Fig. 4 The predicted approval rating of Enrique Peña Nieto (PRI) and the estimated effect of vote-selling
on the rating from the direct question and the list experiment. The figure displays the predicted approval
rating, which ranges from 1 (least supportive) to 11 (most supportive), of Enrique Peña Nieto, the winning
presidential candidate (of the PRI party), among those who reported they had sold their vote and those who
did not, as measured by the direct question (left panel) and the list experiment (right panel). The vertical
bars represent 95% confidence intervals. According to the direct question, vote-sellers rate the candidate
around 1 point lower than those who did not sell their vote. However, the list experiment yields the opposite
conclusion: vote-selling has a positive impact of 0.71 point on the candidate’s approval.

question is the likely culprit. A logistic model of response to the direct question demonstrates that
Peña Nieto supporters may have been less likely to admit that they received payoﬀs (the coeﬃcient
on PRI support is negative and signiﬁcant at the 0.1 level), whereas estimates from the sensitiveitem model from the list experiment show that his supporters were just as likely to sell their votes as
others (see Supplementary Appendix A.3 for full tables of coeﬃcients). We conclude that Peña
Nieto supporters declined to declare their activities openly; the legal battle over vote-buying that
continued after the election may have discouraged the winner’s supporters from admitting that they
received payoﬀs.

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

194

Kosuke Imai et al.

The list experiment not only provides a better measurement of vote-selling activity but also yields
the more intuitive result that vote-selling is associated with higher approval of Peña Nieto. His
party’s history of vote-buying (Cornelius and Craig 1991; Greene 2007) and other evidence from
ﬁeld research during the 2012 general election campaigns suggest that the Peña Nieto campaign
attempted to buy more votes than his rivals. The association between vote-selling and approval of
Peña Nieto could indicate either that his campaign targeted payoﬀs to his supporters or that voteselling worked as intended. The targeting hypothesis is less plausible given that PRI partisanship is
not associated with vote-selling in the sensitive-item model. It is more likely that vote-selling
increased evaluations of Peña Nieto, a ﬁnding consistent with Stokes et al. (2013) and that we
investigate further in other work (see Greene 2014).
6

Concluding Remarks

The list experiment has recently gained popularity among social scientists as a survey methodology
to elicit truthful response for sensitive questions. Although statistical methods have been proposed
to model responses to sensitive items as a dependent variable, no existing method has enabled the
use of the predicted responses from list experiments as the explanatory variables in an outcome
regression model. And yet, researchers may wish to ascertain the eﬀects of sensitive behavior and
attitudes on other actions and opinions. We address this methodological gap by proposing a new
estimation technique. Our one-step estimator can be applied to a wide range of outcome regression
models, thereby providing a ﬂexible tool for survey researchers.
The methodological approach proposed in this article can be extended to other settings. For
example, one can generalize this method to other designs of list experiments such as double list
experiments (Droitcour et al. 1991; Glynn 2013) or other indirect questioning methods such as
endorsement experiments (Bullock, Imai, and Shapiro 2011) and randomized response methods
(Warner 1965; Blair, Imai, and Zhou 2014). These and other related research projects are currently
being conducted by the authors and other researchers.
Finally, as the use of list experiments becomes popular, it is important for applied researchers
to take seriously the required assumptions. Statistical tests for detecting possible violations of such
assumptions and the modeling strategies for addressing them have been developed (Blair and
Imai 2012). A more eﬀective strategy, however, is to incorporate some type of empirical validation
within one’s research design. For example, researchers can use diﬀerent indirect questioning
techniques, such as endorsement experiments and randomized response methods, and examine
whether they provide similar estimates (Blair, Imai, and Lyall 2014). Another strategy is to aggregate individual-level list experiment results and validate them against available true information
(Rosenfeld, Imai, and Shapiro 2014). We believe that like any survey research, an extensive pilot
study and the use of diﬀerent data sources are essential for making empirical analyses credible.
Used carefully, list experiments and other indirect questioning techniques can be powerful tools
for understanding sensitive behaviors and attitudes in a variety of disciplines. The proposed method
increases the utility of the list experiment by allowing researchers to use sensitive information in
ways that were previously impossible.

References
Ahlquist, J., K. Mayer, and S. Jackman. 2013. Alien abduction and voter impersonation in the 2012 US general election:
Evidence from a survey list experiment. Technical Report, University of Wisconsin, Madison.
Biemer, P., and G. Brown. 2005. Model-based estimation of drug-use prevalence using item count data. Journal of Official
Statistics 21(2):287–308.
Blair, G., and K. Imai. 2012. Statistical analysis of list experiments. Political Analysis 20(1):47–77.
Blair, G., K. Imai, and J. Lyall. 2014. Comparing and combining list and endorsement experiments: Evidence from
Afghanistan. American Journal of Political Science 58(4):1043–63.
Blair, G., K. Imai, and B. Park. 2014. List: Statistical methods for the item count technique and list experiment.
Available at the Comprehensive R Archive Network (CRAN). http://CRAN.R-project.org/package¼list (accessed
June 1, 2014).

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

Using the Predicted Responses from List Experiments

195

Blair, G., K. Imai, and Y.-Y. Zhou. 2014. Design and analysis of randomized response technique. Technical report,
Pittsburgh, PA: Princeton University.
Blais, A. 2000. To vote or not to vote: The merits and limits of rational choice theory. Pittsburgh, PA: University of
Pittsburgh Press.
Brusco, V., M. Nazareno, and S. Stokes. 2004. Vote buying in Argentina. Latin American Research Review 39(2):66–88.
Bullock, W., K. Imai, and J. N. Shapiro. 2011. Statistical analysis of endorsement experiments: Measuring support for
militant groups in Pakistan. Political Analysis 19(4):363–84.
Burden, B. C. 2000. Voter turnout and the National Election Studies. Political Analysis 8(4):389–98.
Calvo, E., and M. Murillo. 2004. Who delivers? Partisan clients in the Argentine electoral market. American Journal of
Political Science 48(4):742–57.
_
Carreras, M., and Y. Irepoğlu.
2013. Electoral studies. Electoral Studies 32(4):609–19.
Cornelius, W. 2004. Mobilized voting in the 2000 elections: The changing efficacy of vote buying and coercion in Mexican
electoral politics. In Mexico’s Pivotal Democratic Election, eds. J. I. Dominguez and C. Lawson, 47–65. Stanford, CA:
Stanford University Press.
Cornelius, W., and A. Craig. 1991. The Mexican political system in transition. Monograph Series. La Jolla, CA: Center for
U.S.-Mexican Studies.
Corstange, D. 2009. Sensitive questions, truthful answers? Modeling the list experiment with LISTIT. Political Analysis
17(1):45–63.
———. 2012a. Religion, pluralism, and iconography in the public sphere: Theory and evidence from Lebanon. World
Politics 64(1):116–60.
———. 2012b. Vote-trafficking in Lebanon. International Journal of Middle East Studies 44:483–505.
Cox, G. W., and J. M. Kousser. 1981. Turnout and rural corruption: New York as a test case. American Journal of
Political Science 25(4):646–63.
Cox, G. W., and M. D. McCubbins. 1986. Electoral politics as a redistributive game. Journal of Politics 48(2):370–89.
Diaz-Cayeros, A., F. Estévez, and B. Magaloni. 2009. Welfare benefits, canvassing, and campaign handouts.
In Consolidating Mexico’s democracy: The 2006 presidential campaign in comparative perspective, eds. J. Domı́nguez,
C. Lawson, and A. Moreno, 229–45. Baltimore, MD: Johns Hopkins University Press.
Dixit, A., and J. Londregan. 1996. The determinants of success of special interests in redistributive politics. Journal of
Politics 58:1132–55.
Domı́nguez, J., and J. McCann. 1995. Shaping Mexico’s electoral arena: The construction of partisan cleavages in the
1988 and 1991 national elections. American Political Science Review 89(1):34–48.
Droitcour, J., R. A. Caspar, M. L. Hubbard, and T. M. Ezzati. 1991. The item count technique as a method of indirect questioning: A review of its development and a case study application. In Measurement errors in surveys, eds. P.
P. Biemer, R. M. Groves, L. E. Lyberg, N. A. Mathiowetz, and S. Sudman, 185–210. New York: John Wiley & Sons.
Gans-Morse, J., S. Mazzuca, and S. Nichter. 2014. Varieties of clientelism: Machine politics during elections. American
Journal of Political Science 58(2):415–32.
Glynn, A. N. 2013. What can we learn with statistical truth serum? Design and analysis of the list experiment. Public
Opinion Quarterly 77:159–72.
Gonzalez-Ocantos, E., C. K. de Jonge, C. Meléndez, J. Osorio, and D. W. Nickerson. 2012. Vote buying and social
desirability bias: Experimental evidence from Nicaragua. American Journal of Political Science 56(1):202–17.
Greene, K. F. 2007. Why dominant parties lose: Mexico’s democratization in comparative perspective. New York:
Cambridge University Press.
———. 2014. Back from the dead: Vote-selling zombies and the return of Mexico’s PRI. Paper presented at the Latin
American Studies Association Congress, Chicago.
Greene, K. F., J. Domı́nguez, C. Lawson, and A. Moreno. 2012. The Mexico 2012 Panel Study. Wave 2. Original public
opinion survey, available at http://kgreene.webhost.utexas.edu.
Holbrook, A. L., and J. A. Krosnick. 2010. Social desirability bias in voter turnout reports: Tests using the item count
technique. Public Opinion Quarterly 74(1):37–67.
Imai, K. 2011. Multivariate regression analysis for the item count technique. Journal of the American Statistical
Association 106(494):407–16.
Imai, K., B. Park, and K. F. Greene. 2014. Replication data for: Using the predicted responses from list experiments as
explanatory variables in regression models. The Dataverse Network. http://dx.doi.org/10.7910/DVN/27083 (accessed
June 1, 2014).
Kane, J. G., S. C. Craig, and K. D. Wald. 2004. Religion and presidential politics in Florida: A list experiment. Social
Science Quarterly 85(2):281–93.
Kramon, E. 2009. Vote-buying and political behavior: Estimating and explaining vote-buying’s effect on turnout in
Kenya. Afrobarometer Working Papers 1–31.
Kuklinski, J. H., M. D. Cobb, and M. Gilens. 1997. Racial attitudes and the “New South.” Journal of Politics
59(2):323–49.
Lindbeck, A., and J. W. Weibull. 1987. Balanced-budget redistribution as the outcome of political competition. Public
Choice 52(3):273–97.
Magaloni, B., A. Diaz-Cayeros, and F. Estévez. 2007. Clientelistm and portfolio diversification. Patrons, Clients, and
Policies: Patterns of Democratic Accountability and Political Competition, 182–204. New York, NY: Cambridge
University Press.

Downloaded from https://www.cambridge.org/core. Harvard University, on 01 Mar 2019 at 23:04:21, subject to the Cambridge Core terms of use, available at https://www.cambridge.org/core/terms. https://doi.org/10.1093/pan/mpu017

196

Kosuke Imai et al.

Nichter, S. 2008. Vote buying or turnout buying? Machine politics and the secret ballot. American Political Science
Review 102(1):19–31.
Nichter, S., and B. Palmer-Rubin. 2015. Clientelism, declared support, and Mexico’s 2012 campaign. In Mexico’s evolving
democracy: A comparative study of the 2012 elections, eds. J. Domı́nguez, K. Greene, C. Lawson, and A. Moreno,
220–26. Baltimore, MD: Johns Hopkins University Press.
Penfold Becerra, M. 2007. Clientelism and social funds: Evidence from Chávez’s Misiones. Latin American Politics and
Society 49(4):63–84.
Rosenfeld, B., K. Imai, and J. Shapiro. 2014. An empirical validation study of popular survey methodologies for sensitive
questions. Working paper available at http://imai.princeton.edu/research/Validate.html (accessed June 1, 2014).
Stokes, S. 2005. Perverse accountability: A formal model of machine politics with evidence from Argentina. American
Political Science Review 99(3):315–25.
Stokes, S., T. Dunning, M. Nazareno, and V. Brusco. 2013. Brokers, voters, and clientelism: The puzzle of distributive
politics. Cambridge Studies in Comparative Politics. Cambridge, UK: Cambridge University Press.
Streb, M. J., B. Burrell, B. Frederick, and M. A. Genovese. 2008. Social desirability effects and support for a female
American president. Public Opinion Quarterly 72(1):76–89.
Tourangeau, R., and T. Yan. 2007. Sensitive questions in surveys. Psychological Bulletin 133(5):859–83.
Warner, S. L. 1965. Randomized response: A survey technique for eliminating evasive answer bias. Journal of the
American Statistical Association 60(309):63–69.
Wimbush, J. C., and D. R. Dalton. 1997. Base rate for employee theft: Convergence of multiple methods. Journal of
Applied Psychology 82(5):756–63.
Wolfinger, R., and S. Rosenstone. 1980. Who votes? New Haven, CT: Yale University Press.

