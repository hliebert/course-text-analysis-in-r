Journal of the American Statistical Association

ISSN: 0162-1459 (Print) 1537-274X (Online) Journal homepage: https://www.tandfonline.com/loi/uasa20

Clustering, Spatial Correlations, and
Randomization Inference
Thomas Barrios , Rebecca Diamond , Guido W. Imbens & Michal Kolesár
To cite this article: Thomas Barrios , Rebecca Diamond , Guido W. Imbens & Michal Kolesár
(2012) Clustering, Spatial Correlations, and Randomization Inference, Journal of the American
Statistical Association, 107:498, 578-591, DOI: 10.1080/01621459.2012.682524
To link to this article: https://doi.org/10.1080/01621459.2012.682524

Accepted author version posted online: 14
May 2012.
Published online: 24 Jul 2012.
Submit your article to this journal

Article views: 1518

View related articles

Citing articles: 41 View citing articles

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=uasa20

Clustering, Spatial Correlations, and Randomization
Inference
Thomas BARRIOS, Rebecca DIAMOND, Guido W. IMBENS, and Michal KOLESÁR
It is a standard practice in regression analyses to allow for clustering in the error covariance matrix if the explanatory variable of interest
varies at a more aggregate level (e.g., the state level) than the units of observation (e.g., individuals). Often, however, the structure of the
error covariance matrix is more complex, with correlations not vanishing for units in different clusters. Here, we explore the implications
of such correlations for the actual and estimated precision of least squares estimators. Our main theoretical result is that with equal-sized
clusters, if the covariate of interest is randomly assigned at the cluster level, only accounting for nonzero covariances at the cluster level,
and ignoring correlations between clusters as well as differences in within-cluster correlations, leads to valid confidence intervals. However,
in the absence of random assignment of the covariates, ignoring general correlation structures may lead to biases in standard errors. We
illustrate our findings using the 5% public-use census data. Based on these results, we recommend that researchers, as a matter of routine,
explore the extent of spatial correlations in explanatory variables beyond state-level clustering.
KEY WORDS:

Clustered standard errors; Confidence intervals; Misspecification; Random assignment.

1. INTRODUCTION
Many economic studies that analyze the causal effects of interventions on economic behavior study interventions or treatments that are constant within clusters whereas the outcomes
vary at a more disaggregate level. In a typical example, and the
one we focus on in this article, outcomes are measured at the individual level, whereas interventions vary only at the state (cluster) level. Often, the effect of interventions is estimated using
least squares regression. Since the mid-1980s (Liang and Zeger
1986; Moulton 1986), empirical researchers in social sciences
have generally been aware of the implications of within-cluster
correlations in outcomes for the precision of such estimates. The
typical approach is to allow for correlation between outcomes in
the same state in the specification of the error covariance matrix.
However, there may well be more complex correlation patterns
in the data. Correlation in outcomes between individuals may
extend beyond state boundaries, it may vary in magnitude between states, and it may be stronger in more narrowly defined
geographical areas.
In this article, we investigate the implications, for the repeated sampling variation of least squares estimators based
on individual-level data, of the presence of correlation structures beyond those that are constant within and identical across
states, and vanish between states. First, we address the empirical question whether in census data on earnings with states
as clusters such correlation patterns are present. We estimate
general spatial correlations for the logarithm of earnings, and
find that, indeed, such correlations are present, with substantial correlations within groups of nearby states, and correlations
Thomas Barrios is a Graduate Student, Department of Economics, Harvard
University, Cambridge, MA 02138 (E-mail: tbarrios@fas.harvard.edu). Rebecca Diamond is a Graduate Student, Department of Economics, Harvard University, Cambridge, MA 02138 (E-mail: rdiamond@fas.harvard.edu). Guido
Imbens is Professor, Department of Economics, Harvard University and Research Associate, National Bureau of Economic Research, Cambridge, MA
02138 (E-mail: imbens@harvard.edu). Michal Kolesár is Graduate Student, Department of Economics, Harvard University, Cambridge, MA 02138 (E-mail:
mkolesar@fas.harvard.edu). Financial support for this research was generously
provided through NSF grants 0631252, 0820361, and 0961707. The authors
thank participants in the econometrics workshop at Harvard University, the referees, the editor, and the associate editor for comments, and, in particular, Gary
Chamberlain for helpful discussions.

within smaller geographic units (specifically public use microdata areas—pumas) considerably larger than within states. Second, we address whether accounting for such correlations is important for the properties of confidence intervals for the effects
of state-level regulations or interventions. We report theoretical
results, and demonstrate their relevance using illustrations based
on earnings data and state regulations, as well as Monte Carlo
evidence. The theoretical results show that if covariate values
are as good as randomly assigned to clusters, implying there
is no spatial correlation in the covariates beyond the clusters,
variance estimators that incorporate only cluster-level outcome
correlations remain valid despite the misspecification of the error covariance matrix. Whether this theoretical result is useful
in practice depends on the magnitude of the spatial correlations
in the covariates. We provide some illustrations that show that,
given the spatial correlation patterns we find in the individuallevel variables, spatial correlations in state-level regulations can
have a substantial impact on the precision of estimates of the
effects of interventions.
The article draws on three strands of literature that have
largely evolved separately. First, it is related to the literature on clustering, where a primary focus is on adjustments
to standard errors to take into account clustering of explanatory variables (see, e.g., Liang and Zeger 1986; Moulton 1986;
Bertrand, Duflo, and Mullainathan 2004; Hansen 2007; for textbook discussions, see Diggle et al. 2002; Wooldridge 2002;
Angrist and Pischke 2009). Second, the current article draws
on the literature on spatial statistics. Here, a major focus is on
the specification and estimation of the covariance structure of
spatially linked data (for textbook discussions, see Schabenberger and Gotway 2004; Gelfand et al. 2010). In interesting
recent works, Bester, Conley, and Hansen (2011) and Ibragimov and Müller (2010) link some of the inferential issues in the
spatial and clustering literatures. Finally, we use results from
the literature on randomization inference going back to Fisher
(1925) and Neyman (1990) (for a recent textbook discussion, see

578

© 2012 American Statistical Association
Journal of the American Statistical Association
June 2012, Vol. 107, No. 498, Applications and Case Studies
DOI: 10.1080/01621459.2012.682524

Barrios et al.: Clustering, Spatial Correlations, and Randomization Inference

Rosenbaum 2002). Although the calculation of Fisher exact pvalues based on randomization inference is frequently used in
the spatial statistics literature (e.g., Schabenberger and Gotway 2004), and sometimes in the clustering literature (Bertrand,
Duflo, and Mullainathan 2004; Abadie, Diamond, and Hainmueller 2010), Neyman’s approach to constructing confidence
intervals using the randomization distribution is rarely used in
these settings. We will argue that the randomization perspective
provides useful insights into the interpretation and properties of
confidence intervals in the context of spatially linked data.
The article is organized as follows. In Section 2, we introduce
the basic set-up. Next, in Section 3, using census data on earnings, we establish the presence of spatial correlation patterns
beyond the constant-within-state correlations typically allowed
for in empirical work. In Section 4, we discuss randomizationbased methods for inference, first focusing on the case with
randomization at the individual level. Section 5 extends the results to cluster-level randomization. In Section 6, we present
the main theoretical results. We show that if cluster-level covariates are randomly assigned to the clusters, the standard variance estimator based on within-cluster correlations can be robust to misspecification of the error covariance matrix. Next, in
Section 7, we show, using Mantel-type tests, that a number of
regulations exhibit substantial regional correlations, suggesting
that ignoring the error correlation structure may lead to invalid
confidence intervals. Section 8 reports the results of a small
simulation study. Section 9 concludes. Proofs are collected in
the Appendix.
2. FRAMEWORK
Consider a setting where we have information on N units,
say individuals in the United States, indexed by i = 1, . . . , N.
Associated with each unit is a location Zi , measuring latitude
and longitude for individual i. Associated with a location z are a
unique puma P (z) (puma—a Census Bureau defined area with
at least 100,000 individuals), a state S(z), and a division D(z)
(also a Census Bureau defined concept, with nine divisions in
the United States). In our application the sample is divided into
nine divisions, which are then divided into a total of 49 states (we
leave out individuals from Hawaii and Alaska, and include the
District of Columbia as a separate state), which are then divided
into 2057 pumas. For individual i, with location Zi , let Pi , Si ,
and Di , denote the puma, state, and division associated with the
location Zi . The distance d(z, z ) between two locations z and z
is defined as the shortest distance, in miles, on the Earth’s surface
connecting the two points. To be precise, let z = (zlat , zlong ) be
the latitude and longitude of a location. Then, the formula for
the distance in miles between two locations z and z we use is


) · cos(zlat ) · cos(zlat
)
d(z, z ) = 3,959 × arccos(cos(zlong −zlong

+ sin(zlat ) · sin(zlat
)).

In this article, we focus primarily on estimating the slope coefficient β in a linear regression of some outcome Yi (e.g., the
logarithm of individual-level earnings for working men) on a binary intervention or treatment Wi (e.g., a state-level regulation),
of the form
Yi = α + β · Wi + εi .

(2.1)

579

A key issue is that the explanatory variable Wi may be constant
within clusters of individuals. In our application, Wi varies at
the state level.
Let ε denote the N-vector with typical element εi , and let
Y, W, P, S, and D, denote the N-vectors with typical elements
Yi , Wi , Pi , Si , and Di . Let ιN denote the N-vector of ones, let
Xi = (1, Wi ), and let X and Z denote the N × 2 matrices with
ith rows equal to Xi and Zi , respectively, so that we can write
in matrix notation


(2.2)
Y = ιN · α + W · β + ε = X( α β ) + ε.
N
Let N = i=1 Wi , N0 = N − N1 , W = N1 /N , and Y =
N 1
i=1 Yi /N . We are interested in the distribution of the ordinary least squares estimators:
N
(Yi − Y ) · (Wi −W )
, and α̂ols = Y − β̂ols · W .
β̂ols = i=1N
2
i=1 (Wi − W )
The starting point is the following model for the conditional distribution of Y given the location Z and the covariate
W:
Assumption 1. (Model)
Y|W = w, Z = z ∼ N (ιN · α + w · β, (z)).
Under this assumption, we can infer the exact (finite sample)
distribution of the least squares estimator, conditional on the
covariates X, and the locations Z.
Lemma 1. (Distribution of Least Squares Estimator) Suppose Assumption 1 holds. Then β̂ols is unbiased and Normally
distributed,
E[β̂ols | W, Z] = β,

β̂ols |W, Z ∼ N (β, VM (W, Z)) ,
(2.3)

and

where
VM (W, Z) =

1
N2

2

· W · (1 −

× (Z)( ιN

(W
W )2


W
W)
.
−1

−1 )( ιN

W)





(2.4)

We write the model-based variance VM (W, Z) as a function of
W and Z to make explicit that this variance is conditional on
both the treatment indicators W and the locations Z. This lemma
follows directly from the standard results on least squares estimation and is given without proof. Given Assumption 1, the
exact distribution for the least squares coefficients (α̂ols , β̂ols )
is Normal, centered at (α, β) , and with covariance matrix
  −1  

−1
XX
X (Z)X X X . We then obtain the variance for
β̂ols in Equation (2.4) by writing out the component matrices of
the joint variance of (α̂ols , β̂ols ) .
It is also useful for the subsequent discussion to consider
the variance of β̂ols
, conditional on the locations Z, and conditional on N1 = N
i=1 Wi , without conditioning on the entire
vector W. With some abuse of language, we refer to this as
the unconditional variance VU (Z) (although it is still conditional on Z and N1 ). Because the conditional and unconditional expectation of β̂ols are both equal to β, it follows that

580

Journal of the American Statistical Association, June 2012

Table 1. Summary statistics for census data (N = 2,590,190)
Average log earnings
10.17
Standard deviation of log earnings
0.97
Number of pumas in the sample
2,057
Average number of observations per puma
1,259
Standard deviation of number of observations per puma
409
Number of states (including DC, excluding AK, HI, PR)
49
in the sample
Average number of observations per state
52,861
Standard deviation of number of observations per state
58,069
Number of divisions in the sample
9
Average number of observations per division
287,798
Standard deviation of number of observations
134,912
per division

the unconditional variance is simply the expected value of the
model-based variance:
VU (Z) = E[VM (W, Z) | Z]
N2
· E[(W − N1 /N · ιN ) (Z)
= 2
N0 · N12
× (W − N1 /N · ιN ) | Z].

where ij (Z, γ ) is the (i, j )th element of (Z, γ ). The first
variance component, σε2 , captures the variance of idiosyncratic
errors, uncorrelated across different individuals. The second
variance component, σS2 , captures correlations between individuals in the same state. Estimating σε2 and σS2 on our sample of 2,590,190 individuals by maximum likelihood leads to
σ̂ε2 = 0.929 and σ̂S2 = 0.016. The question addressed in this
section is whether the covariance structure in Equation (3.1)
provides an accurate approximation to the true covariance
matrix (Z). We provide two pieces of evidence that it
is not.
The first piece of evidence against the simple covariance matrix structure is based on simple descriptive measures of the
correlation patterns as a function of distance between individuals. For a distance d (in miles), define the overall, within-state,
and out-of-state covariances as
C(d) = E[Yi · Yj |d(Zi , Zj ) = d],
CS (d) = E[Yi · Yj |Si = Sj , d(Zi , Zj ) = d],

(2.5)

and
CS (d) = E[Yi · Yj |Si = Sj , d(Zi , Zj ) = d].

3. SPATIAL CORRELATION PATTERNS IN EARNINGS
In this section, we provide some evidence for the presence
and structure of spatial correlations, that is, how  varies with Z.
Specifically, we show in our application, first, that the structure
is more general than the state-level correlations that are typically
allowed for, and, second, that this matters for inference.
We use data from the 5% public use sample from the 2000 census. Our sample consists of 2,590,190 men, at least 20 years and
at most 50 years old, with positive earnings. We exclude individuals from Alaska, Hawaii, and Puerto Rico (these states share no
boundaries with other states, and as a result spatial correlations
may be very different than those for other states), and treat DC
as a separate state, for a total of 49 “states.” Table 1 presents
some summary statistics for the sample. Our primary outcome
variable is the logarithm of yearly earnings, in deviations from
the overall mean, denoted by Yi . The overall mean of log earnings is 10.17, and the overall standard deviation is 0.97. We do
not have individual-level locations. Instead, we know for each
individual only the puma of residence, and so we take Zi to be
the latitude and longitude of the center of the puma of residence.
Let Y be the variable of interest, in our case log earnings in
deviations from the overall mean. Suppose we model the vector
Y as
Y | Z ∼ N (0, (Z, γ )).
If researchers have covariates that vary at the state level, the
conventional strategy is to allow for correlation at the same level
of aggregation (“clustering by state”), and model the covariance
matrix as
ij (Z, γ ) = σε2 · 1i=j + σS2 · 1Si =Sj
⎧ 2
2
if i = j
⎪
⎨ σS + σε
2
if i = j, Si = Sj
= σS
⎪
⎩
0
otherwise,

(3.1)

If the model in Equation (3.1) was correct, then CS (d) should
be constant (but possibly nonzero) as a function of the distance
d, and CS (d) should be equal to zero for all d.
We estimate these covariances using averages of the products of individual-level outcomes for pairs of individuals
whose distance is within some bandwidth h of the distance
d:
1|d(Zi ,Zj )−d|≤h · Yi · Yj

C(d) =
i<j


C
S (d) =

1|d(Zi ,Zj )−d|≤h ,
i<j

1|d(Zi ,Zj )−d|≤h · Yi · Yj
i<j,Si =Sj

1|d(Zi ,Zj )−d|≤h ,
i<j,Si =Sj

and

C
S (d) =

1Si =Sj ·1|d(Zi ,Zj )−d|≤h ·Yi ·Yj
i<j

1|d(Zi ,Zj )−d|≤h .
i<j,Si =Sj


Figure 1(a) and (b) show the covariance functions for C
S (d)

(d)
for
the
bandwidth
h
=
50
miles
for
the
within-state
and C
S
and out-of-state covariances. (Results based on a bandwidth
h = 20 are similar.) The main conclusion from Figure 1(a)
is that within-state correlations decrease with distance.
Figure 1(b) suggests that correlations for individuals in different states are nonzero, also decrease with distance, and
are of a magnitude similar to within-state correlations. Thus,
these figures suggest that the simple covariance model in
Equation (3.1) is not an accurate representation of the true covariance structure.
As a second piece of evidence, we consider various parametric
structures for the covariance matrix (Z) that generalize Equation (3.1). At the most general level, we specify the following

Barrios et al.: Clustering, Spatial Correlations, and Randomization Inference

581

Figure 1. Covariance of demeaned log earnings of individuals as function of distance (in miles). Bandwidth h = 50 miles.

form for ij (Z, γ ):
ij (Z, γ )
⎧ 2
⎪σdist · exp(−α · d(Zi , Zj ))
⎪
⎪
⎪
⎪ + σD2 + σS2 + σP2 + σε2
⎪
⎪
⎪
⎪
2
⎪
σdist
· exp(−α · d(Zi , Zj ))
⎪
⎪
⎪
⎪
⎨ + σD2 + σS2 + σP2
=
2
σdist
· exp(−α · d(Zi , Zj ))
⎪
⎪
⎪
⎪
+
σD2 + σS2
⎪
⎪
⎪
⎪
⎪
2
⎪
σdist
· exp(−α · d(Zi , Zj )) + σD2
⎪
⎪
⎪
⎪
⎩ 2
σdist · exp(−α · d(Zi , Zj ))

if i = j ,
if i = j, Pi = Pj ,
if Pi = Pj , Si = Sj ,
if Si = Sj , Di = Dj ,
if Di = Dj .
(3.2)

Beyond state-level correlations, the most general specification
allows for correlations at the puma level (captured by σP2 )
and at the division level (captured by σD2 ). In addition, we
allow for spatial correlation as a smooth-function geographical distance, declining at an exponential rate, captured by
2
· exp(−α · d(z, z )). Although more general than the typσdist
ical covariance structure allowed for, this model still embodies

important restrictions, notably that correlations do not vary by
location. A more general model might allow variances or covariances to vary directly by the location z, for example, with
correlations stronger or weaker in the Western versus the Eastern United States, or in more densely or sparsely populated parts
of the country.
Table 2 gives maximum likelihood estimates for the covariance parameters γ given various restrictions, based on the
log earnings data, with standard errors based on the second
derivatives of the log-likelihood function. To put these numbers in perspective, the estimated value for α in the most
general model, α̂ = 0.0293, implies that the pure spatial com2
· exp(−α · d(z, z )), dies out fairly quickly: at a
ponent, σdist
distance of about 25 miles, the spatial covariance due to the
2
· exp(−α · d(z, z )) component is half what it is at zero
σdist
miles. The covariance of log earnings for two individuals in
the same puma is 0.080/0.948 = 0.084. For these data, the
covariance between log earnings and years of education is approximately 0.3, so the within-puma covariance is substantively
important, equal to about 30% of the log earnings and education
covariance. For two individuals in the same state, but in different
pumas and ignoring the spatial component, the total covariance

Table 2. Estimates for clustering variances for demeaned log earnings. Standard errors based on the second derivative of log-likelihood in
square brackets. Log lik refers to the value of the log-likelihood function evaluated at the maximum likelihood estimates. The last two columns
refer to the implied standard errors if the regressor is an indicator for high-state minimum wage (MW) or an indicator for the state being in New
England or the East-North-Central Division (NE/ENC)
s.e.(β̂)
σε2
0.931
[0.001]
0.929
[0.001]
0.868
[0.001]
0.929
[0.001]
0.868
[0.001]
0.868
[0.001]

σD2

σS2

σP2

2
σdis

α

Log lik

MW

NE/ENC

0

0

0

0

0

−1213298

0.002

0.002

0

0.016
[0.002]
0.011
[0.003]
0.011
[0.002]
0.006
[0.002]
0.006
[0.001]

0

0

0

−1200407

0.080

0.057

0.066
[0.002]
0

0

0

−1116976

0.068

0.049

0

0

−1200403

0.091

0.081

0

0

−1116972

0.081

0.076

0.021
[0.003]

0.029
[0.005]

−1116892

0.074

0.085

0
0.006
[0.002]
0.006
[0.003]
0.005
[0.005]

0.066
[0.002]
0.047
[0.002]

582

is 0.013. The estimates suggest that much of what shows up
as within-state correlations in a model such as Equation (3.1)
that incorporates only within-state correlations, in fact captures
much more local, within-puma, correlations.
To show that these results are typical for the type of correlations found in individual-level economic data, we calculated
results for the same models as in Table 2 for two other variables
collected in the census: years of education and hours worked.
Results for those variables are reported in an earlier version
of the article (Barrios et al. 2010). In all cases, puma-level
correlations are an order of magnitude larger than within-state
out-of-puma-level correlations, and within-division correlations
are of the same order of magnitude as within-state correlations.
The two sets of results, the covariances by distance and the
model-based estimates of cluster contributions to the variance,
both suggest that the simple model in Equation (3.1) that assumes zero covariances for individuals in different states, and
constant covariances for individuals in the same state irrespective of distance, is at odds with the data. Covariances vary substantially within states, and do not vanish at state boundaries.
Now we turn to the second question of this section, whether
the magnitude of the correlations we reported matters for inference. To assess this, we look at the implications of the models
for the correlation structure for the precision of least squares
estimates. To make this specific, we focus on the model in
Equation (2.1), with log earnings as the outcome Yi , and
Wi equal to an indicator that individual i lives in a state
with a minimum wage that is higher than the federal minimum wage in the year 2000. This indicator takes on
the value 1 for individuals living in nine states in our
sample—California, Connecticut, Delaware, Massachusetts,
Oregon, Rhode Island, Vermont, Washington, and DC—and
0 for all other states in our sample (see Figure 2(a)
for a visual impression). (The data come from the website http://www.dol.gov/whd/state/stateMinWageHis.htm. To be
consistent with the 2000 census, we use the information from
2000, not the current state of the law.) In the second to last column in Table 2, under the label “MW,” we report in each row
the standard error for β̂ols based on the specification for (Z, γ )
ˆ = (Z, γ̂ ) is the estimate for
in that row. To be precise, if 
(Z, γ ) in a particular specification, the standard error is



1
W
( ιN W ) (Z, γ̂ )
s.e.(β̂ols ) =
2
−1
2
2
N W (1 − W )
1/2

W
× ( ιN W )
.
−1
With no correlation between units at all, the estimated standard error is 0.002. If we allow only for state-level correlations,
Equation (3.1), the estimated standard error goes up to 0.080,
demonstrating the well-known importance of allowing for correlation at the level that the covariate varies. There are two general
points to take away from the column with standard errors. First,
the biggest impact on the standard errors comes from incorporating state-level correlations (allowing σS2 to differ from zero),
even though according to the variance component estimates
other variance components are substantially more important.
Second, among the specifications that allow for σS2 = 0, however, there is still a substantial amount of variation in the implied

Journal of the American Statistical Association, June 2012

standard errors. Incorporating only σS2 leads to a standard error
around 0.080, whereas also including division-level correlations
(σD2 = 0) increases that to approximately 0.091, an increase of
15%. We repeat this exercise for a second binary covariate, with
the results reported in the last column of Table 2. In this case, the
covariate takes on the value 1 only for the New England (Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island,
and Vermont) and East-North-Central states (Illinois, Indiana,
Michigan, Ohio, and Wisconsin), collectively referred to as the
NE/ENC states from here on. This set of states corresponds
to more geographical concentration than the set of minimum
wage states (see Figure 2(b)). In this case, the impact on the
standard errors of misspecifying the covariance structure (Z)
is even larger, with the most general specification leading to
standard errors that are almost 50% larger than those based on
the state-level correlations specification (Equation (3.1)). In the
next three sections, we explore theoretical results that provide
some insight into these empirical findings.
4. RANDOMIZATION INFERENCE
In this section, we consider a different approach to analyzing
the distribution of the least squares estimator, based on randomization inference (e.g., Rosenbaum 2002). Recall the linear
model in Equation (2.1),
Yi = α + β · Wi + εi ,

with

ε|W, Z ∼ N (0, (Z)).

In Section 2, we analyzed the properties of the least squares estimator β̂ols under repeated sampling. To be precise, the sampling
distribution for β̂ols was defined by repeated sampling in which
we keep both the vector of treatments W and the location Z
fixed on all draws, and redraw only the vector of residuals ε for
each sample. Under this repeated sampling thought-experiment,
the exact variance of β̂ols is VM (W, Z) as given in Lemma 1.
It is possible to construct confidence intervals in a different way, based on a different repeated sampling thoughtexperiment. Instead of conditioning on the vector W and Z, and
resampling the ε, we can condition on ε and Z, and resample
the vector W. To be precise, let Yi (0) and Yi (1) denote the potential outcomes under the two levels of the treatment Wi , and
let Y(0) and Y(1) denote the corresponding N-vectors. Then,
let Yi = Yi (Wi ) be the realized outcome. We assume that the
effect of the treatment is constant, Yi (1) − Yi (0) = β. Defining α = E[Yi (0)], the residual is εi = Yi − α − β · Wi . In this
section, we focus on the simplest case, where the covariate of
interest Wi is completely randomly assigned, conditional on

N
i=1 Wi = N1 .
Assumption 2. Randomization

pr(W = w | Y(0), Y(1), Z) = 1

N
N1


,
N

wi = N1 .

for all w s.t.
i=1

Under this assumption, we can infer the exact (finite sample)
variance for the least squares estimator for β̂ols conditional on
Z and (Y(0), Y(1)):

Barrios et al.: Clustering, Spatial Correlations, and Randomization Inference

583

Figure 2. Spatial correlation of regressors.

Lemma 2. Suppose that Assumption 2 holds and that the
treatment effect Yi (1) − Yi (0) = β is constant for all individuals. Then (1), β̂ols conditional on (Y(0), Y(1)) and Z is
unbiased for β,
E[β̂ols |Y(0), Y(1), Z] = β,

(4.1)

and, (2), its exact conditional (randomization-based) variance is
VR (Y(0), Y(1), Z) = V (β̂ols |Y(0), Y(1), Z)
=
where ε =

N

i=1 εi /N.

N
N0 · N1 · (N − 2)

N

(εi − ε)2 , (4.2)
i=1

Because this result direct follows from results by Neyman
(1990) on randomization inference for average treatment effects,
specialized to the case with a constant treatment effect, the proof
is omitted. Note that although the variance is exact, we do not
have exact Normality, unlike the result in Lemma 1.
In the remainder of this section, we explore two implications of the randomization perspective. First of all, although the
model and randomization variances VM and VR are exact if both
Assumptions 1 and 2 hold, they differ because they refer to different repeated sampling thought-experiments, or, alternatively,
to different conditioning sets. To illustrate this, let us consider
the bias and variance under a third repeated sampling thoughtexperiment, without conditioning on either W or ε, just conditioning on the locations Z and (N0 , N1 ), maintaining both the
model and the randomization assumption.

584

Journal of the American Statistical Association, June 2012

Lemma 3. Suppose Assumptions 1 and 2 hold. Then (1), β̂ols
is unbiased for β,
E[β̂ols |Z, N0 , N1 ] = β,
(2), its exact unconditional variance is

1
trace((Z))
VU (Z) =
N −2

1
N
−
,
ιN (Z)ιN ·
N · (N − 2)
N0 · N1

(4.3)

(4.4)

and (3),
VU (Z) = E [VR (Y(0), Y(1), Z)|Z, N0 , N1 ]
= E [VM (W, Z)|Z, N0 , N1 ] .
Thus, in expectation, VR (Y(0), Y(1), Z), is equal to the expectation of VM (W, Z).
For the second point, suppose we had focused on the repeated sampling variance for β̂ols conditional on W and Z, but
possibly erroneously modeled the covariance matrix as constant
times the identity matrix, (Z) = σ 2 · IN . Using such a (possibly incorrect) model, a researcher would have concluded that the
exact sampling distribution for β̂ols conditional on the covariates
would be
N
β̂ols |W, Z ∼ N (β, VINC ) , where VINC = σ 2 ·
.
N0 · N1
(4.5)
If (Z) differs from σ 2 · IN , then VINC is not in general the
correct (conditional) distribution for β̂ols . However, in some
cases the misspecification need not lead to invalid inferences
in large samples. To make that precise, we first need to define
precisely how inference is performed. Implicitly, the maximum
likelihood estimator for the misspecified variance defines σ 2 as
the probability limit of the estimator:


N
N
1
2
2
2
ln(σ ) −
(Yi − α̂ols − β̂ols Wi )
σ̂ = arg max
2
2σ 2 i=1
=

1
N

N

(Yi − α̂ols − β̂ols Wi )2 .
i=1

The probability limit for this estimator σ̂ 2 , under Assumptions
given in the Lemma below, is plim(trace((Z))/N). Then, the
probability limit of the normalized variance based on the possibly incorrect model is


N2
.
N · VINC = plim(trace ((Z))/N) plim
N0 · N1
The following result clarifies the properties of this probability
limit.

one can, at least in large samples, ignore the off-diagonal elements of (Z), and (mis-)specify (Z) as σ 2 · IN . Although the
resulting variance estimator will not be estimating the variance
under the repeated sampling thought-experiment that one may
have in mind (namely VM (W, Z)), it leads to valid confidence
intervals under the randomization distribution. The result that
the misspecification of the covariance matrix need not lead to
inconsistent standard errors if the covariate of interest is randomly assigned has been noted previously. Greenwald (1983,
p. 328) wrote: “when the correlation patterns of the independent variables are unrelated to those across the errors, then the
least squares variance estimates are consistent.” Angrist and
Pischke (2009, p. 311) wrote, in the context of clustering, that:
“if the [covariate] values are uncorrelated within the groups,
the grouped error structure does not matter for standard errors.”
The preceding discussion interprets this result formally from a
randomization perspective.

5. RANDOMIZATION INFERENCE WITH
CLUSTER-LEVEL RANDOMIZATION
Now let us return to the setting that is the main focus of the
article. The covariate of interest, Wi , varies only between clusters (states), and is constant within clusters. Instead of assuming
that Wi is randomly assigned at the individual level, we now
assume that it is randomly assigned at the cluster level. Let M
be the number of clusters, M1 the number of clusters with all
individuals assigned to Wi = 1, and M0 the number of clusters
with all individuals assigned to Wi = 0. The cluster indicator is

1 if individual i is in cluster/state m,
Cim = 1Si =m =
0 otherwise,
with C the N × M matrix with typical element Cim . For randomization inference, we condition on Z, ε, and M1 . Let Nm be the
number of individuals in cluster m. We now look at the properties
of β̂ols over the randomization distribution induced by this assignment mechanism. To keep the notation precise, let W̃ be the
M-vector of assignments at the cluster level, with typical element
W̃m . Let Ỹ(0)
 and Ỹ(1) be M-vectors, with mth
 element equal to
Ỹm (0) = i:Cim =1 Yi (0)/Nm , and Ỹm (1) = i:Cim =1 Yi (1)/Nm ,
respectively. Similarly,
let ε̃ be an M-vectorwith mth element

equal to ε̃m = i:Cim =1 εi /Nm , and let ε̃ = M
m=1 ε̃m /M.
Formally, the assumption on the assignment mechanism is
now:
Assumption 3. (Cluster Randomization)


M
,
pr(W̃ = w̃ | Z = z) = 1
M1
M

w̃m = M1 , and 0 otherwise.

Lemma 4. Suppose Assumption 1 holds with (Z)
satisfying trace((Z))/N → c for some 0 < c < ∞, and
ιN (Z)ιN /N 2 → 0, and Assumption 2 holds with N1 /N → p
for some 0 < p < 1. Then
c
p
p
.
N · (VINC −VU (Z)) −→ 0, and N · VINC −→
p · (1 − p)

We also make the assumption that all clusters are the same size:

Hence, and this is a key insight of this section, if the assignment
W is completely random, and the treatment effect is constant,

Lemma 5. Suppose Assumptions 3 and 4 hold, and the treatment effect Yi (1) − Yi (0) = β is constant. Then (1), the exact

for all w s.t.
m=1

Assumption 4. (Equal Cluster Size) Nm = N/M for all m =
1, . . . , M.

Barrios et al.: Clustering, Spatial Correlations, and Randomization Inference

sampling variance of βols , conditional on Z and ε, under the
cluster randomization distribution is
M

M
(ε̃m − ε̃)2 ,
VCR (Y(0), Y(1), Z) =
M0 · M1 · (M − 2) m=1
(5.1)
and (2) if also Assumption 1 holds, then the unconditional variance is
VU (Z) = E [VCR (Y(0), Y(1), Z)|Z, M1 ] =




M2
· M · trace C (Z)C − ι (Z)ι .
2
M0 · M1 · (M − 2) · N
(5.2)
The unconditional variance is a special case of the expected
value of the unconditional variance in Equation (2.5), with the
expectation taken over W given the cluster-level randomization.
6. VARIANCE ESTIMATION UNDER
MISSPECIFICATION
In this section, we present the main theoretical result in the
article. It extends the result in Section 4 on the robustness of
model-based variance estimators under complete randomization
to the case where the model-based variance estimator accounts
for clustering, but not necessarily for all spatial correlations, and
that treatment is randomized at cluster level.
Suppose the model generating the data is the linear model in
Equation (2.1), with a general covariance matrix (Z), and
Assumption 1 holds. The researcher estimates a parametric
model that imposes a potentially incorrect structure on the covariance matrix. Let (Z, γ ) be the parametric model for the
error covariance matrix. The model is misspecified in the sense
that there need not be a value γ such that (Z) = (Z, γ ). The
researcher then proceeds to calculate the variance of β̂ols as if
the postulated model is correct. The question is whether this
implied variance based on a misspecified covariance structure
leads to correct inference.
The example we are most interested in is characterized by a
clustering structure by state. In that case, (Z, γ ) is the N × N
matrix with γ = (σε2 , σS2 ) , where
⎧ 2
2
⎪
⎨ σε + σS if i = j


if i = j, Si = Sj ,
(6.1)
ij Z, σε2 , σS2 = σS2
⎪
⎩
0
otherwise.
Initially, however, we allow for any parametric structure
(Z, γ ). The true covariance matrix (Z) may include correlations that extend beyond state boundaries, and that may
involve division-level correlations or spatial correlations that
decline smoothly with distance as in the specification in
Equation (3.2).
Under the (misspecified) parametric model (Z, γ ), let γ̃ be
the pseudo-true value, defined as the value of γ that maximizes
the expectation of the logarithm of the likelihood function,
 


1
1
γ̃ = arg max E − · ln (det ((Z, γ )))− ·ε (Z, γ )−1 ε  Z .
γ
2
2
Given the pseudo-true error covariance matrix (γ̃ ), the
corresponding pseudo-true model-based variance of the least

585

squares estimator, conditional on W and Z, is
VINC,CR

1



W
=
2
−1
2
2
N W (1 − W )


W
.
× ( ιN W )
−1


( ιN



W ) (Z, γ̃ )

Because for some Z, the true covariance matrix (Z) differs
from the misspecified one, (Z, γ̃ ), it follows that in general this
pseudo-true conditional variance VM ((Z, γ̃ ), W, Z) will differ
from the true variance VM ((Z), W, Z). Here, we focus on the
expected value of VM ((Z, γ̃ ), W, Z), conditional on Z, under
assumptions on the distribution of W. Let us denote this expectation by VU ((Z, γ̃ ), Z) = E[VM ((Z, γ̃ ), W, Z)|Z]. The
question is under what conditions on the specification of the
error covariance matrix (Z, γ ), in combination with assumptions on the assignment process, this unconditional variance is
equal to the expected variance with the expectation of the variance under the correct error covariance matrix, VU ((Z), Z) =
E[VM ((Z), W, Z)|Z].
The following theorem shows that if the randomization
of W is at the cluster level, then solely accounting for
cluster-level correlations is sufficient to get valid confidence
intervals.
Theorem 1. (Clustering with Misspecified Error Covariance Matrix) Suppose Assumption 1 holds with (Z) satisfying trace(C (Z)C)/N → c for some 0 < c < ∞, and
ιN (Z)ιN /N 2 → 0, Assumption 3 holds with M1 /M → p for
some 0 < p < 1, and Assumption 4 holds. Suppose also that
(Z, γ ) is specified as in Equation (6.1). Then
p

N · (VINC,CR − VU (Z)) −→ 0, and
c
p
.
N · VINC,CR −→ 2
Nm · p · (1 − p)
This is the main theoretical result in the article. It implies that
if cluster-level explanatory variables are randomly allocated to
clusters, there is no need to consider covariance structures beyond those that allow for cluster-level correlations. In our application, if the covariate (state minimum wage exceeding federal
minimum wage) were as good as randomly allocated to states,
then there is no need to incorporate division- or puma-level correlations in the specification of the covariance matrix. It is, in
that case, sufficient to allow for correlations between outcomes
for individuals in the same state. Formally, the result is limited
to the case with equal sized clusters. There are few exact results
for the case with variation in cluster size, although if the variation is modest, one might expect the current results to provide
useful guidance.
In many econometric analyses, researchers specify the conditional distribution of the outcome given some explanatory
variables, and ignore the joint distribution of the explanatory
variables. The result in Theorem 1 shows that it may be useful to pay attention to this distribution. Depending on the joint
distribution of the explanatory variables, the analyses may be
robust to misspecification of particular aspects of the conditional
distribution. In the next section, we discuss some methods for
assessing the relevance of this result.

586

Journal of the American Statistical Association, June 2012

7. SPATIAL CORRELATION IN STATE AVERAGES
The results in the previous sections imply that inference is
substantially simpler if the explanatory variable of interest is
randomly assigned, either at the unit or cluster level. Here, we
discuss tests originally introduced by Mantel (1967) (see, e.g.,
Schabenberger and Gotway 2004) to analyze whether random
assignment is consistent with the data, against the alternative
hypothesis of some spatial correlation. These tests allow for the
calculation of exact, finite sample, p-values. To implement these
tests, we use the location of the units. To make the discussion
more specific, we test the random assignment of state-level
variables against the alternative of spatial correlation.
Let Ys be the variable of interest for state s, for s = 1, . . . , S,
where state s has location Zs (the centroid of the state). In
the illustrations of the tests, we use an indicator for a state-level
regulation, and the state-average of an individual-level outcome.
The null hypothesis of no spatial correlation in the Ys can be
formalized as stating that conditional on the locations Z, each
permutation of the values (Y1 , . . . , YS ) is equally likely. With S
states, there are S! permutations. We assess the null hypothesis
by comparing, for a given statistic M(Y, Z), the value of the
statistic given the actual Y and Z, with the distribution of the
statistic generated by randomly permuting the Y vector.
The tests we focus on in the current article are based on Mantel
statistics (e.g., Mantel 1967; Schabenberger and Gotway 2004).
These general form of the statistics we use is Geary’s c (also
known as a Black-White, or BW, statistic in the case of binary
outcomes), a proximity-weighted average of squared pairwise
differences:
S−1

S

(Ys − Yt )2 · dst ,

G(Y, Z) =

(7.1)

s=1 t=s+1

where dst = d(Zs , Zt ) is a nonnegative weight monotonically
related to the proximity of the states s and t. Given a statistic,
we test the null hypothesis of no spatial correlation by comparing the value of the statistic in the actual dataset, Gobs , to
the distribution of the statistic under random permutations of
the Ys . The latter distribution is defined as follows. Taking the
S units, with values for the variable Y1 , . . . , YS , we randomly
permute the values Y1 , . . . , YS over the S units. For each of the
S! permutations g, we re-calculate the Mantel statistic, say Gg .
This defines a discrete distribution with S! different values, one
for each allocation. The one-sided exact p-value is defined as
the fraction of allocations g (out of the set of S! allocations)
such that the associated Mantel statistic Gg is less than or equal
to the observed Mantel statistic Gobs :
1
p=
S!

S!

1Gobs ≥Gg .

(7.2)

A low value of the p-value suggests rejecting the null hypothesis
of no spatial correlation in the variable of interest. In practice,
the number of allocations is often too large to calculate the
exact p-value and so we approximate the p-value by drawing
a large number of allocations, and calculating the proportion
of statistics less than or equal to the observed Mantel statistic.
In the calculations below, we use 10,000,000 draws from the
randomization distribution.
We use six different measures of proximity. First, we define
the proximity dst as states s and t sharing a border:

1 if s, t share a border,
B
dst =
(7.3)
0 otherwise.
Second, we define dst as an indicator for states s and t belonging
to the same census division of states (recall that the United States
is divided into nine divisions):

1 if Ds = Dt ,
(7.4)
dstD =
0 otherwise.
The last four proximity measures are functions of the geographical distance between states s and t:
dstGD = −d (Zs , Zt ) ,

and

dstα = exp (−α · d (Zs , Zt )) ,
(7.5)

where d(z, z ) is the distance in miles between two locations z
and z , and Zs is the latitude and longitude of state s, measured as
the latitude and longitude of the centroid for each state. We use
α = 0.00138, α = 0.00276, and α = 0.00693. For these values,
the proximity index declines by 50% at distances of 500, 250,
and 100 miles.
We calculate the p-values for the Mantel test statistic based on
three variables. First, an indicator for having a state minimum
wage higher than the federal minimum wage. This indicator
takes on the value 1 in nine out of the 49 states in our sample,
with these nine states mainly concentrated in the North East
and the West Coast. Second, we calculate the p-values for the
average of the logarithm of yearly earnings. Third, we calculate
the p-values for the indicator for NE/ENC states. The results for
the three variables and six statistics are presented in Table 3. All
three variables exhibit considerable spatial correlation. Interestingly, the results are fairly sensitive to the measure of proximity.
From these limited calculations, it appears that sharing a border
is a measure of proximity that is sensitive to the type of spatial
correlations in the data.
8. A SMALL SIMULATION STUDY
We carried out a small simulation study to investigate the
relevance of the theoretical results from Section 6. In all cases,
the model was
Yi = α + β · Wi + εi ,

g=1

Table 3. p-values for Geary’s c, one-sided alternatives (10,000,000 draws)
Distance weights, 50% decline at:
Proximity
Minimum wage
Log wage
NE/ENC

Border

Division

Distance

500 miles

250 miles

100 miles

<0.0001
0.0005
<0.0001

0.0028
0.0239
<0.0001

0.9960
0.0692
0.0967

0.0093
0.0276
0.0877

0.0365
0.0298
0.0692

0.4307
0.1644
0.0321

Barrios et al.: Clustering, Spatial Correlations, and Randomization Inference

with N = 2,590,190 observations to mimic our actual data. In
our simulations, every state has the same number of individuals,
and every puma within a given state has the same number of
individuals. We considered three distributions for Wi . In all
cases, Wi varies only at the state level. In the first case, Wi = 1
for individuals in nine randomly chosen states. In the second
case, Wi = 1 for the the nine minimum wage states. In the third
case, Wi = 1 for the 11 NE/ENC states. The distribution for ε
is in all cases Normal with mean zero and covariance matrix .
The general specification we consider for  is
⎧ 2
σD + σS2 + σP2 + σε2 if i = j ,
⎪
⎪
⎪
⎪
⎪
⎨σ 2 + σ 2 + σ 2
if i = j, Pi = Pj ,
D
S
P
ij (Z, γ ) =
⎪
σD2 + σS2
if Pi = Pj , Si = Sj ,
⎪
⎪
⎪
⎪
⎩ 2
σD
if Si = Sj , Di = Dj .
(σε2 , σP2 , σS2 , σD2 ),

We look at two different sets of values for
(0.929, 0, 0.016, 0) (only state-level correlations, corresponding to the second pair of rows in Table 2), and
(0.868, 0.005, 0.005, 0.066) (puma-, state-, and division-level
correlations, corresponding to the fifth pair of rows in Table 2).
Given the data, we consider five methods for estimating the
variance of the least squares estimator β̂ols , and thus for constructing confidence intervals. The first is based on the randomization distribution:
M

M
2
V̂CR (Y(0), Y(1), Z) =
ε̃ˆ ,
M0 · M1 · (M − 2) m=1 m
where ε̃ˆ m is the average value of the residual ε̂i = Yi − α̂ols −
β̂ols · Wi over cluster m. The second, third, and fourth variances
are model-based:
1
ˆ
W, Z) =
(W − 1)( ιN W )
V̂M ((Z),
2
2
2
N · W · (1 − W )


W
ˆ
,
× (Z)( ιN W )
−1
ˆ
using different estimates for (Z).
First, we use an infeasible
estimator, namely the true value for (Z). Second, we specify
 2
σS + σε2 if i = j ,
ij (Z, γ ) =
σS2
if i = j, Si = Sj .
We estimate σP2 and σS2 using moment-based estimators, and
plug that into the expression for the covariance matrix. For the
third variance estimator, in this set of three variance estimators,
we specify
⎧ 2
2
2
2
⎪
⎪σD + σS + σP + σε if i = j ,
⎪
⎪
⎪
⎨σ 2 + σ 2 + σ 2
if i = j, Pi = Pj ,
D
S
P
ij (Z, γ ) =
⎪
σD2 + σS2
if Pi = Pj , Si = Sj ,
⎪
⎪
⎪
⎪
⎩ 2
σD
if Si = Sj , Di = Dj ,
and again use moment-based estimators.
The fifth and last variance estimator allows for more general
variance structures within states, but restricts the correlations
between individuals in different states to zero. This estimator
assumes  is block diagonal, with the blocks defined by states,
but does not impose constant correlations within the blocks. The

587

Table 4. Size of t-tests (in %) using different variance estimators for
models with only state-level correlations (S), and models with puma-,
state-, and division-level correlations (PSD; 500,000 draws)
Min.
Wage

Random

Treatment type
Shock type
V̂CR (Y(0), Y(1), Z)
V̂M ((Z), W, Z)
V̂M ((σ̂2 , σ̂S2 ), W, Z)
V̂M ((σ̂2 , σ̂P2 , σ̂S2 , σ̂D2 ), W, Z)
STATA

NE/ENC

S

PSD

S

PSD

S

PSD

5.6
5.0
6.1
6.1
7.6

5.6
5.0
6.1
6.5
7.6

5.6
5.0
6.1
5.7
8.5

16.2
5.0
17.1
9.0
18.5

5.6
5.0
6.1
5.4
7.7

26.3
5.0
27.2
13.8
30.4

estimator for  takes the form
⎧ 2
⎨ε̂i
ˆ STATA,ij (Z) = ε̂i · ε̂j

⎩
0

if i = j ,
if i = j, Si = Sj ,
otherwise,

leading to
V̂STATA =

1

· (W − 1)( ιN
N 2 · W (1 − W )2


W
.
× ( ιN W )
−1
2

W ) STATA (Z)

This is the variance estimator implemented in STATA and widely
used in empirical works.
In Table 4, we report the actual level of tests of the null hypothesis that β = β0 with a nominal level of 5%. First, consider the
two columns with random assignment of states to the treatment.
In that case, all variance estimators lead to tests that perform
well, with actual levels between 5.0% and 7.6%. Excluding the
STATA variance estimator, the actual levels are below 6.5%.
The key finding is that even if the correlation pattern involves
pumas as well as divisions, variance estimators that ignore the
division-level correlations do very well.
When we do use the minimum wage states as the treatment
group, the assignment is no longer completely random. If the
correlations are within state, all variance estimators still perform
well. However, if there are correlations at the division level,
now only the variance estimator using the true variance matrix
does well. The estimator that estimates the division-level correlations does best among the feasible estimators, but, because
the data are not informative enough about these correlations to
precisely estimate the variance components, even this estimator exhibits substantial size distortions. The same pattern, but
even stronger, emerges with the NE/ENC states as the treatment
group.
9. CONCLUSION
In empirical studies with individual-level outcomes and statelevel explanatory variables, researchers often calculate standard
errors allowing for within-state correlations between individuallevel outcomes. In many cases, however, the correlations may
extend beyond state boundaries. Here, we explore the presence
of such correlations, and investigate the implications of their
presence for the calculation of standard errors. In theoretical
calculations we show that under some conditions, in particular random assignment of regulations, correlations in outcomes

588

Journal of the American Statistical Association, June 2012

between individuals in different states can be ignored. However,
state-level variables often exhibit considerable spatial correlation, and ignoring out-of-state correlations of the magnitude
found in our application may lead to substantial underestimation of standard errors.
In practice, we recommend that researchers explicitly explore
the spatial correlation structure of both the outcomes and the
explanatory variables. Statistical tests based on Mantel statistics,
with the proximity based on shared borders, or belonging to a
common division, are straightforward to calculate and lead to
exact p-values. If these test suggest that both outcomes and
explanatory variables exhibit substantial spatial correlation, we
recommend that one should explicitly account for the spatial
correlation by allowing for a more flexible specification than
one that only accounts for state-level clustering.

 N


= σ2 det(C C) det C /σ2


 N−M 
= σ2
Np det(C ).


Lemma A.2. Suppose Assumptions 3 and 4 hold. Then for any
N × N matrix ,


M1 · (M1 − 1) 
M1 · M0
· ι ιN +
· trace C C .
E[W W] =
M · (M − 1) N
M · (M − 1)
Proof of Lemma A.2. We have

M1 /M
if ∀m, Cim = Cj m ,
E[Wi · Wj ] =
(M1 · (M1 − 1))/(M · (M − 1)) otherwise.
it follows that



M1 · (M1 − 1)
M1 · (M1 − 1)
M1
· ιN ιN +
−
· CC
M · (M − 1)
M
M · (M − 1)
M1 · (M1 − 1)
M1 · M0
· ιN ιN +
· CC .
=
M · (M − 1)
M · (M − 1)

E[WW ] =

APPENDIX: PROOFS
For the general model, leaving aside terms that do not involve unknown parameters, the log-likelihood function is
1
L(γ |Y) = − ln (det((Z, γ ))) − ε −1 (Z, γ )ε/2.
2
The matrix (Z, γ ) is large in our illustrations, with dimension
2,590,190 by 2,590,190. Direct maximization of the log-likelihood
function is therefore not feasible. However, because locations are measured by puma locations, (Z, γ ) has a block structure, and calculations
of the log-likelihood simplify and can be written in terms of first and
second moments by puma. We first give a couple of preliminary results.
Theorem A.1. (Sylvester’s Determinant Theorem). Let A and B be
arbitrary M × N matrices. Then:
det(IN + A B) = det(IM + BA ).

M

Proof of Theorem A.1. Consider a block matrix ( M13


det

M1
M3

M2
M4









M2
M4 ).

Then:

I
M1−1 M2
0
det
I
0 M4 − M3 M1−1 M2


= det M1 det M4 − M3 M1−1 M2 .
= det

M1
M3

Similarly,



I
M1 M2
= det
det
M3 M4
0



M2
M1 − M2 M4−1 M3
det
M4
M4−1 M3


= det M4 det M1 − M2 M4−1 M3 .

0
I



Thus,
E[W W]


= trace E[WW ]



M1 · M0
M1 · (M1 − 1)
= trace  ·
· ιN ιN +
· CC
M · (M − 1)
M · (M − 1)


M1 · M0
M1 · (M1 − 1) 
· ιN ιN +
· trace C C .
=
M · (M − 1)
M · (M − 1)


Lemma A.3. Suppose the N × N matrix  satisfies
 = σε2 · IN + σC2 · CC ,
where IN is the N × N identity matrix, and C is an N × M matrix of
zeros and ones, with CιM = ιN and C ιN = (N/M)ιM , so that,
⎧
⎪
σ 2 + σC2 if i = j
⎪
⎨ ε
(A.1)
ij = σC2
if i = j, ∀m, Cim = Cj m ,
⎪
⎪
⎩0
otherwise,
Then, (1)





 
N σC2
· 2 ,
ln (det ()) = N · ln σε2 + M · ln 1 +
M σε

and, (2)
−1 = σε−2 · IN −

σC2
· CC
σε2 · (σε2 + σC2 · N/M)

Letting M1 = IM , M2 = −B, M3 = A , M4 = IN yields the result. 
Lemma A.1. (Determinant of Cluster Covariance Matrix). Suppose
C is an N × M matrix of binary cluster indicators, with C C equal to
a M × M diagonal matrix, is an arbitrary M × M matrix, and IN is
the N-dimensional identity matrix. Then, for scalar σε2 , and
 = σ2 IN + C C
we have

C =

+ σ2 (C C)−1 ,

 N−M
det(C C) det(C ).
det() = σ2

Proof of Lemma A.1. By Sylvester’s theorem:


 N
det() = σ2 det IN + C /σ2 C


 N
= σ2 det IM + C C /σ2


 N
= σ2 det IM + C CC /σ2 − IM

Proof of Lemma A.3. First, consider the first part. Apply Lemma A.1
with
N
= σC2 · IM , and C C =
· IM ,
M

M
· IM .
so that C = σC2 + σε2 ·
N
Then, by Lemma A.1, we have
 
ln det() = (N − M) · ln σε2 + M · ln(N/M) + ln det(C )


 
M
= (N − M)·ln σε2 +M ·ln(N/M)+M ·ln σC2 +σε2 ·
N


 2
N 2
σ + σε2
= (N − M) · ln σε + M · ln
M C


 
N σC2
· 2 .
= N · ln σε2 + M · ln 1 +
M σε

Barrios et al.: Clustering, Spatial Correlations, and Randomization Inference

Next, consider part (2). We need to show that



 2
σC2

= IN ,
·
CC
σε ·IN + σC2 · CC σε−2 ·IN − 2
σε · (σε2 + σC2 · N/M)
which amounts to showing that
σε2 · σC2
 · CC + σC2 · CC σε−2
·
+ σC2 · N/M
σ4
 · CC = 0.
− CC · 2  2 C 2
σε · σε + σC · N/M
−

σε2



σε2

This follows directly from the fact that C C = (N/M) · IM and collect
ing the terms.
Proof of Lemma 3. The unbiasedness result directly follows from the
conditional unbiasedness established in Lemma 2. Next, we establish
the second part of the Lemma. By the Law of Iterated Expectations,
VU (Z) = V (E[β̂ols | Y(0), Y(1), Z] | Z, N1 )
+ E[V (β̂ols |Y(0), Y(1), Z) | Z, N1 ]
= E[V (β̂ols |Y(0), Y(1), Z) | Z, N1 ],

(A.2)

where the second line follows since β̂ols is unbiased. By Lemma 2. we
have
E[V (β̂ols |Y(0), Y(1), Z) | Z, N1 ]

N
= E
N0 · N1 · (N − 2)



N

(εi − ε)2 | Z, N1 .

589

 N 2
is the probability limit of N
i=1
i=1 εi /N, which is the probability
limit of trace((Z)/N ). Then


N
1
N2
2
ε ·
plim (N · VINC ) = plim
N i=1 i N0 · N − 1


trace((Z))
c
N2
= plim
=
·
.
N
N0 · N1
p · (1 − p)
Now consider Equation (A.4). By the conditions in the Lemma,


1
N2
1
trace((Z)) −
ιN (Z)ιN ·
N · VU =
N −2
N · (N − 2)
N0 · N1
1
N2
N
N
· trace((Z)) ·
=
−
N −2 N
N0 · N1
N −2
N2
c
1 
p
.
→
· 2 ιN (Z)ιN ·
N
N0 · N1
p · (1 − p)

Proof of Lemma 5. To show the first part of the Lemma, observe
that under constant cluster size,
M
¯)
(Ỹm − Ỹ¯ )2 (W̃m − W̃
β̂ols = m=1
¯ 2
m (W̃m − W̃ )


where Ỹm = (N/M)−1 i:Cim =1 Yi , and Ỹ¯ = M −1 m Ỹm = Ȳ , and
¯ = W̄ . Therefore, we can apply Lemma 2. treating cluster averages
W̃
(Ỹm , W̃m , ˜m ) as a unit of observation, which yields the result.
To show the second part, again by Lemma 2. β̂ols is unbiased, so
that by the Law of Iterated Expectations, and the first part of the
Lemma,

i=1

Observe that we can write
N

(εi − ε)2 = (ε − ιN ιN ε/N ) (ε − ιN ιN ε/N )

VU (Z) = V (E[β̂ols | Y(0), Y(1), Z] | Z, M1 )
+ E[V (β̂ols | Y(0), Y(1), Z) | Z, M1 ]
= E[V (β̂ols | Y(0), Y(1), Z) | Z, M1 ]


M

2
M
ε̃m − ε̃ | Z, M1
=E
(M − 2) · M0 · M1 m=1

i=1

= ε ε − 2ε ιN ιN ε/N + ε ιN ιN ιN ιN ε/N 2
= ε ε − ε ιN ιN ε/N.
Hence
N
E[ε ε − ε ιN ιN ε/N |Z, N0 , N1 ]
N0 · N1 · (N − 2)
N
trace(E[εε − ιN εε ιN /N |Z, N0 , N1 ])
=
N0 · N1 · (N − 2)
N
(trace((Z)) − ιN (Z)ιN /N )
=
N0 · N1 · (N − 2)

VU (Z) =

which establishes Equation (4.4). Finally, we prove the third part of
the Lemma. By Lemma 1, β̂ols is unbiased conditional on Z, W,
so that by argument like in Equation (A.2) above, we can also
write:
VU (Z) = V (E[β̂ols | Z, W] | Z, N1 ) + E[V (β̂ols |Z, W) | Z, N1 ]
= E[V (β̂ols |Y(0), Y(1), Z) | Z, N1 ]
which equals E [VR (Y(0), Y(1), Z) | Z, N1 ] by (A.2).

c
,
p · (1 − p)

ε̃m = (C C)−1 C ε =

M 
C ε,
N

and

ε̃ =

1 
1
ι (C C)−1 C ε = ιN ε,
M M
N

so that
 

M 
M 
1
1
C ε − ιM ιN ε
C ε − ιM ιN ε
N
M
N
M
  
 

M 
1
1
M 
C − ιM ιN ε
C − ιM ιN ε .
=
N
N
N
N

 

1
M
M 
1


C − ιN ιM
C − ιM ιN ε.
=ε
N
N
N
N


(ε̃m − ε̃)2 =



and then show that
p

Note that in general CιM = ιN , and under Assumption 4, it follows that
C C = (N/M) · IM . We can write

M

Proof of Lemma 4. We will first show that the second claim in the
Lemma holds,
c
p
,
(A.3)
N · VINC −→
p · (1 − p)

N · VU −→

Hence, it suffices to show that

 
 M


2 
 
 M 
M2
ε̃s − ε̃  Z, M1 =
· trace C (Z)C − 2 ι (Z)ι .
E

N2
N
s=1

(A.4)

which together prove the first claim in the Lemma.
Consider Equation (A.3). By the conditions in the Lemma, α̂ols and
β̂ols are consistent for α and β, and therefore the probability limit of σ̂ 2

m=1

Thus,
 M




2 
ε̃s − ε̃  Z, M1
E

m=1
 
 

 

M
M 
1
1
C − ιN ιM
C − ιM ιN ε  Z, M1
= E ε
N
N
N
N

 


 

M 
1
M
1

C− ιN ιM
C − ιM ιN εε  Z, M1
= trace E
N
N
N
N

590

Journal of the American Statistical Association, June 2012



 


M
M 
1
1
C − ιN ιM
C − ιM ιN (Z)
N
N
N
N

 


M 
1
M
1
C − ιM ιN (Z)
C − ιN ιM
= trace
N
N
N
N
 
 M 
M2
= 2 · trace C (Z)C − 2 · ιN (Z)ιN .
N
N
= trace


Proof of Theorem 1. We show
p

N · VU −→

c
,
Nm2 · p · (1 − p)

and
p

N · VINC,CR −→

c
,
Nm2 · p · (1 − p)

which together imply the two claims in the Theorem. First consider the
first claim. The normalized variance is
M2 · N
·(M · trace(C (Z)C)−ι (Z)ι)
M0 ·M1 ·(M − 2)·N 2


M trace (C (Z)C) ι (Z)ι
M2 · N
·
·
−
.
=
M0 · M1 · (M −2) N
N
N2

N · VU (Z) =

By the conditions in the Theorem the probability limit of this expression
is



M trace (C (Z)C) ι (Z)ι
M2 · N
·
·
−
plim
M0 · M1 · (M − 2)
N
N
N2
 



2

M trace (C (Z)C)
M ·N
· plim
·
= plim
M0 · M1 · (M − 2)
N
N
 

ι (Z)ι
− plim
N2
c
.
= 2
Nm · p · (1 − p)
Next, consider the second claim. Now the probability limit of the
model-based variance is

tion based on the specification in Equation (A.1) is


 
 1
−1


1

L σε2 , σS2  Y, Z = − · ln  Z, σε2 , σS2 − · Y  σε2 , σS2
Y.
2
2
The expected value of the log-likelihood function is

 



E L σε2 , σS2 Y, Z  Z
 1

−1

1  
= − ln  Z, σε2 , σS2 − · E Y  Z, σε2 , σC2
Y
2
2
 1
−1

 
  
1
YY
= − · ln  Z, σε2 , σS2 − · trace E  Z, σε2 , σS2
2
2
 1

 
 
−1
1
(Z) .
= − · ln  Z, σε2 , σS2 − · trace  Z, σε2 , σS2
2
2
Using Lemma A.3, this is equal to


  M
 
N


E L σε2 , σS2 Y, Z)  Z = − · ln σε2 −
2
2


1
2
2
· trace((Z))
· ln 1 + N/M · σS /σε −
2 · σε2


σS2

 · trace C (Z)C .
+
2 · σε2 · σε2 + σS2 · N/M
The first derivative of the expected log-likelihood function with respect
to σS2 is

N
∂   2 2 


Y,
Z
E
L
σ
,
σ
Z = − 

ε
S
∂σS2
2 · σε2 + N/M · σS2


trace C (Z)C
+ 2
(σε + σS2 · (N/M))2
Hence, the first-order condition for σ̃S2 implies that




trace C (Z)C = N · σ̃ε2 + σ̃S2 · (N/M) .
For the misspecified error covariance matrix (Z, γ̃ ), we have
M

trace(C (Z, γ̃ )C) =




Nm2 · σ̃S2 + Nm · σ̃ε2 .

m=1

plim(N · VINC,CR (Z))
By equality of the cluster sizes, this simplifies to
⎛


 



2
M
·
N
trace(C (Z, γ̃ )C) = N · σ̃ε2 + σ̃S2 · (N/M) = trace(C (Z)C).

2
C
= plim⎝
·
M
·
trace
C

Z,
σ̃
,
σ̃
ε
S
M0 · M1 · (M − 2) · N 2

⎞


− ι  Z, σ̃ε , σ̃S2 ι ⎠
[Received September 2010. Revised December 2011.]

 

⎛
2
trace C  Z, σ̃ε , σ̃S2 C
M
M
·
N
·⎝ ·
= plim⎝
M0 · M1 · (M − 2)
N
N
 ⎞⎞

ι  Z, σ̃ε , σ̃S2 ι
⎠⎠
−
N2
 ⎞


⎛
⎛
trace C  Z, σ̃ε , σ̃S2 C
M
1
⎠
=
· ⎝plim⎝ ·
Nm · p · (1 − p)
N
N
 ⎞⎞
⎛ 
ι  Z, σ̃ε , σ̃S2 ι
⎠⎠
− plim⎝
N2

 ⎞

⎛
trace C  Z, σ̃ε , σ̃S2 C
1
⎠
· plim⎝
= 2
Nm · p · (1 − p)
N
⎛

Hence, to prove the second claim, it suffices to show that
trace(C (Z)C) = trace(C (Z, (σ̃ε , σ̃S2 ))C). The log-likelihood func-

REFERENCES
Abadie, A., Diamond, A., and Hainmueller, J. (2010), “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s
Tobacco Control Program,” Journal of the American Statistical Association,
105(490), 493–505. [578]
Angrist, J. D., and Pischke, J.-S. (2009), Mostly Harmless Econometrics: An Empiricist’s Companion, Princeton, NJ: Princeton University
Press. [578,584]
Barrios, T., Imbens, G. W., Diamond, R., and Kolesár, M. (2010), Clustering, Spatial Correlations and Randomization Inference, Working Paper
No. 15760, Cambridge, MA: NBER. [582]
Bertrand, M., Duflo, E., and Mullainathan, S. (2004), “How Much Should
We Trust Differences-in-Differences Estimates?” Quarterly Journal of Economics, 119(1), 249–275. [578]
Bester, C. A., Conley, T. G., and Hansen, C. B. (2011), “Inference With Dependent Data Using Cluster Covariance Estimators,” Journal of Econometrics,
165(2), 137–151. [578]
Diggle, P., Heagerty, P., Liang, K.-Y., and Zeger, S. L. (2002), Analysis of
Longitudinal Data, Oxford: Oxford University Press. [578]
Fisher, R. A. (1925), The Design of Experiments (1st ed.), London: Oliver &
Boyd. [578]

Barrios et al.: Clustering, Spatial Correlations, and Randomization Inference
Gelfand, A. E., Diggle, P., Guttorp, P., and Fuentes, M. (2010), Handbook of
Spatial Statistics, London: Chapman & Hall. [578]
Greenwald, B. C. (1983), “A General Analysis of Bias in the Estimated Standard Errors of Least Squares Coefficients,” Journal of Econometrics, 22(3),
323–338. [584]
Hansen, C. B. (2007), “Generalized Least Squares Inference in Panel and Multilevel Models With Serial Correlation and Fixed Effects,” Journal of Econometrics, 140(2), 670–694. [578]
Ibragimov, R., and Müller, U. (2010), “T-Statistic Based Correlation and Heterogeneity Robust Inference,” Journal of Business & Economic Statistics,
28(4), 453–468. [578]
Liang, K.-Y., and Zeger, S. L. (1986), “Longitudinal Data Analysis for Generalized Linear Models,” Biometrika, 73(1), 13–22. [578]

591

Mantel, N. (1967), “The Detection of Disease Clustering and a Generalized
Regression Approach,” Cancer Research, 27(2), 209–220. [586]
Moulton, B. R. (1986), “Random Group Effects and the Precision of Regression
Estimates,” Journal of Econometrics, 32(3), 385–397. [578]
Neyman, J. (1990), “On the Application of Probability Theory to Agricultural
Experiments: Essay on Principles—Section 9,” Statistical Science, 5(4),
465–480. [578,583]
Rosenbaum, P. R. (2002), Observational Studies, New York: Springer-Verlag.
[578,582]
Schabenberger, O., and Gotway, C. A. (2004), Statistical Methods for Spatial
Data Analysis (1st ed.), London: Chapman & Hall. [578,586]
Wooldridge, J. M. (2002), Econometric Analysis of Cross Section and Panel
Data, Cambridge, MA: MIT Press. [578]

