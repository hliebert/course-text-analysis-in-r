Research Article
Received 2 August 2012,

Accepted 22 May 2014

Published online 15 June 2014 in Wiley Online Library

(wileyonlinelibrary.com) DOI: 10.1002/sim.6227

Using an instrumental variable to test for
unmeasured confounding
Zijian Guo,a Jing Cheng,b Scott A. Lorchc and Dylan S. Smalla*†
An important concern in an observational study is whether or not there is unmeasured confounding, that is,
unmeasured ways in which the treatment and control groups differ before treatment, which affect the outcome.
We develop a test of whether there is unmeasured confounding when an instrumental variable (IV) is available.
An IV is a variable that is independent of the unmeasured confounding and encourages a subject to take one
treatment level versus another, while having no effect on the outcome beyond its encouragement of a certain
treatment level. We show what types of unmeasured confounding can be tested for with an IV and develop a test
for this type of unmeasured confounding that has correct type I error rate. We show that the widely used Durbin–
Wu–Hausman test can have inflated type I error rates when there is treatment effect heterogeneity. Additionally,
we show that our test provides more insight into the nature of the unmeasured confounding than the Durbin–Wu–
Hausman test. We apply our test to an observational study of the effect of a premature infant being delivered in a
high-level neonatal intensive care unit (one with mechanical assisted ventilation and high volume) versus a lower
level unit, using the excess travel time a mother lives from the nearest high-level unit to the nearest lower-level
unit as an IV. Copyright © 2014 John Wiley & Sons, Ltd.
Keywords:

instrumental variables; observational study; confounding; comparative effectiveness

1. Introduction
Randomized controlled studies are the gold standard to compare the effects of treatments between different treatment groups. Unfortunately, randomized controlled studies are often not feasible because of cost
or ethical constraints. When randomized studies are not feasible, observational studies provide important
evidence about the comparative effectiveness of different treatments. Because treatments were not randomly assigned, a major concern in an observational study is confounding, meaning that the treatment
groups may differ before treatment in ways that also affect the outcome. If the confounders are measured, these differences can be adjusted for by matching, stratification or, regression [1]. However, there
is often concern that there are unmeasured ways in which the treatment groups differ that affect the outcome, meaning that there is unmeasured confounding. Even when there is unmeasured confounding, it is
possible to obtain a consistent estimate of the causal effect of treatment for a certain sub-population (the
compliers) if an instrumental variable (IV) can be found. An IV is a variable that (i) is independent of the
unmeasured confounding; (ii) encourages, but does not force, a subject to take one treatment level versus
another; and (iii) has no effect on the outcome beyond its encouragement of a certain treatment level. For
discussions of IVs, see [2–9]. In this paper, we develop a method for using an IV to test whether there is
unmeasured confounding. Detecting whether there is unmeasured confounding is valuable in many studies because if unmeasured confounding is found in a given study, it suggests that for studying related
questions, researchers should either try to measure more confounders or seek to find IVs.
The existing and widely used test for whether there is unmeasured confounding using an IV is
the Durbin–Wu–Hausman endogeneity test, hereafter called the DWH test, independently proposed by

a Department of Statistics, The Wharton School, University of Pennsylvania, Philadelphia, PA 19104, U.S.A.
b Division of Oral Epidemiology and Dental Public Health, School of Dentistry, University of California, San Francisco (UCSF),

San Francisco, CA 94143, U.S.A.
of Pediatrics, University of Pennsylvania, Philadelphia, PA 19104, U.S.A.
*Correspondence to: Dylan S. Small, Department of Statistics, The Wharton School, University of Pennsylvania, 400 Huntsman
Hall, Philadelphia, PA 19104, U.S.A.
† E-mail: dsmall@wharton.upenn.edu
c Department

3528

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3529

Durbin [10], Wu [11], and Hausman [12]. The DWH test compares an estimate of the average treatment effect under the assumption that there is no unmeasured confounding to an estimate of the average
treatment effect using an IV that allows for unmeasured confounding. The IV estimate of the average
treatment effect is assumed to be consistent so that a significant difference between it and the estimate
that assumes no unmeasured confounding is taken as evidence of unmeasured confounding. The two estimates of the average treatment effect in the DWH test assume that the treatment effect is homogeneous,
meaning that the treatment effect is not different by covariates; see [4, 13–16] for discussion of homogeneity assumptions. Brookhart et al. [15] noted that if the DWH test rejects, one cannot be sure whether
it is because of unmeasured confounding or treatment effect heterogeneity. Extensions to the conventional
DWH test have been developed that allow for heteroskedasticity [17,18], but these tests all have the property that if they reject, one cannot be sure whether it is because of unmeasured confounding or treatment
effect heterogeneity.
In this paper, we develop a test that distinguishes between unmeasured confounding and treatment
effect heterogeneity. We discuss what types of unmeasured confounding can be tested for and provide a
test with correct type I error rate for the testable types of unmeasured confounding. In addition to having
the advantage over the DWH test of having correct type I error rate for testing unmeasured confounding
when there is treatment effect heterogeneity, our testing approach also provides more insight into the
nature of the unmeasured confounding by providing separate tests for two different types of unmeasured
confounding. In the DWH test, these two types of unmeasured confounding are lumped together.
The motivating application for our work is an observational study of neonatal care that seeks to estimate the effect on mortality of a premature infant being delivered in a high-level neonatal intensive care
unit (NICU) versus a lower-level NICU. A high-level NICU is defined as a NICU that has the capacity
for sustained mechanical-assisted ventilation and delivers at least 50 premature infants per year. Estimating the effect of being delivered at a high-level NICU is important for determining the value of a policy
of regionalization of perinatal care that aims for premature infants to be mostly delivered in high-level
NICUs [19]. Regionalization of perinatal care was developed in the 1970s along with the expansion of
neonatal technologies, but by the 1990s, regionalization began to weaken in many areas of the USA [20].
The difficulty in studying the causal effect of a premature infant being delivered in a high-level versus a
lower-level NICU is confounding by indication—the mother and the mother’s physician may try harder
to deliver an infant at a high-level NICU if the infant is at higher risk of mortality. In our data from
Pennsylvania (described in Section 7), the unadjusted death rate in high-level NICUs is higher than in
low-level NICUs, 23 versus 12 deaths per 1000 deliveries. Because it is unlikely that high-level NICUs
are worse than low-level NICUs, the higher unadjusted death rate in high-level NICUs probably reflects
confounding by indication. Our data contain a number of potential confounders, including birth weight,
month prenatal care started, mother’s education, and whether the mother went into labor prematurely.
After adjustment for measured confounders by propensity score matching, the death rate is 0.5 deaths per
1000 deliveries lower in high-level NICUs [21]. However, we are concerned about unmeasured potential
confounders, such as the severity of a mother’s comorbid condition or an infant’s antenatal condition, lab
results, fetal heart tracing results, the compliance of the mother to medical treatment, and the physician’s
history with the mother. These variables are known to the physicians who assess a mother’s probability of
delivering a high-risk infant. On the basis of this probability, the physicians then play a role in deciding
where the mother should deliver. To attempt to deal with the problem of potential unmeasured confounding, we have collected data on a proposed IV, the excess travel time that a mother lives from the nearest
high-level NICU compared with the nearest lower-level NICU; specifically, the IV is whether or not the
mother’s excess travel time is less than or equal to 10 min. Excess travel time to a hospital delivering
speciality care has been used as an IV in other medical settings, such as studies of the effect of cardiac catheterization on survival in patients who suffered an acute myocardial infarction [22]. In obstetric
care, prior work suggests that women tend to deliver at the closest hospital so that we expect that excess
travel time will have a strong effect on where the infant is delivered [23]. We discuss in Section 7 reasons for thinking excess travel time is a valid IV. Our goal in this paper is to use the putative IV excess
travel time to test whether there is unmeasured confounding in the study of the effect of high-level versus
lower-level NICUs. If unmeasured confounding is found, it suggests that previous studies of the effect
of high-level versus lower-level NICUs that assumed no unmeasured confounding provided biased estimates and that future studies of the effect of NICU level on mortality and related medical questions should
seek to measure more confounders and/or find and measure IVs. Related medical questions include the
effects of high-level NICUs on complications, length of stay, and readmissions, which have not yet been
studied systematically.

Z. GUO ET AL.

The rest of the paper is organized as follows. In Section 2, we set up the causal framework and introduce
notation and assumptions. In Section 3, we discuss what type of unmeasured confounding can be tested
for when there are heterogeneous treatment effects. In Section 4, we develop our method for testing for
unmeasured confounding using an IV. In Section 5, we discuss the DWH test and how it performs when
there are heterogeneous treatment effects. In Section 6, we present a simulation study comparing our
method with the DWH test. In Section 7, we apply our test to the study of the effect of high-level versus
lower-level NICUs. Finally, we provide conclusions in Section 8.

2. The framework
2.1. Notation
The IV Z and the treatment A are assumed to each be binary, where level 0 of the treatment is considered the ‘control’ (lower-level NICU in our application) and level 1 is considered to be the ‘treatment’
(high-level NICU in our application). We let 𝐙 denote the N-dimensional vector of IV values for all N
subjects, with individual elements Zi = z ∈ {0, 1} for subject i; level 1 of the IV is assumed to encourage
receiving the treatment compared with level 0. Let 𝐀𝐳 be the N-dimensional vector of potential treatment under IV assignment 𝐳, with individual element A𝐳i = a ∈ {0, 1} according to whether subject i
would take the control or treatment under 𝐳. We let 𝐘𝐳,𝐚 be the vector of potential responses that {would
𝐳,𝐚
𝐳,𝐚 }
be observed
{ 𝐳 } under IV levels 𝐳 and treatment levels 𝐚, with individual element Yi for subject i. Yi
and Ai are ‘potential’ responses and treatments in the sense that we can observe only one value in each
set. We let Yi and Ai be the corresponding observed outcome and treatment variables for subject i. We let
𝐗i denote the measured confounders for subject i. We assume that 𝐗i includes an intercept.
2.2. Assumptions
We make the same assumptions as [2] within strata of the measured confounders 𝐗.
′

(1) Stable unit treatment value assumption (SUTVA) [24]: (a) If 𝐳i = 𝐳i , then A𝐳i = A𝐳i . (b) If 𝐳i = 𝐳i
′

(3)

(4)
(5)

′

′

and 𝐚i = 𝐚i , then Yi𝐳,𝐚 = Yi𝐳 ,𝐚 . This assumption allows us to write Yi𝐳,𝐚 and A𝐳i as Yiz,a and Azi ,
respectively, for subject i.
The IV is independent of unmeasured confounding: Conditional on 𝐗, for a randomly chosen subject, the IV Z is independent of the vector of potential responses and treatments
(Y 0,0 , Y 0,1 , Y 1,0 , Y 1,1 , A0 , A1 ).
′
′
Exclusion restriction: For each subject i, Yiz,a = Yiz ,a for all z, z and a, that is, the IV level affects
outcomes only through its effect on treatment level. This assumption allows us to define Yia ≡
Yi0,a = Yi1,a for a = 0, 1.
IV affects treatment: P(A = 1|Z = 1) > P(A = 1|Z = 0).
Monotonicity: A1i ⩾ A0i for all i. This assumption says that there is no one who would always do the
opposite of what the IV encourages, that is, no one who would not take the treatment if encouraged
to do so by the IV level but would take the treatment if not encouraged by the IV level.
′

(2)

′

We further make the consistency assumption [25]:
(6) Consistency: No matter how subject i is administered the treatment, the potential outcome is the
same so that
Yi = Ai Yi1 + (1 − Ai )Yi0
where Yi and Ai are the observed outcome and treatment for subject i.
2.3. Compliance class

3530

On the basis of a subject’s joint values of potential treatment (A0i , A1i ), a subject can be classified into one
of four latent compliance classes [2]:
Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

Table I. The relation of observed groups and latent compliance
classes under the monotonicity assumption.
Zi
(observed)

Ai
(observed)

1
1
0
0

1
0
0
1

Ci
(latent)
co (complier)
nt (never-taker)
nt (never-taker)
at (always-taker)

⎧ nt (never-taker)
⎪
⎪ co (complier)
Ci = ⎨
⎪ at (always-taker)
⎪ de (defier)
⎩

or

at (always-taker)

or

co (complier)

)
(
if A0i , A1i = (0, 0)
(
)
if A0i , A1i = (0, 1)
(
)
if A0i , A1i = (1, 1)
(
)
if A0i , A1i = (1, 0)

Under the monotonicity assumption, there are no defiers. We can observe only one of A0i and A1i , so a
subject’s compliance class is not observed directly, but it can be partially identified on the basis of IV level
and observed treatment as shown in Table I. On the basis of Table I, the following quantities are identified
under assumptions 1 − 6 on the basis of the observable data: P(C = at) = P(A = 1|Z = 0), P(C = nt) =
P(A = 0|Z = 1), P(C = co) = 1−P(A = 1|Z = 0)−P(A = 0|Z = 1), E(Y 1 |C = at) = E(Y|Z = 0, A = 1),
E(Y|Z=1,A=1)−[P(C=at)∕{P(C=at)+P(C=co)}]E(Y|Z=0,A=1)
E(Y 0 |C = nt) = E(Y|Z = 1, A = 0), E(Y 1 |C = co) =
,
P(c=co)∕{P(C=at)+P(C=co)}
E(Y|Z=0,A=0)−[P(C=nt)∕{P(C=nt)+P(C=co)}]E(Y|Z=1,A=0)

and E(Y 0 |C = co) =
. The quantities E(Y 1 |C = nt) and
P(c=co)∕{P(C=nt)+P(C=co)}
0
E(Y |C = at) are not identified. [2]
The authors in [26–29] discussed using the framework described in this section to estimate the effect
of a treatment from a randomized trial with noncompliance where random assignment is used as an IV.
Several of these papers consider the additional complication of subsequent missing outcomes.

3. Type of unmeasured confounding that can be tested for using
an instrumental variable
In this section, we discuss what types of unmeasured confounding can be tested for using an IV. Because
the concern about unmeasured confounding is that failure to adjust for unmeasured confounding might
bias a treatment effect estimate, we define there to be unmeasured confounding for a treatment effect
estimate if not adjusting for unmeasured variables would result in a biased estimate of the treatment effect
[30]. There are different treatment effects that are of interest in causal inference, so it is possible for there
to be unmeasured confounding for one type of treatment effect but not for another type of treatment effect.
We will consider testing for unmeasured confounding for three types of treatment effects that are often
estimated in the causal inference literature. For ease of explanation, we will define the treatment effects
conditional on the measured covariates 𝐗; the treatment effect over the whole population of interest can
be found by averaging the estimates of the treatment effect for different values of the measured covariates
𝐗 over the covariate distribution of the population of interest [31]. The three treatment effects we will
consider are the following: (i) the average causal effect over the whole population with covariates 𝐗
(PACEX), 𝜏pacex (𝐗) = E(Y 1 − Y 0 |𝐗); (ii) the average causal effect for the treated subjects with covariates
𝐗 (TACEX), 𝜏tacex (𝐗) = E(Y 1 − Y 0 |A = 1, 𝐗); and (iii) the average causal effect for the compliers with
covariates 𝐗 (CACEX), 𝜏cacex (𝐗) = E(Y 1 − Y 0 |C = co, 𝐗).
We say that there is no unmeasured confounding for a treatment effect 𝜏(𝐗) if the expectation of a
straight comparison between the treated and control subjects’ outcomes conditional on 𝐗 is equal to 𝜏(𝐗),
that is,
No unmeasured confounding for 𝜏(𝐗) ∶ E[Y|A = 1, 𝐗] − E[Y|A = 0, 𝐗] = 𝜏(𝐗)

(1)

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3531

When (1) holds, a consistent estimate of 𝜏(𝐗) under random sampling is the difference in sample means
of the outcomes for treated (A = 1) and control (A = 0) subjects with covariates 𝐗. When 𝐗 is continuous

Z. GUO ET AL.

or high dimensional, regression methods can be used to estimate E[Y|A, 𝐗], and 𝜏(𝐗) can be estimated
̂
̂
by E[Y|A
= 1, 𝐗] − E[Y|A
= 0, 𝐗] [31]. The bottom line is that when (1) holds, the treatment effect
𝜏(𝐗) can be estimated consistently from the observed data on treatments, outcomes, and covariates; in
contrast, when (1) does not hold, additional information like an IV is needed to deal with unmeasured
confounding to obtain consistent estimates.
We now consider under what conditions does no unmeasured confounding for the CACEX, 𝜏cace (𝐗) =
E(Y 1 − Y 0 |C = co), hold. No unmeasured confounding for the CACEX means
E[Y|A = 1, 𝐗] − E[Y|A = 0, 𝐗] = E[Y 1 |C = co, 𝐗] − E[Y 0 |C = co, 𝐗]
We can decompose the expected values on the left hand side of (1) into parts contributed by the
compliance classes:
(
)
P(C = at|𝐗)
E Y 1 |C = at, 𝐗
P(C = at|𝐗) + P(Z = 1|𝐗)P(C = co|𝐗)
(
)
P(Z = 1|𝐗)P(C = co|𝐗)
E Y 1 |C = co, 𝐗
+
P(C = at|𝐗) + P(Z = 1|𝐗)P(C = co|𝐗)
(2)
(
)
)
( 0
P(C = nt|𝐗)
E Y 0 |C = nt, 𝐗
E(Y|A = 0, 𝐗) = E Y |A = 0, 𝐗 =
P(C = nt|𝐗) + P(Z = 0|𝐗)P(C = co|𝐗)
(
)
P(Z = 0|𝐗)P(C = co|𝐗)
E Y 0 |C = co, 𝐗
+
P(C = nt|𝐗) + P(Z = 0|𝐗)P(C = co|𝐗)

(
)
E(Y|A = 1, 𝐗) = E Y 1 |A = 1, 𝐗 =

From (1) and (2), no unmeasured confounding for the CACEX holds if the following two conditions hold:
(
)
(
)
E Y 1 |C = at, 𝐗 = E Y 1 |C = co, 𝐗

(3)

E(Y 0 |C = nt, 𝐗) = E(Y 0 |C = co, 𝐗)

(4)

When (3) and (4) both hold, the CACEX can be estimated by comparing the treated and control groups
conditional on 𝐗 without using an IV. When (3) or (4) does not hold, then in most situations, estimating
the CACEX by just comparing the treated and control groups conditional on 𝐗 without using the IV
will produce a biased estimate. Note that it is possible that such an estimate of the CACEX would be
̂
unbiased if, for example, the bias in E(Y|A
= 1, 𝐗) as an estimate of E(Y 1 |C = co, 𝐗) cancels the bias
0
̂
in E(Y|A
= 0, 𝐗) as an estimate of E(Y |C = co, 𝐗), but such a situation is unlikely. Thus, for practical
purposes, by testing (3) and (4), we are testing whether there is no unmeasured confounding for the
CACEX. We will test (3) and (4) separately because the violation of one condition but not the other
is informative about the nature of the unmeasured confounding; see the discussion of the NICU study
in Section 7.
Because an IV only identifies the CACEX and does not identify the TACEX or the PACEX without further assumptions [2], we cannot test for no unmeasured confounding for the TACEX or PACEX
without further assumptions. We now discuss further assumptions under which the test of (3)-(4) provides a test for no unmeasured confounding for the TACEX or PACEX. Consider first the TACEX. There
is no unmeasured confounding for the TACEX if E[Y|A = 1, 𝐗] − E[Y|A = 0, 𝐗] = E[Y 1 − Y 0 |A = 1, 𝐗].
If we have an assumption that guarantees
[
]
[
]
E Y 1 − Y 0 |A = 1, 𝐗 = E Y 1 − Y 0 |C = co, 𝐗

(5)

then no unmeasured confounding for the TACEX will be equivalent to no unmeasured confounding for
the CACEX, and hence (3) and (4) will guarantee no unmeasured confounding for the TACEX. We will
show that such an assumption that guarantees (5) is that the average treatment effect for the always takers
and compliers is the same,

3532

(
)
(
)
E Y 1 − Y 0 |C = at, 𝐗 = E Y 1 − Y 0 |C = co, 𝐗
Copyright © 2014 John Wiley & Sons, Ltd.

(6)

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

Under (6) and Assumptions 1–6 from Section 2.2, we have the following expression for E(Y 1 − Y 0 |A =
1, 𝐗):
(
)
E Y 1 − Y 0 |A = 1, 𝐗
( 1
)
(
)
= E Y − Y 0 |A = 1, C = co, 𝐗 P(C = co|A = 1, 𝐗) + E Y 1 − Y 0 |C = at, 𝐗 P(C = at|A = 1, 𝐗)
(
)
= E(Y 1 − Y 0 |Z = 1, C = co, 𝐗)P(C = co|A = 1, 𝐗) + E Y 1 − Y 0 |C = at, 𝐗 P(C = at|A = 1, 𝐗)
(
)
(
)
= E Y 1 − Y 0 |C = co, 𝐗 P(C = co|A = 1, 𝐗) + E Y 1 − Y 0 |C = at, 𝐗 P(C = at|A = 1, 𝐗)
(
)
= E Y 1 − Y 0 |C = co, 𝐗
(7)
where the second to the last equality follows from Assumption 2 that Z is independent of potential
outcomes and treatment received conditional on 𝐗 and the last equality follows from (6) and the fact that
P(C = co|A = 1, 𝐗) + P(C = at|A = 1, 𝐗) = 1.
Thus, we conclude that under (6), the test of (3) and (4) tests whether there is no unmeasured confounding
for the TACEX. Now consider the PACEX. If we have an assumption that guarantees
[
]
[
]
E Y 1 − Y 0 |𝐗 = E Y 1 − Y 0 |C = co, 𝐗
(8)
then no unmeasured confounding for the PACEX will be equivalent to no unmeasured confounding for
the CACEX. Using similar reasoning as in (7), such an assumption that guarantees (8) is that the average
treatment effect for all three compliance classes—always takers, compliers, and never takers–is the same,
(
)
(
)
E Y 1 − Y 0 |C = at, 𝐗 = E Y 1 − Y 0 |C = co, 𝐗 = E(Y 1 − Y 0 |C = nt, 𝐗)
(9)
Thus, under (9), the test of (3) and (4) tests whether there is unmeasured confounding for the PACEX. The
assumption (9) means that the treatment effect is the same for the different compliance classes and thus
that there is no effect modification by compliance class. No effect modification by compliance class will
hold if either (a) treatment effects are homogeneous or (b) treatment effects are heterogeneous, but the
heterogeneity can be explained by observed covariates, and we include those covariates in the covariate
vector 𝐗 that we condition on.
In summary, having an IV as defined in Section 2 enables us to test whether there is no unmeasured
confounding for the average treatment effect for compliers (the CACE) by testing (3) and (4). An additional assumption of no effect modification by compliance class means that this test is also a test of
whether there is no unmeasured confounding for the average treatment effect for the whole population.
In the NICU study (second to the last paragraph of introduction and Section 7), there might be effect
modification by compliance class because always takers tend to be sicker babies than compliers or never
takers, and might benefit more from the high technology (e.g., sustained mechanical ventilation) in a highlevel NICU. Even if there is effect modification by compliance class, the test of (3) and (4) is still useful
in two ways. (i) It tests for no unmeasured confounding for the CACE, which is useful to know about. The
CACE is an important causal parameter because, by combining the CACE with information about how
the treatment effect is expected to vary between compliers, always takers, and never takers, we can predict
the effects of increasing or decreasing access to the treatment [6, 32, 33], that is, increasing or decreasing
access to high-level NICUs. If there is unmeasured confounding for the CACE, then the current study and
future studies on the effect of high-level NICUs on mortality and related outcomes should use IV methods
to estimate the CACE. But if there is no unmeasured confounding, it is not necessary to use IV methods.
(ii) If (3) or (4) is rejected, then that means there is an unmeasured variable, compliance class, which is
associated with potential outcomes and treatment even after controlling for the observed covariates 𝐗.
This suggests that some key covariate(s) relating to the treatment and outcome remains unmeasured, and
it would be worthwhile to try to collect more covariates relating to treatment and outcome if possible.

Our approach to testing for no unmeasured confounding is to test (3) and (4) via the following approach:
Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3533

4. Compliance class model test for unmeasured confounding using
an instrumental variable

Z. GUO ET AL.

(1) Specify a model for the potential outcome and compliance class distributions.
(2) Find the unconstrained maximum likelihood of the model using the expectation-maximization
(EM) algorithm.
(3) Find the maximum likelihood of the model under the constraints (3) and (4) using the EM
algorithm.
(4) Test the validity of the constraints (3) and (4) using the likelihood ratio test.
We call our approach the compliance class likelihood ratio test of no unmeasured confounding. We will
explain our approach in detail for binary outcomes and normally distributed outcomes.
4.1. Binary outcome model
We consider a logistic model for the outcome in each compliance class/treatment combination and a
multinomial logistic model for the compliance classes. As a starting point, we consider a model in which
the effect of the covariates 𝐗 on the outcome is the same across the compliance classes.
Model for the outcome:
P(Yia

)
(
exp 𝜅0,t + 𝜿 T1 x + 𝛾I(t = co)a
= 1|Ci = t, 𝐗i = 𝐱) =
(
)
1 + exp 𝜅0,t + 𝜿 T1 x + 𝛾I(t = co)a

(10)

where I(⋅) is the indicator function. The 𝜅0,t ’s measure the difference between the compliance classes
when the IV Z = 0 ,𝜿 1 is the effect of x, assumed to be the same across compliance classes and 𝛾 is the
log odds ratio for the effect of treatment for compliers.
Model for compliance classes: We consider a multinomial logit model. Let the compliers be the
reference category.
1
(
)
(
)
1 + exp 𝛿nt + 𝝉 Tnt 𝐱 + exp 𝛿at + 𝝉 Tat 𝐱
(
)
exp 𝛿nt + 𝝉 Tnt 𝐱
P(Ci = nt|𝐗i = 𝐱) =
(
)
(
)
1 + exp 𝛿nt + 𝝉 Tnt 𝐱 + exp 𝛿at + 𝝉 Tat 𝐱
(
)
exp 𝛿at + 𝝉 Tat 𝐱
P(Ci = at|𝐗i = 𝐱) =
(
)
(
)
1 + exp 𝛿nt + 𝝉 Tnt 𝐱 + exp 𝛿at + 𝝉 Tat 𝐱

P(Ci = co|𝐗i = 𝐱) =

(11)

We condition on the observed values of the covariates 𝐗 and the IV Z, and seek to maximize the
likelihood of Y, A|Z, 𝐗 under the models (10) and (11). We can use the EM algorithm to maximize the
likelihood of the models (10) and (11), where we think of the compliance classes as partially missing
data as in ([7, 8, 34, 35]). If we know the compliance classes, the maximum likelihood estimate (MLE) is
easy to compute. In practice, we are not able to observe the latent compliance classes, so we will use the
EM algorithm to obtain the MLE. For the E step, conditional on observables and parameter estimates in
the previous iteration, the expected value of the complete data log likelihood lC is
E(lC |Y, Z, A, X) =

n
∑

I(Zi = 1, Ai = 1)log P(Zi = 1|𝐗i )

i=1

{( )y {
[
( )}1−yi }]
i
1 − PC1
+ I(Zi = 1, Ai = 1)𝜔1 log P(Ci = co|𝐗i ) + log PC1
i
i
[
{ ( )y {
( )}1−yi }]
at i
1 − Pat
+ I(Zi = 1, Ai = 1)(1 − 𝜔1 ) log P(Ci = at|𝐗i ) + log Pi
i
[
( )}1−yi ]
( nt )yi {
1 − Pnt
+ I(Zi = 1, Ai = 0) log P(Zi = 1|𝐗i ) + log P(Ci = nt|𝐗i ) + log Pi
i

3534

+ I(Zi = 0, Ai = 0)log{1 − P(Zi = 1|𝐗i )}
{( )y {
[
( )}1−yi }]
i
+ I(Zi = 0, Ai = 0)𝜔2 log P(Ci = co|𝐗i ) + log PC0
1 − PC0
i
i
{( )y {
( )}1−yi }]
[
]
nt i
1 − Pnt
+ I(Zi = 0, Ai = 0)(1 − 𝜔2 ) log P(Ci = nt|𝐗i ) + log Pi
i
[
{ ( )y {
( at )}1−yi }]
i
1
−
Pi
+ I(Zi = 0, Ai = 1) log{1 − P(Zi = 1|𝐗i )} + log P(Ci = at|𝐗i ) + log Pat
i

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

where
PC1
i = P(Yi
Pat
i = P(Yi
Pnt
i = P(Yi
PC0
i = P(Yi

)
(
exp 𝜅0,co + 𝜿 T1 x + 𝛾
= 1|Zi = 1, Ci = co, 𝐗i ) =
(
)
1 + exp 𝜅0,co + 𝜿 T1 x + 𝛾
)
(
exp 𝜅0,at + 𝜿 T1 x
= 1|Ci = at, 𝐗i ) =
(
)
1 + exp 𝜅0,at + 𝜿 T1 x
)
(
exp 𝜅0,nt + 𝜿 T1 x
= 1|Ci = nt, 𝐗i ) =
(
)
1 + exp 𝜅0,nt + 𝜿 T1 x
)
(
exp 𝜅0,co + 𝜿 T1 x
= 1|Zi = 0, Ci = co, 𝐗i ) =
(
)
1 + exp 𝜅0,co + 𝜿 T1 x

𝜔1 = P(Ci = co|Yi = y, Zi = Ai = 1, 𝐗i )
P(Ci = co|𝐗i )f (Yi |Zi = 1, Ci = co, 𝐗i )
=
P(Ci = co|𝐗i )f (Yi |Zi = 1, Ci = co, 𝐗i ) + P(Ci = at|𝐗i )f (Yi |Zi = 1, Ci = at, 𝐗i )
𝜔2 = P(Ci = co|Yi = y, Zi = Ai = 0, 𝐗i )
P(Ci = co|𝐗i )f (Yi |Zi = 0, Ci = co, 𝐗i )
=
P(Ci = co|𝐗i )f (Yi |Zi = 0, Ci = co, 𝐗i ) + P(Ci = nt|𝐗i )f (Yi |Zi = 0, Ci = nt, 𝐗i )
We seek to test the null hypothesis that (3) and (4) hold. Under the models (10) and (11), these two
equations can be simplified as
(
)
)
(
exp 𝜅0,co + 𝜿 T1 x + 𝛾
exp 𝜅0,at + 𝜿 T1 x
(
)=
(
)
1 + exp 𝜅0,at + 𝜿 T1 x
1 + exp 𝜅0,co + 𝜿 T1 x + 𝛾
(
)
exp 𝜅0,nt + 𝜿 T1 x
exp(𝜅0,co + 𝜿 T1 x)
(
)=
(
).
1 + exp 𝜅0,co + 𝜿 T1 x
1 + exp 𝜅0,nt + 𝜿 T1 x
By simple calculation, the null hypothesis that (3) and (4) hold is equivalent to the constraints
𝜅0,at = 𝜅0,co + 𝛾
𝜅0,co = 𝜅0,nt

(12)

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3535

Under the null hypothesis, we can maximize the likelihood using the EM algorithm, where we impose
the constraints (12). The likelihood ratio test of (3) and (4) looks at the difference between the maximized
log likelihood of the unconstrained models (10) and (11) and the maximized log likelihood of the model
that constrains (3) and (4) to hold, that is, (12) to hold; under the null hypothesis, two times this difference follows asymptotically a 𝜒 2 distribution with degrees of freedom equal to the number of extra free
parameters in the unconstrained model compared with the constrained model [36]. For the models (10)
and (11), the constraints (12) reduce the number of free parameters by two (because 𝜅0,at and 𝜅0,nt are no
longer free parameters given 𝜅0,co and 𝛾). Note that for the likelihood ratio test, we are considering the
maximized observed data log likelihood rather than the complete data log likelihood. We can test (3) and
(4) separately by carrying out likelihood ratio tests of the constraints 𝜅0,at = 𝜅0,co + 𝛾 and 𝜅0,co = 𝜅0,nt ;
for each of these tests, the null distribution of two times the log likelihood ratio is 𝜒 2 with 1 degree
of freedom.
We now consider a binary outcome model where the treatment effect is heterogeneous, that is, the
treatment effect depends on covariates 𝐗.
Model for the outcome that allows for heterogeneous treatment effects: With covariates 𝐗, we consider
the following model. Let 𝜆t,𝐱 denote the log odds that y = 1 when z = 0 for compliance class t and
covariate vector 𝐱. Let 𝛾𝐱 be the log odds ratio that y = 1 for A = 1 versus A = 0 for compliers with
covariates 𝐗. The model we consider is

Z. GUO ET AL.

P(Yia = 1|Ci = t, 𝐗i = 𝐱) =

exp(𝜆t,𝐱 + 𝛾𝐱 I(t = co)a)
1 + exp(𝜆t,𝐱 + 𝛾𝐱 I(t = co)a)

𝛾𝐱 = 𝛼0 + 𝜶 T1 𝐱

(13)

𝜆t,𝐱 = 𝜅0,t + 𝜿 T1,t 𝐱
The model allows for the treatment effect for compliers to depend on 𝐱 through 𝜶 1 and for the difference
between the compliance classes when Z = 0 to depend on 𝐱 through the 𝜿 1,t ’s.
The expected value of the complete data log likelihood lC and the observed data log likelihood for
the binary outcome model with heterogeneous treatment effect (13) is of the same form as those with
, PC0
, Pat
, and Pnt
:
homogeneous treatment effect with different expressions for PC1
i
i
i
i
PC1
i = P(Yi = 1|Zi = 1, Ci = co, 𝐗i ) =
Pat
i = P(Yi = 1|Ci = at, 𝐗i ) =
Pnt
i = P(Yi = 1|Ci = nt, 𝐗i ) =
PC0
i

exp(𝜆co,𝐱 + 𝛾𝐱 )

1 + exp(𝜆co,𝐱 + 𝛾𝐱 )
exp(𝜆at,𝐱 )

1 + exp(𝜆at,𝐱 )
exp(𝜆nt,𝐱 )

1 + exp(𝜆nt,𝐱 )
exp(𝜆co,𝐱 )
= P(Yi = 1|Zi = 0, Ci = co, 𝐗i ) =
1 + exp(𝜆co,𝐱 )

By simple calculation, the null hypothesis of no unmeasured confounding (3) and (4) under model
(13) is
H0 ∶𝜿 nt = 𝜿 co
(14)
𝜿 at = 𝜿 co + 𝜶
)
(
where 𝜿 t = 𝜅0,t , 𝜿 T1,t and 𝜶 = (𝛼0 , 𝜶 1 )T .
4.2. Normal outcome model
A normal model for the outcome that allows for a heterogeneous treatment effect that is analogous to
(13) is
(
)
(
)
(
)
f Yia |Ci = t, 𝐗i = 𝐱 = N 𝜅0,t + 𝜿 T1,t 𝐱 + 𝛼0 + 𝜶 T1 𝐱 I(t = co)a, 𝜎 2
(15)
The null hypothesis of no unmeasured confounding (3) and (4) under the model (15) is (14).
4.3. Computation of MLE—EM and Broyden–Fletcher–Goldfarb–Shanno optimization
The EM algorithm can sometimes be slow to converge to the MLE near the maximizer of the likelihood
[37, 38]. To speed up the convergence to the MLE as in [37–39], we first run the EM algorithm until it
comes close to convergence and then use the EM estimates as the starting value and maximize the likelihood by a quasi-Newton method, the Broyden–Fletcher–Goldfarb–Shanno algorithm as implemented
in the optim function in R [40]. R functions for computing the EM estimates and Broyden–Fletcher–
Goldfarb–Shanno method and implementing our compliance class likelihood ratio test of no unmeasured
confounding are provided in the Supporting information. Instructions for using the functions and an
example data set are also provided.

5. DWH test

3536

In this section, we will consider the DWH test statistic for testing for no unmeasured confounding using
an IV and its properties. The conventional DWH test is formulated for a model with a continuous outcome
[10–12]. The DWH test statistic TDWH is the following. Let 𝛽̂OLS denote the ordinary least squares (OLS)
estimate of the effect of A on Y controlling for 𝐗. Let 𝛽̂2SLS denote the two-stage least squares estimate of
the effect of A on Y controlling for 𝐗 using Z as an IV; 𝛽̂2SLS is computed by first regressing A on Z, 𝐗 by
Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

̂ 𝐗 by least squares. The DWH
least squares and finding the predicted value Â and then regressing Y on A,
test statistic is an assessment of the difference between the OLS and two-stage least squares estimates of
the causal effect of A on Y,
(
)2
𝛽̂OLS − 𝛽̂2SLS
TDWH =
(16)
)
(
)
(
̂ 𝛽̂OLS
̂ 𝛽̂2SLS − Var
Var
[41]. The variances in (16) are the variances that come from the normal linear regression model and the
normal simultaneous equations model that make the homoskedasticity assumption that Var(Y 0 |𝐗) is equal
to Var(Y 1 |𝐗) and the same for all 𝐗 [41]. Note that there are several asymptotically equivalent forms of
the DWH test, which differ in the way the denominator in (16) is computed; see [41], pp. 50–52. The
null hypothesis of the DWH test can be expressed as the following: there is no unmeasured confounding,
that is, (3) and (4) hold, and the following three assumptions hold: (i) the average treatment effect for
compliers is homogeneous in 𝐗, that is, E(Y 1 − Y 0 |C = co, 𝐗) is the same for all 𝐗; (ii) E(Y 0 |𝐗)) is linear
in 𝐗; and (iii) a homoskedasticity assumption that Var(Y 0 |𝐗) is equal to Var(Y 1 |𝐗) and the same for all
𝐗. The asymptotic null distribution of TDWH is chi-squared with 1 degree of freedom [10–12, 41]. Under
the null hypothesis for the DHW test,
√ the denominator of the DHW test statistic (16) times the sample
size N converges to the variance of N times (𝛽̂OLS − 𝛽̂2SLS ) [12].
We now consider the properties of the DWH test when average treatment effects for compliers are
heterogeneous in 𝐗 . When average treatment effects for compliers are heterogeneous in 𝐗, the DWH test
may reject with probability converging to one even when there is no unmeasured confounding. To show
this, we will show that 𝛽̂OLS and 𝛽̂2SLS can converge to different weighted averages of treatment effects
when average treatment effects for compliers are heterogeneous in 𝐗. Combining this fact with the fact
that under regularity conditions described in [42], the denominator of the DWH test statistic (16) will
converge to 0 shows that TDWH converges in probability to ∞ and rejects with probability 1 even when
there is no unmeasured confounding.
We now consider the properties of 𝛽̂OLS when (3) and (4) hold and treatment effects are heterogeneous
in 𝐗. Let 𝛽𝐗 = E(Y 1 − Y 0 |C = co, 𝐗). Suppose E(Yi |𝐗i , Ai = 0) is linear in 𝐗i . Then E(Y|A = 1, 𝐗) −
E(Y|A = 0, 𝐗) = 𝛽𝐗 . Then, under the assumption that (3) and (4) hold, we have the following expression
for the probability limit of the OLS estimator where E∗ (A|𝐁) is the linear projection of A onto 𝐁 (i.e.,
E∗ (A|𝐁) = 𝜶 T 𝐁, 𝜶 = arg min𝜶∗ E(A − (𝜶 ∗ )T 𝐁)),
[
]
E (Ai − E∗ (Ai |𝐗i ))(Yi − E∗ (Yi |𝐗i ))
plim𝛽̂OLS =
(17)
[
]
E (Ai − E∗ (Ai |𝐗i ))2
[
]
E (Ai − E∗ (Ai |𝐗i ))Yi
= [
]
E (Ai − E∗ (Ai |𝐗i ))2

(18)

[
]
E (Ai − E∗ (Ai |𝐗i ))E(Yi |𝐗i , Ai )
=
[
]
E (Ai − E∗ (Ai |𝐗i ))2

(19)

]
[
E (Ai − E∗ (Ai |𝐗i ))(E(Yi |𝐗i , Ai = 0) + 𝛽𝐗 Ai )
=
[
]
E (Ai − E∗ (Ai |𝐗i ))2

(20)

i

i

i

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3537

[
]
E (Ai − E∗ (Ai |𝐗i ))2 𝛽𝐗
=
(21)
[
]
E (Ai − E∗ (Ai |𝐗i ))2
(
)
where we used the fact that E (Ai − E∗ (Ai |𝐗i ))Xi = 0 to derive (18) and (21), and we iterated expectations over 𝐗i and Ai to derive (19). Equation (21) shows that the OLS estimator converges to a weighted
average of treatment effects at different values of 𝐗, where the values of 𝐗, which obtain the most weight,
are those where E[(Ai − E∗ (Ai |𝐗i ))2 ] is largest. Angrist and Krueger [43] derived similar expressions
assuming that E(Ai |𝐗i ) is linear in 𝐗i . If E(Ai |𝐗i ) is linear in 𝐗, then E[(Ai − E∗ (Ai |𝐗i ))2 ] is the conditional variance of A given 𝐗. If E(Yi |𝐗i , Ai = 0) is not linear in 𝐗i , then plim𝛽̂OLS equals (21) plus
E[(Ai −E∗ (Ai |𝐗i ))E(Y 0 |𝐗)]
.
E[(A −E∗ (A |𝐗 ))2 ]

Z. GUO ET AL.

We now consider the properties of 𝛽̂2SLS when (3) and (4) hold, but treatment effects are heterogeneous
in 𝐗. We assume E(A|𝐗, Z) is linear in 𝐗, Z. Then the plim of 𝛽̂2SLS is the plim of the coefficient on
E(A|𝐗, Z) in the regression of Y on E(A|𝐗, Z) and 𝐗. By the same reasoning as in (17)–(21), this plim is
the weighted average of 𝛽𝐗 over the distribution of 𝐗, weighted by E[{E(A|𝐗, Z) − E∗ (E(A|𝐗, Z)|𝐗)}2 ].
Under the assumption that E(A|𝐗, Z) is linear in 𝐗, Z, these weights equal the conditional variance of
E(A|Z, 𝐗) given 𝐗, which equals P(C = co|𝐗)2 P(Z = 1|𝐗)(1 − P(Z = 1|𝐗)). Thus,
]
[
plim𝛽̂2SLS = E P(C = co|𝐗i )2 P(Zi = 1|𝐗i )(1 − P(Zi = 1|𝐗i ))𝛽𝐗
]
[
E P(C = co|𝐗i )2 P(Zi = 1|𝐗i )(1 − P(Zi = 1|𝐗i )) .

(22)

Thus, the two-stage least squares estimator converges to a weighted average of treatment effects at different values of 𝐗, where the values of 𝐗, which tend to obtain the most weight, are those for which the
proportion of compliers is highest. A value of 𝐗 at which there are no compliers obtains zero weight.
From (21) and (22), the numerator of the DWH test statistic (16) converges in probability to
[
]
]
[
E (A − E∗ (A|𝐗))2 𝛽𝐗
E P(C = co|𝐗)2 P(Z = 1|𝐗)(1 − P(Z = 1|𝐗))𝛽𝐗
[
] − [
]
E P(C = co|𝐗)2 P(Z = 1|𝐗)(1 − P(Z = 1|𝐗))
E (A − E∗ (A|𝐗))2

(23)

And the denominator of (16) converges in probability to 0 under regularity conditions. Thus, under
the regularity conditions, when (23) is not equal to zero, TDWH converges in probability to ∞. In summary, even when there is no unmeasured confounding, we have shown that 𝛽̂OLS and 𝛽̂2SLS can converge
to different weighted averages of treatment effects when average treatment effects for compliers are
heterogeneous in 𝐗, and consequently, the DWH test statistic (16) can converge in probability to ∞.

6. Simulation study
6.1. Normal outcomes
We will compare the compliance class likelihood ratio test of no unmeasured confounding developed
in Section 4 with the DWH test in a simulation study under the normal outcome model (15). We will
consider one binary covariate X. We consider three scenarios:
(I) The null hypothesis of no unmeasured confounding for the CACE, that is, (3) and (4), holds,
which is equivalent for the normal outcome model to (14) holding. Additionally, the complier
treatment effect is homogeneous in X, that is, 𝛼1 = 0. Here we expect that both the DWH test and
our test will have a 0.05 type I error rate.
(II) The null hypothesis of no unmeasured confounding for the CACE holds, but the treatment effect
is heterogeneous in X, that is, 𝛼1 ≠ 0. Here we expect our test will have a 0.05 type I error rate,
but the DWH test will have a greater than 0.05 type I error rate.
(III) The null hypothesis of no unmeasured confounding for the CACE does not hold.
The parameters for (15) for each scenario are shown in Table II. The sample size for each simulation
scenario is 1000, and 1000 simulations were carried out for each scenario. For all the scenarios, the IV
Z was generated as a Bernoulli random variable with
P(Zi = 1|Xi = x) =

exp(−1 + 2x)
1 + exp(−1 + 2x)

The parameters −1 and 2 were chosen so that P(Z|𝐗), and the marginal probability of Z = 1 is about 12 .
Also, for all the scenarios, the model for the compliance class is as follows:
exp(−2.5 + 3.5x)
1 + 2 exp(−2.5 + 3.5x)
1
P(Ci = co|Xi = x) =
1 + 2 exp(−2.5 + 3.5x)

P(Ci = at|Xi = x) = P(Ci = nt|Xi = x) =

3538
Copyright © 2014 John Wiley & Sons, Ltd.

(24)

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

Table II. Parameters of normal outcome model.
Scenario I

Scenario II

Scenario III

0.8
1
0.3
1
0.3
1
0.5
0

0.8
0
0.3
1
0.3
1
0.5
−1

1.5
1
0.3
1
−1
2
0.5
−1

𝜅0,at
𝜅1,at
𝜅0,co
𝜅1,co
𝜅0,nt
𝜅1,nt
𝛼0
𝛼1

Table III. Bias of sample mean of 𝛿t , 𝜏t , 𝜆t , 𝛾t , and 𝜎 2 with 1000
simulations of sample size 1000 for the normal model.
Scenario I

𝛿nt
𝜏nt
𝛿at
𝜏at
𝜅0,at
𝜅1,at
𝜅0,co
𝜅1,co
𝜅0,nt
𝜅1,nt
𝛼0
𝛼1
𝜎2

Scenario II

Scenario III

MLE

RMLE

MLE

RMLE

MLE

RMLE

−0.0477
0.1013
−0.0095
0.0520
0.0059
−0.0063
−0.0019
−0.0166
−0.0032
0.0067
−0.0010
0.0327
−0.0136

−0.0544
0.0999
−0.0105
0.0443
−0.0006
0.0007
−0.0020
0.0037
−0.0021
0.0039
0.0016
−0.0032
−0.0026

−0.0477
0.1007
−0.0095
0.0513
0.0059
−0.0064
−0.0019
−0.0142
−0.0032
0.0064
−0.0010
0.0308
−0.0135

−0.0543
0.0998
−0.0105
0.0442
−0.0006
0.0007
−0.0020
0.0037
−0.0022
0.0039
0.0016
−0.0032
−0.0026

0.0156
0.0128
0.0039
0.0009
0.0037
0.0014
0.0006
0.0596
0.0111
0.0093
0.0069
0.0567
0.0079

−0.0544
0.0998
−0.0104
0.0441
−0.5368
0.1748
−0.1325
−0.1388
1.1675
−1.1386
0.2960
1.3134
0.0921

Table IV. Empirical rejection rates rr
̂ 𝛼 with type I error 𝛼 for the
normal outcome model with 1000 simulations.
I

II

III

Scenario

rr
̂ 𝛼=0.01

rr
̂ 𝛼=0.05

rr
̂ 𝛼=0.01

rr
̂ 𝛼=0.05

rr
̂ 𝛼=0.01

rr
̂ 𝛼=0.05

DWH test
LR test

0.006
0.014

0.042
0.053

0.662
0.014

0.853
0.052

0.98
1

0.997
0.999

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3539

From Section 5, we know that the OLS estimator is a weighted average of the treatment effects with
conditional variance Var(Ai |Xi ) as weights whereas the two-stage least squares estimator is a weighted
average of the treatment effects where the weights are related with P(Ci = co|Xi ). The compliance class
model (24) was chosen so that these two sets of weights differ. In scenario II, we have P(Ci = co|Xi =
1) ≈ 0.16, P(Ci = co|Xi = 0) ≈ 0.85, whereas Var(Ai |Xi = 1) ≈ 0.25, Var(Ai |Xi = 0) ≈ 0.22.
Table III shows the bias of the MLEs over the whole parameter space for the three simulation scenarios as well as the bias of the restricted MLEs (RMLEs) under the constraint (14) of no unmeasured
confounding for the CACE. The MLEs are approximately unbiased for all three scenarios. The RMLEs
are approximately unbiased for the first two scenarios where no unmeasured confounding for the CACE
holds, but are substantially biased in scenario III, where no unmeasured confounding for the CACE does
not hold.
Table IV shows the rejection rates of the compliance class model likelihood ratio test and the DWH test
for the three scenarios. For scenario I, in which the null hypothesis of no unmeasured confounding for the

Z. GUO ET AL.

CACE holds and also the treatment effect for compliers is homogeneous in X, both the compliance class
model likelihood ratio test and the DWH test have empirical rejection rates close to their nominal type
I error level. For scenario II, in which the null hypothesis of no unmeasured confounding for the CACE
holds but the treatment effect for compliers is heterogeneous in X, the compliance class model likelihood
ratio test has an empirical rejection rate close to its nominal type I error rate, but the DWH test rejects far
too often, for example, it rejects 0.853 of the time when the nominal type I error rate is 0.05. For scenario
III, in which the null hypothesis of no unmeasured confounding for the CACE does not hold, both tests
have high power with the compliance class model likelihood ratio test having slightly higher power. In
summary, the simulation study results show that the compliance class model likelihood ratio test has
advantages over the DWH test: the compliance class model likelihood ratio test has comparable power
to the DWH test but keeps close to the correct type I error rate when treatment effects are heterogeneous
in 𝐗, unlike the DWH test.
The compliance class model likelihood ratio test (the LR test) is more computationally intensive than
the DWH test. For the first scenario of the simulation study, on an Optiplex 780 computer with Intel Core
Duo CPU E8400 @ 3.00 GHZ and 4 GB Ram, the DWH test took an average of 0.013 CPU seconds,
while the LR test took an average of 53.7 CPU seconds. The DWH test is faster than the compliance
class likelihood ratio test, but the LR test is not prohibitively slow, taking less than a minute. Although
the DWH test is faster, the DWH test does not work for models with heterogeneous treatment effects.
When dealing with real data, we do not know whether there are heterogeneous treatment effects. When
computational efficiency is not the main concern, we suggest to test for no unmeasured confounding
using the LR test.
6.2. Binary Outcomes
In this section, we study the performance of the compliance class likelihood ratio for no unmeasured
confounding for binary outcomes developed in Section 4.1. We do not consider the DWH because the
DWH test assumes normal outcomes rather than binary outcomes. We simulate the binary outcome model
as in (10) and estimate the parameters by the EM algorithm for the binary outcome. As for the simulation
for normal outcomes, we consider three scenarios: (i) the null hypothesis of no unmeasured confounding
for the CACE holds and the complier treatment effect is homogeneous in X; (ii) the null hypothesis of no
unmeasured confounding for the CACE holds and the complier treatment effect is heterogeneous in X;
and (iii) the null hypothesis of no unmeasured confounding for the CACE does not hold. The parameters
for (10) for each scenario are shown in Table V. The sample size for each simulation scenario is 1000,
and 1000 simulations were carried out for each scenario. For all the scenarios, the IV Z was generated as
a Bernoulli random variable with
P(Zi = 1|Xi = x) =

exp(−2 + x)
.
1 + exp(−2 + x)

The parameters −2 and 1 were chosen so that P(Z|𝐗) and the marginal probability of Z = 1 is about 12 .
Also, for all the scenarios, the model for the compliance class is as follows:

Table V. Parameters of binary outcome model.

3540

𝜅0,at
𝜅1,at
𝜅0,co
𝜅1,co
𝜅0,nt
𝜅1,nt
𝛼0
𝛼1

Copyright © 2014 John Wiley & Sons, Ltd.

Scenario I

Scenario II

Scenario III

0
1
−0.5
1
−0.5
1
0.5
0

0
0
−0.5
1
−0.5
1
0.5
−1

−2
1
−0.5
1
−1
0.5
0.5
−1

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

Table VI. Bias of sample mean of 𝛿t , 𝜏t , 𝜆t , 𝛾t , and 𝜎 2 with 1000
simulations of sample size 1000 for the binary outcome model.
Scenario I

𝛿nt
𝜏nt
𝛿at
𝜏at
𝜅0,at
𝜅1,at
𝜅0,co
𝜅1,co
𝜅0,nt
𝜅1,nt
𝛼0
𝛼1

Scenario II

Scenario III

MLE

RMLE

MLE

RMLE

MLE

RMLE

−0.0099
0.0035
−0.0092
−0.0016
−0.0251
0.0750
−0.0320
0.0465
−0.0882
0.0662
0.0325
0.0989

−0.0109
0.0036
−0.0099
−0.0018
−0.0151
0.0134
−0.0080
0.0081
−0.0080
0.0081
−0.0071
0.0053

−0.0055
0.0030
−0.0029
0.0013
−0.0051
0.0085
−0.0437
0.0520
−0.0860
0.0674
0.0351
−0.0524

−0.0063
0.0030
−0.0032
0.0009
−0.0056
0.0015
−0.0112
0.0095
−0.0112
0.0095
0.0056
−0.0081

0.0010
−0.0009
−0.0014
−0.0045
−0.1259
0.0762
−0.0287
0.0469
−0.0338
0.0171
0.0189
−0.0472

−0.0046
0.0015
−0.0062
−0.0012
1.2684
−0.7181
−0.1162
−0.4199
0.3838
0.0801
−0.6154
0.7019

Table VII. Empirical rejection rates rr
̂ 𝛼 with type I error 𝛼 for the
exponential(1) distribution with 1000 simulations.
I

II

III

Scenario

rr
̂ 𝛼=0.01

rr
̂ 𝛼=0.05

rr
̂ 𝛼=0.01

rr
̂ 𝛼=0.05

rr
̂ 𝛼=0.01

rr
̂ 𝛼=0.05

LR test

0.050

0.014

0.013

0.055

0.994

0.997

exp(−1.5 + 0.1x)
1 + exp(−1.5 + 0.1x) + exp(−1 + 0.05x)
exp(−1 + 0.05x)
P(Ci = nt|Xi = x) =
1 + exp(−1.5 + 0.1x) + exp(−1 + 0.05x)
1
P(Ci = co|Xi = x) =
1 + exp(−1.5 + 0.1x) + exp(−1 + 0.05x)
P(Ci = at|Xi = x) =

(25)

Table VI shows the bias of the MLEs over the whole parameter space for the three simulation scenarios
as well as the bias of the RMLEs under the constraint (14) of no unmeasured confounding for the CACE.
The MLEs are approximately unbiased for all three scenarios. The RMLEs are approximately unbiased
for the first two scenarios where no unmeasured confounding for the CACE holds but are substantially
biased in scenario III, where no unmeasured confounding for the CACE does not hold.
Table VII shows the rejection rates of the compliance class model likelihood ratio test for the three
scenarios. For scenarios I and II, in which the null hypothesis of no unmeasured confounding for the
CACE holds, the compliance class model likelihood ratio test has an empirical rejection rate close to its
nominal type I error rate. For scenario III, in which the null hypothesis of no unmeasured confounding
for the CACE does not hold, the compliance class model likelihood ratio test has high power.

7. Application to study of high-level NICUs versus lower-level NICUs

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3541

We obtained birth certificates from all deliveries occurring in Pennsylvania between 1995 and 2005. The
Pennsylvania Department of Health linked these birth certificates to death certificates using name and date
of birth, and then de-identified the records. We then matched over 98% of birth certificates to maternal and
newborn hospital records using methods described in [21]. Over 80% of the unmatched birth certificate
records were missing hospital, suggesting a birth at home, or a birthing center. The unmatched records had
similar gestational age and racial/ethnic distributions to the matched records. The Institutional Review

Z. GUO ET AL.

Boards of The Children’s Hospital of Philadelphia and the Pennsylvania Department of Health approved
this study.
Infants included in this study had a gestational age between 23 and 37 weeks, and a birth weight
between 400 and 8000 gm. Birth records were excluded if the birth weight was more than five standard
deviations from the mean birth weight for the recorded gestational age in the cohort. There are 192,078
infants in the final cohort. The primary outcome for this study is neonatal death, defined as any death
during the initial birth hospitalization. The IV we consider is Z = 1 if a mother’s excess travel time to the
nearest high-level NICU compared with the nearest hospital is 10 min or less, Z = 0 if her excess travel
time is more than 10 min. The measured confounders 𝐗 are birth weight, an indicator for whether birth
weight is missing, the month of pregnancy that prenatal care started, an indicator for whether this month
is missing, mother’s education, an indicator for whether mother’s education is missing, and an indicator
for whether the mother went into labor prematurely (as compared with having a planned premature birth
by induced labor or C-section).
Mother’s excess travel time Z plausibly satisfies the IV assumptions 1–6 in Section 2.2 for the following
reasons. First, for the SUTVA, whether one mother lives near a high-level NICU or delivers at a highlevel NICU is unlikely to affect another mother and her infant, so SUTVA is plausible. Second, for the IV
being independent of unmeasured confounding, women do not expect to have a premature delivery, and
thus conditional on measured socioeconomic variables such as mother’s education, women do not choose
where to live on the basis of distance to a high-level NICU, making independence from unmeasured
confounding plausible [21]. Third, the exclusion restriction (no direct effect of excess travel time) is
plausible because most mothers have time to deliver at either the nearest high-level or low-level NICU
so that the marginal travel time should not directly affect outcomes [21]. Fourth, for the IV affecting the
treatment, excess travel time is correlated with whether a mother delivers at a high-level NICU because a
mother typically obtains prenatal care from and would prefer to deliver at a close by facility [23]. Fifth,
for the monotonicity assumption, if a mother would travel to go to a high-level NICU when living more
than 10 min further from the high-level NICU than the nearest low-level NICU, she would presumably
also travel to the high-level NICU if it were less than 10 min further than the nearest low-level NICU; thus,
monotonicity is plausible. Sixth, for the consistency assumption, although it is unlikely to hold exactly
because different high-level NICUs may differ in their level of care and different low-level NICUs may
differ in their level of care, it plausibly holds approximately; see [44] for discussion about interpreting
causal estimates when consistency does not hold exactly. In summary, mother’s excess travel time is a
plausible IV.
Lorch et al. [21] and Baiocchi et al. [9] have used mother’s excess travel time as an IV to estimate the
effect of a premature infant being delivered in a high-level NICU versus a low-level NICU for compliers,
that is, E(Y 1 − Y 0 |C = co). Here our focus is on using mother’s excess travel time to test whether there
is unmeasured confounding. Because the outcome, neonatal death, is binary, we consider the binary
outcome models of Section 4.1. First, we test the null hypothesis that there are homogeneous treatment
effects in terms of the measured covariates 𝐗 versus the alternative that there are heterogeneous treatment
effects, that is, test H0 ∶ 𝜶 1 = 𝟎 versus Ha ∶ 𝜶 1 ≠ 𝟎 in model (13). We use a likelihood ratio test to
test this. The test yields a p-value < 0.001, providing evidence of heterogeneous treatment effects. When
there are heterogeneous treatment effects, the DWH test may not properly control the type I error rate,
but the compliance class likelihood ratio test does properly control the type I error rate (Sections 5 and
6), and hence, we will use the compliance class likelihood ratio to test for unmeasured confounding.
Table VIII shows the results of the compliance class likelihood ratio test for unmeasured confounding.
There is strong evidence (p-value < 0.001) that (3) is violated, that is, always takers have different risks of
death than compliers conditional on the measured confounders 𝐗 when both deliver at high-level NICUs.
There is also evidence that never takers have different risks of death than compliers, that is, (4) is violated,

Table VIII. Compliance class likelihood ratio test of no unmeasured
confounding for the NICU study.

3542

Test of (3)
Test of (4)
Test of (3) and (4)

Copyright © 2014 John Wiley & Sons, Ltd.

Test statistic

Degrees of freedom

p-value

75.9
19.6
76.6

8
8
16

<0.001
0.012
<0.001

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

Table IX. Estimated probability of death For different compliance classes with
different covariate values.

Birth weight

Month
prenatal care
started

Mother’s
education

Pnt

Pco,0

Pco,1

Pat

1500
2
High school
0.057
0.047
0.030
0.052
2000
2
High school
0.019
0.018
0.011
0.017
2500
2
High school
0.006
0.006
0.004
0.005
1500
4
High school
0.056
0.051
0.031
0.049
2000
4
High school
0.019
0.019
0.011
0.016
2500
4
High school
0.006
0.007
0.004
0.005
1500
2
College
0.040
0.028
0.018
0.045
2000
2
College
0.013
0.010
0.007
0.015
2500
2
College
0.004
0.004
0.002
0.005
1500
4
College
0.041
0.031
0.019
0.043
2000
4
College
0.014
0.011
0.007
0.014
2500
4
College
0.004
0.004
0.002
0.004
For all sets of covariate values, the mother is assumed to have gone into premature labor,
where Pnt = P(Y 0 = 1|C = nt), Pco,0 = P(Y 0 = 1|C = co), Pco,1 = P(Y 1 = 1|C =
co), Pat = P(Y 1 = 1|C = co).

but the evidence is not as strong as for always takers (p-value = 0.012 compared with p < 0.001). In
summary, there is strong evidence of some unmeasured confounding.
Table IX shows, for various combinations of the measured confounders 𝐗, the estimated probabilities
of death from the model (13) for never takers delivering at low-level NICUs, compliers delivering at lowlevel NICUs, compliers delivering at high-level NICUs, and always takers delivering at high-level NICUs.
For example, for an infant weighing 1500 gm, whose mother started prenatal care in the second month of
pregnancy, whose mother has a high school education, and whose mother went into preterm labor, the risk
of death is 0.057 for never takers delivering at low-level NICUs, 0.047 for compliers delivering at lowlevel NICUs, 0.030 for compliers delivering at high-level NICUs, and 0.052 for always takers delivering
at high-level NICUs. This pattern of similar death rates for never takers and compliers delivering at lowlevel NICUs, considerably lower death rates for compliers versus always takers delivering at high-level
NICUs, and considerably lower death rates for compliers delivering at high-level NICUs versus low-level
NICUs holds for all combinations of the measured confounders 𝐗.

8. Conclusions and discussion

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3543

We have developed a test of whether there is unmeasured confounding when an IV is available. Our test
has correct type I error rate unlike the DWH test, which can have inflated type I error rates when there is
treatment effect heterogeneity. An important additional advantage of our approach over the DWH test is
that it breaks up the test into two parts (3) and (4), providing more information. For the NICU study, we
found evidence that never takers are at a little higher risk of death than compliers when both groups are
delivered at low-level NICUs and that always takers are at a much higher risk of death than compliers
when both groups are delivered at high-level NICUs. This latter piece of evidence means that infants who
are bypassing local hospitals to go to high-level NICUs (i.e., always takers) have unmeasured confounders
that makes them have a higher risk of death than infants who would only deliver at a high-level NICU if
living relatively near to one (i.e., compliers). This suggests that there is some triaging in the way infants
are delivering at high-level NICUs versus low-level NICUs; future studies could examine how effective
this triaging system is.
We have tested the null hypothesis that there is no unmeasured confounding. In some settings, we may
instead want to do an equivalence test of the null that the unmeasured confounding is greater than or equal
to a specified magnitude versus the alternative that it is less than this magnitude. For example, instead of
testing the null hypothesis of (3), we may want to test

Z. GUO ET AL.

H0 ∶ E(Y 1 |C = at, 𝐗) − E(Y 1 |C = co, 𝐗) ⩾ 𝜖 or ⩽ −𝜖 vs.
Ha ∶ −𝜖 < E(Y 1 |C = at, 𝐗) − E(Y 1 |C = co, 𝐗) < 𝜖,

(26)

where 𝜖 is an equivalence margin specified by a subject matter expert. We can test (26) using the two onesided test procedures [45, 46]. We test H0 in (26) at level 𝛼 by testing H01 ∶ E(Y 1 |C = at, 𝐗) − E(Y 1 |C =
co, 𝐗) ⩾ 𝜖 and obtaining one-sided p-value P1 and testing H02 ∶ E(Y 1 |C = at, 𝐗) − E(Y 1 |C = co, 𝐗) ⩽
−𝜖 and obtaining one-sided p-value P2 , and then rejecting H0 if max(P1 , P2 ) ⩽ 𝛼. Similarly, we could
implement an equivalence test of (4)
The compliance class likelihood ratio test of no unmeasured confounding developed in this paper
makes use of assumptions about the probability distribution of the outcome within compliance classes. In
this paper, we have considered normal and binary outcomes. In our application, we know that the outcome
is binary, but in applications in which the outcome is continuous, we may not know the probability
distribution of the outcome. Our test may not perform well when the assumed probability distribution
of the outcome does not hold, and it is useful to evaluate the goodness of fit of the assumed probability
distribution of the outcome. Zhang et al. [47] developed an approach to evaluate the goodness of fit for
a principal stratification model. If the goodness of fit is not adequate, a different probability distribution
model for the outcome can be considered. Our test can easily be extended to non-normal outcome models
by using an analogous EM algorithm as in Section 4.1. Also, rather than assuming a parametric model
for the outcome distribution, a semiparametric model can be assumed such as the semiparametric density
ratio model [7].
In this paper, we have focused on a binary IV. Although mother’s excess travel time is a continuous
variable, we dichotomized it to be a binary IV (whether the excess time is larger than 10 or not). In
practice, it is common that investigators dichotomize multi-level IVs into binary IVs as clinicians may
find it easier to think about the validity of the IV assumptions and the interpretation of the IV estimate in
terms of a binary IV. However, our method can be extended to a multi-level IV. Suppose the multi-level
IV satisfies an extended monotonicity assumption that an individual’s potential level of treatment is an
′
increasing function of the level of the IV, Azi ⩾ Azi if z ⩾ z′ [48]. Let Ti be the smallest z for which Azi = 1
z
where Ti = −∞ if Ai = 1 for all z and Ti = ∞ if Azi = 0 for all z. To test for no unmeasured confounding,
we can formulate a parametric model for Yi1 |Ti , 𝐗i and Yi0 |Ti , 𝐗i as in [48], fit the model by maximum
likelihood, and test (3) and (4).

Appendix A
The complete data log likelihood for the binary outcome is
lC =

n
∑

I(Zi = 1, Ai = 1)log P(Zi = 1|Xi )

[
]
+ I(Zi = 1, Ai = 1, Ci = co) log P(Ci = co|Xi ) + log f (Yi = y|Zi = 1, Ci = co, Xi )
[
]
+ I(Zi = 1, Ai = 1, Ci = at) log P(Ci = at|Xi ) + log f (Yi = y|Zi = 1, Ci = at, Xi )
[
]
+ I(Zi = 1, Ai = 0) log P(Zi = 1|Xi ) + log P(Ci = nt|Xi ) + log f (Yi = y|Zi = 1, Ci = nt, Xi )
+ I(Zi = 0, Ai = 0)log{1 − P(Zi = 1|Xi )}
[
]
+ I(Zi = 0, Ai = 0, Ci = co) log P(Ci = co|Xi ) + log f (Yi = y|Zi = 0, Ci = co, Xi )
[
]
+ I(Zi = 0, Ai = 0, Ci = nt) [log P(Ci = nt|Xi ) + log f (Yi = y|Zi = 0, Ci = nt, Xi )
[
]
+ I(Zi = 0, Ai = 1) log{1 − P(Zi = 1|Xi )} + log P(Ci = at|Xi ) + log f (Yi = y|Zi = 0, Ci = at, Xi )
i=1

The observed data log likelihood for the binary outcome is
l=

n
∑

I(Zi = 1, Ai = 1) log P(Zi = 1|Xi )

i=1

3544

+

n
∑

[
)
) ]
( )yi (
( C1 )yi (
at 1−yi
C1 1−yi
1
−
P
1
−
P
I(Zi = 1, Ai = 1) log P(Ci = at|Xi ) Pat
+P(C
=
co|X
)
P
i
i
i
i
i
i

i=1

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

Z. GUO ET AL.

+

N
∑
i=1
N

+

∑

[
( )yi (
) ]
nt 1−yi
I(Zi = 1, Ai = 0) log P(Zi = 1|Xi ) + log P(Ci = nt|Xi ) Pnt
1
−
P
i
i
[
( )yi (
) ]
at 1−yi
I(Zi = 0, Ai = 1) log P(Zi = 0|Xi )+log P(Ci = at|Xi ) Pat
1−P
i
i

i=1

+

n
∑
i=1
n

+

∑

I(Zi = 0, Ai = 0) log P(Zi = 0|Xi )
[
( )yi (
( C0 )yi (
)
) ]
nt 1−yi
C0 1−yi
I(Zi = 0, Ai = 0) log P(Ci = nt|xi ) Pnt
+P(C
=
co|x
)
P
1−P
1−P
i
i
i
i
i
i

i=1

Acknowledgements
This paper is dedicated to the memory of Dr. Thomas R. Ten Have. Dr. Ten Have was a wonderful mentor and
role model for Drs. Jing Cheng and Dylan Small. He continued providing encouragement on this research until
he passed away. The work of Drs. Jing Cheng and Dylan Small is supported by NIH/NIMH 1RC4MH092722-01
Revised, and Jing Cheng was also supported by grant NIH/NIDCR U54DE019285.

References

Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

3545

1. Rosenbaum PR. Observational Studies, Vol. 2. Springer: New York, 2001.
2. Angrist JD, Imbens GW, Rubin DR. Identification of causal effects using instrumental variables (with discussion). Journal
of the American Statistical Association 1996; 91:444–472.
3. Abadie A. Bootstrap tests for distributional treatment effects in instrumental variable models. Journal of the American
Statistics Association 2002; 97:284–292.
4. Hernán MA, Robins JM. Instruments for causal inference: an epidemiologist’s dream? Epidemiology 2006; 14:360–372.
5. Tan Z. Regression and weighting methods for causal inference using instrumental variables. Journal of the American
Statistical Association 2006; 101:1607–1618.
6. Brookhart MA, Schneeweiss S. Preference-based instrumental variable methods for the estimation of treatment effects:
assessing validity and interpreting results. International Journal of Biostatistics 2007; 3(1):Article 14.
7. Cheng J, Qin J, Zhang B. Semiparametric estimation and inference for distributional and general treatment effects. Journal
of the Royal Statistical Society: Series B 2009; 71:881–904.
8. Cheng J, Small DS, Tan Z, Ten Have TR. Efficient nonparametric estimation of causal effects in randomized trials with
noncompliance. Biometrika 2009; 96:1–9.
9. Baiocchi M, Small DS, Lorch S, Rosenbaum PR. Building a stronger instrument in an observational study of perinatal care
for premature infants. Journal of the American Statistical Association 2010; 105:1285–1296.
10. Durbin J, Errors in variables. Review of the International Statistical Institute 1954; 22:23–32.
11. Wu DM. Alternative tests of independence between stochastic regressors and disturbances. Econometrica 1973; 41:
733–750.
12. Hausman J. Specification tests in econometrics. Econometrica 1978; 41:1251–1271.
13. Wooldridge JM. Econometric Analysis of Cross Section and Panel Data, Vol. 2. MIT Press: Cambridge, 2010.
14. Basu A, Heckman JJ, Navarro-Lozano S, Urzua S. Use of instrumental variables in the presence of heterogeneity and
self-section: an application to treatments of breast cancer patients. Health Economics 2007; 16:1133–1157.
15. Brookhart MA, Rassen JA, Schneeweiss S. Instrumental variable methods in comparative safety and effectiveness research.
Pharmacoepidemiology and Drug Safety 2010; 19:537–554.
16. Tan Z. Marginal and nested structural models using instrumental variables. Journal of the American Statistical Association
2010; 105:157–169.
17. Hahn J, Ham JC, Moon HR. The Hausman test and weak instruments. Journal of Econometrics 2011; 160:289–299.
18. Adkins LC, Campbell RC, Chmelarova V, Hill RC. The Hausman test, and some alternatives with heteroskedastic data. In
Essays in Honor of Jerry Hausman: Advances in Econometrics, Baltagi BH, Newey WK, White HL (eds), Vol. 29. Emerald
Books: Warwick, United Kingdom; 515–456.
19. Lorch S, Myers S, Carr B. The regionalization of pediatric health care. Pediatrics 2010; 126:1182–1190.
20. Howell EM, Richardson D, Ginsburg P, Foot B. Deregionalization of neonatal intensive care in urban areas. American
Journal of Public Health 2002; 92:119–124.
21. Lorch SA, Baiocchi M, Fager C, Small D. The differential impact of delivery hospital on the outcomes of premature infants.
Pediatrics 2012; 130(2):270–278.
22. McClellan M, McNeil B, Newhouse J. Does more intensive treatment of acute myocardial infarction in the elderly reduce
mortality?Analysis using instrumental variables. Journal of the American Medical Association 1994; 272:859–866.
23. Phibbs CS, Mark DH, Luft HS, Peltzmanrennie DJ, Garnick DW, Lichtenberg E, McPhee SJ. Choice of hospital for delivery
– a comparison of high-risk and low-risk women. Health Services Research 1993; 28:201–222.
24. Rubin DB. Statistics and causal inference: comment: which ifs have causal answers. Journal of the American Statistical
Association 1986; 81:961–962.

Z. GUO ET AL.
25. Cole SR, Frangakis CE. The consistency statement in causal inference: a definition or an assumption? Epidemiology 2009;
20(1):3–5.
26. Frangakis CE, Rubin DB. Addressing complications of intention-to-treat analysis in the combined presence of all-or-none
treatment-noncompliance and subsequent outcomes. Biometrika 1999; 86:365–379.
27. O’Malley AJ, Normand SLT. Likelihood methods for treatment noncompliance and subsequent nonresponse in randomized
trials. Biometrics 2005; 61:325–334.
28. Zhou XH, Li SM. ITT analysis of randomized encouragement design studies with missing data. Statistics in Medicine
2006; 25:2737–2761.
29. Jo B, Asparouhov T, Muthén BO. Intention-to-treat analysis in cluster randomized trials with noncompliance. Statistics in
Medicine 2008; 27:5565–5577.
30. VanderWeele TJ, Arah OA. Bias formulas for sensitivity analysis of unmeasured confounding for general outcomes,
treatments, and confounders. Epidemiology 2011; 22:42–52.
31. Imbens G. Nonparametric estimation of average treatment effects under exogeneity: a review. Review of Economics and
Statistics 2004; 86:1–29.
32. Joffe M, Brensinger C. Weighting in instrumental variables and G-estimation. Statistics in Medicine 2003; 22:1285–1303.
33. Small D, Ten Have T, Joffe M, Cheng J. Random effects logistic models for analysing efficacy of a longitudinal randomized
treatment with non-adherence. Statistics in Medicine 2006; 25:1981–2007.
34. Imbens GW, Rubin DB. Estimating outcome distributions for compliers in instrumental variables models. Review of
Economic Studies 1997; 64:555–574.
35. Imbens GW, Rubin DB. Bayesian inference for causal effects in randomized experiments with noncompliance. The Annals
of Statistics 1997; 25:305–327.
36. Casella G, Berger RL. Statistical Inference 2nd ed., 2001,375.
37. Lange K. A quasi-Newton acceleration of the EM algorithm. Statistica Sinica 1995; 5:1–18.
38. Jamshidian M, Jennrich RI. Acceleration of the EM algorithm by using quasi-Newton methods. Journal of the Royal
Statistical Society, Series B 1997; 59:569–587.
39. Press WH, Teukolsky SA, Vetterling WT, Flannery BP. Numerical Recipes. The Art of Scientific Computing, (3rd edn).
Cambridge University Press: New York, 2007.
40. R Core Team. R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing, 2013.
(Available from: http://www.R-project.org/) [Accessed on 20 Febuary 2014].
41. Bowden RJ, Turkington DA. Instrumental Variables. Cambridge University Press: New York, 1984.
42. White H. Asymptotic Theory for Econometricians. Academic Press: New York, 1984.
43. Angrist JD, Krueger AB. Empirical strategies in labor economics. In Handbook of Labor Economics, Ashenfelter O,
Card D (eds). Elsevier: Amsterdam, 1999; 1277–1366.
44. VanderWeele TJ, Hernan MA. Causal inference under multiple versions of treatment. Journal of Causal Inference 2013;
1:1–20.
45. Schuirmann DL. On hypothesis testing to determine if the mean of a Normal distribution is contained in a known interval.
Biometrics 1981; 37:617.
46. Westlake WJ. Response to T.B.L. Kirkwood: bioequivalence testing - a need to rethink. Biometrics 1981; 37:589–594.
47. Zhang JL, Rubin DB, Mealli F. Likelihood-based analysis of causal effects of job-training programs using principal
stratification. Journal of the American Statistical Association 2009; 104(485):166–176.
48. Glickman ME, Normand ST. The derivation of a latent threshold instrumental variables model. Statistica Sinica 2000;
10(2):517–544.

Supporting information
Additional supporting information may be found in the online version of this article at the publisher’s
web site.

3546
Copyright © 2014 John Wiley & Sons, Ltd.

Statist. Med. 2014, 33 3528–3546

