Root-N-Consistent Semiparametric Regression
Author(s): P. M. Robinson
Source: Econometrica, Vol. 56, No. 4 (Jul., 1988), pp. 931-954
Published by: The Econometric Society
Stable URL: https://www.jstor.org/stable/1912705
Accessed: 03-10-2018 07:42 UTC
REFERENCES
Linked references are available on JSTOR for this article:
https://www.jstor.org/stable/1912705?seq=1&cid=pdf-reference#references_tab_contents
You may need to log in to JSTOR to access the linked references.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

The Econometric Society is collaborating with JSTOR to digitize, preserve and extend access
to Econometrica

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

Econometrica, Vol. 56, No. 4 (July, 1988), 931-954

ROOT-N-CONSISTENT SEMIPARAMETRIC REGRESSION
BY P. M. ROBINSON1
One type of semiparametric regression on an Rp X R"-valued random variable (X, Z)

is ,B'X+ @(Z), where P3 and 0(Z) are an unknown slope coefficient vector and function,
and X is neither wholly dependent on Z nor necessarily independent of it. Estimators of P8
based on incorrect parameterization of 0 are generally inconsistent, whereas consistent
nonparametric estimators deviate from P8 by a larger probability order than N- 1/2, where

N is sample size. An estimator generalizing the ordinary least squares estimator of ,B is
constructed by inserting nonparametric regression estimators in the nonlinear orthogonal

projection on Z. Under regularity conditions ,B is shown to be N'/2-consistent for /B and
asymptotically normal, and a consistent estimator of its limiting covariance matrix is given,
affording statistical inference that is not only asymptotically valid but has nonzero
asymptotic first-order efficiency relative to estimators based on a correctly parameterized 0.
We discuss the identification problem and /B's efficiency, and report results of a Monte
Carlo study of finite-sample performance. While the paper focuses on the simplest
interesting setting of multiple regression with independent observations, extensions to other
econometric models are described, in particular seemingly unrelated and nonlinear regressions, simultaneous equations, distributed lags, and sample selectivity models.

KEIwoRDs: Regression, semiparametric model, kernel nonparametric estimators, root
N-consistent estimation, central limit theorem, SUR model, linear simultaneous equations,
distributed lags, heteroskedasticity, sample selectivity.

1. INTRODUCTION

STATISTICAL INFERENCE on a multidimensional random variable commonly
focuses on functionals of its distribution that are either purely parametric or

purely nonparametric. A reasonable parametric model affords precise inferences,
a badly misspecified one, possibly seriously misleading ones, while nonparametric
modeling is associated both with greater robustness and lesser precision. An
intermediate strategy employs a semiparametric form, such as the regression
function

(1.1) E(YIX,Z) =fl'X+@(Z) almostsurely(a.s.),

where (X, Y, Z) is an Px x M q-valued observable random var

MP-valued unknown parameter, and 9 is an unknown real function
Z, and fi are column vectors and the prime indicates transposition
(1.1) might be the outcome of logging a multiplicative model.
Versions of (1.1) have been studied by Cosslett (1984), Shiller (1984), Wahba
(1984, 1985), Stock (1985), Engle et al. (1986), N. Heckman (1986), Rice (1986),

Schick (1986). The statistical objectives in these papers vary, as do the motivating
applications. In most, though not all, of them Z is a scalar nonstochastic design

variable, typically a time index. Our own aim is precise estimation of P3 when Z
l This article is based on research funded by the Economic and Social Research Council (ESRC)
reference number: B00232156. I thank Miguel Delgado for carrying out the simulations reported in
Section 6, and two referees for many incisive and constructive comments which have stimulated
substantial improvements. A previous version was circulated under the title "Adaptive Semiparametric Regression."
931

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

932

P.

M.

ROBINSON

is stochastic and of arbitrary dimension, indeed the value of q nontrivially

influences our methodology and theory. The components of 13 may have interesting economic significance, and some hypotheses of interest may be expressible

purely in terms of ,B, in which event the building of a full parametric model may

be of secondary importance. Good estimates of 13 can assist also in parameterizing 0. We picture a practitioner faced with a large cross-sectional data set
including many candidate explanatory variables, who on the basis of economic
theory or past experience with similar data feels able to parameterize only some
of them. Very crudely, (1.1) describes a qualitative unevenness in prior information. It is possible also to rationalize (1.1) as emerging from some econometric
models involving latent variables: extending models developed by J. Heckman
(1976) and others, a dependent variable is censored or truncated when a latent
variable of possibly unknown distributional form exceeds a (possibly unknown)

function of Z; extending a model of Zellner (1970), a linear regression includes
both observed and latent variables, where the latter are an unknown function of

Z. It is also possible to interpret 13 as the coefficients of the "surprise" component of X, that is the part that cannot be predicted using Z. Both (1.1) and the
conditions we impose on it are restrictive in terms of direct applications, but we

also describe how some of these conditions might be relaxed and how more
general semiparametric models than (1.1) might be estimated.

Under regularity conditions, ordinary least squares (OLS) regression of Y on
X alone consistently and efficiently estimates 13 when E(XO(Z)) = 0, as when
E(X) = 0 and X and Z are statistically independent. Such orthogonality is
present in certain experimental designs and models containing dummy variables,

as well as in some modeling strategies in which Z is not fully or parsimoniously
specified, for example orthogonal polynomial and trigonometric regression. Or-

thogonality can be checked, but it is exceptional, particularly when the explanatory variables include stochastic ones or are large in number. The bias of OLS in
the presence of a nonorthogonal omitted variable is explained in elementary
econometric textbooks. In much applied work there is an understandable tendency to include candidate explanatory variables in an ad hoc, typically linear,

fashion, resulting again in biased estimation. Rigorous statistical analysis of
parametric estimators in the presence of model misspecification is possible; under

typical regularity conditions OLS estimators of 13 based on incorrect parameterization of 9 are asymptotically normal about 13 + B after N1/2 norming, where N
is the number of observations and the "asymptotic bias" B reflects the unknown

9 (see, e.g., White (1982)). Some analysis of B may be possible, allowing

speculation about the direction of bias and the signs of 13P's elements rela

13 + B's. The omission of many variables, or a "large" discrepancy between the
true 0 and the misspecified one, does not necessarily result in incorrect conclu-

sions. On the other hand some applied studies indicate high sensitivity of
parameter estimators to misspecification of the rest of the model. Automatic or

semi-automatic algorithms help bridge the gap between theory and model specification (see, e.g., Amemiya (1980), Stone (1981), and references therein). For
example, stepwise regression selects a parsimonious model with good explanatory

power while keeping some variables (i.e., X) in the regression irrespective of their

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 933

t ratios, though it searches only over linear models. Specification tests are
available, but failure to reject correct specification does not necessarily inspire
confidence in the null hypothesis, and rejection necessitates continuing the model
search.

Consistency for ,8 in the presence of unknown 9 is possible, however. Perhaps
the most obvious source is nonparametric estimation of e(x, z) = E(YIX = x, Z

= z) at a point (x, z). Let e(x, z) be (say) a Nadaraya-Watson kernel estimator
of e(x, z) with differentiable kernel (see, e.g., Prakasa Rao (1983, pp. 33-37,
180-200, 239-247, and Section 2 below)); when X and Z do not overlap,

e=(x/dx)e(x, z) estimates /3 consistently under quite general condition
e.g., Schuster and Yakowitz (1979). Unfortunately e and ex are not N' 2

sistent, because the asymptotically correct centering at ,B is due to a "bandwi
parameter approaching 0, with the effect that, asymptotically, only a vanis
small proportion of the data, "near" (x, z), is used. Indeed, the greater p + q, the

further we fall short of N1/2-consistency, and ex converges even slower than e;
Stone (1982) discusses optimal rates of convergence in nonparametric regression
and its derivatives. Estimators that are consistent but not N1/2-consistent generate inferences which, though asymptotically valid, have zero efficiency relative to

ones based on NI/2-consistent estimators, and while the latter comparison
presents an exaggeratedly pessimistic impression of the finite-sample reality, it is
debatable whether nonparametric estimators should necessarily be preferred to

the "N1/2-inconsistent" ones based on incorrectly parameterizing 9. Averaging ex
over n (x, z)-values only improves rates of convergence if n increases with N, for
example
N

(1.2) * = ex(Xi, Zi) i
i=1

where xi, zi might be either the observed X's and Z's or a sequence of
representative design points, and the wi are probability weights, e.g., w, N- .
(It seems /3* is N'/2-consistent for /3 under suitable conditions, and thus
competitive with the estimator /3 developed below. One might establish /3*'s
limiting distribution and compare its efficiency with Af's.)
Other modifications of nonparametric regression should be mentioned.

Elbadawi et al. (1983) and Gallant (1985) approximate their models by infinite

series, the early terms representing the parametric part (our /'X), the remaining

ones (a trigonometric expansion) representing the nonparametric part (our 9).
The hope is that few of the latter terms will be required, and that /3 will be
estimated with good precision. However, / is not really on a different footing
from the coefficients of the trigonometric expansion, and consistency relies on the
number of terms in the series, hence the number of parameters, going slowly to
infinity with N. While the estimators of Elbadawi et al. (1983) and Gallant (1985)
might well be better in finite samples than pure nonparametric ones, they
converge slower than N1/2 unless the true regression is approximated at a fast

enough rate as N - co. (Actually, identification of /3 requires strong restrictions
on 9; see Section 4 below.) Stone's (1982, 1985) results imply that nonparametric

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

934

P.

M.

ROBINSON

estimators exploiting the additive structure of (1.1) can achieve faster rates of
convergence than pure nonparametric regression on X and Z, but his estimators

do not exploit the partial parameterization of (1.1), and fall short of Nl/'2-consistency. Projection pursuit regression (Friedman and Stuetzle (1981)) entails
some structural restriction of 6, and it is not clear whether it can produce

N 1/2-consistency.
In most of the earlier work relating to (1.1) that was referenced above,

N1/2-consistency of estimation of P is not established, indeed the em
sometimes as much if not more on estimating 0. The exceptions are N. Heckman
(1986) and Rice (1986), who assume Z is a scalar nonstochastic design variable
on the unit interval, the "observations" on which get dense as N-- x, and
Schick (1986), who assumes Z is a scalar uniform random variable. Our setting of

stochastic multi-dimensional Z, of quite general distributional form, is more

suited to econometric applications. Like N. Heckman and Schick we establish not

only N1/2-consistency but asymptotic normality of our estimator (which differs
from theirs and Rice's), and also we give a consistent estimator of the covariance

matrix in the limiting distribution, providing the usual basis for large-sample
interval estimation and hypothesis testing. The only information on finite-sample
properties we present is the outcome of some Monte Carlo simulations.

We compare and contrast our problem and results with ones in the "adaptive
estimation" literature. Authors such as Bickel (1982) and Manski (1984) presented asymptotically efficient estimators of linear and nonlinear regression

estimators in the presence of residuals of unknown distributional form, while

Carroll (1982), Robinson (1985) presented regression estimators that achieve the
asymptotic Gauss-Markov bound in the presence of residuals suffering from

heteroskedasticity of unknown form. Like these authors, we insert nonparametric
shape estimators of the nonparametric component in a standard "parametric"
estimator. Unlike them, we are unable to claim efficiency of our semiparametric
estimator, since the "orthogonality" between the parametric and nonparametric
components of their models (see Begun et al. (1983)) is in general lacking in ours,

and we merely isolate some parametric 6 for which our approach happens to be
as efficient as one which uses information on O's form.
2. ESTIMATOR OF A

The model (1.1) implies that Y- E(YIZ) = ,'(X- E(XIZ)) + U, where
E(UIX, Z) = 0 a.s., suggesting that estimators of the regression functions

E(XIZ), E(Y Z) be inserted prior to application of a standard rule, such as
no-intercept OLS. While a variety of nonparametric regression estimators is

available (two leviews are Prakasa Rao (1983, pp. 239-256), Collomb, (1985)),
the technical difficulties described in Section 3 below are conveniently overcome
by a subset of the Nadaraya-Watson kernel estimators. Introduce even functions
k: 9? and K: pqM related by
q

(2.1) K(z) = k(zi),
i=l

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 935

where z, is z's ith element. Let a be a po

sequence A1,..., AN' introduce the notation

(2.2) Ai= (Naq) 1E Aj.ij Ki= K( i Z),
j=1a

and define, with 11 1, fi=
Section 3, fi "estimates" f

random argument Zi, whi
some other applications of
difficulty owing to the ra
out small fi as do, e.g., Bic
Ii = I(fifI > b), where I is t

(2.3)

/3

where

=SX

for

SX-*,

scalar

N-1EY2N7AiB/I

or

and

Y-

colum

SA

=

S

(2.4) SA-A, B-B = (Al *iAN)

x (diag(Il,... , IN) DD 'diag(Il,... , IN))}(Bl '... BN) 9
where D is the N-rowed identity matrix minus the matrix with (i, j)th element

Kij/fj, so ,B has a generalized least squares (GLS) interpretation, as well as a

no-intercept OLS one. Because Kij = Kji, Kii K(O), only 1N(N - 1) distinct

Kij need be computed; nevertheless (2.3) entails O(p2qN2) operations.
If the Xi, Yi are replaced in ,B by the linear OLS predictors of the Xi, Y, we
have the OLS estimator ,B, say, that corresponds to taking @(Z) linear in Z;

indeed if we take k(u) cc I(I u j < 1) and a large enough, ,B reduces to OLS tha
assumes @(Z) constant. This similarity of /3 to a standard parametric estimator
(not shared by ,3* in (1.2), for example) seems attractive in view of 3 's well
known optimality properties, and it extends to the structure of formulae for
standard errors (see the theorem in Section 3), the only additional statistic needed

to calculate N1/2( 8 - /3)'s estimated covariance matrix a2SX k being

A2 = =Sy_?-+ 2Sy ,x-kf + f'Sx- fi

which estimates a2 =V(YI X, Z), assuming the residuals from (1.1) are con

ally homoskedastic. The extension of ,B to more general semiparametric mo
analogous to f8's in parametric models, as will be indicated in Section 7. ,B a
differ in /'s use of residuals from the best (in least squares sense) predictor
and X given Z, rather than the best linear predictors, and in computational
terms the difference is immense, increasing rapidly with N and q. /3 is likely to be
more expensive of computer time than nonlinear least squares (NLLS) when 0 is

nonlinear in parameters, though its closed form structure is an advantage, it is
straightforward to program, and it avoids the need to choose a vector of starting
values for the iterations and the possibilities of slow or nonexistent convergence.
To compare with other semiparametric treatments of (1.1), Wahba (1984, 1985),

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

936

P.

M.

ROBINSON

Shiller (1984), Engle et al. (1986), N. Heckman (1986), and Rice (1986) use spline
estimation; Stock (1985) uses (untrimmed) kernel estimation, but his focus is not
,B; Schick (1986) uses the kernel idea, but his estimator for his version of (1.1) is
quite different in form. Comparing /B with N1/2-consistent estimators proposed
for other problems, Bickel (1982), Manski (1984), Robinson (1987), Powell et al.
(1986), Schick (1986), and others, all employ, for technical reasons, an element of
" sample-splitting," which in our case might entail replacing N in (2.2) by M < N,

then constructing Sx- *, Sx_ x y- x by summing only over the remaining N - M
observations. By avoiding this device, /3 makes fuller use of the data.
The dependence of /3 on the user-supplied numbers a and b is an undesirable
feature shared with other semiparametric estimators that employ nonparametric
"shape" estimation. The Theorem sets conditions on a and b's rate of decay as
N -x o that are virtually useless to the practitioner. It is not obvious how

sensitive ,B is to a and b, but the effects of extreme choices, while possibly not
catastrophic as in the case of pure nonparametric estimation, are liable to be
serious: "large" a can induce bias, "small" a, imprecision, because l/a can be
thought of like the dimensionalit,y of a parameterization of 0; a "large" b loses

efficiency, a "small" b allows Xi and Yi with small denominators fi to exert
undue influence. Automatic methods such as cross-validation offer an alternative

to trial-and-error choice of a, and it is easy to suggest suitable cross-validating
objective functions, but we will not discuss the details because our theorem
unfortunately does not cater to data-driven a to b. In connection with a, when
q > 1 some refinement in /3 is desirable because of likely scale differences in Z's

elements, indicating that K's argument in (2.2) should be replaced by a- 1(Zi
Zj) where a is here either a diagonal or a positive definite matrix (in the latt
case K is a more general multivariate function than (2.1)). The conditions on a
in our Theorem are straightforwardly generalized in the manner of conditions of
Cacoullos (1966) for diagonal a, and Robinson (1983) for matrix a. We have not
bothered to treat this extension explicitly because our conditions and proofs are
already somewhat complicated, and merely note that it suffices, in the diagonal-a
case, for each diagonal element to decay as N -*00 at the same rate. One

alternative to multidimensional a is scaling the Zi, via the estimated stand
deviations or covariance matrix, though our conditions do not automatically
require that Z have infinite variance.

Finally, we can use ,B to form "estimators" of 0(Z1), 0(Zi)= Yi-/'Xi;

predictors of Y (conditional on Xi, Z1), 1i = /3'Xi + 0(Z1); and estimated residu

als, U = Y- Y-, 1 < i < N. (In fact, a2= SU.) Given (1.1), 'i and Ui should

improve on predictors and residuals based on pure nonparametric regression,
though we make no study of their properties.

3. CONDITIONS AND THEOREM

With the definitions U= Y- /8'X- 0(Z), 0i,= 0/f^, di= UL/lf, w

(3.1)

A

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 937

The component 0i - Oi of the "residual" in (3.1) p
it is hard to see how N1/2-consistency of /B can
the property Sx _k o- = op(N- 1/2). Assuming

= E(XjZ = z) exists, and defining V= X- {(Z), it i

op(N-1/2) and SE-,o_#=op(N-1/2). The last r
establish. After centering the i - i and 0i -

conditional on the Zi, it is not difficult to show
indeed op(N-1/2) so long as a does not approach
this type of condition on a is required elsewhe
However, this centering introduces a term refle
"estimators" 0 and (i of Oi and {i. Such bias can
setting a small enough, to establish S -_ p 0 an

achieving the more ambitious goals of St -g

sistency of ,B, simply by making a approach 0 suit

possible because of the aforementioned limitatio
as in much statistical theory for kernel estima
Stone (1982)) the upper bound on a's rate of deca
dimensionality q of Z increases, so much so tha
N1/2-consistency requires special measures to ens
competing restrictions even exists.

We adopt the "higher-order" kernel approach t
Bartlett (1963) for nonparametric probability an
since developed by many authors and featured pr
ture: a sufficiently smooth function behaves loca

ciently high order, and if this property is exploite
"moments," the bias decreases sufficiently rapidly

DEFINITION 1: K,, 1> 1, is the class of even fun

(3.2) u'k(u)du= Sio (iO...,I-l),

(3.3) k(u) = 0((1 +I Ull+l+e)), some E> 0,
where Si' is Kronecker's delta.
The requirement that k be bounded and integrate to 1 makes f a sensible

estimator of f(Zi). For given 1 satisfying (3.2), (3.3) has a slightly stronger tail
condition on k than f I u'k(u) I du < xo, which is usually employed in the higher-

order kernel literature (see, e.g., (23) on p. 44 of Prakasa Rao (1983)), but kernels
used in practice usually have compact support or decay exponentially. Some of
the kernel literature emphasizes weak conditions on k as a priority, but for
implementation it suffices that the conditions admit a convenient k, and practical
experience suggests less sensitivity to k than to a. If (3.2) holds for some odd 1 it

holds for 1+ 1 also under (3.3). X, contains no nonnegative functions when
1 > 3, indicating the potential for negative estimates of the density of f, although

this seems of little concern in our context. As indicated by a number of authors

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

938

P.

M.

ROBINSON

(e.g., Prakasa Rao (1983, p. 44)) a k e Y is straightforwardly constructed.
Consider, for even 1 > 2,
1/2(1-2)

(3.4) k(u)= E cjU2j4(U),
j=O

where %P is even. Given that we can evaluate the moments m21= Ju2jA(u) du

0 j (1 -2), as readily we may when + (u) = 1I(IuI <1) or + (u)=

(21T) -1/2exp(-_ u2), substitution of the cj satisfying the linear system of
(I - 2) simultaneous equations 01!g'2)cJm2(i+J) = iO' 0 < i < (1 - 2), into (3.4)
produces a k E Xl, if 4 (u) = 0((1 + u 1 21-1 + e)1).

The classes X, confer increasingly small bias on nonparametric kernel esti
tors as 1 increases, but also increasingly large asymptotic variance, the latter

varying directly with Jk(u)2 du. However, the asymptotic distribution in the
Theorem below is independent of k, detecting no advantage or disadvantage in a

X, when 1 is chosen arbitrarily greater than required. Nevertheless, in finit
samples A may inherit variance properties of the kernel estimators from which it
is formed, as might be revealed by a closer approximation to the distribution of

A. Thus, while increasing 1 cannot shrink, and may well widen, the band of
a-sequences satisfying our Theorem, we caution against too generous a choice

of 1. It is interesting that whereas the classes Y, play useful roles of bias-re

tion and of widening the spectrum of admissible bandwidths in nonparametric

estimation, they are decisive in our problem, which requires dealing with a

greater (N1/2) norming than in the central limit theorem for q-variate no
metric estimators ((Naq)l/2). A related bias-reduction device is the "general
jacknife" method suggested by Schucany and Sommers (1977) for kernel density
estimators, later developed by other authors, which would require q + 1 bandwidth numbers to be selected, instead of our single a. In fact, Schucany and
Sommers' approach is used in a different semiparametric estimation problem
from ours by Powell et al. (1986), who extend Stoker's (1986) work on the model

E(Yj X) = F(,B'X) where F is unknown, and there are no functional relation-

ships between components of X. Stoker independently rediscovered a result used
previously by Beran (1977) and Cox (1985) in other semiparametric and nonpara-

metric problems, that h(X) and X's score function have covariance

E((a/dX)h(X)), to suggest a simple estimator of j8 up to undetermined scale
that depends on finite parameterization of X's score function. In a spirit similar
to (1.2), Powell et al. relax the latter requirement by using nonparametric kernel
estimation of the derivative of X's density, solving a bias problem analogous to
ours via an extension of Schucany and Sommers' approach.

The potential of the X, classes to produce N1/2-consistency, or to widen t
band of admissible a-sequences, will not be realized unless the functions 0, (, and
f are collectively sufficiently smooth, and all else being equal it seems reasonable

to suppose that the smoother they are, the better A will be. Let 1 I denote

Eucidean norm.

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 939

DEFINITION 2: a9 I a> 0a O > 0, is the class of f

ing: g is (m - 1)-times partially differentable, for m - 1 < ,u < m and all z; for

some p > O, supy,,f7 Ig(y)-g(z) - Q(Y, z)Il/y-zI < h(z) for all z, where
YZP= {Y: Y - zI <p}; Q = 0 when m = 1; Q is a (m - 1)th-degree homogeneous polynomial in y - z with coefficients the partial derivatives of g at z of
orders 1 through m - 1 when m > 1; and g(z), its partial derivatives of order
m - 1 and less, and h(z), have finite ath moments.

The functions in O,A are thus expanded in a Taylor series with a local Lipschitz

condition on the remainder, (a, u) depending simultaneously on smoothness and

moment properties. Bounded functions in Lip(,u) (the Lipschitz class of degree

t) for O < ,u < 1 are in 9; for . > 1, C contains the bounded and (m - 1)-times
boundedly differentiable functions whose (m - I)th partial derivatives are in

Lip (,I-m + 1). In applying 9, to f, we take a = oc, but we allow for a < c in
Definition 2 because we have no wish to require that Z, ( or 0 are a.s. bounded.

For example, a degree-m polynomial in Z is in 9OOa when E ZI ma < Xo.

THEOREM: Let the following conditions hold: (i) (Xi, Yi, Zj), i = 1,2,..., ar

independent and distributed as (X, Y, Z); (ii) (1.1) is true; (iii) U is independent of

X,Z; (iv) E(U2)=a2< oc; (v) EIX14< oo; (vi) Z admits apdff cA, for some

X > 0; (vii) t E 9 2, for some ,u > 0; (viii) 0 E ,4, for some v > 0; (ix) as N -x 00

Na-2qb4 00 a2min(A+1,)+2min(X+1,)b-4 0, amin(A+1, 2 , y,)b - 0, b O 0;

(x) k E max(l+m-1,l+n- 1), for the integers 1, m, n such that 1- 1 <X <1, m - 1
< ,u < m, n - 1 < v < n. Then the condition

(3.5) 0=E[{(X-E(XIZ)} {X-E(XIZ)}'] is positive definite
is necessary and sufficient for N1/2(/3 - _) 4 N(O o 25-1) and a - 4 a2 1
The proof of this theorem is presented in the form of Appendices. Notice that

(ix) and (x) are to be satisfied simultaneously, for X, ,u, v, 1, m, n satisfying the

stated inequalities, so that, for example, when k E Y2 only, the lower bounds on
a's rate of decay are no better than Na 8b -4 __ 0, a 2b-2 - 0, no matter the
degree of smoothness prevailing. While (ix) prevents b from converging to 0 too
fast, there is nothing to stop it converging arbitrarily slowly. A necessary
condition for reconciling the components of (ix) is
(3.6) X> q-1, X+,u>q-1, X+v>q-1, ,u+v>q.

Conditions (vi)-(viii) are complicated but it is not hard to find examples
satisfying them, as the discussion of Definition 2 indicated, and some simple ones
are used in the simulations of Section 6. Although some smoothness in f, {, 0 is
needed even when q = 1, this need not amount to differentiability, and for other
smallish values of q (vi)-(viii) may not be excessive. Very smooth f, {(f, 0) can
compensate for a not-very-smooth @(t). In view of (3.6), a necessary condition

for (x) is that k Et q 1. Given sufficient smoothness in f, ( and 0, when q < 3,
2 (which includes all even, bounded pdfs with finite fifth moments) admits

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

940

P.

M.

ROBINSON

suitable a and b sequences, although the greater the order of X the greater the
range of a, b sequences satisfying (x). The main restriction on the explanatory

variables is that discrete components of Z (but not X) are ruled out. In fact it is
not difficult to allow Z to have components that are discrete with finite support,

and we can see how to achieve some further relaxation when q < 3, as well as a
variety of trade-offs between conditions, but still the difference between our
conditions on explanatory variables and unknown functional forms and the
weaker ones of Robinson (1987) for a different semiparametric regression problem is considerable, and warrants further investigation.
4. IDENTIFICATION

The necessary and sufficient condition (3.5) is an identification condition,

unfortunately a very restrictive one. It prohibits ,B from including an "intercept"
coefficient; only "slope" coefficients can be estimated. This is less a drawback of

Ai than a consequence of the generality of the semiparametric model (1.1):
,B'X+ 0(Z) = (a + ,B'X) + {0(Z) - a), for all a, and 0(Z) may be redefined as
0(Z) -a. It is possible to identify a if the model is restricted further; for
example Schick (1986) assumes 0 integrates to zero and Z is uniformly distributed, and in fact considers the efficient estimation of a under further conditions.
More generally, (3.5) prevents any element of X from being a.s. perfectly
predictable by Z in the least squares sense. This rules out such important cases as

an unknown regression function of a single variable Z, with ,B'X representing a

truncated Taylor expansion and 0 taking care of the remainder (c.f. White, 1980).
Such models could be said to be more nonparametric than semiparametric (they
are "seminonparametric" in Gallant's (1985) terminology), and again it is the

unrestricted nature of 0 which excludes them, not our method of estimation,

because B'X + 0(Z) = {f 'X + 71(Z)) + ( 0(Z) - 71(Z)}, for all 71(Z). While ,B is
not identified in the linear model

(4.1) Y= a+1'X+y'Z+U

if any X element is linear in Z, (1.1) forbids more general forms of dependence,

and it is only to be expected that this more loosely specified model would entail
stronger identification conditions. Notice that (nonlinear) functional relationships
among X elements are not ruled out. Notice also that identification may be
possible even if X uniquely defines Z, when the converse is not true: for example,

if p=q=1 and Z=X2, then {(z)=Vz(1-2P) and '5=4P(1-P)E(X2),
where P = P(X < 0), so it is necessary and sufficient that X be n
tive nor nonpositive. Given that no elements of the prediction error X - {(Z) are
a.s. zero, the additional condition implied by (3.5) is their lack of multicollinearity, which fails if X itself is collinear.
5. EFFICIENCY OF f

Suppose 0 is a known, partially differentiable function of Z and of a r-dimen-

sional unknown parameter vector 8, 0(Z; 8). If (,8+, 8+) is a NLLS estimator of
This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 941

(,B, 8), then it is well known that under regularity conditions the covariance
matrix in the limiting normal distribution of N1/2(/'- /3) is

(5.1) a ( -Ca a CX)

where Cx = E(XX'), Cxa= E{ X(ad/8)'O(Z; 8)}, Ca = E {(a/dS) x

O(Z; 8)(ad/a)'O(Z; 8)}. Note that (5.1) is the asymptotic Gauss-Markov bound
in case (4.1), and in the nonlinear case is minimal with respect to the class of
weighted NLLS estimator, when U is conditionally homoskedastic, as we have
assumed.

By the Schwarz inequality, (5.1) < a2ck1, so /B+ is at least as efficient as /B.
There is equality between 2 5-1 and (5.1) if and only if E{ E(XI Z)E(XI Z)'} =

xaC-8Cax, that is if

(5.2) E(XIZ)= r(ala s) ( Z; s), a.s.,
for some p X r matrix r. Of course (5.2) includes the case of @(Z) actually

constant, so that at least no efficiency has been lost by our elaborate estimator /
relative to OLS estimation of slope coefficients, which is all that is required then.
If, more generally, O(Z; 8) = a + y'Z, (5.2) can be written

(5.3) E(XIZ) = rF + F2Z, a.s.,
the necessary and sufficient condition for ,B to attain the Gauss-Markov bound

with respect to (4.1). It immediately follows that P3 is then also asymptotically as
efficient as the maximum likelihood estimator based on (4.1) when the distribu-

tion of Y given X, Z is normal. Often (5.3) is assumed in parametric estimation
of "surprise" models.
The intuition behind efficiency condition (5.3) is seen by rewriting (4.1) as

Y= (a + /3'rl) + ,B'V+ (/t'r2 + y')Z + U, under (5.3). By construction, Z and
are orthogonal and E(V) = 0, so were V observable, regressing Y on V would

asymptotically efficiently estimate /; the Theorem demonstrates that /3 is as
totically as efficient as this regression. When /3 is not efficient in this sense, a
indeed no element of the vector equality (5.3) is true, an approximate level-a

Hausman (1978)-type specification test consists of rejecting (4.1) if (with Z. =

(1, Zi'))

(5.4) N-2,8,8 '[Sx - N EXi Xi,
- Ex2Z'(ZiZ') 1 E2ix'}j1 t_A

exceeds the 100(1 - a)th percentile of the P distribu

replaced in (5.4) by the residual mean square in the OLS regression fit of (4.1).

Computationally, (5.4) is far more expensive than statistics based on parametric
omitted variables, and it should be less powerful in the direction of such
alternatives, but if /3 has already been computed (5.4) entails little extra work
might be expected to enjoy reasonable power against a range of alternatives.
Necessary and sufficient conditions on X and Z for (5.3) are given by Kagan
et al. (1973, pp. 11, 12). One interesting case of (5.3) is (X, Z) multivariate

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

942

P.

M.

ROBINSON

normal, but normality is not necessary, except for special structures (Kagan et al.
(1973, Sec. 10.5)). An estimation strategy is suggested in relation to a tentatively
specified linear regression model

(5.5) E(YIW)=a+y'W
where y and W are r x 1. Denoting jth element by subscript j, form y such that

yj is /B with p = 1, q = r- 1; let X= Wj and Z be W with Wj deleted. Then
estimates yj robustly in the sense of being N _2-consistent even if the functional
dependence on the Wk, k #j, has been misrepresented by (5.5). Moreover, if (5.5)

is correct, y is as efficient asymptotically as the OLS estimator of y if the

regression of Wi on all Wk, k #j, is linear, for each j, for example if W is

normal.

6. SIMULATIONS

Finite-sample theory for semiparametric estimators such as /3 is not on the
horizon, even under much more precise distributional assumptions than ours;
indeed little is known about the finite-sample distribution of the nonparametric

regression estimators of which /3 is composed. To gain some idea of finite-sample
performance and the influence of such factors as dimensionality of Z and
order of kernel, a small simulation study was conducted, in double precision
FORTRAN on the University of London's Amdahl computer. Such vast variation of design is possible that the results are in no sense representative, and we
would only wish to add that ,B is invariant to location shifts in X, Y and Z, while

,B -,B (on which all the summary statistics we report depend) is invariant to ,B.
Four different models with varying q (= 1,5,10) and 0 (and satisfying the
regularity conditions of the Theorem) were selected, and three sample sizes,

N = 25, 50, and 200. Because computing time varies greatly with N and q, as
indicated above, the numbers of replications were on a sliding scale, from
100,000 when q =1 or 5 and N= 25, to a mere 1000 when q= 5 or 10 and
N = 200. We obtained a and b by inspecting the results for various values used

on training samples, the only constraint that was initially imposed being that a
and b be monotonic over N and q in a fashion that roughly reflects condition (ix)
of the Theorem. There was no serious attempt at optimal choice but we avoided
values which entailed extreme bias or variability, and used the same values for
model (4.1) and model (6.1) below. We report results only for three different
kernels, selected in order to gauge the implications of kernel order. Kernels 1-3

are in 2, 4, and Y6 respectively, and given by (3.4) with 1 = 2,4,6, respectively, and 4'(u) = (27T)-1/2exp(_- u2). Most of the calculations were also
repeated for the three corresponding kernels formed from 4 (u) = I(IuI < 1);
these are quicker to compute, but having compact support, unless N and/or a

are large enough relative to q it does happen on occasion that Xi Xi, when
estimator breaks down.
In (4.1) we took X and Z to be scalar random variables from a bivariate

normal population with zero means, variances 4 and 3, and covariance 2; U to be

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 943
TABLE I
MODEL (4.1)

N

a

b

r

/4(1)

A(2)

,t(3)

25 1.65 .01 lo, -.1213 -.0254 -.0095 BIAS
.9141 .9750 .9128 VEFFICIENCY
50 1.25 .005 5 X 104 -.0697 -.0094 -.0040 BIAS
.9020 .9626 .9376 VEFFICIENCY
200 0.75 .001 1 -.0161 -.0007 .0003 BIAS
.9607 .9722 .9696 VEFFICIENCY

standard normal; and a =,B= y = 1. Subroutine G05DDF from the NAG library
generated the observations. Let /3 be the OLS (i.e., maximum likelihood) estimator of ,B based on the true model: for (4.1) it is unbiased when N > 3. (Intercept

OLS of Y and X alone, denoted /3, is inconsistent.) While /3 is not unbiased for

finite N, it is as efficient as /3 in (4.1) (see Section 5), so these are relatively
favorable circumstances for ,B, especially as q = 1 only. The results are presented
in Table I, where r is the number of replications. In each table we report the
simulation biases of the /3 estimates, formed from kernels 1-3, and headed /3(i),

i = 1,2,3, and the ratio of /3's simulation standard deviation to ,B(i)'s, called

Vefficiency. The biases in Table I are mostly negative, and decrease a bit as kernel

order increases. The Vefficiencies are not as good as the asymptotic ones.
Table II contains corresponding results for the model

(6.1) Y=a+/3X+yZ2+ U,

under the same specification as before. Now / is no long
cally as OLS /3 based on (6.1) (its asymptotic relative efficiency is 2/3). (/3
happens to be consistent, unbiased when N > 2, and asymptotically efficient for

this model.) The biases are all positive and increase a bit as kernel order

increases. The Vefficiencies are sometimes better, sometimes worse, than
asymptotic ones, though not surprisingly uniformly worse than Table I's.
Finally we tested the method against Z's of much higher dimension, extending
(6.1) to
q

(6.2) Y=a+,X+ E jZ2) + U,
j=1

TABLE II
MODEL (6.1)

N

a

b

r

(1)

(2)

A(3)

25 1.65 .01 105 .0188 .0191 .0204 BIAS
.7862 .7761 .7271 VEFFICIENCY
50 1.25 .005 5 X 104 .0075 .0079 .0083 BIAS
.8754 .8606 .8375 ,EFFICIENCY
200 0.75 .001 .0018 -.0019 .0019 BIAS
.9356 .9299 .9201 ,EFFICIENCY

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

944

P.

M.

ROBINSON

TABLE III

MODEL (6.2), q = 5

N
25

a
3

b

r

.0001

m(1)
105

A(2)

,t(3)

.2774

.1600 .1276 BIAS
.3638 .3527 .3246 ,EFFICIENCY
50 2.4 .00005 25 X 103 .1743 .0988 .0813 BIAS
.3743 .3716 .3231 ,EFFICIENCY
200 1.5 .00001 103 .0693 .0399 .0285 BIAS
.4349 .4245 .3653 ,EFFICIENCY

TABLE IV

MODEL (6.2), q= 10

N

a

b

r

A(')

A(2)

A(3)

25 4.5 10-8 25 X 103 .6688 .3559 .2523 BIAS
.2788 .2231 .1941 ,EFFICIENCY

50 3.25 5 x 10-9 .3357 .1621 .1070 BIAS
.2181 .1972 .1726 ,EFFICIENCY

200 2.25 10-9 10 .1663 .0728 .0485 BIAS
.2081 .2039 .1785 ,EFFICIENCY

where a, ,B and the yj are all 1; U is as before; and X and the Z(j) are

equicorrelated identically distributed N(1, 3) variables, with correlation 2/3. The

asymptotic relative efficiency of /3 to /3 increases from 8/9 when q = 1, to 1 as

q -- oo. Because E(Z(j)) * 0, /3 is inconsistent. Results for cases q = 5 and 10 are
presented in Tables III and IV. The biases are uniformly positive and mostly very

bad, especially in Table IV, though bias does improve materially with increase in

N and, more interestingly, with kernel order. The role played by the higher-order
kernels in the asymptotic theory does therefore seem to have implications for
finite-sample practice. However, they do produce larger variances, as surmised in

Section 3, though even for kernel 1 the Vefficiencies are anything from half (when
q = 5) to less than a quarter (when q = 10) of that predicted by asymptotic
theory. These figures are only slightly influenced by ,'s variances being mostly a
bit lower than the asymptotic ones. Evidently the nonparametric kernel estimates
are so bad for these sample sizes and high-dimensional Z's as to seriously inflate
3's variability.
7. EXTENSIONS

We indicate some extensions of our semiparametric model and estimator that
are of possible econometric interest, without giving full details or regularity
conditions (which have not been worked out), but noting limitations as well as
positive features.
1. Seemingly unrelated regression. A system of J partly linear semiparametric

"seemingly unrelated" regressions is Y(j) = 1(j)X(j) + OJ(Z(j)) + U(j), 1 < J,
where the Oi are unknown functions and X(j), Z(j) all comprise elements of a

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 945

vector W, independent of U* = (U(1
appear in X in one subset of the equations and in Z in another, disjoint, subset.

Given N observations distributed as (W, Y(1),..., Y(J)), the efficiency of J se
arate estimators of the form (2.3) can be improved upon when S = E(U*U*') is
not diagonal, by analogy with Zellner (1963).

2. Simultaneous equations. Consider the structural equation
(7.1) Y=a'Y*+y'X*+0(z)+ U,

where Y* is not uncorrelated with U but X* and Z are independent of U (so
nonlinearities of unknown form in endogenous variables are not allowed, though
(7.1) could be completely nonparametric in exogenous variables). Replacing the
conditional expectations in the projection form of (7.1) by nonparametric "esti-

mators" gives Y- Y= a'(Y* - Y*) + y'(X* - X*) + U. A valid instrument for
- Y* is a vector function of an observable vector W that includes Z and is
independent of U, such that the covariance matrix in the limiting distribution of

our resulting N1/2-consistent estimator of a and y exists. The most efficient
-

A

instrum
which is of unknown form if the structural equations for Y, Y* and any other
endogenous variables contain nonlinearities in the endogenous and/or exogenous
variables of unknown form, or even if the form of nonlinearity is known but

information on Y*'s conditional distribution given W is insufficient to parameter-

ize E(Y* I W). (When 0(Z) is absent but Y* still has nonparametric reduced form
our estimator is similar to Newey's (1986) for nonlinear equations with known
structural form but unknown reduced form.) For a full system or a subsystem of

equations like (7.1), whose residuals are not all uncorrelated, a further improvement in efficiency is possible via an analogue of three stage least squares.

3. Nonlinear regression. Generalize (1.1) to E(YIX, Z) = g(X; yo) + 0(Z),
where g is a known function of X and the unknown s-dimensional parameter

my. We might estimate y0 by y mmizing Ej[Ej{Yi - Yj - g(Xi; y) +

g(Xj; y)}Kij] 2i/fA2 over admissible y's. The prospect of a grid search over s
dimensions to obtain a starting value for iterations is daunting, and it seems
desirable that representation (2.4) be used in both the search and iterations after

storing DD'. In the class g(X; yo) = ah(/3'X) for a an unknown scalar, we may
estimate ,B up to an unknown scale 8, say, using derivatives of nonparametric
regression as described in Section 1 or by Powell et al. (1986); then after

concentrating out ai we need only search over 8.
4. Time Series. It remains to be seen to what extent N1/2-consistency holds
when the data are serially dependent but stationary, not only for /3 but for
analogues of parametric methods for improving efficiency in the presence of
serially dependent residuals. One time series model of interest is the partly
rational distributed lag
p

p

(7.2)
j=1

j=1

where

Zi

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

is

i

946

P.

of

a

M.

ROBINSON

single

variable

Zli,

a

distributed lag in Z11 in a uniform frequency-domain sense, but no such strong

result justifies approximating (7.2) by a linear form. When Ui is serially independent the asymptotic covariance matrix of /3 can be derived from (3.5), where /3 is
automatically identified. A sufficient condition for / to be as efficient as OLS

when 0 is actually linear is that Zi is stationary Gaussian (see Section 5). Whe
Ui is serially dependent, /3 is inconsistent, but a natural extension of Liviata
(1963) instrumental variables estimator is possible. Other time series models that

might be treated are partly linear stationary autoregressions, such as Yi 3Yi -1

0(Yi-2) + Ui-

5. Heteroskedasticity. Assumption (iii) of the Theorem, that U is independent

of the explanatory variables, is familiar, but too strong for many econometric
applications, and in fact it can be relaxed to a milder assumption on conditional
moments, at the cost of some strengthening of other conditions. Under condi-

tional heteroskedasticity (V(UI X, Z) = a 2(X, Z), say) ,B will still be N1/2-co
sistent under appropriate conditions. A parametric form for a 2(X, Z) seems
implausible since the conditional mean is semiparametric, but following Eicker

(1963), a consistent estimator of ' = E[{ X - {(Z)} { X- (Z)}'a2(X, Z)] in ,'s

limiting covariance matrix 0-'Z-' should be - = N-1Yi(Xi -

Xi)'ui2Ii, in the presence of heteroskedasticity of unknown form. A hete
tic (1.1) arises naturally from the semiparametric sample selectivity model

(7.3) Y(1)-=3X+/('L)Z(i) +LU(1), Y(2) = 02(Z(1), Z(2)) +U(2)

where we observe Y(1) when and only when Y(2) > 0, so the second (decisio
equation in (7.3) imparts sample selectivity when U(1) and U(2) are not indep
dent, and where U(1) and U(2) are in any case independent of the disjoint vec

of explanatory variables X, ZM and Z(2)- In the Tobit and some other mod

all explanatory variables in the first (outcome) equation, are present also in the
decision equation, in which case ,B'X is absent and our approach is inapplicable.

On the other hand, we do not assume a parametric conditional distribution of

U(1) given U(2), and allow the decision equation to be nonparametric, in wh
sense (7.3) is more general than J. Heckman's (1976) model. (Some further

generalization of (7.3) is possible.) With Y = Y(l) 1 Y(2) > 0, Z = (Z(1), Z(2)

@ (Z) = P('l)Z(l) + E(U(l) I U(2) ->- 2 (Z)),
we obtain (1.1), and also

V(YIX, Z) = V(U(l)I U(2) > -02(Z)) = (Z),

so we can use P as before, and (5.4) as a test for absence of sample selectivity, bu
we must allow for heteroskedasticity of unknown form in estimating /3's covar
ance matrix if the test rejects. J. Heckman's (1976) estimator is also based on Y's
regression function, but a parametric version. For other work on semiparametric
inference in limited dependent variable models, see e.g. Manski (1975), Cosslett
(1983, 1984), Powell (1984), Chamberlain (1986). Irrespective of (1.1)'s origin, we
may improve upon P3's efficiency in the presence of residual heteroskedasticity of

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 947

unknown form, by GLS-type estimators e
a2(X, Y), c.f. Carroll (1982), Robinson (1985).
6. Multiplicative and other models. An alternative, multiplicative rather than
additive, semiparametric regression function appears in the model Y=

g(X; yo)O(Z) + U, say a semiparametric Cobb-Douglas model with additive
residuals. Then

Y/E(YiZ) =g(X; yo)/E(g(X; yo)IZ) + U.
Nonparametric "estimates" of the two denominators can be inserted, then y0

estimated by NLLS. One can conceive of more general structures which permit
an unknown function of Z to be identified in terms of conditional expectations
of various functionals of Y and X.

Department of Economics, London School of Economics, Houghton St., London
WC2A 2AE, England
Manuscript received May, 1986; final revision received October, 1987.

APPENDIX A: PROOF OF THEoREM

Necessity of (3.5) is obvious. Rewrite fi and a2 using (3.1),

1 -fi=Si1*(Sx_*, _6 + Sx-kua,u-

62 _2 = (SU 1-02) + S0o+ (fi-)'Sx-(-) + 2Se-,U-2(f -f)'Sx_k,u_&- 2( - )'Sx-* ,
where

Sx-f= Sv- Sv- Sv+ Sf + Sv,-i + S j, v- Stu,+ +St_,
Sx-k,e- = sv,-6-s - + St0, S ,u = SX- , u- d = Svu-S u-SVSo + S4 + St-I, U-St-I, d,
Su- d = Su- Su-Su + SC.

The proof is completed by applying Propositions 1-15 established below, which imply via the Cauch

inequality that Svv, S Sv,, SU SG,U and S0,6, all P 0. The propositions apply t

lemmas of Appendix B. We use the abbreviations Ei(.) = E(- I Z.), i = min(X + 1, ),D = min(X

1, v); C denotes a generic constant.

PROPOSITION 1: E(S0_g) = O(N-la-qb-2 + a2Db-2).

PROOF: By identity of distribution, E(S0,_g) = E((01 1} < N-2a-2qb-2E(T2), where T=
Sit,, t, = (01 - O,)Kli, where E(T2) < 2E(Y(tj - t)}2 + 2N2E(t2), where t = El(ti). Conditional on
Z1, the t, - t are independent with mean 0, so E(X(t, - t)}2 =XE(tj - t)2 < NE(t2) = O(Naq) by
k's boundedness and Lemma 3. By Lemma 5, E(t2) = 0(a2(q+
PROPOSITION 2: EISTI = O(N- la b 2+ a2b 2).

PROOF: Use Proposition l's proof and Cauchy inequality.

PROPOSITION 3: N1_2Sgj S C6 O (N- 1/2a- -b2 + N' a + b
PROOF: By Cauchy inequality and Propositions 1 and 2.

PROPOSITION 4: SV = ,0 + O (N- 1/2a-q/2b- 1 + aAb- 1) + o (p)

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

948

P.

M.

ROBINSON

PROOF:
Op

(N-

El

Because

1/2)

N-

by

the

V,

are

Chebyshev

Y-VjVj'

(1

-

Ii

inde

inequali

)

<

{

El

With fi =f(Z1),

P(f < b) < P(if -fA I > b) + P(fi < 2b).
By Chebyshev inequality

P(ih | > b) < 2{ E( f )2 + E(f _fi)2} /b2
wherefi = E1(f1) = (Nag) {K(O) + (N- 1) E1(K12)}.
Thus

E( f-fi)2 6 2E{ a-qEl (K12) f }2 + 2(Naq) 2E{fi + K(0)
= O(a2A + (Naq)2),
by Lemma 4. Because fi-fi = (Naq)-l{Kli -El(Kli)}, whose summands are, conditional on Z1,
independent with zero mean,

E(fi -fi) (Naq) )2E(Kj2j) O(N
then Lemma 6 implies P(A < b) -- 0.

PROPOSITION 5: SV= Op(N 1a"qb 2).

PROOF: Because E(V VIfN) = 0, a.s, where ff (Z1. ZN), E Sl < E(l V1VI) ,

(Naqb) 2YE(I JKl2Kl), where the sum is

(0)El V12+ (N-1)VE(I 212K122 ) CEIXI2 + NE{I V2 K1)}
C(l + Naq )El X12,
by Lemma 2.

PROPOSITION 6: N112Sv,eg = Op(N-l/2a-q/2b-1 + ab-1).
PROOF:

EIN' _D 1l2=N-'IE{l 'I2(0i_Si)2I,} < [El V14 {( 01)4Il}
< (Naqb) -2 { ElXXl4E(T4)}1/2.
Now E(T4) < C[E(Y2(ti - t)}4 + N4E(t4)] by Minkowski inequality, and

Ety(ti t))4< E t,4) + , E(it)2(t _ t)2}
i$j

< NE(t4) + 8N2 [ E(t2t32) + { E(t4) E(t4)}1/2 + E(t4)].
By Schwarz inequality E(t22t32) E((01 E-2)4Kf12KA3 }=E((01 02 )4K 3E1(Kl)) =O(a2q), using

Lemmas 2 and 3, and since E(t4) = O(a4(q+t)) by Lemma 5,
(A1) E(T4) = O(N2a2q + N4a4(q+t)

PROPOSITION 7: N112Sf 0-D = O,(N-l/2a-q/2b-2 + ab-2).
PROOF:

(A.2) E+Nl/2s _12 6 E tN I V( - bi)2,i

(A.3) +|E(N1 V" tj(i i(j i)i
This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 949

Because E(I V1 12IN) (Naqb)2E(, I I12K12ilfN), a.s

(Naqb)-4 times

E(I KI2K?2VT2) 6 CE(I V1 1T2 + NI V2I2 2Kt 2 + NI V212K12Tl), where T1 = T-2
By (A.1), E(I V1 12T2) = O(Naq + N2a2( +t)). Applying Lemmas 2 and 3 and (A.1),

E(I V2I2t2Ki2) 6 [E{l V24E2(K42)} E(t4)] (aq),

(A.4) E(I V2I2Kl2T2) 6 [E{I V2I4E2( K12 )} E{ T14E1(K2 ) }1/2 O(Na2q + N2a3q+2 ).
Thus (A.2)'s right hand side equals O(N-2a-2qb-4 + N-la2t-q). Next

(A.5)

so

E(VEV2IlI2N)

(A.3)

is

CN-3a

where

=

bounded
4b4E

T2

=

{(I

by

N3a

V112

T1-t3.

N2a3q+2t) for i= 1, 2. Applying Lemmas 2 and 3

N

As

+

4b4E(Y2

I

in

V212)(

(A.4)

an

E(I I312|KU3K23 | 32 E { V 4E(K13 ) E3 I K23 1 }E { t34E3 I K23 1 } 0=O( a )
and afortiori, E(I V312IKI3K23It2) = O(aq). Applying Lemma 2 and (A.1),

E(I V3IIKi3K23IT22) [E{IV314EI K43IE3IK23}E {TE1 (I1K3IE3IK23D)]}I]/
which is O(Na3q + N2a4q+2t). Thus (A.3) = O(N-la-qb-4 + a2b -4).

PROPOSITION 8: N1/2SU, {_ = Op(N- /2a- q2b-1 + a'b-1).

PROOF: By independence of }, ( Zi }, EI N1/2Su_ 12 = a 2E{ tr(S_j)}. Apply Propos

PROPOSITION 9: N1l2S0,j= Op (N- l/ka /2b-2 + a-2b)
PROOF:

(A.6) E IN 1 2SCI,_ < E(U1 1141 1) + 21NEf 'lU2(tl41 (4-2 II
Put wi = (t - t,)Kli, W=2w1, W1 = W- w2, W2 = W- w3. The first term on (A.6)'s right-hand

side has bound C(Na b)-4 times

E( E K2i wI W12) C[El W12 + WNEw22 +NE{I W1 1 2E ( K122)}
= O(N2a2q + N3a3q+2n)
using Lemmas 2 and 3 and Proposition l's proof. The second term of (A.6)'s right-hand side is
likewise bounded by C(Naqb)-4 times

NE( I KliK2i I lWI2)

< CN [E(I w2l 1 1l 2EJ IK12 I
+N{W212E1I13
I W312E3IK3IW+
+NE-I(1
+ K131 I +K231
NW212E(IK13 IE3IK23 I})
=O(N 3a 3q + N4a4q+27).

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

950

P.

M.

ROBINSON

PROPOSITION

PROOF:

By

10:

Nll2su(

=

independence

Op

of

{

as in Proposition 5's proof.

PROPOSITION 11: N1l2S IV= Op (N 1/2a -/2b-1).
PROOF: Conditioning first on (Ui, Vi 1, then only on { Vi

EIN112SoIv < E(I Vi 1201211) < C(Naqb) E(l V1 ( 2I Kl) = O(N la-b2).
PROPOSITION 12: Nl/2S = Op (N l/2a /2b2).

PROOF: E I N1 2}12 < E(12I Vl 12) + 2NI E(UiU2V' V21I2) 1. The first term on the right ha

side has bound C(Naqb)-4 times

E(Kl2iE~j1 j2K12j)

C [ EI V1i12 + NE{ V1 1 2E( K122)} +N2E {V312E1( K122) E( K123)}I
= O(N2a2q)

by Lemma 2. After taking expectations over { Ui } and applying (A.5),
| E(U2UVl'V21112) 1

= a2(Naq)4 |E{(jK K2j)(i ' I2KijK2j)?rI21IiI2 } |
< 0J2(Naqb) -4E(F,I KiK2il I F,IVj2IKjjK2jI)
< C(Naqb) E{ (I K121 + NJ K13K23 1)

x(i V1 12IK121 + I V3121K13K231 + NI V4121K14K24I)}

of which the dominant term has bound C(Na2qb2)-2E{( V4lE4(1K14K24

O(N-2a-qb-4).

PROPOSITION 13: Sd = Op(Nla-qb-2).

PROOF: E(S ) =u2(Naqb)-2E(?Kl2iIl)= O(N-'a-qb-2).
PROPOSITION 14: SU = a 2 + & (1).
PROOF: By Khinchine law of large numbers N-1Ui2 P a2, whereas EIN 2-b2(1-Ii) =
uJ2P(f1 < b) -O 0 by Proposition 4's proof.
PROPOSITION 15: N1 2Suv N(O, 2 l)

PROOF: By Levy central limit theorem N-12 biVK d N(O, a2),whereas

ElN-/2 biV( - )| < a2{ EI X14P(p < b) } O
as before.

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 951
APPENDIX B: TECHNICAL LEMMAS
Lemmas 1-3 below are unoriginal, merely versions of results used time after time in the immense
kernel estimation literature, but they are presented for ease of reference, while their short proofs will
aid the reader unfamiliar with kernel manipulations. Although Lemmas 4 and 5's proofs use
techniques familiar in the kernel literature, previous results on effects of higher-order kernels of which
we are aware concern bias of estimation at a fixed, rather than random, point, and we were unable to
find the results we need. It is inconceivable that Lemma 6 is new, but we failed to locate a reference.

LEMMA 1: Let supu I k(u) I + J I uXk(u) I du < oo, for some X > O. Then uniformly in z

(B.1) fJIY-Z z1X1' K((y -z)/a) I dy = 0(aq+X).
PROOF: The left-hand side is

aq+XflyIxjK(y)j dyV aq+xqxf1uxk(u)1 du(flk(u)l du)
LEMMA 2: Let supjt(z)< oo, supuIk(u) I + fI k(u)I du< oo. Then uniformly in z

EIK((Z -z)/a) I = 0(aq).
PROOF: The left-hand side < sup_ f(z)f I K((y - z)/a) I dy; then apply Lemma 1.

LEMMA 3: Let sup_f(z)< oo, EIg(Z)I < oo, supuIk(u)I + JIk(u)I du< oo. Then

Elg(Z1)K12 1 = O(aq).
PROOF: The left-hand side<EI g(Z1 )E1IK121 I < CaqE Ig(Z) , by Lemma 2.
LEMMA 4: For X satisfying I- 1 < X < 1, where 1 1 is an integer, let f E !?0, k E Xl. Then

(B.2) E(a-qEl(K12) -f(Zl)}2 = O(a2X).
PROOF: Let R(y, z) -f(z) be defined relative to f in the way Q(y, z) was in relation to g in
Definition 2, so it is a homogeneous (I- 1)-degree polynomial in y - z with coefficients that are
bounded functions of z, the remainder term in the Taylor expansion being in Lip (X - m + 1). By
(3.2),

J{ R(y, z) -f (z)}K((y-z)/a) dy-O,
so E{K((Z- z)/a)} - aqf(z) is bounded by

J f (y -R (y z) K( a)dy + {R(y, z) -f (z)}K( ) dy

? fY {f(y)-f(z)}K( Ya)dYy
which is bounded by CL(X), where L(X) is the left-hand side of (B.1), because IY-zl >p for

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

952

P.

M.

ROBINSON

y eYz and X>1- 1. Now k(u) = O((1 + Iu
1, not only E{a- K((Z - z)/a) - f(z)} = O(a

gence.

LEMMA

a>1,

5:

letfe

E

E1[

For

X,

iw,

{

,u

satisfying

geG,a,

g(Z1)

ke

I-1

<

X

XI+m-.

-g(Z2)}Kl2]

r

=

<

1,

Th

0(

PROOF: By (3.2), JQ(y, z)R(y, z)K((y-z)/a) dy=-0, so IE[ g(Z)-g(z)}K((Z - z)/a)] is

bounded by

f {g(y) -g(z) - Q(y, z)}f(y)K( a) dy

+ f Q(y,z){f(y)-R(y,z)}K( Yz) dy
zpa

* Q5 {(y, ) R (y,z() K ( dy|

+ f {g(y) -g(z)If (y) K(Y )dy
m-1

* Ch (z)L (,u) + G (z) L L(i + A) + H(z)L (A + IA)
i=l

+ C{g (z) I + El g(Z) I}) aq+71 sup (I u Iq+,,Ik(u)j q}
u

where E(G(Z)G + H(Z)G} < oo. Then again apply Lemma 1 and domin
that min(I, X + 1, X + A) = < min(l+ 1, m) < 1+ m - 1 < q(l+ mr-1
LEMMA 6: himb ,OP(f(Z)<b)=O.
PROOF:

P(f (Z) < b) < bf
dz + P(IZI > B) < (2B)qb + P(I ZI > B)
Izl <B
for all B > 0. For any E > 0, choose B so P(IZI > B) < E; then b < (2B)-qE.

REFERENCES

AMEMIYA, T. (1980): "Selection of Regressors," International Economic Review, 21,
BARTLErT, M. S. (1963): "Statistical Estimation of Density Functions," Sankhya, Ser.
BEGUN, J., W. J. HALL, W. HUANG, AND J. A. WELLNER (1983): "Information an
Efficiency in Parametric-Nonparametric Models," Annals of Statistics, 11, 432-452.
BERAN, R. (1977): "Adaptive Estimates for Autoregressive Processes," Annals of th
Statistical Mathematics, 28, 77-89.

BICKEL, P. (1982): "On Adaptive Estimation," Annals of Statistics, 10, 647-671.
CACOULLOS, T. (1966): "Estimation of a Multivariate Density," Annals of the Institute of Statistical
Mathematics, 18, 179-189.

CARROLL, R. J. (1982): "Adapting for Heteroscedasticity in Linear Models," Annals of Statistics, 10,
1224-1233.

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

SEMIPARAMETRIC REGRESSION 953
CHAMBERLAIN, G. (1986): "Asymptotic Efficiency in Semiparametric Models with Censoring,"
Journal of Econometrics, 32, 189-218.

COLLOMB, G. C. (1985): "Nonparametric Regression: An Up-to-Date Bibliography," Statistics, 2,
309-324.

COSSLETT, S. J. (1983): "Distribution-free Maximum Likelihood Estimator of the Binary Choice
Model," Econometrica, 51, 765-782.

(1984): "Distribution-Free Estimator of a Regression Model with Sample Selectivity,"
manuscript, University of Florida.

Cox, D. D. (1985): "A Penalty Method for Nonparametric Estimation of the Logarithmic Derivative
of a Density Function," Annals of the Institute of Statistical Mathematics, 37, 271-288.
EICKER, F. (1963): "Asymptotic Normality and Consistency of the Least Squares Estimator for
Families for Linear Regressions," Annals of Mathematical Statistics, 34, 447-456.
ELBADAWI, I., A. R. GALLANT, AND G. SouzA (1983): "An Elasticity Can Be Estimated Consistently
Without A Priori Knowledge of its Functional Form," Econometrica, 51, 1731-1751.
ENGLE, R. F., C. W. J. GRANGER, J. RiCE, AND A. WEISS (1986): "Semiparametric Estimates of the
Relation Between Weather and Electricity Demand," Journal of the American Statistical Association, 81, 310-320.
FRIEDMAN, J., AND W. STUETZLE (1981): "Projection Pursuit Regression," Journal of the American
Statistical Association, 76, 817-823.
GALLANT, A. R. (1985): "Identification and Consistency in Seminonparametric Regression," paper
presented at the World Congress of the Econometric Society.
HAUSMAN, J. A. (1978): "Specification Tests in Econometrics," Econometrica, 46, 1251-1271.
HECKMAN, J. J. (1976): "The Common Structure of Statistical Models of Truncation, Sample
Selection and Limited Dependent Variables and a Simple Estimator for Such Models," Annals of
Economic and Social Measurement, 5, 475-492.
HECKMAN, N. E. (1986): "Spline Smoothing in a Partly Linear Model," Journal of the Royal Statistical
Society, Series B, 48, 244-248.
KAGAN, A. M., Y. V. LINNIK, AND C. R. RAo (1973): Characterization Problems in Mathematical
Statistics. New York: Wiley.
LIVIATAN, N. (1963): "Consistent Estimation of Distributed Lags," International Economic Review, 4,
44-52.

MANSKI, C. F. (1975): "Maximum Score Estimation of the Stochastic Utility Model of Choice,"
Journal of Econometrics, 3, 205-228.
(1984): "Adaptive Estimation of Non-Linear Regression Models," (with comment), Econometric Reviews, 3, 145-194.
NEWEY, W. K. (1986): "Efficient Estimation of Models with Conditional Moment Restrictions,"
manuscript, Princeton University.

POWELL, J. L. (1984): "Least Absolute Deviations Estimation for the Censored Regression Model,"
Journal of Econometrics, 25, 303-325.

POWELL, J. L., J. H. STOCK, AND T. M. STOKER (1986): "Semiparametric Estimation of Weighted
Average Derivatives," manuscript, Massachusetts Institute of Technology.
PRAKASA RAO, B. L. S. (1983): Nonparametric Functional Estimation. New York: Academic Press.
RICE, J. (1986): "Convergence Rates for Partially Splined Models," Statistics and Probability Letters,
4, 203-208.

ROBINSON, P. M. (1983): "Nonparametric Estimators for Time Series," Journal of Time Series
Analysis, 4, 185-207.

(1987): "Asymptotically Efficient Estimation in the Presence of Heteroskedasticity of Unknown Form," Econometrica, 55, 875-891.

SCHICK, A. (1986): "On Asymptotically Efficient Estimation in Semiparametric Models," Annals of
Statistics, 14, 1139-1151.
SCHUCANY, W. R., AND J. P. SOMMERS (1977): "Improvement of Kernel Type Density Estimators,"
Journal of the American Statistical Association, 72, 420-423.
SCHUSTER, E., AND S. YAKOWITZ (1979): "Contributions to the Theory of Non-parametric Regression, with Application to System Identification," Annals of Statistics, 7, 139-149.
SCHILLER, R. J. (1984): "Smoothness Priors and Nonlinear Regression," Journal of the American
Statistical Association, 72, 420-423.
STOCK, J. H. (1985): "Nonparametric Policy Analysis; An Application to Estimating Hazardous
Waste Cleanup Benefits," manuscript.
STOKER, T. M. (1986): "Consistent Estimation of Scaled Coefficients," Econometrica, 54, 1461-1481.

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

954

P.

M.

ROBINSON

STONE, C. J. (1981): "Admissible Selection of an Accurate and Parsimonious Normal Linear
Regression Model," Annals of Statistics, 9, 475-485.

(1982): "Optimal Global Rates of Convergence for Nonparametric Regression," Annals of
Statistics, 10, 1040-1053.

(1985): "Additive Regression and Other Nonparametric Models," Annals of Statistics, 13,
689-705.

WAHBA, G., (1984): "Partial Spline Models for the Semi-Parametric Estimation of Functions of

Several Variables," in Statistical Analysis of Time Series. Tokyo: Institute of Statistical Mathematics, 319-329.
(1985): "Discussion to 'Projection Pursuit', by P. J. Huber," Annals of Statistics, 13, 518-521.
WHITE, H. (1980): "Using Least Squares to Approximate Unknown Regression Functions," International Economic Review, 21, 149-170.
(1982): "Maximum Likelihood Estimation of Misspecified Models," Econometrica, 50, 1-25.
ZELLNER, A. (1962): "An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests
for Aggregation Bias," Journal of the American Statistical Association, 57, 348-368.

(1970): "Estimation of Regression Relationships Containing Unobservable Variables," International Economic Review, 11, 441-454.

This content downloaded from 213.55.211.96 on Wed, 03 Oct 2018 07:42:19 UTC
All use subject to https://about.jstor.org/terms

