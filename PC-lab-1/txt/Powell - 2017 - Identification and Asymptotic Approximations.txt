Journal of Economic Perspectives—Volume 31, Number 2—Spring 2017—Pages 107–124

Identification and Asymptotic
Approximations: Three Examples of
Progress in Econometric Theory
James L. Powell

N

ot long after I agreed to write an article for this issue about recent accomplishments in econometric theory, I said to myself, “Well, this will probably
end badly.” Like any academic endeavor, research in econometrics is more
about “progress” and ”themes” than “accomplishments,” and it is difficult for any
participant to give a balanced view of that progress without shortchanging the
contributions outside his or her particular area of expertise. In addition, the last few
decades have seen a growing emphasis on applied/empirical research in economics
relative to theoretical/methodological research, and so it is hard to avoid appearing
defensive. I recognize that a large segment of the discipline regards research contributions of theoretical econometrics as being increasingly divorced from practice.
Although the ultimate aim of econometric theory is empirical application, the
increasing mathematical background needed to make progress in these research
areas can make the motivation and results inaccessible to those outside the field.
But I think it has always been so, from the days of the Cowles Commission to the
present, and I am far from convinced that the gap is widening. It can be hard to
predict which strands of the theoretical literature in econometrics will ultimately
be useful in practice, but the same can be said of most scholarship, and theoretical
econometrics has been making progress in understanding what can be learned
about models from the kind of data we observe in economics. Explaining why all
economists—even practical econometricians—should care about seemingly arcane
■

James L. Powell is the George Break and Helen Schnacke Break Distinguished Professor of
Economics, University of California, Berkeley, California. His email address is powell@econ.
berkeley.edu.
†
For supplementary materials such as appendices, datasets, and author disclosure statements, see the
article page at
https://doi.org/10.1257/jep.31.2.107
doi=10.1257/jep.31.2.107

108

Journal of Economic Perspectives

concerns is a worthy intellectual enterprise, even if the outcome is destined to be
slanted, incomplete, and obscure.1
Instead of presenting a comprehensive survey of the state of the art in econometric theory, I will focus on some interrelated research areas that have received
an increasing emphasis in recent years, both in theoretical econometrics and in the
profession as a whole. These issues arise from some substantial changes in empirical
economics: the size and quality of datasets and computational power has grown
substantially, along with the size and complexity of the econometric models and the
population parameters of interest. With more and better data, it is natural to expect
to be able to answer more subtle questions about population relationships, and to
pay more attention to the consequences of misspecification of the model for the
empirical conclusions.
Much of the recent work in econometrics has emphasized two themes. The
first is the fragility of statistical identification, that is, the issues that arise in seeking
to characterize the possible values of model parameters when the true distribution
of the observable random variables is assumed to be known. As Khan and Tamer
(2010) demonstrate, “irregular identification” has strong consequences for the
attainable precision of estimators and the size and power of tests of hypotheses
about the key parameters in a model. When exact identification of these parameters
fails, it may still be possible to make inferences about their sign or likely range of
values, as is the goal of the recent contributions on partial identification.
The other, related theme in econometric theory involves the way economists
make large-sample approximations to the distributions of estimators and test statistics. In standard asymptotic theory, approximate distributions of these statistics
are derived assuming the model and parameters are held fixed as the sample size
increases. However, it is often more realistic to use “nonstandard” or “alternative”
asymptotics, which assume the model and parameters are allowed to vary with the
number of observations. These issues have gained prominence as structural models
have become more flexible in functional form and identification of the parameters
of interest has become more delicate. In empirical practice, the number of parameters is no more “tending to infinity” than the sample size N, but it seems reasonable
to expect that linking the two will give better large-sample approximations than
taking limits with the number of parameters fixed while the sample size increases.
Similarly, making approximations to sampling distributions where the parameters
are assumed to be “moving” with the sample size toward pathological values can
yield an approximate distribution theory that is less discontinuous between the
regular and pathological cases.
Alternative asymptotics is also at the core of the derivation of sampling distributions of statistics for models with nonparametric components, where flexible
1

When I was in graduate school, econometricians were compared to kickers on an (American) football
team—you wanted good ones, but not too many, and they had trouble communicating with the rest of
the team. An old departmental skit-party some years ago asserted that if I ever wrote a textbook, it would
be titled Econometrics, Without Applications.

James L. Powell

109

parametric forms or other “smoothing” devices are used to make approximations
to general unknown functions in the model, and the number of parameters (say,
K ) in the approximating model is adjusted to achieve an optimal tradeoff between
bias (which decreases with K ) and variance (which increases in K ). Thinking about
this bias–variance tradeoff changes the way efficiency of estimation is characterized.
In standard asymptotics, estimators are approximately unbiased with approximate
variances that decline in inverse proportion to the sample size, and efficiency
comparisons involve comparison of approximate variances. However, for estimators
of nonparametric components, the relationship of precision to the sample size can
vary across procedures, and efficiency comparisons involve relative convergence
rates of (approximate) mean-squared error to zero, rather than the relative levels of
variances. Models with many nonparametric components often have a complicated
interplay of convergence rates for each, and keeping track of exponents can be a
source of frustration to both casual readers and econometricians. The theoretical
challenge is to find a combination of convergence rates that ensures consistency
and a feasible distributional approximation for estimators of the parameters of the
model that we care about.
In what follows, I will discuss how these issues of identification and alternative
asymptotic approximations have been studied in three research areas: analysis of
linear endogenous regressor models with many and/or weak instruments; nonparametric models with endogenous regressors; and estimation of partially identified
parameters. The first illustrates the use of “alternative asymptotics” to get better
approximations to the distributions of estimators, while the other two are different
approaches to identification when strong functional form assumptions are relaxed.
Of course, there is more to recent econometric theory than the work in these three
areas, and I make no claim that the specific articles I’ve picked out for discussion
are the most representative or most important contributions to econometric theory
in recent years. However, I do think these research areas offer good examples of the
progress that has been made in the field. To quote the renowned econometrician
Arthur Goldberger, “econometrics is what econometricians do,” and these are some
things that econometricians have been doing.
Though my own research has touched on only one of these areas (nonparametric endogeneity models), I think that all three have been very active research
areas and all are natural successors to prominent themes of earlier theoretical
work in the 1980s and 1990s. In my view, the hot topics in theoretical econometrics back then were unit roots and cointegration, semiparametric estimation, and
model misspecification. The first of these involved the pathological behavior of
standard estimation procedures for time series data that were nonstationary or
nearly so, and the work on “local to unity” asymptotic theory is an intellectual
ancestor to the alternative asymptotic theory for models with weak instruments.
Like the unit root literature, work on weak- and many-instrument problems was
inspired by simulation and empirical evidence that the usual approaches to estimation and inference could be poorly behaved when the standard assumptions
didn’t fit well.

110

Journal of Economic Perspectives

Similarly, the earlier research on semiparametric models and model misspecification addressed the concern that empirical results for nonlinear econometric
models were sensitive to the strong parametric restrictions imposed to construct
likelihood-based estimators and test statistics. Like the literature on semiparametric
models, the work on nonparametric endogeneity models generally follows a “top
down” approach that starts from a restrictive model and investigates the possibilities
for relaxation of parametric assumptions (like a linear or other parametric form for
a regression function). In contrast, the research on partial identification is more
“bottom up,” starting with minimal assumptions and investigating the consequences
of adding identifying restrictions.
Again, the three examples considered in this article are far from exhaustive
of the range of methodological problems studied by econometricians. Many other
research areas in econometric theory are discussed in the other articles in this
symposium, which consider issues of causality in applied econometrics, time series
econometrics, structural microeconomics, machine learning, and econometric
pedagogy; these review a number of substantial theoretical contributions to econometrics on topics that are not considered here.

Many Instruments and Weak Instruments
A canonical problem in econometrics is the identification and estimation of
the direct effect of an endogenous regressor on an outcome variable in a linear
model. For this purpose, instrumental variables estimation has become a workhorse
of empirical economics. To build some intuition about the issues here, consider a
simple “limited information” model with two equations. The first is an “outcome”
equation for a dependent variable y in terms of a linear combination of a single
endogenous regressor x and a (correlated) mean-zero error term u,
y = γ + βx + u.
The second equation is the “first stage” for the regressor x in terms of a
­K-dimensional vector z of instrumental variables that is independent of u and the
first-stage error v,
x = δ + zπ + v.
A good instrumental variable (or vector of instrumental variables) will be correlated
with the regressor x, but not correlated with the outcome variable y.
As is familiar from introductory econometrics classes, simply regressing y on
x in the first equation will not provide a consistent estimate of the β parameter.
Instead, a bias will arise as long as the covariance between the error terms u and
v is nonzero, which will typically be true when x is truly endogenous. Under standard asymptotic theory, consistent estimation of β can be obtained by two-stage least

Identification and Asymptotic Approximations: Three Examples of Progress

111

squares, first regressing the x variables on the instruments z and then regressing y
on the fitted value ​​xˆ​​ estimated from this first-stage regression. When the number of
instrumental variables in z is small and they explain a lot of the variation in x, the
estimate of the β parameter will be consistent and approximately normally distributed (with the usual fine print, “under suitable regularity conditions”).
However, problems arise when there are many instruments and/or if the
instruments are weakly correlated with the regressors. Standard asymptotics treats
the number of instruments K as fixed, which can be a poor approximation with
many instruments. A large number of instrumental variables raises a risk of “overfitting”—essentially, that the correlation arising from a large number of instrumental
variables may mechanically generate a high R  2, but standard asymptotic theory for
instrumental variables estimation can be a poor approximation to its finite-sample
behavior when the number of instruments is large relative to the sample size.
To get better large-sample approximations to the behavior of instrumental variable estimators with many or weak instruments, Bekker (1994) used nonstandard
asymptotic approximations for the model under which the “first-stage” relation
between the regressors and instruments was assumed to depend on the sample size.
To model the first-stage overfitting phenomenon when the number of instruments
K is large, Bekker assumed that the number of instruments K was proportional to
the number of observations N, that is, K/N = α for some α between zero and one.
Under this assumption, two-stage least squares for this simple regression model is
also inconsistent. The key relationship can be expressed in this way in the form of
a probability limit:
ˆ
​ (​xˆ​, y)
Cov​
α Cov(u, v)
	​​​βˆ ​​2SLS​​​  = ​​ _______
 ​​  → β + ​​ ______________
  
   ​​  ,
Var(z π) + α Var(v)
Var​(​xˆ​)
​ˆ

In this formulation, if K is small compared to the number of observations, then α
is (nearly) zero and the two-stage least squares estimator is a (nearly) consistent
estimate of β. However, if K is large compared to the number of observations, then
the two-stage least squares estimator ​​​βˆ ​​2SLS​​​ will be inconsistent as long as the error
terms u and v have nonzero covariance.
Bekker (1994) shows that another standard estimator of β, the limited information maximum likelihood (LIML) estimator, is consistent even if α is not zero, but
even for LIML the usual form for the asymptotic covariance matrix of the estimator
changes when there are many instruments, so the standard errors for ​​​βˆ ​​LIML​​​ must
account for the estimation error in the first stage.
In addition to the pure overfitting issues with many instruments, inference
when the instruments are weak—meaning, in the one-regressor example, that the
correlation between the instrumental variables z and the regressors x is not very
strong—is even more challenging, as the regression coefficient β may not be consistently estimable. A well-discussed empirical example is Angrist and Krueger’s (1991)
study of the earnings–education relationship. In part of their study, they used date
of birth and state of residence as instrumental variables, based on the insight that
the required years of school attendance and associated laws varied across states and

112

Journal of Economic Perspectives

that students could end up with one more or one less year of schooling depending
on when their birthdate fell in the calendar year. But in their reanalysis of these
data, Bound, Jaeger, and Baker (1995) showed, using simulation results, that the
standard asymptotic theory for instrumental variables estimation can be a poor
approximation to its actual finite-sample behavior when instruments are weak, as
they are for the birthdate and state dummies.
Staiger and Stock (1997) analyzed nearly unidentified models, in which the
instrumental variables are extremely weak, and also found two-stage least squares to
be inconsistent with a nonstandard limiting distribution. The instrument z is “weak”
when the first-stage slope coefficients π are close to zero, so the denominator of the
two-stage least squares estimator (the variance of the fitted value ​​xˆ​​) is also close to
zero. In a seminal application of the “alternative asymptotics” approach, Staiger and
Stock assume the first-stage slope coefficients “shrink to zero” as the sample size
increases, and they show that the estimated ​​​βˆ ​​2SLS​​​ no longer tends in probability to the
true constant value β. Instead, it is variable even as the sample size N tends to infinity,
behaving more like a ratio of two jointly normal random variables which have a covariance matrix equal to the products zu and zv of the instruments and error terms.2
As a check for problems of either weak or many instrumental variables, Staiger
and Stock (1997) suggested what has become a popular “rule of thumb”: that the usual
F-statistic from the first-stage regression of x on the instruments z should exceed
10, rather than a more usual critical level from a table of the F­   -  distribution. The
F   -   statistic is approximately (N – K   )R   2/K(1 – R   2) = (1 – α)R   2/α(1 – R   2). It increases as
the ­first-stage R   2 increases and as the ratio α of number of instruments to observations
declines. Staiger and Stock show that as one uses a higher F   -   statistic, the approximate
bias of two-stage least squares relative to classical least squares diminishes.
Unlike the case with many (but not weak) instruments, where the limited
information maximum likelihood (LIML) estimator is consistent and its standard
errors can be corrected to obtain valid confidence intervals, in a weak instrument
setting there is no way to “fix the standard errors” or to use the usual normal
approximations to make inferences about the regression coefficients β using the
instrumental variable, two-stage least squares, or LIML estimators.3 The challenge
in the weak instrument setting has been to construct confidence regions for β with
the right coverage probability regardless of the strength of the instruments, while
also making the confidence regions not too conservative, meaning they are not
2
In earlier work, Phillips (1989) and Choi and Phillips (1992) showed, in the completely unidentified
case (π = 0), that the two stage-least squares was inconsistent and has a nonstandard, mixed normal
distribution, and in multiparameter problems where some regression coefficients are identified, their
approximate distributions were also nonnormal.
3
In the just-identified case, it is possible to derive the distribution of the ratio of normal variables characterizing the limiting distribution of the coefficient estimator, but that distribution depends on nuisance
parameters in the covariance matrix of zu and zv that are not consistently estimable (because of the
inability to consistently estimate β  ). Dufour (1997) shows that any confidence set with correct coverage
probability in finite samples must be unbounded with positive probability when the true parameter β is
nearly unidentified, so that the usual confidence intervals based on the standard asymptotic distribution
of the t-statistic must have zero coverage probability when the instruments are weak.

James L. Powell

113

so large as to be not useful. The primary approach to confidence set construction
in the weak instrument literature has been “test statistic inversion,” which means
finding the set of values of β
​ 0​for which a test of the null hypothesis H
​ 0​: β = ​β0​fails to
reject. This approach transforms the problem of finding good confidence regions
into the problem of finding good hypothesis tests, where “good” means having a
correct significance level under the null hypothesis, regardless of the number and
strength of the instruments, and high power under the alternative H
​ A​: β ≠ ​β0​. The
first priority is to get correct size (significance level), and tests with correct size can
then be ranked on their power properties.
Staiger and Stock (1997) proposed two methods to obtain confidence sets for
β. The first constructs confidence regions by inverting what is called the Anderson
and Rubin (1949) test statistic: essentially, this approach checks whether the
hypothesized residuals that emerge from an instrumental variables procedure
​ε0​  = y – x​β 0​are linearly related to the instrument vector z using a standard F test
for a regression of ε0 on z. Staiger and Stock’s second proposal combines a confidence interval for the F   -  statistic in the first-stage with a confidence interval using
the distribution of the t    -  statistic for β, conditional on the first-stage F   -  statistic, to
obtain a conservative confidence interval for β. Neither procedure was better in
all cases, with the Anderson–Rubin approach dominating for smaller numbers of
instruments and weaker instruments and vice-versa for the second approach. Wang
and Zivot (1998) showed that the critical values for Anderson–Rubin are valid with
weak instruments for all three test statistics, but confidence sets constructed using
this result will generally be too conservative (larger than needed or useful), especially when the number of instruments is large.
A number of alternative procedures have been proposed for constructing
confidence tests for β under weak instruments. Kleibergen (2002) proposes a
modification of the Lagrange Multiplier (LM) test statistic for H 0: β = β0 using an
improved estimator of the first-stage coefficients π under the null hypothesis, and
shows that this test has correct asymptotic size regardless of the strength of the instruments. Moreira (2003) describes a procedure to make the significance level of a test
using a standard test statistic constant (termed “similar”) across all possible values
of the first-stage coefficient vector π, focusing on the conditional likelihood ratio
(CLR) test. Andrews, Moreira, and Stock (2006) review a number of these efforts
and consider the problem of finding a uniformly most powerful test of H 0: β = β0
among tests that are similar. In the just-identified case with the number of instruments equaling the number of regressors, the tests proposed by Kleibergen (2002)
and Moreira (2003) are equivalent to the Anderson–Rubin test, which is optimal in
this setting. In the over-identified case, where the number of instruments exceeds
the number of regressors, Andrews, Moreira, and Stock show that no uniformly
most powerful invariant test exists, but they also found that Moreira’s conditional
likelihood ratio test was nearly optimal.4
4

Stock and Wright (2000) and Kleibergen (2005) extend the treatment of weak identification to
nonlinear models with moment restrictions on the error terms, while Andrews and Cheng (2012) give a

114

Journal of Economic Perspectives

There are some complications in translating this theoretical result into empirical practice. The test inversion procedure described here gives a joint confidence set
for all of the coefficients of the endogenous regressors, but individual confidence
intervals for particular components can be quite conservative, and thus too broad to
be useful. The method works best when the number of endogenous variables is small
(like the single endogenous regressor x in the earlier example). Even for models with
a single endogenous variable, the confidence region for β using the test inversion
approach need not be a single interval, but may be a collection of intervals, possibly
open-ended. While there have been some empirical applications of confidence sets
that are valid with weak instruments, the main influence of the theoretical work on
weak instruments is likely the routine use of the Staiger and Stock (1997) “rule of
thumb” check of the first-stage F   -  statistic as a diagnostic for adequate performance
of standard inference procedures for instrumental variables estimators.

Nonparametric Identification with Endogenous Regressors
While the weak instrument literature focused on identification problems in
linear models, different problems arise when the linearity of the regression function
is relaxed. Much research in theoretical econometrics in the 1980s and 1990s was
devoted to estimation of models with nonparametric components. In these analyses,
the linearity of the regression function is relaxed, and the object of interest was
either the nonparametrically specified function or a parametric component of the
model (a “semiparametric” problem). Existing results for nonparametric estimation
of smooth and/or monotonic functions were refined and applied to econometric
models, with the goal of producing estimators of any parametric components that
had standard large-sample behavior (that is, their distribution was approximately
normal with approximate variances shrinking to zero inversely with the sample size)
and estimators of nonparametric components that had approximate mean-squared
error shrinking to zero as quickly as possible. The regressors appearing as arguments of the nonparametric functions in these models were generally assumed to
be exogenous—that is, the error terms were independent (at least in expectation)
of the regressors.
More recently, research in theoretical econometrics has sought to extend the
theoretical results available for identification and estimation of nonparametric and
semiparametric models to allow for endogenous variables in the nonparametric
components. Combining nonparametrics and endogeneity turns out to be tricky,
and the various identification strategies for the nonparametric regression functions
all face some limitations.

unified treatment of construction of nearly optimal confidence sets for a general class of weakly identified econometric models.

Identification and Asymptotic Approximations: Three Examples of Progress

115

Early articles on nonparametric endogeneity considered a starting model that
assumes a single dependent variable y and single regressor x are related by the structural equation
y = g(x) + ε
for some unobservable error term ε, where the regression function g(x) is not
restricted to be linear or parametric but is only required to be well-behaved,
meaning that it is smooth and/or monotonic in x. If the regressor x was assumed
to be exogenous, then the function g(x) would be the conditional mean of y given
x, that is, g(x) = E[y|x], which is identified from the joint distribution of the observable variables y and x and can be consistently estimated using existing methods: for
example, local averaging of values of y for nearby values of x or flexible parametric
approximations to g(x).
But when x is endogeneous, the conditional mean of the error term ε given
x is nonzero. One traditional strategy for identification of the regression function
would be to seek out some instrumental variable z, assuming the error ε has mean
zero given z. This instrumental variable approach is fairly flexible, in that it does not
require a full specification of a first-stage equation for the endogenous regressor x,
but, as a result, identification and estimation under this restriction can be tenuous.
The “reduced form” regression function h(z) of the relation of y to the instruments
z is identified and can be estimated using standard nonparametric methods, and
the same holds for the conditional density f (x|z) of the regressors given the instruments (both assumed to be continuously distributed). Alas, this implies that the
unknown “structural” regression function g(x) is the solution to a complicated
(integral) equation involving h(z) and f (x|z).5 Whether the true regression function
g(x) is an identified and thus a unique solution depends on how the density f (x|z)
depends on the instrumental variable z and how variable the instrument z is. Even if
it is directly assumed that the solution g(x) is identified and unique, in general the
solution ​​gˆ​​(x) based on nonparametric estimators of h(z) and f (x|z) will not be consistent, because slight departures of the estimators of h(z) and f (x|z) from their true
values can lead to very large departures in the estimator ​​gˆ​​(x) of the true function
​g​(x), a phenomenon known as the “ill-posed inverse problem.”
Several articles have investigated the possibility of consistent estimation of
the regression function g(x) or similar nonparametric objects. For example, the
papers Newey and Powell (2003), Ai and Chen (2003), and Blundell, Chen, and
Kristensen (2007) used series approximations to estimate the unknown functions
nonparametrically. Alternatively, Hall and Horowitz (2005) and Darolles, Fan,
Formally, writing the moment restriction E[ε|z] = 0 in terms of ε = y – g(x), the unknown regression
function g(x) is a solution to the “reduced form” integral equation

5

h(z) ≡ E[y|z] = E[g(x)|z] = ∫ g(x) f (x|z) dx,

where f (x|z) is the density of the endogenous regressor x (assumed continuously distributed) given the
instruments z.

116

Journal of Economic Perspectives

Florens, and Renault (2011) used estimators based on kernel methods. The first
two articles gave conditions under which g(x) could be consistently estimated but
did not derive rates of convergence of the proposed estimators to the true function,
while the latter three articles provide convergence rates under different characterizations of the degree of smoothness of the regression function g(x) and the
conditional density function f (x|z).6 The convergence rates for the nonparametric
estimators depended upon the extent to which the transformation between g(x)
and the reduced form function h(x) is “ill-posed” or even “severely ill-posed.” These
theoretical results show the sensitivity of the estimators of g(x) to the underlying
smoothness and other features of the unknown functions, making one wonder
how well the methods would perform in practice. However, Blundell, Chen, and
Kristensen (2007) did find that their method gave plausible results in estimation
of Engel curves for household expenditure categories when total expenditure was
treated as an endogenous regressor. Also, the Ai and Chen (2003) and Blundell,
Chen, and Kristensen (2007) articles considered more general econometric models
with both parametric and nonparametric components and demonstrated that,
even when the nonparametric components are imprecisely estimated under the
instrumental variable condition, the parametric part of the model may be precisely
estimated, with approximate variance inversely proportional to the sample size, so
the nonparametric instrumental variables approach may be more appealing when
the parametric part of the model is the main object of interest.
A different strategy for identification of the nonparametric regression function
g(x) with an endogenous regressor puts more structure on the first-stage relationship between the endogenous regressor x and the instrumental variables z, with the
goal of constructing a “control variable” v that can be included to correct for endogeneity in a second-stage nonparametric regression. For example, suppose that the
error ε is assumed to satisfy the “mean exclusion restriction” E[ε|x, v] = E[x|v] ≡ h(v)
for some identified variable v; under this condition, the mean of the dependent variable y given the endogenous regressor x and control variable v takes the a­ dditively
separable form
E[y|x, v] = g(x) + h(v),
so the structural regression function g(x) can be directly estimated from a nonparametric regression of y on x and v. In addition, the control function approach to
modeling nonparametric endogeneity can also be generalized to models in which
the dependent variable is not additively separable in the regression function and
the error term, as discussed in Blundell and Powell (2003) and Imbens and Newey

6
Chernozhukov and Hansen (2005) considered a “quantile” version of this regression problem by
replacing the identifying restriction E[y – g(x)|z] = 0 with the condition Pr[y – g(z) ≤ 0|z] = τ for a known
value of τ between zero and one, which yields a similar “ill-posed” integral equation.

James L. Powell

117

(2009), who also considered estimation of the quantiles (percentiles) of the structural function g(x).7
The tricky part is to find the right specification of the relationship of the regressors x and instruments z that yields the control variable v satisfying the required
mean exclusion assumption. Using the control variable approach, the direct effect
of x on y cannot be directly identified from features of the conditional distribution of y given x (like the conditional expectation) alone, but instead is identifiable
from the conditional distribution of y given both x and a “control variable” v under
the assumption of independence of the errors ε and the endogenous regressors x
given v.8 For example, the average structural function can be estimated by first estimating the nonparametric regression of y on x and v and then averaging it over the
marginal distribution of v. The catch, of course, is to come up with a control variable v that is observable or estimable that satisfies this conditional independence
assumption, which generally involves more assumptions on how the endogenous
regressor x is related to the structural error ε. The simplest case has a first-stage
equation for x that takes the form
x = q(z, v)
for some instrumental variable z and a first-stage error v; this is called a “triangular” model because x appears in the equation for the outcome variable y, but
not vice-versa. Like the definition of the direct effect of x on y, the literature has
a number of possible alternative definitions of the control variable v. The Newey,
Powell, and Vella (1999), Pinkse (2000), and Das, Newey, and Vella (2003) papers
assumed the first-stage had additive errors, q(z, ε) = r(z) + v, and estimated v using
residuals from a nonparametric regression of x on z. This was also true for the
empirical application in Blundell and Powell (2003), which estimated a model of
labor force participation for which the endogenous regressor x includes “outside
income” and a government benefit eligibility variable was used as an instrument.
For panel data applications, Altonji and Matzkin (2005) show how symmetric
functions of the regressors x over all time periods might be used to control for
endogeneity of the regressors x that are specific to each time period. Imbens
7
There is some ambiguity about the proper counterfactual summary measures of the “direct effect”
of x on y when the relationship is nonseparable, that is, y = H(x, ε). The literature on nonseparable
endogeneity models includes various approaches: the average of the structural function H(x, ε) over
the marginal distribution of the errors ε (Blundell and Powell 2003), or the corresponding quantiles of
the structural function (Imbens and Newey 2009); the derivative of the average structural function (in
x), termed the “average partial effect” (Wooldridge 2005); the average derivative of H(x, ε), over the
conditional distribution of ε given x, or “local average response” (Altonji and Matzkin 2005) or derivatives of H(x, ε) evaluated at particular quantiles of ε (Chesher 2003). Florens, Heckman, Meghir, and
Vytlacil (2008) note that the average partial effect is the continuous-regressor analogue to the “average
treatment effect” when x is a binary treatment variable, while the local average response function is the
analogue of the “treatment on treated” effect.
8
For some applications, a weaker form of independence such as conditional mean or quantile independence of ε and x given v is sufficient.

118

Journal of Economic Perspectives

and Newey (2009) showed that an additive error was not needed as long as x was
an increasing function of v, in which case the needed control variable v could
be defined as the conditional distribution function of x given z evaluated at the
observed random variables x and z.9 Each of these specifications of the “control
variable” v is based upon a correct specification of the relationship between x and
the instrumental variables z (including a complete list of the relevant instruments,
to ensure the assumption of independence of ε and x given v). The ability to specify
this first-stage relationship greatly simplifies the identification of the direct effect of
x on y, however that effect is defined.
Identification of structural relations for simultaneous equations systems that
do not have a triangular structure (that is, when the “first-stage” equation for
the endogenous variable x also depends on y) is far more challenging. Benkard
and Berry (2006) noted that earlier results on nonparametric identification of
systems of simultaneous equations were incorrect; corrected conditions for
identification of such systems were derived by Matzkin (2008), conditions that
involved a rank condition on a matrix of derivatives of the structural functions
and error density that yielded the identification results for the triangular systems
considered by Chesher (2003) and by Imbens and Newey (2009) as special cases.
Nonparametric identification of structural equations is even more problematic
when the endogenous regressor x is not continuously distributed, in which case
it is generally impossible to find a control variable v that makes the error ε and
endogenous regressor x conditionally independent. Chernozhukov and Hansen
(2005) showed how the quantile structural function for y could be identified with
a binary endogenous regressor x under the assumption that the rank ordering of
the error ε was preserved conditional on instrumental variables z, but typically
the theoretical results on nonparametric identification with discrete endogenous regressors involve the set identification concepts discussed in the next
section.

Partial Identification and Inference
In some econometric models, the parameters of interest may not be uniquely
determined by the distributions of observable data—that is, they are not “point”
or “fully” identified. Instead, the population distribution may restrict the possible
values of those parameters to some subset (which one hopes is relatively small) of
their possible values, in which case the parameters are said to be “set” or “partially”
identified.
The roots of much of the research on partial identification begin with Manski
(1989, 1990), who provided examples that demonstrated how information on the
identified components of an econometric model can be used to reduce the range
9

Matzkin (2003) showed that this representation for v was observationally equivalent to a general invertible specification for the first-stage function h.

Identification and Asymptotic Approximations: Three Examples of Progress

119

of possible values of parameters that are not fully identified. For a model with a
nonrandomly missing outcome variable y that is bounded between known values yL
and yU, Manski (1989) used the iterated expectations formula
E[y] = E[y | A] Pr{A} + E[y | not A] Pr{not A}
to show that knowledge of the conditional mean E[y | A] of y for some subpopulation A, along with the proportion PA of the population in A, would reduce the width
yU – yL of the range of possible values of the unconditional (population) mean E[y]
by a factor 1 – PA.10 Variations on this idea were applied to obtain identification
bounds for other problems with nonrandomly missing data, like treatment effects
for programs with nonrandom assignment (Manski 1990) and regression functions
for data with censored outcomes or regressors (Horowitz and Manski 1995, 1998).
It can be difficult to determine prior bounds for continuously distributed
outcome variables. However, when the dependent variable is binary and its expectation, a probability, is the parameter of interest, then the bounds zero and one are
automatic. As another useful restriction, monotonicity requirements on unknown
functions can substantially tighten their identifiable regions, as those functions
inherit the largest of the lower bounds (or smallest of the upper bounds) derived
for the function at lower (or higher) values of its argument. Articles by Manski
(1997) and Manski and Pepper (2000) demonstrated how monotonicity restrictions
on unknown functions could sharpen identification bounds for parameters of an
unobservable treatment response schedule when either that schedule is monotonic
in the treatment variable or is monotonically related to an observable instrumental
variable. Manski and Tamer (2002) derived bounds for regression functions when
one of the regressors is interval-censored—that is, it is only known to lie in an interval
with observable endpoints—when the regression is monotonic in the uncensored
regressor.
Sometimes the bounds for the outcome variable of interest arise naturally from
the economic model for the data-generating process. For example, Haile and Tamer
(2003) show how bounds for the distribution function of independent private
values in an English auction can be derived using the (fully identified) distribution
function of order statistics of bids in those auctions. This relationship exploits the
behavioral assumptions that bids never exceed valuations, and also that valuations
never exceed the winning bid of a competitor. Furthermore, given bounds on the
valuation distribution obtained from auctions with different numbers of bidders,
tighter bounds can be obtained using the maximum of the lower bound and the
minimum of the upper bound across auction sizes. Haile and Tamer are also able to
obtain bounds on the optimal reserve price of the seller in this market, and discuss
consistent estimation of the bounds using the empirical distribution of order statistics of bids in auctions of different sizes.

10

The lower and upper bounds for E[y] are yL + PA (E[y | A] – yL) and yU – PA (yU – E[y | A]), respectively.

120

Journal of Economic Perspectives

Another application of the partial identification strategy to a traditional econometrics problem was proposed by Honoré and Tamer (2006), who considered
identification of regression parameters for a dynamic nonlinear panel data model in
which the dependent variable is binary and depends upon several factors: an unknown
linear combination of exogenous regressors; a lagged value of the dependent variable (to capture “state dependence,” an effect emphasized by Heckman 1981); an
individual-specific intercept term α (which works much like a “fixed effect”); and a
time-varying error term.. Identification of the underlying regression coefficients was
known to be challenging for this model even when the error term has a parametric
distribution, due to the difficulty in estimation of the unknown distribution of the
dependent variable for the first time period, for which no lagged value is available
(the “initial conditions” problem).11 Honoré and Tamer treated the distribution of
the initial value of the dependent variable as an unidentified nuisance parameter and
showed how to compute the resulting identified sets of the regression parameters for
common variants of the binary panel data model. Their numerical results suggested
that the identification regions for the key parameters of interest were quite small
for the cases they considered, suggesting that the lack of full identification for these
parameters would be a secondary consideration in empirical applications of these
models. A similar set-identification strategy for the regression parameters in dynamic
panel data models was proposed by Chernozhukov, Fernándex-Val, Hahn, and Newey
(2013); this strategy treated the conditional distribution of the “fixed effect” given
the regressors as the unidentified nuisance parameter and derived estimators of
bounds for the regression parameters and for average and quantile effects, in this
case exploiting the boundedness of the dependent variable between zero and one.
Most of the early papers on set identification (like Haile and Tamer 2003)
proposed estimation of the identified sets using nonparametric estimators of the
identified components, but they less frequently derived inference procedures for
the partially identified parameters in a way that accounted for the estimation error
in the nonparametric components. An exception, from Horowitz and Manski
(2000), considered construction of a confidence set when the estimated identified
set is an interval with estimated endpoints that are approximately normally distributed around the true values. Imbens and Manski (2004) noted that a conservative
95 percent confidence interval for the entire identified set (that is, the confidence interval would cover the true identified set with probability at least equal to
95 percent) could be constructed by adding and subtracting two standard errors
to the estimated upper and lower bounds (respectively). However, as Imbens and
Manski noted, construction of a confidence set for the entire identification region
is a more conservative objective than construction of a traditional confidence set for
11

In a related paper, Honoré and Kyriazidou (2000) showed how the regression parameters could be
(point) identified for panels with four or more time periods, by restricting attention to the subsample
of observations in which the dependent variable changes in the middle two time periods while the
exogenous regressors do not change, but this “regressor matching” approach can be problematic if the
subsample of observations with unchanging regressors is small or empty because some regressors are
continuously distributed or time specific (for example, time dummies).

James L. Powell

121

the single true parameter, which can assume only one of the values in the identified
set; for the latter goal, a 95 percent confidence interval would add and subtract an
estimated critical value c​​​ˆ​​N​​​ times the standard error from the estimated upper and
lower bounds of the identified set, where c​​​ˆ​​N​​​ depends on the sample size and the
width of the identification interval and tends to either 1.645 (the one-sided normal
critical value) or 1.96 (the two-sided critical value) depending on whether the true
identification interval is has positive length or is a single point.
Chernozhukov, Hong, and Tamer (2007) proposed a general method to
construct confidence sets for the identification regions for a parameter vector θ that
is (partially) identified as the solution to a collection of moment inequalities—that is,
all values of θ that satisfy conditions of the form
μ(θ) ≡ E[m(w, θ)] ≤ 0
for some known vector of functions m(w, θ) of the observable data vector w (outcome
variables, regressors, and instruments) and unknown parameter θ. Moment
inequalities can be used to represent the partial identification problems described
earlier, and they often arise in economic models of strategic behavior, as illustrated
by examples given by Chernozhukov, Hong, and Tamer and by Pakes, Porter, Ho,
and Ishii (2015). The identified set of possible true values of θ is the set of values
for which this equality is satisfied, and Chernozhukov, Hong, and Tamer propose
a confidence region for this identified set.12 A number of articles have proposed
different methods to construct confidence sets for classes of moment inequality
and other partial identification problems. Some, like Stoye (2009) and Andrews
and Soares (2010), proposed confidence regions guaranteed to cover only the true
values of the parameter and not the entire identified set (as in Imbens and Manski’s
approach). Others, including Beresteanu and Molinari (2008), Rosen (2008), and
Romano and Shaikh (2010), constructed confidence sets intended to cover the
entire identified set with high probability.
In addition to these general methods for statistical inference, the partial identification approach continues to find applications to thorny identification problems
in structural econometric models, including triangular models with endogenous
binary regressors (Chesher 2005; Shaikh and Vytlacil 2011), nonparametric regression models with endogeneity (Santos 2012), and nonseparable dynamic panel data
models (Chernozhukov, Fernández-Val, Hahn, and Newey 2013). Indeed, articles in
econometrics now regularly include sections discussing bounds for the parameters
of interest when the assumptions for point identification fail to hold, and application
of partial identification methods in econometrics remains a growth area in the field.
Moment inequalities are a generalization of the familiar moment restrictions E[m(w, θ)] = 0 that are
the basis for generalized method-of-moment (GMM) estimation. The equality E[m(w, θ)] = 0 can always
be expressed as the pair of inequalities E[m(w, θ)] ≤ 0 and E[–m(w, θ)] ≤ 0. The procedure discussed
here is analogous to construction of confidence regions for moment equalities using inversion of the
J-test statistic proposed by Hansen (1982) to test the validity of over-identifying restrictions in generalized
method of moments estimation.
12

122

Journal of Economic Perspectives

Conclusions
The three specific research areas discussed here give a glimpse of some trends
in theoretical econometrics, but they are of course not exhaustive of the progress
made in the field. Browsing through outlets for econometric theory like Econometrica, the Review of Economic Studies, the Journal of Econometrics, and Econometric Theory,
among others, I came across many other “wish I’d thought of that” articles. And
in the past few the years, econometric theorists have worked to extend the foundational concepts of endogeneity and causal inference to increasingly complex
problems in statistical inference.
In my view, the biggest current growth areas in econometrics involve analysis
of “high-dimensional” models, in which, like the “many instrument” literature, the
number of parameters K may be as large, or larger than, the sample size N. Such
phenomena arise naturally in economic models of networks, where the number of
potential links between agents in a network grows quadratically with the number of
agents, and the object is to flexibly model the link probabilities or exchanges among
groups of agents. The traditional “selection of regressors” problem in econometrics is another high-dimensional model when the number of potential regressors
is large, and ongoing research is investigating the benefits and pitfalls of different
model selection schemes. Some of these schemes are adapted from the research in
statistics and computer science on “machine learning” (surveyed elsewhere in this
symposium), and adapting these large-scale predictive methods to answer the causal
questions of interest to economists is and will be a hot topic for econometric theory.
It is hard to guess what the next “big idea” in econometrics will be, but I think that,
when viewed in retrospect, it will be a logical successor to the problems considered
in the three research areas discussed above.

■ I am grateful to Bryan Graham, Bo Honoré, Whitney Newey, Michael Jansson, Demian
Pouzo, and the editors for their helpful comments.

References
Ai, Chunrong, and Xiaohong Chen. 2003.
“Efficient Estimation of Models with Conditional
Moment Restrictions Containing Unknown Functions.” Econometrica 71(6): 1795–1843.
Altonji, Joseph G., and Rosa L. Matzkin. 2005.
“Cross Section and Panel Data Estimators for
Nonseparable Models with Endogenous Regressors.” Econometrica 73(4): 1053–1102.

Anderson, Theodore W., and Herman Rubin.
1949. “Estimation of the Parameters of a Single
Equation in a Complete System of Stochastic
Equa­tions.” Annals of Mathematical Statistics 20(1):
46–63.
Andrews, Donald W. K., and Xu Cheng. 2012.
“Estimation and Inference with Weak, SemiStrong, and Strong Identification.” Econometrica

Identification and Asymptotic Approximations: Three Examples of Progress

80(5): 2153–2211.
Andrews, Donald W. K., Marcelo J. Moreira,
and James H. Stock. 2006. “Optimal Two-Sided
Invariant Similar Tests for Instrumental Variables
Regression.” Econometrica 74(3): 715–52.
Andrews, Donald W. K., and Gustavo Soares.
2010. “Inference for Parameters Defined by
Moment Inequalities Using Generalized Moment
Selection.” Econometrica 78(1): 119–57.
Angrist, Joshua D., and Alan B. Krueger. 1991.
“Does Compulsory School Attendance Affect
Schooling and Earnings?” Quarterly Journal of
Economics 106(4): 979–1014.
Bekker, Paul A. 1994. “Alternative Approximations to the Distributions of Instrumental Variable
Estimators.” Econometrica 62(3): 657–81.
Benkard, C. Lanier, and Steven Berry. 2006. “On
the Nonparametric Identification of Nonlinear
Simulataneous Equations Models: Comment on
Brown (1983) and Roehrig (1988).” Econometrica
74(5): 1429–40.
Beresteanu, Arie, and Francesca Molinari. 2008.
“Asymptotic Properties for a Class of Partially Identified Models.” Econometrica 76(4): 763–814.
Blundell, Richard, Xiaohong Chen, and Dennis
Kristensen. 2007. “Semi-Nonparametric IV Estimation of Shape-Invariant Engel Curves.” Econometrica
75(6): 1613–69.
Blundell, Richard, and James L. Powell. 2003.
“Endogeneity in Nonparametric and Semiparametric Regression Models.” Chap. 8 in Advances
in Economics and Econometrics: Theory and Applications, vol. 2, edited by Matias Dewatripont, Lars
P. Hansen, and Stephen Turnovsky. Cambridge
University Press.
Bound, John, David A. Jaeger, and Regina M.
Baker. 1995. “Problems with Instrumental Variables Estimation when the Correlation between
the Instruments and the Endogenous Explanatory
Variable is Weak.” Journal of the American Statistical
Association 90(430): 443–50.
Chernozhukov, Victor, Iván Fernández-Val,
Jinyong Hahn, and Whitney Newey. 2013. “Average
and Quantile Effects in Nonseparable Panel
Models.” Econometrica 81(2): 535–80.
Chernozhukov, Victor, and Christian Hansen.
2005. “An IV Model of Quantile Treatment
Effects.” Econometrica 73(1): 245–61.
Chernozhukov, Victor, Han Hong, and Elie
Tamer. 2007. “Estimation and Confidence Regions
for Parameter Sets in Econometric Models.” Econometrica 75(5): 1243–84.
Chesher, Andrew. 2003. “Identification in
Nonseparable Models.” Econometrica 71(5):
1405–41.
Chesher, Andrew. 2005. “Nonparametric Identification under Discrete Variation.” Econometrica

123

73(5): 1525–50.
Choi, In, and Peter C. B. Phillips. 1992. “Asymptotic and Finite Sample Distribution Theory for IV
Estimators and Tests in Partially Identified Structural Equations.” Journal of Econometrics 51(1–2):
113–50.
Darolles, Serge, Yanqin Fan, Jean-Pierre
Florens, and Eric Renault. 2011. “Nonparametric
Instrumental Regression.” Econometrica 79(5):
1541–65.
Das, Mitali, Whitney K. Newey, and Francis
Vella. 2003. “Nonparametric Estimation of Sample
Selection Models.” Review of Economic Studies 70(1):
33–58.
Dufour, Jean-Marie. 1997. “Some Impossibility
Theorems in Econometrics with Applications to
Structural and Dynamic Models.” Econometrica
65(6): 1365–87.
Florens, Jean-Paul, James J. Heckman, Costas
Meghir, and Edward Vytlacil. 2008. “Identification
of Treatment Effects Using Control Functions in
Models with Continuous, Endogenous Treatment
and Heterogeneous Effects.” Econometrica 76(5):
1191–1206.
Haile, Philip A., and Elie Tamer. 2003. “Inference with an Incomplete Model of English
Auctions.” Journal of Political Economy 111(1): 1–51.
Hall, Peter, and Joel L. Horowitz. 2005.
“Nonparametric Methods for Inference in the
Presence of Instrumental Variables.” Annals of
Statistics 33(6): 2904–29.
Hansen, Lars Peter. 1982. “Large Sample
Properties of Generalized Method of Moments
Estimators.” Econometrica 50(4): 1029–54.
Heckman, James J. 1981. “The Incidental
Parameters Problem and the Problem of Initial
Conditions in Estimating a Discrete Time–Discrete
Data Stochastic Process.” In Structural Analysis of
Discrete Panel Data with Econometric Applications,
edited by Charles F. Manski and Daniel McFadden,
pp. 114–78. Cambridge, MA: MIT Press.
Honoré, Bo E., and Ekaterini Kyriazidou. 2000.
“Panel Data Discrete Choice Models with Lagged
Dependent Variables.” Econometrica 68(4): 839–74.
Honoré, Bo E., and Elie Tamer. 2006. “Bounds
on Parameters in Panel Dynamic Discrete Choice
Models.” Econometrica 74(3): 611–29.
Horowitz, Joel L., and Charles F. Manski. 1995.
“Identification and Robustness with Contaminated
and Corrupted Data.” Econometrica 63(2): 281–302.
Horowitz, Joel L., and Charles F. Manski. 1998.
“Censoring of Outcomes and Regressors Due to
Survey Nonresponse: Identification and Estimation Using Weights and Imputations.” Journal of
Econometrics 84(1): 37–58.
Horowitz, Joel L., and Charles F. Manski.
2000. “Nonparametric Analysis of Randomized

124

Journal of Economic Perspectives

Experiments with Missing Covariate and Outcome
Data.” Journal of the American Statistical Association
95(449): 77–84.
Imbens, Guido W., and Charles F. Manski.
2004. “Confidence Intervals for Partially Identified
Parameters.” Econometrica 72(6): 1845–57.
Imbens, Guido W., and Whitney K. Newey.
2009. “Identification and Estimation of Triangular
Simultaneous Equations Models without Additivity.” Econometrica 77(5): 1481–1512.
Khan, Shakeeb, and Elie Tamer. 2010. “Irregular
Identification, Support Conditions, and Inverse
Weight Estimation.” Econometrica 78(6): 2021–42.
Kleibergen, Frank. 2002. “Pivotal Statistics for
Testing Structural Parameters in Instrumental Variables Regression.” Econometrica 70(5): 1781–1803.
Kleibergen, Frank. 2005. “Testing Parameters
in GMM Without Assuming that They Are Identified.” Econometrica 73(4): 1103–23.
Manski, Charles F. 1997. “Monotone Treatment
Response.” Econometrica 65(6): 1311–44.
Manski, Charles F. 1989. “Anatomy of the Selection Problem.” Journal of Human Resources 24(3):
343–60.
Manski, Charles F. 1990. “Nonparametric
Bounds on Treatment Effects.” American Economic
Review 80(2): 319–23.
Manski, Charles F., and John V. Pepper. 2000.
“Monotone Instrumental Variables: With an Application to the Returns to Schooling.” Econometrica
68(4): 997–1010.
Manski, Charles F., and Elie Tamer. 2002.
“Inference on Regressions with Interval Data on
a Regressor or Outcome.” Econometrica 70(2):
519–46.
Matzkin, Rosa L. 2003. “Nonparametric
Estimation of Nonadditive Random Functions.”
Econometrica 71(5): 1339–75.
Matzkin, Rosa L. 2008. “Identification in
Nonparametric Simultaneous Equations Models.”
Econometrica 76(5): 945–78.
Moreira, Marcelo J. 2003. “A Conditional
Likelihood Ratio Test for Structural Models.”
Econometrica 71(4): 1027–48.
Newey, Whitney K., and James L. Powell. 2003.
“Instrumental Variable Estimation of Nonparametric Models.” Econometrica 71(5): 1565–78.

Newey, Whitney K., James L. Powell, and
Francis Vella. 1999. “Nonparametric Estimation
of Triangular Simultaneous Equations Models.”
Econometrica 67(3): 565–603.
Pakes, Ariel, Jack Porter, Kate Ho, and Joy Ishii.
2015. “Moment Inequalities and Their Application.” Econometrica 83(1): 315–34.
Phillips, Peter C. B. 1989. “Partially Identified
Econometric Models.” Econometric Theory 5(2):
181–240.
Pinkse, Joris. 2000. “Nonparametric Two-Step
Regression Estimation When Regressors and Error
Are Dependent.” Canadian Journal of Statistics
28(2): 289–300.
Romano, Joseph P., and Azeem M. Shaikh.
2010. “Inference for the Identified Set in Partially
Identified Econometric Models.” Econometrica
78(1): 169–211.
Rosen, Adam M. 2008. “Confidence Sets for
Partially Identified Parameters that Satisfy a Finite
Number of Moment Inequalities.” Journal of Econometrics 146(1): 107–117.
Santos, Andres. 2012. “Inference in Nonparametric Instrumental Variables with Partial
Identification.” Econometrica 80(1): 213–75.
Shaikh, Azeem M., and Edward J. Vytlacil.
2011. “Partial Identification in Triangular Systems
of Equations with Binary Dependent Variables.”
Econometrica 79(3): 949–55.
Staiger, Douglas, and James H. Stock. 1997.
“Instrumental Variables Regression with Weak
Instruments.” Econometrica 65(3): 557–86.
Stock, James H., and Jonathan H. Wright. 2000.
“GMM with Weak Identification.” Econometrica
68(5): 1055–96.
Stoye, Jörg. 2009. “More on Confidence
Intervals for Partially Identified Parameters.”
Econometrica 77(4): 1299–1315.
Wang, Jiahui, and Eric Zivot. 1998. “Inference
on Structural Parameters in Instrumental Variables
Regression with Weak Instruments.” Econometrica
66(6): 1389–1404.
Wooldridge, Jeffrey M. 2005. “Unobserved
Heterogeneity and Estimation of Average Partial
Effects.” In Identification and Inference for Econometric
Models, edited by Donald W. K. Andrews and James
H. Stock. Cambridge University Press.

