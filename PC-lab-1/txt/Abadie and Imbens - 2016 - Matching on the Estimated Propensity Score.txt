Econometrica, Vol. 84, No. 2 (March, 2016), 781â€“807

NOTES AND COMMENTS
MATCHING ON THE ESTIMATED PROPENSITY SCORE
BY ALBERTO ABADIE AND GUIDO W. IMBENS1
Propensity score matching estimators (Rosenbaum and Rubin (1983)) are widely
used in evaluation research to estimate average treatment effects. In this article, we derive the large sample distribution of propensity score matching estimators. Our derivations take into account that the propensity score is itself estimated in a first step, prior to
matching. We prove that first step estimation of the propensity score affects the large
sample distribution of propensity score matching estimators, and derive adjustments
to the large sample variances of propensity score matching estimators of the average
treatment effect (ATE) and the average treatment effect on the treated (ATET). The
adjustment for the ATE estimator is negative (or zero in some special cases), implying
that matching on the estimated propensity score is more efficient than matching on the
true propensity score in large samples. However, for the ATET estimator, the sign of
the adjustment term depends on the data generating process, and ignoring the estimation error in the propensity score may lead to confidence intervals that are either too
large or too small.
KEYWORDS: Matching estimators, propensity score matching, average treatment effects, causal inference, program evaluation.

1. INTRODUCTION
PROPENSITY SCORE MATCHING ESTIMATORS (Rosenbaum and Rubin (1983))
are widely used to estimate treatment effects.2 Rosenbaum and Rubin (1983)
defined the propensity score as the conditional probability of assignment to a
treatment given a vector of covariates. Suppose that adjusting for a set of covariates is sufficient to eliminate confounding. The key insight of Rosenbaum
and Rubin (1983) is that adjusting only for the propensity score is also sufficient to eliminate confounding. Relative to matching directly on the covariates,
propensity score matching has the advantage of reducing the dimensionality of
matching to a single dimension. This greatly facilitates the matching process
1
We are grateful to the editor and three referees for helpful comments, to Ben Hansen, Judith Lok, James Robins, Paul Rosenbaum, Donald Rubin, and participants in many seminars for
comments and discussions, and to Jann Spiess for expert research assistance. Financial support
by the NSF through Grants SES 0820361 and SES 0961707 is gratefully acknowledged.
2
Following the terminology in Abadie and Imbens (2006), the term â€œmatching estimatorâ€ is
reserved in this article to estimators that match each unit (or each unit of some sample subset, e.g.,
the treated) to a small number of units with similar characteristics in the opposite treatment arm.
Thus, our discussion does not refer to regression imputation methods, like the kernel matching
method of Heckman, Ichimura, and Todd (1998), which use a large number of matches per unit
and nonparametric smoothing techniques to consistently estimate unit-level regression values
under counterfactual treatment assignments. See Hahn (1998), Heckman, Ichimura, and Todd
(1998), Imbens (2004), and Imbens and Wooldridge (2009) for a discussion of such estimators.

Â© 2016 The Econometric Society

DOI: 10.3982/ECTA11293

782

A. ABADIE AND G. W. IMBENS

because units with dissimilar covariate values may nevertheless have similar
values for their propensity scores.
In observational studies, propensity scores are not known, so they have to
be estimated prior to matching. In spite of the great popularity that propensity
score matching methods have enjoyed since they were proposed by Rosenbaum
and Rubin in 1983, their large sample distribution has not yet been derived for
the case when the propensity score is estimated in a first step.3 A possible reason for this void in the literature is that matching estimators are non-smooth
functionals of the distribution of the matching variables, which makes it difficult to establish an asymptotic approximation to the distribution of matching estimators when a matching variable is estimated in a first step. This has
motivated the use of bootstrap standard errors for propensity score matching
estimators. However, recently it has been shown that the bootstrap is not, in
general, valid for matching estimators (Abadie and Imbens (2008)).4
In this article, we derive large sample approximations to the distribution of
propensity score matching estimators. Our derivations take into account that
the propensity score is itself estimated in a first step. We show that propensity
matching estimators have approximately Normal distributions in large samples. We demonstrate that first step estimation of the propensity score affects
the large sample distribution of propensity score matching estimators, and derive adjustments to the large sample variance of propensity score matching
estimators that correct for first step estimation of the propensity score. We do
this for estimators of the average treatment effect (ATE) and the average treatment effect on the treated (ATET). The adjustment for the ATE estimator is
negative (or zero in some special cases), implying that matching on the estimated propensity score is more efficient than matching on the true propensity
score in large samples. As a result, treating the estimated propensity score as
it was the true propensity score for estimating the variance of the ATE estimator leads to conservative confidence intervals. However, for the ATET estimator, the sign of the adjustment depends on the data generating process, and
ignoring the estimation error in the propensity score may lead to confidence
intervals that are either too large or too small.
2. MATCHING ESTIMATORS
The setup in this article is a standard one in the program evaluation literature, where the focus of the analysis is often the effect of a binary treatment,
3
Influential papers using matching on the estimated propensity score include Heckman,
Ichimura, and Todd (1997), Dehejia and Wahba (1999), and Smith and Todd (2005).
4
In contexts other than matching, Heckman, Ichimura, and Todd (1998), Hirano, Imbens, and
Ridder (2003), Abadie (2005), Wooldridge (2007), and Angrist and Kuersteiner (2011) derived
large sample properties of statistics based on a first step estimator of the propensity score. In
all these cases, the second step statistics are smooth functionals of the propensity scores and,
therefore, standard stochastic expansions for two-step estimators apply (see, e.g., Newey and
McFadden (1994)).

MATCHING ON THE ESTIMATED PROPENSITY SCORE

783

represented in this paper by the indicator variable W , on some outcome variable, Y . More specifically, W = 1 indicates exposure to the treatment, while
W = 0 indicates lack of exposure to the treatment. Following Rubin (1974),
we define treatment effects in terms of potential outcomes. We define Y (1) as
the potential outcome under exposure to treatment, and Y (0) as the potential
outcome under no exposure to treatment. Our goal is to estimate the average
treatment effect,


Ï„ = E Y (1) âˆ’ Y (0) 
where the expectation is taken over the population of interest. Alternatively,
the goal may be estimation of the average effect for the treated,


Ï„t = E Y (1) âˆ’ Y (0)|W = 1 
Estimation of these average treatment effects is complicated by the fact that for
each unit in the population, we observe at most one of the potential outcomes:

Y (0) if W = 0,
Y=
Y (1) if W = 1.
Let X be a vector of covariates of dimension k. The propensity score is p(X) =
Pr(W = 1|X), and pâˆ— = Pr(W = 1) is the probability of being treated. The
following assumption is often referred to as â€œstrong ignorabilityâ€ (Rosenbaum
and Rubin (1983)). It means that adjusting for X is sufficient to eliminate all
confounding.
ASSUMPTION 1: (i) Y (1) Y (0) âŠ¥âŠ¥ W |X almost surely; (ii) p â‰¤ p(X) â‰¤ p
almost surely, for some p > 0 and p < 1.
Assumption 1(i) uses the conditional independence notation in Dawid
(1979). This assumption is often referred to as â€œunconfoundedness.â€ It will
hold, for example, if all confounders are included in X, so that after controlling for X, treatment exposure is independent of the potential outcomes. Hahn
(1998) derived asymptotic variance bounds and studied asymptotically efficient
estimation under Assumption 1(i). Assumption 1(ii) implies that, for almost all
values of X, the population includes treated and untreated units. Moreover,
Assumption 1(ii) bounds the values of the propensity score away from zero
and 1. Khan and Tamer (2010) have shown that this condition is necessary for
root-N consistent estimation of the average treatment effect.
Let Î¼(w x) = E[Y |W = w X = x] and Ïƒ 2 (w x) = var(Y |W = w X = x)
be the conditional mean and variance of Y given W = w and X = x. Similarly, let Î¼Ì„(w p) = E[Y |W = w p(X) = p] and ÏƒÌ„ 2 (w p) = var(Y |W = w
p(X) = p) be the conditional mean and variance of Y given W = w and

784

A. ABADIE AND G. W. IMBENS

p(X) = p. Under Assumption 1,


Ï„ = E Î¼(1 X) âˆ’ Î¼(0 X)
and


Ï„t = E Î¼(1 X) âˆ’ Î¼(0 X)|W = 1
(see Rubin (1974)). Therefore, adjusting for differences in the distribution of
X between treated and nontreated removes all confounding and, therefore,
allows identification of ATE and ATET. Rosenbaum and Rubin (1983) proved
that W and X are independent conditional on the propensity score, p(X),
which implies that under Assumption 1:
 



Ï„ = E Î¼Ì„ 1 p(X) âˆ’ Î¼Ì„ 0 p(X)
and
 




Ï„t = E Î¼Ì„ 1 p(X) âˆ’ Î¼Ì„ 0 p(X) |W = 1 
In other words, under Assumption 1, adjusting for the propensity score only
is enough to remove all confounding. This result motivates the use of propensity score matching estimators. A propensity score matching estimator for the
average treatment effect can be defined as

Ï„Nâˆ— =

N
1 
1 
(2Wi âˆ’ 1) Yi âˆ’
Yj 
N i=1
M jâˆˆJ (i)
M

where M is a fixed number of matches per unit and JM (i) is the set of matches
Ï„Nâˆ— indicates that matching is done on the true
for unit i.5 (The superscript âˆ— on 
propensity score.) For concreteness, in this article we will consider matching
with replacement, so each unit in the sample can be used as a match multiple
times. In the absence of matching ties, the set of matches JM (i) can formally
5
In typical applications, M is small, often M = 1. Choosing a small M reduces finite sample
biases caused by matches of poor quality, that is, matches between individuals with substantial
differences in their propensity score values. Larger values of M produce lower large sample variances (see Abadie and Imbens (2006, Section 3.4)), and one could consider increasing M in a
particular application if such increase has a small effect on the size of the matching discrepancies, which are observed in the data (Abadie and Imbens (2011)). Similarly to Yatchewâ€™s (1997)
work on semiparametric differencing estimators, we derive a large sample approximation to the
distribution of matching estimators for fixed values of M. Large sample approximations based
on fixed values of smoothing parameters have been shown to increase accuracy in other contexts
(see, in particular, Kiefer and Vogelsang (2005)).

MATCHING ON THE ESTIMATED PROPENSITY SCORE

785

be defined as


JM (i) = j = 1     N : Wj = 1 âˆ’ Wi 


1[|p(Xi )âˆ’p(Xk )|â‰¤|p(Xi )âˆ’p(Xj )|] â‰¤ M 

k:Wk =1âˆ’Wi

where 1[Â·] is a binary indicator that takes value 1 if the event inside brackets
is true, and value zero if not. For the average effect on the treated, Ï„t , the
corresponding estimator is
âˆ—
tN


Ï„

N
1 
1 
=
Wi Yi âˆ’
Yj 
N1 i=1
M jâˆˆJ (i)
M

N

where N1 = i=1 Wi is the number of treated units in the sample.
âˆ—
Ï„tN
can be easily derived from reThe large sample distributions of 
Ï„Nâˆ— and 
sults on matching estimators in Abadie and Imbens (2006), applied to the case
where the only matching variable is the known propensity score. However, one
of the assumptions in Abadie and Imbens (2006) requires that the density of
the matching variables is bounded away from zero. Although this assumption
may be appropriate in settings where the matching is carried out directly on
the covariates, X, it is much less appealing for propensity score matching estimators. For example, if the propensity score has the form p(X) = F(X  Î¸),
then even if the density of X is bounded away from zero on its support, the
density of F(X  Î¸) will generally not be bounded away from zero on its support. We therefore generalize the results in Abadie and Imbens (2006) to allow the density of propensity score to take values that are arbitrarily close to
zero.
ASSUMPTION 2: (i) The propensity score p(X) is continuously distributed, has
interval support [p p], and has a density that is a continuous function on [p p];
(ii) for w = 0 1, Î¼Ì„(w p) and ÏƒÌ„ 2 (w p) are Lipschitz-continuous and continuous
in p, respectively; (iii) for w = 0 1, there exists Î´ > 0 such that E[|Y |2+Î´ |W = w
p(X) = p] is uniformly bounded.
ASSUMPTION 3: {(Yi  Wi  Xi )}Ni=1 are independent draws from the distribution
of (Y W  X).
âˆ—
The next proposition presents the large sample distributions of 
Ï„Nâˆ— and 
Ï„tN
under Assumptions 1â€“3.

PROPOSITION 1: Suppose Assumptions 1â€“3 hold. Then, (i)
âˆš  âˆ—
 d 

N 
Ï„N âˆ’ Ï„ â†’ N 0 Ïƒ 2 

786

A. ABADIE AND G. W. IMBENS

where
 



2 
Ïƒ 2 = E Î¼Ì„ 1 p(X) âˆ’ Î¼Ì„ 0 p(X) âˆ’ Ï„

+ E ÏƒÌ„ 1 p(X)
2



1
1
1
+
âˆ’ p(X)
p(X) 2M p(X)





+ E ÏƒÌ„ 2 0 p(X)


1
1
1
+
âˆ’ 1 âˆ’ p(X)
Ã—
1 âˆ’ p(X) 2M 1 âˆ’ p(X)




and (ii)
âˆš  âˆ—
 d 

N 
Ï„tN âˆ’ Ï„t â†’ N 0 Ïƒt2 
where
Ïƒt2 =

2 

 



2 E p(X) Î¼Ì„ 1 p(X) âˆ’ Î¼Ì„ 0 p(X) âˆ’ Ï„t
E p(X)


1

+

 2


2 E ÏƒÌ„ 1 p(X) p(X)
E p(X)

+



2
2 E ÏƒÌ„ 0 p(X)
E p(X)

Ã—




1

1

1
1 p2 (X)
p2 (X)
+ p(X) +
1 âˆ’ p(X) M
2M 1 âˆ’ p(X)




The proof of this proposition is available in the Supplemental Material
(Abadie and Imbens (2016)).
Motivated by the fact that, in observational studies, propensity scores are
not known, we are interested in the case where matching is not on the true
propensity score p(X), but on an estimate of the propensity score. Following
Rosenbaum and Rubin (1983) and most of the empirical literature, we consider
a generalized linear specification for the propensity score, p(x) = F(x Î¸). In
empirical research, the link function F is usually specified as Logit or Probit.
It is straightforward to extend our results to more general parametric models
for the propensity score.6 For unit i, and for arbitrary values for Î¸, let JM (i Î¸)
6

Misspecification of the propensity score typically leads to inconsistency of the treatment effect
estimator, unless the misspecified propensity score constitutes a balancing score, that is, a function, b(X), of the covariates such that X âŠ¥âŠ¥ W |b(X) (see Rosenbaum and Rubin (1983)). Moti-

MATCHING ON THE ESTIMATED PROPENSITY SCORE

787

denote the set of M matches where we match on F(X  Î¸):

JM (i Î¸) = j = 1     N : Wj = 1 âˆ’ Wi 


1[|F(Xi Î¸)âˆ’F(Xk Î¸)|â‰¤|F(Xi Î¸)âˆ’F(Xj Î¸)|] â‰¤ M 

k:Wk =1âˆ’Wi

The matching estimator for the average treatment effect where we match on
F(X  Î¸) is then

Ï„N (Î¸) =

N
1 
1
(2Wi âˆ’ 1) Yi âˆ’
N i=1
M



Yj 

jâˆˆJM (iÎ¸)

Let Î¸âˆ— denote the true value of the propensity score model parameter vector, so that p(X) = F(X  Î¸âˆ— ). Then, the estimator based on matching on the
true propensity score can be written as 
Ï„Nâˆ— = 
Ï„N (Î¸âˆ— ). We are interested in the
case where 
Ï„N (Î¸) is evaluated at an estimator 
Î¸N of Î¸âˆ— , based on a sample
N
{Yi  Wi  Xi }i=1 . We focus on the case where 
Î¸N is the maximum likelihood esti7
mator of Î¸:

Î¸N = arg max L(Î¸|W1  X1      WN  XN )
Î¸

where the log-likelihood function is
L(Î¸|W1  X1      WN  XN )
=

N







Wi ln F Xi Î¸ + (1 âˆ’ Wi ) ln 1 âˆ’ F Xi Î¸ 

i=1

The propensity score matching estimator of Ï„ that matches on the estimated
propensity score can now be written as
N
1 
1

Ï„N = 
Ï„N (
Î¸N ) =
(2Wi âˆ’ 1) Yi âˆ’
N i=1
M



Yj 

jâˆˆJM (i
Î¸N )

vated by this consideration, empirical researchers routinely use measures of balance in the distribution of the covariates between treated and nontreated, conditional on the estimated propensity score, to perform specification searches on the propensity score (see, e.g., Dehejia and Wahba
(1999)). An alternative safeguard against misspecification of the propensity score is the use â€œdoubly robustâ€ matching estimators, like the bias-corrected matching estimator of Abadie and Imbens (2011).
7
It is straightforward to extend our results to other asymptotically linear estimators of Î¸âˆ— .

788

A. ABADIE AND G. W. IMBENS

Similarly, the propensity score matching estimator of Ï„t that matches on the
estimated propensity score can be written as
1 
1
=
Ï„tN (
Î¸N ) =
Wi Yi âˆ’
N1 i=1
M
N


Ï„tN



Yj 

jâˆˆJM (i
Î¸N )

Whenever confusion is possible, we will be explicit in the dependence of the
matching estimators on Î¸. If the argument is omitted, 
Ï„N and 
Ï„tN are used as
Î¸N ) and 
Ï„tN (
Î¸N ), respectively.
shorthand for 
Ï„N (
The two main questions addressed in this article are (i) do the estimators
based on matching on the estimated propensity score have Normal large sample distributions, and (ii) if so, how does their large sample variance compare to
that of the estimators that match on the true propensity score, given in Proposition 1? In the next section, we answer these two questions and derive the
Î¸N ) and 
Ï„tN (
Î¸N ). Conventional linearization
large sample distribution of 
Ï„N (
methods for two-step statistics are difficult to apply in the context of matching
estimators because matching estimators are complicated functionals of the distribution of the data. We therefore follow a different route, building on work
by Andreou and Werker (2012) on residual based statistics, and the martingale
representations for matching estimators derived in Abadie and Imbens (2012).
3. LARGE SAMPLE DISTRIBUTION
In the first part of this section, we derive the large sample approximation
Î¸N ), and in the second part, we present the
to the sampling distribution of 
Ï„N (
Î¸N ).
results for 
Ï„tN (
Let P Î¸ be the distribution of Z = {Y W  X} induced by the propensity score,
F(X  Î¸), the marginal distribution of X, and the conditional distribution of Y
given X and W . We index this distribution P Î¸ by Î¸, and will consider properties
of estimators for different values of Î¸, under the same marginal distribution
for X, and the same conditional distribution for Y given W and X. Given
Assumption 1, the average treatment effect is equal to




Ï„ = E Y (1) âˆ’ Y (0) = E E[Y |W = 1 X] âˆ’ E[Y |W = 0 X] 
From this equation, it can be seen that ATE does not depend on the propensity
score; it only depends on the conditional distribution of Y given W and X and
the marginal distribution of X. The average treatment effect for the treated is


Ï„t = E Y (1) âˆ’ Y (0)|W = 1


= E E[Y |W = 1 X] âˆ’ E[Y |W = 0 X]|W = 1
 


E F X  Î¸âˆ— E[Y |W = 1 X] âˆ’ E[Y |W = 0 X]




=
E F X  Î¸âˆ—

MATCHING ON THE ESTIMATED PROPENSITY SCORE

789

In contrast to the average treatment effect, Ï„, the average treatment effect for
the treated, Ï„t , depends on the propensity score, and we make this dependence
explicit by indexing Ï„t by Î¸ wherever appropriate. In particular, Ï„t = Ï„t (Î¸âˆ— ) is
the average effect of the treatment on the treated.
To derive the large sample distribution of 
Ï„N and 
Ï„tN , we invoke some additional regularity conditions. First, we extend Assumption 2 to hold for all Î¸ in
a neighborhood of Î¸âˆ— .
ASSUMPTION 4: (i) Î¸âˆ— âˆˆ int(Î˜) with Î˜ compact, X has a bounded support,
and E[XX  ] is nonsingular; (ii) F : R â†’ (0 1) is continuously differentiable with
strictly positive and bounded derivative f ; (iii) there exists a component of X that
is continuously distributed, has nonzero coefficient in Î¸âˆ— , and admits a continuous
density function conditional on the rest of X; and (iv) there exists Îµ > 0 such that,
for all Î¸ with Î¸ âˆ’ Î¸âˆ— â‰¤ Îµ, E[Y |W = w F(X  Î¸) = p] is Lipschitz-continuous in
p, var(Y |W = w F(X  Î¸) = p) is continuous in p, and there is Î´ > 0 such that
E[|Y |2+Î´ |W = w F(X  Î¸) = p] is uniformly bounded.
The case of only discrete regressors is left out from Assumption 4, but, as
noted in Abadie and Imbens (2006), it is a simple case to treat separately.
With only discrete regressors, each observation (or each treated observation
in the case of ATET) can be matched to every observation with identical estimated propensity score value in the opposite treatment arm. In this case, the
propensity score matching estimator is identical to the subclassification estimator in Cochran (1968) and Angrist (1998), and valid analytical and bootstrap
standard errors can be easily derived.
We will study the behavior of certain statistics under sequences Î¸N that are
local to Î¸âˆ— . Consider ZNi = {YNiâˆš
 WNi  XNi } with distribution given by the loÎ¸N
âˆ—
cal â€œshiftâ€ P with Î¸N = Î¸ + h/ N, where h is a conformable vector of constants. Let Î›N (Î¸|Î¸ ) be the difference between the value of the log-likelihood
function evaluated at Î¸ and the value of the log-likelihood function evaluated
at Î¸ :




Î›N Î¸|Î¸ = L(Î¸|ZN1      ZNN ) âˆ’ L Î¸ |ZN1      ZNN 
Let Î”N (Î¸) be the normalized score function, or central sequence,
1 âˆ‚
Î”N (Î¸) = âˆš
L(Î¸|ZN1      ZNN )
N âˆ‚Î¸
  
N
  
WNi âˆ’ F XNi
Î¸
1 
   f XNi
XNi   
Î¸ 
=âˆš
F XNi Î¸ 1 âˆ’ F XNi Î¸
N i=1

790

A. ABADIE AND G. W. IMBENS

Finally, let
2


f X Î¸
   XX 
IÎ¸ = E   
F X Î¸ 1âˆ’F X Î¸
be the Fisher Information Matrix for Î¸. The expectation in this equation is
taken over the marginal distribution of X, which does not depend on Î¸, so the
indexing by Î¸ solely reflects the value of Î¸ where f (X  Î¸) and F(X  Î¸) are evaluated. The following intermediate lemma derives some important regularity
properties of the propensity score model that will be needed for our derivations.
LEMMA 1: Suppose that Assumptions 3, 4(i), and 4(ii) hold. Then, under P Î¸N ,
(1)



1
Î›N Î¸âˆ— |Î¸N = âˆ’h Î”N (Î¸N ) âˆ’ h IÎ¸âˆ— h + op (1)
2

(2)

Î”N (Î¸N ) â†’ N(0 IÎ¸âˆ— )

d

where IÎ¸âˆ— is not singular, and
(3)

âˆš
N(
Î¸N âˆ’ Î¸N ) = IÎ¸âˆ’1
âˆ— Î”N (Î¸N ) + op (1)

The proof of this proposition is available in the Supplemental Material
(Abadie and Imbens (2016)). For regular parametric models, equation (1) can
be established using Proposition 2.1.2 in Bickel, Klaassen, Ritov, and Wellner (1998). Also for regular parametric models, equation (2) is derived in
the proof of Proposition 2.1.2 in Bickel et al. (1998). Equation (3) can be
established using the same set of results in combination with classical conditions for asymptotic linearity of maximum likelihood estimators (see, e.g.,
van der Vaart (1998, Theorem 5.39), Lehmann and Romano (2005, Theorem 12.4.1)).
Let EÎ¸ be the expectation operator with respect to the distributions P Î¸ . The
following assumption is a regularity condition that will be used later in this
section.
ASSUMPTION 5: EÎ¸N [r(Y W  X)|W  F(X  Î¸N )] converges to E[r(Y W  X)|
W  F(X  Î¸âˆ— )] almost surely, for any Rk+2 -to-R bounded and measurable function,
r(y w x), continuous in x, and any sequence, Î¸N â†’ Î¸âˆ— .
Primitive conditions for this assumption are provided in the Supplemental
Material.

MATCHING ON THE ESTIMATED PROPENSITY SCORE

791

Î¸N )
3.1. Large Sample Distribution for 
Ï„N (
âˆš
Ï„N âˆ’ Ï„) is based on the techOur derivation of the limit distribution of N(
niques developed in Andreou and Werker (2012) to analyze the limit distribution of residual-based statistics.
derive the
âˆš We proceed inâˆšthree steps. First, we
âˆ—

Ï„N (Î¸N ) âˆ’ Ï„) N(Î¸ âˆ’ Î¸N ) Î›N (Î¸ |Î¸N )) under
joint limit distribution of ( N(
P Î¸N .
PROPOSITION 2: Suppose that Assumptions 1â€“5 hold. Then, under P Î¸N ,
â
â›âˆš 
N 
Ï„N (Î¸N ) âˆ’ Ï„
â âˆšN(
Î¸ âˆ’Î¸ ) â 
 Nâˆ— N
Î›N Î¸ |Î¸N
â›
ââ
 â› Ïƒ2
c  IÎ¸âˆ’1
âˆ’c  h
âˆ—
0
d
 â IÎ¸âˆ’1
â†’Nâ
0
IÎ¸âˆ’1
âˆ’h â â  
âˆ— c
âˆ—



âˆ’h IÎ¸âˆ— h/2
âˆ’h c âˆ’h h IÎ¸âˆ— h
where
(4)




cov X Î¼(1 X)|F X  Î¸âˆ—


c=E
F X  Î¸âˆ—






cov X Î¼(0 X)|F X  Î¸âˆ—
  âˆ—
f X  Î¸âˆ— 
+
1âˆ’F X Î¸

All proofs for the results in this section are provided
in the Appendix.
âˆš
âˆ—
Ï„N (Î¸N ) âˆ’ Ï„), under P Î¸
Asymptotic Normality of the first component, N(
follows from
joint Normality of the last two comâˆš Proposition 1. Asymptotic
Î¸N âˆ’ Î¸N ) and Î›N (Î¸âˆ— |Î¸N ), follows from Lemma 1. Proposition 2
ponents, N(
derives the joint large sample distribution of the three components under P Î¸N .
The proof extends the martingale techniques of Abadie and Imbens (2012) to
derive the result of the proposition.
In the second step of our argument, we use Le Camâ€™s third lemma (e.g.,
van der Vaart (1998, p. 90)). Given the result of Proposition 2, Le Camâ€™s third
âˆ—
lemma implies that, under P Î¸ ,
âˆš 

Ïƒ 2 c  IÎ¸âˆ’1
N 
Ï„N (Î¸N ) âˆ’ Ï„
âˆ—
d
âˆ’c  h
âˆš

â†’N
 âˆ’1
âˆ’1
âˆ’h

I
c
I
N(Î¸N âˆ’ Î¸N )
Î¸âˆ—
Î¸âˆ—
âˆš
âˆ—
Substituting Î¸N = Î¸âˆ— + h/ N, this implies that (still under P Î¸ ), for any h âˆˆ Rk ,
âˆš 
âˆš   âˆ—

Ïƒ 2 c  IÎ¸âˆ’1
N 
Ï„N Î¸ + h/ N âˆ’ Ï„
âˆ—
d
âˆ’c  h
âˆš 
(5)

â†’
N


âˆ’1
âˆ’1
âˆ—
0

IÎ¸âˆ— c IÎ¸âˆ—
N Î¸N âˆ’ Î¸

792

A. ABADIE AND G. W. IMBENS

If equation (5) was an exact result rather than an approximation based on convergence in distribution, it would directly lead to the result of interest. In that
case, it would follow that
âˆš   âˆ—
âˆš 
âˆš 

Î¸N âˆ’ Î¸âˆ—
N 
Ï„N Î¸ + h/ N âˆ’ Ï„ | N 


= h âˆ¼ N 0 Ïƒ 2 âˆ’ c  IÎ¸âˆ’1
âˆ— c
âˆš
âˆ—
âˆ—

(see,
âˆš e.g., Goldberger (1991, p. 197)). Because
âˆšN(Î¸N âˆ’ Î¸ ) = h implies Î¸ +
âˆ—
h/ N = 
Î¸N , and thus implies that 
Ï„N (Î¸ + h/ N) = 
Ï„N (
Î¸N ) = 
Ï„N , the last
displayed equation can also be written as
âˆš
âˆš


N(
Ï„N âˆ’ Ï„)| N(
Î¸N âˆ’ Î¸) = h âˆ¼ N 0 Ïƒ 2 âˆ’ c  IÎ¸âˆ’1
âˆ— c 
Because this conditional distribution does not depend on h, this in turn implies
âˆ—
that, under P Î¸ , unconditionally,
âˆš


N(
Ï„N âˆ’ Ï„) âˆ¼ N 0 Ïƒ 2 âˆ’ c  IÎ¸âˆ’1
âˆ— c 
which is the result we are looking for: the distribution of the matching estimator based on matching on the estimated propensity score.
âˆš
A
challenge
formalizing
this
argument
is
that
convergence
of
N(
Ï„N âˆ’ Ï„)|
âˆš
N(
Î¸N âˆ’ Î¸âˆ— ) = h involves convergence in a conditioning event. To overcome
this challenge, in the third step of the argument, we employ a Le Cam discretization device, as proposed in Andreou
âˆš and Werker (2012). Consider a
grid of cubes in Rk with sides of length d/ N, for arbitrary positive d. Then
Î¸Ì„N is the discretized estimator, defined as the midpoint of the cube 
Î¸N belongs

to. If 
Î¸Nj is the jth component
of
the
k-vector
Î¸
,
then
the
jth
component
of
N
âˆš
âˆš

the k-vector Î¸Ì„N is Î¸Ì„Nj = (d/ N)[ N Î¸Nj /d], where [Â·] is the nearest integer
function. Now we can state the main result of the paper.
âˆ—

THEOREM 1: Suppose Assumptions 1â€“5 hold. Then, under P Î¸ ,
lim lim Pr
dâ†“0 Nâ†’âˆ



=

z
âˆ’âˆ

âˆš  2
âˆ’1/2 


N Ïƒ âˆ’ c  IÎ¸âˆ’1

Ï„N (Î¸Ì„N ) âˆ’ Ï„ â‰¤ z
âˆ— c

1
1
âˆš exp âˆ’ x2 dx
2
2Ï€

An
âˆš implication of Theorem 1 is that we can approximate the distribution
Ï„N (
Î¸N ) âˆ’ Ï„) by a Normal distribution with mean zero and variance
of N(
Ïƒ 2 âˆ’ c  IÎ¸âˆ’1
âˆ— c. The result in Theorem 1 indicates that the adjustment to the stan-

MATCHING ON THE ESTIMATED PROPENSITY SCORE

793

dard error of the propensity score matching estimator for first step estimation
of the propensity score is always negative, or zero in some special cases as discussed below. This implies that using matching on the estimated propensity
score, rather than on the true propensity score, to estimate ATE increases precision in large samples. As we will see later, this gain in precision from using
the estimated propensity score does not necessarily hold for the estimation of
parameters different than ATE.
Equation (4) and the fact that X and W are independent conditional on
the propensity score imply that if the covariance of X and Î¼(W  X) given
F(X  Î¸âˆ— ) and W is equal to zero, then c = 0 and first step estimation
of the
âˆš
Î¸N âˆ’ Î¸âˆ— ).
propensity score does not affect the large sample variance of N(
This would be the case if the propensity score provides no â€œdimension reduction,â€ that is, if the propensity score is a bijective function of X. In that
case, each value of the propensity score corresponds to only one value of X, so
cov(X Î¼(W  X)|W  F(X  Î¸âˆ— )) = 0 and, therefore, c = 0.
For concreteness, our derivations focus on the case of matching with replacement. However, as shown in Abadie and Imbens (2012), martingale representations analogous to the one employed in the proof of Proposition 2 exist for
alternative matching estimators (e.g., estimators that construct the matches
without using replacement). An inspection of the proof of Proposition 2 reveals
that the adjustment term, âˆ’c  IÎ¸âˆ— c, does not depend on the type of matching
employed to obtain the estimators. Therefore, the result in Theorem 1 translates easily to other matching settings. A different type of matching scheme
may change the form of Ïƒ 2 in the result of Theorem 1, but not the adjustment
term, âˆ’c  IÎ¸âˆ— c.
3.2. Large Sample Distribution for 
Ï„tN

âˆš
In this section, we consider the asymptotic distribution for N(
Ï„tN âˆ’
Ï„t (Î¸âˆ— )). The derivations are similar to those for the case of ATE, so we relegate details to the Supplemental Material. The following theorem provides
the result.
THEOREM 2: Suppose Assumptions 1â€“5 hold, and let
ct =

 
 






1
   âˆ—  E Xf X  Î¸âˆ— Î¼Ì„ 1 F X  Î¸âˆ— âˆ’ Î¼Ì„ 0 F X  Î¸âˆ— âˆ’ Ï„t
E F XÎ¸



1
   âˆ—  E cov X Î¼(1 X)|F X  Î¸âˆ—
E F XÎ¸




  âˆ—    âˆ— 
F X  Î¸âˆ—


cov X Î¼(0 X)|F X Î¸
+
f XÎ¸ 
1 âˆ’ F X  Î¸âˆ—
+

794

A. ABADIE AND G. W. IMBENS
âˆ—

Then, under P Î¸ ,
lim lim Pr
dâ†“0 Nâ†’âˆ

âˆš

 
 
âˆ‚Ï„t Î¸âˆ— âˆ’1 âˆ‚Ï„t Î¸âˆ—
N Ïƒ âˆ’c I c +
IÎ¸âˆ—
âˆ‚Î¸
âˆ‚Î¸
2
t

 âˆ’1
t Î¸âˆ— t

âˆ’1/2



Ã— 
Ï„tN (Î¸Ì„N ) âˆ’ Ï„t â‰¤ z

=

z
âˆ’âˆ

1
1
âˆš exp âˆ’ x2 dx
2
2Ï€

Notice that, in contrast to the ATE case, the adjustment for first step estimation of the propensity score for the ATET estimator may result in a decrease or an increase in the standard error. The adjustment to the standard
error of the ATET estimator will be positive, for example, if the propensity
score does not provide â€œdimension reduction,â€ so ct = 0, and âˆ‚Ï„t (Î¸âˆ— )/âˆ‚Î¸ = 0
(which will typically be the case if the average effect of the treatment varies
with the covariates, X). In contrast, if ct = 0 and âˆ‚Ï„t (Î¸âˆ— )/âˆ‚Î¸ = 0, the adjustment is negative. Like for the case of ATE, it can be shown that the adjustment term does not depend on the particular type of matching estimator of
ATET.
4. ESTIMATION OF THE ASYMPTOTIC VARIANCE
In this section, we discuss estimation of the large sample variances of ATE
and ATET adjusting for first step estimation of the propensity
score. As shown
âˆš
Ï„N âˆ’ Ï„) is
in the previous section, the asymptotic variance for N(
(6)

2
Ïƒadj
= Ïƒ 2 âˆ’ c  IÎ¸âˆ’1
âˆ— c

(7)

âˆš

N(
Ï„tN âˆ’ Ï„t ) is
 
 
âˆ‚Ï„t Î¸âˆ— âˆ’1 âˆ‚Ï„t Î¸âˆ—
2
2
 âˆ’1

Ïƒtadj = Ïƒt âˆ’ ct IÎ¸âˆ— ct +
IÎ¸âˆ—
âˆ‚Î¸
âˆ‚Î¸

and the asymptotic variance for

2
2
and Ïƒtadj
, we define estimators of each of the components of
To estimate Ïƒadj
the right-hand sides of equations (6) and (7). First, estimation of the information matrix, IÎ¸âˆ— , is standard:

2
N

Î¸N
f Xi
1




 Xi Xi 
IÎ¸âˆ— =
N i=1 F Xi
Î¸N 1 âˆ’ F Xi
Î¸N

Consider next estimation of the variances corresponding to matching on the
true propensity score, Ïƒ 2 and Ïƒt2 . For these components, we use estimators

MATCHING ON THE ESTIMATED PROPENSITY SCORE

795

that are based on those in Abadie and Imbens (2006). Let KMÎ¸ (i) be the
number of times that observation i is used as a match (when matching on
F(X  Î¸)):
(8)

KMÎ¸ (i) =

N


1[iâˆˆJM (jÎ¸)] 

j=1

and let 
ÏƒÌ„ (Wi  F(Xi Î¸âˆ— )) be an asymptotically unbiased (but not necessarily
consistent) estimator of ÏƒÌ„ 2 (Wi  F(Xi Î¸âˆ— )). The Abadie and Imbens (2006) variance estimators are
2


Ïƒ2 =

N
1 
1
(2Wi âˆ’ 1) Yi âˆ’
N i=1
M

+

N
1 
N i=1

KMÎ¸ (i)
M

2

+



2

Ï„N
Yj âˆ’ 

jâˆˆJM (i
Î¸)

2M âˆ’ 1 KMÎ¸ (i)
M
M



2

ÏƒÌ„ Wi  F Xi Î¸âˆ—

and

Ïƒt2 =

N
N 
1
Wi Yi âˆ’
2
M
N1 i=1



2

Yj âˆ’ 
Ï„tN

jâˆˆJM (i
Î¸)



N


KMÎ¸ (i) KMÎ¸ (i) âˆ’ 1
N 
2

ÏƒÌ„ Wi  F Xi Î¸âˆ— 
+ 2
(1 âˆ’ Wi )
2
N1 i=1
M
To obtain the estimator 
ÏƒÌ„ (Wi  F(Xi Î¸âˆ— )), let HL (i Î¸) be the set of units in
the same treatment arm as unit i that have the closest L values of F(X  Î¸) to
F(Xi Î¸),

HL (i Î¸) = j = 1     N : Wj = Wi 
2



1[|F(Xi Î¸)âˆ’F(Xk Î¸)|â‰¤|F(Xi Î¸)âˆ’F(Xj Î¸)|] â‰¤ L 

k:Wk =Wi

where L is generic notation for a (small) positive integer. Later, we will also
use the set HL(âˆ’i) (i Î¸), which is similarly defined but excludes i:

(âˆ’i)
HL (i Î¸) = j = 1     N : i = j Wj = Wi 

k:Wk =Wi k=i

1[|F(Xi Î¸)âˆ’F(Xk Î¸)|â‰¤|F(Xi Î¸)âˆ’F(Xj Î¸)|] â‰¤ L 

796

A. ABADIE AND G. W. IMBENS

The sets HL (i Î¸), HL(âˆ’i) (i Î¸), and JL (i Î¸) (this last one defined in Section 2)
2
2
and Ïƒtadj
. The value
will be used to estimate the different components of Ïƒadj
of L can vary for different components.
For L â‰¥ 2 (typically, L = 2), consider the following matching estimator of
ÏƒÌ„ 2 (Wi  F(Xi Î¸âˆ— )):


2

ÏƒÌ„ Wi  F Xi Î¸âˆ— =

1
Lâˆ’1


jâˆˆHL (i
Î¸N )

Yj âˆ’

1
L



2

Yk



kâˆˆHL (i
Î¸N )

2
That is, 
ÏƒÌ„ (Wi  F(Xi Î¸âˆ— )) is a local variance estimator that uses information
only from units with the same value of W as unit i and with similar values of
2
Î¸N ). Because 
Î¸N âˆ’ Î¸âˆ— converges in probability to zero, 
ÏƒÌ„ (Wi  F(Xi Î¸âˆ— ))
F(X 
becomes asymptotically unbiased as N â†’ âˆ. But because L is fixed, the vari2
2
ÏƒÌ„ (Wi  F(Xi Î¸âˆ— )) is not
ance of 
ÏƒÌ„ (Wi  F(Xi Î¸âˆ— )) does not converge to zero and 
consistent for ÏƒÌ„ 2 (Wi  F(Xi Î¸âˆ— )). The objects of interest, however, are Ïƒ 2 and
Ïƒ 2 and 
Ïƒt2 average terms that
Ïƒt2 , rather than ÏƒÌ„ 2 (Wi  F(Xi Î¸âˆ— )). The estimators 
are bounded in probability and asymptotically unbiased. As a result, as shown
Ïƒt2 are consistent for Ïƒ 2 and Ïƒt2 , respecin Abadie and Imbens (2006), 
Ïƒ 2 and 
tively.
Next consider estimation of c and ct . Notice first that




cov X Y |F X  Î¸âˆ—  W = 1




= cov X Î¼(1 X)|F X  Î¸âˆ—  W = 1




+ cov X Y âˆ’ Î¼(1 X)|F X  Î¸âˆ—  W = 1



= cov X Î¼(1 X)|F X  Î¸âˆ—




+ cov X Y âˆ’ Î¼(1 X)|F X  Î¸âˆ—  W = 1 

Using the Law of Iterated Expectations,




cov X Y âˆ’ Î¼(1 X)|F X  Î¸âˆ—  W = 1


 
 
= E X Y âˆ’ Î¼(1 X) |F X  Î¸âˆ—  W = 1




âˆ’ E X|F X  Î¸âˆ—  W = 1




Ã— E Y âˆ’ Î¼(1 X)|F X  Î¸âˆ—  W = 1


 
 
= E X Î¼(1 X) âˆ’ Î¼(1 X) |F X  Î¸âˆ—  W = 1




âˆ’ E X|F X  Î¸âˆ—  W = 1




Ã— E Î¼(1 X) âˆ’ Î¼(1 X)|F X  Î¸âˆ—  W = 1
= 0

797

MATCHING ON THE ESTIMATED PROPENSITY SCORE

Therefore,








cov X Î¼(1 X)|F X  Î¸âˆ— = cov X Y |F X  Î¸âˆ—  W = 1 

and the analogous result is valid conditional on Wi = 0:







cov X Î¼(0 X)|F X  Î¸âˆ— = cov X Y |F X  Î¸âˆ—  W = 0 
If Wi = w, cov(Xi  Î¼(w Xi )|F(Xi Î¸âˆ— )) can be estimated as



c
ov Xi  Î¼(w Xi )|F Xi Î¸âˆ—
=

1
Lâˆ’1



Xj âˆ’

jâˆˆHL (i
Î¸N )

1
L



Xk

Yj âˆ’

kâˆˆHL (i
Î¸N )

1
L



Yk 

kâˆˆHL (i
Î¸N )

for L â‰¥ 2 (typically, L = 2). If Wi = w, then



c
ov Xi  Î¼(w Xi )|F Xi Î¸âˆ—
=

1
Lâˆ’1


jâˆˆJL (i
Î¸N )

Xj âˆ’

1
L



Xk

Yj âˆ’

kâˆˆJL (i
Î¸N )

1
L



Yk 

kâˆˆJL (i
Î¸N )

also for L â‰¥ 2 (typically, L = 2). Like 
ÏƒÌ„ (Wi  F(Xi Î¸âˆ— )), the estimators
 âˆ—
c
ov(Xi  Î¼(w Xi )|F(Xi Î¸ )) are asymptotically unbiased and bounded in probability. This allows us to construct a consistent analog estimator of c that averages c
ov(Xi  Î¼(w Xi )|F(Xi Î¸âˆ— )) over the sample:



N
ov Xi  Î¼(1 Xi )|F Xi Î¸âˆ—
1  c



c=
N
F Xi
Î¸N
2

i=1






c
ov Xi  Î¼(0 Xi )|F Xi Î¸âˆ—
  
f Xi
Î¸N 
+

1 âˆ’ F Xi Î¸N
For ct , we propose separate estimators for the two components, ct1 and ct2 ,
where
 

1
ct1 =    âˆ—  E Xf X  Î¸âˆ—
E F XÎ¸





 

Ã— Î¼Ì„ 1 F X  Î¸âˆ— âˆ’ Î¼Ì„ 0 F X  Î¸âˆ— âˆ’ Ï„t 
ct2 =




1
   âˆ—  E cov X Î¼(1 X)|F X  Î¸âˆ—
E F XÎ¸




  âˆ—    âˆ— 
F X  Î¸âˆ—


cov X Î¼(0 X)|F X Î¸
+
f XÎ¸ 
1 âˆ’ F X  Î¸âˆ—

798

A. ABADIE AND G. W. IMBENS

and ct = ct1 + ct2 . The second component, ct2 , is similar to ct , and our proposed estimator for ct2 is correspondingly similar to the estimator for c:
(9)


ct2 =

N



1 
c
ov Xi  Î¼(1 Xi )|F Xi Î¸âˆ—
N1 i=1



   

F Xi
Î¸N
   c
ov Xi  Î¼(0 Xi )|F Xi Î¸âˆ—
Î¸N 
+
f Xi 

1 âˆ’ F Xi Î¸N

The first component, ct1 , involves the regression functions Î¼Ì„(w F(X  Î¸âˆ— )). We
estimate these regression functions using matching:





Î¼Ì„ 0 F Xi Î¸âˆ— =

â§
1
âª
âª
âª
âª
â¨L
1
âª
âª
âª
âª
â©L


(âˆ’i)

jâˆˆHL

Yj

if Wi = 0,

(i
Î¸N )



Yj

if Wi = 1,

jâˆˆJL (i
Î¸N )

and
â§
1 
âª
âª
Yj
âª
âª

  âˆ—  â¨ L jâˆˆJL (iÎ¸N )

Î¼Ì„ 1 F Xi Î¸ = 1

âª
âª
Yj
âª
âªL
â©

if Wi = 0,
if Wi = 1,

(âˆ’i)
jâˆˆHL (i
Î¸N )

for L â‰¥ 1 (typically, L = 1). Our proposed estimator for ct1 is

ct1 =

N

 






1 
Î¸N 
Xi f Xi
Î¼Ì„ 1 F Xi Î¸âˆ— âˆ’ 
Î¼Ì„ 0 F Xi Î¸âˆ— âˆ’ 
Ï„t 
N1 i=1

Notice that if Wi = w, the estimator of Î¼Ì„(w F(Xi Î¸âˆ— )) is an average over
HL(âˆ’i) (i 
Î¸N ), rather than over HL (i 
Î¸N ). If observation i was not excluded
to estimate Î¼Ì„(Wi  F(Xi Î¸âˆ— )), then Yi would be one of the terms of the average 
Î¼Ì„(Wi  F(Xi Î¸âˆ— )). Therefore, if observation i was not excluded, the estimaÎ¸N )(Yi âˆ’ 
tor 
ct1 would contain terms of the type Xi f (Xi
Î¼Ì„(0 F(Xi Î¸âˆ— )) âˆ’ 
Ï„t )

 âˆ—

Ï„t ) when
when Wi = 1 and terms of the type Xi f (Xi Î¸N )(Î¼Ì„(1 F(Xi Î¸ )) âˆ’ Yi âˆ’ 
Wi = 0. These terms estimate E[Xf (X  Î¸âˆ— )(Î¼(1 X) âˆ’ Î¼Ì„(0 F(X  Î¸âˆ— )) âˆ’ Ï„t )]
and E[Xf (X  Î¸âˆ— )(Î¼Ì„(1 F(X  Î¸âˆ— )) âˆ’ Î¼(0 X) âˆ’ Ï„t )], respectively, rather than
E[Xf (X  Î¸âˆ— )(Î¼Ì„(1 F(X  Î¸âˆ— )) âˆ’ Î¼Ì„(0 F(X  Î¸âˆ— )) âˆ’ Ï„t )]. To avoid this problem, we
exclude observation i for the estimation of Î¼Ì„(Wi  F(Xi Î¸âˆ— )).

MATCHING ON THE ESTIMATED PROPENSITY SCORE

799

For the remaining variance component, âˆ‚Ï„t (Î¸âˆ— )/âˆ‚Î¸, notice that
 


âˆ‚Ï„t  âˆ— 
1
Î¸ =    âˆ—  E Xf X  Î¸âˆ— Y (1) âˆ’ Y (0) âˆ’ Ï„t
âˆ‚Î¸
E F XÎ¸
=

 


1
   âˆ—  E Xf X  Î¸âˆ— Î¼(1 X) âˆ’ Î¼(0 X) âˆ’ Ï„t 
E F XÎ¸

To estimate this component, we need to estimate the regression functions
Î¼(1 X) and Î¼(0 X), which is done by matching on the covariates rather than
on the propensity score. Define the matching set (on covariates):

JLX (i) = j = 1     N : Wj = 1 âˆ’ Wi 


1[

Xi âˆ’Xk â‰¤ Xi âˆ’Xj ]

â‰¤L 

k:Wk =1âˆ’Wi

Our estimator of âˆ‚Ï„t (Î¸âˆ— )/âˆ‚Î¸ is
N



1 
1 
âˆ‚Ï„t
Î¸N (2Wi âˆ’ 1) Yi âˆ’
Ï„t 
=
Xi f Xi
Yj âˆ’ 
âˆ‚Î¸
N1 i=1
L X
jâˆˆJL (i)

Putting these results together, our estimator of the large sample variance
of the propensity score matching estimator of the average treatment effect,
adjusted for first step estimation of the propensity score, is
2
c
IÎ¸âˆ’1
= Ïƒ2 âˆ’ 
c

Ïƒadj
âˆ—

The corresponding estimator for the variance for the estimator for the average
effect for the treated is
2

Ïƒadjt
ct +
IÎ¸âˆ’1
=
Ïƒt2 âˆ’ 
ct 
âˆ—



âˆ‚Ï„t âˆ’1 
âˆ‚Ï„t

IÎ¸âˆ—
âˆ‚Î¸
âˆ‚Î¸

Consistency of these estimators for fixed L can be shown using the results in
Abadie and Imbens (2006) and the arguments employed in Section 3.
5. CONCLUSIONS AND EXTENSIONS
In this article, we derive the large sample distribution of propensity score
matching estimators for the case where the propensity score is unknown and
needs to be estimated in a first step prior to matching. We show that first step
estimation of the propensity score generally affects the asymptotic variance
of matching estimators, and derive adjustments for propensity score matching

800

A. ABADIE AND G. W. IMBENS

estimators of ATE and ATET. These results allow, for the first time, valid large
sample inference for estimators that use matching on the estimated propensity
score.
For concreteness, we frame the article within the context of estimation of
average treatment effects under the assumption that treatment assignment is
independent of potential outcomes conditional on a set of covariates, X (Assumption 1(i)). Without this assumption, the results of this article apply still to
the estimation of the â€œcontrolled comparisonâ€ parameters


E E[Y |X W = 1] âˆ’ E[Y |X W = 0]
and


E[Y |W = 1] âˆ’ E E[Y |X W = 0]|W = 1 
which have the same form as ATE and ATET parameters but lack a causal interpretation in the absence of Assumption 1(i). Controlled contrasts are the
building blocks of Oaxacaâ€“Blinder-type decompositions, commonly applied
in economics (Oaxaca (1973), Blinder (1973), DiNardo, Fortin, and Lemieux
(1996)). The ideas and results in this article can easily be applied to other contexts where it is required to adjust for differences in the distribution of covariates between two samples. An important example is estimation with missing
data when missingness is random conditional on a set of covariates (see, e.g.,
Little and Rubin (2002), Wooldridge (2007)).
APPENDIX
Before proving Proposition 2, we introduce some additional notation. Using
Ï„N (Î¸) can be written as
the definition of KMÎ¸ (i) in (8), the estimator 

Ï„N (Î¸) =

N
1 
KMÎ¸ (i)
Yi 
(2Wi âˆ’ 1) 1 +
N i=1
M

Define Î¼Ì„Î¸ (w p) = EÎ¸ [Y |W = w F(X  Î¸) = p], where EÎ¸ is the expectation
operator under P Î¸ ,
N






1  
Î¼Ì„Î¸ 1 F Xi Î¸ âˆ’ Î¼Ì„Î¸ 0 F Xi Î¸ âˆ’ Ï„
DN (Î¸) = âˆš
N i=1
N



1 
KMÎ¸ (i) 
Yi âˆ’ Î¼Ì„Î¸ Wi  F Xi Î¸
+âˆš
(2Wi âˆ’ 1) 1 +
M
N i=1

MATCHING ON THE ESTIMATED PROPENSITY SCORE

801

and
N
1 
(2Wi âˆ’ 1)
RN (Î¸) = âˆš
N i=1







1 
Î¼Ì„Î¸ 1 âˆ’ Wi  F Xj Î¸ 
Ã— Î¼Ì„Î¸ 1 âˆ’ Wi  F Xi Î¸ âˆ’
M jâˆˆJ (i)
M

Now the normalized estimator can be written as
âˆš 

N 
Ï„N (Î¸) âˆ’ Ï„ = DN (Î¸) + RN (Î¸)
PROOF OF PROPOSITION 2: It can be seen that the result of Lemma S.1 in
the Supplemental Material (Abadie and Imbens (2016)) holds uniformly in Î¸
p
for Î¸ âˆ’ Î¸âˆ— â‰¤ Îµ. This implies RN (Î¸N ) â†’ 0. Therefore, in order to prove the
result in the proposition, it suffices to prove that, under P Î¸N ,
â
D (Î¸ )
âˆš N N
â N(
Î¸ âˆ’ Î¸ )â 
 Nâˆ— N
Î›N Î¸ |Î¸N
â›
 â› Ïƒ2
0
d
 â IÎ¸âˆ’1
â†’Nâ
0
âˆ— c

âˆ’h IÎ¸âˆ— h/2
âˆ’h c
â›

c  IÎ¸âˆ’1
âˆ—
IÎ¸âˆ’1
âˆ—
âˆ’h

âˆ’c  h

ââ

âˆ’h â â  
h IÎ¸âˆ— h

By Lemma 1, under P Î¸N ,


1
Î›N Î¸âˆ— |Î¸N = âˆ’h Î”N (Î¸N ) âˆ’ h IÎ¸âˆ— h + op (1)
2
and
âˆš
N(
Î¸N âˆ’ Î¸N ) = IÎ¸âˆ’1
âˆ— Î”N (Î¸N ) + op (1)
Therefore, it suffices to prove that, under P Î¸N ,
(A.1)

DN (Î¸N )
Î”N (Î¸N )

d

â†’N

0
Ïƒ2

c
0

c
IÎ¸âˆ—



To prove (A.1), we extend the martingale representation of matching estimators (Abadie and Imbens (2012)) to allow for estimation of the propensity

802

A. ABADIE AND G. W. IMBENS

score. Consider the linear combination CN = z1 DN (Î¸N ) + z2 Î”N (Î¸N ):
N

 


 


1 
Î¸N âˆ’ Î¼Ì„Î¸N 0 F XNi
Î¸N âˆ’ Ï„
CN = z1 âˆš
Î¼Ì„Î¸N 1 F XNi
N i=1

KMÎ¸N (i)
1 
+ z1 âˆš
(2WNi âˆ’ 1) 1 +
M
N i=1
 



Î¸N
Ã— YNi âˆ’ Î¼Ì„Î¸N WNi  F XNi
 

N

 

W
âˆ’
F
X
Î¸
1
Ni
N
Ni

 
 f XNi
XNi  
Î¸N 
+ z2 âˆš
F XNi Î¸N 1 âˆ’ F XNi Î¸N
N i=1
N

We analyze CN using martingale methods. First, notice that
CN =

3N


Î¾Nk 

k=1

where

 


 


1 
Î¾Nk = z1 âˆš Î¼Ì„Î¸N 1 F XNk
Î¸N âˆ’ Î¼Ì„Î¸N 0 F XNk
Î¸N âˆ’ Ï„
N
 

1 
Î¸N
+ z2 âˆš E XNk |F XNk
N
 

 

Î¸N
WNk âˆ’ F XNk

 
 f XNk
Î¸N
Ã—  
F XNk Î¸N 1 âˆ’ F XNk Î¸N
for 1 â‰¤ k â‰¤ N,


 
1 
Î¾Nk = z2 âˆš XNkâˆ’N âˆ’ E XNkâˆ’N |F XNkâˆ’N
Î¸N
N
 
  


Î¸N f XNkâˆ’N
Î¸N
WNkâˆ’N âˆ’ F XNkâˆ’N
 

 

Ã—
F XNkâˆ’N
Î¸N 1 âˆ’ F XNkâˆ’N
Î¸N
KMÎ¸N (k âˆ’ N)
1
+ z1 âˆš (2WNkâˆ’N âˆ’ 1) 1 +
M
N
 



Î¸N
Ã— Î¼(WNkâˆ’N  XNkâˆ’N ) âˆ’ Î¼Ì„Î¸N WNkâˆ’N  F XNkâˆ’N

MATCHING ON THE ESTIMATED PROPENSITY SCORE

803

for N + 1 â‰¤ k â‰¤ 2N, and
KMÎ¸N (k âˆ’ 2N)
1
Î¾Nk = z1 âˆš (2WNkâˆ’2N âˆ’ 1) 1 +
M
N


Ã— YNkâˆ’2N âˆ’ Î¼(WNkâˆ’2N  XNkâˆ’2N ) 

Î¸N 
for 2N + 1 â‰¤ k â‰¤ 3N. Consider the Ïƒ-fields FNk = Ïƒ{WN1      WNk  XN1



    XNk Î¸N } for 1 â‰¤ k â‰¤ N, FNk = Ïƒ{WN1      WNN  XN1 Î¸N      XNN Î¸N 
XN1      XNkâˆ’N } for N + 1 â‰¤ k â‰¤ 2N, and FNk = Ïƒ{WN1      WNN 
XN1      XNN  YN1      YNkâˆ’N } for 2N + 1 â‰¤ k â‰¤ 3N. Then,

 i



Î¾Nj  FNi  1 â‰¤ i â‰¤ 3N

j=1

is a martingale for each N â‰¥ 1. Therefore, the limiting distribution of CN can
be studied using a Martingale Central Limit Theorem (e.g., Theorem 35.12
in Billingsley (1995, p. 476); importantly, notice that this theorem allows that
the probability space varies with N). Because of Assumption 4, and because
KMÎ¸ (i) has uniformly bounded moments (see Abadie and Imbens (2016)), it
follows that
3N




EÎ¸N |Î¾Nk |2+Î´ â†’ 0

for some Î´ > 0

k=1

Lindebergâ€™s condition in Billingsleyâ€™s theorem follows easily from the last
equation (Lyapunovâ€™s condition). As a result, we obtain that, under P Î¸N ,


d
CN â†’ N 0 Ïƒ12 + Ïƒ22 + Ïƒ32 
where
Ïƒ12 = plim

N


 2

EÎ¸N Î¾Nk
|FNkâˆ’1 

k=1

Ïƒ = plim
2
2

2N


 2

EÎ¸N Î¾Nk
|FNkâˆ’1 

k=N+1

Ïƒ32 = plim

3N

k=2N+1

 2

EÎ¸N Î¾Nk
|FNkâˆ’1 

804

A. ABADIE AND G. W. IMBENS

Assumption 5 implies
 





2 
Ïƒ12 = z12 E Î¼Ì„ 1 F X  Î¸âˆ— âˆ’ Î¼Ì„ 0 F X  Î¸âˆ— âˆ’ Ï„


f 2 X  Î¸âˆ—



+ z2 E   âˆ— 
F X Î¸ 1 âˆ’ F X  Î¸âˆ—


  âˆ—      âˆ— 
Ã— E X|F X Î¸ E X |F X Î¸
z2 
Expectations of the sums of terms involving (1 + KMÎ¸N (i)/M)2 can be calculated as in Abadie and Imbens (2016). We obtain




  âˆ— 
f 2 X  Î¸âˆ—

 var X|F X Î¸
z2
Ïƒ = z E   âˆ— 
F X Î¸ 1 âˆ’ F X  Î¸âˆ—





 
var Î¼(0 X)|F X  Î¸âˆ—
var Î¼(1 X)|F X  Î¸âˆ—
2




+
+ z1 E
F X  Î¸âˆ—
1 âˆ’ F X  Î¸âˆ—
2
2


2

1
+z
E
2M
2
1


  âˆ—

  âˆ— 
1

 âˆ’F X Î¸
var Î¼(1 X)|F X Î¸
F X  Î¸âˆ—




1
  âˆ—  âˆ’ 1 âˆ’ F X  Î¸âˆ—
1âˆ’F X Î¸




Ã— var Î¼(0 X)|F X  Î¸âˆ—
+ z12

1
E
2M




cov X Î¼(1 X)|F X  Î¸âˆ—


+ 2z E
F X  Î¸âˆ—




  âˆ—
cov X Î¼(0 X)|F X  Î¸âˆ—


+
f X Î¸ z1 
1 âˆ’ F X  Î¸âˆ—

2

Here we use the fact that, conditional on the propensity score, X is independent of W . Finally, notice that
N
2
KMÎ¸N (i)
1 
1+
N i=1
M




Ã— var(Yi |Wi  Xi ) âˆ’ E var(Yi |Wi  Xi )|Wi  F Xi Î¸N

MATCHING ON THE ESTIMATED PROPENSITY SCORE

805

is a sum of martingale differences
2

Î¶Ni =

KMÎ¸N (i)
1
1+
N
M




Ã— var(Yi |Wi  Xi ) âˆ’ E var(Yi |Wi  Xi )|Wi  F Xi Î¸N

with respect to the filtration FNi = Ïƒ{W1      WN  X1 Î¸N      X1 Î¸N 
X1      Xi }. As a result, we obtain that, for i > j, E[Î¶Ni Î¶Nj |FNiâˆ’1 ] =
E[Î¶Ni |FNiâˆ’1 ]Î¶Nj = 0. Therefore, using the Law of Iterated Expectations to
eliminate the cross-products, we obtain

E

N
1 
Î¶Ni
N i=1

2 


 N

1
2
= 2E
â†’ 0
Î¶Ni
N
i=1

Therefore,
N
2



KMÎ¸N (i)
1 
E var(Yi |Wi  Xi )|Wi  F Xi Î¸N
Ïƒ = z plim
1+
N i=1
M



E var(Y |X W = 1)|F X  Î¸âˆ—
2


= z1 E
F X  Î¸âˆ—


 
E var(Y |X W = 0)|F X  Î¸âˆ—


+
1 âˆ’ F X  Î¸âˆ—
2
3

2
1



1
  âˆ—  âˆ’ F X  Î¸âˆ—
F XÎ¸


  âˆ— 
Ã— E var(Y |X W = 1)|F X Î¸
+ z12

1
E
2M




1
  âˆ—  âˆ’ 1 âˆ’ F X  Î¸âˆ—
1âˆ’F X Î¸




Ã— E var(Y |X W = 0)|F X  Î¸âˆ— 
+ z12

1
E
2M

Collecting terms and applying the fact that W is independent of X given
F(X  Î¸), we obtain
Ïƒ12 + Ïƒ22 + Ïƒ32 = z12 Ïƒ 2 + z2 IÎ¸âˆ— z2 + 2z2 cz1 

806

A. ABADIE AND G. W. IMBENS

Hence, by the Martingale Central Limit Theorem and the Cramerâ€“Wold device, under P Î¸N ,
DN (Î¸N )
Î”N (Î¸N )

d

â†’N

0
Ïƒ2

c
0

c
IÎ¸âˆ—



proving (A.1) and thus Proposition 2.

Q.E.D.

PROOF OF THEOREM 1: Given our preliminary results, Theorem 1 follows
from Andreou and Werker (2012).
Q.E.D.
The proof of Theorem 2 can be found in the Supplemental Material.
REFERENCES
ABADIE, A. (2005): â€œSemiparametric Difference-in-Differences Estimators,â€ Review of Economic
Studies, 72 (1), 1â€“19. [782]
ABADIE, A., AND G. W. IMBENS (2006): â€œLarge Sample Properties of Matching Estimators for
Average Treatment Effects,â€ Econometrica, 74 (1), 235â€“267. [781,784,785,789,795,796,799]
(2008): â€œOn the Failure of the Bootstrap for Matching Estimators,â€ Econometrica, 76
(6), 1537â€“1557. [782]
(2011): â€œBias-Corrected Matching Estimators for Average Treatment Effects,â€ Journal
of Business and Economic Statistics, 29 (1), 1â€“11. [784,787]
(2012): â€œA Martingale Representation for Matching Estimators,â€ Journal of the American Statistical Association, 107 (498), 833â€“843. [788,791,793,801]
(2016): â€œSupplement to â€˜Matching on the Estimated Propensity Scoreâ€™,â€ Econometrica
Supplemental Material, 84, http://dx.doi.org/10.3982/ECTA11293. [786,790,801,803,804]
ANDREOU, E., AND B. J. M. WERKER (2012): â€œAn Alternative Asymptotic Analysis of ResidualBased Statistics,â€ Review of Economics and Statistics, 94 (1), 88â€“99. [788,791,792,806]
ANGRIST, J. D. (1998): â€œEstimating the Labor Market Impact of Voluntary Military Service Using
Social Security Data on Military Applicants,â€ Econometrica, 66 (2), 249â€“288. [789]
ANGRIST, J. D., AND G. M. KUERSTEINER (2011): â€œCausal Effects of Monetary Shocks: Semiparametric Conditional Independence Tests With a Multinomial Propensity Score,â€ Review of
Economics and Statistics, 93 (3), 725â€“747. [782]
BICKEL, P. J., C. A. KLAASSEN, Y. RITOV, AND J. A. WELLNER (1998): Efficient and Adaptive
Estimation for Semiparametric Models. New York: Springer. [790]
BILLINGSLEY, P. (1995): Probability and Measure. New York: Wiley. [803]
BLINDER, A. S. (1973): â€œWage Discrimination: Reduced Form and Structural Estimates,â€ Journal
of Human Resources, 8 (4), 436â€“455. [800]
COCHRAN, W. G. (1968): â€œThe Effectiveness of Adjustment by Subclassification in Removing Bias
in Observational Studies,â€ Biometrics, 24 (2), 295â€“313. [789]
DAWID, A. P. (1979): â€œConditional Independence in Statistical Theory,â€ Journal of the Royal Statistical Society, Series B, 41 (1), 1â€“31. [783]
DEHEJIA, R., AND S. WAHBA (1999): â€œCausal Effects in Nonexperimental Studies: Reevaluating
the Evaluation of Training Programs,â€ Journal of the American Statistical Association, 94 (448),
1053â€“1062. [782,787]
DINARDO, J., N. M. FORTIN, AND T. LEMIEUX (1996): â€œLabor Market Institutions and the Distribution of Wages, 1973â€“1992: A Semiparametric Approach,â€ Econometrica, 64 (5), 1001â€“1044.
[800]
GOLDBERGER, A. S. (1991): A Course in Econometrics. Cambridge, MA: Harvard University
Press. [792]

MATCHING ON THE ESTIMATED PROPENSITY SCORE

807

HAHN, J. (1998): â€œOn the Role of the Propensity Score in Efficient Semiparametric Estimation
of Average Treatment Effects,â€ Econometrica, 66 (2), 315â€“331. [781,783]
HECKMAN, J., H. ICHIMURA, AND P. TODD (1997): â€œMatching as an Econometric Evaluation
Estimator: Evidence From a Job Training Programme,â€ Review of Economic Studies, 64 (4),
605â€“654. [782]
(1998): â€œMatching as an Econometric Evaluation Estimator,â€ Review of Economic Studies, 65 (2), 261â€“294. [781,782]
HIRANO, K., G. W. IMBENS, AND G. RIDDER (2003): â€œEfficient Estimation of Average Treatment
Effects Using the Estimated Propensity Score,â€ Econometrica, 71 (4), 1161â€“1189. [782]
IMBENS, G. (2004): â€œNonparametric Estimation of Average Treatment Effects Under Exogeneity:
A Review,â€ Review of Economics and Statistics, 86 (1), 1â€“29. [781]
IMBENS, G., AND J. WOOLDRIDGE (2009): â€œRecent Developments in the Econometrics of Program Evaluation,â€ Journal of Economic Literature, 47 (1), 5â€“86. [781]
KHAN, S., AND E. TAMER (2010): â€œIrregular Identification, Support Conditions, and Inverse
Weight Estimation,â€ Econometrica, 78 (6), 2021â€“2042. [783]
KIEFER, N. M., AND T. J. VOGELSANG (2005): â€œA New Asymptotic Theory for HeteroskedasticityAutocorrelation Robust Tests,â€ Econometric Theory, 21, 1130â€“1164. [784]
LEHMANN, E. L., AND J. P. ROMANO (2005): Testing Statistical Hypothesis. New York: Springer.
[790]
LITTLE, R. J., AND D. B. RUBIN (2002): Statistical Analysis With Missing Data (Second Ed.). Hoboken, NJ: Wiley-Interscience. [800]
NEWEY, W. K., AND D. MCFADDEN (1994): â€œLarge Sample Estimation and Hypothesis Testing,â€
in Handbook of Econometrics, Vol. 4, ed. by R. F. Engle and D. McFadden. Amsterdam: Elsevier Science. [782]
OAXACA, R. (1973): â€œMaleâ€“Female Wage Differentials in Urban Labor Markets,â€ International
Economic Review, 14 (3), 693â€“709. [800]
ROSENBAUM, P., AND D. B. RUBIN (1983): â€œThe Central Role of the Propensity Score in Observational Studies for Causal Effects,â€ Biometrika, 70 (1), 41â€“55. [781,783,784,786]
RUBIN, D. B. (1974): â€œEstimating Causal Effects of Treatments in Randomized and NonRandomized Studies,â€ Journal of Educational Psychology, 66 (5), 688â€“701. [783,784]
SMITH, J., AND P. TODD (2005): â€œDoes Matching Overcome LaLondeâ€™s Critique of Nonexperimental Estimators?â€ Journal of Econometrics, 125 (1â€“2), 305â€“353. [782]
VAN DER VAART, A. (1998): Asymptotic Statistics. New York: Cambridge University Press. [790,
791]
WOOLDRIDGE, J. M. (2007): â€œInverse Probability Weighted Estimation for General Missing Data
Problems,â€ Journal of Econometrics, 141 (2), 1281â€“1301. [782,800]
YATCHEW, A. (1997): â€œAn Elementary Estimator of the Partial Linear Model,â€ Economics Letters,
75, 135â€“143. [784]

John F. Kennedy School of Government, 79 John F. Kennedy Street, Cambridge,
MA 02138, U.S.A. and NBER; alberto_abadie@harvard.edu
and
Stanford Graduate School of Business, 655 Knight Way, Stanford, CA 943057298, U.S.A. and NBER; imbens@stanford.edu.
Co-editor Elie Tamer handled this manuscript.
Manuscript received December, 2012; final revision received August, 2015.

