ISSN 1471-0498

DEPARTMENT OF ECONOMICS
DISCUSSION PAPER SERIES

EFFICIENT PROPENSITY SCORE REGRESSION
ESTIMATORS OF MULTI-VALUED TREATMENT EFFECTS
FOR THE TREATED

Ying-Ying Lee

Number 738
January 2015

Manor Road Building, Manor Road, Oxford OX1 3UQ

Efficient propensity score regression estimators of multi-valued
treatment effects for the treated
Ying-Ying Lee

∗

University of Oxford †

January 5, 2015

Abstract
We study the role of the propensity scores in estimating treatment effects for the treated
with a multi-valued treatment. Assume assignment to one of the multiple treatments is
random given observed characteristics. Valid causal comparisons for the subpopulation who
has been treated a particular treatment level are based on two propensity scores — one for
the treated level and one for the counterfactual level. In contrast to the binary treatment
case, these two propensity scores do not add up to one. This is the key feature that allows
us to distinguish different roles of the propensity scores and to provide new insight in wellknown paradoxes in the binary treatment effect and missing data literature: We formally
show that knowledge of the propensity score for the treated level decreases the semiparametric efficiency bound, regardless of knowledge of the propensity score for the counterfactual
level. We propose efficient kernel regression estimators that project on a nonparametrically
estimated propensity score for the counterfactual level and the true propensity score for the
treated level. A surprising result is implied for the binary treatment effect for the treated:
when the propensity scores are known, using one estimated propensity score is not efficient.
Our efficient estimator regresses on a normalized propensity score that utilizes the information contained in the nonparametrically estimated and the true propensity scores.
Keywords: propensity score, multi-valued treatment, semiparametric efficiency bound, unconfoundedness, generated regressor
∗

I thank Bryan Gramham for suggesting the idea. I thank Jack Porter and Keisuke Hirano for helpful comments and discussion.
I also thank seminar participants in Academia Sinica and 2014 Annual Meeting of the Midwest Econometrics Group. All errors are
mine.
†

Department of Economics, University of Oxford. Manor Road Building, Manor Road, OX1 3UQ, United Kingdom. E-mail:
ying-ying.lee@economics.ox.ac.uk Website: https://sites.google.com/site/yyleelilian/

1

1

Introduction

Matching is a widely-used program evaluation estimation method in the economics and statistics literatures. Instead of matching on a rich set of observed characteristics, Rosenbaum and
Rubin (1983) propose a method based on the propensity score, defined as the conditional
probability of receiving a treatment given the observables. This paper studies the role of the
propensity score in estimating treatment effects for the treated with a multi-valued treatment.
The treated is the subpopulation who has received a particular treatment level. The average
treatment effect for the treated reveals the change in the average outcome of the treated
subpopulation if their treatment is switched from the treated level they have received to a
counterfactual level. In many cases of interest, treatments take on more than two values.
For example, participants in active labor market programs often receive different periods or
types of training, such as wage subsidy, vocational training classes, or apprenticeships with
local employers. Policy makers might be interested in what the average wage for the subsidy
recipients would have been if they counterfactually had participated in training classes or
served some apprenticeships (Lechner, 2002; Frölich, 2004b; Cattaneo, 2010, for example).
We examine the role of propensity scores in multi-valued treatment effects for the treated
from three perspectives: identification, semiparametric efficiency, and nonparametric regression estimation. Our results formally generalize the binary treatment literature and provide
new insight in well-known paradoxes. The multi-valued treatment setup enables the investigation of different roles played by the propensity score for the treated level and the propensity
score for the counterfactual level, which are the conditional probabilities of being treated at
the treated level and at the counterfactual level, respectively. In the binary treatment case,
the propensity score of the treatment group and that of the control group always add up to
one. The multi-valued treatment framework provides a general viewpoint to circumvent the
inconvenient fact that the two propensity scores of a binary treatment are degenerated to
one propensity score. There are three main findings:
(i) (Identification) Theorem 1 provides equivalent representations of weak unconfoundedness in terms of the propensity scores. By the equivalent representation, we can use the
propensity scores to construct the coarsest strata where a causal comparison between
two potential outcomes is valid.
(ii) (Semiparametric efficiency bound) Knowledge of the propensity score for the counterfactual level has no effect on the semiparametric efficiency bounds of both the treatment
effects for the treated and for the population. Knowledge of the propensity score for the
treated level decreases the semiparametric efficiency bound.
(iii) (Regression estimator) When the propensity scores are known, the efficient propensity

2

score regression estimator uses the nonparametrically estimated propensity score for
the counterfactual treatment level and the true propensity score for the treated level
as generated regressors. The nonparametric propensity score estimator recovers the
information of pretreatment variables on the outcome distribution that is not captured
by the propensity score.
We consider a multi-valued treatment variable T taking values on a finite discrete set.
Following the Roy (1951)-Rubin (1974) model, assume there exists a set of potential outcomes Y (t) corresponding to each treatment level t. We only observe one of the potential
outcomes Yi = Yi (t) if Ti = t for each individual i. Other potential outcomes Yi (t0 ) for
t0 6= t are unobserved.

1

The object of interest is the average treatment effect for the treated

(ATT) E[Y (t) − Y (t0 )|T = t0 ], the average causal effect of exposing the subpopulation who
has received treatment level t0 to a counterfactual level t. Assume unconfoundedness: the
treatment variable is independent of the potential outcomes given pretreatment variables X.
Denote the propensity score for treatment level t to be Pt (X) ≡ P rob(T = t|X).
The result (i) implies that adjusting for two propensity scores (Pt (X), Pt0 (X)) or a normalized one Pt (X)/(Pt (X) + Pt0 (X)) removes all biases associated with differences in the
pretreatment variables. The identification is first shown by Lechner (2001):


E[Y (t)|T = t0 ] = E E[Y |T = t, Pt (X), Pt0 (X)] T = t0
 


Pt (X)
0
T =t .
= E E Y |T = t,
Pt (X) + Pt0 (X)

(1)

Theorem 1 further generalizes the propensity score methodology in Rosenbaum and Rubin
(1983) to a multi-valued treatment and extends Imbens (2000) and Lechner (2001). Our
equivalent representation is stronger than existing results in the sense that the necessary
and sufficient conditions provide the coarsest strata where the causal inference is valid. One
of the important motivation of the propensity score method is to reduce the dimension of
adjusting for pretreatment variables. So Theorem 1 forms a basis for matching, stratification,
or subclassification estimation and can be applied to the extensive literature of Rosenbaum
and Rubin (1983), Heckman, Ichimura, and Todd (1998), Dehejia and Wahba (2002), Lechner
(2002), Imai and van Dyk (2004), Frölich (2004a), Abadie and Imbens (2012), among others.
We focus on nonparametric regression estimation introduced by Heckman, Ichimura, and
Todd (1998).
The proposed efficient regression estimators based on the identification (1) consists of
three steps: In the first step, the propensity scores (Pt (X), Pt0 (X)) and Pt (X)/(Pt (X) +
Pt0 (X)) are estimated nonparametrically as generated regressors. The second step is a non1

The handbook chapter by Heckman, LaLonde, and Smith (1999), Heckman and Vytlacil (2007), and Imbens
and Wooldridge (2009) provide comprehensive review and discussions on the program evaluation literature.

3

parametric kernel regression of the outcome variable given the generated regressors for the
subgroup who has received the counterfactual level t. The third step is a sample analog that
sums out the propensity scores. We derive the limiting properties using the result of partial
means with generated regressors in Lee (2014). Our propensity score regression estimator for
a binary case inherits the matching estimator introduced by Heckman, Ichimura, and Todd
(1998), where the generated regressor is only one propensity score P̂1 (X),
The result (ii) suggests that we should make use of the true propensity score for the
treated level Pt0 (X) to improve the asymptotic precision. The result (iii) suggests to use the
nonparametrically estimated propensity score for the counterfactual level t P̂t (X), although
its true function Pt (X) is known. That is, when the propensity scores are known, our efficient
regression estimators project on the generated regressors (P̂t (X), Pt0 (X)) or the normalized
one P̂t (X)/(P̂t (X) + Pt0 (X)). The general insight to construct an efficient estimator is to
utilize knowledge of Pt0 (X) and use a nonparametrically estimated P̂t (X) to recover the
information of the pretreatment variables on the outcome that is lost in projection on the
propensity scores.
We contribute an efficient regression estimator for E[Y (1)|T = 0] that projects on the
normalized propensity score P̂1 (X)/(P̂1 (X) + P0 (X)). It might be surprising that when
the propensity score is known, solely regressing on one nonparametrically estimated propensity score P̂1 (X) as in Heckman, Ichimura, and Todd (1998) is not efficient. Intuitively,
this is because P̂1 (X) + P̂0 (X) = 1. Regressing on P̂1 (X) is equivalent to regressing on
P̂1 (X)/(P̂1 (X) + P̂0 (X)), which is less efficient than regressing on P̂1 (X)/(P̂1 (X) + P0 (X)).
Our result responds to Hahn (1998) and Heckman, Ichimura, and Todd (1998) on the question: Is it better to match on P1 (X) or X if you know P1 (X)? It has long been a paradox
in the literature that using the estimated propensity score is more efficient than using the
true one (Hahn, 1998; Imai and van Dyk, 2004; Hirano, Imbens, and Ridder, 2003; Graham,
2011; Abadie and Imbens, 2012). The binary treatment literature has discussed intuition or
examples to understand the paradox, mostly from a view of GMM or inverse propensity score
weighting estimation. We offer a new theoretical explanation from a view of regression estimation: the nonparametrically estimated propensity score recovers the relationship between
the pretreatment variables and the outcome.
We contribute to the program evaluation literature by providing an efficient propensity
score regression estimator for the ATT. The program evaluation estimators that involve
estimated propensity scores as regressors or matching variables are less developed than alternative estimators, such as the propensity score weighting estimators in Hirano, Imbens, and
Ridder (2003). Abadie and Imbens (2012) derive the limiting distribution of propensity score
matching estimators when the propensity score is estimated parametrically in a first step.
There is a recent literature of nonparametric regression with generated regressor. Hahn and

4

Ridder (2013) show that the propensity score regression estimators of Heckman, Ichimura,
and Todd (1998) are efficient when the propensity is not known. Complementary to the
theoretical finding in Hahn and Ridder (2013), Mammen, Rothe, and Schienle (2014), and
Lee (2014) provide concrete estimators and the limit theory. In this paper, we work on a
general multi-valued treatment framework and further consider the efficient estimators when
the propensity scores are known.
The multi-valued treatment effects for the population have been studied in Imbens (2000);
Lechner (2001); Imai and van Dyk (2004); Frölich (2004b); Cattaneo (2010); among others.
Because the treated subpopulation is defined by a treatment level, identification and estimation of the causal effect for the treated requires multiple treatment indicators and propensity
scores for the relevant levels. Cattaneo (2010) calculates the semiparametric efficiency bound
for the causal effects for the population. Building on Cattaneo (2010), we calculate the bounds
for the cases when the propensity scores are unknown and known, for the treated and for the
population.
Our discussion focuses on the mean E[Y (t)|T = t0 ] for ease of exposition, but our results
are general for distributional features of Y (t) for the treated with value t0 . The identification,
semiparametric efficiency bound, and the estimation theory for the quantile treatment effects
for the treated are implied. In Section 4.6, by extending the results to the Hadamarddifferentiable functionals of the partial mean process, we are able to provide the limiting
distribution for estimating common inequality measures and various distributional structural
features; for example, the Lorenz curves and the Gini coefficients (Bhattacharya, 2007; Rothe,
2010; Firpo and Pinto, 2011; Donald, Hsu, and Barrett, 2012; Chernozhukov, Fernández-Val,
and Melly, 2013; Donald and Hsu, 2014).
The paper is organized as follows. Section 2 presents identification based on the propensity
scores. Section 3 presents the semiparametric efficiency bounds for two cases: unknown and
known propensity scores. In Section 4, we introduce the efficient propensity score regression
estimators. The Appendix collects the proofs, the notations, and the regularity conditions
under which the estimators are root-n consistent and reach the efficiency bounds.

2

Identification

This section discusses identification of multi-valued treatment effects for the treated by an
unconfoundedness assumption. Theorem 1 provides equivalent representations of weak unconfoundedness in terms of the propensity scores. The merit of the equivalent representations
is to provide both identification of the average causal effects and the coarsest strata for causal
comparison. We study direct causal comparison for a multi-valued treatment in Section 2.1.
In Section 2.2, we compare our results with the binary treatment effects and the multi-valued

5

treatment effects for the population.
Let the support of the multi-valued treatment variable T be T = {1, 2, ..., J} a finite
discrete set with some fixed positive integer J. Let Dt be the indicator of treatment assignment to level t, i.e., if T = t, Dt = 1; otherwise, Dt = 0. The treatment assignment
P
P
is mutually exclusive to multiple states in T , so t∈T Dt = 1 and T = t∈T t × Dt . We
P
only observe one of the potential outcomes Yi = t∈T Yi (t) × Dt for each individual i. Our
dataset consists of (Yi , Xi , Ti , D1i , ..., DJi ) for i = 1, ..., n. The key assumption maintained
throughout this paper is that treatment assignment is exogenous conditional on pretreatment
variables X or unconfoundedness, also known as selection on observables or conditional independence assumption. The symbols ⊥ and | denote statistical independence and conditioning
respectively.
Assumption 1 (Weak unconfoundedness)
Consider any treatment level t ∈ T . T ⊥ Y (t)|X; equivalently Dt0 ⊥ Y (t) X, for all t0 ∈ T .
Under Weak unconfoundedness Assumption 1, we can identify the conditional mean of Y (t)
for those who are treated at t0 ,
E[Y (t)|T = t0 , X] = E[Y (t)|Dt0 = 1, X] = E[Y (t)|Dt = 1, X] = E[Y |Dt = 1, X].

(2)

To derive the identification by adjusting for X in (2), there is no difference in using the
treatment variable T or the equivalent expression based on the treatment indicator Dt0 in
Assumption 1. But to adjust for the propensity scores, it turns out to be essential to express
Weak unconfoundedness Assumption 1 in terms of Dt0 . The following Theorem 1 shows that
we do not need to adjust for the entire set of the propensity scores. Instead, we only need
to adjust for the propensity scores for relevant treatment levels, captured by the treatment
indicators.
We need a common support assumption, also known as overlap.
Assumption 2 (Overlap)
The propensity score 0 < Pt (x) ≡ P rob(T = t|X = x) < 1, for all x ∈ X and t ∈ T .
It is commonly assumed Strong unconfoundedness T ⊥ {Y (t)}t∈T |X in the treatment effect
literature, for example, Rosenbaum and Rubin (1983), Lechner (2001), Hirano, Imbens, and
Ridder (2003). The joint independence of the potential outcomes is stronger than what is
needed for our identification results. Our results can be easily modified under Strong unconfoundedness. One of the main results of Rosenbaum and Rubin (1983) is that if a treatment
assignment is strongly ignorable — the combination of Strong unconfoundedness and overlap
assumptions — given X, then it is strongly ignorable given any balancing score. A balancing
score is a function of X that is finer than the propensity score in the sense that the propensity score can be expressed as a function of a balancing score. We build on and generalize

6

Rosenbaum and Rubin (1983)’s result to a multi-valued treatment under weakly ignorable
assumption. Theorem 1 provides equivalent representations of Weak unconfoundedness Assumption 1, i.e., the necessary and sufficient conditions, based on the propensity scores. We
first define the normalized propensity score to be the conditional probability of receiving a
treatment level given X when the treatment variable is restricted to take values on a subset
of T .
Definition (Normalized propensity score)
For any S ⊆ T and t ∈ S, define the normalized propensity score
Pt (X)
= P rob(T = t|X, T ∈ S).
s∈S Ps (X)

Pt|S (X) ≡ P

Note that when S = T or T = {1, 2} for a binary treatment, the normalized propensity score
equals the propensity score Pt|T (X) = Pt (X).
Theorem 1 (Weak unconfoundedness - Propensity score)
Suppose Assumption 2 holds. Weak unconfoundedness Assumption 1 has the following equivalent representations: Consider any t ∈ T .
(a) For all t0 ∈ T and for all measurable functions g(X),
n
o
Pt0 (X), g(X) .

Dt0 ⊥ Y (t)

(b) For all S ⊆ T , t0 ∈ S, and for all measurable functions g(X),
Dt0 ⊥ Y (t)

n
o
X
Pt0 |S (X), g(X),
Ds = 1 .
s∈S

Now we discuss two important applications of Theorem 1. The first is identification of
the ATT by adjusting for the propensity scores. Theorem 1(a) implies that for s ∈ {t, t0 },
Ds ⊥ Y (t)



Pt0 (X), Pt (X) .

To see this result, for s = t0 , let g(X) = Pt (X) in Theorem 1(a). And for s = t, let
g(X) = Pt0 (X). We therefore obtain identification of
E[Y (t)|T = t0 , Pt (X), Pt0 (X)] = E[Y |T = t, Pt (X), Pt0 (X)].
It means that within the subpopulation with the same value of (Pt (X), Pt0 (X)), the average
outcome for units with treatment level t is unbiased for the average value of Y (t) for units
with treatment level t0 .

7

Theorem 1(b) has a parallel application: for s ∈ S ≡ {t, t0 },
Ds ⊥ Y (t)



Pt|{t,t0 } (X), Dt + Dt0 = 1 .

(3)

This is because Pt|{t,t0 } (X) + Pt0 |{t,t0 } (X) = 1. We therefore obtain identification of




E Y (t) T = t0 , Pt|{t,t0 } (X) = E Y T = t, Pt|{t,t0 } (X) .
It means that within the subpopulation with the same value of the normalized propensity
score Pt (X)/(Pt (X)+Pt0 (X)), the average outcome for units with treatment level t is unbiased
for the average value of Y (t) for units with treatment level t0 .
By Overlap Assumption 2, the average potential outcome Y (t) for the treated with level
t0

is identified


E[Y (t)|T = t0 ] = E E[Y |T = t, Pt (X), Pt0 (X)] T = t0


= E E[Y |T = t, Pt|{t,t0 } (X)] T = t0 .

(4)
(5)

Lechner (2001) obtains (3) and (5) using the subpopulation who has been treated at level
t or t0 . So the identification problem is reduced to the binary case in Rosenbaum and Rubin
(1983).
The second application of Theorem 1 is the coarsest strata constructed by the propensity
scores for causal comparison. Theorem 1(a) implies that
{Pt0 (X), Pt (X)} is the coarsest conditioning set such that the treatment assignment to level t, t0 , or any other level in the complement {T ∈
/ {t, t0 }} is weakly
unconfounded.
To see this, Theorem 1(a) implies Ds ⊥ Y (t)|{g(X), Pt (X), Pt0 (X)} for s ∈ {t, t0 } and for any
g(X). Let g(X) be a finer function of {Pt0 (X), Pt (X)} in the sense that {Pt0 (X), Pt (X)} =
h(g(X)) for some function h. So the conditioning set is {Pt0 (X), Pt (X), g(X)} = {g(X)}.
Similarly, Theorem 1(b) implies that
within the subpopulation who has received treatment level t or t0 ,



Pt (X)
Pt (X)+Pt0 (X)

defines the coarsest conditioning set such that the treatment assignment to level t
or t0 is weakly unconfounded.
The normalized propensity score Pt|{t,t0 } (X) defines a coarser strata than the propensity
scores {Pt (X), Pt0 (X)}. Both strata are valid for causal comparison, but {Pt|{t,t0 } (X)} has
the advantage to reduce the dimension of the estimation problem to one. The cost is to lose
causal inference on the sample {T ∈
/ {t, t0 }}. The causal comparison of Y (t) and Y (t0 ) is

8

valid within the subpopulation defined by {Pt|{t,t0 } (X), T ∈ {t, t0 }} in the sense that






E Y (t) − Y (t0 ) Pt|{t,t0 } (X), T ∈ {t, t0 } = E Y T = t, Pt|{t,t0 } (X) − E Y T = t0 , Pt|{t,t0 } (X) .
But we cannot make causal inference on the subpopulation defined by {Pt|{t,t0 } (X), T ∈
/
{t, t0 }}. For example, suppose we are interested in the average effect of switching from
treatment level 1 to 2 for those who are receiving any other treatment levels, E[Y (2) −
Y (1)|T ∈
/ {1, 2}]. In this case, adjusting for P1|{1,2} (X) is not sufficient. The identification
requires P1 (X) and P2 (X) for E[Y (2) − Y (1)|T ∈
/ {1, 2}] = E[E[Y |T = 2, P1 (X), P2 (X)] −
E[Y |T = 1, P1 (X), P2 (X)]|T ∈
/ {1, 2}].

2.1

Causal comparison

The subclassification or stratification method involves direct comparison within each strata.
In practice, it can be difficult to define the strata based on high-dimensional pretreatment
variables which motivates the propensity score methodology developed by Rosenbaum and
Rubin (1983). The following Corollary implied by Theorem 1 is important to provide the
coarsest strata to reduce dimension.
Corollary 1
Consider any t ∈ T . Theorem 1 implies for all subsets of treatment levels S ⊆ T and for all
t0 ∈ S,
(a) Dt0 ⊥ Y (t) {Ps (X)}s∈S . The conditioning set defined by the value of {Ps (X)}s∈S is
the coarsest strata such that the treatment assignment to a particular level in S or to
any other level in the complement {T ∈
/ S} is weakly unconfounded.

P
(b) Dt0 ⊥ Y (t) {Ps|S (X)}s∈S − , s∈S Ds = 1 . Define a subset S − ⊂ S to contain all the
P
elements in S but dropping the first one. Since s∈S Ps|S (X) = 1, the conditioning
set {{Ps|S (X)}s∈S } = {{Ps|S (X)}s∈S − }. Then within the subpopulation who has re
ceived treatment in S, {Ps|S (X)}s∈S − is the coarsest strata such that the treatment
assignment to a particular level in S is weakly unconfounded.
An intuition of Corollary 1 is that when more treatment levels are concerned, we need
a finer conditioning set by including more propensity scores. An extreme example is when
S =T,
T ⊥ Y (t) {Ps (X)}s∈T .
This is in line with Imbens (2000) and Lechner (2001) who point out that there is in general
no scalar function of the covariates such that the treatment variable is independent of the set

9

of potential outcomes under Strong unconfoundedness. This is also related to our previous
discussion on expressing Weak unconfoundedness Assumption 1 in terms of the treatment
indicator Dt0 instead of the multi-valued treatment variable T . Using Dt0 helps us focus on
the relevant treatment levels of interest and reduce the dimension of the conditioning set.
Corollary 1 implies the binary case in Rosenbaum and Rubin (1983): Dt ⊥ Y (t)|Pt (X).
We illustrate the roles of the propensity score in direct causal comparison and in estimation by the following example. Suppose we are interested in the causal effect of switching the
treatment level from 1 to 2 for the subpopulation with treatment level 3, E[Y (2)−Y (1)|T = 3].
The set of treatment levels of interest is S ≡ {1, 2, 3} ⊂ T . Corollary 1 implies that the subset defined by the value of (P2|S (X), P3|S (X)) is the coarsest strata where we can make valid
causal comparison for the subpopulation receiving treatment level 3: E[Y (2) − Y (1)|T =
3, P2|S (X), P3|S (X)] = E[Y |T = 2, P2|S (X), P3|S (X)] − E[Y |T = 1, P2|S (X), P3|S (X)]. Alternatively, we can use E[Y (2) − Y (1)|T = 3, P1 (X), P2 (X), P3 (X)] in Corollary 1(a). But, for
estimation, we only need to use one normalized propensity score or two propensity scores as
in (4) and (5) for E[Y (2)|T = 3] and E[Y (1)|T = 3] separately. This is the insight made by
Imbens (2000) that for estimation, it is not necessary to divide the population into subpopulations where causal comparisons are valid. For causal comparison, we need all the propensity
scores of the treatment levels of interest.

2.2

The binary case

Because the multi-valued treatment variable takes on more than two values, Weak unconfoundedness Assumption 1 implies the following weaker condition.
Assumption 3 (Weaker uncondoundedness)
For any t ∈ T , Dt ⊥ Y (t)|X.
When the treatment is binary, Weak unconfoundedness is equivalent to Weaker unconfoundedness Assumption 3, also known as missing at random assumption in the missing data
literature.
Weaker unconfoundedness only suffices to identify the conditional mean of Y (t) for those
who are not treated at level t, i.e., E[Y (t)|T 6= t, X] = E[Y (t)|Dt = 0, X] = E[Y (t)|Dt =
1, X] = E[Y |Dt = 1, X]. No information on any particular treated subpopulation {T = t0 }
can be recovered, in contrast to equation (2). But Weaker unconfoundedness Assumption 3
and Overlap Assumption 2 suffice to identify the population average treatment effect (ATE)
E[Y (t)] = E[E[Y |Dt = 1, X]]. For a multi-valued treatment, we can view the subpopulation
not receiving t0 {T 6= t} as the control group in the binary treatment case. This explains why
for the ATE of a multi-valued treatment, we only need Weaker unconfoundedness Assump-

10

tion 3 as in Imbens (2000) and Cattaneo (2010).

2

But for the ATT, we need a stronger

Weak unconfoundedness Assumption 1 to identify the treated group with value t0 , rather
than everyone else with treatment value different from t0 . This simple observation provides
intuition for identification based on the propensity scores.
We discuss different roles played by the two propensity scores (Pt (X), Pt0 (X)) by the
auxiliary sample and primary sample defined in Chen, Hong, and Tarozzi (2008),
(i) The auxiliary sample is the subpopulation with treatment level t, {T = t} whose Y (t)
is observed.
(ii) The primary sample is from the subpopulation of interest, where we aim to infer the
causal effects and Y (t) might not be observed.
For the ATT, the primary sample is the subpopulation with treatment level t0 , {T = t0 }.
The identification of the ATT uses the observations with treatment level t as the auxiliary
sample to recover the average Y (t) for the primary sample.
The general insight is that we need the propensity scores of both the auxiliary sample and
the primary sample to achieve identification. For the ATE, the primary sample is the entire
population. So E[Y (t)] = E[E[Y |T = t, X]] = E[E[Y |T = t, Pt (X)]]. The treatment group
{T = t} and the control group {T 6= t} are exactly the same as the binary treatment case
where two propensity scores add up to unity. So the propensity score of the control group
is redundant given the propensity score of the auxiliary sample. Therefore, we only need to
adjust for one propensity score for the following conditional causal objects.





1 − Pt (X)
1 − Pt (X))
= E E Y |T = t, Pt (X)]
.
E[Y (t)|T =
6 t] = E E[Y |T = t, X]
P rob(T 6= t)
P rob(T 6= t)
For T = {0, 1},





1 − P1 (X)
1 − P1 (X)
E[Y (0)|T = 1] = E E Y |T = 0, X]
= E E Y |T = 0, P1 (X)]
.
P rob(T = 0)
P rob(T = 0)


This is in line with our discussion on Weaker unconfoundedness. We should view the propensity score of the auxiliary sample Pt (X) playing two roles in estimating E[Y (t)|T 6= t] and the
ATT E[Y (1)−Y (0)|T = 1]: one is for the auxiliary sample and one is for the primary sample.
But for the ATE E[Y (t)], Pt (X) is solely for the auxiliary sample and to reduce dimension.
Hahn (1998) makes this point for a binary treatment from an efficiency point of view. The
above discussion offers a view of identification that is hidden for a binary treatment.
2

Imbens (2000) and Cattaneo (2010) refer to Assumption 3 as “Weak unconfoundedness.” In the model of multivalued treatments, we point out that it is “weaker” than the conventional Weak Unconfoundedness Assumption
1.

11

3

Semiparametric efficiency bounds

Consider a general treated subpopulation {T ∈ S} defined by a set of treatment levels of interest S ⊆ T . We calculate the semiparametric efficiency bound for estimating E[Y (t)|T ∈ S].
The advantage of working on the general treated subpopulation is that it encompasses both
ATT and ATE. For the ATT E[Y (t)|T = t0 ], the primary sample is the treated subpopulation with level t0 by S = {t0 }. For the ATE E[Y (t)], the primary sample is the entire
population by S = T . Another interesting example is the subpopulation defined by a set of
levels S = {1, 2, 3} ⊂ T = {1, 2, 3, 4}. Researchers might be interested in the ATT defined
by E[Y (1) − Y (2)|T ∈ {1, 2, 3}]. Our results suggest that when the primary sample is the
entire population, e.g., E[Y (t)] for the ATE, the semiparametric efficiency bound is the same
whether the propensity scores are known or not. When S is a strict subset of T such that
P
P rob(T ∈ S|X) = t0 ∈S Pt0 (X) < 1, knowledge of the propensity score reduces the semiparametric efficiency bound of E[Y (t)|T ∈ S]. Another contribution is that we formally show
the propensity score for the counterfactual level Pt (X) has no effect on the efficiency bound
of E[Y (t)|T ∈ S] for any S ⊆ T , i.e., for both the ATE and ATT. This result is obscured in
the binary treatment setup, where the propensity scores add up to one.
The calculation of the semiparametric efficiency bound is standard in the literature. We
follow the setup of Cattaneo (2010) who calculates the semiparametric efficiency bound of
the multi-valued treatment effects for the population. The parameter of interest is the distributional feature of the potential outcome Y (t) for the subpopulation {T ∈ S}, denoted by βt .
We define βt via a generic function m : Y × B 7→ Rdm , where the parameter space B ⊂ Rdβ
and dm ≥ dβ . We assume that the moment condition E[m(Y (t); βt )|T ∈ S] = 0 uniquely
defines the parameter βt . For example, the parameter is the mean βt = E[Y (t)|T ∈ S] by
setting m(y; β) = y − β. Under Assumptions 1, 2 and using equation (2), βt satisfies



 P rob(T ∈ S|X)
= 0.
E E m(Y ; βt ) T = t, X
P rob(T ∈ S)
We focus our discussion on the treated subpopulation with a single level t0 , i.e., S = {t0 }.
R
For the mean when m(y; β) = y − β, the parameter βt = E[Y (t)|T = t0 ] = E[Y |X =
x, T = t]dFX|T (x|t0 ). When m(y; β) = 1{y≤β} − τ , the parameter βt is the τ th quantile of
R
the conditional distribution FY (t)|T (·|t0 ) that satisfies FY |T X (βt |t, x)dFX|T (x|t0 ) = τ .
Before we present the semiparametric efficiency bound in Theorem 2 below, we introduce
the main component of the efficient influence function
P
 P 0 P 0 (X)
Dt0
Dt 
0
t ∈S t
ψt (Y, T, X) ≡
m(Y ; βt ) − et (X; βt )
+ et (X; βt ) t ∈S
Pt (X)
pS
pS

12

(6)



where et (x; βt ) ≡ E m(Y ; βt ) T = t, X = x and pS ≡ P rob(T ∈ S). For the population
P
parameter S = T , Theorem 2 coincides with the result in Cattaneo (2010) by t0 ∈S Dt0 = 1,
P
pS = 1, and t0 ∈S Pt0 (X) = 1. Our calculation in the proof shows that the efficient influence
function for the population parameter is reduced from (6) by the following term,
P
 P

Pt0 (X)
0
t0 ∈S Dt0
P
− 1 et (X; βt ) t ∈S
.
pS
t0 ∈S Pt0 (X)

(7)

For the population treatment effects, this term (7) is zero. For the treatment effects for
the treated S ⊂ T , the propensity scores for the treated levels {Pt0 (X)}t0 ∈S contribute this
term (7) in the efficient influence function (6). It comes from the treated observations with a
treatment level in S, i.e., the primary sample. We show below in Theorem 3 that this term
(7) disappears when the propensity scores are known.
Now we formally present the theorems for the semiparametric efficiency bounds. We
consider the object of interest to be a J × 1 vector β ≡ (β1 , ..., βJ )> . Define ψ(Y, T, X) to be
a J × 1 vector whose t-th component is ψt (Y, T, X) defined in (6). The following assumption
ensures the existence of the bound.
Assumption 4
For all t ∈ T :
(i) The propensity score Pt (x) is bounded away from zero and one.




(ii) E m(Y (t); βt )2 T ∈ S < ∞ and E m(Y (t); β) T ∈ S is differentiable in β ∈ B at βt .
(iii) Define the gradient matrix


Γ1

0

...

0






Γ∗ ≡ 



0
..
.

Γ2
..
.

...
..
.

0
..
.

0

0

...

ΓJ



i
∂ h

E
E[m(Y
;
β)|T
=
t,
X]
T
∈
S
 , where Γt ≡

∂β >


β=βt

and 0 is a dm × dβ matrix of zeros. The rank of Γ∗ is dβ J.
Theorem 2 (Efficiency - Unknown propensity score)
Suppose Assumptions 1 and 4 hold. Then the efficient influence function of β is given by
−1 > −1
−1
Ψ = − Γ>
Γ∗ V∗ ψ where V∗ = var[ψ]. The semiparametric efficiency bound for
∗ V∗ Γ∗
−1
−1
any regular estimator of β is given by V ∗ = Γ>
.
∗ V∗ Γ∗
Theorem 2 encompasses the binary treatment literature: ATT in Hahn (1998) and Hirano,
Imbens, and Ridder (2003), the quantile treatment effects in Firpo (2007), distributional
effects in Firpo and Pinto (2011), and the missing data model in Chen, Hong, and Tarozzi
(2008).

13

Theorem 3 (Efficiency - Known propensity score)
Suppose the propensity scores {Pt (X)}t∈T are known. Suppose Assumptions 1 and 4 holds.
−1 0 −1 P S
Then the efficient influence function of β is given by ΨP S = − Γ0∗ V∗−1 Γ∗
Γ∗ V∗ ψ where
ψ P S (Y, T, X) is defined by its t-th component
ψtP S (Y, T, X)


≡

P
 D

Dt
t
t0 ∈S Pt0 (X)
.
m(Y ; βt ) −
− 1 et (X; βt )
Pt (X)
Pt (X)
pS

(8)

The semiparametric efficiency bound of βt with known propensity scores is smaller than the
bound when the propensity scores are unknown. Furthermore, the result still holds when
Pt0 (X) is known for only t0 ∈ S ⊂ T .
The semiparametric efficiency bound of the distributional feature of Y (t) for the population satisfying E[m(Y (t); βt )] = E[E[m(Y ; βt )|T = t, X]] = 0 is the same regardless the
propensity scores are known.
Theorem 3 suggests that even when the propensity scores are known for only the treated
levels in S ⊂ T and Pt (X) is not known (i.e., t ∈
/ S), knowledge of these propensity scores
for the treated levels reduces the semiparametric efficiency bound. The important lesson is
that the asymptotic efficiency is improved by more information on the propensity scores for
the treated level {Pt0 (X)}t0 ∈S , but not affected by the propensity score for the counterfactual
level Pt (X). More specifically, the difference between the efficient influence functions for the
unknown and known propensity scores cases is the term (7), i.e., ψt in (6) can be decomposed
to ψtP S in (8) and the influence from {Pt0 (X)}t0 ∈S in (7). The second part associated with
et (X; βt ) in (8) resembles (7) and comes from the auxiliary sample {T = t}. That is to say
the influence from Pt (X) remains whether Pt (X) is known.
Frölich (2004b) discusses some heuristic intuition using the normalized propensity score
and the result in the binary treatment literature. He notes that the variance bound depends
on which and how many propensity scores are known. Our Theorem 3 provides a theoretical
justification. The multi-valued treatment setup distinguishes different propensity scores,
which are degenerated to one propensity score in a binary treatment case. The main reason
behind this result is that the propensity score for the treated level defines the parameter of
interest βt when S is a strict subset of T . But Pt (X) does not play a role in the definition
of ATE, as pointed out by Chen, Hong, and Tarozzi (2008).
Theorem 3 coincides with the results from the binary treatment effect literature that
knowledge of the propensity score does not affect the efficiency bound of the ATE, but
decreases that of the ATT. That includes the ATE and ATT for a binary treatment in Hahn
(1998), Hirano, Imbens, and Ridder (2003), and the “verify-out-of-sample” for missing data
in Chen, Hong, and Tarozzi (2008). The statement in Chen, Hong, and Tarozzi (2008),
“more information on the propensity score will not affect the asymptotic efficiency bounds

14

for the verify-in-sample case,” does not apply to a multi-valued treatment. We illustrate by
an example of estimating E[Y (1)|T ∈ {1, 2, 3}], where the primary sample is S ≡ {1, 2, 3} ⊂
T = {1, 2, 3, 4}. The auxiliary sample {T = 1} is a subset of the primary sample, so this is
the verify-in-sample case in Chen, Hong, and Tarozzi (2008). But our results suggest that
knowledge of the propensity scores decreases the efficiency bound. This is true whether the
auxiliary sample {T = t} is independent or out of the primary sample {T ∈ S}. The key is
if the propensity score enters the definition of the object of interest E[Y (t)|T ∈ S].
The semiparametric efficiency bound gives insight to construct an efficient regression
estimator: when the propensity scores are known, we should make use of the true propensity
score for the treated subpopulation Pt0 (X). We study the limiting property of our efficient
propensity score regression estimators in the next section.

4

Efficient regression estimators

We estimate the cumulative distribution function of the potential outcome Y (t) for the
treated subpopulation with level t0 , for any t, t0 ∈ T . The estimand is based on the identification in (4) and (5)




FY (t)|T (y|t0 ) ≡ E 1{Y (t)≤y} T = t0 = E E[1{Y ≤y} |T = t, Pt (X), Pt0 (X)]W
 
 
Pt (X)
W
= E E 1{Y ≤y} T = t,
Pt (X) + Pt0 (X)
where the weight W = Dt0 /pt0 or W = Pt0 (X)/pt0 . For the notation in Section 3, m(Y ; β) =
1{Y ≤y} − β for y ∈ Y. The common distributional features, such as mean and quantiles,
can be extended straightforwardly. For example, by changing the dependent variable from
1{Y ≤y} to Y , we estimate the ATT E[Y (t)|T = t0 ]. We provide limit theory for estimating
various distributional features of FY (t)|T (y|t0 ) in Section 4.6.
The regression estimators use the propensity scores as generated regressors. Denote the
generated regressors to be V = v0 (X), a vector of measurable functions v0 of X. We use
and extend the limit theory for the partial mean with generated regressors in Lee (2014).
We construct efficient estimators for two cases — when the propensity scores are unknown
and when they are known. We first present the limit theory using the propensity scores
V = v0 (X) = (Pt (X), Pt0 (X)). We show the limit theory for the estimators regressing on
X in Section 4.3. Then we present the estimators regressing on the normalized propensity
score V = v0 (X) = Pt (X)/(Pt (X) + Pt0 (X)) in Section 4.4. These results imply efficient
estimators for the binary treatment effect for the treated in Section 4.5. Denote the estimand
FY (t)|T (y|t0 ) ≡ βt|t0 (y) ≡ β(y) that suppresses the dependence on t, t0 , for ease of exposition.
All the results presented in this section are for any t, t0 ∈ T and we will omit this statement

15

without loss of clarity.

4.1

Unkonwn propensity scores

The estimation procedure of the propensity score regression estimator follows three steps.

x
k xhl , where h is the bandwidth assumed the
We use a product kernel Kh (x) ≡ h−dx Πdl=1
same for all the elements of the vector x for simplicity, and k is the r-order kernel function
satisfying Assumption 6 in the Appendix.
Step 1. (Generated regressor) Estimate the propensity scores by a kernel regression,
P̂t (x) =

Pn

i=1 Dti Kh1 (Xi

− x)/

Pn

i=1 Kh1 (Xi

− x).

Step 2. (Regression) Estimate the conditional cdf by a kernel regression with the
>
generated regressor V̂i = v̂(Xi ) ≡ P̂t (Xi ), P̂t0 (Xi ) from Step 1.
 Pn
P
F̂Y |T V̂ (y|t, v) = ni=1 1{Yi ≤y} Dti Kh (V̂i − v)
i=1 Dti Kh (V̂i − v).
Step 3. (Partial mean) Consider two estimators:
β̂(y) =

n

n

i=1

i=1

1X
Dt0 i
1X
F̂Y |T V̂ (y|t, V̂i )
where p̂t0 =
Dt0 i
n
p̂t0
n

n
1X
P̂t0 (Xi )
β̂ p (y) =
F̂Y |T V̂ (y|t, V̂i )
.
n
p̂t0
i=1

Note that the weight Dt0 /pt0 only uses the observations from the treated group with level t0 .
The weight Pt0 (X)/pt0 uses all the observations in the sample but contains sample variation
of estimating Pt0 (X). We show that both weights lead to efficient estimators.
The estimator will include two fixed trimming functions in Step 2 and Step 3: In Step
2, the boundary of pretreatment variables X is trimmed. In Step 3, the density of the
conditioning variables are bounded away from zero. The trimming functions ensure uniform
convergence of the first- and second-step estimators over the range of integration that suffices
for deriving the properties of the third-step estimator. In fairness, the choice of fixed trimming
function can affect the interpretation of the estimands considered. That is, we estimate the
ATT for the subpopulation whose pretreatment variables do not take extreme values and
the common support Assumption 4(i) holds. Heckman, Ichimura, and Todd (1998) also
estimate the region of common support. The fixed trimming choice allows us to focus on
the technical issues associated with estimating the generated regressor. We suppress the two
fixed trimming functions for notational ease.
Theorem 4 (Unknown propensity scores)
Suppose Assumptions 1, 4, and the Assumptions in the Appendix hold. Then uniformly in

16

y ∈ Y,
√

n
 √

1 X
n β̂(y) − β(y) = n β̂ p (y) − β(y) + op (1) = √
ψt (Yi , Ti , Xi ; y) + op (1)
n
i=1

where
ψt (Yi , Ti , Xi ; y) ≡


 Dt0 i
Dti Pt0 (Xi )
1{Yi ≤y} − FY |T X (y|t, Xi ) + FY |T X (y|t, Xi ) − β(y)
Pt (Xi ) pt0
pt0

the same efficient influence function (6) derived in Theorem 2.
The asymptotic linear representation in Theorem 4 accounts for the sampling variation
of estimating the propensity scores that are used in the generated regressors and the weight.
The propensity score can be viewed as an index function of the pretreatment variables X.
Lee (2014) shows that the estimation error associated with estimating the propensity scores
as generated regressors is characterized by the index bias FY |T V (y|t, v0 (X)) − FY |T X (y|t, X).
The index bias is the impact of X on the outcome distribution that is not captured by
the propensity scores v0 (X) = (Pt (X), Pt0 (X)). The index bias is the key feature of the
nonparametric regression with generated regressors that has been shown in the literature, for
example, Hahn and Ridder (2013), Escanciano, Jacho-Chávez, and Lewbel (2014), Mammen,
Rothe, and Schienle (2014), Lee (2014) among others. This is the key for the nonparametric
propensity score regression estimator to reach the semiparametric efficiency bound. The
important message is that
(Index bias) The nonparametric estimation of the propensity scores recovers the
information of the pretreatment variables on the outcome distribution that is not
captured by the propensity scores.
More specifically, we show in the proof that the estimation error of the generated regressor
P̂t0 (Xi ) contributes

 D0
 Pt0 (Xi )
ti
− 1 FY |T V (y|t, v0 (Xi )) − FY |T X (y|t, Xi )
−
Pt0 (Xi )
pt0

(9)

to the final estimators β̂(y) and β̂ p (y). Observe that the second part FY |T X (y|t, Xi ) Dt0 i /Pt0 (Xi )−

1 Pt0 (Xi )/pt0 is (7), the part in the efficient influence function from the treated group
{T = t0 }. The nonparametrically estimated propensity scores pick up the additional term
in the efficient influence function in (7) . This is in line with the inverse propensity score
weighting estimator and the doubly robust estimator in Cattaneo (2010). The first part

FY |T V Dt0 i /Pt0 (Xi ) − 1 Pt0 (Xi )/pt0 cancels out the sampling variation of the estimation in
Step 2 and Step 3. The estimation error of the generated regressor P̂t (Xi ) contributes the

17

term
 D

 Pt0 (Xi )
ti
− 1 FY |T V (y|t, v0 (Xi )) − FY |T X (y|t, Xi )
Pt (Xi )
pt0

(10)

to the final estimators. Similarly, the second part associated with FY |T X (y|t, Xi ) recovers the
part of the efficient influence function from using the observations from the auxiliary sample
{T = t} in the inner expectation, i.e., the second part associated with et in (8).
The generated regressor V = (Pt (X), Pt0 (X)) plays two roles — the regressor that determines the regression function FY |T V and the argument of the regression function that is
averaged out in the third step. The estimation error of V̂ from its role as the regressor is the
key of our results. More detail is in Lee (2014) and we discuss some intuition in the following.
The third step integrates over the weight Pt0 (X) and the argument V = (Pt (X), Pt0 (X)) of
the regression function. By changing the order of the integrations in the second step and
third step, the kernel function effectively makes the weight and the argument evaluated at
the estimated V̂ . We then use a Taylor expansion to derive the first order influence of the
estimation error V̂ − V .

4.2

Known propensity scores

When the propensity scores are known, we propose an efficient regression estimator using a
nonparametrically estimated Pt (X) and the true known Pt0 (X) as the generated regressors.
We modify Step 2 and Step 3 in the above estimation procedure to the following
Step 20 . (Regression) Estimate the conditional cdf by a kernel regression using the
regressors (T, V̂ ) where V̂ = (P̂t (X), Pt0 (X))> from Step 1.
Step 30 . (Partial mean)
n

Pt0 (Xi )
1X
F̂Y |T V̂ (y|t, P̂t (Xi ), Pt0 (Xi ))
β̃(y) =
n
p̃t0
i=1

n

1X
where p̃t =
Pt0 (Xi ).
n
i=1

The estimation for the treated subpopulation is more precise by using all observations that
share the same value of the propensity score Pt0 (Xi ), comparing with using only the treated
observations by Dt0 i .
Theorem 5 (Known propensity scores)
Suppose Assumptions 1, 4, and the Assumptions in the Appendix hold. Then uniformly in
y ∈ Y,
√

n

1 X PS
n β̃(y) − β(y) = √
ψt (Yi , Ti , Xi ; y) + op (1)
n


i=1

18

where
ψtP S (Yi , Ti , Xi ; y) ≡


 Pt0 (Xi )
Dti Pt0 (Xi )
1{Yi ≤y} − FY |T X (y|t, Xi ) + FY |T X (y|t, Xi ) − β(y)
Pt (Xi ) pt0
pt0

the same efficient influence function (8) derived in Theorem 3.
It has been a widely-studied puzzle in the literature that using the estimated propensity
score is more efficient than using the true one (Hahn, 1998; Imai and van Dyk, 2004; Hirano,
Imbens, and Ridder, 2003; Graham, 2011; Abadie and Imbens, 2012). Our insight in the
index bias provides a new theoretical explanation. Theorem 3 suggests that knowledge of
Pt (X) does not affect the semiparametric efficiency bound. This gives us the freedom not
to use the true propensity score. From the above discussion on the index bias, the nonparametric estimation of the propensity score captures the information of X on the outcome
distribution that is reduced in the propensity scores. This information is lost if regressing
on the true propensity score. The sampling variation of P̂t (X) picks up the term associated
with the auxiliary sample in the efficient influence function. Therefore, we should use a
nonparametrically estimated Pt (X) even though it is known.
Imai and van Dyk (2004) discuss this issue for subclassification by an application with
randomized treatment assignment. They claim that an estimated propensity score accounts
for the sample-specific relationship of the treatment and the covariates, which is lost in the
true propensity score. Abadie and Imbens (2012) find that when the propensity score is
parametrically specified, for the ATE, matching on the estimated propensity score is more
efficient than matching on the true one. When the number of matches increases with the
sample size, a subclassification or matching estimator is essentially like a nonparametric
regression estimator. We provide a new theoretical explanation that the nonparametrically
estimated propensity scores recover the relationship of the covariates and the outcome.
Theorem 5 parallels previous findings in the inverse propensity score weighting estimators
or GMM framework in Hirano, Imbens, and Ridder (2003), Cattaneo (2010), and Graham
(2011). In contrast, we study this paradox from a view of projection onto the propensity
score. Hirano, Imbens, and Ridder (2003) interpret the efficiency loss of using the true
propensity score by the empirical likelihood estimation. Nonparametrically estimating the
propensity score captures the information content of a conditional moment restriction of the
propensity score (E[Dt − Pt (X)|X] = 0) by a sequence of unconditional moment restrictions.
Graham (2011) calculates the efficiency bound incorporating the conditional moment of the
propensity score as the auxiliary moment. While a parametric estimate of the propensity
score only satisfies a finite number of the moment conditions, using the true propensity score
makes no use of any information contained in the auxiliary moment. So the efficiency is
improved in the same way as adding moment restrictions in a GMM framework.

19

Remark (Propensity score weighting estimation)
Our estimator shares the same spirit with the inverse propensity score weighting estimator.
When the propensity scores are known, the efficient estimator in Hirano, Imbens, and Ridder
(2003) is inversely weighted by the nonparametrically estimated propensity score P̂t (X) and
then reweighed by the true propensity score Pt0 (X) to adjust for the treated group. And the
conditional outcome distribution given T = t, FY |T (y|t) = FY (t)|T (y|t), is efficiently estimated
P
by n−1 ni=1 1{Yi ≤y} Dti n−1 PPnt (XiP)t (Xj ) . In contrast to the case when the propensity score
P̂t (Xi )
j=1
 Pn
P
is unknown, FY |T (y|t) = FY (t)|T (y|t) is efficiently estimated by ni=1 1{Yi ≤y} Dti
i=1 Dti .
For a multi-valued treatment, Ao, Calonico, and Lee (2014) provide efficient inverse
propensity score weighting estimators and doubly robust estimators by extending the idea
in Cattaneo (2010). When the propensity scores are known, we expect the corresponding
efficient estimators to be constructed similarly as Hirano, Imbens, and Ridder (2003). This
is left for future research.

4.3

Regression on observed characteristics

Consider regression estimators adjusting for pretreatment variables X:
n

Dt0 i
1X
β̂x (y) =
F̂Y |T X (y|t, Xi )
n
p̂t0
i=1

n
1X
P̂t0 (Xi )
β̂xp (y) =
F̂Y |T X (y|t, Xi )
n
p̂t0
i=1

n
1X
Pt0 (Xi )
β̃x (y) =
F̂Y |T X (y|t, Xi )
n
p̃t0
i=1

The following Theorem 6 shows that β̂x (y) and β̂xp (y) are semiparametrically efficient when
the propensity scores are unknown. And β̃x (y) is semiparametrically efficient when the
propensity score are known. Hahn (1998) proposes series estimators for β̂x (y) and β̃x (y) for
a binary treatment and shows that they reach the semiparametric efficiency bounds.
Theorem 6 (Regression on X)
Suppose Assumption 5 in the Appendix holds for V = v0 (X) = X. Suppose Assumptions
αv
6 and 7 hold. For β̂xp (y), assume Pt0 (X) ∈ CM
(X ) with αv > dx /2, g < min{1/(dx +

2αv ), 1/dx − η}, and r1 > 1/(2g). Then uniformly in y ∈ Y,
(a) when the propensity scores are unknown,
√

n
 √

1 X
n β̂x (y) − β(y) = n β̂xp (y) − β(y) + op (1) = √
ψt (Yi , Ti , Xi ; y) + op (1);
n
i=1

20

(b) when the propensity scores are known,
√

n

1 X PS
n β̃x (y) − β(y) = √
ψt (Yi , Ti , Xi ; y) + op (1).
n
i=1

Theorem 6(b) implies that the estimator using the true propensity scores V = v0 (X) =
P
(Pt (X), Pt0 (X)) β̃v (y) = n−1 ni=1 F̂Y |T X (y|t, Vi ) Pt0 (Xi )/p̃t0 has the influence function equal
to
ψtP S (Yi , Ti , Vi ; y) ≡


 Pt0 (Xi )
Dti Pt0 (Xi )
1{Yi ≤y} − FY |T V (y|t, Vi ) + FY |T V (y|t, Vi ) − β(y)
.
Pt (Xi ) pt0
pt0

Because the whole set of observables X provides finer conditioning variables than its index
v0 (X), β̃V t regressing on the true propensity scores is less efficient than β̃Xt regressing on
X (Hahn, 1998; Heckman, Ichimura, and Todd, 1998; Lee, 2014). Lemma D.1 in Lee (2014)
formally shows that E[ψtP S (Y, T, V ; y)2 ] ≥ E[ψtP S (Y, T, X; y)2 ]. When the index bias is not
zero, i.e., there exists x such that FY |T V (y|t, v0 (x)) 6= FY |T X (y|t, x), the inequality between
the corresponding asymptotic variances is strict. The difference between ψtP S (Yi , Ti , Xi ; y)
and ψtP S (Yi , Ti , Vi ; y) is exactly the same as the influence of estimating the generated regressor
P̂t (X) in (10). This again shows that P̂t (X) recovers the information lost in regressing on
the true Pt (X).

4.4

Normalized propensity score

Consider the estimator regressing on the normalized propensity score Pt|{t,t0 } (X) = Pt (x)/(Pt (x)+
Pt0 (x)) ≡ bt (X) for notational ease. We first consider the case when we do not know the true
propensity scores. The estimator is modified from the previous procedure.
Step 1b. (Generated regressor) Estimate the normalized propensity scores by a
kernel regression,
b̂t (x) =

Pn

P̂t (x)
P̂t (x) + P̂t0 (x)

= Pn

i=1

− x)
.
Kh1 (Xi − x)

i=1 Dti Kh1(Xi

Dti + Dt0 i

Step 2b. (Regression) Estimate the conditional cdf by a kernel regression with the
generated regressor V̂i = b̂t (Xi ) from Step 1.
 Pn
P
F̂Y |T V̂ (y|t, v) = ni=1 1{Yi ≤y} Dti Kh (b̂t (Xi ) − v)
i=1 Dti Kh (b̂t (Xi ) − v).
Step 3b. (Partial mean)
n

1X
Dt0 i
β̂b (y) =
F̂Y |T V̂ (y|t, b̂t (Xi ))
.
n
p̂t0
i=1

21

Theorem 7 (Unknown normalized propensity scores)
Suppose Assumptions 1, 4, and the Assumptions in the Appendix hold. Then uniformly in
y ∈ Y,
√

n

1 X
ψt (Yi , Ti , Xi ; y) + op (1)
n β̂b (y) − β(y) = √
n
i=1

that has the same efficient influence function in Theorem 4.
When the propensity scores are known, we use the known propensity score for the treated
level Pt0 (X) in estimating the normalized propensity score. So the above Step 1b is modified
to
Step 1b0 . (Generated regressor) Using the known Pt0 (X), estimate the normalized
propensity scores by a kernel regression,
b̃t (x) =

Pn

P̂t (x)
P̂t (x) + Pt0 (x)

= Pn

i=1

− x)
i=1 Dti Kh1 (X
 i
.
Dti + Pt0 (x) Kh1 (Xi − x)

Step 2b and Step 3b are the same. Denote the corresponding estimator using V̂ = b̃t (X) as
a generated regressor to be
n

β̃b (y) =

Pt0 (Xi )
1X
F̂Y |T V̂ (y|t, b̃t (Xi ))
n
p̃t0
i=1

Theorem 8 (Known normalized propensity scores)
Under Assumptions 1, 4, and the Assumptions in the Appendix, for each t ∈ T , uniformly
in y ∈ Y
√

n

1 X PS
n β̃b (y) − β(y) = √
ψt (Yi , Ti , Xi ; y) + op (1)
n
i=1

that has the same efficient influence function in Theorem 5.

4.5

The binary case

Theorem 7 implies that when the propensity score is unknown, the efficient estimator for
E[Y (1)|T = 0] for a binary treatment is
n
 D0i
1X 
β̂b (y) =
Ê Y T = 1, P̂1 (X) = P̂1 (Xi )
.
n
p̂0
i=1

22

When the propensity score is known, Theorem 8 suggests that an efficient regression estimator
for the binary case
β̃b (y) =

n
 P0 (Xi )
1X 
Ê Y T = 1, b̃1 (X) = b̃1 (Xi )
n
p̃0
i=1

where b̃1 (X) =

n

P̂1 (X)
P̂1 (X) + P0 (X)

and p̃0 =

1X
P0 (Xi ).
n
i=1

The efficient estimator β̃b (y) utilizes the knowledge of the propensity score for the treated
level P0 (X). And estimating the propensity score for the counterfactual level P̂1 (X) recovers
the information of the covariates that is lost from regressing on the propensity scores.
When the propensity score is known, it might be intuitive to construct an estimator based
on Heckman, Ichimura, and Todd (1998)
n

P0 (Xi )
1X
Ê[Y |T = 1, P̂1 (X) = P̂1 (Xi )]
.
n
p̃0
i=1

We label this estimator as the HIT estimator. Theorem 9 below implies that the HIT
estimator is not efficient when the propensity score is known. This is in contrast to the
P
efficient estimator for the population Ê[Y (1)] = n−1 ni=1 Ê[Y |T = 1, P̂1 (X) = P̂1 (Xi )] in
Hahn and Ridder (2013); Mammen, Rothe, and Schienle (2014); Lee (2014). This is also
P
different from the estimator regressing on the covariates β̃x (y) = n−1 ni=1 Ê[Y |T = 1, X =
Xi ]P0 (Xi )/p̃0 that is efficient by Theorem 6 and Hahn (1998). From Theorem 4 and the
discussion in the last paragraph of Section 4.1, the generated regressor P̂1 (X) has an influence
term through the weight P0 (X)/p0 = (1 − P1 (X))/p0 . This situation does not appear in the
multi-valued case. We formalize the above discussion in Theorem 9 below. Let the HIT
estimator of FY (t)|T (y|t0 ) be
β̌(y) =

n
 P0 (Xi )
1X 
Ê 1{Y ≤y} T = 1, P̂1 (X) = P̂1 (Xi )
.
n
p̃0
i=1

Theorem 9 (HIT estimator)
Suppose Assumptions 1, 4, and the Assumptions in the Appendix hold. Then uniformly in
y ∈ Y,
√

n
 1X
n β̌(y) − β(y) =
ψtP S (Yi , Ti , Xi ; y)
n
i=1

1
+ D1i − P1 (Xi ) FY |T V (y|1, V (Xi )) − FY |T X (y|1, Xi )
+ op (1).
p0

23

The second line of the above influence function of β̌(y) can be expressed as − D0i −


P0 (Xi ) FY |T V (y|1, V (Xi )) − FY |T X (y|1, Xi ) /p0 that equals the estimation error associated
with estimating P̂0 (X) as a generated regressor in (9) discussed in Section 4.1. Intuitively, this
is because P̂0 (X)+ P̂1 (X) = 1, regressing on P̂1 (X) is effectively regressing on (P̂0 (X), P̂1 (X))
or P̂1 (X)/(P̂0 (X) + P̂1 (X)).

4.6

Inference for the Treatment Effects

Often the objects of ultimate interest are policy effects or inequality measures. Such objects
can be expressed as functionals of the potential outcome distributions identified by the partial
mean and estimated in previous sections. The key to the distribution theory for a class of
smooth functionals is the functional delta method for Hadamard-differentiable functionals in
empirical process theory. These Hadamard-differentiable functionals can be highly nonlinear
functionals of the cdf, but admit a linear functional derivative. Hadamard-differentiablility
is a high-level assumption that could impose restrictions or smoothness on the distribution
functions of potential outcomes. Additional assumptions might be needed for different policy
functionals. For instance, Bhattacharya (2007) gives regularity conditions for Hadamarddifferentiability of Lorenz and Gini functionals.
In this section, we denote the estimand βt|t0 (y) = FY (t)|T (y|t0 ). The corresponding Theorems 4 to 9 in the previous sections provide the uniform asymptotic linear representation for
the corresponding estimator β̂t|t0 (y).
Theorem 10
Consider any t, t0 ∈ T . Assume the conditions in the asymptotic theorem for β̂t|t0 (y) hold
such that uniformly in y ∈ Y
√

n

1 X
n β̂t|t0 (y) − βt|t0 (y) = √
ψt|t0 i (y) + op (1).
n
i=1

Then
1. (Weak Convergence)
√


n β̂t|t0 (·) − βt|t0 (·) ⇒ Gt|t0 (·)

where Gt|t0 (y) is a Gaussian process indexed by y ∈ Y in l∞ (Y), with mean zero and
covariance kernel defined by the limit of the second moment of ψt|t0 i .
2. (Functional Delta Method) Consider the parameter β as an element of a parameter
space Dβ ⊂ l∞ (Y) with Dβ containing the true value βt|t0 . Suppose a functional Γ(β)

24

mapping Dβ to l∞ (W) is Hadamard differentiable in β at βt|t0 with derivative Γ0β .

3

n

1 X 0
n Γ(β̂t|t0 )(w) − Γ(βt|t0 )(w) − √
Γβ (ψt|t0 i )(w) = op (1)
n
i=1

√
n Γ(β̂t|t0 )(w) − Γ(βt|t0 )(w) ⇒ Γ0β (Gt|t0 )(w) ≡ G(w)

√

where G is a Gaussian process indexed by w ∈ W in l∞ (W), with mean zero and
covariance kernel defined by the limit of the second moment of Γ0β (ψt|t0 i ).

5

Conclusion

We examine the role of the propensity scores in estimating the multi-valued treatment effects
for the treated from three perspectives: (i) (Identification) The equivalent representations
of weak unconfoundedness generalize the propensity score methodology in Rosenbaum and
Rubin (1983), Imbens (2000), and Lechner (2001). (ii) (Semiparametric efficiency bound)
We calculate the semiparametric efficiency bounds for the cases when the propensity scores
are unknown and known, for the population and for the treated. Knowledge of the propensity score for the counterfactual level has no role in the semiparametric efficiency bound.
But knowledge of the propensity score for the treated level improves asymptotic precision.
(iii) (Regression estimator) We propose efficient regression estimators that project on two
propensity scores — one for the counterfactual level and one for the treated level. The nonparametrically estimated propensity score recovers the impact of the pretreatment variables
on the outcome that is not captured by the propensity score. By the insight of the efficiency
bound, we should utilize the true propensity score for the treated value. These findings provide new explanation to some paradoxes on the propensity scores in the binary treatment
effect and missing data literature.
This paper provides a theoretical foundation for propensity scores matching methodology.
Our insights on the multi-valued treatment can be extended to the generalized propensity
scores for a continuous treatment in Hirano and Imbens (2004) and Lee (2014). For practical
application of the efficient regression estimators, the bandwidth choice and small-sample
performance are left for future research. For example, Busso, DiNardo, and McCrary (2014)
is an extensive small-sample study on various estimators.
3

See, for example, van der Vaart (2000) for defintion: let Γ be a Hadamard-differentiable functional mapping
from F to some normed space E, with derivative Γ0f , a continuous linear map F 7→ E. For every hn → h and f ∈ F,

1
Γ(f + uhn ) − Γ(f ) = Γ0f (h).
u→0 u
lim

25

Appendix
A

Proofs in Section 2 Identification

Proof of Theorem 1
(a) For any t, t0 ∈ T ,


E[Dt0 |Y (t), Pt0 (X), g(X)] = E E[Dt0 |Y (t), Pt0 (X), g(X), X] Y (t), Pt0 (X), g(X)




= E E[Dt0 |Y (t), X] Y (t), Pt0 (X), g(X) = E E[Dt0 |X] Y (t), Pt0 (X), g(X)


= E Pt0 (X) Y (t), Pt0 (X), g(X) = Pt0 (X)
where the third equality is by Assumption 1. By the law of iterated expectations,
E[Dt0 |Pt0 (X), g(X)] = Pt0 (X). So E[Dt0 |Y (t), Pt0 (X), g(X)] = E[Dt0 |Pt0 (X), g(X)] for
any t, t0 ∈ T and g(X).
The reverse direction follows by letting g(X) = X.
(b) We first prove another equivalent representation to Weak unconfoundedness Assumption 1:
 P
For any t ∈ T , P
for all S ⊆ T , T ⊥ Y (t) X, s∈S Ds = 1 or equivalently
Dt0 ⊥ Y (t) X, s∈S Ds = 1 for t0 ∈ S.
We abuse the notation f for joint probability density functions.


h
i
X
P rob Dt0 = 1 X
f Dt0 = 1, Y (t), X

=
P
P
E Dt0 Y (t), X,
Ds = 1 =
f Y (t), X, s∈S Ds = 1
P rob
s∈S Ds = 1 X
s∈S
h
i
X
= E Dt0 X,
Ds = 1 .
s∈S

The reverse direction follows by letting S = T .
Using the above result, (b) is proved following the same procedure as the proof in (a).

Proof of Corollary 1
Theorem 1 implies for any t0 ∈ S,


P rob Dt0 = 1 Y (t), {Ps (X)}s∈S , g(X) = P rob Dt0 = 1 {Ps (X)}s∈S , g(X) .
Setting g(X) = 1 gives the first result in (a).
Following the same argument in Rosenbaum and Rubin (1983), the set of the propensity scores {Ps (X)}s∈S has a balancing property: within strata with the same value of
{Ps (X)}s∈S , the probability of being assigned to a level s in S or some other levels not
in S does not depend on the value of X. And {Ps (X)}s∈S is the “coarsest” balancing score
and X is the “finest.” 4 The second result in (a) follows by setting g(X) to be a balancing
4

A balancing score g(X) for Dt satisfies E[Dt |X, g(X)] = E[Dt |g(X)]. The left-hand-side is Pt (X) and the

26

score of S. Then the conditioning set {{Ps (X)}s∈S , g(X)} = {g(X)}.
The proof of the result (b) follows the same procedure.

B



Proofs in Section 3 Semiparametric Efficiency Bounds

B.1

Proof of Theorem 2

We follow the procedure in the proof of Theorem 1 in Cattaneo (2010), so we skip the
repetition and note the difference. Consider a regular parametric submodel of the joint
distribution the observed data (Y, T, X) indexed by θ. Define the score to be S(y, t, x; θ0 ) =
Sy (y, t, x) + Sp (t, x) + Sx (x), where
Sy (y, T, x) ≡

X

Dj sj (y, x), sj (y, x) ≡

j∈T

Sp (T, x) ≡

X

Dj

j∈T

Sx (x) ≡

∂
log fY (j)|X (y|x; θ)
∂θ

Ṗj (x)
∂
, Ṗj (x) ≡
Pj (x; θ)
Pj (x)
∂θ

∂
log f (x; θ)
∂θ

θ0

θ0

θ0

,

,

.

The tangent space is characterized by Hy + Hp + Hx in Cattaneo (2010), where Hy ≡
{Sy (Y, T, X) : sj (Y, X) ∈ L20 (FY (j)|X (Y |X)), ∀j ∈ T }, Hp ≡ {Sp (T, X) ∈ L20 (FT |X )}, and
P
Hx ≡ {Sx (X) : Sx (X) ∈ L20 (Fx )}. So E[Sp (T, X)|X] = j∈T Ṗj (X) = 0 and E[Sp2 (T, X)|X] =
P
2
j∈T Ṗj (X)/Pj (X) < ∞. Define the main component of the efficient influence function
P
Pt0 (X)
Dt
0
EIF ≡
m(Y ; βt ) t ∈S
Pt (X)
pS
 D
 X P 0 (X)
 D0

X Pt0 (X)
t
t
t
−
et (X; βt )
−1 +
et (X; βt )
−1
pS
Pt (X)
pS
Pt0 (X)
0
0
t ∈S

t ∈S

belonging to the tangent space.
We first obtain the following equality, for any function A(Y, X),
h

 i


E Dt E A(Y, X) X, Dt X = E A(Y, X) T = t, X Pt (X).

(11)



Let m(β) = m(Y (t); β1 )> , ..., m(Y (t); βJ )> . So Eθ0 [m(β(θ0 ))|T ∈ S] = 0. For any
∂
∈ S]
= A ∂β
E[m(β)|T ∈
θ=θ0

−1
∂
∂
= 0. So ∂θ
β(θ) = − AΓ∗|t0
A ∂θ
Eθ [m(β)|T ∈ S]
.

dβ J × dm J positive semidefinite matrix A,
∂
∂
S] ∂θ
β(θ) + A ∂θ
Eθ [m(β)|T ∈ S]

θ=θ0

∂
∂θ AEθ [m(β(θ))|T

θ=θ0

right-hand-side is E[Pt (X)|g(X)]. Therefore, a function b(X) is a balancing score for Dt if and only if there exists
a function h such that Pt (X) = h(g(X)).

27

The t-th element of

∂
∂θ Eθ [m(β)|T

∈ S] is

P


∂
t0 ∈S Pt0 (X; θ)
Eθ Eθ [m(Y (t); βt )|X]
∂θ
pS (θ)
θ=θ0
Z


X
Ṗt0 (x)
1
=
m(y; βt )fY (t)|X (y|x; θ)f (x; θ)Pt0 (x; θ) st (y, x) +
+ Sx (x) dydx
0
P
(x)
p
S
t
t0 ∈S
P


Pt0 (X)
Dt
0
S(Y, T, X; θ0 )
=E
m(Y ; βt ) t ∈S
Pt (X)
pS
i Ṗ 0 (X) Ṗ (X) 
X  Pt0 (X) h
t
t
+
E
E m(Y ; βt ) T = t, X
.
−
0 (X)
p
P
P
t (X)
S
t
0

θ=θ0

(12)

t ∈S

The first term comes from (11) by setting A(Y, X) = m(Y ; βt )st (Y, X). The second term
needs some calculation.



 
Ṗj (X)
Dt − Pt (X)
Dt − Pt (X)  X
S(Y, T, X; θ0 ) X = E
Dj sj (Y, X) + Dj
+ Sx (X) X
E
Pt (X)
Pt (X)
Pj (X)
j∈T


X

Ṗj (X) 
Dt 
Ṗt (X) 
=E
st (Y, X) +
Dj sj (Y, X) + Dj
X −E
X
Pt (X)
Pt (X)
Pj (X)
j∈T

= E[st (Y, X)|T = t, X] +

Ṗt (X)
−
Pt (X)

X
j∈T

 Ṗ (X)
t
Pj (X)E[sj (Y, X)|T = j, X] + Ṗj (X) =
Pt (X)
(13)

P
using (11), E[sj (Y, X)|T = j, X] = 0 ∀j ∈ T , and E[Sp (T, X)|X] =
j∈T Ṗj (X) = 0.
Therefore, by the law of iterated expectations,


i D − P (X)
Pt0 (X) h
t
t
E
E m(Y, βt ) T = t, X
S(Y, T, X; θ0 )
pS
Pt (X)


Pt0 (X)
Ṗt (X)
=E
et (X; βt )
(14)
pS
Pt (X)

i D 0 − P 0 (X)
X  Pt0 (X) h
t
t
E
E m(Y, βt ) T = t, X
S(Y, T, X; θ0 )
pS
Pt0 (X)
t0 ∈S

X  Pt0 (X)
Ṗt0 (X)
=
E
et (X; βt )
.
(15)
pS
Pt0 (X)
0
t ∈S

We discuss some intuition from the above equations and (12). The first equation (14) comes
from the fact that only the observations with treatment level t are used in the inner regression
to identify the potential outcome Y (t), E[Y (t)|X] = E[Y |T = t, X]. The second equation (15)
is from the outer expectation that uses only the observations from the treated subpopulation
{T ∈ S}.

28

It follows
P



 t0 ∈S Pt0 (X; θ)
∂
Eθ Eθ m(Y, βt ) T = t, X
∂θ
pS (θ)



= E ψ(Y, T, X)S(Y, T, X; θ0 ) .
θ=θ0

∂
So the parameter is path wise differentiable ∂θ
β(θ) = − AΓ∗|t0
−1

−1  
= − AΓ∗
AE[ψS] = E − AΓ∗
Aψ S .

B.2

−1

∂
Eθ [m(β)|T ∈ S]
A ∂θ

θ=θ0



Proof of Theorem 3

The tangent space is Hy +Hx when the propensity score isPknown. The corresponding score is

t0 ∈S Pt0 (X)
t
S(y, t, x; θ0 ) = Sy (y, t, x) + Sx (x). Define EIFP S ≡ PtD(X)
m(Y ; βt ) − et (X; βt ) +
pS
P


t0 ∈S Pt0 (X)
e
(X;
β
)
∈
H
+
H
.
It
suffices
to
show
E
(S
(Y,
T,
X)
+
S
(X))EIF
= 0.
t
t
y
x
y
x
P
S
pS
In the proof of Theorem 2, we obtain
h
X Dt0 − Pt0 (X) Pt0 (X)
i

et (X; βt ) = 0
E Sy (Y, T, X) + Sp (T, X) + Sx (X) EIFP S +
Pt0 (X)
pS
0
t ∈S

when the propensity score is unknown Ṗj (X) 6= 0. It remains to show


P
D −Pt0 (X) Pt0 (X)
(i) E (Sy (Y, T, X)+Sx (X)) t0 ∈S tP0 0 (X)
pS et (X; βt ) = 0 by the calculation in (13).
t

(ii)
P


h
i

Dt
t0 ∈S Pt0 (X)
m(Y ; βt ) − et (X; βt ) = 0
E Sp (T, X)EIFP S = E Ṗt (X) 2
pS
Pt (X)
using (11) and

P

j∈T

Ṗj (X) = 0.

(iii)
 D 0 (X)
 P 0 (X)
i
X h
t
t
E Sp (T, X)
−1
et (X; βt )
Pt0 (X)
pS
t0 ∈S
i
h P 0 Ṗ 0 (X)
i
X h Dt0 Ṗt0 (X)
t ∈S t
=
E
et (X; βt ) = E
et (X; βt )
Pt0 (X) pS
pS
0
t ∈S

is zero for two cases:
1. Ṗt0 (X) = 0, i.e., the propensity score is known. So knowledge of the propensity
score will affect the efficiency bound for estimating the treatment effects on the
treated.
When Ṗt0 (X) = 0 only for S ⊂ T , we work on the same tangent space Hy +Hp +Hx
with the score S(y, t, x; θ0 ) = Sy (y, t, x)+Sp (t, x)+Sx (x) as in the proof of Theorem
2. In this case, EIFP S is the efficient influence function by satisfying (i) and (iii).
2. S = T , i.e., the treatment
h effects for the population. We can also see
ifrom the
Dt −Pt (X)
Dt
= 0 by
following calculation: E Sp (T, X) Pt (X) m(Y ; βt ) − Pt (X) et (X; βt )
P
j∈T Ṗj (X) = 0, regardless Ṗj (X) equals zero.

29

To show the efficiency bound is reduced, we calculate the covariance
h


i
D (X)
P (X)
E EIFP S P t00(X) − 1 tp0 S et (X; βt ) = 0.
t

C



Proofs in Section 4 Estimation

The proofs in this section follow closely to the proofs of Theorem 1 and Theorem 2 in Lee
(2014).

Notation. Let (Z1 , Z1 , ..., Zn ) be an independent and identically distributed (i.i.d.) sequence of random variables taking values in a probability
R space (Z, B) with distribution P .
For some measurable function φ : Z → R, define Eφ = φdP for the empirical process at φ.
Let k · k∞ be the sup-norm, i.e.,, kf k∞ = supx∈X |f (x)|, where X is the support of X. Let
C denote a generic constant.
To employ empirical process theory as part of the estimator behavior argument, we need to
restrict the smoothness and complexity of the conditional cdf of outcomes and the generated
regressor. The smoothness class that we will use is defined next. In words, the partial
derivatives of these functions are uniformly bounded up to some specified orders.
α (S), van der Vaart and Wellner (1996) (P. 154))
Definition (CM
α
CM (S) is defined on a bounded set S in Rds as follows: For any vector q = (q1 , ..., qd ) of qd
P
q.
integers, let Dq denote the differential operator Dq = ∂sq1∂...∂sqd . Denote q. = dl=1 ql and
1

d

α to be the greatest integer strictly
smaller than α. Let kgkα = maxq.≤α sups |Dq g(s)| +

maxq.≤α sups6=s0 |Dq g(s) − Dq g(s0 )| ks − s0 kα−α where maxq.≤α denotes the maximum over
α (S) is
(q1 , ..., qd ) such that q. ≤ α and the suprema are taken over the interior of S. Then CM
the set of all continuous functions g : S ⊂ Rds 7→ R with kgkα ≤ M .
Assumption 5 (Smoothness)
(i) The data {Yi , Ti , Xi }, i = 1, ..., n, is i.i.d.. The random vector V = v0 (X) is a vector of
measurable functions of X.
(ii) The support of V , V, is a compact and convex subset of Rdv . (T, V ) has a probability
density function fT V (t, v), which is bounded away from zero and is ∆-order continuously
differentiable with respect to v, with uniformly bounded derivatives.
(iii) Suppose the unconditional distribution FY (y) is continuous on a compact support
Y ≡ [yl , yu ] ⊂ R. The conditional distribution FY |T V (y|t, v) is ∆-order continuously
differentiable with respect to v, with uniformly bounded derivatives.
α (V) with α > d /2.
(iv) For each fixed y ∈ Y and t ∈ T , FY |T V (y|t, ·) ∈ CM
v

(v) There exists a universal constant C satisfying a Hölder continuity condition: for any
t ∈ T , for any y1 , y2 ∈ Y, FY |T V (y1 |t, ·) − FY |T V (y2 |t, ·) ∞ ≤ C|(y1 − y2 |1/2 .
Assumption 6 (Kernel)
R
The kernel function k(u) : R → R satisfies the following conditions: (i) (r-order) k(u)du =
R
R
1, ul k(u)du = 0 for 0 < l < r, and |ur k(u)|du < ∞ for some r ≥ 2. (ii) (bounded support)
for some L < ∞, k(u) = 0 for |u| > L. (iii) k(u) is r-times continuously differentiable and the

30

derivatives are uniformly continuous and bounded. (iv) For an integer ∆k , the derivatives of
the kernel up to order ∆k exist and are Lipschitz.5
Assumption 7 (Nonparametric tuning parameters)
The bandwidth h satisfies (i) h → 0, (ii) nh2r → 0, and (iii) nhdv +2α / log(n) → ∞, as
n → ∞. The smoothness parameters in Assumptions 5 and 6 satisfy ∆ ≥ α + r, ∆k ≥ α,
and α > dv /2.
Assumption 8 (Generated regressor)
αv
The j-th component of v0 satisfies v0j (X) ∈ CM
(X ) with αv > dx /2, for all j = 1, ..., dv . Let
−δ
the estimator kv̂j − v0j k∞ = op (n ), for all j = 1, ..., dv . Let the second-step bandwidth
h ∼ n−η and the first-step bandwidth h1 ∼ n−g satisfying 0 < η < 1/(2dv ) and 0 < g <
1/(dx + 2αv ). 6
The accuracy of the first-step generated regressor estimation satisfies max{η + 1/4, η(1 +
dv /2)/(1 − dx /(2αv ))} < δ < (1 − gdx )/2. We choose a bias-reducing kernel with order
r1 > 1/(2g) − dx /2. 7 When the weight is estimated by Ŵ = P̂t0 (X)/p̂t0 , further let
g < (1 − ηdv )/dx .
We use a fixed trimming function that chooses a compact, interior subsupport of X such
that the estimators v̂(X) and F̂Y |T V (y|t, V ) satisfies the uniform convergence rate in Result
A.1 in Lee (2014). Therefore, the trimmed estimator consistently estimates FY |T V (y|t, V )
for the subpopulation whose observables X do not take extreme values. Then the third step
uses this subsample with the second trimming function. That is, we work on a compact
subsupport where the density functions are bounded away from zero, as in Assumption 5
(ii). So we can use the uniform linear representation
P̂t (x) − Pt (x) =

n


−1/2
1X
Dti − Pt (x) Kh1 (Xi − x) f (x) + Rnv (x) = Op (nhd1x
)
n

(16)

i=1


where kRnv k∞ = Op log n/(nhd1x ) by choose the order of the kernel r1 . We might estimate the propensity score by other nonparametric estimator that admits a uniform linear
representation.

C.1

Proofs in Sections 4.1 and 4.2

Proof of Theorem 4
We heuristically discuss the proof
P in the following. Define an infeasible estimator as if we
knew the true W , β̂ 0 = n−1 ni=1 F̂Y |T V̂ (y|t, V̂i )Wi . We suppress the dependence on y in
the notations for the estimand β(y) and estimators for simplicity. Corollary 4 in Lee (2014)
5

(iv) ensures that the estimator takes values in a function space not too complex for the stochastic equicontinuity
argument.
αv
dq
dq
6
This condition comes from supx k dx
q v̂(x) − dxq v0 (x)k = op (1) for all q ≤ αv . This ensures P rob(v̂j ∈ CM ) → 1
as n → ∞.
7
It is often assumed undersmoothing for nonparametric estimation. An example of series estimation is in Hirano,
Imbens, and Ridder (2003).

31

implies when the weight is not estimated,
n
n


1 X 0
1 X Dti Pt0 (Xi )
√
β̂ − β = √
1{Yi ≤y} − FY |T X (y|t, Xi )
P (Xi ) pt0
n
n
i=1
i=1 t

Dt0 i 
Dt0 i
− β + FY |T V (y|t, Vi ) Wi −
+ FY |T X (y|t, Xi )
+ op (1).
pt0
pt0

(17)

NowPconsider the case when the weight is estimated. The estimation error from p̂t0 =
n−1 ni=1 Bi contributes additional influence function
β−β

Bi
.
pt0

(18)

For W = Pt0 (X)/pt0 , an additional influence function comes from estimating Pt0 (X):
D 0

ti
FY |T V (y|t, Vi )
− Wi .
pt0

(19)

Combining (17), (18), and (19), we derive Theorem 4.
The following is the detail of the proof. Denote Fi = FY |T V (y|t, Vi ) and F̂i = F̂Y |T V̂ (y|t, V̂i ).
We decompose
n

1X
F̂i Ŵi − E[Fi Wi ]
n
i=1

=

n

n

n

i=1

i=1

i=1

 1X


1X
1X
F̂i Wi − E[Fi Wi ] +
Fi Ŵi − Wi +
F̂i − Fi Ŵi − Wi .
n
n
n

(20)

The first term is derived in Lee (2014) in the above (17). The third term is (nhdv )−1/2 +

hr kŴ − W k∞ by the Assumptions op (n−1/2 ). When Ŵ = P̂t0 (X)/p̂t0 , we need g < (1 −
ηdv )/dx .

Weight. For estimating the weight W = A/B, linearize Ŵi − Wi = Âi /B̂ − Ai /B =

(Âi − Ai )/B − (B̂ − B)A/B 2 + O(|B̂ − B|2 + |B̂ − B|kÂi − Ai k∞ ). The second term in (20) is
n

n

n

 1X
1X
Âi − Ai
1X
B̂ − B
−
FY |T V (y|t, Vi ) Ŵi − Wi =
FY |T V (y|t, Vi )
FY |T V (y|t, Vi )Wi
n
n
B
n
B
i=1
i=1
i=1

+ Op |B̂ − B|2 + |B̂ − B|kÂi − Ai k∞

32

where the last term is op (n−1/2 ) by |B̂ − B| = |p̂t − pt | = Op (n−1/2 ). For the second term,


n
B̂ − B
Wi
1X
= E FY |T V (y|t, Vi )
(B̂ − B)
FY |T V (y|t, Vi )Wi
n
B
B
i=1
!
n
1X
Wi
Wi
+
(B̂ − B)
FY |T V (y|t, Vi )
− E[FY |T V (y|t, Vi )]
n
B
B
i=1

n
1 X Dt0 i
=
β − β + op (n−1/2 )
n
pt0

(21)

i=1

P
when B̂ = p̂t0 = n−1 ni=1 Dt0 i .
For the first term, we use the stochastic equicontinuity argument in Theorem A.1 in Lee
(2014). 8
"
#
n
1X
Âi − Ai
Â(X) − A(X)
= E FY |T V (y|t, v0 (X))
+ op (n−1/2 ).
FY |T V (y|t, Vi )
n
B
B
i=1

When Ai = A(Xi ) = Pt0 (Xi ), by (16),
"
#
Â(X) − A(X)
E FY |T V (y|t, v0 (X))
B
#
"
n

 Kh1 (Xi − X)
1 1X
+ Op kRnv k∞
= E FY |T V (y|t, v0 (X))
Dt0 i − Pt0 (X)
Bn
f (X)
i=1

n


1X
1
=
FY |T V (y|t, Vi )
Dt0 i − Pt0 (Xi ) + Op hr11 + kRnv k∞
n
pt0

(22)

i=1

where the last term is made op (n−1/2 ).

Generated regressor. The key is that P rob(T = t|V = v0 (Xi )) = Pt (Xi ) and hence
∇v P (T = t|V = v)) v=v0 (X ) = (1, 0)> . This determines the influence of estimating Pt (X)
i
as a generated regressor. Also E[W |X = Xi ] = Pt0 (Xi )/pt0 = E[W |V = v0 (Xi )] and


>
∇v E W V = v v=v0 (X ) = 0, 1/pt0 . This determines the influence of estimating Pt0 (X)
i

8

α
In Theorem A.1 in Lee (2014), let f (y, X) = FY |T V (y|t, v0 (X))Pt0 (X). For any fixed y ∈ Y, f (y, X) ∈ CM
(X)
with α > dx /2. Then
n

1 X
sup √
FY |T V (y|t, v0 (Xi ))P̂t0 (Xi ) − FY |T V (y|t, v0 (Xi ))Pt0 (Xi )
n i=1
y∈Y
h
i
√
− n E FY |T V (y|t, v0 (X))P̂t0 (X) − FY |T V (y|t, v0 (X))Pt0 (X)
= op (1).

By 0 < g < 1/(dx + 2αv ) in Assumption 7, supx∈X

α
So P rob P̂t0 (X) ∈ CM
(X) → 1 as n → ∞.

∂α
0
∂xα P̂t (x)

33

−

∂α
0
∂xα Pt (x)


= Op (nhd1x +2αv )−1/2 + hr11 = op (1).

as generated regressor. Corollary 4 in Lee (2014) implies
n
n 



1 X
1 X
√
F̂Y |T V̂ (y|t, V̂i )Wi − β = √
FY |T V (y|t, Vi )Wi − E FY |T V (y|t, V )W
n
n
i=1
i=1





Dti
+
1
− FY |T V (y|t, Vi ) E W V = Vi + ∆ARG + ∆REG + Rn
Pt (Xi ) {Yi ≤y}

where Rn= Op n−κ1 + n−κ2 + n−rη , 0 < κ1 < (1 − dv η)/2 + (δ − η) − δdx /(2αv ), and
κ2 < min 1 − dv η, 2(δ − η) . The estimation errors associated with the generated regressor
can be decomposed to two parts: for the argument in the known regression function


∆ARG = E (v̂(X) − v0 (X))0 ∇V FY |T V (y|t, v) v=v0 (X) W
n

=

>
1X
Dti − Pt (Xi ), Dt0 i − Pt0 (Xi ) ∇v FY |T V (y|t, v)
n
i=1

v=v0 (Xi )

E[W |Xi ] + Op (kRnv k∞ + hr11 )

for the regressor that determines the regression function
∆REG

n



1X
Dti − Pt (Xi ), Dt0 i − Pt0 (Xi ) − ∇v FY |T V (y|t, v)
E W V = v0 (Xi )
=
n
v=v0 (Xi )
i=1




(23)
+ FY |T V (y|t, v0 (Xi )) − FY |T X (y|t, Xi )
− ∇v E W V = v
v=v0 (Xi )


∇v P rob(T = t|V = v)) v=v0 (X ) 

Pt (Xi )
i
E W V = v0 (Xi )
(24)
+
P (T = t|V = v0 (Xi ))
P rob(T = t|V = v0 (Xi ))
+ Op (kRnv k∞ + hr11 )

n

1X
Pt0 (Xi )
Dti − Pt (Xi ), Dt0 i − Pt0 (Xi ) − ∇v FY |T V (y|t, v)
=
n
pt0
v=v0 (Xi )
i=1


 P 0 (X )
1 >
i
t
+ FY |T V (y|t, v0 (Xi )) − FY |T X (y|t, Xi )
,−
+ Op (kRnv k∞ + hr11 ). (25)
Pt (Xi )pt0
pt0
The role of the PS P̂t0 (Xi ) is to recover the causal effects for the treated subpopulation.
From the term ∇v E W V = v in (23),
 regressing on
the PS P̂t0 (Xi ) contributes the term
FY |T X (y|t, Xi ) − FY |T V (y|t, v0 (Xi )) Dt0 i − Pt0 (Xi ) pt0 . This term recovers (15) in the
influence function . The term ∇v P rob(T = t|V = v) in (24) comes from the partial mean
that fixes the treatment level at t for the potential outcome. This term recovers (14) in the
influence function.


34

Proof of Theorem 5
From (25) in the Proof of Theorem 4, ∆REG for V̂i = (P̂t (Xi ), Pt0 (Xi ))> is

n

1X
Dti − Pt (Xi ) − ∇v FY |T V (y|t, v)
n

Pt0 (Xi )
pt0
v=v0 (Xi )
i=1


 P 0 (X )
i
t
(26)
+ Op (kRnv k∞ + hr11 ).
+ FY |T V (y|t, v0 (Xi )) − FY |T X (y|t, Xi )
Pt (Xi )pt0


The term associated with ∇v E W V = v in (23) is dropped because the true Pt0 (X) is used.
Corollary 4 in Lee (2014) implies that
√

n

1 X Dti Pt0 (Xi )
1{Yi ≤y} − FY |T X (y|t, Xi )
n β̂ − β = √
0
P (Xi ) pt
n
i=1 t

Pt0 (Xi )
Pt0 (Xi ) 
+ FY |T X (y|t, Xi )
− β + FY |T V (y|t, Vi ) Wi −
+ op (1).
pt0
pt0
0



By (21), the estimation
error from pt0 contributes an additional term to the influence function

β − βPt0 (Xi ) pt0 . The result is derived.


C.2

Proof of Theorem 6

(a) By the proof of Theorem 4, the estimation error from the weight contributes
 (21) and
√
√
p
− β have the
(22). Theorem 1 in Lee (2014) implies that n β̂X − β and n β̂X
following linear representation
n


1 X
Dti Pt0 (Xi )
√
FY |T X (y|t, Xi )Wi − β +
1{Yi ≤y} − FY |T X (y|t, Xi )
Pt (Xi ) pt0
n
i=1


Dt0 i
Dt0 i
+β−β
+ FY |T X (y|t, Xi )
− Wi + op (1).
pt0
pt0
(b)
√

n

1 X
Pt0 (Xi )
−β
n β̃x − β = √
FY |T X (y|t, Xi )
pt0
n
i=1


Dti Pt0 (Xi )
Pt0 (Xi )
+
1{Yi ≤y} − FY |T X (y|t, Xi ) + β − β
+ op (1).
Pt (Xi ) pt0
pt0


C.3

Proofs in Sections 4.4 and 4.5

Proof of Theorem 7
Let PS (x) = Pt (x)+Pt0 (x). We first derive the uniform linear representation of the generated

35

regressor,
Pn

− x)
Pt (x)f (x)
−
PS (x)f (x)
i=1 (Dti + Dt0 i )Kh1 (Xi − x)
P
n
n
n−1 i=1 Dti Kh1 (Xi − x)
bt (x) 1 X
=
−
(Dti + Dt0 i )Kh1 (Xi − x) + Rnv (x)
PS (x)f (x)
PS (x)f (x) n

b̂t (x) − bt (x) = Pn

i=1 Dti Kh1 (Xi

i=1

n
1 X Dti Kh1 (Xi − x)bt0 (x) Dt0 Kh1 (Xi − x)bt (x)
−
+ Rnv (x)
=
n
PS (x)f (x)
PS (x)f (x)
i=1

where kRnv k∞ = Op (log n/(nhd1x )). By the result of Corollary 4 in Lee (2014), the estimation error associated with b̂t (X) is ∆ARG + ∆REG . Observe that bt (X) + bt0 (X) = 1 and
∂
E[W |bt (X), T ∈ {t, t0 }] = (1 − bt (X))/bt0 . So ∂b
E[W |bt (X) = b, T ∈ {t, t0 }] = −1/bt0 . And
0
P rob(T = t|V = v, T ∈ {t, t }) = bt (x). So ∇v P rob(T = t|V = v, T ∈ {t, t0 })|v=bt (x) = 1. In
(23) and (24), 1/bt0 + (1 − bt (X))/(bt0 bt (x)) = 1/(bt (x)bt0 ). So ∆ARG + ∆REG




1
0
= E b̂t (X) − bt (X) FY |T V (y|t, V (X)) − FY |T X (y|t, X)
T ∈ {t, t }
bt (X)bt0
n

1 X  Dti bt0 (Xi ) Dt0 bt (Xi ) 
PS (Xi )
1
=
−
FY |T V (y|t, V (Xi )) − FY |T X (y|t, Xi )
n
PS (Xi )
PS (Xi )
bt (Xi )bt0 pS
i=1

+ Op (kRnv k∞ + hr11 )
n

1
Pt0 (Xi )
1 X
− Dt0 FY |T V (y|t, V (Xi )) − FY |T X (y|t, Xi )
+ Op (kRnv k∞ + hr11 )
Dti
=
n
Pt (Xi )
pt0
i=1

that is the same as (25) in the proof of Theorem 4.



Proof of Theorem 8
Let PS (x) = Pt (x)+Pt0 (x). We first derive the uniform linear representation of the generated
regressor,
Pn
Pt (x)f (x)
i=1 Dti Kh1 (Xi − x)
−
b̃t (x) − bt (x) = Pn
PS (x)f (x)
i=1 (Dti + Pt0 (x))Kh1 (Xi − x)
Pn
n
−1
n
bt (x) 1 X
i=1 Dti Kh1 (Xi − x)
=
−
(Dti + Pt0 (x))Kh1 (Xi − x) + Rnv (x)
PS (x)f (x)
PS (x)f (x) n
i=1
Pn
n
−1
0
0
n
bt (x)Pt (x) 1 X
i=1 Dti Kh1 (Xi − x)bt (x)
=
−
Kh1 (Xi − x) + Rnv (x)
PS (x)f (x)
PS (x)f (x) n
i=1

n
X
0
D
−
P
(x)
K
(X
−
x)b
(x)
1
ti
t
i
h1
t
=
+ Rnv (x)
n
PS (x)f (x)
i=1

where kRnv k∞ = Op (log n/(nhd1x )). The third and forth equalities use 1 − bt (x) = bt0 (x) and
bt (x)Pt0 (x) = bt0 (x)Pt (x).

36

Following the proof of Theorem 7, ∆ARG + ∆REG




1
0
= E b̃t (X) − bt (X) FY |T V (y|t, V (X)) − FY |T X (y|t, X)
T ∈ {t, t }
bt (X)bt0

n

PS (Xi )
1
1 X Dti − Pt (Xi ) bt0 (Xi )
FY |T V (y|t, V (Xi )) − FY |T X (y|t, Xi )
=
n
PS (Xi )
bt (Xi )bt0 pS
i=1

+ Op (kRnv k∞ + hr11 )
n

 Pt0 (Xi )
1X
=
+ Op (kRnv k∞ + hr11 )
Dti − Pt (Xi ) FY |T V (y|t, V (Xi )) − FY |T X (y|t, Xi )
n
Pt (Xi )pt0
i=1

that is the same as (26) in the proof of Theorem 5.



Proof of Theorem 9
The key is the term ∇v E[W |V = v] in (23). When V = P1 (X), ∇v E[W |V = v] =
∇v P0 (X)/p0 = ∇v (1 − P1 (X))/p0 = −1/p0 . Using the uniform linear representation (16),
∆ARG + ∆REG




 1
P0 (X)
= E P̂1 (X) − P1 (X) FY |T V (y|1, V (X)) − FY |T X (y|1, X)
+
p0 P1 (X)p0
n


1X
1
=
D1i − P1 (Xi ) FY |T V (y|1, V (Xi )) − FY |T X (y|1, Xi )
+ Op (kRnv k∞ + hr11 ).
n
P1 (Xi )p0
i=1

P
Comparing with (26) and following the proof of Theorem 5, we obtain β̌−β = n−1 ni=1 ψtP S +


0 (Xi )
−1/2 ). A simple algebra
D1i − P1 (Xi ) FY |T V (y|t, V (Xi )) − FY |T X (y|t, Xi ) 1−P
P1 (Xi )p0 + op (n
shows that the covariance of the second term and ψtP S is zero.


C.4

Proof of Theorem 10


Define the class of measurable functions H = { Y × T × X → ψ(Y, T, X; y) : y ∈ Y}. By
Lemma A2 in Donald and Hsu (2014) and the Assumptions in the Appendix, H is P -Donsker.
The weak convergence is implied by Donsker’s Theorem in Section 2.8.2 in van der Vaart
and Wellner (1996). We use the Functional Delta Method in Section 3.9 in van der Vaart
and Wellner (1996).


References
Abadie, A. and G. W. Imbens (2012). Matching on the estimated propensity score. Working
paper.
Ao, W., S. Calonico, and Y.-Y. Lee (2014). Evaluating the effects of lengths of participation
in the workforce investment act adult program via decomposition analysis. Working paper.
Bhattacharya, D. (2007). Inference on inequality from household survey data. Journal of
Econometrics 137 (2), 674–707.

37

Busso, M., J. DiNardo, and J. McCrary (2014). New Evidence on the Finite Sample Properties of Propensity Score Matching and Reweighting Estimators. The Review of Economics
and Statistics, forthcoming (forthcoming).
Cattaneo, M. D. (2010). Efficient semiparametric estimation of multi-valued treatment effects
under ignorability. Journal of Econometrics 155 (2), 138–154.
Chen, X., H. Hong, and A. Tarozzi (2008). Semiparametric efficiency in gmm models with
auxiliary data. The Annals of Statistics 36 (2), pp. 808–843.
Chernozhukov, V., I. Fernández-Val, and B. Melly (2013). Inference on counterfactual distributions. Econometrica 81 (6), 2205–2268.
Dehejia, R. H. and S. Wahba (2002). Propensity score matching methods for nonexperimental causal studies. Review of Economics and statistics 84 (1), 151–161.
Donald, S. G. and Y.-C. Hsu (2014). Estimation and inference for distribution functions and
quantile functions in treatment effect models. Journal of Econometrics 178, Part 3 (0),
383–397.
Donald, S. G., Y.-C. Hsu, and G. F. Barrett (2012). Incorporating covariates in the measurement of welfare and inequality: methods and applications. The Econometrics Journal 15 (1), C1–C30.
Escanciano, J. C., D. T. Jacho-Chávez, and A. Lewbel (2014). Uniform convergence of
weighted sums of non and semiparametric residuals for estimation and testing. Journal of
Econometrics 178 (3), 426 – 443.
Firpo, S. (2007). Efficient semiparametric estimation of quantile treatment effects. Econometrica 75 (1), 259–276.
Firpo, S. and C. Pinto (2011). Identification and estimation of distributional impacts of
interventions using changes in inequality measures. Working paper.
Frölich, M. (2004a, February). Finite-Sample Properties of Propensity-Score Matching and
Weighting Estimators. The Review of Economics and Statistics 86 (1), 77–90.
Frölich, M. (2004b, 04). Programme Evaluation with Multiple Treatments. Journal of Economic Surveys, Wiley Blackwell 18 (2), 181–224.
Graham, B. S. (2011). Efficiency bounds for missing data models with semiparametric restrictions. Econometrica 79 (2), 437–452.
Hahn, J. (1998). On the role of the propensity score in efficient semiparametric estimation
of average treatment effects. Econometrica 66 (2), 315–332.
Hahn, J. and G. Ridder (2013). The asymptotic variance of semi-parametric estimators with
generated regressors. Econometrica 81 (1), 315–340.
Heckman, J. J., H. Ichimura, and P. Todd (1998). Matching as an econometric evaluation
estimator. Review of Economic Studies 65 (2), 261–94.

38

Heckman, J. J., R. J. LaLonde, and J. A. Smith (1999). The economics and econometrics of
active labor market programs. Handbook of labor economics 3, 1865–2097.
Heckman, J. J. and E. J. Vytlacil (2007, January). Econometric evaluation of social programs,
part i: Causal models, structural models and econometric policy evaluation. In J. Heckman
and E. Leamer (Eds.), Handbook of Econometrics, Volume 6 of Handbook of Econometrics,
Chapter 70-71. Elsevier.
Hirano, K. and G. W. Imbens (2004). The propensity score with continuous treatments. In
A. Gelman and X.-L. Meng (Eds.), Applied Bayesian Modeling and Causal Inference from
Incomplete-Data Perspectives, pp. 73–84. New York: Wiley.
Hirano, K., G. W. Imbens, and G. Ridder (2003). Efficient estimation of average treatment
effects using the estimated propensity score. Econometrica 71 (4), 1161–1189.
Imai, K. and D. van Dyk (2004). Causal inference with general treatment regimes: Generalizing the propensity score. Journal of the American Statistical Association 99 (467),
854–866.
Imbens, G. (2000). The role of the propensity score in estimating dose-response functions.
Biometrika 87 (3), 706–710.
Imbens, G. W. and J. M. Wooldridge (2009, September). Recent developments in the econometrics of program evaluation. Journal of Economic Literature 47 (1), 5–86.
Lechner, M. (2001). Identification and estimation of causal effects of multiple treatments
under the conditional independence assumption. In M. Lechner and F. Pfeiffer (Eds.),
Econometric Evaluation of Labour Market Policies, Volume 13 of ZEW Economic Studies,
pp. 43–58. Physica-Verlag HD.
Lechner, M. (2002). Program heterogeneity and propensity score matching: An application
to the evaluation of active labor market policies. Review of Economics and Statistics 84 (2),
205–220.
Lee, Y.-Y. (2014). Partial mean processes with generated regressors: Continuous treatment
effects and nonseparable models. Working paper.
Mammen, E., C. Rothe, and M. Schienle (2014). Semiparametric estimation with generated
covariates. working paper.
Rosenbaum, P. R. and D. B. Rubin (1983). The central role of the propensity score in
observational studies for causal effects. Biometrika 70 (1), 41–55.
Rothe, C. (2010). Nonparametric estimation of distributional policy effects. Journal of
Econometrics 155, 56–70.
Roy, A. D. (1951). Some thoughts on the distribution of earnings. Oxford Economic Papers 3 (2), 135–146.
Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology 66 (5), 688–701.

39

van der Vaart, A. (2000). Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.
van der Vaart, A. W. and J. A. Wellner (1996). Weak Convergence and Empirical Processes:
with Application to Statistics. New York: Springer-Verlag.

40

