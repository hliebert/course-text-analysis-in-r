J. Econom. Meth. 2015; 4(1): 153–161

Practitioner’s Corner
Christopher J. Elias*

Percentile and Percentile-t Bootstrap
Confidence Intervals: A Practical Comparison
Abstract: This paper employs a Monte Carlo study to compare the performance of equal-tailed bootstrap
percentile-t, symmetric bootstrap percentile-t, bootstrap percentile, and standard asymptotic confidence
intervals in two distinct heteroscedastic regression models. Bootstrap confidence intervals are constructed
with both the XY and wild bootstrap algorithm. Theory implies that the percentile-t methods will outperform
the other methods, where performance is based on the convergence rate of empirical coverage to the nominal
level. Results are consistent across models, in that in the case of the XY bootstrap algorithm the symmetric
percentile-t method outperforms the other methods, but in the case of the wild bootstrap algorithm the two
percentile-t methods perform similarly and outperform the other methods. The implication is that practitioners that employ the XY algorithm should utilize the symmetric percentile-t interval, while those who opt for
the wild algorithm should use either of the percentile-t methods.
Keywords: bootstrap; confidence interval; Monte Carlo.
JEL Codes: C01; C12; C15; C20; C23
DOI 10.1515/jem-2013-0015
Previously published online July 10, 2014

1 Introduction
Since its re-introduction by Efron (1979), the bootstrap has become a useful tool for the empirical economist,
highlighted by the fact that the technique is standard in many widely available commercial econometric and
statistical software packages. One useful application of the bootstrap is the calculation of confidence intervals in regression models. Although the literature proposes many methods of forming bootstrap confidence
intervals, two general methods, detailed in Hall (1997), Horowitz (2001) and MacKinnon (2002), are the percentile and percentile-t methods. The percentile method chooses appropriate quantiles of an ordered bootstrap sample of coefficient estimates to form the interval, while the percentile-t method chooses appropriate
quantiles of an ordered bootstrap sample of t-statistics to use as critical values in constructing the interval.
Hall (1997) provides theory that implies that in a regression model, as sample size increases, the empirical
coverage (i.e., the percentage of the time the interval contains the true parameter) of an interval formed by
the percentile-t method will converge to the nominal level faster than intervals constructed with either the
percentile method or standard asymptotics. This paper tests this implication through a Monte Carlo simulation study.
The majority of the simulation evidence supporting Hall’s theoretical finding focuses on a hypothesis
testing framework, of which MacKinnon (2002) provides a discussion, while Flachaire (2005) and Cameron,
Gelbach, and Miller (2008) are specific examples. This paper contributes by focusing instead on a confidence
interval framework in the same manner as an experiment conducted by MacKinnon (2002) demonstrating
*Corresponding author: Christopher J. Elias, Department of Economics, 703 Pray-Harrold, Eastern Michigan University,
Ypsilanti, MI, 48197, USA, E-mail: cjelias@gmail.com

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

154

C.J. Elias: Percentile and Percentile-t Bootstrap Confidence Intervals

that the empirical coverage of a percentile-t interval in a heteroscedastic regression model converges to the
nominal level faster than a standard asymptotic interval. This paper extends MacKinnon’s type of analysis
in two ways. First, it incorporates a percentile interval, an equal-tailed percentile-t interval, and a symmetric
percentile-t interval.1 These bootstrap methods were selected because they are relatively well known in the
literature and their inclusion facilitates direct comparison between the different bootstrapping methods and
standard asymptotics. Second, the study examines both the random effects panel data model as well as the
linear model. In both cases heteroscedasticity as a function of the independent variable is present, and the
wild and XY (i.e., paired) bootstrap algorithms are employed to provide consistent estimates of the sampling
distributions of interest.2 Additionally, estimation methods consistent with typical applied work are utilized.
Confidence interval performance is evaluated by analyzing the rate of convergence of empirical coverage to
the nominal level as sample size increases.
The results, supportive of Hall’s findings, are broken up by bootstrap algorithm and are consistent for
both models. In the case of the XY algorithm, the symmetric percentile-t method outperforms the other
methods, but in the case of the wild algorithm, the two percentile-t methods perform similarly and outperform the other methods. The implication is that in settings similar to those studied, practitioners that employ
the XY algorithm should utilize the symmetric percentile-t interval, while those who opt for the wild algorithm should use either of the percentile-t methods.
The paper is organized as follows: Section 2 provides background on the bootstrap, as well as the
methods of percentile and percentile-t confidence interval calculation. Section 3 explains the format of the
Monte Carlo study, the model formulations, and presents results. Section 4 concludes. Appendix A provides
background information on bootstrap algorithms, while Appendix B discusses the robust standard error estimation techniques used in the study.

2 Background of the Bootstrap and Bootstrap Confidence Intervals
If F is the population distribution generating a data sample of size n and G(F) is the sampling distribution of
an estimator ˆθ, then the goal of the bootstrap is to approximate G(F) with G( Fˆ ), where F̂ is an estimator
of F. The process involves repeatedly generating bootstrap samples of size n from Fˆ , calculating an estimate
of θ̂ for each sample, and then using the distribution of estimates to represent G( Fˆ ). Two requirements are
necessary for the consistency of G( Fˆ ). First, F̂ must be consistent for F, which is usually fulfilled by letting
F̂ be the empirical distribution function, which equates to randomly sampling with replacement from the
original data. Second, G must be a smooth function of F, which implies that small changes in F do not cause
large changes in G(F). The linear relationship between the coefficients and the error term in a regression
model satisfies this condition.3
The bootstrap has excellent theoretical properties when applied to asymptotically pivotal quantities.4
In general, as n increases, the error of the bootstrap approximation converges to zero at the same rate as
standard asymptotics, n . However, Hall (1997) uses Edgeworth expansions to demonstrate in a confidence
interval setting that if θ̂ is asymptotically pivotal then the error converges to zero at rate n for equal-tailed
intervals and n2 for symmetric intervals.5 In the linear regression model the t-statistic of a coefficient estimate
1 A key difference between the two percentile-t intervals is in how they are constructed. The equal-tailed interval is not necessarily symmetric about the slope coefficient estimate, while the symmetric interval is. Additionally, the percentile method
requires symmetry of the first-order asymptotic distribution.
2 A theoretical difference between the two algorithms is the imposition of the conditional expectation of the residual; The XY
algorithm conditions on the bootstrap sample of independent data, while the wild algorithm conditions on the original independent data. See Appendix A for background information on these algorithms.
3 Excellent sources of the theoretical background of the bootstrap are available in Efron and Tibshirani (1993), Hall (1997), and
Horowitz (2001).
4 Asymptotically pivotal quantities are functions of the data and unknown parameters whose probability distributions do not
depend on any unknown parameters.
5 See also Horowitz (2001).

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

C.J. Elias: Percentile and Percentile-t Bootstrap Confidence Intervals

155

is asymptotically pivotal and is used to construct confidence intervals and perform hypothesis tests. Hall’s
results imply that both of the percentile-t methods should produce intervals that converge to the nominal
level at a faster rate than do intervals constructed from either the percentile method or through standard
asymptotics.
The general setting under consideration is the linear regression model,
y = X β +e 

(1)

where y is an n × 1 vector of dependent variable data, X is an n × K matrix of independent variable data with
full rank, β is a K × 1 vector, n > K, and  is an n×1 vector of error terms. The error term has an expected value
conditioned on X of zero and a finite variance.

2.1 Calculating Percentile and Percentile-t Bootstrap Confidence Intervals
Define b as a consistent estimator of the parameter vector β using all of the observed data, βk as the kth
element of β, bk as the kth element of b, SEb as the standard error of bk, and bk* as a consistent bootstrap
k
*
}. To construct
replication of bk. Let B be a vector of R ordered (from smallest to largest) bk* s, B ≡ { bk* 1 , bk* 2 ,… , bkR
a level α percentile bootstrap confidence interval for bk, form the interval


*
b*

,
b
 k ( R+ 1) α2 k ( R+ 1) 1− α2  



(2)

For example, if there are R = 999 ordered bootstrap replications and α = 0.1 then the lower bound of the confidence interval is the fiftieth observation of B and the upper bound is the 950th observation of B.6
The bootstrap test statistic tk* is calculated as
tk* =
where SE

bk*
*
k1

bk* − bk

SE

bk*

is the standard error of the bootstrap replication of bk* .7 Let BT1 be a vector of R ordered

*
tk* s, BT 1 ≡ { t , tk* 2 ,… ,tkR
}. To form a level α equal-tailed percentile-t bootstrap confidence interval for bk, form
the interval



bk −SE b t *

, bk −SE b t *




α
α
k k ( R+ 1) 1−
k k ( R+ 1)

 2 
 2  





(3)

For example, if there are R = 999 ordered bootstrap replications and α = 0.1 then the lower bound of the
confidence interval uses the 950th observation of BT1 and the upper bound uses the fiftieth observation
of BT1.
Let tk′ ≡ tk* and BT2 be a vector of R ordered t′ks, BT 2 ≡ { tk′ 1 ,tk′ 2 ,… , tkR
′ }. To form a level α symmetric percentile-t bootstrap confidence interval for bk, form the interval
[ bk −SE b tk′ ( R+ 1)(1− α ) , bk + SE b tk′ ( R+ 1)(1− α ) ]
k

k

(4)



For example, if there are R = 999 ordered bootstrap replications and α = 0.1 then both bounds of the confidence
interval use the 900th observation of BT2.

6 See Efron and Tibshirani (1993) for more details.
7 Note that calculation of this statistic requires computing a standard error of bk* for each bootstrap replication.

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

156

C.J. Elias: Percentile and Percentile-t Bootstrap Confidence Intervals

3 Monte Carlo Study Format, Models, and Results
Consider the terminology of Section 2 and let M represent the number of Monte Carlo repetitions. The general
format of the simulations is
1. Set values of β, n, R, M, and X.
2. Generate the error term and then generate the dependent variable data using the error term and X.
3. Calculate bk, SEb , and a level α asymptotic confidence interval for βk.
4. Conduct a bootstrap replication and calculate bk* , SEb* and tk* .
k
5. Repeat step four R times.
6. Form B and calculate a level α percentile bootstrap confidence interval for βk based on interval (2).
7. Form BT1 and calculate a level α equal-tailed percentile-t bootstrap confidence interval for βk based on
interval (3).
8. Form BT2 and calculate a level α symmetric percentile-t bootstrap confidence interval for βk based on
interval (4).
9. Repeat steps two through eight M times.
10. Determine the number of times βk is contained in the intervals formed in steps three, six, seven, and eight
and divide by M to obtain empirical coverage rates.
k

Independent variable data are drawn from the standard lognormal distribution which will tend to produce
a few extreme observations. One set of data is used for all simulations. For example, simulations using a
sample size of n simply use the first n values from the independent variable data set. The Matlab random
number generator seed state is set to seven.

3.1 The Heteroscedastic Linear Model
Consider the model formulated in equation (1) where the conditional distribution of the error term is

e | X ~ N (0n , Ω )
and Ω is an n × n matrix. The design specifications are: yi = β1+Xi β2+σii, i = 1, 2, …, n, K = 2, β1 = 1, β2 = 1, confidence intervals are generated for β2 (k = 2), i is randomly drawn from the standard normal distribution, σi = Xi,
α = 0.1, R = 999, M = 1000, b2 and b2* are calculated by OLS, and SEb and SE * are calculated using the bias2

b2

corrected heteroscedastic consistent covariance matrix estimator (HCCME) of Eicker (1967) and White (1980)
(See Appendix B.1 for details). Figures 1 and 2 present the results.
0.9
0.85

Empirical coverage

0.8
0.75
0.7
0.65
0.6
0.55

Asymptotic
Percentile
Equal-Tailed percentile-t
Symmetric percentile-t

0.5
0.45

50

100

150
Sample size

200

250

300

Figure 1 Linear Model: XY Bootstrap Convergence.

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

C.J. Elias: Percentile and Percentile-t Bootstrap Confidence Intervals

157

0.9
0.85

Empirical coverage

0.8
0.75
0.7
0.65
0.6
0.55

Asymptotic
Percentile
Equal-Tailed percentile-t
Symmetric percentile-t

0.5
0.45

50

100

150
Sample size

200

250

300

Figure 2 Linear Model: Wild Bootstrap Convergence.

For the XY bootstrap algorithm the equal-tailed percentile-t and percentile bootstrap confidence interval
methods appear to be converging to the nominal level at about the same rate, while the asymptotic method
only slightly lags behind. The symmetric percentile-t method converges to the nominal level the fastest of all
methods, and tends to slightly over-cover at several sample sizes.
In the case of the wild bootstrap, the percentile and asymptotic methods appear to be converging at the
same rate, while the percentile-t methods perform similarly and converge significantly faster than the other
methods.

3.2 The Heteroscedastic Random Effects Panel Data Model
The random effects (RE) panel data model is characterized by data that follows N members of a cross section
through T time periods and is formulated as
yit = xit′ β + γ + ui +eit
where yit is the observation of the dependent variable for member i at time t, xit is a (K-1) × 1 vector of observations of the K-1 independent variables for member i at time t, β is a (K-1) × 1 vector of parameters to be estimated, γ is a constant, ui is the unobserved individual heterogeneity for member i, and it is the error term for
member i at time t.
The model assumes that the unobserved individual heterogeneity is uncorrelated with the independent
variables, implying that the individual heterogeneity can be included into it to form a compound error term
with the following characteristics: ηit = it+ui, E [ η2 | X ] = σ e2 + σ u2 , E [ ηit ηis | X ] = σ u2 for t≠s, E[ηitηjs|X] = 0 for all t
and s if i≠j.8
σ e2 and σ u2 are the variances of it and uit, respectively. For the T observations for each cross section
member let ∑≡[ηiηi′|X]. Then,
 σ e2 + σ u2
σ u2
σ u2  σ u2 


2
2
2
σ e + σ u σ u2  σ u2 
 σ
Σ=  u




 σ u2
σ u2
σ u2  σ e2 + σ u2 
8 See Greene (2007).

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

158

C.J. Elias: Percentile and Percentile-t Bootstrap Confidence Intervals

0.9

Empirical coverage

0.85

0.8

0.75

Asymptotic
Percentile
Equal-Tailed percentile-t
Symmetric percentile-t

0.7

0.65

5

10

15

20

25
30
35
40
Cross section size

45

50

55

60

Figure 3 Random Effects Model: XY Bootstrap Convergence.

The resulting formulation of the model is
yit = γ + Xit β 1 + θit ηit

(5)



The design specifications are: i = 1, 2, …, N, t = 1, 2, …, T, T = 5, K = 2, γ = 1, β1 = 1, confidence intervals are generated for β1 (k = 1), σ e2 = 1, σ u2 = 9, ηi is randomly drawn from a N5(05, ∑) distribution, θit = Xit, α = 0.1, R = 999,
and M = 1000. b1, and b1* are calculated using OLS while SEb and SE * are calculated with clustered robust
1

b1

standard errors, where clustering is done by cross section member and details are provided in Appendix B.2.
Figures 3 and 4 present the results.
The results for this model are similar to those of Section 3.1. In the case of the XY bootstrap, the equaltailed percentile-t method covers at levels very close to the nominal level at cross section sizes ten to twenty,
but then covers at levels similar to the percentile method at larger sample sizes. In terms of conversion, the
equal-tailed percentile-t, percentile, and asymptotic methods appear to be converging at about the same rate.
On the other hand, the symmetric percentile-t method converges the fastest and tends to slightly over-cover
at several sample sizes.

0.9

Empirical coverage

0.85

0.8

0.75

Asymptotic
Percentile
Equal-Tailed percentile-t
Symmetric percentile-t

0.7

0.65

5

10

15

20

25
30
35
40
Cross section size

45

50

55

60

Figure 4 Random Effects Model: Wild Bootstrap Convergence.

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

C.J. Elias: Percentile and Percentile-t Bootstrap Confidence Intervals

159

For the wild bootstrap, the percentile and the asymptotic methods perform similarly, while the percentile-t methods perform similarly and converge faster than the other methods.

3.3 Explaining the Results and Implications
The results are consistent for both models. In the case of the XY algorithm, the symmetric percentile-t outperforms the other methods, but in the case of the wild algorithm the two percentile-t methods perform similarly
and outperform the other methods. As discussed in Section 2, Hall (1997) uses Edgeworth expansions to show
that the coverage errors of the percentile, equal-tailed percentile-t, and symmetric percentile-t methods are
n–1/2, n–1, and n–2, respectively. This is in contrast to an asymptotic interval that has a coverage error of n–1/2.
These facts imply that in this study the symmetric percentile-t method should outperform the other methods,
all else being equal.9 While this was the result that was obtained in the case of the XY algorithm, in the case
of the wild algorithm the two percentile-t methods performed similarly. A possible explanation for this result
should originate from a comparison between the two bootstrap algorithms.
There is evidence in the literature that demonstrates the superiority of the wild algorithm over the XY
algorithm. For example, Mammen (1993) shows that the higher-order accuracy of the wild algorithm is better
than that of the XY algorithm in a model where the number of regressors is increasing in the sample size.
Other examples include Flachaire (2005), MacKinnon (2002), and Horowitz (2001). If, in the present paper,
the wild algorithm is providing improved higher-order accuracy over the XY algorithm, then a possible explanation for the similar performance of the two percentile-t intervals is that the increased accuracy of the wild
algorithm (over that of the XY algorithm) is reducing the difference in the actual (i.e., empirically observed)
coverage errors between the two percentile-t confidence interval methods. I leave it to future work to analyze
this issue further by directly comparing the performance of the two bootstrap algorithms in a confidence
interval setting.
The implication of the results is that practitioners that employ the XY algorithm should utilize the symmetric percentile-t interval, while those who opt for the wild algorithm should use either of the percentile-t
methods.

4 Conclusion
This paper has compared through a Monte Carlo study the performance of bootstrap confidence intervals generated through percentile, equal-tailed percentile-t, and symmetrical percentile-t methods based on both the
XY and wild bootstrap algorithms. Two distinct heteroscedastic regression model scenarios were considered,
and estimation methods consistent with typical applied work were used. Theory implies that the percentile-t
methods should perform better in small samples than the percentile method and the standard method based
on asymptotics. Interval performance was judged by analyzing the rate of convergence of empirical coverage
to the nominal level.
The results were consistent for both models. For the XY algorithm the symmetric percentile-t method
outperformed the other methods, but in the case of the wild algorithm both of the percentile-t methods performed similarly and outperformed the other methods. The implication is that in settings similar to those
studied, practitioners that employ the XY algorithm should utilize the symmetric percentile-t interval, while
those who opt for the wild algorithm should use either of the percentile-t methods.

9 As pointed out by a referee, this is conditional on the ability of the Edgeworth expansion to provide a good quality higher-order
asymptotic approximation. The benefit of the percentile and equal-tailed percentile-t methods is that they rely less on Edgeworth
expansion arguments.

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

160

C.J. Elias: Percentile and Percentile-t Bootstrap Confidence Intervals

Acknowledgments: I am grateful to Professors David Brownstone, William Branch, Fabio Milani, and Dale
Poirier for many useful comments and suggestions. I am also grateful for comments from a referee. All errors
are my own.

Appendix A: Bootstrap Algorithms
Consider the linear regression model of equation (1) and the terminology of Section 2. Define e as the residual
vector formed by
e = y − Xb (A.1)
and ei (i = 1, 2, …, n) as the ith observation of e.
XY (Paired) Bootstrap Algorithm
1. Randomly sample n values with replacement from the X matrix and the corresponding values from the
y vector. This is the bootstrap sample.
2. Use the bootstrap sample to calculate bk* , SE * , and any other necessary values.
bk
3. Repeat steps one and two R times.
Wild Bootstrap Algorithm
1. For each observation ei calculate Fi where Fi is equal to –ei with probability 1/2 and ei with ­probability 1/2.
2. Form a vector from the values calculated in step one and define this vector as e*.
3. Calculate y* = Xb+e*. X and y* constitute the bootstrap sample.
4. Use the bootstrap sample to calculate bk* , SE * , and any other necessary values.
bk
5. Repeat steps one through four R times.10
To remove small sample bias and heteroscedasticity divide the residuals in the wild bootstrap algorithm
by 1 − hi where hi is the diagonal of X(X′X)–1 X′. The wild boostrap imposes the correct moment condition
E(e*|X) = 0, while the XY bootstrap imposes E(e*|X*) = 0.
To apply the XY algorithm to panel data randomly sample with replacement members of the cross section,
selecting all T observations of the dependent and independent variables associated with each cross section
member. In this way all T observations associated with a particular member of the cross section become part
of the bootstrap sample. For the wild algorithm treat ei in equation (A.1) as a vector of T residuals associated
with cross-section member i.11

Appendix B: Robust Standard Error Estimation Techniques
Consider the linear regression model of equation (1), the terminology of Section 2, and equation (A.1). The
heteroscedastic consistent covariance matrix estimator (HCCME) is given as
n
1 n
[ n( X ′X ) −1 ∑( ei2 xi xi′ )( X ′X ) −1 ]
n−k
n i= 1



10 The wild bootstrap was originally proposed by Wu (1986). This version is from MacKinnon (2002).
11 Specific details are provided in Brownstone and Valletta (2001).

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

(B.1)

C.J. Elias: Percentile and Percentile-t Bootstrap Confidence Intervals

161

The estimate of the standard error of bk is the square root of the kth diagonal element of the matrix in
(B.1).
Consider the model and definitions of Section 3.2. The clustered robust covariance matrix estimator is
given as
−1

−1

N
  N N
 N

ˆ iw
ˆ i′Xi  ∑Xi′Xi 
Xi′w
∑Xi′Xi  
∑
 i=1
  N − 1 i=1
  i=1
 

(B.2)

ˆ i is a vector of T OLS residuals for member i. The
where Xi is a T×K matrix of covariates for member i and w
estimate of the standard error of bk is the square root of the kth diagonal element of the matrix in (B.2).12

References
Brownstone, D., and R. Valletta. 2001. “The Bootstrap and Multiple Imputations: Harnessing Increased Computing Power for
Improved Statistical Tests.” The Journal of Economic Perspectives 15 (4): 129–141.
Cameron, A. C., J. B. Gelbach, and D. L. Miller. 2008. “Bootstrap-Based Improvements for Inference with Clustered Errors.” The
Review of Economics and Statistics 90 (3): 414–427.
Efron, B. 1979. “Bootstrap Methods: Another Look at the Jacknife.” The Annals of Statistics 7 (1): 1–26.
Efron, B., and R. Tibshirani. 1993. An Introduction to the Bootstrap. Boca Raton, FL: CRC Press.
Eicker, F. 1967. “Limit Theorems for Regression with Unequal and Dependent Errors.” In Proceedings of the Fifth Berkeley
Symposium on Mathematical Statistics and Probability, edited by Lucien Lecam and Jerzy Neyman, Volume 1, 59–82.
Oakland, CA: University of California Press.
Flachaire, E. 2005. “Bootstrapping Heteroskedastic Regression Models: Wild Bootstrap vs. Pairs Bootstrap.” Computational
Statistics and Data Analysis 49 (2): 361–376.
Greene, W. H. 2007. Econometric Analysis. 6th ed. Upper Saddle River, NJ: Prentice Hall.
Hall, P. 1997. The Bootstrap and Edgeworth Expansion. New York, NY: Springer.
Horowitz, J. 2001. Handbook of Econometrics, volume 5, chapter 52: The Bootstrap, 3159–3228. Philadelphia, PA: Elesevier
Science B.V.
MacKinnon, J. G. 2002. “Bootstrap Inference in Econometrics.” Canadian Journal of Economics 35 (4): 615–645.
Mammen, E. 1993. “Boostrap and Wild Boostrap for High Dimensional Linear Models.” The Annals of Statistics 21 (1): 255–285.
White, H. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.”
Econometrica 48 (4): 817–838.
Wu, C. F. J. 1986. “Bootstrap and Other Resampling Methods in Regression Analysis.” The Annals of Statistics 14 (4): 1261–1295.

12 These versions of the covariance matrix estimators are from Greene (2007).

Brought to you by | Harvard University
Authenticated
Download Date | 3/9/20 8:44 PM

