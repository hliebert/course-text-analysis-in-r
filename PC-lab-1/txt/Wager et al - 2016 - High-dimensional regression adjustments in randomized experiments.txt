High-dimensional regression adjustments in
randomized experiments
Stefan Wagera,b,1, Wenfei Dua, Jonathan Taylora, and Robert J. Tibshirania,c,1
a

Department of Statistics, Stanford University, Stanford, CA 94305; bOperations, Information & Technology, Stanford Graduate School of Business, Stanford
University, Stanford, CA 94305; and cDepartment of Biomedical Data Science, Stanford University, Stanford, CA 94305
Contributed by Robert J. Tibshirani, September 15, 2016 (sent for review July 24, 2016; reviewed by Winston Lin and Dylan Small)

high-dimensional confounders

| randomized trials | regression adjustment

R

andomized controlled trials are often considered the gold
standard for estimating the effect of an intervention, as they
allow for simple model-free inference about the average treatment effect on the sampled population. Under mild conditions,
the mean observed outcome in the treated sample minus the
mean observed outcome in the control sample is a consistent and
unbiased estimator for the population average treatment effect.
However, the fact that model-free inference is possible in randomized controlled trials does not mean that it is always optimal:
As argued by Fisher (1), if we have access to auxiliary features that
are related to our outcome of interest via a linear model, then
controlling for these features using ordinary least squares will
reduce the variance of the estimated average treatment effect
without inducing any bias. This line of research has been thoroughly explored: Under low-dimensional asymptotics where the
problem specification remains fixed while the number of samples
grows to infinity, it is now well established that regression adjustments are always asymptotically helpfulâ€”even in misspecified
modelsâ€”provided we add full treatment-by-covariate interactions
to the regression design and use robust standard errors (2â€“10).
The characteristics of high-dimensional regression adjustments
are less well understood. In a recent advance, Bloniarz et al.
(11) show that regression adjustments are at least sometimes
helpful in high dimensions: Given an â€œultrasparsityâ€ assumption
from the high-dimensional inference literature, they establish that
regression adjustments using the lasso (12, 13) are more efficient
than model-free inference. This result, however, leaves a substantial gap between the low-dimensional regimeâ€”where regression adjustments are always asymptotically helpfulâ€”and the
high-dimensional regime where we have only special-case results.
In this paper, we show that high-dimensional regression adjustments to randomized controlled trials work under much
greater generality than previously known. We find that any regression adjustment with a free intercept yields unbiased estimates
of the treatment effect. This result is agnostic as to whether the
regression model was obtained using the lasso, the elastic net (14),
subset selection, or any other method that satisfies this criterion.

www.pnas.org/cgi/doi/10.1073/pnas.1614732113

We also propose a simple procedure for building practical confidence intervals for the average treatment effect.
Furthermore, we show that the precision of the treatment effect
estimates obtained by such regression adjustments depends only on
the prediction risk of the fitted regression adjustment. In particular,
any risk-consistent regression adjustment can be made to yield efficient estimates of the average treatment effect in the sense of refs.
15â€“18. Thus, when choosing which regression adjustment to use,
practitioners are justified in using standard model selection tools that
aim to control prediction error, e.g., Mallowâ€™s Cp or cross-validation.
This finding presents a striking contrast to the theory of highdimensional regression adjustments in observational studies. In a
setting where treatment propensity may depend on covariates,
simply fitting low-risk regression models to the treatment and
control samples via cross-validation is not advised, as there exist
regression adjustments that have low predictive error but yield
severely biased estimates of the average treatment effect (19â€“
22). Instead, special-case procedures are needed: For example,
Belloni et al. (21) advocate a form of augmented model selection
that protects against bias at the cost of worsening the predictive
performance of the regression model. The tasks of fitting good
high-dimensional regression adjustments to randomized vs. observational data thus present qualitatively different challenges.
The first half of this paper develops a theory of regularized regression adjustments with high-dimensional Gaussian designs. This
analysis enables us to highlight the connection between the predictive accuracy of the regression adjustment and the precision of
the resulting treatment effect estimate and also to considerably
improve on theoretical guarantees available in prior work. In the
second half of the paper, we build on these insights to develop
cross-estimation, a practical method for inference about average
treatment effects that can be paired with either high-dimensional
regularized regression or nonparametric machine-learning methods.
Significance
As datasets get larger and more complex, there is a growing interest in using machine-learning methods to enhance scientific
analysis. In many settings, considerable work is required to make
standard machine-learning methods useful for specific scientific
applications. We find, however, that in the case of treatment effect estimation with randomized experiments, regression adjustments via machine-learning methods designed to minimize test
set error directly induce efficient estimates of the average treatment effect. Thus, machine-learning methods can be used out of
the box for this task, without any special-case adjustments.
Author contributions: S.W. designed research; S.W., W.D., J.T., and R.J.T. performed research; S.W. analyzed data; and S.W. and R.J.T. wrote the paper.
Reviewers: W.L., Columbia University; and D.S., Wharton School, University of
Pennsylvania.
The authors declare no conflict of interest.
1

To whom correspondence may be addressed. Email: swager@stanford.edu or tibs@
stanford.edu.

This article contains supporting information online at www.pnas.org/lookup/suppl/doi:10.
1073/pnas.1614732113/-/DCSupplemental.

PNAS | November 8, 2016 | vol. 113 | no. 45 | 12673â€“12678

STATISTICS

We study the problem of treatment effect estimation in randomized
experiments with high-dimensional covariate information and show
that essentially any risk-consistent regression adjustment can be
used to obtain efficient estimates of the average treatment effect.
Our results considerably extend the range of settings where highdimensional regression adjustments are guaranteed to provide valid
inference about the population average treatment effect. We then
propose cross-estimation, a simple method for obtaining finitesampleâ€“unbiased treatment effect estimates that leverages highdimensional regression adjustments. Our method can be used when
the regression model is estimated using the lasso, the elastic net,
subset selection, etc. Finally, we extend our analysis to allow for
adaptive specification search via cross-validation and flexible nonparametric regression adjustments with machine-learning methods
such as random forests or neural networks.

1. Setting and Notation
We frame our analysis in terms of the Neymanâ€“Rubin potential
outcomes model (23, 24). Given n i.i.d. observations Ã°Xi , Yi , Wi Ã,
Ã°1Ã
Ã°0Ã
i = 1, 2, . . . , n, we posit potential outcomes Yi and Yi ; then,
Ã°Wi Ã
the outcome that we actually observe is Yi = Yi . We focus on
randomized controlled trials, where Wi is independent of all
pretreatment characteristics,
n
o
Ã°0Ã Ã°1Ã
Xi , Yi , Yi
âŠ¥ Wi .
[1]
We take the predictors to be generated as Xi âˆ¼ FÃ° Â· Ã âˆˆ Rp and
assume a homoskedastic linear model in each arm,
Ã°Wi Ã

Yi = cÃ°Wi Ã + Xi Â· Î²Ã°Wi Ã + Â«i

, cÃ°wÃ âˆˆ R, Î²Ã°wÃ âˆˆ Rp ,

[2]

Ã°W Ã

for w = 0, 1, where Â«i i is mean-zero noise with variance Ïƒ 2 ;
more general models are considered later. We use the notation
n0 = jfi : Wi = 0gj and n1 = jfi : Wi = 1gj. We study inference about
the average treatment effect Ï„ = EÂ½Y Ã°1Ã âˆ’ Y Ã°0Ã. In our analysis,
it is sometimes also convenient to study estimation of the conditional average treatment effect
 i
n
h
1X
Ã°1Ã
Ã°0Ã 
E Yi âˆ’ Yi Xi
n i=1


 Â· Î²Ã°1Ã âˆ’ Î²Ã°0Ã + cÃ°1Ã âˆ’ cÃ°0Ã .
=X

Ï„=

[3]

2. Regression Adjustments with Gaussian Designs
Suppose that we have obtained parameter estimates ^cÃ°wÃ, Î²^Ã°wÃ ,
w âˆˆ f0,1g for the linear model Eq. 2 via the lasso, the elastic net,
or any other method. We then get a natural estimator for the
average treatment effect:


 Â· Î²^Ã°1Ã âˆ’ Î²^Ã°0Ã + ^cÃ°1Ã âˆ’ ^cÃ°0Ã .
^Ï„ = X
[4]
In the case where Î²^Ã°wÃ is the ordinary least-squares estimator for
^Ï„, the behavior of this estimator has been carefully studied by
refs. 8 and 10. Our goal is to characterize its behavior for generic
regression adjustments Î²^Ã°wÃ, all while allowing the number of
predictors p to be much larger than the sample size n.
The only assumption that we make on the estimation scheme
is that it be centered: For w âˆˆ f0,1g,
[5]

i.e., the mean of the predicted outcomes matches that of the observed
outcomes; and Î²^Ã°wÃ is translation invariant and depends only on


 W , Yi âˆ’Y W , Wi n .
F Î² = Xi âˆ’X
[6]
i
i
i=1
 w and Y w denote the mean of the outcomes Yi and feaHere, X
tures Xi over all observations with Wi = w. Algorithmically, a simple
way to enforce this constraint is to first center the training samples
 W , Yi â†’ Yi âˆ’ Y W , run any regression method on these
Xi â†’ Xi âˆ’ X
i
i
centered data, and then set the intercept using Eq. 5; this is done by
default in standard software for regularized regression, such as glmnet
(25). We also note that ordinary least-squares regression is always
centered in this sense, even after common forms of model selection.
Now, if our regression adjustment has a well-calibrated intercept as in Eq. 5, then we can write Eq. 4 as
12674 | www.pnas.org/cgi/doi/10.1073/pnas.1614732113

[7]

To move forward, we focus on the case where the datagenerating model for Ã°Xi , Yi Ã is Gaussian; i.e., Xi âˆ¼ N Ã°m, Î£Ã for
some m âˆˆ Rp and positive-semidefinite matrix Î£ âˆˆ RpÃ—p , and
Yi âˆ’ EÂ½Yi jXi , Wi  âˆ¼ N Ã°0, Ïƒ 2 Ã. For our purpose, the key fact about
Gaussian data is that the mean of independent samples is independent of the within-sample spread; i.e.,




 W , Yi âˆ’Y W n âŠ¥ X
 0, X
 1 , Y 0 , Y 1 ,
Xi âˆ’X
[8]
i
i i=1
conditionally on the treatment assignments W1 , . . . , Wn . Thus,
 W and
because Î²^Ã°wÃ depends only on the centered data Xi âˆ’ X
i
Yi âˆ’ Y Wi , we can derive a simple expression for the distribution
of ^Ï„. The following is an exact finite sample result and holds no
 âˆ’X
w
matter how large p is relative to n; a key observation is that X
is mean zero by randomization of the treatment assignment,
for w = 0, 1.
Proposition 1. Suppose that our regression scheme for ^
cÃ°wÃ and Î²^Ã°wÃ is
centered and that our data-generating model is Gaussian as above.
Then, writing kvk2Î£ dvâŠ¤ Î£ v for v âˆˆ Rp ,
d
^Ï„ âˆ’ Ï„jn0 , n1 , Î²^Ã°0Ã , Î²^Ã°1Ã = N Ã°0, AÃ,

A=

As discussed by ref. 17, good estimators for Ï„ are generally good
estimators for Ï„ and vice versa. In the homogeneous treatment
effects model Yi = c + Xi Â· Î² + Wi Ï„ + Â«i , Ï„ and Ï„ coincide.
All technical derivations can be found in the Supporting Information, A. Proofs.

 w Â· Î²^Ã°wÃ + ^cÃ°wÃ ;
Y w = X


 

 Â· Î²^Ã°1Ã âˆ’ Î²^Ã°0Ã + ^cÃ°1Ã âˆ’ ^cÃ°0Ã
^Ï„ = X
 Ã°1Ã 


 âˆ’X
 1 Â· Î²^ âˆ’ X
 âˆ’X
 0 Â· Î²^Ã°0Ã .
= Y 1 âˆ’ Y 0 + X

Î²=

1 1
+
n0 n1

Ïƒ2 + b
Î²âˆ’Î²

n1 Î²Ã°0Ã + n0 Î²Ã°1Ã
,
n

2
Î£

,

[9]

n1 Î²^Ã°0Ã + n0 Î²^Ã°1Ã
b
.
Î²=
n

If the errors in Î²^Ã°0Ã and Î²^Ã°1Ã are roughly orthogonal, then
b
Î²âˆ’Î²

2
Î£

â‰ˆ

n21 ^Ã°0Ã Ã°0Ã
Î² âˆ’Î²
n2

2
Î£

+

n20 ^Ã°1Ã Ã°1Ã
Î² âˆ’Î²
n2

2

[10]

Î£

and, in any case, twice the right-hand side is always an upper
bound for the left-hand side. Thus, the distribution of ^Ï„ effectively depends on the regression adjustments Î²^Ã°wÃ only through
the excess predictive error

2 
2
Î²^Ã°wÃ âˆ’Î²Ã°wÃ Î£ = E Ã°X âˆ’ mÃ Â· Î²^Ã°wÃ âˆ’ Î²Ã°wÃ Î²^Ã°wÃ ,
[11]
where the above expectation is taken over a test set example X.
This implies that, in the setting of Proposition 1, the main practical concern in choosing which regression adjustment to use is to
ensure that Î²^Ã°wÃ has low predictive error.
The above result is conceptually related to recent work by
Berk et al. (3) (also ref. 26), who showed that the accuracy of
low-dimensional covariate adjustments using ordinary leastsquares regression depends on the mean-squared error of the
regression fit; they also advocate using this connection to provide
simple asymptotic inference about Ï„. Here, we showed that a
similar result holds for any regression adjustment on Gaussian
designs, even in high dimensions; and in the second half of this
paper we discuss how to move beyond the Gaussian case.
Risk Consistency and the Lasso. As stated, Proposition 1 provides
the distribution of ^Ï„ conditionally on Î²^Ã°wÃ and so is not directly
comparable to related results in the literature. However, whenever Î²^Ã°wÃ is risk consistent in the sense that


2
R Î²^Ã°wÃ d Î²^Ã°wÃ âˆ’Î²Ã°wÃ Î£ â†’ p 0,
[12]

for w = 0, 1, we can asymptotically omit the conditioning.
Wager et al.

!

Theorem 2. Suppose that, under the conditions of Proposition 1, we
have a sequence of problems where Î²^Ã°wÃ is risk consistent (Eq. 12),
and PÂ½W = 1 â†’ Ï€. Then,

Ïƒ
,
Ï€Ã°1 âˆ’ Ï€Ã

[13]

or, in other words, ^Ï„ is efficient for estimating Ï„ (15â€“18).
In the case of the lasso, Theorem 2 lets us substantially improve over the best existing guarantees in the literature (11). The
lasso estimates Î²^Ã°wÃ as the minimizer over Î² of
X 1
 

 w Â· Î² 2 + nw Î»kÎ²k ,
[14]
Yi âˆ’ Y w âˆ’ Xi âˆ’ X
1
2
fi:W =wg
i

for some penalty parameter Î» > 0. Typically, the lasso is used
when we believe a sparse regression adjustment to be appropriate.
 In our setting, it is well known
 that the lasso satisfies
R Î²^Ã°wÃ = OP kÎ£k2op Î²Ã°wÃ 0 logÃ°pÃ=nw , provided the penalty parameter Î» is well chosen and Î£ does not allow for too much
correlation between features (27, 28).
Thus, whenever we have a sequence of problems as in Theorem
2 where Î²Ã°wÃ is k-sparse, i.e., Î²Ã°wÃ has at most k nonzero entries,
and k logÃ°pÃ=n â†’ 0, we find that ^Ï„ is efficient in the sense of Eq.
13. Note that this result is much stronger than the related result of
ref. 11, which shows that lasso regression adjustments
pï¬ƒï¬ƒï¬ƒyield efficient estimators ^Ï„ in an ultrasparse regime with k  n=logÃ°pÃ.
To illustrate the difference
between these two results, it is well
pï¬ƒï¬ƒï¬ƒ
known that if k  n=logÃ°pÃ, then it is possible to do efficient
inference about the coefficients of the underlying parameter vector
Î² (29â€“31), and so the result of ref. 11 is roughly in line with the rest
of the literature on high-dimensional inference. Conversely, if we
have only k  n=logÃ°pÃ, accurate inference about the coefficients
of Î² is in general impossible without further conditions on the covariance of X (32, 33). However, we have shown that we can still
carry out efficient inference about Ï„. In other words, the special
structure present in randomized trials means that much more is
possible than in the generic high-dimensional regression setting.
Inconsistent Regression Adjustments. Even if our regression adjustment Î²^Ã°wÃ is not risk consistent, we can still use Proposition 1
to derive unconditional results about ^Ï„ whenever
 
2
Î² d b
Î²âˆ’Î² â†’ p Râˆ .
[15]
R b
Î£

We illustrate this phenomenon in the case of ridge regression,
where regression adjustments generally reduceâ€”but do not eliminateâ€”excess test-set risk. Recall that ridge regression estimates
Î²^Ã°wÃ as the minimizer over Î² of
X 1

 
 w Â· Î² 2 + nw Î»kÎ²k2 .
Yi âˆ’ Y w âˆ’ Xi âˆ’ X
2
2
fi:W =wg

[16]

i

The following result relies on random-matrix theoretic tools for
analyzing the predictive risk of ridge regression (34).
Theorem 3. Suppose we have a sequence of problems in the setting of

Proposition 1 with n, p â†’ âˆ and p=n â†’ Î³ âˆˆ Ã°0, âˆÃ, such that the
spectrum of the covariance Î£ has a weak limit. Following ref. 34,
suppose moreover that the true parameters Î²Ã°0Ã and Î²Ã°1Ã are independently and randomly drawn from a random-effects model with


 Î±2

E Î²Ã°wÃ = 0 and Var Î²Ã°wÃ = IpÃ—p ,
p

with Î± > 0.

[17]

Then, selecting Î²^Ã°wÃ in Eq. 4 via ridge regression
pï¬ƒï¬ƒï¬ƒ tuned to minimize
prediction error, and with PÂ½W = 1 â†’ Ï€, we get nÃ°^Ï„ âˆ’ Ï„Ã â‡’ N Ã°0, SÃ,
Wager et al.

Î±
Î³



Ï€
Î³Ïƒ 2

v0 âˆ’Î±2 Ã°1 âˆ’ Ï€Ã



+

1âˆ’Ï€


2
v1 âˆ’Î±Î³Ïƒ2 Ï€



[18]

,

where the vw Ã°âˆ’Î»Ã are the companion Stieltjes transforms of the
limiting empirical spectral distributions for the treated and control
samples, as defined in the proof.
To interpret this result, we note that the quantity vw Ã°âˆ’Î»Ã can
also be induced via the limit (35, 36)
0
!âˆ’1 1
1 @ 1 X
A â†’ p vw Ã°âˆ’Î»Ã, for Î» > 0.
tr
Xi XiâŠ¤ + Î»Inw Ã—nw
nw
nw fi:W =wg
i

Finally, we note that the limiting variance of ^Ï„ âˆ’ Ï„ obtained via
ridge regression above is strictly smaller than the corresponding
variance of the unadjusted estimator, which converges to
Ã°Ïƒ 2 + Ã°Ï€ 2 + Ã°1 âˆ’ Ï€ 2 ÃÃÎ±2 trÃ°Î£Ã=pÃ=Ã°Ï€Ã°1 âˆ’ Ï€ÃÃ; this is because optimally tuned ridge regression strictly improves over the â€œnullâ€
model Î²^Ã°wÃ = 0 in terms of its predictive accuracy.
3. Practical Inference with Cross-Estimation
In the previous section, we found thatâ€”given Gaussianity
assumptionsâ€”generic regression adjustments yield unbiased estimates of the average treatment effect and also that low-risk regression adjustments lead to high-precision estimators. Here, we seek
to build on this insight and to develop simple inferential procedures
about Ï„ and Ï„ that attain the above efficiency guarantees, all while
remaining robust to deviations from Gaussianity or homoskedasticity.
Our approach is built around cross-estimation, a procedure
inspired by data splitting and the work of refs. 37 and 38. We first
split our data into K equally sized folds (e.g., K = 5 or 10) and
then, for each fold k = 1, . . . , K, we compute


Ã°kÃ
Ã°kÃ
 Ã°kÃ âˆ’ X
 Ã°kÃ Â· Î²^Ã°1, âˆ’kÃ
^Ï„Ã°kÃ = Y 1 âˆ’ Y 0 + X
1


[19]
 Ã°kÃ âˆ’ X
 Ã°kÃ Â· Î²^Ã°0, âˆ’kÃ .
âˆ’ X
0
Ã°kÃ

Ã°kÃ

Here, Y 1 , Y 0 , etc., are moments taken over the kth fold,
whereas Î²^Ã°1, âˆ’kÃ and Î²^Ã°0, âˆ’kÃ are centered regression estimators
computed over
P the K âˆ’ 1 other folds. We then obtain an aggregate
estimate ^Ï„ = Kk=1^Ï„Ã°kÃ nÃ°kÃ =n, where nÃ°kÃ is the number of observations in the kth fold. An advantage of this construction is that an
analog to the relation Eq. 8 now automatically holds, and thus our
treatment effect estimator ^Ï„ is unbiased without assumptions. Note
that the result below references both the average treatment effect
Ï„ and the conditional average treatment effect Ï„.
Theorem 4. Suppose that we have n independent and identically

distributed samples satisfying Eq. 1, drawn from a linear model Eq.
2, where Xi has finite first moments and the conditional variance of
Ã°wÃ
Yi given Xi may vary. Then, EÂ½^Ï„jX1 , . . . , Xn  = Ï„. If, moreover,
the Î²^Ã°w, âˆ’kÃ are all risk consistent in the sense of Eq. 12 for
k = 1, . . . , K, and both the signals Xi Â· Î²Ã°wÃ and residuals Yi âˆ’

EÂ½Yi jXi , Wi = w are asymptotically
Gaussian when averaged, then
Ã°wÃ 
writing Ïƒ 2w = EÂ½VarÂ½Yi Xi , we have
pï¬ƒï¬ƒï¬ƒ
Ïƒ2
Ïƒ2
nÃ°^Ï„ âˆ’ Ï„Ã â‡’ N 0, 0 + 1 + Î²Ã°1Ã âˆ’Î²Ã°0Ã
1âˆ’Ï€ Ï€

2
Î£

.

[20]

In the homoskedastic case, i.e., when the variance of Y Ã°wÃ conditionally on X does not depend on X, then the above is efficient.
With heteroskedasticity, the above is no longer efficient because
we are in a linear setting and so inverse-variance weighting could
improve precision; however, Eq. 20 can still be used as the basis
for valid inference about Ï„.
PNAS | November 8, 2016 | vol. 113 | no. 45 | 12675

STATISTICS

pï¬ƒï¬ƒï¬ƒ
nÃ°^Ï„ âˆ’ Ï„Ã â‡’ N 0,

2

S = 2Ïƒ 2 +

2

^, V
^=
Ï„ âˆˆ ^Ï„ Â± z1âˆ’Î±=2 V

K
X
nÃ°kÃ
n
k=1

2

^ k,
V

Cross-Validated Cross-Estimation. High-dimensional regression adjustments usually rely on a tuning parameter that controls the
amount of regularization, e.g., the parameter Î» for the lasso and
ridge regression. Although theory provides some guidance on how
to select Î», practitioners often prefer to use computationally intensive methods such as cross-validation.
Now, our procedure in principle already allows for cross-validation: If we estimate Î²^Ã°0, âˆ’kÃ in Eq. 19 via any cross-validated
regression adjustment that relies only on all but the kth data folds,
then ^Ï„Ã°kÃ will be unbiased for Ï„. However, this requires running
the full cross-validated algorithm K times, which can be very
expensive computationally.
Here, we show how to obtain good estimates of ^Ï„ using only a
single round of cross-validation. First, we specify K regression folds,
 k, w and
and for each k âˆˆ f1, . . . , Kg and w âˆˆ f0,1g we compute X
Y k, w as the mean of all observations in the kth fold with Wi = w.
 k, W and Y~ i =
~ i = Xi âˆ’ X
Next, we center the data such that X
i
Yi âˆ’ Y k, Wi for all observations in the kth fold. Finally, we estimate
Ã°w,
âˆ’kÃ
by running a standard out-of-the-box cross-validated algoÎ²^
~ i , Y~ i , Wi Ã triples with the
rithm (e.g., cv. glmnet for R) on the Ã°X
same K folds as specified before and then use Eq. 19 to compute ^Ï„.
The actual estimator that we use to estimate Î²^Ã°0Ã and Î²^Ã°1Ã in our
experiments is inspired by the procedure of Imai and Ratkovic
(39). Our goal is to let the lasso learn shared â€œmain effectsâ€ for the

1.00
0.95
0.85

200

300

400

500

n

treatment and control groups. To accomplish this, we first run a
2p-dimensional lasso problem,

^ ^Î³ = argmin
Î²,
Î², Î³ Î»Ã°kÎ²k1 + kÎ³k1 Ã

2 
X
~ i Â· Î² + Ã°2Wi âˆ’ 1ÃX
~i Â·Î³
+
Y~ i âˆ’ X
, [23]
and then set Î²^Ã°0Ã = Î²^ âˆ’ ^Î³ and Î²^Ã°1Ã = Î²^ + ^Î³ . We simultaneously tune
Î» and estimate Ï„ by cross-validated cross-estimation as discussed
above. When all our data are Gaussian, this procedure is exactly
unbiased by the same argument as used in Proposition 1; and
even when X is not Gaussian, it appears to work well in our
experiments.
4. Nonparametric Machine-Learning Methods
In our discussion so far, we have focused on treatment effect
estimation using high-dimensional, linear regression adjustments
and showed how to provide unbiased inference about Ï„ under
general conditions. Here, we show how to extend our results
about cross-estimation to general nonparametric regression adjustments obtained using, e.g., neural networks or random forests
(40). We assume a setting where
EÂ½Y Ã°wÃjX = x = Î¼Ã°wÃ Ã°xÃ
for some unknown regression functions Î¼Ã°wÃ Ã°xÃ, and our goal is to
leverage estimates Î¼
^Ã°wÃ Ã°xÃ obtained using any machine-learning
method to improve the precision of ^Ï„, as*,â€ 

1.00

n 
1X
Î¼
^Ã°1,
n i=1

âˆ’iÃ

Ã°Xi Ã âˆ’ Î¼
^Ã°0,

âˆ’iÃ


X Yi âˆ’ Î¼
^Ã°1, âˆ’iÃ Ã°Xi Ã
Ã°Xi Ã +
n1
fi:W =1g
i

[24]

0.90

coverage
500

100

i

diff. in means
Bloniarz et al.
cross estimation

0.85
400

500

Fig. 2. Simulation results with Î² proportional to a permutation of
Ã°1, 2âˆ’1 , 3âˆ’1 , . . . , pâˆ’1 Ã, kÎ²k2 = 2, PÂ½W = 1 = 0.5, Ï = 0.8, and p = 500. All
numbers are based on 1,000 simulation replications. The plots are produced
the same way as in Fig. 1.

0.95

0.10

mean squared interval length

0.05

300

400

X Yi âˆ’ Î¼
^Ã°0, âˆ’iÃ Ã°Xi Ã
âˆ’
,
n0
fi : W =0g

0.02

200

300
n

^Ï„ =

100

200

[22]

where z1âˆ’Î±=2 is the appropriate standard Gaussian quantile. In the
setting of Theorem 4, i.e., with risk consistency and bounded second
moments, we can verify that the ^Ï„Ã°kÃ are asymptotically uncorrelated
and so the above confidence intervals are asymptotically exact.

diff. in means
Bloniarz et al.
cross estimation

diff. in means
Bloniarz et al.
cross estimation

0.80
100

Now, the above moments correspond to observable quantities on
the kth data fold, so we immediately obtain a moment-based
^ k for Vk . Finally, we build Î±-level confidence
plug-in estimator V
intervals for Ï„ as

0.90

coverage

0.10
0.05
0.02

mean squared interval length

0.20

diff. in means
Bloniarz et al.
cross estimation

0.01

Confidence Intervals via Cross-Estimation. Another advantage of
cross-estimation is that it allows for moment-based variance estimates for ^Ï„. Here, we discuss practical methods for building
confidence intervals that cover the average treatment effect Ï„.
We can verify that the variance of ^Ï„Ã°kÃ is Vk after conditioning on
Ã°kÃ
the Î²^Ã°w, âˆ’kÃ and nw , with

X 1
Ã°âˆ’kÃ  Ã°âˆ’kÃ
b
Ã°wÃ
b
.
[21]
Var
Y
âˆ’
X
Â·
Î²
Vk =
Î²
Ã°kÃ
n
wâˆˆf0, 1g w

100

200

n

300

400

500

n

Fig. 1. Simulation results with Î² = Ã°1, 0, 0, . . . , 0Ã, PÂ½W = 1 = 0.2, Ï = 0, and
p = 500. All numbers are based on 1,000 simulation replications. Left panel
^ produced by each estimator (solid
shows both the average variance estimate V
lines) and the actual variance Var[^Ï„] of the estimator (dashed-dotted lines);
^ is directly proportional to the squared length of the confidence
note that V
interval. Right panel depicts realized coverage for both 95% confidence intervals (solid lines) and 99% confidence intervals (dashed lines).

12676 | www.pnas.org/cgi/doi/10.1073/pnas.1614732113

where Î¼
^Ã°w, âˆ’iÃ is any estimator that does not depend on the ith
training example; for random forests, we set Î¼
^Ã°w, âˆ’iÃ Ã°Xi Ã to be the
â€œout-of-bagâ€ prediction at Xi . To motivate Eq. 24, we start from
Eq. 7 and expand out terms using the relation

*We note that Eq. 24 depends on Î¼
^Ã°0, âˆ’iÃ Ã°Xi Ã and Î¼
^Ã°1, âˆ’iÃ Ã°Xi Ã implicitly only through
Ã°âˆ’iÃ
Î¼
^ Ã°Xi Ã = n1 =n^
Î¼Ã°0, âˆ’iÃ Ã°Xi Ã + n0 =n^
Î¼Ã°1, âˆ’iÃ Ã°Xi Ã. It may thus also be interesting to estimate
Ã°âˆ’iÃ
Î¼
^ Ã°Xi Ã directly using, e.g., the â€œtyranny of the minorityâ€ scheme of Lin (8).
â€ 

A related estimator is studied by Rothe in the context of classical nonparametric regression adjustments, e.g., local regression, for observational studies with known treatment
propensities. [Rothe C (2016) The value of knowing the propensity score for estimating
average treatment effects, (IZA), discussion paper 9989.]

Wager et al.

wâˆˆf0, 1g fi:Wi =wg



Yi âˆ’ nn0 Î¼
^Ã°1, âˆ’iÃ Ã°Xi Ã âˆ’ nn1 Î¼
^Ã°0, âˆ’iÃ Ã°Xi Ã
nw Ã°nw âˆ’ 1Ã

2
.

[26]

Formal Results. The following result characterizes the behavior of
this estimator, under the assumption that the estimator is
â€œjackknife compatible,â€ meaning that the expected jackknife
estimate of variance for Î¼
^Ã°wÃ converges to 0. We define this
condition in Supporting Information, A. Proofs and verify that it
holds for random forests.
Theorem 5. Suppose that Î¼
^ is jackknife compatible. Then, the esti-

mator ^Ï„ pinï¬ƒï¬ƒï¬ƒ Eq. 24 is asymptotically unbiased, EÂ½^Ï„jX1 , . . . , Xn  =
Ï„ + oÃ°1= nÃ. Moreover, if theP
regression adjustments Î¼
^w are risk
consistent in the sense thatâ€¡ 1=n ni=1 Ã°^
Î¼Ã°w, âˆ’ iÃ Ã°Xi Ã âˆ’ Î¼Ã°wÃ Ã°Xi ÃÃ2 â†’ p 0,
Ã°wÃ
and the potential outcomes Yi have finite second moments, then ^Ï„
1=2
^
is efficient and Ã°^Ï„ âˆ’ Ï„Ã=Ã°V Ã is asymptotically standard Gaussian.
We note that there has been considerable recent interest in
using machine-learning methods to estimate heterogeneous treatment effects (42â€“45). In relation to this literature, our present goal
is more modest: We simply seek to use machine learning to reduce
the variance of treatment effect estimates in randomized experiments. This is why we obtain more general results than the papers
on treatment heterogeneity.
5. Experiments
In our experiments, we focus on two specific variants of treatment effect estimation via cross-estimation. For high-dimensional
linear estimation, we use the lasso-based method Eq. 23 tuned by
cross-validated cross-estimation. For nonparametric estimation,
we use Eq. 24 with random forest adjustments. We implement
our method as an open-source R package, crossEstimation, built
on top of glmnet (25) and randomForest (46) for R. Supporting
Information, B. Additional Simulation Results and Table S1 have
additional simulation results.
Simulations. We begin by validating our method in a simple
simulation setting with Y = XÎ² + W Ï„ + Â«, where Â« âˆ¼ N Ã°0, 1Ã. In
all simulations, we set the features X to be Gaussian with
autoregressive AR-Ï covariance. We compare our lasso-based
cross-estimation with both the simple difference-in-means estimate ^Ï„ = Y 1 âˆ’ Y 0 and the proposal of Bloniarz et al. (11) that
uses lasso regression adjustments tuned by cross-validation. Our
method differs from that of Bloniarz et al. (11) in that we use a
different algorithm for confidence intervals and also that we use
the joint lasso algorithm Eq. 23 instead of computing separate
lassos in both treatment arms.
Figs. 1 and 2 display results for different choices of Î², Ï, etc.,
while varying n. In both cases, we see that the confidence intervals produced by our cross-estimation algorithm and the
method of Bloniarz et al. (11) are substantially shorter than
those produced by the difference-in-means estimator. Moreover,

â€¡

With random forests, ref. 41 provides such a risk-consistency result.

Wager et al.

0.98

random forest

lasso

Fig. 3. Reduction in squared confidence interval length achieved by random forests and a lasso-based method, relative to the simple differencein-means estimator. Confidence intervals rely on cross-estimation. The plot
was generated using 1,000 simulation replications.

our confidence intervals accurately represent the variance of our
estimator (compare solid and dashed-dotted lines in Figs. 1 and
2, Left) and achieve nominal coverage at both the 95% and 99%
levels. Conversely, especially in small samples, the method of
Bloniarz et al. (11) underestimates the variance of the method
and does not achieve target coverage.
Understanding Attitudes Toward Welfare. We also consider an experimental dataset collected as a part of the General Social
Survey.Â§ The dataset is large (N = 28,646 after preprocessing), so
we know the true treatment effect essentially without error: The
fraction of respondents who say we spend too much on assistance
to the poor is smaller than the fraction of respondents who say
we spend too much on welfare by 0.35. To test our method, we
repeatedly drew subsamples of size n = 2,000 from the full
dataset and examined the ability of both lasso-based and randomforestâ€“based cross-estimation to recover the correct answer. We
had p = 12 regressors.
First, we note that both variants of cross-estimation achieved
excellent coverage. Given a nominal coverage rate of 95%, the
simple difference-in-means estimator, lasso-based cross-estimation, and random-forest cross-estimation had realized coverage
rates of 96.3%, 96.5%, and 95.3%, respectively, over 1,000 replications. Meanwhile, given a nominal target of 99%, the realized
numbers became 99.0%, 99.0%, and 99.3%. We note that this
dataset has non-Gaussian features and exhibits considerable
treatment effect heterogeneity.
Second, Fig. 3 depicts the reduction in squared confidence
interval length for individual realizations of each method. More
^ lasso=rf =V
^ simple , where V
^ is the
formally, we show boxplots of V
variance estimate used to build confidence intervals. Here, we
see that although cross-estimation may not improve the precision
of the simple method by a large amount, it consistently improves
performance by a small amount. Moreover, in this example,

Subjects were asked whether we, as a society, spend too much money either on â€œwelfareâ€ or on â€œassistance to the poor.â€ The questions were randomly assigned and the
treatment effect corresponds to the change in the proportion of people who answer
â€œyesâ€ to the question. This dataset is discussed in detail in ref. 43; we preprocess the data
as in ref. 47.

Â§

PNAS | November 8, 2016 | vol. 113 | no. 45 | 12677

STATISTICS

^=
V

X

0.92

where Î¼
^Ã°1Ã Ã°xÃ = x Â· Î²^Ã°1Ã + ^cÃ°1Ã , etc. The remaining differences between Eq. 24 and Eq. 7 are due to the use of out-of-bag estimation to preserve randomization of the treatment assignment Wi
conditionally on the corresponding regression adjustment. We
estimate the variance of ^Ï„ using the formula
X

1.00

[25]

fi:Wi =1g

0.96

Î¼
^Ã°1Ã Ã°Xi Ã,

0.94

X

relative variance reduction

n
X


1
 âˆ’X
 1 Â· Î²^Ã°1Ã = 1
X
Î¼
^Ã°1Ã Ã°Xi Ã âˆ’
n i=1
n1

6. Discussion
In many applications of machine-learning methods to causal
inference, there is a concern that the risk of specification search,
i.e., trying out many candidate methods and choosing the one
that gives us a significant result, may reduce the credibility of
empirical findings. This has led to considerable interest in
methodologies that allow for complex model-fitting strategies
that do not compromise statistical inference.
One prominent example is the design-based paradigm to
causal inference in observational studies, whereby we first seek
to build an â€œobservational designâ€ by looking only at the features
Xi and the treatment assignments Wi and reveal the outcomes Yi
only once the observational design has been set (48, 49). The
observational design may rely on matching, inverse-propensity

weighting, or other techniques. As the observational design is
fixed before the outcomes Yi are revealed, practitioners can
devote considerable time and creativity to fine-tuning the design
without compromising their analysis.
From this perspective, we have shown that regression adjustments to high-dimensional randomized controlled trials exhibit a
similar opportunity for safe specification search. Concretely,
imagine that once we have collected data from a randomized
experiment, we provide our analyst only with class-wise centered
 W , and Yi âˆ’ Y W . The analyst can then use these
data: Wi , Xi âˆ’ X
i
i
data to obtain any regression adjustment they want, which we will
then plug into Eq. 4. Our results guarantee thatâ€”at least with a
random Gaussian designâ€”the resulting treatment effect estimates will be unbiased regardless of the specification search the
analyst may have done using only the class-wise centered data.
Cross-estimation enables us to mimic this phenomenon with
non-Gaussian data.

1. Fisher R (1925) Statistical Methods for Research Workers (Oliver and Boyd, Edinburgh,
UK).
2. Athey S, Imbens G (2016) The econometrics of randomized experiments. arXiv:
1607.00698.
3. Berk R, et al. (2013) Covariance adjustments for the analysis of randomized field
experiments. Eval Rev 37(3â€“4):170â€“196.
4. Ding P, Feller A, Miratrix L (2016) Decomposing treatment effect variation. arXiv:
1605.06566.
5. Freedman D (2008) On regression adjustments in experiments with several treatments. Ann Appl Stat 2(1):176â€“196.
6. Freedman DA (2008) On regression adjustments to experimental data. Adv Appl Math
40(2):180â€“193.
7. Imbens GW, Wooldridge JM (2009) Recent developments in the econometrics of
program evaluation. J Econ Lit 47(1):5â€“86.
8. Lin W (2013) Agnostic notes on regression adjustments to experimental data: Reexamining Freedmanâ€™s critique. Ann Appl Stat 7(1):295â€“318.
9. Rosenbaum PR (2002) Covariance adjustment in randomized experiments and observational studies. Stat Sci 17(3):286â€“327.
10. Cochran W (1977) Sampling Techniques (Wiley, New York), 3rd Ed.
11. Bloniarz A, Liu H, Zhang CH, Sekhon JS, Yu B (2016) Lasso adjustments of treatment
effect estimates in randomized experiments. Proc Natl Acad Sci USA 113(27):
7383â€“7390.
12. Chen S, Donoho D, Saunders M (1998) Atomic decomposition for basis pursuit. SIAM J
Sci Comput 20(1):33â€“61.
13. Tibshirani R (1996) Regression shrinkage and selection via the lasso. J R Stat Soc B 58:
267â€“288.
14. Zou H, Hastie T (2005) Regularization and variable selection via the elastic net. J R Stat
Soc B 67(2):301â€“320.
15. Bickel P, Klaassen C, Ritov Y, Wellner J (1998) Efficient and Adaptive Estimation for
Semiparametric Models (Springer, New York).
16. Hahn J (1998) On the role of the propensity score in efficient semiparametric estimation of average treatment effects. Econometrica 66(2):315â€“331.
17. Imbens GW (2004) Nonparametric estimation of average treatment effects under
exogeneity: A review. Rev Econ Stat 86(1):4â€“29.
18. Robins JM, Rotnitzky A (1995) Semiparametric efficiency in multivariate regression
models with missing data. J Am Stat Assoc 90(429):122â€“129.
19. Athey S, Imbens GW, Wager S (2016) Efficient inference of average treatment effects
in high dimensions via approximate residual balancing. arXiv:1604.07125.
20. Belloni A, Chernozhukov V, FernÃ¡ndez-Val I, Hansen C (2016) Program evaluation
with high-dimensional data. Econometrica, in press.
21. Belloni A, Chernozhukov V, Hansen C (2014) Inference on treatment effects after
selection among high-dimensional controls. Rev Econ Stud 81(2):608â€“650.
22. Farrell MH (2015) Robust inference on average treatment effects with possibly more
covariates than observations. J Econom 189(1):1â€“23.
23. Neyman J (1990) On the application of probability theory to agricultural experiments.
Essay on principles, section 9. translation of original 1923 paper, which appeared in
Roczniki Nauk Rolniczych. Stat Sci 5(4):465â€“472.
24. Rubin DB (1974) Estimating causal effects of treatments in randomized and nonrandomized studies. J Educ Psychol 66(5):688â€“701.

25. Friedman J, Hastie T, Tibshirani R (2010) Regularization paths for generalized linear
models via coordinate descent. J Stat Softw 33(1):1â€“22.
26. Pitkin E, et al. (2013) Improved precision in estimating average treatment effects.
arXiv:1311.0291.
27. Bickel PJ, Ritov Y, Tsybakov AB (2009) Simultaneous analysis of lasso and Dantzig
selector. Ann Stat 37(4):1705â€“1732.
28. Meinshausen N, Yu B (2009) Lasso-type recovery of sparse representations for highdimensional data. Ann Stat 37(1):246â€“270.
29. Javanmard A, Montanari A (2014) Confidence intervals and hypothesis testing for
high-dimensional regression. J Mach Learn Res 15(1):2869â€“2909.
30. Van de Geer S, BÃ¼hlmann P, Ritov Y, Dezeure R (2014) On asymptotically optimal
confidence regions and tests for high-dimensional models. Ann Stat 42(3):1166â€“1202.
31. Zhang CH, Zhang SS (2014) Confidence intervals for low dimensional parameters in
high dimensional linear models. J R Stat Soc B 76(1):217â€“242.
32. Cai TT, Guo Z (2015) Confidence intervals for high-dimensional linear regression:
Minimax rates and adaptivity. arXiv:1506.05539.
33. Javanmard A, Montanari A (2015) De-biasing the lasso: Optimal sample size for
Gaussian designs. arXiv:1508.02757.
34. Dobriban E, Wager S (2015) High-dimensional asymptotics of prediction: Ridge regression and classification. arXiv:1507.03003.
35. Bai Z, Silverstein JW (2010) Spectral Analysis of Large Dimensional Random Matrices
(Springer, New York), Vol 20.
36. Marchenko VA, Pastur LA (1967) Distribution of eigenvalues for some sets of random
matrices. Math USSR Sbornik 114(4):507â€“536.
37. Aronow PM, Middleton JA (2013) A class of unbiased estimators of the average
treatment effect in randomized experiments. J Causal Inference 1(1):135â€“154.
38. Tibshirani R, Efron B (2002) Pre-validation and inference in microarrays. Stat Appl
Genet Mol Biol 1(1):1â€“15.
39. Imai K, Ratkovic M (2013) Estimating treatment effect heterogeneity in randomized
program evaluation. Ann Appl Stat 7(1):443â€“470.
40. Breiman L (2001) Random forests. Mach Learn 45(1):5â€“32.
41. Scornet E, Biau G, Vert JP (2015) Consistency of random forests. Ann Stat 43(4):
1716â€“1741.
42. Athey S, Imbens G (2016) Recursive partitioning for heterogeneous causal effects.
Proc Natl Acad Sci USA 113(27):7353â€“7360.
43. Green DP, Kern HL (2012) Modeling heterogeneous treatment effects in survey experiments with Bayesian additive regression trees. Public Opin Q 76(3):491â€“511.
44. Hill JL (2012) Bayesian nonparametric modeling for causal inference. J Comput Graph
Stat 20(1):217â€“240.
45. Wager S, Athey S (2015) Estimation and inference of heterogeneous treatment effects using random forests. arXiv:1510.04342.
46. Liaw A, Wiener M (2002) Classification and regression by randomForest. R News 2(3):
18â€“22.
47. Wager S (2016) Causal inference with random forests. PhD thesis (Stanford University,
Stanford, CA).
48. Rosenbaum PR (2002) Observational Studies (Springer, New York).
49. Rubin DB (2007) The design versus the analysis of observational studies for causal
effects: Parallels with the design of randomized trials. Stat Med 26(1):20â€“36.

random forests result in a larger improvement in precision than
lasso-based cross-estimation.

12678 | www.pnas.org/cgi/doi/10.1073/pnas.1614732113

Wager et al.

