Journal of the American Statistical Association

ISSN: 0162-1459 (Print) 1537-274X (Online) Journal homepage: https://www.tandfonline.com/loi/uasa20

Estimating Individualized Treatment Rules Using
Outcome Weighted Learning
Yingqi Zhao , Donglin Zeng , A. John Rush & Michael R. Kosorok
To cite this article: Yingqi Zhao , Donglin Zeng , A. John Rush & Michael R. Kosorok (2012)
Estimating Individualized Treatment Rules Using Outcome Weighted Learning, Journal of the
American Statistical Association, 107:499, 1106-1118, DOI: 10.1080/01621459.2012.695674
To link to this article: https://doi.org/10.1080/01621459.2012.695674

Accepted author version posted online: 04
Jun 2012.
Published online: 08 Oct 2012.
Submit your article to this journal

Article views: 2538

View related articles

Citing articles: 162 View citing articles

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=uasa20

Estimating Individualized Treatment Rules Using
Outcome Weighted Learning
Yingqi ZHAO, Donglin ZENG, A. John RUSH, and Michael R. KOSOROK
There is increasing interest in discovering individualized treatment rules (ITRs) for patients who have heterogeneous responses to treatment.
In particular, one aims to find an optimal ITR that is a deterministic function of patient-specific characteristics maximizing expected clinical
outcome. In this article, we first show that estimating such an optimal treatment rule is equivalent to a classification problem where each
subject is weighted proportional to his or her clinical outcome. We then propose an outcome weighted learning approach based on the
support vector machine framework. We show that the resulting estimator of the treatment rule is consistent. We further obtain a finite sample
bound for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of
the proposed approach is demonstrated via simulation studies and an analysis of chronic depression data.
KEY WORDS:

Bayes classifier; Cross-validation; Dynamic treatment regime; Individualized treatment rule; Risk bound; RKHS; Weighted
support vector machine.

1. INTRODUCTION
In many different diseases, patients can show significant heterogeneity in response to treatments. In some cases, a drug that
works for a majority of individuals may not work for a subset of
patients with certain characteristics. For example, molecularly
targeted cancer drugs are only effective for patients with tumors
expressing targets (Grünwald and Hidalgo 2003; Buzdar 2009),
and significant heterogeneity exists in responses among patients
with different levels of psychiatric symptoms (Piper et al. 1995;
Crits-Christoph et al. 1999). Thus, significant improvements in
public health could potentially result from judiciously treating
individuals based on his or her prognostic or genomic data rather
than a “one size fits all” approach. Treatments and clinical trials
tailored for patients have enjoyed recent popularity in clinical
practice and medical research, and, in some cases, have provided
high quality recommendations accounting for individual heterogeneity (Sargent et al. 2005; Flume et al. 2007; Insel 2009).
These proposals have focused on smaller, specific, and welldefined subgroups, sought to provide guidance in clinical decision making based on individual differences, and have attempted
to achieve better risk minimization and benefit maximization.
One statistical approach for developing individual-adaptive
interventions is to classify subjects into different risk levels
estimated by a parametric or semiparametric regression model
using prognostic factors, and then to assign therapy according to
risk level (Eagle et al. 2004; Marlowe et al. 2007; Cai et al. 2010).
However, the parametric or semiparametric model assumptions
may not be valid due to the complexity of the disease mechanism and individual heterogeneity. Moreover, these approaches
require preknowledge in allocating the optimal treatment to each

Yingqi Zhao is Postdoctoral Fellow, Department of Biostatistics, University
of North Carolina at Chapel Hill, NC 27599 (E-mail: yqzhao@live.unc.edu).
Donglin Zeng is Professor, Department of Biostatistics, University of North
Carolina at Chapel Hill, NC 27599 (E-mail: dzeng@email.unc.edu). A. John
Rush is Professor and Vice-Dean, Office of Clinical Sciences, Duke-National
University of Singapore Graduate Medical School, Singapore 169857 (E-mail:
john.rush@duke-nus.edu.sg). Michael R. Kosorok is Professor and Chair, Department of Biostatistics, and Professor, Department of Statistics and Operations
Research, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599
(E-mail: kosorok@unc.edu). The first, second, and fourth authors were partially
funded by NCI Grant P01 CA142538. The authors thank the editor, associate
editor, and referees for their helpful comments.

risk category. There is also a significant literature examining discovery and development of personalized treatment relying on
predicting patient responses to optional regimens (Rosenwald
et al. 2002; van’t Veer and Bernards 2008), where the optimal
decision leads to the best predicted outcome. One recent article
by Qian and Murphy (2011) applies a two-step procedure that
first estimates a conditional mean for the response and then estimates the rule maximizing this conditional mean. A rich linear
model is used to sufficiently approximate the conditional mean,
with the estimated rule derived via l1 penalized least squares
(l1 -PLS). The method includes variable selection to facilitate
parsimony and ease of interpretation. The conditional mean approximation requires estimating a prediction model of the relationship between pretreatment prognostic variables, treatments,
and clinical outcome using a prediction model. Reduction in the
mean response is related to the excess prediction error, through
which an upper bound can be constructed for the mean reduction of the associated treatment rule. However, by inverting the
model to find the optimal treatment rule, this method emphasizes prediction accuracy of the clinical response model instead
of directly optimizing the decision rule.
In this article, we proposed a new method for solving this
problem that circumvents the need for conditional mean modeling followed by inversion by directly estimating the decision
rule that maximizes clinical response. Specifically, we demonstrate that the optimal treatment rule can be estimated within a
weighted classification framework, where the weights are determined from the clinical outcomes. We then alleviate the computational problem by substituting the 0-1 loss in the classification
with a convex surrogate loss as is done with the support vector
machine (SVM) via the hinge loss (Cortes and Vapnik 1995).
The directness of this outcome weighted learning (OWL) approach enables us to better select targeted therapy while making
full use of available information.
The remainder of the article is organized as follows. In
Section 2, we provide the mathematical concepts and framework
for individualized treatment rules (ITRs), and then formulate

1106

© 2012 American Statistical Association
Journal of the American Statistical Association
September 2012, Vol. 107, No. 499, Theory and Methods
DOI: 10.1080/01621459.2012.695674

Zhao et al.: Estimating Individualized Treatment Rules Using Outcome Weighted Learning

1107

the problem as OWL. The proposed weighted SVM approach
for constructing the optimal ITR is then developed in detail. In
Section 3, consistency and risk-bound results are established for
the estimated rules. Faster convergence rates can be achieved
with additional marginal assumptions on the data generating
distribution. We present simulation studies to evaluate performance of the proposed method in Section 4. The method is then
illustrated on the Nefazodone-CBASP data (Keller et al. 2000)
in Section 5. In Section 6, we discuss future work. The proofs
of theoretical results are given in the Appendix.

optimal ITR, and are likely to produce a suboptimal ITR if the
model for R given (X, A) is overfitted. As an alternative, we
propose a nonparametric approach that directly maximizes the
value function based on an OWL method.
To illustrate our approach, we first notice that searching for
the optimal ITR, D∗ , which maximizes V(D), is equivalent to
finding D∗ that minimizes

2. METHODOLOGY

The latter can be viewed as a weighted classification error, for
which we want to classify A using X but we also weigh each
misclassification event by R/(Aπ + (1 − A)/2). Hence, using
the observed data, we approximate the weighted classification
error by

2.1 Individualized Treatment Rule (ITR)
We assume the data are collected from a two-arm randomized trial. That is, treatment assignments, denoted by
A ∈ A = {−1, 1}, are independent of any patient’s prognostic variables, which are denoted as a d-dimensional vector
X = (X1 , . . . , Xd )T ∈ X . We let R be the observed clinical outcome, also called the “reward,” and assume that R is bounded,
with larger values of R being more desirable. Thus, an ITR is
a map from the space of prognostic variables, X , to the space
of treatments, A. An optimal ITR is a rule that maximizes the
expected reward if implemented.
Mathematically, we can quantify the optimal ITR in terms
of the relationship among (X, A, R). To see this, denote the
distribution of (X, A, R) by P and expectation with respect
to the P is denoted by E. For any given ITR D, we let P D
denote the distribution of (X, A, R) given that A = D(X), that
is, the treatments are chosen according to the rule D; correspondingly, the expectation with respect to P D is denoted by E D . Then
under the assumption that P (A = a) > 0 for a = 1 and −1, it
is clear that P D is absolutely continuous with respect to P and
dP D /dP = I (a = D(x))/P (A = a), where I (·) is the indicator
function. Thus, the expected reward under the ITR D is given as




I (A = D(X))
dP D
dP = E
R ,
E D (R) = RdP D = R
dP
Aπ + (1 − A)/2
where π = P (A = 1). This expectation is called the value function associated with D and is denoted by V(D). Consequently,
an optimal ITR, D∗ , is a rule that maximizes V(D), that is,


I (A = D(X))
R .
D∗ ∈ argmax E
Aπ + (1 − A)/2
D
Note that D∗ does not change if R is replaced by R + c for any
constant c. Thus, without loss of generality, we assume that R
is nonnegative in the following.
2.2 Outcome Weighted Learning (OWL) for Estimating
Optimal ITR
Assume that we observe independent and identically distributed (iid) data (Xi , Ai , Ri ), i = 1, . . . , n from the two-arm
randomized trial described above. Previous approaches to
estimating optimal ITR first estimate E(R|X, A), using the observed data via parametric or semiparametric models, and then
estimate the optimal decision rule by comparing the predicted
value E(R|X, A = 1) versus E(R|X, A = −1) (Robins 2004;
Moodie, Platt, and Kramer 2009; Qian and Murphy 2011).
As discussed before, these approaches indirectly estimate the

E[R|A = 1] + E[R|A = −1] − V(D)


I (A = D(X))
= E
R .
Aπ + (1 − A)/2

n−1

n

i=1

Ri
I (Ai = D(Xi ))
Ai π + (1 − Ai )/2

and seek to minimize this expression to estimate D∗ . Since
D(x) can always be represented as sign(f (x)), for some decision
function f , minimizing the above expression for D∗ is equivalent
to minimizing
n−1

n

i=1

Ri
I (Ai = sign(f (Xi )))
Ai π + (1 − Ai )/2

(2.1)

to obtain the optimal f ∗ , and then setting D∗ (x) = sign(f ∗ (x)).
The above minimization also has the following interpretation.
That is, we intend to find a decision rule that assigns treatments
to each subject only based on their prognostic information. For
subjects observed to have large rewards, this rule is apt to recommend the same treatment assignments that the subjects have
actually received; however, for subjects with small rewards, the
rule is more likely to give the opposite treatment assignments
to what they received. In other words, if we stratify subjects
into different strata based on the rewards, we will expect that
the optimal ITR misclassifies less subjects in the high reward
stratum as compared to the low reward stratum.
In the machine learning literature, Equation (2.1) can
be viewed as a weighted summation of 0-1 loss. It is well
known that minimizing Equation (2.1) is difficult due to the
discontinuity and nonconvexity of 0-1 loss. To alleviate this
difficulty, one common approach is to find a convex surrogate
loss for the 0-1 loss in Equation (2.1) and develop a tractable
estimation procedure (Lugosi and Vayatis 2004; Zhang 2004;
Steinwart 2005). Among many choices of surrogate loss, one
of the most popular is the hinge loss used in the context of
the SVM (Cortes and Vapnik 1995), which we will adopt in
this article. Furthermore, we penalize the complexity of the
decision function to avoid overfitting. In other words, instead
of minimizing Equation (2.1), we aim to minimize
−1

n

n

i=1

Ri
(1 − Ai f (Xi )))+ + λn f 2 ,
Ai π + (1 − Ai )/2
(2.2)

where x + = max(x, 0) and f  is some norm for f . In this
way, we cast the problem of estimating the optimal ITR into a
weighted classification problem using SVM techniques.

1108

Journal of the American Statistical Association, September 2012

2.3 Linear Decision Rule for Optimal ITR
Suppose that the decision function f (x) minimizing
Equation (2.2) is a linear function of x, that is, f (x) = β, x +
β0 , where ·, · denotes the inner product in Euclidean space.
Then the corresponding ITR will assign a subject with prognostic value X into treatment 1 if β, X + β0 > 0 and –1 otherwise.
In Equation (2.2), we define f  as the Euclidean norm of β.
Following the usual SVM, we introduce a slack variable ξi for
subject i to allow a small portion of wrong classification. Denote
C > 0 as the classifier margin. Then minimizing Equation (2.2)
can be rewritten as
C subject to Ai (β, Xi  + β0 )

max

β,β0 ,β=1

≥ C(1 − ξi ), ξi ≥ 0,

 Ri
πi

ξi < s,

where πi = π I (Ai = 1) + (1 − π )I (Ai = −1) and s is a constant depending on λn . This is equivalent to
1
min β2 subject to Ai (β, Xi  + β0 )
2
 Ri
≥ (1 − ξi ), ξi ≥ 0,
ξi < s,
πi
that is,
 Ri
1
min β2 + κ
ξi
2
πi
i=1
n

subject to Ai (β, Xi  + β0 )

estimated decision rule is determined by the support vectors
with α̂i > 0.
2.4 Nonlinear Decision Rule for Optimal ITR
The previous section targets a linear boundary of prognostic
variables. This may not be practically useful since the dimension
of the prognostic variables can be quite high and complicated
relationships may be involved between the desired treatments
and these variables. However, we can easily generalize the previous approach to obtain a nonlinear decision rule for obtaining
the optimal ITR.
We let k : X × X → R, called a kernel function, be continuous, symmetric, and positive semidefinite. Given a real-valued
kernel function k, we can associate with it a reproducing kernel
Hilbert space (RKHS) Hk , which is the completion of the linear
span of all functions {k(·, x), x ∈ X }. The norm in Hk , denoted
by  · k , is induced by the following inner product,
f, gk =

−

n


for f (·) = ni=1 αi k(·, xi ) and g(·) = m
j =1 βj k(·, xj ).
We note that our decision function f (x) is from Hk equipped
with norm  · k . Thus, since any function in Hk takes the form
m
i=1 αi k(·, xi ), it can be shown that the optimal decision function is given by
n


n
 

 
αi Ai XiT β + β0 − (1 − ξi ) −
µi ξi ,

i=1

max
α

n

i=1

1 
αi αj Ai Aj Xi , Xj 
2 i=1 j =1
n

αi −

α̂i Ai k(X, Xi ) + β̂0 ,

i=1

where (α̂1 , . . . , α̂n ) solves
max
α

n

i=1

1 
αi αj Ai Aj k(Xi , Xj )
2 i=1 j =1
n

αi −

n

subject to 0 ≤ αi ≤ κRi /πi , i = 1, . . . , n, and ni=1 αi Ai = 0.
We note that if we choose k(x, y) = x, y, then the obtained
rule reduces to the previous linear rule.
3. THEORETICAL RESULTS

i=1

with αi ≥ 0, µi ≥ 0. Taking derivatives with respect to (β, β0 )
and ξi , we have β = ni=1 αi Ai Xi , 0 = ni=1 αi Ai and αi =
κRi /πi − µi . Plugging these equations into the Lagrange function, we obtain the dual problem

αi βj k(xi , xj ),

i=1 j =1

≥ (1 − ξi ), ξi ≥ 0,

where κ > 0 is a tuning parameter and Ri /πi is the weight for
the ith point. We observe that the main difference compared
to standard SVM is that we weigh each slack variable ξi with
Ri /πi .
After introducing Lagrange multipliers, the Lagrange function becomes
n

Ri
1
β2 + κ
ξi
2
πi
i=1

n 
m


In this section, we establish consistency of the optimal ITR
estimated using OWL. We further obtain a risk bound for the
estimated ITR and show how the bound can be improved for
certain specific, realistic situations.

n

n
i=1

subject to 0 ≤ αi ≤ κRi /πi , i = 1, . . . , n, and
αi Ai = 0.
Quadratic programming algorithms from many widely available software packages can be used to solve this dual problem.
Finally, we obtain that

α̂i Ai Xi ,
β̂ =
α̂i >0

and β̂0 can be solved using the margin points (0 < α̂i , ξˆi =
0) subject to the Karush-Kuhn-Tucker conditions (Hastie,
Tibshirani, and Friedman 2009, p. 421). The decision rule is
given by sign{β̂, X + β̂0 }. Similar to the traditional SVM, the

3.1 Notation
For any ITR D(x) = sign(f (x)) associated with decision
function f (x), we define


R
I (A = sign(f (X)))
R( f ) = E
Aπ + (1 − A)/2
and the minimal risk (called Bayes risk in the learning literature)
as R∗ = inf f {R( f )|f : X → R}. Thus, for the optimal ITR
D∗ (x) = sign(f ∗ (x)) (called the Bayes classifier in the learning
literature), R∗ = R(f ∗ ). In terms of the value function, we note
that V(D∗ ) − V(D) = R( f ) − R(f ∗ ).
In the OWL approach, we substitute 0-1 loss I (A = sign(f
(X))) by a surrogate loss, φ(Af (X)), where φ(t) = (1 − t)+ .

Zhao et al.: Estimating Individualized Treatment Rules Using Outcome Weighted Learning

3.4 Consistency and Risk Bounds

Thus, we define the φ-risk


R
Rφ ( f ) = E
φ(Af (X)) ,
Aπ + (1 − A)/2
and, similarly, the minimal φ-risk as R∗φ = inf f {Rφ ( f )|f :
X → R}.
Recall that the estimated optimal ITR is given by sign(fˆn (X)),
where
1  Ri
{1 − Ai f (Xi )}+ + λn f 2k .
fˆn = argmin
n i=1 πi
f ∈Hk
n

(3.1)
3.2 Fisher Consistency
We establish Fisher consistency of the decision function based
on surrogate loss φ(t). Specifically, the following result holds:

The purpose of this section is to establish the consistency of fˆn , and, moreover, to derive the convergence rate of
R(fˆn ) − R∗ .
First, the following theorem shows that the risk due to fˆn does
converge to R∗ , and, equivalently, the value of fˆn converges
to the optimal value function. Results on consistency of the
SVM have been shown in current literature (e.g., Zhang 2004).
Here we apply the empirical process techniques to show that the
proposed OWL estimator is consistent. The proof of the theorem
is deferred to the Appendix.
Theorem 3.3. Assume that we choose a sequence λn > 0
such that λn → 0 and λn n → ∞. Then for all distributions P,
we have that in probability,


ˆ
lim Rφ (fn ) − inf Rφ ( f ) = 0,
n→∞

Proposition 3.1. For any measurable function f , if f˜ minimizes Rφ ( f ), then D∗ (x) = sign(f˜(x)).
Proof. First, we note
∗

D (x) = sign {E[R|X = x, A = 1] − E[R|X = x, A = −1]} .
Next, for each x ∈ X ,


φ(Af (X))
X=x
E R
Aπ + (1 − A)/2
= E(R|A = 1, X = x)(1 − f (x))
+ E(R|A = −1, X = x)(1 + f (x))
= ((E(R|A = −1, X = x) − E(R|A = 1, X = x))f (x)
+ E(R|A = −1, X = x) + E(R|A = 1, X = x)).

Therefore, f˜(x), which minimizes Rφ ( f ), should be positive
if E(R|A = 1, X = x) > E(R|A = −1, X = x) and negative
if E(R|A = 1, X = x) < E(R|A = −1, X = x). That is, f˜(x)

has the same sign as D∗ (x). The result holds.
The proposition is analogous to results for SVM (e.g., Lin
2002). This theorem justifies the validity of using φ(t) as the
surrogate loss in OWL.
3.3 Excess Risk for R( f ) and Rφ ( f )
The following result shows that for any decision function f ,
the excess risk of f under 0-1 loss is no larger than the excess
risk of f under the hinge loss. Thus, the loss of the value function
due to the ITR associated with f can be bounded by the excess
risk under the hinge loss. The proof of the theorem can be found
in the Appendix.
Theorem 3.2. For any measurable f : X → R and any probability distribution for (X, A, R),
R( f ) − R∗ ≤ Rφ ( f ) − R∗φ .

1109

(3.2)

The proof follows the general arguments by Bartlett, Jordan,
and McAuliffe (2006), in which they bound the risk associated
with 0-1 loss in terms of the risk from surrogate loss, using a
convexified variational transform of the surrogate loss. In our
proof, we extend this concept to our setting by establishing the
validity of a weighted version of such a transformation.

f ∈H̄k

where H̄k denotes the closure of Hk . Thus, if f ∗ belongs to the
closure of lim supn→∞ Hk , where Hk can potentially depend on
n, we have limn→∞ Rφ (fˆn ) = R∗φ in probability. It then follows
that limn→∞ R(fˆn ) = R∗ in probability.
One special situation where f ∗ belongs to the limit space of
Hk is when we choose Hk to be an RKHS with Gaussian kernel
and let the kernel bandwidth decrease to zero as n → ∞. This
will be shown in Theorem 3.4 below.
We now wish to derive the convergence rate of R(fˆn ) − R∗
under certain regularity conditions on the distribution P. Specifically, we need the following “geometric noise” assumption for
P (Steinwart and Scovel 2007): Let
η(x) =

E[R|X = x, A = 1] − E[R|X = x, A = −1]
+ 1/2,
E[R|X = x, A = 1] + E[R|X = x, A = −1]
(3.3)

then 2η(x) − 1 is the decision boundary for the optimal ITR.
We further define X + = {x ∈ X : 2η(x) − 1 > 0}, and X − =
{x ∈ X : 2η(x) − 1 < 0}. A distance function to the boundary
between X + and X − is (x) = d̃(x, X + ) if x ∈ X − , (x) =
d̃(x, X − ) if x ∈ X + , and (x) = 0 otherwise, where d̃(x, O)
denotes the distance of x to a set O with respect to the Euclidean
norm. Then the distribution P is said to have geometric noise
exponent 0 < q < ∞, if there exists a constant C > 0 such that



(X)2
|2η(X) − 1| ≤ Ct qd/2 , t > 0. (3.4)
E exp −
t
In some sense, this geometric noise exponent describes the behavior of the distribution in a neighborhood of the decision
boundary. It is affected by how fast the density of the distance (X) decays along the boundary. For example, assume
the boundary is linear, in which case (x) = |2η(x) − 1|. If for
the density of (X), defined as f (u), we have f (u) ∼ up when
u is close to 0, then we can show q = (p + 2)/d. Larger p corresponds to a faster decaying rate of the density, resulting in
a larger q accordingly. Another example is distinctly separable
data, that is, when |2η(x) − 1| > δ > 0, for some constant δ,
and η is continuous, q can be arbitrarily large.
In addition to this specific assumption for P, we also restrict the choice of RKHS to the space associated with Gaussian

1110

Journal of the American Statistical Association, September 2012

Radial Basis Function (RBF) kernels, that is,


k(x, x ) = exp − σn2 x − x 2 , x, x ∈ X ,
where σn > 0 is a parameter varying with n. The tuning parameter σn is related to approximation properties of Gaussian RBF
kernels. When σn goes large, only observations in the small
neighborhood contribute to the prediction, in which case we
obtain a nonlinear decision boundary or even nonparametric decision rule. If σn does not diverge, then points further away can
contribute to the prediction, resulting in a nearly linear boundary. One advantage of using the Gaussian kernel is that we can
determine the complexity of Hk in terms of capacity bounds
with respect to the empirical L2 -norm, defined as
 n
1/2
1
2
f − gL2 (Pn ) =
|f (Xi ) − g(Xi )|
.
n i=1
For any > 0, the covering number of functional class F with
respect to L2 (Pn ), N (F, , L2 (Pn )), is the smallest number of
L2 (Pn ) -balls needed to cover F, where an L2 (Pn ) -ball
around a function g ∈ F is the set {f ∈ F : f − gL2 (Pn ) < }.
Specifically, according to Theorem 2.1 in the article by
Steinwart and Scovel (2007), we have that for any > 0,
sup log N (BHk , , L2 (Pn )) ≤ cν,δ,m σn(1−ν/2)(1+δ)d

−ν

,

(3.5)

Pn

where BHk is the closed unit ball of Hk , and ν and δ are any
numbers satisfying 0 < ν ≤ 2 and δ > 0.
Under the above conditions, we obtain the following theorem:
Theorem 3.4. Let P be a distribution of (X, A, R) satisfying
condition (3.4) with noise exponent q > 0. Then for any δ >
0, 0 < ν < 2, there exists a constant C (depending on ν, δ, d,
−1/(q+1)d
,
and π ) such that for all τ ≥ 1 and σn = λn
P r ∗ (R(fˆn ) ≤ R∗ + ) ≥ 1 − e−τ ,
where P r ∗ denotes the outer probability for possibly nonmeasurable sets, and


q
(2−ν)(1+δ)
2
τ
2
q+1
− 2+ν
+ (2+ν)(1+q)
− 2+ν
.
n
+
+ λn
= C (λn )
nλn
The first two terms bound the stochastic error, which arises
from the variability inherent in a finite sample size and which
depends on the complexity of Hk in terms of covering numbers,
while the third term controls the approximation error due to
using Hk , which depends on both σn and the noise behavior
in the underlying distribution. We expect better approximation
properties when the RKHS is more complex, but, conversely,
we also expect larger stochastic variability. Using the above
expression, an optimal choice of λn that balances bias and
variance is given by
2(1+q)

λn = n− (4+ν)q+2+(2−ν)(1+δ) ,
so the optimal rate for the risk is


2q
R(fˆn ) − R∗ = Op n− (4+ν)q+2+(2−ν)(1+δ) .
In particular, when data are well separated, q can be sufficiently
large and we can let (δ, ν) be sufficiently small. Then the
convergence rate almost achieves the rate n−1/2 . However,

if the marginal distribution of X has continuous density
along the boundary, it can be calculated that q = 2/d. In this
case, the convergence rate is approximately n−2/(d+2) . Clearly,
the speed of convergence is slower with larger dimension of the
prognostic variable space.
To prove Theorem 3.4, we note that according to
Theorem 3.2, it suffices to prove the result for the excess φ
risk. We also use the fact that
Rφ (fˆn ) − R∗φ = Rφ (fˆn ) − inf Rφ ( f ) + inf Rφ ( f ) − R∗φ .
Hk

Hk

We will then bound the first difference on the right-hand side
using the empirical counterpart plus the stochastic variability
due to the finite sample approximation. The latter can be controlled using large deviation results from empirical processes
and some preliminary bound for fˆn k . The second difference
on the right-hand side will be bounded by using the approximation property of the RKHS and the geometric noise assumption
of the underlying distribution P. The proof is modified based on
the literature by Vert and Vert (2006) and Steinwart and Scovel
(2007), where the weights in the loss function are taken into
consideration. The details are provided in the Appendix.
3.5 Improved Rate with Data Completely Separated
In this section, we show that a faster convergence rate can be
obtained if the data are completely separated. We assume
(A1) ∀x ∈ X , |η(x) − 1/2| ≥ η0 > 0, where η(x) is defined
in Equation (3.3), and η is continuous.
(A2) ∀x ∈ X , min(η(x), 1 − η(x)) ≥ η1 > 0.
Assumption (A1) can be referred as a “low noise” condition equivalent to |E(R|A = 1, X) − E(R|A = −1, X)| ≥ η0 .
Thus, a jump of η(x) at the level of 1/2 requires a gap between
the rewards gained from treatment 1 and −1 on the same patient.
This assumption is an adaptation of the noise condition used in
classical SVM to obtain fast learning rates and it is essentially
equivalent to one of the conditions in the article by Blanchard,
Bousquet, and Massart (2008).
Theorem 3.5. Assume that (A1) and (A2) are satisfied. For
any ν ∈ (0, 1) and q ∈ (0, ∞), let λn = O(n−1/(ν+1) ) and σn =
−1/(q+1)d
. Then
λn


q
1
R(fˆn ) − R∗ = Op n− ν+1 q+1 .
We can let q go to ∞ and ν go to zero, and this theorem shows that the convergence rate for R(fˆn ) − R(f ∗ ) is almost n−1 , a much faster rate compared to what was given in
Theorem 3.4. This result is similar to SVM results described in
the literature by Tsybakov (2004), Steinwart and Scovel (2007),
and Blanchard, Bousquet, and Massart (2008).
To prove Theorem 3.5, we can rewrite the minimization problem in Equation (3.1) as
1  Ri
{1 − Ai f (Xi )}+ + λS 2 .
n i=1 πi
n

min+

S∈R

min

f :f k ≤S

Thus, the problem can be viewed in the model selection framework: a collection of models are balls in Hk , and for each model,
we solve the penalized empirical φ-risk minimization to obtain

Zhao et al.: Estimating Individualized Treatment Rules Using Outcome Weighted Learning

an estimator fˆn . We can use a result for model selection, presented in Theorem 4.3 in the article by Blanchard, Bousquet,
and Massart (2008), to choose the model that yields the minimal penalized empirical φ-risk among all the models. We need
to verify the conditions required for the theorem based on the
weighted hinge loss and the condition on the covering number
of functional class F with respect to L2 (Pn ), that is, condition
(3.5). Proof details are provided in the Appendix.

8
7
6
5

σn

4

4. SIMULATION STUDY
3

We have conducted extensive simulations to assess the smallsample performance of the proposed method. In these simulations, we generate 50-dimensional vectors of prognostic variables X1 , . . . , X50 , consisting of independent U [−1, 1] variates.
The treatment A is generated from {−1, 1} independently of X
with P (A = 1) = 1/2. The response R is normally distributed
with mean Q0 = 1 + 2X1 + X2 + 0.5X3 + T0 (X, A) and standard deviation 1, where T0 (X, A) reflects the interaction between treatment and prognostic variables and is chosen to vary
according to the following four different scenarios:
(1)
(2)
(3)
(4)

T0 (X, A) = 0.442(1 − X1 − X2)A.
T0 (X, A) = X2 − 0.25X12 − 1 A.

T0 (X, A) = 0.5 − X12 − X22 X12 + X22 − 0.3 A.
T0 (X, A) = 1 − X13 + exp(X32 + X5 + 0.6X6 −
(X7 + X8 )2 )A.

The decision boundaries in the first three scenarios are determined by X1 and X2 . Scenario 1 corresponds to a linear decision
boundary in truth, where the shape of the boundary in Scenario
2 is a parabola. The third is a ring example, where the patients
on the ring are assigned to one treatment, and another if inside
or outside the ring. The decision boundary in the fourth example
is fairly nonlinear in covariates, depending on covariates other
than X1 and X2 . For each scenario, we estimate the optimal ITR
by applying OWL. We use the Gaussian kernel in the weighted
SVM algorithm. There are two tuning parameters: λn , the
penalty parameter, and σn , the inverse bandwidth of the kernel.
Since λn plays a role in controlling the severity of the penalty on
the functions and σn determines the complexity of the function
class used, σn should be chosen adaptively from the data simultaneously with λn . To illustrate this, Figure 1 shows the contours of
the value function for the first scenario with different combinations of (λn , σn ) when n = 30. We can see that λn interacts with
σn , with larger λn generally coupled with smaller σn for equivalent value function levels. In our simulations, we apply a fivefold cross-validation procedure in which we search over a prespecified finite set of (λn , σn ) to select the pair maximizing the
average of the estimated values from the validation data. In case
of tied values for parameter pair choices, we first choose the set
of pairs with smallest λn and then select the one with largest σn .
Additionally, comparison is made among the following four
methods:
(1) The proposed OWL using Gaussian kernel (OWLGaussian).
(2) The proposed OWL using linear kernel (OWL-Linear).
(3) The l1 -PLS method developed by Qian and Murphy
(2011), which approximates E(R|X, A) using the ba-

1111

1.2

1.1

1.

1

2

2

1.1

1.2

1
1

1.1 1.2
1

2

1

3

1.1

4

λn

1.2
5

6

7

8

Figure 1. Contour plots of value function for Example 1 with λn ∈
(0, 10) and σn ∈ (0, 10).

sis function set (1, X, A, XA) and applies the LASSO
method for variable selection.
(4) The ordinary least squares method (OLS), which estimates the conditional mean response using the same basis
function set as in Method 3 but without variable selection.
We consider the OWL with linear kernel (Method 2) mainly
to assess the impact of different kernels in the weighted SVM
algorithm. In this case, there is only one tuning parameter, λn ,
which can be chosen to maximize the value function in a crossvalidation procedure. The selection of the tuning parameters in
the l1 -PLS approach follows similarly. The last two approaches
estimate the optimal ITR using the sign of the difference
between the predicted E(R|X, A = 1) and the predicted
E(R|X, A = −1). In the comparisons, the performances of the
four methods are assessed by two criteria: the first criterion is
to evaluate the value function using the estimated optimal ITR
when applying to an independent and large validation data;
the second criterion is to evaluate the misclassification rates
of the estimated optimal ITR from the true optimal ITR using
the validation data. Specifically, a validation set with 10,000
observations is simulated to assess the performance of the
approaches. The estimated value function using any ITR D is
given by Pn∗ [I (A = D(X))R/P (A)]/Pn∗ [I (A = D(X))/P (A)]
(Murphy et al. 2001), where Pn∗ denotes the empirical average
using the validation data and P (A) is the probability of being
assigned treatment A.
For each scenario, we vary sample sizes for training
datasets from 30 to 100, 200, 400, and 800, and repeat the
simulation 1000 times. The simulation results are presented in
Figures 2 and 3, where we report the mean square errors (MSE)
of both value functions and misclassification rates. Simulations
show there are no large differences in the performance if we
replace the Gaussian kernel with the linear kernel in the OWL.
However, there are examples presenting advantages of the Gaussian kernel, which suggests that under certain circumstances, it
is useful to have a flexible nonparametric estimation procedure

1112

Journal of the American Statistical Association, September 2012

Example 1
0.6
0.4
0.0

0.00

200

400

600

800

0

200

400

n

n

Example 3

Example 4

800

2.0

OLS
l1−PLS
OWL−Linear
OWL−Gaussian

0.0

0.00

0.02

MSE

0.04

OLS
l1−PLS
OWL−Linear
OWL−Gaussian

600

1.0

0

MSE

OLS
l1−PLS
OWL−Linear
OWL−Gaussian

0.2

MSE

0.10

0.15

OLS
l1−PLS
OWL−Linear
OWL−Gaussian

0.05

MSE

Example 2

0

200

400

600

800

0

200

n

400

600

800

n

Figure 2. MSE for value functions of individualized treatment rules.

200

400

600

0.15
0.10

800

0

200

400

n

n

Example 3

Example 4

800

0.4

OLS
l1−PLS
OWL−Linear
OWL−Gaussian

0.0

0.0

0.1

MSE

0.2

0.3

OLS
l1−PLS
OWL−Linear
OWL−Gaussian

600

0.2

0

MSE

OLS
l1−PLS
OWL−Linear
OWL−Gaussian

0.00

0.2

MSE

0.4

OLS
l1−PLS
OWL−Linear
OWL−Gaussian

0.0

MSE

Example 2

0.05

0.6

Example 1

0

200

400

n

600

800

0

200

400

600

n

Figure 3. MSE for misclassification rates of individualized treatment rules.

800

Zhao et al.: Estimating Individualized Treatment Rules Using Outcome Weighted Learning

to identify the optimal ITR for the underlying nonparametric
structures. As demonstrated in Figures 2 and 3, the OWL with
either Gaussian kernel or linear kernel has better performance,
especially for small samples, than the other two methods, from
the points of view of producing larger value functions, smaller
misclassification rates, and lower variability of the value function estimates. Specifically, when the approximation models
used in the l1 -PLS and OLS are correct in the first scenario,
the competing methods perform well with large sample size;
however, the OWL still provides satisfactory results even if we
use a Gaussian kernel. When the optimal ITR is nonlinear in X
in the other scenarios, the OWL tends to give higher values and
smaller misclassification rates. OLS generally fails unless the
sample size is large enough since it encounters severe bias for
small sample sizes. This is because without variable selection
for OLS, there is insufficient data to fit an accurate model
with all 50 variables included. We also note that l1 -PLS has
comparatively larger MSE, resulting from high variance of the
method, which may be explained by the conflicting goals of
maximizing the value function and minimizing the prediction
error (Qian and Murphy 2011). Note that a richer class of basis
functions can be used for fitting the regression models. We have
tried a polynomial basis and a wavelet basis to see if they could
improve the performance. However, as a larger set of basis
functions enters the model, we need to take into account higher
dimensional interactions that do not necessarily yield better results. Also, we noted that higher variability is introduced with a
richer basis for the approximation space (results not shown).
Additional simulations are performed by generating binary
outcomes from a logit model. It turns out that the OWL procedures outperform a traditional logistic regression procedure
(results not shown). Finally, using empirical results, we also verify that the cross-validation procedure can indeed identify the
optimal pairs (λn , σn ) with the order desired by the theoretical
−(q+1)d
. The numerical results indicate
results, that is, σn = λn
that log2 σn is linear in log2 λn and the ratio between the slopes
is close to the reciprocal ratio between the dimensions of the
covariate spaces.

5. DATA ANALYSIS
We apply the proposed method to analyze real data from
the Nefazodone-CBASP clinical trial (Keller et al. 2000).
The study randomized 681 outpatients with nonpsychotic
chronic major depressive disorder (MDD), in a 1:1:1 ratio to
either Nafazodone, Cognitive Behavioral-Analysis System of
Psychotherapy (CBASP), or the combination of Nefazodone
and CBASP. The score on the 24-item Hamilton Rating Scale
for Depression (HRSD) was the primary outcome, where higher
scores indicate more severe depression. After excluding some
patients with missing observations, we use a subset with 647
patients for analysis. Among them, 216, 220, and 211 patients
were assigned to Nafazodone, CBASP, and the combined
treatment group, respectively. Overall comparisons using t-tests
show that the combination treatment had significant advantages
over the other treatments with respect to HRSD scores obtained
at the end of the trial, while there are no significant differences
between the nefazodone group and the psychotherapy group.

1113

Table 1. Mean depression scores (the smaller, the better) from
cross-validation procedure with different methods

Nefazodone vs. CBASP
Combination vs. Nefazodone
Combination vs. CBASP

OLS

l1 -PLS

OWL

15.87
11.75
12.22

15.95
11.28
10.97

15.74
10.71
10.86

To estimate the optimal ITR, we perform pairwise comparisons between all combinations of two treatment arms, and, for
each two-arm comparison, we apply the OWL approach. We
only present the results from the Gaussian kernel, since the
analysis shows a similarity with that of the linear kernel. Rewards used in the analyses are reversed HRSD scores and the
prognostic variables X consist of 50 pretreatment variables. The
results based on OWL are compared to results obtained using
the l1 -PLS and OLS methods that use (1, X, A, XA) in their
regression models. For comparison between methods, we calculate the value function from a cross-validation type analysis.
Specifically, the data is partitioned into five roughly equal-sized
parts. We perform the analysis on four parts of the data, and
obtain the estimated optimal ITRs using different methods. We
then compute the estimated value functions using the remaining
fifth part. The value functions calculated this way should better
represent expected value functions for future subjects, as compared to calculating value functions based on the training data.
The averages of the cross-validation value functions from the
three methods are presented in Table 1.
From Table 1, we observe that OLS produces smaller value
functions (corresponding to larger HRSD in the table) than the
other two methods, possibly because of the high-dimensional
prognostic variable space. OWL performs similarly to l1 -PLS,
but gives a 5% larger value function than l1 -PLS when comparing the Combination arm to the Nefazodone arm. In fact, when
comparing combination treatment with nefazodone only, OWL
recommends the combination treatment to all the patients in the
validation data in each round of the cross-validation procedure;
the OLS assigns the combination treatment to around 70% of the
patients in each validation subset; while the l1 -PLS recommends
the combination to all the patients in three out of five validation
sets, and 7% and 28% to the patients for the other two, indicating a very large variability. If we need to select treatment
between combination and psychotherapy alone, the OWL approach recommends the combination treatment for all patients
in the validation process. In contrast, the l1 -PLS chooses psychotherapy for 10 out of 86 patients in one round of validation,
and recommends the combination for all patients in the other
rounds. The percentages of patients who are recommended the
combination treatment range from 66% to 85% across the five
validation datasets when applying OLS. When the two single
treatments are studied, there are only negligible differences in
the estimated value functions from the three methods and the selection results also indicate an insignificant difference between
them. Thus, OWL yields ITRs with not only the best clinical
outcomes, but also the lowest variability compared to the other
methods.

1114

Journal of the American Statistical Association, September 2012

6. DISCUSSION
The proposed OWL procedure appears to be more effective,
across a broad range of possible forms of the interaction between prognostic variables and treatment, compared to previous methods. A two-stage procedure is likely to overfit the
regression model, and thus cause troubles for value function
approximation. The OWL provides a nonparametric approach
that sidesteps the inversion of the predicted model required in
other methods and benefits from directly maximizing the value
function. The convergence rates for the OWL, aiming to identify the best ITR, nearly reach the optimal for the nonparametric
SVM with the same type of assumptions on the separations. The
rates, however, are not directly comparable to Qian and Murphy
(2011), because we allow for complex multivariate interactions
and formulate the problem in a nonparametric framework. The
proposed estimator will lead to consistency and fast rate results,
but is not necessarily the most efficient approach. In some cases
when we have knowledge of the specific parametric form, a
likelihood-based method may be more efficient and aid in the
improvement of the estimation. Other possible surrogate loss
functions, for example, the negative log-likelihood for logistic
regression, can also be useful for finding the desired optimal
ITRs.
Several improvements and extensions are important to
consider. An important extension we are currently pursuing is
to right-censored clinical outcomes. Another extension involves
alleviating potential challenges arising from high-dimensional
prognostic variables. Recall that the proposed OWL is based
on a weighted SVM that minimizes the weighted hinge loss
function subject to an l2 penalty. If the dimension of the
covariate space is sufficiently large, not all the variables would
be essential for optimal ITR construction. By eliminating
the unimportant variables from the rule, we could simplify
interpretations and reduce health care costs by only requiring
collection of a small number of significant prognostic variables.
For standard SVM, the l1 penalty has been shown to be effective
in selecting relevant variables via shrinking small coefficients
to zero (Bradley and Mangasarian 1998; Zhu et al. 2003). It
outperforms the l2 penalty when there are many noisy variables
and sparse models are preferred. Other forms of penalty have
been proposed such as the F∞ norm (Zou and Yuan 2008) and
the adaptive lq penalty (Liu et al. 2007). In the future, we will
examine use of these sparse penalties in the OWL method.
In this article, we only considered binary options for treatment. When there are more than two treatment classes, although we could do a series of pairwise comparisons as done in
Section 5 above, this approach may not be optimal in terms of
identifying the best rule considering all treatments simultaneously. It would thus be worthwhile to extend the OWL approach
to settings involving three or more treatments. The case of multicategory SVM has been studied recently (Lee, Lin, and Wahba
2004; Wang and Shen 2006), and a similar generalization may
be possible for finding ITRs involving three or more treatments.
Another setting to consider is optimal ITR discovery for continuous treatments such as, for example, a continuous range
of dose levels. In this situation, we could potentially use ideas
underlying support vector regression (Vapnik 1995), where the
goal is to find a function that has at most deviation from the

response. Using a similar rationale as the proposed OWL, we
could develop corresponding procedures for continuous treatment spaces through weighing each subject by his/her clinical
outcome.
Obtaining inference for individualized treatment regimens
is also important and challenging. Due to high heterogeneities
among individuals, there may be large variations in the estimated
treatment rules across different training sets. Laber and Murphy
(2011) construct an adaptive confidence interval for the test
error under the nonregular framework. Confidence intervals for
value functions help us determine whether essential differences
exist among different decision rules. Thus, an important future
research topic is to derive the limiting distribution of V(D̂n ) −
V(D∗ ) and to derive corresponding sample size formulas to aid
in design of personalized medicine clinical trials.
In some complex diseases, dynamic treatment regimes may be
more useful than the single-decision treatment rules studied in
this article. Dynamic treatment regimes are customized sequential decision rules for individual patients that can adapt over time
to an evolving illness. Recently, this research area has been of
great interest in long-term management of chronic disease (see,
e.g., Murphy et al. 2001; Thall, Sung, and Estey 2002; Murphy
2003; Robins 2004; Moodie, Richardson, and Stephens 2007;
Zhao et al. 2011). Extension of the proposed OWL approach to
the dynamic setting would be of great interest.
APPENDIX: PROOFS
Proof of Theorem 3.2
We consider the case where rewards are discrete. Arguments for the
continuous rewards setting follow similarly. Let ηr (x) = p(A = 1|R =
r, X = x) and qr (x) = rp(R = r|X = x). We can write



I (A = sign(f (X)))
R = r, X
rp(R = r|X)E
R( f ) = E
Aπ + (1 − A)/2
r

ηr (X)
=E
I (sign(f (X)) = 1)
qr (X)
π
r

1 − ηr (X)
I (sign(f (X)) = −1)
+
1−π
= E[c0 (X)(η(X)I (sign(f (X)) = 1)
+ (1 − η(X))I (sign(f (X)) = −1))],
(A.1)
where c0 (x) = r qr (x)[ηr (x)/π + (1 − ηr (x))/(1 − π )], and η(x),
defined previously in Equation (3.3), is equal to r qr (x)ηr (x)/π c0 (x).
Similarly,
Rφ ( f ) = E [c0 (X)(η(X)φ(f (X)) + (1 − η(X))φ(−f (X)))] .
We define C(η, α) = ηφ(α) + (1 − η)φ(−α). Then the optimal φ-risk
satisfies


R∗φ = E c0 (X) inf C(η(X), α)
α∈R

and



Rφ − R∗φ = E c0 (X) C(η(X), f (X)) − inf C(η(X), α) .
α∈R

By a result in the article by Bartlett, Jordan, and McAuliffe (2006) for
a convexified transform of hinge loss, we have
2η − 1 =

inf

α:α(2η−1)≤0

C(η, α) − inf C(η, α).
α∈R

(A.2)

Zhao et al.: Estimating Individualized Treatment Rules Using Outcome Weighted Learning

Thus, according to Equations (A.1) and (A.2), we have
R( f ) − R∗ ≤ E(I (sign(f (X)) = sign[c0 (X)(η(X) − 1/2)])
× |c0 (X)(2η(X) − 1)|)

= E c0 (X)I (sign(f (X)) = sign[c0 (X)(η(X) − 1/2)])


C(η(X), α) − inf C(η(X), α)
×
inf
α:α(2η(X)−1)≤0
α∈R



≤ E c0 (X) C(η(X), f (X)) − inf C(η(X), α)
= Rφ ( f ) −

R∗φ .

1115

Now we proceed to obtain a bound for the first term on the righthand side of Equation (A.3). To do this, we need the useful Theorem
5.6 in the article by Steinwart and Scovel (2007) presented below:
Theorem 5.6. Steinwart and Scovel (2007). Let F be a convex set
of bounded measurable functions from Z to R and let L : F × Z →
[0, ∞) be a convex and line-continuous loss function. For a probability
measure P on Z, we define
G := {L ◦ f − L ◦ fP ,F : f ∈ F}.

α∈R

The last inequality holds because we always have C(η(x),
f (x)) ≥ inf α∈R C(η(x), α) on the set where sign(f (x)) = sign[c0 (x)
(η(x) − 1/2)] and C(η(x), f (x)) ≥ inf α:α(2η(x)−1)≤0 C(η(x), α) when
sign(f (x)) = sign[c0 (x)(η(x) − 1/2)].

Suppose that there are constants c ≥ 0, 0 < α < 1, δ ≥ 0, and B > 0
with EP g 2 ≤ c(EP g)α + δ and g∞ ≤ B for all g ∈ G. Furthermore,
assume that G is separable with respect to  · ∞ and that there are
constants a ≥ 1 and 0 < p < 2 with
sup log N (B −1 G, , L2 (T )) ≤ a

−p

T ∈Z n

Proof of Theorem 3.3
Define Lφ ( f ) = Rφ(Af )/(Aπ + (1 − A)/2). By the definition of
fˆn , we have for any f ∈ Hk ,
Pn (Lφ (fˆn )) ≤ Pn (Lφ (fˆn ) + λn fˆn 2 ) ≤ Pn (Lφ ( f ) + λn f 2 ),
where Pn denotes the empirical measure of the observed data. Thus,
lim supn Pn (Lφ (fˆn )) ≤ P (Lφ ( f )). It leads to lim supn Pn (Lφ (fˆn )) ≤
inf f ∈H̄k P (Lφ ( f )). Theorem 3.3 holds if we can show Pn (Lφ (fˆn )) −
P (Lφ (fˆn )) → 0 in probability.
To this end, we first obtain a bound for fˆn 2k . Since Pn (Lφ (fˆn )) +
λn fˆn 2 ≤ Pn (Lφ (f )) + λn f 2k for any f ∈ Hk , we can select f = 0
to obtain
E(R)
1 1  Ri
2
.
≤
φ(0) ≤
λn n
πi
λn min{π, 1 − π }
√
ˆ
Let M = 2E(R)/min{π,
1 − π } so that √
the Hk norm
√n (X) is
√
√ of λn f
{ λn f :  λ√
f

≤
M}
bounded by M. Note that the class √
n
k
√ is
contained in a Donsker class. Thus, { λn Lφ ( f ),  λn f k ≤ M}
is also P-Donsker because (1 − Af (X))+ is Lipschitz continuous with
respect to f . Therefore,

 √
√
R
n(Pn − P )Lφ (fˆn ) = λ−1
n(Pn − P )
n
Aπ + (1 − A)/2
 
+ 


= Op λ−1
.
λn − A λn fˆn (X)
×
n
fˆn 2k

Consequently, from nλn → ∞, Pn (Lφ (fˆn )) − P (Lφ (fˆn )) → 0 in probability.

for all > 0. Then there exists a constant cp > 0 depending only on p
such that for all n ≥ 1 and all τ ≥ 1 we have
Pr∗ (T ∈ Z n : RL,P (fT ,F ) > RL,P (fP ,F ) + cp (n, a, B, c, δ, τ )) ≤ e−τ,
where

 a 2/(4−2α+αp)
(n, a, B, c, δ, x) := B 2p/(4−2α+αp) c(2−p)/(4−2α+αp)
n
 1/2
 a 2/(2+p)
p/2 (2−p)/4 a
+B δ
+B
n
n

δx  cτ 1/(2−α) Bτ
+
+
+
.
n
n
n

In their article, fP ,F ∈ F is a minimizer of RL,P ( f ) = E(L(f, z)),
and fT ,F is similarly defined when T is an empirical measure. To
use this theorem, we define F, Z, T , G, fT ,F , and fP ,F according to
our setting.
It suffices to consider the subspace
of Hk , denoted by
√
√
,
as
the
ball
of
H
M/λ
of
radius
M/λ
.
Specifically, we let
BHk
n
k
n
√

M/λn and Z be X . The loss function we consider here
F be BHk
is Lφ ( f ) + λn f 2k and G is the function class
∗
)
Gφ,λn = {Lφ ( f ) + λn f 2k − Lφ (fφ,λ
 n
∗
2
− λn fφ,λn k : f ∈ BHk ( M/λn )},
∗
= argminf ∈BH (√M/λn ) (λn f 2k + Rφ ( f )). fP ,F and fT ,F
where fφ,λ
n
k
∗
correspond to fφ,λ
and fˆn , respectively. Therefore, to apply this then
orem, we will show that there are constants c ≥ 0 and B > 0, which
can possibly depend on n, such that E(g 2 ) ≤ cE(g) and g∞ ≤ B,
∀g ∈ Gφ,λ . Moreover, there are constants c̃ and 0 < ν < 2 with

sup log N (B −1 Gφ,λn , , L2 (Pn )) ≤ c̃

Proof of Theorem 3.4
for all

First, we have

Rφ (fˆn ) − R∗φ ≤ λn fˆn 2k + Rφ (fˆn ) − R∗φ ≤ λn fˆn 2k + Rφ (fˆn )
 

− inf (λn f 2k + Rφ ( f )) + inf (λn f 2k + Rφ ( f ) − R∗φ ) .
f ∈Hk

f ∈Hk

(A.3)
We will bound each term on the right-hand side separately in the
following arguments.
For the second term on the right-hand side of Equation (A.3), we use
Theorem 2.7 in the article by Steinwart and Scovel (2007) to conclude
that




,
(A.4)
inf λn f 2k + Rφ ( f ) − R∗φ = O λq/(q+1)
n
f ∈Hk

when we set σn = λn−1/(q+1)d .

−ν

,

Pn

> 0.

Let CL denote sup{R/ min(π, 1 − π )}, which is finite provided that
R is bounded. Since the weighted hinge loss is Lipschitz continuous
with respect to f , with Lipschitz constant CL , and since f ∞ ≤ f k
given that k(x, x) ≤ 1, for any g ∈ Gφ,λn , we have
∗
∗
)| + λn |f 2k − fφ,λ
2 |
|g| ≤ |Lφ ( f ) − Lφ (fφ,λ
n
n k
∗
≤ CL |f (x) − fφ,λn (x)| + M
√
≤ 2CL Mλn−1/2 + M.
√
Therefore, we can set B = 2CL Mλn−1/2 + M.
For any g ∈ Gφ,λn , we have
∗
∗
)| + λn |f 2k − fφ,λ
2 |
g( f ) ≤ |Lφ ( f ) − Lφ (fφ,λ
n
n k
∗
∗
∗
≤ CL |f − fφ,λn | + λn f − fφ,λn k f + fφ,λn k

∗
= (CL + 2 Mλn )f − fφ,λ
.
n k

(A.5)

1116

Journal of the American Statistical Association, September 2012

where

Squaring both sides and taking expectations yields

∗
2 .
E(g 2 ) ≤ (CL + 2 Mλn )2 f − fφ,λ
n k

(A.6)

On the other hand, from the convexity of Lφ , we have
1
∗
∗
(Lφ ( f ) + λn f 2k + Lφ (fφ,λ
) + λn fφ,λ
2 )
n
n k
2


∗
∗
f + fφ,λ
f 2k + fφ,λ
2
n
n k
≥ Lφ
+ λn
2
2






∗
 f + f ∗ 2
 f − f ∗ 2
f + fφ,λ


φ,λn 
φ,λn 
n
= Lφ
+ λn 
 + λn 





2
2
2
k
k
2

f − f∗ 


φ,λ
n
∗
∗
≥ Lφ (fφ,λ
) + λn fφ,λ
2 + λn 
.
n
n k


2

2

 c̃  2+ν
τ
2ν
2−ν
(n, c̃, B, c, τ ) = B + B 2+ν c 2+ν
+ (B + c) .
n
n

With B and c as defined in Equations (A.5) and (A.7), that is, c̃ =
c2 σn(1−ν/2)(1+δ)d and σn = −λn1/(q+1)d , we obtain
2

(2−ν)(1+δ)

(n, c̃, B, c, τ ) = C1 (λn )− 2+ν + (2+ν)(1+q)

1
n

2
 2+ν

+ C2

τ
, (A.9)
nλn

where C1 and C2 are constants depending on ν, δ, d, M, and π . We
complete the proof of Theorem 3.4 by plugging Equations (A.4) and
(A.9) into Equation (A.3).

k

∗
2 /2.
Taking expectations on both sides leads to E(g) ≥ λn f − fφ,λ
n k
2
Combining this with Equation (A.6), we conclude that E(g ) ≤ cE(g),
where

2
(CL + 2 Mλn )2 .
(A.7)
c=
λn

To estimate the bound for N (B −1 Gφ,λn , , L2 (Pn )), we first have
N (B −1 Gφ,λn , , L2 (Pn ))


= N (B −1 {Lφ ( f ) + λn f 2k : f ∈ BHk ( M/λn )}, , L2 (Pn )).

From the subadditivity of the entropy,
log N (B −1 Gφ,λn , 2 , L2 (Pn ))


≤ log N (B −1 {Lφ ( f ) : f ∈ BHk ( M/λn )}, , L2 (Pn ))

+ log N ({λn f 2k , f ∈ BHk ( M/λn )}, , L2 (Pn )). (A.8)

Using the Lipschitz-continuity of the weighted
hinge loss, we now have
√
∈ B −1 {Lφ ( f ) : f ∈ BHk ( M/λn )} with corresponding
that if u, u √
f, f ∈ BHk ( M/λn ), then u − u L2 (Pn ) ≤ B −1 CL f − f L2 (Pn ) ,
and therefore the first term on the right-hand side of Equation (A.8)
satisfies

log N (B −1 {Lφ ( f ) : f ∈ BHk ( M/λn )}, , L2 (Pn ))



B
≤ log N BHk ( M/λn ),
, L2 (Pn )
CL


B
≤ log N BHk ,
, L2 (Pn )
√
CL M/λn
≤ log N (BHk , 2 , L2 (Pn )).

√
The last inequality follows because B/CL M/λn ≥ 2. It is trivial to
see that for the second term on the right-hand side of Equation (A.8),

M
).
log N ({λn f 2k , f ∈ B( M/λn )}, , L2 (Pn )) ≤ log(
B
Thus,

M
log N (B −1 Gφ,λn , 2 , L2 (Pn )) ≤ log N (BHk , 2 , L2 (Pn ))+ log
.
B
Using Equation (3.5) and a given choice for B, we obtain for all σn > 0,
0 < ν < 2, δ > 0, > 0,
sup log N (B −1 Gφ,λn , , L2 (Pn )) ≤ c2 σn(1−ν/2)(1+δ)d

−ν

,

Pn

where c2 depends on ν, δ, and d.
Consequently, from Theorem 5.6 in the article by Steinwart and
Scovel (2007), there exists a constant cν > 0 depending only on ν such
that for all n ≥ 1 and all τ ≥ 1, we have the bound for the first term
P ∗ (λn fˆn 2k + Rφ (fˆn ) > inf (λn f 2k + Rφ ( f ))
f ∈Hk

+ cν (n, c̃, B, c, τ )) ≤ e−τ ,

Proof of Theorem 3.5
We apply Theorem 4.3 in the article by Blanchard, Bousquet, and
Massart (2008) on the scaled loss function L̃φ ( f ) = Lφ ( f )/CL to
obtain the rates in Theorem 3.5. Without loss of generality, we can
assume that the Bayes classifier f ∗ ∈ Hk , since we can always find
g ∈ Hk such that Rφ (g) = Rφ (f ∗ ) = R∗φ , provided that Hk is dense
in C(X ). Let S be a countable and dense subset of R+ , and let BHk (S)
denote the ball of Hk of radius S. Then BHk (S), S ∈ S is a countable
collection of classes of functions. We can then use Theorem 4.3 in the
article by Blanchard, Bousquet, and Massart (2008) after we verify the
following conditions (H1)–(H4):
(H1) ∀S ∈ S, ∀f ∈ BHk (S), L̃φ ( f )∞ ≤ bS , bS = 1 + S;
(H2) ∀f, f ∈ Hk , var(L̃φ ( f ) − L̃φ (f )) ≤ d 2 (f, f ), d(f, f ) =
f − f L2 (P ) ;
(H3) ∀S ∈ S, ∀f ∈ BHk (S), d 2 (f, f ∗ ) ≤ CS E(L̃φ ( f ) −
L̃φ (f ∗ )), CS = 2(S/η0 + 1/η1 );
(H4) Let
 x

ξ (x) =
log N (BHk , , L2 (Pn ))d .
0

We have

E

sup
f ∈ BHk (S)
d 2 (f, f ) ≤ r


(P − Pn )(L̃φ ( f ) − L̃φ (f )) ≤ inf


12
12
× 4ϑ − √ ξ (ϑ) + √ ξ
n
n

ϑ>0

√ 
r
= ψS (r).
√
2S

that is, ψS is
ψS , S ∈ S, is a sequence of subroot functions,
√
nonnegative, nondecreasing, and ψS (r)/ r is nonincreasing
for r > 0. Denote x∗ as the solution of the equation ξ (x) =
√
nx 2 . If rS∗ denotes the solution of ψS (r) = r/CS , then
√
rS∗ ≤ inf CS {4ϑ − 12ξ (ϑ)/ n} + c2 CS2 x∗2 .
ϑ>0

Under these conditions, we define for n ∈ N the following quantity:


12
γn = inf 4ϑ − √ ξ (ϑ) + x∗2 (n) .
ϑ>0
n
Given Hk is associated with the Gaussian kernel, we can show that
ξ (x)  1−ν for any 0 < ν < 2. Thus, γn  max(n−1/2ν , n−1/(ν+1) ). By
the choice of λn = O(n−1/(ν+1) ) for any ν ∈ (0, 1), this satisfies

log(τ −1 log n) ∨ 1
λn ≥ c γn + η1−1
.
n
Therefore, according to Theorem 4.3 in the article by Blanchard,
Bousquet, and Massart (2008), the following bound holds with

Zhao et al.: Estimating Individualized Treatment Rules Using Outcome Weighted Learning

probability at least 1 − τ , where τ > 0 is a fixed real number:

E(L̃φ (fˆn )) − E(L̃φ (f ∗ )) ≤ 2 inf E(L̃φ ( f )) − E(L̃φ (f ∗ ))
f ∈Hk



2
+ 2λn f k + 4λn 8 + cη1 η0−1 .
The result does not change after we scale back to the original loss
Lφ ( f ). We have shown that inf f ∈Hk [Rφ (f ) − Rφ (f ∗ ) + 2λn f 2k ] =
) in the proof of Theorem 3.4. Thus,
O(λq/(q+1)
n




q
1
= Op n− ν+1 q+1 .
R(fˆn ) − R∗ = Op λq/(q+1)
n
The remainder of the proof is to verify conditions (H1)–(H4).
For condition (H1), L̃φ ( f )∞ ≤ sup{R/(Aπ + (1 − A)/2)}(1 +
S)/CL ≤ 1 + S, f k ≤ S.
For condition (H2), let d(f, f ) = f − f L2 (P ) . Lφ ( f ) is a
Lipschitz function with respect to f with Lipschitz constant CL . Then
L̃φ ( f ) − L̃φ (f ) ≤ |f (x) − f (x)|. Hence, (H2) is easily satisfied.
For condition (H3), the proof is similar to Lemma 6.4 in the article by
Blanchard, Bousquet, and Massart (2008) with CS = 2(S/η1 + 1/η0 ),
where η0 and η1 are as defined in Assumptions (A1) and (A2) of
Section 3.5.
For condition (H4), we introduce the notation for Rademacher averages: let ε1 , . . . , εn be n iid Rademacher random variables, independent
of (Xi , Ai , Ri ), i = 1, . . . , n. For any measurable real-valued function
f , the Rademacher average is defined as Ln f = n−1 ni=1 εi f (Xi ).
Also let Ln (F) be the empirical Rademacher complexity of function
class F, Ln F = supf ∈F Ln f .
First, we have from Lemma 6.7 in the article by Blanchard,
Bousquet, and Massart (2008) that for f ∈ Hk ,


E sup (P − Pn )(L̃φ ( f ) − L̃φ (f )) ≤ 4E[Ln {f − f , f ∈ Hk }].
f ∈Hk

Thus, for the set {f ∈ Hk : f k ≤ S, d 2 (f, f ) ≤ r} and f ∈ BHk (S),


E sup (P − Pn )(L̃φ ( f ) − L̃φ (f ))
f ∈Hk

≤ 4E[Ln {f − f , f ∈ Hk : f k ≤ S, d 2 (f, f ) ≤ r}],
the right-hand side of which is equivalent to 4E[Ln {f, f ∈ Hk :
f k ≤ 2S, f 2L2 (Pn ) ≤ 2r}]. Now we proceed to show that


ELn f ∈ Hk : f k ≤ 2S, f 2L2 (Pn ) ≤ 2r ≤ inf
12
4ϑ + √
n

×



√
√r
2S

ϑ>0



log N (BH , , L2 (Pn ))d

by slightly modifying the procedure in obtaining Dudley’s Entropy Integral for Rademacher complexity of sets of functions. For j ≥ 0, let rj =
√
2r2−j and Tj be a rj -cover of BHk (2S) with respect to the L2 (Pn )norm. For each f ∈ BHk (2S), we can find an f˜j ∈ Tj , such that f −
˜
f˜j L2 (Pn ) ≤ rj . For any N, we express f as f = f − f˜N + N
j =1 (fj −
f˜j −1 ), where f˜0 = 0. Note f˜0 = 0 is an r0 -approximation of f . Hence,


n
N

1
εi f (Xi )− f˜N (Xi )+ (f˜j (Xi )− f˜j −1 (Xi ))
=E
sup
f ∈BHk (2S) n
i=1
j =1


≤E
sup εL2 (Pn ) f − f˜N L2 (Pn )
f ∈BHk (2S)


n
1 ˜
(fj (Xi ) − f˜j −1 (Xi ))
f ∈BHk (2S) n i=1
j =1


N
n

1 ˜
E
sup
(fj (Xi ) − f˜j −1 (Xi )) .
≤ rN +
f ∈BHk (2S) n
j =1
i=1
+

N




E

sup

Note that

2

f˜j − f˜j −1 2L2 (Pn ) ≤ f˜j − f L2 (Pn ) + f − f˜j −1 L2 (Pn )
≤ (rj + rj −1 )2 = 9rj2 .

We therefore have
N




2 log(|Tj ||Tj −1 |)
n
j =1

N

log N (BHk (2S), rj , L2 (Pn ))
≤ rN + 12
(rj −rj +1 )
n
j =1
√
√

 r/ 2S
log N (BHk , , L2 (Pn ))
d .
≤ rN + 12
n
rN+1

Ln (BHk (2S)) ≤ rN +

3rj

For any ϑ > 0, we can choose N = sup{j : rj > 2ϑ}. Therefore,
ϑ < rN+1 < 2ϑ, and rN < 4ϑ. We therefore conclude that
 √r/√2S 
log N (BHk , , L2 (Pn ))
d
Ln (BHk (2S)) ≤ inf 4ϑ + 12
ϑ>0
n
ϑ
√ 

12
r
12
= inf 4ϑ − √ ξ (ϑ) + √ ξ √
= ψS (r).
ϑ>0
n
n
2S
The function ψS is subroot because log N (BHk , , L2 (Pn )) is a
decreasing function of .
 √
To show the upper bound of r ∗ , let tS∗ = c2 CS2 x∗2 . Then tS∗ / 2S =
√
 √
cCS x∗ / 2S, CS /S ≥ 1. Assuming that c ≥ 2, we have tS∗ / 2S ≥
x∗ . Since x −1 ξ (x) is a decreasing function, it follows that
 
√
t∗
n ∗
CS
ξ √ S ≤ c √ ξ (x∗ ) =
t .
cSCS S
2S
2S
Therefore, by selecting an appropriate constant c,


12
12 ∗
ψS (tS∗ ) ≤ inf 4ϑ − √ ξ (ϑ) +
t
ϑ>0
cSCS S
n
√
CS inf ϑ>0 {4ϑ − 12/ nξ (ϑ)} + tS∗
≤
.
CS
The desired result follows from the property of subroot functions,
which states that if ψ : [0, ∞) → [0, ∞) is a sub-root function, then
the unique positive solution of ψ(r) = r, denoted by r ∗ , exists, and
for all r > 0, r ≥ ψ(r) if and only if r ∗ ≤ r (Bartlett, Bousquet, and
Mendelson 2005).

= ψS (r),

ϑ

Ln (BHk (2S))


1117

[Received October 2011. Revised April 2012.]

REFERENCES
Bartlett, P. L., Bousquet, O., and Mendelson, S. (2005), “Local Rademacher
Complexities,” The Annals of Statistics, 33 (4), 1497–1537. [1117]
Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. (2006), “Convexity, Classification, and Risk Bounds,” Journal of American Statistical Association,
101 (473), 138–156. [1109,1114]
Blanchard, G., Bousquet, O., and Massart, P. (2008), “Statistical Performance
of Support Vector Machines,” The Annals of Statistics, 36, 489–531.
[1110,1116,1117]
Bradley, P. S., and Mangasarian, O. L. (1998), “Feature Selection via Concave
Minimization and Support Vector Machines,” in Proceedings of the 15th
International Conference on Machine Learning, San Francisco, CA: Morgan
Kaufmann, pp. 82–90. [1114]
Buzdar, A. U. (2009), “Role of Biologic Therapy and Chemotherapy in Hormone
Receptor and HER2-Positive Breast Cancer,” The Annals of Oncology, 20,
993–999. [1106]
Cai, T., Tian, L., Uno, H., and Solomon, S. D. (2010), “Calibrating Parametric
Subject-Specific Risk Estimation,” Biometrika, 97 (2), 389–404. [1106]
Cortes, C., and Vapnik, V. (1995), “Support-Vector Networks,” Machine Learning, 20, 273–297. [1106,1107]

1118
Crits-Christoph, P., Siqueland, L., Blaine, J., Frank, A., Luborsky, L. S., Onken,
L. S., Muenz, L. R., Thase, M. E., Weiss, R. D., Gastfriend, D. R., Woody,
G. E., Barber, J. P., Butler, S. F., Daley, D., Salloum, I., Bishop, S., Najavits,
L. M., Lis, J., Mercer, D., Griffin, M. L., Moras, K., and Beck, A. T. (1999),
“Psychosocial Treatments for Cocaine Dependence,” Archives of General
Psychiatry, 56, 493–502. [1106]
Eagle, K. A., Lim, M. J., Dabbous, O. H., Pieper, K. S., Goldberg, R. J., de
Werf, F. V., Goodman, S. G., Granger, C. B., Steg, P. G., Gore, J. M., Budaj,
A., Avezum, A., Flather, M. D., Fox, K. A. A., and GRACE Investigators,
(2004), “A Validated Prediction Model for All Forms of Acute Coronary
Syndrome: Estimating the Risk of 6-Month Postdischarge Death in an International Registry,” Journal of the American Medical Association, 291,
2727–2733. [1106]
Flume, P. A., O Sullivan, B. P., Goss, C. H., Mogayzel, P. J., Willey-Courand,
D. B., Bujan, J., Finder, J., Lester, M., Quittell, L., Rosenblatt, R., Vender,
R. L., Hazle, L., Sabadosa, K., and Marshall, B. (2007), “Cystic Fibrosis Pulmonary Guidelines: Chronic Medications for Maintenance of Lung
Health,” American Journal of Respiratory and Critical Care Medicine,
176 (1), 957–969. [1106]
Grünwald, V., and Hidalgo, M. (2003), “Developing Inhibitors of the Epidermal
Growth Factor Receptor for Cancer Treatment,” Journal of the National
Cancer Institute, 95 (12), 851–867. [1106]
Hastie, T., Tibshirani, R., and Friedman, J. H. (2009), The Elements of Statistical
Learning (2nd ed.), New York: Springer-Verlag. [1108]
Insel, T. R. (2009), “Translating Scientific Opportunity Into Public Health Impact: A Strategic Plan for Research on Mental Illness,” Archives of General
Psychiatry, 66 (2), 128–133. [1106]
Keller, M. B., Mccullough, J. P., Klein, D. N., Arnow, B., Dunner, D. L.,
Gelenberg, A. J., Markowitz, J. C., Nemeroff, C. B., Russell, J. M., Thase,
M. E., Trivedi, M. H., and Zajecka, J. (2000), “A Comparison of Nefazodone,
The Cognitive Behavioral-Analysis System of Psychotherapy, and Their
Combination for the Treatment of Chronic Depression,” The New England
Journal of Medicine, 342 (20), 1462–1470. [1107,1113]
Laber, E. B., and Murphy, S. A. (2011), “Adaptive Confidence Intervals for the
Test Error in Classification,” Journal of the American Statistical Association,
106, 904–913. [1114]
Lee, Y., Lin, Y., and Wahba, G. (2004), “Multicategory Support Vector Machines: Theory and Application to the Classification of Microarray Data and
Satellite Radiance Data,” Journal of the American Statistical Association,
99, 67–81. [1114]
Lin, Y. (2002), “Support Vector Machines and the Bayes Rule in Classification,”
Data Mining and Knowledge Discovery, 6, 259–275. [1109]
Liu, Y., Zhang, H., Park, C., and Ahn, J. (2007), “Support Vector Machines
With Adaptive Lq Penalty,” Computational Statistics & Data Analysis, 51
(12), 6380–6394. [1114]
Lugosi, G., and Vayatis, N. (2004), “On the Bayes-Risk Consistency
of Regularized Boosting Methods,” The Annals of Statistics, 32, 30–
55. [1107]
Marlowe, D. B., Festinger, D. S., Dugosh, K. L., Lee, P. A., and Benasutti,
K. M. (2007), “Adapting Judicial Supervision to the Risk Level of Drug
Offenders: Discharge and 6-Month Outcomes From a Prospective Matching
Study,” Drug and Alcohol Dependence, 88 (2), S4–S13. [1106]
Moodie, E. E. M., Platt, R. W., and Kramer, M. S. (2009), “Estimating ResponseMaximized Decision Rules With Applications to Breastfeeding,” Journal of
the American Statistical Association, 104 (485), 155–165. [1107]
Moodie, E. E. M., Richardson, T. S., and Stephens, D. A. (2007), “Demystifying
Optimal Dynamic Treatment Regimes,” Biometrics, 63 (2), 447–455. [1114]
Murphy, S. A. (2003), “Optimal Dynamic Treatment Regimes,” Journal of the
Royal Statistical Society, Series B, 65, 331–366. [1114]

Journal of the American Statistical Association, September 2012
Murphy, S. A., van der Laan, M. J., Robins, J. M., and CPPRG, (2001),
“Marginal Mean Models for Dynamic Regimes,” Journal of the American
Statistical Association, 96, 1410–1423. [1111,1114]
Piper, W. E., Boroto, D. R., Joyce, A. S., McCallum, M., and Azim, H. F. A.
(1995), “Pattern of Alliance and Outcome in Short-Term Individual Psychotherapy,” Psychotherapy, 32, 639–647. [1106]
Qian, M., and Murphy, S. A. (2011), “Performance Guarantees for Individualized Treatment Rules,” The Annals of Statistics, 39, 1180–1210.
[1106,1107,1111,1114]
Robins, J. M. (2004), “Optimal Structural Nested Models for Optimal Sequential
Decisions,” in Proceedings of the Second Seattle Symposium on Biostatistics,
pp. 189–236. [1107]
Rosenwald, A., Wright, G., Chan, W. C., Connors, J. M., Campo, E., Fisher,
R. I., Gascoyne, R. D., Muller-Hermelink, H. K., Smeland, E. B., Giltnane,
J. M., Hurt, E. M., Zhao, H., Averett, L., Yang, L., Wilson, W. H., Jaffe, E. S.,
Simon, R., Klausner, R. D., Powell, J., Duffey, P. L., Longo, D. L., Greiner,
T. C., Weisenburger, D. D., Sanger, W. G., Dave, B. J., Lynch, J. C., Vose,
J., Armitage, J. O., Montserrat, E., López-Guillermo, A., Grogan, T. M.,
Miller, T. P., LeBlanc, M., Ott, G., Kvaloy, S., Delabie, J., Holte, H., Krajci,
P., Stokke, T., Staudt, L. M., and Lymphoma/Leukemia Molecular Profiling
Project, (2002), “The Use of Molecular Profiling to Predict Survival After
Chemotherapy for Diffuse Large B-Cell Lymphoma,” The New England
Journal of Medicine, 346, 1937–1947. [1106]
Sargent, D. J., Conley, B. A., Allegra, C., and Collette, L. (2005), “Clinical
Trial Designs for Predictive Marker Validation in Cancer Treatment Trials,”
Journal of Clinical Oncology, 32, 2020–2027. [1106]
Steinwart, I. (2005), “Consistency of Support Vector Machines and Other Regularized Kernel Classifiers,” IEEE Transactions on Information Theory, 51,
128–142. [1107]
Steinwart, I., and Scovel, C. (2007), “Fast Rates for Support Vector Machines Using Gaussian Kernels,” The Annals of Statistics, 35, 575–607.
[1109,1110,1115,1116]
Thall, P. F., Sung, H.-G., and Estey, E. H. (2002), “Selecting Therapeutic Strategies Based on Efficacy and Death in Multicourse Clinical Trials,” Journal
of the American Statistical Association, 97, 29–39. [1114]
Tsybakov, A. B. (2004), “Optimal Aggregation of Classifiers in Statistical
Learning,” The Annals of Statistics, 32, 135–166. [1110]
van’t Veer, L. J., and Bernards, R. (2008), “Enabling Personalized Cancer
Medicine Through Analysis of Gene-Expression Patterns,” Nature, 452,
564–570. [1106]
Vapnik, V. N. (1995), The Nature of Statistical Learning Theory, New York:
Springer-Verlag. [1114]
Vert, R., and Vert, J.-P. (2006), “Consistency and Convergence Rates of OneClass SVMs and Related Algorithms,” Journal of Machine Learning Research, 7, 817–854. [1110]
Wang, L., and Shen, X. (2006), “Multi-Category Support Vector Machines, Feature Selection, and Solution Path,” Statistica Sinica, 16,
617–633. [1114]
Zhang, T. (2004), “Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization,” The Annals of Statistics, 32 (1),
56–85. [1107,1109]
Zhao, Y., Zeng, D., Socinski, M. A., and Kosorok, M. R. (2011), “Reinforcement
Learning Strategies for Clinical Trials in Nonsmall Cell Lung Cancer,”
Biometrics, 67, 1422–1433. [1114]
Zhu, J., Rosset, S., Hastie, T., and Tibshirani, R. (2003), “1-Norm Support Vector Machines,” in Neural Information Processing Systems,
p. 16. [1114]
Zou, H., and Yuan, M. (2008), “The F∞ -Norm Support Vector Machine,” Statistica Sinica, 18, 379–398. [1114]

