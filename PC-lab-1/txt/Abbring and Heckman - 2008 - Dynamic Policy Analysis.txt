Advanced Studies in Theoretical and Applied Econometrics Volume 46
Managing Editors: J. Marquez, The Federal Reserve Board, Washington, D.C., U.S.A A. Spanos, Virginia Polytechnic Institute and State University, Blacksburg, VA, U.S.A Editorial Board: F.G. Adams, University of Pennsylvania, Philadelphia, U.S.A P. Balestra, University of Geneva, Switzerland M.G. Dagenais, University of Montreal, Canada D. Kendrick, University of Texas, Austin, U.S.A J.H.P. Paelinck, Netherlands Economic Institute, Rotterdam, The Netherlands R.S. Pindyck, Sloane School of Management, M.I.T., U.S.A W. Welfe, University of Lodz, Poland
The titles published in this series are listed at the end of this volume.

La´szlo´ Ma´tya´s · Patrick Sevestre (Eds.)
The Econometrics of Panel Data
Fundamentals and Recent Developments in Theory and Practice
Third Edition
With 13 Figures and 43 Tables

Editors
Prof. La´szlo´ Ma´tya´s Central European University Department of Economics Na´dor u. 9 1051 Budapest Hungary matyas@ceu.hu

Prof. Patrick Sevestre Universite´ Paris 1-Panthe´on Sorbonne Ecole Economique de Paris (Paris School of Economics) 106-112 Boulevard de l'Ho^pital 75013 Paris France sevestre@univ-paris1.fr

ISBN: 978-3-540-75889-1

e-ISBN: 978-3-540-75892-1

Advanced Studies in Theoretical and Applied Econometrics ISSN: 1570-5811

Library of Congress Control Number: 2007940369

2nd edition was published by Springer Netherlands in 1995 c 2008 Springer-Verlag Berlin Heidelberg
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilm or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer. Violations are liable to prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.
Cover design: eStudio Calamar S.L., F. Steinen-Broo, Pau/Girona, Spain
Printed on acid-free paper
987654321
springer.com

Preface
The aim of this third, completely re-written, re-edited and considerably expanded, edition of this book is to provide a general overview of both the basics and recent, more sophisticated, theoretical developments in panel data econometrics. It also aims at covering a number of fields of applications where these methods are used for improving our knowledge and understanding of economic agents' behaviors. Since the pioneering works of Edwin Kuh (1959), Yair Mundlak (1961), Irving Hoch (1962), and Pietro Balestra and Marc Nerlove (1966), the pooling of cross sections and time series data has become an increasingly popular way of quantifying economic relationships. Each series provides information lacking in the other, so a combination of both leads to more accurate, reliable and informative results than would be achievable by one type of series alone. Over the last three decades of the last century, much fundamental work has been done: investigation of the properties of different estimators and test statistics, analysis of dynamic models and the effects of eventual measurement errors, etc.
The more recent years and in particular the ten years elapsed since the second edition of this book have witnessed even more considerable changes. Indeed, our ability to estimate and test nonlinear models have dramatically improved and issues such as the unobserved heterogeneity in nonlinear models, attrition and selectivity bias have received considerable attention. This explains why the number of chapters dealing with such issues has increased in this third edition. Other recent and important developments relate to the issue of unit roots and cointegration in long times series panels as well as that of cross-sectional dependence that occur in particular in spatial models, and else.
The first objective of this book, which takes up Parts I and II, is to give a complete and state of the art presentation of these theoretical developments. Part I is concerned with the basic fixed effects, random effects and random coefficients models, both linear and nonlinear; Part II deals with various extensions: dynamic models with small T panels, dynamic models with large T panels, models with other sources of endogeneity (measurement errors, simultaneity) and also provides an overview of recent developments in several other directions: attrition and selection bias, pseudopanels, semi- and non-parametric methods, the Bayesian approach to panel data, the
v

vi

Preface

poolability of individuals, duration models and point processes, and count data models. The second objective of this volume is to provide insights into the use of panel data in empirical studies. Since the beginnings, interest in panel data has mostly been empirically motivated. Panel data methods have gained an increased importance over time and are now applied in a very large spectrum of economic studies. Part III thus deals with studies in several major fields of applied economics, such as foreign direct investments, production frontiers, linked employer-employee data, labor supply, policy analysis and transitions on the labor market. Some of the chapters in this third edition are revised versions of those already published in the previous ones, while several others are completely new contributions. In this respect, we are particularly happy to welcome aboard our new authors. Their input definitely helped to substantially improve the contents of this volume.
The double emphasis of this book (theoretical and applied), together with the fact that all the chapters have been written by well-known specialists in the field, encourage us to hope that it has now become a standard reference textbook for all those who are concerned with the use of panel data in econometrics, whether they are advanced students, professional economists or researchers. The editors have tried to standardize the notation, language, depth, etc. in order to present a coherent book. However, each chapter is capable of standing on its own as a reference in its own topic.
We must address our thanks to all those who have facilitated the creation of this book: the contributors who produced quality work, then took part in an internal refereeing process to ensure a high overall standard; Kluwer Academic Publishers, who had the foresight to publish in a subject which, at the time of the first edition, had a limited, but expanding, audience; and of course Springer which has become our publisher by now. In particular, Cathelijne van Herwaarden, Herma Drees, Marie Sheldon, Martina Bihn, Ruth Milewski and Isabelle George must be thanked for their help in the realization of this volume. Also, the University of Paris­Val de Marne in France; the Monash Research Fund and the Australian Research Council in Australia, and the Budapest University of Economics and the Hungarian Research Fund (OTKA) in Hungary must be thanked for having provided financial support to the editors for the earlier editions. This third edition has benefited from generous financial support provided by the Central European University and the University of Paris-Val de Marne.

Budapest and Paris January 2008

La´szlo´ Ma´tya´s Patrick Sevestre

Contents
Part I Fundamentals
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Marc Nerlove, Patrick Sevestre and Pietro Balestra 1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Data, Data-Generating Processes (DGP), and Inference . . . . . . . . . 4 1.3 History and Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4 A Brief Review of Other Methodological Developments . . . . . . . . . 13 1.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2 Fixed Effects Models and Fixed Coefficients Models . . . . . . . . . . . . . . 23 Pietro Balestra and Jayalakshmi Krishnakumar 2.1 The Covariance Model: Individual Effects Only . . . . . . . . . . . . . . . . 24 2.1.1 Specification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.1.2 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.1.3 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.2 The Covariance Model: Individual and Time Effects . . . . . . . . . . . . 29 2.2.1 Time Effects Only . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.2.2 Time and Individual Effects . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.2.3 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 2.3 Non-spherical Disturbances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.3.1 What Variance­Covariance Stucture? . . . . . . . . . . . . . . . . . 33 2.3.2 Two General Propositions for Fixed Effects Models . . . . . 34 2.3.3 Individual Fixed Effects and Serial Correlation . . . . . . . . . 36 2.3.4 Heteroscedasticity in Fixed Effects Models . . . . . . . . . . . . 38 2.4 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 2.4.1 Constant Variables in One Dimension . . . . . . . . . . . . . . . . . 40 2.4.2 Variable Slope Coefficients . . . . . . . . . . . . . . . . . . . . . . . . . 41 2.4.3 Unbalanced Panels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
vii

viii

Contents

3 Error Components Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 Badi H. Baltagi, La´szlo´ Ma´tya´s and Patrick Sevestre 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.2 The One-Way Error Components Model . . . . . . . . . . . . . . . . . . . . . . 50 3.2.1 Definition/Assumptions of the Model . . . . . . . . . . . . . . . . . 50 3.2.2 The GLS Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3.2.3 The Feasible GLS Estimator . . . . . . . . . . . . . . . . . . . . . . . . 55 3.2.4 Some Other Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 3.2.5 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 3.3 More General Structures of the Disturbances . . . . . . . . . . . . . . . . . . 64 3.3.1 The Two-Way Error Components Model . . . . . . . . . . . . . . 64 3.3.2 Serial Correlation in the Disturbances . . . . . . . . . . . . . . . . . 70 3.3.3 Two-Way Error Components vs Kmenta's Approach . . . . 73 3.3.4 Heteroskedasticity in the Disturbances . . . . . . . . . . . . . . . . 74 3.4 Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 3.4.1 Testing for the Absence of Individual Effects . . . . . . . . . . 79 3.4.2 Testing for Uncorrelated Effects: Hausman's Test . . . . . . . 80 3.4.3 Testing for Serial Correlation . . . . . . . . . . . . . . . . . . . . . . . . 81 3.4.4 Testing for Heteroskedasticity . . . . . . . . . . . . . . . . . . . . . . . 82 3.5 Estimation Using Unbalanced Panels . . . . . . . . . . . . . . . . . . . . . . . . . 84 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
4 Endogenous Regressors and Correlated Effects . . . . . . . . . . . . . . . . . . 89 Rachid Boumahdi and Alban Thomas 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 4.2 Estimation of Transformed Linear Panel Data Models . . . . . . . . . . . 90 4.2.1 Error Structures and Filtering Procedures . . . . . . . . . . . . . . 91 4.2.2 An IV Representation of the Transformed Linear Model . 93 4.3 Estimation with Time-Invariant Regressors . . . . . . . . . . . . . . . . . . . . 95 4.3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 4.3.2 Instrumental Variable Estimation . . . . . . . . . . . . . . . . . . . . . 96 4.3.3 More Efficient IV Procedures . . . . . . . . . . . . . . . . . . . . . . . 98 4.4 A Measure of Instrument Relevance . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.5 Incorporating Time-Varying Regressors . . . . . . . . . . . . . . . . . . . . . . . 101 4.5.1 Instrumental Variables Estimation . . . . . . . . . . . . . . . . . . . . 102 4.6 GMM Estimation of Static Panel Data Models . . . . . . . . . . . . . . . . . 104 4.6.1 Static Model Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 4.6.2 GMM Estimation with HT, AM and BMS Instruments . . 107 4.7 Unbalanced Panels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5 The Chamberlain Approach to Panel Data: An Overview and Some Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 Bruno Cre´pon and Jacques Mairesse 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 5.2 The Chamberlain  Matrix Framework . . . . . . . . . . . . . . . . . . . . . . . 115

Contents

ix

5.2.1 The  Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 5.2.2 Relations Between  and the Parameters of Interest . . . . . 118 5.2.3 Four Important Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 5.2.4 Restrictions on the Covariance Matrix of the Disturbances124 5.2.5 A Generalization of the Chamberlain Method . . . . . . . . . . 125 5.2.6 The Vector Representation of the Chamberlain
Estimating Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 5.2.7 The Estimation of Matrix  . . . . . . . . . . . . . . . . . . . . . . . . 127 5.3 Asymptotic Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 5.3.1 ALS Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 5.3.2 The Optimal ALS Estimator . . . . . . . . . . . . . . . . . . . . . . . . 132 5.3.3 Specification Testing in the ALS Framework . . . . . . . . . . . 135 5.4 The Equivalence of the GMM and the Chamberlain Methods . . . . . 137 5.4.1 A Reminder on the GMM . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 5.4.2 Equivalence of the GMM and the Chamberlain Methods . 139 5.4.3 Equivalence in Specific Cases . . . . . . . . . . . . . . . . . . . . . . . 140 5.5 Monte Carlo Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 5.5.1 Design of the Simulation Experiments . . . . . . . . . . . . . . . . 144 5.5.2 Consistency and Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 5.5.3 Efficiency and Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . 152 5.5.4 Standard Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 5.5.5 Specification Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 5.6 Appendix A: An Extended View of the Chamberlain Method . . . . . 160 5.6.1 Simultaneous Equations Models . . . . . . . . . . . . . . . . . . . . . 160 5.6.2 VAR Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 5.6.3 Endogenous Attrition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 5.7 Appendix B: Vector Representation of the Chamberlain Estimating Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 5.7.1 The Vec Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 5.7.2 Correlated Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 5.7.3 Errors in Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 5.7.4 Weak Simultaneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 5.7.5 Combination of the Different Cases . . . . . . . . . . . . . . . . . . 166 5.7.6 Lagged Dependent Variable . . . . . . . . . . . . . . . . . . . . . . . . . 167 5.7.7 Restrictions on the Covariance Matrix of the Disturbances167 5.8 Appendix C: Manipulation of Equations and Parameters in the ALS Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 5.8.1 Transformation of the Estimating Equations . . . . . . . . . . . 168 5.8.2 Eliminating Parameters of Secondary Interest . . . . . . . . . . 169 5.8.3 Recovering Parameters of Secondary Interest
Once Eliminated . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 5.8.4 Elimination of Auxiliary Parameters . . . . . . . . . . . . . . . . . . 173 5.9 Appendix D: Equivalence Between Chamberlain's, GMM and Usual Panel Data Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 5.10 Appendix E: Design of Simulation Experiments . . . . . . . . . . . . . . . . 177

x

Contents

5.10.1 Generating Process of the Variable x . . . . . . . . . . . . . . . . . . 177 5.10.2 Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178 5.10.3 Calibration of Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . 179 5.10.4 Three Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180 5.10.5 The Chamberlain and GMM Estimators . . . . . . . . . . . . . . . 180 5.10.6 Standard Errors and Specification Tests . . . . . . . . . . . . . . . 181 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
6 Random Coefficient Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 Cheng Hsiao and M. Hashem Pesaran 6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 6.2 The Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 6.3 Sampling Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 6.4 Mean Group Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 6.5 Bayesian Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 6.6 Dynamic Random Coefficients Models . . . . . . . . . . . . . . . . . . . . . . . 197 6.7 Testing for Heterogeneity Under Weak Exogeneity . . . . . . . . . . . . . 199 6.8 A Random Coefficient Simultaneous Equation System . . . . . . . . . . 203 6.9 Random Coefficient Models with Cross-Section Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206 6.10 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

7 Parametric Binary Choice Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 Michael Lechner, Ste´fan Lollivier and Thierry Magnac 7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 7.2 Random Effects Models Under Strict Exogeneity . . . . . . . . . . . . . . . 217 7.2.1 Errors are Independent Over Time . . . . . . . . . . . . . . . . . . . 218 7.2.2 One Factor Error Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 7.2.3 General Error Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 7.2.4 Simulation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 7.2.5 How to Choose a Random Effects Estimator for an Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 7.2.6 Correlated Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 7.3 Fixed Effects Models Under Strict Exogeneity . . . . . . . . . . . . . . . . . 230 7.3.1 The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231 7.3.2 The Method of Conditional Likelihood . . . . . . . . . . . . . . . 232 7.3.3 Fixed Effects Maximum Score . . . . . . . . . . . . . . . . . . . . . . . 235 7.3.4 GMM Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 7.3.5 Large-T Approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 7.4 Dynamic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 7.4.1 Dynamic Random Effects Models . . . . . . . . . . . . . . . . . . . . 238 7.4.2 Dynamic Fixed Effects Models . . . . . . . . . . . . . . . . . . . . . . 241 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242

Contents

xi

Part II Advanced Topics

8 Dynamic Models for Short Panels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 Mark N. Harris, La´szlo´ Ma´tya´s and Patrick Sevestre 8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 8.2 The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 8.3 The Inconsistency of Traditional Estimators . . . . . . . . . . . . . . . . . . . 252 8.4 IV and GMM Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 8.4.1 Uncorrelated Individual Effects: The Original Balestra­Nerlove Estimator and its Extensions . . . . . . . . . 256 8.4.2 Correlated Individual Effects . . . . . . . . . . . . . . . . . . . . . . . . 257 8.4.3 Some Monte Carlo Evidence . . . . . . . . . . . . . . . . . . . . . . . . 269 8.5 The Maximum Likelihood Estimator . . . . . . . . . . . . . . . . . . . . . . . . . 270 8.6 Testing in Dynamic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 8.6.1 Testing the Validity of Instruments . . . . . . . . . . . . . . . . . . . 272 8.6.2 Testing for Unobserved Effects . . . . . . . . . . . . . . . . . . . . . . 273 8.6.3 Testing for the Absence of Serial Correlation in  . . . . . . . 274 8.6.4 Significance Testing in Two-Step Variants . . . . . . . . . . . . . 275 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
9 Unit Roots and Cointegration in Panels . . . . . . . . . . . . . . . . . . . . . . . . 279 Jo¨rg Breitung and M. Hashem Pesaran 9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 9.2 First Generation Panel Unit Root Tests . . . . . . . . . . . . . . . . . . . . . . . 281 9.2.1 The Basic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 9.2.2 Derivation of the Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 9.2.3 Null Distribution of the Tests . . . . . . . . . . . . . . . . . . . . . . . . 284 9.2.4 Asymptotic Power of the Tests . . . . . . . . . . . . . . . . . . . . . . 287 9.2.5 Heterogeneous Trends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288 9.2.6 Short-Run Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 9.2.7 Other Approaches to Panel Unit Root Testing . . . . . . . . . . 293 9.3 Second Generation Panel Unit Root Tests . . . . . . . . . . . . . . . . . . . . . 295 9.3.1 Cross-Section Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . 295 9.3.2 Tests Based on GLS Regressions . . . . . . . . . . . . . . . . . . . . . 296 9.3.3 Test Statistics Based on OLS Regressions . . . . . . . . . . . . . 297 9.3.4 Other Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 9.4 Cross-Unit Cointegration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 9.5 Finite Sample Properties of Panel Unit Root Tests . . . . . . . . . . . . . . 301 9.6 Panel Cointegration: General Considerations . . . . . . . . . . . . . . . . . . 302 9.7 Residual-Based Approaches to Panel Cointegration . . . . . . . . . . . . . 306 9.7.1 Spurious Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306 9.7.2 Tests of Panel Cointegration . . . . . . . . . . . . . . . . . . . . . . . . . 307 9.8 Tests for Multiple Cointegration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 9.9 Estimation of Cointegrating Relations in Panels . . . . . . . . . . . . . . . . 309 9.9.1 Single Equation Estimators . . . . . . . . . . . . . . . . . . . . . . . . . 309 9.9.2 System Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312

xii

Contents

9.10 Cross-Section Dependence and the Global VAR . . . . . . . . . . . . . . . . 313 9.11 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
10 Measurement Errors and Simultaneity . . . . . . . . . . . . . . . . . . . . . . . . . 323 Erik Biørn and Jayalakshmi Krishnakumar 10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323 10.2 Measurement Errors and Panel Data . . . . . . . . . . . . . . . . . . . . . . . . . . 323 10.2.1 Model and Orthogonality Conditions . . . . . . . . . . . . . . . . . 325 10.2.2 Identification and the Structure of the Second Order Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 10.2.3 Moment Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328 10.2.4 Estimators Constructed from Period Means . . . . . . . . . . . . 331 10.2.5 GMM Estimation and Testing in the General Case . . . . . . 332 10.2.6 Estimation by GMM, Combining Differences and Levels 335 10.2.7 Extensions: Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . 343 10.2.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 10.3 Simultaneity and Panel Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344 10.3.1 SEM with EC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345 10.3.2 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361 10.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365

11 Pseudo-Panels and Repeated Cross-Sections . . . . . . . . . . . . . . . . . . . . 369 Marno Verbeek 11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369 11.2 Estimation of a Linear Fixed Effects Model . . . . . . . . . . . . . . . . . . . 370 11.3 Estimation of a Linear Dynamic Model . . . . . . . . . . . . . . . . . . . . . . . 376 11.4 Estimation of a Binary Choice Model . . . . . . . . . . . . . . . . . . . . . . . . 380 11.5 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
12 Attrition, Selection Bias and Censored Regressions . . . . . . . . . . . . . . . 385 Bo Honore´, Francis Vella and Marno Verbeek 12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385 12.2 Censoring, Sample Selection and Attrition . . . . . . . . . . . . . . . . . . . . 386 12.3 Sample Selection and Attrition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389 12.4 Sample Selection Bias and Robustness of Standard Estimators . . . . 391 12.5 Tobit and Censored Regression Models . . . . . . . . . . . . . . . . . . . . . . . 393 12.5.1 Random Effects Tobit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394 12.5.2 Random Effects Tobit with Endogenous Explanatory Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396 12.5.3 Dynamic Random Effects Tobit . . . . . . . . . . . . . . . . . . . . . . 398 12.5.4 Fixed Effects Tobit Estimation . . . . . . . . . . . . . . . . . . . . . . . 399 12.5.5 Semi-parametric Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 401

Contents

xiii

12.5.6 Semi-parametric Estimation in the Presence of Lagged Dependent Variables . . . . . . . . . . . . . . . . . . . . . . 402
12.6 Models of Sample Selection and Attrition . . . . . . . . . . . . . . . . . . . . . 402 12.6.1 Maximum Likelihood Estimators . . . . . . . . . . . . . . . . . . . . 403 12.6.2 Two-Step Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404 12.6.3 Alternative Selection Rules . . . . . . . . . . . . . . . . . . . . . . . . . 407 12.6.4 Two-Step Estimators with Fixed Effects . . . . . . . . . . . . . . . 408 12.6.5 Semi-parametric Sample Selection Models . . . . . . . . . . . . 409 12.6.6 Semi-parametric Estimation of a Type-3 Tobit Model . . . 410
12.7 Some Empirical Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412 12.7.1 Attrition in Experimental Data . . . . . . . . . . . . . . . . . . . . . . . 412 12.7.2 Real Wages Over the Business Cycle . . . . . . . . . . . . . . . . . 413 12.7.3 Unions and Wages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
13 Simulation Techniques for Panels: Efficient Importance Sampling . . 419 Roman Liesenfeld and Jean-Franc¸ois Richard 13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419 13.2 Pseudorandom Number Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 420 13.2.1 Univariate Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421 13.2.2 Multivariate Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 424 13.3 Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426 13.3.1 General Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426 13.3.2 Efficient Importance Sampling . . . . . . . . . . . . . . . . . . . . . . 428 13.3.3 MC Sampling Variance of (E)IS Estimates . . . . . . . . . . . . 431 13.3.4 GHK Simulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432 13.3.5 Common Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . 432 13.4 Simulation-Based Inference Procedures . . . . . . . . . . . . . . . . . . . . . . . 434 13.4.1 Integration in Panel Data Models . . . . . . . . . . . . . . . . . . . . 434 13.4.2 Simulated Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435 13.4.3 Simulated Method of Moments . . . . . . . . . . . . . . . . . . . . . . 435 13.4.4 Bayesian Posterior Moments . . . . . . . . . . . . . . . . . . . . . . . . 437 13.5 Numerical Properties of Simulated Estimators . . . . . . . . . . . . . . . . . 437 13.6 EIS Application: Logit Panel with Unobserved Heterogeneity . . . . 439 13.6.1 The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439 13.6.2 EIS Evaluation of the Likelihood . . . . . . . . . . . . . . . . . . . . 440 13.6.3 Empirical Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443 13.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445 13.8 Appendix: Implementation of EIS for the Logit Panel Model . . . . . 446 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
14 Semi-parametric and Non-parametric Methods in Panel Data Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 Chunrong Ai and Qi Li 14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 14.2 Linear Panel Data Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452

xiv

Contents

14.2.1 Additive Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452 14.2.2 Multiplicative Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460 14.3 Nonlinear Panel Data Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462 14.3.1 Censored Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . 462 14.3.2 Discrete Choice Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470 14.3.3 Sample Selection Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474 14.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476

15 Panel Data Modeling and Inference: A Bayesian Primer . . . . . . . . . . 479 Siddhartha Chib 15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479 15.1.1 Hierarchical Prior Modeling . . . . . . . . . . . . . . . . . . . . . . . . . 480 15.1.2 Elements of Markov Chain Monte Carlo . . . . . . . . . . . . . . 483 15.1.3 Some Basic Bayesian Updates . . . . . . . . . . . . . . . . . . . . . . . 486 15.1.4 Basic Variate Generators . . . . . . . . . . . . . . . . . . . . . . . . . . . 488 15.2 Continuous Responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489 15.2.1 Gaussian­Gaussian Model . . . . . . . . . . . . . . . . . . . . . . . . . . 490 15.2.2 Robust Modeling of bi: Student­Student and Student-Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . 492 15.2.3 Heteroskedasticity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495 15.2.4 Serial Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496 15.3 Binary Responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497 15.4 Other Outcome Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501 15.4.1 Censored Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501 15.4.2 Count Responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502 15.4.3 Multinomial Responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503 15.5 Binary Endogenous Regressor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504 15.6 Informative Missingness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507 15.7 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508 15.8 Residual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 15.9 Model Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 15.9.1 Gaussian­Gaussian Model . . . . . . . . . . . . . . . . . . . . . . . . . . 512 15.9.2 Gaussian­Gaussian Tobit model . . . . . . . . . . . . . . . . . . . . . 512 15.9.3 Panel Poisson Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513 15.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514

16 To Pool or Not to Pool? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517 Badi H. Baltagi, Georges Bresson and Alain Pirotte 16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517 16.2 Tests for Poolability, Pretesting and Stein-Rule Methods . . . . . . . . . 521 16.2.1 Tests for Poolability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521 16.2.2 Pretesting and Stein-Rule Methods . . . . . . . . . . . . . . . . . . . 525 16.2.3 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526 16.3 Heterogeneous Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527

Contents

xv

16.3.1 Averaging Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529 16.3.2 Bayesian Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530 16.3.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538 16.4 Comments on the Predictive Approach . . . . . . . . . . . . . . . . . . . . . . . 541 16.4.1 From the Post-sample Predictive Density. . . . . . . . . . . . . . . 541 16.4.2 . . . to the Good Forecast Performance of the
Hierarchical Bayes Estimator: An Example . . . . . . . . . . . . 542 16.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
17 Duration Models and Point Processes . . . . . . . . . . . . . . . . . . . . . . . . . . 547 Jean-Pierre Florens, Denis Fouge`re and Michel Mouchart 17.1 Marginal Duration Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548 17.1.1 Distribution, Survivor and Density Functions . . . . . . . . . . 548 17.1.2 Truncated Distributions and Hazard Functions . . . . . . . . . 550 17.2 Conditional Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552 17.2.1 General Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552 17.2.2 The Proportional Hazard or Cox Model . . . . . . . . . . . . . . . 555 17.2.3 The Accelerated Time Model . . . . . . . . . . . . . . . . . . . . . . . . 557 17.2.4 Aggregation and Heterogeneity . . . . . . . . . . . . . . . . . . . . . . 558 17.2.5 Endogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560 17.3 Competing Risks and Multivariate Duration Models . . . . . . . . . . . . 561 17.3.1 Multivariate Durations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 17.3.2 Competing Risks Models: Definitions . . . . . . . . . . . . . . . . 563 17.3.3 Identifiability of Competing Risks Models . . . . . . . . . . . . . 566 17.3.4 Right-Censoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 17.4 Inference in Duration Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570 17.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570 17.4.2 Parametric Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570 17.4.3 Non-parametric and Semi-parametric Models . . . . . . . . . . 576 17.5 Counting Processes and Point Processes . . . . . . . . . . . . . . . . . . . . . . 579 17.5.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579 17.5.2 Stochastic Intensity, Compensator and Likelihood of a Counting Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581 17.6 Poisson, Markov and Semi-Markov Processes . . . . . . . . . . . . . . . . . 584 17.6.1 Poisson Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584 17.6.2 Markov Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585 17.6.3 Semi-Markov Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592 17.7 Statistical Analysis of Counting Processes . . . . . . . . . . . . . . . . . . . . 594 17.7.1 The Cox Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596 17.7.2 The Martingale Estimation of the Integrated Baseline Intensity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597 17.8 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600

xvi

Contents

18 GMM for Panel Data Count Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 603 Frank Windmeijer 18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603 18.2 GMM in Cross-Sections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604 18.3 Panel Data Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606 18.3.1 Strictly Exogenous Regressors . . . . . . . . . . . . . . . . . . . . . . . 607 18.3.2 Predetermined Regressors . . . . . . . . . . . . . . . . . . . . . . . . . . 608 18.3.3 Endogenous Regressors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609 18.3.4 Dynamic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 610 18.4 GMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612 18.5 Applications and Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614 18.6 Finite Sample Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 615 18.6.1 Wald Test and Finite Sample Variance Correction . . . . . . . 615 18.6.2 Criterion-Based Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617 18.6.3 Continuous Updating Estimator . . . . . . . . . . . . . . . . . . . . . . 618 18.6.4 Monte Carlo Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623
19 Spatial Panel Econometrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625 Luc Anselin, Julie Le Gallo and Hubert Jayet 19.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625 19.2 Spatial Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626 19.2.1 Spatial Weights and Spatial Lag Operator . . . . . . . . . . . . . 628 19.2.2 Spatial Lag Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630 19.2.3 Spatial Error Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632 19.3 A Taxonomy of Spatial Panel Model Specifications . . . . . . . . . . . . . 636 19.3.1 Temporal Heterogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637 19.3.2 Spatial Heterogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639 19.3.3 Spatio-Temporal Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 644 19.4 Estimation of Spatial Panel Models . . . . . . . . . . . . . . . . . . . . . . . . . . 648 19.4.1 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . 648 19.4.2 Instrumental Variables and GMM . . . . . . . . . . . . . . . . . . . . 652 19.5 Testing for Spatial Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654 19.5.1 Lagrange Multiplier Tests for Spatial Lag and Spatial Error Dependence in Pooled Models . . . . . . . . . . . . . . . . . . 655 19.5.2 Testing for Spatial Error Correlation in Panel Data Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655 19.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657

Part III Applications

20 Foreign Direct Investment: Lessons from Panel Data . . . . . . . . . . . . . 663 Pierre Blanchard, Carl Gaigne´ and Claude Mathieu 20.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 663 20.2 A Simple Model of FDI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 664 20.2.1 Assumptions and Preliminary Results . . . . . . . . . . . . . . . . . 665

Contents

xvii

20.2.2 Technology and Country Characteristics as Determinants of FDI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666
20.3 Econometric Implementation and Data . . . . . . . . . . . . . . . . . . . . . . . 668 20.3.1 A General Econometric Model . . . . . . . . . . . . . . . . . . . . . . 669 20.3.2 FDI and Data Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 670
20.4 Empirical Estimations: Selected Applications . . . . . . . . . . . . . . . . . . 672 20.4.1 Testing the Trade-Off Between FDI and Exports . . . . . . . . 672 20.4.2 Testing the Role of Trade Policy in FDI . . . . . . . . . . . . . . . 677 20.4.3 Testing the Relationship Between FDI and Exchange Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683
20.5 Some Recent Econometric Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . 690 20.5.1 FDI, Panel Data and Spatial Econometrics . . . . . . . . . . . . . 690 20.5.2 Exchange Rate, Unit Roots and Cointegration . . . . . . . . . . 691
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693
21 Stochastic Frontier Analysis and Efficiency Estimation . . . . . . . . . . . 697 Christopher Cornwell and Peter Schmidt 21.1 Measurement of Firm Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 698 21.2 Introduction to SFA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 700 21.2.1 The Basic SFA Empirical Framework . . . . . . . . . . . . . . . . . 700 21.2.2 Stochastic vs Deterministic Frontiers . . . . . . . . . . . . . . . . . 700 21.2.3 Other Frontier Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 702 21.2.4 SFA with Cross-Section Data . . . . . . . . . . . . . . . . . . . . . . . . 703 21.3 SFA with Panel Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 704 21.3.1 Models with Time-Invariant Inefficiency . . . . . . . . . . . . . . 704 21.3.2 Models with Time-Varying Inefficiency . . . . . . . . . . . . . . . 714 21.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718 21.4.1 Egyptian Tile Manufacturers . . . . . . . . . . . . . . . . . . . . . . . . 718 21.4.2 Indonesian Rice Farmers . . . . . . . . . . . . . . . . . . . . . . . . . . . 720 21.5 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723

22 Econometric Analyses of Linked Employer­Employee Data . . . . . . . 727 John M. Abowd, Francis Kramarz and Simon Woodcock 22.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727 22.2 A Prototypical Longitudinal Linked Data Set . . . . . . . . . . . . . . . . . . 729 22.2.1 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 730 22.2.2 Sampling from Linked Data . . . . . . . . . . . . . . . . . . . . . . . . . 732 22.3 Linear Statistical Models with Person and Firm Effects . . . . . . . . . . 733 22.3.1 A General Specification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 733 22.3.2 The Pure Person and Firm Effects Specification . . . . . . . . 734 22.4 Definition of Effects of Interest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 735 22.4.1 Person Effects and Unobservable Personal Heterogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 735 22.4.2 Firm Effects and Unobservable Firm Heterogeneity . . . . . 736 22.4.3 Firm-Average Person Effect . . . . . . . . . . . . . . . . . . . . . . . . . 737

xviii

Contents

22.4.4 Person-Average Firm Effect . . . . . . . . . . . . . . . . . . . . . . . . . 737 22.4.5 Industry Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 738 22.4.6 Other Firm Characteristic Effects . . . . . . . . . . . . . . . . . . . . 739 22.4.7 Occupation Effects and Other Person × Firm
Interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 739 22.5 Estimation by Fixed Effects Methods . . . . . . . . . . . . . . . . . . . . . . . . . 739
22.5.1 Estimation of the Fixed Effects Model by Direct Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 739
22.5.2 Consistent Methods for  and  (The Firm-Specific Returns to Seniority) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 743
22.6 The Mixed Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 744 22.6.1 REML Estimation of the Mixed Model . . . . . . . . . . . . . . . 746 22.6.2 Estimating the Fixed Effects and Realized Random Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747 22.6.3 Mixed Models and Correlated Random Effects Models . . 748
22.7 Models of Heterogeneity Biases in Incomplete Models . . . . . . . . . 750 22.7.1 Omission of the Firm Effects . . . . . . . . . . . . . . . . . . . . . . . . 750 22.7.2 Omission of the Person Effects . . . . . . . . . . . . . . . . . . . . . . 751 22.7.3 Inter-industry Wage Differentials . . . . . . . . . . . . . . . . . . . . 752
22.8 Endogenous Mobility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753 22.8.1 A Generalized Linear Mixed Model . . . . . . . . . . . . . . . . . . 754 22.8.2 A Model of Wages, Endogenous Mobility and Participation with Person and Firm Effects . . . . . . . . . . . . 755 22.8.3 Stochastic Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 756
22.9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 758 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 758
23 Life Cycle Labor Supply and Panel Data: A Survey . . . . . . . . . . . . . . 761 Bertrand Koebel, Franc¸ois Laisney, Winfried Pohlmeier and Matthias Staat 23.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 761 23.2 The Basic Model of Life Cycle Labor Supply . . . . . . . . . . . . . . . . . . 762 23.2.1 The Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763 23.2.2 First Specifications of the Utility Function . . . . . . . . . . . . . 765 23.3 Taking Account of Uncertainty and Risk . . . . . . . . . . . . . . . . . . . . . . 768 23.3.1 First Developments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 768 23.3.2 Recent Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770 23.3.3 Empirical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 773 23.4 Voluntary and Involuntary Non-participation . . . . . . . . . . . . . . . . . . 774 23.4.1 Accounting for the Participation Decision . . . . . . . . . . . . . 775 23.4.2 Unemployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778 23.5 Alternative Parameterization and Implications . . . . . . . . . . . . . . . . . 779 23.6 Relaxing Separability Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . 783 23.6.1 Relaxing Within-Period Additive Separability . . . . . . . . . . 783 23.6.2 Relaxing Intertemporal Separability in Preferences . . . . . 784

Contents

xix

23.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 790 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 791
24 Dynamic Policy Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795 Jaap H. Abbring and James J. Heckman 24.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795 24.2 Policy Evaluation and Treatment Effects . . . . . . . . . . . . . . . . . . . . . . 796 24.2.1 The Evaluation Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 796 24.2.2 The Treatment Effect Approach . . . . . . . . . . . . . . . . . . . . . . 800 24.2.3 Dynamic Policy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 801 24.3 Dynamic Treatment Effects and Sequential Randomization . . . . . . 803 24.3.1 Dynamic Treatment Effects . . . . . . . . . . . . . . . . . . . . . . . . . 803 24.3.2 Policy Evaluation and Dynamic Discrete-Choice Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 810 24.3.3 The Information Structure of Policies . . . . . . . . . . . . . . . . . 813 24.3.4 Selection on Unobservables . . . . . . . . . . . . . . . . . . . . . . . . . 815 24.4 The Event-History Approach to Policy Analysis . . . . . . . . . . . . . . . . 816 24.4.1 Treatment Effects in Duration Models . . . . . . . . . . . . . . . . 817 24.4.2 Treatment Effects in More General Event-History Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 823 24.4.3 A Structural Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . 828 24.5 Dynamic Discrete Choice and Dynamic Treatment Effects . . . . . . . 829 24.5.1 Semi-parametric Duration Models and Counterfactuals . . 831 24.5.2 A Sequential Structural Model with Option Values . . . . . . 844 24.5.3 Identification at Infinity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 850 24.5.4 Comparing Reduced-Form and Structural Models . . . . . . 851 24.5.5 A Short Survey of Dynamic Discrete-Choice Models . . . . 853 24.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 857 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 857
25 Econometrics of Individual Labor Market Transitions . . . . . . . . . . . . 865 Denis Fouge`re and Thierry Kamionka 25.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 865 25.2 Multi-spell Multi-state Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 867 25.2.1 General framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 867 25.2.2 Non-parametric and Parametric Estimation . . . . . . . . . . . . 872 25.2.3 Unobserved Heterogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . 878 25.3 Markov Processes Using Discrete-Time Observations . . . . . . . . . . . 882 25.3.1 The Time-Homogeneous Markovian Model . . . . . . . . . . . . 883 25.3.2 The Mover-Stayer Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 893 25.4 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 901 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 902

xx

Contents

26 Software Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 907 Pierre Blanchard 26.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 907 26.2 General-Purpose Econometric Packages . . . . . . . . . . . . . . . . . . . . . . 908 26.2.1 EViews (v. 5.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 908 26.2.2 LIMDEP (v. 8) with NLOGIT (v. 3) . . . . . . . . . . . . . . . . . . 912 26.2.3 RATS (v. 6) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 916 26.2.4 SAS (v. 9.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 920 26.2.5 Stata (v. 9) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 923 26.2.6 TSP (v. 5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 927 26.3 High-Level Matrix Programming Languages . . . . . . . . . . . . . . . . . . 930 26.3.1 GAUSS (v. 5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 930 26.3.2 Ox (v. 3.4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 936 26.4 Performance Hints and Numerical Accuracy Evaluation . . . . . . . . . 941 26.4.1 Speed Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 941 26.4.2 Numerical Accuracy Evaluations . . . . . . . . . . . . . . . . . . . . 944 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 949

List of Contributors
Jaap H. Abbring Department of Economics, VU University Amsterdam, De Boelelaan 1105, 1081 HV Amsterdam, The Netherlands, and Tinbergen Institute, e-mail: jabbring@econ.vu.nl John M. Abowd School of Industrial and Labor Relations, Cornell University, Ithaca, NY 14850, USA, e-mail: john.abowd@cornell.edu Chunrong Ai Department of Economics, University of Florida, Warrington College of Business Administration, 224 MAT, P.O. Box 117140, Gainesville, FL 32611-7140, USA, e-mail: chunrong.ai@cba.ufl.edu Luc Anselin School of Geographical Sciences, Arizona State University, Tempe, AZ 85287, USA, e-mail: luc.anselin@asu.edu Pietro Balestra Faculty of Economics, University of Lugano, via Giuseppe Buffi 13, CH-6904 Lugano, Switzerland. Badi H. Baltagi Center for Policy Research, 426 Eggers Hall, Syracuse University, Syracuse, NY 13244-1020, USA, e-mail: bbaltagi@maxwell.syr.edu Erik Biørn Department of Economics, University of Oslo, P.O. Box 1095, Blindern, 0317 Oslo, Norway, e-mail: erik.biorn@econ.uio.no
xxi

xxii

List of Contributors

Pierre Blanchard Erudite, Faculte´ de Sciences Economiques et de Gestion, Universite´ Paris XII Val de Marne, 61 Av. du Ge´ne´ral de Gaulle, 94010 Cre´teil Ce´dex, France, e-mail: blanchard@univ-paris12.fr

Rachid Boumahdi Toulouse School of Economics, GREMAQ and LIHRE; Universite´ des Sciences Sociales de Toulouse, 21 Alle´e de Brienne, 31000 Toulouse, France, e-mail: boumahdi@univtlse1.fr

Jo¨rg Breitung University of Bonn, Institute of Econometrics, Adenauerallee 24-42, 53113 Bonn, Germany, e-mail: breitung@uni-bonn.de

Georges Bresson ERMES (UMR 7181, CNRS), Universite´ Paris II and TEPP (FR 3126, CNRS), Institute for Labor Studies and Public Policies, 12, place du Panthe´on, 75230 Paris Cedex 05, France, e-mail: bresson01@aol.com

Siddhartha Chib Olin Business School, Campus Box 1133, Washington University in St. Louis, 1 Brookings Dr, St. Louis, MO 63130, e-mail: chib@wustl.edu

Christopher Cornwell Department of Economics, University of Georgia, Athens, GA 30602, USA, e-mail: cornwl@terry.uga.edu

Bruno Cre´pon CREST-INSEE (Paris), CEPR (London) and IZA (Bonn), CREST-INSEE, 15 Boulevard Gabriel Pe´ri, 92245 Malakoff Cedex, France, e-mail: crepon@ensae.fr

Jean-Pierre Florens Toulouse School of Economics, Institut Universitaire de France, Universite´ des Sciences Sociales de Toulouse, 21 Alle´e de Brienne, 31000 Toulouse, France, e-mail: florens@cict.fr

Denis Fouge`re CREST-INSEE (Paris), CEPR (London) and IZA (Bonn), CREST-INSEE, 15 Boulevard Gabriel Pe´ri, 92245 Malakoff Cedex, France, e-mail: fougere@ensae.fr
Carl Gaigne´ INRA, UMR1302, SMART, F-35000 Rennes, France, e-mail: gaigne@rennes.inra.fr

List of Contributors

xxiii

Mark N. Harris Department of Econometrics & Business Statistics, Monash University, Victoria 3800, Australia, e-mail: mark.harris@buseco.monash.edu.au
James J. Heckman Department of Economics, University of Chicago, 1126 E. 59th Street, Chicago, IL 60637, USA; American Bar Foundation and Geary Institute, University College Dublin, e-mail: jjh@uchicago.edu
Bo Honore´ Department of Economics, Princeton University, Princeton, NJ 08544-1021, USA, e-mail: honore@Princeton.EDU
Cheng Hsiao University of Southern California and Nanyang Technological University, Department of Economics, University of Southern California, University Park, Los Angeles, California 90089, USA, e-mail: chsiao@usc.edu
Hubert Jayet EQUIPPE, University of Science and Technology of Lille, Faculty of Economics and Social Sciences, 59655 Villeneuve d'Ascq Cedex, France, e-mail: Hubert.Jayet@univ-lille1.fr
Thierry Kamionka CNRS and CREST-INSEE (Paris), CREST-INSEE 15, Boulevard Gabriel Pe´ri, 92245 Malakoff Cedex, France, e-mail: kamionka@ensae.fr
Bertrand Koebel BETA, Universite´ Louis Pasteur, Strasbourg I and IZA, Bonn, 61 Avenue de la Fore^t Noire, F67000 Strasbourg, France, e-mail: koebel@cournot.u-strasbg.fr
Francis Kramarz CREST-INSEE (Paris), CREST-INSEE, 15 Boulevard Gabriel Pe´ri, 92245 Malakoff Cedex, France, e-mail: kramarz@ensae.fr
Jayalakshmi Krishnakumar Department of Econometrics, University of Geneva, 40 Bd. du Pont d'Ave, CH-1211 Geneva 4, Switzerland, e-mail: jaya.krishnakumar@metri.unige.ch
Franc¸ois Laisney BETA, Universite´ Louis Pasteur, Strasbourg I and ZEW, Mannheim, 61 Avenue de la Fore^t Noire, F67000 Strasbourg, France, e-mail: fla@cournot.u-strasbg.fr

xxiv

List of Contributors

Michael Lechner Swiss Institute for Empirical Economic Research (SEW), University of St. Gallen, Varnbu¨hlstr. 14, CH-9000 St. Gallen, Switzerland, e-mail: Michael.Lechner@unisg.ch
Julie Le Gallo CRESE, Universite´ de Franche-Comte´, 45D Avenue de l'Observatoire, 25030 Besanc¸on Cedex, France, e-mail: jlegallo@univ-fcomte.fr
Qi Li Department of Economics, Texas A&M University, College Station, TX 778434228, USA, e-mail: qi@econmail.tamu.edu
Roman Liesenfeld Department of Economics, Christian-Albrechts-Universita¨t Kiel, Olhshausenstr. 40-60, 24118 Kiel, Germany, e-mail: liesenfeld@stat-econ.uni-kiel.de
Ste´fan Lollivier INSEE, 18 boulevard Adolphe Pinard, F-75014 Paris, France, e-mail: stefan.lollivier@insee.fr
Thierry Magnac Universite´ de Toulouse 1, Toulouse School of Economics, Manufacture des Tabacs, 21, Alle´e de Brienne, 31000 Toulouse, France, e-mail: magnac@cict.fr
Jacques Mairesse CREST-INSEE (Paris), UNU-MERIT (University of Maastricht), and NBER. CREST-INSEE, 15 boulevard Gabriel PERI, 92245 MALAKOFF cedex, France, e-mail: mairesse@ensae.fr
Claude Mathieu Erudite, Faculte´ de Sciences Economiques et de Gestion, Universite´ Paris XII Val de Marne, 61 Av. du Ge´ne´ral de Gaulle, 94010 Cre´teil Ce´dex, France, e-mail: mathieu@univ-paris12.fr
La´szlo´ Ma´tya´s Central European University, Department of Economics, Na´dor u. 9, 1051 Budapest, Hungary, e-mail: matyas@ceu.hu
Michel Mouchart Emeritus Professor of Statistics and Econometrics, Institut de statistique, 20 Voie du Roman Pays, B-1348 Louvain-La-Neuve (Belgium), e-mail: Michel.Mouchart@uclouvain.be

List of Contributors

xxv

Marc Nerlove Department of Agricultural and Resource Economics, University of Maryland, 2200 Symons Hall, College Park, MD 20742-5535, USA, e-mail: mnerlove@arec.umd.edu
Hashem Pesaran Cambridge University and USC, Sidgwick Avenue, Cambridge, CB3 9DD, United Kingdom, e-mail: mhp1@econ.cam.ac.uk

Alain Pirotte ERMES (UMR 7181, CNRS), Universite´ Paris II and TEPP (FR 3126, CNRS), Institute for Labor Studies and Public Policies, 12, place du Panthe´on, 75230 Paris Cedex 05, France, e-mail: apirotte@aol.com

Winfried Pohlmeier University of Konstanz, Department of Economics, Box D124, D78457 Konstanz, Germany, e-mail: winfried.pohlmeier@uni-konstanz.de
Jean-Franc¸ois Richard Department of Economics, University of Pittsburgh, Pittsburgh, PA 15260, USA, e-mail: fantin@pitt.edu
Peter Schmidt Department of Economics, Michigan State University, East Lansing, MI 48824, USA, e-mail: schmidtp@msu.edu

Patrick Sevestre Universite´ Paris 1 ­ Panthe´on Sorbonne, Ecole Economique de Paris (Paris School of Economics), 106-112 Boulevard de l'Ho^pital, 75013 Paris, France, e-mail: sevestre@univ-paris1.fr

Matthias Staat University of Mannheim, Department of Economics, D68131 Mannheim, Germany, e-mail: matthias@pool.uni-mannheim.de
Alban Thomas Toulouse School of Economics, INRA; Universite´ des Sciences Sociales de Toulouse, 21 Alle´e de Brienne, 31000 Toulouse, France, e-mail: thomas@toulouse.inra.fr
Francis Vella Department of Economics, Georgetown University, Washington DC, USA, e-mail: fgv@georgetown.edu

xxvi

List of Contributors

Marno Verbeek Department of Financial Management, RSM Erasmus University, Burg. Oudlaan 50, 3062 PA Rotterdam, The Netherlands, e-mail: mverbeek@rsm.nl
Frank Windmeijer Department of Economics, University of Bristol, 8 Woodland Road, Bristol BS8 1TN, UK, e-mail: f.windmeijer@bristol.ac.uk
Simon Woodcock Department of Economics, Simon Fraser University, Burnaby, BC, Canada, e-mail: simon woodcock@sfu.ca

Part I
Fundamentals

Chapter 1
Introduction
Marc Nerlove, Patrick Sevestre and Pietro Balestra

1.1 Introduction
In his famous and influential monograph, The Probability Approach in Econometrics, Haavelmo (1944) laid the foundations for the formulation of stochastic econometric models and an approach which has dominated our discipline to this day. He wrote:
. . . we shall find that two individuals, or the same individual in two different time periods, may be confronted with exactly the same set of specified influencing factors [and, hence, they have the same y, . . . ], and still the two individuals may have different quantities y, neither of which may be equal to y. We may try to remove such discrepancies by introducing more "explaining" factors, x. But, usually, we shall soon exhaust the number of factors which could be considered as common to all individuals, and which, at the same time, were not merely of negligible influence upon y. The discrepancies y - y for each individual may depend upon a great variety of factors, these factors may be different from one individual to another, and they may vary with time for each individual (Haavelmo, 1944, p. 50).
And further that:
. . .we find justification for applying them [stochastic approximations] to economic phenomena also in the fact we usually deal only with--and are interested only in--total or average effects of many individual decisions, which are partly guided by common factors, partly by individual specific factors. . . (Haavelmo, 1944, pp. 51 and 56)
Marc Nerlove Department of Agricultural and Resource Economics, University of Maryland, 2200 Symons Hall, College Park, MD 20742-5535, USA, e-mail: mnerlove@arec.umd.edu
Patrick Sevestre Universite´ Paris 1 ­ Panthe´on Sorbonne, Ecole Economique de Paris (Paris School of Economics), 106-112 Boulevard de l'Ho^pital, 75013 Paris, France, e-mail: sevestre@univ-paris1.fr
Pietro Balestra Faculty of Economics, University of Lugano, via Giuseppe Buffi 13, CH-6904 Lugano, Switzerland
This introductory chapter is dedicated to the memory of our late colleague, Pietro Balestra. This chapter largely draws on his and Nerlove's earlier introduction to the second edition of this volume.

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

3

c Springer-Verlag Berlin Heidelberg 2008

4

M. Nerlove et al.

Marschak (1950, 1953) further amplified Haavelmo's themes in his introduction to Cowles Commission Monographs 10 and 14, observing that:
The numerous causes that determine the error incurred . . . are not listed separately; instead their joint effect is represented by the probability distribution of the error, a random variable (1950, p. 18) [which] . . . is called `disturbance' or `shock,' and can be regarded as the joint effect of numerous separately insignificant variables that we are unable or unwilling to specify but presume to be independent of observable exogenous variables. (1953, p. 12).
In this introduction we examine how the basic principle underlying the formulation of econometric models has been carried forward in the development of econometric models and methods for the analysis of panel data. We argue that while fixed effects models may be appropriate in cases in which a population is sampled exhaustively (e.g., data from geographic regions over time) or in which it is desired to predict individual behavior (e.g., the probability that a given individual in a sample will default on a loan), random effects models are more consistent with Haavelmo's view, quoted above, that the "population" we model in econometrics consists not of an infinity of individuals, in general; but of an infinity of decisions. This is not to say, however, that fixed effects models may not be extremely useful as an analytic device.
Moreover, we shall argue, taking a leaf from Knight (1921), that what differentiates the individuals, who make the decisions with which we are concerned, is largely historical, the "three great accumulating funds of inheritance from the past, material goods and appliances, knowledge and skill, and morale." This view has important implications for the relevance and appropriateness of many of the models and methods for the analysis of panel data which have been developed over the past 40 years. We briefly review these developments here and conclude that not only are random effects models most relevant and appropriate but that often our central analytical and modelling concerns are also dynamic. Thus, the most fruitful developments in this enormous literature have been those which deal with the central issues of history and dynamics.

1.2 Data, Data-Generating Processes (DGP), and Inference
In most applications of statistical analysis in the so-called "hard" sciences, the process by which the observed data are generated is transparent, having usually been determined by the investigator by design. In contrast, in many applications in the social sciences, especially in economics, the mechanism by which the data are generated is opaque. In such circumstances, estimation of the parameters of the statistical or econometric model and the testing of specific hypotheses about it are only half the problem of inference. Understanding the process by which the observations at hand are generated is of equal importance. Were the data for example obtained from a sample of firms selected by stratified random sampling from a census of all firms in the United States in 2000? For example, were they obtained from regulatory activity? In the case of time series, the data are almost always "fabricated" in one way or another, by aggregation, interpolation, or extrapolation, or by all three. The nature of the sampling frame or the way in which the data are fabricated must be

1 Introduction

5

part of the model specification on which parametric inference or hypothesis testing is based. Nonparametric inference imposes fewer restrictions on the specification of the DGP but incorporation of knowledge about the nature of the DGP is of equal importance. Almost all the methodological papers in Parts 1 and 2 of this volume focus primarily on problems of estimation and inference from a parametrically wellspecified model of how the observed data were generated and variously draw their interest from different types of data and their DGPs. In this section we address the issue of why the DGP matters in the context of a specific, although somewhat abstract, example.
Suppose a longitudinal household survey in which the same households are questioned over time about their actions in, say, a number of consecutive months or years and, initially, about various demographic and economic characteristics. These households differ in various ways, some of which we observe and many which we do not. Some of these differences are the result of their past behavior or past circumstances (path dependence), some are differences in tastes or other unobserved characteristics which may be assumed to be permanent (individual heterogeneity), and some are due to peculiarities not permanently associated with time or individual.1
What, in the context of these data, can be considered as random, what is the population from which we may consider the data a sample, and what is a parameter, and what a random variable? These issues are central to an understanding of the DGP.
Statistical and, a fortiori, econometric analysis, are usually based on the idea of sampling from a population in order to draw inferences for the underlying population. But what is the population from which economic data may be supposed to be a sample? In his famous 1944 monograph, Haavelmo (1944, p. 56) wrote, ". . . the class of populations we are dealing with does not consist of an infinity of different individuals, it consists of an infinity of possible decisions which might be taken . . . ". In their recent text, Econometric Theory and Methods, Davidson and MacKinnon (2004, pp. 30­31) make the same point: "In econometrics, the use of the term population is simply a metaphor. A better concept is that of a data-generating process, or DGP. By this term, we mean whatever mechanism is at work in the real world of economic activity giving rise to the numbers in our samples, that is, precisely the mechanism that our econometric model is supposed to describe. A DGP is thus the analog of a population in biostatistics. Samples may be drawn from a DGP just as they may be drawn from a population. In both cases, the samples are assumed to be representative of the DGP or population from which they are drawn."
What is a random variable in this context and what is not? Whether or not a particular variable can be considered a random draw from some population or not, in principle can be decided by applying the principle of "exchangeability" introduced by de Finetti (1930). In a nutshell, the idea, very Bayesian in flavor, is to ask whether we can exchange two elements in a sample and still maintain the same subjective distribution. Thus, in a panel study of households, are any two households in the sample exchangeable without affecting the distribution, from which we imagine

1 In his paper, "Identifying the Hand of the Past: Distinguishing State Dependence from Heterogeneity", Heckman (1991) argues that in general it is not possible to distinguish. The ability to do so rests critically "on maintaining explicit assumptions about the way in which observables and unobservables interact."

6

M. Nerlove et al.

household observables and unobservables to be drawn? In a panel of state data, are

California and Maryland exchangeable without affecting the subjective distribution

of the state effects? It's a dicey question ­ sometimes.

From the standpoint of a Bayesian there is no real distinction between a param-

eter and a random variable, but in this context we could say that a parameter is an

unobserved variable which affects the distribution of the random variables of the

model and is unaffected by the particular values such variables take on. It is what

we wish to estimate and about which we wish to make inferences. A related concept

is that of an exogenous variable. But note here that such an exogenous variable is

still a random variable and not a parameter.

In general, in the formulation of econometric models (i.e., the DGP for the pro-

cess yielding the particular set of data we want to "explain") the distinction between

what can be observed and what is not is fundamental. Linear functions are often

used to describe such a DGP. To get more precisely to the issues posed by the for-

mulation of the DGP for a sample of economic data, we need to include several

observable variables. Suppose that we draw a random sample of N individuals over

T time periods; for example a household survey in which we collect observations

on the income, xit and consumption of household i, yit , for many households N, in

year t over a brief period T . From the survey we have observations on the pairs (xit ,

yit ). Since the households are chosen at random for the survey, but the years over

which they are observed are not, the lists (xi1, yi1, . . . , xiT , yiT ), i = 1, . . . , N, are exchangeable, but the order within each list is not.

Imagine we are estimating a consumption function and assume a linear relation-

ship subject to error:

yit = a + b xit + it

(1.1)

This would be the case, if for example, the joint distribution of variables could
be assumed normal and we were trying to estimate the mean of yit for a particular year t conditional on xit . We might then write it as

it = i + t + uit

(1.2)

where it is an unobserved random variable which is the sum of three effects, all of which are also unobserved: t is a year effect, arguably nonrandom and therefore a parameter to be estimated for each year, t; i is a household effect, which, in view of the way the observations are drawn, should surely be treated as random, and, finally,
uit is a random variable to represent all the rest. We are far from done yet, however. The question remains as to what we should
assume about the observable variables, xit . They are clearly random variables jointly distributed with the variable yit . If not subject to errors of measurement, an assumption difficult to justify in the context of an economic survey, are they also independent of, or at least uncorrelated with, the disturbances it in (1.1)? This question clearly affects not only what we can say about the DGP which generates
our observations, but also how many and what parameters must be considered. Let us examine the regression with some care. Since t is not a random variable but a parameter, consider it to be a constant for each t and add it to the constant a in the
regression equation (1.1):

1 Introduction

7

where

yit = at + bxit + it

(1.3)

at = a + t and it = i + uit .

Suppose that (i1, i2, . . . , iT ) is distributed with mean zero and variance­ covariance matrix  . If xit is strictly exogenous in the regression (1.3), which means

E(it |xi1, xi2, . . . , xiT ) = 0, i and t ,

(1.4)

then (1.3) is the usual panel model. This means that b can be estimated by GLS or ML with a dummy variable for each t. Weak exogeneity is a related concept, introduced by Engle, Hendry and Richard (1983). In the context of the regression (1.3), we say xit is weakly exogenous if it is distributed independently of {xis, yis, for all i and s  t - 1}, if the marginal distribution of {xis, yis, for all i and s  t - 1} does not depend on any unknown parameters in  or on b or the  s, nor does the pdf of xit | {xis, yis, for all i and s  t - 1}. If regression (1.3) satisfies the conditions of strict exogeneity, the likelihood function for the whole sample of observations on x and y factors into two pieces, one of which is the usual regression likelihood and the other is a function of x but not of the parameters in  or b or the  s. In that sense we can treat the observations on x as fixed.
But is exogeneity, weak or strict, a reasonable assumption? Here is what Wooldridge (2002, p. 252) says: "Traditional unobserved components panel models take the xit as fixed. We will never assume the xit are nonrandom because potential feedback from yit to xis for s > t needs to be addressed explicitly."
The assumption that the explanatory variables in the regression are exogenous is generally impossible. If the vector of explanatory variables includes any lagged
values of yit , either explicitly or implicitly, the strict or weak exogeneity is generally impossible. Any meaningful DGP describing individual economic behavior is intrinsically dynamic in the sense that the "hand of the past," whether as a result
of path dependence or of individual heterogeneity, is ever present. To put the point more explicitly, if, among the observed variables are any initial conditions related to past values of the observed yit 's or to unobservables affecting present and past behavior, at least one of the components of xit must be correlated with it . A Hausman test will reject exogeneity of the x's almost certainly. A rejection of exogeneity does not, of course, imply that the unobserved components it of the errors in (1.3) are not random (RE) but fixed (FE).
Unfortunately, as Hsiao (2007) points out, this leaves the econometrician be-
tween Scylla and Charybdis: We're damned if we do assume that the errors are random, and damned if we don't. Although the RE model avoids the classic incidental parameters problem (Neyman and Scott, 1948), it is necessary in order to obtain unbiased estimates to specify the conditional distribution of i given the xit , and the i are unobservable. As Heckman, quoted above, says, one must be willing

8

M. Nerlove et al.

to make "explicit assumptions about the way in which observables and unobservables interact." But most econometricians are not willing to specify such interactions as part of the DGP. Hence, the random effects are treated as parameters rather than random variables. They are viewed as incidental parameters and the object is to get rid of them without distorting the estimates of the structural parameters. There is no universally accepted way of doing so in all contexts, especially not in explicitly dynamic or nonlinear contexts, and, in our view no right way of doing so.

1.3 History and Dynamics

The fundamental fact about society as a going concern is that it is made up of in-

dividuals who are born and die and give place to others; and the fundamental fact

about modern civilization is that, as previously quoted from Knight (1921), it is de-

pendent upon the utilization of three great accumulating funds of inheritance from

the past, material goods and appliances, knowledge and skill, and morale. Besides

the torch of life itself, the material wealth of the world, a technological system of

vast and increasing intricacy and the habituations which fit men for social life must

in some manner be carried forward to new individuals born devoid of all these things

as older individuals pass out.

The moral of Knight's characterization is that history is important and individuals

have histories. We illustrate our general view of the central principle involved using

a simple illustrative example drawn from a paper of Ma´tya´s and Rahman (1992).

Let i index individuals and t time periods. Suppose the relationship we are inter-

ested in estimating is


 yit = sxi,t-s + it . s=0

(1.5)

The variable xit is assumed to be exogenous and distributed independently of the true disturbances it for all finite subsets of the t-index set. We also assume, despite our previous injunction, that

E(it ) = 0, i,t E(it i t ) = 2 for i = i
= 0 otherwise .

and t = t

(1.6)

To guarantee some stability in the relationship we are interested in estimating, we must also assume some convergence properties for the sequence of distributed lag weights. Although stronger than necessary, assume they are square-summable:


 s2 <  .
s=0

(1.7)

Of course, as Ma´tya´s and Rahman note, (1.5) is not estimable with a finite amount of data. Indeed, the time dimension is likely to be very short. Instead, we truncate:

1 Introduction

9

k



k

   yit = sxi,t-s +

sxi,t-s + it  sxi,t-s + i + it .

s=0

s=k+1

s=0

(1.8)

Equation (1.8) is in the form of a frequently used random effect model, except that now the individual-specific effects are interpreted in terms of the past histories of each individual in the panel prior to the time when observation begins. Moreover, the assumption that xit is stochastic, although exogenous, is not innocuous. The implications are:
First, interpreting i as fixed, nonstochastic, is not appropriate. If you accept Haavelmo's view that the class of populations which we imagine (1.8) reflects, consists of decisions rather than identifiable specific individuals, then, in principle, we should not even condition on i. However, an exception to this rule is if, for the particular sample of individuals we have drawn (now we can specifically identify each), we want to predict future values of yit for that individual.
Second, since the xit are themselves considered to be stochastic, for each individual their values over time will in general be correlated. There may also be correlations among xit 's for different values of i if different individuals have some characteristics in common. But we neglect this possibility here. It follows that i and the values xit observed are correlated. Suppose, for example,

xit = ixi,t-1 + it ,

(1.9)

where | i |< 1 and E(it ) = 0, E(it i t ) = i2, i = i and t = t , and E(it i t ) = 0, i = i or t = t , for all i and t. Let S = {0, 1, . . . , K} be the set of indices for which,
given i, xit is observed (normally k will be chosen much less than K). Since

E(xit ) = 0

and

E(xit xi,t- )

=

1

i - i2

i2

,

(1.10) (1.11)

it follows that xit , t  S, will be correlated with i, and the correlation will depend on how close to the beginning of the sample period the observation on xit is taken:

  E(xi i)

=


s E( xi xi,t-s)
s=k+1

=

i2 1 - i2

 si|-s|
s=k+1

,

(1.12)

for   S. Clearly, this makes the likelihood of the sample much more difficult to determine and introduces some of the parameters, namely s, into the relationship between the individual-specific disturbances in (1.8) and the observed past values of the explanatory exogenous variable (we would perhaps be willing to regard i2 and i as nuisance parameters).
The important point about this admittedly unrealistic example is that it shows that
an entirely new set of questions must be considered. In particular, the error which
we make by treating i as independent of the observed values of xit now depends in a complex way on the way in which the distributed lag parameters of interest interact with the nuisance parameters i2 and i. Indeed, matters become even more

10

M. Nerlove et al.

interesting when we note that the unconditional variance of xit is i2/(1 - i2), so that, in general the greater i2 the greater is the signal to noise ratio in (1.8), on the one hand, but, ceteris paribus, the greater is the dependence between xi and i, especially for  near the beginning of the observation period. Other questions we must ask ourselves are: How can we optimally rid ourselves of the nuisance parameters? How badly does a method, which is based on the assumption that i and the observed xi are uncorrelated, approximate the true ML estimates? What constitute appropriate instruments in considering alternative methods to ML? And
so forth.
Consider now an autoregressive model:

yit = yi,t-1 + xit  + i + it , i = 1, . . . , N, t = 1, . . . , T .

(1.13)

If only semi-asymptotics on N is considered (T finite), we need not assume |  |< 1. On the other hand, the process generating the initial observations is very important. As suggested above, this means that the individuals' past history with respect to both the observed variables x and the latent variables  becomes crucial.
We can rewrite (1.13) as

 yit

=

t yi0

+

t

j=0

jxi,t- j

+

1 - t 1-

i

+ it

(1.14)

where

t-1
 it =  ji,t- j . j=0

Thus, each observation on the dependent variable yit can be written as the sum of four terms:
The first, t yi0, depends on the initial values which, as long as T is finite, do influence the behavior of any estimators. Moreover, there is no good reason (as in Balestra and Nerlove, 1966) to assume that these are fixed (to condition upon their values) and independent of individual specific effects. Indeed, unless there is something special about the initial date of observation, there is no justification for treating the initial observation differently from subsequent observations or from the past, but unobserved, previous values.
The second term in (1.14) depends on the current and past values of the exogenous variables xit . The form that this dependence takes depends not only on the dynamics of the model, but also on the way in which individuals' past histories differ (Knight, 1921).
The third term depends on remaining individual specific effects which are assumed to be orthogonal to the individual's past history.
Finally, the last term is a moving average in past values of the remaining disturbances, which may also be written:

it = it + it t  1 it = 0 t = 0 .

1 Introduction

11

Conditioning on the initial observations implies that they can be treated as fixed constants independently of i and it . They need not be independent of any of the lagged values of the explanatory x's which are included. But if any truncation within-sample occurs, the truncation remainder will be part of the individual specific disturbance, as shown above, and thus the initial values of the endogenous variable are not independent of the disturbance and cannot be treated as fixed.
This point can be made in another way (following Chap. 8): Write the cross section of initial observations as a function of past x's, i, and i0,

yi0 = f (xi,0, xi,-1, . . . , i, i0) .

(1.15)

The problem is now related to whether or not we choose to regard i as fixed or random. If i is fixed and thus independent, cross-sectionally, of i0, and if xi,t- j j = 0, 1, . . ., are cross-sectionally exogenous, then the yi0 can be conditioned on. They are still, however, random variables. But, if the i are random variables, the yi0 are not exogenous. This shows that in a dynamic context fixed effects versus error components assumptions make a big difference. Our preceding argument suggests
that the error components assumption is the more appropriate.
In this case, the literature suggests a variety of different assumptions about the
initial observation leading to different optimal estimation procedures and implying
different properties for suboptimal estimates. One line takes the generating pro-
cess of the initial observations to be different from that of subsequent observations.
Anderson and Hsiao (1982), for example, suggest a general form

yi0 = k0 + k11 + k2i0 .

(1.16)

If k1 = k2 = 0, the initial observations are fixed and identical. If k0 = k1 = 0 and k2 = 0, the yi0 are random variables independent of the disturbances in (1.13). If k0 = 0, k1 = 1/(1 - ) and k2 = 1/(1 - 2)1/2 the individual autoregressive processes which generate the y's are stationary, and so forth.
But, although convenient, it is not very reasonable to suppose the initial obser-
vation to be generated by a mechanism much different than that which generates
subsequent observations. Bhargava and Sargan (1983) suggest

yi0 = k0 + xi0 + k1i + k2i0 ,

(1.17)

where the xi0 are exogenous variables, possibly different from xi0 but quite possibly correlated with subsequent observed xit 's and where  may or may not equal  . This formulation obviously encompasses the stricter assumption that the same mechanism generates yi0 and subsequent yit 's and allows the exogenous variables themselves to be generated by other independent dynamical systems.
Assuming fixed effects in a dynamic framework and estimating them as if
they were constants (or eliminating them by taking deviations from individual means) together with the autoregressive coefficient  leads to inconsistent estimates of the latter. This was noted in Nerlove (1971), Nickell (1981) and proved by Sevestre and Trognon (1985). Although yi,t-1 and it are uncorrelated, their

12

M. Nerlove et al.

respective individual means are correlated with each other, with it and with yi,t-1. Instrumental variable methods have been proposed to get around this problem (e.g., Balestra and Nerlove, 1966), but as shown in Nerlove (1971), they can result in very erratic estimates if the instruments themselves have relatively low explanatory value.
Conditioning on the initial values of the endogenous variable also leads to troublesome problems. As noted in Nerlove (1971), the estimates of  appear to be inconsistent even when an error components model is assumed and 2 and 2 are estimated together with other parameters of the model. This was proved in Trognon (1978). Bhargava and Sargan (1983) show that this does not happen when the likelihood function is unconditional, i.e., when it takes into account the density function of the first observation, e.g. as determined by (1.17) and assumptions about the k's, , and the densities of i and i0. Our opinion on this matter is that it is most plausible and appropriate to assume that the mechanism which generates the initial observation is highly similar, if not identical, to that which generates subsequent observations. If observations on past values of the exogenous variables are not generally available, it would be preferable to model their independent determination rather than to assume their joint effect, xi0, to be fixed constants. At least, such an approach would be more consistent with Haavelmo's views as quoted above.
When the solution to the likelihood equations is not on a boundary and when the likelihood function is locally concave at such a solution, the solution with the largest value is consistent, asymptotically efficient, and root-N asymptotically normally distributed with variance-covariance matrix equal the inverse information matrix. Provided the marginal distribution of the initial values yi0, can be correctly specified, the unconditional density of yiT , . . . , yi0, conditional only on the values of observed exogenous variables, gives rise to a likelihood function which has an interior maximum with probability one. If the marginal density of the initial values is misspecified, ML estimates are no longer consistent.
It is not, in fact, difficult to obtain the unconditional likelihood function once the marginal distribution of the initial values is specified. The problem is a correct specification of this distribution. Suppose that the dynamic relationship to be estimated is stationary so that || < 1. Consider (1.14) for yi0 and the infinite past:

 yi0

=



j=1

j xi- j

+

1 1-

i

+ vi0

where

vit =  vit-1 + it .

(1.18)

(Recall that all variables are expressed as deviations from their overall means). If

 = 0, so that the relationship to be estimated is a pure autoregression, the vector

of initial values y0 = (y10, . . . , yN0) has a joint normal distribution with means 0

and

variance­covariance

matrix

(

2 (1-

)2

+

2 (1-

2)

)IN

.

The

unconditional

likelihood

is therefore

1 Introduction

13

log L(, 2, 2|y11, . . . , yNT ; y10, . . . , yN0)

= - NT log(2) - NT log( 2) - N log( ) - N(T - 1) log()

2

2

2

2

  -

1 2

2

i

t

yit -  yit-1

2 - N log 2

2 (1 - )2

+

2 1-2

-1 2

(1

2 - )2

+

2 1-2

 yi20. i

(1.19)

where yit is obtained by applying the standard GLS transformation to yit . To maximize, express 2, 2,  and  in terms of . For given  in the interval [0, 1), concentrate the likelihood function with respect to  2 and . This is a little more complicated than the usual minimization of the sum of squares in the penultimate term because  enters the final term as well. Then do a gradient search on .
When  = 0, things are more complicated still. Various alternative specifications considered in the literature are reported and analyzed in Chap. 8.2 Considerable simplification, however, can be obtained if, following Nerlove (1971), we are willing to assume that xit follows a well-specified common stationary time series model for all individuals i.
With these general principles in mind, we now turn to a review of other methodological developments that are considered in this volume. Indeed, since the early work of Mundlak (1961) and Balestra and Nerlove (1966), panel or longitudinal data have become increasingly important in econometrics, and methods for the analysis of such data have generated a vast literature much of which has been summarized in the first two editions of this volume. In the last ten years there has been an extraordinary further growth, captured here in eleven completely new chapters and fifteen significantly revised chapters which appeared in the earlier editions.

1.4 A Brief Review of Other Methodological Developments
The most common model for the analysis of panel data is the linear model in which explanatory variables are taken to be exogenous, that is independent of the disturbances in the equation or, in the case of the random coefficients model, of the distributions of the coefficients. When the coefficients (except for the constant term) in the linear relationship with which we describe the data are assumed to be constant, it is usual to distinguish between fixed effects and error components models.
2 One interesting possibility discussed there is to choose yi0 a linear function of some observed individual specific time invariant exogenous variables and a disturbance which is decomposed as the sum of the individual specific disturbances i and a remainder. The first-order equations for maximizing the likelihood then take on a simple recursive form when  = 0, and permit other simplification when  = 0. But if we knew some individual specific time invariant observed variable influenced behavior why not incorporate them directly in the equation to be estimated?

14

M. Nerlove et al.

In the case of the former, the intercepts are assumed to vary across individuals at the same point in time and, possibly, over time for all individuals taken together. In the case of the latter, the variations are assumed to be random and uncorrelated both with the observed explanatory variables and the latent disturbance in the equation.
A considerable quantity of interesting mathematics has been developed for both types of models. A number of different projection matrices exist, which take deviations between the raw observations and various means, across individuals, across time periods, over all, and of various means from other means. These projections can be used to define different possible estimators in fixed effects models or the spectral decomposition of the disturbance variance­covariance matrix in the case of error components models. A principal result is then the demonstration, first noted by Maddala (1971), that the Generalized Least Squares (GLS) estimators of the slope parameters in the error components case are a weighted combination of estimators in the fixed effects case (the so-called "between" and "within" distinction among possible estimators). See Chaps. 2 and 3.
An important distinction is made between fully asymptotic theory in which the limiting properties of estimators are analysed when both the number of time periods and the number of individuals goes to infinity and semi-asymptotic theory in which the number of individuals (or the number of time observations) is assumed to increase without bound, that is, asymptotics in only one of two dimensions. Clearly, in the case of random effects models, the moments of the distribution of the effect whose dimension is not increased in the calculation cannot be semi-asymptotically consistently estimated.
As long as the model is not dynamic, that is, does not contain a distributed lag, lagged values of the dependent variable, or the equivalent stock or state variable, the GLS estimators of these coefficients have the usual good small sample and asymptotic properties, The problem, then, is that the elements of the disturbance variance­covariance matrix are unknown. Since consistency of the variance components estimates depends on the asymptotics assumed, the usual justification for a two-stage procedure (feasible GLS or FGLS) based on first-stage consistent estimates of the variances and covariances of the panel model disturbances does not clearly apply. Indeed, in some cases the FGLS may not even be consistent.
Various interesting extensions of both the fixed effects and error components linear models have recently been made and are presented in this volume. They deal with:
(a) random coefficient models (Chap. 6) and spatial models (Chap. 19). These are important and rapidly expanding fields. Indeed, allowing behaviors to vary randomly across individuals can be an attractive way to account for heterogeneity. Also, the existence of spatial dependence should clearly not be ignored when dealing with regional or industry-level data where assuming the "individuals" to behave independently from each other is clearly a strong assumption;
(b) linear models with random regressors and the Chamberlain (1984) approach (Chaps. 4 and 5). As emphasized in the previous section, it is indeed heroic to assume the absence of correlation between the individual effects and the regressors,

1 Introduction

15

in particular. Instrumental variables estimators may then be a useful tool in such a context. The chapter devoted to the Chamberlain approach shows that one important advantage of this approach is to permit a unified treatment of both fixed effects and random effects models in such a context.
(c) data with measurement errors and simultaneous equation models (Chap. 10). Inconsistencies resulting from the simultaneity of individuals'decisions are quite well-known and the treatment of such an issue does not need any long justification. Griliches (1986) persuasively argues the need to understand and model the processes generating errors in economic data in the estimation of economic relations. Griliches and Hausman (1986) provide a pioneering application to panel data. Moreover, problems associated with measurement errors are more important than they might seem at first, because of the increasing importance of so-called "pseudo panel" data (Chap. 11) and the application of measurement error models to the analysis of such data as if they were true panel data. For many types of problems true panel data are not available, but rather several cross sections at different points in time are. For example, surveys of consumer expenditures based on a sample of individual households are made every few years in the UK or the US. Surveys to determine unemployment and labor force participation are made monthly on the basis of a rotating sample. Pseudo panel methods for treating such data are described in Chap. 11. These methods go back to Deaton (1985) who proposed dividing the sample into "cohorts" sharing common demographic, socio-economic, or historical characteristics, then treating the "cohort" averages as observations on "representative" individuals in a panel. Because each "cohort" observation is based on a sample of the true population cohort, the averages, treated as observations, contain sampling errors. Thus, Deaton proposed that the observations be considered as measurements of the "true" values with errors.
What should we make of this approach from the standpoint of the fundamental issues of history and dynamics? It goes without saying that we want to make use of whatever data is available in an appropriate way. The question is what do the cohort averages mean and how should relationships among them be interpreted? Deaton's cohorts and his proposed treatment of cohort averages is similar to the notion of a representative economic agent, introduced by Alfred Marshall in the last century, and in widespread theoretical use today. Kirman (1992) has given a detailed critique of the concept and many of his points apply in the present context. Essentially, relationships among averages, or for representative individuals, are often not interpretable directly in terms of individual behavior since the relationships among the aggregates is often a result of the aggregation. Another way of saying the same thing is that the aggregate relationships are reduced forms from which the underlying structural relations (at the individual level) will not generally be identifiable. This is particularly the case when differences among individuals are historical to a significant degree and when the relationships of interest are dynamic. To the extent that the cohort-defining variables succeed in classifying individuals together who share common histories and exhibit common forms of (dynamic) behavior, the use of pseudo panel data as if they were true panel data subject to sampling error will be successful. But to the extent that unobserved heterogeneity in either respect remains,

16

M. Nerlove et al.

the relationships obtained from pseudo panel data may not permit identification of the underlying structure of interest.
(d) dynamic models are considered in Chaps. 8 and 9. As we have argued above, most relationships of interest are likely to be dynamic and the past histories of individuals are almost always important determinants of current behavior. While Chap. 8 considers the case where the number of periods of observation is finite, Chap. 9 considers the situation where this number of periods can be seen as large enough to consider a T -asymptotics; stationarity of the DGP and the existence of cointegation relationships between variables have to be considered. Let us go back for a while to the former context (finite T ). The GLS estimates in an error components setting are obtained by transforming the observations to weighted sums of Between and Within variances and covariances, using appropriate weights based on the two distinct characteristic roots of the variance­covariance matrix of the residuals uit = i + it . The covariance matrix of the disturbances can be expressed as

 2 =  2((IN  JT ) + (1 - )INT }) =  2((IN  JT ) + (1 - )(I N  IT )) =  2(IN  (JT + (1 - )IT )) .

(1.20)

where  2 = 2 + 2,  = 2/ 2 and its distinct roots are  = (1 - ) + T  and

 = (1 - ). Applying this transformation to the dynamic error components spec-

ification

in

(1.13)

and

replacing

 

=

1 2

=

,

the

normal

equations

to

be

solved

for

the GLS estimates become:

Wyx +  Byx Wyy-1 +  Byy-1

=

Wxx +  Bxx

Wx,y-1 +  Bx,y-1

Wy-1,x +  By-1,x Wy-1,y-1 +  By-1,y-1

.

(1.21)

In this case, the calculated RSS/NT estimates not  2 but  2. As Maddala

(1971)

points

out,

the

GLS

estimates

with



=

1 2

can

be

considered

members

of

a

more general class of estimators obtained through different choices of  . Let ( )

be the estimator of  obtained by solving the above equations for an arbitrary value

of  . Sevestre and Trognon (1985) show that for the case in which  = 0, the purely

autoregressive case, the following inequality holds:

(0)    ( 2)  (1)  () i.e., Within    GLS  OLS  Between .

(1.22)

Remarkably, therefore, the GLS estimate is inconsistent in this case. The problem
is that the lagged dependent variable is correlated even with the transformed disturbance. Since ( ) is a continuous function of  , there exists a value   in the interval [0,  2] for which ( ) = . In an earlier paper, Sevestre and Trognon (1983) have derived this value. They also show that when  = 0, the estimate ( ) behaves almost the same as in the purely autoregressive case. Since the   estimate is con-
sistent when there are no exogenous variables, it remains so when there are. The trick is to obtain a consistent estimate of   which can be accomplished by finding

1 Introduction

17

an appropriate instrumental variable for y-1. Even in this case the results depend heavily on the distribution of the estimate of  .
In the dynamic error components model, not only are the OLS pooled regression estimates, the fixed effects or Within estimates, and the Between estimates inconsistent, but so are the GLS estimates using the true value of . However, the method of instrumental variables may be used to obtain a feasible member of the  -class of estimates which is consistent. Unfortunately, this estimate may have a very large variance. The method of choice in most cases is Maximum Likelihood (ML), provided, of course, that associated computational difficulties can be resolved. But even when the matrix of observed regressors is assumed to be nonstochastic, the properties of ML estimators may no longer be fully optimal asymptotically. Although consistent ML estimates of the coefficients of observed exogenous and of the nonspecific residual variance can be obtained either in the asymptotic or the semi-asymptotic sense, consistent ML estimates of the individual specific residual variance cannot be obtained except in the semi-asymptotic sense. In the dynamic case, however, maximum likelihood based on the likelihood function conditional on the initial observation, or more generally the state, can yield inconsistent estimates (Trognon, 1978).
Other developments covered by specific chapters in this third edition consist of the semi-parametric and non-parametric methods that can be used for analyzing panel data (Chap. 14), the Bayesian approach to panel data analysis (Chap. 15), and the question of the poolability of individuals in a panel (Chap. 16).
Chapters 7, 12, 13, 17 and 18 deal with latent variables and other forms of nonlinear models in a panel data context. Two points are worth making in this respect: First, it is frequently more difficult to see how elements of individual heterogeneity should be introduced, in contrast to the simple way in which such heterogeneity is introduced in equations (whether linear or not) in terms of disturbances. In these non-linear models, even in the case in which all the explanatory variables are truly exogenous, failure to take account of heterogeneity may result in bias, not merely inefficiency, whereas no bias results in the linear case.
The solution in principle is to formulate a model in terms of the probability of individual observations and then to "integrate out" the heterogeneity factors if these can be parametrically specified. In practice, of course, this is rarely possible analytically and may even be extremely difficult computationally. Methods of simulated moments (see McFadden, 1989, and Chap. 13, below) are of considerable utility in this connection.
An important application of latent variables models (which are largely highly nonlinear) is to selection bias and incompleteness in panel data (Chap. 12). In the case of selection bias, a rule other than simple random sampling determines how sampling from the underlying population takes place. Ignoring the nature of the selectivity mechanism may seriously distort the relationship obtained with respect to the true underlying structure. Heckman (e.g., 1990, and references cited therein) has pioneered in this analysis. The greatest problem in panel data in this connection is attrition (sometimes resolved through partial rotation which has its own problems). The probability of nonresponse increases when the same individual is repeatedly sampled. In Chap. 12, it is shown that the crucial question is whether the observed

18

M. Nerlove et al.

values in the sample can be considered as the result of a simple random drawing or whether, on the contrary, they are "selected" by some other rule, random or not. In the case of simple random selection, standard estimation and inference are appropriate, and we say the selection rule is ignorable. On the other hand, if selection is nonrandom with respect to factors reflecting heterogeneity, that is correlated with them, standard techniques yield biased estimates and inferences. In this case the selection rule must be explicitly modelled to correct for selection biases. The authors of Chap. 12 show how this can be done for both and for random effects models. Because consistent estimation in the case of a non-ignorable selection rule is much more complicated than in the ignorable case, several tests are proposed to check whether the selection rule is ignorable.
There are other key methodological chapters in this third edition. These include chapters on the use of simulation techniques for estimating panel models (Chap. 13), on the Generalized Method of Moments for count variable models (Chap. 18) and a long chapter on duration models and point processes (Chap. 17).
Finally, Part III of this third edition contains a number of surveys about possible applications of the above methods. Applications of panel data are very diverse, depending, of course, on the availability of such data in specific substantive contexts. This volume contains new chapters on foreign direct investment (Chap. 20), linked employer­employee data (Chap. 22) and policy analysis (Chap. 24). This third part also contains revised versions of previously published chapters about production frontiers and productive efficiency (Chap. 21), labor supply (Chap. 23), labor market transitions (Chap. 25) and a fully updated version of the software review (Chap. 26). In addition to surveying important substantive areas of research these chapters are particularly useful in illustrating our message.
Obviously, panel data (or pseudo panel data) are essential if we want to estimate dynamic relationships at an individual or disaggregated level. As soon as the focus is on dynamics, historically generated heterogeneity becomes a central issue. Models of factor demand (labor and capital investment) reveal the crucial role of expectations. In this connection it is interesting to note the special impact of heterogeneity on expectations. Panel data provide a unique opportunity to study expectation formation and to test various hypotheses about expectation formation (See e.g., Nerlove, 1983, Nerlove and Schuermann, 1995). Often, however, panel data do not contain direct observations on expectations but, as is typically the case with time series data, only on other variables affected by expectations. In this case, we formulate a model of expectation formulation and infer indirectly the parameters of both the behavioral and the expectational model. To see how heterogeneity plays a critical role, it is useful to consider two simple examples: adaptive expectations and rational expectations.
Suppose that the model we wish to estimate is

yit = xit + uit i = 1, . . . , N; t = 1, . . . , T ,

(1.23)

where expectations are adaptative:

xit =  xi,t-1 + (1 -  )xi,t-1 + vit .

(1.24)

1 Introduction

19

Even if the disturbances in the behavioral equation (1.23) are i.i.d. random variables,

it is unlikely that past history and past experience will play no latent role in the
determination of current expectations, not fully taken into account by xi,t-1. Thus, write

vit = i + it ,

(1.25)

where the individual specific effects are likely to be correlated with past xit 's and also, presumptively, with past uit 's and xit 's. The usual transformation of (1.23) and (1.24) then yields

yit =  yi,t-1 + (1 -  )xi,t-1 +  i + it + uit -  ui,t-1 .

(1.26)

Not only do the usual difficulties, discussed above, arise because of the correlation

between yi,t-1 and i, but the third term of the disturbance is serially correlated. Moreover, if the individual specific disturbances, i, are correlated with past xit 's,

the lagged values of these will no longer serve as instruments.

Still more interesting things happen in the case of rational expectations. In this

case (1.24) is replaced by

xit = E(xit | i,t-1) ,

(1.27)

where i,t-1 is the set of information available to the i-th individual at the time when his expectations are formed. In principle, i,t-1 not only contains that individual's own past history, but also observations on aggregates of individuals, and
may include knowledge of the way in which individual decisions interact to produce
aggregates. For example, supppose

Then, for the i-th individual,

N
 zt = yit . i=1

(1.28)

i,t-1 = {yi,t-1, . . . ; xi,t-1 . . . ; zt-1, . . .} .

(1.29)

Rational expectations imply

yit = E(xit | i,t-1) + uit = E(xit | yi,t-1, . . . ; xi,t-1, . . .) + uit .

(1.30)

Now if the value of xit faced by each individual is a function, peculiar to that indi-

vidual, of zt :

xit = fi(zt ) ,

(1.31)

which may also be stochastic, then

E(xit | i,t-1) = E( fi(zt ) | i,t-1) .

(1.32)

So, for example, if

xit = zt + it ,

(1.33)

20

M. Nerlove et al.

then

E(xit | i,t-1) = E(zt | i,t-1) + E(it | i,t-1) .

(1.34)

The last term on the right hand side of (1.34) will not generally be zero. Suppose it is. Such a simplification does not essentially affect the nature of the difficulties involved. Then

yit = E(zt | i,t-1)+uit = E Hence, if uit = i + it ,

N
 yit | i,t-1
i=1

N
 +uit =  E(yit | i,t-1)+uit . i=1
(1.35)

N

N

  E(yit | i,t-1) =  {E(yit | i,t-1)} + E(i | i,t-1), i = 1, . . . , N .

i=1

i=1

(1.36)

Equation (1.36) are N equations for each t, which, in principle, can be solved for the

N values

E(yit | i,t-1)

in terms of the contents of i,t-1 for all N individuals and the sum of expectations

N
 t-1 = E(i | i,t-1) . i=1

In general

E(yit | i,t-1) = a1gi(i,t-1) + a2t-1 .

Then we can replace the left hand side of (1.34) by

So (1.23) becomes

N
 xit = a1 gi(i,t-1) + a2Nt-1 . i=1

(1.37)

N
 yit = a1 gi(i,t-1) + a2Nt-1 + uit . i=1

(1.38)

It follows that the appropriate equation now contains a specific time-varying, individual-nonspecific, effect in addition to i and it. This effect is correlated with the element in i,t-1 since it is an expectation conditional on i,t-1. Finally, it can be seen that the parameters of gi, a1, and a2 and  are not generally separately identifiable. The bottom line is that, if one believes in rational expectations, one is
in deep trouble dealing with panel data. Unless future values of the exogenous variables are in the information set i,t-1
when expectations are formed, all of the applications discussed in Part III have this
problem.

1 Introduction

21

1.5 Conclusion

In this introductory chapter we have tried to bring out the following points:
(a) One of the main reasons for being interested in panel data is the unique possibility of uncovering disaggregate dynamic relationships using such data sets.
(b) In a dynamic context, one of the primary reasons for heterogeneity among individuals is the different history which each has.
(c) If the relevant "population" is, following Haavelmo, the space of possible decisions, different past histories take the form of individual specific random variables which are generally correlated with all of the variables taken as explanatory, not just the lagged values of the endogenous variable. The former therefore cannot be conditioned upon in the usual way.
(d) Finally, although the adaptive expectations model does not introduce any new complications, rational expectations introduce a time specific, individual nonspecific, component in the error component formulation, as well as a fundamental failure of identifiability.
Panel data econometrics is one of the most exciting fields of inquiry in econometrics today. Many interesting and important problems remain to be solved, general as well as specific to particular applications. We hope that this volume is the place to start.

References
Anderson, T. W., and C. Hsiao (1982) Formulation and Estimation of Dynamic Models Using Panel Data, Journal of Econometrics, 18, 47­82.
Balestra, P., and M. Nerlove (1966) Pooling Cross-Section and Time-Series Data in the Estimation of a Dynamic Economic Model: The Demand for Natural Gas, Econometrica, 34, 585­612.
Bhargava, A., and D. Sargan (1983) Estimating Dynamic Random-Effects Models from Panel Data Covering Short Time Periods, Econometrica, 51, 1635­1659.
Chamberlain, G. (1984) "Panel Data." Pp. 1247­1318 in Z. Griliches and M. Intriligator, eds. Handbook of Econometrics, Vol. 2. New York: Elsevier.
Davidson, R., and J.G. MacKinnon (2004) Econometric Theory and Methods. New York: Oxford University Press.
de Finetti, B. (1930) Problemi Determinati e Indeterminati nel Calculo delle Probabilita`, Rend. R. Acc. Naz. Lincei, Series 6, Vol. 12, fasc. 9.
Deaton, A. (1985) Panel Data from a Time Series of Cross-Sections, Journal of Econometrics, 30, 109­126.
Engle, R.F., D.F. Hendry, and J.-F. Richard (1983). Exogeneity. Econometrica, 51, 277­304. Griliches, Z. (1986) "Economic Data Issues." Pp. 1465­1514 in Z. Griliches and M. Intriligator,
eds. Handbook of Econometrics, Vol. 3. New York: Elsevier. Griliches, Z., and J.A. Hausman (1986) Errors in Variables in Panel Data, Journal of Econometrics,
31, 93­118. Haavelmo, T. (1944) The Probability Approach in Econometrics; Supplement to Economet-
rica, 12.

22

M. Nerlove et al.

Heckman, J.J. (1990) Varieties of Selection Bias, American Economic Review, Papers and Proceedings, 80, 313­318.
Heckman, J. (1991) Identifying the Hand of Past: Distinguishing State Dependence from Heterogeneity, The American Economic Review, 81, 75­79.
Hsiao, C. (2007) Panel Data Analysis. Advantages and Challenges, TEST, 16, 1­22. Kirman, A. (1992) Whom or What Does the Representative Individual Represent? Journal of Eco-
nomic Perspectives, 6, 117­136. Knight, F.H. (1921) Risk, Uncertainty and Profit. Boston: Houghton Mifflin Co. Maddala, G.S. (1971) The Use of Variance Components Models in Pooling Cross-Section and
Time Series Data, Econometrica, 39, 341­358. Marschak, J. (1950) "Statistical Inference in Economics: An Introduction". Pp. 1­50
T.C. Koopmans, ed. in Statistical Inference in Dynamic Economic Model. Koopmans. New York: John Wiley. Marschak, J. (1953) "Economic Measurements for Policy and Prediction." Pp. 1­26 in W.C. Hood and T.P. Koopmans, Studies in Econometric Method ed.. New York: John Wiley. Ma´tya´s, L., and S. Rahman (1992) An Alternative Approach for the Estimation of Distributed Lag Models in Panel Data, Working Paper No. 4/92, Department of Econometrics, Monash University. McFadden, D.L. (1989) A Method of Simulated Moments for Estimation of Discrete Response Models without Numerical Integration, Econometrica, 57, 995­1026. Mundlak, Y. (1961) Empirical Production Functions Free of Management Bias, Journal of Farm Economics, 43, 44­56. Nerlove, M. (1971) Further Evidence on the Estimation of Dynamic Economic Relations from a Time Series of Cross-Sections, Econometria, 39, 359­382. Nerlove, M. (1983) Expectations, Plans and Realizations in Theory and Practice, Econometrica, 51, 1251­1280. Nerlove, M., and T. Schuermann (1995) "Expectations: Are they Rational, Adaptive, or Naive? An Essay in Simulation-Vased Inference." Pp. 354­381 in G.S. Maddala, P. Phillips and T.N. Srinivasan, eds. Advances in Econometrics and Quantitative Economics. Oxford: Basil Blackwell. Neyman, J., and E. Scott (1948) Consistent Estimates Based on Partially Consistent Observations. Econometrica, 16, 1­32. Nickell, S. (1981) Biases in Dynamic Models with Fixed Effects, Econometrica, 49, 1417­1426. Sevestre, P., and A. Trognon (1985) A Note on Autoregressive Error-Components Models, Journal of Econometrics, 29, 231­245. Trognon, A. (1978) Miscellaneous Asymptotic Properties of OLS and ML Estimators in Dynamic Error Components Models, Annales de l'INSEE, 30­31, 631­658. Wooldridge, J.M. (2002) Econometric Analysis of Cross Section and Panel Data, 1st ed. Cambridge: MIT Press.

Chapter 2
Fixed Effects Models and Fixed Coefficients Models
Pietro Balestra and Jayalakshmi Krishnakumar

As noted in the introductory chapter, the simplest and most intuitive way to account for individual and/or time differences in behaviour, in the context of a panel data regression problem, is to assume that some of the regression coefficients are allowed to vary across individuals and/or through time. The regression coefficients are unknown, but fixed parameters. When these are allowed to vary in one or two dimensions, we speak of a fixed effects model (or fixed coefficients model).
It is useful, in this context, to distinguish between two types of regression coefficients: the intercept and the slope parameters. When only variations in the intercept are considered, the resulting regression problem is called a covariance model (or dummy variable model). Among the early proponents of such models in economics, one can cite Mundlak (1961), Hoch (1962), Kuh (1963) and Nerlove (1965). This model is discussed at length in Sect. 2.1 (for the case in which only individual variations occur) and in Sect. 2.2 (for the case in which both individual and time variations appear). The hypothesis of spherical disturbances which is typically maintained in such models is abandoned in Sect. 2.3, where different variance­ covariance structure of the residuals are considered. Particular attention is paid to the problems of serial correlation and heteroscedasticity. Finally, some extensions (including variations of the slope parameters) are taken up in the last Section.

Pietro Balestra Faculty of Economics, University of Lugano, via Giuseppe Buffi 13, CH-6904 Lugano, Switzerland
Jayalakshmi Krishnakumar Department of Econometrics, University of Geneva, 40 Bd. Du pont d'Ave, CH-1211 Geneva 4, Switzerland, e-mail: jaya.krishnakumar@metri.unige.ch
Jaya Krishnakumar would like to dedicate this chapter to the memory of her dear teacher, colleague and friend Pietro Balestra who is no more. She feels really privileged to be the co-author of his last work.

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

23

c Springer-Verlag Berlin Heidelberg 2008

24

P. Balestra and J. Krishnakumar

2.1 The Covariance Model: Individual Effects Only

2.1.1 Specification

In this model, the intercept is allowed to vary from individual to individual, while the slope parameters are assumed to be constant in both the individual and time dimensions.
Consider, by contrast, the case in which all the parameters, including the intercept, are constant. Given a panel sample of N individuals over T periods, the basic linear regression equation takes the form:

yit = 0 + 1x1it + . . . + KxKit + it , i = 1, . . . , N; t = 1, . . . , T = 0 + xit  + it

(2.1)

where yit is the observation on the dependent variable (for individual i at time t), xit is the K row vector of the explanatory variables, it is a non-observable random term,  is the K column vector of the slope parameters and 0 is the intercept.
When different intercepts are permitted for the N individuals, the model becomes:

yit = i + xit  + it

(2.2)

which is the basic (individual effect only) covariance model. The term covariance model is used with reference to the standard analysis of variance layout, which does not consider explicitly any explanatory variables. When the standard analysis of variance effects are combined with those of explanatory variables, the term covariance model is used.
Let us write down the model for the full sample. First, the T observations for individual i can be expressed conveniently in the following matrix form:

yi = eT i + Xi + i

(2.3)

where yi is the T × 1 vector of the yit , eT is the unit vector of size T , Xi is the T × K matrix whose t-th row is xit and i is the T × 1 vector of errors. Next, stacking the individuals one after the other, we have:



     

y1

eT 0 . . . 0

1

X1

1



y2 ...



=



0

eT . . . 0 ...





2 ...



+



X2 ...

 

+



2 ...



yN

0 0 . . . eT N

XN

N

y
(NT ×1)

DN
(NT ×N)


(N×1)

X
(NT ×K)


(NT ×1)

or more simply:

y = DN + X +  .

(2.4)

2 Fixed Effects Models and Fixed Coefficients Models

25

The matrix DN contains a set of N individual dummies, and has the following Kronecker product representation:

DN = IN  eT .

It can easily be verified that the following properties hold:

(i) DNeN = eN  eT = eNT (exhaustivity)

(ii) DNDN = T IN (orthogonality)

(iii) DN DN = IN  eT eT = IN  JT

(iv)

1 T

DN y

=

[y¯1, . . . , y¯N]

,

1 T

DN X

=

[x1, . . . , xN]

where

y¯i

=

1 T

 yit
t

is

the

individual

mean,

xi

=

1 T

 xit
t

is

the

K×1

vector

of

the

individual means of the explanatory variables and, by definition, JT = eT eT is the

unit matrix of order T .

Expression (2.4) represents the basic covariance model (in the case of indi-

vidual effects only). To complete its specification, we adopt the following set of

assumptions:

A1: The explanatory variables are non-stochastic, independent of the errors, and

such that the NT × (N + K) matrix Z = [DN X] has full column rank; A2: The random terms it are independent, homoscedastic (variance  2) with

zero mean.

Note that assumption A1 implies NT > N + K (which is satisfied, for large N

whenever T  2), but also requires that the columns of X be linearly independent

from those of DN. For this to be the case, the matrices Xi must not contain the con-

stant term (an obvious restriction) nor a column proportional to it (which precludes

any variable, such as years of schooling, that is constant for a given adult individual,

although varying from individual to individual).

2.1.2 Estimation

Given assumptions A1 and A2, the OLS estimators of all the regression coefficients in model (2.4) are BLUE. Collecting the regression coefficients in the vector ,  = [  ], and all the explanatory variables (including the N dummies) in the matrix Z = [DN X], the OLS estimator is ^ = (Z Z)-1Z y. The actual computation of ^ requires the inversion of the (N + K) × (N + K) matrix Z Z, which for large N, is
not an attractive operation. Instead, the technique of partitioned regression can be used, which involves the inversion of just a (K × K) matrix.
Using the results of partitioned regression or equivalently those of the Frisch­
Waugh­Lovell theorem, these estimators (and other relevant statistics) can be ex-
pressed as:

 = (X WN X )-1X WN y

^

=

(DN DN )-1DN (y - X ^ )

=

1 T

DN

(y

-

X

^

)

(2.5) (2.6)

26

P. Balestra and J. Krishnakumar

SS = y WNy - ^ X WNy

(2.7)

^ 2 = SS/(NT - N - K)

(2.8)

V (^ ) =  2(X WNX)-1

V (^ )

=

2 T IN

+

1 T

DNXV (^ )X

DN

1 T

(2.9) (2.10)

where WN

= INT - DN (DN DN )-1DN

=

INT

-

1 T

DN

DN

=

INT

-

IN



1 T

JT

is

an

idempotent matrix of order NT and rank NT - N, also called the within projector (see the Appendix to this chapter).

The estimator given in (2.5) and the corresponding SS in (2.7) can also be ob-

tained by OLS on the transformed model

y = X  +  ,

(2.11)

where y = WNy and X = WNX. The transformation WN is very simple: the transformed variables are simply the original variables expressed as deviations from the individual mean (the details are given is the Appendix). Therefore, the it-th equation corresponding to (2.11) is:

(yit - y¯i) = 1(x1it - x¯1i) + . . . + K(xKit - x¯Ki) + it = (xit - xi ) + it

(2.12)

However, it should be remembered that, when working with transformed variables, the actual number of degrees of freedom is NT - N - K and not NT - K (as the above regression wrongly suggests), since in order to transform the variables, the N individual means must be computed, resulting in the loss of N degrees of freedom. Hence, when using a computer program on the transformed data, the variances given by the program must be adjusted accordingly.
If the transformed model is used, the estimators of the i and their respective variances and covariances can be obtained from (2.6) and (2.10), i.e.:

^ i = yi - xi ^

V (^ i)

=

1 T

2

+

xi

V (^

)xi

Cov(^ i, ^ j) = xi V (^ )x j

There are other ways to eliminate the individual effects, but, when properly handled, they give rise to exactly the same estimator obtained previously, namely ^ .
Take the case of the popular first-difference transformation:

yit = yit - yi,t-1 t = 2, . . . , T

2 Fixed Effects Models and Fixed Coefficients Models

27

or the less known deviation from the mean of the preceding observations:

 yit

=

yit

-

t

1 -

1

t-1
yis
s=1

t = 2, . . . , T

These two examples are special cases of the transformation yi = A yi, y = (IN  A )y, where A is a (T - 1) × T matrix of full rank orthogonal to eT : A eT = 0. For
the two examples, the matrix A takes the following form (respectively):









-1 1

-1 1

A =  . . . -1 1



and

A

=  -1/2

-1/2 ...

1



-1 1

-

T

1 -1

-

T

1 -1

-

T

1 -1

...

1

These transformations seem to offer an advantage, since they do not require any adjustment for degrees of freedom (the actual number of observations on the transformed variables available for estimation being the required (NT - N). However, they introduce serial correlation (as in the first difference transformation) or heteroscedasticity (as in the second example) in the transformed model:

y = X +  V () =  2(IN  A A) =  2V .

Hence OLS estimation is no longer BLUE.
Fortunately, the situation is one in which the pure GLS estimator is applicable (the variance­covariance matrix of  being known up to a scalar multiple). This estimator is given by:

^GLS = (X  V -1X )-1X  V -1y = X (IN  A(A A)-1A )X -1 X (IN  A(A A)-1A )y

In the Appendix it is shown that for any A such that A eT = 0, A(A A)-1A =

IT

-

1 T

eT

eT

,

and consequently IN  A(A A)-1A

= WN. The

pure GLS

estimator

is

thus equal to ^ .

Actually the usual covariance estimator also belongs to this class, with A com-

posed of orthogonal rows (as in the second example above) but with unit length, so that A A = IT-1 and V () =  2IN(T-1). Consequently OLS can be applied directly to the transformed model and no correction for degrees of freedom is required. Such

a matrix A is obviously not unique. But if A is chosen as in the second example

above but with normalized rows, the transformation is given by:

yit =

 t - 1
t

yit

-

t

1 -

1

t-i s=1

yis

t = 2, . . . , T

This particular matrix A is called by Arellano (1995) the backward orthogonal deviations operator. Upon reverting the order of both rows and columns of A , one obtains the forward orthogonal deviations operator defined by:

28

P. Balestra and J. Krishnakumar

 yit =

T -t

1T

T -t+1

yit

-

T

-t

yis
s=t+1

t = 1, . . . , T - 1

2.1.2.1 Consistency
Given the double dimension of panel data, asymptotic behavior can be studied in three different ways:
· Case 1: N fixed, T - ; · Case 2: T fixed, N - ; · Case 3: N, T - .
The appropriate choice depends on the nature of the problem. For instance, if the N individuals refer to geographical region (i.e. the States in the U.S.), Case 1 is clearly indicated. The same would be true if N represents the number of the different industrial sectors in a given economy. However, if the individuals are a random sample from a large population (as is often the case in panel data models, with N large and T quite small), the relevant asymptotic is depicted in Case 2.
In all cases, the covariance estimator of the slope parameters (also called the within estimator) is consistent, under the usual regularity conditions. However, for the intercepts the situation is quite different. When N grows, the number of parameters i to be estimated becomes larger and larger. Therefore, the i can be estimated consistently only when N is fixed and T goes to infinity (Case 1). If, however, the true situation is the one corresponding to Case 2, the consistency problem of the individual effects can be circumvented by assuming that they are random variables rather than fixed parameters (see the chapter on error components models).

2.1.3 Inference
Under the normality assumption, the usual t-tests and F-tests can be performed. In particular, if one wishes to test the hypothesis i =  j (for some i and j, i = j) against the alternative i =  j, the quantity
(i -  j)/ V (i -  j)
is distributed, under the null hypothesis, as a t-variable with NT - N - K degrees of freedom.
An interesting question can be asked: are there any individual effects at all? The null hypothesis (of no individual effects) is in this case
1 = 2 = . . . = N = 0
and the corresponding model (called the constrained model) is (2.1) with NT - (K + 1) degrees of freedom. Let SSc be the sum of squared residuals of this

2 Fixed Effects Models and Fixed Coefficients Models

29

constrained model. The unconstrained model is (2.4) or, equivalently (2.11), with NT - N - K degrees of freedom (and SS the corresponding sum of squares). Then, the quantity
(SSc - SS)/(N - 1) SS/(NT - N - K)
is distributed as an F-variable with (N - 1) and (NT - N - K) degrees of freedom. An F-test is therefore appropriate. The number of degrees of freedom in the numerator above is N - 1 and not N, since testing that the N coefficients i are all equal is the same as testing that the N - 1 differences i+1 - i, i = 1, . . . , N - 1, are all zero.

2.2 The Covariance Model: Individual and Time Effects

2.2.1 Time Effects Only

The treatment of time effects is analogous to that of individual effects. It would actually be the same if the observations were ordered by time first and then by individuals. However, it should be kept in mind that we maintain here the same ordering of the observations as in the preceding section.
Briefly stated, the time effect model is

yit = t + xit  + it ,

(2.13)

where to avoid confusion (and to permit the combination of time effects with individual effects) we use the symbol  to designate a time varying intercept. For the full sample, the model becomes:

y = DT  + X + 

(2.14)

where DT is a NT × T matrix of time dummies and  is the T × 1 vector of varying intercepts. In this case, the transformed model is a deviation from the time-means model (with NT - T - K degrees of freedom), i.e.:

(yit - y¯t ) = (xit - x¯t ) + it

(2.15)

where y¯t =

1 N

N


yit

(the time-mean). Estimation and inference proceed as in the case

i=1

of individual effects.

Written in full, the set of time dummies is:



IT

DT

=



IT ..

.



=

eN

 IT

IT

30

P. Balestra and J. Krishnakumar

with the following properties:

(i) DT eT = eN  eT = eNT

(ii) DT DT = NIT

(iii) DT DT = eN eN  IT = JN  IT

(iv)

1 N

DT

y

=

[y¯1, . . . , y¯T ]

.

Finally, the deviation from the time-means matrix, called WT , is:

WT

=

INT

- DT (DT DT )-1DT

=

INT

-

1 N DT DT

=

INT

-

1 N (JN

 IT ) .

2.2.2 Time and Individual Effects

It would seem natural, in order to allow for generic individual and time effects,
to include in the regression equation a full set of individual dummies and a full
set of time dummies. However, this way of proceeding raises an identification problem. Not all the N coefficients i and the T coefficients t are identifiable, since the columns of the matrix [DN DT ] are perfectly collinear (the sum of the first columns, DNeN = eNT , is exactly equal to the sum of the last T columns, DT eT = eNT ).
It can be shown that the rank of [DN DT ] is exactly equal to N + T - 1. Therefore, for identification, one column must be disregarded. This can be done arbitrarily, without affecting the result. Any set of (N + T - 1) linearly independent combinations of the columns of [DNDT ] will do. However, for the sake of symmetry, we prefer to delete one individual dummy (say the last) and one time dummy (again the last) and add the overall constant. Calling DN-1 and DT-1 the sets of N - 1, and T - 1 dummies respectively and  and  the associated vectors of coefficients, the saturated individual and time effects covariance model can be written as:

y = eNT c + DN-1 + DT -1 + X  +  .

(2.16)

The value taken by the intercept for observation (i,t) can be easily read from the Table 2.1:
For the OLS estimation of (2.16), the Assumption A1 of the preceding section must be amended in the sense that now the matrix

[eNT DN-1 DT -1 X ]

Table 2.1 Values of the intercept

di td
t<T t=T

i<N
c + i + t c + i

i=N
c + t c

2 Fixed Effects Models and Fixed Coefficients Models

31

must be of full column rank (equal to N + T - 1 + K). For this to be the case, the matrix X must not (as before) contain variables that assume constant values for each individual, nor (and this is peculiar to time effect models) admit variables that take a constant value for each time period (like, for instance, a price variable in a demand equation which is the same, at time t, for all individuals). Assumption A2 in obviously maintained.
Collecting all the dummies (including the overall constant) in the matrix D = [eNT DN-1 DT-1] and calling  = [c  ] the vector of associated parameters, (2.16) can be put in the following compact way:

y = D + X + 

(2.17)

which is formally analogous to (2.4) of the individual effect model (with just D and  replacing DN and ). Using again the properties of partitioned regression, the following results hold:

^ = (X WNT X )-1X WNT y ^ = (D -1D (y - X^ )) SS = y WNT y - ^ X WNT y ^ 2 = SS/(NT - N - T + 1 - K) V (^ ) =  2(X WNT X )-1 V (^) =  2(D D)-1 + (D D)-1D XV (^ )X D(D D)-1

(2.18) (2.19) (2.20) (2.21) (2.22) (2.23)

where WNT = INT - D(D D)-1D is an idempotent matrix of order NT and rank NT - N - T + 1, also called the within projector (for both individual and time effects).
Alternatively, one can apply OLS to the model transformed by WNT , i.e.

y = X  + 

(2.24)

where y = WNT y and X = WNT X. What does this transformation represent? Can it be given as easy an interpretation as in the simple individual effects model? The answer is yes, but the algebra involved is somewhat complicated and better left for the Appendix. There it is shown that WNT is composed of four separate transformations:

WNT

=

INT

- IN



1 T

JT

-

1 N JN

 IT

+

1 NT

JNT

(2.25)

the first being the identity transformation, the second the individual-mean transformation, the third the time-mean transformation and the last the overall-mean transformation. Therefore, the within transformation consists in subtracting from the original variables both the individual and time-means and in adding the overallmean. The transformed model, accordingly, has the following simple expression:

(yit - y¯i - y¯t + y¯) = (xit - x¯i - x¯t + x¯it)

(2.26)

32

P. Balestra and J. Krishnakumar

If the main interest lies in the slope parameters, then the transformed model should be used. It involves the inversion of a K × K matrix, while the dummy variable specification requires the inversion of a matrix of order N + T - 1 + K. Remember, however, that the exact number of degrees of freedom is NT - N - T + 1 - K and not just NT - K (requiring an adjustment for degrees of freedom when a regression package is used on the transformed data). From the transformed model, using (2.19) and (2.23), one can retrieve the coefficients c,  and , as shown below. But the computational cost may be too high. If one needs to estimate the intercept parameters (and their variances), it might be wise to use dummy variables.
For the sake of completeness (omitting the tedious details), the estimators of the various intercept parameters are given hereafter:
c^ = (yN + yT - y) - (xN + xT - x )^ ^ i = (yi - yN) - (xi - xN)^ i = 1, . . . , N - 1 ^ t = (yt - yT ) - (xt - xT )^ t = 1, . . . , T - 1
These results offer a neat interpretation that sheds some light on the identification problem. The parameter i measures the fixed effect of individual i as deviation from the omitted individual effect; similarly, t measures the fixed effect of period t as deviation from the omitted time dummy; finally, the two omitted individual and time effects are included in the constant c, as deviation from the overall-mean. It is also possible to get exact analytical expressions for all variances and covariances, but, given their limited practical usefulness, they are not reported here.
It should be stressed, as a final point, that all the covariance estimators, given Assumptions A1 and A2, are BLUE. The consistency properties are similar to the ones discussed in Sect. 2.1.2. We thus have:
· Case 1: N fixed, T - : only i's can be estimated consistently; · Case 2: T fixed, N - : only t 's can be estimated consistently; · Case 3: N, T - : a random effects approach is advisable as the number of
parameters tends to infinity (see the next chapter).
The estimators of slope parameters are consistent in all the three cases.

2.2.3 Inference
In the present general covariance model, three interesting hypotheses may be tested: ­ the absence of individual and time effects ( = 0 and  = 0): ­ the absence of time effects ( free and  = 0); ­ the absence of individual effects ( = 0 and  free).
Assuming normality, these are F-tests. Denoting by: ­ SS the sum of squares of the unrestricted covariance model ((2.17) or equivalently
(2.26));

2 Fixed Effects Models and Fixed Coefficients Models

33

­ SSN the sum of squares of the individual dummy variables model ((2.4) or equivalently (2.12));
­ SST the sum of squares of the time dummy model ((2.14) or equivalently (2.15)); ­ SSc the sum of squares of the constant intercept regression model ((2.1));
the appropriate statistics for the three tests are, respectively:

F1

=

(SSc - SS/(NT

SS)/(N + T - 2) -N -T +1-K)

F2

=

(SSN SS/(NT

- SS)/(T - 1) -N-T +1-

K)

F3

=

(SST SS/(NT

- SS)/(N - 1) -N-T +1-

K)

with degrees of freedom appearing, as deflators, in the numerator and denominator. Before ending this section, let us mention that there is no particular problem
regarding prediction in the fixed effect model and it is carried out in the usual manner taking into account the specific effect estimates.

2.3 Non-spherical Disturbances
2.3.1 What Variance­Covariance Stucture?
When the assumption of homoscedastic and independent errors (A2) is abandoned in favour of a more general variance­covariance structure for the residuals, the various estimators presented in the preceding pages are no longer BLUE (although they remain unbiased). Yet efficiency can be achieved by GLS or maximum likelihood techniques.
What is a typical variance­covariance structure in a panel data context? Two different types of correlation must now be considered: serial correlation, as in traditional time series analysis, and correlation among individuals. A general way of looking at this problem (given the adopted ordering of the observations) is to define the variance­covariance structure at the individual level, i.e.
E(i j) = Ai j ,
where Aii is the variance­covariance matrix of the errors of individual i (a positive definite matrix) and Ai j, i = j, is the covariance matrix between the residuals of individual i and individual j. Then, for the full sample, the variance­covariance matrix of  takes the following form:
V () = E(  ) = [Ai j] i, j = 1, . . . , N .
with A ji = Ai j.

34

P. Balestra and J. Krishnakumar

Several special cases are worth noting.
(a) Serially Independent Errors In this case: Ai j = i jIT  V () =   IT ,
where  = [i j] is the (constant) contemporaneous variance­covariance matrix. This specification was adopted by Zellner (1962) in his famous seemingly unrelated regression problem. (b) Individual Independence This case, characterized by

Ai j = 0, i = j  V () = diag (A11, . . . , ANN) ,

covers all types of heteroscedasticity and serial correlation at the individual level. (c) Block-homoscedasticity It is an important special case of individual independence with in addition

Aii = A  i = V () = IN  A .

When A = c1IT + c2eT eT , where c1 and c2 are positive scalars, we have the (individual effect only) error component specification, studied in later chapters. In Sect. 2.3.3 below, we analyze in some details the case of serial correlation. (A being the variance­covariance matrix of a stationary stochastic process). (d) Block-equicorrelation It is defined by

Aii Ai j

= A i = B i=

j

= V () = IN  (A - B) + eNeN  B

with A positive definite, B non negative definite and such that A - B is positive definite. The special situation:

A = (c1 + c3)IT + c2eT eT B = c3IT

(2.27)

(with c1, c2, c3 positive scalars) corresponds to the full (both individual and time effects) error component specification.

2.3.2 Two General Propositions for Fixed Effects Models
Let us now go back to the basic covariance model, which we write as y = L + X + 

(2.28)

2 Fixed Effects Models and Fixed Coefficients Models

35

where L represents a set of fixed effect dummies, either DN for individual effects only), or DT (for time effects only) or D (for both effects) and  is the vector of associated parameters. In such a model, the choice of an appropriate variance­ covariance structure naturally depends on the type of problem considered and very little can be said a priori. However, two general properties can be established, concerning on one hand the relevance of the within transformation and, on the other, the efficiency of the within estimator.
(a) The Within Transformation In the case of spherical disturbances, the BLUE of  can be obtained directly by OLS on the transformed model

y = X  + 

(2.29)

where y = Wy and X = W X with W = I - L(L -1L ). The simplicity of this transformation and its numerical advantages have been pointed out before. Yet, do these benefits carry over in a more general context characterised by a nonspherical variance structure? In other words, is it possible to apply first the within transformation and then use GLS? The answer is contained in the following proposition:
Proposition 2.1. The GLS estimator of  in (2.28) is numerically equivalent to the GLS estimator of  in (2.29) using the same variance-covariance matrix V (), for any observable X and y, if and only if V ()L = LC, for some nonsingular C.
For the proof of this proposition (and Proposition 2.2 below) see Aigner and Balestra (1988, Appendix).
It is easy to verify that the condition of Proposition 2.1 is satisfied in two interesting cases: (i) when L = DN and V () =   IT (since (  IT )DN =   eT = (IN  eT ) = DN) and (ii) when L = DT with a block-equicorrelated covariance structure (since [IN (A-B)+eNeN B]DT = eN (A-B)+NeN  B = eN  (A + (N - 1)B) = DT (A + (N - 1)B)). (b) The Efficiency of the Within Estimator The following proposition holds.
Proposition 2.2. In the presence of fixed effects, the usual within estimator o f  is BLUE if and only if V ()W = W for some positive scalar  .
An important situation in which the condition of this proposition is met occurs when the variance­covariance structure is of the error-component type (see the next chapter). Suppose that only individual effects are considered, i.e., L = DN and V () = IN  A, A = c1IT + c2eT eT . Since W in this case is equal to WN, we successively obtain:

V ()WN

=

IN



A

-

IN



1 T

AeT

eT

=

IN



A(IT

-

1 T

eT

eT

)

=

IN



c1(IT

-

1 T

eT

eT

)

= c1WN

36

P. Balestra and J. Krishnakumar

The reader can easily verify that the same is true when L = D and the variance­ covariance structure is of the general (both individual and time effects) errorcomponent type.
An important conclusion emerges: fixed and random effects are two different alternative ways of considering heterogeneity in behaviour. They cannot be combined. The fixed effect model is particularly appropriate if we are interested in inferring on the behaviour of a specific set of N individuals and especially if the population consists only of these individuals (for example if the `individuals' represent the different regions within a country and the sample covers all of them).

2.3.3 Individual Fixed Effects and Serial Correlation

Serial correlation is a common feature of time series data. How can it be handled in the context of a fixed effect model? Is it possible to recapture some of the computational advantages of the spherical case?
To answer these questions, let us reconsider the individual fixed effect model, (2.4), with serially correlated errors. The simplest possible scheme is an AR(1) process defined by
it = it-1 + uit , |  |< 1
with uit a pure White Noise (with variance  2). Notice, and this is a crucial feature, that the parameters  and  2 are assumed to be the same for all individuals. In such a case, V (i) =  2A, with A a (T × T ) matrix with typical element ast = |t-s|/ (1 - 2). Furthermore, Cov(i,  j) = 0, i = j, and consequently V () =  2(IN  A). It will be recognised that the variance­covariance matrix of this problem is characterised by block-homoscedasticity. Therefore, the estimating procedure outlined below is valid not only for the AR(1) process but also for any ARMA process (with constant parameters across individuals) and indeed for any positive definite matrix A.
In this general setting, the BLUE of  can be obtained in two easy steps. The first step consists in reducing the dimensionality of the problem by eliminating the individual dummies. As shown in Proposition 2.1, the usual within transformation works only under very special conditions, which are not satisfied here. But another easy transformation is available which works in all cases. It suffices to express each variable (both the dependent variable and each explanatory variable) as deviation from the weighted individual mean. For individual i, the weighted individual mean of the dependent variable is defined as

yiw = eT A-1yi/eT A-1eT

(2.30)

and similarly for each explanatory variable. We shall denote by (xiw) the 1 × K vector of the weighted means of these variables.

2 Fixed Effects Models and Fixed Coefficients Models

37

The second step is to set up the generalized regression problem

y~i = X~i + ~i  y~ = X~  + ~

(2.31)

where y~i and X~i are the variables expressed as deviations from the weighted individual mean and treat it as if the variance­covariance matrix of the errors is the same as that of the original problem (i.e.  2(IN  A)). Two estimation procedures are available:
­ either the GLS formula is used, leading to the estimator

  ^ = Xi A-1X~i -1 Xi A-1y~i
­ or OLS is applied, after having transformed y~i and X~i by P, (yi = Py~i, X~i = PX~i) for P such that PAP = I.
This two-step procedure also provides at no extra cost the BLUE estimators of the individual intercepts. These are simply given by:

^ i = yiw - (xwi ) ^

As an illustration, for the AR(1) process, the weighted individual mean is given by:

T -1

eT A-1yi eT A-1eT

yi1 + yiT + (1 - )  yit

=

t=2
2 + (T - 2)(1 - )

and P is the known Prais/Winsten transformation leading to:

yit = 1 - 2y~it = y~it - y~it-1

t =1 t 2

The above result for the AR(1) process is derived in a different manner by Bhargava Franzini and Narendranathan (1982). These authors also propose an adaptation of the Durbin­Watson serial correlation test for the individual effect model and discuss the problem of the estimation of the parameter .
For the general case in which A is an unknown positive difinite matrix, a feasible GLS procedure can be implemented. The model is first estimated by the standard covariance method (or within transformation). From the computed residuals ^i, a consistent estimator of A (when N - ) is given by:

A^

=

1 N

 ^i^i

.

38

P. Balestra and J. Krishnakumar

2.3.4 Heteroscedasticity in Fixed Effects Models

The two most common cases of departure from spherical disturbances are serial correlation and heteroscedasticity. We have just seen how to handle the first one in an appropriate manner. In this section we will see how heteroscedasticity across observations can be specified and dealt with in the context of a fixed effect model.
In particular, the availability of panel data will enable us to estimate the variances for some simple specifications of heteroscedasticity and thus apply the feasible GLS procedure. However, in a general setting, only corrections a` la White/Newey-West can be recommended for want of adequate estimators for the variances parameters.
We will successively look at several cases going from a simple formulation to more complicated ones.

(a) Individual heteroscedasticity only Here we will assume different variances for different individuals, constant over time with zero covariances. Thus:
E(i2t ) = i2 t; i = 1, . . . , N E(ii ) = i2IT  Vi

and E( ) = diag(Vi)   IT
where = diag (i2). This is a special case in which the condition of Proposition 2.1 is satisfied.
Hence one can apply feasible GLS on the within transformed equation using ^  IT whose elements can be consistently estimated by

 ^i2

=

1 T

t

^it 2

with ^it = yit - xit ^w, the within transformed residual. (b) Time-wise heteroscedasticity only
Here we assume that the variances are different from one time period to another but constant over individuals for a given time period. Again zero covariances are assumed between different individuals. Therefore
E(i2t ) = t2  i; t = 1, . . . , T E(ii ) = diag(t2)  

and E( ) = IN  
This is a special case of block homoscedasticity where the covariance matrix can be expressed as IN  A. Hence we can follow the procedure described in Sect. 2.3.3 by taking deviations from the weighted means and applying feasible GLS. Consistent estimation of variances is given by:

2 Fixed Effects Models and Fixed Coefficients Models

39

 ^t2

=

1 N

^it 2
i

(c) Both individual and time-wise heteroscedasticity This is a combination of (a) and (b), maintaining zero covariances among different observations. We have:

E(i2t ) = i2t , i = 1, . . . , N; t = 1, . . . , T E(it  js) = 0 i = j or t = s or both

In this case there is no general feasible GLS estimator possible. Only the usual OLS/within estimators are available whose variance can be consistently estimated by applying the White correction:

  V (^w) = (X X)-1

xit ^it2xit (X  X )-1

it

If a specific structure is suspected for the occurrence of heteroscedasticity, say
i2t = h(a0 + a1z1it + . . . + apzpit)
with z1, . . . , zp being observed variables (which may or may not overlap with the x s), then the usual two step procedure can be followed. First, estimate the auxiliary regression by OLS:

h-1(^it2) = a0 + a1z1it + . . . + apzpit + vit

and estimate i2t as

^i2t = h(a^0 + a^1z1it + . . . + a^pzpit)

Then perform GLS (weighted least squares) by running OLS on

yit ^it

=

i ^it

+

xit ^it



+

it ^it

(d) General block heteroscedasticity Here individuals are assumed to be independent, each having its own variances­ covariance matrix. Thus

E(ii ) = Aii
and E(i j) = 0 i = j
As in (c) above, no feasible GLS procedure is available in this case. The only solution consists in correcting the variance of the within estimator as follows: (Newey-West type correction (cf. Arellano (1987))

40

P. Balestra and J. Krishnakumar

 V (^w) = (X X)-1

Xi^i^i Xi (X  X )-1

i

where ^i = yi - Xi^w is the within residual vector of individual i.

2.4 Extensions

2.4.1 Constant Variables in One Dimension

The generic fixed individual effect considered in Sect. 2.1 may be the result of some factors (such as sex, years of schooling, race, etc.) which are constant through time for any individual but vary across individuals. If observations are available on such variables, we may wish to incorporate them explicitly in the regression equation. The model may thus be written as:

yit = qi + xit  + it

(2.32)

where the row-vector qi now contains the observations on the variables which are constant for individual i, including the constant term, and  si the associated vector of coefficients. We shall assume that there are m such variables, in addition to the
constant. The vector xit , as before, contains K explanatory variables, varying in both dimensions.
Collecting the T observations for individual i, we get

yi = (qi  eT ) + Xi + i and finally, stacking the N individuals, we obtain the full model

(2.33)

y = (Q  eT ) + X + 

(2.34)

where Q is the N × (m + 1) matrix whose i-th row is qi. Next, we note that the columns of (Q  eT ) are linear combinations of the
columns of the matrix of individual dummies. In fact:

(Q  eT ) = (IN  eT )(Q  1) = DNQ

From this, we draw the following conclusions:
(i) if m + 1 > N, the parameter vector  is not identifiable. The slope parameters  can still be estimated by the covariance method (using the within transformation) in an unbiased and consistent way;
(ii) if m + 1 = N, the matrix Q is square. Assuming that it is non-singular, then:
­ the BLUE estimator of  in (2.34) is the covariance estimator;

2 Fixed Effects Models and Fixed Coefficients Models

41

­ the BLUE estimation of  in (2.34) is a linear non-singular transformation of ^ (the coefficient vector of the N individual dummies), i.e.:

^ = Q^



^ = Q-1^

(iii) if m + 1 < N, the covariance estimator of  is no longer BLUE (but it is still unbiased). This is so because the dummy variable model and model (2.34) are related by the definition  = Q . When m + 1 < N, a total of N - m - 1 restrictions are imposed on the vector . Ignoring these restrictions on  is like estimating a model with some additional extraneous variables, which produces unbiased but inefficient estimates. One can test the validity of such restrictions by the usual F-test. The constrained model is, in this case, model (2.34), while the unconstrained model is the individual dummy variables model.

From the above discussion it in clear that when constant individual variables

are explicitly introduced in the regression equation there is no room for dummy

variables (at least in an easily interpretable way).

The same argument applies when the model is extended to include variables that

vary in time, but that are constant for all individuals (such as prices). Consider the

regression equation:

yit = c + qi   + pt  + xit  + it

(2.35)

where qi is the same vector as qi but without the constant (with m components) and pt is the row-vector of n variables (without the constant) that are the same for all individuals at time t. For the full sample we have:

y = eNT c + (Q  eT )  + (eN  P) + X + 

(2.36)

where P is the T × n matrix whose t-th row is pt . Again it can be shown that:
(i) for identification of   and  the necessary order conditions are m < N and n < T;
(ii) the covariance estimator of  is unbiased and consistent in all cases; (iii) when model (2.36) is just identified (m + n = T + N - 2), the covariance esti-
mator of  is BLUE; (iv) when m+n < T +N -2, it is like imposing some restrictions on the coefficients
of the dummy variables.

2.4.2 Variable Slope Coefficients
In the covariance model, only the intercepts are allowed to vary across individuals and/or through time, while the slope parameters are kept constant. However, there are situations in which the slope parameters themselves may exhibit a pattern of variation.

42

P. Balestra and J. Krishnakumar

Consider, for instance, the case in which all regression coefficients are individual

specific:

yit = i + xit i + it = zit i + it

(2.37)

where xit si a (1 × K) vector of explanatory variables, zit = [1 xit ] and i = [i i ]. With the usual notation, for individual i we write

yi = eT i + Xii + i = Zii + i

(2.38)

and, analogously for the full sample:

y = DN + X~  +  = Z~ + 

(2.39)

where  = [1, . . . , N],  = [1, . . . , N],  = [1, . . . , N] and X~ and Z~ are the following block-diagonal matrices:





X1 0

X~ =  . . . 





Z1 0

Z~ =  . . . 

0 XN

0 ZN

of order, respectively, NT × NK and NT × N(K + 1). This is a special case of a

SUR model (cf. Zellner (1962)) with independent errors across observations and

equations (individuals in this context).

Given assumption A2, the BLUE estimator of  in (2.39) is the OLS estimator.

Simple algebra shows that the OLS estimator for the full sample boils down to the

OLS estimator of each individual regression, (2.38), separately. Calling SSi the sum

of squared residuals of the i-th regression problem, the total sum of squares for the

full model, denoted by SS, is simply the sum of the SSi with N(T - K - 1) degrees of freedom. Note that the rank condition for the identification of the i is that each

matrix Zi be of full column rank K + 1 (which requires that T > (K + 1)). A test of homogeneity in behavior (all i being equal to a common vector 0) can

now be easily performed. The constrained model is the pooled model (with constant

coefficients), i.e.,

y = Z0 + 

(2.40)

where Z = [Z1, . . . , ZN]. Its sum of squares is denoted by SSc (with NT - K - 1 degrees of freedom). Then, under normality, the following quantity

(SSc - SS)/(N - 1)(K + 1) SS/N(T - K - 1)

is distributed as an F-variable with (N - 1)(K + 1) and N(T - K - 1) degrees of freedom. An F-test is therefore appropriate.
When the model contains a constant term, an appropriate question to be asked is whether all the slope parameters are constant. In such a case, all intercepts are allowed to vary freely across individuals and the constrained model is the individual dummy variable model, whose sum of squares is denoted by SSN, with NT - N - K degrees of freedom. Then the quantity

2 Fixed Effects Models and Fixed Coefficients Models

43

(SSN - SS)/(N - 1)K SS/N(T - K - 1)

is distributed as an F-variable with (N - 1)K and N(T - K - 1) degrees of freedom. In a similar way we can treat the case of time-dependent slopes (which boils
down to OLS estimation period by period). We just write down, for future reference, the corresponding equations (for N > K + 1):

yit = zit t + it yi = Z~i + i y = Z~ + 

where

 = [1, . . . , T ]





Z~i =  zi1 . . .



ziT

T × T (K + 1)

Z~ = Z~1 . . . Z~N
The most general approach in the case of fixed effects is to consider both individual and time variations in the coefficients. It is very tempting to write the model as:

y = Z~ + Z~ + Z + 

(2.41)

with Z = [Z1, . . . , ZN], but for the same reason as in the covariance model the matrix of explanatory variables [Z~ Z~ Z] is not of full column rank. In fact it is easy to see that the rank of this NT × (K + 1)(N + T + 1) matrix is at most equal to (K + 1) (N + T - 1), since Z~(eN  IK+1) = Z and Z~(eT  IK+1) = Z. Therefore 2(K + 1) restrictions (at least) must be imposed on the regression coefficients. There are many
different ways to do this.
One can, as Hsiao (1986) suggests, minimize the sum of squares in (2.41) subject
to the 2(K + 1) restrictions

N

T

 i = 0,  t = 0 ,

i=1

t=1

or more simply (as was done in the covariance model), one could eliminate one vector i (say the last, and the corresponding K + 1 last columns of Z~) and one vector t (again the last, and the corresponding K + 1 last columns of Z~) and apply OLS. This amounts to using directly the two above restrictions in model (2.41)
together with a slight reparameterisation of the coefficients. In all cases, however, the necessary order condition for identification (NT > (K + 1)(N + T - 1)) must
be met.

44
2.4.3 Unbalanced Panels

P. Balestra and J. Krishnakumar

Up to this point, our attention has focused entirely on genuine (or balanced) panels, a situation in which N individuals are observed over the same T time periods. When the number of observations is not the same for all individuals we speak of an unbalanced panel. In the present subsection we briefly discuss the incidence of unbalancedness on the estimation of fixed effect models.
When only individual fixed effects are considered, no particular new problem arises. The model can still be represented as in (2.4), except that now the matrix DN of individual dummies does not have a nice Kronecker-product representation. Nonetheless, using the standard results of partitioned regression, it can easily be established that the deviation from the individual mean transformation (as in the balanced case) is the right transformation to be used in order to obtain the BLUE of  .
The story is quite different when both individual and time effects are deemed important. The difficulty stems from the fact that in this context the date of any single observation matters. Although it is always possible to work out the transformation that eliminates all fixed effect (the interested reader may consult in this respect the article by Wansbeek and Kapteyn( 1989)), the procedure is too complicated for practical purposes. By far the easiest approach to this problem is to set up a regression equation containing, in addition to the explanatory variables, the overall constant N - 1 individual dummies and T  - 1 time dummies (T  being the set of all dates available in the sample).

Appendix: Matrix Algebra for Balanced Panels

In this Appendix we collect the basic orthogonal projectors which appear in panel data analysis and highlight their relationships and properties.
To make this expository note self confined (and useful for future reference) we recall the following notations:

es is the unit (column) vector of order s (whose elements are all equal to unity); Js = eses is the unit matrix of order s × s; yit is the observation on a relevant variable for individual i (i = 1, . . . , N) at time

t(t = 1, . . . , T );

yi is the (T × 1) vector of observations for individual i;

y is the (NT × 1) vector of all observations : y = [y1, . . . , yN];

y

=

1 NT

  yit
it

is

the

overall

mean;

yi

=

1 T

 yit
t

is

the

individual

mean;

yt

=

1 N

 yit
i

is

the

time

mean.

2 Fixed Effects Models and Fixed Coefficients Models

45

The Basic Projectors

We consider a linear transformation of the vector y : y = My, where M is one of the orthogonal projectors commonly used in panel data analysis. The matrix M is idempotent (and symmetric) and can be viewed as the matrix of the idempotent quadratic form y My = y y.
We distinguish four cases.
Case 1: Just the overall effect

(1.a) The overall mean transformation: yit = y. It replaces each observation with the overall mean. The associated orthogonal projector is:

BNT

=

1 NT

eNT

eNT

=

1 NT

JNT

=

1 N

eN eN



1 T

eT

eT

=

1 N

JN



1 T

JT

(rank BNT = 1)

(1.b) The deviation from the overall mean transformation: yit = yit - y. Associated projector: DBNT = INT - BNT (rank = NT - 1)

Case 2: Individual effects only

(2.a) The individual mean transformation, also called (individual ) between trans-

formation: yit = yi

Associated

projector:

BN

=

IN



1 T

eT eT

=

IN



1 T

JT

(rank

=

N)

(2.b) The deviation from the individual mean transformation, also called (individ-

ual) within transformation: yit = yit - yi

Associated projector: WN = INT - BN (rank = NT - N)

Case 3: Time effects only

(3.a) The time mean transformation, also called (time) between transformation:

yit = yt

Associated

projector:

BT

=

1 N

eN

eN

 IT

=

1 N

JN

 IT

(rank

=

T)

(3.b) The deviation from the time mean transformation, also called (time) within

transformation: yit = yit - yt

Associated projector: WT = INT - BT (rank = NT - T )

Case 4: Both individual and time effects
The (overall) within transformation: yit = yit - yi - yt + y Associated projector: WNT = INT - BN - BT + BNT (rank = (N - 1)(T - 1))
To check that each projector defined above does indeed perform the right transformation can be done easily using the selection vector sit = (EiN)  (EtT ) , where EiN is the i-th elementary vector of order N (and similarly for EtT ). When applied to y, sit selects the observation yit : sit y = yit . We illustrate the procedure for the projector WNT (from which all other cases can easily be derived). We obtain successively:

46

P. Balestra and J. Krishnakumar

yit = sit y = sitWNT y = sit y - sit BN y - sit BT y + sit BNT y

= yit -

(EiN )



1 T

eT

y-

1 N

eiN

 (ETt )

y+

1 N

eN



1 T

eT

y

 =

yit

-

1 T

eT

yi

-

1 N

i

(ETt )

1 yi + NT

eNT y

= yit - yi - yt + y

The Within Transformations and Dummy Variables

It is straightforward to verify that WN and WT are related to their respective sets of dummy variables in the following way:
· WN = I - DN(DN DN )-1DN DN = IN  eT N individual dummies · WT = I - DT (DT DT )-1DT DT = eN  IT T time dummies
For WNT the situation is more delicate. We cannot use the full set of N +T dummy variables, since the matrix [DN DT ] is not of full rank. This can be seen by noting that the sum of the first N columns (DNeN = eNT ) is equal to the sum of the last T columns (DT eT = eNT ). Therefore we have to choose just N + T - 1 linearly independent columns of [DN DT ] or any non-singular transformation of them (the result being invariant to any non singular transformation). To establish the result in the simplest way, we choose to keep the full set of time dummies (DT ) and add N -1 linearly independent combinations of the columns of DN, i.e. D = DNA, with A an N × (N - 1) matrix of full rank. Since D must also be linearly independent when associated with DT , the combination to avoid is DNeN. To ensure this, we choose, in A, (N - 1) columns orthogonal to eN : A eN = 0. We then define the matrix

D = [D DT ]

Now we observe that the columns of D are orthogonal to those of DT , DDT = A DNDT = A (eN  eT ) = A eNeT = 0, so that:

D(D D)-1D = D(DD)-1D + DT (DT DT )-1DT

= DN A(A DN DN A)-1A DN + BT

=

1 T DNA(A

A)-1A

DN + BT

Next we note that eN(eNeN)-1eN + A(A -1A = IN (since the matrix F = [eNA] is non singular and therefore I = F(F F)-1F = eN(eNeN)-1eN + A(A A)-1A and consequently

2 Fixed Effects Models and Fixed Coefficients Models

47

D(D D)-1D

=

1

1

T DnDN - NT DN eN eN DN + BT

=

BN

-

1 NT

eNT eNT

+ BT

=

BN

- BNT

+ BT

This establishes that:

WNT = I - D(D D)-1D

A final remark: the three within projectors WN,WT ,WNT have the property of eliminating all constant effects of the appropriate type (individual, time, or both).

Relationships Between the Different Projectors

For Cases 1 to 3, we have defined for each case two matrices, call them M1 and M2, which have the following three properties:

· they are idempotent (and symmetric): MiMi = Mi · they are mutually orthogonal: MiMj = 0 i = j; · their sum is the identity matrix: M1 + M2 = I.

i = 1, 2;

For Case 4, the situation requises some careful attention. We do indeed have

four idempotent matrices (WNT , BT , BN, BNT ), but they do not share the last two properties. In order to get a decomposition fulfilling the three properties, we rewrite

WNT as:

WNT = INT - (BN - BNT ) - (BT - BNT ) - BNT

and define the four idempotent matrices:

M1 = BN - BNT (deviation of the individual mean from the overall mean) M2 = BT - BNT (deviation of the time mean from the overall mean) M3 = BNT (the overall mean) M4 = WNT (the overall within transformation)
It is now easy to verify that :

MiMi = Mi all i MiMj = 0 i = j  Mi = I.

Properties of the Decomposition

Suppose that s idempotent matrices of order n, Mi i = 1, . . . , s, satisfy the above three conditions and define the following positive definite matrix:

A =  iMi i

i > 0

i =  j

48

P. Balestra and J. Krishnakumar

Then:
1. The scalars i are eigenvalues of the matrix A with multiplicity ri = rank (Mi) 2. | A |= iiri 3. A-1 =  i-1Mi
i
4. QAQ = I for Q =  i-1/2Mi.
i
To prove these results is extremely simple. For result (1), it suffices to postmultiply A successively by M1, M2, . . .. Since AMi = iMi, the columns of Mi are eigenvectors of A associated with i. Given that there are ri linearly independent columns in Mi (ad noting that  ri = n), the multiplicity of i is exactly equal to ri. Result (2) follows from the fact that the determinant of a matrix is equal to the product of its eigenvalues. Finally, result (3) and (4) are verified by simple multiplication (AA-1 = I and QAQ = I).
The usefulness of these results is that the variance­covariance matrix of standard error component models can be expressed in the form of matrix A.

References
Aigner, D.J. and P. Balestra (1988), "Optimal Experimental Design for Error Components Models", Econometrica, 56, (4), 955­971.
Arellano, M. (1987), "Computing Robust Standard Errors for Within-Groups Estimators", Oxford Bulletin of Economics and Statistics, 49, 431­434.
Arellano, M. and O. Bover (1995), "Another Look at the Instrumental Variables Estimation of Error Component Models", Journal of Econometrics, 68, 29­51.
Bhargava, A., L. Franzini and W. Narendranathan (1982), "Serial Correlation and Fixed Effects Model", Review of Economic Studies, 49, 533­549.
Hoch, I. (1962), "Estimation of Production Function Parameters Combining Time Series and Cross Section Data", Econometrica, 30, 34­53.
Hsiao, C. (1986), "Analysis of Panel Data", Cambridge University Press, Cambridge. Kuh, E. (1963), "The Validity of Cross Sectionally Estimated Behaviour Equations in Time Series
Applications", Econometrics, 27, 197­214. Mundlak, Y. (1961), Capital Stock Growth: A Micro-Econometric Approach, Amsterdam: North-
Holland. Nerlove, M. (1965), Estimation and Indentification of Cobb-Dougkas Production Functions,
Chicago: Rand McNally. Wansbeek, T. and A. Kapteyn (1989), "Estimation of the Error Components Model with Incom-
plete Panels", Journal of Econometrics, 41, 341­361. Zellner, A. (1962), "An Efficient Method of Estimating Seemingly Unrelated Regression and Test
for Aggregation Bias", Journal of the American Statistical Association, 57, 348­368.

Chapter 3
Error Components Models
Badi H. Baltagi, La´szlo´ Ma´tya´s and Patrick Sevestre

3.1 Introduction
As discussed in the previous chapters, the disturbances of an econometric model include all factors affecting the behavior/phenomenon under study that the econometrician cannot explicitly specify, because the relevant statistical information either does not exist or is not accessible. This is the so-called unobserved heterogeneity. As an example, factors such as personal ability, adaptability, work diligence, etc. do have an impact on employees' wage profile but are generally not observed (see Chap. 22 on this). As long as they can be assumed not to vary over time, they can be accounted for through individual effects. Such individual effects also allow us to account for unobserved factors affecting, for example, firms behavior (regarding their investments in general and their foreign direct investment in particular, their labor demand, and/or their production efficiency; see Chaps. 20 and 21 below). Indeed, firms' environment as well as their managers' behavior impact upon their efficiency and employment/investment decisions whilst they are not fully observed.
As already stressed in the first chapters, one of the major advantages of panel data is that their "double dimension" enables us to account for these unobservable factors as long as they can be considered fixed over time. The main difference between the fixed effects models considered in the previous chapter and the error components models considered here is the assumption made about those individual effects.
Badi H. Baltagi Center for Policy Research, 426 Eggers Hall, Syracuse University, Syracause, NY 13244-1020, USA, e-mail: bbaltagi@maxwell.syr.edu
La´szlo´ Ma´tya´s Central European University, Department of Economics, Na´dor u. 9, 1051 Budapest, Hungary, e-mail: matyas@ceu.hu
Patrick Sevestre Universite´ Paris 1 ­ Panthe´on Sorbonne, Ecole Economique de Paris (Paris School of Economics), 106-112 Boulevard de l'Ho^pital, 75013 Paris, France, e-mail: Patrick.Sevestre@univ-paris1.fr

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

49

c Springer-Verlag Berlin Heidelberg 2008

50

B.H. Baltagi et al.

The basic assumption underlying the error components model is the absence of correlation between the individual effects and the regressors of the model. Although this (quite restrictive) assumption can be relaxed (see Chap. 4 in particular), we shall stick to it in this chapter as the error components model can be considered as one of the pillars of panel data econometrics.

3.2 The One-Way Error Components Model

3.2.1 Definition/Assumptions of the Model

The one-way error components model can be written as:

K
 yit = 0 + kxit + uit , i = 1, . . . , N and t = 1, . . . , T k=1

(3.1)

with uit = i + it .
The disturbances uit are decomposed into two components, i and it , which explains the model's name. i represents the individual effects, accounting for unobservable factors affecting y and which do not vary over time; it represents the other variables influencing y but which vary both over time and individuals. Both are assumed to be independently distributed across individuals.1 Another impor-
tant assumption underlying the error components model is the strict exogeneity of
regressors, which implies:

E(i | xi1, xi2, . . . , xiT ) = 0, i E(it | xi1, xi2, . . . , xiT ) = 0, i, t

Moreover, i and it are both assumed to be serially uncorrelated and homoske dastic:

V (i | xi1, xi2, . . . , xiT ) = 2 , i Cov(it , it | xi1, xi2, . . . , xiT ) = tt 2, i, t, t Cov(i, it | xi1, xi2, . . . , xiT ) = 0, i, t.
Given the assumption stating the absence of correlation between the regressors and the individual effects, the latter do not affect the conditional expectation of y but do impact its variance. Indeed, given the above assumptions, one has:

K
 E(yit | xi1, xi2, . . . , xiT ) = 0 + kxit k=1

1 See Chap. 18 for a specific framework in which such a correlation across individuals can (must) be assumed.

3 Error Components Models

51

and

Cov(yit , yit

| xi1, xi2, . . . , xiT ) = Cov(uit , uit | xi1, xi2, . . . , xiT )

=

2 + 2 if t = t 2 if t = t .

The presence of the individual effects in the disturbances thus induces, for each individual, some serial correlation across periods. It is worthwhile noticing that this serial correlation does not depend on the time interval between two observations, contrary to the usual pattern of serial correlation in time-series models.
Stacking all the observations related to the individual i, one can write:

yi =

Xi

×



+ ui

(3.2)

(T × 1) (T × (k + 1)) ((k + 1) × 1) (T × 1)

where yi = (yi1, yi2, . . . , yiT ) represents the vector of observations of the dependent variable for the ith individual; Xi the matrix of observations of the explanatory variables (including the constant term) and ui the vector of the disturbances for this individual. Given the assumptions defining this model, the vector of the disturbance
terms has the following properties:

E(ui | xi1, xi2, . . . , xiT ) = 0, i V (ui | xi1, xi2, . . . , xiT ) = A, i

with:

A (T × T )

=

 

2 + 2 2 . . . ..
. . . ..

2 2 + 2
.
.

... ... ... ...

2 2 .
.

 

2

2 . . . 2 + 2

= 2 IT + 2 JT
where IT is the identity matrix of order T and JT is a square (T × T ) matrix of ones. The pattern of this matrix clearly shows the existence of a serial correlation as-
sociated with the individual effects; magnitude of which is independent of the time span between the time periods under consideration.
Stacking then the whole set of individual vectors of observations, y = (y11, y12, . . . , y1T , . . . , yN1, yN2, . . . , yNT ) , such that the slower index is i and the faster index is t, one can write the model as:

y=

X

×



+u

(3.3)

(NT × 1) (NT × (k + 1)) ((k + 1) × 1) (NT × 1)

52
with:
where:

B.H. Baltagi et al.

E(u | x1, x2, . . . , xK) = 0 V (u | x1, x2, . . . , xK) = 2 





2 (NT,

 NT)

=



E(11 ) E(21 )
. . . ..
. . . ..

E(12 ) E(22 )
.
.

... ... ... ...

E(1N ) E2N )
.
.



E(N1 ) E(N2 ) . . . E(N N)





A 0 ... 0

=



. .

0 . . .. . . ..

A . .

... ... ...

0 . .



0 0 ... A

= IN  A = 2 [INT + (2 /2) (IN  JT )]
i.e., using the Within and Between matrix transforms notations:2
V (u) = 2 [WN + ((2 + T 2 )/2)BN ] .
The nullity of all off-diagonal blocks in  just corresponds to the assumed independence across individuals.
The error components model then appears as a rather common regression model with a particular pattern of serial correlation in the disturbances. The well-known result that the OLS estimator of the coefficients is still unbiased and consistent but inefficient then applies. It is also much well-known that the GLS estimator has better properties in this context.

3.2.2 The GLS Estimator
3.2.2.1 Definition
Recall that in a model where the variance­covariance matrix of the disturbances is proportional to , the GLS estimator of the coefficients is given by:
gls = (X -1X )-1X -1y.
2 WN = INT - (IN  JT /T ) and BN = IN  JT /T .

3 Error Components Models

53

Given the particular structure of ,3 this estimator can be written as:

gls = (X WN X +  X BN X )-1(X WN y +  X BN y)

where WN and BN are respectively the Within and Between operatorst and



=

2 2 + T 2

.

The GLS estimator then combines the Within and Between variation of the observations. It does so in an optimal way as it can be shown that GLS corresponds to the minimum variance linear unbiased estimator among all the estimators combining the Within and Between variation. In other words, the value of  that minimizes the variance of:

 ( ) = (X WNX +  X BN X)-1(X WN y +  X BN y)

is just equal to  = 2/(2 + T 2 ) =  . Then, if we knew the value of  (or, equivalently, that of  ), computing the GLS
estimator would be very simple as it is well-known that GLS can be interpreted as OLS on the transformed model:

-1/2 y = -1/2 X  + -1/2 u ,

which, in this particular case, just amounts to:







yit + (  - 1)yi = [xit + (  - 1)xi ]  + it + (  - 1)i

where y¯i =  yit , etc.
t

3.2.2.2 Properties

Given the assumptions stated above, the GLS estimator is unbiased and efficient. Its variance is given by:

V (gls) = 2(X -1X )-1

= 2(X

WN X

+

2

2 +T

2

X

BN X )-1 .

Moreover, if the disturbances are normally distributed, the GLS estimator is also normally distributed:

3 Because the spectral decomposition of  is given by WN + ((2 + T 2 )/2)BN = WN + (1/ )BN , one gets -1 = WN +  BN .

54

B.H. Baltagi et al.

gls



N

( , 2(X

W

X

+

2

2 +T

2

X

BX )-1).

This estimator is consistent for N going to infinity, with finite T , under the same assumptions as above4 and if the variance­covariance matrices of the regressors are
bounded both in the Between and Within dimensions, i.e., if:

X plimN

BN X N

= BNxx ,

a finite positive definite matrix

and,

X plimN

WN X N

= WxNx ,

a finite positive definite matrix.

If moreover, the terms i and it in the disturbances are independently and identically distributed, the GLS estimator is asymptotically efficient and its asymptotic distribution (when N   but T is fixed) is given by:

 N(gls -  )



N

(0 , 2(WxNx +  BxNx)-1) .

Along the same lines, and with assumptions adapted from the above ones, it is possible to show that the GLS estimator is also consistent when both N and T tend to infinity. Its asymptotic distribution is then given by:

 NT (gls -  )



N (0 , 2(WxNxT )-1)

with

plimN,

T 

X

WN X NT

= WxNxT ,

a finite positive definite matrix.

Indeed, when T tends to infinity,  tends to 0 and gls converges to the Within
estimator w described below. It is then obvious that the GLS estimator's variance­ covariance matrix does not depend on the Between variation of the regressors.
Thus, the GLS estimator exhibits good properties, both in finite samples and asymptotically. Unfortunately, it relies on unknown parameters, namely the variance of the individual effects 2 and that of the idiosyncratic element, 2 that appear in  = 2/(2 + T 2 ). In order to compute a "feasible-GLS" estimator, one then has to estimate first the variance components 2 and 2 in order to get in turn an estimate of  .

4 More rigorously, the "Error components-GLS" estimator remains consistent even when one
mis-specifies the variance­covariance matrix of the disturbances. If the true matrix is given by V (u) = 2 = 2, the GLS estimator is consistent as long as the matrix X -1 -1X / N, converges towards a positive definite matrix. But the variance­covariance matrix of the GLS esti-
mated coefficients is clearly inconsistent in this case.

3 Error Components Models

55

3.2.3 The Feasible GLS Estimator

3.2.3.1 Definition

The Feasible-GLS estimator definition is very close to that of true GLS except for the fact that  is replaced by a consistent estimator :

fgls = (X -1X )-1X -1y = (X WNX +  X BN X)-1(X WN y +  X BN y)

with



=

2 2 + T 2

.

3.2.3.2 Variance Estimation

There are several ways to estimate the unknown variances 2 and 2. However, the most commonly used approach is that proposed by Swamy and Arora (1972).
It consists of using the residual variances associated with the regression stated in
the Within and Between dimensions of the observations. Let us first consider the
former, i.e., the regression based on the Within transformation of the equation:

yit - yi = [xit - xi ] + it - i .

That is, in matrix form,

WN y = WN X  +WN  .

In this model:

V (WN ) = 2WN .

As we know from Kruskal's theorem, even though the variance­covariance matrix
of the disturbances is not scalar, OLS on this model is still the BLUE of  . As a consequence, the natural estimator of the variance 2 is given by:

w2

=

ww rank(Mwx)

=

N(T

ww - 1) - kw

with

w = WN y -WN X w = (WN -WN X (X WN X )-1X WN )y = (WN - Pwx)y = Mwxy
which is exactly the residual from the Within regression.

56

B.H. Baltagi et al.

It is quite easy to show that this estimator is unbiased. Indeed,

E( WN) = trace[E(ww)] = trace[E( Mwx)] = E[trace(Mwx )] = trace(Mwx).E( )
= trace[Mwx.2] = 2 × trace[Mwx(WN + (1/ )BN)] = 2 × trace[Mwx] = 2 × trace[WN - Pwx] = 2 × (N(T - 1) - kw)

where kw is the number of regressors in the Within regression. As a consequence,

E

(w2 )

=

E

[

N(T

ww - 1)

-

kw

]

= 2 .

It is worthwhile to note that caution must be exercised when computing this vari-
ance using software packages. Indeed, when estimating the Within regression with
OLS on transformed data, the usual software packages will generally consider that
there are NT observations and kw estimated coefficients. The number of degrees of freedom considered in the computations of variances will then be (NT - kw). This is not correct as the true number of degrees of freedom should account for the in-
dividual effects that are implicitly estimated when doing a Within regression. The correct number of degrees of freedom should be N(T - 1) - kw (i.e., the rank of the WN operator).
Once this potential problem of degrees of freedom is correctly accounted for, the residual variance of the Within regressions provides a consistent estimator of 2 (when N  , T being finite as well as when N and T  ).
Proceeding along the same lines, one can show that the residual variance of the
Between regression, i.e. the variance resulting from applying OLS to the model:

BN y = BN X  + BN u

allows to get a consistent estimate of 2 + 2/T . Indeed, let us consider the residuals of this regression: ub = BN y - BN Xb = Mbxy (computed on NT observations), one can show that

E

(b2)

=

E

[

T

ubub (N - kb

)

]

= 2 + 2/T.

3 Error Components Models

57

Here again, one must pay attention to the degrees of freedom. Using NT
observations, the software will generally set the number of degrees of freedom to NT - kb where kb is the number of regressors in the Between regression while the correct number of degrees of freedom is equal to T (N - kb), due to the T times repetition of the individual means. On the other hand, if the Between regression is
computed on the sample of the N individual means, i.e. if one estimates the model

yi = xi + ui i = 1, . . . , N

the residual variance then directly provides a consistent estimate of 2 + 2/T

E

(b2

)

=

E

[

i ui2 N - kb

]

= 2 + 2/T .

Then, a consistent estimate of  can easily be computed as:



=

w2 T b2

=

2 2 + T 2

.

There are many other ways to estimate the variances.5 However, Maddala and Mount (1973) have shown that the choice of a particular method to estimate these variances does not impact significantly on the properties of the estimated coefficients in the second step of the Feasible-GLS estimator (see also Taylor (1980)).
This does not mean that replacing the true value  by an estimate  does not have any consequence. Although it does not affect the asymptotic properties of the feasible GLS estimator, it does have some influence on its finite sample properties. Indeed, while the GLS estimator is unbiased, the Feasible GLS is not, except under very particular circumstances (see Taylor (1980)).

3.2.3.3 Properties
As just stated above, the Feasible-GLS estimator of the error components model is, as any other Feasible-GLS estimator, biased in finite samples
E(fgls) = E[(X -1X )-1X -1y] =  + E[(X -1X )-1X -1u] = .
Indeed, the dependence between  and the disturbances u impairs the nullity of the second term in the above expectation and thus induces a bias for fgls. However,
5 See Wallace and Hussain (1969), Amemiya (1971), Swamy and Arora (1972).

58

B.H. Baltagi et al.

Taylor (1980) has shown that when the disturbances are normally distributed, fgls is unbiased as long as the variances 2 and 2 are estimated via the Between and Within regressions (cf. supra) and that N  k + 5 and T  2. Moreover, while we know that, in an econometric model with unknown heteroskedasticity or serial
correlation, the true variance of the feasible-GLS estimator of the coefficients is,
in general, unknown, Taylor (1980) has nevertheless provided the formula of the
variance­covariance matrix of the Feasible-GLS estimator of the error components model assuming normality of the disturbances and N  k + 10 and T  2. Unfortunately, the expression of this variance is quite complex and its real computation
not easy. However, an interesting by-product of this result is that Taylor (1980) has
shown that this Feasible-GLS estimator is often more precise than the other esti-
mators available for the error components model. But a further drawback of this
estimator is that, in finite samples, it is not distributed as a Normal, even when the
disturbances are.
The "unsatisfactory" finite sample properties of the feasible-GLS estimator are
quite "classical". Also "classical" are its good asymptotic properties: it is asymptot-
ically equivalent to the GLS estimator as long as N tends to infinity (whatever T ). Then, under the error components assumptions and assuming that X -1X/N converges to a finite positive definite matrix, the Feasible-GLS estimator is consistent for N tending to infinity, T finite.6 Its asymptotic distribution is then given by

 N(fgls -  )



N (0 , 2(BxNx + WxNx )-1) .

Indeed, the Feasible-GLS estimator is asymptotically equivalent to the GLS one. It is then asymptotically efficient. Those properties remain valid when both N and T go to infinity. In particular,

 NT (fgls -  )



N

(0 , 2(WxNxT )-1) .

Then, under the assumptions stated above (which include the strict exogeneity of all regressors), the Feasible-GLS estimator provides very reliable parameter estimates, at least if the individual dimension of the sample is large enough.

3.2.4 Some Other Estimators
GLS and Feasible-GLS thus combine in an optimal way the Within and Between variance of the observations. However, this does not preclude other ways to estimate an error components model. Indeed, comparing those different estimators to each other may be helpful in identifying possible mis-specification errors.

6 cf. footnote 4.

3 Error Components Models

59

3.2.4.1 The OLS Estimator

OLS on the pooled data yields:

ols = (X X )-1X y .

(3.4)

Under the assumptions which we considered earlier in this chapter, this estimator is unbiased and its variance for any linear regression is given by

V (ols) = 2(X X )-1X X (X X )-1 .

When N tends to infinity but T remains finite, the OLS estimator is consistent. If the error components i and it are i.i.d., OLS is asymptotically distributed as a Normal

 N(ols -  )



N

(0 , 2(BNxx + WxNx )-1(WxNx +  BxNx)(BxNx + WxNx )-1) .

But one has to notice that while OLS is still consistent when both N and T go to infinity, its asymptotic variance is unbounded in this case (e.g. see Trognon (1993)).

3.2.4.2 The Between Estimator

This estimator just amounts to applying OLS to the model written in terms of individual means

K
 yi = kxki + ui , k=1

with ui = i + i, i = 1, . . . , N .

Using the above defined Between operator BN, the model can be written, in matrix form, as,
BN y = BN X ×  + BN u . (NT × 1) (NT × kb) (kb×1) (NT × 1)
It is worth mentioning here that the latter way of writing the model induces T repetitions of the model written in the former way, i.e. in terms of the individual means. However, writing the model in such a matrix form allows a more systematic way of presenting the Between estimator. This is just given by:

B = (X BN X )-1X BN y .

(3.5)

which means that the Between estimator makes use of the Between individual variation of the observations only, hence giving a full weight to "permanent" differences across individuals.

60

B.H. Baltagi et al.

Under the assumptions stated above, this estimator is unbiased and its variance is

V (B) = 2(X BN X )-1X BN BN X (X BN X )-1 = (2 + T 2 )(X BN X )-1.

Moreover, it is consistent when N goes to infinity, but T remains finite as well as when both N and T go to infinity. Its asymptotic distribution (for N  ) is normal

 N(B -  )

 N (0 , 2 (BxNx)-1) .

However, as the OLS estimator, the Between estimator does not have a finite variance when N and T go to infinity.

3.2.4.3 The Within Estimator

This estimator, also called "covariance estimator", or "fixed effects estimator", is OLS applied to the model written in differences from individual means

yit - yi = (xit - xi ) + it - i

In matrix form

WN y = WN X ×  + WN  . (NT × 1) (NT × kw) (kw×1) (NT × 1)
The Within estimator then writes as

W = (X WN X )-1X WN y .

(3.6)

This method thus makes use of the Within-individual variation of the observations only. This can be seen as a drawback which discards a large part of the information contained in the raw data. Indeed, in most panels, the differences across individuals is often larger than that "Within-individuals". Moreover, as the above formula clearly shows, this estimator is identical to the one obtained under the assumption that the individual effects are fixed, and not random. As a consequence, the constant term as well as the coefficients of the explanatory variables which are constant over time cannot be estimated using this method. Indeed, let us consider the model
yit = Xit  + Zic + it ;
the Within estimator amounts to discarding the Zi variables from the model, due to the "Within transformation":

yit - yi = (xit - xi ) + it - i .

3 Error Components Models

61

One can solve this problem by applying OLS to

yi - xi W = Zic + ui .

Under the assumptions considered since the beginning of this chapter, W is unbiased and its variance is given by

V (W ) = (X WN X )-1X WN WN X (X WN X )-1 = 2(X WN X )-1 .

This estimator is consistent, both when N goes to infinity, with finite T and when

N and T simultaneously go to infinity. The corresponding asymptotic distributions

are normal

 N(W -  )



N (0 , 2(WxNx )-1)

and

 NT (W -  )



N

(0 , 2(WxNxT )-1)) .

It is remarkable that when both N and T go to infinity, the asymptotic distribution
of the Within estimator is identical to that of the GLS estimator (this is because limT  = limT 2/(2 + T 2 ) = 0). As a consequence, the Within estimator is asymptotically efficient in this case.

3.2.4.4 Reinterpreting Usual Estimators: The  -type Estimators

All the estimators considered until now make use of either the Between variance of the observations, their Within variance, or both. It is then quite natural to group those estimators together, within a class, that may be called " - class" estimators, defined as (see Maddala (1979)):
 ( ) = [X WN X +  X BN X]-1(X WN y +  X BN y) = [X (WN +  BN )X]-1X (WN +  BN )y
where  is a scalar such that, ­ if  = 0,  ( ) = W ; one gets the Within estimator; ­ if  =  ,  ( ) = gls; one gets the GLS estimator; ­ if  =  ,  ( ) = fgls; one gets the Feasible-GLS estimator; ­ if  = 1,  ( ) = ols; one gets the OLS estimator; ­ if  = ,  ( ) = B; one gets the Between estimator.
3.2.4.5 The Maximum Likelihood Estimator

Making the additional assumption that both the individual effects i and the idiosyncratic disturbances it are normally distributed as N(0, 2 ) and N(0, 2) respectively, it is also possible to resort to the maximum likelihood principle to estimate
the error components model. The log-likelihood attached to this model is given by

62

B.H. Baltagi et al.

ln(L)

=-

NT 2

ln(2) -

NT 2

ln(2) +

N 2

ln( 2

2 + T 2

)

-

1 22

(y

-

X



)

-1(y - X )

=-

NT 2

ln(2) -

NT 2

ln(2) +

N 2

ln( )

-

1 22

(y

-

X



)

-1(y - X )

with -1 = WN +  BN . Maximizing this log-likelihood with respect to  , 2 and  does not yield closed form expressions, given the non-linearity in  . However, one can make use of the first order conditions with respect to  and 2:

 ln(L) 

1 = - w2 X

-1(y - X ) = 0

 ln(L)  2

=

-

NT 22

+

1 4

(y

-

X



)

-1(y - X )

=0

in order to concentrate the likelihood. Indeed, from the above equations, we get

MLE = (X -1X )-1X -1y

and

2MLE

=

1 NT

(y

-

X

MLE

)

-1(y - X MLE) = 0 .

Then, following Breusch (1987), one can concentrate the likelihood, i.e., substitute  and 2 for their true values  and 2 so that the likelihood now only depend on one unknown parameter,  :

ln(L)

=

-

NT 2

(1 + ln(2)) -

NT 2

ln[(y - X )

(WN

+  BN

)(y - X )] +

N 2

ln( ) .

Conditionally on  , maximizing this log-likelihood with respect to  leads to



=

(T

(y - X ) W (y - X ) - 1)(y - X ) B(y - X

)

.

Then, an iterative procedure can be set up: taking the Within estimator W as the departure estimate of the procedure, one can estimate  using the above formula by
replacing W for  and then going back to estimating  by MV as defined above. Breusch (1987) has shown that the sequence of the ( j)'s obtained at each iteration ( j) of the procedure forms a monotonic sequence, so that this procedure should lead
to a global maximum.

3 Error Components Models

63

Given the assumptions made, these estimators are consistent when only N   and also when both N and T  . Moreover, MV has an asymptotic Normal distribution, identical to that of the GLS estimator (cf. Trognon (1993).

3.2.5 Prediction

Suppose we want to predict y for the ith individual, S periods ahead. For the model given in (3.3), knowing the variance­covariance structure of the disturbances, Goldberger (1962) showed that the best linear unbiased predictor (BLUP) of yi,T+S is

yi, T +S = Xi, T +Sgls + 

-1 2 ugls

for s

1

where ugls = y - Xgls and  = E(ui,T+S u). Note that for period T + S

ui, T +S = i + i, T +S

and  = 2 (li  eT ) where li is the ith column of IN, i.e. li is a vector that has 1 in the ith position and zero elsewhere and eT is defined in (2.3). In this case



-1 2

=

2 2

(li

 eT )[WN

+ (2/(2 + T 2 ))BN ] =

(2

2 +T

2

)

(li

 eT )

since (li eT )BN = (li eT ) and (li eT )WN = 0. The typical element of 

-1 2

ugls

becomes

(

T 2 (2+T 2

)

)ui,

gls

where

ui, gls

=

tT=1

uit,gls/T .

Therefore, the BLUP for yi,T+S corrects the GLS prediction by a fraction of the

mean of the GLS residuals corresponding to that ith individual (see Taub (1979)).

Baillie and Baltagi (1999) consider the practical situation of prediction from the

error components model when the variance components are not known. They de-

rive both theoretical and simulation evidence as to the relative efficiency of four

alternative predictors:

(i) an ordinary predictor, based on the optimal predictor given above, but with MLEs replacing population parameters,
(ii) a truncated predictor that ignores the error components correction, given by the last term above, but uses MLEs for its regression parameters,
(iii) a misspecified predictor which uses OLS estimates of the regression parameters, and
(iv) a fixed effects predictor which assumes that the individual effects are fixed parameters that can be estimated.

The asymptotic formula for MSE prediction are derived for all four predictors. Using numerical and simulation results, these are shown to perform adequately in realistic sample sizes. Both the analytical and sampling results show that there are

64

B.H. Baltagi et al.

substantial gains in mean square error prediction by using the ordinary predictor instead of the misspecified or the truncated predictors, especially with increasing  = 2 /(2 + 2) values. The reduction in MSE is about ten fold for  = 0.9 and a little more than two fold for  = 0.6 for various values of N and T . The fixed effects predictor performs remarkably well being a close second to the ordinary predictor for all experiments. Simulation evidence confirm the importance of taking into account the individual effects when making predictions. The ordinary predictor and the fixed effects predictor outperform the truncated and misspecified predictors and are recommended in practice.

3.3 More General Structures of the Disturbances
The previous model can be generalized in several ways, by allowing for more general types of serial correlation as well as for possible heteroskedasticity. Let us first consider the two-way error components model, i.e., the model with both individual and time specific effects in the disturbances.

3.3.1 The Two-Way Error Components Model
The two-way error components model allows for specific time effects (t) accounting for unobserved factors assumed to affect all individuals in a similar way at a given point in time.

3.3.1.1 Definition/Assumptions of the Model

This model can be written as

yit = xit  + uit

with uit = i + t + it ,

or, in vector form for all observations

y = X +u

with u =   eT + (eN  IT ) + 
where  is the random vector of time effects (T × 1). As in the previous model we assume that ,  and  are mutually independent,
with 0 means and variance­covariance matrices

3 Error Components Models

65

E( ) = 2 IN ,

E(  ) = 2IT

E( ) = 2INT .

Since the individual and time effects are incorporated in the model through the error structure, our main interest has to focus, as earlier, on the covariance matrix of the disturbance terms

E(uu ) = 2 (IN  JT ) + 2(JN  IT ) + 2INT = 2 .

3.3.1.2 The GLS Estimator

If we want to use the GLS estimator, as in the case of the one-way error components model, we need the inverse of the covariance matrix . Starting from  and using its spectral (eigen value) decomposition

-1

=

WNT

+

2

2 + T 2

BN

+

2

2 + N2

BT

+

2

+

2 T 2

+

N2

JNT NT

.

Now the GLS estimator is

gls = [X

(WNT

+

BN

+ 1BT

+

2

JNT NT

)X ]-1X

(WNT

+  BN

+ 1BT

+

2

JNT NT

)y

where



=

2 2 + T 2

,

1

=

2 2 + N2

,

2

=

2 2 + T 2 + N2

.

It would seem that this estimators is not very operational. However, one can get GLS as an OLS regression by transforming the equation as follows:

(WNT

+

 

BN

+

  1BT

+

 2

JNT NT

)y





 

= [yit - (1 -  )yi - (1 -  1)yt + (1 -  -  1 +  2)y

The small sample properties of the GLS estimator in this model are clearly the same as for the model with only individual effects. So the GLS remains unbiased and BLUE. When only N goes to infinity but T stays finite, the GLS is also consistent and has an asymptotic distribution given by

66

B.H. Baltagi et al.

 N(gls

-



)



N(0

,

2(WxNx

+

2

2 +T

2

BNxx)-1)

,

as long as

XX lim N N

= TxNx

is

a

finite

positive

definite

matrix

as well as

1 lim X N N

WNT X

= WxNx

,

and where BNxx is defined as in the one-way error components model. In the case where both N and T  , the GLS is consistent and its asymptotic

distribution is

 N

T

(gls

-



)



N

(0,

2WxNxT

-1)

.

under the hypotheses

1 lim X N&T  NT

X = TxNxT

is a finite positive definite matrix,7 and

1 lim X N&T  NT

WNT X

= WxNxT

is

also

positive

definite.

Despite its good properties, the GLS estimator is unfortunately of very limited use in practice as we do not know the variance components, so that we must use the Feasible-GLS estimator.

3.3.1.3 The Feasible-GLS Estimator

The first problem to be solved in estimating the model by Feasible-GLS is to find appropriate estimators for the variance components. The starting point could be the error term u and its decomposition, but because we cannot observe it directly we need to estimate it. These estimates can be based on different consistent estimators of the error components model. The necessary expected values to identify the unknown variances are

E(u2it ) = 2 + 2 + 2

 E(( 1
T

t

uit

)2)

=

2

+

1 T

2

+

1 T

2

 1
E (( N

i

uit )2)

=

1 N

2

+

1 N

2

+ 2

so the variance components estimates are

7

This

hypothesis

implies

that

the

limits

of

1 NT

X

BN X,

1 NT

X

BT X and

1 N2T

2

X

JNT X are also finite.

3 Error Components Models

67

2

=

T

T -

1

1 T

i(t uit )2 N-K

-

i t u2it NT -K

2

=

N N-1

1 N

t (i uit )2 T -K

-

i t ui2t NT -K

2

=

i t ui2t NT -K

- 2

- 2

or in another form

T 2

+ 2

=

u N

BN u -K

N2

+ 2

=

u T

BT u -K

2

=

(N

-

u WNT u 1)(T - 1) -

K

-

1

where the u residual vector can be obtained by any consistent estimation of the
model. The above variance components estimators are consistent under the usual conditions. However, if T is finite (N  ) the estimators of 2 are, of course, inconsistent.

3.3.1.4 The OLS and Within Estimators

When only N  , the OLS estimator is not necessarily consistent, even if we

suppose that both

1 lim X N N

X = TxNx

and

1 lim X N N

WNT X

= WxNx

are finite positive definite matrices (see Trognon (1993)). However, when N and

T   the OLS estimator becomes consistent, but unfortunately, its asymptotic co-

variance

matrix

may

not

be

finite

if

either

limN

1 NT

X

BN X

=

0

or

limN

1 NT

X

BT

X = 0.

Now let us turn our attention to the Within estimator. It is clear from the decom-

positions of the error terms that the projection matrix WNT nullifies (similarly as in

the case of the one-way model) the individual and time effects. This means that we

can get an estimator of model by transforming all the variables with WNT and apply

the OLS. We can get this estimator by transforming all variables of the model such

as y

WNT y = [yit - yi - yt + y] ,

and then use the OLS estimator.

68

B.H. Baltagi et al.

As earlier the Within estimator is unbiased, have a normal distribution and a

covariance matrix

V (W ) = 2(X WNT X )-1

As seen, in the asymptotic case the GLS and the Within estimators are asymptotically equivalent, but in the semi­asymptotic case (N only goes to infinity) the GLS remains more efficient than the within estimator.

3.3.1.5 One-Way Versus Two-Way Error Components

This section investigates the consequences of under-specifying or over-specifying the error components model. Since the one-way and two-way error components models are popular in economics, we focus on the following two cases:
(1) Under-Specification Case: In this case the true model is two-way

uit = i + t + it i = 1, . . . , N; t = 1, . . . , T

(3.7)

while the estimated model is one-way

uit = i + it

(3.8)

with i  N (0, 2 ), t  N (0, 2) and it  N (0, 2) independent of each other and among themselves. Knowing the true disturbances u = (u11, . . . , u1T , . . . , uN1, . . . , uNT ), the Best Quadratic Unbiased (BQU) estimators of the variance components for the one-way model are given by

2 = u WN u/ trace(WN) and T 2 + 2 = u BN u/ trace(BN)

Using this fact, one can easily show, (see Baltagi and Li (1991b)) that

E(2) = trace[WN/N(T - 1)] = 2 + 2 ,

(3.9)

which is biased upwards by 2. Similarly,

E(T 2 + 2) = trace[BN /N] = T 2 + 2 + 2 ,

(3.10)

which is also biased upwards by 2. Substituting E(2) from (3.9) in the left-handside of (3.10), one gets E(2 ) = 2 . This shows that knowing the true disturbances, the BQU of 2 for the misspecified one-way model is biased upwards, while the BQU of 2 remains unbiased.
In practice, the true disturbances are not known and may be replaced by the one­
way within residuals uW = y - XW where W is the one­way within parameters estimates. In this case
2 = uWWN uW /N(T - 1)
and one can easily show that plim 2 = 2 + 2.

3 Error Components Models

69

Similarly, substituting uW for u in T 2 + 2 we get
2 = uW BN uW /NT - 2/T
and one can show that plim 2 = 2 . This shows that even if the true disturbances are replaced by the within residuals, the misspecified one-way estimator of 2 remains inconsistent, while that of 2 is consistent. (2) Over­Specification Case: In this case, the true model is one-way, given by (3.8), while the estimated model is two-way, given by (3.7). Knowing the true disturbances, the BQU estimators of the two-way model are given by

2 = u WNT u/(N - 1)(T - 1)

(3.11)

T 2 + 2 = u BN u/(N - 1) N2 + 2 = u BT u/(T - 1)

(3.12) (3.13)

(see Amemiya (1971)), where WNT , BN and BT are defined as above. Therefore,

E(2) = [WNT ]/(N - 1)(T - 1) = 2

(3.14)

E(T 2 + 2) = [BN ]/(N - 1) = T 2 + 2 E(N2 + 2) = [BT ]/(T - 1) = 2 .

(3.15) (3.16)

Substituting (3.14) in the left-hand-side of (3.15) and (3.16), we get E(2 ) = 2 and E(2) = 0. This shows that if the true disturbances are known, the BQU estimators of 2 , 2 and 2 for the misspecified two-way model remain unbiased. If the uit 's are replaced by the two-way within residuals uW = y - XW where W is the two­way within regression estimates given by W = (X WNX)-1X WN y, then one can show, see Baltagi and Li (1991b), that

plimN,T 2 = plimN,T uW WNT uW /(N - 1)(T - 1) = plimN,Ttrace[WNT /(N - 1)(T - 1)] = 2

Similarly, from (3.12) and (3.13), one can show that plimN,T 2 = 2 and plimN,T  2 = 0.
This shows that if the uit 's are replaced by the two-way within residuals the misspecified two-way variance components estimates remain consistent.
Prucha (1984) showed that as long as the estimator of 2 is consistent and the estimators of the other variance components 2 and 2 go to a finite probability limit, as N and T both go to , then the corresponding feasible GLS estimator of  is asymptotically equivalent to the true GLS estimator. This condition is satisfied for the overspecified model but not for the underspecified model.
Deschamps (1991) investigated the consequences of a misspecified error com-
ponents model on the estimated variances of the regression coefficients. In par-
ticular, Deschamps (1991) considered the under-specified case where some error

70

B.H. Baltagi et al.

components are improperly omitted (even though their variances are nonzero), and the remaining variance components are consistently estimated. In this case, Deschamps (1991) shows that the true variances of the estimated regression coefficients are always underestimated in the misspecified model. For the underspecified one­way error components model, with omitted time effects, this inconsistency is unbounded, unless the matrix of regressors satisfies very restrictive assumptions.

3.3.2 Serial Correlation in the Disturbances
The classical error components disturbances assume that the only correlation over time is due to the presence in the panel of the same individual over several periods. This equicorrelation coefficient is given by correl (uit , uis) = 2 /(2 + 2) for t = s. Note that it is the same no matter how far t is from s. This may be a restrictive assumption for economic relationships, like investment or consumption, where an unobserved shock this period will affect the behavioral relationship for at least the next few periods. This type of serial correlation is not allowed for in the simple error components model. Ignoring serial correlation when it is present results in consistent but inefficient estimates of the regression coefficients and biased standard errors. This section introduces serial correlation in the it . We illustrate how one can estimate an autoregressive process of order one AR(1), as in the Lillard and Willis (1978) study on earnings.

3.3.2.1 The AR(1) Process

Lillard and Willis (1978) generalized the error components model to the serially cor-
related case, by assuming that the remainder disturbances (the it ) follow an AR(1) process. In this case i  i.i.d.(0, 2 ), whereas

it = i,t-1 + it

(3.17)

|  | < 1 and it  i.i.d.(0, 2). The i are independent of the it and i0  (0, 2/(1 - 2)). Baltagi and Li (1991a) derived the corresponding Fuller and
Battese (1974) transformation for this model. First, one applies the Prais-Winsten

(PW) transformation matrix





(1 - 2)1/2 0 0 · · · 0 0 0

C = 

- · · 0

1 0 ··· 0 · · ··· · · · ··· · 0 0 · · · -

0 · · 1

0 · · 0



0

0 0 · · · 0 - 1

3 Error Components Models

71

to transform the remainder AR(1) disturbances into serially uncorrelated classical errors. For panel data, this has to be applied for N individuals. The transformed regression disturbances are in vector form
u = (IN C)u = (IN CeT ) + (IN C) Using the fact that CeT = (1 - )T , where T = (, eT-1) and  =
(1 + )/(1 - ), one can rewrite this as u = (1 - )(IN  T ) + (IN C)
Therefore, the variance­covariance matrix of the transformed disturbances is
 = E(uu ) = 2 (1 - )2[IN  T T ] + 2(IN  IT )
since (IN C)E( )(IN C ) = 2(IN  IT ). Alternatively, this can be rewritten as
 = d22 (1 - )2[IN  T T /d2] + 2(IN  IT )
where d2 = T T = 2 + (T - 1) or equivalently,  = 2(IN  T T /d2) + 2(IN  (IT - T T /d2))
where 2 = d22 (1 - )2 + 2. Therefore
 -1/2 = ( / )(IN  T T /d2) + (IN  (IT - T T /d2)) = IN  IT -  (IN  T T /d2)
where  = 1 - ( / ). Premultiplying the PW transformed observations y = (IN  C)y by  -1/2
one gets y =  -1/2y. The typical elements of y =  -1/2y are given by

(yi1 -  i, yi2 -  i, . . . , yiT -  i)
where i = [(yi1 + T2 yit )/d2] for i = 1, . . . , N. The first observation gets special attention in the AR(1) error components model. First, the PW transformation gives it a special weight 1 - 2 in y. Second, the Fuller and Battese transformation also gives it a special weight  = (1 + )/(1 - ) in computing the weighted average i and the pseudo-difference. Note that
(i) if  = 0, then  = 1, d2 = T, 2 = 12 and  =  . Therefore, the typical element of yit reverts to the familiar (yit -  y¯i.) transformation for the one-way error component model with no serial correlation.
(ii) If 2 = 0, then 2 = 2 and  = 0. Therefore, the typical element of yit reverts to the PW transformation yit .

72

B.H. Baltagi et al.

The Best Quadratic Unbiased (BQU) estimators of the variance components are given by
2 = u (IN  (IN  T T /d2)u/N(T - 1) and 2 = u (IN  T T /d2)u/N
of 2 and 2 respectively. Baltagi and Li (1991a) suggest estimating  from Within residuals it as  =
Ni=1 tT=1 i,t i,t-1/ iN=1 tT=2 i2,t-1. Then, 2 and 2 are estimated by substituting OLS residuals u^ from the PW transformed equation using . Using Monte Carlo experiments, Baltagi and Li (1997) found that  performs poorly for small T and recommended an alternative estimator of  which is based on the autocovariance function Qs = E(uit ui,t-s). For the AR(1) model, it is easy to show that Qs = 2 + 2s. From Q0, Q1 and Q2, one can easily show that  + 1 = (Q0 - Q2)/(Q0 - Q1). Hence, a consistent estimator of  (for large N) is given by

 = Q0 - Q2 - 1 = Q1 - Q2

Q0 - Q1

Q0 - Q1

where Qs = Ni=1 tT=s+1 uit ui,t-s/N(T - s) and uit denotes the OLS residuals. 2 and 2 are estimated by substituting OLS residuals u^ from the PW transformed equation using  rather than .
Therefore, the estimation of an AR(1) serially correlated error components model is considerably simplified by

(i) applying the PW transformation in the first step, as is usually done in the timeseries literature, and
(ii) subtracting a pseudo-average from these transformed data in the second step.

3.3.2.2 Kmenta's Approach

(1) The Common  Case In this case the disturbances are assumed to follow a first order autoregressive process with the same  but different variances for different cross sections, i.e.,

uit = ui,t-1 + it

(3.18)

with it  N(0, 2i) and ui0  N(0, 2i/(1 - 2)). The estimation method proposed is to correct for serial correlation in the first step and heteroskedasticity in the
second step. This is accomplished by estimating  by  =   uit ui,t-1/  ui2,t-1 with uit denoting the OLS residuals on the pooled model. Next, the Prais­Winsten transformation is applied, i.e., yit = yit - yi,t-1 for t = 2, . . . , T , and yi,1 = (1 -
2)1/2yi,1 with a similar transformation on the Xit 's. yit is regressed on the Xit 's and the residuals uit 's are formed. Estimates of the variances are obtained as

3 Error Components Models

73

2i =  uit2/(T - K) for i = 1, 2, . . . , N, and yit = yit /i and Xit 's are formed. Finally yit is regressed on the Xit's. This procedure when iterated until convergence will lead to maximum likelihood estimates.

(2) The Varying  Case

Kmenta (1986) also suggested to consider cases where the serial correlation can

vary across individuals

uit = iui,t-1 + it

(3.19)

with it defined above. Maintaining the assumption of independence across individuals, the N i's are
then estimated by i =  uit ui,t-1/ ui2,t-1 for i = 1, 2, . . . , N, and the remaining steps are the same as above.
Kmenta (1986) also considered the situation where some correlation may exist
between individuals. In this case

E(uit u jt ) = i j for i, j = 1, 2, . . . , N ,

and E(it  jt ) = i j with i j = i j/(1 - i j). The variance­covariance matrix is now  = [i jVi j] where

 1

j



2 j

···



T -1 j



Vi j = 

i i2 ...

1 j 1 ...

... j

 .

iT -1 · · ·

i 1

Estimates of the i's and i j's are obtained as before with i j =  eit ejt /(T - K). Rather than applying GLS which inverts and NT × NT matrix, Kmenta (1986) suggests running GLS on the transformed model, i.e., using (y, X) as follows:

 = (X  -1X )-1(X  -1y)

where  =   IT ,  = [i j].  is N × N and if N is larger than T , which is the usual
case in economics, this  is singular. Also,  is not GLS since the Prais­Winsten transformation does not give the right first element of Vi j for i = j. Kmenta suggests ignoring the first observation, i.e., applying Cochrane­Orcutt. The transformation would be correct in this case but we lose N observations, one for each cross section. This could be a lot of observations lost for panels with large N.

3.3.3 Two-Way Error Components vs Kmenta's Approach
The usual error components model as well as the Kmenta technique allow for serial correlation, but in the usual error components model this serial correlation is

74

B.H. Baltagi et al.

constant across time, whereas it decays over time with the Kmenta technique. In its most general case, the Kmenta technique allows for correlation among the different cross sections, whereas the error components technique assumes a lot of independence among the 's,  's and 's. Moreover, the usual error components model has homoskedastic disturbances, whereas the Kmenta technique has heteroskedastic ones. Also, the Kmenta technique estimates a lot of auxiliary parameters, for e.g., N  's and one  in the case of the common rho method, N  's and N 's in case of the varying rho method, and N(N + 1)/2  's and N 's in the cross-sectionally correlated time-wise autoregressive case. In the fixed effects model, one estimates (N - 1) 's and (T - 1) 's, but in the two-way random effects model, one only estimates three variances.
The advantages of both methods are the gains from pooling a larger data set and more variation to explain the underlying economic relationship. However, as usual in economics, the true structure of the disturbances is not known, and the researcher may be at a disadvantage if the wrong error structure is chosen. Given this background, Baltagi (1986) posed the following basic question: Under the best possible situation of applying one technique of pooling, how does the other technique perform? This question is relevant given the wide use of the Kmenta and the error components techniques and their easy accessibility on computer. The performance of these methods is compared by means of Monte-Carlo experiments. First, data are generated with both serially correlated and cross-sectionally heteroskedastic disturbances and both the Kmenta and the familiar variance components methods are applied. Next, data are generated with error components disturbances, and again both techniques of estimation are applied. For N = 25 and T = 10, Baltagi (1986) shows that the error components procedure is more robust to this kind of misspecification than the Kmenta technique. This result should be tempered by the fact that N is large and T is small. This means, that, in the Kmenta case, one is estimating a lot of a auxiliary parameters with a short time series. If T is large and N is small, the Kmenta technique is expected to perform better. Most panels in economics, however, are of the type where N is much larger than T . Also, the Monte-Carlo results show that the error components Feasible GLS estimators differ from each other when the model is misspecified. Negative estimates of the variance components and non­stationary 's occur when the model is misspecified. Finally, OLS performs better than the wrong Feasible GLS estimator, but worse than the correct Feasible GLS estimator. Note that a robust variance­covariance matrix for the OLS estimator under the Kmenta model was proposed by Beck and Katz (1995).

3.3.4 Heteroskedasticity in the Disturbances
The standard error components model assumes that the regression disturbances are homoskedastic with the same variance across time and individuals. This may be a restrictive assumption for panels, where the cross-sectional units may be of varying

3 Error Components Models

75

size and as a result may exhibit different variation. For example, when dealing with different size countries or firms, one should expect to find heteroskedasticity in the disturbance terms. Assuming homoskedastic disturbances when heteroskedasticity is present will still result in consistent estimates of the regression coefficients, but these estimates will not be efficient. Also, the standard errors of these estimates will be biased and one should compute robust standard errors correcting for the possible presence of heteroskedasticity. In this section, we relax the assumption of homoskedasticity of the disturbances and introduce heteroskedasticity through the i as first suggested by Mazodier and Trognon (1978). Next, we suggest an alternative heteroskedastic error components specification, where only the it are heteroskedastic. We derive the true GLS transformation for these two models. We also consider two adaptive heteroskedastic estimators based on these models where the heteroskedasticity is of unknown form. These adaptive heteroskedastic estimators were suggested by Li and Stengos (1994) and Roy (2002).
Mazodier and Trognon (1978) generalized the homoskedastic error components model to the case where the i are heteroskedastic, i.e., i  (0, 2i) for i = 1, . . . , N, but it  i.i.d.(0, 2). In vector form,   (0,  ) where  = diag[2i] is a diagonal matrix of dimension N × N, and   (0, 2INT ). . . .. Therefore, the resulting variance­covariance of the disturbances is given by

 = diag[2i]  JT + diag[2]  IT

where diag[2] is also of dimension N × N. This can be rewritten as follows



=

diag[i2] 

JT T

+ diag[2]  (IT

-

JT T

)

with i2 = T 2i + 2. In this case,



-1/2

=

(diag[

/i]



JT T

)

+ WN

Hence, y =  -1/2y has a typical element yit = yit - iyi where i = 1 - ( /i) for i = 1, . . . , N.
Baltagi and Griffin (1988) provided Feasible GLS estimators including Rao's (1970, 1972) MINQUE estimators for this model. Phillips (2003) argues that this model suffers from the incidental parameters problem and the variance estimates of i (the 2i) cannot be estimated consistently, so there is no guarantee that Feasible GLS and true GLS will have the same asymptotic distributions. Instead, he suggests a stratified error components model where the variances change across strata and provides an EM algorithm to estimate it. It is important to note that Mazodier and Trognon (1978) had already suggested stratification in a two-way heteroskedastic error component model. Also, that one can specify parametric variance functions which avoid the incidental parameter problem and then apply the GLS transformation described above. As in the cross-section heteroskedastic case, one has to know the variables that determine heteroskedasticity, but not necessarily the form.

76

B.H. Baltagi et al.

Adaptive estimation of heteroskedasticity of unknown form has been suggested for
this model by Roy (2002). This follows similar literature on adaptive estimation for
cross-section data. Alternatively, one could keep the i homoskedastic with i  i.i.d.(0, 2 ) and
impose the heteroskedasticity on the it , i.e., it  (0, 2i) (see problem 88.2.2 by Baltagi (1988) and its solution by Wansbeek (1989)). In this case, one obtains

 = E(uu ) = diag[2 ]  JT + diag[2i]  IT

which can be rewritten as



=

diag[T 2

+ 2i] 

JT T

+ diag[2i]  (IT

-

JT T

)

and

-1/2

=

diag[1/i] 

JT T

+ diag[1/i]  (IT

-

JT T

)

and y = -1/2y has a typical element

yit = (y¯i/i) + (yit - y¯i)/i .

Upon rearranging terms, we get

yit

=

1 i (yit

- iy¯i)

where

i = 1 - (i/i)

Estimators for this one-way random effects model with unequal error variances and
no regressors has been studied extensively in the statistics literature, see Rao, Kaplan
and Cochran (1981) for a good review.
One can argue that heteroskedasticity will contaminate both i and it and it is hard to claim that it is in one component and not the other. Randolph (1988) gives
the GLS transformation for a more general heteroskedastic model where both the i and the it are assumed heteroskedastic in the context of an unbalanced panel. In this case, the var(i) = 2i and E( ) = diag[i2t ] for i = 1, . . . , N and t = 1, . . . , Ti. More recently, Li and Stengos (1994) considered the case where i  i.i.d. 0, 2 and E [it |xit ] = 0 with Var [it |xit ] =  (xit )  it . So that the heteroskedasticity is on the remainder error term and it is of an unknown form.
Therefore i2t = E u2it |xit = 2 +it and the proposed estimator of 2 is given by

NT

  uit uis

2

=

i=1 t=s
NT (T

- 1)

where uit denotes the OLS residual. Also,

3 Error Components Models

77

NT
  u2jsKit, js

it

=

j=1 s=1 NT

- 2

  Kit, js

j=1 s=1

where the kernel function is given by Kit, js = K

xit -x js h

and h is the smoothing

parameter. These estimators of the variance components are used to construct a feasible adaptive GLS estimator of  which they denote by GLSAD. The computation of their Feasible GLS estimator is simplified into an OLS regression using a re-

cursive transformation that reduces the general heteroskedastic error components

structure into classical errors, see Li and Stengos (1994) for details. Roy (2002) considered the alternative heteroskedastic model E [i|xi ] = 0 with

Var i|xi =  xi  i

T
with xi =  xit /T and vit  i.i.d. 0, v2 . So that the heteroskedasticity is on the
t=1

individual specific error component and it is of an unknown form. Roy (2002) used

the usual estimator of v2 which is the MSE of the Within regression, and this can

be written as

N


T


[(yit

- yi.) - (xit

- xi

) W ]2

2 = i=1 t=1 N (T - 1) - k

where W is the fixed effects or within estimator of  . Also

NT
  u2jt Ki., j.

i

=

j=1 t=1 NT

- 2

  Ki., j.

j=1 t=1

where the kernel function is given by

Ki., j. = K

xi - x j h

Using these estimators of the variance components, Roy (2002) computed a Feasible GLS estimator using the transformation derived by Baltagi and Griffin (1988). This was denoted by EGLS.
Both Li and Stengos (1994) and Roy (2002) performed Monte Carlo experiments based on the simple regression model with one regressor. They compared the following estimators:

78

B.H. Baltagi et al.

(1) OLS; (2) Fixed effects or within estimator (Within); (3) the conventional GLS estimator for the one way error components model that
assumes the error term uit is homoskedastic (GLSH); and (4) their own adaptive heteroskedastic estimator denoted by (EGLS) for Roy (2002)
and (GLSAD) for Li and Stengos (1994).
Li and Stengos (1994) found that their adaptive estimator outperforms all the other estimators in terms of relative MSE with respect to true GLS for N = 50, 100 and T = 3 and for moderate to severe degrees of heteroskedasticity. Roy (2002) also found that her adaptive estimator performs well, although it was outperformed by fixed effects in some cases where there were moderate and severe degrees of heteroskedasticity. Recently, Baltagi, Bresson and Pirotte (2005) checked the sensitivity of the two proposed adaptive heteroskedastic estimators under misspecification of the form of heteroskedasticity. In particular, they ran Monte Carlo experiments using the heteroskedasticity set up of Li and Stengos (1994) to see how the misspecified Roy (2002) estimator performs. Next, they used the heteroskedasticity set up of Roy (2002) to see how the misspecified Li and Stengos (1994) estimator performs. They also checked the sensitivity of these results to the choice of the smoothing parameters, the sample size and the degree of heteroskedasticity. Baltagi, Bresson and Pirotte (2005) found that in terms of loss in efficiency, misspecifying the adaptive form of heteroskedasticity can be costly when the Li and Stengos (1994) model is correct and the researcher performs the Roy (2002) estimator. This loss in efficiency is smaller when the true model is that of Roy (2002) and one performs the Li and Stengos (1994) estimator. The latter statement is true as long as the choice of bandwidth is not too small. Both papers also reported the 5% size performance of the estimated t-ratios of the slope coefficient. Li and Stengos (1994) found that only GLSAD had the correct size while OLS, GLSH and Within over-rejected the null hypothesis. Roy (2002) found that GLSH and EGLS had the correct size no matter what choice of h was used. Baltagi, Bresson and Pirotte (2005) found that OLS and GLSAD (small h) tend to over-reject the null when true no matter what form of adaptive heteroskedasticity. In contrast, GLSH, EGLS and Within have size not significantly different from 5% when the true model is that of Roy (2002) and slightly over-reject (7­8%) when the true model is that of Li and Stengos (1994).

3.4 Testing
As in any other econometric context, the properties of the estimators considered in this chapter rely on the validity of the assumptions made. It is then essential to check whether these assumptions can be considered as validated, or if they must be rejected. A first question to answer is whether there exist individual effects at all or not.

3 Error Components Models

79

3.4.1 Testing for the Absence of Individual Effects

Several tests have been proposed in the literature in order to check for the absence of individual (random) effects. We shall limit ourselves to three of them, which present the advantage of being quite simple to implement, and which nevertheless have rather good properties.

3.4.1.1 The Analysis of Variance / Fisher's Test

A first way of checking for the absence of individual effects consists of testing for the nullity of their variance 2 :

H0 : against H1 :

2 = 0 2 = 0.

This test is very easy to implement as long as we have run the Within and Between regressions from which we get the estimated residual variances w2 (= 2) and b2 = 2 + 2/T . Under the normality assumption,

(N

(T

-

1)

-

kw)

w2 2

is distributed as a 2 with (N(T - 1) - kw) degrees of freedom, and

(N

-

kb)

2

b2 + 2/T

=

(N

-

kb)

T

T 2

b2 + 2

is also distributed as a 2 but with (N - kb) degrees of freedom. As a consequence,

2 T b2 T 2 + 2 w2

 F(N - kb , N(T - 1) - kw) .

Then, under the null hypothesis, H0 : 2 = 0, we have:

T b2 2

 F(N - kb , N(T - 1) - kw) .

Consequently, one will reject H0 when this statistics is larger than the fractile of the Fisher distribution with (N - kb , N(T - 1) - kw) degrees of freedom. Simply stated, if T times the Between regression individual variance (computed over N
observations) is larger than the residual variance of the Within regression, one must
reject the absence of individual effects: such effects do exist.

80
3.4.1.2 The Lagrange Multiplier Test

B.H. Baltagi et al.

Breusch and Pagan (1979) have proposed to use the Lagrange multiplier test to check for the absence of individual effects. Their idea is that, when there are no such effects, the disturbances of the model are completely idiosyncratic. In that situation, the variance of the disturbances should not significantly differ from that of their individual means (i), once the necessary correction of the scale effect for the variance of a mean has been made. Then, under such an assumption, the statistics

NT g = 2(T - 1)

iN=1(T i)2 Ni=1 tT=1 i2t

-1

2

is asymptotically distributed as a 2 with 1 degree of freedom. Consequently, if this statistics, computed from the OLS regression residuals is greater than 3.84 (when testing at 5%), one will reject the null of absence of individual specific effects. In the opposite case, one should accept this assumption.

3.4.1.3 Honda's Test

A drawback of the above test as proposed by Breusch and Pagan (1979) is that

this is a two-sided test, while a variance should be either null or positive. In order

to circumvent this problem, Honda (1985) has suggested a very simple one-sided

test that just amounts to consider the square root of the Breusch­Pagan statistics.

He showed that, under the null of absence of individual effects, this square root is

distributed as a normal. Then, one should reject the null hypothesis as soon as the

statistics

g=

NT 2(T - 1)

Ni=1(T i)2 Ni=1 tT=1 it 2

-

1

is greater than 1.64. This test then leads to reject the null a bit more often than the Breusch­Pagan test would do.

3.4.2 Testing for Uncorrelated Effects: Hausman's Test
One of the most disputable assumptions underlying the error components model is the absence of correlation between the regressors and the individual effects. Indeed, in many circumstances, this is a quite untenable assumption. It is then important to check for the validity of this assumption as such a correlation would lead to the inconsistency of most of the estimators of the error components model,8 with the notable exception of the Within estimator. Since the latter is based on a transformation
8 The OLS, Feasible-GLS and Between estimators are biased and inconsistent when only N  . Feasible-GLS remain consistent when both N and T  .

3 Error Components Models

81

that discards the individual effects from the model, this makes the assumption of their uncorrelation with the regressors irrelevant to the unbiasedness and consistency of this estimator.
Hausman (1978) has suggested a test that exploits the fact that a couple of esti-
mators may be defined in such a way that one ( (1)) is consistent both under H0 and H1 while the other one ( (2)) is consistent and efficient only if H0 is true and inconsistent otherwise. Then, getting close estimates  (1) and  (2) is an indication that H0 is true while getting very different estimates  (1) and  (2) must be seen as an indication that H0 is not validated. More specifically, Hausman has shown that under H0.

QH = ( (1) -  (2)) [V ( (1)) -V (  (2))]-1( (1) -  (2))
is asymptotically distributed (when N  ) as a Chi-Squared with dim( ) degrees of freedom. If QH is larger than the fractile of the (2dim()) distribution, one must reject H0; while this assumption is accepted otherwise.
In our current context, we can choose the Within estimator as  (1) while the Feasible-GLS estimator is the choice to be made for  (2). Then, the statistics to be computed is given by

QH = (w - fgls) [V (w) - V ( fgls)]-1(w - fgls) .
If QH is greater than the fractile of a (2kw) where kw is the number of regressors in the Within regression, one should reject H0: the absence of correlation between the regressors and the individual effects must be rejected. As a consequence, while the Within estimator is consistent, the Feasible-GLS nor the other estimators (OLS and Between) are consistent in this case.
It is worthwhile noticing that this test can also be conducted in alternative ways. Indeed, Hausman and Taylor (1981) have shown that one can answer the same question by comparing

Qfgls,b = (b - fgls) [V (b) - V ( fgls)]-1(b - fgls) or, alternatively

Qw,b = (b - w) [V (b) +V ( w)]-1(b - w) . to a (2kw). Indeed, these three statistics are (Qs N  ) numerically identical.

3.4.3 Testing for Serial Correlation
In this section, we address the problem of jointly testing for serial correlation and individual effects. Baltagi and Li (1995) derived three LM statistics for an error components model with first-order serially correlated errors. The first LM statistic

82

B.H. Baltagi et al.

jointly tests for zero first-order serial correlation and random individual effects. The second LM statistic tests for zero first-order serial correlation assuming fixed individual effects, and the third LM statistic tests for zero first-order serial correlation assuming random individual effects. In all three cases, Baltagi and Li (1995) showed that the corresponding LM statistic is the same whether the alternative is AR(1) or MA(1).
Let us assume the disturbances to follow a one-way error components model where i  i.i.d.(0, 2 ) and the remainder disturbance follows a stationary AR(1) process: it = i,t-1 + it with |  |< 1, or an MA(1) process: it = it +  i,t-1 with |  |< 1, and it  i.i.d.(0, 2). The joint LM test statistic for H1a: 2 = 0;  = 0 is the same as that for H1b: 2 = 0;  = 0 and is given by

LM1

=

2(T

NT 2 - 1)(T

-

[A2 2)

-

4AB

+

2T

B2]

where u denote OLS residuals, A = [u (IN  JT )u/(u u)] - 1 and B = (u u-1/u u). This is asymptotically distributed (for large N) as 22 under H1a.
Note that the A2 term is the basis for the LM test statistic for H2: 2 = 0 assuming there is no serial correlation (see Breusch and Pagan, 1980 or Sect. 4.1.2). In fact, LM2 = NT /2(T - 1)A is asymptotically distributed (for large N) as N(0, 1) under H2 against the one-sided alternative H2 ; 2 > 0. Also, the B2 term is the basis for the LM test statistic for H3:  = 0 (or  = 0) assuming there are no individual effects (see Breusch and Godfrey, 1981). In fact, LM3 = NT 2/(T - 1)B is asymptotically distributed (for large N) as N(0, 1) under H3 against the one-sided alternative H3 ;  (or  ) > 0. The presence of an interaction term in the joint LM test statistic, emphasizes the importance of the joint test when both serial correlation and
random individual effects are suspected. However, when T is large the interaction
term becomes negligible.
Also, Baltagi and Li (1995) derived two extensions of the Burke, Godfrey and
Termayne (1990) AR(1) vs MA(1) test from the time-series to the panel data litera-
ture. The first extension tests the null of AR(1) disturbances against MA(1) distur-
bances, and the second the null of MA(1) disturbances against AR(1) disturbances
in an error components model. These tests are computationally simple requiring
only OLS or Within residuals.

3.4.4 Testing for Heteroskedasticity
Verbon (1980) derived a Lagrange multiplier test for the null hypothesis of homoskedasticity against the heteroskedastic alternative i  0, 2i and it  0, 2i . In Verbon's model, however, 2i and 2i are, up to a multiplicative constant, identical parametric functions of time invariant exogenous variables Zi, i.e., 2i = 2 f (Zi2) and 2i = v2 f (Zi1) . Lejeune (1996) on the other hand, dealt with maximum likelihood estimation and Lagrange multiplier testing of a

3 Error Components Models

83

general heteroskedastic one-way error components regression model assuming that

i  0, 2i and it  0, 2it where 2i and 2i are distinct parametric functions of exogenous variables Zit and Fi, i.e., 2i = 2h (Zit 1) and 2i = 2 h (Fi2). In the context of incomplete panels, Lejeune (1996) derived two joint LM tests

for no individual effects and homoskedasticity in the remainder error term. The

first LM test considers a random effects one-way error components model with

i  i.i.d. 0, 2 and a remainder error term that is heteroskedastic it  N 0, 2it with 2it = 2h (Zit 1) . The joint hypothesis H0; 1 = 2 = 0, renders OLS the restricted MLE. Lejeune's second LM test considers a fixed effects one-way error

components model where i is a fixed parameter to be estimated and the remainder error term is heteroskedastic with it  N 0, 2it and 2it = 2h (Zit 1) . The joint hypothesis is H0; i = 1 = 0 for all i = 1, 2, .., N. This again renders OLS to be the

restricted MLE.

Holly and Gardiol (2000) derived a score test for homoskedasticity in a one-way

error components model where the alternative model is that the i's are independent and distributed as N(0, 2i ) where 2i = 2 h (Fi2). Here, Fi is a vector of p explanatory variables such that Fi2 does not contain a constant term and h is a

strictly positive twice differentiable function satisfying h (0) = 1 with h (0) = 0

and h (0) = 0. The score test statistic for H0; 2 = 0, turns out to be one half the ex-

plained sum of squares of the OLS regression of (s^/s¯) - N against the p regressors

in

F

as

in

the

Breusch

and

Pagan

test

for

homoskedasticity.

Here

s^i

=

u^i

JT T

u^i

and

s = iN=1 s^i/N where u denote the maximum likelihood residuals from the restricted

model under H0; 2 = 0. This is a one-way homoskedastic error components model

with i  N(0, 2 ).

In the spirit of the general heteroskedastic model of Randolph (1988) and

Lejeune (1996), Baltagi, Bresson and Pirotte (2006) derived a joint Lagrange multi-

plier test for homoskedasticity, i.e., H0; 1 = 2 = 0. Under the null hypothesis, the

model is a homoskedastic one-way error components regression model. Note that

this is different from Lejeune (1996), where under his null, 2 = 0. Allowing for 2 > 0 is more likely to be the case in panel data where heterogeneity across the individuals is likely to be present even if heteroskedasticity is not. The model under the

null is exactly that of Holly and Gardiol (2000) but it is more general under the alter-

native since it does not assume a homoskedastic remainder error term. Next, Baltagi,

et al. (2006) derived an LM test for the null hypothesis of homoskedasticity of the

individual random effects assuming homoskedasticity of the remainder error term,

i.e., 2 = 0 | 1 = 0. Not surprisingly, they get the Holly and Gardiol (2000) LM test.

Last but not least, Baltagi et al. (2006) derived an LM test for the null hypothesis

of homoskedasticity of the remainder error term assuming homoskedasticity of the

individual effects, i.e., 1 = 0 | 2 = 0. Monte Carlo experiments showed that the

joint LM test performed well when both error components were heteroskedastic, and

performed second best when one of the components was homoskedastic while the

other was not. In contrast, the marginal LM tests performed best when heteroskedas-

ticity was present in the right error component. They yielded misleading results if

heteroskedasticity was present in the wrong error component.

84
3.5 Estimation Using Unbalanced Panels

B.H. Baltagi et al.

The presentation of the estimation and testing methods made above was assuming a balanced sample, i.e. that all individuals in the sample are observed over the same period of time. However, in practice, this is almost never the case. Some individuals disappear from the sample, others come in, some of them are absent at some dates, etc. Fortunately, all the methods above still apply with an unbalanced panel sets with only minor changes. As an illustration, let us consider the Feasible-GLS estimator. Following Baltagi (1985, 2005), one can write the corresponding regression as

yit + ( i - 1)yi = [xit + ( i - 1)xi ]  + uit + ( i - 1)ui

(3.20)

where

i

=

2 2 + Ti2

.

Then, the model transformation depends on the number Ti of observations of each individual i. It is then no more possible to estimate the variances 2 and 2 directly
from the Within and Between regressions.

However, the estimated residual variance from the Within regression, given by

w2 = N

1

NT
 [(yit - yi) - (xit - xi ) ]2

 Ti - N - kw i=1 t=1

i=1

still provides an unbiased and consistent estimate of 2. On the contrary, it is no more the case for the Between regression residual variance. The reason is that the
Between regression now relies on the individual means computed over Ti observations, which makes its disturbances become heteroscedastic. Indeed, we have

K
 yi = kxki + ui k=1

where ui = i + i, i = 1, . . . , N

(3.21)

with:

E( ui) = 0 but V ( ui) = 2 + 2/Ti .

The consequence of this heteroscedasticity is the (obvious) inconsistency of the
residual variance as an estimate of 2 + 2/Ti .However, the Between estimator of the coefficients ( ) is still unbiased and consistent in this case and thus, one can consistently estimate 2 by:

 2

=

N

1 - (K

+ 1)

N
[(yi
i=1

- xi

B)2

-

1 Ti

w2 ].

Other ways to proceed by generalizing the procedures proposed by Wallace and Hussain (1969), Amemiya (1971), Swamy and Arora (1972) along the lines

3 Error Components Models

85

suggested in Baltagi and Chang (1994) can be adopted as an alternative. Then, once
estimates of 2 and 2 are obtained, it is easy to transform the model as described above to get the Feasible-GLS estimator.

References
Amemiya T., 1971, The estimation of the variances in a variance-components model, International Economic Review 12, 1­13.
Arellano M., 1987, Computing robust standard errors for within-groups estimators, Oxford Bulletin of Economics and Statistics 49, 431­434.
Baillie R.T. and Baltagi B.H., 1999, Prediction from the regression model with one-way error components, Chap. 10 in C. Hsiao, K. Lahiri, L.F. Lee and H. Pesaran eds., Analysis of Panels and Limited Dependent Variable Models (Cambridge University Press, Cambridge), 255­267.
Baltagi B.H., 1985, Pooling cross-sections with unequal time-series lengths, Economics Letters 18, 133­136.
Baltagi B.H., 1986, Pooling under misspecification: Some Monte Carlo evidence on the Kmenta and the error components techniques, Econometric Theory 2, 429­440.
Baltagi B.H., 1988, An alternative heteroscedastic error component model, problem 88.2.2, Econometric Theory 4, 349­350.
Baltagi B.H., 2005, Econometric Analysis of Panel Data (John Wiley, Chichester). Baltagi B.H. and Chang Y.J., 1994, Incomplete panels: A comparative study of alternative estima-
tors for the unbalanced one-way error component regression model, Journal of Econometrics 62, 67­89. Baltagi B.H. and Griffin J.M., 1988, A generalized error component model with heteroscedastic disturbances, International Economic Review 29, 745­753. Baltagi B.H. and Li Q., 1991a, A transformation that will circumvent the problem of autocorrelation in an error component model, Journal of Econometrics 48, 385­393. Baltagi B.H. and Li Q., 1991b, Variance component estimation under misspecification, problem 91.3.3, Econometric Theory 7, 418­419. Baltagi B.H. and Li Q., 1995, Testing AR (1) against MA (1) disturbances in an error component model, Journal of Econometrics 68, 133­151. Baltagi B.H. and Li Q., 1997, Monte Carlo results on pure and pretest estimators of an error component model with autocorrelated disturbances, Annales D'E´ conomie et de Statistique 48, 69­82. Baltagi B.H., Bresson G. and Pirotte A., 2005, Adaptive estimation of heteroskedastic error component models, Econometric Reviews 24, 39­58. Baltagi B.H., Bresson G. and Pirotte A., 2006, Joint LM test for Heteroskedasticity in a one-way error component model, Journal of Econometrics, 134, 401­417. Beck N. and Katz, J., 1995, What to do (and not to do) with time-series-cross-section data in comparative politics, American Political Science Review 89, 634­647. Breusch T.S., 1987, Maximum likelihood estimation of random effects models, Journal of Econometrics 36, 383­389. Breusch T.S. and Godfrey L.G., 1981, A review of recent work on testing for autocorrelation in dynamic simultaneous models, in D.A. Currie, R. Nobay and D. Peel, eds., Macroeconomic Analysis, Essays in Macroeconomics and Economics (Croom Helm, London), 63­100. Breusch T.S. and Pagan A.R., 1979, A simple test for heteroskedasticity and random coefficient variation, Econometrica 47, 1287­1294. Breusch T.S. and Pagan A.R., 1980, The Lagrange multiplier test and its applications to model specification in econometrics, Review of Economic Studies 47, 239­253.

86

B.H. Baltagi et al.

Burke S.P., Godfrey L.G. and Termayne A.R., 1990, Testing AR(1) against MA(1) disturbances in the linear regression model: An alternative procedure, Review of Economic Studies 57, 135­145.
Deschamps P., 1991, On the estimated variances of regression coefficients in misspecified error components models, Econometric Theory 7, 369­384.
Fuller W.A. and Battese G.E., 1974, Estimation of linear models with cross-error structure, Journal of Econometrics 2, 67­78.
Goldberger A.S., 1962, Best linear unbiased prediction in the generalized linear regression model, Journal of the American Statistical Association 57, 369­375.
Hausman J.A., 1978, Specification tests in econometrics, Econometrica 46, 1251­1271. Hausman J.A. and Taylor W.E., 1981, Panel data and unobservable individual effects, Economet-
rica 49, 1377­1398. Holly A. and Gardiol, L., 2000, A score test for individual heteroscedasticity in a one-way error
components model, Chap. 10 in J. Krishnakumar and E. Ronchetti, eds., Panel Data Econometrics: Future Directions (North-Holland, Amsterdam), 199­211. Honda Y., 1985, Testing the error components model with non-normal disturbances, Review of Economic Studies 52, 681­690. Hsiao C., 1986, Analysis of Panel Data (Cambridge University Press, Cambridge). Kmenta J., 1986, Elements of Econometrics (MacMillan, New York). Lejeune B., 1996, A full heteroscedastic one-way error components model for incomplete panel: Maximum likelihood estimation and Lagrange multiplier testing, CORE discussion paper 9606, Universite Catholique de Louvain, 1­28. Li Q. and Stengos T., 1994, Adaptive estimation in the panel data error component model with heteroskedasticity of unknown form, International Economic Review 35, 981­1000. Lillard L.A. and Willis R.J., 1978, Dynamic aspects of earning mobility, Econometrica 46, 985­1012. Maddala G.S., 1971, The use of variance components models in pooling cross section and time series data, Econometrica 39, 341­358. Maddala G.S. and Mount T.D., 1973, A comparative study of alternative estimators for variance components models used in econometric applications, Journal of the American Statistical Association 68, 324­328. Mazodier P. and Trognon A., 1978, Heteroskedasticity and stratification in error components models, Annales de l'INSEE 30-31, 451­482. Mundlak Y., 1978, On the pooling of time series and cross-section data, Econometrica 46, 69­85. Nerlove M., 1971b, A note on error components models, Econometrica 39, 383­396. Phillips R.L., 2003, Estimation of a stratified error components model, International Economic Review 44, 501­521. Prucha I.R., 1984, On the asymptotic efficiency of feasible Aitken estimators for seemingly unrelated regression models with error components, Econometrica 52, 203­207. Randolph W.C., 1988, A transformation for heteroscedastic error components regression models, Economics Letters 27, 349­354. Rao C.R., 1970, Estimation of heteroscedastic variances in linear models, Journal of the American Statistical Association 65, 161­172. Rao C.R., 1972, Estimation variance and covariance components in linear models, Journal of the American Statistical Association 67, 112­115. Rao S.R.S, Kaplan J. and Cochran W.C., 1981, Estimators for the one-way random effects model with unequal error variances, Journal of the American Statistical Association 76, 89­97. Roy N., 2002, Is adaptive estimation useful for panel models with heteroscedasticity in the individual specific error component? Some Monte Carlo evidence, Econometric Reviews 21, 189­203. Swamy P.A.V.B. and Arora, S.S., 1972, The exact finite sample proporties of the estimators of coefficients in the error components regression models, Econometrica 40, 253­260. Taub A.J., 1979, Prediction in the context of the variance-components model, Journal of Econometrics 10, 103­108.

3 Error Components Models

87

Taylor W.E., 1980, Small sample considerations in estimation from panel data, Journal of Econometrics 13, 203­223.
Trognon A., 1993, Econome´trie des donne´es individuelles-temporelles, Cours polycopie´, ENSAE. Verbon H.A.A., 1980, Testing for heteroscedasticity in a model of seemingly unrelated regression
equations with variance components (SUREVC), Economics Letters 5, 149­153. Wallace T.D. and Hussain A., 1969, The use of error components models in combining cross-
section and time-series data, Econometrica 37, 55­72. Wansbeek T.J., 1989, An alternative heteroscedastic error components model, solution 88.1.1,
Econometric Theory 5, 326.

Chapter 4
Endogenous Regressors and Correlated Effects
Rachid Boumahdi and Alban Thomas

4.1 Introduction
There are several situations in econometric modeling where consistency of parameter estimates is questionable because some explanatory variables may be correlated with the model disturbances. Hence the fundamental exogeneity assumption for the regressors may not be supported by the data, with two implications. First, the source of this correlation might be investigated upon to propose possible corrections. Second, alternative but consistent estimators may be proposed.
One of the most well-known source of endogenous regressors is the case of simultaneous equations models, in which some of the regressors in a given equation are the dependent variables in others and consequently are correlated with the disturbances of the equation under consideration. Another cause of correlation between explanatory variables and model disturbances is when the former are subject to measurement errors. Chapter 9 provides a detailed treatment of simultaneity and measurement error issues in the case of panel data.
There is however an important reason why regressors may be endogenous in the context of panel data. As discussed in the preceding chapters, accounting for individual unobserved heterogeneity is usually done by incorporating random individual-specific effects to the usual idiosyncratic disturbances of the model. Consequently, regressors must be uncorrelated with these individual effects as well for consistent estimates to be obtained. This assumption of no-correlation has been widely criticized by many authors, among which Mundlak (1978).

Rachid Boumahdi Toulouse School of Economics, GREMAQ and LIHRE; Universite´ des Sciences Sociales de Toulouse, 21 Alle´e de Brienne, 31000 Toulouse, France, e-mail: boumahdi@univ-tlse1.fr
Alban Thomas Toulouse School of Economics, INRA; Universite´ des Sciences Sociales de Toulouse, 21 Alle´e de Brienne, 31000 Toulouse, France, e-mail: thomas@toulouse.inra.fr

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

89

c Springer-Verlag Berlin Heidelberg 2008

90

R. Boumahdi and A. Thomas

Consider for example an agricultural production model (crop yield response function) where output depends on a set of inputs (labor, fertilizer, etc.). It is likely that variables outside the scope of farmer's decisions are also impacting the final crop output: soil characteristics (slope, water reserve, etc.) and climatic conditions. Land marginal productivity as represented by soil characteristics is often very difficult to observe with precision, and is often supposed to be part of the farm specific effect. But because farmer's input choice is likely to depend on land productivity, observed input levels are likely to be correlated with the farmer specific effect. This is especially true for fertilizer and water inputs, whose application levels are likely to be negatively correlated with systematic soil fertility and permanent water reserve, respectively.
Another popular example is the case of an individual earning function (wage equation), where the logarithm of the wage rate is explained by variables related to occupation, experience, and education. However, expected marginal productivity of a worker depends on individual ability, which is partly unobserved. In particular, individual ability may positively influence working wages, as well as education level of the individual. If the latter is an explanatory variable in the wage equation while being partly correlated with unobserved ability, individual effects (unobserved ability) may then be correlated with regressors.
This chapter addresses the issue of correlated effects, and endogenous regressors in the case of panel data. We present the main estimation and testing procedures employed in a single-equation, linear panel-data context. Starting with a brief overview of error structures and model transformations (fixed effects, first and quasi differences), we present Instrumental Variable (IV) and Generalized Method of Moments (GMM) procedures for consistent and efficient estimation of static models. We devote a particular section to augmented linear models with time-invariant regressors and show how to identify model parameters. Estimation of this kind of models with IV or GMM is discussed, and we compare in particular the efficiency of these estimators, depending on the validity of a no-conditional-heteroskedasticity assumption. A way to measure instrument relevance in the context of panel data models estimated by instrumental-variables procedures is presented, based on single-parameter information. Estimation by Instrumental Variable of models including time-varying regressors only is also the subject of a section, where endogenous regressors can be of any nature (time-varying only or not). As dynamic panel data models will be the subject of Chap. 8, we do not deal with the vast literature on the subject, that has emerged since the seminal work of Anderson and Hsiao (1982) and Arellano and Bond (1991). We conclude this chapter by a brief presentation of unbalanced panel data models with correlated effects and endogenous regressors, including nested error component models.

4.2 Estimation of Transformed Linear Panel Data Models

Consider the linear panel data model:

yit = xit  + uit , i = 1, . . . , N ; t = 1, . . . , T,

(4.1)

4 Endogenous Regressors and Correlated Effects

91

where xit is a K × 1 vector regressors depending on individual i and time t except the first column of xit which is a vector of ones. The error term uit may contain unobserved individual heterogeneity components, as in the one-way error component specification, uit = i + it . We assume for most of the chapter that the sample is balanced, i.e., each cross-sectional unit has the same number of non missing observations (T ). The case of unbalanced panels will be briefly discussed in Sect. 4.7.
As discussed in Chaps. 2 and 3, a conditional (fixed effects) or a random effects approach will lead to similar results asymptotically under standard assumptions, among which exogeneity of the xit s. On the other hand, when the correlation between uit and some xit s in (4.1) is not accounted for, Ordinary or Generalized Least Squares estimators are not consistent. In this case, an easy way to cope with such endogeneity is simply to filter out this component. Such a strategy is applicable to a variety of error structures, as we now see.

4.2.1 Error Structures and Filtering Procedures

We present here basic transformations for eliminating the unobserved individual heterogeneity component in linear models. The motivation for such filtering in most cases comes from endogeneity issues, and in particular the fact that regressors are correlated with individual effects.
In most applications, the error component structure can be specified as a particular case of the following representation:

uit = i + t vi + it ,

(4.2)

where i and vi are unobserved heterogeneity terms, t is a time effect, and it is i.i.d. across individuals and time periods. Let 2 , v2 and 2 respectively denote the variance of i, vi and it . The most important special cases are:
Case 1. (One-way error component model) t = ¯ t. Case 2. (Two-way error component model) vi = v¯ i. Case 3. (Cross-sectional dependence Type I) i = ¯ i.
Case 1 is by far the most widely used specification. When t is constant across time periods, the error component structure reduces to i + ¯ vi + it  i + it (the one-way specification).
In case 2, t can represent a trend function or simply consist of (non-monotonic) time effects that impact all units in a similar way for a given time period. It may however be of interest in applications to consider heterogeneous trends, where the marginal impact of the common time shock t is individual-specific; this is obtained in case 3. In the general case of (4.2) where i and vi are allowed to vary across units, we have both heterogeneous intercepts and slopes on the time effects.
Let us examine model transformations to eliminate heterogeneous individual heterogeneity terms in each of the cases presented above.

92

R. Boumahdi and A. Thomas

For case 1, the most common practice is to wipe out i with the Within-group

(fixed effects) transformation, it - ¯i = (yit - y¯i) - (xit - x¯i)  , where y¯i denotes the

individual mean for unit i and variable yit . This equation provides a simple way of

obtaining consistent least squares estimation of  under the assumption of strong

exogeneity: E[(xit - x¯i) |is] = 0 s, t.

Alternatively, we may use the first-difference transformation uit = it = yit -

xit  = (yit - yi,t-1) - (xit - xi,t-1)  , and consistent estimation of  then obtains

under the assumption that E[xit |it , i,t-1] = 0, a somehow weaker assumption than

above. In vector form, we can use the T × (T - 1) submatrix LT for performing first

differences:





1 0 0 ··· 0 0

LT

=



-1 ... 0 0

1 ... 0 0

0 ... 0 0

··· ...
··· ···

0 ... -1 0

0 ...
1 -1



.

Using first differences introduces a moving-average serial correlation on the transformed residual (Arellano and Bover, 1995). To remove such a correlation, it is possible to use the Orthogonal deviation procedure:

 

yit

=

 T

T -t -t +

1

1 s=T

yit

-

T

-t

yis
s=t+1

,

(4.3)

i = 1, . . . , N t = 1, . . . , T - 1.
Whatever the transformation considered, be it within-group (fixed effects), first differences or orthogonal deviations, identification of parameter  is possible (except the constant term) because it is assumed that xit is time-varying. First differences and deviations from individual means allow one to obtain the same informa-
tion because operators QT (for fixed effects) and LT (for first differences) span the same column space, with QT = LT (LT LT )-1LT .
The choice between fixed effects and first differences, on the grounds of
efficiency, depends in practice on assumptions made on homoskedasticity assumptions as follows. Maintaining the strict exogeneity assumption E(it |xi, i) = 0,t = 1, . . . , T , where xi = (xi1, . . . , xiT ), if we further assume that E(ii |xi, i) = 2IT (no heteroskedasticity nor serial correlation), then fixed effects is the most efficient
estimator in the class of models satisfying these conditions. On the other hand, if we
replace the latter assumption by

E(ii |xi, i) = 2 IT -1,t = 2, . . . , T,

then it can be shown that the first-difference estimator is more efficient. This is the case when it follows a random walk.
In case 2, filtering of both individual and time effects can be achieved by means of a modified Within operator which simultaneously filters out time-invariant and

4 Endogenous Regressors and Correlated Effects

93

time-varying only components. We will discuss such transformation in detail in the
section on time-varying only regressors below.
The model corresponding to case 3 was suggested by Holtz-Eakin, Newey and Rosen (1988), Ahn, Lee and Schmidt (2001), Lillard and Weiss (1979). Unless t is constant across time periods, Within-group or first-difference transformations will fail to filter out the unobserved individual heterogeneity component i.
Define a new variable rt = t /t-1; substracting from the equation at time t its expression lagged one period and premultiplied by rt , we have

yit - rt yi,t-1 = (xit - rt xi,t-1) + it - rt i,t-1.

(4.4)

The transformed model using the Quasi-differencing technique is now a nonlinear equation with additional parameters to be estimated: rt, t = 2, 3, . . . , T . Interestingly, parameters associated with time-invariant regressors become identified with a nonlinear regression of (4.4). This is the only case of such identification for those parameters in transformed models of the kind presented here.
Consider now the general case (4.1). To eliminate both effects i and vi, it is necessary to use a double-transformation: first differences, and then quasi-differences:

yit - r~t yi,t-1 = ( xit - r~t xi,t-1)  + it - r~t i,t-1,

(4.5)

i = 1, 2, . . . , N, t = 3, 4, . . . , T , where

r~t = t / t-1 = (t - t-1)/(t-1 - t-2).

Such double transformation of the model has been suggested by Nauges and Thomas (2003) in the dynamic panel data context. Wansbeek and Knaap (1999) use a double first-difference transformation in the special case of a dynamic panel data model with a random trend with t = t (the random growth model, see Heckman and Holtz (1989)).
In what follows, we will mostly be working with the one-way error component model uit = i + it .

4.2.2 An IV Representation of the Transformed Linear Model

Most estimators for linear panel data models can be shown to derive from the following orthogonality condition in matrix form:

E A (TU) = 0



1 N

A

TY

=

1 N

A

T

X

,

(4.6)

where A is a NT × L matrix of instruments and T is a NT × NT matrix transfor-
mation operator. Let Q = INT - B and B = IN  (1/T )eT eT denote the Within and Between matrix operators respectively, where eT is a T vector of ones. The fixed effects estimator obtains with A = X and T = Q so that ^W = (X QX)-1X QY because Q is idempotent. In the one-way model, the GLS estimator obtains with A = X and

94

R. Boumahdi and A. Thomas

T =  -1/2 so that ^GLS = (X  -1X)-1X  -1Y , where the covariance matrix of U =  +  is:



= 12Q + 22B,

 -1 = (1/12)Q + (1/22)B,



-

1 2

= (1/1)Q + (1/2)B,

(4.7)

where 12 = 2, 22 = 2 + T 2 . Under the strict exogeneity assumption and assuming that the error structure is
correctly represented, consistent estimates are obtained from moment conditions as in (4.6). Therefore, most popular estimators for linear panel data models can be represented in a IV form.
Depending on assumptions made on the error structure and the choice of the instrument matrix, estimators can be either inconsistent of inefficient, and it is therefore important to test for the validity of conditions underlying the construction of the estimator. To disentangle model misspecification due to an invalid set of instruments from an invalid transformation matrix, different specifications should be tested. Estimates constructed from either the same A but a different T , or the opposite, can be used to form a series of specification tests.
As presented in Chap. 3, the Generalized Least Squares (GLS) estimator may be selected on the grounds of efficiency in the case of the one-way linear panel data model, if assumptions underlying the random-effects specification are valid (in particular, strict exogeneity of the xit s). If however, E(ixit ) = 0, then GLS is not consistent, and fixed effects (or any transformation filtering out unobserved individual effects) should be used instead.
A very simple specification test is the Hausman exogeneity test, constructed as follows (Hausman, 1978). The null hypothesis to test is: H0 : E(xit i) = 0 i, t, and we have two estimators available. ^1 (e.g., the GLS) is consistent are efficient under the null, and inconsistent otherwise, while the fixed effects estimator ^W is consistent under the null and under the alternative, but is not efficient (under the null).
The Hausman test for linear panel data is based on the fact that, under H0, both estimators should be asymptotically equivalent, ^1 being more efficient. The test statistic is

HT = ^W - ^1 Var(^W ) - Var(^1) -1 ^W - ^1 2(K~ ),

where K~ is the column dimension of ^W . Note that ^1 and ^W must have the same dimension, i.e., parameters identified with the fixed effects procedure. Also, the weighting matrix Var(^W ) - Var(^1) is always semidefinite positive because ^1
is more efficient than Within under the null.
Finally, concerning the interpretation of the number of degrees of freedom of the test, the Within estimator is based on the condition E(X QU) = 0, whereas ^1 is based on a larger set of moment conditions. This is in fact the origin of the difference
in efficiency between both estimators. In the case of GLS, the set of conditions is E(X -1U) = 0  E(X QU) = 0 and E(X BU) = 0, and we therefore add K additional conditions (in terms of B), which is the rank of X.

4 Endogenous Regressors and Correlated Effects

95

It is important to note at this stage that both cases considered up to now are rather polar (extreme) cases: either all of the explanatory variables are endogenous, or neither of them is.
If we do not wish to maintain the assumption that all regressors are correlated with individual effects, an alternative estimation method may be considered: TwoStage Least Squares (2SLS) or Instrumental Variable (IV) estimation. Recall that in a cross-section context with N observations, the model would be:

Y = X + , E(X ) = 0, E(A ) = 0,

(4.8)

where A is a N × L matrix of instruments. If K = L, the orthogonality condition is

A (Y - X ) = 0  (A Y ) = (A X) ,

(4.9)

and the IV estimator is ^ = (A X)-1A Y . If L > K, the model is over-identified (L conditions on K parameters). For any matrix A, let P[A] = A(A A)-1A be the projection onto the column space of A. We can construct the quadratic form (Y - X ) P[A](Y - X ) and the IV estimator is ^ = (X P[A]X)-1(X P[A]Y ).
In the cross section context, instruments A originate outside the structural equa-
tion. In panel data models however, as we will see below, the advantage is that
instruments (not correlated with the individual effect) can be obtained directly. An-
other important difference in practice is that, when dealing with panel data, spherical
disturbances can no longer be assumed.

4.3 Estimation with Time-Invariant Regressors
4.3.1 Introduction
When considering estimation of a model with correlated effects, two arguments are in favor of yet another estimation procedure than Fixed Effects. First, one can sometimes obtain more efficient parameter estimates than the Within. Second, using the Within estimator does not enable us to estimate parameters associated to time-invariant explanatory variables. Indeed, as the estimator is built upon differentiating all variables with respect to individual means, then all variables which are individual-specific are dropped from the equation to be estimated.
For these reasons, an estimation method based on instrumental variables is called for. As we will show, Instrumental-Variables (IV) estimators yield more efficient estimators than the Within procedure, while allowing identification of all parameters in the model. To motivate its use, we are going to present in this section an augmented model, in which some of the explanatory variables may be endogenous, and some regressors are not time-varying but only individual-specific. Including individual-specific variables zi is indeed important from an empirical perspective, as many samples contain important information on individuals, which does not vary

96

R. Boumahdi and A. Thomas

over time (e.g., sex, education completed, place of residence if individuals have not moved during the whole sample period).
Hausman and Taylor (1981) ­ hereafter HT ­ consider the following model:

yit = xit  + zi + i + it , i = 1, · · · , N; t = 1, · · · , T,

(4.10)

where it is assumed to be uncorrelated with xit , zi and i while the effects i may be correlated with some explanatory variables in xit and/or zi.
Stacking all NT observations we can write (4.10) as: Y = X +Z + +, where Y is NT × 1, X is NT × K, Z is NT × G,  and  are NT × 1 respectively. If X and Z are uncorrelated with , the Generalized Least Squares (GLS) estimator yields
consistent and efficient parameter estimates:

-1

^ GLS =

1 12



Q

+

1 22



B

1 12



QY

+

1 22



BY

,

(4.11)

where  = [X, Z] and  = [ ,  ]. This estimator may generally be found more sim-

ply

computationally

by

first

transforming

X,

Z

and

Y

to

Y

=



-

1 2

Y,

X

=



-

1 2

X

and

Z

=



-

1 2

Z

and

then

estimating



and



from

the

Ordinary

Least

Squares

(OLS) regression of Y  on X and Z. The estimated variance­covariance matrix of

the GLS estimator ^GLS is:

-1

V (^ GLS) = ^2

1 ^12



Q

+

1 ^22



B

,

(4.12)

where ^2 = ^12 = u^W u^W /(NT - K - G), ^22 = u^Bu^B/(N - K), u^W and u^B are the within and the between residual respectively.

4.3.2 Instrumental Variable Estimation

Following HT, we partition X and Z as follows:

X = [X1, X2] and Z = [Z1, Z2],

where X1 is NT × k1, X2 is NT × k2, Z1 is NT × g1 and Z2 is NT × g2, so that the model in matrix form is

Y = X11 + X22 + Z11 + Z22 +  + .

(4.13)

HT distinguish columns of X and Z which are asymptotically uncorrelated with  from those which are not. They assume, for fixed T and N  , that

1 plim
N

(BX1)



=

0,

1 plim
N

(BX2)



=

0,

1 plim
N

Z1

=

0,

1 plim
N

Z2

=

0.

4 Endogenous Regressors and Correlated Effects

97

The way to estimate model (4.13) using an IV procedure is to rely on the exogeneity conditions above to construct a matrix of instruments. However, the method used differs from the standard one in simultaneous-equations literature. In the latter, a single equation is often estimated, which incorporates some endogenous variables among the regressors. All exogenous variables in the system are used as instruments, that is, exogenous variables not entering the equation of interest are also accounted for. In our case however, all the information is already contained in the single equation, meaning that we are able to construct instruments from variables in (4.13) alone. To see this, note that we are looking for instrument variables not correlated with the individual effect . There are three ways such instruments may be found. First, exogenous variables X1 and Z1 are readily available because of the exogeneity conditions given above. Second, we may also obtain additional instruments through transformations of the original exogenous variables, because such transformations will also be exogenous. Third, we may consider as well transformations of endogenous variables, provided these transformations are not correlated with .
An important aspect of panel data methods is that required transformations are very easily obtained through the use of matrices Q and B defined before. Matrix B calculates individual means of variables across all time periods, leaving the individual component unchanged. Therefore BX1 is clearly applicable as an instrument, whereas BX2 would not be, because endogeneity in X2 comes through the individual component which is correlated with . The Q matrix operates differentiation from individual means, filtering out the individual component. Therefore, QX1 and QX2 are also valid instruments, although the original X2 variable is endogenous.
These considerations led HT to propose an IV estimator for a model corresponding to our (4.14). Their instrument matrix AHT is the following:

AHT = (AH1 T, AH2 T), where A1HT = (QX1, QX2) and A2HT = (BX1, Z1). We can show that:

P[AHT] = AHT(AHT AHT)-1AHT = P[A1HT] + P[A2HT].

(4.14)

To compute the efficient HT estimator we transform (4.13) by premultiplying

it

by



-

1 2

,

so

that

the

error

term

will

have

a

diagonal

covariance

matrix.

Using

HT instruments AH1 T = (QX1, QX2) and AH2 T = (BX1, Z1), the IV estimator can be

written as:

-1

^ IV =

1 12



P[AH1 T]

+

1 22



P[AH2 T]

1 12



P[A1HT]Y

+

1 22



P[A2HT]Y

,

(4.15)

and its variance­covariance matrix is

-1

Var (^IV) = ^2

1 12



P[A1HT]

+

1 22



P[AH2 T]

.

(4.16)

98

R. Boumahdi and A. Thomas

Breusch, Mizon and Schmidt (1989) -hereafter BMS- show that this is equivalent to using the alternative instrument matrices AHT, CHT and DHT defined as follows

AHT = (AH1 T, or CHT = (C1HT, or DHT = (DH1 T,

A2HT), C2HT), DH2 T),

A1HT = (QX1, QX2), C1HT = (Q), D1HT = (QX1, QX2),

A2HT = (BX1, Z1) C2HT = (X1, Z1) DH2 T = (X1, Z1).

We will not enter into too much detail about these equivalences (see BMS, 1989 for more). Note however that the superiority of IV over Within estimators is easily seen, as far as the estimation of parameters  is concerned. The fixed effects procedure amounts to using the Q matrix as a single instrument. As it is well known that an IV estimator is more efficient when we add instruments, it is clear that the Hausman­Taylor estimator is more efficient than the Within estimator, since it entails (BX1, Z1) as additional instruments.
A final difficulty with IV estimators concerns estimation of variance components, because endogeneity of some regressors will yield inconsistent estimates of 2 and 2 if the standard Feasible GLS procedure is used. Hausman and Taylor (1981) describe a method for obtaining consistent estimates. Let ^ denote the Within residual averaged over time periods:

^ = BY - BX^W = (B - BX(X QX)-1X Q)Y = Z +  + B - BX(X QX)-1X Q.

(4.17)

If the last three terms in the equation above are treated as zero-mean residuals, then OLS and GLS estimates of  will be inconsistent. However, consistent estimation is possible if the columns of X1 provide sufficient instruments for the columns of Z2. A necessary condition is that k1 g2. The IV estimator of  is

-1
^B = Z P[R]Z Z P[R]^ ,

(4.18)

where R = (X1, Z1). Now, using parameters estimates ^W and ^B, one forms the

residuals

u^W = QY - QX^W and u^B = BY - BX^W - Z^B.

(4.19)

These two vectors of residuals are finally used in the computation of the variance components as follows.1

^2

=

u^W u^W NT -N

and

^2

=

u^Bu^B N

-

1 T

^2

4.3.3 More Efficient IV Procedures
The Hausman­Taylor IV procedure has proved very popular, because of its relative computational simplicity and intuitive appeal. Since then however, there has been
1 For details, see Hausman and Taylor (1981), p. 1384.

4 Endogenous Regressors and Correlated Effects

99

several improvements along its lines which led to more efficient estimation procedures.
The instruments used by Hausman and Taylor require only minimal exogeneity assumptions on variables, i.e., BX1 and Z1 are not correlated with the individual effect. As a consequence, this estimator may not be the most efficient if exogeneity conditions can be made more restrictive. Amemiya and MaCurdy (1986) ­ hereafter AM ­ suggested a potentially more efficient estimator by assuming that realizations of X1 are not correlated with  in each time period, i.e., for all t = 1, . . . , T and N   they assume that plim(1/N)x1it i = 0. Consequently, we may not only use BX1 as an instrument for individual i at time t, but also the whole series (x1,i1, x1,i2, . . . , x1,iT ). AM define the following NT × T k1 matrix:
X1 = vec eT  x1,i = eT  x1,1, . . . , eT  x1,N , where x1,i = (x1,i1, . . . , x1,iT ) ,
which is such that QX1 = 0 and BX1 = X1. Their instrument matrix is AAM = (AA1 M, A2AM), where A1AM = (QX1, QX2) and AA2 M = (X1, Z1). An equivalent estimator obtains by using the matrix CAM = (C1AM,C2AM), where C1AM = (QX1, QX2) and C2AM = [(QX1), BX1, Z1], (QX1) is constructed the same way as X1 above.
These authors suggest that their estimator is at least as efficient as Hausman­ Taylor if individual effects are not correlated with regressors X1 for each time period.
Note that the AM estimator differs from HT estimator only in its treatment of X1. In fact, A1HT = A1AM and C2AM = ((QX1), BX1, Z1) differs from AH2 T = (BX1, Z1) only by using (QX1). In other words, HT use X1 as two instruments namely QX1 and BX1 whereas AM use each such variable as T + 1 instruments: (QX1) and BX1.
Finally, a third IV method was described in BMS. Following these authors, if the variables in X2 are correlated with effects only through a time-invariant component, then (QX2) would not contain this component and (QX2) is a valid instrument. Their estimator is thus based on the following instrument matrix : ABMS = (AB1 MS, AB2 MS), where AB1 MS = (QX1, QX2) and AB2 MS = [(QX1), (QX2), BX1, Z1]. The estimated variance­covariance matrix of the IV estimator ^IV has the same form as in (4.16), where ^u2 = u^IVu^IV/(NT - K - G) and u^IV is the IV residual.
The Hausman test statistic can be used to check for the vality of the alternative IV estimators described above. The HT-IV estimator can first be compared with the fixed effects, to check that exogeneity assumption on X1 and Z1 are valid? If this is the case, then the more efficient procedures of AM-IV and BMS-IV can be compared with HT-IV to check that additional assumptions described above are supported by the data. See Cornwell and Rupert (1988) for an illustration of these test procedures.

4.4 A Measure of Instrument Relevance
It may be interesting in practice to investigate the performance of instruments in terms of efficiency of IV estimators on an individual-regressor basis. Cornwell and Rupert (1988) and Baltagi and Khanti-Akom (1990) have investigated efficiency

100

R. Boumahdi and A. Thomas

gains of instrumental variable estimators by fitting a wage equation on panel data and applying the methods proposed by HT, AM and BMS. Cornwell and Rupert (1988) found that efficiency gains are limited to the coefficient of time-invariant endogenous variables Z2.
However, Baltagi and Khanti-Akom (1990) using the canonical correlation coefficient for comparing different sets of instrumental variables found that efficiency gains are not limited to the time-invariant variable. They also show that the geometric average of canonical correlations increases as one moves from HT to AM, and then from AM to BMS. In fact, the canonical correlations only measure instrument relevance for the group of endogenous regressors taken as a whole, but cannot be used to measure how a particular group of instruments affects relevance for one endogenous regressor as opposed to another.
More recently, Boumahdi and Thomas (2006) have extended the method proposed by Shea (1997) and Godfrey (1999) to the case of panel data. This method allows for measuring instrument relevance for separate endogenous regressors. Following Shea (1997) and Godfrey (1999), we consider estimation of a single parameter by rewriting the augmented model Y = X + Z +  +  as

Y = M +  +  = M11 + M22 + ,

(4.20)

where M = [X, Z] and  = [ ,  ], M1 is NT × 1 and M2 is NT × (K + G - 1).

Define M1 = (INT - PM2 )M1, M1 = (INT - PM^2 )M^ 1 and M^ j = PAMj, j = 1, 2

where A is the matrix of instruments. In our panel data model, 1 would for example

correspond

to

the

first

variable

in

-

1 2

X2

.

These

definitions

imply

that

M1M1

=

M1M1. Using the same idea as in Shea (1997) and Godfrey (1999) in the case of a linear multiple regression model, we can use as a measure of instrumental variable

relevance, the population squared correlation between M1 and M1 for the model:

2

p2 = plim

M1M1

= plim M1M1 .

M1M1 (M1M1)

M1M1

(4.21)

In applied work, provided N tends to infinity, we can approximate plim M1M1/M1M1 by the following coefficient

R2p

=

M1M1 . M1M1

(4.22)

It is not necessary in practice to compute the above expression, because the coefficient R2p is directly related to the estimated parameter standard errors. To see this,
consider the estimated variance of the first component of GLS and the corresponding component in IV:

V 1GLS = 2GLS M1M1 -1 ,

V

1IV

= 2IV

-1
M1M1 .

4 Endogenous Regressors and Correlated Effects

101

Then, R2p can be written as

R2p

=

2IVV 2GLSV

1GLS 1IV

= M1M1 . M1M1

(4.23)

Consequently, the measure of instrumental variable relevance can be directly obtained by inspecting individual parameter (squared) standard errors.

4.5 Incorporating Time-Varying Regressors

Wyhowski (1994), Boumahdi and Thomas (1997) have extended the augmented model by incorporating time-varying regressors, i.e., variables which are not individual-specific, only time-period-specific. Think for example of a wage equation depending on individual-specific variables such as sex and education, and on time-varying regressors such as unemployment rate, economy-wide growth rate, etc. The intuition behind such a model would be that all individuals are affected by macro-economic variables the same way on average. Consider the two-way error component model as case 2 defined above:

uit = i + t + it .

(4.24)

The extended model we are considering is now the following:

yit = xit  + zi + wt  + i + t + it , i = 1, · · · , N; t = 1, · · · , T, (4.25)

where xit is a K × 1 vector of time-varying explanatory variables, zi is a G × 1 vector of time-invariant explanatory variables, and wt is a H × 1 vector of individualinvariant explanatory variables. Unobserved effects i and t are assumed to have zero mean and variances 2 and 2 respectively. We assume further that E(it ) = 0, E(it is) = 2 for t = s, E(it is) = 0 otherwise and E(iit ) = E(t it ) = 0 i, t.
Stacking all NT observations we can write the model in a compact form as:

Y = X +Z +W + + +.

(4.26)

Let us introduce some notation for this model. As before, B is the Between ma-
trix transforming variables into their means across periods (individual means); we now define B¯ as a matrix transforming a variable into its mean across individuals (time mean). Hence, BY is time-invariant and individual-specific, whereas B¯ is
time-varying and independent from individuals. Let

1 B = IN  T eT eT ,

B¯

=

1 N

eN eN

 IT

,

Q = INT - B - B¯ + J,

J

=

1 NT

eNT eNT

=

BB¯.

102

R. Boumahdi and A. Thomas

The new matrix Q allows to differentiate a given variable according to both its time

and individual means. The J operator performs the total mean of a variable, i.e., JX

is

a

NT

×1

matrix

with

the

same

argument

1 NT

Ni=1 tT=1

Xit .

With

this

notation,

the

variance­covariance matrix of the error term U reads:

 = 12S1 + 22S2 + 32S3 + 42J,

(4.27)

where 12 = 2, 22 = S1 = IT N - S2 - S3 - J,

2 + T 2 , S2 = B - J,

32 = 2 + S3 = B¯ - J,

N2, 42 SkSl = 0

= 2 + N2 + and JJ = J for

T 2 , l=k

and k, l = 1, 2, 3.

It is easy to show that2

-1 = (1/12)S1 + (1/22)S2 + (1/32)S3 + (1/42)J,

(4.28)

and

-

1 2

=

(1/1)S1 + (1/2)S2 + (1/3)S3 + (1/4)J.

(4.29)

If we assume that X, Z and W are uncorrelated with  and  , then model parameters can be estimated by GLS as follows:

  ^GLS =

3 k=1

1 k2



Sk

-1

3 k=1

1 k2



SkY

,

(4.30)

where  = [X, Z,W ] and  = [ ,  ,  ].

4.5.1 Instrumental Variables Estimation
Following HT and Wyhowski (1994) we allow for correlation between a subset of (X, Z,W ) and (,  ), and we partition X, Z and W as follows:
X = (X1, X2, X3, X4), Z = (Z1, Z2) and W = (W1, W2).
Their dimensions are denoted as follows: k1, k2, k3, k4, g1, g2, h1 and h2 for X1, X2, X3, Z1, Z2, W1 and W2 respectively. Furthermore, we assume that X1 is not correlated with  and  , X2 is correlated with  but not  , X3 is correlated with  but not , X4 is correlated with both  and .3
However, Z1 and W1 are assumed uncorrelated with  and  respectively. In other words and following Wyhowski (1994), we assume that, for T fixed and N - :
plim (S2X1)  = 0, plim (S2X3)  = 0, plim (S2Z1)  = 0,
and, for N fixed and T - :
plim (S3X1)  = 0, plim (S3X2)  = 0, plim (S3W1)  = 0.
2 We can also show that  = 2INT + T 2 + N2. 3 Boumahdi and Thomas (1997) have considered another partition of X, Z and W .

4 Endogenous Regressors and Correlated Effects

103

Under this assumption, we can use as an appropriate instrument set:

AHT = (A1HT, A2HT, A3HT),

where AH1 T = (S1X ), A2HT = (S2X1, S2X3, S2Z1) and A2HT = (S3X1, S3X2, S3W1). Then the HT estimator can be written as:

^IV =

  3
k=1

1 k2



P[AHk T]

-1

3 k=1

1 k2



P[AHk T]Y

,

(4.31)

where AHk T is the matrix of instruments. The order condition for existence of the estimator can be obtained by counting instruments and parameters to be estimated.
For parameters  we must have:

K + k1 + k3 + g1  K + G or k1 + k3  g2, and for parameters  , we must have:

K + k2 + h1  K + H or k1 + k2  h2,

where K = k1 + k2 + k3 + k4, G = g1 + g2 and H = h1 + h2. Now, if we assume that plim (S2X1)  = 0 and plim (S2X3)  = 0, t = 1, . . . , T ,
and following AM, X1 and X3 can be used as two instruments: (S1X1, S1X3) and X1, X3. X1 is the NT × T k1 matrix defined as in the one-way AM case presented above, and

X3 = vec eT  x3,i = eT  x3,1, . . . , eT  x3,N , where x3,i = (x3,i1, . . . , x3,iT ) .

Furthermore, if we assume that plim (S3X1)  = 0 and plim (S3X2)  = 0 for

each i, i = 1, . . . , N, then X1 and X2 can be used as two instruments (S1X1, S1X2) and X10, X30, where









X1,11 X1,21 . . . X1,N1

x2,11 x2,21 . . . x2,N1

X10

=



... X1,1T
X1,11 ... X1,1T ... ... ...
X1,11 ...

... X1,2T
X1,21 ... X1,21 ... ... ...
X1,21 ...

... ... . . . X1,NT
. . . X1,N1 ... ... . . . X1,N1 ... ... ... ... ... ...
. . . X1,N1 ... ...

 and

X20

=



... x2,1T
x2,11 ... x2,1T ... ... ...
x2,11 ...

... x2,2T
x2,21 ... x2,21 ... ... ...
x2,21 ...

... ... . . . x2,NT
. . . x2,N1 ... ... . . . x2,N1 ... ... ... ... ... ...
. . . x2,N1 ... ...

 .

X1,1T X1,2T . . . X1,NT

x2,1T x2,2T . . . x2,NT

104

R. Boumahdi and A. Thomas

In this case, the AM instruments can be defined as follows:

AAM = (AA1 M, A2AM, AA3 M), where AA1 M = (S1X ) = A1HT, A2AM = [AH2 T, (S¯X1, S¯X3)] and AA3 M = [AH3 T, (S~X1, S~X2)0], S¯ = INT - S2 - J and S~ = INT - S3 - J. The order condition for  becomes:

K + k1 + k3 + g1 + (T - 1)(k1 + k3)  K + G or T (k1 + k3)  g2, and for parameters  , we must have:

K + k2 + h1 + (N - 1)(k1 + k2)  K + H or N(k1 + k2)  h2.

Now, and following BMS, if X2, X4 are correlated with the individual effect  only through a time-invariant component, and if (X3, X4) are correlated with the time effect  only through a individual-invariant component, the BMS-like instruments are equivalent to the expanded instruments sets:

ABMS = (AB1 MS, AB2 MS, A3BMS),

where A1BMS = (S1X ) = AH1 T = A1AM, AB2 MS = [AA2 M, (S¯X2, S¯X4)] and AB3 MS = [A3AM, (S~X3, S~X4)0]. The order condition for these instruments is T (k1 + k3) + (T-1) (k2 + k4)  g2 for  and N(k1 + k2) + (N - 1)(k3 + k4)  h2 for  .
In order to compute ^IV, we must first estimate parameters 12, 22 and 32. To do this, we can use a consistent estimate of  ,  and , and estimates of the variance components derived from these estimators will be used below for estimating ^IV.
We can summarize the complete procedure as follows:
· Compute the within estimator ^W = (X S1X)-1(X S1Y ) and form the vector of residuals u^w = S1Y - S1X^W to compute

^12 = ^2 = (u^wu^w)/(N - 1)(T - 1) - K.

(4.32)

· Regress S2Y - S2X^W on PA2 Z to get a consistent estimate ^IV and form the residuals vector u^2 = S2Y - S2X^W - S2Z^IV. We can show that for fixed T and

N - :

plim (u^2u^2/N) = 22.

· Regress S3Y - S3X^W on PA3 Z to get a consistent estimate ^IV and form vector of residuals u^3 = S3Y -S3X^W -S3W ^I. We can show that for fixed N and T - :

plim (u^3u^3/T ) = 32.

4.6 GMM Estimation of Static Panel Data Models
The way to deal with correlated effects using an IV procedure is to construct orthogonality conditions from the model residual and instruments such as those presented above (HT, AM, BMS), those instruments being assumed uncorrelated

4 Endogenous Regressors and Correlated Effects

105

with the disturbances (at least asymptotically), and asymptotically correlated with explanatory variables. Consistent parameter estimates are then obtained under the assumption that the model is correctly specified (i.e., that orthogonality conditions are valid), by minimizing a quadratic form in orthogonality conditions (moment restrictions). Depending on the way this criterion is constructed, we obtain either either the Instrumental Variables (IV) under various forms, or the Generalized Method of Moments estimator (GMM, see Hansen (1982)). We now turn to the application of GMM estimation to linear panel data models.

4.6.1 Static Model Estimation

We consider here the general form of Instrumental Variable and GMM estimators
for the static model introduced above, Y = X + Z + U, or in a compact form,
Y =  +U where  = (X, Z) and  = ( ,  ). Let E(AiUi) = 0 denote a L set of orthogonality conditions in vector form, where Ai, i = 1, 2, . . . , N is a T ×L matrix of
instruments. For a fixed T and N  , the empirical counterpart of the orthogonality conditions is (1/N) iN=1 AiUi.
Consider estimating by Generalized Least Squares (GLS) the following equation:

A Y = A X + A Z + A U = A  + A U,

(4.33)

i.e., by minimizing

1

1

min U A Var A U

N

N

-1 1 A U.
N

(4.34)

Letting V denote the variance­covariance matrix of (1/N)A U, the resulting estimator can be written as
^ = ( AV -1A )-1 AV -1A Y

Suppose we do not wish to make assumptions on the structure of the variance matrix V , e.g., disturbances may exhibit serial correlation (in the time dimension) and/or heteroskedasticity. Then the estimator above can be computed using an initial estimate for V , V^ = (1/N) Ni=1 AiU^iU^i Ai, where Ai is the (T, L) matrix of observations about the instrumental variables for the i-th individual and U^i is a (T, 1) initial consistent estimate of Ui, i = 1, . . . , N. This estimator is the GMM (Generalized Method of Moments) under its optimal form, and its exploits the fact that the variance­covariance matrix of Ui is block-diagonal (no correlation across individuals).
It is well known that if the disturbances are both homoskedastic and not serially correlated, so that
Var(A U) = E[A Var(U|A)A] + Var[A E(U|A)A] =  2E(A A)-1

106

R. Boumahdi and A. Thomas

since E(U|A) = 0 and Var(U|A) = U2 I, then the "best" instrumental variables estimator (i.e., GLS applied to model) is given by (see Gourieroux and Monfort,

1989):

^ IV =  A(A A)-1A  -1  A(A A)-1A Y

For panel data however, this is unlikely to be the case because of individual

unobserved heterogeneity, and the variance­covariance matrix of error terms is

 = IN   where  = 2 eT eT + 2IT is a T × T matrix. Suppose a prelimi-

nary estimate of  is available, ^ = IN   ^2 , ^2 , a simple version of this being

^

=

IN



1 N

Ni=1

U^iU^i

.

Replacing



by

^

so

that

 plimN

1 N

A

A

=

plimN

1 N

N i=1

Ai^ Ai

=

V,

we obtain the Three-Stage Least Squares estimator:

^ 3SLS =

A

A A

-1 A 

-1
A

A A

-1 A Y .

It is easy to see that the GMM and the 3SLS are equivalent under the condition of no conditional heteroskedasticity, see Ahn and Schmidt (1999):

E AiUiUi Ai = E AiAi i = 1, . . . , N.
Note that this condition is weaker than the condition that E(UiUi |Ai) =  . If the noconditional heteroskedasticity condition is not satisfied, then GMM is more efficient than 3SLS.
Assuming this conditional holds, 2SLS estimators can also be proposed. A first version of the Two-Stage Least Squares (2SLS) estimator is obtained by premultiplying the model by  -1/2 and then applying instruments A. This is the form used by HT, AM and BMS:
^ IV1 =   -1/2A(A A)-1A  -1/2 -1   -1/2A(A A)-1A  -1/2Y

This estimator is based on the two conditions:

E Ai -1/2Ui = 0,

E Ai -1/2UiUi  -1/2Ai .

Ahn and Schmidt (1999) shows that the 3SLS and the 2SLS estimator above are
equivalent asymptotically if a consistent estimate is used for 4 and if there exists a nonsingular and non-random matrix B such that -1/2A = AB (or equivalently, that -1/2Ai = AiB i = 1, 2, . . . , N).
A second version of the 2SLS estimator is denoted Generalized Instrumental Variables (GIV, see White (1984)), which uses directly -1/2A as instruments:

4 They are numerically equivalent if the same, consistent estimate is used.

4 Endogenous Regressors and Correlated Effects

107

^ GIV =  -1A A -1A -1 A -1 -1  -1A A -1A -1 A -1Y.

Although the two 2SLS estimators seem different, they are equivalent in a panel-data context when the error component structure is of the form  above. Again, a preliminary estimate of  is required to implement these 2SLS estimation procedures.

4.6.2 GMM Estimation with HT, AM and BMS Instruments
In the Instrumental-Variable context with Hausman­Taylor, Amemiya­MaCurdy or Breusch­Mizon­Schmidt instruments described above, we assume an errorcomponent structure and also that endogeneity is caused by correlated effects, either E(X ) = 0 or E(Z ) = 0. In any case, it is maintained that E(X ) = E(Z ) = 0. With GMM, we can consider different exogeneity assumptions related to  or , producing different orthogonality conditions. Apart from the difference between random and fixed effect specifications (instruments correlated or not with ), we can also consider strictly or weakly exogenous instruments if explanatory instruments are correlated with . These different cases are described by Ahn and Schmidt (1999), to which we refer the reader for more information.
Consider the case of strict exogeneity: E(Xisit ) = E(Ziit ) = 0, i, t. The questions we address are the following: is it possible to obtain a more efficient estimator than IV with either HT, AM or BMS, by exploiting more moment conditions? And does this efficiency depend on the assumption made on the assumed variance­ covariance structure?
The first result is that, under the No conditional heteroskedasticity assumption, HT, AM and BMS­2SLS estimators are equivalent to the GMM estimator. From the discussion above, this implies that GMM is more efficient with the same instrument set (and a consistent variance­covariance matrix) than the original version of HT, AM and BMS­2SLS estimators, if this NCH condition is not valid.
Ahn and Schmidt (1995) and Arellano and Bover (1995) note that, under the strict exogeneity assumption, more moment conditions can be used, to improve efficiency of the estimator. The strict exogeneity assumption is E(di  i) = 0, where di = (xi1, . . . , xiT , zi), implying E[(LT  di) ui] = E(LT i  di) = 0.
Arellano and Bover (1995) therefore propose a GMM estimator obtained by replacing (in vector form) QT i by LT  di in the HT, AM or BMS list of instruments. This leads to (T - 1)(kT + g) - k additional instruments, which may cause computational difficulties if T is large. They however also show that under the errorcomponent structure , both sets of instruments provide the same 3SLS (or 2SLS version 1, ^IV1) estimator.5 Consequently, if in addition the No conditional heteroskedasticity assumption is valid, then the 3SLS (or 2SLS version 1, ^IV1) with HT, AM or BMS instruments will be asymptotically equivalent to GMM with the augmented set of instruments.
5 Asymptotically only, if different estimates of  are used.

108

R. Boumahdi and A. Thomas

Im, Ahn, Schmidt and Wooldridge (1999) consider cases where the no conditional
heteroskedasticity assumption holds when the Arellano­Bover set of instruments is used, and  is left unrestricted. They show that the 2SLS estimator version 2 (^IV2) using BMS instruments is equivalent to the 3SLS estimator using Arellano­Bover
instruments, but that this equivalence does not hold for HT or AM instruments. To
solve this problem, Im, Ahn, Schmidt and Wooldridge (1999) propose to replace the fixed effects operator QT by Q = -1 - -1eT (eT -1eT )-1eT -1, such that Q eT = 0, and modifying the matrix of instruments appropriately. This modified 3SLS estimator would be asymptotically equivalent to an efficient GMM estimation
is the NCH condition holds.

4.7 Unbalanced Panels
In the preceding sections we have discussed estimation methods for panel data models when all cross-sectional units are observed for all time periods. In practice, missing observations are often encountered for a given cross-sectional unit and for a given time period. In this case, we have what we call an incomplete panel and the standard estimation methods are not applicable. Fuller and Battese (1974) suggest to add in the list of regressors a set of dummy variables, one for each missing observation. However, as noted by Wansbeek and Kapteyn (1989), this often implies that the number of regressors would increase dramatically (possibly, with the sample size), and in many empirical studies this becomes computationally impractical.
Wansbeek and Kapteyn (1989) consider a two-way unbalanced error component specification for the fixed and random effects models. In the first case (fixed effects) they suggest a new expression for within operator, which generalizes the operator Q given in Sect. 4.2.6 For the second case (random effects), they propose to use the quadratic unbiased and Maximum Likelihood estimators.
More recently, Baltagi and Chang (1994) have considered a one-way error component model with unbalanced data. Using a Monte Carlo simulation experiment, they compare several estimation methods including the Analysis Of Variance (ANOVA), Maximum Likelihood (ML), Restricted Maximum Likelihood (REML), Minimum Norm Quadratic Estimation (MINQUE) and Minimum Variance Quadratic Estimation (MINQUE).
In their simulation and the empirical illustration they propose, they show that in general, better estimates of the variance components do not necessarily imply better estimates of the regression coefficients. Furthermore, MLE and MIVQUE perform better than the ANOVA methods in the estimation of the individual variance component. Finally, for the regression coefficients, the computationally simple ANOVA methods perform reasonably well when compared with the computationally involved MLE and MIVQUE methods.
When the data have a sufficient degree of disaggregation, more than two dimensions of data variation are generally available. One can think for instance of a sample
6 See Wansbeek and Kapteyn (1989), p. 344.

4 Endogenous Regressors and Correlated Effects

109

of observations on firms (level 1) belonging to a particular industry (level 2), within a region (level 3). In this case, several time-invariant heterogeneity components can be introduced in the linear panel data model, giving rise to multi-way error components models. In the nested specification, each successive component in the error term is nested within the preceding component. In the non-nested case, error components are independent of each other, and transformation techniques similar to those employed in the two-way error component model are applicable.
As operator matrices for performing Between and Within transformations under any hierarchical structure are straightforward to construct, fixed effects and GLS estimators are generally available for such models (see, e.g., Antweiler (2001)). In the unbalanced panel data case however, the required algebra to obtain expressions for the Feasible GLS estimator in particular, is more difficult to handle.
Baltagi, Song and Jung (2001) propose a fixed-effects representation and a spectral decomposition of a three-way unbalanced error component model, leading to a Fuller­Battese scalar transformation for this model. They proceed by investigating the performance of ANOVA, Maximum Likelihood and MINQUE estimators of variance components in the unbalanced nested error component model. ANOVA estimators for variance components are BQU (Best Quadratic Unbiased) in the balanced case only, and are only unbiased in the unbalanced case. Monte Carlo experiments reveal that ANOVA methods perform well in estimating regression coefficients, but ML and MINQUE estimators are recommended for variance components and standard errors of regression coefficients. They do not deal with the case of endogenous regressors or correlated effects, beyond the obvious possibility to obtain consistent estimates using fixed effects. The fact that exogenous variables may be available for different levels in the hierarchical structure of the data, leads to a wide variety of possible instruments. For example, if firm-specific individual effects are correlated with decision variables of the firms, price variables at an upper level (county, region) may be used as instruments.
Davis (2002) proposes a unifying approach to estimation of unbalanced multiway error components models, as well as useful matrix algebra results for constructing (Between, Within) transformation matrices. The recurrence relations proposed in the paper allow for direct extension to any number of error components. There are but few empirical applications in the literature using multi-way unbalanced panels, see Davis (2002) and Boumahdi and Thomas (2006) for examples.
For example, the three-way unbalanced error component model is

Y = X + u, u = 1 + 2 + 3 + ,

(4.35)

where  = (1, . . . , L) ,  = (1, . . . , H ) and  = (1, . . . , T ) . Matrices k, k = 1, 2, 3 are constructed by collecting dummy variables for the
relevance of a given observation to a particular group (l, h,t), and have dimension
N × L, N × H and N × T respectively. Letting PA = A(A A)+A and QA = I - PA where + denotes a generalized inverse,
the fixed effects transformation matrix is shown to be Q = QA - PB - PC, where  = [1, 2, 3] and

110

R. Boumahdi and A. Thomas

PA = I - 3(33)+3, QA = I - PA, PB = QA2(2QA2)+2QA, QB = I - PB, PC = QAQB1 1(QAQB)1 + 1QAQB, QC = I - PC.

Under the exogeneity assumption E(X Q) = 0, the fixed-effects estimator is

consistent:

^ = X QX -1 X QY.

(4.36)

Assume instruments W are available such that E(W Q) = 0; then a consistent IV estimator can be constructed as

^ = X PQW X -1 X PQW Y,

(4.37)

where PQW = QW (W QW )-1 W Q. As mentioned above in the one-way unbalanced case, application of IV proce-
dures require consistent estimation of the variance­covariance matrix, as well as an instrument matrix consistent with the unbalanced nature of the sample. Formulae for estimating variance components can be found in Baltagi, Song and Jung (2001) and Davis (2002), although estimation should be adapted along the lines of Hausman and Taylor (1981) because of endogenous regressors. For instrument matrices, the HT specification is directly applicable because it only contains Within transformations and variables in levels. However, the AM and BMS IV estimators suffer from the same difficulty as in the one-way unbalanced case: they are more problematic to adapt because of missing observations in X1 and (QX) matrices. It is not clear whether the usual procedure to replace missing values by zeroes in those matrices produces severe distortions (bias, inefficiency) or not.

References
Ahn, S.C., Y.H. Lee and P. Schmidt [2001]. GMM Estimation of Linear Panel Data Models with Time-Varying Individual Effects. Journal of Econometrics 101, 219­255.
Ahn, S.C. and P. Schmidt [1999]. Estimation of linear panel data models using GMM. In Generalized Method of Moments Estimation (L. Matyas, ed.), pp. 211­247, Cambridge University Press, Cambridge.
Alvarez, J. and M. Arellano [2003]. The Time Series and Cross Section Asymptotics of Dynamic Panel Data Estimators. Econometrica 71, 1121­1159.
Amemiya, T. and T.E. MaCurdy [1986]. Instrumental Variable Estimation of an Error Components Model. Econometrica 54, 869­880.
Anderson, T. and C. Hsiao [1982]. Formulation and Estimation of Dynamic Models Using Panel Data. Journal of Econometrics 18, 47­82.
Antweiler, W. [2001]. Nested Random Effects Estimation in Unbalanced Panel Data. Journal of Econometrics 101, 295­313.
Arellano, M. [1993]. On the Testing of Correlated Effects with Panel Data. Journal of Econometrics 59, 87­97.
Arellano, M. and S. Bond [1991]. Some Tests of Specification for Panel Data: Monte Carlo Evidence and an Application to Employment Equations. Review of Economic Studies 58, 277­297.

4 Endogenous Regressors and Correlated Effects

111

Arellano, M. and O. Bover [1995]. Another Look at the Instrumental Variable Estimation of Error-Components Models. Journal of Econometrics 68, 29­51.
Baltagi, B.H. and Y.-J. Chang [1994]. A Comparative Study of Alternative Estimators for the Unbalanced One-Way Error Component Regression Model. Journal of Econometrics 62, 67­89.
Baltagi, B. and S. Khanti-Akom [1990]. On Efficient Estimation with Panel Data, an Empirical Comparison of Instrumental Variables Estimators. Journal of Applied Econometrics 5, 401­406.
Baltagi, B.H., S.H. Song and B.C. Jung [2001]. The Unbalanced Nested Error Component Regression Model. Journal of Econometrics 101, 357­381.
Biørn, E. [1981]. Estimating Economic Relations from Incomplete Cross-Section/Time-Series Data. Journal of Econometrics 16, 221­236.
Boumahdi, R., J. Chaaban and A. Thomas [2006]. Import demand estimation with country and product effects: Application of multi-way unbalanced panel data models to Lebanese imports. In Panel Data Econometrics: Theoretical Contributions and Empirical Applications (B.H. Baltagi, ed.), Chap. 8, pp. 193­228, Elsevier, Amsterdam.
Boumahdi, R. and A. Thomas [1997]. Estimation des mode`les de donne´es de panel avec re´gresseurs temporels. Annales d'Economie et de Statistique 46, 23­48.
Boumahdi, R. and A. Thomas [2006]. Instrument Relevance and Efficient Estimation with Panel Data. Economics Letters 93, 305­310.
Breusch, T.S., G.E. Mizon and P. Schmidt [1989]. Efficient Estimation Using Panel Data. Econometrica 57, 695­700.
Cornwell C. and P. Rupert [1988]. Efficient Estimation with Panel Data: An Empirical Comparison of Instrumental Variables Estimators. Journal of Applied Econometrics 3, 149­155.
Cre´pon, B., F. Kramarz and A. Trognon [1997]. Parameters of Interest, Nuisance Parameters and Orthogonality Conditions an Application to Autoregressive Error Component Models. Journal of Econometrics 82, 135­156.
Davis, P. [2002]. Estimating Multi-Way Error Components Models with Unbalanced Data Structures. Journal of Econometrics 106, 67­95.
Fuller, W.A. and G.E. Battese [1974]. Estimation of Linear Functions with Cross-Error Structure. Journal of Econometrics 2, 67­78.
Godfrey, L.G. [1999]. Instrument Relevance in Multivariate Linear Models. Review of Economics and Statistics 81, 550­552.
Hansen, L.P. [1982]. Large Sample Properties of Generalized Method of Moments Estimators. Econometrica 50, 1029­1054.
Hausman, J. [1978]. Specification Tests in Econometrics. Econometrica 46(6), 1251­1271. Hausman, J. and W.E. Taylor [1981]. Panel Data and Unobservable Individual Effects. Economet-
rica 49, 1377­1398. Heckman, J.J. and V.J. Holtz [1989]. Choosing Among Alternative Nonexperimental Methods for
Estimating the Impact of Social Programs: The Case of Manpower Training. Journal of the American Statistical Association 84, 862­875. Holtz-Eakin, D., W. Newey and H. Rosen [1988]. Estimating Vector Autoregressions with Panel Data. Econometrica 56, 1371­1395. Hsiao, C. [1986]. Analysis of Panel Data. Cambridge University Press, Cambridge. Im, K.S., S.C. Ahn, P. Schmidt and J.M. Wooldridge [1999]. Efficient Estimation of Panel Data Models with Strictly Exogenous Explanatory Variables. Journal of Econometrics 93, 177­201. Lillard, L. and Y. Weiss [1979]. Components of Variation in Panel Earnings Data: American Scientists 1960­1970. Econometrica 47, 437­454. Mundlak, Y. [1961]. Empirical Production Function Free of Management Bias. Journal of Farm Economics 43, 44­56. Mundlak, Y. [1978]. On the Pooling of Time Series and Cross Section Data. Econometrica 46, 69­85. Nauges, C. and A. Thomas [2003]. Consistent Estimation of Dynamic Panel Data Models with Time-Varying Individual Effects. Annales d'Economie et de Statistique 70, 53­75.

112

R. Boumahdi and A. Thomas

Schmidt, P., S.C. Ahn and D. Wyhowski [1992]. Comment: Sequential Moment Restrictions in Panel Data. Journal of Business and Economic Statistics 10, 10­14.
Shea, J. [1997]. Instrument Relevance in Multivariate Linear Models: A Simple Measure. Review of Economics and Statistics 79, 348­352.
Wansbeek, T. and A. Kapteyn [1989]. Estimation of the Error-Components Model with Incomplete Panels. Journal of Econometrics 41, 341­361.
Wansbeek, T.J. and T. Knaap [1999]. Estimating a Dynamic Panel Data Model with Heterogenous Trends. Annales d'Economie et de Statistique 55­56, 331­349.
Wyhowski, D.J. [1994]. Estimation of a Panel Data Model in the Presence of Correlation Between Regressors and a Two-Way Error Component. Econometric Theory, 10, 130­139.

Chapter 5
The Chamberlain Approach to Panel Data: An Overview and Some Simulations
Bruno Cre´pon and Jacques Mairesse

5.1 Introduction

In this paper, we present the general approach proposed by Chamberlain (1982 and 1984) for the analysis of panel data. Although the 1984 article examines nonlinear models such as probit or logit models for panel data, we only cover here, but in details, the case of linear regression models. However, much of the approach which is applicable in this case, as well as much of the intuition that can be gained from it, applies also to non linear models.
Let us consider the linear regression model of the dependent variable y on K explanatory variables x observed for a balanced panel of N individuals (for example firms) and T periods (for example years):

  yit = xi(tk)bk + vit = xi(tk)bk + i + uit , t = 1, . . . , T, i = 1, . . . , N (5.1)

k

k

The Chamberlain approach or method provides a general unifying framework encompassing both the ideal case of reference of "Non Correlated Errors" (NCE) in which the regressors x in model (5.1) can be assumed to be uncorrelated with both the unobserved individual effects  and the idiosyncratic disturbances u (i.e., with the overall disturbance v), and the paradigmatic case in panel data econometrics of "Correlated Effects" (CE) in which the regressors x are possibly correlated with the individual effects  but are still assumed to be uncorrelated with the idiosyncratic disturbances u (see Mundlak, 1961). It allows to take into account different important types of specification errors other than the existence of correlated effects, in the form of correlations or covariances between the regressors x and both disturbances

Bruno Cre´pon CREST-INSEE (Paris), CEPR (London) and IZA (Bonn), e-mail: crepon@ensae.fr
Jacques Mairesse CREST-INSEE (Paris), UNU-MERIT (Maastricht University), and NBER. CREST-INSEE, 15 boulevard Gabried PERI, 92245 MALAKOFF ce´dex, France, e-mail: mairesse@ensae.fr

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

113

c Springer-Verlag Berlin Heidelberg 2008

114

B. Cre´pon and J. Mairesse

 and u (or v). The variables x, for example, can be affected by random errors of measurement, or they can include lags of the dependent variable y.
The Chamberlain method consists in a first stage in constructing the set (or a subset) of relations between the moments of the variables x and the unknown parameters b, which are implied by the correlations or covariances between the regressors and the disturbances. In a second stage it uses these relations, or estimating equations, to implement Minimum Distance estimators, or so called Asymptotic Least Squares (ALS) estimators, and obtain consistent and possibly asymptotically efficient estimates of the b s. The Chamberlain method is thus basically a method of moments; and it is in fact very close to the generalized methods of moments (GMM) as applied to panel data, for which it provides a different, deeper understanding and a possible alternative.
In Sect. 5.2, we present the first stage of the Chamberlain method, consisting itself of two steps. We first define the so called Chamberlain's matrix  = E(yixi)E(xixi)-1 of dimension (T, KT ), providing a summary of the panel data.1 We then derive the Chamberlain estimating equations, which are the basic relations existing between the parameters b of (primary) interest to be estimated and the coefficients of , or equivalently, and often more simply, the relations between the b s and the coefficients of E(yixi) and E (xixi), that is the covariance matrices of y and the x s across all T periods. We explicitly consider these estimating equations in specific cases of major interest, corresponding to different types of specification errors imposing restrictions on the form of E(vixi),the covariance matrix of the disturbance v and the x's across all T periods. In this section, we also explain how additional restrictions arising from specific assumptions on the form of the covariance matrix E[vivi] of the disturbances can be used to improve the efficiency of the Chamberlain method estimators. We also provide an extended view of the Chamberlain method which applies to more general models than the regression model (5.1).
In Sect. 5.3, we present the second stage of the Chamberlain method, consisting in applying the Asymptotic Least Squares (ALS) to obtain consistent and asymptotically normal estimators of the parameters b's of primary interest, as well as parameters of secondary interest characterizing possible specification errors in model (5.1). We explain how to implement the asymptotically efficient or optimal ALS estimator and perform specification tests. We also explain how the ALS estimating equations can be manipulated and some parameters eliminated without loosing consistency and asymptotic efficiency in estimating the remaining parameters.
In Sect. 5.4, we show how in particular the ALS estimating equations can be reformulated as orthogonality conditions in the panel data GMM framework, and we demonstrate explicitly the asymptotic equivalence between the Chamberlain method and GMM in general and in some of the specific cases of interest.
In the last Sect. 5.5, we present some simulation results illustrating the small sample properties of both the Chamberlain method and the GMM estimators in some of the specific cases of interest previously considered.

1 yi, vi and xi are column vectors of dimension (T, 1) and dimension (KT, 1) respectively. See Sects. 5.2.1 and 5.2.2 below.

5 The Chamberlain Approach to Panel Data

115

The Appendices A, B, C, D and E (respectively Sect. 5.6 to 5.10) provide details on various technical points. They can be skipped by readers who only want to have an overall understanding of the Chamberlain method and its equivalence with GMM. In Appendix A, we show how the Chamberlain approach can be extended to simultaneous linear equations models and to vector autoregression models (VAR) on panel data, and how it can deal also with endogenous attrition. In Appendix B, we show how the Chamberlain estimating equations, written in matrix format in the text, can also be written in a vector format for practical estimation. In Appendix C, we show how the Chamberlain estimating equations can be rewritten in order to eliminate auxiliary parameters (or parameters of secondary interest), while preserving asymptotic consistency and efficiency of the estimators of the parameters of (primary) interest. In Appendix D, we show that the usual basic panel estimators (Random Effects, Within and First Differences) are asymptotically equivalent to the Chamberlain and GMM estimators. In Appendix E, we provide important details on the design and calibration of the simulation experiments presented in the text (Sect. 5.5).

5.2 The Chamberlain  Matrix Framework

5.2.1 The  Matrix

The Chamberlain method is basically a method of moments; it uses the restrictions on the moments of the variables implied by the modeling assumptions to estimate the parameters of interest. In a first stage, the moments of the variables are computed up to the second order forming a set of summary statistics for the data, which can be considered as estimated auxiliary parameters. In a second stage the parameters of interest are estimated on the basis of their relations with these estimated auxiliary parameters, using Minimum Distance or Asymptotic Least Squares (ALS) estimators.
An important feature of the Chamberlain method, as usually presented, is that it summarizes the set of second order moments (variances and covariances) of the dependent and explanatory variables, which is central to the analysis, by the so called Chamberlain  matrix. The  matrix is defined in terms of the coefficients of the linear predictors of the dependent variable at each period given all explanatory variables at all periods. Precisely, if there are T years and K explanatory variables x, the  = [t, j] matrix is of dimension T × KT and is obtained by stacking one above the other the row vectors of dimension 1 × KT of the coefficients of the T separate year regressions such as

yit = t,1xi(11) + · · · + t,T xi(T1) + t,T +1xi(12) + · · · + t,KT xi(TK) + wit

(5.2)

with

E(wit xi(sk)) = 0, s,t, k.

(5.3)

116

B. Cre´pon and J. Mairesse

If we define yi = (yi1, . . . , yiT ) and xi = (xi(11), . . . , xi(T1), . . . , xi(TK)) we can also write

 = E(yixi)E(xixi)-1.

(5.4)

It must be noted that (5.2) with the covariance restriction (5.3) is not a linear regression model strictly speaking, but simply expresses the linear projection of y on all lagged, present and future x 's.2
As a simple illustration, let us take the example of a regression model for a panel of only two years (T =2), with only one explanatory variable (K=1) and the usual error components structure

yit = xit b + i + uit , t = 1, 2, i = 1, . . . , N.

Consider first the simplest standard case that we call "Non Correlated Errors" (NCE), in which it is assumed that

E [ixis] = E [uit xis] = 0, t, s.

In this case, the parameter of interest b can be consistently estimated by simply

using the pooled OLS estimator or the error components GLS estimator. However,

it is also possible to compute the  matrix by considering the two separate year

regressions with both x1 and x2 as explanatory variables. In each year regression

the true coefficient of the contemporaneous x (11 or 22) is equal to the parameter

b and the other coefficient of the lagged or future x (21 or 12) is zero. Thus the

"true"  matrix is

11 12 21 22

=

b0 0b

=b

10 01

.

We thus can expect that the unrestricted  matrix, as estimated in the first stage, will also look roughly diagonal (and with roughly equal diagonal coefficients). However, this may not be so striking in practice, if the individual t,s coefficients are not estimated precisely enough, that is if the sample is not large enough in the individ-
ual dimension and if the year x's (x1 and x2) are more or less collinear (which is often likely when these variables are in absolute level or defined as ratio of different
variables). In the second stage of the Chamberlain method, we can retrieve a consistent estimate b of b from the estimated t, j by applying Asymptotic Least Squares (ALS) to the four "estimating equations" 11 = b, 12 = 0, 21 = 0, 22 = b, or in vector form
(11, 12, 21, 22) = b(1, 0, 0, 1) .

The ALS estimator is defined precisely in the next Sect. 5.3. It is more efficient than the pooled OLS estimator, and also more efficient than the usual error components GLS estimator, under general conditions in which vit has not an error

2 The linear projection is generally denoted by E(yit |xi(k);  = 1, . . . , T ; k = 1, . . . , K) with a  to distinguish it from the conditional expectation E(yit |xi(k);  = 1, . . . , T ; k = 1, . . . , K), which has no a priori reason to be linear in the x's. We do not, however, use this notation here.

5 The Chamberlain Approach to Panel Data

117

component structure and is heteroscedastic and serially correlated.3 One can also test formally whether the  matrix has the right pattern, by an asymptotic 2 test "measuring" the distance between the unrestricted first stage estimate of  and its restricted second stage estimate.4
Let us consider next the other standard case of "Correlated Effects" (CE) in which
the explanatory variable is correlated with the individual effects i but not with the error terms uit

E [uit xis] = 0, t, s E [ixis] = 0.

In this case, the linear projection of the individual effects i on the x's is no longer zero. It can be written as
i = 1xi1 + 2xi2 + wi
with, by definition, E(wixit ) = 0. Hence, in this case, the "true"  matrix has the following distinctive pattern:

11 12 21 22

=

1 + b 2 1 2 + b

=

1 2 1 2

+

b0 0b

where the off-diagonal coefficients will have to be equal within the same columns for a panel with more than two years (T > 2).5 As in the NCE case, such a distinctive pattern may be recognizable on the  matrix as estimated in the first stage, although in general practice this pattern will be badly blurred.
Applying ALS to the four estimating equations

(11, 12, 21, 22) = b(1, 0, 0, 1) + 1(1, 0, 1, 0) + 2(0, 1, 0, 1)

provides a consistent estimator of b, our parameter of primary interest, as well as a consistent estimator of the  's. The coefficients  's are usually called "nuisance parameters", but we will prefer to consider them here as parameters of secondary interest, since they characterize the relation of the unknown individual effects and the known explanatory variables. The ALS estimator is more efficient under general conditions than the usual Within estimator performed on the deviations of the variables from their individual means, that is by simply using pooled OLS on the within transformed regression of yit - yi. on xit - xi. (see Appendix D: Equivalence between Chamberlain's, GMM and usual panel data estimators).

3 The usual error components GLS estimator is optimal under the assumption that i and uit are homoscedastic and uit is serially uncorrelated. 4 Note also that we may allow b to vary over time and test for its constancy.
5 Note that the model with Correlated Effects is identified as soon as we have a panel: T > 1, and even if we allow the b coefficients to vary over time.

118

B. Cre´pon and J. Mairesse

5.2.2 Relations Between  and the Parameters of Interest

In this subsection, we generalize the two previous examples. We present the basic relations which the model implies between the second order moments of the variables y and x [i.e., E(yixi) and E (xixi)] and the parameters of both primary interest and secondary interest, and we show how these relations can be rewritten as restrictions on the  matrix.
Consider the linear regression model with K explanatory variables for a balanced panel of N individuals observed on T periods or years:

  yit = xi(tk)bk + vit = xi(tk)bk + i + uit , t = 1, . . . , T, i = 1, . . . , N (5.5)

k

k

where the b's are the parameters of primary interest. A central idea of the Chamberlain
approach is to view this panel data model as a system of T stacked year equations.
Defining the two (T × 1) column vectors yi and vi by yi = (yi1, . . . , yiT ) and vi = (i + ui1, . . . , i + uiT ) respectively, and the two (KT × 1) and (K × 1) column vectors by xi = (xi(11), . . . , xi(T1), . . . , xi(11), . . . , xi(TK)) and b = (b1, . . . , bK) respectively, we can more compactly rewrite the (5.5) as

yi = M(b)xi + vi

(5.6)

where M(b) = (b1, . . . , bK)  IT is a (T × KT ) matrix. Also denoting by  = E [vixi] the (T × KT ) covariance matrix between the disturbances vi and explanatory vari-
ables xi, we can derive the following moments relations

E yixi = M(b)E xixi + .

(5.7)

Introducing now the matrix  = E(yixi)E(xixi)-1 and denoting by  the (T×KT) matrix E(vixi)E(xixi)-1 = E(xixi)-1 of the coefficients of the linear projection of the disturbances vi on the explanatory variables xi, we can also write equivalently

 = M(b) + .

(5.8)

The equations (5.5) or (5.6), or the moments relations (5.7) or (5.8), simply express an arbitrary decomposition of y into one part corresponding to the explanatory variables x and another one to the disturbances v. Giving them an econometric content (i.e., being able to test them as a regression model and to identify and estimate the b parameters of primary interest) requires imposing restrictions between the v's and the x's. In the Chamberlain method these stochastic restrictions can take different forms in terms of the  or  matrices. The simplest case is the basic one of Non Correlated Errors (NCE) which assumes that  =  = 0.

5 The Chamberlain Approach to Panel Data

119

More generally, let us consider that the (T × KT ) covariance matrix  of vi and xi can be parameterized by a set of (additional) parameters  of secondary interest.6 As long as the dimension of  is not too large (less than KT 2 - K), this implies the
following set of restrictions between the moments of the variables yi and xi

E(yixi) = M(b)E(xixi) + ( ),

(5.9)

which can be rewritten as

 = M(b) + ( )E(xixi)-1.

(5.10)

The core of the Chamberlain method is to derive estimates of the b and  param-

eters from the estimates of the  matrix by applying Asymptotic Least Squares

(ALS) to (5.10).7 In the case, however, in which the matrix  can be parameterized

more conveniently in terms of parameters of secondary interest  than the matrix

 in terms of parameters of secondary interest  , estimates of the b and  param-

eters can be obtained from the estimated  matrix alone by applying ALS to the

equations

 = M(b) + ( ).

(5.11)

In this case, which is that of Correlated Effects (CE), the relations (5.11) take the form of a direct restriction on , and  summarizes all the relevant information brought by the second order moments of the variables, with no need for the estimates
of E(xixi). In general, we have to rely on estimating equations such as (5.9) or (5.10) and
to use estimates of E(xixi) or those of its inverse, which implies some additional complications compared with the more simple implementation of the ALS when the estimating equations can take the form of a direct restriction on  as in (5.11). These complications are sometimes neglected or ignored, as we shall indicate in Sect. 5.3. Note that when  can be conveniently parameterized, usually  cannot (and vice versa). The Correlated Effects case, which we are going to consider in more detail in the next subsection, is an exception where  and  have the same structure where all T coefficients in a given column are equal to the corresponding  or  coefficients. Note that in order to be able to identify fully the model (5.5) as specified, it is necessary to have at least one subset of the estimating equations (5.11) or (5.10) that can be solved for (b,  ) or (b,  ) as a function of  alone, or  and E (xixi). A necessary (but not sufficient) condition for identification is that the dimension of the estimated parameter is less than the number of estimating

6 A (T × KT ) matrix can always be parameterized by its KT 2 coefficients. What we mean here is that it can be parameterized more parsimoniously.
7 Note that it is equivalent to take (5.9) instead of (5.10) as estimating equations, with the only difference of using the estimated covariance matrix of y and x rather than the estimated  matrix. The  matrix has the advantage of being directly expressed in the set up of the regression model under analysis, with coefficients having the dimension of the regression coefficients (and not of covariances). But as we shall see it may be more convenient to rely on equations (5.9).

120

B. Cre´pon and J. Mairesse

equations. Thus, as the number of equations (5.10) or (5.11) is KT 2 and the size of parameter b is K, the dimension of  or  must be less than KT 2 - K.8
The parameterization of the T × KT matrix ( ) or ( ) in the estimating equations (5.10) and (5.11) is essential in the implementation of the Chamberlain method. It expresses specific assumptions on the econometric specification of the model, some of which can be tested by the econometrician, while the others are maintained. In the following Sect. 5.2.3, we present four most important cases of such assumptions which can be combined together and are often considered in practice. While in general, most of the interest is devoted to the estimation of the b s, the  s or  's can receive an interesting interpretation in terms of errors of specification, allowing to test if the model corresponds to one given econometric specification or to another one. Actually, depending on the modeling of ( ) or ( ), there is often a trade-off between the accuracy of the estimation of b on the one hand, and its consistency and robustness to errors of specification in the other hand. Imposing more restrictions on the matrices ( ) or ( ), that is parameterizing them with a smaller vector  or  , yields some efficiency gain in the estimation of b. Conversely, the estimation of b to given errors of specification is more robust when imposing less restrictions on these matrices. In the simulation exercise presented in Sect. 5.5, we show that this can be indeed a crucial trade-off in small or midlle size panel data samples.

5.2.3 Four Important Cases
Let us consider the form of the estimating equations and of the restrictions on the  matrix in four important usual cases: that of Correlated Effects (CE), Errors in Variables (EV), Weak Simultaneity (WS) and Lagged Dependent Variables (LDV). We consider them separately, but they can be combined together easily. These examples are also treated in more detail in Appendix B: Vector representation of the Chamberlain estimating equations.
5.2.3.1 Correlated Effects
In the case of correlated effects we assume that the past, present and future values of the explanatory variables x are not correlated with the (time varying) idiosyncratic disturbance uit , but that they can be correlated with the individual effects i. This implies that the  and the  matrices have the same pattern with equal within-
8 Note that when the estimating equations take the form of (5.11), identification of b and  depends on the particular form of M(b) and ( ). When they take the form of (5.10) or that of (5.9), identification of b and  depends on the form of M(b) and ( ), but also on E (xixi), and requires some specific conditions on E (xixi). See in the next subsection the cases of Errors in Variables (EV) and Weak Simultaneity (WS).

5 The Chamberlain Approach to Panel Data

121

column coefficients, and thus can be parameterized by KT parameters  or  .9 We can simply write

( ) = E ((il) xi) = lE (ixi) = l ( ) = E ((il) xi) E (xixi)-1 = lE (ixi) E (xixi)-1 = l

(5.12)

where l is the (T × 1) vector of 1's,  is the (KT × 1) vector of the covariances of i and the x's, and  is the (KT × 1) vector of the coefficients of the linear projection of i on the x's.

5.2.3.2 Errors in Variables

In the case of errors in variables we assume that the true model is

yi = M(b)xi + vi, i = 1, . . . , N but that instead of x we only observe

xi = xi + ei
where the true xi and the errors of measurement ei are uncorrelated. We also assume for simplicity that the x's are strictly exogenous (i.e., uncorrelated with the overall disturbance v) and that the errors of measurement themselves e are also uncorrelated with the v s. Denoting the covariance matrix of the measurement errors by Ve = E(eiei), we can then write

 = E (vi - M(b)ei) xi = -M(b)Ve

(5.13)

and  = -M(b)VeE xixi -1 .
To identify the model it is necessary to make some simplifying assumptions on Ve. It is usually considered that measurement errors are serially uncorrelated (or so called "white noise"). We show in Appendix B that in this case the matrix  = -M(b)Ve in (5.13) has the simple form of the juxtaposition of K diagonal matrices, with (KT ) possibly different diagonal coefficients. It follows from equation (5.9) that the parameters b are identifiable as long as for each k there is at least one l and two periods (s,t), with s = t, such that E xi(tk)xi(sl) = 0. Note, however, that the KT diagonal elements of the matrix  = -M(b)Ve are of the form Kl=1 blCov ei(tk), ei(tl) , and thus it is only these KT functions of the
KT (K + 1) /2 parameters Cov ei(tk), ei(tl) which are identifiable, not the individual

9 Note that this corresponds simply to the fact that the linear projection of the disturbances vit on the x's is the linear projection of the individual effects i on the x's, and hence does not depend on t. Likewise Cov(vit xis) = Cov (ixis) depends on s, not on t.

122

B. Cre´pon and J. Mairesse

Cov e(itk), e(itl) . This is why it is usually assumed that the measurement errors are not only serially uncorrelated, but also uncorrelated with each other, implying lK=1 blCov e(itk), e(itl) = bkE ei(tk)2 , and thus allowing the estimation of the mea-
surement errors variances E e(itk)2 , and not only the estimation of the parameters of interest b's.
By contrast to  and E(yixi),  and  have a complicated pattern, involving leads and lags of the different x's, and they cannot be directly parameterized, irrespective of E (xixi)-1:

 = M(b) +  = M(b)[I -VeE xixi -1].

(5.14)

It is easy to see that Correlated Effects and Errors in Variables can be considered jointly. Note that Correlated Effects alone take care of possible measurement errors that are constant over time.

5.2.3.3 Weak Simultaneity

"Weak Simultaneity" (WS), as we prefer to call it here, corresponds to the case of predetermination, or weak exogeneity, of the x variables, or of some of them. It allows for lagged effects of the y variable and possibly for contemporaneous twoways effects by not assuming that the past and present idiosyncratic disturbances or shocks uis can affect the current x's, but it assumes that future shocks do not. Note that we can equivalently say that past x 's are uncorrelated with the current shocks uis. In this case, the identifying restrictions are

E (uisxit ) = 0 for s > t

(5.15)

and the matrix  has the characteristic pattern of a repeated upper triangular matrix which can be parameterized by (KT (T + 1) /2) parameters of secondary interest  . As previously, the parameterization of , and hence of  follows from that of  and involves E (xixi)-1 .
Equations (5.15) are enough for identification when the explanatory variables x
are correlated over time. To see this, we can assume for simplicity that K = 1, and
write equations (5.9) for the couples of (s,t) indexes with s > t. We have E (yisxit ) = bE (xisxit ) + E (uisxit ) = bE (xisxit ) , showing that the parameter b will be identified as long as there is at least one (s,t), with s > t, such that E (xisxit ) = 0.
Weak Simultaneity is usually combined with Correlated Effects. As considered here, it assumes either that the unobserved individual effects i are uncorrelated with the x's or that they are equal to zero (i.e., i = 0).
In the case of Correlated Effects and Weak Simultaneity (CEWS), the identifying
restrictions (5.15) become

[E (visxit ) - E (vis-1xit )] = E ((uis - uis-1)xit ) = 0 for s > t + 1.

5 The Chamberlain Approach to Panel Data

123

Assuming for simplicity as above that K = 1, we see that the parameter b will be identified as long as there is at least one (s,t) with s > t + 1 such that we have E ((xis - xis-1) xit ) = 0. This will fail, however, when the x's follow a random walk, or will not work well if they are very persistent (strongly autocorrelated).
Note finally that the case of Weak Simultaneity includes the possibility of errors in variables, if we assume that they are serially uncorrelated.

5.2.3.4 Lagged Dependent Variables

Let us also explicitly consider the case of an autoregressive model, in which Weak Simultaneity and Correlated Effects naturally arise from the presence of lagged dependent variables among the explanatory variables. Assuming a first order autoregressive model to keep computation simple, we can write it as:

 yit = yit-1 + xi(tk)bk + i + uit k
or in a vector format as:

yi = yi(-1) + M (b) xi + il + yi0l1 + ui

where yi(-1) = (0, yi1, . . . , yiT-1), l is the (T × 1) vector (1,1,...,1) and l1 = (1, 0, . . . , 0). Using the (T × T ) matrix L such that yi(-1) = Lyi (i.e., such that all the coefficients of the first subdiagonal are 1 and all the others are zeros), we can

also write

[I - L]yi = M (b) xi + il + yi0l1 + ui.

(5.16)

Assuming that the x's can also be correlated with the fixed effect i as well as with the initial (unknown) yi0, but not with the shocks uit , and denoting respectively by sT=1  sxis the projection of yi0 on xi, and sT=1  sxis the projection of i on xi, we directly obtain from (5.16) the following set of estimating equations in term of the  matrix and of the parameters of primary and secondary interest , b,  and  = 

[I - L] = M (b) + l + l1 .

(5.17)

Although they involve the  matrix alone, these equations do not take the form of direct restrictions on  (as in the Correlated Effects case), but of a more general implicit function f (,  ) = 0, where  = (, b,  , ). They can also be transformed in terms of direct restrictions by left-multiplying them by [I - L]-1, but this leads to more complex nonlinear relations between  and the different parameters of
interest.

124

B. Cre´pon and J. Mairesse

5.2.4 Restrictions on the Covariance Matrix of the Disturbances

The restrictions on the  matrix considered so far arise from assumptions on the covariances between the overall disturbances vi and the xi's, by imposing a given structure to the matrices  or  and allowing them to be more or less simply parameterized. We have not made any assumptions on the structure of the covariance
matrix  = E(vivi) of these disturbances. The Chamberlain method estimators of the parameters of primary interest b and of secondary interest  or  (and their standard errors) are thus robust to non constant year variances and to any kind of serial correlation in these disturbances.10
However, we may be interested in making some simplifying assumptions on the
form of . For example we may want to test that vi has (indeed) a pure error component structure i +uit , or that its time varying component uit is not homoscedastic, or that it is not serially uncorrelated, but generated by an autoregressive (AR) or mov-
ing average (MA) process. Such assumptions also give rise to restrictions on the
covariances matrices of yi and xi, and hence can be used to improve the efficiency of the estimators.
More precisely, consider the case when  can be expressed in terms of additional parameters  (of dimension less than T (T + 1)/2, the number of individual year variances and covariances in ). We can write the following relations:

( ) = E[vivi] = E[(yi - M(b)xi)(yi - M(b)xi) ] = E[yiyi] - M(b)E[xiyi] - E[yi xi]M(b) + M(b)E[xixi]M(b) .

(5.18)

Denoting the residual of the projection of yi on xi by wi = yi - xi and its covariance matrix by Vw = E(wiwi), we can also write

( ) = Vw + ( )E[xixi]-1( )

(5.19)

or

( ) = Vw + ( )E[xixi]( )

(5.20)

The previous relations (5.18) are of course equivalent to the relations (5.19) or
(5.20), but the later relations are probably a better way to write them. If we can
assume that the y's and x 's are normally distributed (or approximately so), we know that the estimates of , E(xixi) and Vw are independent (or approximately so).
In principle these two sets of relations have a similar status to that of the previous
covariance conditions (5.10) and (5.11). Both sets impose restrictions between the parameters of interest b,  or  , and  , and the coefficients of  and covariances

10 The way in which the  or  matrices are parameterized may imply, however, some a priori restrictions on the covariance matrix of the disturbances. Note that if the overall disturbances include additional disturbance terms uncorrelated with the x's, their covariance matrix is modified, while the parameterization of  and of  remains unchanged. Random (uncorrelated) coefficients, for example, can be the source of such additional disturbances.

5 The Chamberlain Approach to Panel Data

125

of yi and xi (to be estimated in the first stage of the Chamberlain method). Both can be used as estimating equations for the parameters of interest in the second stage ALS.11
In practice, however, important differences must be noted. First, one may not be willing to impose any structure on the covariance matrix of the disturbances , because one is not confident or simply not interested in doing so.12 Second the restrictions on  are more complicated. They necessarily involve the moments E(xixi) and E(yiyi) (or Vw) in addition to the  matrix, and they are nonlinear in the parameters of interest. In Appendix C: Manipulation of equations and parameters in the ALS
framework, we show that the nonlinearity problem can be partially overcome if we proceed in three stages instead of two (that is if we impose the restrictions on  only in a third stage).

5.2.5 A Generalization of the Chamberlain Method

In this section we provide an extended view of the Chamberlain methodology which applies to more general models than the basic regression setting just considered. In Appendix A: An extended view of the Chamberlain method, we show how it applies more specifically to simultaneous equations models, vector autoregressions (VAR), and endogenous attrition models.
Assuming it is linear in its disturbances, an econometric model can always be written as a set of stochastic equations expressing these disturbances vi in terms of the variables zi = (yi, xi) ,without making an explicit distinction between the dependent and explanatory variables yi and xi. We can thus write:
vi = vi(zi,  ) = A( )zi + d with E (vi) = 0 and E(vivi) = ( ),
leading to the following moment conditions or estimating equations:

A ( ) E zi + d = 0  ( ) = A ( ) E zizi A ( ) + dd

(5.21)

11 The parameterization of  ( ) of  will usually be linear. This is the case for example when the disturbances have an error component structure, and when the time varying disturbances follow
an MA process. However in some interesting cases the parameterization may be non linear: for example, when the time varying disturbances follow an AR(1) process uit = uit-1 + wit , where wit is a white noise. However, in this situation the model can be transformed by the quasi-difference operator [I - L] into an autoregressive regression, leading to the estimating equations [I - L]  = [I - L] M (b)+( )E (xixi)-1 where   is now the parameterization of the covariance matrix
E [wixi] and the variance matrix  = E(wiwi) can be linearly parameterized. 12 On the contrary, one may be particularly interested in the serial correlation structure of the dependent variable per se without being willing to make any assumption on the  matrix. In this case, we have M(b) =  with ( ) = ( ) = 0, so that Vw =  ( ) are the only equations to be considered.

126

B. Cre´pon and J. Mairesse

where A is a matrix parameterized by a vector of parameters  of (primary and secondary) interest, d is a vector of constants, usually period or year constants, and  is the covariance matrix of the disturbances also parameterized by  .
Writing a model in such a general form usually involves many parameters with
very little substantive content, in which we are not really interested. For example,
in all the cases considered so far, the matrix E(xixi) of second order moments of the explanatory variables is unconstrained and implicitly parameterized by its KT (KT +
1)/2 individual elements. The problem is thus how to eliminate the set, or only a subset, of parameters  s of secondary interest, in such a way that it does not affect the efficiency of the ALS estimator for the remaining subset of parameters  p. The intuitive solution is to solve for  s in an appropriate subset of equations, as function of the  p and the moments of the variables; then to substitute them in the remaining equations, so that they only include  p and the moments; and finally to proceed to the estimation on these equations.
It can be shown that the ALS estimators of the parameters of interest on the
transformed and reduced set of equations can be as efficient as those on the full
set of equations, as long as the numbers of eliminated parameters and eliminated
equations are equal (Cre´pon, Kramarz and Trognon, 1998). This result can be very
useful in practice. For example, one can get rid of all the constant terms d in the
equations (as long as there are no constraints on them) by discarding the first order
conditions and centering the variables at their overall means, or at their period or
year means. The reduced set of moment conditions simply becomes:

( ) = A( )E(zizi)A( ) - A( )E(zi)E(zi)A( ) = A( )Vzi A( )

(5.22) (5.23)

where Vzi = E zizi - E zi E zi . An even more obvious case is when the parameters to be eliminated are separately
entering one equation only. Then one has simply to drop such equations. We can
thus discard all the equations corresponding to the matrix E(xixi) of second order moments of the explanatory variables, if the model does not imply any restrictions
involving it (contrary, for example, to the VAR model we consider in Appendix A).
Likewise, we can eliminate the equations corresponding to the matrix E(yiyi) of the second order moments of the dependent variable if no restrictions are made on
the covariance matrix E(vivi) of the disturbances (contrary to what we do in the previous Sect. 5.2.4 and again in the case of the VAR model in Appendix A).

5.2.6 The Vector Representation of the Chamberlain Estimating Equations
In practice, in order to apply Asymptotic Least squares (ALS) to the Chamberlain estimating equations, we have to write these equations in vector form rather than in matrix form. A systematic method to do this is to apply a "Vec" operator, which

5 The Chamberlain Approach to Panel Data

127

simply stacks one above the others the columns of a matrix. This operator has some

convenient properties that makes such transformations easy and powerful. If A =

(c1 · · · cK), by definition



Vec(A)

=



c1 ...



,

(5.24)

cK

and it can be shown in particular that if the matrix A of dimension L × K is the
external product (v1v2) of the two column vectors v1 and v2 of dimension L and K respectively, then the column vector Vec(A) of dimension LK is equal to the Kronecker product of v1 by v2: Vec(A) = Vec(v1v2) = v2  v1.
In Appendix B we recall some other properties of this operator and apply them to
show that the vector representation of the four important specifications of the linear
regression model (5.5) considered in Sect. 5.2.3, can take the general form:

 - H(m) = 0,

(5.25)

where  = Vec( ) is the column vector of the matrix  stacked by rows,  is the parameter column vector of parameters of primary and secondary interest, and H is
a matrix function of m, where m and the dimension of H vary with the specification.
While the matrix H is constant in the case of Correlated Effects (CE), it depends on m = E(xixi) in the cases of Errors in Variables (EV) and of Weak Simultaneity (WS). In the case of a Lagged Dependent Variable (LDV) specification, H is a function of m =  itself, implying an implicit relation between  and .

5.2.7 The Estimation of Matrix 
The auxiliary parameters entering the Chamberlain estimating equations are the moments of the variables, or are functions of them as the coefficients of ; they are also functions of the covariance matrix Vw of the residuals wi of the linear projection of yi on xi, if restrictions are imposed on the serial correlation of the disturbances. We not only have to estimate these moments, but also the asymptotic variances of their limiting distribution, since these variances play an important role in the practical implementation of ALS estimators in the second stage of the Chamberlain method. As we shall see in the next Sect. 5.3, they are necessary to compute consistent estimators for the standard errors of the estimated parameters of interest, and they are also needed in order to compute the optimal (efficient) ALS estimators and to perform specification tests.
The estimation of the moments and their asymptotic variances is straightforward. Using the notation zi = (yi, xi), we want to estimate the vector of moments m = E(mi) and its covariance matrix Vm = Var(mi), where mi =Vec(zizi) = (zi  zi), or more precisely mi = D(zi  zi), D being a selection matrix which keeps only the

128

B. Cre´pon and J. Mairesse

different cross-products from Vec(zizi).13 The expectation m and covariance Vm in the population are estimated by the corresponding empirical mean m and covariance
Vm in the sample:

 1 N

m

=

N

mi
i=1

and

 Vm

=

1 N

N
(mi
i=1

- m)(mi

- m)

.

Direct application of the weak law of large numbers and of the central limit theorem tells that under very general conditions m is a consistent estimator of m = E (mi) with a normal asymptotic distribution of covariance V m, which is itself consistently
estimated by V m :

m -P E (mi)

with

 N(m

-

E

(mi))

-D

N(0,V

m)

and V m -P V m.

5.2.7.1 Estimation of Matrix  Alone

The vector  (= vec ( )), formed by stacking the column vectors of the transposed matrix , can be directly estimated as the vector of coefficients in the overall system of the T stacked year regressions of yi on IT  xi:

yi = (IT  xi) + wi.

The Generalized Least Squares (GLS) estimator  of  is given by



= =

(IT  xi) (IT  xi)-1(IT

IT



-1
xixi yi



xi

 xi)yi

(5.26)

where a bar over an expression h(zi) stands for the empirical mean over the sample (i.e., h(zi) = 1/N i h(zi)).14 This estimator follows asymptotically a normal distri-
bution and its asymptotic covariance matrix V is equal to:

V = [IT  E(xixi)-1]E(wiwi  xixi)[IT  E(xixi)-1].

(5.27)

13 Using the operator Vech for a symetric matrix, one can also write mi = Vech(zizi). See Appendix B.
14 Note that the GLS estimator  is identical to the estimator obtained by stacking as a column vector the T separate OLS estimators of the row vectors of coefficients (t.) in the T separate year regressions yit =t.xi + wit (or (5.2)), since these T separate regressions have the same regressors xi. The GLS asymptotic covariance matrix V in the case of homoscedastic errors also coincides with the corresponding asymptotic covariance matrix estimated on the basis of the T separate OLS
estimators. However, it is also consistently estimated in the case of heteroscedastic errors (see
below), while the latter is not.

5 The Chamberlain Approach to Panel Data

129

V can be consistently estimated by V obtained by replacing in (5.27) the expectations E(xixi)-1 by the sample averages xixi-1 and the errors wi by the estimated residuals wi = yi - (IT  xi).
V is robust to the heteroscedasticity of the errors wi (White 1980). Note, however, that the middle term E(wiwi  xixi) in the expression of V includes moments of the fourth order, which can be poorly estimated. Under the assumption of ho-
moscedasticity, this term simplifies to:

E(wiwi  xixi) = E(E(wiwi|xi)  xixi) = E(wiwi)  E(xixi) ,

and V also simplifies to: Vc = E(wiwi)  E(xixi)-1 ,

(5.28)

which now involves only moments of the second order that can be more precisely estimated.

5.2.7.2 Joint Estimation of Matrix  and Other Relevant Moments

The Chamberlain estimating equations, as we have seen in Sects. 5.2.2­5.2.4, often

include, in addition to the coefficients of matrix , other relevant moments such

as E(xixi) in the cases of the Error in Variables and Weak Simultaneity specifications, and E(wiwi) when simplifying assumptions are imposed on the structure of covariance matrix of the disturbances. In such cases, the column vector of all

auxiliary parameters to be estimated in the first stage of the Chamberlain method
is not only  but e = ( , mw, mx) where mw =Vec(E(wiwi) = E(wi  wi) and mx =Vec(E(xixi) = E(xi  xi).
While mx can be directly estimated by the corresponding sample average mx, this
is not so for mw since the residuals wi are not observed and have first to be estimated themselves as wi = yi - (IT  xi). However, the estimator computed by simply taking the sample average of the Kronecker product of the estimated residuals: mw = wi  wi has the same asymptotic limiting behavior as if these residuals were exactly
known. It can thus be shown that e = ( , mw, mx) has the following asymptotic
joint normal distribution:









 N



 - wi  wi-mw

 -D

V11 N 0,  V21

V22



xi  xi-mx

V31 V23 V33

with

V11 = V as in (5.27), V21 = E wiwi  (wixiE(xixi)-1) ,

130

B. Cre´pon and J. Mairesse

V31 = E xiwi  (xixiE(xixi)-1) , V22 = E [wiwi  (wiwi)] - mwmw, V32 = E [xiwi  (xiwi)] - mxmw, V33 = E [xixi  (xixi)] - mxmx.
As indicated before for V11 = V , all the asymptotic covariance matrices Vsl involve moments of the fourth order and thus can be poorly estimated. However, also
as before, their expression can be substantially simplified under the assumption that
the distribution of the residuals wi conditional on xi is homoscedastic and normal. Under this assumption, the covariance matrix V11 of  is the one given in (5.28), and V22 is only function of the moments of second order in mw, while V21, V31 and V23 are zero matrices. Likewise, if the normality assumption can be extended to the explanatory variables x, V33 can be expressed as a similar function of the moments of second order in mx.

5.3 Asymptotic Least Squares
5.3.1 ALS Estimation
The Chamberlain second stage estimators are based on the so called Minimum Distance method (Malinvaud, 1970, Chamberlain, 1982) or also known as the Asymptotic Least Squares method (Gourieroux, Monfort and Trognon, 1985). This method applies to situations in which the n parameters of (primary and secondary) interest  to be estimated are related by ng estimating equations to n auxiliary parameters , which have already been consistently estimated. From now on in Sects. 5.3 and 5.4, we shall usually speak of the parameter  and the parameter  and we shall note, whenever needed, their true values by  0 and 0, the assumption being that these true values verify exactly the ng estimating equations g( 0, 0) = 0.

5.3.1.1 Basic Result

Let  be a consistent and asymptotically normal estimator of the auxiliary parame-

ter , computed in a covariance matrix V

first stage on a 
, that is N(

sample of - 0) -D

size N, and let N(0,V ). The

V be its principle

asymptotic of the ALS

method is to choose a  such that estimating equations g( , ) are as close as pos-

sible to zero. Since the dimension ng of g is usually larger than that n of  , it is

impossible to have exactly g( , ) = 0 and hence  is computed by minimizing a

weighted quadratic sum of the g's.

More precisely, defining  (S) as:

 (S) = Arg min g( , ) SNg( , ) ,

(5.29)

5 The Chamberlain Approach to Panel Data

131

where S = (SN)N=1,..., is a sequence of weight matrices possibly depending on the sample, and assuming that the weight matrix SN converges in probability at the
rate N to a given matrix S0 and that the functions g verify some identifiability and regularity conditions,15 it can be shown that  (S) is a consistent and asymptotically normal estimator of the true parameter  0, that is:

 N

(

(S)

-



0

)

-D

N(0,V

(S))

with

 g  g -1  g  g  g  g  g  g -1

V(S) =   S0  

  S0   V   S0     S0  

.

(5.30)

In this formula, the partial derivative matrices

g 

and

g 

are evaluated at  0, 0.

A consistent estimator V(S) can be obtained by computing them at  ,  and by

replacing V by a consistent estimator V . Obviously the choice of the weight ma-

trix affects the asymptotic behavior of the ALS estimator. In Sect. 5.3.2, we show

that there exists an optimal choice such that the corresponding ALS estimator is

asymptotically efficient.

5.3.1.2 Application to the Chamberlain Approach

The implementation of ALS is simplified when the estimating equations are linear in the parameter of interest  and thus the objective function is a simple quadratic function of  . This applies in the Chamberlain framework when the restrictions on the  matrix implied by the modeling assumptions can be written in a vector form as:

0 = H(m0) 0

(5.31)

where m0 is a vector of second order moments of the variables. As shown in Sect. 5.2.6 and Appendix B, this applies in particular for the four main specifications of correlated effects, errors in variables, weak simultaneity and lagged dependent variable and their combinations.
When this is the case, we can derive the following explicit expression for the solution  (S) of (5.29):

 (S) = [H(m) SNH(m)]-1H(m) SN = P(SN, m).

(5.32)

15 The regularity conditions are that g is twice continuously differentiable and that

g 

S0

g 

is

invertible when evaluated at the true  0. The identifiability condition is that g( , 0) = 0 implies

 =  0. This condition requires that ng  n .

132

B. Cre´pon and J. Mairesse

The implementation of ALS is further simplified when the H matrix is constant, as in the case of the correlated effects specification. In this case, the asymptotic covariance matrix of  (S) given by (5.30) simply becomes:

V(S) = [H S0H]-1H S0V S0H[H S0H]-1 = P(S0)V P(S0) .

(5.33)

To obtain both  and a consistent estimator V(S) of its covariance matrix, it is thus enough to know  and a consistent estimator of its covariance matrix V .16
If the H matrix is not a constant, as in the case of three other specifications, the
formula defining V(S) is more complicated:

V(S) = P(S0, m0)V (, m) P(S0, m0)

(5.34)

where instead of simply being V , the matrix V (, m) is

V (, m) =

I

-



H m0 m



0

V ,m

I

-



H m0 m



0

.

(5.35)

In the case of errors in variables or weak simultaneity where m is E(xixi), it is thus necessary to compute the empirical second order moments in xixi in addition to  to obtain  (S). But it is also necessary to compute the covariance matrices of these estimators, which involves moments of the fourth order, in order to estimate the
asymptotic covariance V(S). Neglecting this complication and using formula (5.33) instead of (5.34) will result in a biased estimator for the asymptotic covariance V(S).

5.3.2 The Optimal ALS Estimator

The asymptotic properties of the different ALS estimators  (S) depend on their

limiting weight matrix S0. If W

=

g 

V

g 

is invertible, there is an optimal choice

S0 = W -1 leading to an asymptotically efficient estimator (meaning that for any weight matrix S0 different from S0, there exists a symmetric positive matrix  such that: V(S0) = V(S0) +  ).17 The asymptotic covariance matrix for the optimal ALS estimator  (S) thus simplifies as follows:

16 Note that the formulas of (5.32) and (5.33) giving  and V(S) are the formulas of the

weighted least squares estimator of  0 (with weight matrix SN ) in the linear regression model

 = H(m) 0 +  providing a first order approximation to the estimating equations (5.31), with



=

-



H (m0 m

)

( 0)(m

- m0)

+

(

-

 0 ).

17

The condition that W

=

g 

V

g 

requires that there is no solution to the equation ( g / )v =

0, which, in turn, requires that the dimension of  exceeds or equal that of g : n  ng.

5 The Chamberlain Approach to Panel Data

V (S0) =

g 

S0

g 



-1

=



g 

-1

-1

g g   V  

g  

.

133
(5.36)

When the Chamberlain estimating equations take the form of (5.31): 0 = H(m0) 0 and if the matrix H is constant as in the case of the correlated effects specification, the asymptotic covariance matrix of the optimal ALS estimator simplifies further as:

V (S0) =

H V-1H

-1
.

(5.37)

Note that in the case the optimal ALS estimator is the Generalized Least Squares estimator of  in the linear regression equation  = H +  (see footnote 16 in the preceding Sect. 5.3.1.2).

5.3.2.1 Implementation of the Optimal ALS Estimator

The practical implementation of the optimal ALS estimator  (S) is actually very
similar to that of the Generalized Least Square estimator. Since the optimal weight matrix is generally unknown being a function of the true parameter  0 (and of the variance of the estimated auxiliary parameter  ), it has to be generally per-

formed in two steps. In the first step a consistent ALS estimate  is computed using

an arbitrary weight matrix (and the consistent estimates of  and V already ob-

tained). In the second step, this estimate and the previous estimates of  and V

are used to derive a consistent estimator S0 =

g 

( , )V

g 

( ,

)

-1
of the op-

timal weight matrix S0 and compute the estimator  (S0). Since S0 converges in probability to S0, the estimator  (S0) obtained in this second step is asymptotically
efficient.

It is not always necessary to implement a two step procedure in the context of the Chamberlain framework. When the estimating equations take the form 0 = f ( 0)

the covariance matrix of  is already the W matrix and there is no need for a first step, and the asymptotic covariance matrix of the optimal  (S0) is given directly by
the following expression which generalizes (5.37):

V (S0) =

 

f 

V-1

f 

-1
.

(5.38)

One advantage of the ALS is its flexibility in allowing nested estimation and testing. Let us assume that  0(of dimension n ) is itself subject to restrictions and can be expressed more parsimoniously in terms of a parameter 0 (of smaller dimension
n < n ) as:  0 - q(0) = 0. Two estimation procedures are possible: a direct one in which the estimation of  is performed on the basis of the set of estimating equations

134

B. Cre´pon and J. Mairesse

g(q(0), 0), and an indirect one in which an estimated  is obtained as previously and then used as an auxiliary parameter to estimate  on the basis of the reduced set of estimating equations:  0 - q(0) = 0. It can be shown that the direct and indirect estimators of  are asymptotically equivalent if the optimal ALS is implemented in both cases.

5.3.2.2 Finite Sample Properties of the Optimal ALS Estimator

The optimal properties of the two step ALS estimator are asymptotic ones with the

sample size N. In practice they may require a very large sample size N to hold

precisely enough. Simulation experiments (performed in the related GMM context

by Arellano and Bond, 1991, on small samples of size N = 100) tend to show that the

one step estimators may be practically as good as the two steps optimal estimator.

Moreover, these simulation experiments indicate that the estimated standard errors

of the one step estimators are satisfactory, while the estimated standard errors of the

two steps estimators can be downward biased. These results have been confirmed in

the context of nonlinear models by Bertscheck and Lechner (1995).

Such poor performance is related to the estimation of the optimal weight matrix.

This

matrix

is

the

inverse

of

W

=

g 

V

g 

.

Note

that

W

(not

its

inverse)

enters

in

(5.30) from which the standard errors of the first step estimates are obtained. Thus,

the problem is twofold: to have a good estimator of W and to have a good estima-

tor of its inverse. The reason why W may be poorly estimated is that it involves

moments of the fourth order which, for a sample of a given size, are less precisely

estimated than moments of second order. W inverse can also be poorly estimated

even if W is not, since a small error in W can lead to a magnified error in its inverse.

This happens when W is "badly conditioned", that is when the "condition number"

of this matrix is high, where the condition number of matrix A is defined as:

c(A) = ||A||.||A||-1 = max (eigenvalues of A) .18 min (eigenvalues of A)

A limiting case is when W is not invertible (and the condition number is infinite).
This can happen when the number of observations is not large enough relatively to the size of W . In the case where W = V = [IT  E(xixi)-1][E(wiwi  xixi)][IT  E(xixi)-1], W is invertible only if (wi  xi)(wi  xi) is also invertible, which requires N to be larger than KT 2 (i.e., the dimension of the column vector wi  x). For example, if T = 10 and K = 2 the number of observations N must be greater
than 200.

18 The condition number is always greater than one. Large values indicate that in the computation of the inverse of a matrix A + E the relative error (A + E)-1 - A-1 / A-1 can be high
compared with the relative error E / A (see Stewart, 1973). The condition number is an upper bound for this relative error.

5 The Chamberlain Approach to Panel Data

135

5.3.3 Specification Testing in the ALS Framework

Generally, the number of estimating equations ng is much larger than the dimension n of the parameter of interest  . A specification test of the model based on the fact that the equations must be verified at the true  0 (i.e., a test for overidentifying restrictions) can be implemented. The intuition behind the test is to check whether an appropriately weighted quadratic form of the residuals in the estimating equations is small, implying that these residuals are all small indeed. More precisely, under the null hypothesis that the estimating equations are compatible (i.e.,   0 / g( 0, 0) = 0), it can be shown that the weighted quadratic form of the residuals  (S) converges in distribution towards a 2 with (ng - n ) degrees of freedom
 (S) = Ng( (S), ) V (g( (S), )) - g( (S), ) -P 2(ng - n ), (5.39)
where [V (g( (S), ))]- is a generalized inverse of the asymptotic covariance matrix of the residuals of the estimating equations at the estimated values of the parameters,  and .19 Note that [V (g( (S), ))]- is not the given weight matrix S used in the estimator  , except when S = S is the optimal weight matrix (as explained below in Sect. 5.3.3.1). Thus, when implementing the optimal ALS in a two step procedure, the objective function for the first step is not a valid test statistic; it is only valid in the optimal ALS. In order to perform the test after the first step one has to recompute the objective function using [V (g( (S), ))]- instead of S.
It is also important to note that the test statistics  (S) are asymptotically equivalent under the null hypothesis for all weight matrices S.20 Therefore, the asymptotic properties of the tests does not depend on whether an arbitrary ALS estimator or the optimal one has been used.

5.3.3.1 Andrews' Problem

The actual implementation of the specification test in (5.38) raises a difficult problem, known as Andrews' problem (Andrews, 1985). The covariance matrix V of the residuals has the following form

V (S,W ) = V (g( (S), ))

=

I

-

g 

g 

S

g 

-1

g 

S

W

I

-

g 

g 

S

g 

-1

g 

S

19

V

is

the

asymptotic

covariance

matrix

of

the

residuals

g( (S),

)

multiplied

by

 N,

and

there-

fore the proper covariance matrix of the residuals is V /N. This is why the test statistic  (S) is

written with a factor N.

20 This means that the difference between any two test statistics ( (S1) -  (S2)) converges towards zero in probability. See Gourieroux and Monfort (1989), and also Newey (1985) for a related issue

on specification tests in GMM.

136

B. Cre´pon and J. Mairesse

with

W

=

g 

V

g 

.

By

construction,

this

is

the

covariance

matrix

of

the

pro-

jection of the vector g( 0, ) (whith asymptotic covariance matrix W ) on the space

orthogonal

to

the

subspace

generated

by

the

columns

of

the

matrix

g 

,

i.e.,

I

-

g 

g 

S

g 

-1

g 

S

g( 0, )

in the metric defined by the weight matrix S. Clearly this is not an invertible matrix.

Thus it is necessary to compute the weight matrix used in the test as a generalized

inverse of V . The Andrews' problem arises from the fact that the V matrix is not

known and has to be estimated, and from the non continuity of the generalized

inverse operation. It results that a generalized inverse of a consistent estimator of V is not necessarily a consistent estimator of a generalized inverse V -.

One way to solve this problem is to find a specific generalized inverse of

V (g( (S), )) which is a continuous function of  ,  and of V . There are two

cases in which this can be done. The first is when the test is based on the optimal

ALS estimator. The second corresponds to a re-formulation of the test based on a

reduced form of the estimating equations. 1. It can be shown that the optimal weight matrix W -1 used to implement

the optimal ALS estimator is a particular generalized inverse of the covariance

matrix V .21

Since

W

=

g 

( , 

)V

g 

( ,  )

is

a

consistent

estimator

of

W

and

since the inverse is a continuous operator, W -1 is a consistent estimator of W -1.

Therefore the test can be implemented, by using the objective function of the opti-

mal ALS as a test statistic:

Ng(



,



)

SN

g(


,



)

-D



2(ng

-

n

)

(5.40)

where   is the optimal ALS estimator, and SN = W -1 the optimal weight matrix. 2. Assume that the parameter of interest  can be solved out in terms of the aux-

iliary parameter  using n of the ng estimating equations. After the elimination of



the

remaining

ng - n

equations

h( )

=

0

must

be

verified

at

the

true

value



.
0

These equations h(0) = 0, are the direct expression of the overidentifying restric-

tions of the model and they can be simply tested with the statistic

-1

N h( )

h h V 

h() -D 2(ng - n ).

(5.41)

21 We have to verify that: VW -1V = V. If we write V = [I - P]W [I - P] , with P =

g 

g 

W

-1

g 

-1

g 

W -1,

it

is

straightforward

to

see

that

[I - P]W

=

W

[I

- P]

.

The

result

then follows from the fact that [I - P] is a projector (idempotent), which implies [I - P]2 = [I - P] .

5 The Chamberlain Approach to Panel Data

137

It can be shown that this statistic is asymptotically equivalent to the previous test statistics.22

5.4 The Equivalence of the GMM and the Chamberlain Methods

In this section we show how the extended view of the Chamberlain method given in Sect 5.2.5 can be reinterpreted in terms of the Generalized Method of Moments (GMM). More precisely we show that the relations between the moments and the parameters of interest used as the Chamberlain ALS estimating equations can also be taken as orthogonality conditions which can be used to implement GMM estimators.
Starting with the general linear model

vi = vi(zi,  0) = A( 0)zi,

and with

E(vi) = 0, and E(vivi) = ( 0).

we can write in vector form the following moment conditions:

A( 0)E(zi) = 0 D[A( 0)  A( 0]E(zi  zi) = D Vec (( 0))

(5.42)

where the selection matrix D keeps only the elements of the symetric matrix on the diagonal or below. Since these expressions are linear in the moments, they can be equivalently written as orthogonality conditions

where

E(h(zi,  0)) = 0,





A( )zi

h(zi,  ) =  D[A( )  A( )]zi  zi - D Vec (( )) 

.

(5.43) (5.44)

5.4.1 A Reminder on the GMM
Before proceeding, we recall briefly the principle of GMM estimation. As just indicated, GMM is based on the orthogonality conditions expressing that a given
22 In fact, it is one of them corresponding to the special choice of a weight matrix that only weights the equations used to eliminate  . The problem of the generalized inverse is solved here as in the case of the optimal estimator, because it is possible to find a specific generalized inverse which is a continuous function of the parameters and variances and therefore can be consistently estimated by replacing these parameters and variances by their estimates.

138

B. Cre´pon and J. Mairesse

function h of the parameter  and the variables z has a zero expectation at the true value  0 of the parameter. The principle of estimation is to minimize a quadratic form of the empirical counterpart of these orthogonality conditions with
respect to 

 =  (S) = Arg min


1 N

 i

h(zi,



)

SN

1 N

 i

h(zi,



)

,

where S = [SN]N=1,... is a sequence of weight matrices, and  =  (S) is the resulting GMM estimator of  .
Under some identifiability and regularity conditions on h, it can be shown that whatever the choice of the sequence of weight matrices S, provided it converges in probability to a weight matrix S0, the GMM estimator  converges in probability to the true  0 and is asymptotically normally distributed, with an asymptotic covariance matrix V(S) of  depending on S0. More precisely, we have
 N( -  0)  N(0,V(S)),
with
V(S) = G( 0) S0G( 0) -1 G( 0) S0W S0G( 0) G( 0) S0G( 0) -1 ,

where G( ) = E

h 

(zi,



)

and W = V (h(zi,  0)) = E(h(zi,  0)h(zi,  0) ).

This matrix V(S) is a function of both G( 0) and W , which are unknown, but can

be estimated consistently by:

G( )

=

1 N

 i

 h(zi,  ) 

and

W

=

1 N

 h(zi,  )h(zi,  ) i

.

As for the ALS (see Sect. 5.3.2), there is a special choice S0 of the limit of the weight matrices S that makes the corresponding estimator optimal (based on the same set of orthogonality conditions). This corresponds to S0 = W -1, the inverse of the covariance matrix of the orthogonality conditions. In this case the asymptotic
covariance matrix of  becomes

V(S0) = G( 0) W -1G( 0) -1 .
Since the W matrix is unknown, the optimal GMM estimator cannot be directly implemented. As for the ALS it is necessary to proceed in two steps and to compute in a first step a consistent estimator W based on a GMM estimator with an arbitrary weight matrix. Note that the considerations concerning the small sample properties of the optimal ALS estimators also apply to the optimal GMM estimators (see Sect. 5.3.2.2).

5 The Chamberlain Approach to Panel Data

139

5.4.2 Equivalence of the GMM and the Chamberlain Methods

Denoting Zi the vector formed by the independent elements of zi and zi  zi , the ALS estimating equations (5.41) and the GMM orthogonality conditions (5.42) can be rewritten as:
B( 0)E(Zi) -C( 0) = 0
and E(B( 0)Zi -C( 0)) = 0.
It is easy to see that if we use the same weight matrix S, both estimators are not only asymptotically equivalent but also numerically identical. Indeed the ALS estimator  =  (S) results from the minimization of

B(

)

1 N

 i

Zi

- C(

)

S

B(

)

1 N

 i

Zi

- C(

)

and the GMM estimator  =  (S) from that of

1 N

[B( i

)Zi

- C(

)]

S

1 N

[B( i

)Zi

- C(

)]

.

The two estimators are identical, since the two objective functions are obviously the

same:

1 N

[B( i

)Zi

- C(

)]

=

B(

)

1 N

 i

Zi

-

C(

).

It follows that the optimal estimators are also identical. Indeed, we can verify that the optimal ALS weight matrix SA LS = [B( 0)V (Zi)B( 0) ]-1 is obviously equal to the optimal GMM weight matrix SG MM = [V (B( 0)Zi -C( 0))]-1.
In practice, however, the optimal weight matrices SA LS and SG MM have to be estimated, and this can be done in several ways. Numerical differences between the
optimal ALS and GMM estimators can thus arise in small samples. Let us mention
three reasons why this is actually happening. The first one is just a simple matter
of computation, while the other two are related to the different ways in which the
Chamberlain method and the GMM are implemented. Assume that we dispose of a first step estimate  1 =  1 obtained with either
one of the two methods for a given weight matrix. The ALS optimal weight matrix is computed as SA LS = [B( 1)V (Zi)B( 1) ]-1. For GMM, let us denote by Ri the residual of the orthogonality condition for the observation i: Ri = B( 1)Zi -C( 1). Since E(Ri) = 0, there are two consistent estimators of the optimal GMM weight matrix:

SG1MM = RiRi SG2MM = RiRi - Ri Ri = Ri - Ri

Ri - Ri

140

B. Cre´pon and J. Mairesse

where a bar over an expression stands for its empirical mean over the sample. It is usually SG1MM which is computed, while it is SG2MM which equals SA LS.
The results on the elimination of parameters of secondary interest presented in Appendix C can be extended to the GMM.23 If the number of discarded orthogonality conditions is the same as the number of eliminated parameters, there is no loss of asymptotic efficiency in the estimation of the remaining parameters; if it is larger, there is a loss of asymptotic efficiency, but consistency is preserved. Contrary to the Chamberlain approach, the usual practice of GMM amounts to considering orthogonality conditions which only involve the parameters of primary interest (and implicitly eliminating the parameters of secondary interest). If all such orthogonality conditions are taken into account, both the optimal Chamberlain and GMM estimators are equivalent but not identical, since they are not computed on the basis of the same weight matrix S. If only a subset of them is used (for example, the ones corresponding to the most obvious valid instruments), the GMM estimator is less efficient than the Chamberlain estimator.
The GMM always requires a two step estimation to implement the optimal estimator. In the Chamberlain method this is not always the case. When the estimating equations take the form of a direct linear restriction on , the optimal weight matrix is simply the inverse of the covariance matrix of the  estimator (see 5.37). It is also important to note that in the Chamberlain approach it is possible to keep the same weight matrix when considering a sequence of nested specifications, while in the GMM case a weight matrix must be computed for each different specification. This may be in practice an advantage of the Chamberlain approach.

5.4.3 Equivalence in Specific Cases

We have just shown the general equivalence of the Chamberlain and GMM estimators when all restrictions on all (first and second order) moments of the variables are considered. It is straightforward to see that the equivalence holds as well when we only focus on the conditions involving the joint moments of the dependent and explanatory variables, and any given subset of moment conditions. The ALS estimators based on the estimating equations as written in Sect. 5.2.2 is

0 = M(b0) + ( 0)E xixi -1

or

E(yixi) = M(b0)E

xixi

+

(

)
0

which are clearly equivalent to the GMM estimators based on the orthogonality

conditions

E yixi - M(b0)xixi - ( 0) = 0.

23 See Cre´pon, Kramarz and Trognon (1998).

5 The Chamberlain Approach to Panel Data

141

It may be of some interest to be more specific and illustrate the general equivalence of the GMM and Chamberlain methods in the three important cases of Correlated Effects, Errors in Variables and Weak Simultaneity.

5.4.3.1 Correlated Effects
Let us suppose for convenience that we only have one explanatory variable. Thus we have T 2 estimating equations, or orthogonality conditions, for one parameter of primary interest. We also have T parameters of secondary interest corresponding to the correlated effects. From the expression of  given in Sect. 5.2.2 the T 2 orthogonality conditions can be written as
E yixi - bxixi - l = 0,
where the  's are the covariances between the individual effects and the yearly x's. It is easy to see that since the T × T matrix  = l is constant in columns, premultiplying it by the (T - 1)×T difference matrix  results in a (T - 1)×T zero matrix.24 Thus premultiplying the T 2 orthogonality conditions by  eliminates the T parameters of secondary interest  , and gives (T - 1)×T transformed orthogonality conditions:
E( yixi - bxixi ) = E((yi - bxi)xi) = E((ui)xi) = 0.
This new set of orthogonality conditions simply expresses that the different year x's (in levels) can be used as instruments for the model after transforming it in first differences. They are clearly equivalent to the original set of conditions, since we have lost exactly T of them in eliminating the T parameters of secondary interest  .

5.4.3.2 Errors in Variables

Assuming like in Sect. 5.2.3 that we have serially uncorrelated errors in variables eit (but with possible varying variances E e2it ) and combining them with correlated effects, the orthogonality conditions can be written as

 1 + 1 2

. . . T



E

(yi

xi

-

bxi

xi

)

=



1 ...

2 + 2 T ...



1

2

T + T

24 See footnote 48 in Sect. 5.8.2 for the definition of the difference matrix .

142

B. Cre´pon and J. Mairesse

where t = cov (ixit ) as in the CE case, and t = -bE e2it . We have now 2T parameters of secondary interest, and we are thus looking for T 2 - 2T transformed orthogonality conditions only involving the parameter of primary interest b. If we transform the model in first differences, and consider the year levels of x as potential instruments, clearly the past values lagged by two years and more and the future values are valid instruments, while the present values and the past values lagged by only one year are not since

E (vit xis) = E (((uit - beit ) - (uit-1 - beit-1)) (xis + eis)) = 0 if s = t and s = (t - 1) = -bE ei2s if s = t = bE e2is if s = (t - 1)

We are thus obtaining (T - 1) (T - 2) = T 2 - 3T + 2 orthogonality conditions that involve only the parameter b, but we are still missing (T - 2) of them. These can be obtained by taking the second differences of the model, and instrumenting them by the in­between year levels of x. Clearly we have E (vit xit ) = -E (vit+1xit ), and thus
E ((vit+1 - vit-1) xit ) = 0.
These new (T - 2) equations are by construction independent of the preceding ones. The total set of T (T - 2) orthogonality conditions are those considered by
Griliches and Hausman (1986); they provide estimates as efficient as the ones of the Chamberlain method which is based on the T 2 estimating equations and gives estimates of both the parameter of primary interest and the 2T parameters of secondary interest.

5.4.3.3 Weak Simultaneity

In this case it is assumed that the current shocks are uncorrelated with the past values

of the explanatory variable x (although they may affect its present and future val-

ues). It is easy to see that the matrix  must be upper triangular (see Sect. 5.2.3.3). Combining correlated effects and weak simultaneity, we have the T 2 orthogonality

conditions

 1 + 11 2 + 12 . . . T + 1T 

E (yi xi

-

bxixi)

=



1 ...

2 + 22 . . . T + 2T 

1

2

T + T T

where there is now T (T + 1) /2 parameters of secondary interest st = cov (uis, xit ) for s  t in addition to the T previous ones t = cov (ixit ) for correlated effects.

5 The Chamberlain Approach to Panel Data

143

However, in total, there are only (T (T + 1) /2) + (T - 1) parameters, since only T of the (T + 1) parameters T , 1T , . . . , TT can be identified (from the T covariances of y and the last year level xiT ), and we need T 2 - T (T + 1) /2 - (T - 1) = T 2 - 3T + 2 /2 = (T - 1) (T - 2) /2 orthogonality conditions in terms of the pa-
rameter of interest only. These are exactly provided by instrumenting the model
transformed in first differences with past levels of x lagged by two years and more:

E(uit xis) = E(uit xis) - E(uit-1xis) = 0 if s < (t - 1) .25

5.4.3.4 Restriction on the Covariance Matrix of the Disturbances

Finally, it is worthwhile to consider also the case in which assumptions can be made about the covariance structure of the disturbances. Considering for example the case in which we assume an error components structure, we know that:

E (visvit ) = 2 + u2t t=s,

(5.45)

where u2t is the variance of the random shock uit at time t. This corresponds to (T (T + 1) /2) new equations, to (T + 1) new parameters: 2, u21, . . . , u2T , and thus to ((T - 2) (T + 1) /2) supplementary orthogonality conditions in terms of the parameters of primary interest. We can derive them from the equations (5.45) by writing:
E (vit vis) = E (vit-1vis) , if s < (t - 1) ,

or: and thus:

E (uit uis) = 0, if s < (t - 1) ,

E (uit yis) = E (uit (bxis + i + uis)) = bE (uit xis) + E (uit i) + E (uit uis) = 0, if s < (t - 1) .

These new (T - 1) (T - 2) /2 orthogonality conditions are simply expressing that the past values of y lagged by two years or more are valid instruments. The (T - 2) missing orthogonality conditions are less straightforward to derive (see Ahn and Schmidt 1995). They can be deducted by writing E (uit uit ) = u2t and E (uit uit-1) = u2t-1, implying the (T - 2) conditions:
E ((uit+1 - uit-1) uit ) = 0,

which can be rewritten as:

25 Note that if we can assume stationarity, we can obtain an additional set of orthogonality conditions E(uit xis) = 0 for s < t - 1. See last paragraph of Sect. 5.6.2, and Arellano and Bover (1995) and Blundell and Bond (1998).

144

B. Cre´pon and J. Mairesse

bE ((uit+1 - uit-1) xit ) + E ((uit+1 - uit-1) yit ) = 0.

In the case of Weak Simultaneity, these additional orthogonality conditions are nonlinear in the parameters of interest b, but in the case of both Correlated Effects and Errors in Variables, they simply become:

E [(uit+1 - uit-1) yit ] = 0,

expressing that the in­between year levels of y can be used as valid instruments for the model transformed in second differences.

5.5 Monte Carlo Simulations
To give a feeling of how the Chamberlain method and the GMM perform in practice, we conducted some plausibly calibrated Monte-Carlo simulation experiments (see Tables 5.1 and 5.2). We consider the consistency and efficiency of the different estimators (see Tables 5.3­5.5), for simulated panel data samples of different lenght (T = 3 and 6) and size (N = 100, 400 and 1600), in different "scenarios" corresponding to two main types of specification errors: Correlated Effects and Errors in Variables. We are also concerned with the consistency of the estimated standard errors (see Tables 5.6­5.8) and the performance of specification tests (see Tables 5.9 and 5.10). But first of all, let us provide some indications necessary to understand the design of the simulation experiments and thus the validity and limits of our results (for more details see Appendix E: Design of simulation experiments).

5.5.1 Design of the Simulation Experiments
Taking the simple linear regression model yit = xit + (i + uit ) with only one explanatory variable, we consider three basic "scenarios". The first scenario is that of Non Correlated Errors (NCE), in which the explanatory variable x is uncorrelated with both disturbance terms i and uit . The second one is that of Correlated Effects (CE) where the variable x is correlated with the individual effect i, but remains uncorrelated with uit . The first scenario thus corresponds to the standard basic case, while the second is usually regarded as more realistic since it takes advantage of the avaibility of panel data to control for potential unobserved correlated individual effects. Our third scenario combines the Correlated Effects and Errors in Variables cases (CE + EV) and can be considered as even more realistic. It assumes that the true explanatory variable x in the model is not observed and that the observed variable x is measured with a random measurement error, resulting in an additional error term, say it , in the model and implying a correlation of the current xit with the current it , but not with the future and past is (s > t and s < t). We have also experimented with a fourth scenario combining Correlated Effects and Weak Simultaneity (CE + WS), as well as with scenarios with EV and WS only. These scenarios do

5 The Chamberlain Approach to Panel Data

145

not provide much more insight than the three considered here, and we do not report their results.
We calibrated the experiments so that we can reproduce some of the features found in real data sets, in particular when estimating production functions on firm panel data, as in Mairesse (1990) and Griliches and Mairesse (1998), with y and x measuring respectively the log of the firm labor productivity and the log of the firm capital to labor ratio. Normalizing the (average) true value of the parameter of primary interest  to be 0.5, we assume that the variances of the (simulated) explanatory variable x and of the (simulated) disturbances (i + uit ) are of the same order of magnitude, normalizing them to be both 1. We also assume that most of the variability arises from the cross­sectional dimension of the data: that is, generating x as the sum of a between component i and a within component it , we take Var(i) = 0.8 and Var(it ) = 0.2, and similarly we choose Var(i) = 0.8 and Var(uit ) = 0.2. Note that in addition we assume that the within component (it ) of x is highly serially correlated according to a first order autocorrelation process with parameter 0.7, while we maintain the assumption that the usual errors uit are not (auto)correlated. This implies that the past and future values of x can be used as valid and effective instruments in the case of the CE and CE+EV scenarios. Next, we calibrated the correlated effects and the errors in variables so that we obtain for the true coefficient  of 0.5 an asymptotic upward bias of 0.2 in the cross­sectional dimension (for the usual between estimator) and a downward bias of 0.2 in the time series dimension (for the usual within estimator).26 Finally, we introduced a fair amount of x related heteroscedasticity in the model by assuming that  is not constant but randomly distributed across individuals (with mean 0.5 and standard deviation 0.2), and thus adding to the model another disturbance term of the form (i - )xit .27
For each of the three scenarios we experimented with six panels of different length and size, covering a set of values comparable to those found in many empirical studies. We combined two time spans: a short one (T = 3) and an average one (T = 6), with three cross-section sizes: a small, a medium and a large one (N = 100, N = 400, and N = 1600). For all eighteen resulting configurations we performed one hundred Monte-Carlo replications, on the basis of which we can compare the distributions of the different estimators.
For each simulated sample, in addition to the usual estimators (so called total, between, within, first differences and long differences), we computed four types of Chamberlain's and GMM estimators. These estimators correspond to a sequence of plausible specification errors that an econometrician, without knowing of course the true model specification, might be willing to compute and compare. The first three types of estimators match our three scenarios, being respectively based on the

26 These values are large but remain in the plausible set. Note that, since the (asymptotic) biases of the different estimators are linear functions of these values, simulations with other values do not add much to the analysis. 27 The relative amount of heteroscedasticity generated by this additional disturbance term may be on the low side (see Mairesse, 1990). Note that this term is an additional source of serial correlation in the disturbances, but does not affect the validity of the past and future values of x as instruments.

146

B. Cre´pon and J. Mairesse

assumptions of NCE, CE, and CE+EV specifications. The fourth type assumes a CE+WS specification encompassing the first three ones.
These four types of Chamberlain's and GMM estimators are based on four sets of estimating equations or orthogonality conditions which are sequentially nested They can thus be directly compared in terms of increasing robustness and decreasing efficiency, and they allow for straightforward specification tests. The CE+WS specification only requires the past x to be uncorrelated with the current uit which implies (T - 1)(T - 2)/2 orthogonality conditions for , that is respectively 1 and 10 for T = 3 and T = 6. The CE+EV specification allows the future x to be also uncorrelated with the current uit , which results in T (T - 2) (equals (T - 1)(T - 2)/2 + (T - 1)(T - 2)/2 + (T - 2)) orthogonality conditions for , that is respectively 3 and 24 for T = 3 and T = 6. In the CE specification all the x (present, past and future) are uncorrelated with uit which results in T (T - 1) (equals T (T - 2) + T ) orthogonality conditions, that is respectively 6 and 30 for T = 3 and T = 6. In the NCE specification all the x's are also uncorrelated with the individual effects i which leads to T 2 (equals T (T - 1) + T ) orthogonality conditions, that is respectively 9 and 36 for T = 3 and T = 6.
For all four assumed specifications we have computed two Chamberlain estimators: CHAM1 and CHAM2. CHAM1 is obtained using as weight matrix the inverse of the covariance matrix of  estimated under the assumption of homoscedasticity, while CHAM2 is based on the heteroscedasticity consistent estimated covariance matrix of .28 We have also computed the two comparable GMM estimators: GMM1 and GMM2. GMM1 is the first step estimator using as weight matrix for the orthogonality conditions the inverse of the second order moment matrix of the appropriate instruments, while GMM2 is the corresponding optimal second step estimator.
To summarize the behaviour of our different estimators, we computed the means and standard deviations of their observed distribution over the 100 Monte-Carlo replications. The discussion of the consistency and efficiency to which we turn next is mainly based on these summary statistics as given in Tables 5.2­5.5. For all simulated samples we also computed the asymptotic standard errors of the different estimators and compared their means over the Monte-Carlo replications with the observed standard deviations of the estimators (obtained from the Monte-Carlo replications).29 These results are shown in Tables 5.6­5.8. We also performed several conditional and unconditional 2 specification tests (of overidentifying restrictions) for all four assumed specifications. The results are summarized in Tables 5.9 and 5.10, giving the number of rejections for the 5% significance level over the 100 replications.

28 CHAM2 is theoretically optimal for the NCE and CE specifications but not fully so for the CE+EV and CE+WS specifications. For these two specifications the weight matrix V-1 is not strictly the optimal one, since it does not account for the the fact that E(xixi), the variance­ covariance matrix of the x s, is estimated.
29 In fact we prefered to use the square roots of the means of the asymptotic variances rather than
the means of the square roots. However, this does not seem to make any difference.

5 The Chamberlain Approach to Panel Data

147

5.5.2 Consistency and Bias

The overall plausibility of our simulation design can be appreciated by comparing the usual panel data estimates on the simulated data and those which we found for real data sets and on which we largely based our calibration choices. Table 5.1 presents such estimates of the elasticity of capital parameter in a Cobb­Douglas production function (with constant return to scale) for three samples of manufacturing firms in France, Japan and the US. Table 5.2 shows, for our three scenarios and the average period--medium size configuration (T = 6 and N = 400), the means over the 100 Monte Carlo replications of the corresponding usual panel data estimates of our parameter of interest . The results are very similar for the other five sample configurations.
The basic divergence between the cross-sectional and time series estimates, the fact that the total and between regression estimates tend to be significantly larger than the within, the first and long differences estimates, which is found in the three country samples, is reproduced in the simulated samples when the CE and CE+EV scenarios are enacted. The other revealing discrepancy among the group of time series estimates, namely the fact that the first differences estimates tend to be lower

Table 5.1 Usual panel data estimates of a Cobb-Douglas production function French, Japan and US Manufacturing (1967­1979)

Type of Estimator

France N = 441 T = 13

Japan N = 845 T = 13

USA N = 462 T = 13

Total

0.303 [0.009] 0.174

0.452 [0.007] 0.292

0.221 [0.007] 0.154

Between

0.313 [0.031] 0.192

0.469 [0.023] 0.326

0.222 [0.024] 0.163

Within

0.196 [0.011] 0.052

0.278 [0.009] 0.082

0.213 [0.008] 0.096

First differences

0.260 [0.014] 0.064

0.183 [0.010] 0.031

0.289 [0.009] 0.149

Long differences

0.163 [0.039] 0.038

0.359 [0.029] 0.151

0.178 [0.030] 0.073

In each cell, the first number is the estimated elasticity of capital, the second in brackets is the estimated standard error and the third is the regression R2. Log (Qit /Lit ) = Log (Cit /Lit ) + i + uit , where Q is the deflated sales, L is the number of em-
ployees, and C is the gross book value adjusted for inflation.

148

B. Cre´pon and J. Mairesse

Table 5.2 Usual estimates for three different (true) specifications Means and standard deviations for 100 replications Simulated samples for N = 400 and T = 6

Type of estimator

True specification

NCE

CE

CEEV

Total

0.501 [0.052] 0.198

· 0.659 [0.052] 0.303

· 0.619 [0.052] 0.272

Between

0.501 [0.056] 0.211

· 0.674 [0.057] 0.333

· 0.673 [0.059] 0.323

Within

0.507 [0.032] 0.123

0.507 [0.032] 0.123

· 0.232 [0.028] 0.036

First differences

0.503 [0.046] 0.074

0.503 [0.046] 0.074

· 0.162 [0.036] 0.020

Long differences

0.503 [0.047] 0.186

0.503 [0.047] 0.186

· 0.289 [0.050] 0.071

In each cell the first number and second one in brackets are respectively the mean and standard
deviation of the estimated  for 100 replications, while the third number is the mean of the regression R2 for the 100 replications. · Denotes that the estimator is inconsistent.

than the within and the long differences ones, which can clearly be seen for the Japanese sample, is also exhibited by the CE+EV scenario.30 Note also, that the R2 of the different regressions for the simulated samples in this last scenario are rather close to the corresponding R2 of the regressions for the real samples.
Going beyond the usual set of estimators, we can look at the consistency of the more sophisticated Chamberlain and GMM estimators in Tables 5.3, 5.4 and 5.5. These tables give the means and standard deviations of these estimators (as computed from the Monte-Carlo replications) for the twelve combinations of assumed specifications (NCE, CE, CE+EV, CE+WS) and true specifications (NCE, CE, CE+EV). These tables are given for the three average period configurations (T = 6). We do not report here the results for the three other configurations (T = 3), but we find that the different estimators behave very similarly.
As expected, the Chamberlain and GMM estimators appear to be consistent when they should be, that is when the assumed specification is the true one or when the assumed specification encompasses the true one. On the other hand, these estimators are biased when the assumed specification is wrong, that is when it is incompatible with the true specification; these cases are marked with a bullet in the Tables.
30 These discrepancies among the usual panel data estimates of the production function are much more pronounced when the returns to scale are not constrained to be one. See Mairesse (1990) for more details.

5 The Chamberlain Approach to Panel Data

149

Table 5.3 Chamberlain's and GMM estimates under four different specification assumptions for three different (true) specifications Means and standard deviations for 100 replications Simulated samples for N = 100 and T = 6

Assumed

Type of

True specification

specification

estimator

NCE

CE

CEEV

NCE CE

CHAMB1 CHAMB2 GMM1 GMM2
CHAMB1 CHAMB2 GMM1 GMM2

0.510 [0.061] 0.498 [0.071] 0.513 [0.089] 0.504 [0.069]
0.506 [0.068] 0.497 [0.077] 0.503 [0.097] 0.498 [0.075]

· 0.560 [0.062] · 0.551 [0.075] · 0.668 [0.094] · 0.606 [0.075]
0.507 [0.068] 0.497 [0.077] 0.503 [0.097] 0.498 [0.075]

· 0.343
[0.057] · 0.334
[0.073] · 0.629
[0.094] · 0.497
[0.078]
· 0.236
[0.059] · 0.231
[0.074] · 0.172
[0.078] · 0.205
[0.068]

CE+EV

CHAMB1 CHAMB2 GMM1 GMM2

0.510 [0.114] 0.505 [0.120] 0.500 [0.177] 0.504 [0.122]

0.510 [0.115] 0.507 [0.120] 0.495 [0.178] 0.503 [0.122]

0.415 [0.174] 0.413 [0.197] 0.275 [0.271] 0.370 [0.211]

CE+WS

CHAMB1 CHAMB2 GMM1 GMM2

0.554 [0.303] 0.555 [0.318] 0.546 [0.369] 0.562 [0.312]

0.549 [0.304] 0.545 [0.317] 0.529 [0.366] 0.548 [0.308]

0.292 [0.385] 0.311 [0.396] 0.210 [0.398] 0.287 [0.398]

· Denotes that the estimator is inconsistent. In each cell the first number is the mean and the second the standard deviation of brackets are respectively the mean and standard deviation of ^ over 100 replications.

Nevertheless, it can be seen that for the smaller samples (N = 100) large finite sample biases show up in the two extreme cases in which the true specification is CE+EV and the assumed ones are CE+EV and CE+WS. In the first case (CE+EV if CE+EV) the means of the CHAMB1, CHAMB2, and GMM2 estimates are around 0.40, while in the second case (CE+EV if CE+WS) they are around 0.30. The means of the GMM1 estimates are even lower about respectively 0.30 and 0.20 in these

150

B. Cre´pon and J. Mairesse

Table 5.4 Chamberlain's and GMM estimates under four different specification assumptions for three different (true) specifications Means and standard deviations for 100 replications Simulated samples for N = 400 and T = 6

Assumed specification

Type of estimator

True specification

NCE

CE

CEEV

NCE

CHAMB1 CHAMB2 GMM1 GMM2

0.505 [0.031] 0.504 [0.031] 0.502 [0.052] 0.504 [0.031]

· 0.555
[0.031] · 0.555
[0.033] · 0.659
[0.052] · 0.571
[0.034]

· 0.333
[0.030] · 0.325
[0.031] · 0.619
[0.052] · 0.409
[0.039]

CE

CHAMB1

0.506

0.506

· 0.230

CHAMB2

[0.032] 0.505

[0.032] 0.505

[0.028] · 0.229

GMM1

[0.033] 0.503

[0.033] 0.503

[0.02] · 0.162

GMM2

[0.046] 0.505

[0.046] 0.505

[0.036] · 0.217

[0.032]

[0.032]

[0.029]

CE+EV

CHAMB1 CHAMB2 GMM1 GMM2

0.512 [0.059] 0.513 [0.063] 0.526 [0.091] 0.514 [0.062]

0.512 [0.059] 0.513 [0.063] 0.527 [0.091] 0.514 [0.062]

0.481 [0.098] 0.480 [0.104] 0.422 [0.143] 0.472 [0.104]

CE+WS

CHAMB1 CHAMB2 GMM1 GMM2

0.551 [0.175] 0.549 [0.182] 0.570 [0.211] 0.551 [0.181]

0.550 [0.173] 0.549 [0.181] 0.569 [0.211] 0.550 [0.180]

0.468 [0.282] 0.461 [0.289] 0.368 [0.354] 0.453 [0.290]

· Denotes that the estimator is inconsistent. See Table 5.3

two cases. Although these estimators, given the small sample size configuration, are extremely imprecise with computed dispersions across the 100 replications of respectively 0.2 and 0.4 in the two cases, the t tests of departure from the true value of 0.50 are quite significant.31
31 If  and s are the mean and standard deviation of the distribution of the  estimator computed from R replications, then the standard deviation of themean estimate  is s / R and the t ratio is R( - 0.5)/s . Here, we have for example: t = 100(0.4 - 0.5)/0.2 and t = 100(0.3 - 0.5)/0.4, which are both roughly equal to -5.

5 The Chamberlain Approach to Panel Data

151

Table 5.5 Chamberlain's and GMM estimates under four different specification assumptions for three different (true) specifications Means and standard deviations for 100 replications Simulated samples for N = 1600 and T = 6

Assumed specification

Type of estimator

True specification

NCE

CE

CEEV

NCE CE

CHAMB1 CHAMB2 GMM1 GMM2
CHAMB1 CHAMB2 GMM1 GMM2

0.502 [0.013] 0.502 [0.014] 0.503 [0.023] 0.502 [0.014]
0.502 [0.015] 0.502 [0.015] 0.501 [0.021] 0.502 [0.015]

· 0.553 [0.013] · 0.553 [0.014] · 0.662 [0.023] · 0.560 [0.015]
0.502 [0.015] 0.502 [0.015] 0.501 [0.021] 0.502 [0.015]

· 0.330
[0.013] · 0.321
[0.013] · 0.622 [0.022] · 0.387
[0.016]
· 0.226 [0.013] · 0.223
[0.014] · 0.161
[0.016] · 0.218
[0.014]

CE+EV

CHAMB1 CHAMB2 GMM1 GMM2

0.503 [0.027] 0.503 [0.027] 0.503 [0.050] 0.503 [0.028]

0.503 [0.027] 0.503 [0.028] 0.503 [0.050] 0.503 [0.028]

0.493 [0.049] 0.492 [0.050] 0.460 [0.076] 0.491 [0.051]

CE+WS

CHAMB1 CHAMB2 GMM1 GMM2

0.522 [0.091] 0.522 [0.093] 0.516 [0.123] 0.522 [0.094]

0.522 [0.090] 0.522 [0.092] 0.516 [0.121] 0.521 [0.092]

0.506 [0.152] 0.502 [0.149] 0.442 [0.208] 0.501 [0.152]

· Denotes that the estimator is inconsistent. See Table 5.3

It is reassuring to see, however, that for moderate sample sizes (N = 400) and even more so for larger sample sizes (N = 1600), the finite sample biases have largely vanished. Note though that in most cases of the CE+WS specification these biases tend to remain statistically significant, given that they decrease more slowly than the standard deviations. Note also that the behaviour of the GMM1 estimator (as computed here) is different and less satisfactory than that of the other three estimators.

152
5.5.3 Efficiency and Robustness

B. Cre´pon and J. Mairesse

The simulation results are also instructive from the perspective of the efficiency of the different estimators as measured by their standard deviations computed over the replications. These numbers are given in brackets in Tables 5.6­5.8 for all the cases when the estimators are consistent, that is when the assumed specification encompasses the true specification or is the true one. We can thus compare them in various ways, not only across the different estimators (for given samples sizes and assumed specifications), but also across sample sizes, and across assumed specifications.
Looking first at the efficiency of the four estimators for given sample sizes and assumed specification, we do not find any real surprise. We can first check that CHAMB2, the (nearly) optimal Chamberlain estimator, and GMM2, the optimal GMM estimator, are practically equivalent.32 This is an indication that taking the covariance matrix E(xixi) as known, in order to simplify the computation of the

Table 5.6 Simulated standard deviations and estimated standard errors of the GMM and Chamberlain estimators under four different specification assumptions for three different (true) specifications Standard deviations and means of the estimated standard errors of the estimates for 100 replications Simulated samples for N = 100 and T = 3 and 6

True specification
Assumed specification

NSE NSE CE

CE CEEV CEWS CE

CEEV CEEV CEWS CEEV CEWS

T=3

CHAMB1 CHAMB2 GMM1 GMM2

[0.081] (0.076) [0.087] (0.071) [0.111] (0.097) [0.089] (0.073)

[0.111] (0.116) [0.120] (0.110) [0.128] (0.135) [0.119] (0.114)

[1.322] (1.276) [1.349] (1.253) [1.291] (2.091) [1.437] (2.049)

[33.24] (50.11) [33.24] (50.11) [33.24] (8776) [33.24] (8776)

[0.111] (0.116) [0.122] (0.111) [0.128] (0.135) [0.120] (0.114)

[1.351] (1.245) [1.352] (1.223) [1.340] (2.160) [1.487] (2.067)

[5.157] (5.257) [5.157] (5.257) [5.157] (53.83) [5.157] (53.83)

[3.161] (3.091) [3.233] (3.074) [3.144] (46.74) [7.124] (45.35)

[9.776] (18.16) [9.776] (18.16) [9.776] (577.0) [9.776] (577.0)

T=6

CHAMB1 CHAMB2 GMM1 GMM2

[0.061] (0.052) [0.071] (0.040) [0.089] (0.091) [0.069] (0.041)

[0.068] (0.061) [0.077] (0.049) [0.097] (0.088) [0.075] (0.052)

[0.114] (0.098) [0.120] (0.084) [0.177] (0.175) [0.122] (0.089)

[0.303] (0.315) [0.318] (0.292) [0.369] (0.429) [0.312] (0.320)

[0.068] (0.061) [0.077] (0.049) [0.097] (0.88) [0.075] (0.052)

[0.115] (0.098) [0.120] (0.084) [0.178] (0.175) [0.122] (0.089)

[0.304] (0.312) [0.317] (0.291) [0.366] (0.426) [0.308] (0.317)

[0.174] (0.152) [0.197] (0.128) [0.271] (0.255) [0.211] (0.140)

[0.385] (0.360) [0.396] (0.339) [0.398] (0.472) [0.398] (0.370)

For each estimator the first number (in brackets) is the standard deviation of the estimated  over 100 replications, and the second number (in parentheses) is the mean of the estimated standard errors over the 100 replications.

32 They are strictly identical for the assumed specification CE+WS and T = 3, i.e., when there is only one orthogonality condition and thus no weighting is involved.

5 The Chamberlain Approach to Panel Data

153

Table 5.7 Simulated standard deviations and estimated standard errors of the GMM and Chamberlain estimators under four different specification assumptions for three different (true) specifications Standard deviations and the mean of the estimated standard errors of the estimates for 100 replications Simulated samples for N = 400 and T = 3 and 6

True specification
Assumed specification

NSE NSE CE

CE CEEV CEWS CE

CEEV CEEV CEWS CEEV CEWS

T=3

CHAMB1 CHAMB2 GMM1 GMM2

[0.037] (0.039) [0.038] (0.038) [0.046] (0.049) [0.038] (0.038)

[0.059] (0.059) [0.061] (0.058) [0.065] (0.068) [0.060] (0.058)

[0.543] (0.602) [0.549] (0.600) [0.551] (0.631) [0.555] (0.627)

[1.858] (2.497) [1.858] (2.497) [1.858] (13.06) [1.858] (13.06)

[0.059] (0.059) [0.060] (0.059) [0.065] (0.068) [0.060] (0.058)

[0.540] (0.598) [0.544] (0.595) [0.545] (0.625) [0.545] (0.621)

[3.543] (5.347) [3.543] (5.347) [3.543] (92.59) [3.543] (92.59)

[1.468] (1.854) [1.584] (1.843) [1.463] (9.708) [2.812] (9.608)

[20.35] (70.53) [20.35] (70.53) [20.35] (6958) [20.35] (6958)

T=6

CHAMB1 CHAMB2 GMM1 GMM2

[0.031] (0.028) [0.031] (0.026) [0.052] (0.046) [0.031] (0.026)

[0.032] (0.032) [0.033] (0.031) [0.046] (0.044) [0.032] (0.031)

[0.059] (0.053) [0.063] (0.051) [0.091] (0.090) [0.062] (0.052)

[0.175] (0.170) [0.182] (0.167) [0.211] (0.223) [0.181] (0.172)

[0.032] (0.032) [0.033] (0.031) [0.046] (0.044) [0.032] (0.031)

[0.059] (0.053) [0.063] (0.051) [0.091] (0.090) [0.062] (0.052)

[0.173] (0.170) [0.181] (0.167) [0.211] (0.223) [0.180] (0.172)

[0.098] (0.088) [0.104] (0.085) [0.143] (0.153) [0.104] (0.088)

[0.282] (0.267) [0.289] (0.262) [0.354] (0.367) [0.290] (0.282)

See Table 5.6

Chamberlain estimator in the CE+EV and CE+WS cases may have no consequences in practice.33 We can then verify that the two step estimator GMM2 is indeed more efficient than the one step estimator GMM1 by a (somewhat limited) factor of about 1.5. Finally, we can observe that the CHAMB1 estimator, which would be optimal in the case of homoscedastic errors, is in fact not less efficient than the more general CHAMB2 estimator. This should be related to the relatively modest amount of heteroscedasticity in our simulations, since the x related heteroscedasticity we have introduced, although substantial in terms of parameter heterogeneity (with
E(i - )2 = 0.2), appears small as compared to the overall variability of the errors.34
Looking next at that the efficiency of our four Chamberlain and GMM estimators with sample size, we know a priori that it should increase as N, and this is indeed verified in the simulations. In nearly all cases the standard deviations are divided by a factor of about 2, when going from N = 100 to N = 400 and
33 Further experiments not reported here showed that significant differences between the fully optimal Chamberlain estimators and the nearly optimal ones only occur in rather peculiar cases, where we have to assume that the within component of x is much larger than the between component, and is strongly correlated with the past values of the uit disturbances. 34 However, experimenting with twice as much x heteroscedasticity also did not show up in our results.

154

B. Cre´pon and J. Mairesse

Table 5.8 Simulated standard deviations and estimated standard errors of the GMM and Chamberlain estimators under four different specification assumptions for three different (true) specifications Standard deviations and the means of the estimated standard errors of the estimates for 100 replications Simulated samples for N = 1600 and T = 3 and 6

True specification
Assumed specification

NSE NSE CE

CE CEEV CEWS CE

CEEV CEEV CEWS CEEV CEWS

T=3

CHAMB1 CHAMB2 GMM1 GMM2

[0.020] (0.020) [0.021] (0.019) [0.026] (0.024) [0.021] (0.019)

[0.029] (0.029) [0.029] (0.029) [0.034] (0.034) [0.029] (0.029)

[0.298] (0.306) [0.296] (0.305) [0.298] (0.310) [0.297] (0.309)

[0.404] (0.393) [0.404] (0.393) [0.404] (0.406) [0.404] (0.406)

[0.029] (0.029) [0.029] (0.029) [0.034] (0.034) [0.029] (0.029)

[0.294] (0.304) [0.292] (0.304) [0.294] (0.308) [0.293] (0.308)

[0.396] (0.390) [0.396] (0.390) [0.396] (0.403) [0.396] (0.403)

[0.598] (0.601) [0.587] (0.601) [0.600] (0.813) [0.601] (0.812)

[1.465] (1.547) [1.465] (1.547) [1.465] (6.040) [1.465] (6.040)

T=6

CHAMB1 CHAMB2 GMM1 GMM2

[0.013] (0.014) [0.014] (0.014) [0.023] (0.023) [0.014] (0.014)

[0.015] (0.016) [0.015] (0.016) [0.021] (0.022) [0.015] (0.016)

[0.027] (0.027) [0.027] (0.027) [0.050] (0.045) [0.028] (0.027)

[0.091] (0.088) [0.093] (0.088) [0.123] (0.110) [0.093] (0.088)

[0.015] (0.016) [0.015] (0.016) [0.021] (0.022) [0.015] (0.016)

[0.027] (0.027) [0.028] (0.027) [0.050] (0.045) [0.028] (0.027)

[0.089] (0.088) [0.091] (0.088) [0.121] (0.110) [0.092] (0.088)

[0.049] (0.047) [0.050] (0.046) [0.076] (0.079) [0.051] (0.048)

[0.152] (0.153) [0.149] (0.153) [0.208] (0.206) [0.152] (0.158)

See Table 5.6.

from there to N = 1600. Again exceptions are found in the extreme cases of the CE+WS assumed specification for the short period samples (T = 3) for which the standard deviations are very large (and probably not well measured with 100 replications). In contrast, the improvement in efficiency with the time dimension is not a straightforward matter: it depends on the orthogonality conditions involved and on the weight matrix used.35 Accordingly, it should vary with the assumed specification, which we can see indeed. When moving from T = 3 to T = 6 the standard deviations of the estimators are roughly divided by a factor of 1.5, 2, and 10 for the NCE, CE and CE+EV specifications respectively. They are reduced by a factor which can be even much larger (from 10 to 100) when assuming the CE+WS specification.
Last, but specially compelling, are the findings on the trade off between efficiency and robustness. The differences of efficiency of the estimators across the assumed specifications (for given true specifications) are of similar orders of magnitudes as their observed differences of efficiency between the short and average period samples (for given assumed and true specifications). In the case of the longer samples (T = 6), the standard deviations are thus increased by a factor
35 It should also be remembered that inverting the covariance matrix of the orthogonality conditions (or the covariance matrix of ) implies that T cannot be too large with regard to N.

5 The Chamberlain Approach to Panel Data

155

of about 2 when going from the assumptions of NCE or CE to the weaker one of CE+EV, and increased again by a factor of about 3 when going to the even weaker assumption of CE+WS. For the shorter samples (T = 3) these efficiency factors are much larger, and they vary even more for the more extreme CE+WS assumption.36 We thus have an efficiency factor between the NCE and CE specifications of roughly 1.5, and a factor between the CE and CE+EV specifications of roughly 10 (!).37
It is worth pointing out that contrary to one's first intuition, the loss of efficiency when choosing the CE specification rather than the much stronger NCE specification becomes negligible when T = 6, and remains moderate even when T = 3. The explanation lies in the fact that the T additional orthogonality conditions (of the NCE specification compared to the CE specification) are strongly interrelated, and that in a way they cannot do "much more" than one such condition only.38 The crucial issue when relying on estimators based on the CE specification is not efficiency but the fact that the biases arising from other potential misspecifications such as errors in variables or simultaneity can be greatly aggravated. This is indeed what we can see, as already noted, when the true specification is CE+EV and not CE. On the other hand, efficiency becomes an important consideration when the econometrician suspects that the true specification is indeed CE+EV or CE+WS and wants to avoid such aggravated biases. In this situation efficiency can be an even more important problem than the present simulation results suggest, if the serial correlation of x is very high, and thus if the past x's are poor instruments for the changes in the current x (see Griliches and Mairesse, 1998 and Blundell and Bond, 1998 and 2000).

5.5.4 Standard Errors
Besides discussing the relative efficiency of the estimators, it is also instructive to ask whether they are efficient in absolute terms, that is whether they are precise enough to ensure a satisfactory approximation of the parameters of interest. In other words, are the standard deviations of the estimators sufficiently small? Since in practice the answer is mainly based on the estimated asymptotic standard errors of the parameter estimates, it is interesting to ascertain that these standard errors are indeed consistently estimated. This can be done by verifying that the means of the estimated
36 As already noted, in the EV+WS case, the short sample estimates can be extremely imprecise. For T = 3, they are in fact based on only one orthogonality condition, and follow a Cauchy distribution with infinite variance (!). 37 The observed efficiency factor between the CE+EV and CE+WS estimators varies very much depending on the true specification and the sample size N. It is equal to about: 1.5 if N = 1600 and the true specifications is NSE or CE, to about 3 if N = 1600 and the true specification is CE+EV, and it ranges from 3 to 25 depending on the true specification if N = 100 or 400. 38 This is related to the fact that most of the variance in the x's and the y is cross-sectional ("between", not "within").

156

B. Cre´pon and J. Mairesse

Table 5.9 General tests of specification based on the Chamberlain and GMM estimators under four different specification assumptions for three different (true) specifications Number of rejections out of 100 replications Simulated samples for N = 100, 400 and 1600 and T = 3 and 6

N = 100

N = 400

N = 1600

Tests

True specification

True specification

True specification

NSE CE CEEV NSE CE CEEV

T = 3 NSE CHAM 24 · 30 · 82

7

GMM 4

· 10 · 56

7

CE

CHAM 22 22 · 14

8

GMM 9

11 · 1

8

CEEV CHAM 12 12 8

9

GMM 8

74

7

CEWS CHAM ­

­­

­

GMM ­

­­

­

· 33 · 100

· 28 · 100

9

· 12

7

·9

95

73

­­

­­

T = 6 NSE CHAM 84

GMM 0

CE

CHAM 69

GMM 0

CEEV GHAM 44

GMM 2

CEWS CHAM 16

GMM 3

· 90 · 99

· 1 · 12

67 · 80

0

·2

44 58

10

14 14

20

18 · 45 · 100

3

· 19 · 100

13 12 · 64

2

2

· 33

10 9 18

4

34

7

78

3

53

· Denotes the situations in which the null hypothesis is not true.

NSE CE CEEV

11 · 91 · 100

10 · 90 · 100

89 5

· 25

6

5

· 25

1

2

9

1

2

8

­

­

­

­

­

­

6

· 96 · 100

4

· 96 · 100

10 10 · 98

8

8

· 98

10 10 5

8

8

2

8

8

8

6

7

6

standard errors of the estimators, as computed over the replications, agree well, that is within a margin of error, with the simulated standard deviations of the estimators as obtained from the same replications. The mean standard errors are given in parentheses under the standard deviations given in brackets in Tables 14.6­14.8. When comparing these numbers, we must remember that they are themselves both estimated over replications and known with a margin of error. Making simplifying assumptions based on asymptotic normality, we can say that they are estimated independently with an absolute standard error of  / 2R where  denotes their assumed common meanvalue and R = 100 is the number of replications, that is with a relative error of 1/ 2R or about 7%. We thus can accept that they do not differ significantly at the 5% significance level, if they do not differ by more than 20% (that is approximately (1.96 2) times 7%). We shall also consider for simplicity that the standard deviations of the estimators are satisfactorily small if they are less than 0.1 in absolute value, that is if they provide an approximation of about 20% for a coefficient of 0.5.
It is reassuring to see that in most cases the estimated standard errors and the simulated standard deviations of the estimators are indeed quite close, and well within the 20% margin. Again, as could be expected, the main exceptions arise in the short samples (T = 3) for the estimators assuming the CE+WS specification,

5 The Chamberlain Approach to Panel Data

157

Table 5.10 Nested tests of specification based on the Chamberlain and GMM estimators under four different specification assumptions for three different (true) specifications Number of rejections out of 100 replications Simulated samples for N = 100, 400 and 1600 and T = 3 and 6

Tests

N = 100 True specification

N = 400 True specification

N = 1600 True specification

NSE CE CEEV NSE CE CEEV NSE CE CEEV

T = 3 CEEV CHAM 12 12 8

9

CEWS GMM 8

74

7

CE/

CHAM 22 22 · 14

8

CEWS GMM 9

11 · 1

8

CE/

CHAM 16 16 · 12

4

CEEV GMM 10 10 · 9

4

NSE/ CHAM 24 · 30 · 82

7

CEWS GMM 4

· 10 · 56

7

NSE/ CHAM 18 · 32 · 84

7

CEEV GMM 9

· 10 · 59

4

NSE/ CHAM 13 · 26 · 83

5

CE

GMM 4

· 11 · 71

5

95

1

73

1

9

· 12

89

7

·9

6

5

· 12

13

4

· 11

12

· 33 · 100 11 · 28 · 100 10

· 35 · 100 13

· 31 · 100 13 · 44 · 100 7

· 40 · 100 5

2

9

2

8

5

· 25

5

· 25

11

· 35

11

· 34

· 91 · 100

· 90 · 100

· 94 · 100

· 94 · 100

· 97 · 100

· 97 · 100

T = 6 CEEV CHAM 45

CEWS GMM 3

CE/ CHAM 66

CEWS GMM 2

CE/ CHAM 61

CEEV GMM 4

NSE/ CHAM 85

CEWS GMM 2

NSE/ CHAM 80

CEEV GMM 1

NSE/ CHAM 63

CE

GMM 2

45 52

9

24

6

64 · 77

14

2

· 11

4

56 · 63

13

4

· 16

7

· 92 · 100 20

·2 ·2

5

· 89 ·100 19

· 7 · 34

5

· 84 · 99

14

· 11 · 39

6

9 15

5

67

4

13 · 65

10

4

· 43

9

21 · 80

6

7

· 61

5

· 56 · 100 8 · 24 · 100 4

· 63 · 100 2

· 39 · 100 0 · 71 · 100 1

· 53 · 100 1

· Denotes the situations in which the null hypothesis is not true.

6
4
10
9
5
5 · 96 · 96 · 98 · 98 · 100 · 100

4
4 · 98 · 98 · 100 · 100 · 100 · 100 · 100 · 100 · 100 · 100

and thus based on only one orthogonality condition. In this case the differences are large for N = 100 and remain so for N = 1600.
It is also interesting to note that the estimated standard errors of the optimal Chamberlain and GMM estimators CHAMB2 and GMM2 tend to be significantly too optimistic for the longer and smaller samples (T = 6 and N = 100) and the NSE, CE and CE+EV assumed specifications. This supports the findings by Arellano and Bond (1991), who caution against the possible underestimation of the standard errors of the optimal GMM estimator.39
39 Some further simulations seem to indicate that such finite sample under-estimation occurs when the number of orthogonality conditions is large (or not small enough) compared to the size (N) of the sample. When this happens, the GMM and Chamberlain estimators of the  parameter tend also to suffer from sizeable finite sample bias (compared to the bias of the OLS estimator). See Bound, Jaeger and Baker (1993).

158

B. Cre´pon and J. Mairesse

If we focus now on the absolute magnitude of the standard deviations of the estimators, we get a clearcut picture, highlighting again the important trade off between robustness and efficiency. The precision is acceptable for the estimators assuming the NCE and CE specifications. By contrast, for the estimators assuming the CE+WS specification, it is just acceptable only for the longer and larger samples (T = 6 and N = 1600) and if the true specification is NCE or CE (but not CE+EV). For the estimators assuming the CE+EV specification, the situation is haldway, their precision becoming acceptable for the longer samples (T = 6), with the exception of the case of the CE+EV true specification with N = 100.

5.5.5 Specification Tests
An important question that we also want to illustrate with our simulations, is the performance of the specification tests used to decide whether the specification assumed by the econometrician can be accepted or should be rejected. These are the 2 tests of overidentifying restrictions (presented in Sect. 5.3.3), which we have implemented in the two different ways corresponding to the optimal Chamberlain and GMM estimators. The Chamberlain type tests (denoted as CHAM) are based on the whole set of T 2 estimating equations, thus involving the Ns parameters of secondary interest in addition to the parameter  of primary interest, and they use the T 2 × T 2 CHAM2 weight matrix (i.e., the inverse of the heteroscedasticity consistent  covariance matrix estimate).40 The GMM type tests (denoted as GMM) are based on a reduced set of T 2 - Ns orthogonality conditions which are derived from the full set of T 2 estimating equations (or orthogonality conditions) by eliminating the Ns parameters of secondary interest, and they thus only involve here the unique parameter  of primary interest. They use the T 2 - Ns × T 2 - Ns GMM2 weight matrix (i.e., the inverse of the orthogonality conditions covariance matrix first step estimate).41
We consider both the general tests of the four assumed specifications (NCE), (CE), (CE+EV) and (CE+WS), and the corresponding difference or nested tests of one specification conditional on another (more general) one, that is (CE+EV) given (CE+WS), (CE) given (CE+EV) or (CE+WS), (NCE) given (CE) or (CE+EV) or (CE+WS).42 Tables 5.9 and 5.10 report on these general and difference tests, by
40 Ns is equal to 0, T, 2T and ((T (T + 1) /2 + (T - 1)) respectively for the NCE, CE, CE+EV and CE+WS assumed specifications. 41 While the CHAM tests correspond to Wald type tests of the restriction on , the GMM tests can be viewed as Lagrange multiplier type tests. See also following footnotes 42 and 44. 42 The 2 of the nested tests are simply computed as the differences of the 2 of the corresponding general tests. Our GMM tests are thus implemented with a different weight matrix for the null and alternative hypotheses, while the CHAM tests are computed holding the weight matrix constant for both hypotheses. It is usually considered to be better in practice (in finite samples) to hold the weight matrix constant (as estimated under the null hypothesis, or the alternative hypothesis or even a more general hypothesis). Note that the general specification tests can be simply viewed as difference tests conditional on the alternative "specification" of the unrestricted  matrix.

5 The Chamberlain Approach to Panel Data

159

giving the number of rejections obtained out of the 100 replications with a 5% significance level. They do it for our eighteen configurations of true specifications and sample sizes. The number of rejections is an estimate of the true size of the test (for a 5% nominal size) when the tested specification is true (i.e., when it encompasses the underlying true specification or is identical to it), and an estimate of the power of the test when the tested specification is false. With 100 replications only, these estimates of the tail probabilities of the tests statistics distribution cannot be very accurate and they should be taken with some caution. They are nonetheless quite suggestive.43
The most striking observation is that the CHAM tests tend to reject much more frequently than the GMM tests in the small and medium size samples (N = 100 and N = 400). This could be expected, though perhaps not to such extent, since one would expect that, loosely speaking, the CHAM weight matrices be "larger" than the GMM ones (or the  covariance matrix estimated with no restrictions be "smaller" than the ones estimated with restrictions).44 For the large samples (N = 1600) the two types of tests give quite similar results.
Another way to point out the same finding is to note that the true size of the CHAM tests tends to be much higher in the smaller samples than the (5%) nominal size (i.e., they overreject when the tested specification is true), while the performance of the GMM tests is more satisfactory in this respect. However, and conversely, the power of the CHAM tests tend to be greater than that of the GMM tests in the smaller samples. If we take the example of the test of the (CE) specification in the longer period medium size sample (T = 6, N = 400), the percentages of rejection are respectively about 12% and 2% for the CHAM and GMM tests, when the (CE) specification is correct (i.e., if the underlying true specification is (NCE) or (CE)), and about 64% and 33% when it is not (i.e., if the underlying true specification is CE+EV). Note also in Table 5.10 that for this same example the power of both tests goes up to respectively 80% and 61% when the (CE+EV) is rightly assumed as the alternative hypothesis (instead of implicitly assuming the unrestricted  matrix specification).45
If we want to summarize, the practical conclusion is that in small (and not so small) samples the weighting can matter much for the specification tests, while it seems to matter only little for the estimation of the parameters of interest. This could also be expected, since (loosely speaking again) what matters for the tests is the absolute magnitude of the weights, and for the estimation their relative magnitude. Of course more important than the weighting, and the choice between imple-

43 We have checked, however, in a number of cases, that the percentage of rejections do in fact change very little when performing 1000 replications instead of 100. 44 It is known that when testing linear restrictions in the linear regression model, the 2 statistics are larger for the Wald tests than for the Lagrange multiplier tests, and hence that the former reject more frequently the null hypothesis than the latter for a given significance level. See Berndt and Savin (1977) and Breusch (1979). 45 The corresponding percentages of rejection when we perform 1000 replications instead of 100 are: 16 and 4 (instead of 12 and 2), 57 and 30 (instead of 64 and 33), and 71 and 57 (instead of 80 and 61).

160

B. Cre´pon and J. Mairesse

menting the Chamberlain method, or GMM in this respect, remains the validity and relevance of the instruments, that is their exogeneity and their correlation with the regressors.

5.6 Appendix A: An Extended View of the Chamberlain Method
In this Appendix we show how the generalization of the Chamberlain method presented in 5.2.5 applies to simultaneous equations models, vector autoregressions (VAR) and endogenous attrition models.

5.6.1 Simultaneous Equations Models
Simultaneous equations models on panel data fit straightforwardly within the general Chamberlain framework. Using the same notations as before, but considering that we now have J dependent variables y with yi = yi(11), . . . , yi(T1), yi(12) . . . , y(iTJ) , these models can be writen as:
A11( )yi + A12( )xi = vi.
If we assume that no restrictions are made on the covariance matrices E(xixi) and E(vivi) of the explanatory variables x and of the disturbances v, and applying the result on the elimination of parameters and equations of Sect. 5.2.5, we need only to focus on the moment conditions relative to the joint covariance matrix E(yixi) of the y's and x 's. Assuming also for simplicity that the x's are strictly exogenous, (i.e., E(vixi) = 0), and that the model is not underidentified (the matrices A11 and A12 having appropriate structures), the relevant estimating equations for an efficient estimation of the parameters of interest  are the following:
A11( )E(yixi) + A12( )E(xixi) = 0 or A11( ) + A12( ) = 0.

5.6.2 VAR Models
Panel data Vector Autoregressive (VAR) models can also be considered in the Chamberlain framework. The distinction between dependent and explanatory variables is no longer relevant, and we have a set of endogenous variables explained by their past values. To keep notations simple, let us take the case of a stationary VAR

5 The Chamberlain Approach to Panel Data

161

model of order one for two variables xi and yi. We can write this model as:

yit = yyyit-1 + yxxit-1 + dty + iy + uiyt xit = xyyit-1 + xxxit-1 + dtx + ix + uixt

t = 1, . . . , T

where dty and dtx are year constants; iy and ix are individual effects; uyit and uxit are year varying disturbances.46 As in the case of the lagged dependent variable model of Sect. 5.2.3.4, we can also rewrite it in matrix form as

(I -   L)

yi xi

=

dy dx

+

viy vxi

with

vyi vix

=  l1 +  l +

uiy uix

where L is the lag matrix,  =

yy yx xy xx

, =

iy ix

=

yi0 xi0

,=

iy ix

,

and l1 = (1, 0, . . . , 0). We are thus led to the same general formulations as (5.21)

or (5.22), by setting the A( ) matrix equal to (I -   L) , with variables centered

at their year means. With both y and x being endogenous, the model is not identi-

fied and restrictions have to be imposed for identification on the covariance matrix

 ( ) = E (vivi) of the disturbances. The usual identification assumption in VAR models is that the time varying errors
uxit and uyit are not serially correlated (but may be contemporaneously correlated). The covariance matrix of the disturbances can then be written as:

 ( ) =

VEu(yiuixuyi

)

E(uyi uxi Vuix

)

+V  l1l1 +V  ll + E  

l1l +E 

ll1

where Vuiy , Vuix and E(uixuiy ) are the (T × T ) diagonal matrices of the time varying variances of the disturbances uix, and uiy and of their covariances; V and V are the two (2 × 2) symmetric covariance matrices of ix and iy and ix and iy, and E (  ) is the (2 × 2) matrix of their covariances.
For a total of 2T (2T + 1) /2 = T (2T + 1) estimating equations derived from
(5.22), we have thus (3T + 14) parameters, that is four parameters of primary interest in the  of lagged coefficients (yy yx xy and yy), and 3T + 10 parameters of secondary interest in the covariance matrix  of the disturbances. These estimating equations are nonlinear (in the  parameters), and the direct implementation of ALS could be quite complicated.

46 An interesting non-stationary VAR model for panel data is proposed in Chamberlain (1982)
and in Holtz-Eakin, Newey and Rosen (1988), in which the error terms are of the form t i + uit (i.e., with interactions between the individual effects i and time effects t ), and in which the coefficients yy, yx, xy and xx matrix may also be varying over time.

162

B. Cre´pon and J. Mairesse

There is actually a much better way to solve the problem. It is easy to see that

for each time period s, both yis and xis can be expressed as functions of the cur-

rthenetiannddivpidausat lvaelfufeecstsoftiyheaniddiosiyx,ncarsatwiceldlisatsurtbhaenucnesknuoiysw, .n.

. uyi1 and uixs, . . . , uix1 and initial observations yi0

and xi0. Therefore, under the maintained assumption of no serial correlation in the

time varying disturbances, their correlation with the future values of the residuals

is only a simple function of the 10 parameters of V , V and E ( ). And thus the residuals (vyit - viyt-1) and (vxit - vxit-1) of the model in first differences are not correlated with the past values of the y and x variables lagged by two periods and more

(i.e., yit-2, . . . , yi1 and xit-2, . . . , xi1), and a subset of the estimating equations is more

simply:

E

viyt vixt

(yis, xis)

=0

s < t - 1.

This second set of estimating equations, derived in Holtz-Eakin, Newey and Rosen (1988), has the advantage of eliminating all the parameters of secondary interest and that of being linear in the parameters of primary interest. However, it provides only 2 (T - 1) (T - 2) estimating equations, and there are 2T (T + 1) - 2 (T - 1) (T - 2) = 8T - 4 equations lost for only 3T + 10 parameters eliminated. It follows that the corresponding estimator is not the most efficient. The efficient set of estimating equations is derived in Ahn and Schmidt (1995).
Arellano and Bover (1995) and Blundell and Bond (1998) show that assuming stationarity adds additional estimating equations that can substantially improve efficiency. Indeed, stationarity implies that the initial values yi0 and xi0 are themselves functions of the parameters of primary interest, the individual effects and the infinite number of idiosyncratic disturbances prior to the beginning of the sample, so their covariances with the idiosyncratic disturbances for the period of observation can be expressed as functions of the parameters of primary interest only.

5.6.3 Endogenous Attrition
The general Chamberlain framework can also be helpful in dealing with endogenous attrition. For example, Abowd et al. (1995) consider the case of the simple first order autoregressive model yit = yit-1 + i + uit , which can also be written in vector form as:
[I - L] yi = il + yi0l1 + ui = vi.
The individuals i are firms, appearing and disappearing at individual dates bi and di. The authors consider the general attrition processes compatible with what Rubin (1976) calls data missing at random, where the probability of dying at date d given the firm history yi(d) = yibi , . . . , yidi is equal to the probability given the latent variable yi = yibi , . . . , yiT , that is:

5 The Chamberlain Approach to Panel Data
P di = d|y(id) = P (di = d|yi) . In this case, for firms appearing at a same date b, we can write

163
(5.46)

 ( ) = E vivi = [I - L] E yiyi [I - L]

(5.47)

T
= [I - L]  P (di = d) E yi yi |di = d [I - L] d=1 T
 = [I - L] P (di = d) E E yi yi |yi(d) |di = d [I - L] d=1 T
= [I - L]  P (di = d) E fd yi(d),  |di = d [I - L] d=1

where the probability density of dying at time d is a function fd yi(d),  of the firm
history y(id) and the unknown parameters  . Assuming normality, the conditional expectation E[ fd(yi(d),  )|di = d] can be
written as gd[E(y(id)|di = d), E(yi(d)yi(d) |di = d),  ] showing that (5.47) can be taken as estimating equations for the parameters  and the set of additional auxiliary parameters {E(y(id)|di = d) and E(y(id)y(id) |di = d), for d = 1, . . . , T }.

5.7 Appendix B: Vector Representation of the Chamberlain Estimating Equations
We show here how to write in a vector format the Chamberlain estimating equations written in matrix form. After recalling the main properties of the Vec operator, we use it to obtain the vector representation of the Chamberlain estimating equations in the cases of the CE, EV and WS specifications and their combinations, as well as in the cases of a lagged dependent variable specification and of the existence of restrictions on the covariance matrix of the disturbances.

5.7.1 The Vec Operator
The Vec. operator transform a matrix into a vector by stacking one above the other the different columns of the matrix. This operator has many properties, and we only give here a few important ones (see Magnus and Neudecker, 1988, for a general presentation).

164

B. Cre´pon and J. Mairesse

If A, B and C are three matrices of conforming size, we can write:

Vec(ABC) = C  A Vec(B)

which also implies that

Vec(AB) = Incol(B)  A Vec(B) and Vec(BC) = C  Inrow(B) Vec(B). If V1and V2 are two vectors of any size, we have also
Vec(V1V2) = Vec(V2 V1) . and for two matrices A and B with the same number of columns, we have:

Vec(A) Vec(B) = Tr(A B) .

5.7.2 Correlated Effects

The estimating equations (see (5.12)) are in matrix form the following:

 = b  IT + lc .

Taking the Vec operator after transposing the matrices leads to

 = Vec( ) = Vec(b  IT ) + (l  IKT ) c = Vec(b  IT ) + Gcec .
Writing b = Kk=1 bklk, where (l1, . . . , lK) is the canonical base of RK, we obtain:
Vec(b  IT ) = Vec Kk=1 bklk  IT = Kk=1 bkVec(lk  IT ) = [Vec(l1  IT )| · · · |Vec(lK  IT )] b
The estimating equations can thus be written in vector form as:



=

[G0|Gce]

b c

=

Hce

.

which is a most simple case since the matrix H is constant with only coefficients equal to zero or equal to 1.

5.7.3 Errors in Variables
The estimating equations (see (5.14)) are in this case:  = b  IT I -VeE(xixi)-1 .

5 The Chamberlain Approach to Panel Data

165

They depend on the specific parameterization of the covariance matrix Ve of the measurement errors on the variables x.
Let us consider here the usual case where these errors are assumed to be serially
uncorrelated, but where they can be correlated with each other. Ve is thus the following KT × KT matrix Ve = k,l k,l=1,...,K where k,l is the covariance matrix of the two T × 1 vectors of the measurement errors for the variables k and l. These errors being non serially correlated, the matrices k,l are diagonal and can be written as: k,l = Diag (k,l,1, . . . , k,l,T ), leading to:

 = Vec( ) = Vec(b  IT ) - Vec E(xixi)-1Ve [b  IT ] = G0b - IT  E(xixi)-1 Vec (Ve [b  IT ])

In this expression, the product Ve [b  IT ] has the form:

Ve

[b 

IT ]

=

 

lK=1 bl 1,l ...

 

=

 

Diagt=1,...,T

lK=1 bl 1,l,t ...

 

 Kl=1 bl K,l

Diagt=1,...,T Kl=1 bl 1,l,t

Diagt=1,...,T (1,t )

= 

...

 = Kk=1 tT=1lk  Dt k,t

Diagt=1,...,T (K,t ) = Fev

where  = (11, . . . , 1T , . . . , K1, . . . , KT ) with k,t = lK=1 blk,l,t , and where lk is the k th element of the canonical base of RK, and Dt is the matrix with all zero coefficients except the tth one of the diagonal which is equal to 1. Note that we cannot
identify all parameters, since the coefficients k,l,t are only entering Ve [b  IT ] via k,t = lK=k blk,l,t , and only these functions of the k,l,t are identifiable.
We can thus write in vector form:

 Vec(Ve [b  IT ]) = Vec(lk  Dt )k,t = Mev k = 1, . . . , K t = 1, . . . , T

and therefore also the estimating equations as:

 = G0b - IT  E(xixi)-1 Mev

[G0|Gev (E(xixi))]

b 

= Hev(E(xixi)).

The expression of the covariance matrix, as given in (5.36) involves the gradient  H / m , with m being here= VecE(xixi). Using the relation dVec(A-1) = -A -1  A-1dVec(A) (see Magnus and Neudecker, 1988), we can compute this gra-
dient as:

166

B. Cre´pon and J. Mairesse

 H 0/ m = - ( ( )  IKT ) (E(xixi)-1  E(xixi)-1) = -( ( ) E(xixi)-1)  E(xixi)-1 = -(0 - b0  IT )  E(xixi)-1 .

5.7.4 Weak Simultaneity

The case of weak simultaneity is quite similar to that of errors in variables. The relation between the parameters of interest and the auxiliary parameters includes the second order moment matrix of the explanatory variables. Here the T × KT matrix ( ) is formed of blocks of T × T upper triangular matrices, and thus can be written as:
K
( ) =   Mi j(k)i, j,k , k=1 i j
where the matrices Mi, j(k) have zero coefficients except in the k th T × T block where the (i, j) coefficients are equal to one. We thus have:

Vec  

K
=   Vec Mi j(k)  i, j,k = Mws . k=1 i j

and the estimating equations can be rewritten as:

 = G0b - IT  E(xixi)-1 Mws

= [G0|Gws (E(xixi))]

b 

= Hws(E(xixi)).

5.7.5 Combination of the Different Cases
The estimating equations in vector form, when combining the previous specification errors, are easily obtained by juxtaposition of the matrices G0, Gce, Gev(E(xixi)) and Gws(E(xixi)) in the preceding formulas. For example, in the case of both correlated effects and errors in variables (EC+EV), we can write:
 b
 = G0|Gev E(xixi) |Gce    = Hce,ev(E(xixi)) . c

5 The Chamberlain Approach to Panel Data

167

5.7.6 Lagged Dependent Variable

In the dynamic case, the estimating equations take the form:

[I - L]  = b  IT + l1 + l . Applying the Vec operator after transposition of the matrices, we have:

 = [(L  IKT ) ]  + G0b + l1  IKT + l  IKT 



=

[(L



IKT

)



|G0|l1



IKT

|l



IKT

]



b 



=

HLDV(

)

.



5.7.7 Restrictions on the Covariance Matrix of the Disturbances

Let us give a last example in the cases of restrictions on the covariance matrix of the disturbances (see Sect. 5.2.4). In such cases, when using the Vec. operator to transform the estimating equations, one has to be careful to avoid the duplications due to the symmetry of variance matrices. The vector transformation of a symmetric matrix which picks up only the different elements of a matrix is referred as the Vech operator. One can switch from one operator to the other by premultiplication of a given matrix. Considering for example a symmetric matrix M of size L, one can define two matrices DL and DL+ of size (L (L + 1) /2) × L2 and L2 × (L (L + 1) /2) respectively, such that Vech(M) = DL Vec(M) and Vec(M) = D+L Vech(M).
In the frequent cases where the covariance matrix of the disturbances  is linear in a parameter vector  , it is possible using the Vech operator to write simply:

Vech ( ( )) = K .

Taking the most standard case of the error components model, we have  ( ) = B2J + W2 I, where B2 and W2 are the respective variances of the individual effects i and the idiosyncratic disturbances uit , I is the unity matrix and J the matrix all
the coefficients of which are equal to 1. We can write:

Vech  B2, W2

= [Vech (J) |Vech (I)]

B2 W2

,

or without assuming the constancy over time of the variances of the idiosyncratic disturbances:

168
Vech  B2, W21 , . . . , W2T

B. Cre´pon and J. Mairesse

 B2 

=

[Vech

(J

)

|Vech

(M1

)

|

.

.

.

|Vech

(MT

)]



W2 1 ...



W2 T

where the Mt matrices have all their coefficients equal to zero except the t th diagonal coefficient equal to 1.

5.8 Appendix C: Manipulation of Equations and Parameters in the ALS Framework
ALS is a flexible method allowing the manipulation of equations and parameters without loosing asymptotic efficiency. Provided some simple rules are observed, one can change the way in which the estimating equations are written in order to eliminate auxiliary parameters as well as some parameters of secondary interest, while preserving the asymptotic properties of the estimators.

5.8.1 Transformation of the Estimating Equations

The estimating equations can be transformed without efficiency loss in the estima-

tion of the parameter of interest for a wide range of transformations. This is sum-

marized in the following result.

Let f be a function of the three arguments  , , and g where g  Rng with values

in

Rng ,

such

that:

f ( , , g) =

0

g

=

0

and

(

 

f g

(

0,



0

,

0)

is

invertible.

Then

the

optimal ALS estimator based on h( 0, 0) = f ( 0, 0, g( 0, 0)) = 0 has the same

asymptotic properties as the optimal ALS estimator based on g( 0, 0) = 0.

This result has several implications. When the number of estimating equations

is equal to the dimension of the auxiliary parameter, these equations can be transformed so that they take the form of a direct restriction 0 - h  0 = 0. As already

explained, this has several advantages in the implementation of the optimal ALS

estimator (essentially that it does not require a two steps procedure).

Also, important computational simplifications arise when the estimating equa-

tions can be rewritten as a linear function of the parameter of interest such as 0 - H m0  0 = 0. Let us consider again the case of the autoregressive model. We

have seen in Sect. 5.2.3 that the estimating equations can be written linearly in terms

of the parameters of primary interest  and b, and secondary interest  and  , as

[I - L] = M(b)IT + l + l1 .

5 The Chamberlain Approach to Panel Data

169

If we premultiply them by [I - L]-1, we transform them in the form of a direct restriction on , but which is highly nonlinear in terms of the parameters of interest. The two sets of equations are equivalent, but the first is more convenient to deal with and provides an explicit expression for the estimator of the parameter of interest.

5.8.2 Eliminating Parameters of Secondary Interest

The parameters of interest  are often divided into a subset of parameters of primary interest and a subparameter set of secondary interest.47 It may be convenient to only

estimate the first set, specially when they have a small dimension and the parameters

of secondary interest a high one. As already mentioned, this is possible by simply

eliminating the parameter of secondary interest in the estimating equations. Cre´pon,

Kramarz and Trognon (1998) show that the potential asymptotic efficiency of the

ALS estimator for the parameters of primary interest is unaffected, as long as the

number of estimating equations discarded in the elimination process is equal to the

number of the parameters of secondary interest.

More precisely, let  p and  s be the parameters of primary and secondary interest

of dimension n p and ns and let gp and gs be a partition of the estimating equations

of dimension ngp and ngs . Assume that ngs = ns , and that the ngs × ns square

matrix

 

gs s

(

0,



0

)

is

invertible,

then

there

exists

a

neigborhood

N

of

( 0, 0)

and

a function  of  p and  such that for ( , ) in N, gp  p,  s,  = 0 is equivalent

to  s =   p,  . If  =  p,  s is the optimal ALS estimator based on the full set of estimating equations

g 0,0 = 0

(5.48)

and if  p is the optimal ALS estimator based on the restricted set of estimating equations

h( 0p, 0) = gp(( 0p, 0),  0p, 0) = 0,

(5.49)

then  p is asymptotically equivalent to  p. Taking again the example of the autoregressive model, the 2KT parameters 
and  can be simply eliminated from the estimating equations by first differencing
them and then eliminating the first year estimating equation, i.e., by premultiplying

47 We prefer to speak of parameters of secondary interest rather than calling them nuisance parameters, since these parameters may have an interpretation.

170

B. Cre´pon and J. Mairesse

them successively by an appropriate (T - 1) × T matrix  and an appropriate (T - 2) × (T - 1) matrix E1.48
Instead of using the KT 2 estimating equations to estimate the 2KT + K + 1 parameters, we use only KT 2 - 2KT transformed equations to estimate the K + 1 parameters of primary interest without any loss of asymptotic efficiency.
The specification tests of Sect. 5.3.3 can also be implemented either on the re-
duced set of estimating (5.49) or on the whole set (5.48) including all the parameters.
Under the null hypothesis of correct specification, the corresponding statistics are
asymptotically equivalent. To see this consider the test statistics of (5.36), based
on the implementation of the optimal estimator. They are equivalent to the statis-
tics in (5.39) testing that the residual function of the auxiliary parameters is null
once all the parameters of interest have been eliminated (see end of Sect. 5.3.3).
But in both cases, the elimination can be chosen to end up with the same function of the auxiliary parameters, so the test statitics are equivalent. They follow a 2 distribution with degrees of freedom equal to the difference between the number of equations used in the estimation and the number of estimated parameters.49

5.8.3 Recovering Parameters of Secondary Interest Once Eliminated

Once the parameters of primary interest estimated, it is possible to obtain an estimation of the parameters of secondary interest, potentially, as efficient as if this parameter had been estimated directly. This is obtained by the solution of the following minimisation problem:

 s = Arg min{g ( s,  p, )Sg( s,  p, )},
s

(5.50)

-1

with S = W -1 =

 g0 

V

 g0 

. Note that a simple replacement of  p by  p in

the equations  s = ( p, ) used to eliminate the parameter of primary interest

provides

a

consistent,

but

not

efficient

estimator



# s

=



(

p,



)

of



s.

Let us consider the intermediate situation, in which the estimating equations take

the form

 -1 1
48 The (T - 1) × T matrix  is defined as:  = 

 0
. It transformes any T × K matrix

0 -1 1

m with m = (l1, . . . , lT ) into the (T - 1) × K matrix m = m with m = (l2 - l1, . . . , lT - lT-1).

01

The (T - 2) × (T - 1) E1 matrix is simply defined as E1 = 

.

01

49 This is another way to understand that this quantity has to be constant along the different efficient estimation procedures that can be implemented: to keep efficiency the implicit restriction imposed on the auxiliary parameters must remain the same.

5 The Chamberlain Approach to Panel Data

171

gs( , ) gp( , )

=

( p, ) -  s h( p, )

The efficient estimator  s of the parameters of secondary interest solving (5.50) is

given by50

 s = ( p, ) +W12W2-21h( p, ).

(5.51)

This last equation clearly shows that the optimal estimator of  s is generally different from the estimator obtained by simple replacement  #s .51
The covariance matrix of the estimators is given by the usual formula of (5.36).

It leads in this specific case to the expression

V ( p) =

h p

W2-21

h p

-1

Cov ( p,  s) = V ( p)

 2

+

h 2

W2-21W21

(5.52)

V ( s) = W11 -W12W2-21W21 + Cov ( p,  s) V ( p)-1 Cov ( p,  s)

Let us give two examples in which recovering parameters of secondary interest can be interesting.

1. Specification testing

Consider the case in which the set of estimating equations can be divided into two

subsets

g = (g1, g2)  F(Rn  Rng1 × Rng2 ).

Assume we want to test for the compatibity of the first subset of estimating equations
g1with the second one. This can be done simply in the following way: introduce an extra parameter  1 of dimension ng1 , and consider the new ALS problem given by the estimating equations

g1(

0,



0)

-



0 1

=

0

g2( 0, 0) = 0

The compatibility of g1 with g2 can be reformulated as the assumption  1 = 0. So a test statistic can be computed as

-1

12 = N 1

V


1

 1.

50 The solution of the previous problem is given by:  s = ( p, ) - (S11)-1S12h( p, ), and
we have the relation S1-11S12 = -W12W2-21 from the formula of block inversion (see Magnus and Neudecker 1988)

51 Note, however, that when the residuals h( 2, ) are zero, i.e., when the parameters of primary

interest

are

just

identified,

we

have

1

=



# 1

.

Another

case

when

these

estimators

are

the

same

is

when the matrix W12 = 0.

172

B. Cre´pon and J. Mairesse

It is possible to use different procedures to estimate the parameters of the model.

First eliminate  1 from the set of estimating equations and so estimate  using g2( , ) = 0. This provides a "robust" estimator  2 of  2. It is then straightforward

to implement the test. This requires only to recover an estimator  1 of  1 using (5.51), and the asymptotic covariance matrix using (5.52). Note that it is possible to

test individually the compatibility with the initial set of estimating equations g2 of each equation in g1. This means simply to test that a specific component,  1 = 0. Note also, that once the appropriate set of estimating equations has been selected, it

is straightforward to recover an estimate of  2 as efficient as if it had been directly estimated on the whole set of selected estimating equations. This is simply done

through another ALS step in which  2 and  1 are estimators of auxiliary parameters

0

=

(

0,
1



0)
2

=

(

20,



0 1

)

to

be

constrained

by



0 1

H0

10



0 2

0I



0 2

where H selects in g1 the estimating equations to be removed for the estimation of  2, and 1 the subset of parameters in  1 not constrained to zero.

2. The autoregressive model

The previous device to eliminate and recover some parameters of secondary inter-

est can help to avoid nonlinear estimation. Consider the case of the autoregressive

model and the case where the correlation between disturbances is restricted. As

described in Sect. 5.2.4 this introduces a subset of nonlinear estimating equations.

Depending on the assumptions about the covariance between the explanatory vari-

ables and the disturbances, the usual set of estimating equations, restricting the 

matrix is of the form

[I - L]  = M(b)IT + ( )

(5.53)

or of the form

[I - L]  = M(b)IT + ( )E(xixi)-1.

Let us assume it is of the form of (5.53). When the correlation between the distur-
bances is restricted, i.e., the covariance matrix of vi can be written as a function of the parameters  of dimension less than T (T + 1)/2: E(vivi) = ( ), some additional, but nonlinear, estimating equations are available.

[I - L]Vw[I - L] + ( )E(xixi)( ) = ( ).

(5.54)

The direct introduction of these equations has several disadvantages discussed in Sect. 5.2.4. A way to avoid this is to proceed in several steps. The covariance matrix is first let totally free, hence, it is parameterized by T (T + 1)/2, parameters say 

 E(vivi) = ( ) = klMkl, kl

5 The Chamberlain Approach to Panel Data

173

where Mkl is the matrix which is zero except the elements (k, l) and (l, k) which are one.
Thus we consider the set of estimating equations formed by (5.53) and

[I - L]Vw[I - L] + ( )E(xixi)( ) = ( )

(5.55)

instead of (5.53) and (5.54), the only difference being that now E(vivi) = ( ), instead of ( ).
The parameter  can be eliminated from the set of estimating equations by simply excluding the second order equations (5.55). A "robust to serial correla-
tion" and efficient estimator  = (,  ,  ) of the parameter of primary interest  0 = (0,  0,  0) can be obtained on the basis of the usual (5.53) restricting the  matrix. Following the previous methods, an efficient estimator  of the parameter of secondary interest  can be obtained using (5.54). Now, as in the previous example, we can use the total set of parameters ( ,  ) as an estimator of an auxiliary parameter 0 = ( 0,  0) that can be further constrained to incorporate restrictions on the covariance matrix.
The main advantage of this procedure is that, for a given set of estimating (5.53) restricting the  matrix, the estimation of the covariance matrix of the disturbances is unrestricted. Thus, if the true covariance matrix has a distinctive pattern as that
coming from the error components model, we could in principle recognize it on its estimate  . Notice that the specification test corresponding to the last ALS step is a test of the restriction implied on the covariance matrix of the disturbances. Finally, notice that the parameters of primary interest  can be efficiently eliminated for this last step, all the attention being devoted to the restrictions on  . Once these restrictions are imposed, (5.53) gives a simple way to recover an efficient ultimate estimation of  .

5.8.4 Elimination of Auxiliary Parameters

Auxiliary parameters can be eliminated when estimating equations do not restrict

them. Consider the case in which the auxiliary parameters can be divided into two

sets of subparameters: 

=

(

,
1

2)

and

assume

that

2

enters

only

a

subset

g2

of

the estimating equations g = (g1, g2) that does not restrict it, i.e., the parameters

of interest  can be divided into  =  1,  2 and estimating equations take the

form: g1 (1,  1) = 0, g2 (1, 2,  2) = 0 and  g2/  2 invertible (this implies that

dim (g2) = dim (2)). Then the subset of estimating equations g2 can be dropped for

the optimal ALS estimation of the parameters of interest  1. This has the interesting

consequence that the auxiliary parameters do not need to be estimated.

A useful application arises in the Chamberlain approach when the analysis takes

into account a subset of explanatory variables but does not make any inference

about it. Consider the case in which the set of explanatory variables x can be di-

vided into two subsets x = [x1, x2]. We can decompose the  matrix into two

174

B. Cre´pon and J. Mairesse

parts corresponding to the two subsets of coefficients of the linear projection of

the dependent variable on the whole set of explanatory variables  = [1, 2].

Consider a linear model as in Sect. 5.2.2. We can write  = M (b) + , with

by definition  = E (vi|xi1, xi2) and decompose each matrix M (b) ,  in two parts: M (b) = [M (b1) , M (b2)] and  = [1, 2] . These relations do not repre-
sent a model by themselves without a specific parameterization of the matrix  (or  = E (xixi)-1) and an explicit form for M (b) . If we do not make any inference about the variable x2, the matrix 2 is unrestricted and the related equations can

be dropped. We are then left with the following equations: 1 = M (b1) + 1, or

1 = M (b1) + 1

E (xi1xi1) - E (xi1xi2) E (xi2xi2)-1 E (xi2xi1)

-1
, which lead to a

specific model with a parametrization of either 1 (i.e., E (vi |xi1 - E (xi1|xi2) ) or 1 (i.e., E (vixi1)).

5.9 Appendix D: Equivalence Between Chamberlain's, GMM and Usual Panel Data Estimators

The Random Effects and Correlated Effects models are respectively defined on the basis of the following panel data equation

 yit = xi(tk)bk + i + uit

k = 1, . . . , K

by assuming that xi(tk) is uncorrelated with both disturbances i and uis, or by assuming only that xi(tk) is uncorrelated with the idiosyncratic disturbance uis. The most usual and well known estimators are the Generalized Least Squares (GLS) for the Random Effects model, and the Within and the First Difference estimators for the Correlated Effects model.
The GLS estimator bRE is computed using a consistent estimator  of the covariance matrix  of the overall disturbance vit = i + uis
-1
bRE = xi-1xi xi-1yi

where xi = x(i1) |· · · | xi(K) . For every sequence of matrices  converging in probability to , we can approximately write:



-1 

N bRE - b = xi-1xi

N xi-1vi + op (1) .

The within estimator bCEW is simply computed as
-1
bCEW = xiW xi xiW yi ,

5 The Chamberlain Approach to Panel Data

175

where W = I - J/T . The first difference estimator bCE is computed as

-1

-1

-1

bCE = (xi) V (vi) (xi) (xi) V (vi) yi .

for any consistent estimator V (vi) of V (vi), and we can approximately write:

 N(bCE - b) =

(xi) V (vi)-1 (xi)

-1

 N(xi)

V

(vi)-1

(vi)

+

op

(1)

Our purpose here is to show that these usual estimators are special cases of the more general Chamberlain estimators. Since we know that the Chamberlain method and GMM are equivalent when based on the same second order moment restrictions implied by the model, it is enough to to show that they are special cases of the GMM estimator.
For both the Random Effects and Correlated Effects models, there is a matrix M, such that the orthogonality conditions take the specific form

E (Xi  (Mvi)) = 0 .
where Xi = Vec (xi). In the case of Random Effects M is the (T × T ) identity matrix IT , while in the Correlated Effects case, M is the ((T - 1) × T ) difference matrix .52
The GMM estimator is the GLS estimator based on the orthogonality conditions

Xi  Myi = Xi  (Mxi)b + Xi  (Mvi) .

It is thus defined as

bGMM = .

X i  (Mxi) W -1X i  (Mxi) -1 X i  (Mxi) W -1X i  Myi

where W = E Xi  (Mvi)Xi  (Mvi) , and it is such that:

 N

bGMM - b

=

X X

i i

 

(Mxi (Mxi

) )

W W

-1X i  -1N

-1
(Mxi) Xi  (Mvi)

.

Assuming that the disturbances are homoscedastic with respect to the explanatory variables, that is:
E vivi|X i = E vivi =  ,

we can write more simply:

52 See the definition of the difference matrix  in the footnote 48 in Sect. 5.8.2.

176

B. Cre´pon and J. Mairesse

W = E Xi  (Mvi)Xi  (Mvi) = E XiXi   ,
with  = ME (vivi) M = MM . Denoting by e1 and e2 two random vectors, we can also write:

X i  ei1W -1X i  ei2 = Vec ei1X i E (X iX i)-1  -1Vec ei2X i = Vec E ei1X i + op (1) Vec -1ei2X iE (X iX i)-1
= tr E ei1X i -1ei2X iE (X iX i)-1 + op (1) = tr X iE (X iX i)-1 E ei1X i -1ei2 + op (1)
where E (XiXi)-1 E ei1Xi is the vector of the coefficients of the linear projection of ei1 on Xi.If ei1 is in the subspace generated by the x's, we thus have X iE (X iX i)-1 E ei1X i = ei1, which leads to:
X i  ei1W -1X i  ei2 = ei1-1ei2 + op (1) . It follows that the element (l, k) of the matrix Xi  (Mxi) W -1Xi  (Mxi)

X i  Mxi(l) W -1X i  Mxi(k) ,

is equals to:

Mx(il) -1 Mxi(k) + op (1) .

Similarly, we have: 53

X i  (Mxi) W -1N

X i  (Mvi)

 =N

xiM -1Mvi

+ op (1) .

And we can thus write that:

 N

bGMM - b

= Xi  (Mxi) W -1Xi  (Mxi) -1

Xi



(Mxi)

W

-1

 N

Xi  (Mvi)

-1 

= xiM -1 (Mxi) + op (1)

N xiM -1Mvi + op (1)

-1 

= xiM -1Mxi

N xiM -1Mvi + op (1)

53

Using

thefact

that

E (M vi )

=

0

and

V (M|vi)

exists,

and

hence

that

 NMvi

is

bounded

in

pro-

bability and NMvi = op(1).

5 The Chamberlain Approach to Panel Data

177

Assuming now that M is the identity matrix and  =  like in the Random Effects model case, we have:



-1 

N bGMM - b = xi-1xi

Nxi-1vi + op (1)

= N bRE - b + op (1) ,

showing that indeed in the case of homoscedasticity, the GLS estimator, the GMM and the Chamberlain estimators are asymptotically equivalent.
In the case of the Correlated Effects model, M is the difference matrix, and we have:

 N

bGMM - b

=

(xi) V (vi)-1 (xi)

-1

 N(xi)

V

(vi)-1

(vi)

+

op

(1)

showing that the GMM and the Chamberlain estimators are asymptotically equiva-

lent to the First Difference estimator.

If, moreover, the disturbances uit are not serially correlated, that is if the covariance matrix  = B2J + W2 I, we have  = MM = W2  , and M -1M = W-2 ( )-1 . Noting that we have also  ( )-1  = W , it follows that:54



-1 



N bGMM - b = xiWT xi

NxiWT vi + op (1) = N bW - b + op (1) ,

showing that in the case of Correlated Effects model, the GMM and Chamberlain
estimators are also asymptotically equivalent to the Within estimator when ui is homoscedastic with respect to x and not serially correlated.

5.10 Appendix E: Design of Simulation Experiments
5.10.1 Generating Process of the Variable x
We generate the explanatory variable x as the sum of a between (or cross­sectional or permanent) component i and a within (or time series or time varying) component it. We suppose that the between component i is itself the sum of an exogenous subcomponent ie and a correlated one ic (i.e., correlated with the individual effect i in the regression model). We also assume that the within component it is the sum of a true part it (serially correlated) and an error of measurement itf serially uncorrelated. These different components are generated by four normal and independent random variables i, i, it and it with zero means and standard deviations i , i , it , it . We can thus write:
54  ( )-1  is the projector on the subspace generated by the columns of  . Given that  = IT =  (BT +WT ) = WT , this subspace is included in Im (Wt ) . Because both subspaces are of dimension (T - 1) , they are identical and thus  ( )-1  = W.

178

B. Cre´pon and J. Mairesse

xit = i + it = (ie + ic) + it + itf

ie = i,

ic =  i

it = l=0  it- ,

itf = it .

where the weights  are such that it follows an autoregressive process of parameter  truncated after l years, and such that l=0 2 = 1, implying that the variance of it and it are equal, i.e., V(it ) =V(it ). These weights are thus equal to:

 = 

1-2 1 -  2(l+1)

.

5.10.2 Regression Model
We consider the true regression model:
yit = xit + (i + uit ) ,
where xit is the true (unobserved) value of x (i.e., xit = i+ it ),  is the parameter of interest, and uit is another independent random normal variable of mean zero and standard deviation uit . The estimated regression can thus be written as
yit = xit + (i + (uit - it )).
The dependent yit can also be defined as the sum of a between component i and a within component it :
yit = i + it ,
with
i = i + i = (i +  i) + i
l
 it = it + uit =   it- + uit .  =0
The variances and covariances of the between and within components of the x and y variables have the following form:
V(i) = 2i +  22i Cov (i, i) = V(i) +  2i
V (i) = 22i + (1 +  )2 2i = 2V (i) + (1 + 2 ) 2i V(it ) = t22i + v22it Cov (it , it ) = t22i = V(it ) - 22it V (it ) = 2t22it + u2it = 2V (it ) + u2it - 222it .

5 The Chamberlain Approach to Panel Data

179

The asymptotic biases BB and BW on the between and within regressions (OLS) estimates of the  parameter are thus respectively:

BB

=



2i V (i)

BW

=

-



2

2it V (it

)

.

We also introduce x-related heteroscedasticity in the regression model by assuming that the slope coefficient  is itself randomly distributed across individuals. We thus assume:
i =  + i ,
where i is a normal random variable with mean zero and standard deviation i , independent from the other random variables generating x. The disturbance in the regression thus becomes: vit = vit + ixit , and its covariance matrix conditional on x can be written as
E(vivi|xit ) = E(vivi) + 2i .E xixi .

5.10.3 Calibration of Simulations
We calibrate the simulations so that V (i) = 2i and V (it ) = u2it = 2it . This also implies the following relations between the variances of the random variables i and the specification error parameters  and :
2i = (1 -  2)2i 2it = (1 - 2)u2it = (1 - 2)2it
and the between and within biases become
BB =  BW = -2.
We also normalize the total variance of x to be equal to 1 and impose the shares of the between and within variances to be respectively SB and SW = (1 - SB) , that is:
V (xit ) = V (i) + V (it ) = 1 V (i) = SB V (it ) = SW = (1 - SB) .
The precise value that we choose for the parameters are such as to reproduce some of the basic features found in real panel data sets used to estimate firm production functions as in Mairesse (1990), Mairesse (1990), Hall and Mairesse (1995 and 1996), and Griliches and Mairesse (1998). The true mean value of the parameter of interest is taken to be  = 0.5 and its standard deviation over individuals to be i = 0.2; the shares of the between and within variances of x are respectively SB = 0.8 and SW = 0.2; the weights entering the definition of the true

180

B. Cre´pon and J. Mairesse

within component it of x are obtained with  = 0.7 and l = 5, that is w0 = 0.719, w1 = 0.503, w2 = 0.352, w3 = 0.247, w4 = 0.173 and w5 = 0.121. The correlated effect parameter  is chosen equal to 0.2 so that BB = 0.2 and the errors in variables
parameter is chosen equal to 0.2/0.5 so that BW = -0.2.

5.10.4 Three Scenarios
Basically we consider the three scenarios of Non Correlated Errors (NCE), of Correlated Effects (CE) with a value of 0.2 for the between bias BB, and of Correlated Effects and Errors in Variables (CE+EV), with a value of 0.2 for the between bias BB and of -0.2 for the within bias BW . We investigate these three scenarios for six combinations of cross­sectional and time series sample sizes: N = 100, 400, and 1600, and T = 3 and 6. For each of the eighteen (3 × 6) configurations of scenarios and sample sizes, we performed 100 replications. We also experimented with 1000 replications but found only small differences in the results.

5.10.5 The Chamberlain and GMM Estimators

For each of the three true specifications (NCE), (CE) and (CE+EV), we assume these three different specifications and also the Correlated Effects and Weak Simultaneity specification (CE+WS). For each of the twelve combinations of true and assumed specifications, we compute two Chamberlain estimators: CHAMB1 and CHAMB2 and two GMM estimators: GMM1 and GMM2.
The CHAMB1 and CHAMB2 estimators are based on estimating equations of the form 0 = H m0  0. They differ in their weight matrices S1 and S2 which are consistent estimates of the inverse of the variance matrices of , respectively assuming homoscedasticity and allowing for possible heteroscedasticity; i.e.,

plimS1 = Vc-1 = [E(wiwi)  E(xixi)-1]-1; plimS2 = V-1 = [I  E(xixi)-1E(wiwi  xixi)I  E(xixi)-1]-1.

The matrix S1 is non­optimal since we have introduced x related heteroscedasticity

in the model. The matrix S2 is optimal for the (NCE) and (CE) assumed specifications (in which H m0 = H). It is not "fully" optimal for the (CE+EV) and

(CE+WS) assumed specifcations, in which cases it is (in principle) necessary to

take into account the estimation of E (xixi). The fully optimal weight matrix is a

consistent estimate of the inverse of the covariance matrix of estimating equations

involving

the

gradient

D0

=

 H(m) 0 m

(m0).

The GMM1 and GMM2 estimators are based on orthogonality conditions that

take the form E(zii) = 0, where zi is the appropriate matrix of instruments and i stands for the residuals (i = vi) when assuming (NCE), and for their first
difference (i = vi) when assuming (CE), (CE+EV) and (CE+WS). The weight

5 The Chamberlain Approach to Panel Data

181

matrix used for GMM1 is W1 = zi zi-1 while the weight matrix used for GMM2 is
-1
W2 = ziiizi , where the i are the first step estimated residuals. As suggested by Arellano and Bond (1991), in the case of the (CE), (CE+EV) and (CE+WS) assumed
specifications, we could also have used for the first step weight matrix an estimate of E(ziDzi) with D =  (where  is the (T - 1)T difference matrix and D is the (T - 1)(T - 1) matrix such that its diagonal coefficients are equal to 2 and its first upper and lower diagonal coefficients are equal to -1, and all other coefficients are
zero). D is the optimal weight matrix in the cases of Correlated Effects if the time
varying disturbances uit are homoscedastic and serially uncorrelated.

5.10.6 Standard Errors and Specification Tests

The standard errors are derived from the covariance matrix of the estimated parameters which are computed using the following formulas:
CHAMB1 [H S1H]-1H S1S2-1S1H[H S1H]-1 CHAMB2 [H S2H]-1 GMM1 [G W1G]-1G W1W2-1W1[G W1G]-1 GMM2 [G W2G]-1
where G is the gradient of the orthogonality conditions with respect to the parameter of interest, that is G = E (zixi) if (NCE), and G = E (zixi) if (CE), (CE+EV) and (CE+WS).
The specification tests are implemented using the CHAMB2 and GMM2 estimators. They are based on the following 2 statistics:
CHAMB N  - H (m)  S2  - H (m) 
GMM N zii W2 zii

Acknowledgments We are grateful to Lazlo Matyas and Patrick Sevestre for their energetic editorial help and friendly collaboration in the various stages of writing this chapter. We have also benefited from comments by G. Chamberlain, Z. Griliches, T. Klette, M. Lechner, and F. Windmeijer on its first version published in the second edition of this book, and by B. Hall, P. Mohnen and W. Raymond on the present revised version for this new edition.

References
Abowd J., B. Crepon, F. Kramarz and A. Trognon (1995): A la Recherche des Moments Perdus, Covariance Models for Unbalanced Panels, NBER Technical Working Paper n180.
Andrews, D.W.K. (1985): Asymptotic Results for Generalised Wald Tests, Cowles Foundation D.P, 761R.
Ahn, S.C. and P. Schmidt (1995): Efficient Estimation of Models for Dynamic Panel Data, Journal of Econometrics, 68, 5­25.

182

B. Cre´pon and J. Mairesse

Arellano, M. and S.R. Bond (1991): Some Tests of Specification for Panel Data: Monte Carlo Evidence and an Application to Employment Equations, Review of Economic Studies, 58, 277­297.
Arellano, M. and O. Bover (1995): Another Look at the Instrumental Variable Estimation of ErrorComponents Models, Journal of Econometrics, 68, 29­52.
Berndt, E. and N. Savin (1977): Conflict among Criteria for Testing Hypotheses in the Multivariate Regression Model, Econometrica, 45, 1263­1278.
Bertscheck, I. and M. Lechner (1995): GMM Estimation of Panel Probit Models: Nonparametric Estimation of the Optimal Instruments, Humboldt Universitat zu Berlin, Discussion Paper N.25.
Blundell, R.W. and S. Bond (1998): Initial conditions and Moment Restrictions in Dynamic Panel Data Models, Journal of Econometrics, 87, 115­143.
Blundell, R.W. and S. Bond (2000): GMM Estimation with Highly Persistent Panel Data: An Application to Production Function Estimation, Econometric Reviews, 19, 321­340.
Bound, J, D.A. Jaeger, and R. Baker (1993): The Cure Can Be Worse than the Disease: A Cautionary Tale Regarding Instrumental Variables, National Bureau of Economic Research, Technical Working Paper N.137.
Breusch, T.S. (1979): Conflict Among Criteria for Testing Hypotheses: Extensions and Comments, Econometrica, 47, 203­207.
Chamberlain, G. (1982): Multivariate Regression Model for Panel Data, Journal of Econometrics, 18, 5­46.
Chamberlain, G. (1984): Panel Data in Handbook of Econometrics ed. by Z. Griliches and M.D. Intriligator, Amsterdam, North Holland Publishing Co, pp. 1247­1318.
Cre´pon, B., F. Kramarz and A. Trognon (1998): Parameters of Interest, Nuisance Parameters and Orthogonality Conditions: An Application to Autoregressive Error Component Models, Journal of Econometrics, 82, 135­156.
Gourieroux, C., A. Monfort and A. Trognon (1985): Moindres Carres Asymptotiques, Annales de l'INSEE, 58, 91­122.
Gourieroux, C. and A. Monfort (1989): A General Framework for Testing a Null Hypothesis in a Mixed Form, Econometric Theory, 5, 63­82.
Griliches, Z. and J. Hausman (1986): Errors in Variables in Panel Data, Journal of Econometrics, 31, 93­118.
Griliches, Z. and J. Mairesse (1998): Production Functions: The Search for Identification, in Econometrics and Economic Theory in the 20th Century : The Ragnar Frish Centennial Symposium ed. by S. Stro¨m, Cambridge, Cambridge University Press, pp. 169­203.
Hall B.H.H and Mairesse, J. (1995): Exploring the Relationship between R-D and Productivity in French Manufacturing Firms, Journal of Econometrics, 65, 263­293.
Hall B.H.H and Mairesse, J. (1996): Estimating the Productivity of Research and Development in French and United States Manufacturing Firms : An Exploration of Simultaneity Issues with GMM Methods, in International Productivity Differences and Their Explanations ed. by K. Wagner and Bart Van Ark, Elsevier Science, pp. 285­315.
Holtz-Eakin, D., W.K. Newey, and H. Rosen (1988): Estimating Vector Autoregressions with Panel data, Econometrica, 56, 1371­1395.
Malinvaud, E. (1970): Statistical Methods of Econometrics, North Holland Publishing Co, Amsterdam.
Mundlak, Y. (1961): Empirical Production Function Free of Management Bias, Journal of Farm Economics, 43, 45­56.
Magnus, J.R. and H. Neudecker (1988): Matrix Differential Calculus with Applications in Statistics and Econometrics, John Wiley and Sons, Hoboken.
Mairesse, J. (1990): Time series and Cross-sectional Estimates on Panel Data: Why are they Different and Why Should they Be Equal? in Panel Data and Labor Market Studies ed. by J. Hartog et al., Amsterdam, North-Holland Publishing Co, pp. 81­95.
Mairesse, J. and Z. Griliches (1990): Heterogeneity in Panel Data: Are There Stable Production Functions? in Essays in Honour of Edmond Malinvaud ed. by P. Champsaurs et al., Cambridge, MIT Press, pp. 192­231.

5 The Chamberlain Approach to Panel Data

183

Newey, W.K. (1985): Generalised Method of Moments Specification Testing, Journal of Econometrics, 29, 229­256.
Rubin, D.B. (1976): Inference and Missing Data, Biometrika, 63, 581­92. Stewart G.W. (1973): Introduction to Matrix Computations, Academic Press, Orlando. White, H. (1980): A Heteroskedasticity Consistent Covariance Matrix Estimator and a Direct Test
for Heteroskedasticity, Econometrica 48, 817­838.

Chapter 6
Random Coefficient Models
Cheng Hsiao and M. Hashem Pesaran

6.1 Introduction

Consider a linear regression model of the form

y =  x + u,

(6.1)

where y is the dependent variable and x is a K × 1 vector of explanatory variables. The variable u denotes the effects of all other variables that affect the outcome of y but are not explicitly included as independent variables. The standard assumption is that u behaves like a random variable and is uncorrelated with x. However, the emphasis of panel data is often on the individual outcomes. In explaining human behavior, the list of relevant factors may be extended ad infinitum. The effect of these factors that have not been explicitly allowed for may be individual specific and time varying. In fact, one of the crucial issues in panel data analysis is how the differences in behavior across individuals and/or through time that are not captured by x should be modeled.
The variable intercept and/or error components models attribute the heterogeneity across individuals and/or through time to the effects of omitted variables that are individual time-invariant, like sex, ability and social economic background variables that stay constant for a given individual but vary across individuals, and/or period individual-invariant, like prices, interest rates and widespread optimism or pessimism that are the same for all cross-sectional units at a given point in time but vary through time. It does not allow the interaction of the individual specific and/or

Cheng Hsiao University of Southern California and Nanyang Technological University, Singapore, e-mail: chsiao@usc.edu
M. Hashem Pesaran Cambridge University and USC, Sidgwick Avenue, Cambridge, CB3 9DD, United Kingdom, e-mail: mhp1@econ.cam.ac.uk

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

185

c Springer-Verlag Berlin Heidelberg 2008

186

C. Hsiao and M.H. Pesaran

time varying differences with the included explanatory variables, x. A more general formulation would be to let the variable y of the individual i at time t be denoted as

yit =  it xit + uit , = 1it x1it + . . . + kit xkit + uit ,

(6.2)

i = 1, . . . , N, and t = 1, . . . , T. Expression (6.2) corresponds to the most general specification of the panel linear data regression problem. It simply states that each individual has their own coefficients that are specific to each time period. However, as pointed out by Balestra (1996) this general formulation is, at most, descriptive. It lacks any explanatory power and it is useless for prediction. Furthermore, it is not estimable as the number of parameters to be estimated exceeds the number of observations. For a model to become interesting and to acquire explanatory and predictive power, it is essential that some structure is imposed on its parameters.
One way to reduce the number of parameters in (6.2) is to adopt an analysis of variance framework by letting

N

T

  kit = k + ki + kt , ki = 0, and kt = 0, k = 1, . . . , K.

i=1

t=1

(6.3)

This specification treats individual differences as fixed and is computationally simple. The drawback is that it is not parsimonious, and hence reliable estimates of ki and kt are difficult to obtain. Moreover, it is difficult to draw inference about the population if differences across individuals and/or over time are fixed and different.
An alternative to the fixed coefficient (or effects) specification of (6.3) is to let ki and kt be random variables and introduce proper stochastic specifications. This is commonly called the "random coefficients" model. The random coefficient specification reduces the number of parameters to be estimated substantially, while still allowing the coefficients to differ from unit to unit and/or from time to time.
In Sect. 6.2 we introduce various types of random coefficients models and suggest a common framework for them. In Sects. 6.3 and 6.4 we consider the fundamental issues of statistical inference of a random coefficients formulation using the sampling approach. In Sect. 6.5 we consider a Bayesian approach. Section 6.6 considers the generalization to a dynamic framework. Issues of testing for homogeneity under weak exogeneity are discussed in Sect. 6.7. Discussions on random coefficients, simultaneous equation systems and cross-sectional dependence are provided in Sects. 6.8 and 6.9. Conclusions are in Sect. 6.10.

6.2 The Models
Let there be observations for N cross-sectional units over T time periods. Suppose the variable y for the ith unit at time t is specified as a linear function of K strictly exogenous variables, xkit ,k = 1, 2, . . . , K, in the form1
1 The case where one or more of the regressors are weakly exogenous is considered in Sect. 6.6.

6 Random Coefficient Models

187

K
 yit = kit xkit + uit , k=1
=  it xit + uit , i = 1 . . . , N, t = 1, . . . , T,

(6.4)

where uit denotes the random error term, xit is a K × 1 vector of exogenous variables and  it is the K × 1 vector of coefficients. The random coefficients approach assumes that the coefficients  it are drawn from probability distributions with a fixed number of parameters that do not vary with N and/or T. Depending on the type of
assumption about the parameter variation, we can further classify the models into
one of two categories: stationary and non-stationary random-coefficients models.
The stationary random-coefficients models regard the coefficients as having constant means and variance-covariances. Namely, the K × 1 vector  it is specified as

 it =  +  it , i = 1, . . . , N, t = 1, . . . , T,

(6.5)

where  is a K ×1 vector of constants, and  it is a K ×1 vector of stationary random variables with zero means and constant variance­covariances. For instance, in the
Swamy (1970) type random coefficient models,

 it =  +  i, i = 1, . . . , N, t = 1, . . . , T,

(6.6)

and

E( i) = 0, E( ixit ) = 0,

(6.7)

E( i j) =

, if i = j, 0, if i = j.

Hsiao (1974, 1975) considers the following type of model

 it =  +  it

(6.8)

=  +  i + t , i = 1, . . . , N, t = 1, . . . , T,

and assumes

E( i) = E(t ) = 0, E  it = 0,

E  ixit = 0, E t xit = 0,

E i j =

, if i = j, 0, if i = j,

E( t  s) =

, if t = s, 0, if t = s.

(6.9)

Alternatively, a time varying parameter model may be treated as realizations of a stationary stochastic process, thus  it can be written in the form,

 it =  t = H t-1 +  t ,

(6.10)

188

C. Hsiao and M.H. Pesaran

where all eigenvalues of H lie inside the unit circle, and  t is a stationary random variable with mean  . Then the Hildreth and Houck (1968) type model is obtained
by letting H = 0 and  t be i.i.d.; for the Pagan (1980) model, H = 0 and

 t -  =  t -  = a(L)t ,

(6.11)

where  is the mean of  t and a(L) is the ratio of polynomials of orders p and q in the lag operator L(Lt = t-1) and t is independent normal. The Rosenberg (1972, 1973) return-to-normality model assumes the absolute value of the characteristic roots of H be less than 1 with  t independently normally distributed with mean  = (IK - H) .
The nonstationary random coefficients models do not regard the coefficient vec-
tor as having constant mean or variances. Changes in coefficients from one obser-
vation to the next can be the result of the realization of a nonstationary stochastic
process or can be a function of exogenous variables. When the coefficients are real-
izations of a nonstationary stochastic process, we may again use (6.10) to represent
such a process. For instance, the Cooley and Prescott (1976) model can be obtained by letting H = IK and  = 0. When the coefficients  it are functions of individual characteristics or time variables (e.g. Amemiya (1978), Boskin and Lau (1990),
Hendricks, Koenker, and Poirier (1979), Singh, Nagar, Choudhry and Raj (1976),
Swamy and Tinsley (1977) and Wachter (1976)) we can let

 it = qit +  it .

(6.12)

While the detailed formulation and estimation of the random coefficients model depends on the specific assumptions about the parameter variation, many types of the random coefficients models can be conveniently represented using a mixed fixed and random coefficients framework of the form (e.g. Hsiao (1990) and Hsiao, Appelbe and Dineen (1992))

yit = zit  + wit  it + uit, i = 1, . . . , N, t = 1, . . . , T,

(6.13)

where zit and wit are vectors of exogenous variables with dimensions and p respectively,  is an × 1 vector of constants,  it is a p × 1 vector of random variables, and
uit is the error term. For instance, the Swamy type model ((6.6) and (6.7)) can be obtained from (6.13) by letting zit = wit = xit ,  =  , and  it =  i; the Hsiao type
model (6.8) and (6.9) is obtained by letting zit = wit = xit ,  =  , and it = i +t ;
the stochastic time varying parameter model (6.10) is obtained by letting zit = xit , wit = xit [H, IK],  =  , and  it = t = [ t-1, ( t -  ) ]; and the model where  it is a function of other variables (6.12) is obtained by letting zit = xit  qit ,  = vec(), wit = xit , it =  it , etc.
For ease of illustrating the fundamental issues involved in estimating a random
coefficients model we shall make the simplifying assumption that  it =  i and  i are independently normally distributed over i with mean 0 and covariance , denoted

6 Random Coefficient Models

189

by  i  N(0, ).2 In other words, there are only individual-specific effects,  i, and these individual-specific effects stay constant over time. Under this simplified assumption, model (6.13) can be written in the stacked form

y = Z + W + u,

(6.14)

where









y1

yi1

u1

ui1

y
NT ×1

=

 ... yN

 ,

yi
T ×1

=

 ... yiT

 ,

u
NT ×1

=

 ... uN

 ,

ui
T ×1

=

 ... uiT

 ,





Z
NT×

Z1 =  ...  , Zi
T×

=



zi1 ...



,

ZN

ziT

W
NT ×N p

=



W1



0 ...

0

0 W2

··· ··· ...

0 0
WN

  ,

Wi
T×p

=





wi1 ...

wiT

  ,



1

and  = 
N p×1

...

 .

N

(6.15)

We further assume that  and u are mutually independent with

E (u) = 0, and E uu = C.

(6.16)

6.3 Sampling Approach

Let v = W + u,

then E (v) = 0 and

E vv = W(IN )W + C = .

Model (6.14) can be viewed as a linear regression model of the form

(6.17) (6.18)

y = Z + v,

(6.19)

where the composite error term, v, has a nonspherical covariance matrix. From a sampling point of view, the interest for model (6.19) will lie in (a) estimating the

2 A model allowing the coefficients to vary across individuals and over time is very difficult to estimate. So far, most random coefficients models either assume  it =  i or  it =  t . Here we shall only focus on the former. For the case of  it =  t , as in (6.10), one can employ Kalman filter type procedures to obtain MLE and carry out predictions. For details see Hsiao (2003).

190

C. Hsiao and M.H. Pesaran

mean coefficient vector , (b) estimating the covariance matrix of v, , and (c) predicting yit .
If  and C are known, the best linear unbiased estimator of  is the generalized least squares (GLS) estimator

¯ = (Z -1Z)-1(Z -1y),

(6.20)

with covariance matrix

D = Cov (¯) = (Z -1Z)-1.

(6.21)

If  and C are unknown, we can apply a two step GLS estimator. In the first step we estimate  and C. In the second step we estimate  by substituting the estimated  and C into (6.20) and treating them as if they were known. Provided  and C can be consistently estimated, the two step GLS will have the same asymptotic efficiency
as the GLS estimator.
Similarly, we can obtain the best linear unbiased predictor of yi f using the formula

yi f = zi f  + E(i f | v), = zi f  + Cov (i f , v) Var (v)-1v.

(6.22)

Because  and v are unknown, their estimated values, ¯ and v^ = y - Z¯ are substituted into (6.22) in practice.
Equations (6.20)­(6.22) provide a general principle for efficient inference of a
random coefficients model. To illustrate relations to a specific type of random coefficients model, we consider a Swamy type model (6.4), (6.6) and (6.7), assuming that the regressors zit , are strictly exogenous.3
Under the assumptions of Swamy (1970), we have

where

Z = XA, W = X,  =  ,  i =  +  i,

(6.23)

A
NT ×K

=

(IK, IK, .., IK)







X1

X = 
NT ×K

...

0

 ,

Xi
T ×K

=



xi1 ...

 .

0 XN

xiT

(6.24)

For simplicity, we also assume that uit is independently distributed across i and over

t with

E ui2t = i2.

(6.25)

3 For estimation of correlated random coeffcient model using the instrumental variables approach, see Murtazashvili and Wooldridge (2007).

6 Random Coefficient Models

191

Then  is block diagonal, with the ith diagonal block equal to

i = XiX i + i2IT .

(6.26)

Substituting (6.23)­(6.26) into (6.20), the best linear unbiased estimator of the mean coefficient vector  is

¯ GLS = A X -1XA -1 A X-1y,

N

-1 N

 =

Xi-i 1Xi

 Xii-1yi ,

i=1

i=1

N
=  Ri^i, i=1

(6.27)

where

N

-1

-1

-1

 Ri =

 + ^i

 + ^i ,

i=1

(6.28)

and

^i = (XiXi)-1Xiyi, ^i = V ^i = i2(XiXi)-1.

(6.29)

The last expression of (6.27) is obtained by repeatedly utilizing the identity relation,

(E + BFB )-1 = E-1 - E-1B(B E-1B + F-1)-1B E-1.

(6.30)

It shows that the GLS estimator is a matrix weighted average of the least squares estimator for each cross-sectional unit (6.29), with the weights inversely proportional to their covariance matrices. It also shows that the GLS estimator requires only a matrix inversion of order K, and so it is not much more complicated to compute than the sample least squares estimator.
The covariance matrix of the GLS estimator is

  Cov ¯ GLS = A X -1XA -1 =

N

-1

Xii-1Xi =

N

-1

-1

 + ^i

.

i=1

i=1

(6.31)

If both errors and  i are normally distributed, the GLS estimator of  is the maxi-

mum likelihood estimator (MLE) of  conditional on  and i2. Without knowledge of  and i2, we can estimate  ,  and i2, i = 1, . . . , N simultaneously by the maximum likelihood method. However, computationally it can be tedious. A natural

alternative is to first estimate i then substitute the estimated i into (6.27). Swamy proposes using the least squares estimator of  i,^ i = (XiXi)-1Xiyi and
residuals ui = yi - Xi^i to obtain unbiased estimators of i2, i = 1, . . . , N, and . Noting that

ui = [IT - Xi(XiXi)-1Xi]ui,

(6.32)

192

C. Hsiao and M.H. Pesaran

and ^ i =  i + (XiXi)-1Xiui,
we obtain the unbiased estimators of i2 and  as:

i2

=

uiui T -K

,

=

T

1 - K yi[IT

- Xi(XiXi)-1Xi]yi,

(6.33) (6.34)

  

=

1N N - 1 i=1

N
^i - N-1 ^ j
j=1

  N
^i - N-1 ^ j
j=1

-

1 N

N
i2(XiXi)-1.
i=1

(6.35)

Just as in the error-components model, the estimator (6.35) is not necessarily nonnegative definite. In this situation, Swamy [also see Judge, Griffiths, Hill, Lu¨tkepohl and Lee (1985)] has suggested replacing (6.35) by

  

=

N

1 -1

N i=1

N
^i - N-1 ^ j
j=1

N
 ^i - N-1 ^ j . j=1

(6.36)

This estimator, although biased, is nonnegative definite and consistent when T tends to infinity.

6.4 Mean Group Estimation

A consistent estimator of  can also be obtained under more general assumptions

concerning  i and the regressors. One such possible estimator is the Mean Group (MG) estimator proposed by Pesaran and Smith (1995) for estimation of dynamic

random coefficient models. The MG estimator is defined as the simple average of

the OLS estimators, ^ i:

N
 ¯ MG = N-1 ^i.

(6.37)

i=1

When the regressors are strictly exogenous and the errors, uit are independently distributed, an unbiased estimator of the covariance matrix of ¯ MG can be computed as

Cov ¯ MG = N-1,

where  is given by (6.36). For a proof first note that under the random coefficient model we have

6 Random Coefficient Models

193

^ i =  +  i +  i,

where

 i = (XiXi)-1Xiui,

and ¯ MG =  + ¯ + ¯ ,

where



=

1 N

iN=1  i

and



=

1 N

iN=1  i.

Therefore

 i - ¯ MG = ( i -  ) +  i -  ,

 i - ¯ MG  i - ¯ MG = ( i -  ) ( i -  ) +  i-  i- + ( i -  )  i -  +  i -  ( i -  ) ,

and
N
E
i=1
But

 i - ¯ MG

 i - ¯ MG

 = (N - 1) +

1- 1 N

N
i2

XiXi

-1 .

i=1

Cov ¯ MG = Cov ( ) + Cov  ,

 =

1 N

+

1 N2

N
i2E
i=1

XiXi -1 .

Using the above results it is now easily seen that

E Cov ¯ MG = Cov ¯ MG ,

as required. Finally, it is worth noting that the MG and the Swamy estimators are in fact
algebraically equivalent for T sufficiently large.

6.5 Bayesian Approach
One can also derive the solutions for the model (6.14) from a Bayesian point of view. The Bayes approach assumes that all quantities, including the parameters, are random variables. Therefore, as part of the model, prior probability distributions are introduced for the parameters. The prior distribution is supposed to express a state of knowledge or ignorance about the parameters before the data is obtained.

194

C. Hsiao and M.H. Pesaran

The model (6.14) with the assumption that  is fixed and  i is random, can be viewed as the state of knowledge about the parameters  and  before the data are obtained: The prior distributions of  and  are independent. There is no information on  but there is information on  i, which is normally distributed with mean 0 and covariance matrix . This prior information is combined with the model (6.14) and data, y and z, to revise the probability distribution of  and  , which is called the
posterior distribution. From this distribution inferences are made.
Formally, we assume that

A1. The prior distributions of  and  are independent, that is,

p(,  ) = p() · p( ).

(6.38)

A2. There is no information about ,

p()  constant.

(6.39)

A3. There is prior information about  ,

  N(0, IN  ).

(6.40)

Theorem 6.1. Suppose that, given  and  ,

y  N(Z + W, C).

(6.41)

Under A1­A3, (a) the marginal distribution of y given  is

y  N(Z, C + W(IN  )W ),

(6.42)

(b) the distribution of  given y is N(¯, D), where ¯ and D are given by (6.20) and (6.21), respectively.
(c) the distribution of  given y is N( , D), where

 = {W [C-1 - C-1Z(Z C-1Z)-1Z C-1]W + (IN  -1)}-1 ·{W [C-1 - C-1Z(Z C-1Z)-1Z C-1]y},

(6.43)

and D = {W [C-1 - C-1Z(Z C-1Z)-1Z C-1]W + (IN  -1)}-1.

(6.44)

See Appendix A for a proof. Recall that

 = A +  ,

(6.45)

6 Random Coefficient Models

195

and therefore the Bayes estimator of  can be obtained by substituting the Bayes estimators of  and  (6.27) and (6.43) into (6.45), namely:

  = A¯ GLS +  = X C-1X + -1 -1 X C-1y + -1A¯ GLS ,

(6.46)

where

 = IN  .

When E uiu j = i2IT if i = j, and 0 otherwise, as assumed by Swamy (1970), we

have


i =

-^i1 + -1 -1

-^i1^i + -1¯ GLS

, i = 1, 2, . . . , N

(6.47)

where

^i = (XiXi)-1Xiyi, and ^i = V (^i) = i2(XiXi)-1.

(6.48)

The Bayes estimator (6.47), is identical to the Lindley and Smith (1972) estimator for a linear hierarchical model. This is to be expected since the Swamy type assumptions and the Lindley­Smith linear hierarchical model are formally equivalent.
The above estimator can also be written as



 i

=

Hi^i

+

(IK

- Hi) ¯ GLS,

where

Hi =

-^i1 + -1 -1 -^i1 = 

 +  ^i

-1
.



weshtiimchatsohroowfs¯th. aAt lsoi ,iRs ai

weighted average defined by (6.28)

of the OLS estimator, can be written as

^i, and

the

Swamy

Ri =

N

-1

 H j Hi,

j=1

and hence

  N-1

N


i =

N

Ri^i = ¯ GLS,

i=1

i=1

namely the simple mean of the Bayes estimators (which could be viewed as the Bayes Mean Group estimator) is equal to the Swamy estimator of ¯ .

Remark 6.1. It is useful to put the random coefficients model in a Bayesian framework because many of the estimators based on the sampling approach can also be derived from the Bayes approach. For instance, as one can see from theorem 6.1(b) conditional on  and C, the Bayes estimator of  for the model (6.14) is identical to the GLS estimator of  (6.20). Furthermore, a Bayesian framework makes it clear

196

C. Hsiao and M.H. Pesaran

the role of prior knowledge or ignorance about the parameter  = (,  ) given y. The parameters  are treated as random variables and all probability statements are conditional. Ignorance about  would necessitate a specification of a diffuse prior to  , which is typically specified as

p( )  constant.

On the other hand, information about  would necessitate a specification of an informative prior. The Swamy type random coefficients formulation of  i having mean  and covariance  is equivalent to specifying an informative prior for the parameters  i.
Remark 6.2. Typically, we use the expected value of an i.i.d. random variable as a
predictor of the random variable. In panel data, we have two dimensions, a crosssectional dimension and a time series dimension. Even though  i is assumed independently distributed across i, once a particular  i is drawn, it stays constant over time for the ith cross-sectional unit. Therefore, it makes sense to predict  i, (for an example, see Hsiao, Mountain, Tsui and Luke Chan (1989)). The Bayes predictor of  i is different from the classical sampling approach predictor. For instance, for the Swamy type model the sampling approach predictor of  i =  +  i defined by (6.23) is the least squares estimator (6.48). The Bayes predictor of  i, given by (6.46) or (6.47), is a weighted average between the least squares estimator of  i and the overall mean  . In other words, the Bayes estimator of the individual coefficients  i "shrinks" the estimate of  i based on the information of the ith individual (6.48) towards the grand mean  . An intuitive reason for doing so is because if the actual differences in  i can be attributable to the work of chance mechanisms as postulated by de Finetti's (1964) exchangeability assumption, information about  i can be obtained by examining the behaviour of others in addition to those of the ith cross-sectional unit because the expected value of  i is the same as  j. When there are not many observations (i.e. T is small) with regard to the ith individual, information about  i can be expanded by considering the responses of others. When T becomes large, more information about  i becomes available and the weight gradually shifts towards the estimate based on the ith unit. As T  , the Bayes estimator approaches the least squares estimator ^ i.
Remark 6.3. The derivation of the posterior distribution and the Bayes estimators  and  of model (6.14) is based on known C and . When C and  are unknown, in principle, we can first assign a joint prior of  ,  , C and , and combine this with the likelihood function to obtain the joint posterior distribution. This distribution then has to be integrated with respect of C and . In practice, this is most complex to execute. Lindley and Smith (1972), therefore, suggest to approximate the posterior distribution of  and  conditional on the modal value of  and C. The modal estimates of  and C may be found by supposing  and  known, and then replacing  and  in the results by their modes. The sequence of iterations typically starts with assumed values of  and C to calculate the mode of  and  , say ¯(1) and  (1).

6 Random Coefficient Models

197

Treating ¯(1) and  (1) as known, we can find the mode for  and C, say (1) and C(1). The (1) and C(1) are then used to find (2) and  (2), and so on.
For the Swamy type model (6.6) and (6.7) under the assumption that -1 has a Wishart distribution with p degrees of freedom and matrix R, it is shown by Lindley and Smith (1972) that the mode estimator of  is

=

 R +

N

(

 i

-

¯

)(

 i

-

¯

)

/(N + p - K - 2).

i=1

(6.49)

6.6 Dynamic Random Coefficients Models

Because of the inertia in human behaviour or institutional or technological rigidity, often a behavioural equation is specified with lagged dependent variable(s) appearing as regressor(s). We will consider a dynamic model of the form

yit = iyi,t-1 + xit  i + uit , i = 1, 2, . . . , N; t = 1, 2, . . . , T,

(6.50)

where xit is a K × 1 vector of exogenous variables, and the error term uit is assumed to be independently, identically distributed over t with mean zero and variance i2, and is independent across i. Let  i = (i,  i) . We assume that  i is independently
distributed across i with

E ( i) =  = ,  ,

(6.51)

E ( i - )( i - ) = .

Rewrite  i =  +  i, (6.51) and (6.52) are equivalent to

E ( i) = 0, E  i j =

 i f i = j, 0 i f i = j.

(6.52) (6.53)

Although we may maintain the assumption (6.9) that E ( ixit ) = 0, we can no longer assume that E ( iyi,t-1) = 0. Through continuous substitutions, we have


 yi,t-1 = ( + i1) jxi,t- j-1( +  i2) j=o   + ( + i1) jui,t- j-1. j=o

(6.54)

It follows that E( iyi,t-1) = 0. The violation of the independence between the regressors and the individual ef-
fects  i implies that the pooled least squares regression of yit on yi,t-1, and xit will yield inconsistent estimates of  , even for T and N sufficiently large. Pesaran and

198

C. Hsiao and M.H. Pesaran

Smith (1995) have noted that as T  , the least squares regression of yit on yi,t-1 and xit yields a consistent estimator of  i, i. They suggest a mean group estimator of  by taking the average of  i across i,

 ¯ MG

=

1 N

N
i.
i=1

(6.55)

The mean group estimator is consistent when both N and T  . In finite T,  i for  i is biased to the order of 1/T . (Hurwicz (1950), Kiviet and Phillips (1993)) and the limited Monte Carlo appears to show that the mean group estimator can
be severely biased when T is very small (Hsiao, Pesaran and Tahmiscioglu 1999). However, under the assumption that yi0 are fixed and known and  i and uit are independently normally distributed, as discussed in Sect. 6.5 we can implement the Bayes estimator of  i conditional on i2 and ,

   B = N i2(WiWi)-1 +  -1 -1 N i2(WiWi)-1 +  i,

i=1

i=1

(6.56)

where here Wi = (yi,-1, Xi) with yi,-1 = (yi0, yi1, . . . , yiT -1) . This Bayes estimator
is a weighted average of the least squares estimator of individual units with the
weights being inversely proportional to individual variances. When T  , N  , and N/T 3/2  0, the Bayes estimator is asymptotically equivalent to the mean group
estimator (6.55) (Hsiao et al. 1999). In practice, the variance components, i2 and  are rarely known. The Monte
Carlo studies conducted by Hsiao et al. (1999) show that by following the approach of Lindley and Smith (1972) in assuming that the prior-distributions of i2 and  are independent and are distributed as

N
 P(-1, 12, . . . , n2) = W (-1|(rR)-1, r) i-2, i=1

(6.57)

yields a Bayes estimator almost as good as the Bayes estimator with known  and i2, where W (.) represents the Wishart distribution with scale matrix, rR, and degrees of freedom r (e.g. Anderson (1984)).
The Hsiao et al. (1999) Bayes estimator is derived under the assumption that the
initial observation yi0 are fixed constants. As discussed in Anderson and Hsiao (1981, 1982), this assumption is clearly unjustifiable for a panel with finite T . However,
contrary to the sampling approach where the correct modelling of initial obser-
vations is quite important, the Hsiao et al. (1999) Bayesian approach appears to
perform fairly well in the estimation of the mean coefficients for dynamic random
coefficient models as demonstrated in their Monte Carlo studies.

Remark 6.4. Model (6.50) has not imposed any constraint on the coefficient of the
lag dependent variable, i. Often an investigator would like to impose the stability condition |i| < 1. One way to impose the stability condition on individual units

6 Random Coefficient Models

199

would be to assume that i follows a Beta distribution on (0,1). For a Bayes estimator under this assumption see Liu and Tiao (1980).

6.7 Testing for Heterogeneity Under Weak Exogeneity

Given the importance of heterogeneity, it is very important to test for it. There are at least three different categories of tests available: (i) direct tests of parameter equality of the type used by Zellner (1962) in a SURE framework; (ii) Hausman (1978) type tests of the difference between two estimators of  (or its subset); or (iii) Swamy (1970) type tests based on the dispersion of individual slope estimates from a suitable pooled estimator. The first type of test is generally applicable when N is relatively small and T sufficiently large. Here we shall examine types (ii) and (iii), and assume that N and T are sufficiently large.
The Hausman method can be used in cases where it the two estimators are consistent under the null of homogeneity, whilst only one of them is efficient. Also, under the alternative hypothesis the two estimators converge to different values.
Denote the efficient estimator by subscript "e" and the inefficient but consistent estimator (under the alternative hypothesis) by the subscript "c". Then we have

V ( c - e) = V ( c) -V ( e).

(6.58)

This is the result used by Hausman (1978) where it is assumed that  e is asymptotically the most efficient estimator. However, it is easily shown that (6.58) hold under
a weaker requirement, namely when the (asymptotic) efficiency of  e cannot be enhanced by the information contained in  c. Consider a third estimator  , defined as a convex combination of  c and  e

q   = (1 -  )q  e +  q  c,

(6.59)

where q is a vector of constants, and  is a scalar in the range 0    1. Since, by assumption, the asymptotic efficiency of  e cannot be enhanced by the knowledge of  c, then it must be that V (q  )  V (q  e), and hence the value of  that minimises V (q  ), say  , should be zero. However, using (6.59) directly, we have

  = q [V ( e) - Cov( e, c)]q = 0, q V ( c - e)q

(6.60)

and hence q [V ( e) - Cov( e, c)]q = 0. But, if this result is to hold for an arbitrary vector q, we must have

V ( e) = Cov( e, c).

(6.61)

200

C. Hsiao and M.H. Pesaran

Using this in

V ( c - e) = V ( c) +V ( e) - 2 Cov( e, c),

yields (6.58) as desired.
In the context of testing for slope heterogeneity a number of different Hausman
type tests can be used. One possibility would be to compare the pooled estimator of  , defined by4

N

-1 N

  ¯ OLS =

WiWi

Wiyi

i=1

i=1

with the mean group estimator ¯ MG, defined by (6.55). When the focus of at-

tention

is

on

the

mean

long

run

coefficients





=

E (

i/(1

-

i)),

as

in

Pesaran,


Shin and  OLS/(1

Smith (1999) - ^OLS) and 

the
 MG

heterogeneity test could

=

N -1

Ni=1




i,

where



 i

be =

based directly on ^i/(1 - ^i). Under

 OLS = the null

of homogeneity the pooled and the mean group estimators are both consistent, al-

though only the mean group estimator is consistent under the alternative hypothesis
when lagged values of the dependent variables are included in the model. Under the full homogeneity assumption ( i =  , i2 =  2), the asymptotic vari-
ance matrices of the pooled and the mean group estimators (for a fixed N and a large

T ) are given by

 
Cov( T

¯

OLS)

=

2 N

N

-1

N-1 i ,

i=1

(6.62)

and

 
Cov( T

¯

MG)

=

2 N

N
N-1 i-1
i=1

,

(6.63)

where i = p limT(WiWi/T ). Also we have

 Cov( T

¯

 OLS, T

¯

MG)

 =Cov( T

¯

OLS)

thus directly establishing that





 Cov T

¯ MG - ¯ OLS

  2 

= N



N
N-1 i-1
i=1

-

N
N-1 i
i=1

-1 ,

which is a positive definite matrix, assuming that i =  j, for some i and j.5 This condition is generally satisfied when the model contains regressors with heterogeneous variances. The above results suggest the following statistic for testing the homogeneity hypothesis:

h = NT ¯ MG - ¯ OLS V-1 ¯ MG - ¯ OLS ,

4 Similar exercises can also be carried out using fixed or random effects estimators. But to keep the exposition simple here we focus on pooled estimators. 5 For a proof see the Appendix in Pesaran, Smith and Im (1996).

6 Random Coefficient Models

201

where

 N
 V = M2 G N-1 i=1

WiWi

-1
-

T

 N-1 N WiWi
i=1 T

 -1
.

(6.64)

and

M2 G

=

1 N

Ni=1

i2.

In

computing

h,

one

could

also

equally

use

O2LS

instead

of

M2 G. Under the null hypothesis

H0 : i = ,  i =  , and i2 =  2, for all i,

and for N and T sufficiently large we have

h a K2+1. When the focus of the analysis is on the long run coefficients we first note that6


 OLS

- 

=

(OLS

-

)  + ( OLS (1 - OLS)

-



)

.

Therefore, under the homogeneity hypothesis, we have, for a large T

  
Cov T OLS

=

2 N(1 - )2

D

N
N-1 i
i=1

-1
D,

where D = ( , IK) is a K × (K + 1). Similarly,

(6.65)

 Cov

 T



 MG

=

2 N(1 - )2

D

N
N-1 i-1
i=1

D.

(6.66)

To estimate (6.65), and (6.66), the unknown parameters  2, , and  could be estimated either from pooled or mean group estimators. Using the mean group estimators, the test of the homogeneity or the long run coefficients can then be based on the following Hausman-type statistic:

h  = NT (1 - MG)2





 MG -  OLS

-1 



DMGVDMG

 MG -  OLS ,


where DMG = ( MG, IK), and V is given by (6.64). In general DMGVDMG is of full rank. Under the null hypothesis, for large N and T, h  K2 .
There are two major concerns with the routine use of the Hausman procedure

as a test of slope homogeneity. It could lack power for certain parameter values, as

it's implicit null does not necessarily coincide with the null hypothesis of interest.

Second, and more importantly, the Hausman test will not be applicable in the case

of panel data models containing only strictly exogenous regressors (i = 0 in (6.50) for all i) or in the case of pure autoregressive models ( i = 0 in (6.50) for all i). In the former case, both estimators, ¯ OLS and ¯ MG, are unbiased under the null and

6

Recall

that

under

homogeneity

hypothesis





=



/(1

-

)

and



 OLS

=



OLS/(1

-

^OLS).

202

C. Hsiao and M.H. Pesaran

the alternative hypotheses and testwill have no power. Whilst, in the case of pure autoregressive panel data models NT ¯ OLS - ¯ and NT ¯ MG - ¯ will be
asymptotically equivalent and the asymptotic variance of ¯ MG - ¯ OLS is zero under H0.
Phillips and Sul (2003) propose a different type of Hausman test where instead of comparing two different pooled estimators of the regression coefficients (as discussed above), they propose basing the test of homogeneity on the difference between the individual estimates and a suitably defined pooled estimator. In the context of the panel regression model (6.50), their test statistic can be written as

G = ^ N -N  ¯ OLS ^ -g 1 ^ N -N  ¯ OLS ,

where ^ N = (^ 1,^ 2, . . . ,^ N) is an N (K + 1) × 1 stacked vector of all the N individual estimates, N is a (N × 1) vector of unity, and ^ g is a consistent estimator of g,
the asymptotic variance matrix of ^ N -  N  ¯ OLS, under H0. Assuming H0 holds and N is fixed, then G d N2(K+1) as T  , so long as the g is a non-stochastic positive definite matrix.
As compared to the Hausman test based on ¯ MG - ¯ OLS, the G test is likely to be more powerful; but its use will be limited to panel data models where N is small
relative to T . Also, the G test will not be valid in the case of pure dynamic models,
very much for the same kind of reasons noted above in relation to the Hausman test
based on ¯ MG - ¯ OLS. It can be shown in the case of pure autoregressive models ( i = 0 in (6.50) for all i), Rank(g) = N - 1 and g is non-invertible.
Swamy (1970) bases his test of slope homogeneity on the dispersion of individual
estimates from a suitable pooled estimator. Swamy's test is developed for panels
where N is small relative to T , but allows for cross section heteroscedasticity. Based
on the Swamy's (1970) work, Pesaran and Yamagata (2008) propose standardized
dispersion statistics that are asymptotically normally distributed for large N and T . Consider a modified version of Swamy's (1970) test statistic7

N
S~ =  i - ¯ WOLS
i=1

WiWi ~i2

 i - ¯ WOLS

(6.67)

7 Swamy's (1970) statistic is defined by

N
S^ =  i - ¯ WOLS
i=1

WiWi ^i2

 i - ¯ WOLS

,

where

¯ WOLS =

N

-1 N

 ^i-2WiWi

 ^i-2Wiyi,

i=1

i=1

with ^i2 = T -1 yi - Wi i yi - Wi i . Swamy shows that under H0, S^ d (2N-1)(K+1) as T   for a fixed N, and nonstochastic regressors.

6 Random Coefficient Models
where ~i2 is an estimator of i2 based on ¯ OLS, namely ~i2 = T -1 yi - Wi¯ OLS yi - Wi¯ OLS ,

203
(6.68)

and ¯ WOLS is the weighted pooled estimator also computed using ~i2, namely

  ¯ WOLS =

N WiWi i=1 ~i2

-1

N i=1

Wiyi ~i2

.

(6.69)

Suppose for the model defined by (6.50), the following relation holds:

N
 N-1/2S~ = N-1/2 zi + Op T -1 + Op N-1/2 , i=1

(6.70)

where

zi = T -1/2uiWi

T -1WiWi -1 T -1/2Wiui . uiui/T

Since, under H0, zi d K2+1 as T  , it is reasonable to conjecture that up to order T -1, E (zi) and z2 = Var (zi) are given by (K + 1) and 2 (K + 1), respectively. Then,
supposing E (zi) = (K + 1) + O T -1 ,

we can write

N -1/2

S~ - (K + 1) z

N
 = N-1/2 i=1

S~ - E(zi) z

+ Op

 N T

+ Op T -1 ,

therefore

~ = N-1/2

S~ - (K + 1) 2(K + 1)

d N (0, 1)

 as N and T   in no particular order, such that N/T  0.

Importantly, this test is valid when the Hausman type test or G test procedure

might fail to be applicable, as stated above. Moreover, this test procedure is expected

to have higher power than the Hausman type test, where the latter is applicable.

6.8 A Random Coefficient Simultaneous Equation System

The generalisation of a single equation random coefficients model to a simultaneous equation system raises complicated issues of identification and estimation. To show this let us consider a system of G equations

YiBi + Xii = Ui, i = 1, . . . , N,

(6.71)

204

C. Hsiao and M.H. Pesaran

where Yi and Xi are the T × G and T × K matrices of endogenous and exogenous variables, respectively, Ui is the T × G matrices of errors, Bi and i are the G × G and K × G matrix of the coefficients of the endogenous variables and exogenous
variables, respectively. The reduced form, then, is of the form

Yi = -XiiB-i 1 + UiBi-1, = Xii + Vi,

(6.72)

where Suppose that

i = -iB-i 1, Vi = UiBi-1.

(6.73) (6.74)

Bi = B +  i, i =  + i,

(6.75) (6.76)

where  i and  i are G × G and G × K matrices of random variables independently distributed over i with means 0 and covariances  and , defined by  = E[(vec
 i) (vec  i) ] and  = E[(vec  i)(vec  i) ]. Then

E (i) = -E[( +  i)(B +  i)-1], =  B-1.

(6.77)

In other words, identification conditions of structural parameters cannot be derived by assuming that when sample size approaches infinity, ^ will converge to  B-1. In fact the assumption of (6.75) raises intractable difficulties at the levels of identification and estimation.
Kelejian (1974) has studied the problem of identification under (6.75) and (6.76). His results imply that any feedback between the endogenous variables must be avoided and that identifiability and interdependence exclude each other (also see Raj and Ullah (1981)). In other words, for any one equation we may treat all the other variables as predetermined. Therefore, for ease of analysis, instead of assuming (6.75), we shall assume that

Bi = B,  i,

(6.78)

where B is a non-singular matrix with fixed elements.
The combination of (6.76) and (6.78) amounts to assuming a random coefficients reduced form of (6.51), where i = -iB-1 = -( +  i)B-1, and

E (i) = -B-1,

(6.79)

Cov(i) = [B-1  Ik][B-1  Ik] = .

(6.80)

6 Random Coefficient Models

205

Assume that Ui are independently distributed over time but are contemporaneously correlated, then

Cov(Ui) = E[vec(Ui)vec(Ui) ] = Ci  IT .

(6.81)

Furthermore, we assume that Ui and  i are mutually independent and are independent of Xi. Then the reduced form (6.72) can be written as

Yi = Xi + Vi,

(6.82)

where Vi = -Xi iB¯ -1 + UiB¯ -1 and E (Vi ) = 0,

Cov(Vi) = B¯ -1 CiB¯ -1  IT + (IG  Xi)(IG  Xt ) = Qi, i = 1, . . . , N.
The GLS estimator of ¯ is then equal to Balestra and Negassi (1992)

(6.83)

N

-1 N

 vec(GLS) =

R~ -i 1

 R~ -i 1vec^ i ,

i=1

i=1

(6.84)

where

R~ i = Qi  (XiXi)-1 + , ^ i = (XiXi)-1XiYi.

(6.85) (6.86)

If B-1 CiB-1 and  are unknown, a two-step GLS procedure can be applied. In the first step, we estimate B-1 CiB-1 and  by

B-1 CiB-1

=

T

1 -

K

V~ i

V~ i,

V~ i

=

Yi

-

Xii,

vec

()

=

1 N-1

N
[vec
i=1

(i - )][vec (i - )]

,

 

=

1 N

N
i.
i=1

(6.87)

In the second step, we estimate  using (6.84) by substituting R~ i for R~ i. If our interest is in the structural form parameters B and , we can either solve for
B and  from the reduced form estimate ¯ , or we can estimate them directly using instrumental variables method. Rewrite the first equation of the structural form in
the following way,

yi1 = Yi1¯1 + Xi1i1 + ui1, = Zi1 1 + i1, i = 1, . . . , N,

(6.88)

206

C. Hsiao and M.H. Pesaran

where yi1 is the T × 1 vector of the first endogenous variables and Yi1 is the T × g

matrix of the other endogenous variables appearing in the first equation g  G -

1, Xi1 i1 are

is g

the ×1

T× and

k k

matrix of included exogenous variables k  K, and

×

1

vectors

of

coefficients,

respectively

with

i1

=

[

 i1

¯1 and

+



 i1

],

and

Zi1

=

[Yi1,

Xi1

],



1

=

[¯1

,

¯1

],

vi1

=

ui1

+

Xi1

 i1

.

Balestra

and

Negassi

(1992)

suggest the following instrumental variables estimator

N

-1 N

  1 =

Zi1FiZi1

 Zi1Fi i1 ,

i=1

i=1

(6.89)

where and

 i1 = [Zi1Xi(XiXi)-1XiZi1]-1Zi1Xi(XiXi)-1Xiyi1,
Fi = Xi(Xi11Xi1 + 12Ik )-1Xi, 1 = E  i1 i1 .

(6.90) (6.91)

One can also derive the Bayes solutions for a simultaneous equations system of the form (6.71), (6.76) and (6.78) using a method analogous to that of Sect. 6.4. Considering one equation of (6.72) at a time, the results of sect. 4 can be applied straightforwardly. Similar results for the system of (6.72) can also be derived if the prior restrictions on  are ignored. Of course, restricted reduced form estimators can also be derived. The computation, though, can be laborious.
The results of Sect. 6.4 can also be used to derive a Bayes estimator for the structural form (6.88) based on a limited information approach. Let

Yi1 = Yi1 + Vi1,

(6.92)

where Yi1 = Xii1, and i1 = (XiXi)-1XiYi1. Substituting Yi1 for Yi1 in (6.88),

we have

yi1

=

Yi1

 1

+

Xi1i1

+



i1,

(6.93)

where



i1

=

ui1

+

Vi1


1.

Conditioning

on

i1,

we

can

treat

Yi1

and

Xi1

as

the

set

of

exogenous variables. Equation (6.93) is of the form of the mixed fixed and random

coefficients

model

(6.14)

and

the

Bayes

estimators

of




1,



 1

and

i1

are

given

in

Sect. 6.4 (for detail see Hsiao, Appelbe and Dineen (1992)). Of course, one should

keep in mind that now the Bayes estimator is the conditional posterior mean given

the estimated i1.

6.9 Random Coefficient Models with Cross-Section Dependence
In principle, the random coefficient model (6.14) can be easily adapted to allow for dependence across the error terms, uit , i = 1, 2, . . . , N. But, without plausible restrictions on the error covariances the number of unknown parameters of the model

6 Random Coefficient Models

207

increases at the rate of N2, which would be manageable only when N is relatively

small (typically 10 or less). To deal with the problem of cross section dependence

when N is large a number of different approaches have been advanced in the lit-

erature.8 In the case of spatial panels where a natural distance measure (or an im-

mutable ordering of cross section units) is available the dependence is tested and

modelled with "spatial lags", using techniques familiar from the time series litera-

ture. Anselin (2001) provides a recent survey of the literature on spatial economet-

rics. A number of studies have also used measures such as trade or capital flows to

capture economic distance, as in Lee and Pesaran (1993), Conley and Topa (2002)

and Pesaran, Schuermann and Weiner (2004).

But, in the absence of suitable distance measures or natural orderings of the cross

section units a number of investigators have attempted to model the cross section

dependence using single or multiple factor residual models where uit is specified

in terms of a finite number of common factors. A convenient parameterization is

given by

uit =

i

 ift + it ,

1+ i i

(6.94)

where  i is a s × 1 vector of individual-specific factor loadings, ft is an s × 1 vector of unobserved (latent) factors, and it is an idiosyncratic error assumed to be distributed independently across i, the unobserved factors, ft, and the observed regressors, xit , with mean zero and a unit variance. Since the common factors are unobserved, without loss of generality we also assume that ft  (0, Is).
Under the above set up, and conditional on a given set of factor loadings, the
cross-correlations of the errors are given by

i j =  ji =

1+ i i

 i j 1/2 1 +  j j

1/2 .

(6.95)

Complicated covariance structures can be accommodated by the residual factor for-

mulation through differences across factor loadings and by using a sufficiently large

number of factors. A random coefficient specification can also be assumed for the

factor loadings:

 i = ¯ +  i,

(6.96)

where ¯ is a vector of fixed constants

E( i) = 0,E  ift = 0,

E  ixit = 0,E  i i = 0,

E  i i

=

 , 0,

if if

i i

= =

j, j,

.

(6.97)

and  is a non-negative definite matrix. The average degree of cross dependence, defined by E (i j) for i = j is governed by ¯ and the distribution of  i. The average
8 Tests of error cross section dependence in the case of large panels are proposed by Pesaran (2004).

208

C. Hsiao and M.H. Pesaran

cross section dependence will Typically one would expect ¯

be =

zero 0.

if

¯

=

0,

and



i

is

symmetrically

distributed.

Examples of studies that have used the residual factor structure to model cross

section dependence include Holtz-Eakin, Newey, and Rosen (1988), Ahn, Lee and

Schmidt (2001), Coakley, Fuertes and Smith (2005), Bai and Ng (2004), Kapetanios

and Pesaran (2007), Phillips and Sul (2003), Moon and Perron (2004), and Moon,

Perron and Phillips (2007) and Pesaran (2006, 2007). The studies by Holtz-Eakin

et al. and Ahn et al. focus on single factor residual models and allow for time-

varying individual effects in the case of panels with homogeneous slopes where T

is fixed and N  . Phillips and Sul (2003) suggest using SURE-GLS techniques

combined with median unbiased estimation in the case of first order autoregres-

sive panels. Coakley, Fuertes and Smith (2002) propose a principal components ap-

proach which is shown by Pesaran (2006) to be consistent only when the factors and

the included regressors are either asymptotically uncorrelated or are perfectly cor-

related. In the more general case Pesaran (2006) shows that consistent estimation of

the random coefficient models with a multi-factor residual structure can be achieved

(under certain regularity conditions) by augmenting the observed regressors with the

cross section averages of the dependent variable and individual-specific regressors,

namely

N

N

  y¯t = w jy jt , and x¯it = w jx jt ,

j=1

j=1

(6.98)

for any set of weights such that

 wi = O

1 N

N
, |wi| < K < .
i=1

An obvious example of such a weighting scheme is wi = 1/N.9

6.10 Concluding Remarks
When the included conditional variables together with the conventional variable intercept or error components (e.g. Hsiao (2003, Chap. 3)) cannot completely capture systematic differences across cross-sectional units and/or over time, and the possibility of adding additional conditional variables is not an option, either due to data unavailability or the desire to keep the model simple, there is very little alternative but to allow the slope coefficients to vary across cross-section units or over time. If we treat all these coefficients as fixed and different, there is no particular reason to pool the data, except for some efficiency gain in a Zellner's (1962) seemingly unrelated regression framework. Random coefficients models appear to be an attractive middle ground between the implausible assumption of homogeneity across
9 Note that the non-parametric variance­covariance matrix estimator proposed in Pesaran (2006) is robust to heteroscedastic and/or serially correlated idiosyncratic errors, it .

6 Random Coefficient Models

209

cross-sectional units or over time and the infeasibility of treating them all differently, in the sense of being draws from different probability distributions. Other intermediate formulations could also be considered. For example, as argued by Pesaran, Shin and Smith (1999), in the context of dynamic models it would be plausible to impose the homogeneity hypothesis on the long-run coefficients but let the shortrun dynamics to vary freely across the cross-section units. In this Chapter various formulations are surveyed and their implications discussed. Our review has been largely confined to linear panel data models with stationary regressors. The analysis of random coefficient models with unit roots and cointegration is reviewed in Breitung and Pesaran (2007) in this volume. Parameter heterogeneity in non-linear panel data models poses fundamentally new problems and needs to be considered on a case-by-case basis.

Appendix A: Proof of Theorem 1

To prove part (a) of the theorem, we write (6.41) in the form of (6.19) and (6.17). Putting u  N(0, C) and   N(0, IN  ) together with (6.17), the result follows.
To prove (b), we use Bayes's theorem, that is

p(|y)  p(y|)p(),

(6.99)

where p(y|) follows from (6.42) and p() is given by (6.39). The product on the

right

hand

side

of

(6.99)

is

proportional

to

exp{-

1 2

Q},

where

Q

is

given

by

Q = (y - Z) [C + W(IN  )W ]-1(y - Z)

(6.100)

= ( - ¯) D-1( - ¯) + y {-1 - -1Z[Z DZ]-1Z -1}y.

The second term on the right hand side of (6.100) is a constant as far as the distribution of  is concerned, and the remainder of the expression demonstrates the truth of (b).
To prove (c), we use the relations

p( |y) = p( , |y)d = [p(|y,  )d]p( |y)

(6.101)

and

p( , |y)  p(y|,)p( , ) = p(y|,)p( ) · p().

(6.102)

Under where

(6.38)­(6.40), Q is given by

the

right

hand

side

of

(6.102)

is

proportional

to

exp{-

1 2

Q},

210

C. Hsiao and M.H. Pesaran

Q = (y - Z -W  ) C-1(y-Z - W) +  (IN  -1)

= y C-1y +  Z C-1Z +  W C-1W

-2 Z C-1y - 2 W C-1y + 2 Z C-1W +  (IN  -1)

= Q1 + Q2 + Q3,

(6.103)

with
Q1 = { - (Z C-1Z)-1[Z C-1(y - W )} (Z C-1Z) ·{ - (Z C-1Z)-1[Z C-1(y - W )]},

(6.104)

Q2 = { - DW [C-1 - C-1Z(Z C-1Z)-1Z C-1]y} D-1 ·{ - DW [C-1 - C-1Z(Z C-1Z)-1Z C-1]y}

(6.105)

and

Q3 = y {C-1 - C-1Z(Z C-1Z)-1Z C-1 - [C-1 - C-1Z(Z C-1Z)-1Z C-1]

·WD-1W [C-1 - C-1Z(Z C-1Z)-1Z C-1]}y.

(6.106)

As far as the distribution of p( , |y) is concerned, Q3 is a constant. The conditional

distribution

of



given

y

and



is

proportional

to

exp{-

1 2

Q1},

which

integrates

to

1.

Therefore,

the

marginal

distribution

of



given

y

is

proportional

to

exp{-

1 2

Q2},

demonstrates (c).

Substituting (6.23)­(6.26) into (6.42) we obtain the Bayes solutions for the

Swamy type random coefficients model: (i) the distribution of  given y is N(¯ ,D),

and (ii) the distribution of  given y is normal with mean

 = {X [C-1 - C-1XA(A X C-1XA)-1A X C-1]X + (IN  -1)}-1 ·{X [C-1 - C-1XA(A X C-1XA)-1A X C-1]y}

= D{X [C-1 - C-1XA(A X C-1XA)-1A X C-1]y},

(6.107)

and covariance

D = {X [C-1 - C-1XA(A X C-1XA)-1A X C-1]X + (IN  -1)}-1. (6.108)

Letting  = IN   and repeatedly using the identity (6.30) we can write (6.108) in the form

D = [X C-1X + -1]-1{I - X C-1XA[A X C-1X(X C-1X + -1)-1X C-1XA

-A X C-1XA]-1A X C-1X[X C-1X + -1]-1}

= [X C-1X + -1]-1{I + X C-1XA[A X (XX + C)XA]-1

A (X C-1X-1 - -1) · [X C-1X + -1]-1}

= [X C-1X + -1]-1 + X (XX + C)-1XA[A X (XX + C)-1XA]-1

·A X (XX + C)-1X.

(6.109)

6 Random Coefficient Models

211

Substituting (6.109) into (6.107) we have

 = [X C-1X + -1]-1X C-1y

-(X C-1X + -1)-1(X C-1X + -1 - -1)A(A X C-1XA)-1A X C-1y

+X (XX + C)-1XA[A X (XX + C)-1XA]-1A X [C-1

-(XX + C)-1y - X (XX + C)-1XA[A X (XX + C)-1XA]-1

·[I - A X(XX + C)-1XA](A X C-1XA)-1A X C-1y

=

(X

C-1X

+

-1


)-1X

C-1y

-

A(A

X

C-1XA)-1A

X

C-1y

+(X C-1 + -1)-1-1A(A X C-1XA)-1A X C-1y

-X (XX + C)-1XA[A X (XX + C)-1XA]-1A X (XX + C)-1y

+X (XX + C)-1XA(A X C-1XA)-1A X C-1y

=

(X

C-1X

+

-1


)-1X

C-1y - X

(XX

+ C)-1XA .

(6.110)

Acknowledgments We are grateful to an anonymous referee, G. Bresson, A. Pirotte, and particularly Takashi Yamagata for their careful reading of an early version and for pointing out many typos and for their good suggestions. We would also like to thank J. Breitung and Ron Smith for helpful comments.

References
Ahn, S.G., Y.H. Lee and P. Schmidt, (2001), "GMM Estimation of Linear Panel Data Models with Time-varying Individual Effects", Journal of Econometrics, 102, 219­255.
Amemiya, T. (1978), "A Note on a Random Coefficients Model", International Economic Review, 19, 793­796.
Anderson, T.W. (1984), An Introduction to Multivariate Statistical Analysis, 2nd edition, New York, Wiley.
Anderson, T.W. and C. Hsiao (1981), "Estimation of Dynamic Models with Error Components", Journal of the American Statistical Society, 76, 598­606.
Anderson, T.W. and C. Hsiao (1982),"Formulation and Estimation of Dynamic Models Using Panel Data", Journal of Econometrics, 18, 47­82.
Anselin, L. (2001), "Spatial Econometrics", in B. Baltagi (ed.), A Companion to Theoretical Econometrics, Oxford, Blackwell.
Bai, J. and S. Ng (2004), "A Panic Attack on Unit Roots and Cointegration", Econometrica, 72, 1127­1177.
Balestra, P. (1996), "Introduction to Linear Models for Panel Data", in L. Ma´tya´s and P. Sevestre (eds.), The Econometrics of Panel Data: A Handbook of the Theory with Applications, New york, Springer.
Balestra, P. and S. Negassi (1992), "A Random Coefficient Simultaneous Equation System with an Application to Direct Foreign Investment by French Firms", Empirical Economics, 17, 205­220.
Boskin, M.J. and L.J. Lau (1990), "Post-War Economic Growth in the Group-of-Five Countries: A New Analysis", CEPR No. 217, Stanford University.

212

C. Hsiao and M.H. Pesaran

Breitung J. and M.H. Pesaran (2007), "Unit Roots and Cointegration in Panels", In this volume. Coakley, J., A. Fuertes and R.P. Smith (2002), "A Principal Components Approach to
Cross-Section Dependence in Panels", Unpublished manuscript, Birkbeck College, University of London. Coakley, J., A. Fuertes and R.P. Smith (2005), "Unobserved Heterogeneity in Panel Time Series Models", Computational Statistics & Data Analysis, Forthcoming. Conley, T.G. and G. Topa (2002), "Socio-economic Distance and Spatial Patterns in Unemployment", Journal of Applied Econometrics, 17, 303­327. Cooley, T.F. and E.C. Prescott (1976) "Estimation in the Presence of Stochastic Parameter Variation", Econometrica, 44, 167­184. de Finetti, B. (1964), "Foresight: Its Logical Laws. Its Subjective Sources", in J.E. Kyburg, Jr., and H.E. Smokle (eds.), Studies in Subjective Probability, New Yor, Wiley, 93­158. Hausman, J.A. (1978), "Specification Tests in Econometrics", Econometrica, 46, 1251­1271. Hendricks, W., R. Koenker and D.J. Poirier (1979), "Residential Demand for Electricity: An Econometric Approach", Journal of Econometrics, 9, 33­57. Hildreth, C. and J.P. Houck (1968), "Some Estimators for a Linear Model with Random Coefficients", Journal of the American Statistical Association, 63, 584­595. Holtz-Eakin, D, W.K. Newey and H. Rosen (1988), "Estimating Vector Autoregressions with Panel Data", Econometrica, 56, 1371­1395. Hsiao, C. (1974), "Statistical Inference for a Model with Both Random Cross-Sectional and Time Effects", International Economic Review, 15, 12­30. Hsiao, C. (1975), "Some Estimation Methods for a Random Coefficients Model", Econometrica, 43, 305­325. Hsiao, C. (1990), "A Mixed Fixed and Random Coefficients Framework for Pooling Crosssection and Time Series Data", Paper presented at the Third Conference on Telecommunication Demand Analysis with Dynamic Regulation, Hilton, Head, S. Carolina. Hsiao, C. (2003), Analysis of Panel Data, Economic Society monographs no. 34, 2nd ed., New York: Cambridge University Press. Hsiao, C., T.W. Appelbe, and C.R. Dineen (1992), "A General Framework for Panel Data Models ­ With an Application to Canadian Customer-Dialed Long Distance Telephone Service", Journal of Econometrics, 59, 63­86. Hsiao, C., D.C. Mountain, K.Y. Tsui and M.W. Luke Chan (1989), "Modeling Ontario Regional Electricity System Demand Using a Mixed Fixed and Random Coefficients Approach", Regional Science and Urban Economics 19, 567­587. Hsiao, C., M.H. Pesaran and A.K. Tahmiscioglu (1999), "Bayes Estimation of Short-Run Coefficients in Dynamic Panel Data Models", in C. Hsiao, L.F. Lee, K. Lahiri and M.H. Pesaran (eds.), Analysis of Panels and Limited Dependent Variables Models, Cambridge: Cambridge University Press, 268­296. Hurwicz, L. (1950), "Least Squares Bias in Time Series", in T.C. Koopman, (ed.), Statistical Inference in Dynamic Economic Models, New York: Wiley, 365­383. Judge, G.G., W.E. Griffiths, R.C. Hill, H. Lu¨tkepohl and T.C. Lee (1985), The Theory and Practice of Econometrics, 2nd ed., New York: Wiley. Kapetanios, G. and M.H. Pesaran (2007), "Alternative Approaches To Estimation and Inference in Large Multifactor Panels: Small Sample Results with an Application to Modelling of Asset Return", in G. Phillips and E. Tzavalis (eds.), The Refinement of Econometric Estimation and Test Procedures: Finite Sample and Asymptotic Analysis, Cambridge, Cambridge University Press. Kelejian, H.H. (1974), "Random Parameters in Simultaneous Equation Framework: Identification and Estimation", Econometrica, 42, 517­527. Kiviet, J.F. and G.D.A. Phillips (1993), "Alternative Bias Approximation with Lagged-Dependent Variables", Econometric Theory, 9, 62­80. Lee, K.C., and M.H. Pesaran (1993), "The Role of Sectoral Interactions in Wage Determination in the UK Economy", The Economic Journal, 103, 21­55.

6 Random Coefficient Models

213

Lindley, D.V. and A.F.M. Smith (1972), "Bayes Estimates for the Linear Model", Journal of the Royal Statistical Society, B. 34, 1­41.
Liu, L.M. and G.C. Tiao (1980), "Random Coefficient First-Order Autoregressive Models", Journal of Econometrics, 13, 305­325.
Moon, H.R. and B. Perron, (2004), "Testing for a Unit Root in Panels with Dynamic Factors", Journal of Econometrics, 122, 81­126.
Moon, H.R., B. Perron, and P.C.B. Phillips (2007), "Incidental Trends and the Power of Panel Unit Root Tests", in Journal of Econometrics, 141, 416­459 .
Murtazashvili I. and J.M. Wooldridge (2007), "Fixed Effects Instrumental Varaibles Estimation in Correlated Random Coeffcient Panel Data Models", forthcoming in Journal of Econometrics.
Pagan, A. (1980), "Some Identification and Estimation Results for Regression Models with Stochastically Varying Coefficients", Journal of Econometrics, 13, 341­364.
Pesaran, M.H. (2004), "General Diagnostic Tests for Cross Section Dependence in Panels", Cambridge Working Papers in Economics, No. 435, University of Cambridge and CESifo Working Paper Series No. 1229.
Pesaran, M.H., (2006), "Estimation and Inference in Large Heterogeneous Panels with a Multifactor Error Structure",Econometrica, 74, 967­1012.
Pesaran, M.H. (2007), "A Simple Panel Unit Root Test in the Presence of Cross Section Dependence", Journal of Applied Econometrics, 22, 265­312.
Pesaran, M.H. and R. Smith (1995), "Estimation of Long-Run Relationships from Dynamic Heterogeneous Panels", Journal of Econometrics, 68, 79­114.
Pesaran, M.H., R. Smith and K.S. Im (1996), "Dynamic Linear Models for Heterogeneous Panels", Chap. 8 in Ma´tya´s, L. and P. Sevestre (eds.), The Econometrics of Panel Data: A Handbook of Theory with Applications, 2nd revised edition, Doredrecht: Kluwer Academic Publications.
Pesaran, M.H.,Y. Shin and R.P. Smith, (1999), "Pooled Mean Group Estimation of Dynamic Heterogeneous Panels", Journal of the American Statistical Association, 94, 621­634.
Pesaran, M.H., T. Schuermann, and S.M. Weiner. (2004). "Modeling Regional Interdependencies using a Global Error-Correcting Macroeconometric Model." (with discussion), Journal of Business and Economic Statistics 22, 129­162 and 175­181.
Pesaran, M.H. and T. Yamagata (2008), "Testing Slope Homogeneity in Large Panels", Journal of Econometrics, 142, 50­93.
Phillips, P.C.B. and D. Sul (2003), "Dynamic Panel Estimation and Homogeneity Testing Under Cross Section Dependence", Econometrics Journal, 6, 217­259.
Raj, B. and A. Ullah (1981), Econometrics: A Varying Coefficients Approach, London, Croom Helm.
Rosenberg, B. (1972), "The Estimation of Stationary Stochastic Regression Parameters Reexamined", Journal of the American Statistical Association, 67, 650­654.
Rosenberg, B. (1973), "The Analysis of a Cross-Section of Time Series by Stochastically Convergent Parameter Regression", Annals of Economic and Social Measurement, 2, 399­428.
Singh, B., A.L. Nagar, N.K. Choudhry, and B. Raj (1976), "On the Estimation of Structural Changes: A Generalisation of the Random Coefficients Regression Model", International Economic Review, 17, 340­361.
Swamy, P.A.V.B. (1970), "Efficient Inference in a Random Coefficient Regression Model", Econometrica, 38, 311­323.
Swamy, P.A.V.B. and P.A. Tinsley (1977), "Linear Prediction and Estimation Methods for Regression Models with Stationary Stochastic Coefficients", Federal Reserve Board Division of Research and Statistics, Special Studies Paper No. 78, Washington, D.C.
Wachter, M.L. (1976), "The Changing Cyclical Responsiveness of Wage Inflation", Brookings Paper on Economic Activity, 115­168.
Zellner, A. (1962), "An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for Aggregation Bias", Journal of the American Statistical Association, 57, 348­368.

Chapter 7
Parametric Binary Choice Models
Michael Lechner, Ste´fan Lollivier and Thierry Magnac

7.1 Introduction
Binary dependent data are a common feature in many areas of empirical economics as, for example, in transportation choice, the analysis of unemployment, labor supply, schooling decisions, fertility decisions, innovation behaviour of firms, etc. As panel data is increasingly available, the demand for panel data models coping with binary dependent variables is also increasing. Also, dramatic increases in computer capacity have greatly enhanced our ability to estimate a new generation of models. The second volume of this handbook contains several applications based on this type of dependent variable and we will therefore limit this chapter to the exposition of econometric models and methods.
There is a long history of binary choice models applied to panel data which can for example be found in Arellano and Honore (2001), Baltagi (2000), Hsiao (1992, 1996, 2003), Lee (2002) or Sevestre (2002) as well as in chapters of econometrics textbooks as for instance Greene (2003) or Wooldridge (2002). Some of these books and chapters do not devote much space to the binary choice model. Here, in view of other chapters in this handbook that address related nonlinear models (qualitative, truncated or censored variables, nonparametric models, etc.), we focus on the parametric binary choice model and some of its semiparametric extensions. The binary choice model provides a convenient benchmark case, from which many results can be generalised to limited dependent variable models such as multinomial discrete choices (Train, 2002), transition models in continuous time (Kamionka, 1998) or to structural dynamic discrete choice models that are not studied here.
Michael Lechner Swiss Institute for Empirical Economic Research (SEW), University of St. Gallen, Varnbu¨hlstr. 14, CH-9000 St. Gallen, Switzerland, e-mail: Michael.Lechner@unisg.ch
Ste´fan Lollivier INSEE, 18 boulevard Adolphe Pinard, F-75014 Paris, France, e-mail: stefan.lollivier@insee.fr
Thierry Magnac Universite´ de Toulouse 1, Toulouse School of Economics, Manufacture des Tabacs, 21, Alle´e de Brienne, 31000 Toulouse, France, e-mail: magnac@cict.fr

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

215

c Springer-Verlag Berlin Heidelberg 2008

216

M. Lechner et al.

We tried to be more comprehensive than the papers and chapters mentioned and we provide an introduction into the many issues that arise in such models. We also try not only to provide an overview of different models and estimators but also to make sure that the technical level of this chapter is such that it can easily be understood by the applied econometrician. For all technical details, the reader is referred to the specific papers.
Before we discuss different versions of the binary choice panel data models, define first the notation for the data generating process underlying the prototypical binary choice panel model:
yit = 1{yit > 0} for any i = 1, . . . , N and t = 1, . . . , T,
where 1{.} is the indicator of the event between brackets and where the latent dependent variables yit are written as:
yit = Xit  + it ,
where  denotes a vector of parameters, Xit is a 1×K vector of explanatory variables and error terms it stand for other unobserved variables. Stacking the T observations of individual i,
Yi = Xi + i ,
where Yi = (yi1, ., yiT ) is the vector of latent variables, Xi = (Xi1, ., XiT ) is the T × K matrix of explanatory variables and i = (i1, ., iT ) is the T × 1 vector of errors.
We focus on the estimation of parameter  and of parameters entering the distribution function of it . We do not discuss assumptions under which such parameters can be used to compute other parameters, such as causal effects (Angrist, 2001). We also consider balanced panel data for ease of notation although the general case of unbalanced panel is generally not much more difficult if the data is missing at random (see Chap. 12).
As usual in econometrics we impose particular assumptions at the level of the latent model to generate the different versions of the observable model to be discussed in the sections of this chapter. These assumptions concern the correlation of the error terms over time as well as the correlation between the error terms and the explanatory variables. The properties for various conditional expectations of the observable binary dependent variable are then derived. We assume that the observations are obtained by independent draws in the population of statistical units `i', also called individuals in this chapter. Working samples that we have in mind are much larger in dimension N than in dimension T and in most cases we consider asymptotics in N holding T fixed although we report on some recent work on large T approximations. Time effects can then be treated in a determistic way. In this chapter we frequently state our results for an important special case, the panel probit model where error terms i are assumed to be normally distributed.
In Sect. 7.2 of this chapter we discuss different versions of the static random effects model when the explanatory variables are strictly exogenous. Depending on the autocorrelation structure of the errors different estimators are available and

7 Parametric Binary Choice Models

217

we detail their attractiveness in each situation by trading-off their efficiency and robustness with respect to misspecification. Section 7.3 considers the static model when a time invariant unobservable variable is correlated with the time varying explanatory variables. The non linearity of binary choice models makes it pretty hard to eliminate individual fixed effects in likelihood functions and moment conditions, because the usual `differencing out trick' of the linear model does not work except in special cases. Imposing quite restrictive assumptions is the price to pay to estimate consistently parameters of interest. Finally, Sect. 7.4 addresses the important issue of structural dynamics for fixed and random effects, in other words cases when the explanatory variables include lagged endogenous variables or are weakly exogenous only.

7.2 Random Effects Models Under Strict Exogeneity

In this section we set up the simplest models and notations that will be used in the rest of the chapter. We consider in this chapter that random effects models defined as in Arellano and Honore (2001) as models where errors in the latent model are independent of the explanatory variables.1 This assumption does not hold with respect to the explanatory variables in the current period only but also in all past and future periods so that explanatory variables are also considered in this section to be strictly exogenous in the sense that:

Ft (it |Xi) = Ft (it ) ,

(7.1)

where Ft (it ) denotes the marginal distribution function of the error term in period t. When errors are not independent over time, it will also at times be useful to impose

a stronger condition on the joint distribution of the T errors terms over time, denoted

F(T )(·):

F(T )(i|Xi) = F(T )(i) .

(7.2)

Note that as in binary choice models in cross-sections, marginal choice probabilities can be expressed in terms of the parameters of the latent model:

P(yit = 1|Xi) = E(yit = 1|Xi) = E(yit = 1|Xit = xit ) = 1 - Ft (-Xit  ) .

(7.3)

It also emphasizes that the expectation of a Bernoulli variable completely describes its distribution.

1 One needs to assume independence between errors and regressors instead of assuming that correlations are equal to zero because of the non-linearity of the conditional expectation of the dependent variable with respect to individual effects.

218

M. Lechner et al.

We already said that we would consider random samples only. Individual observations are independent and if  generically denote all unknown parameters including those of the distribution function of errors, the sample likelihood function is the product of individual likelihood functions:

N
L( ) =  Li(Yi|Xi;  ) i=1
where Yi = (yi1, ., yiT ) is the vector of binary observations.

7.2.1 Errors are Independent Over Time

When errors are independent over time, the panel model collapses to a crosssectional model with NT independent observations and the maximum likelihood estimator is the standard estimator of choice. The likelihood function for one observation is given by:

T
 Li(Yi|Xi;  ) = [1 - Ft (-Xit  )]yit Ft (-Xit  )(1-yit ) . t=1

(7.4)

Later it will be pointed out that even if true errors are not independent over time, nevertheless the pseudo-maximum likelihood estimator (incorrectly) based on independence ­ the so called `pooled estimator' ­ has attractive properties (Robinson, 1982).
Let (·) denote the cumulative distribution function (cdf) of the univariate zero mean unit variance normal distribution, we obtain the following log-likelihood function for the probit model:

Li(Yi|Xi;  , 2, . . . , T ; 1 = 1)

T
= yit ln 
t=1

Xit  t

+(1 - yit ) ln[1 - 

Xit  t

].

Note that to identify the scale of the parameters, the standard error of the error term in the first period is normalised to 1 (1 = 1). If all coefficients are allowed to vary over time in an unrestricted way, then more variances have to be normalised.2
In many applications however, the variance of the error is kept constant over time (t = 1). For notational convenience this assumption will be maintained in the remainder of the chapter.

2 See for example the discussion in Chamberlain (1984).

7 Parametric Binary Choice Models

219

7.2.2 One Factor Error Terms

7.2.2.1 The Model
Probably the most immediate generalisation of the assumption of independent errors over time is a one-factor structure where all error terms are decomposed into two different independent components. One is constant over time (ui) and is called the individual effect, the other one being time variable (vit ), but identically and independently distributed (iid) over time and individuals. Thus, we assume that for i = 1, . . . , N and t = 1, . . . , T :
T
 it = ui + vit , Fv(T )(vi1, . . . , viT |Xi) = Fvt (vit ) ; t=1
T
 Fu(,Tv)(ui, vi1, . . . , viT |Xi) = Fu(ui) Fvt (vit ) . t=1
The individual effect, ui, can be interpreted as describing the influence of timeindependent variables which are omitted from the model and that are independent of the explanatory variables. Note that the one-factor decomposition is quite strong in terms of its time series properties, because the correlation between the error terms of the latent model does not die out when the time distance between them is increased.
To achieve identification, restrictions need to be imposed on the variances of each error component which are denoted v2 and u2. For example, variance v2 can be assumed to be equal to a given value (to 1 in the normal case), or one can consider the restriction that the variance of the sum of error terms is equal to 1 (u2 + v2 = 1). It simplifies the comparison with cross section estimations. In this section, we do not restrict u and v for ease of notation though such a restriction should be imposed at the estimation stage.

7.2.2.2 Maximum Likelihood Estimation
The computation of the log-likelihood function is difficult when errors are not independent over time or have not a one-factor structure since the individual likelihood contribution is defined as an integral with respect to a T dimensional distribution function. Assumptions of independence or one-factor structure simplify the computation of the likelihood function (Butler and Moffitt, 1982).
The idea is the following. For a given value of ui, the model is a standard binary choice model as the remaining error terms vit are independent between dates and individuals. Conditional on ui, the likelihood function of individual i is thus:
T
 Li(Yi|Xi, ui;  ) = [1 - Fv(-Xit  - ui)]yit [Fv(-Xit  - ui)]1-yit t=1

220

M. Lechner et al.

The unconditional likelihood function is derived by integration:

Li(Yi|Xi;  ) =

+ -

Li(Yi|Xi, ui;  )

fu(ui)dui

.

(7.5)

The computation of the likelihood function thus requires simple integrations only. Moreover, different parametric distribution functions for ui and vit can be specified in this `integrating out' approach. For instance, the marginal distribution functions of the two error components can be different as in the case with a normal random effect and logistic iid random error.3 Also note that the random effect may be modelled in a flexible way. For example Heckman and Singer (1984), Mroz (1999), and many others suggested the modeling framework where the support of individual effects of ui is discrete so that the cumulative distribution function of ui is a step function. Geweke and Keane (2001) also suggest mixtures of normal distribution functions.
For the special case of a T normal variate error, ui, the log-likelihood of the resulting probit model is given by:

Li(Yi|Xi;  ) =

 + T

=



- t=1

Xit  + uui v

yit
1-

Xit  + uui v

1-yit

 (ui)dui,

(7.6)

where  (·) denotes the density function of the standard normal distribution. In this case, the most usual identification restriction is u2 + v2 = 1, so that the disturbances can be written as:
it = ui + 1 - 2vit ,
where ui and vit are univariate normal, N(0, 1), and  > 0. Parameter 2 is the share of the variance of the error term due to individual effects.
The computation of the likelihood function is a well-known problem in mathematics and is performed using gaussian quadrature. The most efficient method of computation that leads to the so called `random effects probit estimator' uses the Hermite integration formula (Butler and Moffitt, 1982). See also the paper by Guilkey and Murphy (1993) for more details on this model and estimator as well as Lee (2000) for more discussion about the numerical algorithm.
Finally, Robinson (1982) and Avery, Hansen and Hotz (1983) show that the pooled estimator is an alternative to the previous method. The pooled estimator is the pseudo-maximum likelihood estimator where it is incorrectly assumed that errors are independent over time. As a pseudo likelihood estimator, it is consistent though inefficient. Note that the standard errors of estimated parameters are to be computed using pseudo-likelihood theory (Gourie´roux, Monfort and Trognon, 1984).

3 As it can be found in STATA for instance.

7 Parametric Binary Choice Models

221

7.2.3 General Error Structures

Obviously, the autocorrelation structure implied by the one factor-structure is very restrictive. Correlations do not depend on the distance between periods t and t . The general model that uses only the restrictions implied by (7.1) and (7.2) poses, however severe computational problems. Computing the maximum likelihood estimator requires high dimensional numerical integration. For example, Gaussian quadrature methods for the normal model do not work in practice when the dimension of integration is larger than four.
There are two ways out of these computational problems. First, instead of computing the exact maximum likelihood estimator, we can use simulation methods and approximate the ML estimator by simulated maximum likelihood (SML). It retains asymptotic efficiency under some conditions that will be stated later on (e.g. Hajivassiliou, McFadden and Ruud (1996)). In particular, SML methods require that the number of simulations tends to infinity to obtain consistent estimators. As an alternative there are estimators which are more robust to misspecifications in the serial correlation structure but which are inefficient because they are either based on misspecified likelihood functions (pseudo-likelihood) or on moment conditions that do not depend on the correlation structure of the error terms (GMM, e.g. Avery, Hansen and Hotz (1983), Breitung and Lechner (1997), Bertschek and Lechner (1998) and Inkmann (2000)). Concerning pseudo-ML estimation, we already noted that the pooled probit estimator is consistent irrespective of the error structure. Such a consistency proof is however not available for the one-factor random effects probit estimator.
Define the following set function:

D(Yi) =

Yi



RT

such

that

0 -

yit < < yit

+ <0

if if

yit yit

= =

1 0

The contribution of observation i to the likelihood is:

(7.7)

Li(Yi |Xi;  ) = E [1 {Yi  D(Yi)}]

(7.8)

In probit models, i is distributed as multivariate normal N(0, ),  being a T × T variance­covariance matrix. The likelihood function is:

Li(Yi|Xi;  ) =

 (T )(Yi - Xi , )dYi,

D(Yi)

where  (T)(·) denotes the density of the T -variate normal distribution. In the general case, the covariance matrix of the errors  is unrestricted (except
for identification purposes, see above). It is very frequent however to restrict its structure to reduce the number of parameters to be estimated. The reason for doing so is computation time, stability of convergence, occurrence of local extrema and the difficulties to pin down (locally identify) the matrix of correlations when the sample size is not very large. In many applications the random effects model discussed in

222

M. Lechner et al.

the previous section is generalised by allowing for an AR(1) process in the time
variant error component (vit ). Other more general structures however are feasible as well if there are enough data.
We will see below how to use simulation to approximate the likelihood func-
tion by using Simulated Maximum Likelihood (SML). Another popular estimation
method consist in using conditional moments directly. They are derived from the
true likelihood function and are approximated by simulation (Method of Simulated
Moments or MSM). McFadden (1989) proposed to consider all possible sequences of binary variables over T periods, Y , where  runs from 1 to 2T . Choice indicators are defined as di = 1 if i chooses sequence  and is equal to 0 otherwise. A moment estimator solves the empirical counterpart of the moment condition:

2T

 E

Wi [di - Pi ( )] = 0 ,

 =1

(7.9)

where Pi ( ) = Li(Y | Xi;  ) is the probability of sequence  (i.e. such that Yi = Y ). The optimal matrix of instruments Wi in the moment condition is:

Wi

=

 log[Pi ( )] 

 =0

,

where parameter 0 is the true value of  . In practice, any consistent estimator is a good choice to approximate parameter 0. The first of a two-step GMM proce-
dure using the moment conditions above and identity weights can lead to such a consistent estimate. It is then plugged in the expression for Wi at the second step.
Even if T is moderately large however, the number of sequences  is geometric in T (2T ) and functions Pi ( ) can be very small. What proposes Keane (1994) is to replace in (7.9) unconditional probabilities by conditional probabilities:

T1

  E

W~ it j (dit j - Pit j( )) = 0 ,

t=1 j=0

where dit j = 1 if and only if yit = j and where:

Pit j( ) = P(yit = j | yi1, ., yit-1, Xi;  )

=

P(yit = j, yi1, ., yit-1 | Xi;  ) P(yi1, ., yit-1 | Xi;  )

is the conditional probability of choice j conditional on observed lagged choices.
Finally, maximising the expectation of the log-likelihood function E log[Li(Yi | Xi;  )] is equivalent to solving the following system of score equations with respect to  :
E [Si( )] = 0 ,

7 Parametric Binary Choice Models

223

where

Si( )

=

 log[Li(Yi|Xi; )] 

is

the

score

function

for

individual

i.

It

can

be

shown

that, in most limited dependent variable models (Hajivassiliou and McFadden,

1998):

 

Li(Yi

|

Xi;



)

=

E

[gi(Yi

-

Xi

)1

{Yi



D(Yi)}]]

where:

gi(u) =

Xi -1u -1(uu - )-1/2

The score function can then be written as a conditional expectation:

Si( ) = E [gi(Yi - Xi ) |Yi  D(Yi) ]

(7.10)

which opens up the possibility of computing the scores by simulations (Method of Simulated Scores, MSS, Hajivassiliou and McFadden, 1998).

7.2.4 Simulation Methods

Simulation methods (SML, MSM, MSS) based on the criteria established in the

previous section consist in computing the expectation of a function of T random variates. The exact values of these high dimensional integrals are too difficult to

compute and these expectations are approximated by sums of random draws using

laws of large numbers:

 1
H

H h=1

f (h)

P
H 

E

f ()

when h is a random draw from a distribution. In the case of panel probit models, it is a multivariate normal distribution function, N(0, ).
It is not the purpose of this chapter to review the general theory of simulation

(see Gourie´roux and Monfort (1996) and Geweke and Keane (2001)). We review the properties of such methods in panel probit models only to which we add a brief ex-

planation of Gibbs resampling methods which borrow their principle from Bayesian techniques.

7.2.4.1 The Comparison Between SML, MSM, MSS in Probit Models

The naive SML function is for instance:

 1
H

H
I
h=1

{Yi



D(Yi)}

where I[Yi  D(Yi)] is a simulator. It is not continuous with respect to the parameter of interest however and this simulation method is not recommendable.

224

M. Lechner et al.

What is recommended is to use a smooth simulator which is differentiable with respect to the parameter of interest. The Monte Carlo evidence that the Geweke­ Hajivassiliou­Keane (GHK) simulator is the best one in multivariate probit models seems overwhelming (see Geweke and Keane (2001) and Hajivassiliou, McFadden and Ruud (1996), for a presentation).
The asymptotic conditions concerning the number of draws (H) and leading to consistency, absence of asymptotic bias and asymptotic normality are more or less restrictive according to each method, SML, MSM or MSS (Gourie´roux and Monfort, 1993). The method of simulated moments (MSM) yields consistent, asymptotically unbiased and normally distributed estimators as N   when H is fixed because the moment condition (7.9) is linear in the simulated expression (or the expectation). In Keane's (1994) version of MSM where conditional probabilities are computed by taking ratios, the estimator is only consistent when the number of draws tends to infinity. Similarly, because a logarithmic transformation is taken, SML is not consistent when H is fixed. Consistency is obtained when H grows at any rate towards infinity (Lee, 1992). Furthermore, a sufficient condition toobtain asymptotically unbiased, asymptotically normal and efficient estimates is N/H  0 as N   (Lee, 1992; Gourie´roux and Monfort, 1993).
It is the reason why some authors prefer MSM to SML. As already said, MSM however requires the computation of the probabilities of all the potential paths with longitudinal data although the less intensive method proposed by Keane (1994) seems to work well in panel probit models (Geweke, Keane and Runkle, 1997). The computation becomes cumbersome when the number of periods is large and there is evidence that small sample biases in MSM are much larger than the simulation bias (Geweke and Keane, 2001). Lee (1995) proposed procedures to correct asymptotic biases though results are far from impressive (Lee, 1997; Magnac, 2000). The GHK simulator is an accurate simulator though it may require a large number of draws to be close to competitors such as Monte Carlo Markov Chains (MCMC) methods (Geweke, Keane and Runkle, 1997). There seems to be a general consensus between authors about the deterioration of all estimators when the amount of serial correlation increases.
Another way to obtain consistent estimators for fixed H is the method of simulated scores (MSS) if the simulator is unbiased. It seems that it is simpler than MSM because it implicitly solves the search for optimal instruments. Hajivassiliou and McFadden (1998) proposes an acceptance­rejection algorithm consisting in rejecting the draw if the condition in (7.10) is not verified. The simulator is not smooth however and as already said a smooth simulator seems to be a guarantee of stability and success for an estimation method. Moreover, in particular when T exceeds four or five, it is possible for some individuals that the acceptance condition is so strong that no draw is accepted. Other methods consist in considering algorithms either based on GHK simulations of the score or on Gibbs resampling. Formulas and an evaluation are given in Hajivassiliou, McFadden and Ruud (1996).4

4 Hajivassiliou and McFadden (1998) first propose to simulate the numerator and the denominator separately. Of course, this method does not lead to unbiased simulation because the ratio is not linear but, still, as simulators are asymptotically unbiased, those MSS estimators are consistent whenever H tends to infinity. The authors furthermore argue that using the same random draws

7 Parametric Binary Choice Models

225

7.2.4.2 Gibbs Sampling and Data Augmentation

It is possible however to avoid maximisation by applying Gibbs sampling techniques
and data augmentation in multiperiod probit models (Geweke, Keane and Runkle,
1997; Chib and Greenberg, 1998; Chib, 2001). Though the original setting of Monte
Carlo Markov Chains (MCMC) is Bayesian, it can be applied to classical settings
as shown by Geweke, Keane and Runkle (1997). The posterior density function of parameter  given the data (Y, X) = {(Yi, Xi); i = 1, ., n} can indeed be used to compute posterior means and variance­covariance matrices to be used as classical
estimators and their variance­covariance matrices. To compute the posterior density p( | Y, X), we rely on two tools. One is the
Metropolis­Hastings algorithm which allows for drawing samples in any (well be-
haved) multivariate density function, the other is Gibbs resampling which allows to
draw in the conditional densities instead of the joint density function.
In the case of panel probit models, it runs as follows. First, let us `augment' the data by introducing the unknown latent variables Yi = Xi +  in order to draw from the posterior density p( ,Y  | Y, X) instead of the original density function. The reason is that it will be much easier to sample into density functions conditional on the missing latent variables. Second, parameter  is decomposed into different blocks (1, ., J) according to the different types of parameters in  or in  the variance­covariance matrix.5
Let us choose some initial values for  , say  (0) and proceed as follows. Draw Y  in the distribution function p(Y  |  (0),Y, X) ­ it is a multivariate truncated normal density function ­ in a very similar way to the GHK simulator. Then draw a new value for the first block 1 in  , i.e., from p(1 | Y , -(01),Y, X) where -(01) is constructed from parameter  (0) by omitting 1(0). Denote this draw 1(1). Do similar steps for all blocks j = 2, . . . , J, using the updated parameters, until a new value  (1) is completed. Details of each step are given in Chib and Greenberg (1998). Repeat the whole step M times ­ M depends on the structure of the problem (Chib, 2001). Trim the beginning of the sample { (0), . . . ,  (m)}, the first 200 observations say. Then, the empirical density function of { (m+1), . . . ,  (M)} is p( | Y, X). Once again, this method is computer intensive with large samples and many dates. It is
however a close competitor to SML and MSS (Geweke and Keane, 2001).

7.2.4.3 Using Marginal Moments and GMM
Instead of working with the joint distribution function, the model defined by (7.8) implies the following moment conditions about the marginal period-by-period distribution functions.6

for the denominator and the numerator decreases the noise. The other method based on Gibbs resampling seems expensive in terms of computations using large samples though it is asymptocally unbiased as soon as H tends to infinity faster than log(N). 5 See Chib and Greenberg (1998) to assess how to do the division into blocks according to the identifying or other restrictions on parameter  or on matrix . 6 The following section heavily draws from Bertschek and Lechner (1998).

226

M. Lechner et al.

E[M(Y, X; 0)|X] = 0, M(Y, X;  ) = [m1(Y1, X;  ), . . . , mt (Yt , X;  ), . . . , mT (YT , X;  )] , mt (Yt , X;  ) = Yt - [1 - F(-Xt  )] .

(7.11)

For the probit model the last expression specialises to mt (Yt , Xt ;  ) = Yt - (Xt  ). Although the conditional moment estimator (CME) based on these marginal moments will be less efficient than full information maximum likelihood (FIML), these moment estimators have the clear advantage that fast and accurate approximation algorithms are available and that they do not depend on the off-diagonal elements of the covariance matrix of the error terms. Thus, these nuisance parameters need not be estimated to obtain consistent estimates of the scaled slope parameters of the latent model. At least, these estimators yields interesting initial conditions and previous methods can be used to increase efficiency.
As in the full information case, there remains the issue of specifying the instrument matrix. First, let us consider a way to use these marginal moments under our current set of assumptions in the asymptotically efficient way. Optimal instruments are given by:
A(Xi, 0) = D(Xi, 0) (Xi, 0)-1 ;

D(Xi,



)

=

E



M(Y, Xi, 



)

|X

=

Xi

;

(Xi,  ) = E[M(Y, Xi,  )M(Y, Xi,  ) ]|X = Xi .

(7.12) (7.13)

For the special case of the probit model under strict exogeneity the two other elements of (7.13) have the following form:

Dit (Xit ; 0) = - (Xit 0)Xit

its(Xit , 0) = [E(Yt - it )(Ys - is)|X = Xi]

=

it (1 - it ) if t = s i(t2s) - it is if t = s

(7.14) (7.15)

where it = (Xit 0) and i(t2s) = (2)(Xit 0, Xis0, ts) denotes the cdf of the bivariate normal distribution with correlation coefficient ts. The estimation of the optimal instruments is cumbersome because they vary with the regressors in a non-
linear way and depend on the correlation coefficients.
There are several different ways to obtain consistent estimates of the optimal in-
struments. Bertschek and Lechner (1998) propose to estimate the conditional matrix
nonparametrically. They focus on the k-nearest neighbour (k-NN) approach to estimate (Xi), because of its simplicity. k-NN averages locally over functions of the data of those observations belonging to the k-nearest neighbours. Under regularity conditions (Newey, 1993), this gives consistent estimates of (Xi) evaluated at ~N and denoted by ~ (Xi) for each observation `i' without the need for estimating ts. Thus, an element of (Xi) is estimated by:

7 Parametric Binary Choice Models

227

N
 ~its(Xi) = wi jtsmt (y jt , Xjt ; ~N )ms(yit , Xit ; ~N ) , j=1

(7.16)

where wi jts represents a weight function. This does not involve an integral over a bivariate distribution. For more details on different variants of the estimator and how to implement it, the reader is referred to Bertschek and Lechner (1998). In their Monte Carlo study it appeared that optimal (nonparametric) Conditional Moment estimators based on moments rescaled to have a homoscedastic variance performed much better in small samples. They are based on:

mWt (Yt , X;  ) =

mt (Yt , Xt ;  )

.

E[mt (Yt , Xt ;  )2|X = Xi]

(7.17)

The expression of the conditional covariance matrix of these moments and the conditional expectation of the first derivatives are somewhat different from the previous ones, but the same general estimation principles can be applied in this case as well.7 Inkman (2000) proposes additional Monte Carlo experiments comparing GMM estimators to SML with and without heteroskedasticity.

7.2.4.4 Other Estimators Based on Suboptimal Instruments
Of course there are many other specifications for the instrument matrix that lead to consistent, although not necessarily efficient, estimators for the slope coefficients. Their implementation as well as their efficiency ranking is discussed in detail in Bertschek and Lechner (1998). For example they show that the pooled probit estimator is asymptotically equivalent to the previous GMM estimator when the instruments are based on (7.13) to (7.16) but the off-diagonal elements of (Xi) are set to zero. Avery, Hansen and Hotz (1983) also suggest to improve the efficiency of the pooled probit by exploiting strict exogeneity in another way by stacking the instrument matrix, so as to exploit that the conditional moment in period t is also uncorrelated with any function of regressors from other periods.
Chamberlain (1980) suggests yet another very simple route to improve the efficiency of the pooled probit estimator when there are arbitrary correlations of the errors over time which avoids setting up a `complicated' GMM estimator. Since cross-section probits give consistent estimates of the coefficients for each period (scaled by the standard deviation of the period error term), the idea is to perform T probits period by period (leading to T × K coefficient estimates) and combine them in a second step using a Minimum Distance estimator. The variance­covariance matrix of estimators at different time periods should be computed to construct efficient estimates at the second step although small sample bias could also be a problem (Altonji Segal, 1996). In the case of homoscedasticity over time this step will be simple GLS, otherwise a nonlinear optimisation in the parametric space is required.8
7 For all details, the reader is referred to Bertschek and Lechner (1998). 8 Lechner (1995) proposes specification tests for this estimator.

228
7.2.5 How to Choose a Random Effects Estimator for an Application

M. Lechner et al.

This section introduced several estimators that are applicable in the case of random effect models under strict exogeneity. In practice the question is what correlation structure to impose and which estimator to use. Concerning the correlation structure, one has to bear in mind that exclusion restrictions are important for nonparametric identification and thus that explanatory variables should be sufficiently variable across time in order to permit the identification of a very general pattern of correlation of errors. For empirical applications of the estimators that we have reviewed, the following issues seem to be important: small sample performance, ease of computation, efficiency, robustness. We will address them in turn.
With respect to small sample performance of GMM estimators, Monte Carlo simulations by Breitung and Lechner (1997), Bertschek and Lechner (1998) and Inkmann (2000) suggest that estimators based on too many overidentifying restrictions (i.e. too many instruments), like the sequential estimators and some of the estimators suggested by Avery, Hansen and Hotz (1983) are subject to the typical weak instruments problem of GMM estimation due to too many instruments. Thus they are not very attractive for applications. The exactly identified estimators appear to work fine.
`Ease of computation' is partly a subjective judgement depending on computing skill and software available. Clearly, pooled probit is the easiest to implement, but random effects ML is available in many software packages as well. Exact ML is clearly not feasible for T larger than 4. For GMM and simulation methods, there is GAUSS code available on the Web (Geweke and Keane (2001) for instance) but they are not part of any commercial software package. The issue of computation time is less important now that it was some time ago (Greene, 2002) and the simulation estimators are getting more and more implementable with the increase of computing power. Asymptotic efficiency is important when samples are large. Clearly, exact ML is the most efficient one and can in principle be almost exactly approximated by the simulation estimators discussed.
With respect to robustness, it is probably most important to consider violations of the assumption that explanatory variables at all periods are exogeneous and restrictions of the autocorrelation structure of the error terms. We will address the issue of exogeneity at the end of this chapter though the general conclusions are very close to the linear case, as far as we know. Concerning the autocorrelation of errors, pooled probit either in its pseudo-ML or GMM version is robust if it uses marginal conditional moments. It is not true for the other ML estimators that rely on the correct specification of the autocorrelation structure. Finally, GMM estimators as they have been proposed here are robust against any autocorrelation. However, they obtain their efficiency gains by exploiting strict exogeneity and may become inconsistent if this assumption does not hold (with the exception of pooled probit, of course).

7 Parametric Binary Choice Models

229

7.2.6 Correlated Effects

In the correlated effects (or unrelated effects) model, we abandon the assumption

that individual effects and explanatory variables are independent. In analogy with

the linear panel data case, Chamberlain (1984) proposes, in a random effect panel

data nonlinear model, to replace the assumption that individual effects ui are inde-

pendent of the regressors by a weaker assumption. This assumption is derived from

writing a linear regression:

ui = Xi + i

(7.18)

where explanatory variables at all periods, Xi, are now independent of the redefined individual effect i. This parametrization is convenient but not totally consistent with the preceding assumptions: considering the individual effect as a function of
the Xi variables makes its definition depend on the length of the panel. However, all results derived in the previous section can readily be applied by replacing explanatory variables Xit by the whole sequence Xi at each period.9
To recover the parameters of interest,  , two procedures can be used. The first method uses minimum distance estimation and the so called -matrix technique of Chamberlain (Cre´pon and Mairesse, 1996). The reduced form:

yit = Xit + i + vit ,

(7.19)

is first estimated. The second step consists in imposing the constraints given by:

t =  +  et

(7.20)

where et is an appropriate known matrix derived from (7.18) and (7.19). The second procedure uses constrained maximum likelihood estimation by im-
posing the previous constraint (7.20) on the parameters of the structural model. The assumption of independence between i and Xi is quite strong in the non-
linear case in stark contrast to the innocuous non-correlation assumption in the linear case. Moreover, it also introduces constraints on the data generating process of Xi if one wants to extend this framework when additional period information comes in Honore´ (2002). Consider that we add a new period T + 1 to the data and rewrite the projection as:
ui = Xi~ + XiT +1~T +1 + ~ i
By substracting both linear regressions at times T and T + 1 and taking expectations conditional on information at period T , it implies that:

E(XiT +1 | Xi) = Xi( - ~)/~T +1

which is not only linear in Xi but also only depend on parameters governing the yit process.

9 The so-called Mundlak (1978) approach is even more specific since individual effects ui are

written

as

a

function

of

averages

of

covariates,

1 T

tT=1 xit

only

and

a

redefined

individual

effect

i.

230

M. Lechner et al.

It is therefore tempting to relax (7.18) and admit that individual effects are a more general function of explanatory variables:

ui = f (Xi) + i

where f (.) is an unknown function satisfying weak restrictions (Newey, 1994). Even if the independence assumption between the individual effect i and explanatory variables Xi is still restrictive ­ because the variance of i is constant for instance ­ this framework is much more general than the previous one. What Newey (1994)
proposes is based on the cross section estimation technique that we already talked
about.
Consider the simple one-factor model where the variance of the individual-andperiod specific shocks is not period-dependent, v2, and where the variance of i is such that v2+2 is normalized to one. We therefore have:

E(yit | Xi) = (Xit  + f (Xi))

where  is the distribution function of a zero-mean unit-variance normal variate. It

translates into:

-1(E(yit | Xi)) = Xit  + f (Xi)

(7.21)

By any differencing operator (Arellano, 2003) and for instance by first differencing, we can eliminate the nuisance function f (Xi) to get:

-1(E(yit | Xi)) - -1(E(yit-1 | Xi)) = (Xit - Xit-1)

(7.22)

The estimation runs as follows. Estimates of E(yit | Xi) at any period are first obtained by series estimation (Newey, 1994) or any other nonparametric method (kernel, local linear, smoothing spline, see Pagan and Ullah, 1998 for instance). A consistent estimate of  is then obtained by using the previous moment condition (7.22).
A few remarks are in order. First, Newey (1994) proposes such a modeling framework in order to show how to derive asymptotic variance­covariance matrices of semi-parametric estimators. As it is outside of the scope of this chapter, the reader is refered, for this topic, to the original paper. It can also be noted that as an estimate of f (Xi) can be obtained, in a second step, by using the equation in levels (7.21). One can then use a random effect approach to estimate the serial correlation of the random vector, vit . Finally, there is a nonparametric version of this method (Chen, 1998) where  is replaced by an unknown function to be estimated, under some identification restrictions.

7.3 Fixed Effects Models Under Strict Exogeneity
In the so-called fixed effect model, the error component structure of Sect. 7.2.2 is assumed. The dependence between individual effects and explanatory variables is now unrestricted in contrast to the independence assumption in the random

7 Parametric Binary Choice Models

231

effects model. In this section, we retain the assumption of strict exogeneity that explanatory variables and period-and-individual shocks are independent. We write the model as:

yit = 1{Xit  + ui + vit > 0}

(7.23)

where additional assumptions are developed below. As the conditional distribution of individual effects ui is unrestricted, the vector
of individual effects should be treated as a nuisance parameter that we should either consistently estimate or that we should eliminate. If we cannot eliminate the fixed effects, asymptotics in T are required in most cases.10 It is because only T observations are available to estimate each individual effect. It cannot be consistent as N   and its inconsistency generically contaminates the estimation of the parameter of interest. It gives rise to the problem of incidental parameters (Lancaster, 2000). The assumption that T is fixed seems to be a reasonable approximation with survey data since the number of periods over which individuals are observed is often small. At the end of the section however, we will see how better large T approximations can be constructed for moderate values of T .
The other route is to difference out the individual effects. It is more difficult in non-linear models than in linear ones because it is not possible to consider linear transforms of the latent variable and to calculate within-type estimators. In other words, it is much harder to find moment conditions and specific likelihood functions that depend on the slope coefficient but do not depend on the fixed effects. In short panels, ML or GMM estimation of fixed effects probit models where the individual effects are treated as parameters to be estimated are severely biased if T is small (Heckman, 1981a).
In the first sub- sections we discuss some methods that appeared in the literature that circumvent this problem and lead to consistent estimators for N   and T is fixed. Of course, there is always a price to pay either in terms of additional assumptions needed or in terms of the statistical properties of these estimators.

7.3.1 The Model

As already said, we consider (7.23) and we stick to the assumption of strict exogeneity of the explanatory variables:

Ft (it |ui, Xi1, . . . , XiT ) = Ft (it |ui) .

(7.24)

Using the error component structure of Sect. 7.2.2, we can reformulate this assumption:

Fvt (vit |ui, Xi1, . . . , XiT ) = Fvt (vit ) .

(7.25)

10 Not in all cases, the example of count data being prominent (Lancaster, 2000).

232

M. Lechner et al.

Note that Ft (it |Xi1, . . . , XiT ) = Ft (it ) and also note that the distribution of the individual effect is unrestricted and can thus be correlated with observables. In most cases we will also impose that the errors are independent conditional on the fixed effect:

T
 F(i1, . . . , iT |ui, Xi1, . . . , XiT ) = Ft (it |ui) t=1
T
 F(vi1, . . . , viT |ui, Xi1, . . . , XiT ) = Fvt (vit ). t=1

(7.26)

There are two obvious difficulties with respect to identification in such a model. First, it is impossible to identify the effects of time-invariant variables.11 It has serious consequences because it implies that choice probabilities in the population are not identified. We cannot compare probabilities for different values of the explanatory variables. In other words, a fixed effect model that does not impose some assumption on distribution of the fixed effects cannot be used to identify causal (treatment) effects. This sometimes overlooked feature limits the use of fixed effects models.12 What remains identified are the conditional treatment effects, conditional on any (unknown) value of the individual effect.
The second difficulty is specific to discrete data. In general, the individuals who stay all over the period of observation in a given state do not provide any information concerning the determination of the parameters. It stems from an identification problem, the so called mover-stayer problem. Consider someone which stays in state 1 from period 1 to T . Let vi be any value of the individual-and-period shocks. Then if the individual effect ui is a coherent value in model (7.23) with staying in the state all the time, then any value u¯i  ui is also coherent with model (7.23). Estimations are thus implemented on the sub-sample of people who move at least once between the two states (`moving' individuals).

7.3.2 The Method of Conditional Likelihood
The existence of biases leads to avoid direct ML estimations when the number of dates is less than ten (Heckman, 1981a). In certain cases, the bias can consist in multiplying by two the value of some parameters (Andersen, 1970; Chamberlain, 1984; Hsiao, 1996). This features makes this estimator pretty unattractive in large N, small T type of applications. If the logit specification is assumed however, it is possible to set up a conditional likelihood function whose maximisation gives consistent estimators of the parameters of interest  , regardless the length of the time period.
11 It is however possible to define restrictions to identify these effects, see Chaps. 4 and 5. 12 The claim that a parametric distributional assumption of individual effects is needed for the identification of causal treatment effects is however overly strong. What is true is that the estimation of the conditional distribution function of individual effects is almost never considered though it can be under much weaker assumptions than parametric ones.

7 Parametric Binary Choice Models

233

Conditional logit: T periods

In the case where random errors, vit , are independent over time and are logistically distributed, the sum yi+ = tT=1 yit , is a sufficient statistic for the fixed effects, in the sense that the distribution of the data given yi+ does not depend on the fixed effect.
Consider the logit model:

P(yit = 1|Xi, ui) = F(Xit  + ui) ,

(7.27)

where

F (z)

=

exp(z) 1+exp(z)

=

1 1+exp(-z)

The idea is to compute probabilities conditional on the number of times the indi-

viduals is in state 1:

T
 Li( ) = P yi1 = i1, . . . , yiT = iT | Xi, ui, yit = yi+ t=1

T

exp  Xit  it

=

t=1

T

 exp  Xit  dt

dBi

t=1

where

T

T

Bi = d = (d1, . . . , dT ) such that dt  {0, 1}and  dt =  yit

t=1

t=1

T
The set Bi differs between individuals according to the value of  yit , i.e., the
t=1
number of visits to state 1. Parameter  is estimated by maximising this conditional log-likelihood function. The estimator is consistent as N  , regardless of T (Andersen, 1970, Chamberlain, 1980, 1984, Hsiao, 1996). Nothing is known about its efficiency as in general conditional likelihood estimators are not efficient. Note that only the `moving' individuals are used in the computation of the conditional likelihood. Extensions of model (7.27) can be considered. For instance, Thomas (2003) develops the case where individual effect are multiplied by a time effect which is to be estimated.
The estimation of such a T -period model is also possible by reducing sequences of T observations into pairs of binary variables. Lee (2002) develop two interesting cases. First, the T periods can be chained sequentially two-by-two and a T = 2 conditional model can be estimated (as in Manski, 1987 see below). All pairs of periods two-by-two could also be considered. These decompositions will have an interest when generalizing conditional logit, when considering semi-parametric methods or more casually, as initial conditions for conditional maximum likelhood. It is why we now review the T = 2 case.

234
7.3.2.1 An Example: The Two Period Static Logit Model

M. Lechner et al.

The conditional log-likelihood based on the logit model with T =2 computed with moving individuals is given by:

  L

=

log
di=1

exp(Xi2 ) exp(Xi1 ) + exp(Xi2 )

+ log
di=0

exp(Xi1 ) exp(Xi1 ) + exp(Xi2 )

,

where for moving individuals, the binary variable di is:

di = 1 if yi1 = 0, yi2 = 1 di = 0 if yi1 = 1, yi2 = 0

Denote Xi = Xi2 - Xi1. The conditional log-likelihood becomes:

  L

=

i|di=1

log

1

exp(Xi ) + exp(Xi

)

+

i|di=0

log

1

+

1 exp(Xi

)

which is the expression of the log-likelihood of the usual logit model:

P(di = 1|Xi) = F(Xi )

(7.28)

adjusted on the sub-sample of moving individuals. Note that the regressors do not include an intercept, since in the original model the intercept was absorbed by the individual effects.

7.3.2.2 A Generalization

The consistency properties of conditional likelihood estimators are well known (Andersen, 1970) and lead to the interesting properties of conditional logit. This method has however been criticized on the ground that assuming a logistic function is a strong distributional assumption. When the errors vi1 and vi2 are independent, it can be shown that the conditional likelihood method is applicable only when the errors are logistic (Magnac (2004)). It is possible however to relax the independence assumption between errors vi1 and vi2 to develop a richer semi-parametric or parametric framework in the case of two periods. As above, pairing observations two-by-two presented by Lee (2002) can be used when the number of periods is larger.
The idea relies on writing the condition that the sum yi1 + yi2 = 1 is a sufficient statistic in the sense that the following conditional probability does not depend on individual effects:

2

2

  P yi1 = 1, yi2 = 0 | Xi, ui, yit = 1 = P yi1 = 1, yi2 = 0 | Xi, yit = 1

t=1

t=1

7 Parametric Binary Choice Models

235

In that case, the development in the previous section can be repeated because the conditional likelihood function depends on parameter  and not on individual effects. It can be shown that we end up with an analog of (7.28) where distribution F(.) is a general function which features and semi-parametric estimation are discussed in Magnac (2004).

7.3.3 Fixed Effects Maximum Score
The methods discussed until Sect. 3.2.2 are very attractive under one key condition, namely that the chosen distributional assumptions for the latent model are correct, otherwise the estimators will be typically inconsistent for the parameters of the model. However, since those functional restrictions are usually chosen for computational convenience instead of a priori plausibility, models that require less stringent assumptions or which are robust to violations of these assumptions, are attractive. Manski (1987) was the first to suggest a consistent estimator for fixed effects models in situations where the other approaches do not work. His estimator is a direct extension of the maximum score estimator for the binary model (Manski, 1975). The idea of this estimator for cross-sectional data is that if the median of the error term conditional on the regressors is zero, then observations with Xi > 0 (resp. < 0) will have P(y = 1 |Xi > 0) > 0 .5 (resp < 0.5). Under some regularity conditions this implies that E {sgn(2yi - 1)sgn(Xi )} is uniquely maximised at the true value (in other words (2yi - 1) and (Xi ) should have the same sign). Therefore, the analogue estimator obtained by substituting expectations by means is consistent although not asymptotically normal and converges at a rate N1/3 to a non-normal distribution (Kim and Pollard, 1990). There is however a smoothed version of this estimator where the sign function is substituted with a kernel type function, which is asymptotically normal and comes arbitrarily close to N-convergence if tuning parameters are suitably chosen (Horowitz, 1992). However, Chamberlain (1992) shows that it is not possible of attaining a rate of N in the framework adopted by these papers.
Using a similar reasoning as in the conditional logit model and using the assumption that the distribution of the errors over time is stationary, Manski (1987) showed that, conditional on X:
P(y2 = 1 |y2 + y1 = 1, X) > 0.5 if (X2 - X1) > 0
Therefore, for a given individual higher values of Xt  are more likely to be associated with yt = 1. In a similar fashion as the cross-sectional maximum score estimator, this suggests the following conditional maximum score estimator:
N
 ^N = arg max sgn(yi2 - yi1)sgn[(Xi2 - Xi1) ]  i=1

236

M. Lechner et al.

For longer panels one can consider all possible pairs of observations over time:

N
^N = arg max   sgn(yis - yit )sgn[(Xis - Xit ) ]  i=1 s<t
The estimator has similar properties than the cross-sectional M-score estimator, in the sense that it is consistent under very weak conditions, but not asymptotically normal and converges at a rate slower than N. Kyriazidou (1995) and Charlier, Melenberg and van Soest (1995) show that the same `smoothing trick' that worked for the cross-sectional M-score estimator also works for the conditional panel version. Hence, depending on the choice of smoothing parameters, the rate of convergence may come arbitrarily close to N.
In practice, there are few applications of this estimator, since many difficulties arise: the solution of the optimisation problem is not unique, and the optimisation can be very complicated, because of the step function involved.
Other semi-parametric methods of estimation include Lee (1999) and Honore´ and Lewbel 2002). In the first paper, an assumption about the dependence between individual effects and explanatory variables allows for the construction of the method of moments estimator which is root-N consistent and asymptotically normal. In the second paper, another partial independence assumption is made as well as assumptions about the large support of one special continuous covariate. By linearizing the model (Lewbel, 2000), one can return to the reassuring world of linear models and difference out the individual effects. The reader is referred to the original papers in both cases.

7.3.4 GMM Estimation

A possible solution to solving the problem posed by the presence of unobservable individual effect is to propose moment conditions which will be approximately satisfied provided that the individual effects are small, and estimators based on such moments (Laisney and Lechner, 2002). Consider the moment condition for any t = 1, . . . , T :
E(yt |Xi, ui) = F(xit  + ui)
When the individual effect is close enough to the value of u~, the first order Taylor approximation around u = u~ is exact, so we can write for any s,t = 1, . . . , T :

U

- u~

=

E [yt

|X, u ] - F(Xt  f (Xt  + u~)

+ u~)

=

E [ys

|X,u ] - F(Xs f (Xs + u~)

+ u~)

Thus, for any s,t = 1, . . . , T ; s = t, the following function,

mts(y, X;  )

=

yt

- F(Xt  f (Xt  -

- u~) u~)

-

ys

- F(Xs f (Xs -

- u~) u~)

7 Parametric Binary Choice Models

237

has a conditional mean of zero at the true value of  , given X = Xi . It can be used as the basis for (almost) consistent estimation of the panel probit model with fixed effects close to u~. Under standard regularity conditions, a GMM estimator of the coefficients for the time varying regressors of the panel model based on these moment functions is consistent (almost, given the Taylor approximation) and N asymptotically normal (Newey, 1993; Newey and McFadden, 1994).

7.3.5 Large-T Approximations
Finally, there are some new developments that are only briefly sketched here and that rely on large-T approximations in parametric binary models. The inspiration comes from Heckman (1981a) pioneering work. Monte Carlo experiments can indeed be used to assess the magnitude of the bias of fixed effect estimators in binary probit or logit models as it was developed in the previous section. This bias due to the presence of incidental parameters is of order O(T -1) in panel probit and for values around T = 10 the bias is found to be small (see also Greene, 2002).
A first direction for improving estimators is to assess and compute the bias either analytically or by using jackknife techniques as proposed by Hahn and Newey (2004). Under assumptions of independence over time of regressors and disturbances, bias-corrected estimators can be easily constructed. Hahn and Kuesteiner (2004) relax the assumption of independence over time by proposing another analytical correction of the bias and that could also apply to the dynamic case (see next section).
The second direction relies on parameter orthogonalization. Inconsistency of fixed effects estimators occurs because the number of useful observations to estimate individual effects is fixed and equal to T and because there is contamination from the inconsistency of individual effect estimates into the parameters of interest. If, as in the Poisson count data example,13 parameters of interest and individual effects can be factored out in the likelihood function (Lancaster, 2003) contamination is absent. Parameters are said to be orthogonal. These cases are not frequent however. The pionnering work of Cox and Reid (1987) uses a weaker notion of information orthogonality. At the true parameter values, the expectation of the cross derivative of the likelihood function w.r.t. the parameter of interest and the nuisance parameters is equal to zero. The invariance of likelihood methods to reparametrizations can then be used. The reparametrization which is interesting to use is the one (if it exists) that leads to information orthogonality. If this reparametrization is performed and if the nuisance parameters are integrated out in Bayesian settings, or concentrated out in classical settings, the bias of the ML estimator is of order 1/T 2 instead of 1/T (in probability). For Probit (or other parametric) models, this method is proposed by Lancaster (2003) in a Bayesian setting. General theory in parametric non linear models in the Bayesian case is developed by Woutersen (2002). In the classical case, the panel static probit model is studied in a Monte Carlo experiment
13 As described in Montalvo (1997) and Blundell, Griffith and Windmeijer (2002).

238

M. Lechner et al.

as an example by Arellano (2003) and in a dynamic case by Carro (2003). They show that for moderate T (4.6), the bias is small. It is smaller than the value for T advocated by Heckman (1981a) though these values shall be theoretically validated in each instance where it is applied, as always when using Monte Carlo experiments about approximations.

7.4 Dynamic Models
In dynamic models where explanatory variables comprise lagged endogenous variables and other predetermined variables, we could further abandon the assumption that individual-and-period shocks and explanatory variables are independent. We distinguish again random and fixed effects models. This section is short not because the subject is unimportant but because the main ideas are extensions of the strict exogeneity case. There is one original issue however that we shall insist on, which is the choice of initial conditions.

7.4.1 Dynamic Random Effects Models

There are many potential sources of dynamics in econometric models. Some sources are easily dealt with in the framework of the last section: coefficients changing over time, lagged values of the strictly exogenous explanatory variables, correlation of random effects over time. There could also be true state dependence that is structural dependence on the lagged dependent variable or feedback effects of dependent variables on explanatory variables. Those explanatory variables are thus predetermined instead of strictly exogeneous. Most behavioral economic models using time-series or panel data are likely to be dynamic in this sense.
There are various dynamic discrete models as introduced by Heckman (1981a). The latent model that we study in this section, is written as:

yit = yit-1 + Xit  + ui + vit

(7.29)

where individual effects ui or individual-and-period specific effects vit are or can be dependent of explanatory variables yit-1 and Xit and/or the future of these variables. It is in this sense that right-hand side variables are endogenous in this section. For
simplicity we here consider one lag only and that vit are independent of the past and present of (yit-1, Xit ).
As an alternative to this model (7.29), there is a class of models in which the lagged latent variable, yit-1, is included among explanatory variables instead of the binary variable yit . This type of dynamics is called habit persistence. Because recursive substitution techniques can be used ­ the lagged latent variable is re-
placed recursively by their expression (7.29) ­ these habit persistence models can

7 Parametric Binary Choice Models

239

be transformed into static models where explanatory variables include lags of the
exogenous variables and where some care should be taken with the initial condition, yi1. These types of models are discussed briefly in Heckman (1981b). Estimation of the structural parameters in the case of binary choice is detailed in Lechner (1993).
Moreover, this framework does not accomodate weak endogeneity which is one of
the focus of this section.

7.4.1.1 Initial Conditions

When the lagged endogenous variable is present, there is an initial condition problem as in the linear case though it is more diffcult to deal with. Assuming for the moment that there are no other explanatory variables,  = 0, the likelihood function is written by conditioning on individual effects as in the previous section:

T
 l(yiT , ., yi2, yi1 | ui) = l(yit | yit-1, ui)l(yi1 | ui) t=2

It is obvious that one needs additional information for deriving l(yi1 | ui) that model (7.29) is not providing. It is analogous to the linear case and the assumptions that initial conditions are exogenous or that initial conditions are obtained by initializing the process in the infinite past were soon seen to be too strong or misplaced. They are generally strongly rejected by the data. Heckman (1981) proposed to use an auxiliary assumption such as:

yi1 =  ui + vi01 .

(7.30)

The complete likelihood function is then obtained by integrating out, ui, as before. Another route was suggested by Wooldridge (2002) or Arellano and Carrasco
(2003). Instead of using the complete joint likelihood function, they resort to the following conditional likelihood function:

T
 l(yiT , ., yi2 | yi1, ui) = l(yit | yit-1, ui) . t=2

When integrating out ui, one now needs to choose the conditional distribution function f (ui | y1) which might be written as the auxiliary model which marries well with the approach of Chamberlain seen above:

ui =  yi1 + i

(7.31)

It should be noted that one loses information and that it is not immediately clear whether restriction (7.30) is more restrictive than (7.31) in particular when other explanatory variables are present in the model.

240
7.4.1.2 Monte Carlo Experiments of Simulation Methods

M. Lechner et al.

In the literature, some papers report Monte Carlo experiments of random effects dynamic models estimated by simulation (Keane, 1994; Chib and Jeliazkov, 2002; Lee, 1997). There seems to be a consensus on a few results. Estimates of the autoregressive parameter seem to be downward biased while parameters of the variance of random effects can be upward or downward biased according to the model (Lee, 1997). Biases increase when serial correlation is stronger though it can be counteracted by increasing the number of draws either for SML or MSM as well as for Gibbs sampling. Biases also increase when the number of periods increases. Misspecification of initial conditions introduces fairly large biases in the estimation.

7.4.1.3 A Projection Method

For treating the weakly endogenous case, there has been an interesting sugges-
tion proposed by Arellano and Carrasco (2003). Let it = (yit-1, Xit ) be the relevant conditional information in period t that is grouped into the information set, it = (it , it-1) where i0 is the empty set. Variables it summarize the relevant past of the process until period t, that is the sequence of lagged endogeneous vari-
ables, explanatory variables and their lags and any other piece of information such
as instruments for instance. Assume that it = ui + vit is such that:

it | it N(E(ui | it ), t2)

where independence between vit and the information set it has been used. Thus, it rules out serial correlation in the usual sense14 while allowing for feedback.

It thus constitutes a generalization of the setting of the projection method of

Chamberlain (1980) and Newey (1994) that we presented in the previous section. The sequence of conditional means E(ui | it ) are related by the moment
conditions:

E(E(ui | it ) | it-1) = E(ui | it-1)

(7.32)

Write now the conditional means:

E(yit | it ) = 

yit-1 + Xit  + E(ui | it ) t

which translates into:

t .-1(E(yit | it )) = yit-1 + Xit  + E(ui | it )

14 Individual-and-period vit-1 is not included in it , only yit-1 is.

7 Parametric Binary Choice Models

241

The moment condition (7.32) is thus:

E(t .-1(E(yit | it )) - (yit-1 + Xit  ) | it-1) = = t-1.-1(E(yit | it-1)) - (yit-2 + Xit-1 )
As before, some nonparametric estimates of E(yit | it ) can be obtained and plugged in this moment condition.
As it is formally identifical to the approach proposed by Newey (1994), the same remarks can be addressed to this approach. There may however be a curse of dimensionality coming in because the dimension of it is growing with the number of periods. Arellano and Carrasco (2003) proposes simplifications and the reader is referred to the original paper.

7.4.2 Dynamic Fixed Effects Models
Chamberlain (1985) extends the conditional logit method to the case where the lagged endogenous variable is the only covariate (see also Magnac, 2000, for multinomial and dynamic models where lags can be larger than 1). Sufficient statistics are now a vector of three variables. On top of the sum of binary variables, the binary variables at the first and last period are added to the list. For instance, in the case where only one lag is used, the smallest number of periods for identification is equal to 4 and the useful information is contained in the intermediate periods from t = 2 to T - 1. The main drawback of this method is that, in the logit case and in the model with one lag, the sum of binary variables, the first and last values of the binary variables are not sufficient statistics if other explanatory variables are present in the model.
If explanatory variables are discrete, the idea proposed by Honore´ and Kyriazidou (2000) is to consider only the observations such that explanatory variables are constant in the intermediate periods from t = 2 to T - 1. Conditional to the values of these explanatory variables, the sum of binary variables, the first and last values of the binary variables are now sufficient statistics. In order to accomodate continuous variables, Honore´ and Kyriazidou (2000) proposes to use observations such that explanatory variables are approximately constant in the intermediate periods from 2 to T - 1. The statistics described above are approximately sufficient. Observations can be weighted according to the degree of such an approximation. Under some conditions the estimator is consistent and asymptotically normal, but due to the nonparametric part, its convergence rate is less than N. Note also that this construction rules out time dummies, which cannot by definition be similar in two periods.

242
References

M. Lechner et al.

Altonji, J.G., and L.M., Segal, 1996, "Small-sample bias in GMM estimation of covariance structures", Journal of Business Economics and Statistics, 14: 353­366.
Andersen, E.B., 1970, "Asymptotic properties of conditional maximum likelihood estimators", Journal of the Royal Statistic Society, Series B, 32: 283­301.
Angrist, J.D., 2001, "Estimation of limited dependent variable models with dummy endogenous regressors: Simple strategies for empirical practice", Journal of Business Economics and Statistics, 19:2­16.
Arellano, M., 2003, "Discrete choice with panel data", Investigaciones Economicas, 27, 423­458. Arellano, M., and R., Carrasco, 2003, "Binary choice panel data Models with Predetermined Vari-
ables", Journal of Econometrics, 115, 125­157. Arellano, M., and B., Honore´, 2001, "Panel data models: Some recent developments", in
J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, North Holland: Amsterdam V(53):3229­3296. Avery, R.B, L.P., Hansen, and V.J., Hotz, 1983, "Multiperiod probit models and orthogonality condition estimation", International Economic Review, 24:21­35. Baltagi, B.H., 2000, Econometric Analysis of Panel Data, Wiley: London. Butler, J., and R., Moffitt, 1982, "A computationally efficient quadrature procedure for the onefactor multinomial probit model ", Econometrica, 50(3): 761­764. Bertschek, I., and M., Lechner, 1998, "Convenient estimators for the panel probit model", Journal of Econometrics, 87: 329­371. Blundell, R., R., Griffith, and F., Windmeijer, 2002, "Individual Effects and Dynamics in Count data Models", Journal of Econometrics, 108: 113­131. Breitung, J., and M., Lechner, 1997, "Some GMM estimation methods and specification tests for nonlinear models", in L. Ma´tya´s and P.Sevestre (Eds.), The Econometrics of Panel Data, 2nd ed., Dordrecht: Kluwer, 583­612, 1996. Carro, J.M., 2003, "Estimating dynamic panel data discrete choice models with fixed effects", Working paper, CEMFI, 0304. Chamberlain, G., 1980, "Analysis of covariance with qualitative data", Review of Economic Studies, 47: 225­238. Chamberlain, G., 1984, "Panel Data", in Z. Griliches and M.D. Intrilligator (Eds.), Handbook of Econometrics, vol II, ch 22, Elsevier Science: Amsterdam, 1248­1318. Chamberlain, G., 1985, "Heterogeneity, omitted variable bias and duration dependence", in Longitudinal Analysis of Labor Market Data, in J.J. Heckman and B. Singer (Eds.), Cambridge UP: Cambridge. Chamberlain, G., 1992, "Binary response models for panel data: Identification and information", Mimeo, Harvard University: Cambridge. Charlier, E., B., Melenberg, and A., van Soest, 1995, "A smoothed maximum score estimator for the binary choice panel model and an application to labour force participation", Statistica Neerlandica, 49: 324­342. Chen, S., 1998, "Root-N consistent estimation of a panel data sample selection model", unpublished manuscript, Hong Kong university. Chib, S., 2001, "Markov Chain Monte Carlo Methods: Computation and Inference", in J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, V(57):3570­3649. Chib, S., and E., Greenberg, 1998, "Analysis of multivariate probit models", Biometrika, 85:347­61. Chib, S., and I., Jeliazkov, 2002, "Semiparametric hierarchical bayes analysis of discrete panel data with state dependence", Washington University, working paper. Cox, D.R., and M., Reid, 1987, "Parameter orthogonality and approximate conditional inference", Journal of the Royal Statistical Society, Series B, 49:1­39.

7 Parametric Binary Choice Models

243

Cre´pon, B., and J., Mairesse, 1996, "The chamberlain approach to panel data: An overview and some simulation experiments", in L. Matyas and P. Sevestre (Eds.), The Econometrics of Panel Data, Kluwer: Amsterdam.
Geweke, J., M., Keane, and D.E., Runkle, 1997, "Statistical inference in the multinomial multiperiod probit model", Journal of Econometrics, 80, 125­165.
Geweke, J.F., and M., Keane, 2001, "Computationally intensive methods for integration in econometrics", in J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, V(56):3465­3568.
Gourie´roux, C., and A., Monfort, 1993, "Simulation-based inference: A survey with special reference to panel data models", Journal of Econometrics, 59: 5­33.
Gourie´roux, C., and A., Monfort, 1996, Simulation-based Econometric Methods, Louvain: CORE Lecture Series.
Gourie´roux, C., A., Monfort, and A., Trognon, 1984, "Pseudo-likelihood methods - Theory", Econometrica, 52: 681­700.
Greene, W., 2002, "The Bias of the fixed effects estimator in non linear models", New York University: New York, unpublished manuscript.
Greene, W., 2003, Econometric Analysis, 5th ed., Prentice Hall: Englewood Cliffs. Guilkey, D.K., and Murphy, J.L., 1993, "Estimation and testing in the random effects probit
model", Journal of Econometrics, 59: 301­317. Hahn, J., and G., Kuersteiner, 2004, "Bias reduction for dynamic nonlinear panel models with fixed
effects", MIT unpublished manuscript. Hahn, J., and W., Newey, 2004, "Jackknife and analytical Bias reduction for nonlinear panel data
models", Econometrica, 72:1295­1319. Hajivassiliou, V., and D., McFadden, 1998, "The method of simulated scores for the estimation of
LDV models ", Econometrica, 66: 863­896. Hajivassiliou, V., D., McFadden, and P., Ruud, 1996, "Simulation of multivariate normal rect-
angle probabilities and their derivatives. Theorical and computational results", Journal of Econometrics, 72: 85­134. Heckman, J.J., 1981a, "The incidental parameters problem and the problem of initial conditions in estimating a discrete time ­ discrete data stochastic process and some Monte-Carlo evidence," in C. Manski and D. McFadden (Eds.), Structural Analysis of Discrete Data, MIT Press, Cambridge, MA, 179­195. Heckman, J.J., 1981b, "Statistical models for discrete panel Data" in C. Manski and D. McFadden (Eds.), Structural Analysis of Discrete Data, MIT Press, Cambridge, MA, 114:178. Heckman, J.J., and B., Singer, 1984, "A method for minimizing the impact of distributional assumptions in econometric models for duration data", Econometrica, 52:271­320. Honore´, B., 2002, "Non-linear models with panel data", WP CEMMAP, 13/02. Honore´, B., and E., Kyriazidou, 2000, "Panel data discrete choice modles with lagged dependent variables", Econometrica, 68:839­874. Honore´, B.E., and A., Lewbel, 2002, "Semiparametric binary choice panel data models without strict exogeneity", Econometrica, 70:2053­2063. Horowitz, J., 1992, "A smoothed maximum score estimator for the binary response model", Econometrica, 60: 505­531. Hsiao, C., 1992, "Logit and Probit Models", in L. Ma´tya´s and P. Sevestre (Eds.), The Econometrics of Panel Data: Handbook of Theory and Applications, Chap. 11: 223­241, Kluwer: Amsterdam. Hsiao, C., 1996, "Logit and probit models", in L. Ma´tya´s and P. Sevestre (Eds.), The Econometrics of Panel Data: Handbook of Theory and Applications, 2nd ed., Chap. 16: 410­428, Kluwer: Amsterdam. Hsiao, C., 2003, Analysis of panal data, 2nd ed., Cambridge University Press, Econometric Society Monographs, 11. Inkman, J., 2000, "Misspecified heteroskedasticity in the panel probit model: A small sample comparison of GMM and SML estimators", Journal of Econometrics, 97: 227­259. Kamionka, T., 1998, "Simulated maximum likelihood estimation in transition models", Econometrics Journal, 1:C129­153.

244

M. Lechner et al.

Keane, M.P., 1994, "A computationally efficient practical simulation estimator for panel data", Econometrica, 62:95­116.
Kim, J., and D., Pollard, 1990, "Cube root asymptotics", Annals of Statistics, 18: 191­219. Kyriazidou, E., 1995, Essays in Estimation and Testing of Econometric Models, Ph.D. dissertation,
Northwestern University. Laisney, F., and M., Lechner, 2002, "Almost consistent estimation of panel probit models with
`Small' fixed effects", Discussion paper no. 2002­15, University of St. Gallen. Lancaster, A., 2000, "The incidental parameter problem since 1948", Journal of Econometrics,
95:391­413. Lancaster, A., 2003, An Introduction to Modern Bayesian Econometrics, Blackwell: Oxford. Lechner, M., 1993, "Estimation of limited dependent variable habit persistence models on panel
data with an application to the dynamics of self-employment in the former east germany", in H. Bunzel, P. Jensen, and N. Westerga°rd-Nielson, (Eds.), Panel Data and Labour Market Dynamics,: North-Holland Amsterdam, 263­283. Lechner, M., 1995, "Some specification tests for probit models estimated on panel data", Journal of Business & Economic Statistics, 13: 475­488, 1995. Lee, L.F., 1992, "On efficiency of methods of simulated moments and maximum simulated likelihood estimation of discrete response models", Econometric Theory, 8:518­552. Lee, L.F., 1995, "Asymptotic bias in simulated maximum likelihood estimation of discrete choice models", Econometric Theory, 11:437­483. Lee, L.F., 1997, "Simulated maximum likelihood estimation of dynamic discrete choice statistical models: Some Monte carlo results", Journal of Econometrics, 82:1­35. Lee, L.F., 2000, "A numerically stable quadrature procedure for the one-factor random component discrete choice model", Journal of Econometrics, 95: 117­129. Lee, M.J., 1999, "A root-n consistent semiparametric estimator for related-effect binary response panel data", Econometrica, 67:427­33. Lee, M.J., 2002, Panel Data Econometrics, Academic Press: New York. Lewbel, A., 2000, "Semiparametric qualitative response model estimation with unknown Heteroskedasticity or instrumental variables", Journal of Econometrics, 97:145­177. McFadden, D., 1989, "A method of simulated moments for estimation od discrete response models without numerical integration", Econometrica, 57: 995­1026. Magnac, T., 2000, "State dependence and unobserved heterogeneity in youth employment histories", The Economic Journal, 110:805­837. Magnac, T. 2004, "Binary Variables and Sufficiency: Generalizing the Conditional Logit", Econometrica, 72:1859­1876. Manski, C.F. 1975, "Maximum Score Estimation of the Stochastic Utility Model", Journal of Econometrics, 3, 205­228. Manski, C.F., 1987, "Semiparametric Analysis of Random Effects Linear Models from Binary Panel Data", Econometrica, 55, 357­362. Montalvo, J.G., 1997, "GMM estimation of count-panel-data models with Fixed Effects and Predetermined Instruments", Journal of Business Economics and Statistics, 15: 82­89. Mroz, T., 1999, "Discrete factor approximation in simultaneous equation models: Estimating the impact of a dummy endogenous variable on continuous outcome", Journal of Econometrics,92: 233­274. Mundlak, Y., 1978, "On the Pooling of Time Series and Cross Section Data", Econometrica, 46: 69­85. Newey, W., 1993, "Efficient estimation of models with conditional moment restrictions", in G.S. Maddala, C. Rao, Vinod, H. (Eds.), Handbook of Statistics, Vol. 11, Ch. 16, NorthHolland: Amsterdam. Newey, W., 1994, "The asymptotic variance of semiparametric estimators", Econometrica, 62:1349­1382. Newey, W.K., and McFadden, D., 1994, "Large sample estimation and hypothesis testing", in R.F. Engle and D.L. McFadden (eds.), Handbook of Econometrics, Vol. 4, 2113­2245, NorthHolland: Amsterdam.

7 Parametric Binary Choice Models

245

Pagan, A., and A., Ullah, 1998, Nonparametric Econometrics, Cambridge UP, Cambridge. Robinson, P.M., 1982, "On the asymptotic properties of estimators of models containing limited
dependent variables", Econometrica, 50:27­41. Sevestre, P., 2002, Econome´trie des donne´es de panel, Dunod: Paris. Thomas, A., 2003, "Consistent estimation of binary-choice panel data models with heterogeneous
linear trends", LEERNA-INRA Toulouse, unpublished manuscript. Train, K., 2002, Discrete Choices with Simulation, Cambridge UP:Cambridge. Wooldridge, J., 2000, Introductory Econometrics, 2nd ed., South-Western College Publishing:
Boston, MA. Wooldridge, J., 2002, "Simple solutions to the initial conditions problem in dynamic non linear
panel data models with unobserved heterogeneity", WP CEMMAP, London, 18/02. Woutersen T., 2002, "Robustness against incidental parameters", Western Ontario, unpublished
manuscript.

Part II
Advanced Topics

Chapter 8
Dynamic Models for Short Panels
Mark N. Harris, La´szlo´ Ma´tya´s and Patrick Sevestre

8.1 Introduction
The empirical analysis of economic behavior often entails specifying dynamic econometric models; that is, models with lagged dependent variable(s) among the regressors. As in time-series context, when the model is dynamic, standard estimation methods based on least squares generally do not lead to estimators having good properties. Indeed, for dynamic panel data models, methods such as OLS or the Within estimators are not consistent. This results from the fact that, due to the unobserved effects, the lagged dependent variable and the disturbance terms are correlated.
Therefore one has to resort to alternative methods. The most commonly used approach is that of GMM, relying on a properly defined set of instrumental variables, or equivalently, a set of orthogonality conditions (see Chap. 4). A large part of this chapter is devoted to the presentation of a number of such consistent estimators. We consider different sets of assumptions about the exogeneity of the regressors (other than the lagged dependent variable), focusing in particular on their possible correlation with the individual effects. We also consider maximum likelihood estimation, although it is less frequently used in practice as its computation is typically much more complex. Finally, besides the estimation of unknown parameters, hypothesis testing is also considered.

Mark N. Harris Department of Econometrics & Business Statistics, Monash University, Victoria 3800, Australia, e-mail: mark.harris@buseco.monash.edu.au
La´szlo´ Ma´tya´s Central European University, Department of Economics, Na´dor u. 9, 1051 Budapest, Hungary, e-mail: matyas@ceu.hu
Patrick Sevestre Universite´ Paris 1 ­ Panthe´on Sorbonne, Ecole Economique de Paris (Paris School of Economics), 106-112 Boulevard de l'Ho^pital, 75013 Paris, France, e-mail: sevestre@univ-paris1.fr

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

249

c Springer-Verlag Berlin Heidelberg 2008

250

M.N. Harris et al.

It is worth noting that, given the typical dimensions of most microeconomic panel data sets, the focus of this chapter is on "short panels", i.e., panels with a large number of observations on economic units (N) over a limited number of time periods (T ).1 Moreover, some guidelines for practitioners are also given about the finite sample behavior of the proposed methods by summing up some available Monte Carlo simulation evidence as well as a few other finite sample results (e.g., Kiviet, 1995).

8.2 The Model

Consider that the economic behavior of interest can be represented by the following

regression model

E (y |y-1, X,  ) = y-1 + X + ,

(8.1)

where y is the variable of interest. It is determined by: its past realisation, y-1; a matrix of observed characteristics, X (typically including a constant term); and an unobserved, or "individual", effect, .2  (a scalar) and  (a vector) are the un-

known coefficients to be estimated. Although X may contain both time-variant and

time-invariant observed characteristics, it will be implicitly assumed throughout this chapter that it contains only the former.3 Note also, for ease of exposition, we as-

sume that the panel is balanced (each economic unit is observed over the same time period).4 Due to the dimensions of typical panel data sets, we assume a random

effects approach throughout. However, we do not rule out the possibility of a corre-

lation between those random individual effects and the regressors, a situation which

is quite close to that associated with a fixed effects approach.

The sample regression function associated with the above model can be written

as

yit = yi,t-1 + xit  + i + it , i = 1, . . . , N; t = 1, . . . , T,

(8.2)

where the i's are i.i.d. (0, 2 ) and the it 's are idiosyncratic error terms, also with mean 0 and variance 2. Indeed, in microeconomic panels, it is convenient (and most often relevant) to treat the cross-sectional observations as independent, identi-

cally distributed draws from the population of interest.

It is clear at this stage that, by definition of the model, the lagged endogenous
variable yi,t-1 is correlated with the random individual effects i. Different sets of assumptions can be made regarding the structure of the correlation of the other regres-
sors (X, the matrix stacked version of xit ) with the individual effects i and the error terms, it . Sticking to the error components framework we will first assume that

1 Other cases encountered in macroeconomics and finance, where the number of time periods is large, are considered in Chap. 9. 2 Although we restrict the exposition to the case of just one lag of the dependent variable, most of the methods can be suitably adapted to the case of additional lags. 3 Time invariant regressors do not cause any further complications per se, but they are eliminated by transformations such as the Within and first-differencing. 4 Again, as long as the attrition is exogenous, all methods can be suitably adapted for unbalanced panels.

8 Dynamic Models for Short Panels

251

E (i |xi1, . . . , xiT ) = 0,

(8.3)

and

E (it |xi1, . . . , xiT , yi,t-1, . . . , yi0, i ) = 0, t = 1, . . . , T,

(8.4)

i.e., X is strictly exogenous. However, one can also consider that in some situations, either the former or the
latter of these two assumptions, or even both, will not hold. Relaxing the former leads to the "correlated effects model" while relaxing the latter can be done by considering the regressors to be only weakly exogenous, or by assuming that they are endogenous. Both of these cases will be considered in this chapter.
A last, but important, remark has to be made at this point. Treating the number of time periods, T , as fixed has two notable implications which depart from what is generally assumed in a time-series context: (1) the stationarity assumption (|| < 1) is not necessary; and (2) the generating process of the initial observations, yi0, is important. Indeed, the correlation of the lagged endogenous variable with the disturbances of the model partly depends on this generating process. In order to highlight this dependence, let us rewrite the model, by successive backward substitution, as

  yit

=

t yi0

+

t-1
 jxi,t- j
j=0

+

1 - t 1-

i

+

t-1
 ji,t- j
j=0

 =

t

yi0

+

t-1 j=0



j xi,t -

j

+

1 - t 1-

i

+

it .

(8.5)

Each observation on the endogenous variable can be written as the sum of four

variables. The first one, t yi0, depends upon the initial values; the second one on

contemporaneous

and

past

values

of

X;

the

third

one,

1-t 1-

i,

is

proportional

to

the

unobserved individual effect; and the last one can be rewritten as an autoregressive

process with fixed initial values

it = i,t-1 + it i0 = 0.

(8.6)

The generating process of the initial observations yi0, i = 1, 2, . . . , N, in particu-

lar their possible correlation with the individual effects i, should not be ignored:

it affects the asymptotic properties of many estimators. Indeed, cov(yi,t-1, i) =

t

cov(yi0

,

i

)

+

1-t 1-

2

,

which

shows

that

this

covariance

clearly

depends

on

the

as-

sumption one makes about the initial observations. One could assume, for example,

that the initial observations yi0 are just fixed constants or, equivalently, that they are independent of both i and it . According to this rather extreme assumption, their

data generating process is completely independent of that of any subsequent obser-

vation yit , for t = 1, . . . , T . However, since the first date of observation of the sample

is often just the result of data availability, there is no real justification for such a

strong assumption. Conversely, one may consider that the process under study is

stationary and that all observations, whatever their date, obey the same process. If

the process is purely autoregressive, this amounts to

252

M.N. Harris et al.

yi0

=

i 1-

+

i0 . 1 - 2

Here one has V (yit |xi1, xi2, . . . , xiT ) = 2 /(1 - )2 + 2/(1 - 2) and cov(yit , i |xi1, xi2, . . . , xiT ) = 2 /(1 - ), t = 0, 1, . . . , T. In the general case where there are exogenous regressors, one can set

yi0

=

xi0  

+

i 1-



+

i0 . 1 - 2

There are many other possible assumptions that can be made here. One that im-

plies a non-specified correlation between the initial observations and the individual

effects is

yi0 = xi0 +  i + i0.

Here, the conditional variance of the initial observations is  22 + 2 and their covariance with the individual effects is  2 .
Correctly specifying the generating process of the initial observations is critical

when one wishes to use maximum likelihood estimation. Indeed, a misspecification

in this respect would lead to inconsistent estimators in most situations (see Sect. 8.5

below as well as Nerlove (2002), or Alvarez and Arellano (2003), for a more detailed

treatment of maximum likelihood estimation of dynamic models).

8.3 The Inconsistency of Traditional Estimators

For the sake of simplifying notation, let us rewrite (8.2) in more compact form, stacking over time and individuals and letting X = [y-1, X] and  = [,  ]
y = X + u, u =  + .

Following Maddala (1971), we know that the usual error components model es-

timators belong to a general class of estimators called the "-class" (see Chap. 3).

The -class estimators are computed using the OLS estimator on the transformed

model







WN +  BN y = WN +  BN X  + WN +  BN u,

(8.7)

where WN and BN are the Within and Between matrix operators, as defined in Chap. 2 and Chap. 3.
For each   [0, ] one obtains estimators  ( ) and  ( ). This class contains all the classical estimators of the error components model such as the Within estimator (if  = 0), the OLS estimator ( = 1), the GLS estimator  =  2 , the Between estimator ( = ), and so on. Almost all the  -class estimators are (asymptotically in N) biased in dynamic models. To shed some light on the structure of these biases let us consider the simple AR(1) dynamic error components model ( = 0)

8 Dynamic Models for Short Panels
yit = yi,t-1 + i + it .

253
(8.8)

As outlined above, this equation can be written as

 yit

=

t yi0

+

1 - t 1-

i

t-1
+  ji,t- j.
j=0

(8.9)

To facilitate the asymptotic calculations we assume that the initial observations yi0 are i.i.d. variables characterized by their second moments E(yi20) and by their correlation with i; E (yi0i). Hence, the asymptotic bias of any  -class estimator depends upon E yi20 and E (yi0i). Indeed, using the strong law of large numbers, the
asymptotic limit of the above estimators,  ( ) = limN  ( ), depends on the
following quantities (Sevestre and Trognon, 1983, 1985)

  1
plim N NT

yi,t-1(i

+ it )

=

1 - T 1-

E(yi0i) +

T

- 1 - T  + T (1 - )2

2

and

 1
plim N N

y¯i,-1(i

+

i)

=

1 - T 1-

E (yi0 i )

+

T

-1-T+ (1 - )2

T

(2

+

1 T

2).

The complexity of the formula does not allow us to provide the full analytical expressions of the biases for all estimators here, but Sevestre and Trognon (1983, 1985) show that  ( ) is an increasing function of  such that

plim  (0) <  < plim   2 < plim  (1) < plim  () .

N

N

N

N

In other words, the Within estimator of the lagged dependent variable's coefficient
under-estimates its true value (Nickell, 1981), whilst the Between and OLS estima-
tors over-estimate it. One can show that this ranking is inverted for the coefficients
of the X variables as long as these variables are positively autocorrelated (and that
these coefficients are positive); the opposite being true when any one of these con-
ditions is reversed. One particular exception to this ranking is worth noting: when
the initial observations are assumed to be fixed constants, or to be independent of the individual effects and the it 's, the GLS estimator is consistent (for N   but fixed T ), as is the Feasible-GLS one (on the assumption that the variances 2 and 2 have been consistently estimated). To get an idea of the biases of the OLS and Within estimators, consider, for example, a purely autoregressive model, such as (8.8), with T = 10,  = 2 / 2 + 2 = 0.5 and  = 0.9. For the Within estimator, we get

plim  (0) -  = -0.080 if yi0 = i + i0,
N

254

M.N. Harris et al.

and

plim
N

 (0) -



=

-0.243

if

yi0

=

i 1-

+

while for the OLS estimator we have

i0 , 1 - 2

plim  (1) -  = 0.176 and plim  (1) -  = 0.095

N

N

for, respectively, the same two assumptions concerning yi0. Clearly, the magnitude of the bias of the usual panel data estimators strongly depends on the assumptions concerning the initial observations.
However, although biased in finite T samples, the Within estimator tends to have a relatively small variance, especially when compared to consistent GMM-type estimators. In Monte Carlo studies the Within estimator appears to remain within a very tight, albeit biased, range: especially when compared to the often volatile performance of consistent estimators. Kiviet (1995) makes use of this fact combined with an approximation of the (small sample) bias of this estimator. What Kiviet (1995) suggests is to use the Within estimator, but to subtract off the approximation of its bias. In theory this yields an unbiased, or at least less biased, estimator with a very robust performance. In Monte Carlo studies, this estimator does, indeed, perform well. In practice, the drawbacks of such a procedure are twofold: the standard errors of the bias adjusted estimator are extremely complicated; and the bias adjustment term is a function of the true parameters in the model which are unknown (Kiviet (1995) suggests using an initial consistent estimator, although this conflicts with the idea of staying away from potentially volatile GMM-type estimators).
There is another simple alternative. From the above ranking of the  -class estimators, it is clear that there exists a value    [0,  2] such that limN ( ) = . Sevestre and Trognon (1983) have shown that

 =

Q (1 - )

,

1-T 1-

E (yi0 i ) 2

+ Q (1 - 

+T)

(8.10)

with Q = (T - 1 - T  + T )/T (1 - )2,  = 2 /(2 + 2).
The simple method of firstly estimating   by a consistent estimator  , then  by    and  by    leads to a consistent two-step estimator. Unfortunately, its finite and asymptotic distributions heavily depend on the distribution of   : if the initial estimates of the parameters involved in   are poor, the resulting   
exhibits rather unsatisfactory behavior (Sevestre and Trognon, 1990).5

5 In general   =  2, which confirms the inconsistency of the GLS estimator in such a model. Nevertheless, if E (yi0i) = 0, then   =  2 and this shows again that, in this particular situation, the GLS estimator is consistent.

8 Dynamic Models for Short Panels

255

8.4 IV and GMM Estimators

The most commonly used approach to estimating linear dynamic panel data models is undoubtedly the GMM estimator. In a general setting, recall that this method yields consistent estimators of models such as

y = X + u,

where X is used to denote that some, or all, of the regressors are correlated with the disturbances: E (u |X ) = 0; plim(X u/NT ) = 0; and where these disturbances may exhibit some serial correlation or heteroskedasticity; V (u |X ) =  2 = .6
Then, assuming the existence of a set of instrumental variables, Z, satisfying in par-
ticular the absence of any (asymptotic) correlation with u and a non-null (asymptotic) correlation with the regressors, X, it is well known that the following instru-
mental variables estimator

IV = X  Z Z Z -1 Z X  -1 X  Z Z Z -1 Z y

(8.11)

= X  PZX  -1 X  PZy ,

with PZ = Z (Z Z)-1 Z , is consistent and has an asymptotic distribution  N ( - )  N (0,V ()) ,

where

V () = plim X  PZX  -1 N N

× plim X  PZPZX  ×

plim X  PZX 

-1
.

N

N

N N

However, this is only efficient when the disturbances are i.i.d. (i.e., if  = I). If they are heteroskedastic and/or serially correlated, one can improve the efficiency by resorting to the linear GMM estimator

 = X  Z Z Z -1 Z X  -1 X  Z Z Z -1 Z y,

(8.12)

assuming the form of  is known (which can happen in some cases; see Sect. 4.2.1 below).  is consistent and has an asymptotic distribution given by
 N ( - )  N (0,V ())

with
6 See Chap. 4.

V () =

X  Z(Z Z)-1Z X  plim

-1
.

N

N

256

M.N. Harris et al.

If  is unknown, it is possible to estimate the quantity (Z Z) required to construct the linear GMM estimator of (8.12) as

 Z

Z =

1N

N

Zi
n=1

uiui

Zi

where ui is a vector of residuals obtained from a first step consistent estimator. Note that all elements of X that are exogenous with respect to u, implicitly
enter Z as their own instruments. Now, the main question is to find instruments,
Z, that satisfy the above conditions given the assumptions about the disturbances
including their possible correlations with the regressors. Similarly, in the discussions that follow below, any elements of X (or X) that are exogenous with respect to  and  (or )­that is the relevant error term(s) in the model­are (implicitly, or explicitly) contained as their own instruments in Z.

8.4.1 Uncorrelated Individual Effects: The Original Balestra­Nerlove Estimator and its Extensions

As outlined in the previous sections, problems arise in estimating an autoregres-

sive (or dynamic) panel data model with unobserved effects, due to the correlation

between the lagged dependent variable and these unobserved effects, i. In their

seminal paper, Balestra and Nerlove (1966) proposed an instrumental variables ap-

proach to estimate such a model, relying on the assumption that X (or a subset of

X) is independent of both i and it .7 Then these variables, and their lags, can be

used as valid instruments. Following the above notation, the estimator suggested by

Balestra and Nerlove (1966) is an instrumental variables estimator with the follow-

ing set of instruments

 xi0

Zi

=



xi1 ...
xi,T -2

xi1 

xi2 ...
xi,T -1



xi,T -1 xiT

where xit stands for the subset of X (possibly X itself) which contains the strictly exogenous regressors and xi,t-1 their one-period lagged value.8 In other words, in a
GMM framework, the model is estimated using only the following 2 × K orthogonality conditions,9 where K is the number of strictly exogenous regressors

7 In fact, they assume E(Xi,t- it ) = 0,   0. 8 We assume that observations on y and X are available from t = 0 to T.
9 In fact, one could easily go to 2 × K × T instruments by considering 2 × K separate orthogonality conditions for each time period.

8 Dynamic Models for Short Panels

257

E xit uit = 0
and E xi,t-1uit = 0,
recalling that uit = i + it . One of the major drawbacks of this estimator, as of other instrumental variables
estimators, is its frequent empirical imprecision, even in large samples. There are several solutions to this problem. One is to increase the number of orthogonality conditions along the lines suggested by Hausman and Taylor (1981), Amemiya and MaCurdy (1986) or Breusch, Mizon and Schmidt (1989).10 Indeed, assuming as in Balestra and Nerlove (1966), that at least some of the regressors are both uncorrelated with the individual effects and strictly exogenous, any of the past, present or future values of these regressors (or combinations of them, such as their individual means and/or their difference from the individual means) can be used as instruments. As an example, the following set of K × T 2 orthogonality conditions would be valid

E xi uit = 0, t = 1, . . . , T ;  = 1, . . . , T,

with K being again, the number of strictly exogenous regressors. In the case where the regressors would still be uncorrelated with the individual effects but only weakly exogenous, then only their past values should be used as instruments.

8.4.2 Correlated Individual Effects

8.4.2.1 Model in First Differences ­ Instruments in Levels/First Differences

A commonly used approach to the estimation of dynamic panel data models where some (or all) elements of X are possibly correlated with the individual effects, is to write the model in first differences so that the individuals effects are, being time invariant, discarded

yit = yi,t-1 + xit  + uit

(8.13)

= yi,t-1 + xit  + it .
Using previous notation one can rewrite this model in matrix form as
y = X + u = X + 

(8.14)

where the lagged endogenous variable, and possibly some other regressors, are correlated with the disturbances, both in finite samples (E (u |X ) = 0) and

10 Those estimators belong to the Generalized Instrumental Variables class of estimators where the instrumental variables principle is applied to the transformed model -1/2y = -1/2X + -1/2u.

258

M.N. Harris et al.

asymptotically (plim (X u/NT ) = 0) and where, because of the assumed error components structure of the disturbances,  follows an MA(1) process, that is11

V (u |X ) = V ( |X ) = 2

(8.15)

with





2 -1 0 . . . 0



=

IN





-1
0 0

2 ... ...

-1 ... -1

0 ... 2

...
0 -1



.

0 . . . 0 -1 2

Applying OLS or any other basic estimator (such as the Within or Between estima-
tors) to this model, however, does not yield consistent estimators of the parameters, given the correlation between yi,t-1 and i,t-1. Then, one can use instrumental variables, or GMM, in this context too.

Balestra­Nerlove on First Differences
Let us first assume that the only cause of any potential endogeneity of X is through its correlation with the individual effects; that is, conditionally on the individual effects, X (or a subset, X thereof) is strictly exogenous. Then, these regressors or their first differences, are valid instruments for the model written in first differences. In other words, one can estimate this model using the orthogonality conditions:
E (xit uit ) = 0 and/or E(xit uit ) = 0 and E(xi,t-1uit ) = 0 and/or E(xi,t-1uit ) = 0.
Assuming that the variables in X are only weakly exogenous, only their past values up to t - 2 at most can be used as valid instruments
E (xi,t-2uit ) = 0 and/or E(xi,t-2uit ) = 0 and E(xi,t-3uit = 0 and/or E(xi,t-3uit ) = 0.
As already mentioned, the small number (2K) of these orthogonality conditions limits the efficiency of the resulting estimators. Increasing asymptotic efficiency can be achieved by either taking into account the serial correlation of the disturbances induced by the first differencing operation and/or by using the whole sequence of xit , t = 0, . . . , T as instruments, as long as they are strictly exogenous.

11 Although more general error structure in terms of heteroskedasticity and/or serial correlation can also be allowed for (see below).

8 Dynamic Models for Short Panels

259

A further option was pursued by Anderson and Hsiao (1982) to get additional instruments/orthogonality conditions.

Anderson and Hsiao (1982) Estimators
As mentioned above, first differencing of the model leads to disturbances that follow an MA(1) process assuming that the original ones were i.i.d. So yi,t-2 as well as yi,t-2 (= yi,t-2 - yi,t-3), are correlated with yi,t-1 (= yi,t-2 - yi,t-3) but not with uit (= it = it - i,t-1), which is the disturbance of the transformed model, so they are valid instruments. In addition to yi,t-2 and yi,t-2 one can still use X and/or its lags, depending on whether these variables are strictly exogenous with respect to  or not. In terms of orthogonality conditions, one can write, in the former case
E (yi,t-2uit ) = 0 and/or E (yi,t-2uit ) = 0 and E (xit uit ) = 0 and/or E (xit uit ) = 0
or, alternatively, for the latter case
E(yi,t-2uit ) = 0 and/or E(yi,t-2uit ) = 0 and E(xi,t-2uit ) = 0 and/or E(xi,t-2uit ) = 0.
The interest of these estimators is twofold. First, their implementation is very simple. Second, they do not necessarily require X to be uncorrelated with the individual effects.
However, in some instances, use of the instrument yi,t-2 yields inefficient estimators (Arellano, 1989), suggesting that yi,t-2 is preferable. Indeed, Anderson and Hsiao (1982) estimators in practice often appear to lead to rather erratic parameter estimates due to their low efficiency (see, for example, Sevestre, 1984 and Arellano, 1989). Several reasons may explain this. First, due to the first differencing, this method makes use of only the time variability of the observations which, at least for micro datasets, is often much less important than the between-individuals variability. Also, linked to the first differencing of the data there is the problem of low correlation between the instruments and the regressors (the so-called "weak instruments" problem). Second, due to the lags involved, these estimators effectively require the first, second and third time periods respectively to be removed, when yi,t-2 and yi,t-2 are used as instruments. Given the often limited time dimension of the panel, this reduces significantly the number of observations available for estimation.
Finally, as already mentioned, another reason for the often poor performance of these estimators, is the low number of instruments used. Indeed, increasing the number of instruments (in a reasonable way, see below) is known to be favourable to the behavior of the estimators. Moreover, it must also be mentioned that these estimators do not explicitly take into account the serial correlation of the disturbances which follow an MA (1) process after the model has been first differenced, although it can be suitably adapted along the lines suggested above (and also below with regard to the Arellano and Bond (1991) estimator).

260
Arellano and Bond (1991) Estimator

M.N. Harris et al.

Arellano and Bond (1991) proposed an estimator aimed at tackling the problems associated with the low number of instruments/orthogonality conditions and the serial correlation in the disturbances of the first differenced model.
On the first point, Arellano and Bond (1991) show that there exist many more instruments than those put forward by Anderson and Hsiao (1982). As an example, consider the case of a panel with five periods of observations, t = 0, 1, . . . , 4. For each time period, the model can be written as

for t = 2: yi2 - yi1 = (yi1 - yi0) + (xi2 - xi1)  + i2 - i1 for t = 3: yi3 - yi2 = (yi2 - yi1) + (xi3 - xi2)  + i3 - i2 and for t = 4: yi4 - yi3 = (yi3 - yi2) + (xi4 - xi3)  + i4 - i3.

In period t = 2 the variable yi0 is a valid instrument since it is obviously correlated with yi1 - yi0 but not with i2 - i1 (as long as the it 's are serially uncorrelated). Indeed, when t = 2, yi0 is nothing more than the instrument yi,t-2 proposed by Anderson and Hsiao (1982). When t = 3, the instrument proposed by Anderson and
Hsiao (1982) is yi1. However, yi0 is also a valid instrument here since, given the autoregressive nature of the model, it is correlated with yi2 - yi1 while, given the assumption of no serial correlation of the 's, it is not correlated with i3 - i2. This provides two instruments for estimating the model at time t = 3. Along the same
lines, when t = 4, the variables yi0, yi1 and yi2 are all valid instruments. The full set of instruments is given by
Zi = (Zi(y), Zi(x))

where



yi0 0 . . . . . . 0 . . .

Zi(y) = 

0 0

yi0 yi1 ...

0 ...

0 ... ... . . .



0

0 0



0 . . . 0 0 yi0 . . . yiT -2

(8.16)

and where the set of instruments Zi(x) is defined according to the assumptions made about the exogeneity of X.12 Although it is rather common in practice to set Zi(x) = X, one may augment the number of instruments based on X. For example, assuming that, conditionally on the individual effects, X is strictly exogenous (i.e.,
X = X), one can use



Zi(x)

=



xi0 0
0

... ... ...

xiT 0

0 xi0

... ... ...

0
xiT 0

0 ... 0 ... ...



0

0 0



(8.17)

0 ... ...

. . . 0 xi0 . . . xiT

12 Along the same lines one may easily redefine the correct set of X variables that can be used as instruments depending on their correlation with  and .

8 Dynamic Models for Short Panels

261

whilst where they are only weakly exogenous, one could use



Zi(x)

=



xi0 0
0

0
xi0 ...

... xi1

0 0

... ... ...

0 0 0

0 0 ...

... ...



0

0 0

 .

0 ...

. . . 0 xi1 . . . xiT -2

The associated orthogonality conditions can be written as

E(yi,t- it ) = 0, t = 2, . . . , T ;   2

and, depending on the assumptions regarding the exogeneity of X

(8.18)

E(xi it ) = 0, t = 1, . . . , T ;  = 1, . . . , T

or E(xi,t- it ) = 0, t = 2, . . . , T ;   2.

Moreover, in order to further increase efficiency, Arellano and Bond (1991) propose

to account for the serial correlation of the disturbances in the first-differenced model.

Indeed, one can redefine the previously defined GMM estimator, equation (8.12), except that y and X are replaced by their first differences and  by





2 -1 0 . . . 0



=

IN





-1
0 0

2 ... ...

-1 ... -1

0 ... 2

...
0 -1



,

(8.19)

0 . . . 0 -1 2

such that the GMM estimator writes as  = (X  Z Z Z -1 Z X )-1X  Z Z Z -1 Z y.

(8.20)

They also suggested a variant of this estimator which is robust to heteroskedasticity.
Let us denote ui = (ui2, ui3, . . . , uiT ) , the vector of residuals obtained from using one of the previously presented consistent estimators­typically using (8.20) ­ and  the unspecified variance­covariance matrix of the disturbances u. Then, one can consistently estimate Z Z by

 Z

Z =

1 N

N
Zi
i=1

uiui

Zi.

The "robust" linear GMM estimator is then

r = X  Z(Z Z)-1Z X  -1

× X Z(Z Z)-1Z y.

(8.21)

262

M.N. Harris et al.

There is evidence that the standard errors of the robust two-step GMM variant of this estimator are unreliable unless N is very large. Windmeijer (2005) however, suggests a method for correcting this small-sample bias (see Sect. 8.6).
Unfortunately, as already mentioned regarding the Anderson and Hsiao (1982) estimators, those suggested by Arellano and Bond (1991) also often suffer from a lack of precision due to the first differencing of the model and the lack of correlation of the instruments with the regressors. This may sometimes result in rather erratic parameter estimates. Some alternatives do exist though, to circumvent this problem.

8.4.2.2 Model in Levels ­ Instruments in First Differences

As already mentioned in Chap. 4, there are several ways to tackle the problem of unobserved effects being correlated with explanatory variables. As sketched out above, the most common one, the Within transformation, does not work in the context of dynamic models, nor does OLS on a first differenced model. Indeed, first differencing creates MA (1) disturbances for the transformed model, thus inducing a correlation with the lagged dependent variable.
However, assuming that the covariance between the regressors X and the individual effects is constant over time, the first differences of those regressors are uncorrelated with the unobserved effects. This makes them valid instruments as long as they are also exogenous with respect to it . Indeed, assuming that

cov(xit , i) = i , t

and that, conditionally on the unobserved effects, X is strictly exogenous, one can

make use of the following instruments



Zi(x)

=



xi1 0
0

... ... ...

xiT 0

0 xi1

... ... ...

0
xiT 0

0 0 ...

... ...



0

0 0



(8.22)

0 ...

. . . 0 xi1 . . . xiT

while, if they are only weakly exogenous, we can use



xi2 0 0 . . . 0 0

Zi(x) = 

0 ...

xi2 xi3 0

...

... ...

...

0 . . . 0 0 xi2 · · ·

 0

0

 .

xi,T -1

This corresponds to the following orthogonality conditions

E(xi uit ) = 0,  = 1, . . . , T ; t = 1, . . . , T if the regressors are strictly exogenous, and

8 Dynamic Models for Short Panels

263

E(xi,t- juit ) = 0, j = 1, . . . , t - 1, t = 2, . . . , T
if those regressors are only weakly exogenous. Moreover, under the assumption that the covariance between the lagged endoge-
nous variable and the individual effects is also constant over t (which amounts to assuming stationarity of yit ), lagged values of the first difference of that lagged endogenous variable are also valid instruments as they are in this case uncorrelated with the disturbances.13
Indeed, as an example, let us consider again the simple case of a panel with five periods of observations (t = 0, 1, . . . , 4). The model can be written as

for t = 2 : yi2 = yi1 + xi2 + i + i2 for t = 3 : yi3 = yi2 + xi3 + i + i3 and for t = 4 : yi4 = yi3 + xi4 + i + i4.

In period t = 2 the variable yi1 - yi0 is a valid instrument since it is obviously cor-
related with yi1 but not with i2 as long as the it 's are serially uncorrelated and
that cov(yi1 - yi0, i) = 0 as long as the stationarity assumption is fulfilled since in that case, cov(yi1, i) = cov(yi0, i) = 2 /(1 - ). Along the same lines, when t = 3, yi2 - yi1 is a valid instrument while yi1 - yi0 also remains valid. This provides
two instruments for estimating the model at time t = 3.With t = 4 the variables

yi1 - yi0, yi2 - yi1 and yi3 - yi2 are now all valid instruments, such that one could also consider the following set of instruments



yi1 0 0 . . . 0 0

Ziy = 

0 ...

yi1 yi2 0 ... ... ...

...

 0

0

 .

0 . . . 0 0 yi1 · · · yi,T -1

That is

E(yi,t-1uit ) = 0, t = 2, . . . , T.

8.4.2.3 Other Transformations for Models with Correlated Individual Effects
Arellano and Bover Estimator
A drawback associated with first differencing of the model is the induced serial correlation in the disturbances of the transformed model. This is why Arellano and Bover (1995) suggested a transformation of the model that discards the individual effects without inducing any serial correlation in the remaining disturbances. This transformation, called forward orthogonal deviation, is defined by the (T - 1) × T matrix
13 This argument closely follows that of Blundell and Bond (1998) discussed below.

264

M.N. Harris et al.

H = diag[(T - 1)/T, (T - 2)/(T - 1), (T - 3)/(T - 2), . . . , 1/2] × H+

with





1 1/(1 - T ) 1/(1 - T ) · · · 1/(1 - T ) 1/(1 - T ) 1/(1 - T )

H

+

=



0 ...
0

1 0

1/(2 - T ) ...
0

··· ···

1/(2 - T ) ...
1

1/(2 - T ) -1/2

1/(2 - T ...
-1/2

)



.

0

0

0

···

0

1

-1

Applying this transformation to the vector ui of the disturbances (uit = i + it ) for the ith individual will result in a transformed vector ui of size (T - 1) defined as
ui = Hui

with typical elements

uit

=

dt

× (uit

-

T

1 -t

(ui,t+1

+ ui,t+2

+ . . . + uiT ))

=

dt

×

(it

-

T

1 -

t

(i,t+1

+

i,t+2

+

.

.

.

+

iT

))

where dt = (T - t)/(T - t + 1). The interesting result here is that we have V (ui ) =  2IT-1, i.e., the transformed
disturbance vector does not include the individual effects anymore and is, moreover, serially uncorrelated. So, instrumental variable estimators of the H transformed

model will not be subject to any loss of efficiency because of ignored serial correlation. In particular, any set of predetermined variables (be they lagged values of y or

X) will constitute valid instruments. Explicitly, the estimator suggested by Arellano and Bover (1995) first involves

transforming the system of T equations using the nonsingular transformation given

by Hi

Hi =

H eT /T

(8.23)

where eT is a (column) vector of ones of size T . H can be defined as above, or indeed by any (T - 1) × T matrix of rank (T - 1) such that HeT = 0 : H for example, could also be the first (T - 1) rows of the Within group operator or the first

difference operator, although interestingly, the estimator is invariant to the specific choice of H. As the first (T - 1) transformed errors

u+i = Hiui =

H  ui ui

,

(8.24)

are free of i, all exogenous variables are valid instruments for these first equations. Moreover, assuming serial independence of the disturbance terms it , along the lines of the Arellano and Bond (1991) estimator, the series (yi0, yi1, . . . , yi,t-1) is also a

8 Dynamic Models for Short Panels

265

valid instrument. This assumption however, requires more structure for H, which

now additionally has to be upper triangular (Arellano and Bover, 1995). This defines

the matrix of valid instruments to be





ZiABov =  (Xi , yi0) (Xi , yi0, yi1) . . . (Xi , yi0, . . . , yi,T -1) 0  ,

0

Xi

(8.25)

where Xi = (xi1, . . . , xiT ) as long as X is strictly exogenous. Stacking the Zi's and letting H = IN  Hi, the estimator is obtained by estimating the transformed model

Z Hy = Z HX +Z Hu

(8.26)

by GLS using V (Z Hu) = Z H H Z. As with previous estimators, the covariance of the transformed system H =
H H must be estimated from residuals obtained from a preliminary consistent estimator of Hiui = u+i . One option is Arellano and Bover (1995)

 H

=

1 N

i

ui+ui+ ,

(8.27)

which is an unrestricted estimator of H . The restricted estimator under the usual assumptions of the error components model is

H = HH

(8.28)

where  = IN  2 JT + 2IT and 2 and 2 are consistent estimators of 2 and 2.

8.4.2.4 Wansbeek­Bekker Estimator

The approach suggested by Wansbeek and Bekker (1996) extends those of Anderson and Hsiao (1982), Arellano (1989) and Arellano and Bond (1991), for example, such that now both lags and leads (and linear combinations) of the dependent variable are included in the instrument set. By defining the variable y from period t = 1 to t = T , the estimator considers linear functions of y+ as instruments, where y+ is the stacked vector of observations defined from t = 0 to t = T for each individual. The linear functions are defined by a ((T + 1) × T ) matrix Ai, which yields Ai y+ as the full instrument set. Important restrictions are imposed on Ai such that

AieT = 0 and E y+Ai = traceAiE iy+ = 0

(8.29)

which respectively ensure elimination of the individual effects and consistency of the estimator (i is the T × 1 vector of disturbances it for individual i).

266

M.N. Harris et al.

These conditions for Ai define its structure such that its rows must sum to zero, as must each of its lowest T quasi-diagonal elements (in particular, the lower left element is zero). Ai is unspecified apart from the restrictions of (8.29); the estimator's variance is a function of Ai. The optimal value of Ai can be found by constrained optimisation such that the estimator's (scalar) variance is minimised whilst ensuring that the appropriate restrictions hold (Wansbeek and Bekker, 1996). However, this is done under the simplifying assumption that  = 0 although the estimator can be "operationalised" to allow for exogenous variables as well (Harris and Ma´tya´s, 2000). In this instance, the full operational instrument set is defined as

ZWB = A y+, X .

(8.30)

With known Ai and hence known Z, the (operational) estimator is a straightforward application of the linear GMM estimator, using u2 (Z Z) as an approximation to the
variance of Z u. The asymptotic variance is given by

u2

1 plim N N

X  PZ X  -1

,

(8.31)

where PZ = Z (Z Z)-1 Z , which, from (8.30) is a function of Ai. The optimal choice of Ai is that which minimises (8.31), such that Ai conforms with its appropriate restrictions. If one is only interested in the variance of the parameter vector (and
not covariances of particular elements of it), the optimal estimator can be obtained
by constrained optimisation, where Ai is that which minimises the trace of (8.31), treating u2 as a constant, subject to the restrictions given by (8.29).
The list of valid instruments can also be expanded to include not only A y+, but also A X+ for example, such that

ZW B+ = A y+, A X+, X .

(8.32)

Again, the instrument set for these estimators can suitably be adopted for different assumptions regarding the exogeneity of the elements of X. Despite some evidence on the good performance of these estimators (Wansbeek and Bekker, 1996; Harris and Ma´tya´s, 2000, 2004), they are not yet commonly used in practice.

8.4.2.5 Combining Levels and Differences: The Blundell and Bond System Estimator
Over the last decade the practice of estimating dynamic panel data models has mainly consisted of using the GMM estimator suggested by Arellano and Bond (1991) i.e., first differencing the model and using lagged levels of the endogenous explanatory variable as instruments. From an empirical point of view though, the resulting parameter estimates were often unsatisfactory: imprecise and not very robust (for example, to a slight change in the instrument set). Blundell and Bond (1998) have shown that one likely explanation for this is the lack of correlation between the instruments (considered to be lagged values of the endogenous variable (e.g., yi,t-2

8 Dynamic Models for Short Panels

267

and so on), and the regressors in the model once it has been first differenced. This is the now well-known weak instruments problem.
Assume, for the sake of simplicity, that the sample consists of three periods (t = 0, 1, 2), and that the model is a purely autoregressive one. The only valid instrument is yi,t-2 and the appropriate orthogonality condition is
E [yi0 (yi2 - yi1)] = 0.
The question arises of the correlation between the regressor yi1 and the instrument yi0. Given the specification of the model, one can write
yi1 = yi0 + i + i1
and
yi1 = ( - 1) yi0 + i + i1 = yi0 + i + i1.

Blundell and Bond (1998) show that

plim
N



=

( - 1)

q (2 /2) + q

with

q

=

(1 - )2 1 - 2

.

In other words, when   1, or when the ratio 2 /2  , the correlation between the instrument and the regressor tends to 0, which induces the erratic

behavior of the estimator. Blundell and Bond (1998) therefore suggest adding

supplementary orthogonality conditions, based on the additional assumption of

"quasi-stationarity" of yit . This assumption amounts to considering that the initial observations yi0 are generated according to

yi0

=

xi0

+

i 1-



+

i0.

Indeed, under this assumption, one can write

E (yi1i2) = 0

and, with a larger number of periods,

E (yi,t-1it ) = 0, t = 2, 3, . . . , T.

One can then instrument the model in levels by lagged first differences of yit , following the suggestion made by Arellano and Bover (1995) for models with correlated individual effects. Blundell and Bond (1998) then suggest a "system GMM" estimator that consists of stacking the Arellano and Bond (1991) orthogonality conditions with the above ones. In other words, one stacks the model "in levels" and that in "first differences"

y y

=  y-1 y-1

+

X X

+

  +

,

268

M.N. Harris et al.

and estimate this system using GMM with the following set of instruments Z = Z and Zl, where the former correspond to the instruments for the model in first
differences, and the latter to those associated with the model in levels, such that





yi2 0 . . . 0

Zil = 

0 ...

yi3 ...

... 0



0 . . . 0 yi,T -1

and in full

 Zi 0 0 . . . 0 

Zi = 

0 ... ...

yi2 0 . . .

0 yi3 0

...

...

... ...
0



0 0 . . . 0 yi,T -1

Blundell and Bond (1998) provide some Monte-Carlo simulation results showing that this procedure leads to a more efficient and robust estimator. Of course, one could further consider extensions to such an approach: any of the valid instruments previously described for the model in first differences are valid candidates for Z (although Blundell and Bond (1998) suggest using those given in Arellano and Bond (1991)); and, depending upon the assumptions one is prepared to make regarding the exogeneity of the X variables, further moment conditions based on these, as suggested above, could also be used to further augment the instrument set.

8.4.2.6 Using Further Orthogonality Conditions: Ahn and Schmidt Estimator
It is generally considered that increasing the number of orthogonality conditions is a way to improve the asymptotic efficiency of an estimator; an argument used by Arellano and Bond (1991), for example. Along the same lines, Ahn and Schmidt (1995, 1997, 1999) have shown that one can deduce supplementary orthogonality conditions from some of the assumptions that define the dynamic error components model.
Indeed, the assumption of absence of serial correlation of the it disturbances implies the following orthogonality conditions
E (iT it ) = 0, t = 2, . . . , T - 1
that is
E yiT - yi,T -1 - xiT  yit - yi,t-1 - xit  = 0, t = 2, . . . , T - 1.

8 Dynamic Models for Short Panels

269

Moreover, the homoskedasticity of it allows one to write

E (yi,t-2i,t-1 - yi,t-1it ) = 0, t = 4, . . . , T

that is

E[yi,t-2(yi,t-1 - yi,t-2 - Xi,t-1 ) - yi,t-1(yit - yi,t-1 - Xit  )] = 0,
t = 3, . . . , T.
Estimation can now be undertaken using non-linear GMM techniques. However, the asymptotic efficiency gain may not prove to be very important, and moreover, must be weighed against the increased complexity of the estimation procedure.

8.4.3 Some Monte Carlo Evidence
An extensive amount of work has been undertaken analysing the small sample performance of the GMM-type estimators (see, for example, Arellano and Bond, 1991, Arellano and Bover, 1995, Kiviet, 1995, Judson and Owen, 1999, Harris and Ma´tya´s, 2004). With such a vast array of experimental evidence, results, of course, vary quite dramatically depending on sample sizes (in N, T and the number of Monte Carlo replications), parameter settings and the assumed data generating process (for example, whether the model is strictly a simple autoregressive one, or not).
Notwithstanding these comments, in the majority of the studies simple IV estimators that have the same number of instruments as endogenous variables, fare poorly due to their lack of finite moments (Kinal, 1980). Typically, Arellano's (1989) estimator tends to have much better performance than that of Anderson and Hsiao (1982), as do the Arellano and Bond (1991) one(s), although often their relative performance is quite close. A clear message is that in small T panels, GMMestimators using a large number of moment conditions (for example, the Arellano and Bond (1991), Arellano and Bover (1995) and non-linear GMM estimators) can all suffer, to a significant extent, from the resulting small-sample bias. Therefore, if practitioners are using such estimators they should attempt to limit the number of conditions so used. An estimator which appears to have an extremely robust performance is the Balestra and Nerlove (1966) estimator, which is, moreover, very simple to construct. Not only does this estimator perform well across numerous settings (parameters, sample sizes) but has also been shown, in a Monte Carlo setting, to perform very well when the true data generating process has been seriously misspecified (Harris and Ma´tya´s, 1996). Also evident from the numerous Monte Carlo results is that as   1, simple OLS (or FGLS) is the preferred option.

270
8.5 The Maximum Likelihood Estimator14

M.N. Harris et al.

An alternative to GMM estimation is to use maximum likelihood techniques, assuming that both the individual effects i and the idiosyncratic disturbances it are normally distributed. A simple way to estimate the model might be to consider the likelihood of the observed sample for t = 1, . . . , T , conditional on the initial observations yi0

 LNT

(,  , 2 , 2|y0, X )

=

-NT 2

ln 2

-

N 2

ln det()

-

1 2

i

ui -1ui

with

ui = (yi1 - yi0 - xi1 , . . . , yiT - yi,T -1 - xiT  )  = 2[WN + (1/ )BN].
This was first explored in the seminal paper by Balestra and Nerlove (1966), with a conditional likelihood function in which the initial observations were assumed to be fixed; the main advantage of this assumption being that this leads to GLS-like estimators, which are quite easy to compute. However, such ML estimators are, for a wide range of combinations of the parameters, equal to the OLS estimator and hence they are not consistent (Trognon, 1978).
This important drawback does not occur when the likelihood function takes into account the density function of the first observations, i.e., when the likelihood function is unconditional (Bhargava and Sargan, 1983). To illustrate the unconditional ML estimator, let us consider the following model

yit = yi,t-1 + xi1  + zi + i + it

(8.33)

where, along the lines suggested by Bhargava and Sargan (1983), Chamberlain (1982) or Blundell and Smith (1991), the initial values are assumed to be defined by

yi0 = zi + ui0 .

(8.34)

It is convenient to decompose the specific effect i as in the following regression

i = ui0 + i ,

where i is independent of ui0. In this model, (ui0, i, i1, . . . , iT ) are i.i.d. N(0, diag(u2, 2, 2eT )) and the log-likelihood function is

14 For a detailed exposition of the application of maximum likelihood techniques to dynamic error components models, see Nerlove (2002).

8 Dynamic Models for Short Panels

271

 LNT (,  , , , , u2, 2, 2)

=-

N(T + 1) 2

ln

2

-

N 2

ln

u2

-

1 2u2

i

ui20

 -

N 2

ln

det()

-

1 2

i

ui -1ui

with

ui = (yi1 - yi0 -  xi1 - zi - ui0, . . . , yiT - yi,T -1 -  xiT - zi - ui0) ui0 = yi0 - zi.

The first order conditions are then as follows

  a.

L 1   = 2

i

yi,-1WN

ui

+

(2

1 +T

2)

i

yi,-1BN ui = 0

(8.35)

  b.

L 1   = 2

i

xi

WN

ui

+

(2

1 +T

2)

xi BN ui = 0
i

 c.

L 

=

(2

1 + T 2)

i

zieT ui = 0

 d.

L

1

  = (2 + T 2)

i

ui0eT ui = 0

  e.

L

1

  = (2 + T 2)

i

 zi eT

ui

+

1 u2

zi ui0 = 0
i

 f .

L  2

=

-

N(T - 22

1)

+

1 24

i

ui WN ui = 0

 g.

L  (2 + T 2)

=

- (2

N + T 2)

+

(2

1 + T 2)2

ui BN ui = 0
i

 h.

L  u2

=

-

N 2u2

+

1 24u

ui20 = 0 .
i

This set of equations implies that the ML estimators of  and u2 are OLS estimators on equation (8.34). If ui0 is the residual of this equation, then the other ML estimators are the solutions of (8.35), where ui0 has been replaced by ui0 in ui . In other words, the ML estimators of the model given by (8.33) and (8.34), are the combi-
nation of OLS estimation of (8.34) followed by ML estimation of (8.33), where the
unobservable ui0 is replaced by its estimator ui0. In the case where the initial observations are defined as

yi0 = zi + xi0 + ui0

(8.36)

but the model for other periods as given in (8.33) is unchanged, the variable xi0 does not enter the autoregressive equation and the previous simplification disappears. Nevertheless here, Sevestre and Trognon (1990) suggest an estimation technique

272

M.N. Harris et al.

equivalent to ML estimation based on the following extended autoregressive model

yit = yi,t-1 + xit  + zi + xi0 + ui0 + i + it .
The ML estimator of ,  , , , 2, u2, 2 on this auxiliary model are asymptotically equivalent to the ML estimator of the true model. If  is the OLS estimator of  on (8.36), a more efficient estimator is given by  =  + /, which is asymptotically efficient if  and  are the ML estimators of  and  in the auxiliary model (see Sevestre and Trognon, 1990 and Blundell and Smith, 1991).
It appears that the unconditional ML estimator can be worked out in a very simple way. Obviously, in the case when the disturbances are normal, such an ML procedure yields the asymptotically most efficient estimator, but this good behavior is also apparent in small samples too as evidenced by simulation studies (Sevestre and Trognon, 1990).

8.6 Testing in Dynamic Models
8.6.1 Testing the Validity of Instruments
Many potential candidates have been presented so far that consistently estimate a linear dynamic panel data model. Of course, an obvious question is which one to use?, as in practice different choices can lead to vastly different parameter estimates and inference (Lee et al., 1998). One can base judgment on the available Monte Carlo evidence (see Sect. 4.3), but this is typically inconclusive. The concept of a "good" instrument is twofold: exogeneity and relevance (one might also be concerned with efficiency in practice, with computational complexity).
The assumption about the absence of any (asymptotic) correlation between the instrumental variables and the disturbances is commonly tested using the Sargan (Hansen) test (Sargan, 1958 and Hansen, 1982). This is dealt with in Chap. 4 and as such, we do not dwell on this issue here. An issue to bear in mind here is that for this (these) test(s) to be valid, any heteroskedasticity and/or serial correlation present, must have been taken into account in estimation.
The other aspect of the choice of "good" instruments lies in the strength of the correlation between the endogenous regressors and the instruments. Indeed, the concept of relevance and of weak instruments in the dynamic linear panel setting was considered by Blundell and Bond, (1998), who show that a small correlation results in erratic parameter estimates. Moreover, there is a significant amount of recent literature highlighting the deleterious effects of weak instruments in general (see, for example Wang and Zivot, 1998, Woglom, 2001 and Hahn and Hausman, 2002).
Almost inevitably, the discussion of instrument relevance has traditionally focussed on the correlation between the endogenous regressor(s) and the instrument(s). This is the approach followed in Bound et al. (1995) and Shea (1997). Thus

8 Dynamic Models for Short Panels

273

one could potentially choose across estimators, based on R2 and partial R2 statistics. Such an approach is taken further by Poskitt and Skeels (2004) who, instead, develop a procedure based on a measure of lack of correlation. This approach appears preferable, as it is possible to develop the sampling distribution of the proffered measure, such that standard inferential procedures can be utilised. Moreover, the Poskitt and Skeels (2004) test statistic is readily computed using a set of auxiliary regressions and can be extended to the case where there are multiple endogenous regressors. This procedure, which has yet to be explicitly extended for use within a panel data setting, could thus be used to statistically test for the presence of weak instruments and, in conjunction with the Sargan test, be used to determine the most appropriate IV estimator. Another way to determine the relevance of the instruments is to compute canonical correlations (Mairesse et al., 1999).

8.6.2 Testing for Unobserved Effects

As shown in previous chapters, there are many ways to test for the presence of
unobserved effects. In the case of the dynamic panel model, it is possible to use
Hausman's (1978) test statistic. If the regressors, except for the lagged endogenous variable, are strictly exogenous and the it disturbances are homoskedastic and serially uncorrelated, then in the absence of any individual effects, i.e., under H0, the OLS estimator is consistent and asymptotically efficient. However, as shown, it is in-
consistent when such effects exist. On the other hand, we have presented numerous
estimators which are consistent whether there are individual effects or not (denote these generically  ). Thus in order to test for the presence of individual effects, one may compute the following statistic15

QH = (  - OLS)

V (

) -V (OLS)

-1
(

 - OLS)

Asymptotically under the null hypothesis, QH  d2im() and the test rejects for large values: OLS is inconsistent and one must include unobserved effects in the model
and apply consistent techniques. On the other hand, under H0, OLS can be safely used.
However, it is rather unlikely in practice that all of the assumptions ensuring the
optimality of OLS in the absence of individual effects, are satisfied. When the dis-
turbances are heteroskedastic, one would use (Feasible) GLS instead of OLS as a basis of comparison with GMM estimates. If the it disturbances are serially correlated, thus inducing a correlation with the lagged endogenous variable, one should
compare two different GMM estimators. One estimator would be consistent only in the absence of individual effects (e.g., in the case where the it generating process is an MA(q), one might think of using lags of order q + 1 or more of the endoge-

15 In cases where the difference in variances appears to be non-invertible, one may resort to the generalised inverse.

274

M.N. Harris et al.

nous variables as instruments; indeed, this is consistent under H0 but not under the alternative). The other estimator might make use of strictly and doubly exogenous variables as instruments (i.e., variables that are exogenous both with respect to the individual effects and the it disturbances).

8.6.3 Testing for the Absence of Serial Correlation in 

The presence of serial correlation in it will typically invalidate the use of lagged values (and first differences of such) of the endogenous variable as instruments. So in these circumstances, it is crucial to test for such serial correlation. Probably the most widely used test here is that proposed by Arellano and Bond (1991), which is based upon the model estimated in first differences. Let  be the vector of residuals from the model in first differences, -2 its second lag value, and  the reduction of the vector  allowing computation of the product -2.16 The test statistic is
m2 = -2  1/2

where

  1/2 =



i,-2



 i



 i



i,-2

i

- 2-2X X  Z

 2Z Z

-1
Z

X

-1

 -1
× X Z  2Z Z

Zi i i,-2

i

+ -2XV () X -2 ,

with X similarly defined as the reduction of the regressors matrix X = [y-1, X] and where, as before,  = [,  ] .
This test provides a measure of the importance of serial correlation of order 2
once the model is written in first differences. If the it 's are serially uncorrelated, those of the model, given by it = it - it-1 follow an MA(1) process and thus, are not correlated at order 2. On the contrary, if it appears to be correlated of order 2, one can infer that the disturbances it exhibit some serial correlation.
Arellano and Bond (1991) show that, under the null of no serial correlation in
it at order 2, the m2 statistic is asymptotically distributed as a standard normal variate. One rejects H0 of no serial correlation when m2 is less than -1.64.17

16 i.e., one discards, for each individual, the first two observations of the residual vector. 17 The test is one-sided as it is indeed unlikely that the distrurbances of the first differenced model will exhibit positive serial correlation.

8 Dynamic Models for Short Panels

275

One can again also appeal to the Sargan/Hansen statistic to test for serial

correlation in it . First estimate the model in first differences so that the individual effects are discarded. Under the assumption of no serial correlation in it , it

follow an MA(1) process and, as shown above. the series yi,t-2, yi,t-3, etc. are valid instruments for estimating this model. However, if the it 's are serially correlated,

this series no longer constitutes a valid instrument set.

This implies that one can test H0 (it is serially uncorrelated) against H1 by com-

paring the difference between Sargan/Hansen statistics corresponding to two instru-

ment sets: Z0 which contains the instruments defined by the series yi,t-2, yi,t-3, . . . and Z1, where Z1 is an instrument set not dependent on the assumption of it not be-

ing serially correlated. Indeed, to increase the test's power, one might be more spe-

cific for H1 and test H0 against H1 with the latter hypothesizing, for example, that the it 's follow an MA(1) process. In this case, one would compare the Sargan/Hansen

statistics associated with Z0 as above, but with Z1 comprising of yi,t-3 as one of

the instruments (since this would be a valid choice even under H1). Denote the dif-

ference between the two Sargan/Hansen statistics by DQsh. Under the null this is

distributed

as



2 p0-

p1

where

p0

is

the

number

of

instruments

in

Z0

and

p1

that

in

Z1.

8.6.4 Significance Testing in Two-Step Variants

Arguably the most frequently used approach in estimating dynamic panel data models, is to follow the approach of Arellano and Bond (1991), and in particular the two-step GMM variant (popularity of these approaches may be due to the fact that GAUSS code has long been made available by the authors, and now estimation can be undertaken routinely in STATA and LIMDEP, for example).18 A major drawback to this approach though, is that the two-step standard errors have been shown to be unreliable (Arellano and Bond, 1991; Blundell and Bond, 1998; Harris and Ma´tya´s, 2004 and others), so that often researchers base inference on the two-step parameter estimates but using standard errors obtained in the first step. Essentially, the problem arises as the standard expressions for the conventional asymptotic variances omit the extra variation in the efficient GMM weighting matrix, VN (1), which is based on the one-step estimates, 1 where

 VN

(1)

=

1 N

N
Zi 1i1iZi
i=1

and where 1i are the residuals from the first-step GMM estimator. The formula for the small sample bias corrected variance, Vbc (2), of 2 is rather
complicated and is given by Windmeijer (2005) as

18 There has also been a recent rise in the number of applications using the Blundell and Bond (1998) estimator.

276

M.N. Harris et al.

Vbc (2) = N X ZVN-1 (1) Z X -1 + ND X ZVN-1 (1) Z X -1 + N X ZVN-1 (1) Z X -1 D
+ DV (1) D

where, using the notation D [., j] to denote all rows of the jth column of the matrix D

D [., j] = - X ZVN (1) Z X -1 X ZVN (1) ×

 - 1
N

N
Zi
i=1

xi j1i + 1ixi j Zi

VN-1 (1) Z 2

and xi j is the (T × 1) jth column of Xi. Monte Carlo experiments suggest this correction does indeed work well (Windmeijer, 2005). This approach of testing linear restrictions in dynamic linear panel data models estimated using one- and two-step GMM using linear moment conditions, is further developed in Bond and Windmeijer (2005), who consider bootstrapped versions of this Wald test and the LM test (as well as three other criterion-based tests).

References
Ahn S.C. and P. Schmidt (1995), Efficient Estimation of Models for Dynamic Panel Data, Journal of Econometrics, Vol. 68, No. 1, pp. 5­27.
Ahn S.C. and P. Schmidt (1997), Efficient Estimation of Dynamic Panel Data Models: Alternative Assumptions and Simplified Estimation, Journal of Econometrics, Vol. 76, No. 1­2, pp. 309­321.
Ahn S.C. and P. Schmidt (1999), Estimation of Linear Panel Data Models Using GMM, in Generalised Method of Moments Estimation, L. Ma´tya´s ed., pp. 211­245, Cambridge University Press, Cambridge.
Alvarez J. and M. Arellano (2003), The Time Series and Cross-Section Asymptotics of Dynamic Panel Data Estimators, Econometrica, Vol. 71, pp. 1121­1159.
Amemiya T. and T.E. MaCurdy (1986), Instrumental Estimation of an Error Components Model, Econometrica, Vol. 54, pp. 869­881.
Anderson T.W. and C. Hsiao (1982), Formulation and Estimation of Dynamic Models Using Panel Data, Journal of Econometrics, Vol. 18, pp. 578­606.
Arellano, M. (1989), A Note on the Anderson­Hsiao Estimator for Panel Data, Economics Letters, Vol. 31, pp. 337­341.
Arellano M. and S. Bond (1991), Some Tests of Specification for Panel Data: Monte-Carlo Evidence and an Application to Employment Equations, Review of Economic Studies, Vol. 58, pp. 127­134.
Arellano M. and O. Bover (1995), Another Look at the Instrumental Variables Estimation of ErrorComponents Models, Journal of Econometrics, Vol. 68, No. 1, pp. 29­51.
Balestra P. and M. Nerlove (1966), Pooling Cross-Section and Time-Series Data in the Estimation of a Dynamic Model: The Demand for Natural Gas, Econometrica, Vol. 34, pp. 585­612.
Bhargava, A. and J.D. Sargan (1983), Estimating Dynamic Random Effects Models from Panel Data Covering Short Time Periods, Econometrica, Vol. 51, pp. 1635­1659.

8 Dynamic Models for Short Panels

277

Blundell R. and S. Bond (1998), Initial Conditions and Moment Restrictions in Dynamic Panel Data Models, Journal of Econometrics, Vol. 87, pp. 115­143.
Blundell R. and R. Smith (1991), Conditions initiales et estimation efficace dans les mode`les dynamiques sur donne´es de panel, Annales d'Economie et de Statistique, No. 20­21, pp. 109­123.
Bond S. and F. Windmeijer (2005), Finite Sample Inference for GMM Estimators in Linear Panel Data Models, Econometric Reviews, Vol. 24, pp. 1­37.
Bound J., D.A. Jaeger and R.M. Baker (1995), Problems with Instrumental Variables Estimation When the Correlation Between the Instruments and the Endogenous Explanatory Variable is Weak, Journal of the American Statistical Association, Vol. 90, pp. 443­450.
Breusch T.S., Mizon G.E. and P. Schmidt (1989), Efficient Estimation Using Panel Data, Econometrica, Vol. 57, pp. 695­700.
Chamberlain, G. (1982), Multivariate Regression Models for Panel Data, Journal of Econometrics, Vol. 18, No. 1, pp. 5­46.
Hahn J. and J. Hausman (2002), A New Specification Test for the Validity of Instrumental Variables, Econometrica, Vol. 70, pp. 163­189.
Hansen L. (1982), Large Sample Properties of Generalised Method of Moments Estimators, Econometrica, Vol. 50, pp. 1029­1054.
Harris M.N. and L. Ma´tya´s (1996), The Robustness of Estimators for Dynamic Panel Data Models to Misspecification, Monash University Working Paper, No. 9/96.
Harris M.N. and L. Ma´tya´s (2000), Performance of the Operational Wansbeek­Bekker Estimator for Dynamic Panel Data Models, Applied Economics Letters, Vol. 7, No. 3, pp. 149­153.
Harris M.N. and L. Ma´tya´s (2004), A Comparative Analysis of Different IV and GMM Estimators of Dynamic Panel Data Models, International Statistical Review, Vol. 72, No. 3, pp. 397­408.
Hausman J. (1978), Specification Tests in Econometrics, Econometrica, Vol. 46, pp. 1251­1271. Hausman J. and W.E. Taylor (1981), Panel Data and Unobservable Individual Effects, Economet-
rica, Vol. 49, pp. 1377­1398. Judson R.A. and A.L. Owen, 1999, Estimating Dynamic Panel Data Models: A guide for Macroe-
conomists, Economics Letters, Vol. 65, pp. 9­15. Kinal T.W. (1980), The Existence of K-Class Estimators, Econometrica, Vol. 48, pp. 241­249. Kiviet J.F. (1995), On Bias, Inconsistency and Efficiency of Various Estimators in Dynamic Panel
Data Models, Journal of Econometrics, Vol. 68, No. 1, pp. 53­78. Lee M., R. Longmire, L. Ma´tya´s and M.N. Harris (1998), Growth Convergence: Some Panel Data
Evidence, Applied Economics, Vol. 30, No. 7, pp. 907­912. Maddala G.S. (1971), The Use of Variance Components Models in Pooling Cross-Section and
Time Series Data, Econometrica, Vol. 39, pp. 341­358. Mairesse J., B. Hall, B. Mulkay (1999), Firm-Level Investment in France and the United States: An
Exploration of What We Have Learned in Twenty Years, Annales d'Economie et de Statistique, No. 55­56, pp. 27­68. Nerlove M. (2002), Essays in Panel Data Econometrics, Cambridge University Press, Cambridge. Nickell S. (1981), Biases in Models with Fixed Effects, Econometrica, Vol. 49, pp. 1417­1426. Poskitt D.S. and C.L. Skeels (2004), Assessing the Magnitude of the Concentration Parameter in a Simultaneous Equations Model, Department of Econometrics and Business Statistcs, Monash University, Working Paper, No. 29/04. Sargan J.D. (1958), The Estimation of Economic Relationships Using Instrumental Variables, Econometrica, Vol. 26, pp. 393­415. Sevestre P. (1984), Mode`les dynamiques a` erreurs compose´es, Ph.D. Dissertation, Universite´ Paris 1, Panthe´on-Sorbonne. Sevestre P. and A. Trognon (1985), A Note on Autoregressive Error Component Models, Journal of Econometrics, Vol. 28, No. 2, pp. 231­245. Sevestre P. and A. Trognon (1983), Proprietes de Grands Echantillons d'une Classe d'Estimateurs Des Mode`les Autore´gressifs a` Erreurs Compose´es, Annales de l'INSEE, No. 50, pp. 25­49. Sevestre P. and Trognon, A. (1990), Consistent Estimation Methods for Dynamic Error Component Models: Small and Large Sample Properties, ERUDITE WP No. 90­03.

278

M.N. Harris et al.

Shea J. (1997), Instrumental Relevance in Multivariate Linear Models: A Simple Measure, The Review of Economics and Statistics, Vol. 79, pp. 348­352.
Trognon, A. (1978), Miscellaneous Asymptotic Properties of Ordinary Least Squares and Maximum-Likelihood Methods in Dynamic Error-Components Models, Annales de l'INSEE, Vol. 30­31, pp. 631­657.
Wang J. and E. Zivot (1998), Inference on Structural Parameters in Instrumental Variables Regression with Weak Instruments, Econometrica, Vol. 66, pp. 1389­1404.
Wansbeek T. and P. Bekker (1996), On IV, GMM and ML in a Dynamic Panel Data Model, Economic Letters, Vol. 51, No. 2, pp. 145­152.
Windmeijer F. (2005), A Finite Sample Correction for the Variance of Linear Efficient Two-Step GMM Estimators, Journal of Econometrics,Vol. 126, No. 1, pp. 25­51.
Woglom G. (2001), More Results on the Exact Small Sample Properties of the Instrumental Variable Estimator, Econometrica, Vol. 69, pp. 1381­1389.

Chapter 9
Unit Roots and Cointegration in Panels
Jo¨rg Breitung and M. Hashem Pesaran

9.1 Introduction
Recent advances in time series econometrics and panel data analysis have focussed attention on unit root and cointegration properties of variables observed over a relatively long span of time across a large number of cross section units, such as countries, regions, companies or even households. Such panel data sets have been used predominately in testing the purchasing power parity and output convergence, although the panel techniques have also been adapted more recently to the analysis of business cycle synchronization, house price convergence, regional migration and household income dynamics. This paper provides a review of the theoretical literature on testing for unit roots and cointegration in panels where the time dimension (T ), and the cross section dimension (N) are relatively large. In cases where N is large (say over 100) and T small (less than 10) the analysis can proceed only under restrictive assumptions such as dynamic homogeneity and/or local cross section dependence as in spatial autoregressive or moving average models. In cases where N is small (less than 10) and T is relatively large standard time series techniques applied to systems of equations, such as the Seemingly Unrelated Regression Equations (SURE), can be used and the panel aspect of the data should not pose new technical difficulties.
One of the primary reasons behind the application of unit root and cointegration tests to a panel of cross section units was to gain statistical power and to improve on the poor power of their univariate counterparts. This was supported by the application of what might be called the first generation panel unit root tests to

Jo¨rg Breitung University of Bonn, Institute of Econometrics, Adenauerallee 24-42, 53113 Bonn, Germany, e-mail: breitung@uni-bonn.de
M. Hashem Pesaran Cambridge University and USC, Sidgwick Avenue, Cambridge, CB3 9DD, United Kingdom, e-mail: mhp1@econ.cam.ac.uk

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

279

c Springer-Verlag Berlin Heidelberg 2008

280

J. Breitung and M. Hashem Pesaran

real exchange rates, output and inflation. For example, the augmented Dickey and Fuller (1979) test is typically not able to reject the hypothesis that the real exchange rate is nonstationary. In contrast, panel unit root tests applied to a collection of industrialized countries generally find that real exchange rates are stationary, thereby lending empirical support to the purchasing power parity hypothesis (e.g. Coakley and Fuertes 1997 and Choi 2001).
Unfortunately, testing the unit root and cointegration hypotheses by using panel data instead of individual time series involves several additional complications. First, panel data generally introduce a substantial amount of unobserved heterogeneity, rendering the parameters of the model cross section specific. Second, in many empirical applications, particularly the application to the real exchange rates mentioned above, it is inappropriate to assume that the cross section units are independent. To overcome these difficulties, variants of panel unit root tests are developed that allow for different forms of cross sectional dependence.1 Third, the panel test outcomes are often difficult to interpret if the null of the unit root or cointegration is rejected. The best that can be concluded is that "a significant fraction of the cross section units is stationary or cointegrated". The panel tests do not provide explicit guidance as to the size of this fraction or the identity of the cross section units that are stationary or cointegrated. Fourth, with unobserved I(1) (i.e. integrated of order unity) common factors affecting some or all the variables in the panel, it is also necessary to consider the possibility of cointegration between the variables across the groups (cross section cointegration) as well as within group cointegration. Finally, the asymptotic theory is considerably more complicated due to the fact that the sampling design involves a time as well as a cross section dimension. For example, applying the usual Dickey­Fuller test to a panel data set introduces a bias that is not present in the case of a univariate test. Furthermore, a proper limit theory has to take into account the relationship between the increasing number of time periods and cross section units (cf. Phillips and Moon 1999).
By comparison to panel unit root tests, the analysis of cointegration in panels is still at an early stage of it's developments. So far the focus of the panel cointegration literature has been on residual based approaches, although there has been a number of attempts at the development of system approaches as well. As in the case of panel unit root tests, such tests are developed based on homogenous and heterogeneous alternatives. The residual based tests were developed to ward against the "spurious regression" problem that can also arise in panels when dealing with I(1) variables. Such tests are appropriate when it is known a priori that at most there can be only one within group cointegration in the panel. System approaches are required in more general settings where more than one within group cointegrating relation might be present, and/or there exist unobserved common I(1) factors.
Having established a cointegration relationship, the long-run parameters can be estimated efficiently using techniques similar to the ones proposed in the case of single time series models. Specifically, fully-modified OLS procedures, the

1 In fact the application of the second generation panel unit root tests to real exchange rates tend to over-turn the earlier test results that assume the cross section units are independently distributed. See Moon and Perron (2004) and Pesaran (2008).

9 Unit Roots and Cointegration in Panels

281

dynamic OLS estimator and estimators based on a vector error correction representation were adopted to panel data structures. Most approaches employ a homogenous framework, that is, the cointegration vectors are assumed to be identical for all panel units, whereas the short-run parameters are panel specific. Although such an assumption seems plausible for some economic relationships (like the PPP hypothesis mentioned above) there are other behavioral relationships (like the consumption function or money demand), where a homogeneous framework seems overly restrictive. On the other hand, allowing all parameters to be individual specific would substantially reduce the appeal of a panel data study. It is therefore important to identify parameters that are likely to be similar across panel units whilst at the same time allowing for sufficient heterogeneity of other parameters. This requires the development of appropriate techniques for testing the homogeneity of a sub-set of parameters across the cross section units. When N is small relative to T, standard likelihood ratio based statistics can be used. Groen and Kleibergen (2003) provide an application. Testing for parameter homogeneity in the case of large panels poses new challenges that require further research. Some initial attempts are made in Pesaran, Smith and Im (1996), Phillips and Sul (2003a) and Pesaran and Yamagata (2008).
This paper reviews some recent work in this rapidly developing research area and thereby updating the earlier excellent surveys of Banerjee (1999), Baltagi and Kao (2000) and Choi (2006). The remainder of the paper is organized as follows: Sect. 9.2 sets out the basic model for the panel unit root tests and describes the first generation panel unit root tests. Second generation panel unit root tests are described in Sect. 9.3, and a brief account of the small sample properties of the panel unit root tests is provided in Sect. 9.5. General issues surrounding panel cointegration, including the problem of cross-section cointegration, are discussed in Sect. 9.6. Residual-based and system approaches to testing for cointegration in panels are reviewed in Sect. 9.7 and 9.8; and estimation of the cointegration relations in panels is discussed in Sect. 9.9. Panels with unobserved common factors, allowing for cross-section cointegration, are reviewed in Sect. 9.10. Some concluding remarks are provided in Sect. 9.11.

9.2 First Generation Panel Unit Root Tests

9.2.1 The Basic Model

Assume that time series {yi0, . . . , yiT } on the cross section units i = 1, 2, . . . , N are generated for each i by a simple first-order autoregressive, AR(1), process

yit = (1 - i)i + iyi,t-1 + it ,

(9.1)

where the initial values, yi0, are given, and the errors it are identically, independently distributed (i.i.d.) across i and t with E(it ) = 0, E(i2t ) = i2 <  and

282

J. Breitung and M. Hashem Pesaran

E(i4t ) < . These processes can also be written equivalently as simple Dickey­ Fuller (DF) regressions

yit = -ii + iyi,t-1 + it ,

(9.2)

where yit = yit - yi,t-1, i = i - 1. In further developments of the model it is also helpful to write (9.1) or (9.2) in mean-deviations forms y~it = iy~i,t-1 + it , where y~it = yit - i. The corresponding DF regression in y~it is given by

y~it = iy~i,t-1 + it . The null hypothesis of interest is

(9.3)

H0 : 1 = · · · = N = 0 ,

(9.4)

that is, all time series are independent random walks. We will consider two alternatives:

H1a : 1 = · · · = N   and  < 0
H1b : 1 < 0 , · · · , N0 < 0, N0  N .
Under H1a it is assumed that the autoregressive parameter is identical for all cross section units (see, for example, Levin and Lin (1993, LL), and Levin, Lin and Chu, 2002). This is called the homogeneous alternative. H1b assumes that N0 of the N (0 < N0  N) panel units are stationary with individual specific autoregressive coefficients. This is referred to as the heterogeneous alternatives (see, for example, Im, Pesaran and Shin (2003, IPS). For the consistency of the test it is assumed that N0/N   > 0 as N  . Different panel testing procedures can be developed depending on which of the two alternatives is being considered. The panel unit root statistics motivated by the first alternative, H1a, pools the observations across the different cross section units before forming the "pooled" statistic, whilst the tests developed against the heterogeneous alternatives, H1b, operates directly on the test statistics for the individual cross section units using (standardized) simple averages of the underlying individual statistics or their suitable transformations such as rejection probabilities. Despite the differences in the way the two tests view the alternative hypothesis both tests can be consistent against both types of the alternatives. Also interpretation of the outcomes of both tests is subject to similar considerations discussed in the introduction. When the null hypothesis is rejected one can only conclude that a significant fraction of the AR(1) processes in the panel does not contain unit roots.

9.2.2 Derivation of the Tests
The various first generation panel unit roots proposed in the literature can be obtained using the pooled log-likelihood function of the individual Dickey­Fuller regressions given by (9.2).

9 Unit Roots and Cointegration in Panels

283

  N
NT ( ,  ) =
i=1

-T 2

log 2i2

-

1 2i2

T
(yit
t=1

+ ii

- iyi,t-1)2

,

(9.5)

where  = (1, . . . , N) , i = (i, i2) and  = (1, . . . , N) . In the case of the

homogeneous alternatives, H1a, where i =  , the maximum likelihood estimator of

 is given by

^ ( )

=

iN=1 tT=1 i-2yit (yi,t-1 - i) iN=1 tT=1 i-2 (yi,t-1 - i)2

.

(9.6)

The nuisance cross-section specific parameters i can be estimated either under the
null or the alternative hypothesis. Under the null hypothesis i is unidentified, but as
we shall see it is often replaced by yi0, on the implicit (identifying) assumption that
y~i0 = 0 for all i. For this choice of i the effective number of time periods used for
estimation of i is reduced by one. Under the alternative hypothesis the particular estimates of i and i2 chosen naturally depend on the nature of the alternatives envisaged. Under homogeneous alternatives, i =  < 0, the ML estimates of i and i2 are given as non-linear functions of ^. Under heterogeneous alternatives i and i2 can be treated as free parameters and estimated separately for each i.
Levin et al. (2002) avoid the problems associated with the choice of the estima-

tors for i and base their tests on the t-ratio of  in the pooled fixed effect regression

yit = ai +  yi,t-1 + it , it i.i.d.(0, i2) .

The t-ratio of the FE estimator of  is given by

 =

N


i-2 yi Me yi, -1

i=1

N


i-2

yi, -1Meyi, -1

i=1

(9.7)

where yi = (yi1, yi2, . . . , yiT ) , yi,-1 = (yi0, yi1, . . . , yi,T -1) , Me = IT - eT (eT eT )-1eT , eT is a T × 1 vector of ones,

^i2

=

yiMi yi T -2

,

(9.8)

Mi = IT - Xi(XiXi)-1Xi, and Xi= (eT , yi,-1). The construction of a test against H1b is less clear because the alternative consists
of a set of inequality conditions. Im, Pesaran and Shin (1995, 2003) suggest the mean of the individual specific t-statistics2

2 Andrews (1998) has considered optimal tests in such situations. His directed Wald statistic that gives high weights to alternatives close to the null (i.e. the parameter c in Andrews (1998) tends to zero) is equivalent to the mean of the individual specific test statistics.

284

J. Breitung and M. Hashem Pesaran

 ¯

=

1 N

N
i
i=1

,

(9.9)

where

i =

yiMeyi,-1 1/2 ,

^i yi,-1Meyi,-1

(9.10)

is the Dickey­Fuller t-statistic of cross section unit i.3 LM versions of the t-ratios of  and i, that are analytically more tractable, can also be used which are given by

~ =

N


~ i-2 yi Me yi, -1

i=1

,

N


~i-2

yi, -1Meyi, -1

i=1

(9.11)

and

~i =

yiMeyi,-1 1/2 ,

(9.12)

~i yi,-1Meyi,-1

where ~i2 = (T - 1)-1yiMeyi. It is easily established that the panel unit root tests based on  and ~ in the case of the pooled versions, and those based on ¯ and

N
~ = N-1  ~i i=1
in the case of their mean group versions are asymptotically equivalent.

(9.13)

9.2.3 Null Distribution of the Tests

To establish the distribution of ~ and ~, we first note that under i = 0, yi = ivi = i(vi1, vi2, . . . , viT ) , where vi (0, IT ) and yi,-1 can be written as

yi,-1 = yi0eT + isi,-1 ,

(9.14)

where yi0 is a given initial value (fixed or random), si,-1 = (si0, si1, . . . , si,T-1) , with sit = tj=1 vi j, t = 1, 2, . . . , T, and si0 = 0. Using these results in (9.11) and (9.12) we have

3 The mean of other unit-root test statistics may be used as well. For example, Smith, Leybourne, Kim and Newbold (2004) suggest to use the mean of the weighted symmetric test statistic proposed for single time series by Park and Fuller (1995) and Fuller (1996, Sect. 10.1.3), or the Max-ADF test proposed by Leybourne (1995) based on the maximum of the original and the time reversed Dickey­Fuller test statistics.

9 Unit Roots and Cointegration in Panels

N
 ~ = i=1

 T -1viMesi,-1 vi Me vi

,

N

i=1

si, -1Mesi, -1 vi Me vi

and

N
~ = N-1

 T - 1viMesi,-1

i=1 (viMevi)1/2 si,-1Mesi,-1

1/2 .

285
(9.15)
(9.16)

It is clear that under the null hypothesis both test statistics are free of nuisance pa-

rameters and their critical values can be tabulated for all combinations of N and T

assuming, for example, that it (or vit ) are normally distributed. Therefore, in the case where the errors, it, are serially uncorrelated an exact sample panel unit root test can be developed using either of the test statistics and no adjustments to the

test statistics are needed. The main difference between the two tests lies in the way

information on individual units are combined and their relative small sample per-

formance would naturally depend on the nature of the alternative hypothesis being

considered.

Asymptotic null distributions of the tests can also be derived depending on

whether (T, N)  , sequentially, or when both N and T  , jointly. To derive

the asymptotic distributions we need to work with the standardized versions of the

test statistics

ZLL =  - E  , Var 

(9.17)

and



ZIPS =

N [¯ - E(i)] , Var(i)

(9.18)

assuming that T is sufficiently large such that the second order moments of i and  exist. The conditions under which i has a second order moment are discussed

in IPS and it is shown that when the underlying errors are normally distributed

the second order moments exist for T > 5. For non-normal distributions the ex-

istence of the moments can be ensured by basing the IPS test on suitably trun-

cated versions of the individual t-ratios. (see Pesaran (2008) for further details).

The exact first and second order moments of i and ~i for different values of T are

given in IPS (2003, Table 1). Using these results it is also possible to generalize

the IPS test for unbalanced panels. Suppose the number of time periods available

on the ith cross section unit is Ti, the standardized IPS statistics will now be given

by

 N
ZIPS =

¯ - N-1 iN=1 E(iTi )

,

(9.19)

N-1 Ni=1 Var(iTi )

286

J. Breitung and M. Hashem Pesaran

where E(iTi ) and Var(iTi ) are, respectively, the exact mean and variance of the DF statistics based on Ti observations. IPS show that for all finite Ti > 6, ZIPS d N (0, 1) as N  . Similar results follow for the LL test.
To establish the asymptotic distribution of the panel unit root tests in the case of
T  , we first note that for each i

i d i =

1 0

Wi(a)dWi

(a)

1 0

Wi(a)2da

,

where Wi(a) is a demeaned Brownian motion defined as Wi(a) = Wi(a)-

1 0

Wi(a)da

and W1(a), . . . , WN(a) are independent standard Brownian motions. The existence

of the moments of i are established in Nabeya (1999) who also provides numerical

values for the first six moments of the DF distribution for the three standard spec-

ifications; namely models with and without intercepts and linear trends. Therefore, since the individual Dickey­Fuller statistics 1, . . . , N are independent, it follows that 1, 2, . . . N are also independent with finite moments. Hence, by standard central limit theorems we have



ZIPS

--d-
T 

N [¯ - E(i)] --d- N (0, 1) , Var(i) N

where ¯ = N-1 iN=1 i. Similarly,

ZLL

=



- E( ) Var( )

---d--
(T,N)

N (0, 1) .

To simplify the exposition the above asymptotic results are derived using a se-

quential limit theory, where T   is followed by N  . However, Phillips and

Moon (1999) show that sequential convergence does not imply joint convergence

so that in some situations the sequential limit theory may break down. In the case

of models with serially uncorrelated errors, IPS (2003) show that the t-bar test is in

fact valid for N and T   jointly. Also as we shall see it is conjectured that the

IPS test is valid for the case of serially correlated errors as N and T   so long as

N/T  k where k is a finite non-zero constant.

Maddala and Wu (1999) and Choi (2001) independently suggested a test against

the heterogenous alternative H1b that is based on the p-values of the individual statistic as originally suggested by Fisher (1932). Let i denote the p-value of the

individual specific unit-root test applied to cross-section unit i. The combined test

statistic is

N
 = -2  log(i) . i=1

(9.20)

Another possibility would be to use the inverse normal test defined by

9 Unit Roots and Cointegration in Panels

287

 ZINV

=

1 N

N
-1 (i)
i=1

,

(9.21)

where (·) denotes the cdf of the standard normal distribution. An important advantage of this approach is that it is possible to allow for different specifications (such
as different deterministic terms and lag orders) for each panel unit. Under the null hypothesis  is 2 distributed with 2N degrees of freedom. For
large N the transformed statistic

 ¯ 

=

-

1 N

N
[log(i)
i=1

+

1]

,

(9.22)

is shown to have a standard normal limiting null distribution as T, N  , sequentially.

9.2.4 Asymptotic Power of the Tests

It is interesting to compare the asymptotic power of the test statistics against the sequence of local alternatives

H:

i,NT

= 1-

ci TN

.

(9.23)

Following Breitung (2000) and Moon, Perron and Phillips (2007) the asymptotic distribution under H is obtained as Z j d N (-c¯ j, 1), j =LL, IPS, where c¯ = limN N-1 Ni=1 ci and

1 = E

1

E

Wi(a)2da , 2 =

0

1 0

Wi(a)2

da

. Var(i)

It is interesting to note that the local power of both test statistics depends on the

mean c¯. Accordingly, the test statistics do not exploit the deviations from the mean

value of the autoregressive parameter.

Moon et al. (2007) derive the most powerful test statistic against the local alternative (9.23). Assume that we (randomly) choose the sequence c1, . . . , cN instead of the unknown values c1, . . . , cN. The point optimal test statistic is constructed using
the (local-to-unity) pseudo differences

ci yit

=

yit

-

(1 -

ci/T

 N)yi,

t-1

for

t

=

1,

...,

T

.

For the model without individual constants and homogeneous variances the point optimal test results in the statistic

288

J. Breitung and M. Hashem Pesaran

  1
VNT =  2

NT
(ci yit )2 - (yit )2
i=1 t=1

- 12 , 2

where E(ci)2 = 2. Under the sequence of local alternatives (9.23) Moon et al. (2007, Theorem 7) derive the limiting distribution as

VNT d N -E(cici), 22 .
The upper bound of the local power is achieved with ci = ci , that is, if the local alternatives used to construct the test coincide with the actual alternative. Unfortunately, in practice it seems extremely unlikely that one could select values of ci that are perfectly correlated with the true values, ci. If, on the other hand, the variates ci are independent of ci, then the power is smaller than the power of a test using identical values ci = c for all i. This suggests that if there is no information about variation of ci, then a test cannot be improved by taking into account a possible heterogeneity of the alternative.

9.2.5 Heterogeneous Trends

To allow for more general mean functions we consider the model:

yit = idit + y~it ,

(9.24)

where dit represents the deterministics and y~it = iy~i,t-1 + it . For the model with a constant mean we let dit = 1 and the model with individual specific time trends dit is given by dit = (1, t) . Furthermore, structural breaks in the mean function can be accommodated by including (possibly individual specific) dummy variables in
the vector dit . The parameter vector i is assumed to be unknown and has to be estimated. For the Dickey­Fuller test statistic the mean function is estimated under the alternative, that is, for the model with a time trend ^ idit results from a regression of yit on a constant and t (t = 1, 2, . . . , T ). Alternatively, the mean function can also be estimated under the null hypothesis (cf. Schmidt and Phillips, 1992) or under a local alternative (Elliott, Rothenberg and Stock, 1996).4
Including deterministic terms may have an important effect on the asymptotic
properties of the test. Let y~t and y~i,t-1 denote estimates for y~it = yit - E(yit ) and y~i,t-1 = yi,t-1 - E(yi,t-1). In general, running the regression

y~it =  y~i,t-1 + eit
does not render a t-statistic with a standard normal limiting distribution due to the fact that y~i,t-1 is correlated with eit . For example, if dit is an individual specific

4 See, e.g. Choi (2002) and Harvey, Leybourne and Sakkas (2006).

9 Unit Roots and Cointegration in Panels

289

constant such that y~it = yit - T -1(yi0 + · · · + yi,T-1) we obtain under the null hypothesis

 1
lim E T  T

T
eit y~i,t-1
t=1

= -i2/2 .

It follows that the t-statistic of  = 0 tends to - as N or T tends to infinity. To correct for the bias, Levin et al. (2002) suggested using the correction terms

 aT (^ ) = E

1 i2T

T
y~it y~i,t-1
t=1

T

Var T -1  y~it y~i,t-1

b2T (^ ) =

i2E

t=1

T -1

T


y~2i, t -1

t=1

(9.25) (9.26)

where  = (^ 1, ^ 2, . . . ., ^ N) , and ^ i is the estimator of the coefficients of the deterministics, dit , in the OLS regression of yit on dit . The corrected, standardized statitic
is given by

ZLL(^ ) =

N


T
 y~it y~i,t-1/i2

- NTaT (^ )

i=1 t=1

.

bT (^ )

N


T


y~2i, t -1 /i2

i=1 t=1

Levin et al. (2002) present simulated values of aT (^ ) and bT (^ ) for models with constants, time trends and various values of T . A problem is, however, that for unbalanced data sets no correction terms are tabulated.
Alternatively, the test statistic may be corrected such that the adjusted t-statistic

ZLL(^ ) = [ZLL(^ ) - aT (^ )]/bT (^ )

is asymptotically standard normal. Harris and Tzavalis (1999) derive the small sample values of aT (^ ) and bT (^ ) for T fixed and N  . Therefore, their test statistic can be applied for small values of T and large values of N.
An alternative approach is to avoid the bias ­ and hence the correction terms ­
by using alternative estimates of the deterministic terms. Breitung and Meyer (1994)
suggest using the initial value yi0 as an estimator of the constant term. As argued by Schmidt and Phillips (1992), the initial value is the best estimate of the constant
given the null hypothesis is true. Using this approach the regression equation for a
model with a constant term becomes

yit =  (yi,t-1 - yi0) + vit .

290

J. Breitung and M. Hashem Pesaran

Under the null hypothesis, the pooled t-statistic of H0 :   = 0 has a standard normal limit distribution.
For a model with a linear time trend a minimal invariant statistic is obtained by the transformation (cf. Ploberger and Phillips, 2002)

xit

=

yit

- yi0

-

t T

(yiT

- yi0)

.

In this transformation subtracting yi0 eliminates the constant and (yiT - yi0)/T = (yi1 + · · · + yiT )/T is an estimate of the slope of the individual trend function.
To correct for the mean of yit a Helmert transformation can be used

yit = st

yit

-

T

1 -

t

(yi, t +1

+

·

·

·

+

yiT

)

,

t = 1, . . . , T - 1

where st2 = (T - t)/(T - t + 1) (cf. Arellano, 2003, p. 17). Using these transformations the regression equation becomes

yit =  xi,t-1 + vit .

(9.27)

It is not difficult to verify that under the null hypothesis E(yit xi,t-1) = 0 and, thus, the t-statistic for   = 0 is asymptotically standard normally distributed (cf. Breitung, 2000).
It is important to note that including individual specific time trends substantially reduce the (local) power of the test. This was first observed by Breitung (2000) and studied more rigorously by Ploberger and Phillips (2002) and Moon et al. (2007). Specifically, the latter two papers show that a panel unit root test with incidental trends has nontrivial asymptotic power only for local alternatives with rate T -1N-1/4. A similar result is found by Moon, Perron and Phillips (2006) for the test suggested by Breitung (2000).
The test against heterogeneous alternatives H1b can be easily adjusted for individual specific deterministic terms such as linear trends or seasonal dummies. This can be done by computing IPS statistics, defined by (9.18) and (9.19) for the balanced and unbalanced panels, using Dickey­Fuller t-statistics based on DF regressions including the deterministics idit , where dit = 1 in the case of a constant term, dit = (1, t) in the case of models with a linear time trend and so on. The mean and variance corrections should, however, be computed to match the nature of the deterministics. Under a general setting IPS (2003) have shown that the ZIPS statistic converges in distribution to a standard normal variate as N, T  , jointly.
In a straightforward manner it is possible to include dummy variables in the vector dit that accommodate structural breaks in the mean function (see, e.g., Murray and Papell, 2002; Tzavalis, 2002; Carrion-I-Sevestre, Del Barrio and Lopez-Bazo, 2005; Breitung and Candelon, 2005; Im, Lee and Tieslau, 2005).

9 Unit Roots and Cointegration in Panels

291

9.2.6 Short-Run Dynamics

If it is assumed that the error in the autoregression (9.1) is a serially correlated stationary process, the short-run dynamics of the errors can be accounted for by including lagged differences

yit = idit + iyi,t-1 + i1yi,t-1 + · · · + i, pi yi,t-pi + it .

(9.28)

For example, the IPS statistics (9.18) and (9.19) developed for balanced and un-

balanced panels can now be constructed using the ADF(pi) statistics based on the above regressions. As noted in IPS (2003), small sample properties of the test can be

much improved if the standardization of the IPS statistic is carried out using the sim-
ulated means and variances of i(pi), the t-ratio of i computed based on ADF(pi) regressions. This is likely to yield better approximations, since E [i(pi)], for example, makes use of the information contained in pi while E [i(0)] = E(i) does not. Therefore, in the serially correlated case IPS propose the following standardized

t-bar statistic

 N
ZIPS =

¯

-

1 N

Ni=1

E

[i ( pi )]

1 N

iN=1 Var

[i ( pi )]

---d-- N (0, 1) .
(T, N)

(9.29)

The value of E [i(p)] and Var [i(p)] simulated for different values of T and p, are provided in Table 3 of IPS. These simulated moments also allow the IPS panel unit
root test to be applied to unbalanced panels with serially correlated errors. For tests against the homogenous alternatives, 1 = · · · = N =  < 0, Levin
et al. (2002) suggest removing all individual specific parameters within a first step regression such that eit (vi,t-1) are the residuals from a regression of yit (yi,t-1) on yi,t-1, . . . , yi,t-pi and dit . In the second step the common parameter  is estimated from a pooled regression

(eit /i) =  (vi,t-1/i) + it ,
where i2 is the estimated variance of eit . Unfortunately, the first step regressions are not sufficient to remove the effect of the short-run dynamics on the null distribution of the test. Specifically,

 lim E
T 

T

1 -

p

T
eit vi,t-1/i2
t = p+1

=

¯i i

a(^ ) ,

where



2 i

is

the

long-run

variance

and

a(^ )

denotes

the

limit

of

the

correction

term

given in (9.25). Levin et al. (2002) propose a nonparametric (kernel based) estimator

for ¯i2

292

J. Breitung and M. Hashem Pesaran

  s¯i2

=

1 T

T y~2it + 2 K

t=1

l=1

K+1-l K+1

T
 y~it y~i,t-l
t=l+1

,

(9.30)

where y~it denotes the demeaned difference and K denotes the truncation lag. As noted by Phillips and Ouliaris (1990), in a time series context the estimator of the long-run variance based on differences is inappropriate since under the stationary alternative s¯2i p 0 and, thus, using this estimator yields an inconsistent test. In contrast, in the case of panels the use of s¯i2 improves the power of the test, since with s¯i2 p 0 the correction term drops out and the test statistic tends to -.
It is possible to avoid the use of a kernel based estimator of the long-run variance by using an alternative approach suggested by Breitung and Das (2005). Under the null hypothesis we have
i(L)yit = idit + it ,
where i(L) = 1 - i1L - · · · - i,pi Lp and L is the lag operator. It follows that gt = i(L)[yit - E(yit )] is a random walk with uncorrelated increments. Therefore, the serial correlation can be removed by replacing yit by the pre-whitened variable y^it = i(L)yit , where i(L) is an estimator of the lag polynomial obtained from the leastsquare regression

yit = idit + i1yi,t-1 + · · · + i,pi yi,t-pi + it .

(9.31)

This approach may also be used for modifying the "unbiased statistic" based on the t-statistic of   = 0 in (9.27). The resulting t-statistic has a standard normal limiting distribution if as T   is followed by N  .
A related approach is suggested by Westerlund (2008). He suggests to test the
unit root hypothesis by running a modified ADF regression of the form

yit = idit + iyi,t-1 + i1yi,t-1 + · · · + i, pi yi,t-pi + it .

(9.32)

where yi,t-1 = (i/s¯i)yi,t-1 and s¯2i is a consistent estimator of the long-run variance,  i2. Westerlund (2008) recommends to use a parametric estimate of the longrun variance based on an autoregressive representation. This transformation of the lagged dependent variable eliminates the nuisance parameters in the asymptotic distribution of the ADF statistic and, therefore, the correction for the numerator of the corrected t-statistic of Levin et al. (2002) is the same as in the case without short-run dynamics.
Pedroni and Vogelsang (2005) have proposed a test statistic that avoids the specification of the short-run dynamics by using an autoregressive approximation. Their test statistic is based on the pooled variance ratio statistic

ZNwT

=

T ci(0) Ns2i

,

9 Unit Roots and Cointegration in Panels

293

where ci( ) = T -1 tT= +1 y~it y~i,t- , y~it = yit -^ idit and si2 is the untruncated Bartlett kernel estimator defined as si2 = T=+-1T+1(1 - | |/T )ci( ). As has been shown by Kiefer and Vogelsang (2002) and Breitung (2002), the limiting distribution of such
"nonparametric" statistics does not depend on nuisance parameters involved by the
short run dynamics of the processes. Accordingly, no adjustment for short-run dy-
namics is necessary.

9.2.7 Other Approaches to Panel Unit Root Testing
An important problem of combining Dickey­Fuller type statistics in a panel unit root test is that they involve a nonstandard limiting distribution. If the panel unit root statistic is based on a standard normally distributed test statistic zi, then N-1/2 iN=1 zi has a standard normal limiting distribution even for a finite N. In this case no correction terms need to be tabulated to account for the mean and the variance of the test statistic.
Chang (2002) proposes a nonlinear instrumental variable (IV) approach, where the transformed variable
wi,t-1 = yi,t-1e-ci|yi,t-1|
with ci > 0 is used as an instrument for estimating i in the regression yit = iyi,t-1 + it (which may also include deterministic terms and lagged differences). Since wi,t-1 tends to zero as yi,t-1 tends to ± the trending behavior of the nonstationary variable yi,t-1 is eliminated. Using the results of Chang, Park and Phillips (2001), Chang (2002) showed that the Wald test of  = 0 based on the nonlinear IV estimator possesses a standard normal limiting distribution. Another important property of the test is that the nonlinear transformation also takes account of possible contemporaneous dependence among the cross section units. Accordingly, Chang's panel unit root test is also robust against cross-section dependence.
It should be noted that wi,t-1  [-(cie)-1, (cie)-1] with a maximum (minimum) at yi,t-1 = 1/ci (yi,t-1 = -1/ci). Therefore, the choice of the parameter ci is crucial for the properties of the test. First, the parameter should be proportional to inverse of the standard deviations of yit . Chang notes that if the time dimension is small, the test slightly over-rejects the null and therefore she proposes to use a larger value of K to correct for the size distortions.
An alternative approach to obtain an asymptotically standard normal test statistic is to adjust the given samples in all cross-sections so that they all have sums of squares yi21 + · · · + yi2ki = i2cT 2 + hi, where hi p 0 as T  . In other words, the panel data set becomes an unbalanced panel with ki time periods in the i'th unit. Chang and Park (2004) calls this setting the "equi-squared sum contour", whereas the traditional framework is called the "equi-sample-size contour". The nice feature of this approach is that it yields asymptotically standard normal test statistics.

294

J. Breitung and M. Hashem Pesaran

An important drawback is, however, that a large number of observations may be discarded by applying this contour which may result in a severe loss of power.
Hassler, Demetrescu and Tarcolea (2006) have suggested to use the LM statistic for a fractional unit root as an asymptotically normally distributed test statistic. This test statistic is uniformly most powerful against fractional alternatives of the form (1 - L)dyit = it with d < 1. Although usually panel unit root tests are used to decide whether the series are I(1) or I(0), it can be argued that fractional unit root tests also have a good (albeit not optimal) power against the I(0) alternative (e.g. Robinson, 1994).
As in the time series case it is possible to test the null hypothesis that the series are stationary against the alternative that (at least some of) the series are nonstationary. The test suggested by Tanaka (1990) and Kwiatkowski, Phillips, Schmidt and Shin (1992) is designed to test the hypothesis H0 : i = 0 in the model

yit = idit + irit + uit , t = 1, . . . , T ,

(9.33)

where rit is white noise with unit variance and uit is stationary. The cross-section specific KPSS statistic is

 i

=

1 T 2¯T2,i

T
Si2t
t=1

,

where ¯T2,i denotes a consistent estimator of the long-run variance of yit and Sit = t =1 yi -^ idi is the partial sum of the residuals from a regression of yit on the deterministic terms (a constant or a linear time trend). The individual test statistics can be combined as in the test suggested by IPS (2003) yielding

¯ = N-1/2 iN=1 [i - E(i)] , Var(i)

where asymptotic values of E(i) and Var(i) are derived in Hadri (2000) and values for finite T and N   are presented in Hadri and Larsson (2005).

The test of Harris, Leybourne and McCabe (2004) is based on the stationarity

statistic

 Zi(k) = T ci(k)/zi(k) ,

where ci(k) denotes the usual estimator of the covariance at lag k of cross section unit i and z2i(k) is an estimator of the long-run variance of zikt = (yit -^ idit )(yi,t-k - ^ idi,t-k). The intuition behind this test statistic is that for a stationary and ergodic time series we have E[ci(k)]  0 as k  . Since z2i is a consistent estimator for the
variance of ci(k) it follows that Zi(k) converges to a standard normally distributed random variable as k   and k/ T   < .

9 Unit Roots and Cointegration in Panels

295

9.3 Second Generation Panel Unit Root Tests

9.3.1 Cross-Section Dependence

So far we have assumed that the time series {yit }tT=0 are independent across i. However, in many macroeconomic applications using country or regional data it is found that the time series are contemporaneously correlated. Prominent examples are the analysis of purchasing power parity and output convergence.5 The literature on modelling of cross section dependence in large panels is still developing and in what follows we provide an overview of some of the recent contributions.6
Cross section dependence can arise due to a variety of factors, such as omitted observed common factors, spatial spill over effects, unobserved common factors, or general residual interdependence that could remain even when all the observed and unobserved common effects are taken into account. Abstracting from common observed effects and residual serial correlation a general specification for cross sectional error dependence can be written as

yit = -ii + iyi,t-1 + uit ,

where

uit = i ft + it ,

(9.34)

or

ut =  f t +  t ,

(9.35)

ut = (u1t , u2t , . . . , uNt ) , ft is an m × 1 vector of serially uncorrelated unobserved common factors, and  t = (1t , 2t , . . . , Nt ) is an N × 1 vector of serially uncorrelated errors with mean zero and the positive definite covariance matrix  , and  is an N × m matrix of factor loadings defined by  = (1, 2, . . . , N) .7 Without loss
of generality the covariance matrix of ft is set to Im, and it is assumed that ft and  t are independently distributed. If 1 = · · · = N, then t =  ft is a conventional "time effect" that can be removed by subtracting the cross section means from the
data. In general it is assumed that i, the factor loading for the ith cross section unit,
differs across i and represents draws from a given distribution.
Under the above assumptions and conditional on i, i = 1, 2, . . . , N, the covariance matrix of the composite errors, ut , is given by  =  +  . It is clear that without further restrictions the matrices  and  are not separately identified. The properties of  also crucially depend on the relative eigenvalues of  and  , and their limits as N  . Accordingly two cases of cross-section dependence can

5 See, for example, O'Connell (1998) and Phillips and Sul (2003b). Tests for cross section independence of errors with applications to output growth equations are considered in Pesaran (2004). 6 A survey of the second generation panel unit root tests is also provided by Hurlin and Mignon (2004). 7 The case where ft and/or it might be serially correlated will be considered below.

296

J. Breitung and M. Hashem Pesaran

be distinguished: (i) Weak dependence. This cases arises if it is assumed that the

eigenvalues of  are bounded as N  . This assumption rules out the presence

of unobserved common factors, but allows the cross section units to be, for ex-

ample, spatially correlated with a finite number of "neighbors". (ii) Strong depen-

dence. In this case some eigenvalues of  are O(N), which arises when there are

unobserved common factors. When N is fixed as T   both sources of depen-

dence could be present. But for N   (and particularly when N > T ) it seems

only sensible to consider cases where rank() = m  1 and  is a diagonal matrix. A general discussion of the concepts of weak and strong cross section depen-

dence is provided in Pesaran and Tosetti (2007) where it is shown that all spatial

econometric models considered in the literature are examples of weak cross section

dependence.

A simple example of panel data models with weak cross section dependence is

given by

     

y1t

a1

y1,t-1

u1t

 ...  =  ...  +   ...  +  ... 

(9.36)

yNt

aN

yN , t -1

uNt

or

yt = a +  yt-1 + ut ,

(9.37)

where ai = - i and yt , yt-1, a and ut are N × 1 vectors. The cross-section correlation is represented by a non-diagonal matrix

 = E(ut ut ), for all t ,

with bounded eigenvalues. For the model without constants Breitung and Das (2005)

showed that the regression t-statistic of  = 0 in (9.37) is asymptotically distributed

as N (0, v) where

t r( 2 /N )

v

=

lim
N

(t r /N )2

.

(9.38)

Note that tr() and tr(2) are O(N) and, thus, v converges to a constant that can be shown to be larger than one. This explains why the test ignoring the cross-
correlation of the errors has a positive size bias.

9.3.2 Tests Based on GLS Regressions
Since (9.37) can be seen as a seemingly unrelated regression system, O'Connell (1998) suggests to estimate the system by using a GLS estimator (see also Flores, Jorion, Preumont and Szarfarz, 1999). Let  = T -1 tT=1 ut ut denote the sample covariance matrix of the residual vector. The GLS t-statistic is given by

9 Unit Roots and Cointegration in Panels

297

tgls(N) =

T


yt



-1
yt-1

t=1

,

T


yt-1

-1
yt-1

t=1

where yt is the vector of demeaned variables. Harvey and Bates (2003) derive the limiting distribution of tgls(N) for a fixed N and as T  , and tabulate its asymptotic distribution for various values of N. Breitung and Das (2005) show that if yt = yt - y0 is used to demean the variables and T   is followed by N  , then the GLS t-statistic possesses a standard normal limiting distribution.
The GLS approach cannot be used if T < N as in this case the estimated covari-
ance matrix  is singular. Furthermore, Monte Carlo simulations suggest that for reasonable size properties of the GLS test, T must be substantially larger than N (e.g. Breitung and Das, 2005). Maddala and Wu (1999) and Chang (2004) have suggested a bootstrap procedure that improves the size properties of the GLS test.

9.3.3 Test Statistics Based on OLS Regressions

An alternative approach based on "panel corrected standard errors" (PCSE) is considered by Jo¨nsson (2005) and Breitung and Das (2005). In the model with weak dependence, the variance of the OLS estimator  is consistently estimated by

var( ) =

T

 yt-1yt-1

t=1 T

2.

 yt-1yt-1

t=1

If T   is followed by N   the robust t statistic trob =  / var( ) is asymptotically standard normally distributed (Breitung and Das, 2005).
If it is assumed that the cross correlation is due to common factors, then the largest eigenvalue of the error covariance matrix, , is Op(N) and the robust PCSE approach breaks down. Specifically, Breitung and Das (2008) showed that in this case trob is distributed as the ordinary Dickey-Fuller test applied to the first principal component.
In the case of a single unobserved common factor, Pesaran (2008) has suggested a simple modification of the usual test procedure. Let y¯t = N-1 iN=1 yit and y¯t = N-1 Ni=1 yit = y¯t - y¯t-1. The cross section augmented Dickey­Fuller (CADF) test is based on the following regression
yit = ai + iyi,t-1 + biy¯t-1 + ciy¯t + eit .

298

J. Breitung and M. Hashem Pesaran

 In this regression the additional variables y¯t and y¯t-1 are N-consistent estimators for the rescaled factors ¯ ft and ¯ tj-=10 f j, where ¯ = N-1 iN=1 i. Pesaran (2008) showed that the distribution of the regression t-statistic for i = 0 is free of nuisance parameters. To test the unit root hypothesis in a heterogenous panel the average of

the N individual CADF t-statistics (or suitably truncated version of them) can be

used. Coakley, Kellard and Smaith (2005) apply the CADF test to real exchange

rates of 15 OECD countries.

9.3.4 Other Approaches

A similar approach was proposed by Moon and Perron (2004) and Phillips and Sul (2003a). The test of Moon and Perron (2004) is based on a principal components estimator of m < N common factors f1t , . . . , fmt in (9.34). The number of common factors can be consistently determined by using the information criteria suggested by Bai and Ng (2002). Let Vm = [v1, . . . , vm] denote the matrix of m orthonormal eigenvectors associated with m largest eigenvalues of . The vector of common factors are estimated as

ft = [ f1t , . . . , fmt ] = Vmyt .

As shown by Bai and Ng (2002), the principal component estimator ft yields a consistent estimator of the factor space as min(N, T )  . Thus, the elements of the vector

IN - VmVm yt  QVm yt ,

(9.39)

are consistent estimates of the idiosyncratic components it as N  . Therefore, by assuming that it is i.i.d., the pooled regression t-statistic

tM P =

T

t


=1

yt

QVm

yt-1

.

T

t


=1

yt

-1

QVm

yt

-1

has a standard normal limiting distribution as (N, T  ) and lim infN,T log N/ log T  0 (cf. Moon and Perron, 2004).
Hassler et al. (2006) suggest a simple correction for error cross-section dependences that can be used to combine the p-values of individual specific unit root tests, assuming that the correlation among the p-values is constant. The authors found that the suggested combination of p-values yields reliable results even in cases where the correlation is different among the cross-section units.

9 Unit Roots and Cointegration in Panels

299

9.4 Cross-Unit Cointegration

As argued by Banerjee, Marcellino and Osbat (2005) panel unit root tests may be

severely biased if the panel units are cross-cointegrated, namely if under the null

hypothesis (of unit roots) one or more linear combinations of yt are stationary. This needs to be distinguished from the case where the errors are cross correlated without

necessarily involving cointegration across the cross section units. Under the former

two or more cross section units must share at least one common stochastic trend.

Such a situation is likely to occur if the PPP hypothesis is examined (cf. Lyhagen,

2000, Banerjee et al., 2005; Wagner, 2007).

The tests proposed by Moon and Perron (2004) and Pesaran (2008) are based on

the model

yit = (1 - i) i + iyi,t-1 + i ft + it ,

(9.40)

Under the unit root hypothesis, i = 1, this equation yields

yit = yi0 + is f t + sit ,

where

s ft = f1 + f2 + . . . + ft , sit = i1 + i2 + . . . + it .

Clearly, under the null hypothesis all cross section units are related to the common stochastic component, s ft , albeit with varying effects, i. This framework rules out cross-unit cointegration as under the null hypothesis there does not exist a linear combination of y1t , . . . , yNt that is stationary. Therefore, tests based on (9.40) are designed to test the joint null hypothesis: "All time series are I(1) and not cointegrated".
To allow for cross-unit cointegration Bai and Ng (2004) proposed analyzing the common factors and idiosyncratic components separately. A simple multi-factor example of Bai and Ng framework is given by

yit = i + i gt + eit , gt = gt-1 + vt , eit = iei,t-1 + it ,
where gt is the m × 1 vector of unobserved components, vt and it are stationary common and individual specific shocks, respectively. Two different sets of null hypotheses are considered: H0a :(testing the I(0)/I(1) properties of the common fcators) Rank() = r  m, and H0b : (panel unit root tests) i = 1, for all i. A test of H0a is based on common factors estimated by principal components and cointegration tests are used to determine the number of common trends, m-r. Panel unit root tests are then applied to the idiosyncratic components. The null hypothesis that the time series have a unit root is rejected if either the test of the common factors or the test

300

J. Breitung and M. Hashem Pesaran

for the idiosyncratic component cannot reject the null hypothesis of nonstationary components.8 As has been pointed out by Westerlund (2007a), replacing the unobserved idiosyncratic components by estimates introduces an asymptotic bias when pooling the t-statistic (or p-values) of the panel units which renders the pooled tests in Bai and Ng (2004) asymptotically invalid. However, in a recent paper Bai and Ng (2007) show that pooled panel unit root tests can still be applied to the estimated idiosyncratic components if the tests are based on the pooled estimator of the largest autoregressive root.
The implementation of the Bai and Ng procedure requires estimates of m and r that might require very large N and T (e.g. Gengenbach, Palm and Urbain, 2006a). Note also that the panel structure of the data does not enhance the power of testing H0a, which primarily depends on the time dimension. The cross section dimension only helps in getting more precise estimates of the unobserved common factors. As a result, in panels of typical sample sizes we cannot hope to learn much about the order of integration by applying unit root or cointegration tests to the estimated common factors. Since the low power of testing H0a shrinks down the overall power of the combined test procedure it is very hard in practice to establish the stationarity of the variables even if N is extremely large.
To allow for short-run and long-run (cross-unit cointegration) dependencies, Chang and Song (2005) suggest a nonlinear instrument variable test procedure. As the nonlinear instruments suggested by Chang (2002) are invalid in the case of crossunit cointegration panel specific instruments based on the Hermit function of different order are used as nonlinear instruments. Chang and Song (2005) showed that the t-statistic computed from the nonlinear IV statistic are asymptotically standard normally distributed and, therefore, a panel unit statistics against the heterogeneous alternative H1b can be constructed that has an standard normal limiting distribution.
Choi and Chue (2007) employ a subsampling procedure to obtain tests that are robust against a wide range of cross-section dependence such as weak and strong correlation as well as cross-unit cointegration. To this end the sample is grouped into a number of overlapping blocks of b time periods. Using all (T - b + 1) possible overlapping blocks, the critical value of the test is estimated by the respective quantile of the empirical distribution of the (T - b + 1) test statistics computed. The advantage of this approach is that the null distribution of the test statistic may depend on unknown nuisance parameters. Whenever the test statistics converge in distribution to some limiting null distribution as T   and N fixed, the subsample critical values converge in probability to the true critical values. Using Monte Carlo simulations Choi and Chue (2007) demonstrate that the size of the subsample test is indeed very robust against various forms of cross-section dependence.

8 An alternative factor extraction method is suggested by Kapetanios (2007) who also provides detailed Monte Carlo results on the small sample performance of panel unit root tests based on a number of alternative estimates of the unobserved common factors. He shows that the factor-based panel unit root tests tend to perform rather poorly when the unobserved common factor is serially correlated.

9 Unit Roots and Cointegration in Panels

301

9.5 Finite Sample Properties of Panel Unit Root Tests

It has become standard to distinguish first generation panel unit root tests that are based on the assumption of independent cross section units and second generation tests that allow for some kind of cross-section dependence. Maddala and Wu (1999) compared several first generation tests. For the heterogeneous alternative under consideration they found that in most cases the Fisher test (9.20) performs similar or slightly better than the IPS statistic with respect to size and power. The Levin and Lin statistic (in the version of the 1993 paper) performs substantially worse. Similar results are obtained by Choi (2001). Madsen (2003) derived the local power function against homogeneous alternatives under different detrending procedures. Her Monte Carlo simulations support her theoretical findings that the test based on estimating the mean under the null hypothesis (i.e. the initial observation is subtracted from the time series) outperforms tests based on alternative demeaning procedures. Similar findings are obtained by Bond, Nauges and Windmeijer (2002).
Moon and Perron (2004) compare the finite sample powers of alternative tests against the homogeneous alternative. They found that the point-optimal test of Moon, Perron and Phillips (2007) performs best and show that the power of this test is close to the power envelope. Another important finding from these simulation studies is the observation that the power of the test drops dramatically if a time trend is included. This confirms theoretical results on the local power of panel unit root tests derived by Breitung (2000), Ploberger and Phillips (2002) and Moon et al. (2007).
Hlouskova and Wagner (2006) compare a large number of first generation panel unit root tests applied to processes with MA(1) errors. Not surprisingly, all tests are severely biased as the root of the MA process approaches unity. Overall, the tests of Levin et al. (2002) and Breitung (2000) have the smallest size distortions. These tests also perform best against the homogenous alternative, where the autoregressive coefficient is the same for all panel units. Of course this is not surprising as these tests are optimal under homogeneous alternatives. Furthermore, it turns out that the stationarity tests of Hadri (2000) perform very poorly in small samples. This may be due to the fact that asymptotic values for the mean and variances of the KPSS statistics are used, whereas Levin et al. (2002) and IPS (2003) provide values for small T as well.
The relative performance of several second generation tests have been studied by Gutierrez (2003), and Gengenbach et al. (2006a), where the cross-section dependence is assumed to follow a factor structure. The results very much depend on the underlying model. The simulations carried out by Gengenbach et al. (2006a) show that in general, the mean CADF test has better size properties than the test of Moon and Perron (2004), which tends to be conservative in small samples. However the latter test appears to have more power against stationary idiosyncratic components. Since these tests remove the common factors, they will eventually indicate stationary time series in cases where the series are actually nonstationary due to a common stochastic trend. The results of Gengenbach et al. (2006a) also suggest that

302

J. Breitung and M. Hashem Pesaran

the approach of Bai and Ng (2004) is able to cope with this possibility although the power of the unit test applied to the nonstationary component is not very high.
In general, the application of factor models in the case of weak correlation does not yield valid test procedures. Alternative unit root tests that allow for weak cross section dependence are considered in Breitung and Das (2005). They found that the GLS t-statistic may have a severe size bias if T is only slightly larger than N. In these cases Chang's (2004) bootstrap procedure is able to improve the size properties substantially. The robust OLS t-statistic performs slightly worse but outperforms the nonlinear IV test of Chang (2002). However, Monte Carlo simulations recently carried out by Baltagi, Bresson and Pirotte (2007) show that there can be considerable size distortions even in panel unit root tests that allow for weak dependence. Interestingly enough Pesaran's test, which is not designed for weak cross section dependence, tends to be the most robust to spatial type dependence.

9.6 Panel Cointegration: General Considerations
The estimation of long-run relationships has been the focus of extensive research in time series econometrics. In the case of variables on a single cross section unit the existence and the nature of long-run relations are investigated using cointegration techniques developed by Engle and Granger (1987), Johansen (1991, 1995) and Phillips (1991). In this literature residual-based and system approaches to cointegration are advanced. In this section we review the panel counter part of this literature. But before considering the problem of cointegration in a panel a brief overview of the cointegration literature would be helpful.
Consider the ni time series variables zit = (zi1t , zi2t , . . . , zinit ) observed on the ith cross section unit over the period t = 1, 2, . . . , T , and suppose that for each i
zi jt  I(1), j = 1, 2, . . . ., ni .
Then zit is said to form one or more cointegrating relations if there are linear combinations of zi jt 's for j = 1, 2, . . . , ni that are I (0) i.e. if there exists an ni × ri matrix (ri  1) such that
i zit =  it  I (0) . ri × ni ni × 1 ri × 1
ri denotes the number of cointegrating (or long-run) relations. The residual-based tests are appropriate when ri = 1, and zit can be partitioned such that zit = (yit , xit ) with no cointegration amongst the ki × 1 (ki = ni - 1) variables, xit . The system cointegration approaches are much more generally applicable and allow for ri > 1 and do not require any particular partitioning of the variables in zit .9 Another main difference between the two approaches is the way the stationary component of
9 System approaches to cointegration analysis that allow for weakly exogenous (or long-run forcing) variables has been considered in Pesaran, Shin and Smith (2000).

9 Unit Roots and Cointegration in Panels

303

 it is treated in the analysis. Most of the residual-based techniques employ nonparametric (spectral density) procedures to model the residual serial correlation in
the error correction terms,  it , whilst vector autoregressions (VAR) are utilized in the development of system approaches.

In panel data models the analysis of cointegration is further complicated by het-

erogeneity, unbalanced panels, cross section dependence, cross unit cointegration

and the N and T asymptotics. But in cases where ni and N are small such that iN=1ni is less than 10, and T is relatively large (T > 100), as noted by Banerjee,
Marcellino and Osbat (2004), many of these problems can be avoided by applying

the system cointegration techniques to the pooled vector, zt = (z1t , z2t , . . . , zNt ) . In this setting cointegration will be defined by the relationships  zt that could contain cointegration between variables from different cross section units as well as coin-

tegration amongst the different variables specific to a particular cross section unit.

This framework can also deal with residual cross section dependence since it allows

for a general error covariance matrix that covers all the variables in the panel.

Despite its attractive theoretical features, the `full' system approach to panel

cointegration is not feasible even in the case of panels with moderate values of

N and ni. See Sect. 9.10 below for further details. In practice, cross section cointegration can be accommodated using common factors as in the work of Bai and

Ng (2004), Pesaran (2006), Pesaran, Schuermann and Weiner (2004, PSW) and its

subsequent developments in Dees, di Mauro, Pesaran and Smith (2007, DdPS). Bai

and Ng (2004) consider the simple case where ni = 1 but allow N and T to be large. But their set up can be readily generalized so that cointegration within each cross

section unit as well as across the units can be considered. Following DdPS suppose that10

zit = id dt + i f ft +  it ,

(9.41)

for i = 1, . . . , N; t = 1, 2, . . . , T , and to simplify the exposition assume that ni = n, where as before dt is the s × 1 vector of deterministics (1,t) or observed common factors such as oil prices, ft is a m × 1 vector of unobserved common factors, id and i f are n × s and n × m associated unknown coefficient matrices,  it is an n × 1 vector of error terms.
Unit root and cointegration properties of zit , i = 1, 2, . . . , N, can be analyzed by allowing the common factors, ft , and/or the country-specific factors,  it , to have unit roots. To see this suppose

ft =  (L)  t ,  t  IID (0, Im) ,
 it = i (L) vit , vit  IID (0, In) , where L is the lag operator and

(9.42) (9.43)

10 DdPS also allow for common observed macro factors (such as oil prices), but they are not included to simplify the exposition.

304

J. Breitung and M. Hashem Pesaran





 (L) =   L , i (L) =  i L .

=0 m×m

=0 n×n

(9.44)

The coefficient matrices,  and i , i = 1, 2, . . . , N, are absolute summable, so that Var (ft ) and Var ( it ) are bounded and positive definite, and [i (L)]-1 exists. In particular we require that


||  i i ||  K <  , =0

(9.45)

where K is a fixed constant. Using the familiar decomposition

 (L) =  (1) + (1 - L) (L) , and i (L) = i (1) + (1 - L)i (L) ,

the common stochastic trend representations of (9.42) and (9.43) can now be written
as ft = f0 +  (1) st +  (L) ( t -  0) ,

and  it =  i0 + i (1) sit + i (L) (vit - vi0) ,

where

t

t

st =   j and sit =  vi j .

j=1

j=1

Using the above results in (9.41) now yields

zit = ai + iddt + i f  (1) st + i (1) sit +i f  (L) t + i (L) vit ,

where11

ai = i f [f0 -  (L)  0] +  i0 - i (L) vi0 .

In this representation  (1) st and i (1) sit can be viewed as common global and individual-specific stochastic trends, respectively; whilst  (L)  t and i (L) vit are the common and individual-specific stationary components. From this result it is
clear that, in general, it will not be possible to simultaneously eliminate the two
types of the common stochastic trends (global and individual-specific) in zit . Specific cases of interest where it would be possible for zit to form a cointegrating
vector are when  (1) = 0 or i (1) = 0. Under the former panel cointegration exists if i (1) is rank deficient. The number of cointegrating relations could differ across i and is given by ri = n - Rank [i (1)]. Note that even in this case zit can be crosssectionally correlated through the common stationary components,  (L)  t . Under i (1) = 0 for all i with  (1) = 0, we will have panel cointegration if there exists

11 In usual case where dt is specified to include an intercept, 1, ai can be absorbed into the deterministics.

9 Unit Roots and Cointegration in Panels

305

n × ri matrices i such that i i f  (1) = 0. Notice that this does not require  (1) to be rank deficient.
Turning to the case where  (1) and i (1) are both non-zero, panel cointegration could still exist but must involve both zit and ft . But since ft is unobserved it must be replaced by a suitable estimate. The global VAR (GVAR) approach of
Pesaran et al. (2004) and Dees et al. (2007) implements this idea by replacing ft with the (weighted) cross section averages of zit . To see how this can be justified first differencing (9.41) and using (9.43) note that

[i (L)]-1 (1 - L) zit - iddt - i f ft = vit .

Using the approximation
p
(1 - L) [i (L)]-1   i L = i (L, p) , =0
we obtain the following approximate VAR(p) model

i (L, p) zit - iddt - i f ft  vit .

(9.46)

When the common factors, ft , are observed the model for the ith cross-section unit decouples from the rest of the units and can be estimated using the econometric
techniques developed in Pesaran et al. (2000) with ft treated as weakly exogenous. But in general where the common factors are unobserved appropriate proxies for
the common factors can be used. There are two possible approaches, one could
either use the principal components of the observables, zit , or alternatively following Pesaran (2006) ft can be approximated in terms of z¯t = N-1iN=1zit , the cross section averages of the observables. To see how this procedure could be justified in the
present context, average the individual equations given by (9.41) over i to obtain

z¯t = ¯ ddt + ¯ f ft + ¯ t ,

(9.47)

where ¯ d = N-1iN=1id, ¯ f = N-1iN=1i f , and ¯ t = N-1iN=1 it . Also, note from (9.43) that

N
 ¯ t - ¯ t-1 = N-1  j (L) v jt . j=1

(9.48)

But using results in Pesaran (2006), for each t and as N   we have ¯ t -¯ t-1 q.m. 0, and hence ¯ t q.m. ¯ , where ¯ is a time-invariant random variable. Using this result in (9.47) and assuming that the n × m average factor loading coefficient matrix, ¯ f , has full column rank (with n  m) we obtain

ft q.m. ¯ f ¯ f -1 ¯ f z¯t - ¯ ddt - ¯ ,

306

J. Breitung and M. Hashem Pesaran

which justifies using the observable vector {dt , z¯t } as proxies for the unobserved common factors.
The various contributions to the panel cointegration literature will now be reviewed in the context of the above general set up. First generation literature on panel cointegration tends to ignore the possible effects of global unobserved common factors, or attempts to account for them either by cross-section de-meaning or by using observable common effects such as oil prices or U.S. output. This literature also focusses on residual based approaches where it is often assumed that there exists at most one cointegrating relation in the individual specific models. Notable contributions to this strand of the literature include Kao (1999); Pedroni (1999, 2001, 2004); and more recently Westerlund (2005a). System approaches to panel cointegration that allow for more than one cointegrating relations include the work of Larsson, Lyhagen and Lothgren (2001), Groen and Kleibergen (2003) and Breitung (2005) who generalized the likelihood approach introduced in Pesaran, Shin and Smith (1999). Like the second generation panel unit root tests, recent contributions to the analysis of panel cointegration have also emphasized the importance of allowing for cross section dependence which, as we have noted above, could be due to the presence of common stationary or non-stationary components or both. The importance of allowing for the latter has been emphasized in Banerjee et al. (2004) through the use of Monte Carlo experiments in the case of panels where N is very small, at most 8 in their analysis. But to date a general approach that is capable of addressing all the various issues involved does not exist if N is relatively large.
We now consider in some further detail the main contributions, beginning with a brief discussion of the spurious regression problem in panels.

9.7 Residual-Based Approaches to Panel Cointegration

Under this approach zit is partitioned as zit = (yit , xit ) and the following regressions

yit = idit + xit  + uit , i = 1, 2, . . . , N ,

(9.49)

are considered, where as before idit represent the deterministics and the k × 1 vector of regressors, xit , are assumed to be I(1) and not cointegrated. However, the innovations in xit , denoted by it = xit - E(xit ), are allowed to be correlated
with uit . Residual-based approaches to panel cointegration focus on testing for unit
roots in OLS or panel estimates of uit .

9.7.1 Spurious Regression
Let wit = (uit , it ) and assume that the conditions for the functional central limit theorem are satisfied such that

9 Unit Roots and Cointegration in Panels

307

 1
T

[·T ]
wit
t=1

d

1i /2Wi(·)

,

where Wi is a (k + 1) × 1 vector of standard Brownian motions, d denotes weak convergence on D[0, 1] and

i =

i2, u  i, u

 i, u i, 

.

Kao (1999) showed that in the homogeneous case with i = , i = 1, . . . , N, and
abstracting from the deterministics, the OLS estimator  converges in probability to the limit -1 u, where it is assumed that wit is i.i.d. across i. In the heterogeneous case  and  u are replaced by the means  = N-1 iN=1 i, and  u = N-1 Ni=1  i,u, respectively (cf. Pedroni, 2000). In contrast, the OLS estimator of  fails to converge within a pure time series framework (cf. Phillips 1987). On the other hand, if xit and yit are independent random walks, then the t-statistics for the hypothesis that one component of  is zero is Op(T 1/2) and, therefore, the tstatistic has similar properties as in the time series case. As demonstrated by Entorf
(1997) and Kao (1999), the tendency for spuriously indicating a relationship among
yit and xit may even be stronger in panel data regressions as in the pure time series case. Therefore, it is important to test whether the errors in a panel data regression
like (9.49) are stationary.

9.7.2 Tests of Panel Cointegration

As in the pure time series framework, the variables in a regression function can be tested against cointegration by applying unit roots tests of the sort suggested in the previous sections to the residuals of the estimated regression. Unfortunately, panel unit root tests cannot be applied to the residuals in (9.49) if xit is (long-run) endogenous, that is, if  u = 0. Letting T   be followed by N  , Kao (1999) show that the limiting distribution of the DF t-statistic applied to the residuals of a pooled OLS regression of (9.49) is

 (t - N K)/K

d

N (0, 1) ,

(9.50)

where the values of K and K depend on the kind of deterministics included in the
regression, the contemporaneous covariance matrix E(wit wit ) and the long-run covariance matrix i. Kao (1999) proposed adjusting t by using consistent estimates of K and K, where he assumes that the nuisance parameters are the same for all
panel units (homogenous short-run dynamics).
Pedroni (2004) suggest two different test statistics for the models with heterogeneous cointegration vectors. Let uit = yit - ^ idit - i xit denote the OLS residual of

308

J. Breitung and M. Hashem Pesaran

the cointegration regression. Pedroni considers two different classes of test statistics: (i) the "panel statistic" that is equivalent to the unit root statistic against homogeneous alternatives and (ii) the "Group Mean statistic" that is analogous to the panel unit root tests against heterogeneous alternatives. The two versions of the t statistic are defined as

NT

-1/2 N T

N

  panel ZPt = ~N2T

u^2i, t -1

  u^i,t-1u^it - T  ^ i

i=1 t=1

i=1 t=1

i=1

N

T

-1/2 T

  group-mean ZPt =

^i2e u^2i,t-1

 u^i,t-1u^it - T ^ i

i=1

t=1

t=1

where ^ i is a consistent estimator of the one-sided long run variance i = j=1

E(eit ei,t- j), eit = uit - iui,t-1, i = E(uit ui,t-1)/E(u2i,t-1), ^i2e denotes the esti-

mated variance of eit

and

~ p,

~

2 p

such

that

and (ZPt

~ N2 T

=

N-1 iN=1 ^i2e.

Pedroni presents

values

of

p,



2 p

- p N)/p and (ZPt - ~ p N)/~ p have standard nor-

mal limiting distributions under the null hypothesis.

Other residual-based panel cointegration tests include the recent contribution of

Westerlund (2005a) that are based on variance ratio statistics and do not require

corrections for the residual serial correlations.

The finite sample properties of some residual based test for panel cointegration

are discussed in Baltagi and Kao (2000). Gutierrez (2003) compares the power of

various panel cointegration test statistics. He shows that in homogeneous panels

with a small number of time periods Kao's tests tend to have higher power than

Pedroni's tests, whereas in panels with large T the latter tests performs best. Both

test outperform the system test suggested by Larssen et al. (2001). Hlouskova and

Wagner (2007) compare various panel cointegration tests in a large scale simula-

tion study. They found that Pedroni's (2004) test based on ADF regressions per-

forms best, whereas all other tests tend to be severely undersized and have very low

power in may cases. Furthermore, the system tests suffer from large small sample

distortions and are unreliable tools for finding out the correct cointegration rank.

Gengenbach et al. (2006b) investigate the performance of Pedroni's tests in cross-

dependent models with a factor structure.

9.8 Tests for Multiple Cointegration
It is also possible to adapt Johansen's (1995) multivariate test based on a VAR representation of the variables. Let i(r) denote the cross-section specific likelihood-ratio ("trace") statistic of the hypothesis that there are (at most) r stationary linear combinations in the cointegrated VAR system given by zit = (yit , xit ) . Following the unit root test proposed in IPS (2003), Larsson et al. (2001) suggested the standardized LR-bar statistic

9 Unit Roots and Cointegration in Panels

309

(r)

=

1 N

N

i=1

i(r) - E[i(r)] Var[i(r)]

,

to test the null hypothesis that r = 0 against the alternative that at most r = r0  1. Using a sequential limit theory it can be shown that (r) is asymptotically standard normally distributed. Asymptotic values of E[i(r)] and Var[i(r)] are tabulated in Larsson et al. (2001) for the model without deterministic terms and Breitung (2005) for models with a constant and a linear time trend. Unlike the residual-based tests, the LR-bar test allows for the possibility of multiple cointegration relations in the panel.
It is also possible to test the null hypothesis that the errors of the cointegration regression are stationary. That is, under the null hypothesis it is assumed that yit , xit are cointegrated with cointegration rank r = 1. McCoskey and Kao (1998) suggest a panel version of Shin's (1994) cointegration test based on the residuals of a fully modified OLS regression. Westerlund (2005b) suggests a related test procedure based on the CUSUM statistic.

9.9 Estimation of Cointegrating Relations in Panels
9.9.1 Single Equation Estimators
First, we consider a single-equation framework where it is assumed that yit and the k × 1 vector of regressors xit are I(1) with at most one cointegrating relations amongst them, namely that there exists a linear relationship of the form (9.49) such that the error uit is stationary. As before it is assumed that zit = (yit , xit ) is i.i.d. across i, and the regressors, xit , are not cointegrated. We do not explicitly consider deterministic terms like individual specific constants or trends as the asymptotic theory applies to mean- or trend-adjusted variables as well.
It is assumed that the vector of coefficients,  , is the same for all cross-section units, that is, a homogeneous cointegration relationship is assumed. Alternatively, it may be assumed that the cointegration parameters are cross section specific (heterogenous cointegration).
By applying a sequential limit theory it can be shown that the OLS estimator of  is T N consistent and, therefore, the time series dimension is more informative on the long-run coefficients than the cross-section dimension. Furthermore, is important to notice that ­ as in the time series framework ­ the OLS estimator is consistent but inefficient in the model with endogenous regressors.
Pedroni (1995) and Phillips and Moon (1999, p. 1085) proposed a "fullymodified OLS" (FM-OLS) approach to obtain an asymptotically efficient estimator for homogenous cointegration vectors. This estimator adjusts for the effects of endogenous regressors and short-run dynamics of the errors (cf. Phillips and Hansen, 1990). To correct for the effect of (long-run) endogeneity of the regressors,

310

J. Breitung and M. Hashem Pesaran

the dependent variable is adjusted for the part of the error that is correlated with the

regressor

yi+t

=

yit

-

i,

u

-1 i, 

xit

.

(9.51)

A second correction is necessary when computing the OLS estimator

NT

-1 N T

  FM =

xit xit

 (xit yi+t - i,u) ,

i=1 t=1

i=1 t=1

(9.52)

where



  i,u = E

i,t- juit .

j=0

The nuisance parameters can be estimated consistently using familiar nonparametric procedures.
An alternative approach is the "Dynamic OLS" (DOLS) estimator suggested by Saikkonen (1991). This estimator is based on the error decomposition



 uit =

k  xi,t+k + vit ,

k=-

(9.53)

where vit is orthogonal to all leads and lags of xit . Inserting (9.53) in the regression

(9.49) yields



 yit =  xit +

k  xi,t+k + vit .

(9.54)

k=-

In practice the infinite sums are truncated at some small numbers of leads and lags (cf. Kao and Chiang, 2000, Mark and Sul, 2003). Westerlund (2005c) considers data dependent choices of the truncation lags. Kao and Chiang (2000) show that in the homogeneous case with i =  and individual specific intercepts the limiting distribution of the DOLS estimator DOLS is given by

 T N(DOLS -  )

d

N

(0, 6 u2| -1) ,

where

u2| = u2 - u-1u .

Furthermore, the FM-OLS estimator possesses the same asymptotic distribution

as the DOLS estimator. In the heterogeneous case  and u2| are replaced by



= N-1 Ni=1 i, 

and



2 u|

= N-1 iN=1 i2, u| ,

respectively

(cf.

Phillips

and

Moon, 1999). Again, the matrix i can be estimated consistently (for T  ) by

using a nonparametric approach.

In many applications the number of time periods is smaller than 20 and, therefore,

the kernel based estimators of the nuisance parameters may perform poorly in such

9 Unit Roots and Cointegration in Panels

311

small samples. Pesaran et al. (1999) adapted a parametric model to estimate the cointegration vector based on the error correction format

yit = iyi,t-1 + i xit + vit ,

(9.55)

where for simplicity of exposition we have abstracted from deterministics and lagged changes in yit and xit .12 It is assumed that the long-run parameters are identical across the cross section units, i.e., i = -i/i =  for i = 1, . . . , N. Economic theory often predicts the same cointegration relation(s) across the cross section units, although is often silent on the magnitude of short-run dynamics, i, across i. For example, the long-run relationships predicted by the PPP, the uncovered inter-
est parity, or the Fisher equation are the same across countries, although the speed
of convergence to these long-run relations could differ markedly over countries due to differences in economic and political institutions.13 For further discussions see,
for example, Pesaran (1997). Letting it ( ) = yi,t-1 -  xit , the model is rewritten as

 yit = iit ( ) + vit .

(9.56)

Pesaran et al. (1999) have suggested an ML estimation method based on the concentrated likelihood function

 Lc( )

=

c

N
-
i=1

T 2

log |i2,v( )|

,

(9.57)

where c is a constant, and

 i2,v( )

=

1 T

T
vit ( )2,
t=1

T



vit ( )

=

yit

-



 yit
t=1 T

it

(

)

 it ( )

.

 it ( )2

t=1

Pesaran et al. (1999) suggested a Gauss­Newton algorithm to maximize (9.57). The means of the error correction coefficients are estimated by the simple average of the individual coefficients (or the ML estimates) of i, i = 1, . . . , N. This estimator is called the pooled mean group estimator.

12 Since there are no restrictions on the additional variables they can be concentrated out from the likelihood function by replacing yit , yi,t-1 and xit by residuals obtained from regressions on the deterministic terms and lagged differences of the variables,  zi,t-1,  zi,t-2, . . . 13 The problem of testing the slope homogeneity hypothesis in panels is reviewed in Hsiao and
Pesaran (2008).

312
9.9.2 System Estimators

J. Breitung and M. Hashem Pesaran

The single equation estimators have several drawbacks that can be avoided by using a system approach. First, these estimators assume that all regressors are I(1) and not cointegrated. If there are more than one cointegration relationships, then the matrix  is singular and the asymptotic results are no longer valid. Second, the cointegration relationship has to be normalized such that the variable yit enters with unit coefficient. As has been argued by Boswijk (1995), this normalization is problematical if the original coefficient of the variable yit tends to zero.
In the case of short panels with T fixed and N large, Binder, Hsiao and Pesaran (2005) consider estimation and inference in panel vector autoregressions (PVARs) with homogeneous slopes where (i) the individual effects are either random or fixed, (ii) the time-series properties of the model variables are unknown a priori and may feature unit roots and cointegrating relations. Generalized Method of Moments (GMM) and Quasi Maximum Likelihood (QML) estimators are obtained and compared in terms of their asymptotic and finite sample properties. It is shown that the asymptotic variances of the GMM estimators that are based on levels as well as first-differences of the model variables depend on the variance of the individual effects; whereas by construction the fixed effects QML estimator is not subject to this problem. Monte Carlo evidence is provided showing that the fixed effects QML estimator tends to outperform the various GMM estimators in finite sample under both normal and non-normal errors. The paper also shows how the fixed effects QML estimator can be successfully used for unit root and cointegration tests in short panels.
In the case of panels with large N and T , Larsson and Lyhagen (1999), Groen and Kleibergen (2003) and Breitung (2005) consider the vector error correction model (VECM) for the k + 1 dimensional vector zit = (yit , xit ) given by

zit = ii zi,t-1 + wit ,

(9.58)

where wit = (uit , it ) and once again we leave out deterministic terms and lagged differences. To be consistent with the approaches considered above, we confine ourselves to the case of homogenous cointegration, that is, we let i =  for i = 1, . . . , N. Larsson and Lyhagen (1999) propose a ML estimator, whereas the estimator of Groen and Kleibergen (2003) is based on a nonlinear GMM approach.
It is well known that the ML estimator of the cointegration parameters for a single series may behave poorly in small samples. Phillips (1994) has shown that the finite sample moments of the estimator do not exist. Using Monte Carlo simulations Hansen, Kim and Mittnik (1998) and Bru¨ggemann and Lu¨tkepohl (2005) found that the ML estimator may produce implausible estimates far away from the true parameter values. Furthermore the asymptotic 2 distribution of the likelihood ratio test for restrictions on the cointegration parameters may be a poor guide for small sample inference (e.g. Gredenhoff and Jacobson, 2001).
To overcome these problems, Breitung (2005) proposed a computationally convenient two-step estimator, which is adopted from Ahn and Reinsel (1990). This

9 Unit Roots and Cointegration in Panels

313

estimator is based on the fact that the Fisher information is block-diagonal with re-
spect to the short and long-run parameters. Accordingly, an asymptotically efficient
estimator can be constructed by estimating the short and long-run parameters in separate steps. Suppose that the n × r matrix of cointegrating vectors is "normalized" as  = (Ir, B) , where Ir is the identity matrix of order r and B is the (n - r) × r matrix of unknown coefficients.14 Then  is exactly identified and the Gaussian ML estimator of B is equivalent to the OLS estimator of B in

zit = Bz(i,2t)-1 + vit ,

(9.59)

where zi(t2) is the r × 1 vector defined by zit = [z(it1) , z(it2) ] , and
zit = ( ii-1 i)-1 ii-1zit - z(i,1t)-1 .
 The matrices  i and i can be replaced by T -consistent estimates without affecting the limiting distribution. Accordingly, these matrices can be estimated for each panel unit separately, e.g., by using Johansen's (1991) ML estimator. To obtain the same normalization as in (9.59) the estimator for  i is multiplied with the r × r upper block of the ML estimator of  .
Breitung (2005) showed that the limiting distribution of the OLS estimator of B is asymptotically normal. Therefore, tests of restrictions on the cointegration parameters have the standard limiting distributions (i.e. a 2 distribution for the usual Wald tests).
Some Monte Carlo experiments were performed by Breitung (2005) to compare the small sample properties of the two-step estimator with the FM-OLS and DOLS estimators. The results suggest that the latter two tests may be severely biased in small samples, whereas the bias of the two-step estimator is relatively small. Furthermore, the standard errors (and hence the size properties of the tstatistics) of the two-step procedure are more reliable than the ones of the semiparametric estimation procedures. In a large scale simulation study, Hlouskova and Wagner (2007) found that the DOLS estimator outperforms all other estimators, whereas the FM-OLS and the two-step estimator perform similarly.

9.10 Cross-Section Dependence and the Global VAR
As pointed out earlier an important limitation of the econometric approaches discussed so far is that they assume that all cross-section units are independent. In many applications based on multi-country data sets this assumption is clearly unrealistic.
14 The analysis can be readily modified to take account of other types of exact identifying restrictions on  that might be more appropriate from the view-point of long-run economic theory. See Pesaran and Shin (2002) for a general discussion of identification and testing of cointegrating relations in the context of a single cross section units.

314

J. Breitung and M. Hashem Pesaran

To accommodate cross-dependence among panel units Mark, Ogaki and Sul (2004)

and Moon and Perron (2007) proposed a Dynamic Seemingly Unrelated Regres-

sion (DSUR) estimator. Their approach is based on a GLS estimator of the dynamic

representation (9.54) when there exists a single cointegrating relation between yit and xit , and does not allow for the possibility of cross unit cointegration. Let

hit (p) = xi,t-p , . . . , xi,t+p and hpt = (h1t (p) , . . . , hNt (p) ) . To correct for

endogeneity of the regressors, first yit and xit are regressed on hpt . Let y~it and x~it denote the resulting regression residuals. Furthermore, define y~t = (y~1t , . . . , y~Nt )

and Xt = (x~1t , . . . , x~Nt ) . The DSUR estimator of the (homogeneous) cointegration

vector is

T-p

T-p

  dsur =

Xt -uu1Xt

Xt u-u1y~t

(9.60)

t = p+1

t = p+1

where uu denotes the long-run covariance matrix of ut = (u1t , . . . , uNt ) , namely

uu

=

lim
T 

1 T

E

T
 ut
t=1

T
 ut
t=1

,

for a fixed N. This matrix is estimated by using an autoregressive representation of ut. See also (9.53). An alternative approach is suggested by Breitung (2005), where a SUR procedure is applied in the second step of the two-step estimator.
To estimate panel data regression models with a multifactor error structure Pesaran (2006) proposed the common correlated effects (CCE) estimator. The basic idea of this estimation procedure is to filter the individual-specific regressors by means of cross-section averages such that the differential effects of unobserved common factors are eliminated. An extension of this analysis to non-stationary common factors is provided in Kapetanios, Pesaran and Yamagata (2006). Also as shown by Kapetanios (2007) the CCE estimator can be applied to a cointegrated panel data regression. An empirical application to the analysis of house prices in the US is provided in Holly, Pesaran and Yamagata (2007).
Bai and Kao (2005), Westerlund (2007b), and Bai, Kao and Ng (2007) suggest estimators for the cointegrated panel data model given by

yit =  xit + i ft + eit

(9.61)

where ft is a r × 1 vector of common factors and eit is the idiosyncratic error. Bai and Kao (2005) and Westerlund (2007b) assume that ft is stationary. They suggest an FM-OLS cointegration regression that accounts for the cross-correlation due to the common factors. Bai et al. (2007) consider a model with nonstationary factors. Their estimation procedure is based on a sequential minimization of the criterion function

NT

  SNT ( , f1, . . . , fT , 1, . . . , N) =

(yit -  xit - i ft )2

i=1 t=1

(9.62)

9 Unit Roots and Cointegration in Panels

315

subject to the constraint T -2 tT=1 ft ft = Ir and iN=1 ii being diagonal. The asymptotic bias of the resulting estimator is corrected for by using an additive bias adjustment term or by using a procedure similar to the FM-OLS estimator suggested by Phillips and Hansen (1990).
A common feature of these approaches is that cross-section dependence can be represented by a contemporaneous correlation of the errors, and do not allow for the possibility of cross unit cointegration. In many applications it is more realistic to allow for some form of dynamic cross-section dependence. A general model to accommodate cross-section cointegration and dynamic links between panel units is the panel VECM model considered by Groen and Kleibergen (2003) and Larsson and Lyhagen (1999). As in Sect. 9.6, let zit denote a n-dimensional vector of times series on the ith cross section unit. Consider the nN × 1 vector zt = (z1t , . . . , zNt ) of all available time series in the panel data set. The VECM representation of this time series vector is

zt = zt-1 + 1zt-1 + · · · + pzt-p + ut .

(9.63)

For cointegrated systems rank( ) < nN. It is obvious that such systems typically involve a large number of parameters as the number of parameters increases with N2. Therefore, to obtain reliable estimates of the parameters T must be considerably
larger than N. In many macroeconomic applications, however, the number of time
periods is roughly as large as the number of cross-section units. Therefore, a simple structure must be imposed on the matrices , 1, . . . , p that yields a reasonable approximation to the underlying dynamic system.
The Global VAR (GVAR) introduced by Pesaran, Schuermann and Weiner (2004)
and further developed in Dees et al. (2007) can be seen as a theory guided reduction
of the general dynamic model given by (9.41), (9.42) and (9.43). In the context of
this set up the individual cross section models in the GVAR can be approximated by the VARZ(pi, pi) in zit and zit :15

i (L, pi) zit - id dt - i f zit = vit .

(9.64)

for i = 1, 2, . . . , N, where dt are the observed common effects (such as intercepts, time trends or oil prices), and zit is defined by
N
 zit = wi jz jt . j=1

The weights, wi j, j = 1, 2, . . . , N must satisfy the following conditions

N

N

wii = 0,  wi j = 1, and  w2i j  0, as N   ,

j=1

j=1

15 VARZ* represents a VAR model augmented with zit as weakly exogenous variables.

316

J. Breitung and M. Hashem Pesaran

and could be time varying. Typical examples of such weights are wi j = 1/(N - 1), for i = j, trade weights or other measures of economic distance between the cross
section units. The estimation of (9.64) can proceed by treating the cross section av-
erages as weakly exogenous I(1) variables using standard time series cointegration techniques developed, for example, in Pesaran et al. (2000). The assumption that zit are weakly exogenous I(1), or long-run forcing, for zit , can be tested. For further details see Pesaran et al. (2004). It turns out that this is a reasonable assumption for
all countries except for the U.S. where most of the variables should be treated as
endogenous.

9.11 Concluding Remarks
As this review shows the literature on panel unit roots and cointegration has been expanding very rapidly; in part responding to the complex nature of the interactions and dependencies that generally exist over time and across the individual units in the panel. Observations on firms, industries, regions and countries tend to be cross correlated as well as serially dependent. The problem of cross section dependence is particularly difficult to deal with since it could arise for a variety of reasons; spatial spill over effects, common unobserved shocks, social interactions or a combination of these factors. Parameter heterogeneity and deterministics also pose additional difficulties and how they are treated under the null and the alternative hypothesis can affect the outcome of the empirical analysis.
Initially, the panel unit root and cointegration tests were developed assuming that the errors of the individual equations are cross sectionally independent. These, referred to as the first generation tests, continue to form an important part of the literature, providing a theoretical basis for the more recent (second generation) developments that attempt to take account of the residual cross section dependence in panels in the case of panels where the time dimension and the cross section dimension are both relatively large. In the analysis of cointegration the hypothesis testing and estimation problems are also further complicated by the possibility of cross section cointegration. These and other issues are currently the subject of extensive research.
Acknowledgments We are grateful to Jushan Bai, Badi Baltagi, Jaroslava Hlouskova, George Kapetanios, Uwe Hassler, Serena Ng, Elisa Tosetti, Ron Smith, and Joakim Westerlund and an anonymous referee for comments on a preliminary version of this chapter.

References
Ahn, S.K. and G.C. Reinsel (1990), Estimation for Partially Nonstationary Multivariate Autoregressive Models, Journal of the American Statistical Association, 85, 813­823.
Andrews, D.W.K. (1998), Hypothesis Testing with a Restricted Parameter Space, Journal of Econometrics, 84, 155­199.

9 Unit Roots and Cointegration in Panels

317

Arellano, M. (2003), Panel Data Econometrics, Oxford: Oxford University Press. Bai J. and C. Kao (2005), On the Estimation and Inference of a Panel Cointegration Model
with Cross-Sectional Dependence, in: B. Baltagi (Ed.), Contributions to Economic Analysis, Elsevier, Amsterdam). Bai, J. and S. Ng (2002), Determining the Number of Factors in Approximate Factor Models, Econometrica, 70, 191­221. Bai, J. and S. Ng (2004), A Panic Attack on Unit Roots and Cointegration, Econometrica, 72, 1127­1177. Bai, J. and S. Ng (2007), Panel Unit Root Tests with Cross-Section Dependence: A Further Investigation, New York University, unpublished. Bai, J., C. Kao and S. Ng (2007), Panel Cointegration with Global Stochastic Trends, New York University, unpublished. Baltagi, B.H. and C. Kao (2000), Nonstationary Panels, Cointegration in Panels and Dynamic Panels, A Survey, in: B. Baltagi (ed.), Nonstationary Panels, Panel Cointegration, and Dynamic Panels, Advances in Econometrics, Vol. 15, JAI Press, Amsterdam, 7­52. Baltagi, B.H., G. Bresson, and A. Pirotte, (2007), Panel Unit Root Tests and Spatial Dependence, Journal of Applied Econometrics, 22, 339­360. Banerjee, A. (1999), Panel Data Unit Roots and Cointegration: An Overview, Oxford Bulletin of Economics and Statistics, 61, 607­629. Banerjee, A., M. Marcellino and C. Osbat (2004), Some Cautions on the Use of Panel Methods for Integrated Series of Macroeconomic Data, Econometrics Journal, 7, 322­340. Banerjee, A., M. Marcellino and C. Osbat (2005), Testing for PPP: Should we use Panel Methods? Empirical Economics, 30, 77­91. Binder, M., Hsiao, C. and M. H. Pesaran (2005), Estimation and Inference in Short Panel Vector Autoregressions with Unit Roots and Cointegration, Econometric Theory 21, 795­837. Bond, S., C. Nauges and F. Windmeijer (2002), Unit Roots and Identification in Autoregressive Panel Data Models: A Comparison of Alternative Tests, unpublished. Boswijk, H. P. (1995). Efficient Inference on Cointegration Parameters in Structural Error Correction Models, Journal of Econometrics, 69, 133­158. Breitung, J. (2000), The Local Power of Some Unit Root Tests for Panel Data, in B. Baltagi (ed.), Nonstationary Panels, Panel Cointegration, and Dynamic Panels, Advances in Econometrics, Vol. 15, JAI Press, Amsterdam, 161­178. Breitung, J. (2002), Nonparametric Tests for Unit Roots and Cointegration, Journal of Econometrics, 108, 343­363. Breitung, J. (2005), A Parametric Approach to the Estimation of Cointegration Vectors in Panel Data, Econometric Reviews, 151­174. Breitung J. and B. Candelon (2005), Purchasing Power Parity During Currency Crises: A panel unit root test under structural breaks, World Economic Review, 141, 124­140. Breitung, J. and S. Das (2005), Panel Unit Root Tests Under Cross Sectional Dependence, Statistica Neerlandica, 59, 414­433. Breitung, J. and S. Das (2008), Testing for Unit Roots in Panels With a Factor Structure, forthcoming in: Econometric Theory. Breitung, J. and W. Meyer (1994), Testing for Unit Roots in Panel Data: Are Wages on Different Bargaining Levels Cointegrated? Applied Economics, 26, 353­361. Bru¨ggemann, R. and H. Lu¨tkepohl (2005), Practical Problems with Reduced Rank ML Estimators for Cointegration Parameters and a Simple Alternative, Oxford Bulletin of Economics and Statistics, 67, 673­690. Carrion-i-Sevestre, J.L., T. Del Barrio and E. Lopez-Bazo (2005), Breaking the Panels: An Application to the GDP Per Capita, Econometrics Journal, 8, 159­175. Chang, Y. (2002), Nonlinear IV Unit Root Tests in Panels with Cross-Sectional Dependency, Journal of Econometrics, 110, 261­292. Chang, Y. (2004), Bootstrap Unit Root Tests in Panels with Cross-Sectional Dependency, Journal of Econometrics, 120, 263­293.

318

J. Breitung and M. Hashem Pesaran

Chang, Y., and J.Y. Park (2004), Taking a New Contour: A New Approach to Panel Unit Root Tests, Rice University, unpublished.
Chang, Y., J.Y. Park and P.C.B. Phillips (2001), Nonlinear Econometric Models with Cointegrated and Deterministically Trending Regressors, Econometrics Journal, 4, 1­36.
Chang, Y. and W. Song (2005), Unit Root Tests for Panels in the Presence of Short-Run and Long-Run Dependencies: Nonlinear IV Approach with Fixed N and Large T , Rice University, unpublished.
Choi, I. (2001), Unit Root Tests for Panel Data, Journal of International Money and Finance, 20, 249­272.
Choi, I. (2002), Combination Unit Root Tests for Cross-Sectionally Correlated Panels, in Econometric Theory and Practice: Frontiers of Analysis and Applied Research, Essays in Honor of P.C.B. Phillips, Cambridge University Press, Cambridge.
Choi, I. (2006), Nonstationary Panels, in K. Patterson and T.C. Mills (eds.) Palgrave Handbooks of Econometrics, Vol. 1, 511­539. Palgrave Macmillan, New York.
Choi, I. and T.K. Chue (2007), Subsampling Hypothesis Tests for Nonstationary Panels with Applications to Exchange Rates and Stock Prices, Journal of Applied Econometrics, 22, 233­264.
Coakley, J. and A.M. Fuertes (1997), New Panel Unit Root Tests of PPP, Economics Letters, 57, 17­22.
Coakley, J., N. Kellard and S. Smaith (2005), The PPP debate: Price Matters!, Economic Letters, 88, 209­213.
Dees, S., F. di Mauro, M. H. Pesaran and L. V. Smith (2007), Exploring the International Linkages of the Euro Area: A Global VAR Analysis, Journal of Applied Econometrics, 22, 1, 1­38.
Dickey, D.A. and W.A. Fuller (1979), Distribution of the Estimates for Autoregressive Time Series With a Unit Root, Journal of the American Statistical Association 74, 427­431.
Elliott, G., T. Rothenberg, and J. Stock (1996), Efficient Tests for an Autoregressive Unit Root, Econometrica, 64, 813­836.
Engle, R.F., and C.W.J. Granger (1987), Co-integration and Error Correction: Representation, Estimation, and Testing, Econometrica, 55, 251­276.
Entorf, H. (1997), Random Walks With Drifts: Nonsense Regression and Spurious Fixed-effects Estimation, Journal of Econometrics, 80, 287­296.
Fisher, R.A. (1932), Statistical Methods for Research Workers, Oliver and Bond, Edinburgh, 4th ed.
Flores, R., P. Jorion, P.Y. Preumont and A. Szarfarz (1999), Multivariate Unit Root Tests of the PPP Hypothesis, Journal of Empirical Finance, 6, 335­353.
Fuller, W.A., (1996), Introduction to Statistical Time Series, Second Edition, Wiley. Gengenbach, C. F.C. Palm and J.-P. Urbain (2006a), Panel Unit Root Tests in the Presence of Cross-
Sectional Dependencies: Comparison and Implications for Modelling, Universiteit Maastricht, unpublished. Gengenbach, C. F.C. Palm and J.-P. Urbain (2006b), Cointegration Testing in Panels with Common Factors", Oxford Bulletin of Economics and Statistics, 68, 683­719. Gredenhoff, M. and T. Jacobson (2001), Bootstrap Testing Linear Restrictions on Cointegrating Vectors, Journal of Business and Economic Statistics, 19, 63­72. Groen, J.J.J. and F. Kleibergen (2003), Likelihood-Based Cointegration Analysis in Panels of Vector Error-Correction Models, Journal of Business and Economic Statistics 21: 295­318. Gutierrez, L. (2003), On the Power of Panel Cointegration Tests: A Monte Carlo comparison, Economics Letters, 80, 105­111. Gutierrez, L. (2006), Panel Unit Roots Tests for Cross-Sectionally Correlated Panels: A Monte Carlo Comparison, Oxford Bulletin of Economics and Statistics, 68, 519­540. Hadri, K. (2000), Testing for Stationarity in Heterogeneous Panel Data, Econometrics Journal, 3, 148­161. Hadri, K. and R. Larsson (2005), Testing for Stationarity in Heterogeneous Panel Data Where the Time Dimension is Fixed, Econometrics Journal, 8, 55­69. Hansen, G., J.R. Kim and S. Mittnik (1998), Testing Cointegrating Coefficients in Vector Autoregressive Error Correction Models, Economics Letters, 58, 1­5.

9 Unit Roots and Cointegration in Panels

319

Harris, D., S. Leybourne and B. McCabe (2004), Panel Stationarity Tests for Purchasing Power Parity with Cross-sectional Dependence, Journal of Business and Economic Statistics, 23, 395­409.
Harris, R.D.F. and E. Tzavalis (1999), Inference for Unit Roots in Dynamic Panels where the Time Dimension is Fixed, Journal of Econometrics, 91, 201­226.
Harvey, A. and D. Bates (2003), Multivariate Unit Root Tests, Stability and Convergence, University of Cambridge, DAE Working Paper No. 301, University of Cambridge, England.
Harvey, D.I., S.J. Leybourne, and N.D. Sakkas (2006), Panel Unit Root Tests and the Impact of Initial Observations, Granger Centre Discussion Paper No. 06/02, University of Nottingham.
Hassler, U., M. Demetrescu and A. Tarcolea (2006), Combining Significance of Correlated Statistics with Application to Panel Data, Oxford Bulletin of Economics and Statistics, 68, 647­663.
Hlouskova, J. and M. Wagner (2006), The Performance of Panel Unit Root and Stationarity Tests: Results from a Large Scale Simulation Study, Econometric Reviews, 25, pp. 85­116.
Hlouskova, J. and M. Wagner (2007), The Performance of Panel Cointegration Methods: Results from a Large Scale Simulation Study, Institute for Advanced Studies, Vienna, unpublished.
Holly, S., M.H. Pesaran, and T. Yamagata (2007), A Spatial-Temporal Model of House Prices in the US, Cambridge University, unpublished.
Hsiao, C., and M. H. Pesaran (2007), Random Coefficient Panel Data Models, in L. Matyas and P. Sevestre, (eds.), The Econometrics of Panel Data, Kluwer Academic Publishers.
Hurlin, C. and V. Mignon, (2004), Second Generation Panel Unit Root Tests, THEMA-CNRS, University of Paris X, unpublished.
Im, K.S., J. Lee and M. Tieslau (2005), Panel LM Unit Root Tests with Level Shifts, Oxford Bulletin of Economics and Statistics, 63, 393­419.
Im, K.S., M.H. Pesaran, and Y. Shin (1995), Testing for Unit Roots in Heterogenous Panels, DAE Working Papers Amalgamated Series No. 9526, University of Cambridge.
Im, K.S., M.H. Pesaran, and Y. Shin (2003), Testing for Unit Roots in Heterogenous Panels, Journal of Econometrics, 115, 53­74.
Johansen, S. (1991), Estimation and Hypothesis Testing of Cointegrating Vectors in Gaussian Vector Autoregressive Models, Econometrica, 59, 1551­1580.
Johansen, S. (1995), Likelihood-based Inference in Cointegrated Vector Autoregressive Models, Oxford: Oxford University Press.
Jo¨nsson, K. (2005), Cross-Sectional Dependency and Size Distortion in a Small-sample Homogeneous Panel-data Unit Root Test, Oxford Bulletin of Economics and Statistics, 63, 369­392.
Kao, C. (1999), Spurious Regression and Residual-based Tests for Cointegration in Panel Data, Journal of Econometrics, 90, 1­44.
Kao, C. and M.-H. Chiang (2000), On the Estimation and Inference of a Cointegrated Regression in Panel Data, in: Baltagi B. (ed.), Nonstationary Panels, Panel Cointegration, and Dynamic Panels, Advances in Econometrics, Vol. 15, Amsterdam: JAI Press, 161­178.
Kapetanios, G. (2007), Dynamic Factor Extraction of Cross-Sectional Dependence in Panel Unit Root Tests, Journal of Applied Econometrics, 22, 313­338.
Kapetanios, G. and M.H. Pesaran (2007), Alternative Approaches To Estimation And Inference In Large Multifactor Panels: Small Sample Results With An Application To Modelling Of Asset Return", in G. Phillips and E. Tzavalis (eds.), The Refinement of Econometric Estimation and Test Procedures: Finite Sample and Asymptotic Analysis, Cambridge University Press, Cambridge.
Kapetanios, G., M. H. Pesaran and T. Yamagata, (2006), Panels with Nonstationary Multifactor Error Structures, CESifo Working Paper No. 1788.
Kiefer, N. and T. Vogelsang (2002), Heteroskedasticity-Autocorrelation Robust Standard Errors Using the Bartlett Kernel Without Truncation, Econometrica, 70, 2093­2095.
Kwiatkowski, D., Phillips, P.C.B., Schmidt, P. and Shin, Y. (1992) Testing the Null Hypothesis of Stationary Against the Alternative of a Unit Root: How Sure are we that Economic Time Series Have a Unit Root? Journal of Econometrics 54, 159­178.

320

J. Breitung and M. Hashem Pesaran

Larsson, R. and J. Lyhagen (1999). Likelihood-based Inference in Multivariate Panel Cointegration Models. Working paper series in Economics and Finance, no. 331, Stockholm School of Economics.
Larsson, R., J. Lyhagen and M. Lothgren (2001), Likelihood-based Cointegration Tests in Heterogenous Panels, Econometrics Journal 4, 109­142.
Levin, A. and C.F. Lin (1993), Unit root tests in panel data: asymptotic and finite-sample properties. Unpublished manuscript, University of California, San Diego.
Levin, A., C. Lin, and C.J. Chu (2002), Unit Root Tests in Panel Data: Asymptotic and Finitesample Properties, Journal of Econometrics, 108, 1­24.
Leybourne, S.J. (1995), "Testing for Unit Roots using Forward and Reverse Dickey-Fuller Regressions", Oxford Bulletin of Economics and Statistics, 57, 559­571.
Lyhagen, J. (2000), Why not Use Standard Panel Unit Root Tests for Testing PPP, Stockholm School of Economics, unpublished.
Maddala, G.S. and Wu, S. (1999), A Comparative Study of Unit Root Tests with Panel Data and a New Simple Test, Oxford Bulletin of Economics and Statistics, 61, 631­652.
Madsen, E. (2003), Unit Root Inference in Panel Data Models Where the Time-series Dimension is fixed: A comparison of different test, CAM working paper No. 2003­13.
Mark, N.C. and D. Sul (2003), Cointegration Vector Estimation by Panel DOLS and Long-run Money Demand, Oxford Bulletin of Economics and Statistics, 65, 655­680.
Mark, N.C., M. Ogaki and D. Sul (2004), Dynamic Seemingly Unrelated Cointegration Regression, forthcoming in: Review of Economic Studies.
McCoskey, S. and C. Kao (1998), A Residual-Based Test of the Null of Cointegration in Panel Data, Econometric Reviews, 17, 57­84.
Moon, R. and B. Perron (2004), Testing for Unit Root in Panels with Dynamic Factors, Journal of Econometrics, 122, 81­126.
Moon, H.R. and B. Perron (2007), Efficient Estimation of the SUR Cointegration Regression Model and Testing for Purchasing Power Parity, forthcoming in: Econometric Reviews.
Moon, H.R., B. Perron, and P.C.B. Phillips (2006), On the Breitung Test for Panel Unit Roots and Local Asymptotic Power, Econometric Theory, 22, 1179­1190.
Moon, H.R., B. Perron, and P.C.B. Phillips (2007), Incidental Trends and the Power of Panel Unit Root Tests, Journal of Econometrics, 141, 416­459.
Murray, C. J. and D.H. Papell (2002), Testing for Unit Roots in Panels in the Presence of Structural Change With an Application to OECD Unemployment, in B. Baltagi (ed.), Nonstationary Panels, Panel Cointegration, and Dynamic Panels, Advances in Econometrics, Vol. 15, JAI, Amsterdam, 223­238.
Nabeya, S. (1999), Asymptotic Moments of Some Unit Root Test Statistics in the Null Case, Econometric Theory, 15, 139­149.
O'Connell, P. (1998), The Overvaluation of Purchasing Power Parity, Journal of International Economics, 44, 1­19.
Park, H. and W. Fuller (1995), Alternative Estimators and Unit Root Tests for the Autoregressive Process, Journal of Time Series Analysis, 16, 415­429.
Pedroni, P. (1995), Panel Cointegration: Asymptotic and Finite Sample Properties of Pooled Time Series Test with an Application to the PPP Hypothesis, Indiana University Working Papers in Economics, No. 95­013.
Pedroni, P. (1999), Critical Values for Cointegration Tests in Heterogeneous Panels with Multiple Regressors, Oxford Bulletin of Economics and Statistics, 61, 653­670.
Pedroni, P. (2000), Fully Modified OLS for Heterogenous Cointegrated Panels, in: Baltagi B. (ed.), Nonstationary Panels, Panel Cointegration, and Dynamic Panels, Advances in Econometrics, Vol. 15, Amsterdam: JAI Press, pp. 93­130.
Pedroni, P. (2001), Purchasing Power Parity Tests in Cointegrated Panels, Review of Economics and Statistics, 83, 727­731.
Pedroni, P. (2004), Panel Cointegration: Asymptotic and Finite Sample Properties of Pooled Time Series Tests With an Application to the PPP Hypothesis, Econometric Theory, 20, 597­625.

9 Unit Roots and Cointegration in Panels

321

Pedroni, P. and T. Vogelsang (2005), Robust Tests for Unit Roots in Heterogeneous Panels, Williams College, unpublished.
Pesaran, M.H. (1997), The Role of Economic Theory in Modelling the Long Run, Economic Journal 107, 178­91.
Pesaran M.H. (2004), General Diagnostic Tests for Cross Section Dependence in Panels, Cambridge Working Papers in Economics, No. 435, University of Cambridge, and CESifo Working Paper Series No. 1229.
Pesaran, M.H., (2006), Estimation and Inference in Large Heterogeneous Panels with a Multifactor Error Structure, Econometrica, 74, 967­1012.
Pesaran, M.H. (2007), A Simple Panel Unit Root Test in the Presence of Cross Section Dependence, Journal of Applied Econometrics, 22, 265­312.
Pesaran, M.H. and Y. Shin (2002), Long-Run Structural Modelling, Econometric Reviews, 21, 49­87.
Pesaran, M.H., T. Schuermann, and S.M. Weiner (2004), Modelling Regional Interdependencies Using a Global Error-Correcting Macroeconometric Model, Journal of Business and Economic Statistics, 22, 129­162.
Pesaran, M.H., Y. Shin and R.P. Smith (1999), Pooled Mean Group Estimation of Dynamic Heterogeneous Panels, Journal of the American Statistical Association, 94, 621­624.
Pesaran, M.H., Y. Shin and R.J. Smith (2000), Structural Analysis of Vector Error Correction Models with Exogenous I(1) Variables, Journal of Econometrics, 97, 293­343.
Pesaran, M.H., R. Smith, and K.S. Im (1996), Dynamic linear models for Heterogenous Panels. In: Matyas, L., Sevestre, P. (eds), The Econometrics of Panel Data: A Handbook of the Theory with Applications, second revised edition, 145­195.
Pesaran, M.H. and Tosetti, E. (2007), Large Panels with Common Factors and Spatial Correlations, Faculty of Economics and Politics, University of Cambridge.
Pesaran, M.H. and T. Yamagata (2008), Testing Slope Homogeneity in Large Panels, Journal of Econometrics, 142, 50­93.
Phillips, P.C.B. (1991), Optimal Inference in Co-integrated Systems, Econometrica, 59, 282­306. Phillips, P.C.B. (1994), Some Exact Distribution Theory for Maximum Likelihood Estimators of
Cointegrating Coefficients in Error Correction Models, Econometrica, 62, 73­93. Phillips, P.C.B. and B.E. Hansen (1990), Statistical Inference in Instrumental Variable Regression
with I(1) Processes, Review of Economic Studies 57: 99­125. Phillips, P.C.B. and H.R. Moon (1999), Linear Regression Limit Theory for Nonstationary Panel
Data, Econometrica, 67, 1057­1111. Phillips, P.C.B. and S. Ouliaris (1990), Asymptotic Properties of Residual Based Tests for
Cointegration, Econometrica, 58, 165­193. Phillips, P.C.B. and D. Sul (2003a), Dynamic Panel Estimation and Homogeneity Testing Under
Cross Section Dependence, Econometrics Journal, 6, 217­259. Phillips, P.C.B. and D. Sul (2003b), The Elusive Empirical Shadow of Growth Convergence,
Cowles Foundation Discussion Paper 98, Yale University. Ploberger, W. and P.C.B. Phillips (2002), Optimal Testing for Unit Roots in Panel Data, unpub-
lished. Robinson, P.M. (1994), Efficient Tests of Nonstationary Hypotheses, Journal of the American
Statistical Association, 89, 1420­1437. Saikkonen, P. (1991), Asymptotic Efficient Estimation of Cointegration Regressions, Econometric
Theory, 7, 1­21. Schmidt, P. and P.C.B. Phillips (1992), LM Test for a Unit Root in the Presence of Deterministic
Trends, Oxford Bulletin of Economics and Statistics 54, 257­287. Shin, Y. (1994), A Residual-based Test of the Null of Cointegration Against the Alternative of no
Cointegration, Econometric Theory, 10, 91­115. Smith, V., S. Leybourne, T.-H. Kim and P. Newbold (2004), More Powerful Panel Data Unit Root
Tests With an Application to Mean Reversion in Real Exchange Rates, Journal of Applied Econometrics, 19, 147­170. Tanaka, K. (1990), Testing for a Moving Average Root, Econometric Theory, 6, 433­444.

322

J. Breitung and M. Hashem Pesaran

Tzavalis, E. (2002), Structural Breaks and Unit Root Tests for Short Panels, Queen Mary University of London, unpublished.
Wagner, M. (2007), On PPP, Unit Roots and Panels, forthcoming in: Empirical Economics. Westerlund, J. (2005a), New Simple Tests for Panel Cointegration', Econometric Reviews, 24, 3,
297­316. Westerlund, J. (2005b), A Panel CUSUM Test of the Null of Cointegration, Oxford Bulletin of
Economics and Statistics, 62, 231­262. Westerlund, J. (2005c), Data Dependent Endogeneity Correction in Cointegrated Panels, Oxford
Bulletin of Economics and Statistics, 67, 691­705. Westerlund, J. (2007a), A Note on the Pooling of Individual PANIC Unit Root Tests, Luund
University, unpublished. Westerlund, J. (2007b), Estimating Cointegrated Panels with Common Factors and The Forward
Rate Unbiasedness Hypothesis, Journal of Financial Econometrics, 3, 491­522. Westerlund, J. (2008), Some Cautions on the LLC Panel Unit Root Test, forthcoming in: Empirical
Economics.

Chapter 10
Measurement Errors and Simultaneity
Erik Biørn and Jayalakshmi Krishnakumar

10.1 Introduction
This chapter is concerned with the problem of endogeneity of certain explanatory variables in a regression equation. There are two potential sources of endogeneity in a panel data model with individual and time specific effects : (i) correlation between explanatory variables and specific effects (when treated random) and (ii) correlation between explanatory variables and the residual/idiosyncratic error term.
The first case was extensively dealt with in Chap. 4 of this book and hence we will not go into it here. In this chapter we are more concerned with a non-zero correlation between the explanatory variables and the overall error consisting of both the specific effect and the genuine disturbance term. One might call it double endogeneity as opposed to the single endogeneity in the former situation.
In this chapter we consider two major causes of this double endogeneity encountered in practical situations. One of them is the presence of measurement errors in the explanatory variables. This will be the object of study of Sect. 10.2. Another major source is the simultaneity problem that arises when the regression equation is one of several structural equations of a simultaneous model and hence contains current endogenous explanatory variables. Sect. 10.3 will look into this problem in detail and Sect. 10.4 concludes the chapter.

10.2 Measurement Errors and Panel Data

A familiar and notable property of the Ordinary Least Squares (OLS) when there are random measurement errors (errors-in-variables, EIV) in the regressors is that the coefficient estimators are inconsistent. In the one regressor case (or the multiple

Erik Biørn Department of Economics, University of Oslo, P.O. Box 1095, Blindern, 0317 Oslo, Norway, e-mail: erik.biorn@econ.uio.no
Jayalakshmi Krishnakumar Department of Econometrics, University of Geneva, 40 Bd. Du pont d'Ave, CH-1211 Geneva 4, Switzerland, e-mail: jaya.krishnakumar@metri.unige.ch

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

323

c Springer-Verlag Berlin Heidelberg 2008

324

E. Biørn and J. Krishnakumar

regressor case with uncorrelated regressors) under standard assumptions, the slope coefficient estimator is biased towards zero, often denoted as attenuation. More seriously, unless some `extraneous' information is available, e.g. the existence of valid parameter restrictions or valid instruments for the error-ridden regressors, slope coefficients cannot (in general) be identified from standard data [see Fuller (1987, Sect. 1.1.3)].1 This lack of identification in EIV models, however, relates to unidimensional data, i.e., pure (single or repeated) cross-sections or pure time-series. If the variables are observed as panel data, exhibiting two-dimensional variation, it may be possible to handle the EIV identification problem and estimate slope coefficients consistently without extraneous information, provided that the distribution of the latent regressors and the measurement errors satisfy certain weak conditions.
Briefly, the reason why the existence of variables observed along two dimensions makes the EIV identification problem easier to solve, is partly (i) the repeated measurement property of panel data, so that the measurement error problem can be reduced by taking averages, which, in turn, may show sufficient variation to permit consistent estimation, and partly (ii) the larger set of other linear data transformations available for estimation. Such transformations, involving several individuals or several periods, may be needed to take account of uni-dimensional `nuisance variables' like unobserved individual or period specific heterogeneity, which are potentially correlated with the regressor.
Our focus is on the estimation of linear, static regression equations from balanced panel data with additive, random measurement errors in the regressors by means of methods utilizing instrumental variables (IVs). The panel data available to an econometrician are frequently from individuals, firms, or other kinds of micro units, where not only observation errors in the narrow sense, but also departures between theoretical variable definitions and their observable counterparts in a wider sense may be present.
From the panel data literature which disregards the EIV problem we know that the effect of, for example, additive (fixed or random) individual heterogeneity within a linear model can be eliminated by deducting individual means, taking differences over periods, etc. [see Baltagi (2001, Chap. 2) and Hsiao (2003, Sect. 1.1)]. Such transformations, however, may magnify the variation in the measurement error component of the observations relative to the variation in the true structural component, i.e., they may increase the `noise/signal ratio'. Hence, data transformations intended to `solve' the unobserved heterogeneity problem in estimating slope coefficients may aggravate the EIV problem. Several familiar estimators for panel data models, including the fixed effects within-group and between-group estimators, and the random effects Generalised Least Squares (GLS) estimators will then be inconsistent, the bias depending, inter alia, on the way in which the number of individuals and/or

1 Identification under non-normality of the true regressor is possible, by utilizing moments of the distribution of the observable variables of order higher than the second [see Reiersøl (1950)]. Even under non-identification, bounds on the parameters can be established from the distribution of the observable variables [see Fuller (1987, p. 11)]. These bounds may be wide or narrow, depending on the covariance structure of the variables; see Klepper and Leamer (1984), Bekker et al. (1987), and Erickson (1993).

10 Measurement Errors and Simultaneity

325

periods tend to infinity and on the heterogeneity of the measurement error process; see Griliches and Hausman (1986) and Biørn (1992, 1996). Such inconsistency problems will not be dealt with here. Neither will we consider the idea of constructing consistent estimators by combining two or more inconsistent ones with different probability limits. Several examples are given in Griliches and Hausman (1986), Biørn (1996), and Wansbeek and Meijer (2000, Sect. 6.9).
The procedures to be considered in this section have two basic characteristics: First, a mixture of level and difference variables are involved. Second, the orthogonality conditions derived from the EIV structure ­ involving levels and differences over one or more than one periods ­ are not all essential, some are redundant. Our estimation procedures are of two kinds: (A) Transform the equation to differences and estimate it by IV or GMM, using as IVs level values of the regressors and/or regressands for other periods. (B) Keep the equation in level form and estimate it by IV or GMM, using as IVs differenced values of the regressors and/or regressands for other periods. In both cases, the differencing serves to eliminate individual heterogeneity, which is a potential nuisance since it may be correlated with the latent regressor vector. These procedures resemble, to some extent, procedures for autoregressive (AR) models for panel data without measurement errors (mostly AR(1) equations with individual heterogeneity and often with exogenous regressors added) discussed, inter alia, by Anderson and Hsiao (1981, 1982), Holtz-Eakin et al. (1988), Arellano and Bond (1991), Arellano and Bover (1995), Ahn and Schmidt (1995), Blundell and Bond (1998), and Harris et al. (2007).
If the distribution of the latent regressor vector is not time invariant and the second order moments of the measurement errors and disturbances are structured to some extent, a large number of consistent IV estimators of the coefficient of the latent regressor vector exist. Their consistency is robust to potential correlation between the individual heterogeneity and the latent regressor. Serial correlation or non-stationarity of the latent regressor is favourable from the point of view of identification and estimability of the coefficient vector.
The literature dealing specifically with panel data with measurement errors is not large. The (A) procedures above extend and modify procedures described in Griliches and Hausman (1986), which is the seminal article on measurement errors in panel data, at least in econometrics. Extensions are discussed in Wansbeek and Koning (1991), Biørn (1992, 1996, 2000, 2003), and Biørn and Klette (1998, 1999), and Wansbeek (2001). Paterno et al. (1996) consider Maximum Likelihood analysis of panel data with measurement errors and is not related to the (A) and (B) procedures to be discussed here.

10.2.1 Model and Orthogonality Conditions
Consider a panel data set with N ( 2) individuals observed in T ( 2) periods and a relationship between y (observable scalar) and a (K × 1) vector  (latent),

326

E. Biørn and J. Krishnakumar

yit = c +  it  + i + uit ,

i = 1, . . . , N; t = 1, . . . , T,

(10.1)

where (yit , it ) is the value of (y, ) for individual i in period t, c is a scalar constant,  is a (K × 1) vector and i is a zero (marginal) mean individual effect, which we consider as random and potentially correlated with  it , and uit is a zero mean
disturbance, which may also contain a measurement error in yit . We observe

xit =  it + vit ,

i = 1, . . . , N; t = 1, . . . , T,

(10.2)

where vit is a zero mean (K × 1) vector of measurement errors. Hence,

yit = c + xit  + it , it = i + uit - vit  .

(10.3)

We can eliminate i from (10.3) by taking arbitrary backward differences yit = yit - yi , xit = xit - xi , etc., giving

yit =  xit  + it , it = uit - vit  .

(10.4)

We assume that ( it , uit , vit , i) are independent across individuals [which excludes random period specific components in ( it , uit , vit )], and make the following basic orthogonality assumptions:

Assumption (A):

E(vit ui ) = E( it ui ) = E(ivit ) = 0K1, E( i  vit ) = 0KK , E(iuit ) = 0,

i = 1, . . . , N, t,  = 1, . . . , T,

where 0mn denotes the (m × n) zero matrix and  is the Kronecker product operator. Regarding the temporal structure of the measurement errors and disturbances, we assume either that

Assumption (B1): Assumption (C1):

E(vit vi ) = 0KK, |t -  | > ,

E(uit ui ) = 0,

|t -  | > ,

where  is a non-negative integer, indicating the order of the serial correlation, or

Assumption (B2): Assumption (C2):

E(vit vi ) is invariant to t,  , t =  , E(uit ui ) is invariant to t,  , t =  ,

which allows for time invariance of the autocovariances. The latter will, for instance, be satisfied if the measurement errors and the disturbances have individual specific components, say vit = v1i + v2it , uit = u1i + u2it , where v1i, v2it , u1i, and u2it are independent IID processes.
The final set of assumptions relate to the distribution of the latent regressor vector  it :

10 Measurement Errors and Simultaneity

327

Assumption (D1): Assumption (D2): Assumption (E):

E( it ) E(i it ) rank(E[ ip( it )]) = K

is invariant to t, is invariant to t, for some p,t,  different,

Assumptions (D1) and (D2) hold when  it is stationary for all i [(D1) alone imposing mean stationarity]. Assumption (E) imposes non-IID and some form of autocorrelation or non-stationarity on  it . It excludes, for example, the case where  it has an individual specific component, so that  it =  1i +  2it , where  1i and  2it are independent (vector) IID processes.
Assumptions (A)­(E) do not impose much structure on the first and second order moments of the uit s, vit s,  it s and is. This has both its pros and cons. It is possible to structure this distribution more strongly, for instance assuming homoskedasticity and normality of uit , vit , and i, and normality of  it . Exploiting this stronger structure, e.g., by taking a LISREL of LIML approach, we might obtain more efficient
(but potentially less robust) estimators by operating on the full covariance matrix of the yit s and the xit s rather than eliminating the is by differencing. Other extensions are elaborated in Sect. 10.2.7.

10.2.2 Identification and the Structure of the Second Order Moments

The distribution of ( it , uit , vit , i) must satisfy some conditions to make identification of  possible. The nature of these conditions can be illustrated as follows. Assume, for simplicity, that this distribution is the same for all individuals and that
(A) holds, and let

C( it , i ) = t , E( it i) = t  , E(i2) =   , E(vit vi ) = tvv , E(uit ui ) = tuu,

i = 1, . . . , N, t,  = 1, . . . , T,

where C denotes the covariance matrix operator. It then follows from (10.1) and (10.2) that the second order moments of the observable variables can be expressed as

C(xit , xi ) = t + tvv , C(xit , yi ) = t  + t  , C(yit , yi ) =  t  + (t  )  +   + tuu +   ,

i = 1, . . . , N, t,  = 1, . . . , T. (10.5)

The identifiability of  from second order moments in general depends on
whether or not knowledge of C(xit , xi ), C(xit , yi ), and C(yit , yi ) for all available t and  is sufficient for obtaining a unique solution for  from (10.5), given the restrictions imposed on the t s, t s, tuus, and   . The answer, in general, depends on T and K. With no further information, the number of elements in

328

E. Biørn and J. Krishnakumar

C(xit , xi ), and C(yit , yi ) (all of which can be estimated consistently from corresponding sample moments under weak conditions) equal the number of unknown

elem ecnatnsniontbtvev

and

tuu,

which

is

1 2

KT (KT

+ 1)

and

1 2

T (T

+ 1),

respectively.

Then

identified, and C(xit , yi ) contains the only additional information

available for identifying  , t , and t , given the restrictions imposed on the lat-

ter two matrices.

Consider two extreme cases. First, if T = 1, i.e., if we only have cross-section

data, and no additional restrictions are imposed, there is an identification problem for any K. Second, if T > 2 and  it  IID(  , ), vit  IID(01,K,vv), uit  IID(0,  uu), i  IID(0,   ), we also have lack of identification in general.
We get an essentially similar conclusion when the autocovariances of  it are time

invariant and it is IID across i. From (10.5) we then get

C(xit , xi ) = t (  + vv), C(xit , yi ) = t    , C(yit , yi ) = t (    +  uu) +   ,

(10.6)

where t = 1 for t =  and = 0 for t =  , and so we are essentially in the same situation with regard to identifiability of  as when T = 1. The `cross-period' equations (t =  ) then serve no other purpose than identification of   , and whether T = 1 or T > 1 realizations of C(xit , xit ), C(xit , yit ), and C(yit , yit ) are available in (10.6) is immaterial to the identifiability of  ,  , vv, and  uu. In intermediate situations, identification may be ensured when T  2. These examples illustrate that in order to ensure identification of the slope coefficient vector from panel data, there should
not be `too much structure' on the second order moments of the latent exogenous re-
gressors along the time dimension, and not `too little structure' on the second order
moments of the errors and disturbances along the time dimension.

10.2.3 Moment Conditions
A substantial number of (linear and non-linear) moment conditions involving yit , xit , and it can be derived from Assumptions (A)­(E). Since (10.1)­(10.3) and Assumption (A) imply
E(xit xi ) = E( it i ) + E(vit vi ), E(xit yi ) = E( it i ) + E[ it (i + c)], E(yit yi ) = c2 + E(i2) +  E( it i ) +  E[ it (i + c)]
+E[(i + c) i ] + E(uit ui ), E(xit i ) = E( it i) - E(vit vi ) , E(yit i ) =  E( it i) + E(i2) + E(uit ui ),

10 Measurement Errors and Simultaneity

329

we can derive moment equations involving observable variables in levels and differences:

E[xip(xit )] = E[ ip( it )] + E[vip(vit )], E[xip(yit )] = E[ ip( it )] , E[(xipq)yit ] = E[( ipq) it ] + E[( ipq)(i + c)],

(10.7) (10.8) (10.9)

as well as moment equations involving observable variables and errors/disturbances:

E[xip(it )] = - E[vip(vit )] , E[yip(it )] = E[uip(uit )], E[( xipq)it ] = E[( ipq)i] - E[(vipq)vit ] , E[(yipq)it ] =  E[( ipq)i] + E[(uipq)uit ],

(10.10) (10.11) (10.12) t,  , p, q = 1, . . . , T. (10.13)

Not all of the equations in (10.7)­(10.13), whose number is substantial even for small T , are, of course, independent. Depending on which (B), (C), and (D) assumptions are valid, some terms on the right hand side of (10.9)­(10.13) will vanish. Precisely, if T > 2, then (10.3), (10.5), and (10.10)­(10.13) imply the following moment conditions, or orthogonality conditions (OC), on the observable variables and the errors and disturbances

(B2), or (B1) with |t - p|, | - p| > ,t =  = E[xip(it )] = E[xip(yit )] - E[xip( xit )] = 0K1. (10.14)

(C2), or (C1) with |t - p|, | - p| > ,t =  = E[yip(it )] = E[yip(yit )] - E[yip( xit )] = 0.

(10.15)

(D1), (D2) and (B2), or (B1) with |t - p|, |t - q| > , p = q = E[(xipq)it ] = E[(xipq)yit ] - E[( xipq)xit ] = 0K1. (10.16)

(D1), (D2), and (C2), or (C1) with |t - p|, |t - q| > , p = q = E[(yipq)it ] = E[(yipq)yit ] - E[(yipq)xit ] = 0.

(10.17)

The treatment of the intercept term c in constructing (10.16) and (10.17) needs
a comment. When the mean stationarity assumption (D1) holds, using IVs in dif-
ferences annihilates c in the moment equations, since then E( xipq) = 0K1 and E(yipq) = 0. If, however, we relax (D1), which is unlikely to hold in many practical situations, we get

E[(xipq)it ] = E[(xipq)yit ] - E[xipq]c - E[(xipq)xit ] = 0K1, E[(yipq)it ] = E[(yipq)yit ] - E[yipq]c - E[(yipq)xit ] = 0.
Using E(it ) = E(yit ) - c - E(xit ) = 0 to eliminate c leads to the following modifications of (10.16) and (10.17):

330

E. Biørn and J. Krishnakumar

(D1), (D2) and (B2), or (B1) with |t - p|, |t - q| > , p = q, = E[( xipq)it ] = E[( xipq)(yit - E(yit ))] - E[( xipq)(xit - E(xit ))] = 0K1.
(D1), (D2), and (C2), or (C1) with |t - p|, |t - q| > , p = q, = E[(yipq)it ] = E[(yipq)(yit - E(yit ))] - E[(yipq)(xit - E(xit ))] = 0.

To implement these modified OCs in the GMM procedures to be described below

for the level equation, we could replace E(yit ) and E(xit ) by corresponding global or period specific sample means.

The conditions in (10.14)­(10.17) are not all independent. Some are redundant,

since they can be derived as linear combinations of other conditions.2 We confine

attention to (10.14) and (10.16), since (10.15) and (10.17) can be treated similarly.

When



=

0,

the

total

number

of

OCs

in

both

(10.14)

and

(10.16)

is

1 2

KT

(T

-

1)

(T -2). Below, we prove that

(a) When (B2) and (C2), or (B1) and (C1) with  = 0, are satisfied, all OCs in (10.14) can be constructed from all admissible OCs relating to equations differ-
enced over one period and a subset of OCs relating to differences over two periods. When (B1) and (C1) are satisfied with an arbitrary , all OCs in (10.14) can be constructed from all admissible OCs relating to equations differenced over one period and a subset of OCs relating to differences over 2( +1) periods. (b) When (B2) and (C2), or (B1) and (C1) with  = 0, are satisfied all OCs in (10.16) can be constructed from all admissible OCs relating to IVs differenced over one
period and a subset of IVs differenced over two periods. When (B1) and (C1) are satisfied with an arbitrary , all OCs in (10.16) can be constructed from all admissible OCs relating to IVs differenced over one period and a subset of IVs differenced over 2( +1) periods.

We denote the non-redundant conditions defined by (a) and (b) as essential OCs.
Since (10.14) and (10.16) are symmetric, we prove only (a) and derive (b) by way
of analogy. Since xipit = xip(tj=+1 i j, j-1), we see that if (hypothetically)
all p = 1, . . . , T combined with all t >  would have given admissible OCs, (10.14) for differences over 2, 3, . . . , T - 1 periods could have been constructed from the conditions relating to one-period differences only. However, since (t,  ) = (p, p - 1), (p + 1, p) are inadmissible, and [when (B2) holds] (t,  ) = (p + 1, p - 1) is admissible, we have to distinguish between the cases where p is strictly outside and strictly inside the interval ( ,t). From the identities

xipit = xip(tj=+1 i j, j-1) for p = 1, . . . ,  -1, t + 1, . . . , T,

xipit

=

xi

p(

p-1 j= +1

i

j,

j-1

+ i,p+1,p-1

+ tj=p+2

i j, j-1)

for

p =  +1, . . . ,t-1,

when taking expectations, we then obtain

2 This redundancy problem is discussed in Biørn (2000). Essential and redundant moment conditions in AR models for panel data are discussed in Ahn and Schmidt (1995), Arellano and Bover (1995), and Blundell and Bond (1998). A general treatment of redundancy of moment conditions in GMM estimation is found in Breusch et al. (1999).

10 Measurement Errors and Simultaneity

331

Proposition 10.1. A. When (B2) and (C2) are satisfied, then

(a) E[xip(it,t-1)] = 0K1 for p = 1, . . . ,t -2,t +1, . . . , T ; t = 2, . . . , T are K(T -1)(T -2) essential OCs for equations differenced over one period.

(b) E[xit (it+1,t-1)] = 0K1 for t = 2, . . . , T -1 are K(T -2) essential OCs for equations differenced over two periods.

(c) The

other

OCs

are

redundant:

among

the

1 2

K

T

(T

-

1)(T

-

2)

conditions

in

(10.14), only a fraction 2/(T -1), are essential.

B. When (B1) and (C1) are satisfied for an arbitrary , then

(a) E[xip(it,t-1)] = 0K1 for p = 1, . . . ,t - -2,t + +1, . . . , T ; t = 2, . . . , T are essential OCs for equations in one-period differences.
(b) E[xit (it++1,t--1)] = 0K1 for t =  +2, . . . , T - -1 are essential OCs for equations in 2( +1) period differences.
(c) The other OCs in (10.14) are redundant.

Symmetrically, from (10.16) we have

Proposition 10.2. A. When (B2) and (C2) are satisfied, then

(a) E[(xip,p-1)it ] = 0K1 for t = 1, . . . , p - 2, p + 1, . . . , T ; p = 2, . . . , T are K (T -1)(T -2) essential OCs for equations in levels, with IVs differenced over

one period.

(b) E[(xit+1,t-1)it ] = 0K1 for t = 2, . . . , T -1 are K(T -2) essential OCs for equations in levels, with IVs differenced over two periods.

(c) The

other

OCs

are

redundant:

among

the

1 2

K

T

(T

-

1)(T

-

2)

conditions

in

(10.16), only a fraction 2/(T -1), are essential.

B. When (B1) and (C1) are satisfied for an arbitrary , then

(a) E[(xip,p-1)it ] = 0K1 for t = 1, . . . , p- -2, p+ +1, . . . , T ; p = 2, . . . , T are essential OCs for equations in levels, with IVs differenced over one period.
(b) E[(xit++1,t--1)it ] = 0K1 for t =  +2, . . . , T - -1 are essential OCs for equations in levels, with IVs differenced over 2( +1) periods.
(c) The other OCs in (10.16) are redundant.

These propositions can be (trivially) modified to include also the essential and redundant OCs in the ys or the ys, given in (10.15) and (10.17).

10.2.4 Estimators Constructed from Period Means
Several consistent estimators of  can be constructed from differenced period means. These estimators exploit the repeated measurement property of panel data, while the differencing removes the latent heterogeneity. From (10.3) we obtain

332

E. Biørn and J. Krishnakumar

sy¯·t = sx¯·t + s¯·t , s = 1, . . . , T -1; t = s+1, . . . , T, (10.18)

(y¯·t - y¯) = (x¯·t - x¯)  + (¯·t - ¯),

t = 1, . . . , T,

(10.19)

where

y¯·t

=

1 N

i

yit ,

y¯

=

1 NT

i

t

yit ,

x¯ ·t

=

1 N

i xit ,

x¯

=

1 NT

i t

xit ,

etc.

and

s

de-

notes differencing over s periods. When (A) is satisfied, the (weak) law of large

numbers implies, under weak conditions [confer McCabe and Tremayne (1993,

Sect. 3.5)],3 that plim(¯·t ) = 0, plim(x¯·t - ¯ ·t ) = 0K1, so that plim[x¯·t ¯·t ] = 0K1

even

if

plim[

1 N

iN=1

xit

it

]

=

0K1

.

From

(10.18)

and

(10.19)

we

therefore

get

plim[(sx¯·t )(sy¯·t )] = plim[(sx¯·t )(sx¯·t )] , plim[(x¯·t -x¯)(y¯·t -y¯)] = plim[(x¯·t -x¯)(x¯·t -x¯) ] .

(10.20) (10.21)

Hence, provided that E[(s¯ ·t )(s¯ ·t ) ] and E[(¯ ·t - ¯ )(¯ ·t - ¯ ) ] have rank K, which is ensured by Assumption (E), consistent estimators of  can be obtained by applying OLS on (10.18) and (10.19), which give, respectively,

   s =

T t=s+1

(s x¯ ·t

)

(s x¯ ·t

)

-1

tT=s+1(sx¯·t ) (sy¯·t ) , s = 1, . . . , T -1,

(10.22)

   BP =

T t=1

(x¯ ·t

-

x¯ )(x¯ ·t

-

x¯ )

-1

T t=1

(x¯ ·t

-

x¯ )(y¯·t

-

y¯)

.

(10.23)

The latter is the `between period' (BP) estimator. The consistency of these estimators simply relies on the fact that averages of a large number of repeated measurements of an error-ridden variable give, under weak conditions, an error-free measure of the true average at the limit, provided that this average shows variation along the remaining dimension, i.e., across periods. Shalabh (2003) also discusses consistent coefficient estimation in measurement error models with replicated observations. The latter property is ensured by Assumption (E). A major problem with these estimators is their low potential efficiency, as none of them exploits the between individual variation in the data, which often is the main source of variation.
Basic to these conclusions is the assumption that the measurement error has no period specific component, which, roughly speaking, means that it is `equally difficult' to measure  correctly in all periods. If such a component is present, it will not vanish when taking plims of period means, i.e., plim(v¯·t) will no longer be zero, (10.20) and (10.21) will no longer hold, and so  s and  BP will be inconsistent.

10.2.5 GMM Estimation and Testing in the General Case
We first consider the GMM principle in general, without reference to panel data and measurement error situations. Assume that we want to estimate the (K × 1)
3 Throughout plim denotes probability limits when N goes to infinity and T is finite.

10 Measurement Errors and Simultaneity

333

coefficient vector  in the equation4

y = x + ,

(10.24)

where y and  are scalars and x is a (1 × K) regressor vector. There exists an instrument vector z, of dimension (1 × G), for x (G  K), satisfying the OCs

E(z ) = E[z (y - x )] = 0G1.

(10.25)

We have n observations on (y, x, z), denoted as (y j, x j, z j), j = 1, . . . , n, and define the vector valued (G × 1) function of corresponding empirical means,

gn(y, x, z; )

=

1 n

nj=1 z j(y j

- x j ).

(10.26)

It may be considered the empirical counterpart to E[z (y - x )] based on the sample. The essence of GMM is to choose as an estimator for  the value which brings the value of gn(y, x, z; ) as close to its theoretical counterpart, 0G1, as possible. If G = K, an exact solution to gn(y, x, z; ) = 0G1 exists and is the simple IV estimator

  = [ j z jx j]-1[ j z jy j].

If G > K, which is the most common situation, GMM solves the estimation problem by minimizing a distance measure represented by a quadratic form in gn(y, x, z; ) for a suitably chosen positive definit (G × G) weighting matrix Wn, i.e.,



 GMM

=

argmin



[gn

(y,

x,

z;



)

Wngn

(y,

x,

z;



)].

(10.27)

All estimators obtained in this way are consistent. A choice which leads to an

asymptotically efficient estimator of  , is to set this weighting matrix equal (or

proportional) to the inverse of (an estimate of) the (asymptotic) covariance matrix

of

1 n

nj=1

zjj;

see,

e.g.,

Davidson

and

MacKinnon

(1993,

Theorem

17.3)

and

Harris

and Ma´tya´s (1999, Sect. 1.3.3).

If  is serially uncorrelated and homoskedastic, with variance 2, the appropriate choice is simply Wn = [n-22 nj=1 z jz j]-1. The estimator obtained from (10.27) is then

 GMM = [( j x jz j)( j z jz j)-1( j z jx j)]-1 × [( j x jz j)( j z jz j)-1( j z jy j)],

(10.28)

which is the standard Two-Stage Least Squares (2SLS) estimator. If  j has an unspecified heteroskedasticity or has a more or less strictly specified autocorrelation, we can reformulate the OCs in an appropriate way, as will be exemplified below. Both of these properties are essential for the application of GMM to panel data.

4 We here, unlike in Sects. 10.2.1­10.2.4, let the column number denote the regressor and the row number the observation. Following this convention, we can express the following IV and GMM estimators in the more common format when going from vector to matrix notation.

334

E. Biørn and J. Krishnakumar

To operationalize the latter method in the presence of unknown heteroskedasticity,
we first construct consistent residuals  j, usually from (10.28), which we consider as a first step GMM estimator, and estimate Wn by Wn = [n-2  j z j"2j z j]-1; see White (1984, Sects. IV.3 and VI.2). Inserting this into (10.27) gives

 GMM = [( j x jz j)( j z j2j z j)-1( j z jx j)]-1 × [( j x jz j)( j z j2j z j)-1( j z jy j)].

(10.29)

This second step GMM estimator is in a sense an optimal GMM estimator in the presence of unspecified error/disturbance heteroskedasticity.
The validity of the orthogonality condition (10.25) can be tested by the SarganHansen statistic [confer Hansen (1982), Newey (1985), and Arellano and Bond (1991)], corresponding to the asymptotically efficient estimator  GMM:
J = [( j  jz j)( j z j2j z j)-1( j z j j)]-1.

Under the null, J is asymptotically distributed as 2 with a number of degrees of freedom equal to the number of overidentifying restrictions, i.e., the number of orthogonality conditions less the number of coefficients estimated under the null.
The procedures for estimating standard errors of  GMM and  GMM can be explained as follows. Express (10.24) and (10.25) as

y = X + , E( ) = 0, E(Z  ) = 0, E( ) = ,

where y, X, Z, and  correspond to y, x, z and , and the n observations are placed along the rows. The two generic GMM estimators (10.28) and (10.29) have the form

 = [X PZX]-1[X PZy],  = [X PZ()X]-1[X PZ()y],

PZ = Z(Z Z)-1Z , PZ() = Z(Z Z)-1Z .

Let the residual vector obtained from the former be  = y - X and

XZ

ZZ

Z

SXZ = SZX =

, n

SZZ =

, n

SZ = SZ =

, n

Z Z

Z  Z

Z  Z

SZ  Z = n , SZ Z =

n , SZ Z =

. n

Inserting for y in the expressions for the two estimators gives

 n(

-

)

=

 n[X

PZ

X]-1[X

PZ

]

=

[SXZ S-ZZ1SZX

]-1

SX Z

SZ-Z1

Z n

,

 n(

-

)

=

 n[X

PZ

()X]-1[X

PZ ()

]

=

[SX Z

SZ-1 Z

SZX

]-1

SX

Z

S-Z 1 Z

Z n

,

10 Measurement Errors and Simultaneity

335

and hence
n( -  )( -  ) = [SXZ SZ-Z1SZX ]-1[SXZ S-ZZ1SZ Z S-ZZ1SZX ][SXZ SZ-Z1SZX ]-1, n( -  )( -  ) = [SXZ S-Z1Z SZX ]-1[SXZ SZ-1Z SZ Z S-Z1Z SZX ][SXZ S-Z1Z SZX ]-1. The asymptotic covariance matrices of n and n can then, under suitable regularity conditions, be written as [see Bowden and Turkington (1984, pp. 26, 69)]
aV(n ) = lim E[n( - )( - ) ] = plim[n( - )( - ) ], aV(n ) = lim E[n( - )( - ) ] = plim[n( - )( - ) ].

Since SZZ and SZZ coincide asymptotically, we get, letting bars denote plims,

 aV( n



)

=

[SX

Z

S-ZZ1SZX

]-1[SX

Z

SZ-Z1SZZ

S-ZZ1SZX

][SX

Z

S-ZZ1SZX

]-1,

 aV( n



)

=

[SX

Z

SZ-1 Z

SZX

]-1.

Replacing the plims SXZ, SZX , SZZ and SZZ by their sample counterparts, SXZ, SZX , SZZ and SZ^^Z and dividing by n, we get the following estimators of the asymptotic
covariance matrices of  and  :

V( )

=

1 n

[SX

Z

S-ZZ1SZX

]-1[SX

Z

S-ZZ1SZ^^

Z

S-ZZ1SZX

][SX

Z

S-ZZ1SZX

]-1

= [X PZX]-1[X PZ PZX][X PZX]-1,

V( )

=

1 n

[SX

Z

S-Z^1^

Z

SZX

]-1

= [X

Z(Z 

Z)-1Z

X]-1

= [X

PZ ( 

)X]-1.

These are the generic expressions for estimating variances and covariances of the
GMM estimators (10.28) and (10.29). When calculating  in practice, we replace PZ() by PZ( ) = Z(Z  Z)-1Z [see White (1982, 1984)].

10.2.6 Estimation by GMM, Combining Differences and Levels
Following this general description of the GMM, we can construct estimators of  by replacing the expectations in (10.14)­(10.17) by sample means taken over i and minimizing their distances from the zero vector. There are several ways in which this idea can be operationalized. We can
(i) Estimate equations in differences, with instruments in levels, using (10.14) and/or (10.15) for (a) one (t,  ) and one p, (b) one (t,  ) and several p, or (c) several (t,  ) and several p jointly.

336

E. Biørn and J. Krishnakumar

(ii) Estimate equations in levels, with instruments in differences, using (10.16) and/or (10.17) for (a) one t and one (p, q), (b) one t and several (p, q), or (c) several t and several (p, q) jointly.
In cases (i.a) and (ii.a), we obtain an empirical distance equal to the zero vector, so no minimization is needed. This corresponds, formally, to the situation with `exact identification' (exactly as many OCs as needed) in classical IV estimation. In cases (i.b), (i.c), (ii.b), and (ii.c), we have, in a formal sense, `overidentification' (more than the necessary number of OCs), and therefore construct `compromise estimators' by minimizing appropriate quadratic forms in the corresponding empirical distances.
We now consider cases (a), (b), and (c) for the differenced equation and the level equation.
(a) Simple period specific IV estimators
Equation in differences, IVs in levels. The sample mean counterpart to (10.14) and (10.15) for one (t,  , p) gives the estimator

 p(t) = [Ni=1 zip(xit )]-1[Ni=1 zip(yit )],

(10.30)

where zip = xip or equal to xip with one element replaced by yip. Equation in levels, IVs in differences. The sample mean counterpart to (10.16) and (10.17) for one (t, p, q) gives the estimator

 (pq)t = [Ni=1(zipq)xit ]-1[Ni=1(zipq)yit ],

(10.31)

where  zipq =  xipq or equal to  xipq with one element replaced by yipq. Using (10.14)­(10.17) we note that
· When zip = xip (p =  ,t) and zipq = xipq (t = p, q), Assumption (B2) is necessary for consistency of  p(t) and  (pq)t . If yip is included in zip (p =  ,t), and ypq is included in zipq (t = p, q), Assumption (C2) is also necessary for consistency of  p(t) and  (pq)t .
· Assumptions (D1) and (D2) are necessary for consistency of  (pq)t, but they are not necessary for consistency of  p(t). Since the correlation between the regressors and the instruments, say between zip
and  xit , may be low, (10.30) and (10.31) may suffer from the `weak instrument problem', discussed in Nelson and Startz (1990), Davidson and MacKinnon (1993, pp. 217­224), and Staiger and Stock (1997). The following estimators may be an answer to this problem.
(b) Period specific GMM estimators
We next consider estimation of  in (10.4) for one pair of periods (t,  ), utilizing as IVs for  xit all admissible xips, and estimation of  in (10.3), for one period (t), utilizing as IVs for xit all admissible  xipqs.

10 Measurement Errors and Simultaneity

337

To formalize this, we define the selection and differencing matrices





((T -2) × T ) matrix

Pt

=



obtained by deleting from the T -dimensional identity matrix

 ,

rows t and 





d21

Dt

=



...
dt-1,t dt+1,t dt+2,t
...

-2 -1 +1

 ,

t,  = 1, . . . , T,

dT,T -1

where dt is the (1 × T ) vector with element t equal to 1, element  equal to -1 and
zero otherwise, so that Dt is the are one-period [(T -2) × T )] differencing matrix, except that dt,t-1 and dt+1,t are replaced by their sum, dt+1,t-1.5 We use the notation

yi· = (yi1, . . . , yiT ) , yi(t) = Pt yi·,
yi(t) = Dt yi·,

Xi· = (xi1, . . . , xiT ) ,
xi(t) = Pt Xi·, Xi(t) = Dt xi·,

xi(t) = vec(Xi(t)) , xi(t) = vec(Xi(t)) ,

etc. Here Xi(t) denotes the [(T -2) × K] matrix of x levels obtained by deleting rows t and  from Xi·, and Xi(t) denotes the [(T -2) × K] matrix of x differences
obtained by stacking all one-period differences between rows of Xi· not including period t and the single two-period difference between the columns for periods t + 1

and t - 1. The vectors yi(t) and yi(t) are constructed from yi· in a similar way. Stacking yi(t), yi(t), xi(t), and  xi(t), by individuals, we get





Y(t )

=



y1(t ) ...

 ,

yN(t )





Y(t)

=



y1(t) ...

 ,

yN(t)





X(t )

=



x1(t ) ...

 ,

xN(t )





X(t)

=



x1(t) ...

 ,

xN(t)

which have dimensions (N × (T -2)), (N × (T -2)), (N × (T -2)K), and (N × (T -2)K), respectively. These four matrices contain the IVs to be considered below.

Equation in differences, IVs in levels. Write (10.4) as

yt = Xt +  t ,
where yt = (y1t , . . . , yNt ) , Xt = ( x1t , . . . ,  xNt ) , etc. Using X(t) as IV matrix for Xt , we obtain the following estimator of  , specific to period (t,  ) differences and utilizing all admissible x level IVs,
5 The two-period difference is effective only for t = 2, . . . , T -1.

338

E. Biørn and J. Krishnakumar

-1

-1

 x(t ) = (Xt ) X(t ) X(t )X(t ) X(t )(Xt )

-1
× (Xt ) X(t ) X(t )X(t ) X(t )(yt )

-1

-1

= i(xit )xi(t ) i xi(t )xi(t )

i xi(t )(xit )

-1

× i(xit )xi(t ) i xi(t )xi(t )

i xi(t)(yit ) .

(10.32)

It exists if X(t)X(t) has rank (T -2)K, which requires N  (T -2)K. This GMM estimator, which exemplifies (10.28), minimizes the quadratic form:

1 N

X(t

)

t

1

-1

N2 X(t )X(t )

1 N

X(t

)

t

.

The weight matrix (N-2X(t)X(t))-1 is proportional to the inverse of the (asymptotic) covariance matrix of N-1X(t) t when  it is IID across i, possibly with a

variance depending on (t,  ). The consistency of  x(t) relies on Assumptions (B2) and (E).

Interesting modifications of  x(t) are:

(1) If var(it ) = it varies with i and is known, we can increase the efficiency of (10.32) by replacing xi(t)xi(t) by xi(t)it xi(t), which gives an asymptoti-

cally optimal GMM estimator.6 i xi(t)it xi(t) for unknown it proceeds as in (10.29).

(2)

I(nXs(ttea)d... Yo(tf

using )).

X(t )

as

IV

matrix

for

 Xt ,

as

in

(10.32),

we

may

use

Equation in levels, IVs in differences. Write (10.3) as

yt = ceN + Xt +  t ,

where eN is the N-vector of ones, yt = (y1t , . . . , yNt ) , Xt = (x1t , . . . , xNt ) , etc. Using  X(t) as IV matrix for Xt , we get the following estimator of  , specific to period t levels, utilizing all admissible x difference IVs,

6 For a more general treatment of asymptotic efficiency in estimation with moment conditions, see Chamberlain (1987) and Newey and McFadden (1994).

10 Measurement Errors and Simultaneity

339

-1

-1

 x(t) = Xt (X(t)) (X(t))(X(t)) (X(t)) Xt

-1
× Xt (X(t)) (X(t)) (X(t)) (X(t)) yt

-1

-1

= i xit (xi(t)) i(xi(t))(xi(t)) i(xi(t))xit

-1
× i xit (xi(t)) i(xi(t))(xi(t)) i(xi(t))yit . (10.33)

It exists if (X(t)) (X(t)) has rank (T -2)K, which again requires N  (T -2)K. This GMM estimator, which also exemplifies (10.28), minimizes the quadratic form:

1 N

(X(t))



t

1 N2

(X(t))

(X(t))

-1

1 N

(X(t))



t

.

The weight matrix [N-2(X(t)) (X(t))]-1 is proportional to the inverse of the (asymptotic) covariance matrix of N-1(X(t)) t when it is IID across i, possibly

with a variance depending on t. The consistency of  x(t) relies on (B3), (D1), (D2), and the validity of (E3) for all (p, q).

Interesting modifications of  x(t) are:

(1) If var(it ) = it varies with i and is known, we can increase the efficiency

of (10.33) by replacing (xi(t)) (xi(t)) by (xi(t)) it (xi(t)), which gives an

asymptotically optimal GMM estimator. Estimation of i(xi(t)) it (xi(t)) for

unknown it proceeds as in (10.29).

(2)

I(nsXte(ta)d...

of using Y(t)).

X(t)

as

IV

matrix

for

Xt ,

as

in

(10.33),

we

may

use

If we replace assumptions (B2) and (C2) by (B1) or (C1) with arbitrary , we

must ensure that the IVs have a lead or lag of at least  +1 periods to the regressor,

to `get clear of' the  period memory of the MA() process. Formally, we then

replace Pt and Dt by7





matrix obtained by

Pt

( )

=



deleting from the T -dimensional
identity matrix rows  - , . . . ,  +





,

and t - , . . . ,t + 





d21

Dt ( )

=



...
dt - -1,t - -2 dt + +1,t - -1 dt + +2,t + +1
...



,

t,  = 1, . . . , T,

dT,T -1

and otherwise proceed as above.
7 The dimension of these matrices depends in general on .

340

E. Biørn and J. Krishnakumar

(c) Composite GMM estimators We finally consider GMM estimation of  when we combine all essential OCs delimited by Propositions 10.1 and 10.2. We here assume that either (B1) and (C1) with  = 0 or (B1) and (B2) are satisfied. If  > 0, we can proceed as above, but must ensure that the variables in the IV matrix have a lead or lag of at least  +1 periods to the regressor, to `get clear of' the  period memory of the MA() process, confer Part B of Propositions 10.1 and 10.2.

Equation in differences, IVs in levels. Consider (10.5) for all  = t - 1 and all  = t - 2. These (T -1) + (T -2) equations stacked for individual i read

 yi21   xi21 

 i21 



yi32 ...
yi,T,T -1 yi31 yi42 ...



=



xi32 ...
xi,T,T -1 xi31 xi42 ...



+



i32 ...
i,T,T -1 i31 i42 ...



,

yi,T,T -2

xi,T,T -2

i,T,T -2

(10.34)

or, compactly,

yi = (Xi) +  i.

The IV matrix, according to Proposition 10.1, is the ((2T -3) × KT (T -2)) matrix8





Zi

=



xi(21) 0 ... 0
0
0 ...

0
xi(32) ... 0
0
0 ...

···
··· ... ···
···
··· ...

0
0 ... xi(T,T -1) 0
0 ...

0
0 ... 0
xi2
0 ...

0
0 ... 0
0
xi3 ...

···
··· ... ···
···
··· ...

0
0 ... 0
0
0 ...

 .

0 0 ··· 0

0 0 · · · xi,T -1

(10.35)

Let y = [(y1) , . . . , (yN) ] ,  = [( 1) , . . . , ( N) ] , X = [(X1) , . . . , (XN) ] , Z = [Z1, . . . , ZN] .

8 Formally, we here use different IVs for the (T -1) + (T -2) different equations in (10.4), with  as a common slope coefficient.

10 Measurement Errors and Simultaneity

341

The GMM estimator corresponding to E[Zi(i)] = 0T(T-2)K,1, which minimizes [N-1( ) Z](N-2V)-1[N-1Z ( )] for V = Z Z, can be written as

 Dx = (X) Z(Z Z)-1Z (X) -1 (X) Z(Z Z)-1Z (y) = [i(Xi) Zi] [i ZiZi]-1 [i Zi(Xi)] -1
× [i(Xi) Zi] [i ZiZi]-1 [i Zi(yi)] .

(10.36)

It is possible to include not only the essential OCs, but also the redundant OCs when constructing this GMM estimator. The singularity of Z Z when including all OCs, due to the linear dependence between the redundant and the essential OCs, may be treated by replacing standard inverses in the estimation formulae by generalised
(Moore-Penrose) inverses. The resulting estimator is  Dx, which is shown formally in Biørn and Klette (1998).
If  has a non-scalar covariance matrix, a more efficient GMM estimator is obtained for V = VZ() = E[Z ( )( ) Z], which gives

 Dx = (X) ZV-Z(1)Z (X) -1 (X) ZVZ-(1)Z (y) .

(10.37)

We

can

estimate

1 N

VZ(

)

consistently

from

the

residuals

obtained

from

(10.37),

 i = yi - (Xi) Dx, by means of [see White (1984, Sects. IV.3 and VI.2) and (1986, Sect. 3)]

 VZ ( )
N

=

1 N

N
Zi( i)( i)
i=1

Zi.

(10.38)

Inserting (10.38) in (10.37), we get the asymptotically optimal (feasible) GMM estimator9

 Dx = [i(Xi) Zi][i Zi i iZi]-1[i Zi(Xi)] -1

× [i(Xi) Zi][i Zi i iZi]-1[i Zi(yi)] .

(10.39)

(Txhi(ets,te-1)e...sytiim(t,ta-to1)r)s

can be and all xit

tmoo(dxiifit ...eydit ),bwyhiecxhteanlsdoinegxplionit

(10.37) the OCs

all xi(t,t-1) in the ys.

to

Equation in levels, IVs in differences. Consider next the T stacked level equations

for individual i [confer (10.3)]

      



yi1 ...



=



c ...



+



xi1 ...



+ 

i1 ...

 ,

(10.40)

yiT

c

xiT

iT

9 It is possible to include the redundant OCs also when constructing this GMM estimator. Using generalised (Moore-Penrose) inverses, the estimator remains the same.

342

E. Biørn and J. Krishnakumar

or, compactly,

yi = eT c + Xi +  i.

The IV matrix, according to Proposition 10.2, is the (T × T (T -2)K) matrix10

 xi(1) · · · 0  Zi =  ... . . . ...  .
0 · · · xi(T )

(10.41)

Let y = [y1, . . . , yN] ,  = [ 1, . . . , N] , X = [X1, . . . , XN] , Z = [(Z1) , . . . , (ZN) ] .
The GMM estimator corresponding to E[(Zi) i] = 0T(T-2)K,1, which minimizes

[N-1 (Z)](N-2V)-1[N-1(Z)  ] for V = (Z) (Z), can be written as
 Lx = X (Z)[(Z) (Z)]-1(Z) X -1 × X (Z)[(Z) (Z)]-1(Z) y
= [i Xi(Zi)] [i(Zi) (Zi)]-1 [i(Zi) Xi] -1
× [i Xi(Zi)] [i(Zi) (Zi)]-1 [i(Zi) yi] .

(10.42)

If  has a non-scalar covariance matrix, a more efficient GMM estimator is obtained for V = V(Z) = E[(Z)  (Z)], which gives

 Lx = X (Z)V(-1Z) (Z) X -1 X (Z)V-(1Z) (Z) y .

(10.43)

We

can

estimate

1 N

V(Z)

consistently

from

the

residuals

obtained

from

(10.43),

by

 V(Z)
N

=

1 N

N
(Zi)
i=1

 i i(Zi).

(10.44)

Inserting (10.44) in (10.44), we get the asymptotically optimal (feasible) GMM estimator

-1

-1

 Lx = [i Xi(Zi)] i(Zi)  i i (Zi) [i(Zi) Xi]

-1
× [i Xi(Zi)] i(Zi)  i i (Zi) [i(Zi) yi] .

(10.45)

10 Again, we formally use different IVs for different equations, considering (10.40) as T different equations with  as a common slope coefficient.

10 Measurement Errors and Simultaneity

343

These estimators can be modified by extending all xi(t) to (xi(t) ... yi(t)) in (10.41), which also exploit the OCs in the ys. Other moment estimators, which will not be discussed specifically in the present EIV context, are considered for situations with
predetermined IVs in Ziliak (1997), with the purpose of reducing the finite sample
bias of asymptotically optimal GMM estimators.

10.2.7 Extensions: Modifications
All the methods presented so far rely on differencing as a way of eliminating the individual effects, either in the equation or in the instruments. This is convenient for the case where the individual heterogeneity has an unspecified correlation with the latent regressor vector and for the fixed effects case. Other ways of eliminating this effect in such situations are discussed in Wansbeek (2001). Their essence is to stack the matrix of covariances between the regressand and the regressors and eliminating these nuisance parameters by suitable projections. Exploiting a possible structure, suggested by our theory, on the covariance matrix of the  it s and i across individuals and periods, may lead to further extensions. Additional exploitable structure may be found in the covariance matrix of the yit s. The latter will, however, lead to moment restrictions that are quadratic in the coefficient vector  . Under non-normality, higher order moments may also, in principle, be exploited to improve efficiency, but again at the cost of a mathematically less tractable problem.
In a random effects situation, with zero correlation between  it and i, and hence between xit and i, differencing or projecting out the is will not be efficient, since they will not exploit this zero correlation. The GLS estimator, which would have been the minimum variance linear unbiased estimator in the absence of measurement errors, will no longer, in general, be consistent [see Biørn (1996, Sect. 10.4.3)], so it has to be modified. Finally, if the equation contains strongly exogenous regressors in addition to the error-contaminated ones, further moment conditions exist, which can lead to improved small sample efficiency of the GMM estimators. An improvement of small sample efficiency may also be obtained by replacing IV or GMM by LIML estimation; see Wansbeek and Meijer (2000, Sect. 6.6).

10.2.8 Concluding Remarks
Above we have demonstrated that several, rather simple, GMM estimators which may handle jointly the heterogeneity problem and the measurement error problem in panel data, exist. These problems may be `intractable' when only pure (single or repeated) cross section data or pure time series data are available. Estimators using either equations in differences with level values as instruments, or equations in levels with differenced values as instruments are useful. In both cases, the differences may be taken over one period or more.

344

E. Biørn and J. Krishnakumar

Even for the static model considered here, instruments constructed from the regressors (xs) as well as from the regressands (ys) may be of interest. GMM estimators combining both instrument sets in an optimal way are usually more precise than those using either of them. Although a substantial number of orthogonality conditions constructed from differences taken over two periods or more are redundant, adding the essential two-period difference orthogonality conditions to the one-period conditions in the GMM algorithm may significantly affect the result [confer the examples in Biørn (2000)].
Using levels as instruments for differences, or vice versa, as a general estimation strategy within a GMM framework, however, may raise problems related to `weak instruments'. Finding operational ways of identifying such instruments among those utilizing essential orthogonality conditions in order to reduce their potential damage with respect to inefficiency, is a challenge for future research.

10.3 Simultaneity and Panel Data
Simultaneous equation models (SEM) or structural models as they are also sometimes called, have been around in the economic literature for a long time dating back to the period when the Econometric Society itself was formed. In spite of this long history, their relevance in modelling economic phenomena has not diminished; if at all it is only growing over time with the realisation that there is a high degree of interdependence among the different variables involved in the explanation of any socio-economic phenomenon.
The theory of simultaneous equations has become a must in any econometric course whatever level it may be. This is due to the fact any researcher needs to be made attentive to the potential endogenous regressor problem, be it in a single equation model or in a system of equations and this is the problem that the SEM theory precisely deals with.
At this stage it may be useful to distinguish between interdependent systems i.e. simultaneous equations and what are called systems of regression equations or seemingly unrelated regressions (SUR) in which there are no endogenous variables on the right hand side but non-zero correlations are assumed between error terms of different equations. We will see later in the section that the reduced form of a SEM is a special case of SUR.
In a panel data setting, in addition to the simultaneous nature of the model which invariably leads to non-zero correlation between the right hand side variables and the residual disturbance term, there is also the possibility of the same variables being correlated with the specific effects. However unlike in the correlated regressors case of Chap. 4 eliminating the specific effect alone does not solve the problem here and we need a more comprehensive approach to tackle it. We will develop generalizations of the two stage least squares (2SLS) and three stage least squares (3SLS) methods that are available in the classical SEM case. These generalizations can also be presented in a GMM framework, giving the corresponding optimal estimation in this context.

10 Measurement Errors and Simultaneity

345

The most commonly encountered panel data SEM is the SEM with error component (EC) structure. Thus a major part of this chapter will the devoted to this extension and all its variants. Other generalizations will be briefly discussed at the end.

10.3.1 SEM with EC

10.3.1.1 The Model

This model proposes to account for the temporal and cross-sectional heterogeneity of panel data by means of an error components structure in the structural equations of a simultaneous equation system. In other words, the specific effects associated with pooled data are incorporated in an additive manner in the random element of each equation.
Let us consider a complete linear system of M equations in M current endogenous variables and K exogenous variables. We do not consider the presence of lagged endogenous variables in the system. The reader is referred to the separate chapter of this book dealing with dynamic panel data models for treatment of such cases.
By a `complete' system, we assume that there are as many equations as there are endogenous variables and hence the system can be solved to obtain the reduced form. Further, we also assume that the data set is balanced i.e. observations are available for all the variables for all the units at all dates. Once again, the case of unbalanced panel data sets is dealt with in a separate chapter of the book.
We write the M­th structural equation of the system as follows:11

yit m + xit m + umit = 0, m = 1, . . . M

(10.46)

where yit is the (1 × M) vector of observations on all the M endogenous variables for the i-th individual at the t-th time period; xit is the (1 × K) vector of observations on all the K exogenous variables for the i-th individual at the t-th time period; m and m are respectively the coefficient vectors of yit and xit ; and umit is the disturbance term of the m-th equation for the i-th individual and the t-th time period.
More explicitly,

yit = [y1it . . . yMit ]; xit = [x1it . . . xKit ]; m = [1m . . . M m]; m = [1m . . . Km] .

11 Note that the constant term is included in the  vector, contrary to the introductory chapters, and hence xit contains 1 as its first element.

346

E. Biørn and J. Krishnakumar

By piling up all the observations in the following way:



y11

Y

=



... y1T ...



;



x11

X

=



... x1T ...



;



um11

um

=



...
um1T ...



,

yNT

xNT

umNT

equation (10.46) can be written as:

Y m + Xm + um = 0, m = 1, . . . M

Defining

 = [1 . . . M ]; B = [1 . . . M ]; U = [u1 . . . uM] ,

we can write the whole system of M equations as:

(10.47)

Y+XB+U = 0 .

(10.48)

Before turning to the error structure, we add that the elements of  and B satisfy certain a priori restrictions, crucial for identification, in particular the normalisation rule (ii = -1) and the exclusion restrictions (some elements of  and B are identically zero).
Following an error components pattern, it is assumed that each structural equation error umit is composed of three components: an individual effect mi, a time effect mt and a residual error mit . Formally, we have:
Assumption 1:

umit = mi + mt + mit ,

m = 1, . . . , M i = 1, . . . , N t = 1, . . . , T .

(10.49)

By denoting



lT (1 × T ) = [1 . . . 1];

 m1
m =  ...  ; mN

 m1
m =  ...  ; mT

m11

m

=



...
m1T ...

 ,

mNT

the above decomposition (10.49) can be written for all the observations, as:

um = (IN  lT )m + (lN  IT )m + m, m = 1, . . . , M .

10 Measurement Errors and Simultaneity

347

Assumption 2:

E(m) = 0; E(m) = 0; E(m) = 0 , m = 1, . . . , M .

Assumption 3:

Assumption 4:

E(mm ) = mm IN , E(mm ) = mm IT , E(mm ) = mm INT ,

m, m = 1, . . . , M m, m = 1, . . . , M m, m = 1, . . . , M

E(mm ) = 0; E(mm ) = 0; E(mm ) = 0, m, m .

We will also assume independence, two by two, among the different components whenever required, and normality of their distribution for ML estimation.

Assumption 5:
The error components are independent of the exogenous variables.
From these assumptions, the covariance matrix between um and um , denoted as mm , can be derived as:

mm = E(umum ) = mm (IN  lT lT ) + mm (lN lN  IT ) + mm INT . (10.50)

The spectral decomposition of mm is given by (see Nerlove (1971))

mm = 1mm M1 + 2mm M2 + 3mm M3 + 4mm M4

(10.51)

where and

1mm = mm 2mm = mm + T mm 3mm = mm + Nmm 4mm = mm + T mm + Nmm

M1

=

INT

-

1 T

(IN

 lT lT ) -

1 N

(lN lN

 IT ) +

1 NT

lNT lNT

of rank m1 = (N - 1)(T - 1);

M2

=

1 T

(IN



lT

lT

)

-

1 NT

lNT

lNT

of rank m2 = N - 1;

M3

=

1 N

(lN lN



IT

)

-

1 NT

lNT

lNT

of rank m3 = T - 1;

1 M4 = NT lNT lNT

of rank m4 = 1

(10.52)

348

E. Biørn and J. Krishnakumar

with Further, we note that

4
 Mi = INT ; MiMj = i jMi .
i=1

By denoting

lNT Mi = 0 , i = 1, 2, 3 ; lNT M4 = lNT .

 = [mm ] ,  = [mm ] ,  = [mm ] , m, m = 1, . . . , M, relations (10.52) can be written in matrix form as:

1 =  ; 2 =  + T  ; 3 =  + N ; 4 =  + T  + N .

Note that  ,  and  are uniquely determined from 1, 2, 3, 4 and vice versa.

Finally, the variance­covariance matrix of the structural form can be verified

to be:

4
 = E((vec U)(vec U) ) =  i  Mi

(10.53)

i=1

with i = [imm ] m, m = 1, . . . M for i = 1, 2, 3, 4.

The inverse and determinant of  (useful for the estimation procedures of later sections) are given by (see Baltagi (1981) or Balestra and Krishnakumar (1987)):

4
 -1 = -i 1  Mi ; |  | = 4i=1 | i |mi . i=1

(10.54)

10.3.1.2 The Reduced Form and the Identification Problem

By definition, the reduced form of a system of simultaneous equations is the solution of the system for the endogenous variables in terms of the exogenous variables and the disturbances. For our model, it is given by:

Y = X+V

where

 = -B-1 ; V = -U-1 .

By using the properties of vec, we can write

vec V = (--1  I) vec U

and thus we have:

E(vec V ) = 0

10 Measurement Errors and Simultaneity

349

and

 = E((vec V )(vec V ) ) = (--1  I)(--1  I)
4
 = -1 i-1  Mi i=1
4
=  i  Mi i=1

where

i = -1 i-1 , i = 1, 2, 3, 4.

It can be easily verified that each reduced form equation has a three components error structure like any structural equation and the covariances across different reduced form equations are also of the same nature as those across different structural equations. However, an important point in which the reduced form differs from the structural form is that the right hand side variables of the former are uncorrelated with the errors whereas it is not the case in the latter due to simultaneity.
Thus the reduced form is a seemingly unrelated regression (SUR) model with error components. This model was originally proposed by Avery (1977) and is an important extension of panel data specifications to systems of equations. Our reduced form is in fact a special case of such a model as the explanatory variables are the same in each equation. Avery (1977) treated a more general case in which each equation has its own set of explanatory variables. This interpretation of our reduced form enables us to provide an interesting application of Avery's model combining SUR with error components (EC). We do not intend to go into the details of the inference procedures for the reduced form for want of space. In general, both ML and feasible GLS can be applied. Both are consistent, asymptotically normal and equivalent. The reader is referred to Krishnakumar (1988) for detailed derivations.
In the context of any simultaneous equation model, it is important to consider the problem of identification prior to estimation. In the case of the classical simultaneous equation model (with homoscedastic and non­auto­correlated errors), there is abundant literature on identification (see, for instance, Koopmans (1953), Fisher (1966), Rothenberg (1971), and Hausman and Taylor (1983)).
In our case of SEM with EC, as long as there are no a priori restrictions on the structural variances and covariances (i.e. no `covariance restrictions' in the terminology of Hausman and Taylor), the identification problem is exactly the same as that of the classical model. In other words, in such a situation, we can separate the discussion on the identification of  and B from that of the i (s), i = , , . Thus, we would have the same rank and order conditions of identifiability of the elements of  and B, and the same definitions of under-identified, just-identified and overidentified equations. Once the structural coefficients are identified, the identification of the structural variance­covariance matrices is immediate, through the equations relating them to the reduced form covariance matrices.

350

E. Biørn and J. Krishnakumar

Now, if we impose additional a priori restrictions on the structural variances and covariances, then it is no longer possible to separate the equations relating (, B) to  from those relating i(s) to i(s), i = , ,  and one has to study the existence and uniqueness of solutions for the full system consisting of all the identifying
equations, given the prior restrictions. This has been done for the classical simul-
taneous equation model by Hausman and Taylor (1983). One can follow the same
approach for our model but one has to keep in mind the fact that, in the classical case, there is only one  whereas in our case there are three of these sets of relations: i -1i , i = , , .
One type of a priori covariance restrictions that do not need any particular analysis is that either  or  is identically equal to zero (i.e. only one specific effect is present in the model) and hence is identified. Note that, in this case, the corresponding  matrix ( or  ) is also zero and the spectral decomposition of  (and ) is reduced to two terms only.

10.3.1.3 Structural Form Estimation

Generalised Two Stage Least Squares

Let us consider a structural equation, say the m­th one and write it as:

ym = Ymm + Xmm + um ,

(10.55)

in which the normalisation rule (mm = -1) and the exclusion restrictions are already substituted. We assume that these are the only a priori information available. Note that Ym and Xm denote the matrices of observations on the Mm included endogenous and Km included exogenous right hand side variables respectively and m
and m denote their respective coefficients. By defining

Zm = [Ym Xm];

m =

m m

,

we can rewrite (10.55) as

ym = Zm m + um

and we recall (see (10.51)) that

(10.56)

4
 E(um um ) = mm = imm Mi . i=1
The endogenous right hand side variables of (10.55) are correlated with both the individual effects and the residual error term. Hence classical methods like the OLS, GLS, or within will all yield inconsistent estimators and an appropriate procedure

10 Measurement Errors and Simultaneity

351

is given by the IV method which typically consists in premultiplying the equation
in question by a matrix of valid instruments and then applying GLS to the trans-
formed equation.
In the classical case, the instrument for Zm is taken to be X (see, for instance, Theil (1971)). In our case, it can be shown that, of all the transformations of X,
say FX, the one which minimises the asymptotic variance­covariance matrix of the resulting estimator of m, is given by F = -mm1 . In other words, any other transformation would lead to an estimator with an asymptotic variance­covariance matrix `greater' than the one obtained using -mm1 (`greater' is used to mean that the difference would be positive definite). This result is based on Theorem 5 of
Balestra (1983). Its application to our model can be found in Krishnakumar (1988). Therefore the optimal instrument for Zm is given by m-m1 X and premultiplying
(10.56) by X -mm1 , we get:

X m-m1 ym = X -mm1 Zmm + X m-m1 um .

(10.57)

Applying GLS on (10.57), we obtain what we call the generalised two stage least squares (G2SLS) estimator of m:

m,G2SLS =[Zmm-m1 X (X m-m1 X )-1X m-m1 Zm]-1 × Zmm-m1 X (X -mm1 X )-1X -mm1 ym

(10.58)

Now, the above estimator is not feasible as mm is unknown. Hence we need a prior estimation of the variance components. By analysis of variance (cf. Amemiya (1971)) of the errors of the m­th structural equation, the following estimators of the imm(s) are obtained:

1mm

=

1 (N - 1)(T

- 1) umM1um

2mm

=

1 N - 1 umM2um

3mm

=

1 T - 1 umM3um

4mm = 2mm + 3mm - 1mm

(10.59)

These formulae contain um which is also unknown. However, it can be estimated as follows. Premultiplying (10.56) by the instrument M1X, we get:
X M1ym = X M1Zmm + X M1um .

Note that, if the equation has an intercept, it gets eliminated by this transformation and we will be left with:

X M1ym = X M1Zm m + X M1um

(10.60)

where Zm denotes the matrix of right hand side variables excluding the vector of ones and m the respective coefficients. That is, we have split Zm and m as:

352

E. Biørn and J. Krishnakumar

Zm = [YmlNT Xm ] ;

 m
m =  am  bm

and redefined Zm and m as

Zm = [Ym Xm] ;

m =

m bm

.

Performing GLS on (10.60), we obtain a consistent estimator of m called the covariance or the within 2SLS estimator:

m,cov2SLS = [Zm M1X (X M1X )-1X M1Zm ]-1Zm M1X (X M1X )-1X M1ym (10.61)

The intercept is estimated as:

am,cov2SLS

=

1 NT

lNT (ym

- Zm m ,cov2SLS)

.

From these estimators, we can predict um as:

um,cov2SLS = ym - Zm m,cov2SLS - lNT am,cov2SLS .

Substituting um,cov2SLS for um in (10.59), we obtain imm, i = 1, 2, 3, 4 and mm = 4i=1 immMi, leading to the following feasible G2SLS estimator of m:

m,fG2SLS = [Zmm-m1 X (X m-m1 X )-1X m-m1 Zm]-1 ×Zmm-m1 X (X m-m1 X )-1X m-m1 ym

(10.62)

Before giving the limiting distribution of the above estimators, we just mention

that all our estimators are consistent. Another interesting point to note is that all the

three estimators--Cov2SLS, G2SLS, fG2SLS--have the same limiting distribution.

It is given by (see Krishnakumar (1988) for derivation):



N NT

(am - am) (m - m )

 N 0,

mm + mm

0

0

 mm (Pm Rm Pm )-1

where





10

Pm =  0 m  ; 0 Hm

Rm =

1/(mm + mm)

0

0

1/(mm)R

with m being the coefficient matrix of X in the reduced form equations for Ym except for the column of ones and Hm being a selection matrix such that Xm = XHm .

10 Measurement Errors and Simultaneity

353

Generalised Three Stage Least Squares

The extension from G2SLS to generalised 3SLS (G3SLS) can be done in two ways. In what follows, we present both the ways and show that they yield asymptotically equivalent estimators.
The reader will recall that the G2SLS method uses the instrument matrix -mm1 X for the m­th equation. Applying to each structural equation of the system, its corresponding transformation given by -mm1 X, m = 1, 2, . . . , M, we obtain:
X -111y1 = X 1-11Z11 + X 1-11u1 ...
X -M1MyM = X M-1MZMM + X M-1Mum

or where12

X -1y = X -1Z + X -1u

(10.63)

X = IX D = diag [11 . . . MM] Z = diag [Z1 . . . ZM]  = [1 . . . M] u = [u1 . . . uM] y = [y1 . . . yM]

Now, let us apply GLS to the transformed system (10.63) to obtain our first generalised 3SLS (G3SLS­I) estimator:

G3SLS-I

=

[Z

D-1X (X

D-1D-1

X

)-1X

D-1

-1
Z]

× Z D-1X (X D-1D-1X )-1X D-1y

(10.64)

Note that this way of generalising is analogous to the way that classical 2SLS is
extended to 3SLS by Zellner and Theil (1962). However, there is also a second way
of approaching the problem, that we briefly present below. Recall that our reason for choosing m-m1 X as the instrument for Zm in the G2SLS
procedure was that it minimised the asymptotic covariance matrix of the resulting
coefficient estimator. Now, let us write the whole system as:

y = Z +u

with E(u) = 0 and E(uu ) = 

12 Note that this particular notation for D is valid only for this chapter.

354

E. Biørn and J. Krishnakumar

and find the best transformation F of (I  X) for choosing the instruments. By the same reasoning as for G2SLS, we would get F = -1. Using -1(I  X) as instruments and estimating  by GLS on the transformed system yields our second
G3SLS (G3SLS­II) estimator:

G3SLS-II = [Z -1X (X -1X )-1X -1Z]-1 × Z -1X (X -1X )-1X -1y

(10.65)

Both these G3SLS estimators can be made feasible by replacing the variance components present in  by their corresponding estimates given by analysis of variance:

1mm

=

1 (N - 1)(T - 1)

um M1 um

2mm

=

1 N-1

3mm

=

1 T -1

um M2 um um M3 um

4mm = 2mm + 3mm - 1mm

(10.66)

for m, m = 1, . . . M. Note that for the um(s), we can take:

um,cov2SLS = ym - Zmm,cov2SLS

or um,fG2SLS = ym - Zmm,fG2SLS

or even

um,cov3SLS = ym - Zmm,cov3SLS

where m,cov3SLS is yet another 3SLS estimator obtained by using the instrument matrix (I  M1X) for the system and estimating by GLS.
From the estimates of imm (s) given by (10.66), we form  = i4=1 i  Mi with i = [imm ], m, m = 1, . . . , M and use it in (10.64) and (10.65) to get the feasible G3SLS estimators.

It is remarkable that due to the special structure of the error-components covari-

ance matrix, all these 3SLS estimators, namely the pure G3SLS-I, pure G3SLS-II,

cov3SLS, feasible G3SLS-I and feasible G3SLS-II, have the same limiting distribu-

tion given by:



N NT

(a - a) ( - )

N

0,

 + 

0

0 [¯ (- 1  R)¯ ]-1

where a is a (M × 1) column vector containing the intercepts of each equation i.e. a = [a1 . . . aM] and  is ((M - 1)M × 1) containing the other non-zero

10 Measurement Errors and Simultaneity

355

coefficients of each equation i.e.  = [1 . . . M ] and where ¯ = diag ([m Hm ]), m = 1, . . . , M.
Finally, let us note that, though we assume the presence of an intercept in each equation, the above results can be easily generalised to the case in which some
equations have an intercept and others do not.

Error Components Two Stage Least Squares

This is an alternative method of estimating the parameters of a single structural

equation. This method is proposed by Baltagi (1981) and inspired from the feasible Aitken procedure developed by Maddala (1971) for a single equation error compo-

nents model.

In this method, the structural equation in question say the m­th one, is

successively transformed by the matrices of eigenvectors associated with the dis-
tinct characteristic roots of mm and GLS is performed on a system comprising all
the three transformed equations. Before going further, let us introduce some more
notations. From Sect. 10.3.1.1 we know that the distinct eigenvalues of mm are 1mm , 2mm , 3mm and 4mm . The matrices whose columns are the eigenvectors associated with these roots are Q1 , Q2 ,Q3 and lNT / NT respectively where Q1 = C2  C1 ,Q2 = C2  lT / T , Q3 = lN / N  C1 such that OT = [lT / T C1] and ON = [lN/ N C2] are orthogonal. Note that Q jQ j are unique for j = 1, 2, 3 and Q jQ j = Mj, j = 1, 2, 3.
Now, let us apply the transformations Q j, j = 1, 2, 3 to our structural equa-
tion (10.56):

Q jym = Q jZmm + Q jum, j = 1, 2, 3 .

(10.67)

It is easily verified that

E(Q jumumQ j) =

 jmmIm j 0

for j = j for j = j

Thus the transformed errors have a scalar variance­covariance matrix but are still correlated with the right hand side variables. Hence an IV technique is used with Q jX as instruments for Q jZm. This gives:

m( j,)2SLS = [ZmQ jQ jX (X Q jQ jX )-1X Q jQ jZm]-1 × ZmQ jQ jX (X Q jQ jX )-1 X Q jQ jym,

j = 1, 2, 3 (10.68)

These 2SLS estimators are in turn used to estimate the variance components:

 jmm

=

1 mj

(Q

j

ym

-

Q

j

Zm

m( j,)2SLS)

(Q jym - Q jZmm( j,)2SLS),

4mm = 2mm + 3mm - 1mm

j = 1, 2, 3 (10.69)

356

E. Biørn and J. Krishnakumar

This is a generalisation of the Swamy and Arora (1972) method. The above procedure gives three different estimators of the same m. Therefore, we can combine all the three transformed equations of (10.67) together and estimate the whole system by GLS. We have:









X X

Q1Q1ym Q2Q2ym



=

X X

Q1Q1Zm Q2Q2Zm

X  m +  X

Q1Q1um Q2Q2um



.

X Q3Q3ym

X Q3Q3Zm

X Q3Q3um

(10.70)

Using the Swamy and Arora estimates (10.69) of the variance components and performing feasible GLS on (10.70), we get the error components two stage least squares (EC2SLS) estimator:

 m,EC2SLS =

3 j=1

1  jmm ZmQ jQ jX (X

Q jQ jX )-1X

Q jQ jZm

-1

 ×

3 j=1

1  jmm

[ZmQ jQ jX (X

Q jQ jX )-1X

Q jQ jym]

(10.71) (10.72)

It can be shown that the above estimator is a weighted average of the three 2SLS estimators given in (10.68).
The limiting distribution of the EC2SLS estimator is the same as that of the feasible G2SLS estimator.

Error Components Three Stage Least Squares

In this section, we present an extension of the EC2SLS method to the whole system. We start with
y = Z +u
and transform it successively by (IM  Q j), j = 1, 2, 3 to give:

y( j) = Z( j) + u( j), j = 1, 2, 3

(10.73)

where y( j) = (IM  Q j)y ; Z( j) = (IM  Q j)Z ; u( j) = (IM  Q j)u and

E(u( j)u( j) ) =  j  Im j, j = 1, 2, 3 .

Using X( j) = (IM  Q jX) as instruments for Z( j) and applying GLS, we get:

I(Vj)GLS = [Z( j) {-j 1  PX( j) }Z( j)]-1 ×[Z( j) {-j 1  PX( j) }y( j)] j = 1, 2, 3
where for any matrix A, PA denotes the projection matrix A(A A)-1A .

(10.74)

10 Measurement Errors and Simultaneity

357

The unknown variance components are estimated by

 jmm

=

1 mj

[Q

j

ym

-

Q

j

Zm

m( j,)2SLS

]

[Q jym - Q jZm m( j),2SLS] ,

4mm = 2mm + 3mm - 1mm

j = 1, 2, 3

Now, recognising once again that the same  is being estimated three times separately, we can combine all the three transformed systems of (10.73) and estimate the global system by (feasible) IVGLS. The resulting estimator is called the error component 3SLS (EC3SLS) estimator of :

 EC3SLS =

3 j=1

Z( j)

(-j 1



PX( j)

)Z(

j)

-1

 ×

3 j=1

Z(

j)

(-j 1



PX( j) )y( j)

(10.75) (10.76)

The above estimator also has the same interpretation as the EC2SLS one, in that it is a weighted average of the three 3SLS estimators of (10.75) (see Baltagi (1981) for further details).
Finally, the limiting distribution of the EC3SLS estimator can be shown to be the same as that of the G3SLS estimators of the previous section and hence is asymptotically equivalent to them.

Full Information Maximum Likelihood
The full information maximum likelihood (FIML) procedure consists in maximising the log-likelihood function of the model with respect to the structural parameters given the a priori restrictions. As in all constrained maximisation problems, there are two ways of tackling it--(i) by maximising the corresponding Lagrangian function with respect to the same parameters and a set of multipliers associated with the constraints; (ii) by substituting the constraints in the objective function and performing maximisation without constraints. In this section, we will briefly review both the approaches. The reader will note that neither of them yield explicit analytical solutions and hence both require numerical iterative procedures to arrive at the solution. Moreover, in the first approach adopted by Balestra and Krishnakumar (1987) and Krishnakumar (1988), the a priori restrictions on the structural coefficients are assumed to be any linear ones whereas in the second approach followed by Prucha (1985), only the normalisation and exclusion restrictions are considered.
Recalling our structural model:
Y  + XB +U = 0.
and separating the intercept of each equation from the other terms, we can write:
Y  + lNT a + XB +U = 0

358

E. Biørn and J. Krishnakumar

or lNT a + Z +U = 0

where

 =

 B

.

Note that in case only some equations have an intercept and others do not, the fol-

lowing procedure can be easily modified accordingly.

Now, the a priori restrictions on the coefficients can be written as (say we have

p of them):

S0 0 0 S

a vec 

=

s0 s

(10.77)

These include the normalisation rule, the exclusion restrictions and any other linear constraints. To these, we add the symmetry conditions for  j(s) written as:

C vec  j = 0 , j =  ,  ,  .

(10.78)

The log-likelihood function of the model can be written as follows, after a few simplifications and rearrangements:

 lnL

=

const

-

1 2

4
miln
i=1

|

i

|

1 + NTln
2

|

L



|2

-

1 t r(N Taa 2

+ ZlNT a

+ alNT Z)4-1

 -

1 2

tr

4 i=1

ZMiZ-i 1

(10.79)

with L such that  = L. Thus we have to maximise (10.79) with respect to a, ,  ,  and  under the constraints (10.77) and (10.78).
Here again we will not describe the procedure in detail for brevity's sake and the

reader is invited to consult Balestra and Krishnakumar (1987) for more information

on the algorithm to be implemented in order to obtain a numerical solution. We give

below the limiting distribution of the FIML estimator:

 T (aML - a)
NT vec(,ML - )

 N 0,

 + 

0

0 F[F (- 1  P)F]-1F

where

P =

 I

R

( I)

When the a priori restrictions are only the normalisation and exclusions, we have the same limiting distribution for the FIML as the one for the (feasible) G3SLS. Hence, in this case, the FIML and the fG3SLS are of the same asymptotic efficiency.

10 Measurement Errors and Simultaneity

359

As mentioned in the beginning of this section, there is a second approach to the constrained maximisation problem which consists in replacing the constraints in the objective function and then maximising the latter with no constraints. This has been done by Prucha (1985) for our model for the case of the usual restrictions only and is called the normal FIML (NFIML) estimator. The normal equations of the above maximisation programme lead to an IV interpretation of the ML estimator which can be used as an estimator­generating equation to form a general class of estimators called the NFIMLA estimator (the subscript A indicates that the estimator can be viewed as an approximation of the NFIML estimator). Further Prucha also shows that under certain conditions, all members of the NFIMLA class are asymptotically equivalent among themselves and to the NFIML estimator.

10.3.1.4 Asymptotic Comparisons of the Various Structural Estimators
In this section, we will summarise the different asymptotic equivalences mentioned earlier and state a few more results regarding the just­identified case.
First, let us briefly recall the results that we already know in the case of the usual restrictions. We have the asymptotic equivalence of the various 2SLS estimators namely, the cov2SLS, fG2SLS and EC2SLS. Among the system methods, we have the asymptotic equivalence of cov3SLS, fG3SLS-I, fG3SLS-II, EC3SLS and FIML estimators.
Regarding the just-identified case, we will mention the important results without deriving them. The reader is referred to the original works by Krishnakumar (1988) and Baltagi (1981) for proofs.
When a single equation, say the m-th one, is just-identified:
(i) the indirect least squares estimators obtained using the covariance estimator of , is exactly equal to the cov2SLS estimator;
(ii) the indirect least squares estimator obtained using the feasible GLS estimator of  has the same limiting distribution as the feasible G2SLS estimator;
(iii) the EC2SLS estimator can be expressed as a weighted combination of three indirect estimators of m;
(iv) the three 2SLS estimators of (10.68) are respectively equal to the indirect least squares estimators based on the between groups, between time periods and within variations estimators of the reduced form;
(v) all these estimators--feasible G2SLS, cov2SLS, indirect estimators based on cov or fGLS and EC2SLS--are asymptotically equivalent.
When the whole system is just-identified:
(i) fG3SLS-I reduces to fG2SLS whereas the fG3SLS-II does not; (ii) fG3SLS-I, fG3SLS-II, fG2SLS and the indirect estimators are all asymptoti-
cally equivalent; (iii) EC3SLS does not reduce to EC2SLS;

360

E. Biørn and J. Krishnakumar

(iv) EC3SLS and EC2SLS have the same limiting distribution and (v) all these estimators--fG3SLS-I, fG3SLS-II, cov3SLS, fG2SLS, cov2SLS,
EC3SLS, EC2SLS--are asymptotically equivalent.

10.3.1.5 Small Sample Properties
There are essentially two ways of arriving at the small sample behaviour of econometric estimators. One is by analytically deriving the exact distribution or an approximation to it and the other is by `constructing' the distribution through simulations (also called Monte-Carlo experiments).
In the case of the reduced form, the unbiasedness of the various coefficient and variance components estimators is proved without great difficulty (see Krishnakumar (1988)). However, exact efficiency properties are yet to be established and so far nothing is known.
In the case of the structural form estimators, things get very complicated. In the classical simultaneous model, several authors have dealt with the problem of finding the exact distributions of the two stage and three stage estimators. The reader is invited to consult Phillips (1982) for more information in the classical case. In the SEM with EC case, we have no result on the exact density functions of the various structural estimators. However, we do have results on approximations to finite sample moments using series expansions methods. These methods are used even when we have the analytical expression of density functions since they yield much less complicated expressions. In these methods, the estimator is developed around its true value in a series of terms of orders decreasing in the powers of the sample size. Then the series is truncated upto a desired order and the expectation of the truncated series is calculated to get the bias upto that order. This procedure has been applied to our model by Krishnakumar (1988), following the approach of Nagar (1959), to get approximations for the bias of cov2SLS and fG2SLS estimators. We will not go deeper into this aspect here. The results and derivations can be found in Krishnakumar (1988).
Now, we turn to the second approach--the Monte-Carlo study. This method consists in specifying a true model giving values for all the parameters, generating the random elements and the observations on the exogenous variables, calculating the endogenous variables and estimating the parameters using only the observations. By running the procedure a number of times with different sets of observations (keeping the true values unchanged), one can `construct' the distribution curve of the estimator and derive its mean, variance, mean square error and so on. These criteria can be used to compare the performance of different estimation methods. In addition, the whole exercice can be repeated for different sets of true values.
Baltagi (1984) carried out such a Monte-Carlo experiment for the SEM with EC, in which he compared various least squares and IV estimators of a two-equation structural model, keeping the same true values for the coefficients and changing only the values of the variance components. In what follows, we will briefly review

10 Measurement Errors and Simultaneity

361

the main results concerning the structural form and the reduced form. For results regarding the variance components estimators, the reader is referred to Baltagi (1984).
First, the structural form results. The classical 2SLS has a smaller bias than the EC2SLS but the EC2SLS has a lower root mean square error (RMSE) than the classical 2SLS. Better estimates of the structural variance components do not necessarily imply better estimates of the structural coefficients. In general, 3SLS dominates 2SLS and EC3SLS dominates EC2SLS in RMSE though the superiority of EC3SLS over EC2SLS does not hold for all the structural parameters. There is gain in performing EC3SLS rather than classical 3SLS according to RMSE. Similar results are also obtained if we use global criteria like the normalised mean square deviation and the normalised mean absolute deviation which give a single indicator for the combined performance of all parameter estimators.
Now, the reduced form results. Performing feasible GLS on each reduced form equation is better than performing OLS or LSDV, according to RMSE. But, according to the same criterion, feasible GLS on the entire system does not necessarily produce better results than feasible GLS on each equation separately. Baltagi notes that this could be due to the fact that there are only two equations in the model and may not be so in larger models. Once again better estimates of the variance components do not necessarily imply better feasible GLS estimates of coefficients. The same results are maintained even according to global criteria.
Ma´tya´s, and Lovrics (1990) investigate the small scale properties of 5 limited information estimators for SEM with EC models by means of a Monte Carlo study. They compare the OLS estimator, the within estimator, the pure G2SLS, and two feasible G2SLS estimators (one with OLS as the first step and the other with within). Their findings are as follows: The OLS estimator remains biased in all cases. But it is still recommended for very small N and T (N < 10, T < 20) due to its stability as the G2SLS/within 2SLS are unstable and have a large dispersion. For N < 10 and T > 20 they favour the G2SLS/within 2SLS estimators and for a sufficient (N > 15 - 20) as long as T > 5. There is practically no difference between the three G2SLS (pure and the two feasible) estimators.
Baltagi and Chang (2000) study the relative performance of several estimators of a two-equation SEM with unbalanced panel data. Among the single equation methods they compare 2SLS, W2SLS and the EC2SLS and among the system estimators they look at 3SLS, W3SLS and EC3SLS. They observe that most of the results obtained for the balanced case carry over to the unbalanced one.

10.3.2 Extensions
10.3.2.1 Simultaneous Equation Models with Correlated Specific Effects
In the SEM with EC discussed in the previous subsections, it was assumed that the error components were uncorrelated with the exogenous variables. Cornwell et al. (1992) extend our model to the case in which this assumption is dropped.

362

E. Biørn and J. Krishnakumar

They allow for the possibility of only some variables being correlated with the error components (singly exogenous) while the others are independent of them (doubly exogenous). Their model is specified as follows:

ym = Ymm + Xmm + Zmm + m + m , m = 1, . . . , M .

(10.80)

A distinction is also made between time-varying exogenous variables (X) and the time-invariant exogenous variables (Z) and only individual effects are present in the model (i.e. we have only a two-components error term). Denoting,

Rm = [Ym Xm Zm] ;  m = [m  m m] , we can write (10.80) as

ym = Rmm + (m + m) , m = 1, . . . , M
The 2SLS proposed transforms the equation say the first one by 1-121 to get:
-1121 y1 = -1121 R1 1 + 1-121 (1 + 1)
and use instruments of the form A = [QvX PvB] with different choices for B. Three different choices are proposed the first one corresponding to the instrument set of Hausman and Taylor (1981), the second one inspired from Amemiya and McCurdy (1986) and the third based on Breusch (1987). The Three Stage Least Squares generalises the procedure for the whole model.
The authors also derive estimators in the case in which the nature of the correlation between the exogenous variables and the specific effects may vary from equation to equation. In other words, we may have an exogenous variable correlated with the specific effect in one equation but uncorrelated with the specific effect in another equation. In this case, the instrument set also varies across equations.
In case the specific effects are assumed to be fixed the authors show that the model can be estimated by OLS after a within transformation.

10.3.2.2 Simultaneous Error Component Models with Censored Endogenous Variables

Another recent extension is the inclusion of censored endogenous variables in a simultaneous EC model, by Vella and Verbeek (1999). Their model is a two­equation system in which the first one is the primary focus and the second one is already in the reduced form. For i = 1, . . . , N; t = 1, . . . , T we have:

yit = m1(xit , zit ; 1) + i + it zit = m2(xit , zit ; 2) + i + it

(10.81) (10.82)

10 Measurement Errors and Simultaneity

363

zit = h(zit , 3) yit = k(yit )

where i indexes individuals (i = 1, . . . , N), t time periods (t = 1, . . . , T ), yit and zit are latent endogenous variables with observed counterparts yit and zit ; m1 and m2 denote general functions characterized by the unknown parameters in 1 and 2, respectively. The mapping from the latent to the observed variables is through
the censoring functions h and k, h depending on another unknown parameter
vector 3.
An error component structure is specified for the disturbance term of each equa-
tion (i and it for (10.81) and i and it for (10.82)) with the components being independent across individuals. Denoting it = i + it and uit = i + it , it is as-
sumed that ui | Xi  NID(0, 2 2I),

E(it | Xi, ui) = 1uit + 2u¯i

(10.83)

where  is a vector of ones, ui is the T vector of uit s for individual i, Xi = [xi1, . . . , xiT ] and u¯i = T -1 tT=1 uit ; 1 and 2 are unknown constants. Equation (10.83) reflects the endogenous character of zit .
Two variants are considered for the censoring mechanisms: (1) zit is censored through h(·) and yit observed only for certain values of zi1, . . . , ziT i.e

yit = yit if gt (zi1, . . . , zit ) = 1 = 0 (unobserved) if gt (zi1, . . . , ziT ) = 0

and (2) zit is observed and only yit is censored through k(·).
The first model allows for a conditional moment estimation where (10.82) is first estimated by ML and then (10.81) by conditional moment method after adding in its right hand side the conditional expectation of its errors given the exogenous variables and the errors of (10.82), in order to take into account the endogeneity of zit. For the second variant, a two step conditional ML approach is proposed by first estimating the second equation by ML as zit is observed and then the first equation by conditional ML i.e. maximising the conditional likelihood given zi. Generalisations to multiple endogenous variables are briefly mentioned.
The first method is applied to a model for analyzing the influence of the number of hours worked on the hourly wage rate keeping in mind the potential endogeneity of the former. Through this application the authors point out the usefulness of the two step methods in a context where the maximum likelihood procedure is impractical.

364
10.4 Conclusion

E. Biørn and J. Krishnakumar

To conclude we would like to make a few general remarks. First let us add a word on the different uses of the same terminology and a possible confusion arising from it, especially for students. As mentioned before, the problem of regressors correlated with the error term (whatever component of it) results in inconsistent/biased OLS/GLS estimates and one has to resort to IV/GMM methods. When data are in a one-dimensional form, there is no room for confusion. However in a panel data setting, the same terminology of `endogeneity of regressors' may be used whether it concerns correlation with specific effects or with the residual disturbance term. Though it is correct to use the same name in both cases, the researcher has to check what type of endogeneity she is faced with before adopting a solution. Some methods or transformations that are valid for one may not be valid for the other and vice versa.
Again keeping the students in mind we would like to point out that the terms IV and GMM can rightly be used in an interchangeable fashion as all IV estimators can also be interpreted as GMM estimators using the corresponding moment conditions. But one should understand how the same estimator can be obtained by both ways especially for implementing the estimation methods in any software package which may not explicitly have one or the other term in its commands.
We now turn to areas where research could be continued in this topic. First of all, the reader would have noticed that we have not specially dealt with hypothesis testing in our chapter. This is because the tests on various coefficients and variance components are only asymptotic, based on the limiting distributions of the respective estimators and can be derived relatively easily as straightforward extensions of their counterparts in the single-equation model. No exact results are available so far on the distributions. This precisely leads us to one possible area for further theoretical research namely, derivation of the exact distributions of the various estimators developed above, or better approximations to the exact distribution than the asymptotic ones, especially for small samples, using recent techniques like bootstrap or saddlepoint approximations.
Finally, regarding the practical implementation of the various IV methods, we are happy to note that many of the above procedures have been included in the econometric software available on the market. G2SLS, within-2SLS and EC-2SLS are easily implemented in STATA which offers many estimation and inference possibilities with panel data in general. Matrix manipulations are also convenient in this programme which allows for easy and quick transformations of variables before entering them in a regression. Other packages like TSP, LIMDEP and RATS have also included panel data estimation possibilities. The reader is invited to go through the chapter devoted to this topic in this volume for an excellent review of the different options available.

10 Measurement Errors and Simultaneity

365

References

Ahn, S.C., and P. Schmidt (1995): Efficient Estimation of Models for Dynamic Panel Data. Journal of Econometrics, 68, 5­27.
Amemiya, T. (1971): The Estimation of Variances in a Variance Components Model. International Economic Review, 12, 1­13.
Amemiya, T. and T.E. McCurdy (1986): Instrumental Variable Estimation of an Error Components Model. Econometrica, 54, 869­881.
Anderson, T.W., and C. Hsiao (1981): Estimation of Dynamic Models with Error Components. Journal of the American Statistical Association, 76, 598­606.
Anderson, T.W., and C. Hsiao (1982): Formulation and Estimation of Dynamic Models Using Panel Data. Journal of Econometrics, 18, 47­82.
Arellano, M., and S. Bond (1991): Some Tests of Specification for Panel Data: Monte Carlo Evidence and an Application to Employment Equations. Review of Economic Studies, 58, 277­297.
Arellano, M., and O. Bover (1995): Another Look at the Instrumental Variable Estimation of ErrorComponents Models. Journal of Econometrics, 68, 29­51.
Avery, R.B. (1977): Error Component Models and Seemingly Unrelated Regressions. Econometrica, 45, 199­209.
Balestra, P. (1983): La De´rivation Matricielle. Collection de l'Institut de Mathe´matiques Economiques de Dijon, 12, Sirey, Paris.
Balestra, P. and J. (Varadharajan­)Krishnakumar (1987): Full Information Estimations of a System of Simultaneous Equations with Error Component Structure. Econometric Theory, 3, 223­246.
Baltagi, B.H. (1981): Simultaneous Equations with Error Components. Journal of Econometrics, 17, 189­200.
Baltagi, B.H. (1984): A Monte Carlo Study for Pooling Time Series of Cross-Section Data in the Simultaneous Equations Model. International Economic Review, 25, 603­624.
Baltagi, B.H. (2001): Econometric Analysis of Panel Data, second edition. Chichester: Wiley. Baltagi, B.H. and Y-J. Chang (2000): Simultaneous Equations with Incomplete Panels,
Econometric Theory, 16, 269­279. Bekker, P., A. Kapteyn, and T. Wansbeek (1987): Consistent Sets of Estimates for Regressions
with Correlated or Uncorrelated Measurement Errors in Arbitrary Subsets of All Variables. Econometrica, 55, 1223­1230. Biørn, E. (1992): The Bias of Some Estimators for Panel Data Models with Measurement Errors. Empirical Economics, 17, 51­66. Biørn, E. (1996): Panel Data with Measurement Errors. Chap. 10 in The Econometrics of Panel Data. Handbook of the Theory with Applications, ed. by L. Ma´tya´s and P. Sevestre. Dordrecht: Kluwer. Biørn, E. (2000): Panel Data with Measurement Errors. Instrumental Variables and GMM Estimators Combining Levels and Differences. Econometric Reviews, 19, 391­424. Biørn, E. (2003): Handling the Measurement Error Problem by Means of Panel Data: Moment Methods Applied on Firm Data. Chap. 24 in Econometrics and the Philosophy of Economics, ed. by B. Stigum. Princeton: Princeton University Press. Biørn, E., and T.J. Klette (1998): Panel Data with Errors-in-Variables: Essential and Redundant Orthogonality Conditions in GMM-Estimation. Economics Letters, 59, 275­282. Biørn, E., and T.J. Klette (1999): The Labour Input Response to Permanent Changes in Output: An Errors in Variables Analysis Based on Panel Data. Scandinavian Journal of Economics, 101, 379­404. Blundell, R., and S. Bond (1998): Initial Conditions and Moment Restrictions in Dynamic Panel Data Models. Journal of Econometrics, 87, 115­143. Bowden, R.J. and D.A. Turkington (1984): Instrumental Variables. Econometric Society Publication, No. 8, Cambridge University Press, Cambridge.

366

E. Biørn and J. Krishnakumar

Breusch, T.S., G.E. Mizon and P. Schmidt (1987): Efficient Estimation Using Panel Data, Michigan State University Econometrics Workshop Paper 8608.
Breusch, T., H. Qian, P. Schmidt, and D. Wyhowski (1999): Redundancy of Moment Conditions. Journal of Econometrics, 91, 89­111.
Chamberlain, G.(1987): Asymptotic Efficiency in Estimation With Conditional Moment Restrictions. Journal of Econometrics, 34, 305­334.
Cornwell, C., P. Schmidt and D. Wyhowski (1992): Simultaneous Equations and Panel Data. Journal of Econometrics, 51, 151­181.
Davidson, R., and J.G. MacKinnon(1993): Estimation and Inference in Econometrics. Oxford: Oxford University Press.
Erickson, T. (1993): Restricting Regression Slopes in the Errors-in-Variables Model by Bounding the Error Correlation. Econometrica, 91, 959­969.
Fisher, F.M. (1966): The Identification Problem in Econometrics, New York: McGraw-Hill. Fuller, W.A. (1987): Measurement Error Models. New York: Wiley. Griliches, Z., and J.A. Hausman (1986): Errors in Variables in Panel Data. Journal of
Econometrics, 31, 93­118. Hansen, L.P. (1982): Large Sample Properties of Generalized Method of Moments Estimators.
Econometrica, 50, 1029­1054. Harris, D. and L. Ma´tya´s (1999): Introduction to the Generalized Method of Moments Estima-
tion. Chap. 1 in Generalized Method of Moments Estimation, ed. by L. Ma´tya´s. Cambridge: Cambridge University Press. Harris, M., L. Ma´tya´s and P. Sevestre, (2007): Dynamic Models for "Short Panels". Chap. 8 in this volume. Hausman, J.A., and W.E. Taylor (1981): Panel Data and Unobservable Individual Effects. Econometrica 49, 1377­1398. Hausman, J.A. and W.E. Taylor (1983): Identification in Linear Simultaneous Equations Models with Covariance Restrictions: An Instrumental Variables Interpretation. Econometrica 51, 1527­1549. Holtz-Eakin, D., W. Newey, and H.S. Rosen (1988): Estimating Vector Autoregressions with Panel Data. Econometrica, 56, 1371­1395. Hsiao, C. (2003): Analysis of Panel Data, 2nd edition. Cambridge: Cambridge University Press. Klepper, S., and E. Leamer (1984): Consistent Sets of Estimates for Regressions with Errors in All Variables. Econometrica, 52, 163­183. Koopmans, T.C. (1953): Identification Problems in Economic Model Construction, in Studies in Econometric Method (Cowles Commission Monograph 14), ed. by W.C. Hood and T.C. Koopmans, New York: John Wiley and Sons. Krishnakumar, J. (1988): Estimation of Simultaneous Equation Models with Error Components Structure. Berlin, Heidelberg: Springer-Verlag. Maddala, G.S. (1971): The Use of Variance Components Models in Pooling Cross Section and Time Series Data. Econometrica, 39, 341­358. Ma´tya´s, L. and L. Lovrics (1990): Small Sample Properties of Simultaneous Error Components Models. Economics Letters 32, 25­34. McCabe, B., and A. Tremayne (1993): Elements of Modern Asymptotic Theory with Statistical Applications. Manchester: Manchester University Press. Nagar, A.L. (1959): The Bias and Moment Matrix of the General k­class Estimators of the Parameters in Simultaneous Equations. Econometrica, 27, 575­595. Nelson, C.R., and R. Startz (1990): Some Further Results on the Exact Small Sample Properties of the Instrumental Variable Estimator. Econometrica, 58, 967­976. Nerlove, M. (1971): A Note on Error Components Models. Econometrica, 39, 383­396. Newey, W.K. (1985): Generalized Method of Moments Specification Testing. Journal of Econometrics, 29, 229­256. Newey, W.K., and D. McFadden (1994): Large Sample Estimation and Hypothesis Testing. Chap. 36 in Handbook of Econometrics, Vol. IV, ed. by R.F. Engle and D.L. McFadden. Amsterdam: North-Holland.

10 Measurement Errors and Simultaneity

367

Paterno, E.M., Y. Amemiya, and Y. Amemiya (1996): Random Effect and Random Coefficient Analysis with Errors-in-Variables. 1996 Proceedings of the Business and Economic Statistics Section, pp. 76­79.
Phillips, P.C.B. (1982): Small Sample Distribution Theory in Econometric Models of Simultaneous Equations, Cowles Foundation Discussion Paper No. 617, Yale University.
Prucha, I.R. (1985): Maximum Likelihood and Instrumental Variable Estimation in Simultaneous Equation Systems with Error Components. International Economic Review, 26, 491­506.
Reiersøl, O. (1950): Identifiability of a Linear Relation Between Variables which are Subject to Error. Econometrica, 18, 375­389.
Rothenberg, T.J. (1971): Identification in Parametric Models. Econometrica, 39, 577­592. Shalabh (2003): Consistent Estimation of Coefficients in Measurement Error Models with Repli-
cated Observations. Journal of Multivariate Analysis, 86, 227­241. Staiger, D., and J.H. Stock (1997): Instrumental Variables Regression With Weak Instruments.
Econometrica, 65, 557­586. Swamy, P.A.V.B. and S.S. Arora (1972): The Exact Finite Sample Properties of the Estimators of
Coefficients in the Error Components Regression Models. Econometrica, 40, 261­275. Theil, H. (1971): Principles of Econometrics. Amsterdam: North-Holland Publishing Company. Vella, F. and M. Verbeek (1999): Two-step Estimation of Panel Data Models with Censored En-
dogenous Variables and Selection Bias. Journal of Econometrics, 90, 239­263. Wansbeek, T.J. (2001): GMM Estimation in Panel Data Models with Measurement Error. Journal
of Econometrics, 104, 259­268. Wansbeek, T.J., and R.H. Koning (1991): Measurement Error and Panel Data. Statistica
Neerlandica, 45, 85­92. Wansbeek, T.J., and E. Meijer (2000): Measurement Error and Latent Variables in Econometrics.
Amsterdam: Elsevier. White, H. (1982): Instrumental Variables Regression with Independent Observations. Economet-
rica, 50, 483­499. White, H. (1984): Asymptotic Theory for Econometricians. Orlando: Academic Press. White, H. (1986): Instrumental Variables Analogs of Generalized Least Squares Estimators. In
Advances in Statistical Analysis and Statistical Computing. Theory and Applications, vol. 1, ed. by R.S. Mariano, Greenwich: JAI Press, pp. 173­227. Zellner, A. and H. Theil (1962): Three Stage Least Squares: Simultaneous Estimation of Simultaneous Equations. Econometrica, 30, 54­78. Ziliak, J.P. (1997): Efficient Estimation With Panel Data When Instruments Are Predetermined: An Empirical Comparison of Moment-Condition Estimators. Journal of Business and Economic Statistics, 15, 419­431.

Chapter 11
Pseudo-Panels and Repeated Cross-Sections
Marno Verbeek

11.1 Introduction
In many countries there is a lack of genuine panel data where specific individuals or firms are followed over time. However, repeated cross-sectional surveys may be available, where a random sample is taken from the population at consecutive points in time. Important examples of this are the Current Population Survey in the U.S.A., and the Family Expenditure Survey in the United Kingdom. While many types of models can be estimated on the basis of a series of independent cross-sections in a standard way, several models that seemingly require the availability of panel data can also be identified with repeated cross-sections under appropriate conditions. Most importantly, this concerns models with individual dynamics and models with fixed individual-specific effects.
Obviously, the major limitation of repeated cross-sectional data is that the same individuals are not followed over time, so that individual histories are not available for inclusion in a model, for constructing instruments or for transforming a model to first-differences or in deviations from individual means. All of these are often applied with genuine panel data. On the other hand, repeated cross-sections suffer much less from typical panel data problems like attrition and nonresponse, and are very often substantially larger, both in number of individuals or households and in the time period that they span.
In a seminal paper, Deaton (1985) suggests the use of cohorts to estimate a fixed effects model from repeated cross-sections. In his approach, individuals sharing some common characteristics (most notably year of birth) are grouped into cohorts, after which the averages within these cohorts are treated as observations in a pseudo panel. Moffitt (1993) and Collado (1997), in different ways, extend the approach of Deaton to nonlinear and dynamic models. Alternative estimators for
Marno Verbeek Department of Financial Management, RSM Erasmus University, Burg. Oudlaan 50, 3062 PA Rotterdam, The Netherlands, e-mail: mverbeek@rsm.nl

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

369

c Springer-Verlag Berlin Heidelberg 2008

370

M. Verbeek

the model with individual dynamics, including the one proposed by Girma (2000), are evaluated in Verbeek and Vella (2005). Alternative types of asymptotics are discussed in McKenzie (2004). In this chapter we shall discuss the identification and estimation of panel data models from repeated cross sections. In particular, attention will be paid to linear models with fixed individual effects, to models containing lagged dependent variables and to discrete choice models.
Models containing individual effects that are correlated with the explanatory variables ("fixed effects models") often arise naturally from economic theory, for example in life cycle models where the individual effects represent marginal utility of wealth (see, for example, Heckman and McCurdy (1980) or Browning, Deaton and Irish (1985)). Individual dynamics also often follow from economic theory, reflecting adjustment costs, habit persistence, or intertemporal optimization. Consequently, from an economic point of view it is important to be able to estimate dynamic models and models with fixed individual effects, even in the absence of genuine panel data. While it is possible to estimate such models using repeated cross-sections, we shall see below that such approaches typically require strong identification conditions, which are often hard to test.
Estimation techniques based on grouping individual data into cohorts are identical to instrumental variables approaches where the group indicators are used as instruments. Consequently, the grouping variables should satisfy the appropriate conditions for an instrumental variables estimator to be consistent (including a rank condition). This not only requires that the instruments are exogenous (in the sense of being uncorrelated to the unobservables in the equation of interest), but also relevant, i.e. appropriately correlated to the explanatory variables in the model. Loosely speaking, the latter requirement means that cohorts are defined as groups whose explanatory variables change differentially over time. Even if the instruments are exogenous and relevant, their large number and the fact that they may be only weakly correlated with the explanatory variables they are supposed to instrument may imply that the resulting estimators perform poorly because of the "weak instruments" problem (see Bound, Jaeger and Baker (1995), or Staiger and Stock (1997)).
The structure of this chapter is as follows. In Sect. 11.2 we present the basic linear model. Sect. 11.3 pays attention to linear dynamic models, while Sect. 11.4 briefly discusses the estimation of binary choice models. Sect. 11.5 concludes. A related survey can be found in Ridder and Moffitt (2007).

11.2 Estimation of a Linear Fixed Effects Model

We start with analyzing a simple linear model with individual effects given by

yit = xit  + i + uit , t = 1, . . . , T,

(11.1)

where xit denotes a K-dimensional vector of explanatory variables, and  is the parameter vector of interest. The index i refers to individuals and throughout this chapter we shall assume that the available data set is a series of independent

11 Pseudo-Panels

371

cross-sections, such that observations on N individuals are available in each period.1 For simplicity, we shall assume that E{xit uit } = 0 for each t.
If the individual effects i are uncorrelated with the explanatory variables in xit , the model in (11.1) can easily be estimated consistently from repeated cross-sections by pooling all observations and performing ordinary least squares treating i + uit as composite error term. This exploits the K moment conditions in

E{(yit - xit  )xit } = 0.

(11.2)

However, in many applications the individual effects are likely to be correlated with

some or all of the explanatory variables, so that at least some of the moment con-

ditions in (11.2) are not valid. When genuine panel data are available, this can be

solved using a fixed effects approach which treats i as fixed unknown parameters. In other words, each individual has its own intercept term. For estimating  ,

this is equivalent to using the within-transformed explanatory variables xit - x¯i as instruments for xit in (11.1), where x¯i = T -1 tT=1 xit . Obviously, when repeated observations on the same individuals are not available, such an approach cannot

be used.

Deaton (1985) suggests the use of cohorts to obtain consistent estimators for 

in (11.1) when repeated cross-sections are available, even if i is correlated with

one or more of the explanatory variables. Let us define C cohorts, which are groups

of individuals sharing some common characteristics. These groups are defined such

that each individual is a member of exactly one cohort, which is the same for all

periods. For example, a particular cohort may consist of all males born in the pe-

riod 1950­1954. It is important to realize that the variables by which cohorts are

defined should be observed for all individuals in the sample. This rules out time-

varying variables (e.g. earnings), because these variables are observed at different

points in time for the individuals in the sample. The seminal study of Browning,

Deaton and Irish (1985) employs cohorts of households defined on the basis of five-

year age bands subdivided as to whether the head-of-the-household is a manual

or non-manual worker. Blundell, Duncan and Meghir (1998) employ year-of-birth

intervals of 10 years, interacted with two education groups, Banks, Blundell and

Preston (1994) use five-year age bands, while Propper, Rees and Green (2001) use

7 date of birth groups and 10 regions to construct cohorts.2

If we aggregate all observations to cohort level, the resulting model can be

written as

y¯ct = x¯ct  + ¯ ct + u¯ct , c = 1, . . . ,C; t = 1, . . . , T,

(11.3)

where y¯ct is the average value of all observed yit 's in cohort c in period t, and similarly for the other variables in the model. The resulting data set is a pseudo panel or synthetic panel with repeated observations over T periods and C cohorts. The main

1 Because different individuals are observed in each period, this implies that i does not run from 1 to N for each t. 2 Some authors employ the term "cohorts" to specifically reflect year-of-birth groups. We use "cohorts" in a broader sense, as groups of individuals (households, firms) sharing some common characteristics (most often including year-of-birth).

372

M. Verbeek

problem with estimating  from (11.3) is that ¯ ct depends on t, is unobserved, and is likely to be correlated with x¯ct (if i is correlated with xit ). Therefore, treating ¯ ct as part of the random error term is likely to lead to inconsistent estimators. Alternatively, one can treat ¯ ct as fixed unknown parameters assuming that variation over time can be ignored (¯ ct = c). If cohort averages are based on a large number of individual observations, this assumption seems reasonable and a natural estimator for  is the within estimator on the pseudo panel, given by

CT

-1 C T

^W =  (x¯ct - x¯c)(x¯ct - x¯c)

 (x¯ct - x¯c)(y¯ct - y¯c),

c=1 t=1

c=1 t=1

(11.4)

where x¯c = T -1 tT=1 x¯ct is the time average of the observed cohort means for cohort c. The properties of this estimator depend, among other things, upon the type of asymptotics that one is willing to employ. Deaton (1985) considers the asymptotic properties of this estimator when the number of cohorts C tends to infinity. This requires that the number of individuals N tends to infinity with (more or less) constant cohort sizes. Moffitt (1993), on the other hand, assumes that C is constant while the number of individuals tends to infinity. In this approach, cohort sizes tend to infinity, asymptotically.
The estimators proposed by Moffitt (1993) are based on the idea that grouping can be viewed as an instrumental variables procedure. To illustrate this, we shall reformulate the above estimator as an instrumental variables estimator based on a simple extension of (11.1). First, decompose each individual effect i into a cohort effect c and individual i's deviation from this effect. Letting zci = 1 (c = 1, . . . ,C) if individual i is a member of cohort c and 0 otherwise, we can write

C
 i = czci + vi, c=1

(11.5)

which can be interpreted as an orthogonal projection. Defining  = (1, . . . , C) and zi = (z1i, . . . , zCi) and substituting (11.5) into (11.1), we obtain

yit = xit  + zi + vi + uit .

(11.6)

If i and xit are correlated, we may also expect that vi and xit are correlated. Consequently, estimating (11.6) by ordinary least squares would not result in consistent
estimators. Now, suppose that instruments for xit can be found that are uncorrelated with vi + uit . In this case, an instrumental variables estimator would typically produce a consistent estimator for  and c. A natural choice is to choose the cohort dummies in zi, interacted with time, as instruments, in which case we derive linear predictors from the reduced forms

xk,it = zikt + wk,it , k = 1, . . . , K, t = 1, . . . , T,

(11.7)

11 Pseudo-Panels

373

where kt is a vector of unknown parameters. The linear predictor for xit from this is given by x^it = x¯ct , the vector of averages within cohort c in period t. The resulting instrumental variables estimator for  is then given by

CT

-1 C T

  ^IV1 =

(x¯ct - x¯c)xit

 (x¯ct - x¯c)yit ,

c=1 t=1

c=1 t=1

(11.8)

which is identical to the standard within estimator based on the pseudo panel of cohort averages, given in (11.4).
The instrumental variables interpretation is useful because it illustrates that alternative estimators may be constructed using other sets of instruments. For example, if cohorts are constructed on the basis of age (year of birth), a more parsimonious function of age can be employed in (11.5) rather than a full set of age dummies. For example, zi may include functions of year of birth, rather than a set of dummy variables. As argued by Moffitt (1993), it is likely that yit will vary smoothly with cohort effects and, hence, those effects will be representable by fewer parameters than a full set of cohort dummies. Further, the instrument set in (11.7) can be extended to include additional variables. Most importantly however, the instrumental variables approach stresses that grouping data into cohorts requires grouping variables that should satisfy the typical requirements for instrument exogeneity and relevance. Basically, the approach of Deaton (1985) assumes that the cohort dummies, interacted with time dummies, provide valid instruments for all explanatory variables in the model (including the full set of cohort dummies). This requires that the instruments are uncorrelated with the equation's error term, and imposes a rank condition stating that the instruments are "sufficiently" correlated with each of the explanatory variables.
As mentioned above, the asymptotic behavior of pseudo panel data estimators can be derived using alternative asymptotic sequences. In addition to the two dimensions in genuine panel data (N and T ), there are two additional dimensions: the number of cohorts C, and the number of observations per cohort nc. We consider the following possibilities, which are typical for most studies:
1. N  , with C fixed, so that nc  ; 2. N   and C  , with nc fixed. 3. T  , with N, C fixed (so that nc is also fixed);
McKenzie (2004) also considers asymptotic sequences where T   and nc  . Note that asymptotic theory is not meant as a guideline for how our estimators will behave when we get more data. Rather, we appeal to asymptotic theory when some dimension of the sample we already have is large enough for this to be appropriate. Whether or not asymptotic theory provides a reasonable approximation of the finite sample properties of pseudo panel data estimators is an empirical question, and many papers present Monte Carlo studies to obtain some insight into this issue.
The following list provides an overview of the sample sizes used in several important empirical papers.

374

M. Verbeek

T

C

n¯c

Browning, Deaton and Irish (1985)

7 16 190

Banks, Blundell and Preston (1994)

20 11 354

Blundell, Browning and Meghir (1994) 17

9 520

Alessie, Devereux and Weber (1997)

14

5 >1000

Blundell, Duncan and Meghir (1998)

25

8 142

Propper, Rees and Green (2001)

19 70

80

For most applications either Type 1 or Type 2 asymptotics provides the most rea-

sonable choice, and in many cases type 1 asymptotics is (implicitly or explicitly)

employed. In the theoretical literature, Moffitt (1993) and Verbeek and Vella (2005)

employ type 1 asymptotics, while Deaton (1985), Verbeek and Nijman (1993) and

Collado (1997) employ Type 2 (with or without T  ). Under Type 1 asymp-

totics, the fixed effects estimator based on the pseudo panel, ^W , is consistent for  ,

provided that

  1
plim nc CT

CT
(x¯ct
c=1 t=1

- x¯c)(x¯ct

- x¯c)

(11.9)

is finite and invertible, and that

  1
plim nc CT

CT
(x¯ct
c=1 t=1

- x¯c)¯ ct

=

0.

(11.10)

While the first of these two conditions is similar to a standard regularity condition,

in this context it is somewhat less innocent. It states that the cohort averages exhibit

genuine time variation, even with very large cohorts. Whether or not this condition

is satisfied depends upon the way the cohorts are constructed, a point to which we

shall return below.

Because ¯ ct  c, for some c if the number of observations per cohort tends to infinity, (11.10) will be satisfied automatically. Consequently, letting nc   and

using Type 1 asymptotics is a convenient choice to arrive at a consistent estimator for

 , see Moffitt (1993) and Ridder and Moffitt (2007). However, as argued by Verbeek

and Nijman (1992) and Devereux (2007), even if cohort sizes are large, the small-

sample bias in the within estimator on the pseudo panel may still be substantial.

Deaton (1985) proposes an alternative estimator for  that does not reply upon having a large number of observations per cohort, using Type 2 asymptotics.3 A

convenient starting point for this estimator is the cohort population version of (11.3),

given by

yct = xct  + c + uct , c = 1, . . . ,C; t = 1, . . . , T,

(11.11)

3 As argued by McKenzie (2004), in many applications cohorts are defined by age groups and hence a fixed number of cohorts is most likely to be of interest, which is inconsistent with Type 2 asymptotics. If C   with N  , one needs to think of what this means for the distribution of population cohort means as well as the distribution of individual observations around these means. For example, it would be hard to argue that the covariance matrix on the right-hand side of (11.12) below is independent of how many cohorts are distinguished. See Verbeek and Nijman (1992) for more discussion and a Monte Carlo experiment that takes this issue into account.

11 Pseudo-Panels

375

where the variables denote unobservable population cohort means, and where c is the cohort fixed effect, which is constant because population cohorts contain the
same individuals in each period. Now, x¯ct and y¯ct can be considered as error-ridden measurements of xct and yct . In particular, it is assumed that the measurement errors are distributed with zero mean, independent of the true values, i.e.

y¯ct - yct x¯ct - xct

 IID

0 0

;

00  

,

(11.12)

where the population cohort means are treated as fixed unknown constants. Although ,  and 00 are unknown, they can easily be estimated consistently (for N or T tending to infinity), using the individual data. Once estimates for  and  are available, it is easy to adjust the moment matrices in the within estimator to eliminate
the variance due to measurement error (cf. Fuller, 1987). This leads to the following
errors-in-variables estimator

CT

-1

^D =  (x¯ct - x¯c)(x¯ct - x¯c) - ^

c=1 t=1

CT
×  (x¯ct - x¯c)(y¯ct - y¯c) - ^ , c=1 t=1

(11.13)

where ^ and ^ are estimates of  and  , respectively, and where  = (T - 1)/T. As discussed in Verbeek and Nijman (1993), the original estimator presented by Deaton (1995) is characterized by  = 1. However, eliminating the incidental parameters (in c) first by within transforming the data, and working out the appropriate moments, suggests  = (T - 1)/T, which leads to better small sample properties.
Under Type 1 asymptotics, the number of observations per cohort tends to infinity and both  and  tend to zero, as well as their estimators. In this case ^D is asymptotically equivalent to ^W . Accordingly, most empirical studies ignore the errors-in-variables problem and use standard estimators, like ^W , see, for example, Browning, Deaton and Irish (1985), with an average cohort size of 190, or Blundell,
Browning and Meghir (1994), with cohort sizes around 500. Unfortunately, there
is no general rule to judge whether nc is large enough to use asymptotics based on nc  . Verbeek and Nijman (1992) analyze the bias in ^W for finite values of nc. Depending upon the way in which the cohorts are constructed, the bias in the stan-
dard within estimator may still be substantial, even if cohort sizes are fairly large. In
general, it holds that, for given nc, the bias is smaller if the cohorts are chosen such that the relative magnitude of the measurement errors is smaller compared to the
within cohort variance of xct . In practice, however, it may not be easy to construct cohorts in such a way. More recently, Devereux (2007) argues that cell sizes should
be much larger, possibly 2000 or more.
In addition to the sizes of the cohorts, the way in which the cohorts are con-
structed is important. In general, one should be equally careful in choosing cohorts
as in selecting instruments. In practice, cohorts should be defined on the basis of

376

M. Verbeek

variables that do not vary over time and that are observed for all individuals in the sample. This is a serious restriction. Possible choices include variables like age (date of birth), gender, race, or region.4 Identification of the parameters in the model requires that the reduced forms in (11.7) generate sufficient variation over time. This requirement puts a heavy burden on the cohort identifying variables. In particular, it requires that groups are defined whose explanatory variables all have changed differentially over time.
Suppose, as an extreme example, that cohorts are defined on the basis of a variable that is independent of the variables in the model. In that case, the true population cohort means xct would be identical for each cohort c (and equal the overall population mean) and the only source of variation left in the data that is not attributable to measurement error would be the variation of xct over time. If these population means do not change over time, all variation in the observed cohort averages x¯ct is measurement error and the errors-in-variables estimator ^D does not have a well-defined probability limit.

11.3 Estimation of a Linear Dynamic Model

An important situation where the availability of panel data seems essential to identify and estimate the model of interest is the case where a lagged dependent variable enters the model. Let us consider a simple extension of (11.1) given by

yit = yi,t-1 + xit  + i + uit , t = 1, . . . , T,

(11.14)

where the K-dimensional vector xit may include time-invariant and time-varying variables. When genuine panel data are available, the parameters  and  can be estimated consistently (for fixed T and N  ) using the instrumental variables estimators of Anderson and Hsiao (1981) or, more efficiently, using the GMM esti-
mator of Arellano and Bond (1991). These estimators are based on first-differencing
(11.14) and then using lagged values of yi,t-1 as instruments. In the present context, yi,t-1 refers to the value of y at t - 1 for an individual
who is only observed in cross-section t. Thus, an observation for yi,t-1 is unavailable. Therefore, the first step is to construct an estimate by using information on the y-values of other individuals observed at t - 1. To do so, let zi denote a set of time-invariant variables, including an intercept term. Now, consider the orthogonal
projection in cross-section t of yit upon zi,

E{yit |zi} = zi0t , t = 1, . . . , T,

(11.15)

where E denotes the orthogonal projection (for a given t). This is similar to the reduced forms for xk,it in (11.7). Following Moffitt (1993), one obtains an estimate of yi,t-1 as the predicted value from this regression, substituting the appropriate z

4 Note that residential location may be endogenous in certain applications.

11 Pseudo-Panels

377

values for the individuals in cross-section t. That is,

y^i,t-1 = zi^0,t-1,

(11.16)

noting that ^0,t-1 is estimated from data on different individuals than those indexed by i. In many circumstances it is convenient to think of zi as a vector of dummy variables, corresponding to mutually exclusive cohorts, as in the previous section. In this case, the orthogonal projection in (11.15) corresponds to the conditional expectation and (11.16) corresponds to taking period-by-period sample averages within person i's cohort.
Now, insert these predicted values into the original model to get:

yit = y^i,t-1 + xit  + i,t , t = 1, . . . , T ;

(11.17)

where

it = i + uit + (yi,t-1 - y^i,t-1).

(11.18)

No matter how y^i,t-1 is generated, its inclusion implies that one of the explanatory variables is measured with error, although the measurement error will be (asymptotically) uncorrelated with the predicted value.5 To see whether it would be useful
to estimate (11.17) by ordinary least squares, let us first of all make the assumption
that the instruments in zi are exogenous, so that

E{(i + uit )zi} = 0, t = 1, . . . , T.

(11.19)

This excludes the possibility that there are cohort effects in the unobservables.
While this may appear unreasonable, this assumption is made in Moffitt (1993),
Girma (2000) and in a number of cases in McKenzie (2004). Under (11.19) it can be argued that y^i,t-1 and i,t are uncorrelated, which is a necessary condition for OLS applied to (11.17) to be consistent. In addition, consistency of OLS requires that xit and it are uncorrelated. This assumption may also be problematic, even in cases where the explanatory variables are exogenous to begin with, i.e. even if

E{(i + uit )xit } = 0, t = 1, . . . , T.

(11.20)

This is because xit is likely to be correlated with yi,t-1 - y^i,t-1. Consider, for example, a case where high x-values in one period on average cor-
respond with high x-values in the next period. If the  coefficients are positive this will generally imply that a high value for xi,t-1, which is unobservable, will result in an underprediction of yi,t-1. On the other hand, xi,t-1 is positively correlated with xit . Consequently, this will produce a positive correlation between it and xit , resulting in an inconsistent estimator for  . This inconsistency carries over to  unless y^i,t-1 is uncorrelated with xit . As a result, the estimator suggested by Moffitt (1993), based on applying OLS to (11.17), is typically inconsistent unless there are either

5 Unlike the standard textbook measurement error examples.

378

M. Verbeek

no time-varying exogenous regressors or the time-varying exogenous variables do not exhibit any serial correlation (see Verbeek and Vella, 2005).
To overcome the problem of correlation between the regressors and the error term in (11.17) one may employ an instrumental variables approach. Note that now we need instruments for xit even though these variables are exogenous in the original model. Because these instruments will have to satisfy a condition like (11.19), a natural choice is to use the same instruments for xit as we did for yi,t-1. This will also guarantee that the instruments are uncorrelated with the prediction error yi,t-1 - y^i,t-1 in it .
As before, when the instruments zi are a set of cohort dummies, estimation of (11.17) by instrumental variables is identical to applying OLS to the original model where all variables are replaced by their (time-specific) cohort sample averages. We can write this as

y¯ct = y¯c,t-1 + x¯ct  + ¯ct , c = 1, . . . , C; t = 1, . . . , T,

(11.21)

where all variables denote period-by-period averages within each cohort. For this approach to be appropriate, we need that y¯c,t-1 and x¯ct are not collinear, which requires that the instruments capture variation in yi,t-1 independent of the variation in xit . That is, the time-invariant instruments in zi should exhibit sufficient correlation with the exogenous variables in xit and the (unobserved) lagged dependent variable yi,t-1, while at the same time they should not be correlated with it . Given these stringent requirements, it is likely that in many applications the number of available valid instruments is small. Verbeek and Vella (2005) provide more details on this rank condition.
The pairwise quasi-differencing approach of Girma (2000) deviates from the above estimation strategy in two respects, although it essentially makes the same assumptions. First, the lagged value of y is not approximated by the lagged cohort average but by an arbitrarily selected observation from the cohort. Second, the instruments are not the cohort dummies, but individual, or averaged, observations from the cohort. As a result, Girma's approach employs a noisy approximation to the unobserved lagged values as well as noisy instruments. Although, under appropriate assumptions, this noise will cancel out asymptotically, there does not seem to be any gain in using such an approach (see Verbeek and Vella (2005) for more discussion).
The availability of appropriate instruments satisfying condition (11.19) may be rather limited, because cohort effects in the unobservables are not allowed. It is possible to include cohort fixed effects in the model in essentially the same way as in the static case by including the cohort dummies zi in the equation of interest, with time-invariant coefficients. This imposes (11.5) and results in

where

yit = y^i,t-1 + xit  + zi + it , t = 1, . . . , T, it = vi + uit + (yi,t-1 - y^i,t-1),

(11.22) (11.23)

11 Pseudo-Panels

379

and E{zivi} = 0 by construction. This also allows us to relax (11.20) to

E{(vi + uit )xit } = 0, t = 1, . . . , T.

(11.24)

Under these conditions, one would estimate (11.22) by instrumental variables using
zi, interacted with time dummies, as instruments. Verbeek and Vella (2005) refer to this as the augmented IV estimator noting that a time-varying  would make the model unidentified. To achieve identification, we need to assume that y¯c,t-1 and x¯ct exhibit time variation and are not collinear. This condition puts additional restric-
tions upon the relationships between the instruments zi and xit and yi,t-1. Among other things, at least three cross-sections are needed to identify the model under
these assumptions.
Computation of this augmented IV estimator is remarkably simple if zi is a set of cohort dummies. One simply aggregates the data into cohort averages, which gives

y¯ct = y¯c,t-1 + x¯ct  + c + ¯ c,t ,

(11.25)

where c = zi denotes a cohort-specific fixed effect. Applying OLS to (11.25) corresponds to the standard within estimator for (,  ) based upon treating the cohort-level data as a panel, which is consistent under the given assumptions (and some regularity conditions) under Type 1 asymptotics (N   with C fixed). The usual problem with estimating dynamic panel data models (see Nickell (1981)),6
does not arise because under assumption (11.24) the error term, which is a within
cohort average of individual error terms that are uncorrelated with zi, is asymptotically zero.7 However, it remains to be seen whether suitable instruments can be
found that satisfy the above conditions, because the rank condition for identification
requires that the time-invariant instruments have time-varying relationships with the
exogenous variables and the lagged dependent variable, while they should not have
any time-varying relationship with the equation's error term. While this seems unlikely, it is not impossible. When zi is uncorrelated with it , it is typically sufficient that the means of the exogenous variables, conditional upon zi, are time-varying; see Verbeek and Vella (2005) for more details. Under Type 2 asymptotics (N   with C  ), we encounter similar problems as in the static case, and Collado (1997) discusses how this is handled in the dynamic model, by extending the approach of
Deaton (1985). The resulting estimator is similar to the GMM-type estimators that
are applied with genuine panel data (Arellano and Bond, 1991), but where the mo-
ment matrices are adjusted to reflect the errors-in-variables problem (for finite nc). Both Girma (2000) and McKenzie (2004) consider the linear dynamic model
with cohort-specific coefficients in (11.14). While this extension will typically only
make sense if there is a fairly small number of well-defined cohorts, it arises nat-
urally from the existing literature on dynamic heterogeneous panels. For example,

6 With genuine panel data, the within estimator in the dynamic model has a substantial bias for small and moderate values of T . 7 Recall that, asymptotically, the number of cohorts is fixed and the number of individuals goes to infinity.

380

M. Verbeek

Robertson and Symons (1992) and Pesaran and Smith (1995) stress the importance of parameter heterogeneity in dynamic panel data models and analyze the potentially severe biases that may arise from handling it in an inappropriate manner. In many practical applications, investigating whether there are systematic differences between, for example, age cohorts, is an interesting question. Obviously, relaxing specification (11.14) by having cohort-specific coefficients puts an additional burden upon the identifying conditions. Further, note that using Type 2 asymptotics, where the number of cohorts increases with sample size, does not make much sense in these cases.

11.4 Estimation of a Binary Choice Model

In this section we briefly consider the estimation of a binary choice model on the basis of repeated cross-sections. In a binary choice model the outcome variable takes on only two different values, coded as 0 and 1. For example, the dependent variable could reflect whether or not a household owns a house, or whether or not an individual has a paid job. The model of interest is given by

yit = xit  + i + uit , t = 1, . . . , T,
where yit is a latent variable, and we observe yit = 1 if yit > 0, = 0 otherwise.

(11.26) (11.27)

With genuine panel data, popular parametric estimators for this model are the ran-
dom effects probit estimator and the fixed effects logit estimator. The first approach assumes that the unobservables i and uit are normally distributed and independent of the explanatory variables in xit . The corresponding likelihood function takes into account that different observations on the same individual are dependent. With re-
peated cross-sections, this dependence is zero by construction and the binary choice probit model can be estimated as a pooled probit assuming i + uit is N(0, 1).
Estimation becomes more complicated if one wants to allow i and xit to be correlated, as in the fixed effects case. With genuine panel data, one option is to
explicitly model this correlation, as in the Chamberlain (1984) approach, who proposes to parametrize the conditional expectation of i given the exogenous variables as a linear function of the xit 's. That is,

E(i|xi1, . . . , xiT ) = xi11 + . . . + xiT T ,

(11.28)

which allows us to write

i = xi11 + . . . + xiT T + i,

(11.29)

11 Pseudo-Panels

381

where E(i|xi1, . . . , xiT ) = 0. Substituting (11.29) into (11.26) produces

yit = xi1t1 + . . . + xiT tT + i + uit , t = 1, . . . , T,

(11.30)

where ts =  + s if s = t and ts = s otherwise. Making distributional assumptions on i and uit (e.g. normality) allows the application of standard maximum likelihood. However, when only repeated cross-sections are available, we do not ob-
serve the full history of the explanatory variables, as required in (11.29), and this
approach is not feasible. Collado (1998) shows how this model can be estimated
using cohort data, based on substituting the cohort specific means x¯c1, . . . , x¯cT into (11.30). Using Type 2 asymptotics, with C   and more or less fixed cohort sizes, this introduces an errors-in-variables problem in the equation. However, under nor-
mality the covariances between the explanatory variables and the disturbances are
known functions of the variances of the measurement error (which can be identified
from the individual data). Collado (1998) derives the corresponding probability that yit = 1, which can be used to estimate t for each cross section t. Next, the structural parameters  (and  ) can be estimated using a minimum distance estimator. Note that yit as well as yit are not aggregated to cohort averages in this approach.
An alternative approach is proposed by Moffitt (1993) and is based on estimating
the binary choice model by instrumental variables, where the cohort dummies (or
other functions of the variables that define cohorts) are used as instruments. As before, this is based on Type 1 asymptotics (with C fixed and N  ). Using (11.5), write the latent variable equation as

yit = xit  + zi + vi + uit , t = 1, . . . , T.

(11.31)

Assuming, as before, that the cohort indicators, interacted with time, provide valid instruments, we can estimate the binary choice model by instrumental variables. This requires the assumption that vi + uit is normally distributed; see Ridder and Moffitt (2007) for more details. Moffitt (1993) and Ridder and Moffitt (2007) also discuss extensions to discrete choice models with a lagged dependent variable.

11.5 Concluding Remarks
In this chapter we have briefly discussed the problem of estimating panel data models from a time series of independent cross-sections. In particular, attention was paid to the estimation of static fixed effects models, to dynamic models with individual effects and to binary choice models.
The approach proposed by Deaton (1985) is to divide the population into a number of cohorts, being groups of individuals sharing some common characteristics, and to treat the observed cohort means as error-ridden measurements of the population cohort means. The resulting estimator for the static linear model with fixed effects is a corrected within estimator based on the cohort aggregates. Moffitt (1993) extends the work of Deaton by considering a general instrumental

382

M. Verbeek

variables framework, of which explicit grouping is a special kind. While both approaches assume that N   they differ in the assumptions about what happens to the cohorts when N increases. In Deaton's approach, the number of cohorts C increases with N (with more or less constant cohort sizes), while in Moffitt's approach, the number of cohorts (which is equivalent to the number of instruments) is fixed and cohort sizes increase with N. In this latter approach, the errors-in-variables problem disappears.
Both Moffitt (1993) and Collado (1997) consider the linear dynamic model, based on different types of asymptotics. As argued by Verbeek and Vella (2005), the fixed effects estimator based on the pseudo panel of cohort averages may provide an attractive choice, even when a lagged dependent variable is included in the model. This deviates from the genuine panel data case, where the standard fixed effects estimator suffers from a substantial small-T bias in dynamic models. A Monte Carlo experiment by Verbeek and Vella (2005) shows that the bias that is present in the within estimator for the dynamic model using genuine panel data (see Nickell (1981)), is much larger than what is found for similar estimators employed upon cohort aggregates.
However, an important issue in both the static and dynamic models is the validity and relevance of the instruments that are used to construct the cohorts. A necessary condition for consistency of most estimators is that all exogenous variables exhibit genuine time-varying cohort-specific variation. That is, the cohorts have exogenous variables that change differentially over time. While it is not obvious that this requirement will be satisfied in empirical applications, it is also not easy to check, because estimation error in the reduced form parameters may hide collinearity problems. That is, sample cohort averages may exhibit time-variation while the unobserved population cohort averages do not.

References
Alessie, R., M.P. Devereux and G. Weber (1997), Intertemporal Consumption, Durables and Liquidity Constraints: A Cohort Analysis, European Economic Review, 41, 37­59.
Anderson, T.W. and C. Hsiao (1981), Estimation of Dynamic Models with Error Components, Journal of the American Statistical Association, 76, 598­606.
Arellano, M. and S. Bond (1991), Some Test of Specification for Panel Data: Monte Carlo Evidence and an Application to Employment Equations, Review of Economic Studies, 58, 277­297.
Banks, J., R. Blundell and I. Preston (1994), Life-Cycle Expenditure Allocations and the Consumption Costs of Children, European Economic Review, 38, 1391­1410.
Blundell, R., M. Browning and C. Meghir (1994), Consumer Demand and the Life-Cycle Allocation of Household Expenditures, Review of Economic Studies, 61, 57­80.
Blundell, R., A. Duncan and C. Meghir (1998), Estimating Labor Supply Responses Using Tax Reforms, Econometrica, 66, 827­861.
Bound, J., D. Jaeger and R. Baker (1995), Problems with Instrumental Variables Estimation When the Correlation Between Instruments and the Endogenous Explanatory Variable is Weak, Journal of the American Statistical Association, 90, 443­450.
Browning, M., A. Deaton and M. Irish (1985), A Profitable Approach to Labor Supply and Commodity Demands over the Life Cycle, Econometrica, 53, 503­543.

11 Pseudo-Panels

383

Chamberlain, G. (1984), Panel Data, in: Z. Griliches and M.D. Intrilligator, eds., Handbook of Econometrics, Volume 2, Elsevier Science, North-Holland.
Collado, M.D. (1997), Estimating Dynamic Models from Time Series of Independent CrossSections, Journal of Econometrics, 82, 37­62.
Collado, M.D. (1998), Estimating Binary Choice Models from Cohort Data, Investigaciones Econo´micas, 22, 259­276.
Deaton, A. (1985), Panel Data from Time Series of Cross Sections, Journal of Econometrics, 30, 109­126.
Devereux, P. (2007), Small Sample Bias in Synthetic Cohort Models of Labor Supply, Journal of Applied Econometrics, 22, 839­848.
Fuller, W.A. (1987), Measurement Error Models, John Wiley and Sons, New York. Girma, S. (2000), A Quasi-Differencing Approach to Dynamic Modelling from a Time Series of
Independent Cross-Sections, Journal of Econometrics, 98, 365­383. Heckman, J.J. and Th.E. McCurdy (1980), A Life-Cycle Model of Female Labour Supply, Review
of Economic Studies, 47, 47­74. McKenzie, D.J. (2004), Asymptotic Theory for Heterogeneous Dynamic Pseudo-Panels, Journal
of Econometrics, 120, 235­262. Moffitt, R. (1993), Identification and Estimation of Dynamic Models with a Time Series of Re-
peated Cross-Sections, Journal of Econometrics, 59, 99­123. Nickell, S. (1981), Biases in Dynamic Models with Fixed Effects, Econometrica, 49, 1417­1426. Pesaran, M.H. and R. Smith (1995), Estimating Long-Run Relationships from Dynamic Heteroge-
neous Panels, Journal of Econometrics, 68, 79­113. Propper, C., H. Rees and K. Green (2001), The Demand for Private Medical Insurance in the UK:
A Cohort Analysis, The Economic Journal, 111, C180­C200. Ridder, G and R. Moffitt (2007), The Econometrics of Data Combination, in: J.J. Heckman and
E.E. Leamer, eds., Handbook of Econometrics, Volume 6B, Elsevier Science, North-Holland. Robertson, D. and J. Symons (1992), Some Strange Properties of Panel Data Estimators, Journal
of Applied Econometrics, 7, 175­189. Staiger, D. and J.H. Stock (1997), Instrumental Variables Regressions with Weak Instruments,
Econometrica, 65, 557­586. Verbeek, M. and Th.E. Nijman (1992), Can Cohort Data Be Treated As Genuine Panel Data?,
Empirical Economics, 17, 9­23. Verbeek, M. and Th.E. Nijman (1993), Minimum MSE Estimation of a Regression Model with
Fixed Effects from a Series of Cross-Sections, Journal of Econometrics, 59, 125­136. Verbeek, M. and F. Vella (2005), Estimating Dynamic Models from Repeated Cross-Sections, Jour-
nal of Econometrics, 127, 83­102.

Chapter 12
Attrition, Selection Bias and Censored Regressions
Bo Honore´, Francis Vella and Marno Verbeek

12.1 Introduction
In micro-econometric applications issues related to attrition, censoring and nonrandom sample selection frequently arise. For example, it is quite common in empirical work that the variables of interest are partially observed or only observed when some other data requirement is satisfied. These forms of censoring and selectivity frequently cause problems in estimation and can lead to unreliable inference if they are ignored. Consider, for example, the problems which may arise if one is interested in estimating the parameters from a labor supply equation based on the examination of a panel data set and where one's objective is to make inferences for the whole population rather than only the sample of workers. The first difficulty that arises is that hours are generally only observed for individuals that work. In this way the hours measure is generally censored at zero and this causes difficulties for estimation as straightforward least squares methods, either over the entire sample or only the subsample of workers, are not generally applicable. Second, many of the explanatory variables of interest, such as wages, are also censored in that they are only observed for workers. Moreover, in these instances many of these variables may also be endogenous to labor supply and this may also create complications in estimation. While panel data are frequently seen as a way to overcome issues related to endogeneity as the availability of repeated observations on the same unit can allow the use of various data transformations to eliminate the cause of the endogeneity, in many instances the use of panel data can complicate matters. For example,
Bo Honore´ Department of Economics, Princeton University, Princeton, NJ 08544-1021, USA, e-mail: honore@Princeton.EDU
Francis Vella Department of Economics, Georgetown University, Washington DC, USA, e-mail: fgv@georgetown.edu
Marno Verbeek Department of Financial Management, RSM Erasmus University, Burg. Oudlaan 50, 3062 PA Rotterdam, The Netherlands, e-mail: mverbeek@rsm.nl

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

385

c Springer-Verlag Berlin Heidelberg 2008

386

B. Honore´ et al.

in many cases different observations are not independent because of the presence of individual or time-specific unobserved heterogeneity in the equation of interest, which needs to be accounted for. This increases computational demands, particularly in non-linear models. Also, when one has repeated observations on the same unit it may be the case that some units exit from the data. This last complication is known as attrition and this can further complicate estimation if this exit occurs in a non-random manner.
The aim of this chapter is to present an overview of panel data models involving sample selection, endogenous explanatory variables, censoring and attrition and discuss their estimation. We consider this chapter complementary to Verbeek and Nijman (1996) in that we do not provide a discussion of the various forms of nonresponse that can arise in panel data nor do we explore the issues related to ignorability of non-response. Readers interested in these issues should refer to Verbeek and Nijman (1996). Rather, in this chapter we choose to focus our attention on the more important estimators for empirical work and also review the more recent innovations in this literature. In doing so we focus not only on the standard parametric, likelihood based, procedures for models with sample selectivity and censoring but we also discuss the appropriate semi-parametric procedures which are available. In the following two sections we present the general model and provide a heuristic description of the issues related to selectivity and attrition. We then examine the applicability of standard linear regression based on random or fixed effects procedures for these models. Having established that these procedures are generally only applicable under very restrictive conditions, we focus on the various available alternative forms of estimation. Thus the subsequent sections deal with parametric and semi-parametric estimation of various forms of the model based on different distributional assumptions and different forms of censoring operating in the model. The chapter concludes with a short summary of some empirical applications which involve the use of panel data, sample selection, censoring or endogenous regressors.

12.2 Censoring, Sample Selection and Attrition

To discuss the models and estimators that we consider in this chapter we first present a general model. We then impose restrictions on this model to produce special cases of interest, and consider the various estimators which can be employed to estimate the parameters of interest for these special cases. The general structure of the models considered in this chapter is represented by the following system of simultaneous equations:

yit = m1(xit , zit , yi,t-1; 1) + uit , zit = m2(xit , x1it , zi,t-1; 2) + vit , zit = h(zit ; 3), yit = yit if gt (zi1, . . . , ziT ) = 1 ,
= 0 (or unobserved) otherwise ,

(12.1) (12.2) (12.3) (12.4)

12 Attrition, Selection Bias and Censored Regressions

387

where i indexes individuals (i = 1, . . . , N) and t indexes time (t = 1, . . . , T ); yit and zit are latent endogenous variables with observed counterparts yit and zit ; xit and x1it are vectors of exogenous variables; m1 and m2 denote general functions characterized by the unknown parameters in 1 and 2, respectively. While we will generally focus on the case where we impose index restrictions on the conditional means, we
write the model in the more general form by employing the unknown functions m1 and m2 to capture possible non-linearities. The mapping from the latent variable to its observed counterpart occurs through the censoring functions h and gt noting that the former may depend on the unknown parameter vector 3. We will generally focus on the case where h(.) is an indicator function producing the value 1 if zit > 0, in which case there are no unknown parameters in the censoring process. However, when we consider the available two-step estimators we will also consider
some popular alternative selection rules and these may involve the estimation of
additional parameters. The function gt indicates that yit may only be observed for certain values of
zi1, . . . , ziT . This includes sample selection where yit is only observed if, for example, zit = 1 or, alternatively in the balanced subsample case, if zi1 = . . . = ziT = 1. Alternatively, we will consider a special case of interest in which we replace the
censoring mechanism in (12.4) with

yit = yit · I(yit > 0) ,

(12.5)

where I(.) is an indicator function operator which produces the value 1 if event (.) occurs and zero otherwise. The model which incorporates (12.4) as the censoring or selection rule corresponds with the sample selection model. The model with (12.5) as the censoring mechanism corresponds to the censored regression model.
The above model is very general and nests many models of interest as special cases. For example, it encompasses the static sample selection and censored regression models in which we only observe the dependent variable of primary interest for some subset of the data depending on the operation of a specific selection rule. The primary difference between these two, captured in the censoring processes, is that the sample selection model allows for different factors driving the censoring, zit , and the variation in yit . In this sense it is a double index model. In contrast, the censored regression model imposes that a single index explains the latent variable in the censoring decision and also the variation in yit . The difference between the two is not only a statistical issue in that in many economic models for which panel data estimation is applicable it is possible that the selection rule is based on a different process than that generating variation in the primary dependent variable of interest.
The model also incorporates a potential role for dynamics in both the y equation and the censoring process. That is, while panel data are frequently seen as a mechanism for eliminating unobservables which create difficulties in estimation, an important feature and major attraction of panel data is that it provides the ability

388

B. Honore´ et al.

to estimate the dynamics of various economic relationships based on individual behavior. Clearly this is generally not possible with cross-sectional data. Some of the estimators we discuss below provide some scope for estimating such relationships.
Note that an important feature of these models is related to identification. In many of the models that we consider it is possible to obtain identification of the parameters of interest by simply relying on non-linearities which arise from the distributional assumptions. In general, this is not an attractive, nor frequently accepted, means of identification. As these issues are frequently quite complicated we avoid such a discussion by assuming that the elements in the vector x1it appear as explanatory variables in the selection equation (12.2) but are validly excluded from the primary equation (12.1). In this way the models are generally identified. Readers who are particularly interested in identification should examine the cited papers for a more detailed discussion.
A key aspect of any panel data model is the specification and treatment of its disturbances. We write the respective equations' errors as

uit = i + it vit = i + it

(12.6) (12.7)

which indicates that they comprise individual effects, i and i, and individual specific time effects, it and it , which are assumed to be independent across individuals. This corresponds to the typical one-way error components model. Moreover, we allow the errors of the same dimension to be correlated across equations. In some instances we will assume that both the individual effects and the idiosyncratic disturbances can be treated as random variables, distributed independently of the explanatory variables. In such cases, we will often assume that the error components are drawn from known distributions. For many empirical applications, however, these assumption are not appropriate. For example, one may expect that some subset of the explanatory variables are potentially correlated with the one or both of the different forms of disturbances. Accordingly, it is common to treat the individual effects as fixed effects, which are potentially correlated with the independent variable, and we will consider the available procedures for estimating under such conditions. Second, while distributional assumptions are frequently useful from the sake of implementation, for many applications they may not be appropriate. As many of the procedures we examine are likelihood based any misspecification of the parametric component may lead to the resulting estimators being inconsistent. Thus, while we begin the analysis of each sub-model by making distributional assumptions regarding the disturbances we will also examine some semi-parametric estimators which do not rely on distributional assumptions. Finally, note that for the majority of models the parameters of primary interest are those contained in the vector 1, the variance 2 and, when appropriate, 2 . In some instances, however, there may be interest in the 2 vector.

12 Attrition, Selection Bias and Censored Regressions

389

12.3 Sample Selection and Attrition

Before focussing on the estimation of the parameters described in the above model it is useful to briefly discuss the problem of selection bias through (12.4), or the difficulties which arise from the problems with the presence of censoring via (12.5). It is also useful to consider the effect of sample attrition which captures the process by which some observations disappear, potentially non-randomly, from the sample over time. To illustrate these issues we will follow the discussion in Vella (1998) for the cross-sectional case which is based on the original motivation of Heckman (1974, 1979) and Gronau (1974). Assume that we are interested in examining the determinants of the wages of females, yit , when we only observe the wages of females who are working a positive number of hours. To determine which factors influence wages we examine a panel data set of women where only a sub-sample are engaged in market employment and report wages for each period t. Moreover, assume that the sample comprises of three types; (i) those working and reporting wages for the entire period; (ii) those who fail to work for at least one period but remain in the sample for the entire period; and (iii) those who either work or do not work but do appear in the sample for the entire period. First, the differences between the workers and non-workers determines whether the issue of selection bias might arise. Second, the differences between those who remain in the sample or disappear from the sample determines whether the problem of attrition bias occurs. To illustrate how these biases may arise let us characterize each individual by both her endowments of observable and unobservable characteristics.
First assume that the working sub-sample is chosen randomly from the population. If the working sub-sample have similar endowments of characteristics as the non-working sample there is no reason to suspect selectivity bias will be induced by examining the working sample. That is, as the sample is randomly chosen the average characteristics, in terms of both observable and unobservables, of the working sample should be similar to the average characteristics of the population. The same is also true of attrition bias. That is, provided that individuals simply disappear from the sample in a random manner there is no reason to expect that the attrition affects the characteristics of the sample.
Let us now consider where the decisions to work or remain in the sample are no longer determined by random processes. In this instance, depending on the nature of these processes, the working and non-working samples that one observes potentially have different characteristics from each other and may have different characteristics from those who are no longer in the original sample. Sample selection bias arises when some component of the work decision is relevant to the wage determining process, while attrition bias results from the wage determining process not being independent of the decision to remain in the sample. That is, when some of the determinants of the work/attrition decisions are also influencing the wage. However, if the relationship between each of these respective decisions and the wage is purely

390

B. Honore´ et al.

through the observables one can control for this by including the appropriate conditioning variables in the wage equation. That is, one is able to control for these potential biases by including the appropriate variables in the wage equation. Thus, sample selection or attrition bias will not arise purely on the basis of differences in observable characteristics.1
However, if we now assume the unobservable characteristics affecting the work/attrition decisions are correlated with the unobservable characteristics affecting the wage we generate a relationship between the work/attrition decisions and the process determining wages. Controlling for the observable characteristics when explaining wages is insufficient as some additional processes are influencing the wage. That is, the process determining whether an individual works and the process determining whether an individual remains in the sample are also affecting the wage. If these unobservable characteristics are correlated with the observables then the failure to include an estimate of the unobservables will lead to incorrect inference regarding the impact of the observables on wages. Thus a bias will be induced due to the sample selection and attrition.
This discussion highlights that sample selectivity bias operates through unobservable elements, and their correlation with observed variables. More explicitly, we can see that the presence of selection bias will be determined by the relationship between the two composite errors. It is driven by the correlations between uit and vit (or vi1, . . . , viT ) and their components. Thus in the panel data case one can immediately observe that there are two ways in which the model can be contaminated with selectivity. First, there is the possibility that the individual effects are correlated across equations. Second, there is the possibility that the idiosyncratic disturbances are correlated across equations.
Attrition bias can also be seen as a special case of selection bias in that it arises by the same underlying process. Namely, the unobservable components of the decision to remain in the sample are correlated with the unobservable components of the work or wage equations. In this case, however, one has less flexibility in modelling the attrition as we will not observe the time varying determinants of attrition over the entire sample.
In general, sample selection and attrition problems may arise when a rule other than simple random sampling determines how sampling from the underlying population takes place. This selection rule may distort the representation of the true population and consequently distort inferences based on the observed data using standard methods. Distorting selection rules may be the outcome of self-selection of economic agents, non-response decisions of agents or the consequence of the sample survey process.

1 This assumes that the inclusion of these observables is considered appropriate in a wage equation. For example, lagged wages may affect sample selection or attrition, while lagged wages are typically not included in a wage equation.

12 Attrition, Selection Bias and Censored Regressions

391

12.4 Sample Selection Bias and Robustness of Standard Estimators

One can easily illustrate the problems generated by the presence of attrition or selection bias by examining the properties of standard estimators for the primary equation where we estimate only over the sample of uncensored observations. To do so consider the simplest case of (12.1) where the dependent variable is written as a linear function of only the exogenous explanatory variables:

yit = xit  + i + it ,

(12.8)

where we consider that each of the selection rules captured in (12.4) and (12.5) can
be written as zit = 1. In these instances one should consider that what corresponds to the OLS estimation in the cross-section is OLS estimation over the pooled sample for which yit is observed. To illustrate the problems with such pooled estimation of (12.8) we can take expectations of (12.8) conditional upon yit being observed, which gives

E(yit |xit , zit = 1) = xit  + E(i|xit , zit = 1) + E(it |xit , zit = 1) ,

(12.9)

noting that the last two terms will in general have non-zero values, which are potentially correlated with the x s, due to the dependence between i and i, and it and it . These terms will, in general, be non-zero whenever Pr{zit = 1|yit , xit } is not independent of yit . Accordingly, least squares estimation of (12.8) will lead to biased estimates of  due to this misspecification of the mean.
This above result is well known in the cross-sectional case and is a restatement of the results of Heckman (1979). In that paper the sample selection/censoring problem is shown to be related to the misspecification of the conditional mean. Heckman shows that to correct for this misspecification of the mean, an additional variable can be included, constructed through the use of parameters from an auxiliary equation, explaining the probability of censoring. However, given that in the panel data setting we have repeated observations on the individual one might think that the availability of panel data estimators which exploit the nature of the error structure might provide some scope to eliminate this bias without the use of such a variable. Accordingly, it is useful to discuss the properties of the standard fixed effects and random effects estimators in the linear model when the selection mechanism is endogenous. Thus we first consider estimation of (12.8) by the standard linear fixed effects or random effects procedures.
To consider these estimators we first introduce some additional notation. Observations on yit are treated as available if zit = 1 and missing if zit = 0. We define ci = tT=1 zit , so that ci = 1 if and only if yit is observed for all t. The first estimators for  that we consider are the standard random effects estimators. Defining

i = 1 -

2 2 + Ti2

392

B. Honore´ et al.

where Ti = tT=1 zit denotes the number of time periods yit is observed, the random effects estimator based on the unbalanced panel (using all available cases) can be
written as

NT

-1

  ^RUE =

zit (xit - ix¯i)(xit - ix¯i)

i=1 t=1

NT

  ×

zit (xit - ix¯i)(yit - iy¯i)

i=1 t=1

(12.10)

where x¯i = Ti-1 tT=1 zixit and y¯i = Ti-1 tT=1 ziyit denote averages over the available observations. In some cases attention may be restricted to the balanced sub-panel
comprising only those individuals that have completely observed records. The re-
sulting random effects estimator is given by

NT

-1

  ^RBE =

ci(xit - ix¯i)(xit - ix¯i)

i=1 t=1

NT

  ×

ci(xit - ix¯i)(yit - iy¯i) .

i=1 t=1

(12.11)

Note that all units for which ci = 1 will have the same value for i. Under appropriate regularity conditions, these two estimators are consistent for N   if

E(i + it |zi) = 0 ,

(12.12)

where zi = (zi1, . . . , ziT ) . This condition states that the two components of the error term in the model are mean independent of the sample selection indicators in zi (conditional upon the exogenous variables). This appears to be a very strong condition and essentially implies that the selection process is independent of both of the unobservables in the model. One would suspect that for a large range of empirical cases this is unlikely to be true and this does not appear to be an attractive assumption to impose.
Given that the random effects estimator does not appear to be useful in the presence of selection bias it is worth focussing on the suitability of the fixed effects estimators of  . For the unbalanced panel the estimator can be written as

NT

-1

  ^FUE =

zit (xit - x¯i)(xit - x¯i)

i=1 t=1

NT

  ×

zit (xit - x¯i)(yit - y¯i) ,

i=1 t=1

(12.13)

while the corresponding estimator for the balanced sub-panel is given by

12 Attrition, Selection Bias and Censored Regressions

393

NT

-1

  ^FBE =

ci(xit - x¯i)(xit - x¯i)

i=1 t=1

NT
×   ci(xit - x¯i)(yit - y¯i) . i=1 t=1

(12.14)

Under appropriate regularity conditions, consistency of these two estimators re-

quires that

E(it - ¯i|zi) = 0 ,

(12.15)

where ¯i = Ti-1 tT=1 zit it . Clearly, this indicates that the estimation over the subsample for which zit = 1 will produce consistent estimates if the random com-
ponent determining whether zit = 1 is eliminated in the fixed effects transformation.
That is, the unobservable component determining selection for each individual is

time-invariant. While this may be true in certain instances it is likely that in many

empirical examples such an assumption would not be reasonable as it imposes that

the selection process is independent of the idiosyncratic errors.

This discussion illustrates that the conventional linear panel data estimators

are inappropriate for the linear model with selection. The random effects esti-

mator essentially requires that selection is determined outside the model while

the fixed effects estimator imposes that, conditional on the individual effects, the

selection process is determined outside the model. While the fixed effects esti-

mator is more robust, it still is unsatisfactory for most empirical examples of

panel data models with selectivity. Accordingly, we now begin to examine a range

of estimators which handle the situation for which (12.12) and (12.15) are not

satisfied.

12.5 Tobit and Censored Regression Models

The first model considered can be fully described by a subset of the equations capturing the general model outlined above. The model has the form

yit = m1(xit , yi,t-1; 1) + uit , yit = yit if yit > 0,
= 0 (or unobserved) otherwise.

(12.16) (12.17)

This considers a latent variable yit , decomposed into a conditional mean depending upon xit and possibly a lagged observed outcome yi,t-1, and an unobserved mean zero error term uit . The observed outcome equals the latent value if the latter is positive and zero otherwise. This model is the panel data extension of the tobit type I
(under certain distributional assumptions) or censored regression model which is

394

B. Honore´ et al.

commonly considered in cross-sectional analyses. A special case, which we do not consider here, arises when both yit and xit are unobserved if yit  0.
We now consider estimation of this standard censored regression model in
(12.16) and (12.17) under different sets of assumptions. The simplest case arises when the lagged dependent variable is excluded from (12.16), and when it is assumed to be drawn from a normal distribution, independent of the explanatory
variables. We then consider the model where we allow for a lagged dependent vari-
able. As we will see the estimation is somewhat more difficult because one has to
incorporate the additional complications arising from the initial conditions. We then
proceed to a consideration of the model where we relax the distributional assump-
tions that we impose on the error terms.

12.5.1 Random Effects Tobit

First, we consider the static tobit model, given by

yit = m1(xit ; 1) + uit ,

where the censoring rule is stated in (12.5)

yit = yit if yit > 0, yit = 0 if otherwise.

(12.18)

We also assume that uit has mean zero and constant variance, independent of (xi1, . . . , xiT ). In order to estimate 1 by maximum likelihood we add an additional assumption regarding the joint distribution of ui1, . . . , uiT . The likelihood contribution of individual i is the (joint) probability/density of observing the T outcomes
yi1, . . . , yiT , which is determined from the joint distribution of the latent variables yi1, . . . , yiT by integrating over the appropriate intervals. In general, this will imply T integrals, which in estimation are typically to be computed numerically. When
T = 4 or more, this makes maximum likelihood estimation infeasible.
If the uit are assumed to be independent, we have that

 f (yi1, . . . , yiT |xi1, . . . , xiT ; 1) = f (yit |xit ; 1) , t

where 1 contains all relevant parameters (including 1), which involves T onedimensional integrals only (as in the cross-sectional case). This, however is highly
restrictive. If, instead, we impose the error components assumption that uit = i + it , where it is i.i.d. over individuals and time, we can write the joint probability/density as

12 Attrition, Selection Bias and Censored Regressions

395


f (yi1, . . . , yiT |xi1, . . . , xiT ; 1) = f (yi1, . . . , yiT |xi1, . . . , xiT , i; 1) f (i)di
-



 =

f (yit |xit , i; 1) f (i)di,

- t

(12.19)

where f is generic notation for a density or probability mass function. This is a
feasible specification that allows the error terms to be correlated across different
periods, albeit in a restrictive way. The crucial step in (12.19) is that conditional upon i the errors from different periods are independent.
In principle arbitrary assumptions can be made about the distributions of i and it . For example, one could assume that it is i.i.d. normal while i has a Student tdistribution. However, this may lead to distributions for i + it that are nonstandard and this is unattractive. Accordingly, it is more common to start from the joint distri-
bution of ui1, . . . , uiT . We assume that the joint distribution of ui1, . . . , uiT is normal with zero means and variances equal to 2 + 2 and cov{uit , uis} = 2 , s = t. This is the same as assuming that i is NID(0, 2 ) and it is NID(0, 2). The likelihood function can then be written as in (12.19), where

f

(i)

=

1 2 

exp

-1 2

i 2 

.

(12.20)

and

f

(yit |xit , i; 1)

=

1 2 

exp

-1 2

yit - m1(xit ; 1) - i 2 

if yit > 0

= 1-

m1(xit ; 1) + i 

if yit = 0 ,

(12.21)

where  denotes the standard normal cumulative density function. The latter two expressions are similar to the likelihood contributions in the cross-sectional case, with the exception of the inclusion of i in the conditional mean. The estimation of this model is identical to estimation of the tobit model in the cross-sectional setting except that we now have to account for the in clusion of the individual specific effect. As this individual effect is treated as a random variable, and the disturbances in the model are normally distributed, the above procedure is known as random effects tobit. Note that while we do not do so here, it would be possible to estimate many of the models considered in the survey of crosssectional tobit models by Amemiya (1984) by allowing for an individual random effect.

396

B. Honore´ et al.

12.5.2 Random Effects Tobit with Endogenous Explanatory Variables

The discussion of the random effects tobit model in the previous section assumed that the disturbances are independent of the explanatory variables. One useful extension of the model would be instances where some of the explanatory variables were treated as endogenous. This is similar to the cross-sectional model of Smith and Blundell (1986) who present a conditional ML estimator to account for the endogeneity of the explanatory variables.2 The estimator simply requires estimating the residuals from the model for the endogenous explanatory and including them as an additional explanatory variable in the cross-sectional tobit likelihood function. Vella and Verbeek (1999) extend this to the panel case by exploiting the error components structure of the model. We now present this case where we assume the endogenous explanatory variable is fully observed. The model has the following form:

yit = m1(xit , zit ; 1) + i + it zit = m2(xit , x1it , zi,t-1; 2) + i + it yit = yit · (yit > 0)

(12.22) (12.23) (12.24)

The model's disturbances are assumed to be generated by the following distribution:

i + i i + i

|Xi  NID

0 0

,

2  + 2I   +  I 2 + 2I

(12.25)

where  is a T -vector of ones. Exploiting this joint normality assumption allows us

to write

E(uit |Xi, vi) = 1vit + 2v¯i ,

(12.26)

where 1 =  /2, 2 = T ( -  2/2)/(2 + T 2) and v¯i = T -1 tT=1 vit . As the endogenous explanatory variable is uncensored the conditional distribu-

tion of the error terms in (12.22) given zi remains normal with an error components structure. Thus one can estimate the model in (12.22) and (12.24) conditional on the

estimated parameters from (12.23) using the random effects likelihood function, after making appropriate adjustments for the mean and noting that the variances now

reflect the conditional variances. Write the joint density of yi = (yi1, . . . , yiT ) and zi given Xi as:3

f (yi|zi, Xi; 1, 2) f (zi|Xi; 2) ,

(12.27)

2 Rivers and Vuong (1988) consider the cross-sectional probit counterpart of the Smith and Blundell (1986) procedure.
3 When (12.23) is dynamic with an exogenous initial value zio, (12.27) is valid if zio is included in Xi. When the initial value is endogenous, we need to include zi0 in zi.

12 Attrition, Selection Bias and Censored Regressions

397

where 1 denotes (1, 2 , 2,  ,  ) and 2 denotes (2, 2, 2). We first estimate 2 by maximizing the marginal likelihood function of the zi's. Subsequently, the conditional likelihood function

 f (yi|zi, Xi; 1, ^2) i

(12.28)

is maximized with respect to 1 where ^2 denotes a consistent estimate of 2. The conditional distribution of yi given zi is multivariate normal with an error components structure. The conditional expectation can be derived directly from (12.26), substituting vit = zit - m2(xit , x1it , zi,t-1; 2), while the covariance structure corresponds to that of 1i + 2,it , where 1i and 2,it are zero mean normal variables with zero covariance and variances

12 = V {1i} = 2 - 2 -2 ,

(12.29)

22

= V {2,it } = 2 -

T 2 2 + 2  2 - 2 2 2(2 + T 2)

.

(12.30)

These follow from straightforward matrix manipulations and show that the error
components structure is preserved and the conditional likelihood function of (12.22)
and (12.24) has the same form as the marginal likelihood function without endogenous explanatory variables.4
The conditional maximum likelihood estimator can be extended to account for
multiple endogenous variables as the appropriate conditional expectation is easily
obtained as all endogenous regressors are continuously observed. Even if the re-
duced form errors of the endogenous regressors are correlated, provided they are
characterized by an error components structure it can be shown that the conditional distribution of i + it also has an error components structure. Time-specific heteroskedasticity in it does not affect the conditional expectations and can be incorporated by having 12 vary over time. The model can also be estimated, along the lines suggested above, over subsets of the data chosen on the basis of zit .
One obvious complication which arises in estimation of these models is that the
standard errors have to be adjusted for the estimation uncertainty in the correc-
tion terms. This is an example of the standard "generated regressor problem" even
though the second step is estimated by maximum likelihood. Vella and Verbeek
(1999) provide the formulae for the standard errors in this particular context but for
a more general treatment the reader is referred to Newey (1984).
In general the conditional maximum likelihood estimator cannot be employed when zit = zit . Thus the family of sample selection models considered below cannot be estimated by conditional maximum likelihood. One interesting exception,

4 The algebraic manipulations are simplified if 12 and 22 replace the unconditional variances 2 and 2 in 1. In this case, estimates for the latter two variances are easily obtained in a third step from the estimates from the first stage for 2 and 2, and the estimated covariances from the mean function, using the equalities in (12.29) and (12.30).

398

B. Honore´ et al.

however, is when the primary equation is estimated over the subsample of individuals that have zis = zis, for all s = 1, . . . , T.5 This follows from the result that the error components structure is preserved when the reduced form dependent variables are
observed.
Due to the presence of endogeneity in this model it is clear that one needs to
carefully consider identification. In these models there is no non-linearity induced
in the correction terms, but the non-linearity of m1 or m2 will identify the model. In the linear case, or if one does not want to rely on non-linearities for identification,
exclusion restrictions are required. More explicitly, for each endogenous explana-
tory variable we need one exclusion restriction in the primary equation, unless, as
before, the endogeneity can be restricted to be related to the time-invariant components only ( = 0). This requires that x1it is nonempty and has elements validly excluded from (12.22).

12.5.3 Dynamic Random Effects Tobit

The ability to estimate dynamic relationships from individual level data is an impor-

tant attraction of panel data. Accordingly, an extension to the above model which

involves the inclusion of a lagged dependent variable is of economic interest. Let

us now reconsider the random effects tobit model, and generalize the latent variable

specification to

yit = m1(xit , yi,t-1; 1) + i + it ,

(12.31)

with yit = yit if yit > 0 and 0 otherwise. Now consider maximum likelihood estimation of this dynamic random effects tobit model, making the same distributional

assumptions as above. In general terms, the likelihood contribution of individual i

is given by (compare (12.19))



f (yi1, . . . , yiT |xi1, . . . , xiT ; 1) = f (yi1, . . . , yiT |xi1, . . . , xiT , i; 1) f (i)di


=
-

-
T
 f (yit |yi,t-1, xit , i; 1)
t=2

f (yi1|xi1, i; 1) f (i)di ,

(12.32)

where

f (yit |yi,t-1, xit , i; 1)

=  1 exp - 1

2 

2

yit - m1(xit , yi,t-1; 1) - i 2 

= 1-

m1(xit , yi,t-1; 1) + i 

if yit = 0.

if yit > 0,

5 A similar argument is exploited in Arellano et al. (1999).

12 Attrition, Selection Bias and Censored Regressions

399

This is completely analogous to the static case and yi,t-1 is simply included as an additional explanatory variable. However, the term f (yi1|xi1, i; 1) in the likelihood function may cause problems. It gives the distribution of yi1 without knowing its previous value but conditional upon the unobserved heterogeneity term i.
If the initial value is exogenous in the sense that its distribution does not depend upon i, we can place the term f (yi1|xi1, i; 1) = f (yi1|xi1; 1) outside the integral. In this case, we can simply consider the likelihood function conditional upon yi1 and ignore the term f (yi1|xi1; 1) in estimation. The only consequence may be a loss of efficiency if f (yi1|xi1; 1) provides information about 1. This approach would be appropriate if the starting value is necessarily the same for all individuals or if it is
randomly assigned to individuals.
However, it may be hard to argue in many applications that the initial value yi1 is exogenous and does not depend upon a person's unobserved heterogeneity. In that case we would need an expression for f (yi1|xi1, i; 1) and this is problematic. If the process that we are estimating has been going on for a number of periods before the current sample period, f (yi1|xi1, i; 1) is a complicated function that depends upon person i's unobserved history. This means that it is typically impossible to derive an expression for the marginal probability f (yi1|xi1, i; 1) that is consistent with the rest of the model. Heckman (1981) suggests an approximate solution to this
initial conditions problem that seems to work reasonably well in practice. It requires
an approximation for the marginal distribution of the initial value by a tobit func-
tion using as much pre-sample information as available, without imposing restrictions between its coefficients and the structural parameters in 1. Vella and Verbeek (1998, 1999) provide illustrations of this approach. Wooldridge (2005) suggests an
alternative approach that is based on considering the likelihood function conditional
upon the initial values yi1 and then making parametric assumptions about the distribution of the unobserved effect conditional upon the initial value and any exogenous explanatory variables, f (i|yi1, xi1; 1), rather than f (yi1|xi1, i; 1) f (i). Because the impact of the initial conditions diminishes if the number of sample periods T
increases, one may decide to ignore the problem when T is fairly large.

12.5.4 Fixed Effects Tobit Estimation

The fully parametric estimation of the tobit model assumes that both error components have a normal distribution, independent of the explanatory variables. Clearly, this is restrictive and a first relaxation arises if we treat the individual-specific effects i as parameters to be estimated, as is done in the linear fixed effects model. However, such an approach is generally not feasible in non-linear models. The loglikelihood function for the fixed effects tobit model has the general form

NT
log L =   log f (yit |xit , i; 1) , i=1 t=1

(12.33)

400

B. Honore´ et al.

where (as before)

f

(yit |xit , i; 1)

=

1 2 

exp

-1 2

yit - m1(xit ; 1) - i 2 

= 1-

m1(xit ; 1) + i 

if yit = 0.

if yit > 0

Maximization of (12.33) can proceed through the inclusion of N dummy variables to capture the fixed effects or using an alternative strategy, described in Greene (2004), which bypasses the large computation demands of including so many additional variables.
This fixed effects tobit estimators is subject to the incidental parameter problem (Neyman and Scott, 1948, Lancaster, 2000), and results in inconsistent estimators of the parameters of interest if the number of individuals goes to infinity while the number of time periods is fixed. It was generally believed that the bias resulting from fixed effects tobit was large although more recent evidence provided by Greene suggests this may not be the case. On the basis of Monte Carlo evidence, Greene (2004) concludes that there is essentially no bias in the estimates of 1. However, the estimate of  is biased and this generates bias in the estimates of the marginal effects. Greene also concludes that the bias is small if T is 5 or greater.
Hahn and Newey (2004) suggest two approaches to bias reduction in fixed effects estimation of non-linear models such as the fixed effects tobit model. The first procedure is based on the use of jackknife methods and exploits the variation in the fixed effects estimator when each of the observations are, in turn, separately deleted. By doing so one is able to form a bias-corrected estimator using the Quenouille (1956) and Tukey (1958) jackknife formula. For simplicity, let m1(xit ; 1) = xit  and let (t) denote the fixed effects estimator based on the subsample excluding the obser-
vations for the tth wave. The jackknife estimator (JK) is defined to be

T
 JK = T  - (T - 1) (t)/T , t=1

where  is the fixed effects estimator based on the entire panel. Hahn and Newey note that the panel jackknife is not particularly complicated. While it does require (T + 1) fixed effects estimations of the model one can employ the algorithm pro-
posed by Greene, discussed above, and the estimates of  and i can be used as starting values.
The second procedure is an analytic bias correction using the bias formula obtained from an asymptotic expansion as the number of periods grows. This is based on an approach suggested by Waterman et al. (2000) and is also related to the approach adopted by Woutersen (2002). Note that while none of these authors examine the fixed effect tobit model, preferring to focus mainly on discrete choice models, the approaches are applicable. Hahn and Newey (2004) provide some simulation evidence supporting the use of their procedures in the fixed effects probit model.

12 Attrition, Selection Bias and Censored Regressions

401

12.5.5 Semi-parametric Estimation

As shown in Honore´ (1992) is also possible to estimate the parameters of panel data tobit models like (12.16) and (12.17) with no assumptions on the distribution of the individual specific effects and with much weaker assumptions on the transitory errors.
To fix ideas, consider a model with a linear index restriction, that is
yit = xit  + i + it ,
and
yit = yit if yit > 0, yit = 0 otherwise.

The method proposed in Honore´ (1992) is based on a comparison of any two time periods, t and s. The key insight behind the estimation strategy is that if it and is are identically distributed conditional on (xit , xis) then

vist ( ) = max{yis, (xis - xit )  } - max{0, (xis - xit )  } = max{i + is, -xis , -xit  } - max{-xis , -xit  }
and

vits ( ) = max{yit , (xit - xis)  } - max{0, (xit - xis)  } = max{i + it , -xit  , -xis } - max{-xit  , -xis }
are also identically distributed conditional on (xit , xis). This can be used to construct numerous moment conditions of the form

E [(g (vist ( )) - g (vits ( ))) h (xit , xis)] = 0

(12.34)

If g is increasing and h (xit , xis) = xis - xit , these moment conditions can be turned into a minimization problem which identifies  subject to weak regularity conditions. For example, with g (d) = d, (12.34) corresponds to the first-order conditions
of the minimization problem

minimize E b

max{yis, (xis - xit ) b} - max{yit , - (xis - xit ) b} - (xis - xit ) b 2 + 2 · 1{yis < (xis - xit ) b}((xis - xit ) b - yis)yit + 2 · 1{yit < - (xis - xit ) b}(- (xis - xit ) b - yit )yis

which suggests estimating  by minimizing

402

B. Honore´ et al.

n
  max{yis, (xis - xit ) b}
i=1 s<t
- max{yit , - (xis - xit ) b} - (xis - xit ) b 2
+ 2 · 1{yis < (xis - xit ) b}((xis - xit ) b - yis)yit
+ 2 · 1{yit < - (xis - xit ) b}(- (xis - xit ) b - yit )yis

(12.35)

The objective function in (12.35) is convex in b, as are other objective functions based on (12.34). This means that it is extremely easy to find the estimator
 . [Charlier et al. (2000) consider a conditional moment conditions estimator based on (12.34). This estimator is more efficient that then one obtained by minimizing (12.35) but it is more difficult to calculate.
Honore´ and Kyriazidou (2000) discuss estimators defined by a general g (d) as well as estimators based on moment conditions that are derived under the stronger assumption that the distribution of (it , is) is exchangeable conditional on (xit , xis).

12.5.6 Semi-parametric Estimation in the Presence of Lagged Dependent Variables
Honore´ (1993), Hu (2002) and Honore´ and Hu (2004) show how one can modify the moment conditions in (12.34) in such a way that one can allow for lagged dependent variables as explanatory variables. The specifics for this differs depending on whether the lagged latent or the lagged censored variable is used, and the main difficulty in this literature is that it is not easy to show that the moment conditions are uniquely satisfied at the true parameter values.

12.6 Models of Sample Selection and Attrition
As discussed above the tobit model has the somewhat unattractive feature that the index that explains the censoring also is required to explain the variation in the dependent variable of primary interest. We now turn our attention to the estimation of the model where the selection process is driven by a different index to that generating the dependent variable of primary interest. One might think, for example, that the number of hours an individual works depends on some different factors than those determining the work decision. At the very least one might think that the weights on each of the factors differs across decisions. For the sake of simplification we introduce the following form of the model

12 Attrition, Selection Bias and Censored Regressions

403

yit = xit  + i + it , zit = xit 21 + x1it 22 + i + vit , zit = I(zit > 0), yit = yit · zit

(12.36) (12.37) (12.38)

where we again highlight that the vector x1it is nonempty (and not collinear with xit ). While this is sometimes seen as a controversial assumption we do not discuss the merits of such an approach. Below, we also discuss the scope of introducing dynamics into the primary equation.

12.6.1 Maximum Likelihood Estimators

Given that we can make distributional assumptions regarding the error components it is natural to construct a maximum likelihood estimator for all the parameters in (12.36) and (12.38). Consider the case where the individual effect is treated as a random effect and the disturbances are all normally distributed. To derive the likelihood function of the vectors zi and yi, we first write

log f (zi, yi) = log f (zi|yi) + log f (yi)

(12.39)

where f (zi|yi) is the likelihood function of a conditional T -variate probit model and f (yi) is the likelihood function of a Ti-dimensional error components regression model, where Ti = t zit . The second term can be written as

log

f

(yi)

=

-Ti 2

log

2

-

Ti

- 2

1

log

2

-

1 2

(2

+

Ti2

)

 -

1 22

T t=1

zit

(yit

-

xit 

)2

-

2(2

Ti + Ti2

)

(y¯i

-

x¯i

)2.

(12.40)

The first term in (12.39) requires the derivation of the conditional distribution of the error term in the probit model. From the assumption of joint normality and defining it = zit (i + it ), the conditional expectation of vit = i + it is given by

 E

(i

+

it

|i1

,

.

.

.

,

iT

)

=

zit

  2

it

-

2

2 + Ti2

T
it
t=1

 +

2

  + Ti2

T
it
t=1

(12.41)

Using our distributional assumptions the conditional distribution of i + it given i1, . . . , iT corresponds to the unconditional distribution of the sum of three normal variables eit + 1i + zit 2i whose distribution is characterized by

404

B. Honore´ et al.

E(1i) = E(2i) = 0, E(eit ) = cit V (eit ) = 2 - zit 2 /2 = st2 V (1i) = 2 - Ti2 (2 + Ti2 )-1 = k1 V ( 2i) = 2 2 -2(2 + Ti2 )-1 = k2 cov( 1i,  2i) = -  (2 + Ti2 )-1 = k12,
where the other covariances are all zero and note that we do not explicitly add an index i to the variances st2, k1 and k2. Similar to the unconditional error components probit model the likelihood contribution can be written as

f (zi|yi) =

T

t=1

dit

xit



21

+

x1it



22

+ st

cit

+

1i

+

zit



2i

× f (1i,  2i)d 1id 2i

where dit = 2zit - 1 and f (., .) is the density of 1i and 2i. Using these various expressions it is now possible to construct the complete likelihood function. Computation of the maximum likelihood estimator requires numerical integration over two dimensions for all individuals which are not observed in each period. Thus the computational demands are reasonably high and as a result this approach has not been proven to be popular in empirical work.

12.6.2 Two-Step Estimators

A shortcoming with the maximum likelihood approach outlined above is that the model can be sometimes difficult to estimate due to its computational demands. While there is relatively little experience of estimating such models in the panel setting (see Keane et al., 1988, for an example) it is clear that the ML selection type corrections in the cross-section setting is far less popular than the subsequently developed two-step estimators. To present the two-step estimators in the panel setting we follow the approach of Vella and Verbeek (1999). In this case we again start with the model presented in (12.36), (12.37) and (12.38). Note that although we focus on estimating the above model we retain some degree of generality. This allows us to more easily talk about extensions of the above model to alternative forms of censoring. The approach that we adopt is a generalization of the Heckman (1979) cross-sectional estimator to the panel data model. For the model that immediately follows, the estimation procedure is also found in Ridder (1990) and Nijman and Verbeek (1992).
To motivate a two-step estimator in this setting we begin by conditioning (12.36) on the vector zi (and the matrix of exogenous variables Xi) to get

E(yit |Xi, zi0, zi) = xit  + E(uit |Xi, zi0, zi) .

(12.42)

12 Attrition, Selection Bias and Censored Regressions

405

If the mean function of (12.37) does not depend upon zi,t-1 and sample selection only depends on the current value of zit , it is possible to condition only on zit and not zi = (zi1 . . . .ziT ) and base estimators on the corresponding conditional moments (see Wooldridge, 1995). In this case zi0 drops from the conditioning set. We assume, as before, that the error terms in the selection equation vit = i + it exhibit the usual one-way error components structure, with normally distributed components. That is

vi|Xi  NID(2u + 2I) .
Note that while we do make explicit distributional assumptions about the disturbances in the main equation we assume

E(uit |Xi, vi) = 1vit + 2vi .

(12.43)

Equation (12.43) implies that the conditional expectation E(it |Xi, zi0, zi) is a linear function of the conditional expectation of vit and its individual specific mean noting that the  s are parameters to be estimated. To derive the conditional expectation of the terms on the right hand side of (12.43) we use

E(uit |Xi, zi0, zi) = [i + E(it |Xi, zi0, zi, i)] f (i|Xi, zi0, zi) di ,

(12.44)

where f (i|Xi, zi0, zi) is the conditional density of i. The conditional expectation E(it |Xi, zi0, zi, i) is the usual cross-sectional generalized residual (see Gourieroux et al., 1987, Vella, 1993) from (12.37) and (12.38), since, conditional on i, the errors from this equation are independent across observations. The conditional dis-
tribution of i can be derived using the result

f (i|Xi, zi0, zi) =

f (zi, zi0|Xi, i) f (i) f (zi, zi0|Xi)

,

(12.45)

where we have used that i is independent of Xi and

f (zi, zi0|Xi) = f (zi, zi0|Xi, i) f (i)di

(12.46)

is the likelihood contribution of individual i in (12.37) and (12.38). Finally

T

 f (zi, zi0|Xi, i) =

f (zit |Xi, zi,t-1, i) f (zi0|Xi, i) ,

t=1

(12.47)

where f (zit |Xi, zi,t-1, i) has the form of the likelihood function in the cross-sectional case. If we assume that f (zi0|Xi, i) does not depend on i, or any of the other error components, then zi0 is exogenous and f (zi0|Xi, i) = f (zi0|Xi). Thus we can condition on zi0 in (12.46) and (12.47) and obtain valid inferences neglecting its distribution. In general, however, we require an expression for the distribution of
the initial value conditional on the exogenous variables and the i. As stated above

406

B. Honore´ et al.

in the discussion of the random effects tobit model, the typical manner in which this is done is to follow Heckman (1981) in which the reduced form for zi0 is approximated using all presample information on the exogenous variables.6
Thus the two-step procedure takes the following form. The unknown parameters in (12.37) and (12.38) are estimated by maximum likelihood while exploiting the random effects structure. Equation (12.44) is then evaluated at these ML estimates by employing the expression for the likelihood function in an i.i.d. context, the corresponding generalized residual, and the numerical evaluation of two one dimensional integrals. This estimate, and its average over time for each individual provide two additional terms to be included in the primary equation. The additional parameters, corresponding to 1 and 2, can then be estimated jointly with  . Under the null hypothesis of exogenous sample selection, 1 = 2 = 0, and there is no need to adjust the standard errors. Thus the standard Wald test is a test of sample selection bias. However, in general the standard errors need to be adjusted for heteroskedasticity, serial correlation, and for estimation of the correction terms.
As we noted above, the correction terms have been written to allow greater flexibility with respect to the censoring process. We address this issue in the following section. However, as the model in (12.36), (12.37) and (12.38) is perhaps the most commonly encountered for panel data models with selectivity it is useful to see the form of the correction terms. The first step is to estimate the model by random effects probit to obtain estimates of the 2s and the variances 2 and 2. We then compute (12.44) and its individual specific average after inserting the following terms

E(it |Xi, zi0, zi, i)

=

dit 

 

xit 21 + x1it 22 + i



dit

xit

21

+

x1it 

22

+

i

,

(12.48)

where  denotes the standard normal density function, and

f (i|Xi, zi0, zi) =

tT=1 

dit

xit

21

+

x1it 

22

+

i

 -

tT=1



dit

xit

21

+ x1it 

22

+



1 



i 

1 



 

, (12.49) d

where dit = 2zit - 1. The model can be estimated by maximum likelihood if we make some addi-
tional distributional assumptions regarding the primary equation errors. If all error
components are assumed to be homoskedastic and jointly normal, excluding au-
tocorrelation in the time-varying components, it follows that (12.43) holds with 1 =  /2 and 2 = T ( -  2/2)/(2 + T 2). This shows that 2 is nonzero even when the individual effects i and i are uncorrelated. In contrast, the two-step approach readily allows for heteroskedasticity and autocorrelation in the

6 Note that apart from its dependence on i, the specification of f (zi0|Xi, i) can be tested separately from the rest of the model.

12 Attrition, Selection Bias and Censored Regressions

407

primary equation. Moreover, the assumption in (12.43) can easily be relaxed to, for

example:

E(uit |Xi, vi) = 1t vi1 + 2t vi2 + . . . + Tt viT .

(12.50)

By altering (12.43) this approach can be extended to multiple sample selection rules. With two selection rules, z1,it and z2,it , say, with reduced form errors v1,it and v2,it , respectively, (12.43) is replaced by

E(uit |Xi, v1,i, v2,i) = 11v1,it + 12v¯1,i + 21v2,it + 22v¯2,i .

(12.51)

Computation of the generalized residuals, however, now requires the evaluation of E{v j,it |Xi, z1,i, z2,i} for j = 1, 2. Unless z1,i and z2,i are independent, conditional upon Xi, the required expressions are different from those obtained from (12.44) and (12.45) and generally involve multi-dimensional numerical integration.
While the two-step procedure is highly parameterized, many of the above assumptions can be tested empirically. While relaxing normality in the reduced form is typically computationally difficult, it is possible to test for departures from normality. It is also possible to test for serial correlation and heteroskedasticity using conditional moment tests. Also, we assume that the variables in Xi are strictly exogenous. This assumption excludes lagged dependent variables in the primary equation as well as feedback from lagged values of y to current x's. If components of Xi are not strictly exogenous they should be included in z and excluded from the reduced form.

12.6.3 Alternative Selection Rules
The discussion above frequently assumes that the selection rule is based on a binary outcome and for this reason the selection process was based on the use of the random effects probit likelihood function. However, just as in the cross-sectional case where the selection rule has been extended to alternatives rather than just the binary case (see Vella, 1993) it is useful to do so in the panel context. Two obvious, and practical, extensions are the two following models. The first is the extension to panel data of the Tobit type 3 model given by
yit = xit 1 + 2zit + uit , zit = xit 21 + x1it 22 + vit , zit = zit · I(zit > 0), yit = yit · I(zit > 0).
In this case one sees that the primary equation may or may not have the censoring variable as an endogenous explanatory variable and the censoring equation is censored at zero but observed for positive values. In our wage example discussed above, the extension implies that we observe not only whether the individual works but also the number of hours. We also allow the number of hours to affect the wage rate. For

408

B. Honore´ et al.

this model we would first estimate the censoring equation by random effects tobit. We would then use these estimates, along with the appropriate likelihood contribution and tobit generalized residual, to compute (12.44) which are to be included in the main equation. Note that due to the structure of the model the inclusion of the correction terms accounts for the endogeneity of zit in the main equation.
A second model of interest is where the zit is observed as an ordinal variable, taking values j for j = 1, . . . , J, and where the values of yit are only observed for certain values of j. In this case, where the dummies denoting the value of zit do not appear in the model, we would conduct estimation in the following way. Estimate the censoring equation by random effects ordered probit and then compute the corrections based on (12.44) accordingly. Then estimate the main equation over the subsample for zit corresponding to a specific value and including the correction terms. When one wishes to include the dummies denoting the value of zit as additional explanatory variable it is necessary to pool the sample for the different values of zit and include the appropriate corrections.

12.6.4 Two-Step Estimators with Fixed Effects

A feature of the two-step estimator discussed above is their reliance on the assumption that the individual effect is random variable and independent of the explanatory variables. While the approach proposed by Vella and Verbeek (1999) is somewhat able to relax the latter assumption it is generally difficult to overcome. For this reason, as we noted above in the discussion of the censored regression model, it is generally more appealing to treat the individual fixed component of the error term as a fixed effect which may be correlated with the explanatory variables. We noted above that the results of Hahn and Newey (2004) would allow one to estimate a fixed effects tobit model and then perform the appropriate bias correction. Accordingly, it would be useful to adopt the same approach in the sample selection model and this has been studied by Fernandez-Val and Vella (2005). The basic model they study has the form

yit = xit  + i + it , zit = xit 21 + x1it 22 + i + it , zit = I(zit > 0), yit = yit · zit ,

(12.52) (12.53) (12.54) (12.55)

where the i and i are individual specific fixed effects, potentially correlated with each other and the explanatory variables, and the it and it are random disturbances which are jointly normally distributed and independent of the explanatory variables. While Fernandez-Val and Vella (2005) consider various forms of the censoring function, such as described in the previous section, we focus here on the standard case where the selection rule is a binary censoring rule.
The estimators proposed by Fernandez-Val and Vella (2005) are based on the following approach. One first estimates the reduced form censoring rule by the

12 Attrition, Selection Bias and Censored Regressions

409

appropriate fixed effects procedure. In the case of the standard selection rule this would be fixed effects probit but in the case of tobit censoring rules or ordered selection rules one would then use fixed effects tobit or fixed effects ordered probit respectively. Once these estimates are obtained one uses the bias correction approaches outlined in Hahn and Newey (2004) to adjust the estimates. With these bias corrected estimates one then computes the appropriate correction terms which generally correspond to the cross-sectional generalized residuals. One then estimates the main equation, (12.52), by a linear fixed effects procedure and bias correct the estimates. Fernandez-Val and Vella (2005) study the performance of this procedure to a range of models for alternative forms of censoring. These include the static and dynamic binary selection rule, and the static and dynamic tobit selection rule. They find that the Monte Carlo evidence suggests these procedures are very effective in eliminating selectivity bias. In instances where the adjustments were made to account for the endogeneity of the explanatory variables the procedures were also effective.

12.6.5 Semi-parametric Sample Selection Models

Kyriazidou (1997) also studied the model in (12.52), (12.53), (12.54) and (12.55). Her approach is semi-parametric in the sense that no assumptions are placed on the individual specific effects i and i and the distributional assumptions on the transitory errors it and it are weak.
It is clear that (21, 22) can be estimated by one of the methods for estimation of discrete choice models with individual specific effects, such as Rasch's (1960, 1961) conditional maximum likelihood estimator, Manski's (1987) maximum score estimator or the smoothed versions of the conditional maximum score estimator con-
sidered in Charlier et al. (1995) or Kyriazidou (1995). Kyriazidou's insight into estimation of  combines insights from the literature on the estimation of semiparametric sample selection models (see Powell, 1987) with the idea of eliminating
the individual specific effects by differencing the data. Specifically, to difference out the individual specific effects i, one must restrict attention to time periods s and t for which y is observed. With this "sample selection", the mean of the error term in
period t is

it = E( it | it > -xit 21 - x1it 22 - i, is > -xis21 - x1is22 - i, i)

where i = (xis, x1is, xit , x1it , i, i). The key observation in Kyriazidou (1997) is that if (it , it ) and (is, is) are independent and identically distributed (conditional on (xis, x1is, xit , x1it , i, i)), then for an individual i, who has xit 21 + x1it 22 = xis21 + x1is22,

it = E( it | it > -xit 21 - x1it 22 - i, i) = E( is| is > -xis21 - x1is22 - i, i) = is.

(12.56)

410

B. Honore´ et al.

This implies that for individuals with xit 21 + x1it 22 = xis21 + x1is22, the same differencing that will eliminate the fixed effect will also eliminate the effect of sam-
ple selection. This suggests a two-step estimation procedure similar to Heckman's (1976, 1979) two-step estimator of sample selection models: first estimate (21, 22) by one of the methods mentioned earlier, and then estimate  by applying OLS to the first differences, but giving more weight to observations for which (xit - xis) 21 +
(x1it - x1is) 22 is close to zero:

n

  ^2 =

(xit - xis) (xit - xis) K

i=1 s<t

(xit - xis) 21 + (x1it - x1is) 22 hn

-1
yit yis

  n

×

(xit - xis) (xit - xis) K

i=1 s<t

(xit - xis) 21 + (x1it - x1is) 22 hn

yit yis

where K is a kernel and hn is a bandwidth which shrinks to zero as the sample size increases. Kyriazidou (1997) showed that the resulting estimator is n-consistent and asymptotically normal. Kyriazidou (2001) shows how the same approach can be used to estimate models when lagged dependent variables are included as explanatory variables in (12.52) or (12.53).
As pointed out in Honore´ and Kyriazidou (2000), the estimators proposed in Honore´ (1992) and Kyriazidou (1997) can be modified fairly trivially to cover static panel data versions of the other tobit-type models discussed in Amemiya (1985).

12.6.6 Semi-parametric Estimation of a Type-3 Tobit Model

One paper which explores the semi-parametric estimation of panel data models with
a tobit type censoring rule is Lee and Vella (2006). To present this idea first consider the cross-sectional estimator they propose.7 They consider the following model:

yi = xi + ui, zi = xit 21 + x1it 22 + vi zi = max(0, zi), si = I(zi > 0),
(xi, zi, siyi) is observed, i.i.d. across i ,

(12.57) (12.58) (12.59) (12.60)

and impose the following mean independence assumption E(ui|vi, xi, si)=E(ui|vi, si). The approach to obtain consistent estimates of  is to purge the (12.57) equation of the component related to the selection equation (12.58) error. To do this they sug-
gest a Robinson (1988) type procedure in which they regress yi - E(yi|vi, si = 1) on xi - E(xi|vi, si = 1) noting the inclusion of vi in the conditioning set eliminates

7 Semi-parametric estimation of the cross-sectional form of this model is also considered in Honore´ et al. (1997), Chen (1997) and Li and Wooldridge (2002).

12 Attrition, Selection Bias and Censored Regressions

411

the source of the selection problem.8 The model is semi-parametric in that one does not make distributional assumptions about the disturbances. Rather, one estimates the selection model (12.58) and (12.59) parameters by some appropriate semi-parametric estimator and the estimates vi as zi - x1i21 - x2i22 (if si = 1), where the 21 and 22 denote the first step semi-parametric estimates. The expectations E(yi|vi, si = 1) and E(xi|vi, si = 1) can be estimated non-parametrically. Lee and Vella (2006) argue that this approach can be extended to additional forms of endogeneity and selectivity by simply including the appropriate reduced form residual(s) in the conditioning set. This type of estimator is useful in the two wave panel context and Lee and Vella consider two models which adopt alternative strategies for dealing with dynamics in the model. The first is where the lagged dependent variable appears in the conditional mean and the model has the following form:

yit = yi,t-1y + xit  + uit , zit = xit 21 + x1it 22 + vit zit = max(0, zit ), sit = I(zit > 0), t = 1, 2,
(xi1, xi2, zi1, zi2, si1yi1, si2yi2) is observed, i.i.d. across i.

(12.61)

The outcome equation can only be estimated over the subpopulation si1 = si2 = 1, which poses a double selection problem. Thus one estimates over this subsample after subtracting off the component of the outcome equation related to the two selection residuals. The mean independence condition assumption required is E(ui2|vi1, vi2, xi2, yi1, si) = E(ui2|vi1, vi2, si) and one estimates

yi2 - E(yi2|vi1, vi2) = [yi1 - E(yi1|vi1, vi2)]y + [xi2 - E(xi2|vi1, vi2)]  + 

over the subsample corresponding to si1 = si2 = 1. Lee and Vella also consider the treatment of dynamics through the inclusion of a time invariant individual fixed effect i. The main equation is static and is of the form:

yit = xit  + i + it .

The double selection problem arises if the first-differenced outcome equation is estimated to eliminate a time-constant error which is potentially related to xit 's:

yi = xi + i, yi  yi2 - yi1, xi  xi2 - xi1, i  i2 - i1 .
The mean independence assumption required is E(i|vi1, vi2, xi, si) = E(i|vi1, vi2, si) and one estimates

yi - E(yi|vi1, vi2) = [xi - E(xi|vi1, vi2)]  + 

over the subsample corresponding to si1 = si2 = 1.
8 The same estimator for the cross-sectional case was independently suggested in Li and Wooldridge (2002).

412
12.7 Some Empirical Applications

B. Honore´ et al.

We conclude this chapter by discussing several empirical papers in which special cases of the general model in (12.1), (12.2), (12.3) and (12.4) are implemented. In each of these applications, economic agents select themselves into a certain state (e.g. "working", "union member", or "participant in a social program") and this self-selection is likely to be endogenous. In most cases fully parametric estimators are employed.

12.7.1 Attrition in Experimental Data

Hausman and Wise (1979) was one of the first studies to discuss the problem of attrition bias in experimental or panel data. Their analysis was aimed at measuring the effects of the Gary income maintenance experiment. In this experiment people were exposed to a particular income/tax policy, and the effects of this policy on monthly earnings were studied. Their sample consisted of 585 black males observed before the experiment took place (t = 1). In the second period, a treatment (i.e., an income guarantee/tax rate combination) was given to 57% of them, the other part was kept in the sample as a control group. So to analyze the effects of the experiment, Hausman and Wise were able to compare the behavior of a treatment group with that of a contemporaneously observed control group, as well as with its own pre-experimental behavior. The problem with estimating the effects from the experiment on earnings was that the second period suffered from high rates of attrition. From the experimental group 31% dropped out of the sample, while almost 41% of the individuals in the control group were not observed at t = 2. Moreover, it was thought likely that those individuals stay in the sample that benefit most from the experiment, i.e., those individuals that experience an increase in earnings due to the experiment. Obviously, such a self-selection is related to the unobservables in the equation of interest, which makes a tobit-type model appropriate.
The model considered by Hausman and Wise (1979) is fairly simple, because it is a fully-parametric two period model, where attrition (self-selection) only takes place in the second period. For each individual a treatment dummy dit is defined, which is equal to zero at t = 1 for all individuals and equal to one in period 2 for those individuals that receive treatment. The model is then given by

yit = dit  + xit  + i + it , t = 1, 2 ,

(12.62)

where  measures the effect of the treatment ("the treatment effect") and where xit contains individual-specific exogenous variables, including an intercept or a time
trend. Because (by assumption) selection takes place in the second period only, the
selection equation can be described by a univariate probit model. In particular, it is assumed that yit is observed if t = 1 and if zi2 = I(zi2 > 0) = 1, where

12 Attrition, Selection Bias and Censored Regressions

413

zi2 = xi221 + x1i222 + yi2 + vi2 .

(12.63)

All error terms are assumed to be normally distributed, with mutual independence of i, it and vi2. Note that, unlike before, (12.62) is not written as a reduced form and includes the dependent variable from the primary equation. As long as  = 0. attrition depends upon the endogenous variable yi2 and OLS applied to (12.62) is inconsistent. Because yi2 is not observed for those individuals with zi2 = 0, we substitute (12.62) into (12.62) to obtain the reduced form

zi2 = xi221 + x1i222 + (di2 + xi2 ) + (i + i2) + vi2 ,

(12.64)

or, after some appropriate redefinitions,

zi2 = wi22 + x1i222 + vi2 .

(12.65)

The probit error term vi2 = (i + i2) + vi2 will be correlated with both i and i2 as long as  = 0. Consequently, if one selects on participation in period 2 (zi2 = 1), this may not only affect inferences for period 2, but also for period 1 (unless 2 = 0).
The likelihood contributions of the model consisting of (12.62) and (12.65) are given in Hausman and Wise and are special cases of those considered in Sect. 12.6.1. If specification (12.62) contains a time effect and a treatment dummy only, OLS produces an estimate of the treatment effect of -0.06. Correcting for attrition bias and applying maximum likelihood increases this effect to -0.11. If (12.62) contains a number of additional explanatory variables, both approaches yield roughly the same answer: -0.08. Consequently, Hausman and Wise conclude that within the context of a structural model, some attrition bias seems to be present, but not enough to substantially alter the estimate of the experimental effect.
In the Hausman and Wise model, it is assumed that selection into the experiment is random. In many other cases, however, individuals are allowed to select themselves into the experiment. Even in the absence of attrition this may lead to a selection bias problem. See Heckman (2001) or Wooldridge (2002, Chap. 18) for more discussion.

12.7.2 Real Wages Over the Business Cycle
Keynes (1936) believed that the movement of real wages over the business cycle was countercyclical. A large number of empirical studies on this issue, based on macro as well as micro data, have lead to a diversity of results. In an attempt to reconcile these results, Keane et al. (1988) consider the question to what extent aggregation bias (or selection bias) is able to explain the differences. Aggregation bias arises if people going in and out of the labor force are not random. In that case, the average wage changes over time due to a changing composition of the work force, even though real wage levels are unaffected. If, for example, low-wage industries are more cyclically sensitive, a countercyclical bias in the conclusion is expected.

414

B. Honore´ et al.

Keane et al. (1988) use panel data from the National Longitudinal Survey of

Young Men (NLS) over the period 1966­1981. The use of micro data has the ad-

vantage that a large part of the individual heterogeneity is observed. The model is

given by

yit = xit 1 + 2urt + i + it ,

(12.66)

where yit is the log of the (potentially unobserved) real hourly wage rate, and urt denotes the national unemployment rate. The vector xit contains individual-specific variables (education, experience, race, etc.), as well as a time trend. The parameter 2 is the main parameter of interest: a positive 2 corresponds to a countercyclical behavior in the wage, while a negative value indicates procyclical behavior. To

correct for the possibility of aggregation bias (selection bias), there is an additional

equation explaining employment, given by

zit = xit 21 + x1,it 22 + i + it .

(12.67)

An individual is employed (and a wage rate is observed) if zit = I(zit > 0) = 1. Thus

we have

yit = yit · I(zit > 0) .

(12.68)

Aggregation bias is procyclical if the covariance between the error terms in (12.66) and (12.67) is negative. In that case, individuals with relatively high wages are more likely to leave the labor market in case of increasing employment.
Keane et al. (1988) estimate two different specification of the model: one excluding individual-specific variables in (12.66) and (12.67) and one including a small number of such variables. In addition, four different estimation strategies are used: OLS without any corrections, maximum likelihood without individual effects in (12.66) and (12.67), with random effects (along the lines discussed in Sect. 12.6.1) and with fixed effects. Where needed, normality of the error components is assumed. The OLS estimate for 2 of -0.0071 shows evidence of a procyclical behavior in the wage. The addition of the extra regressors results in an estimate of -0.0096, implying that the failure to control for observed heterogeneity leads to a countercyclical bias. The estimates from the fixed effects model show insignificant unemployment rate coefficients, implying an acyclic wage. The correlation coefficient between it and vit is estimated to be -0.222. This result implies that the OLS unemployment coefficient is procyclically biased. Finally, if a random effects specification is estimated, the unemployment rate coefficients are negative and significant in both specifications. For the specification including observed heterogeneity the unemployment rate coefficient of -0.0066 is still considerably below the OLS effect of -0.0096. This indicates that a procyclical bias is still present, but weaker than was indicated by the fixed effects model. The random effects results indicate a negative correlation of the transitory errors (the correlation coefficient between it and vit is -0.252), but a positive correlation of the individual effects i and i (with an estimated correlation coefficient of 0.436). The resulting composite correlation is virtually zero.

12 Attrition, Selection Bias and Censored Regressions

415

The general conclusion from the results is that the failure to account for selection effects, biases the behavior of the real wage in a procyclical direction. Apparently, high-wage workers are more likely to become unemployed in a downturn.

12.7.3 Unions and Wages

Empirical studies of the union impact on wages typically attempt to estimate how observationally equivalent workers' wages differ in union and non-union employment. This is known as the "union effect". However, as the unobserved factors that influence the sorting into union and non-union employment may also affect wages it is necessary to incorporate how the unobserved heterogeneity responsible for the union/non-union decision is rewarded in the two sectors. Panel data studies of the union effect generally control for this endogeneity through fixed effects or alternative instrumental variables estimators. These procedures are inflexible in their treatment of worker heterogeneity as they generally assume the endogeneity is individual specific and fixed. A preferable approach, adopted by Vella and Verbeek (1998), is based on decomposing the endogeneity underlying union status into an individual specific component and an individual/time specific effect.
Vella and Verbeek (1998) consider the following equations explaining (log) union wages y1,it and (log) non-union wages y0,it ,

y j,it = x j,it  j +  j,i +  j,it , j = 0, 1 ,

(12.69)

where x j,it is a vector of characteristics, including time dummies. For a given individual, we observe his wage in the union or the non-union sector. Selection into the union sector is described by a random effects probit model of the form

zit = xit 21 + x1,it 22 + zi,t-1 + i + it zit = I(zit > 0).

(12.70)

This is a dynamic model, implying that the probability of working in the union sector is affected by the worker's status in the previous year.
The random components are assumed to be i.i.d. drawings from a multivariate normal distribution, where the effects from the different equations are potentially correlated. The endogeneity of union status (zit ) is driven by the correlations between the components in (12.69) and (12.70). The wage equation is estimated by OLS, fixed effects and by the two-step method described in Sect. 12.6.1, imposing that  j is identical across sectors (except for the intercept term). The differences in the intercept capture the union effect. The data, taken from the National Longitudinal Survey (Youth Sample), comprise a sample of full-time working males in the USA, who have completed their schooling by 1980 and who are then followed over the period 1980­1987. This provides a balanced panel of 545 individuals.

416

B. Honore´ et al.

The estimates for the union effect vary widely across the different methods. The OLS estimate is 0.146, corresponding to a union effect of about 15%. After correcting for the endogeneity of union status, the estimated union effect increases to 0.214 or about 21%. However, the random effects contribute significantly, making the union premium highly variable across individuals. Interestingly, the empirical results indicate that the random effects are valued differently by sector. That is, it is inappropriate to assume that the random components in (12.69) are identical for both sectors ( j = 0 and j = 1). This is consistent with the idea that workers have sector-specific skills.

References
Amemiya, T. (1984): Tobit Models: A Survey, Journal of Econometrics, 24, 3­61. Amemiya, T. (1985): Advanced Econometrics, Harvard University Press, Cambridge, MA. Arellano, M., O. Bover and J.M. Labeaga (1999): Autoregressive Models with Sample Selectivity
for Panel Data, in C. Hsiao, K. Lahiri, L. F. Lee, and H. Pesaran, eds., Analysis of Panels and Limited Dependent Variable Models, Cambridge University Press, Cambridge, MA, 23­48. Charlier, E., B. Melenberg and A. van Soest (1995): A Smoothed Maximum Score Estimator for the Binary Choice Panel Data Model and an Application to Labour Force Participation, Statistica Neerlandica, 49, 324­342. Charlier, E., B. Melenberg and A. van Soest (2000): Estimation of a Censored Regression Panel Data Model Using Conditional Moments Restrictions Efficiently, Journal of Econometrics, 95, 25­56. Chen, S. (1997): Semi-Parametric Estimation of the Type 3 Tobit Model, Journal of Econometrics, 80, 1­34. Fernandez-Val, I. and F. Vella (2005): Fixed Effects Estimation of Panel Models with Selection Bias, unpublished manuscript, MIT. Gourieroux, C., A. Monfort, E. Renault and A. Trognon (1987): Generalized Residuals, Journal of Econometrics, 34, 5­32. Greene, W.H. (2004): Fixed Effects and Bias Due to the Incidental Parameters Problem in the Tobit Model, Econometric Reviews, 23, 125­147. Gronau, R. (1974): Wage Comparisons ­ A Selectivity Bias, Journal of Political Economy, 82, 1119­1143. Hahn, J. and W.K. Newey (2004): Jacknife and Analytical Bias Reduction for Non-Linear Panel Models, Econometrica, 72, 1295­1319. Hausman, J.A. and D.A. Wise (1979): Attrition Bias in Experimental and Panel Data: The Gary Income Maintenance Experiment, Econometrica, 47, 455­473. Heckman, J.J. (1974): Shadow Prices, Market Wages and Labor Supply, Econometrica, 42, 679­694. Heckman, J.J. (1976): The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple estimator for Such Models, Annals of Economic and Social Measurement, 15, 475­492. Heckman, J.J. (1979): Sample Selection Bias as a Specification Error, Econometrica, 47, 153­161. Heckman, J.J. (1981): The Incidental Parameters Problem and the Problem of Initial Conditions in Estimating a Discrete-Time Discrete-Data Stochastic Process, in C.F. Manski and D. McFadden, eds., Structural Analysis of Discrete Data with Econometric Applications, MIT Press, Cambridge, MA, 179­195. Heckman, J.J. (2001): Micro Data, Heterogeneity, and the Evaluation of Public Policy: Nobel Lecture, Journal of Political Economy, 109, 673­748.

12 Attrition, Selection Bias and Censored Regressions

417

Honore´, B.E. (1992): Trimmed LAD and Least Squares Estimation of Truncated and Censored Regression Models with Fixed Effects, Econometrica, 60, 533­565.
Honore´, B.E. (1993): Orthogonality Conditions for Tobit Models with Fixed Effects and Lagged Dependent Variables, Journal of Econometrics, 59, 35­61.
Honore´, B.E. and L. Hu (2004): Estimation of Cross-Sectional and Panel Data Censored Regression Models with Endogeneity, Journal of Econometrics,122, 293­316.
Honore´, B.E. and E. Kyriazidou (2000): Estimation of Tobit­Type Models with Individual Specific Effects, Econometric Reviews, 19, 341­366.
Honore´, B., E. Kyriazidou and C. Udry (1997): Estimation of Type 3 Tobit Models Using Symmetric Trimming and Pairwise Comparisons, Journal of Econometrics, 76, 107­128.
Hu, L. (2002): Estimation of a Censored Dynamic Panel Data Model, Econometrica, 70, 2499­2517.
Keane, M., R. Moffitt and D. Runkle (1988): Real Wages Over the Business Cycle: Estimating the Impact of Heterogeneity with Micro Data, Journal of Political Economy, 96, 1232­1266.
Keynes, J.M. (1936): The General Theory of Employment, Interest and Money, MacMillan, London.
Kyriazidou, E. (1995): Essays in Estimation and Testing of Econometric Models, Northwestern University, Ph.D. dissertation.
Kyriazidou, E. (1997): Estimation of a Panel Data Sample Selection Model, Econometrica, 65, 1335­1364.
Kyriazidou, E. (2001): Estimation of Dynamic Panel Data Sample Selection Models, Review of Economic Studies, 68, 543­572.
Lancaster, T. (2000): The Incidental Parameter Problem Since 1948, Journal of Econometrics, 95, 391­413.
Lee, M.J. and F. Vella (2006): A Semi-Parametric Estimator for Censored Selection Models with Endogeneity, Journal of Econometrics, 130, 235­252.
Li, Q. and J.M. Wooldridge (2002): Semi-Parametric Estimation of Partially Linear Models for Dependent Data with Generated Regressors, Econometric Theory, 18, 625­645.
Manski, C. (1987): Semi-Parametric Analysis of Random Effects Linear Models from Binary Panel Data, Econometrica, 55, 357­362.
Newey, W.K. (1984): A Method of Moments Interpretation of Sequential Estimators, Economics Letters, 14, 201­206.
Neyman, J. and E. Scott (1948): Consistent Estimates Based on Partially Consistent Observations, Econometrica, 16, 1­32.
Nijman, T.E. and M. Verbeek (1992): Nonresponse in Panel Data: The Impact on Estimates of a Life Cycle Consumption Function, Journal of Applied Econometrics, 7, 243­257.
Powell, J.L. (1987): Semi-Parametric Estimation of Bivariate Latent Models, Working Paper No. 8704, Social Systems Research Institute, University of Wisconsin­Madison.
Quenouille, M. (1956): Notes on Bias in Estimation, Biometrika, 43, 353­360. Rasch, G. (1960): Probabilistic Models for Some Intelligence and Attainment Tests, Denmarks
Pædagogiske Institut, Copenhagen. Rasch, G. (1961): On the General Laws and the Meaning of Measurement in Psychology, Pro-
ceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Vol. 4, University of California Press, Berkeley and Los Angeles. Ridder, G. (1990): Attrition in Multi-Wave Panel Data, in J. Hartog, G. Ridder and J. Theeuwes, eds., Panel Data and Labor Market Studies, Elsevier, North Holland. Rivers, D. and Q. Vuong (1988): Limited Information Estimators and Exogeneity Tests for Simultaneous Probit Models, Journal of Econometrics, 39, 347­366. Robinson, P. (1988), Root-N Consistent Semi-Parametric Regression, Econometrica, 56, 931­954. Smith, R.J. and R. Blundell (1986): An Exogeneity Test for a Simultaneous Equation Tobit Model with an Application to Labor Supply, Econometrica, 54, 679­685. Tukey, J. (1958): Bias and Confidence in Not-Quite Large Samples, Annals of Mathematical Statistics, 29, 614.

418

B. Honore´ et al.

Vella, F. (1993): A Simple Estimator for Simultaneous Models with Censored Endogenous Regressors, International Economic Review, 34, 441­457.
Vella, F. (1998): Estimating Models with Sample Selection Bias: A Survey, Journal of Human Resources, 33, 127­169.
Vella, F. and M. Verbeek (1998): Whose Wages Do Unions Raise? A Dynamic Model of Unionism and Wage Rate Determination for Young Men, Journal of Applied Econometrics, 13, 163­189.
Vella, F. and M. Verbeek (1999): Two-Step Estimation of Panel Data Models with Censored Endogenous Regressors and Selection Bias, Journal of Econometrics, 90, 239­263.
Verbeek, M. and T.E. Nijman (1996): Incomplete Panels and Selection Bias, in L. Matyas and P. Sevestre, eds., The Econometrics of Panel Data. Handbook of the Theory with Applications, 2nd edition, Kluwer Academic Publishers, Dordrecht, 449­490.
Waterman, R., B. Lindsay, H. Li and B. Li (2000): Projected Score Methods for Nuisance Parameters: Asymptotics and Neyman-Scott Problems, unpublished manuscript.
Wooldridge, J.M. (1995): Selection Corrections for Panel Data Models Under Conditional Mean Independence Assumptions, Journal of Econometrics, 68, 115­132.
Wooldridge, J.M. (2002): Econometric Analysis of Cross-Section and Panel Data, MIT Press, Cambridge, MA.
Wooldridge, J.M. (2005): Simple Solutions to the Initial Conditions Problem in Dynamic, Nonlinear Panel Data Models with Unobserved Heterogeneity, Journal of Applied Econometrics, 20, 39­54.
Woutersen, T.M. (2002): Robustness Against Incidental Parameters, unpublished manuscript, Johns Hopkins University.

Chapter 13
Simulation Techniques for Panels: Efficient Importance Sampling
Roman Liesenfeld and Jean-Franc¸ois Richard

13.1 Introduction
The recent years have witnessed an explosive growth in the use of simulation techniques in econometrics made possible by impressive advances in computing power. See e.g., the special issue of the Journal of Applied Econometrics Brown et al. (1993) on "Econometric Inference using Simulation Techniques". See also Stern (1997) for a survey of simulation-based estimation with special emphasis on multivariate probit models. Among the methods surveyed by Stern, one of particular interest is the popular GHK simulator developed by Geweke (1991), Hajivassiliou (1990) and Keane (1994) (see also Geweke et al. (1994) and Geweke et al. (1997)). Bo¨rsch­Supan and Hajivassiliou (1993) compare the performance of an acceptance-rejection method proposed by Stern (1992) with that of the GHK technique. See Bo¨rsch­Supan et al., (1990) for an empirical application of the GHK to a multiperiod-multinomial probit model of living arrangements for the elderly. Greene (2004) compares GMM estimations ­ as proposed by Bertschek and Lechner (1998) ­ with simulated maximum likelihood for panel probit models allowing for unobserved heterogeneity along the time dimension, with an application to product innovation activity of German manufacturing firms (initially studied by Bertschek (1995)). It is important to note that the simulation techniques proposed in these papers are typically of low dimensions (either time or decision space) but cannot easily deal with the much higher dimensions required to handle random effects across individual units. This chapter is not aimed at providing a complete survey of the relevant literature. Instead we will draw upon our own experience with the

Roman Liesenfeld Department of Economics, Christian-Albrechts-Universita¨t Kiel, Olshausenstr. 40-60, 24118 Kiel, Germany, e-mail: liesenfeld@stat-econ.uni-kiel.de
Jean-Franc¸ois Richard Department of Economics, University of Pittsburgh, Pittsburgh, PA 15260, USA, e-mail: fantin@pitt.edu

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

419

c Springer-Verlag Berlin Heidelberg 2008

420

R. Liesenfeld and J.-F. Richard

use of Monte-Carlo (hereafter MC) simulation techniques in classical and Bayesian econometrics and will attempt to provide readers with an "helicopter tour"- of some of their key features.
Moreover, we will largely focus our attention on a powerful high dimensional simulation technique known as Efficient Importance Sampling (hereafter EIS) which, as we shall discuss, is particularly well adapted to handling unobserved heterogeneity in large panel data sets. (Another important simulation technique known as Gibbs sampling is presented in Chap. 15 and will, therefore, not be discussed here.) In particular, the full potential of EIS will be highlighted in the context of an empirical application to a panel logit model with random effects along both dimensions (time and individual). As we shall discuss further below, the EIS method allows for efficient evaluation of likelihood functions for non-linear panel data models in the presence of unobserved heterogeneity along both dimensions, where the interaction between individual and time random effects prevent factorizations into problems of lower dimensionality. This is an important extension for conceptional as well as practical reasons. By modeling unobserved heterogeneity across individual units in the form of random effects, we can identify the impact of time invariant regressors. On the practical side this possibility enables researchers to make informed choices among fixed and random effects formulations of their model, instead of being forced to select fixed effects by default.
The most important usage of MC-EIS techniques in modern econometrics is that of a powerful numerical technique for the evaluation of high­dimensional (functional) integrals which are analytically intractable. Therefore, in sharp contrast with recent literature, we will insist upon separate treatment of the numerical and statistical properties of "simulation estimators". In a fundamental sense, MC simulation estimators ought to be treated for what they are, i.e., numerical approximations to a set of statistics of interest (whose statistical properties are to be separately discussed and might themselves have to be numerically approximated).
This chapter is organized as follows: Random number generation is discussed in Sect. 13.2; Importance Sampling and EIS is introduced in Sect. 13.3; Simulation based inference techniques are surveyed in Sect. 13.4 with reference to panel data models; their numerical properties are analyzed in Sect. 13.5; An empirical application of EIS is presented in Sect. 13.6; Sect. 13.7 concludes; A technical appendix details the implementation of EIS for large panel data sets up to a degree of details allowing for customization to one's specific application.
As for notation, we will use matched pairs of capital and lower case letters to respectively denote random variables and realizations thereof. A superscript  denotes random draws. A superscript ­ paired with a subscript S denotes arithmetic sample means over S random draws.

13.2 Pseudorandom Number Generation
The cornerstone of any simulation based inference technique lies in the generation by computer of sequences of "pseudorandom" numbers. Following Devroye (1986), we will assume the availability of a uniform [0, 1] (linear congruential)

13 Simulation Techniques for Panels

421

pseudorandom number generator. See Fishman and Moore (1982, 1986) for a statistical evaluation of congruential generators. Crude generators can be improved upon by "(re)shuffling", an option which is available in program libraries such as IMSL (see the 1991 IMSL User's Manual Stat/Library). See Press et al. (1986) for a portable reshuffling subprogram in FORTRAN or C. See also Fishman (1996) for a broad in-depth discussion of Monte Carlo simulation.
Sequences of pseudorandom numbers are in fact fully deterministic and uniquely characterized by their length and initial "seed". This fundamental property enables us to reproduce at will any sequence of pseudorandom numbers. It follows that regeneration of large sequences of pseudorandom numbers provides an efficient alternative to their storage on disk since (re)generation typically is faster than swapping. Also regeneration is the key to the technique of Common Random Numbers (CRN's) which plays an essential role in the design of simulation based inference techniques and is discussed in Sect. 13.3.3 below.
Uniform pseudorandom numbers can be transformed into pseudorandom draws from a broad range of distributions following techniques discussed, e.g. in Devroye (1986), the most important of which are surveyed here.

13.2.1 Univariate Distributions

Let X denote a univariate random variable with distribution function F(x |  ), where  is a vector of preassigned parameters. For the ease of presentation we will assume that X is a continuous random variable. Extensions to discrete random variables are straightforward. At a high level of generality pseudorandom draws of X are obtained by transformation of a sequence of independent uniform [0, 1] pseudorandom draws U = (U1, . . . ,UK)

X =  (U;  ) ,

(13.1)

where K is determined by an appropriate stopping rule and may itself be random. The most commonly used (transformation) techniques are inversion, rejection (or acceptance) and decomposition which are briefly described next. Additional details and extensions, as well as other techniques can be found in Devroye (1986).

13.2.1.1 Inversion

The random variable U = F(X |  ) is uniformly distributed on [0, 1]. Let F-1(·;  ) denote the inverse of F for any given  . It follows that the random variable

X = F-1 (U;  )

(13.2)

when U is uniform on [0, 1] has F for its distribution function. The technique of inversion consists in generating sequences of uniform [0, 1] pseudorandom draws {Ui} and transforming them into sequences {Xi} by means of (13.2).

422

R. Liesenfeld and J.-F. Richard

Example 13.1. Let X be a Weibull random variable with parameters  > 0 and  > 0. Its distribution function is

F (x | ,  ) = 1 - exp (- x ) .

(13.3)

The transformation (13.2) is then given by

1

1/

X = -  ln (1 -U)

(13.4)

where V = 1 -U is uniform on [0, 1].

The inversion technique is easy to implement as long as an analytical expression is available for F-1. However, it is generally inefficient in terms of computing time relatively to more performant generation techniques combining fast rejection and decomposition as described below. This is particularly relevant if F has to be numerically inverted, in which case large scale simulation can become prohibitively time consuming. In such cases we recommend initial tabulation of the inverse for the relevant range of parameter values and subsequent numerical interpolation within the core simulation algorithm. This is quite relevant since, as we shall discuss further in Sect. 13.3.3 below, inversion though inefficient may be required in order to apply the critical concept of Common Random Numbers.

13.2.1.2 Rejection (Acceptance)

Assume X has a density function f (x |  ) which is not easily amenable to "direct" simulation (or for which direct simulation is relatively inefficient). For example,

Bayesians and also users of Gibbs sampling techniques (as described in Chap. 15)

frequently face situations where f is known only up to a proportionality factor.

In such cases f (x |  ) is given in the form of a "density kernel" (x |  ) whose

"integrating constant"


k ( ) =  (x |  ) dx
-

(13.5)

requires numerical evaluation. The rejection technique which does not require

knowledge of k( ) is well adapted to such circumstances. It requires the construc-

tion of an "envelope" density function (x |  ) for which a generator is available

and which is such that

sup
x

 (x |  )  (x |  )

= c ( ) < 

(13.6)

for all relevant  's. If, in particular, the support of  is infinite, condition (13.6) requires that the tails of  be not "thinner" than those of . It follows that normal density functions with their "thin" tails cannot be used as envelope densities for

distributions with thicker tails such as student-t distributions, unless appropriate truncation of the support is deemed acceptable.

13 Simulation Techniques for Panels

423

Under condition (13.6) the rejection technique runs as follows: to generate a random draw of X,
(a) generate a uniform U on [0, 1]; (b) generate Z with density function (z |  ); (c) If c( ) · (Z |  ) ·U > (Z |  ), then "reject" Z and return to step (a); otherwise (d) X = Z (i.e., Z is "accepted"). It is straightforward to verify that

Pr (Z  a | Z is accepted) = F (a |  )

(13.7)

i.e., that X has the required distribution. The unconditional probability of acceptance is given by

Pr (Z is accepted) = k ( ) · [c ( )]-1  1 .

(13.8)

Devroye (1986) ­ see also Geweke (1994) ­ proposes to select optimized envelope density functions within a parametric class M = {m(x | ) ;   A} by solving the following optimization problem

 ( ) = arg min sup [ln  (x |  ) - ln m (x | )]
A x

(13.9)

and choosing (x |  ) = m(x | ( )). The most performant generation algorithms for common distributions often use rejection in combination with the decomposition technique which is described next. Compound acceptance rates in excess of 0.95 are not uncommon. See Devroye, (1986) for examples and details.

We conclude this brief discussion of the rejection technique with two additional
comments. First, the application of the rejection principle greatly simplifies if the random variable X can be obtained by means of a transformation X = (X;  ) of a "standardized" random variable X with a density kernel  which does not depend on  . Under such circumstances we only have to construct a single optimized envelope function  for , notwithstanding the fact that the construction of Common Random Numbers simplifies accordingly.
Second, the rejection algorithm as described above requires the evaluation of (z |  ) for all draws, which can be computationally demanding. A performant refinement of rejection is provided by the "squeeze" principle whose object is to squeeze (z |  ) between a pair of functions a(z |  ) and b(z |  ) which are (much) quicker to evaluate than  itself (being, for example, piecewise linear). The squeeze version of step (c) of the rejection algorithm runs as follows:

(c.1) c( ) · (Z |  ) ·U < a(Z |  )  "quick" acceptance of Z; else, (c.2) c( ) · (Z |  ) ·U > b(Z |  )  "quick" rejection of Z; else, (c.3) run step (c) of the rejection algorithm.

Elaborate implementations of the squeeze principle may require evaluation of  for less than 10% of the actual draws.

424
13.2.1.3 Decomposition

R. Liesenfeld and J.-F. Richard

A powerful technique for the construction of highly efficient generators consists in the "decomposition" of a density kernel (x |  ) into a mixture of the form

k-1
 (x |  ) =  pi ( ) qi (x |  ) + pk ( ) qk (x |  ) , i=1

(13.10)

where the qi's for i : 1  k - 1 correspond to distributions that are obtained by simple transformations of uniforms on [0, 1], such as sums or extremums of pairs of
uniforms. The qi's often have non overlapping supports allowing for separate efficient treatments of the central and tail areas of . The "remainder" density kernel qk typically requires more demanding generation procedures but efficient decompositions result in low values for the remainder probability pk( ). According to formula (13.10) a draw of X proceeds in two steps:

(a) A discrete random indicator I  {1, 2, . . . , k} is drawn according to the probabilities {pi( )};
(b) Conditionally on I = i, X is drawn from the distribution with kernel qi(x |  ).
A continuous version of formula (13.10) is given by

 (x |  ) = p (y |  ) q (x | y,  ) dy

(13.11)

and can be used for such distributions as the student-t distribution (which is obtained by a continuous mixture of a Normal distribution for x conditionally on y =  -2 and a gamma distribution for  -1).

13.2.2 Multivariate Distributions
The generation of a multivariate random variable X is based upon recursive factorizations of its density kernel (x |  ) into lower dimensional density kernels. Two types of factorizations are discussed next: sequential (or recursive) factorizations and Gibbs factorizations. Factorizations of different types often are combined together for the purpose of constructing a complete factorization of a multivariate distribution into univariate components to which the techniques described in Sect. 13.2.1 can be applied.
13.2.2.1 Sequential Factorizations
Sequential factorizations are typically used when the components of X = (X1, . . . , X p) are naturally (pre)-ordered, such as in the context of time series problems. The density kernel of X is then factorized as

13 Simulation Techniques for Panels

425

p
  x1, . . . , xp |  = i(xi | x(i-1),  ) , i=1

(13.12)

where x(0) =  and x(i-1) = (x1, . . . , xi-1) for i : 2  p. For the ease of notation preassigned (fixed) initial conditions are included in  and randomized initial conditions in X itself.
Let x~ denote an arbitrary random draw of X. It is produced by sequential draws of the Xi's conditionally on  and X(i-1) = x~(i-1) according to the density kernels i(xi | x~(i-1),  ) for i : 1  p. In the context of Gourieroux and Monfort (1994), to which we shall refer further below, such simulations are called "path simulations".
Note that in the context of time series, models are generally formulated in sequential form and the joint density kernel (x |  ) is rarely explicitely given. In other contexts the ordering of the components of X may be arbitrary and its actual choice
based upon considerations of numerical convenience. For example, random draws
of a multivariate Normal density are typically based upon a Cholesky decomposi-
tion of its covariance matrix for whatever ordering of its components is deemed to
be convenient.
As discussed further below, conditional independence assumptions play a central
role in the formulation of panel data models and can produce significant simplifica-
tions of formula (13.12). For example, in the context of the application discussed in Sect. 13.6 below, x is partitioned into x = ( ,  ) where  = (1, . . . , T ) denotes time random effects and  = (1, . . . , N) individual random effects in a non­linear panel model. Efficient numerical integration of x conditional on  and the actual sample y will be based upon a factorization of the form

T

N

  ,  |  , y   = t (t |  (t-1), ,  ) · i (i |  ,  ) .

t=1

i=1

(13.13)

Along similar lines, the concept of "exchangeable" distribution which is familiar
to Bayesians assumes that the xi's are identically independently distributed with density kernel q, conditionally on an unobserved common random component x0 with density kernel p. Therefore, it is based upon a factorization of the form

P
 (x,  ) = p (x0 |  )  q (xi | x0,  ) dx0 . i=1

(13.14)

13.2.2.2 Gibbs Sampling
Gibbs sampling is extensively analyzed in Chap. 15 and is only briefly discussed here for the sake of completeness and comparison. Gibbs sampling is based upon the observation that a density kernel for the distribution of an arbitrary component Xi conditional on  and on all other components of X is trivially obtained by regrouping all terms depending upon xi, in the expression of (x |  ). Depending upon the circumstances, such density kernels can either characterize

426

R. Liesenfeld and J.-F. Richard

known distributions for which generators immediately are available or, in any event, are amenable to simulation by application of the principles described in Sect. 13.2.1.
Gibbs sampling is then based upon a Markov scheme as described in Chap. 15. It offers the advantage that it is easily implementable for a broad range of applications. It can, however, be inefficient as a large number of auxiliary draws are required in order to produce draws from the actual distribution of X |  .

13.3 Importance Sampling
While MC techniques have long been used to simulate the finite sample properties of a broad range of ("classical") statistics, one of the most important numerical development in recent years has been their increasing usage as a numerical method for evaluating (large-dimensional) analytically intractable integrals. It is hardly surprising that the initial impetus and many important developments on that front came from Bayesian statisticians and econometricans who critically depend upon the availability of efficient numerical procedures in order to evaluate posterior moments and other quantities of interest to them. See the pioneering contribution of Kloek and van Dijk (1978), or Geweke (1989, 1994) for more recent developments. The presentation which follows is based upon Richard and Zhang (2007). See also Liesenfeld and Richard (2003a) for a non technical presentation of Importance Sampling and Liesenfeld and Richard (2003b,c) for applications of EIS within the context of stochastic volatility models ­ including an explicit comparison with Gibbs sampling.

13.3.1 General Principle

Assume one has to evaluate a functional integral of the form

G () = g (x, ) · p (x | ) dx ,
S()

(13.15)

where g is a function which is integrable w.r.t. a conditional density p(x | ) with support S(). The actual composition of x and  largely is problem dependent. Let  , y and  denote parameters, data and latent (unobservable) variables, respectively. Important applications requiring the evaluation of integrals of the form given
is (13.15) are: (i) The Bayesian evaluation of posterior "odds" and/or moments for which  = y and x = ( ,  ); (ii) The classical evaluation of "marginalized" likelihood functions for which  = ( , y) and x =  ; (iii) The classical evaluation of Methods of Moments estimators for which  =  and x = (y,  ).

13 Simulation Techniques for Panels

427

MC estimator of G() in formula 13.15 is given by

 G¯S;p ()

=

1 S

S
g (x~i,
i=1

)

,

(13.16)

where the x~'s are i.i.d. draws from p(x | ) and S denotes the number of draws. The replacement of p by an alternative simulator with density  calls for the following reformulation of G()

G () = g (x, ) ·  (x, ) ·  (x | ) dx ,

(13.17)

where



(x,

)

=

p 

(x (x

| |

) )

.

(13.18)

Note that the expectation of (X, ) on  equals one. The corresponding MC estimate of G(), known as an "importance sampling" estimate, is given by

 G¯ S;

()

=

1 S

S

i=1

(x~i,

) · g (x~i,

)

,

(13.19)

where the x~i's now are i.i.d. draws from (x | ). The MC sampling variance of G¯S; () as an estimate of G() is given by

V

G¯ S; ()

1 =
S

EX

g2 (X, ) · 2 (X, ) |  - G2 ()

·

(13.20)

(x | ) is the actual importance sampling density used in the construction of the EIS­MC estimates of the relevant integrals. (As we discuss in Sect. 13.4.2 below, p is instrumental in the construction of ).
We will assume here that p is a genuine density function in that it integrates to one on its support S(). That assumption is routinely satisfied for classical inference procedures where p(x | ) represents a sampling distribution. In contrast it is frequently violated in Bayesian applications where p(x | ) represents a posterior density obtained by application of Bayes theorem which, therefore, often takes the form of a density kernel whose integrating constant is unknown and is itself to be numerically evaluated. Most quantities of interest are then ratios of integrals with the integral of p itself in the denominator.
In most applications p(x | ) will be a direct byproduct of the stochastic (sequential) specification of the model under consideration. We shall refer to it as an "initial sampler". For example, in Dynamic Latent Variables models, p typically corresponds to the marginal density of the latent process. See Hendry and Richard (1992) for details. Another example of initial sampler will be presented in sect. 13.6 below. Note that formulae (13.19) and (13.20) cover as special case that of the initial sampler with the simplification that if   p, then   1.

428

R. Liesenfeld and J.-F. Richard

Theoretically, an optimal choice  for  would be one such that the product g· in formula (13.17) does not depend on x. That is to say, if there exists an operational sampler (x | ) and a "remainder" function g() such that

g (x, ) · p (x | ) =  (x | ) · g ()

(13.21)

then G()  g() and, furthermore, V G¯S; () = 0 in which case a single draw from  would produce the exact result. Note that when formula (13.21) holds,  corresponds to the posterior density of x given . Except for the simplest models  is generally not amenable to MC simulations. Furthermore, in high-dimensional problems, it is often the case that the MC sampling variance V G¯S;p() is so large that accurate MC estimation of G() using draws from the initial sampler p would require prohibitively large numbers of draws. See, in particular, the comments in McFadden (1989) as to the impracticability of simulated ML estimation in the context of discrete response models, or Danielsson and Richard (1993) for a striking example of the inaccuracy of naive MC estimates in the context of a stochastic volatility model. A number of "acceleration" techniques are available whereby the numerical accuracy of MC estimates can be enhanced, often at negligible increases in the cost of computations. See, e.g., Hendry (1984), Geweke (1988, 1994) or Davidson and McKinnon (1992). Nevertheless, these techniques constitute at best a partial remedy to the initial selection of an "inefficient" MC sampler and, under most circumstances, the only solution consists in the replacement of the initial sampler p by a more efficient importance sampler , i.e. one which is such that G() can be accurately estimated with manageable number of draws. The literature on importance sampling provides useful examples of efficient samplers for specific classes of models. See Liesenfeld and Richard (2003a) for references. Until recently, however, there did not exist a generic algorithm to construct efficient importance samplers for (very) high-dimensional integrals of the type associated with high­frequency dynamic latent variable models (such as stochastic volatility models) and/or large panel models. Generalizing earlier results obtained by Danielsson and Richard (1993), Richard and Zhang (2007) proposed a generic least squares algorithm for the automated construction of Efficient Importance Samplers (EIS).

13.3.2 Efficient Importance Sampling
In this section we outline the general principle underlying EIS, refering the reader to Richard and Zhang (2007) for additional details. The specific implementation of EIS to panel data with unobserved heterogeneity will be presented in Sect. (13.6) below. We now assume that the function g(x, ) in (13.15) is strictly positive on the support S(), which, for example, is the case in all applications where G() represents a likelihood function marginalized w.r.t. latent variables.

13 Simulation Techniques for Panels

429

The construction of an EIS starts with the selection of a parametric class of samplers. Let M = {m(x | );   A} denote such a class. Typically, M would include parametric extensions of the initial sampler p designed to provide flexible approximations to the product g · p in (13.15), that is to say better approximations than p itself to the implicit posterior density (x | ) in (13.21). Following (13.20), the selection of an "optimal" sampler within M is tantamount to solving the following
optimization problem:

 () = arg min [V (; )] , with
 A

(13.22)

V (; ) =

 2 (x; m (x |

) )

dx

-

G2

()

 (x; ) = g (x, ) · p (x | )

The variance V (; ) may be rewritten as

(13.23) (13.24)

V (, ) = G () · h d2 (x; , ) ·  (x; ) dx

(13.25)

with

d (x; , ) = ln

 (x; ) G () · m (x | )

(13.26)

h(c)

=


ec

+

e-c

-

2

.

(13.27)

This EIS optimization problem can be significantly simplified further. First, we can replace m(x | ) in (13.26) by a density kernel k(x; ). Let () denote the integrating constant of that kernel, so that

m (x | ) = -1 () · k (x; ) .

(13.28)

Without loss of generality we then rewrite d(x; , ) as follows

d (x; , ) = [ln  (x; ) -  - ln k (x; )] ,

(13.29)

where  = ln [G()/()] does not depend on x and is treated as an (additional) intercept in the optimization problem. Next, we note that if m(x | ) belongs to the exponential family of distributions, then there exists an auxiliary reparametrisation such that ln k(x; ) is linear in , say

ln k (x; ) = c (x) ·  .

(13.30)

Finally, an efficient sampler will obviously be one such that k(x; ) closely mimics  (x; ), in which d(x; , ) is expected to be close to zero on average. Heuristically, this justifies replacing h(c) in (13.27) by c, its leading term in a Taylor series expansion around zero. This amounts to approximating V (; ) in (13.22) by

430

R. Liesenfeld and J.-F. Richard

Q (; ) = d2 (x; , ) · g (x, ) · p (x | ) dx

(13.31)

resulting in a simpler Generalized Least Squares (GLS) optimization problem. Let ^ () denote the GLS solution to that problem. A more formal justification for the replacement of V by Q can be found in Richard and Zhang (2007) and follows from the inequality

V (^ () ; )  V ( () ; )  h [Q (^ () ; )] .

(13.32)

An operational EIS implementation consists of solving first the simpler GLS problem and computing the two extreme bounds in (13.32) in order to assess whether additional efficiency gains would justify solving the computationally more demanding optimization problem in (13.25). Among all applications of EIS we have run over the last few years, including the one discussed in Sect. 13.6 below, there has never been one where the computation of () would have been justified. In practice, both optimization problems will be approximated by their finite sample MC counterparts. In particular, the MC version of the GLS optimization problem is given by

R
^ R () = arg min  ln  x~ j;  -  - ln k x~ j;  2 g x~ j,  AR j=1

(13.33)

where x~ j; j : 1  R denotes i.i.d. draws from the initial sampler p(x | ). Since, in general, the MC sampling variance of g(x, ) on draws from p is expected to be very large (which is why EIS is needed!) it is advisable to delete g(x; ) from (13.33) and solving instead the LS problem. Note that, as typical within a LS framework, high variance in the draws from p actually helps securing an accurate global solution in (13.33).
In high­dimensional problems, the global optimization problem in (13.33) needs to be replaced by a sequence of manageable lower dimensional optimization problems. Here, we just outline the principle of such factorizations, refering the reader to Richard and Zhang (2007) for details and to Sect. 13.6 for a specific implementation. In line with (13.12), we assume that  (x; ) is factorized as

p
 (x; ) =  i(x(i); ) i=1

(13.34)

with

i(x(i); ) = gi(x(i); ) · pi(xi | x(i-1), )

(13.35)

and the pi's defining a (sequential) initial sampler. The importance sampler m(x | a) is partitioned conformably into

p
 m (x | a) = mi(xi | x(i-1); ai), ai  Ai. i=1

(13.36)

13 Simulation Techniques for Panels

431

Let ki(x(i); ai) denote a kernel for mi, with i(x(i-1); ai) denoting its integrating constant (with respect to xi only), whence

mi(xi

|

x(i-1),

ai)

=

ki(x(i); ai) i(x(i-1); ai)

,

(13.37)

with

i(x(i-1); ai) = ki(x(i); ai)dxi.

The key issue is that we can't expect to be able to approximate i(x(i); ), whose integral in xi, is not known, by m(xi | x(i-1), ai) which by definition integrates to one. Instead we could try to approximate i by a kernel ki(x(i); ai), subject to the
sole restriction that ki has to be analytically integrable with respect to xi, so that once we have selected a^i we have an analytical expression for i. Obviously, by doing so, i is not accounted for in the ai LS optimization problem but, since it only depends
on x(i-1) it can be transfered back into the a i-1 LS optimization problem. In other words, the step i optimization subproblem consists of approximating the product
ii+1 by a kernel ki, specifically

R

 ^ i,R () = arg min

ln i(x~(i), j; ) · i+1(x~(i), j; ^ i+1,R ())

iAiiR j=1

2
-i - ln ki(x~(i), j; i)

(13.38)

for i : p  1 (with p+1  1), where {(x~1, j, . . . , x~ p, j); j : 1  R} denotes i.i.d. "trajectories" drawn sequentially from {pi(xi | x~(i-1), j; )}. An example of such a sequential EIS implementation in the context of panel data is presented in Sect. 13.6
below.

13.3.3 MC Sampling Variance of (E)IS Estimates
A frequent criticism raised against (E)IS is the possibility that the variance V (; ) in formula (13.23) might not exist, typically because the approximating kernel k(x; ) has thinner tails than the integrand  (x; ). This criticism calls for important qualifications. Foremost, it applies to all MC methods relying upon approximations of the integrand, including Gibbs and Metropolis-Hastings procedures. Actually, if the variance of an IS estimate is infinite under a class M of auxiliary samplers, so will be the variance of any other MC estimate relying upon the same class. While this concern is addressed in the theoretical MCMC literature (see, e.g. Theorem 7.15 in Robert and Casella (2004)), the empirical MCMC literature largely ignores the possibility that an MCMC estimate might be invalid as the result of inappropriate selection of its auxiliary samplers.
Actually, Richard and Zhang (2007) propose a powerful test of the existence of the variance of an EIS estimate. This test consists of estimating V (; ) in

432

R. Liesenfeld and J.-F. Richard

formula (13.23) under two alternative samplers. It is an immediate by-product of the (E)IS evaluation and, most importantly, does not require any additional draws. Traditional verifications of the existence of the variance are typically based upon a very large number of draws in the hope of generating a very low probability `outlier', which would destabilize the IS estimate. Such tests based upon the detection of a very rare event are notoriously unreliable. It is worth noting that the above mentioned test can also be applied to any other method relying upon auxiliary samplers, allowing for a very effective and unified test of variance finiteness in MC estimation.

13.3.4 GHK Simulator
The GHK simulator, to which we referred in our introduction, also belongs to a class of importance samplers. It is specifically designed to numerically evaluate probabilities of rectangles within a multivariate probit framework. It relies upon a triangular decomposition of the covariance matrix to construct an importance sampler in the form of a sequence of (conditional) univariate truncated Gaussian samplers (see, e.g., Gourieroux and Monfort (1994) for details). GHK has been widely and successfully applied to the numerical evaluation of likelihood functions for multivariate probit models.
In contrast with the EIS method described above, GHK is not designed to handle high-dimensional integrals nor does it includes an auxiliary optimization step (the latter is by no means as critical as for EIS in view of the lower dimensionality). Actually, GHK and EIS serve complementary purposes and can usefully be combined together to evaluate the likelihood of multivariate probit models with unobserved heterogeneity across individual units (in addition to the other dimensions handled by GHK). Specifically, in the context of formula (13.38), GHK would be used to evaluate lower dimensional integrals (e.g. multivariate probits) embedded in the expression of i(·). That is to say GHK would be used to evaluate lowdimensional inner integrals, while EIS would apply to (much higher-dimensional outer integrals. Such an application goes beyond the objectives of the present chapter but belongs to our research agenda. Also we intend to analyze in the future possibilities of incorporating within GHK an EIS-optimization step to increase its numerical efficiency.

13.3.5 Common Random Numbers
There was an important reason for carrying along  as an argument in all expressions from formula (13.15) onward: most (classical) simulation based inference procedures, some of which are discussed in Sect. 13.4 below, require the evaluation of a function G() to be maximized (minimized) in .

13 Simulation Techniques for Panels

433

As we replace G() by its functional estimate G^S() an issue of smoothness immediately arises. Independent MC estimation of G() at neighboring values of  would result in excessive wiggling of G^S(), even when the latter is accurately estimated by EIS. This point is well recognized in the literature on simulated
estimators--see, e.g. McFadden (1989), Pakes and Pollard (1989); Gourieroux and
Monfort (1994)--where "smoothness" constitutes a critical condition for the vali-
dation of the asymptotic properties of these estimators. We will only consider here
the obvious numerical problems that excessive wiggling would create for the optimization of G^S(). Though we might consider smoothing G^S( ) in the process of optimization, a more direct remedy is available which takes advantage of the id-
iosyncrasies of pseudorandom number generation. The idea simply is that of reusing the same uniform pseudorandom numbers for the estimation of G() at different 's, a technique which is known as that of "Common Random Numbers" (hereafter CRN's). Let x~i j, i : 1  S denote the random draws of X used for the estimation of G^S( j). They can be obtained by transformation of a common sequence {u~i; i : 1  S} of uniform pseudorandom numbers, i.e.

x~i j =  u~i;  j .

(13.39)

This procedure will induce high positive correlations between estimates of G() at neighboring values of . It will often suffice to secure sufficient smoothness for numerical optimization to succeed. Numerical evaluation of the derivatives of G^S() often remains delicate. Our own experience suggests using a "simplex" optimization
algorithm. If analytical derivatives are available for the integrand in formula (13.15),
MC estimates of the derivatives of G() should be evaluated alongside with that of G() itself.
The application of formula (13.39) requires attention when rejection techniques
are being used as the actual number of uniform pseudorandom draws required to
produce x~i j might vary with j. The simplest scenario is that when X is obtained by transformation of a "standardized" random variable X whose distribution does not depend on , i.e. when

X = 1 (X; ) and X = 2 (U) .

(13.40)

In such cases, we only have to generate (or regenerate at will) a single sequence {x~i } from which the x~i j's are obtained by means of the transformation 1 for all relevant values of . If, on the other hand, X is obtained directly from U and rejection is involved, it becomes very difficult to effectively implement CRN's. The alternative is then to rely upon the inversion technique, as defined in (13.2). Since, however, the inversion technique is typically much more time consuming than the most performant rejection techniques, its use can significantly increase overall computing time. Taking full advantage of the typical very low dimensionality of sequential EIS kernels, careful reliance upon numerical interpolation techniques can usefully be considered ­ though such discussion goes beyond the objectives of this chapter.

434

R. Liesenfeld and J.-F. Richard

13.4 Simulation-Based Inference Procedures

In this section we briefly discuss the use of MC integration in the context of three of the most commonly used inference techniques: Maximum likelihood, (generalized) Method of Moments and Bayesian posterior moments. Details of implementation are highly problem dependent and will not be considered here.

13.4.1 Integration in Panel Data Models
There exists a growing number of panel data applications requiring multidimensional integrations for which no analytical solutions are available and quadrature (non­stochastic) rules are impractical. A few examples are listed below:
(a) Multinomial probit models involve integrals whose dimensionality equals the number of alternatives minus one;
(b) The elimination of latent variables in nonlinear models also requires numerical integration. The dimensionality of integration typically is that of the vector of latent variables;
(c) Relatedly, the elimination of individual unobserved heterogeneity factors is also done by integration. Though it is occasionally possible to find analytical solutions under specific choices of distributions, numerical integration is required at a higher level of generality;
(d) The evaluation of a likelihood function for panel data with missing observations necessitates the computation of (multidimensional) integrals which, except for special cases, has to be done numerically.
Note also that the application of Bayesian inference techniques to any of these models generally requires additional (numerical) integrations with respect to the parameters themselves. Such an application is discussed in Sect. 25.3 in the context of Markov processes. Finally, there are numerous extensions of the models specifically discussed in this handbook that would require (additional) numerical integrations.
Simulation techniques are increasingly used to evaluate such integrals. A few references of interest in the context of panel data models are Lerman and Manski (1981), McFadden (1989), Pakes and Pollard (1989) or Bo¨rsch­Supan and Hajivassiliou (1993). These contributions and others have led to the development of a number of simulation based inference techniques some of which are briefly described below. See also Gourieroux and Monfort (1993) for a recent survey with reference to panel data or Gourieroux and Monfort (1994) for an in-depth analysis of simulation based econometric techniques.
Two key features characterize this line of work. Firstly, it often relies upon non "efficient" Monte-Carlo procedures (in the numerical sense). Some of the comments found in the literature as to the impracticability of MC likelihood evaluation ­ see, e.g. McFadden (1989) ­ have to be qualified in that context. Secondly, it conflates

13 Simulation Techniques for Panels

435

"statistical" and "numerical" properties of simulation estimators, a point we will address further in Sect. 13.5 below.

13.4.2 Simulated Likelihood

Within a likelihood framework,  in formula (13.15) consists of observables y and unknown parameters  , while  regroups all unobservables. The simplest case is that where a "marginalized" likelihood has to be evaluated which is of the form

L  ; y = f y,  |  d ,

(13.41)

where f denotes the joint density of all relevant variables, observables and unob-
servables. In most cases f takes the form of a product of component densities, e.g.
in the form of (13.12) or (13.13). Such factorizations may considerably simplify the
actual implementation of a simulation algorithm but will not be specifically considered here. Let L^ S( ; y) denote a functional MC estimator of L( ; y) obtained by MC simulation. A simulated maximum likelihood (SML) estimator of  is given by

^ S

y

= arg max


ln L^ S

; y

.

(13.42)

An example of SML estimation using EIS with be presented in Sect. 13.6 below. There are numerous important problems (such as discrete choice models) where
the likelihood function itself is not in the form of an integral but depends upon integrals which have to be numerically evaluated. A general formulation of such problems would take the following form

L  ; y = h G y |  ; y,  , with

(13.43)

G y |  =  y,  |  d .

(13.44)

Examples can be found, e.g. in Gourieroux and Monfort (1994), together with extensions to simulated pseudo maximum likelihood estimation.

13.4.3 Simulated Method of Moments
There exist many excellent discussions of the Generalized Method of Moments (GMM) and of the Simulated Method of Moments (MSM) in the recent literature. The short presentation which follows draws upon Davidson and McKinnon (1993) for GMM and Pakes and Pollard (1989) for MSM. See also McFadden (1989) or Gourieroux and Monfort (1994). Let

436

R. Liesenfeld and J.-F. Richard

G ( ) = H y,  · fY y |  dy

(13.45)

together with

H y,  = h y,  ;  · f  | y,  d

(13.46)

denote a set of moments conditions assumed to be zero at the true value  0. The empirical conditions associated with (13.45) are given by

NT
 H(y j,  0) = 0 ,
j=1

(13.47)

where {y j; j : 1  NT } denotes the actual sample. (The "short hand" notation used in (13.45) appears to suggest that the y j's are i.i.d. with density fY (y |  ). Results are
also available for dependent y's. See, e.g., Gallant (1987). Let Y = (y1, . . . , yNT ). A GMM estimator of  is given by

NT

NT

^ (Y ) = arg min


 H(y j,  )
j=1

A (Y )

 H(y j,  )
j=1

,

(13.48)

where A(Y ) is a symmetric positive definite matrix. The consistency of ^ (Y ) obtains under a broad range of conditions. Efficiency requires that

 plim A (Y ) = AVar
NT 

1 NT

NT
H(y j,
j=1

 0)

:= A0 .

The

asymptotic

variance

of

 N

T

^

(Y

)

on

condition

(13.49)

is

given

by

AVar

 NT

^

(Y

)

=

D0A-0 1D0 -1 ,

with

(13.49) (13.50)

 D0 = plim NT 

1

NT

H

(y

,
j

)

NT j=1  

.
 = 0

(13.51)

Let H^ S (y j,  ) denote a (convergent) MC estimator of H(y,  ). An MSM estimator of  is given by

NT

NT

  ^

S

(Y

)

=

arg

min


H^ S(y j,  )
j=1

A (Y )

H^ S(y j;  )
j=1

.

(13.52)

As discussed earlier it is important that CRN's be used in drawing the  's from f ( | y,  ).

13 Simulation Techniques for Panels

437

13.4.4 Bayesian Posterior Moments

Let ( ) denote a prior density for the parameters of the marginalized likelihood L( ; y), as definded in (13.41). The posterior density of  is proportional to the product of  by L and its integrating constant is generally unknown. The posterior density of a function h( ) is given by

h =

h ( ) f y,  |   ( ) d d .

f y,  |   ( ) d d

(13.53)

A convergent (E)IS estimator of h is given by

S

 (~ i, ~ i, y)h(~i)

h^ S = i=1 S

,

 (~ i, ~ i; y)

i=1

(13.54)

where {(~ i, ~ i); i : 1  S} are i.i.d. draws from a sampler ( ,  | y) and  = f /. Details are found e.g., in Geweke (1994).
Note that the evaluation of h^S only requires a single set of joint draws from . In contrast, maximization of L^ S( ; y) in (13.42) requires a new set of (CRN) draws of
 for each value of  at which the likelihood has to be evaluated.

13.5 Numerical Properties of Simulated Estimators
There is a fundamental difference between the ways in which classical and Bayesian econometricians evaluate the properties of simulated estimators. Bayesians treat the actual sample y as fixed. The only source of randomness to be accounted for originates from the auxiliary MC sample. It follows, in particular, that Bayesians routinely assess the numerical accuracy of their MC estimates of quantities of interest, e.g. in the form of MC standard deviations.
In contrast classical econometricians insist upon treating ^ S(y) as an estimate of  itself and, therefore, have to account for two independent sources of randomness: the actual sample y and the auxiliary MC sample  . We find three major drawbacks to the classical analysis of simulated estimators:
(i) It confuses the issue of assessing the statistical properties of ^ (y) as an estimate of  with that of evaluating the numerical accuracy of ^ S(y) as an MC estimate of ^ (y);
(ii) It complicates the analysis of simulation estimators since the two sources of randomness apply at fundamentally different levels. Specifically, the statistical properties of ^ (y) are determined by data availability. In contrast, the numerical properties of ^ S(y) are fully controlled by the analyst and can be arbitrarily

438

R. Liesenfeld and J.-F. Richard

reduced either by increasing MC sample sizes and/or, more efficiently, by increasing the efficiency of the simulation techniques (which is precisely what EIS has been designed for). (iii) It applies to situations where the observables y and the latent variables  are jointly simulated. However, as we argued above, integration of  can often be carried out much more efficiently by using EIS in which case numerical accuracy is typically far greater than statistical accuracy and conventional formulae do not apply.

Nowhere is the problem more apparent than in the context of simulated ML estimators. The statistical properties of an ML estimator ^ (y) are well understood
and follow from the application of a Central Limit Theorem (hereafter CLT) to the
derivatives of the logarithm of the likelihood function. In contrast MC estimation
applies to the likelihood function itself. Since integrals and logarithms do not commute, it follows that ^ S(y) is an inconsistent estimator of  for fixed S and comments to that effect abound in the literature.
We propose instead to keep treating ^ (y) as a statistical estimator of  (y) and, when ^ (y) cannot be computed, to treat ^ S(y) as a numerical estimator of ^ (y), not of  . Statistical inference then follows standard procedures. For example, we
already know that, under appropriate conditions,

 NT

^

y

-

d N (0,V ( )) ,

(13.55)

where NT is actual sample size and V ( ) is a covariance matrix to be estimated by a matrix V^ (y). When no analytical expressions are available for the likelihood
function, we can use MC simulation as a numerical device for computing approximations to ^ (y) and V^ (y). At this stage of the analysis, our sole problem is that of
assessing the numerical accuracy of these approximations which are to be treated
as functions of  , given y.
For large S's, we can apply standard techniques to obtain "asymptotic" MC sampling distributions for ^ S(y)--here again y is kept fixed at its observed value. Under conditions such as those found, e.g., in Geweke (1994), a CLT applies to L^ s( ; y)--
not to its logarithm--and to its derivatives. In particular,

 S

 L^ S  ; y 

L ; y - 

d N 0,   ; y

,

(13.56)

where ( ; y) is a covariance matrix that can be estimated alongside with ^ S(y). It follows that

 S

^ S

y

- ^

y

d N 0, P-1  ; y   ; y P-1  ; y

(13.57)

with

P ; y

=

1 plim S S

 2L^ S  ; 

y

.

(13.58)

13 Simulation Techniques for Panels

439

Estimates of P( ; y) and ( ; y) can be obtained as byproducts of the simulation runs.
In general, however, it is much easier to produce "finite sample" numerical co-
variance matrices. All that is required is to rerun the entire MC­ML algorithm under different seeds for the  's, producing thereby i.i.d. draws of ^ S(y) for a given y. Similarly finite sample statistical covariance matrices can be obtained by simulation of y for a given set of  CRNs. (That is to say, the  draws used for any particular y and which, for reasons of numerical efficiency need to be conditional upon y, are to
be obtained by transformation of a fixed set u of canonical draws--see (13.39).)
In other words, once a MC simulation program has been produced to compute
simulated ML estimators for a specific problem, it is generally trivial to produce
separate estimates of numerical and statistical accuracy by embedding that program into two distinct external simulation loops (one for  (U) given y and the other for y given U). Note that it is also possible to construct joint simulation of  (U) and y to produce a measure of the compound uncertainty of ^ S(y) as an estimate of  .

13.6 EIS Application: Logit Panel with Unobserved Heterogeneity

13.6.1 The Model

In the following, we discuss the application of EIS to the evaluation of the likelihood of a panel logit model with unobserved heterogeneity, illustrating the full sequen-

tial implementation of the procedure. Other applications of EIS are, for example,

the estimation of various univariate and multivariate specifications of the stochastic volatility model for financial returns by Liesenfeld and Richard (2003c), the estima-

tion of dynamic parameter-driven count-data models by Jung and Liesenfeld (2001) and the estimation of stochastic autoregressive intensity processes for financial mar-

ket activities on a trade-by-trade basis by Bauwens and Hautsch (2003).

Consider the following model for the latent variable yti for individual i and time

period t:

yti =  zti + i + t + ti ,

i : 1  N, t : 1  T ,

(13.59)

where zti is a vector of explanatory variables and ti is an i.i.d. logistic variable with zero mean and variance 2/3. i represents individual random effects and is assumed to be i.i.d. Gaussian with zero mean and variance 2. t captures time
random effects and is assumed to follow a stationary autoregressive process

t = 1t-1 + · · · + kt-k + t ,

(13.60)

where t is an i.i.d. Gaussian variable with zero mean and variance 2 such that the stationary mean of t is zero and the stationary variance is 2 = 2/(1 - 12 - · · · - k2). It is assumed that the components ti, i, and t are mutually independent. The

440

R. Liesenfeld and J.-F. Richard

observable variable is given by the dummy variable yti = I{yti0} and the vector of parameters to be estimated is  = ( ,  , 1, . . . , k,  ) . Let  = (1, . . . , T ) and  = (1, . . . , N) , then the likelihood function associated with y = (y11, . . . , yTN)
can be written as

L  ; y = g ,  ;  , y p (,  ;  ) dd ,

(13.61)

with

  g ,  ;  , y

NT
=
i=1 t=1

1

1-yti

1 + exp {vti}

exp {vti} yti 1 + exp {vti}

(13.62)

 p (,  ;  )  -N exp

-

1 22

N i=1

i2

| |-1/2

· exp

-1 2

-1

(13.63)

vti =  zti + i + t ,

(13.64)

where  denotes the stationary variance­covariance matrix of  . A natural MC estimator of this likelihood function for given values of  and y is

given by

 L¯ S;p

; y

=

1 S

S
g(~r, ~ r;  , y) ,
r=1

(13.65)

where {(~r, ~ r); r : 1  S} are i.i.d. draws from p(,  ;  ). Since the natural sampling density p directly obtained from the statistical formulation of the model, does
not incorporate critical information about the latent processes conveyed by the observations y, the natural estimator L¯ S;p is highly inefficient. In fact, for all practical purposes, a prohibitively large MC sample size would be required to obtain reasonably accurate estimates of L. Moreover, the implicit "posterior" density of (,  ) is much tighter than its "prior" (the natural sampler), since the sample conveys sig-
nificant information on unobserved heterogeneity. Whence the "important" domain

of integration where g effectively contributes to the value of the integral is much
tighter than that implied by the natural sampler. Thus, the probability that a MC draw (~r, ~ r) hits exactly this region is almost zero and, hence, the natural MC estimate L¯ S;p is severely downward biased.

13.6.2 EIS Evaluation of the Likelihood
As discussed above (see formulae (13.34) to (13.37)), the global high­dimensional EIS optimization problem (13.33) associated with L( ; y) has to be broken down

13 Simulation Techniques for Panels

441

into low-dimensional subproblems according to an appropriate factorization of the integrand  (,  ;  , y) = g(,  ;  , y)p(,  ;  ). Since the empirical application discussed below is based on a data set with N T , the integrand will be factorized into a product of N terms each of which depends upon a single i and a remainder (for T N, one would instead factorize  into T terms, each of which depends upon a single t )

N
L  ; y = 0 ( ;  )  i i,  ;  , y dd , i=1

(13.66)

with

 i i,  ;  , y

 -1 exp

-

i2 22

T t=1

1 1 + exp{vti}

1-yti

·

exp {vti} 1 + exp {vti}

yti
,

(13.67)

0( ;  )  | |-1/2 exp

-

1 2



-1

.

(13.68)

According to this factorization, the global optimization problem associated with  can be factorized into subproblems each of which is related to a single component i (i : 0  N) whose salient characteristics ought to be captured by a corresponding efficient sampling density. Note that, even though the i's and t 's are, according to the assumptions of the statistical model, stochastically independent, i introduces interdependencies between these variables. In order to take this into account the
efficient sampler can be constructed as a sequence of sampling densities with an unconditional density for  and a sequence of conditional densities for i |  . The resulting factorization of the efficient sampler is given by

N
m (,  | ) = m0( ; 0)  mi (i |  ; i) , i=1

(13.69)

where m0 and {mi} are specified (as a natural choice) as parametric extensions of the corresponding densities defining the natural sampler p, and  = (0, 1, . . . , N) is a vector of auxiliary parameters. For any given value of , the likelihood (13.66)
can be rewritten as

 L  ; y =

0 ( ;  ) N m0 ( ; 0) i=1

i i,  ;  , y mi (i |  ; i)

m (,  | ) dd ,

(13.70)

and the corresponding MC estimate of the likelihood is given by

L~ S;m  ; y,  =

  1
S

S r=1

0(~ r (0) ; m0(~ r (0) ;

) 0)

N i=1

i(~ir (i) , ~ r mi(~ir (i) | ~

(0) ; r (0)

 , y) ; i)

,

(13.71)

442

R. Liesenfeld and J.-F. Richard

where {[~1r(1), . . . , ~Nr(N), ~ r(0)]; r : 1  S} are i.i.d. draws from the auxiliary importance sampling density m(,  |).
As discussed above, the optimal sampling density mi requires constructing a functional approximation ki(i;  , i) for i(i,  ;  , y) with the requirement that its integral with respect to i (depending upon  ) can be computed analytically. Specifically, the function ki(i;  , i) serves as a density kernel for mi(i| ; i) which is given by

mi (i

|

;

i) =

ki (i;  , i) i ( , i)

,

where

i ( , i) =

ki (i;  , i) di . (13.72)

Note that a good match between the i's alone and the ki's would leave the sequence of i's unaccounted for. But since the i's do not depend on the i's they can be attached to the problem of matching 0 by m0. Accordingly, the likelihood can be
rewritten as

 L  ; y =

0 ( ;  ) iN=1 i ( ; , i) N i i,  ;  , y

m0 ( ; 0)

i=1 ki (i;  ; i)

(13.73)

·m (,  | ) dd .
Taken all together, the sequential implementation of the global high­dimensional EIS optimization problem requires solving a sequence of N + 1 low-dimensional (weighted) LS problems of the form

S

 ^ i

=

arg min
i r=1

ln i(~ir, ~ r;  , y) - ci

- ln ki(~ir; ~ r, i) 2 gi(~ir, ~ r;  , y)

(13.74)

for i : 1  N and

S

N

  ^ 0

=

arg min
0 r=1

ln[0(~ r;  ) i(~ r, ^ i)] - c0
i=1

2

- ln m0(~ r; 0) ,

(13.75)

where the weights {gi(i,  ;  , y); i : 1  N} are given by the N terms of the outer product of the function g(,  ;  , y) given in (13.62) (the weight for the LS problem (13.75) is by construction g0(·) = 1). {(~1r, . . . , ~Nr, ~ r), r : 1  S} are i.i.d. draws from the natural sampler p, and the ci's and c0 are unknown constants to be estimated jointly with the i's and 0.
One iteration of the EIS optimization algorithm generally suffices to produce
a vastly improved importance sampler. Nevertheless, a small number of iterations
where the natural sampler p for the (weighted) LS problems is replaced by the

13 Simulation Techniques for Panels

443

previous stage importance sampler produces further efficiency gains. For such iterations to converge to a fixed value of auxiliary parameters , which are expected to produce the optimal sampler, it is necessary to use CRNs implying that all draws for the i's and  for any given sampling density are obtained from a fixed set of standardized random numbers (see, Sect. (13.3.3)). Also, as discussed earlier, it is
generally preferable to set the LS weights in (13.74) equal to one in the first iteration.
Furthermore, observe that for the logit model with Gaussian random effects the
use of parametric extensions of p for the construction of the efficient sampler m implies that the ki's are Gaussian density kernels for the i's given  , and m0 be a multivariate Gaussian density for  . Thus, the LS problems (13.74) and (13.75) become linear in i and 0. Moreover, note that in this case 0 as well as the i's are Gaussian kernels for  allowing for a perfect fit in the LS problem (13.75) and for an analytical construction of the efficient sampler m0 for  . (For a full description of the implementation of the efficient sampling procedure for the logit panel model,
see the Appendix.) Finally, the MC likelihood estimate for any admissible value of  and y based on
the optimal sampler is obtained by substituting {i , i : 0  N} by {^ i , i : 0  N} in equation (13.71). Based on the efficient sampling procedure, the ML parameter estimates of  are obtained by maximizing L~ S,m( ; y, ^ ) with respect to  , using an iterative numerical optimizer. The convergence of such an optimizer requires the use of CRNs in order to ensure that L~ S,m( ; y, ^ ) is a smooth function in  .

13.6.3 Empirical Application

We applied the EIS algorithm described above to the ML estimation of a panel logit model for the union/non-union decision of young men. This application is based upon the framework and dataset used in the study of Vella and Verbeek (1998).
In particular, the reduced form model for the choice of individual i in period t of a union or a non-union employment is assumed to be

yti =  zti + yt-1,i + i + t + ti ,

i : 1  N, t : 2  T,

(13.76)

where the latent variable yti represents the individual benefits of a union membership. The observed union status is given by the dummy yti = I{yti0}. As proposed by Vella and Verbeek (1998) the union status is explained by individuals' characteristics and a
set of industry dummies summarized in the vector zti and by the lagged union status yt-1,i. The lagged union status is included to capture individuals' propensity to remain in the initially chosen status. For simplicity, we assume that the initial state y1,i is a fixed non­stochastic constant for individual i. (A more sophisticated alternative to
handle this initial condition problem proposed by Heckman (1981) and followed by
Vella and Verbeek (1998) is to approximate the marginal probability of y1,i using all information on the exogenous variables in period t = 1.) For the time random effect t we use a first­order autoregressive process.

444

R. Liesenfeld and J.-F. Richard

In contrast to this panel logit specification with random individual and random dynamic time effects, Vella and Verbeek (1998) employ a corresponding probit model with random individual and fixed time effects. Here a logit specification is used just for computational convenience, but a substitution of the logistic distribution for ti by a normal distribution or any other suitable distribution requires only minor modifications in the EIS algorithm. The use of random individual and fixed time effects enables Vella and Verbeek (1998) to rely on standard integration procedures for one­dimensional integrals to evaluate the likelihood function, but their procedure cannot be applied to evaluate the likelihood for random individual and random dynamic time effects jointly. By applying EIS we can do the latter and compare both approaches.
The data used to estimate the model are taken from the National Longitudinal Survey (NLS Youth Sample) and contain observations on 545 males for the years 1980­1987 (for a detailed description of the data, see Vella and Verbeek (1998)). The ML EIS estimates of the model based upon a simulation sample size S = 300 and three iterations of the efficient sampling algorithm are given in Table 13.1. Each likelihood evaluation requires approximately 2 seconds on a Pentium 4, 3.06 GHz personal computer for a code written in GAUSS. A full ML estimation requires approximately 122 BFGS iterations and takes of the order of 230 min. The parameter estimates are numerically accurate, as indicated by the MC (numerical) standard errors, which were computed from 20 ML estimations conducted under different sets of CRNs.
The parameter estimates for the impact of the individuals' characteristics and the industry dummies on the probability of union membership are consistent with those reported by Vella and Verbeek (1998). In particular, except for the variable log(1 + experience), whose estimated impact is not significant, the signs of the parameter estimates are all the same in both estimates. To make the values of the parameter estimates from our logit specification comparable with those from Vella and Verbeek's probit specification, one can divide our parameter estimates by (2/3+^2 +^2)1/2. The result (not presented here) shows that the values are for all parameters very close together. Furthermore, only for the variables health disability, Lives in North East, and Entertainment our model estimate leads to different conclusions with respect to statistical significance compared to Vella and Verbeek's estimate.
The estimate of the variance parameter of the random individual effects  is significantly greater than zero and its estimate of 1.77 indicates that 48.6% of total variation is explained by cross individual variation which is lower than the 57% reported by Vella and Verbeek (1998). Furthermore, the estimate of the variance parameter  is 2.19 standard errors larger than zero and implies that 0.5% of the total variance is explained by random time effects. Finally, observe that the autoregressive coefficient is not significantly different from zero. Together with the fact that lagged union status has a highly significantly positive impact on the probability of union membership this indicates that the dynamics in the union/non-union decision is dominated by an idiosyncratic component rather than by an aggregate common component.

13 Simulation Techniques for Panels

445

Table 13.1 ML efficient sampling estimates of the panel logit model for union membership

Variable
Constant Lagged union status log(1+experience) Years of schooling Married Black Hispanic Lives in rural area Has health disability Lives in North East Lives in south Lives in Northern Central

Estimate
-1.1912 1.9685
-0.2523 -0.0385
0.3408 1.2835 0.6267 0.0071 -0.6448 0.4194 -0.0593 0.3996

asy. std. error
1.2607 0.1483 0.3075 0.0595 0.1430 0.2722 0.2209 0.1901 0.1934 0.2348 0.2164 0.2265

MC std. error
0.1373 0.0565 0.0439 0.0063 0.0080 0.0354 0.0280 0.0148 0.0123 0.0229 0.0234 0.0159

Industry Dummies Agricultural Mining Construction Manufacturing Transportation Trade Finance Business & repair service Personal service Entertainment Professional & related services

-1.4372 -0.6509 -1.1622 -0.5519 -0.2467 -1.4442 -3.0984 -2.0654 -2.0703 -1.5235 -0.4990

0.4450 0.4995 0.3498 0.3044 0.3547 0.3127 0.5065 0.3880 0.3936 0.4227 0.3299

0.0294 0.0355 0.0229 0.0141 0.0195 0.0169 0.0902 0.0207 0.0219 0.0324 0.0149

Random Effects Parameters
  1
Log-likelihood value

1.7735 0.1774 -0.1124 -1303.71

0.1367 0.0809 0.5171

0.0777 0.0154 0.1690 2.1713

Note: Asymptotic standard errors are obtained from a numerical approximation to the Hessian. The ML efficient sampling estimates are based on a MC sample sice of S = 300 and three iterations of the construction of the efficient sampler.

13.7 Conclusion
Simulation based inference procedures have become a key component of the micro­ and macroeconometrician's toolbox. This chapter was never meant to provide a systematic survey of the recent literature. As mentioned in the course of the discussion, excellent surveys are available elsewhere (and often require more than a single chapter!). See, in particular, Gourieroux and Monfort (1993, 1994). We have attempted to selectively discuss issues which, based upon our own experience, constitute the cornerstones of an efficient usage of MC simulation techniques, with emphasis on efficient integration of random effects in panel models.

446

R. Liesenfeld and J.-F. Richard

Much of our discussion applies to classical and Bayesian procedures as well, largely because we insist on interpreting MC integration as a numerical technique for constructing approximations to expression which depend upon (highdimensional) integrals for which no analytical expressions are available.
We find ourselves at odds with the recent (classical) literature on simulation based estimation on two key counts.
Firstly, "natural" MC simulation is increasingly outdated and often utterly impractical in applications requiring moderate to high­dimensional interdependent integration (especially for "marginalized" likelihood evaluation). There exists an increasing range of operational "acceleration" procedures, most prominently Efficient Importance Sampling, which can produce considerable efficiency gains which generally far outweighs moderate increases in the cost of computations for any given number of draws. Yet acceleration procedures are largely ignored by the classical literature.
Secondly, one ought to draw a clear distinction between the statistical properties of an estimator and the numerical accuracy of its simulated counterpart. Unsurprisingly, the implementation of that key distinction greatly simplifies the conceptual and practical analysis of simulation based estimators.

Acknowledgments Financial support for this work has been provided by the National Science Foundation (SES­9223365). We are especially indebted to Jean­Pierre Florens, John Geweke, David Hendry, Jay Kadane and Wei Zhang for numerous helpful discussions relative to the use of simulation techniques in econometrics.

13.8 Appendix: Implementation of EIS for the Logit Panel Model

Implementation of the efficient sampling procedure for the likelihood evaluation for
the panel logit model (13.59)­(13.60) starts with the selection of the class of density kernels ki for the auxiliary samplers mi capable of approximating i as defined in (13.68). Since the natural sampler for i is a constituent component of i, a natural choice for mi is, as mentioned above, a parametric extension of the natural sampler. In our case, this leads to a Gaussian density kernel ki for i |  . In particular, the following parametrization is used:

ki (i;  , i) = exp

-1 2

bivi + viCivi

-

i2 22

,

(13.77)

where bi = (b1i, . . . , bTi) , Ci = diag (ci) , ci = (c1i, . . . , cTi)

(13.78)

vi =  + i + Zi , with  = (1, . . . , 1) , Zi = z1i, . . . , zTi , (13.79)

and the auxiliary parameters are i = (bi, ci) . Note that under this parametrization of ki the factor exp{-i2/(22)} cancels out in the LS problems (13.74). In order

13 Simulation Techniques for Panels

447

to derive the conditional mean and variance of the Gaussian sampling density mi, it is useful to rewrite ki as follows:

ki (i;  , i) = exp

-1 2

1 2

+



Ci

i2 + bi + 2 Ci i i + bi i + iCi i

,

(13.80)

where i =  + Zi . Accordingly, the conditional mean and variance of i| on mi are obtained as

i = -i2

1 2

bi

+



Ci

i

and

i2

=

2 1 +  Ci2

.

(13.81)

Integrating ki with respect to i leads to the following form of the integrating con-

stant:

i ( , i)  exp

-1 2

bi

i+

iCi

i

-

i2 i2

.

(13.82)

which itself is a Gaussian density kernel for  . Based on these functional forms, the

computation of an efficient MC estimate of the likelihood for the panel logit model

requires the following steps:

Step (1): Use the natural sampling density p to draw S independent realizations of the latent processes (~r, ~ r).
Step (2): Use these random draws to solve the sequence of N weighted (un-

weighted for the first iteration of importance sampling construction) LS problems

defined in (13.74). The ith weighted LS problem is characterized by the following

linear auxiliary regression:

T
 [ytiv~tir - ln (1 + exp {v~tir})] = constant + (-b1i/2) v~1ir + · · · + (-bTi/2) v~Tir
t=1
(13.83) + (-c1i/2) v~21ir + · · · + (-cTi/2) v~T2 ir + ir ,
with weights:

T
gi(~ir, ~ r;  , y) =
t=1

1

1-yti

1 + exp {v~1ir}

exp {v~1ir}

yti
,

1 + exp {v~1ir}

(13.84)

where ir denotes the regression error term and {v~tir; r : 1  S} are the simulated draws of vti.
Step (3): The function to be approximated by the Gaussian sampler m0 is given by:

  N
0 ( ;  ) i ( i, i)  exp
i=1

-1 2

N
 -1 +
i=1

bi i +

iCi

i

-

i2 i2

,

(13.85)

448

R. Liesenfeld and J.-F. Richard

which has as mentioned above the form of a Gaussian kernel for  . Accordingly, the mean and variance-covariance matrix of  on m0 are obtained as

N

0 = 0

i2ci

i=1

ciZi

+

1 2



bi

-

1 2

bi

-

Ci

Zi



N

-1

 0 = -1 + Ci - i2cici

.

i=1

(13.86) (13.87)

Use this sampling density m0 to draw S trajectories {~ r(^ 0); r : 1  S}. Condi-

tional on these trajectories, draw from the conditional densities {mi} characterized

by the moments {(~r(^ 1, . . . , ^ N

()1; 3~.8r(1)^ t0h)e)}vethcetoerfsfi{c~ier(nt^ s1a,m. .p. ,li^nNg)e;srti:m1ate

S}. Based on the of the likelihood

draws is cal-

culated according to (13.71).

References
Bauwens, L. and N. Hautsch (2003), Stochastic Conditional Intensity Process, Mimeo, Universite Catholique de Louvain.
Bertschek, I. (1995), Product and Process Innovation as a Response to Increasing Imports and Foreign Direct Investment, Journal of Industrial Economics, 43, pp. 341­357.
Bertschek, I. and M. Lechner (1998), Convenient Estimators for the Panel Probit Model, Journal of Econometrics, 87, pp. 329­372.
Brown, B.W., A. Monfort and H.K. Van Dijk (1993), Econometric Inference using Simulation Techniques, Journal of Applied Econometrics, 8, pp. s1­s3 (special issue).
Bo¨rsch­Supan, A. and V. Hajivassiliou (1993), Smooth Unbiased Multivariate Probability Simulators for Maximum Likelihood Estimation of Limited Dependent Variable Models, Journal of Econometrics, 58, pp. 347­368.
Bo¨rsch­Supan, A., V. Hajivassiliou, L. Kotlikoff and J. Morris (1990), Health, Children, and Elderly Living Arrangements: A Multiperiod-Multinomial Probit Model with Unobserved Heterogeneity and Autocorrelated Errors, National Bureau of Economic Research, Working Paper No. 3343.
Danielsson, J. and J.F. Richard (1993), Accelerated Gaussian Importance Sampler with Application to Dynamic Latent Variable Models, Journal of Applied Econometrics, 8, pp. 153­173.
Davidson, R. and J.G. McKinnon (1992), Regression­based Methods for Using Control Variates in Monte Carlo Experiments, Journal of Econometrics, 54, pp. 203­222.
Davidson, R. and J.G. McKinnon (1993), Estimation and Inference in Econometrics, Oxford University Press, Oxford.
Devroye, L. (1986), Non­Uniform Random Variate Generation, Springer­Verlag, New York. Fishman, G.S. (1996), Monte Carlo Concepts, Algorithms, and Applications, Springer, New York. Fishman, G.S. and L.R. Moore, III (1982), A Statistical Evaluation of Multiplicative Random Num-
ber Generators with Modulus 231 - 1, Journal of the American Statistical Association, 77, pp. 129­136. Fishman, G.S. and L.R. Moore, III (1986), An Exhaustive Analysis of Multiplicative Congruential Random Number Generators with Modulus 231 - 1, SIAM Journal on Scientific and Statistical Computing, 7, pp. 24­45. Gallant, A.R. (1987), Nonlinear Statistical Models, John Wiley & Sons, New York.

13 Simulation Techniques for Panels

449

Geweke, J. (1988), Antithetic Acceleration of Monte Carlo Integration in Bayesian Inference, Journal of Econometrics, 38, pp. 73­89.
Geweke, J. (1989), Bayesian Inference in Econometric Models Using Monte Carlo Integration, Econometrica, 57, pp. 1317­1340.
Geweke, J. (1991), Efficient Simulation from the Multivariate Normal and Student-t Distributions Subject to Linear Constraints, Computer Science and Statistics: Proceedings of the TwentyThird Symposium on the Interface, pp. 571­578.
Geweke, J. (1994), Monte Carlo Simulation and Numerical Integration, Federal Reserve Bank of Minneapolis, Working Paper No. 526.
Geweke, J., M. Keane and D. Runkle (1994), Alternative Computational Approaches to Inference in the Multinomial Probit Model, Review of Economics and Statistics, 76, pp. 609­632.
Geweke, J., M. Keane and D. Runkle (1997), Statistical Inference in the Multinomial Multiperiod Probit Model, Journal of Econometrics, 81, pp. 125­166.
Greene, W. (2004), Convenient Estimators for the Panel Probit Model: Further Results, Empirical Economics, 29, pp. 21­47.
Gourieroux, C. and A. Monfort (1993), Simulation Based Inference: A Survey with Special Reference to Panel Data Models, The Journal of Econometrics, 59, pp. 5­33.
Gourieroux, C. and A. Monfort (1994), Simulation Based Econometric Methods, CORE Lecture Series, CORE Foundation, Louvain­la­Neuve.
Hajivassiliou, V. (1990), Smooth Simulation Estimation of Panel Data LDV Models, Mimeo, Yale University.
Heckman, J.J. (1981), The Incidental Parameters Problem and the Problem of Initial Conditions in Estimation a Discrete Time­Discrete Data Stochastic Process, in Structural Analysis of Discrete Data with Econometric Applications (chapter 4) Manski, C.F. and D. McFadden, MIT Press, Cambridge.
Hendry, D.F. (1984), Monte Carlo Experimentation in Econometrics, in The Handbook of Econometrics (chapter 16) Griliches, Z. and M. Intriligator, North-Holland, Amsterdam.
Hendry, D.F. and J.F. Richard (1992), Likelihood Evaluation for Dynamic Latent Variable Models, in Computational Economcis and Econometrics (chapter 1) Amman, H.M, Belsley, D.A. and C.F. Pau, Kluwer Academic Publishers, Dordrecht.
Jung, R.C. and R. Liesenfeld (2001), Estimating Time Series Models for Count Data Using Efficient Importance Sampling, Allgemeines Statistisches Archiv, 85, pp. 387­407.
Keane, M. (1994), A Computationally Practical Simulation Estimator for Panel Data, Econometrica, 62, pp. 95­116.
Kloek, T. and H.K. van Dijk (1978), Bayesian Estimates of Equation System Parameters: An Application of Integration by Monte Carlo, Econometrica, 46, pp. 1­19.
Lerman, S. and C. Manski (1981), On the Use of Simulated Frequencies to Approximate Choice Probability, in Structural Analysis of Discrete Data with Econometric Applications (chapter 7) C. Manski and D. McFadden, MIT Press, Cambridge.
Liesenfeld, R. and J.F. Richard (2003a), Monte Carlo Methods and Bayesian Computation: Importance Sampling, in The International Encyclopedia of the Social and Behavioral Sciences (pp. 10000­10004) Smelser, N.J. and P.B. Baltes, Elsevier Science, Oxford.
Liesenfeld, R. and J.F. Richard (2003b), Estimation of Dynamic Bivariate Mixture Models: Comments on Watanabe (2000), The Journal of Business and Economic Statistics, 21, pp. 570­576.
Liesenfeld, R. and J.F. Richard (2003c), Univariate and Multivariate Volatility Models: Estimation and Diagnostics, The Journal of Empirical Finance, 10, pp. 505­531.
McFadden, D. (1989), A Method of Simulated Moments for Estimation of Discrete Response Models Without Numerical Integration, Econometrica, 57, pp. 995­1026.
Pakes, A. and D. Pollard (1989), Simulation and the Asymptotics of Optimization Estimators, Econometrica, 57, pp. 1027­1058.
Press, W.h., B.P. Flannery, S.A. Teukolsky and W.T. Vetterling (1986), Numerical Recipes (The Art of Scientific Computing), Cambridge University Press, Cambridge.
Richard, J.F. and W. Zhang (2007), Efficient High-Dimensional Importance Sampling, Journal of Econometrics, 141, pp. 1385­1411.

450

R. Liesenfeld and J.-F. Richard

Robert, C.P. and G. Casella (2004), Monte Carlo Statistical Methods, Springer, New York. Stern, S. (1992), A Method for Smoothing Simulated Moments of Discrete Probabilities in Multi-
nomial Probit Models, Econometrica, 60, pp. 943­952. Stern, S. (1997), Simulation-Based Estimation, Journal of Economic Literature, 35,
pp. 2006­2039. Stat/Library User's Manual (1991) IMSL, IMSL Inc., Houston. Vella, F. and M. Verbeek (1998), Whose Wages do Unions Raise? A Dynamic Model of Union-
ism and Wage Rate Determination for Young Men, Journal of Applied Econometrics, 13, pp. 163­283.

Chapter 14
Semi-parametric and Non-parametric Methods in Panel Data Models
Chunrong Ai and Qi Li

14.1 Introduction

Common approach for modeling panel data {(yit , xit ), i = 1, 2, . . . , N; t = 1, 2, . . . , T }

involves postulating that the data generating process depends on a time invariant in-

dividual specific effect i and some model parameters, and satisfies some statistical restrictions. The individual effect i is not observed and is assumed to be randomly

distributed across individuals. The model parameter may include finite dimensional

as well as infinite dimensional parameter. Estimation of the parameter of interest de-

pends on the statistical restrictions imposed on the data generating process and on

the relative values of N and T . Since many different restrictions can be imposed on

the data generating process and different restrictions often lead to different estima-

tion methods, the econometric literature on estimation of panel data models is large

and is scattered around various academic outlets. The aim of this handbook is to

provide a thorough survey of the vast literature on estimation of panel data models.

Within the general objective of the handbook, this chapter attempts to survey recent

development on estimation methods of a particular class of models: semiparamet-

ric and nonparametric panel data models. For discussions on parametric panel data

models, see Baltagi (2005) and Hsiao (2003).

A model is semiparametric or nonparametric if the model parameter includes

the infinite dimensional parameter. By this definition, the classical linear panel data

model

yit = xit o + i + uit , i = 1, 2, . . . , N; t = 1, 2, . . . , T ,

(14.1)

Chunrong Ai Department of Economics, University of Florida, Warrington College of Business Administration, 224 MAT, P.O. Box 117140, Gainesville, FL 32611-7140, USA, e-mail: chunrong.ai@cba.ufl.edu
Qi Li Department of Economics, Texas A&M University, College Station, TX 77843-4228, USA, e-mail: qi@econmail.tamu.edu

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

451

c Springer-Verlag Berlin Heidelberg 2008

452

C. Ai and Q. Li

is a semiparametric model if the joint distribution of (xit , i, uit ) is not parameterized. In fact, by this definition, almost all panel data models that have been studied in the literature, including those discussed in other chapters of this handbook, can be classified as the semiparametric models. Thus, surveying the literature on this general class of semiparametric and nonparametric models will duplicate the discussions in the other chapters of the handbook. Our focus here is instead on a smaller class of semiparametric and nonparametric panel data models that are not covered by the other chapters. Specifically, we will survey the literature on: (1) partly linear or fully nonparametric panel data models and (2) panel data discrete choice and censored regression models with unknown error distribution. We note that Arellano and Honore (2001) reviewed the panel data discrete choice and censored regression models. Although we will survey those same models, we will include more recent results as well as various extensions to those models.
Throughout the chapter, we will use i = 1, 2, . . . , N to denote an individual and t = 1, 2, . . . , T to denote time. T is small relative to N. We will restrict our discussion to the balanced panel (i.e., T does not vary with i ) for the purpose of simplifying exposition. The methods we review here are easily applicable to the unbalanced panel. We will use 1{A} to denote the indicator function that takes the value 1 if event A occurs and the value 0 otherwise. sgn(·) is the sign function that takes the value 1 if · > 0, the value 0 if · is zero, and the value -1 if · is negative. For each i, denote xi = (xi1, xi2, . . . , xiT ), yi = (yi1, . . . , yiT ), and ui = (ui1, . . . , uiT ). Finally, variables with superscript  denote the latent variables that are not always observed directly.

14.2 Linear Panel Data Model

14.2.1 Additive Effect

We begin with the classical linear panel data model given by (14.1), where the
individual effect enters the model additively and xit does not include the timeinvariant regressors, wi. The linear and the additive structure of the model allows us to remove the individual effect through mean-differencing:

   yit

-

1 T

T
yis
s=1

=

(xit

-

1 T

T
xis)
s=1

o

+ uit

-

1 T

T
uis
s=1

.

A common assumption for this model is that the explanatory variables satisfy the following strict exogeneity condition:

Assumption 1 (Strictly exogenous regressors). E (ui|xi) = 0. Under this condition, the coefficient on the time-variant regressors, o, can be estimated by the fixed effects (within) estimator given by

    N T
 = arg min
 i=1 t=1

yit

-

1 T

T
yis
s=1

- (xit

-

1 T

T
xis)
s=1



2
.

14 Semi-parametric and Non-parametric

453

The fixed effects estimator  is consistent and asymptotically normally distributed under some standard regularity conditions. The fixed effects estimator  is
inconsistent, however, if the model is misspecified. The model is misspecified, for
example, if the deterministic part xit o is incorrectly specified or if Assumption 1 is not satisfied. To avoid potential specifications, one can use more flexible spec-
ifications. For instance, Robinson's (1988) partly linear function x1it o+ ho(x2it ) is more flexible than the linear function xit o; the additive nonparametric function h1o(x1it )+ h2o(x2it ) is more flexible than the partly linear function; and the nonparametric function ho(xit ) is the most flexible specification. All of these specifications can be nested in the following general index model:

m
 yit = v0(xit , o) + h j0(v j(xit , o)) + i + uit j=1

(14.2)

where v j(·) for j = 0, 1, . . . , m are known functions and h j0(·) = 0 for j = 1, 2, . . . , m are unknown functions. The parameter of interest now includes the finite dimensional parameter o and the infinite dimensional parameter

ho(·) = (h1o(·), · · ·, hmo(·)) .

Identification of model (14.2) may require some restrictions on (o, ho(·)). Notice that it is impossible to distinguish (hso(·), i) from (hso(·) - , i + ) for any constant  and for any s. Therefore, identification of the model parameter requires
some location restrictions such as h jo(0) = 0 for all j. Identification of the model parameter may also require scaling restrictions when the function v j(x,  ) for some j > 0 is homogenous of degree one in  . To illustrate, consider the simpler model

yit = ho(xit o) + i + uit .

Evidently we cannot distinguish (o, ho(·)) from (o, ho(·)), with ho(·) = ho(·/)
and o = o, for any nonzero constant . On the other hand, a scaling restriction such as oo = 1 or o = (1, 2o, . . . , ko) if the first coefficient in o is nonzero distinguishes (o, ho(·)) from (o, ho(·)) and hence should be imposed for identification purpose. Finally, identification of the model parameter may require exclusion restrictions when v j(x,  ) and vs(x,  ) for some s = j are homogenous of degree one in the regressors. To see this, consider

yit = h1o(x1it ) + h2o(x2it ) + i + uit .

Clearly (h1o, h2o) is not distinguishable from (h1o + g(x3it ), h2o - g(x3it )) for any function g if x1it and x2it contain the common regressor x3it . In this case, an exclusive restriction such as that x1it and x2it are mutually exclusive distinguishes (h1o, h2o) from (h1o + g(x3it ), h2o - g(x3it )) and hence must be imposed.
Suppose that the parameter of interest is identified up to some location restrictions. Under Assumption 1, a natural approach for estimating (o, ho) would be to apply the fixed effects estimation. The problem with this approach, however, is

454

C. Ai and Q. Li

that ho is infinite dimensional and cannot be estimated from the finite data points. One can use various nonparametric estimation techniques to estimate the unknown function ho such as kernel, nearest neighbor and series method. Model (14.2) has an additive structure. It is known that the nonparametric series approach is the most convenient to estimate restricted nonparametric regression models such as additive, multiplicative and monotonic restrictions. Therefore, in this chapter we will mainly focus on the series estimation method. For using kernel marginal integration method to estimate additive models, see Linton and Nielsen (1995), Newey (1994), and Tjostheim and Auestad (1994).
One can replace the unknown functions with finite dimensional series approximations and then apply the fixed effects estimation to the model as if the finite dimensional approximations are the correct specifications. Specifically, for each j, let p j(·) = (p j1(·), q j2(·), . . . , p jkj (·)) denote known basis functions that approximate h jo(·) in the sense that, for some constant vector  j of dimension k j × 1 and a scalar constant  j > 0,
h jo(·) = p j(·)  j + O(k-j  j ) = hk j j(·) + O(k-j  j ) .

The simplest series base function is the power series, {1, x, x2, . . . }. However, estimation based on power series can be sensitive to outliers and, for this reason, the power series is not typically used for nonparametric series estimation. Instead, the piecewise local polynomial spline is the most commonly used base function in nonparametric series estimation. An rth order univariate B-spline base function is given by (see Chui (1992, Chap. 4))

 Br(x|t0, . . . ,tr)

=

(r

1 - 1)!

r
(-1) j
j=0

r j

[max(0, x - t j)]r-1 ,

(14.3)

where t0, . . . ,tr are the evenly spaced design knots on the support of X, and a is
the distance between knots. When r = 2 (14.3) gives a piecewise linear spline, and
when r = 4, it gives piece-wise cubic splines (i.e., third order polynomials).
In finite sample applications, the approximating function hkj j(·) is obviously finite dimensional, depending on k j unknown coefficients  j. If each h jo(·) is re-
placed by hkj j(·), the total number of unknown coefficients in the approximating functions is k = k1 + . . . + km. Let d denote the dimension of  and denote  = (1, . . . , m) . Clearly, in order to estimate (o, ), the total number of coefficients to be estimated must be smaller than the sample size: k + d < NT . Moreover, the approximating functions must satisfy the location restriction hkj j(0) = p j(0)  j = 0. Notice that it is always possible to choose p j(·) so that p j(0) = 0. Without loss of
generality, we will assume that the basis functions p j(·) satisfy p j(0) = 0. Hence, there is no need to impose restriction on  j. Now treat each approximation p j(·)  j as if it is the correct specification of h jo(·). The fixed effects estimator (o, ) is
given by

 

  N T
= arg min
, j ,1 jm i=1 t=1

yit - v0(xit ,  ) - mj=1 p j(v j(xit ,  ))  j-

1 T

Ts=1(yis

-

v0(xis,  ) -

mj=1

p j(v j(xis,  ))

j)

2
.

(14.4)

14 Semi-parametric and Non-parametric

455

The unknown functions h jo(·) for j = 1, . . . , m are estimated by h j(·) = p j(·)  j. Obviously, in order for the fixed effects estimator to be consistent, the approxima-

tion errors must shrink to zero as sample size goes to infinity. This can happen if

we

require

each

kj

goes

to

infinity

as

N



but

at

a

slower

rate

so

that

kj N



0.

Shen (1997), Newey (1997) and Chen and Shen (1998) show that both  and h j(·),

j = 1, 2, . . . , m are consistent. They also show that, if k   at certain rate, the

estimator  is root-N consistent and asymptotically normally distributed, and any

smooth functional of h j(·) for j = 1, 2, . . . , m, is asymptotically normally distributed. Moreover, Shen (1997) shows that the usual covariance matrix of the fixed effects

estimator  is a consistent estimator of the asymptotic variance-covariance of  .

Denote  = ( ,  ) with  = ( ,  ) and denote

T
l1(yi, xi, ) =
t=1

yit - v0(xit ,  ) - mj=1 p j(v j(xit ,  ))  j-

2

.

1 T

sT=1(yis

- v0(xis, 

)

-

mj=1

p j(v j(xis,  ))

j)

Denote

(14.5)

 V =

N  2l1(yi, xi, )

-1
×

i=1   

 × N  2l1(yi, xi, ) -1
i=1   

N  l1(yi, xi, )  l1(yi, xi, )

i=1





= V V . V V

Then, V is the covariance matrix of the fixed effects estimator  if hkj j(·) is the correct specification of h jo(·) for fixed value of k. V is not the covariance matrix of
 when k   because P; (·) ; is not a root-N consistent estimator for h jo(·). But the upper-left block V is a consistent estimator for the covariance matrix of  .
The fixed effects estimator  can be used for testing model specifications. For example, applying the idea of Hong and White (1995), one can develop a consistent test for the more restrictive model (14.1) against the general index model (14.2). Specifically, let  denote the fixed effects estimator for model (14.1). The test statistic is constructed by comparing the fitted values under both models:

   N T
= i=1 t=1

yit

- xit 

-

1 T

T
(yis
s=1

- xis )







v0(xit ,  ) + mj=1 p j(v j(xit ,  ))  j - xit 

.

-

1 T

Ts=1(v0(xis,  )

+

mj=1

p j(v j(xis,  ))

j

- xit  )

456

C. Ai and Q. Li

The asymptotic distribution of  can be established with techniques similar to those developed in Hong and White (1995), Fan and Li (1996), and Li and Wang (1998). Applying the same idea, one can also develop a consistent test for the partly specified model (14.2) against the fully nonparametric panel data model:

yit = g(xit ) + i + uit ,

(14.6)

where g(·) is an unknown function satisfying g(0) = 0. Suppose that g(·) is approx-
imated by the basis functions q(xit ) = (q1(xit ), . . . ., qk(xit )) in the sense that there exist k × 1 vector  and a constant  > 0 such that

g(xit ) = q(xit )  + O(k- ) .

Suppose that q(0) = 0 is satisfied, then one can estimate  by the following fixed effects estimator:

   N T

 = arg min = 0

q(0) 

i=1 t=1

yit

-

q(xit )



-

1 T

T
(yis
s=1

-

q(xis)



)

2
.

g(xit ) is estimated by g(xit ) = q(xit )  . The test statistic in this case is





  N T

=



yit - v0(xit ,  ) - mj=1 p j(v j(xit ,  ))  j-



i=1 t=1

1 T

Ts=1(yis

-

v0(xis,  )

-

mj=1

p j(v j(xis,  ))

j)





 q(xit )  - v0(xit ,  ) - mj=1 p j(v j(xit ,  ))  j-  .

1 T

Ts=1(q(xis)



-

v0(xis,  ) - mj=1

p j(v j(xis,  ))

j)

Again, the asymptotic distribution of the test statistic can be established with the
techniques developed in Hong and White (1995).
One drawback of the fixed effects estimation is that it cannot estimate the effect
of time-invariant regressors. In some applications, researchers may want to estimate
the effect of the time-invariant regressors. To do so, researchers may impose the restrictions E (i|xi, wi) = wio and E (ui|xi, wi, i) = 0, and then apply the random effects estimation. The random effects estimator is consistent if both restrictions
are satisfied. The random effects estimator is inconsistent, however, if either or both restrictions are not satisfied; particularly if E (i|xi, wi) = wio is not satisfied. Thus, to avoid potential specification errors like this, it is better to leave the conditional mean E (i|xi, wi) = hm+1,o(xi, wi) unspecified and consider the following model

m
 yit = v0(xit , o) + h jo(v j(xit , o)) + hm+1,o(xi, wi) + (uit + i) j=1

(14.7)

where i = i - hm+1,o(xi, wi). The unknown function hm+1,o(xi, wi) can be estimated exactly the same way as other unknown functions. Let

14 Semi-parametric and Non-parametric

457

pm+1(x, w) = (pm+1,1(x, w), pm+1,2(x, w), . . . , pm+1,ko (x, w))

denote known basis functions that for some km+1 × 1 vector m+1 and some constant m+1 > 0 satisfying
hm+1,o(x, w) = pm+1(x, w) m+1 + O(km-+m1+1 ) .

Suppose that the following stronger condition is satisfied:
Assumption 1' (Strictly exogenous regressors). E (ui|xi, wi) = 0. Replace the unknown functions with their approximations. Under Assumption 1', the random effects estimator is given by

  N T

( , , m+1)

=

arg min
 , j,1 jm+1 i=1 t=1

yit - v0(xit ,  )- mj=1 p j(v j(xit ,  ))  j - pm+1(xi, wi) m+1

2
.

The unknown functions h j(·) for j = 1, . . . , m + 1 are estimated by h j(·) = p j(·)  j. Again, under conditions similar to those of Shen (1997) and Newey (1997), it can be shown that  is root-N consistent and asymptotically normally distributed, and any smooth functional of h j(·) for j = 1, . . . , m + 1, is asymptotically normally dis-
tributed. The asymptotic variance-covariance of  can be estimated consistently by the usual random effects covariance matrix of  . Denote  = ( ,  , m+1) ,  = ( ,  , m+1) and denote

T

m

2

  l2(yi, xi, ) = yit - v0(xit ,  ) - p j(v j(xit ,  ))  j - pm+1(xi, wi) m+1 .

t=1

j=1

Denote

 V =

N  2l2(yi, xi,  ) i=1   

-1
×

 ×

N  2l2(yi, xi,  ) i=1   

-1

N  l2(yi, xi,  )  l2(yi, xi,  )

i=1





= V V1 . V1 V11

Then, V is the usual random effects covariance matrix of  and the upper-left block V is the covariance matrix of  .
Notice that hm+1(xi, wi) estimates the part of the individual effect that is correlated with the explanatory variables. It can be used for testing specification such as E (i|xi, wi) = wio. In principal, a test statistic based on

N

2

 hm+1(xi, wi) - wi ,

i=1

458

C. Ai and Q. Li

where  is the random effects estimator of the coefficients on the time-invariant regressors, can be constructed with techniques similar to those developed by Hong and White (1995). The estimator hm+1(xi, wi), however, cannot be used for testing the presence of individual effect. A kernel-based test has been proposed by Li and Wang (1998) and implemented by Kniesner and Li (2002).
One potential criticism of model (14.7) is that the unknown function hm+1 depends on too many regressors. Unless researchers have a very large cross sectional sample, practically this model cannot be estimated with high precision. An alternative approach is to write E (i|wi) = hoo(wi). With i = i - hoo(wi) , the model now becomes

m
 yit = v0(xit , o) + h j(v j(xit , o)) + hoo(wi) + (uit + i) . j=1

(14.8)

Now let po(w) = (po1(w), po2(w), . . . , poko (w)) denote known basis functions that for some ko × 1 vector o1 and some scalar o > 0 satisfies

hoo(w) = po(w) o + O(ko-o ) .

Recall that l1(yi, xi, ) is given in (14.5). Denote

T

m

2

  l3(yi, xi, , o) = yit - v0(xit ,  ) - p j(v j(xit ,  ))  j - po1(wi) o .

t=1

j=1

The parameter  and the unknown coefficient vector o can be estimated jointly by solving the following equations:

N
i=1

 l1(yi, xi,  , ) 

=

0

,

N
i=1

 l3(yi, xi,  , , o)  o

=

0

.

In other words, ( , , o) is the moment estimator solving the above moment conditions. The unknown functions are estimated by h j(·) = p j(·)  j, j = 0, 1, . . . , m. Under some sufficient conditions, it can be shown that the estimator  and h j(·) are consistent and that  is asymptotically normally distributed and its asymptotic variance­covariance can be estimated consistently by the usual covaraince matrix of  , a corresponding submatrix of the usual covariance matrix of the moment estimator ( , , o).
The strict exogeneity condition rules out endogenous regressors and predeter-
mined regressors such as the lagged dependent variables. This condition, however,
can be relaxed if instrumental variables are available. To demonstrate, consider
model (14.2) again. Now eliminating the individual effect by simple
time-differencing, we obtain

14 Semi-parametric and Non-parametric

459

m
 yit - yis = v0(xit , o) + h jo(v j(xit , o)) j=1
m
 -[v0(xis, o) + h jo(v j(xis, o))] + uit - uis . j=1

Suppose that there exists the variables zits that satisfy
Assumption 2 (instruments). For all i and all s = t, E (uit - uis|zits) = 0. Suppose that Assumption 2 uniquely identifies the parameter of interest. Again, we will replace the unknown functions with their approximations. In addition, we will approximate the conditional expectation by finite number of unconditional expectations. Specifically, let rst (zits) = (rst1(zits), . . . , rstkst (zits)) denote known basis functions that approximate any measurable and square integrable function of zits. With h = (h1(·), . . . , hm(·)), define
m
 it ( , h) = yit - v0(xit ,  ) - h j(v j(xit ,  )) . j=1

Assumption 2 implies

E{[it (o, ho) - is(o, ho)] × rst (zits)} = 0 for any s = t .
The above unconditional moment conditions do not uniquely identify the parameter (o, ho) because ho is infinite dimensional. But, when these unknown functions are replaced by their approximations, we will assume that the following moment conditions

E{(it (o, hk) - is(o, hk)) × rst (zits)} = 0 for any s = t ,
with hk = (p1(·) 1, . . . , pm(·) m), uniquely identifies (o, ). This identification requires that the number of moment conditions s=t kst is larger than the number of coefficients to be estimated, k + d . Let R1i( , ) denote the column vector formed by
(it (o, hk) - is(o, hk)) × rst (zits) for all s = t .
Then, E{R1i( , )} = 0. The Generalized Method of Moments (hereafter GMM) estimator based on these moment restrictions is given by

N

 ( IV , IV ) = arg min  ,

R1i( , )
i=1

N
 R1i( , ) .
i=1

The

unknown

functions

are

estimated

by

hIjV

=

p

j (·)



IV j

for

all

j.

Ai

and

Chen

(2003, 2005) show that  IV is root-N consistent and asymptotically normally

distributed and its asymptotic variance-covariance matrix is estimated consistently

by the usual GMM covariance matrix of  IV . Recall that  = ( ,  ) . Denote

460

C. Ai and Q. Li

V IV =

N  R1i( IV , IV )

i=1



-1

N  R1i( IV , IV )

i=1



N

 ×

R1i( IV , IV ) × R1i( IV , IV )

i=1

×

N  R1i( IV , IV )

i=1



-1

N  R1i( IV , IV )

i=1



= VIV VIV . VIV VIV

Then, V IV is the usual covariance matrix of the GMM estimator . The upper-left block VIV is the covariance matrix of  IV .
For model (1.7), we assume that the instrumental variables zit satisfies the following assumption.
Assumption 3 (instruments). For all i and t, E (i + uit |zit ) = 0. Let rt (zit ) = (rt1(zit ), . . . , rtkt (zit )) denote known basis functions that approximate any measurable and square integrable function. Suppose that the number of moment conditions t kt is larger than the number of coefficients k + d + km+1. Let R2i( , , m+1) denote the column vector formed by

m
 yit - v0(xit , o) - p j(v j(xit , o))  j - pm+1(xi, wi) m+1 × rt (zit ) for all t . j=1

Then, E{R2i( , )} = 0. The GMM estimator based on these moment restrictions is given by

N

 (

IV

,



IV

,

mIV+1)

=

arg

min
 ,,m+1

R2i( , , m+1)
i=1

N
 R2i( , , m+1) .
i=1

The

unknown

functions

are

estimated

by

hIjV

=

p

j (·)



IV j

for

all

j.

Again,

Ai

and

Chen (2003, 2005) show that  IV is root-N consistent and asymptotically normally

distributed and its asymptotic variance­covariance is estimated consistently by the

usual GMM covariance matrix of  IV . Similar estimators also can be constructed

for model (14.8). See Das (2003) for further extensions.

14.2.2 Multiplicative Effect
The additive structure of the model above, though common in empirical work, is restrictive in the sense that the model assumes that the marginal effects of the explanatory variables are identical for all individuals with the same explanatory

14 Semi-parametric and Non-parametric

461

variable values. This restriction can be relaxed with a multiplicative individual effect such as in the following model
m-1
 yit = v0(xit , o) + h j(v j(xit , o)) + ihm(vm(xit , o)) + uit . j=1
In this model, the marginal effects of the explanatory variables vary with individuals via the unknown function hm(·). We now illustrate how the estimators discussed above are extended to models of this sort.
For identification purpose, the function hm(·) is assumed to satisfy the normalization: hm(0) = 1. Because the individual effect is multiplicative, simple timedifferencing will not eliminate the individual effect. However, it is still possible to solve for the individual effect from one period (t) and then substitute it into another period (s < t) to obtain:

m-1
 yit - v0(xit , o) - h j(v j(xit , o)) - uit hm(vm(xis, o)) j=1 m-1
 = yis - v0(xis, o) - h j(v j(xis, o)) - uis hm(vm(xit , o)) . j=1
Define
m-1
 its( , h) = yit - v0(xit ,  ) - h j(v j(xit ,  )) hm(vm(xis,  )) . j=1
Suppose that

E{its(o, h0) - ist (o, ho)|zits} = 0 for all s = t .
Let R3i( , ) denote the column vector formed by
(its(o, hk) - ist (o, hk)) × rst (zits) for all s = t . Then, E{R3i( , )} = 0. The GMM estimator based on these moment restrictions is given by

N

 ( IV , IV ) = arg min  ,

R3i( , )
i=1

N
 R3i( , ) .
i=1

The

unknown

functions

are

estimated

by

hIjV

=

p

j

(·)



IV j

for

all

j.

It

follows

from

Ai and Chen (2003, 2005) that  IV is root-N consistent and asymptotically normally

distributed and its asymptotic variance­covariance is estimated consistently by the

usual GMM covariance matrix of  IV .

The marginal effect of the time-invariant regressors can be estimated by simple

least squares. Substituting for the individual effect i = ho(xi, wi) + i yields:

462

C. Ai and Q. Li

m-1
 yit = v0(xit , o) + h j(v j(xit , o)) + ho(xi, wi)hm(vm(xit , o)) j=1
+ihm(vm(xit , o)) + uit .

Suppose that E{uit |xi, wi} = 0 is satisfied. Then the coefficients o and  = (0, . . . , m) can be estimated by the following nonlinear least squares:

  N
( , ) = arg min

yit - v0(xit ,  ) - mj=-11 q j(v j(xit ,  ))  j

2
.

i=1 t<s -[ qm(vm(xis,  )) m]po(xi, wi) o

The asymptotic properties of the estimator can be derived by applying the results of Shen (1997).

14.3 Nonlinear Panel Data Model
A key structure of the linear panel data model, which is exploited by all of the estimators reviewed above, is that the observed dependent variable is a linear function of the individual effect. This linear relationship allows us to eliminate the individual effect through simple time-differencing. It also allows us to estimate the conditional mean E (i|xi, wi) function under the condition that (xi, wi) is meanindependent of (i, uit ) (i = i - E(i|xi, wi)). If the observed dependent variable is a nonlinear function of the individual effect, the individual effect cannot be eliminated through simple time-differencing and E (i|xi, wi) cannot be estimated consistently under the mean-independence condition. Panel data limited and qualitative dependent variable models are important examples where the observed dependent variable is a nonlinear function of the individual effect. In these models, some "nonlinear-differencing" techniques are required to remove the individual effect. The aim of the rest of this chapter is to review the "nonlinear-differencing" techniques proposed in the literature. We first review the panel data censored regression model, also known as Type I Tobit model, then the panel data discrete choice model, and lastly the panel data sample selection model, also known as Type II Tobit model.

14.3.1 Censored Regression Model

We begin with the panel data censored regression model, which is given by

yit = xit o + i + uit yit = max{0, yit },

i = 1, 2, . . . , N; t = 1, 2, . . . , T

(14.9)

14 Semi-parametric and Non-parametric

463

In this model, the latent dependent variable yit is linear in the individual effect i, but the observed dependent variable yit is nonlinear in i, with the nonlinearity arising from censoring. A simple time-differencing of the observed dependent variable does not remove i. To see why simple time-differencing of the observed dependent variables does not remove the individual effect, for any period t and at the true value o, write
yit - xit o = max{yit - xit o, -xit o} = max{i + uit , -xit o} .

Clearly, yit - xit o is the censored error term i + uit , with -xit o as the censoring value. Similarly, for any period s,

yis - xiso = max{i + uis, -xiso}
is the censored error term i + uis, with -xiso as the censoring value. Applying simple time-differencing, we obtain:

yit - xit o - (yis - xiso) = max{i + uit , -xit o} - max{i + uis, -xiso} .
The individual effect is clearly not eliminated by simple time-differencing. From the point of view of estimating the unknown parameter o, it is not neces-
sary to remove the individual effect at every data point. As long as the differenced error term: max{i + uit , -xit o} - max{i + uis, -xiso} has a zero conditional mean given the explanatory variables, the parameter o can be estimated consistently by standard regression techniques. Unfortunately, the differenced error term does not have a zero conditional mean when: (i) the error terms uit and uis, conditional on the regressors and the individual effect, are not identically distributed or (ii) the censoring values -xit o and -xiso are not identical. Thus, to obtain a consistent estimator of o, condition (i) and (ii) must not be satisfied. Condition (i) is ruled out by the following condition:
Assumption 4. The error terms uit and uis, conditional on (xit , xis, i), are identically distributed. Condition (ii) is satisfied by artificially censoring the observed dependent variables so that both error terms are censored at the same value max{-xit o, -xiso}. Specifically, define the artificially censored error terms as:

e(yit - xit o, xiso) = max{yit - xit o, -xiso} = max{i + uit , -xit o, -xiso} ,

e(yis - xiso, xit o) = max{yis - xiso, -xit o} = max{i + uis, -xit o, -xiso} .
It then follows from Assumption 4 that e(yit - xit o, xiso) and e(yis - xiso, xit o), conditional on the explanatory variables and the individual effect, are identically distributed. This in turn implies that

464

C. Ai and Q. Li

E{e(yit - xit o, xiso) - e(yis - xiso, xit o)|xit , xis} = 0 .

(14.10)

o now can be estimated consistently from the conditional moment restriction (14.10) by standard regression techniques such as GMM, provided some identification condition is satisfied. Since (14.10) is obtained by applying simple time-differencing after some nonlinear transformation of the observed dependent variable, this approach is called "nonlinear-differencing".
Although the true value o can be estimated consistently from (14.10) by GMM, estimation methods using zero conditional mean conditions are often more complex than the methods using zero unconditional mean conditions. A simpler and better approach is to find a convex objective function whose first order condition coincides with some unconditional moment conditions implied by (14.10). For instance, the objective function
A(0) d=ef E{r(yit , yis, (xit - xis) o)} ,

with

 

y21/2

-

y1

-

y1y2

if   -y2 ;

r(y1

,

y2

,

)

=



(y1 - y2 - y22/2 + y2

)2/2 - y1y2

if - y2 <  < y1 ; if y1  i ;

satisfies the following unconditional moment condition



A( 

)

|

=0

=

E

(e(yit - xit o, xiso) - e(yis - xiso, xit o))(xit - xis)

=0,

which is obviously implied by (14.10). Given that r(y1, y2, )  0 for all (y1, y2, ), this suggests a nonlinear least squares estimator of o:

N

   = arg min

r

 i=1 t<s

yit , yis, (xit - xis) 

.

(14.11)

Under some sufficient conditions, Honore´ (1992) shows that  is consistent and asymptotically normally distributed and its asymptotic variance-covariance is estimated consistently by the usual nonlinear least squares covariance matrix of  :

 N 2r
V =   i=1 t<s

yit , yis, (xit - xis)   2

-1 (xit - xis)(xit - xis) 





×



iN=1

t<s



r(yit ,yis,(xit -xis)


)

(xit

-

xis

×

t<s

 r(yit ,yis,(xit -xis)


)

(xit

-

xis)

)



 N 2r
×  i=1 t<s

yit , yis, (xit - xis)   2

-1 (xit - xis)(xit - xis)  .

14 Semi-parametric and Non-parametric

465

Notice that Assumption 4 implies that

E  (e(yit - xit o, xiso)) -  (e(yis - xiso, xit o))|xit , xis = 0

(14.12)

holds for any function  (·). Honore´'s (1992) estimator clearly does not use all
information. More efficient estimator can be constructed from (14.12). For some
integers k1 and k2, let q(u) = (q1(u), q2(u), . . . , qk1 (u)) denote known basis functions that approximate any square integrable function of u, and let p(xit , xis) = (p1(xit , xis), p2(xit , xis), . . . , pk2 (xit , xis)) denote known basis functions that approximate any square integrable function of (xit , xis). Condition (14.12) implies

E{(q(e(yit - xit o, xiso)) - q(e(yis - xiso, xit o)))  p(xit , xis)} = 0, for t > s

where  denotes the Kronecker product. Denote

(yi, xi,  ) = vec

[q(e(yit - xit  , xis )) - q(e(yis - xis , xit  ))]  p(xit , xis), t = s + 1, . . . , T ; s = 1, 2, . . . , T - 1

.

The unknown parameter o can be estimated by GMM:

N

N

   GMM = arg min 

(yi, xi,  )
i=1

-1

(yi, xi,  )

i=1

,

where  is some known positive definite matrix, or by the empirical likelihood

method:

N

 

EL

=

arg min max ln(1 + 
  i=1

(yi, xi,  ))

.

By allowing k1 and k2 to grow with sample size, Ai (2005) shows that  GMM is consistent and asymptotically normally distributed and that it is efficient for model (14.12). The asymptotic variance­covariance of  GMM is estimated consistently by the usual GMM covariance matrix of  GMM. The asymptotic distribution of  EL also can be derived with the techniques developed in Donald, Imbens, and Newey (2004).
A drawback of the GMM estimation is that the objective function may not be glob-
ally convex and may have many local minimizers. But this problem can be resolved
with Honore´'s (1992) estimator as the starting value.
Assumption 4 can be strengthened by requiring the error terms to satisfy the
conditional pairwise exchangeability condition:

Assumption 4'. The pair (uit , uis) is identically distributed as (uis, uit ) conditional on (xit , xis, i). This condition implies that

E  e(yit - xit o, xiso)] -  [e(yis - xiso, xit o) | xit , xis = 0

(14.13)

for any odd function  (·). Since  (u) = u is an odd function, Honore´'s (1992) estimator is still consistent under Assumption 4'. Other consistent estimators can

466

C. Ai and Q. Li

be constructed analogously for arbitrary odd function. See Honore´ and Kyriazidou (2000) and Charlier et al. (2000) for details. Efficient GMM estimator for this model can also be constructed in the same manner as for (14.12) by requiring that q(u) are odd functions.
Under Assumption 4, Honore´'s (1992) estimator can be easily extended to the following partially additive panel data Tobit model:

yit =

x0it o + mj=1 h j(x jit ) + i + uit , if the RHS > 0; and

0

otherwise,

where h j(·) are unknown functions. For identification purpose, we assume that the
unknown functions satisfy the location restriction h j(0) = 0 for all j and the exclusive restriction that x1it , . . . , xmit are mutually exclusive. Suppose that each h j(·) is approximated by the linear sieve pkj j (·)  j, where pkj j (·) is a vector of approximating functions satisfying pkj j (0) = 0. The unknown parameter o and the coefficients  = (1, . . . , m) are estimated by

   (

,



)

=

arg

min
 ,

N i=1

t<s

r(yit

,

yis,

(x0it

-

x0is)



+

m
(
j=1

pkj

j

(x

jit

)

-

pkj

j

(x

jis))



j)

.

The unknown functions are estimated by h j(·) = pkj j (·)  j. Ai and Li (2005) show
that the estimator ( , h1, . . . , hm) is consistent and derive its asymptotic distribution. The asymptotic variance­covariance of  is estimated consistently by the usual nonlinear least squares covariance matrix of  , which is the corresponding submatrix of the nonlinear least squares covariance matrix of ( , ). The usual nonlinear least squares covariance matrix of , however, should not be viewed as the estimator for the asymptotic variance­covariance of  because  does not have a asymptotic normal distribution.
By exploiting all moment conditions, efficient GMM estimator for this model
can be constructed in the same manner as for (14.12). Let xit denote the union of x0it , . . . , xmit . Denote

(yi, xi,  , ) = vec

(A(yit , xit , xis,  , ) - A(yis, xis, xit ,  , ))  p(xit , xis) t = s + 1, . . . , T ; s = 1, . . . , T - 1

where

  A(yit , xit , xis,  , ) = q

e

m
yit - x0it  -

pkj j (x jit )  j, x0is + m

pkj j (x jis)  j

.

j=1

j=1

The unknown parameter o is either estimated by GMM:

N

N

  ( GMM,  GMM) = arg min  ,

(yi, xi,  , )
i=1

-1 (yi, xi,  , )
i=1

14 Semi-parametric and Non-parametric

467

or by empirical likelihood:

N

  EL = arg min max ln  ,  i=1

1+

(yi, xi,  , )

.

By allowing k1 and k2 to grow with sample size at certain rates, Ai and Li

(2005) show that  GMM is consistently and asymptotically normally distributed and

h

j

=

pkj

j

(·)



GMM j

is

consistent.

The

asymptotic

variance­covariance

of



GMM

is

es-

timated consistently by the usual GMM covariance matrix of  GMM, which is the

corresponding submatrix of the usual GMM covariance matrix of ( GMM, GMM).

Assumption 4 is weaker than the one we normally make for Tobit models. It

permits dependent data and allows for dependence of the error term on the ex-

planatory variables (e.g., heteroskedastic error). But it is still restrictive. It rules

out predetermined or endogenous explanatory variables, for example. To allow

for predetermined or endogenous explanatory variables, we must modify the non-

linear least squares and GMM procedures reviewed above. To illustrate, consider

the case of predetermined regressors first. Denote xit = (x1it , x2it ) and decompose o = (1o, 2o) accordingly. Suppose that x2it is predetermined (e.g. lagged dependent variable). Replace Assumption 4 with

Assumption 5. For any t > s, the error terms uit and uis, conditional on (x1it , xis, i), are identically distributed.
This condition is obviously weaker than Assumption 4. To see why Assump-
tion 5 permits predetermined regressors, suppose that uit is independent of x1i = (x1i1, x1i2, . . . , x1iT ) and x2ti = (x2i1, x2i2, . . . , x2it ). Then, for any s < t, (uit , uis) is independent of (x1i, x2si) and Assumption 5 is satisfied as long as uit and uis are identically distributed.
The problem with the predetermined regressors is that the censoring value xit o is correlated with uis. If both censoring values are used to censor both error terms, the censored error terms will not have the same distribution. One way to resolve this
difficulty is to drop the predetermined regressors from xit o. Suppose that x2it 2o  0 holds with probability one. Define:

(yit - xit o, x1it 1o, xiso) = max{yit - xit o, -x1it 1o, -xiso} = max{i + uit , -x1it 1o, -xiso};
(yis - xiso, x1it 1o) = max{yis - xiso, -x1it 1o} = max{i + uis, -x1it 1o, -xiso} .

Assumption 5 and the condition x2it 2o  0 with probability one imply (yit - xit o, x1it 1o, xiso) and (yis - xiso, x1it 1o) are identically distributed given (x1it , xis). This in turn implies:

E  ((yit - xit o, x1it 1o, xiso)) -  ((yis - xiso, x1it 1o))|x1it , xis = 0 (14.14)

for any function  (·). The parameter o now can be estimated from the conditional moment condition (14.14) by GMM or the empirical likelihood. Specifically, let p(x1it , xis) = (p1(x1it , xis), p2(x1it , xis), . . . , pk2 (x1it , xis)) denote known

468

C. Ai and Q. Li

basis functions that approximate any square integrable function of (xit , xis). Condition (14.12) implies

E{(q((yit - xit o, x1it 1o, xiso)) - q((yis - xiso, x1it 1o)))  p(x1it , xis)} = 0 , for t > s

Denote





 [q((yit - xit o, x1it 1o, xiso)) - q((yis - xiso, x1it 1o))] 

(yi, xi,  ) = vec 

p(x1it , xis), t = s + 1, . . . , T ; s = 1, 2, . . . , T - 1

 .

The unknown parameter o is either estimated by GMM:

N

N

   GMM = arg min 

(yi, xi,  )
i=1

-1

(yi, xi,  )

i=1

,

where  is some known positive definite matrix, or by empirical likelihood:

N

  EL

=

arg min max ln(1 + 
  i=1

(yi, xi,  ))

.

The asymptotic distribution of  GMM can be derived exactly the same way as in Honore´ and Hu (2004), while the asymptotic distribution of  EL can be derived exactly the same way as in Donald, Imbens, and Newey (2004).
The condition that x2it 2o  0 holds with probability one is critical for the above estimator. This condition appears very restrictive. Fortunately in most applications,
the predetermined regressors are lagged dependent variables that are always non-
negative and usually have positive coefficients. In those applications, this condition is imposed through restriction on the coefficients 2  0.
Next, consider the case where (x2it , x2is) are endogenous. Let (zit , zis) denote the instrumental variables for (x2it , x2is). Depending on the restrictions we impose on the endogenous regressors, we may make one of the following two assumptions:

Assumption 6. The two terms (uit , x2it , x2is) and (uis, x2it , x2is), conditional on(x1it , x1is, zit , zis, i), are identically distributed.
Assumption 6'. The error terms uit and uis, conditional on (x1it , x1is, zit , zis, i), are identically distributed.
Under Assumption 6, e(yit - xit o, xiso) and e(yis - xiso, xit o), conditional on (x1it , x1is, zit , zis, i), are identically distributed, implying that

E{ (e(yit - xit o, xiso)) -  (e(yis - xiso, xit o))|x1it , x1is, zit , zis} = 0 (14.15)

14 Semi-parametric and Non-parametric

469

for any function  (·). Thus o can be estimated consistently from the conditional
moment restriction (14.15) by GMM or empirical MLE. Notice that condition
(14.15) does not require that x2it 2o  0 and x2is2o  0 hold with probability one. If we require that x2it 2o  0 and x2is2o  0 hold with probability one, we can drop x2it and x2is from the censoring values and modify the censored error terms as:

(yit - xit o, x1it 1o, x1is1o) = max{yit - xit o, -x1it 1o, -x1is1o} = max{i + uit , -x1it 1o, -x1is1o} ;
(yis - xiso, x1it 1o, x1is1o) = max{yis - xiso, -x1it 1o, -x1is1o} = max{i + uis, -x1it 1o, -x1is1o} .

Assumption 6' implies

E

 ((yit - xit o, x1it 1o, x1is1o)) |x1it , x1is, zit , zis = 0

- ((yis - xiso, x1it 1o, x1is1o))

(14.16)

for any function  (·). Again, o can be estimated consistently from the conditional moment restriction (14.16) by GMM or empirical MLE. The condition that x2it 2o  0 and x2is2o  0 hold with probability one is more restrictive then previous case since the endogenous regressors may have negative coefficients.
The ideas described above can be easily extended to the following dynamic latent
dependent variable model:

yit = oyit-1 + xit o + i + uit yit = max{0, yit } .

(14.17)

To illustrate, suppose that xit is strictly exogenous. Note that this model differs from other Tobit models in that the lagged latent dependent variable may not be observed. First, we select a subsample in which yit-2 > 0 for some t. Then

yit-1 - oyit-2 - xit-1o = max{i + uit-1, -oyit-2 - xit-1o}; yit - oyit-1 - xit o = max{i + uit , -oyit-1 - xit o} .

It is reasonable to assume that the variable yit-2 is independent of uit-1 and uit . yit-1 (and hence yit-1), on the other hand, is not independent of uit-1. Assume that o > 0. Denote the censored error terms as
e(yit - oyit-1 - xit o, oyit-2 + xit-1o, xit o) = max{yit - oyit-1 - xit o, -oyit-2 - xit-1o, -xit o} = max{i + uit , -oyit-2 - xit-1o, -xit o};
e(yit-1 - oyit-2 - xit-1o, xit o) = max{yit-1 - oyit-2 - xit-1o, -xit o} = max{i + uit-1, -oyit-2 - xit-1o, -xit o} .

470

C. Ai and Q. Li

Although the above censored error terms are identically distributed if uit and uit-1 are identically distributed, the first censored error term is not feasible since the
lagged latent dependent variable is not observed. One way to resolve this difficulty is to impose the condition yit-1 > 0 so that the latent dependent variable is observed. However, because uit is independent of, but uit-1 is not independent of, yit-1 > 0, the constraint yit-1 > 0 imposes restriction on uit-1 but not on uit and, as a result, the two censored error terms are not identically distributed. To ensure that both cen-
sored error terms are identically distributed, we must impose the same constraints on uit-1 and uit . For instance, we can require that the errors to satisfy

min{i + uit-1, i + uit } = max{-oyit-2 - xit-1o, -xit o} .

(14.18)

This condition implies that

yit-1  max{0, oyit-2 + xit-1o - xit o}  0 and hence yit-1 > 0; yit  max{oyit-1 + xit o - oyit-2 - xit-1o, oyit-1} > 0 .

Assumption 7. For any t, the error terms uit and uit-1, conditional on (yit-2 > 0, xit , xis, i), are identically distributed.
Under assumption 7, and conditional on

Aits = {yit-2 > 0, yit-1  max{0, oyit-2 + xit-1o - xit o} and yit  max{oyit-1 + xit o - oyit-2 - xit-1o, oyit-1} ,

e(yit - oyit-1 - xit o, oyit-2 + xit-1o, xit o) and e(yit-1 - oyit-2 - xit-1o, xit o) are identically distributed. This leads to the following conditional moment conditions:

E

1(Aits) ×

 (e(yit-1 - oyit-2 - xit-1o, xit o))-  (e(yit - oyit-1 - xit o, oyit-2 + xit-1o, xit o))

| xit , xis

=0

for any function  (·). The parameter o can now be estimated from the above conditional moment restriction, and the asymptotic properties of the estimator can be derived exactly the same way as in Hu (2002).

14.3.2 Discrete Choice Model
A key aspect of the "nonlinear differencing" technique developed for the Tobit model is that the latent dependent variable is observed partly so that trimming can be used to restore the symmetry of the distribution of the observed data. This trick does not work for the panel data discrete choice model because the latent dependent variable is not observed at all. A new "nonlinear differencing" approach must be developed. In this subsection, we review those new "nonlinear differencing" techniques. We begin with the panel binary choice model given by

14 Semi-parametric and Non-parametric

471

yit = 1 xit o + i + uit > 0 , i = 1, 2, . . . , n; t = 1, 2, . . . , T

(14.19)

where xit is a vector of time-varying explanatory variables, i is an individual specific intercept, and uit is the error term. Notice that, for any two time periods t and s, simple time-differencing gives:

yit - yis = 1 xi,t o + i + uit > 0 - 1 xi,so + i + uis > 0 .

Taking expectation, we obtain:

E{yit - yis|xit , xis, i} = Pr(uit > -xi,t o - i|xit , xis, i) - Pr(uis > -xi,so - i|xit , xis, i) .

Obviously, simple time-differencing does not eliminate the individual effect unless ocxio,nt rtroheel=artiexgdi,shtwoh.iatMhndasgnsnisdk(exi i,(ht1a9os8-t7h)ex,i,sshaomow)e.evsBiegarsn,eoadbssoxeni,rtvteohsi-sthxoai,bst,seoifr,vtthahteeinodn(i,fyfiMte-reannycsiske)diispprprooopbsoaitsbievildeiltyya maximum score estimator that maximizes the sample correlation:

n

   = arg max

(yit - yis)  sgn((xit - xis)  ) .

  =1 i=1 s<t

Clearly, Manski's estimator is defined on the differenced data through sign function. Since sign function is nonlinear, this technique is still called as "nonlinear differencing".
To ensure that the differenced probability has the same sign as xi,t o - xi,so, we impose the following condition:
Assumption 8. For any t > s, the error terms uit and uis, conditional on (xit , xis, i), are identically distributed.

Like Assumption 4 for the Tobit model, Assumption 8 is weaker than the one we normally make for the binary choice model. For example, it does not require specifying the error distribution and permit dependent data and heteroskedasticity. Under some additional conditions and with some scale normalization on the parameter, Manski (1987) shows that the maximum score estimator is consistent. However, his estimator is not root-N consistent and is not asymptotically normally distributed. The nonnormal asymptotic distribution of his estimator is the result of the nonsmooth objective function. If the smoothing technique suggested by Horowitz (1992) is used here, it can be shown that the resulting estimator is asymptotically normally distributed, although the rate is still slower than root-N, (see Kyriazidou (1997) and Charlier et. al. (1995) for details).
Extension of Manski's idea to the following nonparametric panel data model

yit = 1 {h(xit ) + i + uit > 0} , i = 1, 2, . . . , N; t = 1, 2, . . . , T

(14.20)

where h(·) is of unknown form, is straightforward. Let pk(x)  = p1(x)1 + . . . + pk(x)k denote the approximation to h(x). Denote:

472

C. Ai and Q. Li

n

   = arg max

(yit - yis)  sgn((pk(xit ) - pk(xis)) ) .

 =1 i=1 s<t

and h(x) = pk(x) . Then using the techniques developed by Shen (1997) and
Manski (1987), it can be shown that h(x) is consistent under both the sup and L2 norm.
Return to model (14.19). Like Assumptions 4, 8 rules out the predetermined explanatory variables such as the lagged dependent variables. If the predetermined explanatory variables are allowed for, the trick used by Honore´ and Kyriazidou (2000) can be used here to estimate model (14.19). Specifically, decompose xit o = x1it 1o + x2it 2o. Suppose that x2it are the predetermined explanatory variables. Consider three periods r < s < t. The insight of Honore´ and Kyriazidou (2000) gives the following estimator:

n
   = arg max  i=1 r<s<t

K

x1is -x1it n

× (yis - yir)×

sgn((x1is - x1ir) 1 + (x2it - x2ir) 2)

where K(·) denotes the kernel function and n denotes the bandwidth. Again, only
consistency of this estimator is proved by Honore´ and Kyriazidou (2000). For the nonparametric model (1.20) with h(x) = h1(x1) + h2(x2), let pk11 (x1) 1 =
p11(x1)11 +. . .+ p1k1 (x1)1k1 denote the approximation to h1(x1) and pk22 (x2) 2 = p21(x2)21 + . . . + p2k2 (x2)2k2 denote the approximation to h2(x2). Then, Honore´ and Kyriazidou's (2000) idea gives the following estimator:

n

   = arg max
  i=1 r<s<t



 sgn

K

x1is -x1it n

× (yis - yir)×

(p1k1 (x1is) - pk11 (x1ir)) 1 + (pk22 (x2it ) - p2k2 (x2ir)) 2

.

Again, it can be shown that h j(x) = pkj(x)  j is consistent. The important question is whether we can achieve root-N consistency and asymp-
totic normality. Anderson (1970) answered this question by considering the Logit
version of model (14.19). His "nonlinear differencing" idea is based on a conditional maximum likelihood approach. Define i = tT=1 yit which takes values in {0, 1, . . . , T } (since yit  {0, 1}). Also define it = 1 if yit = 1, and it = 0 otherwise.
Then the conditional likelihood estimator is given by

n
 = arg max log
 i=1

exp(tT=1 yit xit  ) i1+...+iT =i exp(tT=1 dit xit  )

,

where i1+...+iT =i denotes sum over all possible combinations of (i1, . . . , liT ) with tT=1 it = i (= Ti=1 yit ). Anderson showed that the conditional maximum likelihood estimator is root-N consistent and asymptotically normally distributed.

14 Semi-parametric and Non-parametric

473

This "nonlinear differencing" idea is easily extended to the Logit version of model (1.20), with

n
 = arg max log
 i=1

exp(tT=1 yit pk(xit ) ) i1+...+iT =i exp(tT=1 it pk(xit ) )

and h(x) = pk(x) . The consistency and the asymptotic normality of smooth functionals of h(x) can be proved exactly as in Shen (1997).
The "nonlinear differencing" idea for the Logit model also can be extended to allow for the predetermined explanatory variables in the panel data Logit model. As showed in Honore´ and Kyriazidou (2000), at least three periods of data are required; and the model parameter is estimated by:

  n



=

arg

max


i=1

r<s<t

1{yir

+

yis

= 1} K

x1is - x1it n



log

[exp((x1ir - x1is) 1 + (x2ir - x2it ) 2)]yir 1 + exp((x1ir - x1is) 1 + (x2ir - x2it ) 2)

.

However, this estimator is not root-N consistent, it has the usual nonparametric kernel estimation rate of convergence.
It is clear from the above discussions that the maximum score estimator is not root-N consistent but it imposes the weakest restrictions on the distribution of the error terms. On the other hand, the conditional maximum likelihood estimator is root-N consistent but imposes the strongest assumption on the distribution of the error terms. A natural question is were there exist other restrictions on the error distribution that permit root-N consistent estimator for the model parameter. Unfortunately, Chamberlain (1993) gives a surprisingly negative answer to this question. He showed that even if the errors are i.i.d. and independent of the explanatory variables and the individual effects, the model parameter can be estimated root-N consistently only in the Logit case. Hahn (2001) considers the semiparametric information bound in dynamic panel Logit models with fixed effects. Hahn shows that the conditional maximum likelihood estimator is not semiparametrically efficient for models with only the lagged dependent variable. For more general models with regressors include time dummies, Hahn shows that the semiparametric information bound is singular, therefore, root-N consistent estimation is infeasible in more general models. Therefore, to obtain a root-N consistent estimator, it is clear that some additional assumptions must be imposed on the correlation between the explanatory variables and the individual effect. Lee (1999) takes differences across individuals in addition to the time-difference proposed by Manski (1987). Lee shows that the resulting estimator is a root-N consistent semiparametric estimator that does not depend on a smoothing parameter. Honore´ and Lewbel (2002) require that there exists a "special regressor", which is continuous with bounded support and is independent of the individual effect and the error term. The role of this "special regressor" is to pull the individual effect out of the nonlinear function. Specifically,

474

C. Ai and Q. Li

write xit o = x1it + x2it 1o, where x1it is that "special regressor" and its coefficient is normalized to unity for identification purpose, and x2it denote the predetermined regressors. For any two periods r > s, let zis denote the instrumental variables consisting of all predetermined variables up to time s. Under some conditions on
the special regressor, Honore´ and Lewbel showed that

E

zis(yit - 1{x1it > 0}) ft (x1it |x2it , zis)

= zisx2it 1o + E{zisi}, t = r, s

where ft denotes the conditional density of x1it conditional on (x2it , zis). The individual effect now can be eliminated through simple time-differencing between period r and period s, and hence the parameter can be estimated by simple instrumental variable estimation. For details see Honore´ and Lewbel (2002). Even when the parameters in a dynamic discrete choice model are not identified, it may be possible to bound the parameters in a narrow region, see Honore´ and Tamer (2005) for more details on how to find the bounds for the parameters in dynamic discrete choice panel data models.

14.3.3 Sample Selection Model

The panel data sample selection model is given by:

yit = dit (xit o + i + uit ), dit = 1{zit o + i + it > 0} ,

(14.21)

where xit and zit are explanatory variables, i and i are individual effects, and uit and it are the remainder error terms. This model consists of a binary selection equation and a regression equation which is sometimes censored. Thus, it is more
complicated than the panel data binary choice model and the panel data Tobit model. The coefficient o can be estimated consistently by any of the methods described above for the panel discrete choice model. But to estimate the coefficient o, a new differencing technique is needed. For any two periods t > s, note that, conditional on dit = 1, dis = 1, xit , zit , xis, zis, i, i:

yit |dit =1,dis=1,xit ,zit ,xis,zis,i,i = xit o + i + uit |dit =1,dis=1,xit ,zit ,xis,zis,i,i , yis|dit =1,dis=1,xit ,zit ,xis,zis,i,i = xiso + i + uis|dit =1,dis=1,xit ,zit ,xis,zis,i,i .
If uit |dit =1,dis=1,xit ,zit ,xis,zis,i,i and u |is dit =1,dis=1,xit ,zit ,xis,zis,i,i are identically distributed, then we have
E{ (yit - xit o)|dit = 1, dis = 1, xit , zit , xis, zis, i, i} = E{ (yis - xiso)|dit = 1, dis = 1, xit , zit , xis, zis, i, i} ,

14 Semi-parametric and Non-parametric

475

for any function  (·). And the parameter o can be estimated from the above conditional moment restriction by standard regression techniques such as GMM.
The problem with this approach is that uit |dit =1,dis=1,xit ,zit ,xis,zis,i,i and uis|dit =1, dis = 1, xit , zit , xis, zis, i, i are not identically distributed unless zit o = ziso. This leads to the following estimator

   = arg min 

i

dit disK
s<t

(zit - zis)  n

[(yit - xit  ) - (yis - xis )]2

where  is a consistent estimator of  , obtained by any techniques described above for the panel binary choice model. Kyiazidou (1997) proves the consistency of  and derives its asymptotic distribution. Because this estimator uses a kernel weight function, the estimator is not root-N consistent, it has the standard nonparametric estimation rate of convergence.
If uit |dit =1,dis=1,xit ,zit ,xis,zis,i,i and u |is dit =1,dis=1,xit ,zit ,xis,zis,i,i satisfy the stronger exchangeability condition, then we have

E{ (yit - xit o - yis + xiso)|dit = 1, dis = 1, xit , zit , xis, zis, i, i} = 0 for any odd function  (·). In this case, a new class of estimator can be obtained:

  

= arg min


i

dit disK
s<t

(zit - zis)  n

(yit - xit  ) - (yis - xis )].

The asymptotic distribution of the estimator  can be derived exactly the same way as in Kyiazidou (1997).
Wooldridge (1995) also proposes some estimation methods that allow for the unobserved effects in both the regression and selection equations to be correlated with the observed variables and the error distribution in the regression equation to be unspecified. Lee (2001) proposes a semiparametric first-difference estimator for panel censored-selection models when the selection equation is of Tobit type. Also, Lee's estimator does not require smoothing.

14.4 Conclusion
In this chapter, we survey the large and growing literature on semiparametric and nonparametric panel data models. Our survey indicates that substantial progress has been made in semiparametric and nonparametric linear panel data models and panel data Tobit models. The progress made in the area of panel discrete choice and sample selection models, however, is less satisfactory, though considerable scholarly work has been devoted to this area. Far less satisfactory progress has been made in the applications of the techniques surveyed here to analyze real data. Future research in this exciting area should be focused on developing root-N

476

C. Ai and Q. Li

consistent estimator for the panel data discrete choice under some suitably conditions. Once the root-N consistent estimator for the panel data binary choice model is developed, the root-N consistent estimator for the panel data sample selection models should be easily constructed. Furthermore, applications of the existing techniques must be encouraged.
Due to space limitation, we do not cover topics on bias reduction techniques for nonlinear panel data models (Hahn and Newey (2004)), nor do we discuss the general nonseparable panel data models with endogenous regressors considered by Altonji and Matzkin (2005), or the panel data Poisson and duration models. See Blundell, Griffith and Windmeijer (2002), Das and Ying (2005), Horowitz and Lee (2004), Lee (2004) and Van den Berg (2001) and the references therein for discussions on nonlinear count and duration models.
Finally, our discussion on estimation of a nonparametric regression model with fixed effects is based on the within transformation (and series approximation). Baltagi and Li (2002) consider series estimation based on first-difference rather than the within transformation. Carroll, Henderson and Li (2005) propose a nonparametric kernel estimator based on first-differencing.

References
Ai, C. (2005). Some efficient estimators for panel data Tobit models, Unpublished manuscript. Ai, C. and Chen, X. (2003). Efficient etimation of conditional moment restrictions models contain-
ing unknown functions, Econometrica, 71: 1795­1843. Ai, C. and Chen, X. (2005). Asymptotic distribution of smooth functionals of sieve minimum
distance estimator, Unpublished manuscript. Ai, C. and Li, Q. (2005). Estimation of partly specified panel data Tobit models, Unpublished
manuscript. Altonji, J. and Matzkin, R. (2005). Cross section panel data estimators for nonparametric models
with endogenous regressors, Econometrica, 73: 1053­1102. Anderson, E. (1970). Asymptotic properties of conditional maximum likelihood estimators, Jour-
nal of the Royal Statistical Society, 32: 283­301. Arellano, M. and Honore, B. (2001). Panel data models: Some recent developments, in Handbook
of Econometrics, J. J. Heckman and E. Leamer (eds.), Vol. 5, Chap. 53, Amsterdam: NorthHolland, 3229­3296. Baltagi B. H. (2005). Econometric Analysis of Panel Data (3rd edition), New York: Wiley and Sons. Baltagi, B.H. and Li, Q. (2002). Series estimation of partially linear models with fixed effects, Annals of Economics and Finance, 3: 103­116. Blundell, Griffith, R. and Windmeijer, F. (2002). Individual effects and dynamics in count data models. Journal of Econometrics, 108: 113­131. Carroll, R., Henderson, D. and Li, Q. (2005). Nonparametric estimation of panel data models with fixed effects, Unpublished manuscript. Chamberlain, G. (1993). Feedback in panel data models, Unpublished manuscript. Charlier, E., Melenberg, B. and Soest, A.V. (1995). A smoothed maximum score estimator for the binary choice panel data model and an application to labour force participation, Statistica Neerlandica, 49: 324­342. Charlier, E., Melenberg, B. and Soest, A.V. (2000). Estimation of a censored regression panel data model using conditional moment restrictions efficiently, Journal of Econometrics, 95: 25­56.

14 Semi-parametric and Non-parametric

477

Chen, X. and Shen, X. (1998). Sieve extremum estimates for weakly dependent data, Econometrica, 66: 289­314.
Chui, C.K. (1992). An Introduction to Wavelets. San Diego, CA: Academic Press, Inc. Das, M. (2003). Identification and sequential estimation of panel data models with insufficient
exclusion restrictions, Journal of Econometrics, 114: 297­328. Das, M. and Ying, Z. (2005). Linear regression for dependently censored panel duraton models
with nonadditive fixed effects, Unpublished manuscript. Donald, S., Imbens, G. and Newey, W.K. (2004). Empirical likelihood estimation and consistent
tests with conditional moment restrictions, Journal of Econometrics, 117: 55­93. Fan, Y. and Li, Q. (1996). Consistent model specification tests: Omitted variable and semiparamet-
ric functional forms, Econometrica, 64: 865­890. Hahn, J. (2001). The information bound of a dynamic panel Logit model with fixed effects. Econo-
metric Theory 17, 913­932. Hahn, J. and Newey, W. K. (2004). Jackknife and analytical bias reduction for nonlinear panel
models, Econometrica, 72: 1295­1319. Hong, Y. and White, H. (1995). Consistent specification testing via nonparametric series regres-
sion, Econometrica, 63: 1133­1159. Honore´, B. (1992). Trimmed LAD and least squares estimation of truncated and censored regres-
sion models with fixed effects, Econometrica, 60: 533­565. Honore´, B. and Hu, L. (2004). Estimation of cross sectional and panel data censored regression
models with endogeneity, Journal of Econometrics, 122: 293­316. Honore´, B. and Kyriazidou, E. (2000). Panel data discrete choice models with lagged dependent
variables, Econometrica, 68: 839­874. Honore´, B. and Lewbel, A. (2002). Semiparametric binary choice panel data models without
strictly exogenous regressors, Econometrica, 70: 2053­2063. Honore´, B. and Tamer, E. (2005). Bounds on parameters in dynamic discrete choice models,
Econometrica, 74: 611­630. Horowitz, J. (1992). A smoothed maximum score estimator for the binary response model, Econo-
metrica, 60: 505­531. Horowitz, J.L., and Lee, S. (2004). Semiparametric estimation of a panel data proportional hazard
model with fixed effects, Journal of Econometrics, 119: 155­198. Hsiao, C. (2003). Analysis of Panel Data (2nd. edition), Combridge: Cambridge University Press. Hu, L. (2002). Estimating a censored dynamic panel data model with an application to earnings
dynamics, Econometrica, 70: 2499­2517. Kniesner, T. and Li, Q. (2002). Semiparametric panel data models with heterogeneous dynamic
adjustment: Theoretical consideration and an application to labor supply, Empirical Economics, 27: 131­148. Kyriazidou, E. (1997). Estimation of a panel data sample selection model, Econometrica, 65: 1335­1364. Lee, M.J. (1999). A root-N consistent semiparametric estimator for fixed effects binary response panel data, Econometrica, 67: 427­433. Lee, M.J. (2001), First-Difference estimator for panel censored-selection models, Economics Letters, 70: 43­49. Li, Q. and Hsiao, C. (1998). Testing serial correlation in semiparametric panel data models, Journal of Econometrics, 87: 207­237. Li, Q. and Wang, S. (1998). A simple bootstrap test for a parametric regression functional form, Journal of Econometrics, 87: 145­165. Linton, O. and Nielsen, J.P. (1995). A kernel method of estimating structured nonparametric regression based on marginal integration, Biometrika, 83: 93­100. Manski, C. (1987). Semiparametric analysis of random effects linear models from binary panel data, Econometrica, 55: 357­362. Newey, W.K. (1994). Kernel estimation of partial means in a general variance estimator, Econometric Theory, 10: 233­253.

478

C. Ai and Q. Li

Newey, W.K. (1997). Convergence rates and asymptotic normality for series estimators, Journal of Econometrics, 79: 147­168.
Robinson, P. (1988). Root-N-consistent semiparametric regression, Econometrica, 56: 931­954. Shen, X. (1997). On methods of sieves and penalization, Annals of Statistics, 25: 2555­2591. Tjostheim, D. and Auestad, B.H. (1994). Nonparametric identification of nonlinear time series:
projections, Journal of American Statistical Association, 89:1398­1409. Van den Berg, G. (2001). Duration models: specification, identification, and multiple durations, in
Handbook of Econometrics, Volume 5, Amsterdam: North-Holland, 3381­3460. Wooldridge, J.M. (1995). Selection corrections for panel data models under conditional mean
independence assumptions, Journal of Econometrics, 68: 115­132.

Chapter 15
Panel Data Modeling and Inference: A Bayesian Primer
Siddhartha Chib

15.1 Introduction
In this chapter we discuss how Bayesian methods are used to model and analyze panel data. As in other areas of econometrics and statistics, the growth of Bayesian ideas in the panel data setting has been aided by the revolutionary developments in Markov chain Monte Carlo (MCMC) methods. These methods, applied creatively, allow for the sophisticated modeling of continuous, binary, censored, count and multinomial responses under weak assumptions. The purpose of this largely selfcontained chapter is to summarize the various modeling possibilities and to provide the associated inferential techniques for conducting the prior-posterior analyses.
The apparatus we outline in this chapter relies on some powerful and easily implementable Bayesian precepts (for a textbook discussion of Bayesian methods, see Congdon (2001)). One theme around which much of the discussion is organized is hierarchical prior modeling (Lindley and Smith (1972)) which allows the researcher to model cluster-specifc heterogeneity (and its dependence on cluster-specific covariates) through random effects and random-coefficients in various interesting ways. Another theme is the use of the general approaches of Albert and Chib (1993) and Chib (1992) for dealing with binary, ordinal and censored outcomes. A third theme is the use of flexible and robust families of parametric distributions to represent sampling densities and prior distributions. A fourth theme is the comparison of alternative clustered data models via marginal likelihoods and Bayes factors, calculated via the method of Chib (1995). A final theme is the use of MCMC methods (Gelfand and Smith (1990), Tierney (1994), Chib and Greenberg (1995, 1996)) to sample the posterior distribution, to calculate the predictive density and the posterior distribution of the residuals, and to estimate the marginal likelihood.
Siddhartha Chib Olin Business School, Campus Box 1133, Washington University in St. Louis, 1 Brookings Dr, St. Louis, MO 63130, e-mail: chib@wustl.edu

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

479

c Springer-Verlag Berlin Heidelberg 2008

480

S. Chib

Because implementation of the Bayesian paradigm is inextricably tied to MCMC methods, we include a brief overview of MCMC methods and of certain basic results that prove useful in the derivation of the conditional densities that form the basis for model fitting by MCMC simulation methods. Methods for producing random variates from a few common distributions are also included. After these preliminaries, the chapter turns to the analysis of panel data models for continuous outcomes followed by a discussion of models and methods for binary, censored, count and multinomial outcomes. The last half of the chapter deals with the problems of an endogenous covariate, informative missingness, prediction, residual analysis and model comparison.

15.1.1 Hierarchical Prior Modeling

The Bayesian approach to panel data modeling relies extensively on the idea of
a hierarchical prior which is used to model the heterogeneity in subject-specific
coefficients and the distribution of the errors and the random effects. Suppose
that for the ith cluster (subject) in the sample we are interested in modeling the distribution of yi = (yi1, . . . , yini ) on a continuous response y. Also suppose that Wi = (wi1, . . . , wini ) is a ni × q matrix of observations on q covariates wit whose effect on y is assumed to be cluster-specific. In particular, suppose that for the ith
subject at the tth time point one writes

yit = wit i + it , i = 1, 2, . . . , N; t = 1, 2, . . . , ni

(15.1)

or equivalently for all observations in the ith cluster

yi = Wii + i, i = 1, 2, . . . , N i  P

where i is the cluster-specific coefficient vector and i = (i1, . . . , ini ) is the error distributed marginally with mean zero according to the distribution P (to be modeled
below).
In the context of observational data, one is concerned about the presence of un-
observed confounders (variables that simultaneously affect the covariates wit and the error it ). Under such endogeneity of the covariates, E(i|Wi, i) is not zero and the cluster-specific effects are not identified without additional assumptions and the
availability of instruments. To make progress, and to avoid the latter situation, it is
common to assume that the covariates wit are strictly exogenous in the sense that i is uncorrelated with Wi and i, which implies that it is uncorrelated with past, current and future values of wit , given i, or in other words, that the distribution of i given (Wi, i) is P. In the Bayesian context, this strict exogeneity assumption is not required and analysis can proceed under the weaker sequential exogeneity assumption wherein it is uncorrelated with wit given past values of wit and i. Most of our analysis, in fact, is conducted under this assumption, although we do not make it
explicit in the notation. There are situations, of course, where even the assumption

15 Panel Data Modeling and Inference

481

of sequential exogeneity is not tenable. We consider one such important case below

where a time-varying binary covariate (a non-randomly assigned "treatment") is

correlated with the error. We show how the Bayesian analysis is conducted when an

instrument is available to model the marginal distribution of the treatment.

In practice, even when the assumption of sequential exogeneity of the covariates

wit holds, it is quite possible that there exist covariates ai : r × 1 (with an intercept included) that are correlated with the random-coefficients i. These subject-specific covariates may be measurements on the subject at baseline (time t = 0) or other

time-invariant covariates. In the Bayesian hierarchical approach this dependence on

subject-specific covariates is modeled by a hierarchical prior. One quite general way

to proceed is to assume that



 

 11  



i1
i2 ...



=



ai 0
...

iq

0

0 ai ... 0

··· ··· ...
···

··· ··· ...
···

0 0 ... ai



22 ...
...



+



bi1 bi2 ...
biq



qq

i

Ai

bi



or in vector-matrix form

i = Ai + bi

where Ai is a q×k matrix given as Iq ai , k = r ×q,  = (11, 22, . . . , qq) is a k ×1 dimensional vector, and bi is the mean zero random effects vector (uncorrelated with Ai and i) that is distributed according to the distribution Q. This is the second-stage of the model. It may be noted that the matrix Ai can be the identity matrix of order q or the zero matrix of order q. Thus, the effect of ai on i1 (the intercept) is measured by 11, that on i2 is measured by 22 and that on iq by qq.
In the same way, the hierarchical approach can be used to model the distributions
P and Q. One way is to assume that each of these distributions belong to the (hierarchical) scale mixture of normals family. Formally, to model the distribution of i, we could, for example, let

i| 2, i,i  Nni (0,  2i-1i) i  G

where i is a positive-definite matrix depending perhaps on a set of unknown parameters  ,  2 is an unknown positive scale parameter, and i is the random scale parameter that is drawn independently across clusters from some distribution G (say with known parameters). If for example, we assume that
G = G G , G 22
where G denotes the gamma distribution, then the distribution of i marginalized over i is multivariate-t with density proportional to

482

S. Chib

||1/2

1+



1 

2



i-1

-( +ni )/2
.

Similarly, to model the random effects vector bi we could let

bi|i, D  Nq 0, i-1D i  F

where D is a full matrix and i is a positive random variable drawn independently across clusters from a distribution F.
The Bayesian hierarchical model is completed through the specification of prior densities on all the non-cluster-specific coefficients. In general terms, we let

( , D,  2,  )  

where  is some suitable parametric distribution. Interestingly, it is possible to model the prior distribution in stages by putting a prior on the parameters (hyperparameters) of . Note that the latter distribution is a prior distribution on parameters from the different stages of the hierarchical model.
As another example of a hierarchical model, suppose that X1i is an additional ni × k1 matrix of observations on k1 covariates whose effect on y is assumed to be non-cluster-specific. Now suppose that the model generating yi is taken to

yi = X1i1 + Wi2i + i, i = 1, 2, . . . , N

(15.2)

where, as above, the distribution of the subject-specific 2i is modeled as
2i = Ai2 + bi
with the remaining components of the model unchanged. In this hierarchical model, if Ai is not the zero matrix then identifiability requires that the matrices X1i and Wi have no covariates in common. For example, if the first column of Wi is a vector of ones, then X1i cannot include an intercept. If Ai is the zero matrix, however, Wi is typically a subset of X1i.
These two types of hierarchical Bayesian models play a large role in the Bayesian analysis of clustered data. Notice that both models share the same form. This is seen by inserting the model of the cluster-specific random coefficients into the first stage which yields
yi = Xi + Wibi+i , i| 2, i, i  Nni (0,  2i-1i) bi|i, D  Nq 0, i-1D
i  G , i  F ( , D,  2)  

15 Panel Data Modeling and Inference

483

where in the first type of hierarchical model

Xi = WiAi and in the second type of hierarchical model
Xi = (X1i WiAi) with  = (1 2) , as is readily checked. The latter model is therefore the canonical Bayesian hierarchical model for continuous clustered data.

15.1.2 Elements of Markov Chain Monte Carlo
The basic idea behind MCMC methods is quite simple. Suppose that ( |y)  ( )p(y| ) is the posterior density for a set of parameters   d in a particular Bayesian model defined by the prior density ( ) and sampling density or likelihood function p(y| ) and that interest centers on the posterior mean  = d (|y)d. Now suppose that this integral cannot be computed analytically and that the dimension of the integration exceeds three or four (which essentially rules out the use of standard quadrature-based methods). In such cases one calculates the integral by Monte Carlo sampling methods. The general idea is to abandon the immediate task at hand (which is the computation of the above integral) and to ask how the posterior density ( |y) may be sampled. The reason for changing our focus is that if we were to have the draws
 (1), . . . , (M)  ( |y) ,
from the posterior density, then provided the sample is large enough, we estimate not just the above integral but also other features of the posterior density by taking those draws and forming the relevant sample-based estimates. For example, the sample average of the sampled draws is our simulation-based estimate of the posterior mean, while the quantiles of the sampled output are estimates of the posterior quantiles, with other summaries obtained in a similar manner. Under suitable laws of large numbers these estimates converge to the posterior quantities as the simulationsize becomes large. In short, the problem of computing an intractable integral is reduced to the problem of sampling the posterior density.
The sampling of the posterior distribution is, therefore, the central focus of Bayesian computation. One important breakthrough in the use of simulation methods was the realization that the sampled draws need not be independent, that simulation-consistency can be achieved with correlated draws. The fact that the sampled variates can be correlated is of immense practical and theoretical importance and is the defining characteristic of Markov chain Monte Carlo methods, popularly referred to by the acronym MCMC, where the sampled draws form a Markov chain. The idea behind these methods is simple and extremely general. In order to sample a given probability distribution, referred to as the target distribution, a suitable Markov chain is constructed with the property that its limiting, invariant distribution,

484

S. Chib

is the target distribution. Once the Markov chain has been constructed, a sample of draws from the target distribution is obtained by simulating the Markov chain a large number of times and recording its values. Within the Bayesian framework, where both parameters and data are treated as random variables and inferences about the parameters are conducted conditioned on the data, the posterior distribution of the parameters provides a natural target for MCMC methods.
Markov chain sampling methods originate with the work of Metropolis, Rosenbluth, Rosenbluth, Teller and Teller (1953) in statistical physics. A vital extension of the method was made by Hastings (1970) leading to a method that is now called the Metropolis­Hastings algorithm (see Chib and Greenberg (1995) for a detailed summary). This algorithm was first applied to problems in spatial statistics and image analysis (Besag (1974)). A resurgence of interest in MCMC methods started with the papers of Geman and Geman (1984) who developed an algorithm, a special case of the Metropolis method that later came to be called the Gibbs sampler, to sample a discrete distribution, Tanner and Wong (1987) who proposed a MCMC scheme involving data augmentation to sample posterior distributions in missing data problems, and Gelfand and Smith (1990) where the value of the Gibbs sampler was demonstrated for general Bayesian problems with continuous parameter spaces.
The Gibbs sampling algorithm is one of the simplest Markov chain Monte Carlo algorithms and is easy to describe. Suppose that for some grouping of the parameters into sub-blocks, say  1 and  2 (the extension to more than two blocks is straightforward), the set of full conditional densities

1( 1|y, 2)  p(y| 1, 2)( 1, 2)

(15.3)

2( 2|y, 1)  p(y| 1, 2)( 1, 2)

(15.4)

are tractable (that is, of known form and readily sampled). Then, one cycle of the Gibbs sampling algorithm is completed by sampling each of the full conditional densities, using the most current values of the conditioning block. The Gibbs sampler in which each block is revised in fixed order is defined as follows.
Algorithm: Gibbs Sampling

1. Specify an initial value  (0) = ( (10), (20)) :

2. Repeat for j = 1, 2, . . . , n0 + G.

Generate



( j) 1

from

1( 1|y,

2( j-1))

Generate



( j) 2

from

2( 2|y,

(1j))

3. Return the values { (n0+1), (n0+2), . . . , (n0+G)} .

To illustrate the manner in which the blocks are revised, consider Fig. 15.1 which traces out a possible trajectory of the sampling algorithm under the assumption that each block consists of a single component. The contours in the plot represent the joint distribution of  and the labels "(0)", "(1)" etc., denote the simulated values. Note that one iteration of the algorithm is completed after both components are

15 Panel Data Modeling and Inference

485

(3)

2

(2) (1)
(0) 1
Fig. 15.1 Gibbs algorithm: An illustrative sequence of three draws

revised. Also notice that each component is revised along the direction of the coordinate axes. This feature is a source of problems if the two components are highly correlated because then the contours become compressed and movements along the coordinate axes tend to produce only small moves.
In some problems it turns out that the full conditional density cannot be sampled directly. In such cases, the intractable full conditional density is sampled via the Metropolis­Hastings (M­H) algorithm. For specificity, suppose that the full conditional density ( 1|y, 2) is intractable. Let

q1( 1, 1|y,  2)

denote a suitably chosen proposal density of making a transition from  1 to  1, given the data and the values of the remaining blocks (see for example Chib and

Greenberg (1995)). Then, in the first step of the jth iteration of the MCMC algo-

rithm,

given

the

values



( j-1) 2

of

the

remaining

block,

the

updated

iterate

of

1

is

drawn as follows.

Algorithm: Metropolis­Hastings for sampling an intractable 1( 1|y, 2)

1. Propose a value for  1 by drawing:  1  q1( 1( j-1), ·|y,  2( j-1))

486

S. Chib

2.

Calculate

the

probability

of

move



(

( j-1) 1

,



1|y,



2( j-1))

given

by

min

1,

 (  (

1|y, 1|y,

2( j-1))q1( 2( j-1))q1(

1, 1( j-1)|y, 1( j-1), 1|y,

( j-1) 2

)

( j-1) 2

)

.

3. Set



( j) 1

=

1



( j-1) 1

with prob ( 1( j-1), 1|y, 2( j-1))

with

prob

1

-



(

( j-1) 1

,



1|y,

2( j-1))

.

A similar approach is used to sample  2 if the full conditional density of  2 is intractable. These algorithms are extended to more than two blocks in a straightfor-

ward manner (Chib (2001)).

15.1.3 Some Basic Bayesian Updates

We now summarize four results that appear in the development of the MCMC algorithms for the various models that are discussed below. These results provide, for the stated models, the posterior distribution of a set of parameters, conditional on the other parameters of the model. The results are stated in some generality and are specialized, as needed, in the subsequent discussion.
Result 15.1 Suppose that
yi = Xi + Wibi + i , i| 2, i  Nni (0,  2i-1i) , i  N bi|i, D  Nq(0, i-1D)   Nk(0, B0)
where yi = (yi1, . . . , yini ) is a vector of ni observations on the dependent variable for subject i. Then marginalized over {bi}

 |y,  2, {i} , {i} ,i, D  Nk ^ , B

(15.5)

where
and Result 15.2

N
 ^ = B B-0 10 + XiV-i 1yi , i=1

N

-1

 B = B0-1 + Xi V-i 1Xi

i=1

Vi =  2i-1i + i-1WiDWi Suppose that

yi = Xi + Wibi + i , i| 2, i  Nni (0,  2i-1i) bi|i, D  Nq(0, i-1D)

(15.6) (15.7) (15.8)

15 Panel Data Modeling and Inference

487

Then where and
Result 15.3

bi|yi,  ,  2, i, i, D  Nq b^ i, Di
b^ i =  -2iDiWii-1(yi - Xi ) Di = (iD-1 +  -2iWii-1Wi)-1. Suppose that

(15.9) (15.10) (15.11)

yi = Xi + Wibi + i , i| 2, i  Nni (0,  2i-1i) bi|i, D  Nq(0, iD) , i  N D-1  Wq(0, R0)

where WT (, R) is the Wishart distribution with density

|W|(-T -1)/2 c |R|/2 exp

- 1 tr R-1W 2

, |W| > 0,

 c =

T
2T /2 T (T -1)/4 

 +1-i

i=1

2

-1

is the normalizing constant and R is a hyperparameter matrix (Roberts (2001)). Then

D-1|{bi}, y,i, {i} , {i} = D-1|{bi}, i  Wq (0 + N, R)

(15.12)

where Result 15.4

N

-1

 R = R0-1 + ibibi

.

i=1

Suppose that

yi = Xi + Wibi + i , i| 2, i  Nni (0,  2i-1i) , i  N  2  I G v0 , 0 22

(15.13)

where I G (a, b) is the inverse-gamma distribution with density ( 2|a, b)   2 -a+1 exp -b/ 2 . Then

 2|y,  , {bi} , {i}  I G

0 +  ni , 0 + 

2

2

(15.14)

where

N
  = iei -i 1ei i=1

(15.15)

488

S. Chib

and ei = (yi - Xi - Wibi)

15.1.4 Basic Variate Generators

In the application of MCMC methods it often occurs that the simulation of the given
target distribution is reduced to a sequence of simulations from standard and familiar
univariate and multivariate distributions. With that in mind, we present simulation
routines for the distributions that are encountered in the sequel. Gamma Variate: To obtain  from G (,  ) with density proportional to
-1 exp (- ), we draw  from G (, 1) and set  =  / . A draw of a chisquared variate v2 with  degrees of freedom is obtained by drawing from a G (/2, 1/2) distribution.
Inverse-Gamma Variate: A random variable that follows the inverse-gamma distribution I G (,  ) is equal in distribution to the inverse of random variable that follows the G (,  ) distribution. Therefore, an inverse-gamma variate is obtained by drawing  from G (,  ) and setting  = 1/ .
Truncated Normal Variate: A variate from

  T N (a,b)(,  2) ,

a univariate normal distribution truncated to the interval (a, b), is obtained by

the inverse-cdf method. The distribution function of the truncated normal random

variable is



0

if  < a

F (t )

=



1 p2 - p1

1

(

t- 

)

-

(

a- 

)

if a <  < b if b < 

(15.16)

where

p1 = 

a- 

; p2 = 

b- 

Therefore, if U is uniform on (0, 1), then

 =  +  -1 (p1 +U(p2 - p1))

(15.17)

is the required draw. Here -1 is the inverse-cdf of the standard normal distribution and can be computed by the method of Page (1977).
Multivariate Normal Vector: To obtain a random vector  from Nk(,), we draw  from Nk(0, Ik) and set  =  + L where  = LL .
Wishart Matrix: To obtain a random positive-definite matrix W from WT (v, R), one first generates the random lower triangular matrix T = (ti j), such that

15 Panel Data Modeling and Inference

489

tii  v2-i+1 and ti j  N (0, 1)

Then the quantity

W = LTT L

where R = LL is the required draw.

15.2 Continuous Responses

As discussed in Sect. 1.1, Bayesian hierarchical modeling of subject-specific coefficients leads to the canonical model for unbalanced continuous outcomes
yi = Xi + Wibi+i , i| 2, i,i  Nni (0,  2i-1i) bi|i, D  Nq 0, i-1D
i  G , i  F ( , D,  2)  

where yi = (yi1, . . . , yini ) is the data on the ith individual over the ni time periods, Wi is a set of variables whose effect bi is assumed to heterogenous, Xi is a set of raw covariates or the matrix WiAi or (X1i WiAi) if the model is derived from a hierarchical specification in which the heterogeneity depends on cluster-specific
covariates Ai. There are many ways to proceed from this point. If G and F are degenerate at
one, we get the Gaussian-Gaussian model. If we assume that

G = G G , G 22

and

F = G F , F

22

then the distributions of i and bi marginalized over i and i are multivariate student-t with G and F degrees of freedom, respectively. This model may be called the Student­Student model. Other models are obtained by making specific assump-
tions about the form of i. For example, if i is assumed to to be serially correlated according to say an ARMA process, then i is the covariance matrix of the assumed ARMA process. The distribution  is typically specified in the same way, regardless

of the distributions adopted in other stages of the model. Specifically, it is common to assume that the parameters ( , D,  2) are apriori mutually independent with

  Nk(0, B0) ;  2  I G

v0 , 0 22

; D-1  Wp(0, R0)

490
15.2.1 Gaussian­Gaussian Model

S. Chib

To see how the analysis may proceed, consider the model in which the distributions of the error and the random-effects are both Gaussian. In particular, suppose that
i| 2  Nni (0,  2i),
bi|D  Nq(0, D) , i  N
where the matrix i is assumed to be known. Under these assumptions the joint posterior of all the unknowns, including the random effects {bi}, is given by
N
 ( , {bi}, D-1,  2|y) = ( , {bi}, D-1,  2) f (yi| , bi,  2)p(bi|D). (15.18) i=1
Wakefield, Smith, Racine Poon and Gelfand (1994) propose a Gibbs MCMC approach for sampling the joint posterior distribution based on full blocking (i.e., sampling each block of parameters from their full conditional distribution). This blocking scheme is not very desirable because the random effects and the fixed effects  tend to be highly correlated and treating them as separate blocks creates problems with mixing (Gelfand, Sahu and Carlin (1995)).
To deal with this problem, (Chib and Carlin (1999)) suggest a number of reduced blocking schemes. One of the simplest proceeds by noting that  and {bi} can be sampled in one block by the method of composition: first sampling  marginalized over {bi} and then sampling {bi} conditioned on  . What makes reduced blocking possible is the fact that the conditional distribution of the outcomes marginalized over bi is normal which can be combined with the assumed normal prior on  in the usual way. In particular,
f (yi| , D,  2) = f (yi| , bi,  2)g(bi|D)dbi
 |Vi|-1/2 exp{(-1/2)(yi - Xi ) V-i 1(yi - Xi )} ,
where Vi =  2i + WiDWi , which, from Result 15.1, leads to the conditional posterior of  (marginalized over {bi}).
The rest of the algorithm follows the steps of Wakefield et al. (1994). In particular, the sampling of the random effects is from independent normal distributions that are derived by treating (yi - Xi ) as the "data," bi as the regression coefficient and bi N q(0, D) as the prior and applying Result 15.2. Next, conditioned on {bi}, the full conditional distribution of D-1 becomes independent of y and is obtained by combining the Wishart prior distribution of D-1 with the normal distribution of {bi} given D-1. The resulting distribution is Wishart with updated parameters obtained from Result 15.3. Finally, Result 15.4 yields the full-conditional distribution of  2. In applying these results, i and i are set equal to one in all the expressions.

15 Panel Data Modeling and Inference

491

Algorithm: Gaussian­Gaussian Panel (Wakefield et al. (1994) and Chib and Carlin (1999))

1. Sample (a.) (b.)
2. Sample 3. Sample
4. Goto 1

 |y,  2, D  Nk ^ , B bi|y,  ,  2, D  Nq b^ i, Di , i  N

D-1|y,  , {bi} ,  2  Wq {0 + N, R}

 2|y,  , {bi} , D  I G

0 +  ni , 0 + 

2

2

15.2.1.1 Example

As an illustration, we consider data from a clinical trial on the effectiveness of two antiretroviral drugs (didanosine or ddI and zalcitabine or ddC) in 467 persons with advanced HIV infection. The response variable yi j for patient i at time j is the square root of the patient's CD4 count, a seriological measure of immune system health and prognostic factor for AIDS-related illness and mortality. The data set records patient CD4 counts at study entry and again at 2, 6, 12, and 18 months after entry, for the ddI and ddC groups, respectively.
The model is formulated as follows. If we let yi denote a ni vector of responses across time for the ith patient, then following the discussion in Carlin and Louis (2000), suppose

yi| , bi,  2  Nni (Xi + Wibi,  2i), i = Ini bi|D  N2(0, D) , i  467 ,

(15.19)

where the jth row of the patient i's design matrix Wi takes the form wi j = (1,ti j), ti j belongs to the set {0, 2, 6, 12, 18} and the fixed design matrix Xi is obtained by horizontal concatenation of Wi , diWi and aiWi, where di is a binary variable indicating whether patient i received ddI (di = 1) or ddC (di = 0), and ai is a binary variable indicating if the patient was diagnosed as having AIDS at baseline (ai = 1) or not (ai = 0).
The prior distribution of  : 6 × 1 is assumed to be N 6(0, B0) with
0 = (10, 0, 0, 0, -3, 0) , and
B0 = Diag(22, 12, (.1)2, 12, 12, 12) ,

while that on D-1 is taken to be Wishart W (R0/0, 2, 0) with 0 = 24 and R0 = diag(.25, 16). Finally,  2 is apriori assumed to follow the inverse-gamma
distribution

492

S. Chib

 2  I G 0 , 0 , 22
with 0 = 6 and 0 = 120 (which imply a prior mean and standard deviation both equal to 30).
The MCMC simulation is run for 5000 cycles beyond a burn-in of a 100 cycles. The simulated values by iteration for each of the ten parameters are given in Fig. 15.2. Except for the parameters that are approximately the same, the sampled paths of the parameters are clearly visible and display little correlation.
These draws from the posterior distribution are used to produce different summaries of the posterior distribution. In Fig. 15.3 we report the marginal posterior distributions in the form of histogram plots. We see that three of the regression parameters are centered at zero, that D11 is large and D22 (which is the variance of the time-trend random effect) is small.

15.2.2 Robust Modeling of bi: Student­Student and Student-Mixture Models
We now discuss models in which the error distribution of the observations in the ith cluster is multivariate-t and the distribution of bi is modeled as multivariate-t or a mixture of normals. To begin, consider the student-student model
25

20

15

10

Value

5

0

-5

-10 0

500 1000 1500 2000 2500 3000 3500 4000 4500 Iteration

Fig. 15.2 Aids clustered data: Simulated values by iteration for each of ten parameters

5000

15 Panel Data Modeling and Inference

493

1500

Frequency

1000

500

08 10 12 -0.4 -0.2 0 -0.5 0

0.5 -0.2 0 0.2 -6 -4 -2

0

1

2

3

4

1500

Frequency

1000

500

-00.2 0 0.2 2

3

5

2

4 10 20 D11

30 -0.5 0 D21

0.5 0

0.05 0.1 D22

Fig. 15.3 Aids clustered data: Marginal posterior distributions of parameters based on 5000 MCMC draws

yi = Xi + Wibi + i, i| 2, i  Nni (0,  2i-1i)

bi|i, D  Nq(0, i-1D); i  N

i  G

G , G 22

; i  G

F , F 22

  Nk(0, B0) ;  2  I G

v0 , 0 22

; D-1  Wp(0, R0)

This model is easily analyzed by including i and i, i  N, in the sampling. In that case, we follow the Gaussian­Gaussian MCMC algorithm, except that each step is implemented conditioned on {i} and {i} and two new steps are added in which {i} and {i} are sampled. The quantities that go into forming the various parameters in these updates are all obtained from the results of Sect. 1.3.

Algorithm: Student­Student Panel

1. Sample

(a)

 |y,  2, D, {i} , {i}  Nk ^ , B

(b)

bi|y,  ,  2, D, i, i  Nq b^ i, Di ,

494

S. Chib

2. Sample

(a) (b) 3. Sample 4. Sample

i|y,  , {bi} ,  2  G

G + ni , G +  -2eii-1ei

2

2

,

i|bi, D  G

F + q , F + bi D-1bi

2

2

,i  N

D-1|y,  , {bi} ,  2, {i} , {i}  Wq {0 + N, R}

 2|y,  , {bi} , D, {i} , {i}  I G

0 +  ni , 0 + 

2

2

5. Goto 1

Another possibility is to assume that bi is drawn from a finite mixture of Gaussian distributions. For example, one may assume that bi  q1N (0, D1) + q2N (0, D2 = D1) where  > 1 and q j is the probability of drawing from the jth component of the mixture. Chen and Dunson (2003), for example, use a particular mixture prior in which one of the component random effects variances can be zero, which leads to a method for determing if the particular effect is random. Like any Bayesian analysis
of a mixture model, analysis exploits the hierarchical representation of the mixture distribution:

bi|si = j  N (0, D j) Pr(si = j) = q j , j = 1, 2
where si = {1, 2} is a latent population indicator variable. The MCMC based fitting of this Gaussian-mixture model proceeds by sampling the posterior distribution
( , {bi}, D1-1,  2, {i} , , {si}, q|y) = ( , {bi}, D-1 1,  2, {i} , , {si}) f (y|  , {bi},  2, {i})

N
 = ( )(D-1)( 2)( )(q) f (yi| , bi,  2, i)p(bi|si, Dsi )p(si|q)p(i) i=1
where the prior on  is (say) inverse-gamma and that of q = (q1, q2) a Dirichlet with density proportional to q1m10-1q2m20-1 where the hyper-parameters m10 and m20 are known. This posterior density is sampled with some minor modifications of the Student­Student algorithm. Steps 1 and 2 are now conditioned on {si}; as a result Vi in the updates is replaced by Vsi =  2i-1i + WiDsi Wi and Di by Dsi = (D-si 1 +  -2iWi-i 1Wi)-1. Step 3 is now the sampling of D-1 1 where the sum

15 Panel Data Modeling and Inference

495

over the outer-product of the bi's is replaced by i:si=1 bibi + -1 i:si=2 bibi . Steps 4 and 5 are unchanged. Finally, two new steps are inserted: Step 6 for sampling
 and Step 6 for sampling q. Each of these steps is straightforward. In Step 6 we sample  from an updated inverse-gamma distribution based on those bi that are associated with population 2; the update is therefore from the model bi|, D1  N (0, D1),   IG(a0/2, b0/2) which leads to an inverse-gamma distribution. The updated distribution of q in Step 7 is easily seen to be Dirichlet with parameters
m10+ m1 and m20 + m2, respectively, where m j are the total number of observations ascribed to population j in that iteration of the MCMC sampling.

15.2.3 Heteroskedasticity

The methods described above are readily adapted to deal with heteroskedasticity in
the observation error process by parameterizing the error covariance matrix  2i. Instead of assuming that i| 2  Nni (0,  2i), we assume

i|i2  Nni (0, i2Ini )

where i2 can be modeled hierarchically by assuming that

i2|0  IG

0 , 0 22

0  G

00 , 00 22

a specification that appears in Basu and Chib (2003). In the first stage of this prior
specification, one assumes that conditioned on the scale of the inverse-gamma distribution, i2 is inverse-gamma and then the scale is in turn allowed to follow a gamma distribution. The fitting of this model is quite similar to the fitting of the Gaussian­Gaussian model except that  2i is replaced by i2Ini in Steps 1 and 2, i is replaced by Ini in Steps 3 and 4 is modified and a new Step 5 is inserted for the sampling of 0.

Algorithm: Gaussian­Gaussian Hetroskedastic Panel (Basu and Chib (2003))

1. Sample

(a)  |y, i2 , D  Nk ^ , B

(b) 2. Sample

bi|y,  , i2 , D  Nq b^ i, Di , i  N D-1|y,  , {bi} ,  2  Wq {0 + N, R}

496

S. Chib

3. Sample

i2|y,  , {bi}, D, 0  I G

0 + ni , 0+ yi - Xi - Wibi 2

2

2

4. Sample

0|i2  G

0 + 00 , i-2 + 00

2

2

5. Goto 1

15.2.4 Serial Correlation

To deal with the possibility of serial correlation in models with multivariate-t error and random effects distributions we now assume that
i|i,   Nni (0,  2i-1i)
where i = i( ) is a ni ×ni covariance matrix that depends on a set of p parameters  = (1, . . . , p). Typically, one will assume that the errors follow a low-dimensional stationary ARMA process and the matrix i will then be the covariance matrix of the ni errors. In that case,  represents the parameters of the assumed ARMA process. The fitting of this model by MCMC methods is quite straightforward. The one real new step is the sampling of  by the M­H algorithm along the lines of Chib and Greenberg (1994).

Algorithm: Student­Student Correlated Error Panel

1. Sample

(a)
(b)
2. Sample (a)

 |y,  2, D, {i} ,   Nk ^ , B bi|y,  ,  2, D, {i} , {i} ,   Nq b^ i, Di ,

i|y,  , {bi} ,  2,   G

G + ni , G +  -2ei-i 1ei

2

2

,

(b)

i|bi, D  G

F + q , F + bi D-1bi

2

2

,i  N

3. Sample

D-1|y,  , {bi} ,  2, {i} ,   Wq {0 + N, R}

15 Panel Data Modeling and Inference

497

4. Sample 5. Sample

 2|y,  , {bi} , D,   I G

0 +  ni , 0 + 

2

2

n
  |y,  , {bi} , D,  2, {i}  ( ) N yi|Xi + Wibi,  2i-1i i=1

6. Goto 1

In the sampling of  in the above algorithm we use the tailored proposal density as suggested by Chib and Greenberg (1994). Let

n

 ^ = arg max ln 

( ) N
i=1

yi|Xi + Wibi,  2i-1i

g( )

be the conditional mode of the full conditional of  that is found by (say) a few steps of the Newton­Raphson algorithm, and let V be the symmetric matrix obtained by
inverting the negative of the Hessian matrix (the matrix of second derivatives) of ln g( ) evaluated at ^. Then, our proposal density is given by

q( |y,  , {bi}, D,  2) = tp( |^, V, )

a multivariate-t density with mean ^, dispersion matrix V and  degrees of freedom. In this M­H step, given the current value  , we now generate a proposal value  from this multivariate-t density and accept or reject with probability of move

( ,  |y,  , {bi}, D,  2) = min

1,

g( g(

) )

tp( tp(

|^, V, ) |^, V, )

If the proposal value is rejected we stay at the current value  and move to Step 1 of the algorithm. As before, by setting i and i to one we get the Gaussian­Gaussian version of the autoregressive model.

15.3 Binary Responses
Consider now the situation in which the response variable is binary (0, 1) and the objective is to fit a panel model with random effects. The classical analysis of such models (under the probit link) was pioneered by Chamberlain (1980), Heckman (1981) and Butler and Moffitt (1982).
Suppose that for the ith individual at time t, the probability of observing the outcome yit = 1, conditioned on the random effect bi, is given by

498

S. Chib

Pr(yit = 1|bi) = (xit  + wit bi) ,
where  is the cdf of the standard normal distribution, and bi|D  Np(0, D) independent of xit . Since the ni observations in the ith cluster are correlated, the joint density of the observations yi = (yi1, . . . , yini ) is

Pr(yi| , D) =

T
[(xit  + wit bi)]iyt [1 - (xit  + wit bi)]1-yit
t=1

×N (bi|0, D) d bi

Under the assumption that the observations across individuals are independent, the likelihood function of the parameters ( , D) is the product of Pr(yi| , D). Although methods are now available to evaluate this integral under some special circumstances, it turns out that it is possible to circumvent the calculation of the likelihood function. The method relies on the approach that was introduced by Albert and Chib (1993).
To understand the Albert and Chib algorithm, consider the cross-section binary probit model in which we are given n random observations such that Pr(yi = 1) = (xi  ). An equivalent formulation of the model is in terms of latent variables z = (z1, . . . , zn) where
zi|  N (xi  , 1), yi = I[zi > 0] ,
and I is the indicator function. Albert and Chib (1993) exploit this equivalence and propose that the latent variables {z1, . . . , zn}, one for each observation, be included in the MCMC algorithm along with the regression parameter  . In other words, they suggest using MCMC methods to sample the joint posterior distribution

N
 ( , z|y)  ( ) N (zi|xi  , 1) I(zi > 0)yi + I(zi < 0)1-yi i=1
where the term in braces in the probability of yi given ( , zi) and is one for yi = 1 when zi > 0 and is one for yi = 0 when zi < 0. The latter posterior density is sampled by a two-block Gibbs sampler composed of the full conditional distributions:

1.  |y, z 2. z|y, .

Even though the parameter space has been enlarged, the introduction of the latent
variables simplifies the problem considerably. The first conditional distribution, i.e.,  |y, z, is the same as the distribution  |z since knowledge of z means that y has no additional information for  . The distribution  |z is easy to derive since the response variable is continuous. The second conditional distribution, i.e., z|y,  , factors into n distributions zi|yi,  and is easily seen to be truncated normal given the value of yi. Specifically, if yi = 1, then

zi  T N (0,)(xi  , 1)

(15.20)

15 Panel Data Modeling and Inference

499

a truncated normal distribution with support (0, ), whereas if yi = 0, then

zi  T N (-,0](xi  , 1)

(15.21)

a truncated normal distribution with support (-, 0). These truncated normal dis-

tributions are simulated by the method given in Sect. 1.4. For the case of (15.20), it

reduces to

xi  + -1 (-xi  ) +U 1 - (-xi  )

and for the case (15.21) to

xi  + -1 U(-xi  ) ,

where U is a uniform random variable on (0,1). Hence, the algorithm proceeds through the simulation of  given the latent data and the simulation of the latent data given (y,  ).
Given this framework, the approach for the panel probit model becomes trans-
parent. For the ith cluster, we define the vector of latent variable

zi = Xi + Wibi + i , i  Nni (0, Ini )

and let

yit = I[zit > 0]

where zi = (zi1, . . . , zini ) , Wi is a set of variables whose effect bi is assumed to heterogenous, Xi is a set of raw covariates or the matrix WiAi or (X1i WiAi) if the model is derived from a hierarchical specification in which the heterogeneity de-
pends on cluster-specific covariates Ai. The MCMC implementation in this set-up proceeds by including the {zit } in the sampling. Given the {zit } the sampling resembles the steps of the Gaussian­Gaussian algorithm with zit playing the role of yit and  2i-1i = Ini . The sampling of zit is done marginalized over {bi} from the conditional distribution of zit |zi(-t), yit ,  , D, where zi(-t) is the vector zi excluding zit . It should be emphasized that the simulation of these distributions does not require the
evaluation of the likelihood function.

Algorithm: Gaussian­Gaussian Panel Probit (Chib and Carlin (1999)) 1. Sample
(a)
zit |zi(-t), yit ,  , D  N (it , vit ) I (zit < 0)1-yit + I (zit > 0)yit it = E(zit |zi(-t),  , D) vit = Var(zit |zi(-t),  , D)
(b)
N
  |{zit }, D  Nk B(B0-10 + Xi V-i 1zi), B i=1

500

S. Chib

(c)
2. Sample 3. Goto 1

N
 B = (B-0 1 + Xi Vi-1Xi)-1; Vi = Ini + WiDWi i=1
bi|y,  , D  Nq DiWi (zi - Xi ), Di , i  N Di = (D-1 + Wi Wi)-1
D-1|y,  , {bi}  Wq {0 + N, R}

Because of this connection with the continuous case, the analysis of binary panel data may be extended in ways that parallel the developments in the previous section. For example, we can analyze binary data under the assumption that i is multivariate-t and/or the assumption that the random effects distribution is student-t or a mixture of normals. We present the algorithm for the student­student binary response panel model without comment.

Algorithm: Student­Student Binary Panel

1. Sample

(a)

zit |zi(-t), yit ,  , D  N (it , vit ) I (zit < 0)1-yit + I (zit > 0)yit it = E(zit |zi(-t),  , D, i) vit = Var(zit |zi(-t),  , D, i)

(b) (c) 2. Sample

 |{zit }, D {i} , {i}  Nk ^ , B bi|zi,  , D, i, i  Nq b^ i, Di

(a)

i|y,  , {bi} ,  2  G

G

+

ni

,

G

+



-2

ei



-1 i

ei

2

2

,i  N

(b) 3. Sample 4. Goto 1

i|bi, D  G

G + q , G + bi D-1bi

2

2

,i  N

D-1| {zit } ,  , {bi} , {i} , {i}  Wq {0 + N, R}

15 Panel Data Modeling and Inference

501

The fact that this, and other model variants for binary responses, are handled effortlessly is a testament to the flexibility and power of the Bayesian approach.

15.4 Other Outcome Types
15.4.1 Censored Outcomes
Given the discussion of the binary response models in the preceding section it should not be surprising that the Bayesian approach to censored data would proceed in much the same fashion. Consider then a Gaussian­Gaussian Tobit panel data model for the ith cluster:
zi = Xi + Wibi + i , i  Nni (0,  2Ini ) bi  Nq(0, D) ( ,  2, D)  
where the observed outcomes are obtained as
yit = max {zit , 0}
This model is fit along the lines of the Gaussian­Gaussian model by adopting the strategy of Chib (1992) wherein one simulates zit for those observations that are censored from a truncated normal distribution, truncated to the interval (-, 0). In our description of the fitting method we let yiz be a ni × 1 vector with ith component yit if that observation is not censored and zit if it is censored. A new Step 1 is inserted in which the latent zit are sampled conditioned on the remaining values of yiz in the ith cluster, which we denote by yiz(-t); then in Step 2 the only change is that instead of yi we use yiz; in Step 3 in the sampling of bi we replace the vector yi by the most current value of yiz; Step 4 for the sampling of D-1 is unchanged; and in Step 5 dealing with the sampling of  2 we use yiz in place of yi in the definition of .
Algorithm: Gaussian­Gaussian Tobit Panel
1. Sample (a)
zit |yiz(-t), yit ,  ,  2, D  N (it , vit ) I (zit < 0) if yit = 0 it = E(zit |yiz(-t),  ,  2, D) vit = Var(zit |yiz(-t),  ,  2, D)
(b)  |yz,  2, D  Nk ^ , B

502

S. Chib

(c) 2. Sample 3. Sample
4. Goto 1

bi|yz,  ,  2, D  Nq b^ i, Di , i  N

D-1|yz,  , {bi},  2  Wq {0 + N, R}

 2|yz,  , {bi} , D  I G

0 +  ni , 0 + 

2

2

Just as in the case of continuous and binary outcomes, this algorithm is easily modified to allow the random effects have a student-t or a mixture of normals distribution and to allow the observation errors be student-t. Analysis of any of these models is quite difficult from the frequentist perspective.

15.4.2 Count Responses

Bayesian methods are also effectively applied to panel data in which the responses are counts. A framework for fitting such models under the assumption that the distribution of the counts, given the random effects, is Poisson is developed by Chib, Greenberg and Winklemnann (1998). To describe the set-up, for the ith cluster

yit | , bi  Poisson(it ) ln(it ) = ln it + xit  + wit bi

where the covariate vectors xit and wit are the tth row of the matrices Xi and Wi, respectively, and Xi are the raw covariates or the matrix WiAi or (X1i WiAi) if the model is derived from a hierarchical specification in which the heterogeneity depends on cluster-specific covariates Ai. The quantity it which is one if each count is measured over the same interval of time. This specification of the model produces
the likelihood function

n
f (y| , D) =  N (bi|0, D)p(yi| , bi)dbi i=1

(15.22)

where

 p(yi|

,

bi)

=

ni t=0

iyt it

exp (-it yit !

)

(15.23)

is the product of the Poisson mass function with mean it. The interesting aspect of the MCMC algorithm in this case is the sampling of

both  and {bi} by tailored M­H steps. This is because the full conditional distributions in this model do not belong to any known family of distributions. At each

step of the algorithm, there are n + 1 M­H steps. It may appear that the computa-

tional burden is high when n is large. This turns out not to be case.

15 Panel Data Modeling and Inference

503

Algorithm: Panel Poisson Chib, Greenberg and Winklemnann (1998))

1. Calculate the parameters (m0, V0) as the mode and inverse of the negative Hessian of
N
log Nk(  |0, B0) +  log p(yi| , bi) i=1
propose   T ( |m0, V0, ) (the multivariate-t density) and move to  with probability

min

iN=1 p(yi| Ni=1 p(yi|

, ,

bi bi

)Nk )Nk

( (

|0, B) |0, B)

T T

( (

|m0, V0, ) |m0, V0, )

,

1

2. Calculate the parameters (mi, Vi) as the mode and inverse of the negative Hessian of log Nq(bi|0, D) + log p(yi| , bi)
propose bi  T (bi|mi, Vi, ) and move to bi with probability

min

p(yi| , bi p(yi| , bi

)Nq(bi |0, D) )Nq(bi|0, D)

T T

(bi|mi, (bi |mi,

Vi, Vi,

 

) )

,

1

3. Sample

D-1|y,  , {bi},  2  Wq {0 + N, R}

15.4.3 Multinomial Responses

Multinomial panel responses arise in several different areas and the fitting of this model when the link function is assumed to be multinomial logit is exactly the same as the algorithm for count responses. The only difference is that instead of the Poisson link function we now have the multinomial logit link. Let yit be a multinomial random variable taking values {0, 1, . . . , J} and assume that

exp  j + xit j + wit jbi Pr(yit = j| , bi) = lJ=0 exp l + xitl + witlbi

where for identifiability 0 is set equal to zero. The joint probability of the outcomes in the ith cluster, conditioned on the bi, is now given by

ni
 p(yi| , bi) = Pr(yit = jt | , bi) t=1

(15.24)

where jt is the observed outcome at time t. The structure of the problem is seen to be identical to that in the count case and the preceding algorithm applies directly to this problem by replacing the mass function in (15.23) with the mass function in (15.24).

504

S. Chib

Chiang, Chib and Narasimhan (1999) develop an interesting variant of this model in which the possible values that yit can take is not the same across clusters. Such a situation arises when the multinomial outcomes are choices made by a subject (for example choice of transportation mode or choice of brand of a product) and where the assumption that the choice set is the same across subjects is too strong and must be relaxed. The model discussed in the paper only appears to be fittable by Bayesian methods. The paper includes a detailed example.

15.5 Binary Endogenous Regressor

In many applied studies, one is interested in the effect of a given (binary) covariate on the response but under the complication that the binary covariate is not sequentially randomly assigned. In other words, the assumption of sequential exogeneity is violated. This problem has not been extensively studied in the literature but interestingly it is possible to develop a Bayesian approach to inference that in many ways is quite straightforward. For concreteness, suppose that in the context of the model in (15.2) the last covariate in x1it (namely x12it ) is the covariate of interest and the model is given by
yit = x11it 11 + x12it 12 + wit c2i + eit
where x1it = (x11it , x12it ) and x11it : k11 × 1. Assume that the covariates x11it and wit : q × 1 satisfy the assumption of sequential exogeneity but that x12it does not. Now let zit : kz × 1 be time-varying instruments and suppose that the model generating the endogenous covariate is

x12it = I x11it  + wit d2i + zit  + uit > 0

where

eit uit

 N2

0 0

,=

11 12 12 1

and 12 = 0. Letting x12it = x11it  + wit d3i + zit  + ui, the model is reexpressed as



11

yit x12it

=

x11it x12it 0 0 0 0 x11it zit



12 



yit


X1it

1

+

wit 0 0 wit

c2i d2i

+

eit uit

Wit

2i

it

or as

yit = X1it 1 + Wit 2i + it

15 Panel Data Modeling and Inference

505

where 1 is k1 × 1 with k1 = 2k11 + 1 + kz and 2i is 2q × 1. If we assume that 2i as before is modeled in terms of covariates ai : r × 1 as

c2i d2i

=

Iq  ai 0 0 Iq  ai

21 22

+

b1i b2i

or compactly as

2i = Ai2 + bi

where 2 : k2 × 1 and k2 = 2qr, then we can rewrite the outcome vector for subject i at time t as
yit = Xit  + Wit bi + it

where

Xit = (X1it , AiWit )

 = (1, 2) : k × 1, and k = k1 + k2. This is similar to the models that we have dealt with except that this is a system of two equations for each (i,t) with the second component of the outcome being latent. For the ith cluster the preceding model (in conjunction with the standard assumptions about bi) is written as

yi = Xi + Wibi+i , i|i,  N2ni (0, i-1 {Ini  }) bi|D  N2q (0, D)

This model is (these appear in

fit along the rows 2, 4, 6,

lines etc.

of in

the the

binary vector

panel by yi ) from

simulating {x12it }tn=i 1 in yi appropriate truncated nor-

mal distributions, according to the device of Albert and Chib (1993), marginalized

over bi. In our description of the fitting method given below it is to be understood that yi contains the most recently simulated values of {x12it }tn=i 1. A new step is the sampling of (11, 12). The best way of working with these parameters is to reparameterize them to ( 2, 12) where  2 = 11 - 122 and then assuming that prior
information on the transformed parameters is represented by the conditionally con-

jugate distribution

( 2, 12) = I G

 2| 0 , 0 22

N

12|m0,  2M0

(15.25)

Now conditioned on {x12it }i,t and {bi} it follows that

yit = 12uit + vit

(15.26)

where

yit = yit - x11it 11 - x12it 12 - wit c2i , uit = x12it - x11it  + wit d2i + zit 

506

S. Chib

and vit  N(0,  2)
The prior in (15.25) and the sampling model in (15.26) when combined by Bayes theorem produce an updated distribution of ( 2, 12) that is sampled in one block. To see the details, we express the model in (15.26) for all M = Ni=1 Ti observations as
y = 12u + v
where v  N (0,  2IM). By simple calculations it is seen that the updated distribution of  2 marginalized over 12 is

I G  2| 0 + M , 0 + (y - um0) (IM + uM0u )-1 (y - um0)

2

2

while that of 12 conditioned on  2 is N 12|,W  -2 M0m0 + u y ,W =  2 M0 + u u -1

Algorithm: Gaussian­Gaussian Binary Endogenous Panel 1. Sample

(a)

x12it | (yi\x12it ) , x12it ,  , D,  N (it , vit )
I (x12it < 0)1-x12it + I (x12it > 0)x12it
it = E(x12it | (yi \x12it ) , x12it ,  , D,) vit = Var(x12it | (yi \x12it ) , x12it ,  , D,)

(b)  |y,, D  Nk ^ , B

(c) 2. Sample 3. Sample

bi|y,  , D,  Nq b^ i, Di , i  N D-1|y,  , {bi},  2  Wq {0 + N, R}

(a)

 2|y,  , {bi} , D 

I G  2| 0 + M , 0 + (y - um0) (IM + uM0u )-1 (y - um0)

2

2

15 Panel Data Modeling and Inference

507

(b)
12|y,  , {bi} ,  2, D  N 12|,W  -2 M0m0 + u y ,W =  2 M0 + u u -1

4. Goto 1

15.6 Informative Missingness

It is possible to develop a range of panel data models in which the outcome on a
given subject at time t is potentially missing. Each individual at time t supplies two
observations: cit and yit . The variable cit is binary and takes the value 1 in which case yit is observed or the value 0 in which case the observation yit is missing. The two random variables are correlated due to the presence of common unobserved
random variables. The missigness mechanism is thus non-ignorable. To describe
the basic components of such a model, suppose yit is the outcome (which could be continuous, discrete, or censored) and cit is an indicator variable of non-missigness. As an example suppose that the variable cit is one if the individual is working and 0 otherwise and yit is a continuous variable indicating the person's wage. Thus, the variable yit is observed when cit is one; otherwise the variable yit is missing. Let cit denote a continuous random variable that is marginally generated as

cit = xit i + zit i + ui

and let

cit = I (cit > 0)

where i and i are subject-specific coefficients and zit is an additional covariate (the instrument). For simplicity we are assuming that the effect of each covariate is subject-specific although this can be relaxed, much as we have done in the models discussed previously. Also suppose that the outcome yit (under the assumption that it is continuous) is marginally generated as

yit = xit i + it

where

it uit

 N2

0 0

, =

11 12 12 1

To complete the model, we specify the distribution of the heterogenous coefficients with a hierarchical prior. Let i = (i , i , i) and assume that

i| , D  N (Ai , D)

where D is a full matrix. Under this latter specification, the two components of the model are tied together not just by correlation amongst the errors but also by the

508

S. Chib

dependence between i and (i, i) as measured by the off-diagonal blocks D12 and D13 of D. It is also assumed that the covariates xit and zit are observable even when yit is missing (ie., when cit = 0).
We mention that a variant of this model is considered by Chib, Seetharaman and Strijnev (2004). In that model yit is multinomial indicating choice amongst a given set of brands in a particular category (say cola) and cit is a variable that indicates whether purchase into the category occurs at shopping visit t; if the individual does not purchase in the category then the brand-choice outcome is missing. They describe the Bayesian MCMC fitting of the model and apply the model and the algorithm to a scanner panel data set.

15.7 Prediction

In some problems one is interested in predicting one or more post-sample observations on a given individual. Specifically, for an individual in the sample, we are interested in making inferences about the set of observations

yi f = (yini+1, . . . , yini+s)
given sample data and a particular hierarchical Bayesian model. In the Bayesian context, the problem of prediction is solved by the calculation of the predictive density
f (yi f |y) = p(yi f |y, i, )(i, |y)did

where i denotes the set of cluster-specific unobserved random-variables (such as zi in binary and censored response models and the random effects bi) and  denote the entire set of parameters. The predictive density is the density of yi f marginalized over (i, ) with respect to the posterior distribution of (i,  ).
This predictive density is summarized in the same way that we summarized the
posterior density of the parameters­by sampling it. Sampling of the predictive den-
sity is conducted by the method of composition. According to the method of composition, if f (y) = f (y|x)(x)dx, and x(g) is a draw from (x), then y(g) drawn from f (y|x(g)) is a draw from f (y). Thus, a draw from the marginal is obtained simply by sampling the conditional density f (y|x) for each value drawn from (x).
The method of composition leads to an easily implementable procedure for calcu-
lating the predictive density in every panel data model that we have considered. For example in the Gaussian-Gaussian model, given ( (g),  2(g), b(ig)), the gth MCMC draw on ( ,  2, bi), the gth draw from the predictive density is obtained by drawing

i(tg)  N (0,  2(g)) , t = ni + 1, . . . , ni + s

and setting

yi(tg) = xit  (g) + wit bi(g) + i(tg) , t = ni + 1, . . . , ni + s

The resulting sample of draws are summarized in terms of moments, quantiles and

density plots.

15 Panel Data Modeling and Inference

509

15.8 Residual Analysis

One approach to Bayesian residual analysis relies on the idea of "realized errors" introduced by Zellner (1975) and studied more recently by Chaloner and Brant (1988) and Albert and Chib (1995). The idea is to compute the posterior distribution of the error and define a residual to be outlying if the posterior distribution is concentrated on large values.
Consider for simplicity the Gaussian­Gaussian model for continuous responses. In that case, the error conditioned on yit is given by

it = yit - xit  - wit bi
and, therefore, the posterior distribution of it is determined by the posterior distribution of  and bi. To obtain this posterior distribution, at each iteration of the sampling, we compute the value

i(tg) = yit - xit  (g) - wit b(ig)

where { (g),bi(g)} are the gth sampled values. Then, the collection of values {i(tg)} constitutes a sample from the posterior distribution (it|y). There are various ways to summarize this posterior distribution in order to find outlying observations. One possibility is to compute the posterior probability

Pr

it 

> k|y

where k is 2 or 3, and compare the posterior probability (computed from the simulated draws i(tg)/ (g)) with the prior probability that the standardized residual is bigger that k in absolute value. The observation is classified as on outlier if the ra-
tio of the posterior probability to the prior probability is large. Interestingly, similar
ideas are used in panel probit models as discussed by Albert and Chib (1995).

15.9 Model Comparisons

Posterior simulation by MCMC methods does not require knowledge of the normalizing constant of the posterior density. Nonetheless, if we are interested in comparing alternative models, then knowledge of the normalizing constant is essential. This is because the standard and formal Bayesian approach for comparing models is via Bayes factors, or ratios of marginal likelihoods. The marginal likelihood of a particular model is the normalizing constant of the posterior density and is defined as

m(y|M ) = p(y|M , )( |M )d ,

(15.27)

510

S. Chib

the integral of the likelihood function with respect to the prior density. If we have two models Mk and Ml, then the Bayes factor is the ratio

Bkl

=

m(y|Mk) m(y|Ml )

.

(15.28)

Computation of the marginal likelihood is, therefore, of some importance in Bayesian statistics (DiCicio, Kass, Raftery and Wasserman (1997), Chen and Shao (1998), Roberts (2001)). Unfortunately, because MCMC methods deliver draws from the posterior density, and the marginal likelihood is the integral with respect to the prior, the MCMC output cannot be used directly to average the likelihood. To deal with this problem, a number of methods have appeared in the literature. One simple and widely applicable method is due to Chib (1995) which we briefly explain as follows.
Begin by noting that m(y) by virtue of being the normalizing constant of the posterior density can be expressed as

m(y|M )

=

p(y|M , )( |M ) ( |M , y)

,

(15.29)

for any given point   (generally taken to be a high density point such as the posterior mean). Thus, provided we have an estimate ^( |M , y) of the posterior
ordinate, the marginal likelihood is estimated on the log scale as

log m(y|M ) = log p(y|M ,  ) + log ( |M ) - log ^ ( |M , y) . (15.30)

In the context of both single and multiple block M­H chains, good estimates of the posterior ordinate are available. For example, when the MCMC simulation is run with B blocks, to estimate the posterior ordinate we employ the marginal-conditional decomposition
( |M , y) = ( 1|M , y) × . . . × ( i|M , y, i-1) × . . . × ( B|M , y, B-1) , (15.31)
where on letting  i = ( 1, . . . , i) and  i = ( i, . . . , B) denote the list of blocks upto i and the set of blocks from i to B, respectively, and z denoting the latent data, and dropping the model index for notational convenience, the typical term is of the form



(

 i

|y,



i-1)

=

( i |y,



 i-1

,



i+1,

z)



(

i+1,

z|

y,



i-1)d

i+1

dz

This is the reduced conditional ordinate. It is important to bear in mind that in

finding the reduced conditional ordinate one must integrate only over ( i+1, z) and

that

the

integrating

measure

is

conditioned

on



 i-1

.

Consider first the case where the normalizing constant of each full conditional

density is known. Then, the first term of (15.31) is estimated by the Rao­Blackwell

method. To estimate the typical reduced conditional ordinate, one conducts a MCMC

run consisting of the full conditional distributions

15 Panel Data Modeling and Inference

511

( i|y, i-1, i+1, z) ; . . . ; ( B|y, i-1, i, . . . , B-1, z) ;(z|y, i-1, i) (15.32)
where the blocks in  i-1 are set equal to  i-1. By MCMC theory, the draws on ( i+1, z) from this run are from the distribution ( i+1, z|y, i-1) and so the reduced conditional ordinate is estimated as the average

M

 ^

(

i |

y,



 i-1

)

=

M-1

( i|y, i-1, i+1,( j), z( j))

j=1

over the simulated values of  i+1 and z from the reduced run. Each subsequent reduced conditional ordinate that appears in the decomposition (15.31) is estimated in the same way though, conveniently, with fewer and fewer distributions appearing in the reduced runs. Given the marginal and reduced conditional ordinates, the marginal likelihood on the log scale is available as

B

log

m^ (y|M

)

=

log

p(y|M

,



)

+

log



(



|M

)

-



log

^

(



i |M

,

y,

 i-1

)

i=1

(15.33)

where p(y|M ,  ) is the density of the data marginalized over the latent data z.

Consider next the case where the normalizing constant of one or more of the full

conditional densities is not known. In that case, the posterior ordinate is estimated

by a modified method developed by Chib and Jeliazkov (2001). If sampling is con-

ducted in one block by the M­H algorithm, then it can be shown that the posterior

ordinate is given by

 (

|y)

=

E1

{( , |y)q( E2 {( ,

 , |y)} |y)}

where the numerator expectation E1 is with respect to the distribution ( |y) and the denominator expectation E2 is with respect to the proposal density of  conditioned on  , q( , |y), and ( , |y) is the probability of move in the M­H step. This
leads to the simulation consistent estimate

^ ( |y)

=

M-1 Mg=1 ( (g), |y)q( (g), |y) J-1 Mj=1 ( , ( j)|y)

,

(15.34)

where { (g)} are the given draws from the posterior distribution while the draws  ( j) in the denominator are from q( , |y), given the fixed value  .
In general, when sampling is done with B blocks, the typical reduced conditional
ordinate is given by

( i|y, 1, . . . , i-1) = E1



(

i

,



 i

|y,



i-1,



i+1)qi(

i

,



 i

|y,



i-1,



i+1)

E2



(

 i

,



i|y,

i-1,



i+1

)

(15.35)

512

S. Chib

where

E1

is

the

expectation

with

respect

to



(

i+1

|y,



 i-1

)

and

E2

that

with

respect to the product measure ( i+1|y, i)qi( i , i|y, i-1, i+1). The quantity



(

i,



i|y,



 i-1

,



i+1)

is

the

usual

conditional

M­H

probability

of

move.

The

two

expectations are estimated from the output of the reduced runs in an obvious way.

15.9.1 Gaussian­Gaussian Model
As an example of the calculation of the marginal likelihood consider the calculation of the posterior ordinate for the Gaussian­Gaussian continuous response model. The ordinate is written as
(D-1,  2,  |y) = (D-1|y)( 2|y, D)( |y, D,  2) ,
where the first term is obtained by averaging the Wishart density over draws on {bi} from the full run. To estimate the second ordinate, which is conditioned on D, we run a reduced MCMC simulation with the full conditional densities
( |y, D,  2) ; ( 2|y,  , D, {bi}) ; ({bi}|y,  , D,  2) ,
where each conditional utilizes the fixed value of D. The second ordinate is now estimated by averaging the inverse-gamma full conditional density of  2 at  2 over the draws on ( , {bi}) from this reduced run. The third ordinate is multivariate normal as given above and available directly.

15.9.2 Gaussian­Gaussian Tobit model
As another example, consider the Gaussian­Gaussian Tobit censored regression model. The likelihood ordinate is not available directly but can be estimated by a simulation-based approach. For the posterior ordinate we again utilize the decomposition
(D-1,  2,  |y) = (D-1|y)( 2|y, D)( |y, D,  2) ,
where the first term is obtained by averaging the Wishart density over draws on {zi} and {bi} from the full run. To estimate the second ordinate, which is conditioned on D, we run a reduced MCMC simulation with the full conditional densities
( |yz, D,  2) ; ({zi}|y,  , D,  2); ( 2|yz,  , D, {bi}) ; ({bi}|yz,  , D,  2) ,
and estimate the second ordinate by averaging the inverse-gamma full conditional density of  2 at  2 over the draws on ( , {zi}, {bi}) from this run. Finally, to

15 Panel Data Modeling and Inference

513

estimate the last ordinate we also fix  2 at  2 and continue the reduced runs with the full-conditional densities

( |yz, D,  2) ; ({zi}|y,  , D,  2); ({bi}|yz,  , D,  2) ,

and average the multivariate normal density given in Step 1 of the MCMC algorithm at the point  .

15.9.3 Panel Poisson Model

As a last example of the calculation of the marginal likelihood, consider the panel

poisson model in which the full conditional of  is not of known form. Now the

posterior ordinate given the sampling scheme in the Panel count algorithm is de-

composed as

(D-1,  |y) = (D-1|y)( |y, D)

where the first ordinate is found by averaging the Wishart density over draws on {bi} from the full run. The second ordinate is found by the method of Chib and Jeliazkov (2001) as

^ ( |y, D)

=

M-1 Mg=1 ( (g),  |y, {b(ig)})q( |y, {bi(g)}) J-1 Jj=1 ( ,  ( j)|y, {b(i j)})

where the draws in the numerator are from a reduced run comprising the full conditional distributions of  and {bi}, conditioned on D whereas the draws in the
denominator are from a second reduced run comprising the full conditional distributions of {bi}, conditioned on (D,  ) with an appended step in which  ( j) is drawn from q( |y, {b(i j)}). The log of the likelihood ordinate p(y| , D) is found by importance sampling.

15.10 Conclusion
In this chapter we have illustrated how Bayesian methods provide a complete inferential tool-kit for a variety of panel data models. The methods are based on a combination of hierarchical prior modeling and MCMC simulation methods. Interestingly, the approaches are able to tackle estimation and model comparison questions in situations that are quite challenging by other means. We discussed applications to models for continuous, binary, censored, count, multinomial response models under various realistic and robust distributional and modeling assumptions. The methods are quite practical and straightforward, even in complex models settings such as those with binary and count responses, and enable the calculation of the entire posterior distribution of the unknowns in the models. The algorithm for

514

S. Chib

fitting panel probit models with random effects is particularly interesting in that it highlights the value of augmentation in simplifying the simulations and in circumventing the calculation of the likelihood function. Procedures for dealing with missing data, predicting future outcomes and for detecting outliers have also been discussed.
The methods discussed in this chapter, which have arisen in the course of a revolutionary growth in Bayesian statistics in the last decade, offer a unified approach for analyzing a whole array of panel models. The pace of growth of Bayesian methods for longitudinal data continues unimpeded as the Bayesian approach attracts greater interest and adherents.

References
ALBERT, J. and S. CHIB (1993), Bayesian analysis of binary and polychotomous response data, Journal of the American Statistical Association, 88, 669­679.
ALBERT, J. and S. CHIB (1995), Bayesian residual analysis for binary response models, Biometrika, 82, 747­759.
BASU, S. and S. CHIB (2003), Marginal likelihood and Bayes factors for Dirichlet process mixture models, Journal of the American Statistical Association, 98, 224­235.
BESAG, J. (1974), Spatial interaction and the statistical analysis of lattice systems (with discussion), Journal of the Royal Statistical Society, B, 36, 192­236.
BUTLER, J. S. and R. MOFFITT (1982), A computationally efficient quadrature procedure for the one factor multinomial probit model, Econometrica, 50, 761­764.
CARLIN, B. P. and T. LOUIS (2000), Bayes and Empirical Bayes Methods for Data Analysis, 2nd ed., Chapman and Hall, New York.
CHALONER, K. and R. BRANT (1988), A Bayesian approach to outlier detection and residual analysis, Biometrika, 75, 651­659.
CHAMBERLAIN, G. (1980), Analysis of covariance with qualitative data, Review of Economic Studies, 47, 225­238.
CHEN, Z. and D. B. DUNSON (2003), Random effects selection in linear mixed models, Biometrics, 59, 762­769.
CHEN, M-H and Q-M. SHAO (1998), On Monte Carlo methods for estimating ratios of normalizing constants, Annals of Statistics, 25, 1563­1594.
CHIANG, J., S. CHIB and C. NARASIMHAN (1999), Markov Chain Monte Carlo and models of consideration set and parameter heterogeneity, Journal of Econometrics, 89, 223­248.
CHIB, S. (1992), Bayes regression for the Tobit censored regression model, Journal of Econometrics, 51, 79­99.
CHIB, S. (1995), Marginal likelihood from the Gibbs output, Journal of the American Statistical Association, 90, 1313­1321.
CHIB, S. (2001), Markov Chain Monte Carlo methods: Computation and inference, in Handbook of Econometrics volume 5 (eds., J. J. Heckman and E. Leamer), Amsterdam: North Holland 3569-3649.
CHIB, S. and E. GREENBERG (1994), Bayes inference for regression models with ARMA(p, q) errors, Journal of Econometrics, 64, 183­206.
CHIB, S. and E. GREENBERG (1995), Understanding the Metropolis­Hastings algorithm, American Statistician, 49, 327­335.
CHIB, S. and E. GREENBERG (1996), Markov chain Monte Carlo simulation methods in econometrics, Econometric Theory, 12, 409­431.
CHIB, S. E. GREENBERG, and R. WINKLEMNANN (1998), Posterior simulation and Bayes factors in panel count data models, Journal of Econometrics, 86, 33­54.

15 Panel Data Modeling and Inference

515

CHIB, S. and B. P. CARLIN (1999), On MCMC sampling in hierarchical longitudinal models, Statistics and Computing, 9, 17­26.
CHIB, S. and I. JELIAZKOV (2001), Marginal likelihood from the Metropolis­Hastings output, Journal of the American Statistical Association, 96, 270­281.
CHIB, S., P. B. SEETHARAMAN, and A. STRIJNEV (2004) Model of brand choice with a nopurchase option calibrated to scanner panel data, Journal of Marketing Research, 184­196.
CONGDON, P. (2001) Bayesian Statistical Modelling, John Wiley & Sons, Chichester. DICICCIO, T. J., R. E., KASS, A. E., RAFTERY and L. WASSERMAN (1997), Computing Bayes
factors by combining simulation and asymptotic approximations, Journal of the American Statistical Association, 92, 903­915. GELFAND, A. E. and A. F. M. SMITH (1990), Sampling-based approaches to calculating marginal densities, Journal of the American Statistical Association, 85, 398­409. GELFAND, A. E., S. K. SAHU, and B. P. CARLIN (1995), Efficient parameterizations for normal linear mixed models, Biometrika, 82, 479­488. GEMAN, S. and D. GEMAN (1984), Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images, IEEE Transactions on Pattern Analysis and Machine Intelligence, 12, 609­628. HASTINGS, W. K. (1970), Monte Carlo sampling methods using Markov chains and their applications, Biometrika, 57, 97­109. HECKMAN, J.J. (1981), Statistical models for discrete panel data, in Structural Analysis of Discrete Data with Econometric Applications, (eds., C. F. Manski and D. McFadden), Cambridge: MIT Press, 114­178. LINDLEY, D.V. and A.F.M. SMITH (1972), Bayes estimates for the linear model, Journal of the Royal Statistical Society B, 34, 1­41. METROPOLIS, N., A. W. ROSENBLUTH, M. N. ROSENBLUTH, A. H. TELLER, and E. TELLER (1953), Equations of state calculations by fast computing machines, Journal of Chemical Physics, 21, 1087­1092. PAGE, E (1977), Approximations to the cumulative normal function and its inverse for use on a pocket calculator, Applied Statistics, 26, 75­76. ROBERTS, C.P. (2001), The Bayesian Choice, New York: Springer Verlag. RIPLEY, B. (1987), Stochastic Simulation, New York: John Wiley & Sons. STOUT, W. F. (1974), Almost Sure Convergence, New York: Academic Press. TANNER, M. A. and W. H. WONG (1987), The calculation of posterior distributions by data augmentation, Journal of the American Statistical Association, 82, 528­549. TIERNEY, L. (1994), Markov chains for exploring posterior distributions (with discussion), Annals of Statistics, 22, 1701­1762. WAKEFIELD, J. C., A. F. M. SMITH, A. Racine POON, and A. E. GELFAND (1994), Bayesian analysis of linear and non-linear population models by using the Gibbs sampler, Applied Statistics, 43, 201­221. ZELLNER, A (1975), Bayesian analysis of regression error terms, Journal of the American Statistical Association, 70, 138­144.

Chapter 16
To Pool or Not to Pool?
Badi H. Baltagi, Georges Bresson and Alain Pirotte

16.1 Introduction
For panel data studies with large N and small T , it is usual to pool the observations, assuming homogeneity of the slope coefficients. The latter is a testable assumption which is quite often rejected. Moreover, with the increasing time dimension of panel data sets, some researchers including Robertson and Symons (1992), Pesaran and Smith (1995), and Pesaran, Smith and Im (1996) have questioned the poolability of the data across heterogeneous units. Instead, they argue in favor of heterogeneous estimates that can be combined to obtain homogeneous estimates if the need arises. Maddala, Trost, Li and Joutz (1997) on the other hand argue that the heterogeneous time series estimates yield inaccurate estimates and even wrong signs for the coefficients, while the panel data estimates are not valid when one rejects the hypothesis of homogeneity of the coefficients. If one is after reliable coefficient estimates, Maddala, Trost, Li and Joutz (1997) argue in favor of shrinkage estimators that shrink the heterogeneous estimators towards the pooled homogeneous estimator. Proponents of the homogeneous panel estimators have acknowledged the potential heterogeneity among the cross-sectional units, but have assumed that the efficiency

Badi H. Baltagi Center for Policy Research, 426 Eggers Hall, Syracuse University, Syracuse, NY 13244-1020, USA, e-mail: bbaltagi@maxwell.syr.edu
Georges Bresson ERMES (UMR 7181, CNRS), Universite´ Paris II and TEPP (FR 3126, CNRS), Institute for Labor Studies and Public Policies, 12, place du Panthe´on, 75230 Paris Cedex 05, France, e-mail: bresson01@aol.com
Alain Pirotte ERMES (UMR 7181, CNRS), Universite´ Paris II and TEPP (FR 3126, CNRS), Institute for Labor Studies and Public Policies, 12, place du Panthe´on, 75230 Paris Cedex 05, France, e-mail: apirotte@aol.com
 We dedicate this chapter to the memory of G.S. Maddala who wrote the original chapter with W. Hu.

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

517

c Springer-Verlag Berlin Heidelberg 2008

518

B.H. Baltagi et al.

gains from pooling outweighed these costs; see Hsiao (2003) on the benefits of panels. Clearly, in panel data sets with T very small, traditional homogeneous panel estimators would appear the only viable alternative. But as T reaches 50 years of post-war annual data, the choice no longer seems clear-cut.
In the context of dynamic demand for gasoline across 18 OECD countries over the period 1960­1990, Baltagi and Griffin (1997) argued for pooling the data as the best approach for obtaining reliable price and income elasticities. They also pointed out that pure cross-section studies cannot control for unobservable country effects, whereas pure time-series studies cannot control for unobservable oil shocks or behavioral changes occurring over time. Baltagi and Griffin (1997) compared the homogeneous and heterogeneous estimates in the context of gasoline demand based on the plausibility of the price and income elasticities as well as the speed of adjustment path to the long-run equilibrium. They found considerable variability in the parameter estimates among the heterogeneous estimators, some giving implausible estimates, while the homogeneous estimators gave similar plausible short-run estimates that differed only in estimating the long-run effects. Baltagi and Griffin (1997) also compared the forecast performance of these homogeneous and heterogeneous estimators over 1, 5, and 10 years horizon. Their findings show that the homogeneous estimators outperformed their heterogeneous counterparts based on mean squared forecast error. This result was replicated using a panel data set of 21 French regions over the period 1973­1998 by Baltagi, Bresson, Griffin and Pirotte (2003). Unlike the international OECD gasoline data set, the focus on the inter-regional differences in gasoline prices and income within France posed a different type of data set for the heterogeneity versus homogeneity debate. The variation in these prices and income were much smaller than international price and income differentials. This in turn reduces the efficiency gains from pooling and favors the heterogeneous estimators, especially given the differences between the Paris region and the rural areas of France. Baltagi, Bresson, Griffin and Pirotte (2003) showed that the time series estimates for each region are highly variable, unstable, and offer the worst out-of-sample forecasts. Despite the fact that the shrinkage estimators proposed by Maddala, Trost, Li and Joutz (1997) outperformed these individual heterogeneous estimates, they still had a wide range and were outperformed by the homogeneous estimators in out-of-sample forecasts. In addition, Baltagi, Griffin and Xiong (2000) carried out this comparison for a dynamic demand for cigarettes across 46 U.S. states over 30 years (1963­1992). Once again the homogeneous panel data estimators beat the heterogeneous and shrinkage type estimators in RMSE performance for out-of-sample forecasts. In another application, Driver, Imai, Temple and Urga (2004) utilize the Confederation of British Industry's (CBI) survey data to measure the impact of uncertainty on U.K. investment authorizations. The panel consists of 48 industries observed over 85 quarters 1978(Q1) to 1999(Q1). The uncertainty measure is based on the dispersion of beliefs across survey respondents about the general business situation in their industry. Following Baltagi and Griffin (1997) and Pesaran and Smith (1995), this paper questions the poolability of this data across different industries. The heterogeneous estimators considered are OLS and 2SLS at the industry level, as well as the unrestricted SUR estimation method. Fixed effects, random effects, pooled 2SLS, and restricted SUR are the homogeneous estimators considered.

16 To Pool or Not to Pool?

519

The panel estimates find that uncertainty has a negative, non-negligible effect on investment, while the heterogeneous estimates vary considerably across industries. Forecast performance for 12 out-of-sample quarters 1996(Q2) to 1999(Q1) are compared. The pooled homogeneous estimators outperform their heterogeneous counterparts in terms of RMSE.
Maddala, Trost, Li and Joutz (1997) applied classical, empirical Bayes and Bayesian procedures to the problem of estimating short-run and long-run elasticities of residential demand for electricity and natural gas in the U.S. for 49 states over 21 years (1970­1990). Since the elasticity estimates for each state were the ultimate goal of their study they were faced with three alternatives. The first is to use individual time series regressions for each state. These gave bad results, were hard to interpret, and had several wrong signs. The second option was to pool the data and use panel data estimators. Although the pooled estimates gave the right signs and were more reasonable, Maddala, Trost, Li and Joutz (1997) argued that these estimates were not valid because the hypothesis of homogeneity of the coefficients was rejected. The third option, which they recommended, was to allow for some (but not complete) heterogeneity or (homogeneity). This approach lead them to their preferred shrinkage estimator which gave them more reasonable parameter estimates. In a followup study, Baltagi, Bresson and Pirotte (2002) reconsidered the two U.S. panel data sets on residential electricity and natural-gas demand used by Maddala, Trost, Li and Joutz (1997) and compared the out-of-sample forecast performance of the homogeneous, heterogeneous, and shrinkage estimators. Once again the results show that when the data is used to estimate heterogeneous models across states, individual estimates offer the worst out-of-sample forecasts. Despite the fact that shrinkage estimators outperform these individual estimates, they are outperformed by simple homogeneous panel data estimates in out-of-sample forecasts. Admittedly, there are additional case studies using U.S. data, but they do add to the evidence that simplicity and parsimony in model estimation offered by the homogeneous estimators yield better forecasts than the more parameter consuming heterogeneous estimators.
Proponents of the heterogeneous estimators include Pesaran and Smith (1995) and Pesaran, Shin and Smith (1999), who advocate abandoning the pooled approach altogether because of the inherent parameter heterogeneity, relying instead upon the average response from individual regressions. In fact, an earlier paper by Robertson and Symons (1992) studied the properties of some panel data estimators when the regression coefficients vary across individuals, i.e., they are heterogeneous but are assumed homogeneous in estimation. This was done for both stationary and nonstationary regressors. The basic conclusion was that severe biases can occur in dynamic estimation even for relatively small parameter variation. Using an empirical example of a real wage equation for a panel of 13 OECD countries observed over the period 1958­1986, Robertson and Symons (1992) show that parameter homogeneity across countries is rejected and the true relationship appears dynamic. Imposing false equality restriction biases the coefficient of the lagged wage upwards and the coefficient of the capital-labor ratio downwards.
Pesaran and Smith (1995) consider the problem of estimating a dynamic panel data model when the parameters are individually heterogeneous and illustrate their results by estimating industry-specific U.K. labor demand functions. In this case the model is given by

520

B.H. Baltagi et al.

yit = iyi,t-1 + ixit + uit i = 1, . . . , N t = 1, . . . , T

(16.1)

where i is i.i.d. ( , 2) and i is i.i.d. ( , 2). Further i and i are independent of yis, xis, and uis for all s. The objective in this case is to obtain consistent estimates of the mean values of i and i. Pesaran and Smith (1995) present four different estimation procedures:

(1) aggregate time-series regressions of group averages; (2) cross-section regressions of averages over time; (3) pooled regressions allowing for fixed or random intercepts; (4) separate regressions for each group, where coefficients estimates are averaged
over these groups.

They show that when T is small (even if N is large), all the procedures yield inconsistent estimators. The difficulty in obtaining consistent estimates for  and  can be explained by rewriting the above equation as

yit =  yi,t-1 +  xit + it

(16.2)

where it = uit + (i -  )yi,t-1 + (i -  )xit . By continuous substitution of yi,t-s it is easy to see that it is correlated with all present and past values of yi,t-1-s and xi,t-s for s 0. The fact that it is correlated with the regressors renders the OLS estimator inconsistent, and the fact that it is correlated with (yi,t-1-s, xi,t-s) for s > 0 rules out

the possibility of choosing any lagged value of yit and xit as legitimate instruments.

When both N and T are large, Pesaran and Smith (1995) show that the cross-section

regression procedure will yield consistent estimates of the mean values of  and  .

Intuitively, when T is large, the individual parameters i and i can be consistently

estimated using T observations of each individual i, say i and i; then, averaging

N

N

these individual estimators,  i/N and  i/N, will lead to consistent estimators

i=1

i=1

of the mean values of  and  .

Hsiao and Tahmiscioglu (1997) use a panel of 561 U.S. firms over the period

1971­1992 to study the influence of financial constraints on company investment.

They find substantial differences across firms in terms of their investment behavior.

When a homogeneous pooled model is assumed, the impact of liquidity on firm

investment is seriously underestimated. The authors recommend a mixed fixed and

random coefficients framework based on the recursive predictive density criteria.

Pesaran, Smith and Im (1996) investigated the small sample properties of vari-

ous estimators of the long-run coefficients for a dynamic heterogeneous panel data

model using Monte Carlo experiments. Their findings indicate that the mean group

estimator performs reasonably well for large T . However, when T is small, the mean

group estimator could be seriously biased, particularly when N is large relative to

T . Pesaran, Shin and Smith (1999) examine the effectiveness of alternative bias-

correction procedures in reducing the small sample bias of these estimators using

Monte Carlo experiments. An interesting finding is that when the coefficient of the

lagged dependent variable is greater than or equal to 0.8, none of the bias correc-

tion procedures seem to work. Hsiao, Pesaran and Tahmiscioglu (1999) suggest a

16 To Pool or Not to Pool?

521

Bayesian approach for estimating the mean parameters of a dynamic heterogeneous panel data model. The coefficients are assumed to be normally distributed across cross-sectional units and the Bayes estimator is implemented using Markov Chain Monte Carlo methods. Hsiao, Pesaran and Tahmiscioglu (1999) argue that Bayesian methods can be a viable alternative in the estimation of mean coefficients in dynamic panel data models even when the initial observations are treated as fixed constants. They establish the asymptotic equivalence of this Bayes estimator and the mean group estimator proposed by Pesaran and Smith (1995). The asymptotics are carried out for both N and T   with N/T  0. Monte Carlo experiments show that this Bayes estimator has better sampling properties than other estimators for both small and moderate size T . Hsiao, Pesaran and Tahmiscioglu (1999) also caution against the use of the mean group estimator unless T is sufficiently large relative to N. The bias in the mean coefficient of the lagged dependent variable appears to be serious when T is small and the true value of this coefficient is larger than 0.6. Hsiao, Pesaran and Tahmiscioglu (1999) apply their methods to estimate the q investment model using a panel of 273 U.S. firms over the period 1972­1993.
Depending on the extent of cross-sectional heterogeneity in the parameters, researchers may prefer these heterogeneous estimators to the traditional pooled homogeneous parameter estimators. In fact, Hsiao, Pesaran and Tahmiscioglu (1999) argued that there is not clarity in the literature about the appropriate estimation technique for dynamic panel data models, especially when the time series is short. They suggested a hierarchical Bayes approach to the estimation of such models using Markov Chain Monte Carlo methods (via Gibbs sampling).
By now, it is well known that pooling in the presence of parameter heterogeneity can produce misleading results. So, it is important to know if the pooling assumption is justified. Section 16.2 describes tests for poolability and Stein-rule methods. This is illustrated for a Tobin q investment application based on Hsiao and Tahmiscioglu (1997). Section 16.3 presents several heterogeneous estimators based on the sampling approach, the averaging approach and the Bayesian approach. Section 16.4 revisits the comparison of the out of sample forecast performance of the homogeneous and heterogeneous estimators in the context of the Tobin q application.

16.2 Tests for Poolability, Pretesting and Stein-Rule Methods

16.2.1 Tests for Poolability

The question of whether to pool the data or not naturally arises with panel data. The restricted model is the pooled model:

yit =  + Xit  + uit , i = 1, . . . , N, t = 1, . . . , T

(16.3)

which utilizes a one-way error component model for the disturbances:

522

B.H. Baltagi et al.

uit = i + vit

(16.4)

where i denotes the unobservable individual specific effect and vit denotes the remainder disturbance. In vector form, (16.3) can be written as:

y = eNT + X + u = Z + u

(16.5)

where y is (NT × 1), X is (NT × (k - 1)), Z = [eNT , X],  = (,  ), eNT is a vector of ones of dimension NT , and u is (NT × 1). Equation (16.4) can be written as:

u = Z  + v

(16.6)

where

Z = IN  eT ,  = (1, . . . , N) and v  i.i.d. 0, v2INT .

IN is an identity matrix of dimension N, eT is a vector of ones of dimension T , and
 denotes the Kronecker product. If the i are assumed to be fixed parameters, the
model is called the fixed effects model. If the i are assumed to be random (i.e., i  i.i.d. 0, 2 ), the model is called the random effects model.
This pooled model represents a behavioral equation with the same parameters

across individuals and over time. The unrestricted model, however, is the same be-

havioral equation but with different parameters across individuals and/or time. The

question of whether to pool or not to pool boils down to the question of whether the

parameters vary across individuals and/or over time. In what follows, we study the

tests of poolability of the data for the case of pooling across individuals keeping in

mind that the other case of pooling over time can be obtained in a similar fashion.

For the unrestricted model, we have a regression equation for each individual

given by:

yi = Zii + ui, i = 1, . . . , N

(16.7)

where yi is (T × 1), Zi = [eT , Xi], Xi is (T × k - 1), i = (i, i ), and ui is (T × 1). So, i is different for every individual equation. We want to test the hypothesis

H0 : i =  , i.

So, under H0, we can write the restricted model as: y = Z + u. The unrestricted

model can also be written as:



   

Z1 0 · · · 0

1

u1

y

=

Z 

+u

=



0 ...

Z2 · · · ...

0 ...





2 ...



+



u2 ...



(16.8)

0 0 · · · ZN N

uN

where Z = ZI with I = (eN  Ik), eN is a vector of ones of dimension N, and Ik is an identity matrix of dimension k.

16 To Pool or Not to Pool?

523

16.2.1.1

Test for Poolability Under u  N

0,



2 u

I

N

T

We suppose that the disturbance u follows an normal distribution of zero mean and

constant variance u2INT , i.e., i = 0, 2 = 0 . There is no individual specific effect. Then, the minimum variance unbiased (MVU) estimator for  is the OLS esti-

mator:

OLS = MLE = Z Z -1 Z y

and therefore

y = ZOLS + uOLS.

Similarly, the MVU for i is given by:

i,OLS = i,MLE = Zi Zi -1 Zi yi

and therefore

yi = Zii,OLS + ui,OLS.

Under H0, the following test statistic:

Fobs =

N
uOLSuOLS -  ui,OLSui,OLS / (N - 1) k
i=1 N
 ui,OLSui,OLS /N (T - k)
i=1

(16.9)

is distributed as F ((N - 1) k, N (T - k)). Hence, the critical region for this test is defined as:
{Fobs > F ((N - 1) k, N (T - k) ; 0)}
where 0 denotes the significance level of the test. This is exactly the Chow test extended to the case of N linear regressions. Therefore, if an economist has reason to believe that assumption u  N 0, u2INT is true, and wants to pool his data across individuals, then it is recommended that he test for the poolability of the data using the Chow test given in (16.9) . For an extension of the Chow test for poolability to a non-parametric panel data model that is robust to functional form misspecification, see Baltagi, Hidalgo, and Li (1996).
The problem with the Chow test is that  = u2INT . In fact, for the one-way error component model

 = E uu = Z E   Z + E vv = 2 (IN  JT ) + v2 (IN  IT )

(16.10)

where JT = eT eT . Therefore, even if we assume normality on the disturbances, the Chow statistic will not have the F-distribution described above. However, a gener-
alized Chow test which takes into account the general form of  will be the right
test to perform. This is taken up in the next section.

524

B.H. Baltagi et al.

16.2.1.2 Test for Poolability Under the General Assumption u  N (0, )

All we need to do is transform our model (under both the null and alternative hypothesis) such that the transformed disturbances have a variance of u2INT , then apply the Chow test on the transformed model. Given  = u2, we premultiply the restricted model by -1/2 and call -1/2y = y. Hence:

y = Z +u

with E [uu ] = -1/2E [uu ] -1/2 = u2INT . Similarly, we premultiply the unrestricted model (16.8) by -1/2 and call -1/2Z = Z. Therefore,
y = Z +u

with E [uu ] = u2INT . At this stage, we can test H0 : i =  for every i = 1, 2, . . . , N, simply by using the Chow test only now on the transformed models. Under H0, the
following test statistic:

y -1 Z Z -1Z -1 Z - Z Z -1Z -1 Z -1 y/ (N - 1) k

Fobs =

y -1y - y -1Z (Z -1Z)-1 Z -1y /N (T - k)

(16.11)

is distributed as F ((N - 1) k, N (T - k)). It is important to emphasize that (16.11) is operational only when  is known. This test is a special application of a general test for linear restrictions described by Roy in 1957 and used by Zellner in 1962 to test for aggregation bias in a set of seemingly unrelated regressions. In case  is unknown, we replace  in (16.11) by a consistent estimator (say ) and call the
resulting test statistic Fobs. One of the main motivations behind pooling a time series of cross-sections is
to widen our database in order to get better and more reliable estimates of the parameters of our model. Using the Chow test, the question of whether "to pool or not to pool" is reduced to a test of the validity of the null hypothesis H0 : i =  for all i. Imposing these restrictions (true or false) will reduce the variance of the pooled estimator, but may introduce bias if these restrictions are false. Baltagi (2005, pp. 54­58) discusses three mean squared error criteria suggested by Wallace (1972) for  = u2INT and by McElroy (1977) for  = u2. These MSE criteria do not test H0, but rather help us choose on pragmatic grounds between the restricted pooled model and the unrestricted heterogeneous model. Using Monte Carlo experiments, Baltagi (1981) shows that the Chow test performs poorly, rejecting poolability when true under a random error component model whenever the variance components are large. Weaker MSE criteria reduced the frequency of type I error committed by the Chow test. However, the weaker MSE criteria performance was still poor compared to the Roy­Zellner test or the extensions of these weaker MSE criteria for a general .

16 To Pool or Not to Pool?

525

Recently, Bun (2004) focused on testing the poolability hypothesis across crosssection units assuming constant coefficients over time. In particular, this testing applies to panel data with a limited number of cross-section units, like countries or states observed over a long time period, i.e., with T larger than N. Bun (2004) uses Monte Carlo experiments to examine the actual size of various asymptotic procedures for testing the poolability hypothesis. Dynamic regression models as well as nonspherical disturbances are considered. Results show that the classical asymptotic tests have poor finite sample performance, while their bootstrapped counterparts lead to more accurate inference. An empirical example is given using panel data on GDP growth and unemployment rates in 14 OECD countries over the period 1966­1990. For this data set, it is shown that the classical asymptotic tests reject poolability while their bootstrap counterparts do not.

16.2.2 Pretesting and Stein-Rule Methods

Choosing a pooled estimator if we do not reject H0 : i =  for all i, and the heterogeneous estimator if we reject H0 leads to a pretest estimator.1 This brings into question the appropriate level of significance to use with this preliminary test. In
fact, the practice is to use significance levels much higher than 5%; see Maddala
and Hu (1996).
Another problem with the pretesting procedure is that its sampling distribution
is complicated; see Judge and Bock (1978). Also, these pretest estimators are dom-
inated by Stein-rule estimators under quadratic loss function. Using a wilderness
recreation demand model, Ziemer and Wetzstein (1983) show that a Stein-rule estimator gives better forecast risk performance than the pooled (OLS) or individual estimators (i,OLS). The Stein-rule estimator is given by:

iS =

c Fobs

OLS +

1- c Fobs

i,OLS.

(16.12)

The optimal value of the constant c suggested by Judge and Bock (1978) is:

(N - 1) k - 2

c

=

N

(T

-

k)

+

. 2

Note that iS shrinks i,OLS towards the pooled estimator OLS explaining why this estimator is often called Stein-rule shrinkage estimator. When N is large, the factor c is roughly k/(T -k). If, in addition, the number of explanatory variables k is small relative to the number of time periods T , c will be small and, for a given
Fobs, the shrinkage factor towards the pooled estimator (c/Fobs) will be small. The

1 Hsiao and Pesaran (2007) present several Hausman type tests for slope heterogeneity based on the mean group estimator proposed by Pesaran, Shin and Smith (1999) (see the Sect. 16.3.1).

526

B.H. Baltagi et al.

Bayesian and empirical Bayesian methods (discussed later) imply shrinking towards a weighted mean of the i and not the pooled estimator  .

16.2.3 Example

We illustrate the tests of poolability and the Stein-rule method using a simple dynamic version of the classical Tobin q investment model studied by Hsiao and Tahmiscioglu (1997):

or in vector form

I K

= i + 1i
it

I K

+ 2iqit + uit
it-1

yit = Zit i + uit

(16.13)

where Iit denotes investment expenditures by firm i during period t, Kit is the replacement value of the capital stock, and qit is Tobin's q of the firm. Tobin's q theory relates investment to marginal q, which is the ratio of the market value of new investment goods to their replacement cost. Thus, investment will be an increasing function of marginal q. Because marginal q is unobservable, it is common in empirical work to substitute it with average or Tobin's q. If a firm has unexploited profit opportunities, then an increase of its capital stock price of 1 unit will increase its market value by more than one unit (q > 1). Firms can be expected to increase investment until marginal q equals 1. On the other hand, if a firm has already more than adequate capital, then an increase in capital stock by one unit will increase its market value by less than one unit (q < 1). 1i is the investment inertia of firm i and (1 - 1i) is the speed of adjustment. The panel data set used in this study contains 337 U.S. firms over 17 years (1982­1998).2 Hsiao, Pesaran and Tahmiscioglu (1999) argued that the troubling findings of large estimates often obtained for the adjustment cost parameters and the implied slow speeds of adjustment of the capital stock to its equilibrium value may be due, at least partly, to the use of inappropriate estimation techniques when there is significant heterogeneity in firm investment responses to the q variable. The restricted model is:

or in vector form

I K

=  + 1
it

I K

+ 2qit + uit
it-1

yit = Zit  + uit .

(16.14)

2 This was kindly provided by Cheng Hsiao and A. Kamil Tashmiscioglu. This is not exactly the same data set as the one used by Hsiao, Pesaran and Tahmiscioglu (1999) which contains 273 firms over the 20 years (1973­1992). For a detailed description of these variables, see Hsiao and Tahmiscioglu (1997).

16 To Pool or Not to Pool?

Table 16.1 Tests of poolability for the Tobin q investment model3 N = 337, T = 16

OLS

LSDV


1
2
Chow test (intercept and slopes) Chow test (slopes only) Roy­Zellner test Stein-rule shrinkage factor

0.0872
(25.695)
0.4469
(39.138)
0.0079
(18.829)

--
0.2777
(22.136)
0.0157
(23.180)
2.6026  F(1008, 4381) 2.4234 F(672, 4718) 1.5796  F(1008, 4381)
0.9118

527
FGLS
0.0926
(2.585)
0.3252
(26.929)
0.0123
(21.838)

We want first to check whether coefficients are constant or not across firms, that is if H0 : i =  for all i.
Table 16.1 shows that the Chow test for poolability across firms gives an observed F-statistic of 2.6026 which is distributed as F(1008, 4718) under H0 : i =  for i = 1, . . . , N. There are 1008 restrictions and the test rejects poolability across
firms for all the coefficients. One can test for poolability of slopes only, allowing for
varying intercepts. The restricted model is the within regression with firm dummies
(LSDV). The observed F-statistic of 2.4234 is distributed as F(672, 4718) under H0 :  ji =  j for j = 1, 2 and i = 1, . . . , N. This again is significant at the 5% level and rejects the poolability of the slopes across firms. The Roy­Zellner test for poola-
bility across firms, allowing for one-way error component disturbances, yields an observed F-value of 1.5796 and is distributed as F(1008, 4381) under H0 : i =  for i = 1, . . . , N. This still rejects poolability across firms even after allowing for
one-way error component disturbances. The Stein-rule shrinkage factor is 91.18%, so the Stein-rule estimator iS is a linear combination of 8.82% weight on the pooled estimator OLS and 91.18% on the heterogeneous estimator i,OLS.

16.3 Heterogeneous Estimators

When the data do not support the hypothesis of fixed coefficients, it would seem reasonable to allow for variations in the parameters across cross-sectional units. For the ith individual, a single-equation model can be written as:

yi = Zii + ui, i = 1, . . . , N

(16.15)

that is:

yi  N (Zii, i)

(16.16)

3 t-Statistics are in parentheses.

528

B.H. Baltagi et al.

with i = E [uiui] = i2IT . If all the parameters are treated as fixed and different for cross-sectional units, there will be Nk parameters with only NT observations. Obviously, there is no way we can obtain any meaningful estimates of i, especially when k is closed to T . Alternatively, each regression coefficient can be viewed as
a random variable with a probability distribution. The random coefficients specifi-
cation reduces the number of parameters to be estimated substantially, while still allowing the coefficients to differ from unit to unit and/or from time to time. i are assumed to be independently normally distributed with mean  and covariance  (with Cov (i,  j) = 0, i = j):

i =  + i, i  N  ,  .

(16.17)

Substituting i =  + i into (16.15) yields:

yi = Zi + vi
where vi = Zii + ui. This leads us to the Swamy (1970) model. Stacking all NT observations, we have:

(16.18)

y = Z +v

(16.19)

where v = Z + u. The covariance matrix for the composite disturbance term v is bloc-diagonal, and is defined by

V [v] =  = diag (1, . . . , N)

where

i = ZiZi + i2IT .

The best linear unbiased estimator of  for (16.19) is the GLS estimator:

N
  GLS = ii,OLS i=1

where

-1
i,OLS = Zi Zi Zi yi

and

i =

N +i2 Zi Zi -1 -1
i=1

-1

+i2

Zi Zi

-1

-1
.

(16.20) (16.21) (16.22)

The covariance matrix for the GLS estimator is:

V  GLS =

N

+i2

Zi Zi

-1 -1

-1
.

i=1

(16.23)

16 To Pool or Not to Pool?

529

Swamy proposed to use the least-squares estimators i and their residuals ri = yi - Zii,OLS to obtain unbiased estimators of i2 and  (see Hsiao and Pesaran (2007)). Swamy (1970) provides an asymptotic normal and efficient estimator of the mean coefficients. Pesaran and Smith (1995), and Pesaran, Shin and Smith (1999) advocate alternative estimators which they call respectively the Mean Group estimator and the Pooled Mean Group estimator.

16.3.1 Averaging Estimators

The Mean Group estimator is obtained by estimating the coefficients of each crosssection separately by OLS and then taking an arithmetic average:

 

=

1 N

N
i,OLS.
i=1

(16.24)

When T  , i,OLS  i and (16.24) will be consistent when N also goes to infinity. This estimator has obviously only asymptotic justification. However, it would be interesting to have some idea about its performance in finite sample, particularly as compared to Bayesian type estimators.
Pesaran, Shin and Smith (1999) proposed an estimator called the Pooled Mean Group estimator which constrains the long-run coefficients to be the same among individuals. Suppose that we want to estimate an ADL model:

p

q

  yit = Zit i + uit = i jyi,t- j + i jxi,t- j + i dt + uit

j=1

j=0

(16.25)

where xit is a (k × 1) vector of explanatory variables and dt is a (s × 1) vector of observations on fixed regressors such as intercept and time trends or variables that vary only over time. We can re-parametrize (16.25):

p-1

q-1

  yit = iyi,t-1 + i xit + ijyi,t- j + ij xi,t- j + i dt + uit

j=1

j=0

(16.26)

where

p

q

p

q

    i = - 1 - i j , i = i j, ij = -

im and ij = -

im.

j=1

j=0

m= j+1

m= j+1

If we stack the time series observations for each group, the error-correction model (16.26) becomes:

530

B.H. Baltagi et al.

p-1

q-1

  yi = iyi,-1 + i Xi + ijyi,- j + i j Xi,- j + i Dt + i

(T,1)

(T,1)

(T,k) j=1 (T,1) j=0 (T,k)

(16.27)

where yi = (yi1, . . . , yiT ) , Xi = (xi1, . . . , xiT ) , and Dt = (d1, . . . , dT ) . If i < 0, there exists a long-run relationship between yit and xit defined by:

yit =

-

i i

xit + vit , i.

(16.28)

Pesaran, Shin and Smith (1999) constrain the long-run coefficients on Xi defined by i = (-i/i) to be the same across individuals or across groups of individuals:

i =  , i.

(16.29)

So, the ECM can be written more compactly as:

yi = ii ( ) +Wii + ui

(16.30)

where the error correction component is:

i ( ) = yi,-1 - Xi

and

Wi = yi,-1, . . . , yi,-p+1, Xi, Xi,-1, . . . , Xi,-q+1, D i = i1, i2, . . . , ip-1, i0, i1, . . . , iq-1, i .
If the disturbances are normally distributed, the ML estimation of the long-run coefficients  and the individual-specific error-correction coefficients i are obtained by maximizing the concentrated likelihood (see Pesaran, Shin and Smith (1999)).

16.3.2 Bayesian Framework
The underlying probability interpretation for a Bayesian is a subjective one, referring to a personal degree of belief. The rules of probability calculus are used to examine how prior beliefs are transformed to posterior beliefs by incorporating data information.4 Here we only consider cases where the model parameter vector  is of finite dimension. A Bayesian then focuses on the inference of  (treated as a random variable) conditional on y and the underlying model M, summarized in the posterior density p  |y, M . The observations in y define a mapping from the prior
4 See Chib (2001).

16 To Pool or Not to Pool?

531

p  into p  |y, M . The posterior distribution of  can be derived by expressing
the likelihood function conditional on the initial values yi0 and combining it with the prior distribution of  :

p  |y, yi0  p y| p  .

Lindley and Smith (1972) proposed a three-stage hierarchy. The joint density function of the data y is such that:
y  N (Z , ) where  = E uu

is indexed by a k-vector  of unknown parameters assumed to be normally distributed
  N , .
The third stage of the hierarchy corresponds to the prior distribution of 
  N (, ) .

Using the properties of the multivariate normal distribution, we can define the conditional density of y given  . If y  N (Z , ) and   N  ,  , then the marginal distribution of y conditional on  is
y  N Z,

where  =  + Z Z . Combining this with the prior distribution of  yields to the posterior density of  . Then, the posterior density is proportional to:

p  |y, yi0

 exp

-1 y-Z 2

-1 y - Z - 1  -  2

-1  - 

.

Assuming prior ignorance at the third stage of the hierarchy (i.e., -1 = 0) yields to the following posterior distribution of  :
  N Z -1Z -1 Z -1y, Z -1Z -1

From a frequentist point of view and in order to estimate  ,  , i2 and , we must theoretically maximize the following log likelihood (see Maddala, Trost, Li and
Joutz (1997)):

532

B.H. Baltagi et al.

we get

 Log L  ,  , i2,  | y, Z

=

Cst

-

T 2

N
Log i2
i=1

-

N 2

Log ||

 - 1
2

N i=1

1 i2

(yi - Zii)

(yi

- Zii)

 - 1 N
2 i=1

i - 

-1 i - 

i2

=

1 T

(yi - Zii)

(yi - Zii)

  =

1N N i=1

i - 

i - 

 

=

1 N

N
i
i=1

and

i =

-1

+

1 i2

Zi

Zi

-1

-1

+

1 i2

Zi

Zii,OLS

.

which is the same as i =  +  Zi Zi Zi + i2IT -1 yi - Zi

For estimating Maximum Likelihood parameters i,  and i2, we must run a first step and use the OLS estimator for each individual.
The traditional approach to estimating regression coefficients with panel data is a dichotomy of either estimating i from the data on the ith cross-section unit or from the pooled sample. The general solution that emerges from the Bayesian approach is to shrink each individual i from the ith cross-section towards a common estimate  .
We suppose that i  N  ,  .

This statement defines the prior distribution of i. The parameters  and  are un-
known, then we must make some assumptions. After this, we can obtain the posterior distribution of i. If  , i2 and  were known, then the posterior distribution of i is given by:

i =

-1

+

1 i2

Zi

Zi

-1

-1

+

1 i2

Zi

Zii,OLS

16 To Pool or Not to Pool?

533

and its variance:

V

i

=

-1

+

1 i2

Zi

Zi

-1

where i,OLS is the OLS estimator of i. The posterior distribution mean of  is

defined by:

 


=

1 N

N
i.
i=1

Lindley and Smith (1972) have shown that prior distributions for nuisance parameters (including the variance­covariance matrix of the hyperparameters like  ) lead to integrals which cannot be all expressed in closed form. They suggest an approximation which consists of using the mode of the posterior distribution rather than the mean.5 The former empirical Bayes estimator has been followed by other empirical Bayes methods such as iterative Bayes and empirical iterative Bayes estimators (see Maddala, Trost, Li and Joutz (1997) and Table 16.2).
A more flexible tool is the rejection sampling method discussed in Gelfand and Smith (1990, 1992) when the only requirement is that the maximum value of sampling density Max p (y| , M) be finite. Hence, if we can draw from the prior

p ( ), we can generate drawings from the posterior p ( |y, M) simply by rejection. Markov Chain Monte Carlo (MCMC) simulations versions of this accept­ reject algorithm have been recently proposed. As underlined by Chib (2001, 2007), these methods have revolutionized Bayesian statistics. One very popular MCMC method, introduced by Gelfand and Smith (1990), is called the Gibbs sampling method. Therefore, a full Bayesian implementation of the model is now feasible using sampling-based approaches to calculate marginal densities. Using Gibbs sampling, Hsiao, Pesaran and Tahmiscioglu (1999) have proposed the "Hierarchical Bayes" estimator.

16.3.2.1 Iterative Bayes Estimator

In general, i2 are  unknown parameters. Then we must make some prior assumptions about these parameters. Smith (1973) proposed for -1 a conjugate Wishart distribution and for i2 some independent inverse 2 distributions (see the Sect. 16.3.2.4). Instead of the mean of the distribution, he used the posterior mode
of the distribution:

i2

=

T

+

1 i

+

2

ii +

yi - Zii

yi - Zii

(16.31)

and

 

=

N

-k

1 -2

+



N
R+
i=1

i -  

i -  

(16.32)

5 This approximation is likely to be good only if the samples are fairly large and the resulting posterior distributions approximatively normal.

534

B.H. Baltagi et al.

where i, i , , and R are hyperparameters of the prior distribution. Smith (1973) proposes to approximate these hyperparameters by using i = 0,  = 1 and R is a diagonal matrix with small positive entries (= 0.001). The estimators are:

i2

=

T

1 +2

yi - Zii

yi - Zii

(16.33)

 

=

N

1 -k

-1

N
R+
i=1

i -  

i -  

(16.34)

i =

-1

+

1 i2

Zi

Zi

-1

-1



+

1 i2

Zi

Zii,OLS

(16.35)

and

 


=

1 N

N
i.
i=1

(16.36)

The equations (16.33) to (16.36) must be estimated by iterative procedure. The initial iteration use OLS estimates.

16.3.2.2 Empirical Bayes Estimator

This estimator has been proposed by Smith (1973). It is a quite different as empirical Bayes's estimator proposed by Rao (1975). It is defined as:

 


=

1 N

N
i,OLS
i=1

i2

=

T

1 -k

yiyi - yiZii,OLS

 

=

N

1 -

1

N i=1


i,OLS - 


i,OLS - 

and

i =

-1

+

1 i2

Zi

Zi

-1

-1



+

1 i2

Zi

Zii,OLS

.

This estimator is based on OLS estimates. The estimators of i2 and  are unbiased if Zi contains only exogenous variables.

16.3.2.3 Empirical Iterative Bayes Estimator
This estimator was proposed by Maddala, Trost, Li and Joutz (1997). The parameters i2 and  are estimated by:

16 To Pool or Not to Pool?

535

i2

=

T

1 -

k

yi - Zii

yi - Zii

and

 

=

N

1 -

1

N
R+
i=1

i -  

i -  

;

then we can compute

i =

-1

+

1 i2

Zi

Zi

-1

-1



+

1 i2

Zi

Zii,OLS

and

V

i

=

-1

+

1 i2

Zi

Zi

-1
.

For the first iteration, we use the OLS estimates. Maddala, Trost, Li and Joutz (1997) argue that the iterative process for estimating  and  will yield to more efficient

estimates of these parameters.

16.3.2.4 Hierarchical Bayes Estimator

We have previously seen that Lindley and Smith (1972) have proposed a three-stage

hierarchy. The first stage of the hierarchy corresponds to the joint density function

of the data y such that:

y  N (Z , )

where  = E [uu ]. The second stage of the hierarchy is defined as

  N ,

and the third stage of the hierarchy corresponds to the prior distribution of  :
  N (, ) .
So, the marginal distribution of y conditional on  is y  N Z,
where  =  + Z Z . Assuming prior ignorance at the third stage of the hierarchy (i.e., -1 = 0) yields to the following posterior distribution of  :
  N Z -1Z -1 Z -1y, Z -1Z -1   N ,V .

536

Table 16.2 Shrinkage estimators

Estimators of i2

B.H. Baltagi et al.

Swamy ML Iterative Bayes Empirical Bayes Empirical Iterative Bayes
Swamy
ML Iterative Bayes Empirical Bayes Empirical Iterative Bayes
Swamy ML Bayes
Swamy Empirical Bayes Others

1 T -k

yi - Zii,OLS

yi - Zii,OLS

1 T

yi - Zii

yi - Zii

1 T +2

yi - Zii

yi - Zii

1 T -k

yiyi - yiZii,OLS

1 T -k

yi - Zii

yi - Zii

Estimators of 



=

1 N-1

N

i=1

i,OLS

-

1 N

N
 i,OLS
i=1

i,OLS

-

1 N

N
 i,OLS
i=1

-

1 N

N


i2

(Zi Zi)-1

i=1

=

1 N

N


i - 

i=1

i - 

N





=

1 N-k-1

R+ 
i=1

i - 


i - 



=

1 N-1

N



i,OLS - 

i=1


i,OLS - 

N







=

1 N-1

R+ 
i=1

i - 

i - 

Estimators of i

No estimator

i = i =

-1-+1 +1i21Zi2 iZZiiZi-1-1-1-1+

1 i2

Zi

Zi

i,OLS

+

1 i2

Zi Zii,OLS

Estimators of 

N

 =  ii,OLS

i=1


=

1 N

N
 i,OLS

i=1


 (and  ) =

1 N

N


i

i=1

Following (16.20), (16.21), and (16.22), we get:

where

   = N ii,OLS i=1

i =

N

+i2

-1 -1
Zi Zi

-1

+i2

-1 -1

Zi Zi

.

i=1

(16.37)

16 To Pool or Not to Pool?

537

The posterior mean   is a weighted average of the least squares estimates of individuals units and can be used as a point estimate of  . In practice, the variance components  and i2 in (16.37) are unknown. The proposed solution of Lindley and Smith was an approximation that consists of using the mode of the posterior
distribution rather than the mean. The proposed solution was labelled the empiri-
cal Bayes estimator. Normally, the marginal posterior densities of the parameters
of interest can be obtained by integrating out the hyperparameters from the joint
posterior density:
p ( |y) = p  | , y p  |y d .

The required integration poses an insurmountable challenge and closed-form analytic solutions cannot be obtained. Nevertheless, a full Bayesian implementation of the model is now feasible as a result of recent advances in sampling-based approaches to calculate marginal densities. The Gibbs sampling algorithm has been used successfully by Hsiao, Pesaran and Tahmiscioglu (1999).6
The Gibbs sampler is an iterative Markov Chain Monte Carlo (MCMC) method which only requires the knowledge of the full conditional densities of the parameter vector; see Chib (2001, 2007).
Starting from some arbitrary initial values, say  (0) = 1(0), 2(0), . . . , k(0) for a parameter vector  = (1, 2, . . . , k), it samples (generates) alternately from the conditional density of each component of the parameter vector conditional on the values of other components sampled in the last iteration M. The vectors
 (1),  (2), . . . ,  (M) will form a Markov Chain with transition probability from
stage  to the next one  being:

H  ,  = p 1|2, 3, . . . , k, y p 2|1, 3, . . . , k, y . . . p (k|1, . . . , k-1, y) .

As the number of iterations M  , the samples values can be regarded as drawing from the true joint and marginal posterior densities. In order to implement Gibbs sampling, we need to specify the prior distribution of the hyperparameters. They are assumed to be independent and distributed as:

N

 p -1,  = Wk -1| (R)-1 , 

i-2

i=1

where Wk represents the Wishart distribution7 with scale matrix (R) and degrees of freedom . With this structure, the joint density of all the parameters may be
written as:

6 See also Hsiao (2003). 7 A random symmetric positive definite (k, k) matrix A is said to follow a Wishart distribution Wk (A|u, v) if the density of A is given by:

|A|(v-k-1)/2 |u|v/2 exp

- 1 tr u-1A 2

.

See Koop (2003).

538

B.H. Baltagi et al.

  p i,  , , i2|y, yi0

N
 i-T exp
i=1

-1 2

N
i-2 (yi - Zii)
i=1

(yi - Zii)

 ×||-

N 2

exp

-1 N 2 i=1

i - 

-1 i - 

×||-

1 2

exp

-1

 -

-1  - 

2

×||-

1 2

(

-k-1)

exp

- 1tr (R) -1

2

N
 × i-2. i=1

The first line of the above formula corresponds to the standard likelihood function and the others represent the prior information. The relevant conditional distributions that are needed to implement the Gibbs sampler in this case are obtained from the joint posterior density:

p i|y,  , -1, 12, . . . , N2

= N Ai i-2Zi yi + -1 , Ai

p  |y, 1, . . . , N , -1, 12, . . . , N2 = N B N-1 + -1 , B

p -1|y, 1, . . . , N ,  , 12, . . . , N2 = Wk

N

-1

 i -  i -  + R ,  +N

i=1

p i2|y, 1, . . . , N ,  , -1

= IG T /2, (yi - Zii) (yi - Zii) /2

where

 Ai =

i-2Zi Zi + -1 -1 , B =

N-1 + -1

-1 , 

=

1 N

N
i
i=1

for i = 1, . . . , N and IG denotes the inverse gamma distribution. These values sampled after some initial number of iterations can be used to construct estimates of the parameters of interest. Hsiao, Pesaran and Tahmiscioglu (1999) call the estimator of  obtained using Gibbs sampling the "hierarchical Bayes" estimator.

16.3.3 An Example
Following Hsiao, Pesaran and Tahmiscioglu (1999), Baltagi, Bresson and Pirotte (2004) considered the simple dynamic version of the classical Tobin q investment model (16.13). The panel data set used in this study contains 337 U.S. firms over

16 To Pool or Not to Pool?

539

17 years (1982­1998). The estimation results are given in Table 16.3. We first give the results of 9 homogeneous panel data estimators. These include OLS, which ignores the individual effects; the Within estimator, which allows for fixed individual effects; and FGLS, which assumes that individual effects are random. Hsiao, Pesaran and Tahmiscioglu (1999) report the fixed effects estimates for a subset of

Table 16.3 Estimates of the q investment model

Model type

Intercept

OLS Within

0.086 (24.528)

FGLS 2SLS Within-2SLS

0.092 (18.823) 0.087 (17.646)

2SLS­KR FD2SLS

0.083 (15.990)

FD2SLS­KR

FDGMM

Heterogeneous estimators Average OLS Swamy Average ML Average 2SLS Average empirical Bayes Average iterative empirical Bayes Average iterative Bayes Hierarchical Bayes Pooled mean group

0.019 (2.557) 0.041 (4.915) 0.050 (20.483) 0.017 (0.981) 0.037 (8.148) 0.054 (25.059) 0.055 (32.123) 0.063 (15.080) 0.039 (10.197)

Note: Numbers in parentheses denote t-statistics.

(I/K)i, t-1
0.464 (38.718) 0.287 (21.565) 0.339 (26.612) 0.457 (21.017) 0.163 (5.429) 0.453 (21.072) 0.296 (10.047) 0.313 (16.419) 0.347 (37.978)
0.277 (20.254) 0.327 (20.357) 0.313 (55.947) 0.280 (5.694) 0.297 (29.185) 0.332 (57.470) 0.342 (71.069) 0.423 (26.496) 0.374 (20.613)

qi, t
0.007 (17.445) 0.015 (21.921) 0.011 (20.363) 0.007 (16.737) 0.016 (22.176) 0.008 (16.020) 0.026 (21.407) 0.022 (20.648) 0.022 (29.833)
0.042 (17.011) 0.028 (9.655) 0.026 (34.101) 0.042 (11.972) 0.032 (24.557) 0.023 (34.983) 0.022 (39.124) 0.014 (12.490) 0.022 (34.416)

540

B.H. Baltagi et al.

273 firms over a longer time period 1973­1992. The estimate of the q coefficient is 0.009 compared to 0.015 for our sample. The corresponding estimates of the coefficient of lagged (I/K) are 0.295 and 0.287, respectively. The pooled OLS estimates yield a lower q coefficient estimate of 0.007 and a higher coefficient of lagged (I/K) of 0.464. The FGLS estimates lie in between the OLS and Within estimates.
Since our model is dynamic, we also focus on pooled estimators employing twostage least squares (2SLS) using as instruments the exogenous variables and their lagged values. These 2SLS estimates were pretty close to OLS, while the Within 2SLS estimates yielded a lower estimate of the coefficient of lagged (I/K) than that of Within. In addition, we report the first-difference 2SLS (FD2SLS) estimator proposed by Anderson and Hsiao (1982) in which fixed or random individual effects are eliminated and predetermined variables are used as instruments. This yielded an even higher estimate of the q coefficient (0.026) than 2SLS but a lower estimate of the coefficient of lagged (I/K) of 0.296. Keane and Runkle (1992) (hereafter denoted by KR) suggest a modification of the 2SLS estimator that allows for any arbitrary type of serial correlation in the remainder error term. We refer to this estimator as 2SLS­KR. Still another variant of this estimator allows for any arbitrary form of serial correlation in the first differenced model. This is denoted as the FD2SLS­KR estimator. The 2SLS­KR estimates are close to those of 2SLS, while the FD2SLS­KR estimates are close to those of FD2SLS. Finally, following Arellano and Bond (1991), we used a generalized method of moments (GMM) estimator on the first-difference specification (FDGMM) with instruments in levels. This incorporates more orthogonality conditions than are usually used by the Anderson and Hsiao (1982) estimator as well as a general robust variance­ covariance matrix specification allowed by GMM. This yielded estimates close to those of FD2SLS.
For the heterogeneous estimators of Table 16.3, we first compute individual OLS and 2SLS regressions. The average OLS and 2SLS estimates of the q coefficient are around 0.042, while the estimates of the coefficient of lagged (I/K) are around 0.28. These are higher for Tobin's q coefficient estimate and lower for the estimate of the coefficient of lagged (I/K) than the mean group estimator obtained by Hsiao, Pesaran and Tahmiscioglu (1999). The latter were 0.037 and 0.323, respectively. We also computed the Swamy (1970) random coefficient regression estimator which is a weighted average of the individual least squares estimates where the weights are inversely proportional to their variance­covariance matrices. This yielded a lower q coefficient estimate of 0.026 than average OLS and a higher estimate of the coefficient of lagged (I/K) of 0.327. From the individual Maximum Likelihood estimators, based on the normality assumption, several shrinkage estimators have been proposed in the literature including the empirical Bayes estimator, the iterative Bayes estimator, and the iterative empirical Bayes estimator. The average ML estimates are close to those of Swamy. The average empirical Bayes estimate of the q coefficient is 0.032 while that of average iterative empirical Bayes and average iterative Bayes are 0.023 and 0.022, respectively. Next, we compute the Hsiao, Pesaran and Tahmiscioglu (1999) hierarchical Bayes estimates. This yields a q coefficient estimate of 0.014 compared to 0.0174 for the different sample used by

16 To Pool or Not to Pool?

541

Hsiao, Pesaran and Tahmiscioglu (1999). The corresponding estimates of the coefficient of lagged (I/K) are 0.423 and 0.431, respectively. Normal densities for lagged (I/K) and q coefficients are drawn. Finally, we compute the Pesaran, Shin and Smith (1999) Pooled Mean Group estimator. This estimator constrains the long-run coefficients to be identical but allows the short-run coefficients and error variances to differ across individuals. Long-run coefficients and individual-specific error correction coefficients are estimated using Maximum Likelihood. These ML estimates are referred to as pooled mean group estimators in order to highlight the pooling effect of the homogeneity restrictions on the estimates of the long-run coefficients and the fact that averages across individuals are used to obtain individual-wide mean estimates of the error-correction coefficients and the other short-run parameters of the model. This yields a q coefficient estimate of 0.022 and a lagged (I/K) coefficient estimate of 0.374.

16.4 Comments on the Predictive Approach
In some problems, it is interesting to predict one or more post-sample observations on a given individual over several periods. In the Bayesian context, the problem of prediction is solved by calculation of the predictive density.

16.4.1 From the Post-sample Predictive Density. . .
A fundamental goal in any statistical analysis is to predict a set of future observations at time (T + ), say YT+ , given the observed data YT and the underlying model M. Forecasting in the Bayesian context is done through the calculation of the prediction density defined as the distribution YT+ conditioned on (YT , M) but marginalized over the parameters  . The post-sample predictive density is defined as (see Hsiao and Tahmiscioglu (1997)):
p (YT+ |YT , M) = p (YT+ |YT , M,  ) p ( |YT , M) d
where p (YT+ |YT , M,  ) is the conditional density of YT+ given (YT , M,  ) and the marginalization is with respect to the posterior density p ( |YT , M) of  . Thus, when interested in forecasting future values YT+ , one uses the posterior distribution p ( |YT , M) to integrate out the parameters and gets the predictive density where p (YT+ |YT ,  , M) is obtained from the sampling model. The Bayesian approach naturally gives rise to predictive densities where all parameters are integrated out, making it a perfect tool for forecasting. Of course, all this comes at a cost, which is typically of a computational nature. In general, the predictive density is not available in closed form. An analytical solution to the computational problem is provided by summarizing prior information through restrictions to natural-conjugate prior

542

B.H. Baltagi et al.

densities (see Zellner (1971)). A natural-conjugate prior shares the functional form of the likelihood. When it belongs to exponential families, this leads to posterior densities of the same form. But now, a full Bayesian implementation of the model is now feasible using sampling-based approaches to calculate marginal densities. Using Gibbs sampling, Hsiao, Pesaran and Tahmiscioglu (1999) have proposed the "hierarchical Bayes" estimator. For individual i, if we want to predict y at time T + 1, say yi,T+1, we should use the conditional density p (yi,T+1|YT ) where YT includes all observed data (yi,1, . . . , yi,T ) as well as the data on explanatory variables
Zi,1, . . . , Zi,T , Zi,T+1 where Zi,T+1is the k-vector of future explanatory variables
1, yi,T , Xi,T +1 .8 Consequently, for the first step-ahead forecast, the predictive density is defined as:

p (yi,T +1|Yi,T ) = p (yi,T +1|Yi,T ,  ) p ( |Yi,T ) d
 N (E (yi,T +1|Yi,T ) ;V (yi,T +1|Yi,T )) , Yi,T  (yi ... yi,T +1) .
So, the expected future value yi,T+1 is the mean of draws from the normal distribution. Using the properties of the multivariate normal distribution, we define the conditional density of yi,T+2 given the observed data and the parameters, and so on. So, differences--between forecast values at time T + --for several Bayes estimators (Empirical Bayes, Iterative Bayes, Empirical Iterative Bayes and Hierarchical Bayes) come from the differences in the estimation of  , , i2 .

16.4.2 . . . to the Good Forecast Performance of the Hierarchical Bayes Estimator: An Example
Let us consider our simple dynamic version of the classical Tobin q investment model (16.13). For prediction comparison, Baltagi, Bresson and Pirotte (2004) have estimated the model using the observations from 1982 to 1993 and have reserved the last 5 years for obtaining forecasts (1994­1998). Table 16.4 gives a comparison of various predictors using the RMSE criterion for the q investment model. Because of the ability of an estimator to characterize long-run as well as short-run responses is at issue, the average RMSE is calculated across the 337 firms at different forecast horizons. Specifically, each model was applied to each firm, and out-of-sample forecasts for 5 years were calculated. The relative forecast rankings are reported in Table 16.4 after 1 and 5 years. The overall average ranking for the full 5 year period is also reported. A comparison of heterogeneous versus homogeneous estimators reveals some interesting patterns. The average OLS, average 2SLS, and the pooled mean group estimators perform poorly, ranking always in the bottom of Table 16.4 no matter what forecast horizon we look at. The Swamy random coefficients
8 For more details, see Chib (2005).

Table 16.4 Comparison of forecast performance of the q investment model

1st year

5th year

Ranking

Estimator

RMSE1

Estimator

1.

Hierarchical Bayes

2.

Individual ML

3.

Iterative Bayes

4.

Iterative empirical Bayes

5.

FGLS

6.

Empirical Bayes

7.

OLS

8.

2SLS­KR

9.

2SLS

10.

FD2SLS

11.

FD2SLS­KR

12.

Within

13.

FDGMM

14.

Individual OLS

15.

Within-2SLS

16.

Individual 2SLS

17.

Swamy

18.

Pooled mean group

19.

Average OLS

20.

Average 2SLS

1RMSE × 10-2

6.6781 6.9151 6.9651 7.0024 7.0722 7.0805 7.1541 7.1773 7.1970 7.4861 7.5008 7.5030 7.6695 7.7484 7.8644 8.5933 11.9773 12.9823 14.9043 15.5311

OLS 2SLS­KR 2SLS Hierarchical Bayes FGLS Iterative empirical Bayes Iterative Bayes Within Within-2SLS Individual ML Empirical Bayes FD2SLS­KR FD2SLS FDGMM Individual OLS Swamy Average OLS Average 2SLS Individual 2SLS Pooled mean group

RMSE1
10.0769 10.0825 10.0915 10.1428 10.1968 10.4385 10.6349 10.9203 10.9614 10.9756 11.4226 11.9677 12.0473 12.5747 13.6907 16.1467 19.833 21.8026 21.8941 22.0320

Five-year average
Estimator
Hierarchical Bayes FGLS Iterative empirical Bayes Iterative Bayes OLS 2SLS­KR 2SLS Individual ML Empirical Bayes Within Within-2SLS FD2SLS­KR FD2SLS FDGMM Individual OLS Swamy Individual 2SLS Average OLS Pooled mean group Average 2SLS

RMSE1
8.5307 8.8064 8.8069 8.8464 8.8957 8.9089 8.9239 8.9909 9.2750 9.2786 9.4586 9.9345 9.9486 10.2930 10.6765 14.0715 14.1792 17.2825 17.4408 18.6442

543

16 To Pool or Not to Pool?

544

B.H. Baltagi et al.

estimator did not perform well either, having a rank of 17 or 16 depending on the forecast horizon. The weak forecast performance of the average and the Swamy estimators relative to the homogeneous estimators arises because of the parameterinstability problem of the individual firm regressions. The shrinkage iterative Bayes and iterative empirical Bayes estimators perform well, ranking 3, 4 in the first year, 7, 6 in the 5th year, and 4, 3 for the 5 year average. The overall RMSE forecast rankings offer a strong endorsement for the iterative shrinkage estimators. However, this good performance is closely matched by some of the homogeneous estimators: FGLS, OLS, 2SLS­KR and 2SLS. These rank 5, 7, 8, 9 in the first year, 5, 1, 2, 3 in the 5th year, and 2, 5, 6, 7 for the 5 year average. Hsiao, Pesaran and Tahmiscioglu (1999) also compared the out-of-sample forecasts for their sample using the fixed effects, the mean group, the corrected mean group, average empirical Bayes, and hierarchical Bayes for a 5 year horizon. The hierarchical Bayes estimator was found to perform the best for 1-to-5 year forecasts using the RMSE criterion. For our sample, our results confirm Hsiao, Pesaran and Tahmiscioglu (1999) conclusions. The hierarchical Bayes estimators ranks 1 for the first year, 4 for the 5th year, and 1 for the 5 year average. Its forecast performance is better than all other heterogeneous estimators and is better than usual homogeneous estimators (OLS, FGLS, 2SLS, . . . ) for the first year and for the 5 year average.
Baltagi, Bresson and Pirotte, (2004) reconsider the Tobin q investment model studied by Hsiao, Pesaran and Tahmiscioglu, (1999) using a panel of 337 U.S. firms over the period 1982­1998. It contrasts the out-of-sample forecast performance of 9 homogeneous panel data estimators and 11 heterogeneous and shrinkage Bayes estimators over a 5 year horizon. Results show that the average heterogeneous estimators perform the worst in terms of mean squared error, while the hierarchical Bayes estimator suggested by Hsiao, Pesaran and Tahmiscioglu (1999) performs the best. Homogeneous panel estimators and iterative Bayes estimators are a close second.

16.5 Conclusion
Although the performance of various estimators and their corresponding forecasts may vary in ranking from one empirical example to another (see Baltagi (1997)), Baltagi, Griffin and Xiong (2000), Baltagi, Bresson, Griffin and Pirotte (2003) and Baltagi, Bresson and Pirotte (2002, 2004), the consistent finding in all these studies is that homogeneous panel data estimators perform well in forecast performance mostly due to their simplicity, their parsimonious representation, and the stability of the parameter estimates. Average heterogeneous estimators perform badly due to parameter estimate instability caused by the estimation of several parameters with short time series. Shrinkage estimators did well for some applications, especially iterative Bayes and iterative empirical Bayes. For the Tobin q example, the hierarchical Bayes estimator performs very well and gives in mean the best forecasts.

16 To Pool or Not to Pool?

545

References

Anderson, T.W. and C. Hsiao, 1982, Formulation and estimation of dynamic models using panel data, Journal of Econometrics 18, 47­82.
Arellano, M. and S. Bond, 1991, Some tests of specification for panel data: Monte Carlo evidence and an application to employment equations, Review of Economic Studies 58, 277­297.
Baltagi, B., 1981, Pooling: An experimental study of alternative testing and estimation procedures in a two-way error components model, Journal of Econometrics 17, 21­49.
Baltagi, B., 2005, Econometric Analysis of Panel Data, Wiley, Chichester. Baltagi, B.H. and J.M. Griffin, 1997, Pooled estimators vs. their heterogeneous counterparts in the
context of dynamic demand for gasoline, Journal of Econometrics 77, 303­327. Baltagi, B.H., J. Hidalgo, and Q. Li, 1996, A non-parametric test for poolability using panel data,
Journal of Econometrics 75, 345­367. Baltagi, B.H., J.M. Griffin, and W. Xiong, 2000, To pool or not to pool: Homogeneous versus
heterogeneous estimators applied to cigarette demand, Review of Economics and Statistics 82, 117­126. Baltagi, B.H., G. Bresson, and A. Pirotte, 2002, Comparison of forecast performance for homogeneous, heterogeneous and shrinkage estimators: Some empirical evidence from US electricity and natural-gas consumption, Economics Letters 76, 375­382. Baltagi, B.H., G. Bresson, J.H. Griffin, and A. Pirotte, 2003, Homogeneous, heterogeneous or shrinkage estimators? Some empirical evidence from french regional gasoline consumption, Empirical Economics 28, 795­811. Baltagi, B.H., G. Bresson, and A. Pirotte, 2004, Tobin q: Forecast performance for hierarchical Bayes, heterogeneous and homogeneous panel data estimators, Empirical Economics 29, 107­113. Bun, M., 2004, Testing poolability in a system of dynamic regressions with nonspherical disturbances, Empirical Economics 29, 89­106. Chib, S., 2001, Markov chain Monte Carlo methods: Computation and inference, in Handbook of Econometrics, J. Heckman and E. Leamer, eds., vol. 5, Chap. 57, North-Holland, Amsterdam, 3570­3649. Chib, S., 2007, Panel data modeling and inference: A bayesian primer, in The Econometrics of Panel Data: Fundamentals and Recent Developments in Theory and Practice, L. Ma`tya`s and P. Sevestre, eds., Chap. 14, Kluwer Academic Publishers, Dordrecht (forthcoming). Driver, C., K. Imai, P. Temple, and A. Urga, 2004, The effect of uncertainty on UK investment authorisation: Homogeneous vs. heterogeneous estimators, Empirical Economics 29, 115­128. Gelfand, A.E. and A.F.M. Smith, 1990, Sampling-based approaches to marginal densities, Journal of the American Statistical Association 46, 84­88. Gelfand, A.E. and A.F.M. Smith, 1992, Bayesian statistics without tears: A sampling­resampling perspective, American Statistician 46, 84­88. Hsiao, C., 2003, Analysis of Panel Data, Second Edition, Cambridge University Press, Cambridge. Hsiao, C. and H.M. Pesaran, 2007, Random coefficient panel data models, in The Econometrics of Panel Data: Fundamentals and Recent Developments in Theory and Practice, L. Ma`tya`s and P. Sevestre, eds., Chap. 5, Kluwer Academic Publishers, Dordrecht (forthcoming). Hsiao, C. and A.K. Tahmiscioglu, 1997, A panel analysis of liquidity constraints and firm investment, Journal of the American Statistical Association 92, 455­465. Hsiao, C., M.H. Pesaran, and A.K. Tahmiscioglu, 1999, Bayes estimation of short-run coefficients in dynamic panel data models, in Analysis of Panels and Limited Dependent Variable Models, C. Hsiao, K. Lahiri, L.-F. Lee, and M.H. Pesaran, eds., Cambridge University Press, Cambridge, 268­296. Judge, G.G. and M.E. Bock, 1978, The Statistical Implications of Pre-Test and Stein-Rule Estimators in Econometrics, North-Holland, Amsterdam.

546

B.H. Baltagi et al.

Keane, M.P. and D.E. Runkle, 1992, On the estimation of panel-data models with serial correlation when instruments are not strictly exogenous, Journal of Business and Economic Statistics 10, 1­9.
Koop, G., 2003, Bayesian Econometrics, Wiley, Chichester. Lindley, D.V. and A.F.M. Smith, 1972, Bayes estimates for the linear model, Journal of the Royal
Statistical Society B, 34, 1­41. Maddala, G.S., R.P. Trost, H. Li, and F. Joutz, 1997, Estimation of short-run and long-run elas-
ticities of energy demand from panel data using shrinkage estimators, Journal of Business and Economic Statistics 15, 90­100. Maddala, G.S. and W. Hu, 1996, The pooling problem, in The Econometrics of Panel Data: A Handbook of Theory with Applications, L. Ma`tya`s and P. Sevestre, eds., Kluwer Academic Publishers, Dordrecht, 307­322. McElroy, M.B., 1977, Weaker MSE criteria and tests for linear restrictions in regression models with non-spherical disturbances, Journal of Econometrics 6, 389­394. Pesaran, M.H. and R. Smith, 1995, Estimating long-run relationships from dynamic heterogenous panels, Journal of Econometrics 68, 79­113. Pesaran, M.H. and Z. Zhao, 1999, Bias reduction in estimating long-run relationships from dynamic heterogeneous panels, in Analysis of Panels and Limited Dependent Variable Models, C. Hsiao, K. Lahiri, L.-F. Lee and M.H. Pesaran, eds., Cambridge University Press, Cambridge, 297­322. Pesaran, M.H., R. Smith, and K.S. Im, 1996, Dynamic linear models for heterogenous panels, in The Econometrics of Panel Data: A Handbook of Theory with Applications, L. Ma`tya`s and P. Sevestre, eds., Kluwer Academic Publishers, Dordrecht, 145­195. Pesaran, M.H., Y. Shin, and R. Smith, 1999, Pooled mean group estimation of dynamic heterogeneous panels, Journal of the American Statistical Association 94, 621­634. Rao, C.R., 1975, Simultaneous estimation of parameters in different linear models and applications to biometric problems, Biometrics 31, 545­554. Robertson, D. and J. Symons, 1992, Some strange properties of panel data estimators, Journal of Applied Econometrics 7, 175­189. Roy, S.N., 1957, Some Aspects of Multivariate Analysis, Wiley, New York. Smith, A.F.M., 1973, A general bayesian linear model, Journal of the Royal Statistical Society B, 35, 67­75. Swamy, P.A.V.B., 1970, Efficient inference in a random coefficient regression model, Econometrica 38, 311­323. Wallace, T.D., 1972, Weaker criteria and tests for linear restrictions in regression, Econometrica 40, 689­698. Zellner, A., 1962, An efficient method of estimating seemingly unrelated regressions and tests for aggregation bias, Journal of the American Statistical Association 57, 348­368. Zellner, A., 1971, An Introduction to Bayesian Inference in Econometrics, Wiley, New York. Ziemer, R.F. and M.E. Wetzstein, 1983, A Stein-rule method for pooling data, Economics Letters 11, 137­143.

Chapter 17
Duration Models and Point Processes
Jean-Pierre Florens, Denis Fouge`re and Michel Mouchart

Many economic phenomena are characterized by the observation of a sequence of events on a continuous interval of time. Think, for instance, to observing the dates of a specific type of financial transactions, or to observing the dates of changes of the individual labor market situation (full-time employed, part-time employed, unemployed, etc.). The length of the interval between two successive events is called a duration. A duration is a positive random variable, denoted T , representing the length of a time period spent by an individual or a firm in a given state. For simplicity, we assume that the distribution of T is not defective, i.e. Pr(T = ) = 0. This variable is also called a failure time when the date of change is interpreted as a breakdown or a failure.
The most elementary duration model is based on a "death process" {Xt , t  R+}, for which Xt takes its values in the discrete state space {E0, E1}. At the time origin, called the birth date, the process is in state E0, i.e. X0 = E0. Trajectories of the process Xt have at most a unique transition from state E0 to state E1, which occurs at time T , called the death date. Consequently, the duration T generated by a trajectory of the death process Xt is defined as follows:
T = inf{t | Xt = E1} .

In most structural models, T is a continuous random variable, but the empirical distribution function is a discrete time process and nonparametric methods are of-

Jean-Pierre Florens Toulouse School of Economics, Institut Universitaire de France, Toulouse, France, e-mail: florens@cict.fr
Denis Fouge`re CNRS, CREST-INSEE (Paris), CEPR (London) and IZA (Bonn), e-mail: fougere@ensae.fr
Michel Mouchart Emeritus Professor of Statistics and Econometrics, Institut de statistique, 20 Voie du Roman Pays, B-1348 Louvain-La-Neuve (Belgium), e-mail: Michel.Mouchart@uclouvain.be

This chapter summarizes and updates Chap. 9 (on Duration Models) and Chap. 20 (on Point Processes) in Ma´tya´s and Sevestre (1996). Being a survey, this chapter by-passes many details, explanations and justifications, given in Mouchart (2004) in a textbook form, with a similar approach and notation.

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

547

c Springer-Verlag Berlin Heidelberg 2008

548

J.-P. Florens et al.

ten based on (functional) transformations of the empirical distribution function, considered as the best estimator of the "true" distribution function. Therefore, in this chapter, we explicitly consider both continuous and discrete durations.
The first section of this survey concentrates on marginal models of durations, i.e. models without explanatory variables. It presents the main functions characterizing the distribution of a duration variable, the survivor and hazard functions among others. Section 17.2 is devoted to the presentation of conditional duration models, and more particularly, proportional hazards and accelerated life models, which incorporate the effects of explanatory variables in two different ways. In this section, a special emphasis is put on the problem of unobserved individual heterogeneity. The basic duration model treats a single spell (of unemployment, for example) ending with a given kind of transition (from unemployment to employment, for example).1 But, in general, as a death could be due to various causes, an individual could exit from unemployment to enter one among different states: full-time employment, part-time employment, or training, for example. When a single-spell duration has many (at least two) outcomes, the duration model may be modelled by means of a so-called competing risks model. Competing risks models are presented in the third section, which also contains a discussion on their identifiability. The right-censoring issue is presented here as a particular case of a competing risks duration model. The fourth section is concerned with statistical inference, with a special emphasis on non- and semi- parametric estimation of single-spell duration models.
The remaining part of this chapter is devoted to point processes, which can be viewed as a generalization of duration models. Such processes are a mathematical formalization which allows to examine individual mobilities or transitions between a finite number of discrete states through (continuous) time. They are particularly useful for the microeconometric analysis of labor market dynamics. Section 17.5 sets forth the main definitions for point and counting processes. Distribution, intensity and likelihood functions of such processes are also examined. Section 17.6 presents important elementary examples of point processes, namely Poisson, Markov and semi-Markov processes. Such processes are of great interest because they are well adapted to the case of observed censored or truncated realizations. The last section presents a general semiparametric framework for studying point processes with explanatory variables. It also focuses on the definition of martingale estimators, which are particularly useful in this framework.

17.1 Marginal Duration Models
17.1.1 Distribution, Survivor and Density Functions
We first recall the general definition of the distribution function and of its complement, the survivor function. Next, we give more details for the continuous and
1 Recently, duration models have been used to analyze the determinants of time intervals between two successive changes in the price of a product sold in a given outlet (see, for instance, Fouge`re, Le Bihan and Sevestre (2007)).

17 Duration Models and Point Processes

549

the discrete cases, particularly from the point of view of the continuity of these functions.
Definition 17.1. (Distribution function) The distribution function of the duration variable T is denoted F and is defined as

F(t) = Pr(T  t), t  0 .

The main properties of the distribution function F are: F(t)  [0, 1] , F is monotone non-decreasing, right continuous and limt F(t) = 1. Definition 17.2. (Survivor function) The survivor function of the duration variable T , denoted S, is defined as
S(t) = Pr(T  t) = 1 - F(t) + Pr(T = t) .

Its main properties are: S(t)  [0, 1] , S is monotone non-increasing, left-continuous and limt S(t) = 0.
Definition 17.3. (Density function) If there exists a function f : R+  R+ such that

F(t) =

t
f (u)du

or

f (t) = dF = - dS ,

0

dt

dt

f is called the density of T .

Thus, the density function may be interpreted as the "instantaneous probability" of a failure, a death or an exit (from unemployment, for instance). Remember that in the continuous case, there exists a value of t such that F(t) = S(t) = 0.5; that value is the median of the distribution.

Definition 17.4. (Discrete duration)

 ( f j, a j), j  J  N, f j > 0,

jJ f j = 1, 0  a j < a j+1

such that

F(t) =  jJ f j 1 t  a j = { j|a jt} f j

or equivalently

S(t) =  jJ f j 1 t  a j = { j|a jt} f j

f j = F (a j) - F a j- = F (a j) - F a j-1

= S (a j) - S a j+ = S (a j) - S a j+1

550

J.-P. Florens et al.

In the framework of a death process, the event {T = a j} means "alive up to age a j and dead at age a j" and that event has probability f j.

17.1.2 Truncated Distributions and Hazard Functions

The use of statistical duration models may be justified by several arguments:
(i) Problem of time dependence. Consider the following question. What is the "instantaneous" probability of dying at time t given you are still living at time t? More generally, this is the problem of the probability law of duration T , conditional on T  t (remember that the event {T  t} means "still alive at time t"). This problem is exactly that of analyzing the dynamic behavior of the process. Such conditional distributions are "truncated" distributions.
(ii) The preceding question is often so natural that modelling those truncated distributions may be economically more meaningful than modelling the untruncated distributions. For instance, in job search models, the reservation wage, at a given instant, is a function of the duration of unemployment up to that instant.
(iii) Right-censoring (see Sect. 17.3.4) makes truncated distributions particularly useful.

Definition 17.5. (Integrated hazard function) The integrated hazard function of the duration variable T is denoted  and is defined as

 : R+  R+

t

(t) =

1 [0,t[ S (u) dF (u)

The function  is monotone non-decreasing, left-continuous and verifies  (0) = 0 and  () = . As we will see later, the integrated hazard function is a useful tool for characterizing some duration distributions. Let us consider now the hazard function (or age-specific failure rate).
(i) Continuous case
In the continuous case, there is a density function f (t) and

(t) = t f (u) du = - t 1 dS(u) = -ln S(t) .

0 S(u)

0 S(u)

Definition 17.6. (Hazard function) The hazard function of the duration variable T is denoted  and is defined as

 (t) = d(t)/dt = f (t)/S(t) = -d ln S(t)/dt .

17 Duration Models and Point Processes

551

The function  (t) may be viewed as the "instantaneous probability" of leaving the current state, indeed

 (t)

=

lim
0

Pr[t



T

<

t+ 

|

T



t]

.

Thus,  (t) is also called the "age-specific failure rate" or the "age-specific death

rate". The function 

is non negative and

t 0



(u) du

<

,

t



R+,

but

 0



(u)

du =  for non-defective distributions. Note that  is not necessarily monotone.

Straightforward relationships between the distribution, survivor and hazard func-

tions should be noticed:

(t) =

t 0



(u)

du,

f (t) =  (t) exp

-

t 0



(u)

du

S(t) = exp

-

t 0



(u)

du

, F(t) = 1 - exp

-

t 0



(u)

du

which shows that each of these functions completely characterizes the distribution of a duration.

Definition 17.7. (Temporal independence) The hazard function of the duration T has the property of temporal independence if and only if it is constant over time, i.e.  (t) =  , t  R ( > 0)

(ii) Discrete case

Remember that, in the discrete case, for any (integrable) function g(u) we have

g(u)dF(u) =  g(a j) f j =  g(a j) f j1{a j < t} .

[0,t[

{ j|a j<t}

j

Therefore

  (t) =

fj =

fj

.

{ j|a j<t} S(a j) { j|a j<t} f j + f j+1 + . . .

So, we obtain the discrete version of the (instantaneous) hazard function as

j

= (a j+ ) - (a j) =

fj +

fj f j+1 + f j+2 + . . .

=

fj S(a j)

.

In particular, 1 = f1. The last formula may also be interpreted as

 j = Pr(T = a j | T  a j) .

To deduce relationships between survivor and hazard functions in the discrete case, let us write the survivor function as:

 S(t) = { j|a j<t}(1 -  j)

552

J.-P. Florens et al.

based on the familiar identity

a0 + a1 = a0

1 + a1 a0

a0 + a1 + a2 = a0

1 + a1 a0

···

1 + a2 a0 + a1

0 j<k a j = a0 1 j<k

1+

aj

0m< j-1 am

applied to:

 S(t) = 1 - { j|a j<t}  j .

Thus we obtain the relationship

ln S(t) = { j|a j<t} ln(1 -  j)  - { j|a j<t}  j = - (t)
if  j is "small", i.e. -ln (1 -  j)   j. Thus, in the discrete case, (t) is approximately equal to -ln S(t) if all  j are small, while in the continuous case, (t) is exactly equal to -ln S(t). Moreover, in the discrete case:

 f j =  j 1i j-1(1 - i) .
Figure 17.1 presents the main distributions used for the statistical analysis of duration data.

17.2 Conditional Models
17.2.1 General Considerations
17.2.1.1 The Two Levels of Analysis to be Considered
(i) For a descriptive (or exploratory) data analysis, covariates may be used to control for observable factors of heterogeneity by performing separate analyses.
(ii) When the objective is to estimate a structural model, the parameter of interest may be such that the (marginal) process generating some covariates may be uninformative about the parameter of interest which, at the same time, is a function of a parameter sufficient to parametrize the process conditional on those covariates. Those covariates are then called "exogenous variables" and are generally denoted by Z whereas the other variables, denoted by Y (or T , in case of a duration variable), are called "endogenous", because the model describes the way they are generated conditionally on the exogenous variables.

Distribution Exponential Weibull

Parameters
 >0  > 0,  > 0

Survivor function S(t) exp (-t) exp (-t )

Density function f(t)  exp( - t)
t -1 exp(-t )

Gamma
Generalized Gamma Lognormal

>0,  >0

1  (s) -1 exp(-s)ds ( ) t

(t) -1 exp(-t) ( )

 >0,  >0,  >0

1 ( )

 t

 (s)

-1 exp(-(s)

)ds

 (t) -1 exp(-(t) ) ( )

, > 0

1-  lnt -  

1  lnt -  t 

Log-logistic

 > 0, > 0

Singh-Maddala  > 0, > 0, > 0

[ ] 1+ (t) -1

[ ] 1+ (t)

-

 



 t -1[1+ (t) ]-2
[ ]  t -1 1+ (t) -2

Fig. 17.1 Examples of distributions for durations

Hazard function h(t) 
t -1

(t) -1 exp(-t)  (s) -1 exp(-s)ds
t

 (t) -1 exp(-(t) )   (s) -1 exp(-(s) )ds
t

1  lnt -  t 

1- 

lnt -  

 t -1[1+ (t) ]-1

[ ]  t -1 1+ (t) -1

Time variation of the hazard function dh(t)/dt constant
(dh(t)/dt=0) increasing if  >1 decreasing if  <1 constant if  =1
(h(t)=) increasing if  >1, decreasing if  <1, constant if  =1
(h(t)=) Gamma distribution with parameter
 >0 et  >0 if  =1, exponential distribution with parameter  if  =1 and  =1
increasing, then
decreasing
increasing then decreasing if  >1,
decreasing if   1
log-logistic (,) if =, Weibull (,  ) if =0,
exponential() if =0 and  =1, increasing then decreasing if  >1,
decreasing if  <1

553

17 Duration Models and Point Processes

554

J.-P. Florens et al.

In such a case, it is admissible to specify only the process conditional on those exogenous variables, leaving the marginal process generating those exogenous variables virtually unspecified. In other words, for the parameter of interest, p(t | z,  ) is as informative as p(t, z |  ). According to a general principle of parsimony, the conditional model is therefore preferred.

17.2.1.2 How to Specify conditional Models
(i) In general, a natural way of specifying conditional models is to make the parameters of a distribution dependent on the conditioning variable. Thus, in FT (t |  ), one would transform  into g(z,  ) where g would be a known function. For example, Y  N(,  2) could be transformed into (Y | Z)  N( +  Z,  2). Similarly, T  exp( ) could be transformed into (T | Z)  exp[g(Z,  )] where, e.g. g(Z,  ) = exp(-Z  ).
(ii) When modelling individual data (and, in particular, duration data), a frequently used strategy consists of starting with a so-called "baseline" distribution for a reference individual, i.e. either an individual not belonging to the treatment group (e.g. an individual for which Z = 0) or a "representative" individual (e.g. an individual for which Z = E(Z)) and thereafter modelling, what makes the other individuals different from that individual of reference. Typical examples are the following:
· in the proportional hazard model, the global effect of all regressors Z is to multiply the baseline hazard function by a scale factor,
· in the accelerated life model, the global effect of all regressors Z is to rescale the duration variable. From now on, we shall only use the notation  for the complete parameter characterizing the conditional distribution generating (T | Z). This vector is decomposed into  = (,  ) where  parametrizes the baseline distribution and  represents the effect of the exogenous variables.

17.2.1.3 Time-Varying and Time-Constant Covariates Must be Distinguished
The covariates may represent:
· individual characteristics, such as gender, level of education, and so on, which are fixed over time,
· other individual characteristics, such as marital status, number of children, eligibility to social benefits or programs, which are typically varying through time,
· but also characteristics of the macroeconomic environment, such as the unemployment rate, the job vacancy rate, the employment structure, and so on, which are also time-varying but possibly common to several individuals.
Some variables may also represent interactions between several covariates. The dynamic properties of the model and the estimation procedures crucially depends on whether the covariates are time-dependent or not.

17 Duration Models and Point Processes

555

17.2.1.4 Interpretation of the Parameters

Most models are typically nonlinear in the sense that partial derivatives (of interest) are not constant, but are functions of the values of the covariates and/or of the duration. This feature clearly makes the interpretation of the coefficients more difficult. Furthermore, those partial derivatives are often not those of conditional expectations (as in regression analysis) but those of hazard functions (i.e. of "instantaneous probabilities").

17.2.2 The Proportional Hazard or Cox Model

17.2.2.1 Definition

In the proportional hazard model, the effect of the exogenous variable is specified as multiplying a baseline hazard function by a function that depends on the exogenous variable. When Z is not time-dependent, this model is defined as

T (t | z,  ) = 0(t | )g(z,  ),  = (,  ) ,
where 0(t | ) is the so-called baseline hazard function and g is a known function. The proportional hazard model is equivalently characterized as

t
T (t | z,  ) = g(z,  ) 0(u | )du = g(z,  ) 0(t | ) ,
0

ST (t | z,  ) = exp

-g(z,  )

t 0

0(u

|



)du

= exp {-g(z,  ) 0(t | )}

= [S0(t | )]g(z, ) where 0 and S0 are implicitly defined. Thus

fT (t | z,  ) = T (t | z,  ) ST (t | z,  )

= g(z,  ) 0(t | ) [S0(t | )]g(z,)

17.2.2.2 Identification
The problem of identifying separately the functions g and 0 comes from the fact that for any k > 0 : g · 0 = gk · k-10. A rather natural solution consists of defining a reference individual, i.e. a particular value z0 of Z for which g(z0,  ) = 1,  . Consequently, T (t | z0,  ) = 0(t | ). When Z = 0 is meaningful, a typical normalization is g(0,  ) = 1.

556

J.-P. Florens et al.

In the proportional hazard model with time-constant covariates, the first-order

derivative

 z

ln

T (t

|

z,  )

=

 z

ln

g(z,  )

,

depends on z and  only and is therefore independent of t.

17.2.2.3 Semi-parametric Modelling
When interest is focused on the role of the exogenous variables,  is treated as a nuisance parameter and  is the sole parameter of interest. In such a case, modelling often relies on one of the following two extreme possibilities:
(i) 0(t | ) is specified in the most simplest way such as 0(t | ) = 0(t), i.e. is completely known, or 0(t | ) = , i.e. the baseline distribution is exponential and therefore depends on only one unknown parameter;
(ii) 0(t | ) is specified in the most general way: 0(t | ) = (t), i.e. a functional parameter ( is a non-negative function such that its integral on the positive real line diverges). This is a semiparametric model with parameter  = (,  ), where  takes its value in a functional space, whereas  takes its value in a (finite dimensional) Euclidean space. This approach is particularly attractive in situations where economic theory would not give much information on the structure of 0(t | ).

17.2.2.4 A Particular Case

The function g(z,  ) should clearly be non-negative. An easy way to obtain that property without restriction on  is the log-linear specification, viz.:

g(z,  ) = exp(z  ),   Rk .

In such a case 0(t|) = T (t|0,  ). That specification has a number of interesting properties. First, let us remark that:

 z

ln

T (t

|

z,  )

=

 z

ln

g(z,  )

=



,

i.e. z has a constant proportional effect on the instantaneous conditional probability of leaving state E0. If z is not time-dependent, one may also write

ST (t | z,  ) = exp {-0(t | ) exp(z  )} = [S0(t | )]exp(z )

fT (t | z,  ) = 0(t | ) exp(z  ) [S0(t | )]exp(z )

Let us define

t = -ln 0(t | ) - z  .

17 Duration Models and Point Processes

557

where t has a completely specified distribution, independent of , z or  , namely a unit double-exponential distribution. Then we may write

-ln 0(t | ) = z  + t .

This is a (non-normal) nonlinear regression but linear if  is known. This feature of the proportional hazard model was used by Han and Hausman (1990) for conducting a semiparametric estimation on grouped duration data.

17.2.3 The Accelerated Time Model

17.2.3.1 The Basic Idea

In the accelerated time model, the effect of the exogenous variable is specified as

modifying the time scale. For the ease of exposition, we assume that the exoge-

nous variables are not time-dependent. The accelerated time model is accordingly

defined as

T = [g(z,  )]-1T0 or T0 = g(z,  )T

or, equivalently,

T (t | z,  ) = g(z,  ) × 0 [t g(z,  ) | ] T (t | z,  ) = 0 [t g(z,  ) | ] ST (t | z,  ) = S0 [t g(z,  ) | ] fT (t | z,  ) = g(z,  ) f0 [t g(z,  ) | ]

with, as usual,  = (,  ). This specification may be particularly attractive when the baseline distribution admits a scale parameter.

17.2.3.2 Empirical Test for the Accelerated Time Model
Let us consider the quantile functions, i.e. the inverse of the survivor (rather than, as more usually, the distribution) functions:
qT (p | z,  ) = ST-1(p | z,  ) , 0  p  1 ,
q0(p | ) = S0-1(p | ), 0  p  1 .
Because of the strict monotonicity (in the continuous case) of the survivor function, we have
q0(p | ) = g(z,  ) · qT (p | z,  ) . In the {q0(p | ), qT (p | z,  )}­space, this gives, for a fixed value of z, an homogenous straight line, the gradient of which is given by g(z,  ). This feature suggests that an easy empirical test for the accelerated time model may be obtained through

558

J.-P. Florens et al.

an examination of the so-called "Q­Q-plot" (i.e. plot of the two quantiles) for a fixed value of Z and a fixed (typically, estimated) value of  = (,  ).

17.2.3.3 Regression Representation of the Accelerated Time Model
The accelerated time model may also be written, in logarithmic terms, as
ln T = ln T0 - ln g(z,  ) . If we define 0 = E [ln T0] and  = ln T0 - E [ln T0] , we may also write
ln T = 0 - ln g(z,  ) +  .
In particular, (i) if ln T0  N(,  2), i.e. T0  LN(,  2), then   N(0,  2). Thus we obtain a
normal regression model (if there is no censoring); (ii) if g(z,  ) = exp (z  ), we obtain a linear regression model: ln T = 0 - z  + .

17.2.3.4 Particular Case: Weibull Baseline
In the particular case of a Weibull baseline distribution, namely 0 (t|x) = t , where  = (, ), along with a log-linear effect of the exogenous variable, namely g (z,  ) = exp ( z), we obtain:
PH (t|z,  ) = exp PH z  t
AT (t|z,  ) =  [t exp AT z ]
The two models, proportional hazards and accelerated time, become therefore identical under the reparametrization PH = AT .

17.2.4 Aggregation and Heterogeneity
Heterogeneity is the problem created by the non-observability or the omission of relevant exogenous variables. Aggregating over heterogenous individuals may create complicated structures of the hazard function. The analytical aspect is shown, for the general case, in the next lemma. An example illustrates a simple application of this lemma. Then it is shown that aggregation destroys the exponentiality of a duration.

17 Duration Models and Point Processes

559

17.2.4.1 A Basic Lemma

Let T | Z  FTZ and Z  FZ , i.e. Pr (T  t | Z = z) = FT (t | z) and Pr (Z  z) = FZ (z)
Then

fT (t) = fT (t | z) dFZ (z)

ST (t) = ST (t | z) dFZ (z)

T (t)

=

fT (t) ST (t)

=

fT (t | z) dFZ (z) ST (t | z) dFZ (z)

=

T (t | z)

ST

ST (t | (t | z)

z) dFZ

(z)

dFZ

(z)

= T (t | z) dFZ (z | T  t)

This lemma may be interpreted as follows: aggregating over heterogenous individuals, characterized by z, produces a duration distribution for which the hazard function T (t) is a weighted average of the individual hazard functions T (t | z). This possibly complicated weighting scheme may eventually account for complex hazard functions when analyzing aggregate data. A simple example illustrates this point.

17.2.4.2 An Example

Let Z = 0 for individuals with a low educational level, and Z = 1 for individuals with
a high educational level. The distribution of this variable over the whole population is defined by Pr(Z = z) =  z(1 -  )1-z. Moreover, we suppose that:

Then we can deduce

(T | Z = j)  FTj , j = 0, 1

fT (t) =  fT (t | z = 1) + (1 -  ) fT (t | z = 0)

ST (t) =  ST (t | z = 1) + (1 -  ) ST (t | z = 0)

T (t)

=

fT (t) ST (t)

=



 ST1

fT1 (t ) (t) + (1 -  ) ST0 (t)

+

(1

-



)



ST1

(t

)

fT0 (t ) + (1 -



)

ST0

(t)

=

T1 (t )



ST1

(t)

 +

ST1 (1

(t) -

)

ST0

(t)

+

T0 (t )



ST1

(1 (t)

- +

 ) ST0 (t) (1 -  ) ST0

(t)

560

J.-P. Florens et al.

17.2.4.3 The "Mover­Stayer" Lemma

Lemma 17.1. If (T | Z)  exp {0(Z)} and Z  FZ arbitrary, then T (t) is monotone decreasing.

Proof. Indeed, we successively obtain:





ST (t) = ST (t | z) dFZ (z) = exp [-t 0 (z)] dFZ (z)

0

0

fT

(t)

=

-

d dt

ST

(t)

=


0 (z) exp [-t 0 (z)] dFZ (z)
0

T (t)

=

fT (t) ST (t)

=

 0

0 (z) exp [-t

0 (z)]

dFZ

(z)

 0

exp [-t

0 (z)]

dFZ

(z)

It is then easy to check that

d dt

T (t)

<

0

t, FZ(Z),

0(Z)

(see, for example, Fourgeaud, Gourie´roux and Pradel (1990)).

This lemma may be interpreted as follows. Individuals are characterized by their value of z. Large values of 0(z) represent so-called "movers": they will leave first, while individuals represented by small value of 0(z), the so-called "stayers", will leave (in probability) later. This explains why T (t) will be decreasing because being determined at each t by the remaining individuals with smaller values of 0(z). This lemma also shows that although each individual duration has exponential du-
ration, the appropriate distribution not only is not exponential but has necessarily a
decreasing hazard rate, whatever is the distribution of Z.

17.2.5 Endogeneity
In the previous section, we have considered models where the covariates are exogenous. In many cases, this assumption is not realistic. Consider, for example, a model constructed in the following way: T is a duration generated conditionally on Z = (Z1, Z2), where Z2 is an individual characteristic and Z1 is the level of a treatment. The variable Z2 is known by persons who assign the treatment but unknown by the statistician. If the parameters of interest are the parameters of the conditional distribution of T given (Z1, Z2) these parameters are in general not identified by the conditional distribution of T given Z1 (after integration of Z2). Using econometric terminology, Z1 becomes an endogenous variable. Endogeneity of treatments in duration models has been studied by Abbring and Van den Berg (2003).

17 Duration Models and Point Processes

561

17.3 Competing Risks and Multivariate Duration Models

17.3.1 Multivariate Durations

17.3.1.1 Introduction
Multivariate durations distributions are used in different situations. The first context is the analysis of multivariate elementary point processes, which occurs when we observe life lengths of several individuals belonging to the same family, or unemployment spells of couples. This is also the case when, for a given individual, we define a multivariate point process corresponding, for instance, to her labor market trajectories and to her marriage/divorce history. Another use is in point processes with more than one transition, as in the analysis of biographical data on unemployment. Yet another use is in situations where the vector of durations is latent and some sampling scheme allows one to observe only a part of this vector; this is the case in competing risks models to be presented later on.
In this section we focus our attention on general issues, namely basic definitions and properties, and methods of construction. For expository purposes we limit the presentation to bivariate distributions; extensions to more than two dimensions are fairly obvious, although notations may become cumbersome.

17.3.1.2 Basic Concepts
We start with the multivariate survivor function defined and denoted as
ST1,T2 (t1,t2) = Pr(T1  t1, T2  t2) . In what follows we assume that ST1,T2 is twice differentiable but in the last section we show how to treat a continuous but not everywhere differentiable survivor function as well. The multivariate density is defined as
2 fT1,T2 (t1,t2) =  t1 t2 ST1,T2 (t1,t2) . The marginal survivor and density functions are defined as
ST1 (t1) = ST1,T2 (t1, 0)
d fT1 (t1) = - dt1 ST1 (t1) and similarly for T2. Often we shall write, for simplicity, S1,2, f1,2 or S j( j = 1, 2) instead of ST1,T2 , etc.

562

J.-P. Florens et al.

Conditional distributions occur in different contexts and should be carefully
distinguished according to the relevant conditioning event. Thus we need both S1|2(t1|T2 = t2), f1|2(t1|T2 = t2) and S1|2(t1|T2  t2), f1|2(t1|T2  t2). They are defined and denoted as follows:

S1|2(t1|t2)

=

Pr(T1



t1

|

T2



t2)

=

S1,2 (t1 , t2 ) S2(t2)

f1|2(t1|t2)

=

-

  t1

S1|2

(t1

|

T2



t2)

=

-

  t1

S1,2(t1

,

t2)

S2(t2)

.

Furthermore, as shown more precisely in next subsection,

S1=|2(t1|t2)

=

Pr(T1



t1|T2

=

t2)

=

-

  t2

S1,2 (t1 , t2 ) f2(t2)

f1=|2(t1

|t2)

=

-

  t1

S1=|2

(t1|t2)

=

f1,2(t1,t2) . f2(t2)

To each of these univariate conditional distributions, there corresponds a unique hazard function. For instance, marginal hazard functions are defined and denoted as:

 j(t j)

=

lim
0

1 

Pr [t j



Tj

<

tj

+



|

Tj



tj]

= - d ln S j(t j) = f j(t j)

dt j

S j(t j)

Conditional hazard functions are respectively defined as

1|2(t1|t2)

=

lim
0

1 

Pr [t1



T1

<

t1

+



|

T1



t1, T2



t2]

=

f1|2(t1|t2) S1|2(t1|t2)

=

-

  t1

ln

S1,2 (t1 , t2 )

1=|2(t1|t2)

=

lim
0

1 

Pr [t1



T1

<

t1

+



|

T1



t1, T2

=

t2]

=

f1=|2(t1|t2) S1=|2(t1|t2)

=

-

  t1

ln

-

  t2

S1,2

(t1

,

t2)

17 Duration Models and Point Processes

563

17.3.1.3 Construction of Multivariate Distributions

Several techniques for constructing multivariate distributions are worth mentioning. The most trivial one is the case of independent components in which case the joint survivor and density functions are the products of (arbitrary) corresponding marginal functions, and in which the conditional survivor, density and hazard functions coincide with the corresponding marginal functions.
For the dependent case, two general procedures are: (i) take two univariate distributions, choose one to be marginal and take the other one to be conditional to the first by making its parameters to be a function of the conditioning variable; (ii) take a joint distribution with survivor S(t1,t2, y) where y is an auxiliary variable such that S(t1,t2 | y) is meaningful, and marginalize it into S1,2(t1,t2).

17.3.2 Competing Risks Models: Definitions
Competing risks duration models may be applied to situations where the state space E has more than two elements: E = {E0, E1, . . . , EJ} , J > 2. Such models involve specifying not only the date at which the process leaves the initial state E0, but also which state in {E1, . . . EJ} is entered.
Consider, for instance, a medical trial where a patient is submitted to a "treatment" for a supposedly known disease and where the survival time is observed. Typically, the cause of death is multiple; in particular, it may be different from the disease for which the treatment was originally designed, and the cause is possibly associated with the treatment itself. One says that several risks "compete" to cause the death of the patient. Similarly, in the labor market, when the initial state E0 is unemployment, it may be relevant to distinguish several exit states, for example fulltime employment, part-time employment or early retirement. The relevance of these distinctions is based on the fact that economic, social and institutional factors may be important to explain both durations and transitions of the individual trajectories; in other words, they are particularly important when analyzing biographical data.
Thus the data have the form (T, K) where T is the sojourn duration in the initial state and K is the destination state. Therefore the law of such a process is specified by the so-called sub-distribution
Pr (T  t, K = k) = Pr(Tj  Tk  t,  j = k)
Competing risk models provide a specification of Pr(T  t, K = k) based on the following idea. T represents the duration of sojourn in the initial state E0, whatever the destination state is. The latent random variable Tj would represent the duration of sojourn in the initial state if E j were the only possible destination. In the competing risk models, if ties have zero probability, i.e. Pr(Ti = Tj) = 0, i = j, the Tj's are connected by the relationships:

564

J.-P. Florens et al.

T = min j{Tj}, j = 1, · · · J, K = argmin j{Tj = T } .

Thus, the Tj's are latent duration variables because only their minimum is observed. This structure permits to write easily the marginal laws of T and K, which are given by:

ST (t) = Pr(T  t) = Pr  j=1,...,K (Tj  t)
Pr[K = k] = Pr  j=k (Tk < Tj)
Intuitively, k is the index of the lowest latent duration (given an ascending order on the j s). In order to evaluate the likelihood function, we start by the joint survivor function, using  as an upper index in the notation of the joint distribution of the latent durations (T1, . . . , TJ) to stress that those durations are latent:

S(t1, . . . ,tJ) = Pr(T1  t1, . . . , TJ  tJ)
for any (t1, . . .tJ)  R+J . The survivor function of the observed duration T = min j(Tj) satisfies

ST (t) = S(t, . . . ,t), t  R+ The marginal survivor function of the latent duration Tj, for j = 1, . . . , J, is denoted S j and defined as:

S j(t j) = S(0, . . . , 0,t j, 0, . . . , 0) . In the case where the Tj's are independent, we have

S(t1,

.

.

.

,

tJ

)

=

Jj=1

S

 j

(t

j

)

.

Now, let us suppose that the functions S and consequently ST and S j are continuously differentiable. The marginal and relevant conditional hazard functions of the

latent duration Tj, for j = 1, . . . , J, are denoted and defined as



j (t )

=

lim
0

1 

Pr(t



Tj

<

t

+



|

Tj



t)

=

-d

ln

S

j  (t )/dt ,



 j|T

(t

)

=

lim
0

1 

Pr(t



Tj

<

t

+

|

T



t)

=

-

 t

j

ln

S(t1, . . . ,tJ)

|t1=t2=···=tJ =t

t  R+,

where



 j|T

(t

)

is

a

short

cut

for

Tj |T

(t

|t

).

When

the

Tj

s

are

mutually

independent,

it

is obvious that:



 j|T

(t

)

=



j

(t

),

for any t  R+ .

17 Duration Models and Point Processes

565

The hazard function of the observed duration T is denoted and defined as

T (t)

=

lim
0

1 

Pr(t

T

< t + | T

 t)

= -d ln ST (t)/dt , t + .

J

 =



 j|T

(t)

j=1

because, in the definition of hT (t), the derivative of ST (t) is a directional derivative (in the direction of the main diagonal (1, 1, . . . , 1)) of S(t1, . . . ,tJ). In the continuously differentiable case, the likelihood function may be evaluated by differentiating the sub-distribution, namely:

d

lT,K(t, k)

=

- dt

Pr(T



t,K

=

k)

=

-

d dt

Pr

 j=k (Tj > Tk  t)

Remember that a basic result of differential calculus gives:

S(t1, . . . ,tJ) = -

 u=tk

  tk

S(t1,

.

.

.

,

t

j

)

du

and, similarly, a basic result of conditional probability gives:


S(t1, . . . ,tJ) = u=tk Sk=¯|k = (t1, . . . ,tk-1,tk+1, . . . ,tJ | Tk = u) fk(u) du where k¯ = {1, 2, . . . , j} \ {k} and

Sk=¯|k(t1, . . . ,tk-1,tk+1, . . . ,tJ | Tk = tk)

= Pr (T1  t1, . . . , Tk-1  tk-1, Tk+1  tk+1, . . . , TJ  tJ | Tk = tk) Thus the likelihood function may be written as:

S=j¯| j(t1, . . . ,t j-1,t j+1, . . . ,tJ

|

tj)

=

-

 tj

S(t1, . . . ,tJ) f j(t j)

.

In the sequel we use the following simplified notation

S=j¯| j(t) = S=j¯| j(t,t, . . . ,t | t)

Then, the sub-distribution may be written as:

Sk¯|k(u) fk(u)du
tk

566

J.-P. Florens et al.

Therefore,

d lT,K(t, k) = - dt


t Sk=¯|k(u) fk(u)du

= Sk=¯|k(t) × fk(t)

=

-

  tk

S(t

,

.

.

.

,

t

)

=

-S

(t

,

.

.

.

t

)

×

  tk

ln

S(t

,

.

.

.

,

t

)

Using a disjunctive coding for the exit state, namely

A = (A1, . . . , AJ) , A j = I{K = j}

we may also write

  J
lT,A(t, a) =

f j(t)S=j¯| j(t)

aj
= ST (t)

J



 j|T

(t

)

aj

.

j=1

j=1

In case of independent latent durations, we have:
 lT,K(t, k) = fk(t) S j(t) j=k

= k(t)ST (t) .

17.3.3 Identifiability of Competing Risks Models
The basic idea of competing risks models is to interpret the data (T, K), representing the sojourn duration in the initial state and the label of the exit state, as the observation of the minimum component of a random vector along with the coordinate where the minimum is obtained. Intuition suggests that these observations give no information on the question whether the coordinate of the random vector, i.e. of the latent durations, are independent or not. This intuition is confirmed by next theorem Theorem 17.1. Let us denote S = {S(t1 · · ·tJ)} the set of J-dimensional survivor functions, SI = {S  S | S(t1 · · ·tJ) =  j S j(t j)} the subset of J-dimensional survivor functions with independent components, l(t, k) the likelihood function for a model in S , and lI(t, k) the likelihood function for a model in SI. Then:
S  S , ! SI  SI such that l(t, k) = lI(t, k)

17 Duration Models and Point Processes

567

In particular,



, j|T

(t

)

=



j,I

(t

)

In the continuous case, the proof of this theorem comes from the fact that, in the general case, l(t, k) = k|,(t)ST (t) and that T (t) =  j  j|,T(t), i.e. the distribution of the observed duration depends only on the sum of the conditional hazard functions. Therefore the equality k|,T(t) = k,I(t) ensures the equality of likelihood functions. Mouchart and Rolin (2002) gives a slightly more general statement and proof of this theorem.
This theorem means that to any competing risks model with dependent latent durations, one may associate an observationally equivalent model with independent latent durations. The rule of association is simply to build the joint latent distribution with marginal hazard functions of the independent model that are equal to the conditional hazard functions of the dependent model. To illustrate this point, we can consider the following bivariate example. Suppose that the joint survivor function of the two latent durations (T1, T2) is given by:

S(t1,t2) = exp 1 - 1t1 - 2t2 - exp [12(1t1 + 2t2)]

where 1, 2 > 0 and 12 > -1. Here the parameter 12 measures the dependence between the two latent durations T1 and T2 in the sense that T1 and T2 are independent once 12 = 0. The conditional and marginal hazard functions of this model are respectively:



, j|T

(t

)

=



j

1 + 12 exp[12(1 + 2)t]

,

j = 1, 2

and  j(t) =  j[1 + 12 exp( j12t)],
Marginal survivor functions are then

j = 1, 2 .

Sj (t j) = exp 1 -  jt j - exp(12 jt j) , j = 1, 2 ,

from which it is obvious that S(t1,t2) = S1(t1)S2(t2), (t1,t2)  R2+
except if 12 = 0. The likelihood element of an observation (t, k) may be written as l(t, k) = k{1 + 12 exp[12(1 + 2)t]} × exp{1 - (1 + 2)t - exp[12(1 + 2)t]}

The observationally equivalent model (i.e. having the same likelihood function) with independent latent durations has marginal hazard functions given by  j|,T(t) above and eventually marginal and joint survivor functions of latent durations given by:

568

J.-P. Florens et al.

S j,I(t j) = exp SI(t1,t2) = exp

j 1 + 2

-

 jt j

-

j 1 + 2

exp 12(1

+

2)t j

,

1

-

1t1

-

2t2

-

1

1 +

2

1 exp 12(1 + 2)t1

j = 1, 2

+2 exp 12(1 + 2)t2

Note that the latent models are clearly different unless 12 = 0, i.e.
S(t1,t2) = SI(t1,t2)
but the statistical models are observationally equivalent, i.e. l(t, k) = lI(t, k). Note also that both latent models have been identifiably parametrized, but the parameters have very different meaning in the two latent models. In particular, 12 measures the association among the latent variables in the case of dependence whereas 12 is a common parameter of the two marginal distributions in the case of independent latent variables. The identifiability of the competing-risks duration model with unobserved heterogeneity has been studied by Heckman and Honore´ (1989). Their results have been completed by those obtained by Honore´ (1993) for duration models with multiple spells and with unobserved heterogeneity.

17.3.4 Right-Censoring
One usual feature of duration data is that the sampling scheme often produces rightcensored observations, i.e. observations which have not yet left the initial state E0 at the end of the sampling period. For example, in the case of single-spell unemployment duration data, the sampling scheme is often the following. Individual observations are sampled from the inflow of individuals entering unemployment at time t0 and followed up until date C, which is possibly determined by the researcher. Now let us assume that C is greater than t0. Some observations correspond to individuals leaving the unemployment status before C, in which case they generate complete unemployment durations. Other sampled individuals have not left the unemployment state at date C and so they generate right-censored unemployment durations. Rather than sampling from the inflow into unemployment at a given date t0, the analyst may sample from inflows considered at several staggered dates t01,t02, . . . and follow up observations once again up to a censoring time C. Right-censoring can be modelled using the framework of the competing risks models with state space {E0, E1, . . . , EJ}, J > 1, where the last state EJ denotes the right-censored situation. To illustrate this kind of formalization, let us consider a bivariate competing risks model (T1, T2) with state space {E0, E1, E2}, E0 labelling unemployment, E1 employment and E2 right-censoring. Thus T2 = C. In other words, censoring is often associated with a residual state in a model with multiple states. Suppose first that all individual observations are sampled at the same date t0. Without loss of generality,

17 Duration Models and Point Processes

569

one may write t0 = 0 (after some relevant time translation). Within the framework presented in the previous section, this model may be viewed as resulting from a latent survivor function S1,2(t1,t2 |  ) with parameter  , and a generic element of the likelihood function may be written as:
lT,D(t, d) = f1(t |  )S2=|1(t |  ) d f2(t |  )S1=|2(t |  ) 1-d

where D = I{T1T2}. In view of the identification problem, and because in many cases censoring mechanisms are independent of the unemployment process, it is often assumed that T1 and T2 are independent. Then,
lT,D(t, d) = [ f1(t |  )S2(t |  )]d [ f2(t |  )S1(t |  )]1-d
If moreover  may be factorized into  = (1, 2), such that 1 characterizes the distribution of T1 and 2 the distribution of T2, the likelihood reduces to

where

lT,D(t, a) = L1(1)L2(2)

L1(1) = f1(t | 1)dS1(t | 1)1-d .
The parameters of interest are in general those of the distribution of duration T1, and their estimation could be deduced from L1(1) only. Then the generic element of the relevant factor of the likelihood function is f1(t | 1) (resp. S1(t | 1)) for an uncensored (resp. a right-censored) observation.
Another model generating censored data may be the following one. Let T0 be the age of an individual entering unemployment. This age is randomly generated by the individual previous labor market history. The duration of the unemployment spell is T1 and the age at the end of the unemployment spell is then T0 + T1. The econometric model specifies the joint distribution of (T0, T1) and these two random variables are not, in general, assumed to be independent. A natural specification could be a sequential one: the (marginal) distribution of T0 is first specified and a conditional distribution of T1 given T0 completes the model.
Let us now assume that all the individuals are observed at a given date T. In general this date is also random but, for simplicity, we consider T as fixed (the model is conditional to T). Let us also assume that the sample is constructed in such a way that T0  T (all the individuals have entered unemployment). Then T0 is always observed but T1 is not censored if T0 +T1  T. Otherwise, the unemployment spell duration is censored.
Let us define T2 = T - T0. From the distribution of (T0, T1) we obtain the distribution of (T1, T2), and we may consider the observations as generated by a censored duration model: T1 is observed only if T1  T2. But the following specification of a likelihood based on the generic element:
lT,D(t, d) = f1(t)dS1(t)1-d

570

J.-P. Florens et al.

where T = min(T1, T2), D = I(T1  T2), f1 and S1 are the density and the survivor functions of T1, is incorrect for two reasons:
(i) First if T0 and T1 are dependent, T1 and T2 are also dependent and the likelihood function must be based on their joint distribution.
(ii) The censoring mechanism is different from the usual competing risks model because T0 or T2 is always observed and the likelihood of the actual data must be the density of (T2, T, D). The generic element of this likelihood is then
lT2,T,D(t2,t, d) = f2(t2) f1=|2(t | t2)a S1=|2(t | t2)1-d
using our previous notations. Finally, note that the identification result of Sect. 17.3.3 does not apply to this case since the censoring mechanism is different from the competing risks model.

17.4 Inference in Duration Models
17.4.1 Introduction
Models actually used in econometrics for dealing with duration data are characterized by two noteworthy features: durations are non-negative random variables and most data sets involve right-censored data. In this section, we focus our attention on the implications of censoring, both for adapting the inference procedure and for evaluating the consequences of misspecification. We first review the inference in parametric models, both in the marginal and in the conditional case, with a particular attention on a rigorous specification of the likelihood function; next we consider non- and semi-parametric models. In each case, we first specify the structure of the model and next give some illustrations with significantly relevant particular cases.

17.4.2 Parametric Models
17.4.2.1 Inference in Marginal Models The Basic Model The basic model considers a random censoring process that is independent of the duration variable. Let us introduce the following notations:

17 Duration Models and Point Processes

571

 = (1, . . . , n) denote latent durations,  = (1, . . . , n) denote latent censoring indicators, T = (T1, . . . , Tn) , with Ti = i  i, are observed durations D = (D1, . . . , Dn) , with Di = I{ii} = I{Ti=i}, X = (X1, . . . , Xn) , with Xi = (Ti, Di) , denote complete data X = (T, D) with dim (X) = (n × 2, 1)  is a sufficient parametrisation for the process generating (,  )

Assumptions
A.1 (independent sampling): i(i, i) |  A.2 (independent censoring): ii |  A.3 (definition of  as a sufficient parametrization for ): i |  A.4 (definition of  as a sufficient parametrization for  ): i |  A.5 (variation-free parameters) : ( , )   ×  A.6  is the only parameter of interest

Latent Likelihood

Under (A.1) to (A.5), the complete latent likelihood is therefore:

  L( ) = f (i |  )· f (i | ) = L1( )· L2()

i

i

Under (A.6), the relevant latent likelihood is

L1( ) =  f (i |  ) = f ( |  ) i

Actual Likelihood
Considering the actually available data, namely (T, D), the complete actual likelihood is

  L( ) = f (Ti |  )Di S (Ti |  )1-Di f (Ti | )1-Di S (Ti | )Di

i

i

= L1( )L2()

Under (A.6), the relevant actual likelihood is:

  L1( ) = f (Ti |  )Di S (Ti |  )1-Di =  (Ti |  )Di S (Ti |  )

i

i

572

J.-P. Florens et al.

Thus the logarithm of the relevant actual likelihood is:

L( ) = ln L1( ) =  Di ln f (Ti |  ) + (1 - Di) ln S (Ti |  )

i

i

= [Di ln  (Ti |  ) + ln S (Ti |  )] i

= [Di ln  (Ti |  ) -  (Ti |  )] i

The Exponential Case

The consequences of censoring are best understood by considering with some de-
tail the case where the duration of interest is exponentially distributed, which means that f (i |  ) =  e-i while f (i | ) is left unspecified. Thus, the latent process generating  is a member of the exponential family, i i = + is a minimal sufficient complete statistic of the latent process and, for a sample of size n, the Fisher information is n -2. With censoring, the relevant actual likelihood is written as:

L( ) =  Di ln  -  Ti  = (ln  )D+ -  T+

i

i

where D+ = i Di and T+ = i Ti. The score and the statistical information are accordingly:

S( )

=

d d

L( )

=

D+ 

- T+

J( )

=

-

d2 d 2

L( )

=

D+ 2

taking into account that J( ) and therefore I( ) are block diagonal. Therefore the maximum likelihood estimator of  is:

^ML

=

D+ T+

Let us recall that:

n(^ML,n -  ) -L N 0, [I( )]-1

where

I( ) = V

d d

L(

)

|



=

E[J( )| ]

=

E[D+ | 2

]

Note that:

E[Di |  ] = Pr[i  i |  ] = E[F (i |  ) |  ] = 1 - E[e-i |  ]

Therefore:

E[Di |  ] = 1 - E[e-i | ] = 1 - e-i dF (i)

17 Duration Models and Point Processes

573

In practice, I( ) is estimated as:

I(^MV,n)

=

D+ ^M2 L,n

Let us turn now to the uncensored case. In the model with censoring, there is only one parameter,   R+, and the bivariate statistic (D+, T+) is minimal sufficient but not complete. This is an example of a curved exponential family with canonical parameter ( , ln  ). Also, let us notice the differences in the maximum likelihood estimations:

D+ - n > D+

L( ) = (ln  )D+ -  T+ - n ln  -  T+

^Mc L =

D+ T+

- ^MncL =

n T+

>

D+ T+

In other words, the cost of overlooking censoring may be appreciated by considering the difference between the (true) Fisher information, and the numerical value of the maximum likelihood estimator:

^Mc L ^MncL

=

D+ n



1

and = 1  D+ = n

17.4.2.2 Inference in Conditional Models

The General Statistical Model

Let us introduce the following definitions and assumptions:
·  = (,  )   ×   Rk × Rk , k and k finite. · Data:
Yi = (Ti, Di), Y = (Y1, . . . ,Yn)

Xi = (Yi, Zi), X = (X1, . . . , Xn)
· Definition of  and  : Z |  and Y  | Z,  · Assumptions
variation-free parameters: (,  )   ×  conditional independence: iYi | Z,  and YiZ | Zi,   is the only parameter of interest

Therefore, the relevant actual loglikelihood takes the form:

L( ) =  Di ln f (Ti | zi,  ) + (1 - Di)ln S (Ti | zi,  )

i

i

=  Di ln l (Ti | zi,  ) -   (Ti | zi,  )

i

i

574

J.-P. Florens et al.

The score and the statistical information are equal to:

S( )

=

d d

 L( ) =

i

Di  (Ti | zi,  )

d d

 (Ti

| zi,  ) -  i

d d



(Ti

| zi,  )

 I( ) =

-d2 d d

L( ) =

i

Di

[

(Ti

|zi

,



)]-2

d d



(Ti

|zi

,



)

d d

 (Ti|zi,  )

  -

i

Di

(Ti

|zi

,



)-1

d2 d d

 (Ti|zi,  ) +

i

d2 d d

 (Ti|zi,  )

Notice once more that the expectation of I( ) depends both on  and K, and thus, on the parameter of the censoring variable.

The Proportional Hazard Model

When

 (t | z,  ) = g(z,  )0(t | ) ,

the log-likelihood function may be written as:

L( ) =  Di ln  (Ti | z,  ) -   (Ti | z,  )

i

i

=  Di ln g(zi,  ) +  Di ln 0(Ti|) -  g(zi,  )0(Ti|)

i

i

i

and, under the log-linear specification g(z,  ) = exp z  :

L( ) =   Dizi +  Di  ln 0(Ti|) - ezi 0(Ti|)

i

i

i

The Mixed Proportional Hazard Model and its Identifiability
The mixed proportional hazard (MPH) model is characterized by the following hazard function:
T (t | z) = 0 (t) g (z) 
where 0 (t) is a baseline hazard function, g (z) is the function measuring the proportional effect of observable covariates z on the hazard function, and  is an individual-specific random term representing unobserved individual heterogeneity. The cumulative density function of  is denoted H. This model is supposed to verify the following assumptions:
Assumption 1: The covariate vector z is a finite-dimensional vector of dimension k (1  k  ). The function g (z) is positive for every z  Z  Rk.

17 Duration Models and Point Processes

575

Assumption 2: The function 0 (t) is positive and continuous on [0, ), except that limt0 0 (t) may be infinite. For every t  0,

t

t

0(u)du < 
0

while

lim
t

0

0(u)du = 

Assumption 3: The distribution H of the random term  in the inflow (i.e. when t = 0) satisfies Pr {  ]0, )} = 1.

Assumption 4: The individual value of  is time-invariant.

Assumption 5: In the inflow (i.e. when t = 0),  is independent of z.

This model is nonparametrically identified if there is a unique set of functions 0, g and H that generates the observable distribution of the data, namely F (t | z) . Conditions for identification are the following (see Van den Berg (2001), for a clear exposition):

Assumption 6: (variation in observed covariates): The set Z of possible values of z contains at least two values, and g (z) is not constant on Z .

Assumption 7: (normalizations): For some a priori chosen t0 and z0, there holds:

t0
0 (u) du = 1 and g (z0) = 1
0

Assumption 8: (tail of the unobserved heterogeneity distribution): E () < .

Assumptions 6 and 8 can be alternatively stated:

Assumption 6b: (variation in observed covariates): The vector z includes an element za such that the set Z a of its possible values contains a non-empty open interval. For given values of the other elements of z, the value of za varies over this interval. Moreover, g (z) as a function of za is differentiable and not constant on this
interval.

Assumption 8b: (tail of the unobserved heterogeneity distribution): The random vari-

able  is continuous, and the probability density function h () of  verifies the

following property:

h ()

lim
 

 -1-V

( )

=

1

where   [0, 1] is fixed in advance, and where V () is a function such that:

V (s)

lim

=1.

s V (s)

Identification of the MPH model has been analyzed successively by Elbers and Ridder (1982), Heckman and Singer (1984b), Ridder (1990), Melino and Sueyoshi (1990), and Kortram, Lenstra, Ridder and Van Rooij (1995).

576

J.-P. Florens et al.

The Accelerated Life Model

When  (t | z,  ) = g(z,  ) 0(t g(z,  ) | ), the log-likelihood function, for an arbitrary family of baseline distributions, may be written as:

L( ) =  Di[ln g(zi,  ) + ln 0(Ti g(zi,  )|)] -  (Ti g(zi,  )|)

i

i

When the baseline distribution is exponential, namely when 0(ti|) = , we obtain:

L( ) =  Di[ln g(zi,  ) + ln ] -   Tig(zi,  )

i

i

In the particular case where g(zi,  ) = exp zi , we obtain a proportional hazard model. More generally, this is also the case for a Weibull baseline distribution:

L( ) = ln   Di +    Dizi -  Tiezi

i

i

i

17.4.3 Non-parametric and Semi-parametric Models

17.4.3.1 Marginal Models: The Kaplan­Meier Estimator of the Survivor Factor

If we want to estimate ST (t) in presence of right-censoring, a simple idea is to adjust the hazard rates of the product form of the (discrete) empirical survivor function. With the same data as for the parametric models:

we now evaluate:

Yi = (Ti, Di) Ti = min(i, i) Di = I{Ti=i}

Ti  T(1) < T(2), . . . , T(n) (order statistics) Di  D1, D2, . . . , Dn: (censoring indicators corresponding to the T(i))
 R(t) = I{T(i)t} i
 B(T(i)) = D jI{Tj=T(i)} j

17 Duration Models and Point Processes

577

Thus R(t) represents the number of individuals at risk at time t, i.e. those who are neither "dead" nor censored at time t-, and B(T(i)) represents the number of deaths (i.e. exiting without being censored) at the observed time T(i). A natural way of taking censoring into account is to consider that at the time T(i), B(T(i)) is the realization
of a binomial variable with parameter R T(i) ,  (T(i)) . Then the hazard function at (observed) time T(i) and the survivor functions are estimated as:

^ (T(i))

=

B(T(i)) R(T(i))

 S^KM(t) =

[1 - ^ (T(i))]

{T(i) <t }

Remarks

1. If at T(i) there are only censored data, we have B(T(i)) = 0 and therefore S^KM(T(i)) is continuous at T(i).
2. If the largest observation is a censored one, S^KM(t) is strictly positive and con-
tinuous, at T(n):

S^KM(t) = S^KM(T(n)) > 0, t > T(n)

If

furthermore

T(n-1)

is

not

censored,

lim
t

FK

M

(t

)

>

0,

which

means

that

F

could

be defective. A natural interpretation of this occurrence, in the case of a life

duration, is the following: if the largest observation does not correspond to an

exit (or a death), there is no empirical reason not to believe that such a life could

possibly be infinite. If one is willing to avoid defective distributions, one may

modify the Kaplan­Meier estimator as follows:

 SKmM(t) =

[1 - h^ (T(i))]I{tmax{Di Ti}} = FKM(t)I{tmax{Di Ti}}

{T(i) t }

where max{Di Ti} represents the largest uncensored duration. 3. If there are no ties at T(i), then:

B(T(i)) = Di, R(t(i)) = n - i + 1,

 SKM(t) = {T(i) t }

1 - Di n-i+1

In many data sets, ties are observed, as a matter of fact. They call for two remarks: (i) even if F and F are continuous, Pr( =  ) > 0 is possible when  is not independent of  (see, for instance, Marshall and Olkin (1967)); (ii) the rounding problem: although theoretical models assume the time is continuous, actual
measurements are discrete in nature. We have just seen that the Kaplan­Meier

578

J.-P. Florens et al.

estimator accommodates for ties. When the rounding problem is too severe because spells are actually observed through intervals, truncated survivor functions may be used for an explicit modelling. 4. If, at the largest observation, some censored and uncensored data are tied, the estimated distribution, S^KM(T(i)), is again defective and discontinuous at T(n), with:
S^KM(T(n)) > S^KM(T(-)) > 0

17.4.3.2 Conditional Models: The Semi-parametric Proportional Hazard Model (The Cox Model)

Remember that in  = (,  ),  is a sufficient parameter for the baseline distribution, whereas  is introduced for describing the action of the exogenous variables. The semiparametric version of the proportional hazard model takes the form:

T (t | z,  ) = (t) exp z 
where (t) = 0(t|z,  ), which is the baseline hazard function, is now a functional parameter. Thus the parameter space takes the following form:

 = (,  )   ×   = { : R+  R+ |  is continuous and   Rk


(t)dt = }
0

The functional parameter  is often a nuisance parameter, whereas the Euclidean parameter  is the parameter of interest. It is therefore important to try to separate inferences on  and  . A natural idea is to construct a statistic W = f (Y ) such that the likelihood function LY|Z(,  ) factorizes as follows:

LY |Z(,  ) = LW |Z( ) × LY |W,Z(,  )
In such a case, the inference on  would be made simpler by considering only the partial likelihood LW|Z( ) instead of LY|Z(,  ). A heuristic argument in favour of this simplification is that the information on  contained in LY|W,Z(,  ) is likely to be "eaten up" by the functional parameter . This simplified estimator may now be build as follows. Similarly to the Kaplan­Meier estimator, let us reorder the sample according to the observed durations:

and let us also define:

Ti - T(1) < T(2) < . . . < T(n) Di - D1, D2, . . . , Dn

17 Duration Models and Point Processes

579

 R(t) =

I{T(i) t }

1in

R(t) = {k|T (k)  t} = {i|Ti  t}

Thus R(t) represents the number of individuals at risk at time t and R(t) represents the set of such individuals. Notation will be usefully simplified as follows:

R(i) = R(T(i)),

R(i) = R(T(i))

Let us now represent the sample (T1, . . . , Tn) by its order statistics (T(1) . . . T(n)) and its rank statistics (R1, . . . , Rn) where Ri is the rank of the i-th observation in the vector of order statistics. Giving the rank statistics, which plays the role of W in the

previous expression, we may write the likelihood function of the rank statistics as

follows:



Di



Di

 L( ) =



ezi



1in kR(i) ezk

 =



ezi



1iD+ kR(i) ezk

where D+ = i Di. The (partial) likelihood estimator of  is then defined as
^ = arg max L( )

This estimator is consistent and its asymptotic properties have been studied e.g. by Tsiatis (1981) and by Andersen, Borgan, Gill and Keiding (1993).

17.5 Counting Processes and Point Processes
Point processes provide the framework for modelling trajectories with more than one transition and more than two states (such trajectories are sometimes called duration models with multiple spells and multiple states). Formally a point process is a continuous time process with a finite state space and right continuous with left limit (cadlag) trajectories. A point process is conveniently represented by means of a multivariate counting process that counts, as time increases, the number of possible transitions. Consequently, we will first present counting processes.

17.5.1 Definitions
Let us consider a (finite or infinite) sequence (Tp)p1 of increasing random durations (0 < T1 < T2 < . . .). This sequence characterizes a univariate counting process:
Nt =  I(Tp  t) p1

580

J.-P. Florens et al.

Nt

2

1

Up

t

T1

T2

Tp­1

Tp

Fig. 17.2 A realization of a univariate counting process

The trajectories of Nt are right continuous, and such that N0 = 0 and Nt only increases by jumps of size 1. A typical realization is shown in Fig. 17.2.
A duration model defines a process with a single jump (Nt = I(T  t)). From the definition of Nt , we can deduce easily the definition of the date Tp of the j-th jump of the process:
Tp = inf{t|Nt = p}, p  1
The distribution of Nt may be characterized by the distribution of the sequence (Tp)p1. Equivalently, that sequence may be replaced by the sequence of positive numbers:
Up = Tp - Tp-1 (T0 = 0)
The random variable is now the duration between the (p - 1)-th and the p-th jumps. If the random variables (Up)p1 are i.i.d., the process is called a renewal process. The information denoted by FtN and carried by Nt , observed from 0 to t (included), is equivalent to the knowledge of T1, . . . , Tp (Tp  t < Tp+1) and the event Tp+1 > t. Equivalently this information may be described by the random variables U1, . . . ,Up and by the event Up+1 > t - qp=1 Uq.
A multivariate counting process is a vector Nt = (Nt1, . . . , NtJ) of counting processes. This vectorial process is characterized by J sequences (Tpj)p1 ( j = 1, . . . , J) of increasing durations and by:
 Ntj = I(Tpj  t) p1
The information content of the observation of this multivariate counting process up to time t is described by the family of random variables Tpj such that Tpjj  t, and by the J events Tpjj+1 > t.
A multivariate counting process may also be represented by a unique sequence (Tr)r1 of the jump times of any element of the vector Nt , and by er (r  1) which is a discrete-time process valued in (1, . . . , J). In this sequence (Tr, er)r1, er ; indicates the component j that jumps at date Tr. Note that the sequence (Tr) has the property:

17 Duration Models and Point Processes

581

J
N¯t =  Ntj =  I(Tr  t)

j=1

r1

The distribution of Nt may then be described by the sequence of conditional distributions:
(Tr, er) | (Ts, es)s=1,...,r-1
Consider for example a bivariate duration (T 1, T 2), where Pr(T 1 = T 2) = 0. This pair defines two single jump counting processes:

Nt1 = I(T 1  t) and Nt2 = I(T 2  t)

Then the (Tr)r sequence becomes: T1 = min(T 1, T 2), T2 = max(T 1, T 2)

and

e1 = I(T 1 < T 2) + 2I(T 2  T 1) e2 = 3 - e1

A point process is a continuous-time process valued in a finite (or more generally discrete) state space {1, . . . , K}. Such a process Xt may represent, for example, the labor market situation of an individual at time t. In such a case, the set {1, . . . , K} describes the different possible labor market states (full-time employed, part-time
employed, unemployed, retired,. . . ) and Xt is characterized by the dates of the transitions between two different states. Indeed, a point process defines a multivariate
counting process. Consequently, we denote by j = (k, k ) the pair of states such that a transition from k to k is possible and {1, . . . , J} is the set of all these ordered pairs. Then (Tpj)p1 is the sequence of jump times from k to k if j = (k, k ) and

 Ntj = I(Tpj  t) p1

This multivariate counting process satisfies the following constraint by construction: after a jump of the component Ntj, j = (k, k ), the next process which may jump is necessarily an element of the subfamily (Nt ) where = (k , k") and k" = k .

17.5.2 Stochastic Intensity, Compensator and Likelihood of a Counting Process

The stochastic intensity of a univariate counting process is defined as follows:

hN (t)

=

lim
 t 0

1 t

Pr(Nt +  t

- Nt

=

1

|

FtN-)

582

J.-P. Florens et al.

If for instance Nt = I(T  t), this definition implies, where T is a continuous variable, that h(t) =  (t), which is the hazard function of T if T > t and h(t) = 0 after
the jump T . Equivalently:

hN(t) = T (t)(1 - Nt-),

where Nt- = I(T < t). If Nt is a general univariate counting process Nt = p1 I(Tp  t), the stochastic
intensity is obtained by the following rule:
· If t > maxp(Tp) then h(t) = 0 · If t verifies Tp-1 < t  Tp (where p = Nt + 1) then

hN (t) = p(t | T1, . . . , Tp-1)

where p is the hazard function of the duration Tp conditional on T1, . . . , Tp-1. If the model is specified in terms of Up = Tp - Tp-1, we have

hN

(t)

=



U p

(t

-

Tp-1

|

U1,

.

.

.

,Up-1)

where



U p

is

the

hazard

function

of

Up

given

U1, . . .Up-1.

This

definition

is

easily

extended to multivariate counting processes. The stochastic intensity is then multi-

variate and for each j  {1, . . . , J} :

hNj (t)

=

lim
 t 0

1 t

Pr(Ntj+ t

-

Ntj

=

1

|

FtN-)

where FtN- represents the information carried by all the coordinates of the process observed before t.
If Ntj = p1 I(Tpj  t), hNj (t) is null if t > maxp(Tpj). For each coordinate , we can choose p = Nt + 1 such that
Tp -1  t < Tp
(where Tp = + if Nt never jumps after Tp -1) and Ti (t) is equal to the hazard function of Tpjj at the point t, given all the Tq , = j and q < p , and the family of events Tp  t. Let us take as an example the bivariate counting process Nt1 = I(T 1  t), Nt2 = I(T 2  t). The stochastic intensity h1N(t) is equal to the hazard function of T 1 conditional on T 2 = t2 if T 2 < t or conditional on T 2  t if T 2  t. The compensator of univariate counting process Nt with stochastic intensity hN(t) is defined by
t
HN(t) = h(s)ds
0
For a duration model Nt = I(T  t), HN(t) is equal to the integrated hazard (t) if T > t and equal to (T ) if T  t.

17 Duration Models and Point Processes

583

For a multivariate counting process Ntj, we define a vector of compensators by:

HNj (t) =

t 0

hNj

(s)ds

.

From now on, we simplify the notation HNj (t) into Htj similarly to Nt instead of N(t). The compensators are positive and non-decreasing predictable processes satisfying
H0 = 0. The main property of the compensator is that the difference:

Mt = Nt - Ht
is a zero mean FtN- martingale (i.e. E(Mt | FsN) = Ms). The decomposition Nt = Ht + Mt is called the Doob­Meyer decomposition of the process Nt . The same decomposition may be constructed for a multivariate counting process. In that case, Mtj is a martingale with respect to the information sets generated by the whole process (Nt1, . . . , NtJ).
The stochastic intensity and the compensator both determine an elegant expression of the likelihood of a counting process. Consider first a univariate process Nt = p1 I(Tp  t). If the process is observed between 0 and t such that Tp-1 < t < Tp, the likelihood of this observation is:

p-1

 (t) =

fq(Tq | T1, . . . , Tq-1) × Sp(t | T1, . . . , Tp-1)

q=1

where fq and Sq are respectively the density and the survivor functions of Tq given T1, . . . , Tq-1. One can easily check that:
 (t) = h(Tq)e-Ht Tq t
or
t
ln (t) = ln h(s)dNs - Ht
0
In this expression, we use the stochastic integral notation:

t

g(s)dNs =  g(Tp)

0

Tp t

The stochastic intensity notation can be generalized to multivariate processes for which the likelihood corresponding to the observation of all the coordinates of the process up to time t is equal to:

J
ln l(t) = 

t ln h j(s)dNsj - Htj

j=1 0

This way of writing the likelihood function is the basis for Cox's estimation and martingale estimations, to be presented in the last section of this chapter.

584

J.-P. Florens et al.

17.6 Poisson, Markov and Semi-Markov Processes

In this section, we give first the example of a well-known single counting process, namely the Poisson process. Then we examine point processes displaying Markovian or semi-Markovian properties.

17.6.1 Poisson Processes

We consider the familiar Poisson process as an example of a univariate counting

process. Let M be a positive measure on R+ with density m with respect to the

Lebesgue measure, i.e., M([a, b]) =

b a

m(x)

d(x).

A stochastic process Nt is a Poisson process associated with the measure M if its

distribution satisfies the following requirements:

(i) N0 = 0 , (ii) Nt is a process with independent increments: t1, . . . ,tn, the random variables
(Nti - Nti-1 )i=1,...,n are independent random variables, (iii) the distribution of (Nt - Ns) is a Poisson distribution for any s < t, which means
that:

Pr(Nt

- Ns

=

k)

=

M([s,t])k e-M([s,t]) k!

These three properties imply that a Poisson process is a counting process with unit jumps. If m(x) is equal to some positive constant  , then the process is said to be homogeneous and we may verify that sojourn times Up = Tn - Tn-1 are i.i.d. random variables with an exponential distribution with parameter  > 0. The homogeneous Poisson process is then the renewal process characterized by the exponential
distribution.
The compensator and the intensity of a Poisson process, with respect to its canonical filtration, are equal to H = M([0,t]) and to m(t), respectively. This result follows
from the equalities:

h(t)

=

lim
 t 0

1 t

Pr(Nt + t

- Nt

=

1

|

FtN- )

1

=

lim
 t 0

t

Pr(Nt + t

- Nt

=

1

|

Nt )

=

lim
 t 0

1 t

[M(t , t

+

 t])e-M([t,t+t])]

= m(t)

In particular, if the process is homogeneous, h(t) is constant. The likelihood (t) relative to the observation of the process Nt between 0 and t is derived from the intensity and the compensator, i.e.

17 Duration Models and Point Processes

585

t
ln (t) = [ln m(s)] dNs - M([0,t])
0
=  ln m(n) - M([0,t]) . n t

If Nt is an homogeneous Poisson process with parameter  , its likelihood satisfies:

ln (t) = Nt ln  - t .

17.6.2 Markov Processes
17.6.2.1 Definitions
We consider a point process X = (Xt )tR+ valued in the finite state space E = {1, . . . , K}. The distribution of Xt is totally defined by a projective system:
Pr(Xt1 = j1, . . . , Xtp = jp) for any finite subset (t1, . . . ,tp) of R+ satisfying t1 < t2 < · · · < tp. From these probabilities, one can compute:
Pr(Xtp = jp | Xt1 = j1, . . . , Xtp-1 = jp-1)
and the process Xt is a Markov process if:
Pr(Xtp = jp | Xt1 = j1, . . . , Xtp-1 = jp-1) = Pr(Xtp = jp | Xtp-1 = jp-1)
It follows that a Markov process is characterized by the distribution of the initial condition, i.e. by the distribution of X0, and by the transition probabilities:
p jk(s, s + t) = Pr(Xs+t = k | Xs = j)
defined for any s and t  R+, and for any j and k  E. The Markov process is said to be time-homogeneous if:
p jk(s, s + t) = p jk(0,t), (s,t)  R+ × R+, ( j, k)  E2 ,
i.e. if the transition probability does not depend on the origin of the time set, but only on the difference between the two dates s and (s + t). For a time-homogeneous Markov process, we denote the transition probability p jk(0,t) by p jk(t) and the matrix with elements p jk(t) by P(t). So, P(t) is a K × K matrix of non-negative numbers such that the sum of each row is equal to one, i.e.
K
 p jk(t) = 1
k=1

586

J.-P. Florens et al.

Moreover, decomposing the trajectory on [0,t] into two sub-trajectories on [0, s] and [s,t], we obtain the following properties of the matrices P(t):

p jk(t) = K=1 p j (s) p k(t - s) ,  0  s  t,  ( j, k)  E × E

or equivalently:

P(t) = P(s) P(t - s) , 0  s  t .

We will now restrict our attention to processes satisfying some regularity conditions.

Definition 17.8. A time-homogeneous Markov process Xt is said to be standard if:

(i)  j  E, limt0 p j j(t) = 1 , and then, k = j, limt0 p jk(t) = 0,

(ii) q jk  R+, ( j, k)  (E × E), with k = j,

q jk

=

limt0

1 t

p jk(t)

=

d dt

p jk(t)

|t=0,

qj

j

=

- limt0

1 t

(1

-

p j j(t))

=

- k= j

q jk

As a direct consequence, quantities q jk satisfy the following properties:

(i)

Kk=1 q jk = 0,

j  E,

(ii) q jk  0, k = j, and q j j  0,

jE .

If j = k, q jk is called the intensity of transition from state j to state k. The matrix

Q is called the intensity matrix or the generator of the process Xt . Writing p j j as

p j j(t)

=

1-  k= j

pk j(t),

the

previous

definition

implies

that

Q

=

d dt

P(t )|t =0

Theorem 17.2. The transition matrix P(t) of the time-homogeneous standard Markov process Xt satisfies the forward matrix equation

d dt

P(t)

=

P(t)

·

Q

and the backward matrix equation

d dt

P(t)

=

Q

·

P(t

)

.

Proof. See Doob (1953), pp. 240­241, or Bhattacharya and Waymire (1990), pp. 263­267.
These two equations are known as the Kolmogorov forward and backward differential equations, respectively. In general, these equations do not have a unique solution; however, Xt is said to be regular if the solution, subject to the border condition P(0) = I, is unique and has the exponential form given in the following theorem (where I is the identity matrix).

17 Duration Models and Point Processes

587

Theorem 17.3. If the time-homogeneous Markov process Xt with generator Q is regular, then the matrix


P(t) = exp(Qt) =  tnQn/n! n=0

(17.1)

exists for any t, and is the unique solution to the Kolmogorov differential equations subject to the border condition P(0) = I.

Proof. See Doob (1953), pp. 240­241, or Bhattacharya and Waymire (1990), pp. 267­275.

17.6.2.2 Distributions Related to a Time-Homogeneous Standard Markov Process

Since the state space E is finite, the Markov process Xt moves by jumping from one state to another. Let 0 = T0 < T1 < T2 < · · · , be the times of these transitions. As the sample paths of the process Xt are right-continuous step functions, we can define Yn = XTn as the state entered at Tn. Moreover, we set:

Un = Tn - Tn-1 , n  N , and U0 = 0

The random variable Un represents the sojourn duration of the process in state
Yn-1 = XTn-1 entered at time Tn-1. A Markov point process Xt can be represented by a multivariate counting process characterized by the sequence (Tn, en)n0. In this representation, en is the transition at time Tn, i.e.:

en = (Yn-1,Yn) with Yn-1 = Yn .

Thus en takes its value in a finite set with K(K - 1) elements. Yet, the represen-
tation of Xt as a point process is easier to formalize. So, we are interested in the distribution of the sequences (Tn,Yn)n0 or (Un,Yn)n0, rather than of the sequence (Tn, En)n0.
For that purpose, we firstly set  j = -q j j for any j  E, and we define quantities  jk as follows:

· If  j = 0, j  E, · If  j = 0 , j  E

 j j = 0 and  jk = q jk/ j , k = j  j j = 1 and  jk = 0 , k = j

Theorem 17.4. If Xt is a time-homogeneous standard Markov process, then
(i) (Un,Yn)n0 is a Markov sequence and (Un,Yn) is independent of Un-1 given Yn-1. Moreover Un and Yn are conditionally independent given Yn-1.
(ii) Un given Yn-1 = j has an exponential distribution with parameter  j if  j = 0. If  j = 0, the state j is absorbing and Un =  with probability 1.
(iii) Y = (Yn)n0 is a Markov chain with transition matrix:

588

J.-P. Florens et al.

Pr(Yn = k | Yn-1 = j) =  jk, ( j, k)  E × E

Proof. See Bhattacharya and Waymire (1990), pp. 275­279.

Theorem 17.5. If Xt is irreducible ( j, k, m such that p jk(m) > 0) and recurrent (Pr(inf{m | Yn+m = j} <  | Yn = j) = 1) then:

(i) the limits of transition probabilities p jk(t) exist and are independent of the ini-

tial state, i.e.

lim
t

p

jk

(t)

=

k

(ii) either  = (1, . . . , K) = (0, . . . , 0), in which case all states are said to be null recurrent, or Kk=1 k = 1, in which case all states are said to be non-null recurrent (or positive recurrent if k > 0, k  E).

Proof. See Cox and Miller (1966), pp. 106­117.

The limiting distribution  is also invariant or stationary, because:

 =  · P(t) , t  R+

In the case of an irreducible, recurrent non-null Markov process with generator Q, calculation of the vector  is made easier by noting that  is the unique invariant distribution probability satisfying the linear equation:

·Q = 0

Moreover, if the embedded Markov chain Y is also irreducible and recurrent nonnull, Y has a limit distribution v satisfying:

v = v·R

where R is the transition matrix of the embedded Markov chain. The relationship between the two limit distributions  and v is:

j =

vj j

K vk
k=1 k

,

jE

or equivalently:

vj =

jj Kk=1 kk

,

jE

Generally, v and  are different. The last equation has a very interesting interpretation: since v j is the long-run frequency of visits of the chain Y to state j, and since (1/ j) is the mean duration of a sojourn of the process Xt in state j, then  j, which is the long-run proportion of occupation of state j for the process xt , is calculated as the long-run global duration of sojourn in state j for the process Xt (calculated as

17 Duration Models and Point Processes

589

the product of v j and 1/ j), divided by the sum of the long-run global durations of sojourn in the different states.

17.6.2.3 Statistical Inference for Time-Homogeneous Markov Models

Now we shall discuss the problem of estimating the generator Q of a time-

homogeneous Markov process Xt from the observation of N independent sample paths over a fixed time interval [0, T ]. Firstly, we consider the nonparametric

case of N i.i.d. realizations of Xt over [0, T ]. Here the likelihood function LQ(N) is given by



N
 LQ(N) = i=1  Pr(x0(i) = Y0(i)) × e-(T -ni (i))yni (i)



 ×

ni-1 j=0

qY

(i) j

,Y

(i) j+1

e-u(ji+) 1Y

(i) j

 

where ni is the number of transitions observed for the i­th sample path over [0, T ], 0 < 1(i) < 2(i) < · · · < n(ii) being the ordered sequence of transition times for this sample, and {(u0(i),Y0(i)), (u1(i),Y1(i)), . . . , (un(ii),Yn(ii))} being the sequence of successive sojourn durations and visited states for the i­th sample path, with the conventions: u0(i) = 0 and un(ii)+1 = T - n(ii).
If we denote NT,N( j, k) the total number of transitions from state j to state k observed over the N realizations and DT,N( j) the total length of time that state j is
occupied during these N sample paths, then it is easy to show that the maximum
likelihood estimator for q j,k is given by:

q j,k(T, N)

=

NT,N ( j, k) DT,N ( j)

if j = k and DT,N( j) = 0. If DT,N( j) = 0, the MLE of q j,k does not exist and we adopt the convention that:

q j,k(T, N) = 0 if j = k and DT,N( j) = 0 .
Asymptotic properties of the MLE estimates q j,k(T, N) when T   and N is fixed (typically, N = 1), or when N   and T is fixed, are given by the following theorems (see Basawa and Prakasa Rao (1980), pp. 195­197).

Theorem 17.6. If there is a positive probability that the j-th state will be occupied at some date t  0, then

plim q j,k(T, N) = q j,k a.s.
T 

590

J.-P. Florens et al.

and if each state has a positive probability of being occupied, then the random

variables

N1/2(q j,k(T, N) - q j,k)
k= j

are asymptotically normal and independent with zero mean and variance

q j,k

T 0

Pr[Xt

=

j]

dt

Moreover, q j,k(T, N) is asymptotically efficient when N tends to infinity.

Theorem 17.7. If the time-homogeneous standard Markov process Xt is regular and recurrent positive, then

plim q j,k(T, 1) = q j,k a.s.
T 
and the random variables
{T 1/2(q j,k(T, 1) - q j,k} j,k=1,...,K, j=k
are asymptotically normal and independent with zero mean and variance q j,k/ Q( j, j) where  is the product of the non-zero eigenvalues of Q and Q( j, j) is the ( j, j)-th cofactor of Q.

In the last case (i.e. when N = 1, T  ), it is shown that

Since

T
Pr[xt = j]dt = Q( j, j)T -1 + o(T ) for T  
0

1 T

E

  

LogLQ(1)  q j,k

2 

=

T 0

Pr[xt

=

j]

dt

T q j,k

it follows from the previous theorem that

T 1/2 q j,k(T, 1) - q j,k d N 0, q j,k/Q( j, j)
and so q j,k(T, 1) is asymptotically efficient for T  . Now let us suppose that transition intensities are functions of a set  = (1, . . . , p)
of unknown parameters, i.e. they have the form q j,k( ). The problem is then to obtain a MLE of  from N independent observations of the process xt ( ) over the period [0, T ]. In this case, the likelihood function is:

17 Duration Models and Point Processes

591



N 
LQ(N,  ) = i=1 Pr

x0(i,  ) = Y0(i)

×

-(T
e

-n(ii) )

Y(n(ii))



 ×

ni-1 j=0

qY(j(i))

,Y

(i) j+1

e-u(ji+) 1Y(j(i))

 

N

 =

Pr(x0(i,  ) = Y0(i)) ×

K

(q(j,k))NT,N ( j,k)

i=1

j,k=1; j=k

 ×

K

e-

( j

)

DT,N

(

j)

j=1

where:

· DT,N ( j) = Ni=1 n=i 0 u(i+) 1I(Y (i) = j) is the total sojourn duration in state j, which

is observed over the N sample paths (with the convention un(ii)+1 = T - n(ii)),

·

NT,N ( j, k) = Ni=1 n=i-01 I

Y (i)

=

j,

Y

(i) +1

=

k

is the total number of transitions

from j to k, observed over the N sample paths.

With the assumption that the initial state Y0(i) does not depend on  , the ML equations for estimating  are:





 

Log LQ(N,  )  m

=

K  NT,N ( j, k) - DT,N ( j)q(jk )  dq(jk )

j,k=1 j=k

q(jk )

dm

= 0,

m = 1, . . . ,

p

In the case where N = 1, Billingsley (1961), p. 46, has shown that these equations yield a consistent solution  = (1, . . . , p) such that

NT1/2( -  ) d N(0, i( )-1) as T   where NT is the total number of transitions during the interval [0, T ] and

i(

)

=

-

1 NT



E

 2 Log LQ( )

 m  m 



m=1,...,


p

 =

 K
j,k=1

v j( )  j( ) q(jk )

 dq(jk ) dm



dq(jk ) dm



j=k

m, m =1,··· ,p

v( ) = [v j( )] j=1,...,K being the limit distribution of the embedded Markov chain associated to the process Xt .

592
17.6.3 Semi-Markov Processes

J.-P. Florens et al.

Semi-Markov processes can be viewed as direct extensions of Markov processes. Using notations and concepts introduced for the characterization of a Markov process, we will say that a stochastic process {Xt }t0 taking its value in the discrete state space E = {1, . . . , K} is semi-markovian if the sequence {Yn}n0 of states visited remains a Markov chain, but time un spent in the state Yn-1 need not be exponentially distributed and may depend on the next state entered, namely Yn.
Definition 17.9. If (Yn)nN and (un)nN denote respectively the sequences of visited states and sojourn durations of a continuous-time process {Xt }t0 with a discrete state space E = {1, . . . , K}, then {Xt }t0 is a semi-Markov process if:

Pr Y = j, u  t | (Yn)0-1, (un)0-1

= Pr {Y = j, u  t | Y -1}

 N, j  E, t  R+

with the convention u0 = 0. Moreover, a semi-Markov process {Xt }t0 is said to be time-homogeneous if transition probabilities

Pr {Y = j, u  t | Y -1 = i} = P(i, j,t) , (i, j)  E × E
do not depend on . The function P is called the kernel of the semi-Markov process {xt }t0. Then the sequence Y = (Yn)nN is a Markov chain with transition matrix:
R(i, j) = P(i, j, ) = lim P(i, j,t) , (i, j)  E × E
t

and u1, u2, . . . are conditionally independent given Y . If the kernel P is defined as
P(i, j,t) = i j(1 - e-it ) , (i, j)  E × E

where i ]0, [, ii = 0 and  jE i j = 1 , i  E, then {Xt }t0 is a timehomogeneous Markov process with generator Q(i, j) = qi j = ii j, j = i. On the other hand, if E = {i} is a singleton, then (un)nN is a time-homogeneous renewal process with an inter­arrival time distribution of the form F(t) = P(i, i,t).
The law of a semi-Markov process {Xt }t0 is jointly characterized by the transition probability R(i, j) of the embedded Markov chain (Yn)nN and the conditional sojourn distributions:

G(i, j,t) = Pr{u  t | Y -1 = i,Y = j},  N, (i, j)  E × E

The kernel function of this semi-Markov process is then defined as:

P(i, j,t) = R(i, j) G(i, j,t)

17 Duration Models and Point Processes

593

from which are deduced unconditional sojourn distributions:

F(i,t) = Pr{u  t | Y -1 = i} =  P(i, j,t) jE

Let us recall that if the Markov chain Y = (Yn)nN is irreducible and recurrent nonnull, there exists a limiting probability distribution v on E of the form:

v j =  vi R(i, j), j  E iE

or in matrix notation:

v = vR

Moreover, if u1(i) = E[u1 | Y0 = i] < , i  E, then the limit distribution of the semi-Markov process {Xt }t0 is given by:

j

=

lim
t

Pr{xt

=

j}

=

v ju1( j) iE viu1(i)

Notice that this relation between  and v is more general than the one for Markov processes, for which u1(i) = i-1.
The main statistical problem is to estimate the semi-Markov kernel P. Here
we concentrate on a fully nonparametric estimation procedure for a semi-Markov
process{Xt }t0, where the distribution of a sojourn in state i does not depend on the next state to be entered, i.e.:

G(i, j,t) = F(i,t), (i, j)  E × E, t  R+

Then R(i, j) and F(i,t) can be estimated from N i.i.d. realizations of {Xt }t0 over a fixed time interval [0, T ]. In that case, let us denote NT,N(i, j) and NT,N(i) =  jE NT,N(i, j) the number of transitions from i to j in [0, T ] and the number of sojourns in state i completed before time T , respectively. Then nonparametric
maximum-likelihood estimators of the unconditional sojourn distributions and of
the transition matrix of the embedded Markov chain are respectively given by:

NT,N (i)

 F(i,t) = NT,N(i)-1

I(Y -1 = i, u  t)

=1

and R(i, j) = NT,N(i, j)/NT,N(i) .
Consequently, one obtains:

P(i, j,t) = R(i, j) · F(i,t) .

Asymptotic properties (convergence, normality) of these nonparametric estimators are reviewed by Karr (1986), Theorem 8.33. Non-parametric estimation of

594

J.-P. Florens et al.

the kernel P of partially observed renewal processes has been considered by Gill (1980) and surveyed by Karr (1986), pp. 347­351.

17.7 Statistical Analysis of Counting Processes
In this section, we present both the statistical analysis of counting processes based on martingale estimators and the extension to these processes of the semiparametric inference initially proposed by Cox (1972, 1975), for duration models. For that purpose, we consider a multivariate counting process with covariates, but our presentation is restricted to the case of a non-censored independent sampling scheme for a counting process derived from a point process.
Let us denote n the number of individuals and i an element of {1, . . . , n}. For any individual i, we observe both the path Xti of a point process valued in a discrete state space E with K elements and the path of a (multivariate) covariate process Zi = (Zti)t . These two processes are observed over an interval [0, T ] for any i. Given (Zti)t , the (Xti)t 's are assumed to be independent. The distribution of (Xti)t is also assumed to be independent of the (Ztj)t for any j = i, i.e. it is independent of the covariate processes of other individuals.
Now we have to describe the distribution of (Xti)t given (Zti)t . This goal is achieved by representing the point process (Xti)t through a multivariate counting process (Nti, j)t , where j = (k, ), k,  E, k = , j  {1, . . . , J = K(K - 1)}. This counting process increases by jumps of size one when the individual i moves from state k to state . The distribution of (Nti, j)t , given (Zti)t , is characterized by its stochastic intensity with respect to the filtration generated by both the past of all the Nti, j processes, for i  {1, . . . , n} and j  {1, . . . , J}, and by the whole trajectories of all the (Zti) processes. These stochastic intensities are assumed to take the following form:
hti, j =  j (Zti)t ,  tj Yti, j , i = 1, . . . , n , j = 1, . . . , J
where:
(i)  j (Zti)t ,  is a known positive function depending on an unknown parameter     Rp ; in practice, each  j may depend on a subvector of  only, and then one has to check if the vector  is identified by the vector of the  j functions;
(ii) tj is the baseline intensity function of Nti, j; it does not depend on individual i; the model is then a proportional hazard type model in which covariates act multiplicatively through the  j functions on the baseline intensity; moreover, tj is assumed to be non-stochastic and hence a function valued in R+; in the semiparametric approach, the function tj is assumed to be totally unknown and the distribution of the Xti processes are then characterized by parameters  and by functions (tj), j = 1, . . . , J;

17 Duration Models and Point Processes

595

(iii) finally, Yti, j is a family of observable predictable stochastic processes valued in {0, 1}; we restrict our attention to the case where Yti, j characterize the fact that the individual i is "at risk" at time t for jumping from state k to state , if
j = (k, ), or equivalently:

Yti, j = 1 if xti- = k Yti, j = 0 elsewhere
As hti, j is the stochastic intensity with respect to all the trajectories of the covariate processes, it would be dependent of all the (Zti)t , i = 1, . . . , n. However, we have assumed that (Xti)t is independent of (Zti )t for any i = i given (Zti)t , and this assumption is expressed by the fact that  j depends only on (Zti)t . In fact, this requirement is not an assumption but is a condition on the definition of the (Zti) processes which may have some elements in common. Moreover,  j may be a function of the whole trajectory of (Zti)t or of the current value Zti only. The first case requires the continuous-time observation of covariates, which is unre-
alistic, or some approximation procedures such as discretization of stochastic in-
tegrals. The more common case is the one where the instantaneous probability
of a jump from state k to state for the individual i depends only on the current value of the process Zi, which implies that  j (Zti)t ,  may be written as  j(Zti,  ). For example, if (Zti) is a q-dimensional process, a usual specification is the following:

 j(Zti,  ) = exp(Zti  j)
where  j  Rq and  = ( j) j=1,...,J. More generally, such a specification may be constrained by imposing that some components of Zti in the  j function are eliminated.
Up to an additive constant, the log-likelihood of the model is equal to

LT ( ,  1, . . . ,  J)

   n J
=

T ln  j(Zti,  ) dNti, j + J

T ln tj dNtj

i=1 j=1 0

j=1 0

  n J
-

T  j(Zti,  )tjYti, j dt

i=1 j=1 0

where Ntj = ni=1 Nti, j. The maximum likelihood estimator of  can be derived from this last equation if the tj are known functions of unknown parameters. However, the log-likelihood is unbounded if the functions tj are taken as arguments: tj may be chosen arbitrarily large at observed jump times (and then the second element in the right hand side of the log-likelihood equation may be as large as desired) and null at other times (and then the third element in the r.h.s. of the log-likelihood equation becomes equal to zero). Then it appears clearly that estimation must be

596

J.-P. Florens et al.

based on a different procedure: parameters  can be estimated by maximizing Cox's partial likelihood, and integrals of the tj's are then estimated through martingale techniques.

17.7.1 The Cox Likelihood

Following an argument given by Karr (1986), Chap. 5, the Cox's likelihood can be

derived as the difference between the log-likelihood function of the observations

and

the

log-likelihood

function

of

the

N

j t

processes.

This

difference

is

a

function

of  only, and can be heuristically interpreted as the logarithm of the density of

the Nti, j given Ntj and the covariates. Given the same filtration, intensities of the Ntj

processes are the sum over i of hti, j, i.e.

 htj = tj

n
 j(Zti,  )Yti, j

i=1

and the log-likelihood of the statistic Ntj is equal to

LT ( ,  1, . . . ,  J)

   J
=

Tn
ln

J
 j(Zti,  )Yti, j dNtj +

T ln tj dNtj

j=1 0

i=1

j=1 0

  J
-

T tj

n
 j(Zti,  )Yti, j) dt

j=1 0

i=1

The Cox likelihood is then defined as:

CT ( ) = LT ( ,  1, . . . ,  J) - LT ( ,  1, . . . ,  J)
J
=  CTj ( ) j=1

where

  n
CTj ( ) =

T ln  j(Zti,  ) dNti, j -

T
ln

n
 j(Zti,  )Yti, j dNtj

i=1 0

0

i=1

or equivalently:

exp[CTj ( )]

=

ni=1 ui, jT  j(Ziui, j ,  )

uj T

in=1



j

(Zi


j u

,



)Yi,uj j

In this last expression, the second product of the numerator is computed for all the observed jump times ui, j of the process Nti, j and the product in the denominator

17 Duration Models and Point Processes

597

is

computed

for

all

the

jump

times



j u

of

the

process

N tj ,

i.e.

for

all

the

transitions

from state k to state (if j = (k, )) observed over all the individuals. Parameters

 are estimated via the maximization of CT ( ). Moreover, if  j(Zti,  ) depends on a subvector  j such that all the  j's are variation free, the estimator of  j may be obtained through a maximization of CTj ( ) = CTj ( j) only. In this case, observations of Nti, j for any i and t are sufficient for the estimation of  j.
Asymptotic properties of the maximand of CT ( ), denoted  , have been studied

initially by Andersen and Gill (1982) and surveyed, for example, by Karr (1986),

Ceshtiamp.at5o.rUonfderwuhseunalnrteegnudlsartiotycaonndditthioant s, nit(co-uld

be shown that  is ) is asymptotically

a consistent normal with

variance explicitly given, for example, by Karr (1986), Chap. 5, formulas (5.90a)

to (5.91).

17.7.2 The Martingale Estimation of the Integrated Baseline Intensity

For simplicity, let us first present martingale estimators for i.i.d. counting processes,
i.e. without presence of covariates. The likelihood of such a model is obtained by setting  j(Zti,  ) equal to 1 in the log-likelihood function:

LT ( ,  1, . . . ,  J)

   n J
=

T ln  j(Zti,  ) dNti, j + J

T ln tj dNtj

i=1 j=1 0

j=1 0

  n J
-

T  j(Zti,  )tjYti, j dt

i=1 j=1 0

In this case, one can easily verify that the log-likelihood is a function of Ntj only, up to an additive constant. This means that these processes constitute a sufficient
statistic. Indeed, in this case, the log-likelihood function becomes:

J
LT ( 1, . . . ,  J) =

T ln tj dNtj -

T

tjY

j t

dt

j=1 0

0

and the processes Ntj have the following stochastic intensities:

htj

=

tj

·

Y

j t

where

Y

j t

=

in=1 Yti, j

is

the

number

of

individuals

at

risk

for

the

transition

of

type

j

(from state k to state ) at time t.

We want to estimate the integrals of tj for any j. However, in practice, infor-

mation is only available for the time interval in which there exists some individuals

598

J.-P. Florens et al.

from the sample who are at risk for the analyzed transition. Functions of interest are then:

tj =

t

sj

I(Y

j s

>

0)

ds

0

where

I(Y

j s

>

0)

=

1

if

Y

j s

>

0

and

0

elsewhere.

The martingale estimator of tj is defined by:

tj =

t
(Y

j s

)-1

I(Y

j s

>

0)

dN

j s

0

This estimator may be heuristically justified by the following argument. Let us start with the differential representation of a counting process:

dNsj = hsj ds + dMsj where Msj is a martingale. In our model, this expression becomes

dNsj

=

sj

·

Y

j s

ds

+

dMsj

which

can

be

pre-multiplied

by

(Y

j s

)-1

I(Y

j s

>

0)

to

give:

(Y sj)-1I(Y

j s

>

0) dNsj

=

sj

I(Y

j s

>

0)ds +

(Y

j s

)-1I(Y

j s

>

0)

dMsj

Integrating the two sides of this relation yields:

tj = tj +

t
(Y

sj )-1

I(Y

j s

>

0)

dMsj

0

The difference between tj and tj is then a stochastic integral of a predictable process with respect to a martingale; so it is a martingale (see Dellacherie and
Meyer (1980), Chap. 7, Theorem 3). Moreover, it can be verified that

E(tj - tj) = 0

and

< tj - tj >=

t

sj

(Y

j s

)-1

I(Y

j s

>

0) ds

0

Let us recall that the predictable variation Etj =< tj - tj > plays the role of an instantaneous variance. In particular:

V (tj - tj) = E(< tj - tj >)

17 Duration Models and Point Processes

599

Using a martingale estimation approach, Etj may be estimated by

Etj =

t (Y sj)-2

I(Y

j s

>

0) ds

0

Under standard regularity conditions, estimators tj are asymptotically well-behaved. They are consistent in a strong sense

E sup(tj - tj)2  0 when n  
t

and n1/2(tj - tj) is asymptotically distributed as a centered Gaussian martingale with continuous trajectories and whose predictable variation may be estimated by nEtj .
Let us now return to the general model with covariates. The differential representation of a process Nti, j is then:
dNsi, j =  j(Zsi,  ) · sj · Ysi, jds + dEsi, j

where

Esi, j

is

a

zero-mean

martingale.

From

the

definition

of

dN

j s

,

we

obtain:

n

 d

N

j s

=

 j(Zsi,  ) · Ysi, j

sj

ds

+

dE

j s

i=1

in

which

E

j s

=

in=1

Esi, j

is

still

a

zero-mean

martingale.

Now let us define:

n
 wtj( ) =  j(Zsi,  ) ·Ysi, j i=1
Assuming that  is known, the problem of inference on the integral of the tj's is identical to the previous case without covariates. The function parameters are now:

tj( ) =

t
I(wsj( ) > 0) sj ds

0

and their estimators are given by:

tj( ) =

t

[wsj

(

)]-1

I(wsj

(

)

>

0)

dN

j s

0

If a Cox procedure is initially used and provides an estimator  of  regardless of the tj's, an estimator of tj( ) is obtained by substituting  for  in this last expression. It can be proved (see Andersen and Gill (1982)) that asymptotic properties of tj( ) are identical to those of tj( ) and that estimators tj( ) are independent of each other and independent of  asymptotically.

600
17.8 Conclusions

J.-P. Florens et al.

This chapter focused on definitions and statistical analysis of duration models and point processes. More extensive presentations are contained in textbooks by Kalbfleisch and Prentice (1980), Lawless (1982), Jacobsen (1982), Cox and Oakes (1984), Karr (1986), Daley and Vere-Jones (1988), Lancaster (1990), Andersen, Borgan, Gill and Keiding (1993), or in detailed surveys by Heckman and Singer (1984a), Kiefer (1988), Serfozo (1990), Van den Berg (2001). Markov chains have been completely studied by Chung (1967), Freedman (1971), Revuz (1975) and by Ethier and Kurtz (1986).

References
Abbring, J. and Van den Berg, G. (2003): The nonparametric identification of treatment effects in duration models, Econometrica, 71, 1491­1518.
Andersen, P.K. and Gill, R. (1982): Cox's regression model for counting processes: a large sample study, Annals of Statistics, 10, 1100­1120.
Andersen, P.K., Borgan, Ø., Gill, R.D. and Keiding, N. (1993): Statistical models based on counting processes. New-York: Springer.
Basawa, I.V. and Prakasa Rao, B.L.S. (1980): Statistical inference of stochastic processes. New-York: Academic Press.
Bhattacharya, R.N. and Waymire, E.C. (1990): Stochastic processes with applications. New-York: Wiley and Sons.
Billingsley, P. (1961): Statistical inference for Markov processes. Chicago: The University of Chicago Press.
Chung, K.L. (1967): Markov chains with stationary transition probabilities. New-York: SpringerVerlag.
Cox, D.R. (1972): Regression models and life tables (with discussion), Journal of the Royal Statistical Society, Series B, 34, 187­220.
Cox, D.R. (1975): Partial likelihood, Biometrika, 62, 269­276. Cox, D.R. and Miller, H.D. (1966): The theory of stochastic processes. London: Methuen. Cox, D.R. and Oakes, D. (1984): Analysis of survival data. London: Chapman and Hall. Daley, D.J. and Vere-Jones, D. (1988): An introduction to the theory of point processes. New-York:
Springer-Verlag. Dellacherie, C. and Meyer, P.A. (1980): Probabilite´ et potentiel (Chapitres V a` VIII: The´orie des
martingales). Paris: Hermann. Doob, J.L. (1953): Stochastic processes. New-York: Wiley and Sons. Elbers, C. and Ridder, G. (1982): True and spurious dependence: the identifiability of the propor-
tional hazard model, Review of Economic Studies, 49, 403­409. Ethier, S.N. and Kurtz, T.G. (1986): Markov processes: characterization and convergence.
New-York: Wiley and Sons Fouge`re, D., Le Bihan, H. and Sevestre, P. (2007): Heterogenity in consumer price stickiness: a
microeconometric approach, Journal of Business and Economic Statistics, 25, 247­264. Fourgeaud, C., Gourie´roux, C. and Pradel, J. (1990): Heterogeneity and hazard dominance in du-
ration data models (in french), Annales d'Economie et de Statistique, 18, 1­24. Freedman, D. (1971): Markov chains. San Francisco: Holden-Day. Gill, R.D. (1980): Non-parametric estimation based on censored observations of a Markov renewal
process, Zeitschrift fu¨r Wahrscheinlichkeitstheorie und Verwandte Gebiete, 53, 97­116.

17 Duration Models and Point Processes

601

Han, A. and Hausman, J.A. (1990): Specification and semiparametric estimation of duration models, Journal of Applied Econometrics, 5, 1­28.
Heckman, J.J. and Honore´, B. (1989): The identifiability of the competing risks model, Biometrika, 76, 325­330.
Heckman, J.J. and Singer, B. (1984a): Econometric duration analysis, Journal of Econometrics, 24, 63­132.
Heckman, J.J. and Singer, B. (1984b): The identifiability of the proportional hazards model, Review of Economic Studies, 51, 231­243.
Honore´, B. (1993): Identification results for duration models with multiple spells, Review of Economic Studies, 60, 241­246.
Jacobsen, M. (1982): Statistical analysis of counting processes. Berlin: Springer-Verlag. Kalbfleisch, J.D., and Prentice, R.L. (1980): The statistical analysis of failure time data. New York:
Wiley. Karr, A.F. (1986): Point processes and their statistical inference. New-York: Marcel Dekker. Kiefer, N. (1988): Economic duration data and hazard functions, Journal of Economic Literature,
XXVI, 646­679. Kortram, R.A., Lenstra, A.J., Ridder, G. and Van Rooij A.C.M. (1995): Constructive identification
of the mixed proportional hazards model, Statistica Neerlandica, 49, 269­281. Lancaster, T. (1990): The Econometric analysis of transition data, Econometric Society Mono-
graphs, Cambridge University Press: New York. Lawless, J.F. (1982): Statistical models and methods for lifetime data. New York: Wiley. Marshall, A.W. and Olkin, I. (1967): A multivariate exponential distribution, Journal of the
American Statistical Association, 62, 30­44. Ma´tya´s, L. and Sevestre, P. (editors)(1996): The econometrics of panel data, Second Revised Edi-
tion. Dordrecht: Kluwer Academic Publishers. Melino A. and Sueyoshi, G.T. (1990): A simple approach to the identifiability of the proportional
hazard model, Economics Letters, 33, 63­68. Mouchart, M. (2004), The econometrics of duration data and of point processes, Lecture Notes,
http://www.stat.ucl.ac.be/ISpersonnel/mouchart/DuDa040908b.pdf Mouchart, M. and J.-M. Rolin (2002): Competing risks models: problems of modelling and of
identification, in Life tables, modelling survival and death, edited by G. Wunsch, M. Mouchart and J. Duche^ne, Dordrecht: Kluwer Academic Publishers, 245­267. Ridder G. (1990): The nonparametric identification of generalized accelerated failure-time models, Review of Economic Studies, 57, 167­181. Revuz, D. (1975): Markov chains. New-York: North Holland/American Elsevier. Serfozo, R.F. (1990): Point processes, in Handbook in operations research and management science, Vol. 2, edited by D.P. Heyman and M.J. Sobel, Amsterdam: North-Holland, 1­94. Tsiatis, A.A. (1981): A large sample study of Cox's regression model, Annals of Statistics, 9, 93­108. Van den Berg G. (2001): Duration models: specification, identification and multiple durations, in Handbook of econometrics, Vol. 5, edited by J. Heckman and E.Leamer, Amsterdam: NorthHolland, 3381­3460.

Chapter 18
GMM for Panel Data Count Models
Frank Windmeijer

18.1 Introduction
This chapter gives an account of the recent literature on estimating (dynamic) models for panel count data. Specifically, the treatment of unobserved individual heterogeneity that is correlated with the explanatory variables and the presence of explanatory variables that are not strictly exogenous are central. Moment conditions are discussed for these types of problems that enable estimation of the parameters by the Generalised Method of Moments (GMM). Interest in exponential regression models has increased substantially in recent years. The Poisson regression model for modelling an integer count dependent variable is an obvious example where the conditional mean function is routinely modelled to be exponential. But also models for continuous positive dependent variables that have a skewed distribution are increasingly being advocated to have an exponential conditional mean function. Although for these data the log transformation can be applied to multiplicative models, the "retransformation" problem often poses severe difficulties if the object of interest is the level of for example costs, see e.g. Manning, Basu and Mullahy (2005). Santos Silva and Tenreyro (2006) also strongly recommend to estimate the multiplicative models directly, as the log transformation can be unduly restrictive. Although the focus of this chapter is on models for count data, almost all procedures can directly be applied to models where the dependent variable is a positive continuous variable and the conditional mean function is exponential. The one exception is the linear feedback model as described in Sect. 18.3.4, which is a dynamic model specification specific to discrete count data.
Section 18.2 discusses instrumental variables estimation for count data models in cross sections. Section 18.3 derives moment conditions for the estimation of (dynamic) models for count panel data allowing for correlated fixed effects and

Frank Windmeijer Department of Economics, University of Bristol, 8 Woodland Road, Bristol BS8 1TN, UK, e-mail: f.windmeijer@bristol.ac.uk

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

603

c Springer-Verlag Berlin Heidelberg 2008

604

F. Windmeijer

weakly exogenous regressors. Section 18.4 discusses GMM estimation. Section 18.5 reviews some of the applied literature and software to estimate the models by nonlinear GMM. As standard Wald tests based on efficient two-step GMM estimation results are known to have poor finite sample behavior, Sect. 18.6 considers alternative test procedures that have recently been proposed in the literature. It also considers estimation by the continuous updating estimator (CUE) as this estimator has been shown to have a smaller finite sample bias than one- and two-step GMM. As asymptotic standard errors for the CUE are downward biased in finite samples we use results from alternative, many weak instrument asymptotics that lead to a larger asymptotic variance of the CUE.

18.2 GMM in Cross-Sections

The Poisson distribution for an integer count variable yi, i = 1, . . . , N, with mean i is given by
e-i  yi P (yi) = yi!
and the Poisson regression model specifies i = exp (xi ), where xi is a vector of explanatory variables and  a parameter vector to be estimated. The log-likelihood function for the sample is then given by

N
ln L =  yi ln (i) - i - ln (yi!) i=1

with first-order condition

  ln L


=

N
xi (yi
i=1

-

i)

=

0

.

(18.1)

It is therefore clear that the Poisson regression estimator is a method of moments estimator. If we write the model with an additive error term ui as

yi = exp xi + ui = i + ui

with E (xiui) = E (xi (yi - i)) = 0 ,
this is clearly the population equivalent of the sample first order condition in the Poisson regression model.
An alternative moment estimator is obtained by specifying the error term as multiplicative in the model
yi = exp xi  wi = iwi
with associated moment conditions

18 GMM for Panel Data Count Models

605

E ((wi - 1) |xi) = E

yi - i i

|xi

= 0.

(18.2)

Mullahy (1997) was the first to introduce GMM instrumental variables esti-
mation of count data models with endogenous explanatory variables. He used the
multiplicative setup with xi being correlated with the unobservables wi such that E ((wi - 1) |xi) = 0 and the moment estimator that solves (18.2) is therefore not consistent. There are instruments zi available that are correlated with the endogenous regressors, but not with wi such that

E ((wi - 1) |zi) = E

yi - i i

|zi

= 0.

(18.3)

Denote1

gi = zi

yi - i i

,

then the GMM estimator for  that minimises

  QN ( ) =

1N

N

gi
i=1

WN-1

1N

N

gi
i=1

is consistent, where WN is a weight matrix. The efficient two-step weight matrix is

given by

 WN

1

1N

=

N

gi
i=1

1

gi

1

where





gi 1 = zi  yi - exp xi 1 

exp xi 1

with 1 an initial consistent estimator. Angrist (2001) strengthens the arguments for using these moment conditions for causal inference as he shows that in a model with

endogenous treatment and a binary instrument, the Mullahy procedure estimates a

proportional local average treatment effect (LATE) parameter in models with no

covariates.

Windmeijer and Santos Silva (1997) propose use of the additive moment

conditions

E ((yi - i) |zi) = 0,

(18.4)

estimating the parameters  again by GMM, with in this case gi = zi (yi - i). They and Mullahy (1997) compare the two sets of moment conditions and show that both sets cannot in general be valid at the same time. One exception is when there is

1 From the conditional moments (18.3) it follows that any function h (z) are valid instruments, which raises the issue of optimal instruments. Here, we will only consider h (z) = z .

606

F. Windmeijer

classical measurement error in an explanatory variable, as in that case both additive and multiplicative moment conditions are valid. Consider the simple model
yi = exp ( + xi ) + ui but xi is not observed. Instead we observe xi
xi = xi + i
and estimate  in the model

yi = exp ( + xi - i ) + ui .

When instruments zi are available that are correlated with xi and independent of the i.i.d measurement errors i, then the multiplicative moment conditions

E

yi - i i

|zi

=0

are valid, where

i = exp ( + xi )  =  + ln (E [exp (- )]) ,

and the latter expectation is assumed to be a constant. However, also the additive moment conditions are valid as

E ((yi - i) |zi) = 0.

18.3 Panel Data Models
Let yit denote the discrete count variable to be explained for subject i, i = 1, . . . , N, at time t, t = 1, . . . , T ; and let xit denote a vector of explanatory variables. An important feature in panel data applications is unobserved heterogeneity or individual fixed effects. For count data models these effects are generally modelled multiplicatively as
yit = exp xit  + i + uit = it i + uit ,
where i = exp (i) is a permanent scaling factor for the individual specific mean. In general, it is likely that the unobserved heterogeneity components i are correlated with the explanatory variables, E (xit i) = 0, and therefore standard random effects estimators for  will be inconsistent, see Hausman, Hall and Griliches (1984). This section will describe moment conditions that can be used to consistently estimate the parameters  when there is correlation between i and xit and allowing for different

18 GMM for Panel Data Count Models

607

exogeneity properties of the explanatory variables, i.e. the regressors being strictly
exogenous, predetermined or endogenous. Throughout we assume that the uit are not serially correlated and that E (uit |i) = 0, t = 1, . . . , T .

18.3.1 Strictly Exogenous Regressors

When the xit are strictly exogenous, there is no correlation between any of the idiosyncratic shocks uis, s = 1, . . . , T and any of the xit , t = 1, . . . T , and the conditional mean of yit satisfies

E (yit |i, xit ) = E (yit |i, xi1, . . . , xiT ) .

For this case, Hausman, Hall and Griliches (1984) use the Poisson conditional maximum likelihood estimator (CMLE), conditioning on tT=1 yit , which is the sufficient statistic for i. This method mimics the fixed effect logit approach of Chamberlain (1984). However, the Poisson maximum likelihood estimator (MLE) for  in a
model with separate individual specific constants does not suffer from the incidental
parameters problem, and is therefore consistent and the same as the CMLE. To see
this, note that the maximum likelihood first order conditions for the i are given by

   lnL
 i

=

T t=1



(yit

ln (it i) -  i

it i)

=

T t=1

yit i

- it

=0

and therefore the MLE for i is given by

i(ML)

=

yi i

,

where yi = T -1 tT=1 yit and i = T -1 tT=1 exp (xit  ). The MLE of the fixed effect is independent of i. Substituting the fixed effects estimates in the first order conditions
for  results in the moment conditions

   lnL


(i

)

=

N i=1

t

T =1

yit

-

it

yi i

xit = 0 .

When xit is strictly exogenous,

  plimN

1 N

 ln L 

(i)

=

plimN

1 N

NT i=1 t=1

uit

-

it i

ui

xit = 0 ,

with ui = T -1 tT=1 uit , and therefore the MLE for  is consistent.2 It is further identical to the CMLE. The latter can be seen as follows. The Poisson conditional
log likelihood function is given by

2 Lancaster (2002) finds the same result for the Poisson model by means of a decomposition of the likelihood.

608

F. Windmeijer

NT

NT

T

lnCL =   (yit + 1) -   yit ln  exp -(xit - xis)  ,

i=1 t=1

i=1 t=1

s=1

where (.) is the gamma function, see Hausman, Hall and Griliches (1984, p. 919). The first-order condition for  is

   

lnCL 

=

NT

yit

i=1 t=1 Ts=1 exp (-(xit

- xis)

T
exp  ) s=1

-(xit - xis) 

(xit - xis)

    =

NT
yit xit
i=1 t=1

NT

-

yit

i=1 t=1

Ts=1 xis exp (xis ) Ts=1 exp (xis )

  N T

=

xit

i=1 t=1

yit

-

it

yi i

,

which is exactly the same as the MLE first order condition for  .

The first order conditions imply that the Poisson MLE for  is equivalent to the

moment estimator in a model where the ratio of individual, or within group, means

are used to approximate the individual specific effects. This mean scaling model is

given by

yit

=

it

yi i

+ uit ,

(18.5)

where

uit

=

uit

-

it i

ui.

Blundell,

Griffith

and

Windmeijer

(2002)

call

the

resulting

estimator the within group mean scaling estimator.3

18.3.2 Predetermined Regressors

A regressor is predetermined when it is not correlated with current and future shocks, but it is correlated with past shocks:

E (xit uit+ j) = 0, j  0 E (xit uit-s) = 0, s  1 .

With predetermined regressors, the within group mean scaling estimator is no longer consistent. Chamberlain (1992) has proposed a transformation that eliminates the fixed effect from the multiplicative model and generates orthogonality conditions that can be used for consistent estimation in count data models with predetermined regressors. The quasi-differencing transformation is

sit

=

yit

it-1 it

- yit-1

=

uit

it-1 it

- uit-1

.

3 Clearly, the Poisson pseudo-likelihood results are preserved, see also Wooldridge (1999).

18 GMM for Panel Data Count Models

609

Let xit-1 = (xi1, . . . , xit-1). When xit is predetermined, the following moment conditions hold:

E sit |xit-1 = E

E

uit |xti

it-1 it

-

uit-1

|xit-1

= 0.

(18.6)

Wooldridge (1991) proposed the following quasi-differencing transformation

qit

=

yit it

-

yit-1 it-1

=

uit it

-

uit-1 it-1

,

with moment conditions E qit |xit-1 = E

E

(uit |xit it

)

-

uit-1 it-1

|xit-1

=0.

It is clear that a variable in xit can not have only non-positive or non-negative values, as then the corresponding estimate for  is infinity. A way around this

problem is to transform xit in deviations from its overall mean, xit = xit - x, with

x

=

1 NT

Ni=1

tT=1 xit ,

see

Windmeijer

(2000).

Both moment conditions can also be derived from a multiplicative model

specification

yit = exp xit  + i wit = it iwit ,

where xit is now predetermined w.r.t. wit . Again, we assume that the wit are not serially correlated and not correlated with i, and E (wit ) = 1. The Chamberlain quasi-differencing transformation in this case is equivalent to

sit

=

yit

it-1 it

- yit-1

=

iit-1 (wit

- wit-1)

,

with moment conditions

E sit |xit-1 = E iit-1E (wit - wit-1) |i, xit-1 |xit-1 = 0 .

Equivalently, for the Wooldridge transformation,

qit

=

yit it

-

yit-1 it-1

= i (wit

- wit-1)

and E qit |xit-1 = E iE (wit - wit-1) |i, xti-1 |xti-1 = 0 .

18.3.3 Endogenous Regressors
Regressors are endogenous when they are correlated with current (and possibly past) shocks E (xit uit-s) = 0, s  0, for the specification with additive errors uit , or when

610

F. Windmeijer

E (xit wit-s) = 0, s  0, for the specification with multiplicative errors wit . It is clear from the derivations in the previous section that we cannot find valid sequential conditional moment conditions for the specification with additive errors due to the nonseparability of the uit and it . For the multiplicative error specification, there is again non-separability of it-1 and (wit - wit-1) for the Chamberlain transformation and so
E sit |xti-2 = E iit-1E (wit - wit-1) |i, xit-1 |xit-2 = 0 .

In contrast, the Wooldridge transformation does not depend on it or it-1 in this case. Valid moment conditions are then given by

E qit |xti-2 = E iE (wit - wit-1) |i, xit-2 |xit-2 = 0 .

Therefore, in the case of endogenous explanatory variables, only the Wooldridge transformation can be used for the consistent estimation of the parameters  . This includes the case of classical measurement error in xit , where the measurement error is not correlated over time.

18.3.4 Dynamic Models
Specifying dynamic models for count data by including lags of the dependent count variables in the explanatory part of the model is not as straightforward as with linear models for a continuous dependent variable. Inclusion of the lagged dependent variable in the exponential mean function may lead to rapidly exploding series. A better starting point is to specify the model as in Cre´pon and Duguet (1997)
yit = h (yit-1, ) exp xit  + i + uit ,
where h (., .) > 0 is any given function describing the way past values of the dependent variable are affecting the current value.
Let dit = 1{yit =0} ,
then a possible choice for h (., .) is
h (yit , ) = exp (1 ln (yit-1 + cdit-1) + 2dit-1) ,
where c is a pre-specified constant. In this case, ln (yit-1) is included as a regressor for positive yit-1, and zero values of yit-1 have a separate effect on current values of yit . Cre´pon and Duguet (1997) considered
h (yit , ) = exp ( (1 - dit-1)) ,
and extensions thereof to several regime indicators.

18 GMM for Panel Data Count Models

611

Blundell, Griffith and Windmeijer (2002) propose use of a linear feedback model for modelling dynamic count panel data process. The linear feedback model of order 1 (LFM(1)) is defined as

yit = yit-1 + exp(xit  + i) + uit = yit-1 + it i + uit ,

where the lag of the dependent variable enters the model linearly. Extending the
model to include further lags is straightforward. The LFM has its origins in the
Integer-Valued Autoregressive (INAR) process and can be motivated as an entryexit process with the probability of exit equal to (1 - ). The correlation over time for the INAR(1) process without additional regressors is similar to that of the AR(1) model, corr (yit , yit- j) =  j.
For the patents-R&D model, Blundell, Griffith and Windmeijer (2002) consider
the economic model

Pit = k Rit + (1 - ) Rit-1 + (1 - )2 Rit-2 . . . i + it

(18.7)

where Pit and Rit are the number of patents and R&D expenditures for firm i at time t

respectively, k is a positive constant and R&D expenditures depreciate geometrically

at rate . The long run steady state for firm i, ignoring feedback from patents to

R&D, can be written as

Pi

=

k 

Ri

i

,

and  can therefore be interpreted as the long run elasticity. Inverting (18.7) leads to

Pit = kRi i + (1 - ) Pit-1 + uit

and so in the LFM model

Pit = Pit-1 + exp (k +  ln (Rit )) i + uit

the estimate for  is an estimate of the depreciation factor (1 - ) and the estimate for  is an estimate of the long run elasticity of the R&D returns to patents.
Even when the xit are strictly exogenous, the within group mean scaling estimator will be inconsistent for small T , as the lagged dependent variable is a predetermined
variable. For estimation by GMM, the Chamberlain quasi-differencing transforma-
tion for the LFM(1) model is given by

sit

=

(yit

-

yit

-1)

it-1 it

- (yit-1

- yit-2)

(18.8)

and the Wooldridge quasi-differencing transformation is given by

qit

=

yit

-  yit-1 it

-

yit-1 -  yit-2 it-1

.

612

F. Windmeijer

For predetermined xit the following moment conditions hold E(sit |yti-2, xit-1) = 0; E(qit |yti-2, xit-1) = 0,
while for endogenous xit , only the Wooldridge moment conditions are valid E(qit |yti-2, xti-2) = 0 .

18.4 GMM

The orthogonality conditions as described in the sections above can be used to consistently estimate the model parameters by the GMM estimation technique (see Hansen, 1982). Let  be the k-vector of parameters to be estimated, e.g. for the LFM(1) model  = (,  ) . The model has a true parameter 0 satisfying the q moment conditions
E [gi (0)] = 0 .
The GMM estimator  for 0 is defined as

   = arg min  

1 N

N
gi
i=1

( )

WN-1

1 N

N
gi
i=1

( )

,

where  is a compact set of parameter values; WN satisfies plimN WN = W ,

with W a positive definite matrix. Regularity conditions are assumed such that

limN

1 N

Ni=1

gi

(

)

=

E [gi ( )]

and

1 N

iN=1

gi

(0)



N (0, )

where



=

limN

1 N

iN=1

E

gi (0) gi (0)

. Let  ( ) = E [ gi ( ) /  ] and

0   (0), then N  - 0 has a limiting normal distribution,

 N  - 0  N (0,VW ) ,

where

VW = 0W -10 -1 0W -1W -10 0W -10 -1 .

The efficient two-step GMM estimator, denoted 2, is based on a weight matrix

that satisfies plimN WN = , with VW =

0 -10

-1
in that case. A weight

matrix that satisfies this property is given by

 WN

1

1N

=

N

gi
i=1

1

gi

1

,

18 GMM for Panel Data Count Models

613

where 1 is an initial consistent estimator for 0.

Denote

g

(

)

=

1 N

iN=1

gi

(

).

The

standard

test

for

overidentifying

restrictions

is

N times the minimised GMM criterion

NQWN 2 = Ng 2 WN-1 1 g 2 ,

which has an asymptotic chi-squared distribution with q - k degrees of freedom when the moment conditions are valid.
For the Chamberlain quasi-differencing transformation the GMM estimator  minimises

   = arg min  

1 N

N
si ( )
i=1

Zi

WN-1

1 N

N
Zi si
i=1

( )

,

where, for the LFM(1) model, si ( ) is the T - 2 vector (si3, si4, . . . , siT ) , with sit as

defined in (18.8), Zi is the matrix of instruments and WN is a weight matrix. When

the full sequential set of instruments is used and xit is predetermined, the instrument

matrix for the LFM(1) model is given by





yi1 xi1 xi2

Zi = 

...

 .

yi1 · · · yiT -2 xi1 · · · xiT -1

The efficient weight matrix is

 WN

1

=

1 N

N
Zi si(1)si(1)
i=1

Zi

,

where

1

can

be

a

GMM

estimator

using

for

example

WN

=

1 N

Ni=1

Zi

Zi

as

the

initial

weight matrix. As stated above, under the assumed regularity conditions both 1

and 2 are asymptotically normally distributed. The asymptotic variance of 1 is

computed as

var 1

1 =
N

C 1

WN-1C 1

× C 1 WN-1C 1

-1
C 1 WN-1WN 1 WN-1C 1
-1

where

 C

1

=

1 N

N i=1



Zi si ( 

)

|1

.

The asymptotic variance of the efficient two-step GMM, estimator is computed as

var 2

=1 N

C 2

WN-1 1 C 2

-1
.

614
18.5 Applications and Software

F. Windmeijer

The instrumental variables methods for count data models with endogenous regressors using cross section data, as described in Sect. 18.2, have often been applied in the health econometrics literature. For example, Mullahy (1997) uses the multiplicative moment conditions to estimate cigarette demand functions with a habit stock measure as endogenous regressor. Windmeijer and Santos Silva (1997) estimate health care demand functions with a self reported health measure as possible endogenous variable, while Vera-Hernandez (1999) and Schellhorn (2001) estimate health care demand functions with endogenous insurance choice. An example outside the health econometrics literature is Kelly (2000) who models the number of crimes with police activity as an endogenous regressor.
The count panel data literature has largely focused on estimating models for patenting and the returns to R&D investments, which started with the seminal paper of Hausman, Hall and Griliches (1984). Following the development of the quasi-differencing approach of Wooldridge (1991, 1997), Chamberlain (1992), and Montalvo (1997), Cincera (1997), Cre´pon and Duguet (1997), Blundell, Griffith and Van Reenen (1999) and Blundell, Griffith and Windmeijer (2002) developed and/or estimated patent (or innovation) production functions using the GMM framework allowing for correlated firm specific effects and weakly exogenous inputs. More recently, Kim and Marschke (2005) use the GMM framework to find a relationship between a firms' patenting behavior and scientist turnover, whereas Salomon and Shaver (2005) estimate a linear feedback model and find that exporting has a positive effect on innovating behavior of the firm.
The latter two publications estimated the models using ExpEnd, Windmeijer (2002). This is a user friendly open source GAUSS (Aptech Systems, 2005) code for nonlinear GMM estimation of the models described in the previous sections.4 For cross-section data, ExpEnd estimates simple robust Poisson regression models using moment conditions (18.1); and instrumental variables regressions using Mullahy's (1997) multiplicative moment conditions (18.3) or the additive moment conditions (18.4). For panel data, ExpEnd estimates pooled robust Poisson regression models; fixed effects models, using the mean scaling model (18.5); and the quasidifferencing models using the Chamberlain (1992) or the Wooldridge (1991, 1997) transformation, for static, distributed lag and linear feedback models. For the quasidifferencing models, sequential and so-called stacked IV type instruments can be specified, in both cases allowing for a flexible lag length of the instruments. For overidentified models one- and two-step GMM parameter estimates are reported, together with asymptotic standard errors. The test for overidentifying restrictions is reported and for the panel data models the output further includes a test of first and second order serial correlation of the quasi-differencing "residuals" sit ( ) or qit ( ). If the model is correctly specified one expects to find an MA(1) serial correlation structure.

4 For a review, see Romeu (2004).

18 GMM for Panel Data Count Models

615

Another package that enables researchers to estimate these model types is TSP Hall and Cummins (2005). Kitazawa (2000) provides various TSP procedures for the estimation of count panel data models. Also LIMDEP, Greene (2005), provides an environment where these models can be estimated.

18.6 Finite Sample Inference
Standard Wald tests based on two-step efficient GMM estimators are known to have poor finite sample properties (see e.g. Blundell and Bond (1998)). Bond and Windmeijer (2005) therefore analysed the finite sample performance of various alternative test procedures for testing linear restrictions in linear panel data models. The statistics they found to perform well in Monte Carlo exercises were an alternative two-step Wald test that uses a finite sample correction for the asymptotic variance matrix, the LM test, and a simple criterion-based test. In this section we briefly describe these procedures and adapt them to the case of nonlinear GMM estimation where necessary.
Newey and Smith (2004) have shown that the GMM estimator can further also suffer from quite large finite sample biases and advocate use of Generalized Empirical Likelihood (GEL) estimators that they show to have smaller finite sample biases. We will consider here the performance of the Continuous Updating Estimator (CUE) as proposed by Hansen, Heaton and Yaron (1996), which is a GEL estimator. The Wald test based on the CUE has also been shown to perform poorly in finite samples by e.g. Hansen, Heaton and Yaron (1996). Newey and Windmeijer (2005) derive the asymptotic distribution of the CUE when there are many weak moment conditions. The asymptotic variance in this case is larger than the usual asymptotic one and we will analyse the performance of an alternative Wald test that uses an estimate for this larger asymptotic variance, together with a criterion based test for the CUE as proposed by Hansen, Heaton and Yaron (1996).
The estimators and test procedures will be evaluated in a Monte Carlo study of testing linear restrictions in a static count panel data model with an explanatory variable that is correlated with the fixed unobserved heterogeneity and which is predetermined. The Chamberlain quasi-differencing transformation will be used with sequential moment conditions.

18.6.1 Wald Test and Finite Sample Variance Correction

The standard Wald test for testing r linear restrictions of the form r (0) = 0 is

calculated as

-1
Wald = r  R var  R r  ,

616

F. Windmeijer

where R =  r ( ) /  , and has an asymptotic r2 distribution under the null. Based on the two-step GMM estimator and using its conventional asymptotic variance estimate, the Wald test has often been found to overreject correct null hypotheses severely compared to its nominal size. This can occur even when the estimator has negligible finite sample bias, due to the fact that the estimated asymptotic standard errors can be severely downward biased in small samples. Windmeijer (2005) proposed a finite sample variance correction that takes account of the extra variation due to the presence of the estimated parameters 1 in the weight matrix. He showed by means of a Monte Carlo study that this correction works well for in linear models, but it is not clear how well it will work in nonlinear GMM.
To derive the finite sample corrected variance, let

 g ( )

=

1 N

N
gi
i=1

( ) ;

C

( )

=

 g ( )  ;

G

( )

=

C ( ) 

,

and

b0,WN

=

1 2



QWN 

|0

= C (0)

WN-1g (0) ;

A0,WN

=

1 2

 

2QWN 

|0

= C (0)

WN-1C (0) + G (0)

Ik  WN-1g (0)

.

A standard first order Taylor series approximation of 2 around 0, conditional on WN 1 , results in

2

-

0

=

-A-1
0 ,WN

(1)

b0

,WN

(1

)

+

Op

N -1

.

A further expansion of 1 around 0 results in

2 - 0 = -A-01,WN (0)b0,WN (0) + D0,WN (0) 1 - 0 + Op N-1 ,

(18.9)

where

 WN

(0)

=

1 N

N
gi (0) gi (0)
i=1

and

 D0,WN (0) =  

-A-01,WN ( )b0,WN ( ) |0

is a k × k matrix. Let 1 be a one-step GMM estimator that uses a weight matrix WN that does not
depend on estimated parameters. An estimate of the variance of 2 that incorporates the term involving the one-step parameter estimates used in the weight matrix can
then be obtained as

18 GMM for Panel Data Count Models

617

varc 2

=

1 N

A-1
2 ,WN

(1)C

2

WN-1

1

C

2

A-1
2 ,WN

(1)

+

1 N

D2

,WN

(1)

A-1
1 ,WN

C

1

WN-1C

2

A-1
2 ,WN

(1)

+

1 N

A-1
2 ,WN

(1)C

2

WN-1C

1

A-1
1 ,WN

D2

,WN

(1)

+ D2,WN (1)var 1 D2,WN (1) ,

where the jth column of D2,WN(1) is given by

D2,WN (1)[.,

j]

=

A-1
2 ,WN

(1)C

2

WN-1 1



WN ( j

)

|2WN-1

1

g

2

,

and

 WN ( )
j

=

1 N

N i=1



gi ( j

)

gi

(

)

+ gi ( )

 gi ( ) j

.

The alternative two-step Wald test that uses a finite sample correction for the asymptotic variance matrix is then defined as

-1
Waldc = r 2 R varc 2 R r 2 .

The term D0,W(0) 1 - 0 in (18.9) is itself Op N-1 and in this general setting, incorporating non-linear models and/or non-linear moment conditions, whether taking account of it will improve the estimation of the small sample variance substantially depends on the other remainder terms which are of the same order.

18.6.2 Criterion-Based Tests
Using the notation as in Bond and Windmeijer (2005), the standard two-step Wald test can be computed as a criterion difference
Wald = N g 2 WN-1 1 g 2 - g 2 WN-1 1 g 2 ,
where 1 and 2 are the one-step and two-step GMM estimators in the unrestricted model, whereas 2 is a two-step GMM estimator in the restricted model, but using as a weight matrix the consistent estimate of  based on the unrestricted one-step GMM estimator, WN 1 , see Newey and West (1987).

618
The LM test can also be computed as a criterion difference

F. Windmeijer

LM = N g 2 WN-1 1 g 2 - g 2 WN-1 1 g 2 ,

where 1 and 2 are the one-step and two-step GMM estimators in the restricted model, whereas 2 is a two-step GMM estimator in the unrestricted model, but using as a weight matrix the consistent estimate of  under the null, based on the restricted one-step GMM estimator, WN 1 . The LM test has an asymptotic r2 distribution under the null.
The criterion-based test statistic considered by Bond, Bowsher and Windmeijer (2001) is given by

DRU = N g 2 WN-1 1 g 2 - g 2 WN-1 1 g 2 .

DRU is the "likelihood ratio" test equivalent for GMM, and is the difference between
the test statistics for overidentifying restrictions in the restricted and unrestricted models. Under the null, DRU has an asymptotic r2 distribution.

18.6.3 Continuous Updating Estimator

The Continuous Updating Estimator (CUE) is given by

CU

=

arg min Q ( ) ;
 

Q ( )

=

1 g ( ) 2

WN-1 ( ) g ( )

,

where, as before,

 WN

( )

=

1 N

N
gi ( ) gi ( )
i=1

and so the CUE minimises the criterion function including the parameters in the

weight matrix. The limiting distribution under standard regularity conditions is

given by

 N

CU - 0

 N (0,V ) ; V = 0 -10 -1

and is the same as the efficient two-step GMM estimator. The asymptotic variance of the CUE is computed as

var CU

1 =
N

C CU

WN-1 CU C CU

-1
,

18 GMM for Panel Data Count Models

619

which is used in the calculation of the standard Wald test. Again, it has been shown by e.g. Hansen, Heaton and Yaron (1996) that the asymptotic standard errors are severely downward biased, leading to overrejection of a true null hypothesis using the Wald test.
Newey and Windmeijer (2005) derive the asymptotic distribution of the CUE under many weak instrument asymptotics. In these asymptotics, the number of instruments is allowed to grow with the sample size N, with the increase in number of instruments accompanied by an increase in the concentration parameter. The resulting limiting distribution of the CUE is again the normal distribution, but convergence is at a slower rate than N. The asymptotic variance is in this case larger than the asymptotic variance using conventional asymptotics, and can be estimated consistently by

var

CU

= 1 H-1 cN

CU

S

CU

WN-1 CU S CU H-1 CU

,

where

H ( ) =

 2Q ( )  ;

S ( ) = (S1 ( ) , S2 ( ) , . . . , Sk ( ))

S j ( ) =



g ( j

)

-



j

(

)

WN-1

(

)

g

(

)

  j ( ) =

1 N

N i=1



gi ( ) j

gi

(

)

.

Here, unlike the usual asymptotics, the middle matrix S CU WN-1 CU S CU estimates a different, larger object than the Hessian. Also, the use of the Hessian is important, as the more common formula C CU WN-1 CU C CU has extra random terms that are eliminated in the Hessian under the alternative asymptotics.
Hansen, Heaton and Yaron (1996) proposed the use of a criterion-based test similar to DRU , but based on the CUE. Their test statistic DCRUU is defined as
DCRUU = N Q CU - Q CU ,

where CU and CU are the CUEs for the unrestricted and restricted models respectively. Under the null, DCRUU has an asymptotic r2 distribution.

18.6.4 Monte Carlo Results

In this section we will illustrate the finite sample performance of the GMM estimators and the test statistics as discussed in the previous sections by means of a small Monte Carlo study. The data generating process is given by

620

F. Windmeijer

yit  Poisson (exp (xit  + i + it ))
xit = xit-1 + i +  it-1 + it i  N 0, 2 ; it  N 0, 2 ; it  N 0, 2 ,  = 0.5;  = 0.1;  = 0.3; 2 = 0.3; 2 = 0.3; 2 = 0.25  = {0.5, 0.8} .

The dependent variable is a count variable, generated from the Poisson distribution with unobserved fixed normally distributed heterogeneity i and further idiosyncratic normally distributed heterogeneity it . The xit are correlated with the i and it-1 and are therefore predetermined.
Table 18.1 presents estimation results from 10, 000 Monte Carlo replications for
the one- and two-step GMM estimators as well as the continuous updating estimator for T = 6, N = 250 and  = 0.5 or  = 0.8, using the moment conditions (18.6) as proposed by Chamberlain (1992). The instruments set is given by





xi1

Zi = 

xi1 xi2 ...



xi1 · · · xiT -1

and hence there are a total of 15 moment conditions.The one-step GMM estimator

uses

WN

=

1 N

Ni=1

Zi Zi

as

the

weight

matrix.

When  = 0.5, the instruments are quite strong. The one-step GMM estima-

tor, denoted GMM1 in the table, has a moderate downward bias of -0.0408. Its

standard deviation is 0.1053, which seems well approximated by the asymptotic

standard error. The mean of the estimated standard errors is equal to 0.1031. The

two-step GMM estimator, denoted GMM2, has a smaller bias of -0.0211 and a

smaller standard deviation of 0.0803, representing a substantial efficiency gain with

more than a 23% reduction in standard deviation. In contrast to the one-step esti-

Table 18.1 Estimation results

 = 0.5
GMM1 GMM2 CUE  = 0.8 GMM1 GMM2 CUE

Mean Bias -0.0408 -0.0211
0.0043
-0.1136 -0.0537
0.0033

St Dev
0.1053 0.0803 0.0904
0.2094 0.1335 0.1885

Se
0.1031 0.0652 0.0652
0.1773 0.0908 0.0879

Sec
0.0799 0.0918
0.1365 0.1459

Median Bias -0.0409 -0.0209
0.0024
-0.0974 -0.0498
0.0029

IQR
0.1381 0.1077 0.1165
0.2435 0.1558 0.1742

Note: T = 6, N = 250,  = 0.5, 10,000 replications, sec denotes finite sample corrected standard errors for GMM2 and those resulting from many weak instrument asymptotics for CUE, IQR= Inter Quartile Range

18 GMM for Panel Data Count Models

621

mator, the mean of the usual asymptotic standard errors is 19% smaller than the standard deviation. However, taking account of the extra variation due to the presence of the one-step estimates in the weight matrix results in finite sample corrected standard errors with a mean of 0.0799, which is virtually identical to the standard deviation. The CUE has a very small bias of 0.0043, with a standard deviation of 0.0904, which is larger than that of the two-step GMM estimator, but smaller than that of the one-step estimator. The mean of the usual asymptotic standard errors is exactly the same as that of the two-step GMM estimator and in this case it is almost 28% smaller than the standard deviation. The standard errors resulting from the many weak instruments asymptotics have a mean of 0.0918, which is virtually the same as the standard deviation.
Figure 18.1 shows p-value plots for the hypothesis H0 :  = 0.5, comparing nominal size with rejection frequencies. The various Wald tests are denoted W1, W2, W2C, WCU and WCUC based on one-step GMM, two-step GMM with usual standard errors, two-step GMM with finite sample corrected standard errors, CUE and CUE with standard errors resulting from the many weak instruments asymptotics, respectively. As expected, W2 and WCU overreject the null hypothesis substantially. W1 and W2C perform much better, but are still moderately oversized due to the bias of the estimators. WCUC has a very good performance in terms of size of the test, the rejection frequencies being very close to the 45o line. The two-step GMM based LM and DRU tests also perform very well, their p-value plots being quite similar to

rejection frequency 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16

45° W1 W2 W2C WCU WCUC LM DRU DRCUU
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 nominal size
Fig. 18.1 P-value plot, H0 :  = 0.5

622

F. Windmeijer

that of WCUC. Finally, the CUE based DCRUU performs well, but tends to overreject moderately at higher values of the nominal size.
When  = 0.8, the instruments become weaker as the xit series become more persistent. The one-step GMM estimator now has a quite large downward bias of -0.1136. Its standard deviation is 0.2094, which is now less well approximated by the asymptotic standard error, with the mean of the estimated standard errors being equal to 0.1773. The two-step GMM estimator has a smaller, but still quite substantial bias of -0.0537 and a smaller standard deviation of 0.1335. The mean of the usual asymptotic standard errors is 0.0908, again substantially smaller than the standard deviation. The mean of the finite sample corrected standard errors is 0.1365, which is again very close to the standard deviation. The CUE, also with these weaker instruments, has a very small bias of 0.0033, with a standard deviation of 0.1885. In this case the so-called no moment-problem starts to become an issue for the CUE, though, with some outlying estimates inflating the standard deviation, see Guggenberger (2005). It is therefore better to look at the median bias and inter quartile range (IQR) in this case, which shows that the CUE is median unbiased with an IQR which is only slightly larger than that of the two-step GMM estimator, 0.1742 versus 0.1558 respectively.
Figure 18.2 shows p-value plots for the hypothesis H0 :  = 0.8. W2 and WCU overreject the null hypothesis even more than when  = 0.5. W1 performs better, but is still substantially oversized. W2C and WCUC perform quite well and quite similar, slightly overrejecting the null. The two-step GMM based LM and DRU are again the

rejection frequency 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16

45° W1 W2 W2C WCU WCUC LM DRU DCRUU
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 nominal size
Fig. 18.2 P-value plot, H0 :  = 0.8

18 GMM for Panel Data Count Models

623

best performers in terms of size, whereas the CUE based DCRUU performs worse than W2C and WCUC.
Summarising, it is clear that use of the finite sample corrected standard errors for
the two-step GMM estimator and the standard errors from the many weak instru-
ment asymptotics for the CUE improve the size performance of the Wald tests for
these estimators considerably. The simple criterion based DRU test performs very well in these examples, as was the case in Bond and Windmeijer (2005) for linear
panel data models.

References
Angrist J.D., 2001, Estimation of limited dependent variable models with dummy endogenous regressors: simple strategies for empirical practice, Journal of Business & Economic Statistics 19, 2­16.
Aptech Systems, Inc., 2005, GAUSS, Advanced Mathematical and Statistical System, Black Diamond, WA, USA.
Blundell R. and S.R. Bond, 1998, Initial conditions and moment restrictions in dynamic panel data models, Journal of Econometrics 87, 115­144.
Blundell R., R. Griffith and J. van Reenen, 1999, Market share, market value and innovation in a panel of British manufacturing firms, Review of Economic Studies 66, 529­554.
Blundell R., R. Griffith and F. Windmeijer, 2002, Individual effects and dynamics in count data models, Journal of Econometrics 108, 113­131.
Bond S.R., C. Bowsher and F. Windmeijer, 2001, Criterion-Based inference for GMM in autoregressive panel data models, Economics Letters 73, 379­388.
Bond S.R. and F. Windmeijer, 2005, Reliable inference for GMM estimators? Finite sample procedures in linear panel data models, Econometric Reviews 24, 1­37.
Chamberlain G., 1984, Panel data. In: Griliches, Z. and M. Intriligator (Eds.), Handbook of Econometrics, North Holland, Amsterdam.
Chamberlain G., 1992, Comment: Sequential moment restrictions in panel data, Journal of Business & Economic Statistics 10, 20­26.
Cincera M., 1997, Patents, R&D, and technological spillovers at the firm level: some evidence from econometric count models for panel data, Journal of Applied Econometrics 12, 265­280.
Cre´pon B. and E. Duguet, 1997, Estimating the innovation function from patent numbers: GMM on count panel data, Journal of Applied Econometrics 12, 243­263.
Greene W.H., 2005, LIMDEP 8.0, Econometric Software, Inc., Plainview, NY, USA. Guggenberger P., 2005, Monte-carlo evidence suggesting a no moment problem of the continuous
updating estimator, Economics Bulletin 3, 1­6. Hall B. and C. Cummins, 2005, TSP 5.0, TSP International, Palo Alto, CA, USA. Hansen L.P., 1982, Large sample properties of generalized method of moments estimators,
Econometrica 50, 1029­1054. Hansen L.P., J. Heaton and A. Yaron, 1996, Finite-sample properties of some alternative GMM
estimators, Journal of Business & Economic Statistics 14, 262­280. Hausman J., B. Hall and Z. Griliches, 1984, Econometric models for count data and an application
to the patents-R&D relationship, Econometrica 52, 909­938. Kelly M., 2000, Inequality and crime, The Review of Economics and Statistics 82, 530­539. Kitazawa Y., 2000, TSP procedures for count panel data estimation, Kyushu Sangyo University. Lancaster T., 2002, Orthogonal parameters and panel data, Review of Economic Studies 69,
647­666.

624

F. Windmeijer

Kim J. and G. Marschke, 2005, Labor mobility of scientists, technological diffusion and the firm's patenting decision, The RAND Journal of Economics 36, 298­317.
Manning W.G., A. Basu and J. Mullahy, 2005, Generalized modeling approaches to risk adjustment of skewed outcomes data, Journal of Health Economics 24, 465­488.
Montalvo J.G., 1997, GMM estimation of count-panel-data models with fixed effects and predetermined instruments, Journal of Business and Economic Statistics 15, 82­89.
Mullahy J., 1997, Instrumental variable estimation of Poisson regression models: application to models of cigarette smoking behavior, Review of Economics and Statistics 79, 586­593.
Newey W.K. and R.J. Smith, 2004, Higher order properties of GMM and generalized empirical likelihood estimators, Econometrica 72, 219­255.
Newey W.K. and K.D. West, 1987, Hypothesis testing with efficient method of moments estimation, International Economic Review 28, 777­787.
Newey W.K. and F. Windmeijer, 2005, GMM with many weak moment conditions, cemmap Working Paper No. CWP18/05.
Romeu A., 2004, ExpEnd: Gauss code for panel count data models, Journal of Applied Econometrics 19, 429­434.
Salomon R.M. and J.M. Shaver, 2005, Learning by exporting: new insights from examining firm innovation, Journal of Economics and Management Strategy 14, 431­460.
Santos Silva J.M.C. and S. Tenreyro, 2006, The log of gravity, The Review of Economics and Statistics, 88, 641­658.
Schellhorn M., 2001, The effect of variable health insurance deductibles on the demand for physician visits, Health Economics 10, 441­ 456.
Vera-Hernandez, A.M., 1999, Duplicate coverage and demand for health care. The case of Catalonia, Health Economics 8, 579­598.
Windmeijer F., 2000, Moment conditions for fixed effects count data models with endogenous regressors, Economics Letters 68, 21­24.
Windmeijer F., 2002, ExpEnd, a Gauss programme for non-linear GMM estimation of exponential models with endogenous regressors for cross section and panel data, cemmap Working Paper No. CWP14/02.
Windmeijer F., 2005, A finite sample correction for the variance of linear efficient two-step GMM estimators, Journal of Econometrics 126, 25­517.
Windmeijer F. and J.M.C. Santos Silva, 1997, Endogeneity in count data models: an application to demand for health care, Journal of Applied Econometrics 12, 281­294.
Wooldridge J.M., 1991, Multiplicative panel data models without the strict exogeneity assumption, MIT Working Paper No. 574.
Wooldridge J.M., 1997, Multiplicative panel data models without the strict exogeneity assumption, Econometric Theory 13, 667­678.
Wooldridge J.M., 1999, Distribution-free estimation of some nonlinear panel data models, Journal of Econometrics 90, 77­97.

Chapter 19
Spatial Panel Econometrics
Luc Anselin, Julie Le Gallo and Hubert Jayet

19.1 Introduction

Spatial econometrics is a subfield of econometrics that deals with the incorporation of spatial effects in econometric methods (Anselin, 1988a). Spatial effects may result from spatial dependence, a special case of cross-sectional dependence, or from spatial heterogeneity, a special case of cross-sectional heterogeneity. The distinction is that the structure of the dependence can somehow be related to location and distance, both in a geographic space as well as in a more general economic or social network space. Originally, most of the work in spatial econometrics was inspired by research questions arising in regional science and economic geography (early reviews can be found in, among others, Paelinck and Klaassen, 1979; Cliff and Ord, 1981; Upton and Fingleton, 1985; Anselin, 1988a; Haining, 1990; Anselin and Florax, 1995). However, more recently, spatial (and social) interaction has increasingly received more attention in mainstream econometrics as well, both from a theoretical as well as from an applied perspective (see the recent reviews and extensive references in Anselin and Bera, 1998; Anselin, 2001b, 2002; Florax and Van Der Vlist, 2003; and Anselin et al., 2004).
The central focus in spatial econometrics to date has been the single equation cross-sectional setting. However, as Arrelano argues in the introduction to his recent panel data econometrics text, "the field [of econometrics of panel data] has expanded to cover almost any aspect of econometrics" (Arellano, 2003, p. 2). It is therefore not surprising that this has included spatial econometrics as well. For

Luc Anselin School of Geographical Sciences, Arizona State University, Tempe, AZ 85287, USA, e-mail: luc.anselin@asu.edu
Julie Le Gallo CRESE, Universite´ de Franche-Comte´, 45D Avenue de l'Observatoire, 25030 Besanc¸on Cedex, France, e-mail: jlegallo@univ-fcomte.fr
Hubert Jayet EQUIPPE, University of Science and Technology of Lille, Faculty of Economics and Social Sciences, 59655 Villeneuve d'Ascq Cedex, France, e-mail: Hubert.Jayet@univ-lille1.fr

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

625

c Springer-Verlag Berlin Heidelberg 2008

626

L. Anselin et al.

example, the second edition of Baltagi's well known panel data text now includes a brief discussion of spatial panels (Baltagi, 2001, pp. 195­197), and an increasing number of papers are devoted to the topic (see the reviews in Anselin, 2001b; Elhorst, 2001, 2003, as well as the recent papers by Baltagi et al., 2007, 2006; Kapoor et al., 2007; and Pesaran, 2004; among others).
In this chapter, we review and organize this recent literature and emphasize a range of issues pertaining to the specification, estimation and diagnostic testing for spatial effects in panel data models. Since this encompasses a large and rapidly growing literature, we limit our attention to models with continuous dependent variables,1 and to a design where the cross-sectional dimension (N) vastly exceeds the time dimension (N T ). We also avoid duplication by excluding aspects of the standard treatment of heterogeneity and dependence in panel data models, as well as the case where cross-sectional dependence is modeled by relying on the time dimension (e.g., as in the classic SURE case with fixed N, and some more recent extensions, such as Chen and Conley, 2001).
The chapter is organized into five remaining sections. First, we define the notion of spatial effects more precisely and provide a brief outline of how the traditional cross-sectional models can be extended to panel data model specifications. Next, we consider this more closely and develop a taxonomy of space-time models. We then turn to the issues of model estimation and diagnostic testing. We close with some concluding remarks.

19.2 Spatial Effects

As a point of departure, consider a simple pooled linear regression model:

yit = xit  + it ,

(19.1)

where i is an index for the cross-sectional dimension, with i = 1, . . . , N, and t is an index for the time dimension, with t = 1, . . . , T .2 Using customary notation, yit is an observation on the dependent variable at i and t, xit a 1 × K vector of observations on the (exogenous) explanatory variables,  a matching K × 1 vector of regression coefficients, and it an error term.
Given our interest in spatial effects, the observations will be stacked as successive
cross-sections for t = 1, . . . , T , referred to as yt (a N × 1 vector of cross-sectional observations for time period t), Xt (a N ×K matrix of observations on a cross-section of the explanatory variables for time period t) and t (a N × 1 vector of cross-sectional

1 The treatment of spatial effects in panel data models with discrete dependent variables is still in its infancy. 2 Note that we couch the discussion using "time" as the second dimension for the sake of simplicity. In general, it is also possible to have the second dimension reflect another cross-sectional characteristic, such as an industry sector, and. along the same lines, extension to higher order panel structures are possible as well.

19 Spatial Panel Econometrics

627

disturbances for time period t). In stacked form, the simple pooled regression then

becomes:

y = X +,

(19.2)

with y as a NT × 1 vector, X as a NT × K matrix and  as a NT × 1 vector. In general, spatial dependence is present whenever correlation across cross-
sectional units is non-zero, and the pattern of non-zero correlations follows a certain spatial ordering. When little is known about the appropriate spatial ordering, spatial dependence is reduced to simple cross-sectional dependence. For example, the error terms are spatially correlated when E[it  jt ] = 0, for a given t and i = j, and the non-zero covariances conform to a specified neighbor relation. Note how the correlation is purely cross-sectional in that it pertains to the same time period t.
The neighbor relation is expressed by means of a so-called spatial weights matrix. We will briefly review the concept of spatial weights (and the associated spatial lag operator) and outline two classes of specifications for models with spatial dependence. In one, the spatial correlation pertains to the dependent variable, in a so-called spatial lag model, in the other it affects the error term, a so-called spatial error model. The two specifications can also be combined, resulting in so-called higher order spatial models. While these models and terms are by now fairly familiar in the spatial econometric literature, we thought it useful to briefly review them and to illustrate how they may be incorporated into a panel data setting.3
The second class of spatial effects, spatial heterogeneity, is a special case of the observed and unobserved heterogeneity which is treated prominently in the mainstream panel data econometrics literature. For example, a heterogeneous panel would relax the constant regression coefficient in (19.1), and replace it by:

yit = xit i + it ,
where the i is a K ×1 vector of regression coefficients specific to the cross-sectional unit i.
This heterogeneity becomes spatial when there is a structure to the variability across the i that is driven by spatial variables, such as location, distance or region. In the spatial literature, discrete spatial variability is referred to as spatial regimes (Anselin, 1988a). The continuous case can be modeled as a special form of random coefficient variation (where the covariance shows a spatial pattern), or deterministically, as a function of extraneous variables (so-called spatial expansion, e.g., Casetti, 1997), or as a special case of local regression models (so-called geographically weighted regression, Fotheringham et al., 2002). Neither of these has seen application in panel data contexts.4

3 A more extensive technical review can be found in Anselin and Bera (1998). 4 In the literature of spatial statistics, spatially varying coefficients are treated in (Bayesian) hierarchical models (Gelfand et al., 2003; Gamerman et al., 2003).

628

L. Anselin et al.

Since most econometric aspects of spatial heterogeneity can be handled by means of the standard panel data methods, we will focus the discussion that follows on spatial dependence and will only consider the heterogeneity when it is relevant to the modeling of the dependence.

19.2.1 Spatial Weights and Spatial Lag Operator
A spatial weights matrix W is a N ×N positive matrix in which the rows and columns correspond to the cross-sectional observations. An element wi j of the matrix expresses the prior strength of the interaction between location i (in the row of the matrix) and location j (column). This can be interpreted as the presence and strength of a link between nodes (the observations) in a network representation that matches the spatial weights structure. In the simplest case, the weights matrix is binary, with wi j = 1 when i and j are neighbors, and wi j = 0 when they are not. By convention, the diagonal elements wii = 0. For computational simplicity and to aid in the interpretation of the spatial variables, the weights are almost always standardized such that the elements in each row sum to 1, or, wisj = wi j/  j wi j.5 A side effect of this standardization is that the sum of all elements in W equals N, the number of cross-sectional observations. Whereas the original weights are often symmetric, the row-standardized form is no longer, which is an unusual complication with significant computational consequences.
The specification of the spatial weights is an important problem in applied spatial econometrics.6 Unless the weights are based on a formal theoretical model for social or spatial interaction, their specification is often ad hoc. In practice, the choice is typically driven by geographic criteria, such as contiguity (sharing a common border) or distance, including nearest neighbor distance (for examples and further discussion, see, e.g., Cliff and Ord, 1981, pp. 17­19; Anselin, 1988a, Chap. 3).
Generalizations that incorporate notions of "economic" distance are increasingly used as well (e.g., Case et al., 1993; Conley and Ligon, 2002; Conley and Topa, 2002). A slightly different type of economic weights are so-called block weights, where all observations in the same region are considered to be neighbors (and not only the adjoining observations). More formally, if there are Ng units in a block (such as counties in a state), they are all considered to be neighbors, and the spatial weights equal 1/(Ng - 1) for all observations belonging to the same block (see, e.g., Case, 1991, 1992; and, more recently, Lee, 2002).
So far, the weights considered were purely cross-sectional. To extend their use in a panel data setting, they are assumed to remain constant over time.7
5 In what follows, we will use the symbol W for the spatial weights and assume rowstandardization. 6 An extensive discussion of spatial weights is outside the scope of this chapter. For a detailed assessment of technical issues, see the recent review papers by Anselin and Bera (1998), and Anselin (2002). 7 Since the spatial weights enter into a model premultiplied by a scalar parameter, changes in the interaction structure over time can be accounted for by allowing this parameter to vary. Alternatively,

19 Spatial Panel Econometrics

629

Using the subscript to designate the matrix dimension, with WN as the weights for the cross-sectional dimension, and the observations stacked as in (19.2), the full NT × NT weights matrix then becomes:

WNT = IT WN ,

(19.3)

with IT as an identity matrix of dimension T . Unlike the time series case, where "neighboring" observations are directly
incorporated into a model specification through a shift operator (e.g., t - 1), this is not unambiguous in a two dimensional spatial setting. For example, observations for irregular spatial units, such as counties or census tracts, typically do not have the same number of neighbors, so that a spatial shift operator cannot be implemented. Instead, in spatial econometrics, the neighboring observations are included through a so-called spatial lag operator, more akin to a distributed lag than a shift (Anselin, 1988a). In essence, a spatial lag operator constructs a new variable that consists of the weighted average of the neighboring observations, with the weights as specified in W . More formally, for a cross-sectional observation i for variable z, the spatial lag would be  j wi jz j. In most applications, the bulk of the row elements in wi j are zero (resulting in a sparse structure for W ) so that in effect the summation over j only incorporates the "neighbors," i.e., those observations for which wi j = 0. In matrix notation, this corresponds to the matrix operation WNyt , in which the N × N cross-sectional weights matrix is postmultiplied by a N × 1 vector of cross-sectional observations for each time period t = 1, . . . , T .
Spatial variables are included into a model specification by applying a spatial lag operator to the dependent variable, to the explanatory variables, or to the error term. A wide range of models for local and global spatial externalities can be specified in this manner (for a review, see Anselin, 2003). This extends in a straightforward manner to the panel data setting, by applying the NT × NT weights from (19.3) to the stacked y, X or  from (19.2).
More precisely, in the same notation as above, a vector of spatially lagged dependent variables follows as:

Wy = WNT y = (IT WN)y,

(19.4)

a matrix of spatially lagged explanatory variables as:

W X = WNT X = (IT WN)X ,

and a vector of spatially lagged error terms as:

W  = WNT  = (IT WN) .

but less tractable, would be to let the weights vary and keep the parameter constant. Obviously, letting both parameter and weights vary over time would lead to problems with identification and interpretation (for example, see Druska and Horrace, 2004).

630

L. Anselin et al.

The incorporation of these spatial lags into a regression specification is considered next.

19.2.2 Spatial Lag Model

A spatial lag model, or, mixed regressive spatial autoregressive model, includes a spatially lagged dependent variable on the RHS of the regression specification (Anselin, 1988a). While usually applied in a pure cross-sectional setting, it can easily be extended to panel models. Using the stacked (19.2) and the expression for the spatial lag from (19.4), this yields:

y = (IT WN)y + X +  ,

(19.5)

where  is the spatial autoregressive parameter, and the other notation is as before. In a cross-section, a spatial lag model is typically considered as the formal spec-
ification for the equilibrium outcome of a spatial or social interaction process, in which the value of the dependent variable for one agent is jointly determined with that of the neighboring agents.8 This model is increasingly applied in the recent literature on social/spatial interaction, and is used to obtain empirical estimates for the parameters of a spatial reaction function (Brueckner, 2003) or social multiplier (Glaeser et al., 2002). It should be noted that other formulations to take into account social interaction have been suggested as well (e.g., Manski, 2000; Brock and Durlauf, 2001) mostly in the context of discrete choice. The modeling of complex neighborhood and network effects (e.g., Topa, 2001) requires considerable attention to identification issues, maybe best known from the work of Manski on the "reflection problem" (Manski, 1993). Because of this theoretical foundation, the choice of the weights in a spatial lag model is very important.
At first sight, the extension of the spatial lag model to a panel data context would presume that the equilibrium process at hand is stable over time (constant  and constant W ). However, the inclusion of the time dimension allows much more flexible specifications, as outlined in Sect. 19.3.
The essential econometric problem in the estimation of (19.5) is that, unlike the time series case, the spatial lag term is endogenous. This is the result of the twodirectionality of the neighbor relation in space ("I am my neighbor's neighbor") in contrast to the one-directionality in time dependence (for details, see Anselin and Bera, 1998). The consequence is a so-called spatial multiplier (Anselin, 2003) which formally specifies how the joint determination of the values of the dependent

8 In spatial statistics, the preferred perspective is that of a conditional process, which is geared to spatial prediction (see Cressie, 1993; Stein, 1999; and for a discussion of the implications in a spatial econometric context, Anselin, 2002). Rather than specifying the joint distribution of all the yi in the system, each yi is modeled conditional upon the y j for the neighbors. For detailed discussion, see the previous references.

19 Spatial Panel Econometrics

631

variables in the spatial system is a function of the explanatory variables and error terms at all locations in the system.
The extent of the joint determination of values in the system can be seen by expressing (19.5) as a reduced form:

y = IT  (IN - WN )-1 X  + IT  (IN - WN )-1  ,

(19.6)

with the subscripts indicating the dimensions of the matrices. The inverse matrix expression can be expanded and considered one cross-section at a time, due to the block-diagonal structure of the inverse. A a result, for each N × 1 cross-section at time t = 1, . . . , T :
yt = Xt  + WNXt  + 2WN2Xt  + . . . + t + WNt + 2WN2t . . .
The implication of this reduced form is that the spatial distribution of the yit in each cross-section is determined not only by the explanatory variables and associated regression coefficients at each location (Xt ), but also by those at neighboring locations, albeit subject to a distance decay effect (the increasing powers of  and WN). In addition, the unobserved factors contained in the error term are not only relevant for the location itself, but also for the neighboring locations (WN), again, subject to a distance decay effect. Note that in the simple pooled model, this spatial multiplier effect is contained within each cross-section and does not spill over into other time periods.9
The presence of the spatially lagged errors in the reduced form illustrates the joint dependence of the WNyt and t in each cross-section. In model estimation, this simultaneity must be accounted for through instrumentation (IV and GMM estimation) or by specifying a complete distributional model (maximum likelihood estimation).
Even without a solid theoretical foundation as a model for social/spatial interaction, a spatial lag specification may be warranted to spatially detrend the data. This is referred to as a spatial filter:

[IT  (IN - WN)] y = X + ,

(19.7)

with the LHS as a new dependent variable from which the effect of spatial autocor-
relation has been eliminated. In contrast to time series, a simple detrending using  = 1 is not possible, since that value of  is not in the allowable parameter space.10 As a consequence, the parameter  must be estimated in order for the spatial filtering to be operational (see Anselin, 2002).

9 This can be relaxed in more flexible space-time models, see, for example, Sect. 19.3.3.1. 10 For row-standardized weights,  = 1 violates a standard regularity condition for spatial models that requires that the inverse (I - WN )-1 exists (Kelejian and Prucha, 1999).

632
19.2.3 Spatial Error Model

L. Anselin et al.

In contrast to the spatial lag model, a spatial error specification does not require a theoretical model for spatial/social interaction, but, instead, is a special case of a non-spherical error covariance matrix. An unconstrained error covariance matrix at time t, E[it  jt ],  i = j contains N × (N - 1)/2 parameters. These are only estimable for small N and large T , and provided they remain constant over the time dimension. In the panel data setting considered here, with N T , structure must be imposed in order to turn the covariance matrix into a function of a manageable set of parameters.
Four main approaches have been suggested to provide the basis for a parsimonious covariance structure: direct representation, spatial error processes, spatial error components, and common factor models. Each will be reviewed briefly.

19.2.3.1 Direct Representation

The direct representation approach has its roots in the geostatistical literature and the use of theoretical variogram and covariogram models (Cressie, 1993). It consists of specifying the covariance between two observations as a direct function of the distance that separates them,  i = j and t = 1, . . . , T :

E[it  jt ] =  2 f (, di j),

(19.8)

where  is a parameter vector, di j is the (possibly economic) distance between observation pairs i, j,  2 is a scalar variance term, and f is a suitable distance decay function, such as a negative exponential.11 The parameter space for  should be such that the combination of functional form and the distance metric ensures that the resulting covariance matrix is positive definite (for further discussion, see, e.g, Dubin, 1988).
An extension to a panel data setting is straightforward. With  2t,N as the error covariance matrix that results from applying the function (19.8) to the N × 1 cross-sectional error vector in time period t, the overall NT × NT error variancecovariance matrix NT becomes a block diagonal matrix with the N × N variance matrix for each cross-section on the diagonal.12 However, as specified, the function (19.8) does not vary over time, so that the result can be expressed concisely as:
NT =  2 [IT  N ] ,
with t,N = N t.13
11 For the sake of simplicity, we use a homoskedastic model with constant variance across all time periods. This restriction can be readily relaxed. Similarly, the assumption of isotropy (only distance matters, not direction) may be relaxed by including separate functions to account for directional effects. 12 In the notation that follows, we use the subscripts T , N and NT to refer to the dimension of the matrix, and the subscript t to refer to the cross-section at time t. 13 Note that this simplification only holds in the strictly homogeneous case with t2 =  2 t.

19 Spatial Panel Econometrics

633

19.2.3.2 Spatial Error Processes

Whereas the direct representation approach requires a distance metric and functional form for the distance decay between a pair of observations, spatial error processes are based on a formal relation between a location and its neighbors, using a spatial weights matrix. The error covariance structure can then be derived for each specified process, but typically the range of neighbors specified in the model is different from the range of spatial dependence in the covariance matrix. This important aspect is sometimes overlooked in empirical applications.
In analogy to time series analysis, the two most commonly used models for spatial processes are the autoregressive and the moving average (for extensive technical discussion, see Anselin, 1988a; Anselin and Bera, 1998; Anselin, 2003, and the references cited therein).
A spatial autoregressive (SAR) specification for the N × 1 error vector t in period t = 1, . . . , T , can be expressed as:

t = WN t + ut ,

where WN is a N × N spatial weights matrix (with the subscript indicating the dimension),  is the spatial autoregressive parameter, and ut is a N × 1 idiosyncratic error vector, assumed to be distributed independently across the cross-sectional dimension, with constant variance u2.
Continuing in matrix notation for the cross-section at time t, it follows that:
t = (IN - WN )-1 ut ,

and hence the error covariance matrix for the cross-section at time t becomes: t,N = E[t t ] = u2 (IN - WN )-1 IN - WN -1 ,
or, in a simpler notation, with BN = IN - WN: t,N = u2(BN BN )-1 .

As before, in this homogeneous case, the cross-sectional covariance does not vary over time, so that the full NT × NT covariance matrix follows as:

NT = u2 IT  (BN BN )-1 .

(19.9)

Note that for a row-standardized weights matrix, BN will not be symmetric. Also, even though WN may be sparse, the inverse term (BNBN)-1 will not be sparse and suggests a much wider range of spatial covariance than specified by the non-zero
elements of the weights matrix. In other words, the spatial covariance structure in-
duced by the SAR model is global. A spatial moving average (SMA) specification for the N × 1 error vector t in
period t = 1, . . . , T , can be expressed as:

634

L. Anselin et al.

t = WN ut + ut ,
where  is the moving average parameter, and the other notation is as before. In contrast to the SAR model, the variance covariance matrix for an error SMA process does not involve a matrix inverse:

t,N = E[t t ] = u2 IN + (WN +WN ) + 2WNWN ,

(19.10)

and, in the homogenous case, the overall error covariance matrix follows directly as:
NT = u2 IT  IN + (WN +WN ) + 2WNWN .
Aso, in contrast to the SAR model, the spatial covariance induced by the SMA model is local.14

19.2.3.3 Spatial Error Components
A spatial error components specification (SEC) was suggested by Kelejian and Robinson as an alternative to the SAR and SMA models (Kelejian and Robinson, 1995; Anselin and Moreno, 2003). In the SEC model, the error term is decomposed into a local and a spillover effect.
In a panel data setting, the N × 1 error vector t for each time period t = 1, . . . , T , is expressed as:

t = WN t + t ,

(19.11)

where WN is the weights matrix, t is a N × 1 vector of local error components, and t is a N × 1 vector of spillover error components. The two component vectors are assumed to consist of i.i.d terms, with respective variances 2 and 2, and are uncorrelated, E[it  jt ] = 0,  i, j,t.
The resulting N × N cross-sectional error covariance matrix is then, for t =
1, . . . , T :

t,N = E[t t ] = 2 WNWN + 2IN .

(19.12)

In the homogeneous model, this is again unchanging across time periods, and the overall NT × NT error covariance matrix can be expressed as:

14 For example, with WN specified as first order contiguity, the spatial covariance in (19.10) only includes first and second order neighbors.

19 Spatial Panel Econometrics

635

NT = 2INT + 2 (IT  WNWN ) .

Comparing (19.10 and 19.12), it can be readily seen that the range of covariance induced by the SEC model is a subset that of the SMA model, and hence also a case of local spatial externalities.

19.2.3.4 Common Factor Models

In the standard two-way error component regression model, each observational unit contains an unobserved error component due to individual heterogeneity and one due to a time period effect, in addition to the usual idiosyncratic error term (e.g., Baltagi, 2001, p. 31). In our notation:
it = i + t + uit ,
with i as the cross-sectional component, with variance 2, t as the time component, with variance 2, and uit as an idiosyncratic error term, assumed to be i.i.d with variance u2. The three random components are assumed to be zero mean and to be uncorrelated with each other. The random components i are assumed to be uncorrelated across cross-sectional units, and the components t are assumed to be uncorrelated across time periods. This model is standard, except that for our purposes, the data are stacked as cross-sections for different time periods. Consequently, the N × 1 cross-sectional error vector t for time period t = 1, . . . , T , becomes:

t =  + t N + ut ,

(19.13)

where  is a N × 1 vector of cross-sectional error components i, t is a scalar time component, N is a N × 1 vector of ones, and ut is a N × 1 vector of idiosyncratic errors.
The structure in (19.13) results in a particular form of cross-sectional (spatial)
correlation, due to the common time component:

E[t t ] = 2IN + 2N N + u2IN ,
where the subscript N indicates the dimension of the identity matrices. Note that the second term in this expression indicates equicorrelation in the cross-sectional dimension, i.e., the correlation between two cross-sectional units i, j equals 2, no matter how far these units are apart. While perfectly valid as a model for general (global) cross-sectional correlation, this violates the distance decay effect that underlies spatial interaction theory.
The complete NT × 1 error vector can be written as (see also Anselin, 1988a, p. 153):
 = (T  IN) + (IT  N) + u ,

636

L. Anselin et al.

where the subscripts indicate the dimensions,  is a T × 1 vector of time error components, u is a NT × 1 vector of idiosyncratic errors, and the other notation is as before. The overal error variance covariance matrix then follows as:

NT = 2(T T  IN ) + 2(IT  N N ) + u2INT .
Note the how the order of matrices in the Kronecker products differs from the standard textbook notation, due to the stacking by cross-section.
A recent extension of the error component model can be found in the literature on heterogeneous panels. Here, the time component is generalized and expressed in the form of an unobserved common effect or factor ft to which all cross-sectional units are exposed. However, unlike the standard error component model, each crosssectional unit has a distinct factor loading on this factor. The simplest form is the so-called one factor structure, where the error term is specified as:

it = i ft + uit ,

with i as the cross-sectional-specific loading on factor ft , and uit as an i.i.d zero mean error term. Consequently, cross-sectional (spatial) covariance between the

errors at i and j follows from the the inclusion of the common factor ft in both error terms:

E[it  jt ]

=

i

j



2 f

.

The common factor model has been extended to include multiple factors. In these specifications, a wide range of covariance structures can be expressed by including sufficient factors and through cross-sectional differences among the factor loadings (for further details, see Driscoll and Kraay, 1998; Pesaran, 2002; and Hsiao and Pesaran, 2008).

19.3 A Taxonomy of Spatial Panel Model Specifications
So far, we have considered the introduction of spatial effects for panel data in the form of spatial lag or spatial error models under extreme homogeneity. The point of departure was the pooled specification, (19.1), and lag and error models are obtained as outlined in Sects. 19.2.2 and 19.2.3. We now extend this taxonomy by introducing heterogeneity, both over time and across space, as well as by considering joint spacetime dependence.
It should be noted that a large number of combinations of space-time heterogeneity and dependence are possible, although many of those suffer from identification problems and/or are not estimable in practice. In our classification here, we purposely limit the discussion to models that have seen some empirical applications (other, more extensive typologies can be found in Anselin, 1988a, Chap. 4; Anselin, 2001b; Elhorst, 2001, 2003).

19 Spatial Panel Econometrics

637

19.3.1 Temporal Heterogeneity

19.3.1.1 General Case

Temporal heterogeneity is introduced in the familiar way in fixed effects models, by allowing time-specific intercepts and/or slopes, and in random effects models, by incorporating a random time component or factor (see Sect. 19.2.3.4). The addition of a spatially lagged dependent variable or spatially correlated error term in these models is straightforward. For example, consider a pooled model with timespecific intercept and slope coefficient to which a spatially autoregressive error term is added. The cross-section in each period t = 1, . . . , T , is:

yt = t + Xt t + t ,

(19.14)

with
t = tWN t + ut
where t is a period-specific spatial autoregressive parameter, t is the periodspecific intercept and t a (K - 1) × 1 vector of period-specific slopes. Since T is fixed (and the asymptotics are based on N  ), this model is a straightforward replication of T cross-sectional models. A spatial lag specification is obtained in a similar way.

19.3.1.2 Spatial Seemingly Unrelated Regressions

A generalization of the fixed effects model that has received some attention in the

empirical literature (e.g., Rey and Montouri, 1999) allows the cross-sectional error

terms t to be correlated over time periods. This imposes very little structure on the

form of the temporal dependence and is the spatial counterpart of the classic SURE

model. It is referred to as the spatial SUR model (see Anselin, 1988a, Chap. 10;

and Anselin, 1988b). In matrix form, the equation for the cross-sectional regression

in each time period t = 1, . . . , T , is as in (19.14), but now with the constant term

included in the vector t :

yt = Xt t + t ,

(19.15)

with the cross-equation (temporal) correlation in general form, as:

E[t s] = tsIN , s = t ,

where ts is the temporal covariance between s and t (by convention, the variance terms are expressed as t2). In stacked form (T cross-sections), the model is:

y = X +,

(19.16)

with

638

L. Anselin et al.

E[ ] = T  IN

(19.17)

and T is the T × T temporal covariance matrix with elements ts. Spatial correlation can be introduced as a spatial lag specification or a spatial error specification. Consider the spatial lag model first (see Anselin, 1988a for details). In each cross-section (with t = 1, . . . , T ), the standard spatial lag specification holds, but now with a time-specific spatial autoregressive coefficient t :

yt = tWN yt + Xt t + t .

To consider the full system, let  be a T K × 1 vector of the stacked time-specific t , for t = 1, . . . , T .15 The corresponding NT × KT matrix X of observations on the
explanatory variables then takes the form:





X1 0 . . . 0

X = .0. .

X2 ...

... ...

.0. . .

(19.18)

0 0 . . . XT

Also, let the spatial autoregressive coefficients be grouped in a T × T diagonal ma-

trix RT , as:





1 0 . . . 0

RT = .0. .

2 ...

... ...

.0. . .

0 0 . . . T

The full system can then be expressed concisely as:

y = (RT WN)y + X + ,

(19.19)

with the error covariance matrix as in (19.17). In empirical practice, besides the standard hypothesis tests on diagonality of the
error covariance matrix and stability of the regression coefficients over time, interest in the spatial lag SUR model will center on testing the hypothesis of homogeneity of the spatial autoregressive coefficients, or, H0 : 1 = 2 = . . . = T = . If this null hypothesis can be maintained, a simplified model can be implemented:

y = (IT WN)y + X +  .

Spatial error autocorrelation can be introduced in the spatial SUR model in the form of a SAR or SMA process for the error terms (see Anselin, 1988a, Chap. 10). For example, consider the following SAR error process for the cross-section in each time period t = 1, . . . , T :

t = tWN t + ut .

(19.20)

15 We are assuming the same number of explanatory variables (K) in each equation, but this can be readily generalized to allow the number of explanatory variables to vary by time period.

19 Spatial Panel Econometrics

639

The cross-equation covariance is introduced through the remainder error term ut, for which it is assumed that E[ut ] = 0, E[ut ut ] = t2IN, and E[ut us] = tsIN, for t = s. As a result, the covariance matrix for the stacked NT × 1 error vector u becomes the counterpart of (19.17):
E[uu ] = T  IN ,

with, as before, T as a T × T matrix with elements ts. The SAR error process in (19.20) can also be written as:

t = (IN - tWN )-1 ut ,

or, using the simplifying notation Bt,N = (IN - tWN), as:

t = Bt-,N1 ut .

The overall cross-equation covariance between error vectors t and s then becomes:

E[t s] = Bt-,N1 E[ut us]Bs-,N1 = ts Bt-,N1 Bs-,N1 ,

which illustrates how the simple SUR structure induces space-time covariance as well (the Bt-,N1 matrices are not diagonal).
In stacked form, the error process for the NT × 1 error vector  can be written as:

 = B-NT1 u ,

with BNT as the matrix:

BNT = [INT - (T WN )],

(19.21)

and T as a T × T diagonal matrix containing the spatial autoregressive coefficients

t,t = 1, . . . , T . The overall error covariance matrix for the stacked equations then

becomes:

E[ ] = B-NT1 (T  IN )B-NT1 .

(19.22)

As in the spatial lag SUR model, specific interest in the spatial error SUR model
centers on the homogeneity of the spatial autoregressive parameters, H0 : 1 = 2 = . . . = T =  . If the homogeneity holds, the expression for BNT (19.21) simplifies to:

BNT = [IT  (IN - WN)] .

19.3.2 Spatial Heterogeneity
We limit our attention in the treatment of spatial heterogeneity to models with unobserved heterogeneity, specified in the usual manner as either fixed effects or random effects. Both have been extended with spatial lag and spatial error specifications.

640
19.3.2.1 Fixed Effects Models

L. Anselin et al.

The classic fixed effects model (e.g., Baltagi, 2001, pp. 12­15; and Arellano, 2003, pp. 11­18) includes an individual specific "dummy variable" to capture unobserved heterogeneity. For each observation i,t this yields, keeping the same notation as before:

yi,t = i + xit  + it
for i = 1, . . . , N, t = 1, . . . , T , and with an additional constraint of the form i i = 0, such that the individual effects i are separately identifiable from the constant term in  .
As is well known, consistent estimation of the individual fixed effects is not possible when N  , due to the incidental parameter problem. Since spatial models rely on the asymptotics in the cross-sectional dimension to obtain consistency and asymptotic normality of estimators, this would preclude the fixed effects model from being extended with a spatial lag or spatial error term (Anselin, 2001b).
Nevertheless, it has been argued that when the interest is primarily in obtaining consistent estimates for the  coefficients, the use of demeaned spatial regression models may be appropriate, for example, using the standard maximum likelihood estimation expressions (Elhorst, 2003, p. 250­251).
There are a number of aspects of this approach that warrant closer attention. One is that the demeaning operator takes on a different form from the usual expression in the literature, since the observations are stacked as cross-sections for different time periods. Also, the demeaned models no longer contain a constant term, which may be incompatible with assumptions made by standard spatial econometric software. More importantly, the variance covariance matrix of the demeaned error terms is no longer 2I, but becomes 2Q, where Q is the demeaning operator (this aspect is ignored in the likelihood functions presented in Elhorst, 2003, p. 250).
To illustrate these points, consider a fixed effects spatial lag model in stacked form, using the same setup as in (19.5), with the addition of the fixed effects:

y = (IT WN)y + (T  ) + X + ,

(19.23)

where  is a N × 1 vector of individual fixed effects, with the constraint that  N = 0, and, as before, E[ ] = 2INT . Note the difference with the classic formulation in the Kronecker product for the fixed effects, due to the stacking of

cross-sections, rather than individual time series.

The demeaned form of (19.23) is obtained by substracting the average for

each cross-sectional unit computed over the time dimension, which wipes out

the individual fixed effects (as well as the constant term). Formally, this can be

expressed as:

QNT y = (IT WN )QNT y + QNT X  + QNT ,

(19.24)

where QNT is the demeaning operator (and QNT X and  no longer contain a constant term). The demeaning operator is a NT × NT matrix that takes the form:

19 Spatial Panel Econometrics

641

QNT = INT - (T T /T  IN )

with, as before,  as a vector of ones and the subscripts denoting the dimension of vectors and matrices. Again, note the difference with the standard textbook notation, due to stacking by cross-section. The matrix QNT is idempotent, and, as a result, the variance of the error in (19.24) becomes:

E[ ] = 2QNT ,

where QNT is singular. Consequently, |QNT | = 0 and the regular inverse of the matrix QNT does not exist. In the non-spatial case, this problem disappears because of the properties of the generalized inverse Q- (see, e.g., Hsiao, 1986, p. 222). In the
absence of spatial variables, the regression in demeaned X and y can be treated as a special case of GLS estimation, with Q- as the generalized inverse, such that QQ-Q = Q. As a result ^ = (X Q Q-QX)-1X Q Q-Qy = (X QX)-1X Qy, which no longer involves the generalized inverse in the actual calculations.
However, the log-likelihood for the spatial lag model with demeaned variables is based on the multivariate normality of the error term . In this case, the singularity of QNT constitutes a problem since the (joint unconditional) likelihood becomes degenerate.16 In the non-spatial literature on dynamic panels with individual fixed
effects, this problem is avoided by considering a transformed likelihood function
based on first differences of the variables (see Hsiao et al., 2002). The extension of
this idea to spatial models remains to be considered.
A likelihood approach to the spatial error model faces a similar complication.

19.3.2.2 Random Effects Models
In the random effects approach to modeling unobserved heterogeneity, interest has centered on incorporating spatial error correlation into the regression error term, in addition to the standard cross-sectional random component. Note that the latter induces serial correlation over time (of the equi-correlated type). Here, we focus attention on the one-way error component specification (e.g, Baltagi, 2001, pp. 15­20; Arellano, 2003, Chap. 3).17
In contrast to the fixed effects case, asymptotics along the cross-sectional dimension (with N  ) present no problem for random effects models. The standard specification of the error term in this model, is, for each i,t:
it = i + it ,

16 In Elhorst, 2003 (p. 250), the log-likelihood uses  2I as the error variance, not  2QNT . 17 Explicit space-time dependence is treated in Sect. 19.3.3.

642

L. Anselin et al.

where i IID(0, 2) is the cross-sectional random component, and it IID(0, 2) is an idiosyncratic error term, with i and it independent from each other. In each cross-section, for t = 1, . . . , T , the N × 1 error vector t becomes:

t =  + t ,

(19.25)

where  is a N × 1 vector of cross-sectional random components. Spatial error autocorrelation can be introduced into this expression in a number
of different ways. A first approach follows the analogue from the time domain and specifies a SAR process for the error component t, for t = 1, . . . , T (Anselin, 1988a, p. 153; and, more recently, Baltagi et al., 2003):

t = WN t + ut ,

(19.26)

with  as the spatial autoregressive parameter (constant over time), WN as the spatial weights matrix, and ut as an i.i.d idiosyncratic error term with variance u2.
Using the notation BN = IN - WN, we obtain the familiar result:

t = (IN - WN )-1ut = B-N1ut .

This illustrates how the spatial autocorrelation pertains to a cross-section in each

time period (the error vector ut ) separately. In stacked form, the NT × 1 error term

then becomes:

 = (T  IN ) + (IT  B-N1)u,

(19.27)

where u IID(0, u2INT ) is a NT × 1 vector of idiosyncratic errors. The corresponding variance­covariance matrix for  follows as:

NT = E[ ] = 2(T T  IN ) + u2[IT  (BN BN )-1] .

(19.28)

Note that the first component induces correlation in the time dimension, but not in the cross-sectional dimension, whereas the opposite holds for the second component (correlation only in the cross-sectional dimension).
A second specification for spatial correlation in this model applies the SAR process first and the error components specification to its remainder error (Kapoor et al., 2007). Consider a SAR process for the NT × 1 error vector :

 =  (IT WN) +  ,

or, using similar notation as the spatially correlated component in (19.27):
 = (IT  B-N1) .
Now, the innovation vector  is specified as a one way error component model (Kapoor et al., 2007):
 = (T  IN) + u ,

19 Spatial Panel Econometrics

643

with  as the N × 1 vector of cross-sectional random components, and u IID(0, u2INT ). In stacked form, the full error vector follows as:

 = (IT  B-1)[(T  IN) + u] .

The corresponding error variance covariance matrix is:

NT = E[ ] = (IT  B-N1)[2(T T  IN ) + u2INT ](IT  BN-1 ).

(19.29)

Again, this model combines both time-wise as well as cross-sectional correlation, with the latter pertaining to both the time specific error ut as well as the time invariant error component .
Recently, an encompassing specification was suggested (Baltagi et al., 2006) that includes both forms as special cases and provides a useful starting point for a wide range of specification tests. In this model, an explicit distinction is made between permanent and time variant spatial correlation. The former introduces a spatial autoregressive process for the N × 1 error component :

 = 1WN  + u1 ,
or, with A = IN - 1WN,  = A-1u1, where u1 is a N × 1 vector of time invariant idiosyncratic errors with variance u21.
Time variant spatial correlation is included as a spatial autoregressive process for each time-specific error vector t , as:

t = 2WN t + u2t ,
or, with B = IN - 2WN, t = B-1u2t , where u2t is a N × 1 vector of time invariant idiosyncratic errors with variance u22.
The stacked error term then follows as:

 = (T  IN )A-1u1 + (IT  B-1)u2 ,

and u2 consists of the stacked u2t . The corresponding error variance­covariance matrix follows as:

 = u21[T T  (A A)-1] + u22[IT  (B B)-1] .

The first element in this expression contains both time and spatial correlation, whereas the second term only contains spatial correlation.
The two earlier specifications are found by imposing parameter constraints on the encompassing form. More precisely, for 1 = 0, the first model is obtained, and for

644

L. Anselin et al.

1 = 2, the second model follows. When both 1 = 2 = 0, the standard non-spatial random effects model is obtained.
Statistical inference for error components models with spatial SAR processes can be carried out as a special case of models with non-spherical error covariance. This is addressed in Sects. 19.4 and 19.5.

19.3.3 Spatio-Temporal Models
The incorporation of dependence in both time and space dimensions in an econometric specification adds an additional order of difficulty to the identification of the NT × (NT - 1)/2 elements of the variance covariance matrix. An important concept in this regard is the notion of separability. Separability requires that a NT × NT space-time covariance matrix NT can be decomposed into a component due to space and a component due to time (see, e.g., Mardia and Goodall, 1993), or:
NT = T  N ,
where T is a T × T variance covariance matrix for the time-wise dependence and N is a N × N variance covariance matrix for the spatial dependence.18 This ensures that the space-time dependence declines in a multiplicative fashion over the two dimensions. It also addresses a central difficulty in space-time modeling, i.e., the lack of a common "distance" metric that works both in the cross-sectional and the time dimension. The approach taken in spatial panel econometrics is to define "neighbors" in space by means of a spatial weights matrix and "neighbors" in time by means of the customary time lags. However, the speed of the dynamic space-time process may not be compatible with these choices, leading to further misspecification.
19.3.3.1 Model Taxonomy
Ignoring for now any space-time dependence in the error terms, we can distinguish four basic forms to introduce correlation in both space and time in panel data models (following Anselin, 2001b, p. 317­318). As before, we focus on models where N T and do not consider specifications where the time dimension is an important aspect of the model.19 To facilitate exposition, we express these models for a N × 1 cross-section at time t = 1, . . . , T .
18 The notion of separable stationary spatio-temporal processes originates in the geostatistical literature, but can be readily applied to the current framework. Extension to non-separable structures have been suggested in the recent literature (e.g., Cressie and Huang, 1999). 19 In the statistical literature, specifications of space-time dependence are often conceptualized as hierarchical or multilevel models. This is beyond the scope of our current review (see, for example, Waller et al., 1997a, b; Wikle et al., 1998; Banerjee et al., 2004, and the extensive set of references therein).

19 Spatial Panel Econometrics

645

Pure space recursive models, in which the dependence pertains only to neighboring locations in a previous period:

yt = WN yt-1 + Xt  + t ,

(19.30)

with  as the space-time autoregressive parameter, and WNyt-1 as a N × 1 vector of observations on the spatially lagged dependent variable at t - 1. Note that this can be readily extended with time and spatial lags of the explanatory variables, Xt-1 or WNXt . However, since WNyt-1 already includes WNXt-1, adding a term of this form would create identification problems. This is sometimes overlooked in other taxonomies of dynamic space-time models (e.g., in the work of Elhorst, 2001, p. 121, where space-time lags for both the dependent and the exploratory variables are included in the specification).
Consider the space-time multiplier more closely. Start by substituting the equation for yt-1 in (19.30), which yields:

yt = WN [WN yt-2 + Xt-1 + t-1] + Xt  + t ,
or, yt = 2WN2yt-2 + Xt  + WN Xt-1 + t + WN t-1 .

Successive substitution reveals a space-time multiplier that follows from a series of consecutively higher orders of both spatial and time lags applied to the X (and error terms). Also, since the spatial dependence takes one period to manifest itself, this specification becomes quite suitable to study spatial diffusion phenomena (see the early discussion in Upton and Fingleton, 1985; and Dubin, 1995).
Time-space recursive models, in which the dependence relates to both the location itself as well as its neighbors in the previous period:

yt =  yt-1 + WN yt-1 + Xt  + t ,

(19.31)

with  as the serial (time) autoregressive parameter, operating on the cross-section of dependent variables at t - 1. Spatially lagged contemporaneous explanatory variables (WNXt ) may be included as well, but time lagged explanatory variables will result in identification problems. This model has particular appeal in space-time forecasting (e.g., Giacomini and Granger, 2004).
Again, the nature of the space-time multiplier can be assessed by substituting the explicit form for the spatially and time lagged terms:

yt =  [ yt-2 + WN yt-2 + Xt-1 + t-1] +WN [ yt-2 + WN yt-2 + Xt-1 + t-1] +Xt  + t ,

or,

646

L. Anselin et al.

yt = ( 2 + 2 WN + 2WN2)yt-2 +Xt  + ( + WN )Xt-1 +t + ( + WN)t-1 ,

revealing a much more complex form for the effect of space-time lagged explanatory variables (and errors), including the location itself as well as its neighbors.
Time-space simultaneous models, which include a time lag for the location itself together with a contemporaneous spatial lag:

yt =  yt-1 + WN yt + Xt  + t ,

with  as the (contemporaneous) spatial autoregressive parameter. The mulitplier in this model is complex, due to the combined effect of the cross-sectional spatial multiplier (in each period) and the space-time multiplier that follows from the time lag in the dependent variable. First, consider the pure crosssectional multiplier:
yt = (IN - WN )-1[ yt-1 + Xt  + t ] .

Next, substitute the corresponding expression for yt-1:
yt = (IN - WN )-1[ [(IN - WN )-1( yt-2 +Xt-1 + t-1)] + Xt  + t ] ,
which yields:
yt =  2(IN - WN )-2yt-2 +(IN - WN )-1Xt  +  (IN - WN )-2Xt-1 +(IN - WN )-1t +  (IN - WN )-2t-1 .
From this it follows that the inclusion of any spatially lagged X in the original specification will lead to identification problems.
Time-space dynamic models, where all three forms of lags for the dependent variable are included:
yt =  yt-1 + WN yt + WN yt-1 + Xt  + t .
While this model is sometimes suggested as a general space-time specification, it results in complex nonlinear constraints on the parameters, and, in practice, often suffers from identification problems. For example, focusing only on the time lagged terms and substituting their expression for t - 1 (and rearranging terms) yields:

19 Spatial Panel Econometrics

647

yt =  [ yt-2 + WN yt-1 + WN yt-2 + Xt-1 + t-1] +WN [ yt-2 + WN yt-1 + WN yt-2 + Xt-1 + t-1] +WNyt + Xt  + t ,

or, grouping by time period:

yt = WN yt + Xt  + t + WN yt-1 + WN2yt-1 +  Xt-1 + WN Xt-1
+ t-1 + WN t-1 + 2yt-2 + WN yt-2 + 2WN2yt-2 .
The same types of space-time dependence processes can also be specified for the error terms in panel data models (e.g., Fazekas et al., 1994). However, combinations of both spatially lagged dependent variables and spatially lagged error terms may lead to identification problems unless the parameters of the explanatory variables are non-zero. An alternative form of error space-time dependence takes the error components approach, to which we turn briefly.

19.3.3.2 Error Components with Space-Time Dependence

The starting point for including explicit serial dependence (in addition to the equicorrelated form) in random effects models is the spatially autocorrelated form considered in (19.25­19.26). However, instead of the indiosyncratic error ut in 19.26, a serially correlated term t is introduced (Baltagi et al., 2007):

t = WN t + t

(19.32)

with

t =  t-1 + ut ,

(19.33)

where, as before, ut is used to denote the idiosyncratic error, and t = 1, . . . , T . The counterpart of the N × 1 cross-sectional error vector t in (19.27) becomes:

t = (IN - WN )-1t = B-N1t ,

with  replacing the original error u. In stacked form, this becomes:

 = (T  IN) + (IT  BN-1) ,
with  of dimension N × 1 and both  and  of dimension NT × 1. The serial correlation in  will yield serial covariances of the familiar AR(1) form, with:

E[i,t i,t-k] = u2

k 1-2

,

648

L. Anselin et al.

for k = 0, . . . , T - 1, and i = 1, . . . , N, where u2 is the variance of the error term u. Grouping these serial covariances into a T × T variance covariance matrix T
yields the overall variance covariance matrix for  as (Baltagi et al., 2007):

NT = E[ ] = 2(T T  IN ) + [T  (BN BN )-1] .

19.4 Estimation of Spatial Panel Models
The estimation of panel data models that include spatially lagged dependent variables and/or spatially correlated error terms follows as a direct extension of the theory developed for the single cross-section. In the first case, the endogeneity of the spatial lag must be dealt with, in the second, the non-spherical nature of the error variance covariance matrix must be accounted for. Two main approaches have been suggested in the literature, one based on the maximum likelihood principle, the other on method of moments techniques. We consider each in turn.
We limit our attention to models with a parameterized form for the spatial dependence, specified as a spatial autoregressive process.20 Note that some recent results in the panel econometrics literature have also addressed estimation in models with general, unspecified cross-sectional correlation (see, e.g., Driscoll and Kraay, 1998; Coakley et al., 2002; Pesaran, 2002).

19.4.1 Maximum Likelihood Estimation
The theoretical framework for maximum likelihood estimation of spatial models in the single cross-section setup is by now well developed (see, among others, Ord, 1975; Mardia and Marshall, 1984; Anselin, 1988a; Cressie, 1993; Anselin and Bera, 1998). While the regularity conditions are non-standard, and require a consideration of triangular arrays (Kelejian and Prucha, 1999), the results for error terms with a Gaussian distribution are fairly well established.
In practice, estimation consists of applying a non-linear optimization to the loglikelihood function, which (in most circumstances) yields a consistent estimator from the numerical solution to the first order conditions. Asymptotic inference is based on asymptotic normality, with the asymptotic variance matrix derived from the information matrix. This requires the second order partial derivatives of the loglikelihood, for which analytical solutions exist in many of the models considered (for technical details, see the review in Anselin and Bera, 1998).
A main obstacle in the practical implementation of ML estimation in a single cross-section is the need to compute a Jacobian determinant for an N-dimensional matrix (the dimension of the cross-section). In panel data models, this Jacobian is
20 Models with other forms for the error dependence have seen limited attention in a panel data context and are not considered here.

19 Spatial Panel Econometrics

649

of dimension N × T , but it can often be simplified to a product of T N-dimensional determinants. The classic solution to this problem is to decompose the Jacobian in terms of the eigenvalues of the spatial weights matrix. For example, in the spatial lag model, the Jacobian would be |IN - WN| = i(1 - i), with i as the eigenvalues of WN (Ord, 1975).21 For large cross-sections, the computation of the eigenvalues becomes numerically unstable, precluding this method from being applicable. Alternative solutions avoid the computation of the Jacobian determinant, but instead approximate it by a polynomial function or by means of simulation methods (Barry and Pace, 1999). Other methods are based on Cholesky or LU decomposition methods that exploit the sparsity of the spatial weights (Pace and Barry, 1997), or use a characteristic polynomial approach (Smirnov and Anselin, 2001).
We now briefly review a number of useful log-likelihood expressions that result when incorporating spatial lag or spatial error terms in panel data settings. Numerical procedures to carry out estimation and inference can be implemented along the same lines as for the single cross-section, and will not be further elaborated.

19.4.1.1 Spatial Lag Models

As a point of departure, consider the pooled spatial lag model given in (19.5).
Assuming a Gaussian distribution for the error term, with  N(0, 2INT ), the loglikelihood (ignoring the constants) follows as:

L

=

ln |IT

 (IN

-

WN )|

-

NT 2

ln 2

-

1 22





,

with  = y - (IT WN)y - X , and |IT  (IN - WN)| as the Jacobian determinant of the spatial transformation. Given the block diagonal structure of the Jacobian, the log-likelihood further simplifies to:

L

=

T

ln |IN

- WN| -

NT 2

ln 2

-

1 22



,

(19.34)

which boils down to a repetition of the standard cross-sectional model in T crosssections.
Generalizing this model slightly, we now assume  N(0,  ) to allow for more complex error covariance structures (including spatial correlation). The loglikelihood remains essentially the same, except for the new error covariance term:

L

=

T

ln |IN

- WN| -

1 2

ln | |

-

1 2



 -1.

(19.35)

21 In practice, the log Jacobian is used, with ln |IN - WN | = i ln(1 - i).

650

L. Anselin et al.

Two special cases result by introducing some structure into the variance covariance matrix  . First, consider the classic one-way error components model from (19.25), which, in stacked form, becomes (again, using cross-sections for T time periods and assuming a Gaussian distribution for ):

 = (T  IN) + u .

The error covariance matrix follows as:

NT = E[ ] = 2(T T  IN ) + u2INT .

(19.36)

Using standard results, the inverse and determinant of this NT × NT matrix can be expressed in terms of matrix determinants and inverses of orders N and T only. Inserting (19.36 into 19.35) yields the log-likelihood for the spatial lag model with error components as:

L

=

T

ln |IN

- WN| -

1 2

ln |2(T T

 IN ) + u2INT |

-

1 2



2(T T  IN ) + u2INT -1  .

A second specification of interest is the SUR model that includes a spatial lag term,
(19.19). Its log-likelihood can be obtained in a similar fashion. Using the same no-
tation and stacking of observation matrices and parameters as in (19.18­19.19), the log Jacobian follows as ln |INT - (RT  WN)|. The block diagonal structure of the matrix can be exploited to simplify this expression to t ln |IN - tWN| (with the sum over t = 1, . . . , T ). Using (19.17) for the error variance covariance matrix in the
SUR model, the log-likelihood follows as:

 L =

t

ln

|IN

-

tWN |

-

N 2

ln

|T

|

-

1 2



T-1  IN  ,

with  = [INT - (RT  WN)]y - X (for further details, see Anselin, 1988a, pp. 145­146).

19.4.1.2 Spatial Error Models

The log-likelihood functions for the various spatial error models considered in this chapter follow directly as special cases of the standard result for maximum likelihood estimation with a non-spherical error covariance (Magnus, 1978). With  N(0,  ) as the error vector, the familiar expression for the log-likelihood is (ignoring the constant terms):

L = - 1 ln | | - 1   -1 .

2

2

19 Spatial Panel Econometrics

651

In the pooled model with SAR error terms, (19.9), the relevant determinant and inverse matrix are:
|IT  (BN BN )-1| = |BN |-2T
with BN as in (19.9), and:

N-T1

=

1 u2

[IT

 (BN BN )]

.

The corresponding log-likelihood function is then:

L

=

-NT 2

ln u2

+T

ln |BN|

-

1 2u2



[IT



(BNBN )]

,

with  = y - X . The estimates for the regression coefficient  are the result of a spatial FGLS, using a consistent estimator for  :

^ = [X (IT  BN BN )X ]-1X (IT  BN BN )y.

(19.37)

Exploiting the block diagonal nature of BNBN, this is equivalent to a regression of the stacked spatially filtered dependent variables, (IN - WN)yt on the spatially filtered explanatory variables (IN - WN)Xt , as a direct generalization of the single cross-section case.
Two special cases are of particular interest. One is the random effects model
with spatial error correlation. Its error variance covariance matrix, (19.28), can
be simplified in order to facilitate the computation of the determinant and inverse term needed in the log-likelihood. Set  = 2/u2, such that NT = u2NT , with:

NT = T T  IN + [IT  (BN BN )-1] ,

using the same notation and observation stacking as for (19.28). This particular expression allows the determinant and inverse to be obtained as (see Anselin, 1988a, p. 154, for details):

|NT | = |(BN BN )-1 + (T )IN ||BN |-2(T -1)

and,

N-T1

=

T T T

 [(BN BN )-1 + (T )IN ]-1 + (IT

- T T T

)  (BNBN ) .

The log-likelihood thus becomes:

652

L. Anselin et al.

L

=

-NT 2

ln u2

- (T

- 1) ln |BN|

-

1 2

ln

|(BN

BN

)-1

+

(T



)IN

|

-

1 2u2



[ T T T

 [(BN BN )-1 + (T )IN ]-1]

-

1 2u2



[(IT

-

T T T

)



(BNBN )]

,

with  = y - X . A second special case is the the spatial SUR model with spatial SAR error au-
tocorrelation. Its error variance covariance matrix is given by (19.22). The required determinant and inverse for the log-likelihood are (see Anselin, 1988a, p. 143):

|BN-T1 (T  IN )BN-T1 | = |T |N |BNT |-2 ,

and, [B-NT1 (T  IN )B-NT1 ]-1 = BNT [T-1  IN ]BNT .
Furthermore, due to the block-diagonal structure of BNT :

 ln |BNT | = ln |IN - tWN| . t

The log-likelihood for this model then follows as:

L

=

-

N 2

ln

|T

|

+

 t

ln

|IN

-

tWN |

-

1 2



BNT (T-1

 IN )BNT 

,

with BNT  corresponding to the residuals from spatially filtered dependent and explanatory variables, [INT - (T  WN)](y - X ), a generalization of the pooled model case.

19.4.2 Instrumental Variables and GMM
As an alternative to reliance on an often unrealistic assumption of normality and to avoid some of the computational problems associated with the Jacobian term in ML estimation, instrumental variables and GMM methods have been suggested for single cross-section spatial regression models (e.g., Anselin, 1988a, 1990; Kelejian and Robinson, 1993; Kelejian and Prucha, 1998, 1999; Conley, 1999). These can be extended to the panel data setting. We will consider the spatial lag and spatial error models in turn.

19 Spatial Panel Econometrics

653

19.4.2.1 Spatial Lag Models

The endogeneity of the spatially lagged dependent variable suggests a straightforward instrumental variables strategy in which the spatially lagged (exogenous) explanatory variables W X are used as instruments (Kelejian and Robinson, 1993; Kelejian and Prucha, 1998; and also Lee, 2003 for the choice of optimal instruments). This applies directly to the spatial lag in the pooled model, where the instruments would be (IT  WN)X (with X as a stacked NT × (K - 1) matrix, excluding the constant term).
A special case is the spatial SUR model with a spatial lag term, (19.19). Following the same approach as taken in the single cross-section, consider the spatially lagged dependent variable and the explanatory variables in each equation grouped into a matrix Zt = [WNyt Xt ], with parameter vector t = [t t ] . The individual Zt terms can be stacked into a NT × T (K + 1) matrix Z, using the same setup as in (19.18), with a matching stacked coefficient vector . For each equation, construct a matrix of instruments, Ht = [Xt WNXt ], stacked in block-diagonal form into H. With a consistent estimate for the error variance covariance matrix, ^T  IN, the model parameters can be estimated by means of the IV estimator with a general non-spherical error variance (Anselin, 1988a, p. 146):

^ = Z H[H (^T  IN)H]-1H Z -1 Z H[H (^T  IN)H]-1H y

(19.38)

with an estimate for the coefficient variance as: Var[^] = Z H[H (^T  IN)H]-1H Z -1 .

This suggests an iterative spatial three stages least squares estimator (S3SLS): first
estimate each regression using spatial 2SLS (S2SLS); use the S2SLS residuals to obtain a consistent estimate of ^ ; and finally use ^ in (19.38). Consistency and asymptotic normality of the spatial generalized IV estimator can be based on the argu-
ments developed for the cross-sectional S2SLS case (Kelejian and Robinson, 1993;
Kelejian and Prucha, 1998).

19.4.2.2 Spatial Error Models
The spatially weighted least squares result (19.37) for the regression parameters in the pooled model with SAR errors also holds in a more general setting, without assuming normality. As long as a consistent estimator for the nuisance parameter  can be obtained, the FGLS estimator will also be consistent for  .
In the single cross-section, a consistent estimator can be constructed from a set of moment conditions on the error terms, as demonstrated in the Kelejian­Prucha generalized moments (KPGM) estimator (Kelejian and Prucha, 1999). These conditions can be readily extended to the pooled model, by replacing the single equation spatial weights by their pooled counterparts (IT WN). The point of departure is the stacked vector of SAR errors:

654

L. Anselin et al.

 =  (IT WN) + u ,
where both  and u are NT × 1 vectors, and u IID[0, u2INT ]. The three KPGM moment conditions (Kelejian and Prucha, 1999, p. 514) pertain to the idiosyncratic error vector u. Extending them to the pooled setting yields:

E[

1 NT

u

u]

=

u2

1 E[
NT

u

(IT

WN )(IT

W )u]

=

1 N

u2tr(WN

WN

)

1 E[
NT

u

(IT

 WN )u]

=

0

,

where tr is the matrix trace operator and use is made of tr(IT  WNWN) = T trWNWN, and tr(IT WN) = 0.
The estimator is made operational by substituting u =  -  (It  WN), and replacing  by the regression residuals. The result is a system of three equations in  ,  2 and u2, which can be solved by nonlinear least squares (for technical details, see Kelejian and Prucha, 1999). Under some fairly general regularity conditions,
substituting the consistent estimator for  into the spatial FGLS (19.37) will yield a consistent estimator for  . Recently, this approach has been extended to the error
components model with spatial error dependence (19.29), yielding a system of six
moment equations (for details, see Kapoor et al., 2007).

19.5 Testing for Spatial Dependence
Testing for spatial effects in spatial panel models centers on the null hypotheses H0 :  = 0 and/or H0 :  = 0 in the various models that include spatial lag terms or spatial error autocorrelation. Arguably, the preferred approach is based on Lagrange Multiplier (LM) or Rao Score (RS) tests, since these only require estimation of the model under the null, avoiding the complexities associated with ML estimation (for a recent review, see Anselin, 2001a). The test statistics developed for the single cross-section case can be readily extended to the pooled model. In addition, specialized diagnostics have been developed to test for spatial effects in spatial SUR (Anselin, 1988b), and for error components models (Anselin, 1988a; Baltagi et al., 2003, 2006, 2007). More recently, a strategy has been suggested to test for general unspecified cross-sectional dependence (Pesaran, 2004).
We focus our attention on the LM tests and first briefly review the generic case. This is followed by an illustration of applications of the LM principle to tests against error correlation in the spatial SUR and error components models.

19 Spatial Panel Econometrics

655

19.5.1 Lagrange Multiplier Tests for Spatial Lag and Spatial Error Dependence in Pooled Models

The results for the pooled models follow as straightforward generalizations of the
single cross-section case, with proper adjustments for the spatial weights matrix and
weights matrix traces. Consider the pooled regression model (19.2) as the point of departure, with e = y - X^ as a NT × 1 vector of regression residuals.
The single cross-section Lagrange Multiplier test statistic for spatial error correlation, LME (Burridge, 1980), which is asymptotically distributed as 2(1), is readily extended to the pooled model with spatial weights matrix (IT WN) as:

LME

=

[e (IT WN)e/(e e/NT )]2 tr[(IT WN2) + (IT WNWN )]

or, using simplified trace terms:

LME

=

[e

(IT WN)e/(e e/NT )]2 T tr(WN2 +WNWN )

.

Similarly, the single cross-section LM test statistic for a spatial lag alternative, LML (Anselin, 1988a), becomes:

LML

=

[(W y^)

[e (IT WN)y/(e e/NT )]2 M(W y^)/^ 2] + T tr(WN2 +WNWN)

with W y^ = (IT  WN)X^ as the spatially lagged predicted values in the regression, and M = INT - X(X X)-1X . This statistic is also asymptotically distributed as 2(1).
This simple approach can be generalized to account for more realistic error variance structures, such as heteroskedasticity across the time periods, in the same manner that heteroskedasticity is included in test statistics for the single cross-section (see, e.g., Kelejian and Robinson, 1998). Alternatively, each of the test statistics can be robustified against the alternative of the other form, using the standard approach (see Anselin et al., 1996).

19.5.2 Testing for Spatial Error Correlation in Panel Data Models
19.5.2.1 Spatial SUR Model
In the spatial SUR model (19.15­19.17), the LM test statistics are based on the residuals from a standard ML or FGLS estimation. In contrast to the pooled model, the null hypothesis pertains to T parameter constraints, H0 : 1 = . . . = T = 0 for the spatial error alternative.
To construct the statistic, consider a N × T matrix E with the N × 1 individual equation residual vectors as columns. The LME test statistic then follows as (Anselin, 1988b):

656
LME = T (^T-1  E WN E)J-1(^T-1  E WN E) T with  as the Hadamard product, and
J = [tr(WN2)]IT + [tr(WNWN )](^T-1  ^T ) The LME statistic is distributed asymptotically as 2(T ).

L. Anselin et al.

19.5.2.2 Error Components Models

In the error components model with spatial autoregressive errors (19.25­19.26),
the null hypothesis is H0 :  = 0. A LM test statistic can be constructed from the residuals obtained by estimating the standard error components model by FGLS
or ML. With e as the NT × 1 residual vector, and, to simplify notation, with ^ = (^2/^u2)/[1 + T (^2/^u2)], the test statistic follows as (Anselin, 1988a, p. 155):

LME =

(1/^u2)e [[IT + ^ (T ^ - 2)T T ] WN]e 2 , p

with p = (T 2^ 2 - 2^ + T )(trWN2 + trWNWN). It is distributed asymptotically as  2 (1).
When the point of departure is not the error components model, but the pooled
specification (19.2), both the error component and the spatial parameter can be con-
sidered as part of the null hypothesis, and a number of interesting combinations
result. The resulting tests can be classified as marginal, joint or conditional, de-
pending on which combinations of parameters restrictions are considered (Baltagi
et al., 2003). Specifically, marginal tests would be on either H0 :  = 0 (the spatial parameter)
or on H0 : 2 = 0 (the error component), based on the residuals of the pooled model. A joint test is on H0 :  = 2 = 0, and conditional tests are for H0 :  = 0 (assuming 2  0), or H0 : 2 = 0 (assuming  may or may not be zero). Each case yields a LM statistic using the standard principles applied to the proper likelihood function (for
details, see Baltagi et al., 2003). This rationale can be further extended to include a time-wise dependent process with parameter  , as in (19.32­19.33) (for detailed derivations, see Baltagi et al., 2007).

19.6 Conclusions
The econometrics of panel data models with spatial effects constitutes an active area of research, as evidenced by a growing number of recent papers on the topic. The focus to date has been primarily on theoretical and methodological aspects. Arguably, the dissemination of these methods to empirical practice has been hampered

19 Spatial Panel Econometrics

657

by the lack of ready to use software. None of the standard econometric packages include built-in facilities to carry out single cross-section spatial econometrics, let alone spatial panel econometrics.
For single cross-section spatial econometrics, there are now several software resources available, ranging from freestanding packages such as GeoDa (Anselin et al., 2006), to collections of routines in Matlab (James LeSage's collection of routines at http://www.spatialeconometrics.com) and R (Bivand, 2002). However, apart from a few Matlab routines for spatial fixed effects models developed by Paul Elhorst (see http://www.spatialeconometrics.com), the situation is rather bleak for panel spatial econometrics in general. A promising development in this regard is the effort under the auspices of the U.S. Center for Spatially Integrated Social Science (CSISS) to develop software for spatial econometrics in the open source Python language. The PySpace collection of modules that is currently under active development includes the basic tests and estimation methods for the pooled panel model as well as the spatial SUR model (Anselin and Le Gallo, 2004).
While much progress has been made, many areas remain where very little insight has been gained into the complexities that result from explicitly introduction spatial dependence and spatial heterogeneity into panel data models. Directions with particular promise for future research would be the extension to models with discrete dependent variables. Also of particular interest to applied researchers would be greater insight into the trade offs involved in using strategies for general crosssectional dependence relative to the use of parameterized spatial processes.
It is hoped that the review provided in the current chapter may provide a stimulus and resource to theoretical and applied researchers alike to aid in pursuing these directions in the future.

Acknowledgments The research by Luc Anselin and Julie Le Gallo was supported in part by the U.S. National Science Foundation Grant BCS-9978058 to the Center for Spatially Integrated Social Science (CSISS).

References
Anselin, L. (1988a). Spatial Econometrics: Methods and Models. Kluwer Academic Publishers, Dordrecht, The Netherlands.
Anselin, L. (1988b). A test for spatial autocorrelation in seemingly unrelated regressions. Economics Letters, 28:335­341.
Anselin, L. (2001a). Rao's score test in spatial econometrics. Journal of Statistical Planning and Inference, 97:113­139.
Anselin, L. (2001b). Spatial econometrics. In Baltagi, Badi, editor, A Companion to Theoretical Econometrics, pages 310­330. Blackwell, Oxford.
Anselin, L. (2002). Under the hood. Issues in the specification and interpretation of spatial regression models. Agricultural Economics, 27(3):247­267.
Anselin, L. (2003). Spatial externalities, spatial multipliers and spatial econometrics. International Regional Science Review, 26(2):153­166.

658

L. Anselin et al.

Anselin, L. and Bera, A. (1998). Spatial dependence in linear regression models with an introduction to spatial econometrics. In Ullah, Amman and Giles, David E.A., editors, Handbook of Applied Economic Statistics, pages 237­289. Marcel Dekker, New York.
Anselin, L., Bera, A., Florax, Raymond J.G.M., and Yoon, M. (1996). Simple diagnostic tests for spatial dependence. Regional Science and Urban Economics, 26:77­104.
Anselin, L. and Florax, Raymond J.G.M. (1995). New Directions in Spatial Econometrics. Springer-Verlag, Berlin.
Anselin, L., Florax, Raymond J.G.M., and Rey, Sergio J. (2004). Econometrics for spatial models, recent advances. In Anselin, Luc, Florax, Raymond J.G.M., and Rey, Sergio J., editors, Advances in Spatial Econometrics. Methodology, Tools and Applications, pages 1­25. Springer-Verlag, Berlin.
Anselin, L. and Le Gallo, J. (2004). Panel Data Spatial Econometrics with PySpace. Spatial Analysis Laboratory (SAL). Department of Agricultural and Consumer Economics, University of Illinois, Urbana-Champaign, IL.
Anselin, L. and Moreno, R. (2003). Properties of tests for spatial error components. Regional Science and Urban Economics, 33(5):595­618.
Anselin, L., Syabri, I., and Kho, Y. (2006). Geoda, an introduction to spatial data analysis. Geographical Analysis. 38(1):5­22.
Arellano, M. (2003). Panel Data Econometrics. Oxford University Press, Oxford, United Kingdom.
Baltagi, Badi H. (2001). Econometric Analysis of Panel Data (Second Edition). John Wiley & Sons, Chichester, United Kingdom.
Baltagi, Badi H., Egger, P., and Pfaffermayr, M. (2006). A generalized spatial panel data model with random effects. Working paper, Syracuse University, Syracuse, NY.
Baltagi, Badi H., Song, Seuck H., Jung, Byoung C., and Koh, W. (2007). Testing for serial correlation, spatial autocorrelation and random effects using panel data. Journal of Econometrics, 140(1):5­51.
Baltagi, Badi H., Song, Seuck H., and Koh, W. (2003). Testing panel data regression models with spatial error correlation. Journal of Econometrics, 117:123­150.
Banerjee, S., Carlin, Bradley P., and Gelfand, Alan E. (2004). Hierarchical Modeling and Analysis for Spatial Data. Chapman & Hall/CRC, Boca Raton, FL.
Barry, Ronald P. and Pace, R. Kelley (1999). Monte Carlo estimates of the log determinant of large sparse matrices. Linear Algebra and its Applications, 289:41­54.
Bivand, R. (2002). Spatial econometrics functions in R: Classes and methods. Journal of Geographical Systems, 4:405­421.
Brock, William A. and Durlauf, Steven N. (2001). Discrete choice with social interactions. Review of Economic Studies, 68(2):235­260.
Brueckner, Jan K. (2003). Strategic interaction among governments: An overview of empirical studies. International Regional Science Review, 26(2):175­188.
Burridge, P. (1980). On the Cliff-Ord test for spatial autocorrelation. Journal of the Royal Statistical Society B, 42:107­108.
Case, Anne C. (1991). Spatial patterns in household demand. Econometrica, 59:953­965. Case, Anne C. (1992). Neighborhood influence and technological change. Regional Science and
Urban Economics, 22:491­508. Case, Anne C., Rosen, Harvey S., and Hines, James R. (1993). Budget spillovers and fiscal policy
interdependence: Evidence from the states. Journal of Public Economics, 52:285­307. Casetti, E. (1997). The expansion method, mathematical modeling, and spatial econometrics. In-
ternational Regional Science Review, 20:9­33. Chen, X. and Conley, Timothy G. (2001). A new semiparametric spatial model for panel time
series. Journal of Econometrics, 105:59­83. Cliff, A. and Ord, J. Keith (1981). Spatial Processes: Models and Applications. Pion, London. Coakley, J., Fuentes, A.-M., and Smith, R. (2002). A principal components approach to cross-
section dependence in panels. Working Paper, Department of Economics, Birkbeck College, University of London, London, United Kingdom.

19 Spatial Panel Econometrics

659

Conley, Timothy G. (1999). GMM estimation with cross-sectional dependence. Journal of Econometrics, 92:1­45.
Conley, Timothy G. and Ligon, E. (2002). Economic distance, spillovers and cross country comparisons. Journal of Economic Growth, 7:157­187.
Conley, Timothy G. and Topa, G. (2002). Socio-economic distance and spatial patterns in unemployment. Journal of Applied Econometrics, 17:303­327.
Cressie, N. and Huang, H.-C. (1999). Classes of nonseparable spatio-temporal stationary covariance functions. Journal of the American Statistical Association, 94:1330­1340.
Cressie, N. (1993). Statistics for Spatial Data. Wiley, New York. Driscoll, John C. and Kraay, Aart C. (1998). Consistent covariance matrix estimation with spatially
dependent panel data. The Review of Economics and Statistics, 80:549­560. Druska, V. and Horrace, William C. (2004). Generalized moments estimation for spatial panel
data: Indonesian rice farming. American Journal of Agricultural Economics, 86(1):185­198. Dubin, R. (1988). Estimation of regression coefficients in the presence of spatially autocorrelated
errors. Review of Economics and Statistics, 70:466­474. Dubin, R. (1995). Estimating logit models with spatial dependence. In Anselin, Luc and Florax,
Raymond J.G.M., editors, New Directions in Spatial Econometrics, pages 229­242. SpringerVerlag, Berlin. Elhorst, J. Paul (2001). Dynamic models in space and time. Geographical Analysis, 33:119­140. Elhorst, J. Paul (2003). Specification and estimation of spatial panel data models. International Regional Science Review, 26(3):244­268. Fazekas, I., Florax, R., and Folmer, H. (1994). On maximum likelihood estimators of parameters of spatio-temporal econometric models. Technical Report No. 109/1994, Kossuth University, Debrecen, Hungary. Florax, Raymond J.G.M. and Van Der Vlist, Arno J. (2003). Spatial econometric data analysis: Moving beyond traditional models. International Regional Science Review, 26(3):223­243. Fotheringham, A. Stewart, Brunsdon, C., and Charlton, M. (2002). Geographically Weighted Regression. John Wiley, Chichester. Gamerman, D., Moreira, Ajax R.B., and Rue, H. (2003). Space-varying regression models: Specifications and simulation. Computational Statistics & Data Analysis, 42(3):513­533. Gelfand, Alan E., Kim, H.-J., Sirmans, C.F., and Banerjee, S. (2003). Spatial modeling with spatially varying coefficient processes. Journal of the American Statistical Association, 98:387­396. Giacomini, R. and Granger, Clive W.J. (2004). Aggregation of space-time processes. Journal of Econometrics, 118:7­26. Glaeser, Edward L., Sacerdote, Bruce I., and Scheinkman, Jose A. (2002). The social multiplier. Technical Report 9153, NBER, Cambridge, MA 02138. Haining, R. (1990). Spatial Data Analysis in the Social and Environmental Sciences. Cambridge University Press, Cambridge. Hsiao, C. (1986). Analysis of Panel Data. Cambridge University Press, Cambridge. Hsiao, C. and Pesaran, M. Hashem (2008). Random coefficient panel data models. In Matyas L. and Sevestre P., editors, The Econometrics of Panel Data. Kuwer Academic Publishers, Dordrecht. Hsiao, C., Pesaran, M. Hashem, and Tahmiscioglu, A. Kamil (2002). Maximum likelihood estimation of fixed effects dynamic panel models covering short time periods. Journal of Econometrics, 109:107­150. Kapoor, M., Kelejian, Harry H., and Prucha, Ingmar R. (2007). Panel data models with spatially correlated error components. Journal of Econometrics, 140(1):97­130. Kelejian, Harry H. and Prucha, I. (1998). A generalized spatial two stage least squares procedures for estimating a spatial autoregressive model with autoregressive disturbances. Journal of Real Estate Finance and Economics, 17:99­121. Kelejian, Harry H. and Prucha, I. (1999). A generalized moments estimator for the autoregressive parameter in a spatial model. International Economic Review, 40:509­533.

660

L. Anselin et al.

Kelejian, Harry H. and Robinson, Dennis P. (1993). A suggested method of estimation for spatial interdependent models with autocorrelated errors, and an application to a county expenditure model. Papers in Regional Science, 72:297­312.
Kelejian, Harry H. and Robinson, Dennis P. (1995). Spatial correlation: A suggested alternative to the autoregressive model. In Anselin, Luc and Florax, Raymond J.G.M., editors, New Directions in Spatial Econometrics, pages 75­95. Springer-Verlag, Berlin.
Kelejian, Harry H. and Robinson, Dennis P. (1998). A suggested test for spatial autocorrelation and/or heteroskedasticity and corresponding Monte Carlo results. Regional Science and Urban Economics, 28:389­417.
Lee, L.-F. (2002). Consistency and efficiency of least squares estimation for mixed regressive, spatial autoregressive models. Econometric Theory, 18(2):252­277.
Lee, L.-F. (2003). Best spatial two-stage least squares estimators for a spatial autoregressive model with autoregressive disturbances. Econometric Reviews, 22:307­335.
Magnus, J. (1978). Maximum likelihood estimation of the GLS model with unknown parameters in the disturbance covariance matrix. Journal of Econometrics, 7:281­312. Corrigenda, Journal of Econometrics 10, 261.
Manski, Charles F. (1993). Identification of endogenous social effects: The reflexion problem. Review of Economic Studies, 60:531­542.
Manski, Charles F. (2000). Economic analysis of social interactions. Journal of Economic Perspectives, 14(3):115­136.
Mardia, K.V. and Goodall, C. (1993). Spatio-temporal analyses of multivariate environmental monitoring data. In Patil, G.P. and Rao, C.R., editors, Multivariate Environmental Statistics, pages 347­386. Elsevier, Amsterdam.
Mardia, K.V. and Marshall, R.J. (1984). Maximum likelihood estimation of models for residual covariance in spatial regression. Biometrika, 71:135­146.
Ord, J. Keith (1975). Estimation methods for models of spatial interaction. Journal of the American Statistical Association, 70:120­126.
Pace, R. Kelley and Barry, R. (1997). Sparse spatial autoregressions. Statistics and Probability Letters, 33:291­297.
Paelinck, J. and Klaassen, L. (1979). Spatial Econometrics. Saxon House, Farnborough. Pesaran, M. Hashem (2002). Estimation and inference in large heterogenous panels with cross
section dependence. DAE Working Paper 0305 and CESifo Working Paper no. 869, University of Cambridge, Cambridge, United Kingdom. Pesaran, M. Hashem (2004). General diagnostic tests for cross section dependence in panels. Working paper, University of Cambridge, Cambridge, United Kingdom. Rey, Sergio J. and Montouri, Brett D. (1999). US regional income convergence: A spatial econometrics perspective. Regional Studies, 33:143­156. Smirnov, O. and Anselin, L. (2001). Fast maximum likelihood estimation of very large spatial autoregressive models: A characteristic polynomial approach. Computational Statistics and Data Analysis, 35:301­319. Stein, Michael L. (1999). Interpolation of Spatial Data, Some Theory for Kriging. Springer-Verlag, New York. Topa, G. (2001). Social interactions, local spillover and unemployment. Review of Economic Studies, 68(2):261­295. Upton, Graham J. and Fingleton, B. (1985). Spatial Data Analysis by Example. Vol. 1: Point Pattern and Quantitative Data. Wiley, New York. Waller, L., Carlin, B., and Xia, H. (1997a). Structuring correlation within hierarchical spatiotemporal models for disease rates. In Gre´goire, T., Brillinger, D., Russek-Cohen, P., Warren, W., and Wolfinger, R., editors, Modeling Longitudinal and Spatially Correlated Data, pages 309­319. Springer-Verlag, New York. Waller, L., Carlin, B., Xia, H., and Gelfand, A. (1997b). Hierarchical spatio-temporal mapping of disease rates. Journal of the American Statistical Association, 92:607­617. Wikle, Christopher K., Berliner, L. Mark, and Cressie, N. (1998). Hierarchical Bayesian spacetime models. Environmental and Ecological Statistics, 5:117­154.

Part III
Applications

Chapter 20
Foreign Direct Investment: Lessons from Panel Data
Pierre Blanchard, Carl Gaigne´ and Claude Mathieu

20.1 Introduction
Since the 1980s, foreign direct investment (FDI) flows have grown substantially, especially throughout OECD countries (UNCTAD (2002)). The average share of FDI outflows in GDP went from around 2% in 1985 to almost 11% by the end of the 1990s. In 2000, OECD countries were the source of 90% FDI flows, and the recipient of 79%. International corporations are now major actors in international trade since their contribution to global production climbed to 11% in 2001. It is not surprising that, over the last two decades, FDI has spawned a significant amount of academic research and the literature continues to grow at an impressive rate. The empirical literature has expanded at a rapid pace in many different directions. Regardless of the question studied, the nature of the problem itself generally requires using panel data estimation methods because flows (or stocks) of FDI between pairs of countries (or between country-industry pairs) are analyzed for one or several time period. The purpose of this chapter is to provide a selective survey of the empirical literature using panel data.
As we will see in the next section, the theoretical literature has identified two dimensions acting upon the structure of FDI. The main components of firm/industry characteristics are transport costs, plant scale economies and factor intensities whereas market size, tariff levels and factor abundance are the main components of country features. As a result, using the econometrics of panel data is a natural way to evaluate the determinants to FDI. This issue is discussed in Sect. 20.3. In Sect. 20.4,
Pierre Blanchard Erudite, Faculte´ de Sciences Economiques et de Gestion, Universite´ Paris XII Val de Marne, 61 Av. du Ge´ne´ral de Gaulle, 94010 Cre´teil Ce´dex, France, e-mail: blanchard@univ-paris12.fr
Carl Gaigne´ INRA, UMR1302, SMART, F-35000 Rennes, France, e-mail: gaigne@rennes.inra.fr
Claude Mathieu Erudite, Faculte´ de Sciences Economiques et de Gestion, Universite´ Paris XII Val de Marne, 61 Av. du Ge´ne´ral de Gaulle, 94010 Cre´teil Ce´dex, France, e-mail: mathieu@univ-paris12.fr

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

663

c Springer-Verlag Berlin Heidelberg 2008

664

P. Blanchard et al.

we present three types of empirical studies using the econometrics of panel data. The first type concerns the trade-off between producing at home or abroad. The second type studies more precisely the role of trade policies (anti-dumping, threat of protectionism, custom union) in the decision to establish an additional plant in a foreign country. The last type focuses on the impact of financial factors on the level of FDI. In the last section, we discuss recent econometric issues related to estimating FDI models using panel data.
Before presenting the micro-foundations of the decision to produce abroad, we have to define foreign direct investment. FDI refers to investments by multinational firms (MNF) in affiliates or subsidiaries. It consists of two broad categories: (i) direct net transfers from the parent company to the foreign affiliate, either through equity or debt; and (ii) reinvested earnings by a foreign affiliate. FDI is generally thought as a real capital flow between countries, the main interest in our analysis. Still, statistical information on FDI involves financial flows that do not necessarily correspond to an international allocation of productive capital. Indeed, FDI is comprised of several types of capital. First, it contains real investment in plants and equipment, either in the form of new plants and equipment or plant expansion. Second, a major part of FDI consists of the financial flows associated with mergers and acquisitions. This implies an ownership change in the absence of any real investment. OECD (2000) estimates suggest that mergers and acquisitions account for more than 60% of all FDI in developed countries. Others components of FDI are joint ventures and equity increases. The latter component typically comprises investment in financial capital. The distinction between the various types of FDI is important because the different components may have different explanations.

20.2 A Simple Model of FDI
In the 1980s, trade economists proposed refinements of the factor-proportions approach to explain the emergence of multinational corporations (e.g. Helpman (1984)). They determine the conditions under which firms have an incentive to become a "vertical" multinational, that is to separate headquarters from plant. A vertical multinational activity arises between countries that differ significantly in relative endowments. However, in order to explain the existence of foreign direct investments among similar countries, an alternative approach has been proposed by different authors (e.g. Markusen (1984)). The purpose is to determine the conditions under which firms produce the same product in multiple plants, serving local markets by local production. A firm will probably be a horizontal multinational when trade costs are relatively high and plant-level scale economies are low enough. This theoretical literature on FDI is generally characterized by general equilibrium models (see Markusen (1995)). However, in order to make the results accessible, we do not develop a general equilibrium model of plant location in this section. The objective of this section is to show basic mechanisms at work by developing a simple model of foreign investments, which is close to Markusen (2002). We will see

20 Foreign Direct Investment: Lessons from Panel Data

665

how the main characteristics of technologies and countries interact to determine the choice of firms to engage in FDI and the type of FDI (horizontal or vertical). Technology features include plant-level and firm-level scale economies whereas country features include trade costs and global market size as well as differences in market size and marginal costs of production.

20.2.1 Assumptions and Preliminary Results

Consider one good produced by a single firm and sold in two markets/countries (h and f ). Countries may differ in population size and/or in technology. The production of the good implies two types of sunk cost: a plant-specific fixed cost (G) by production unit and a firm-specific cost (F). Consumers are internationally immobile and both markets are segmented. The firm practices third degree price discrimination without threat of arbitrage by consumers. There are three alternatives modes of serving both markets. (i) By a national firm with a single plant located in country h (type-n). The national firm serves country f by exporting, which implies operational costs t such as transportation costs as well as other nontariff trade barriers. We assume that t is symmetric between countries. (ii) By a horizontal multinational with two plants located in both countries (type-h). The horizontal multinational serves country f by establishing a subsidiary abroad, which implies further plant-specific fixed cost G. (iii) By a vertical multinational with the headquarter located in country h and one plant in country f that serves both markets (type-v).
The inverse demand function in each country is given by

pi j = a - (b/Li)qi j

(20.1)

where pi j, qi j are price and quantity of the good produced in country i = h, f and sold in country j = h, f . In addition Li is the population in country i = h, f . We assume that a, b > 0.
The expression of profits of a type-n firm is expressed as follows:

n = (a - (b/Lh)qhh)qhh + [(a - (b/L f )qh f ) -t]qh f - ch(qhh + qh f ) - G - F (20.2)

where ch is the marginal cost of production prevailing in country h, and F a firmspecific fixed cost. By solving the first-order conditions, the profit-maximizing output in both markets is given by,

qhh

=

a - ch 2b

Lh

and

qh f

=

a

- ch 2b

-

t

Lh

(20.3)

Consider now that the firm is a horizontal multinational. Its profit function is

h = [(a - (b/Lh)qhh) - ch]qhh - G + [(a - (b/L f )bq f f ) - c f ]q f f - G - F (20.4)

666

P. Blanchard et al.

The supply on the foreign market corresponds to

qf f

=

a-cf 2b

Lf

(20.5)

Note that the supply on the domestic market is qhh from (20.3). Finally, when the multinational adopts a type-v structure, its profit equation is
expressed as follows:

v = [(a - (b/Lh)q f h) - c f - t]q f h + [(a - (b/L f )q f f ) - c f ]q f f - G - F (20.6)

Maximizing (20.6) gives the export sales from country f to country h:

qf h

=

a-cf 2b

-t

Lh

(20.7)

whereas the sales in country f (qf f ) are given by (20.5). We can now summarize the total profits under the three alternative modes of
serving country h and f by introducing (20.3), (20.5) and (20.7) in (20.2), (20.4)
and (20.6), respectively,

n = h = v =

a - ch 2b

2
Lh +

a - ch - t 2b

2
Lf -G-F

a - ch 2b

2
Lh +

a-cf 2b

2
L f - 2G - F

a-cf -t 2b

2
Lh +

 -cf 2b

2
Lf -G-F

(20.8) (20.9) (20.10)

20.2.2 Technology and Country Characteristics as Determinants of FDI
The previous three profit equations enable us to determine the main factors that determine the choice for a firm about whether or not to engage in foreign investment and the type of FDI (horizontal or vertical). To simplify the analysis, we assume that a > ch  c f and Lh  L f where a is sufficiently large as well as L f . We consider four configurations: (i) the characteristics of both countries are identical; (ii) the size of the home market is larger; (iii) the marginal cost of production is lower in the foreign country; (iv) combination of cases (ii) and (iii).
(i) First, we assume that countries are identical with respect to technology and factor endowments (ch = c f = c and Lh = L f = L). So, we have n = v. Trivial calculations show that the firm decides to produce in both countries (h > n) if and only if trade costs are high enough or equivalently when t > tnh where

20 Foreign Direct Investment: Lessons from Panel Data

667

tnh  (a - c) 1 -

1

-

L

f

4bG (a - c)2

>0

(20.11)

The threshold value tnh increases when G declines and decreases when the population size of countries (L) grows. In other words, the firm will be likely a horizontal multinational when trade costs are high relatively to plant scale economies and when the markets to serve are large enough.
(ii) Assuming now that countries are only different in population size with ch = c f = c and Lh > L f . Therefore, country h has an advantage in market size. In this case, regardless of values of trade costs, profits when the firm adopts a type-n structure is always superior to profits when it chooses a type-v structure (n > v). The critical value of trade costs above which the national firm becomes a horizontal multinational is identical to tnh, except that L f is now lower than Lh. As a result, it appears that, when the market size is higher at home, convergence in population size between countries prompts the firm to establish a second plant abroad.
(iii) We now consider the case where countries are only different in production costs with ch > c f and Lh = L f = L. Stated differently, country f has an advantage in production costs. In this configuration, we have n < v regardless of trade costs. When production costs differ among countries, the firm has a strong incentive to become vertical multinational. In addition, the multinational produces in both countries if and only if t > tvh where

tvh = (a - c f ) 1 -

1

-

b2

4G L(a -

ch

)2

(20.12)

It is readily confirmed that horizontal direct investments are favored when marginal costs converge.
(iv) Finally, with ch > c f and Lh > L f , we consider the case where the advantage in market size benefits country h while the advantage in production costs benefits country f . This configuration is more complex because we must rank three profit equations: v, n and h. Figure 20.1 shows graphically the profits of each regime against trade costs. It is straightforward to check that v(t = 0) > n(t = 0) and that profits in both structures (type-v and -n) decline when trade costs increase. In addition, we have v(t = 0) > h. As a result, the multinational is more likely to have a vertical structure when trade costs are very low. Further, as profits do not vary with respect to trade costs when the firm is characterized by a type-h structure (see the dashed lines in Fig. 20.1), a horizontal FDI is more likely to take place when trade costs are high enough. Finally, the firm becomes national when trade costs take intermediate values. Note that the relative position of the profit curves depends on the size of plant scale economies (G). More precisely, a fall in G increases the profits more when the multinational is located in both countries than when the firm produces in a single country.

668

P. Blanchard et al.

Fig. 20.1 Profit functions according to the firm type

Observe also that we have v(t) > n(t) if and only if t < tnv where

tnv



(ch

-

c

f

)

Lh Lh

+ -

L L

f f

(20.13)

Then, the firm is likely to prefer to serve both countries from the foreign country when trade costs are low enough. In addition, when the size of markets diverges noticeably (Lh grows or L f declines), the type-v firm is more likely to occur (tnv increases). Consequently, the advantage in production costs dominates the advantage in market size when trade costs are sufficiently low, while the advantage in market size dominates the advantage in production costs when trade costs are high enough.
To summarize our analysis, we first recall the main conditions under which a firm engages in horizontal FDI: countries are similar in market size and in marginal production costs, the "world" demand is sufficiently high, the plant-specific fixed cost is low relative to the firm-specific fixed cost and trade costs are high enough. In addition, the firm is more likely to be a vertical multinational when trade costs are low enough and when the difference in production costs is sufficiently high. Finally, notice that FDI and trade are substitutes when multinationals are horizontal and complementary when multinationals are vertical.

20.3 Econometric Implementation and Data
The basic model of the previous section has allowed us to identify the factors acting upon the emergence of FDI at two levels: at firm/industry level (technology, plant scale economies, factor intensities and transport costs) and at country level (market size, tariff levels and factor endowments). As a result, panel data models have been

20 Foreign Direct Investment: Lessons from Panel Data

669

extensively used for analyzing the factors determining the international allocation of foreign investments.

20.3.1 A General Econometric Model

Ideally, in order to control for observed and unobserved heterogeneity between host

and domestic countries and for time effects, we need to estimate triple indexed models (see Matyas (2001), Egger and Pfaffermayr (2003)). A basic specification1 is, for

instance,

FDIi jt = xi jt  + i +  j + t + i j + ui jt

(20.14)

where FDIi jt is the amount of outward FDI of country i (home) held in the country j (host) at year t and xi jt is a vector of regressors.2 As suggested by the theoretical model, the variables included in the regressors list may be: a measure of bilateral country size (e.g. the sum of bilateral GDP); an index of size similarity; one or more measure of differences in relative endowments (e.g. capital stock and/or skilled labor ratios between the home and host countries); a variable measuring trade costs (tariffs, distance, for example). Nevertheless, several variables are added in order to control for investment, political, financial risks, non tariff barriers, openness policy. . . . Because the specification takes into account the effect of "gravity" factors (e.g. market size, distance), this model is usually called the gravity model and is commonly used not only for FDI analysis but also for modelling trade between countries. The parameters i and  j are introduced in order to control for heterogeneity across (host and domestic) countries (due for instance to legal or cultural characteristics) whereas t captures any time-specific effect common to all pairs of countries such as business cycle effects, or changes in the degree of openness across all economies. The term i j accounts for all time-invariant effects between two given countries such as common language and common borders.3 These effects are modelled either as fixed (fixed effects model) or as random (random effects model) or, in very few studies, with a random coefficients specification (e.g. Feinberg and Keane (2001) and Balestra and Negassi (1992)). The Hausman test is frequently used in order to choose between the fixed effects and the random effects specification.
A second reason for using panel data for estimating FDI models relies often on the necessity to take into account the correlation between contemporaneous FDI flows and those of the previous year due to adjustment and sunk costs. By

1 A more general specification is given by Baltagi, Egger and Pfaffermayr (2003). See Sect. 20.4.
2 Several variants of this specification are frequently used in applied works, e.g.: (i) one can explain bilateral FDI from country i to country j for a given year (FDIi j). (ii) It is also possible to focus on FDI from a given home country to several host countries at time t (FDI jt ) or in sector s at time t for each host country j (FDI jst ); (iii) one can also model FDI from a parent firm i to affiliates j at time t (FDIi jt ). 3 Note also that the country-pair effects may differ according to the direction of FDI (i.e. i j =  ji) which can be tested for.

670

P. Blanchard et al.

including FDIi jt-1 in the model, we have a dynamic specification of FDI. Although this allows us to distinguish between short-run and long-run effects, it creates a correlation between the lagged dependant variable and the error term. However in this case, the usual estimators (OLS, within, GLS) are biased and inconsistent (for short T ). With panel data, this problem may be solved by transforming the model in first differences. Moreover, using the time dimension, we can quite easily find instruments for endogenous regressors, FDIi jt-1 of course, but also for other explanatory variables which may be endogenous in such a context, e.g. GDP, exchange rate.
Therefore, it is not surprising that a vast and recent econometric literature using panel data has emerged on these topics with a great variability in the estimation methods used.

20.3.2 FDI and Data Issues
Testing model (20.14) requires data that vary in different dimensions (firm/ industry, country and time). There are two main types of data on foreign direct investment: (i) the balance of payments provides information on inward and outward flows of FDI and the stocks derived from accumulated FDI flows. Such data are available at country level and vary over time; (ii) the second type of data is about operations of individual multinational firm at home and abroad. We discuss the advantages and disadvantages of these two types of data sets.
Type (i). Different international institutions publish international data on FDI based on the balance of payments, such as the International Monetary Fund (IMF), the United Nations (via the UNCTAD World Investment Report) and the Organization for Economic Cooperation and Development (OECD). These data sets cover many countries but many of them deviate significantly from the international guidelines for the compilation of balance of payments and international investment position statistics in the IMF's Balance of Payment Manual (5th edition) and in the OECD's Benchmark Definition of Foreign Direct Investment (3rd edition). We choose to describe more precisely the data provided by the OECD (see Lipsey (2001), for a description of data from IMF and United Nations) for two main reasons: first, FDI between OECD countries represent more than 60% of the overall FDI; second, these countries are more in accordance with the recommendations of Survey of Implementation of International Methodological Standards for Direct Investment (SIMSDI) which is a comprehensive study of data sources, collection methods, and dissemination and methodological practices for FDI statistics. The Directorate for Financial, Fiscal and Enterprise Affairs of OECD yields statistics on FDI transactions and positions, published under the title International Direct Investment Statistics Yearbook. The flows and stocks of FDI are compiled by using the balance of payments and the international investment positions, respectively. Both data sets are available for inward and outward FDI by partner country and by

20 Foreign Direct Investment: Lessons from Panel Data

671

industry (according to ISIC Rev. 3 classifications) in the standard format defined by the international guidelines. A few OECD countries do not provide complete information and/or deviate from the agreed international standards established by the IMF. Moreover, the database covers 28 OECD countries over the 1980­2000 period. As a result, the cross-sectional comparability of the data is improving and balanced panels can be easily implemented (see IMF and OECD (2001) and Nicoletti, Golub, Hajkova, Mirza and Yoo (2003) for further details).
Type (ii). Although it is becoming more widely recognized that data need to account for heterogeneity between MNF, this type of information is still relatively scarce. Some countries collect information about inward FDI (France, Germany, Italy, among other) or outward FDI (Japan, see Falzoni (2000) for a description). With few exceptions, only the US and Sweden produce data for both outward and inward FDI.4 Moreover, information about the characteristics relative to the parent companies and their affiliates are less frequent.5 In fact, the US Bureau of Economic Analysis (BEA) provides the more extensive database about the operations of the affiliates and their parent companies. Indeed, available data give specific information about gross product, employment, wages and R&D expenditures for each domestic or foreign unit belonging to a MNF. In the same vein, the Research Institute of Industrial Economics (IUI) in Sweden compiles a dataset which is based on a questionnaire sent to all Swedish MNF, containing information on parent companies as well as on the operations of each individual subsidiary (see Braunerhjelm, Ekholm, Grundberg and Karpaty (1996) for a detailed description of this database). However, this survey is only implemented approximately every four years since the 1960s. This means it is difficult to build a panel with a time dimension. Even though the BEA only conducts benchmark surveys every 5 years, its annual surveys can be used to build a more detailed panel dataset. A more important limit is that this dataset covers only US bilateral activity. Note that this limit is applied to all MNF databases developed by the different national official statistical departments (see for example the database on intra-firm international trade of the Department for Industrial Studies and Statistics (SESSI) of the French Ministry for Economic Affairs, Finance and Industry) and also to data gathered by private agencies (see Head and Ries (2001) for the database of Toyo Keizai on Japanese MNF). In addition, studies testing the trade-off between US export and FDI use the ratio of US exports to the sales of US multinational affiliates as the dependent variable. Indeed, the BEA's database does not contain firms which are only exporters. Consequently, the multinational firm sales must be aggregated at the level of industries in order to make comparable FDI and export data.

4 See Stephan and Pfaffmann (1998) and Lipsey (2001) for a description and a discussion of different national sources of FDI as in Sweden, Germany, Japan and Canada. 5 Note that these characteristics are not necessarily comparable when they exist (see for example the survey about the foreign affiliates of the French Ministry of Finance in 2000).

672

P. Blanchard et al.

20.4 Empirical Estimations: Selected Applications

20.4.1 Testing the Trade-Off Between FDI and Exports

20.4.1.1 FDI Versus Exports
A branch of the empirical literature on multinational production-location decisions has used the BEA database in order to study the determinants of FDI at the country/industry level. Within this literature, Brainard (1997) was the first to use direct industry- and country-specific measures of several determinants of FDI. Her objective is to test the determinants of the horizontal integration of multinationals (see Sect. 20.2). However, the author also controls for the possibility that multinational activity is motivated by gaining access to factor supplies (vertical integration). This work has been extended in two ways. First, Helpman, Melitz and Yeaple (2004) focus on the horizontal dimension of FDI location decision by taking into account the heterogeneity within sectors. In parallel, Yeaple (2003) controls more explicitly for the vertical dimension of FDI decisions by including the interaction between factor intensities and factor abundance. Combining the approaches of Brainard, Helpman et al. and Yeaple, the general empirical model is given by:
  EXSHjs = 0 + c ccft js + l lscales + uUCjs + s +  j +  js (20.15)
where EXSH js is, in the three papers, the ratio of total US sales of good s in country j to the sum of local affiliate sales and exports from US to that host country.6 Note that Brainard and Yeaple consider also the share of US imports as a dependent variable. In this subsection, we only focus on outward foreign investment.
cft js is a vector of trade costs, such as transport and insurance costs (FREIGHT js) and tariff barriers (TARIFF js). In Brainard and Yeaple, FREIGHT js is measured by the freight and insurance charges reported by importers to the US Bureau of Census to calculate freight factors such as the ratio of charges to import values.7 The data on TARIFF js comes from a 1988/1989 database by the General Agreement on Tariffs and Trade on ad valorem tariffs at the 3-digit SIC level of industry. In Helpman et al., FREIGHT js is computed as the ratio of CIF imports into the US to FOB imports from the data presented by Feenstra (1997) whereas TARIFF js is calculated at the BEA industry/country level.
scales stands for scale economies in each industry. Two types of scale economies must be distinguished: at the corporate level (CSCALEs) and at the plant level (PSCALEs). They correspond to G and F in our theoretical model, respectively. In Brainard, CSCALEs is measured as the number of nonproductive workers in the
6 Relatively to model (20.14), we deleted the subscript i since we have one home country (the USA) and introduced a further dimension (the sector) indexed by the subscript s. Hence, s is a sector-specific component, capturing the sectoral characteristics of firms in each country that are unobservable or omitted from equation, but do not vary over country. 7 No comparable data are available from exporters. The authors assume that transport costs are symmetric, which introduces measurement error in the outward estimates.

20 Foreign Direct Investment: Lessons from Panel Data

673

average US-based firm and, in Yeaple, as the average number of nonproduction employees at the firm level. In Brainard, PSCALEs is defined as the number of production employees in the median US plant ranked by value added, whereas Yeaple uses the average number of production workers in the US plants. Helpman et al. calculate the average number of non-production workers at the six-digit level. They then compute this measure for every three-digit level as the average of the within three-digit sectors, weighted by the six-digit level sales in this sector.
The variable UCjs stands for unit costs of production, introduced in order to control for factor-proportions differences. In Yeaple, this cost is a vector of variables that reflect a potential host country's unit cost of production by sector. Brainard considers variations only in country characteristics since the proxy used is the differential in per-worker income whereas Helpman et al. use only cross-industry variations in technology such as capital and R&D intensities.
As expected, the studies by Brainard and Helpman et al. suggest that the share of affiliate sales is increasing in trade barriers, transport costs and corporate sale economies and decreasing in production scale economies. The empirical analysis of Brainard suggests also that the comparative advantage motive for FDI is far less important. These findings support the horizontal model of FDI. However, by considering the interaction between factor abundance and factor intensities at highly disaggregated level, Yeaple shows that the comparative advantage in production cost is also a key determinant of FDI.

20.4.1.2 Horizontal Versus Vertical FDI
The previous empirical works suggest FDI decision may be motivated by both horizontal and vertical considerations. Recent theoretical works, which are called knowledge-capital models (henceforth KC model), show that vertical and horizontal firms can emerge simultaneously (cf. Markusen, Venables, Eby-Konan and Zhang (1996), Markusen (1997)). The main feature of the KC models is that exploitation of factor-price differences interacts with multi-plant scale economies to explain the decision and the nature of foreign investments. The results of these models, arising from simulations, relate the decision to produce abroad to country characteristics. Examples of empirical papers in this field are Carr, Markusen and Maskus (2001) and Blonigen, Davies and Head (2003) as well as Markusen and Maskus (2002a,b). Again, the data used in these empirical studies comes from the US BEA, although data are aggregated across industries to the country-level to form a panel of cross-country observations over the period 1986­1994.
The KC model is a more elaborate version of the model developed in Sect. 20.2. The central idea is that the services of knowledge and knowledge-generating activities can be spatially separated from production and supplied to production facilities. As knowledge-based services are more skill intensive than production, the multinational corporations have an incentive to locate the first (resp., second) activity in country where skilled (resp., unskilled) labor is relatively cheap. Consequently, the multinational firm can be vertically integrated. In addition, the output of

674

P. Blanchard et al.

knowledge-base activities can be used simultaneously by multiple producers. Then, the existence of firm-level scale economies implies that the same products or services are produced in different countries. In this case, multinational firms can be horizontally integrated.
As in the theoretical model, three types of firms can emerge: (i) horizontal multinationals where plants are located in different countries and headquarters set up in the home country (type-h); (ii) vertical multinationals where production takes place in a single plant located in a foreign country while the headquarters are maintained in the home country (type-v); and (iii) national firms with a single plant where the production and knowledge-based services are located only in the home country (type-n). Given the new assumptions, the last regime is dominant in the country with the large market size and skilled worker endowment and when foreign investment barriers are high. Type-h firms are likely to be dominant if transport costs are high enough and if the nations are similar in relative factor endowments as well as in size. In other words, if countries are dissimilar either in size or in factor endowments, one nation will be favored. For example, if nations have the same factor endowments but differ in size, firms located in the larger country benefit from lower production cost. Thus, vertical multinationals may emerge when the home country is skilled-laborabundant and small, unless trade costs from the host country (where production takes place) back to the parent country (where the headquarter is located) are too excessive.
Given mechanisms discussed above, Carr, Markusen and Maskus (2001) estimate the following equation:

FDIi jt = 0 + 1GDPsumi jt + 2(GDPdi fi jt )2 + 3SKILLdi fi jt + 4[GDPdi fi jt × SKILLdi fi jt ] + mci jt  +  j + ui jt

(20.16)

The dependent variable (FDIi jt ) is the real volume of production (sales) by manufacturing affiliates in each host country j that are majority owned by parents in domestic country i. The variable GDPsumi jt is the bilateral sum of real GDP levels at home and abroad (the joint market size) whereas (GDPdi fi jt )2 is the squared difference in real GDP between home and foreign countries. Then, SKILLdi fi jt stands for the difference in skilled-labor abundance in both countries. Note that the variable GDPdi fi jt × SKILLdi fi jt captures the fact that affiliates sales are higher when the home country has a small size and is skilled-labour-abundant. Finally, mci jt is a vector of multinationalization cost variables such as the perceived costs of investing in, and exporting to, the host country as well as the perceived trade costs in exporting to the parent country.
A fixed effect ( j) is introduced for each foreign country. The results with country-pair dummies are not reproduced in the paper. As expected, outward investment increases with the joint market size, the convergence in GDP between the parent country and any host country and the abundance in skilled workers of the parent nation. Moreover, when the country fixed effects are introduced, the difference in skill endowments has a smaller role but remains significant while the other variables keep the same impact.

20 Foreign Direct Investment: Lessons from Panel Data

675

These results seem to offer direct support for the KC model and to reject the horizontal model. Indeed, the last model predicts that absolute skill difference is negatively related to affiliate sales (Markusen and Venables (2000)). In contrast, in KC model, the production of foreign affiliates grows when the difference in skilledlabour abundance declines. However, Carr, Markusen and Maskus (2001) estimate a pooled coefficient on a difference term that takes both positive and negative values. This introduces a substractive linear constraint which can lead to a sign reversal in the pooled or restricted coefficient. Indeed, when the difference is negative (resp., positive), the rise in differences implies a convergence (resp., divergence) in skilllabour endowments. From the same database, Blonigen, Davies and Head (2003) exactly replicate the analysis of Carr et al. except that they consider the absolute values of skill difference. In this way, these variables are always decreasing in skill similarity. With this correct specification, Blonigen et al. obtain coefficient signs that support the horizontal model. This result suggests that the preponderance of multinational activity in developed countries is horizontal in nature.

20.4.1.3 Exports and FDI: Substitutes or Complements
Another way to determine the preponderance of horizontal FDI is to test whether FDI and exports are substitutes or complements. Our analysis in Sect. 20.2 suggests that substitution is the expected relationship under horizontal investments. This result arises from the fact that this model focuses on trade in final goods. When intermediate goods are introduced, foreign investment and export may simultaneously increase or decrease. Indeed, the rise in the production of affiliates induces an increase in imported inputs from the home country which corresponds to intrafirm trade when they come from parent companies (see for example Feinberg and Keane (2006) Hanson, Mataloni and Slaughter (2005)). Then, sales abroad of final goods and exports of intermediate goods can be complements. Several studies have examined the empirical relationship between production abroad and exports.
From a panel of Japanese firms over time, Head and Ries (2001) show that overseas investment and exports are complements. This result is obtained for the entire sample and by controlling for fixed firm effects. However, when the sample concerns the large assemblers that are not vertically integrated, the production of plants located abroad and the exports are substitutes. By using data published by the BEA which varies by country and over time (between 1977 and 1994), Clausing (2000) finds evidence that US multinationals activity and US exports are complements. Indeed, by using a gravity equation specification of trade, the author shows that a rise in affiliate local sales net of the value of imports from the US parent company increases the US exports. This result is robust when country-specific effects are controlled for.
It is clear that the relationship between foreign investments and exports depends on the level of aggregation of data. The studies using firm-level data underestimate the complementary effect since firms may purchase a number of inputs from independent suppliers that are set up in their domestic country. At the opposite, when

676

P. Blanchard et al.

data are not disaggregated, the complementary effect is overestimated. Swenson (2004) examines how the change in US imports of product k from country i is related to changes in FDI stocks measured at three aggregation levels: product k (3-digit), 2-digit industry which produces k and overall manufacturing. By controlling for endogeneity, the empirical analysis reveals that US imports and foreign investment in US are substitutes at the product-level while, at the overall manufacturing level, they are complements. Note that any nation or industry fixed effects drop out from the estimating equation.
Finally, Egger (2001) proposes a dynamic treatment of the bilateral economic relationship, which would allow a useful distinction between short-run and longrun relationships. The analysis is based on a dynamic bivariate panel framework. The data cover the period 1986­1996 for bilateral relationships between the 15 EU members. The empirical model is given by:

dEXi jt = 0 + 1dEXi jt-1 + 2dFDIi jt-1 + 3Zi jt + t + i jt dFDIi jt = 0 + 1dEXi jt-1 + 2dFDIi jt-1 + 3Zi jt + t + ui jt

(20.17)

where dEXi jt and dFDIi jt are first differences of exports and stocks of outward FDI from country i to country j at period t, respectively. The use of first differences as well as the Hansen (1982) two-step generalized methods of moments controls for the correlation between lagged endogenous regressors and the error term. Note that exports is included in the FDI equation as a lagged variable. The explanation offered by the author is as follows: before setting up a plant in a country to serve this market, firms look at their export performance. Then, Zi jt is a vector of variables similar to (20.15) and (20.16). Finally, t and t are time-specific fixed effects. These effects take into account business cycles affecting Europe as a whole. The estimated coefficients of the lagged endogenous variables are significant suggesting that adjustment costs play is a major role in FDI and exports. The estimation results indicate also that outward FDI does not influence exports in the short-run, and viceversa. Consequently, it is difficult to reach a clear conclusion on the complementary or substitutive nature of FDI and exports.

20.4.1.4 Exports and FDI: The Role of Distance
Among the key determinants of the decision to produce abroad, some variables such as distance and sunk costs do not vary over the time. However, using first differences or within transformation does not permit to measure the impact of all time invariant factors.8 In addition, these explanatory variables are likely to be correlated with the time effect. As a result, the Hausman­Taylor model should allow for testing the role of the distance in FDI and using time effects. The difficulty arises from the choice of variables which are considered as doubly exogenous (not correlated with the unobserved effects) and as singly exogenous (correlated with
8 Note that Carr, Markusen and Maskus (2001) do not take into account this problem when they estimate the model (20.16).

20 Foreign Direct Investment: Lessons from Panel Data

677

the unobserved effects). Although this econometric issue is very important, empirical papers presented in Sect. 20.4.1. do not take account of this bias. There are few papers using the Hausman­Taylor model to study the determinants of FDI. Egger and Pfaffermayr (2004a) is a notable exception.9 Their data concern FDI from the US and Germany to other countries between 1989 and 1999. By controlling for (fixed) time effects and (random) industry-country pair effects, the authors find that distance has a significant and positive impact on outward FDI and that exports and outward FDI are complementary in the US and (weakly) substitutes in Germany. Moreover, an over-identification test suggests that distance and relative factor endowments are singly exogenous.

20.4.2 Testing the Role of Trade Policy in FDI
Almost all empirical contributions reviewed in the previous subsection consider that tariff-jumping is an important motive for FDI. The role of tariff barriers in the decision to produce abroad has also received specific attention from several empirical analysis. These studies are important since they test the ability of policy makers to influence international trade and FDI. The recent interest in the impact of trade policy on the decision to produce abroad arises also from important reductions in tariffs, quota and voluntary export restraints (VERs) and from an increasing number of countries with anti-dumping laws, because of numerous multilateral trade agreements (see Baccheta and Bora (2001) and Blonigen and Prusa (2003)). Consequently, it is not surprising that the tariff-jumping FDI analysis has concerned three main aspects: (i) anti-dumping (AD) policies; (ii) the threat of a protectionist policy (the so-called quid pro quo FDI hypothesis); and (iii) the transition periods of trade liberalization. Again, the use of panel data econometrics is crucial in these three domains. Indeed, testing the role of (i), (ii) and (iii) requires data that vary over time and information at product/firm level.
20.4.2.1 Effects of Anti-dumping Laws
As stated by Blonigen and Prusa (2003), Since 1980, GATT/WTO members have filed more complaints under the AD statute than under all other trade laws combined. . . . So, among other related questions, a growing number of empirical works using panel data study the effects of antidumping actions on FDI (these are mostly oriented toward Japanese firms).
By using a panel of 7 countries (6 EU members plus the US) over the period 1980­1991, Barell and Pain (1999) estimate a model which relates Japanese direct investment flows in country j at time t to a variable10 denoting the "discounted stock" of anti-dumping cases (SAD jt ) in the EU or in the US, where
9 More precisely, a seemingly unrelated regression Hausman­Taylor model is considered because they specify a system of two equations, exports and outward FDI, as in Egger (2001). 10 In addition to several other regressors, such as market size and relative labour cost.

678

P. Blanchard et al.

 SAD jt  AD jt + i (AD jt-i) /i.

(20.18)

In this way, past anti-dumping actions may have a persistent, but progressively weaker, effect on Japanese FDI. The model is estimated by using the within estimator. The main result is that the level of AD has a positive effect on FDI. However, the authors use a very aggregated and quite short panel data (N = 7 and T = 12), even if they test carefully for the presence of heteroscedasticity and serial correlation of errors (see their Appendix A).
A more convincing analysis is provided by Blonigen (2002).11 He first observes that: In August 1993, Eastman Kodak Company filed a US antidumping petition against US imports of photographic paper originating from plants owned by Fuji Photo Film in Japan and the Netherlands. . . . While this led to an ensuing suspension agreement that led to substantially lower imports for a brief period, Fuji soon located a photographic paper manufacturing plant to the United States. . . . AD duties may result from a complex mechanism which requires the use of very disaggregated data. They are observed by all firms, they may change over time when the foreign firm modifies its dumping behavior (it may obtain refunds of AD duties in some cases) or if the US Department of Commerce changes the way it fixes AD duties. For this reason, Blonigen uses a panel data including firm and product combinations involved in US anti-dumping investigations from 1980 through 1990. By using a probit model, the author evaluates the probability for a Japanese firm, subject to anti-dumping duties, of locating its production for a given product in the US. From a technical point of view, the model used is a pooled probit which includes industry dummies in order to control for unobserved industry characteristics. The main result is that AD duties have a significant but small effect on FDI probability. Moreover, this effect is stronger when the firm has previous multinational production experience.
One interesting variant of Blonigen's approach may be found in Girma, Greenaway and Wakelin (2002). The authors introduce a time dimension in the panel and apply a different estimation strategy. Their basic model explains the presence (measured in terms of employment or fixed assets) of Japanese firms in the UK by a set of explanatory variables including the cumulated number of anti-dumping cases against Japanese firms measured as in (20.18). In this way, past anti-dumping actions may have a persistent, but progressively weaker, effect on Japanese FDI. The panel consists of 223 sectors observed over 1988­1996. The variables are constructed by aggregation of firms data. This permits a better evaluation of the tariffs and cumulative anti-dumping variables, as well as accounting for their time variabilities. However, for 146 industries, the dependent variable is equal to zero. So, the authors use Heckman's two-step estimation method. In the first stage, they estimate the probability of having Japanese FDI in the sector by using a probit model. In the second stage, they restrain their sample to the sectors with strictly

11 Belderbos and Sleuwaegen (1998) follow broadly the same approach with data on Japanese FDI in the EU. They use a panel consisting of 131 firms and 345 (electronic) products, and so observed at a very disaggregated level. The authors confirm that VERs, antidumping actions and tariffs favor Japanese FDI and have a negative effect on firm-level exports to Europe.

20 Foreign Direct Investment: Lessons from Panel Data

679

positive FDI and explain the level of FDI in these sectors by unit labor costs and the cumulative number of anti-dumping cases. Additionally, in order to control for a selectivity bias, the inverse Mills ratios estimated at the first stage are introduced in the second stage. The model also includes time dummy variables which capture some UK business cycle effects. The main result is that Japanese FDI in the UK depends significantly on anti-dumping actions, and, to a more limited extent, on VERs and tariff barriers.

20.4.2.2 Effects of the Threat of Protectionism

In the literature on the quid pro quo FDI hypothesis (see Bhagwati, Dinopoulos and

Wong (1992) and Grossman and Helpman (1996)) FDI may be caused by the threat

of protectionism, and not only by actual protectionism as in the tariff-jumping anal-

ysis. Foreign investment may be used by international corporations as an instrument

to defuse a possible protectionist action. In this case, when a firm establishes an

overseas local production unit and creates jobs, the host country has less incentives

to adopt protectionist measures.

Testing the quid pro quo hypothesis is difficult because the threat of protection

is not observed and must be distinguished from actual protection.12 Blonigen and

Feenstra (1997) have proposed a solution using a less aggregated panel dataset of

Japanese FDI in the US across 4-digit manufacturing industries from 1981 to 1988.

First, they define the threat of protection in industry i and year t - 1 as a latent

variable (Z) defined by

Zit-1 = wit-1 + it-1

(20.19)

i.e. it relies on a set of variables w (including real Japanese import growth, US real GNP growth). Now, consider that we observe at time t - 1 if an US anti-dumping action is being engaged (Zit-1 = 1) or not (Zit-1 = 0) by the administration against Japanese firms in a given industry. Suppose also that,

Zit-1 = 1 when Zit > 0 Zit-1 = 0 when Zit < 0

(20.20)

Hence, a US anti-dumping action (Zit-1) at time t - 1 is an indicator of the threat of protection (Zit ) at time t. In a first step, model (20.19) is estimated as a pooled probit model (a random effects model would be probably a better solution) which
allows the authors to compute the predicted probability of protection Zit-1. Finally, this variable is introduced in the Japanese FDI equation:

FDIit = xit  + Zit-1 + it

(20.21)

12 A previous attempt to evaluate quid pro quo FDI was done by Ray (1991). Unfortunatly, the analysis is conducted at industry level, probably inappropriate because trade protection is more often product-specific.

680

P. Blanchard et al.

where xit contains a variable measuring actual protection in addition to Z. Then, this specification evaluates separately actual protection and threat effects. Equation (20.21) can be estimated in a convergent way by OLS if it-1 is independent of it .13 In fact, as FDI values are not systematically reported in their database (ITA), the authors choose to specify their dependent variable in the second equation as the discrete number of FDI occurrences in a 4-digit industry in year t. As a result, a random effects negative binomial specification14 is adopted which is an extension of the Poisson model by introducing an individual unobserved effect in the conditional mean (each industry is assumed to be characterized by a specific propensity to do FDI). The main result is that Japanese FDI are highly sensitive not only to the actual anti-dumping measures but also to the threat of such measures.
Finally, notice that without the use of panel data (sectors/firms and time), it is probably impossible to split tariff-jumping and qui pro quo effects. Nevertheless, the random effects negative binomial specification requires that xit to be strictly exogenous conditional on the unobserved effects. As R&D expenditures are included in the regressors, this may raise some problems. As suggested by Hausman, Hall and Griliches (1984), it would be useful to estimate a fixed effects negative binomial model which allows for dependence between xi and the unobserved heterogeneity term.

20.4.2.3 Effects of Periods of Trade Liberalization
Another way to assess the impact of tariffs on FDI is to study how MNFs react during trade liberalization periods or when regional economic integration occurs. Over the past years, there has been an important increase in efforts among countries to achieve regional economic integration. Trade agreements largely differ on the degree of integration they imply: free-trade areas (NAFTA-1994, EFTA-1960); customs unions (Mercosur-1995), common markets (European Single Market-1992); or economic unions (Maastricht Treaty on the European Union-1998). Most studies on the relationship between regional integration and FDI have focused on the EU and NAFTA experiences.
Concerning NAFTA, an interesting analysis is due to Feinberg and Keane (2001). They analyze the effects of US and Canadian tariff reductions on the production location decisions of 701 majority-owned US-based MNF parents and their Canadian affiliates.15 Their study has two main interesting features. First, data are observed at a firm level (and not as usually at a more aggregated one industry and/or country level) over a relatively large period (1983­1992) that includes both the Tokyo Round and the Canada-US Free Trade Agreement. Such panel data allow authors to examine the effects of tariff reductions on changes in MNF production-location and, at the same time, to control for time, firm and industry effects. Secondly, the
13 See the discussion in Blonigen and Feenstra (1997), and especially footnote 10. See also Maddala (1983). 14 See Hausman, Hall and Griliches (1984). 15 They observe that US and Canadian tariffs dropped by approximately 62.5% from 1983 to 1992.

20 Foreign Direct Investment: Lessons from Panel Data

681

authors use a random coefficient approach.16 The regression model is expressed as follows:

Yit = 0 + (1 + i1)CTit + (2 + i2)U Tit + (3 + i)TRENDt + 4Zit + i + it
with it =it-1 + it

(20.22)

where i1 N(0, 21 ), i2 N(0, 22 ) and i N(0, 2). The variable Yit is defined in different ways, for instance exports from Canadian affiliate i to its US parent or exports from US parent i to its Canadian affiliate. CTit and UTit are respectively Canadian and US tariffs in the industry to which firm i belongs at time t, and Zit includes others exogenous variables like transport costs, relative factor costs, GDP for each country and manufacturing wages. Such a specification has several advantages. First, the i's may capture across-firm heterogeneity in tariff responses whereas i and i control for heterogeneity in the time trend (business cycle) for the former and for unobserved time-invariant firm specific characteristics for the latter. Note also that this specification is quite parsimonious if we compare it to a fixed effects approach. Second, once the population mean for each  and the variance of the is are estimated, the authors construct estimates (a posteriori) of the individual firm i. Then they compute the mean of each i within several industries defined at the disaggregated 3-digit level. Lastly, they decompose the total variance of the firm-specific i between across- and within-industries. The main results are twofold. First, the effect of Canadian tariff reductions on US parent exports to Canadian affiliates is very low (a 1% reduction in the Canadian tariffs increases US parent sales to Canadian affiliates by 1.6% on average, and moreover, the coefficient is significant only at the 20% level). At the same time, reductions in the US tariffs imply a greater Canadian affiliate production for sales into the US. Hence, trade liberalization appears to have been trade-creating. . . and does not induce a "hollowing out" of Canadian manufacturing. Second, within-industry (firm) effects explain more than 75% of the variance in the random tariff coefficient. So, firms' response to a change in the tariff depends heavily on unobserved firm characteristics (technology and organization). Industry characteristics (scale economies and product differentiation) are not a major determinant of the pattern of adjustment even if the industry is narrowly defined. Nevertheless, as noted by the authors themselves, the random coefficients specification may not be adequate if adjustment costs in production are high and if a negative cross-sectional correlation exists, at the preliberalization period, between tariffs and trade flows.17 In this case, a fixed-effects Tobit model may be a solution, but its estimation is computationally more difficult.

16 In fact, the model is a random effects tobit model, estimated by ML, because some Yit are equal to zero, when, for instance, affiliates produce and sell all their production in Canada.
17 See footnote 16, p. 127 in Feinberg and Keane (2001) for more details.

682

P. Blanchard et al.

Concerning the EU, there exist many studies on the impact of the various stages

of the integration process on FDI.18 Typically, regional trading groups, currency

unions. . . are captured by dummy variables. More recent works on this topic rely

on two further considerations. First, an integration process takes time to be imple-

mented and absorbed by the economies. So, integration effects must be modeled

in a dynamic way, in order to distinguish between short-run and long-run effects

and between announcement and post integration effects. Second, as it is necessary

to control for many unobserved factors (host and home countries, time, integration

phases effects), many dummies have to be introduced in the model, which can lead

to a serious loss of degrees of freedom and/or multicollinearity problems. Several

recent papers deal with these issues.19

Egger and Pfaffermayr (2004-b) try to isolate the impact on FDI of three EU

integration phases: the Single Market Program, the 1995 enlargement of the EU and

the Agreements between the EU and the Eastern European countries. They use a

FDI gravity model with bilateral and time effects in which they add 20 bilateral

integration group effects (e.g. EU 12, EFTA, rest of the World, CEEC) interacted

with all the three phase dummies (1986­1992, 1993­1994 and 1995­1998) that

gives 60 integration dummies. The model, estimated by the within estimator20, may

be defined as:

FDIi jt = xi jt  +  + t + i j + kp + ui jt

(20.23)

where p (= 1, 2, 3) represents the integration phase and k (= 1, . . . , 20) the country group. The estimation period is 1986­1998 and the unbalanced panel contains 3642 observations (with 13 home and 55 host countries). The main conclusion is that the integration effects on FDI are substantial and positive, but largely anticipated by the countries. Once the integration process is officially completed, regional integration has no more effects on FDI. However, the difference-in-differences estimator does not eliminate factors evolving differently over time between countries. So, if unobserved heterogeneity remains in the data, omitted variable bias may be a real problem. Moreover, as the number of countries is not "large" in this work, correct inference may be complicated (see Wooldridge (2003)).
In a very detailed work, Nicoletti, Golub, Hajkova, Mirza and Yoo (2003) use new structural policy indicators constructed by the OECD to estimate the impact of various trade policies on trade and FDI. Among many factors (FDI restrictions, bilateral tariffs and non-tariff protection), they study the role of belonging to a free trade area on FDI. They estimate two bilateral equations of FDI (one for outward stocks, one for outflows). The general model is

18 One of the first attempts is given by Brenton, Di Mauro and Lucke (1999) who unfortunatly do not use the panel dimension of their data. 19 There is a growing empirical literature on this subject, e.g. Girma (2002), Mold (2003), Altomonte and Guagliano (2003), Bevan and Estrin (2004), Carstensen and Toubal (2004) and Yeyati, Stein and Daude (2003) among others. More recent papers take into account the endogeneity of free trade agreements (Baier and Bergstrand (2007)) by of estimating a model on panel data with IV and control-function techniques. 20 Also called, in this context, the difference-in-differences estimator.

20 Foreign Direct Investment: Lessons from Panel Data

683

   FDIi jt = x xXi jt + c cCit + p pP jt
+ i +  j + it + i j +  jt + ui jt

(20.24)

where i (resp., j) represents the home (resp., host) country, FDIi jt is the log of bilateral FDI outward stocks or flows at time t, Xi jt are country-partner pair specific variables,Cit are country specific variables, and Pjt are partner pair specific variables. As in Egger and Pfaffermayr (2004-b), the model contains many dummies in order to control for observed and unobserved factors relative to time and (host and home) countries. Nevertheless, Nicoletti et al. adopt a different estimation strategy. Host-specific and home-specific effects are eliminated by using "transformed least squares", i.e., by expressing the data as deviations from the average home country or the average host country. In this way, all home and host specific dummies are removed from the model. They use OECD data described in the previous section, so potentially 28 × 27 × 21 = 15876 observations are available, but, due to numerous missing values, only about 4500 are used in the estimations. The main conclusion is that participation in free-trade agreements has had significant quantitative effects on FDI, particularly within the EU. For instance, they estimate the increase in FDI stocks to be up to 100% for Czech Republic, Hungary and Poland between 1990 and 2004.
As shown by the two previous works, dynamic aspects (e.g. anticipations) seem to play a major role when one tries to assert effects of regional integration on FDI.

20.4.3 Testing the Relationship Between FDI and Exchange Rate
In the second half of the 1980s the value of the yen increased while the dollar experienced a sharp depreciation. This phenomenon could explain why Japanese FDI increased rapidly in the US during this period. However, the relationship between the exchange rate and FDI is not evident. Under the assumption of a perfect international credit market, firms have the same advantage/disadvantage to purchase any particular asset abroad or at home. In other words, entrepreneurs are able to borrow at the same opportunity cost whatever their location and their nationality. Consequently, the variations of exchange rates do not affect the structure of the private capital account of countries' balance of payments between portfolio investment and FDI. Since the beginning of the 1990s, imperfections in the capital market have become the main argument used in the literature to justify why it is necessary to revisit the relationship between exchange rates and FDI. This question is discussed in the next subsection. We will report empirical studies that highlight the role of imperfections on the product market, on the one hand, and, the volatility of the exchange rate, on the other hand, in the relationship between exchange rate and foreign investments.

684
20.4.3.1 Role of Imperfections on Capital Markets

P. Blanchard et al.

Two types of capital market imperfections play a key role in the relationship between the decision to produce abroad and the exchange rate: the existence of asymmetric information and the capacity of banks to grant loans.
Froot and Stein (1991) propose an adverse selection model where there exists asymmetric information between lenders and borrowers/firms about the future profit from an investment project. Moreover, the creditors incur a monitoring cost if they want to observe the profit realized by the borrowers. This monitoring cost is what causes external resources to be more expensive than internal resources and explains why firms do not finance the whole of their investment by loans. The investment project concerns the purchase of a domestic firm either by another domestic company or by a foreign multinational firm through a bidding process. As the domestic currency experiences a real depreciation, the self-financing capacity of the MNF grows relative to that of the other domestic bidder, so that (ceteris paribus) the MNF increases its probability of winning the auction. From this analysis, the link between real exchange rate and FDI is obvious.21
To verify the validity of these different theoretical arguments, Froot and Stein (1991) use annual panel data coming from the International Trade Administration (ITA) of the US Trade Department and for the period 1977­1987. From this database, the authors examine whether the wealth effect may be differentiated across industries or across different types of FDI. Indeed, the US FDI inflows are disaggregated by source country, and by industry as well as by type of purchases/transactions (plant acquisition or expansion, merger and acquisition, joint-ventures). Results suggest that the real exchange rate has not the same effect on the different parts of the total foreign capital inflows into the US. The dollar variations only have a significant effect on inward direct investments, as expected. The estimates of the real exchange rate effects seem more convincing at the level of the different types of FDI transactions. The exchange rate has a statistically significant impact with the right sign on FDI associated with mergers and acquisition operations.22
Nevertheless, as the model is estimated by pooled OLS, it does not take into account individual or time effects. This limit is important since no other variables are introduced in the model which control for the alternative explanations of FDI (such as distance, trade costs).
This criticism has been removed by Klein and Rosengren (1994). They adopt the approach retained in Froot and Stein (1991) but consider a fixed-effects specification to take into account the heterogeneity between the source countries. Moreover, considering that the ITA data used by Froot and Stein (1991) are not necessarily comprehensive, Klein and Rosengren prefer to complete their empirical analysis

21 This analysis does not hold for other types of inward investment such as foreign investment in Treasury securities or in corporate stocks and bonds. For these portfolio investments, the monitoring costs are expected to be small and, thus, uncorrelated with the real exchange rate. 22 The exchange rate has also a significant impact on joint-ventures and new plant FDI. This last result is problematic since the bidding approach developed by authors does not really concern these types of transaction.

20 Foreign Direct Investment: Lessons from Panel Data

685

by using the BEA measure of FDI although this includes foreign acquisitions of existing American-target firms and the establishment of new plants by MNF.23 The available sample for the BEA series is over the 1979­1991 period while the ITA annual data concern 1977­1987. As in Froot and Stein, the real exchange rate has always a statistically significant impact with the right sign not only on FDI, as a whole, but also on foreign mergers and acquisitions operations. Note that in accordance with the theoretical conclusions of Froot and Stein, the effect is lower on FDI than on mergers and acquisitions. On the other hand, as a log­log specification is only used by Klein and Rosengren, it is difficult to know whether the estimation of the fixed-effects model really modifies the values of the parameters compared to the pooled OLS estimation. In order to control for alternative explanations for FDI, Klein and Rosengren introduce as a regressor in their model the relative-labor-cost between the US and the source countries. They find that the wealth effect is always at work for FDI through mergers and acquisitions in the US while the relative-laborcost has no impact. Then, these different results suggest the empirical validity of the conclusions drawn by Froot and Stein. However, the potential correlation between the disturbances of the models relative to the different type of FDI is not taken into account through, for example, SUR estimation.
However, during the mid-1990s, Japanese FDI fell whereas the yen appreciated significantly. To explain this feature, Klein, Peek and Rosengren (2002) focus on the role played by the financial intermediation. In a country where the relationships between firms and banks are very close, the financial intermediation is dominant. In this context, firms' ability to engage in FDI is influenced by the capacity of banks to grant loans. It is the relative access to credit (RAC) hypothesis. In the 1990s, the Japanese bank sector experienced a collapse causing Japanese firms to be constrained in the financing of their investment projects. Thus, the value of Japanese FDI as a share of total inward US FDI reached a peak of 30% in 1990 and then declined during the following years by only 1% of total inward US FDI by 1998. It is the validity of the RAC hypothesis that is tested by Klein, Peek and Rosengren (2002).
A database is constructed by Klein et al. from firm-level FDI ITA over the period 1987­1994. They use the number of FDI projects since the amount of FDI is not systematically available. Moreover, Japanese firm characteristics (size, profitability, market value and industry) come from the Pacific-Basin Capital Markets Databases. From the Japan Company Handbook, are identified the 11 primary (first referenced) banks of the Japanese firms included in the sample. During the sample period, few Japanese firms change their primary bank. In order to obtain an independent/objective evaluation of the banks' financial health and their evolution over the sample period, Klein, Peek and Rosengren (2002) use the time series of Moody's long-term deposit ratings. Thus, the authors exploit the time-heterogeneity between banks although all of them experienced a downgrade in their Moody's ratings during the last years of the period. The empirical model is represented as follows:

23 Despite the difference in definition of FDI, the correlation between the BEA measure and the ITA measure is quite high (0.86). This result confirms the preponderance of mergers and acquisitions in the US inward FDI.

686

P. Blanchard et al.

RATEFDIit = 0 + 1DPROFITit-1 + dratingit-1 + dmacroit-1 + it

(20.25)

where it N(0,  2). In this specification, the dependant variable, RATEFDIit , is the variation rate in the number of FDI projects toward the US, financed by Japanese primary bank i during year t. dratingit-1 contains two measures of changes of Moody's long-term deposit ratings for the Japanese main banks. A first dummy variable takes the value 1 if the bank i has a change in its rating, during year t - 1, and 0 otherwise. Its effect on FDI is a priori negative. A second dummy variable is introduced. Its coefficient is also expected to be negative because this variable takes the value 1 when there are two or more downgrades and 0 otherwise. DPROFITit-1 corresponds to the variation of the profit sum of the firms associated with bank i. This variable measuring the change in the health of firms is assumed to favor FDI. dmacroit-1 contains a set of three macroeconomic variables intended to control for differences in wealth and economic activity between Japan and the US. The first variable is introduced to control the variation of wealth between both countries in the spirit of Froot and Stein (1991). The impact of this variable on FDI should be positive. The change in the US unemployment rate and the change in the Japanese job-offers-to-applicants ratio are used to control for the macroeconomic business cycle in both countries. The effect of these two variables on Japanese FDI to United States is assumed negative.
The coefficients estimated are in accordance with the expectations. Thus, the multiple-level (single-level) downgrade of a bank during the period causes a 70% (30%) reduction in the number of Japanese FDI projects that use this bank as their main lender. On the other hand, the wealth effect is not statistically significant, weakening the argument developed by Froot and Stein (1991). Then, the decreasing number of the Japanese FDI projects in the US over the 1990s seems to be explained by the collapse of the Japanese banking sector rather than by the loss of competitiveness of the Japanese Firms. Moreover, to show the robustness of their estimates, Klein, Peek and Rosengren (2002) provide estimates from two restricted samples including only multiple-year FDI firms or banks financing the most FDI projects. In fact, the individual effects, which are likely to be correlated with some regressors, have not been removed by the variable transformation used. Indeed, the first difference transformation is not applied to the explained variable since DFDIit is a rate. Therefore, the estimation method is pooled OLS which is biased and inconsistent.

20.4.3.2 Role of Imperfection on Product Markets
The relationship between FDI and the exchange rate can also be explained by imperfections in the product market. Blonigen (1997) establishes three conditions for the existence of a specific relationship between the (real) exchange rate and FDI. First, the opportunity to purchase a target firm which owns a specific asset. The transfer of this specific asset is realized at a low cost between different facilities whatever their nationality. The target firm may be bought either by a (US) domestic firm or by

20 Foreign Direct Investment: Lessons from Panel Data

687

a (Japanese) foreign one. Second the domestic and foreign markets are segmented.

This market imperfection challenges the law of one price and price adjustments that

could compensate for a change in the nominal exchange rate. Third, the access to

the foreign market must be limited to the domestic firm. Otherwise, both acquiring

rivals would have the same return on the specific asset abroad. Then, the domes-

tic firm knows entry barriers on the foreign market. These three conditions being

verified, a real depreciation of the domestic currency (the US dollar) relative to the

foreign one (the yen) leads to an increase in the surplus of the foreign firm. Con-

sequently, the foreign firm has an incentive to make a higher bid than its rival for

buying the target firm. The direct consequence is that a greater foreign acquisition

of the US assets must be expected during a period of real dollar depreciation, other

things being equal. This analysis exclusively concerns the inward FDI associated

with mergers and acquisitions operations.

From a balanced panel of 361 industries both manufacturing and nonmanufactur-

ing over the period 1975­1992, Blonigen (1997) analyzes the positive relationship

between the number of Japanese acquisitions by industry and by year (NFAit ) into the US and the real exchange rate (RERit ) at industry level.24 The specification has

the following form:

Pr(NFAit ) = f (RERit , it , it )

(20.26)

where it includes variables having an important role in this approach: (i) the number of acquisitions of US target firms by other US firms (proxying the supply of specific assets on the US market); (ii) the share of Japanese value added in each industry (for the US market penetration of Japanese firm); (iii) the annual real growth of Japanese GDP (a proxy for Japanese demand for specific assets); (iv) the annual growth in the Tokyo Stock Price index (its effect is assumed positive). This variable is used as a proxy for the outgrowth of the speculative "bubble" economy of Japan in the late 1980s and early 1990s. The variables included in it must control for other explanations found in the traditional literature on FDI, analyzed previously.
The data on the number of foreign acquisitions are typical of count data which can vary from zero to several or even many, for some industries. The negative binomial model (Hausman, Hall and Griliches (1984)) is used for estimation and represents a generalization of the Poisson distribution with an additional parameter allowing the variance to exceed the mean. Indeed, for the manufacturing and nonmanufacturing sectors over the 1975­1992 period, the number of Japanese acquisitions (NFA) is ranged from 0 to 89, with a mean of 16 and a standard deviation of 20. Beyond this problem of overdispersion (or underdispersion), the model includes individual fixed or random effects to take into account the cross-sectional heterogeneity. From this model and under the assumption of fixed effects, Pr(NFAit ) can be written as,

Pr(NFAit )

=

(it + NFAit ) (it (NFAit + 1)

i

it

1 + i

1

NFAit

1 + i

(20.27)

24 The dollar value for acquisitions is not retained as a dependent variable since it is missing for over one-third of the observations.

688

P. Blanchard et al.

where  is the gamma function. The parameter i is the individual effect while it depends on the covariates by the following function:

ln it = RERit + it  + it 

(20.28)

Under the assumption that NFAit are independent over time, t NFAit also has a negative binomial distribution with parameters i and t it . In a context of random effects now, to permit a tractable negative binomial mass function, i/(1 + i) is assumed to be distributed as a beta random variable with shape parameters (a, b). The model with fixed or random effects can be estimated via the maximum-likelihood method Greene (2004).
The estimates of both types of models support the main hypotheses formulated by Blonigen (1997). In other words, a real appreciation of the yen relative to the dollar leads to an increase in the number of Japanese acquisitions in the US. This result holds for manufacturing industries rather than for nonmanufacturing and for the industries with high levels of R&D. Moreover, the parameters associated with the US supply and Japanese demand of specific assets have the positive expected sign. On the other hand, the alternative explanations of Japanese FDI based on the US tariff-jumping and the Japanese speculative bubble are unsatisfactory. Therefore, these different results exhibit in accordance with Froot and Stein (1991) a wealth effect. However, this effect is mainly present in industries where specific assets like innovation are present and it concerns foreign mergers and acquisitions, the most important component of FDI. Then, this analysis goes further into the specific relationship between FDI and exchange rate.25
However, no proxies are introduced to estimate the level of entry barriers in the Japanese markets for the US firms while this is one hypothesis among the most important of this original approach. Moreover, the estimates from random and fixed effects models are both reported but they are not compared using a Hausman test to indicate whether the industry-specific effects are correlated with the regressors. Without results of this specification test, it is difficult to deduce the appropriateness of both models. Another important issue is how to introduce the fixed effects in the negative binomial model. In this case, the fixed effects are conditioned out of the likelihood function (see Allison and Waterman (2002)).

20.4.3.3 Role of the Exchange Rate Volatility
Since the 1970s and the end of generalized system of fixed exchange rates, economists have also devoted much attention to the effects of exchange rate volatility on FDI. The effect of volatility of the real exchange rate on FDI depends on whether firms may choose to export or to invest abroad, on their behavior towards risk and on their expectations about the future profits from FDI.
25 These results are broadly supported by the empirical analysis of Guo and Trivedi (2002) that assigns the industries to high- and low-sensitivity FDI categories relative to the exchange rate movements.

20 Foreign Direct Investment: Lessons from Panel Data

689

Following Dixit (1989), Campa (1993) develops an option model to explain why

the MNF make FDI or not (see also Altomonte and Pennings, 2003). In this frame-

work, as opposed to the traditional theory of investments under uncertainty, the ex-

pected future profits of a MNF, assumed risk-neutral, take into account the exchange risk of entering the foreign market.26 Assuming the future values of the nominal ex-

change rate are lognormally distributed with a variance  that grows linearly with

the time horizon. Then, even if a positive drift may lead to an appreciation of the for-

eign profits in domestic currency, at the same time the volatility in the future of the

nominal exchange rate may be too important to discourage FDI. In fact, the MNF

retains an option to enter the foreign market at any moment in time. This option has

a price which is the sunk cost of entering the market through FDI. This sunk cost

makes an irrevocable commitment of the MNF when it exercises the option. More-

over, the value of the option­ equivalent to the value of the investment opportunity-

is the expected present discounted value of future profits from serving the foreign

market minus the amount of FDI realized at time t. Here, the decision to make FDI

is equivalent to deciding at which time to exercise such an option. The MNF holds

back for an extra period (e.g. stays out of the foreign market one more time) as long

as the expected change on the option's value is higher than the expected return of

the present time. From his theoretical model, Campa (1993) deduces some predic-

tions about the effects of exchange rate volatility on FDI. The higher the exchange rate R, the rate of change  and the uncertainty  , the more valuable the option to

enter is and the fewer events of entry observed. Furthermore, the lower the marginal

cost MC and the cost to entry in the foreign market k, the higher the expectation of

future profits from the activity abroad. Note that the marginal cost is not expressed

in foreign currency since Campa (1993) limits his empirical analysis to FDI related

to wholesale activities. Thus, the model concerns a MNF producing a good in the

home country and selling it in a foreign market via a sales subsidiary. The veri-

fication of these different predictions allows the author to construct the following

reduced form:

nit = f (,  , e f /$, k, MC)

(20.29)

where the explained variable, nit , is the number of MNF that enter the US wholesale trade industry i in a given year t. The construction of regressors  and  depends

on the MNF's expectations about the evolution of these two variables in the future.

These two variables correspond to the average and the standard deviation, respec-

tively, of the monthly change in the logarithm of the anticipated exchange rate. Two

types of anticipation are considered: perfect and static expectations. The exchange

rate level e f /$ is defined as the annual average of the exchange rate in units of domestic currency of the MNF per one unit US dollar, in the year of entry. The level of

the sunk costs k is proxied by two variables. The first regressor is the ratio of fixed

assets to net wealth of all US firms in an industry and the second is the ratio of me-

dia expenditures to turnover in each US industry. The marginal cost MC is proxied

26 Note that the uncertainty can also concern the foreign production costs (see Brandao De Brito and De Mello Sampayo, 2002)

690

P. Blanchard et al.

by the unit labor cost w since capital is assumed fixed, its cost being included in the entry cost k.
The database is a panel of 61 US wholesale trade industries, defined at the fourdigit level, for the period 1981­1987. This sample contains a total of 768 entries of MNF in the US. Even though the MNF come from 35 different countries, the geographical concentration of origin countries is high. Thus, Japan, the UK, Germany, France and Canada account for almost 80% of the 768 foreign entries on the US markets. The dependant variable nit is censored since it takes values from 0 to 40 with a large fraction of zeros. To take account of the truncated distribution of the dependant variable, Campa (1993) uses a Tobit estimation.
The estimates are in accordance with the predictions of the theoretical analysis. Thus, the estimated parameters have the right sign, except for . The uncertainty of the exchange rate has a significant negative effect on the number of MNF entering the US markets. The labor cost w is not significant in the estimates weakening the range of the theoretical model. Nevertheless, both proxies of the entry costs Sunk and Adv have significant negative coefficients while the level of the exchange rate R has a positive effect, as expected. However, this last result is the opposite of the conclusions of previous papers where the specific relationship between FDI and the level of exchange rate is explained through market imperfections. In fact, the wealtheffect argument developed by Froot and Stein (1991) is not valid in the present analysis. Indeed, this argument concerns only the FDI realized in manufacturing industries and it cannot be used for FDI in wholesale trade industries.

20.5 Some Recent Econometric Issues
The use of panel data presents specific features which introduce econometric complications. In this last section, we discuss two main problems that have been recently treated in the estimation of FDI models.

20.5.1 FDI, Panel Data and Spatial Econometrics
The first problem concerns the specification of the empirical model. Recent theoretical developments have stressed that a MNF may engage in FDI activities depending not only on home and host characteristics, but also depending upon the neighboring host's specificities. First, a multinational firm may use a host country as an export platform to other near markets for minimizing trade costs. Second, it may also split its production in several vertical units with respect to the relative factor costs between countries. By definition, an econometric bilateral model of FDI does not take into account the specificities of the neighboring host country. In order to control for the correlation between inward FDI of one country and FDI of its neighbors, we may use spatial panel data model estimation methods. Baltagi, Egger

20 Foreign Direct Investment: Lessons from Panel Data

691

and Pfaffermayr (2007) and Blonigen, Davies, Waddell and Naughton (2007) are two interesting studies on this topic. The first one analyzes US outward FDI stock in country-industry pairs (in 1989­1999) whereas the second one focuses on FDI from the US to 20 OECD countries (between 1980­2000). We can summarize their approach as follows (for simplicity, we present mainly the Blonigen et al. methodology, which is simpler ­ but less general ­ than Baltagi et al.). Consider that the data are sorted by time t (first sort key) and by host country j (second sort key), so we can omit the t index. For simplicity, we present the specification assuming that the panel is a balanced one (Nt = N),

fdi = X ×  +  × W × fdi + u

(20.30)

where fdi and u are N × 1 vectors (row j refers to the host country j) and X is a N × k matrix of regressors.  × W × fdi reflects the spatial autoregression term, where W(N × N) is a (row normalized) spatial lag weighting matrix (for each year
t) (see Chap. 19 in this volume for the expression of W) where the components
(w(di j)) are a weighted function depending upon the distance between country i and country j. According to Blonigen et al., a positive  means an agglomeration effect or vertical production organization while a negative  suggests FDI are used for export-platform reasons. Further, the spatial error term is defined as

u = Wu + 

with | | < 1. Notice also that the Baltagi et al. specification is more general since it includes country-industry-pairs effects and also spatially weighted average of regressors. Errors are spatially correlated when  = 0. However, recall that OLS estimators are still consistent but are inefficient. Finally, one interesting aspect of this model is that W × fdi is endogenous and correlated with u. To estimate this model, Blonigen et al. apply a maximum likelihood method while Baltagi et al. use the fixed and the random effects 2SLS estimator (using the second and third order spatial lags of the exogenous regressors as instruments).
In Blonigen et al. and Baltagi et al., estimations exhibit a significant spatial dependence, which is negative in the former article and positive in the latter article. In addition, spatial correlation of errors are only detected in Baltagi et al. Even though results are different in some respects, the estimation of a spatial panel data model of FDI is required in order to control for the correlation between the inward FDI of different neighboring countries.

20.5.2 Exchange Rate, Unit Roots and Cointegration
As many empirical analysis on FDI very often use non-stationary variables and models in levels, it is necessary to test for unit roots and cointegration in order to avoid spurious regressions. This is a particular problem in FDI-exchange-rate models while it is not treated in the empirical literature (see for example Froot and

692

P. Blanchard et al.

Stein (1991) and Klein, Peek and Rosengren (2002)). Up to now, there have been few studies analyzing FDI determinants on panel data with this methodology which, in the context of countries panel data estimation, presents several specific features.
On the one hand, it is frequently advanced that panel-based unit root tests have higher power than unit root tests based on time series. Indeed, as countries' data are used for FDI analysis, panel data sets are sometimes characterized by large N and large T dimensions. An illustration of such tests is given by Hsiao and Hsiao (2004) who use an (unbalanced) panel data with only 5 countries observed between 1987 and 2002. They apply several tests (for example IPS and ADF-Fisher tests27) and conclude that FDI is stationary, the exchange rate, GNP and wage differential variables are not. Moreover, they show that the first differences of the non stationary series are I(0), so these variables are integrated of order one. These results seem to be frequently encountered even if the opposite conclusion is often obtained in the literature about the stationarity of FDI.28
On the other hand, given this result, the estimation of FDI determinants, using cross-countries data over time, raises several other issues. First, if the micro relationships are made of I(1) variables where each country has its own specific cointegrating relation, it is probably better to estimate the model on each country separately. Nevertheless, with too few annual periods, making inference in such a context may be difficult. Second, a solution to deal with spurious regressions is to take first differences of the I(1) variables and to apply usual panel data estimators in a framework of pooling with I(0) variables. Nonetheless, in this case, variables that are constant in the time dimension are removed from the model and a part of the long-run information is removed. For these reasons, De Santis, Anderton and Hijzen (2004) and Hsiao and Hsiao (2004) suggest proceeding in two steps. First, panel cointegration tests are applied in order to guard against the spurious regression problem. Once again, using panel data may improve the small sample properties of such tests, even if there is not general agreement on this point. From several tests (e.g. multivariate augmented Dickey­Fuller, Im­Pesaran and Shin tests), they reject the null hypothesis that the residuals of the panel regressions are I(1), i.e. they reject the null hypothesis of no cointegration in their panel data. Second, they must decide how to do estimation and inference in panel data cointegration models.29 Among many possibilities (e.g. OLS, Mean group, FMOLS, Within estimators), the two studies choose to use the within estimator. Indeed, as the residuals of the within estimator are stationary, within estimates are probably not spurious.30 Moreover, when T is large and N is moderately large, Phillips and Moon (1999) shows that the within estimator consistently estimates in many cases the long-run effects and has a limiting normal distribution.

27 See Chap. 9 in this volume. 28 See Branda~o de Brito and Mello Sampayo (2004). 29 Branda~o de Brito and Mello Sampayo (2004) estimate the cointegration relationships and the error-correction mechanism equations for each country separately. As T = 7, it is not sure that such an approach is correct. 30 As the De Santis, Anderton and Hijzen (2004) model is a dynamic one, they applied also the Arellano-Bond estimator on first differences and find similar results.

20 Foreign Direct Investment: Lessons from Panel Data

693

Hsiao and Hsiao (2004) study the determinants of FDI flows in China from Hong Kong, Japan, Taiwan, Korea and the US between 1987 and 2002 (N = 5 & T = 14, 15 or 16) whereas De Santis, Anderton and Hijzen (2004) analyze the factors influencing stocks of FDI in the US from eight Euro area countries (1980­2001), so that N = 8 and T = 22. Even though the data sets used are different, the specifications are somewhat closed and include some common regressors (lag FDI, exchange rate, GDP and wage differential). Limiting the presentation to the exchange rate, in both studies, a negative and significant relationship is found between the exchange rate and FDI. Nevertheless, as lag FDI is included in the regressors list, it is questionable that the Phillips and Moon (1999) results apply.

Acknowledgments We would like to thank Joe Byrne for helpful comments on an earlier draft.

References
Allison, P. D. and Waterman, R. (2002). Fixed-Effects Negative Binomial Regression Models. In Stolzenberg R. M. (ed.), Sociological Methodology. Oxford: Basic Blackwell, pp 247­265.
Altomonte, C. and Guagliano, C. (2003). Comparative Study of FDI in Central and Eastern Europe and the Mediterranean. Economic Systems, 27:223­246.
Altomonte, C. and Pennings, E. (2006). The Hazard Rate of Foreign Direct Investment: A Structural Estimation of a Real Option Model. Oxford Bulletin of Economics and Statistics, 68(5):569­593.
Baccheta, M. and Bora, B. (2001). Post-Uruguay Round Market Access Barriers for Industrial Products. Policy Issues in International Trade and Commodities, Study Series No. 12, UNCTAD. New York and Geneva: United Nations.
Baier, S. C. and Bergstrand, H. (2007). Do Free Trade Agreements Actually Increase Members' International Trade? Journal of International Economics, 71(1):72­95.
Balestra, P. and Negassi, S. (1992). A Random Coefficient Simultaneous Equation System with an Application to Foreign Investment by French Firms. Empirical Economics, 17(1):202­220.
Baltagi, B., Egger, P. and Pfaffermayr, M. (2003). A Generalised Design for Bilateral Trade Flow Models. Economics Letters, 80(3):391­397.
Baltagi, B., Egger, P. and Pfaffermayr, M. (2007). Estimating Models of Complex FDI: Are There Third-Country Effects? Journal of Econometrics, 140(1):260­281.
Barell, R. and Pain, N. (1999). Trade Restraints an Japanese Direct Investment Flows. European Economic Review, 43(1):29­45.
Belderbos, R. and Sleuwaegen, L. (1998). Tariff Jumping DFI and Export Subsitution: Japanese Electronics Firms in Europe. International Journal of Industrial Organization, 16(4):601­638.
Bevan, A. and Estrin, S. (2004). The Determinants of Foreign Investment into European Transition Economies. Journal of Comparative Economics, 32:775­787.
Bhagwati, J. N., Dinopoulos, E. and Wong, K.-Y. (1992). Quid Pro Quo Foreign Investment. American Economic Review, 82(2):186­190.
Blonigen, B. A. (1997). Firm-Specific Assets and the Link Between Exchange Rates and Foreign Direct Investment. American Economic Review, 87(3):447­465.
Blonigen, B. A. (2002). Tariff-Jumping Antidumping Duties. Journal of International Economics, 57(1):31­49.
Blonigen, B. A., Davies, R. B. and Head, K. (2003). Estimating the Knowledge-Capital Model of the Multinational Enterprise: Comment. American Economic Review 93:980­994.

694

P. Blanchard et al.

Blonigen, B. A., Davies, R. B., Waddell, G. R. and Naughton, H. (2007). FDI in Space: Spatial Autoregressive Relationships in Foreign Direct Investment. European Economic Review, 51(5):1303­1325.
Blonigen, B. A. and Feenstra, R. C. (1997). Protection Threats and Foreign Direct Investment. In Feenstra, R.C. (ed.), The Effects of U.S. Trade Protection and Promotion Policies. Chicago: National Bureau of Economic Research and University of Chicago Press, pp 55­80.
Blonigen, B. A. and Prusa, T. J. (2003). Antidumping. In Choi, E. K. and Harrigan, J. (eds), Handbook of International Trade. Cambridge, MA: Blackwell Publishers, pp 251­284.
Brainard, S. L. (1997). An Empirical Assessment of the Proximity-Concentration Trade-Off Between Multinational Sales and Trade. American Economic Review, 87(4):520­544.
Brandao De Brito, J. and De Mello Sampayo, F. (2005). The Timing and Probability of FDI: An Application to the United States Multinational Enterprises. Applied Economics, 37(4):417­437.
Brandao de Brito, J. and Mello Sampayo, F. (2004). The Locational Determinants of the U.S. Multinationals Activities. Working Papers, No. 11­04, Banco de Portugal.
Braunerhjelm, P., Ekholm, K., Grundberg, L., and Karpaty, P. (1996). Swedish Multinational Corporations: Recent Trends in Foreign Activities. Research Institute of Industrial Economics (IUI), Working Paper No. 462.
Brenton, P., Di Mauro, F. and Lu¨cke, M. (1999). Economic Integration and FDI: An Empirical Analysis of Foreign Investment in the EU and in Central and Eastern Europe. Empirica, 26(2):95­121.
Campa, J. M. (1993). Entry by Foreign Firms in the United States Under Exchange Rate Uncertainty. Review of Economics and Statistics, 75(4):614­622.
Carr, D. L., Markusen, J. R. and Maskus, K. E. (2001). Estimating the Knowledge-Capital Model of the Multinational Enterprise. American Economic Review, 91(3):693­708.
Carstensen, K. and Toubal, F. (2004). Foreign Direct Investment in Central and Eastern European Countries: A Dynamic Panel Data Analysis. Journal of Comparative Economics, 32:3­22.
Clausing, K. A. (2000). Does Multinational Activity Displace Trade? Economic Inquiry, 38(2):190­205.
De Santis, R. A., Anderton, R. and Hijzen, A. (2004). On the Determinants of Euro Area FDI to the United States: The Knowledge-Capital-Tobin's Q Framework. Working Papers Series, No. 329, European Central Bank.
Dixit, A. (1989). Hysteresis, Import Penetration and Exchange Rate Pass-Through. Quarterly Journal of Economics, 104(2):205­228.
Egger, P. (2001). European Exports and Outward Foreign Direct Investment. Weltwirtschaftliches Archiv, 137(3):427­449.
Egger, P. and Pfaffermayr, M. (2003). The Proper Panel Econometric Specification of the Gravity Equation: A Three-Way Model with Bilateral Interaction. Empirical Economics, 28:571­580.
Egger, P. and Pfaffermayr, M. (2004a). Distance, Trade and FDI: A Hausman-Taylor SUR Approach. Journal of Applied Econometrics, 19(2):227­246.
Egger, P. and Pfaffermayr, M. (2004b). Foreign Direct Investment and European Integration in the 1990s. The World Economy 27(1):99­110.
Falzoni, A. M. (2000). Statistics of Foreign Investment and Multinational Corporations: A Survey. University of Bergamo, Centro de studi Luca d'Agliano and CESPRI.
Feenstra, R. C. (1997). U.S. Exports, 1972­1994: With State Exports and Other U.S. Data. NBER Working Paper 5990. Cambridge, MA: National Bureau of Economic Research, Inc.
Feinberg, S. E. and Keane, M. P. (2001). U.S.­Canada Trade Liberalization and MNC Production. Review of Economics and Statistics , 83(1):118­132.
Feinberg, S. E. and Keane, M. P. (2006). Accounting for the Growth of MNC-Based Trade Using a Structural Model of U.S. MNCs. American Economic Review, 96(5):1515­1558.
Froot, K. A. and Stein J. C. (1991). Exchange Rates and Foreign Diect Investment: An Imperfect Market Approach. Quarterly Journal of Economics, 106(4):1191­1217.
Girma, S. (2002). The Process of European Integration and the Determinants of Entry by Non-EU Multinationals in the UK Manufacturing. The Manchester School, 70:315­335.

20 Foreign Direct Investment: Lessons from Panel Data

695

Girma, S., Greenaway, D. and Wakelin, K. (2002). Does Antidumping Stimulate FDI? Evidence from Japanese Firms in the UK. Review of World Economics, 3:414­436.
Greene, W. H. (2004). The Behaviour of the Maximum Likelihood Estimator of Limited Dependent Variable Models in the Presence of Fixed Effects. Econometrics Journal, 7(1):98­119.
Grossman, G. and Helpman, E. (1996). Foreign Investment with Endogenous Protection. In R.C. Feenstra, G.M. Grossman, and D.A. Irwin (eds.), The Political Economy of Trade Policy, Cambridge, MA: MIT Press.
Guo, J. Q. and Trivedi, P. K. (2002). Firm-Specific Assets and the Link between Exchange Rates and Japanese Foreign Direct Investment in the United States: A Re-Examination. Japanese Economic Review, 53(3):337­349.
Hansen, L. P. (1982). Large Sample Properties of Generalized Method Moments Estimators. Econometrica, 50:1029­1054.
Hanson, G. H., Mataloni, R. J. and Slaughter M. J. (2005). Vertical Production Networks in Multinational Firms. Review of Economics and Statistics, 87(4):664­678.
Hausman, J., Hall, B. H. and Griliches, Z. (1984). Econometric Models for Count Data with an Application to the Patents-R&D Relationship. Econometrica, 52(4):909­938.
Head, K. and Ries J. (2001). Overseas Investment and Firm Exports. Review of International Economics, 9(1):108­122.
Helpman, E. (1984). A Simple Theory of International Trade with Multinational Corporations. Journal of Political Economy, 92(3):451­471.
Helpman, E., Melitz, M. J. and Yeaple, S. R. (2004). Export versus FDI with Heterogeneous Firms. American Economic Review, 94(1):300­316.
Hsiao, F. S. T. and Hsiao, M. C. W. (2004). The Chaotic Attractor of Foreign Direct Investment­ Why China? A Panel Data Analysis. Journal of Asian Economics, 15(4):641­670.
IMF and OECD (2001). Foreign Direct Investment Statistics: How Countries Measure FDI. Washington, DC: IMF and OECD.
Klein, M. W., Peek, J. and Rosengren E. S. (2002). Troubled Banks, Impaired Foreign Direct Investment: The Role of Relative Access to Credit. American Economic Review, 92(3):664­682.
Klein, M. W. and Rosengren, E. S. (1994). The Real Exchange Rate and Foreign Direct Investment in the United States: Relative Wealth vs. Relative Wage Effects. Journal of International Economics, 36(3­4):373­389.
Lipsey, R. (2001) Foreign Direct Investment and the Operations of Multinational Firms: Concepts, History and Data. NBER Working Paper 8665. Cambridge, MA: National Bareau of Economic Research, Inc.
Maddala, G. S. (1983). Limited-Dependent and Qualitative Variables in Econometrics. Cambridge, MA: Cambridge University Press.
Markusen, J. R. (1984). Multinationals, Multi-Plant Economies, and the Gains from Trade. Journal of International Economics, 16:205­226.
Markusen, J. R. (1995). The Boundaries of Multinational Enterprise and the Theory of International Trade. Journal of Economic Perspectives, 9:169­189.
Markusen, J. R. (1997). Trade versus Investment Liberalization. NBER Working Paper 6231. Cambridge, MA: National Bureau of Economic Research, Inc.
Markusen, J. R. (2002). Multinational Firms and the Theory of International Trade. Cambridge, MA: MIT Press.
Markusen, J. R. and Maskus, K. E. (2002-a). Discriminating Among Alternative Theories of the Multinational Enterprise. Review of International Economics, 10:694­707.
Markusen, J. R. and Maskus, K. E. (2002-b). A Unified Approach to Intra-Industry Trade and Direct Foreign Investment. In Lloyd, P.J. and Lee, H.-H., (eds), Frontiers of Research in IntraIndustry Trade. New York: Palgrave Macmillan, pp 199­219.
Markusen J. R. and Venables, A. (2000). The Theory of Endowment, Intra-Industry and MultiNational Trade. Journal of International Economics, 52:209­234.
Markusen J. R., Venables, A. , Eby-Konan, D. and Zhang, K. (1996). A Unified Treatment of Horizontal Direct Investment, Vertical Direct Investment, and the Pattern of Trade in Goods

696

P. Blanchard et al.

and Services. NBER Working Paper 5696. Cambridge, MA: National Bureau of Economic Research, Inc. Ma´tya´s, L. (2001). Modelling Export Flows in the APEC Region: Static and Dynamic Gravity Model Approaches. The Asian Pacific Journal of Economics and Business, 5(1):97­118. Mold, A. (2003). The Impact of the Single Market Programme on the Locational Determinants of US Manufacturing Affiliates: An Econometric Analysis. Journal of Common Market Studies, 41(1):37­62. Nicoletti, N., Golub, S., Hajkova, D., Mirza, D., and Yoo, K. Y. (2003). Policies and International Integration: Influences on Trade and Foreign Direct Investment, OECD Working Paper No. 359. OECD Economics Department. OECD (2000). OECD Economic Outlook, 68. Paris Phillips, P. and Moon, H. (1999). Linear Regression Theory for Non-Stationary Panel Data. Econometrica, 67:1057­1111. Ray, J. (1991). Foreign Takeovers and New Investments in the United States. Contemporary Policy Issues, 9(2):59­71. Stephan, M. and Pfaffmann, E. (1998). Detecting the Pitfalls of Data on Foreign Direct Investment: A Guide to the Scope and Limits of FDI-Data as an Indicator of Business Activities of Transnational Corporations. Discussion-Paper 98­02, University of Stuttgart. Swenson, D. L. (2004). Foreign Investment and the Mediation of Trade Flows. Review of International Economics, 12(4):609­629. UNCTAD (2002). World Investment Report, Transnational Corporations and Export Competitiveness. United Nations, Geneva. Wooldridge, J. M. (2003). Cluster-Sample Methods in Applied Econometrics. American Economic Review, 93(2):133­138. Yeaple, S. R. (2003). The Complex Integration Strategies of Multinationals and Cross Country Dependencies in the Structure of Foreign Direct Investment. Review of Economics and Statistics, 85(3):726­734. Yeyati, A. L., Stein, E. and Daude, C. (2003). Regional Integration and the Location of FDI. Working paper No. 492, Washington, DC: Inter-American Development Bank.

Chapter 21
Stochastic Frontier Analysis and Efficiency Estimation
Christopher Cornwell and Peter Schmidt

Theoretically, a production function gives the maximum possible output with a given set of inputs. This is different from its common regression counterpart, which specifies the conditional mean of output. The production function defines a boundary or "frontier", deviations from which can be interpreted as inefficiency. The econometrics of stochastic frontier analysis (SFA) provides techniques for modelling the frontier concept within a regression framework so that inefficiency can be estimated.
Obviously, the notion of a frontier can be extended to other representations of technology. Further, with behavioral assumptions like cost minimization, allocative inefficiency can be distinguished from the technical errors. We discuss ways to make this distinction empirically, but in this chapter we concentrate primarily on the estimation of production frontiers and measures of technical inefficiency relative to them.
The literature on SFA is now roughly 30 years old and surveys have appeared periodically (Førsund, Lovell and Schmidt (1980), Schmidt (1985­86), Lovell and Schmidt (1988), Bauer (1990) and Greene (1993)). In addition, the literature has been given a textbook treatment by Kumbhakar and Lovell (2000). Aside from reviewing recent advances in SFA, this chapter differs from the earlier surveys in its focus on the use of panel data and attention to questions of econometric and statistical detail.
In general, the frontier specifications we consider are variants of the general panel-data regression model:

yit = t + xit  + vit - uit = it + xit  + vit ,

(21.1)

Christopher Cornwell Department of Economics, University of Georgia, Athens, GA 30602, USA, e-mail: cornwl@terry.uga.edu
Peter Schmidt Department of Economics, Michigan State University, East Lansing, MI 48824, USA, e-mail: schmidtp@msu.edu

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

697

c Springer-Verlag Berlin Heidelberg 2008

698

C. Cornwell and P. Schmidt

where yit is output for firm i (i = 1, . . . , N) at time t (t = 1, . . . , T ), xit is a vector of inputs and vit is a random error. In contrast to vit , uit is a one-sided error (uit  0), capturing the shortfall of yit from the frontier, (t + xit  + vit ), The term "stochastic frontier" follows from the fact that the frontier specification
includes vit . Defining it = t - uit , we have a model in which inefficiency is reflected in
differences between firms in the intercepts. Various special cases arise depending on the restrictions placed on the it . The early literature on SFA developed in a pure cross-section (T = 1) context, where identification requires strong assump-
tions about the distributions of vi and ui. The application and extension of paneldata econometrics to SFA grew out dissatisfaction with these assumptions. The first panel frontiers treated inefficiency as a time-invariant firm effect, i =  - ui. Estimates of the i can be obtained using standard panel techniques and converted into estimates of inefficiency. The time-invariance restriction can substitute for the distri-
butional assumptions necessary for cross-section SFA. Later work on panel frontiers introduced specifications for the it that relax the time-invariance assumption, while retaining the advantages of panel data.

21.1 Measurement of Firm Efficiency

In general, when we say that a firm produces efficiently, we mean this in both a technical and allocative sense. Here our emphasis will be on technical efficiency, but we will pay some attention to allocative efficiency as well, in both cases following the canonical approach to the measurement problem developed by Farrell (1957).
A firm is technically efficient if it uses the minimal level of inputs given output and the input mix or produces the maximal level of output given inputs. The first definition is formalized in Farrell's input-based measure,

I (y, x) = min[b : f (bx)  y] ,

(21.2)

where I indicates the proportion of x necessary to produce y, holding the input
ratios constant, and f is a standard, neoclassical (frontier) production function. This
measure is illustrated in Fig. 21.1, which depicts an inefficient firm producing output yA with input vector xA. Technically efficient production occurs along the isoquant, Isoq[L(yA)] = [x : I (yA, x) = 1], where L(y) = [x : (y, x) is feasible] is the input requirements set. Because only bxA is required to produce yA, both inputs must be
scaled back by the factor (1 - b) to achieve technical efficiency.
While this measure is used widely, its appeal diminishes when the input set is not
strictly convex (the isoquant is not everywhere downward sloping). For example, the input vector xB is technically efficient according to the Farrell input measure,
although the same level of output could be produced with less of x1. In this case, a distinction exists between the isoquant and the efficient subset, ES[L(yA)] = [x : x  L(yA), and x~  x implies x~ / L(yA)], with ES[L(yA)]  Isoq[L(yA)]. In most

21 Stochastic Frontier Analysis and Efficiency Estimation

699

x2 Isoq[L(y A/a)]

x

bxA

xC

xA Isoq[L(yA)]

Fig. 21.1 Farrell measures of technical efficiency

xB x1

econometric specifications this distinction has no practical significance, because the

functional forms used in empirical work impose equivalence between the efficient

subset and the isoquant (Lovell (1993) and Greene (1993)).

Corresponding to the output-oriented definition of efficiency is Farrell's output-

based measure,

O(y, x) = min a : f (x)  y . a

(21.3)

Holding inputs constant, 1/O gives the amount by which output could be expanded. From the perspective of the output-based measure, the firm producing yA with xA in Fig. 21.1 will also be technically efficient if it operates on Isoq[L(yA/a)].
Fa¨re and Lovell (1978) showed that if f is homogeneous of degree r (r = returns to scale), then y = f (bx) = br f (x) = a f (x) and a = br. Thus, I = O
only under constant returns. When technology is not homogeneous, there is no straightforward interpretation of O in terms of I , a result that has some im-
plications for how technical efficiency is estimated (Atkinson and Cornwell
(1994a)).
A firm is allocatively inefficient when the marginal rate of substitution between
any two of its inputs is not equal to the corresponding input price ratio. This is true of the firm using xA in Fig 21.1, instead of the cost-minimizing input vector x. Let p be the input price vector corresponding to the isocost line through x. Then the

700

C. Cornwell and P. Schmidt

(input-based) technical efficiency of the firm producing with xA is b = p (bxA)/p xA, and since p x = p xC, its allocative efficiency is the ratio p xC/p (bx). It follows that total or cost efficiency of the firm is given by p xC/p xA, or the product of
technical and allocative efficiency.

21.2 Introduction to SFA

21.2.1 The Basic SFA Empirical Framework

We begin with the Farrell output-based technical efficiency measure in (21.3), which relates observed output, yi, to the production frontier, f (xi;  ), as follows:

yi = ai f (xi;  ), 0 < ai  1 ,

(21.4)

The basic empirical framework for SFA is a regression specification involving a logarithmic transformation of (21.4) that adds a random error term (vi), as in

ln yi = ln f (xi;  ) + vi - ui ,

(21.5)

where ui = - ln ai  0 represents technical inefficiency and output is bounded from above by the stochastic frontier f (xi;  ) exp(vi). The output-based measure of technical efficiency is obviously recovered as exp(-ui).
Models like (21.5) were first introduced by Aigner, Lovell and Schmidt (1977)
and Meeusen and van den Broeck (1977). These papers expressed the view that
the frontier specification should be like any other regression function, which is to
say, stochastic. Thus, the vi serve the same purpose as any conventional regression disturbance--to account for random unobserved factors.
The central econometric issue in models like (21.5) is how to treat the ui. With cross-section data they are usually assumed to follow some non-negative distri-
bution, conditional on xi. Panel data afford the opportunity to view (21.5) as a standard unobserved-effects model and avoid the distributional assumption. Other issues, such as choosing a functional form and the specification for f (xi;  ), are also important insofar as they affect the estimation of firm efficiency.

21.2.2 Stochastic vs Deterministic Frontiers
The earliest attempts to quantify production inefficiency treated the frontier as deterministic, ignoring the role of vi. The classic example of this approach is Aigner and Chu (1968). Aigner and Chu calculated  as the solution to either the linear or quadratic programming problem, taking f (xi;  ) to be Cobb-Douglas, and

21 Stochastic Frontier Analysis and Efficiency Estimation

701

computed technical inefficiency as deviations from the fitted frontier. By ignoring

vi, all deviations from the frontier were regarded as inefficiency. Further, because

there is no stochastic structure to these models, it does not make sense to talk about

the statistical properties of their approach.

Closely related to the Aigner­Chu procedure is the non-parametric program-

ming technique of data envelopment analysis (DEA). With DEA the goal is to

"envelop" the data with a quasi-convex hull. Since DEA is non-parametric, it is

robust to misspecification of the functional form for f (xi;  ). See Cooper, Seiford

and Zhu (2004) for a recent survey of DEA.

The analysis of a deterministic frontiers can be made statistical by treating ui  yi - f (xi;  ) as random variables. A simple strategy is assume the ui are iid with a constant mean  and constant variance, and uncorrelated with xi. In the Cobb-

Douglas setup of Aigner and Chu, this recasts in problem as a regression of the

form

K
 ln yi =  + k ln xik - ui , k=1

(21.6)

where  = ( - ) and ui = ui - . Ordinary least squares (OLS) consistently estimates  and the ks, from which a "corrected" OLS (COLS) estimator of 

can be obtained:

^ = ^  + miax(-u^i ) ,

(21.7)

where u^i = ln yi - ^  - k ^k ln xik. Then, letting u^i denote the corrected residuals based on ^ , technical efficiencies can be estimated as exp(-u^i). However, the distribution of ^ is unknown even asymptotically.
Likelihood-based approaches to (21.6) exist as well; for example, the solutions to the Aigner­Chu linear (quadratic) programming problem is a maximum-likelihood estimator (MLE) if the ui are exponential (half-normal) (Schmidt (1976)). Still, the properties of these estimators remain unknown, because the range of yi depends on  , violating one of the regularity conditions for the usual properties of MLEs to hold.
Similarly, a statistical analysis of DEA is possible if assumptions are made about the nature of the randomness in the data. One possibility that has been suggested is to assume simply that the data points (yi, xi) are a random sample from the set of feasible production points. Under this assumption, plus some regularity conditions on the distribution of these points in the neighborhood of the frontier, the DEA measure is a consistent estimator of the efficiency level of a given firm, and its rate of convergence is known. The asymptotic distribution theory is rather complicated. Bootstrapping is also possible, although there are some non-standard features of the bootstrap that are necessary in this setting. For a survey, see Simar and Wilson (2000).
We do not recommend deterministic frontiers. This is partly due to our philosophical view of the nature of randomness in the world, and partly due to the relative complexity of statistical inference in deterministic frontier models.

702
21.2.3 Other Frontier Functions

C. Cornwell and P. Schmidt

Circumstances and objectives sometimes arise that make alternative representations of technology a more desirable framework for efficiency analysis. These include the presence of multiple outputs, exogeneity assumptions and interest in estimating allocative efficiency.
Recently, it has become popular to accommodate multiple outputs through the use of distance functions (e.g. Coelli and Perelman (1996), Morrison, Johnston and Frengley (2000), and Atkinson, Cornwell and Honerkamp (2003)), which are directly related to the Farrell measures of technical inefficiency. For example, the input distance function is defined as the maximum scale factor necessary to place x on the boundary of L(y):

DI (y, x)  max  :


x 

 L(y) ,

(21.8)

where y is a vector of outputs. The reciprocal of DI is just the Farrell input measure, which implies b in (21.2) is 1/ . The empirical counterpart to (21.8) can be

expressed as

0 = ln DI (yi, xi) + vi - ui ,

(21.9)

where ui = - ln bi. Estimation of the technology parameters can proceed as a straightforward application of the generalized method of moments (GMM), since standard assumptions about xi, vi and ui imply a set of moment conditions that identify the model (see Atkinson, Cornwell and Honerkamp (2003)). Extracting estimates of the ui is possible with the methods described in the next section.
The most commonly adopted strategy for estimating technical and allocative efficiency together is to adopt a cost function framework. One might also use a cost function to accommodate multiple outputs or because it is more reasonable to assume output is exogenous. The usual cost frontier specification is derived from (21.2) as

p

p

1

C = g y, = min

(bx) : f (bx) = y = g(y, p) ,

b

bx b

b

(21.10)

where C is observed cost, p is a vector of input prices and the last equality follows from the fact that a cost function is linearly homogeneous in p. Equation (21.10) leads to empirical models of the form

lnCi = ln g(yi, pi) + vi + ui ,

(21.11)

where ui = - ln bi. The ui in (21.11) measure cost efficiency, which will generally include both technical and allocative distortions. Below we discuss how to distinguish between the two sources of error.

21 Stochastic Frontier Analysis and Efficiency Estimation

703

21.2.4 SFA with Cross-Section Data

21.2.4.1 Estimating the Basic Stochastic Production Frontier
Estimation of (21.5) usually hinges on distributional assumptions for the vi and ui. Such assumptions are required to estimate technical efficiency at the firm level with cross-section data. The usual choices are vi | xi  N(0, v2) and ui | xi  N+(0, u2) (half-normal). Other possibilities for ui include exponential, truncated normal and gamma, and evidence suggests that frontier estimates are not robust to the choice (Schmidt and Lin (1984)). Given distributions for vi and ui and a functional form selected for f (xi;  ), the standard approach is to estimate (21.5) by ML and is automated in popular econometric software such as Stata, TSP and Limdep. There is also a COLS option for the stochastic frontier case in which the OLS estimator of the intercept is corrected by a consistent estimator of E(ui), identified through the higher-order moments of the OLS residuals.
In most cases, the whole point of the frontier estimation exercise is to compare efficiencies at the firm level. Thus the focus of estimation ultimately is on the residu-
als, but no matter how they are computed, they represent (vi - ui), not u^i. Estimation
of firm-specific efficiencies requires that u^i be extracted from (vi - ui). Jondrow, Lovell, Materov and Schmidt (1982) proposed an estimator for the u^i
based on E[ui | (vi - ui)] evaluated at (vi - ui). Under the usual assumptions of the model, consistent estimates of the technology parameters can be obtained via ML
or OLS, from which the (vi - ui) can be calculated. Although the Jondrow et al. estimator is not consistent (because the variation associated with the distribution of ui conditional on (vi - ui) is independent of N), there is no alternative consistent estimator of firm-level efficiency when using cross-section data.

21.2.4.2 Estimating Technical and Allocative Efficiency

Schmidt and Lovell (1979) first demonstrated how to incorporate allocative distortions by introducing errors in the first-order conditions for cost minimization. With distributional assumptions for the allocative errors, they estimated the first-order conditions along with the production frontier. Because Schmidt and Lovell adopted the self-dual Cobb-Douglas functional form, their decomposition of cost efficiency into technical and allocative components was straightforward.
A more typical framework for estimating technical and allocative efficiency jointly is a cost system with the general form,

Ci = g(yi, pi) exp(vi + ui + i) sik = s(yi, pi) exp(ik),

(21.12) (21.13)

704

C. Cornwell and P. Schmidt

where sik is the observed share of the kth input, s(·) is the optimal share implied by Shephard's lemma applied to the deterministic component of the cost function, g(yi, pi), and i and ik are random disturbances reflecting allocative inefficiency.
There is an inherent econometric challenge in estimating (21.12) and (21.13)
with cross-section data, because of the relationship between the allocative errors. Allocative inefficiency raises costs, so i must be one-sided, but allocative distortions involve over- and under-utilization of inputs, so the ik will be two-sided. Further, i and ik will be correlated with each other. Without relying on functional form restrictions, or assuming the problem away by asserting independence between i and ik, estimation is complicated. Kumbhakar (1997) derived a general solution to the problem, but his model is highly nonlinear in the terms representing alloca-
tive efficiency and therefore difficult to estimate. More optimistically, Atkinson and
Cornwell (1994b) show how panel data can obviate the problem entirely.

21.3 SFA with Panel Data

21.3.1 Models with Time-Invariant Inefficiency

The models we consider in this section are special cases of (21.1), with it = i =

 - ui, so that

yit = i + xit  + vit .

(21.14)

From the viewpoint of the panel-data literature, (21.14) is just a standard unobserved-effects model. Unless otherwise noted, we maintain the following assumptions for (21.14):

(A.1) E(vit | xio, i) = 0, t = 1, . . . , T (A.2) E(vivi | xio, i) = v2IT

(21.15) (21.16)

where xoi = (xi1, . . . , xiT ) and vi is T × 1. Thus we generally treat the variables in xit as strictly exogenous (which, in a production context, could perhaps be defended using the argument of Zellner, Kmenta and Dreze (1966)) and require the vit to be conditionally homoscedastic and serially uncorrelated. Approaches to estimating (21.14) differ depending on what is assumed about the i (ui).
From this point on, we will no longer make an explicit notational distinction
between a variable and its logarithm. To be consistent with most empirical specifi-
cations, we will assume y and x are measured in logs. Thus, (21.14) can be thought of as a Cobb-Douglas production frontier. However, the form of f (xi;  ) is not very important for how we proceed with estimation, as long as the unobserved ef-
fect/inefficiency is additive.

21 Stochastic Frontier Analysis and Efficiency Estimation

705

21.3.1.1 Advantages of Panel Data

The use of panel data to fit production relationships dates back at least to Mundlak (1961), who used repeated observations on farms to control for unobserved soil quality and managerial ability that affect output and may be correlated with inputs. In most applications outside the SFA literature, this is the primary motivation for using panel data--to control for unobservables that may be correlated with xit .
The first use of panel data in SFA was by Pitt and Lee (1981), but not until Schmidt and Sickles (1984) was the link between the frontier and panel-data literatures systematically established. They identified three advantages of panel data for SFA. First, the assumption of independence between xi and ui invoked in crosssection estimation can be relaxed. Second, specific distributional assumptions for vi and ui, required in cross-section data to estimate efficiency at the firm level, can be avoided. Third, firm-level efficiency can be estimated more precisely, and, in the case where T  , consistently. There is one caveat, however. These benefits come at the expense of another assumption--that inefficiency does not vary over time. The longer the panel, the less sense this assumption makes.

21.3.1.2 Estimating the Basic Panel Frontier Model

It is common in the panel-data literature to say that estimation of (21.14) depends on whether the i are fixed or random. As argued originally by Mundlak (1978) and emphasized by Wooldridge (2002), this terminology misses the point. Of course the i are random; the issue is whether they are correlated with xit . To take a fixedeffects (FE) approach to estimation is to allow arbitrary correlation between xit and i. A random-effects (RE) specification generally denies this possibility, or allows such correlation only in very specific ways. This point is especially important in the
SFA literature, where correlation between inputs and inefficiency (ui) is a concern. To facilitate the review of estimator choices for (21.14), we rewrite the model
combining all T observations for a single firm:

yi = Xi + eT i + vi ,

(21.17)

where yi and vi are vectors of length T , Xi is T × K and eT is a T × 1 vector of ones. We begin the review maintaining assumptions (A.1) and (A.2) and leaving
open the possibility that xit is correlated with ui. Under these two assumptions, the asymptotically efficient procedure is the FE estimator,

N

-1 N

  ^FE =

XiMiXi

XiMiyi ,

i=1

i=1

(21.18)

where Mi = IT - eT (eT eT )-1eT is the familiar projection that transforms the data into deviations from firm means (for example, yit - y¯i, y¯i = T -1 t yit ). The estimator is easily computed as OLS of yit - y¯i on xit - x¯i.

706

C. Cornwell and P. Schmidt

More asymptotically efficient estimators exist if correlation between xit and ui can be ruled out. An assumption like

(A.3) E(ui | xio) =  ,

(21.19)

does this, where  > 0 indicates that the ui are drawn from a one-sided distribution. Notationally, we accommodate  as in (21.6), by defining  = ( - ) and ui = ui - . Then, (21.17) becomes

yi = Xi + eT  + i ,

(21.20)

where i = vit - ui. Along with (A.3) it is also common to assume

(A.4) E(u2i | xoi ) = u2 ,

(21.21)

which implies E(ii )  i = v2IT + u2eT eT . Under (A.1)­(A.4), the standard RE estimator,

  ^ 
^

=
RE

N

-1 N

(eT , Xi) i-1(eT , Xi)

(eT , Xi) i-1yi ,

i=1

i=1

(21.22)

is asymptotically efficient. Calculating (21.22) is equivalent OLS of i-1/2 Yi on i-1/2, where i-1/2 = IT -
(1 - )Pi,  = [v2/(v2 + T u2)]1/2 and Pi = IT - Mi. The form of i-1/2 implies a "quasi-demeaning" of the data, (for example, yit - (1 - )y¯i), that subsumes the FE transformation. Clearly, as T  ,   0 and ^RE  ^FE. Actual implementation requires consistent estimators for v2 and u2. There are a number of alternatives, but the most popular follows Wallace and Hussain (1969) and estimates the variance
components using the FE and "between" residuals, which are obtained from OLS
of y¯i on x¯i. Occasionally, the RE estimator is justified on the grounds that some of the vari-
ables of interest do not vary over time and such variables are swept away by the
FE transformation. This is not necessary because the coefficients of time-invariant variables (say zi) can be estimated as OLS of (y¯i - x¯i^FE) on zi. However, the estimated coefficients of zi will be consistent only if the time-invariant variables are uncorrelated with ui. In this case, one would not use the RE estimator either, for the same reason.
Hausman and Taylor (1981) offered a solution to this problem in the form of an efficient instrumental-variables (IV) estimator that allows some variables in xit and zi to be correlated with the ui. Letting Xi = (Xi, Zi), their estimator can be written as

  ^ 
^

=
HT

N

-1 N

(eT , Xi ) i-1/2PAi i-1/2(eT , Xi )

(eT , Xi ) -i 1/2PAi -i 1/2yi ,

i=1

i=1

(21.23)

21 Stochastic Frontier Analysis and Efficiency Estimation

707

where PAi is the projection onto the instrument set Ai = (MiXi, PiXi1, Zi1) and (Xi1, Zi1) designates variables that are uncorrelated with ui. Identification requires that there are at least as many variables in Xi1 as in Zi2. Assuming (A.1), (A.2), (A.4) (appropriately modified to include zi) hold, along with the Hausman-Taylor orthogonality conditions, ^HT is the efficient GMM estimator using the instruments in Ai.
Amemiya and MaCurdy (1986) show that more instruments are implied by the
assumption that Xi1 and Zi1 are uncorrelated with ui. Their efficient IV estimator has the same form as (21.23), but uses the instrument set [MiXi, IT  (xio1, zi1)]. Breusch, Mizon and Schmidt (1989) further extend the Hausman-Taylor estimator under the
additional assumption that the correlation between Xi2 and ui is constant over time. Regardless of the instrument employed, any estimator of the form given in (21.23)
can be carried out by applying the RE data transformation to (21.20) and estimating
the transformed regression by IV. The assumptions that lead to more efficient estimators than ^FE can be tested
using the well known methodology of Hausman (1978). A Hausman test of the difference between ^FE and ^RE will provide evidence on whether the data support (A.3). The restrictions embodied in the efficient IV estimators can be tested in a
similar fashion, or by using the GMM-based test of overidentification suggested by
Hansen (1982).
Finally, it is worth pointing out that most popular econometric software automates ^FE and ^RE, and the Hausman test of their difference. Some (for example, Stata and Limdep) also contain procedures to compute the Hausman­Taylor and
Amemiya­McCurdy estimators. However, as we have discussed, all of these estima-
tors are easily implemented with standard OLS or IV packages after appropriately
transforming the data.

21.3.1.3 Firm-Specific Technical Efficiency Estimates

Given any consistent estimator  , firm-specific estimates of technical inefficiency

can be obtained using a COLS procedure as with a deterministic frontier. This in-

volves calculating

u^i = ^ - ^ i, ^ = max(^ i) ,
i

(21.24)

normalizing the frontier in terms of the best firm in the sample. Then, the remaining firms' efficiency levels are estimated by exp(-u^i), which is consistent as T   (assuming ^ is).
In the FE case, i can be estimated as ^ i = y¯i - x¯i ^FE , or by direct OLS estimation of (21.17) in which the i appear as coefficients of firm-specific dummy variables. The latter is cumbersome if the sample contains a large number of firms, but
some software packages (Stata and Limdep) offer this as an alternative to their regular FE procedure (OLS on demeaned data). Because the FE estimator of  is consistent under relatively weak conditions, it is appealing as a basis for SFA. However,
its appeal diminishes if the empirical frontier specification includes time-invariant

708

C. Cornwell and P. Schmidt

regressors. As we suggested earlier, the effects of these variables can be parsed out of ^ i, but their estimated coefficients will be consistent only if the time-invariant variables are uncorrelated with ui, and if that is the case the Hausman-Taylor estimator is preferred.
We should point out that, while ^ is consistent as T  , it is biased upward

when T is fixed. This upward bias is due to the "max" operation, and is consequently

more severe the larger N is. In fact, Park and Simar (1994) show that consistency of ^ requires the condition that (ln N)/T 1/2  0, so that N cannot increase too fast as T increases. The upward bias of ^ in the fixed-T case causes a downward bias in

estimated efficiencies (that is, in the u^i). We will comment more on this issue when we discuss inference on the inefficiencies.

A more difficult problem is distinguishing inefficiency from unobservable time-

invariant variables. The COLS procedure will overstate a firm's inefficiency if there

are time-invariant unobservables, but the alternatives require more assumptions.

One example is Heshmati and Kumbhakar (1994), who deal with capital as a fixed

unobservable in a study of Swedish dairy farms. Their strategy is to assume that (yit - xit  ) can be decomposed as (i + vit + uit ), with uit  0, and treat the i as a fixed firm effect (representing unobserved capital). In addition, they take the vit and uit to be conditionally normal and half-normal, as in standard cross-section SFA. They impose independence between xit and uit , and they also assume that the uit are independent over time. The latter is a very unrealistic assumption. This approach

will likely understate inefficiency because any time-invariant component of ineffi-

ciency is eliminated with the fixed effects, and any persistent component will be at

least partially eliminated.

The RE specification accommodates time-invariant regressors, but care should

be taken in testing the assumptions that serve as a basis for estimation, whether the

estimator is RE or efficient IV. Residuals constructed from either estimator can be

used to estimate i =  - ui and carry out the COLS procedure in (21.24). If the

RE estimator is justified, an alternative is to use the best linear unbiased predictor

(BLUP) of ui ,

u^i

=

-^u2 t ^it T ^u2 + ^v2

,

(21.25)

in the COLS procedure.

Finally, as the Heshmati and Kumbhakar (1994) example illustrates, it is also

possible to proceed with essentially the same assumptions as in the cross-section

case. The only advantage of panel data then is the added precision that comes from

repeated observations on each firm. Battese and Coelli (1988) typify this approach, assuming the ui are truncated normal and the vit are normal, conditional on xoi . They estimate ,  and the parameters of the error distributions by ML. An ad-

vantage of this approach, if xit and ui are independent, is that the frontier intercept  is estimated directly, without the need for the "max" operation in (21.24). Thus,

the estimated frontier is not normalized in terms of the best firm and the best firm

need not be defined as 100 percent efficient. Battese and Coelli showed how to

obtain firm-specific efficiency estimates by generalizing the Jondrow et al. (1982)

21 Stochastic Frontier Analysis and Efficiency Estimation

709

decomposition for a panel-data setting. The Battese­Coelli procedure is usually available in software packages that support ML estimation of the cross-section frontier model.
Regardless of the method, the estimation of firm-specific technical inefficiency is straightforward. However, inference regarding the u^i is not. This is especially true for the COLS procedure; because of the "max" operation in (21.24), standard distributional results do not apply. We take up the problem of inference in a separate section below.

21.3.1.4 Explaining Firm Efficiency
Often one is interested not only in estimating efficiency levels, but also in determining whether observable firm characteristics can explain them. For example, one might ask whether state-owned or privately owned enterprises differ in their efficiency levels, or whether big firms are more efficient than small firms. Questions like these can be addressed in the context of a stochastic frontier model in which the distribution of technical inefficiency depends on such firm characteristics.
To be more explicit, we consider a stochastic frontier model like (21.1) above, and now assume that the technical inefficiency term uit depends on some observed variables zit , with the dependence expressed as uit (zit , ). We treat these variables as exogenous, so they can include inputs or functions of inputs, but they should not be a function of output.
As a specific example, the model of Reifschneider and Stevenson (1991), Caudill and Ford (1993) and Caudill, Ford and Gropper (1995) (hereafter, RSCFG) assumes that uit is distributed as N(0, it )+, where it is a function of zit and . One possibility is it = exp(zit ). Since the expected value of uit is proportional to it , we have parameterized the mean of technical inefficiency. However, since the variance of uit is proportional to i2t , we have also parameterized its variance. As a result specifications of this type are also referred to as models of heteroskedasticity. Kumbhakar and Lovell (2000) discuss models of heteroskedasticity in one place (Sect. 3.4) and incorporating exogenous influences on efficiency in another (Chap. 7), but in our view these are the same. We will discuss these models from the point of view of explaining efficiency.
Many empirical analyses have proceeded in two steps. In the first step, one estimates the stochastic frontier model and firms' efficiency levels, ignoring z. In the second step, one tries to see how efficiency levels vary with z, perhaps by regressing a measure of efficiency on z. It has long been recognized that such a two-step procedure will give biased results. Since E(y | x, z) depends on both x and z, the first-step regression of y on x will be biased by the omission of z, if x and z are correlated. A more subtle point is that the calculation of the firm-specific inefficiencies depends on the variances of vit and uit . Ignoring the fact that the variance of uit is not constant, these estimates will be under-dispersed. These points are discussed in Kumbhakar and Lovell (2000), page 119 and Chap. 7, and in Wang and

710

C. Cornwell and P. Schmidt

Schmidt (2002), Sect. 2.3. Simulations reported in Wang and Schmidt indicate these biases are very severe. Accordingly, we do not recommend two-step procedures.
The alternative to a two-step procedure is one-step estimation by ML. For example, under the assumptions of the RSCFG model, the density for observation i,t is well defined. The likelihood then follows from an additional assumption on the independence, or form of dependence, over time at the firm level. This will be discussed in more detail below.
The literature contains several alternatives to the RSCFG setup described above. One is the model employed by Kumbhakar, Ghosh and McGuckin (1991), Huang and Liu (1994), and Battese and Coelli (1995) (hereafter, KGMHLBC), which assumes that the distribution of uit is N(it , it )+. So, compared to the RSCFG specification, this model parameterizes the mean rather than the variance of the pretruncation normal distribution. Several possibilities have been suggested for the parameterization of it , including it = zit  and it =  · exp(zit ). The KGMHLBC model is heavily used in empirical applications, in part because it is readily available in the FRONTIER software (Coelli 1996). Another is the model of Wang (2002), in which the distribution of uit is N(it , i2t )+, and where it and it both depend on zit . Wang's model allows for non-monotonic effects of zit on uit and can be used to test the adequacy of the simpler specifications.
We now return to the point made above about the nature of dependence over time. The simplest assumption, and the one most commonly made, is that (conditional on zi1, . . . , ziT ) the uit are independent over time. Since the vit are also typically assumed to be independent over time, the errors (vit - uit ) are independent over time, and the likelihood is just the product, over all i and t, of the density for observation i,t. It is widely recognized that the independence assumption is unrealistic. It is less widely recognized that the MLE assuming independence is consistent even if the independence assumption is false. In this case, however, a non-standard (robust) covariance matrix calculation is required for the estimates. This is a textbook point in the more general panel-data context, and is discussed in the frontiers context by Alvarez, Amsler, Orea and Schmidt (2004).
Some of the models reviewed in this section satisfy the scaling property that uit = h(zit , ) · u~it , where u~it does not depend on zit . For example, the RSCFG model has this property, with u~it distributed as N(0, 1)+, and with the scaling function h(zit , ) equal to the parameterized function it . If the scaling property holds, Battese and Coelli (1992) show how to construct the likelihood under the assumption that the underlying random variable u~it is time-invariant (and hence just equals u~i). However, no model currently exists that allows correlation over time in a less restricted form.

21.3.1.5 Inference Based on Estimated Efficiencies
One of the advantages of SFA (over approaches based on deterministic frontier specifications) is the ability to measure the uncertainty of efficiency estimates. In addition to providing point estimates of a firm's level of efficiency, confidence intervals and hypothesis tests can be constructed.

21 Stochastic Frontier Analysis and Efficiency Estimation

711

First, consider an error-components setup with the vit assumed to iid normal and the ui are iid truncated normal, conditional on xio, as in Battese and Coelli (1988). They show that the ui conditional on (vi1 - ui, vi2 - ui, . . . , viT - ui) have a normal distribution truncated from below at zero. The mean and variance (before truncation)
of the normal distribution are given by (21.9) and (21.10) of Battese and Coelli; the
mean depends on the average residual for the firm. The suggested point estimate (or
prediction) for ui is the mean of the truncated distribution, as given by their (21.11). However, we can also obtain confidence intervals for ui directly from this distribution; for example, a 95 percent confidence interval for ui is given by the range between the 2.5 and 97.5 percentiles of the truncated normal conditional distribu-
tion of ui. This possibility was first noted by Horrace and Schmidt (1996). Similar methods apply for inefficiency defined as exp(-ui). Similar comments also apply in the cross-sectional case when the method of Jondrow et al. (1982) is used to
estimate ui. Matters are more complicated under weaker assumptions that prevail in most
panel settings. Recall the COLS estimator for ui given in (21.24). Standard results give the joint distribution of the ^ i, and the difficult nature of the inferential problem is due to the max operation. To emphasize this point, for the moment we will ignore
the possible inaccuracy of the max operation in picking the maximal population intercept. Suppose that the maximal estimated intercept is ^ m, where m represents a specific observation, and note that m may or may not be the maximal intercept in the population. Then, u^i = ^ m - ^ i and we can use standard methods to construct a confidence interval for m - i. For example, if the vit are normal or if T is large, confidence intervals would be based on the Student's t or standard normal
distributions.
There is also an extensive literature on multiple comparison procedures. A
good general discussion is given by Hochberg and Tamhane (1987). These procedures allow the construction of simultaneous confidence intervals for the (N - 1)dimensional vector of differences (m - i, i = m). This is a "multiple comparison with a control" (MCC) problem, since for the moment we are treating m as a control, or standard of comparison, without being concerned about whether it is in fact
the maximal population intercept. Dunnett (1955) gives an easily computable solution to the MCC problem for the special case that the  j are equicorrelated, and relevant tabulations are given in Hochberg and Tamhane (1987), Dunnett (1964), Dunn
and Massey (1965) and Hahn and Hendrickson (1971). Horrace and Schmidt (2000)
provide evidence that the equicorrelated assumption is very nearly met in some ap-
plications and discuss approximate solutions when it is not met. These confidence
intervals may encompass both positive and negative values because they do not assume that m is the maximal population intercept.
From the SFA perspective, we are interested in simultaneous confidence intervals for the N-dimensional vector of differences ( - i, i = 1, . . . , N), where  is the maximal intercept in the population rather than in the sample. This is a "multiple
comparison with the best" (MCB) problem which differs from the MCC problem
because it is not assumed that we know which observation corresponds to the max-
imal population intercept. This problem was solved by Edwards and Hsu (1983),

712

C. Cornwell and P. Schmidt

who showed how MCB intervals could be constructed from MCC intervals. Other
relevant references include Hsu (1981, 1984), and a survey is given by Horrace and
Schmidt (2000). The MCB intervals give non-negative lower and upper bounds for the differences ui =  - i, and the lower bound equals zero for a subset of the firms. The MCB intervals are wider than the corresponding MCC intervals because
they include uncertainty about which observation is best. Some empirical examples
of MCB intervals are given in Sect. 21.4.
Another possible method of inference based on the FE estimates is bootstrap-
ping. We will begin with a very brief discussion of bootstrapping in the general setting where we have a parameter  , and there is an estimator ^ based on a random sample (z1, . . . , zN). The following bootstrap procedure will be repeated many times, say for b = 1, . . . , B where B is large. For iteration b, construct "pseudo data", z(1b), . . . , z(Nb), by sampling randomly with replacement from the original data. From the pseudo data, construct the estimate ^ (b). The basic result of the bootstrap is that under fairly general conditions the asymptotic (large-N) distribution of (^ (b) - ^ ) conditional on the sample is the same as the (unconditional) asymptotic distribution of (^ -  ). Thus, for large N the distribution of ^ around  is the same as the bootstrap distribution of ^ (b) around ^ , which is revealed by the large number of bootstrap draws.
We now consider the application of the bootstrap to the specific case of the FE
estimates. Our discussion follows Simar (1992). Define the residuals based on the FE estimates of  and i as v^it = yit - ^ i - xit ^FE . The bootstrap samples will be drawn by resampling these residuals, because the vit are the quantities analogous to the zis in the previous paragraph, in the sense that they are assumed to be iid, and they are the observable versions of the vit . (The sample size N above corresponds to NT .) So, for bootstrap iteration b = 1, . . . , B, we calculate the bootstrap sample v^(itb) and the pseudo data, yit = ^ i + xit ^FE + v^(itb). From these data we get the bootstrap estimates of the inefficiencies, and the bootstrap distribution of these estimates is
used to make inferences about the actual inefficiencies. We note that the estimates depend on the quantity maxi ^ j. Since "max" is not a
smooth function, it is not immediately apparent that this quantity is asymptotically
normal, and if it were not the validity of the bootstrap would be in doubt. A rigorous
proof of the validity of the bootstrap for this problem is given by Hall, Ha¨rdle and
Simar (1995). They prove the equivalence of the following three statements: (i) maxi ^ j is asymptotically normal; (ii) the bootstrap is valid as T   with N fixed; and (iii) there are no ties for maxi ^ i, that is, there is a unique index i such that i = maxi  j. There are two important implications of this result. First, the bootstrap will not be reliable unless T is large. Second, this is especially true if there are near ties for maxi  j , in other words, when there is substantial uncertainty about which firm is best.
Simulation results reported in Kim (1999) are fairly pessimistic. The bootstrap
does not lead to very reliable inference on the individual firm efficiencies unless T
is very large, or the variance of vit is quite small.

21 Stochastic Frontier Analysis and Efficiency Estimation

713

A final possibility for inference is to be a Bayesian. In a Bayesian analysis one
postulates a prior distribution for the parameters of the problem, and combines the
prior with the likelihood to obtain a posterior distribution upon which inference is
based. In models like those we consider here, and in fact in many other models, this
inference is done by drawing from the posterior distribution using Markov-Chain
Monte Carlo methods.
We will begin with a "Bayesian FE model", due to Koop, Osiewalski and Steel (1997). They postulate an "uninformative" prior for the parameters  , v2 and i. If the vit are iid normal, the mean of the posterior distribution of  is the usual FE estimate, which explains the name of the model. Now consider the inefficiency terms ui = max j  j - i or the inefficiencies exp(-ui ). An important point is that an uninformative (flat) prior for the i implies a flat prior for the ui , but a (very) informative prior for exp(-ui). In fact, the prior for exp(-ui) is proportional to [exp(-ui)]-1, which very, very strongly favors low efficiencies. In a sense this is the Bayesian counterpart to the downward bias of the efficiency estimates using FE
that was discussed in Sect. 21.3.1.3. Indeed, the empirical results given in Kim and
Schmidt (2000) show a strong similarity between inferences based on the Bayesian
FE results and inferences based on bootstrapping the FE estimates.
Koop, Osiewalski and Steel also discuss RE Bayesian models, in which a proper, informative prior is used for the ui (not the ui). In this model, we estimate absolute rather than relative efficiency, and we treat  , v2, the overall intercept  and the inefficiencies ui or exp(-ui) as parameters. They consider, for example, independent exponential priors for the ui. Kim and Schmidt find, unsurprisingly, that the results from a Bayesian analysis with exponential prior inefficiency are quite similar to the
results from classical MLE if an exponential distribution is assumed for inefficiency,
and the Battese­Coelli result is used to extract the efficiencies. If such results are
generally true, as they probably are, it suggests that it does not make much differ-
ence whether one is a Bayesian or not; it just matters how strong the assumptions
are that one is willing to make about the efficiency distribution. An interesting point
is that in this case it is probably easier to be a Bayesian, in a numerical sense, and it
also allows more flexibility in choice of distribution.

21.3.1.6 Estimating Technical and Allocative Efficiency
Recall the cost frontier and share equations given in (21.12) and (21.13), where allocative inefficiency is reflected in a one-sided disturbance in the cost equation (i) and a two-sided error in the share equations (ik), while technical inefficiency is represented solely through a one-sided cost equation error (ui). As we noted in Sect. 21.2.4.2, the choices for estimating such a system of equations are to either adopt a restrictive functional form, assume i and ik are independent, or attempt to estimate the specification proposed by Kumbhakar (1997). Although the latter captures the salient features of the relationship between the i and ik and does not impose a functional-form restrictions, the specification is difficult to estimate because it is highly nonlinear in the terms involving the allocative errors.

714

C. Cornwell and P. Schmidt

An alternative to using error components is to model deviations from costminimizing behavior in terms of parameters that scale prices. In this case, the firm is assumed to minimize shadow cost, recognizing that although the input mix may be incorrect when judged in terms of market prices, it can be seen as efficient when related to shadow prices. The firm minimizes actual costs (is allocatively efficient) only if the ratio of shadow prices equals the ratio of market prices. This parametric approach was developed in a cross-section context by Lau and Yotopoulos (1971), and later extended by Toda (1976), Lovell and Sickles (1983) and Atkinson and Halvorsen (1984).
Atkinson and Cornwell (1994b) generalized the parametric approach to a panel data setting. Reformulating (21.10) as a shadow cost-minimization problem, they consider the estimation of a system of equations like

Cit = g(yit , pit ) exp(vit + ui) sitk = s(yit , pit ) exp(itk),

(21.26) (21.27)

where pitk = ik pitk is a vector of shadow prices where the ik are parameters to be estimated. Because allocative inefficiency is identified through the ik, the difficulty of fully specifying the relationship between cost and share-equation allocative errors is obviated. Further, the itk can be viewed (appropriately) as conventional random errors.
From a panel-data perspective, the system in (21.26) and (21.27) is an
unobserved-effects model, where the effects appear as slope coefficients as well
as additive intercept terms. As Atkinson and Cornwell show, FE estimation of such
a model is straightforward. Firm-specific technical efficiency estimates can be constructed from the u^i using COLS. Estimates of ik indicate the relative over (^ik < 1) or under-utilization (^ik > 1) of an input. Together, the u^i and ^ik can be translated into an estimate of the potential cost savings from eliminating inefficiency.

21.3.2 Models with Time-Varying Inefficiency

While there are great benefits to treating efficiency as if it is time-invariant, time invariance is a strong assumption, especially in longer panels. Now we relax this assumption, explicitly taking up the the SFA model given in (21.1), where t defines the frontier intercept in period t. In the context of this model, the problem is simple in principle. A firm's level technical efficiency in each time can be estimated period in COLS fashion as exp(-u^it ), where

u^it = ^t - ^ it , ^t = max(^ it ) .
i

(21.28)

In practice, however, we cannot expect to identify the it without placing some additional structure on the frontier model. Different papers have restricted the it in

21 Stochastic Frontier Analysis and Efficiency Estimation

715

different ways. Below we consider the most widely utilized specifications as well as some more recently proposed alternatives.

21.3.2.1 The Model of Cornwell, Schmidt and Sickles

Cornwell, Schmidt and Sickles (1990) (hereafter CSS) approached the problem

from the standpoint of a panel regression model with individual-specific slope coef-

ficients:

yit = xit  + wit i + vit ,

(21.29)

where wit is an L × 1 vector of variables whose coefficients, i, vary over i. Clearly, (21.29) is a special case of the production frontier in (21.1) with it = wit i. CSS provide an empirical illustration based on the specification wit = [1, t, t2] so that

it = wit i = i1 + i2 t + i3 t2 .

(21.30)

Obviously if wit contains only a constant, (21.29) reduces to the usual unobserved effects model, which is to say, the basic panel frontier with time-invariant technical efficiency.
In addition to proposing a specification for it , CSS extend the standard FE and RE panel estimators to models like (21.29). To discuss these estimators, consider the expression of (21.29) that combines all T observations on a single firm:

yi = Xi + Wii + vi ,

(21.31)

where Wi is a T × L matrix. In addition, assume
(A.1 ) E(vit | xio, wio, i) = 0, t = 1, . . . , T (A.2 ) E(vivi | xoi , woi , i) = v2IT ,

(21.32) (21.33)

parallel to (21.15) and (21.16) in Sect. 21.4.1. As shown by CSS, the extension of the FE estimator in (21.18) is

N

-1 N

  ^FE =

XiMWi Xi

XiMWi yi ,

i=1

i=1

(21.34)

where MWi = IT - Wi(WiWi)-1Wi is a generalization of the demeaning projection, Mi. Under (A.1 ) and (A.2 ), ^FE is consistent and asymptotically normal, but note that identification requires L  T .
The RE estimator in (21.22) can be likewise extended with the addition of assumptions parallel to (21.19) and (21.21):

(A.3 ) E(i | xoi , wio) = o (A.4 ) E(ii | xoi , woi ) =  .

(21.35) (21.36)

716

C. Cornwell and P. Schmidt

After invoking (A.3 ) and (A.4 ) and writing i as o + i, (21.31) becomes

yi = Xi + Wio + i i = Wii + vi.

(21.37)

The RE estimator of  and o is given by

  ^
^ o

=
RE

N

-1 N

(Xi, Wi) i-1(Xi, Wi)

(Xi, Wi) i-1yi ,

i=1

i=1

(21.38)

where now i = cov(i) = u2IT + WiWi. Consistent estimators of u2 and  are provided in CSS. Under (A.1 )­(A.4 ), ^RE is asymptotically efficient, but this claim hinges on (A.3 ).
CSS also extended the Hausman­Taylor efficient IV estimator to the model with individual-specific slope coefficients. This means partially relaxing (A.3 ) and allowing some of the variables in (Xi, Wi) to be correlated with i. Assuming there are enough orthogonality conditions to satisfy identification requirements, CSS show that  and o can be estimated as

 ^
^ o

N

-1

= (Xi, Wi) i-1/2PAi i-1/2(Xi, Wi)

HT i=1

N
 × (Xi, Wi) i-1/2PAi i-1/2yi, i=1

(21.39)

where PAi is the projection onto the transformed instrument set Ai = i-1/2Ai and Ai is the natural extension of the original Hausman and Taylor instrument set. Although CSS do not pursue it, (21.39) encompasses extensions to the Amemiya­
MaCurdy and Breusch­Mizon­Schmidt estimators as well. The estimator in (21.39)
is the efficient GMM estimator under assumptions (A.1 ), (A.2 ), (A.4 ) and the or-
thogonality conditions imposed by Ai. It is worth pointing out that, unlike in (21.23), this efficient-GMM equivalence depends on the use of transformed instruments in
PAi . Although the RE data transformation is more complicated in this case, in principle ^HT can be computed by premultiplying (21.38) by i-1/2 and performing IV using Ai as instruments.
Firm-specific technical inefficiencies can be estimated using methods directly
analogous to those Sect. 21.3.1.3. In the FE case, this involves estimating the elements of i either by OLS of yit - xit ^FE on wit or directly as coefficients of firm dummies interacted with wit . Then compute the ^ it as wit ^i and u^it as in (21.28). Because the frontier intercept may vary from period to period, the temporal pattern
of technical efficiency will vary from firm to firm. Consider, for example, the CSS specification of it given in (21.30). Although ^ it will be quadratic in t for each firm, ^t may not be, which implies uit may not be either. The setup for estimating technical efficiencies is essentially the same in the RE case, whether  and o are estimated by RE or or efficient IV. The only difference is the set of residuals used in
the calculations.

21 Stochastic Frontier Analysis and Efficiency Estimation

717

21.3.2.2 The Models of Kumbhakar and Battese and Coelli

Kumbhakar (1990) and Battese and Coelli (1992) proposed time-varying efficiency

specifications of the form

uit = (t,  )i ,

(21.40)

where i  0 is a scalar and (t,  ) is a scalar function of time and a vector of parameters,  . Kumbhakar assumes (t,  ) = [1 + exp(bt + ct2)]-1, with  = (b, c). Depending on the values of b and c, the temporal pattern of inefficiency could be in-
creasing or decreasing, concave or convex. Battese and Coelli propose an alternative model, (t,  ) = 1 + 1(t - T ) + 2(t - T )2, where  = (1, 2).
Because (t,  ) does not vary by firm in (21.40), the temporal pattern of technical efficiency is the same for all firms, in contrast to CSS. Also different from CSS,
Kumbhakar and Battese and Coelli couch their specifications in panel extensions of
the classic cross-section SFA model introduced in Sect. 21.2. Thus, estimation of their models depends on distributional assumptions for i and vit that impose independence between efficiency and xit . Kumbhakar and Battese and Coelli derive the MLEs for their respective models (treating the i as truncated normal and vit as normal, conditional on xit ) and show how to estimate firm-specific technical efficiencies by extending Jondrow et al. (1982).
It is possible to estimate the models of Kumbhakar and Battese and Coelli under
weaker conditions than they imposed. For example, we could assume (A.4 ) instead,
and integrate (21.40) into a RE panel regression model like (21.38) as follows:

yit = xit  - (t,  )o + [vit - (t,  )(i - o)] .

(21.41)

Such model can be estimated by nonlinear least squares and firm-specific technical
efficiencies obtained using the procedure in (21.28). All that is required is a simple regression of firm-i residuals on (t,  ) to estimate (i - o). However, FE estimation of (21.41) is econometrically more complicated because the unobserved effects
do not enter additively. This point will be discussed more fully in the next section.
Finally, the connection between specifications like (21.40) and those similar to (21.30) is straightforward when we express the former in terms of it . Suppose, instead of (21.40), we asserted that it = (t,  )i. So long as (t,  ) is positive for all t, then t = max j( jt ) = (t,  ) max j( j) and uit = (t,  )[max j( j - i)], so that the it and uit have the same temporal pattern, determined by the function (t,  ), and this pattern is the same for all firms.

21.3.2.3 The Model of Ahn, Lee and Schmidt
The models presented in Sects. 21.4.1 and 21.4.2 allow technical inefficiency to vary over time, but in a structured way. In this section we consider an alternative model that was originally proposed by Kiefer (1980), and which was subsequently applied to the frontiers problem by Lee and Schmidt (1993), and further analyzed by Ahn, Lee and Schmidt (2001) and Han, Orea and Schmidt (2005). In this model the

718

C. Cornwell and P. Schmidt

temporal pattern of inefficiency is arbitrary, but (as in (21.40) above) it is restricted to be the same for all firms. The specification is

it = t i ,

(21.42)

where the t are parameters to be estimated. One can think of (21.42) as a special case of (21.40) with (t,  ) represented by a set of time dummies. As such, an advantage of (21.42) is that any parametric form such as Kumbhakar's is a testable special case. See, for example, Bai (2003).
The RE estimator of this model raises no new issues, but its FE estimator is interesting. We consider assumptions similar to (A.1 ) and (A.2 ); that is, strict exogeneity of the regressors and the white noise property of the errors. Ahn, Lee and Schmidt propose GMM estimators that impose the restrictions implied by these assumptions. An surprising result is that the moment conditions based on the white noise assumption are useful (result in an increase in asymptotic efficiency) even if the errors are normal. This is certainly not the case in the usual linear regression model without fixed effects. They also analyze the true FE estimator, defined by the minimization of i t (yit - xit  - t i)2 with respect to  , t and i. The consistency of this estimator requires the white noise assumption. Also, given the white noise assumption, this estimator has a non-standard form for its covariance matrix, and it is less efficient than the efficient GMM estimator, even if the errors are normal. Once again these are results that are not true in the linear regression model without fixed effects.
Han, Orea and Schmidt (2005) extend this analysis to the case that t is a parametric function of time and some parameters. Therefore they make possible a FE analysis of models like those of Kumbhakar or Battese and Coelli (discussed in the previous section). The essential results of Ahn, Lee and Schmidt extend to this case. This means that a true FE analysis is possible, but it depends on a white noise assumption, and it requires a non-standard calculation of the covariance matrix of the estimates.

21.4 Applications
In this section we will discuss two empirical applications of the techniques that this paper has described. References to additional applications can be found in the survey papers listed in Sect. 21.1 above.

21.4.1 Egyptian Tile Manufacturers
First, we review the analysis of Egyptian tile manufacturers as originally conducted by Seale (1985, 1990). The author personally collected data on a set of firms in the

21 Stochastic Frontier Analysis and Efficiency Estimation

719

Fayoum region of Egypt, and was involved in, but did not supervise, the collection of data in another region (Kalyubiya). The total sample consisted of 9 firms in the Fayoum region and 16 in Kalyubiya. Data were collected over a 66-week period in 1982­1983. This time period was divided into 3-week time intervals, so that the maximum number of observations was 22. However, because firms did not produce in all periods, the actual number of observations was not the same for all firms; that is, the panel was unbalanced.
The firms make floor tiles using a rather simple technology: sand, cement and water are mixed and pressed into tiles, which are dried in the sun. Three types of tiles are made, and the firms do not produce other products. The capital of the firms consists of a few types of machines: mixers, electric presses, manual presses and polishers. There are only two skill categories of workers.
The original data were aggregated into measures of output, labor and capital. (Because the physical inputs are used in essentially fixed proportions to output, labor and capital were the only inputs to be included in the production function.) Because of the relatively small number of output and input types, and because the data were collected personally by the individual conducting the analysis, the aggregation process is probably much less troublesome than in the typical production function analysis.
The basic empirical results were generated in 1984 and 1985 and used the methodology available at that time; namely, the MLE of Pitt and Lee (1981) and the FE and RE estimators of Schmidt and Sickles (1984), suitably modified to account for the unbalanced nature of the panel. A Cobb-Douglas production function was assumed. Hausman tests rejected the RE specifications, and so the focus was on the FE treatment. The estimated coefficient of capital (machine hours) was positive but insignificant, while the estimated coefficient of labor was insignificantly different from unity. Thus, for all practical purposes, estimated efficiency differences reflect differences in output per worker-hour.
Firm efficiencies were estimated separately for the two areas, since they were viewed as distinct markets. The estimates of technical efficiency ranged from 100 to 71 percent in the Fayoum area and from 100 to 56 percent in the Kalyubiya area. This is a reasonable range given the costs of transporting output, and the least efficient firms were located in small and remote villages where competition from larger and more efficient firms was not a real threat.
Seale argues convincingly that his efficiency estimates do indeed reflect differences that one might interpret as inefficiency (as opposed to measurement error, omitted inputs, etc.). For example, consider the following description of an inefficient firm (Seale (1985, page 175)): "The organization of the firm could be improved; the working area around the electric press is organized for three workers only, while many tileries with an electric press are able to provide adequate space for four workers to form tiles. The total working area, though large, is cluttered with broken tiles and empty sacks, giving a general impression of disarray." Furthermore, Seale ranked the firms in terms of their apparent efficiency after his initial visits to them, but before the data were collected and analyzed. His a priori rankings were very similar to those from the statistical analysis. In fact, the rank correlation

720

C. Cornwell and P. Schmidt

coefficient was 0.98 for the Fayoum region, in which he lived and supervised the data collection effort, and 0.72 in Kalyubiya. This is fairly convincing evidence, unfortunately of a type that we cannot expect generally to be available, that the efficiency estimation exercise has been basically successful.
These data have subsequently been analyzed by a number of others, including Horrace and Schmidt (1996, 2000) and Kim and Schmidt (2000). The following results are from Kim and Schmidt. For reasons of space we will quote only the results for one firm, number 4, which is the median-efficient firm based on the FE estimates.
The FE estimates yield an efficiency level for firm 4 of 0.895. A set of 90 percent MCB intervals give a confidence interval for firm 4 of [0.648, 1]. The "one" here is exact--it is not the result of rounding. The usual percentile bootstrap gives a 90 percent confidence interval of [0.692, 0.940]. The Bayesian FE model gives a point estimate (mean of the posterior distribution) of 0.812, which is somewhat lower, and a 90 percent confidence interval (this is not a Bayesian word, but it is a Bayesian calculation) of [0.688, 0.945]. Note the similarity of the Bayesian interval to the interval from bootstrapping the FE estimates.
RE models give results that are relatively similar. For the half-normal MLE, the point estimate of efficiency for firm 4 is 0.885 and a 90 percent confidence interval, based on the Battese­Coelli method, is [0.787, 0.978]. For the exponential MLE, we obtain 0.896 and [0.799, 0.984], and the Bayesian exponential model with an uninformative prior for the exponential parameter yields 0.891 and [0.782, 0.986].
Kim and Schmidt argue that these results are optimistic, in the sense that the choice of specific model is not too important, and the results are precise enough to be of some potential use.

21.4.2 Indonesian Rice Farmers
Next, we turn to the analysis of Indonesian rice farmers. These data have been analyzed by Erwidodo (1990), Lee and Schmidt (1993) and Horrace and Schmidt (1996). The data contain information on 171 rice farms in Indonesia, for six growing seasons. They were collected by the Agro Economic Survey, as part of the Rural Dynamic Study in the rice production area of the Cimanuk River Basin, West Java, and obtained from the Center for Agro Economic Research, Ministry of Agriculture, Indonesia. In particular, they were not collected as primary data by the individuals later involved in the analysis, though Erwidodo was personally familiar with farming practices in the area. Time periods are growing seasons, of which there are two per year; three of the six time periods are dry seasons and three are wet seasons. The data were collected from six different villages that contain 19, 24, 37, 33, 22 and 36 farm families, respectively. This is a balanced panel in the sense that every family is observed for the same six time periods.
Output is production of rough rice, in kilograms. The inputs include seed, urea, tri-sodium phosphate (TSP), labor and land area. Erwidodo considered both

21 Stochastic Frontier Analysis and Efficiency Estimation

721

Cobb-Douglas and translog specifications, but we will follow Lee and Schmidt and discuss only results for the Cobb-Douglas specification; this does not make much difference. Besides the inputs, the equation that is estimated also includes some dummy variables, as follows. DP is a dummy variable equal to one if pesticides are used, and zero otherwise. DV1 equals one if high-yielding varieties of rice are planted, while DV2 equals one if mixed varieties are planted; the omitted category represents traditional varieties. DSS equals one in the wet season and zero otherwise. DR1, . . . , DR5 are dummy variables representing the six villages, and are intended to control for differences in soil quality or other relevant factors across villages. Finally, DSIZE is a dummy variable equal to one if the land area is greater than 0.5 hectare. Erwidodo included this variable while Lee and Schmidt did not, but in fact it makes little difference to the efficiency estimation exercise. We will report results only for the specification that does not include DSIZE. The data are described in detail in Erwidodo (1990).
Erwidodo estimated the model using the standard panel-data techniques: OLS, FE and RE estimators. The results based on the three methods are quite similar; correspondingly, the appropriate Hausman test failed to reject the RE specification. The estimated coefficients of the five input variables were all positive and significant at the usual critical levels. The elasticities ranged from 0.47 for land area to 0.078 for TSP, using the RE estimates, and from 0.43 to 0.09 using the FE estimates. Returns to scale were insignificantly different from unity. The coefficient estimates of the dummies for rice variety and for wet season were significantly different from zero, while the rest of the dummy variables were usually insignificant. The results that were significant indicate that high-yielding rice varieties have higher yields than traditional varieties, and that output is higher in the wet season than in the dry season.
Erwidodo calculates measures of both technical and allocative inefficiency, but we will discuss measures of technical inefficiency only. He calculates estimates of technical inefficiency in three ways: (i) the simple FE calculation given in (21.24) above; (ii) the RE calculation involving the best linear predictor, given in (21.25) above; and (iii) the method of Battese and Coelli (1988). Thus in Erwidodo's implementation of method (iii), distributional assumptions are used in the separation of inefficiency from noise even though they were not used in estimation. We should also note that we might expect the FE results (i) to differ rather substantially from the RE results (ii) or (iii) since in the FE regression we cannot include the time-invariant village dummy variables, and thus differences across villages in soil quality or other relevant time-invariant factors are not controlled.
Erwidodo actually reports his results only for method (iii). Battese and Coelli assumed that ui is distributed as N(, u2) truncated below at zero. Erwidodo assumed  = 0 so that ui is half-normal. In this case var(ui) = u2( - 2)/. The usual variance components estimates that are part of the RE procedure yield an estimate of var(ui), and this can be converted into an estimate of u2 by multiplying it by /( - 2). It appears that Erwidodo used his estimate of var(ui) as an estimate of u2, neglecting the factor /( - 2), which made his technical inefficiency figures too small. Horrace and Schmidt (1996) recalculation of Erwidodo's results yields

722

C. Cornwell and P. Schmidt

farm-specific inefficiency estimates ranging from 3.5 to 25.8 percent, with a mean of 10.6 percent.
Using the same data, Lee (1991) calculates technical inefficiency measures based on FE estimation and method (i) above. Technical inefficiency now ranges from zero to 64.6 percent, with a mean of 56.7 percent. Estimation by RE and use of method (ii) gives results that are very similar to those for the FE estimator; for example, mean technical inefficiency is then 57.1 percent. These results are consistent with Erwidodo's report that FE and RE generated much higher levels of technical inefficiency that the Battese­Coelli method, but that all three methods give similar rankings.
Clearly there are striking differences between these results. To interpret them, it is interesting to look at the precision of the estimates, as reflected in the relevant confidence intervals. These results are given in Horrace and Schmidt (1996, 2000) and Kim and Schmidt (2000). As in the previous section, here we report the results only for the median firm, number 15. For this firm, the FE estimates give an efficiency level of 0.554. The 90 percent MCB confidence interval is [0.300, 1] and the percentile bootstrap interval is [0.398, 0.646]. The Bayesian FE model gives a point estimate of 0.509 and a 90 percent interval of [0.383, 0.656]. Once again the Bayesian FE estimates are similar to the classical FE estimates and the bootstrap.
As we saw above, the RE efficiencies are much higher. For the half-normal MLE, the point estimate of efficiency for firm 15 is 0.923 with a 90 percent confidence interval of [0.792, 0.990]. For the exponential MLE, we obtain 0.935 and [0.834, 0.996], and for the Bayesian exponential model with uninformative prior on the exponential parameter we get 0.935 and [0.823, 0.996].
Clearly these results are less precise than for the previous data set, and the choice of technique matters more. Kim and Schmidt argue that this is a difficult data set to analyze, because T is fairly small and because the variance of noise (v) is large relative to the variance of inefficiency (u). In this case we can gain a lot of precision by putting more structure on the model, but unfortunately the choice of what structure to impose influences the results more strongly. There is no obvious solution to this problem other than to analyze data that have more favorable characteristics.
Lee (1991) and Lee and Schmidt (1993) have also applied the time-varying efficiency model of subsection 3.2.3 to the Erwidodo data. Compared to the simpler model with time-invariant efficiency, this model does not make much difference in the estimates of the technical parameters (regression coefficients) or in the average level of inefficiency. It does yield an interesting temporal pattern of inefficiency (see Fig. 8.1, page 251, of Lee and Schmidt), with significantly higher efficiency levels in time periods t = 3 and t = 4 than in the other time periods. However, given the confidence intervals reported in the previous paragraphs for the simpler model, it might be argued that a model with less structure is the opposite of what is needed for this application.

21 Stochastic Frontier Analysis and Efficiency Estimation

723

21.5 Concluding Remarks

In this chapter, we have given given a broad survey of the stochastic frontier approach to efficiency measurement, with an emphasis on the use of panel data. While a considerable number of details were discussed, we have tried to emphasize two main points. The first main point is that it is really a misuse of words to discuss the measurement of efficiency; properly, we should refer to estimation of efficiency. The estimation of efficiency is essentially a statistical problem, in the sense that the results are subject to uncertainty, and this is true whether traditional statistical methods are used or not. There are two main advantages to an explicitly statistical approach, such as is possible using stochastic frontier models. First, an accommodation can be made to statistical noise. Second, measures of the uncertainty of the results can be generated. Our empirical results in Sect. 21.6 show the importance of this second point. Using a deterministic (non-statistical) model does not remove this uncertainty; it only hides it.
Our second main point is that panel data are useful because they allow weaker assumptions or greater precision under a given set of assumptions, than would be possible with a single cross section. Most of the work so far on the use of panel data for efficiency estimation has emphasized the possibility of weakened assumptions and more flexible models. In retrospect, this may have been a mistake. Certainly we should suspect that the usual trade-off between flexibility of the model and precision of results applies. If efficiency estimates were more routinely reported along with appropriate measures of the uncertainty associated with them, this trade-off could be made more intelligently.

References
Ahn, S. C., Y. H. Lee and P. Schmidt (2001): GMM Estimation of Linear Panel Data Models with Time-Varying Individual Effects, Journal of Econometrics, 101, 219­256.
Aigner, D. J. and S. Chu (1968): On Estimating the Industry Production Function, American Economic Review, 58, 826­839.
Aigner, D. J., C. A. K. Lovell and P. Schmidt (1977): Formulation and Estimation of Stochastic Frontier Production Function Models, Journal of Econometrics, 6, 21­37.
Alvarez, A., C. Amsler, L. Orea and P. Schmidt (2004): Interpreting and Testing the Scaling Property in Models Where Inefficiency Depends on Firm Characteristics, unpublished manuscript.
Amemiya, T. and T. E. MaCurdy (1986): Instrumental Variable Estimation of an Error-Components Model, Econometrica, 54, 869­891.
Atkinson, S. E. and C. Cornwell (1994a): Estimating Output and Input Technical Efficiency Using a Flexible Functional Form and Panel Data, International Economic Review, 35, 245­256.
Atkinson, S. E. and C. Cornwell (1994b): Parametric Measurement of Technical and Allocative Inefficiency with Panel Data, International Economic Review, 35, 231­244.
Atkinson, S. E. and C. Cornwell and O. Honerkamp (2003): Measuring Productivity Change Using a Malmquist Index: Stochastic Distance Function Estimation vs DEA, Journal of Business and Economic Statistics, 21, 284­295.
Atkinson, S. E. and R. Halvorsen (1984): Parametric Efficiency Tests, Economies of Scale, and Input Demand in U.S. Electric Power Generation, International Economic Review, 25, 647­662.

724

C. Cornwell and P. Schmidt

Bai, J. (2003): Inferential Theory for Factor Models of Large Dimensions, Econometrica, 71, 135­172.
Battese, G. E. and T. J. Coelli (1988): Prediction of Firm-Level Technical Efficiencies with a Generalized Frontier Production Function and Panel Data, Journal of Econometrics, 38, 387­399.
Battese, G. E. and T. J. Coelli (1992): Frontier Production Functions, Technical Efficiency and Panel Data: With Applications to Paddy Farmers in India, Journal of Productivity Analysis, 3, 153­169.
Battese, G.E. and T. J. Coelli (1995): A Model for Technical Inefficiency Effects in a Stochastic Frontier Production Function for Panel Data, Empirical Economics, 20, 325­332.
Bauer, P. (1990): Recent Developments in the Econometric Estimation of Frontiers, Journal of Econometrics, 46, 39­56.
Breusch, T. S., G. E. Mizon and P. Schmidt (1989): Efficient Estimation Using Panel Data, Econometrica, 57, 695­700.
Caudill, S. B. and J. M. Ford (1993): Biases in Frontier Estimation Due to Heteroskedasticity, Economics Letters, 41, 17­20.
Caudill, S. B., J. M. Ford and D. M. Gropper (1995): Frontier Estimation and Firm-Specific Inefficiency Measures in the Presence of Heteroskedasticity, Journal of Business and Economic Statistics, 13, 105­111.
Coelli, T. J. (1996): A Guide to FRONTIER Version 4.1: A Computer Program for Stochastic Frontier Production and Cost Function Estimation, CEPA Working Paper 96/7, Department of Econometrics, University of New England, Armidale NSW Australia.
Coelli, T. J. and S. Perelman (1996): Efficiency Measurement, Multiple-Output Technologies and Distance Functions: with Application to European Railways, CREPP working paper, 96/05, University of Lie`ge, Belgium.
Cooper, W., L. Seiford and J. Zhu (2004): Handbook on Data Envelopment Analysis, Boston: Kluwer Academic Publishers.
Cornwell, C., P. Schmidt and R. C. Sickles (1990): Production Frontiers with Time-Series Variation in Efficiency Levels, Journal of Econometrics, 46, 185­200.
Dunn, O. J. and F. J. Massey (1965): Estimation of Multiple Contrasts using t-distribution, Journal of the American Statistical Association, 60, 573­583.
Dunnett, C. W. (1955): A Multiple Comparison Procedure for Comparing Several Treatments with a Control, Journal of the American Statistical Association, 50, 1096­1121.
Dunnett, C. W. (1964): New Tables for Multiple Comparisons with a Control, Biometrics, 20, 482­491.
Edwards, D. G. and J. C. Hsu (1983): Multiple Comparisons with the Best Treatment, Journal of the American Statistical Association, 78, 965­971.
Erwidodo (1990): Panel Data Analysis on Farm-Level Efficiency, Input Demand and Output Supply of Rice Farming in West Java, Indonesia, unpublished Ph. D. dissertation, Department of Agricultural Economics, Michigan State University.
Fa¨re, R. and C. A. K. Lovell (1978): Measuring the Technical Efficiency of Production, Journal of Economic Theory, 19, 150­162.
Farrell, M. S. (1957): The Measurement of Productive Efficiency, Journal of the Royal Statistical Society, A, 120, 253­281.
Førsund, F., C. A. K. Lovell and P. Schmidt (1980): A Survey of Frontier Production Functions and of Their Relationship to Efficiency Measurement, Journal of Econometrics, 13, 5­25.
Greene, W. H. (1993): The Econometric Approach to Efficiency Analysis, in H. Fried, C. A. K. Lovell and S. Schmidt eds., The Measurement of Productive Efficiency, New York: Oxford University Press.
Hahn, G. J. and R. W. Hendrickson (1971): A Table of Percentage Points of the Distribution of the Largest Absolute Value of k Student t Variables and its Applications, Biometrika, 58, 323­332.
Hall, P., W. Ha¨rdle and L. Simar (1995): Iterated Bootstrap with Applications to Frontier Models, Journal of Productivity Analysis, 6, 63­76.
Han, C., L. Orea and P. Schmidt (2005): Estimation of a Panel Data Model with Parametric Temporal Variation in Individual Effects, Journal of Econometrics, 126, 241­267.

21 Stochastic Frontier Analysis and Efficiency Estimation

725

Hansen, L. (1982): Large Sample Properties of Generalized Method of Moments Estimators, Econometrica, 50, 1029­1054
Hausman, J. (1978): Specification Tests in Econometrics, Econometrica, 46, 1251­1271. Hausman, J. and W. Taylor (1981): Panel Data and Unobservable Individual Effects, Econometrica,
49, 1377­1399. Heshmati, A. and S. Kumbhakar (1994): Farm Heterogeneity and Technical Efficiency: Some Re-
sults from Swedish Dairy Farms, Journal of Productivity Analysis, 5, 45­61. Hochberg, Y. and A. C. Tamhane (1987): Multiple Comparison Procedures, New York: Wiley. Horrace, W. C. and P. Schmidt (1996): Confidence Statements for Efficiency Estimates from
Stochastic Frontier Models, Journal of Productivity Analysis, 7, 257­282. Horrace, W. C. and P. Schmidt (2000): Multiple Comparisons with the Best, with Economic Ap-
plications, Journal of Applied Econometrics, 15, 1­26. Hsu, J. C. (1981): Simultaneous Confidence Intervals for All Distances from the Best, Annals of
Statistics, 9, 1026­1034. Hsu, J.C. (1984): Constrained Simultaneous Confidence Intervals for Multiple Comparisons with
the Best, Annals of Statistics, 12, 1145­1150. Huang, C. J. and J. T. Liu (1994): Estimation o f a Non-Neutral Stochastic Frontier Production
Function, Journal of Productivity Analysis, 5, 171­180. Jondrow, J., C. A. K. Lovell, I. Materov and P. Schmidt (1982): On the Estimation of Technical
Inefficiency in the Stochastic Frontier Production Function Model, Journal of Econometrics, 19, 233­238. Kiefer, N. M. (1980): A Time Series ­ Cross Section Model with Fixed Effects with an Intertemporal Factor Structure, unpublished manuscript, Cornell University. Kim, Y. (1999): A Study in Estimation and Inference on Firm Efficiency, unpublished PhD dissertation, Department of Economics, Michigan State University. Kim, Y. and P. Schmidt (2000): A Review and Empirical Comparison of Bayesian and Classical Approaches to Inference on Efficiency Levels in Stochastic Frontier Models, Journal of Productivity Analysis, 14, 91­118. Koop, G., J. Osiewalski and M. Steel (1997): Bayesian Efficiency Analysis through Individual Effects: Hospital Cost Frontiers, Journal of Econometrics, 76, 77­106. Kumbhakar, S. (1990): Production Frontiers, Panel Data, and Time-Varying Technical Inefficiency, Journal of Econometrics, 46, 201­212. Kumbhakar, S. (1997): Modelling Allocative Efficiency in a Translog Cost Function and Cost Shares: An Exact Relationship, Journal of Econometrics, 76, 351­356. Kumbhakar, S., S. Ghosh and J. T. McGuckin (1991): A Generalized Production Frontier Approach for Estimating Determinants of Inefficiency in U.S. Dairy Farms, Journal of Business and Economic Statistics, 9, 279­286. Kumbhakar, S. and C. A. K. Lovell (2000): Stochastic Frontier Analysis, Cambridge: Cambridge University Press. Lau, L. J. and P. A. Yotopoulos (1971): A Test for Relative Efficiency and an Application to Indian Agriculture, American Economic Review, 61, 94­109. Lee, Y. H. (1991): Panel Data Models with Multiplicative Individual and Time Effects: Applications to Compensation and Frontier Production Functions, unpublished Ph. D. dissertation, Department of Economics, Michigan State University. Lee, Y. H. and P. Schmidt (1993): A Production Frontier Model with Flexible Temporal Variation in Technical Efficiency, in H. Fried, C. A. K. Lovell and S. Schmidt eds., The Measurement of Productive Efficiency, New York: Oxford University Press. Lovell, C. A. K. (1993): Production Frontiers and Productive Efficiency, in H. Fried, C. A. K. Lovell and S. Schmidt eds., The Measurement of Productive Efficiency, New York: Oxford University Press. Lovell, C. A. K. and P. Schmidt (1988): A Comparison of Alternative Approaches to the Measurement of Productive Efficiency, in A. Dogramaci and R. Fa¨re, eds., Applications of Modern Production Theory: Efficiency and Production, Boston: Kluwer Academic Publishers.

726

C. Cornwell and P. Schmidt

Lovell, C. A. K. and R. C. Sickles (1983): Testing Efficiency Hypotheses in Joint Production, Review of Economics and Statistics, 65, 51­58.
Meeusen, W. and J. van den Broeck (1977): Efficiency Estimation from Cobb-Douglas Production Functions with Composed Error, International Economic Review, 18, 435­444.
Morrison, C., W. Johnston and G. Frengley (2000): Efficiency in New Zealand Sheep and Cattle Farming: The Impacts of Regulatory Reform, Review of Economics and Statistics, 82, 325­337.
Mundlak, Y. (1961): Empirical Production Function Free of Management Bias, Journal of Farm Economics, 43, 44­56.
Mundlak, Y. (1978): On the Pooling of Time-Series and Cross-Section Data, Econometrica, 46, 69­85.
Park, B. and L. Simar (1994): Efficient Semiparametric Estimation in a Stochastic Frontier Model, Journal of the American Statistical Association, 89, 929­936.
Pitt, M. and L. Lee (1981): The Measurement and Sources of Technical Inefficiency in the Indonesian Weaving Industry, Journal of Development Economics, 9, 43­64.
Reifschneider, D. and R. Stevenson (1991): Systematic Departures from the Frontier: A Framework for the Analysis of Firm Inefficiency, International Economic Review, 32, 715­723.
Schmidt, P. (1976): On the Statistical Estimation of Parametric Frontier Production Functions, Review of Economics and Statistics, 58, 238­239.
Schmidt, P. (1985­86): Frontier Production Functions, Econometric Reviews, 4, 289­328. Schmidt, P. and T. F. Lin (1984): Simple Tests of Alternative Specifications in Stochastic Frontier
Models, Journal of Econometrics, 24, 349­361. Schmidt, P. and C. A. K. Lovell (1979): Estimating Technical and Allocative Inefficiency Relative
to Stochastic Production and Cost Frontiers, Journal of Econometrics, 9, 343­366. Schmidt, P. and R. C. Sickles (1984): Production Frontiers and Panel Data, Journal of Business
and Economic Statistics, 2, 367­374. Seale, J. L. Jr. (1985): Fixed Effect Cobb-Douglas Production Functions for Floor Tile Firms,
Fayoum and Kalyubiya, Egypt, 1981­1983, unpublished Ph. D. dissertation, Departments of Economics and Agricultural Economics, Michigan State University. Seale, J. L. Jr. (1990): Estimating Stochastic Frontier Systems with Unbalanced Panel Data: The Case of Floor Tile Manufactories in Egypt, Journal of Applied Econometrics, 5, 59­74. Simar, L. (1992): Estimating Efficiencies from Frontier Models with Panel Data: A Comparison of Parametric, Non-Parametric and Semi-Parametric Methods with Bootstrapping, Journal of Productivity Analysis, 3, 171­203. Simar, L. and P. W. Wilson (2000): Statistical Inference in Nonparametric Frontier Models: The State of the Art, Journal of Productivity Analysis, 13, 49­78. Toda, Y. (1976): Estimation of a Cost Function when Cost is not a Minimum: the Case of Soviet Manufacturing Industries, 1958­1971, Review of Economics and Statistics, 58, 259­268. Wallace T. D. and A. Hussain (1969): The Use of Error Components Models in Combining CrossSection and Time-Series Data, Econometrica, 37, 55­72. Wooldridge, J. (2002): Econometric Analysis of Cross-Section and Panel Data, Cambridge: MIT Press. Wang, H. J. (2002): Heteroskedasticity and Non-Monotonic Efficiency Effects in a Stochastic Frontier Model, Journal of Productivity Analysis, 18, 241­253. Wang, H. J. and P. Schmidt (2002): One-Step and Two-Step Estimation of the Effects of Exogenous Variables on Technical Efficiency Levels, Journal of Productivity Analysis, 18, 129­144. Zellner, A., J. Kmenta and J. Dreze (1966): Specification and Estimation of Cobb-Douglas Production Functions, Econometrica, 34, 784­795.

Chapter 22
Econometric Analyses of Linked Employer­Employee Data
John M. Abowd, Francis Kramarz and Simon Woodcock

22.1 Introduction
There has been a recent explosion in the use of linked employer-employee data to study the labor market. This was documented, in part, in our Handbook of Labor Economics chapter (Abowd and Kramarz, 1999a).1 Various new econometric methods have been developed to address the problems raised by integrating longitudinal employer and employee data. We first described these methods in Abowd and Kramarz (1999b). In this chapter, we present a survey of these new econometric methods, with a particular emphasis on new developments since our earlier articles.
Linked employer-employee data bring together information from both sides of the labor market. They therefore permit, for the first time, equilibrium analyses of labor market outcomes. They also allow researchers to investigate the joint role of worker and firm heterogeneity, both observed and unobserved, on labor market outcomes. Labor economists have taken full advantage of these data to revisit classic questions and to formulate new ones, and much has been learned as a result. For example, Abowd, Kramarz, Lengermann, and Roux (2005) have revisited the classic question of inter-industry wage differentials to determine whether they are

John M. Abowd School of Industrial and Labor Relations, Cornell University, Ithaca, NY 14850, USA, e-mail: john.abowd@cornell.edu
Francis Kramarz CREST-INSEE, Paris, France, e-mail: kramarz@ensae.fr
Simon Woodcock Department of Economics, Simon Fraser University, Burnaby, BC, Canada, e-mail: simon woodcock@sfu.ca
1 See also Lane, Burgess and Theeuwes (1998) for a review of uses of longitudinal linked employer-employee data.

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

727

c Springer-Verlag Berlin Heidelberg 2008

728

J.M. Abowd et al.

attributable to workers or firms. Abowd, Kramarz, Lengermann and Perez-Duarte (2003) use linked employer-employee data to examine whether "good" workers are employed by "good" firms. Dostie (2005) presents new evidence on the returns to seniority and its relation to turnover; and Woodcock (2003) examines the role of heterogeneity and worker-firm learning on employment and wage dynamics. These applied endeavors have demonstrated the value of linked employer-employee data. They have also spurred the development of new econometric methods to analyze these data. These new methods, rather than specific applications, are the primary focus of this chapter.
A distinguishing feature of longitudinal linked employer­employee data is that individuals and their employers are identified and followed over time. Furthermore, the relation between employer and employee, called a job relation, is continuously monitored. From a statistical perspective, there are three populations under simultaneous study. Individuals are sampled from the population of households, workplaces are sampled from the population of businesses, and jobs are sampled from the population of employment histories. Because of the multiple sampling frames involved, it is necessary to be precise about the statistical structure of the variables under study, since they may come from the individual, employer, or job frame. Measured characteristics of the individual, employer, and job are collected at multiple points in time, which may or may not be synchronous. To make clear the importance of careful elaboration of the sample structure for the variables under study, we will consider a prototypical integrated employer­employee database before turning to specific statistical models. The specific statistical models that we consider are generalizations of the specifications we first used in Abowd, Kramarz and Margolis, (1999, AKM hereafter) as well as in more recent research.
We have noted a general misunderstanding of some recent, and some not-so-recent, empirical methods used by statisticians. We therefore make an effort to relate these methods to those used by panel data econometricians. We show the relation between various fixed effects estimators and estimators popular in the variance components literature ­ in particular, mixed-effects estimators (see Searle, Casella and McCulloch (1992). As we will see, statisticians and econometricians have different parameters of interest, the former relying more on the variance components and the design of the data, the latter being more concerned with endogeneity in its various guises. These generate a variety of distinct computational issues. Consequently econometricians and statisticians have independently developed a variety of tools to estimate the effects of interest. However, the realized effects have the same interpretation under all methods that we consider.
We begin, in Sect. 22.2, by describing a prototypical longitudinal linked data set and discussing the related problems of missing data and sampling from integrated data. In Sect. 22.3, we present two specifications for linear statistical models that relate linked employer and employee data to outcomes measured at the individual level. In the first and more general specification, person effects and firm effects can reflect interaction between observable person or firm characteristics and unobserved person and firm effects. For instance, match effects are potentially representable in this setting. In the second and simpler specification, a typical individual

22 Econometric Analyses

729

has a zero mean for the measured outcomes. Person effects measure deviations over time from this zero mean that do not vary as the employee moves from firm to firm. Firm effects measure deviations from this zero mean that do not vary as the firm employs different individuals. We continue, in Sect. 22.4, by defining a variety of effects that are functions of the basic person and firm effects. Section 22.5 considers the estimation of the person and firm effects by fixed-effects methods. Section 22.6 discusses the use of mixed-effects estimators, the question of orthogonal design, and their relation with various correlated random-effects specifications. In Sect. 22.7 we discuss the important heterogeneity biases that arise when either the person or firm effects are missing or incompletely specified. We discuss the consequences of endogenous mobility in Sect. 22.8, and conclude in Sect. 22.9.

22.2 A Prototypical Longitudinal Linked Data Set 2
To summarize the complete likelihood function for linked longitudinal employeremployee data, we adopt the formalization in Abowd and Woodcock (2001). They considered statistical models for imputing missing data in linked databases using the full-information techniques developed by Rubin (1987). Their prototypical longitudinal linked data set contains observations about individuals and their employers linked by means of a work history that contains information about the jobs each individual held with each employer. The data are longitudinal because complete work history records exist for each individual during the sample period and because longitudinal data exist for the employer over the same period.
Suppose we have linked data on N workers and J firms with the following file structure. There are three data files. The first file contains data on workers, U, with elements denoted ui, i = 1, . . . , N. In the discussion below these data are timeinvariant but in other applications they need not be. Call U the individual characteristics file. The second data file contains longitudinal data on firms, Z, with elements z jt , j = 1, . . . , J and t = 1, . . . , Tj. Call Z the employer characteristics file. The third data file contains work histories, W, with elements wit , i = 1, . . . , N and t = 1, . . . , Ti. Call W the work history file. It contains data elements for each employer who employed individual i during period t. The data U and W are linked by a person identifier. The data Z and W are linked by a firm identifier; we conceptualize this by the link function j = J(i,t) which indicates the firm j at which worker i was employed at date t. For clarity of exposition, we assume throughout that all work histories in W can can be linked to individuals in U and firms in Z and that the employer link J(i,t) is unique for each (i,t).3
2 This section is based on Abowd and Woodcock (2001). 3 The notation to indicate a one-to-one relation between work histories and indviduals when there are multiple employers is cumbersome. See Abowd and Stinson (2003) for a complete development of the likelihood function allowing for multiple employers during the period.

730
22.2.1 Missing Data

J.M. Abowd et al.

Abowd and Woodcock consider the problem of imputing missing data in a longitudinal linked database. Their approach is based on the Sequential Regression Multivariate Imputation (SRMI; see Ragunathan et al., 2001). When imputing missing data in each of the three files, they condition the imputation on as much available information as possible. For example, when imputing missing data in the individual characteristics file U they condition not only on the non-missing data in U (observed characteristics of the individual) but also on characteristics of the jobs held by the individual (data in W ) and the firms at which the individual was employed (data in Z). Similarly, when conditioning the imputation of missing data in W and Z, they condition on non-missing data from all three files. In this manner, their imputation is based on the complete likelihood function for the linked longitudinal data.
The Abowd and Woodcock technique necessitates some data reduction. To understand the data reduction, consider imputing missing data in the individual characteristics file U. Since individuals have work histories with different dynamic configurations of employers, explicitly conditioning the missing data imputation of individual characteristics on every variable corresponding to each job held by each worker is impractical ­ there are a different number of such variables for each observation to be imputed. A sensible alternative is to condition on some function of the available data that is well defined for each observation. For example, to impute missing data in U, one could condition on the person-specific means of time-varying work history and firm variables. Similar data reductions are required to impute missing data in the other files. In what follows, we use the functions g, h, m and n to represent data reductions that span sampling frames.
Abowd and Woodcock note the importance of conditioning the imputation of time-varying variables on contemporaneous data and leads and lags of available data. Because the dynamic configuration of work histories varies from worker to worker and the pattern of firm "births" and "deaths" varies from firm to firm, not every observation with missing data has the same number of leads and lags available to condition the imputation. In some cases, there are no leads and lags available at all. They suggest grouping observations by the availability of dynamic conditioning data (i.e., the number of leads and lags available to condition missing data imputations) and separately imputing missing data for each group. This maximizes the set of conditioning variables used to impute each missing value. Again, some data reduction is generally necessary to keep the number of groups reasonable. For example, one might only condition on a maximum of s leads and lags, with s = 1 or s = 2. They parameterize the set of dynamic conditioning data available for a particular observation by it in the work history file, and  jt in the firm file. It may also be desirable to split the observations into separate groups on the basis of observable characteristics, for example sex, full-time/part-time employment status, or industry. They parameterize these groups by i in the individual file, it in the work history file, and  jt in the firm file.
The key aspects of the SRMI algorithm are as follows. One proceeds sequentially and iteratively through variables with missing data from all three files, at each stage

22 Econometric Analyses

731

imputing missing data conditional on all non-missing data and the most recently

imputed values of missing data. The optimal imputation sequence is in increasing

degree of missingness. As each variable in the sequence comes up for imputation,

observations are split into groups based on the value of it ,  jt , i, it , and/or  jt . The imputed values are sampled from the posterior predictive distribution of a parametric

Bayesian imputation model that is specific to each group. After the imputes are

drawn, the source file for the variable under imputation is reassembled from each

of the group files. Before proceeding to the next variable, all three files must be

updated with the most recent imputations, since the next variable to be imputed may

reside in another file (U,W, or Z). At the same time, the functions of conditioning

data (including leads and lags) described above generally need to be re-computed.

The procedure continues for a pre-specified number of rounds or until the imputed

values are stable.

Explicitly specifying the posterior predictive densities from which the imputa-

tions are drawn is notationally cumbersome. For completeness, we reproduce these

directly from Abowd and Woodcock in (22.1), (22.2), and (22.3). For a particular

variable under imputation, subscripted by k, they denote by U<k the set of variables in U with less missing data than variable k; W<k and Z<k are defined analogously. They denote by U>k the set of variables in U with more missing data than variable k, and define W>k and Z>k similarly. They use the subscript obs to denote variables with no missing data. They also subscript conditioning variables by i, j, and t as

appropriate to make clear the relationships between variables in the three data files.

The predictive densities from which the round + 1 imputations are drawn are


fuk uk gk hk

U<( k+,i1), U>( k),i, Uobs,i ,

Z<( k+,J1()i,t), Z>( k),J(i,t), Zobs,J(i,t)

W<( k+,it1), W>( k),it , Wobs,it

t =Ti t=1



t =Ti t=1

,  pk (k|.) dk

, i, k

(22.1)

 fwk wk
 fzk zk



U<( k+,i1), U>( k),i, Uobs,i ,

Z<( k+,J1()i,), Z>( k),J(i,), Zobs,J(i,)

w(k,i)

 =t +s
,
 =t -s, =t

W<( k+,i1), W>( k),i , Wobs,i

 =t +s
,
 =t -s

 =t +s
,
 =t -s
it , it , k



pk

(k|.) dk

(22.2)



mk U<( k+,J1-)1(i,t), U>( k),J-1(i,t), Uobs,J-1(i,t) ,

z(k, nk

)j ==Wtt-+<(ssk,+,J=1-)t1,(i,Z)<(,k+W, 1j>)(,k)Z,J>(-1k)(,ij,,)Z,obs,j= t



Wobs,J -1 (i, )


 =t

 =t +s
+s=t-s ,
-s

,



pk

(k|.)

dk,

 jt ,  jt , k

(22.3)

732

J.M. Abowd et al.

where f·k is the likelihood defined by an appropriate generalized linear model for variable k, k are unknown parameters, and the posterior densities pk (k|.) are conditioned on the same information as f·k. Repeating the missing data imputation method M times yields M sets of completed data files (Um,W m, Zm) which they call the completed data implicates m = 1, . . . , M.
Equations (22.1­22.3) describe the complete set of conditional distributions of each variable in the linked longitudinal employer­employee data, given all other variables. Hence, they form the basis for sampling from this complete distribution. One can use these equations in a Gibbs sampler or other Monte Carlo Markov Chain algorithm to draw a complete sample of linked longitudinal data that has the same likelihood function as the original analysis sample. Abowd and Woodcock use this property to draw partially synthetic data from the joint posterior predictive distribution.

22.2.2 Sampling from Linked Data
Many of the estimators discussed below are computationally intensive. Because many longitudinal linked databases are constructed from administrative records they are very large.4 Thus researchers are sometimes faced with the prospect of sampling from the linked data to facilitate estimation. In principle, sampling from any one of the frames (workers, firms, or jobs) that comprise the linked data is straightforward. However, the estimators discussed below rely on links between sampling frames (i.e., observed worker mobility between firms) for identification. Small simple random samples of individuals may not retain sufficient "connectedness" between sampling frames for identification.5
Woodcock (2003) considers the problem of sampling from linked data while preserving a minimum degree of connectedness between sampling frames. He presents a "dense" sampling algorithm that guarantees each sampled worker is connected to at least n others by a common employer. The sample is otherwise representative of the population of individuals employed in a reference period. The dense sampling algorithm is straightforward. It operates on the population of jobs at firms with at least n employees in the reference period t. In the first stage, sample firms with probabilities proportional to their employment in period t. In the second stage, sample a minimum of n employees from each sampled firm, with probabilities inversely proportional to the firm's employment in period t. A simple application of Bayes' rule demonstrates that all jobs active in period t have an equal probability of being sampled. The sample is thus equivalent to a simple random sample of jobs active in period t, but guarantees that each sampled worker is connected to at least n others.

4 See Abowd and Kramarz (1999a) for a typology. 5 See Sect. 22.5.1.2 below for a discussion of connectedness and its role in identifying person and firm effects.

22 Econometric Analyses

733

22.3 Linear Statistical Models with Person and Firm Effects

22.3.1 A General Specification

We consider the general linear statistical model:

yit = xit  + qit,J(i,t)i + rit,J(i,t)J(i,t) + it

(22.4)

where yit is an observation for individual i = 1, . . . , N, t = ni1, . . . , niTi , Ti is the total number of periods of data available for individual i, and the indices ni1, . . . , niTi indicate the period corresponding to the first observation on individual i through

the last observation on that individual, respectively. The vectors xit contain P time-

varying, exogenous characteristics of individual i; the vectors qit,J(i,t), and rit,J(i,t) contain respectively Q and R exogenous characteristics of individual i and (or) firm

J(i,t). Both vectors include indicators that associate an observation and a person

(for q) or a firm (for r). We denote the design matrices of these indicators by D and

F, respectively. The vector i is a size Q vector of person effects; J(i,t) is a size R vector of firm effects; and it is the statistical residual. The first period available

for any individual is arbitrarily dated 1 and the maximum number of periods of

data available for any individual is T . Assemble the data for each person i into

conformable vectors and matrices

 yi,ni1
yi =  · · ·  ,

 yi,niTi



xi,ni1,1 · · · xi,ni1,P

Xi = 

···

,



xi,niTi

,1


·

·

·

xi,niTi ,P

i,ni1

i =  · · · 

i,niTi

where yi and i are Ti × 1 and Xi is Ti × P with similar definitions for Qi,J(i,.) and Ri,J(i,.).
We assume that a simple random sample of N individuals is observed for a max-
imum of T periods. Assume further that i has the following properties:

E i|Xi, Qi,J(i,.), Ri,J(i,.) = 0
and Cov i, m|Xi, Qi,J(i,.), Ri,J(i,.), Xm, Qm,J(m,.), Rm,J(m,.)
= {Ti }i , i = m 0, otherwise

734

J.M. Abowd et al.

where {Ti }i means the selection of rows and columns from a T ×T positive definite symmetric matrix  such that the resulting Ti × Ti positive definite symmetric matrix corresponds to the periods {ni1, ni2, . . . , niTi }.6 In full matrix notation we have

y = X + D, Q  + F, R  + 

(22.5)

where: X is the N × P matrix of observable, time-varying characteristics (in deviations from the grand means); D is the N × N design matrix of indicator variables for the individual; Q is the N × (Q - 1)N matrix of the observable characteristics in q with person-specific effects; F is the N × J design matrix of indicator variables for the firm; R is the N × (R - 1)J matrix of observable characteristics in r with firm-specific effects; y is the N × 1 vector of dependent data (also in deviations from the grand mean);  is the conformable vector of residuals; and N = Ni=1 Ti.
The vector y is ordered according to individuals as

 y1
y = ···

(22.6)

yN

and X, Q, R and  are ordered conformably. A typical element of y is yit and a typical element of X, or any similarly organized matrix, as x(i,t)p where the pair (i,t) denotes the row index and p denotes the column index. The effects in (22.4) and (22.5) are:  , the P × 1 vector of coefficients on the time-varying personal characteristics;  , the QN × 1 vector of individual effects; and , the RJ × 1 vector of firm effects. When estimating the model by fixed effects methods, identification of the effects is accomplished by imposing a zero sample mean for i and J(i,t) taken over all (i,t).7 In the mixed effects case, identification is achieved by assuming the random effects
have zero conditional mean and finite conditional variance.

22.3.2 The Pure Person and Firm Effects Specification

A simpler specification is:

yit = xit  + i + J(i,t) + it

(22.7)

with variables defined as above except that i is the pure person effect and J(i,t) is the pure firm effect. We now assume that i has the following properties:8

6 See Sect. 22.6 for a specific example of {Ti }i. 7 Further details of identification requirements are discussed in Sect. 22.5.1.
8 The zero conditional mean assumption (22.8) has been interpreted as an assumption of "exoge-
nous mobility," since it precludes any relationship between an individuals employment location (measured by Fi) and the errors i. See AKM for further discussion, and Sect. 22.8 below for recent work that accomodates endogenous mobility.

22 Econometric Analyses

735

E [i|Di, Fi, Xi] = 0

(22.8)

and

Cov [i, m|Di, Dm, Fi, Fm, Xi, Xm] =

{Ti }i , i = m 0, otherwise

where Di and Fi are those elements of D and F, respectively, corresponding to person i. In full matrix notation we have

y = X + D + F + 

(22.9)

where: X is the N × P matrix of observable, time-varying characteristics (in deviations from the grand means); D is the N × N design matrix of indicator variables for the individual; F is the N × J design matrix of indicator variables for the employer at which i works at date t (J firms total); y is the N × 1 vector of dependent data
(also in deviations from the grand mean);  is the conformable vector of residuals; and N = Ni=1 Ti.
The effects in (22.7) and (22.9) are:  , the P×1 vector of coefficients on the time-
varying personal characteristics;  , the N × 1 vector of individual effects; and , the
J × 1 vector of firm effects. As above, identification of the effects is accomplished
by imposing a zero sample mean for i and J(i,t) taken over all (i,t) for fixedeffects estimators, and by assuming of zero conditional mean and finite conditional
variance for random-effects estimators.

22.4 Definition of Effects of Interest
Many familiar models are special cases of the linear model in (22.4) and (22.5) or the simpler version in (22.7) and (22.9). In this section we define a variety of effects of interest that are functions of the person and firm effects specified in the preceding section. These definitions allow us to consider these familiar models using common notation and internally coherent definitions. We use the example of estimating inter-industry wage differentials, frequently called industry effects, to illustrate some important issues.

22.4.1 Person Effects and Unobservable Personal Heterogeneity

The person effect in (22.7) combines the effects of observable time-invariant personal characteristics and unobserved personal heterogeneity. We decompose these two parts of the pure person effect as

i = i + ui

(22.10)

736

J.M. Abowd et al.

where i is the unobservable personal heterogeneity, ui is a vector of time-invariant personal characteristics, and  is a vector of effects associated with the timeinvariant personal characteristics. An important feature of the decomposition in (22.10) is that estimation can proceed for the person effects, i, whether random or fixed, without direct estimation of . Since many linked employer­employee data sets contain limited, or missing, information on the time-invariant characteristics ui, we describe the estimation algorithms in terms of i; however, when data on ui are available, equivalent techniques can be used for estimation in the presence of i (see AKM for the fixed effects case, Woodcock (2003) for the mixed effects case). The design matrix D in (22.9) can be augmented by columns associated with
the observables ui so that the statistical methods discussed below are applicable to the estimation of the effect specified in (22.10).
This specification can be further generalized by incorporating time-varying ob-
servable characteristics of the worker, qit , or of the firm, q jt , that may well be interacted as in (22.4) and (22.5) to give:

 jit = i + ui + qit i + q jt i

(22.11)

where i and i are vectors of effects associated with the time-varying person and firm observable characteristics. Statistical analysis of the effects defined by (22.11) is accomplished by augmenting the columns of D to reflect the data in q jt and qit . The formulae shown in the estimation sections below can then be applied to the augmented design matrix.

22.4.2 Firm Effects and Unobservable Firm Heterogeneity

The firm effect in (22.7) combines the effects of observable and unobserved time-

invariant characteristics of the firm. It can also be generalized to contain the effects

of time-varying characteristics of the firm and time-varying characteristics of the

employee­employer match as in (22.4) and (22.5). We illustrate each of these pos-

sibilities in this subsection.

We can decompose the pure firm effect of (22.7) into observable and unobserv-

able components as

j = j +vj

(22.12)

where  j is unobservable firm heterogeneity, v j is a vector of time-invariant firm characteristics, and  is a vector of associated effects.
Time-varying firm and employer­employee match characteristics require a redefinition of the simple firm effect as  jit . The addition of the i and t subscripts allows the firm effect to vary over time and across employer­employee matches.
Now let the firm observable characteristics be time-varying, v jt , and denote the observable match characteristics by r jit . Then we can write the firm effect as

 jit =  j + v jt  + r jit  j

(22.13)

22 Econometric Analyses

737

where  j is a vector of effects associated with the match characteristics. Statistical analysis of the effects defined by equation (22.13) is accomplished by augment-
ing the columns of F to reflect the data in v jt and r jit . The formulas shown in the estimation sections below can then be applied to the augmented design matrix.

22.4.3 Firm-Average Person Effect

For each firm j we define a firm-average person effect

¯ j



¯ j

+ u¯ j

=

{(i,t)| J(i,t)= j} i Nj

(22.14)

where

Nj   1(J(i,t) = j) (i,t)

and the function 1(A) takes the value 1 if A is true and 0 otherwise. The importance
of the effect defined in (22.14) may not be apparent at first glance. Consider the difference between  j and ¯j. The former effect measures the extent to which firm j deviates from the average firm (averaged over individuals and weighted by employ-
ment duration) whereas the latter effect measures the extent to which the average
employee of firm j deviates from the population of potential employees. In their
analysis of wage rate determination, AKM refer to the firm-average person effect, ¯j, as capturing the idea of high (or low) wage workers while the pure firm effect,  j, captures the idea of a high (or low) wage firm. Both effects must be specified and estimable for the distinction to carry empirical import.

22.4.4 Person-Average Firm Effect

For each individual i consider the person-average firm effect defined as

¯ i



¯i

+ v¯i

=

t

J(i,t)it Ti

.

(22.15)

This effect is the individual counterpart to the firm-average person effect. Limited sample sizes for individuals make estimates of this effect less useful in their own right; however, they form the basis for conceptualizing the difference between the effect of heterogeneous individuals on the composition of a firm's workforce, as measured by the effect defined in (22.14), and the effect of heterogeneous firms on an individual's career employment outcomes, as measured by the effect in (22.15).

738
22.4.5 Industry Effects9

J.M. Abowd et al.

Industry is a characteristic of the employer. As such, the analysis of industry effects in the presence of person and firm effects can be accomplished by appropriate definition of the industry effect with respect to the firm effects. We call the properly defined industry effect a "pure" industry effect. Denote the pure industry effect, conditional on the same information as in (22.7) and (22.9), as k for some industry classification k = 1, . . . , K. Our definition of the pure industry effect is simply the correct aggregation of the pure firm effects within the industry. We define the pure industry effect as the one that corresponds to putting industry indicator variables in equation (22.9) and, then, defining what is left of the pure firm effect as a deviation from the industry effects. Hence, k can be represented as an employment-duration weighted average of the firm effects within the industry classification k:

  N T
k 
i=1 t=1

1(K(J(i,t)) = k)J(i,t) Nk

where

J
Nk   1(K( j) = k)Nj j=1

and the function K( j) denotes the industry classification of firm j. If we insert this pure industry effect, the appropriate aggregate of the firm effects, into (22.7), then

yit = xit  + i + K(J(i,t)) + (J(i,t) - K(J(i,t))) + it or, in matrix notation as in (22.9),

y = X + D + FA + (F - FA) + 

(22.16)

where the matrix A, J × K, classifies each of the J firms into one of the K industries;

that is, a jk = 1 if, and only if, K( j) = k. Algebraic manipulation of (22.16) reveals that the vector , K × 1, may be interpreted as the following weighted average of

the pure firm effects:

  (A F FA)-1A F F.

(22.17)

and the effect (F - FA) may be re-expressed as MFAF, where MZ  I - Z (Z Z)- Z denotes the column null space of an arbitrary matrix Z, and ()- is a computable generalized inverse. Thus, the aggregation of J firm effects into K industry effects, weighted so as to be representative of individuals, can be accomplished directly by the specification of (22.16). Only rank(F MFAF) firm effects can be separately identified using unrestricted fixed-effects methods; however, there is neither an omitted variable nor an aggregation bias in the estimates of (22.16), using either of class of estimators discussed below. Equation (22.16) simply decomposes F into two orthogonal components: the industry effects FA, and what is left of the

9 This section is based upon the analysis in Abowd, Finer and Kramarz (1999).

22 Econometric Analyses

739

firm effects after removing the industry effect, MFAF. While the decomposition is orthogonal, the presence of X and D in (22.16) greatly complicates the estimation by either fixed-effects or mixed-effects techniques.

22.4.6 Other Firm Characteristic Effects
Through careful specification of the firm effect in (22.13), we can estimate the average effect associated with any firm characteristic, v jt , or any interaction of firm and personal characteristics, r jit, while allowing for unobservable firm and personal heterogeneity.

22.4.7 Occupation Effects and Other Person × Firm Interactions
If occupation effects are interpreted as characteristics of the person, then they are covered by the analysis above and can be computed as functions of  as described in (22.11). Occupation effects are often interpreted as an interaction between person and firm effects (Groshen (1991a,b, implicitly). Mixed effects specifications are most appropriate in this case, and are discussed in Sect. 22.6.

22.5 Estimation by Fixed Effects Methods
In this section we present methods for estimating the pure person and firm effects specification (22.7) by direct least squares, and consistent methods for estimating generalizations of this specification.

22.5.1 Estimation of the Fixed Effects Model by Direct Least Squares
This subsection directly draws from Abowd, Creecy and Kramarz (2002) (ACK, hereafter). The normal equations for least squares estimation of fixed person, firm, and characteristic effects are of very high dimension. Thus estimating the full model by fixed-effect methods requires special algorithms. In our earlier work, e.g., Abowd, Finer and Kramarz (1999) (AFK, hereafter) and AKM, we relied on statistical approximations to render the estimation problem tractable. More recently, ACK developed new algorithms that permit the exact least squares estimation of all the effects in (22.7). These algorithms are based on the iterative conjugate gradient method and rely on computational simplifications admitted by the sparse structure

740

J.M. Abowd et al.

of the least squares normal equations. They have some similarity to methods used in the animal and plant breeding literature.10 ACK also developed new methods for
computing estimable functions of the parameters of (22.7).

22.5.1.1 Least Squares Normal Equations

The full least squares solution to the estimation problem for (22.7) solves the normal

equations for all estimable effects:



   

XXXDXF 

Xy

D X D D D F   = D y

(22.18)

FXFDFF 

Fy

In typical applications, the cross-product matrix on the left-hand side of the equation is too high-dimensional to solve using conventional algorithms (e.g., those implemented in SAS, Stata, and other general purpose linear modeling software based on variations of the sweep algorithm for solving (22.18)). AKM present a set of approximate solutions based on the use of different conditioning effects, Z. AFK applies the best of these approximations with a much higher-dimension Z.

22.5.1.2 Identification of Individual and Firm Effects
Many interesting economic applications of (22.7) make use of the estimated person and firm effects. Estimation requires a method for determining the identified effects11. The usual technique of sweeping out singular row/column combinations from the normal (22.18) is not applicable to the ACK method because they solve the normal equations without inverting the cross-product matrix. Hence, identification requires finding conditions under which the normal equations (22.18) can be solved exactly for some estimable functions of the person and firm effects. In this subsection we ignore the problem of identifying the coefficients  because in practice this is rarely difficult.
The identification problem for the person and firm effects can be solved by applying graph-theoretic methods to determine groups of connected individuals and firms. Within a connected group of persons/firms, identification can be determined using conventional methods from the analysis of covariance. Connecting persons and firms requires that some of the individuals in the sample be employed at multiple
10 See Abowd and Kramarz (1999a) for a longer dicussion of the relation of these models to those found in the breeding literature. The techniques are summarized in Robinson (1991) and the random-effects methods are thoroughly discussed in Neumaier and Groeneveld (1996). The programs developed for breeding applications cannot be used directly for the linked employer­ employee data application because of the way the breeding effects are parameterized. 11 Standard statistical references, for example Searle et al. (1992), provide general methods for finding the estimable functions of the parameters of (22.7). These methods also require the solution of a very high dimension linear system and are, therefore, impractical for our purposes.

22 Econometric Analyses

741

employers. When a group of persons and firms is connected, the group contains all the workers who ever worked for any of the firms in the group and all the firms at which any of the workers were ever employed. In contrast, when a group of persons and firms is not connected to a second group, no firm in the first group has ever employed a person in the second group, nor has any person in the first group ever been employed by a firm in the second group. From an economic perspective, connected groups of workers and firms show the realized mobility network in the economy. From a statistical perspective, connected groups of workers and firms block-diagonalize the normal equations and permit the precise statement of identification restrictions on the person and firm effects.
The following algorithm constructs G mutually-exclusive groups of connected observations from the N workers in J firms observed over the sample period.12
For g = 1, . . . , repeat until no firms remain: The first firm not assigned to a group is in group g. Repeat until no more firms or persons are added to group g: Add all persons employed by a firm in group g to group g. Add all firms that have employed a person in group g to group g. End repeat.
End for.
At the conclusion of the algorithm, the persons and firms in the sample have been divided into G groups. Denote the number of individuals in group g by Ng, and the number of employers in the group by Jg. Some groups contain a single employer and, possibly, only one individual. For groups that contain more than one employer, every employer in the group is connected (in the graph-theoretic sense) to at least one other employer in the group. Within each group g, the group mean of y and Ng - 1 + Jg - 1 person and firm effects are identified. After the construction of the G groups, exactly N + J - G effects are estimable. See the proof in Appendix 1 of ACK.13

22.5.1.3 Normal Equations after Group Blocking
The identification argument can be clarified by considering the normal equations after reordering the persons and firms by group. For simplicity, let the arbitrary
12 This algorithm finds all of the maximally connected sub-graphs of a graph. The relevant graph has a set of vertices that is the union of the set of persons and the set of firms and edges that are pairs of persons and firms. An edge (i, j) is in the graph if person i has worked for firm j. 13 The grouping algorithm constructs groups within which "main effect" contrasts due to persons and firms are identified. In the linear models literature the "groups" are called "connected data". See Searle (1987, pp. 139­149) for a discussion of connected data. See Weeks and Williams (1964) for the general algorithm in analysis of variance models.

742

J.M. Abowd et al.

equation determining the unidentified effect set it equal to zero, i.e., set one person

or firm effect equal to zero in each group. Then the column associated with this

effect can be removed from the reorganized design matrix and we can suppress the

column associated with the group mean. The resulting normal equations are:

 XX



D1X F1 X
D2X F2 X
···
DGX FGX

X D1
D1 F1
0 0
···
0 0

X F1
D1 D1
0 0
···
0 0

X D2
0 0
D2D2 F2 D2
···
0 0

X F2
0 0
D2F2 F2 F2
···
0 0

···
··· ···
··· ···
···
··· ···

X DG
0 0
0 0
···
DGDG FGDF

X FG
0 0
0 0
···
DGFG FGFG

 

 


1 1
2 2
···
G G

 

=

 

Xy
D1y F1 y
D2y F2 y
···
DGy FGy

 

(22.19)

After reordering by group, the cross-products matrix is block diagonal. This matrix has full column rank and the solution for the parameter vector is unique. ACK do not solve (22.19) directly. Rather, they apply the technique discussed below to estimate the identifiable effects.

22.5.1.4 Estimation by Direct Solution of the Least Squares Problem
Appendix 2 in ACK shows the exact algorithm used to solve equation (22.18). It is a variant of the conjugate gradient algorithm, customized to exploit the sparse representation of (22.18) and to accommodate very large problems with many X variables. In practice, ACK apply this algorithm to the full set of persons, firms and characteristics shown in the design matrices of (22.7) and (22.18). Unlike (22.19), the cross-product matrix in (22.18) is not of full rank. Although the algorithm ACK use converges to a least squares solution, the parameter estimates are not unique. They subsequently apply the following identification procedure to the estimated effects. In each group, they eliminate one person effect by normalizing the group mean person effect to zero. ACK also normalize the overall mean person and firm effects to zero. This procedure identifies the grand mean of the dependent variable (or the overall regression constant if X and y have not been standardized to mean zero) and a set of N + J - G - 1 person and firm effects measured as deviations from the grand mean of the dependent variable.14
14 The computer software is available from the authors for both the direct least squares estimation of the two-factor analysis of covariance and the grouping algorithm. Computer software that implements both the random and fixed effects versions of these models used in breeding applications can be found in Groeneveld (1998). The specific algorithm we use can be found in Dongarra et al. (1991) p. 146.

22 Econometric Analyses

743

22.5.2 Consistent Methods for  and  (The Firm-Specific Returns to Seniority)

The preceding discussion focused on estimation of the pure person and firm effects model (22.7). In this subsection, we discuss methods presented in AKM for consistent estimation of more general representations of the person and firm effects. In particular, we discuss consistent estimation of  and  j in the general representation of the firm effect (22.13). The method relies on within-individual-firm differences of the data. It is robust in the sense that it requires no additional statistical assumptions beyond those specified in (22.4) and the general definition of the firm effect (22.13).15 We should note, however, that this estimation technique relies heavily on the assumption of no interaction between X and F. Consider the first differences:

yi,nit - yinit-1 = (xinit - xinit-1 ) + J(i,nit )(sinit - sinit-1 ) + init - init-1 (22.20) for all observations for which J(i, nit ) = J(i, nit-1), and where sinit represents worker i's seniority at firm J (i, nit ) in period nit .16 In matrix form:

y = X +F + 

(22.21)

where y is N × 1, X is N × P, F is N × J,  is N × 1, and N is equal to
the number of (i,t) combinations in the sample that satisfy the condition J(i, nit ) = J(i, nit-1). The matrix F is the rows of the design of  that correspond to the personyears (i,t) for which the condition J(i, nit ) = J(i, nit-1) is satisfied. The least squares estimates of  and  are,

 = ( X MF X )-1 X MF y

(22.22)

 = (F F)-1F ( y - X ).

(22.23)

A consistent estimate of V[ ] is given by

where

V[ ] = ( X MF X )-1( X MF MF X )( X MF X )-1





[ 1] 0 · · · 0

  

0 ···

[ 2] · · · ··· ···

0 ···



0

0 · · · [ N ]

15 We have excluded v jt  from the firm effect (22.13), and assume a pure person effect i. 16 In our preceding notation for the general firm effect (22.13), seniority is an element of observable match-specific characteristics ri jt .

744

J.M. Abowd et al.

and

 [ i]  



2 in2

 in3  in2 ···

 inT1  in2

in2 in3 · · ·
2
in3 · · · ··· ···
inT1  in3 · · ·



 in2
 in3 ···

 inTi  inTi

 .

 inTi  inTi

It is understood that only the rows of  that satisfy the condition J(i, nit) = J(i, nit-1) are used in the calculation of , which is therefore N × N. Notice that this estimator does not impose all of the statistical structure of the basic linear model
(22.7).

22.6 The Mixed Model
In this Section, we focus on a mixed model specification of the pure person and firm effects model. The mixed model arises when some, or all, of the effects in (22.9) are treated as random, rather than fixed, effects. There is considerable confusion in the literature about the comparison of fixed and mixed effects specifications, and so we take pains in this section to define terms in a manner consistent with the enormous statistical literature on this subject.
Consider the matrix formulation of the pure person and firm effects model, given in (22.9). We focus on the cases treated by Woodcock (2003) and Abowd and Stinson (2003), where the parameters  on observable characteristics are treated as fixed, and where the pure person and firm effects  and  are random.17 This specification corresponds closely to the hierarchical models that are common in some other applied settings, for instance in the education literature.18
The mixed model is completely specified by (22.9) and the stochastic assumptions19

17 In fact, Woodcock (2003) decomposes the pure person effect i into observable (ui) and unobserved components (i) as in equation (22.10). He treats  as fixed and i as random. For clarity of exposition we focus here on the simpler case where i is random. 18 In the education literature, schools are analogous to firms and students are analogous to workers.
Because education data typically exhibit far less mobility (of students between schools) than we
observe in labor market data, the usual specification nests student effects within school effects. The analogous hierarchical specification is therefore yit = xit  + i j +  j + it , where i j is the person effect (nested within firm), and where  j and i j are specified as random effects. Dostie (2005) and Lillard (1999) estimate related mixed effects specifications for wages where the firm effect is nested within individuals, e.g., yit = xit  + i + i j + it . 19 In general, statisticians do not explicitly condition these expectations on X because they are
primarily concerned with experimental data, where X constitutes part of the experimental design.
Econometricians, however, are most often confronted with observational data. In this setting, X
can rarely be considered a fixed component of the experimental design.

22 Econometric Analyses
E [ |X] = E [|X] = E [|D, F, X] = 0

745
(22.24)

 
Cov  

 X

=

 

2IN 0

0 2 IJ

 0 0 .



0 0R

(22.25)

It is worth noting that unlike some random effects specifications encountered elsewhere in the econometric literature, the mixed model we have specified does not assume that the design of the random effects (D and F) is orthogonal to the design (X) of the fixed effects ( ). Such an assumption is almost always violated in economic data.
A variety of parameterizations of the residual covariance R are computationally feasible. Woodcock (2003) considers several in detail. Abowd and Stinson (2003) consider two more in the context of specifications that allow for multiple jobs in the same (i,t) pair and multiple measures of the dependent variable. The simplest parameterization is R = 2IN . This specification is useful for making comparisons with the fixed-effect estimation procedure.
The most general parameterization estimated by Woodcock (2003) allows for a completely unstructured residual covariance within a worker-firm match. Let M denote the number of worker-firm matches (jobs) in the data, and let ¯ denote the maximum observed duration of a worker-firm match. Suppose the data are ordered by t within j within i. In the balanced data case, where there are ¯ observations on each worker-firm match, we can write

R = IM W

(22.26)

where W is the ¯ × ¯ within-match error covariance.20 The extension to unbalanced data, where each match between worker i and firm j has duration i j  ¯, is fairly straightforward. Define a ¯ × i j selection matrix Si j with elements on the principal diagonal equal to 1, and off-diagonal elements equal to zero.21 Si j selects those
rows and columns of W that correspond to observed earnings outcomes in the match
between worker i and firm j. Then in the unbalanced data case, we have

20 Woodcock (2003) estimates this parameterization of R under the assumption that W is symmetric and positive semi-definite.
21 For example, if ¯ = 3 and a match between worker i and firm j lasts for 2 periods,
 10
Si j =  0 1  . 00

746

J.M. Abowd et al.

 S11W S11 0

0

···

0

0

R = 

0 0 ... 0
0

...

0

···

0

0

0 S1J1W S1J1 · · ·

0

0

...

...

...

...

...

0

0

· · · SN1W SN1 0

0

0

···

0

...

0



0 0 ... 0
0

 . (22.27)

0

0

0

···

0

0 SNJN W SNJN

22.6.1 REML Estimation of the Mixed Model

Mixed model estimation is discussed at length in Searle et al. (1992) and McCulloch

and Searle (2001). There are three principal methods that can be applied to estimate

the variance components 2, 2 and R : ANOVA, Maximum Likelihood (ML), and Restricted Maximum Likelihood (REML). ANOVA and ML methods are famil-

iar to most economists; REML less so.22 Since REML is by far the most commonly

used estimation method among statisticians, it is worth giving it a brief treatment.

REML is frequently described as maximizing that part of likelihood that is in-

variant to the fixed effects (e.g.,  ). More precisely, REML is maximum likelihood

on linear combinations of the dependent variable y, chosen so that the linear combi-

nations do not contain any of the fixed effects. As Searle et al. (1992, pp. 250­251)

show, these linear combinations are equivalent to residuals obtained after fitting the

fixed portion of the model (e.g., X ) via least squares.23 The linear combinations

k y are chosen so that

k X = 0 

(22.28)

which implies

k X = 0.

(22.29)

Thus k projects onto the column null space of X, and is therefore

k = c IN - X X X - X

(22.30)

 c MX

(22.31)

for arbitrary c , and where A- denotes the generalized inverse of A. When X has rank r  p, there are only N - r linearly independent vectors k satisfying (22.28).

22 REML estimation of mixed models is commonplace in statistical genetics and in the plant and animal breeding literature. In recent years, REML has in fact become the mixed model estimation method of choice in these fields, superceding ML and ANOVA.
23 Note this exercise is heuristic and serves only to motivate the REML approach. Under the stochastic assumptions (22.24) and (22.25), the least squares estimator of  is not BLUE. The BLUE of  is obtained by solving the mixed model equations (22.35).

22 Econometric Analyses

747

Define K = C MX with rows k satisfying (22.28), and where K and C have full row rank N - r. REML estimation of the variance parameters is maximum likelihood on K y under normality. For y  N (X , V) it follows that

K y  N 0, K VK

(22.32)

where V = DD 2 + FF 2 + R is the conditional covariance of y implied by (22.25). The REML log-likelihood (i.e., the log-likelihood of K y) is

log

LREML

=

-

1 2

(N



-

r)

log

2

-

1 2

log

K VK

- 1y K 2

K VK

-1 K y.

(22.33)

The REML estimator of the variance parameters has a number of attractive properties. First, REML estimates are invariant to the choice of K .24 Second, REML estimates are invariant to the value of the fixed effects (i.e.,  ). Third, in the balanced data case, REML is equivalent to ANOVA.25 Under normality, it thus inherits the minimum variance unbiased property of the ANOVA estimator.26 Finally, since
REML is based on the maximum likelihood principle, it inherits the consistency,
efficiency, asymptotic normality, and invariance properties of ML.
Inference based on REML estimates of the variance components parameters is
straightforward. Since REML estimation is just maximum likelihood on (22.33),
REML likelihood ratio tests (REMLRTs) can be used. In most cases, REMLRTs are
equivalent to standard likelihood ratio tests. The exception is testing for the presence of some random effect . The null is 2 = 0. Denote the restricted REML loglikelihood by log LR EML. The REMLRT statistic is  = -2 (log LR EML- log LREML) . Since the null puts 2 on the boundary of the parameter space under the alternative hypothesis,  has a non-standard distribution. Stram and Lee (1994) show the asymptotic distribution of  is a 50:50 mixture of a 02 and 12. The approximate p-value of the test is thus 0.5 1 - Pr 12   .

22.6.2 Estimating the Fixed Effects and Realized Random Effects
A disadvantage of REML estimation is that it provides no direct means for estimating the fixed covariate effects  . Henderson, in Henderson, Kempthorne, Searle and von Krosigk (1959) derived a system of equations that simultaneously yield the BLUE of  and Best Linear Unbiased Predictor (BLUP) of the random effects.
24 Subject to rows k satisfying (22.28). 25 The usual statistical definition of balanced data can be found in Searle (1987). Under this definitions, longitudinal linked data on employers and employees are balanced if we observe each worker employed at every firm, and all job spells have the same duration. Clearly, this is not the usual case. 26 In contrast, ML estimators of variance components are biased since they do not take into account degrees of freedom used for estimating the fixed effects.

748

J.M. Abowd et al.

These equations have become known as the mixed model equations or Henderson equations. Define the matrix of variance components

G=

2IN 0 0 2 IJ

.

(22.34)

The mixed model equations are

 X R-1X

D F

R-1X

X R-1 D F

  ~  

D F

R-1

DF

+

G-1





~ ~



=



X R-1y 

D F

R-1y 

(22.35)

where ~ denotes solutions for the fixed effects, and ~ and ~ denote solutions for the
random effects. In practice, of course, solving (22.35) requires estimates of R and G. Common practice is to use REML estimates G~ and R~.
The BLUPs ~ and ~ have the following properties. They are best in the sense of
minimizing the mean square error of prediction

E

~ ~

-

 

A

~ ~

-

 

(22.36)

where A is any positive definite symmetric matrix. They are linear in y, and unbiased in the sense E(~ ) = E ( ) and E(~ ) = E () .
The solutions to (22.35) also have a Bayesian interpretation. If we suppose
that the prior distribution for  is N (0, ) and the prior distribution for ( , ) is N (0, G), then the posterior mean E [( ,  , )|y]  (~ , ~ , ~ ), the solution of (22.35), as ||  . (See Goldberger (1962), Searle et al. (1992, pp. 331­333) and Robinson
(1991)).
The mixed model equations make clear the relationship between the fixed and mixed model estimation. In particular, as |G|   with R = 2IN , the mixed model equations (22.35) converge to the normal equations (22.18). Thus the mixed model solutions (~ , ~ , ~ ) converge to the least squares solutions (^ , ^ , ^ ). In this sense the
least squares estimator is a special case of the mixed model estimator.

22.6.3 Mixed Models and Correlated Random Effects Models
Since Chamberlain (1984) introduced his extension of methods by Crame´r (1946) and Mundlak (1978) for handling balanced panel data models with random effects that were correlated with the X variables, econometricians have generally referred to the Chamberlain class of models as "correlated random-effects models." Statisticians, on the other hand, usually mean the Henderson (1953) formulation of the mixed-effects model that gives rise to (22.35), with G nondiagonal, when they refer to a correlated random-effects model.
It is important to distinguish between correlated random-effects models based on the mixed model equations (G nondiagonal) and orthogonal design models, which

22 Econometric Analyses

749

can occur within either a fixed-effects or random-effects interpretation of the person and firm effects. Orthogonal design means that one or more of the following conditions hold:
X D = 0, orthogonal person-effect design and personal characteristics X F = 0, orthogonal firm-effect design and personal characteristics D F = 0, orthogonal person-effect and firm-effect designs

An economy with random assignment of persons to firms could satisfy these condi-

tions. However, virtually all longitudinal linked employer­employee data, as well

as most other observational data in economics, violate at least one of these or-

thogonal design assumptions. Recognition of the absence of orthogonality between

the effects is the basis for the fixed-effects estimator approximations discussed in

Sect. 22.5 and the difficulty associated with solving the mixed-model equations, in

general (see Robinson, 1991, Searle et al., 1992, Neumaier and Groeneveld, 1996,

and Groeneveld, 1998).

To relate the Chamberlain-style correlated random-effects model to the mixed

model estimator, we consider a single time-varying X, which we give the compo-

nents of variance structure:

xit = i + it

(22.37)

where

Corr[i, i] = 0

V[it ] = 

and Corr[it , ns] = 0 i, n, s,t

This specification implies that Corr[i, J(i,t)] = 0 as long as G is nondiagonal. Then, to derive the Chamberlain estimating system for a balanced panel data model, as-
sume that Ti = T for all i and compute the linear projection of yi on xi

yi = xi + i

(22.38)

where  is the T × T matrix of coefficients from the projection and i is the T × 1 residual of the projection. Chamberlain provides an interpretation of the coefficients in  that remains valid under our specification.
Because the firm effect is shared by multiple individuals in the sample, however, the techniques proposed by Chamberlain for estimating equation (22.38) require modification. The most direct way to accomplish the extension of Chamberlain's methods is to substitute equation (22.37) into equation (22.7), then restate the system of equations as a mixed model. For each individual i in period t we have

yit xit

=

i + J(i,t) + it i + it

.

(22.39)

750

J.M. Abowd et al.

where i = i + i and it = it + it  . Stacking yi and xi, define



mi 

yi xi

m1 , and m   · · · 
mN

All other vectors are stacked conformably. Then, the mixed-effects formulation of (22.39) can be written as

m = D1 + D2 + F3 + 

(22.40)

where D1, D2, and F3 are appropriately specified design matrices,  is the N × 1 vector of person effects entering the y equation,  is the N × 1 vector of person

effects entering the x equation, and



1



=



1 ··· N



N

is the stacked joint error vector. Problems of this form, with , , and  correlated and D1, D2, and F3 nonorthogonal look unusual to economists but are quite common in animal science and statistical genetics. Software to solve the mixed model equations and estimate the variance matrices for (22.40) has been developed by Groeneveld (1998) and Gilmour, Thompson and Cullis (1995) and some applications, other than the one presented above, are discussed in Robinson (1991) and Tanner (1996). The methods exploit the sparse structure of D1, D2, and F3 and use analytic derivatives to solve (22.35). Robert (2001) and Tanner (1996) provide algorithms based on simulated data techniques.

22.7 Models of Heterogeneity Biases in Incomplete Models
The analyses in this section are based upon the exact fixed-effects estimator for model (22.9) given by the solution to (22.18).

22.7.1 Omission of the Firm Effects
When the estimated version of (22.9) excludes the firm effects, , the estimated person effects,  , are the sum of the underlying person effects,  , and the employmentduration weighted average of the firm effects for the firms in which the worker was employed, conditional on the individual time-varying characteristics, X:

22 Econometric Analyses

751

  =  + (D MX D)-1D MX F.

(22.41)

Hence, if X were orthogonal to D and F, so that D MX D = D D and D MX F = D F, then the difference between   and  , which is just an omitted variable bias, would be an N × 1 vector consisting, for each individual i, of the employment-duration weighted average of the firm effects  j for j  {J(i, ni1), . . . , J(i, niT )}:

 i

-

i

=

Ti t=1

J(i,nit ) Ti

,

the person-average firm effect. Similarly, the estimated coefficients on the timevarying characteristics in the case of omitted firm effects,  , are the sum of the parameters of the full conditional expectation,  , and an omitted variable bias that depends upon the conditional covariance of X and F, given D:
  =  + (X MDX )-1X MDF.

22.7.2 Omission of the Person Effects

Omitting the pure person effects ( ) from the estimated version of (22.9) gives estimates of the firm effects, , that can be interpreted as the sum of the pure firm

effects, , and the employment-duration weighted average of the person effects of

all of the firm's employees in the sample, conditional on the time-varying individual

characteristics:

 =  + (F MX F)-1F MX D .

(22.42)

Hence, if X were orthogonal to D and F, so that F MX F = F F and F MX D = F D, the difference between  and , again an omitted variable bias, would be a J × 1 vector consisting of the employment-duration weighted average of person effects i for (i,t)  {J(i,t) = j and t  {ni1, . . . , niTi }} for each firm j. That is,

  N Ti



 j

-

j

=

i=1 t=1

i 1(J(i, nit ) = j) Nj

,

the firm-average person effect. The estimated coefficients on the time-varying characteristics in the case of omitted individual effects,  , are the sum of the effects of time-varying personal characteristics in (22.9),  , and an omitted variable bias that
depends upon the covariance of X and D, given F:

  =  + (X MF X )-1X MF D .

(22.43)

This interpretation applies to studies like Groshen (1991a, 1991b, 1996).

752
22.7.3 Inter-industry Wage Differentials

J.M. Abowd et al.

We showed above that industry effects are an aggregation of firm effects that may be inconsistently estimated if either person or firm effects are excluded from the equation. We consider these issues now in the context of inter-industry wage differentials as in Dickens and Katz (1987), Krueger and Summers (1987, 1988), Murphy and Topel (1987), Gibbons and Katz (1992). The fixed or random effects estimation of the aggregation of J firm effects into K industry effects, weighted so as to be representative of individuals, can be accomplished directly by estimation of (22.16). Only rank(F MFAF) fixed firm effects can be separately identified; however, the mixed-effects model can produce estimates of all realized industry and firm effects.
As shown in AKM, fixed-effects estimates of industry effects, , that are computed on the basis of an equation that excludes the remaining firm effects, MFAF, are equal to the pure industry effect, , plus an omitted variable bias that can be expressed as a function of the conditional variance of the industry effects, FA, given the time-varying characteristics, X, and the person effects, D:

-1
 =  + A F M D X FA A F M D X MFAF

which simplifies to  =  if, and only if, the industry effects, FA, are orthogonal to the subspace MFAF, given D and X, which is generally not true even though FA and MFAF are orthogonal by construction. Thus, consistent fixed-effects estimation of the pure inter-industry wage differentials, conditional on time-varying personal characteristics and unobservable non-time-varying personal characteristics requires identifying information on the underlying firms unless this conditional orthogonality condition holds. Mixed-effects estimation without identifying information on both persons and firms likewise produces realized inter-industry wage effects that confound personal and firm heterogeneity.
Similarly, AKM show that fixed-effects estimates of the coefficients of the timevarying personal characteristics,  , are equal to the true coefficients of the linear model (22.9),  , plus an omitted variable bias that depends upon the conditional covariance between these characteristics, X, and the residual subspace of the firm effects, MFAF, given D:

-1

 =  + X M

X

D FA

X M D FA MFAF

which, once again, simplifies to   =  if, and only if, the time-varying personal characteristics, X, are orthogonal to the subspace MFAF, given D and FA, which is also not generally true. Once again, both fixed-effects and mixed-effects estimation of the  coefficients produces estimates that confound personal and firm heterogeneity when both types of identifying information are not available.

22 Econometric Analyses

753

To assess the seriousness of the heterogeneity biases in the estimation of industry effects, AKM propose a decomposition of the raw industry effect into the part due to individual heterogeneity and the part due to firm heterogeneity. Their formulas apply directly to the fixed-effects estimator of (22.9) and can be extended to the estimated realized effects in a mixed-effects model. When (22.16) excludes both person and firm effects, the resulting raw industry effect, k, equals the pure industry effect, , plus the employment-duration weighted average residual firm effect inside the industry, given X, and the employment-duration weighted average person effect inside the industry, given the time-varying personal characteristics X:
 =  + (A F MX FA)-1A F MX (MFAF + D )

which can be restated as  = (A F MX FA)-1A F MX F + (A F MX FA)-1A F MX D ,

(22.44)

which is the sum of the employment-duration weighted average firm effect, given X and the employment-duration weighted average person effect, given X. If industry effects, FA, were orthogonal to time-varying personal characteristics, X, and to the design of the personal heterogeneity, D, so that A F MX FA = A F FA, A F MX F = A F F, and A F MX D = A F D, then, the raw inter-industry wage differentials, , would simply equal the pure inter-industry wage differentials, , plus the employment-duration-weighted, industry-average pure person effect, (A F FA)-1 A F D , or

  k

=

k

+

N Ti i=1 t=1

1[K(J(i,

nit )) Nk

=

k]i

Thus, statistical analyses of inter-industry differentials that exclude either person or firm effects confound the pure inter-industry wage differential with an average of the person effects found in the industry, given the measured personal characteristics, X.

22.8 Endogenous Mobility
The problem of endogenous mobility occurs because of the possibility that individuals and employers are not matched in the labor market on the basis of observable characteristics and the person and firm effects. A complete treatment of this problem is beyond the scope of this article; however, it is worth noting that the interpretation of (22.7) and (22.9) as conditional expectations given the person and firm effects is not affected by some forms of endogenous mobility. If the mobility equation is also conditioned on X, D, and, F, then the effects in the referenced equations are also structural as long as mobility does not depend upon .
Matching models of the labor market, such as those proposed by Jovanovic (1979) and Woodcock (2003) imply the existence of a random effect that is the interaction

754

J.M. Abowd et al.

of person and firm identities. Such models are amenable to the statistical structure laid out in Sect. 22.6; however, to our knowledge the application of such techniques to this type of endogenous mobility model has only been attempted recently using linked employer-employee data. We present these attempts now.

22.8.1 A Generalized Linear Mixed Model

Mixed model theory and estimation techniques have been applied to nonlinear models with linear indices. These are usually called generalized linear mixed models, and include such familiar specifications as the probit, logit, and tobit models augmented to include random effects. See McCulloch and Searle (2001) for a general discussion.
Woodcock (2003) estimates a mixed probit model with random person and firm effects as the first step of a modified Heckman two-step estimator. The goal is to correct for truncation of the error distribution in a mixed model of earnings with random person and firm effects. This truncation arises from endogenous mobility in the context of an equilibrium matching model. Specifically, the Woodcock (2003) matching model predicts that earnings are observed only if the worker-firm match continues, and that the continuation decision depends on person-, firm-, and tenurespecific mobility effects that are correlated with the person and firm effects in the earnings equation. At tenure , the match continues only if it  ¯i where

¯i = - - i -  j

i  j

N

0 0

,

2 0 0 2

.

(22.45)

When it  N (0,V ) , the marginal probability of observing the earnings outcome yit is

Pr (it  ¯i ) = 1 - 

- - i -  j V1/2

=

 + i +  j V1/2

(22.46)

where  is the standard normal CDF. Then we have

 E [yit |it  ¯i ] =  + xit  + i +  j +V1/2

=  + xit  + i +  j + V1/2i

 +i + j V1/2
 +i + j V1/2

where i is the familiar Inverse Mills' Ratio.

(22.47)

22 Econometric Analyses

755

The truncation correction based on (22.46) and (22.47) proceeds as follows. The first step is to estimate a continuation probit at each tenure level with random personand firm-specific mobility effects i and  j . Woodcock (2003) estimates probits using the Average Information REML algorithm of Gilmour et al. (1995), applied to the method of Schall (1991). The Schall (1991) method extends standard methods for estimating generalized linear models to the random effects case. The basic idea is to perform REML on a linearization of the link function . The process requires an iterative reweighting of the design matrices of fixed and random effects in the linearized system, see Schall (1991) for details. With estimates of the realized random effects ~it and ~j in hand, Woodcock (2003) constructs an estimate ~ i of the Inverse Mills' Ratio term for each observation. Including ~ i as an additional time-varying covariate in the earnings equation corrects for truncation in the error distribution due to endogenous mobility.

22.8.2 A Model of Wages, Endogenous Mobility and Participation with Person and Firm Effects
Following Buchinsky, Fouge`re, Kramarz and Tchernis (2003), and the structural interpretation they develop, Beffy, Kamionka, Kramarz and Robert (2003, BKKR hereafter) jointly model wages with a participation equation and an inter-firm mobility equation that include state-dependence and unobserved heterogeneity. A firmspecific unobserved heterogeneity component is added to the person-specific term. Like the linear models discussed in detail above, the wage equation includes person and firm effects.
Inter-firm mobility at date t depends on the realized mobility at date t - 1. Similarly, participation at date t depends on past participation and mobility. Hence, we include initial conditions, modeled following Heckman (1981). This yields the following system of equations:
Initial Conditions:
zi1  U1,...,J yi1 = I XiY1Y0 + zYi1,E + vi1 > 0
wi1 = yi1 XiW1 W + zWi1,E + i1
mi1 = yi1I XiM1 0M + zMi1,E + ui1 > 0 .
Main Equations: t > 1,
zit = yit-1 ((1 - mit-1)zit-1 + mit-1it ) + (1 - yit-1)   U1,...,J it  U(1,...,J)-(zit-1)

756


J.M. Abowd et al.


yit = I Mmit-1 + Y yit-1 + XiYt Y + zYit,E + iY,I + vit > 0
yit

wit = yit XiWt W + zWit ,E + iW,I + it





mit = yit I mit-1 + XiMt M + zMit ,E + iM,I + uit > 0 .
mit

The variable zit denotes the latent identifier of the firm and J(i,t) denotes the realized identifier of the firm at which worker i is employed at date t. Therefore, J(i,t) = zit if individual i participates at date t. yit and mit denote, respectively, participation and mobility, as previously defined. yit is an indicator function, equal to 1 if the individual i participates at date t. mit is an indicator function that takes values according to Table 22.1.
The variable wit denotes the logarithm of the annualized total labor costs. The variables X are the observable time-varying as well as the time-invariant characteristics for individuals at the different dates. Here,  I and  E denote the random effects specific to, respectively, individuals or firms in each equation. u, v and  are the error terms. There are J firms and N individuals in the panel of length T .

22.8.3 Stochastic Assumptions

In order to specify the stochastic assumptions for the person and firm-effects, BKKR first rewrite their system of equations as:

zit = yit-1 ((1 - mit-1)zit-1 + mit-1it ) + (1 - yit-1)



yit = I Mmit-1 + Y yit-1 + XiYt Y + Ezit Y,E + Iit Y,I + vit > 0
yit

Table 22.1 Mobility Indicator

yit = 1 yit = 0

yit+1 = 1 mit = 1 if J(i,t + 1) = J(i,t) mit = 0 if J(i,t + 1) = J(i,t) mit = 0 p.s.

yit+1 = 0 mit censored
mit = 0 p.s.

22 Econometric Analyses

757

wit = yit

XiWt 

W

+

zEit

 W,E

+

Iit W,I

+

it



mit = yit .I mit-1 + XiMt M + Ezit  M,E + Iit  M,I + uit > 0
mit

for each t > 1, where Eit is a design matrix of firm effects for the couple (i,t). Hence, it is a 1 × J matrix composed of J - 1 zeros and of a 1 at column zi,t. Similarly, Iit is a 1 × N matrix composed of N - 1 zeros and of a 1 at column i. The model includes two dimensions of heterogeneity. This double dimension crucially
affects the statistical structure of the likelihood function. The presence of firm ef-
fects makes the likelihood non-separable (person by person). Indeed, the outcomes
of two individuals employed at the same firm, not necessarily at the same date, are
not independent.
The next equations present the stochastic assumptions for the person and firm
effects:

 E = Y,E , M,E , Y,E , W,E ,  M,E of dimension [5J, 1]  I = Y,I, W,I,  M,I of dimension [3N, 1].

Moreover,

 E | E  N (0, DE0 )  I| I  N (0, D0I ) D0E =  E  IJ DI0 =  I  IN

(22.48) (22.49) (22.50) (22.51)

where  E (resp.  I) is a symmetric positive definite matrix [5, 5] (resp. [3, 3]) with

mean zero. Notice that these assumptions imply that correlations between the wage,

the mobility, and the participation equations come from both person and firm hetero-

geneity (in addition to that coming from the idiosyncratic error terms). Furthermore,

these assumptions exclude explicit correlation between different firms (for instance,

the authors could have considered a non-zero correlation of the firm effects within

an industry, a non-tractable assumption). Notice though that BKKR could have in-

cluded in the wage equation, for instance, the lagged firm effects of those firms at

which a worker was employed in her career. This is difficult, but feasible in this

framework.

Finally, they assume that the idiosyncratic error terms follow:



  



vit

0

1 yw ym

 it  iid N  0  ,  yw  2 wm  .

uit

0

ym  wm 1

758

J.M. Abowd et al.

Notice that experience and seniority are complex and highly non linear functions of the participation and mobility equations. Because all these person and firm effects are correlated between equations, the presence of experience and seniority in the wage equation induces a correlation between these two variables and the person and the firm effect in the same equation. Indeed, in the terminology introduced above, the BKKR model exhibits correlated random effects.
BKKR estimate this model on French data using Monte-Carlo Markov Chain methods (Gibbs sampling and the Hastings-Metropolis algorithms).

22.9 Conclusion
We have presented a relatively concise tour of econometric issues surrounding the specification of linear models that form the basis for the analysis of linked longitudinal employer­employee data. Our discussion has focused on the role of person and firm effects in such models, because these data afford analysts the first opportunity to separately distinguish these effects in the context of a wide variety of labor market outcomes. We have shown that identification and estimation strategies depend upon the observed sample of persons and firms (the design of the person and firm effects) as well as on the amount of prior information one imposes on the problem, in particular, the choice of full fixed-effects or mixed-effects estimation.
We do not mean to suggest that these estimation strategies are complete. Indeed, many of the methods described in this chapter have been used by only a few analysts and some have not been used at all in the labor economics context. We believe that future analyses of linked employer­employee data will benefit from our attempt to show the relations among the various techniques and to catalogue the potential biases that arise from ignoring either personal or firm heterogeneity.

References
Abowd, J.M., R. Creecy, and F. Kramarz, "Computing Person and Firm Effects Using Linked Longitudinal Employer­Employee Data," Cornell University working paper, (2002).
Abowd, J.M., H. Finer, and F. Kramarz, "Individual and Firm Heterogeneity in Compensation: An Analysis of Matched Longitudinal Employer and Employee Data for the State of Washington," in J. Haltiwanger, J. Lane, J. Spletzer, K. Troske eds, The Creation and Analysis of Employer­ Employee Matched Data, Amsterdam, North-Holland, (1999): 3­24.
Abowd, J.M. and F. Kramarz, "The Analysis of Labor Markets Using Matched EmployerEmployee Data," in Handbook of Labor Economics, O. Ashenfelter and D. Card, (eds). chapter 26, volume 3B, (Amsterdam, North Holland, (1999a): 2629­2710.
Abowd, J.M. and F. Kramarz, "The Analysis of Labor Markets Using Matched Employer­ Employee Data," Labour Economics 6, (1999b): 53­74.
Abowd, J.M, F. Kramarz, and D.N. Margolis, "High Wage Workers and High Wage Firms," Econometrica 67, 2, (1999): 251­333.

22 Econometric Analyses

759

Abowd, J.M, F. Kramarz, P. Lengermann, and S. Perez-Duarte, "Are Good Workers Employed by Good Firms? A Test of a Simple Assortative Mating Model for France and the United States," Crest working paper (2003).
Abowd, J., F. Kramarz, P. Lengermann, and S. Roux "Persistent Inter-Industry Wage Differences: Rent-Sharing and Opportunity Costs," Crest Working Paper, (2005).
Abowd, J.M. and M.H. Stinson, "Estimating Measurement Error in SIPP Annual Job Earnings: A Comparison of Census Survey and SSA Administrative Data," US Census Bureau LEHD Program Working Paper (2003).
Abowd, J.M. and S. Woodcock, "Disclosure Limitation in Longitudinal Linked Data," in Confidentiality, Disclosure, and Data Access: Theory and Practical Applications for Statistical Agencies, P. Doyle, J. Lane, L. Zayatz, and J. Theeuwes (eds.), Amsterdam: North Holland, (2001), 215­277.
Buchinsky M., D. Fouge`re, F. Kramarz, and R. Tchernis, "Interfirm Mobility, Wages, and the Returns to Seniority and Experience in the U.S.," Crest working paper (2003).
Beffy M., T. Kamionka, F. Kramarz, and C.P. Robert, "Job Mobility and Wages with Worker and Firm Heterogeneity", Crest working paper, (2003).
Chamberlain, G., "Panel Data," in Handbook of Econometrics, ed. by Z. Griliches and M.D. Intrilligator. Amsterdam: North Holland, chapter 22, (1984): 1248­1318.
Crame´r, H., Mathematical Models of Statistics (Princeton, NJ: Princeton University Press, (1946). Dickens, W.T. and L.F. Katz, "Inter-Industry Wage Differences and Industry Characteristics," in
Unemployment and the Structure of Labor Markets, (eds.) Kevin Lang and Jonathan S. Leonard (eds.). Oxford: Basil Blackwell (1987). Dongarra, J., I. Duff, D. Sorensen and H. Van der Vorst, Solving Linear Systems on Vector and Shared Memory Computers, Philadelphia: SIAM, (1991). Dostie, B., "Job Turnover and the Returns to Seniority," Journal of Business and Economic Statistics, 23, 2, (2005): 192­199. Gibbons, R. and L. Katz, "Does Unmeasured Ability Explain Inter-Industry Wage Differentials?" Review of Economic Studies, 59, (1992): 515­535. Gilmour, A.R., R. Thompson, and B.R. Cullis, "Average Information REML: An Efficient Algorithm for Variance Parameter Estimation in Linear Mixed Models." Biometrics, 51, (1995): 1440­1450. Goldberger, A.S., "Best Linear Unbiased Prediction in the Generalized Linear Regression Model," Journal of the American Statistical Association, 57 (1962): 369­75. Groeneveld, E., VCE4 User's Guide and Reference Manual (Ho¨ltystrass, Germany: Institute of Animal Husbandry and Animal Behavior, (1998). Groshen, E., "Sources of Intra-Industry Wage Dispersion: How Much do Employers Matter?" Quarterly Journal of Economics, 106, (1991a): 869­884. Groshen, E., "The Structure of the Female/Male Wage Differential: Is it Who You Are, What You Do, or Where You Work?" Journal of Human Resources, 26, (1991b): 457­472. Groshen, E., "American Employer Salary Surveys and Labor Economics Research: Issues and Contributions," Annales d'e´conomie et de statistique, 41/42, (1996): 413­442. Heckman J.J., "Heterogeneity and State Dependence", in Studies in Labor Market, Rosen S. (ed.) University of Chicago Press, (1981). Henderson, Charles Roy, "Estimation of Variance and Covariance Components," Biometrics 9 (1953): 226­252. Henderson, Charles Roy, O. Kempthorne, S.R. Searle and C.M. Von Krosigk, "The Estimation of Environmental and Genetic Trends from Records Subject to Culling," Biometrics, 15, 2, (1959): 192­218. Jovanovic, B.,"Job Matching and the Theory of Turnover," Journal of Political Economy, 87, (1979): 972­990. Krueger, A.B. and L.H. Summers, "Reflections on the Inter-industry Wage Structure," in Unemployment and the Structure of Labor Markets, Kevin Lang and Jonathan S. Leonard (eds.), New York: Basil Blackwell, (1987).

760

J.M. Abowd et al.

Krueger, A.B. and L.H. Summers, "Efficiency Wages and the Inter-Industry Wage Structure," Econometrica, 56, (1988): 259­293.
Lane, J., S. Burgess and J. Theeuwes, "The Uses of Longitudinal Matched Employer/Employee Data in Labor Market Analysis." Proceedings of the American Statistical Association, (1998).
Lillard, L.A., "Job Turnover Heterogeneity and Person-Job-Specific Time-Series Wages," Annales D'E´conomie et de Statistique, 55­56 (1999): 183­210.
Lillard, L.A. and C.W. Panis, aML Multilevel Multiprocess Statistical Soft-ware, Release 1.0 EconWare, Los Angeles, California (2000).
McCulloch C.E. and S.R. Searle, Generalized, Linear, and Mixed Models, New York: John Wiley and Sons (2001).
Mundlak, Y., "On the Pooling of Time Series and Cross Section Data," Econometrica, 46 (1978): 69­85.
Murphy, K.M. and R.H. Topel, "Unemployment, Risk, and Earnings: Testing for Equalizing Wage Differences in the Labor Market" in Unemployment and the Structure of the Labor Market, Kevin Lang and Jonathan S. Leonard (eds.). New York: Basil Blackwell, (1987).
Neumaier, A. and E. Groeneveld, "Restricted Maximum Likelihood Estimation of Covariance in Sparse Linear Models," working paper, Institut fu¨r Mathematik, Wien University, Austria, (1996).
Raghunathan, T.E., J.M. Lepkowski, J. Van Hoewyk, and P. Solenberger, "A Multivariate Technique for Multiply Imputing Missing Values Using a Sequence of Regression Models," Survey Methodology, 27, (2001): 85­95.
Robert C.P., The Bayesian Choice: from Decision-Theoretic Motivations to Computational Implementation, New York: Springer-Verlag, (2001).
Robinson, G.K., "That BLUP is a Good Thing: The Estimation of Random Effects," Statistical Science, 6, (1991): 15­51.
Schall R., "Estimation in Generalized Linear Models with Random Effects," Biometrika, 78, 4, (1991): 719­727.
Searle, S.R., Linear Models for Unbalanced Data, New York: John Wiley and Sons, 1987. Searle, S.R., George Casella and Charles E. McCulloch, Variance Components New York: John
Wiley and sons, (1992). Stram, D.O. and J.W. Lee, "Variance Component Testing in the Longitudinal Mixed Effects
Model," Biometrics, 50, (1994): 1171­1177. Tanner, M.A., Tools for Statistical Inference: Methods for the Exploration of Posterior Distribu-
tions and Likelihood Functions New York: Springer, (1996). Weeks, D.L. and D.R. Williams, "A Note on the Determination of Connectedness in an N-way
cross classification." Technometrics, 6, (1964): 319­324. Woodcock, S.D., "Heterogeneity and Learning in Labor Markets", in Essays on Labor Market
Dynamics and Longitudinal Linked Data, Cornell University Ph.D. Thesis (2003).

Chapter 23
Life Cycle Labor Supply and Panel Data: A Survey
Bertrand Koebel, Franc¸ois Laisney, Winfried Pohlmeier and Matthias Staat

23.1 Introduction
The econometrics of labor supply belongs to one of the technically most advanced fields in microeconometrics. Many specific issues such as the proper modelling of tax structures, the existence of fixed costs as well as rationing have been treated in numerous articles so that marginal gains in substantive economic insights seem low and entry costs into the field prohibitively high. Not surprisingly, one of the most obvious paths for research on labor supply, the (micro-) econometric analysis of the individual's labor supply over the life cycle, has by now gained much more attention than 10 years ago. The increased availability of panel data for many countries, as well as the development of appropriate econometric techniques, have made econometric studies of intertemporal labor supply behavior using panel data not only interesting on purely theoretical grounds, they have also helped to achieve a better understanding of individual retirement behavior, the functioning of institutional settings in different countries (such as taxes, vocational training programmes, day-care for children) and the distribution of income and wealth, to name only a few.
Estimation of labor supply functions using panel data has started out in the eighties, and the number of studies reporting on such estimation is rapidly increasing.
Bertrand Koebel BETA, Universite´ Louis Pasteur, Strasbourg I and IZA, Bonn, 61 Avenue de la Fore^t Noire, F67000 Strasbourg, France, e-mail: koebel@cournot.u-strasbg.fr
Franc¸ois Laisney BETA, Universite´ Louis Pasteur, Strasbourg I and ZEW, Mannheim, 61 Avenue de la Fore^t Noire, F67000 Strasbourg, France, e-mail: fla@cournot.u-strasbg.fr
Winfried Pohlmeier University of Konstanz, Department of Economics, Box D124, D78457 Konstanz, Germany, e-mail: winfried.pohlmeier@uni-konstanz.de
Matthias Staat University of Mannheim, Department of Economics, D68131 Mannheim, Germany, e-mail: matthias@pool.uni-mannheim.de

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

761

c Springer-Verlag Berlin Heidelberg 2008

762

B. Koebel et al.

Earlier studies using panel data mainly concentrated on participation. Thus, it is not surprising that the excellent surveys of Pencavel (1986), Heckman and MaCurdy (1986) and Killingsworth and Heckman (1986) hardly touched the subject.1 The latter survey concluded a comparison of a large number of cross section studies with the words: "[these studies] seem to have reduced the mean and substantially increased the variance of [. . .] what might be called the reasonable guesstimate of the wage elasticity of female labour supply [. . .].2 However, [. . .] studies based on alternative behavioural models--notably, life cycle models, which have been used relatively little in empirical studies--are also likely to provide important insights" (pp. 196­197).
Earlier surveys of some of the material covered here can be found in Blundell (1987, 1988), Blundell, Fry and Meghir (1990), Card (1994), MaCurdy et al. (1990) and Blundell and MaCurdy (1999).
As we shall see, there has been a trend away from models that take advantage of panel data almost exclusively in order to control for unobserved heterogeneity, towards fully dynamic models where wages become endogenous, and consequently the concept of wage elasticity loses much of its appeal.
This chapter aims at providing the reader with a thread through the literature on the topic. However, we make no claim to exhaustivity, and concentrate mainly on the theoretical aspects of the studies. In Sect. 23.2 we describe the basic model of life cycle  -constant labor supply. Sect. 23.3 is devoted to extensions taking account of uncertainty and risk, while Sect. 23.4 discusses voluntary and involuntary nonparticipation, as well as accounting for taxation. Sect. 23.5 presents an alternative specification which leaves the  -constant framework, and discusses its implications, in particular for modelling the impact of taxes on labor supply. In Sect. 23.6 we discuss studies relaxing within-period and between-period additive separability, and focusing on rational habit formation and human capital accumulation. Sect. 23.7 concludes and opens towards other strands of the literature that contribute to the understanding of labor supply.

23.2 The Basic Model of Life Cycle Labor Supply
We shall not restate here the theoretical developments contained in the survey of Killingsworth and Heckman (1986) (pp. 144­179) but refer the reader to them. Killingsworth and Heckman insist on the pioneering work of Mincer (1962). They show that "the distinction between permanent and transitory wages is not particularly useful from a theoretical standpoint" (p. 158) and demonstrate the usefulness of Frisch demands as an alternative to the permanent vs. transitory distinction. 3 They also discuss models with endogenous wages and conclude: "although much informal discussion implicitly or explicitly emphasizes the interrelationships be-
1 Yet see Chap. 5 in Killingsworth (1983), pp. 207­330. 2 Here we shall not restrict attention to female labor supply. 3 The uninformed reader will find a definition below.

23 Life Cycle Labor Supply and Panel Data

763

tween (. . .) work and wages in a life-cycle setting, rigorous analysis of such issues using formal life-cycle labour supply models with endogenous wages is still in its infancy" (p. 178). Here we will describe the models used for estimation in a selection of papers representative of the trend over the last 25 years. Along the way we also give some details on the estimation techniques and on the results, illustrating the fact that econometric modelling is by no means linear: there is a feedback of estimation results on model specification.

23.2.1 The Framework

The seminal paper, as far as empirically implementable models are concerned, is MaCurdy (1981).4 The assumptions retained are fairly stringent and include known
life length T , perfect foresight and perfect credit markets, as well as rates of time
preference that may differ across individuals and do not change over time. At time 1 an individual chooses {Cit , Lit , Ait }tT=1 in order to maximizes discounted utility

T

1

t=1 (1 + i)t-1 Uit (Cit , Lit )

(23.1)

subject to the sequence of budget constraints

Ait = (1 + rt ) Ai,t-1 + wit Nit -Cit , t = 1, . . . , T .

(23.2)

The variable C denotes real consumption, L leisure, A end of period assets in real terms, N hours of work (N = L¯ - L, where L¯ denotes maximum time available in each period for allocation between leisure and market work), r is the real interest rate, w the real wage,  the rate of time preference, and A0 denotes initial assets. The within-period utility function Uit is assumed to be concave.
The first-order conditions, assuming an interior optimum, include the budget
restrictions (23.2) and

 Uit  Cit

= it ,

 Uit  Lit

= it wit ,

t = 1, . . . , T,

(23.3) (23.4)

where it denotes the Lagrange multiplier of the budget constraint in period t. Notice that (pseudo) optimal demands can be derived by solving (23.2)­(23.4), eliminating it , to obtain Cit (Sit , wit ), Lit (Sit , wit ), where Sit  Ait - (1 + rt ) Ai,t-1 denotes the level of saving or dissaving.5 In the timewise additive separable case, net saving Sit is a sufficient statistic of all the future as far as the present decision is concerned. In

4 For the purpose of comparability with later sections, we slightly depart from MaCurdy's exposition and notations. 5 MaCurdy (1983, p.271) calls C and L pseudo demand functions.

764

B. Koebel et al.

general, the argument Sit of functions Cit and Lit will not be arbitrary, but optimally chosen by individuals. For this reason it will depend of the entire wage profile, of
the initial wealth Ai0 and of the interest and time preference rates. This functional dependence in general implies correlation between Sit and the past and future variables, and shocks and thus calls for instrumental variable estimation methods.
Instead of considering C and L, MaCurdy (1981) derives the Frisch demands Cit (it , wit ) , Lit (it , wit ), obtained by solving (23.3)­(23.4). The Lagrange multiplier it measures the impact of a marginal increase in Ait on the optimal value of objective (23.1). From the envelope theorem, we have

it

=

1 + rt+1 1 + i

it+1

,

(23.5)

or, using a first-order approximation around i = rt+1 = 0,

ln it  rt+1 - i + ln i,t+1 .

(23.6)

The value of it is implicitly determined by substitution of the demand functions C and L in (23.2). Thus, it is a function of the entire wage profile, of the initial wealth Ai0 and of the interest and time preference rates rt and i. Just as Sit , it is a sufficient statistic which summarizes the impact of all the future variables on the
present decision. As before, the use of instrumental variables is recommended for
parameter estimation. Using (23.5) and (23.6), we can write

 it

=

i0

t k=1

1 + i 1 + rk

(23.7)

or, assuming small values for i and the rk,

ln it  ti - Rt + ln i0 ,

(23.8)

where Rt = tk=1 rk, and substitute this term into functions C and L to obtain Cit (wit , i0) and Lit (wit , i0) .

The concavity of Uit implies

 Cit  wit

 0,

 Cit  i0

 0,

and

 i0  Ai0

 0,

 Lit  wit

 0,

 Lit  i0

 0,

 i0  wit

 0,

 2Lit  i20

 0,

t = 1, . . . , T.

(23.9)

where i0 {wit }tT=0 , Ai0 is the value of the multiplier corresponding to the optimal solution.

23 Life Cycle Labor Supply and Panel Data

765

Both types of demand functions are related by:

L (i0, wit ) = L (Si0, wit ) .

Two measures have focused the interest of economists: the Frisch elasticity of labor supply with respect to the wage, denoting Nit = N (i0, wit ),

e



 Nit  wit

wit Nit

,

and the intertemporal elasticity of substitution between labor supplies of two consecutive periods:

ies 

 Nit /Ni,t+1  (wit /wi,t+1)

wit /wi,t+1 Nit /Ni,t+1

,

which gives the inverse of the percentage change in the relative labor supplies

(of two consecutive periods), when the ratio of relative wages wit /wi,t+1 increases by 1%.

23.2.2 First Specifications of the Utility Function

MaCurdy (1981) specifies the following additively separable within-period utility function for individual i:

Uit (Cit , Lit ) = CitCit - Nit Nit N , [Nit = L¯ - Lit ], i = 1, . . . , I .

(23.10)

Concavity requires 0 <  < 1, N > 1. Heterogeneity, both observed and unobserved, is modelled through random preferences with the specification

ln Nit = i - uit ,

(23.11)

where uit is i.i.d. with zero expectation (note that time­varying characteristics are excluded by assumption).
The resulting Frisch labor supply and consumption demand equations are:

ln Nit

=

1 N -

1

(ln

it

- ln N

+ ln wit

- i

+ uit )

lnCit

=



1 -

1 (ln it

-

ln Cit

-

ln  ).

(23.12)

Using (23.8), we obtain (assuming i = )

ln Nit = Fi + bt - Rt +  ln wit + uit

(23.13)

766

B. Koebel et al.

with

Fi

=

1 N -

1

(ln i0

- i

-

ln N),



=

1 N - 1 ,

b = ,

uit = -uit .

This is a linear panel model with an individual-specific effect Fi, which has to be treated as a fixed effect because it is correlated with wit via i0. Notice that when the i are not all identical, there is in addition heterogeneity in the parameter b. In this model, the Frisch elasticity of labor supply is given by  = 1/ (N - 1) and is also equal to the intertemporal elasticity of substitution.
Moreover, MaCurdy considers the following linear approximation of Fi:

T
 Fi = Zi + t ln wit + Ai0 + i , t=1

(23.14)

where Zi denotes a vector of household characteristics and i a residual term. According to (23.9), t and  should be negative. Note that coefficients are identical across households. Combined with the additional assumption of a quadratic form
for the profile of log wages,

ln wit = 0i + 1it + 2it2 + it ,

(23.15)

this leads to

Fi = Zi + 0i0 + 1i1 + 2i2 + Ai0 + i ,

(23.16)

with

T

 j =  tt j,

j = 0, 1, 2 .

t=1

Interpretation:  is the intertemporal substitution (or  -constant, or Frisch) elasticity. It describes the reaction to an evolutionary change of the wage rate along the wage profile. It is positive since N > 1. Along a profile, evolutionary changes take place. MaCurdy calls changes between profiles parametric or profile changes. A change  > 0 from a wage profile I to an otherwise identical profile II at time s causes the Frisch labor supply of profile II to be lower than that of profile I in all periods t = s, because II < I by (23.9). Equation (23.14) implies

FII - FI = s < 0 .

The net effect on labor supply in period s, ( + s), can be positive or negative.  + s and s are the usual uncompensated (own- and cross-period) elasticities, and the corresponding compensated elasticities are  + s - Es and s - Es , respectively, where Es denotes real earnings in period s. If leisure is a normal good ( < 0), we have
 >  + s - Es >  + s ,
i.e. e > eu > eA ,

23 Life Cycle Labor Supply and Panel Data

767

where e is the wage elasticity with constant marginal utility of wealth, eA is the wage elasticity with constant (lifetime) wealth and eu is the wage elasticity with constant (lifetime) utility. Bover (1989) and Blundell, Meghir and Neves (1993)
give useful discussions of the relationships between these elasticities.
Estimation is conducted in two stages. Stage 1: (23.12) is estimated in first differences:6

 ln Nit = b - rt+1 +  ln wit + it , t = 2, . . . , , i = 1, . . . , I .

MaCurdy (1981) considers the Frisch labor supply equations across the  available time periods as a system. No restrictions are imposed on the temporal covariance structure of . As the level of wages may depend upon unobserved individual characteristics which also affect the amount of working time, the variable wit can be suspected to be correlated with it . MaCurdy uses system estimation (2SLS and 3SLS), and treats ln wit as endogenous, with instruments derived from a human capital type equation.
In this way, the reactions of Nit to the evolutionary changes in wit are completely described by . In order to also describe the reactions of labor supply to parametric changes in wages, information on the sensitivity of Fi with respect to wit is needed.
Stage 2: Given the first stage parameter estimates, the fixed effects can be esti-
mated using (23.12) as:

 F^i

=

1 

 t=1

ln Nit - b^t + ^Rt - ^ ln wit

.

(23.17)

A similar method is used to obtain estimates of the hi parameters, which then allows to estimate the unknown parameter of (23.16). These estimates can then be used to identify the labor demand reaction to a shift in the wage profile and to obtain an estimate of the wage elasticity eA.
Note that there are also contributions estimating pseudo supply functions. For instance, Conway and Kniesner (1994) consider the following econometric specification:
Nit = Fi + wit + Sit + Zit  + uit ,
which is a linear pseudo labor function (depending upon savings), where variables wit and/or Sit are allowed to be correlated with the random term uit and individual specific heterogeneity Fi. They use a sample of prime aged men from the PSID who worked each year from 1978 to 1982 and experiment with different types of instruments. They find that pseudo labor supply is decreasing in the wage in 59 out of the 60 regressions considered. This finding is at odds with (23.9).

6 Henceforth,  will denote the first difference operator. Another possibility would be to use within estimation. One advantage of estimation in first differences, however, is that no strict exogeneity assumption is needed.

768
23.3 Taking Account of Uncertainty and Risk

B. Koebel et al.

So far we have only considered labor substitution over time, which measures changes in labor supply in response to anticipated wage changes. How individuals react in response to unanticipated wage and interest rate changes is important to better understand the labor market impacts of monetary and fiscal policies for example. The labor market implications of wage and interest rate volatility may also have consequences for the optimal design of labor contracts and the organization of financial markets.
MaCurdy (1983) was the first to propose an empirical framework allowing to cope with uncertainty. He showed that uncertainty concerning wages and interest rates can be accounted for by slightly adapting the model with certainty, so that most uncertainty can be summarized into an additive residual term. The use of adequate instruments then allows to consistently estimate the parameters of interest. Some 20 years later, Pistaferri (2003) showed that a more precise modelling of uncertainty yields a different specification of labor supply relationships. This allows economists to study how labor supply reacts to unanticipated changes not only in wages and interest rates, but also in other dimensions like wealth or family composition. As soon as uncertainty is introduced in the model, risk also naturally arises in the specification of labor supply. Lich-Tyler (2002) investigated this second issue. Both topics are related and can be presented within a comprehensive framework.

23.3.1 First Developments

Following MaCurdy (1983), we assume uncertainty concerning future wages and interest rates. Replanning for the future takes place in every period, on the basis of the new information obtained. The individual maximizes expected discounted utility in period t:

T

1

Et s=t (1 + i)s-t Uis(Cis, Lis) ,

(23.18)

subject to the budget restriction (23.2). If we exclude corner solutions, the first-order conditions include (23.3) and (23.4) at period t = 1.
As in static models, the ratio of first derivatives is still equal to relative prices, so that this can provide the basis for estimating demand elasticities. This estimation strategy was followed by MaCurdy (1983), using instrumental variables for controlling the endogeneity of Cit and Lit . Note that also the functions Cit and Lit are just the same as in the certain case. This might suggest that differences between the certain and uncertain cases are not important in the time additive separable case. However, the level of saving Sit chosen in period t for some configuration of expected future wage and interest rate paths, can turn out not having been optimal ex post, once time discloses additional information. This is why replanning is necessary at each period.

23 Life Cycle Labor Supply and Panel Data

769

The Lagrange multipliers now satisfy

it = Et

1 + rt+1 1 + i

i,t

+1

,

(23.19)

implying that the individual decides on savings in such a way that the discounted expected utility of wealth remains constant. If we assume that there is no uncertainty about rt+1 we have

it

=

1 + rt+1 1 + i

Et

i,t

+1

,

which leads to the (first-order) approximation

ln it  Et ln i,t+1 - i + rt+1 = ln i,t+1 - i + rt+1 + ei,t+1,

(23.20)

where the random term ei,t+1, a forecast error of the marginal utility of next period, satisfies Et (ei,t+1) = 0. Once substituted in the  -constant demands in first difference obtained from (23.12):

 ln Nit   ln wit +  (ln i,t+1 - ln it ) ,

(23.21)

this yields

 ln Nit   ln wit +  (i - rt+1) - et+1 .

(23.22)

From (23.19) and the expression N (it , wit ) of Frisch labor supply, it can be seen that expected changes in i,t+1 are already taken into account for determining labor supply at period t. As a consequence, only unexpected changes in the marginal utility of wealth influence changes in labor supply through ei,t+1. This is the economic interpretation of the residual term in (23.22).
As in the certain case, the  -constant demands can be relied on for estimation. The "fixed effects" techniques remain available in the presence of uncertainty about the wage profile. Under rational expectations, the orthogonality between ei,t+1 and the information available at time t suggests application of the Generalized Method of Moments (GMM). Exposition here has been kept fairly sketchy, and we refer the reader to Altug and Miller (1990) for a more elaborate treatment spelling out the implications of assuming a competitive environment with complete markets.
Others contributions in this vein investigate the impact of unexpected capital, windfall gains, house price shocks and inheritance on labor supply: see Joulfaian and Wilhelm (1994) and Henley (2004). Both studies report that unexpected gains exert (mostly) significant negative effects on working hours, but their impact is relatively small in absolute value.

770
23.3.2 Recent Contributions

B. Koebel et al.

Now we turn to the contributions of Pistaferri (2003) and Lich-Tyler (2002) , who derive a labor supply specification from a more precise approximation of the relationship between consecutive marginal utilities of wealth (23.19). Although we do not follow exactly each author's presentation, we hope that our interpretation does a good job of summarizing the main novelty of both contributions.
Without (intra-period) additive separability between consumption and leisure (see next section), the  -constant demands in first differences (23.21) become

 ln Nit   ln wit +  (ln i,t+1 - ln it ) ,

(23.23)

where  >  when C and L are substitutes and  <  when they are complements. Instead of approximating ln it by (23.20), let us use a second order Taylor approximation to the random function t+1(1 + rt+1) / (1 + ) in the neighborhood of its
arguments' mean and take its expectation to obtain

ln it  ln

1

+ 1

Et +

rt+1 

Et

i,t+1

(23.24)

1 + 2 (1 + ) Et

rt+1 - Et (rt+1) i,t+1 - Et (i,t+1)

01

rt+1 - E (rt+1)

1 0 i,t+1 - E (i,t+1)



ln

Et

i,t+1

-



+

Et rt+1

+

Covt

(rt+1, i,t+1) (1 + )

.

Similarly, it can be shown that:7

Et

ln i,t+1



ln Et i,t+1 -

Vart (i,t+1) 2 (Et i,t+1)2

.

Replacing these expressions into (23.23) yields

 ln Nit   ( - rt+1) +  ln wit + 

rt+1

-

Et rt+1

-

1

1 +



Covt

(rt+1,

i,t+1)

(23.25)

+

ln

i,t

+1

-

Et

ln

i,t+1

-

Vart 2 (Et

(i,t+1) i,t+1)2

.

This is the extended  -constant labor supply relationship which depends on two new kinds of explanatory variables: (i) innovations in the marginal utility of wealth and
interest rate, and (ii) risk in the marginal utility of wealth and interest rate, reflected in the variance­covariance terms. An increase in Vart (i,t+1) has the same effect as reducing the marginal utility of wealth at period t + 1. From economic theory, we

7 For any positive random variable, say v, it can be seen that Et ln v  ln Et v - Vt v / 2 (Et v)2 .

23 Life Cycle Labor Supply and Panel Data

771

expect that  > 0, which means that a greater than expected interest rate increases current labor supply. Similarly, individuals or time periods with high risk (reflected by the variance terms) are characterized by a more decreasing labor supply profile than individuals/periods with low risk.8
In order to obtain an empirically tractable expression for labor supply dynamics, it is necessary to find an observable analogue for the last terms in (23.25). Hence, it is necessary to understand how the marginal utility of wealth evolves over the life cycle. Two strategies have been relied on for this purpose. Pistaferri (2003) translates the uncertainty and risk on marginal utility of wealth i,t+1 into uncertainty and risk on wages. His strategy relies on two assumptions; one about the expectation error (assumed to follow an MA(1) process), and one linking the marginal utility of wealth to wages, as in (23.14). Lich-Tyler (2002) relies on definition of the marginal utility of wealth to obtain an estimable expression for ln i,t+1 - Et ln i,t+1.
We follow Lich-Tyler's strategy to derive a simplified version of the model. In our time separable framework, let us define the period t indirect utility function:9

V (rt , wit , Ait ) = max U (C, N - N) : (1 + rt ) Ai,t-1 + wit N = C + Ait .
C,N

Then

it

=

V A

(rt , wit , Ait )

,

(23.26)

which can be used to obtain an expression for ln i,t+1 - Et ln i,t+1. Using a firstorder Taylor approximation to

i,t+1

=

V A

(rt+1, wi,t+1, Ai,t+1)

.

(23.27)

in the neighborhood of Et (rt+1, wi,t+1, Ai,t+1), omitting the arguments in the various functions, yields

i,t+1

V A

+

(rt+1

-

Et rt+1)

 2V  A r

+ (wi,t+1

-

Et wi,t+1)

 2V  A w

+ (Ai,t+1 - Et Ai,t+1)

 2V  A2

.

(23.28)

Hence

Vart (i,t+1) 2Et (i,t+1)2

ArVart (rt+1) + AwVart (wi,t+1) + AAVart (Ai,t+1) + covariance terms ,

(23.29)

8 On that account, it would be interesting to extend the model to allow for individual specific interest rates, and use information on household exposure to financial market risks for evaluating their labor supply behavior.
9 Strictly speaking, Ai,t-1 should appear as an argument in function V. But examination of (23.28) shows that the corresponding terms are equal to zero, hence the simplification.

772

B. Koebel et al.

with

A j



1 2

 2V / A j V / A

2
 0,

Equation (23.28) can also be used to calculate

j = r, w, A .

Covt (rt+1, i,t+1)

=

 2V  A r Vt (rt+1) +

 2V  A w Covt (rt+1, wi,t+1)

 2V +  A2 Covt (rt+1, Ai,t+1) .

For simplicity, we assume that Covt (rt+1, i,t+1) is constant in the sequel. Using a first-order Taylor approximation to ln  in the neighborhood of the real-
ization (r, wi, Ai)t+1 gives

Et

ln

i,t+1

=

Et

ln

V A

(rt+1,

wi,t+1,

Ai,t+1)

ln

V A

+

(Et rt+1

-

rt+1)



2V / A V / A

r

+ (Et wi,t+1 - wi,t+1)

 2V / A w V / A

+ (Et Ai,t+1 - Ai,t+1)

 2V / A2 V / A

.

Thus,

ln i,t+1 - Et ln i,t+1

-Ar (rt+1 - Et rt+1) - Aw (wi,t+1 - Et wi,t+1)

-AA (Ai,t+1 - Et Ai,t+1) .

(23.30)

where AA denotes the measure of absolute risk aversion in wealth (in terms of the indirect utility function), and

A

j



-



2V / A V / A

j

,

j = r, w, A ,

denote the change in marginal utility of wealth due to unanticipated changes in the explanatory variables. Notice that ln i,t+1 - Et ln i,t+1 is uncorrelated with rt+1, wi,t+1 and Ai,t+1 under the assumption of rational expectations, see Hansen and Singleton (1982). In this case, former models that have neglected risk, and summed up ln i,t+1 - Et ln i,t+1 with the residual term, mainly incur a loss in information and do not lead to an estimation bias.
Replacing (23.29) and (23.30) into (23.25) and adding a residual term uit yields

 ln Nit = 0 +  ( - rt+1) +  ln wit +r (rt+1 - Et rt+1) + rVart (rt+1) +w (wi,t+1 - Et wi,t+1) + wVart (wi,t+1) +A (Ai,t+1 - Et Ai,t+1) + AVart (Ai,t+1) + uit .

(23.31)

23 Life Cycle Labor Supply and Panel Data

773

The parameter 0 comprises the covariance terms between the different types of risk. The parameters r   (1 - Ar) ,  j  -A j, and  j  -A j for j = w, A, reflect risk aversion with respect to variable j. It can directly be seen that risk has a negative impact on  ln Nit . The impacts of unanticipated changes in w, A are asymmetric. In the case where the marginal utility of wealth is decreasing in wit , Aw > 0, and as  > 0, we have w < 0. Positive innovations in wages (i.e. wi,t+1 > Et wi,t+1) lead the individual to work less at t + 1, whereas negative innovations have the
opposite effect. In summary, unanticipated wage changes have the opposite impact
to anticipated wage changes.

23.3.3 Empirical Results
With this framework it now becomes possible to investigate empirically the impacts of anticipated and unanticipated wage change on labor supply, and how individuals react to an increase in the variability of the lifetime wage profile. For instance, they could adopt a precautionary labor supply behavior in order to try to compensate the risk of a wage profile.
Pistaferri (2003) uses panel data from the Bank of Italy (Survey of Household Income and Wealth), which comprises subjective information for each individual on her anticipated wage profile and price inflation (implying cross-sectional variability in the real interest rates). The difference between observed and anticipated wage gives the unanticipated wage profile. Pistaferri's empirical specification is a special case of (23.31):
 ln Nit   ( - Et rt+1) +  ln wit +  it + Vart-1 (it ) + uit ,
where
it  ln wit - Et-1 ln wit . Notice that in the neighborhood of zero, Vart-1 (it )  Vart-1 (wit ) / (Et-1wit )2 .
Lich-Tyler (2002) sums up the unanticipated changes into a residual term vit . Using the wealth identity (23.2), it can be seen that the wealth risk Vart-1 (Ait ) is driven by risk in the interest rate and risk in the future wage path. Assuming "that the wealth risk associated with a permanent wage change depends on the remaining work years of the individual and the amount of wage volatility" (Lich-Tyler, p.18), we write Vart-1 (Ait ) = A2i,t-1Vart-1 (rt )+wt Vart-1 (wit ) (65 - t) . Putting things together, (23.31) boils down to
 ln Nit = 0 +  ( - rt ) +  ln wit + Vart-1 (it ) +rVart-1 (rt ) + rAA2i,t-1Vart-1 (rt ) + wt Vart-1 (wit ) (65 - t) + vit .
Lich-Tyler relies on the PSID data for parameter estimation. In a first stage, the variance terms are estimated from the data, using various regressions.

774

Table 23.1 Labor supply estimates accounting for uncertainty and risk

 



r

rA

wt

Pistaferri (i) 0.59 0.70 -0.20 -0.11 ­

­

­

(0.29) (0.09) (0.09) (0.03)

Pistaferri (ii) 0.22 0.26 0.05 -0.05 ­

­

­

(0.18) (0.05) (0.06)

(0.01)

Lich-Tyler 0.01 0.29 ­
(0.04) (0.09)

-0.13 -12.9 -0.05 -0.012
(0.06) (4.6) (0.02) (0.005)

B. Koebel et al.

Some parameter estimates from both contributions are summarized in Table 23.1, estimated standard errors are given in parentheses.
The first line of Table 23.1, Pistaferri (i) shows the result of Pistaferri's basis estimates, whereas the second line, Pistaferri (ii), reports estimates of a model controlling for unemployment constraints. Further model estimations and robustness checks provide support for the first set of results. Pistaferri's estimate of the intertemporal elasticity of substitution is 0.70, which is somewhat higher than those usually reported. The last line of Table 23.1 gives the estimates obtained by Lich-Tyler. In this case, the elasticity of substitution of 0.29 is in line with those usually obtained from simpler models with the PSID data set. Pistaferri's estimate of the impact of wage innovation is significantly negative in his first model only. It implies that an unexpected 10% permanent upward shift in the wage profile decreases labor supply in all future periods by about 2.5%.
In all cases, the different types of risk have negative impact on the growth of labor supply. This finding is consistent with precautionary labor supply behavior. The estimates of  are quite similar in the Pistaferri and Lich-Tyler studies. Whereas Pistaferri finds his estimate of wage risk to have a very limited impact on working behavior, Lich-Tyler's conclusions are quite different. His simulations show that wage risk can explain wide differences in working hour profiles (see his Fig. 23.2, p.35).

23.4 Voluntary and Involuntary Non-participation
Depending on the economic context, individuals are not always willing to work, or able to find a job, or able to work their desired amount of time. Taking this distinction into account is important for avoiding estimation biases. For instance, if after an increase in wages, a person loses her job, this does not mean that her labor supply decreases in wages.

23 Life Cycle Labor Supply and Panel Data

775

23.4.1 Accounting for the Participation Decision

The prototype here is the paper by Heckman and MaCurdy (1980) which also presents the first estimation of a Tobit model on panel data.10 The specification does not differ much from that of MaCurdy (1981) but now the individual considered is a married woman. Accounting for the participation decision is important because selecting only working individuals leads to a selection bias.
Separability between the leisures of husband and wife is assumed, and the specification chosen for the utility function is

Uit (Cit , Lit ) = CitCit + Lit LitL ,

(23.32)

with 0 < L < 1, 0 <  < 1. Maximization of (23.1) subject to (23.2), taking the possible nonparticipation into account, yields

ln Lit

=

  

1 L -

1

(ln

it

ln L¯

- ln L

+ ln wit

- ln Lit )

if Lit  L¯ , otherwise.

(23.33)

The stochastic assumptions adopted are

ln Lit = Zit  + 1i + u1it , ln wit = Xit  + 2i + u2it ,

(23.34) (23.35)

Eu jit = 0, Eu jit ukis = ts jk, j, k = 1, 2, i = 1, ..., n, s, t = 1, ..., T .

where 1i and 2i are individual fixed effects capturing unobserved heterogeneity in the specifications of ln Lit and ln wit , and ts is the Kronecker symbol. The error terms u1it and u2it are assumed independent of all other variables in the RHS of (23.34) and (23.35). The unobserved heterogeneity of the preference parameter

Lit , which reflects individuals' implicit valuation of leisure, may well be correlated with the unobserved heterogeneity 2i driving the wage of individual i. In this case, wages are endogenous in (23.33). Substituting (23.34) and (23.35) into the labor

supply function helps to circumvent this problem.

Heckman and MaCurdy consider the reduced form:



ln Lit

=

 fi +  ln L¯

 -r L - 1t

- Zit

 L - 1

+ Xit

 L - 1

+ vit

if Lit  L¯ , otherwise.

(23.36)

where and

fi

=

1 L -

1

(ln

i0

- ln L

- 1i

+ 2i)

,

10 See also Heckman and MaCurdy (1982)

776

B. Koebel et al.

1 vit = L - 1 (-u1it + u2it ) .
Equations (23.35) and (23.36) are simultaneously estimated by ML, assuming normality for u1it and u2it .11 Identification of all parameters requires exclusion restrictions between X and Z. The fixed effects are fi in the hours equation and 2i in the wage equation. The estimation can only be performed for women who worked at least once in the observed periods. Correction for the corresponding selection bias is found to have only a minor impact. Since asymptotic arguments are not justified in the time dimension (only eight waves), estimates of the fixed effects are not consistent and this leads in principle to the inconsistency of all the coefficients.12 However, (i) Heckman (1981) performed Monte-Carlo simulations for fixed effects Probit with eight waves and found that the fixed effects Probit performed well when the explanatory variables were all strictly exogenous, (ii) Tobit should perform even better because it is a combination of Probit and linear regression. The fixed effects (incidental parameters) are estimated simultaneously with the parameters of interest through alternated iteration on both subsets of parameters.13 Yet their economic interpretation is difficult because the influence of f is mixed with that of the time invariant variables in Zt and the same holds for 2 and the time invariant variables in Xt . Regressions of the fixed effects on those time invariant variables completes the picture and allows one to reach conclusions like the following: current-period household income (exclusive of the wife's earnings) has no significant impact on labor supply, in contrast to an 8 year average income (proxy for the permanent income).
Another study taking the participation decision into account is Jakubson (1988). The specification is the same as above but separate identification of  and  is left aside and Jakubson specifies Xt  Zt . The model is thus considerably simplified and takes the Tobit form

ln Lit =

 -r

 -

fi +  - 1t + Xit  - 1 + vit if

Lit  L¯ ,

ln L¯

otherwise.

(23.37)

Jakubson presents three approaches to the estimation of (23.37): simple pooling, treatment of fi as a random effect taking into account the correlation with X (using Chamberlain's, 1984 approach) and, as before, treatment of fi as a fixed effect. For the fixed effects, the considerations above still hold, while convergence for the random effects specification is ensured even for short panels as long as their stochastic specification is correct.
The main conclusions are: (i) the panel estimates (fixed or random effects) of the influence of children on labor supply are only about 60% of the cross section

11 We do not mean to suggest that there are no alternatives to ML with joint normality in this context, and the interested reader is referred to Wooldridge (1995) and Dustmann and Rochina-Barrachina (2000) for some of these. 12 That is, for N  . 13 A computationally more efficient alternative is discussed by Greene (2004).

23 Life Cycle Labor Supply and Panel Data

777

estimates, due to the neglect of individual effects in the latter; (ii) as concerns the life cycle hypothesis, like in the Heckman and MaCurdy study, current income does not have a significant influence in the fixed effects estimation, yet this does not hold true for random effects.
Disregarding the inconsistency problem associated with fixed effects here, and considering that sampling may be endogenous (one of the selection criteria being "stable marriage," see Lundberg, 1988) the fixed effects approach might seem preferable on a priori grounds. However, as we shall see in the following section, the entire specification is questionable.
Accounting for taxes is feasible in the framework discussed here, as documented by Laisney, Lechner, VanSoest and Wagenhals (1993). This study keeps the assumptions of explicit additivity of the intertemporal utility function and of intertemporal separability of the budget constraint. The specification postulates parallel within-period preferences, i.e.

Uit (Cit , Lit ) = Git [Cit + Vit (Lit )] ,

(23.38)

where G is an increasing function. This specification yields a useful benchmark,
because the corresponding labor supply equation is independent of the marginal utility of wealth, i0 (and thus coincides with the Marshallian and the Hicksian labor supply equations). This clearly solves several of the econometric problems discussed above. Choosing a Box-Cox specification Vit (Lit ) = it (LitL - 1)/L and keeping specifications (23.34) and (23.35) for the taste shifter it and the gross wage wit yields the labor supply equation

ln Lit

=

1 L - 1

(ln wit

+ ln[1 - t (wit

Nit )] - ln it ) + vit

,

(23.39)

where t denotes the marginal tax rate, assumed here to vary only with earnings. This equation is very similar to (23.33), the specification of Heckman and MaCurdy (1980) apart from the fact that it does not include it and i. However, as will be discussed in the next section, the Heckman­MaCurdy specification requires the restriction that Git is the identity, so that, although the two labor supply equations are nested, the overall specifications are not. In the same spirit, it can be seen that the labor supply (23.61) and (23.33) of the Browning, Deaton and Irish (1985) and Heckman and MaCurdy (1980) specifications can be nested in the more general model

(Lit ) 

-1

=

-it

-  ln w(Nit ) - 1

1 w(Nit )

-

 ln it

+

vit

,

(23.40)

where w(Nit ) denotes the real net (marginal) wage rate associated with Nit . The Browning et al. specification corresponds to the linear form  = 1, whereas the Heckman­MaCurdy specification corresponds to the logarithmic specification obtained for the limiting case  = 0, with 1 = 0.

778

B. Koebel et al.

The model is estimated, taking the participation decision into account, using an unbalanced panel of married women drawn from the German Socio Economic Panel 1985­1989, using Mundlak's (1978) approach to modelling random effects for  and Chamberlain's (1984) minimum distance estimator, whereby the first stage of the estimation procedure consists of (pseudo-) maximum likelihood simultaneous estimation of (23.34), (23.35) and (23.40). Following MaCurdy et al. (1990), the marginal tax rate is approximated by a smooth increasing function. A further distinctive feature of this study is that desired hours of work are used as the dependent variable, instead of effective hours of work. This weakens to some extent the critique of Tobit-type models of labor supply made by Mroz (1987).

23.4.2 Unemployment

Certainly one of the most questionable assumptions made so far is the assump-

tion that unemployment is voluntary. Ham (1986) produces empirical evidence

against this hypothesis in the context of life cycle models (see also Ashenfelter and

Ham, 1979). Ham uses the following modification of MaCurdy's model. If an addi-

tional restriction consisting of a ceiling to the number of hours worked exists, and

if Tu is the set of indices of the periods where this restriction holds for individual i,

we have

ln Nit < Fi + bt - Rt +  ln wit + uit for t  Tu ,

(23.41)

ln Nit = Fi + bt - Rt +  ln wit + uit for t  Tu ,

(23.42)

where Fi corresponds to a higher value of  than when Tu = : the profile of expected wages at each period is lower than in the absence of unemployment periods. Therefore, (23.13) will yield large residuals for t  Tu if unemployment is not the outcome of a free choice. The idea is then to estimate either

ln Nit = Fi + bt - Rt +  ln wit + 1Uit + uit

(23.43)

or

ln Nit = Fi + bt - Rt +  ln wit + 2Hiut + uit ,

(23.44)

where Uit = 1 if t  Tu and 0 otherwise, and Hiut denotes yearly hours of unemployment. If the free choice assumption is correct, then 1 (or 2) will not significantly differ from zero. Otherwise one would expect negative values.

The free choice assumption is clearly rejected for both specifications (23.43) and

(23.44), as well as for other specifications allowing for uncertainty, nonlinearity (with the additional term (ln wit )2), nonseparability, see (23.61), as well as for var-
ious assumptions on the covariance structure of the residuals. The results of these

tests suggest modelling these restrictions explicitly. Lilja (1986) makes several pro-

posals in this direction.
However, MaCurdy et al. (1990) criticizes Ham's argument and shows that 1 (or 2) significant in (23.43) or (23.44) is compatible with voluntary unemployment

23 Life Cycle Labor Supply and Panel Data

779

caused by a lower wage offer wit for t  Tu: "The reasoning underlying the testing of exclusion restrictions in labour supply functions relies on the argument that wages fully capture the influences of demand-side factors in a supply decision. This reasoning is sound but the variable identified as relevant by intertemporal substitution theory is the offer wage; and the offer wage deviates from the observed market wage if unemployment occurs at all" (MaCurdy 1990, p. 228; see also Card, 1987, who interprets Ham's findings in favor of demand-side conditions as the main determinant of observed hours).

23.5 Alternative Parameterization and Implications

Browning (1986) and Blundell, Fry and Meghir (1990) point out that the specifica-

tion of  -constant systems, where  , or ln  , appear additively and can be treated

as an individual-specific effect turns out to be extremely restrictive in the models of

MaCurdy (1981) and Browning et al. (1985). In this case, the labor supply functions

share the form

gi(Nit ) = fi(wit ;  ) +  ln it

(23.45)

where gi and fi are some functions, and  and  are parameters. After replacing ln it by (23.8), first differentiation for individual i allows us to get rid of individual heterogeneity. The devastating consequence is that such intertemporal preferences are completely identified (up to a monotonic transformation) on a single cross section, given that some variation in the wages or prices can be observed. Thus, this type of specification hardly qualifies for exploiting panel data.
An alternative strategy consists in estimating the within-period preferences by eliminating  , either directly between two goods or indirectly via the period budget equation, and then estimating the time preference rate  separately. The advantage is that no restriction on within-period preferences is required. Panel data are not absolutely necessary for this strategy: a time series of independent cross sections proves to be sufficient and even has some advantages in providing valid instrumental variables more easily, see Blundell, Fry and Meghir (1990). Blundell, Browning and Meghir (1994) give a good example of the application of this strategy to demands for goods. Four important panel studies on labor supply use this alternative strategy.
MaCurdy (1983) proposes to directly estimate the marginal rate of substitution functions. The first-order conditions (23.3) and (23.4) give

Uit / Nit Uit /Cit

= -wit

.

(23.46)

The advantage over estimating Marshallian demands is that this allows estimation of preferences that do not imply a closed-form expression for the demand functions. The estimation of (23.46) does not require a panel. A cross section with enough price variation, or indeed a time series of cross sections, can be sufficient.

780

B. Koebel et al.

In spite of this, MaCurdy chooses the restrictive form

Uit

=

Git

(Uit )

=

it

(Uit

+ ) 

-1

,

with

Uit

=

it

(Cit

+ C)C C

-

(Nit

+ N )N N

,

and

(23.47) (23.48)

it = exp[Xit  + it ] ,

(23.49)

it = exp[Xit  + it ] .

(23.50)

The parameters  , ,  , , C, N, C, and N are constant across individuals and over time. This utility function is still additive, yet no longer explicitly additive, and this form of Ut allows for several well-known special cases such as CES, addilog and Stone-Geary. The Frisch labor supply function corresponding to (23.47) is usu-
ally different from (23.45). There is no identification problem here since (23.49)
and (23.50) are estimated in two different dimensions: (23.50) is estimated in the
"individual" dimension and (23.49) in the "time" dimension. Equations (23.46) and
(23.48) yield

ln wit = -Xit  + (N - 1) ln(Nit + N) - (C - 1) ln(Cit + C) - it , (23.51)

which provides consistent estimates (on a single cross section if desired) for , N, C, N and C. Using those one can obtain it by substitution of Xit  + it from (23.51) into (23.50). Estimates for the parameters  and  can be obtained as fol-
lows. Substitution of (23.3) into (23.6) gives

ln

 Uit  Cit

= rt+1 - i + ln

 Ui,t +1  Ci,t +1

+ ei,t+1 .

(23.52)

The above specification leads to

ln

 Uit  Cit

-

ln

 Ui,t +1  Ci,t +1

=

rt+1 - i - (Xi,t+1 - Xit )

+(1 -  ) ln(Uit+1 + ) - ln(Uit + ) + ei,t+1 .

(23.53)

Since estimates for Uit and Uit /Cit are available from the parameter estimation of (23.51), specification (23.53) can be seen as a regression from which the still
unknown parameters  and  of the monotonic transformation Git can now be identified. Either time series or panel data contain all the information needed to
estimate (23.53). Instrumental variables are necessary to take account of the endogeneity of Uit and Ui,t+1, and Pagan's (1984) method of correcting the variance of

23 Life Cycle Labor Supply and Panel Data

781

the estimators would be advisable here, because estimated parameters are used in the construction of regressors as well as regressands in (23.53). Taking account of measurement errors in hours, wages or consumption would be difficult because such errors would contaminate it, see (23.51), and would therefore produce nonlinear errors in the variables in (23.53).
The study of Blundell et al. (1993) of intertemporal labor supply of married women starts from the following Marshallian supply specification for within-period desired hours of work

Nit =  (wit , Zit ) -  (wit , Zit ) [Sit + a(wit , Zit )] + uit ,

(23.54)

where wit is the real marginal after tax wage rate, Sit is a measure for unearned income and Zit is a vector of characteristics. This supply function can be derived by Roy's identity from the indirect utility function

1 V (wit , Sit , Zit ) = 1 +  (Zit )

Sit + a(wit , Zit )

1+(Zit )
-1

,

b(wit , Zit )

(23.55)

with (wit , Zit ) =  a/ wit and  (wit , Zit ) = ( b/ wit ) /b. The parameters of  (Zit ) which do not show up in the labor supply function are estimated in a second stage, using GMM and relying on a procedure analogue to (23.53). Although the study discusses several different elasticities, we shall only comment on  -constant elasticities of labor supply with respect to the net wage, computed at the means of various subsamples of employed women. These range between 0.57 for childless women with unemployed blue-collar husbands and 1.39 for women whose youngest child is at most two and whose husbands are employed white-collars, a subsample with typically low labor supply.
More on taxes: relaxing the intertemporal separability of the budget constraint. As pointed out by Blomquist (1985), capital taxation will usually break the intertemporal separability of the intertemporal budget constraint. When the constraints (23.2) are replaced by

Ait = (1 + rt ) Ai,t-1 + wit Nit - Cit - T (wit Nit + rt Ai,t-1) , t = 1, . . . , T ,

where the function T denotes the tax scheme. In this case, it is in general not possible to write the Frisch labor supply in function of an additive and constant  -term which can be easily differentiated out. Ziliak and Kniesner (1999) consider instead a Marshallian labor supply function of the form

Nit = wit + Ai,t-1 +  Ait + Zit  + i + it ,

(23.56)

where i denotes an individual effect. The wealth measure used for Ait is a construct analogue to the familiar virtual income used in static models of labor supply with taxes. Notice that both current assets and assets in the previous period condition this labor supply function, which is thereby different from the pseudo supply function

782

B. Koebel et al.

Nit (Ait - (1 + rt ) Ai,t-1, wit ) of Sect. 23.2. Parameters reflecting the intertemporal utility function are then recovered in a second stage which is very similar to (23.52­23.53).
In contrast with Blundell et al., Ziliak and Kniesner use (balanced) panel data (PSID for 1978­1987), 532 continuously married, continuously working men aged 22­51 in 1978), in both stages of the estimation procedure. In the first stage, this has the advantage of allowing a better control of unobserved heterogeneity. Ziliak and Kniesner estimate (23.56) in first differences by optimal GMM assuming absence of autocorrelation in the process it , using internal instruments dated t - 2 and other instruments dated t - 1 and t - 2. Estimation in the second stage is conducted on the same panel, with internal instruments dated t - 4 and other instruments dated t - 3 and t - 4. A consistent estimator of the variance of the second stage estimator, taking into account the variability of the estimated quantities, is obtained following Newey (1984). Our impression is that potentially important efficiency gains might be obtained quite easily by (a) moving from a balanced to an unbalanced panel, which would substantially increase the number of observations, and (b) extending the instrument set by taking instruments dated up to the named dates rather than only instruments at the above dates.
Results from Step 1 show that the model conditioning on assets at two subsequent dates outperforms a model conditioning on savings. Two series of estimates are presented for Step 2, depending on whether the subjective discount rate, assumed constant over time, is allowed to vary over individuals or not. The former specification is the preferred one. Even though the  -constant specification was not used for parameter estimation, the  -constant elasticities of labor supply with respect to the gross wage are easily computed from (23.56). Mean  -constant elasticities by wealth quartile vary between 0.14 for the lowest quartile and 0.20 for the highest. Recall that this represents the response to an expected wage change. By contrast the authors reckon that the average elasticity of labor supply with respect to an unexpected wage change will be roughly constant across wealth quartiles, at about 0.16. Ziliak and Kniesner also compute deadweight loss measures associated to four tax reforms, but reporting on these would take us too far off our track.
Errors in variables are thoroughly treated by Altonji (1986), using instrumental variables methods. Unfortunately, in order to obtain the required linearity, Altonji uses a version of MaCurdy's (1981) restrictive form, i.e. an explicitly additive within-period utility function

Uit

=

Cit C

Cit C

-

Nit N

Nit N

,

(23.57)

where Cit and Nit are time-varying taste modifiers. The  -constant demands are

ln Nit = cst + N[ln wit + ln it + t ln(1 + ) - ln Nit ] ,

(23.58)

lnCit = cst + C[ln it + t ln(1 + ) - ln Cit ] .

(23.59)

23 Life Cycle Labor Supply and Panel Data

783

Rather than estimating (23.58) in first differences,14 Altonji proposes substituting ln it +t ln(1 + ) out of (23.58) and (23.59). He then assumes that the observations contain the measurement errors vNit , vC it , and eit , and consist in nit = ln Nit + vN it , cit = lnCit + vC it and wit = ln wit + eit . Since wit is not directly observed but is calculated by dividing period income by Nit , vN it is correlated with eit but neither of the two will be correlated with vC it . Thus, we obtain the model:

nit

=

cst + N wit

+

N C

cit

+ N

ln

Cit Nit

+ vN it

- N eit

-

N C

vC it

.

(23.60)

The advantage over first differences is that the substitution using cit does not bring lagged wages into the equation. Even more important perhaps, the assumption
about expectations that was used above to motivate estimating first differences under uncertainty is now unnecessary. Instruments are used for wit and cit . The results do not differ much from MaCurdy's. See also Imai and Keane (2004) for a different
treatment of the problem of errors in variables.

23.6 Relaxing Separability Assumptions
We now discuss studies relaxing within-period and between-period additive separability.

23.6.1 Relaxing Within-Period Additive Separability

When the within period utility function is additively separable, the Frisch demand functions satisfy the restrictions

N  it

it N

=

N  wit

wit N

and

C  wit = 0 ,

see (23.12). These restrictions are not simply a consequence of the functional form

adopted in (23.12), indeed they characterize within-period additive separability. The

importance of relaxing the assumption of separability between leisure and goods

is indicated in Browning and Meghir (1991) who reject this assumption, testing

it within a very general scheme using 1979­1984 FES data (time series of cross

sections): preferences about goods are specified in a flexible way, with conditional

cost functions where no behavioral assumption concerning labor supply or participation decision is needed.15 Here we shall be concerned only with relaxing the

assumption of additive separability between the two "goods" leisure and aggregate

consumption.

14 Yet this is done for comparison. 15 Yet their model is not cast in the life cycle framework and the implications of their study for life cycle models should be elucidated.

784

B. Koebel et al.

Browning et al. (1985) estimate the following specification in first differences:

Nit = 1(ait ) +  ln wit + 1

1 wit

+  ln it

,

Cit

=

2

(a

it

)

-

2

 wit

+

ln it

,

(23.61) (23.62)

where ait is a vector of household characteristics. Symmetry of the demand functions implies that 1 = 2 =  and within-period additive separability is equivalent to  = 0. Browning et al. (1985) estimate the equations separately, i.e. they do not enforce the identity 1 = 2, as would be feasible in this context since there is no adding-up restriction (in contrast with a Marshallian demand system). However, they find 1 and 2 to be significantly different from zero and to have opposite signs, which makes the entire specification appear questionable. Note that, although Browning et al. consider aggregate consumption, no problem arises from working with several consumption goods. Yet, durables should be given special attention, as they might be more properly treated as assets.
So far we have focused on the preferences of an individual. In practice, however, economists often work with "household preferences". One of the many reasons for doing this is the difficulty of isolating individual from household consumption in survey data. Another assumption, which is necessary for the validity of the specifications that we have considered so far is the separability of the labor supplies of the different potential earners in a household. If it holds, the earnings of the other household members can be accounted for in Ait , because then the influence of hours and wages of other household members boils down to a pure income effect. Otherwise the model is misspecified.

23.6.2 Relaxing Intertemporal Separability in Preferences
Although relaxing this assumption is no easy task, it is important because all the studies that test the assumption clearly reject it. If the estimation results are to be used in policy analysis, the specification must produce interpretable parameters and not merely a separability test. In this respect, it seems difficult to simultaneously model the multiple reasons that lead to the rejection of separability. Most empirical studies therefore concentrate on only one of these aspects. The modelling of partial adjustment, rational habit formation and human capital accumulation in an optimization scheme over the life cycle is such a feasible extension.
Yet, before turning to structural models relaxing the intertemporal separability assumption, it is interesting to discuss the results of a VAR approach to modelling the relationship between wages and hours of work using panel data. As a prototype for this kind of approach we will focus on the study by Holtz-Eakin, Newey and Rosen (1988), but also refer the reader to Abowd and Card (1989).
Holtz-Eakin et al. analyze a sample of 898 males from the Panel Study of Income Dynamics (PSID) over 16 years. They estimate linear equations for wages and

23 Life Cycle Labor Supply and Panel Data

785

hours, with lags of equal lengths on both wages and hours on the right hand side of each equation, and individual effects. Note that the equation on hours does not nest the simple life-cycle model of MaCurdy (1981) since the contemporaneous wage is excluded and no serial correlation is allowed. By contrast, the form of the wage equation could be justified by human capital considerations. However, attempts at interpreting these reduced form equations are not in line with the VAR approach. The model of Holtz-Eakin et al. does not a priori impose the stationarity of the coefficients over time, not even for the individual effect. The estimation strategy relies on GMM, combined with quasi-differencing along the lines of Chamberlain (1984, p. 1263) in order to eliminate the individual effect while allowing for nonstationarity. Errors in variables are easily dealt with in this linear GMM framework, but again under the restrictive assumption of no serial correlation. Starting with a maximum lag length of three periods (involving four lags of the original variables in the quasidifferenced equations) parameter stability is rejected for none of the two equations, and the analysis proceeds more simply with first differences. The next step concerns testing the lag-length, and the assumption that one lag is sufficient to describe the data is rejected in no equation at the 1% level, but rejected in the hours equation at the 5% level.
Furthermore, one cannot reject the assumption that lagged hours could be excluded from the wage equation. The same holds for lagged wages in the hours equation, when using only one lag, but not if two lags are retained (an argument in favor of nesting the noncausality test within the hypothesis about the lag length is that in this way the test statistics turn out to be asymptotically independent, which facilitates pin-pointing the reasons for rejection of the joint hypothesis). Tests for measurement error bias are constructed using internal instruments in the simple first-order autoregressive models, in order to increase the power of the test. The assumption of absence of measurement error cannot be rejected at the 5% level, but there is evidence that the test may have low power in this instance. Most results are qualitatively, and, what is more surprising, quantitatively replicated on a sample from the National Longitudinal Survey (NLS). The authors conclude (p. 1393): "Our empirical results are consistent with the absence of lagged hours in the wage forecasting equation, and thus with the absence of certain human capital or dynamic incentive effects. Our results also show that lagged hours are important in the hours equation, which is consistent with the alternatives to the simple labour supply model that allow for costly hours adjustment or preferences that are not time separable [our emphasis]. As usual, of course, these results might be due to serial correlation in the error term or functional form misspecification".

23.6.2.1 Rational Habit Formation
Bover (1991) estimates a rational habits model in a certainty framework with a minimum amount of replanning. The salient feature of her approach is that the model specification is constructed in such a way that it allows for an explicit expression of the marginal utility of wealth  , as a function of future wages, initial wealth,

786

B. Koebel et al.

the (constant) interest rate, and preference parameters. The advantage of such an expression is that it allows a direct analysis of wealth effects on intertemporal labor supply (see Card, 1994, for the potential importance of such effects), whereas the approach of MaCurdy (1981) allows such an analysis only in a very indirect and unsatisfactory way. However, this comes at a large cost, as we shall see. In period t the individual maximizes

T
t=1

(1

1 + )t-1

[(1

-

it ) ln(Cit

-

c)

+

it

ln(N

+

 Ni,t-1

- Nit )]

(23.63)

subject to (23.2). The parameter  now measures the habit persistence. The Stone-Geary specification (23.63) was also used by Ashenfelter and Ham 1979 in order to derive an explicit expression for it under perfect foresight. The novel feature here lies in the relaxation of the intertemporal separability assumption through
the rational habit formation assumption. (In a previous paper Bover, 1986, consid-
ered two alternative models, one with partial adjustment and one with myopic habit
formation, which did not take account of all direct and indirect influences of current
labor supply on future decisions, as the rational habit formation model does, but she
found all these models to be empirically indistinguishable.) Defining Nit = Nit -  Ni,t-1 and wit = Tj=-0t (1 + r)- j jwi,t+ j allows one to
rewrite (23.63) and (23.2) in the usual form of a separable intertemporal utility function with arguments {Nit , Cit }t=1,...,T and an additively separable intertemporal budget constraint. The corresponding Frisch demands are linear in it and the expression of the latter is obtained by substituting these into the budget constraint. The reason for the subscript t in it is the replanning that takes place at each period, when the individual forms new predictions about his wage profile. The somewhat
arbitrary assumption here is that each individual's future wages lie on a specific lin-
ear time trend, and that the individual learns more about the two coefficients of this
relationship as more time passes by. This is disturbing, because if the relationship
were deterministic, two observations would suffice to pin it down without any error, and if not we have uncertainty about future wages, whereas the derivation of it assumed that wit is known.
This specification yields a nonlinear model where the dynamics are only present
in the error term. The model can be exactly linearized through transformations of
the exogenous variables on the one hand, and the parameters on the other. The er-
ror specification is of the error components type with the unobserved heterogeneity
subsumed in a time-invariant individual effect. Bover estimates the dummy vari-
able model with unrestricted covariance for the residual error term, including also
time dummies and using instruments to cope with potential endogeneity and mea-
surement error problems concerning the wage variable. The instruments used have
the property that they are strictly exogenous conditional on the individual effect. A 2 test of the overidentifying restrictions leads to no clear-cut rejection of the specification. The results show that lagged hours have a significant effect on the
current decision.

23 Life Cycle Labor Supply and Panel Data

787

While Bover substitutes the marginal utility of wealth in the Euler equation with a very special assumption about the wage path, Hotz, Kydland and Sedlacek (1988) (HKS) consider the stochastic Euler equations, characterizing the first-order conditions of the dynamic optimization problem. This strategy allows to consider more general specifications for the utility functions. In period t the individual maximizes

 Et

T t=1

(1

1 +

)t

-1

Uit

(Cit

,

Lit

+



ait

)

,

(23.64)

with

ait = (1 - )ai,t-1 + Li,t-1 ,

subject to (23.2). As before Lit denotes leisure. This specification nests intertemporal separability ( = 0) and the models of Johnson and Pencavel (1984) and Bover (1986, 1991), where only the labor supply of the previous period does play a role in the preferences of the current period ( = 1).
In order to avoid misspecification, stemming from potential endogeneity of
wages, HKS only use the Euler equation for consumption. They specify Uit to be translog and separately estimate the parameters for two age groups. Since parameters  and  are identified under the maintained assumption of no contemporaneous additive separability between Lit + ait and Cit , this allows testing the form of the intertemporal nonseparability in preferences. Moreover, a score test of the wage exo-
geneity is offered. HKS also explain how to cope with a certain degree of correlation
between individuals through macroeconomic shocks or regional variables. Using a
sample of 482 men from the PSID, they reach the following conclusions. The estimated parameters  and (1 - ) are positive and well determined and therefore intertemporal separability is rejected, and not only Li,t-1 but also leisure decisions in previous years have a direct influence on current decisions. The (within period) separability between Lit + ait and Cit in the translog utility function is also rejected, as is exogeneity of the wages. A slightly disturbing result is the negativity of
the estimated rate of time preference.
The theoretical setting (Euler equation) implies orthogonality between the residual at time t and all the information available up to t - 1. Thus, in GMM estimation, all variables dated t - 1 or earlier qualify in principle as instruments for the equation dated t. This implication of theory can be tested by a 2­test of overidentifying restrictions using two sets of instruments, where one is restricted to strictly exogenous
instruments. HKS conduct such a test and do not reject the null of orthogonality.

23.6.2.2 Human Capital Formation
Hotz et al. and Bover assume that the wage path is not influenced by the hours decision, thus assuming intertemporal separability in the budget constraint. By contrast, Shaw (1989) and Imai and Keane (2004) relax that assumption, i.e. they allow for nonseparability in the budget constraint (but not in the preferences). For Shaw,  = 0 in (23.64). The budget restriction is as before given by (23.2). However, Shaw

788

B. Koebel et al.

defines the real wage wit as the product Rit Kit of the human capital stock Kit and its rental rate Rt and chooses a quadratic approximation f for the relationship between Ki,t+1 on the one side and Kit and Nit on the other side, which yields the atypical earnings function

wi,t+1 = f Ri,t+1

Nit

,

wit Rit

.

(23.65)

Although Shaw considers a timewise separable direct utility function, this last equation, which makes future wages an increasing function of the current wage, renders the indirect utility function nonseparable. The first-order optimality condition with respect to leisure now reads:

 Uit  Lit

= it wit

1

+

Rit

 f -1  Nit

Nit wit

,

t = 1, . . . , T ,

(23.66)

which differs from the timewise separable optimality condition (23.4). When working today increases future wages, this leads individuals to work more as predicted by timewise separable models.16 Imai and Keane (2004) provide a further contribution along these lines.
Shaw specifies Uit to be translog (as in HKS). Preference parameters are estimated by GMM using the orthogonality conditions in the stochastic Euler equations. This contrasts with Imai and Keane (2004) who solve the stochastic dynamic programming problem backwards.
Shaw's conclusions are as follows. The rental rate of human capital varies considerably over time and the number of hours worked has a strong influence on future wages. This result offers a possible explanation for the misspecification of the usual static earnings function. Because of the model structure and especially the fact that the nonlinearity is within the budget constraint, the overall implications of the model can only be evaluated by simulation. This reveals that the intertemporal elasticity of labor supply is not constant as is usually assumed in static models, but instead rises over the life cycle. Her model is estimated over a samples of 526 men from the PSID. Due to the high degree of nonlinearity in the Euler equations, Shaw does not handles measurement errors or unobserved heterogeneity. In particular, the presence of unobserved heterogeneity is problematic as it can bias the conclusions about state dependence in dynamic models (see Chamberlain, 1984).
A reason why the models of Shaw and HKS have been estimated with male rather than with female labor supply may be that the estimation method used does not readily extend to discrete data. Altug and Miller (1991) propose a solution to that problem. We shall not go into the details of their paper, but it seems worth mentioning that this is a very sophisticated and innovative study, which also

16 Notice that for the alternative specification allowing the earning function f to depend on the cumulative hours of work (and concave in this variable), there are especially young individuals who have incentives to work more whereas older individuals for which human capital investment become less attractive ­ given the fact that retirement is nearer than for young individuals ­ to work less.

23 Life Cycle Labor Supply and Panel Data

789

considerably improves upon the treatment of aggregate shocks adopted by the two studies just mentioned. The main drawback is that estimation of the model of Altug and Miller is intricate, combining GMM with simulation of participation probabilities and iterative estimation of Euler equations, including nonparametric regressions at each iteration. In short, it required the use of a supercomputer. Another drawback, a theoretical one, is that the model heavily relies on the assumption that actual hours of work differ from expected or contracted hours of work in a stochastic manner. While this may be attractive for some occupations (think of academics), it is much less convincing for most others. To our knowledge, this is the only study of female labor supply allowing for nonseparability both in the preferences and in the budget constraint.
The study of Eckstein and Wolpin (1989), which is based on explicit solution of the dynamic optimization problem facing individuals rather than on the exploitation of first-order conditions, shares this generality but restricts attention to the participation decision and disregards aggregate shocks. Hence it does not exactly fit the framework of this survey.17 It has however inspired the work of Imai and Keane (2004), to which we now turn. The main goal of their study is to reconcile the microeconometric evidence on the ies with the higher values adopted by macroeconomists in the calibration of real business cycle models. Their framework assumes both intertemporal and within period additive separability of preferences, but it allows for on-the-job human capital accumulation to affect the wage path, which breaks intertemporal separability in the budget constraint, as in Shaw (1989). It also allows for measurement errors in wages, labor supply, and assets in a maximum likelihood framework with fully parametric distributional assumptions. Missing data on assets are also handled through both distributional assumptions and the intertemporal budget constraint. The functional forms adopted for the subutility functions from consumption and leisure are the same as in MaCurdy (1981), except for the presence of age effects in the former. The intertemporal budget constraint is again (23.2) where t denotes the age of the individual, and the real wage rate wt, assimilated with the human capital stock, evolves according to

wt+1 = g (Nt , wt ,t) t+1 ,
where t+1 is a wage shock and g is a deterministic function of hours worked and human capital at age t, and age itself.
Imai and Keane argue that neglecting human capital accumulation biases ies estimates towards zero. On the one hand, as the wage increases over the life-cycle, the substitution effect leads to an increase in labor supply. On the other hand, concavity of the value function in human capital lowers the rate of return to human capital investment and reduces the incentive to supply labor. The combination of the two effects leads to a fairly flat hours-wage profile, and attributing this to the substitution effect only leads to an underestimation of the ies.
Indeed, estimating their model on a fairly homogeneous sample of 1000 randomly chosen white males from the 1979 cohort of NLSY observed at ages 20­36

17 A summary of that study is in the 1996 version of this survey.

790

B. Koebel et al.

and continuously reporting positive yearly hours of work, Imai and Keane obtain an estimate of the ies of 3.82 with a very small standard error.18 They then simulate data from their model over the life-cycle up to age 65 and estimate the ies on various subsets using the OLS and IV methods of MaCurdy and Altonji. The results show that the estimated ies is much lower in these estimates than the true ies in the simulated data, and that estimates are particularly low for individuals in the 20­36 age group, underscoring the fact that the human capital component of the return to labor supply is much greater for the young. Indeed, IV results obtained from the original data yield an ies below 0.3, more than ten times smaller than the ML estimate.
Imai and Keane (2004) estimate the marginal rate of substitution between consumption and labor supply, which corresponds to wit [1+Rit  f -1/ Nit (Nit /wit )] in (23.66). Their results range from about 2wit for 20 years old individuals to wit for 60 years old. This means that the effective wage is higher than market wages wit due to high return in human capital, which induces young people to work more (at given wage) than predicted by (23.4).

23.7 Conclusion
Taking stock, we can draw the following conclusions. Firstly, in our opinion, there has so far still being too little emphasis on the relaxation of ad hoc functional form assumptions. In a way, this is understandable, because researchers have been busy introducing and manipulating new and sometimes complex econometric methods. Yet it is disturbing to see how popular the additively separable Box-Cox type specification has remained over the 25 past years, even in studies allowing much more flexible approaches. The greater flexibility of the alternative to Frisch demands, consisting in separate estimation of within-period and intertemporal preference parameters, has not yet been used fully in life-cycle labor supply studies. Secondly, given the small sample sizes and the more or less pronounced arbitrariness of the selection, most of the studies we have discussed definitely have a methodological rather than a substantive character.
Before closing this chapter we would still like to point out a series of papers which do not completely fit under its heading but contribute to the understanding of labor supply reactions. Blundell, Duncan and Meghir (1998) use past fiscal reforms in order to estimate labor supply responses. For estimation they rely on a series of cross-sections but their innovative approach can easily be adapted to panel data. A growing body of literature relies on daily information on wages and working time for particular worker groups to investigate the sensitivity of working time to wages: cabdrivers have been considered by Camerer, Babcok, Loewenstein and Thaler (1997) and by Farber (2005), stadium vendors by Oettinger (1999), bicycle messengers by Fehr and Go¨tte (2007). This type of data exhibits two important
18 The estimation method is too complex to be described in any detail in a survey. It entails several clever approximations aiming at reducing the number of evaluation points and the dimension of the optimisation space.

23 Life Cycle Labor Supply and Panel Data

791

advantages over usual panel data: these workers choose daily the number of working hours they want to work, and daily variations of their hourly wage can reasonably be considered as transitory changes. Their results tend to suggest a negative relationship between wages and working hours. Finally, there is a burgeoning literature on the estimation of collective models of household labor supply, i.e. models where the existence of autonomous decision makers within the household is explicitly acknowledged, and the central assumption is that household allocations are Pareto efficient. For instance, Blundell, Chiappori, Magnac and Meghir (2005) estimate a static model on repeated cross-sections, focusing on the participation/nonparticipation decision of the husband while allowing free choice of hours for the wife (including nonparticipation). The longitudinal information contained in panel data allows the study of intertemporal household allocations in the collective framework, as exemplified by Mazzocco (2007).

Acknowledgments This is an update on Chap. 28 of the 1996 edition of this book. Besides incorporating new references, we also modified the focus. The reader is referred to the former version for a summary of quantitative results, as well as for data issues. We would like to thank Richard Blundell, Tom Kniesner, Gauthier Lanot, Michael Lechner, Friedhelm Pfeiffer, Jo¨rn-Steffen Pischke, Jean-Marc Robin and Gerhard Wagenhals for comments.

References
Abowd, J. and D. Card (1989): "On the Covariance Structure of Earnings and Hours Changes," Econometrica, 57(2), 411­445.
Altonji, J.G. (1986): "Intertemporal Substitution in Labour Supply: Evidence from Micro-Data," Journal of Political Economy, 94 (3.2), S176­S215.
Altug, S. and R.A. Miller (1990): "Household Choices in Equilibrium," Econometrica, 58, 543­570.
Altug, S. and R.A. Miller (1991): "Human Capital Accumulation, Aggregate Shocks and Panel Data Estimation," Discussion Paper 9128, CentER, Tilburg University.
Ashenfelter, O. and J. Ham (1979): "Education, Unemployment and Earnings," Journal of Political Economy, 87, S99­S116.
Blomquist, S. (1985): "Labour Supply in a Two-Period Model: The Effect of a Nonlinear Progressive Income Tax," Review of Economic Studies, 52, 514­524.
Blundell, R.W. (1987): "Econometric Approaches to the Specification of Life-Cycle Labor Supply and Commodity Demand Behaviour," Econometric Reviews, 6(1), 147­151.
Blundell, R.W. (1988): "Consumer Behaviour: Theory and Empirical Evidence - A Survey," Economic Journal, 98, 16­65.
Blundell, R.W and T. MaCurdy (1999): "Labor Supply: A Review of alternative Approaches," in Handbook of Labor Economics, Volume 3A, O. Ashenfelter and D. Card (eds.), Elsevier Science, Amsterdam.
Blundell, R.W. and C. Meghir (1990): "Panel Data and Life Cycle Models," Chapter 2 in Panel Data and Labour Market Studies, J. Hartog, G. Ridder and J. Theeuwes (eds.), North Holland, Amsterdam.
Blundell, R.W., M. Browning and C. Meghir (1994): "Consumer Demand and the Life-Cycle Allocation of Household Expenditures," Review of Economic Studies, 61, 57­80.
Blundell, R.W, A. Duncan and C. Meghir (1998): "Estimating Labor Supply Responses Using Tax Reforms," Econometrica, 66, 827­861.

792

B. Koebel et al.

Blundell, R.W., V. Fry and C. Meghir (1990): "Preference Restrictions in Microeconometric Models of Life Cycle Behaviour," Chapter 2 in Microeconometrics: Surveys and Applications, J.P. Florens, M. Ivaldi, J.J. Laffont and F. Laisney (eds.), Basil Blackwell, Oxford.
Blundell, R.W, C. Meghir and P. Neves (1993): "Labour supply and intertemporal substitution," Journal of Econometrics, 59, 137­160.
Blundell, R.W, P.-A. Chiappori, T. Magnac and C. Meghir (2005): "Collective Labour Supply: Heterogeneity and Nonparticipation," IZA Discussion Paper 1785.
Bover, O. (1986): "Some Dynamic Life Cycle Models of Labour Supply Estimated From Panel Data," Oxford Applied Economics Discussion Papers, 13.
Bover, O. (1989): "Estimating Intertemporal Labour Supply Elasticities Using Structural Models," Economic Journal, 99, 1026­1039.
Bover, O. (1991): "Relaxing Intertemporal Seperability: A Rational Habits Moel of Labour Supply Estimated from Panel Data," Journal of Labour Economics, 9, 85­100.
Browning, M. (1986): "The costs of using Frisch demand functions that are additive in the marginal utility of expenditure," Economics Letters, 21(3), 205­207.
Browning, M. and C. Meghir, (1991): "The Effects of Labour Supply on Commodity Demands," Econometrica, 59, 925­952.
Browning, M., A. Deaton and M. Irish (1985): "A Profitable Approach to Econometric Approaches to Labor Supply and Commodity Demands over the Life Cycle," Econometrica, 53, 503­543.
Camerer, C., L. Babcok, G. Loewenstein and R. Thaler, 1997, "Labor Supply of New York City Cabdrivers: One Day at a Time," Quaterly Journal of Economics, 112, 407­441.
Card, D. (1987): "Supply and Demand in the Labor Market," Princeton University Industrial Relations Section, Working Paper No. 228.
Card, D. (1994): "Intertemporal Labor Supply: an Assessment", in Advances in Econometrics, Sixth World Congress, C. Sims (ed.), Cambridge University Press, New York.
Chamberlain, G. (1984): "Panel Data," Chapter 22 in Handbook of Econometrics, Vol II, Z. Griliches and M.D. Intriligator (eds.), North-Holland, Amsterdam.
Conway, K. S. and T. J. Kniesner (1994): "Estimating labor supply with panel data," Economics letters, 44, 27­33.
Dustmann C. and M. E. Rochina-Barrachina (2000): "Selection correction in panel data models: an application to labour supply and wages," IZA Discussion Paper 162.
Eckstein, Z. and K.I. Wolpin (1989): "Dynamic Labour Force Participation of Married Women and Endogenous Work Experience," Review of Economic Studies, 56, 375­390.
Farber, Henry S. (2005): "Is Tomorrow Another Day? The Labor Supply of New York City Cabdrivers," Journal of Political Economy, 119, 46­82.
Fehr, E. and L. Go¨tte (2007): "Do Workers Work More When Wages are High", American Economic Review, 97(1), 298­317.
Greene, W.H. (2004): "Fixed Effects and the Incidental Parameters Problem in the Tobit Model," Econometric Reviews, 23(2), 125­148.
Ham, J.C. (1986): "Testing Whether Unemployment Represents Intertemporal Labour Supply Behaviour," Review of Economic Studies, 53(4), 559­578.
Hansen, L. P. and J. S. Singleton (1982): "Generalized Instrumental Variables Estimation of Nonlinear Rational Expectations Models," Econometrica, 50, 1269­1286.
Heckman, J.J. (1981): "The Incidental Parameters Problem and the Problem of initial Conditions in Estimating a Discrete Time - Discrete Data Stochastic Process and Some Monte-Carlo Evidence," in Structural Analysis of Discrete Data, C. Manski and D. McFadden, (eds.), MIT Press, Cambridge, MA, 179­195.
Heckman, J.J. and T.E. MaCurdy (1980): "A Life Cycle Model of Female Labour Supply," Review of Economic Studies, 47, 47­74.
Heckman, J.J. and T.E. MaCurdy (1982): "Corrigendum on A Life Cycle Model of Female Labour Supply," Review of Economic Studies, 49, 659­660.
Heckman, J.J. and T.E. MaCurdy (1986): "Labor Econometrics," Chapter 32 in Handbook of Econometrics, Vol III, Z. Griliches and M.D. Intriligator (eds.), North-Holland, Amsterdam.

23 Life Cycle Labor Supply and Panel Data

793

Henley, A. (2004): "House Price Shocks, Windfall Gains and Hours of Work: British Evidence", Oxford Bulletin of Economics and Statistics, 66(4), 439­456.
Holtz-Eakin, D., W. Newey and H.S. Rosen (1988): "Estimating Vector Autoregressions with Panel Data," Econometrica, 56, 1371­1395.
Hotz, V.J., F.E. Kydland and G.L. Sedlacek (1988): "Intertemporal Preferences and Labour Supply," Econometrica, 335­360.
Imai, S. and M. P. Keane (2004): "Intertemporal Labor Supply and Human Capital Accumulation.," International Economic Review, 45(2), 601­641.
Jakubson, G. (1988): "The Sensitivity of Labor Supply Parameter Estimates to Unobserved Individual Effects: Fixed- and Random-Effects Estimates in a Nonlinear Model Using Panel Data," Journal of Labor Economics, 6(3), 302­329.
Johnson, T.R. and J.H. Pencavel (1984): "Dynamic Hours of Work Functions for Husbands, Wives and Single Females," Econometrica, 52, 363­389.
Joulfaian, D. and M. O. Wilhelm (1994): "Inheritance and Labor Supply," Journal of Human Resources, 29, 1205­1234.
Killingsworth, M.R. (1983): Labor Supply, Cambridge University Press, Cambridge. Killingsworth, M.R. and J.J. Heckman (1986): "Female Labor Supply: A Survey," Chhapter 2 in
Handbook of Labor Economics, Vol I, O. Ashenfelter and R. Layard, (eds.), North-Holland, Amsterdam. Laisney, F., M. Lechner, A. van Soest and G. Wagenhals (1993): "A Life Cycle Labour Supply Model with Taxes Estimated on German Panel Data : The Case of Parallel Preferences," Economic and Social Review, 24, 335­368. Lich-Tyler, S. (2002): "Life-cycle labor supply under uncertainty," presented at the 2002 North American Econometric Society Winter Meeting. Lilja, R. (1986): Econometric Analyses of Family Labour Supply over the Life Cycle Using US Panel Data, The Helsinki School of Economics, Helsinki. Lundberg, S.J. (1988): "Labor Supply of Husbands and Wives: A Simultaneous Equations Approach," Review of Economics and Statistics, 70(2), 224­235. MaCurdy, T.E. (1981): "An Empirical Model of Labor Supply in a Life-Cycle Setting," Journal of Political Economy, 89, 1059­1085. MaCurdy, T.E. (1983): "A Simple Scheme for Estimating an Intertemporal Model of Labor Supply and Consumption in the Presence of Taxes and Uncertainty," International Economic Review, 24, 265­290. MaCurdy, T.E. (1990): "Appraising Tests of the Intertemporal Substitution Hypothesis," in Panel Data and Labour Market Studies, J. Hartog, G. Ridder and J. Theeuwes (eds.), North Holland, Amsterdam. MaCurdy, T.E., D. Green and H. Paarsch (1990): "Assessing Empirical Approaches for Analyzing Taxes and Labor Supply," Journal of Human Resources, 25, 413­490. Mazzocco, M. (2007): "Household Intertemporal Behavior: a Collective Characterization and a Test of Commitment", Review of Economic Studies, 74(3), 857­895. Mincer, J. (1962): "Labor Force Participation of Married Women: A Study of Labor Supply," Aspects of Labor Economics, NBER, Princeton University Press, Princeton, N.J., 63­97. Mroz, T.A. (1987): "The Sensitivity of an Empirical Model of Married Women's Hours of Work to Economic and Statistical Assumptions," Econometrica, 55, 765­799. Mundlak, Y. (1978): "On the Pooling of Time-Series and Cross-Section Data," Econometrica, 46, 69­85. Newey, W.K., (1984): "A Method of Moments Interpretation of Sequential Estimators," Economics Letters, 14, 201­206. Oettinger G. S. (1999): "An Empirical Analysis of the Daily Labor Supply of Stadium Vendors," Journal of Political Economy, 107, 360­392. Pagan, A. (1984): "Econometric Issues in the Analysis of Regressions with Generated Regressors," International Economic Review, 25, 221­247. Pencavel, J. (1986): "Labor Supply of Men: A Survey," in O. Ashenfelter and R. Layard (eds.), Handbook of Labor Economics, North-Holland, Amsterdam.

794

B. Koebel et al.

Pistaferri, L. (2003): "Anticipated and unanticipated wage changes, wage risk and life cycle labor supply," Journal of Labor Economics, 21, 729­754.
Shaw, K. (1989): "Life Cycle Labor Supply with Human Capital Accumulation," International Economic Review, 30(2), 431­457.
Wooldridge, J. M. (1995), "Selection corrections for panel data models under conditional mean independence assumptions,"emph Journal of Econometrics, 68, 115­132.
Ziliak, J.P. and T.J. Kniesner (1999): "Estimating Life-Cycle Labor Supply Tax Effects," Journal of Political Economy, 107, 326­359.

Chapter 24
Dynamic Policy Analysis
Jaap H. Abbring and James J. Heckman

Abstract This chapter studies the microeconometric treatment-effect and structural approaches to dynamic policy evaluation. First, we discuss a reduced-form approach based on a sequential randomization or dynamic matching assumption that is popular in biostatistics. We then discuss two complementary approaches for treatments that are single stopping times and that allow for non-trivial dynamic selection on unobservables. The first builds on continuous-time duration and event-history models. The second extends the discrete-time dynamic discrete-choice literature.

24.1 Introduction
The methods discussed in Parts 1 and 2 of this volume are useful for microeconometric policy evaluation. That field analyzes the effects of policy interventions on individual outcomes. Panel data facilitate the identification and estimation of such effects. Panel data are especially helpful in analyzing the individual dynamic consequences of policies and outcomes, which are mostly neglected in the vast crosssectional literature on this topic. Not surprisingly, panel-data methods are becoming more widely used in the microeconometric policy evaluation literature. In this chapter, we critically review recently developed methods and their applications.
The outline of the chapter is as follows. Section 24.2 presents the policy evaluation problem and discusses the treatment-effect approach to policy evaluation. It establishes the notation used in the rest of this chapter. Section 24.3 reviews an approach to the analysis of dynamic treatment effects based on a sequential
Jaap H. Abbring Department of Economics, VU University Amsterdam, De Boelelaan 1105, 1081 HV Amsterdam, The Netherlands, and Tinbergen Institute. e-mail: jabbring@econ.vu.nl.
James J. Heckman Department of Economics, University of Chicago, 1126 E. 59th Street, Chicago IL 60637, USA; American Bar Foundation and Geary Institute, University College Dublin, e-mail: jjh@uchicago.edu.

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

795

c Springer-Verlag Berlin Heidelberg 2008

796

J.H. Abbring and J.J. Heckman

randomization assumption that is popular in biostatistics (Gill and Robins, 2001; Lok 2007; Robins, 1997) and has been applied in economics (see Fitzenberger, Osikominu, and Vo¨lter, 2006; and Lechner and Miquel, 2002). This is a dynamic version of matching. We relate the assumptions justifying this approach to the assumptions underlying the econometric dynamic discrete-choice literature based on Rust's (1987) conditional-independence condition which, as discussed in Sect. 24.5.5 below, is frequently invoked in the structural econometrics literature. We note the limitations of the dynamic matching treatment-effect approach in accounting for dynamic information accumulation. In Sects. 24.4 and 24.5, we discuss two econometric approaches for the analysis of treatment times that allow for non-trivial dynamic selection on unobservables. Section 24.4 discusses the continuous-time event-history approach to policy evaluation (Abbring and Van den Berg, 2003b, 2005) and Abbring (2008). Section 24.5 introduces an approach developed by Heckman and Navarro (2007) that builds on and extends the discrete-time dynamic discrete-choice literature. Like the analysis of Abbring and Van den Berg, it does not rely on the conditional-independence assumptions used in dynamic matching. The two complementary approaches surveyed in this chapter span the existing econometric literature on dynamic treatment effects.

24.2 Policy Evaluation and Treatment Effects
24.2.1 The Evaluation Problem
We introduce some key ideas and set up the notation for this chapter by reviewing the static policy evaluation problem discussed in, e.g., Heckman and Vytlacil (2007a). Let  be the set of agent types. It is the sample space of a probability space (, I , P), and all choices and outcomes are random variables defined on this probability space. Each agent type    represents a single agent in a particular state of nature. We could distinguish variation between agents from within-agent randomness by taking  = J × ~ , with J the set of agents and ~ the set of possible states of nature. However, we do not make this distinction explicit in this chapter, and often simply refer to agents instead of agent types.1
Consider a policy that targets the allocation of each agent in  to a single treatment from a set S . In the most basic binary version, S = {0, 1}, where "1" represents "treatment", such as a training program, and "0" some baseline, "control" program. Alternatively, S could take a continuum of values, e.g., R+ = [0, ), representing, e.g., unemployment benefit levels, or duration of time in a program.
A policy p = (a, )  A ×T  P consists of a planner's rule a :   B for allocating constraints and incentives to agents, and a rule  :  × A  S that generates
1 For example, we could have  = [0, 1] indexing the population of agents, with P being Lebesgue measure on [0, 1]. Alternatively, we could take  = [0, 1] × ~ and have [0, 1] represent the population of agents and ~ states of nature.

24 Dynamic Policy Analysis

797

agent treatment choices for a given constraint allocation a. This framework allows agent 's treatment choice to depend both on the constraint assignment mechanism a--in particular, the distribution of the constraints in the population--and on the constraints a()  B assigned to agent .
The randomness in the planner's constraint assignment a may reflect heterogene-
ity of agents as observed by the planner, but it may also be due to explicit random-
ization. For example, consider profiling on background characteristics of potential
participants in the assignment a to treatment eligibility. If the planner observes some
background characteristics on individuals in the population of interest, she could
choose eligibility status to be a deterministic function of those characteristics and,
possibly, some other random variable under her control by randomization. This in-
cludes the special case in which the planner randomizes persons into eligibility. We
denote the information set generated by the variables observed by the planner when
she assigns constraints, including those generated through deliberate randomization, by IP.2 The planner's information set IP determines how precisely she can target each agent's  when assigning constraints. The variables in the information set fully determine the constraints assignment a.
Subsequent to the planner's constraints assignment a, each agent  chooses treatment (, a). We assume that agents know the constraint assignment mechanism a in place. However, agents do not directly observe their types , but only observe realizations IA() of some random variables IA. For given a  A , agent 's treatment choice (, a) can only depend on  through his observations IA(). Typically, IA() includes the variables used by the planner in determining a(), so that agents know the constraints that they are facing. Other components of IA() may be determinants of preferences and outcomes. Variation in IA() across  may thus reflect preference heterogeneity, heterogeneity in the assigned constraints, and heterogeneity in outcome predictors. We use IA to denote the information set generated by IA.3 An agent's information set IA determines how precisely the agent can tailor his treatment choice to his type . For expositional convenience, we assume that agents know more when choosing treatment than what the planner knows when assigning constraints, so that IA  IP. One consequence is that agents observe the constraints a() assigned to them, as previously discussed. In turn, the econometrician may not have access to all of the information that is used by the agents when

2 Formally, IP is a sub- -algebra of I and a is assumed to be IP-measurable.
3 Formally, IA is a sub- -algebra of I --the  -algebra generated by IA--and     (, a)  S should be IA-measurable for all a  A . The possibility that different agents have different information sets is allowed for because a distinction between agents and states of nature is implicit.
As suggested in the introduction to this section, we can make it explicit by distinguishing a set J of agents and a set ~ of states of nature and writing  = J × ~ . For expositional convenience,
let J be finite. We can model the case that agents observe their identity j by assuming that the random variable JA on  that reveals their identity, that is JA( j, ~ ) = j, is in their information set IA. If agents, in addition, observe some other random variable V on , then the information set IA generated by (JA,V ) can be interpreted as providing each agent j  J with perfect information about his identity j and with the agent- j-specific information about the state of nature ~ encoded in the random variable ~  V ( j, ~ ) on ~ .

798

J.H. Abbring and J.J. Heckman

they choose treatment.4 In this case, IA  IE , where IE denotes the econometrician's information set.
We define sp() as the treatment selected by agent  under policy p. With p = (a, ), we have that sp() = (, a). The random variable sp :   S represents the allocation of agents to treatments implied by policy p.5 Randomness in this
allocation reflects both heterogeneity in the planner's assignment of constraints and
the agents' heterogenous responses to this assignment. One extreme case arises if
the planner assigns agents to treatment groups and agents perfectly comply, so that B = S and sp() = (, a) = a() for all   . In this case, all variation of sp is due to heterogeneity in the constraints a() across agents . At the other extreme, agents do not respond at all to the incentives assigned by mechanisms in A , and (a, ) = (a , ) for all a, a  A and   . In general, there are policies that have a nontrivial (that is, nondegenerate) constraint assignment a, where at
least some agents respond to the assigned constraints a in their treatment choice, (a, ) = (a , ) for some a, a  A and   .
We seek to evaluate a policy p in terms of some outcome Yp, for example, earnings. For each p  P, Yp is a random variable defined on the population . The evaluation can focus on objective outcomes Yp, on the subjective valuation R(Yp) of Yp by the planner or the agents, or on both types of outcomes. The evaluation can be performed relative to a variety of information sets reflecting different actors (the
agent, the planner and the econometrician) and the arrival of information in different
time periods. Thus, the randomness of Yp may represent both (ex ante) heterogeneity among agents known to the planner when constraints are assigned (that is, variables in IP) and/or heterogeneity known to the agents when they choose treatment (that is, information in IA), as well as (ex post) shocks that are not foreseen by the policy maker or by the agents. An information-feasible (ex ante) policy evaluation by the
planner would be based on some criterion using the distribution of Yp conditional on IP. The econometrician can assist the planner in computing this evaluation if the planner shares her ex ante information and IP  IE .
Suppose that we have data on outcomes Yp0 under policy p0 with corresponding treatment assignment sp0 . Consider an intervention that changes the policy from the actual p0 to some counterfactual p with associated treatments sp and outcomes Yp . This could involve a change in the planner's constraint assignment from a0 to a for given 0 =  , a change in the agent choice rule from 0 to  for given a0 = a , or both.
The policy evaluation problem involves contrasting Yp and Yp0 or functions of these outcomes. For example, if the outcome of interest is mean earnings, we might be interested in some weighted average of E[Yp -Yp0 | IP], such as E[Yp -Yp0 ]. The special case where S = {0, 1} and sp = a = 0 generates the effect of abolishing the program. Implementing such a policy requires that the planner be able to induce
all agents into the control group by assigning constraints a = 0. In particular, this

4 See the discussion by Heckman and Vytlacil (2007b, Sects. 2 and 9). 5 Formally, {sp}pA ×T is a stochastic process indexed by p.

24 Dynamic Policy Analysis

799

assumes that there are no substitute programs available to agents that are outside the planner's control (Heckman and Vytlacil, 2007b, Sect. 10).
For notational convenience, write S = sp0 for treatment assignment under the actual policy p0 in place. Cross-sectional micro data typically provide a random sample from the joint distribution of (Yp0 , S).6 Clearly, without further assumptions, such data do not identify the effects of the policy shift from p0 to p . This identification problem becomes even more difficult if we do not seek to compare the counterfactual policy p with the actual policy p0, but rather with another counterfactual policy p that also has never been observed. A leading example is the binary case in which 0 < Pr(S = 1) < 1, but we seek to know the effects of sp = 0 (universal nonparticipation) and sp = 1 (universal treatment), where neither policy has ever been observed in place.
Panel data can help to evaluate the type of static policies discussed so far, if interpreted as short-run or even one-shot policies. Suppose that we have data on outcomes in two periods in which two different policies were in place. In a world in which outcomes in any period are not affected by the policy or outcomes in the other period, such data are directly informative on the contrast between outcomes under both policies.
The standard microeconometric approach to the policy evaluation problem assumes that the (subjective and objective) outcomes for any individual agent are the same across all policy regimes for any particular treatment assigned to the individual (see, e.g., Heckman, LaLonde, and Smith, 1999). Heckman and Vytlacil (2007a) present a detailed account of the policy-invariance assumptions that justify this practice. They simplify the task of evaluating policy p to determining (i) the assignment sp of treatments under policy p and (ii) treatment effects for individual outcomes. Even within this simplified framework, there are still two difficult, and distinct, problems in identifying treatment effects on individual outcomes:

(A) The Evaluation Problem: that we observe an agent in one treatment state and seek to determine that agent's outcomes in another state; and
(B) The Selection Problem: that the distributions of outcomes for the agents we observe in a given treatment state are not the marginal population distributions that would be observed if agents were randomly assigned to the state.

The assignment mechanism sp of treatments under counterfactual policies p is straightforward in the case where the planner assigns agents to treatment groups and agents fully comply, so that sp = a. More generally, an explicit model of agent treatment choices is needed to derive sp for counterfactual policies p. An explicit model of agent treatment choices can also be helpful in addressing the selection problem, and in identifying agent subjective valuations of outcomes. We now formalize the notation for the treatment-effect approach that we will use in this section using the

6 Notice that a random sample of outcomes under a policy may entail nonrandom selection of treatments as individual agents select individual treatments given  and the constraints they face assigned by a.

800

J.H. Abbring and J.J. Heckman

potential-outcome framework of Neyman (1923), Roy (1951), Quandt (1958, 1972), and Rubin (1974).7

24.2.2 The Treatment Effect Approach
For each agent   , let y(s, X(),U()) be the potential outcome when the agent is assigned to treatment s  S . Here, X and U are covariates that are not causally affected by the treatment or the outcomes.8,9 In the language of Kalbfleisch and Prentice (1980) and Leamer (1985), we say that such covariates are "external" to the causal model. X is observed by the econometrician (that is, in IE ) and U is not.
Recall that sp is the assignment of agents to treatments under policy p. For all policies p that we consider, the outcome Yp is linked to the potential outcomes by the consistency condition Yp = y(sp, X,U). This condition follows from the policyinvariance assumptions. It embodies the assumption that an agent's outcome only depends on the treatment assigned to the agent and not separately on the mechanism used to assign treatments. This excludes (strategic) interactions between agents and equilibrium effects of the policy.10 It ensures that we can specify individual outcomes y from participating in programs in S independently of the policy p and treatment assignment sp. Economists say that y is autonomous, or structurally invariant with respect to the policy environment (see Frisch, 1938; Hurwicz, 1962; and Heckman and Vytlacil, 2007a).11
We illustrate the treatment-effect approach with a basic example. Consider the evaluation of an intervention that changes the policy from p0 to p in terms of its mean effect E[Yp - Yp0 ] on outcomes. For expositional convenience, let treatment be binary: S = {0, 1}. Suppose that we have a cross-sectional sample from the joint distribution of (Yp0 , S, X). Assume that treatment assignment under both the actual policy p0 and the alternative policy p is randomized, that is, both S and sp are independent of the determinants (X,U) of the potential outcomes. Then, because of the policy-invariance conditions,
7 See Heckman et al. (1999); Heckman and Vytlacil (2007a); and Thurstone (1930), for results in econometrics and extensive reviews of the econometric literature. 8 This "no feedback" condition requires that X and U are the same fixing treatment to s for all s. See Haavelmo (1943), Pearl (2000), or the discussion in Heckman and Vytlacil (2007a,b). 9 Note that this framework is rich enough to capture the case in which potential outcomes depend on treatment-specific unobservables as in Sect. 24.5, because these can be simply stacked in U and subsequently selected by y. For example, in the case where S = {0, 1} we can write y(s, X, (U0,U1)) = sy1(X,U1) + (1 - s)y0(X,U0) for some y0 and y1. A specification without treatment-dependent unobservables is more tractable in the case of continuous treatments in Sect. 24.3 and, in particular, continuous treatment times in Sect. 24.4. 10 See Pearl (2000); Heckman (2005); and Heckman and Vytlacil (2007a). 11 See also Aldrich (1989) and Hendry and Morgan (1995). Rubin's (1986) stable-unit-treatmentvalue assumption is a version of the classical invariance assumptions of econometrics (see Abbring, 2003; and Heckman and Vytlacil, 2007a, for discussion of this point).

24 Dynamic Policy Analysis

801

E[Yp -Yp0 ] = E[y(1, X,U) - y(0, X,U)] Pr(sp = 1) - Pr(S = 1) .
The mean effect of the intervention on outcomes equals the "average treatment effect" E[y(1, X,U) - y(0, X,U)] times the net increase in the assignment to treatment 1. The policy evaluation problem boils down to identifying the average treatment effect, the distribution of the actual treatment assignment S, and the distribution of treatment assignment sp under the alternative policy p . Under the assumption of randomized assignment, and provided that 0 < Pr(S = 1) < 1, the average treatment effect is identified as E[Yp0 |S = 1] - E[Yp0|S = 0]. The distribution of S is identified directly from the data. The distribution of sp is often known, as in the case of universal nonparticipation (sp = 0) or universal treatment (sp = 1). Otherwise, it needs to be identified using a model of treatment choice.
Heckman and Vytlacil (2007a,b) review more general evaluation problems and econometric methods that do not rely on randomized assignment, such as the methods of matching and instrumental variables. Clearly, panel data, combined with stationarity assumptions, can help in addressing the selection problem in the evaluation of static policies. We will not dwell on this application of panel data to the evaluation of static policies, but now turn to the dynamic policy evaluation problem.

24.2.3 Dynamic Policy Evaluation
Interventions often have consequences that span over many periods. Policy interventions at different points in time can be expected to affect not only current outcomes, but also outcomes at other points in time. The same policy implemented at different time periods may have different consequences. Moreover, policy assignment rules often have non-trivial dynamics. The assignment of programs at any point in time can be contingent on the available data on past program participation, intermediate outcomes and covariates.
The dynamic policy evaluation problem can be formalized in a fashion similar to the way we formalized the static problem in Sect. 24.2.1. In this subsection, we analyze a discrete-time finite-horizon model. We consider continuous-time models in Sect. 24.4. The possible treatment assignment times are 1, . . . , T¯ . We do not restrict the set S of treatments. We allow the same treatment to be assigned on multiple occasions. In general, the set of available treatments at each time t may depend on time t and on the history of treatments, outcomes, and covariates. For expositional convenience, we will only make this explicit in Sects. 24.4 and 24.5, where we focus on the timing of a single treatment.
We define a dynamic policy p = (a, )  A ×T  P as a dynamic constraint assignment rule a = {at }tT¯=1 with a dynamic treatment choice rule  = {t }tT¯=1. At each time t, the planner assigns constraints at() to each agent   , using information in the time-t policy-p information set IP(t, p)  I . The planner's information set IP(t, p) could be based on covariates and random variables under the planner's control, as well as past choices and realized outcomes. We denote the sequence of

802

J.H. Abbring and J.J. Heckman

planner's information sets by IP(p) = {IP(t, p)}tT¯=1. We assume that the planner does not forget any information she once had, so that her information improves over time and IP(t, p)  IP(t + 1, p) for all t.12
Each agent  chooses treatment t (, a) given their information about  at time t under policy p and given the constraint assignment mechanism a  A in place.
We assume that agents know the constraint assignment mechanism a in place. At time t, under policy p, agents infer their information about their type  from random variables IA(t, p) that may include preference components and determinants of constraints and future outcomes. IA(t, p) denotes the time-t policy-p information set generated by IA(t, p) and IA(p) = {IA(t, p)}tT¯=1. We assume that agents are increasingly informed as time goes by, so that IA(t, p)  IA(t + 1, p).13 For expositional convenience, we also assume that agents know more than the planner at each time t, so that IP(t, p)  IA(t, p).14 Because all determinants of past and current constraints are in the planner's information set IP(t, p), this implies that agents observe (a1(), . . . , at ()) at time t. Usually, they do not observe all determinants of their future constraints (at+1(), . . . , aT¯ ()).15 Thus, the treatment choices of the agents may be contingent on past and current constraints, their preferences, and on
their predictions of future outcomes and constraints given their information IA(t, p) and given the constraint assignment mechanism a in place.
Extending the notation for the static case, we denote the assignment of agents to treatment t at time t implied by a policy p by the random variable sp(t) defined so that sp(,t) = t (, a). We use the shorthand stp for the vector (sp(1), . . . , sp(t)) of treatments assigned up to and including time t under policy p, and write sp = sTp¯ . The assumptions made so far about the arrival of information imply that treatment
assignment sp(t) can only depend on the information IA(t, p) available to agents at time t.16
Because past outcomes typically depend on the policy p, the planner's informa-
tion IP(p) and the agents' information IA(p) will generally depend on p as well. In the treatment-effect framework that we develop in the next section, at each time
t different policies may have selected different elements in the set of potential out-
comes in the past. The different elements reveal different aspects of the unobserv-
ables underlying past and future outcomes. We will make assumptions that limit
the dependence of information sets on policies in the context of the treatment-effect
approach developed in the next section.
Objective outcomes associated with policies p are expressed as a vector of timespecific outcomes Yp = (Yp(1), . . . ,Yp(T¯ )). The components of this vector may also be vectors. We denote the outcomes from time 1 to time t under policy p by

12 Formally, the information IP(p) that accumulates for the planner under policy p is a filtration in I , and a is a stochastic process that is adapted to IP(p). 13 Formally, the information IA(p) that accumulates for the agents is a filtration in I . 14 If agents are strictly better informed, and IP(t, p)  IA(t, p), it is unlikely that the planner catches up and learns the agent's information with a delay (e.g., IA(t, p)  IP(t + 1, p)) unless agent's choices and outcomes reveal all their private information.
15 Formally, a1, . . . , at are IA(t, p)-measurable, but at+1, . . . , aT¯ are not. 16 Formally, {sp(t)}tT¯=1 is a stochastic process that is adapted to IA(p).

24 Dynamic Policy Analysis

803

Ypt = (Yp(1), . . . ,Yp(t)). We analyze both subjective and objective evaluations of policies in Sect. 24.5, where we consider more explicit economic models. Analogous to our analysis of the static case, we cannot learn about the outcomes Yp that would arise under a counterfactual policy p from data on outcomes Yp0 and treatments sp0 = S under a policy p0 = p without imposing further structure on the problem.17 We follow the approach exposited for the static case and assume policy invariance of individual outcomes under a given treatment. This reduces the evaluation of a dynamic policy p to identifying (i) the dynamic assignment sp of treatments under policy p and (ii) the dynamic treatment effects on individual outcomes. We focus our discussion on the fundamental evaluation problem and the selection problem that haunt inference about treatment effects. In the remainder of the section, we review alternative approaches to identifying dynamic treatment effects, and some approaches to modeling dynamic treatment choice. We first analyze methods recently developed in statistics.

24.3 Dynamic Treatment Effects and Sequential Randomization
In a series of papers, Robins extends the static Neyman­Roy­Rubin model based on selection on observables to a dynamic setting (see, e.g., Robins, 1997, and the references therein). He does not consider agent choice or subjective evaluations. Here, we review his extension, discuss its relationship to dynamic choice models in econometrics, and assess its merits as a framework for economic policy analysis. We follow the exposition of Gill and Robins (2001), but add some additional structure to their basic framework to exposit the connection of their approach to the dynamic approach pursued in econometrics.

24.3.1 Dynamic Treatment Effects
24.3.1.1 Dynamic Treatment and Dynamic Outcomes
To simplify the exposition, suppose that S is a finite discrete set.18 Recall that, at each time t and for given p, treatment assignment sp(t) is a random variable that only depends on the agent's information IA(t, p), which includes personal knowledge of preferences and determinants of constraints and outcomes. To make this dependence explicit, suppose that external covariates Z, observed by the econometrician (that is, variables in IE ), and unobserved external covariates V1 that
17 If outcomes under different policy regimes are informative about the same technology and preferences, for example, then the analyst and the agent could learn about the ingredients that produce counterfactual outcomes in all outcome states. 18 All of the results presented in this subsection extend to the case of continuous treatments. We will give references to the appropriate literature in subsequent footnotes.

804

J.H. Abbring and J.J. Heckman

affect treatment assignment are revealed to the agents at time 1. Then, at the start of each period t  2, past outcomes Yp(t - 1) corresponding to the outcomes realized under treatment assignment sp and external unobserved covariates Vt enter the agent's information set.19 In this notation, IA(1, p) is the information  (Z,V1) conveyed to the agent by (Z,V1) and, for t  2, IA(t, p) =  (Ypt-1, Z,V t ), with V t = (V1, . . . ,Vt ). In the notation of the previous subsection, IA(1, p) = (Z,V1) and, for t  2, IA(t, p) = (Ypt-1, Z,V t ). Among the elements of IA(t, p) are the determinants of the constraints faced by the agent up to t, which may or may not be observed
by the econometrician. We attach ex post potential outcomes Y (t, s) = yt (s, X,Ut), t = 1, . . . , T¯ , to each
treatment sequence s = (s(1), . . . , s(T¯ )). Here, X is a vector of observed (by the econometrician) external covariates and Ut , t = 1, . . . , T¯ , are vectors of unobserved external covariates. Some components of X and Ut may be in agent information sets. We denote Y t (s) = (Y (1, s), . . . ,Y (t, s)), Y (s) = Y T¯ (s), and U = (U1, . . . ,UT¯ ). As in the static case, potential outcomes y are assumed to be invariant across policies p,
which ensures that Yp(t) = yt (sp, X,Ut ). In the remainder of this section, we keep the dependence of outcomes on observed covariates X implicit and suppress all
conditioning on X. We assume no causal dependence of outcomes on future treatment:20
(NA) For all t  1, Y (t, s) = Y (t, s ) for all s, s such that st = (s )t ,
where st = (s(1), . . . , s(t)) and (s )t = (s (1), . . . , s (t)). Abbring and Van den Berg
(2003b) and Abbring (2003) define this as a "no-anticipation" condition. It re-
quires that outcomes at time t (and before) be the same across policies that allo-
cate the same treatment up to and including t, even if they allocate different treat-
ments after t. In the structural econometric models discussed in Sects. 24.3.2.2 and
24.5 below, this condition is trivially satisfied if all state variables relevant to out-
comes at time t are included as inputs in the outcome equations Y (t, s) = yt (s,Ut), t = 1, . . . , T¯ .
Because Z and V1 are assumed to be externally determined, and therefore not affected by the policy p, the initial agent information set IA(1, p) =  (Z,V1) does not depend on p. Agent  has the same initial data (Z(),V1()) about his type  under all policies p. Thus, IA(1, p) = IA(1, p ) is a natural benchmark information set for an ex ante comparison of outcomes at time 1 among different policies. For t  2, (NA) implies that actual outcomes up to time t - 1 are equal between policies p and p , Ypt-1 = Ypt-1, if the treatment histories coincide up to time t - 1 so that stp-1 = stp-1. Together with the assumption that Z and V t are externally determined, it follows that agents have the same time-t information set structure about  under policies p and p , IA(t, p) =  (Ypt-1, Z,V t ) =  (Ypt-1, Z,V t ) = IA(t, p ),

19 Note that any observed covariates that are dynamically revealed to the agents can be subsumed in the outcomes. 20 For statistical inference from data on the distribution of (Yp0 , S, Z), these equalities only need to hold on events {   : St () = st }, t  1, respectively.

24 Dynamic Policy Analysis

805

if stp-1 = stp-1.21,22 In this context, IA(t, p) = IA(t, p ) is a natural information set for an ex ante comparison of outcomes from time t onwards between any two policies p and p such that stp-1 = stp-1.
With this structure on the agent information sets in hand, it is instructive to
review the separate roles in determining treatment choice of information about  and knowledge about the constraint assignment rule a. First, agent 's time-t treatment choice sp(,t) = t (, a) may depend on distributional properties of a, for example the share of agents assigned to particular treatment sequences, and on the past and current constraints (a1(), . . . , at ()) that were actually assigned to him. We have assumed both to be known to the agent. Both may differ between policies, even if the agent information about  is fixed across the policies. Second, agent 's time-t treatment choice may depend on agent 's predictions of future constraints and outcomes. A forward-looking agent  will use observations of his covariates Z() and V t () and past outcomes Ypt-1() to infer his type  and subsequently predict future external determinants (Ut (), . . . ,UT¯ ()) of his outcomes and (Vt+1(), . . . ,VT¯ ()) of his constraints and treatments. In turn, this information updating allows agent  to predict his future potential outcomes (Y (t, s, ), . . . ,Y (T¯ , s, )) and, for a given policy regime p, his future constraints (at+1(), . . . , aT¯ ()), treatments (sp(t + 1, ), . . . , sp(T¯ , )), and realized outcomes (Yp(t, ), . . . ,Yp(T¯ , )). Under different policies, the agent may gather different information on his type  and therefore come up with different predictions of the external determinants of his future potential outcomes and con-
straints. In addition, even if the agent has the same time-t predictions of the ex-
ternal determinants of future constraints and potential outcomes, he may translate
these into different predictions of future constraints and outcomes under different
policies.
Assumption (NA) requires that current potential outcomes are not affected by
future treatment. Justifying this assumption requires specification of agent infor-
mation about future treatment and agent behavior in response to that information.
Such an interpretation requires that we formalize how information accumulates for agents across treatment sequences s and s such that st = (s )t and (st+1, . . . , sT¯ ) = (st+1, . . . , sT¯ ). To this end, consider policies p and p such that sp = s and sp = s . These policies produce the same treatment assignment up to time t, but are different
in the future. We have previously shown that, even though the time-t agent information about  is the same under both policies, IA(t, p) = IA(t, p ), agents may have different predictions of future constraints, treatments and outcomes because
the policies may differ in the future and agents know this. The policy-invariance

21 If stp-1() = stp-1() only holds for  in some subset t-1   of agents, then Ypt-1() = Ypt-1() only for   t-1, and information coincides between p and p only for agents in t-1. Formally, let t-1 be the set {   : stp-1() = stp-1()} of agents that share the same treatment up to and including time t - 1. Then, t-1 is in the agent's information set under both policies, t-1  IA(t, p)  IA(t, p ). Moreover, the partitioning of t-1 implied by IA(t, p) and IA(t, p ) is the same. To see this, note that the collections of all sets in, respectively, IA(t, p) and IA(t, p ) that are weakly included in t-1 are identical  -algebras on t-1.
22 Notice that the realizations of the random variables Ypt-1, Z, V t may differ among agents.

806

J.H. Abbring and J.J. Heckman

conditions ensure that time-t potential outcomes are nevertheless the same under each policy. This requires that potential outcomes be determined externally, and are not affected by agent actions in response to different predictions of future constraints, treatments and outcomes. This assumption rules out investment responses to alternative policies that affect potential outcomes.
In general, different policies in P will produce different predictions of future constraints, treatment and outcomes. In the dynamic treatment-effect framework, this may affect outcomes indirectly through agent treatment choices. If potential outcomes are directly affected by agent's forward-looking decisions, then the invariance conditions underlying the treatment-effect framework will be violated. Section 24.3.3 illustrates this issue, and the no-anticipation condition, with some examples.

24.3.1.2 Identification of Treatment Effects
Suppose that the econometrician has data that allows her to estimate the joint distribution of (Yp0 , S, Z) of outcomes, treatments and covariates under some policy p0, where S = sp0 . These data are not enough to identify dynamic treatment effects.
To secure identification, Gill and Robins (2001) invoke a dynamic version of the matching assumption (conditional independence) which relies on sequential randomization:23
(M-1) For all treatment sequences s and all t,
S(t)(Y (t, s), . . . ,Y (T¯ , s)) | (Ypt-0 1, St-1 = st-1, Z),
where the conditioning set (Yp00 , S0 = s0, Z) for t = 1 is Z.
Equivalently, S(t)(Ut , . . . ,UT¯ ) | (Ypt-0 1, St-1, Z) for all t without further restricting the data. Sequential randomization allows the Yp0 (t) to be "dynamic confounders"-- variables that are affected by past treatment and that affect future treatment assignment.
The sequence of conditioning information sets appearing in the sequential randomization assumption, IE (1) =  (Z) and, for t  2, IE (t) =  (Ypt-0 1, St-1, Z), is a filtration IE of the econometrician's information set  (Yp0 , S, Z). Note that IE (t)  IA(t, p0) for each t. If treatment assignment is based on strictly more information than IE , so that agents know strictly more than the econometrician and act on their superior information, (M-1) is likely to fail if that extra information also affects outcomes. Heckman and Vytlacil (2007b) make this point in a static setting.
23 Formally, we need to restrict attention to sequences s in the support of S. Throughout this section, we will assume this and related support conditions hold.

24 Dynamic Policy Analysis

807

Together with the no-anticipation condition (NA), which is a condition on outcomes and distinct from (M-1), the dynamic potential-outcome model set up so far is a natural dynamic extension of the Neyman­Roy­Rubin model for a static (stratified) randomized experiment.
Under assumption (M-1) that the actual treatment assignment S is sequentially randomized, we can sequentially identify the causal effects of treatment from the distribution of the data (Yp0 , S, Z) and construct the distribution of the potential outcomes Y (s) for any treatment sequence s in the support of S.
Consider the case in which all variables are discrete. No-anticipation condition (NA) ensures that potential outcomes for a treatment sequence s equal actual (under policy p0) outcomes up to time t - 1 for agents with treatment history st-1 up to time t - 1. Formally, Y t-1(s) = Ypt-0 1 on the set {St-1 = st-1}. Using this, sequential randomization assumption (M-1) can be rephrased in terms of potential outcomes: for all s and t,
S(t)(Y (t, s), . . . ,Y (T¯ , s)) | (Y t-1(s), St-1 = st-1, Z) .

In turn, this implies that, for all s and t,

Pr (Y (t, s) = y(t) | Y t-1(s) = yt-1, St = st , Z = Pr Y (t, s) = y(t) | Y t-1(s) = yt-1, Z ,

(24.1)

where yt-1 = (y(1), . . . , y(t - 1)) and y = yT¯ . From Bayes' rule and (24.1), it follows that

Pr (Y (s) = y|Z)
T¯
= Pr (Y (1, s) = y(1) | Z)  Pr Y (t, s) = y(t) | Y t-1(s) = yt-1, Z t=2
= Pr (Y (1, s) = y(1) | S(1) = s(1), Z)
T¯
 × Pr (Y (t, s) = y(t) | Y t-1(s) = yt-1, St = st , Z . t=2
Invoking (NA), in particular Y (t, s) = Yp0 (t) and Y t-1(s) = Ypt-0 1 on {St = st }, produces

Pr (Y (s) = y|Z) = Pr Yp0 (1) = y(1) | S(1) = s(1), Z
T¯
 × Pr Yp0 (t) = y(t) | Ypt-0 1 = yt-1, St = st , Z . (24.2) t=2

808

J.H. Abbring and J.J. Heckman

This is a version of Robins' (1997) "g-computation formula".24,25 We can sequentially identify each component on the left hand side of the first expression, and hence identify the counterfactual distributions. This establishes identification of the distribution of Y (s) by expressing it in terms of objects that can be identified from data. Identification is exact (or "tight") in the sense that the identifying assumptions, no anticipation and sequential randomization, do not restrict the factual data and are therefore not testable (Gill and Robins, 2001).26
Example 24.1. Consider a two-period (T¯ = 2) version of the model in which agents take either "treatment" (1) or "control" (0) in each period. Then, S(1) and S(2) have values in S = {0, 1}. The potential outcomes in period t are Y (t, (0, 0)), Y (t, (0, 1)), Y (t, (1, 0)) and Y (t, (1, 1)). For example, Y (2, (0, 0)) is the outcome in period 2 in the case that the agent is assigned to the control group in each of the two periods. Using Bayes' rule, it follows that

Pr (Y (s) = y|Z) = Pr (Y (1, s) = y(1) | Z) Pr (Y (2, s) = y(2) | Y (1, s) = y(1), Z) . (24.3)
The g-computation approach to constructing Pr (Y (s) = y|Z) from data replaces the two probabilities in the right-hand side with probabilities of the observed (by the econometrician) variables (Yp0 , S, Z). First, note that Pr (Y (1, s) = y(1) | Z) = Pr (Y (1, s) = y(1) | S(1) = s(1), Z) by (M-1). Moreover, (NA) ensures that potential outcomes in period 1 do not depend on the treatment status in period 2, so
that

Pr (Y (1, s) = y(1) | Z) = Pr Yp(0) (1) = y(1) | S(1) = s(1), Z .

24 Gill and Robins (2001) present versions of (NA) and (M-1) for the case with more general distributions of treatments, and prove a version of the g-computation formula for the general case. For a random vector X and a function f that is integrable with respect to the distribution of X, let xA f (x) Pr(X  dx) = E[ f (X)1(X  A)]. Then,

Pr (Y (s)  A|Z) =

Pr
yA

Yp0 (T¯ )  dy(T¯ ) | YpT¯0-1 = yT¯ -1, ST¯ = sT¯ , Z

...

× Pr Yp0 (2)  dy(2) | Yp0 (1) = y(1), S2 = s2, Z

× Pr Yp0 (1)  dy(1) | S(1) = s(1), Z ,

where A is a set of Y (s). The right-hand side of this expression is almost surely unique under regularity conditions presented by Gill and Robins (2001).
25 An interesting special case arises if the outcomes are survival indicators, that is if Yp0 (t) = 1 if the agent survives up to and including time t and Yp0 (t) = 0 otherwise, t  1. Then, no anticipation (NA) requires that treatment after death does not affect survival, and the g-computation formula simplifies considerably (Abbring, 2003).
26 Gill and Robins' (2001) analysis only involves causal inference on a final outcome i.e., our Y s, T¯ and does not invoke the no-anticipation condition. However, their proof directly applies to the case studied in this chapter.

24 Dynamic Policy Analysis

809

Similarly, subsequently invoking (NA) and (M-1), then (M-1), and then (NA), gives

Pr (Y (2, s) = y(2) | Y (1, s) = y(1), Z)

= Pr Y (2, s) = y(2) | Yp0 (1), S(1) = s(1), Z

(by (NA) and (M-1))

= Pr Y (2, s) = y(2) | Yp0 (1), S = s, Z

(by (M-1))

= Pr Yp0 (2) = y(2) | Yp0 (1), S = s, Z . (by (NA))

Substituting these equations into the right-hand side of (24.3) gives the g-computation formula,

Pr (Y (s) = y |Z) = Pr Yp0 (1) = y(1) | S(1) = s(1), Z × Pr Yp0 (2) = y(2) | Yp0 (1) = y(1), S = s, Z .
Note that the right-hand side expression does not generally reduce to Pr Yp0 = y| S = s, Z). This would require the stronger, static matching condition SY (s) | Z, which we have not assumed here.

Matching on pretreatment covariates is a special case of the g-computation approach. Suppose that the entire treatment path is assigned independently of potential outcomes given pretreatment covariates Z or, more precisely, SY (s) | Z for all s. This implies sequential randomization (M-1), and directly gives identification of the distributions of Y (s)|Z and Y (s). The matching assumption imposes no restriction on the data since Y (s) is only observed if S = s. The no-anticipation condition (NA) is not required for identification in this special case because no conditioning on St is required. Matching on pretreatment covariates is equivalent to matching in a static model. The distribution of Y (s)|Z is identified without (NA), and assuming it to be true would impose testable restrictions on the data. In particular, it would imply that treatment assignment cannot be dependent on past outcomes given Z. The static matching assumption is not likely to hold in applications where treatment is dynamically assigned based on information on intermediate outcomes. This motivates an analysis based on the more subtle sequential randomization assumption. An alternative approach, developed in Sect. 24.5, is to explicitly model and identify the evolution of the unobservables.
Gill and Robins claim that their sequential randomization and no-anticipation assumptions are "neutral", "for free", or "harmless". As we will argue later, from an economic perspective, some of the model assumptions, notably the no-anticipation assumption, can be interpreted as substantial behavioral/informational assumptions. For example, Heckman and Vytlacil (2005, 2007b) and Heckman and Navarro (2004) show how matching imposes the condition that marginal and average returns are equal. Because of these strong assumptions, econometricians sometimes phrase their "neutrality" result more negatively as a non-identification result (Abbring and Van den Berg, 2003b), since it is possible that (M-1) and/or (NA) may not hold.

810

J.H. Abbring and J.J. Heckman

24.3.2 Policy Evaluation and Dynamic Discrete-Choice Analysis

24.3.2.1 The Effects of Policies

Consider a counterfactual policy p such that the corresponding allocation of treatments sp satisfies sequential randomization, as in (M-1):
(M-2) For all treatment sequences s and all t,
sp (t)(Y (t, s), . . . ,Y (T¯ , s)) | (Ypt-1, stp-1 = st-1, Z).
The treatment assignment rule sp is equivalent to what Gill and Robins (2001) call a "randomized plan". The outcome distribution under such a rule cannot be constructed by integrating the distributions of {Y (s)} with respect to the distribution of sp , because there may be feedback from intermediate outcomes into treatment assignment. Instead, under the assumptions of the previous subsection and a support condition, we can use a version of the g-computation formula for randomized plans given by Gill and Robins to compute the distribution of outcomes under the policy p :27

Pr Yp = y|Z

=  Pr Yp0 (1) = y(1) | S(1) = s(1), Z Pr sp (1) = s(1) | Z sS

T¯
 × Pr Yp0 (t) = y(t) | Ypt-0 1 = yt-1, St = st , Z t=2

× Pr sp (t) = s(t) | Ypt-1 = yt-1, stp-1 = st-1, Z

(24.4)

In the special case of static matching on Z, so that sp U | Z, this simplifies to integrating the distribution of Yp0 | (S = s, Z) over the distribution of sp |Z:28
27 The corresponding formula for the case with general treatment distributions is

Pr Yp  A|Z

=
yA sS

Pr Yp0 (T¯ )  dy(T¯ ) | YpT¯0-1 = yT¯ -1, ST¯ = sT¯ , Z × Pr sp (T¯ )  ds(T¯ ) | YpT¯ -1 = yT¯ -1, sTp¯ -1 = sT¯ -1, Z

... × Pr Yp0 (2)  dy(2) | Yp0 (1) = y(1), S(1) = s(1), Z × Pr sp (2)  ds(2) | Yp (1) = y(1), sp (1) = s(1), Z × Pr Yp0 (1)  dy(1) | S(1) = s(1), Z Pr sp (1)  ds(1) | Z .

The support condition on sp requires that, for each t, the distribution of sp (t) | (Ypt-1 = yt-1, stp-1 = st-1, Z = z) is absolutely continuous with respect to the distribution of S(t) | (Ypt-0 1 = yt-1, St-1 = st-1, Z = z) for almost all (yt-1, st-1, z) from the distribution of (Ypt-0 1, St-1, Z). 28 In the general case, this condition becomes

Pr Yp  A|Z

=

Pr
sS

Yp0  A | S = s, Z

Pr

sp

 ds | Z

.

24 Dynamic Policy Analysis

811

Pr Yp = y|Z =  Pr Yp0 = y | S = s, Z Pr sp = s | Z . sS

24.3.2.2 Policy Choice and Optimal Policies
We now consider the problem of choosing a policy p that is optimal according to some criterion. This problem is both of normative interest and of descriptive interest if actual policies are chosen to be optimal. We could, for example, study the optimal assignment a of constraints and incentives to agents. Alternatively, we could assume that agents pick  to maximize their utilities, and use the methods discussed in this section to model .
Under the policy invariance assumptions that underlie the treatment-effect approach, p only affects outcomes through its implied treatment allocation sp. Thus, the problem of choosing an optimal policy boils down to choosing an optimal treatment allocation sp under informational and other constraints specific to the problem at hand. For example, suppose that the planner and the agents have the same information, IP(p) = IA(p), the planner assigns eligibility to a program by a, and agents fully comply, so that B = S and sp = a. Then, sp can be any rule from A and is adapted to IP(p) = IA(p).
For expositional convenience, we consider the optimal choice of a treatment assignment sp adapted to the agent's information IA(p) constructed earlier. We will use the word "agents" to refer to the decision maker in this problem, even though it can also apply to the planner's decision problem. An econometric approach to this problem is to estimate explicit dynamic choice models with explicit choiceoutcome relationships. One emphasis in the literature is on Markovian discretechoice models that satisfy Rust's (1987) conditional-independence assumption (see Rust, 1994). Other assumptions are made in the literature and we exposit them in Sect. 24.5.
Here, we explore the use of Rust's (1987) model as a model of treatment choice in a dynamic treatment-effect setting. In particular, we make explicit the additional structure that Rust's model, and in particular his conditional-independence assumption, imposes on Robins' dynamic potential-outcomes model. We follow Rust (1987) and focus on a finite treatment (control) space S . In the notation of our model, payoffs are determined by the outcomes Yp, treatment choices sp, the "cost shocks" V , and the covariates Z. Rust (1987) assumes that {Yp(t - 1),Vt, Z} is a controlled first-order Markov process, with initial condition Yp(0)  0 and control sp.29 As before, Vt and Z are not causally affected by choices, but Yp(t) may causally depend on current and past choices. The agents choose a treatment assignment rule sp that maximizes

29 Rust (1987) assumes an infinite-horizon, stationary environment. Here, we present a finitehorizon version to facilitate a comparison with the dynamic potential-outcomes model and to link up with the analysis in Sect. 24.5.

812

J.H. Abbring and J.J. Heckman

T¯
 E t {Yp(t - 1),Vt , sp(t), Z} + T¯+1{Yp(T¯ ), Z} IA(1) , t=1

(24.5)

for some (net and discounted) utility functions t and IA(1) = IA(1, p), which is independent of p. T¯+1{Yp(T¯ ), Z} is the terminal value. Under standard regularity conditions on the utility functions, we can solve backward for the optimal policy sp.
Because of Rust's Markov assumption, sp has a Markovian structure,

sp(t)(Ypt-2,V t-1) | [Yp(t - 1),Vt , Z] ,
for t = 2, . . . , T¯ , and {Yp(t - 1),Vt, Z} is a first-order Markov process. Note that Z enters the model as an observed (by the econometrician) factor that shifts net utility. A key assumption embodied in the specification of (24.5) is time-separability of utility. Rust (1987), in addition, imposes separability between observed and unobserved state variables. This assumption plays no essential role in expositing the core ideas in Rust, and we will not make it here.
Rust's (1987) conditional-independence assumption imposes two key restrictions on the decision problem. It is instructive to consider these restrictions in isolation from Rust's Markov restriction. We make the model's causal structure explicit using the potential-outcomes notation. Note that the model has a recursive causal structure--the payoff-relevant state is controlled by current and past choices only-- and satisfies no-anticipation condition (NA). Setting Y (0, s)  0 for specificity, and ignoring the Markov restriction, Rust's conditional-independence assumption requires, in addition to the assumption that there are no direct causal effects of choices on V , that

Y (s,t)V t | Y t-1(s), Z , and Vt+1V t | Y t (s), Z ,

(24.6) (24.7)

for all s and t. As noted by Rust (1987, p. 1011), condition (24.6) ensures that the observed (by the econometrician) controlled state evolves independently of the unobserved payoff-relevant variables. It is equivalent to (Florens and Mouchart, 1982)30
(M-3) [Y (s,t), . . . ,Y (s, T¯ )] V t | Y t-1(s), Z for all t and s.
In turn, (M-3) implies (M-1) and is equivalent to the assumption that (M-2) holds for all sp .31
Condition (24.7) excludes serial dependence of the unobserved payoff-relevant variables conditional on past outcomes. In contrast, Robins' g-computation framework allows for such serial dependence, provided that sequential randomization holds if serial dependence is present. For example, if V U|Z, then (M-1) and its variants hold without further assumptions on the time series structure of Vt .
30 Note that (24.6) is a Granger (1969) noncausality condition stating that, for all s and conditional on Z, V does not cause Y (s). 31 If V has redundant components, that is components that do not nontrivially enter any assignment rule sp, (M-3) imposes more structure, but structure that is irrelevant to the decision problem and its empirical analysis.

24 Dynamic Policy Analysis

813

The first-order Markov assumption imposes additional restrictions on potential outcomes. These restrictions are twofold. First, potential outcomes follow a firstorder Markov process. Second, s(t) only directly affects the Markov transition from Y (t, s) to Y (t + 1, s). This strengthens the no-anticipation assumption presented in Sect. 24.3.1.1. The Markov assumption also requires that Vt+1 only depends on Y (s,t), and not on Y t-1(s), given Y (s,t).
In applications, we may assume that actual treatment assignment S solves the Markovian decision problem. Together with specifications of t, this further restricts the dynamic choice-outcome model. Alternatively, one could make other assumptions on S and use (24.5) to define and find an optimal, and typically counterfactual, assignment rule sp .
Our analysis shows that the substantial econometric literature on the structural empirical analysis of Markovian decision problems under conditional independence can be applied to policy evaluation under sequential randomization. Conversely, methods developed for potential-outcomes models with sequential randomization can be applied to learn about aspects of dynamic discrete-choice models. Murphy (2003) develops methods to estimate an optimal treatment assignment rule using Robins' dynamic potential-outcomes model with sequential randomization (M-2).

24.3.3 The Information Structure of Policies
One concern about methods for policy evaluation based on the potential-outcomes model is that potential outcomes are sometimes reduced-form representations of dynamic models of agent's choices. A policy maker choosing optimal policies typically faces a population of agents who act on the available information, and their actions in turn affect potential outcomes. For example, in terms of the model of Sect. 24.3.2.2, a policy may change financial incentives--the b  B assigned through a could enter the net utilities t --and leave it to the agents to control outcomes by choosing treatment. In econometric policy evaluation, it is therefore important to carefully model the information IA that accumulates to the agents in different program states and under different policies, separately from the policy maker's information IP.
This can be contrasted with common practice in biostatistics. Statistical analyses of the effects of drugs on health are usually concerned with the physician's (planner's) information and decision problem. Gill and Robins' (2001) sequential randomization assumption, for example, is often justified by the assumption that physicians base their treatment decisions on observable (by the analyst) information only. This literature, however, often ignores the possibility that many variables known to the physician may not be known to the observing statistician and that the agents being given drugs alter the protocols.
Potential outcomes will often depend on the agent's information. Failure to correctly model the information will often lead to violation of (NA) and failure of invariance. Potential outcomes may therefore not be valid inputs in a policy evaluation

814

J.H. Abbring and J.J. Heckman

study. A naive specification of potential outcomes would only index treatments by actual participation in, e.g., job search assistance or training programs. Such a naive specification is incomplete in the context of economies inhabited by forwardlooking agents who make choices that affect outcomes. In specifying potential outcomes, we should not only consider the effects of actual program participation, but also the effects of the information available to agents about the program and policy. We now illustrate this point.
Example 24.2. Black, Smith, Berger, and Noel (2003) analyze the effect of compulsory training and employment services provided to unemployment insurance (UI) claimants in Kentucky on the exit rate from UI and earnings. In the program they study, letters are sent out to notify agents some time ahead whether they are selected to participate in the program. This information is recorded in a database and available to them. They can analyze the letter as part of a program that consists of information provision and subsequent participation in training. The main empirical finding of their paper is that the threat of future mandatory training conveyed by the letters is more effective in increasing the UI exit rate than training itself.
The data used by Black et al. (2003) are atypical of many economic data sets, because the data collectors carefully record the information provided to agents. This allows Black et al. to analyze the effects of the provision of information along with the effects of actual program participation. In many econometric applications, the information on the program under study is less rich. Data sets may provide information on actual participation in training programs and some background information on how the program is administered. Typically, however, the data do not record all of the letters sent to agents and do not record every phone conversation between administrators and agents. Then, the econometrician needs to make assumptions on how this information accumulates for agents. In many applications, knowledge of specific institutional mechanisms of assignment can be used to justify specific informational assumptions.
Example 24.3. Abbring, Van den Berg, and Van Ours (2005) analyze the effect of punitive benefits reductions, or sanctions, in Dutch UI on re-employment rates. In the Netherlands, UI claimants have to comply with certain rules concerning search behavior and registration. If a claimant violates these rules, a sanction may be applied. A sanction is a punitive reduction in benefits for some period of time and may be accompanied by increased levels of monitoring by the UI agency.32 Abbring et al. (2005) use administrative data and know the re-employment duration, the duration at which a sanction is imposed if a sanction is imposed, and some background characteristics for each UI case.
Without prior knowledge of the Dutch UI system, an analyst might make a variety of informational assumptions. One extreme is that UI claimants know at the start of their UI spells that their benefits will be reduced at some specific duration if they are still claiming UI at that duration. This results in a UI system with entitlement periods that are tailored to individual claimants and that are set and revealed
32 See Grubb (2000) for a review of sanction systems in the OECD.

24 Dynamic Policy Analysis

815

at the start of the UI spells. In this case, claimants will change their labor market behavior from the start of their UI spell in response to the future benefits reduction (e.g., Mortensen, 1977). At another extreme, claimants receive no prior signals of impending sanctions and there are no anticipatory effects of actual benefits reductions. However, agents may still be aware of the properties of the sanctions process and to some extent this will affect their behavior. Abbring et al. (2005) analyze a search model with these features. Abbring and Van den Berg (2003b) provide a structural example where the data cannot distinguish between these two informational assumptions. Abbring et al. (2005) use institutional background information to argue in favor of the second informational assumption as the one that characterizes their data.
If data on information provision are not available and simplifying assumptions on the program's information structure cannot be justified, the analyst needs to model the information that accumulates to agents as an unobserved determinant of outcomes. This is the approach followed, and further discussed, in Sect. 24.5.
The information determining outcomes typically includes aspects of the policy. In Example 24.2, the letter announcing future training will be interpreted differently in different policy environments. If agents are forward looking, the letter will be more informative under a policy that specifies a strong relation between the letter and mandatory training in the population than under a policy that allocates letters and training independently. In Example 24.3, the policy is a monitoring regime. Potential outcomes are UI durations under different sanction times. A change in monitoring policy changes the value of unemployment. In a job-search model with forward-looking agents, agents will respond by changing their search effort and reservation wages, and UI duration outcomes will change. In either example, potential outcomes are not invariant to variation in the policy. In the terminology of Hurwicz (1962), the policy is not "structural" with regard to potential outcomes and violates the invariance assumptions presented in Heckman and Vytlacil (2007a). One must control for the effects of agents' information.

24.3.4 Selection on Unobservables
In econometric program evaluations, (sequentially) randomized assignment is unlikely to hold. We illustrate this in the models developed in Sect. 24.5. Observational data are characterized by a lot of heterogeneity among agents, as documented by the empirical examples in Abbring and Heckman (2007) and in Heckman et al. (1999). This heterogeneity is unlikely to be fully captured by the observed variables in most data sets. In a dynamic context, such unmeasured heterogeneity leads to violations of the assumptions of Gill and Robins (2001) and Rust (1987) that choices represent a sequential randomization. This is true even if the unmeasured variables only affect the availability of slots in programs but not outcomes directly. If agents are rational, forward-looking and observe at least some of the unmeasured variables that the econometrician does not, they will typically respond to these variables through their

816

J.H. Abbring and J.J. Heckman

choice of treatment and investment behavior. In this case, the sequential randomization condition fails.
For the same reason, identification based on instrumental variables is relatively hard to justify in dynamic models (Hansen and Sargent, 1980; Rosenzweig and Wolpin, 2000; Abbring and Van den Berg, 2005). If the candidate instruments only vary across persons but not over time for the same person, then they are not likely to be valid instruments because they affect expectations and future choices and may affect current potential outcomes. Instead of using instrumental variables that vary only across persons, we require instruments based on unanticipated person-specific shocks that affect treatment choices but not outcomes at each point in time. In the context of continuously assigned treatments, the implied data requirements seem onerous. To achieve identification, Abbring and Van den Berg (2003b) focus on regressor variation rather than exclusion restrictions in a sufficiently smooth model of continuous-time treatment effects. We discuss their analysis in Sect. 24.4. Heckman and Navarro (2007) show that curvature conditions, not exclusion restrictions, that result in the same variables having different effects on choices and outcomes in different periods, are motivated by economic theory and can be exploited to identify dynamic treatment effects in discrete time without literally excluding any variables. We discuss their analysis in Sect. 24.5. We now consider a formulation of the analysis in continuous time.

24.4 The Event-History Approach to Policy Analysis
The discrete-time models just discussed in Sect. 24.3 have an obvious limitation. Time is continuous and many events are best described by a continuous-time model. There is a rich field of continuous-time event-history analysis that has been adapted to conduct policy evaluation analysis.33 For example, the effects of training and counseling on unemployment durations and job stability have been analyzed by applying event-history methods to data on individual labor-market and training histories (Ridder, 1986; Card and Sullivan, 1988; Gritz, 1993; Ham and LaLonde, 1996; Eberwein, Ham, and LaLonde, 1997; Bonnal, Fouge`re, and Se´randon, 1997). Similarly, the moral hazard effects of unemployment insurance have been studied by analyzing the effects of time-varying benefits on labor-market transitions (e.g., Meyer, 1990; Abbring et al., 2005; Van den Berg, Van der Klaauw, and Van Ours, 2004). In fields like epidemiology, the use of event-history models to analyze treatment effects is widespread (see, e.g., Andersen, Borgan, Gill, and Keiding, 1993; Keiding, 1999).
The event-history approach to program evaluation is firmly rooted in the econometric literature on state dependence (lagged dependent variables) and heterogeneity (Heckman and Borjas, 1980; and Heckman, 1981a). Event-history models along
33 Abbring and Van den Berg (2004) discuss the relation between the event-history approach to program evaluation and more standard latent-variable and panel-data methods, with a focus on identification issues.

24 Dynamic Policy Analysis

817

the lines of Heckman and Singer (1984, 1986) are used to jointly model transitions into programs and transitions into outcome states. Causal effects of programs are modelled as the dependence of individual transition rates on the individual history of program participation. Dynamic selection effects are modelled by allowing for dependent unobserved heterogeneity in both the program and outcome transition rates.
Without restrictions on the class of models considered, true state dependence and dynamic selection effects cannot be distinguished.34 Any history dependence of current transition rates can be explained both as true state dependence and as the result of unobserved heterogeneity that simultaneously affects the history and current transitions. This is a dynamic manifestation of the problem of drawing causal inference from observational data. In applied work, researchers avoid this problem by imposing additional structure. A typical, simple, example is a mixed semi-Markov model in which the causal effects are restricted to program participation in the previous spell (e.g., Bonnal et al. 1997). There is a substantial literature on the identifiability of state-dependence effects and heterogeneity in duration and event-history models that exploit such additional structure (see Heckman and Taber, 1994; and Van den Berg, 2001 for reviews). Here, we provide discussion of some canonical cases.

24.4.1 Treatment Effects in Duration Models
24.4.1.1 Dynamically Assigned Binary Treatments and Duration Outcomes
We first consider the simplest case of mutual dependence of events in continuous time, involving only two binary events. This case is sufficiently rich to capture the effect of a dynamically assigned binary treatment on a duration outcome. Binary events in continuous time can be fully characterized by the time at which they occur and a structural model for their joint determination is a simultaneous-equations model for durations. We develop such a model along the lines of Abbring and Van den Berg (2003b). This model is an extension, with general marginal distributions and general causal and spurious dependence of the durations, of Freund's (1961) bivariate exponential model.
Consider two continuously-distributed random durations Y and S. We refer to one of the durations, S, as the time to treatment and to the other duration, Y , as the outcome duration. Such an asymmetry arises naturally in many applications. For example, in Abbring et al.'s (2005) study of unemployment insurance, the treatment is a punitive benefits reduction (sanction) and the outcome re-employment. The re-employment process continues after imposition of a sanction, but the sanctions process is terminated by re-employment. The current exposition, however, is symmetric and unifies both cases. It applies to both the asymmetric setup of the sanctions example and to applications in which both events may causally affect the other event.
34 See Heckman and Singer (1986).

818

J.H. Abbring and J.J. Heckman

Let Y (s) be the potential outcome duration that would prevail if the treatment

time is externally set to s. Similarly, let S(y) be the potential treatment time result-

ing from setting the outcome duration to y. We assume that ex ante heterogeneity

across agents is fully captured by observed covariates X and unobserved covariates

V , assumed to be external and temporally invariant. Treatment causally affects the

outcome duration through its hazard rate. We denote the hazard rate of Y (s) at time

t for an agent with characteristics (X,V ) by Y (t|s, X,V ). Similarly, outcomes affect the treatment times through its hazard S(t|y, X,V ). Causal effects on hazard

rates are produced by recursive economic models driven by point processes, such as

search models. We provide an example below, and further discussion in Sect. 24.4.3.

Without loss of generality, we partition V into (VS,VY ) and assume that Y (t|s, X,V ) = Y (t|s, X,VY ) and S(t|y, X,V ) = S(t|y, X,VS). Intuitively, VS and VY

are the unobservables affecting, respectively, treatment and outcome, and the joint

distribution of (VS,VY ) is unrestricted. In particular, VS and VY may have elements

in common.

The corresponding integrated hazard rates are defined by Y (t | s, X,VY ) =

t 0

Y

(u

|

s,

X

,VY

)du

and

S(t

|

y,

X

,

VS)

=

t 0

S

(u

|

y,

X

,

VS

)du.

For

expositional

con-

venience, we assume that these integrated hazards are strictly increasing in t. We

also assume that they diverge to  as t  , so that the duration distributions are
non-defective.35 Then, Y (Y (s) | s, X,VY ) and S(S(y) | y, X,VS) are unit exponential for all y, s  R+.36 This implies the following model of potential outcomes and treatments,37

Y (s) = y(s, X,VY , Y ) and S(y) = s(y, X,VS, S) ,
for some unit exponential random variables Y and S that are independent of (X,V ), y = Y-1, and s = -S 1.
The exponential errors Y and S embody the ex post shocks that are inherent to the individual hazard processes, that is the randomness in the transition process after conditioning on covariates X and V and survival. We assume that Y S, so that {Y (s)} and {S(y)} are only dependent through the observed and unobserved covariates (X,V ). This conditional-independence assumption is weaker than the conditional-independence assumption underlying the analysis of Sect. 24.3 and used

35 Abbring and Van den Berg (2003b) allow for defective distributions, which often have structural

interpretations. For example, some women never have children and some workers will never leave

a job. See Abbring (2002) for discussion.

36 Let T | X be distributed with density f (t|X), non-defective cumulative distribution function

F(t|X), and hazard rate  (t|X) = f (t|X)/[1 - F(t|X)]. Then,

T 0



(t|X

)dt

=

-

ln[1

-

F

(T

|X )]

is

a

unit exponential random variable that is independent of X.

37 The causal hazard model only implies that the distributions of Y and S are invariant across assigned treatments and outcomes, respectively; their realizations may not be. This is sufficient for

the variation of y(s, X,VY , Y ) with s and of s(y, X,VS, S) with y to have a causal interpretation. The further restriction that the random variables Y and S are invariant is made for simplicity, and
is empirically innocuous. See Abbring and Van den Berg (2003b) for details and Freedman (2004)

for discussion.

24 Dynamic Policy Analysis

819

in matching, because it allows for conditioning on the invariant unobservables V . It shares this feature with the discrete-time models developed in Sect. 24.5.
We assume a version of the no-anticipation condition of Sect. 24.3.1.1: for all t  R+,
Y (t|s, X,VY ) = Y (t|s , X,VY ) and S(t|y, X,VS) = S(t|y , X,VS)
for all s, s , y, y  [t, ). This excludes effects of anticipation of the treatment on the outcome. Similarly, there can be no anticipation effects of future outcomes on the treatment time hazard.

Example 24.4. Consider a standard search model describing the job search behavior of an unemployed individual (e.g., Mortensen, 1986) with characteristics (X,V ). Job offers arrive at a rate  > 0 and are random draws from a given distribution F. Both  and F may depend on (X,V ), but, for notational simplicity we suppress all explicit representations of conditioning on (X,V ) throughout this example. An offer is either accepted or rejected. A rejected offer cannot be recalled at a later time. The individual initially receives a constant flow of unemployment-insurance benefits. However, the individual faces the risk of a sanction--a permanent reduction of his benefits to some lower, constant level--at some point during his unemployment spell. During the unemployment spell, sanctions arrive independently of the joboffer process at a constant rate  > 0. The individual cannot foresee the exact time a sanction is imposed, but he knows the distribution of these times.38 The individual chooses a job-acceptance rule so as to maximize his expected discounted lifetime income. Under standard conditions, this is a reservation-wage rule: at time t, the individual accepts each wage of w(t) or higher. The corresponding re-employment hazard rate is  (1 - F(w(t))). Apart from the sanction, which is not foreseen and arrives at a constant rate during the unemployment spell, the model is stationary. This implies that the reservation wage is constant, say equal to w0, up to and including time s, jumps to some lower level w1 < w0 at time s and stays constant at w1 for the remainder of the unemployment spell if benefits would be reduced at time s.
The model is a version of the simultaneous-equations model for durations. To see this, let Y be the re-employment duration and S the sanction time. The potentialoutcome hazards are

Y (t|s) =

0 if 0  t  s 1 if t > s ,

where 0 =  [1 - F(w0)] and 1 =  [1 - F(w1)], and clearly 1  0. Similarly, the potential-treatment time hazards are S(t|y) =  if 0  t  y, and 0 otherwise. Note that the no-anticipation condition follows naturally from the recursive structure
of the economic decision problem in this case, in which we have properly accounted
for all relevant components of agent information sets. Furthermore, the assumed

38 This is a rudimentary version of the search model with punitive benefits reductions, or sanctions, of Abbring et al. (2005). The main difference is that in the present version of the model the sanctions process cannot be controlled by the agent.

820

J.H. Abbring and J.J. Heckman

independence of the job offer and sanction processes at the individual level for given (X,V ) implies that Y S.
The actual outcome and treatment are related to the potential outcomes and treatments by S = S(Y ) and Y = Y (S). The no-anticipation assumption ensures that this system has a unique solution (Y, S) by imposing a recursive structure on the underlying transition processes. Without anticipation effects, current treatment and outcome hazards only depend on past outcome and treatment events, and the transition processes evolve recursively (Abbring and Van den Berg, 2003b). Together with a distribution G(· | X) of V | X, this gives a non-parametric structural model of the distribution of (Y, S) | X that embodies general simultaneous causal dependence of Y and S, dependence of (Y, X) on observed covariates X, and general dependence of the unobserved errors VY and VS.
There are two reasons for imposing further restrictions on this model. First, it is not identified from data on (Y, S, X). Take a version of the model with selection on unobservables (VY / VS | X) and consider the distribution of (Y, S)|X generated by this version of the model. Then, there exists an alternative version of the model that satisfies both no-anticipation and VY VS | X, and that generates the same distribution of (Y, S)|X (Abbring and Van den Berg, 2003b, Proposition 1). In other words, for each version of the model with selection on unobservables and anticipation effects, there is an observationally-equivalent model version that satisfies no-anticipation and conditional randomization. This is a version of the nonidentification result discussed in Sect. 24.3.1.
Second, even if we ensure nonparametric identification by assuming noanticipation and conditional randomization, we cannot learn about the agent-level causal effects embodied in y and s without imposing even further restrictions. At best, under regularity conditions we can identify Y (t|s, X) = E[Y (t|s, X,VY ) |X,Y (s)  t] and S(t|y, X) = E[S(t|y, X,VS)|X, S(y)  t] from standard hazard regressions (e.g., Andersen et al., 1993; Fleming and Harrington, 1991). Thus, we can identify the distributions of Y (s)|X and S(y)|X, and therefore solve the selection problem if we are only interested in these distributions. However, if we are also interested in the causal effects on the corresponding hazard rates for given X,V , we face an additional dynamic selection problem. The hazards of the identified distributions of Y (s)|X and S(y)|X only condition on observed covariates X, and not on unobserved covariates V , and are confounded with dynamic selection effects (Heckman and Borjas, 1980; Heckman and Singer, 1986; Meyer, 1996; Abbring and Van den Berg, 2005). For example, the difference between Y (t|s, X) and Y (t|s , X) does not only reflect agent-level differences between Y (t|s, X,VY ) and Y (t|s , X,VY ), but also differences in the subpopulations of survivors {X,Y (s)  t} and {X,Y (s )  t} on which the hazards are computed.
In the next two subsections, we discuss what can be learned about treatment effects in duration models under additional model restrictions. We take the noanticipation assumption as fundamental. As explained in Sect. 24.3, this requires that we measure and include in our model all relevant information needed to define potential outcomes. However, we relax the randomization assumption. We first consider Abbring and Van den Berg's (2003b) analysis of identification without

24 Dynamic Policy Analysis

821

exclusion restrictions. They argue that these results are useful, because exclusion restrictions are hard to justify in an inherently dynamic setting with forward-looking agents. Abbring and Van den Berg (2005) further clarify this issue by studying inference for treatment effects in duration models using a social experiment. We discuss what can be learned from such experiments at the end of this section.

24.4.1.2 Identifiability Without Exclusion Restrictions

Abbring and Van den Berg consider an extension of the multivariate Mixed Proportional Hazard (MPH) model (Lancaster, 1979) in which the hazard rates of Y (s) | (X,V ) and S(y) | (X,V ) are given by

Y (t | s, X,V ) = and
S(t | y, X,V ) =

Y (t)Y (X)VY Y (t)Y (X)Y (t, s, X)VY
S (t )S (X )VS S(t)S(X)S(t, y, X)VS

if t  s if t > s
if t  y if t > y,

(24.8) (24.9)

respectively, and V = (VS,VY ) is distributed independently of X. The baseline haz-

ards Y : R+  (0, ) and S : R+  (0, ) capture duration dependence of the

inSd(itv)id=ual0ttraSn(si)tdion<rate,s.foTrhaellinttegrRa+te.dThhaezarredgsreasrseor Yfu(nt)ct=ion0st

Y ( Y :

)d X

< 

 and (0, )

and S : X  (0, ) are assumed to be continuous, with X  Rq the support of

X. In empirical work, these functions are frequently specified as Y (x) = exp(x Y )

and S(x) = exp(x S) for some parameter vectors Y and S. We will not make

such parametric assumptions. Note that the fact that both regressor functions are

defined on the same domain X is not restrictive, because each function Y and S can "select" certain elements of X by being trivial functions of the other elements. In the parametric example, the vector Y would only have nonzero elements for those regressors that matter to the outcome hazard. The functions Y and S capture the causal effects. Note that Y (t, s, X) only enters Y (t | s, X,V ) at du-
rations t > s, so that the model satisfies no anticipation of treatment assumption

(NA). Similarly, it satisfies no anticipation of outcomes and has a recursive causal

structure as required by the no-anticipation assumption. If Y = 1, treatment is ineffective; if Y is larger than 1, it stochastically reduces the remaining outcome

duration.

Note that this model allows Y and S to depend on elapsed duration t, past

endogenous events, and the observed covariates X, but not on V . Abbring and Van

den Berg also consider an alternative model that allows Y and S to depend on

unobservables in a general way, but not on past endogenous events.

Abbring and Van den Berg show that these models are nonparametrically identi-

fied from single-spell data under the conditions for the identification of competing-

risks models based on the multivariate MPH model given by Abbring and Van den

Berg (2003a). Among other conditions are the requirements that there is some

822

J.H. Abbring and J.J. Heckman

independent local variation of the regressor effects in both hazard rates and a finitemean restriction on V , which are standard in the analysis of multivariate MPH models. With multiple-spell data, most of these assumptions, and the MPH structure, can be relaxed (Abbring and Van den Berg, 2003b).
The models can be parameterized in a flexible way and estimated by maximum likelihood. Typical parameterizations involve linear-index structures for the regressor and causal effects, a discrete distribution G, and piecewise-constant baseline hazards S and Y . Abbring and Van den Berg (2003c) develop a simple graphical method for inference on the sign of ln(Y ) in the absence of regressors. Abbring et al. (2005) present an empirical application.

24.4.1.3 Inference Based on Instrumental Variables
The concerns expressed in Sect. 24.3.4 about the validity of exclusion restrictions in dynamic settings carry over to event-history models.
Example 24.5. A good illustration of this point is offered by the analysis of Eberwein et al. (1997), who study the effects of a training program on labor-market transitions. Their data are particularly nice, as potential participants are randomized into treatment and control groups at some baseline point in time. This allows them to estimate the effect of intention to treat (with training) on subsequent labor-market transitions. This is directly relevant to policy evaluation in the case that the policy involves changing training enrollment through offers of treatment which may or may not be accepted by agents.
However, Eberwein et al. are also interested in the effect of actual participation in the training program on post program labor-market transitions. This is a distinct problem, because compliance with the intention-to-treat protocol is imperfect. Some agents in the control group are able to enroll in substitute programs, and some agents in the treatment group choose never to enroll in a program at all. Moreover, actual enrollment does not take place at the baseline time, but is dispersed over time. Those in the treatment group are more likely to enroll earlier. This fact, coupled with the initial randomization, suggests that the intention-to-treat indicator might be used as an instrument for identifying the effect of program participation on employment and unemployment spells.
The dynamic nature of enrollment into the training program, and the eventhistory focus of the analysis complicate matters considerably. Standard instrumentalvariables methods cannot be directly applied. Instead, Eberwein et al. use a parametric duration model for pre and post program outcomes that excludes the intention-to-treat indicator from directly determining outcomes. They specify a duration model for training enrollment that includes an intention-to-treat indicator as an explanatory variable, and specify a model for labor-market transitions that excludes the intention-to-treat indicator and imposes a no-anticipation condition on the effect of actual training participation on labor-market transitions. Such a model is consistent with an environment in which agents cannot perfectly foresee the actual training time they will be assigned and in which they do not respond to information

24 Dynamic Policy Analysis

823

about this time revealed by their assignment to an intention-to-treat group. This is a strong assumption. In a search model with forward-looking agents, for example, such information would typically affect the ex ante values of unemployment and employment. Then, it would affect the labor-market transitions before actual training enrollment through changes in search efforts and reservation wages, unless these are both assumed to be exogenous. An assumption of perfect foresight on the part of the agents being studied only complicates matters further.

Abbring and Van den Berg (2005) study what can be learned about dynamically assigned programs from social experiments if the intention-to-treat instrument cannot be excluded from the outcome equation. They develop bounds, tests for unobserved heterogeneity, and point-identification results that extend those discussed in this section.39

24.4.2 Treatment Effects in More General Event-History Models
It is instructive to place the causal duration models developed in Sect. 24.4.1 in the more general setting of event-history models with state dependence and heterogeneity. We do this following Abbring's (2008) analysis of the mixed semi-Markov model.
24.4.2.1 The Mixed Semi-Markov Event-History Model
The model is formulated in a fashion that is analogous to the frameworks of Heckman and Singer (1986). The point of departure is a continuous-time stochastic process assuming values in a finite set S at each point in time. We will interpret realizations of this process as agents' event histories of transitions between states in the state space S .
Suppose that event histories start at real-valued random times T0 in an S -valued random state S0, and that subsequent transitions occur at random times T1, T2, . . . such that T0 < T1 < T2 < · · · . Let Sl be the random destination state of the transition at Tl. Taking the sample paths of the event-history process to be right-continuous, we have that Sl is the state occupied in the interval [Tl, Tl+1).
Suppose that heterogeneity among agents is captured by vectors of time-constant observed covariates X and unobserved covariates V .40 In this case, state dependence in the event-history process for given individual characteristics X,V has a causal
39 In the special case that a static treatment, or treatment plan, is assigned at the start of the spell, standard instrumental-variables methods may be applied. See Abbring and Van den Berg (2005). 40 We restrict attention to time-invariant observed covariates for expositional convenience. The analysis can easily be adapted to more general time-varying external covariates. Restricting attention to time-constant regressors is a worst-case scenario for identification. External time variation in observed covariates aids identification (Heckman and Taber, 1994).

824

J.H. Abbring and J.J. Heckman

interpretation.41 We structure such state dependence by assuming that the event-
history process conditional on X,V is a time-homogeneous semi-Markov process.
Conditional on X,V the length of a spell in a state and the destination state of the
transition ending that spell depend only on the past through the current state. In our notation, (Tl, Sl){(Ti, Si), i = 0, . . . , l - 1} | Sl-1, X,V , where Tl = Tl - Tl-1 is the length of spell l. Also, the distribution of (Tl, Sl)|Sl-1, X,V does not depend on l. Note that, conditional on X,V , {Sl, l  0} is a time-homogeneous Markov chain under these assumptions.
Non-trivial dynamic selection effects arise because V is not observed. The event-
history process conditional on observed covariates X only is a mixed semi-Markov
process. If V affects the initial state S0, or transitions from it, subpopulations of agents in different states at some time t typically have different distributions of the
unobserved characteristics V . Therefore, a comparison of the subsequent transitions
in two such subpopulations does not only reflect state dependence, but also sort-
ing of agents with different unobserved characteristics into the different states they
occupy at time t. We model {(Tl, Sl), l  1}|T0, S0, X,V as a repeated competing-risks model.
Due to the mixed semi-Markov assumption, the latent durations corresponding to
transitions into the possible destination states in the lth spell only depend on the
past through the current state Sl-1, conditional on X,V . This implies that we can fully specify the repeated competing-risks model by specifying a set of origindestination-specific latent durations, with corresponding transition rates. Let Tjlk denote the latent duration corresponding to the transition from state j to state k in
spell l. We explicitly allow for the possibility that transitions between certain (or-
dered) pairs of states may be impossible. To this end, define the correspondence Q : S   (S ) assigning to each s  S the set of all destination states to which transitions are made from s with positive probability.42 Here,  (S ) is the set of all subsets of S (the "power set" of S ). Then, the length of spell l is given by Tl = minsQ(Sl-1) TSll-1s, and the destination state by Sl = arg minsQ(Sl-1) TSll-1s.
We take the latent durations to be mutually independent, jointly independent
of T0, S0, and identically distributed across spells l, all conditional on X,V . This reflects both the mixed semi-Markov assumption and the additional assumption
that all dependence between the latent durations corresponding to the compet-
ing risks in a given spell l is captured by the observed regressors X and the un-
observables V . This is a standard assumption in econometric duration analysis,
which, with the semi-Markov assumption, allows us to characterize the distribution of {(Tl, Sl), l  1}|T0, S0, X,V by specifying origin-destination-specific hazards  jk(t|X,V ) for the marginal distributions of Tjlk|X,V .

41 We could make this explicit by extending the potential-outcomes model of Sect. 24.4.1.2 to the general event-history setup. However, this would add a lot of complexity, but little extra insight. 42 Throughout this section, we assume that Q is known. It is important to note, however, that Q can actually be identified trivially in all cases considered.

24 Dynamic Policy Analysis

825

We assume that the hazards  jk(t|X,V ) are of the mixed proportional hazard (MPH) type:43

 jk(t|X,V ) =

 jk(t) jk(X )Vjk 0

if k  Q( j) otherwise.

(24.10)

The baseline hazards  jk : R+  (0, ) have integrated hazards  jk(t) =

t 0



jk

(

)d

< , for all t  R+. The regressor functions  jk : X  (0, ) are assumed to be

continuous. Finally, the (0, )-valued random variable Vjk is the scalar component

of V that affects the transition from state j to state k. Note that we allow for general

dependence between the components of V . This way, we can capture, for example,

that agents with lower re-employment rates have higher training enrolment rates.

This model fully characterizes the distribution of the transitions {(Tl, Sl), l  1} conditional on the initial conditions T0, S0 and the agent's characteristics X,V . A complete model of the event histories {(Tl, Sl), l  0} conditional on X,V would in addition require a specification of the initial conditions T0, S0 for given X,V . It

is important to stress here that T0, S0 are the initial conditions of the event-history

process itself, and should not be confused with the initial conditions in a particular

sample (which we will discuss below). In empirical work, interest in the dependence

between start times T0 and characteristics X,V is often limited to the observation that

the distribution of agents' characteristics may vary over cohorts indexed by T0. The

choice of initial state S0 may in general be of some interest, but is often trivial. For example, we could model labor-market histories from the calendar time T0 at which

agents turn 15 onwards. In an economy with perfect compliance to a mandatory

schooling up to age 15, the initial state S0 would be "(mandatory) schooling" for all.

Therefore, we will not consider a model of the event history's initial conditions, but

instead focus on the conditional model of subsequent transition histories.

Because of the semi-Markov assumption, the distribution of {(Tl, Sl), l  1}| T0, S0, X,V only depends on S0, and not T0. Thus, T0 only affects observed event histories through cohort effects on the distribution of unobserved characteristics V .

The initial state S0, on the other hand, may both have causal effects on subsequent

transitions and be informative on the distribution of V . For expositional clarity, we

assume that V (T0, S0, X). This is true, for example, if all agents start in the same

state, so that S0 is degenerate, and V is independent of the start date T0 and the

observed covariates X.

An econometric model for transition histories conditional on the observed covari-

ates X can be derived from the model of {(Tl, Sl), l  1}|S0, X,V by integrating out

V . The exact way this should be done depends on the sampling scheme used. Here,

we focus on sampling from the population of event-histories. We assume that we observe the covariates X, the initial state S0, and the first L¯ transitions from there.

Then, we can model these transitions for given S0, X by integrating the conditional

model over the distribution of V .

43 Proportionality can be relaxed if we have data on sufficiently long event-histories. See Honore´ (1993) and Abbring and Van den Berg (2003a,b) for related arguments for various multi-spell duration models.

826

J.H. Abbring and J.J. Heckman

Abbring (2008) discusses more complex, and arguably more realistic, sampling schemes. For example, when studying labor-market histories we may randomly sample from the stock of the unemployed at a particular point in time. Because the unobserved component V affects the probability of being unemployed at the sampling date, the distribution of V |X in the stock sample does not equal its population distribution. This is again a dynamic version of the selection problem. Moreover, in this case, we typically do not observe an agent's entire labor-market history from T0 onwards. Instead, we may have data on the time spent in unemployment at the sampling date and on labor-market transitions for some period after the sampling date. This "initial-conditions problem" complicates matters further (Heckman, 1981b).
In the next two subsections, we first discuss some examples of applications of the model and then review a basic identification result for the simple sampling scheme above.

24.4.2.2 Applications to Program Evaluation

Several empirical papers study the effect of a single treatment on some outcome

duration or set of transitions. Two approaches can be distinguished. In the first ap-

proach, the outcome and treatment processes are explicitly and separately specified.

The second approach distinguishes treatment as one state within a single event-

history model with state dependence.

The first approach is used in a variety of papers in labor economics. Eberwein

et al. (1997) specify a model for labor market transitions in which the transition

intensities between various labor market states (not including treatment) depend on

whether someone has been assigned to a training program in the past or not. Abbring

et al. (2005) and Van den Berg et al. (2004) specify a model for re-employment

durations in which the re-employment hazard depends on whether a punitive ben-

efits reduction has been imposed in the past. Similarly, Van den Berg, Holm, and

Van Ours (2002) analyze the duration up to transition into medical trainee posi-

tions and the effect of an intermediate transition into a medical assistant position (a

"stepping-stone job") on this duration. In all of these papers, the outcome model is

complemented with a hazard model for treatment choice.

These models fit into the framework of Sect. 24.4.1.2 or a multi-state exten-

sion thereof. We can rephrase the class of models discussed in Sect. 24.4.1.2 in

terms of a simple event-history model with state-dependence as follows. Distin-

guish three states, untreated (O), treated (P) and the exit state of interest (E), so that

S = {O, P, E}. All subjects start in O, so that S0 = O. Obviously, we do not want to allow for all possible transitions between these three states. Instead, we restrict the

correspondence Q representing the possible transitions as follows:

  {P, E}

s = O,

Q(s)

=



{E } 0/

if s = P, s = E.

24 Dynamic Policy Analysis

827

State dependence of the transition rates into E captures treatment effects in the sense of Sect. 24.4.1.2. Not all models in Abbring and Van den Berg (2003b) are included in the semi-Markov setup discussed here. In particular, in this paper we do not allow the transition rate from P to E to depend on the duration spent in O. This extension with "lagged duration dependence" (Heckman and Borjas, 1980) would be required to capture one variant of their model.
The model for transitions from "untreated" (O) is a competing risks model, with program enrolment (transition to P) and employment (E) competing to end the untreated spell. If the unobservable factor VOE that determines transitions to employment and the unobservable factor VOP affecting program enrolment are dependent, then program enrolment is selective in the sense that the initial distribution of VOE -- and also typically that of VPE --among those who enroll at a given point in time does not equal its distribution among survivors in O up to that time.44
The second approach is used by Gritz (1993) and Bonnal et al. (1997), among others. Consider the following simplified setup. Suppose workers are either employed (E), unemployed (O), or engaged in a training program (P). We can now specify a transition process among these three labor market states in which a causal effect of training on unemployment and employment durations is modeled as dependence of the various transition rates on the past occurrence of a training program in the labor market history. Bonnal et al. (1997) only have limited information on agents' labor-market histories before the sample period. Partly to avoid difficult initial-conditions problems, they restrict attention to "first order lagged occurrence dependence" (Heckman and Borjas, 1980) by assuming that transition rates only depend on the current and previous states occupied. Such a model is not directly covered by the semi-Markov model, but with a simple augmentation of the state space it can be covered. In particular, we have to include lagged states in the state space on which the transition process is defined. Because there is no lagged state in the event-history's first spell, initial states should be defined separately. So, instead of just distinguishing states in S  = {E, O, P}, we distinguish augmented states in S = {(s, s )  (S   {I}) × S  : s = s }. Then, (I, s), s  S , denote the initial states, and (s, s )  S the augmented state of an agent who is currently in s and came from s = s . In order to preserve the interpretation of the model as a model of lagged occurrence dependence, we have to exclude certain transitions by specifying
Q(s, s ) = {(s , s ), s  S \{s }} .

This excludes transitions to augmented states that are labeled with a lagged state different from the origin state. Also, it ensures that agents never return to an initial state. For example, from the augmented state (O, P)--previously unemployed and currently enrolled in a program--only transitions to augmented states (P, s )-- previously enrolled in a program and currently in s --are possible. Moreover, it is not possible to be currently employed and transiting to initially unemployed, (I, O).

44 Note that, in addition, the survivors in O themselves are a selected subpopulation. Because V affects survival in O, the distribution of V among survivors in O is not equal to its population distribution.

828

J.H. Abbring and J.J. Heckman

Rather, an employed person who loses her job would transit to (E, O)--currently unemployed and previously employed.
The effects of training, for example, are now modeled as simple state-dependence effects. For example, the effect of training on the transition rate from unemployment to employment is simply the contrast between the individual transition rate from (E, O) to (O, E) and the transition rate from (P, O) to (O, E). Dynamic selection into the augmented states (E, O) and (P, O), as specified by the transition model, confounds the empirical analysis of these training effects. Note that due to the fact that we have restricted attention to first-order lagged occurrence dependence, there are no longer-run effects of training on transition rates from unemployment to employment.

24.4.2.3 Identification Without Exclusion Restrictions
In this section, we state a basic identification result for the following sampling scheme. Suppose that the economist randomly samples from the population of event-histories, and that we observe the first L¯ transitions (including destinations) for each sampled event-history, with the possibility that L¯ = .45 Thus, we observe a random sample of {(Tl, Sl), l  {0, 1, . . . , L¯ }}, and X.
First note that we can only identify the determinants of  jk for transitions ( j, k) that occur with positive probability among the first L¯ transitions. Moreover, without further restrictions, we can only identify the joint distribution of a vector of unobservables corresponding to (part of) a sequence of transitions that can be observed among the first L¯ transitions.
With this qualification, identification can be proved by extending Abbring and Van den Berg's (2003a) analysis of the MPH competing risks model to the present setting. This analysis assumes that transition rates have an MPH functional form. Identification again requires specific moments of V to be finite, and independent local variation in the regressor effects.

24.4.3 A Structural Perspective
Without further restrictions, the causal duration model of Sect. 24.4.1.1 is versatile. It can be generated as the reduced form of a wide variety of continuous-time economic models driven by point processes. Leading examples are sequential jobsearch models in which job-offer arrival rates, and other model parameters, depend on agent characteristics (X,V ) and policy interventions (see, e.g., Mortensen, 1986, and Example 24.4).
The MPH restriction on this model, however, is hard to justify from economic theory. In particular, nonstationary job-search models often imply interactions
45 Note that this assumes away econometric initial-conditions problems of the type previously discussed.

24 Dynamic Policy Analysis

829

between duration and covariate effects; the MPH model only results under strong assumptions (Heckman and Singer, 1986; Van den Berg, 2001). Similarly, an MPH structure is hard to generate from models in which agents learn about their individual value of the model's structural parameters, that is about (X,V ), through Bayesian updating.
An alternative class of continuous-time models, not discussed in this chapter, specifies durations as the first time some Gaussian or more general process crosses a threshold. Such models are closely related to a variety of dynamic economic models. They have attracted recent attention in statistics (see, e.g., Aalen and Gjessing, 2004). Abbring (2007) analyzes identifiability of "mixed hitting-time models", continuous-time threshold-crossing models in which the parameters depend on observed and unobserved covariates, and discusses their link with optimizing models in economics. This is a relatively new area of research, and a full development is beyond the scope of this paper. It extends to a continuous-time framework the dynamic threshold crossing model developed in Heckman (1981a,b) that is used in the next subsection of this chapter.
We now discuss a complementary discrete-time approach where it is possible to make many important economic distinctions that are difficult to make in the setting of continuous-time models and to avoid some difficult measure-theoretic problems. In the structural version, it is possible to specify precisely agent information sets in a fashion that is not possible in conventional duration models.

24.5 Dynamic Discrete Choice and Dynamic Treatment Effects
Heckman and Navarro (2007) and Cunha, Heckman, and Navarro (2007) present econometric models for analyzing time to treatment and the consequences of the choice of a particular treatment time. Treatment may be a medical intervention, stopping schooling, opening a store, conducting an advertising campaign at a given date or renewing a patent. Associated with each treatment time, there can be multiple outcomes. They can include a vector of health status indicators and biomarkers; lifetime employment and earnings consequences of stopping at a particular grade of schooling; the sales revenue and profit generated from opening a store at a certain time; the revenues generated and market penetration gained from an advertising campaign; or the value of exercising an option at a given time. Heckman and Navarro (2007) unite and contribute to the literatures on dynamic discrete choice and dynamic treatment effects. For both classes of models, they present semiparametric identification analyses. We summarize their work in this section. It is formulated in discrete time, which facilitates the specification of richer unobserved and observed covariate processes than those entertained in the continuous-time framework of Abbring and Van den Berg (2003b).
Heckman and Navarro extend the literature on treatment effects to model choices of treatment times and the consequences of choice and link the literature on treatment effects to the literature on precisely formulated structural dynamic

830

J.H. Abbring and J.J. Heckman

discrete-choice models generated from index models crossing thresholds. They show the value of precisely formulated economic models in extracting the information sets of agents, in providing model identification, in generating the standard treatment effects and in enforcing the nonanticipating behavior condition (NA) discussed in Sect. 24.3.1.46
They establish the semiparametric identifiability of a class of dynamic discretechoice models for stopping times and associated outcomes in which agents sequentially update the information on which they act. They also establish identifiability of a new class of reduced-form duration models that generalize conventional discrete-time duration models to produce frameworks with much richer time series properties for unobservables and general time-varying observables and patterns of duration dependence than conventional duration models. Their analysis of identification of these generalized models requires richer variation driven by observables than is needed in the analysis of the more restrictive conventional models. However, it does not require conventional period-by-period exclusion restrictions, which are often difficult to justify. Instead, they rely on curvature restrictions across the index functions generating the durations that can be motivated by dynamic economic theory.47 Their methods can be applied to a variety of outcome measures including durations.
The key to their ability to identify structural models is that they supplement information on stopping times or time to treatment with additional information on measured consequences of choices of time to treatment as well as measurements. The dynamic discrete-choice literature surveyed in Rust (1994) and Magnac and Thesmar (2002) focuses on discrete-choice processes with general preferences and state vector evolution equations, typically Markovian in nature. Rust's 1994 paper contains negative results on nonparametric identification of discrete-choice processes. Magnac and Thesmar (2002) present some positive results on nonparametric identification if certain parameters or distributions of unobservables are assumed to be known. Heckman and Navarro (2007) produce positive results on nonparametric identification of a class of dynamic discrete-choice models based on expected income maximization developed in labor economics by Flinn and Heckman (1982), Keane and Wolpin (1997) and Eckstein and Wolpin (1999). These frameworks are dynamic versions of the Roy model. Heckman and Navarro (2007) show how use of cross-equation restrictions joined with data on supplementary measurement systems can undo Rust's nonidentification result. We exposit their work and the related literature in this section. With their structural framework, they can distinguish objective outcomes from subjective outcomes (valuations by the decision maker) in a dynamic setting. Applying their analysis to health economics, they can identify the causal effects on health of a medical treatment as well as the associated subjective

46 Aakvik, Heckman, and Vytlacil (2005); Heckman, Tobias, and Vytlacil (2001, 2003); Carneiro, Hansen, and Heckman (2001, 2003) and Heckman and Vytlacil (2005) show how standard treatment effects can be generated from structural models. 47 See Heckman and Honore´ (1989) for examples of such an identification strategy in duration models. See also Cameron and Heckman (1998).

24 Dynamic Policy Analysis

831

pain and suffering of a treatment regime for the patient.48 Attrition decisions also convey information about agent preferences about treatment.49
They do not rely on the assumption of conditional independence of unobserv-
ables with outcomes, given observables, that is used throughout much of the dy-
namic discrete-choice literature and the dynamic treatment literature surveyed in Sect. 24.3.50 As noted in Sect. 24.2, sequential conditional independence assumptions underlie recent work on reduced-form dynamic treatment effects.51 The semi-
parametric analysis of Heckman and Navarro (2007) based on factors generalizes
matching to a dynamic setting. In their paper, some of the variables that would
produce conditional independence and would justify matching if they were ob-
served, are treated as unobserved match variables. They are integrated out and their distributions are identified.52 They consider two classes of models. We review
both.

24.5.1 Semi-parametric Duration Models and Counterfactuals
Heckman and Navarro (2007), henceforth HN, develop a semiparametric index model for dynamic discrete choices that extends conventional discrete-time duration analysis. They separate out duration dependence from heterogeneity in a semiparametric framework more general than conventional discrete-time duration models. They produce a new class of reduced-form models for dynamic treatment effects by adjoining time-to-treatment outcomes to the duration model. This analysis builds on Heckman (1981a,b,c).
Their models are based on a latent variable for choice at time s,
I(s) =  (s, Z (s)) - (s) ,
where the Z(s) are observables and (s) are unobservables from the point of view of the econometrician. Treatments at different times may have different outcome consequences which they model after analyzing the time to treatment equation. Define D(s) as an indicator of receipt of treatment at date s. Treatment is taken the first time I(s) becomes positive. Thus,
D(s) = 1[I(s)  0, I(s - 1) < 0, . . . , I(1) < 0] ,
48 See Chan and Hamilton (2006) for a structural dynamic empirical analysis of this problem. 49 See Heckman and Smith (1998). Use of participation data to infer preferences about outcomes is developed in Heckman (1974). 50 See, e.g., Rust (1987); Manski (1993); Hotz and Miller (1993) and the papers cited in Rust (1994). 51 See, e.g., Gill and Robins (2001) and Lechner and Miquel (2002). 52 For estimates based on this idea, see Carneiro et al. (2003); Aakvik et al. (2005); Cunha and Heckman (2007, 2008); Cunha, Heckman, and Navarro (2005, 2006); and Heckman and Navarro (2005).

832

J.H. Abbring and J.J. Heckman

where the indicator function 1 [·] takes the value of 1 if the term inside the braces is true.53 They derive conditions for identifying a model with general forms of du-
ration dependence in the time to treatment equation using a large sample from the
distribution of (D, Z).

24.5.1.1 Single-Spell Duration Model

Individuals are assumed to start spells in a given (exogenously determined) state and to exit the state at the beginning of time period S.54 S is thus a random vari-
able representing total completed spell length. Let D(s) = 1 if the individual ex-
its at time s, S = s, and D(s) = 0 otherwise. In an analysis of drug treatments, S
is the discrete-time period in the course of an illness at the beginning of which the drug is administered. Let S¯ (< ) be the upper limit on the time the agent being studied can be at risk for a treatment. It is possible in this example that D(1) = 0, . . . , D(S¯) = 0, so that a patient never receives treatment. In a schooling example, "treatment" is not schooling, but rather dropping out of schooling.55 In this case, S¯ is an upper limit to the number of years of schooling, and D(S¯) = 1 if D(1) = 0, . . . , D(S¯ - 1) = 0.
The duration model can be specified recursively in terms of the threshold-
crossing behavior of the sequence of underlying latent indices I(s). Recall that I(s) =  (s, Z (s)) - (s), with Z(s) being the regressors that are observed by the analyst. The Z(s) can include expectations of future outcomes given current infor-
mation in the case of models with forward-looking behavior. For a given stopping time s, let Ds = (D(1), . . . , D(s)) and designate by d(s) and ds values that D(s) and Ds assume. Thus, d(s) can be zero or one and ds is a sequence of s zeros or a se-
quence containing s - 1 zeros and a single one. Denote a sequence of all zeros by
(0), regardless of its length. Then,

D(1) = 1 [I(1)  0]

and

1 [I(s)  0] if Ds-1 = (0)

D(s) =

0

otherwise,

s = 2, . . . , S¯ .

(24.11)

For s = 2, . . . , S¯, the indicator 1 [I(s)  0] is observed if and only if the agent is still at risk of treatment, Ds-1 = (0). To identify period s parameters from period
s outcomes, one must condition on all past outcomes and control for any selection
effects.

53 This framework captures the essential feature of any stopping time model. For example, in a search model with one wage offer per period, I(s) is the gap between market wages and reservation wages at time s. See, e.g., Flinn and Heckman (1982). This framework can also approximate the explicit dynamic discrete-choice model analyzed in Sect. 24.5.2. 54 Thus we abstract from the initial-conditions problem discussed in Heckman (1981b). 55 In the drug treatment example, S may designate the time a treatment regime is completed.

24 Dynamic Policy Analysis

833

Let Z = Z(1), . . . , Z(S¯) , and let  = ((1), . . . , (S¯)). Assume that Z is statistically independent of . Heckman and Navarro (2007) assume that (s, Z(s)) = Z(s)s. We deal with a more general case. (Z) = (1, Z(1)), . . . , (S¯, Z(S¯)) . We let  denote the abstract parameter. Depending on the values assumed by (s, Z(s)), one can generate very general forms of duration dependence that depend on the values assumed by the Z(s). HN allow for period-specific effects of regressors on the latent indices generating choices.
This model is the reduced form of a general dynamic discrete-choice model. Like many reduced-form models, the link to choice theory is not clearly specified. It is not a conventional multinomial choice model in a static (perfect certainty) setting with associated outcomes.

24.5.1.2 Identification of Duration Models with General Error Structures and Duration Dependence
Heckman and Navarro (2007) establish semiparametric identification of the model of equation (24.11) assuming access to a large sample of i.i.d. (D, Z) observations. Let Zs = (Z(1), . . . , Z(s)). Data on (D, Z) directly identify the conditional probability Pr(D(s) = d (s) |Zs, Ds-1 = (0)) a.e. FZs|Ds-1=(0) where FZs|Ds-1=(0) is the distribution of Zs conditional on previous choices Ds-1 = (0). Assume that (, F )   × H , where F is the distribution of  and  × H is the parameter space. The goal is to establish conditions under which knowledge of Pr(D(s) = d(s)|Z, Ds-1 = (0)) a.e. FZ|Ds-1=(0) allows the analyst to identify a unique element of  × H . They use a limit strategy that allows them to recover the parameters by conditioning on large values of the indices of the preceding choices. This identification strategy is widely used in the analysis of discrete choice.56
They establish sufficient conditions for the identification of model (24.11). We prove the following more general result:
Theorem 24.1. For the model defined by (24.11), assume the following conditions:
(i)   Z. (ii)  is an absolutely continuous random variable on RS¯ with support Ss¯=1((s),
(s)), where -  (s) < (s)  +, for all s = 1, . . . , S¯. (iii) The (s, Z(s)) satisfy the Matzkin (1992) conditions for identification of non-
parametric binary choice models, s = 1, . . . , S¯.57 (iv) Supp s-1(Z), Z(s) = Supp s-1(Z) × Supp (Z(s)), s = 2, . . . , S¯. (v) Supp((Z))  Supp().
56 See, e.g., Manski (1988); Heckman (1990); Heckman and Honore´ (1989, 1990); Matzkin (1992, 1993); Taber (2000); and Carneiro et al. (2003). A version of the strategy of this proof was first used in psychology where agent choice sets are eliminated by experimenter manipulation. The limit set argument effectively uses regressors to reduce the choice set confronting agents. See Falmagne (1985) for a discussion of models of choice in psychology. 57 See Abbring and Heckman (2007, Appendix B.1) for a review of the conditions Matzkin (1992) imposes for identification of nonparametric binary choice models. See also Matzkin (1994).

834

J.H. Abbring and J.J. Heckman

Then F and (Z) are identified, where the (s, Z(s)), s = 1, . . . , S¯, are identified over the relevant support admitted by (ii).

Proof. We sketch the proof for S¯ = 2. The result for general S¯ follows by a recursive application of this argument. Consider the following three probabilities.

(a)

Pr(D(1) = 1 | Z = z) =

(1,z(1))  (1)

f(1)(u) du

(b)

Pr(D(2) = 1, D(1) = 0 | Z = z) =

(2,z(2))  (2)

¯ (1) (1,z(1))

f(1),(2)(u1, u2) du1du2.

(c)

Pr(D(2) = 0, D(1) = 0 | Z = z) =

¯ (2) (2,z(2))

¯ (1) (1,z(1))

f(1),(2)(u1, u2) du1du2.

The left-hand sides are observed from data on those who stop in period 1 (a); those

who stop in period 2 (b); and those who terminate in the "0" state in period 2 (c).

From Matzkin (1992), we can identify (1, z(1)) and F(1) from (a). Using (b),

we can fix z(2) and vary (1, z(1)). From (iv) and (v), there exists a limit set Z1,

possibly dependent construct

on

z(2),

such

that

limz(1)Z1

(1, z(1))

=

 (1).

Thus

we

can

¯ (2)

Pr(D(2) = 0 | Z = z) =

f(2)(u2) du2

(2,z(2))

and identify (2, z(2)) and F(2). Using the (1, z(1)), (2, z(2)), one can trace out the joint distribution F(1),(2) over its support. Under the Matzkin conditions, identification is achieved on a non-negligible set. The proof generalizes in a straightforward way to general S¯.

Observe that if the (s) are bounded by finite upper and lower limits, we can only determine the (s, Z(s)) over the limits so defined. Consider the first step of
the proof. Under the Matzkin conditions, F(1) is known. From assumption (ii), we can determine

(1, z(1)) = F-(11)(Pr(D(1) = 1 | Z = z)) ,
but only over the support ((1), ¯ (1)). If the support of (1) is R, we determine (1, z(1)) for all z(1). Heckman and Navarro (2007) analyze the special case (s, Z(s)) = Z(s)s and invoke sequential rank conditions to identify s, even over limited supports. They also establish that the limit sets are non-negligible in this case so that standard definitions of identifiability (see, e.g., Matzkin, 1992) will be satisfied.58 Construction of the limit set Zs, s = 1, . . . , S¯, depends on the functional form specified for the (s, Z(s)). For the linear-in-parameters case (s, Z(s)) = Z(s)s, they are obtained by letting arguments get big or small. Matzkin (1992) shows how to establish the limit sets for functions in her family of functions.

58 Heckman and Navarro (2007) prove their theorem for a model where D(s) = 1[I(s)  0] if Ds-1 = (0), s = 2, . . . , S¯. Our formulation of their result is consistent with the notation in this
chapter.

24 Dynamic Policy Analysis

835

A version of Theorem 24.1 with (s, Z(s)) = Z(s)s that allows dependence between Z and s except for one component can be proved using the analysis of Lewbel (2000) and Honore´ and Lewbel (2002).59
The assumptions of Theorem 24.1 will be satisfied if there are transition-specific exclusion restrictions for Z with the required properties. As noted in Sect. 24.4, in models with many periods, this may be a demanding requirement. Very often, the Z variables are time invariant and so cannot be used as exclusion restrictions. Corollary 1 in HN, for the special case (s, Z(s)) = Z(s)s, tells us that the HN version of the model can be identified, even if there are no conventional exclusion restrictions and the Z(s) are the same across all time periods, if sufficient structure is placed on how the s vary with s. Variations in the values of s across time periods arise naturally in finite horizon dynamic discrete-choice models where a shrinking horizon produces different effects of the same variable in different periods. For example, in Wolpin's (1987) analysis of a search model, the value function depends on time and the derived decision rules weight the same invariant characteristics differently in different periods. In a schooling model, parental background and resources may affect education continuation decisions differently at different stages of the schooling decision. The model generating (24.11) can be semiparametrically identified without transition-specific exclusions if the duration dependence is sufficiently general. For a proof, see Corollary 1 in Heckman and Navarro (2007).
The conditions of Theorem 24.1 are somewhat similar to the conditions on the regressor effects needed for identification of the continuous-time event-history models in Sect. 24.4. One difference is that the present analysis requires independent variation of the regressor effects over the support of the distribution of the unobservables generating outcomes. The continuous-time analysis based on the functional form of the mixed proportional hazard model (MPH) as analyzed by Abbring and Van den Berg (2003a) only requires local independent variation.
Theorem 24.1 and Corollary 1 in HN have important consequences. The  (s, Z(s)), s = 1, . . . , S¯, can be interpreted as duration dependence parameters that are modified by the Z(s) and that vary across the spell in a more general way than is permitted in mixed proportional hazards (MPH), generalized accelerated failure time (GAFT) models or standard discrete-time hazard models.60 Duration dependence in conventional specifications of duration models is usually generated by variation in model intercepts. The regressors are allowed to interact with the duration dependence parameters. In the specifications justified by Theorem 24.1, the "heterogeneity" distribution F is identified for a general model. No special "permanenttransitory" structure is required for the unobservables although that specification is traditional in duration analysis. Their explicit treatment of the stochastic structure

59 HN discuss a version of such an extension at their website. Lewbel's conditions are very strong. To account for general forms of dependence between Z and s requires modeling the exact form of the dependence. Nonparametric solutions to this problem remain an open question in the literature on dynamic discrete choice. One solution is to assume functional forms for the error terms, but in general, this is not enough to identify the model without further restrictions imposed. See Heckman and Honore´ (1990).
60 See Ridder (1990) for a discussion of these models.

836

J.H. Abbring and J.J. Heckman

of the duration model is what allows HN to link in a general way the unobservables generating the duration model to the unobservables generating the outcome equations that are introduced in the next section. Such an explicit link is not currently available in the literature on continuous-time duration models for treatment effects surveyed in Sect. 24.4, and is useful for modelling selection effects in outcomes across different treatment times. Their outcomes can be both discrete and continuous and are not restricted to be durations.
Under the conditions given in Corollary 1 of HN, no period-specific exclusion conditions are required on the Z. Hansen and Sargent (1980) and Abbring and Van den Berg (2003b) note that period-specific exclusions are not natural in reduced-form duration models designed to approximate forward-looking life cycle models. Agents make current decisions in light of their forecasts of future constraints and opportunities, and if they forecast some components well, and they affect current decisions, then they are in Z (s) in period s. Corollary 1 in HN establishes identification without such exclusions. HN adjoin a system of counterfactual outcomes to their model of time to treatment to produce a model for dynamic counterfactuals. We summarize that work next.

24.5.1.3 Reduced-Form Dynamic Treatment Effects
This section reviews a reduced-form approach to generating dynamic counterfactuals developed by HN. They apply and extend the analysis of Carneiro et al. (2003) and Cunha et al. (2005, 2006) to generate ex post potential outcomes and their relationship with the time to treatment indices I(s) analyzed in the preceding subsection. With reduced-form models, it is difficult to impose restrictions from economic theory or to make distinctions between ex ante and ex post outcomes. In the structural model developed below, these and other distinctions can be made easily.
Associated with each treatment time s, s = 1, . . . , S¯, is a vector of T¯ outcomes,
Y (s, X,U (s)) = (Y (1, s, X,U (1, s)) , . . . ,Y (t, s, X,U (t, s)) , . . . ,Y (T¯ , s, X,U (T¯ , s))) .
Outcomes depend on covariates X and U (s) = (U (1, s) , . . . ,U (t, s) , . . . , U(T¯ , s)) that are, respectively, observable and unobservable by the econometrician. Elements of Y (s, X,U (s)) are outcomes associated with stopping or receiving treatment at the beginning of period s. They are factual outcomes if treatment s is actually selected (S = s and D(s) = 1). Outcomes corresponding to treatments s that are not selected (D(s ) = 0) are counterfactuals. The outcomes associated with each treatment may be different, and indeed the treatments administered at different times may be different.
The components Y (t, s, X,U(t, s)) of the vector Y (s, X,U (s)) can be interpreted as the outcomes revealed at age t, t = 1, . . . , T¯ , and may themselves be vectors. The reduced-form approach presented in this section is not sufficiently rich to capture the notion that agents revise their anticipations of components of Y (s, X,U (s)),

24 Dynamic Policy Analysis

837

s = 1, . . . , S¯, as they acquire information over time. This notion is systematically developed using the structural model discussed below in Sect. 24.5.2.
The treatment "times" may be stages that are not necessarily connected with real times. Thus s may be a schooling level. The correspondence between stages and times is exact if each stage takes one period to complete. Our notation is more flexible, and time and periods can be defined more generally. Our notation in this section accommodates both cases.
Henceforth, whenever we have random variables with multiple arguments R0(s, Q0, . . . ) or R1(t, s, Q0, . . . ) where the argument list begins with treatment state s or both age t and state s (perhaps followed by other arguments Q0, . . . ), we will make use of several condensed notations: (a) dropping the first argument as we collect the components into vectors R0(Q0, . . . ) or R1(s, Q0, . . . ) of length S¯ or T¯ , respectively, and (b) going further in the case of R1, dropping the s argument as we collect the vectors R1(s, Q0, . . . ) into a single S¯ × T¯ array R1(Q0, . . . ), but also (c) suppressing one or more of the other arguments and writing R1(t, s) or R1(t, s, Q0) instead of R1(t, s, Q0, Q1, . . .), etc. This notation is sufficiently rich to represent the life cycle of outcomes for persons who receive treatment at s. Thus, in a schooling example, the components of this vector may include life cycle earnings, employment, and the like associated with a person with characteristics X, U (s) , s = 1, . . . , S¯, who completes s years of schooling and then forever ceases schooling. It could include earnings while in school at some level for persons who will eventually attain further schooling as well as post-school earnings.
We measure age and treatment time on the same time scale, with origin 1, and let T¯  S¯. Then, the Y (t, s, X,U(t, s)) for t < s are outcomes realized while the person is in school at age t (s is the time the person will leave school; t is the current age) and before "treatment" (stopping schooling) has occurred. When t  s, these are post-school outcomes for treatment with s years of schooling. In this case, t - s is years of post-school experience. In the case of a drug trial, the Y (t, s, X,U (t, s)) for t < s are measurements observed before the drug is taken at s and if t  s, they are the post-treatment measurements.
Following Carneiro et al. (2003), the variables in Y (t, s, X,U(t, s)) may include discrete, continuous or mixed discrete-continuous components. For the discrete or mixed discrete-continuous cases, HN assume that latent continuous variables cross thresholds to generate the discrete components. Durations can be generated by latent index models associated with each outcome crossing thresholds analogous to the model presented in (24.11). In this framework, for example, we can model the effect of attaining s years of schooling on durations of unemployment or durations of employment.
The reduced-form analysis in this section does not impose restrictions on the temporal (age) structure of outcomes across treatment times in constructing outcomes and specifying identifying assumptions. Each treatment time can have its own age path of outcomes pre and post treatment. Outcomes prior to treatment and outcomes after treatment are treated symmetrically and both may be different for different treatment times. In particular, HN can allow earnings at age t for people who receive treatment at some future time s to differ from earnings at age t for people

838

J.H. Abbring and J.J. Heckman

who receive treatment at some future time s , min (s , s ) > t even after controlling for U and X.
This generality is in contrast with the analyses of Robins (1997) and Gill and Robins (2001) discussed in Sect. 24.3 and the analysis of Abbring and Van den Berg (2003b) discussed in Sect. 24.4. These analyses require exclusion of such anticipation effects to secure identification, because their models attribute dependence of treatment on past outcomes to selection effects. The sequential randomization assumption (M-1) underlying the work of Gill and Robins allows treatment decisions S(t) at time t to depend on past outcomes Ypt-0 1 in a general way. Therefore, without additional restrictions, it is not possible to also identify causal (anticipatory) effects of treatment S(t) on Ypt-0 1. The no-anticipation condition (NA) excludes such effects and secures identification in their framework.61 It is essential for applying the conditional independence assumptions in deriving the g-computation formula.
HN's very different approach to identification allows them to incorporate anticipation effects. As in their analysis of the duration model, they assume that there is an exogenous source of independent variation of treatment decisions, independent of past outcomes. Any variation in current outcomes with variation in future treatment decisions induced by this exogenous source cannot be due to selection effects (since they explicitly control for the unobservables) and is interpreted as anticipatory effects of treatment in their framework. However, their structural analysis naturally excludes such effects (see Sect. 24.5.2 below). Therefore, a natural interpretation of the ability of HN to identify anticipatory effects is that they have overidentifying restrictions that allow them to test their model and, if necessary, relax their assumptions.
In a model with uncertainty, agents act on and value ex ante outcomes. The model developed below in Sect. 24.5.2 distinguishes ex ante from ex post outcomes. The model developed in this section cannot because, within it, it is difficult to specify the information sets on which agents act or the mechanism by which agents forecast and act on Y (s, X,U (s)) when they are making choices.

61 The role of the no-anticipation assumption in Abbring and Van den Berg (2003b) is similar. However, their main analysis assumes an asymmetric treatment-outcome setup in which treatment is not observed if it takes place after the outcome transition. In that case, the treatment time is censored at the outcome time. In this asymmetric setup, anticipatory effects of treatment on outcomes cannot be identified because the econometrician cannot observe variation of outcome transitions with future treatment times. This point may appear to be unrelated to the present discussion, but it is not. As was pointed out by Abbring and Van den Berg (2003b), and in Sect. 24.4, the asymmetric Abbring and Van den Berg (2003b) model can be extended to a fully symmetric bivariate duration model in which treatment hazards may be causally affected by the past occurrence of an outcome event just like outcomes may be affected by past treatment events. This model could be used to analyze data in which both treatment and outcome times are fully observed. In this symmetric setup, any dependence in the data of the time-to-treatment hazard on past outcome events is interpreted as an effect of outcomes on future treatment decisions, and not an anticipatory effect of treatment on past outcomes. If one does not restrict the effects of outcomes on future treatment, without further restrictions, the data on treatments occurring after the outcome event carry no information on anticipatory effects of treatment on outcomes and they face an identification problem similar to that in the asymmetric case.

24 Dynamic Policy Analysis

839

One justification for not making an ex ante ­ ex post distinction is that the agents being modeled operate under perfect foresight even though econometricians do not observe all of the information available to the agents. In this framework, the U (s) , s = 1, . . . , S¯, are an ingredient of the econometric model that accounts for the asymmetry of information between the agent and the econometrician studying the agent.
Without imposing assumptions about the functional structure of the outcome equations, it is not possible to nonparametrically identify counterfactual outcome states Y (s, X,U (s)) that have never been observed. Thus, in a schooling example, HN assume that analysts observe life cycle outcomes for some persons for each stopping time (level of final grade completion) and our notation reflects this.62 However, analysts do not observe Y (s, X,U (s)) for all s for anyone. A person can have only one stopping time (one completed schooling level). This observational limitation creates our evaluation problem, the "fundamental problem of causal inference".63
In addition to this problem, there is the standard selection problem that the Y (s, X,U (s)) are only observed for persons who stop at s and not for a random sample of the population. The selected distribution may not accurately characterize the population distribution of Y (s, X,U (s)) for persons selected at random. Note also that without further structure, we can only identify treatment responses within a given policy environment. In another policy environment, where the rules governing selection into treatment and/or the outcomes from treatment may be different, the same time to treatment may be associated with entirely different responses.64 We now turn to the HN analysis of identification of outcome and treatment time distributions.

24.5.1.4 Identification of Outcome and Treatment Time Distributions
We assume access to a large i.i.d. sample from the distribution of (S,Y (S, X,U(S)), X, Z), where S is the stopping time, X are the variables determining outcomes and Z are the variables determining choices. We also know Pr(S = s | Z = z), for s = 1, . . . , S¯, from the data. For expositional convenience, we first consider the case of scalar outcomes Y (S, X,U (S)). An analysis for vector Y (S, X,U (S)) is presented in HN and is discussed below.
Consider the analysis of continuous outcomes. HN analyze more general cases. Their results extend the analyses of Heckman and Honore´ (1990); Heckman (1990) and Carneiro et al. (2003) by considering choices generated by a stopping time model. To simplify the notation in this section, assume that the scalar outcome associated with stopping at time s can be written as Y (s) =  (s, X) +U (s), where Y (s)
62 In practice, analysts can only observe a portion of the life cycle after treatment. See the discussion on pooling data across samples in Cunha et al. (2005) to replace missing life cycle data. 63 See Holland (1986) or Gill and Robins (2001). 64 This is the problem of general equilibrium effects, and leads to violation of the policy invariance conditions. See Heckman et al. (1998), Heckman et al. (1999) or Abbring and Van den Berg (2003b) for discussion of this problem.

840

J.H. Abbring and J.J. Heckman

is shorthand for Y (s, X,U (s)). Y (s) is observed only if D (s) = 1 where the D (s) are generated by the model analyzed in Theorem 24.1. Write I(s) =  (s, Z(s)) - (s). Assume that the  (s, Z(s)) belong to the Matzkin (1992) class of functions. We use the condensed representations I,  (Z), , Y ,  (X) and U as described in the previous subsection.
Heckman and Navarro permit general stochastic dependence within the components of U, within the components of  and across the two vectors. They assume that (X, Z) are independent of (U, ). Each component of (U, ) has a zero mean. The joint distribution of (U, ) is assumed to be absolutely continuous.
With "sufficient variation" in the components of  (Z), one can identify (s, X), [(1, Z (1)), . . . , (s, Z (s))] and the joint distribution of U(s) and s. This enables the analyst to identify average treatment effects across all stopping times, since one can extract E(Y (s) - Y (s ) | X = x) from the marginal distributions of Y (s), s = 1, . . . , S¯.
Theorem 24.2. Write s(Z) = ((1, Z(1), . . . , (s, Z(s))). Assume in addition to the conditions in Theorem 24.1 that
(i) E[U(s)] = 0. (U(s), s) are continuous random variables with support Supp(U(s)) × Supp(s) with upper and lower limits (U(s), s) and (U(s), s), respectively, s = 1, . . . , S¯. These conditions hold for each component of each subvector. The joint system is thus variation free for each component with respect to every other component.
(ii) (U(s), s)  (X, Z), s = 1, . . . , S¯ (independence). (iii) (s, X) is a continuous function, s = 1, . . . , S¯. (iv) Supp ((Z), X) = Supp ((Z)) × Supp(X).
Then one can identify (s, X), s(Z), Fs,U(s), s = 1, . . . , S¯, where (Z) is identified over the support admitted by condition (ii) of Theorem 24.1.

Proof. See Abbring and Heckman (2007), Appendix C.

The proof in Abbring and Heckman (2007, Appendix C) covers the case of vector Y (s, X,U(s)) where each component is a continuous random variable. Appendix D of Abbring and Heckman (2007) states and proves a more general theorem for agespecific outcomes Y (t, s, X,U(t, s)), t = 1, . . . , T¯ , where Y can be a vector of continuous and discrete outcomes. In particular, HN can identify age-specific earnings flows associated with multiple sources of income.
Theorem 24.2 does not identify the joint distribution of Y (1) , . . . ,Y S¯ because analysts observe only one of these outcomes for any person. Observe that exclusion restrictions in the arguments of the choice of treatment equation are not required to identify the counterfactuals. What is required is independent variation of arguments which might be achieved by exclusion conditions but can be obtained by other functional restrictions (see HN, Corollary 1, for example). One can identify the  (s, X) (up to constants) without the limit set argument. Thus, one can identify certain features of the model without using the limit set argument. See HN.
As a by-product of Theorem 24.2, one can construct various counterfactual distributions of Y (s) for agents with index crossing histories such that D(s) = 0 (that is,

24 Dynamic Policy Analysis

841

for whom Y (s) is not observed). Define B(s) = 1 [I(s)  0], Bs = (B(1), . . . , B(s)), and let bs denote a vector of possible values of Bs. D(s) was defined as B(s) if Bs-1 = (0) and 0 otherwise. Theorem 24.2 gives conditions under which the coun-
terfactual distribution of Y (s) for those with D(s ) = 1, s = s, can be constructed.
More generally, it can be used to construct

Pr Y (s)  y (s) | Bs = bs , X = x, Z = z

for all of the 2s possible sequences bs of Bs outcomes up to s  s. If bs equals a sequence of s - 1 zeros followed by a one, then Bs = bs corresponds to D(s ) = 1. The event Bs = (0) corresponds to Ds = (0), i.e., S > s . For all other sequences bs , Bs = bs defines a subpopulation of the agents with D(s ) = 1 for some s < s and multiple index crossings. For example, Bs = (0, 1, 0) corresponds to D(2) = 1 and I(3) < 0. This defines a subpopulation that takes treatment at time 2, but that would not take treatment at time 3 if it would not have taken treatment at time 2.65 It is tempting to interpret such sequences with multiple crossings as corresponding to multiple entry into and exit from treatment. However, this is inconsistent with the stopping time model (24.11), and would require extension of the model to deal with recurrent treatment. Whether a threshold-crossing model corresponds to a structural model of treatment choice is yet another issue, which is taken up in the next section and is also addressed in Cunha, Heckman, and Navarro (2007).
The counterfactuals that are identified by fixing D (s ) = 1 for different treatment times s in the general model of HN have an asymmetric aspect. HN can generate Y (s) distributions for persons who are treated at s or before. Without further structure, they cannot generate the distributions of these random variables for people who receive treatment at times after s.
The source of this asymmetry is the generality of duration model (24.11). At each stopping time s, HN acquire a new random variable (s) which can have arbitrary dependence with Y (s) and Y (s ) for all s and s . From Theorem 24.2, HN can identify the dependence between (s ) and Y (s) if s  s. They cannot identify the dependence between (s ) and Y (s) for s > s without imposing further structure on the unobservables.66 Thus, one can identify the distribution of college outcomes for high school graduates who do not go on to college and can compare these to outcomes for high school graduates, so they can identify the parameter "treatment on the untreated." However, one cannot identify the distribution of high school outcomes for college graduates (and hence treatment on the treated parameters) without imposing further structure.67 Since one can identify the marginal distributions under

65 Cunha et al. (2007) develop an ordered choice model with stochastic thresholds.
66 One possible structure is a factor model which is applied to this problem in the next section.
67 In the schooling example, one can identify treatment on the treated for the final category S¯ since DS¯-1 = (0) implies D S¯ = 1. Thus at stage S¯ - 1, one can identify the distribution of Y S¯ - 1 for persons for whom D (0) = 0, . . . , D S¯ - 1 = 0, D S¯ = 1. Hence, if college is the terminal state, and high school the state preceding college, one can identify the distribution of high school outcomes for college graduates.

842

J.H. Abbring and J.J. Heckman

the conditions of Theorem 24.2, one can identify pairwise average treatment effects for all s, s .
It is interesting to contrast the model identified by Theorem 24.2 with a conventional static multinomial discrete-choice model with an associated system of counterfactuals, as presented in Heckman and Vytlacil (2007a, Appendix B) and analyzed in Abbring and Heckman (2007, Sect. 2). Using standard tools, it is possible to establish semiparametric identification of the conventional static model of discrete choice joined with counterfactuals and to identify all of the standard mean counterfactuals. For that model there is a fixed set of unobservables governing all choices of states. Thus the analyst does not acquire new unobservables associated with each stopping time as occurs in a dynamic model. In a dynamic model, selection effects for Y (s) depend on the unobservables up to s but not later innovations Selection effects in a static discrete-choice model depend on a fixed set of unobservables for all outcomes. With suitable normalizations, HN identify the joint distributions of choices and associated outcomes without the difficulties, just noted, that appear in the reduced-form dynamic model. HN develop models for discrete outcomes including duration models.

24.5.1.5 Using Factor Models to Identify Joint Distributions of Counterfactuals
From Theorem 24.2 and its generalizations reported in HN, one can identify joint distributions of outcomes for each treatment time s and the index generating treatment times. One cannot identify the joint distributions of outcomes across treatment times. Moreover, as just discussed, one cannot, in general, identify treatment on the treated parameters.
Aakvik et al. (2005) and Carneiro et al. (2003) show how to use factor models to identify the joint distributions across treatment times and recover the standard treatment parameters. HN use their approach to identify the joint distribution of Y = (Y (1), . . . ,Y (S¯)).
The basic idea underlying this approach is to use joint distributions for outcomes measured at each treatment time s along with the choice index to construct the joint distribution of outcomes across treatment choices. To illustrate how to implement this intuition, suppose that we augment Theorem 24.2 by appealing to Theorem 2 in Carneiro et al. (2003) to identify the joint distribution of the vector of outcomes at each stopping time along with Is = (I (1) , . . . , I (s)) for each s. For each s, we may write
Y (t, s, X,U (t, s)) =  (t, s, X) +U (t, s) , t = 1, . . . , T¯ I(s) =  (s, Z(s)) - (s) .
The scale of (s, Z(s)) is determined from the Matzkin (1992) conditions. If we specify the Matzkin functions only up to scale, we determine the functions up to

24 Dynamic Policy Analysis

843

scale and make a normalization. From Theorem 24.2, we can identify the joint distribution of ((1), . . . , (s), U(1, s), . . . ,U(T¯ , s)).
To review these concepts and their application to the model discussed in this section, suppose that we adopt a one-factor model where  is the factor. It has mean zero. The errors can be represented by

(s) = s + (s) U (t, s) = t,s + t,s,

t = 1, . . . , T¯ ,

s = 1, . . . , S¯ .

The  are independent of all of the (s), t,s and the 's are mutually independent mean zero disturbances. The s and t,s are factor loadings. Since  is an unobservable, its scale is unknown. One can set the scale of  by normalizing one factor loading, say T¯,S¯ = 1. From the joint distribution of (s,U (s)), one can identify 2, t,s, s,t = 1, . . . , T¯ , for s = 1, . . . , S¯, using the arguments presented in, e.g., Abbring and Heckman (2007, Sect. 2.8). A sufficient condition is T¯  3, but this ignores pos-
sible additional information from cross-system restrictions. From this information,
one can form for t = t or s = s or both,

Cov U (t, s) ,U t , s = t,st ,s 2 ,
even though the analyst does not observe outcomes for the same person at two different stopping times. In fact, one can construct the joint distribution of (U, ) = (U (1) , . . . ,U S¯ , ). From this joint distribution, one can recover the standard mean treatment effects as well as the joint distributions of the potential outcomes. One can determine the percentage of participants at treatment time s who benefit from participation compared to what their outcomes would be at other treatment times. One can perform a parallel analysis for models for discrete outcomes and durations. The analysis can be generalized to multiple factors. Conventional factor analysis assumes that the unobservables are normally distributed. Carneiro et al. (2003) establish nonparametric identifiability of the  's and the 's and their analysis of nonparametric identifiability applies here.
Theorem 24.2, strictly applied, actually produces only one scalar outcome along with one or more choices for each stopping time.68 If vector outcomes are not available, access to a measurement system M that assumes the same values for each stopping time can substitute for the need for vector outcomes for Y . Let Mj be the jth component of this measurement system. Write

Mj =  j,M(X ) +Uj,M, j = 1, . . . , J ,

where Uj,M are mean zero and independent of X. Suppose that the Uj,M have a one-factor structure so Uj,M =  j,M +  j,M,
j = 1, . . . , J, where the  j,M are mean zero, mutually independent random variables, independent of the  . Adjoining these measurements to the one outcome measure
Y (s) can substitute for the measurements of Y (t, s) used in the previous example.

68 HN and Abbring and Heckman (2007) analyze the vector-outcome case.

844

J.H. Abbring and J.J. Heckman

In an analysis of schooling, the Mj can be test scores that depend on ability  . Ability is assumed to affect outcomes Y (s) and the choice of treatment times in-
dices.
The factor models implement a matching on unobservables assumption, {Y (s)}sS¯=1  S | X, Z,  . HN allow for the  to be unobserved variables and present conditions under which their distributions can be identified.

24.5.1.6 Summary of the Reduced-Form Model
A limitation of the reduced-form approach pursued in this section is that, because the underlying model of choice is not clearly specified, it is not possible without further structure to form, or even define, the marginal treatment effect analyzed in Heckman and Vytlacil (1999, 2001, 2005, 2007a,b) or Heckman, Urzua, and Vytlacil (2006). The absence of well defined choice equations is problematic for the models analyzed thus far in this section of our chapter, although it is typical of many statistical treatment effect analyses.69 In this framework, it is not possible to distinguish objective outcomes from subjective evaluations of outcomes, and to distinguish ex ante from ex post outcomes. Another limitation of this analysis is its strong reliance on large support conditions on the regressors coupled with independence assumptions. Independence can be relaxed following Lewbel (2000) and Honore´ and Lewbel (2002). The large support assumption plays a fundamental role here and throughout the entire evaluation literature.
HN develop an explicit economic model for dynamic treatment effects that allows analysts to make these and other distinctions. They extend the analysis presented in this subsection to a more precisely formulated economic model. They explicitly allow for agent updating of information sets. A well posed economic model enables economists to evaluate policies in one environment and accurately project them to new environments as well as to accurately forecast new policies never previously experienced. We now turn to an analysis of a more fully articulated structural econometric model.

24.5.2 A Sequential Structural Model with Option Values
This section analyzes the identifiability of a structural sequential optimal stopping time model. HN use ingredients assembled in the previous sections to build an economically interpretable framework for analyzing dynamic treatment effects. For specificity, HN focus on a schooling model with associated earnings outcomes that is motivated by the research of Keane and Wolpin (1997) and Eckstein and Wolpin
69 Heckman (2005) and Heckman and Vytlacil (2007a,b) point out that one distinctive feature of the economic approach to program evaluation is the use of choice theory to define parameters and evaluate alternative estimators.

24 Dynamic Policy Analysis

845

(1999). They explicitly model costs and build a dynamic version of a Roy model. We briefly survey the literature on dynamic discrete choice in Sect. 24.5.5 below.
In the model of this section, it is possible to interpret the literature on dynamic treatment effects within the context of an economic model; to allow for earnings while in treatment as well as grade-specific tuition costs; to separately identify returns and costs; to distinguish private evaluations from "objective" ex ante and ex post outcomes and to identify persons at various margins of choice. In the context of medical economics, HN consider how to identify the pain and suffering associated with a treatment as well as the distribution of benefits from the intervention. They also model how anticipations about potential future outcomes associated with various choices evolve over the life cycle as sequential treatment choices are made.
In contrast to the analysis of Sect. 24.5.1, the identification proof for their dynamic choice model works in reverse starting from the last period and sequentially proceeding backward. This approach is required by the forward-looking nature of dynamic choice analysis and makes an interesting contrast with the analysis of identification for the reduced-form models which proceeds forward from initial period values.
HN use limit set arguments to identify the parameters of outcome and measurement systems for each stopping time s = 1, . . . , S¯, including means and joint distributions of unobservables. These systems are identified without invoking any special assumptions about the structure of model unobservables. When they invoke factor structure assumptions for the unobservables, they identify the factor loadings associated with the measurements (as defined in Sect. 24.5.1.5) and outcomes. They also nonparametrically identify the distributions of the factors and the distributions of the innovations to the factors. With the joint distributions of outcomes and measurements in hand for each treatment time, HN can identify cost (and preference) information from choice equations that depend on outcomes and costs (preferences). HN can also identify joint distributions of outcomes across stopping times. Thus, they can identify the proportion of people who benefit from treatment. Their analysis generalizes the one shot decision models of Cunha and Heckman (2007, 2008); Cunha et al. (2005, 2006) to a sequential setting.
All agents start with 1 year of schooling at age 1 and then sequentially choose, at each subsequent age, whether to continue for another year in school. New information arrives at each age. One of the benefits of staying in school is the arrival of new information about returns. Each year of schooling takes 1 year of age to complete. There is no grade repetition. Once persons leave school, they never return.70 As a consequence, an agent's schooling level equals her age up to the time S  S¯ she leaves school. After that, ageing continues up to age T¯  S¯, but schooling does not. We again denote D(s) = 1(S = s) for all s  {1, . . . , S¯}. Let (t) = 1 if a person has left school at or before age t; (t) = 0 if a person is still in school.
A person's earnings at age t depend on her current schooling level s and whether she has left school on or before age t ((t) = 1) or not ((t) = 0). Thus,

70 It would be better to derive such stopping behavior as a feature of a more general model with possible recurrence of states. Cunha et al. (2007) develop general conditions under which it is optimal to stop and never return.

846

J.H. Abbring and J.J. Heckman

Y (t, s, (t), X) =  (t, s, (t), X) +U (t, s, (t)) .

(24.12)

Note that Y (t, s, 0, X) is only meaningfully defined if s = t, in which case it denotes the earnings of a person as a student at age and schooling level s. More precisely, Y (s, s, 0, X) denotes the earnings of an individual with characteristics X who is still enrolled in school at age and schooling level s and goes on to complete at least s + 1 years of schooling. The fact that earnings in school depend only on the current schooling level, and not on the final schooling level obtained, reflects the noanticipation condition (NA). U (t, s, (t)) is a mean zero shock that is unobserved by the econometrician but may, or may not, be observed by the agent. Y (t, s, 1, X) is meaningfully defined only if s  t, in which case it denotes the earnings at age t of an agent who has decided to stop schooling at s.
The direct cost of remaining enrolled in school at age and schooling level s is

C (s, X, Z (s)) =  (s, X, Z (s)) +W (s)

where X and Z (s) are vectors of observed characteristics (from the point of view of the econometrician) that affect costs at schooling level s, and W (s) are mean zero shocks that are unobserved by the econometrician that may or may not be observed by the agent. Costs are paid in the period before schooling is undertaken. The agent is assumed to know the costs of making schooling decisions at each transition. The agent is also assumed to know the X and Z = (Z(1), . . . , Z(S¯ - 1)) from age 1.71
The optimal schooling decision involves comparisons of the value of continuing in school for another year and the value of leaving school forever at each age and schooling level s  {1, . . . , S¯ - 1}. We can solve for these values, and the optimal schooling decision, by backward recursion.
The agent's expected reward of stopping schooling forever at level and age s (i.e., receiving treatment s) is given by the expected present value of her remaining lifetime earnings:

T¯ -s 1 j
R (s, Is) = E j=0 1 + r Y (s + j, s, 1, X) Is ,

(24.13)

where Is are the state variables generating the age-s-specific information set Is.72 They include the schooling level attained at age s, the covariates X and Z, as well
as all other variables known to the agent and used in forecasting future variables. Assume a fixed, nonstochastic, interest rate r.73 The continuation value at age and
schooling level s given information Is is denoted by K (s, Is).

71 These assumptions can be relaxed and are made for convenience. See Carneiro, Hansen, and Heckman (2003), Cunha, Heckman, and Navarro (2005) and Cunha and Heckman (2007) for a discussion of selecting variables in the agent's information set. 72 We only consider the agent's information set here, and drop the subscript A for notational convenience. 73 This assumption is relaxed in HN who present conditions under which r can be identified.

24 Dynamic Policy Analysis

847

At S¯ - 1, when an individual decides whether to stop or continue on to S¯, the expected reward from remaining enrolled and continuing to S¯ (i.e., the continuation
value) is the earnings while in school less costs plus the expected discounted future return that arises from completing S¯ years of schooling:

K S¯ - 1, IS¯-1 = Y S¯ - 1, S¯ - 1, 0, X -C S¯ - 1, X, Z S¯ - 1

1 +E
1+r

R

S¯, IS¯

| IS¯-1

where C S¯ - 1, X, Z S¯ - 1 is the direct cost of schooling for the transition to S¯.
This expression embodies the assumption that each year of school takes 1 year of
age. IS¯-1 incorporates all of the information known to the agent. The value of being in school just before deciding on continuation at age and
schooling level S¯ - 1 is the larger of the two expected rewards that arise from stopping at S¯ - 1 or continuing one more period to S¯:

V S¯ - 1, IS¯-1 = max R S¯ - 1, IS¯-1 , K S¯ - 1, IS¯-1 .

More generally, at age and schooling level s, this value is

V (s, Is) = max {R (s, Is) , K (s, Is)}

Y (s, s, 0, X) -C (s, X, Z (s))

= max

R (s, Is) ,

+

1

1 +

r

E

(V

(s

+

1,

Is+1

)

|

Is)

.74

Following the exposition of the reduced-form decision rule in Sect. 24.5.1, define the decision rule in terms of a first passage of the "index" R(s, Is) - K(s, Is),

D (s) = 1 [R(s, Is) - K(s, Is)  0, R(s - 1, Is-1) - K(s - 1, Is-1) < 0, . . . , R(1, I1) - K(1, I1) < 0] .

An individual stops at the schooling level at the first age where this index becomes positive. From data on stopping times, one can nonparametrically identify

74 This model allows no recall and is clearly a simplification of a more general model of schooling with option values. Instead of imposing the requirement that once a student drops out the student never returns, it would be useful to derive this property as a feature of the economic environment and the characteristics of individuals. Cunha et al. (2007) develop such conditions. In a more general model, different persons could drop out and return to school at different times as information sets are revised. This would create further option value beyond the option value developed in the text that arises from the possibility that persons who attain a given schooling level can attend the next schooling level in any future period. Implicit in this analysis of option values is the additional assumption that persons must work at the highest level of education for which they are trained. An alternative model allows individuals to work each period at the highest wage across all levels of schooling that they have attained. Such a model may be too extreme because it ignores the costs of switching jobs, especially at the higher educational levels where there may be a lot of job-specific human capital for each schooling level. A model with these additional features is presented in Heckman et al. (2007).

848

J.H. Abbring and J.J. Heckman

the conditional probability of stopping at s,





R(s, Is) - K(s, Is)  0,

Pr (S = s | X, Z) = Pr  R(s - 1, Is-1) - K(s - 1, Is-1) < 0, . . . , X, Z .

R(1, I1) - K(1, I1) < 0

HN use factor structure models based on the  introduced in Sect. 24.5.1 to define the information updating structure. Agents learn about different components of  as they evolve through life. The HN assumptions allow for the possibility that agents may know some or all the elements of  at a given age t regardless of whether or not they determine earnings at or before age t. Once known, they are not forgotten. As agents accumulate information, they revise their forecasts of their future earnings prospects at subsequent stages of the decision process. This affects their decision rules and subsequent choices. Thus HN allow for learning which can affect both pretreatment outcomes and posttreatment outcomes.75,76 All dynamic discretechoice models make some assumptions about the updating of information and any rigorous identification analysis of this class of models must test among competing specifications of information updating.
Variables unknown to the agent are integrated out by the agent in forming expectations over future outcomes. Variables known to the agent are treated as constants by the agents. They are integrated out by the econometrician to control for heterogeneity. These are separate operations except for special cases. In general, the econometrician knows less than what the agent knows. The econometrician seeks to identify the distributions of the variables in the agent information sets that are used by the agents to form their expectations as well as the distributions of variables known to the agent and treated as certain quantities by the agent but not known by the econometrician. Determining which elements belong in the agent's information set can be done using the methods exposited in Cunha et al. (2005) and Cunha and Heckman (2007) who consider testing what components of X, Z,  as well as  are in the agent's information set. We briefly discuss this issue at the end of the next section.77 HN establish semiparametric identification of the model assuming a given information structure. Determining the appropriate information structure

75 This type of learning about unobservables can be captured by HN's reduced-form model, but not by Abbring and Van den Berg's (2003b) single-spell mixed proportional hazards model. Their model does not allow for time-varying unobservables. Abbring and Van den Berg develop a multiple-spell model that allows for time-varying unobservables. Moreover, their nonparametric discussion of (NA) and randomization does not exclude the sequential revelation to the agent of a finite number of unobserved factors although they do not systematically develop such a model. 76 It is fruitful to distinguish models with exogenous arrival of information (so that information arrives at each age t independent of any actions taken by the agent) from information that arrives as a result of choices by the agent. The HN model is in the first class. The models of Miller (1984) or Pakes (1986) are in the second class. See our discussion in Sect. 24.5.5. 77 The HN model of learning is clearly very barebones. Information arrives exogenously across ages. In the factor model, all agents who advance to a stage get information about additional factors at that stage of their life cycles but the realizations of the factors may differ across persons.

24 Dynamic Policy Analysis

849

facing the agent and its evolution is an essential aspect of identifying any dynamic discrete-choice model.
Observe that agents with the same information variables It at age t have the same expectations of future returns, and the same continuation and stopping values. They make the same investment choices. Persons with the same ex ante reward, state and preference variables have the same ex ante distributions of stopping times. Ex post, stopping times may differ among agents with identical ex ante information. Controlling for It, future realizations of stopping times do not affect past rewards. This rules out the problem that the future can cause the past, which may happen in HN's reduced-form model. It enforces the (NA) condition of Abbring and Van den Berg. Failure to accurately model It produces failure of (NA).
HN establish semiparametric identification of their model without period-byperiod exclusion restrictions. Their analysis extends Theorems 24.1 and 24.2 to an explicit choice-theoretic setting. They use limit set arguments to identify the joint distributions of earnings (for each treatment time s across t) and any associated measurements that do not depend on the stopping time chosen. For each stopping time, they construct the means of earnings outcomes at each age and of the measurements and the joint distributions of the unobservables for earnings and measurements. Factor analyzing the joint distributions of the unobservables, under conditions specified in Carneiro et al. (2003), they identify the factor loadings, and nonparametrically identify the distributions of the factors and the independent components of the error terms in the earnings and measurement equations. Armed with this knowledge, they use choice data to identify the distribution of the components of the cost functions that are not directly observed. They construct the joint distributions of outcomes across stopping times. They also present conditions under which the interest rate r is identified.
In their model, analysts can distinguish period by period ex ante expected returns from ex post realizations by applying the analysis of Cunha et al. (2005). See the survey in Heckman, Lochner, and Todd (2006) and Sect. 2 of Abbring and Heckman (2007) for discussions of this approach. Because they link choices to outcomes through the factor structure assumption, they can also distinguish ex ante preference or cost parameters from their ex post realizations. Ex ante, agents may not know some components of  . Ex post, they do. All of the information about future rewards and returns is embodied in the information set It . Unless the time of treatment is known with perfect certainty, it cannot cause outcomes prior to its realization.
The analysis of HN is predicated on specification of agent information sets. These information sets should be carefully distinguished from those of the econometrician. Cunha et al. (2005) present methods for determining which components of future outcomes are in the information sets of agents at each age, It . If there are components unknown to the agent at age t, under rational expectations, agents form their value functions used to make schooling choices by integrating out the unknown components using the distributions of the variables in their information sets. Components that are known to the agent are treated as constants by the individual in forming the value function but as unknown variables by the econometrician and

850

J.H. Abbring and J.J. Heckman

their distribution is estimated. The true information set of the agent is determined from the set of possible specifications of the information sets of agents by picking the specification that best fits the data on choices and outcomes penalizing for parameter estimation. If neither the agent nor the econometrician knows a variable, the econometrician identifies the determinants of the distribution of the unknown variables that is used by the agent to form expectations. If the agent knows some variables, but the econometrician does not, the econometrician seeks to identify the distribution of the variables, but the agent treats the variables as known constants.
HN can identify all of the treatment parameters including the pairwise average treatment effect (ATE), the marginal treatment effect (MTE) for each transition (obtained by finding mean outcomes for individuals indifferent between transitions), all of the treatment on the treated and treatment on the untreated parameters and the population distribution of treatment effects by applying the analysis of Carneiro et al. (2003) and Cunha et al. (2005) to this model. Their analysis can be generalized to cover the case where there are vectors of contemporaneous outcome measures for different stopping times. See HN for proofs and details.78

24.5.3 Identification at Infinity
Heckman and Navarro (2007), and many other researchers, rely on identification at infinity to obtain their main identification results. Identification at infinity is required to identify the average treatment effect (ATE) using IV and control function methods and in the reduced-form discrete-time models developed in the previous subsections. While this approach is controversial, it is also testable. In any sample, one can plot the distributions of the probability of each state (exit time) to determine if the identification conditions are satisfied in any sample. Figure 24.1, presented by HN from the research of Heckman, Stixrud, and Urzua (2006), shows such plots for a six-state static schooling model that they estimate. To identify the marginal outcome distributions for each state, the support of the state probabilities should be the full unit interval. The identification at infinity condition is clearly not satisfied in their data.79 Only the empirical distribution of the state probability of graduating from a 4-year college comes even close to covering the full unit interval. Thus, their empirical results rely on parametric assumptions, and ATE and the marginal distributions of outcomes are nonparametrically nonidentified in their data without invoking additional structure.

78 The same limitations regarding independence assumptions between the regressors and errors discussed in the analysis of reduced forms apply to the structural model. 79 One can always argue that they are satisfied in an infinite sample that has not yet been realized. That statement has no empirical content.

24 Dynamic Policy Analysis

851

15

10

Sample frequency

5

0

0

.25

.5

.75

1

Probability

HS Droupout Graduate High School 2-year College Graduate
Source: Heckmana and Navarro (2007).

GEDs (High School Equivalents) Attend Some College 4-year College Graduate

Fig. 24.1 Sample distribution of schooling attainment probabilities for males from the National Longitudinal Survey of Youth

24.5.4 Comparing Reduced-Form and Structural Models
The reduced-form model analyzed in Sect. 24.5.1 is typical of many reduced-form statistical approaches within which it is difficult to make important conceptual distinctions. Because agent choice equations are not modeled explicitly, it is hard to use such frameworks to formally analyze the decision makers' expectations, costs of treatment, the arrival of information, the content of agent information sets and the consequences of the arrival of information for decisions regarding time to treatment as well as outcomes. Key behavioral assumptions are buried in statistical assumptions. It is difficult to distinguish ex post from ex ante valuations of outcomes in the reduced-form models. Cunha et al. (2005); Carneiro et al. (2003) and Cunha and Heckman (2007, 2008) present analyses that distinguish ex ante anticipations from ex post realizations.80 In reduced-form models, it is difficult to make the distinction between private evaluations and preferences (e.g., "costs" as defined in this section) from objective outcomes (the Y variables).
Statistical and reduced-form econometric approaches to analyzing dynamic counterfactuals appeal to uncertainty to motivate the stochastic structure of models. They do not explicitly characterize how agents respond to uncertainty or make treatment choices based on the arrival of new information (see Robins, 1989, 1997; Lok, 2007; Gill and Robins, 2001; Abbring and Van den Berg, 2003b; and Van der Laan and Robins, 2003). The structural approach surveyed in Sect. 24.5.2 and developed by
80 See the summary of this literature in Heckman et al. (2006).

852

J.H. Abbring and J.J. Heckman

HN allows for a clear treatment of the arrival of information, agent expectations, and the effects of new information on choice and its consequences. In an environment of imperfect certainty about the future, it rules out the possibility of the future causing the past once the effects of agent information are controlled for.
The structural model developed by HN allows agents to learn about new factors (components of  ) as they proceed sequentially through their life cycles. It also allows agents to learn about other components of the model (see Cunha et al., 2005). Agent anticipations of when they will stop and the consequences of alternative stopping times can be sequentially revised. Agent anticipated payoffs and stopping times are sequentially revised as new information becomes available. The mechanism by which agents revise their anticipations is modeled and identified. See Cunha et al. (2005, 2006); Cunha and Heckman (2007, 2008) and Abbring and Heckman (2007) for further discussion of these issues and Heckman et al. (2006) for a partial survey of recent developments in the literature.
The clearest interpretation of the models in the statistical literature on dynamic treatment effects is as ex post selection-corrected analyses of distributions of events that have occurred. In a model of perfect certainty, where ex post and ex ante choices and outcomes are identical, the reduced-form approach can be interpreted as approximating clearly specified choice models. In a more general analysis with information arrival and agent updating of information sets, the nature of the approximation is less clear cut. Thus, the current reduced-form literature is unclear as to which agent decision-making processes and information arrival assumptions justify the conditional sequential randomization assumptions widely used in the dynamic treatment effect literature (see, e.g., Gill and Robins, 2001; Lechner and Miquel, 2002; Lok, 2007; Robins, 1989, 1997; Van der Laan and Robins, 2003). Section 24.3.2.2 provides some insight by highlighting the connection to the conditional-independence assumption often employed in the structural dynamic discrete-choice literature (see Rust, 1987; and the survey in Rust, 1994). Reducedform approaches are not clear about the source of the unobservables and their relationship with conditioning variables. It would be a valuable exercise to exhibit which structural models are approximated by various reduced-form models. In the structural analysis, this specification emerges as part of the analysis, as our discussion of the stochastic properties of the unobservables presented in the preceding section makes clear.
The HN analysis of both structural and reduced-form models relies heavily on limit set arguments. They solve the selection problem in limit sets. The dynamic matching models of Gill and Robins (2001) and Lok (2007) solve the selection problem by invoking recursive conditional independence assumptions. In the context of the models of HN, they assume that the econometrician knows the  or can eliminate the effect of  on estimates of the model by conditioning on a suitable set of variables. The HN analysis entertains the possibility that analysts know less than the agents they study. It allows for some of the variables that would make matching valid to be unobservable. As we have noted in early subsections, versions of recursive conditional independence assumptions are also used in the dynamic discretechoice literature (see the survey in Rust, 1994). The HN factor models allow analysts

24 Dynamic Policy Analysis

853

to construct the joint distribution of outcomes across stopping times. This feature is missing from the statistical treatment effect literature.
Both HN's structural and reduced-form models of treatment choice are stopping time models. Neither model allows for multiple entry into and exit from treatment, even though agents in these models would like to reverse their treatment decisions for some realizations of their index if this was not too costly (or, in the case of the reduced-form model, if the index thresholds for returning would not be too low).81 Cunha, Heckman, and Navarro (2007) derive conditions on structural stopping models from a more basic model that entertains the possibility of return from dropout states but which nonetheless exhibits the stopping time property. The HN identification strategy relies on the nonrecurrent nature of treatment. Their identification strategy of using limit sets can be applied to a recurrent model provided that analysts confine attention to subsets of (X, Z) such that in those subsets the probability of recurrence is zero.

24.5.5 A Short Survey of Dynamic Discrete-Choice Models
Rust (1994) presents a widely cited nonparametric nonidentification theorem for dynamic discrete-choice models. It is important to note the restrictive nature of his negative results. He analyzes a recurrent state infinite horizon model in a stationary environment. He does not use any exclusion restrictions or cross outcome-choice restrictions. He uses a general utility function. He places no restrictions on periodspecific utility functions such as concavity or linearity nor does he specify restrictions connecting preferences and outcomes. One can break Rust's nonidentification result with additional information.
Magnac and Thesmar (2002) present an extended comment on Rust's analysis including positive results for identification when the econometrician knows the distributions of unobservables, assumes that unobservables enter period-specific utility functions in an additively separable way and is willing to specify functional forms of utility functions or other ingredients of the model, as do Pakes (1986), Keane and Wolpin (1997), Eckstein and Wolpin (1999) and Hotz and Miller (1988, 1993). Magnac and Thesmar (2002) also consider the case where one state (choice) is absorbing (as do Hotz and Miller, 1993) and where the value functions are known at the terminal age (T¯ ) (as do Keane and Wolpin, 1997 and Belzil and Hansen, 2002). In HN, each treatment time is an absorbing state. In a separate analysis, Magnac and Thesmar consider the case where unobservables from the point of view of the econometrician are correlated over time (or age t) and choices (s) under the assumption that the distribution of the unobservables is known. They also consider the case where exclusion restrictions are available. Throughout their analysis, they maintain
81 Recall that treatment occurs if the index turns positive. If there are costs to reversing this decision, agents would only reverse their decision if the index falls below some negative threshold. The stopping time assumption is equivalent to the assumption that the costs of reversal are prohibitively large, or that the corresponding threshold is at the lower end of the support of the index.

854

J.H. Abbring and J.J. Heckman

that the distribution of the unobservables is known both by the agent and the econometrician.
HN provide semiparametric identification of a finite-horizon finite-state model with an absorbing state with semiparametric specifications of reward and cost functions.82 Given that rewards are in value units, the scale of their utility function is fixed as they also are in models of profit-maximizing firms. Choices are not invariant to arbitrary affine transformations so that one source of nonidentifiability in Rust's analysis is eliminated. They can identify the error distributions nonparametrically given their factor structure. They do not have to assume either the functional form of the unobservables or knowledge of the entire distribution of unobservables.
HN present a fully specified structural model of choices and outcomes motivated by, but not identical to, the analyses of Keane and Wolpin (1994, 1997) and Eckstein and Wolpin (1999). In their setups, outcome and cost functions are parametrically specified. Their states are recurrent while those of HN are absorbing. In the HN model, once an agent drops out of school, the agent does not return. In the Keane­Wolpin model, an agent who drops out can return. Keane and Wolpin do not establish identification of their model, whereas HN establish semiparametric identification of their model. They analyze models with more general times series processes for unobservables. In both the HN and Keane­Wolpin frameworks, agents learn about unobservables. In the Keane­Wolpin framework, such learning is about temporally independent shocks that do not affect agent expectations about returns relevant to possible future choices. The information just affects the opportunity costs of current choices. In the HN framework, learning affects agent expectations about future returns as well as current opportunity costs.
The HN model extends previous work by Carneiro et al. (2003); Cunha and Heckman (2007, 2008) and Cunha et al. (2005, 2006) by considering explicit multiperiod dynamic models with information updating. They consider one-shot decision models with information updating and associated outcomes.
Their analysis is related to that of Taber (2000). Like Cameron and Heckman (1998), both HN and Taber use identification-in-the-limit arguments.83 Taber considers identification of a two period model with a general utility function, whereas in Sect. 24.5.2, we discuss how HN consider identification of a specific form of the utility function (an earnings function) for a multiperiod maximization problem. As in HN, Taber allows for the sequential arrival of information. His analysis is based on conventional exclusion restrictions, but the analysis of HN is not. They use outcome data in conjunction with the discrete dynamic choice data to exploit cross-equation restrictions, whereas Taber does not.
The HN treatment of serially correlated unobservables is more general than any discussion that appears in the current dynamic discrete choice and dynamic treatment effect literature. They do not invoke the strong sequential conditional independence assumptions used in the dynamic treatment effect literature in statistics (Gill

82 Although their main theorems are for additively separable reward and cost functions, it appears that additive separability can be relaxed using the analysis of Matzkin (2003). 83 Pakes and Simpson (1989) sketch a proof of identification of a model of the option values of patents that is based on limit sets for an option model.

24 Dynamic Policy Analysis

855

and Robins, 2001; Lechner and Miquel, 2002; Lok, 2007; Robins, 1989, 1997), nor do they invoke the closely related conditional temporal independence of unobserved state variables given observed state variables invoked by Rust (1987); Hotz and Miller (1988, 1993); Manski (1993) and Magnac and Thesmar (2002) (in the first part of their paper) or the independence assumptions invoked by Wolpin (1984).84 HN allow for more general time series dependence in the unobservables than is entertained by Pakes (1986), Keane and Wolpin (1997) or Eckstein and Wolpin (1999).85
Like Miller (1984) and Pakes (1986), HN explicitly model, identify and estimate agent learning that affects expected future returns.86 Pakes and Miller assume functional forms for the distributions of the error process and for the serial correlation pattern about information updating and time series dependence. The HN analysis of the unobservables is nonparametric and they estimate, rather than impose, the stochastic structure of the information updating process.
Virtually all papers in the literature, including the HN analysis, invoke rational expectations. An exception is the analysis of Manski (1993) who replaces rational expectations with a synthetic cohort assumption that choices and outcomes of one group can be observed (and acted on) by a younger group. This assumption is more plausible in stationary environments and excludes any temporal dependence in unobservables. In recent work, Manski (2004) advocates use of elicited expectations as an alternative to the synthetic cohort approach.
While HN use rational expectations, they estimate, rather than impose the structure of agent information sets. Miller (1984), Pakes (1986), Keane and Wolpin (1997), and Eckstein and Wolpin (1999) assume that they know the law governing the evolution of agent information up to unknown parameters.87 Following the procedure presented in Cunha and Heckman (2007, 2008); Cunha et al. (2005, 2006) and Navarro (2005), HN can test for which factors ( ) appear in agent information sets at different stages of the life cycle and they identify the distributions of the unobservables nonparametrically.
The HN analysis of dynamic treatment effects is comparable, in some aspects, to the recent continuous-time event-history approach of Abbring and Van den Berg (2003b) previously analyzed. Those authors build a continuous-time model of counterfactuals for outcomes that are durations. They model treatment assignment times using a continuous-time duration model.
84 Manski (1993) and Hotz and Miller (1993) use a synthetic cohort effect approach that assumes that young agents will follow the transitions of contemporaneous older agents in making their lifecycle decisions. Manski and Hotz and Miller exclude any temporally dependent unobservables from their models. The synthetic cohort approach has been widely used in labor economics at least since Mincer (1974). See Ghez and Becker (1975), MaCurdy (1981) and Mincer (1974) for applications of the synthetic cohort approach. For empirical evidence against the assumption that the earnings of older workers are a reliable guide to the earnings of younger workers in models of earnings and schooling choices for recent cohorts of workers, see Heckman et al. (2006). 85 Rust (1994) provides a clear statement of the stochastic assumptions underlying the dynamic discrete-choice literature up to the date of his survey. 86 As previously noted, the previous literature assumes learning only about current costs. 87 They specify a priori particular processes of information arrival as well as which components of the unobservables agents know and act on, and which components they do not.

856

J.H. Abbring and J.J. Heckman

The HN analysis is in discrete time and builds on previous work by Heckman (1981a,c) on heterogeneity and state dependence that identifies the causal effect of employment (or unemployment) on future employment (or unemployment).88 They model time to treatment and associated vectors of outcome equations that may be discrete, continuous or mixed discrete-continuous. In a discrete-time setting, they are able to generate a variety of distributions of counterfactuals and economically motivated parameters. They allow for heterogeneity in responses to treatment that has a general time series structure.
As noted in Sect. 24.5.4, Abbring and Van den Berg (2003b) do not identify explicit agent information sets as HN do in their paper and as is done in Cunha et al. (2005), and they do not model learning about future rewards. Their outcomes are restricted to be continuous-time durations. The HN framework is formulated in discrete time, which facilitates the specification of richer unobserved and observed covariate processes than those entertained in the continuous-time framework of Abbring and Van den Berg (2003b). It is straightforward to attach a vector of treatment outcomes in the HN model that includes continuous outcomes, discrete outcomes and durations expressed as binary strings.89 At a practical level, the approach often can produce very fine-grained descriptions of continuous-time phenomena by using models with many finite periods. Clearly, a synthesis of the eventhistory approach with the HN approach would be highly desirable. That would entail taking continuous-time limits of the discrete-time models. It is a task that awaits completion.
Flinn and Heckman (1982) utilize information on stopping times and associated wages to derive cross-equation restrictions to partially identify an equilibrium job search model for a stationary economic environment where agents have an infinite horizon. They establish that the model is nonparametrically nonidentified. Their analysis shows that use of outcome data in conjunction with data on stopping times is not sufficient to secure nonparametric identification of a dynamic discrete-choice model, even when the reward function is linear in outcomes unlike the reward functions in Rust (1987) and Magnac and Thesmar (2002). Parametric restrictions can break their nonidentification result. Abbring and Campbell (2005) exploit such restrictions, together with cross-equation restrictions on stopping times and noisy outcome measures, to prove identification of an infinite-horizon model of firm survival and growth with entrepreneurial learning. Alternatively, nonstationarity arising from finite horizons can break their nonidentification result (see Wolpin, 1987). The HN analysis exploits the finite-horizon backward-induction structure of our model in conjunction with outcome data to secure identification and does not rely on arbitrary period by period exclusion restrictions. They substantially depart from the assumptions maintained in Rust's nonidentification theorem (1994). They achieve identification by using cross-equation restrictions, linearity of preferences

88 Heckman and Borjas (1980) investigate these issues in a continuous-time duration model. See also Heckman and MaCurdy (1980). 89 Abbring (2008) considers nonparametric identification of mixed semi-Markov event-history models that extends his work with Van den Berg. See Sect. 24.4.

24 Dynamic Policy Analysis

857

and additional measurements, and exploiting the structure of their finite horizon nonrecurrent model. Nonstationarity of regressors greatly facilitates identification by producing both exclusion and curvature restrictions which can substitute for standard exclusion restrictions.

24.6 Conclusion
This paper has surveyed recent approaches to using panel data to evaluate policies. We have compared and contrasted the statistical dynamic treatment approach based on sequential conditional independence assumptions that generalize matching to approaches developed in econometrics. We compared and contrasted a continuoustime event-history approach developed by Abbring and Van den Berg (2003b) to discrete-time reduced-form and structural models developed by Heckman and Navarro (2007), and Cunha et al. (2005).

References
Aakvik, A., J. J. Heckman, and E. J. Vytlacil (2005). Estimating treatment effects for discrete outcomes when responses to treatment vary: An application to Norwegian vocational rehabilitation programs. Journal of Econometrics 125(1­2), 15­51.
Aalen, O. O. and H. K. Gjessing (2004, December). Survival models based on the OrnsteinUhlenbeck process. Lifetime Data Analysis 10(4), 407­423.
Abbring, J. H. (2002, February). Stayers versus defecting movers: A note on the identification of defective duration models. Economics Letters 74(3), 327­331.
Abbring, J. H. (2003). Dynamic econometric program evaluation. Discussion Paper 804, IZA, Bonn. Paper prepared for the H. Theil Memorial Conference, Amsterdam, 16­18 August 2002.
Abbring, J. H. (2007). Mixed hitting-time models. Discussion Paper 07-57/3, Tinbergen Institute, Amsterdam.
Abbring, J. H. (2008). The event-history approach to program evaluation. In D. Millimet, J. Smith, and E. Vytlacil (Eds.), Modeling and Evaluating Treatment Effects in Econometrics, Volume 21 of Advances in Econometrics. Oxford: Elsevier Science, pp. 33­55.
Abbring, J. H. and J. R. Campbell (2005). A firm's first year. Discussion Paper 05-046/3, Tinbergen Institute, Amsterdam.
Abbring, J. H. and J. J. Heckman (2007). Econometric evaluation of social programs, part III: Distributional treatment effects, dynamic treatment effects, dynamic discrete choice, and general equilibrium policy evaluation. In J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, Volume 6, pp. 5145­5303, Amsterdam: Elsevier.
Abbring, J. H. and G. J. Van den Berg (2003a, September). The identifiability of the mixed proportional hazards competing risks model. Journal of the Royal Statistical Association, Series B 65(3), 701­710.
Abbring, J. H. and G. J. Van den Berg (2003b, September). The nonparametric identification of treatment effects in duration models. Econometrica 71(5), 1491­1517.
Abbring, J. H. and G. J. Van den Berg (2003c). A simple procedure for inference on treatment effects in duration models. Discussion Paper 2003:19, IFAU, Uppsala.

858

J.H. Abbring and J.J. Heckman

Abbring, J. H. and G. J. Van den Berg (2004, January). Analyzing the effect of dynamically assigned treatments using duration models, binary treatment models, and panel data models. Empirical Economics 29(1), 5­20.
Abbring, J. H. and G. J. Van den Berg (2005). Social experiments and instrumental variables with duration outcomes. Discussion Paper 05-047/3, Tinbergen Institute, Amsterdam.
Abbring, J. H., G. J. Van den Berg, and J. C. Van Ours (2005, July). The effect of unemployment insurance sanctions on the transition rate from unemployment to employment. Economic Journal 115(505), 602­630.
Aldrich, J. (1989, January). Autonomy. Oxford Economic Papers 41(1), 15­34. Andersen, P. K., Ø. Borgan, R. Gill, and N. Keiding (1993). Statistical Models Based on Counting
Processes. New York: Springer-Verlag. Belzil, C. and J. Hansen (2002, September). Unobserved ability and the return to schooling. Econo-
metrica 70(5), 2075­2091. Black, D. A., J. A. Smith, M. C. Berger, and B. J. Noel (2003, September). Is the threat of reem-
ployment services more effective than the services themselves? Evidence from random assignment in the UI system. American Economic Review 93(4), 1313­1327. Bonnal, L., D. Fouge`re, and A. Se´randon (1997, October). Evaluating the impact of French employment policies on individual labour market histories. Review of Economic Studies 64(4), 683­713. Cameron, S. V. and J. J. Heckman (1998, April). Life cycle schooling and dynamic selection bias: Models and evidence for five cohorts of American males. Journal of Political Economy 106(2), 262­333. Card, D. and D. G. Sullivan (1988, May). Measuring the effect of subsidized training programs on movements in and out of employment. Econometrica 56(3), 497­530. Carneiro, P., K. Hansen, and J. J. Heckman (2001, Fall). Removing the veil of ignorance in assessing the distributional impacts of social policies. Swedish Economic Policy Review 8(2), 273­301. Carneiro, P., K. Hansen, and J. J. Heckman (2003, May). Estimating distributions of treatment effects with an application to the returns to schooling and measurement of the effects of uncertainty on college choice. International Economic Review 44(2), 361­422. Chan, T. Y. and B. H. Hamilton (2006). Learning, private information and the economic evaluation of randomized experiments. Journal of Political Economy 114(6), 997­1040. Cunha, F. and J. J. Heckman (2007). The evolution of earnings risk in the US economy. Presented at the 9th World Congress of the Econometric Society, London 2005, under revision. Cunha, F. and J. J. Heckman (2008). A framework for the analysis of inequality. Macroeconomic Dynamics. Forthcoming. Cunha, F., J. J. Heckman, and S. Navarro (2005, April). Separating uncertainty from heterogeneity in life cycle earnings, The 2004 Hicks Lecture. Oxford Economic Papers 57(2), 191­261. Cunha, F., J. J. Heckman, and S. Navarro (2006). Counterfactual analysis of inequality and social mobility. In S. L. Morgan, D. B. Grusky, and G. S. Fields (Eds.), Mobility and Inequality: Frontiers of Research in Sociology and Economics, Chap. 4, pp. 290­348. Stanford, CA: Stanford University Press. Cunha, F., J. J. Heckman, and S. Navarro (2007). The identification and economic content of ordered choice models with stochastic cutoffs. International Economic Review. 48(4), 1273­1309. Eberwein, C., J. C. Ham, and R. J. LaLonde (1997, October). The impact of being offered and receiving classroom training on the employment histories of disadvantaged women: Evidence from experimental data. Review of Economic Studies 64(4), 655­682. Eckstein, Z. and K. I. Wolpin (1999, November). Why youths drop out of high school: The impact of preferences, opportunities, and abilities. Econometrica 67(6), 1295­1339. Falmagne, J.-C. (1985). Elements of Psychophysical Theory. Oxford Psychology Series No. 6. New York: Oxford University Press.

24 Dynamic Policy Analysis

859

Fitzenberger, B., A. Osikominu, and R. Vo¨lter (2006, May). Get training or wait? Long-run employment effects of training programs for the unemployed in West Germany, Discussion paper 2121, IZA, Bonn.
Fleming, T. R. and D. P. Harrington (1991). Counting Processes and Survival Analysis. New York: Wiley.
Flinn, C. and J. J. Heckman (1982, January). New methods for analyzing structural models of labor force dynamics. Journal of Econometrics 18(1), 115­68.
Florens, J.-P. and M. Mouchart (1982). A note on noncausality. Econometrica 50, 583­591. Freedman, D. A. (2004, August). On specifying graphical models for causation and the identifica-
tion problem. Evaluation Review 28(4), 267­293. Freund, J. E. (1961, December). A bivariate extension of the exponential distribution. Journal of
the American Statistical Association 56(296), 971­977. Frisch, R. (1938). Autonomy of economic relations. Paper given at League of Nations. Reprinted in
D.F. Hendry and M.S. Morgan (1995), The Foundations of Econometric Analysis, Cambridge: Cambridge University Press. Ghez, G. R. and G. S. Becker (1975). The Allocation of Time and Goods over the Life Cycle. New York: National Bureau of Economic Research. Gill, R. D. and J. M. Robins (2001, December). Causal inference for complex longitudinal data: The continuous case. Annals of Statistics 29(6), 1785­1811. Granger, C. W. J. (1969, August). Investigating causal relations by econometric models and crossspectral methods. Econometrica 37(3), 424­438. Gritz, R. M. (1993, May­June). The impact of training on the frequency and duration of employment. Journal of Econometrics 57(1­3), 21­51. Grubb, D. (2000). Eligibility criteria for unemployment benefits. In OECD Economic Studies, Special issue: Making Work Pay, Number 31, pp. 147­184. OECD. Haavelmo, T. (1943, January). The statistical implications of a system of simultaneous equations. Econometrica 11(1), 1­12. Ham, J. C. and R. J. LaLonde (1996, January). The effect of sample selection and initial conditions in duration models: Evidence from experimental data on training. Econometrica 64(1), 175­205. Hansen, L. P. and T. J. Sargent (1980, February). Formulating and estimating dynamic linear rational expectations models. Journal of Economic Dynamics and Control 2(1), 7­46. Heckman, J. J. (1974, July). Shadow prices, market wages, and labor supply. Econometrica 42(4), 679­694. Heckman, J. J. (1981a). Heterogeneity and state dependence. In S. Rosen (Ed.), Studies in Labor Markets, National Bureau of Economic Research, pp. 91­139. Chicago: University of Chicago Press. Heckman, J. J. (1981b). The incidental parameters problem and the problem of initial conditions in estimating a discrete time-discrete data stochastic process and some Monte Carlo evidence. In C. Manski and D. McFadden (Eds.), Structural Analysis of Discrete Data with Econometric Applications, pp. 179­85. Cambridge, MA: MIT Press. Heckman, J. J. (1981c). Statistical models for discrete panel data. In C. Manski and D. McFadden (Eds.), Structural Analysis of Discrete Data with Econometric Applications, pp. 114­178. Cambridge, MA: MIT Press. Heckman, J. J. (1990, May). Varieties of selection bias. American Economic Review 80(2), 313­318. Heckman, J. J. (2005a). The scientific model of causality. Unpublished manuscript, University of Chicago, Department of Economics. Heckman, J. J. (2005b, August). The scientific model of causality. Sociological Methodology 35(1), 1­97. Heckman, J. J. and G. J. Borjas (1980, August). Does unemployment cause future unemployment? Definitions, questions and answers from a continuous time model of heterogeneity and state dependence. Economica 47(187), 247­283. Special Issue on Unemployment.

860

J.H. Abbring and J.J. Heckman

Heckman, J. J. and B. E. Honore´ (1989, June). The identifiability of the competing risks model. Biometrika 76(2), 325­330.
Heckman, J. J. and B. E. Honore´ (1990, September). The empirical content of the Roy model. Econometrica 58(5), 1121­1149.
Heckman, J. J., R. J. LaLonde, and J. A. Smith (1999). The economics and econometrics of active labor market programs. In O. Ashenfelter and D. Card (Eds.), Handbook of Labor Economics, Volume 3A, Chap. 31, pp. 1865­2097. New York: North-Holland.
Heckman, J. J., L. J. Lochner, and C. Taber (1998, January). Explaining rising wage inequality: Explorations with a dynamic general equilibrium model of labor earnings with heterogeneous agents. Review of Economic Dynamics 1(1), 1­58.
Heckman, J. J., L. J. Lochner, and P. E. Todd (2006). Earnings equations and rates of return: The Mincer equation and beyond. In E. A. Hanushek and F. Welch (Eds.), Handbook of the Economics of Education, Chap. 7, pp. 307­458. Amsterdam: North-Holland.
Heckman, J. J. and T. E. MaCurdy (1980, January). A life cycle model of female labour supply. Review of Economic Studies 47(1), 47­74.
Heckman, J. J. and S. Navarro (2004, February). Using matching, instrumental variables, and control functions to estimate economic choice models. Review of Economics and Statistics 86(1), 30­57.
Heckman, J. J. and S. Navarro (2005). Empirical estimates of option values of education and information sets in a dynamic sequential choice model. Unpublished manuscript, University of Chicago, Department of Economics.
Heckman, J. J. and S. Navarro (2007, February). Dynamic discrete choice and dynamic treatment effects. Journal of Econometrics 136(2), 341­396.
Heckman, J. J. and B. S. Singer (1984, January­February). Econometric duration analysis. Journal of Econometrics 24(1­2), 63­132.
Heckman, J. J. and B. S. Singer (1986). Econometric analysis of longitudinal data. In Z. Griliches and M. D. Intriligator (Eds.), Handbook of Econometrics, Volume 3, Chap. 29, pp. 1690­1763. Amsterdam: North-Holland.
Heckman, J. J. and J. A. Smith (1998). Evaluating the welfare state. In S. Strom (Ed.), Econometrics and Economic Theory in the Twentieth Century: The Ragnar Frisch Centennial Symposium, pp. 241­318. New York: Cambridge University Press.
Heckman, J. J., J. Stixrud, and S. Urzua (2006, July). The effects of cognitive and noncognitive abilities on labor market outcomes and social behavior. Journal of Labor Economics 24(3), 411­482.
Heckman, J. J. and C. Taber (1994). Econometric mixture models and more general models for unobservables in duration analysis. Statistical Methods in Medical Research 3(3), 279­299.
Heckman, J. J., J. L. Tobias, and E. J. Vytlacil (2001, October). Four parameters of interest in the evaluation of social programs. Southern Economic Journal 68(2), 210­223.
Heckman, J. J., J. L. Tobias, and E. J. Vytlacil (2003, August). Simple estimators for treatment parameters in a latent variable framework. Review of Economics and Statistics 85(3), 748­754.
Heckman, J. J., S. Urzua, and E. J. Vytlacil (2006). Understanding instrumental variables in models with essential heterogeneity. Review of Economics and Statistics 88(3), 389­432.
Heckman, J. J., S. Urzua, and G. Yates (2007). The identification and estimation of option values in a model with recurrent states. Unpublished manuscript, University of Chicago, Department of Economics.
Heckman, J. J. and E. J. Vytlacil (1999, April). Local instrumental variables and latent variable models for identifying and bounding treatment effects. Proceedings of the National Academy of Sciences 96, 4730­4734.
Heckman, J. J. and E. J. Vytlacil (2001). Causal parameters, treatment effects and randomization. Unpublished manuscript, University of Chicago, Department of Economics.
Heckman, J. J. and E. J. Vytlacil (2005, May). Structural equations, treatment effects and econometric policy evaluation. Econometrica 73(3), 669­738.

24 Dynamic Policy Analysis

861

Heckman, J. J. and E. J. Vytlacil (2007a). Econometric evaluation of social programs, part I: Causal models, structural models and econometric policy evaluation. In J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, Volume 6B, pp. 4779­4874, Amsterdam: Elsevier.
Heckman, J. J. and E. J. Vytlacil (2007b). Econometric evaluation of social programs, part II: Using the marginal treatment effect to organize alternative economic estimators to evaluate social programs and to forecast their effects in new environments. In J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, Volume 6B, pp. 4875­5144, Amsterdam: Elsevier.
Hendry, D. F. and M. S. Morgan (1995). The Foundations of Econometric Analysis. New York: Cambridge University Press.
Holland, P. W. (1986, December). Statistics and causal inference. Journal of the American Statistical Association 81(396), 945­960.
Honore´, B. E. (1993, January). Identification results for duration models with multiple spells. Review of Economic Studies 60(1), 241­246.
Honore´, B. E. and A. Lewbel (2002, September). Semiparametric binary choice panel data models without strictly exogenous regressors. Econometrica 70,(5), 2053­2063.
Hotz, V. J. and R. A. Miller (1988, January). An empirical analysis of life cycle fertility and female labor supply. Econometrica 56(1), 91­118.
Hotz, V. J. and R. A. Miller (1993, July). Conditional choice probabilities and the estimation of dynamic models. Review of Economic Studies 60(3), 497­529.
Hurwicz, L. (1962). On the structural form of interdependent systems. In E. Nagel, P. Suppes, and A. Tarski (Eds.), Logic, Methodology and Philosophy of Science, pp. 232­239. Palo Alto: Stanford University Press.
Kalbfleisch, J. D. and R. L. Prentice (1980). The Statistical Analysis of Failure Time Data. New York: Wiley.
Keane, M. P. and K. I. Wolpin (1994, November). The solution and estimation of discrete choice dynamic programming models by simulation and interpolation: Monte Carlo evidence. The Review of Economics and Statistics 76(4), 648­672.
Keane, M. P. and K. I. Wolpin (1997, June). The career decisions of young men. Journal of Political Economy 105(3), 473­522.
Keiding, N. (1999, September). Event history analysis and inference from observational epidemiology. Statistics in Medicine 18(17­18), 2353­2363.
Lancaster, T. (1979, July). Econometric methods for the duration of unemployment. Econometrica 47(4), 939­956.
Leamer, E. E. (1985, Spring). Vector autoregressions for causal inference? Carnegie-Rochester Conference Series on Public Policy 22, 255­303.
Lechner, M. and R. Miquel (2002). Identification of effects of dynamic treatments by sequential conditional independence assumptions. Discussion paper, University of St. Gallen, Department of Economics.
Lewbel, A. (2000, July). Semiparametric qualitative response model estimation with unknown heteroscedasticity or instrumental variables. Journal of Econometrics 97(1), 145­177.
Lok, J. (2007). Statistical modelling of causal effects in continuous time. Annals of Statistics. Forthcoming.
MaCurdy, T. E. (1981, December). An empirical model of labor supply in a life-cycle setting. Journal of Political Economy 89(6), 1059­1085.
Magnac, T. and D. Thesmar (2002, March). Identifying dynamic discrete decision processes. Econometrica 70(2), 801­816.
Manski, C. F. (1988, September). Identification of binary response models. Journal of the American Statistical Association 83(403), 729­738.
Manski, C. F. (1993, July). Dynamic choice in social settings: Learning from the experiences of others. Journal of Econometrics 58(1­2), 121­136.
Manski, C. F. (2004, September). Measuring expectations. Econometrica 72(5), 1329­1376. Matzkin, R. L. (1992, March). Nonparametric and distribution-free estimation of the binary thresh-
old crossing and the binary choice models. Econometrica 60(2), 239­270.

862

J.H. Abbring and J.J. Heckman

Matzkin, R. L. (1993, July). Nonparametric identification and estimation of polychotomous choice models. Journal of Econometrics 58(1­2), 137­168.
Matzkin, R. L. (1994). Restrictions of economic theory in nonparametric methods. In R. Engle and D. McFadden (Eds.), Handbook of Econometrics, Volume 4, pp. 2523­58. New York: North-Holland.
Matzkin, R. L. (2003, September). Nonparametric estimation of nonadditive random functions. Econometrica 71(5), 1339­1375.
Meyer, B. D. (1990, July). Unemployment insurance and unemployment spells. Econometrica 58(4), 757­782.
Meyer, B. D. (1996, January). What have we learned from the Illinois reemployment bonus experiment? Journal of Labor Economics 14(1), 26­51.
Miller, R. A. (1984, December). Job matching and occupational choice. Journal of Political Economy 92(6), 1086­1120.
Mincer, J. (1974). Schooling, Experience and Earnings. New York: Columbia University Press for National Bureau of Economic Research.
Mortensen, D. T. (1977, July). Unemployment insurance and job search decisions. Industrial and Labor Relations Review 30(4), 505­517.
Mortensen, D. T. (1986). Job search and labor market analysis. In O. Ashenfelter and P. R. C. Layard (Eds.), Handbook of Labor Economics, Volume 2 of Handbooks in Economics, pp. 849­919. New York: Elsevier Science.
Murphy, S. A. (2003, May). Optimal dynamic treatment regimes. Journal of the Royal Statistical Association, Series B 65(2), 331­366.
Navarro, S. (2004). Semiparametric identification of factor models for counterfactual analysis. Unpublished manuscript, University of Chicago, Department of Economics.
Navarro, S. (2005). Understanding Schooling: Using Observed Choices to Infer Agent's Information in a Dynamic Model of Schooling Choice When Consumption Allocation is Subject to Borrowing Constraints. Ph.D. Dissertation, University of Chicago, Chicago, IL.
Neyman, J. (1923). On the application of probability theory to agricultural experiments. Essay on principles. Roczniki Nauk Rolniczych 10, 1­51. in Polish; edited and translated version of Sect. 9 by D.M. Dabrowska and T.P Speed (1990), Statistical Science, 5, 465­472.
Pakes, A. (1986, July). Patents as options: Some estimates of the value of holding european patent stocks. Econometrica 54(4), 755­784.
Pakes, A. and M. Simpson (1989). Patent renewal data. Brookings Papers on Economic Activity (Special Issue), 331­401.
Pearl, J. (2000). Causality. Cambridge, England: Cambridge University Press. Quandt, R. E. (1958, December). `The estimation of the parameters of a linear regression sys-
tem obeying two separate regimes'. Journal of the American Statistical Association 53(284), 873­880. Quandt, R. E. (1972, June). `A new approach to estimating switching regressions'. Journal of the American Statistical Association 67(338), 306­310. Ridder, G. (1986, April). An event history approach to the evaluation of training, recruitment and employment programmes. Journal of Applied Econometrics 1(2), 109­126. Ridder, G. (1990, April). The non-parametric identification of generalized accelerated failure-time models. Review of Economic Studies 57(2), 167­181. Robins, J. M. (1989). The analysis of randomized and non-randomized aids treatment trials using a new approach to causal inference in longitudinal studies. In L. Sechrest, H. Freeman, and A. Mulley (Eds.), Health Services Research Methodology: A Focus on AIDS, pp. 113­159. Rockville, MD: U.S. Department of Health and Human Services, National Center for Health Services Research and Health Care Technology Assessment. Robins, J. M. (1997). Causal inference from complex longitudinal data. In M. Berkane (Ed.), Latent Variable Modeling and Applications to Causality. Lecture Notes in Statistics, pp. 69­117. New York: Springer-Verlag. Rosenzweig, M. R. and K. I. Wolpin (2000, December). Natural "natural experiments" in economics. Journal of Economic Literature 38(4), 827­874.

24 Dynamic Policy Analysis

863

Roy, A. (1951, June). Some thoughts on the distribution of earnings. Oxford Economic Papers 3(2), 135­146.
Rubin, D. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology 66, 688­701.
Rubin, D. B. (1986). Statistics and causal inference: Comment: Which ifs have causal answers. Journal of the American Statistical Association 81(396), 961­962.
Rust, J. (1987, September). Optimal replacement of GMC bus engines: An empirical model of Harold Zurcher. Econometrica 55(5), 999­1033.
Rust, J. (1994). Structural estimation of Markov decision processes. In R. Engle and D. McFadden (Eds.), Handbook of Econometrics, pp. 3081­3143. New York: North-Holland.
Sims, C. A. (1972, September). Money, income, and causality. American Economic Review 62(4), 540­552.
Taber, C. R. (2000, June). Semiparametric identification and heterogeneity in discrete choice dynamic programming models. Journal of Econometrics 96(2), 201­229.
Thurstone, L. L. (1930). The Fundamentals of Statistics. New York: The Macmillan Company. Urzua, S. (2005). Schooling choice and the anticipation of labor market conditions: A dynamic
choice model with heterogeneous agents and learning. Unpublished manuscript, University of Chicago, Department of Economics. Van den Berg, G. J. (2001). Duration models: Specification, identification and multiple durations. In J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, Volume 5 of Handbooks in Economics, pp. 3381­3460. New York: North-Holland. Van den Berg, G. J., A. Holm, and J. C. Van Ours (2002, November). Do stepping-stone jobs exist? Early career paths in the medical profession. Journal of Population Economics 15(4), 647­665. Van den Berg, G. J., B. Van der Klaauw, and J. C. Van Ours (2004, January). Punitive sanctions and the transition rate from welfare to work. Journal of Labor Economics 22(1), 211­241. Van der Laan, M. J. and J. M. Robins (2003). Unified Methods for Censored Longitudinal Data and Causality. New York: Springer-Verlag. Wolpin, K. I. (1984, October). An estimable dynamic stochastic model of fertility and child mortality. Journal of Political Economy 92(5), 852­874. Wolpin, K. I. (1987, July). Estimating a structural search model: The transition from school to work. Econometrica 55(4), 801­817.

Chapter 25
Econometrics of Individual Labor Market Transitions
Denis Fouge`re and Thierry Kamionka

25.1 Introduction
During the last 20 years, the microeconometric analysis of individual transitions has been extensively used for investigating some problems inherent in the functioning of contemporary labor markets, such as the relations between individual mobility and wages, the variability of flows between employment, unemployment and non-employment through the business cycle, or the effects of public policies (training programs, unemployment insurance, . . . ) on individual patterns of unemployment. Typically, labor market transition data register sequences of durations spent by workers in distinct states, such as employment, unemployment and non-employment. When individual participation histories are completely observed through panel or retrospective surveys, the econometrician then disposes of continuous-time realizations of the labor market participation process. When these histories are only observed at many successive dates through panel surveys, the available information is a truncated one; more precisely it takes the form of discretetime observations of underlying continuous-time processes. Our presentation of statistical procedures used for analysing individual transition or mobility histories is based on the distinction between these two kinds of data.

Denis Fouge`re CNRS, CREST-INSEE (Paris), CEPR (London) and IZA (Bonn), e-mail: fougere@ensae.fr
Thierry Kamionka CNRS and CREST-INSEE, Paris, CREST-INSEE 15, Boulevard Gabriel Pe´ri, 92245 Malakoff cedex, France, e-mail: kamionka@ensae.fr

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

865

c Springer-Verlag Berlin Heidelberg 2008

866

D. Fouge`re and T. Kamionka

Statistical models of labor market transitions can be viewed as extensions of the single-spell unemployment duration model (see Chap. 14, this volume). Theoretically, a transition process is a continuous-time process taking its values in a finite discrete state space whose elements represent the main labor force participation states, for example employment, unemployment and non-employment.
The goal is then to estimate parameters which capture effects of different time-independent or time-varying exogenous variables on intensities of transition between states of participation. Here transition intensities represent conditional instantaneous probabilities of transition between two distinct states at some date. Typically, the analyst is interested in knowing the sign and the size of the influence of a given variable, such as the unemployment insurance amount or the past training and employment experiences, on the transition from unemployment to employment for example, and more generally in predicting the effect of such variables on the future of the transition process. For this purpose, she can treat these variables as regressors in the specification of transition intensities. Doing that, she estimates a reduced-form model of transition. Estimation of a more structural model requires the specification of an underlying dynamic structure in which the participation state is basically the choice set for a worker and in which parameters to be estimated influence directly individual objective functions (such as intertemporal utility functions) which must be maximized under some revelant constraints inside a dynamic programming setup. Such structural models have been surveyed by Eckstein and Wolpin (1989) or Rust (1994).
Our survey focuses only on reduced-form transition models, which have been extensively used and estimated in labor microeconometrics. The first section contains a general presentation of the statistical modelling of the transition process for continuous-time (event-history) data. The first section briefly recalls the useful mathematical definitions, essentially the ones characterizing the distribution of the joint sequence of visited states and of sojourn durations in these states. It also presents parametric and nonparametric estimation procedures, and ends with the issue of the unobserved heterogeneity treatment in this kind of process.
The second section deals with inference for a particular class of transition processes, namely markovian processes or simple mixtures of markovian processes, using discrete-time panel observations. Here the main problem is the embeddability of the discrete-time Markov chain into a continuous time one. In other words, the question is whether or not the discrete-time panel observations of a transition process are generated by a continuous-time homogeneous Markov process. After a discussion of this problem, the second section presents maximum-likelihood and bayesian procedures for estimating the transition intensity matrix governing the evolution of the continuous-time markovian process. Particular attention is paid to the estimation of the continuous-time mover­stayer model, which is the more elementary model of mixed Markov processes.
The conclusion points out some extensions.

25 Econometrics of Individual Transitions Models

867

25.2 Multi-spell Multi-state Models

25.2.1 General framework

25.2.1.1 Notations

Let us consider a cadlag1 stochastic process Xt , t  R+, taking its value in a finite discrete-state space denoted E = {1, . . . , K}, K  N and K  2. In other words, K represents the total number of states for the process, and Xt is the state occupied at time t by the individual (so Xt  E, t  R+). Let {xt , t  R+} be a realization of this process. We suppose that all the individual realizations of this process are identically and independently distributed: to simplify the notations, we can then omit the index for individuals.
As an illustration we consider the case of a labor force participation process describing the state occupied by a worker at time t. In order to simplify, we set:

  1 if the individual is employed at time t

Xt = 

2 3

if the individual is unemployed at time t if the individual is out of the labor force at time t

(25.1)

Now we suppose that each individual process is observed from the date of entry into the labor market, denoted 0 for the individual, up to an exogenously fixed time e (e > 0). An example of realization of process Xt is represented in Fig. 25.1.
This figure shows that the individual is first employed from time 0 up to time 1, then unemployed from time 1 up to time 2, then employed once again from

xt

3

2

1

0

1

2

3

t e

Fig. 25.1 A realization of the process Xt

1 "cadlag" means right-continuous, admitting left limits. For the definition of a cadlag process, see Chap. 17, Sect. 17.1, this volume.

868

D. Fouge`re and T. Kamionka

time 2 up to time 3, and finally out of the labor force (from time 3 on) when the observation stops at time e. If we denote:

u =  -  -1 , = 1, 2, . . .

(25.2)

the sojourn duration in state x( -1) reached by the individual at time ( -1) (before a transition to state x at time  ), the process Xt can be equivalently characterized by the sequences ( , x ) ;  N or {(u , xk=0uk ) ;  N} with u0 = 0.
Now suppose that process Xt is observed from the exogenous date s, with s 
]0, 1[, up to time e and that the date of entry into the state occupied at time s
(i.e. the date of entry into the labor market, 0) is unknown to the analyst. Then,
the sojourn duration in state xs = x0 is said to be left-censored. Symmetrically, for the example in Fig. 25.1, the sojourn duration in state xe = x3 is said to be right-censored, because the couple (4, x4 ) is not observed.
We restrict now our attention to non left-censored samples, i.e. such that s = 0, for all individuals.2 We define the event-history corresponding to process Xt for the
observation period [0, e] as:

 = 0, x0 , 1, x1 , . . . , n, xn

(25.3)

where n is the number of transitions, i.e. the number of modifications, of the studied process during the period [0, e]. This event-history can be equivalently defined as:

=

0

,

u1,

x0+u1

,

u2

,

x0+u1+u2

,

.

.

.

,

un

,

x0

+



n =1

u

This realization of the process from time 0 to time e can be written:

(25.4)

 = ((0, x0 ), (u1, x1 ), . . . , (un, xn ), (un+1, 0))

(25.5)

where un+1 = e - n is the duration of the last observed spell. The last spell is right-

censored. Indeed, n+1 and xn+1 are not observed. Consequently, we fix xn+1 = 0 in

order to signify that the last duration is at least equal to un+1. This realization of the

process can be rewritten

 = (y0, y1, . . . , yn, yn+1)

(25.6)

where

  (0, x0 ) if k = 0

yk

=



(k, xk ) (n+1, 0)

if if

1kn k =n+1

Let us define a spell as a period of time delimited by two successive transitions. The
history of the process is a sequence of variables yk = (uk, xk ), where uk is the length of spell k and xk is the state occupied by the individual at time k.

2 The statistical treatment of left-censored spells has been considered by Heckman and Singer (1984), Ondrich 1985 and Amemiya 2001.

25 Econometrics of Individual Transitions Models

869

25.2.1.2 Distributions of Spell Durations

Suppose now that the process enters state x -1 (x -1  {1, . . . , K}) at time  -1 ( = 1, . . . , n + 1). Let us examine the probability distribution of the sojourn du-
ration in state x -1 entered after the ( - 1)-th transition of the process. For that purpose, we assume that this sojourn duration is generated by a conditional proba-
bility distribution P given the event-history (y0, . . . , y -1) and a vector of exogenous variables z, defined by the cumulative distribution function

F(u | y0, . . . , y -1; z;  ) = Pr [U  u | y0, . . . , y -1; z;  ] = 1 - S(u | y0, . . . , y -1; z;  )

(25.7)

where  is a vector of unknown parameters. Here U denotes the random variable corresponding to the duration of the - th spell of the process, starting with its ( - 1) - th transition. S(u | y0, . . . , y -1; z;  ) is the survivor function of the sojourn duration in the - th spell. If the probability distribution P admits a density f with
respect to the Lebesgue measure, then:

u
F(u | y0, . . . , y -1; z;  ) = f (t | y0, . . . , y -1; z;  ) dt
0

(25.8)

and

f (u | y0, . . . , y -1; z;  ) =

d du

F (u

|

y0, . . . , y

-1; z;  )

=

-

d du

S(u

|

y0, . . . , y

-1; z;  )

(25.9)

If the function f (u | y0, . . . , y -1; z;  ) is cadlag, then there exists a function, called the hazard function of the sojourn duration in the - th spell, defined as

h(u | y0, . . . , y -1; z;  ) =

f (u | y0, . . . , y -1; z;  ) S(u | y0, . . . , y -1; z;  )

=

-

d du

log

S(u | y0, . . . , y

-1; z;  )

or equivalently as

(25.10)

h(u|y0, . . . , y

-1;

z;



)

du

=

lim
d u0

Pr

[u

U

<u+d u

| U u; y0, . . . , y du

-1; z]

(25.11)

From (25.9), it follows that:
u
- log S(u | y0, . . . , y -1; z;  ) = h(t | y0, . . . , y -1; z;  ) dt
0
= H(u | y0, . . . , y -1; z;  )

(25.12)

870

D. Fouge`re and T. Kamionka

The function H (u | y0, . . . , y -1; z) is called the conditional integrated hazard function of the sojourn in the - th spell, given the history of the process up to time  -1.
Reduced-form statistical models of labour-market transitions can be viewed as
extensions of competing risks duration models or multi-states multi-spells duration
models. These concepts will now be specified.

25.2.1.3 Competing Risks Duration Models

Let us assume that the number of states K is strictly greater than 2 (K > 2) and that,
for each spell, there exists (K -1) independent latent random variables, denoted Uk, (k = x -1 ; k  E). Each random variable Uk, represents the latent sojourn duration in state x -1 before a transition to state k (k = x -1 ) during the - th spell of the process.
The observed sojourn duration u is the minimum of these (K - 1) latent
durations:

u

= inf
k=x -1

uk,

(25.13)

Then, for any  -1  :

K
S(u | y0, . . . , y -1; z;  ) =  S(u, k | y0, . . . , y -1; z;  ) k=1 k= j

(25.14)

where S(u, k | y0, . . . , y -1; z;  ) = Pr(Uk,  u | y0, . . . , y -1; z) is the conditional survival function of the sojourn duration in state x -1 before a transition to state k during the - th spell of the process, given the history of the process up to
time  -1. Let g(u, k | y0, . . . , y -1; z;  ) be the conditional density function of the latent so-
journ duration in state x -1 before a transition to state k, and hk(u | y0, . . . , y -1; z;  ) the associated conditional hazard function. Then we have the relations:

hk(u

|

y0, . . . , y

-1; z;  )

=

g(u, k S(u, k

| |

y0, . . . , y y0, . . . , y

-1; z;  ) -1; z;  )

(25.15)

and

u
S(u, k | y0, . . . , y -1; z;  ) = exp - hk(t | y0, . . . , y -1; z;  ) dt
0

(25.16)

25 Econometrics of Individual Transitions Models

871

Let us remark that (25.14) and (25.16) imply:





u

S(u | y0, . . . , y -1; z;  ) = exp -

 hk(t | y0, . . . , y -1; z;  ) dt

0 k=x -1

(25.17)

Thus the conditional density function of the observed sojourn duration in state
j during the - th spell of the process, given that this spell starts at time  -1 and ends at time  -1 + u by a transition to state k, is:

f (u, k | y0, . . . y -1; z;  ) = hk(u | y0, . . . , y -1; z;  ),

× exp

uK

-

 hk (t | y0, . . . , y -1; z;  ) dt

0 k =1

k =x -1

(25.18)

This is the likelihood contribution of the - th spell when this spell is not rightcensored (i.e. when  =  -1 + u  e). When the - th spell lasts more than e -  -1, the contribution of this spell to the likelihood function is:
S(e -  -1 | y0, . . . , y -1; z;  ) = Pr(U > e -  -1 | y0, . . . , y -1; z)

25.2.1.4 Multi-spells Multi-states Duration Models

These models are the extension of the preceding independent competing risks model, which treats the case of a single spell (the -th spell) with multiple destinations. In the multi-spells multi-states model, the typical likelihood contribution has the following form:

n+1
L ( ) =  f (y | y0, . . . , y -1; z;  ) =1

(25.19)

where f (y | y0, . . . , y -1;  ) is the conditional density of Y given Y0 = y0,Y1 = y1, . . . ,Y -1 = y -1, Z = z and  is a vector of parameters. Definition (25.18) im-
plies that:

n
L ( ) =  f ( - -1, x |y0, . . . , y -1; z;  ) =1 × Sn+1(e - n|y0, . . . , yn; z;  )

(25.20)

The last term of the right-hand side product in (25.20) is the contribution of the last observed spell, which is right-censored. References for a general presentation

872

D. Fouge`re and T. Kamionka

of labor market transition econometric models can be found in surveys by Flinn and Heckman (1982a, b, 1983a) or in the textbook by Lancaster 1990a.

25.2.2 Non-parametric and Parametric Estimation

25.2.2.1 Non-parametric Estimation

The Kaplan-Meier Estimator

In the elementary duration model, a nonparametric estimator of the survivor function
can be obtained using the Kaplan-Meier estimator for right-censored data. Let us
suppose that we observe I sample paths (i.i.d. realizations of the process Xt ) with the same past history [0, n-1]. Let I be the number of sample paths such that n,i  T2 and I - I the number of sample paths for which the n-th spell duration is right-censored, i.e. n,i > T2, i denoting here the index of the process realization (i = 1, . . . , I). If n,1, . . . , n,I are the I ordered transition dates from state Xn-1 (i.e. n,1  . . .  n,I  T2), the Kaplan-Meier estimator of the survivor function Sn(t | [0, n-1]) is:

 S^n(t | [0, n-1]) = i:n,i t

1 - di ri

i = 1, . . . , I , t ]n-1, T2]

(25.21)

where ri is the number of sample paths for which the transition date from state Xn-1 is greater than or equal to n,i and di is the number of transition times equal to n,i. An estimator for the variance of the survivor function estimate is given by the Greenwood's formula:

Var S^n(t | [0, n-1])

 S^n(t | [0, n-1])

2×

di

i:n,it ri(ri - di)

(25.22)

This estimator allows to implement nonparametric tests for the equality of the survivor functions of two different subpopulations (such as the Savage and log-rank tests).
In the case of multiple destinations (i.e. competing risks models), we must restrict the set of sample paths indexed by i  {1, . . . , I } to the process realizations experiencing transitions from the state Xn-1 to some state k (k = Xn-1 ). Transitions to another state than k are considered as right-censored durations. If we set Xn-1 = j, then the Kaplan-Meier estimator of the survivor function S jk(t | [0, n-1]) is given by the appropriate application of formula (25.21), and an estimator of its variance is given by formula (25.22).

25 Econometrics of Individual Transitions Models

873

The Aalen Estimator

The function H (u | [0,  -1]), defined in (25.12) and giving the integrated hazard function of the sojourn duration in the -th spell, can be estimated nonparametrically
using the Aalen estimator (Aalen, 1978):

 H^ (u | [0,  -1]) = i: -1 ,i<u

di ri

(25.23)

H^ (u | [0,  -1]) is an unbiased estimator of H (u | [0,  -1]), and an estimator of its variance is given by:

 var

H^ (u | [0,  -1])

=

di

i: -1 ,i<u ri(ri - di)

(25.24)

In the competing risks model, (25.12) is equivalent to:

- log S jk(u | [0,  -1]) =

u 0

h jk(t

|

[0, 

-1])

dt

= Hjk(u | [0,  -1])

(25.25)

where Hjk(u | [0,  -1]) is the integrated intensity (or hazard) function for a transition from state j to state k (k = j) during the - th spell of the process, and given the past history [0,  -1] of the process. The Aalen estimator of this function can be derived from the formula (25.24) by considering indexes i corresponding to transitions from state j to state k during the - th spell of the process; indexes corresponding to other types of transition from state j are now considered as
right-censored durations. The Aalen estimator can be used to implement nonpara-
metric tests for the equality of two or more transition intensities corresponding to
distinct transitions.

25.2.2.2 Specification of Conditional Hazard Functions

The Markov Model

In a markovian model, the hazard functions hk(t | y0, . . . , y -1; z;  ) depend on t, on
states x -1 and on k, but are independent of the previous history of the process. More precisely:

hk(t | y0, . . . , y -1; z;  ) = hk(t | x -1 ; z;  ), k = x -1

(25.26)

and h j(t | y0, . . . , y -1; z;  ) = 0, if j = x -1

874

D. Fouge`re and T. Kamionka

When the Markov model is time-independent, it is said to be time-homogeneous. In this case:

hk(t | x -1 ; z;  ) = hk(x -1 ; z;  ) = hx -1 ,k(z;  ) , k = x -1 , t  R+ (25.27)

The particular case of a continuous-time markovian model observed in discretetime will be extensively treated in the following subsection (this Chapter). Let us now consider two simple examples of markovian processes.

Example 25.1. Consider the case of a time-homogeneous markovian model with two

states (K = 2) and assume that:



  if x -1 = 1 and k = 2

hk(t

| x

-1 ;  ) = 

 if x -1 = 2 and 0 otherwise

k=1

(25.28)

with  = (,  ). The parameter  > 0 is the instantaneous rate of transition from state 1 (for instance, the employment state) to state 2 (for instance, the unemployment state). Reciprocally,  > 0 is the instantaneous rate of transition from state 2 to state 1.
Durations of employment (respectively, unemployment) are independently and identically distributed according to an exponential distribution with parameter  (respectively, with parameter  ). If p1(t0) and p2(t0) denote occupation probabilities of states 1 and 2 at time t0 respectively, then occupation probabilities at time t (t > t0) are respectively defined by:

 p1(t) =  +  +
 p2(t) =  +  +

p1(t0

)

-



 +



p2(t0

)

-



 +



e-(+ )t e-(+ )t

(25.29)

Let (p1, p2) denote the stationary probability distribution of the process. Then it is easy to verify from (25.29) that:

p1

=



 +



and

p2

=



 +

(25.30)

In the economic literature, there are many examples of stationary job search models generating such a markovian time-homogeneous model with two states (employment and unemployment): see, for instance, the survey by Mortensen 1986. Extensions to three-states models (employment, unemployment and out-of-labor-force states) have been considered, for example, by Flinn and Heckman (1982a) and Burdett et al. (1984a, b). Markovian models of labor mobility have been estimated, for instance, by Tuma and Robins (1980), Flinn and Heckman (1983b), Mortensen and Neuman (1984), Olsen, Smith and Farkas, (1986) and Magnac and Robin (1994).

Example 25.2. Let us consider now the example of a non-homogeneous markovian model with two states (employment and unemployment, respectively denoted

25 Econometrics of Individual Transitions Models

875

1 and 2). Let us assume that the corresponding conditional hazard functions verify



 h2(t;  ) if x -1 = 1 and k = 2

hk(t

| x

-1 ;  ) = 

h1(t;  ) if x 0 otherwise

-1

= 2 and k = 1

(25.31)

Let p(0) = (p1(t0), p2(t0)) denote the initial probability distribution at time t0. The distribution of state occupation probabilities at time t, denoted p(t) = (p1(t), p2(t)) , is given by:

t

p1(t) = exp - [h1(s;  ) + h2(s;  )] ds
t0

t

s

× p1(t0) + h1(s;  ) exp (h1(u;  ) + h2(u;  )) du

t0

t0

ds (25.32)

and p2(t) = 1 - p1(t) (see Chesher and Lancaster, 1983). Non-homogeneous markovian models are often used to deal with processes
mainly influenced by the individual age at the transition date. For example, let us
consider a transition process {Xt }t0 with state-space E = {1, 2, 3}, and for which the time scale is the age (equal to At at time t). If the origin date of the process (i.e. the date of entry into the labor market) is denoted A0 for a given individual, then a realization of the process {Xt }t0 over the period [A0 , e] is depicted in Fig. 25.2.
Now let us suppose that transition intensities at time t depend only on the age
attained at this time and are specified such as:

hk t | y0, . . . , y -1; A0 ;  = hk(At ; x -1 ;  ) = exp x -1 ,k + x -1 ,k At

(25.33)

where  j,k and  j,k ( j, k  E×E and k = j) are parameters to be estimated. In formula (25.33), the individual index is omitted for simplifying notations. By noting

Fig. 25.2 A realization of a non-homogeneous markovian model

876

D. Fouge`re and T. Kamionka

that:

At = A -1 + (At - A -1 ) = A -1 + ut

(25.34)

where ut denotes the time already spent in the - th spell at date t, it is possible to write again transition intensities as:

hk(t | y0, . . . , y -1; A0 ;  ) = exp x -1 ,k + x -1 ,k A -1 + x -1 ,k ut

(25.35)

and to deduce the survivor function of the sojourn duration in the -th spell which has the form:

S(u | y0, . . . , y -1; A0 ;  )

 
 = exp - k=x -1



A -1 +u exp(x
A -1

-1 ,k+x

-1 ,k A

-1 +x

-1 ,k ut

 ) dt

(25.36)

where  1. By setting ut = t - A -1 in expression (25.36), it follows that:

S{u | y0, . . . , y -1; A0 ;  } =





 exp - k=x

-1

exp(x -1 ,k) x -1 ,k

exp(x -1 ,k (A -1 +u))- exp(x -1 ,k A -1 ) 

(25.37)

if x -1 ,k = 0. Then the likelihood contribution of the - th spell beginning at age A -1 with a transition to state x -1 and ending at age A with a transition to state x is:

L = f (A -A -1 , x | y0, . . . , y -1; A0 ;  ) = hx ( | y0, . . . , y -1; A0 ;  ) S(A -A -1 | y0, . . . , y -1; A0 ;  )

= exp

x 

-1

,k

+

x

-1 ,k

A



 ×

exp -
k

=x

-1

exp(x -1 ,k x -1 ,k

)

exp(x -1 ,k A )- exp(x -1 ,k A -1 ) 

(25.38)

Non-homogeneous markovian models of transitions between employment and unemployment have been estimated, for example, by Ridder 1986 and Trivedi and Alexander (1989).

25 Econometrics of Individual Transitions Models

877

Semi-Markov Models

In semi-Markov models, hazard functions depend only on the currently occupied
state (denoted x -1 for spell ), on the destination state (denoted k), on the sojourn duration in state x -1 and on the time of entry into the currently occupied state. If the spell corresponding to the currently occupied state is the - th spell of
the process, then hazard functions of the semi-Markov model have two alternative
representations:

hk(t | y0, . . . , y -1;  ) = hk(t |  -1; x -1 ;  )

(25.39)

or

hk(u | y0, . . . , y -1;  ) = hk(u |  -1; x -1 ;  )

(25.40)

where u = t -  -1 is the time already spent in the current state (i.e. in the - th spell of the process). When the hazard functions do not depend on the date  -1 of
the last event, but depend only on the time already spent in the current state, then the

semi-Markov model is said to be time-homogeneous. In this case, hazard functions

defined in (25.40) are such that:

hk(u |  -1; x -1 ;  ) = hk(u | x -1 ;  ), u  R+

(25.41)

In this model, the mean duration of a sojourn in state x -1 can be calculated using definitions of hazard and survivor functions, and thus it is given by:



 E(U | x -1 ;  ) = 0

u S(u | x -1 ;  )

hk(u | x -1 ;  )
k=x -1

du

(25.42)

where U is the random variable representing the duration of a spell and

u

 S(u | x -1 ;  ) = exp(-

0

hk(s | x -1 ;  ) d s)
k=x -1

(25.43)

This conditional expectation can be obtained using the following property:


E(U | x -1 ;  ) = 0 S(u | x -1 ;  ) d u

(25.44)

(see, for instance, Klein and Moeschberger (2003)). Semi-markovian models of transition between two or three states have been estimated by Flinn and Heckman (1982b), Burdett, Kiefer and Sharma (1985), Bonnal, Fouge`re and Se´randon (1997), and Gilbert, Kamionka and Lacroix (2001).

878
25.2.3 Unobserved Heterogeneity

D. Fouge`re and T. Kamionka

Here heterogeneity is supposed to cover individual observable and unobservable characteristics. Once again, we will omit the individual index.

25.2.3.1 Correlation Between Spells

Let us assume that the conditional model is time-homogeneous semi-markovian and

hk(u | y0, . . . , y -1; z; v;  ) = hk(u | x -1 ; z; vx -1 ,k; x -1 ,k)

(25.45)

where v is a vector of individual unobserved heterogeneity terms and  is the vector
of parameters to be estimated.
Let hk(u | x -1 ; z; vx -1 ,k; x -1 ,k) denote the conditional hazard function for the sojourn duration in the - th spell of the participation process, when the currently
occupied state is state x -1 and the destination state is k. Here z is a vector of exogenous variables, possibly time-dependent, v( j,k) is an heterogeneity random term, which is unobserved, and  jk is a vector of parameters. The preceding hazard function is often supposed to be equal to:

hk(u | x -1 ; z, vx -1 ,k, x -1 ,k) = exp (z; u ; x -1 ,k) + vx -1 ,k

(25.46)

Several assumptions can be made concerning the unobserved random terms v j,k. Firstly, v j,k can be supposed to be specific to the transition from j to k, so
v j,k = v j ,k for any ( j, k) = ( j , k ).
It can be also specific to the origin state, in which case:

v j,k = v j for any k = j.
Finally, v j,k can be supposed to be independent of states j and k and thus to be fixed over time for each individual, i.e.

v j,k = v for any ( j, k)  E×E, k = j.

This last assumption will be made through the remaining part of our presentation. Let us remark that a fixed heterogeneity term is sufficient to generate some correlation between spells durations. If we assume that v has a probability density function with respect to the Lebesgue measure denoted g(v | ), where  is a parameter, then we can deduce that the marginal survivor function of the sojourn duration in the - th spell of the process, when current state is x -1 , has the form:

25 Econometrics of Individual Transitions Models

879

S(u

| x -1 ; z; x -1 ) =

S(u
DG

| x -1 ; z; v; x -1 ) g(v | ) d v

u

= exp - exp(v)

DG

0

 exp((z;t; x -1 ,k) dt g(v | ) d v
k=x -1

(25.47)

where x -1 = (x -1 ,k)k=x -1 ,  and DG is the support of the probability distribution of the random variable v.
Such formalizations of heterogeneity have been used for estimation purposes by Heckman and Borjas (1980), Butler et al. (1986), (1989), Mealli and Pudney (1996), Bonnal, Fouge`re and Se´randon (1997), Gilbert, Kamionka and Lacroix (2001), and Kamionka and Lacroix (2003).
· Example. To illustrate the treatment of unobserved heterogeneity in transition processes, let us consider a realization of a two state time-homogeneous Markov process. More precisely, let us assume that this realization generates a complete spell in state 1 over the interval [0, 1] and a right-censored spell in state 2 over the interval [1, e[. Transition intensities between the two states are given by:

hk(t | x -1 ; vx -1 ; x -1 ) = x -1 + vx -1

(25.48)

where k  {1, 2}, x -1 > 0 and t  R+, 1 and 2 are two positive parameters, and v1 and v2 are two random variables supposed to be exponentially distributed with a density function g(v | ) =  exp(- v),  > 0. We want to deduce the likelihood

function for this realization of the process when v1 and v2 are supposed to be spellspecific and independent (v1 = v2 and v1v2) or fixed over time (v1 = v2 = v). In the first case (v1 = v2 and v1v2), the conditional likelihood function is:

Lv( ) = f (1, x1 | x0; v;  ) S(e-1 | x1 ; v;  ),

= (1+v1) exp {-(1+v1)1} exp {-(2+v2)(e-1)} (25.49)

where v = (v1, v2) ,  = (1, 2) , x0 = 1 and x1 = 2. Because v1 and v2 are unobserved, we must deal with the following marginalized likelihood function:



L(;  ) =

L(v1, v2, 1, 2) g(v1 | ) g(v2 | ) d v1 d v2

00

= f (1, x1 | x0; ;  ) S(e - 1 | x1; ;  )

(25.50)

where

f (1, x1 | x0; ;  ) = exp(-1 1)

 1 + 

1

+

1

1 +



and S(e - 1 | x1 ; ;  ) = exp(-2 (e - 1))

 (e - 1) + 

(25.51)

880

D. Fouge`re and T. Kamionka

are the marginalized density and survivor functions of sojourn durations 1 and (e - 1) in the first and second spells respectively.
When the heterogeneity term is fixed over time (v1 = v2 = v), then the marginal likelihood contribution is:


L(,  ) = (1+v) exp {-(11+2(e-1)+v e)}  exp(- v) d v,
0

=

exp

{-11

-

2(e

-

1)}



 +

e

1

+



 +

e

(25.52)

which is obviously not equal to the product of the marginalized density and survivor functions of the sojourn durations in the first and second spells as in the case where v1 = v2.
Now, let us assume that there exists a function  defining a one-to-one relation between v and some random variable , such as:

v = (, )

(25.53)

For instance,  can be the inverse of the c.d.f. for v, and  can be uniformly distributed on [0, 1]. Then:

1
S(u | x -1 ; z; x -1 ) = 0 S(u | x -1 ; z; (, ); x -1 )  () d

(25.54)

where  (.) is the density function of . The marginal hazard function for the sojourn in the - th spell can be deduced from (25.54) as:

h(u

|

x

-1 ; z; x

-1

)

=

-

d du

S(u

| x -1 ; z; x -1 )

(25.55)

Using definitions (25.54) and (25.55), the individual contribution to the likelihood function can be easily deduced and maximized with respect to  , either by usual procedures of likelihood maximization if the integrals (25.40) and (25.41) can be easily calculated, or by simulation methods (see, e.g., Gourie´roux and Monfort, 1997) in the opposite case.
For instance, let us consider the case of a semi-markovian model where the individual heterogeneity term is fixed over time, i.e. v j,k = v for any ( j, k)  E×E. From (25.20) and (25.46)­(25.47), the typical likelihood contribution in the present case is:

n
 Lv( ) = hx  -  -1 | x -1 ; z; v; x -1 ,x =1

n+1



  × exp =1

-

 -1

hk(t | x -1 ; z; v; x -1 ,k) dt
k=x -1

(25.56)

25 Econometrics of Individual Transitions Models

881

with n+1 = e by convention. Using relation (25.53), the marginalized likelihood contribution obtained by integrating out  is:

1
L ( ) = L(,)( )  () d
0

(25.57)

When the integral is not analytically tractable, simulated ML estimators of pa-
rameters  and ( jk)k= j can be obtained by maximizing the following simulated likelihood function with respect to  and ( jk)k= j:

 LN( )

=

1 N

N
L(n,) ( )
n=1

(25.58)

where n is drawn from the distribution with density function  (.), which must be conveniently chosen (for asymptotic properties of these estimators, see Gourie´roux and Monfort, (1997)).

25.2.3.2 Correlation Between Destination States

Let us assume that the conditional hazard function for the transition into state k is given by the expression

hk(u | y0, . . . , y -1; z; v;  ) = hk0(u; ) (y0, . . . , y -1; z;  ) k

(25.59)

where (.) is a positive function depending on the exogenous variables and the

history of the process, k an unobserved heterogeneity component specific to the individual (k > 0),  and  are vectors of parameters, hk0(u; ) is a baseline hazard function for the transition to state k (k  {1, . . . , K}). Let us assume that (see Gilbert

et al., 2001)

k = exp(ak v1 + bk v2)

(25.60)

where ak and bk are parameters such that ak = I[k  2] for k = 1, . . . , K and b1 = 1. The latent components v1 and v2 are assumed to be independently and identically distributed with a p.d.f. denoted g(v; ), where  is a parameter and vs  DG, s = 1, 2.
In this two factor loading model, the correlation between log(k) and log(k ), k,k , is given by the expression

k,k =

ak ak + bk bk ak2 + b2k ak2 + bk2

(25.61)

where k, k = 1, . . . , K. The contribution to the conditional likelihood function of a given realization of the process w = (y1, . . . , yn, yn+1) is:

882

D. Fouge`re and T. Kamionka

n+1

L ( )=

 f (y |y0, . . . , y -1; z; v1, v2;  ) g(v1; ) g(v2; ) d v1 d v2

DG DG =1

(25.62)

where

f (u, k | y0, . . . , y -1; z; v1, v2;  ) = hk(u | y0, . . . , y -1; z; v1, v2;  )k

u



 × exp - 0 j=x -1 h j(t | y0, . . . , y -1; z; v1, v2;  ) dt

(25.63)

and the conditional hazard function is given by expression (25.59). The exponent

k is equal to 1 if k  {1, . . . , K}, and to 0 otherwise.  is a vector of parameters and  = (,  ). As the last spell is right-censored, the corresponding contribution

of this spell is given by the survivor function





 un+1



 f (yn+1|y0, . . . , yn; z; v1, v2;  )= exp -
0

j=xn h j(t|y0, . . . , yn; z; v1, v2;  ) dt

(25.64)

where yn+1 = (un+1, 0) (state 0 corresponds to right-censoring). Bonnal et al. (1997) contains an example of a two factor loading model. Lin-

deboom and van den Berg (1994), Ham and Lalonde (1996) and Eberwein et al.

(1997), (2002) use a one factor loading model in order to correlate the con-

ditional hazard functions. A four factor loading model has been proposed by

Mealli and Pudney (2003). Let us remark that, in the case of bivariate duration

models, association measures were studied by Van den Berg 1997. Discrete dis-

tributions of the unobserved heterogeneity component can be alternatively used

(see, for instance, Heckman and Singer (1984) Gritz 1993, Baker and Melino

(2000)).

This way to correlate the transition rates using a factor loading model is par-

ticularly useful for program evaluation on nonexperimental data. In this case, it is

possible to characterize the impact on the conditional hazard functions of previous

participation to a program by taking into account selectivity phenomena at entry

into the program.

25.3 Markov Processes Using Discrete-Time Observations
The econometric literature on labor mobility processes observed with discrete-time panel data makes often use of two elementary stochastic processes describing individual transitions between a finite number of participation states.
The first one is the continuous-time Markov chain, whose parameters can be estimated through the quasi-Newton (or scoring) algorithm proposed by Kalbfleisch and Lawless (1985). This kind of model allows to calculate stationary probabilities

25 Econometrics of Individual Transitions Models

883

of state occupation, the mean duration of sojourn in a given state, and the intensities of transition from one state to another.
A main difficulty can appear in this approach: in some cases the discrete-time Markov chain cannot be represented by a continuous-time process. This problem is known as the embeddability problem which has been surveyed by Singer and Spilerman (1976a), b) and Singer (1981, 1982). However, some non-embeddable transition probability matrices can become embeddable after an infinitesimal modification complying with the stochastic property. This suggests that the embeddability problem can be due to sampling errors.
Geweke et al., (1986a) established a bayesian method to estimate the posterior mean of the parameters associated with the Markov process and some functions of these parameters, using a diffuse prior defined on the set of stochastic matrices. Their procedure allows to determine the embeddability probability of the discretetime Markov chain and to derive confidence intervals for its parameters under the posterior.
The second frequently used modelization incorporates a very simple form of heterogeneity among the individuals: this is the mover-stayer model, which was studied in the discrete-time framework by Frydman 1984, Sampson 1990 and Fouge`re and Kamionka (2003). The mover-stayer model is a stochastic process mixing two Markov chains. This modelling implies that the reference population consists of two types of individuals: the "stayers" permanently sojourning in a given state, and the "movers" moving between states according to a non-degenerate Markov process.
These two modelizations will be successively studied in the following subsection.

25.3.1 The Time-Homogeneous Markovian Model

Let us consider a markovian process {Xt , t  R+} defined on a discrete state-space E = {1, . . . , K}, K  N, with a transition probability matrix P(s,t) with entries p j,k(s,t), ( j, k)  E × E, 0  s  t, where:

p j,k(s,t) = Pr{Xt = k | Xs = j}

(25.65)

K
and  p j,k(s,t) = 1. If this markovian process is time-homogeneous, then: k=1

p j,k(s,t) = p j,k(0,t - s)  p j,k(t - s), 0  s  t

(25.66)

or equivalently:

P(s,t) = P(0,t - s)  P(t - s), 0  s  t

(25.67)

This implies that transition intensities defined by:

h j,k

=

lim
t0

p j,k(t,t

+

t)/t,

t  0, ( j, k)  E × E,

j=k

(25.68)

884
are constant through time, i.e.:

D. Fouge`re and T. Kamionka

hk(t | x -1 ;  ) = h j,k(t |  ) = h j,k , t  0, ( j, k)  E × E, j = k

(25.69)

where x -1 = j. These transition intensities are equal to the hazard functions previously defined in (25.26) and (25.27). The K × K transition intensity matrix, which is associated to the time-homogeneous markovian process {Xt ,t  R+}, is denoted
Q and has entries:

 h j,k



R+

if

j

=

k,

( j,k)



E ×E

 q( j, k)

=



K
- h j,m  0 i f
m=1

j = k,

jE

m= j

(25.70)

Let us denote Q the set of transition intensity matrices, i.e. the set of (K × K) ma-

trices with entries verifying the conditions (25.70). It is well known (cf. Doob (1953,

p. 240 and 241) that the transition probability matrix over an interval of length T can

be written:

P(0, T ) = exp(QT ), T  R+

(25.71)

where exp(A) = k=0 Ak/k! for any K × K matrix A. Main properties of the time-homogeneous markovian process {Xt , t  R+} with
state-space E, are the following:

· sojourn times in state j ( j  E) are positive random variables, which are exponentially distributed with parameter -q( j, j):

u j  exp(-q( j, j)), j = 1, . . . , K

(25.72)

with E[u j] = var[u j]1/2 = -q( j, j)-1, · the probability of a transition to state k given that the process is currently in state
j (k = j) is independent of the sojourn time in state j, and is found to be:

r j,k = -q( j, k)/q( j, j), k = j, ( j, k)  E × E

(25.73)

· if the time-homogeneous Markov process {Xt } is ergodic, its equilibrium (or limiting) probability distribution is denoted P = (p1, . . . , pK ) and defined as the unique solution to the linear system of equations:

K
Q P = 0 , with  pi = 1 i=1

(25.74)

25 Econometrics of Individual Transitions Models

885

25.3.1.1 Maximum Likelihood Estimator of the Matrix P Using Discrete-Time (Multiwave) Panel Data

Let us suppose now that we observe  independent realizations of the process {Xt } at equally spaced times T0, T1, . . . , TL (L > 1) such as: T - T -1 = T , = 1, . . . , L. Let us denote:
· n j,k( ) the number of individuals who were in state j at time T -1 and who are in state k at time T ,
· n j( - 1) the number of individuals who were in state j at time T -1. Maximizing the conditional likelihood function given the initial distribution at T0:

LK

  L(P(0, T )) =

p j,k(T -1, T ) n j,k( )

=1 j,k=1

K
=

p j,k(0, T )  L=1n j,k( )

j,k=1

(25.75)

K
with  p j,k(0, T ) = 1, gives the ( j, k) entry of the MLE P(0, T ) for P(0, T ):

k=1

L

L

p j,k(0, T ) =  n j,k( ) /  n j( - 1)

(25.76)

=1

=1

(see Anderson and Goodman, 1957). If the solution Q to the equation:

P(0, T ) = exp(QT ), T > 0

(25.77)

belongs to the set Q of intensity matrices, then Q is a MLE estimator for Q. Nevertheless, two difficulties may appear:3
· the (25.77) can have multiple solutions Q  Q: this problem is known as the aliasing problem;4
· none of the solutions Q to the equation (25.77) belongs to the set Q of intensity matrices; in that case, the probability matrix P(0, T ) is said to be non-embeddable with a continuous-time Markov process.

25.3.1.2 Necessary Conditions for Embeddability
The unique necessary and sufficient condition for embeddability was given by Kendall, who proved that, when K = 2, the transition matrix P(0, T ) is embeddable
3 A detailed analysis of these problems is developed in papers by Singer and Spilerman (1976 a and b). 4 The aliasing problem has also been considered by Phillips (1973).

886

D. Fouge`re and T. Kamionka

if and only if the trace of P(0, T ) is strictly greater than 1. When K  3, only necessary conditions are known; they are the following:5 1st necessary condition (Chung, 1967):
· if p j,k(0, T ) = 0,then p(jn,k)(0, T ) = 0, n  N, where p(jn,k)(0, T ) is the entry ( j, k) of the matrix [P(0, T )]n,
· if p j,k(0, T ) = 0, then p(jn,k)(0, T ) = 0, n  N;

2nd necessary condition (Kingman, 1962): det P(0, T ) > 0, 3rd necessary condition (Elfving, 1937):
· no eigenvalue i of P(0, T ) can satisfy | i |= 1, other than i = 1; · in addition, any negative eigenvalue must have even algebraic multiplicity;
4th necessary condition (Runnenberg, 1962): the argument of any eigenvalue i of P(0, T ) must satisfy:

1+ 1 2K

  arg (log i) 

3- 1  2K

This last condition plays an important role in the remainder of the analysis.

25.3.1.3 Resolving the Equation P^(0, T ) = exp(Q^T )

The proof of the following theorem can be found in Singer and Spilerman (1976a):

If P(0, T ) has K distinct 6 eigenvalues (1, . . . , K) and can be written P(0, T ) = A × D × A-1, where D = diag(1, . . . , K) and the eigenvector corresponding to i (i = 1, . . . , K) is contained in the i-th column of the (K × K) matrix A, then:





log(P(0, T ))

=

QT

=

A × 

logk1 (1) ...

... ...

0 ...

 × A-1

0 . . . logkK (K)

(25.78)

where logki (i) = log | i | +(argi + 2ki )i, ki  Z, is a branch of the logarithm of i, when i  C .7

5 Singer and Spilerman (1976a) and Geweke, Marshall and Zarkin (1986b) survey this problem. 6 The case of repeated eigenvalues arises very rarely in empirical applications. For its treatment, the reader can consult Singer and Spilerman (1976a), p. 19­25). 7 Let us recall that the logarithmic function is multiple valued in thecomplex set C . If z = a + ib (z  C ), then: logk(z) = log | z | +i( + 2k ), k  Z, with | z |= a2 + b2, and  = arg(z) = tan-1(b/a). Each value for k generates a distinct value for log(z), which is called a branch of the logarithm.

25 Econometrics of Individual Transitions Models

887

Since (25.77) has as many solutions Q as there are combinations of the form (logk1 (1), . . . , logkK (K)), the number of these solutions is infinite when the matrix P(0, T ) has at least two complex conjugate eigenvalues. However, an important implication of the fourth necessary condition for embeddability is that only finitely many branches of log(P(0, T )) need to be checked for membership in Q. Indeed, this condition implies:

i, - Li(K)  ki  Ui(K)

(25.79)

where

Ui(K) = intpt

log

|

i

|

tan{(

1 2

+

1 K

)}-

|

arg

i

|

2

Li(K) = intpt

log

|

i

|

tan{(

3 2

-

1 K

)}-

|

arg

i

|

2

the function "intpt" being the integer part of a real number. So the number of branches of i which must be computed is equal to Li(K) +Ui(K) + 1, the last one corresponding to the main branch (with ki = 0). Then the number of solutions Q that must be examined for membership in Q is denoted k(P) and is equal to:

 
 k(P) = 

v
{L j(K) +Uj(K) + 1}
j=1

if v  1

1

if v = 0

(25.80)

where v denotes the number of complex conjugate eigenvalue pairs of the matrix P(0, T ). Let us remark that:

· for a real eigenvalue, only the principal branch of the logarithm must be examined: other branches (with ki = 0) correspond to complex intensity matrices Q;
· each element of a complex conjugate eigenvalue pair has the same number of candidate branches (see (25.79)); moreover, only combinations of branches in-
volving the same ki in each element of the pair must be computed; all others correspond to complex intensity matrices; this fact explains why the calculation of k(P) is based on the number of complex conjugate eigenvalue pairs, and
why the number of branches that need to be checked for each pair j is equal to L j(K) +Uj(K) + 1 rather than {L j(K) +Uj(K) + 1}2.

If (25.77) has only one solution Q  Q, this solution is the MLE for the intensity matrix of the homogeneous continuous-time Markov process {Xt , t  R+}; an esti-
mator for the asymptotic covariance matrix of Q has been given by Kalbfleisch and
Lawless (1985).

888
25.3.1.4 The Scoring Procedure

D. Fouge`re and T. Kamionka

Kalbfleisch and Lawless (1985) have proposed to maximize with respect to  the conditional likelihood function (25.75), i.e.

K
L( ) =

exp(QT )

 L=1ni, j( (i, j)

),

QQ

i, j=1

(25.81)

through a scoring algorithm. In this expression, {exp(QT )}i, j is the entry (i, j) of the matrix exp(QT ) = P(0, T ) and  is the vector of extra diagonal elements of the matrix Q (   (Q)). If it is assumed that matrix Q has K distinct eigenvalues, denoted (d1, · · · , dK), matrices Q and P(0, T ) can be written as:

Q = A DQA-1 = A diag (d1, · · · , dK)A-1 and P(0, T ) = exp(QT ) = A exp(DQT )A-1
= A diag(ed1T , · · · , edKT )A-1 = A diag(1, · · · , K)A-1 (25.82)

These formulae lead to a convenient expression of the score (or gradient) vector, which is:

  S( ) =

 log L(Q)  qk

=

K i, j=1

L
ni, j(
=1

)

 {exp(QT )}(i, j)/ qk {exp(QT )}(i, j)

(25.83)

where

    {exp(QT )} 

 qk

=
s=1

 Qs  qk

Ts s!

=

 s=1

s-1 r=0

Qr

Q  qk

· Qs-1-r · T s s!

= AVk A-1

the matrix

  Vk

 s-1

=

DrQ

s=1 r=0

A-1

Q  qk

A

DQs-1-r

Ts s!

having elements:

 (Gk 
(Gk

edit - ed jt )(i, j) di - d j , i = )(i, j)t edit , i = j,

j,

where (Gk )(i, j) is the entry (i, j) of the matrix Gk

=

A-1

Q  qk

A.

25 Econometrics of Individual Transitions Models

889

The information matrix, which has the form

  E

-

 2 log L(  qk  qk

)

=

L K E[Ni( - 1)]  pi, j(0, T )  pi, j(0, T )

=1 i, j=1 pi, j(0, T )

 qk

 qk

(25.84)

(see Kalbfleisch and Lawless ((1985), p. 864), is estimated by:

  M( ) =

L K ni( - 1)  pi, j(0, T )  pi, j(0, T )

=1 i, j=1 pi, j(0, T )  qk

 qk

(25.85)

The iterative formula for the scoring algorithm being:

n+1 = n + M(n)-1S(n)

where n  0 and an initial value 0 =  (Q0) is still to be chosen. Two cases must be considered (the case with multiple solutions in Q is excluded):

· equation (25.77) admits only one solution for Q and this solution belongs to the set Q of transition intensity matrices: Q^ is the MLE of the transition matrix Q of the time-homogeneous markovian process, and the matrix M( (Q))-1 gives a consistent estimate of the covariance matrix of ^ =  (Q);
· the unique solution Q0 = Q to (25.77) doesn't belong to the set Q; however, it may exist matrices P~(0, T ) = exp(Q~T ) "close" to P(0, T ) and which are embeddable, i.e. such that Q~  Q; in this case, the scoring algorithm of Kalbfleisch and
Lawless (1985) can be applied to the maximization of the likelihood (25.81) sub-
ject to the constraint Q  Q; this constraint can be directly introduced into the
iterative procedure by setting



 exp(ai, j), ai, j  R, j = i, (i, j)  E × E

 qi, j

=



qii

=

-

K
k=1

qik, i

=

j,

i



E

k=i

and the initial value Q0 can be chosen to verify:

(25.86)

Q0 = argmin Q0 - Q Q Q

(25.87)

where

Q

=

1 T

log P(0, T ).

25.3.1.5 Bayesian Inference
Geweke, Marshall and Zarkin (1986a) have developed a bayesian approach for statistical inference on Q (and functions of Q) by using a diffuse prior on the set of stochastic matrices. This approach can be justified by two arguments:

890

D. Fouge`re and T. Kamionka

· when the MLE of Q is on the parameter set boundary, standard asymptotic theory cannot be applied any more; bayesian inference overcomes this difficulty: the posterior confidence interval for Q can be viewed as its asymptotic approximation;
· moreover, bayesian inference allows incorporating into the choice of the prior distribution some information external to the sample (for example, the distribution of sojourn durations in each state).
Let us denote PK the set of (K × K) stochastic matrices, i.e. Pk = P  MK,K : i, j  E, pi, j  0 and Kj=1 pi, j = 1 , PK the set of (K × K) embeddable stochastic matrices, i.e. PK = {P  MK,K : P  PK and Q  Q, P(O, T ) = exp(QT ), T > 0}. For any P  PK, k(P) denotes the number of combinations of the form (25.78) belonging to Q and verifying (25.77). Now let us consider a prior distribution on P  PK, denoted (P), a prior distribution on Q, denoted hk(P) and verifying kk=(1P) hk(P) = 1 for P  PK, and a R-valued function of interest denoted g(Q). If the posterior embeddability probability of P is defined as:

Pr(P  PK | N) =

PK PK

L(P; N)(P)dP L(P; N)(P)dP

>

0

(25.88)

then the expectation of g(Q) is equal to

E[g(Q) | N, P  PK] =

PK kk=(1P) hk(P)g[Qk(P)]L(P; N)(P)dP PK L(P; N)(P)dP

(25.89)

where the entry (i, j) of the matrix N is L=1 ni, j( ), L(P; N) is the likelihood function and Qk(P) is the transition intensity matrix corresponding to the k-th combination of logarithms of the eigenvalues of matrix P. The function of interest g(Q) can be, for example, g(Q) = qi, j, (i, j)  E × E, or:
g(Q) = E (qi, j - E(qi, j | N; P  PK))2 | N; P  PK
which is equivalent to:
g(Q) = E qi2, j | N; P  PK - E2 qi, j | N; P  PK
The embeddability probability for P and the first moment of g(Q) may be computed using Monte-Carlo integration. This involves the choice of an importance function from which a sequence of matrices {Pi}  PK can be easily generated (see Geweke et al., (1986a), for such a function). Now let us consider a function J(Pi) such that J(Pi) = 1 if Pi  PK and J(Pi) = 0 otherwise. If (Pi) is bounded above, then:

25 Econometrics of Individual Transitions Models

I

 J(Pi)L(Pi; N)(Pi)/I(Pi)

 lim
I

i=1

I

L(Pi; N)(Pi)/I(Pi)

i=1

= Pr[P  PK | N] a.s.

891
(25.90)

Moreover, if Hk(P) is a multinomial random variable such that Pr[Hk(P) = 1] = hk(P), and if g(Q) is bounded above, then

I k(Pi)
  Hk(Pi)g[Qk(Pi)]J(Pi)L(Pi; N)(Pi)/I(Pi)

lim i=1 k=1
I

I
 J(Pi)L(Pi; N)(Pi)/I(Pi)

i=1

= E[g(Q) | N; P  PK] a.s.

(25.91)

(see Geweke et al., (1986a), p. 658).

25.3.1.6 Tenure Records
Up to now we concentrated on the statistical analysis of discrete-time observations of an underlying continuous-time Markov process. The available information is sometimes richer than the one brought by discrete-time data, but not as complete as the one contained in continuous-time data. Indeed it can consist, for a given individual, in the joint sequence {(xT , dT )} =0,···,L of occupied states {xT } =0,···,L and of times {dT } =0,···,L already spent in these states at distant observation times {T } =0,···L. Such data have been studied in the continuous-time markovian framework by Magnac and Robin (1994), who proposed to call this kind of observations "tenure records". Figure 25.3 gives an example of a tenure record.
In this example, T0, T1, T2 and T3 are the exogenous survey dates. The process {Xt }t0 is first observed to be in state xT0 = 1 at time T0: it occupies this state from date (T0 - d0) on. It is then observed to be in state 3 at successive times T1 and T2. This state was entered at time (T1 - d1) = (T2 - d2). Finally, the process is at time T3 in state xT3 = 1 from date (T3 - d3) on. Indeed it is possible that a spell covers two survey dates, as it is the case for the second observed spell in the preceding example: obviously, the information collected in T1 is redundant.
Let us remark that in tenure records data sets, any sojourn duration is rightcensored with probability one. Typically, a tenure record consists of a sequence {xT , d ,t } =0,···,L with the convention tL = . The process {Xt }t0 enters state xT at time (T - d ) and is observed to stay in this state for a duration greater than d . Then the process is not observed (i.e. is truncated) during a period of length

892

D. Fouge`re and T. Kamionka

Fig. 25.3 An example of a tenure record

t = (T +1 - d +1) - T . Let hi j(s,t) be the probability that the process {Xt } en-

ters state j at time t given that it was in state i at time s(s < t). If {Xt } is time-

homogeneous markovian, then hi j(0,t - s)  hi j(t - s), s < t. In this case, hi j(t) is

equal to:

K
 hi j(t) = pik(t) qk j, (i, j)  E × E

(25.92)

k=1 k= j

Consequently, the likelihood function for a tenure record {xT , d , t } =0,···,L is the following:

L-1

 L =

S(d

|

xT

)

hxT

,xT

(t
+1

)

S(dL | xTL )

=0

L-1
 = exp(-xTL dL) =0

K

 exp(-xT d )

{exp(Q t )}(xT ,k) · qk,xT +1

k=1

k=xT +1

(25.93)

where S(u | xT ) is the survivor function of the sojourn duration in state xT and Q is the transition intensity matrix with entries:

 

-i

=

-

K


qik ,

if

j

=

i

Q(i, j) = 

k=1 k=i

qi j, if j = i

Magnac and Robin (1994) show that tenure records allow to identify the intensity of transition from one state to the same state (for example, employment) when

25 Econometrics of Individual Transitions Models

893

within-state mobility is allowed (i.e. when a worker can directly move from one job to another). Discrete-time observations do not present this advantage.
For a treatment of incomplete records, particularly in presence of unobserved heterogeneity see, for instance, Kamionka 1998. Magnac et al. (1995) propose to use indirect inference to estimate the parameters of a transition model under a semiMarkov assumption in the context of a censoring mechanism.

25.3.2 The Mover-Stayer Model

25.3.2.1 MLE for the Discrete-Time Mover-Stayer Model

The mover-stayer model has been introduced by Blumen et al. 1955 for study-
ing the mobility of workers in the labor market. Subsequently, Goodman (1961),
Spilerman (1972) and Singer and Spilerman (1976a) have developed the statisti-
cal analysis of this model, essentially on the discrete-time axis. The mover-stayer model in discrete time is a stochastic process {X ,  N}, defined on a discrete state-space E = {1, . . . , K}, K  N, and resulting from the mixture of two independent Markov chains; the first of these two chains, denoted {X1,  N} is degenerate, i.e. its transition probability matrix is the identity matrix, denoted I. The other chain, denoted {X2 ,  N} is characterized by a non-degenerate transition matrix M(s, u) = mi, j(s, u) , i, j = 1, . . . , K, 0  s  u, where:

mi, j(s, u) = Pr{Xu2 = j | Xs2 = i}, i, j  E, s, u  N, s  u

(25.94)

K
and  mi, j(s, u) = 1. j=1
Moreover, the Markov chain {X2,  N} is assumed to be time homogeneous, i.e.:

mi, j(s, u) = mi, j(0, u - s)  mi, j(u - s), 0  s  u

(25.95)

which is equivalent to:

M(s, u) = M(0, u - s)  M(u - s), 0  s  u

(25.96)

Now let us assume that the mixed process {X ,  N} is observed at fixed and equally distant times: 0, T, 2T, . . . , LT, with T > 0 and L  N (L  1). Transition probabilities for this process are given by the formulas:

pi, j(0, kT ) = Pr[XkT = j | X0 = i], i, j  E, k = 1, . . . , L

(25.97)

=

(1 - si)[mi, j(T )](k) if j = i si + (1 - si)[mi,i(T )](k) if j = i

where [mi, j(T )](k) is the entry (i, j) of the matrix [M(T )]k, and (si, 1 - si), with si  [0, 1], is a mixing measure for state i  E. So, in the mover-stayer model, the

894

D. Fouge`re and T. Kamionka

reference population is composed of two kinds of individuals: the "stayers", permanently sojourning in the same state, and the "movers", who move from one state to another according to the time-homogeneous Markov chain with transition probability matrix M(s, u), s  u. The proportion of "stayers" in state i (i  E) is equal to si.
The estimation of the transition matrix M(0, T ) and of the mixing measure s from a sample of N independent realizations of the process {X ,  N}, has been extensively treated by Frydman (1984) and then carried out by Sampson (1990). The method developed by Frydman relies on a simple recursive procedure, which will be rapidly surveyed. Formally, the form of the sample is:

{X0(n), XT (n), X2T (n), . . . , XLT (n); 1  n  N}
where XkT(n) (k = 0, . . . , L) is the state of the process for the n-th realization at time kT , and (L + 1) is the number of equally spaced dates of observation.
Let us denote ni0,...,iLT the number of individuals for which the observed discrete path is (i0, . . . , iLT ), ni(kT ) the number of individuals in state i at time kT , ni j(kT ) the number of individuals who are in state i at time (k - 1)T and in state j at time (kT ), ni the number of individuals who have a constant path,8 i.e. i0 = iT = . . . = iLT = i, i  E, ni j = kL=1 ni j(kT ) the total number of observed transitions from state i to state j, ni = Lk=-01 ni(kT ) the total number of visits to state i before time (LT ), i  0 the proportion of individuals initially (i.e. at date 0) in state i, i  E, with Ki=1 i = 1.
The likelihood function for the sample is (Frydman, 1984, p. 633):

where:

K

K

  L = ini(0) Li

i=1

i=1

(25.98)

Li = {si + (1 - si)[mii(0, T )]L}ni (1 - si)ni(0)-ni [mii(0, T )]nii-Lni
K
 × [mik(0, T )]nik k=1 k=i

In this last expression, ni(0) is the number of individuals in state i at time 0, ni is the number of individuals permanently observed in state i, (ni(0) - ni) is the number of individuals initially in state i who experience at least one transition in
the L following periods, nik is the total number of transitions from state i to state k. Maximizing the function (25.98) with respect to M and s subject to the constraints si  0, i  E, is equivalent to maximize the K expressions:

Li = Log Li + isi, i = 1, . . . , K

(25.99)

8 Among the individuals permanently sojourning in state i, we must distinguish the "stayers" from
the "movers"; indeed, the probability that a "mover" is observed to be in state i at each observation point is strictly positive and equal to {mii(0, T )}L.

25 Econometrics of Individual Transitions Models

895

for which the first-order derivatives relatively to si are:

 Li  si

=

ni{1 - [mii(0, T )]L} si + (1 - si)[mii(0, T )]L

-

ni(0) - ni 1 - si

+ i

=

0

(25.100)

Two situations should be considered: First case: If si > 0, then i = 0 and:

si

=

ni - ni(0)[mii(0, T )]L ni(0){1 - [mii(0, T )]L}

(25.101)

As shown by Frydman (1984, p. 634­635), the ML estimators of transition probabilities mi j (with fixed i, and j varying from 1 to K) are given by the recursive equation:

j-1

K

mi j(0, T ) = ni j{1 - mii(0, T ) -  mik(0, T )}/  nik, j = i, i, j  E

k=1

k= j

k=i

k=i

(25.102)

To solve (25.102), it is necessary to begin by setting j = 1 if i = 1 and j = 2 if i = 1. Furthermore, mii(0, T ) is the solution, belonging to the interval [0, 1], to the equation:

[ni - Lni(0)][mii(0, T )]L+1 + [Lni(0) - nii][mii(0, T )]L +[Lni - ni ]mii(0, T ) + (nii - Lni) = 0

(25.103)

Frydman

(1984)

doesn't

notice

that

si0

whenever

(

ni ni(0)

)[mii(0,

T

)]L,

where

(ni/ni(0)) is the proportion of individuals permanently observed in state i. In that

case, the initial assumption si > 0 is violated, and it is necessary to consider the case

where si = 0.

Second case: If si = 0, then:

mi j(0, T ) = ni j/ni ,  i, j = 1, . . . , K

(25.104)

This is the usual ML estimator for the probability of transition from i to j for a first-order Markov chain in discrete time (for example, see Anderson and Goodman (1957), or Billingsley (1961). A remark, which is not contained in the paper by Frydman (1984), must be made. It may appear that Lni = nii (with nii = 0), which means that no transition from state i to any other distinct state is observed. This case arises when the number ni of individuals permanently observed in state i is equal to the number ni(0) of individuals initially present in state i (if ni(0) = 0). Then the estimation problem has two solutions:

· si=1 and mii is non-identifiable (see (25.101) and (25.103)), · si = 0 and mii = 1.

The first solution corresponds to a pure model of "stayers" in state i, the second to a time-homogeneous Markov chain in which state i is absorbing. The mover-stayer

896

D. Fouge`re and T. Kamionka

model, as a mixture of two Markov chains, is not appropriate any more for state i. When this case appears in the applied work, we propose to choose the solution si = 0 and mii = 1, especially for computing the estimated marginal probabilities of the form Pr[XkT = i], k = 0, . . . , L, i = 1, . . . , K. The analytical expression of the estimated asymptotic covariance matrix for ML estimators M and s can be calculated using second derivatives of expression (25.99).

25.3.2.2 Bayesian Inference for the Continuous-Time Mover-Stayer Model

The mover-stayer model in continuous-time is a mixture of two independent Markov chains; the first one denoted {Xt1, t  R+} has a degenerate transition matrix equal to the identity matrix I; the second one denoted {Xt2, t  R+} has a non-degenerate transition matrix M(s,t), 0  s  t, verifying over any interval of length T :

M(0, T ) = exp(QT ), T  R+

(25.105)

Setting M(0, kT ) = mi, j(0, kT ) , we get: P(0, kT ) = diag(s) + diag(IK - s){exp(QT )}K, T  0, k = 1, . . . , L

(25.106)

where s = (s1, . . . , sK) , (IK - s) = (1 - s1, . . . , 1 - sK) , and diag(x) is a diagonal matrix with vector x on the main diagonal. From the discrete-time ML estimators of stayers' proportions s and of the transition probability matrix M(0, T ), it is then possible to obtain the ML estimator of the intensity matrix Q by resolving (25.105) (see subsection 2.1 above). But, due to the possible problem of nonembeddability of the matrix M(0, T ), it could be better to adopt a bayesian approach, as the one proposed by Fouge`re and Kamionka (2003). This approach is summarized below.

Definition 25.1 To write the likelihood-function and the expected value under the
posterior of some function of parameters, additional notation is needed. Let MK be the space of K × K stochastic matrices:

K
MK = {M = mi j : mi j  0, i, j  E and  mi j = 1,  i  E}. j=1

Clearly, the transition probability matrix M(0, T ) belongs to MK. Let (M, s)
be a prior mapping MK × [0, 1] into R (the uniform prior will be used in the application). (M, s) is defined for M  MK and for a vector of mixing measures s = {si , i  E}  [0, 1]K. [0, 1]K denotes the cartesian product of K copies of
[0, 1]. Let us denote Q the space of intensity matrices:

Q = {Q = qi j : qi j  0, i, j  E, i = j and qii  0, i  E}.

25 Econometrics of Individual Transitions Models

897

If M(0, T ) is embeddable, there exists at least one matrix Q  Q defined by the equation M(0, T ) = exp(QT ), where T is the number of time units between observations. Let MK the space of embeddable stochastic matrices:
MK = { M(0, T )  MK :  Q  Q, exp(QT ) = M(0, T )}.
If DK = MK × [0, 1]K represents the parameters space for the model, then the space DK = MK × [0, 1]K denotes the set of embeddable parameters and DK  DK. As it was shown in subsection 2.1, the solution to M(0, T ) = exp(QT ) may not be unique: this is the aliasing problem.
Let us consider now the set of matrices Q(k)  Q, solutions of the equation Q(k) = log(M(0, T ))/T , for k = 1 , . . . , B(M). B(M) is the number of continuous-time underlying processes corresponding to the discrete-time Markov chain represented by M(0, T )  MK. We have B(M)  N and B(M) = 0 if M / MK . Denote Q(k)(M) the intensity matrix that corresponds to the k-th solution of log(M), k = 1, . . . , B(M). Q(k)(M), 1  k  B(M), is a function defined for M  MK , Q(k)(M)  Q. Let h(k)(M) be a probability density function induced by a prior probability distribution on the k-th solution of the equation M(0, T ) = exp(QT ) when M  MK . By definition, h(k)(M) verifies kB=(M1 ) h(k)(M) = 1.
Let g(Q, s) be a function defined for (Q, s)  Q × [0, 1]K. This function is such that the evaluation of its moments (in particular, the posterior mean and the posterior standard deviation) is a question of interest. Thus, the posterior probability that the transition probability matrix M is embeddable has the form:

Pr[(M, s)  DK | (N, n)] =

L(M, s; N, n)(M, s) d(M, s)
DK
L(M, s; N, n)(M, s) d(M, s)
DK

(25.107)

Likelihood and Importance Functions

The likelihood function L  L(M, s; N, n) up to the initial distribution of the process

{X(t), t  0} is

K
L   Li

(25.108)

i=1

where:

Li = [ si + (1 - si) × {exp(QT )}Lii ]ni × (1 - si)ni(0)-ni

K

 ×{exp(QT )}iniii-Lni

{exp(QT )}nikik ,

k=i,k=1

(25.109)

{exp(Q T )}i,k denoting the entry (i, k) of the K × K matrix exp(Q T ). If Pr [M  MK | N, n] > 0, then

898

D. Fouge`re and T. Kamionka

E[g(Q, s) | (N, n); (M, s)  DK]

(25.110)

B(M)
 h(k)(M) g(Q(k)(M), s) L(M, s; N, n) (M, s) d(M, s)
= DK k=1 L(M, s; N, n) (M, s) d(M, s)
DK
In order to evaluate the integrals inside expressions (25.107) and (25.110), an
adaptation of the Monte-Carlo method may be used because an analytical expression for Q(k)(M) or B(M) when K  3 has not been found yet. Let I(M, s) be a probability density function defined for (M, s)  DK. I(M, s) is the importance function from which a sequence {Mi, si} of parameters will be drawn. We suppose that I(M, s) > 0 and that (M, s) and g(Q, s) are bounded above.
Let J(M) a function defined for M  MK:

J(M) =

1 if M  MK 0 otherwise

Then and

I

 J(Mi) L(Mi, si; N, n) (Mi, si)/I(Mi, si)

lim
I +

i=1

I

 L(Mi, si; N, n) (Mi, si)/I(Mi, si)

i=1

a=.s Pr[(M, s)  DK | N, n]

E[g(Q, s) | N, n; (M, s)  DK] a=.s

(25.111)

  I B(M) h(k)(Mi) g[Q(k)(Mi), si]J(Mi) L(Mi, si; N, n) (Mi, si)

lim i=1 k=1

I(Mi, si)

I+

I

 J(Mi) L(Mi, si; N, n) (Mi, si)/I(Mi, si)

i=1

(25.112)

where Pr[(M, s)  DK | N, n] is the probability under the posterior that the discretetime Mover­Stayer model is embeddable with the continuous time one, and E[g(Q, s) | N, n; (M, s)  DK ] defines the posterior moments of the parameters' function of interest.
For a better convergence of estimators (25.111) and (25.112), I(M, s) should be
concentrated on the part of DK where L(M, s; N, n) is nonnegligible. For that purpose, if (M, s) is not concentrated on some part of the set DK (that's the case when  is uniform), I(M, s) can be taken proportional to the likelihood L(M, s; N, n). Be-
cause drawing (M, s) from L(M, s; N, n) is difficult, Fouge`re and Kamionka (2003)

25 Econometrics of Individual Transitions Models

899

choose a normal expansion for L(M, s; N, n) with mean the ML estimator (M, s) and
with covariance matrix the inverse of the information matrix estimated at (M, s). When g(Q, s) and (M, s) are bounded above, the convergence of the estima-
tor (25.112) is obtained almost surely. When the function g(Q, s) does not verify
this property (for instance, if we are interested in the estimation of qi j), the convergence of the expression (25.112) relies on the existence of the posterior mean: E[g(Q, s) | (M, s)  DK ; N, n].
The covariance matrix V associated to L(M, s; N, n) is block diagonal with blocks
consisting of matrices Vi, i = 1, . . . , K, defined as:

Vi(M, s) = -E

 2Log(Li(M, s; N, n))  k l

-1
= Ri(M, s)-1

(25.113)

with k, l =

mi, j , si ,

i, j  E iE

where Ri(M, s)

is the

i - th diagonal

block

of

the

information matrix R(M, s) associated to L(M, s; N, n). Then a sequence of draws

{(Mk, sk)}k=1,...,I can be generated according to the density of a multivariate normal distribution with mean (M, s) and covariance matrix V (M, s) = R(M, s)-1. If we

suppose that Vi- = PiPi is the Choleski's decomposition of the matrix Vi- obtained by dropping the last row and column of matrix Vi , and if yk  N(0K, IK), then

  

si

si

zk = Pi yk + 

mi1 ...

  N(

mi1 ...

 ,Vi-)

(25.114)

miK-1

miK-1

Finally, we can obtain miK by setting miK = 1 - Kj=-11 mi, j. Inside the pro-
cedure, si, (mi,1, . . . , mi,K), and Vi are estimated by their MLE, respectively s^i, (m^ i,1, . . . , m^ i,K), and V^i. For more details, see Fouge`re and Kamionka (2003).

Limiting Probability Distribution and Mobility Indices

The mobility of movers can be appreciated by examination of the mobility indices for continuous-time Markov processes proposed by Geweke et al. (1986b). For the movers process with intensity matrix Q, four indices of mobility can be considered:

M1(Q) = - log[det(M(0, T ))]/K = -tr(Q)/K

K

K

  M2(Q) = (im) qi j | i - j |

i=1

j=1

K
 M3(Q) = - (jm)qi j j=1

M4(Q) = -e[log(2)]

(25.115)

900

D. Fouge`re and T. Kamionka

where:
· (im) is the equilibrium probability in state i for the movers, given by equation Q i(m) = 0, with iK=1i(m) = 1,
· the eigenvalues of the matrix M(0, T ) denoted by 1, . . . , K, are ordered so that | 1 | . . . | K |,
· e denotes the real part of the logarithm of the eigenvalue 2.

We can also define the equilibrium (or limiting) probability distribution for the mixed "mover-stayer" process {Xt ,t  R+}. For state i, the limiting probability, de-
noted i, is given by:

K
 i = sii + i(m) (1 - s j) j, i  E j=1

(25.116)

where:
·  = {i, i  E} is the initial probability distribution (i.e. at the date 0) for the process {Xt ,t  R+},
· and i(m) is the limiting probability of "movers" in state i.
It is easily verified that, for a purely markovian process (one for which si = 0, i  E), the formula (25.116) becomes i = i(m). The mobility indices (25.115) and the limiting distribution (25.116) can be estimated using formula (25.112) and taking respectively g(Q, s) = Mk(Q) (1  k  4), or g(Q, s) = .

Bayesian Inference Using Gibbs Sampling

The likelihood function of the sample X can be written

N2

  L(X|s, M, X0)=

L (X(n)|s, M, Xo(n), zn=k) Pr[zn=k|s, M, Xo(n)]

n=1 k=1

where L is the conditional contribution of the individual n given the initial state Xo(n) and the unobserved heterogeneity type zn. zn is an unobserved indicator taking the value 1 if the individual is a stayer or the value 2 if the individual is
a mover. The prior density on the parameter  = (s, M) is assumed to be the product of
the conjugate densities 1(s) and 2(M), where

 1(M)

=

K j=1

(a j + b j) (a j)(b j)

saj

j

-1(1

-

s

j

)b

j

-1

is the Dirichlet distribution with parameters a j > 0, b j > 0, j = 1, . . . , K, and

25 Econometrics of Individual Transitions Models

901

K

  2(M) =

K



ik

k=1

K

 i=1

(ik)

k=1

K miji j-1
i, j=1

is the matrix beta distribution with parameter i j > 0, i, j = 1, . . . , K. The conditional distribution of the unobserved type zn is thus

zn |  , X(n)  B(1; p(X(n);  ))

(25.117)

where

p(Xn;  ) =

L (X(n) | s, M, Xo(n), zn = 1) Pr[zn = 1 | s, M, Xo(n)]
2

 L (X(n) | s, M, Xo(n), zn = i) Pr[zn = i | s, M, Xo(n)]

i=1

Combining the prior and the sample informations we obtain that

N

N

  s j | X, Z  Dirichlet a j + i(jn)(2 - zn), b j + i(jn)(zn - 1)

n=1

n=1

(25.118)

N
M | X, Z  Matrix beta  ik + (zn - 1)Ni(kn); i, k = 1, . . . , K n=1

(25.119)

The Gibbs sampling algorithm runs like this: Initialization: Fix an initial value  (0) = (s(0), M(0)). Update from  (m) to  (m+1) by doing : 1 - Generate Z(m) according to the conditional distribution (25.117), given  =  (m)
and X; 2 - Generate  (m+1) = (s(m+1), M(m+1)) using the conditional distribution (25.118) and (25.119), given Z = Z(m) and X.
Under general regularity conditions and for m large enough, the resulting random variable  (m) is distributed according to the stationary posterior distribution ( | X). Draws from the stationary posterior distribution ( | X) may be used to obtain posterior estimates of  using an expression similar to the one given by (25.112)
(see Fouge`re and Kamionka, (2003)). Step one of the algorithm corresponds to a
data augmentation step (see, Robert and Casella, (2002)).

25.4 Concluding Remarks
This chapter has introduced reduced-form models and statistical methods for analyzing longitudinal panel data on individual labor market transitions. The first section gave a very general presentation of methods concerning continuous-time

902

D. Fouge`re and T. Kamionka

observations, while the second section focused on the treatment of discrete-time observations for continuous-time discrete-state processes.
Obviously, our survey did not intend to cover exhaustively a continuously and rapidly growing literature. Among subjects treated in this field of research, two topics seem to be especially important. The first one is the treatment of endogenous selection bias in dynamic populations (see Lancaster and Imbens, (1990), (1995), Lancaster, (1990b), Ham and Lalonde, (1996), and Fouge`re, Kamionka and Prieto, (2005)). Indeed, some sampling schemes for continuous-time discrete state space processes are such that the probability of being in the sample depends on the endogenous variable, i.e. being in a given state (for example, unemployment) at some date. Consequently inference from these endogenous samples requires specific statistical methods which have begun to be elaborated (see the papers quoted above). Another research area is the evaluation of the effect of public interventions such as employment and training programs. Here the main problem is knowing if these programs have a joint positive effect on earnings and employment rates of beneficiaries (see, for example, papers by Card and Sullivan, (1988), Heckman, (1990), Eberwein, Ham and Lalonde, (1997), Bonnal, Fouge`re and Se´randon, (1997), Heckman, Lalonde and Smith, (1999)). In order to avoid misleading results, this evaluation must take into account the selection biases induced simultaneously by the process of eligibility to the program and by the sampling scheme. Thus these two fields of research are very closely connected.

References
Aalen O.O., 1978, "Non parametric inference for a family of counting processes", The Annals of Statistics, Vol. 6, 701­726.
Amemiya T., 2001, "Endogenous Sampling in Duration Models", Monetary and Economic Studies, Vol. 19, No. 3, 77­96.
Anderson T.W. and Goodman L.A., 1957, "Statistical inference about Markov chains", Annals of Mathematical Statistics, Vol. 28, 89­110.
Baker M. and Melino A., 2000, "Duration dependence and nonparametric heterogeneity: a Monte Carlo study", Journal of Econometrics, Vol. 96, No. 2, 357­393.
Billingsley P., 1961, Statistical inference for Markov processes. The University of Chicago Press. Blumen I., Kogan M. and Mac Carthy P.J., 1955, "The industrial mobility of labor as a proba-
bility process", Cornell Studies of Industrial and Labor Relations, Vol. 6, Ithaca N.Y., Cornell University Press. Bonnal L., Fouge`re D. and Se´randon A., 1997, "Evaluating the impact of french employment policies on individual labour market histories", Review of Economic Studies, Vol. 64, No. 4, 683­713. Burdett K., Kiefer N., Mortensen D.T. and Neuman G., 1984a, "Earnings, unemployment and the allocation of time over time", Review of Economic Studies, Vol. 51, No. 4, 559­578. Burdett K., Kiefer N., Mortensen D.T. and Neuman G., 1984b, "Steady states as natural rates in a dynamic discrete choice model of labor supply", in: Studies in labor market dynamics, G.R. Neuman and N.C. Westergard-Nielsen (eds.), Berlin: Springer-Verlag, 74­97. Burdett K., Kiefer N. and Sharma S., 1985, "Layoffs and duration dependence in a model of turnover", Journal of Econometrics, Vol. 28, No. 1, 51­69. Butler J.S., Anderson K.H. and Burkhauser R.V., 1986, "Testing the relationship between work and health: a bivariate hazard model", Economics Letters, Vol. 20, 383­386.

25 Econometrics of Individual Transitions Models

903

Butler J.S., Anderson K.H. and Burkhauser R.V., 1989, "Work and health after retirement: a competing risks model with semiparametric unobserved heterogeneity", Review of Economics and Statistics, Vol. 70, No. 1, 46­53.
Card D. and Sullivan D., 1988, "Measuring the effect of subsidized training programs on movements in and out of employment", Econometrica, Vol. 56, 497­530.
Chesher A., and Lancaster T., 1983, "The estimation of models of labour market behavior", Review of Economic Studies, Vol. 50, No. 4, 609-624.
Chung K.L., 1967, Markov chains with stationary transition probabilities. Berlin: Springer - Verlag. Doob J.L., 1953, Stochastic processes. New-York: Wiley. Eberwein C., Ham J.C. and Lalonde R.J., 1997, "The impact of being offered and receiving class-
room training on the employment histories of disadvantaged women: evidence from experimental data", The Review of Economic Studies, Vol. 64, No. 4, 655­682. Eberwein C., Ham J.C. and Lalonde R.J., 2002, "Alternative methods of estimating program effects in event history mdoels", Labour Economics, Vol. 9, 249­278. Eckstein Z. and Wolpin K.I., 1989, "The specification and estimation of dynamic stochastic discrete choice models", Journal of Human Resources, Vol. 24, No. 4, 562­598. Elfving G., 1937, "Zur Theorie der Markoffschen Ketten", Acta Social Science Fenicae, series A2, No. 8, 1­17. Flinn C.J. and Heckman J.J., 1982a, "New methods for analyzing structural models of labor force dynamics", Journal of econometrics, Vol. 18, 115­168. Flinn C.J. and Heckman J.J., 1982b, "Models for the analysis of labor force dynamics", in: Advances in Econometrics, Vol. 1, R. Basmann and G. Rhodes (eds.), Greenwich, Conn.: JAI Press, 35­95. Flinn C.J. and Heckman J.J., 1983a, "The likelihood function for the multistate-multiepisode model", in: Advances in econometrics, Vol. 2, R. Basmann and G. Rhodes (eds.), Greenwich, Conn.: JAI Press, 225­231. Flinn C.J. and Heckman J.J., 1983b, "Are unemployment and out of the labor force behaviorally distinct labor force states?", Journal of labor economics, Vol. 1, No. 1, 28­42. Fouge`re D. and Kamionka T., 2003, "Bayesian inference for the mover-stayer model in continuoustime with an application to labour market transition data", Journal of Applied Econometrics, Vol. 18, No. 6, 697­723. Fouge`re D., Kamionka T. and Prieto A., 2005, "Stock-sampling bias in unemployment competingrisks duration models", mimeo, CREST-INSEE, Paris. Frydman H., 1984, "Maximum likelihood estimation in the mover-stayer model", Journal of the American Statistical Association, Vol. 79, 632­638. Geweke J., Marshall R.C. and Zarkin G.A., 1986a, "Exact inference for continuous time Markov chains". Review of Economic Studies, Vol. 53, 653­669. Geweke J., Marshall R.C. and Zarkin G.A., 1986b, "Mobility indices in continuous-time Markov chains", Econometrica, Vol. 54, No. 6, 1407­1423. Gilbert L., Kamionka T. and Lacroix G., 2001, "The Impact of Government-Sponsored Training Programs on the Labor Market Transitions of Disadvantaged Men", Crest Working Paper 200115, Paris. Goodman L.A., 1961, "Statistical methods for the mover-stayer model", Journal of the American Statistical Association, Vol. 56, 841­868. Gourie´roux C. and Monfort A., 1997, Simulation-based econometric methods, Oxford: Oxford University Press. Gritz R.M., 1993, "The impact of training on the frequency and the duration of employment", Journal of Econometrics, Vol. 57, 21­51. Ham J.C. and Lalonde R.J., 1996, "The Effect of Sample Selection and Initial Conditions in Duration Models: Evidence from Experimental Data on Training", Econometrica, Vol. 64, No. 1, 175­205. Heckman J., 1990, "Alternative approaches to the evaluation of social programs ; Econometric and experimental methods", Barcelona Lecture, 6th World Congress of the Econometric Society, Barcelona, Spain.

904

D. Fouge`re and T. Kamionka

Heckman J.J. and Borjas G.J., 1980, "Does unemployment cause future unemployment? Definitions, questions and answers from a continuous time model of heterogeneity and state dependence", Economica, Vol. 47, 247­283.
Heckman J.J., Lalonde R. and Smith J., 1999, "The economics and econometrics of active labor market policies", in: The Handbook of Labor Economics, Vol. 3A, O. Ashenfelter and D. Card (eds.), Amsterdam, North-Holland, 1865­2097.
Heckman J.J., and Singer B., 1984, "Econometric duration analysis", Journal of Econometrics, Vol. 24, 63­132.
Kalbfleisch J.D. and Lawless J.F., 1985, "The analysis of panel data under a Markov assumption", Journal of the American Statistical Association, Vol. 80, No. 392, 863­871.
Kamionka T., 1998, "Simulated maximum likelihood estimation in transition models", Econometrics Journal, Vol. 1, C129­153.
Kamionka T. and G. Lacroix, 2003, "Assessing the impact of non-response on the treatment effect in the Canadian self-sufficiency experiment", Crest working paper 2003-37, forthcoming in Annales d'Economie et de Statistique.
Kingman J.F.C., 1962, "The imbedding problem for finite Markov chains", Zeitschrift fur Wahrscheinlichkeitstheorie, No. 1, 14­24.
Klein J.P. and Moeschberger M.L., 2003, "Survival analysis: techniques for censored and truncated data", Second Edition, New York: Springer.
Lancaster T., 1990a, The econometric analysis of transition data. New York, USA, Cambridge University Press, 336p.
Lancaster T., 1990b, "A paradox in choice-based sampling", Working Paper 9017, Department of Economics, Brown University.
Lancaster T. and Imbens G.W., 1990, "Choice-based sampling of dynamic populations", in: Panel data and labor market studies, J. Hartog, G. Ridder and J. Theeuwes (eds.), Amsterdam, NorthHolland, 21­44.
Lancaster T. and Imbens G.W., 1995, "Optimal stock/flow panels", Journal of Econometrics, Vol. 66, No. 1­2, 325­348.
Lindeboom M. and van den Berg G., 1994, "Heterogeneity in models for bivariate survival: the importance of the mixing distribution", Journal of the Royal Statistical Society, Series B, 56, 49­60.
Magnac T. and Robin J.M., 1994, "An econometric analysis of labour market transitions using discrete and tenure data", Labour Economics, Vol. 1, 327­346.
Magnac T., Robin J.M. and Visser M., 1995, "Analysing Incomplete Individual Employment Histories Using Indirect Inference", Journal of Applied Econometrics, Vol. 10, S153­S169.
Mealli F. and Pudney S., 1996, "Occupational pensions and job mobility in Britain: estimation of a random-effects competing risks model", Journal of Applied Econometrics, Vol. 11, 293­320.
Mealli F. and Pudney S., 2003, "Applying heterogeneous transition models in labor economics: the role of youth training in labour market transitions", in: Analysis of Survey Data, R. L. Chambers and C. J. Skinner (eds.), New-York, Wiley, 245­274.
Mortensen D., 1986, "Job search and labor market analysis", in: Handbook of Labor Economics, Vol. 2, O. Ashenfelter and R. Layard (eds), Amsterdam, North-Holland, 849­919.
Mortensen D.T. and Neuman G.R., 1984, "Choice or chance? A structural interpretation of individual labor market histories", in: Studies in labor market dynamics, edited by G.R. Neuman and N.C. Westergard-Nielsen, Berlin: Springer-Verlag, 98­131.
Olsen R., Smith D. and Farkas G., 1986, "Structural and reduced-form models of choice among alternatives in continuous time: youth employment under a guaranted jobs program", Econometrica, Vol. 54, 375­394.
Ondrich J., 1985, "The Initial Conditions Problem in Work History Data", Review of Economics and Statistics, Vol. 67, No. 3, 411­421.
Phillips P.C.B., 1973, "The problem of identification in finite parameter continuous time models", Journal of Econometrics, Vol. 1, No. 4, 351­362.
Ridder G., 1986, "An event history approach to the evaluation of training, recruitment and employment programmes", Journal of applied econometrics, Vol. 1, No. 2, 109­126.

25 Econometrics of Individual Transitions Models

905

Robert Ch. P. and G. Casella, 2002, Monte Carlo Statistical Methods. New York: Springer. Runnenberg J.Th., 1962, "On Elfving's problem of imbedding a discrete-time Markov chain in
a continuous-time one for finitely many states", Proceedings, Koninklijke Nederlandse Akademic van Wetenschappen, ser. A, Mathematical Sciences, Vol. 65, No. 5, 536­41. Rust J., 1994, "Dynamic structural models; Problems and prospects: discrete decision processes", in C. Sims (ed): Advances in Econometrics, Sixth World Congress, Vol. II, Econometric Society Monographs, Cambridge: Cambridge University Press, 119­170. Sampson M., 1990, "A Markov chain model for unskilled workers and the highly mobile", Journal of the American Statistical Association, Vol. 85, No. 409, 177­180. Singer B., 1981, "Estimation of nonstationary Markov chains from panel data", Sociological Methodology, San Francisco. Samuel Leinhart-Jossey-Bass Publishers. Singer B., 1982, "Aspects of non-stationarity", Journal of Econometrics, Vol. 18, 169­190. Singer B. and Spilerman S., 1976a, "The representation of social processes by Markov models", American Journal of Sociology, Vol. 82, No. 1, 1­54. Singer B. and Spilerman S., 1976b, "Some methodological issues in the analysis of longitudinal surveys", Annals of Economic and Social Measurement, Vol. 5, No. 4, 447-474. Spilerman S., 1972, "Extensions of the mover-stayer model", American Journal of Sociology, Vol. 78, No. 3, 599­626. Trivedi P.K. and Alexander J.N., 1989, "Re-employment probability and multiple spells: a partiallikelihood approach", Journal of business and economic statistics, Vol. 7, No. 3, 395­401. Tuma N.B. and Robins P.K., 1980, "A dynamic model of employment behavior: an application to the Seattle and Denver Income Maintenance Experiments", Econometrica, Vol. 48, No. 4, 1031­1052. Van den Berg G.J., 1997, "Association measures for durations in bivariate hazard rate models", Journal of Econometrics, Vol. 79, No. 2, 221­245.

Chapter 26
Software Review
Pierre Blanchard

26.1 Introduction
Since the previous edition of this book, econometric software used for estimating panel data models have been improved on three different scales. First, many packages allow now to estimate linear models on huge panel data sets made up of thousands of individuals with moderate execution times, even on a personal computer. Second, the estimation of linear dynamic panel data models is becoming more and more frequent. With several packages, it is now easy, i.e. with short programming, to estimate such models by instrumental variable method or by generalized method of moments. Lastly, the evolution has been the most significant in non linear model estimation on panel data. As shown in this book, an important literature on discrete choice and count models estimation with panel data, to name only a few, has been developed. Estimating such models is now an essential trend for applied econometricians and the need for appropriate econometric software is great.
Furthermore, a rapid glance to publications1 frequently reviewing econometric software shows that the number of software used by econometricians (including their add-on modules and user application programs) has considerably increased. Even if we restrain ourselves to those which can be easily used with panel data, a great deal offer various econometric methods and provide different environments. Software for econometrics on panel data can broadly be classified into two main groups:
· First, we find general-purpose econometric packages, like LIMDEP, RATS, SAS, TSP. . . using command-driven languages, pull-down menus or both. They are

Pierre Blanchard Erudite, Faculte´ de Sciences Economiques et de Gestion, Universite´ Paris XII Val de Marne, 61 Av. du Ge´ne´ral de Gaulle, 94010 Cre´teil Ce´dex, France, e-mail: blanchard@univ-paris12.fr
1 For instance, Computational Economics, Economic Journal, Journal of Applied Econometrics, Journal of Economic Surveys. . .

L. Ma´tya´s, P. Sevestre (eds.), The Econometrics of Panel Data,

907

c Springer-Verlag Berlin Heidelberg 2008

908

P. Blanchard

easy to use and flexible to allow many, but not all, sorts of model estimation. For specific problems, they offer very often a matrix programming language. · The second group consists of high-level matrix programming languages such as GAUSS, Ox. . . A common feature of these languages is that they are oriented towards intensive matrix manipulations. They require a good level of skill in programming, but they have the advantage of being largely flexible and, potentially at least, very fast. They are mainly used for hard to program estimation methods (non linear models), Monte-Carlo simulation. . .
Among the six software selected in the 1996's version of this chapter, five of them have made significant improvements and then will be reviewed here: four generalpurpose econometric packages, LIMDEP, RATS, TSP, SAS and one high-level matrix programming language, GAUSS. Three others, not included in the previous edition, were added: two general-purpose econometric packages: EViews and Stata; one high-level matrix programming language: Ox.
Moreover, some very well known software are not included in this review, for instance, Shazam, SPSS. . . (general-purpose econometric packages), O-Matrix, R, S-Plus. . . (high-level matrix programming languages) and some specialized software (Frontier. . . ). We did not review them because we need some basic econometric methods for panel data (within, between, FGLS, IV. . . estimators) which must be available without tedious programming (or with template programs and adds-ons).
In the first two sections of this chapter, we analyze the selected software belonging to each two groups enumerated above. Most of these software are regularly reviewed, so we discuss them mainly in terms of panel data management and relevant econometric methods they offer. We illustrate2 their capabilities in estimating first, linear static and dynamic panel data models and, second (when possible), several non linear models (a random effects model and a random effects probit model estimated by maximum likelihood method).
As emphasized by McCullough and Vinod (1999), numerical accuracy is an important, but often neglected, characteristic of econometric software. Section 26.3 is devoted to this problem and to performance evaluation.

26.2 General-Purpose Econometric Packages
26.2.1 EViews (v. 5.1)
EViews (Econometric Views) is mainly used for graphical and statistical analysis, time series estimation and model simulation (see Roberts, 1995, Holly and Turner, 1998, Sparks, 1997 and Noble, 1999). Thanks to version 4, it was already possible to estimate some panel data models with the pool object, which is useful
2 All the programs, data and web links used in this chapter can be found on a companion web site of this volume (http://www.univ-paris12.fr/epd).

26 Software Review

909

Table 26.1 EViews code #1 (grunfeld eviews.prg)

cd c:\epd\data\eviews

' default directory

wfcreate(wf="grunfeld") a 1935 1954 10 ' creating the workfile

read(t=xls,a2) "grunfeld.xls" 11

' loading the excel file

when dealing with a panel dataset with small N and large T. The last release3 offers now extended panel data management capabilities and estimation features for balanced and unbalanced panel, particularly when the panel datasets contain many individuals. One of the interesting features of EViews is that this software is designed to be used mainly by pull-down menu. A program mode (with also a command line interface) is also available, and, for space limitation reasons, we limit our presentation to the batch mode, which is not necessarily the simplest one. Another particular aspect of this software is the concept of workfile which is central to EViews' working. A workfile is a container for EViews objects (equations, graphs, matrices, series. . . ) and each workfile may be formed of one or more workfile pages (each containing specific objects). EViews can manage series of maximum 4 million observations for each, and the total number of observations is only limited by available RAM size.4 When a user works on a panel data set, its workfile will contain several series which are associated with an individual identifier and a date identifier. Entering panel data in the workfile depends on the nature of the panel data set.
If the panel is a balanced one and if the basic dataset is, for instance, in an Excel format, the following code5 (see Table 26.1) will execute this task for the Grunfeld data (N = 10, T = 20).
In the wfcreate instruction, a stands for annual data (beginning in 1935 and ending in 1954, but other periodicities instead of annual may be used) and 10 indicates than the panel contains 10 individuals. The read instruction can import ASCII, Lotus and Excel files. For reading more file formats (Stata, SAS transport, HTML, GAUSS, GiveWin, RATS, TSP. . . ), in addition to the previous ones, one can use the wfopen instruction (see Table 26.2).
If the panel is unbalanced, you must follow three steps. First, you have to create an unstructured workfile with the wfcreate instruction. Second, read the external data file (or even an EViews file) with a read or wfopen instruction. Third, it is necessary to structure the workfile as an unbalanced panel with the pagestruct instruction applied to an individual identifier (say ident) and to a date identifier

3 EViews 6 is now available. 4 For instance, with 256 Mb RAM, a workfile may contain more than 1900 series with 100,000 observations each. 5 In this chapter, sofware commands, options and Web links are written in Typewriter font. For all softwares, except EViews, Ox and Stata, the first letter of a command is written in uppercase; file names and (user) variable names are written in lowercase. For Stata, commands, file names and variables names are always in lowercase. For Ox, names are case sensitive. We follow the EViews documentation in writing all in lower cases.

910

P. Blanchard

Table 26.2 EViews code #2 (ab eviews.prg)

cd c:\epd\data\eviews

' default directory

wfcreate(wf="ab") u 1 1031

' creating the undated structure

wfopen "c:\epd\data\excel\ab.xls"

' loading the excel file

pagestruct ident @date(year)

' creating the panel structure

(say year). These steps are illustrated by the following piece of code using data from Arellano and Bond, 1991 (hereafter AB), saved initially in an Excel file.
Of course, if the data are already saved in a panel data workfile, wfopen "c:\epd\data\excel\ab.xls" would be sufficient. If the workfile has not a panel structure, add only a pagestruct instruction (you will need an individual identifier and a date identifier) after the instruction wfopen.
Moreover, a sort instruction with several sort keys is available and merging workfiles is also possible by using links facilities (easy to do by menus). Alphanumeric series and several date formats are recognized in a workfile, but EViews considers alphanumeric series in an Excel or an ASCII files as missing values (coded by na). A complete programming language may be used for complex tasks.
Once, your workfile is ready, graphics, descriptive statistics and estimation on panel data estimation are quite straightforward. First, EViews offers several useful tools (statistics, graphics, test for equality of means and variances between groups. . . ) as shown by the following program reproduce in Table 26.3.
This code illustrates another EViews characteristic: the object concept. For instance, the previous program defines a group object (named g1 referring to the variables year and gi). Then, we can apply some view (a graph, some tests for instance) or some procedures (e.g. an equation estimation) to this object. Static panel data estimation is easily carried out by few instructions. Nevertheless, there is one exception: the between estimator which requires a great deal of programming (this cannot be done by menus). We do not report how to obtain between estimation due to space limitations (see grunfeld eviews.prg).
The ls instruction (see Table 26.4) offers several options which are very useful for panel data estimation, particularly several ways to apply robust methods for computing the coefficient standard errors (White, SUR. . . ). One-way and two-way specifications are also supported as well as different ways to estimate the component variances in random effects models (Swamy-Arora, Wallace-Hussain, WansbeekKapteyn).

Table 26.3 EViews code #3 (grunfeld eviews.prg)

vc.statby ident

' descriptive statistics by individual

group g1 year gi

' defining a group of variables

g1.xy(panel=individual)

' individual graphs

gi.testby(mean) ident

' anova

26 Software Review
Table 26.4 EViews code #4 (grunfeld eviews.prg)
equation ols.ls gi c vf vc equation within.ls(cx=f) gi vf vc c equation ec.ls(cx=r) gi vf vc c ec.ranhaus

911
' ols ' within
' fgls 'Hausman test

Dynamic panel data estimations6 are equally easy to obtain by the use of menu (Proc and by choosing the GMM/DPD method) or by the Dynamic Panel Wizard (a succession of 6 dialog windows). This can be also done by programming as indicated in Table 26.5 which can reproduce the AB results (Tables 4 and 5, pp. 290, 292). We can also apply the Anderson­Hsiao estimator (hereafter AH, see Anderson and Hsiao, 1981). Note that the underscore (" ") is the continuation line character.
This program requires several explanations.
1. First, note the use (new to EViews 5) of replacement variables (called sometimes macro variables) quite useful for defining variable lists used repetitively. For instance, %inst = "w w(-1) k k(-1) k(-2) ys ys(-1) ys(-2)" defines a list of instruments which can be used when {%inst} is referred to in the program. Remark also that there is only one instruction in each line. A long instruction may be set in several lines with the (underscore) character.
Table 26.5 EViews code #5 (ab eviews.prg)
' define replacement variables %rhs = "n(-1) n(-2) w w(-1) k k(-1) k(-2) ys ys(-1) ys(-2)" %model = "n " + %rhs %inst = "w w(-1) k k(-1) k(-2) ys ys(-1) ys(-2)" ' (5g) OLS with White robust se equation ols.gmm(per=f,gmm=perwhite) {%model} @ {%rhs} ' (5h) LSDV with White robust se equation within.ls(cx=f,per=f,cov=perwhite,nodf) {%model} ' (5e) AH with n(-3) in diff., White robust se. equation ahd.gmm(cx=fd,per=f,levelper,cov=perwhite,nodf)
{%model} @ n(-2) n(-3) {%inst} ' (5f) AH with n(-3) in lev., White Period Robust se. equation ahl.gmm(cx=fd,per=f,levelper,
gmm=ident,cov=perwhite,nodf) {%model} @ n(-2) @lev(n(-3)) {%inst} ' (4a1) AB first-diff., period dum. in lev., 1st step robust. equation aba1.gmm(cx=fd,per=f,levelper,cov=perwhite,nodf) {%model} @ @dyn(n) {%inst} ' (4a2) - AB first-diff., period dum. in lev., 2nd step robust equation aba2.gmm(cx=fd,per=f,levelper,gmm=perwhite)
{%model} @ @dyn(n) {%inst}

6 Instrumental variable estimation can be obtained with the tsls instruction.

912

P. Blanchard

2. EViews propose several functions that are useful for panel data. @expand(ident) owes to create automatically individual dummy variables (one for each individual in the panel). There are also instructions for creating trend variables (@trend, @trendc. . . ).
3. Option cx= requires fixed effects estimation when cx=f, first difference estimation when cx=fd and orthogonal deviation when cx=od. By default, no transformation is done. When per=f, time dummies are included, and if levelper is specified, time dummies are in levels even if cx=fd or cx=od.
4. gmm and cov Options define GMM weighting (identity, White. . . ) and coefficient covariance computation method (White, SUR, ordinary. . . ).
5. The special instruction @dyn(n,-2,-5) permits to include lags of n from 2 to 5 as instruments as suggested by Arellano and Bond, (1991). With @dyn(n), EViews will incorporate all the possible lags.
There are other EViews capabilities that have to retain our attention. EViews 5 is the first software which offers panel data unit root tests without any programming. In recent years, there has been growing interest in the use of macroeconomic panel data (sets of countries, regions or industries -large N- on many periods -large T-). It is frequently advocated that panel data unit root tests have higher power than those done on time series data. With EViews, several tests are available (provided with many options): (1) Levin, Lin and Chu, (2) Breitung, (3) Im, Pesaran and Shin, (4) Fisher-type using ADF (5) Fisher-type using PP and (6) Hadri tests. The availability of these tests associated to many estimation methods on pooled time-series cross-section data (T large, N small) will certainly be useful for panel data macroeconomists.
Up to now, non linear model estimation (binary probit/logit, ordered models, censored models, count models. . . ) are limited to cross section data. Nevertheless, EViews offers a specific procedure (called the Log Likelihood Object) to estimate other non linear models by the maximum likelihood method (see chronoml eviews.prg used in Sect. 26.4.1). Yet, it seems difficult to adapt it to non linear panel data estimation (random effects probit model, by instance) when it is necessary to evaluate separately the log likelihood by individuals on Ti periods.
The on-line help and the paper documentation (2 volumes, more than one hundred pages on pooled and panel data management and estimation) are quite good. The web site contains mainly commercial information. A EViews group discussion via E-mail or via Usenet is not available. It would be useful that, in the future, more program examples on panel data will be downloadable.

26.2.2 LIMDEP (v. 8) with NLOGIT (v. 3)
LIMDEP7 (LIMited DEPendent variable models) was initially designed, as its name indicates, in order to estimate models having limited or qualitative dependant
7 LIMDEP 9 and NLOGIT 4 are available.

26 Software Review

913

variables. Successive releases have improved its coverage of econometric methods in many domains.
As noted by McKenzie and Takaoka (2003), LIMDEP Version 8 (with 3.0 for Nlogit) "represents a significant expansion of the estimation techniques for panel data models". For some discrete choice models estimation on panel data, you will need Nlogit, available at a substantial extra cost. In all cases, very few programming is required.
LIMDEP requires a microcomputer running under Windows 95 or a later version and works either in batch mode or by menu. LIMDEP uses four key windows: the output window (which reports log and results), the project window, the command window and the data editor window. As our focus is concentrated only on batch mode, we let aside the use of menu and use mainly the command window. On this point, one of LIMDEP's interesting feature is that it produces a trace file (trace.lim) which gives a complete trace of the LIMDEP's session (list of commands obtained when using the mode menu, for instance). Further details on LIMDEP's general features may be founded in Fry (1996), Heywood and Rebelo (1996), Wright (1996), McCullough (1999b), McKenzie (2000) and McKenzie and Takaoka (2003).
As it is simply impossible to give a complete list of the LIMDEP capabilities in panel data field, we sketch only the most important. Note that, generally speaking, little programming is required.
As we focus on the batch mode, we only need to describe the panel data file and the file (.lim) containing LIMDEP's code .
Firstly, LIMDEP may manage large (balanced or unbalanced) panel datasets with a limit8 of 900 variables (see the rows instruction). LIMDEP may read various file formats, mainly ASCII, XLS, binary. . . A panel data is supposed to be organized by individuals; if not, a Sort instruction with one key is available for re-ordering the panel if necessary (you may use too the Reorder options of Regress command). Merging panel datasets is not possible only in the case when you have to merge a panel dataset containing invariant variables with a usual panel dataset (in Read instruction). Obviously, you need a common individual identifier. So, that is why one needs (unless your data are balanced) a panel dataset that contains an individual identifier (say ident) which can be of any numerical values. Note also that missing values are managed in a very particular way by LIMDEP. Missing values are coded with -999. Whether these observations are eliminated for computations or not depends on the command being used. For panel data estimation, LIMDEP will exclude all the rows for an individual if one observation is missing. Notice that, with panel data, the Create instruction like Create; x1 = x[-1] $ does not take into account the switching from an individual to another one. In this case, you can use the Reject instruction to eliminate some observations, but this may contradict the Pds specification (see below, but don't use the Skip instruction). So, users have to be very careful, in particular with dynamic models.

8 See Limdep's documentation for a complete list of program limits. For instance, with the fixed effects binary logit model, 100 periods for one individual is a maximum.

914

P. Blanchard

Table 26.6 Limdep code #1 (grunfeld.lim)
Reset $ Read ; File = C:\epd\data\excel\grunfeld.xls; Format=Xls ; Names $ Title ; ols, within and fgls estimations $ Regress ; Lhs = gi ; Rhs = One, vf, vc ; Pds=20 ; Panel $ Title ; between estimation $ Regress ; Lhs = gi ; Rhs = One, vf, vc ; Pds=20 ; Means ; Panel $

Secondly, the definition of the panel data structure is set up when specifying the estimation instructions, and not when reading the data file. In Table 26.6, we present a simple example on Grunfeld's data.
This short program suggests several remarks:
· In batch mode, you have to be careful in typing commands which use $ and ; symbols in a different way from other software (TSP, SAS. . . ). Commands end with a $ and ; is the symbol for separating options (if any) in a command. Comments are included with the ? comment ; or with the /* comment */ syntax. In the same way, LIMDEP's programming language may be different from others in the way it treats loop's instructions, procedures etc.
· As our panel is balanced, we specify Pds = 20, because the panel has 20 periods by individual (a group for LIMDEP). If the panel was unbalanced, you have to write Pds = ni where ni is a variable which gives, each individual, the number of period observations (for instance 4, 4, 4, 4, 2, 2, 3, 3, 3). Our model being a linear one,9 it would be possible to replace the Pds specification by Str = ident where ident is a numerical variable containing a unique numeric identifier code. It is possible to create a ni (and a new ident identifier) variable by the following code: Regress ; Lhs = One; Rhs = One; Str = ident ; Panel $. This will create two new variables: groupti which contains the number of observations by individual, and stratum containing a new individual identifier equal to 1, 2, . . . , N. Another useful instruction (not shown above) Expand, may transform a categorical variable into a set of dummy variables.
· Our Panel option requires an estimation for the panel data, in our example, a one-way fixed or random effects linear models. Through Means, Fixed Effects or Random Effects options, it is possible to obtain only specific results. Note also, that the output (not shown) is very clear and detailed; many useful tests are automatically reported (Hausman, F, LR and LM tests). Other tests may be programmed with LIMDEP's programming language. For a two-way specification, just specify Period = timev where timev is a variable name containing integers like 1, 2, . . . , Ti but not like timev = 1981, 1982, . . . , 1999.

9 For non linear models estimation on panel data, the STR specification is not possible, you must provide Pds = varname or Pds = number.

26 Software Review

915

· Many other possibilities are available (with specific instructions or program templates): Restricted estimation, White robust estimation, random effects model estimation with autocorrelated error structure, estimation by 2SLS (for fixed effects model), the Hausman-Taylor (hereafter HT) estimator for random effects model, estimation of Hildreth-Houck and Swamy's random coefficients models etc.
For dynamic linear panel data models, Arellano­Bond-Bover GMM's estimator is available with LIMDEP, but it correctly works only with LIMDEP 9 (see the program called ab.lim, not reproduced here).
As far as non linear models on panel data are concerned, the range of LIMDEP capabilities is huge. With LIMDEP, you can estimate probit/logit, truncation, ordered probit/logit, poisson, negative binomial, bivariate probit, sample selection and stochastic frontier models with either fixed or random effects. With the Nlogit package, multinomial logit, heteroscedastic extreme value, random parameters logit, nested logit, latent class, multinomial multiperiod probit models among others may also be estimated. We will study only two illustrations. First, in Table 26.7, we estimate (on Grunfeld's data) a one-way linear random effects model by ML.
This example shows how we can use the Gxbr instructions for computing individual means, and the deviations to individual means. It indicates also that it is easy to write the log-likelihood for an observation. Note that it is possible to supply (not shown here) first (but not second) derivatives when speed and convergence problems are important. If the log likelihood has to be computed in using separately the Ti log-likelihood for an individual, it will be more complicated.
Fortunately, for many situations, LIMDEP does it automatically. Very few programming is then required for estimating mixed, binary, multinomial probit/logit. . . models. For instance, in Table 26.8, we estimate on Keane and Wolpin's data (hereafter KW, see Keane and Wolpin, 1997) a random effects binary probit model.

Table 26.7 Limdep code #2 (grunfeld.lim)
? creating the individual means (used by ml) Matrix ; mgib = Gxbr(gi,ident) ; mvfb = Gxbr(vf,ident) ;
mvcb = Gxbr(vc,ident) $ Create ; gib = Mgib(ident) ; vfb = Mvfb(ident) ;
vcb = Mvcb(ident) $ ? creating deviations to individual means Create ; giw = gi - gib ; vfw = vf - vfb ; vcw = vc - vcb $ Title ; ML estimation of error components model $ Calc ; tt = 20 ; nn = 10 ; nt = 200 $ Maximize ; Start = 1.0 , 1.0 , 1.0 , 1.0 , 1.0 ;
Labels = b0 , b1 , b2, s2u , s2a ; Alg = Bfgs ; Fcn = -Log(2*3.14116)/2-Log(s2u)*(tt-1)/(2*tt)
-Log(s2u+tt*s2a)/(2*tt)-(1/(2*s2u))*((giw - b1*vfw b2*vcw)^2)
-(1/(2*(s2u+tt*s2a)))*((gib - b0 - b1*vfb b2*vcb)^2) $

916

P. Blanchard

Table 26.8 Limdep code #3 (discrete choice.lim)

Reset $ ? NB: ident (1 to 1838), year (81 to 87) ? NB : choice 1= school, 2=home, 3=wcollar, 4=bcollar, 5=serv ? Reading an Excel data file Read ; File = c:\epd\data\excel\kw.xls;Format=XLS;Names $ ? creating a variable named groupti given Ti Regress ; Lhs = One ; Rhs = One ; Str = ident ; Panel $ ? Creating the 0/1 variable for binomial models Create ; status = choice > 2 $ Title ; one way random effects probit model $ Probit;Lhs=status;Rhs=One,educ,exper,expersq,black;
Pds= groupti ; Random Effects; Maxit = 500 $ Stop $

The documentation is good, large (may be too large, one Reference Guide and two Econometric Modeling Guides, plus a separate documentation for NLOGIT, all with many examples) and sometimes we encounter some repetitions (see, for instance, § R6-6 and § R16-24). Nevertheless, even if you have not planned to use LIMDEP, you ought to read, at least, the two chapters entirely devoted to panel data models (chapter E8, Linear Models for Panel Data and E14, Panel Data for Binary Choice). Indeed, they contain very interesting information concerning these models, in particular, many illuminating technical details and very judicious notes on practical aspects of econometric modeling with panel data. The LIMDEP's listserv is of equal interest.

26.2.3 RATS (v. 6)
Even if RATS (Regression Analysis of Time Series) is mainly designed for time series analysis, it also provides a few special instructions to estimate linear models on panel data.
One interesting RATS' feature is that it deals explicitly with the panel data structure. This can be done with the Calendar and Allocate instructions10 which define the data structure. For instance, if you use the Grunfeld's data (a balanced panel dataset), you may write, at the beginning of your RATS program, the following code:11
Calendar(Panelobs=20) 1935 1 1 Allocate 10//1954:1
10 Allocate is now optional with RATS 6. 11 A Wizard environment is also available for some simple tasks.

26 Software Review

917

This panel data set have 10 individuals (firms), observed between 1935 and 1954 (20 years). The observation for a variable, say y, for individual n3 observed in 1950 is referenced like y(3//1950:1). Note that, for RATS, a panel data set is always organized by individual (all time periods for the first individual, followed by all time period for the second individual,. . .). Moreover, the periodicity of the panel may be annual, quarterly, biannual, monthly, weekly. . . In adjusting the Calendar and Allocate, you could extract easily a sub sample from the file too.
The panel doesn't need to be balanced, as it is the case for AB data. Some caution is yet required: if you are reading a non RATS data file (Excel for instance), your file must be "balanced" in a special sense, i.e. each individual must be observed during the same period, but when an individual is not observed in a given year, variables (but not the date's entry) have to be set to missing values (%NA for RATS). You can balance it manually or in using the Pform instruction (see ab rats.prg for an illustration). This instruction may reorganize unbalanced panel data series (an index series for the individuals and for the time periods are then required); RATS inserts missing values for all variables when a year for an individual is missing. When the operation is realized, you can save your "balanced panel dataset" in a RATS data file. It is then possible, for instance, that an individual begins in 1976 and stops in 1986 and another one begins in 1979 and stops in 1983.
Suppose now that your panel data set is well defined. Thanks to the Calendar and Allocate instructions, creating new series becomes very simple. Particularly, if you create a lagged series by SET yl1 / = y{1}, the series yl1 contains a missing value in the first year of each individual. This same property applies to differenced and led series. Moreover, RATS documentation provides simple code for creating trend series, individual and time dummies using specific panel function %trend(t), %indiv(t) and %period(t). When your variables (the series for RATS) are correctly defined, you have several ways for estimating a panel data model.
RATS offers various instructions to estimate a static linear model with random or fixed effects (one-way or two-way).
To estimate a (one-way) random and fixed effects model on the Grunfeld's data, the most simple way is to use the Pregress instruction, see Table 26.9.
Unbalanced panels are correctly taken into account in the computation of i. Another way that aims at estimating a (one-way) fixed or random effects model consists in using, first the Panel instruction to transform the series in taking deviation to the individual means, and then to estimate the transformed model by OLS. As an example, in order to apply the within estimator, we can transform the original series in deviation to the individual means by instructions reported in Table 26.10 which create giwit = giit -gii., . . . and then, estimate the transformed model by OLS. Note that (Dfc=10) is necessary for correcting the number of degrees of freedom that are lost by substracting out the 140 (we suppose N = 140) individual means. The Pregress instruction do not have a constant term. We can proceed in a similar way to estimate a model by between or FGLS estimator. Moreover, a random effects model can be also estimated in using first, the instruction Pstats applied to the OLS residuals to obtain estimations of a u2 and

918

P. Blanchard

Table 26.9 RATS code #1 (grunfeld rats.prg)
End 1 Calendar(Panelobs=20) 1935 1 1 Allocate 10//1954:1 * Reading the xls data file Open Data C:\epd\data\excel\grunfeld.xls Data(Unit=Data,Format=Xls,Org=Obs) / ident year gi vf vc Close Data * within Pregress(Effects=Indiv,Method=Fixed) gi # vf vc * fgls Pregress(Effects=Indiv,Method=Random) gi # Constant vf vc

2, second, in computing i by usual formula, third in transforming the original series with the Panel instruction under the form gireit = giit - igii., . . . and finally in applying OLS on transformed series.
From the point of view of the panel data model estimation process, RATS is flexible and can deal with large panel datasets thanks to its use of virtual memory. Nevertheless, the output regression is incomplete, only an F test for the presence of individual (or time) specific effects and a LR test for equal variance of perturbations across individuals (with the Pstats instruction) are reported. Neither Hausman test, nor other tests for the presence of individual (or time) specific effects (two sided and one sided LM, . . . ) are available without programming. Of course, it is always possible to program them with RATS in using its matrix language or with users's procedures (if any).
On the Estima Web site, there are several programs that can be used in estimating dynamic linear models on panel data by GMM. However some programming efforts are required as, for instance, in the design of the instruments matrix used by AB estimator (see ab rats.prg). Nevertheless, in simpler cases, for instance, estimating a dynamic panel data model by AH method is easy to realize. It is achieved by the code given in Table 26.11.
There are no specialized instructions for estimating non linear models on panel data (probit random effects model,. . . ). But, maximum likelihood estimation may be done using the Maximize instruction. For instance, with RATS, it is easy to estimate a random effects model by ML. One needs only to write the likelihood for an observation using individual means and deviation to individual means created
Table 26.10 RATS code #2 (grunfeld rats.prg)
Panel(Entry=1.0,Indiv=-1.0) gi / giw Panel(Entry=1.0,Indiv=-1.0) vf / vfw Panel(Entry=1.0,Indiv=-1.0) vc / vcw Linreg(Dfc=10) giw # vfw vcw

26 Software Review

919

Table 26.11 RATS code #3 (ab rats.prg)
Calendar(Panelobs=9) 1976 1 1 Allocate 140//1984:1 * reading the data file and variable creation not reproduced * computing variables in difference Diff n / dn ; Diff w / dw ; Diff k / dk ; Diff ys / dys * estimation by Anderson-Hsiao IV, cf. AB (1991), pp. 292 col e Instruments dn{2 3} dw{0 1} dk{0 1 2} dys{0 1 2} dumyear Linreg(Optimalweights,Instruments,Robusterrors,Lags=T-1) dn # dn{1 2} dw{0 1} dk{0 1 2} dys{0 1 2} dumyear

by the Panel instruction. Remember that RATS requires that you write the log likelihood for an individual i at time t, which is done by the Frml instructions reproduced in Table 26.12.
If your log likelihood involves computation of individual log likelihood by product of time period observations (for instance as in a random effects probit model), this will be more difficult to realize. Moreover, as it seems impossible to define first and second derivatives, it will be difficult to achieve convergence in some complicated cases.
More generally (cf. McCullough (1997) and Heywood and Rebelo (1996)), RATS works through an interactive mode as well as in batch mode and is available for many platforms (Windows, Macintosh, Unix, VMS. . . ). RATS has a simple but good interface (basically, a program window and an output/log window). It can read and write numerous file formats (ASCII, XLS, DBF12. . . ) and also has some databank management functions. Nevertheless, a merge instruction is not offered and the sort instruction (Order) admits only one sort key, which is not always adequate for some panel data management. Another deficiency is the lack of cross-tabulation instruction.
The Estima Web site contains a wide range of useful information, in particular for panel data, some RATS programs or procedures for (Pedroni) unit root and
Table 26.12 RATS code #4 (grunfeld rats.prg)
Nonlin b1 b2 b3 s2u s2b Compute b1=b2=b3=s2u=s2b=1 Frml resww = giw - b2*vfw - b3*vcw Frml resbb = gib -b1 - b2*vfb - b3*vcb Frml logl = -0.5*Log(2*%PI) -Log(s2u)*(tt-1)/(2*tt) $
-Log(s2u+tt*s2b)/(2*tt) -(1/(2*s2u))*(resww**2) $ -(1/(2*(s2u+tt*s2b)))*(resbb**2) Maximize(Iter=2000,Notrace,Print) logl /
12 This format is quite useful for file conversion because DBF file are not restricted on the number of records and of variables, contrarly to Excel which is limited to 65536 lines and 256 columns.

920

P. Blanchard

cointegration testing with panel data, estimation of spatial correlation consistent co-
variance matrix from panel data, estimation of Swamy's random coefficient model.
In addition, you can download some panel data example programs with their data
sets to reproduce the results presented, for instance, in Wooldridge (2002).
The documentation is good (many examples with data & programs). Neverthe-
less, in some cases, computational methods are not fully described, for instance: how does the Pstats instruction work? how are obtained the estimations of a u2, 2 and then i with the Pregress instruction? This may be not a problem due to the very effective working of the RATS discussion list.

26.2.4 SAS (v. 9.1)
SAS is probably the best known of all statistical software packages and is available virtually on all platforms except on Macintosh computers. Microcomputer and mainframe versions have exactly the same features with a homogeneous user's interface (mainly three windows: program, log and output ones). Working in interactive and batch modes, SAS covers a wide range of statistical and econometric methods available in several modules which can be rented separately (SAS/Base, SAS/Stat, SAS/ETS, SAS/Graph. . . ).
An SAS program consists usually in series of Data steps which create SAS tables on which we apply Procedure steps to carry out statistical, graphical and other types of analysis. SAS uses virtual memory and offers a powerful macro-language (SAS/Macro included in SAS/Base) and a good matrix programming language (SAS/IML) as a separate module. Even if some improvements had been reached with the last release, SAS is very often criticized for its outdated features which do not support many econometric estimation methods and econometric testing. This is particularly true for panel data estimation.
SAS provides very few specific procedures for static linear model estimation on panel data. The most useful one is Proc Tscsreg which is illustrated in Table 26.13 applied to Grunfeld's data.
Several points stand out in Proc Tscsreg. First, the panel dataset may be either balanced or unbalanced using the Id instruction for balanced (or unbalanced data or Cs= and Ts= options for balanced data). Second, two-way model estimations are possible in specifying Fixedtwo or Rantwo options and some variants are available (first-order autoregressive structure for the error term. . . ). Third, this procedure reports an Hausman test and an F test for no fixed effects. Finally, with a great number of individuals (> 5000 with the configuration used in this review), a virtual memory problem may appear when using the within estimator (probably due to the computation of fixed effects without using Frisch-Waugh method). In this case, it is better to use Proc Gml with the Absorb (using Frisch-Waugh method) instruction as shown in Table 26.13.
Nevertheless, FGLS estimation is always very slow with numerous individuals (with Proc Tscsreg) due to the preliminary estimation of the fixed effects

26 Software Review

921

Table 26.13 SAS code # 1 (grunfeld.sas) Libname in 'c:\epd\data\sas' ; Run ;
Proc Tscsreg Data = in.grunfeld ; Id ident year ; Model gi = vf vc / Fixone ; Title within ;
Run ;

Proc Tscsreg Data = in.grunfeld ; Id ident year ; Model gi = vf vc / Ranone ; TITLE fgls ; Run ;

Proc Glm Data = in.grunfeld ; Model gi = vf vc ; Absorb ident ; Title within with Proc Glm ; Run ;
Proc Mixed Data = in.grunfeld ; Class ident ; Model gi = vf vc / S ; Random ident / S ; Method = Ml ;
Title ML estimation of RE model ; Run ;

models used for computing 2. So, very often, users prefer first, to transform the variables in computing individual means, differences and quasi-differences to individual means and second, to estimate the transformed models by OLS obtaining between, within or FGLS estimates. The resulting program (not reported here because of space limitation) is longer but faster in executing on large panel datasets.
For maximum likelihood estimation of the random effects (linear) model, a simple solution (very time consuming with numerous individuals) may be implemented with Proc Mixed13 (see Table 26.13).
There is a new (experimental) procedure for estimating dynamic linear model on panel data, Proc Panel, but, to our knowledge, it does not correctly work for dynamic models estimated by GMM. Note however that Proc Panel allows to create new variables (lags for instance) in the body of the procedure. The Proc Model instruction which may estimate models by IV and GMM works only on time series or cross section data. It is easy to implement AH methods, but Proc Model cannot reproduce standard errors robust to general cross-section and time series heteroscedasticity as in Arellano and Bond (1991). This may be done with a SAS/IML program, so more complex programming is required (this is done in ab.sas).
The situation is less controversial for non linear model estimation on panel data. Even if many procedures which implement Logit/Probit estimation are mainly designed for cross-section data, others work with panel data.14 SAS may estimate fixed effects logit models with conditional maximum likelihood, generalized estimating equations (GEE with Proc Genmod), random effects logit and probit models on
13 Note that SAS offers an other procedure for ML estimation, Proc Nlp which is part of SAS/OR module. Proc Mixed must be prefered to Proc Nlin which can estimate some models by ML (with the model.like = logl ; and loss = -logl ; instructions). See grunfeld.sas for an illustration. 14 See Allison (1999) for a detailed presentation.

922

P. Blanchard

panel data. As a SAS program spends a great part of its time reading and writ-

ing data on a hard disk, time execution for some non linear models may become

extremely long, and sometimes, problems like insufficient memory occur. The in-

structions reported in Table 26.14 illustrate some SAS capabilities in discrete choice

model estimation on KW data (variables creation not reported).

Another difficulty can be relevant with SAS. Indeed, there are many procedures

to estimate discrete choice models (Proc Logistic,15 Proc Mixed, Proc

Catmod, Proc Nlmixed, Proc Glm, Proc Probit. . . )

each

with many different options. Therefore, it may be difficult to determine which one

corresponds more to the purpose. Sometimes, the same results may be achieved

by several procedures. Two procedures were recently introduced: Proc Mdc to

estimate mixed logit models and Proc Qlim for (mainly) multinomial logit and

tobit models, but only on cross-section data.

A crucial point must be here taken into consideration. For some procedures

(Proc Model, Proc Nlmixed, Proc Nlin. . . ), it is possible to insert

programming statements (Do-loop, If expressions, arrays manipulation, macro-

instructions. . . ), so that, the user may adapt, for example, the computation of his

log likelihood to a special problem. Yet, this way of programming may be difficult.

Table 26.14 SAS code # 3 (discrete choice.sas)
/* conditional fixed effects logit model */ Proc Logistic Data = temp ; Model choice = educ exper expersq ;
Strata ident ; Run ;
/* random effects logit model */ Proc Nlmixed data = in.kw ;
Parms b0=1 b1=0 b2=0 b3=0 b4=0 s2u=1 ; pi = Constant('PI'); eta = b0 + b1*educ + b2*exper + b3*expersq +b4*black+ u ; expeta = exp(-eta) ; p = 1/(1+expeta) ; Model choice ~Binary(p) ; Random u ~Normal(0,s2u) Subject =
ident ; Estimate "rho" s2u/(s2u+(pi**2)/3) ; Run ;
/* random effects probit model */ Proc Nlmixed data = in.kw ;
Parms b0=1 b1=0 b2=0 b3=0 b4=0 s2u=1 ; eta = b0 + b1*educ + b2*exper + b3*expersq + b4*black + u ; IF (choice = 1) Then p = Probnorm(eta) ; Else p = 1 Probnorm(eta) ; ll = log(p) ; Model status ~General(ll) ; Random u ~Normal(0,s2u) Subject = ident ; Estimate "rho" s2u/(s2u+1) ; Run ;

15 Proc Logistic models Prob(y=0). Using the descending option allows to model Prob(y=1) instead.

26 Software Review

923

However, it is hard to believe today that the scope of panel instructions would improve in the future. Indeed, the efforts aimed at developing SAS are massively concentrated upon the modernization of other parts of the software (web integration, data mining, data warehousing. . . ). Here again, because of the vast community of SAS users, it is often easy to find SAS programs (with macro instructions and/or IML) for panel data estimation. Nevertheless, in the future, it is possible that the new version of IML, called SAS/IML Workshop (yet available under a test version) changes considerably this situation. Indeed, in an IML procedure, DATA and PROC steps cannot be integrated16 into the IML program. IML Workshop enables you to utilize DATA and PROC steps (and external C/Fortran/Java functions) as subroutines to an IML program, which may simplify greatly some programming task.
Moreover, the great power of its data management instructions (ideally designed for panel data) justifies the use of SAS by itself. With SAS, the most difficult operations on panel data sets like merging, matching. . . become virtually instantaneous with very little programming. In addition the ODS (Output Delivery Service) permits to save all or parts of the results in files with HTML, RTF (Word), PostScript or PDF formats. These reasons explain why SAS is frequently used over the world, despite its high costs and its basic limitations.
The documentation is huge (for instance, only for SAS/STAT module, 3 volumes, near 4000 pages!) with a terminology and examples coming very often from biostatistics, agronomy, psychology, sociology. . . Nevertheless, the Web site proposes interesting technical supports with on line documentations, many data files and SAS example's programs. The SAS Users Group International (SUGI) and the SAS discussion list are exceptionnally active and useful.

26.2.5 Stata (v. 9)
Even if Stata is mainly oriented towards econometrics, social science and biostatistics on longitudinal data, version 917 has developed significantly on different topics, particularly for panel data estimation. From a general view point, Stata presents three main characteristics:
· First, it is available in many platforms (Windows, MacOS, UNIX. . . ) and in three different versions: Small Stata managing a limited number of observations and variables, Intercooled Stata which is less limited because of the use of virtual memory and Stata/SE adequate for analyzing larger datasets. The Intercooled version that we use in this review has only a maximum matrix size of (800×800) and the number of variables cannot go beyond 2047 in a dataset.
· Second, its interface is simple but perfectible (the results window could be ameliorated); Stata works through a command mode (in the same way in different platforms) and a Graphical User Interface. When working in a command mode,
16 Exactly as Data and Proc steps cannot interchange informations with an other Data or Proc step without the use of a table (or a SAS macro variable). 17 Stata 10 is now available.

924

P. Blanchard

note than Stata's commands are typed in lowercase letters (cf. below for some exceptions); the same apply for variable names but not for file name. This is not really a problem because Stata has an excellent and really unified syntax which can be learned very quickly (except, may be, when using the macro language). In many cases, it is better to include all commands in a (ASCII) do-file; it's the solution we advocate in this review. · Third, Stata is a modular software, i.e. many commands are in fact contained in (ASCII) ado-files (automatic do-files), written with a powerful (but sometimes unusual) programming language which makes an extensive use of macros. This is the case for many new panel data commands. The main advantage is that many users' specific procedures are available from the Web site. Moreover, a (rather simple, only one window is available) internal editor for ado and do files is provided.
From a panel data estimation's method viewpoint, Stata is very interesting especially because it comes with useful functions. First, linear panel data estimation methods are numerous and easy to implement. The code (a do file) reproduced in Table 26.15 shows how to estimate the Grunfeld model by OLS, within, FGLS methods.
We note that panel data specific instructions begin with an xt word. Moreover, in this example, the panel structure is not declared when reading the datafile but it is realized thanks to the use of i(ident) option in the xt instruction, which announce either a balanced or unbalanced panel dataset. For Stata, a panel dataset is always organized by individual (but it can be reshaped with a reshape instruction or with xtdata instruction). Other options (available for most commands) are useful:
· if condition for applying a command on a subsample defined by a condition (e.g. if age < 60),
· in range for applying a command on a specific list of observations (e.g. in 100/200),
· weight varpond for weighting observations according to values taken by varpond variable.
· There is also a special syntax by varlist: command for applying a command depending on the value taken by one or several variables (e.g. by ident: xtsummarize). This is very useful for panel data.

Table 26.15 Stata code # 1 (grunfeld.do)
use c:\epd\data\stata\grunfeld.dta regress gi vf vc xtreg gi vf vc, fe i(ident) xtreg gi vf vc, be i(ident) xtreg gi vf vc, re theta i(ident) xthaus xttest0

// reading the dataset // ols // within // between // fgls // hausman test // BP LM test

26 Software Review

925

There are several useful instructions which describe (xtdes), summarize (xtsum), tabulate (xttab), graph (xtdata, xtline) variables in a panel data set. Stata allow several estimation methods for static or dynamic panel data models (IV, GMM, 2SLS. . . ). This can be illustrated through the estimation of a dynamic panel data model's on AB data (cf. Table 26.16; reading data file and creation of some variables are not reported).
Several topical points can be raised here. First, the symbol /// is the continuing line operator. Second, applying AB GMM estimator with xtabond instruction (or AH estimator with xtivreg or ivreg2 instructions) is very simple. Moreover, system estimators are available with xtabond2 and xtdpdsys instructions. Third, note that in the context of a dynamic estimation, it is necessary to define explicitly the panel structure by tsset instruction (another variant implies to use iis ident and tis year instructions). Creating variables in first differences may be done by, for example, generate dk = d.k and we can use in a similar way f (forward), s (seasonal) and d (difference). We must also underline the fact that we define lagged variables by, e.g., l(0/2).(dk dys) which creates lags 0,1 and 2 of the two variables These operators may also be combined. For example, l2d.(k ys) gives the same results as l(0/2).(dk dys). With many instruments, you will probably need to adjust the matsize parameter. For complicated problemss, Stata offers a new and quite powerful matrix programming language (Mata).
For non linear estimation on panel data, Stata enables to proceed in different ways. First, when maximum likelihood is required, it is possible for users to write their own likelihood function and then to call Stata for maximizing it. One of the most interesting features of this ML module is its ability to maximize the log likelihood summing up the individuals components obtained by time aggregation (for instance, as in a random effects probit). This works on a balanced or an unbalanced panel. Moreover, it is possible, but not necessary, to write the first and second derivatives. But, as the Stata language is somewhat specific (intensive use of macro

Table 26.16 Stata code # 2 (ab.do)

// reading data file and new variables creation not reported

set matsize 200

// used by xtabonb

tsset ident year // panel data structure necessary for xtabond

// AH-d AB, table 5-e pp. 292

xtivreg n l2.n l(0/1).w l(0/2).(k ys) year3-year9 ///

(l.n = l3.n), fd

// AH-l with ivreg2 (gmm) : table 5-f pp. 292

ivreg2 d.n l2d.n l(0/1)d.w l(0/2)d.(k ys) year3-year9 ///

(l1d.n = l3.n), cluster(ident) gmm

// one-step results (not robust)

xtabond n l(0/1).w l(0/2).(k ys) year4-year9, lags(2)

// one-step results + robust : table 4-a1 pp. 290

xtabond n l(0/1).w l(0/2).(k ys) year4-year9, lags(2) robust

// two-step results : table a2 pp. 292

xtabond n l(0/1).w l(0/2).(k ys) year4-year9, lags(2) twostep

926

P. Blanchard

Table 26.17 Stata code # 3 (grunfeld ml.do)
clear program drop all program define mlere
version 8.0 args todo b lnf tempvar theta1 z T S z2 Sz 2 a tempname s u s e mleval `theta1' = `b', eq(1) mleval `s u' = `b', eq(2) scalar mleval `s e' = `b', eq(3) scalar quietly {
gen double `z' = $ML y1 - `theta1' by i: gen `T' = cond( n== N, N,.) by i: gen double `S z2' = cond( n== N,sum(`z'^2),.) by i: gen double `Sz 2' = cond( n== N,sum(`z')^2,.) gen double `a' = `s u'^2 / (`T'*`s u'^2 + `s e'^2) mlsum `lnf' = -.5*`T'*ln(2* pi*`s e'^2) ///
-.5*ln(`T'*`s u'^2/`s e'^2 +1) /// -.5*( (`S z2'-`a'*`Sz 2')/`s e'^2 ) /// if `T'~= . } end use c:\epd\data\stata\grunfeld.dta sort ident year ml model d0 mlere (eq1: gi=vf vc) (s u:) (s e:), /// max init(1 1 1 1 1,copy) ml display

variables), this may be difficult to do as shown in Table 26.17 (the code is adapted from Gould, Pitblado and Sribney, 2003).
Second, as in the following example (Random Effects probit model), Stata provides many modules (Ado-files) doing automatically ML estimation on some non linear models on panel data: stochastic frontier models, fixed-effects & random effects logit models, random-effects probit models (with a powerful quadcheck instruction which checks the sensibility of estimation results to the selected number of quadrature points), random-effects tobit models. . . We give now (see Table 26.18) an illustration of using such estimation instructions on KW data.
Other general Stata's features should be taken into consideration (for a more complete review, see Kolenikov (2001), Barrachina and Llopis (2002)). Stata may read and write various file formats: Stata, ASCII (very easily), and dBase, MS Access, Excel files. . . with the ODBC instruction. The graphics module is very good. Stata offers also several instructions (append, merge, joinby. . . ) to merge a dataset with another one. Many instructions (recode, autocode, tabulate. . . ) permit to create dummy variables very easily. The documentation is excellent (13 volumes, may be the best one of all general purposes packages documentation reviewed in this chapter). There is a special volume which is dedicated to cross-

26 Software Review

927

Table 26.18 Stata code # 4 (discrete choice.do)

clear

set memory 10m

version 8

use c:\epd\data\stata\kw.dta

// reading the dataset

// creating the endogeneous var. (0/1)

generate status = (choice==1)*1 + (choice==2)*2 + (choice>=3)*3 generate choice1 = (status==1)*0 + (status==2)*0 + (status==3)*1 // conditional fixed-effects logistic regression

clogit choice1 educ exper expersq black, group(ident)

// random effects logit model

xtlogit choice1 educ exper expersq black, i(ident) re

// random effects probit model

xtprobit choice1 educ exper expersq black, i(ident) re

sectional time series models estimation (Stata, 2003), containing very useful technical appendices. The Stata Web site is equally very convincing: it is possible to call directly a dataset by an http instruction in a Stata program and the user may find several ado files which can be downloaded, installed and used with Stata. Automatic updating is also available.

26.2.6 TSP (v. 5)
This latest version of TSP (Time Series Processor) has several new features which introduce substantial improvements compared to the later version (4.2) reviewed in the second edition of this volume, some of them being yet available with the version 4.5.
First, two new interfaces are today available: (a) "TSP through the Looking Glass" which is a two windows interface (a program window and an output/log window), (b) "TSP through GiveWin" (GiveWin is an interactive menu-driven graphicsoriented program in which TSP, as some other software Ox, PC-Give. . . , may be integrated) but with a moderate extra-cost. In this review, we use "TSP through the Looking Glass".
Second, many limitations of the previous versions have been removed: two dimensional matrices are now available, the graphic module has been enhanced, (with GiveWin, it becomes really nice; note too that special graphics for panel data are available in TSP), and the size of the dataset may be extremely large. Nevertheless, the programming language has some limitations (e.g. string management features).
From the point of view of panel data management, a major change is that the Freq instruction can manage panel data structure (balanced or unbalanced) and allow to handle missing values, leads and lags. Suppose you have a balanced panel dataset (Grunfeld's data with 10 individuals observed 20 years, 1935­1954), you may write, at the beginning of your program, Freq(Panel,T=20).

928
Table 26.19 TSP code #1 (grunfeld.tsp)
Freq(Panel,Id=ident) ; ? reading an xls file Read(File='c:\epd\data\excel\grunfeld.xls') ; Freq(Panel,Id=ident) A; ? A stands for annual data ? Panel data estimation (ols, between, within, fgls) Panel(Robust,Hcomega=Block) gi c vf vc ;

P. Blanchard

For unbalanced data (for instance, AB data, 140 individuals, 1976­1984), it is necessary to have in the data file one series having a numerical identifier for each individual (for instance, ident = 1, 2, 3, . . . , 140). Then, you can define Freq(Panel,id=ident) A;.
Linear panel data estimation is mainly done with the usual Panel instruction, which is illustrated on the following example using Grunfeld's data. Note that the syntax of Table 26.19 is clear and concise.
The output is very detailed; an F test of fixed effects and an Hausman test are automatically reported.
Several recent improvements are of major interest for panel data models. New estimation commands allow to estimate (1) RE (individual and/or time) model by ML, (2) RE or FE model with a AR1 error term by ML, (3) RE or FE probit model and (4) 2SLS and GMM with FE. When estimation methods and tests on panel data models (linear and nonlinear) are not available with pre-programmed instructions; fortunately, on two TSP companion Web sites, it is possible to find dozens of programs mainly for IV, GMM. . . panel data estimation. Of course, users have to write programs based on a good matrix language and with a powerful maximum likelihood instruction (look at the Differ instruction, for instance). We are going to illustrate these points with the same two examples used previously: a dynamic linear model by AH estimator and a random effects model estimated by ML.
For dynamic linear model on panel data, applying the AH estimator, is easy (in Table 26.20, we do not report code for reading the data file).
Table 26.20 TSP code #2 (ab.tsp)
? creating new variables dn = n - n(-1) ;dw = w - w(-1) ; ? gener is optional dk = k - k(-1) ;dys = ys - ys(-1) ; Dummy year ; ? creates year dummies year1...year9 ? Panel data estimation : AHd, cf. A&B pp. 292 List lvar dn(-3) dn(-2) dw dw(-1) dk dk(-1) dk(-2)
dys dys(-1) dys(-2) year5-year9 ; Select .Not.Miss(dn(-3)) ; ? AHd robust 2sls(Robust,Hcomega=Block,Inst=lvar) dn c dn(-1) dn(-2)
dw dw(-1) dk dk(-1) dk(-2) dys dys(-1) dys(-2) year6-year9 ;

26 Software Review

929

Table 26.21 TSP code #3 (discrete choice.tsp)
Title 'Random effects probit' ; Probit(Rei) choice C educ exper expersq black ; Title 'Fixed effects logit' ; Probit(Fei) choice C educ exper expersq black ;

Note the use of the Dummy instruction which creates very easily time dummies, denoted year1...year9. With the last release, when using the Robust and Hcomega=Block TSP's options, the SEs are robust to general cross-section heteroscedasticity and time series autocorrelation. Nevertheless, for GMM estimation (AB estimator), there is not a Hcomega option, so so we cannot reproduce exactly AB results. On TSP companion Web site, a sample program is available to do this task, but the solution is too complicated (in creating the instruments matrix and computing the var-cov matrix) to be reproduced here.
TSP offers also some instructions for estimating logit/probit models on panel data. For instance, the example of Table 26.21 shows how to estimate random effects probit and fixed effects probit models on KW data.
Note that TSP uses analytical first and second derivatives in these maximum likelihood estimation, which implies that this code is executed very quickly. For other maximum likelihood estimations, there are two ways to specify the log likelihood. First, we can use the Frml and Eqsub instructions. The major advantage of this method is that TSP can compute first and second analytical derivatives (see above). This method contributes largely to the speed and the possibility to achieve convergence. When it is difficult to write the log likelihood with one or more Frml instructions (e.g. with panel data, it is necessary to sum up the log likelihood on time periods), the user may write it with the Proc method. If this case, derivatives are evaluated numerically, sometimes not very adequately, and are very time consuming. We illustrate here only the first possibility on the estimation of a linear random effects models on Grunfeld's data (see Table 26.22, some parts of the program are not reported).
Table 26.22 TSP code #4 (grunfeld.tsp)
? log likelihood as in Nerlove (2002) Param beta0,1 beta1,1 beta2,1 sigma2,1 rho,0.5 ; Frml rei logl = - Log(sigma2)/2 - (0.5/(sigma2))*((e)^2)
- Log(ksi)/(2*t) - Log(eta)*(t-1)/(2*t) ; Frml ksi 1 + rho*t - rho ; Frml eta 1 - rho ; Frml e gistar - beta1*vfstar - beta2*vcstar- beta0/sqrt(ksi) ; Esqub rei e ksi eta ; Title 'ML Random individual effects' ; Ml(Maxit=100,Hiter=N,Hcov=N) rei ;

930

P. Blanchard

More generally, TSP can run on various type of hardware (Windows PC, Apple Macintosh, Unix workstations and some mainframes). The Sort instruction admits only one key, and merging panel datasets does not seem not easy. Lastly, crosstabulation instructions are not available. It can read and write numerous file formats (ASCII, XLS, DBF, Stata. . . ) and also provide some databank management capabilities. For more details, one can refer to Silk (1997), Lubrano (1998), Merino (1998).
The documentation is good, but contains too few examples on panel data, which is not really a problem due to the great amount of explanations available on the Web site. More information may be obtained by contacting the efficient technical support.

26.3 High-Level Matrix Programming Languages
Econometricians use more and more matrix programming language in at least two situations:
1. They need to apply new econometric techniques, not yet available in general econometric packages, and difficult to implement with them,
2. The econometric method is too time-intensive and so, requires a very efficient programming language (Monte-Carlo simulation, estimation by simulation. . . ) without requiring knowledge of a low level programming language18 (e.g. C or C++).
Matrix programming languages are very often an efficient solution to these difficulties, easier than using low level programming languages. Two packages seem to dominate, at least when panel data estimation is concerned: GAUSS and Ox.

26.3.1 GAUSS (v. 5)
GAUSS19 is an econometrics and statistics oriented programming language designed to perform statistical and mathematical (specially matrix) calculus. GAUSS is available in two versions: GAUSS for UNIX workstations and GAUSS for Windows. It consists mainly of three components: (1) The GAUSS programming language, (2) a library of application modules (Time Series, Maxlik, GAUSSplot. . . ) and (3) some add-on programs (GaussX. . . ), these last two components available with an extra cost. Due to its speed, to its intuitive and powerful syntax, and above
18 See Cribari-Neto (1999) and Eddelbu¨ttel (1996) for the interest to use low level programming languages for econometric purposes. 19 Version 8 is available and offers new or updated libraries (Algorithmic Derivatives, Discrete Choice, Symbolic Tools, GAUSSplot. . . ). There is also a new feature, directly related to panel data models, which allows to use 3 (or more) dimensional arrays. This may be quite useful to manage balanced panel data in arrays with individual, time and regressor dimensions.

26 Software Review

931

all, to the free availability of numerous user's programs (DPD, Expend. . . ), GAUSS is probably the most commonly used of the high-level matrix programming languages, particularly in panel data econometrics (cf. Cribari-Neto (1997), Heywood and Rebelo (1996) and Vinod (2000) for a more general presentation).
For panel data estimation, various strategies may be followed. For static linear models on panel data, we can first use the TSCS command (included in the Time Series application module - with an extra cost) which may estimate a panel data model by OLS, within and FGLS estimators, providing also an F test of no individual fixed effects and a Hausman test (fixed effects model vs random effects model). In Table 26.23, there is an example of TSCS's use on Grunfeld's data. TSCS it not very sophisticated but is fast and can manage a panel data set (balanced or unbalanced) of whatever size thanks to its working by block of individuals. As TSCS command is written in GAUSS (put in a procedure), then users can ameliorate it by adding its own code for doing new tests, etc. Another way to estimate panel data models consists in writing on your own a GAUSS program. In this case, the program would be quite different depending on the fact than your panel data set is balanced or not and be of a moderate size or not. With a small balanced panel dataset (for instance the Grunfeld's data), programming OLS, between, within and FGLS is easy, as shown in Table 26.24, where we reproduce a simplified but working example (we limit us to within estimation). This brief example illustrates GAUSS main characteristics: GAUSS works mainly on matrices and vectors (xdat, x and y). Once, the matrices defined (by extracting columns of xdat), computation of an estimator, e.g. OLS, is made by bols = Invpd(x'x)*x'y;. For within estimation, it is a little bit more complicated, we must first create yit - yi. and xit - xi.. To achieve this result, we must reconfigurate the y matrix (and also the x matrix) by ynt = Reshape(y,nn,tt) ; (ynt has now nn lines and tt columns). Then, individual means are created by yb = Meanc(ynt') ; (note the need for transposing because Meanc computes means by columns) and finally yw = Vecr(ynt-yb) ; computes deviation to individual means to stack in an (nn,1) vector. Lastly, within estimation is done by applying OLS on transformed data by bwith = Invpd(xw'xw)*xw'yw ;. This may be completed by computing residuals, tests. . . In fact, the most tedious part is often to obtain a correct presentation of the output (the Print and Printfm instructions

Table 26.23 Gauss code #1 (grunfeld tscs.prg)
Cls ; New ; Closeall ; Library Tscs ; #Include tscs.ext ; Tscsset ; lhs = { gi } ; exog = { vf, vc } ; grp = { ident } ; tsmeth = 1 ; filename = "c:\\epd\\data\\gauss\\grunfeld"; {bwtih,vbwith,mdv,bec,vbec,mec} = Tscs(filename,lhs,exog,grp) ;

932

P. Blanchard

Table 26.24 Gauss code #2 (grunfeld gauss.prg)

/* initializations */
Cls ; New ; nn = 10 ; tt = 20 ; nt = nn*tt ; file = "c:\\epd\\data\\gauss\\grunfeld" ;

/* reading the Gauss data file */ Open f1 = ^file For Read ; vnoms = Getname(file) ;

xdat = Readr(f1,nt) ; f1 = Close(f1) ;

/* xdat is a matrix (200,# of variables in the file)*/ /* defining y and x matrices from xdat */ Let pexo = vf vc ; Let pendo = gi ;

lvexo = Indcv(pexo,vnoms) ; lvendo = Indcv(pendo,vnoms) ;

x = Submat(xdat,0,lvexo)~Ones(nt,1) ; y = Submat(xdat,0,lvendo) ;

/* Within transformation */ ynt = Reshape(y,nn,tt) ; w = Vecr(ynt-Meanc(ynt')) ;

x1nt = Reshape(x[.,1],nn,tt) ; x2nt = Reshape(x[.,2],nn,tt) ;

xw = Vecr(x1nt-Meanc(x1nt'))~Vecr(x2nt-Meanc(x2nt')) ;

/* within */ bwith = Invpd(xw'xw)*xw'yw ; ? bwith ; end ;

/* to be completed */

are complex and not very powerful; this is also true for printing instructions provided by Ox and SAS/IML).
With a large unbalanced panel dataset, the program must be adapted in the following way. For memory space limitations, computations must be done individual by individual. So, it is useful to read first an auxiliary file (say auxti) giving for each individual (represented by an identification variable, say ident), the number of times this individual is observed, say, a variable named tii = 3, 5, 8, 3. . . for instance. Second, we read the data file20 (called mydata) and do the computations individual by individual (or with more programming by block of individuals). The following code illustrates this idea (note than DPD-GAUSS, but not TSCS, use this principle). We limit ourselves to within estimation due to space limitation. This code is reproduced in Table 26.25.
A reshape operation is then not necessary, because we work individual by individual. The program will be slower due to this working (but it may be speeded up if working by block of individuals which implies a complication in programming), and because probably several do loops on the data file will be necessary, one for computing OLS, between and within estimation and at least another one for FGLS estimation, tests, etc.
For dynamic linear model estimation on panel data, fortunately things are simpler because of the DPD-GAUSS program (cf. Arellano and Bond, 1998) which computes estimates for dynamic models from balanced or unbalanced panel data. This program provides several estimators (OLS, within, GMM, instrumental variables,
20 It is also possible to read all the variables into memory (GAUSS put them in a workspace) and then to do the computations by individual. This will speed up the execution and, at the same time, will save memory.

26 Software Review

933

Table 26.25 Gauss code #3 (simple by indiv.prg)

New ;

/* reading the auxiliary datafile */ file = "c:\\epd\\data\\gauss\\auxti" ;

Open f1 = ^file For Read ;

nind = Rowsf(f1) ; z = Readr(f1,nind) ;

f1 = Close(f1) ; tii = z[.,2] ;

/* defining variable names */ file = "c:\\epd\\data\\gauss\\mydata" ;

vnoms = Getname(file) ;

Let pexo = x1 x2 x3 x4 x5 x6 x7 x8 x9 ; Let pendo = y ;

lvexo = Indcv(pexo,vnoms) ; lvendo = Indcv(pendo,vnoms) ;

/* read and compute by individual */ Open f2 = ^file for Read ;

i = 1 ; xwtxw = 0 ; xwtyw = 0 ;

Do While i <= nind ;

data = Readr(f2,tii[i]) ;

y = Submat(data,0,lvendo) ; x = Submat(data,0,lvexo) ;

/* between and within transformation */ xm = Meanc(x) ; ym = Meanc(y) ; xw = x - xm' ; yw = y - ym' ;

xwtxw = xwtxw + xw'xw ; xwtyw = xwtyw + xw'yw ;

i=i+1; Endo ;

/* next individual */

f2 = Close(f2) ;

bw = Invpd(xwtxw)*xwtyw ; ? bw ; End ;

system estimators. . . ) and robust test statistics (Sargan test, tests for serial correlation. . . ). Reading the data by block, DPD is not limited by a maximum number of observations. The instrumental variables matrix can contain several hundred of columns, the main limitation being its invertibility. In order to work with the DPD program, the user has to supply two GAUSS data file: one containing the NT observations of the k variables (sorted by individual and consecutive), and the second one (an auxiliary file) indicating the structure of the main data set. For the AB data,21 this auxiliary file looks as in Table 26.26.

Table 26.26 Contents of the auxiliary file (abaux.dat)

nbyear 7 8 9

count 103 23 14

21 These files are provided with DPD-GAUSS. We only change their names.

934

P. Blanchard

These two files22 (named ab.dat and abaux.dat) permit to DPD to read quickly the panel but require that the data file must be sorted by nbyear and by individual. Note that the main dataset should contain a time stratification variable and, optionally, an individual stratification variable. The next step is to modify, with the GAUSS editor, an (ASCII) file: DPD.RUN (you can change its name). This file contains GAUSS instructions which define the name and path of main and auxiliary datasets, the name of time, individual stratification, independent and dependent variables and the definition of instruments. . . This program calls for two related programs (located in the Gauss directory): DPD.FNS containing some functions for data transformations and DPD.PRG, the main program file. DPD.RUN may look as in Table 26.27 (instructions used only for printing, unmodified lines and some comments are not reproduced).
This code allows to reproduce AB results (see Arellano and Bond (1991), Table 4-a1, a2). One of the crucial points is the choice of the instrumental variables and the DPD function named GMM() which returns the optimal instrument matrix for the GMM estimators. This matrix may be combined with other matrices using the vertical concatenation operator (~). DPD-GAUSS allows also to use system GMM estimators combining moment conditions for equations in first differences with moment conditions for equations in levels (with the functions Lev1(), Diflev(). . . Another major interest of DPD-GAUSS is that the authors provide the source code, so you can adapt it to your own problem if required. If you want to estimate your model by OLS or within, you need only to adapt the

Table 26.27 Gauss code #5 (ab gauss.prg)

bat=1;

/* mode batch */

imod=1;

/* model in diff. */

icon = 1 ; irob = 1 ; /* constant and robust estimation

choices */

open f1="c:\\epd\\data\\gauss\\xdata"; /* main data */

open f2="c:\\epd\\data\\gauss\\auxdata";

/* auxiliary data */

yearcol=2;

/* Data column for year */

year1=1976;

/* First year of data */

nyears=9;

/* Number of years in data set */

lag=2;

/* Longest lag to be constructed due to nt-2*/ /* 2 first obs. by indiv. skipped */

data=ln(data);

/* all variables in log */

y=dif(3,0);

/* endogeneous var. */

/* exogeneous var: note the use of the diff function */

x=dif(3,1)~dif(3,2)~dif(4,0)~dif(4,1)~dif(5,0)~dif(5,1)

~dif(5,2)~dif(6,0)~dif(6,1)~dif(6,2);

/* instruments definition */

z=gmm(3,2,99)~x[.,3:10]; /* note the use of gmm function */

/* more lines follow */

22 Recall than with GAUSS, a dataset is made, in general, of two files: one with the extension .dat containing the data, the second one with the extension .dht, giving the columns' names. So, in our example, four files are used: ab.dat, ab.dht, abaux.dat and abaux.dht.

26 Software Review

935

choice of the instruments matrix in writing z=Ols ; (or z=x ;) for OLS and z = Wgroups ; for within estimator. To be used correctly DPD requires a bit of knowledge of GAUSS and a little idea of DPD working (individual by individual, all the variables are in the matrix called here data). A careful reading of the user's guide for DPD (Arellano and Bond, 1998) is absolutely necessary.
For non linear or maximum likelihood estimation, three application modules are very useful: constrained optimization (CO), maximum likelihood (Maxlik) and constrained maximum likelihood (CML) modules. We are going to illustrate the use of Maxlik on a simple example,23 estimation of an error components model by ML on Grunfeld's data (data reading and variables creation not reproduced), see Table 26.28.
It is clear according to this example that GAUSS programming supposes a good knowledge of basic programming concept (procedures, local and global variables. . . ).
The GAUSS documentation is good, but contains too few examples. The Aptech web site is mainly limited to commercial information. Fortunately, there are numerous users programs which can be easily founded and used, e.g. for panel data estimation:

Table 26.28 Gauss code #6 (grunfeld ml gauss.prg)
New ; Closeall ; Library Maxlik ; #Include Maxlik.ext;
/* ML estimation of error components model */ xb = xb.*.Ones(tt,1) ; yb = yb.*.Ones(tt,1) ; maxl = yb~Ones(nt,1)~xb~yw~xw ; Maxset ; Maxclr ; x0 = 1~1~1~1~0.5 ; /* initialization values */ {bml,logl,g,h,retc} = Maxlik(maxl,0,&logl,x0) ;
Proc logl(b,z) ; Local llog , ksi , eta , part3 ; ksi = 1 + b[5]*tt - b[5] ; eta = 1 - b[5] ; part3 = ( z[.,5]/Sqrt(eta) + z[.,1]/Sqrt(ksi) )
- b[2]*( z[.,3]/Sqrt(ksi) + z[.,6]/Sqrt(eta) ) - b[3]*( z[.,4]/Sqrt(ksi) + z[.,7]/Sqrt(eta) ) b[1]/Sqrt(ksi) ; llog = -nt*Ln(2*PI)/2 -nt*Ln(b[4])/2 -nn*Ln(ksi)/2 (nt-nn)*Ln(eta)/2 -(1/(2*b[4]))*(part3'*part3) ; Retp(llog) ; Endp ; End ;

23 A more complex one (random effect probit model estimation) is given in reprobit gauss.prg).

936

P. Blanchard

· C. Kao and I. Choi provide free programs for unit root and cointegration tests for panel data,
· Many GAUSS programs on linear and non linear (binary, censored, count, sample-selection. . . ) models are provided by Lee (2002),
· Train (2003) provides free code for mixed logit estimation for panel data, · Windmeijer (2000) offers a GAUSS program for non-linear GMM estimation
of exponential models with endogenous regressors for cross section and panel (dynamic) count data models (see also Romeu, 2004).

26.3.2 Ox (v. 3.4)
Ox is a true object matrix programming language available for many platforms (Windows, DOS, Unix, Linux,. . . ).
All Ox versions are free for academic use, except the Windows version. The free versions are named OxConsole (for DOS/UNIX) as they are called by a command line in a console windows. An editor is not provided with this basic version which cannot moreover visualize graphs (but it can save them in a Postscript file).
The Windows version (called Ox Professional24) may use two types of interface: first, GiveWin, and, secondly, OxEdit. OxEdit (provided with a purchase of Ox) is a text editor developed, as for Ox, by J. Doorniks. In this review, we use Ox Professional. (v. 3.3) for Windows with OxEdit. OxEdit has two interesting features: first, when OxEdit edits an Ox source code, it uses colors25 to distinguish between instruction keywords, comments, numbers, syntax errors. . . ; second, it may be used as a front-end to Ox (but also to C, C++, TEX, LATEX. . . ). Nevertheless, an advantage of GiveWin over OxEdit is that it allows users to modify a graph created by Ox by adding text, labels,. . . Moreover, GiveWin enables to manage data (editing, variable creation. . . ) more easily.
The Ox language has several specific features:
· It has broadly a similar syntax to C, C++ and Java (e.g. Kerninghan and Ritchie, 1998, Stroustrup, 1992). All indexing of matrices and vectors start at 0 and not at 1. The main difference from C, C++ is that a matrix is a standard type in Ox. So, even if all variables must be declared before use, their type is defined only implicitly. Therefore, a variable may start as an integer and then be redefined as a matrix.
· As GAUSS, it has an extended graphical, mathematical and statistical functions library and similar matrix operator (concatenation, inversion. . . ). Ox also allows

24 Ox 4.1 is now available. Note that we do not review another well-known econometric software, PcGive for two reasons: (1) space limitations and, (2) PcGive is mainly written in Ox language and shares several features with Ox. Nevertheless, it uses pull-down menus and is a simple and efficient alternative to Ox. 25 This is also true for GiveWin and for the SAS Enhanced Editor.

26 Software Review

937

for vectorized code, more efficient for programming and execution, and very often, Ox and GAUSS syntaxes are similar. Nevertheless, in some cases (Doloops. . . ), Ox syntax may be unusual for non C, C++ users. · Nevertheless, Ox syntax is case sensitive and many instructions must be written in lowercase, some exceptions arise when using Ox classes (see below). Variable names cannot exceed 60 characters (the first one being a letter). · Ox can read various datafile formats: PcGive/GiveWin, XLS, GAUSS, Stata, ASCII. . .
Some of these aspects may be illustrated through a short example (closed to the GAUSS26 one, cf. pp. xxix), given in Table 26.29.
In fact, the main difference between GAUSS and Ox is the concept of Class. Ox provides pre-programmed classes,27 such as the Database class (used to store data with database functionalities). One of the major interest of the class concept is that it avoids using global variables. It is probably with this concept of classes (an optionally feature) that a non C,C++ user could encounter trouble. Using classes is very useful to develop professional applications. Moreover, if creating Ox class may be difficult,28 using it is not, as we can see in Table 26.30 which illustrates some uses of the Database class (used by many other classes).

Table 26.29 Ox code #1 (grunfeld1.ox)

#include <oxstd.h>

main()

{

decl data, x, y, nt, nn, tt, ynt, yb, yw,

x1nt, xb1, x1w, x2nt, xb2, x2w, xw, bwith ;

data = loadmat("c:\\epd\\data\\excel\\grunfeld.xls");

nt = 200 ; nn= 10 ; tt = 20 ;

// balanced panel data set

y = data[][2] ; x = data[][3:4] ; // defining y and x matrices

ynt = reshape(y,nn,tt) ; yb = meanr(ynt) ; yw

= vecr(ynt-yb) ;

x1nt = reshape(x[][0],nn,tt) ; xb1 = meanr(x1nt) ; x1w

= vecr(x1nt-xb1) ;

x2nt = reshape(x[][1],nn,tt) ; xb2 = meanr(x2nt) ; x2w

= vecr(x2nt-xb2) ;

xw = x1w~x2w ; bwith = invertsym(xw'xw)*xw'yw ;

println("bw = " , bwith) ;

}

26 Note also that it is possible to run a GAUSS program under Ox, see Doornik (2004), Laurent and Urbain (2004) and Viton (2003). Of course, Ox can be interfaced with C, C++ programs. 27 Following Podovinsky (1999), a class provides "an abstract definition of an object (both in terms of variables - the data - and functions - the methods that apply to the data-)". 28 A user interested in writing his own Ox classes may refer to Doornik, Draisma and Ooms, (2001,Chap. 8, pp. 78­91).

938

P. Blanchard

Table 26.30 Ox code #2 (grunfeld2.ox)

#include <oxstd.h> #import <database> // don't put a ;

main()

{

decl mydb, mnames, names1, y, ly, mdata, ident, year ;

mydb = new Database();

mydb.Load("c:\\epd\\data\ox\\grunfeld.in7"); // load data

mydb->Info() ;

// info on the file

mnames = mydb->GetAllNames() ; // load var. names into a vector

println("# of obs = " , mydb->GetSize() ) ; // print nb. of obs.

println("var. names = " , mnames ) ; // and var. names

mdata = mydb->GetAll() ; // load all data in a matrix

ident = mydb->GetVar("ident") ; // load ident var. into ident

mydb->SetSelSample(-1,1,-1,1) ; // set sample, necessary

year = mydb->GetVar("year") ; // load year var. into year

mydb->Select(0, {"gi",0,0, "vf",0,0} ) ; // 2 var. in group 0

println("# of indiv : ", columns(unique(ident)),

"\n period : ", min(unique(year)) ,

" : ", max(unique(year)) ) ;

y = mydb->GetGroup(0) ; // load 2 var from group 0 into y

ly = log(y) ; // taking log of 2 variables

names1 = {"y", "log(y)"} ;

// print result for checking

println("%c", names1, (y~ly)[0:1][]) ; // 2 rows, all columns

delete mydb;

// finished with object

}

This short program suggests two points:
· With the Database class, it is easy to read (and to create) a data file with mydb->Load(...); Moreover, mydb->Info() ;may be used to obtain some file information (variable names, number of missing values. . . ).
· We see according to this example that this class enables to access to the data but also to predetermined functions working on this data (which may be different from the class used). For instance, the GetSize() function returns the number of observations in the file. GetVar(...) and GetAll() allow to put some or all variables in a vector or in a matrix.
· In the same way, with Select(...) and GetGroup(...), we can define some object (a group named 0, 1 containing one or several variables. . . ) and then manipulating it.
· Don't forget to define the SetSelSample(...) which is mandatory. Remember that missing values are dropped out. Note also, that SetSelSample(...) must be defined just after the Select(...) instruction, but only if you use Select(...) and GetGroup(...)...
· There are many functions which may be used for each class. Note however that the Database class is not, as it stands here, able to manage panel data if, for

26 Software Review

939

instance, we want to create lags (and to put a missing value for the first observation for each individual). This can be realized by an other class, DPD class which has many of the Database class instructions (plus some other ones, that we will examine later) or by hand (in a loop).
Ox comes with several (free) packages or classes:29 Maximization package (numerical optimization and differentiation), Probability package (density, cumulative density, random number. . . ), Simulation class (Monte Carlo experiments), QuadPack (numerical integration), Lapack (matrix calculus). . . Among these packages, three may be useful for panel data estimation.
Ox-DPD (cf. Doornik, Arellano and Bond, 2002) is a package for estimation of dynamic panel data models, whose functionalities are closed to DPD GAUSS. In Table 26.31, we illustrate Ox-DPD use by an example.

Table 26.31 Ox code #3 (ab.ox)
#include <oxstd.h> #import <packages/dpd/dpd> main() { decl dpd = new DPD(); // defining data set and model used dpd.Load("c:\\epd\\data\ox\\abdata.in7"); // load data dpd.SetOptions(TRUE); // robust standard errors dpd.SetYear("YEAR"); dpd.SetGroup("IND"); dpd.SetDummies(D CONSTANT + D TIME); // specify dummies dpd.Select(Y VAR, {"n", 0, 0}); // endogeneous & regressors dpd.Select(X VAR, {"n", 1, 2, "w", 0, 1, "k", 0, 2, "ys", 0, 2}); print("\n\n***** Within *****");
dpd.SetTransform(T WITHIN); dpd.Estimate(); print("\n\n***** AH diff *****");
dpd.Select(I VAR, {"n", 2, 3, "w", 0, 1, "k", 0, 2, "ys", 0, 2});
dpd.SetTransform(T DIFFERENCES); dpd.Estimate(); print("\n\n***** AB col a2 *****");
dpd.DeSelect() ; // reformulate model, warning, see the text dpd.SetYear("YEAR"); dpd.SetGroup("IND"); dpd.SetDummies(D CONSTANT + D TIME) ; dpd.Select(Y VAR, {"n", 0, 0}); dpd.Select(X VAR, {"n", 1, 2, "w", 0, 1, "k", 0, 2, "ys", 0, 2}); dpd.Select(I VAR, {"w", 0, 1, "k", 0, 2, "ys", 0, 2}); dpd.Gmm("n", 2, 99); // GMM-type instrument dpd.SetTest(1, 2); // Sargan,AR 1-2 tests dpd.SetMethod(M 2STEP); dpd.Estimate(); // 2-step estimation delete dpd; // finished with object }
29 Loosely speaking, there is a difference between a package and a class: a package is a compiled version of one or more classes.

940

P. Blanchard

We can make two comments about this code.
· The panel structure is declared with the dpd.SetYear("YEAR") instruction. So you do not need an individual identifier in your datafile (Ox-DPD creates an index variable by differencing the year variable). If you have one, it is better to write also dpd.SetIdent("IDENT") when, e. g., for some individuals the last year is 1998 and the first one of the next individual is 1999. One tricky point is that you must not confuse uppercase (``IDENT'') and lowercase (``ident'') depending upon the way you create the variable.
· Be careful with the statements dpd.Select(...), dpd.Gmm(...). . . For instance, if you write dpd.Select(X VAR, {"n", 0, 1}); and after dpd.Select(X VAR, {"k", 1, 2}); there will be 4 variables in the regressors list, nt , nt-1, kt-1 and kt-2. This explains why you will need the dpd.DeSelect() ; which clears completely the list.

Second, with the the Maximization package, it is easy (and in a very fast manner) to estimate a random effects model by ML on Grunfeld data, as shown in Table 26.32. Note two points: 1) GAUSS offers broadly the same functionalities, but Ox does not provide a constrained optimization and a constrained maximum likelihood modules; 2) If the log likelihood has to be evaluated by individuals, the program becomes more more complicated (see probitreml.ox for an illustration).
Lastly, we must mention also a new (free) package, DCM (Discrete Choice Models) written by Weeks and Eklo¨f (see Eklo¨f and Weeks 2004a, b). DCM is a package to estimate several classes of discrete choice multinomial models, particularly, conditional logit, mixed logit, multinomial probit, nested logit, ordered probit. . . on cross-section and (in somes cases) on panel data. DCM is a class written in Ox and its use is simple and flexible. This is especially true concerning30 the structure of the database to be read (which consists in observations about individual and/or choice characteristics for a combination of individuals, time and alternatives). DCM can read several structures with few and simple instructions. As, up to now, in the version 1.0, all panel data features are not yet implemented (only for conditional logit and mixed logit model), we do not give more details on it (see discrete choice.ox for a simple example), but it is a promising package.
As a matrix programming language, Ox competes directly with GAUSS for several reasons: First, Ox is cheaper than GAUSS (recall that some Ox versions are free); Second, if you are a C or C++ users, the object-oriented programming approach gives more flexibility; Lastly, in many cases, Ox is faster than GAUSS (see Steinhaus, 2002, Ku¨sters and Steffen, 1996 and Sect. 26.4.1 for an evaluation of speed in a panel data context). For more general reviews, cf. Cribari-Neto (1997), Cribari-Neto and Zarkos (2003), Kenc and Orszag (1997) and Podovinsky (1999).
Ox has an excellent (but quite technical) documentation (see Doornik, Draisma and Ooms (2001) and Doornik (2001)). The Web site is excellent (Doornik's one, because Timberlake web site is only commercial) with the possibility to consult or

30 It is also easy to store the estimation results in a LATEXformatted table.

26 Software Review

941

Table 26.32 Ox code #4 (chronoml.ox)
#include <oxstd.h> #include <oxfloat.h> #import <maximize> decl gY, gX, gTT ; // global data LOGLreml(const coeff, const mloglik, const
avScore, const amHessian) { decl llognc ; llognc = -log(2*M PI)/2- (gTT-1)*log(coeff[3]^2)/(2*gTT)
- log(coeff[3]^2+gTT*coeff[4]^2)/(2*gTT) - (1/(2*coeff[3]^2))*((gY[][1]- coeff[1]*gX[][2] - coeff[2]*gX[][3]).^2) - (1/(2*coeff[3]^2+gTT*2*coeff[4]^2)) *((gY[][0]- coeff[0]- coeff[1]*gX[][0]- coeff[2]*gX[][1]).^2 ); mloglik[0] = double( meanc(llognc) ); return 1; } main() { decl coeff, valfunc, coder, data, x, y, nt, nn, tt, yb, yw, xb, xw, mhess, mcovar ; data = loadmat("c:\\epd\\data\\excel\\grunfeld.xls") ; y = data[][2] ; x = data[][3:4] ; // y, x yb = data[][5] ; xb = data[][6:7] ; // yb, xb yw = data[][8] ; xw = data[][9:10] ; // yw, xw tt = 20 ; gY = yb~yw ; gX = xb~xw ; gTT = tt ; print("Random effects model by ML","\n\n"); coeff = <-42.71; 0.115; 0.23; 94; 94>; // ols starting values coder = MaxBFGS(LOGLreml, &coeff, &valfunc, 0, TRUE) ; Num2Derivative(LOGLreml,coeff,&mhess) ; mcovar = invertgen(-mhess) / rows(y) ; print("parameters:", coeff' ,"\ nstd err.:", sqrt(diagonal(mcovar)) ) ; }

download the Ox documentation. The Ox-users discussion group is not very active but the Doornik's support is very efficient.

26.4 Performance Hints and Numerical Accuracy Evaluation
26.4.1 Speed Comparison31
First, in Table 26.33 we provide, for each software, time execution32 to estimate a linear panel data model with 10 regressors by OLS, within and FGLS estimators on a
31 More detailed speed evaluations (but not on panel data) can be found in Ku¨sters and Steffen (1996), Nerlove (1998) and Steinhaus (2002). 32 In this review, we use a Pentium 4, 1.6 GHz, with 70 Go HDD and 256 MB RAM running under Microsoft Windows 2000.

942

P. Blanchard

Table 26.33 Speed evaluations

Software

Linear Model Data Reading Timea
+Execution Time

ML Estimation Resultsb

EViews GAUSS
LIMDEP Ox Pro. RATS SAS
Stata B TSP

7s TSCS: 2s User's program: 1.7s 4s DPD/Ox: 5s User's program: 1.3s 5s 10s
17s
7s

Marquardt (s2,160,5.2s), BHHH (s2,200,6.6s)c BFGS(s1,124,0.23s), DFP(ns) Newton (s2,77,0.86s), BHHH (s2,967,3.64s)d BFGS (ns), DFP (s2,16,3.8s) Newton (s1,34,36.38s), BHHH (ns)e BFGS (s1,247,0.17s) f
BFGS (s1,117,2s), BHHH (s1,930,35s)g Newton(s2,5,0.06s), Gauss (s1,207,0.51s) Marquardt (s1,81,0.18s)h Newton-Raphson (s1,11,1.25s)i HHH (s2,937,8.5s), Newton (s1,28, 0.25s) Gauss(s1,28,1s), BFGS (s2,32,0.16s) disCrete hessian (s1,28,1.08s) j

a The files were saved in the proprietary's format for each software. File sizes are about 8-15 Mb. b For each available algorithm, we give between parenthesis (1) the convergence status (s1) (resp.
s2) convergence with the first (resp. second) starting values set. Ns indicates no solution, (2) , the
number of iterations and (3) the execution time in seconds. c with the LogL object and the ml instruction. d with the Fastmax procedure. e with the Maximize instruction. f with the Maximization package and the MaxBFGS function. g with the maximize instruction. h with Proc Nlin. i with the ml command. j with the ml instruction.

large33 unbalanced panel data set. It contains 10.000 individuals observed between 5 and 12 years (85.160 observations), randomly generated (as in Nerlove, 1971). Second, in the same table, we indicate time performance and convergence status to estimate a random effects model by the maximum likelihood method (see Hsiao, 1986 for the definition of the log-likelihood34). Grunfeld's data are used.
33 In several cases, it is probably necessary to adjust the memory size: for TSP, using OPTIONS instruction, for Limdep using TOOLS/OPTIONS/PROJECT menu, for SAS in modifying the CONFIG file and setting memory 10m for Stata. 34 See Nerlove (2002) for alternative specifications.

26 Software Review

943

Consider, first, the linear model estimation case. In general, speed performances are good and relatively closed. Ox seems to be the faster, and does a little better than GAUSS when using a specific program. Nevertheless, if we use Ox with the DPD class, the program is slower but the results are more detailed, and more estimation methods and options are available. LIMDEP, RATS and TSP are also quite fast when estimating linear panel data models. The slower ones are Stata and SAS. For SAS, it is due to its numerous read/write operations. There is no apparent explanation for Stata. Probably, using the most powerful version of Stata (Stata/SE) would reduce time execution for this software. Note also that GAUSS and Ox are more complex to use in comparison with the other packages. Finally, we can remark that the estimates obtained (not reported here) are very closed between software. The main differences come from FGLS estimation due to different computations of i.
Nevertheless, there are several special cases. With RATS, if we use the Pregress instruction, execution time grows to 11mn (due to the estimation of the N fixed effects). It is then better to use the Panel instructions (cf. pp. xiii in this chapter) with large panels. For SAS, for the same reason, it was impossible, with my configuration, to apply Tscsreg procedure. SAS reported an unsufficient memory error message even when the virtual memory was largely increased. So we use the Glm procedure to estimate the FE model and Proc Mixed for the RE model. Nevertheless, this last instruction is very inefficient because the estimation is done by ML. Hence, we program directly all the variable transformations and use OLS on transformed variables in order to implement the between, within and FGLS estimators. For GAUSS, we use first the TSCS module and also a specific program we wrote. This program loads all the data in memory and does two Do-loops individual by individual for the computations. So, for all these reasons, in some cases, comparing performance hints is difficult.
General lessons for the maximum likelihood estimation are also difficult to draw. First, note that we try to "standardize" the setup for ML estimation for all software. The likelihood function is the same in each case without specifying first or second derivatives. We do not use any built-in instruction owing to estimate directly RE model by ML without defining explicitely the log-likelihood. As far as it was possible, the convergence criterion is based on the percentage changes in coefficients and/or on the norm of the gradient (both equal to 10-5). Finally, we define two sets of starting values for the coefficients: first very far ones from the solution (labelled as s1); second, s2 is defined according to the OLS estimation. At this point, several comments may be done. First, once again, Ox performs extremely well (quick convergence with s1) but it offers only one algorithm, BFGS.35 RATS and Stata have also good convergence properties: they converge quickly with s1 (with the BFGS method for RATS). All other software achieve convergence, and sometimes very quickly, sometimes even with s1, but not for all the available algorithms. Nevertheless, EViews converges only with s2. Finally, the most surprising result is that

35 Newton algorithm may be used but only when providing, at least, first derivatives.

944

P. Blanchard

a given method may perform very well with a software but not with another one. So, users have to check their final estimation results in using different algorithms, different starting values and (if possible) different software.

26.4.2 Numerical Accuracy Evaluations
Econometricians have devoted considerable efforts to develop better or new estimation methods. Less was done in controlling numerical accuracy of computational routines used for implementing them. In applied works, panel data users encounter frequently errors or warning messages such as not invertible matrix, not convergence, underflow, overflow. . . without knowing if these problems come from the data or the algorithm used.
Computational errors arise from a variety of reasons:
· Inexact binary representation of a real value (e.g. 0.1 may be in fact treated as 0.099999964),
· Rounding errors because of limited computer's word length, · Algorithmic errors (truncation, bugs or inadequate algorithm).
Most of the time (see McCullough and Vinod, 1999,McCullough, 1998,1999a, 1999b), software numerical accuracy is evaluated by comparison with the benchmark results for the Statistical Reference data Sets (SRD) provided by the National Institute of Standards and Technology (NIST, cf. Gill, Guthrie, Lagergren and Rogers, 2001). These reference datasets are mainly artificial datasets created to evaluate software with ill-conditioned data in four general areas useful for our purpose: univariate statistics, analysis of variance, linear regression and non linear regression. Moreover, they are classified according to their difficulty level (lower, average, higher) depending on the number of constant leading digits (3, 7 or 13). NIST provided, for each case, certified values up to 15 (11 for nonlinear estimation) significant digits (i.e. the first nonzero digit and all succeeding digits).
We present, for each software, in Table 26.34 a synthesis of NIST benchmark tests. For each test category and for each difficulty level (in the following order: lower, average and higher), we give the mean, and between parenthesis, the minimum and the maximum of number of digits accuracy obtained in using all the datasets for a given level. This number (called LRE) is between 0 (fail) and 15 (perfect result up to 15 digits. It is frequently admitted36 that, for a lower level of difficulty, LRE must be equal or superior to 10 (6 for nonlinear estimation), whereas for high level, a LRE value equal or greater than 4 or 5 is reasonable.

36 See, for example, Nerlove 2001.

26 Software Review

945

Three main conclusions may be drawn on these results:
1. First, these results should be taken with care. As there are obtained on very special datasets, their interest for usual econometric practice is controversial. For instance, note that the dataset (SmLs09.dat, high difficulty level) used for checking Anova computations contains 18.009 observations like 1000000000000.n where n equals 2, 3, 4, 5 or 6!. For non linear estimations, the variety of available algorithms and the different settings of convergence criteria make some comparisons quite difficult (see Lilien, 2000). Nevertheless, an important point consists in the fact that all software send a warning or an error message when colinearity or non convergence problems occur.
2. For univariate statistics, Anova and linear regression, it seems that we can be relatively confident in the results obtained by all our reviewed software. More precisely, univariate statistics are very accurately computed. This is also true for standard errors (about 13 digits) but less for the first order autocorrelation coefficient: EViews and RATS fail to compute it correctly in some cases (6 and 3 times respectively). For anova, EViews has a 0 score 3 times. For the most difficult case (smls09), only Ox, SAS, Stata and TSP obtain 2­4 correct digits. Linear estimation results are very closed and good with one exception: with the Filippeli dataset, all software whose results are reported, except LIMDEP and Ox,37 obtain a zero score.
3. Benchmark results for non linear regression are more mitigated and vary considerably upon the software used and the tests done. Two main comments may de done: First, SAS, Stata and TSP have excellent performances (never less than 5­6 digits, most of the time about 8­10 correct digits) but EViews and RATS works also well even if they fail in some difficult cases. Second, the more surprising result is the poor performance of Gauss and Ox. For some datasets, this may be explained by the fact we do not provide analytical first derivatives. But SAS and Stata also use numerical derivatives. We must note that we use the software defaults for non linear estimation, setting only the maximum number of iterations to 1000 and the convergence criteria to 1e-11. So, at least for Gauss which offers many options (algorithm. . . ), it is possible to obtain better results in modifying the non linear setup. Nevertheless, it is doubtful that the results will be completely changed (see Vinod, 2000).
We conclude this section by giving some information on uniform random generation numbers. In Table 26.35, we summarize the main characteristics of uniform random generator number used by the software that we review. We give the results of a very well known RNG test, called DIEHARD test which consists in 18 specific tests.
We can make several remarks. First, it must be noticed that all analyzed software except RATS and SAS, provide recent RNG. Indeed, RNG with a period of 232 (or less) are useless for large simulation work. For instance, admit that

37 Another exception is Mathematica, cf. Nerlove, 2001.

946

P. Blanchard

Table 26.34 Numerical accuracy results

Softwarea Univariate Anovac Statisticsb

Linear Regressiond

Non Linear Regressiond,e, f

15 (15­15) 9.1 (0­15)

12.8 (12.2­13.3) 9.6 (8­11)

EViews 5g 15 (15­15) 9.2 (0­15)

15 (15­15)

8 (0­11)

15 (15­15) 2.2 (0­3.3)

6.4 (0­9.9)

8.5 (0­10.9)

15 (15­15) 13.4 (12.4­14.5) 11.7 (11.3­12.1) 5.9 (3­7.8)

GAUSS 5h 14.5 (14­15) 7.9 (6.5­8.5)

14.9 (14.7­15) 3 (0­6.4)

14 (14­14) 1.6 (0­2.7)

6.4 (0­13.4) 4.6 (0­7.8)

15 (15­15) 13.2 (12.4­14) 13.3 (13.2­13.3) 8.9 (7.1­9.9)

LIMDEP 7i 14.5 (14­15) 7.7 (6.2­8.4)

14.9 (14.7­15) 7.3 (0­10.6)

14 (14­14) 1.4 (0­2.4)

10.2 (6.7­14.6) 8.3 (7­9.5)

15 (15­15) 14.5 (12.9­15.0) 13.1 (12.7­13.6) 5.2 (0­9.8)

Ox Pro. j 15 (15­15) 9.9 (9.3­11.7) 14.9 (14.7­15) 3.2 (0­9)

15 (15­15) 3.3 (3.3­3.3)

9.8 (7.3­12.8) 4.5 (0---10.5)

14.5 (14­15) 13.6 (12.4­14.4) 11.8 (11.5­12.1) 6.7 (5.5­8.8)

RATS 6k 14 (14­14) 7.9 (6.5­8.5)

14.9 (14.7­15) 5.8 (0­10.6)

15 (15­15) 1.6 (0­2.7)

6.2 (0­9.1)

6.7 (3.4­9.4)

15 (15­15) 13.6 (12.7­15) 11.9 (11.5­12.3) 10 (7.4­11)

SAS 9.1l 14.5 (14­15) 9.9 (8.8­10.4) 14.9 (14.7­15) 9.8 (6.7­11)

14 (14­14) 4.3 (4.2­4.4)

6.3 (0­9.6)

9.6 (7.6­11)

15 (15­15) 13.7 (13.1­15) 12.1 (11.5­12.8) 8.5 (6.7­9.4)

Stata 8.1m 15 (15­15) 10.2 (10.2­10.4) 14.9 (14.7­15) 8.1 (4.8­10.9)

15 (15­15) 4.3 (4.2­4.4)

6.9 (0­12.1) 7.2 (6­8.3)

15 (15­15) 13.7 (12.3­14.7) 12.5 (12.1­12.9) 9.7 (7.9­11)

TSP 4.5n 14.5 (14­15) 10.25 (10.2­10.4) 14.9 (14.7­15) 9.7 (6.5 ­11)

14 (14­14) 3.7 (2.1­4.6)

8.4 (0­12.8) 9.4 (7.9­11)

a Author's computations in all cases. b For univariate statistics, we report only the results for mean computation. c For anova, we report only the results for the F result. d For linear and non linear regression, we report only the results for the estimated coefficients. e When non convergence was encountered with the first set of starting values, we provide the re-
sults obtained with the second set of initial values. f EViews and TSP use analytical first derivatives. The other softwate use numerical first deriva-
tives. g Used instructions: stats, testby and ls. h Used instructions: meanc, stdc, ols and the Constrained Optimization module. For anova, a spe-
cific code is used (see Vinod, 2000). i Used instructions: dstat, xvcm, regress and nlsq j Used instructions: meanc, varc, acf, olsc and MaxBFGS. For anova, a specific code is used (see
Vinod, 2000). k Used instructions: statistics, correlate, pstat, linreg and nlls. l Used instructions: proc means, proc arima (+ proc autoreg), proc anova, proc reg and proc nlin. m Used instructions: summarize, corrgram, anova, reg and nl. n Used instructions: msd, bjident, olsq and lsq.

26 Software Review

947

Table 26.35 Random generator number
Softwarea Function Reference Name

Max. No. of success to Period Diehard testsb

Knuth (1997)

2129 18/18

EViews rnd

L'Ecuyer (1999)

2319 18/18

Matsumoto-Nishimura (1998) 219937 18/18

GAUSS

rndu rndKMu

Kennedy-Gentle (1980) Marsaglia (2000)

232 - 1 8/18 108888 17/18

LIMDEP rnu

L'Ecuyer (1999) Park-Miller (1988)

2191 18/18 232-1 17/18

Ox

ranu

Marsaglia (1999)

260 17/18

L'Ecuyer (1997) RATS %uniform Park-Miller (1987)

2113 18/18 232-1 16/18

SAS

ranuni uniform

Fishman and Moore (1982)

231 - 1 14/18

Stata TSP

uniform Marsaglia (1994)

random

L'Ecuyer (1990) L'Ecuyer (1999)

2126 231-1 2319

18/18 17/18 18/18

a author calculations. b Diehard tests return a p-value. A test fails when the p-value is ¡ 0.01 or ¿ 0.99. When a test returns
several p-values, it fails if more than 2 p-values are ¡ 0.01 or ¿ 0.99.

we follow Knuth's recommendation, i.e. set the maximum number of draws as d < period/1000. So, in using a RNG with a period of 232, we would only allow for about 400 replications in a panel data simulation study with NT = 1000 and 10 variables. So, some care is needed when using SAS and RATS RNG. Second, even if the fact to pass the DIEHARD test (or other ones) does not prove that a RNG is a performing one, but failure to pass several tests proves that it should not be used. These two issues are important since the quality of a uniform RNG is crucial for the quality of a non uniform one and, so determines in part the validity of the simulation results. On this last ground, users may be confident with the 8 software we reviewed.
Acknowledgments I am grateful to Aptech System Inc. (GAUSS), Econometric Software Inc. (LIMDEP), Estima (RATS), Quantitative Micro Software (EViews), Stata Corp., Timberlake Consultants (Ox), TSP International, SAS Institute, for having provided me freely a copy for review purposes.
I sent a prior draft of the review to each of the software editors for checking. Any remaining errors are my own. I am grateful to C. Cummins (TSP), W. Greene (LIMDEP, NLOGIT), G. Liang (EViews), T. Doan (RATS), R. Schoenberg (GAUSS), D. M. Drukker (Stata) for helpful comments and advice. I would like to thank M. Harris for helpful comments on an earlier draft.

Appendix: General Product Information

Table 26.A.1 Software general informations

Name
EViews 6 (2007) GAUSS 8 (2007) LIMDEPc 9 (2007) Oxe 4.1 (2007) RATS 6.35 (2007) TSP 5 (2005) SAS 9.1 (2003) Stata 10 (2007)

Platform/OSa
Win98+ Windows (32-bit) Unix, Linux
Windows (32-bit)
Windows Mac Unix, Linux Windows, Mac Unix, Linux Windows, Mac Unix, Vax-Vms All except Macintosh Windows, Mac Unix, Linux

Student Version Yesl (v.3, $40) No
Yesd (free)
Yes (85£) f
Yes ($300)g
Yes ($100)h Yesi No j

a Win98+ = Windows 98 and later, WinNT+ = Windows NT, 2000, XP. b Academic, Windows version, one copy. c With NLOGIT. d EA-Limdep: http://www.prenhall.com/greene/ealimdep.htm e With GiveWin. f OxConsole (DOS/Unix) free. g $60 with a limit of 6000 obs. h With a limit of 16000 numbers. $75 without GiveWin. i according to country. j Small Stata 8, $400. k With complete documentation set.

Pricesb
$495k (V. 6) $1000 LIMDEP $595 Nlogit $795 £250
$500
$500e
i
$985 (v. SE 10)k

Web Site
http://www.eviews.com http://www.aptech.com http://www.limdep.com http://www.doornik.com http://www.tspintl.com http://www.estima.com http://www.sas.com http://www.stata.com

P. Blanchard

948

26 Software Review

949

References

Allison, P. D. (1999). Logistic Regression Using the SAS System: Theory and Application. John Wiley & Sons, New York.
Anderson, T. W. and Hsiao, C. (1981). Estimation of Dynamic Models with Errors components, Journal of the American Statistical Association, 76:598­606.
Arellano, M. and Bond, S. (1991). Some Tests of specification for Panel Data: Monte Carlo Evidence and an Application to Employment Equations. Review of Economic Studies, 58:277­297.
Arellano, M. and Bond, S. (1998). Dynamic panel Data Estimation Using DPD98 for GAUSS: A Guide for Users. Mimeo.
Barrachina, R. and Sanchis Llopis, J. A. (2002). Stata 7.0 para windows. Revista de Economia Aplicada, 10:163­174.
Cribari-Neto, F. (1997). Econometric Programming Environments: GAUSS, Ox and S-Plus. Journal of Applied Econometrics, 12:77­89.
Cribari-Neto, F. (1999). C for Econometricians, Computational Economics. 14:135­149. Cribari-Neto, F. and Zarkos, S. G. (2003). Econometric and Statistical Computing Using Ox. Com-
putational Economics, 21:277­295. Doornik, J. A. (2001). Object-Oriented Matrix Programming using Ox. 4th edition. Timberlake
Consultants Press, London. Doornik, J. A. (2004). Ox Appendices. Mimeo. Doornik, J. A., Arellano, M. and Bond, S. (2002). Panel data estimation using DPD for Ox. Mimeo. Doornik, J. A., Draisma G. and Ooms M. (2001). Introduction to Ox. Timberlake Consultants
Press, London. Eddelbu¨ttel, D. (1996). Object-Oriented Econometrics: Matrix Programming in C++ Using GCC
and NEWMAT. Journal of Applied Econometrics, 11:199­209. Eklo¨f, M. and Weeks, M. (2004a). Estimation of Discrete Choice Models Using DCM for Ox.
Mimeo. Eklo¨f, M. and Weeks, M. (2004b). Discrete Choice Models (DCM): An Object-Oriented Package
for Ox. Mimeo. Fry, T. R. (1996). LIMDEP 7.0. Journal of Economic Surveys, 10:461­465. Gill, L., Guthrie, W., Lagergren, E., and Rogers, J. (2001). Statistical Reference Datasets (StRD)
for Assessing the Numerical Accuracy of Statistical Software, Proceedings of the Joint Statistical Meetings, Anaheim. Gould, W., Pitblado, J. and Sribney, S. (2003). Maximum Likelihood Estimation with Stata. 2nd edition. Stata Press College Station. Heywood, G. and Rebelo, I. (1996). Time series analysis of cross-sections: A software guide. The Economic Journal, 106:271­275. Holly, S. and Turner, P. (1998). Programming Facilities in EViews. The Economic Journal, 108:1632­1639. Hsiao, C. (1986). Analysis of Panel Data. Cambridge University Press Cambridge. Keane, M. P. and Wolpin, K. I. (1997). The Career Decisions of Young Men. Journal of Political Economy, 105:473­522. Kenc, T. and Orszag, J. M. (1997). Ox: an object-oriented matrix language. The Economic Journal, 107:256­259. Kerninghan, B. W. and Ritchie, D. M. (1988). The C Programming Language. 2nd edition. Prentice Hall New Jersey. Kolenikov, S. (2001). Review of Stata 7. Journal of Applied Econometrics, 16:637­646. Ku¨sters, U. and Steffen, J. P. (1996). Matrix Programming Languages for Statistical Computing: A Detailed Comparison of GAUSS, Matlab, and Ox. mimeo, Katholischen Universita¨t Eichsta¨tt. Laurent, S. and Urbain, J.-P. (2004). Bridging the Gap Between Ox and GAUSS using OxGAUSS. mimeo. Lee, M.-T. (2002). Panel Data Econometrics, Academic Press, San Diego. Lilien, D. M. (2000). Econometric Software Reliability and Nonlinear Estimation in EViews: Comment. Journal of Applied Econometrics, 15:107­111.

950

P. Blanchard

Lubrano, M. (1998). TSP 4.4: Old but Powerfull Econometrics Software. The Economic Journal, 108:1621­1627.
McCullough, B. D. (1997). A Review of RATS v4.2: Benchmarking Numerical Accuracy. Journal of Applied Econometrics, 12:181­190.
McCullough, B. D. (1998). Assessing the Reliability of Statistical Software: Part I. The American Statistician, 52:358­366.
McCullough, B. D. (1999a). Assessing the Reliability of Statistical Software: Part II. The American Statistician, 53:149­159.
McCullough, B. D. (1999b). Econometric Software Reliability: EViews, LIMDEP, SHAZAM and TSP. Journal of Applied Econometrics, 14:191­202.
McCullough, B. D. and Vinod, H. D. (1999). The Numerical Reliability of Econometrics Software. Journal of Economic Literature, 37:633­665.
McKenzie, C. R. (2000). LIMDEP, Version 7, The Economic Journal, 110:F455­F461. McKenzie, C. R. and Takaoka, S. (2003). 2002: A LIMDEP Odyssey. Journal of Applied Econo-
metrics, 18:241­247. Merino, F. (1998). TSP 4.4 for non time series. The Economic Journal, 108:1627­1632. Nerlove, M. (1971). Further Evidence on the Estimation of Dynamic Economic Relations From a
Time Series of Cross Sections. Econometrica, 39:359­381. Nerlove, M. (1998). GAUSS and Matlab: Comparison and Evaluation with a Supplementary Dis-
cussion of MAPLE and MATHEMATICA. mimeo. Nerlove, M. (2001). On the Numerical Accuracy of Mathematica 4.1 for Doing Ordinary Least-
Squares Regressions. mimeo. Nerlove, M. (2002). Essays in Panel Data Econometrics. Cambridge University Press Cambridge. Noble, N. R. (1999). Software Reviews: EViews 3.0. International Journal of Forecasting,
15:222­224. Podovinsky, J. M. (1999). Ox 2.10: Beast of Burden or Object of Desire? Journal of Economic
Surveys, 13:491­502. Roberts, J. (1995). Econometrics Views Version 1.0. The Economic Journal, 105:1048­1055. Romeu, A. (2004). ExpEnd: GAUSS Code for Panel Count-data Models. Journal of Applied
Econometrics, 19(3):429­434. Silk, J. (1997). TSP 4.4: A Review, Journal of Applied Econometrics, 12:445­453. Sparks, G. R. (1997). Econometric Views 2.0, Journal of Economic Surveys, 11:107­113. Stata Corp., (2003). Cross-Sectional Time-Series. Stata Press, Collage Station. Steinhaus, S. (2002). Comparison of Mathematical Programs for Data Analysis. mimeo. Stroustrup, B. (1992). The C++ Programming Language. Addison-Wesley, Murray Hill NJ. Train, K. (2003). Discrete Choice Methods with Simulation. Cambridge University Press
Cambridge. Vinod, H. D. (2000). Review of GAUSS for Windows, Including its Numerical Accuracy. Journal
of Applied Econometrics, 15:211­220. Viton, P. A. (2003). Running GAUSS Programs Under Ox3. Mimeo. Windmeijer, F.(2000). A finite sample correction for the variance of linear two-step GMM estima-
tors. IFS working paper W00/19. Wooldridge, J. M. (2002). Econometric Analysis of Cross Section and Panel Data. MIT Press. Wright, R. E. (1996). LIMDEP: Limited Dependant Variables Models, version 7.0. The Economic
Journal, 106:1458­1460.

Advanced Studies in Theoretical and Applied Econometrics

1. J.H.P. Paelinck (ed.): Qualitative and Quantitative Mathematical Economics. 1982

ISBN 90-247-2623-9

2. J.P. Ancot (ed.): Analysing the Structure of Econometric Models. 1984

ISBN 90-247-2894-0

3. A.J. Hughes Hallet (ed.): Applied Decision Analysis and Economic Behaviour. 1984

ISBN 90-247-2968-8

4. J.K. Sengupta: Information and Efficiency in Economic Decision. 1985

ISBN 90-247-3072-4

5. P. Artus and O. Guvenen (eds.), in collaboration with F. Gagey: International Macroeco-

nomic Modelling for Policy Decisions. 1986

ISBN 90-247-3201-8

6. M.J. Vilares: Structural Change in Macroeconomic Models. Theory and

Estimation. 1986

ISBN 90-247-3277-8

7. C. Carraro and D. Sartore (eds.): Development of Control Theory for Economic Analysis.

1987

ISBN 90-247-3345-6

8. D.P. Broer: Neoclassical Theory and Empirical Models of Aggregate Firm Behaviour.

1987

ISBN 90-247-3412-6

9. A. Italianer: Theory and Practice of International Trade Linkage Models. 1986

ISBN 90-247-3407-X

10. D.A. Kendrick: Feedback. A New Framework for Macroeconomic Policy. 1988

ISBN 90-247-3593-9; Pb: 90-247-3650-1

11. J.K. Sengupta and G.K. Kadekodi (eds.): Econometrics of Planning and

Efficiency. 1988

ISBN 90-247-3602-1

12. D.A. Griffith: Advanced Spatial Statistics. Special Topics in the Exploration of Quantita-

tive Spatial Data Series. 1988

ISBN 90-247-3627-7

13. O. Guvenen (ed.): International Commodity Market Models and Policy Analysis. 1988

ISBN 90-247-3768-0

14. G. Arbia: Spatial Data Configuration in Statistical Analysis of Regional

Economic and Related Problems. 1989

ISBN 0-7923-0284-2

15. B. Raj (ed.): Advances in Econometrics and Modelling. 1989 ISBN 0-7923-0299-0

16. A. Aznar Grasa: Econometric Model Selection. A New Approach. 1989

ISBN 0-7923-0321-0

17. L.R. Klein and J. Marquez (eds.): Economics in Theory and Practice. An Eclectic Ap-

proach. Essays in Honor of F. G. Adams. 1989

ISBN 0-7923-0410-1

18. D.A. Kendrick: Models for Analyzing Comparative Advantage. 1990

ISBN 0-7923-0528-0

19. P. Artus and Y. Barroux (eds.): Monetary Policy. A Theoretical and

Econometric Approach. 1990

ISBN 0-7923-0626-0

20. G. Duru and J.H.P. Paelinck (eds.): Econometrics of Health Care. 1990

ISBN 0-7923-0766-6

21. L. Phlips (ed.): Commodity, Futures and Financial Markets. 1991

ISBN 0-7923-1043-8

Advanced Studies in Theoretical and Applied Econometrics

22. H.M. Amman, D.A. Belsley and L.F. Pau (eds.): Computational Economics and

Econometrics. 1992

ISBN 0-7923-1287-2

23. B. Raj and J. Koerts (eds.): Henri Theil's Contributions to Economics and Econometrics.

Vol. I: Econometric Theory and Methodology. 1992

ISBN 978-0-7923-1548-3

24. B. Raj and J. Koerts (eds.): Henri Theil's Contributions to Economics and Econometrics.

Vol. II: Consumer Demand Analysis and Information Theory. 1992

ISBN 978-0-7923-1665-7

25. B. Raj and J. Koerts (eds.): Henri Theil's Contributions to Economics and Econometrics.

Vol. III: Economic Policy and Forecasts, and Management Science. 1992

ISBN 978-0-7923-1664-0

Set (23­25) ISBN 978-0-7923-1666-4

26. P. Fisher: Rational Expectations in Macroeconomic Models. 1992

ISBN 978-0-7923-1903-0

27. L. Phlips and L.D. Taylor (eds.): Aggregation, Consumption and Trade. Essays in Honor

of H.S. Houthakker. 1992

ISBN 978-0-7923-2001-2

28. L. Matyas and P. Sevestre (eds.): The Econometrics of Panel Data. Handbook of Theory

and Applications. 1992

ISBN 978-0-7923-2043-2

29. S. Selvanathan: A System-Wide Analysis of International Consumption

Patterns. 1993

ISBN 978-0-7923-2344-0

30. H. Theil in association with D. Chen, K. Clements and C. Moss: Studies in Global

Econometrics. 1996

ISBN 978-0-7923-3660-0

31. P.J. Kehoe and T.J. Kehoe (eds.): Modeling North American Economic

Integration. 1995

ISBN 978-0-7923-3751-5

32. C. Wells: The Kalman Filter in Finance. 1996

ISBN 978-0-7923-3771-3

33. L. Matyas and P. Sevestre (eds.): The Econometrics of Panel Data. A Handbook of the

Theory with Applications. 2nd Revised ed. 1996

ISBN 978-0-7923-3787-4

34. B. Assarsson, D. Edgerton, A Hummelmose, I Laurila, K. Rickertson and P.H. Vale:

The Econometrics of Demand Systems. With Applications to Food Demand in the North

Countries. 1996

ISBN 978-0-7923-4106-2

35. D.A. Griffith, C.G. Amrhein and J-M. Huriot (eds.): Econometric Advances in Spatial

Modelling and Methodology. Essays in Honour of Jean Paelinck. 1998

ISBN 978-0-7923-4915-0

36. R.D.H. Heijmans, D.S. G. Pollock and A. Satorra (eds.): Innovations in Multivariate

Statistical Analysis. 2000

ISBN 978-0-7923-8636-0

37. R. MacDonald and I. Marsh: Exchange Rate Modelling. 2000

ISBN 978-0-7923-8668-1

38. L. Bauwens and P. Giot: Econometric Modelling of Stock Market Intraday

Activity. 2001

ISBN 978-0-7923-7424-4

39. J. Marquez: Estimating Trade Elasticities. 2002

ISBN 978-1-4020-7159-1

40. R. Bhar and S. Hamori: Hidden Markov Models. Applications to Financial Economics.

2004

ISBN 978-1-4020-7899-6

41. P. Tryfos: The Measurement of Economic Relationships. 2004

ISBN 978-1-4020-2838-0

Advanced Studies in Theoretical and Applied Econometrics

42. M.O. Haque: Income Elasticity and Economic Development. Methods and

Applications. 2005

ISBN 978-0-387-24292-7

43. I.F. Razafimahefa and S. Hamori: International Competitiveness in Africa. Policy Impli-

cations in the Sub-Saharan Region. 2007

ISBN 978-3-540-68920-1

44. C. G. Renfro: The Practice of Econometrics Theory. 2008 Forthcoming

ISBN 978-3-540-75570-8

46. L. Ma´tya´s and P. Sevestre: The Econometrics of Panel Data. Fundamentals and Recent

Developments in Theory and Practice. 3rd ed. 2008

ISBN 978-3-540-75889-1

springer.com

