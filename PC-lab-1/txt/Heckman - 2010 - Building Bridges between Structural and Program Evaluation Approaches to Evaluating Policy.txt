Journal of Economic Literature 48 (June 2010): 2, 356–398
http:www.aeaweb.org/articles.php?doi=10.1257/jel.48.2.356

Building Bridges Between Structural
and Program Evaluation Approaches
to Evaluating Policy
James J. Heckman*
This paper compares the structural approach to economic policy analysis with the pro­
gram evaluation approach. It offers a third way to do policy analysis that combines the
best features of both approaches. I illustrate the value of this alternative approach by
making the implicit economics of LATE explicit, thereby extending the interpretability
and range of policy questions that LATE can answer. (JEL C21, E61)

1.

conflicts between “structural” and “reduced
form” approaches.2 In current usage, “structural” is taken to mean parametric, explicitly
formulated, empirical economic models.
“Reduced form” has multiple and sometimes conflicting meanings. One strand of the

Introduction

F

ew topics in economics evoke more
­ assion than discussions about the corp
rect way to do empirical policy analysis.1
These discussions are sometimes framed as

1 See the essays in the symposium “Con out of
Economics,” Journal of Economic Perspectives, Vol. 24, No.
2, Spring 2010 (Joshua D. Angrist and Jorn-Steffen Pischke
2010; Liran Einav and Jonathan D. Levin 2010; Michael
P. Keane 2010; Edward E. Leamer 2010; Aviv Nevo and
Michael D. Whinston 2010; Christopher A. Sims 2010;
James H. Stock 2010).
2 James J. Heckman and Edward J. Vytlacil (2007a)
­discuss the concept of structure and reduced form as defined
by the pioneering Cowles Commission econometricians who
developed the first rigorous framework for inference and
policy analysis. This concept received its clearest statement
in a classic paper by Leonid Hurwicz (1962). A structural
relationship in its original usage is a relationship invariant to
a class of policy interventions and can be used to make valid
policy forecasts for policies in that class. The explicit parametrizations used in the modern version of the “structural”
literature are intended to represent policy invariant parameters. Reduced forms are one representation of a structure
that represent endogenous variables in terms of exogenous
variables. Current meanings of “structure” and “reduced
form” have changed greatly from their original meanings, but
that is not the point of this essay.

* University of Chicago, University College Dublin,

Cowles Foundation, Yale University, and the American
Bar Foundation. I thank Ed Vytlacil for numerous discussions on the topics discussed in this paper over the years.
I also thank him for his comments on this paper. I thank
Philipp Eisenhauer, Miriam Gensowski, Tim Kautz,
Rodrigo Pinto, Steve Stern, and Ben Williams for helpful
comments. I draw on comments and suggestions made on
precursors to this paper by Steve Durlauf, Lars Hansen,
Richard Robb, and Jose Scheinkman, although none have
read this version. The precursor papers are Heckman
(1992, 1997, 2000, 2008). I thank Philipp Eisenhauer for
his help with constructing the figures in this paper. I thank
students in Econ 373 at the University of Chicago, Winter
2010, for serving as guinea pigs for testing the arguments
of this paper. Heckman thanks the National Institutes
of Health (R01-HD054702), the JB and MK Pritzker
Family Foundation, and the American Bar Foundation.
Heckman also thanks the Cowles Foundation at Yale University, which supported a visit that facilitated completion
of this research. Supplementary material is presented at
http://jenni.uchicago.edu/building_bridges /.

356

Heckman: Building Bridges
reduced form approach uses explicit economic models to motivate and interpret
empirical analyses and approximates the
economic models using simple econometric techniques. Arnold C. Harberger (1964),
Robert Shimer and Ivan Werning (2008), Raj
Chetty (2009), and Einav, Amy Finkelstein,
and Mark R. Cullen (2009) are good examples of this approach. Chetty (2009) surveys
a large literature in this tradition.
Another strand is the “program evaluation”
approach surveyed in Guido W. Imbens and
Jeffrey M. Wooldridge (2009). This approach
focuses on “effects” defined by experiments
or surrogates for experiments as the objects
of interest, and not the parameters of explicit
economic models. It often leaves implicit the
economics and policy relevance of the output from its procedures. This paper compares structural and program evaluation
approaches—the contrast between the eco­
nomic parameters featured in the structural
approach and the “effects” featured in the
program evaluation approach.
Explicit structural models facilitate policy
analysis. However, after sixty years of experience with fitting explicit structural models on
a variety of data sources, empirical economists
have come to appreciate the practical difficulties that arise in identifying, and precisely
estimating, the parameters of fully specified
structural models that can answer a wide variety of policy questions. There have been many
demonstrations of the sensitivity of estimates
of structural models to assumptions about
functional forms and distributions of unobservables. Ta-Chung Liu (1960), David F. Hendry
(1980), Sims (1980), and Leamer (1983) gave
early warnings about the fragility of standard
econometric estimates of explicit economic
models. Mark R. Killingsworth and Heckman
(1986) and John H. Pencavel (1986) summarize structural estimates of the effects of taxes
and wages on labor supply and report estimates
from the literature that are sometimes absurd.
H. Gregg Lewis (1986) reports the ­sensitivity

357

of structural estimates of the causal effect of
unionism on wages to the use of alternative
methodologies and reports many estimates
that are incredibly large. An influential paper
by Robert J. LaLonde (1986) is widely interpreted as having demonstrated that standard
structural estimation methods applied to nonexperimental data cannot duplicate the estimates obtained from a job training experiment.
These and other studies reported in the
literature more than twenty years ago fueled
the flight of many empirical economists from
structural models, even though Heckman
and V. Joseph Hotz (1989) cautioned that
many applications of the structural approach
by those comparing structural estimates
with experimental estimates did not perform
specification tests to see if the estimated
structural models were concordant with the
preprogram data. They show that when such
tests are performed, the surviving structural
models closely match the estimates produced
from the experiment analyzed by LaLonde,
findings duplicated for other experiments
(see Petra E. Todd and Kenneth I. Wolpin
2006, Orazio Attanasio, Costas Meghir, and
Ana Santiago 2009, and the discussion in
Keane, Todd, and Wolpin forthcoming).
The perceived failures of the structural
methods of the 1970s and 1980s produced
two different methodological responses.3
One response—the “program evaluation”
approach—was a retreat to statistics, away
from the use of explicit economic models either
in formulating economic policy questions
or in suggesting frameworks for ­estimating
­models to answer such questions. The second
3 Throughout this essay, I consider methodologies
that conduct primary empirical analyses. I do not discuss
calibration. Practitioners of the calibration approach use
well-posed economic models but typically use estimates
of key parameters taken from the literature with all of the
attendant problems that the parameters utilized are not
necessarily appropriate for the model being calibrated.
For discussions of calibration, see Lars Peter Hansen
and Heckman (1996), Finn E. Kydland and Edward C.
Prescott (1996), and Sims (1996).

358

Journal of Economic Literature, Vol. XLVIII (June 2010)

response was development of a more robust
version of the structural approach.
The program evaluation approach replaces
the traditional paradigm of economic policy
evaluation with the paradigm of the randomized controlled trial. In place of economic
models of counterfactuals, practitioners of
this approach embrace a statistical model of
experiments due to Jerzy Neyman (1923) and
David R. Cox (1958) that was popularized
by Donald B. Rubin (1974, 1978, 1986), and
Paul W. Holland (1986). In this approach, the
parameters of interest are defined as summaries of the outputs of experimental interventions. This is more than just a metaphorical
usage. Rubin and Holland argue that causal
effects are defined only if an experiment can
be performed.4 This conflation of the separate tasks of defining causality and identifying causal parameters from data is a signature
feature of the program evaluation approach.
It is the consequence of the absence of clearly
formulated economic models. The probability
limits of estimators, and not the parameters of
well-defined economic models, are often used
to define causal effects or policy effects.
The retreat to statistics in the program evaluation literature left a lot of economics behind.
A big loss was the abandonment of economic
choice theory. Important distinctions about ex
ante and ex post outcomes and subjective and
objective evaluations that are central to structural econometrics were forgotten.
The influence of the program evaluation
approach is widespread. It is now commonplace for many empirical economists to use
the language of “treatment” and “control” to
describe the comparisons made in empirical
policy studies.
The structural response to the perceived failures of the 1970s and 1980s structural models
has focused on nonparametric ­identification
and estimation of well-posed ­economic
4 For a recent statement of this position, see Richard
Berk, Azusa Li, and Laura J. Hickman (2005).

­ odels within which to conduct policy analym
ses. This line of work preserves the goals of the
Cowles Commission pioneers of the structural
approach—to estimate models that can make
forecasts for a range of widely different policies and criteria. It is more explicit than the
program evaluation approach in articulating
economic models. It embraces developments
in dynamic economics, game theory, auction
theory, and the theory of mechanism design.
The richness of the theoretical models (in contrast to the intuitive “effects” promoted in the
program evaluation literature) make the fruit
of this approach more intellectually interesting. It also produces estimates that cumulate
across studies.5
On the negative side of the ledger on
structural estimation, the often complex
computational methods that are required to
implement this approach make it less transparent. Replication and sensitivity analyses
are often more difficult in this approach
than in the program evaluation approach.
Economists advocating the program evaluation approach dismiss the structural
approach as overly complex and not “credible,” focusing on the statistical and computational properties of estimators as the
measure of the credibility of procedures
(see, e.g., Angrist and Pischke 2008, 2010).6
An important paper by Jacob Marschak
(1953) suggests a middle ground between
these two camps, and is a motivation for the
present paper. Writing in the early 1950s

5 Rosa L. Matzkin (2007) provides a valuable overview
of the literature. See also Daniel Ackerberg et al. (2007)
and Susan Athey and Philip A. Haile (2007) for developments in IO and auction theory respectively. See Jaap H.
Abbring and Heckman (2007) for discussion of identification of models of dynamic discrete choice and Victor
Aguirregabiria and Pedro Mira (2010) for a survey of computational methods.
6 For brevity, in this paper my emphasis is on microeconometric approaches. There are parallel developments
and dichotomies in the macro time series and policy evaluation literatures. See Heckman (2000) for a discussion of
that literature.

Heckman: Building Bridges
during the first wave of structural econometrics, Marschak noted that for many problems of policy analysis, it is not necessary to
identify or estimate fully specified structural
models—the goal of structural analysis as
conceived by the Cowles pioneers and successor generations of structural economists.
Marschak’s Maxim suggests that economists
should solve well-posed economic problems with minimal assumptions. All that is
required to conduct many policy analyses
or to answer many well-posed economic
questions are policy invariant combinations
of the structural parameters that are often
much easier to identify than the individual
parameters themselves and that do not
require knowledge of individual structural
parameters. This approach advocates transparency and empirical robustness as does
the program evaluation approach, but it also
focuses attention on answering clearly stated
economic and policy questions.
This approach is often less computationally
intensive and focuses on a more limited range
of policy questions than the very large range of
policy questions contemplated by the Cowles
pioneers. The computationally less demanding models, more transparent sources of identifiability and the relative ease of performing
replication and sensitivity analyses give credibility to this approach. At the same time, this
approach improves on the program evaluation
approach by producing estimates that have
clear economic and policy relevance.
The plan of this paper is as follows. In the
next section, I review the range of questions
that arise in evaluating economic policies
and how the program evaluation approach
and the structural approach address them. I
use the Roy model (1951) and its extensions
as examples of widely used and prototypical
structural models. In the following section,
I apply Marschak’s Maxim to the analysis of
the Roy model. This produces an empirical approach that simplifies policy analysis for a certain class of policies. It links the

359

Roy model to the Local Average Treatment
(LATE) framework of Imbens and Angrist
(1994). This approach facilitates the interpretation of LATE and extends the range of
questions LATE answers. The final section of
the paper summarizes the argument.
This paper is not a position piece for or
against any particular statistical methodology. It is about the interpretability of estimates and their policy relevance. It is an
exploration of how to get the most out of the
data using economics to define the questions
of interest and statistics to help answer them.
2. The Structural versus the Program
Evaluation Approach to Evaluating
Economic Policies
Policy analysis is all about identifying counterfactual states. Counterfactual policy states
are possible outcomes in different hypothetical states of the world. An example is the set
of outcomes of the same persons in different tax regimes. Causal comparisons entail
contrasts between outcomes in alternative
possible states holding factors other than
the features of the policy being analyzed the
same across the contrasts. Thus the person
subject to a particular policy is the same as
the person who is not, except for treatment
status and, possibly, the outcome associated
with treatment status.
The concept of causality at the individual
level is based on the notion of controlled variation—variation in treatment holding other
factors constant. This is Alfred Marshall’s
(1890) ceteris paribus clause that has been
the operational definition of causality in economics for over a century.7 It is distinct from
other notions of causality sometimes used in

7 See Heckman (2000) for a discussion of the intellectual history of causality in economics.

360

Journal of Economic Literature, Vol. XLVIII (June 2010)

economics that are based on prediction (e.g.,
Clive W. J. Granger, 1969, and Sims, 1972).8
There are two distinct tasks in causal inference and policy analysis: (a) Defining counterfactuals and (b) Identifying causal models
from data. Table 1 delineates the two distinct
problems.
The first task entails the application of
economic models. Models are descriptions
of hypothetical worlds obtained by varying—hypothetically—the factors determining outcomes. Models may be motivated by
empirical evidence, and they may crystalize
evidence. They are, however, abstract representations of the evidence with an internal
logic of their own.
The second task is inference from data. It
requires solving the identification problem
including solving practical problems of inference from empirical samples.9 Economists
sometimes differ over what constitutes
admissible data for examining any policy
question, what prior information should be
used and how the prior information should
be used. There are no sharp rules to settle
these differences.
Part of the controversy surrounding the
construction of policy counterfactuals for
evaluating policies is a consequence of analysts being unclear about the two distinct
tasks represented in table 1 and sometimes
confusing them. Particular methods of
estimation (e.g., randomization, matching
or instrumental variable estimation) have
become associated with the definition of
“causal parameters” because issues of definition and identification are often conflated.
8 Holland (1986) makes useful distinctions among commonly used definitions of causality. Nancy Cartwright
(2004) discusses a variety of definitions of causality from a
philosopher’s perspective.
9 Many econometricians, but not all, distinguish the task
of identification from the task of inference. In this distinction, identification is about recovering parameters from
population data distributions, where sampling variation is
not an issue, and inference is about properties of sampling
distributions.

The structural econometric approach to
policy evaluation separates these two tasks
and emphasizes the role of models in defining hypotheticals and causal effects. Some
statisticians reject the use of models in defining causality and seek an assumption-free
approach to causal inference and policy analysis (see, e.g., John W. Tukey, 1986).
Any estimator makes assumptions (often
implicit) about the behavior of the agents
being analyzed. For example, the ability
of a randomized controlled trial to identify
parameters of interest depends on assumptions about the behavior of the agents being
subject to randomization.10 The structural
approach is explicit about these assumptions.
The program evaluation approach is often
not. Some economists confuse the absence
of explicit statements of assumptions with
the absence of assumptions.11
The “causal models” advocated in the
program evaluation literature are motivated
by the experiment as an ideal. They do not
clearly specify the theoretical mechanisms
determining the set of possible counterfactual outcomes, how hypothetical counterfactuals are realized or how hypothetical
interventions are implemented except to
compare “randomized” with “nonrandomized” interventions. They focus on outcomes,
leaving the model for selecting outcomes
and the preferences of agents over expected
outcomes unspecified.
The emphasis on randomization or its
surrogates, like matching or instrumental
variables, rules out a variety of alternative
channels of identification of policy effects
from data. The emphasis on randomization
has practical consequences leading to the
conflation of Task 1 with Task 2 in table 1.
10 For example, risk-averse agents may not participate
in randomized controlled trials. For discussion of this and
other examples, see Heckman (1992) and Heckman and
Jeffrey A. Smith (1995).
11 Mark R. Rosenzweig and Wolpin (2000) present a
catalogue of examples of this practice.

Heckman: Building Bridges

361

Table 1

Two Distinct Tasks that Arise in the Analysis of Causal Models
Task

Description

Requirements

1

Defining the Set of Hypotheticals or
Counterfactuals

A Well-Specified Economic Theory

2

Identifying Causal Parameters from Data

Mathematical Analysis of Point or Set Identification Joined
with Estimation and Testing Theory

Since a randomized protocol is used to
define the parameters of interest, this practice sometimes leads to the confusion that
randomization is the only way—or at least
the best way—to identify causal parameters
from data.
The models in the program evaluation literature do not specify the sources of randomness generating variability among agents,
i.e., they do not specify why otherwise observationally identical people make different
choices. They do not distinguish what is in
the agent’s information set from what is in
the observing economist’s information set,
although the distinction is fundamental in
justifying the properties of any estimator
for solving selection and evaluation problems. They do not allow for interpersonal
interactions inside and outside of markets in
determining outcomes that are at the heart
of game theory, general equilibrium theory,
and models of social interactions and contagion (see, e.g., Aguirregabiria and Mira
2010; Donald J. Brown and Matzkin 1996;
William A. Brock and Steven N. Durlauf
2001; Durlauf and H. Peyton Young 2001;
Charles F. Manski 1993; Elie Tamer 2003).
The goal of the structural econometrics
literature, like the goal of all science, is to
understand the causal mechanisms producing effects so that one can use empirical
versions of models to forecast the effects of
interventions never previously experienced,

to calculate a variety of policy counterfactuals and to use theory to guide choices of estimators to interpret evidence and to cumulate
evidence across studies. These activities
require models for understanding “causes of
effects” in contrast to the program evaluation
literature that focuses only on the “effects of
causes” (Holland 1986).
Before turning to a specific comparison
of the two approaches, it is useful to review
the variety of questions that arise in policy
analysis. What are the economically interesting questions? The success or failure of any
methodology hinges on how well it answers
substantive policy questions. Thus, it is helpful to have a list in front of us to examine
which questions are addressed or ignored by
different approaches.
2.1 Policy Evaluation Problems and
Criteria of Interest
Three broad classes of policy evaluation
problems arise in economics. Policy evaluation problem one is:
P1 Evaluating the Impacts of Imple­
mented Interventions on Outcomes
Including Their Impacts on the WellBeing of the Treated and Society at
Large.
It is useful to distinguish objective or public
outcomes that can in principle be measured

362

Journal of Economic Literature, Vol. XLVIII (June 2010)

by all external observers from “subjective”
outcomes that are the evaluations of the
agents experiencing treatment (e.g., patients)
or the agents prescribing treatment (e.g.,
physicians). Objective outcomes are intrinsically ex post (“after the fact”) in nature. The
literature on program evaluation focuses on
ex post objective outcomes.
The structural literature studies both subjective and objective outcomes. Subjective
outcomes can be ex ante (“anticipated”)
or ex post. The outcome of a medical trial
produces both a cure rate and the pain and
suffering of the patient. Ex ante anticipated
pain and suffering from a procedure may be
different from ex post realized pain and suffering. A similar distinction arises in an analysis of the returns to schooling. Monetary
returns are only part of total benefits which
include important elements of psychic cost.12
Ex ante evaluations of outcomes by agents
may differ from their ex post evaluations.
Thus in advance of going to school, students may have expectations about rewards
and costs that differ from their realizations.
Expected rewards govern responses to market incentives. The impacts in P1 include
individual level or population level counterfactuals and their valuations. “Well-being” in
P1 includes the valuations of the outcomes
of interventions by the agents being analyzed or other parties (e.g., the parents of
the agent or “society” at large). They may be
ex ante or ex post, and both are of interest
in evaluating policy. Regret and anticipation
are important aspects of public approval of
policies.
P1 is the problem of internal validity. It
is the problem of identifying the impacts
of an intervention conducted in a given
­environment. Solving just this problem can
be a challenging task and good answers are
valuable. However, most economic policy
12 See, e.g., Gary S. Becker (1964) and Flavio Cunha,
Heckman, and Salvador Navarro (2005).

e­ valuation is conducted with an eye toward
the future and toward informing decisions
about new policies and applications of old
policies to new environments.
There is a second problem frequently
encountered in policy analysis.
P2 Forecasting the Impacts (Constructing
Counterfactual States) of Interventions
Implemented in One Environment in
Other Environments, including Impacts
on Well-Being.

This is the problem of external validity:
taking a treatment parameter or a set of
parameters identified in one environment to
another environment.13
The most ambitious problem is forecasting
the effects of a new policy, never previously
experienced:
P3 Forecasting the Impacts of Interventions
(Constructing Counterfactual States
Asso­ciated with Interventions) Never
Historically Experienced, including
Their Impacts on Well-Being.

P3 is a problem that economic policy analysts
have to solve daily. Structural econometrics
addresses this question. The program evaluation approach does not.
2.2 A Prototypical Economic Model for
Policy Evaluation
Abstract discussions of policy evaluation
problems become very tedious very fast.
To be specific and to keep the discussion
focused, consider the following version of
the Roy model, which is a useful framework
for policy evaluation.14 The Roy model and
13 See, e.g., Donald T. Campbell and Julian C. Stanley
(1963), Heckman, LaLonde, and Smith (1999), or William
R. Shadish, Thomas D. Cook, and Campbell (2002).
14 Heckman (2008) and Heckman and Vytlacil
(2007a, 2007b) present general discussions of the policy
evaluation problem.

Heckman: Building Bridges
its extensions undergird a huge literature
in microeconometrics.15 It was applications
of the Roy model that fueled the flight from
structural econometrics in the 1980s.
The Roy model is a model of hypothetical outcomes. Economic theory defines the
space of possible counterfactual outcomes.
It also specifies agent decision rules. It executes task 1 of table 1.
A. D. Roy (1951) considered an economy where agents face two potential outcomes (​Y0​ ​,​ Y1​ ​) with distribution ​F​Y​  0​ ​,​ ​ Y​1(​
​ y0​ ​, ​y​1​)
where “0” refers to the no treatment state
and “1” refers to the treated state and (​y​0​, ​
y​1​) are particular values of random variables
(​Y0​ ​,​ Y1​ ​). More generally, the set of potential
outcomes is {​Ys​​​}​s∈​where  is the set of indices of potential outcomes. In the Roy model,
 = {0, 1}. I focus on two outcome models
to simplify the exposition. In the application
of Reuben Gronau (1974) and Heckman
(1974), ​Y0​ ​  is the value of ­nonmarket time,
and ​Y1​ ​ is the value of market time. In Robert
J. Willis and Sherwin Rosen (1979), ​Y0​ ​ is the
present value of high school earnings, and ​
Y​ 1​  is the present value of college earnings.
In the application of Insan Tunali (2000), ​Y0​ ​ 
and ​Y1​ ​ are incomes in two regions. The central question recognized in this literature is
that analysts observe either ​Y0​ ​ or ​Y1​ ​, but not
both, for any person. In the program evaluation literature, this is called the evaluation
problem.
In addition to this problem, there is the
selection problem. The values of ​Y0​ ​  or ​Y1​ ​ 
that are observed are not necessarily a random sample of the potential ​Y0​ ​ or ​Y1​ ​ distributions. In the original Roy model, an agent
selects into sector 1 if ​Y1​ ​ > ​Y0​ ​. Let D be an
indicator or dummy variable. Then
(1)

D = 1(​Y1​ ​  > ​Y​ 0​),

15 See Heckman (2001) and Heckman and Vytlacil
(2007a, 2007b) for surveys. See also Francis Vella (1998)
and Keane, Todd, and Wolpin (forthcoming).

363

where “1” is a function that is 1 if the
­condition in the argument holds and is 0
otherwise.
A variety of more general decision rules
have been considered in the structural literature. A simple generalization of the Roy
model adds cost C. This can be thought of as
a cost of moving from “0” to “1,” e.g., tuition
in the Willis–Rosen model, costs of work in
the Gronau–Heckman model, or costs of
migration in the Tunali model. The decision
rule becomes
(2)

D = 1(​Y1​ ​  − ​Y​ 0​  − C > 0).

This framework defines a set of counterfactual outcomes and costs (​Y0​ ​,​ Y1​ ​, C)
with distribution ​F​​Y​ ​,​Y​ ​,C​(​y0​ ​, ​y​1​, c) and a
0 1
mechanism for selecting which element of
(​Y0​ ​, ​Y1​ ​) is observed for any person. The outcome observed for any person, Y, can be
written as
(3)

Y = D​Y1​ ​  + (1 − D)​Y0​ ​.

This representation is the Richard E.
Quandt (1958, 1972) switching regression
framework.
Agents may make their choices under
imperfect information. Let  denote the
agent’s information. In advance of participation, the agent may be uncertain about all
components of (​Y0​ ​,​ Y1​ ​, C). The expected benefit is ​I​D​= E(​Y1​ ​ − ​Y0​ ​ − C |    ). Then
(4)

D = 1(​ID​ ​ > 0).

Moreover, the decision maker selecting
“treatment” may be different than the person who experiences the outcomes (​Y0​ ​,​ Y1​ ​).
Thus parents may make schooling decisions
for their children; doctors may make treatment decisions for patients. More ­generally,

364

Journal of Economic Literature, Vol. XLVIII (June 2010)

­decisions to participate may entail joint
approval of all parties.16
The ex post objective outcomes are
(​Y0​ ​,​ Y1​ ​). The ex ante outcomes are E(​Y0​ ​ |   )
and E(​Y1​ ​ |    ). The ex ante subjective evaluation is ​ID​ ​. The ex post subjective evaluation is ​Y1​ ​ − ​Y0​ ​ − C. Agents may regret their
choices because realizations may differ from
anticipations.
The ex ante versus ex post distinction is
essential for understanding behavior. In
environments of uncertainty, agent choices
are made in terms of ex ante calculations. Yet
the treatment effect literature largely reports
ex post returns. For example, the recent literature on the returns to schooling reports
ex post returns (David H. Autor, Lawrence
F. Katz, and Melissa S. Kearney 2005; Katz
and Autor 1999; Katz and Kevin M. Murphy
1992). Yet it is the analysis of ex ante returns
that is needed to understand why, over time,
responses to increases in ex post returns to
schooling have been so sluggish.17
Pedro Carneiro, Karsten T. Hansen, and
Heckman (2001, 2003), Cunha, Heckman,
and Navarro (2005, 2006) and Cunha and
Heckman (2007) develop econometric
methods for distinguishing ex ante from ex
post evaluations of social programs. Abbring
and Heckman (2007) provide an extensive
survey of this literature.18
In the language of the program evaluation literature, ​Y1​ ​ − ​Y0​ ​ is the individual level
treatment effect. It is also the Marshallian
ceteris paribus causal effect. Because of the
evaluation problem, it is generally impossible to identify individual level treatment
16 See Dale J. Poirier (1980) and Henry S. Farber (1983).
17 The econometrician may possess a different infor-

mation set, ​e​. Choice probabilities computed against
one information set are not generally the same as those
computed against another information set. Operating
with hindsight, the econometrician may be privy to some
information not available to agents when they make their
choices.
18 Manski (2004) surveys a rich literature on the elicitation of expectations.

effects (Task 2). Even if it were possible, ​
Y​ 1​ − ​Y0​ ​  does not reveal the ex ante subjective evaluation ​ID​ ​ or the ex post assessment ​
Y​ 1​ − ​Y0​ ​ − C.
Economic policies can operate through
changing (​Y0​ ​,​ Y1​ ​) or through changing C.
Thus, in the college going example, policies
may reduce tuition costs or reduce commuting
costs (David Card 2001). They may tax future
earnings. The structural approach considers
policies affecting both returns and costs.19
2.3 Population Parameters of Interest
Because it is generally impossible to identify
individual-level treatment effects, analysts
typically seek to identify parameters defined
at the population level. Conventional parameters include the Average Treatment Effect
(ATE = E(​Y1​ ​ − ​Y0​ ​)), the effect of Treatment
on The Treated (T T = E(​Y1​ ​ − ​Y0​ ​ | D = 1)),
or the effect of Treatment on the Untreated
(TUT = E(​Y1​ ​ − ​Y0​ ​ | D = 0)).
However, in addressing economic policy evaluation questions, a variety of other
population level parameters are often more
interesting. In positive political economy,
the fraction of the population that perceives
a benefit from treatment is of interest. This is
called the voting criterion and is
Pr (​I​D​ > 0) = Pr (E(​Y1​ ​ − ​Y0​ ​ − C |  ) > 0).
In gauging support for a policy in place, the
percentage of the population that ex post
perceives a benefit may also be of interest:
Pr (​Y1​ ​ − ​Y0​ ​ − C > 0).
More generally, for evaluation of the distribution of welfare, knowledge of the ex ante
and ex post joint distributions of outcomes are

19 See, e.g., Heckman, Lance Lochner, and Christopher
Taber (1998a, 1998b, 1998c), Esther Duflo (2004), Jeremy
Lise, Shannon Seitz, and Jeffrey Smith (2005, 2006), James
Albrecht, Gerard Van den Berg, and Susan Vroman (2009),
or Donghoon Lee and Wolpin (2006).

Heckman: Building Bridges
of interest.20 Because of the evaluation problem, it is very difficult to identify the joint distribution because we generally do not observe ​
Y​ 0​ and ​Y​ 1​ together. This problem plagues all
methodologies including social experiments.21
Determining marginal returns to a policy
is a central goal of economic analysis. In the
generalized Roy model, the margin is specified by people who are indifferent between
“1” and “0,” i.e., those for whom ​I​D​= 0 . The
mean effect of treatment for those at the
margin of indifference is
E(​Y1​ ​ − ​Y0​ ​ | ​ID​ ​ = 0).
I discuss approaches for identifying this
parameter in section 3.
2.4 Treatment Effects versus Policy Effects
Different policies can affect treatment
choices and outcomes differently. Each of the
population-level treatment effects discussed
in the previous subsection can be defined for
different policy regimes. Economists are often
more interested in the effects of ­policies on outcomes than in conventional treatment effects.
To illustrate this point, consider the Policy
Relevant Treatment Effect (Heckman and
Vytlacil, 2001c) which extends the Average
Treatment Effect by accounting for voluntary
participation in programs. It is designed to
address problems P2 and P3. Let “b” represent a baseline policy (“  before”) and “a” represent a policy being evaluated (“after”). Let ​
Y a​ ​be the outcome under policy a, while ​Y​   b​ is
the outcome under the baseline. (​Y​ a0​ ​, ​Y​ a1​,​  ​Ca​ ​)
and (​Y​ b0​ ​, ​Y​ b1​ ​, ​Cb​ ​) are outcomes under the two
policy regimes.
Policy invariance facilitates the job of
answering problems P2 and P3. If some
20 See Heckman, Smith, and Nancy Clements (1997)
and Abbring and Heckman (2007) for discussions of these
parameters.
21 See Abbring and Heckman (2007) for discussions of
alternative approaches to identify or bounding these joint
distributions.

365

parameters are invariant to policy changes,
they can be safely transported to different policy environments. Structural econometricians
search for policy invariant “deep parameters”
that can be used to forecast policy changes.22
Under one commonly invoked form of
policy invariance, policies keep the potential outcomes unchanged for each person: ​Y​ a0​​ = ​Y​ b0​ ​, ​Y​ a1​​ = ​Y​ b1​ ​, but affect costs
(​Ca​ ​≠ ​Cb​ ​).23 A tuition policy in the absence
of general equilibrium effects is an example. Invariance of this type rules out social
effects including peer effects and general
equilibrium effects. Let ​D ​a​ and ​D ​b​ be the
choice taken under each policy regime.
Invoking invariance of potential outcomes,
the observed outcomes under each policy
regime are ​Y​  a​ = ​Y1​  ​​D​a​ + ​Y0​ ​(1 − ​D  ​a​) and ​Y​b​
= ​Y1​  ​​D​b​ + ​Y0​ ​(1 − ​D​b​). The Policy Relevant
Treatment Effect (PRTE) is
PRTE = E(​Y​a​ − ​Y  ​b​).
This is the Benthamite comparison of aggregate outcomes under policies “a” and “b”.24
PRTE extends ATE by recognizing that policies affect incentives to participate (C) but
do not force people to participate. Only if C
is very large under b and very small under a,
so there is universal nonparticipation under
b and universal participation under a, would
ATE and PRTE be the same parameter.
2.5 The Structural Approach versus the
Program Evaluation Approach
The recent literature on program evaluation in economics draws on a model of
counterfactuals and causality attributed to
22 Ragnar Frisch (1933, translated in 2009) considered these notions under the concept of “autonomy.” See
Marschak (1953) and Hurwicz (1962), who develop refinements of this concept.
23 Heckman and Vytlacil (2007a, 2007b) discuss a variety of invariance assumptions.
24 If potential outcomes are not policy invariant, one
would work with (​Y​ b0​ ​, ​Y​ b1​ ​) and (​Y​ a0​,​  ​Y​ a1​)​  .

366

Journal of Economic Literature, Vol. XLVIII (June 2010)

Rubin by Holland (1986).25 It defines causality using experimental manipulations and
thereby creates the impression in the minds
of many followers of this approach that random assignment is the most convincing way
to identify causal models.
Neyman and Rubin postulate counterfactuals {​Ys​ ​​}​s∈​  .26 They do not develop choice
mechanisms that determine which outcome
is selected or the subjective evaluations of
treatments. There is no discussion of the
mechanisms producing the outcomes studied or the relationship between outcomes
and choice mechanisms.
Rubin (1986) invokes a portion of the traditional econometric invariance assumptions
developed by Hurwicz (1962).27 Since he
does not develop models for choice or subjective evaluations, he does not consider the
more general invariance conditions for both
objective and subjective evaluations that
are features of the structural literature.28
The range of issues covered by the two
approaches is given in table 2.29
The Neyman–Rubin model does not
consider many issues discussed in structural econometrics. It is at best an incomplete introduction to some of the important
issues in evaluating social policies.30 The
analysis in Rubin’s 1974 and 1978 papers is
a dichotomy between randomization and

25 See,

e.g., Imbens and Wooldridge (2009).
26
 = {0, 1} in the Roy example.
27 He calls it “SUTVA” for Stable Unit Treatment Value
Assumption.
28 See, e.g., Heckman (2008) or Heckman and Vytlacil
(2007a, 2007b) for discussions of these conditions.
29 Not every paper in the empirical structural literature
addresses all of the issues in table 2 in deriving its estimates, but most papers in this tradition are explicit in noting which questions are not addressed.
30 It is a mark of their detachment from economics that advocates of the program evaluation approach in
economics claim that Marshall’s ceteris paribus concept
(Marshall 1890) for defining causality was developed in a
1974 paper by Rubin, and that they also attribute versions
of the Cowles Commission policy invariance assumptions
to Rubin.

­ onrandomization, and not an explicit treatn
ment of particular selection mechanisms in
the nonrandomized case as is developed in
the structural literature.
The statisticians who have had the greatest impact on the program evaluation literature in economics conflate the two tasks
stated in table 1. The discussion of Holland
(1986) illustrates this point and the central
role of the randomized controlled trial to the
Holland–Rubin analysis. After explicating
the “Rubin model,” Holland makes a very
revealing claim: there can be no causal effect
of gender on earnings because analysts cannot randomly assign gender. This statement
confuses the act of defining a causal effect
(a purely mental act performed within a
model) with empirical difficulties in estimating it.31 The local average treatment
effect “LATE” parameter of Imbens and
Angrist (1994), discussed in section 3, follows in this tradition and uses instrumental
variables as surrogates for randomization.
LATE is defined by an instrument and
conflates tasks 1 and 2 of table 1. In section 3, I present a framework that defines
the LATE parameter within the generalized
Roy model discussed in subsection 2.2 that
separates issues of definition of parameters
from issues of identification.
2.6 Identifying Policy Parameters
The structural approach to policy evaluation addresses policy evaluation questions

31 As another example of the same point, Rubin denies
that it is possible to define a causal effect of sex on intelligence because a randomization cannot in principle be performed. “Without treatment definitions that specify actions
to be performed on experimental units, I cannot unambiguously discuss causal effects of treatments” (Rubin 1978, p.
39). In this and many other passages in the statistics literature, a causal effect is defined by a randomization. Issues of
definition and identification are confused. This confusion
continues to flourish in the literature in applied statistics.
For example, Berk, Li, and Hickman (2005) echo Rubin
and Holland by insisting that if an experiment cannot “in
principle” be performed, a causal effect cannot be defined.

Heckman: Building Bridges

367

Table 2

Comparison of the Aspects of Evaluating Social Policies that are Covered by the
Neyman–Rubin Approach and the Structural Approach
Neyman–Rubin Framework
Yes
No (choice-mechanism
implicit)

Structural Framework
Yes
Yes

Models for the causes of potential outcomes
Ex ante versus ex post counterfactuals
Treatment assignment rules that recognize the
voluntary nature of participation

No
No
No

Yes
Yes
Yes

Social interactions, general equilibrium effects and
contagion

No (assumed away)

Yes (modeled)

Internal validity (problem P1)
External validity (problem P2)
Forecasting effects of new policies (problem P3)
Distributional treatment effects
Analyze relationship between outcomes and choice
equations

Yes
No
No
No a  
No (implicit)

Yes
Yes
Yes
Yes (for the general case)
Yes (explicit)

Counterfactuals for objective outcomes (Y0 , Y1)
Agent valuations of subjective outcomes (ID)

a

An exception is the special case of common ranks of individuals across counterfactual states: “rank invariance.”
See the discussion in Abbring and Heckman (2007).

P1–P3 by estimating models for ​Y0​ ​, ​Y1​ ​, and
C in different economic environments.
Commonly used specifications write
(5)

	​Y​ 1​  = ​μ​1​(X) + ​U​ 1​,

	​Y​ 0​  = ​μ​0​(X) + ​U​ 0​,
C = ​μ​C​(Z) + ​U​ C​,32
where (X, Z) are observed by the analyst, and ​
U​ 0​, ​U​ 1​, ​U​ C​are unobserved. Economic theory
specifies the ingredients in Z and X. In general,
there is no “objective” way to choose these conditioning variables. Any argument for inclusion
or exclusion of variables has to be made by an
appeal to theory—implicit or explicit.
32 μ​ ​(X) = E(​Y​ ​ | X);

​1

1

μ
​ ​0​(X) = E(​Y0​ ​ | X); ​μ​C​(Z) = E(C | Z).

To simplify notation, I define Z to include
all of X. Variables in Z not in X are instruments.
Write ​I​D​= E(​Y1​ ​ − ​Y0​ ​ − C |  ) = ​μ​D​(Z) − V
where ​μD​ ​(Z) = E(​μ1​ ​(X) − ​μ0​ ​(X) − ​μC​ ​(Z) |  )
and V = − E(​U1​ ​ − ​U0​ ​ − ​UC​  ​ |  ). In this notation, choice equation (3) can be expressed as
(6)

D = 1(​μD​ ​(Z) > V  ).

In the early literature that implemented
this approach, ​μ​0​(X), ​μ​1​(X), and ​μ​C​(Z) were
assumed to be linear in the parameters, and
the unobservables were assumed to be normal and distributed independently of X and
Z.
The caricature of the structural approach
in the recent program evaluation literature is
that linearity and normality are essential to
this approach. In truth, the essential aspect
of the structural approach is joint modeling

368

Journal of Economic Literature, Vol. XLVIII (June 2010)

of outcome and choice equations. Structural
econometricians have developed nonparametric identification analyses for the Roy
and generalized Roy models. See Heckman
and Bo E. Honoré (1990), Heckman (1990),
Hyungtaik Ahn and James L. Powell (1993),
Donald W. K. Andrews and Marcia M. A.
Schafgans (1998), and Mitali Das, Whitney
K. Newey, and Vella (2003). The field has
moved well beyond the parametric functional
forms used in the early papers that were the
targets of the 1980’s criticism. Traditional distributional and parametric assumptions are
relaxed in the recent structural econometric
literature. (See Ackerberg et al. 2007; Athey
and Haile   2007; Matzkin 1992, 1993, 1994,
2007, 2010a, 2010b; Powell 1994; Vella 1998;
Abbring and Heckman 2007; and Keane,
Todd, and Wolpin forthcoming; for reviews.)
As an illustration of the benefits of the
structural approach for solving policy problem P3, consider the analysis of college
choice by Stephen V. Cameron and Taber
(2004). Suppose that one seeks to know the
effects of increases in the expected gross
returns to college E((​Y1​ ​ − ​Y0​ ​) |  ) on college
choices. From equation (6), if one knows the
effects of variations in tuition (C) on college
choices, one can use the choice outcomes
associated with variations in C to accurately
predict the response to changes of equal
magnitude (in opposite sign) in expected
mean gross returns, even if returns to schooling have never varied in the past.
2.7 Marschak’s Maxim and the Relationship
Between the Structural Economics
Literature and the Program Evaluation
Literature: A Synthesis
Structural models make explicit the preferences and constraints that govern individual decisions, the rules of interaction among
agents in market and social settings, and the
sources of variability across agents. These
features facilitate finding answers to policy

questions P1–P3. They are absent from the
program evaluation literature.
At the same time, that literature makes
fewer statistical assumptions in terms of
independence, functional form, exclusion,
and distributional assumptions than the standard structural estimation literature in econometrics. This is an attractive feature of the
program evaluation approach.33 The greater
simplicity of its estimation schemes fosters
transparency, replicability, and sensitivity
analyses.34 While the structural economics literature has advanced greatly in recent years
in terms of producing a robust version of its
product, it is an unpleasant fact that fullyspecified structural models are often harder
to compute. It is more difficult to replicate
the estimates from them and to test the sensitivity of estimates to assumptions.
The two approaches can be reconciled by
noting that for many policy questions, it is
not necessary to identify fully specified models to answer a range of policy questions. It
is often sufficient to identify policy-invariant combinations of structural parameters.
These combinations are often much easier
to identify (i.e., require fewer and weaker
assumptions), and do not require knowledge
of the particular individual structural parameters that form the combination.
Marschak (1953) recognized that the
answers to many policy evaluation questions
do not require knowledge of all of the component parts of full structural models. I call
this principle Marschak’s Maxim in honor of
his insight. Consider estimating the marginal
effect of policy expansions. The traditional
structural approach identifies the component
parts of E(​Y​ 1​ − ​Y​ 0​ | ​ID​ ​= 0) constructed from
33 A recent exception to this robust approach is the analysis of Angrist and Pischke (2008) who claim that policy
evaluation models should be based on linear-in-parameter
estimating equations.
34 Manski (1995, 2003) has developed an elaborate
methodology for sensitivity analysis in the program evaluation literature for certain classes of data.

Heckman: Building Bridges
estimates of the parameters of equations (5)
and (6) and assembles them to estimate the
marginal effect of the policy expansion (see
Anders Björklund and Robert Moffitt 1987).
In the next section, I exposit an approach
that is consistent with Marschak’s Maxim
that directly identifies the combination of
parameters that define E(​Y1​ ​ − ​Y0​ ​ | ​ID​ ​= 0)
to solve policy problems, rather than identifying the component parts of the structural
model and building it up from the components. Marschak’s Maxim is an application of
Occam’s Razor to policy evaluation.35
3. Using Economics to Interpret What
LATE Estimates and to Make it Useful
for Evaluating a Broad Range of Policies
This section presents an example of an
approach to policy evaluation that implements Marschak’s Maxim in the context
of LATE. It makes the implicit economics
in LATE explicit and thereby expands the
range of policy questions that LATE can
address.
In the economic theory of policy evaluation, a comparison between marginal benefits
and marginal costs determines the optimal
size of social programs. For example, to evaluate the optimality of a policy that promotes
expansion in college attendance, analysts
need to estimate the return to college for the
marginal student, and compare it to the marginal cost of the policy. This task requires that
analysts identify marginal returns.
In the spirit of the program evaluation literature, in the following discussion, I ignore
general equilibrium effects, and I do not
emphasize the ex ante and ex post distinction. Both topics are addressed in many

35 Heckman and Richard Robb (1985, p. 190) apply a
version of Marschak’s Maxim to methods in program evaluation. Thus matching can identify the average treatment
effect or treatment on the untreated without identifying
the component parts of equations (5) and (6).

369

papers in the structural approach. To simplify the notation, I keep the conditioning
variables X implicit unless it clarifies matters
by making them explicit. I follow standard
conventions and denote random variables
by capital letters and their realizations by
the corresponding lower case letters. Thus
Z = z means that random variable Z takes
the value z. Z is a vector with K components,
Z = (​Z​1​, … , ​ZK​ ​). ​z​ j​ means a particular realization of Z, i.e., ​z​ j​= (​z​ j1​,​  … , ​z​ jK​)​  . I assume
for simplicity that all means are finite.
The equation for ex post outcome Y as a
function of participation status is
Y = α + β D + ε,

(7)

where D is a dummy variable indicating participation in a program, β is the individual
return to participation or treatment effect
and ε is an error term that is unobserved
by the analyst. Equation (7) is one representation of the Quandt switching regression
model (3). In terms of counterfactual notation, α = ​μ0​ ​, ε = ​U​0​  and ​Y0​ ​ = ​μ0​ ​  + ε, and
β = (​Y1​ ​ − ​Y0​ ​) =  ​μ1​ ​ − ​μ0​ ​  +  ​U1​ ​  −  ​U0​ ​.
Estimating marginal returns to a policy
that changes D is a relatively simple task if
the effect of the policy is the same for everyone (conditional on X). This is the case when ​
U​1​ − ​U​0​ = 0 , and ε = ​U0​ ​ = ​U1​ ​ . In this case,
the mean marginal and average returns are
the same for all people with the same X.
The recent literature on policy evaluation allows for the possibility that β varies
among people even after conditioning
on X.
__
Denoting the mean of β by ​β ​, the outcome
equation can be written as
_

_

(8) 	Y = α + ​β​D + {ε + (β − ​β​)D},
_

where ​β​ =  ​μ​1 ​ −  ​μ0​ ​.36 If β is uncorrelated
with D, the only new econometric problem

36  I

assume that the mean is finite: E | β | < ∞.

370

Journal of Economic Literature, Vol. XLVIII (June 2010)

that arises in the analysis of (8) that is not
present in the traditional analysis of (7) is that
the error term is heteroscedastic. As in the
case where β is a common parameter shared
by everyone with the same X, the main_ econometric problem for inference about ​β​is that
D is correlated with ε.37
β is statistically independent of D if,
given X, agents cannot anticipate their ex
post idiosyncratic
gains from participation
_
so β − ​β​ is independent of D because it is
not in the agent’s information set . Another
reason why β might be_independent of D is
that agents know β − ​β​, but do not act on
it in choosing D. In both cases, mean marginal returns are the same as mean average
returns. Under standard conditions, applica_
tion of instrumental variables identifies ​β​.38
One does not have to specify the model by
which D_ is selected. All valid instruments
identify ​β​.
The recent literature analyzes the less
conventional case where agents know and
make choices about D with at least partial
knowledge of β = ​Y1​ ​ − ​Y0​ ​, and the agent
knows more about ​Y1​ ​ − ​Y0​ ​  than what is in
the observing economists’ conditioning set
(X, Z). Instrumental
_ variables (IV) do not in
general estimate ​β​ and instrumental variables estimators using different instruments
have different probability limits.39 Structural
selection models can _estimate the distribution of β (and hence ​β​) and answer a range
of the public policy evaluation questions
discussed in section 2 but under assumptions that are held to be “incredible” in the

37 If this problem is solved, it is possible to estimate the
distribution of β (see, e.g., Heckman and Smith 1998). _
38 Matching and selection methods also identify ​β​
under their assumed conditions. Regression discontinuity methods are a local version of instrumental variables
(Jinyong Hahn, Todd, and Wilbert Van der Klaauw 2001;
Heckman and Vytlacil 2007b).
39 See Heckman and Robb (1985), p. 196; Heckman
(1997); Heckman, Daniel Schmierer, and Sergio Urzua
(forthcoming).

program evaluation literature.40 Angrist and
Pischke (2008, 2010) offer the Local Average
Treatment Effect (LATE) as a “credible”
alternative to structural methods.
Under the conditions reviewed in the next
subsection, Imbens and Angrist (1994) show
that instrumental variable estimators identify LATE, which measures the mean gross
return to treatment for individuals induced
into treatment by a change in an instrument.
The LATE parameter is widely interpreted
as estimating the mean return at the margin
defined by manipulation of the instrument.
_
In general, LATE is not the same as ​β​, but
it might be all that is needed to evaluate any
particular policy. The key question is “what
question does LATE answer?” Unfortunately,
the people induced to go into state 1 (D = 1)
by a change in any particular instrument need
not be the same as the people induced to go
to state 1 by policy changes other than those
corresponding exactly to the variation in the
instrument. A desired policy effect may not
directly correspond to the variation captured
by the IV. The people induced to change state
by the instrument are not ­identified in LATE.
Widely held intuitions about what IV identifies break down in this case since different
instruments identify different parameters.
Moreover, if there is a vector of instruments
that generates choices and the components
of the vector are intercorrelated, IV estimates
using the components of Z as instruments,
one at a time, do not, in general, identify the
policy effect corresponding to varying that
instrument, keeping all other instruments
fixed, the ceteris paribus effect of the change
in the instrument. Recent research that
builds on and improves LATE shows how
to use the generalized Roy model implicit in
LATE to estimate the mean marginal returns
to alternative ways of producing marginal
expansions of programs when variation in the
40 See Abbring and Heckman (2007) for a survey of
methods for estimating and bounding the distribution of β.

Heckman: Building Bridges
available instruments does not correspond
exactly to the variation induced by proposed
policies.41 This research also enables analysts
to determine the people who are affected by
changes in instruments. I first review LATE
and then consider recent extensions of it.
3.1 LATE
LATE is defined by the variation of an
instrument. The instrument in LATE plays
the role of a randomized assignment. Indeed,
randomized assignment is an instrument.42
Using the notation of section 2, ​Y0​ ​ and ​Y1​ ​ are
potential ex post outcomes. Instrument Z
assumes values in , z ∈ . D(z) is an indicator of hypothetical choice representing what
choice the individual would have made had
the individual’s Z been exogenously set to z.
D(z) = 1 if the person chooses (is assigned
to) 1. D(z) = 0 , otherwise. One can think of
the values of z as fixed by an experiment or
by some other mechanism independent of
(​Y0​ ​,​ Y1​ ​). All policies are assumed to operate
through their effects on Z. It is assumed that
Z can be varied conditional on X.
Imbens and Angrist (1994) make three
assumptions to define LATE. Their first
assumption is an instrumental variables
assumption formulated in terms of a model
of counterfactuals:
(IA–1)

​ ​)
(​Y0​ ​,​ Y1​ ​, {D(z)​}z∈

  ∥  

Z|X

where “  ∥  ” denotes independence, and
A   ∥   B | X means A is independent of B
​ ​) are ranconditional on X. (​Y1​ ​,​ Y0​ ​, {D(z)​}z∈
dom variables defined over the population.
Assumption (IA–1) states that the values of
potential outcomes and potential choices are
independent of Z (conditioning on X).

41

See Heckman and Vytlacil (2005, 2007a, 2007b),
Heckman, Urzua, and Vytlacil (2006), and Carneiro,
Heckman, and Vytlacil (2009, 2010).
42 Heckman (1996).

371

Imbens and Angrist also assume a rank
condition:
(IA–2) Pr(D = 1 | Z = z) is a nontrivial func­
tion of z conditional on X.
This says that the distribution of P(Z)
= Pr(D = 1 | Z) is nondegenerate conditional
on X.
To make IV identify a treatment effect,
they invoke a monotonicity condition on the
D(z) at the individual level.
(IA–3) For any two values of Z, say Z = ​z​1​ 
and Z = ​z​2​, either D(​z1​ ​) ≥ D(​z2​ ​) for all per­
sons, or D(​z1​ ​) ≤ D(​z2​ ​) for all persons.
This condition is a statement across people. ​
z​1​ and ​z2​ ​ are two different values of vector Z.
Fixing the instrument at two values ​z1​ ​  and ​
z​2​ moves choices across people in the same
direction (either in favor of 1 or against it).
This condition does not require that for any
other two values of Z, say ​z​3​ and ​z4​ ​, the direction of the inequalities on D(​z3​ ​) and D(​z4​ ​)
have to be ordered in the same direction as
they are for D(​z​1​) and D(​z2​ ​). It only requires
that the direction of the inequalities are the
same across people. Thus for any person,
D(z) need not be monotonic in z.43
Under these conditions, Imbens and
Angrist establish that for two distinct values
of Z, ​z1​ ​ and ​z2​ ​, IV applied to (7) identifies
LATE(​z​2​,​ z1​ ​)
= E(​Y1​ ​ − ​Y0​ ​ | D(​z2​ ​)  = 1, D(​z1​ ​) = 0),
if the change from ​z1​ ​  to ​z2​ ​  induces people
into the program (D(​z2​ ​) ≥ D(​z1​ ​)).44 This is
43 For this reason, Heckman, Urzua, and Vytlacil (2006)
call this condition “uniformity.”
44 This expression is easily modified to cover the opposite case where the change in Z reduces program participation. (IA–3) rules out both cases arising at the same time.

372

Journal of Economic Literature, Vol. XLVIII (June 2010)

the mean return to participation in the program for people induced to switch treatment
status by the change from ​z1​ ​ to ​z2​ ​.45
LATE does not identify which people
are induced to change their treatment status by the change in the instrument. It also
leaves unanswered many of the policy questions discussed in section 2. For example,
if a proposed program changes the same
components of vector Z as used to identify
LATE but at different values of Z (say ​z4​ ​, ​z3​ ​ ),
LATE(​z2​ ​, ​z​1​) does not identify LATE(​z​4​, ​z3​ ​).
If the policy operates on different components of Z than are used to identify LATE,
one cannot safely use LATE to identify marginal returns to the policy. LATE answers a
version of policy problem P1 for objective
outcomes, but ignores P2 and P3. It does
not, in general, identify treatment on the
treated, ATE or the other parameters discussed in section 2.
3.2 Making Explicit the Implicit Economics
of LATE
In a fundamental paper, Vytlacil (2002)
shows that the LATE model is equivalent
to a nonparametric version of the generalized Roy model. The Imbens–Angrist conditions imply the generalized Roy model,
and the generalized Roy model implies the
LATE model. Vytlacil’s analysis is the basis
for defining LATE abstractly within a wellposed economic model and separating the
task of definition (Task 1 of table 1) from
the task of identification (Task 2 of table 1).
Vytlacil’s analysis clarifies the implicit economic assumptions of LATE, what features of the generalized Roy model LATE
­estimates, and what policy questions LATE
addresses. It also extends the range of policy
questions that LATE can answer.
By Vytlacil’s theorem, the Imbens–Angrist
conditions imply (and are implied by) a con45

If “monotonicity” were not invoked, changes in Z
from ​z1​ to ​z2​ could induce two way flows.

tinuous latent variable discrete choice model,
which represents the individual’s decision to
enroll in the program being studied. Recall
that ​ID​ ​ (in equation (4)) is the net benefit to
the individual of enrolling in the program. A
person takes treatment D = 1 (e.g., goes to
college) if ​ID​ ​> 0; otherwise D = 0. Vytlacil
shows that the treatment choice equation
underlying LATE can be expressed in terms
of observed (Z) and unobserved (V  ) variables
that can be represented by equation (6):
​I​D​= ​μD​ ​(Z) − V and D = 1 if ​ID​ ​> 0; D = 0
otherwise, where V is a continuous random
variable with distribution function ​F​V​.46 ​
μ​D​(Z) is defined in the discussion preceding
equation (6). V may depend on ​U​0​ and ​U1​ ​ in
a general way.47
LATE assumes that (​U​0​,​ U1​ ​, V) are independent of Z given X. This relaxes the
independence assumption (between X and
the unobservables) that was frequently
maintained in the early structural literature. The counterfactual choice indicator
is generated by choice equation (6): D(z)
= 1(​μD​ ​(z)  > V). This representation makes
explicit the implicit random variable (V )
used to define D(z) in the analysis of Imbens
and Angrist, and the independence between
Z and V that is part of condition (IA–1).
The additive separability between ​μ​D​(Z)
and V in the latent index model (6) plays an
essential role in LATE. Model (6) is far from
the most general possible representation of
choices. If choice responses to variations in Z
are heterogeneous in a general way, the same
change in Z could lead some persons toward
and other persons away from participation in
the program, and the separability between
​μ​D​(Z) and V in (6) would break down.

46 Recall that I keep the X implicit, but it is implicitly
conditioned on throughout this paper.
47 In the original Roy model (1951), V = − (​U ​ − ​U ​).
1
0

Heckman: Building Bridges
Another way to say this is that monotonicity
condition (IA–3) would be violated.48
To understand the economic model
implicit in LATE, let P(z) denote the probability of taking treatment (e.g., attending college, D = 1) conditional on Z = z:
P(z) ≡ Pr(D = 1 | Z = z). From equation (6),
P(z) = Pr( ​μ​D​(z) > V  ) = ​FV​  (​ ​μD​ ​(z)). P(z) is a
monotonic transformation of the mean utility
function ​μD​ ​(z) in discrete choice theory. P(z)
is sometimes called the propensity score.
Define random variable ​U​D​= ​FV​ ​(V ),
which is uniformly distributed over the interval [0, 1] and thus the ​p  th​​quantile of ​UD​ ​is p,
i.e., the proportion of ​UD​ ​ that is p or lower.
Different values of ​UD​ ​ correspond to different quantiles of V. We can rewrite (6) using ​
F​ V(​ ​ μD​ ​(Z)) = P​( Z )​ so that
(9)

D = 1(P(Z) > ​U​D​).

From the estimated propensity score, one
can identify the ex ante net benefit ​I​D​ up
to scale. Thus, one can determine for each
value of Z = z, what proportion of people
perceive that they will benefit from the program and the intensity of their benefit. Using
the nonparametric identification analyses of
Stephen R. Cosslett (1983), Manski (1988),
Roger W. Klein and Richard H. Spady (1993),
and Matzkin (1992, 1993, 1994, 2007), one
can nonparametrically identify the distribution of V and the mean valuation ​μ​D​(Z) (up
to scale).49 Thus, from agent choices, one can
supplement the information in LATE and
ascertain ex ante subjective evaluations.
As a consequence of Vytlacil’s theorem,
the LATE assumptions imply the ­selection
model representation (i.e., the generalized Roy model) and using the selection
48 See Heckman, Urzua, and Vytlacil (2006) for a discussion of this case. An example is ​μD​(Z) = γ Z, where
γ varies among people so that the same change in Z can
produce differences among people in choice responses to
variations in Z in addition to the variation produced by V.
49 The scale is the standard deviation of V, ​σ ​ .
V

373

model representation, one can establish that
E(Y | Z = z) = E(Y | P(Z) = P(z)). Under the
LATE assumptions, Z enters the model only
through its effect on P(Z). This property is
called index sufficiency where P(Z) is the
index. It is a central property of the LATE
model.
As a consequence of Vytlacil’s theorem,
one can define LATE (​z​2​, ​z1​ ​) using the latent
variable ​U​D​ and the values taken by P(Z)
when Z = ​z​1​ and Z = ​z2​ ​. To do so, I use the
property that the Z enter the model only
through P(Z).
(10)

LATE(​z​2​, ​z1​ ​)
= E(​Y1​ ​ − ​Y0​ ​ | P(​z1​ ​) ≤ ​U​D​ ≤ P(​z2​ ​)).

This is the mean gross return to persons
whose ​UD​ ​ ∈ [P(​z1​ ​), P(​z2​ ​)].50
The LATE parameter can be defined
within the generalized Roy model, without reference to an instrument. Thus the
LATE produced by economic theory can be
expressed as
(11)

_

u​​ D​)
LATE(​​u ​D​ ​, ​​__
_

​u​​ D​ ≤ ​U​D​ ≤ ​​u ​​D​),
= E(​Y1​ ​ − ​Y0​ ​ | __
the mean gross return to persons whose ​
_
U​D​ ∈ [ ​​__
u​​ D​, ​​u ​D​ ​]. This is a theoretical construct
(task 1). Proceeding in this fashion, we separate task 1 of table 1 from task 2. A choice of
two values of Z (​z1​ ​ and ​z​2​) picks specific val_
ues of [ ​​__
u​​ D​, ​​u ​D​ ​] that identify the model-generated LATE from data (say Pr(D = 1 | Z = ​
2
z​1​) = ​p​1​  = ​​u
__​​ D​ and Pr(D = 1 | Z = ​z​ ​) = ​
_
p​2​ = ​​u ​D​ ​). This is task 2.

50 Because ​​U​  ​​ is a continuous random variable, the disD
tinction between strict and weak inequalities is irrelevant
in defining the expressions in section 3 of this paper.

Journal of Economic Literature, Vol. XLVIII (June 2010)

374

3.2.1 The Surplus from Treatment and the
Marginal Treatment Effect
Using Vytlacil’s theorem, it is possible to
understand more deeply what economic questions LATE answers. Toward that end, it is
useful to introduce the Marginal Treatment
Effect (MTE) and show how it can be used to
unify the literature on treatment effects and to
make explicit the economic content of LATE.
For P(Z) = p, the mean gross gain of moving from “0” to “1” for people with ​U​D​ less
than or equal to p is
(12) E(​Y1​ ​ − ​Y0​ ​ | P(Z) ≥ ​U​D​, P(Z) = p)
= E(​Y1​ ​ − ​Y0​ ​ | p ≥ ​U​D​)

= E(​Y1​ ​ − ​Y0​ ​ | ​μD​ ​(z) ≥ V).51
The first equality follows from the LATE
assumption that (​Y0​ ​, ​Y1​ ​) are independent
of the instruments Z (IA–1) and hence any
functions of Z. The second equality follows
from the definition of the propensity score.
The mean gross gain in the population (or
gross surplus S(  p)) that arises from participation in the program for people whose ​U​D​
is at or below p is the product of the gain
to people whose ​UD​ ​is at or below p and the
proportion of people whose ​UD​ ​is at or below
p: E(​Y1​ ​ − ​Y0​ ​ | p ≥ ​UD​ ​)p = S( p).
Using Vytlacil’s theorem, we can move
from the theory (task 1 of table 1) to the
data (task 2 of table 1) to identify the gross
surplus S( p). The mean of Y given P(Z) = p
depends on the gross surplus:
(13)

E(Y | P(Z) = p)

= E(​Y0​ ​ + 1( p ≥ ​UD​ ​)(​Y1​ ​ − ​Y0​ ​))

= E(​Y0​ ​) + E(​Y1​ ​ − ​Y0​ ​ | p ≥ ​U​D​) p.
8
S( p)

51 The

mean net gain is E(​Y1​ ​ − ​Y0​ ​ − C | p ≥ ​UD​ ​).

We can identify the left-hand side of (13)
for all values of p in the support of P(Z).52
This is task 2 in table 1. It is not necessary
to impose functional forms to obtain this
expression, and one can avoid one of the
criticisms directed against 1980’s structural
econometrics. The surplus can be defined
for all values of p ∈ [0, 1] whether or not the
model is identified.
If p is increased by a small change in z,
some people near the margin of indifference who chose not to participate in the
program would now choose to participate.
Small ­variations in p identify the mean marginal gross return to a policy expansion that
changes P(Z). Formally, the marginal increment in outcomes is
∂E(Y | P(Z) = p)
  
 ​
(14) ​ __
∂p
= E(​Y1​ ​ − ​Y0​ ​ | ​UD​ ​ = p)
∂S(  p) 53
 ​.
= ​ _
∂p

This is the mean marginal gross return to
treatment for persons indifferent between
participation in the program or not at mean
scale utility level p = ​UD​ ​, and it is also the
marginal change in the gross surplus. The
sample analogue of (14) is the local instrumental variable (LIV) estimator of Heckman
and Vytlacil (1999, 2005).54 Adopting a
nonparametric approach to estimating
52 The support of a random variable is the region where
it has positive density.
53 For any two random variables M, N with density
f (m, n) where m and n are realizations of M and N, where
N is a uniform random variable in the interval [0, 1],
∞
E(M | r ≥ N) Pr(r ≥ N) = ​∫−∞
​  ​m ​∫0r​ ​  ​fM,N(m, n) dm dn,
∞
where Pr(r > N) = ​∫−∞
​  ∫
​ 0r​ ​  ​fM,N(m, n) dm dn = ​∫0r​ ​ f​N(n) dn
= r, where fN(n) = 1. Thus

∂ [E(M | r > N) Pr(r > N)]

   
   = E(M | N = r).
 ​
​ __
∂r

In the expression in the text, M = ​Y1​ ​ − ​Y0​ ​ and N = ​UD​ ​.
54 Karim Chalak, Susanne Schennach, and Halbert
White (2010) develop the sampling properties of this estimator under general conditions.

Heckman: Building Bridges
E(Y | P(Z) = p) avoids extrapolation outside
of the sample support of P(Z) and produces
a data sensitive structural analysis.
A generalization of this parameter defined
for other points of evaluation of ​u​D​ is the
Marginal Treatment Effect (MTE):
MTE(​u​D​) ≡ E(​Y1​ ​ − ​Y0​ ​ | ​UD​ ​ = ​u​D​).
This parameter is very useful in understanding how to go from IV estimates to policy
effects and in interpreting the economics of
LATE.55 Recall that ​U​D​is a uniform random
variable in the interval [0,1], so that MTE
for different ​uD​ ​ values shows how the mean
gross returns to the program vary with different quantiles of the unobserved component
of the utility of participation, ​UD​ ​.
Expression (13) can be simplified to
(15)

E(Y | P(Z) = p)

∫

p

= E(​Y0​ ​) + ​ ​  ​ MTE(​uD​ ​) d​uD​ ​, ​
0
5

375

in the program. This is so because marginal
increases in P(z) at high levels of P(z) induce
those individuals with high ​UD​ ​ values into
treatment. This is a consequence of the
economic choice model (9). Those with low
values of ​U​D​ already participate in the program for low values of P(z) = p. A marginal
increase in P(z) starting from a high value
has no effect on the participation decision
of those with low values of ​U​D​. From LIV,
it is possible to identify returns at all quantiles of ​U​D​ within the support of the distribution of P(Z) to determine which persons
(identified by the quantile of the unobserved component of the desire to take the
treatment, ​U​D​) are induced to go into the
treatment (D = 1) by a marginal change in
P(z), i.e., analysts can define the margins of
choice traced out by variations in different
instruments as they shift P(z). This clarifies
what empirical versions of LATE identify
by showing that all instruments operate
through P(z), and variations around different levels of P(z) identify different stretches
of the MTE. I now develop this point.

S( p)

where S( p) = ​∫0p​ ​ MTE(​uD​ ​)​ d​uD​ ​  and  ∂S( p)/∂ p
= MTE( p). Figure 1 plots E(Y | P(Z) = p)
(Figure 1(a)) and its derivative (Figure 1(b))
using values derived from a model discussed
in Heckman and Vytlacil (2005). In this analysis, E(Y | P(Z) = p) increases at a diminishing rate in p, so MTE(​u​D​) is decreasing in ​
u​D​, i.e., there are diminishing returns to the
marginal entrants attracted into the program
by increasing P(z).
Notice from (14) that persons with larger
values of P(z) identify the return for those
with larger values of ​U​D​, i.e., values of ​UD​ ​
that make persons less likely to participate
55 MTE was introduced into the literature on policy
evaluation by Björklund and Moffitt (1987) and extended
in Heckman and Vytlacil (1999, 2001b, 2005, 2007b).

3.2.2. The Fundamental Role of the Choice
Probability in Understanding What
Instrumental Variables Estimate 		
When β Depends on D
For any two values of p, say ​p​1​  and ​
p​2​, ­generated by two different values of Z,
where ​p2​ ​ > ​p1​ ​,
S( ​p​2​) − S( ​p1​ ​)
= E(​Y1​ ​ − ​Y0​ ​ | ​p1​ ​ ≤ ​UD​ ​≤ ​p2​ ​)
× Pr( ​p1  
​ ​ ≤ ​ U​D​≤ ​p2​ ​)
= E(​Y1​ ​ − ​Y0​ ​ | ​p1​ ​ ≤ ​UD​ ​ ≤ ​p2​ ​)( ​p2​ ​ − ​p1​ ​),
where the last expression follows from the
fact that Pr( ​p1​ ​ ≤ ​UD​ ​≤ ​p2​ ​) = ​p2​ ​ − ​p1​ ​. Thus,

Journal of Economic Literature, Vol. XLVIII (June 2010)

376
25

0.4
0.35
0.3

15

0.25

MTE

E[Y|P(Z) = p]

20

10

0.2

0.15
0.1

5

0.05
0

0

0.1

0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

p

(a) Plot of the E(Y|P(Z) = p)

1

0

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

uD

1

(b) Plot of MTE (uD): The derivative of
E(Y|P(Z) = p) evaluated at points p = uD

Figure 1. Plots of E(Y | P(Z) = p) and the MTE Derived from E(Y | P(Z) = p)

Source: Heckman and Vytlacil (2005).

∫

​p2​ ​

S( ​p​2​) − S( ​p1​ ​) = ​ ​  ​ MTE(​uD​ ​) ​d​uD​ ​.
​p1​ ​

This expression can be obtained directly
from equation (15). From the definition of
LATE,
(16)

​ ​p​p​ 2​​ ​ ​​  ​MTE(​uD​ ​) d​uD​ ​
∫
__
  
  
 ​ 
LATE(​p​2​, ​p1​ ​) = ​  1
​p2​ ​ − ​p1​ ​
S( ​p2​ ​) − S( ​p1​ ​)
  
 ​.
= ​ __
​p2​ ​ − ​p1​ ​

Thus LATE is the chord of the gross surplus function over the interval [ ​p​1​, ​p2​ ​].
The model-generated LATE approximates
MTE(​u​D​) over an interval. By the mean value
theorem, LATE(​p​2​, ​p1​ ​) = MTE(​uD​ ​( ​p2​ ​, ​p1​ ​))
where ​u​D​(​p2​ ​, ​p1​ ​) is a point of evaluation and ​
u​D​( ​p2​ ​, ​p1​ ​) ∈ [ ​p1​ ​, ​p2​ ​]. The model-generated
LATE can be identified if there are values
˜)
˜ and ​ z​​
​ ˜˜ , such that Pr(D = 1 | Z = ​ z​
of Z, say ​ z​
​ ˜˜ ) = ​p2​ ​. Under stan= ​p1​ ​ and Pr(D = 1 | Z = ​ z​​
dard regularity conditions

	​ lim
​ LATE( ​p2​ ​, ​p1​ ​) = MTE( ​p1​ ​).
​p​ ​→​p​ ​
2

1

If we partition the support of ​uD​ ​ into M
discrete and exhaustive intervals
​ ​), [​uD,1
​ ​, ​uD,2
​ ​), … , [​uD,M−1
​
​, ​uD,M
​ ​],
[​uD​ ,0​, ​uD,1
​ ​= 1 , we can define
where ​uD​ ,0​ = 0 and ​uD,M
​ ​) = E(​Y0​ ​)
E(Y | ​UD​ ​≤ ​uD,k
k

+ ​∑​  ​
​  LATE(​uD,j
​ ​, ​uD,
​ j−1​)η j ,
j=1

where η j = ​uD,j
​ ​− ​uD,
​ j−1​. Thus
M

​ LATE(​uD,j
​ ​,​uD,j−1
​ ​)η j,
(17) E(Y) = E(​Y0​ ​) + ​∑​  ​
j=1

which is the counterpart to expression (15)
when p = 1. It shows how mean income can

Heckman: Building Bridges
be represented as a sum of incremental gross
surpluses above E(​Y0​ ​).
These expressions are derived from an
underlying theoretical model. Whether
or not the components can be identified
from the data depends on the support of
Pr(D = 1 | Z). If Pr(D = 1 | Z = z) assumes
values at only a discrete set of support points,
say ​p​1 ​ < ​p2​ ​ < ⋯ < ​pL​ ​, we can only identify
LATE in intervals with boundaries defined
by ​u​D,ℓ​= ​pℓ​​, ℓ = 1, … , L.
MTE(​uD​ ​) and the model-generated LATE
(10) are structural parameters in the sense
that changes in Z (conditional on X) do not
affect MTE(​uD​ ​) or theoretical LATE. They
are invariant with respect to all policy changes
that operate through Z. Conditional on X ,
one can transport MTE and the derived theoretical LATEs across different policy environments and different data sets. These policy
invariant parameters implement Marschak’s
Maxim since they are defined for combinations of the parameters of the generalized Roy
model. Instead of separately estimating the
components of the selection model presented
in section 2.6, one can identify an interpretable marginal gross benefit function by using
the derivative of E(Y | P(Z) = p).
This deeper understanding of LATE facilitates its use in answering out-of-­sample policy
question P3 for policies that operate through
changing Z. Thus if one computes a LATE
for any two pairs of values Z = ​z1​ ​, and Z = ​z2​ ​,
with associated probabilities Pr(D = 1 | Z = ​
z​1​) = P(​z1​ ​) = ​p1​ ​  and Pr(D = 1 | Z = ​z2​ ​) =
P(​z​2​) = ​p​ 2​ , one can use it to evaluate any
˜ and ​ z​​
​ ˜˜ such that
other pair of policies ​ z​
˜)
Pr(D = 1 | Z = ​z1​ ​) = Pr(D = 1 | Z = ​ z​
and

= ​p​1​

Pr(D = 1 | Z = ​z​2​) = Pr(D = 1 | Z =  ​ z​​
​ ˜˜ )
= ​p​2​.

377

Thus, one can use an empirical LATE determined for one set of instrument configurations to identify outcomes for other sets of
instrument configurations that produce the
same ​p​1​  and ​p​2​, i.e., one can compare any
˜ {z | P(z) = ​p1​ ​} with
­policy described by ​ z​ ∈
​ ˜˜ ∈ {z | P(z) = ​p2​ ​} and not just
any policy ​ z​​ 
the policies associated with ​z​1​  and ​z​2​  that
identify the sample LATE. This is a powerful result and enables analysts to solve policy
evaluation question P3 to evaluate new policies never previously implemented if they
can be cast in terms of variations in P(Z) over
the empirical support on Z.56
Variation in different components of Z
produce variation in P(Z). Analysts can aggregate the variation in different components of
Z into the induced variation in P(Z) to trace
out MTE(​uD​ ​) over more of the support of ​u​D​
than would be possible using variation in any
particular component of Z. The structural
approach enables analysts to determine what
stretches of the MTE different instruments
identify and to determine the margin of ​U​D​
identified by the variation in an instrument.
Figure 2 reproduces the MTE displayed
in figure 1(b) on a different scale. Consider
values of instruments that are associated
with P(z) = ​p​2​  and P(z) = ​p​1​. They identify the MTE at a value of ​uD​ ​ in the interval ​
u​D​∈ [ ​p1​ ​, ​p2​ ​], as depicted in the graph. This
56   We only require Y observations for each value of
P(Z) = p for each p in the target population, not values
of Y for all Z. Assuming data on (Y, D, Z) triples, a completely nonparametric approach to identifying P(Z) would
require that all LATEs required to answer P3 would
already be identied in the sample used to address P1 and
identify P(Z), i.e., there is no distinction between P1 and
P3. However, one can imagine cases where the analyst has
access to a richer set of data on (D, Z) where data on Y are
not available. Using the index sufficiency property, analysts
can determine E(Y | Z = z*) = E(Y | P(Z) = P(z*)), even if
no Y is observed for a Z = z*, so long as there is some value
of Z = z** in the sample such that P(z*) = P(z**). Moreover,
if one adopts a parametric functional form for P(Z), one
can answer a much wider range of P3 questions. The same
is true if a nonparametric P(Z) is available from another
sample. Structural invariance would justify combination of
information across samples.

Journal of Economic Literature, Vol. XLVIII (June 2010)

378

Mean
marginal
gain

LATE ( p2 , p1)
MTE
LATE ( p4 , p3)

uD ( p4 , p3 )

uD (p2 , p1)

0

p1

p2

uD

p3

p4

1

Figure 2. MTE as a Function of ​uD​ ​: What Sections of the MTE Different Values of the Instruments and
­Different Instruments Approximate

is LATE( ​p​2​, ​p1​ ​). If there is continuous variation in Z, it could be used to trace out the
entire interval of MTE for P(Z) ∈ [ ​p1​ ​, ​p2​ ​]
using LIV. Independent of any instrument,
we can define the LATE and MTE using the
underlying economic model. In this fashion,
we separate the task of definition of parameters from the task of identifying them.
Instruments associated with higher values of P(Z), [ ​p​3​, ​p4​ ​], identify the LATE in
a different stretch of the MTE associated
with higher values of ​u​D​. Thus different
instruments can identify different parameters. Continuous instruments identify
entire stretches of the MTE while discrete
­instruments define the MTE at discrete
points of the support (i.e., the LATE associated with the interval defined by the values assumed by P(Z)). As a consequence

of Vytlacil’s Theorem, one can identify the
intervals of ​u​D​ implicit in using LATEs
formed using different instruments.
If the MTE does not depend on ​u​D​,
E(Y | P(Z) = p) = E(​Y0​ ​) + ( ​μ1​ ​ − ​μ0​ ​)p, and
all
__ instruments identify the same parameter: ​
β ​= ​μ1​ ​ − ​μ0​ ​. In this case, MTE is a flat line
parallel to the ​uD​ ​ axis. This is the case traditionally assumed in the analysis of instrumental variables.
A test of whether MTE(​uD​ ​) depends on ​uD​ ​,
or a test of nonlinearity of E(Y | P(Z) = p) in
p, is a test of whether different instruments
estimate the same parameter.57 The LATE
model and its extensions overturn the logic
57 See Heckman, Schmierer, and Urzua (forthcoming)
for a formal development of these tests and some Monte
Carlo evidence on their performance.

Heckman: Building Bridges
of the J. Durbin (1954)–De-Min Wu (1973)–
Jerry A. Hausman (1978) test for overidentification. Variability among the estimates
from IV estimators based on different instruments may have nothing to do with the validity of any particular instrument, but may just
depend on what stretch of the MTE they
approximate.
3.3 All Treatment Effects Are Weighted
Averages of the MTE
Using the economics implicit in LATE unifies and interprets the literature on treatment
effects. All of the conventional treatment
effects featured in the program evaluation
literature can be written as weighted averages of the MTE or the structural LATEs
where the weights can be estimated from the
data over the sample support of P(Z). Thus
for treatment effect e,
(18)

Treatment Effect(e)

    

∫

1

= ​ ​  ​ MTE(​uD​ ​)​he​ ​(​uD​ ​) ​d​uD​ ​,
0

e

where ​h​ ​(​uD​ ​) is a weighting function. For ATE, ​
h​e​(​uD​ ​) = 1 and ATE = ​∫01​ ​  ​MTE(​uD​ ​) d​uD​ ​.
Using the linearity of the integral, one can
always break (18) into a sum of components
​ ​),[​uD,1
​ ​, ​uD,2
​ ​), … ,
over the intervals [​u​D,0​, ​uD,1
​, ​uD,M
​ ​] to obtain
[​uD,M−1
​
Treatment Effect(e)
M

∫

= ​∑​  ​​
​
j=1

​u​ D, j​

​​ MTE(​uD​ ​) ​he​ ​(​uD​ ​) ​d​uD​ ​,

​uD,
​ j−1​

​ j−1​ > ​uD,
​ j−2​ > ⋯. By the
where ​u​D,   j​> ​uD,   
mean value theorem,58 we may express each
of the integrals in the sum as

∫
​

379

​uD,  
​ j​

​​ MTE(​uD​ ​) ​he​ ​(​uD​ ​) ​du
​ D​ ​

​uD,  
​ j−1​

∫

= ​h​ ​(​u​ *j,j−1​) ​
e

​uD,   
​ j​

​​ MTE(​uD​ ​) ​du
​ D​ ​

​uD,  
​ j−1​

​ [​uD,   
​ j−1​, ​uD,   
​ j​    ). From (16) we
for some ​u​ j*,  j−1​ ∈
obtain

∫

e

​ ​ ​(​u​ j,  *j−1​) ​
h

​u​D, j​

​​ MTE(​uD​ ​) ​du
​ D​ ​

​uD
​ ,   j−1​

​ j​ − ​uD,    
​ j−1​) LATE(​uD,    
​ j​, ​u​D,    j−1​).
= h
​ ​e​(​u​ *j,  j−1​)(​uD,    
Thus we may write expression (18) as
Treatment Effect(e)
M

e
​ LATE(​uD,
​ j​, ​uD,
​ j−1​)h​​ 
​​ ˜ ​(​u​ *j, j−1​),
= ​∑​  ​
j=1

˜ e​(​u​ * ​) = ​he​ ​(​u​ * ​)(​u​ ​ − ​u​ ​). In
where ​​ h​​
j, j−1
j, j−1
D, j
D, j−1
the special case of ATE, ​he​ ​(​u​ *j, j−1​) = 1 and
M

​ LATE(​uD,
​ j​, ​uD,
​ j−1​)​η j​​ ,
ATE = ​∑​  ​
j=1

where ​η  j​​= ​uD,
​ j​ − ​uD,
​ j−1​.
The Policy Relevant Treatment Effect
(PRTE), defined in section 2, is
E(Y |Alternative Policy a)
− E(Y | Baseline Policy b)
= E(​Y​  a​) − E(​Y  ​b​)

∫

1

​ ​(​uD​ ​) ​du
​ D​ ​,
= ​ ​  ​ MTE(​uD​ ​)​hPRTE
0

58 The mean value theorem for integrals states that,
under standard regularity conditions (continuity and
integrability),

​∫a​  ​   ​​G(t)ϕ(t) dt = G(t*) ​∫a​ ​  ​ϕ(t) dt, for t* ∈ [a, b].
b

b

where ​hPRTE
​ ​(​uD​ ​) = ​F​ bP​(​  ​uD​ ​) − ​F​ aP​(​  ​uD​ ​), and ​F​ bP​ ​ 
is the distribution of P(Z) under policy b, and ​
F​ aP​​  is the distribution of P(Z) under policy

Journal of Economic Literature, Vol. XLVIII (June 2010)

380

a.59 Using the mean value theorem, one can
generate a counterpart expression in terms
of LATEs.
The PRTE weights MTE(​uD​ ​) by the change
in the distribution of the probabilities of participation at different values of ​u​D​.60 Thus for
a typical MTE as graphed in figure 1(a), if a
policy shifts the distribution of participants
toward low ​uD​ ​ values, it generates a positive
PRTE, since MTE(​uD​ ​) is higher for low ​uD​ ​
values than it is for high ​u​D​values.
Notice that the same MTE(​uD​ ​) can be used
to evaluate the impacts of a variety of different policies. MTE(​uD​ ​) is a structural function
since it is invariant across policies that affect
the distribution of the P(Z) but not the distribution of the potential ex post outcomes. We
can evaluate the effect of new policies never
previously experienced if we can characterize
the distributions of P(Z) for those policies.
Table 3 displays the weights of the MTE
that produce the traditional treatment
parameters. All of the weights can be estimated from the distribution P(Z).61 There
are corresponding expressions for the case
of discrete support for P(Z) that can be
obtained using the mean value theorem. The
weights integrate to 1. When β is independent of D, MTE(​
​ ​) does not depend on ​
__uD
μ0​ ​ = ​β ​), so all treatment paramu​D​(= ​μ1​ ​ − ​__
eters equal ​β ​.
Figure 3 plots an MTE taken from the
analysis of Heckman and Vytlacil (2005) and
the weights for MTE associated with ATE,
59

2.3,

Assuming policy invariance as defined in subsection

∫
= ∫
​​ ​  ​ ​1​ ​ ​ (​u​ ​)E(​Y​ ​ | ​U​ ​= ​u​ ​)du
​ ​  ​ c ∫
+ ​∫ ​  ​ 1  ​ (​u​ ​) E(​Y​ ​ | ​U​ ​= ​u​ ​)] d​u​ ​d  ​f​ ​(t) dt,
1

E(Y) = ​ ​  ​ E(Y | P(Z) ​ = t) f P(t) dt
0

1

1

0

0

[0, t]

1

D

D

D

D

1

0

(t, 1]

D

0

D

where

1 if 0 ≤ uD ≤ t
      
 ​
1[0, t](uD) = e ​
  
0 otherwise.

Changing the order of the integration,

D

D

P

TT, and TUT for a case where β is not independent of D. ATE weights ​uD​ ​ evenly. TT
oversamples low values of ​u​D​ (associated
with persons more likely to participate in the
program). TUT oversamples high ​u​D​. In this
example, because MTE(​u​D​) is decreasing in ​
u​D​, TT > ATE > TUT.
3.4 What Does Conventional IV Estimate?

In most empirical studies, more than the
two values of Z are used to construct IV estimates. For this case, Imbens and Angrist
(1994) use weights developed by Shlomo
Yitzhaki (1989) to express IV as a weighted
average of component LATEs defined for
different values of the instruments. The
weights used by Imbens–Angrist are positive for each component LATE for the
special instrument they consider (P(Z) or
some monotonic function of P(Z)). Since
MTE(​u​D​) may change sign over the interval ​
u​D​ ∈ [0, 1], the IV may be negative even if
some portion of the MTE(​u​D​) is positive. For
general instruments that are not monotonic
functions of P(Z), the output of IV is even
more ambiguous. The IV weights can be
negative over regions of ​u​D​ ∈ [0, 1]. Thus an
IV based on general instruments may have
a sign opposite to the true causal effect as
defined by the MTE. Even if each component of LATE is positive, in the general case
E(Y) = ∫
​ 01​ ​  ​[​∫01​ ​  ​[​1​[​uD​ ​,1]​ (t) E(​Y1​ ​ | ​UD​ ​= ​uD​ ​)

+ ​1[0,​
​ uD​ ​)​ (t) E(​Y0​ ​ | ​UD​ ​= ​uD​ ​)]  ​fP​ ​(t) dt] d​uD​ ​

1
= ∫
​ 0​ ​  [​ (1 − ​FP​ ​(​uD​ ​))E(​Y1​ ​ | ​UD​ ​= ​uD​ ​)
+ ​FP​ ​(​uD​ ​)E(​Y0​ ​ | ​UD​ ​= ​uD​ ​)] d​uD​ ​.

Comparing policy a to policy b, E(​Y​ a​ | X) − E(​Y​ b​ | X)
= ​∫01​ ​  ​E((​Y1​ ​  − ​Y​ 0​) | X, ​U​D​ = ​u​D​)(​F​ bP​(​  ​uD​ ​) − ​F​ aP​(​  ​uD​ ​)) d​uD​ ​.
An alternative proof is given in Appendix A.
60 The PRTE can be interpreted as an economically
more explicit version of Stock’s (1989) nonparametric
policy analysis parameter for a class of policy interventions
with explicit agent preferences where the policies evaluated operate solely on agent choice sets.
61 I discuss the relationship between MTE and IV in the
next subsection. For the general case, knowledge of the
joint distribution of Z, P(Z) is required.

Heckman: Building Bridges

381

Table 3

IV weights

Treatment parameter
weights

MTE Weights For Different Treatment Parameter and IVs.
(Fp is the distribution of P. fP is its density.)
​ ATE
h
​ ​(​uD​ ​) = 1

∫

1

1  ​
​hTT
​ ​(​uD​ ​) = c​ ​ ​ ​fP​ ​( p) ​dpd ​ _
​uD​ ​
E(P)

∫

​uD​ ​

1
​ ​TUT​(​uD​ ​) = c​ ​  ​ ​fP​ ​( p) ​dpd ​ _
h
 ​
0
E(1 − P)
​h​PRTE​(​uD​ ​) = ​F​ bP​ ​(​uD​ ​) − ​F​ aP​(​  ​uD​ ​)

∫

1

1  ​  for P(Z) as an instrument
​h​ IV​(​uD​ ​) = c​ ​ ​ ( p − E(P)) ​fP​ ​( p) ​dpd ​ _
​uD​ ​
Var(P)

∫∫
1

∞

​ j − E(  J)) fJ,P  (  j, p) ​ ​dj dp
​ (
​ ​ ​ ​
​uD​ ​ −∞
 ​
   
   for a general instrument J(Z)*, a function of Z
​h​IV​(​uD​ )​ = ​ ___
Cov (  J,P)

* fJ , P ( j, p) is the joint density of J and P. For derivations of these weights, see Heckman and Vytlacil (1999, 2005,
2007b).

IV can be negative. Negative components of  
MTE(​u​D​) weighted by negative weights can
generate a positive IV.
This analysis is constructive because the
weights can be identified from the data.
Analysts can ascertain whether or not the
weights are negative and over what regions
of ​uD​ ​. In this subsection, I analyze the case
where P(Z) is the instrument and the weights
are positive. I analyze the general case in section 3.6 below.
To understand what IV identifies, consider a linear regression approximation of
E(Y | P(Z) = p):

where
Cov(Y, P(Z))
 ​
b = ​_
Var(P(Z))
Cov(E(Y | P(Z)), P(Z))
  
  
 ​ .
= __
​
Var(P(Z))
b is the same as the IV estimate of “the
effect” of D on Y using P(Z) as an instrument
since Cov(P(Z), D) = Var(P(Z)).63
Using condition (IA–1) (in particular that
P(Z) is independent of ​Y0​ ​), and expression
(15), we obtain

E*(Y | P(Z) = p) = a + bp,62
62 E*(M | N) denotes linear projection, i.e., the linear
regression of M on N.

63 D(Z) = P(Z) + τ
where E(τ | P(Z)) = 0, thus
Cov(Y,P(Z))/Cov(D,P(Z))  = Cov(Y,P(Z))/Var(P(Z)).

Journal of Economic Literature, Vol. XLVIII (June 2010)

382
h(uD)

MTE
0.35

3.5

3
MTE

2.5
TT
TUT

2

1.5

1

ATE

0.5

0

0
0

0.1

0.2

0.3

0.4

0.5

uD

0.6

0.7

0.8

0.9

1

Figure 3. MTE and the Weights for the Marginal Treatment Effect for Different Parameters for the Model
Graphed in Figure 1
Source: Heckman and Vytlacil (2005).

(19)

Cov(Y, P(Z))
b = ​_
 ​
Var(P(Z))
Cov(S(P(Z)), P(Z))
  
  
 ​
= __
​
Var(P(Z))
Cov  Q∫
​ 0P(Z)
​  ​  ​MTE(​u​D)​ d​u​D,​ P(Z)R
___
= ​
  
 ​ .
Var(P(Z))

Note that when MTE(​u​D​)_is constant in
​uD​ ​ (MTE(​u​D​) = ​μ​1​ − ​μ0​ ​  = ​β​) so that β is

independent of D, the numerator of the preceding expression simplifies to

∫

P(Z)

Cov a​ ​ 
0

​ MTE(​uD​ ​) ​du
​ D​ ​, P(Z)b
_

_

= Cov (​β​P(Z), P(Z)) = ​β​ Var  (P(Z))
_

so b = ​μ1​ ​ − ​μ0​ ​ = ​β​. This is the traditional
result for IV. In this case, the marginal

Heckman: Building Bridges
s­ urplus is the same as the average surplus
for all values of p. Expression (19) arises
because D depends on β (= ​Y1​ ​ − ​Y0​ ​), something assumed away in traditional applications of IV. As a consequence, in general,
the marginal surplus is not the average
surplus.
An explicit expression for the numerator
of (19) is
Cov (Y, P(Z))

∫ ∫
1

p

= ​ ​  ​ c​ ​  ​ MTE(​uD​ ​) ​ ​ ​du​D​d ( p − E(P))  ​f​P​( p) dp.
0

0

Reversing the order of the integration of the
terms on the right-hand side and respecting
the requirement that 0 < ​u​D​ < p < 1, we
obtain
Cov  (Y, P(Z))
  
 ​
b = ​ _
Var(P(Z))
​ 01​ ​  ​MTE(​uD​ ​) S​∫​u1​ ​ ​​  ​( p − E(P))  ​fP​ ​( p) dpT d​u​D​
∫
D
___
= ​ 
    
   
 ​
Var(P(Z))

∫

1

An alternative expression for the weight is as
the mean of left truncated P(Z):

​ h​ IV
​ ​)
P(Z)​ ​(​uD
E(P(Z) − E(P(Z)) | P(Z) > u
​ D​ ​) Pr(P(Z) > u
​ D​ ​)
= ____
​
    
   
​ ,
Var(P(Z))

which shows that the weight on the
MTE (​uD​ ​) is non-negative for all ​uD​ ​.
The weights can be estimated from the
sample distribution of P(Z). The weights
for P(Z) as an instrument have a distinctive
profile. It is readily verified that they are
non-negative, reach a peak at the mean of
the distribution of P(Z), and are zero at the
extremes ​u​D​= 0 and ​u​D​= 1. The weights
integrate to 1.66 Figure 4 plots the IV weights
and the MTE from a study by Heckman and
Vytlacil (2005). Comparing the IV weights
with the weights for different treatment
effects enables analysts to determine how
closely IV approximates any particular mean
treatment effect.
For discrete valued instruments mapped
​ 1​ ​ < P(​z2​ ​) = ​p2​ ​ < ⋯ < P(​zL​ ​)  
into P(​z​1​) = p
= ​p​L​  ,

= ​ ​  ​ MTE(​uD​ ​)​ h​ IV
​ ​) ​d​uD​ ​ ,
P(Z)​ ​(​uD

L−1

​ ​, ​pℓ​​)​λℓ​​,
IV = ​∑ ​​  ​LATE( ​pℓ+1

0

where

∫
​  ( p − E(P))  ​fP​ ​( p) dp
​ ​u1​ ​  ​
D​
__
​
​ 
(
u
​
​
)
​
=
​
  
   64, 65
 ​ .
​ h​ IV
P(Z)
D
Var(P(Z))

383

ℓ=1

L
where ​λℓ​​ = (1/Var(P(Z))) ​∑ t>ℓ​  ​​( ​pt​​ − E(P))
×  ​fP​ ​( ​pt​​) and ​fP​ ​( ​pt​​) is the probability that P(Z)
= ​pt​​. For a proof, see Heckman, Urzua, and
Vytlacil (2006) or Appendix B.

3.5 The Problem of Limited Support
64 This result is due to Yitzhaki (1989) and is elaborated
in Heckman, Urzua, and Vytlacil (2006) and Heckman and
Vytlacil (2007a). The Yitzhaki paper is posted at the Web
site for Heckman, Urzua, and Vytlacil (2006).
65 Under the conditions of Fubini's theorem, it is valid
to reverse the order of the integration. See the discussion
in the preliminary remarks of Appendix A.

Before turning to the analysis of general
instruments, I consider the problem of limited support for P(Z) for the special instrument P(Z) used by Imbens and Angrist.
1
​ ​ ( p − E(P))  ​fp​ ​( p) dpD d​u​D​= Var(P(Z)).
​ 0 C ​​∫​u​  ​  ​

66 ∫ ​​
1

D

Journal of Economic Literature, Vol. XLVIII (June 2010)

384
h IV(uD)

MTE

5

0.5

4

3

MTE

2

1

IV

0

–1

–2

– 0.3

–3
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

uD
Figure 4. The MTE and IV Weights as a Function of ​uD​ ​
Source: Heckman and Vytlacil (2005).

Analysis of this simple case establishes principles that apply to more general instruments.
While the various treatment parameters
can be defined from the generalized Roy
model, they may not necessarily be identified from the data. Both the nonparametric
structural approach and the nonparametric program evaluation approach avoid the
problem of extrapolating estimates outside
the support of the data.67 The early ­literature

in ­structural econometrics imposed strong
functional forms (typically linearity) to obtain
estimates.68 P(Z) may not be identified over
the full unit interval. Thus the lowest sample
value of P(Z) may exceed zero and the largest
value of P(Z) may be less than 1.
In addition, P(Z) may only assume discrete
values. This limits the identifiability of MTE.
In this case, only LATE over intervals of
​u​D​ ∈ [0, 1] can be identified from the values

67 Angrist and Pischke (2008, 2010) are exceptions.
They advocate use of linear equations in estimating treatment effects.

68 As did the program evaluation literature. See Burt S.
Barnow, Glen G. Cain, and Arthur S. Goldberger (1980).

Heckman: Building Bridges
of P(Z) = P(z) associated with the discrete
instruments.69
One approach to this problem developed
by Manski (1990, 1995, 2003) is to produce
bounds on the treatment effects. Heckman
and Vytlacil (1999, 2000, 2001a, 2001b,
2007b) develop specific bounds for the generalized Roy model that underlies the LATE
model. The bounds developed in the literature are for conventional treatment effects
and not for policy effects.
Carneiro, Heckman, and Vytlacil (2010)
consider an alternative approach based on
marginal policy changes. Many proposed
policy changes are incremental in nature,
and a marginal version of the PRTE is all that
is required to answer questions of economic
interest. When some instruments are continuous, it is possible under the conditions
in their paper to identify a marginal version
of PRTE (MPRTE). MPRTE is in the form
of representation (18) where the weights can
be identified from the data and the support
requirements are more limited than the conditions required to identify PRTE for large
changes in policies. Their paper presents a
derivation of the weights for classes of policy
expansions.70 Application of these data sensitive nonparametric approaches enables analysts to avoid one source of instability of the
estimates of policy effects that plagued 1980s
econometrics.
3.6 More General Instruments
Typically, economists use a variety of
instruments one at a time and not just
P(Z), or some function of P(Z), as an instrument and compare the resulting estimates
(see, e.g, Card, 1999, 2001). When there is
69 Bounds for MTE and LATE in the case of limited
support are presented in Heckman and Vytlacil (1999,
2001a, 2001b, 2007b). Manski (1990, 1995, 2003) presents
bounds for a wide array of models.
70 Hidehiko Ichimura and Taber (2002) develop an
alternative local approach that does not exploit the structure of the generalized Roy model that underlies LATE.

385

­selection on the basis of gross gains (β ⊥
/⊥   D)
so that the marginal gross surplus is not the
same as the average gross surplus, different
instruments identify different parameters.
IV is a weighted average of MTEs where the
weights integrate to 1 and can be estimated
from sample data. However, in the case of
general instruments, the weights can be negative over stretches of ​uD​ ​.
Consider using the first component of
Z, ​Z​1​, as an instrument for D in equation
(7). Suppose that Z contains two or more
­elements (Z = (​Z​1​, … , ​ZK​ ​), K ≥ 2). The economics implicit in LATE informs us that Z
determines the distribution of Y through
P(Z). Any correlation between Y and ​Z​1​ arises
from the statistical dependence between ​Z​1​ 
and P(Z) operating to determine Y.
The IV estimator based on Z
​ ​1​ is
Cov(Y, ​Z1​ ​)
I​V​Z​ 1​ ​​ = _
​
 ​
Cov(D, ​Z1​ ​)
Cov(E(Y | ​Z1​ ​), ​Z1​ ​)
  
  
 ​ .
= ​ __
Cov(D, ​Z1​ ​)
Note, however, that choices (and hence Y)
are generated by the full vector of Z operating through P(Z). The analyst may only use ​
Z​1​ as an instrument but the underlying economic model informs us that the full vector
of Z determines observed Y. Conditioning
only on ​Z1​ ​ leaves uncontrolled the influence
of the other elements of Z on Y. This is a new
phenomenon in IV that would not be present
if D did not depend on β(= ​Y1​ ​ − ​Y0​ ​). An IV
based on ​Z1​ ​  identifies an effect of ​Z1​ ​  on Y
as it operates directly through ​Z1​ ​ (​Z1​ ​ changing P(​Z​ 1​, … , ​ZK​ ​)) holding other elements in
Z constant and indirectly through the effect
of ​Z1​ ​ as it covaries with (​Z2​ ​, … , ​ZK​ ​), and how
those variables affect Y through their effect
on P(Z).
A linear regression analogy helps to fix ideas.
Suppose that outcome Q can be expressed

386

Journal of Economic Literature, Vol. XLVIII (June 2010)

as a linear function of W = (​W1​ ​, … , ​WL​  ​), an
L-dimensional regressor:
L

​ ϕ​ℓ​Wℓ​​+ ε,
Q = ∑
​ ​  ​​
ℓ=1

where E(ε | W) = 0. If we regress Q only on ​
W​ 1​, we obtain in the limit the standard omitted variable result that the estimated “effect”
of ​W1​ ​ on Q is
(20)

Cov(Q, ​W1​ ​)
 ​  = ​ϕ​1​
​ _
Var(​W1​ ​)
L

Cov(​Wℓ​ ​, ​W1​ ​)
​ ϕ​ℓ​ _
​ 
 ​
+ ​∑ ​  ​​
Var(​W1​ ​)
ℓ=2
where ​ϕ​1​ is the ceteris paribus direct effect
of ​W1​ ​ on Q and the summation captures the
rest of the effect (the effect on Q of ​W1​ ​ operating through covariation between ​W1​ ​  and
the other values ​Wℓ​ ​  , ℓ ≠ 1). An analogous
problem arises in using one instrument at a
time to identify “the effect” of ​Z​1​.
Thus if the analyst does not condition on
the other elements of Z in using ​Z​1​  as an
instrument, the margin identified by variations of ​Z1​ ​  does not in general correspond
to variations arising solely from variations in ​
Z​1​, holding the other instruments constant.
The margin of choice implicitly defined by
the variation in ​Z1​ ​  is difficult to interpret
and depends on the parameters of the generalized Roy model generating outcomes as
well as on the sample dependence between
instrument ​Z​1​  and P(Z). Thus an IV based
on ​Z​1​  mixes causal effects with sample
dependence effects among the correlated
regressors.71 In a study of college going, if ​
Z​1​ and ​Z2​ ​ are tuition and distance to college,
71 Relationships that combine sample and structural
relationships were called “mongrel” relationships by the
early structural econometricians (see Lawrence R. Klein
1953).

respectively, the ­instrument ​Z​1​ ­identifies
the direct effect of variation in tuition on
college attendance and the effect of distance to college on college attendance as
distance covaries with tuition in the sample
used by the analyst. This is not the ceteris
paribus effect of a variation in tuition. It
does not correspond to the answer needed
to predict the effects of a policy that operates solely through an effect on tuition. In
models in which D depends on β, the traditional instrumental variable argument
that analysts do not need a model for D and
can ignore other possible determinants of
D other than the instrument being used,
breaks down. To interpret which margin is
identified by different instruments requires
that the analyst specify and account for all of
the Z that form P(Z). Since different economists may disagree on the contents of Z,
­different economists using ​Z​1​  on the same
data will obtain the same point estimate but
may ­disagree about the interpretation of the
margin identified by variation in ​Z​1​.
To establish these points, note that as a
consequence of Vytlacil’s theorem, Z enters
the distribution of Y only through P(Z).
Thus the conditional distribution of Y given ​
Z​1​ = ​z1​ ​  operates through the effect of ​Z​1​ 
as it affects P(Z). That is a key insight from
Vytlacil’s theorem. Thus
E(Y | ​Z1​ ​  = ​z1​ ​)

∫

1

​ p) ​gP​ (Z) | ​Z1​ ​​( p, ​z1​ ​) dp
= ​ ​  ​ E(Y | P(Z) =
0

where ​g​P(Z) | ​Z1​ ​​( p, ​z1​ ​) is the conditional density
of P(Z) given ​Z1​ ​= ​z1​ ​.72 Putting all of these
ingredients together, and using (15), we
obtain

72 g​P(Z) | ​Z​ ​ ​( p, ​z​ ​)

​

1

1

= ​gP​ (Z)​, ​Z​1​ ( p, ​z1​ ​)/​g​Z​ 1​ ​ ​(​z1​ ​).

Heckman: Building Bridges
E(Y | ​Z​1​= ​z1​ ​)

∫

1

= E(​Y0​ ​) + ​ ​  ​ S( p) ​gP​ (Z) |​Z1​ ​​( p, ​z1​ ​) ​dp
0

∫ C∫
1

D

p

= E(​Y0​ ​) + ​ ​  ​ ​ ​ ​  ​ MTE(​uD​ ​) ​d​uD​ ​ ​ ​
0
0
8
S( p)

× ​g​P(Z) | ​Z1​ ​​( p, ​z1​ ​) dp.

Using this expression to
Cov(Y, ​Z1​ ​)/Cov(D, ​Z1​ ​), we obtain

compute

I​V​​Z1​ ​​
​∫ ∞ ​  ​
​ (​z​ ​ − E(​Z​ ​)) ∫
​ 1​ ​  ​S( p)​gP​ (Z) | ​Z1​ ​​( p, ​z​ ​) dp d​z​ ​

1
1
1
1
−∞
0
   
  ​
= ​ _______________________
Cov(​Z​ ​, D)
1

​∫ ∞ ​  ​
​ (​z1​ ​ − E(​Z1​ ​))​∫ 1​ ​  ​S​∫ p​ ​  ​MTE(​uD​ ​) d​uD​ ​T​g​P(Z) | ​Z1​ ​​( p, ​z1​ ​) dp d​z1​ ​

−∞
0
0
= ​ ______________________________
    
   ​
Cov(​Z​ ​, D)
1

This expression integrates the argument in
the numerator with respect to ​u​D​  , p, and ​z1​ ​ 
in that order. Reversing the order of integration to integrate with respect to p, ​z​1​  ,
and ​u​D​in that order, we obtain

∫

387

The weight integrates to 1 but can be negative over stretches of ​u​D​.73 At the extremes
(​u​D​= 0, 1), the weights are zero.
An illuminating way to represent this
weight is
​ ​)
​h​ IV
​Z1​ ​ ​​(​uD
E(​Z​ ​− E(​Z1​ ​) | P(Z) > u
​ D​ ​) Pr(P(Z) > u
​ D​ ​)
= ___
​ 1
    
   
 ​ 
.
Cov(​Z1​ ​, D)
As ​uD​ ​is increased, the censored (by the condition P(Z) > ​u​D​) mean of (​Z1​ ​ − E(​Z1​ ​)) may
switch sign, and hence the weights may be
negative over certain ranges. Thus the IV
estimator may have a sign opposite to the
true causal effect (defined by the MTE).
Figure 6 illustrates this possibility for
the distribution of the data Z = (​Z1​ ​, ​Z2​ ​)
shown in figure 5, where Z is continuously
distributed. The support of the data only
permits identification of P(Z) over the
interval [0.1, 0.9]. Thus none of the conventional treatment parameters is identified.
From LIV, we can identify the MTE over
the interval [0.1, 0.9]. We can also identify
the weights over this interval. For values
of ​u​D​> 0.65, the weights are negative in
this example. Thus it is possible that the
IV based on ​Z​1​ can be negative even if the
MTE is everywhere positive. Table 4, taken
from Heckman, Urzua, and Vytlacil (2006),
shows how three different distributions of

1

​ ​) ​d​uD​ ​,
I​V​​Z1​ ​ ​ = ​ ​  ​ MTE(​uD​ ​)​h​ IV
​Z1​ ​ ​​(​uD
0

where
​ ​)
​h​ IV
​Z1​ ​ ​​(​uD
∞
∫
​ (​z1​ ​ − E(​Z​1​)) ​∫​u1​ ​  ​​
​  ​
​  g​P(Z) | ​Z1​ ​​( p, ​z1​ ​) dp d​z​1​
​ −∞
D​
___
=​
    
   .
 ​
Cov(​Z1​ ​, D)

73 Cov(​Z​ ​,
1

Y)
= E[(​Z1​ ​− E(​Z1​ ​)) 1(​UD​ ​< P(Z))(​Y1​​− ​Y0​​)]
= E[(​Z1​ ​−E(​Z1​ ​))1(​UD​ ​< P(Z))E(​Y1​​− ​Y0​​| Z, ​UD​ ​)]
= E[(​Z1​ ​− E(​Z1​ ​)) 1(​UD​ ​< P(Z))E(​Y1​​− ​Y0​​ | ​UD​ ​)]
= ​E​U​ D​ ​​[​EZ​ ​[(​Z1​ ​− E(​Z​1​))1(​UD​ ​< P(Z)) | ​U​D​]E[​Y1​​− ​Y0​​| ​UD​ ​)]]
1
= ∫
​ 0​ ​  ​ {E(​Z1​ ​ − E(​Z1​ ​) | P(Z) ≥ ​uD​ ​) Pr(P(Z) ≥ ​uD​ ​)
    × E(​Y1​ ​ − ​Y0​ ​ | ​UD​ ​= ​uD​ ​)} d​uD​ ​
= ​∫ 1​ ​ MTE(​uD​ ​)​ E(​Z1​ ​ − E(​Z1​ ​)|P(Z) ≥ ​uD​ ​)Pr(P(Z)≥ ​uD​ ​)d​uD​ ​.
0

The expression for Cov(​Z​ 1​, D) is Cov(​Z1​ ​, D) =
E[(​Z1​ ​  − E(​Z1​ ​))1(​UD​ ​ < P(Z))]. The expression for the
weight for MTE can thus be written as ​h​ IV
​ ​) =
​Z1​ ​​ ​(​uD
Cov(​Z​1​, Pr(P(Z) > ​u​D​))/Cov(​Z1​ ​, Pr(P(Z) > ​U​D​)) so it is easy
to see that the weights integrate to 1.

Journal of Economic Literature, Vol. XLVIII (June 2010)

388

0.20

Joint Density

0.15
0.10
0.05
0.00
6

1
4
0

2

Z

2

0

−1

Z1

−2
−2

Figure 5. Joint Density of Instruments Z = (​Z1​ ​, ​Z2​ ​)

Z for the same underlying policy-invariant
model with the same ATE can produce very
different IV estimates.
This analysis elucidates the benefits and
limitations of the method of randomized
controlled experiments. Experiments that
manipulate ​Z1​ ​ independently of other components of Z isolate the effects of ​Z​1​  on
outcomes in comparison with the effects
obtained by sample variation in ​Z​1​  correlated with other components of Z. Neither
set of variations may identify the returns to
any given policy unless the experimentally
induced variation corresponds exactly to the
variation induced by the policy. Economists
can use experimental variation to identify the MTE. The features of a proposed

policy are described by its effects on the
PRTE weights as it affects the distribution
of P(Z). Proceeding in this way, one can use
experiments to address a range of questions
beyond the effects directly identified by the
experiment.
Using the implicit economic theory underlying LATE, economists can do better than
just report an IV estimate. We can be data
sensitive but not at the mercy of the data. We
can determine the MTE (or LATEs) over
the identified regions of ​u​D​ in the empirical support of P(Z). We can also determine
the weights over the empirical support of
P(Z) to determine whether they are negative or positive. We can bound estimates of
the unidentified parameters. (See Heckman

Heckman: Building Bridges

389

4
IV Weights

3

MTE

IV Weights

2

1

0

–1

–2
0.0

0.2

0.4

uD

0.6

0.8

1.0

Figure 6. MTE and IV Weights for a General Instrument ​Z1​ ​, a Component of Z = (​Z1​ ​, ​Z2​ ​)

and Vytlacil 1999, 2001a, 2001b, 2007b.) We
can construct the effects of policy changes
for new policies that stay within the support of P(Z) (see Carneiro, Heckman, and
Vytlacil 2010).
3.7 Policy Effects, Treatment Effects,
and IV
A main lesson of this paper is that policy
effects are not generally the same as treatment
effects and, in general, neither are ­produced
from IV estimators. Since randomized assignments of components of Z are instruments,
this analysis also applies to the output of
randomized experiments. The economic
approach to policy evaluation formulates
policy questions using well-defined economic
models. It then uses whatever statistical tools
it takes to answer those questions. Policy
questions and not statistical methods drive

analyses. Well-posed economic models are
scarce in the program evaluation approach.
Thus in contrast to the structural approach,
it features statistical methods over economic
content. “Credibility” in the program evaluation literature is assessed by statistical properties of estimators and not economic content
or policy relevance.
We can do better than hoping that an
instrument or an estimator answers policy
problems. By recovering economic primitives, we can distinguish the objects various
estimators identify from the objects needed
to address policy problems. Constructing the
PRTE is an example of this approach.74
74 An alternative approach developed in Heckman
and Vytlacil (2005) constructs combinations of instruments using sample data on Z that address specific policy
questions.

390

Journal of Economic Literature, Vol. XLVIII (June 2010)

Table 4

IV Estimator for Three Different Distributions of Z but the Same Generalized Roy Model.
Data Distribution
1
2
3

IV

ATE

0.434
0.078
–2.261

0.2
0.2
0.2

Source: Heckman, Urzua, and Vytlacil (2006, table 3).

Figure 7, taken from an analysis of the
returns to attending college by Carneiro,
Heckman, and Vytlacil (2009), plots the
estimated weights for MTE from a marginal change in policy that proportionally
expands the probability of attending college
for everyone. The figure also plots the estimated MTE and the IV weight using P(Z) as
an instrument. The IV weights and the policy
weights are very different. The policy weights
oversample high values of ​u​D​ compared to
the IV weights. Since the MTE is declining
in ​uD​ ​, this translates into an IV estimate of
0.095 compared to a marginal policy effect
of 0.015.75 The IV estimate would suggest a
substantial mean marginal gross return. The
true marginal policy effect is much lower.
Since the MTE can be estimated (or approximated) from the data and the policy weights
constructed from the data, one can produce
more accurate policy forecasts using the economics of the model.
3.8 Multiple Choices
Imbens and Angrist analyze a two choice
model. Heckman, Urzua, and Vytlacil (2006,
forthcoming) and Heckman and Vytlacil
(2007b) extend their analysis to an ordered

75 These estimated effects are statistically significantly
different from each other (see Carneiro, Heckman, and
Vytlacil 2009).

choice model and to general unordered
choice models.76
In the special case where the analyst
seeks to estimate the mean return to those
induced into a choice state by a change in
an instrument compared to their next best
option, the LATE framework remains useful (see Heckman, Urzua, and Vytlacil 2006,
forthcoming; Heckman and Vytlacil 2007b).
If, however, one is interested in identifying
the mean returns to any pair of outcomes,
unaided IV will not do the job. Structural
methods are required.
In general unordered choice models,
agents attracted into a state by a change in an
instrument come from many origin states, so
there are many margins of choice. Structural
models can identify the gains arising from
choices at these separate margins. This is a
difficult task for IV without invoking structural assumptions. Structural models can
also identify the fraction of persons induced
into a state coming from each origin state.
IV alone cannot. See Heckman and Urzua
(2010).
76 Angrist and Imbens (1995) propose an ordered choice
version of their 1994 paper. As shown by Heckman, Urzua,
and Vytlacil (2006) and Heckman and Vytlacil (2007b),
their proposed extension has some unsatisfactory features
which can be removed by using an extension of the generalized Roy model to an ordered choice model using
the choice framework of Cunha, Heckman, and Navarro
(2007) and Carneiro, Hansen, and Heckman (2003).

Heckman: Building Bridges

391

0.5

MTE

0.4

IV
Policy weight

MTE, Weights

0.3

0.2

0.1

0

–0.1

–0.2

0

0.1

0.2

0.3

0.4

uD

0.5

0.6

0.7

0.8

0.9

1

Figure 7. MTE and Weights for IV and MPRTE in the Carneiro–Heckman–Vytlacil (2009) Analysis of
the Wage Returns to College
Source: Carneiro, Heckman, and Vytlacil (2009).
Notes: The scale of the y-axis is the scale of the MTE, not the scale of the weights, which are scaled to fit the
picture. The IV is P(Z).

4.

Conclusions

This paper compares the structural
approach to empirical policy analysis with
the program evaluation approach. It offers a
third way to do policy analysis that combines
the best features of both approaches. This
paper does not endorse or attack any particular statistical methodology. Economists are
fortunate to have a rich menu of estimation
methods from which to choose.

This paper advocates placing the economic and policy questions being addressed
front and center. Economic theory helps
to sharpen statements of policy questions.
Modern advances in statistics can make the
theory useful in addressing these questions.
A better approach is to use the economics to
frame the questions and the statistics to help
address them.
Both the program evaluation approach
and the structural approach have desirable

392

Journal of Economic Literature, Vol. XLVIII (June 2010)

features. Program evaluation approaches
are generally computationally simpler than
structural approaches, and it is often easier
to conduct sensitivity and replication analyses
with them. Identification of program effects is
often more transparent than identification of
structural parameters. At the same time, the
economic questions answered and the policy
relevance of the treatment effects featured
in the program evaluation approach are often
very unclear. Structural approaches produce
more interpretable parameters that are better suited to conduct counterfactual policy
analyses.
The third way advocated in this essay is
to use Marschak’s Maxim to identify the
policy relevant combinations of structural
parameters that answer well-posed policy
and economic questions. This approach
often simplifies the burden of computation, ­facilitates replication and sensitivity
analyses, and makes identification more
­transparent. At the same time, application
of this approach forces analysts to clearly
state the goals of the policy analysis—
something many economists (structural or
program evaluation) have difficulty doing.
That discipline is an added bonus of this
approach.
I have illustrated this approach by using
the economics implicit in LATE to interpret
the margins of choice identified by instrument variation and to extend the range of
questions LATE can answer. This analysis is a
prototype of the value of a closer integration
of theory and robust statistical methods to
evaluate public policy.
Appendices
A. Derivation of the Weights for PRTE

∫ ∫| f

J,K( j, k) | dj

the value of the integral for the region 0 < j
<k<1

∫∫
1

k

​ ​  ​​ ​  ​ fJ,K (   j, k) ​ ​dj dk
0

0

∫∫
1

1

= ​ ​  ​​ ​  ​fJ,K  (  j, k) ​ ​dk dj.
0

j

Derivation: We can write

E(Y | Baseline b)

∫

1

= ​ ​ ​E ​(Y | P(Z) = p) ​f​ bP​ ​( p) dp
0

= E[1(P(Z) ≥ ​U​D)​ ​Y1​ ​+ 1(P(Z) < U
​ D​ ​)​Y0​ ​]

∫∫∫
1

p

= ​ ​  ​ ​ ​  ​ ​
0

0

∞

​ ​​y​1​ ​​f​​Y1​ ​,​UD​ ​ ​​ (​y1​ ​, ​uD​ ​) ​dy​ ​ 1​

−∞

d​uD​ ​​f​ bP​​ ( p) dp

∫∫∫
1

1

+ ​ ​  ​ ​ ​  ​ ​
0

p

∞

​ ​​y0​ ​ ​​f​​Y0​ ​,​UD​ ​​ ​(​y0​ ​, ​uD​ ​) ​dy​ 0​ ​

−∞

d​u​D​​f​ bP​​ ( p) dp

because P(Z)   ∥   ​UD​ ​, ​Y1​ ​, ​Y0​ ​ | X.
Thus E(Y | Baseline b)

∫∫
1

p

​ ​​f​ bP​(​  p) dp
= ​ ​  ​ ​ ​  ​ E(​Y1​ ​ | ​UD​ ​ ​ ​= ​uD​ ​) d​uD  
0

Preliminary Remarks: Recall that, if for
two random variables J and K for 0 ≤ j ≤ 1
and 0 ≤ k ≤ 1, with density fJ,K(   j, k)

dk < ∞,

0

∫∫
1

1

+ ​ ​  ​ ​ ​  ​ E(​Y0​ ​ | ​UD​ ​ ​ ​= ​uD​ ​) d​uD​ ​  ​f​ bP​(​  p)dp.
0

p

Heckman: Building Bridges
Interchanging the limits of each integral
E(Y | Baseline)

∫

1

C∫ ​ ​​ F​  ​(​  p) ​ dpD​d​u​ ​

= ​ ​  ​ E(​Y1​ ​ | ​UD​ ​ = ​uD​ ​) ​​ ​
0

∫

1

​uD
​ ​

C∫

1

b
P

D

​uD
​ ​

D

+ ​ ​  ​ E(​Y0​ ​ | ​UD​ ​ ​ = ​uD​ ​) ​ ​ ​  ​ ​F​ bP​(​  p) ​dp ​ d​uD​ ​
0

∫

0

1

= ​ ​  ​ E(​Y1​ ​ | ​UD​ ​ ​= ​uD​ ​) ​C1 −
0

∫

D​ d​uD​ ​

​F​ bP​(​  ​uD​ ​)

1

+ ​ ​  ​ E(​Y0​ ​| ​UD​ ​ ​ = ​uD​ ​)​F​ bP​(​  ​uD​ ​) d​uD​ ​.
0

By a parallel argument for E(Y | Alternative
Policy a)

∫

1

= ​ ​  ​ E(​Y1​ ​ | ​UD​ ​ ​= ​u​D​)[1 − ​F​ aP​(​  ​uD​ ​)] d​uD​ ​
0

∫

393

Pr(P(Z) > ​u​D​) is constant in ​u​D​for ​u​D​within
​  ​) interval. Let ​λ​ℓ​ denote the
any ( ​pℓ​ ​, ​pℓ+1
​  ​).
weight on the LATE for the interval ( ​pℓ​ ​,​pℓ+1
Under monotonicity condition (IA–3),
​IV​​Z1​ ​ ​ =
K−1

∫E(​Y​ ​ − ​Y​ ​ | ​U​ ​  =  ​u​ ​)​h​  ​ ​  (​u​ ​) d​u​ ​
1

∫

0

D

D

IV
​Z1​ ​

D

D

​pℓ+1
​  ​

= ​∑ ​​ ​λ​ℓ​ ​ ​ E(​Y1​ ​ − ​Y0​ ​  |​UD​ ​= ​uD​ ​) ​
ℓ=1

​pℓ​ ​

1
 ​  d​uD​ ​
× ​ _
( ​pℓ+1
​  ​ − ​pℓ​ ​)
K−1

​ ​)​λℓ​​.
= ​∑ ​​ ​Δ​ LATE​​( ​pℓ​​, ​pℓ+1
ℓ=1

Let ​z​ i1​​  be the ith smallest value of the support of ​Z​1​:
​  ​− ​pℓ​ ​)
(21) ​λ​ℓ​  = ( ​pℓ+1
∑
​ z​ i1​​ − E(​Z​1​))​∑ t>ℓ​ (​ 
​ (​​
​ f (​z​ i1​,​  ​pt​​))
​
   
  
 ​ .
    × ___
​  i=1
Cov(​Z1​ ​, D)
I

K

1

+ ​ ​  ​ E(​Y0​ ​ | ​UD​ ​ ​= ​uD​ ​)​F​ aP​(​  ​uD​ ​) d​uD​ ​.
0

Subtracting the first expression from the second expression we obtain the expression in
the text

∫

1

PRTE = ​ ​  ​E(​Y1​ ​− ​Y0​ ​ | ​UD​ ​ ​= ​uD​ ​)(​F​ bP​(​  ​uD​ ​)
0

− ​F​ aP​(​  ​uD​ ​)) d​uD​ ​.

B. IV for Discrete Instruments
Suppose that the support of the distribution of P(Z) contains a finite number of values ​p1​ ​ < ​p2​ ​ < ⋯ < ​pK​ ​. The support of the
instrument ​Z1​ ​  is also discrete, taking I distinct values. E(​Z​1​ | P(Z) > ​uD​ ​) is constant in ​
​  ​) interval, and
u​D​ for ​u​D​ within any ( ​pℓ​ ​, ​pℓ+1

References
Abbring, Jaap H., and James J. Heckman. 2007. “Econometric Evaluation of Social Programs, Part III: Distributional Treatment Effects, Dynamic Treatment
Effects, Dynamic Discrete Choice, and General
Equilibrium Policy Evaluation.” In Handbook of
Econometrics, Volume 6B, ed. James J. Heckman
and Edward E. Leamer, 5145–5303. Amsterdam and
Oxford: Elsevier, North-Holland.
Ackerberg, Daniel, C. Lanier Benkard, Steven Berry,
and Ariel Pakes. 2007. “Econometric Tools for
Analyzing Market Outcomes.” In Handbook of
­Econometrics, Volume 6A, ed. James J. Heckman
and Edward E. Leamer, 4171–4276. Amsterdam and
Oxford: Elsevier, North-Holland.
Aguirregabiria, Victor, and Pedro Mira. 2010. “Dynamic
Discrete Choice Structural Models: A Survey.” Jour­
nal of Econometrics, 156(1): 38–67.
Ahn, Hyungtaik, and James L. Powell. 1993. “Semiparametric Estimation of Censored Selection Models
with a Nonparametric Selection Mechanism.” Jour­
nal of Econometrics, 58(1–2): 3–29.
Albrecht, James, Gerard J. van den Berg, and Susan
Vroman. 2009. “The Aggregate Labor Market Effects

394

Journal of Economic Literature, Vol. XLVIII (June 2010)

of the Swedish Knowledge Lift Program.” Review of
Economic Dynamics, 12(1): 129–146.
Andrews, Donald W. K., and Marcia M. A. Schafgans.
1998. “Semiparametric Estimation of the Intercept
of a Sample Selection Model.” Review of Economic
Studies, 65(3): 497–517.
Angrist, Joshua D., and Guido W. Imbens. 1995. “TwoStage Least Squares Estimation of Average Causal
Effects in Models with Variable Treatment Intensity.” Journal of the American Statistical Association,
90(430): 431–42.
Angrist, Joshua D., and Jorn-Steffen Pischke. 2008.
Mostly Harmless Econometrics: An Empiricist’s
Companion. Princeton and Oxford: Princeton University Press.
Angrist, Joshua D., and Jorn-Steffen Pischke. 2010.
“The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con
Out of Econometrics.” Journal of Economic Perspec­
tives, 24(2): 3–30.
Athey, Susan, and Philip A. Haile. 2007. “Nonparametric Approaches to Auctions.” In Handbook of
Econometrics, Volume 6A, ed. James J. Heckman
and Edward E. Leamer, 3847–3965. Amsterdam and
Oxford: Elsevier, North-Holland.
Attanasio, Orazio, Costas Meghir, and Ana Santiago.
2009. “Education Choices in Mexico: Using a Structural Model and a Randomized Experiment to Evalute Progresa.” IFS/EDEPO Working Paper.
Autor, David H., Lawrence F. Katz, and Melissa S.
Kearney. 2005. “Trends in U.S. Wage Inequality: Reassessing the Revisionists.” National Bureau of Economic Research Working Paper 11627.
Barnow, Burt S., Glen G. Cain, and Arthur S. Goldberger. 1980. “Issues in the Analysis of Selectivity Bias.” In Evaluation Studies, Volume 5, ed. E.
Stromsdorfer and G. Farkas, 42–59. Beverly Hills,
Calif: Sage Publications.
Becker, Gary S. 1964. Human Capital: A Theoretical
and Empirical Analysis with Special Reference to
Education. New York: National Bureau of Economic
Research.
Berk, Richard, Azusa Li, and Laura J. Hickman. 2005.
“Statistical Difficulties in Determining the Role of
Race in Capital Cases: A Re-analysis of Data from
the State of Maryland.” Journal of Quantitative
Criminology, 21(4): 365–90.
Björklund, Anders, and Robert Moffitt. 1987. “The
Estimation of Wage Gains and Welfare Gains in
Self-Selection.” Review of Economics and Statistics,
69(1): 42–49.
Brock, William A., and Steven N. Durlauf. 2001. “Interactions-Based Models.” In Handbook of Economet­
rics, Volume 5, ed. James J. Heckman and Edward E.
Leamer, 3297–3380. Amsterdam; London and New
York: Elsevier Science, North-Holland.
Brown, Donald J., and Rosa L. Matzkin. 1996. “­Testable
Restrictions on the Equilibrium Manifold.” Econo­
metrica, 64(6): 1249–62.
Cameron, Stephen V., and Christopher Taber. 2004.
“Estimation of Educational Borrowing Constraints

Using Returns to Schooling.” Journal of Political
Economy, 112(1): 132–82.
Campbell, Donald T., and Julian C. Stanley. 1963.
Experimental and Quasi-Experimental Designs for
Research. Chicago: Rand McNally.
Card, David. 1999. “The Causal Effect of Education
on Earnings.” In Handbook of Labor Economics,
Volume 3A, ed. Orley Ashenfelter and David Card,
1801–63. Amsterdam; New York and Oxford: Elsevier Science, North-Holland.
Card, David. 2001. “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.” Econometrica, 69(5): 1127–60.
Carneiro, Pedro, Karsten T. Hansen, and James J.
Heckman. 2001. “Removing the Veil of Ignorance
in Assessing the Distributional Impacts of Social
Policies.” Swedish Economic Policy Review, 8(2):
273–301.
Carneiro, Pedro, Karsten T. Hansen, and James J.
Heckman. 2003. “Estimating Distributions of Treatment Effects with an Application to the Returns to
Schooling and Measurement of the Effects of Uncertainty on College Choice.” International Economic
Review, 44(2): 361–422.
Carneiro, Pedro, James J. Heckman, and Edward J.
Vytlacil. 2009. “Estimating Marginal Returns to Education.” Unpublished.
Carneiro, Pedro, James J. Heckman, and Edward J.
Vytlacil. 2010. “Evaluating Marginal Policy Changes
and the Average Effect of Treatment for Individuals
at the Margin.” Econometrica, 78(1): 377–94.
Cartwright, Nancy. 2004. “Causation: One Word, Many
Things.” Philosophy of Science, 71(5): 805–19.
Chalak, Karim, Susanne Schennach, and Halbert
White. 2010. “Local Indirect Least Squares and
Average Marginal Effects in Nonseperable Structural Systems.” Boston College Department of Economics Working Paper 680.
Chetty, Raj. 2009. “Sufficient Statistics for Welfare
Analysis: A Bridge between Structural and ReducedForm Methods.” Annual Review of Economics, 1:
451–88.
Cosslett, Stephen R. 1983. “Distribution-Free Maximum Likelihood Estimator of the Binary Choice
Model.” Econometrica, 51(3): 765–82.
Cox, David R. 1958. Planning of Experiments. New
York: Wiley.
Cunha, Flavio, and James J. Heckman. 2007. “The Evolution of Inequality, Heterogeneity and Uncertainty
in Labor Earnings in the U.S. Economy.” National
Bureau of Economic Research Working Paper 13526.
Cunha, Flavio, James J. Heckman, and Salvador
Navarro. 2005. “Separating Uncertainty from Heterogeneity in Life Cycle Earnings.” Oxford Eco­
nomic Papers, 57(2): 191–261.
Cunha, Flavio, James J. Heckman, and Salvador
Navarro. 2006. “Counterfactual Analysis of Inequality and Social Mobility.” In Mobility and Inequality:
Frontiers of Research in Sociology and Economics,
ed. Stephen L. Morgan, David B. Grusky, and Gary S.
Fields, 290–346. Stanford: Stanford University Press.

Heckman: Building Bridges
Cunha, Flavio, James J. Heckman, and Salvador
Navarro. 2007. “The Identification and Economic
Content of Ordered Choice Models with Stochastic
Thresholds.” International Economic Review, 48(4):
1273–1309.
Das, Mitali, Whitney K. Newey, and Francis Vella.
2003. “Nonparametric Estimation of Sample Selection Models.” Review of Economic Studies, 70(1):
33–58.
Duflo, Esther. 2004. “The Medium Run Effects of
Educational Expansion: Evidence from a Large
School Construction Program in Indonesia.” Journal
of Development Economics, 74(1): 163–97.
Durbin, J. 1954. “Errors in Variables.” Review of the
International Statistical Institute, 22: 23–32.
Durlauf, Steven N., and H. Peyton Young, eds. 2001.
Social Dynamics. Washington, D.C.: Brookings Institution Press; Cambridge and London: MIT Press.
Einav, Liran, Amy Finkelstein, and Mark R. Cullen.
Forthcoming. “Estimating Welfare in Insurance
Markets Using Variation in Prices.” Quarterly Jour­
nal of Economics, 125(3).
Einav, Liran, and Jonathan D. Levin. 2010. “Empirical
Industrial Organization: A Progress Report.” Journal
of Economic Perspectives, 24(2): 145–62.
Farber, Henry S. 1983. “The Determination of the
Union Status of Workers.” Econometrica, 51(5):
1417–37.
Frisch, Ragnar. 2009. Problems and Methods of Econo­
metrics: The Poincaré Lectures of Ragnar Frisch,
1933. New York: Routledge.
Granger, Clive W. J. 1969. “Investigating Causal Relations by Econometric Models and Cross-Spectral
Methods.” Econometrica, 37(3): 424–38.
Gronau, Reuben. 1974. “Wage Comparisons—A Selectivity Bias.” Journal of Political Economy, 82(6):
1119–43.
Hahn, Jinyong, Petra E. Todd, and Wilbert Van der
Klaauw. 2001. “Identification and Estimation of
Treatment Effects with a Regression-Discontinuity
Design.” Econometrica, 69(1): 201–09.
Hansen, Lars Peter, and James J. Heckman. 1996. “The
Empirical Foundations of Calibration.” Journal of
Economic Perspectives, 10(1): 87–104.
Harberger, Arnold C. 1964. “The Measurement of
Waste.” American Economic Review, 54(3): 58–76.
Hausman, Jerry A. 1978. “Specification Tests in Econometrics.” Econometrica, 46(6): 1251–71.
Heckman, James J. 1974. “Shadow Prices, Market
Wages, and Labor Supply.” Econometrica, 42(4):
679–94.
Heckman, James J. 1990. “Varieties of Selection Bias.”
American Economic Review, 80(2): 313–18.
Heckman, James J. 1992. “Haavelmo and the Birth of
Modern Econometrics: A Review of The History of
Econometric Ideas by Mary Morgan.” Journal of Eco­
nomic Literature, 30(2): 876–86.
Heckman, James J. 1996. “Randomization as an Instrumental Variable.” Review of Economics and Statis­
tics, 78(2): 336–41.
Heckman, James J. 1997. “Instrumental Variables: A

395

Study of Implicit Behavioral Assumptions Used in
Making Program Evaluations.” Journal of Human
Resources, 32(3): 441–62.
Heckman, James J. 2000. “Causal Parameters and Policy Analysis in Economics: A Twentieth Century Retrospective.” Quarterly Journal of Economics, 115(1):
45–97.
Heckman, James J. 2001. “Micro Data, Heterogeneity,
and the Evaluation of Public Policy: Nobel Lecture.”
Journal of Political Economy, 109(4): 673–748.
Heckman, James J. 2008. “Econometric Causality.”
International Statistical Review, 76(1): 1–27.
Heckman, James J., and Bo E. Honoré. 1990. “The
Empirical Content of the Roy Model.” Economet­
rica, 58(5): 1121–49.
Heckman, James J., and V. Joseph Hotz. 1989. “Choosing among Alternative Nonexperimental Methods
for Estimating the Impact of Social Programs: The
Case of Manpower Training.” Journal of the Ameri­
can Statistical Association, 84(408): 862–74.
Heckman, James J., Robert J. LaLonde, and Jeffrey
A. Smith. 1999. “The Economics and Econometrics
of Active Labor Market Programs.” In Handbook of
Labor Economics, Volume 3A, ed. Orley Ashenfelter
and David Card, 1865–2097. Amsterdam; New York
and Oxford: Elsevier Science, North-Holland.
Heckman, James J., Lance Lochner, and Christopher
Taber. 1998a. “Explaining Rising Wage Inequality: Explorations with a Dynamic General Equilibrium Model of Labor Earnings with Heterogeneous
Agents.” Review of Economic Dynamics, 1(1): 1–58.
Heckman, James J., Lance Lochner, and Christopher
Taber. 1998b. “General-Equilibrium Treatment
Effects: A Study of Tuition Policy.” American Eco­
nomic Review, 88(2): 381–86.
Heckman, James J., Lance Lochner, and Christopher
Taber. 1998c. “Tax Policy and Human-Capital Formation.” American Economic Review, 88(2): 293–97.
Heckman, James J., and Richard Robb. 1985. “Alternative Methods for Evaluating the Impact of Interventions.” In Longitudinal Analysis of Labor Market
Data, ed. James J. Heckman and Burton Singer, 156–
245. Cambridge; New York and Sydney: Cambridge
University Press.
Heckman, James J., Daniel Schmierer, and Sergio
Urzua. Forthcoming. “Testing the Correlated Random Coefficient Model.” Journal of Econometrics.
Heckman, James J., and Jeffrey A. Smith. 1995.
“Assessing the Case for Social Experiments.” Journal
of Economic Perspectives, 9(2): 85–110.
Heckman, James J., and Jeffrey A. Smith. 1998. “Evaluating the Welfare State.” In Econometrics and Eco­
nomic Theory in the Twentieth Century: The Ragnar
Frisch Centennial Symposium, ed. Steinar Strøm,
241–318. Cambridge; New York and Melbourne:
Cambridge University Press.
Heckman, James J., Jeffrey A. Smith, and Nancy Clements. 1997. “Making the Most Out of Programme
Evaluations and Social Experiments: Accounting for
Heterogeneity in Programme Impacts.” Review of
Economic Studies, 64(4): 487–535.

396

Journal of Economic Literature, Vol. XLVIII (June 2010)

Heckman, James J., and Sergio Urzua. 2010. “Comparing IV with Structural Models: What Simple IV
Can and Cannot Identify.” Journal of Econometrics,
156(1): 27–37.
Heckman, James J., Sergio Urzua, and Edward J. Vytlacil. 2006. “Understanding Instrumental Variables
in Models with Essential Heterogeneity.” Review of
Economics and Statistics, 88(3): 389–432.
Heckman, James J., Sergio Urzua, and Edward J. Vytlacil. Forthcoming. “Instrumental Variables in Models with Multiple Outcomes: The General Unordered
Case.” Les Annales d’Economie et de Statistique.
Heckman, James J., and Edward J. Vytlacil. 1999.
“Local Instrumental Variables and Latent Variable
Models for Identifying and Bounding Treatment
Effects.” Proceedings of the National Academy of
Sciences, 96(8): 4730-34.
Heckman, James J., and Edward J. Vytlacil. 2000. “The
Relationship between Treatment Parameters within
a Latent Variable Framework.” Economics Letters,
66(1): 33–39.
Heckman, James J., and Edward J. Vytlacil. 2001a.
“Instrumental Variables, Selection Models, and Tight
Bounds on the Average Treatment Effect.” In Econo­
metric Evaluation of Labour Market Policies, ed.
Michael Lechner and Friedhelm Pfeiffer, 1–15. Heidelberg and New York: Physica; Mannheim: Centre
for European Economic Research.
Heckman, James J., and Edward J. Vytlacil. 2001b.
“Local Instrumental Variables.” In Nonlinear Sta­
tistical Modeling: Proceedings of the Thirteenth
International Symposium in Economic Theory and
Econometrics: Essays in Honor of Takeshi Amemiya,
ed. Cheng Hsiao, Kimio Morimune, and James L.
Powell, 1–46. Cambridge; New York and Melbourne:
Cambridge University Press.
Heckman, James J., and Edward J. Vytlacil. 2001c.
“Policy-Relevant Treatment Effects.” American Eco­
nomic Review, 91(2): 107–11.
Heckman, James J., and Edward J. Vytlacil. 2005.
“Structural Equations, Treatment Effects, and
Econometric Policy Evaluation.” Econometrica,
73(3): 669–738.
Heckman, James J., and Edward J. Vytlacil. 2007a.
“Econometric Evaluation of Social Programs, Part I:
Causal Models, Structural Models and Econometric
Policy Evaluation.” In Handbook of Econometrics,
Volume 6B, ed. James J. Heckman and Edward E.
Leamer, 4779–4874. Amsterdam and Oxford: Elsevier, North-Holland.
Heckman, James J., and Edward J. Vytlacil. 2007b.
“Econometric Evaluation of Social Programs, Part
II: Using the Marginal Treatment Effect to Organize Alternative Econometric Estimators to Evaluate Social Programs, and to Forecast their Effects in
New Environments.” In Handbook of Econometrics,
Volume 6B, ed. James J. Heckman and Edward E.
Leamer, 4875–5144. Amsterdam and Oxford: Elsevier, North-Holland.
Hendry, David F. 1980. “Econometrics—Alchemy or
Science?” Economica, 47(188): 387–406.

Holland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association,
81(396): 945–60.
Hurwicz, Leonid. 1962. “On the Structural Form of
Interdependent Systems.” In Logic, Methodology
and Philosophy of Science, ed. Ernest Nagel, P. Suppes, and A. Tarski, 232–39. Stanford: Stanford University Press.
Ichimura, Hidehiko, and Christopher Taber. 2002.
“Semiparametric Reduced-Form Estimation of
Tuition Subsidies.” American Economic Review,
92(2): 286–92.
Imbens, Guido W., and Joshua D. Angrist. 1994. “Identification and Estimation of Local Average Treatment
Effects.” Econometrica, 62(2): 467–75.
Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.
“Recent Developments in the Econometrics of Program Evaluation.” Journal of Economic Literature,
47(1): 5–86.
Katz, Lawrence F., and David H. Autor. 1999. “Changes
in the Wage Structure and Earnings Inequality.”
In Handbook of Labor Economics, Volume 3A, ed.
Orley Ashenfelter and David Card, 1463–1555.
Amsterdam; New York and Oxford: Elsevier Science,
North-Holland.
Katz, Lawrence F., and Kevin M. Murphy. 1992.
“Changes in Relative Wages, 1963–1987: Supply and
Demand Factors.” Quarterly Journal of Economics,
107(1): 35–78.
Keane, Michael P. 2010. “A Structural Perspective on
the Experimentalist School.” Journal of Economic
Perspectives, 24(2): 47–58.
Keane, Michael P., Petra E. Todd, and Kenneth I.
Wolpin. Forthcoming. “The Structural Estimation of
Behavioral Models: Discrete Choice Dynamic Programming Methods and Applications.” In Handbook
of Labor Economics, Volume 4.
Killingsworth, Mark R., and James J. Heckman. 1986.
“Female Labor Supply: A Survey.” In Handbook of
Labor Economics, Volume 1, ed. Orley Ashenfelter
and Richard Layard, 103–204. Amsterdam; Oxford
and Tokyo: North-Holland
Klein, Lawrence R. 1953. A Textbook of Econometrics.
Evanston: Row, Peterson and Co.
Klein, Roger W., and Richard H. Spady. 1993. “An Efficient Semiparametric Estimator for Binary Response
Models.” Econometrica, 61(2): 387–421.
Kydland, Finn E., and Edward C. Prescott. 1996. “The
Computational Experiment: An Econometric Tool.”
Journal of Economic Perspectives, 10(1): 69–85.
LaLonde, Robert J. 1986. “Evaluating the Econometric
Evaluations of Training Programs with Experimental
Data.” American Economic Review, 76(4): 604–20.
Leamer, Edward E. 1983. “Let’s Take the Con Out of
Econometrics.” American Economic Review, 73(1):
31–43.
Leamer, Edward E. 2010. “Tantalus on the Road to
Asymptopia.” Journal of Economic Perspectives,
24(2): 31–46.
Lee, Donghoon, and Kenneth I. Wolpin. 2006. “Intersectoral Labor Mobility and the Growth of the

Heckman: Building Bridges
­Service Sector.” Econometrica, 74(1): 1–46.
Lewis, H. Gregg. 1986. Union Wage Effects: A Survey.
Chicago: University of Chicago Press.
Lise, Jeremy, Shannon Seitz, and Jeffrey Smith. 2005.
“Equilibrium Policy Experiments and the Evaluation
of Social Programs.” Queen’s University Department
of Economics Working Paper 1076.
Lise, Jeremy, Shannon Seitz, and Jeffrey Smith. 2006.
“Evaluating Search and Matching Models Using
Experimental Data.” Queen’s University Department of Economics Working Paper 1074.
Liu, Ta-Chung. 1960. “Underidentification, Structural
Estimation, and Forecasting.” Econometrica, 28(4):
855–65.
Manski, Charles F. 1988. “Identification of Binary
Response Models.” Journal of the American
­Statistical Association, 83(403): 729–38.
Manski, Charles F. 1990. “Nonparametric Bounds on
Treatment Effects.” American Economic Review,
80(2): 319–23.
Manski, Charles F. 1993. “Identification of Endogenous
Social Effects: The Reflection Problem.” Review of
Economic Studies, 60(3): 531–42.
Manski, Charles F. 1995. Identification Problems in the
Social Sciences. Cambridge and London: Harvard
University Press.
Manski, Charles F. 2003. Partial Identification of Prob­
abilities Distributions. New York and Heidelberg:
Springer.
Manski, Charles F. 2004. “Measuring Expectations.”
Econometrica, 72(5): 1329–76.
Marschak, Jacob. 1953. “Economic Measurements for
Policy and Prediction.” In Studies in Econometric
Method, ed. William C. Hood and Tjalling C. Koopmans, 1–26. New York: Wiley.
Marshall, Alfred. 1890. Principles of Economics. London: Macmillan and Company.
Matzkin, Rosa L. 1992. “Nonparametric and Distribution-Free Estimation of the Binary Threshold Crossing and the Binary Choice Models.” Econometrica,
60(2): 239–70.
Matzkin, Rosa L. 1993. “Nonparametric Identification
and Estimation of Polychotomous Choice Models.”
Journal of Econometrics, 58(1–2): 137–68.
Matzkin, Rosa L. 1994. “Restrictions of Economic
Theory in Nonparametric Methods.” In Handbook
of Econometrics, Volume 4, ed. Robert F. Engle and
Daniel L. McFadden, 2523–58. Amsterdam; London
and New York: Elsevier, North-Holland.
Matzkin, Rosa L. 2007. “Nonparametric Identification.” In Handbook of Econometrics, Volume 6B, ed.
James J. Heckman and Edward E. Leamer, 5307–68.
Amsterdam and Oxford: Elsevier, North-Holland.
Matzkin, Rosa L. 2010a. “Estimation of Nonparametric
Models with Simultaneity.” Unpublished.
Matzkin, Rosa L. 2010b. “Identification in Nonparametric Limited Dependent Variable Models with
Simultaneity and Unobserved Heterogeneity.”
Unpublished.
Nevo, Aviv, and Michael D. Whinston. 2010. “Taking the Dogma Out of Econometrics: Structural

397

­ odeling and Credible Inference.” Journal of Eco­
M
nomic Perspectives, 24(2): 69–82.
Neyman, Jerzy. 1923. “Statistical Problems in Agricultural Experiments.” Journal of the Royal Statistical
Society II (Supplement 2): 107–80.
Pencavel, John H. 1986. “Labor Supply of Men: A Survey.” In Handbook of Labor Economics, Volume 1,
ed. Orley Ashenfelter and Richard Layard, 3–102.
Amsterdam; Oxford and Tokyo: North-Holland.
Poirier, Dale J. 1980. “Partial Observability in Bivariate Probit Models.” Journal of Econometrics, 12(2):
209–17.
Powell, James L. 1994. “Estimation of Semiparametric
Models.” In Handbook of Econometrics, Volume 4,
ed. Robert F. Engle and Daniel L. McFadden, 2443–
2521. Amsterdam; London and New York: Elsevier,
North-Holland.
Quandt, Richard E. 1958. “The Estimation of the
Parameters of a Linear Regression System Obeying
Two Separate Regimes.” Journal of the American
Statistical Association, 53(284): 873–80.
Quandt, Richard E. 1972. “A New Approach to Estimating Switching Regressions.” Journal of the Ameri­
can Statistical Association, 67(338): 306–10.
Rosenzweig, Mark R., and Kenneth I. Wolpin. 2000.
“Natural ‘Natural Experiments’ in Economics.” Jour­
nal of Economic Literature, 38(4): 827–74.
Roy, A. D. 1951. “Some Thoughts on the Distribution of Earnings.” Oxford Economic Papers, 3(2):
135–46.
Rubin, Donald B. 1974. “Estimating Causal Effects
of Treatments in Randomized and Nonrandomized
Studies.” Journal of Educational Psychology, 66(5):
688–701.
Rubin, Donald B. 1978. “Bayesian Inference for Causal
Effects: The Role of Randomization.” Annals of Sta­
tistics, 6(1): 34–58.
Rubin, Donald B. 1986. “Statistics and Causal Inference: Comment: Which Ifs Have Causal Answers.”
Journal of the American Statistical Association,
81(396): 961–62.
Shadish, William R., Thomas D. Cook, and Donald T.
Campbell. 2002. Experimental and Quasi-Experi­
mental Designs for Generalized Causal Inference.
Boston: Houghton Mifflin.
Shimer, Robert, and Ivan Werning. 2008. “Liquidity
and Insurance for the Unemployed.” American Eco­
nomic Review, 98(5): 1922–42.
Sims, Christopher A. 1972. “Money, Income, and Causality.” American Economic Review, 62(4): 540–52.
Sims, Christopher A. 1980. “Macroeconomics and
Reality.” Econometrica, 48(1): 1–48.
Sims, Christopher A. 1996. “Macroeconomics and
Methodology.” Journal of Economic Perspectives,
10(1): 105–20.
Sims, Christopher A. 2010. “But Economics Is Not an
Experimental Science.” Journal of Economic Per­
spectives, 24(2): 59–68.
Stock, James H. 1989. “Nonparametric Policy Analysis.” Journal of the American Statistical Association,
84(406): 567–75.

398

Journal of Economic Literature, Vol. XLVIII (June 2010)

Stock, James H. 2010. “The Other Transformation in
Econometric Practice: Robust Tools for Inference.”
Journal of Economic Perspectives, 24(2): 83–94.
Tamer, Elie. 2003. “Incomplete Simultaneous Discrete
Response Model with Multiple Equilibria.” Review
of Economic Studies, 70(1): 147–65.
Todd, Petra E., and Kenneth I. Wolpin. 2006. “Assessing the Impact of a School Subsidy Program in Mexico: Using a Social Experiment to Validate a Dynamic
Behavioral Model of Child Schooling and Fertility.”
American Economic Review, 96(5): 1384–1417.
Tukey, John W. 1986. “Alternative Methods for Solving the Problem of Selection Bias in Evaluating the
Impact of Treatments on Outcomes: Comment.”
In Drawing Inferences from Self-Selected Sam­
ples, ed. Howard Wainer, 108–110. New York:
­Springer-Verlag.

Tunali, Insan. 2000. “Rationality of Migration.” Inter­
national Economic Review, 41(4): 893–920.
Vella, Francis. 1998. “Estimating Models with Sample Selection Bias: A Survey.” Journal of Human
Resources, 33(1): 127–69.
Vytlacil, Edward J. 2002. “Independence, Monotonicity, and Latent Index Models: An Equivalence
Result.” Econometrica, 70(1): 331–41.
Willis, Robert J., and Sherwin Rosen. 1979. “Education
and Self-Selection.” Journal of Political Economy,
87(5): S7–36.
Wu, De-Min. 1973. “Alternative Tests of Independence
between Stochastic Regressors and Disturbances.”
Econometrica, 41(4): 733–50.
Yitzhaki, Shlomo. 1989. “On Using Linear Regressions
in Welfare Economics.” Hebrew University Department of Economics Working Paper 217.

This article has been cited by:
1. Halbert White, Karim Chalak. 2013. Identification and Identification Failure for Treatment Effects
Using Structural Systems. Econometric Reviews 32:3, 273-317. [CrossRef]
2. Martin Petrick, Patrick Zier. 2012. Common Agricultural Policy effects on dynamic labour use in
agriculture. Food Policy 37:6, 671-678. [CrossRef]
3. Lance Lochner, Alexander Monge-Naranjo. 2012. Credit Constraints in Education. Annual Review
of Economics 4:1, 225-256. [CrossRef]
4. Paolo Frumento, Fabrizia Mealli, Barbara Pacini, Donald B. Rubin. 2012. Evaluating the Effect
of Training on Wages in the Presence of Noncompliance, Nonemployment, and Missing Outcome
Data. Journal of the American Statistical Association 107:498, 450-466. [CrossRef]
5. Alec Ian Gershberg, Pablo Alberto Gonz##lez, Ben Meade. 2012. Understanding and
Improving Accountability in Education: A Conceptual Framework and Guideposts from Three
Decentralization Reform Experiences in Latin America. World Development 40:5, 1024-1041.
[CrossRef]
6. D. A. Miteva, S. K. Pattanayak, P. J. Ferraro. 2012. Evaluation of biodiversity policy instruments:
what works and what doesn't?. Oxford Review of Economic Policy 28:1, 69-92. [CrossRef]
7. John M. Brooks, Elizabeth A. Chrischilles, Mary Beth Landrum, Kara B. Wright, Gang Fang, Eric
P. Winer, Nancy L. Keating. 2012. Survival Implications Associated with Variation in Mastectomy
Rates for Early-Staged Breast Cancer. International Journal of Surgical Oncology 2012, 1-9.
[CrossRef]
8. Victoria Zinde-Walsh. 2011. Presidential Address: Mathematics in economics and econometrics.
Canadian Journal of Economics/Revue canadienne d'##conomique 44:4, 1052-1068. [CrossRef]
9. Pedro Carneiro, , James J. Heckman, , Edward J. Vytlacil. 2011. Estimating Marginal Returns to
Education. American Economic Review 101:6, 2754-2781. [Abstract] [View PDF article] [PDF
with links]
10. Fran##ois Claveau. 2011. Evidential variety as a source of credibility for causal inference: beyond
sharp designs and structural models. Journal of Economic Methodology 18:3, 233-253. [CrossRef]
11. Peter Arcidiacono, Paul B. Ellickson. 2011. Practical Methods for Estimation of Dynamic Discrete
Choice Models. Annual Review of Economics 3:1, 363-394. [CrossRef]
12. Jens Ludwig, , Jeffrey R. Kling, , Sendhil Mullainathan. 2011. Mechanism Experiments and Policy
Evaluations. Journal of Economic Perspectives 25:3, 17-38. [Abstract] [View PDF article] [PDF
with links]
13. G. W. Harrison. 2011. Randomisation and Its Discontents. Journal of African Economies 20:4,
626-652. [CrossRef]
14. G. W. Harrison. 2011. Experimental methods and the welfare evaluation of policy lotteries.
European Review of Agricultural Economics 38:3, 335-360. [CrossRef]
15. Andrew Dillon. 2011. Do Differences in the Scale of Irrigation Projects Generate Different Impacts
on Poverty and Production?. Journal of Agricultural Economics 62:2, 474-492. [CrossRef]
16. Judea Pearl. 2011. Statistics and Causality: Separated to Reunite-Commentary on Bryan Dowd's
###Separated at Birth###. Health Services Research 46:2, 421-429. [CrossRef]

17. Bryan E. Dowd. 2011. Separated at Birth: Statisticians, Social Scientists, and Causality in Health
Services Research. Health Services Research 46:2, 397-420. [CrossRef]
18. Ian M. McCarthy, Rusty TchernisOn the Estimation of Selection Models when Participation is
Endogenous and Misclassified 27, 179-207. [CrossRef]
19. C. B. Barrett, M. R. Carter. 2010. The Power and Pitfalls of Experiments in Development
Economics: Some Non-random Reflections. Applied Economic Perspectives and Policy 32:4,
515-548. [CrossRef]
20. Donna K. Ginther. 2010. AN INTERVIEW WITH JAMES J. HECKMAN. Macroeconomic
Dynamics 14:04, 548-584. [CrossRef]

