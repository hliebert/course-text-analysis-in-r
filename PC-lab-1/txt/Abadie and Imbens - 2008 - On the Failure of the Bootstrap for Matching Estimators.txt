Econometrica, Vol. 76, No. 6 (November, 2008), 1537‚Äì1557

NOTES AND COMMENTS
ON THE FAILURE OF THE BOOTSTRAP FOR
MATCHING ESTIMATORS
BY ALBERTO ABADIE

AND

GUIDO W. IMBENS 1

Matching estimators are widely used in empirical economics for the evaluation of
programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case
with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods
in this setting are the analytic asymptotic variance estimator of Abadie and Imbens
(2006a) as well as certain modifications of the standard bootstrap, like the subsampling
methods in Politis and Romano (1994).
KEYWORDS: Average treatment effects, bootstrap, matching.

1. INTRODUCTION
MATCHING METHODS have become very popular for the estimation of treatment effects in the absence of experimental data.2 Researchers using matching
methods often apply the bootstrap to calculate the standard errors. However,
bootstrap inference for matching estimators has not been formally justified.
This article addresses the question of the validity of the standard bootstrap
for nearest-neighbor matching estimators with replacement and a fixed number of neighbors. We focus on the case of a fixed number of neighbors because
it conforms to the usual practice in empirical economics, where researchers applying matching estimators typically employ nearest-neighbor matching with a
very limited number of neighbors (e.g., one). We show in a simple case, with a
single continuous covariate, that the standard bootstrap fails to provide asymptotically valid standard errors, in spite of the fact that the matching estimator is
root-N consistent and asymptotically normal with no asymptotic bias. We show
that the average bootstrap variance can overestimate as well as underestimate
the asymptotic variance of matching estimators. We provide some intuition for
the failure of the bootstrap in this context.3
1
We are grateful for comments by Peter Bickel, St√©phane Bonhomme, Joel Horowitz, Francis Kramarz, Whitney Newey, seminar participants at Princeton, CEMFI, CREST, and Harvard/MIT, and two anonymous reviewers. Financial support for this research was generously provided through NSF Grants SES-0350645 (Abadie), SES-0136789, and SES-0452590 (Imbens).
2
For example, Dehejia and Wahba (1999). See Rosenbaum (2001) and Imbens (2004) for surveys.
3
Other examples of failure of the bootstrap arise in the contexts of estimating the maximum
of the support of a random variable (Bickel and Freedman (1981)), estimating the average of a

¬© 2008 The Econometric Society

DOI: 10.3982/ECTA6474

1538

A. ABADIE AND G. W. IMBENS

There are valid alternatives to the bootstrap for inference with matching
estimators. In Abadie and Imbens (2006a), we derived conditions under which
the nearest-neighbor matching estimator with replacement and a fixed number
of matches is root-N consistent and asymptotically normal, and we proposed
an analytic estimator of the asymptotic variance. Under those conditions, the
validity of certain alternatives to the bootstrap, such as subsampling (Politis
and Romano (1994)) or the M-out-of-N bootstrap (Bickel, G√∂tze, and van
Zwet (1997)), can be established from general results.4
2. SETUP
2.1. Basic Model
In this article we adopt the standard model of treatment effects under unconfoundedness (Rosenbaum and Rubin (1983), Heckman, Ichimura, and Todd
(1998), Rosenbaum (2001), Imbens (2004)). The goal is to evaluate the effect
of a treatment on the basis of data on outcomes, treatments, and covariates
for treated and untreated units. We have a random sample of N0 units from
the control (untreated) population and a random sample of N1 units from the
treated population, with total sample size N = N0 + N1 . Each unit is characterized by a pair of potential outcomes, Yi (0) and Yi (1), denoting the outcomes
under the control and active treatments, respectively. We observe Yi (0) for
units in the control sample and Yi (1) for units in the treated sample. For all
units, we observe a covariate vector, Xi .5 Let Wi indicate whether a unit is from
the control sample (Wi = 0) or the treatment sample (Wi = 1). For each unit,
we observe the triple (Xi  Wi  Yi ), where Yi = Wi Yi (1) + (1 ‚àí Wi )Yi (0) is the
observed outcome. Let X be an N-column matrix with column i equal to Xi
and assume analogous notation for Y and W.
In this article, we focus on matching estimation of the average treatment
effect for the treated:
œÑ = E[Yi (1) ‚àí Yi (0)|Wi = 1]
We make the following two identifying assumptions:
variable with infinite variance (Arthreya (1987)), and superefficient estimation (Beran (1984)).
Resampling inference in these contexts can be conducted using alternative methods such as subsampling (Politis and Romano (1994)) and versions of the bootstrap where the size of the bootstrap sample is smaller than the sample size (e.g., Bickel, G√∂tze, and van Zwet (1997)). See Hall
(1992) and Horowitz (2003) for general discussions.
4
See, for example, Politis, Romano, and Wolf (1999).
5
To simplify our proof of lack of validity of the bootstrap, we will consider in our calculations
the case of a scalar covariate. With higher dimensional covariates there is the additional complication of biases that may dominate the asymptotic distribution of matching estimators (Abadie
and Imbens (2006a)).

BOOTSTRAP FOR MATCHING ESTIMATORS

1539

ASSUMPTION 2.1‚ÄîUnconfoundedness: For almost all x, (Yi (1) Yi (0)) is independent of Wi conditional on Xi = x or
(Yi (0) Yi (1)) ‚ä•
‚ä• Wi | Xi = x (a.s.).
ASSUMPTION 2.2‚ÄîOverlap: For some c > 0 and almost all x,
c ‚â§ Pr(Wi = 1|Xi = x) ‚â§ 1 ‚àí c
A nearest-neighbor matching estimator of œÑ matches each treated unit i to
the control unit j with the closest value for the covariate and then averages the
within-pair outcome differences, Yi ‚àí Yj , over the N1 matched pairs. In this
article, we focus on the case of matching with replacement, so each control
unit may be used as a match for more than one treated unit.
For each treated unit i, let Di be the distance between the covariate value
for observation i and the covariate value for the closest untreated match:
Di =

min

j=1N:Wj =0

Xi ‚àí Xj 

Then let

J (i) = {j ‚àà {1 2     N} : Wj = 0 Xi ‚àí Xj  = Di }
be the set of closest matches for treated unit i. If unit i is an untreated unit, then
J (i) is defined to be the empty set. When Xi is continuously distributed, the set
J (i) will consist of a single index with probability 1, but for bootstrap samples
there will often be more than one index in this set (because an observation
from the original sample may appear multiple times in the bootstrap sample).
For each treated unit, i, let
YÃÇi (0) =


1
Yj
#J (i) j‚ààJ (i)

be the average outcome in the set of the closest matches for observation i,
where #J (i) is the number of elements of the set J (i). The matching estimator of œÑ is then
(1)

œÑÃÇ =

1 
(Yi ‚àí YÃÇi (0))
N1 i:W =1
i

1540

A. ABADIE AND G. W. IMBENS

For the subsequent discussion, it is useful to write the estimator in a different
way. Let Ki denote the weighted number of times unit i is used as a match (if
unit i is an untreated unit, with Ki = 0 if unit i is a treated unit):
‚éß
if Wi = 1,
‚é™
‚é® 0

1
Ki =
 if Wi = 0.
1{i ‚àà J (j)}
‚é™
‚é©
#
J
(j)
W =1
j

Then we can write
(2)

œÑÃÇ =

N
1 
(Wi ‚àí (1 ‚àí Wi )Ki )Yi 
N1 i=1

Let
‚éß
0
‚é™
‚é®
2


1
Ksqi =
1{i
‚àà
J
(j)}

‚é™
‚é©
#J (j)

if Wi = 1,
if Wi = 0.

Wj =1

In Abadie and Imbens (2006a) we proved that under certain conditions (for
example, when Xi is a scalar variable) the nearest-neighbor matching estimator in (1) is root-N consistent and asymptotically normal with zero asymptotic
bias.6 We also proposed a consistent estimator for the asymptotic variance of œÑÃÇ:
V AI =

N
N
1 
1  2
2
(Y
‚àí
YÃÇ
(0)
‚àí
œÑÃÇ)
+
(K ‚àí Ksqi )œÉ 2 (Xi  Wi )
i
i
N12 i=1
N12 i=1 i

where œÉ 2 (Xi  Wi ) is an estimator of the conditional variance of Yi given Wi and
Xi , based on matching. Let l(i), be the closest match to unit i, in terms of the
covariates, among the units with the same value for the treatment (that is, units
in the treatment groups are matched to units in the treatment group, and units
in the control group are matched to units in the control group).7 Then
1
œÉ 2 (Xi  Wi ) = (Yi ‚àí Yl(i) )2 
2
6
More generally, in Abadie and Imbens (2007), we proposed a bias correction that makes
matching estimators root-N consistent and asymptotically normal regardless of the dimension of
Xi .
7
To simplify the notation, here we consider only the case without matching ties. The extension
to accommodate ties is immediate, but it is not required for the purpose of the analysis in this
article.

BOOTSTRAP FOR MATCHING ESTIMATORS

1541

Let V(œÑÃÇ) be the variance of œÑÃÇ. In Abadie and Imbens (2006a), we showed
that (under regularity conditions) the normalized version of the variance estimator, N1 V AI is consistent for the normalized variance, N1 V(œÑÃÇ):
p

N1 (V(œÑÃÇ) ‚àí V AI ) ‚àí‚Üí 0
2.2. The Bootstrap
In this article we consider two versions of the bootstrap variance commonly
used in empirical research. The first version centers the bootstrap variance at
the matching estimate in the original sample. The second version centers the
bootstrap variance at the mean of the bootstrap distribution of the matching
estimator.
Consider a random sample Z = (X W Y) with N0 controls and N1 treated
units. The matching estimator, œÑÃÇ, is a functional t(¬∑) of the original sample:
œÑÃÇ = t(Z). We construct a bootstrap sample, Zb , with N0 controls and N1 treated
by sampling with replacement from the two subsamples. We then calculate the
bootstrap estimator, œÑÃÇb , applying the functional t(¬∑) to the bootstrap sample:
œÑÃÇb = t(Zb ). We denote expectations over the bootstrap distribution (conditional on the sample) as E[¬∑|Z]. The first version of the bootstrap variance is
the second moment of (œÑÃÇb ‚àí œÑÃÇ) conditional on Z:
V BI = E[(œÑÃÇb ‚àí œÑÃÇ)2 |Z]
The second version of the bootstrap variance centers the bootstrap variance at
the bootstrap mean, E[œÑÃÇb |Z], rather than at the original estimate, œÑÃÇ:
V BII = E (œÑÃÇb ‚àí E[œÑÃÇb |Z])2 |Z 
Although these bootstrap variances are defined in terms of the original
sample Z, in practice an easier way to calculate them is by drawing B bootstrap samples. Given B bootstrap samples with bootstrap estimates œÑÃÇb , for
b = 1     B, we can obtain unbiased estimators for these two variances as
1
(œÑÃÇb ‚àí œÑÃÇ)2 
B b=1
B

VÃÇ BI =

1 
œÑÃÇb ‚àí
=
B ‚àí 1 b=1
B

VÃÇ

BII

and
1
œÑÃÇb
B b=1
B

2



We will focus on the first bootstrap variance, V BI , and its expectation,
E[V BI ]. We shall show that, in general, N1 (E[V BI ] ‚àí V(œÑÃÇ)) does not converge
to zero. We will show that in some cases the limit of N1 (E[V BI ] ‚àí V(œÑÃÇ)) is
positive and that in other cases this limit is negative. As a result, we will show

1542

A. ABADIE AND G. W. IMBENS

that N1 V BI is not a consistent estimator of the limit of N1 V(œÑÃÇ). This will indirectly imply that N1 V BII is not consistent either. Because E[(œÑÃÇb ‚àí œÑÃÇ)2 |Z] ‚â•
E[(œÑÃÇb ‚àí E[œÑÃÇb |Z])2 |Z], it follows that E[V BI ] ‚â• E[V BII ]. Thus in the cases where
the limit of N1 (E[V BI ] ‚àí V(œÑÃÇ)) is smaller than zero, it follows that the limit of
N1 (E[V BII ] ‚àí V(œÑÃÇ)) is also smaller than zero.
3. AN EXAMPLE WHERE THE BOOTSTRAP FAILS
In this section we discuss in detail a specific example where we can calculate
the limits of N1 V(œÑÃÇ) and N1 E[V BI ].
3.1. Data Generating Process
We consider the following data generating process (DGP):
ASSUMPTION 3.1: The marginal distribution of the covariate X is uniform on
the interval [0 1].
ASSUMPTION 3.2: The ratio of treated and control units is N1 /N0 = Œ± for some
Œ± > 0.
ASSUMPTION 3.3: The propensity score, e(x) = Pr(Wi = 1|Xi = x), is constant.
ASSUMPTION 3.4: The distribution of Yi (1) is degenerate with Pr(Yi (1) = œÑ) =
1, and the conditional distribution of Yi (0) given Xi = x is normal with mean 0
and variance 1.
It follows from Assumptions 3.2 and 3.3 that the propensity score is e(x) =
Œ±/(1 + Œ±).
3.2. Exact Variance and Large Sample Distribution
The data generating process implies that, conditional on X = x, the average
treatment effect is equal to E[Yi (1) ‚àí Yi (0)|Xi = x] = œÑ for all x. Therefore,
the average treatment
effect forthe treated is equal to œÑ. Under this data gen
erating process i Wi Yi /N1 = i Wi Yi (1)/N1 = œÑ, which along with equation
(2) implies
œÑÃÇ = œÑ ‚àí

N
1 
Ki Yi 
N1 i=1

Conditional on X and W, the only stochastic component of œÑÃÇ is Y. By Assumption 3.4, given Wi = 0, the Yi ‚Äôs are mean zero, unit variant, and independent

BOOTSTRAP FOR MATCHING ESTIMATORS

1543

of X. Thus E[œÑÃÇ|X W] = œÑ. Because (i) E[Yi Yj |Wi = 0 X W] = 0 for i = j,
(ii) E[Yi2 |Wi = 0 X W] = 1, and (iii) Ki is a deterministic function of X and
W, it also follows that the conditional variance of œÑÃÇ given X and W is
V(œÑÃÇ|X W) =

N
1  2
K 
N12 i=1 i

The variance of the matching estimator is equal to the variance of E[œÑÃÇ|X W]
plus the expectation of V(œÑÃÇ|X W). Because V(E[œÑÃÇ|X W]) = V(œÑ) = 0, the exact unconditional variance of the matching estimator equals the expected value
of the conditional variance:
(3)

V(œÑÃÇ) = E(V(œÑÃÇ|X W)) =

N0
E[Ki2 |Wi = 0]
N12

LEMMA 3.1‚ÄîExact Variance: Suppose that Assumptions 2.1, 2.2, and 3.1‚Äì3.4
hold. Then:
(i) The exact variance of the matching estimator is
V(œÑÃÇ) =

3 (N1 ‚àí 1)(N0 + 8/3)
1

+
N1 2 N1 (N0 + 1)(N0 + 2)

(ii) As N ‚Üí ‚àû,
3
N1 V(œÑÃÇ) ‚Üí 1 + Œ±
2


‚àö
3
d
(iii) N 1 (œÑÃÇ ‚àí œÑ) ‚àí‚Üí N 0 1 + Œ± 
2
See the Appendix for proofs.
3.3. The Bootstrap Variance
Now we analyze the properties of the bootstrap variance, V BI . As before, let
Z = (X W Y) denote the original sample. Notice that
(4)

E[V BI ] = E E[(œÑÃÇb ‚àí œÑÃÇ)2 |Z] = E[(œÑÃÇb ‚àí œÑÃÇ)2 ]

is the expected bootstrap variance. Notice also that the expectation E[(œÑÃÇb ‚àí
œÑÃÇ)2 |Z] is taken over the bootstrap distribution (conditional on Z). The expectation E[(œÑÃÇb ‚àí œÑÃÇ)2 ] averages E[(œÑÃÇb ‚àí œÑÃÇ)2 |Z] over the population distribution of
Z. Let Kbi be the number of times that unit i in the original sample is used as
a match in bootstrap sample b. For the DGP of Section 3.1,
(5)

œÑÃÇb = œÑ ‚àí

N
1 
Kbi Yi 
N1 i=1

1544

A. ABADIE AND G. W. IMBENS

From equations (2) and (5) we obtain
2
N
1 
(Kbi ‚àí Ki )Yi
E[(œÑÃÇb ‚àí œÑÃÇ) ] = E
N1 i=1


N
1 
N0
2
=E
(Kbi ‚àí Ki ) = 2 E (Kbi ‚àí Ki )2 |Wi = 0 
2
N1 i=1
N1



(6)

2

The following lemma establishes the limit of N1 E[V BI ] under our DGP.
LEMMA 3.2 ‚ÄîBootstrap Variance: Suppose that Assumptions 3.1‚Äì3.4 hold.
Then, as N ‚Üí ‚àû,
(7)

3 5 exp(‚àí1) ‚àí 2 exp(‚àí2)
+ 2 exp(‚àí1)
N1 E[V BI ] ‚Üí 1 + Œ±
2
3(1 ‚àí exp(‚àí1))

Recall that the limit of the normalized variance of œÑÃÇ is 1 + (3/2)Œ±. For small
values of Œ± the limit of the expected bootstrap variance exceeds the limit variance by the third term in (7), 2 exp(‚àí1) 074, or 74%. For large values of Œ±,
the second term in (7) dominates and the ratio of the limit expected bootstrap
and limit variance is equal to the factor in the second term of (7) multiplying (3/2)Œ±. Since (5 exp(‚àí1) ‚àí 2 exp(‚àí2))/(3(1 ‚àí exp(‚àí1))) 083, it follows
that as Œ± increases, the ratio of the limit expected bootstrap variance to the
limit variance converges to 0.83, suggesting that in large samples the bootstrap
variance can under- as well as overestimate the true variance.
3.4. Failure of the Bootstrap
So far, we have established the relationship between the limiting variance
of the estimator and the limit of the average bootstrap variance. We end this
section with a discussion of the implications of the previous two lemmas for
the validity of the bootstrap. The bootstrap provides a valid estimator of the
asymptotic variance of the simple matching estimator if

 p
N1 E[(œÑb ‚àí œÑ)2 |Z] ‚àí V(œÑ) ‚àí‚Üí 0
Lemmas 3.1 and 3.2 show that
3
N1 V(œÑ) ‚àí‚Üí 1 + Œ±
2
and
3 5 exp(‚àí1) ‚àí 2 exp(‚àí2)
N1 E[(œÑb ‚àí œÑ)2 ] ‚àí‚Üí 1 + Œ±
+ 2 exp(‚àí1)
2
3(1 ‚àí exp(‚àí1))

BOOTSTRAP FOR MATCHING ESTIMATORS

1545

Assume that the bootstrap provides a valid estimator of the asymptotic variance of the simple matching estimator. Then
3
p
N1 E[(œÑb ‚àí œÑ)2 |Z] ‚àí‚Üí 1 + Œ±
2
Because N1 E[(œÑb ‚àí œÑ)2 |Z] ‚â• 0, it follows by the Portmanteau lemma (see, e.g.,
van der Vaart (1998, p. 6)) that, as N ‚Üí ‚àû,
3
1 + Œ± ‚â§ lim E N1 E[(œÑb ‚àí œÑ)2 |Z] = lim N1 E[(œÑb ‚àí œÑ)2 ]
2
3 5 exp(‚àí1) ‚àí 2 exp(‚àí2)
+ 2 exp(‚àí1)
=1+ Œ±
2
3(1 ‚àí exp(‚àí1))
However, the algebraic inequality
3
3 5 exp(‚àí1) ‚àí 2 exp(‚àí2)
1+ Œ±‚â§1+ Œ±
+ 2 exp(‚àí1)
2
2
3(1 ‚àí exp(‚àí1))
does not hold for large enough Œ±. As a result, the bootstrap does not provide a
valid estimator of the asymptotic variance of the simple matching estimator.
The second version of the bootstrap provides a valid estimator of the asymptotic variance of the simple matching estimator if
 p

N1 E (œÑb ‚àí E[œÑb |Z])2 |Z ‚àí V(œÑ) ‚àí‚Üí 0
Assume that the second version of the bootstrap provides a valid estimator of
the asymptotic variance of the simple matching estimator. Then
3
p
N1 E (œÑb ‚àí E[œÑb |Z])2 |Z ‚àí‚Üí 1 + Œ±
2
Notice that E[(œÑb ‚àí E[œÑb |Z])2 |Z] ‚â§ E[(œÑb ‚àí œÑ)2 |Z]. By the Portmanteau lemma,
as N ‚Üí ‚àû,
3
1 + Œ± ‚â§ lim inf E N1 E (œÑb ‚àí E[œÑb |Z])2 |Z
2
‚â§ lim E N1 E[(œÑb ‚àí œÑ)2 |Z] = lim N1 E[(œÑb ‚àí œÑ)2 ]
3 5 exp(‚àí1) ‚àí 2 exp(‚àí2)
=1+ Œ±
+ 2 exp(‚àí1)
2
3(1 ‚àí exp(‚àí1))
Again, this inequality does not hold for large enough Œ±. As a result, the second
version of the bootstrap does not provide a valid estimator of the asymptotic
variance of the simple matching estimator.

1546

A. ABADIE AND G. W. IMBENS

Because the variance is an unbounded functional, inconsistency of the bootstrap variance does not necessarily imply inconsistency of the bootstrap estimator of the asymptotic distribution of matching estimators. However, using an
argument similar
‚àö to the one applied above, it is easy to see that the bootstrap
distribution ‚àö
of N1 (œÑb ‚àí œÑ) is not consistent, in general, for the asymptotic distribution of N1 (œÑ ‚àí œÑ). The
‚àö reason is that if the bootstrap is consistent for the
N1 (œÑ ‚àí œÑ), then the limit inferior of the variance
asymptotic
distribution
of
‚àö
of N1 (œÑb ‚àí œÑ) should not be smaller than 1 + (3/2)Œ±, which we have shown
happens for large enough Œ±.
As is apparent from equation (2), the matching estimator becomes linear after conditioning on X and W. The reason is that K1      KN are fixed
once we condition on X and W. This implies that the wild bootstrap of H√§rdle and Mammen (1993) can be used to estimate the conditional distribution of matching estimators.8 The reason why the bootstrap fails to reproduce
the unconditional distribution of œÑÃÇ is that the bootstrap fails to reproduce
the distribution of Ki , even in large samples. To gain some intuition about
this, consider the DGP of Section 3.1. Equations (3), (4), and (6) imply that
N1 (E[V BI ] ‚àí V(œÑÃÇ)) ‚Üí 0 if and only if E[(Kbi ‚àí Ki )2 |Wi = 0] ‚àí E[Ki2 |Wi = 0] ‚Üí
0. Consider the situation when Œ± = N1 /N0 is small. Then, because the number of control units is large relative to the number of treated units, most observations in the control group are used as a match no more than once, so
Pr(Ki > 1|Wi = 0) is small. In a bootstrap sample, however, treated units can
appear multiple times. Every time that a treated unit appears in the bootstrap
sample, this unit is matched to the same control unit, creating instances in
which Kbi ‚àí Ki > 1. This problem does not disappear by increasing the sample
size. As a result, even in large samples, the bootstrap fails to reproduce the
distribution of Ki and, in particular, it fails to reproduce E[Ki2 |Wi = 0].
4. CONCLUSION
The results in this article have an immediate implication for empirical practice: bootstrap standard errors are not valid as the basis for inference with simple nearest-neighbor matching estimators with replacement and a fixed number of neighbors. In Abadie and Imbens (2006a), we proposed a valid estimator
of the variance of matching estimators that is based on a normal approximation to the asymptotic distribution of these estimators. Simulation results in
Abadie and Imbens (2006b) suggest that the analytic standard errors proposed
in Abadie and Imbens (2006a) work well even in fairly small samples. Alternative inferential methods for matching estimators are the subsampling method
of Politis and Romano (1994) and the M-out-of-N bootstrap of Bickel, G√∂tze,
and van Zwet (1997).
8

We are grateful to a referee for suggesting this.

BOOTSTRAP FOR MATCHING ESTIMATORS

1547

In this article we consider only simple nearest-neighbor matching estimators
with a fixed number of matches. Heckman, Ichimura, and Todd (1998) have
proposed kernel-based matching methods for which the number of matches
increases with the sample size. Because these estimators are asymptotically
linear, we anticipate that the bootstrap provides valid inference. The same conjecture applies to other asymptotically linear estimators of average treatment
effects, such as the propensity score weighting estimator proposed by Hirano,
Imbens, and Ridder (2003). In addition, if Xi includes only discrete covariates
with a finite number of possible values, then a simple matching estimator can
be constructed to match each observation in the treatment group to all untreated observations with the same value of Xi . This matching estimator is just
a weighted average of differences in means across groups defined by the values
of the covariates. As a result, the standard bootstrap provides valid inference
in this context.
APPENDIX
Before proving Lemma 3.1, we introduce some notation and preliminary
results. Let X1      XN be a random sample from a continuous distribution.
Let Mj be the index of the closest match for unit j. That is, if Wj = 1, then Mj
is the unique index (ties happen with probability 0), with WMj = 0, such that
Xj ‚àí XMj  ‚â§ Xj ‚àí Xi  for all i such that Wi = 0. If Wj = 0, then Mj = 0. Let
Ki be the number of times unit i is the closest match for a treated observation:
Ki = (1 ‚àí Wi )

N


Wj 1{Mj = i}

j=1

Following this definition, Ki is zero for treated units. Using this notation, we
can write the estimator for the average treatment effect on the treated as
œÑÃÇ =

N
1 
(Wi ‚àí Ki )Yi 
N1 i=1

Also, let Pi be the probability that the closest match for a randomly chosen
treated unit j is unit i, conditional on both the vector of treatment indicators
W and on the vector of covariates for the control units X0 :
Pi = Pr(Mj = i|Wj = 1 W X0 )
For treated units, we define Pi = 0.
The following lemma provides some properties of the order statistics of a
sample from the standard uniform distribution.

1548

A. ABADIE AND G. W. IMBENS

LEMMA A.1: Let X(1) ‚â§ X(2) ‚â§ ¬∑ ¬∑ ¬∑ ‚â§ X(N) be the order statistics of a random
sample of size N from a standard uniform distribution, U(0 1). Then, for 1 ‚â§ i ‚â§
j ‚â§ N,
r
E X(i)
(1 ‚àí X(j) )s =

i[r] (N ‚àí j + 1)[s]

(N + 1)[r+s]

where for a positive integer a and a nonnegative integer b: a[b] = (a + b ‚àí 1)!/(a ‚àí
1)!. Moreover, for 1 ‚â§ i ‚â§ N, X(i) has a Beta distribution with parameters (i N ‚àí
i + 1); for 1 ‚â§ i ‚â§ j ‚â§ N, (X(j) ‚àí X(i) ) has a Beta distribution with parameters
(j ‚àí i N ‚àí (j ‚àí i) + 1).
The proof of this lemma and of other preliminary lemmas in this appendix
are available in the working paper version of this article (Abadie and Imbens
(2006b)).
Notice that the lemma implies the following results:
i
for 1 ‚â§ i ‚â§ N
N +1
i(i + 1)
2
E[X(i)
for 1 ‚â§ i ‚â§ N
]=
(N + 1)(N + 2)
E[X(i) ] =

E[X(i) X(j) ] =

i(j + 1)
(N + 1)(N + 2)

for

1 ‚â§ i ‚â§ j ‚â§ N

First we investigate the first two moments of Ki , starting by studying the conditional distribution of Ki given X0 and W.
LEMMA A.2 ‚ÄîConditional Distribution and Moments of Ki : Suppose that
Assumptions 3.1‚Äì3.3 hold. Then the distribution of Ki conditional on Wi = 0, W,
and X0 is binomial with parameters (N1  Pi ):
Ki |Wi = 0 W X0 ‚àº B (N1  Pi )
This implies the following conditional moments for Ki :
E[Ki |W X0 ] = (1 ‚àí Wi )N1 Pi 
E[Ki2 |W X0 ] = (1 ‚àí Wi )(N1 Pi + N1 (N1 ‚àí 1)Pi2 )
To derive the marginal moments of Ki we need first to analyze the properties
of the random variable Pi . Exchangeability of the units implies that the marginal expectation of Pi given N0 , N1 , and Wi = 0 is equal to 1/N0 . To derive the
second moment of Pi , it is helpful to express Pi in terms of the order statistics

BOOTSTRAP FOR MATCHING ESTIMATORS

1549

of the covariates for the control group. For control unit i, let Œπ(i) be the order
of the covariate for the ith unit among control units:
Œπ(i) =

N

(1 ‚àí Wj )1{Xj ‚â§ Xi }
j=1

Furthermore, let X0(k) be the kth order statistic of the covariates among
the control units, so that X0(1) ‚â§ X0(2) ‚â§ ¬∑ ¬∑ ¬∑ ‚â§ X0(N0 ) , and for control units,
X0(Œπ(i)) = Xi . Ignoring ties, which happen with probability zero, a treated unit
with covariate value x will be matched to control unit i if
X0(Œπ(i)+1) + X0(Œπ(i))
X0(Œπ(i)‚àí1) + X0(Œπ(i))
<x<

2
2
if 1 < Œπ(i) < N0 . If Œπ(i) = 1, then x will be matched to unit i if
x<

X0(2) + X0(1)

2

and if Œπ(i) = N0 , x will be matched to unit i if
X0(N0 ‚àí1) + X0(N0 )
< x
2
To obtain Pi , we need to integrate the density of X conditional on W = 1, f1 (x),
over these sets. With a uniform distribution for the covariates in the treatment
group (f1 (x) = 1 for x ‚àà [0 1]), we obtain the following representation for Pi :
‚éß
if Œπ(i) = 1,
‚é® (X0(2) + X0(1) )/2
if 1 < Œπ(i) < N0 ,
Pi = (X0(Œπ(i)+1) ‚àí X0(Œπ(i)‚àí1) )/2
(A.1)
‚é©
1 ‚àí (X0(N0 ‚àí1) + X0(N0 ) )/2 if Œπ(i) = N0 .
LEMMA A.3‚ÄîMoments of Pi : Suppose that Assumptions 3.1‚Äì3.3 hold. Then:
(i) The second moment of Pi conditional on Wi = 0 is
E[Pi2 |Wi = 0] =

3N0 + 8

2N0 (N0 + 1)(N0 + 2)

(ii) The Mth moment of Pi is bounded by

E[PiM |Wi = 0] ‚â§

1+M
N0 + 1

M


The proof of this lemma follows from equation (A.1) and Lemma A.1 (see
Abadie and Imbens (2006b)).

1550

A. ABADIE AND G. W. IMBENS

PROOF OF LEMMA 3.1: First we prove (i). The first step is to calculate
E[Ki2 |Wi = 0]. Using Lemmas A.2 and A.3,
E[Ki2 |Wi = 0] = N1 E[Pi |Wi = 0] + N1 (N1 ‚àí 1)E[Pi2 |Wi = 0]
=

N1 3 N1 (N1 ‚àí 1)(N0 + 8/3)

+
N0 2 N0 (N0 + 1)(N0 + 2)

Substituting this into (3), we get
V(œÑÃÇ) =

N0
1
3 (N1 ‚àí 1)(N0 + 8/3)

E[Ki2 |Wi = 0] =
+
2
N1 2 N1 (N0 + 1)(N0 + 2)
N1

proving part (i).
Next, consider part (ii). Multiply the exact variance of œÑÃÇ by N1 and substitute
N1 = Œ±N0 to get
N1 V(œÑÃÇ) = 1 +

3 (Œ±N0 ‚àí 1)(N0 + 8/3)

2 (N0 + 1)(N0 + 2)

Then take the limit as N0 ‚Üí ‚àû to get
3
lim N1 V(œÑÃÇ) = 1 + Œ±
N‚Üí‚àû
2
Finally, consider part (iii). Let S(r j) be a Stirling number of the second
kind. For any nonnegative integer M, the Mth moment of Ki given W and X0
is (Johnson, Kotz, and Kemp (1993))
E[K |X0  Wi = 0] =
M
i

M
j

S(M j)N1 !Pi
j=0

(N1 ‚àí j)!



Therefore, applying Lemma A.3(ii), we obtain that the moments of Ki are
uniformly bounded:
E[KiM |Wi = 0] =

M

S(M j)N1 !
j=0

‚â§

j

E[Pi |Wi = 0]


j
M

S(M j)N1 ! 1 + M
j=0

‚â§

(N1 ‚àí j)!

M

j=0

(N1 ‚àí j)!

N0 + 1

S(M j)Œ±j (1 + M)j 

BOOTSTRAP FOR MATCHING ESTIMATORS

1551

Notice that



N
1  2
N0
3
E
Ki =
E[Ki2 |Wi = 0] ‚Üí 1 + Œ±
N1 i=1
N1
2

V

N
N0
1  2
Ki ‚â§ 2 V(Ki2 |Wi = 0) ‚Üí 0
N1 i=1
N1

because cov(Ki2  Kj2 |Wi = Wj = 0 i = j) ‚â§ 0 (see Joag-Dev and Proschan
(1983)). Therefore,
N
1  2 p
3
Ki ‚Üí 1 + Œ±
N1 i=1
2

Finally, we write
œÑÃÇ ‚àí œÑ =

N
1 
Œæi 
N1 i=1

where Œæi = ‚àíKi Yi . Conditional on X and W, the Œæi are independent, and the
distribution of Œæi is degenerate at zero for Wi = 1 and normal N (0 Ki2 ) for
Wi = 0. Hence, for any c ‚àà R,
Pr

N
1  2
K
N1 i=1 i

‚àí1/2





N1 (œÑÃÇ ‚àí œÑ) ‚â§ c X W =

(c)

where (¬∑) is the cumulative distribution function of a standard normal variable. Integrating over the distribution of X and W yields
Pr

N
1  2
K
N1 i=1 i

‚àí1/2



N1 (œÑÃÇ ‚àí œÑ) ‚â§ c =

Now, Slustky‚Äôs theorem implies (iii).

(c)
Q.E.D.

Next we introduce some additional notation. Let Rbi be the number of
times unit i is in the bootstrap sample. In addition, let Dbi be an indicator
for inclusion of unit i in the bootstrap sample, so that Dbi = 1{Rbi > 0}. Let
N
Nb0 = i=1 (1 ‚àí Wi )Dbi be the number of distinct control units in the bootstrap
sample. Finally, define the binary variable Bi (x) for i = 1     N to be the indicator for the event that in the bootstrap sample a treated unit with covariate
value x would be matched to unit i. That is, for this indicator to be equal to 1,

1552

A. ABADIE AND G. W. IMBENS

the following three conditions need to be satisfied: (i) unit i is a control unit,
(ii) unit i is in the bootstrap sample, and (iii) the distance between Xi and x is
less than or equal to the distance between x and any other control unit in the
bootstrap sample. Formally,

min
|x ‚àí Xk | and Dbi = 1 Wi = 0,
1 if |x ‚àí Xi | =
k:Wk =0Dbk =1
Bi (x) =
0 otherwise.
For the N units in the original sample, let Kbi be the number of times unit i is
used as a match in the bootstrap sample:
Kbi =

N


Wj Bi (Xj )Rbj 

j=1

Equation (6) implies
N1 E[V BI ] =

1
E[(Kbi ‚àí Ki )2 |Wi = 0]
Œ±

The first step in deriving this expectation is to establish some properties of Dbi ,
Rbi , Nb0 , and Bi (x).
LEMMA A.4‚ÄîProperties of Dbi , Rbi , Nb0 , and Bi (x): Suppose that Assumptions 3.1‚Äì3.3 hold. Then, for w ‚àà {0 1} and n ‚àà {1     N0 }:
(i) Rbi |Wi = w Z ‚àº B (Nw  1/Nw );
(ii) Dbi |Wi = w Z ‚àº B (1 1 ‚àí (1 ‚àí 1/Nw )Nw );
 0 
N
(iii) Pr(Nb0 = n) = NN‚àín
(n!/N0 0 )S(N0  n);
0
(iv) Pr(Bi (Xj ) = 1|Wj = 1 Wi = 0 Dbi = 1 Nb0 ) = 1/Nb0 ;
(v) For l = j,


Pr Bi (Xl )Bi (Xj ) = 1|Wj = Wl = 1 Wi = 0 Dbi = 1 Nb0
=

3Nb0 + 8
;
2Nb0 (Nb0 + 1)(Nb0 + 2)

(vi) E[Nb0 /N0 ] = 1 ‚àí (1 ‚àí 1/N0 )N0 ‚Üí 1 ‚àí exp(‚àí1);
(vii) (1/N0 )V(Nb0 ) = (N0 ‚àí 1)(1 ‚àí 2/N0 )N0 + (1 ‚àí 1/N0 )N0 ‚àí N0 (1 ‚àí
1/N0 )2N0 ‚Üí exp(‚àí1)(1 ‚àí 2 exp(‚àí1))
Next, we prove a general result for the bootstrap. Consider a sample of size
N, indexed by i = 1     N. Let Dbi indicate whether observation i is in bootN
strap sample b. Let Nb = i=1 Dbi be the number of distinct observations in
bootstrap sample b.

BOOTSTRAP FOR MATCHING ESTIMATORS

1553

LEMMA A.5‚ÄîBootstrap: For all m ‚â• 0,
m 

N ‚àí Nb
‚Üí exp(‚àím)
E
N
and


E

N
Nb

m 


‚Üí

1
1 ‚àí exp(‚àí1)

m


LEMMA A.6‚ÄîApproximate Bootstrap K Moments: Suppose that Assumptions 3.1‚Äì3.3 hold. Then:
2
(i) E[Kbi
|Wi = 0] ‚Üí 2Œ± + 32 (Œ±2 /(1 ‚àí exp(‚àí1)));
(ii) E[Kbi Ki |Wi = 0] ‚Üí (1 ‚àí exp(‚àí1))(Œ± + 32 Œ±2 ) + Œ±2 exp(‚àí1)
PROOF: Here we prove (i). The proof of part (ii) is similar in spirit, but much
longer (see Abadie and Imbens (2006b)). Notice that for i j l, such that Wi = 0
and Wj = Wl = 1,
‚ä• Dbi  Bi (Xj ) Bi (Xl )
(Rbj  Rbl ) ‚ä•
Notice also that {Rbj : Wj = 1} are exchangeable with

Rbj = N1 
Wj =1

Therefore, applying Lemma A.4(i), for Wj = Wl = 1,
cov(Rbj  Rbl ) = ‚àí

1 ‚àí 1/N1
V(Rbj )
=‚àí
‚àí‚Üí 0
(N1 ‚àí 1)
(N1 ‚àí 1)

As a result,
E[Rbj Rbl |Dbi = 1 Bi (Xj ) = Bi (Xl ) = 1 Wi = 0 Wj = Wl = 1 j = l]

‚àí E[Rbj |Dbi = 1 Bi (Xj ) = Bi (Xl ) = 1 Wi = 0
2
Wj = Wl = 1 j = l] ‚àí‚Üí 0
By Lemma A.4(i),
E[Rbj |Dbi = 1 Bi (Xj ) = Bi (Xl ) = 1 Wi = 0 Wj = Wl = 1 j = l] = 1
Therefore,
E[Rbj Rbl |Dbi = 1 Bi (Xj ) = Bi (Xl ) = 1 Wi = 0 Wj = Wl = 1 j = l]
‚àí‚Üí 1

1554

A. ABADIE AND G. W. IMBENS

In addition,
E[R2bj |Dbi = 1 Bi (Xj ) = 1 Wj = 1 Wi = 0]
= N1 (1/N1 ) + N1 (N1 ‚àí 1)(1/N12 ) ‚àí‚Üí 2
Notice that
Pr(Dbi = 1|Wi = 0 Wj = Wl = 1 j = l Nb0 )
= Pr(Dbi = 1|Wi = 0 Nb0 ) =

Nb0

N0

Using Bayes‚Äô rule,
Pr(Nb0 = n|Dbi = 1 Wi = 0 Wj = Wl = 1 j = l)
= Pr(Nb0 = n|Dbi = 1 Wi = 0)
=

Pr(Dbi = 1|Wi = 0 Nb0 = n) Pr(Nb0 = n)
Pr(Dbi = 1|Wi = 0)

=

n/N0 Pr(Nb0 = n)

1 ‚àí (1 ‚àí 1/N0 )N0

Therefore,
N0 Pr(Bi (Xj ) = 1|Dbi = 1 Wi = 0 Wj = 1)
= N0

N0


Pr(Bi (Xj ) = 1|Dbi = 1 Wi = 0 Wj = 1 Nb0 = n)

n=1

√ó Pr(Nb0 = n|Dbi = 1 Wi = 0 Wj = 1)
 
N0

1
1 n
Pr(Nb0 = n)
= N0
=
N
n N0 1 ‚àí (1 ‚àí 1/N0 ) 0
1 ‚àí (1 ‚àí 1/N0 )N0
n=1
‚àí‚Üí

1

1 ‚àí exp(‚àí1)

In addition,


N02 Pr Bi (Xj )Bi (Xl )|Dbi = 1 Wi = 0 Wj = Wl = 1 j = l Nb0

2
N02 (Nb0 + 8/3)
3
3
1
p
=
‚àí‚Üí

2 Nb0 (Nb0 + 1)(Nb0 + 2)
2 1 ‚àí exp(‚àí1)

1555

BOOTSTRAP FOR MATCHING ESTIMATORS

Therefore,
N0



2

N02 Pr(Bi (Xj )Bi (Xl )|Dbi = 1 Wi = 0 Wj = Wl = 1 j = l Nb0 )

n=1

√ó Pr(Nb0 = n|Dbi = 1 Wi = 0 Wj = Wl = 1 j = l)
2
N0 

3 N02 (n + 8/3)
n/N0 Pr(Nb0 = n)
=
2 n(n + 1)(n + 2) 1 ‚àí (1 ‚àí 1/N0 )N0
n=1


N0
1
N04 (n + 8/3)2
9
‚â§
Pr(Nb0 = n)
4 1 ‚àí exp(‚àí1) n=1
n6
Notice that
N0

N 4 (n + 8/3)2
0

n6

n=1

Pr(Nb0 = n)



 N0  4
16 64 
N0
‚â§ 1+
+
Pr(Nb0 = n)
3
9 n=1 n
which is bounded away from infinity (this is shown in the proof of Lemma A.5).
Convergence in probability of a random variable along with boundedness of
its second moment implies convergence of the first moment (see, e.g., van der
Vaart (1998)). As a result,


N02 Pr Bi (Xj )Bi (Xl )|Dbi = 1 Wi = 0 Wj = Wl = 1 j = l

2
3
1
‚àí‚Üí

2 1 ‚àí exp(‚àí1)
Then, using these preliminary results, we obtain
2
E[Kbi
|Wi = 0]

 N N



Wj Wl Bi (Xj )Bi (Xl )Rbj Rbl Wi = 0
=E
j=1 l=1

=E

 N




Wj Bi (Xj )R Wi = 0



2
bj

j=1



+E

N


j=1 l=j



Wj Wl Bi (Xj )Bi (Xl )Rbj Rbl Wi = 0



1556

A. ABADIE AND G. W. IMBENS

= N1 E[R2bj |Dbi = 1 Bi (Xj ) = 1 Wj = 1 Wi = 0]
√ó Pr(Bi (Xj ) = 1|Dbi = 1 Wj = 1 Wi = 0)
√ó Pr(Dbi = 1|Wj = 1 Wi = 0)
+ N1 (N1 ‚àí 1)E[Rbj Rbl |Dbi = 1 Bi (Xj ) = Bi (Xl ) = 1
Wj = Wl = 1 j = l Wi = 0]
√ó Pr(Bi (Xj )Bi (Xl ) = 1|Dbi = 1 Wj = Wl = 1 j = l Wi = 0)
√ó Pr(Dbi = 1|Wj = Wl = 1 j = l Wi = 0)
‚àí‚Üí 2Œ± +

Œ±2
3

2 (1 ‚àí exp(‚àí1))

Q.E.D.

PROOF OF LEMMA 3.2: From preliminary results,
N1 E[V BI ]
1
2
(E[Kbi
|Wi = 0] ‚àí 2E[Kbi Ki |Wi = 0] + E[Ki2 |Wi = 0])
Œ±

1
3
Œ±2
‚Üí
2Œ± +
Œ±
2 (1 ‚àí exp(‚àí1))



3 2
3 2
exp(‚àí1)
2
‚àí 2(1 ‚àí exp(‚àí1)) Œ± + Œ± +
Œ± +Œ±+ Œ±
2
1 ‚àí exp(‚àí1)
2


3
3
=Œ±
‚àí 3(1 ‚àí exp(‚àí1)) ‚àí 2 exp(‚àí1) +
2(1 ‚àí exp(‚àí1))
2
=

+ 2 ‚àí 2 + 2 exp(‚àí1) + 1
3 5 exp(‚àí1) ‚àí 2 exp(‚àí2)
+ 2 exp(‚àí1)
=1+ Œ±
2
3(1 ‚àí exp(‚àí1))

Q.E.D.

REFERENCES
ABADIE, A., AND G. IMBENS (2006a): ‚ÄúLarge Sample Properties of Matching Estimators for Average Treatment Effects,‚Äù Econometrica, 74, 235‚Äì267. [1538,1540,1541,1546]
(2006b): ‚ÄúOn the Failure of the Bootstrap for Matching Estimators,‚Äù Working Paper,
available at http://www.ksg.harvard.edu/fs/aabadie. [1546,1548,1549,1553]
(2007): ‚ÄúBias-Corrected Matching Estimators for Average Treatment Effects,‚Äù Unpublished Manuscript, John F. Kennedy School of Government, Harvard University. [1540]
ARTHREYA, K. (1987): ‚ÄúBootstrap of the Mean in the Infinite Variance Case,‚Äù Annals of Statistics,
15, 724‚Äì731. [1538]
BERAN, R. (1984): ‚ÄúBootstrap Methods in Statistics,‚Äù Jahresberichte des Deutschen Mathematischen Vereins, 86, 14‚Äì30. [1538]
BICKEL, P., AND D. FREEDMAN (1981): ‚ÄúSome Asymptotic Theory for the Bootstrap,‚Äù Annals of
Statistics, 9, 1196‚Äì1217. [1537]

BOOTSTRAP FOR MATCHING ESTIMATORS

1557

BICKEL, P., F. G√ñTZE, AND W. VAN ZWET (1997): ‚ÄúResampling Fewer Than n Observations:
Gains, Losses and Remedies for Losses,‚Äù Statistica Sinica, 7, 1‚Äì31. [1538,1546]
DEHEJIA, R., AND S. WAHBA (1999): ‚ÄúCausal Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs,‚Äù Journal of the American Statistical Association, 94,
1053‚Äì1062. [1537]
HALL, P. (1992): The Bootstrap and Edgeworth Expansions. New York: Springer Verlag. [1538]
H√ÑRDLE, W., AND E. MAMMEN (1993): ‚ÄúComparing Nonparametric versus Parametric Regression Fits,‚Äù Annals of Statistics, 21, 1926‚Äì1947. [1546]
HECKMAN, J., H. ICHIMURA, AND P. TODD (1998): ‚ÄúMatching as an Econometric Evaluation
Estimator,‚Äù Review of Economic Studies, 65, 261‚Äì294. [1538,1547]
HIRANO, K., G. IMBENS, AND G. RIDDER (2003): ‚ÄúEfficient Estimation of Average Treatment
Effects Using the Estimated Propensity Score,‚Äù Econometrica, 71, 1161‚Äì1189. [1547]
HOROWITZ, J. (2003): ‚ÄúThe Bootstrap,‚Äù in Handbook of Econometrics, Vol. 5, ed. by J. Heckman
and E. Leamer. New York: Elsevier Science. [1538]
IMBENS, G. (2004): ‚ÄúNonparametric Estimation of Average Treatment Effects Under Exogeneity,‚Äù Review of Economics and Statistics, 86, 4‚Äì29. [1537,1538]
JOAG-DEV, AND F. PROSCHAN (1983): ‚ÄúNegative Association of Random Variables, With Applications,‚Äù The Annals of Statistics, 11, 286‚Äì295. [1551]
JOHNSON, N., S. KOTZ, AND A. KEMP (1993): Univariate Discrete Distributions (Second Ed.). New
York: Wiley. [1550]
POLITIS, N., AND J. ROMANO (1994): ‚ÄúLarge Sample Confidence Regions Based on Subsamples
Under Minimal Assumptions,‚Äù The Annals of Statistics, 22, 2031‚Äì2050. [1538,1546]
POLITIS, N., J. ROMANO, AND M. WOLF (1999): Subsampling. New York: Springer Verlag. [1538]
ROSENBAUM, P. (2001): Observational Studies (Second Ed.). New York: Springer Verlag. [1537,
1538]
ROSENBAUM, P., AND D. RUBIN (1983): ‚ÄúThe Central Role of the Propensity Score in Observational Studies for Causal Effects,‚Äù Biometrika, 70, 41‚Äì55. [1538]
VAN DER VAART, A. (1998): Asymptotic Statistics. New York: Cambridge University Press. [1545,
1555]

John F. Kennedy School of Government, Harvard University, 79 John F.
Kennedy Street, Cambridge, MA 02138, U.S.A. and NBER; alberto_abadie@
harvard.edu, http://www.ksg.harvard.edu/fs/aabadie/
and
Dept. of Economics, Harvard University, M-24 Litauer Center, 1805 Cambridge Street, Cambridge, MA 02138, U.S.A. and NBER; imbens@harvard.edu,
http://www.economics.harvard.edu/faculty/imbens/imbens.html.
Manuscript received May, 2006; final revision received May, 2008.

