Journal of Business & Economic Statistics

ISSN: 0735-0015 (Print) 1537-2707 (Online) Journal homepage: https://www.tandfonline.com/loi/ubes20

Including Covariates in the Regression
Discontinuity Design
Markus Frölich & Martin Huber
To cite this article: Markus Frölich & Martin Huber (2018): Including Covariates in
the Regression Discontinuity Design, Journal of Business & Economic Statistics, DOI:
10.1080/07350015.2017.1421544
To link to this article: https://doi.org/10.1080/07350015.2017.1421544

View supplementary material

Accepted author version posted online: 15
Jan 2018.
Published online: 05 Sep 2018.
Submit your article to this journal

Article views: 471

View Crossmark data

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=ubes20

Supplementary materials for this article are available online. Please go to http://tandfonline.com/r/JBES

Including Covariates in the Regression
Discontinuity Design
Markus FRÖLICH
Center for Evaluation and Development (C4ED), University of Mannheim, J-PAL, L7, 3-5, D-68131 Mannheim,
Germany (froelich@c4ed.org, froelich@uni-mannheim.de)

Martin HUBER
Department of Economics, University of Fribourg, Bd. de Pérolles 90, CH-1700 Fribourg, Switzerland
(martin.huber@unifr.ch)
This article proposes a fully nonparametric kernel method to account for observed covariates in regression
discontinuity designs (RDD), which may increase precision of treatment effect estimation. It is shown that
conditioning on covariates reduces the asymptotic variance and allows estimating the treatment effect at
the rate of one-dimensional nonparametric regression, irrespective of the dimension of the continuously
distributed elements in the conditioning set. Furthermore, the proposed method may decrease bias and
restore identification by controlling for discontinuities in the covariate distribution at the discontinuity
threshold, provided that all relevant discontinuously distributed variables are controlled for. To illustrate
the estimation approach and its properties, we provide a simulation study and an empirical application to
an Austrian labor market reform. Supplementary materials for this article are available online.
KEY WORDS: Causal effect; Complier; Endogeneity; LATE; Nonparametric regression; Treatment
effect.

1. INTRODUCTION

poorly in the RDD, where a good fit is only needed at the treatment threshold, see Gelman and Imbens (2018). Extrapolation
from far-away data points is also inherent in linear regression
where one linearly controls for covariates.) Consider the setup
of the RDD: D is a binary treatment indicator, Y is the outcome
variable of interest, and Z is the “forcing variable” with a known
threshold z0 at which the treatment probability Pr(D = 1|Z) is
discontinuous. There are various motivations for accounting for
covariates, denoted by X. A first reason is variance reduction,
which is well known for the parametric case. But gains in precision can also be achieved in the nonparametric setup, as flexibly
including covariates and averaging them out in an appropriate
way reduces the asymptotic variance of the estimated treatment
effect. We show that under mild regularity conditions, incorporating covariates permits estimating the treatment effect at
the rate for one-dimensional nonparametric regression, that is,
2
n− 5 (where n is the sample size), irrespective of the dimension of the continuously distributed elements in X. Hence,
the curse of dimensionality does not apply due to smoothing
over X.
Second, as pointed out by Imbens and Lemieux (2008),
covariates may mitigate small sample biases in cases where
the number of observations close to the threshold z0 is small
such that one has to include observations in the estimation that
are further apart and may potentially differ in X. Controlling
for X might eliminate some of the bias that is introduced by

The regression discontinuity design (RDD) has received
tremendous attention in many fields, for example, labor markets,
political economy, health, education, psychology, criminology,
as a credible approach to identifying causal effects without having to resort to fully randomized experiments. Hahn, Todd, and
van der Klaauw (2001) formalized the assumptions required to
identify causal effects in the RDD and provided nonparametric (local linear) estimators. Porter (2003) complemented their
work by alternative estimators. Lee and Card (2008) considered the case when the forcing variable is discrete. McCrary
(2008) proposed a test for the manipulation of the running variable related to the continuity of its density function. Imbens
and Lemieux (2008), van der Klaauw (2008), and Lee and
Lemieux (2010) surveyed the applied and theoretical literature on the RDD. Imbens and Kalyanaraman (2012) discussed
optimal bandwidth selection in terms of squared error loss,
while Calonico, Cattaneo, and Titiunik (2014) proposed methods for robust inference along with optimal bandwidth selection.
Dong (2014) presented an alternative to some of the identifying
assumptions in Hahn, Todd, and van der Klaauw (2001).
In this article, the regression discontinuity approach is
extended to incorporate covariates in a fully nonparametric
way. Our estimator is based on a local nonparametric regression approach, that is, kernel-based estimation, which allows
deriving closed-form expressions for bias and variance. (An
alternative approach could use global nonparametric methods
such as sieves or polynomials of increasing order. However,
such global methods, which are capable of fitting regression
curves at many points by means of extrapolation, may perform

© 2018 American Statistical Association
Journal of Business & Economic Statistics
XXXX 2018, Vol. 0, No. 0
DOI: 10.1080/07350015.2017.1421544
1

2

Journal of Business & Economic Statistics, XXXX 2018

observations further away from the threshold, as illustrated by
Black, Galdo, and Smith (2007). However, biases related to
unobserved characteristics cannot be accounted for.
Third, we also permit for situations where the density f (X|Z)
is discontinuous at z0 , which may point to a failure of the RDD
assumptions, see Lee (2008), such that the simple RDD estimator is generally inconsistent. Our approach nevertheless
identifies a local treatment effect in cases in which X contains
all variables that (i) are imbalanced around the threshold and (ii)
affect the outcome variable. With this respect, our contribution
distinguishes itself from a more recent article on RDD with
covariates by Calonico et al. (2018), who assume f (X|Z) to
be continuous at z0 . Under that stronger identifying condition
not needed here, Calonico et al. (2018) discussed potential
precision gains when linearly (rather than nonparametrically
as in our method) controlling for X and provided methods for
optimal bandwidth selection and robust inference.
One example for f (X|Z) being discontinuous at z0 is “classical confounding” where manipulation of Z at the threshold
is selective with respect to characteristics that may also affect
the outcome, see, for instance, Urquiola and Verhoogen (2009).
If all confounding characteristics are observed in the data, our
method yields the treatment effect on compliers at the threshold.
See also van der Klaauw (2008) for confounding in the context
of dynamic treatment assignment, where observed earlier treatment eligibility or participation (X) jointly affects the (current)
forcing variable Z and Y . As a further example, consider the case
when Z not only affects D, but also further variables that affect
Y . This may occur in spatial RDDs where Z is based on distance
to geographical borders. Eugster et al. (2017), for instance, use
the (mainly French and German) language border within administrative units of Switzerland to estimate the effects of culture
on unemployment. The authors consider a measure of the “taste
for leisure” as one particular indicator of culture. However, in
addition to this treatment variable, further community-based
covariates that are likely affected by culture also change discontinuously at the border. Controlling for X is therefore necessary
as Z would otherwise violate the exclusion restriction with
respect to Y at the threshold through its influence on X. Identification of a causal effect is, however, only obtained if X are not
“bad controls” which are affected by unobservables that also
influence Y .
The remainder of this article is organized as follows.
Section 2 discusses the identification of the treatment effect in
the presence of covariates. Section 3 proposes two estimators
and examines their properties and shows that one of them
2
achieves the n− 5 convergence rate. Section 4 provides a simulation study that (among others) illustrates the implications
of confounding related to observed covariates at the threshold
when applying RDD with and without controlling for X. Section
5 presents an empirical application to Austrian labor market
reform previously considered by Lalive (2008) to estimate the
effect of age-dependent eligibility to unemployment benefits
on unemployment duration. As employees at risk of becoming
unemployed might negotiate the exact date of dismissal with
their employers, manipulation at the age threshold is a concern.
We therefore control for a range of labor market-relevant characteristics that are potential confounders and find our results to
differ from RDD without X. Section 6 concludes.

2. RDD WITH COVARIATES
We define causal effects using the potential-outcome notation
in the framework known as the Neyman–Fisher–Rubin causal
model. (See Neyman (1923), Fisher (1935), and Rubin (1978).)
Following the setup of Hahn, Todd, and van der Klaauw (2001),
let Di ∈ {0, 1} be a binary treatment variable, let Yi0 , Yi1 be the
individual potential outcomes, and Yi1 − Yi0 the individual treatment effect. The potential outcomes as well as the treatment
effects Yi1 − Yi0 are permitted to vary across individuals, that is,
no constant treatment effect is assumed. Let Zi be a variable that
influences the treatment variable in a discontinuous way.
In the literature, two distinct designs are examined: the sharp
design where Di changes for everyone at a known threshold z0 ,
and the fuzzy design where Di changes only for a subset of individuals. In the sharp design (Trochim 1984), participation status
is given by a deterministic function of Z, for example,
Di = 1(Zi ≥ z0 ).

(1)

This implies that all individuals change program participation
status exactly at z0 . The fuzzy design, on the other hand, permits
D to also depend on other factors but assumes that the treatment
probability changes discontinuously at z0 :
lim E [D|Z = z0 + ε] − lim E [D|Z = z0 − ε] = 0.

ε→0

ε→0

(2)

Note that the fuzzy design includes the sharp design as a special
case when the left-hand side of (2) is equal to one. For this reason, the subsequent discussion mostly focuses on the more general fuzzy design. (Battistin and Rettore (2008) introduced the
mixed sharp fuzzy design as a special case of the fuzzy design.)
See Hahn, Todd, and van der Klaauw (2001) for more details.
Identification is feasible under the continuity of the mean
potential outcomes at z0 and relies on comparing the observed
outcomes of those individuals to the left of the threshold with
those to the right. In addition to continuity of E[Y d |Z = z] in
z at z0 for d = {0, 1}, Hahn, Todd, and van der Klaauw (2001)
considered two alternative identifying assumptions:
HTK1:

Yi1 − Yi0 ⊥⊥Di |Zi

for Zi near z0

(3)

or



HTK2: Yi1 − Yi0 , Di (z) ⊥
⊥Zi

near z0

and there exists e > 0

such that Di (z0 + ε) ≥ Di (z0 − ε) for all 0 < ε < e. (4)

Assumption (3) is a local selection on observables assumption and identifies the average treatment effect at the threshold:
E[Y 1 − Y 0 |Z = z0 ]. Assumption (4) is an instrumental variables assumption that identifies a local average treatment effect
(LATE) for a local group of compliers at the threshold:
lim E[Y 1 − Y 0 |D(z0 + ε) > D(z0 − ε), Z = z0 ].

ε→0

In the sharp design, everyone is a complier at z0 and assumption
(3) is meaningless (i.e., has no identifying power) such that one
needs assumption (4). In the fuzzy design, one typically invokes
(4), since the conditional independence assumption (3) does not
permit treatment selection based on individual gains Yi1 − Yi0 .
It is worth mentioning that Dong (2014) recently showed that
alternatively to (4), identification of the LATE is obtained by
making a continuity assumption of Z in the neighborhood of

Frölich and Huber: Including Covariates in the Regression Discontinuity Design

z0 . (Continuity of Z implies the smoothness of mean potential
outcomes conditional on compliance behavior and of the shares
of subgroups defined upon compliance at the threshold, which
is sufficient for identification.)
In the following, we introduce observed covariates Xi and
assume that (4) is valid conditional on X. As an example,
suppose that there exists a liberalized education market in
which schools may charge tuition fees, and that by law classes
must be split if the number of students surpasses a particular
threshold. As argued in Urquiola and Verhoogen (2009) for the
case of Chile, schools close to the threshold might adjust tuition
fees, thereby causing discontinuities in the admitted students’
socioeconomic characteristics such as household income and
parents’ education. Assume that the latter variables also affect
the outcome of interest, for example, students’ educational
degree, which implies a violation of HTK2 when assessing the
educational effect of class size. However, if household income,
parents’ education, and all other variables imbalanced at the
threshold and affecting the outcome are observed, (4) holds conditional on Xi . (Whether it is plausible to assume that all imbalanced covariates affecting the outcome are observed depends on
the empirical problem and the richness of data. In the context of
Urquiola and Verhoogen (2009), for instance, ambition might
(in addition to parents’ education and household income) play
a role for selectively (re)placing students into particular class
sizes. One would therefore want to condition on a rich set
of socio-economic household characteristics and personality
traits, for example, provided by means of a household survey.)
By an analogous reasoning as in HTK, and further assumptions
made precise below, it follows immediately that the treatment
effect on the local compliers conditional on X is identified as


lim E Y 1 − Y 0 |X, D(z0 + ε) > D(z0 − ε), Z = z0
ε→0

=

m+ (X, z0 ) − m− (X, z0 )
,
d + (X, z0 ) − d − (X, z0 )

(5)

where m+ (X, z) = limε→0 E[Y |X, Z = z + ε] and m− (X, z) =
limε→0 E[Y |X, Z = z − ε] and d + (X, z) and d − (X, z) defined
analogously with D replacing Y .
In this article, however, we focus on identifying and estimating the unconditional effect


(6)
lim E Y 1 − Y 0 |D(z0 + ε) > D(z0 − ε), Z = z0 ,
ε→0

that is, the effect on all compliers without conditioning on X.
We identify this effect by first controlling for X and thereafter
averaging over X. There are at least three reasons, why estimating the unconditional effect (6) is interesting (or even more
interesting than the conditional effect (5)). First, for the purpose
of evidence-based policy-making a small number of summary
measures can be more easily conveyed to policy makers and the
public than a large number of estimated effects at every value
of X. Second, unconditional effects can be estimated more precisely than conditional effects. Third, the definition of unconditional effects does not depend on the variables included in
X. (This, of course, is only true if X exclusively contains pretreatment variables.) One can therefore consider different sets
of control variables X and still estimate the same object, which
is useful for examining robustness of the results to the set of
control variables. See also Frölich (2007).

3

For showing identification of the unconditional effect (6), we
first introduce some further notation. Let Nε be a symmetric
ε neighborhood about z0 and partition Nε into Nε+ = {z : z ≥
z0 , z ∈ Nε } and Nε− = {z : z < z0 , z ∈ Nε }. According to their
reaction to the instrument z over Nε we can partition the population into four subpopulations:
τi,ε = a

if

Di (z) = 1

∀z ∈ Nε−

and

Di (z) = 1

∀z ∈ Nε+

τi,ε = n

if

Di (z) = 0

∀z ∈ Nε−

and

Di (z) = 0

∀z ∈ Nε+

τi,ε = c

if

Di (z) = 0

∀z ∈ Nε−

and

Di (z) = 1

∀z ∈ Nε+

τi,ε = d

if

Di (z) = 1

∀z ∈ Nε−

and

Di (z) = 0

∀z ∈ Nε+ .

These subpopulations are a straightforward extension of the
LATE concept of Imbens and Angrist (1994). The first group
contains those units that will always be treated (if Z ∈ Nε ), the
second contains those that will never be treated (if Z ∈ Nε ), and
the third and fourth group contains the units that are treated only
on one side of z0 . (In the appendix, we also consider a possible fifth group of indefinite units, for which no left-limit of
Di (z) may exist. We assume this group to not exist, that is, we
require that all units have well-defined left-limits of Di (z).) We
will assume that the fourth group, that is, the “defiers,” has measure zero for ε sufficiently small. Note that in the sharp design,
everyone is a complier for any ε > 0.
Under the following assumption, we can identify the treatment effect for the local compliers, that is, for those who switch
from D = 0 to 1 at z0 . (The conditions in Assumption 1 are very
similar, but a little weaker, to a conditional-on-X version of (4).)
It is assumed throughout that the covariates X are continuously
distributed with a Lebesgue density. This assumption is made
for convenience to ease exposition, particularly in the derivation of the asymptotic distributions later on. Discrete covariates can (at the expense of more cumbersome notation) easily
be included in X, as the derivation of the asymptotic distribution only depends on the number of continuous regressors in
X, while discrete variables do not affect the asymptotic properties. In fact, identification does not require any continuous X
variables. Only Z has to be continuous near z0 , but could have
masspoints elsewhere.
Assumption 1. For a symmetric neighborhood Nε about z0
and for almost every X
(i) Existence of compliers
limε→0 Pr(τε = c|Z ∈ Nε ) > 0
(ii) Monotonicity
limε→0 Pr(τε = c|Z ∈ Nε ) + Pr(τε = a|Z ∈ Nε ) +
Pr(τε = n|Z ∈ Nε ) = 1
(iii) Independent IV
limε→0 Pr(τε = t|X, Z ∈ Nε+ ) − Pr(τε = t|X, Z ∈
Nε− ) = 0 for t ∈ {a, n, c}
(iv) IV Exclusion
limε→0 E[Y 1 |X, Z ∈ Nε+ , τε = t] − E[Y 1 |X, Z ∈ Nε− ,
τε = t] = 0 for t ∈ {a, c}
limε→0 E[Y 0 |X, Z ∈ Nε+ ,
τε = t] − E[Y 0 |X, Z ∈ Nε− , τε = t] = 0 for t ∈ {n, c}
(v) Common support
limε→0 Supp(X|Z ∈ Nε+ ) = limε→0 Supp(X|Z ∈ Nε− )
(vi) Density at threshold
FZ (z) is differentiable at z0 and fZ (z0 ) > 0

4

Journal of Business & Economic Statistics, XXXX 2018

limε→0 FX|Z∈Nε+ (x) and limε→0 FX|Z∈Nε− (x) exist and
are differentiable in x with pdf f + (x|z0 ) and f − (x|z0 ),
respectively.
(vii) Bounded moments
E[Y 1 |X, Z] and E[Y 0 |X, Z] are bounded away from ±
infinity a.s. over Nε
Concerning notation, f + (x, z0 ) = f + (x|z0 ) f (z0 ) refers to the
joint density of X and Z whereas f + (x|z0 ) refers to the conditional density of X.
This assumption requires that in a neighborhood about
z0 , the threshold acts like a local instrumental variable.
Assumptions 1(i) to (iv) are instrumental variable assumptions for a binary instrument, as discussed, for example, in
Imbens (2001). The monotonicity assumption 1(ii) rules out
defiers at the threshold z0 , while 1(i) requires the existence
of compliers. We note that 1(i) and 1(ii) could be relaxed
to a local version of the compliers-defiers assumption of de
Chaisemartin (2017), which allows for defiers under particular
conditions, at the cost of identifying the effects only for a subset
of compliers (the so-called “comvivors”). Assumptions 1(iii)
and 1(iv) represent the exclusion restriction, conditional on X.
Assumption 1(v) requires common support because we need to
integrate over the support of X in (7). (If this assumption is not
satisfied, one can redefine (7) by restricting it to the common
support.) Assumption 1(vi) implies positive density at z0 , such
that observations close to z0 exist.
We also assume the existence of the limit density functions
f + (x|z0 ) and f − (x|z0 ) at the threshold z0 . So far, we do not
assume anything about their continuity with respect to z. In other
words, the conditional density could be discontinuous, that is,
f + (x|z0 ) = f − (x|z0 ), in which case controlling for X is important for identification and thus consistent estimation, or it could
be continuous, that is, f + (x|z0 ) = f − (x|z0 ), in which case identification does not hinge on controlling for observed covariates.
The latter may, however, reduce the variance of the point estimator, as discussed below. (Note that Assumption 1 is somewhat stronger than needed for identification. Assumptions (1i)
to (1iv) could be replaced with other assumptions that identify
the local treatment effect conditional on X. For instance, if local
compliers and local defiers had the same treatment effect, one
could drop the monotonicity assumption. In addition, the existence of a density function for X is not needed.)
Assumption (1vii) requires the conditional expectation functions to be bounded from above and below in a neighborhood of
z0 . It is invoked to permit interchanging the operations of integration and taking limits via the dominated convergence theorem. (This assumption is certainly stronger than needed and
could be replaced with some other smoothness conditions on
E[Y d |X, Z] in a neighborhood of z0 .)
Theorem 1 (Identification of complier treatment effect).
Under Assumption 1, the local average treatment effect γ
for the subpopulation of local compliers is nonparametrically
identified as


γ = lim E Y 1 − Y 0 |Z ∈ Nε , τε = c
ε→0

 +
 +
−
m (x, z0 ) − m− (x, z0 ) · f (x|z0 )+2 f (x|z0 ) dx
= 
.
+
−
(d + (x, z0 ) − d − (x, z0 )) · f (x|z0 )+2 f (x|z0 ) dx

(7)



Proof. See the appendix.

Under Assumption 1, the treatment effect for the local
compliers is identified as a ratio of two integrals, as shown in
Theorem 1. The numerator in (7) is the intention-to-treat (ITT)
effect of Z on Y , weighted by the conditional density of X, at z0 .
(In the limit, the density of X conditional on Z being within a
+
−
symmetric neighborhood around z0 is given by f (x|z0 )+2 f (x|z0 ) .)
The denominator in (7) gives the effect of Z on D, that is, the
fraction of compliers, at z0 . Thus, the ratio of integrals gives
the ITT effect multiplied with the inverse of the number of
compliers, corresponding to the LATE at z0 .
The ratio of integrals expression in (7) is obtained by applying
iterated expectations to
E[Y 1 − Y 0 |Z ∈ Nε , τε = c]
to obtain
=

E[Y 1 − Y 0 |X = x, Z ∈ Nε , τε = c] · fX|Z∈Nε ,τε =c (x) dx.

(8)
Clearly, the density f (X|Z ∈ Nε , τε = c) among the local compliers is not identified since the type τε is unobservable. However, by applying Bayes’ theorem to f (X|Z ∈ Nε , τε = c) and
replacing the first term in (8) with ( 5) (before taking limits), several terms cancel out and we obtain after various calculations the
expression (7), which relies on observed variables only. See the
supplementary appendix for detailed derivations. We thereby
have identified the average effect. Similarly, we could identify
quantile treatment effects by combining the previous derivations
with the reasoning in Frölich and Melly (2013) and Frandsen,
Frölich, and Melly (2012).
So far, we have identified the treatment effect for the compliers in the fuzzy design. Without restrictions on treatment effect
heterogeneity, it is impossible to identify the effects for alwaysand never-participants since they would never change treatment
status in a neighborhood of z0 . However, in the sharp design,
everyone is a complier at z0 , that is, d + (x, z0 ) − d − (x, z0 ) = 1,
and the expression (7) simplifies to


lim E Y 1 − Y 0 |Z ∈ Nε =

 +

m (x, z0 ) − m− (x, z0 )

ε→0

·

f + (x|z0 ) + f − (x|z0 )
dx.
2

(9)

The estimand (9) in the sharp design is identical to the numerator
of (7). The following discussion focusses on the estimation of
(7), where the numerator and denominator of (7) are analyzed
separately. Therefore, the asymptotic distribution of ( 9) in the
sharp design is immediately obtained by using the results for
the numerator of (7) only. We also note that the estimands (7)
and (9) bear some resemblance to the partial means estimator of
Newey (1994). Both the numerator and denominator of (7) have
a partial means form, in that averages over the covariates X are
taken, at the left and the right limit at z0 .
Instead of generalizing assumption (4) to permit for further
covariates X, we could alternatively start from the conditional
independence assumption (3). To conserve space, we, however,
do not analyze this in much detail since most applied work
either uses a sharp design (where (3) is meaningless) or otherwise refers to (4 ). Consider an extension of (3) by including

Frölich and Huber: Including Covariates in the Regression Discontinuity Design

covariates X:
Yi1

− Yi0 ⊥⊥Di |Xi , Zi

for Zi near z0 .

(10)

Analogously to the derivations in Hahn, Todd, and van der
Klaauw (2001) it follows that
E[Y 1 − Y 0 |X, Z = z0 ] =

m+ (X, z0 ) − m− (X, z0 )
.
d + (X, z0 ) − d − (X, z0 )

Similarly to the derivations for Theorem 1, one can show that
the unconditional treatment effect for the population near the
threshold is
m+ (x, z0 ) − m− (x, z0 )
E[Y 1 − Y 0 |Z = z0 ] =
d + (x, z0 ) − d − (x, z0 )
f + (x|z0 ) + f − (x|z0 )
dx.
(11)
2
This expression differs from (7) and (9) in that it is an integral
of a ratio and not a ratio of integrals. The results derived in
Section 3 therefore do not apply to (11). In addition, expression (11) may be difficult to estimate in small samples as the
denominator can be close to zero for some values of x. (This
problem is of much less concern for estimators of (7) and (9)
as those are based on a ratio of two integrals and not on an
integral of a ratio. For those estimators the problem of very
small denominators for some values of X averages out.)
Instead of using (10), one might be willing to strengthen the
latter assumption to
·

5

spirit of Racine and Li (2004). Define κ and κ̄ as univariate
kernel functions, where κ is a second-order kernel (assumed
to be symmetric and integrating to one) and κ̄ is a kernel
of order λ ≥ 2. The following kernel constants
for κ will be
∞
∞
used later: μl = −∞ ul κ (u)du and μ̄l = 0 ul κ (u)du and μ̃ =
μ̄2
− μ̄21 . (With symmetric kernel μ̄0 = 12 .) Furthermore define
2
∞
μ̈l = 0 ul κ 2 (u)du. (For the Epanechnikov kernel with support
[−1, 1], that is, K(u) = 34 (1 − u2 )1(|u| < 1) the kernel constants are μ0 = 1, μ1 = μ3 = μ5 = 0, μ2 = 0.2, μ4 = 6/70,
μ̄0 = 0.5, μ̄1 = 3/16, μ̄2 = 0.1, μ̄3 = 1/16, μ̄4 = 3/70. The
∞
kernel constants for κ̄ are defined as ηl = −∞ ul κ̄ (u)du and
∞ l 2
η̇l = −∞ u κ̄ (u)du. The kernel function κ̄ being of order λ
means that η0 = 1 and ηl = 0 for 0 < l < λ and ηλ = 0.)
We will consider two different choices for Kh (u) in (13). The
conventional choice would be to use a positive (i.e., second
order) and symmetric kernel
Kh (u) =

1
κ (u).
h

(14)

However, as shown below, the use of this “naive” kernel function
1
(14) leads at best to a convergence rate of n− 3 of (13).
As an alternative, we consider a boundary kernel
1
Kh (u) = (μ̄2 − μ̄1 |u|) · κ (u)
h

(15)

in (13), and we will see that this leads to a convergence rate
2
of n− 5 of (13), that is, the rate of univariate nonparametric
for Zi near z0 .
(12) regression. This is achieved through smoothing with implicit
Yi1 , Yi0 ⊥⊥Di |Xi , Zi
double boundary correction. (See, for example, Jones (1993) or
This permits identifying the treatment effect as
Jones and Foster (1996) for similar boundary kernels, or Gasser

 1
0
and Müller (1979), Gasser, Müller, and Mammitzsch (1985),
E Y − Y |Z = z0
Müller (1991) or Tenreiro (2013) for a more general discussion
= (E [Y |D = 1, X = x, Z = z0 ] − E [Y |D = 0, X = x, Z = z0 ]) on various forms of boundary kernels or boundary corrections
including the derivation of optimal boundary kernels for density
f + (x|z0 ) + f − (x|z0 )
estimation, estimation of distribution functions, or estimation
dx,
·
of nonparametric curves, etc.)
2
In the following, we will refer to estimator (13) with kernel
where E[Y |D, X, Z = z0 ] can be estimated by a combination
function
(14) as γ̂naive . Estimator (13) with kernel function (15)
of the left- and right-hand side limits. This approach does not
is
denoted
as γ̂RDD . Because of the asymptotic properties derived
exclusively rely on comparing observations across the threshold
below
we
recommend
the use of γ̂RDD .
but also uses variation within either side of the threshold. The
In
either
case,
estimation
proceeds in two steps and requires
estimand has a similar structure as (7) and (9) and the estimation
nonparametric
first
step
estimates
of m+ , m− , d + , and d − . (In
properties derived later could easily be extended to this case.
+
−
the sharp design (9), d and d are not estimated but set to 1
and 0, respectively.) These can be estimated nonparametrically
3. ESTIMATION
by considering only observations to the right or the left of z0 ,
respectively. Since this corresponds to estimation at a boundary
A straightforward estimator of (7) is
point, local linear regression is suggested, which is known to




n
Zi −z0
+
−
i=1 m̂ (Xi , z0 ) − m̂ (Xi , z0 ) · Kh
h
than conventional Nadaraya–
(13) display better boundary behavior
γ̂ =


,
n  ˆ+
+
ˆ− (Xi , z0 ) · Kh Zi −z0
(X
,
z
)
−
d
d
(x,
z
Watson
kernel
regression.
m
i
0
0 ) is estimated by local linear
i=1
h
regression as the value of a that solves
where m̂ and dˆ are nonparametric estimators and Kh (u) is a kern

2



nel function. For the sharp design (9) the estimator simplifies
Y j − a − b Z j − z0 − c X j − x · K j I +
arg
min
j ,
to
a,b,c
0
)
(m̂+ (Xi , z0 ) − m̂− (Xi , z0 ) ) · Kh ( Zi −z
h
0
Kh ( Zi −z
h

)

j=1

.

For practical convenience, we will mostly work with product kernel functions below. Product kernel functions also have
the advantage that one can easily incorporate discrete X in the

(16)

where I +
j = 1(Z j > z0 ) and a product kernel is used
K j = K j (x, z0 ) = κ

Z j − z0
hz

·

L

l=1

κ̄

X jl − xl
hx

,

(17)

6

Journal of Business & Economic Statistics, XXXX 2018

where L is the dimension of X, and κ and κ̄ are univariate kernel
functions with κ a second-order kernel and κ̄ a kernel of order
λ ≥ 2.
A result derived later will require higher-order kernels (i.e.,
λ > 2) if the number of continuous regressors is larger than
3. For applications with at most three continuous regressors, a
second-order kernel will suffice such that κ̄ = κ can be chosen.
Note that three different bandwidths hz , hx , h are used. h is the
bandwidth in the matching estimator (13) to compare observations to the left and right of the threshold, whereas hz and hx
determine the local smoothing area for the local linear regression in (16), which uses observations only to the right or only
to the left of the threshold. We need some smoothness assumptions as well as conditions on the bandwidth values. (Note that
the above setup includes global linear regression for the special
case where all bandwidth values are set to infinity. In this case,
the estimator (16) corresponds to a linear regression using only
data points to the right; and analogously on the left-hand side.
While a bandwidth value of infinity minimizes variance it could
lead to a large bias if the true regression curve is nonlinear.
The estimator analyzed below seeks to minimize mean squared
error, that is, the sum of the squared bias and variance.)
Assumption 2.
(i) IID sampling: The data {(Yi , Di , Zi , Xi )} are iid from R ×
R × R × RL
(ii) Smoothness:
- m+ (x, z), m− (x, z), d + (x, z), d − (x, z) are λ times continuously differentiable with respect to x at z0 with λth
derivative Hölder continuous in an interval around z0 ,
- f + (x, z) and f − (x, z) are λ − 1 times continuously differentiable with respect to x at z0 with (λ − 1)th derivative Hölder continuous in an interval around z0 ,
- m+ (x, z), d + (x, z), and f + (x, z) have two continuous
right derivatives with respect to z at z0 with second
derivative Hölder continuous in an interval around z0 ,
- m− (x, z), d − (x, z), and f − (x, z) have two continuous
left derivatives with respect to z at z0 with second
derivative Hölder continuous in an interval around z0 ,
(iii) the univariate Kernel functions κ and κ̄ in (17) are symmetric, bounded, Lipschitz, integrate to one and are zero
outside a bounded set; κ is a second-order kernel and κ̄ is
a kernel of order λ,
(iv) Bandwidths: The bandwidths satisfy h, hz , hx → 0 and
nh → ∞ and nhz → ∞ and nhz hLx → ∞.
(v) Conditional variances: The left and right limits of the
conditional variances limε→0 E[(Y − m+ (X, Z))2 |X, Z =
z + ε] and limε→0 E[(Y − m− (X, Z))2 |X, Z = z − ε] exist
at z0 .
3.1 Properties of γ̂naive
With these preliminaries, we consider the properties of γ̂naive and
γ̂RDD . The estimator γ̂naive is, in essence, a combination between
local linear regression in the first step and Nadaraya–Watson
regression in the second step. Although this estimator appears
to be the most obvious one for estimating (7), it has worse statistical properties than γ̂RDD in the sense that it achieves a lower
rate of convergence. This is due to the missing boundary correction in the second step.

Proposition 1 (Asymptotic properties of γ̂naive ). Under
Assumptions 1, 2, and 3, the bias and variance terms of γ̂naive ,
which is the estimator (13) with kernel function (14), are of
order


Bias(γ̂naive ) = O h + h2z + hλx
1
1
+
nh nhz

var(γ̂naive ) = O

.

For the sharp design (9), the same results apply. The exact
expressions for bias and variance are given in the appendix.
From this result it can be seen that the fastest rate of convergence possible for γ̂naive by appropriate bandwidth choices is
1
n− 3 . (In the special case where the density is continuous, that
is, f − (x|z0 ) = f + (x|z0 ), the bias term with respect to the band2
width h is O(h2 ) such that a convergence rate of n− 5 is possible. In this article, we focus on the estimator proposed in the
2
next section, though, because it can obtain n− 5 rate irrespective of whether the density is continuous or not.) It is straightforward to show asymptotic normality for this estimator, but
the (first-order) approximation may not be very useful in practice as it would be dominated by the bias and variance terms
1
). The terms corresponding to the estimation error
O(h) and O( nh
+
of m̂ (x, z0 ), m̂− (x, z0 ), dˆ+ (x, z0 ), dˆ− (x, z0 ) would be of lower
order and thus ignored in the first-order approximation. The bias
and variance approximation thus obtained would be the same
as in a situation where m+ (x, z0 ), m− (x, z0 ), d + (x, z0 ), d − (x, z0 )
were known and not estimated. Hence, such an approximation
might not be very accurate in small samples. A more useful
approximation can be obtained by retaining also the lower order
terms. However, it seems more promising to use γ̂RDD instead.
3.2 Properties of γ̂RDD
The estimator γ̂RDD is based on (13), but uses the boundary kernel (15) in the second smoothing step, instead of (14). It thereby
attains the convergence rate of a one-dimensional nonparametric
regression estimator, irrespective of the dimension of X. It thus
obtains the fastest convergence rate possible and is not affected
by a curse of dimensionality. This is achieved by smoothing
over all regressors X and by an implicit boundary adaptation
with respect to Z. (In addition, the bias and variance terms due
to estimating m+ , m− , d + , d − and due to estimating the density
−
+
functions f (x|z0 )+2 f (x|z0 ) by the empirical distribution functions
converge at the same rate.)
We derive the asymptotic distribution of this estimator and
show that the asymptotic variance becomes smaller the more
covariates X are included. For the optimal convergence result
further below, we need to be specific about the choice of the
bandwidth values.
Assumption 3. The bandwidths satisfy the following
conditions:
√
lim nh5 = r < ∞
n→∞

lim

n→∞

hz
= rz
h

with 0 < rz < ∞

hλ/2
x
= rx < ∞.
n→∞ h
lim

Frölich and Huber: Including Covariates in the Regression Discontinuity Design

This assumption ensures that the bias and standard deviation
2
of the estimator converge at rate n− 5 to zero, that is, at the rate
of a univariate nonparametric regression. Note that the last condition of Assumption 3 provides an upper bound on hx , whereas
Assumption (2iv) provides a lower bound on hx . Suppose that
hx depends on the sample size in the following way:

7


= (d + (x, z0 ) − d − (x, z0 )) ·

f − (x|z0 )+ f + (x|z0 )
dx
2
∂
f (xi ,z0 )
∂
f (x0 ,z0 )
∂ λ−2 f + (x0 ,z0 ) −1
+
and
ωs = { s!(λ−s)!·∂xλ−s −
·(
)
λ−1
λ−2
∂x1
∂xl
l
λ−1−s +
f (xi ,z0 )
∂
(λ−2)!
}/ f + (xi , z0 ) and ωs− defined
(λ−1)!s!(λ−1−s)!
∂xλ−1−s

where

λ−s +

analogously
and VRDD

λ−1 +

l

hx ∝ nζ ,

⎛

then the bandwidth conditions of Assumptions 2 and 3 together
require that
2
4
<ζ ≤− .
(18)
5L
5λ
This implies that hx converges at a slower rate to zero than h and
hz when L ≥ 4, that is, when X contains four or more continuous
regressors. Therefore, a necessary condition for Assumptions 2
4
2
< − 5λ
or equivalently λ > L2 .
and 3 to hold jointly is that − 5L
As further discussed below, this requires higher-order kernels
if X contains four or more continuous regressors, where as conventional kernels are sufficient otherwise. Assumption 3 is sufficient for the bias and variance to converge at the univariate nonparametric rate, which is summarized in the following theorem.

=

μ̄22 μ̈0

−

×
+
+

− 2μ̄2 μ̄1 μ̈1 +
2 4μ̃2 f 2 (z )
0

μ̄21 μ̈2

⎜1
×⎜
⎝ rz



f + (x, z0 ) + f − (x, z0 )

2

2 2+
σY2+ (x, z0 ) − 2γ σY2+
D (x, z0 ) + γ σD (x, z0 )
f + (x, z0 )
2 2−
σY2− (x, z0 ) − 2γ σY2−
D (x, z0 ) + γ σD (x, z0 )
dx
−
f (x, z0 )



m+ (x, z0 ) − γ d + (x, z0 ) − m− (x, z0 ) + γ d − (x, z0 )

2

⎞

 ⎟
· f + (x, z0 ) + f − (x, z0 ) dx⎟
⎠,

Theorem 2 (Asymptotic distribution of γ̂RDD ).
(a) Under Assumptions 1 and 2, the bias and variance terms of
γ̂RDD , which is the estimator (13) with kernel function (15),
are of order


Bias(γ̂RDD ) = O h2 + h2z + hλx
var(γ̂RDD ) = O

1
1
+
nh nhz

(b) Under Assumptions 1, 2, and 3, the estimator is asymptotically normally distributed and converges at the univariate
nonparametric rate
√
nh (γ̂RDD − γ ) → N (BRDD , VRDD ) ,
where BRDD
=



r μ̄22 − μ̄1 μ̄3
4μ̃ f (z0 )
− d − (x, z0 )
+


m+ (x, z0 ) − m− (x, z0 ) − γ d + (x, z0 )

 ∂ 2 f +
∂2 f −
(x,
z
)
+
(x, z0 ) dx
0
∂z2
∂z2
∂ 2 m+ (x, z0 ) ∂ 2 m− (x, z0 )
−
∂z2
∂z2

rrz2 μ̄22 − μ̄1 μ̄3
2μ̃

∂ 2 d + (x, z0 )
∂ 2 d − (x, z0 ) f − (x, z0 ) + f + (x, z0 )
dx
+
γ
∂z2
∂z2
2 f (z0 )

L
λ−1 s +
∂ m (x, z0 ) +
∂ λ m+ (x, z0 )
rrx2 ηλ
+
ωs
+
λ
∂xls
λ! · ∂xl
l=1
s=1

λ−1 s −
∂ m (x, z0 ) − f − (x, z0 ) + f + (x, z0 )
∂ λ m− (x, z0 )
dx
−
−
ωs
∂xls
2 f (z0 )
λ! · ∂xlλ
s=1
−γ

−

γ rrx2 ηλ

L

l=1

∂ λ d − (x, z0 )
−
−
λ! · ∂xlλ



∂ λ d + (x, z0 )
+
λ! · ∂xlλ

λ−1

λ−1

∂ s d + (x, z0 ) +
ωs
∂xls
s=1

∂ s d − (x, z0 ) −
ωs
∂xls
s=1



f − (x, z0 ) + f + (x, z0 )
dx,
2 f (z0 )

where
σY2+ (X, z) = limε→0 E[(Y − m+ (X, Z))2 |X, Z =
+
z + ε] and σY2+
D (X, z) = limε→0 E[(Y − m (X, Z))(D −
d + (X, Z))|X, Z = z + ε] and σD2+ (X, z) = limε→0 E[(D −
d + (X, Z))2 |X, Z = z + ε] and analogously for σY2+
2+
(X, z), σY2+
D (X, z), and σD (X, z).
For the sharp design (9), the same results are obtained but the
formulas are simpler. d + and d − are not estimated but set to 1
and 0, respectively. This implies that = 1 and the terms σD2+ ,
2−
+
−
σD2− , σY2+
D , σY D and all derivatives of d (x, z0 ) and d (x, z0 ) are
zero.
Note that Assumption 3 is stronger than needed for the
2
results of Theorem 3. For obtaining n− 5 convergence weaker
rate conditions would suffice. In other words, it would not be
needed that the ratios of the bandwidths converge to a welldefined limit point. Assumption 3 permits obtaining concise
and explicit expressions for bias and variance, though. We also
see that undersmoothing is permitted: For a choice of r = 0 in
Assumption 3, the limit bias term is zero, that is, BRDD = 0.
Such undersmoothing is convenient, for example, for developing test statistics. (We thank a referee for pointing this out.)
Part (18) of Assumption 3 requires that λ > L2 to control the
bias due to smoothing in the X dimension. If X contains at most
three continuous regressors, a second order kernel λ = 2 can be
used. Otherwise, higher order kernels are required to achieve an
2
n− 5 convergence rate. Instead of using higher order kernels, one
could alternatively use local higher order polynomial regression
instead of local linear regression (16). However, when the number of regressors in X is large, this could be inconvenient to
implement in practice since a large number of interaction and
higher order terms would be required, which could give rise to
problems of local multicollinearity in small samples and/or for
small bandwidth values. On the other hand, higher order kernels
are very convenient to implement when a product kernel (17) is
used. Higher order kernels are only necessary for smoothing in
the X dimension but not for smoothing along Z.

8

Journal of Business & Economic Statistics, XXXX 2018

When a second-order kernel is used and X contains at most
three continuous regressors, the bias term BRDD simplifies to
r μ̄22 − μ̄1 μ̄3
4μ̃ f (z0 )




m+ (x, z0 ) − m− (x, z0 ) − γ d + (x, z0 )

 ∂ 2 f +
∂2 f −
− d − (x, z0 )
(x,
z
)
+
(x, z0 ) dx
0
∂z2
∂z2
+

rrz2 μ̄22 − μ̄1 μ̄3
2μ̃

∂ 2 m+ (x, z0 ) ∂ 2 m− (x, z0 )
−
∂z2
∂z2

f − (x, z0 ) + f + (x, z0 )
∂ 2 d + (x, z0 )
∂ 2 d − (x, z0 )
dx
·
+γ
2
2
∂z
∂z
2 f (z0 )
L  2 +
rr2 μ2
∂ m (x, z0 ) ∂ 2 m− (x, z0 )
+ x
−
2
∂xl2
∂xl2
l=1
 −
∂ 2 d + (x, z0 )
∂ 2 d − (x, z0 )
f (x, z0 ) + f + (x, z0 )
−γ
+
γ
dx.
·
2 f (z0 )
2 · ∂xl2
2 · ∂xl2
−γ

It remains to be discussed how the bandwidth values h, hz ,
and hx should be chosen in practice. It is beyond the scope of
this article to develop a data-driven bandwidth selector, and we
therefore limit ourselves to a procedure that is rate optimal, that
is, satisfies Assumptions 2 and 3 as n increases to infinity. The
first part of Assumption 3 suggests to choose h proportional to
1
n− 5 , which corresponds to the rate for univariate nonparametric
regression. A simple procedure is to choose h via (least-square)
cross-validation with respect to a nonparametric regression of
Y on Z (outside of a neighborhood around z0 ), which is known
to provide a bandwidth that converges at the desired rate. (At
the same time it is known that the bandwidth obtained by crossvalidation converges only very slowly to the true optimal bandwidth. Nevertheless, many applied researchers proceed by using
the bandwidth obtained from cross-validation and then examine
the sensitivity of the final estimation results to changes in the
bandwidth values by reestimating with various multiples and/or
fractions of the original bandwidth values.)
With an estimate for h, we can choose hz = h which is permitted by Assumptions 2 and 3. If X contains at most three continuous regressors, we can also choose hx = h. On the other hand,
if L ≥ 4, then hx should converge at a slower rate than h and hz .
Assumptions 2 and 3 give us some leeway in the exact choice
of hx . If we would like to make the bias small (for reasons discussed in the next section), we would choose the lower bound of
4
(18) to set hx = c1 · n− 5L +δ for a small positive δ and some positive constant c1 . This contrasts with the choice for h which is
1
given as h = c2 · n− 5 . We do not know the optimal c1 and c2 , but
since we only aim for a rate optimal choice, we can set c1 = c2
4
4
1
1
to obtain hx = c1 · n− 5L +δ = c1 n− 5L +δ · n 5 n− 5 such that
hx = n

1−4/L+5δ
5

· h.

We can thus use the bandwidth h obtained via cross-validation
1−4/L+5δ
and multiply it with n 5
for some small δ to obtain the
(larger) bandwidth value for hx . Having estimated γ̂RDD with
these bandwidths, one would usually examine the robustness of
the results to the bandwidths values.

3.3 Variance Reduction Through the Use of Control
Variables
In most of the discussion so far, it was permitted that f (x|z) is
discontinuous at z0 such that controlling for X allows reducing
bias. In the case where f (x|z) is continuous, controlling for X is
still helpful: It can reduce the variance of the estimator, which
is shown in the following theorem. Suppose that the covariates
are identically distributed on both sides of the threshold (i.e.,
f (x|z) is continuous) such that γ is identified with and without controlling for any X. In this case, one could use γ̂RDD with
X being the empty set. This estimator is henceforth denoted
as γ̂no X . Alternatively, one could use a set of control variables
X in the estimator, which we denote as γ̂RDD as before. Suppose that both estimators are consistent for γ . As shown below,
γ̂no X generally has a larger asymptotic variance than γ̂RDD . (We
would like to point out that the result in Theorem 4 only refers
to the variance. While we find that covariates reduce variance,
we do not have a corresponding result for the bias. Hence, in
certain situations, asymptotic bias could possibly increase and
we, therefore, cannot rule out that the inclusion of covariates X
in certain cases could even increase MSE if in such situations
an increase in squared bias is larger than the decrease of variance due to the inclusion of X.) On the other hand, an ordering
of squared biases seems impossible under general conditions.
However, by Assumption 3 we can set r = 0, that is, choose a
bandwidth sequence such that the ratio of the squared bias to
variance converges to zero. Such undersmoothing implies that
the asymptotic bias BRDD is zero and the mean squared error is
thus identical to VRDD . With such undersmoothing, we only need
to analyze the asymptotic variance. As outlined below, there are
precision gains by controlling for X even if the RDD estimator
would be consistent without covariates.
For stating Theorem 3 in a concise way, some further notation
is required. Let w + (X, z) = limε→0 E[Y − γ D|X, Z = z + ε]
be the right limit of the difference between Y and γ D,
and w+ (z) = limε→0 E[Y − γ D|Z = z + ε] be the corresponding expression without conditioning on X. (This
also contains the sharp design (9) as a special case,
and
where
w+ (X, z) = limε→0 E[Y − γ |X, Z = z + ε]
−
|X,
Z
=
z
−
ε].)
Define
the
variance
of
w (X, z) = limε→0 E[Y

w+ (X, z0 ) as V + = {w + (x, z0 ) − w + (z0 )}2 f (x|z0 )dx. Define
w − (X, z), w − (z) and V − analogously as the left limits. Theorem
3 shows that there is a reduction in variance if V + = 0 and/or
V − = 0.
To gain some intuition, note that V + is the variance of the conditional expectation of Y given X plus the variance of the conditional expectation of γ D given X minus the covariance between
these two terms. Hence, V + is usually nonzero if X is a predictor
of Y and/or of D. On the other hand, V + and V − are zero only if
X neither predicts Y nor D. (This discussion excludes the unreasonable case where it predicts
both but not Y − γ D.) Define

further the covariance C as (w + (x, z0 ) − w + (z0 ))(w − (x, z0 ) −
w − (z0 )) f (x|z0 )dx. For the case where V + and V − are both
nonzero, we define the correlation coefficient R = √VC+V − . Now,
we can state the result in terms of the variances and the
correlation coefficient, which also depends on the bandwidth
sequences. The variance of γ̂RDD is a function of smoothing in
the Z dimension via h and hz . The γ̂no X estimator only depends

Frölich and Huber: Including Covariates in the Regression Discontinuity Design

on hz since there is no smoothing in the second step. A natural choice would thus be h = hz . (The variance of γ̂RDD can be
reduced even further relative to γ̂no X by choosing hz < h, but
this would be more of a technical trick than a substantive result.)
This implies rz = 1 in Assumption 3. Using this notation, the
difference in the asymptotic variances can be written as


rz − 2 + rz − 2 −
V +
V − rzC
VRDD − Vno X =
2
2
×

μ̄22 μ̈0 − 2μ̄2 μ̄1 μ̈1 + μ̄21 μ̈2
2 μ̃2 f (z )r
0 z

V + + rz −2
V− −
or, if V + and V − are both nonzero, as = { rz −2
2
2
√
2
2
μ̄ μ̈ −2μ̄ μ̄ μ̈ +μ̄ μ̈
rz R V +V − }( 2 0 2 μ̃22 f (z1 0 )r1 z 1 2 ), as derived in the appendix.
This implies the following.
Theorem 3. Let γ̂RDD be the estimator (13) with kernel
function (15) using the set of regressors X, and let γ̂no X be the
estimator with X being the empty set. Denote the asymptotic
variance of γ̂no X by Vno X and assume that both estimators consistently estimate γ and satisfy Assumptions 2 and 3. Assume
further that the distribution of X is continuous at z0 , that is,
f + (X, z0 ) = f − (X, z0 ) a.s.
(a) If V + = V − = 0 then
VRDD = Vno X .
(b) Under any of the following conditions
VRDD < Vno X ,
- if V + = 0 and V − = 0 or vice versa and rz < 2
- or if V + = 0 and V − = 0 and R ≥ 0 and rz < 2
- or if V + = 0 and V − = 0 and −1 < R < 0 and
1+R
rz < 2 1−R
2.
+
- or if V = 0 and V − = 0 and R = −1 and rz < 1.
Hence, if, in case (a) of Theorem 3, where X has no predictive
power neither for Y nor for D, the asymptotic variances are the
same. On the other hand, if X has predictive power either for
Y or for D and one uses the same bandwidths for both estimators (hz = h), the RDD estimator with covariates has a strictly
smaller variance. (In the sharp design (9), X cannot have predictive power for D (conditional on Z), hence predictive power for
Y is needed.) This holds in all cases except for the very implausible scenario where w + (X, z0 ) and w − (X, z0 ) are negatively correlated with a correlation coefficient of −1. In most economic
applications, however, one would rather expect a positive correlation.
(γ̂RDD has a smaller variance than γ̂no X as it exploits the available information more effectively. Consider, for simplicity, the
sharp design. γ̂no X estimates the conditional mean of Y left and
right of the threshold. In terms of iterated expectations, the left
limit of the mean of Y at the threshold could be estimated as
the left limit of the mean of Y conditional on X averaged out
with respect to the distribution of X, using only data points to
the left of the threshold. In contrast, γ̂RDD estimates the left limit
of the mean of Y conditional on X, but then takes averages with
respect to the distribution of X in the neighborhood of z0 . In
the case where the distribution of X is continuous at z0 , that is,

9

f + (X, z0 ) = f − (X, z0 ), the estimator γ̂RDD uses the data points
Xi in the left and in the right neighborhood of z0 to estimate
f (X, z0 ), whereas γ̂no X uses only the data on one side of the
threshold. This implies that γ̂RDD uses more information in the
estimation of the empirical distribution function F (X, z0 ), which
leads to the variance reductions in Theorem 3.
Theorem 3 can easily be extended to show that the RDD estimator with a larger regressor set X, that is, where X ⊂ X, has
smaller asymptotic variance than the RDD estimator with X.
(The proof is analogous and is omitted.) Hence, one can combine specific covariates for eliminating bias with adding further
covariates to reduce variance. The more variables are included
in X the smaller the variance will be.)
4. SIMULATIONS
This section presents a simulation study to investigate the
finite sample performance of the suggested method in the context of the sharp and fuzzy RDD. Starting with the former, we
consider the following data-generating process (DGP):
Z, U, V, W ∼ N (0, 1) independently of each other,
D = I{Z > 0},

X1 = αD + 0.5U,

X2 = αD + 0.5V,

Y = D + 0.5Z − 0.25DZ + 0.25Z 2 + β(X1 + X2 )
+

β 2
(X + X22 ) + W.
2 1

(19)

Both the running variable Z and the unobservables U, V, W ,
which affect the covariates X1 , X2 and the outcome Y , respectively, are standard normally distributed. The parameter α
reflects the strength of the association between the distributions
of X1 , X2 and the treatment state D. β determines the impact of
X1 , X2 and their higher order terms on Y . In the simulations, we
consider various combinations of α and β. First, we set α = 0
and β = 0.4 such that the covariates affect the outcome, but
are balanced around the threshold. In this case, controlling for
X = (X1 , X2 ) is not necessary for the consistency of RDD, but
might reduce the variance. Second, we set α = 0.2 and β = 0.4,
implying that the distribution of X differs across treatment states
at the threshold and that X affects Y .
We run 1000 simulations and consider sample sizes of
n = 1000 and 4000 to analyze RDD estimation based on the
boundary kernel γ̂RDD , see (15). Least-square cross-validation
(CV) is used to select the bandwidths for the estimation of
m+ (x, z) and m− (x, z) (using local linear regression) as well
as Kh (u) required in (13), (For m+ (X, Z) and m− (X, Z), the
bandwidth selector CV only uses treated and nontreated observations, respectively.) based on the np package for the statistical
software R by Hayfield and Racine (2008). In addition, we also
make use of undersmoothing and oversmoothing by taking half
or twice the CV bandwidth, respectively (CV/2, 2CV).
We compare our method to conventional RDD estimation
without covariates as implemented in the rdd package for R by
Dimmery (2016), which is based on a local linear regression of Y
on Z. We consider several bandwidths choices, namely, the values picked by the CV procedure for γ̂RDD ; the method of Imbens
and Kalyanaraman (2012) (IK) for optimal bandwidth selection
in RDD; the robust inference approach of Calonico, Cattaneo,

10

Journal of Business & Economic Statistics, XXXX 2018

Table 1. Simulations—sharp RDD
γ̂RDD
Bandwidth

CV

CV/2

α = 0, β = 0.4
bias
sdev
rmse
α = 0.2, β = 0.4
bias
sdev
rmse

γ̂RDD

RDD without X
2CV

CV

IK

CCT

LM

CV

RDD without X

CV/2

2CV

n = 1000
0.00
0.15
0.15

−0.00
0.27
0.27

−0.00
0.17
0.17

−0.00
0.27
0.27

0.00 −0.00
0.13
0.43
0.13
0.43
n = 1000
−0.01
0.14
0.14

0.18
0.45
0.48

CV

IK

CCT

LM

−0.01
0.10
0.10

−0.00
0.08
0.08

0.17
0.10
0.20

0.17
0.08
0.19

n = 4000
0.01
0.20
0.20

0.00
0.22
0.22

−0.00
0.15
0.15

−0.00
0.09
0.09

−0.00
0.12
0.12

−0.00
0.09
0.09

0.18
0.20
0.27

0.18
0.22
0.28

0.17
0.15
0.23

−0.00
0.09
0.09

−0.00
0.13
0.13

−0.01
0.09
0.09

−0.01 −0.00
0.27
0.10
0.27
0.10
n = 4000
0.16
0.30
0.34

0.17
0.10
0.20

NOTE: “CV,” “CV/2,” “2CV” stands for bandwidth selection based on least-square cross-validation, as well as twice and half that value. “IK” is the optimal Imbens–Kalyanaraman
(2012) bandwidth. “CCT” is the robust inference approach of Calonico, Cattaneo, and Titiunik (2014) (CCT). “LM” is the local cross-validation approach of Ludwig and Miller (2007)
based on the median values of the running variable above and below the threshold. “bias,” “sdev,” and “rmse” report the bias, standard deviation, and root mean squared error of the
respective method.

and Titiunik (2014) (CCT) as implemented as default option in
the rdrobust package for R by Calonico, Cattaneo, and Titiunik
(2015); and the local cross-validation approach of Ludwig and
Miller (2007) (LM) based on the median values of the running
variable above and below the threshold. In all estimations, the
Epanechnikov kernel is used.
Table 1 reports the bias, standard deviation, and root mean
squared error (RMSE) of the estimators for various choices of
α, β in the sharp RDD. When setting α = 0, β = 0.4, all procedures are unbiased as expected. Under either sample size,
γ̂RDD outperforms RDD without X in terms of precision when
using the same CV bandwidth for both estimators. Furthermore, γ̂RDD with CV is in most cases also more precise than
RDD without X based on the IK, CCT, and LM bandwidths.
(Under n = 1000, α = 0, β = 0.4, the means (standard deviations) of the CV, IK, CCT, and LM bandwidths for Z are
0.16 (0.06), 0.84 (0.29), 0.66 (0.11), 1.58 (0.51), respectively.
The means and standard deviations are very similar under n =
1000, α = 0.2, β = 0.4.) As expected, a smaller bandwidth
(CV/2) increases the standard deviation of γ̂RDD , while a larger

bandwidth (2CV) slightly decreases it. For n = 4000, however,
the differences in precision are quite moderate for various bandwidth choices.
When setting α = 0.2 and β = 0.4, the biases of γ̂RDD are
again close to zero, while this is no longer the case for RDD
without X. For n = 1000, γ̂RDD with CV and 2CV dominates
any RDD without X in terms of bias, standard deviation, and
root mean squared error (RMSE), while γ̂RDD with CV/2 is less
precise. Under n = 4000, all three versions of γ̂RDD have a considerably smaller RMSE than any RDD without X.
Second, we consider the case of a fuzzy RDD. We modify
the DGP by replacing D = I{Z > 0} in (19) with D = I{−1 +
2I{Z > 0} + 0.5U + Q > 0}, with Q ∼ N (0, 1) independently
of any other variable. D is now endogenous even at the threshold due to U entering both the treatment and outcome equation.
The bandwidths used for the estimation of d + (x, z) and d − (x, z)
required for the fuzzy RDD method are selected in an analogous
way as for m+ (x, z) and m− (x, z). We also consider fuzzy RDD
estimation without covariates based on Dimmery (2016) with
CV, IK, CCT, and LM bandwidth choices, respectively. (Under

Table 2. Simulations—fuzzy RDD
γ̂RDD
Bandwidth

CV

CV/2

γ̂RDD

RDD without X
2CV

α = 0, β = 0.4

CV

IK

CCT

LM

CV

CV/2

RDD without X
2CV

n = 1000

bias
sdev
rmse
α = 0.2, β = 0.4

−0.01
0.27
0.27

0.00
0.42
0.42

−0.02
0.22
0.22

bias
sdev
rmse

−0.01
0.28
0.28

−0.00
0.52
0.52

−0.03
0.23
0.23

IK

CCT

LM

−0.01
0.16
0.16

-0.01
0.12
0.12

0.27
0.16
0.31

0.27
0.12
0.30

n = 4000

−0.05 −0.02
0.76
0.34
0.76
0.34
n = 1000
0.25
0.67
0.72

CV

0.27
0.33
0.43

−0.01
0.34
0.34

−0.01
0.24
0.24

0.01
0.16
0.16

−0.00
0.18
0.18

0.01
0.14
0.14

0.27
0.34
0.43

0.27
0.23
0.36

0.01
0.15
0.15

0.01
0.20
0.20

0.00
0.15
0.15

−0.01 0.00
0.34 0.16
0.34 0.16
n = 4000
0.25
0.39
0.46

0.28
0.16
0.32

NOTES: “CV,” “CV/2,” “2CV” stands for bandwidth selection based on least-square cross-validation, as well as twice and half that value. “IK” is the optimal Imbens–Kalyanaraman
(2012) bandwidth. “CCT” is the robust inference approach of Calonico, Cattaneo, and Titiunik (2014) (CCT). “LM” is the local cross-validation approach of Ludwig and Miller (2007)
based on the median values of the running variable above and below the threshold. “bias,” “sdev,” and “rmse” report the bias, standard deviation, and root mean squared error of the
respective method.

Frölich and Huber: Including Covariates in the Regression Discontinuity Design

n = 1000, α = 0, β = 0.4, the means (standard deviations) of
the CV, IK, CCT, and LM bandwidths for Z are 0.23 (0.07),
0.84 (0.29), 0.66 (0.11), 1.73 (0.59), respectively. The means
and standard deviations are very similar under n = 1000, α =
0.2, β = 0.4.) The results are reported in Table 2 and show a
qualitatively similar pattern as for the sharp RDD. However,
standard errors are generally larger as estimation is based on
the compliers only, which by the definition of the DGP make up
for about 65% of the population.

5. APPLICATION
As an empirical illustration of our method, we use data from
Lalive (2008), who studies a labor market program introduced
in June 1988 that extended the maximum duration of unemployment benefits from 30 to 209 weeks for job seekers aged
50 or older in certain regions of Austria under particular conditions. This suggests the use of a sharp RDD for assessing
the program’s effect on labor market outcomes such as unemployment duration. The treatment is defined based on the age
threshold of 50. As acknowledged by Lalive (2008), however,
a concern is that employees and companies could manipulate
age at entry into unemployment, for example, by postponing a
layoff in a way that the age requirement is just satisfied. This
is a common concern in many applications. If such manipulations are selective with respect to employee characteristics that
also affect labor market outcomes, conventional RDD without
covariates fails to identify the effect of the program due to confounding related to an imbalance of the characteristics around
the threshold. In contrast, our method remains consistent if all
labor market relevant characteristics are plausibly observed in

11

the data. As a word of caution, however, we would like to point
out that this cannot be taken for granted in our application.
For instance, unobserved individual characteristics like motivation, (dis-)utility from work, and self-confidence might predict
both manipulation and labor market success. To consistently
estimate the program effect by our method, it is required that
these factors do not entail confounding conditional on the socioeconomic and employment-related characteristics available in
the data (see the discussion below).
Our analysis makes use of the Austrian social security
database, which includes information on job seekers (age,
employment, unemployment, and earnings history) and the
employers (region and industry), and the Austrian unemployment register, which contains information on the place of
residence and socio-economic characteristics. The universe of
inflows into unemployment between 1986 and 1995 is covered,
and the inflow sample can be followed up until the end of
1998. We refer to Lalive (2008) for a description of sample
adjustments made to the dataset. Specifically, we consider the
female subsample in the age bracket 46 to 53 years living in a
region where the program had been introduced, consisting of
5659 observations. The outcome variable Y is unemployment
duration, measured as weeks registered at the unemployment
office. The running variable Z is distance to the age threshold of
50, measured in months divided by 12. Table 3 reports sample
means and balancing tests at the threshold for potentially labor
market relevant characteristics, which serve as X. The tests are
based on running RDD estimations with the elements in X as
outcome variables using the ‘rdd’ package, which performs
local linear regression around the threshold. Estimates, standard
errors, and p-values are reported for the IK bandwidth and half
of it. Indeed, several covariates are imbalanced around the

Table 3. Covariate sample means and balance tests at the threshold
IK

Married (binary)
Single (binary)
Education: medium (binary)
Education: high (binary)
Foreign (binary)
Replacement rate
log wage in last job
Actual to potential work experience
White collar worker (binary)
Industry: agriculture (binary)
Industry: utilities (binary)
Industry: food (binary)
Industry: textiles (binary)
Industry: wood (binary)
Industry: machines (binary)
Industry: other manufacturing (binary)
Industry: construction (binary)
Industry: tourism (binary)
Industry: traffic (binary)
Industry: services (binary)

IK/2

Sample mean

Difference

p-Value

Difference

p-Value

0.75
0.09
0.22
0.08
0.02
0.44
6.15
0.89
0.32
0.02
0.00
0.05
0.12
0.03
0.08
0.11
0.03
0.32
0.02
0.17

0.16
−0.05
0.02
0.04
0.01
−0.01
0.12
0.02
0.16
−0.01
0.00
−0.02
0.02
0.00
0.04
0.03
0.03
−0.03
−0.03
−0.05

0.00
0.05
0.51
0.03
0.37
0.01
0.00
0.06
0.00
0.65
0.32
0.31
0.54
0.82
0.05
0.31
0.03
0.46
0.07
0.14

0.16
−0.05
−0.00
0.04
0.01
−0.01
0.18
0.00
0.15
0.02
0.00
−0.03
−0.03
0.02
0.06
0.04
0.04
−0.02
−0.02
−0.03

0.01
0.13
0.99
0.14
0.59
0.03
0.00
0.77
0.00
0.20
0.32
0.44
0.38
0.20
0.06
0.33
0.02
0.73
0.37
0.50

NOTE: “IK,” “IK/2” denote the optimal Imbens–Kalyanaraman (2012) bandwidth and half that value in an RDD estimation when using each of the covariates as outcome. p-Values are
based on analytic standard errors and account for clustering of age (measured in months).

12

Journal of Business & Economic Statistics, XXXX 2018

Table 4. Effect estimates
γ̂RDD
Bandwidth h
Treatment effect
Standard error
p-Value

RDD without X

0.1

0.2

0.3

0.4

0.5

0.1

0.2

0.3

0.4

0.5

115.31
4.23
0.00

112.74
4.09
0.00

110.76
4.14
0.00

109.71
4.03
0.00

108.64
4.41
0.00

134.25
9.72
0.00

143.67
12.49
0.00

141.41
9.90
0.00

137.99
8.45
0.00

132.55
8.03
0.00

NOTES: The bandwidths hx , hz for the first step estimates of m+ and m− entering γ̂RDD (see Section 3) are picked by least-square cross-validation. For bandwidth h on the running
variable Z in γ̂RDD and RDD without X, several values are considered as indicated in the table. Standard errors are based on bootstrapping the estimate 999 times. Sample size is 5659
observations. X includes the variables given in Table 3: marital status, education, migration status, replacement rate, log wage in last job, actual to potential work experience, white collar
worker, and industry.

threshold, which concerns among others marital status, wage
in the last job, and being a white collar worker. (To control the
family-wise error rate of multiple testing in Table 3, one may
apply the (conservative) Bonferroni correction: divide the nominal level of significance by the number of tested covariates (in
our case 20) and reject an individual null hypothesis of covariate balance if the corresponding p-value is even lower. For log
wage in last job and white collar worker, the null hypothesis
is rejected under either bandwidth at the nominal 5% level of
significance.) The results therefore suggest that observations
slightly above the age threshold have somewhat more favorable
labor market relevant characteristics than those slightly below.
Our RDD estimator derived from Equation (7) controls for
differences in X by giving appropriate weights to each of these
characteristics, according to their distribution about the threshold. Consider, for example, the variable marital status, which
is significantly different in Table 3. On average, 75% of the
observations in the sample are married, but the (conditional)
probability of being married is discontinuous at the threshold:
The nonparametric estimates of the probability from the left
and right are 63.7% and 79.9%, respectively. In a symmetric
neighborhood about the threshold, the probability of being married is thus 71.8%. Our method proceeds by estimating the
outcome unemployment duration for married women left and
right of the threshold and multiplying with a weight of 0.718.
An analogous approach applies to unmarried women using a
weight of 0.282. Hence, a weighted average with respect to
the fraction of married women in a symmetric neighborhood
about the threshold is taken. This removes the discontinuity in
marital status: The 63.7% married women to the left are upweighted with 0.718/0.637, while the 79.9% married women to
the right are down-weighted with 0.718/0.799. Accordingly, the
36.3% unmarried women to the left are down-weighted with
0.282/0.363, while those 20.1% to the right are up-weighted
with 0.282/0.201. In contrast, RDD estimation not controlling
for X compares the unemployment duration left and right of the
threshold without weighting, thereby ignoring that there are, for
example, fewer married women to the left than to the right of the
threshold.
Table 4 presents the results for γ̂RDD when using crossvalidation for the bandwidth selection of hx , hz in the first-step
estimation of m+ and m− . Different from the simulations in
Section 4, however, the covariates now contain both continuous
and discrete elements. We therefore apply the method of Racine
and Li (2004), which allows for both continuous and discrete
regressors by means of product kernels and is implemented

in the “np” package of Hayfield and Racine (2008). We use
the Epanechnikov, Wang and van Ryzin (1981), and Aitchison
and Aitken (1976) kernel functions for continuous, ordered
discrete, and unordered discrete covariates, respectively. We
consider several choices for bandwidth h in the Epanechnikovbased boundary kernel function for the running variable in
(13): 0.1, 0.2, . . . , 0.5. We also compare the results to RDD
regression without covariates based on the “rdd” package
with the same bandwidth choice h. The standard errors of
any method are based on nonparametrically bootstrapping the
respective estimates 999 times, that is, randomly resampling the
original data with replacement and applying the estimators to
the bootstrap samples. The γ̂RDD estimates point to a substantial
increase in unemployment duration by about 110 weeks.
The results are highly significant, as the standard errors of
roughly 4 weeks are quite moderate. When using RDD without
X, both the effect of about 140 weeks and the standard error
of about 10 weeks are substantially higher. For each bandwidth
value considered, the estimates are statistically significantly different between the methods (at the 5% level based on bootstrapping the differences in the estimates 999 times). This indicates
that there might be some confounding due to observed covariates. Also the effects reported in Table 3 columns (3) and (4) of
Lalive (2008) when omitting X and either using a global RDD
model with a higher order polynomial for the running variable or
a local linear model with a very small bandwidth are somewhat
higher than γ̂RDD (122 to 126 weeks). In contrast, the effect of
103 weeks presented in column (6) of Table 3 in Lalive (2008)
is based on linearly controlling for covariates. Our somewhat
higher (and at the 5% level statistically significantly different)
estimates (when bootstrapping the differences) are likely due to
using a more flexible specification with respect to the association of Y and X.

6. CONCLUSION
In this article, the regression discontinuity design (RDD) has
been generalized to incorporate covariates X in a fully nonparametric way. Including covariates can reduce the variance and
eliminate biases if X is discontinuously distributed at the threshold. It has been shown that the curse of dimensionality does not
apply and that the average treatment effect (on the local compli2
ers) can be estimated at rate n− 5 irrespective of the dimension of
X. For achieving this rate, a boundary RDD estimator has been
suggested. We investigated the finite sample properties of our

Frölich and Huber: Including Covariates in the Regression Discontinuity Design

estimator in simulations and applied it to estimate the effect of
age-dependent unemployment benefits on unemployment duration in Austrian labor market reform, where manipulation at the
threshold is a potential concern.
SUPPLEMENTARY MATERIALS
The supplementary appendix contains all proofs.
ACKNOWLEDGMENTS
This is a substantially revised version of the 2007 IZA Working Paper 3024.
The authors have benefitted from comments by three anonymous referees, the
associate editor, and the editor.

FUNDING
Markus Frölich acknowledges financial support from the Research Center
SFB 884 ‘Political Economy of Reforms’ Project B5, funded by the German
Research Foundation (DFG).
[Received January 2015. Revised November 2017.]

REFERENCES
Aitchison, J., and Aitken, C. (1976), “Multivariate Binary Discrimination by the
Kernel Method,” Biometrika, 63, 413–420. [12]
Battistin, E., and Rettore, E. (2008), “Ineligibles and Eligible Non-Participants
as a Double Comparison Group in Regression-Discontinuity Designs,” Journal of Econometrics, 142, 715–730. [2]
Black, D., Galdo, J., and Smith, J. (2007), “Evaluating the Regression Discontinuity Design Using Experimental Data,” Mimeo, University of Michigan,
USA. [2]
Calonico, S., Cattaneo, M. D., Farrell, M. H., and Titiunik, R. (2018), “Regression Discontinuity Designs Using Covariates,” Review of Economics and
Statistics, forthcoming. [2]
Calonico, S., Cattaneo, M. D., and Titiunik, R. (2014), “Robust Nonparametric
Confidence Intervals for Regression-Discontinuity Designs,” Econometrica,
82, 2295–2326. [1]
——— (2015), “rdrobust: An R Package for Robust Nonparametric Inference in
Regression-Discontinuity Designs,” R Journal, 7, 38–51. [10]
de Chaisemartin, C. (2017), “Tolerating Defiance? Local Average Treatment
Effects Without Monotonicity,” Quantitative Economics, 8, 367–396. [4]
Dimmery, D. (2016), Package ‘rdd’, Manual for the Statistical Software
‘R’,” The Comprehensive R Archive Network, available at https://cran.rproject.org/web/packages/rdd/rdd.pdf [9,10]
Dong, Y. (2014), “An Alternative Assumption to Identify LATE in Regression
Discontinuity Designs,” unpublished manuscript, University of California
Irvine. [1,2]
Eugster, B., Lalive, R., Steinhauer, A., and Zweimüller, J. (2017), “Culture, Work Attitudes and Job Search: Evidence From the Swiss Language
Border,” Journal of the European Economic Association, 15, 1056–1100.
[2]
Fisher, R. (1935), Design of Experiments, Edinburgh: Oliver and Boyd. [2]
Frandsen, B., Frölich, M., and Melly, B. (2012), “Quantile Treatment Effects
in the Regression Discontinuity Design,” Journal of Econometrics, 168,
382–395. [4]
Frölich, M. (2007), “Nonparametric IV Estimation of Local Average Treatment
Effects With Covariates,” Journal of Econometrics, 139, 35–75. [3]
Frölich, M., and Melly, B. (2013), “Unconditional Quantile Treatment Effects
Under Endogeneity,” Journal of Business and Economic Statistics (JBES),
31, 346–357. [4]

13

Gasser, T., and Müller, H. (1979), “Kernel Estimation of Regression Functions,”
in Smoothing Techniques for Curve Estimation, Lecture Notes in Mathematics 757, eds. T. Gasser, and M. Rosenblatt, Berlin: Springer, pp. 23–68.
[5]
Gasser, T., Müller, H., and Mammitzsch, V. (1985), “Kernels for Nonparametric
Curve Estimation,” Journal of the Royal Statistical Society, Series B, 47,
238–252. [5]
Gelman, A., and Imbens, G. (2018), “Why High-Order Polynomials Should Not
be Used in Regression Discontinuity Designs,” Business & Economic Statistics, forthcoming. [1]
Hahn, J., Todd, P., and van der Klaauw, W. (2001), “Identification and Estimation of Treatment Effects with a Regression-Discontinuity Design,” Econometrica, 69, 201–209. [1,2,5]
Hayfield, T., and Racine, J. (2008), “Nonparametric Econometrics: The np Package,” Journal of Statistical Software, 27, 1–32. [9,12]
Imbens, G. (2001), “Some Remarks on Instrumental Variables,” in Econometric Evaluation of Labour Market Policies, eds. M. Lechner, and F. Pfeiffer,
Heidelberg: Physica/Springer, pp. 17–42. [4]
Imbens, G., and Angrist, J. (1994), “Identification and Estimation of Local Average Treatment Effects,” Econometrica, 62, 467–475. [3]
Imbens, G., and Kalyanaraman, K. (2012), “Optimal Bandwidth Choice for the
Regression Discontinuity Estimator,” The Review of Economic Studies, 79,
933–959. [1,9]
Imbens, G. W., and Lemieux, T. (2008), “Regression Discontinuity Designs: A
Guide to Practice,” Journal of Econometrics, 142, 615–635. [1]
Jones, M. (1993), “Simple Boundary Correction for Kernel Density Estimation,”
Statistics and Computing, 3, 135–146. [5]
Jones, M., and Foster, P. (1996), “A Simple Nonnegative Boundary Correction
Method for Kernel Density Estimation,” Statistica Sinica, 6, 1005–1013. [5]
Lalive, R. (2008), “How Do Extended Benefits Affect Unemployment Duration? A Regression Discontinuity Approach,” Journal of Econometrics, 142,
785–806. [2,11,12]
Lee, D. (2008), “Randomized Experiments From Non-Random Selection in
U.S. House Elections,” Journal of Econometrics, 142, 675–697. [2]
Lee, D., and Card, D. (2008), “Regression Discontinuity Inference With Specification Error,” Journal of Econometrics, 142, 655–674. [1]
Lee, D., and Lemieux, T. (2010), “Regression Discontinuity Designs in Economics,” Journal of Economic Literature, 48, 281–355. [1]
Ludwig, J., and Miller, D. L. (2007), “Does Head Start Improve Children’s Life
Chances? Evidence from a Regression Discontinuity Design,” The Quarterly Journal of Economics, 122, 159–208. [10]
McCrary, J. (2008), “Manipulation of the Running Variable in the Regression Discontinuity Design: A Density Test,” Journal of Econometrics, 142,
698–714. [1]
Müller, H. (1991), “Smooth Optimum Kernel Estimators Near Endpoints,”
Biometrika, 78, 521–530. [5]
Newey, W. (1994), “Kernel Estimation of Partial Means and a General Variance
Estimator,” Econometric Theory, 10, 233–253. [4]
Neyman, J. (1923), “On the Application of Probability Theory to Agricultural
Experiments. Essay on Principles,” Statistical Science, 5, 463–480. [2]
Porter, J. (2003), “Estimation in the Regression Discontinuity Model,” Mimeo.
[1]
Racine, J., and Li, Q. (2004), “Nonparametric Estimation of Regression Functions with Both Categorical and Continuous Data,” Journal of Econometrics,
119, 99–130. [5,12]
Rubin, D. (1978), “Bayesian Inference for Causal Effects: The Role of Randomization,” Annals of Statistics, 6, 34–58. [2]
Tenreiro, C. (2013), “Boundary Kernels for Distribution Function Estimation,”
REVSTAT-Statistical Journal, 11, 169–190. [5]
Trochim, W. (1984), Research Design for Program Evaluation: The RegressionDiscontinuity Approach, Beverly Hills: Sage Publications. [2]
Urquiola, M., and Verhoogen, E. (2009), “Class-Size Caps, Sorting, and the
Regression-Discontinuity Design,” The American Economic Review, 99,
179–215. [2,3]
van der Klaauw, W. (2008), “Breaking the Link Between Poverty and Low Student Achievement: An Evaluation of Title I,” Journal of Econometrics, 142,
731–756. [1,2]
Wang, M., and van Ryzin, J. (1981), “A Class of Smooth Estimators for Discrete
Distributions,” Biometrika, 68, 301–309. [12]

