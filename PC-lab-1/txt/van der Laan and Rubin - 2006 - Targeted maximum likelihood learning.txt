The International Journal of
Biostatistics
Volume 2, Issue 1

2006

Article 11

Targeted Maximum Likelihood Learning
Mark J. van der Laan, Division of Biostatistics, School of
Public Health, University of California, Berkeley
Daniel Rubin, University of California, Berkeley

Recommended Citation:
van der Laan, Mark J. and Rubin, Daniel (2006) "Targeted Maximum Likelihood Learning," The
International Journal of Biostatistics: Vol. 2: Iss. 1, Article 11.
DOI: 10.2202/1557-4679.1043

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

Targeted Maximum Likelihood Learning
Mark J. van der Laan and Daniel Rubin

Abstract
Suppose one observes a sample of independent and identically distributed observations from
a particular data generating distribution. Suppose that one is concerned with estimation of a
particular pathwise differentiable Euclidean parameter. A substitution estimator evaluating the
parameter of a given likelihood based density estimator is typically too biased and might not even
converge at the parametric rate: that is, the density estimator was targeted to be a good estimator
of the density and might therefore result in a poor estimator of a particular smooth functional of
the density. In this article we propose a one step (and, by iteration, k-th step) targeted maximum
likelihood density estimator which involves 1) creating a hardest parametric submodel with
parameter epsilon through the given density estimator with score equal to the efficient influence
curve of the pathwise differentiable parameter at the density estimator, 2) estimating epsilon with
the maximum likelihood estimator, and 3) defining a new density estimator as the corresponding
update of the original density estimator. We show that iteration of this algorithm results in a
targeted maximum likelihood density estimator which solves the efficient influence curve
estimating equation and thereby yields a locally efficient estimator of the parameter of interest,
under regularity conditions. In particular, we show that, if the parameter is linear and the model is
convex, then the targeted maximum likelihood estimator is often achieved in the first step, and it
results in a locally efficient estimator at an arbitrary (e.g., heavily misspecified) starting density.
We also show that the targeted maximum likelihood estimators are now in full agreement
with the locally efficient estimating function methodology as presented in Robins and Rotnitzky
(1992) and van der Laan and Robins (2003), creating, in particular, algebraic equivalence between
the double robust locally efficient estimators using the targeted maximum likelihood estimators as
an estimate of its nuisance parameters, and targeted maximum likelihood estimators. In addition, it
is argued that the targeted MLE has various advantages relative to the current estimating function
based approach. We proceed by providing data driven methodologies to select the initial density
estimator for the targeted MLE, thereby providing data adaptive targeted maximum likelihood
estimation methodology. We illustrate the method with various worked out examples.
KEYWORDS: causal effect, cross-validation, efficient influence curve, estimating function,
locally efficient estimation, loss function, maximum likelihood estimation, sieve, targeted
maximum likelihood estimation, variable importance
Author Notes: This research was supported by NIH grant R01 GM071397.

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

1

Introduction

Let O1 , . . . , On be n independent and identically distributed (i.i.d.) observations of an experimental unit O with probability distribution P0 ∈ M, where
M is the statistical model. For the sake of presentation, we will assume that
M is dominated by a common measure µ so that we can identify each possible
probability measure P ∈ M by its density p = dP/dµ. In the discussion we
point out that our methods are not restricted to models dominated by a single
measure. Let Pn be the empirical probability distribution of O1 , . . . , On which
0
puts mass 1/n on each of the n observations. Let p0 = dP
be the density of
dµ
p0 with respect to a dominating measure µ, and let pn be a density estimator
of p0 . For example, pn ≡ Φ(Pn ) could be the maximum likelihood estimator
defined by the following mapping Φ
pn = Φ(Pn ) ≡ arg max

P ∈M

n
X

log

i=1

dP
(Oi ).
dµ

Alternatively, if the model M is too large in the sense that the maximum
likelihood estimator is too variable or even inconsistent, then one typically
proposes a sieve Ms ⊂ M, indexed by indices s, approximating M, and
computes candidate maximum likelihood estimators
pns = Φs (Pn ) ≡ arg max

P ∈Ms

n
X
i=1

log

dP
(Oi ).
dµ

In such a setting it remains to data adaptively select s. For example, one could
use likelihood based cross-validation to select s:
sn = arg max EBn
s

X

0
log Φs (Pn,B
)(Oi ),
n

i:Bn(i)=1

where Bn ∈ {0, 1}n is a random vector of binary variables defining a random
split in a training sample {i : Bn (i) = 0} and validation sample {i : Bn (i) = 1},
0
1
and Pn,B
, Pn,B
denote the empirical probability distributions of the training
n
n
and validation sample, respectively. Now, one would define the estimator of
p0 as the cross-validated maximum likelihood estimator given by
pn = Φ(Pn ) ≡ pnsn = Φsn (Pn ).
It is common practice to evaluate one or many Euclidean valued smooth
functionals Ψ(pn ) of the density estimator pn and view them as estimators of
the parameter Ψ(p0 ) for given parameter mappings Ψ : M → IRd . Although
1

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

this method is known to result in efficient estimators of Ψ(p0 ) in parametric models (i.e., M in the above definition of pn is a parametric model), in
general, such substitution estimators are not correctly trading off bias and
variance with respect to the parameter of interest ψ0 = Ψ(p0 ). For example,
a univariate (standard) kernel density estimator optimizing the mean squared
error with respect to p0 , assuming a continuous second derivative, can have
bias of the order n−2/5 based on an optimal bandwidth of the order n−1/5 . The
corresponding substitution estimator of the cumulative distribution function
at a point can have bias which converges to zero at the same rate n−2/5, but
a variance of O(1/n), so that the substitution estimator has a variance (1/n)
which is smaller than the square bias (n−4/5 ) by an order of magnitude. In
particular, the smoothed empirical cumulative distribution
functions would
√
not even converge at root-n rate due to the fact that n times the bias n−2/5
does not converge to zero: that is, in this kernel density estimator example
√ −2.5
nn
→ ∞, so that the relative efficiency of the empirical cumulative distribution function and this smooth cumulative distribution function converges
to zero. This shows that substitution estimators based on optimal (for the
purpose of the density itself) density estimators of the cumulative distribution
function are typically theoretically inferior to other more targeted estimators
of the parameter of interest. In general, substitution estimators based on density estimators might simply not be very good estimators, and, in particular,
likelihood based substitution estimators will often fail to be asymptotically efficient due to the bias caused by the curse of dimensionality: the kernel density
example already shows the failure of likelihood based learning of smooth parameters of a density of a univariate random variable, and it gets much worse
for densities of multivariate random variables. This issue has been stressed
repeatly by Robins and co-authors (see e.g., Robins and Rotnitzky (1992) and
van der Laan and Robins (2003)). This article proposes a method which, given
a particular pathwise differentiable parameter of interest, allows one to map a
density estimator (such as pn or pns for each s) into a targeted maximum likelihood density estimator so that the corresponding substitution estimator of ψ0
is locally efficient, under reasonable conditions: that is, if the starting density
estimator is consistent, it will typically be efficient, and otherwise in certain
classes of problems it might still be consistent and asymptotically linear.
Specifically, in this article we propose a one step maximum likelihood density estimator which involves 1) creating a parametric model with Euclidean
parameter  (e.g., the same dimension d as the parameter ψ0) through a given
density estimator p0n (e.g., s-specific MLE pns ) at  = 0 whose scores include
the components of the efficient influence curve of the pathwise differentiable
parameter at the density estimator p0n , 2) estimating  with the maximum
DOI: 10.2202/1557-4679.1043

2

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

likelihood estimator of this parametric model, and 3) defining a new density
estimator p1n as the corresponding fluctuation of the original density estimator
p0n . In addition, iterating this process results in a sequence of pkn with increasing log-likelihood converging to a solution of the efficient influence curve
estimating equation, and thereby typically results in a locally efficient substitution estimator of ψ0. We refer to this solution as the targeted maximum
likelihood estimator based on the initial p0n . We provide various examples in
which this targeted maximum likelihood estimator is achieved at the first step
of the algorithm.
In particular, one can map each model based MLE pns into a targeted MLE
∗
pns (targeted towards ψ0). We suggest that it is appropriate to select among
this collection of targeted MLEs p∗ns with likelihood based cross-validation, as
explained heuristically in our accompanying technical report: targeted MLE’s
are comparable w.r.t. to being fully trained w.r.t. estimation of the parameter
of interest, which makes the log-likelihood an appropriate criteria to select
among them. That is, let p∗ns = Φ̂∗s (Pn ) be the s-specific targeted MLE applied
to the initial density estimator pns . Let
sn = arg max EBn
s

X

0
log Φ̂∗s (Pn,B
)(Oi ),
n

i:Bn (i)=1

where Bn ∈ {0, 1}n is a random vector of binary variables defining a random
split in a training sample {i : Bn (i) = 0} and validation sample {i : Bn (i) = 1},
0
1
and Pn,B
, Pn,B
denote the empirical probability distributions of the training
n
n
and validation sample, respectively, as above. Now, likelihood cross-validated
targeted MLE is defined as:
p∗n = Φ̂(Pn ) ≡ p∗nsn = Φ̂∗sn (Pn ).
We also note that the candidate models indexed by s can be chosen to represent
a sieve in a possibly misspecified (big) model M, as long as this model M
is still such that the Kullback-Leibler projection of the true density p0 on
this model identifies the parameter of interest Ψ(p0 ) correctly: for example,
if the parameter of interest is a parameter of a regression of an outcome Y
on covariates W , then one might select as big model the normal densities
with unspecified conditional mean, given W , and certain possibly misspecified
conditional variance, even though the true density p0 is not a member of this
model.

1.1

Organization of article.

In Section 2, given an initial density estimator p0n (e.g., pns ) of p0 , we formally
define the k-th order targeted maximum likelihood density estimator pkn , and
3

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

corresponding targeted maximum likelihood estimator Ψ(pkn ) of ψ0 . We illustrate the targeted MLE of the cumulative distribution function at a point in a
nonparametric model. In this case, it appears that the first step targeted MLE
of ψ0 algebraically equals the empirical cumulative distribution function, for
any given initial density estimator p0n . Thus, while the original substitution
estimator of the cumulative
distribution function would not converge at the
√
parametric rate 1/ n due to it being too biased, the first order targeted bias
corrected density estimator estimates the cumulative distribution function efficiently. In Section 3 we establish that the targeted MLE solves the efficient
influence curve estimating equation, which provides the basis of its asymptotic
efficiency for ψ0 . In Section 4 we present general templates for establishing
consistency, asymptotic linearity and efficiency of the targeted MLE of ψ0,
which provides a particular powerful theorem for convex models and linear
pathwise differentiable parameters stating that the targeted MLE will be consistent and asymptotically linear for an arbitrary starting density, and it will
be efficient if the starting (or its targeted MLE version) density consistently
estimates the efficient influence curve. We illustrate the latter result with two
examples. In Section 5 we discuss the relation, and in particular, the algebraic
equivalence, between targeted maximum likelihood estimation and estimating
function based estimation if one estimates the nuisance parameters in the estimating functions with the targeted MLE. We point out that targeted MLE
is more widely applicable by not relying on being able to map the efficient
influence curve in a corresponding estimating function, and it deals naturally
with the issue of multiple solutions of estimating equations. In Subsection 5.1
we focus on censored data models to make the comparison with the estimating function methodology in van der Laan and Robins (2003). In particular,
we present the targeted MLE approach which results in algebraic equivalence
between the Inverse Probability of Censoring Weighted estimator, the double robust IPCW estimator, and the targeted MLE of a parameter of the full
data distribution based on observing n i.i.d. observations of a censored data
structure under coarsening at random (CAR). These results show that the
targeted MLE does not only provide a boost for likelihood based estimation,
but it also provides an improvement relative to the current implementation of
locally efficient estimation based on estimating function methodology. In Section 6 we present important examples illustrating the power and computational
simplicity of this new targeted maximum likelihood estimator: estimation of
a marginal causal effect, and the parametric component in a semiparametric regression model, and we present a simulation to illustrate the targeted
MLE. In Section 7 we present a loss based approach of targeted MLE learning
based on the unified loss function based approach in van der Laan and Dudoit
DOI: 10.2202/1557-4679.1043

4

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

(2003). We end this article with a discussion in Section 8. In our accompanying technical report we show generalizations of the targeted MLE of pathwise
differentiable parameters to targeted MLE of general parameters.

1.2

Some relevant literature overview.

There exist various methods for construction of an efficient estimator of a parameter based on parametric models. In particular, Fisher’s method of maximum likelihood estimation can be applied, or closely related M-estimate (i.e.,
estimators defined as solutions of estimating equations) methods which work
under minimal conditions. Maximum likelihood estimation in semiparametric
models has been an extensive research area of interest. Here we suffice with
a referral to van der Vaart and Wellner (1996b) for a partial overview of the
theory for the analysis of maximum likelihood. There are plenty of examples in
which the straightforward semiparametric MLE even fails to be consistent, but
often an appropriate regularization can be applied to repair the consistency
of the semiparametric MLE: e.g., see van der Laan (1995) for such examples
based on censored data. However, as argued above in the kernel density estimator example, maximum likelihood based smoothing/model selection will
often provide the wrong trade-off of bias and variance for specific smooth parameters. The literature (notably Robins and co-authors) has recognized this
problem with likelihood based estimation. For example, smoothing survival
functions or smoothing the nonparametric components in a semiparametric regression model requires so called “under-smoothing” in order to obtain root-n
consistency for the parameter of interest: see e.g., Cosslett (2004).
For an overview of the literature on efficient estimation of pathwise differentiable parameters in semiparametric models we refer to Bickel et al. (1993b).
In particular, the latter presents the general one step estimator based on an
estimate of the efficient influence curve: see e.g. Klaassen (1987). For an
overview of the literature on locally efficient estimating function based estimation of pathwise differentiable parameters based on censored longitudinal
data (starting with the ground breaking paper Robins and Rotnitzky (1992)),
we refer to van der Laan and Robins (2003).
A unified loss function approach based methodology for estimation and
estimator selection, and concrete illustration of this method in various examples is presented in van der Laan and Dudoit (2003). This methodology is
general by allowing the loss function to be an unknown function of the experimental unit and the parameter values. van der Laan and Rubin (2005)
and van der Laan and Rubin (2006) present an alternative unified estimating
function methodology for both estimation and estimator selection. The latter
5

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

two methodologies provide two general strategies for data adaptive estimation
of any parameter in any model.
We note that these (unified) loss function and (unified) estimating function based approaches give up on using the log-likelihood as loss function for
the purpose of estimator selection and estimation when the parameter of interest is not the actual density of the data, but a particular parameter of
it: these methods replace the log-likelihood loss function by a loss function
or an estimating function targeted at the parameter of interest. From that
point of view, the current article shows that it is not necessary to replace the
log-likelihood loss function by a targeted loss function, but that one can also
target the directions in which one maximizes the log-likelihood.

2

Targeted maximum likelihood estimators.

Let Ψ : M → IRd be a pathwise differentiable parameter at any density p ∈ M,
where M denotes the statistical model consisting of the possible densities
p = dP/dµ of O with respect to some dominating measure µ. That is, given a
sufficiently rich class of one-dimensional regular parametric submodels {pδ : δ}
with parameter δ of M through the density p at δ = 0, we have for each of
these submodels pδ with score s at δ = 0 and pδ=0 = p
d
Ψ(pδ )|δ=0 = Ep S(p)(O)s(O)
dδ
for some S(p) ∈ (L20 (p))d , where L20 (p) denotes the Hilbert space of functions
of O with mean 0 and finite variance under P , endowed with inner product
hh1 , h2 iP = Ep h1 (O)h2 (O). This random variable S(p) ∈ (L20 (p))d is called a
gradient of the pathwise derivative at p. Let T (p) ⊂ L20 (p) be the tangent space
at p which is defined as the closure of the linear span of the scores s of this
class of submodels through p. If the model is not locally saturated in the sense
⊥
that T (p) = L20 (p), then there can be many gradients. Let Tnuis
(p) ⊂ L20 (p) be
the orthogonal complement of the so called nuisance tangent space, where the
latter is defined as the closure of the linear span of all scores of pδ for which
the pathwise derivative equals 0 (see van der Laan and Robins (2003), Chapter
1). As in van der Laan and Robins (2003), we denote the set of gradients at
⊥∗
⊥
p with Tnuis
(p) ⊂ (Tnuis
(p))d . Let S ∗(p) be the so called canonical gradient
which is the unique gradient whose d components S ∗ (p)j , j = 1, . . . , d, are
elements of the tangent space T (P ). A submodel {p : } with score S ∗(p) at
 = 0 is often referred to as a hardest submodel (Bickel et al. (1993a)), as we
will also do in this article.
DOI: 10.2202/1557-4679.1043

6

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

Let (O, p) → D(p)(O) be a point-wise well defined class of functions on
the Cartesian product of the support of O and the model M, which satisfies
D(p) = S ∗(p) P0 -a.e. for all p ∈ M.
As an example, consider letting O be a Euclidean valued d-variate random
variable with density p0 . Let M be the class of all continuous densities with
R
respect to Lebesgue measure µ, and let Ψ(p) = 0t p(o)dµ(o) be the cumulative
distribution function at a point t ∈ IR corresponding with density p. In this
case Ψ : M → IR is pathwise differentiable parameter at p with efficient
influence curve S(p)(O) = I(O ≤ t) − Ψ(p), and, because the model is locally
saturated, it is also the only influence curve/gradient. So D(p) = I(O ≤ t) −
Ψ(p). Similarly, given a set of user supplied points {t1 , . . . , td }, we could define
Rt
the d-dimensional Euclidean parameter Ψ(p) = (Ψ(p)(tj ) ≡ 0 j p(o)dµ(o) : j =
1, . . . , d) representing the cumulative distribution function at d points. In this
case, D(p) = (I(O ≤ tj ) − Ψ(p)(tj ) : j = 1, . . . , d) has d components.
A general methodology for construction of functions Dh (p) indexed by an
⊥
h ∈ H so that {Dh (p) : h ∈ H} ⊂ Tnuis
(p) (or equality) is presented in
van der Laan and Robins (2003). In van der Laan and Robins (2003) the
class of functions {Dh (p) : h ∈ H} is referred to as a representation of the
orthogonal complement of the nuisance tangent space, which is then used
to map into a class of corresponding estimating functions for the pathwise
differentiable parameter p → Ψ(p) of the form p → Dh (Ψ(p), Υ(p)) with Υ
representing a nuisance parameter. In van der Laan and Robins (2003), for a
variety of general classes of models and censored data structures O, explicit
representations of the orthogonal complement of the nuisance tangent space,
⊥
⊥∗
Tnuis
(p), corresponding gradients, Tnuis
(p), and canonical gradient S ∗(p), have
been provided.
Let p0n = Φ(Pn ) ∈ M be a density estimator of p0 = dP0 /dµ. Define now
a parametric submodel {p0n () :  ∈ IRk } ⊂ M through p0n at  = 0 whose
linear span of scores of  at  = 0 includes all d components of D(pn ). One
possibility is to choose  ∈ IRd of the same dimension as D(p) and arrange
that the score of j at  = 0 equals Dj (p), j = 1, . . . , d. For example, if the
model M is convex then the following model typically applies
p0n () ≡ (1 + > D(p0n ))p0n ,

(1)

where  ∈ IRd denotes the parameter ranging over all values for which p0n ()
is a proper density. Note that indeed p0n (0) = p0n , p0n () is a density (positive
d
valued and integrates till 1) for  small enough, and d
log p0n ()
= D(p0n ).
=0

7

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

One can also use an exponential family
p0n () ≡ C(, p0n ) exp(> D(p0n ))p0n
for C(, p0n ) be a normalizing constant. In general, one can choose a parameterization  → p0n () ∈ M which is smooth in  at  = 0 and whose score at
 = 0 equals D(p0n ). However, we will also consider submodels p0n () with additional scores in order to arrange that the targeted MLE will be fully targeted
towards estimation of D(p0 ).
Let
n
n = (Pn | p0n ) ≡ arg

X

max
0

{:pn ()∈M}

log p0n ()(Oi )

i=1

be the maximum likelihood estimator of  treating the density estimator p0n as
given and fixed. We will assume that the maximum is attained in the interior
of M so that n solves the estimating equation:
0=

d 0
p ()
d n
Pn 0
.
pn ()

R

Here we use the common notation P f ≡ f (o)dP (o). For example, if p0n () =
(1 + > D(p0n ))p0n , as one might choose in convex models, then we have that n
is the solution of
n
1X
D(p0n )(Oi )
0=
.
0
n i=1 1 + >
n D(pn )(Oi )
This defines now an updated density estimator
p1n ≡ p0n (n ) = p0n ((Pn | p0n )) ∈ M.
Note that this simply defines a method for mapping an initial density estimator
p0n ∈ M in a new density estimator p1n ∈ M, which we call the first step
targeted maximum likelihood estimator. By iterating this process one obtains
the k-step targeted maximum likelihood estimator pkn , k = 1, . . .
Definition 1 Given an initial density estimator p0n = Φ̂0 (Pn ) based on the
empirical probability distribution Pn , a parametric fluctuation {p0n () : } ⊂ M
d
satisfying p0n (0) = p0n , and d
log p0n ()
= D∗ (p0n ), where the linear span of
=0
the components of D∗ (p0n ) include all d components of a canonical gradient
D(p0n ) of the parameter of interest Ψ : M → IRd at p0n , a maximum likelihood
estimator
n
(Pn | p0n ) ≡ arg max


DOI: 10.2202/1557-4679.1043

X

log p0n ()(Oi )

i=1

8

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

of , we define the first step targeted maximum likelihood density estimator as
p1n = Φ̂1 (Pn ) ≡ p0n ((Pn | p0n )).
This process can be iterated to define the k-step targeted maximum likelihood
density estimator as
pk+1
= Φ̂k+1 (Pn ) ≡ pkn ((Pn | pkn )), k = 0, 1, . . ..
n
The corresponding k-step targeted maximum likelihood estimator of ψ0 is
defined as
Ψ̂k (Pn ) = Ψ(pkn ).
The targeted maximum likelihood estimator is defined as
= Φ̂∗(Pn ) ≡ lim Ψ(pkn ),

n

k→∞

assuming this limit exists.

2.1

Example: Estimating the CDF.
R

t
Consider an initial data generating density p0 = f , let F (t) = −∞
f (o)do
denote the associated CDF at some fixed point t ∈ IR, and consider the parametric model

(

)

1
1
f (o) = (1 + [I(o ≤ t) − F (t)])f (o) : −
≤≤
,
1 − F (t)
F (t)

(2)

where one can check that the range restraint on  serves merely to ensure that
the family is indeed a proper class of densities. Consider estimating  from
maximum likelihood based on an i.i.d. sample {Oi }ni=1 . The log likelihood is,
l() =

n
X

log(1 + [I(Oi ≤ t) − F (t)]) +

i=1

n
X

log f (Oi ).

(3)

i=1

Its derivative is,
0

l () =

n
X
i=1

I(Oi ≤ t) − F (t)
.
1 + [I(Oi ≤ t) − F (t)]

(4)

Its second derivative is easily seen to be,
00

l () = −

n
X
i=1

(

I(Oi ≤ t) − F (t)
1 + [I(Oi ≤ t) − F (t)]

)2

.

(5)
9

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

Because the log likelihood is concave, we know that the maximum is achieved
if l0 () = 0 has a solution. Letting Fn (·) denote the empirical distribution
function, note that we can decompose the terms in l0 () into two parts (those
for which I(Oi ≤ t) are 0 or 1), and the MLE of  can be seen to solve,
0 = l0()
n
X

I(Oi ≤ t) − F (t)
i=1 1 + [I(Oi ≤ t) − F (t)]
1 − F (t)
−F (t)
= nFn (t)
+ n(1 − Fn (t))
.
1 + [1 − F (t)]
1 − F (t)
=

Moving the second term on the right to the other side of the equation, dividing
both sides by n, and multiplying both sides by (1 + [1 − F (t)])(1 − F (t)),
the equation reduces to,
Fn (t)(1 − F (t))(1 − F (t)) = (1 − Fn (t))F (t)(1 + (1 − F (t))).

(6)

This is linear in , and one can check that the solution is
Fn (t)(1 − F (t)) − (1 − Fn (t))F (t)
F (t)(1 − F (t))
Fn (t) − Fn (t)F (t) − F (t) + Fn (t)F (t)
=
F (t)(1 − F (t))
Fn (t) − F (t)
=
.
F (t)(1 − F (t))

n =

(7)

Because 0 ≤ Fn (t) ≤ 1, one can check that indeed
−

1
F (t)
1 − F (t)
1
=−
≤ n ≤
=
,
1 − F (t)
F (t)(1 − F (t)
F (t)(1 − F (t))
F (t)

(8)

so the range restraint on  for the family (2) always holds for the maximum
likelihood estimator, meaning that fn (·) is a proper density. Now, the resulting CDF at t for this density is then,
Fn (t) =
=
=

Z

t

−∞
Z t
−∞
Z t
−∞

fn (o)do
(1 + n [I(o ≤ t) − F (t)])f (o)do
f (o)do + n

DOI: 10.2202/1557-4679.1043

Z

t
−∞

I(o ≤ t)f (o)do − 1F (t)

Z

t

f (o)do

−∞

10

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

= F (t) + n F (t) − n F (t)2 = F (t) + 1 F (t)(1 − F (t))
Fn (t) − F (t)
= F (t) +
F (t)(1 − F (t)) from (7)
F (t)(1 − F (t))
= F (t) + Fn (t) − F (t) = Fn (t).
Therefore, for any initial density f (·) and any time point t, the targeted likelihood maximum likelihood estimator of the CDF reduces to the empirical
distribution estimator in a single step. This result immediately generalizes to
R
Ψ(p) = A p(o)dµ(o) for any measurable set A.

3

Solving the efficient estimating equation.

We have the following trivial, but useful result. It states that if the MLE’s
(Pn | pkn ) at step k of the targeted MLE algorithm converge to zero for k → ∞
(as one expects to hold if the log likelihood of the data is uniformly bounded
in the model M), then the algorithm converges to a solution of the efficient
influence curve equation Pn D(p) = 0 in the sense that Pn D(pkn ) → 0.
Result 1 Let Pn be given. Assume that
d k
p ()
pk0 (0)
lim lim sup | Pn d k n
− Pn nk
|→ 0,
→0 k→∞
pn ()
pn (0)

(9)
k0

that for each k there exist a constant matrix Ak so that Ak ppnk = D(pkn ) with
n
lim supk→∞ k Ak k< ∞, where k A k denotes a matrix norm.
d

pk ()

If (Pn | pkn ) solves Pn dpk n() = 0 for all k, and (Pn | pkn ) → 0 for k → ∞,
n
then we have
Pn D(pkn ) → 0 for k → ∞.
The condition (9) holds if the score of the one-dimensional submodel p()
at  converges to the score at  = 0 for  → 0 uniformly in a set containing
the k-step targeted MLE’s pkn , k = 1, 2, . . ., and that for each p ∈ M, the
0 (0)
linear span of the components pp(0)
includes the components of D(p). Since
the likelihood increases at each step one might indeed expect that typically the
targeted MLE algorithm will converge and thereby that (Pn | pkn ) → 0. That
is, Result 1 essentially states that, if the targeted MLE algorithm converges,
then the algorithm will converge to a solution of the efficient influence curve
equation in the sense that by choosing k large enough Pn D(pkn ) ≈ 0 with

11

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

arbitrary small deviation from 0.
Proof. Let k = (Pn | pkn ), k = 0, . . .. If k → 0 for k → ∞, then
d k
p ( )
dk n k
Pn k
pn (k )

pk0
(0)
− Pn nk
→0
pn (0)
k0

(0)
for k → ∞. Let Ak be such that Ak ppnk (0)
= D(pkn ). By assumption, the matrix
n
has a norm bounded uniformly in k. Thus, we also have
d k
p ( )
dk n k
Pn Ak k
pn (k )

− Pn D(pkn ) → 0

for k → ∞. However, Pn ddk pkn (k )/pkn (k ) = 0 (and thus Ak applied to this
equals 0 as well), which shows that Pn D(pkn ) → 0. 2

4

Efficiency of targeted likelihood estimation.

In this section we provide templates for proving consistency, asymptotic linearity and efficiency of the targeted maximum likelihood estimator of a path-wise
differentiable parameter. Since convexity of the model and linearity of the
parameter allows a particular strong result, we separate this situation from
the general case.

4.1

Linear parameters in convex models.

Let p∞
n denote the limit of our algorithm if it exists as a density with respect
to µ in M, and otherwise it represents a pkn ∈ M for a large enough k. If
the condition of the above Result 1 holds, then p∞
n ∈ M, and for all practical
∞
purposes, we have Pn D(pn ) = 0. If this is true, then this result can be used
to establish efficiency of the substitution estimator Ψ(p∞
n ) as an estimator
of ψ0 under the assumption that the parameter Ψ : M → IRd is linear and
M is convex, under weak regularity conditions. Specifically, by the identity
for convex models and linear parameters in van der Laan (1998) we have
Ψ(p) − Ψ(p0 ) = −P0 D(p) for any p, p0 ∈ M for which p0 /p < ∞. Thus, if
p∞
n ∈ M and it is bounded away from 0 on the support of p0 , then combining
Pn D(p∞
n ) = 0 with the latter identity gives us
∞
Ψ(p∞
n ) − Ψ(p0 ) = (Pn − P0 )D(pn ).

(10)

∞
∞
Even if p∞
n does not satisfy p0 /pn < ∞, then the identity Ψ(pn ) − Ψ(p0 ) =
∞
−P0 D(pn ) can still be established under a continuity condition on p → P0 D(p)

DOI: 10.2202/1557-4679.1043

12

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

(see van der Laan (1998)), so that (10) can even be established for density
estimators not satisfying this support condition.
Applying empirical process theory (van der Vaart and Wellner (1996a)) now
∞
proves that Ψ(p∞
n ) is root-n consistent if D(pn ) falls in a P0 Donsker class with
2
probability tending to 1. If one can now also establish that P0 (D(p∞
n )−D(p1 ))
converges to zero in probability for a certain p1 ∈ M, then it follows that
Ψ(p∞
n ) is asymptotically linear with influence curve D0 (p1 ) ≡ D(p1 )−P0 D(p1 ):
√
Ψ(p∞
n ) − Ψ(p0 ) = (Pn − P0 )D0 (p1 ) + oP (1/ n),
where we note that p1 can be an arbitrary limit (i.e., p1 6 =p0 is allowed). In
particular, if the limit p1 is such that D(p1 ) = D(p0 ), then Ψ(p∞
n ) is asymptotically linear with influence curve D(p0 ). Thus, if D(p0 ) is the efficient influence
curve, then Ψ(p∞
n ) is asymptotically efficient.
Theorem 1 Suppose the conclusion of Result 1 holds, and K = K(n) is
chosen large enough so that the targeted MLE pn = pK
n satisfies Pn D(pn ) =
√
R(n, K(n)) = oP (1/ n) (where limK→∞ R(n, K) = 0). Assume that pn ∈ M,
p0 /pn < ∞ uniformly over a support of p0 , M is convex, and Ψ : M → IRd is
linear. Then
Ψ(pn ) − Ψ(p0 ) = (Pn − P0 )D(pn ) + R(n, K(n)).
If D(pn ) falls in a P0 Donsker class with probability tending to 1, then
√
Ψ(pn ) − ψ0 = OP (1/ n).
If it is also shown that P0 (D(pn ) − D(p1 ))2 → 0 in probability for n → ∞ for
some p1 ∈ M, then it follows that Ψ(pn ) is asymptotically linear with influence
curve D(p1 ) − P0 D(p1 ):
√
Ψ(pn ) − Ψ(p0 ) = (Pn − P0 )D(p1 ) + oP (1/ n).
In particular, if D(p1 ) = D(p0 ), and D(p0 ) is the efficient influence curve of
Ψ at p0 , then Ψ(pn ) is asymptotically efficient.
This shows that the targeted MLE of a linear parameter in a convex model
is typically consistent and asymptotically linear for arbitrary starting density
∞
p0n , and if the targeted MLE p∞
n is consistent in the sense that P0 (D(pn ) −
2
D(p0 )) → 0 with probability tending to 1 for n converging to infinity (e.g.,
the initial starting density p0n would already yield a consistent estimator D(pn0 )
of D(p0 )), then the targeted MLE will also be efficient. We will now provide
13

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

two examples illustrating this theorem. The first example represents a case
in which the targeted MLE is efficient for arbitrary starting density p0n . The
second example represents the case that the targeted MLE is consistent and
asymptotically linear for arbitrary starting density p0n , and is efficient if the
starting density consistently estimates D(p0 ).
Example 1 ((Efficiency of a smooth cumulative distribution funcR
tion) In this example we have D(p)(O) = I(O ≤ t) − 0t p(o)dµ(o). A targeted MLE pn solving Pn D(pn ) = 0 satisfies that Ψ(pn ) = Pn I(· ≤ t) equals
the empirical cumulative distribution function at t and is therefore asymptotically efficient, for arbitrary starting density p0 . Thus in this example the
initial density does not need to be consistent in order to make the targeted
MLE asymptotically efficient. Suppose that p0nh is indexed by a bandwidth
or model choice h, and let p∗nh be the targeted MLE density estimator using
as starting density p0nh . Each of the targeted MLE’s p∗nh results in the same
estimator of the cumulative distribution function Ψ(p0 ) at time t. If one uses
likelihood cross-validation to select h, then one selects among all of these targeted MLE’s the one which is supposedly closest to the true density p0 with
respect to Kullback-Leibler divergence, which now provides a valid and reasonable criteria since all the candidates density estimators already map into
efficient (and algebraically equivalent) estimators of ψ0.
Example 2 ((Local efficiency of targeted MLE based on censored
data) We consider a particular example of a censored data structure to illustrate that Theorem 1 yields local efficiency of the targeted MLE based on
CAR censored data structures based on any starting density p0n , under very
weak conditions.
Suppose that the full data structure X = (W, Y (a) : a ∈ {0, 1}) on the
experimental unit consists of a set of baseline covariates W , and treatment
specific outcomes Y (a), indexed by treatment values a ∈ {0, 1}. Suppose that
the observed data structure O = (W, A, Y = Y (A)) ∼ p0 , and it is assumed
that the conditional probability distribution g0 (· | X) of A, given X, satisfies
g0 (A | X) = g0 (A | W ): that is, A is independent of X, given W . Suppose
that this conditional probability distribution of g0 (A | W ) of A, given W , is
known, and satisfies 0 < g0 (1 | W ) < 1, as it would be in a randomized trial
aiming to establish the causal effect of A on Y . Let M be the class of all
densities of O with respect to an appropriate dominating measure. We have
M = {p(O) = QXA (W, Y )g0 (A | X) : QX0, QX1},
where the full data sub-distributions QXa(w, y) = PW,Y (a)(w, y) are joint densities of (W, Y (a)), a ∈ {0, 1}, and are unspecified. As a consequence, M is
DOI: 10.2202/1557-4679.1043

14

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

a convex model. Let Ψ : M → IR be defined as Ψ(p) = Ep (Y (1) − Y (0)) =
Ep (Ep (Y | A = 1, W ) − Ep (Y | A = 0, W )), which is often called the marginal
causal effect of treatment A on the outcome Y . In this case, Ψ(p) is pathwise
differentiable at p with efficient influence curve S(p) defined by
S(p) =

(Y − Q(p)(A, W ))(A − (1 − A))
+ Q(p)(1, W ) − Q(p)(0, W ) − Ψ(p),
g(p)(A | W )

where g(p)(· | W ) = P rp (A = · | W ) = g0 (· | W ), and Q(p)(A, W ) =
Ep (Y | A, W ). Note that Ψ(p) depends on p through Q(p) and its marginal
distribution pW of W . Due to the factorization of the density of O in a QX factor and g0 factor, this is also the efficient influence curve if g0 is unknown
or modelled. The class of all gradients at p ∈ M is given by:
(

)

(Y − Q(A, W ))(I(A = 1) − I(A = 0))
+ Q(1, W ) − Q(0, W ) − Ψ(p) : Q ,
g0 (A | W )

where Q can be an arbitrary function of A, W .
So we could define
DQ (p)(O) ≡

(Y − Q(A, W ))(A − (1 − A))
+ Q(1, W ) − Q(0, W ) − Ψ(p),
g0 (A | W )

and D(p) = DQ(p) (p) represents the efficient influence curve. We are now ready
to define the targeted MLE of p0 with respect to the parameter ψ0 .
Let p0n be an initial density estimator of p0 . For example, p0n could correspond with the empirical distribution of W , and a normal distribution for
the conditional density of Y , given A, W , with mean Q0n (A, W ) and variance
σn2 (A, W ), where Q0n is an estimate of Q(p0 )(A, W ) = E0 (Y | A, W ). Let p∗n be
a targeted MLE, as we explicitly define in the later Section 6 in detail, solving
Pn D(p∗n ) = 0. In Section 6, we show for a particular hardest submodel pkn ()
consisting of normal densities of Y , conditional on A, W , with  corresponding
with a fluctuation of current regression Qkn (A, W ), that the targeted MLE is
achieved in the first step (i.e., p∗n = p1n ), and indeed solves the score equation
Pn D(p1n ) = 0. Let’s consider this particular targeted MLE for illustration, but
the following arguments apply to any targeted MLE solving Pn D(p∗n ) = 0.
Application of the theorem teaches us that
Ψ(p∗n ) − ψ0 = (Pn − P0 )DQ(p∗n ) .
Since g0 is bounded away from zero, if Q1n is a nice smooth function (e.g.,
with a uniformly bounded uniform sectional variation norm, van der Laan
15

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11
∗
(1995)), it follows that
√ DQ(pn) falls in a P0 -Donsker class, and0 thus that
∗
Ψ(pn ) − ψ0 = OP (1/ n). If the initial regression estimator Qn = Q(p0n )
converges to a possibly misspecified Q1 = Q(p1 ), then it follows that Ψ(p∗n )
is asymptotically linear with influence curve DQ(p1 )(O), where p1 is the possibly misspecified limit of p1n . Finally, if Q0n is actually consistent for Q(p0),
then the targeted MLE of ψ0 is asymptotically efficient. We can use likelihood
based cross-validation to select among targeted MLE’s indexed by different
candidate initial estimators Q0n , thereby improving the efficiency relative to
a targeted MLE with a fixed initial Q0n . Thus this example teaches us that
the targeted MLE Ψ(p∗n ) of ψ0 , which typically equals the first step targeted
MLE, is consistent and asymptotically linear for arbitrary initial regression
estimator Q0n , and it is efficient if Q0n happens to be consistent, where the
latter can potentially be achieved by using a machine learning type algorithm
and selecting the fine tuning parameters with likelihood based cross-validation.
These results still carry through if g0 is unknown but is known to belong to a
parametric model.

4.2

Local efficiency for general smooth parameters.

The remarkable robustness with respect to the starting density p0n as observed
in the previous subsection is a consequence of the convexity of the model and
linearity of the parameter Ψ. In general, such results cannot be expected to
hold. In this subsection we present a more general approach for establishing
the wished asymptotic linearity and efficiency of the targeted MLE of any
pathwise differentiable parameter.
Let p∞
n ∈ M denote the limit of the targeted MLE algorithm if it exists
and otherwise it represents a pkn for a large k. If the targeted MLE solves
the efficient influence curve equation, then for all practical purposes, we have
Pn D(p∞
n ) = 0. Let R(p, p0 ) be defined by
Ψ(p) − Ψ(p0 ) = −P0 D(p) + R(p, p0 )
for any p ∈ M. We note that by pathwise differentiability of Ψ at p, R(p, p0 )
represents a second order term in the difference p − p0 . Combining Pn D(p∞
n ) =
0 with the latter identity gives us
∞
∞
Ψ(p∞
n ) − Ψ(p0 ) = (Pn − P0 )D(pn ) + R(pn , p0 ).

Applying empirical process theory now proves that Ψ(p∞
n ) is root-n consis∞
tent if D(pn ) falls√in a P0 Donsker class with probability tending to 1, and
∞
2
R(p∞
n , p0 ) = oP (1/ n). If one can now also establish that P0 (D(pn ) − D(p1 ))
DOI: 10.2202/1557-4679.1043

16

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

converges to zero in probability for a possibly misspecified p1 ∈ M, then it follows that Ψ(p∞
n ) is asymptotically linear with influence curve D(p1 )−P0 D(p1 ):
√
Ψ(p∞
n ) − Ψ(p0 ) = (Pn − P0 )D(p1 ) + oP (1/ n).
In particular, if D(p1 ) = D(p0 ), then the targeted MLE is asymptotically effi√
cient. Note that the asymptotic linearity requires that R(p∞
n , p0 ) = oP (1/ n),
while the convexity of the model and linearity of the parameter as assumed in
the previous subsection allowed us to avoid such a condition: i.e. in that case
we had R(p, p0 ) = 0 for arbitrary p ∈ M with p0 /p < ∞.

5

Fusion of MLE and estimating equations

In this section we show that the targeted MLE can be viewed as a solution of
an optimal estimating equation for the parameter of interest, if one estimates
the nuisance parameters with the targeted MLE itself. This comparison can
only be made by making the assumption that the efficient influence curve can
be viewed as an estimating function of the parameter of interest, which is
needed for the estimating function methodology (van der Laan and Robins
(2003)), but not for targeted MLE.
As previously argued, a sieve-based maximum likelihood estimator of a
pathwise differentiable parameter is based on choices such as the sieve and
the criteria for trading off variance and bias, which is completely unrelated to
the actual parameter Ψ. As a consequence, such likelihood based estimators
suffer, in principle, from serious bias for the parameter of interest ψ0. Let p0n
be such a likelihood based estimator of p0 and Ψ(p0n ) be the corresponding
substitution estimator of ψ0 .
On the other hand, estimating function methodology (van der Laan and
Robins (2003)) constructs estimating functions Dh (ψ, υ)(O) for the parameter
of interest ψ indexed by a choice h, based on a representation of the orthogonal
⊥
complement of the nuisance tangent space p → Tnuis
(p) (i.e., Dh (Ψ(p), Υ(p)) ∈
⊥
Tnuis (p) for all h), which typically also depend on an unknown nuisance parameter Υ satisfying Ep Dh (Ψ(p), Υ(p)) = 0 for all p ∈ M. The current
recommendation in estimating function methodology (see e.g., van der Laan
and Robins (2003)) proposes to use an external estimator υn of nuisance parameters and estimate ψ0 with the solution of 0 = Pn Dhn (ψ, υn ) = 0 in ψ. For
example, one could use the maximum likelihood estimator p0n and estimate ψ0
with the solution ψn0 of 0 = Pn Dh(p0n )(ψ, Υ(p0n )). This estimator ψn0 is not
necessarily, and in fact, will typically not be equal to Ψ(p0n ). Thus, even if
the nuisance parameters are based on a maximum likelihood estimator p0n , the
17

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

resulting estimating function based estimators of ψ0 are intrinsically different
from (and less biased than) the likelihood based estimator Ψ(p0n ).
However, let pn be the targeted maximum likelihood estimator based on
hardest submodels at p with efficient influence curve D(p) = Dh(p) (Ψ(p), Υ(p))
and starting with the initial density estimator p0n , so that pn solves Pn D(pn ) =
Dh(pn )(Ψ(pn ), Υ(pn )) = 0. Again, we consider the (now targeted) maximum
likelihood estimator Ψ(pn ) versus the estimating function based estimator
described in the previous paragraph. The estimating function based estimator ψn of ψ0 is defined as the solution of the estimating equation 0 =
Pn Dh(pn ) (ψ, Υ(pn )), which differs from above by now using the targeted MLE
pn (based on p0n ) to estimate the index and nuisance parameters (instead of
likelihood based p0n ). Because Pn Dh(pn ) (Ψ(pn ), Υ(pn )) = 0, it follows that the
estimating function based estimator ψn now equals Ψ(pn ), assuming that this
solution is unique. That is, if one estimates the nuisance parameters and index
in the estimating function methodology with a targeted maximum likelihood
estimator pn , then the (or, at least, one of the) estimating function based estimator ψn and the targeted maximum likelihood estimator Ψ(pn ) are identical.
Note that the targeted MLE is more general than the estimating function
based methodology since it does not require the representation of an estimating
function as a function of the parameter of interest and a variation independent nuisance parameter, thereby making it more widely applicable. Another
advantage of targeted MLE relative to estimating function based estimation
that it is invariant to monotone transformations of the parameter of interest.

5.1

CAR-censored data models

This targeted MLE approach has a particular nice application in estimation of
pathwise differentiable parameters based on censored data under the coarsening at random assumption (Heitjan and Rubin (1991), Jacobsen and Keiding
(1995), Gill et al. (1997), van der Laan and Robins (2003)). That is, let
O = Φ(C, X) ∼ p0 for some known many to one mapping Φ, X ∼ FX0 is
the full data structure one wishes to observe on a randomly sampled experimental unit, and assume that the conditional distribution of the censoring
variable C, given X, i.e., the censoring mechanism, satisfies coarsening at
random (CAR). In this case it is known that the density of O factorizes as:
p0 (0) = g(p0 )(O | X)Q(p0 )(O), where g(p0 )(O | X) (which is only a function
of O by CAR) is the conditional density of O, given X, which thus only depends on the conditional distribution of C, given X. The Q(p0 ) factor only
depends on the distribution FX0 of the full data structure X (van der Laan
and Robins (2003)). Thus given a model M for O obtained by modelling
DOI: 10.2202/1557-4679.1043

18

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

FX0 and or the censoring mechanism g0 (O | X), each p ∈ M is identified
by (g(p), Q(p)). Let Ψ(p) = Ψ(Q(p)) be a pathwise differentiable parameter of the Q(p)-part of the density p of O: i.e., it represents an identifiable
parameter of FX . In this case, it is known that the efficient influence curve
D(p) = D(g(p), Q(p)) at p ∈ M is orthogonal to the tangent space TCAR (p)
of the censoring mechanism g at p only assuming CAR (i.e., the Hilbert space
in L20 (P ) spanned by all scores of parametric submodels through g(p) at p),
where TCAR (p) = {h(O) : Ep (h(O) | X) = 0} consists of all functions of O
with conditional mean, given X, equal to zero. As a consequence, given an
initial estimator Q0 of Q(p0 ) and g 0 of g(p0 ), a hardest parametric model for
0
0
0
0 0
0 can be chosen to be of the form p () ≈ (1 + D(p ))p = g Q (), where
0
0 0
0
Q () ≈ (1 + D(Q , g ))Q . That is, the hardest parametric model only corresponds with changing Q0 , but it leaves g 0 untouched. The targeted MLE
approach proceeds now as defined above.

5.2

Targeting the censoring mechanism.

In this subsection we propose a targeted maximum likelihood methodology
for estimation of ψ0 which involves updating of estimators of both g0 and Q0.
As shown in van der Laan and Robins (2003) (Theorem 1.3), we have that
any gradient D(p) can be decomposed as D(p) = DIP CW (p) − DCAR (p) with
DIP CW being a so called Inverse Probability of Censoring Weighted (IPCW)
function, and DCAR (p) = Π(DIP CW (p) | TCAR (p)) is the projection of the
IPCW function DIP CW (p) onto TCAR (p) in the Hilbert space L20 (p). In order
to relate these functions to estimating functions for ψ0 (as in van der Laan and
Robins (2003)) we will also sometimes use DIP CW (p) = DIP CW (g(p), Ψ(p))
and D(p) = D(g(p), Q(p), Ψ(p)) in the case that these functions can be represented as an estimating function in ψ indexed by nuisance parameters being functions of g(p) and Q(p): we note that the IPCW estimating function
typically only depends on p through g(p) and Ψ(p). Given an initial estimator p0n = (gn0 , Q0n ), in the censored data literature one defines the IPCWestimator and DR-IPCW estimator as the solutions of the estimating equations Pn DIP CW (gn0 , ψ) = 0 and Pn D(gn0 , Q0n , ψ) = 0, respectively, and Ψ(Q0n )
is called the likelihood based estimator (making the assumption that Q0n is
likelihood based).
We will now describe the targeted MLE algorithm also involving the updating of gn0 . At step k it now involves also a parametric submodel g(pkn )(2)
through g(pkn ) with score DCAR (gnk , Qkn ) at 2 = 0. It can be shown that
DCAR (g(p), Q(p)) corresponds with the efficient influence curve of the parameter Φ(g) = Ep DIP CW (g, Q(p)) at g = g(p), so that this parametric submodel
19

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

makes the estimator of g0 targeted for estimation of the mean of the IP CW component of the efficient influence curve. In particular, it is also the parametric submodel which makes the IPCW estimator ψn,IP CW , defined as the
solution of the IPCW estimating equation 0 = Pn DIP CW (gn , ψ), efficient if
the submodel is correctly specified, under regularity conditions. As above, let
Qkn (1) be a parametric submodel through Qkn with score D(gnk , Qkn ) at 1 = 0.
Targeted MLE algorithm:
• Set k = 0.
• Let pkn = (gnk , Qkn ).
• Let 1nk = arg max1 Pn log Qkn (1), and 2nk = arg max2 Pn log gnk (2).
• Set gnk+1 = gnk (2n ) and Qk+1
= Qkn (1n). Set pk+1
= (gnk+1 , Qk+1
n
n
n ).
• Set k = k + 1, and iterate this process utill convergence.
If 1nk and 2nk converge to zero for k → ∞ (which can be expected because
both factors g and Q of the likelihood are increasing at each step), then the
targeted MLE algorithm will converge to a simultaneous solution of
lim Pn DCAR (g k , Qk ) = 0 and lim Pn D(g k , Qk ) = 0.
k

k

Equivalence of IPCW, DR-IPCW, and targeted MLE: As a consequence of the decomposition D(p) = DIP CW (p) − DCAR (p), this implies also
limk DIP CW (g k , Ψ(Qk )) = 0. Note that the double robust IPCW estimator
defined as the solution in ψ of Pn D(gnk , Qkn , ψ) = 0, the targeted maximum
likelihood estimator Ψ(Qkn ), and the IPCW estimator defined as the solution
of Pn D(gnk , ψ) = 0, all based on these targeted MLE’s gnk , Qkn are identical
up to an arbitrarily small error decreasing in k (assuming uniqueness of the
DR-IPCW and IPCW solution).

6

Examples of targeted maximum likelihood.

In this section we provide some important examples of the targeted MLE
to illustrate its remarkable simplicity and good properties. For additional
examples we refer to our accompanying technical report.

DOI: 10.2202/1557-4679.1043

20

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

6.1

Estimation of a mean in a nonparametric model.

Consider an initial data generating density p0n (with respect to a dominating
measure µ) of a possibly multivariate random variable O, a given function
w(·), and define the parameter of interest as
Ψ(p) = Ep [w(O)] =

Z

w(o)p(o)dµ(o).

For the exponential family
(

p0n ()(x)

)

exp((w(x) − ψn0 ))p0n (x)
=R
: ,
exp((w(x) − ψn0 ))p0n (x)dµ(x)

consider attempting to estimate  with maximum likelihood based on an i.i.d.
sample {Oi }ni=1 . Here ψn0 = Ψ(p0n ). The log likelihood is then,
l() =

n
X

[log(p0n (Oi ))+(w(Oi )−ψn0 )−log

Z

exp((w(x) −

ψn0 ))p0n (x)dµ(x)



].

i=1

In our accompanying technical report we show that (for each initial p0n ) the onestep targeted maximum likelihood estimator Ψ(p1n ) = Ψ(p0n (n ) of the mean of
P
w(O) equals the sample mean W̄n = n1 ni=1 w(Oi ). For the detailed proof we
refer to our technical report.

6.2

Estimation of a marginal causal effect.

Double robust locally efficient estimation of the causal effect of a point treatment assuming a marginal structural model has been provided in Robins
(2000), Robins and Rotnitzky (2001), and Robins et al. (2000): see also van der
Laan and Robins (2003).
Let O = (W, A, Y ), W be a vector of baseline covariates, A be a binary
treatment variable, and Y an outcome of interest. Let M be the class of all
densities of O with respect to an appropriate dominating measure: so M is
nonparametric up to possible smoothness conditions. Let Ψ : M → IR be
defined as Ψ(p) = Ep (Ep (Y | A = 1, W ) − Ep (Y | A = 0, W )), where it is
assumed 0 < P (A = 1 | W ) < 1 with probability one so that this parameter
is well defined. This parameter corresponds with the marginal causal effect of
A on Y if one assumes the usual consistency assumption, temporal ordering
assumption, and randomization assumption required for causal inference. In
order to acknowledge that this parameter is of interest in general, van der
Laan (2006) refers to this parameter as the variable importance of variable
21

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

A. This parameter Ψ(p) is pathwise differentiable at p with efficient influence
curve S(p) defined by
S(p) =

(Y − Q(p)(A, W ))(I(A = 1) − I(A = 0))
g(p)(A | W )
+Q(p)(1, W ) − Q(p)(0, W ) − Ψ(p),

where g(p)(· | W ) = P rp (A = · | W ), and Q(p)(A, W ) = Ep (Y | A, W )
(see e.g., Robins (2000), van der Laan (2006)). Note that Ψ(p) depends on
p through Q(p) and its marginal distribution pW of W . Because the model
is locally saturated, it is also the only influence curve/gradient (Gill et al.
(1997)). So we set D(p) = S(p).
We can decompose this efficient score D(p) into three subcomponents as
follows:
D(p) = D(p) − Ep (D(p) | A, W ) + Ep(D(p) | A, W ) − Ep (D(p) | W )
+Ep (D(p) | W ) − Ep D(p),
which corresponds with scores for p(Y | A, W ), p(A|W ) and p(W ), respectively. We have
D1 (p)(O) ≡ D(p) − Ep (D(p) | A, W )
A − (1 − A)
= (Y − Q(p)(A, W ))
g(p)(A | W )
Ep (D(p) | A, W ) − Ep (D(p) | W ) = 0
D2 (p) ≡ Ep (D(p) | W ) − Ep (D(p))
= Q(p)(1, W ) − Q(p)(0, W ) − Ψ(p).
Consider an initial density estimator p0n of the density p0 of (W, A, Y )
with marginal distribution of W being the empirical probability distribution
of W1 , . . . , Wn . We have that D(p0n ) = D1 (p0n ) + D2 (p0n ) and thus that a onedimensional p0n () with score D(p0n ) at  = 0 corresponds with a zero score for
g(p0n ). In addition, we have that Pn D2 (p0n ) = 0 (i.e., the empirical distribution
of W is a nonparametric maximum likelihood estimator) so that p0n () can be
selected to only vary p0n (Y | A, W ) with a score D1 (pn ) at  = 0.
We now propose an easily implemented targeted maximum likelihood estimator of the marginal causal effect by using a normal regression model as
hardest submodel. Specifically, consider an initial density estimator p0n with
marginal distribution of W equal to the empirical probability distribution
of W1 , . . . , Wn , and let the conditional probability density p0n (Y | A, W ) =
DOI: 10.2202/1557-4679.1043

22

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning
1
f ({Y − Q0n (A, W )}/σ(Q0n)(A, W )) be a normal density with mean
σ(Q0n )(A,W ) 0
Q0n (A, W ) and variance σ(Q0n )2 (A, W ). Here f0 denotes the N (0, 1) density.
In addition, g(pn0 )(A | W ) is a particular fit of the conditional density of A,
given W . We now consider as possible submodels p0n ()

p0n ()(Y

!

1
Y − Q0n (A, W ) − h(p0n )(A, W )
| A, W ) =
f
,
0
σ(Q0n (A, W )
σ(Q0n )(A, W )

where the function h will be specified so that the score of p0n at  = 0 equals
the efficient influence curve at p0n . The maximum likelihood estimator of  is
simply given by the weighted least squares estimator for a univariate linear
regression model:
n = arg min


n
X

(Yi − Q0n (Ai, Wi ) − h(p0n )(Ai , Wi ))2

i=1

1
σ(Q0n )2(Ai , Wi )

.

The score of p0n ()(Y | A, W ) at a value  is given by:
S() = −

Y − Q0n (A, W ) − h(p0n )(A, W )
h(p0n )(A, W ),
0
2
σ(Qn) (A, W )

and n solves indeed Pn S(n ) = 0. If we set
h(p0n )(A, W )

≡

!

I(A = 1)
I(A = 0)
− 0
σ(Q0n)2 (A, W ),
0
gn (1 | W ) gn (0 | W )

then the score S(0) = D1 (p0n ) = (Y − Q0n (A, W ))(I(A = 1)/gn0 (1 | W ) − I(A =
0)/gn0 (0 | W )) of p0n ()(Y | A, W ) at  = 0 corresponds with the efficient
influence curve at p0n . As in our previous subsection, since p0n (W ) equals the
empirical distribution of W the MLE of 1 → Pn log p0 (1 )(W ) equals  = 0,
and gn0 (A | W ) will not be varied by p0n (): that is, the marginal distribution
of W and the treatment mechanism g 0 (A | W ) will not be updated in the
algorithm for calculating the targeted maximum likelihood estimator.
Let p1n = p0n (n ) whose conditional distribution of Y , given A, W , is a
normal density with mean Q1n (A, W ) and variance σ 2(Q1n )(A, W ), where
Q1n (A, W ) = Q(p1n )(A, W ) = Q0n (A, W ) + n h(p0n )(A, W ).
The corresponding estimate of ψ0 is given by
Ψ(p1n ) =

n
1X
Q1 (1, Wi ) − Q1n (0, Wi ).
n i=1 n

23

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

It is straightforward to show that Pn D(p1n ) = 0 in the case that σn0 (A, W )
is constant in the model {p0n () : }, but is simply set at an initial estimate.
Thus in this case the targeted maximum likelihood is achieved at the first step.
For arbitrary fixed values of σ(A, W ), the targeted MLE is locally efficient in
the sense that if g(p0n ) is consistent at some rate, then it is consistent and
asymptotically linear for arbitrary Q0n , and it is efficient if Q0n is consistent for
Q0(A, W ). Likewise, a consistent Q1n (A, W ) will lead to a consistent estimator
of the parameter of interest ψ0 , even with an arbitrary fit of the treatment
mechanism g(A|W ). Iterative estimation of σ provides no (asymptotic) reward, and could simply be omitted by setting (e.g.) σ at an initial estimate,
so that the targeted MLE is achieved in a single step.

6.3

Targeting the treatment mechanism as well.

We will now proceed with this example, but also use for g0 a targeted maximum
likelihood estimator. Our goal is to make the IPTW estimator ψn,IP T W =
I(Ai =1)−I(Ai =0)
1 Pn
corresponding wiht the targeted MLE gn an efficient
i=1 Yi
n
gn (Ai |Wi )
0
estimator. Let g(pn )(A | W ) be an initial estimator and represent it as a
logistic function:
g(p0n )(1 | W ) =

1
.
1 + exp(−m0n(W ))

Consider as parametric submodel
g(p0n )(2 )(1 | W ) =

1
1+

exp(−m0n (W )

− 2 h(p0n )(W ))

.

(11)

Let 2n = arg max Pn log g(p0n )(). In practice this can be done by fitting a
logistic regression in the covariates m0n (W ) and h(p0n )(W ), setting the intercept equal to zero, and setting the coefficient in front of m0n (W ) equal to 1,
and set 2n equal to fitted coefficient in front of h(p0n )(W ). It is also fine to
refit the intercept and coefficient in front of m0n (W ), since choosing additional
parameters still guarantees that the linear span of scores includes the score of
h(p0n )(W ). We have
d
log g(p0n )(2 )
(O) = h(p0n )(W )(A − g(p0n )(1 | W )).
d2
2 =0
Solving for h so that
h(W )(A − g(p0n )(1 | W )) = DCAR (p0n )(O)
DOI: 10.2202/1557-4679.1043

24

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

=

Q(p0n )(A, W )
{I(A = 1) − I(A = 0)}
gn0 (A | W )
−{Q(p0n )(1, W ) − Q(p0n )(0, W )}

yields the solution
h(p0n )(W ) =

Q(p0n )(1, W )
Q(p0n )(0, W )
+
.
g(p0n )(1 | W ) g(p0n )(0 | W )

We are now ready to present the proposed targeted MLE which also targets
the treatment mechanism fit.
The algorithm for targeted maximum likelihood estimation of a
marginal causal effect, including the targeting of the treatment mechanism. Thus the algorithm for targeted maximum likelihood estimation of
0
0 can be described as follows. Let k = 0, and let g (A | W ) and the regression
fit Q0 (A, W ) of E0(Y | A, W ) be given. Let
hk1 = h1 (g k , Qk )(A, W ) ≡

!

I(A = 1)
I(A = 0)
− k
σ(Qk )2 (A, W )
k
g (1 | W ) g (0 | W )

and
hk2 = h2 (g k , Qk )(W ) =

Qk (1, W )
Qk (0, W )
+
.
g k (1 | W ) g k (0 | W )

Let mk (W ) = log(g k (1 | W )/g k (0 | W )) so that g k (1 | W ) = 1/(1 +
exp(−mk (W )). Consider the logistic regression model
g k (2 )(1 | W ) =

1
1+

exp(−mk (W )

− 2 hk2 (W ))

.

Let 2n (k) = arg max2 Pn log g k (2 ) be the maximum likelihood estimator of
this univariate logistic regression model, and let
1n (k) = arg min
1

n
X

(Yi − Qk (Ai, Wi ) − 1hk1 (Ai, Wi ))2

i=1

1
σ(Qk )2(Ai , Wi )

,

the univariate least squares estimator of 1 .
Now, update g k and Qk as follows:
Qk+1 (A, W ) = Qk (A, W ) + 1n (k)hk1 (A, W )
mk+1 (A, W ) = mk (W ) + 2n (k)hk2 (W )
1
g k+1 (A | W ) =
1 + exp(−mk+1 (W ))
Set k = k + 1 and iterate this algorithm.
25

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

Equivalence of IPTW, DR-IPTW, and targeted maximum likelihood estimators. Recall that the efficient influence curve function is decomposed as D(g, Q)(O) = DIP T W (g, Q) − DCAR (g, Q), where DIP T W (g, Q) =
)
Y
(I(A = 1) − I(A = 0)) − Ψ(Q), and DCAR (g, Q) = Q(A,W
(I(A =
g(A|W )
g(A|W )
1) − I(A = 0)) − (Q(1, W ) − Q(0, W )). For k converging to infinity the
targeted MLE yields a final estimator gn of the treatment mechanism and a
regression fit Qn (A, W ) so that the score equations of the two submodels in 1
and 2 are solved at 1 = 2 = 0:
Pn D(gn , Qn ) = 0 and Pn DCAR (gn , Qn ) = 0.
This implies also that
Pn DIP T W (gn , Qn ) = 0.
Thus, we can conclude that the three estimators
Ψn,IP T W

n
1X
Yi
=
(I(Ai = 1) − I(Ai = 0))
n i=1 gn (Ai | Wi )
n
1X
Yi
(I(Ai = 1) − I(Ai = 0))
n i=1 gn (Ai | Wi )
− DCAR (gn , Qn )(Ai , Wi )
n
1X
=
Qn (1, Wi ) − Qn (0, Wi )
n i=1

Ψn,DR−IP T W =

Ψn,M LE

are algebraically identical: Ψn,IP T W = Ψn,DR−IP T W = Ψn,M LE . That is, the
targeted MLE Ψ(Qn ) equals the IPTW and DR-IPTW estimator based on the
targeted MLE (gn , Qn ) as estimators of the nuisance parameters (g0 , Q0) in the
corresponding estimating equations. Preliminary results suggest that consistency of the resulting targeted likelihood algorithm depends on the consistency
of either the g0 or Q0 component of the initial density estimator.

6.4

Simulation for marginal variable importance.

Simulated data can be used to illustrate the benefits of the targeted likelihood
procedure. We simulated replicates of the data structure O = (W, A, Y ) ∼ p0
representing baseline covariates, a binary treatment, and a response measurement on a subject, and attempted to estimate the causal effect of treatment
A on response Y . We generated 1000 datasets of size n = 200 according to
the following mechanism:
W ∼ U (0, 1)
DOI: 10.2202/1557-4679.1043

26

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

A ∈ {0, 1}
1
1 + exp(−8W 2 + 8W − 1)
 ∼ N (0, 1),  ⊥ (W, A)
Y = AQ(1, W ) + (1 − A)Q(0, W ) + 
2
Q(0, W ) = − , Q(1, W ) = −(8W 2 − 8W + 1)
3
g(1|W ) = P (A = 1|W ) =

Here O represented a censored data structure. The unavailable counterfactual data was given by,
X = (W, Y0, Y1 ) = (W, Q(0, W ) + , Q(1, W ) + ).
It could be be verified that the coarsening at random assumption held, or that,
{A ⊥ X|W },
as well as the experimental treatment assignment assumption, implied by,
0 < 0.26 < g(1|W ) < .74 < 1 with probability one.
Together these assumptions made it possible to estimate the parameter,
Ψ(p0 ) = E[Y1 ] − E[Y0] = 1,
representing the counterfactual mean difference between the treatment group
(A = 1) and the control group (A = 0).
The standard estimators for this problem are the inverse probability of
treatment (IPTW), maximum likelihood (G-computation), and doubly robust
(efficient) estimators. These respectively depend on fitting either the censoring
mechanism g or the nuisance parameter Q(A, W ) = E[Y |W ], and are given as
A
1−A
follows, where hg (A, W ) = g(1|W
− g(0|W
:
)
)
n
1X
Ψn,IPTW (g) =
Yi hg (Ai , Wi )
n i=1

Ψn,MLE (Q) =

n
1X
[Q(1, Wi ) − Q(0, Wi )]
n i=1

n
1X
Ψn,DR-IPTW (g, Q) = Ψn,IPTW + Ψn,MLE −
hg (Ai, Wi )Q(Ai, Wi )
n i=1

Typically estimation is based on forming external estimates of at least one
of the two nuisance parameters g or Q, and then applying one of the IPTW,
27

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

maximum likelihood, or double robust estimators. The three estimators can
potentially be very different from one another, leading to difficulties when
interpreting the data. Targeted likelihood resolves this problem, by estimating
both nuisance parameters g and Q accurately with maximum likelihood, but in
a way so that the IPTW, maximum likelihood, and doubly robust estimators
are algebraically equivalent.
As our initial fit to p0 prescribed that {Y |A, W } followed a Gaussian distribution with fixed variance, the hardest one-dimensional submodel  → p
for estimation of Ψ(p0 ) could be given by,
2
{Y |A, W } ∼ N (Q(0)
n (A, W ) + hg (A, W ), σ ),

while the laws of {W } and {A|W } were left unchanged. The maximum likelihood estimator of  became,
n =

Pn

i=1

hg (Ai , Wi )(Yi − Q(0)
n (Ai , Wi ))
,
Pn
i=1 hg (Ai , Wi )

leading to the updated estimate of Q(A, W ) = E[Y |A, W ],
(0)
Q(1)
n (A, W ) = Qn (A, W ) + n hg (A, W ).

When the treatment mechanism g was not updated, the targeted likelihood
algorithm converged in a single iteration. Note that the update did not depend in any way on the choice of variance σ 2 for the law of {Y |A, W }, so
long as it was a constant. The parameter Ψ(p0 ) was then estimated with
(1)
Ψ(p(n )), which was equal to Ψn,MLE (Q(1)
n ) and Ψn,DR-IPTW (g, Qn ). The
treatment mechanism g could also be updated with targeted likelihood, to
make the IPTW estimator equivalent with the maximum likelihood and double robust estimators. This was done by making a one-dimensional model
g (1|W ) through g(1|W ) at  = 0, whose score at  = 0 was the projection of
the IPTW estimator’s influence curve on TCAR. Such a submodel could be
formed by taking,
logit(g (1|W )) = g(1|W ) + [

Q(1, W ) Q(0, W )
+
].
g(1|W )
g(0|W )

Because this was simply a logistic model for {A|W }, we could estimate 
through logistic regression. After iterating the targeted likelihood procedure
to update both of the Q and g nuisance parameters until convergence, the
IPTW, maximum likelihood, and double robust estimators of Ψ(p0 ) became
equivalent.
DOI: 10.2202/1557-4679.1043

28

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

For this data structure, Ψn,DR-IPTW (g, Q) was asymptotically efficient,
meaning that its asymptotic performance was superior to any other regular
estimator. This efficient estimator could not be used directly on observed
data, due to its dependence on the unknown nuisance paramters g and Q. We
assessed the quality of an estimator Ψn through the ratio
R(Ψn ) =

Ep0 [n|Ψn − Ψ(p0 )|2]
Ep0 [n|Ψn,DR-IPTW (g, Q) − Ψ(p0 )|2 ]

For large enough sample size n, and consistent and asymptotically linear Ψn ,
this approximated the asymptotic relative efficiency of Ψn to the efficient estimator, and necessarily exceeded one. We approximated R(Ψn ) after forming
Ψn on 1000 simulated datasets of size n = 200.
In our simulations, we considered known censoring mechanism g, as could
occur in a randomized clinical trial. We misspecified the nuisance parameter
Q, by estimating E[Y |W ] in the A = 0 and A = 1 strata with linear regression, while quadratic regression would have been appropriate. This first-order
approximation to Q lead to an inaccurate maximum likelihood estimator, having R(Ψn ) = 2.63. Confidence intervals for R(Ψn ) were negligible, due to the
number of simulations. The misspecified nuisance parameter Q did not affect
the performance of the IPTW estimator, or the consistency of the double robust estimator, which respectively had asymptotic relative efficiencies R(Ψn )
of 1.18 and 1.15. Note that the IPTW estimator was unbiased, but was less
accurate than the double robust estimator with misspecified Q. After updating Q with a single targeted likelihood iteration, R(Ψn ) decreased to 1.10.
The resulting estimator was then a maximum likelihood estimator (and double robust estimator) with updated Q, and the update greatly increased of
the accuracy of the parameter estimate. When also updating the censoring
mechanism g, the asymptotic relative efficiency dropped even further to 1.07,
making the estimator almost equivalent with the efficient estimator. In spite
of the fact that the censoring mechanism g was already known, estimating it
from the data was nevertheless beneficial, as could be surmised from Chapter
2.3.7 of (van der Laan and Robins (2003)).
Thus, the targeted likelihood algorithm allowed us to estimate the nuisance
parameters g and Q with maximum likelihood in a manner such that three
standard estimators become identical, and led to better performance than
was achieved by the initial IPTW, maximum likelihood, and double robust
estimators.

29

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

6.5

Semiparametric regression example.

Let O = (W, A, Y ) ∼ p0 and consider the semiparametric regression model
M = {p : Ep (Y | A, W ) − Ep(Y | A = 0, W ) = m(A, W | β(p))} for some
parametrization β → m(A, W | β) satisfying m(0, W | β) = 0 for all β ∈ IRd .
This is equivalent with assuming E0 (Y | A, W ) = m(A, W | β0 ) + θ0(W )
with θ0 unspecified and m(0, W | β) = 0, and can therefore also be viewed
as a semiparametric regression model. It has been recognized that a maximum likelihood fit (e.g., generalized additive models) of the semiparametric
regression suffers from bias for the parametric part, so that one needs to undersmooth the nonparametric components in the semiparametric regression
model. However, the literature does not provide practical guidance about how
to undersmooth. Therefore, the targeted MLE approach presented here provides an importance practical improvement. Let Ψ(p) = β(p) ∈ IRd be the
parameter of interest.
This type of semiparametric regression models has been considered by
various authors (e.g., Newey (1995); Rosenbaum and Rubin (1983); Robins
et al. (1992); Robins and Rotnitzky; Yu and van der Laan (2003)). The latter three articles derive the orthogonal complement of the nuisance tangent
space (i.e., the set of all gradients of the pathwise derivative), the efficient influence curve/canonical gradient, and establish the wished double robustness
of the corresponding estimating functions. In particular, for our purpose we
refer to Theorem 2.1 and 2.2 in Yu and van der Laan (2003) for the following
statements.
The orthogonal complement of the nuisance tangent space is given by:
⊥
Tnuis
(p) = {Dh (p) : h} ⊂ L20(P ),

where Dh (p)(O) ≡ (h(A, W ) − Ep (h(A, W ) | W ))(Y − m(A, W | β(p)) −
Ep (Y | A = 0, W )). The orthogonal complement of the nuisance tangent
space corresponds with the set of gradients for Ψ at p given by:
n

o

⊥
Tnuis
(p)∗ = −c(p)(h)−1 Dh (p)(O) : h = (h1 , . . . , hd ) ,

where c(p)(h) =

d
E D (p, β)
,
dβ p h
β=β(p)

and Dh now represents a vector function

(Dh1 , . . . , Dhd ). The efficient influence curve is identified by a closed form index
h(p) (see e.g., Yu and van der Laan (2003)), which is provided below (12). Let
D(p) = Dh(p) (p) be this efficient influence curve at p as identified by this index
h(p).
Let g(p) be the conditional density of A, given W , under p, let Q(p) be
the conditional distribution of Y , given A, W , under p. We note that the
DOI: 10.2202/1557-4679.1043

30

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

parameter Ψ(p) is only a function of Q(p), and the density factorizes as p(O) =
p(W )g(p)(A | W )Q(p)(Y | A, W ). As a consequence, the elements Dh (p)
are orthogonal to the tangent spaces of the nuisance parameter g(p) and the
nuisance parameter p(W ). That is, we can decompose the efficient score D(p)
into three subcomponents as follows:
D(p) = D(p) − Ep (D(p) | A, W ) + Ep(D(p) | A, W ) − Ep (D(p) | W )
+Ep (D(p) | W ) − Ep D(p),
which corresponds with scores for p(Y | A, W ), p(A|W ) and p(W ) at p,
respectively, but Ep (D(p) | A, W ) − Ep (D(p) | W ) = 0 and Ep (D(p) |
W ) − E(D(p)) = 0. Thus the efficient influence curve D(p) represents only a
score for Q(p)(Y | A, W ), and indeed satisfies Ep (D(p)(O) | A, W ) = 0.
Consider an initial density estimator p0n = (p0nW , g(p0n ), Q(p0n )) of (W, A, Y )
with marginal distribution of W being the empirical probability distribution
of W1 , . . . , Wn . Above we showed that a submodel p0n () through p0n with score
D(p0n ) at  = 0 can be selected to only vary the conditional density Q(p0n ) of
Y , given A, W , with a score D(p0n ) at  = 0. Such a submodel will now be
presented.
Let p0n ∈ M. Suppose that Q(p0n ) is a normal distribution with mean
0
θ(pn )(A, W ) = Ep0n (Y | A, W ) and variance σ 2(A, W ) = σ 2(Q0n )(A, W ). Recall
that D(p0n ) = (h(p0n )(A, W ) − Ep0n (h(p0n ) | W ))(Y − m(A, W | β(p0)) − Ep0n (Y |
A = 0, W )). For notational convenience, we will represent this function as
h(p0n )(A, W )(Y − Ep0n (Y | A, W )) with now h(p0n ) so that Ep0n (h(p0n )(A, W ) |
W ) = 0. Consider the parametric submodel of M defined as the normal
density with conditional variance σ 2(A, W ) and conditional mean m(A, W |
βn0 ()) + θn0 (). That is,
Q0n ()(Y

!

1
Y − m(A, W | βn0 ()) − θn0 ()(W )
| A, W ) =
f0
,
σ(A, W )
σ(A, W )

where βn0 (0) = β(Q0n ), θn0 (0) = θ(Q0n ) = EQ0n (Y | A = 0, W ), and f0 is the
standard normal density. We note that this is a valid submodel through Q0n
at  = 0. Let β() ≡ β(Q0n ) +  and θn0 () = θ(Q0n ) + > r. It remains to find a
function r(W ) so that the score of Q0n () at  = 0 equals the efficient influence
curve D(p0n ).
We have that the score S() at  is given by (note that f00 (x)/f0 (x) = 2x/σ 2 )
S()σ 2(A, W )
= (Y − m(A, W |

βn0 ())

−

θn0 ()(W ))

(

d
d
m(A, W | βn0 ()) − θn0 ()(W )
d
d

)

31

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

=

(

)

d
m(A, W | βn0 ()) − r(W )) (Y − m(A, W | βn0 ()) − θn0 ()(W )).
dβn0 ()

Solving for r so that S(0) = D(p0 ) yields the equation
0
h(p0n )(A,
n W )(Y − EQ (Y | A, W )) =
o
1
d
0
m(A,
W
|
β(Q
))
−
r(W
)
(Y − EQ0n (Y | A, W )).
2
0
n
σ (A,W ) dβ(Q )
n

In order to have that the score equals Dh for a particular h(A, W ) with
Ep0n (h(A, W ) | W ) = 0, we need
r(p0n )(W ) =

Ep0n



0 m(A,W |β 0 )
d/dβn
n
σ 2 (A,W )

Ep0n



1
σ 2 (A,W )

|W

|W





.

This yields the following score for our submodel p0n () at  = 0:
S(0) = h(p0n )(A, W )(Y − m(A, W | β(Q0n)) − θ(Q0n )(W )),
where
1

h(p0n )(A, W ) ≡

σ 2(A, W )

d
m(A, W | β(Q0n))
dβ(Q0n)

Ep0n
1
− 2
σ (A, W )



d
m(A, W
dβ(Q0n)

| β(Q0n))/σ 2 (A, W ) | W



Ep0n (1/σ 2 (A, W ) | W )

.

This choice h(p0n ) gives a score S(0) equal to the efficient influence curve (see
e.g., Yu and van der Laan (2003)). So we succeeded in finding a submodel
p0n () with a score at  = 0 equal to the efficient influence curve at p0n . Thus
we are now ready to define the targeted MLE.
Consider the log-likelihood for p0n () in :
!

n
1X
Yi − m(Ai, Wi | βn0 + ) − (θn0 (W ) + > r(p0n )(W ))
l() ≡
log f0
.
n i=1
σ(A, W )

Let n be the maximizer, which can thus be computed with standard weighted
least squares regression:
n = arg min



2
1
0
0
0
Y
−
m(A
,
W
|
β
+
)
−
θ
(W
)
−
r(p
)(W
)
.
i
i
i
i
i
n
n
n
2
i=1 σ (Ai , Wi )

n
X

The score equation 0 = d/dl() = Pn S() for n is given by
n

0 = Pn

d

o

m(βn0()) − r(p0n )) (Y − m(βn0 ()) − θn0 − > r(p0n ))
dβ 0 ()
n

DOI: 10.2202/1557-4679.1043

σ2

.
32

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

In the sequel we consider the case that m(A, W | β) = β > m1(A, W ) is linear in β for some specified covariate vector m1(A, W ). In this case we have
d/dβm(A, W | β) = m1(A, W ) so that the score equation Pn S() = 0 reduces
to:
0
{m1 − r(p0n )} (Y − (βn0 + n )m1 − θn0 − >
n r(pn ))
0 = Pn
.
(12)
σ2
Firstly, we note that n exist in closed form:
{m1 − r(p0n )} (Y − βn0>m1 − θn0 )
,
σ2
where the d × d matrix An is given by
n = An−1Pn

An ≡

n
n
o
1X
1
0
m
(A
,
W
)
−
r(p
)(W
)
(m1(Ai , Wi ) + r(p0n )(Wi ))> .
1
i
i
i
n
n i=1 σ 2(Ai , Wi )

Let p0n (n ) be the new density estimator. Recall that the distribution of
(A, W ) under p0n (n ) is still the same as under p0n , because p0n () only updates
the conditional distribution of Y , given A, W . We now wish to investigate
if the first step targeted MLE p1n ≡ p0n (n ) already solves the efficient score
equation: Pn D(p1n ) = Pn D(p0n (n )) = 0. We have that Pn D(p0n (n )) is given by
Pn

{m1 − r(p0n (n ))} (Y − (βn0 + n )m1 − θn0 − n r(p0 (n )))
.
σ2

Because r(p0n ()) = r(p0n ), it follows that Pn D(p0 (n )) is given by
Pn

{m1 − r(p0n )} (Y − (βn0 + n )m1 − θn0 − n r(p0n ))
,
σ2

but the latter equals zero by the fact that Pn S(n ) = 0 (12). This proves that,
if m(A, W | β) is linear in β, then the targeted maximum likelihood estimator
is achieved in the first step of the algorithm and solves the efficient influence
curve estimating equation Pn D(p) = 0. If one would also update σ 2(A, W )
in the submodel p0n (), then the algorithm would have to be iterated in order
to converge to a targeted MLE solving Pn D(p) = 0. Similarly, for nonlinear
models m(A, W | β) the targeted MLE algorithm will also need to be iterated
till convergence.

7

Targeted MLE as loss based estimation.

In the previous sections we defined a targeted MLE in terms of an initial
density estimator and the targeted MLE algorithm applied to this initial density estimator. In order to provide a general data adaptive likelihood based
33

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

approach for construction of targeted MLE’s (also allowing for an integrated
data adaptive approach for searching over the initial densities, just as in sieve
based MLE), we now note that the targeted MLE approach corresponds with
a particular modified log-likelihood loss function. Specifically, let
L(p | P0 ) ≡ − log p∗ (p),
where p∗ (p) is defined as the limit for k → ∞ of the targeted MLE applied to
P0 and starting at p:
pk+1 = arg

max

p∈{pk ():}

P0 log p.

(13)

Note that L(p | P0 ) is a loss function for densities p of the data indexed by unknown nuisance parameters, since the k0 ≡ arg max P0 log pk () are unknown.
However, estimation of the unknown nuisance parameter corresponds simply
with applying the targeted MLE algorithm to the data starting at p. The loss
function satisfies
p0 = arg min P0 L(p | P0 ),
p∈M

because p∗ (p0 ) = p0 and p0 = arg minp∈M −P0 log p. Therefore, we can apply
the unified loss based learning approach presented in van der Laan and Dudoit
(2003) based on this new loss function L(p | P0 ) for a candidate density p.
Succinctly, this loss based learning approach works as follows. Let Ms ⊂ M
be a sieve of M indexed by fine tuning parameters s. Let
psn = Φ̂s (Pn ) ≡ arg min Pn L(p | Pn ) = arg max Pn log p∗n (p),
p∈Ms

p∈Ms

where p∗n (p) represents the limit density of the targeted MLE algorithm starting at p applied to the data Pn . Note that this maximization corresponds
with maximizing the log likelihood over solutions of Pn D(p∗ ) = 0, where the
p∗ = p∗ (p) is restricted by the constraints on the initial p. We can select s
with likelihood based cross-validation:
1
0
0
sn = Ŝ(Pn ) ≡ arg min EBn Pn,B
L(Φ̂s (Pn,B
) | Pn,B
),
n
n
n
s

resulting in the targeted ML density estimator
pn ≡ psn n = Φ̂S(P
ˆ n ) (Pn )
and targeted ML estimator of ψ0 given by ψn = Ψ(pn ).

DOI: 10.2202/1557-4679.1043

34

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

8

Discussion.

In this article we assumed a model in terms of densities with respect to a
known dominating measure, and our targeted MLE density estimators are
assumed to be dominated by this dominating measure. This allowed us to
simplify the presentation of the method. However, we also wish to stress that
the presented targeted maximum likelihood estimation methodology can easily
be generalized to targeted maximum likelihood estimation in models in terms
of probability distributions including (say) discrete as well as continuous distributions, just as this is common practice in maximum likelihood estimation
in semiparametric models. The targeted MLE algorithm takes as input an
initial density with respect to a specified dominating measure, and is based on
a hardest submodel in terms of densities with respect to this same dominating
measure. Thus, the targeted MLE algorithm can be applied to discrete distributions as well as continuous distributions, and as a consequence, the (loss
based) targeted MLE learning as presented in Section 7 applies to models that
are not necessarily dominated by a single dominating measure.
As a further generalization, the iterative principle underlying this work can
be applied to loss functions other than the negative log likelihood. Given a
loss function defined on the data and parameter space (and possibly a nuisance parameter η), we can make a one-dimensional -extension through a
space containing both the parameter Ψ and nuisance parameter η, initialize
the parameter estimate at Ψ(0), and then update the parameter estimate by
P
choosing  to minimize the empirical risk n1 ni=1 L(Oi , Ψ()|η()). The red
quirement underlying the procedure is that d
L(O, Ψ()|η())|=0 is equal to
an estimating equation for the parameter Ψ. If this condition is met, then
solving this estimating equation should correspond to convergence of the iterative empirical risk minimization algorithm. Hence, applying the algorithm
with such a loss function L(O, Ψ|η) leads to a fusion of general loss based
estimation and estimating function methodology.
Given a density estimator we defined a targeted density estimator through
an iterative maximum likelihood algorithm along hardest submodels with a
score equal to the efficient influence curve of the parameter of interest. This
tool allows us to map any candidate density p into its targeted version p∗n (p).
We now showed that by using the minus log density as loss function and
thereby use the log-likelihood criteria in combination with the cross-validated
log-likelihood criteria, but restricted to targeted density estimators only, we can
build data adaptive sieve based algorithms for generating a final targeted ML
density estimator and corresponding substitution estimator of the parameter
of interest.
35

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

By restricting the log-likelihood criteria and cross-validated log-likelihood
criteria to targeted densities only, targeted maximum likelihood estimation
provides now a purely likelihood based methodology for estimation of any
kind of parameter such as pathwise differentiable parameters and infinite dimensional parameters: see our accompanying technical report.
In particular, we showed that targeted maximum likelihood estimation
completely unifies maximum likelihood estimation and estimating function
based estimation, and results in important improvements in both. Targeted
MLE also deals naturally with the issue of multiple solutions of estimating
equations by using the log-likelihood as the criteria to be maximized. Another
nice feature of targeted MLE is that it always improves on the initial density
estimator by increasing the log-likelihood fit. As a consequence, when targeted
MLE is applied to estimate pathwise differentiable parameters of a full data
distribution FX in CAR censored data models as in (van der Laan and Robins
(2003)), if one applies the targeted MLE to an initial p0n = (gn0 , Q0n ) with
gn0 and Q0n being fits of the censoring mechanism g0 and the FX -factor Q0
of the density p0 , then it provides an estimator which is guaranteed to be
more efficient than the double robust IPCW estimator based on estimating
the nuisance parameters (g0 , Q0) with p0n . So the targeted MLE algorithm
provides a natural way to always improve on any initial double robust IPCW
locally efficient estimator as presented in van der Laan and Robins (2003).

References
P.J. Bickel, A.J. Klaassen, Y. Ritov, and J.A. Wellner. Efficient and adaptive inference in semiparametric models. Johns Hopkins university press,
Baltimore, 1993a.
P.J. Bickel, C.A.J. Klaassen, Y. Ritov, and J. Wellner. Efficient and Adaptive
Estimation for Semiparametric Models. Springer-Verlag, 1993b.
S.R. Cosslett. Efficient semiparametric estimation of censored and truncated
regressions via smooth self-consistency equation. Econometrica, 72(4):1277–
1284, 2004.
R.D. Gill, M.J. van der Laan, and J.M. Robins. Coarsening at random: characterizations, conjectures and counter-examples. In D.Y. Lin and T.R. Fleming, editors, Proceedings of the First Seattle Symposium in Biostatistics,
pages 255–94, New York, 1997. Springer Verlag.

DOI: 10.2202/1557-4679.1043

36

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

van der Laan and Rubin: Targeted Maximum Likelihood Learning

D.F. Heitjan and D.B. Rubin. Ignorability and coarse data. Annals of statistics, 19(4):2244–2253, December 1991.
M. Jacobsen and N. Keiding. Coarsening at random in general sample spaces
and random censoring in continuous time. Annals of Statistics, 23:774–86,
1995.
C.A.J. Klaassen. Consistent estimation of the influence function of locally
asymptotically linear estimators. Annals of Statistics, 15:1548–1562, 1987.
W.K. Newey. Semiparametric efficiency bounds. Journal of applied econometrics, 1(4):335–341, 1995. ISSN 1350-7265.
J. M. Robins and A. Rotnitzky. Comment on the Bickel and Kwon article,
”Inference for semiparametric models: Some questions and an answer”. Statistica Sinica, 11(4):920–936, 2001.
J. M. Robins, A. Rotnitzky, and M.J. van der Laan. Comment on ”On Profile Likelihood” by S.A. Murphy and A.W. van der Vaart. Journal of the
American Statistical Association – Theory and Methods, 450:431–435, 2000.
J.M. Robins. Robust estimation in sequentially ignorable missing data and
causal inference models. In Proceedings of the American Statistical Association, 2000.
J.M. Robins, S.D Mark, and W.K. Newey. Estimating exposure effects by modelling the expectation of exposure conditional on confounders. Biometrics,
48:479–495, 1992.
J.M Robins and A. Rotnitzky. Comment on Inference for semiparametric
models: some questions and an answer, by Bickel, P.J. and Kwon.
J.M. Robins and A. Rotnitzky. Recovery of information and adjustment for dependent censoring using surrogate markers. In AIDS Epidemiology, Methodological issues. Bikhäuser, 1992.
P.R. Rosenbaum and D.B. Rubin. The central role of the propensity score in
observational studies for causal effects. Biometrika, 70:41–55, 1983.
M.J. van der Laan. Efficient and Inefficient Estimation in Semiparametric
Models. Centre of Mathematics and Computer Science (CWI), Amsterdam,
1995.

37

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

The International Journal of Biostatistics, Vol. 2 [2006], Iss. 1, Art. 11

M.J. van der Laan. Identity for npmle in censored data models. Lifetime Data
Models, 4(0):83–102, 1998.
M.J. van der Laan. Statistical inference for variable importance. International
Journal of Biostatistics, 2(1), 2006.
M.J. van der Laan and S. Dudoit. Unified cross-validation methodology for selection among estimators and a general cross-validated adaptive epsilon-net
estimator: Finite sample oracle inequalities and examples. Technical report,
Division of Biostatistics, University of California, Berkeley, November 2003.
M.J. van der Laan and J.M. Robins. Unified methods for censored longitudinal
data and causality. Springer, New York, 2003.
M.J. van der Laan and D. Rubin. Estimating function based cross-validation
and learning. Technical report 180, Division of Biostatistics, University of
California, Berkeley, 2005.
M.J. van der Laan and D. Rubin. Estimating function based cross-validation.
In J. Fan and H.L. Koul, editors, Frontiers of Statistics, pages 87–108. Imperial College Press, 2006.
A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes.
Springer-Verlag, New York, 1996a.
A. W. van der Vaart and J. A. Wellner. Weak Convergence and Emprical
Processes. Springer-Verlag New York, 1996b.
Z. Yu and M.J. van der Laan. Measuring treatment effects using semiparametric models. Technical report, Division of Biostatistics, University of
California, Berkeley, 2003.

DOI: 10.2202/1557-4679.1043

38

Brought to you by | Harvard University
Authenticated
Download Date | 2/19/19 11:15 PM

