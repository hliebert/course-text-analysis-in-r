Chapter 75

THE ECONOMETRICS OF DATA COMBINATION*
GEERT RIDDER
Department of Economics, University of Southern California, Los Angeles, USA
e-mail: ridder@usc.edu
ROBERT MOFFITT
Department of Economics, Johns Hopkins University, Baltimore, USA
e-mail: moffitt@jhu.edu

Contents
Abstract
Keywords
1. Introduction
2. Merging samples with common units
2.1. Broken random samples
2.2. Probabilistic record linkage
2.2.1. Matching with imperfect identifiers
2.2.2. Matching errors and estimation

3. Independent samples with common variables
3.1. Fréchet bounds and conditional Fréchet bounds on the joint distribution
3.2. Statistical matching of independent samples

4. Estimation from independent samples with common variables
4.1. Types of inference
4.2. Semi- and non-parametric inference
4.2.1. Conditional independence
4.2.2. Exclusion restrictions
4.3. Parametric inference
4.3.1. Conditional independence
4.3.2. Exclusion restrictions
4.4. The origin of two-sample estimation and applications
4.5. Combining samples to correct for measurement error

5. Repeated cross sections
5.1. General principles

5470
5470
5471
5474
5474
5477
5477
5480
5484
5484
5491
5494
5494
5494
5494
5495
5498
5498
5501
5507
5510
5513
5513

* We thank J. Angrist, J. Currie, J.J. Heckman, C.F. Manski, and a referee for helpful comments on an earlier
draft.

Handbook of Econometrics, Volume 6B
Copyright © 2007 Elsevier B.V. All rights reserved
DOI: 10.1016/S1573-4412(07)06075-8

5470

G. Ridder and R. Moffitt

5.2. Consistency and related issues
5.3. Binary choice models
5.4. Applications

6. Combining biased samples and marginal information
6.1. Biased samples and marginal information
6.2. Identification in biased samples
6.3. Non-parametric and efficient estimation in biased samples
6.3.1. Efficient non-parametric estimation in biased samples
6.3.2. Efficient parametric estimation in endogenously stratified samples
6.3.3. Efficient parametric estimation with marginal information

Appendix A
References

5517
5520
5523
5525
5525
5528
5532
5532
5533
5537
5541
5543

Abstract
Economists who use survey or administrative data for inferences regarding a population
may want to combine information obtained from two or more samples drawn from the
population. This is the case if there is no single sample that contains all relevant variables. A special case occurs if longitudinal or panel data are needed but only repeated
cross-sections are available.
In this chapter we survey sample combination. If two (or more) samples from the
same population are combined, there are variables that are unique to one of the samples
and variables that are observed in each sample. What can be learned by combining such
samples, depends on the nature of the samples, the assumptions that one is prepared to
make, and the goal of the analysis. The most ambitious objective is the identification and
estimation of the joint distribution, but often we settle for the estimation of economic
models that involve these variables or a subset thereof. Sometimes the goal is to reduce
biases due to mismeasured variables.
We consider sample merger by matching on identifiers that may be imperfect in the
case that the two samples have a substantial number of common units. For the case
that the two samples are independent, we consider (conditional) bounds on the joint
distribution. Exclusion restrictions will narrow these bounds. We also consider inference
under the strong assumption of conditional independence.

Keywords
sample combination, matching, nonparametric identification, repeated cross-sections
JEL classification: C13, C14, C23, C21, C42, C81

Ch. 75:

The Econometrics of Data Combination

5471

1. Introduction
Economists who use survey or administrative data for inferences regarding a population
may want to combine information obtained from two or more samples drawn from the
population. This is the case if (i) there is no single sample that contains all relevant
variables, (ii) one of the samples has all relevant variables, but the sample size is too
small, (iii) the survey uses a stratified design. A special case of (i) occurs if longitudinal
or panel data are needed, while only repeated cross sections are available.
There are good reasons why data sets often do not have all relevant variables. If
the data are collected by interview, it is advisable to avoid long questionnaires. If the
data come from an administrative file, usually only variables that are relevant for the
eligibility for a program and for the determination of the benefits or payments associated
with that program are included. Hence, unless a survey was designed to include all
the relevant variables for a particular research project, there is no single data set that
contains all variables of interest. However, often the variables are available in two or
more separate surveys. In that case it is natural to try to combine the information in the
two surveys to answer the research question.
In this chapter we survey sample combination. What can be learned by combining
two or more samples depends on the nature of the samples and the assumptions that one
is prepared to make. If two (or more) samples from the same population are combined,
there are variables that are unique to one of the samples and variables that are observed
in each sample. To be specific, consider a population and assume that for each member
of the population we can define the variables Y, Z, X. Sample A contains the variables
Y, Z and sample B the variables X, Z. The variables in Y are unique to sample A and
those in X are unique to sample B. Hence, we have random samples from overlapping
(in variables) marginal distributions.
How one uses this information depends on the goal of the study. We distinguish between
(i) Identification and estimation of the joint distribution of X, Y, Z. This was the
original motivation for the type of sample merging that is discussed in Section 3.2. The hope was that with the merged sample the distributional impact of
taxes and social programs could be studied. An example is a study of the effect
of a change in the tax code on the distribution of tax payments. In principle,
tax returns contain all the relevant variables. However, if the change depends on
variables that did not enter the tax code before, or if it is desired to estimate the
effect for specific subgroups that are not identifiable from the tax returns, the
need arises to obtain the missing information from other sources. The joint distribution is also the object of interest in non-parametric (conditional) inference.
This is obviously the most ambitious goal.
(ii) Estimation of economic models that involve X, Y, Z (or a subset of these variables). Such models are indexed by a vector of parameters θ that is of primary
interest, and, as will become clear in Section 4.3, parametric restrictions are
helpful (but not necessary) in securing identification by sample combination.

5472

G. Ridder and R. Moffitt

An example is the estimation of the effect of age at school entry on the years of
schooling by combining data from the US censuses in 1960 and 1980 [Angrist
and Krueger (1992)].
(iii) Estimation of an economic model with mismeasured variables. In this case sample A contains Y, X, Z and sample B X ∗ , Z with X ∗ the correct value and X
the mismeasured value of the same variable, e.g. income. If X is self-reported
income, this variable may be an imperfect indicator of true income X ∗ . A better
indicator is available in administrative data, e.g. tax records. Hence, it is desirable to combine these samples to obtain a dataset that has both the correctly
measured variable and Y . Again this was a motivation for the type of sample
merger discussed in Section 3.2. In Section 4.5 we show that sample merger is
not necessary to avoid measurement error bias.
For problems of type (i) there are a number of methods that merge the samples A
and B into one sample that is treated as a random sample from the joint distribution of
X, Y, Z. Because the common variables Z 1 are often not of independent interest, we
assume for the moment that the researcher is satisfied with a random sample from the
joint distribution of X, Y . Sample merging is discussed in Sections 2 and 3. Its success depends on two factors: (i) the number of members of the population that are in
both samples, and (ii) the degree to which these common members can be identified
from the common variables Z. In the simplest case Z identifies members of the population uniquely, for instance if Z is an individual’s Social Security Number or some
other unique identifier (measured without error). If the common members are a random
sample from the population, then the merged sample is indeed a random sample from
the population distribution of X, Y . Complications arise if the number of population
members that are in both samples is substantial, but they cannot be identified without
error. We discuss estimation in samples that have been merged. Because the matching
process is not perfect the merging introduces a form of measurement or matching error.
The analogy is almost complete because the bias is similar to the attenuation bias in
models with mismeasured independent variables
The merger of samples has also been attempted in the case that the fraction of units
that are in both samples is negligible. Indeed the techniques that have been used to
merge such samples are the same as for samples with common units that cannot be
identified with absolute certainty. Only under the strong assumption of conditional independence of Y and X given Z, we can treat the merged or matched sample as a random
sample from the joint distribution of Y, Z, X (Section 4). As shown in Section 4 it is
preferable not to merge the two samples, even if the assumption of conditional independence is correct. Under conditional independence we can estimate the joint distribution
of Y, Z, X and any identified conditional model without merging the samples. If the assumption of conditional independence does not hold and our goal is to recover the joint

1 Sometimes variables have to be transformed to make them equal in both samples. For instance, A may
contain the age and B the year of birth.

Ch. 75:

The Econometrics of Data Combination

5473

distribution of Y, Z0 , X with Z0 a subvector of Z, then the two samples give bounds on
this joint distribution. Point identification is possible if we specify a parametric model
for the conditional distribution of Y given X, Z0 , f (y | x, z0 ; θ ) or moments of that
distribution, e.g. the conditional mean. In both cases, it is essential that some of the
common variables in Z are not in Z0 , i.e. that there are exclusion restrictions. In Section 4.5 we also consider the case that one or more of the variables of a survey is subject
to measurement error, while there is a second survey that has error free data on these
variables, but does not contain data on the other relevant variables in the first survey.
We show that the merger of the two samples is again not the solution, but that such data
are helpful in reducing or even eliminating the errors-in-variables bias.
A special case of sample combination with some distinct variables are synthetic cohorts obtained from repeated cross sections. In that case Y and X are the same variables
in two time periods and Z is the variable that identifies the cohort. This special case
deserves separate consideration and is discussed in Section 5.
In Section 6 we consider the combination of samples with common variables that are
drawn from possibly overlapping subpopulations of some target population. We distinguish between (i) all samples have the same set of variables, but they are drawn from
distinct subpopulations, (ii) there is one sample that has all variables of interest and at
least one other sample that is drawn from the same population, but contains a subset of
the variables of interest. Case (i) occurs if the sample design is stratified. Often, a simple
random sample from a population is not the most efficient sample design. If subpopulations are identifiable from the sampling frame, a design that oversamples heterogeneous
subpopulations and undersamples homogeneous ones, requires fewer observations to
achieve the same precision. Such a sample design is called a stratified design (with unequal probabilities of selection). It may even be that in a simple random sample certain
subpopulations that are of particular interest will not be represented at all. For instance,
if the dependent variable is the indicator of a rare event, there may be insufficient observations to study factors that affect the occurrence of the event. With a stratified sample
design the samples from the strata must be combined. The procedure depends on the
type of inference, in particular on whether the inference is on the conditional distribution of a (vector of) dependent variable(s) given a set of conditioning variables or not.
If the strata are subsets of the support of the conditioning variables or of variables that
are independent of the dependent variables given the conditioning variables, then the
stratified sample can be treated as a random sample. If the inference is unconditional or
if the strata are subsets of the support of the dependent variables, then we cannot treat
the stratified sample as if it were a random sample. The correct procedure is to use some
weighting scheme that uses the inverse probability of selection as the sampling weights
[Horvitz and Thompson (1952)].
In case (ii) a small sample with all relevant variables is typically combined with a
larger sample with fewer variables. The goal is to increase the precision of the estimates
obtained from the small sample. The main difference between the sample combination
considered in Sections 2–4 and in Section 6 is that in Sections 2–4 the issue is whether
the distribution, moments or parameters of interest are identified from the combined

5474

G. Ridder and R. Moffitt

samples. In Section 6 identification is usually ensured (see Section 6.2 for a discussion
of the conditions) and the focus is on efficient inference.
A stratified sample is a special case of a sample in which the probability that a population unit is included in the sample depends on the variables of interest. In a stratified
sample this probability is a known function of the variables that define the strata. The
more general case in which this probability is not known and has to be estimated occurs for instance, if responses are missing for a fraction of the population. A special
case is the estimation of treatment effects where the counterfactual outcome is missing. If the probability of observation depends on the independent variables, but not on
the dependent variable, the data are Missing At Random (MAR). If we have a random
sample from the marginal population distribution of the independent variables, either
because we observe the independent variables even if the dependent variable is missing,
or because we have an independent random sample from this distribution, then we can
estimate parameters of the distribution of the dependent variable. Hirano, Imbens and
Ridder (2003) have shown that the efficient estimator uses estimated sampling weights,
even if these weights are known. This seems to be a generic result, because the efficient
estimator in stratified samples also requires estimated weights.
This chapter provides a common framework for research in different fields of economics and statistics. It is mostly a survey, but we also point at some areas, for instance
non-parametric identification of joint distributions by exclusion restrictions, that have
not been explored yet. Although we survey empirical applications we have not attempted to include all studies that use some form of data combination. By bringing
together research that until now was rather disjoint we hope to stimulate further research on data combination.

2. Merging samples with common units
An obvious way to combine information in two samples is to merge the samples. If the
two samples have a substantial number of common units, the natural action is to link
the records relating to the same unit. The linkage of records for the same unit is usually
called exact matching. This term is misleading, because it suggests that the linkage is
without errors. Record linkage is easy if both records contain a unique identifier, e.g. an
individual’s social security number, that is observed without error. Card, Hildreth and
Shore-Sheppard (2001) match survey to administrative data, and find that even in the
administrative data the social security numbers are often misreported. If the two surveys
are independently drawn samples from two overlapping populations, the linked records
are a sample from the intersection of the two populations.
2.1. Broken random samples
DeGroot, Feder and Goel (1971), DeGroot and Goel (1976) and DeGroot and Goel
(1980) consider the reconstruction of a broken random sample, i.e. a random sample in

Ch. 75:

The Econometrics of Data Combination

5475

which the identity of the members is observed with error. Besides its intrinsic interest,
we discuss their method because of its similarity to methods used to merge samples that
have no common units.
Consider a random sample of size N from a population and assume that the identity of the units in the random sample is observed with error, i.e. a record consist of
(Yi , Z1i , Z2j , Xj ) with
Zki = Zi + εki ,

k = 1, 2.

(1)

The identifier Z is observed with error and unit i is erroneously linked to unit j . We
ignore for the moment Y, X.2 We also assume that Z, ε1 , ε2 are jointly normally distributed,3 and as a consequence the observed Z1 , Z2 have a bivariate normal distribution
with means μ1 , μ2 , standard deviations σ1 , σ2 , and correlation coefficient ρ. Let φ denote a permutation of 1, . . . , N so that Z1i is linked with Z2φ(i) . The loglikelihood of
the sample Z1i , Z2φ(i) , i = 1, . . . , N, is


ln L μ1 , μ2 , σ12 , σ22 , ρ, φ
 N

N
N
= C − log 1 − ρ 2 − log σ12 − log σ22
2
2
2
1
−
2(1 − ρ)2

N 

(z2φ(i) − μ2 )2
(z1i − μ1 )(z2φ(i) − μ2 )
(z1i − μ1 )2
×
+
−
2ρ
. (2)
σ1 σ2
σ12
σ22
i=1

Note that the vector φ is treated as a vector of parameters, i.e. the likelihood is the joint
distribution if φ is the correct linkage. Maximizing the loglikelihood with respect to the
means and variances yields the usual MLE for these parameters. If we substitute these
MLE and maximize with respect to ρ we obtain the concentrated loglikelihood that only
depends on φ


N
log 1 − ρφ2
(3)
2
with ρφ the sample correlation coefficient between Z1i , Z2φ(i) , i = 1, . . . , N . This
sample correlation coefficient depends on the permutation φ. It is easily verified for
N = 2 and it can be shown for all N [Hájek and Šidak (1967)] that the average of the
sample correlation coefficient over all permutations is equal to 0. Hence the smallest
value for ρφ is ρmin < 0 and the largest ρmax > 0. If the order statistics of Z1 , Z2 are
denoted by Z1(i) , Z2(i) , then it is intuitively clear that the sample correlation coefficient
is maximal if Z1(i) is linked with Z2(i) , and minimal if Z1(i) is linked with Z2(N−i+1) .
The first permutation is denoted by φmax , the second by φmin . Because the concentrated
L(φ) = C −

2 If Y, X are correlated (given Z , Z ) they could be helpful in reconstructing the correctly linked sample.
1 2
3 This assumption can be relaxed, see DeGroot, Feder and Goel (1971).

5476

G. Ridder and R. Moffitt

2
2
loglikelihood increases with ρφ2 , the MLE of ρ is ρmax if ρmax
> ρmin
and ρmin if
the reverse inequality holds. In the first case the likelihood is maximized if we link
according to the order statistics, and in the second case if we link in the reverse order.
As is obvious from the loglikelihood in (2) the nature of the linkage, i.e. the choice of φ,
depends only on the sign of ρ. The MLE for ρ suggests the following rule to decide on
2 then we estimate the sign of ρ as +1, while we use the opposite
2
this sign: if ρmax
> ρmin
sign if the reverse inequality holds. DeGroot and Goel (1980) conduct some sampling
experiments that show that for values of ρ of 0.9, i.e. a relatively small measurement
error in the identifier, this procedure yields the correct sign in more than 75% of the
replications (for sample sizes ranging from 5 to 500).
Obviously, if the Z1 , Z2 are observations on a common identifier, we do not have to
estimate the sign of ρ, because the correlation is positive, unless we make extreme assumptions on the correlation between the two measurement errors. The optimal linkage
is then on the order statistic of Z1 and Z2 . Maximization of the loglikelihood (2) with
respect to the permutation φ is equivalent to maximization of
N


(4)

z1i z2φ(i)

i=1

and this is in turn equivalent to minimization of
N

i=1

2
z1i
+

N


2
z2i
−2

i=1

N


z1i z2φ(i) =

i=1

N


(z1i − z2φ(i) )2 .

(5)

i=1

Hence the Euclidean or L2 distance between the vectors of observed identifiers is minimized. As we shall see, this rule that is derived for the case of exact matching with
mismeasured identifiers, is also used in the case that there are no common units in the
samples.
If there are multiple identifiers, i.e. if Z is a K vector and Z1 , Z2 have multivariate
normal distributions with means μ1 , μ2 , variance matrices Σ11 , Σ22 , and covariance
matrix Σ12 , the factor of the likelihood function that depends on the permutation φ is


N
1   12
z1i Σ z2φ(i) .
ln L(μ, Σ12 ) = exp −
(6)
2
i=1

In this expression


−1
−1
−1
Σ12 Σ22 − Σ21 Σ11
Σ12 .
Σ 12 = −Σ11

(7)

This likelihood factor is the probability that the permutation φ is the correct match
and hence maximization of the likelihood function is equivalent to maximization of the
probability of a correct match.
The maximization of the likelihood factor in (6) is equivalent to the maximization of
N

i=1

z1i C12 z2φ(i)

(8)

Ch. 75:

The Econometrics of Data Combination

5477

with C12 = −Σ 12 . This is equivalent to the minimization of
N

(z1i − z2φ(i) ) C12 (z1i − z2φ(i) ),

(9)

i=1

i.e. the quadratic distance with matrix C12 between the vectors of identifiers. The same
distance measure is sometimes used if the samples have no common units and Z is a
vector of common characteristics (see Section 3.2).
Because all units must be matched the maximization of (8) is equivalent to the minimization of
N
N 


dij z1i C12 z2j

(10)

i=1 j =1

subject to for i = 1, . . . , N, j = 1, . . . , N,
N

i=1

dij =

N


dij = 1

(11)

j =1

and dij = 0, 1. This is a linear assignment problem, an integer programming problem
for which efficient algorithms are available.
This procedure requires an estimate of Σ12 , the covariance matrix of Z1 and Z2 . Note
that in the case of a single identifier only the sign of this covariance was needed. If the
errors in the identifiers are independent in the two samples, an estimate of the variance
matrix of the true identifier vector Z suffices. The extension of DeGroot and Goel’s
MLE to the multivariate case has not been studied.
2.2. Probabilistic record linkage
2.2.1. Matching with imperfect identifiers
The ML solution to the reconstruction of complete records assumes that the mismeasured identifiers are ordered variables. The method of probabilistic record linkage can
be used if the matching is based on (mismeasured) nominal identifiers, such as names,
addresses or social security numbers. Probabilistic record linkage has many applications. It is used by statistical agencies to study the coverage of a census, by firms that
have a client list that is updated regularly, and by epidemiologists who study the effect
of a potentially harmful exposure [see Newcombe (1988), for a comprehensive survey
of the applications]. In epidemiological studies a sample of individuals who have been
exposed to an intervention is linked with a population register to determine the effects
on fertility and/or mortality, the latter possibly distinguished by cause [Newcombe et
al. (1959), Buehler et al. (2000), Fair et al. (2000)]. Probabilistic record linkage is also
used in queries from a large file, e.g. finding matching fingerprints or DNA samples.
The implementation of probabilistic record linkage depends on the specific features of

5478

G. Ridder and R. Moffitt

the data. In this survey we only describe some general ideas. We use the setup of Fellegi
and Sunter (1969), although we change it to stress the similarity with the reconstruction
of broken random samples (Section 2.1) and statistical matching (Section 3.2).
Initially we assume that there is a single identifier Z that identifies each member of
the population uniquely. We have two samples of sizes N1 and N2 from the population.
These samples need not be of equal size and, although it is assumed that a substantial
fraction of the units in both samples are common, the remaining units are unique to one
of the samples. This is a second departure from the assumptions made in the case of a
broken random sample. A key ingredient of probabilistic matching is the record generating model that describes how the observed identifiers in the records are related to the
unique true identifier. It is obvious that errors in names and reported social security numbers cannot be described by a simple model with additive measurement error [Fellegi
and Sunter (1969), Copas and Hilton (1990), and Newcombe, Fair and LaLonde (1992),
develop alternative record generating models]. To keep the exposition simple, we will
stick with the additive model of Equation (1). The main ideas can be explained with this
model and are independent of a specific model of the record generating process.
The first step is to define a comparison vector Wij for each pair i, j , with i with
identifier Z1i in the first and j with identifier Z2j in the second random sample. An
obvious choice is Wij = Z2j − Z1i , but we can also include Z1 and use the comparison
vector Wij = (Z2j − Z1i , Z1i ) . Define Mij as the indicator of the event that i and j
are matched, i.e. are the same unit. If we assume that the measurement errors in the
two samples are independent of each other and of the true identifier Z, and that the
identifiers of distinct units are independently distributed in the two samples, we have,
for Wij = Z2j − Z1i , with f the density of ε2 − ε1 and Gk the cdf of Z in sample k,
h(wij | Mij = 1) = f (wij ),
h(wij | Mij = 0) =

f (wij − z + z) dG1 (z) dG2 (z ).

(12)

For every pair i, j we consider the density ratio, provided that the denominator is
greater than 0 (if the denominator is 0, the match can be made without error),
h(wij | Mij = 1)
.
h(wij | Mij = 0)

(13)

This ratio gives the relative likelihood that the comparison vector is from a matched
pair. Just as in a statistical test of the null hypothesis that i, j refer to the same unit, we
decide that the pair is matched if the density ratio exceeds a threshold. Note that with
this matching rule unit i may be matched with more than one unit in sample 2 and unit
j may be matched with more than one unit in sample 1.
To illustrate the procedure we consider a simple case. The distribution of the identifier
is usually discrete. Here we assume that there is a superpopulation of identifiers from
which the identifiers in the (finite) population are drawn. In particular, we assume that
the Z’s in both samples are independent draws from a normal distribution with mean
μ and variance σ 2 . A uniform distribution may be a more appropriate choice in many

Ch. 75:

The Econometrics of Data Combination

5479

instances. The measurement errors are also assumed to be normally distributed with
mean 0 and variances σ12 , σ22 .
Under these assumptions, the density ratio is
φ(z2j − z1i ; σ12 + σ22 )
φ(z2j − z1i ; 2σ 2 + σ12 + σ22 )


2σ 2 + σ12 + σ22
σ2
2
exp
−
−
z
)
.
(z
=
2j
1i
σ12 + σ22
(2σ 2 + σ12 + σ22 )(σ12 + σ22 )

(14)

The cutoff value for the density ratio can also be expressed as
(z2j − z1i )2 < C

(15)

and we match if this inequality holds. C is a constant that is chosen to control either the
probability of a false or a missed match. If we take the first option we choose C such
that
√
C
2Φ
(16)
− 1 = α.
2σ 2 + σ12 + σ22
The advantage of this choice is that the cutoff value can be computed with the (estimated) variances of the observed identifiers Z1i and Z2j which are σ 2 +σ12 and σ 2 +σ22 ,
respectively. Estimation of the variances of the measurement errors is not necessary. If
there are multiple identifiers, the criterion for matching i and j is


(z2j − z1i ) (Σ1 + Σ2 )−1 − (2Σ + Σ1 + Σ2 )−1 (z2j − z1i ) < C,
(17)
i.e. the quadratic distance with the specified matrix between the observed identifiers
is less than a threshold. To use this criterion we need estimates of Σ and Σ1 + Σ2 .
If Σ  Σ1 + Σ2 the criterion can be approximated by a quadratic form with matrix
(Σ1 + Σ2 )−1 , and the distance is chi-squared distributed for matches. In that case it is
more convenient to choose C to control the probability of a missed match.
In general, the estimation of the parameters that enter the density ratio is the most
problematic part of probabilistic linkage. Tepping (1968), Copas and Hilton (1990) and
Belin and Rubin (1995) propose estimation methods that use a training sample in which
it is known which pairs are matched to estimate the parameters of the distribution of the
comparison vector among matched and unmatched pairs.
It is interesting to compare probabilistic record linkage to the method that was proposed for the reconstruction of a broken random sample. Instead of minimizing the
(average) distance between the identifiers as in (5), we choose a cutoff value for the
distance and match those pairs with a distance less than the cutoff value. In probabilistic record linkage a record may be linked with two or more other records. If the true
identifiers are sufficiently distinct and/or if the measurement errors are relatively small
the probability of this event is negligible. Alternatively, we can choose the record that
has the largest match probability.

5480

G. Ridder and R. Moffitt

2.2.2. Matching errors and estimation
The term exact matching is a misnomer when dealing with samples that have been
matched using identifiers that are subject to error. Matching error biases estimates of
parameters. In this section we consider the case that a random sample from a population
is matched (with error) to a register that contains each unit in the sample. There has
been very little work on biases due to matching errors. Usually, matched samples are
analyzed as if there are no mismatches. This section provides a framework that can be
used to assess potential biases and to obtain unbiased estimates if some knowledge of
the matching process is available.
We assume that a random sample of size N1 is matched with a register of size N2
that is a random sample from the target population or the complete target population
(N2 > N1 ). For example, we have a sample of taxpayers that is matched with the register of tax returns. The sample contains a variable X and an identifier Z1 that is measured
with error and the register contains a variable Y and an identifier Z2 that is also measured with error. The true identifier is denoted by Z. We want to study the relation
between X and Y or in general statistics defined for the joint distribution of X, Y . In
fact, we show that the joint distribution of X, Y is (non-parametrically) identified, if the
matching probabilities are available.
The data are generated as follows. First, a sample of size N2 is drawn from the joint
distribution of X, Y, Z. This sample is the register. Next, we generate the mismeasured
identifiers Z1 , Z2 , e.g. according to (1) or some other record generating model discussed
in the previous section. We observe Yj , Z2j , j = 1, . . . , N2 . The next step is to draw
N1 < N2 observations from the register without replacement. This is the sample, for
which we observe Xi , Z1i , i = 1, . . . , N1 . Note that in this case all members in the
sample are represented in the register.
The bias induced by the matching errors depends on the relation between the mismeasured identifier and the variables of interest. For instance, if the identifier is a
(misreported) social security number, then it is reasonable to assume that both the identifier Z and the observed values Z1 , Z2 are independent of the variables of interest. If,
in addition, there is a subsample with correctly reported identifiers Z1 = Z2 = Z, e.g.
the subsample with Z1 = Z2 (this is an assumption), then this subsample is a random
sample from the joint distribution of the variables of interest. However, often common
variables beside the identifier are used to match units i and j with z1i = z2j , e.g. we
match i and j if z1i and z2j are close and i and j have the same gender, age, and
location etc. Note that the additional common variables need not be observed with error in the two samples. However, the probability that the match is correct depends on
these additional common variables that in general are correlated with variables of interest. In this case, even if we can identify a subsample in which all matches are correct,
this subsample is not a random sample from the joint distribution of the variables of
interest.
Here we only consider the case that Z, Z1 , Z2 are independent of X, Y . The general
case can be analyzed in much the same way. Note that this is the simplest case for

Ch. 75:

The Econometrics of Data Combination

5481

probabilistic record linkage. There is an interesting contrast with statistical matching,
as discussed in the next section, because there the quality of the approximation relies
heavily on the correlation between the identifiers and the variables of interest.
The quality of the matches depends on the matching method that in turn depends
on the record generating model. We use the same example that was considered in
Section 2.2.1. The record generating model is as in (1) and Z, ε1 and ε2 are all independently normally distributed. Under these assumptions i in the sample is matched
with φ(i) in the register if and only if |z2φ(i) − z1i | < C with C determined e.g. as
in (16) or by some other rule. We can derive an expression for the probability that the
match is correct given that we use this matching rule, i.e. the probability of the event
that Zi = Zφ(i) given that |Z2φ(i) − Z1i |  C. Substitution of (1) and using the independence of the reporting errors and the true value gives by Bayes’ theorem
Pr(Miφ(i) = 1)



= Pr Zi = Zφ(i)  |Z2φ(i) − Z1i |  C
=
=

Pr(Zi = Zφ(i) ) Pr(|ε2φ(i) − ε1i | < C)
Pr(Zi = Zφ(i) ) Pr(|ε2φ(i) − ε1i | < C) + Pr(Zi = Zφ(i) ) Pr(|Zφ(i) + ε2φ(i) − Zi − ε1i | < C)
 C 
1
N2 Φ
σ12 +σ22
1
N2 Φ



C
σ12 +σ22



+

N2 −1 
N2 Φ

C

.

(18)

σ12 +σ22 +2σ 2

This expression for the probability of a correct match under the given matching rule
has a Bayesian flavor. The probability of a correct match, if a unit in the sample is
matched at random with a unit in the register is N12 . This is also the limit of the probability of a correct match if C → ∞. The probability decreases in C. If C ↓ 0 we obtain
the limit
1
N2
1
N2

+

N2 −1
N2



σ12 +σ22
σ12 +σ22 +2σ 2

(19)

and this probability approaches 1 if the reporting error in the identifier is small. Hence,
we improve on random matching by using the noisy identifiers. Of course, if we choose
C too small, there will be few matches. As will be seen below, the variance of estimators
is inversely proportional to the probability of a correct match, so that if our goal is
to estimate parameters accurately we face a trade-off between the number of matched
observations and the probability that the match is correct. Although this analysis is for
a specific record generating model, the trade-off is present in all matched samples.
If we match i in the sample to φ(i) in the register, if |Z2φ(i) − Z1i |  C, then the
conditional probability of a correct match given the identifiers Z1 , Z2 is



Pr Miφ(i) = 1  Z1i , Z2φ(i)



= Pr Zi = Zφ(i)  |Z2φ(i) − Z1i |  C, Z1i , Z2φ(i)

5482

G. Ridder and R. Moffitt

=

Pr(Miφ(i)

Pr(Miφ(i) = 1)φ1 (Z2φ(i) − Z1i )
= 1)φ1 (Z2φ(i) − Z1i ) + Pr(Miφ(i) = 0)φ2 (Z2φ(i) − Z1i )

(20)

with

φ1 (Z2φ(i) − Z1i ) = φ Z2φ(i) − Z1i

φ2 (Z2φ(i) − Z1i ) = φ Z2φ(i) − Z1i



 |Z2φ(i) − Z1i |  C; σ 2 + σ 2 ,
1
2


 |Z2φ(i) − Z1i |  C; 2σ 2 + σ 2 + σ 2 .
1
2

Now we are in a position to discuss estimation. Consider a pair i, φ(i) matched according to a matching rule, e.g. the rule above, from the N1 × N2 possible pairs. The joint
distribution of Xi , Z1i , Yφ(i) , Z2φ(i) has density g(xi , z1i , yφ(i) , z2φ(i) ) with
g(xi , z1i , yφ(i) , z2φ(i) )
= g(xi , z1i , yφ(i) , z2φ(i) , Miφ(i) = 1) + g(xi , z1i , yφ(i) , z2φ(i) , Miφ(i) = 0).
(21)
If the joint density of X, Y is f (x, y), then because we assume that X, Y and Z, Z1 , Z2
are independent,
g(xi , z1i , yφ(i) , z2φ(i) , Miφ(i) = 1)
= f (xi , yφ(i) ) Pr(Miφ(i) = 1 | z1i , z2φ(i) )g(z1i , z2φ(i) )

(22)

and
g(xi , z1i , yφ(i) , z2φ(i) , Miφ(i) = 0)
= f1 (xi )f2 (yφ(i) ) Pr(Miφ(i) = 0 | z1i , z2φ(i) )g(z1i , z2φ(i) ).

(23)

Substituting (22) and (23) in (21), and using g(xi , z1i , yφ(i) , z2φ(i) ) = f (xi , yφ(i) ) ×
g(z1i , z2φ(i) ), we can solve for f (xi , yφ(i) )
g(xi , yφ(i) ) − Pr(Miφ(i) = 0 | z1i , z2φ(i) )f1 (xi )f2 (yφ(i) )
Pr(Miφ(i) = 1 | z1i , z2φ(i) )
g(xi , yφ(i) ) − f1 (xi )f2 (yφ(i) )
= f1 (xi )f2 (yφ(i) ) +
Pr(Miφ(i) = 1 | z1i , z2φ(i) )

f (xi , yφ(i) ) =

(24)

if the denominator is greater than 0, which is the case for any sensible matching rule.
The distributions on the right-hand side of this expression are all observed. Hence
this identification result is non-parametric, although it requires that the matching probabilities are known or that they can be estimated.
Often we are not interested in the joint distribution of Y, X, but in a population parameter θ0 that is the unique solution to a vector of population moment conditions


E m(Xi , Yi ; θ ) = 0.
(25)
These population moment conditions refer to the correctly matched observations. If two
observations are incorrectly matched, they are stochastically independent. In general

Ch. 75:

The Econometrics of Data Combination

for i = j


E m(Xi , Yj ; θ ) = 0

5483

(26)

is solved by θ1 = θ0 . In other words, the parameter cannot be identified from the two
marginal distributions.
The solution for the joint population distribution in (24) suggests the sample moment
conditions that combine information from the sample and the register
N1
m(xi , yφ(i) ; θ )
1 
N1
Pr(Miφ(i) = 1 | z1i , z2φ(i) )
i=1

−

N1
N1 
1 − Pr(Mj φ(k) = 1 | z1j , z2φ(k) )
1 
m(xj , yφ(k) ; θ )
N12 j =1 k=1 Pr(Mj φ(k) = 1 | z1j , z2φ(k) )

(27)

and the weighted GMM estimator of θ either makes (27) equal to 0 or is the minimizer
of a quadratic form in these sample moment conditions. In this expression (but not in
(24)) it is implicitly assumed that the probability that a unit in the sample is matched
with two or more units in the register is negligible. This simplifies the notation.
We obtain a particularly simple result if we use the identifiers to match the sample to
the register, but ignore them in the inference, i.e. in (21) we start with the joint distribution of Xi , Yφ(i) , so that
f (xi , yφ(i) ) = f1 (xi )f2 (yφ(i) ) +

g(xi , yφ(i) ) − f1 (xi )f2 (yφ(i) )
.
Pr(Miφ(i) = 1)

This will give consistent, but less efficient, estimates. Let the probability of a correct
match Pr(Miφ(i) = 1) = λ. If X and Y have mean 0, then
cov(Xi , Yφ(i) )
(28)
.
λ
With the same assumption we find for the moment conditions of a simple linear regression with an intercept


E (Yi − α − βXi )Xi
cov(Xi , Yi ) =

E[(Yφ(i) − α − βXi )Xi ] − (1 − λ)[E(Yφ(i) )E(Xi ) − αE(Xi ) − βE(Xi2 )]
,
λ
(29)
E[Yi − α − βXi ]
E[Yφ(i) − α − βXi ] − (1 − λ)[E(Yφ(i) ) − α − βE(Xi )]
=
λ
= E[Yφ(i) − α − βXi ].
(30)
=

Setting these conditions equal to 0 and solving for the parameters we find that
β=

cov(Xi , Yφ(i) )
,
λ var(Xi )

5484

G. Ridder and R. Moffitt

α = E(Yφ(i) ) − βE(Xi )

(31)

and, if we substitute the sample statistics for the population statistics, we obtain the estimator suggested by Neter, Maynes and Ramanathan (1965) and Scheuren and Winkler
(1993). The results in this section generalize their results to arbitrary moment conditions
and less restrictive assumptions on the sampling process. In particular, we show that the
matching probabilities that are computed for probabilistic linkage can be used to compute the moment conditions for the matched population. This is important because the
simulation results in Scheuren and Winkler (1993) show that the bias induced by false
matches can be large.
The asymptotic variance of the estimator for β is
var(β̂) =

σ2
.
N1 λ2 var(X)

(32)

The variance decreases with the matching probability. The GMM estimator is consistent
if the matching probability is positive.

3. Independent samples with common variables
3.1. Fréchet bounds and conditional Fréchet bounds on the joint distribution
Exact or probabilistic matching is not advisable if the fraction of units that are in both
samples is small. If the fraction is negligible, we may treat the two random samples
as independent samples that have no units in common. Although exact or probabilistic
matching produces more informative data, the fear that linked files pose a threat to
the privacy of individuals who, with some effort, may be identifiable from the linked
records, has prevented the large scale matching of administrative and survey data.4 As
a consequence, often the only available samples that contain all relevant variables are
relatively small random samples from a large population. It is safe to assume that these
random samples have no common units.
The two independent random samples identify the marginal distributions of X, Z
(sample A) and Y, Z (sample B). If there are no common variables Z, the marginal distributions put some restrictions on the joint distribution of X, Y . These Fréchet (1951)
bounds on the joint distribution are not very informative. For example, if the marginal
and joint distributions are all normal, there is no restriction on the correlation coefficient
of X and Y , i.e. it can take any value between −1 and 1.

4 Fellegi (1999) notes that public concern with file linkage varies over place and time and that, ironically,
the concern is larger if the linkage is performed by government agencies than if private firms are involved.
Modern data acquisition methods like barcode scanners and the internet result in large files that are suitable
for linkage.

Ch. 75:

The Econometrics of Data Combination

5485

With common variables Z the Fréchet bounds can be improved. The bounds for the
joint conditional cdf of X, Y given Z = z are




max F (x | z) + F (y | z) − 1, 0  F (x, y | z)  min F (x | z), F (y | z) . (33)
Taking the expectation over the distribution of the common variables Z we obtain



E max F (x | Z) + F (y | Z) − 1, 0



 F (x, y)  E min F (x | Z), F (y | Z) .
(34)
The bounds are sharp, because the lower and upper bounds, E[max{F (x | Z) + F (y |
Z) − 1, 0}] and E[min{F (x | Z), F (y | Z)}] are joint cdf’s of X, Y with marginal cdf’s
equal to F (x) and F (y). Note that because the expectation of the maximum is greater
than the maximum of the expectations (the reverse relation holds for the expectation
of the minimum), the Fréchet bounds with common variables are narrower than those
without. If either X or Y are fully determined by Z, then the joint cdf is identified. To
see this let the conditional distribution of X given Z = z be degenerate in x(z). Define
A(x) = {z | x(z)  x}. Then F (x | z) = 1 if z ∈ A(x) and F (x | z) = 0 if z ∈ A(x)c .
Substitution in (34) gives that the lower and upper bounds coincide and that


 

F (x, y) = E F (y | Z)  Z ∈ A(x) Pr Z ∈ A(x) .
(35)
In the special case that the population distribution of X, Y, Z is trivariate normal, the
only parameter that cannot be identified is the correlation between X and Y . We have
2
ρXY = ρXY |Z 1 − ρXZ
1 − ρY2 Z + ρXZ ρY Z .

(36)

This gives the bounds
ρXZ ρY Z −

2
1 − ρY2 Z
1 − ρXZ

 ρXY  ρXZ ρY Z +

2
1 − ρXZ
1 − ρY2 Z .

(37)

The lower bound reaches its minimum −1 if ρXZ = −ρY Z (the upper bound is
2 ) and the upper bound reaches its maximum 1 if ρ
1 − 2ρXZ
XZ = ρY Z (the lower
2 . Also if either ρ
or
ρ
is
equal
to
1,
then
ρXY = ρXZ ρY Z . The
bound is −1 + 2ρXZ
XZ
YZ
2
length of the interval is 2 1 − ρXZ
1 − ρY2 Z and hence the bound is more informative
if the correlation between either Z and X or Z and Y is high.
An example illustrates how much correlation between X, Y and Z is required to narrow bounds. Consider a linear regression model

Y = α + βX + U

(38)

where X and U are independent and normally distributed. If σX , σY denote the standard
deviation of X and Y , respectively, we have
|β|
σY
=√
σX
R2

(39)

5486

G. Ridder and R. Moffitt

with R 2 the coefficient of determination of the regression. If we multiply the bounds in
(37) by σσXY we obtain an interval for the slope β. If p denotes the relative (with respect to
β) length of the interval and we consider the case that the correlation between X and Z
and Y and Z are equal, we obtain the following expression for the required correlation
ρXZ =

√
p R2
1−
.
2

(40)

The correlation decreases with R 2 and the (relative) length of the interval for β. For
instance, if we want a 0.20 relative length for a regression with an R 2 of 0.9, we need
that ρXZ = ρY Z = 0.95. In general, the correlation that is needed to obtain informative bounds is rather high, and this illustrates the limited information about the relation
between X and Y in the combined sample.
The Fréchet bounds on the joint cdf in (34) treat the variables X and Y symmetrically.
As the notation suggests, often Y is the dependent and X the independent variable in
a relation between these variables, and we focus on the conditional distribution of Y
given X. An important reason to do this, is that we may assume that this conditional
distribution is invariant under a change in the marginal distribution of X. For example,
Cross and Manski (2002) consider the case that Y is the fraction of votes for a party
and X is the indicator of an ethnic group. It is assumed that the ethnic groups vote in
the same way in elections, but that the ethnic composition of the voters changes over
time. If we have the marginal distributions of Y (election results by precinct) and X
(ethnic composition by precinct), what can we say about future election results, if we
have a prediction of the future composition of the population, i.e. the future marginal
distribution of X?
Horowitz and Manski (1995) and Cross and Manski (2002) have derived bounds for
the case that X is a discrete variable with distribution
Pr(X = xk ) = pk ,

k = 1, . . . , K.

(41)

We first derive their bounds for the case that there are no common variables Z. They
consider bounds on the conditional expectation
 


E g h(Y ), X  X = x
with g bounded and monotone in h for almost all x. A special case is g(h(Y ), X) =
I (Y  y) which gives the conditional cdf. Because the conditional expectation above
is continuous and increasing in F (y | x), in the sense that the expectation with respect
to F1 (y | x) is not smaller than that with respect to F2 (Y | x), if F1 (y | x) first-order
stochastically dominates F2 (y | x), we can derive bounds on this expectation from
bounds on the conditional cdf.
In the sequel we derive bounds both on the conditional cdf F (y | x) and on
F (y; xk ) = Pr(Y  y, X = xk ). We first derive bounds on these cdf’s for a given k.

Ch. 75:

The Econometrics of Data Combination

5487

Next we consider the K-vector of these cdf’s. Note that by the law of total probability
K


F (y; xk ) = F (y)

k=1

which imposes an additional restriction on the vector F (y; xk ), k = 1, . . . , K.
The Fréchet bounds on F (y; xk ) are




max F (y) − (1 − pk ), 0  F (y; xk )  min F (y), pk .

(42)

For each k these bounds are sharp, because both the lower and upper bound are increasing in y, and they both increase from 0 to pk , i.e. they are F̃ (y; xk ) for some random
variables Ỹ and X̃.
The bounds in (42) imply that if pk  12
y < F −1 (pk ),

0  F (y; xk )  F (y),
0  F (y; xk )  pk ,

F −1 (pk )  y < F −1 (1 − pk ),

F (y) − (1 − pk )  F (y; xk )  pk ,
with an obvious change if pk >
conditional cdf of Y given X = xk
0  F (y | xk ) 

F (y)
,
pk

1
2.

y  F −1 (1 − pk ),

(43)

Upon division by pk we obtain bounds on the

y < F −1 (pk ),

0  F (y | xk )  1, F −1 (pk )  y < F −1 (1 − pk ),
F (y) − (1 − pk )
 F (y | xk )  1, y  F −1 (1 − pk ).
pk

(44)

The bounds have an appealing form. The lower bound is the left truncated cdf of Y
where the truncation point is the (1−pk )th quantile of the distribution of Y and the upper
bound is the right truncated cdf with truncation point equal to the pk th quantile. These
bounds on the conditional cdf of Y were derived by Horowitz and Manski (1995) and
Cross and Manski (2002). They are essentially Fréchet bounds on the joint distribution.
Next, we consider bounds on the vector F (y; .) = (F (y; x1 ) . . . F (y; xK )) . For
K = 2 the bounds in (42) are (without loss of generality we assume p1 < 12 , i.e.
p2 = 1 − p1 > p1 )
0  F (y; x1 )  F (y),
0  F (y; x1 )  p1 ,

y < F −1 (p1 ),
F −1 (p1 )  y < F −1 (p2 ),

F (y) − p2  F (y; x1 )  p1 ,
0  F (y; x2 )  F (y),

y  F −1 (p2 ),

y < F −1 (p1 ),

F (y) − p1  F (y; x2 )  F (y),
F (y) − p1  F (y; x2 )  p2 ,

F −1 (p1 )  y < F −1 (p2 ),
y  F −1 (p2 ).

(45)

5488

G. Ridder and R. Moffitt

Figure 1. Bounds on (F (y; x1 ), F (y; x2 )) for three values of y.

By the law of total probability F (y; .) satisfies for all y
K


F (y; xk ) = F (y).

(46)

k=1

Hence, the vector of conditional cdf’s is in a set that is the intersection of the Fréchet
bounds in (45) and the hyperplane in (46). The resulting bounds on (F (y; x1 ), F (y; x2 ))
are given in Figure 1 for three values of y with y1 < F −1 (p1 ), F −1 (p1 )  y2 <
F −1 (p2 ), and y3  F −1 (p2 ). The Fréchet bounds on (F (y; x1 ), F (y; x2 )) are the
squares. The law of total probability selects two vertices of these squares as the extreme points of the set of (F (y; x1 ), F (y; x2 )) that satisfy both the Fréchet bounds and
the law of total probability. Bounds on the conditional cdf’s F (y | x1 ) and F (y | x2 )
are obtained upon division by p1 and p2 , respectively. This amounts to a change in the
units in Figure 1 and except for that the figure is unchanged.
From (45) the lower bound on F (y; x1 ) is

0,
y < F −1 (p2 ),
FL (y; x1 ) =
F (y) − p2 , y  F −1 (p2 ),

Ch. 75:

The Econometrics of Data Combination

5489

and the upper bound is

F (y), y < F −1 (p1 ),
FU (y; x1 ) =
p1 ,
y  F −1 (p1 ).
Note that both the lower and upper bound increase from 0 to p1 with y, and hence
are equal to F̃ (y; x1 ) for some random variables Ỹ and X̃. The corresponding upper
and lower bounds on F (y; x2 ) are FU (y; x2 ) = F (y) − FL (y; x1 ) and FL (y; x2 ) =
F (y) − FU (y; x1 ), and these bounds are equal to F̃ (y; x2 ) for some random variables
Ỹ and X̃. This establishes that the bounds are sharp. A general proof of this statement
can be found in Cross and Manski.
The bounds on the conditional cdf’s F (y | x1 ) and F (y | x2 ) are also given in
Figure 2. By the law of total probability, the lower bound of F (y | x1 ) corresponds with
upper bound of F (y | x2 ) and the other way around. Note that the bounds are narrower
for F (y | x2 ) because x2 has a higher probability than x1 . From this figure we can
obtain bounds on the conditional median of Y given X. We find that the change in this
conditional median has bounds
1
1 1
− p1 − F −1 1 − p1  med(Y | x2 ) − med(Y | x1 )
F −1
2 2
2
1 1
1
+ p1 − F −1 p1 .
 F −1
(47)
2 2
2
Note that the lower bound is negative and the upper bound positive for all p1 , so that
it is impossible to sign the change of the conditional median with this information.
This suggests that the relation between Y and X cannot be inferred from two marginal
distributions without common variables.
If K  3 the bounds can be derived in the same way. First, we order the pk by
increasing size. Next, we find the hypercubes that correspond to the Fréchet bounds
on F (y; .). As in Figure 1 the vertices depend on the value of y, i.e. for which k we
have F −1 (pk )  y < F −1 (pk+1 ). Finally, we select the vertices that satisfy the law
of total probability. These are the extreme points of the set of admissible F (y; xk ),
k = 1, . . . , K. To be precise, the set is the convex hull of these extreme points. As we
shall see below, for prediction purposes it is sufficient to find the vertices.
The main reason for bounds on the conditional cdf of Y given X, instead of on the
joint cdf of Y, X, is that it is usually assumed that the conditional cdf is invariant with
respect to changes in the distribution of X. Of course, this is a common assumption
in conditional econometric models with fixed parameters. An obvious application is to
conditional prediction. Cross and Manski consider the prediction of the outcome of a
future election assuming that the voting behavior of demographic groups remains the
same, but that the composition of the population changes and the future composition of
the population can be predicted accurately.
The predicted distribution of the future outcome F̃ (y) satisfies
F̃ (y) = F (y; x1 )

p̃1
p̃2
+ F (y; x2 )
p1
p2

(48)

5490

G. Ridder and R. Moffitt

Figure 2. Bounds on F (y | x1 ) and F (y | x2 ).

Ch. 75:

The Econometrics of Data Combination

5491

with p̃1 the future fraction with X = x1 . Again, without loss of generality we assume
p1 < 12 . We can further distinguish between p̃1  p1 and p̃1 > p1 . In the former case
the bounds on the predicted cdf can be found as in Figure 1. In that figure we indicate
the bounds for F −1 (p1 )  y < F −1 (p2 ). The bounds are obtained by intersecting the
set of feasible (F (y; x1 ), F (y; x2 )) with (48). We find


p̃2
p̃1
F (y)  F̃ (y)  min
F (y), 1 , y < F −1 (p1 ),
p1
p2



p̃2
p̃2 
1 − F (y)  F̃ (y)  min
F (y), 1 , F −1 (p1 )  y < F −1 (p2 ),
1−
p2
p2


p̃2 
p̃1 
1−
(49)
1 − F (y)  F̃ (y)  1 −
1 − F (y) , y  F −1 (p2 ).
p2
p1
As is obvious from Figure 1, the bounds increase with the difference between p1 and p̃1 .
For K  3 the bounds on the predicted cdf are found by evaluating
K

p̃k
k=1

pk

F (y; xk )

(50)

at the extreme points of the set of feasible F (y; .).
As noted, a key assumption in the derivation of the bounds is that X is a discrete
variable. From (44) it is obvious that the bounds on the conditional cdf become uninformative if pk goes to 0, i.e. the bounds become 0  F (y | xk )  1 for all y. Hence, if X
is close to continuous the bounds on the conditional cdf’s are not useful. If the support
of Y is bounded, e.g. if it is a dichotomous variable, then the bounds on the support can
be used to obtain bounds on conditional expectations. Such bounds are of a different
nature and beyond the scope of this chapter.
3.2. Statistical matching of independent samples
The Fréchet bounds exhaust the information on the joint distribution of X, Y . If we
merge the samples A and B no information is added, and our knowledge of the joint
distribution of X and Y does not increase. How much we can learn about the joint distribution of X, Y is completely determined by the relation between X and Z in sample
A and that between Y and Z in sample B.
In spite of this, the temptation to match two samples that do not have common
units as if they were two samples with a substantial degree of overlap has been irresistible. A number of authors have proposed methods for this type of file matching
[Okner (1972), Ruggles and Ruggles (1974), Radner (1974), Ruggles, Ruggles and
Wolff (1977), Barr and Turner (1978), Kadane (1978); see also the survey in Radner
et al. (1980)]. These methods are direct applications of those that are used in the reconstruction of broken random samples and probabilistic matching. Let the sample A be
xi , z1i , i = 1, . . . , N1 , and the sample B be yi , z2i , i = 1, . . . , N2 . The vectors z1 and
z2 contain the same variables and the subscript only indicates whether the observation is

5492

G. Ridder and R. Moffitt

in sample A or B. Because the samples A and B do not contain common units, the fact
that z1i and z2j are close does not imply that they refer to the same unit or even similar
(except for these variables) units. If we match unit i in A to unit j in B we must decide
which of the vectors z1i or z2j we include in the matched file. If we use the observation
for file A, then this file is referred as the base file, and file B is called the supplemental
file.
The two methods that have been used in the literature are constrained and unconstrained matching. Both methods require the specification of a distance function
D(z1i , z2j ). In (9) (for broken random sample) and (17) (for probabilistic record linkage) we specify the distance function as a quadratic function of the difference, but other
choices are possible.5 In practice, one must also decide on which variables to include
in the comparison, i.e. in the z vector. The Fréchet bounds suggest that the joint distribution of X, Y is best approximated, if the correlation between either X or Y and Z
or the R 2 in a regression of either X or Y on Z is maximal. Often, the units that can
be matched are restricted to, e.g. units that have the same gender. In that case gender is
called a cohort variable.
With constrained matching every unit in sample A is matched to exactly one unit in
sample B. Often A and B do not have an equal number of units. However, both are
random samples from a population and hence the sampling fraction for both samples is
known (assume for the moment that the sample is obtained by simple random sampling).
The inverse of the sampling fraction is the sample weight, wA for sample A and wB
for sample B. Assume that the weights are integers. Then we can replicate the units
in sample A wA times and those in sample B wB times to obtain two new samples
that have the same number of units M (equal to the population size). Now we match
the units in these samples as if they were a broken random sample, i.e. we minimize
over dij , i = 1, . . . , M, j = 1, . . . , M, with dij = 1 if i and j are matched
M
M 


dij D(z1i , z2j )

(51)

i=1 j =1

subject to
M


dik = 1,

k=1
M


dkj = 1,

(52)

k=1

for all i = 1, . . . , M, j = 1, . . . , M. If we choose distance function (9) we obtain the
same solution as in a broken random sample. Of course, there is little justification for
this matching method if the samples A and B have no common units.
5 Rodgers (1984) finds no systematic differences in the performance of distance functions, although he
comments that the Mahalanobis distance using an estimated variance matrix does not perform well.

Ch. 75:

The Econometrics of Data Combination

5493

The method of constrained matching was first proposed by Barr and Turner (1978).
An advantage of this method is that the marginal distributions of X and Y in the merged
file are the same as those in the samples A and B. A disadvantage is that the optimization
problem in (51) is computationally burdensome.
In unconstrained matching the base file A and the supplemental file B are treated
asymmetrically. To every unit i in file A we match the unit j in file B, possibly restricted
to some subset defined by cohort variables, that minimizes D(z1i , z2j ). It is possible that
some unit in B is matched to more than one unit in A, and that some units in B are not
matched to any unit in A. As a consequence, the distribution of Z2 , Y in the matched file
may differ from that in the original sample B. Note that if we use the distance function
(17), unconstrained matching is formally identical to probabilistic record linkage. Of
course, there is no justification for this method, if the samples A and B have no common
units. The first application of unconstrained matching was by Okner (1972) who used
the 1967 Survey of Economic Opportunity as the base file and the 1966 Tax File as the
supplemental file to create a merged file that contained detailed data on the components
of household income.
The merger of two files using either unconstrained or constrained matching has been
criticized since its first use. In his comment on Okner’s (1972) method, Sims (1972)
noted that an implicit assumption on the conditional dependence of X, Y given Z is
made, usually the assumption that X, Y are independent conditional on Z. A second
problem is best explained if we consider matching as an imputation method for missing
data. File A contains X, Z1 and Y is missing. If we assume conditional independence,
an imputed value of Y is a draw from the conditional distribution of Y given Z1 = z1 .
Such a draw can be obtained from file B, if for one of the units in file B Z2 = z1 . If
such a unit is not present in file B, we choose a unit with a value of Z2 close to z1 . This
is an imperfect imputation, and we can expect that the relation between Z1 and Y in the
merged file is biased. Indeed, Rodgers (1984) reports that the covariance between Z1
and Y is underestimated, as one would expect. An alternative would be to estimate the
relation between Y and Z2 in sample B, e.g. by a linear regression, and use the predicted
value for Z1 = z1 , or preferably a draw from the estimated conditional distribution of
Y given Z1 = z1 , i.e. include the regression disturbance variability in the imputation.6
The imputation becomes completely dependent on model assumptions, if the support of
Z1 is larger than that of Z2 . In general the distribution of X, Y, Z can only be recovered
on the intersection of the supports of Z1 and Z2 . If both samples are random samples
from the same population, as we assume here, then the supports coincide.
It is possible to evaluate the quality of the data produced by a statistical match, by
matching two independent subsamples from a larger dataset. The joint distribution in
the matched sample can be compared to the joint distribution in the original dataset.
Evaluation studies have been performed by, among others, Ruggles, Ruggles and Wolff
(1977), and Rodgers and DeVol (1982). It comes as no surprise that the conclusion from

6 Even better: also include the variability due to parameter uncertainty.

5494

G. Ridder and R. Moffitt

these evaluations is that the joint distribution of X, Y cannot be estimated from the joint
marginal distributions of X, Z and Y, Z.
As noted, matching can be considered as an imputation method for missing data.
Rubin (1986) has suggested that instead of merging the files A and B, it is preferable to
concatenate them, and to impute the missing Y in file A and missing X in file B using
the estimated relations between X and Z1 (file A) and Y and Z2 (file B). In particular,
he suggests not to use a single draw from the (estimated) conditional distribution of X
given Z1 = z2 and of Y given Z2 = z1 , effectively assuming conditional independence,
but to add draws from the distributions of X given Z1 = z2 , Y = y and Y given Z2 = z1 ,
X = x assuming a range of values for the conditional correlation. The resulting datasets
reflects the uncertainty on the conditional correlation and the variability of parameter
estimates over the imputations indicates the sensitivity of these estimates to assumptions
on the conditional correlation. Further developments along these lines can be found in
Raessler (2002).

4. Estimation from independent samples with common variables
4.1. Types of inference
Without further assumptions the (conditional) Fréchet bounds on the joint cdf is all that
can be learned from the two samples. These bounds are usually not sufficiently narrow,
unless the common variables are highly correlated with Y and X. In this section we
explore what additional assumptions are needed to improve the inference.
We consider (i) conditional independence, and (ii) exclusion restrictions. Exclusion
restrictions refer to the situation that the distribution of Y given X, Z is independent of
a subvector Z0c of Z, and hence depends only on the other variables Z0 in Z. We also
consider both non-parametric inference, i.e. the goal is to estimate the joint distribution of Y, X, Z0 or the conditional distribution of Y given X, Z0 or moments of these
distributions, and parametric inference, i.e. the joint distribution of Y, X, Z0 or the conditional distribution of Y given X, Z0 is in a parametric class. Parametric assumptions
play an important role in inference from independent samples, a theme that is repeated
in Section 5 on inference in repeated cross sections.
None of the methods discussed below requires that the two samples are merged. All
computations can be done on the two samples separately.
4.2. Semi- and non-parametric inference
4.2.1. Conditional independence
If Y, X are stochastically independent given the common variables Z, then the joint
density of X, Y is


f (x, y) = E f (x | Z)f (y | Z) .
(53)

Ch. 75:

The Econometrics of Data Combination

5495

Although the joint distribution is identified, often we just want to compute an expectation E(g(X, Y )). We have

 


 

 
E g(X, Y ) = EY Z E g(X, Y )  Y, Z = EY Z E g(X, Y )  Z ,
(54)
where the last equality holds by conditional independence. Note that the inner conditional expectation is with respect to the distribution of X given Z that is identified from
sample A, and that the outer expectation is with respect to the joint distribution of Y, Z
that is identified from sample B. We implicitly assume that the distributions of Z1 and
Z2 in the samples A and B are identical. This is true if both samples are from the same
population.
For a fixed value of Y , we can estimate the inner conditional expectation by a nonparametric regression (e.g. kernel or series) estimator of g(X, y) on Z using sample A.
The estimator of E(g(X, Y )) is then obtained by averaging this regression estimator
over Y, Z in sample B. The analysis of this estimator is beyond the scope of this chapter.
It is similar to the semi-parametric imputation estimator proposed by Imbens, Newey
and Ridder (2004) and Chen, Hong and Tarozzi (2004) who establish semi-parametric
efficiency for their estimator. Their results can be directly applied to this estimator. In
the literature it has been suggested that for the estimation of E(g(X, Y )) we must first
estimate the joint distribution of X, Y [see Sims (1972) and Rubin (1986)], but this is
not necessary. Note that a similar method can be used to estimate E(g(X, Y, Z0 )) with
Z0 a subvector of Z.
4.2.2. Exclusion restrictions
If we are not prepared to assume that X, Y are conditionally independent given Z,
we can only hope for bounds on the expected value E(g(X, Y, Z0 )). Such bounds are
given by Horowitz and Manski (1995) and Cross and Manski (2002) and can be derived in the same way as the bounds in Section 3.1. In particular, they derive bounds
on E[g(h(Y, Z0 ), X, Z0 ) | X = x, Z0 = z0 ] with g bounded and monotone in h for
(almost all) x, z0 .
We consider two possibilities: (i) the conditional distribution of Y given X, Z depends
on all variables in Z, (ii) this conditional distribution only depends on a subvector Z0
of Z and is independent of the other variables Z0c in Z. Note that the possibilities are
expressed in terms of the conditional distribution of Y given X (and Z or Z0 ). This
suggests that Y is considered as the dependent variable and that X, Z are explanatory
variables.
If assumption (i) applies, the bounds derived above are bounds on F (y; . | Z = z)
or F (y | ., Z = z). If we are interested in F (y; .) or F (y | .), we have to average
over the marginal distribution of Z or the conditional distribution of Z given X = xk
(F (y | X = xk , Z) has to be averaged over this distribution). As noted in Section 3.1
this averaging results in narrower bounds, but as noted in that section the correlation
between Y and Z and X and Z must be high to obtain informative bounds.

5496

G. Ridder and R. Moffitt

Assumption (ii) that states that the vector of common variables Z0c can be omitted
from the relation between Y and X, Z is more promising. As stated, assumption (ii)
focuses on conditional (in)dependence of Y and Z0c given X, Z0 . Alternatively, the assumption can be expressed as conditional mean (in)dependence or conditional quantile
(in)dependence. In that case, we identify or obtain bounds on the conditional mean or
quantile. We only discuss conditional (in)dependence. The derivation of bounds on the
conditional mean from bounds on the conditional cdf is complicated by the fact that the
conditional mean is not a continuous function of the conditional cdf. However, if the
assumptions are expressed as restrictions on the conditional mean, this does not matter.

Assumption (ii) is an exclusion restriction. If we decompose Z = (Z0 Z0c ) , then
c
Z0 is excluded from the conditional distribution of Y given X, Z. Exclusion restrictions are powerful and often are sufficient to identify F (y | x, z0 ). We maintain the
assumption that X is discrete. This simplifies the analysis substantially. This is not surprising, because non-parametric identification under exclusion restrictions is an inverse
problem, and it is well known that inverse problems are much harder for continuous
distributions [see, e.g. Newey and Powell (2003)]. First, we consider conditions under
which F (y | x, z0 ) is non-parametrically identified. Next, we consider the underidentified case, and we show that we can find bounds that improve on the bounds that hold
without an exclusion restriction.
Without loss of generality we omit Z0 . The common variable Z is excluded from the
conditional cdf of Y given X, Z. We denote
Pr(X = xk | Z = z) = pk (z).

(55)

With the exclusion restriction we have that for all z,
F (y | z) =

K


F (y | xk )pk (z).

(56)

k=1

If Z is also discrete, (56) is a linear system of equations with unknowns F (y | xk ), i.e.
K unknowns. Hence, this system has a unique solution if Z takes at least L  K values
and the L × K matrix, with (l, k)th component pk (zl ) has rank equal to K. In that case
F (y | .) is exactly identified. If the rank of this matrix is strictly greater than K (this
requires that L > K), then the equation has no solution. Hence, if L > K a test of the
rank of the matrix, and in particular a test whether the rank is equal to K is a test of
the overidentifying restrictions, or in other words, a test of the exclusion restriction. If
the exclusion restriction is rejected, we can allow the conditional cdf of Y given X, Z
to depend on Z. For instance, if X takes two values and Z contains two variables, of
which the first takes two values and the second four, then we obtain an exactly identified
model by allowing the conditional cdf to depend on the first variable in Z.
If X and Z take two values, i.e. K = L = 2, the solution to (56) is
F (y | x1 ) =

p2 (z1 )F (y | z2 ) − p2 (z2 )F (y | z1 )
,
p1 (z2 ) − p1 (z1 )

Ch. 75:

The Econometrics of Data Combination

F (y | x2 ) =

p1 (z2 )F (y | z1 ) − p1 (z1 )F (y | z2 )
.
p1 (z2 ) − p1 (z1 )

5497

(57)

Note that this implies that
F (y | x2 ) − F (y | x1 ) =

F (y | z2 ) − F (y | z1 )
.
p1 (z2 ) − p1 (z1 )

(58)

If conditional cdf’s are replaced by conditional expectations, this is the Wald estimator
[Wald (1940)], which is the Instrumental Variable (IV) estimator for a dichotomous
endogenous variable with a dichotomous instrument.
Solving (56) for the case that X is continuous is much harder. In effect, we have to
find the components of a mixture in the case that the mixing distribution is known. The
problem is that the solution is not continuous in F (y | .) unless restrictions are imposed
on these conditional distributions. For instance, if Z is independent of Y, X (exclusion
restriction) and the joint distribution of Y, X is normal, then the covariance of Y, X can
be recovered from

−1 
E(X | Z = z) − μX .
E(Y | Z = z) = μY + ΣY X ΣXX

(59)

with μ the mean and Σ the covariance matrix of the joint normal distribution. Further
details on weaker restrictions can be found in Newey and Powell (2003).
The similarity of the non-parametric two-sample estimator and the corresponding IV
estimator with endogenous X and Z as instrumental variable, can lead (and as will be
noted in Section 4.4 has led) to much confusion. In particular, it does not mean that we
should consider X as an endogenous variable.
If L < K the conditional cdf F (y | .) is not identified. In that case we can use the
results in Horowitz and Manski (1995) and Cross and Manski (2002) to obtain bounds
(see the discussion in Section 3.1). The exclusion restriction imposes additional restrictions on the conditional cdf. Figure 3 illustrates these bounds for the case K = 3,
L = 2. In this figure the two triangles give the sets of F (y | x1 ), F (y | x2 ), F (y | x3 )
that are consistent with sample information if Z = z1 or Z = z2 . Because Z takes
both values and is excluded from the conditional distribution of Y given X = x,
F (y | x1 ), F (y | x2 ), F (y | x3 ) has to be in the intersection of these triangles.
Note that the extreme points are the Wald estimators of F (y | x1 ), F (y | x3 ) and
F (y | x2 ), F (y | x3 ) for the case that F (y | x2 ) and F (y | x1 ) are set to 0, respectively.
In general the extreme points are Wald estimators for conditional cdf’s that are obtained
by imposing identifying restrictions. Figure 3 is drawn for pk (zl )  12 , k = 1, 2, 3,
l = 1, 2, and y < min{F −1 (pk (zl )), k = 1, 2, 3, l = 1, 2}. The other bounds can be
obtained in the same way. Note that the exclusion restriction gives a narrower bound. To
see this, compare the bound on F (y | x1 ) in the figure to those for Z = z1 or Z = z2 ,
1)
2)
which are 0 (lower bound) and Fp(y|z
and Fp(y|z
(upper bound), respectively.
1 (z1 )
1 (z2 )

5498

G. Ridder and R. Moffitt

Figure 3. Bounds on F (y | x1 ), F (y | x2 ), F (y | x3 ) in underidentified case; pk (zl )  12 , k = 1, 2, 3,
l = 1, 2, and y < min{F −1 (pk (zl )), k = 1, 2, 3, l = 1, 2}.

4.3. Parametric inference
4.3.1. Conditional independence
Often two samples are merged to estimate a parametric relation between a dependent
variable Y , present in one sample, and a vector of independent variables X some of
which may be only present in an independent sample. We assume that sample A contains
X, Z, sample B contains Y, Z and that we estimate a relation between Y and X, Z0 with
Z0 a subvector of Z. This relation has a vector of parameters θ and we assume that
the population parameter vector θ0 is the unique solution to the population moment
conditions


E m(Y, X, Z0 ; θ ) = 0.
(60)
This framework covers Maximum Likelihood (ML) and Generalized Method of Moments (GMM). Initially, we assume that X and Y are conditionally independent given Z.

Ch. 75:

The Econometrics of Data Combination

Under conditional independence we have


 


E m(Y, X, Z0 ; θ ) = EY Z EX m(Y, X, Z0 ; θ )  Y, Z
 
 
= EY Z EX m(Y, X, Z0 ; θ )  Z .

5499

(61)

If we have an estimate of the conditional distribution of X given Z, identified in sample
A, we can estimate E(m(y, X, z0 ; θ ) | Z = z) for fixed values Y = y and Z = z using
the data from sample A. The sample moment conditions corresponding to (61) are
N2



1 


E
X|Z m(Yj , X, Z02j ; θ ) Z2j = 0,
N2

(62)

j =1

where the hat indicates that the conditional expectation is estimated using the data from
sample A.
As an example consider the regression model
Y = β1 X + β2 Z0 + ε.

(63)

The scalar dependent variable Y and a vector of common variables Z1 are observed in
sample A. The (scalar) independent variable X and a vector of the common variables
Z2 are observed in sample B (the subscript on Z indicates the sample). We assume that
Z1 and Z2 are independently and identically distributed. The scalar variable Z0 is a
component of Z. The parameters β1 , β2 are identified by
E(ε | X, Z) = 0.

(64)

In general this assumption is too strong, because it generates more moment conditions
than are needed to identify the regression parameters. These parameters are identified,
even if (scalar) X is correlated with ε, provided that Z has two variables that are not
correlated with ε. In general, Z is chosen to ensure that the variables in the relation that
are in sample A and those that are in sample B are conditionally independent given Z,
and Z may contain many variables. It is not even necessary to assume that all the variables in Z are exogenous, as suggested by (64). If X is exogenous, only Z0 (or one other
variable in Z) has to be exogenous.
We first consider the case that both X and Z0 are exogenous. The population moment
conditions are


E (Y − β1 X − β2 Z0 )X = 0,


E (Y − β1 X − β2 Z0 )Z0 = 0.
(65)
Under conditional independence these can be written as

  

EY Z2 Y EX|Z1 (X | Z2 ) − β1 EX|Z1 X 2  Z2 − β2 Z02 EX|Z1 (X | Z2 ) = 0,



EY Z2 Y − β1 EX|Z1 (X | Z2 ) − β2 Z02 Z02 = 0.

(66)
(67)

In these expressions EX|Z1 (X | Z2 ) is the conditional expectation of X given Z1 that
can be estimated from sample A and that is a function of Z1 , with Z2 substituted for Z1 .

5500

G. Ridder and R. Moffitt

In other words, it is the imputed X in sample B based on Z2 observed in sample B and
using the conditional expectation of X given Z1 in sample A.
If we substitute the sample moments for EY Z2 [Y EX|Z1 (X | Z2 )], EY Z2 [EX|Z1 (X |
Z2 )], EY Z2 [EX|Z1 (X 2 | Z2 )], and EY Z2 [Z02 EX|Z1 (X | Z2 )], we obtain the sample
moment conditions that can be solved to obtain the estimator of the regression coefficients. From GMM theory [Hansen (1982)] it follows that this estimator is consistent
and asymptotically normal. If the number of moment conditions exceeds the number
of parameters, we obtain an efficient estimator by minimizing a quadratic form in the
sample moment conditions with the inverse of the variance matrix of these conditions
as weighting matrix.
It is interesting to note that the GMM estimator obtained from (66)–(67) is not the
imputation estimator obtained by replacing the unobserved X in sample B by its imputed
value. The imputation estimator is not even available, if X and Z0 are both exogenous
and Z = Z0 .
If Z contains at least one additional exogenous variable, Z0c , we can choose to use the
moment condition corresponding to Z0c , instead of the moment condition corresponding
to X, even if X is exogenous. In that case we can replace the moment conditions (65)
by


E (Y − β1 X − β2 Z0 )Z0c = 0,


E (Y − β1 X − β2 Z0 )Z0 = 0.

(68)

Because the Z’s are in both samples, all expected values in these population moment conditions can be obtained from sample A (E(XZ0 ), E(XZ0c )), sample B
(E(Y Z0 ), E(Y Z0c )) or both (E(Z02 ), E(Z0 Z0c )). Hence, in this case we need not make
the assumption of conditional independence of X and Y given Z. Note that this is true,
irrespective of whether X is endogenous or not. Key are the availability of additional
common variables that can replace X in the moment conditions and the additive separability of variables that are in different samples in the residual Y − β1 X − β2 Z0 . We
shall explore this below.
In the example the distribution of X given Z was not needed to obtain the GMM
estimator, because the moment conditions were quadratic in X and only E(X | Z) and
E(X 2 | Z) had to be estimated. In general, this will not be the case, and an assumption on this conditional distribution is needed. Econometricians are usually reluctant to
specify the distribution of exogenous variables, and for that reason we may consider a
semi-parametric alternative in which EX|Z1 (m(y, X, z0 ; θ ) | Z = z) is estimated by
a non-parametric regression (series or kernel estimator) of m(y, Xi , z0 ; θ ) on Z1i in
sample A. This gives ÊX|Z (m(y, X, z0 ; θ )) which is substituted to obtain the sample
moment conditions as an average in sample B. This estimator is similar to the estimator
considered in Chen, Hong and Tarozzi (2004) and Imbens, Newey and Ridder (2004),
and their results can be used to analyze this estimator.

Ch. 75:

The Econometrics of Data Combination

5501

4.3.2. Exclusion restrictions
In Section 4.2.2 we discussed conditions under which exclusion restrictions are sufficient for the non-parametric identification of the conditional distribution of Y given
X, Z0 . In this section we consider parametric inference. The assumptions we impose
are convenient, but stronger than needed. In particular, we restrict the discussion to additively separable moment conditions. The existing literature only considers this case.
If the exclusion restrictions identify the joint distribution as explained in Section 4.2.2,
the separability assumption can be relaxed. This has not been studied, and developing
procedures for this case is beyond the scope of this chapter.
The setup and notation is as in Section 4.2.2 with Z0c the components of Z that are
not in the relation and satisfy (69), i.e. that are exogenous for the relation between Y
and X, Z0 . We consider moment conditions that can be written as


 
E f (Y ; θ ) − g(X, Z0 ; θ ) h Z0 , Z0c = 0
(69)
with f, g, h known functions and θ a vector of parameters. If Y is scalar, then so is g.
The dimension of h is not smaller than that of θ . In general, this implies that the dimension of Z0c has to exceed that of X,7 i.e. the number of common exogenous variables
that are excluded from the relation cannot be smaller than the number of variables in X.
If we assume that some variables in either X or Z0 are endogenous we need as many
additional variables in Z0c as there are endogenous variables among X, Z0 .
The estimator based on the population moment conditions (69) is called the TwoSample Instrumental Variable (2SIV) estimator. In the case that all variables are observed in a single sample, the estimator based on the moment conditions in (69) is
related to Amemiya’s nonlinear simultaneous equations estimator [see, e.g. Amemiya
(1985, Chapter 8)].
We discuss three examples of models that give moment conditions as in (69): the
linear regression model, the probability model for discrete dependent variables, and the
mixed proportional hazard model for duration data. In all models we take h(Z0 , Z0c ) =

(Z0 Z0c ) . For the linear regression model the moment conditions are


E Y − β0 − β1 X − β2 Z0 = 0,
(70)
 



E Y − β0 − β1 X − β2 Z0 Z0 = 0,
(71)
 c



E Y − β0 − β1 X − β2 Z0 Z0 = 0.
(72)
Note that we can replace X by E(X | Z0 , Z0c ).8 We can even replace X by the linear
approximation to this conditional expectation, i.e. by π0 + π1 Z0 + π2 Z0c where the vector π minimizes E[(X − π0 − π1 Z0 − π2 Z0c )2 ]. This gives the estimating equations of
7 If Z is exogenous, then functions, e.g. powers, of Z are also exogenous. To avoid identification by
0
0
functional form, we need the additional exogenous variables in Z0c .
8 This is a consequence of the equivalence of 2SLS and IV in this type of models.

5502

G. Ridder and R. Moffitt

the two-stage linear imputation estimator first suggested by Klevmarken (1982). In the
first stage, the vector of independent variables X is regressed on the common exogenous
variables Z0 , Z0c using data from sample A. This estimated relation is used to compute
the predicted value of X in sample B, using the common variables as observed in sample B. These predicted values are substituted in the estimating equations that now only
contain variables observed in sample B.
The second example is the probability model for discrete dependent variables. If we
consider a dummy dependent variable then we specify


Pr(Y = 1 | X, Z0 ) = G β0 + β1 X + β2 Z0
(73)
with G a cdf of some continuous distribution, e.g. the standard normal (Probit) or logistic cdf (Logit). The moment conditions are



E Y − G β0 + β1 X + β2 Z0 = 0,
(74)
 




E Y − G β0 + β1 X + β2 Z0 Z0 = 0,
(75)
 c




E Y − G β0 + β1 X + β2 Z0 Z0 = 0.
(76)
Except for the logit model, these moment conditions do not give the efficient estimator
of β. To obtain the efficient estimator we must multiply the residual by
g(β0 + β1 X + β2 Z0 )
.
G(β0 + β1 X + β2 Z0 )(1 − G(β0 + β1 X + β2 Z0 ))

(77)

The resulting moment equation cannot be computed from the separate samples.
Ichimura and Martinez-Sanchis (2005) discuss this case and also derive bounds on
the parameters if there is no point identification.
The last example is the Mixed Proportional Hazard (MPH) model for duration data.
In that model the hazard rate h of the duration Y is specified as


h(y | x, V ; θ ) = λ(y; θ1 ) exp θ2 X + θ3 Z0 V
(78)
with λ the baseline hazard and V a random variable that is independent of Z0 , Z1 and
that captures the effect of omitted variables. By (78) we have that
ln Λ(Y ; θ1 ) + θ2 X + θ3 Z0 = U

(79)

with U independent of Z0 , Z1 and Λ the integral of λ. This gives the moment conditions

 
E ln Λ(Y ; θ1 ) + θ2 X + θ3 Z0 Z0 = 0,
(80)
 c



E ln Λ(Y ; θ1 ) + θ2 X + θ3 Z0 Z0 = 0.
(81)
The number of variables in Z0c must at least be equal to the number of parameters in
(θ1 θ2 ) .9 Alternatively, we can identify θ1 by making assumptions on the functional
9 If we assume the baseline hazard is Weibull we can identify the regression parameters up to scale. These
parameters can be identified, if we choose a functional form for the baseline hazard that is not closed under a
power transformation.

Ch. 75:

The Econometrics of Data Combination

5503

form of the regression function. For instance, if we maintain the hypothesis that the
regression function is linear, we can use powers of the variables in Z0c in the moment
conditions. In that case no additional common variables are needed.10 Besides the MPH
model, we can estimate other transformation models from two independent samples. Examples are the Box–Cox transform [Box and Cox (1964)] and the transform suggested
by Burbidge, Magee and Robb (1988).11
These three examples correspond to linear regression, nonlinear regression and transformation models. Other models, as the Tobit model, can also be estimated with this
type of data. For the Tobit model we can employ the two-part estimation method that
yields moment conditions as in (69). Only in the linear regression model is the GMM estimator equivalent to a (linear) imputation estimator. In the other examples, imputation
yields biased estimates.
The additional common variables Z0c must be exogenous. They also have to be correlated with the variables in X. In other words, they must satisfy the requirements for
valid instruments for X, irrespective of whether the variables in X are exogenous or
endogenous. As noted before, the separability of the moment conditions is a sufficient,
but not necessary condition for identification.
The asymptotic distribution theory of the 2SIV estimator based on (69) raises some
new issues. First, we introduce some notation. Let
 


m(θ ) = f (Y ; θ ) − g(X, Z0 ; θ ) h Z0 , Z0c
(82)
and for i = 1, . . . , N1 , j = 1, . . . , N2 ,


c
,
m2j (θ ) = f (Yj ; θ )h Z02j , Z02j


c
m1i (θ ) = g(Xi , Z01i ; θ )h Z01i , Z01i

(83)

with the second subscript in, e.g. Z01i indicating that the common included exogenous
variable Z0 is observed in sample A etc. Using this notation, the sample moment conditions are
mN (θ ) =

N2
N1
1 
1 
m2j (θ ) −
m1i (θ ).
N2
N1
j =1

(84)

i=1

We make the following assumptions (the derivatives in the assumptions are assumed
to exist and to be continuous in θ ):
c and
(A1) The common variables in samples A and B, the random vectors Z01 , Z01
c
Z02 , Z02 are independently but identically distributed.

10 Provided that the identification condition (A3) below is satisfied.
11 The latter transform is used by Carroll, Dynan and Krane (1999) who use two independent samples to

estimate their regression model. Because their model has a ‘missing parameter’ and not a missing regressor,
they do not use 2SIV.

5504

G. Ridder and R. Moffitt

(A2) If N1 , N2 → ∞
∂mN
∂m
p
(θ ) −
→E
(θ )

∂θ
∂θ 
uniformly for θ ∈ Θ with Θ the parameter space.
∂m
(A3) The rank of the matrix E( ∂θ
 (θ0 )) is equal to the dimension of θ .
Assumption (A1) ensures that the limit in (A2) holds pointwise for every θ ∈ Θ. Assumption (A3) is the identification condition. The probability limit of the derivative of
the moment conditions is


∂f (Y ; θ ) 
∂g(X, Z01 ; θ ) 
∂m
c
c
(θ ) = E
h Z02 , Z02
h Z01 , Z01
−E
.
E



∂θ
∂θ
∂θ
(85)
This matrix can be estimated consistently from the samples A and B, because the expectations only involve variables that are observed in the same sample.
The 2SIV is formally defined by
θ̂N = arg min mN (θ ) WN mN (θ )
θ∈Θ

(86)

with WN a weighting matrix that satisfies
p

WN −
→W

(87)

with W a positive definite matrix and N → ∞ if N1 , N2 → ∞. In Appendix A we
show that assumptions (A1)–(A3) are sufficient for weak consistency of the 2SIV.
If (A1) does not hold, the 2SIV is biased. The probability limit is the minimizer of
 



∂m
∂m

(θ∗ ) (θ − θ0 )
(θ∗ ) W E
(θ − θ0 ) E
∂θ
∂θ 








∂m
(θ∗ ) (θ − θ0 ) + E m(θ0 ) W E m(θ0 )
+ 2E m(θ0 ) W E
(88)

∂θ
but the last two terms do not vanish. We can use this expression to find the asymptotic
bias of the 2SIV estimator.
The optimal weight matrix W is the inverse of the variance matrix of mN (θ0 ). To
derive the asymptotic variance matrix we have to make an assumption on the rate at
which the sample sizes increase. Such an assumption was not needed to establish weak
consistency of the 2SIV estimator. We assume
N2
= λ with 0 < λ < ∞.
(A4) limN1 →∞,N2 →∞ N
1
Consider, using the fact that E(m(θ0 )) = 0 if (A1) is true,


N2



1 
N2 mN (θ0 ) = √
m2j (θ0 ) − E m2j (θ0 )
N2 j =1

−

N1



N2 1 
m1i (θ0 ) − E m1i (θ0 ) .
√
N1 N1
i=1

(89)

Ch. 75:

The Econometrics of Data Combination

Hence, the asymptotic variance matrix of the moment conditions is






M(θ0 ) = lim E N2 mN (θ0 )mN (θ0 ) = λ Var m2j (θ0 ) + Var m1i (θ0 )
N2 →∞

5505

(90)

and the inverse of this matrix is the optimal choice for W (θ0 ). This matrix can be easily estimated if we have an initial consistent estimator. Note that by the √
central limit
theorem for i.i.d. random variables (if the asymptotic variance is finite) N2 mN (θ0 )
converges to a normal distribution with mean 0. However, if (A1) does not hold and as
a consequence E(m(θ0 )) = 0, the mean diverges. This will affect the interpretation of
the test of overidentifying restrictions that will be discussed below.
Under (A1)–(A4)



d
N2 (θ̂N − θ0 ) −
→
N 0, V (θ0 )
(91)
with


−1
∂m
∂m
(θ0 )
(θ0 ) W (θ0 )E
V (θ0 ) = E
∂θ
∂θ 



∂m
·E
(θ0 ) W (θ0 ) λ Var m2j (θ0 )
∂θ


∂m
+ Var m1i (θ0 ) W (θ0 )E
(θ0 )
∂θ 

−1
∂m
∂m
· E
(θ0 )
.
(θ0 ) W (θ0 )E
∂θ
∂θ 

(92)

See Appendix A for a proof.
The preceding discussion suggest a two-step procedure. In the first step we use a
known weight matrix, e.g. WN = I . The resulting 2SIV estimator is consistent, but not
efficient. In the second step, we first estimate the optimal weight matrix, the inverse of
(90). This matrix only depends on the first-step consistent estimator and moments that
2
can be computed from the two independent samples A and B (for λ we substitute N
N1 ).
Next, we compute the efficient 2SIV estimator (86) with this weight matrix. This estimator has asymptotic variance
−1





 ∂m
∂m
(θ0 )
(θ0 ) λ Var m2j (θ0 ) + Var m1i (θ0 ) E
E
(93)
∂θ
∂θ 
which can be estimated from the independent samples.
In general, the efficient 2SIV estimator is less efficient than efficient estimators based
on a sample that contains all the variables. In the case that the information matrix only
depends on variables in sample A, we can estimate the variance of the efficient estimator, even if the estimator itself cannot be computed from the independent samples. The
inverse of the information matrix gives an indication of the efficiency loss, due to the
fact that we do not have a sample that has all variables.

5506

G. Ridder and R. Moffitt

If the number of moment conditions is larger than the number of parameters, we can
test the overidentifying restrictions. The test statistic is




 −1

N2
 m1i (θ̂N )
 m2j (θ̂N ) + Var
TN = N2 mN (θ̂N )
(94)
mN (θ̂N ),
Var
N1
d
 denotes the sample variance. If (A1)–(A4) hold, then TN −
→
χ 2 (dim(m) −
where Var
dim(θ )). Appendix A contains a proof.
As noted before, rejection of the overidentifying restrictions indicates that either
some of the common variables that are used as instruments are not exogenous or that
they are not identically distributed in the samples A and B.
Although the technique of choice for estimating relations from combined samples
has been GMM, Maximum Likelihood can be used as well. A reason for the preference
for GMM (or IV) may be that in that framework it is easier to obtain consistent estimates of structural parameters if some of the regressors are endogenous. Orthogonality
conditions for equation errors and instrumental variables are more natural in GMM. To
define the Two-Sample Maximum Likelihood (2SML) estimator we start with a parametric model for the conditional distribution of Y given X, Z0 , f (y | x, z0 ; θ ). Because
X is not observed in sample A, we use sample B to estimate the conditional density
of X given Z0 , Z1 . We can use a parametric or a non-parametric estimator for the latter conditional density. The likelihood contributions are obtained from the conditional
density of Y given Z0 , Z1

f (y | z0 , z1 ; θ ) =

f (y | x, z0 ; θ )g(x | z0 , z1 ) dx.

(95)

With a parametric estimator for g(x | z0 , z1 ) the 2SML estimator is a conventional
MLE with all the usual properties. The properties of the 2SML with a non-parametric
estimator of this conditional density have not been studied. In Section 4.2.2 we considered non-parametric identification of f (y | x1 , z0 ), and non-parametric identification
is sufficient for parametric identification. Again Chen, Hong and Tarozzi (2004) and
Imbens, Newey and Ridder (2004) provide the framework in which the 2SML can be
analyzed.
2SIV or 2SML are used if some of the explanatory variables in a relation are not
measured in the same sample as the dependent variable. Another situation occurs in
models with generated regressors, in which the parameters of the generated regressor
cannot be estimated from the same sample. An important example of a generated regressor is the sample selection correction function. An example is the estimation of
a wage equation on a sample of working individuals. This yields biased estimates of
the regression coefficients if a positive fraction of the population under consideration
does not work. A method to reduce this bias is to include a sample selection correction
function [Heckman (1979)]. The parameters of this function cannot be estimated from
the sample of working individuals. However, if an independent sample is available that
contains both working and nonworking individuals but no information on wages, then

Ch. 75:

The Econometrics of Data Combination

5507

the parameters can be estimated from this sample. This allows us to compute the sample
selection correction for the working individuals.
Another example of a generated regressor is Carroll, Dynan and Krane (1999) who
estimate the effect of the probability of becoming unemployed on the wealth to income ratio. They estimate the wealth equation with data from the Survey of Consumer
Finances (SCF). However, the SCF has no information on unemployment. The probability of becoming unemployed is estimated from the Current Population Survey (CPS)
and because the variables that enter this probability are also observed in the SCF, this
probability can be imputed in the SCF. Note that in these examples there are no missing variables. Only the parameters that enter the generated regressor are estimated from
an independent sample. This type of data combination can be treated as any estimation problem with a generated regressor [Pagan (1984)]. The fact that the parameter is
estimated from an independent sample even simplifies the distribution theory.
4.4. The origin of two-sample estimation and applications
Of the methods discussed in this section only the 2SIV estimator is prominent in econometrics. The first author who suggested this estimator was Klevmarken (1982). Since
then it was rediscovered independently by Angrist and Krueger (1992) and Arellano
and Meghir (1992).12 Klevmarken derives the 2SIV estimator for a single equation that
is part of a system of linear simultaneous equations. In our notation he considers
Y = β0 + β1 X + β2 Z0 + ε

(96)

with X observed in sample A and Y in sample B, while Z0 is a subvector of the common variables Z. He also assumes that all the variables in X are endogenous,13 that all
the common variables Z are exogenous and that Z contains all exogenous variables.14
If we compare these assumptions with ours, we see that Klevmarken’s assumptions are
far too strong and limit the application of 2SIV to rather special cases. In particular,
the assumption that Z contains all exogenous variables seems to be inspired by a desire to give a structural interpretation to the first-stage imputation regression, in which
X is regressed on the exogenous variables in Z. Such an interpretation is not needed,
and hence the only requirement is the order condition discussed in the previous subsection. Moreover, not all common variables need to be exogenous, as long as this order
condition is satisfied. Finally, some of the variables in X may be exogenous. Klevmarken states that we can only allow for exogenous variables if the joint distribution of
X and Z is multivariate normal, which ensures that the conditional mean of X given Z
is linear in Z. As the derivation in the previous subsection shows, a linear conditional
mean is not essential for the 2SIV estimator. In the linear regression model replacing

12 These authors do not cite Klevmarken’s contribution.
13 Klevmarken (1982, p. 160).
14 Klevmarken (1982, p. 159).

5508

G. Ridder and R. Moffitt

the conditional expectation by the linear population projection on Z will not affect the
moment conditions15 and hence the assumption of multivariate normality is not needed.
Carroll and Weil (1994) start from the same model as Klevmarken. They claim16 that
to compute the variance matrix of the 2SIV estimator it is required that in one of the
datasets we observe Y, X, Z. The discussion in the previous subsection shows that this
is not necessary. The problem with their approach is that their estimator of the variance
matrix requires the residuals of the regression and these cannot be recovered from the
independent samples.
At this point, we should clarify the role of endogenous and exogenous regressors in
2SIV estimation. The natural solution to missing variables in a statistical relation is imputation of these variables. Indeed, the 2SIV estimator in the linear regression model
can be seen as an imputation estimator. Econometricians are used to imputation if the
regression contains some endogenous variables. In the Two-Stage Least Squares (2SLS)
estimator the endogenous variables are replaced by a predicted or imputed value. Hence,
it is not surprising that 2SIV was originally developed for linear regression models with
endogenous regressors. Our derivation shows that such a restriction is not necessary,
and in particular, that the 2SIV only imputes missing variables, if the model is a linear regression. In the general case specified in (69), there is no imputation of missing
variables.
After Klevmarken (1982) the 2SIV estimator was reinvented independently by
Arellano and Meghir (1992) and Angrist and Krueger (1992). Arellano and Meghir
consider moment restrictions of the form (we use our earlier notation with Z1 , Z2 the
common variables Z as observed in sample A and B, respectively)


E m(X, Z1 ; θ ) = 0,


E m(Y, Z2 ; θ ) = 0,
(97)
i.e. the moment restrictions are defined for the samples A and B separately. These separate moment restrictions are obtained if we consider the linear regression model (96).
If we relate the X to the exogenous common variables Z
X = ΠZ + η,

(98)

we can substitute this in (96) to obtain
Y = β0 + β1 ΠZ + β2 Z0 + ε + β1 η.

(99)

If the order condition is satisfied, we can estimate β from the linear regression in (99).
In particular, (98) can be estimated from sample A and (99) from sample B. The corresponding moment conditions are

15 Provided that the distribution of the common variables in the two samples is the same.
16 See the Technical Appendix to in their paper.

Ch. 75:

The Econometrics of Data Combination



E (X1 − ΠZ1 ) Z1 = 0,
 

E Y − β0 − β1 ΠZ2 − β2 Z02 Z2 = 0

5509

(100)

and this has the form (97). Note again that the linear first step can be seen as a linear
population projection and is valid even if the conditional expectation of X1 given Z
is not linear (provided that Z1 and Z2 have the same distribution). Also the moment
restrictions are nonlinear in the structural parameters β. Arellano and Meghir (1992)
propose to estimate β0 , π = Π  β1 and β2 , and to use Chamberlain’s (1982) minimum
distance estimator in a second stage to obtain an estimate of the structural parameters.
Their estimator is equivalent to the imputation estimator. In particular, it can only be
used if the X enters linearly in the moment conditions, and it cannot be used if we
estimate a model with a nonlinear (in X) moment condition.
Arellano and Meghir apply their estimator to a female labor supply equation. In this
equation the dependent variable, hours, is observed in the UK Labor Force Survey
(LFS), the European counterpart of the US Current Population Survey. Two of the independent variables, the wage rate and other income, are obtained from a budget survey,
the Family Expenditure Survey (FES). This situation is common: budget surveys contain detailed information on the sources of income, while labor market surveys contain
information on labor supply and job search. An indicator whether the woman is searching for (another) job is one of the explanatory variables. Arellano and Meghir estimate
the labor supply equation using the LFS data after imputing the wage rate and other
income, using a relation that is estimated with the FES data. The common variables
(or instruments) that are used in the imputation, but are excluded in the labor supply
equation are education and age of husband and regional labor market conditions.
Angrist and Krueger (1992) consider the linear regression model
Y = β0 + β1 X + ε

(101)

with X, Z1 observed in sample A and Y, Z2 in sample B with A and B independent
samples from a common population. They assume that all common variables are exogenous, and they implicitly assume that the number of (exogenous) common variables
exceeds the number of variables in X, i.e. that the order condition is satisfied. Under
these conditions the 2SIV estimator is based on a special case of the moment conditions
in (70)–(72).
Angrist and Krueger apply the 2SIV estimator to study the effect of the age at school
entry on completed years of schooling. Children usually go to school in the year in
which they turn 6. If this rule were followed without exceptions, then the age at school
entry would be determined by the birthdate. However, exceptions occur and there is
some parental control over the age at school entry which makes this variable potentially
endogenous. Angrist and Krueger assume that the date of birth is not correlated with any
characteristic of the child and hence has no direct effect on completed years of schooling. Because there is no dataset that contains both age at school entry and completed
years of schooling, Angrist and Krueger combine information in two US censuses, the
1960 and the 1980 census. Because they use 1% (1960) and 5% (1980) samples they

5510

G. Ridder and R. Moffitt

assume that the number of children who are in both samples is negligible. They compute the age at school entry from the 1960 census and the completed years of schooling
from the 1980 census. The common variable (and instrument) is the quarter in which
the child is born.
Other applications of 2SIV are Carroll and Weil (1994), Lusardi (1996), Dee and
Evans (2003), and Currie and Yelowitz (2000). Carroll and Weil (1994) combine data
from the 1983 Survey of Consumer Finances (SCF) that contains data on savings and
wealth and the Panel Study of Income Dynamics (PSID) that contains data on income
growth to study the relation between the wealth income ratio and income growth. The
common variables are education, occupation, and age of the head of the household.
Lusardi (1996) estimates an Euler equation that relates the relative change in consumption to the predictable component of income growth. Because the consumption data in
the PSID are unreliable, she uses the Consumer Expenditure Survey (CEX) to obtain the
dependent variable. She also shows that the income data in the CEX are measured with
error (and that number of observations with missing income is substantial) and for that
reason she uses the PSID to measure income growth. She experiments with different sets
of common exogenous variables that contain household characteristics (marital status,
gender, ethnicity, presence of children, number of earners), education and occupation
(interacted with age), education (interacted with age). Dee and Evans (2003) study the
effect of teen drinking on educational attainment. The problem they face is that there is
no dataset that has both information on teen drinking and on later educational outcomes.
Moreover, drinking may be an endogenous variable, because teenagers who do poorly
in school may be more likely to drink. Data on teen drinking are obtained from the
1977–1992 Monitoring the Future (MTF) surveys, while data on educational outcomes
are obtained from the 5% public use sample from the 1990 US census. The common
exogenous variables are the minimum legal drinking age that differs between states, but
more importantly increased over the observation period, state beer taxes, ethnicity, age
and gender. The indicator of teen age drinking is considered to be endogenous. Currie
and Yelowitz (2000) consider the effect of living in public housing on outcomes for
children. The outcome variables, living in high density housing, overcrowding in the
house, being held back at school, are from the 1990 census. The indicator of living in
public housing is from the pooled 1990–1995 March supplements to the Current Population Survey (CPS). This indicator is assumed to be endogenous in the relation with
the outcome variables. The common exogenous variable is the sex composition of the
household where households with two children of different gender are more likely to
live in public housing because they qualify for larger units.
4.5. Combining samples to correct for measurement error
One of the reasons to merge datasets is that the variables in one of the sets is measured
more accurately. An example is the study by Okner (1972) who merged the 1967 Survey of Economic Opportunity with the 1966 Tax File using file matching, because the
income measures reported in the SEO were thought to be inaccurate. In this section we

Ch. 75:

The Econometrics of Data Combination

5511

show that even for this purpose the datafiles need not be merged, and that we can correct
for measurement error in one (or more) of the explanatory variables with only marginal
error free information.
The procedure that we describe works even if there are no common variables in the
two datasets. If there are common variables and if these are exogenous and not correlated with the measurement error, we can use the 2SIV estimator to obtain consistent
estimates of the coefficients in a linear relation where some independent variables are
measured with error.
There is a larger literature on the use of validation samples to correct for measurement error. In a validation sample both X1 and the true value X1∗ (and X2 ) are observed.
This allows for weaker assumptions on the measurement error process. In particular, the
measurement error can be correlated with X1∗ and with X2 . This type of sample combination is beyond the scope of the present chapter. Validation samples are rare, because
they require the matching of survey and administrative data. Chen, Hong and Tamer
(2005) propose a method for the use of validation samples if variables are measured
with error.
We consider a simple example of a conditional distribution with pdf f (y | x1∗ , x2 ; θ ).
There are two explanatory variables X1∗ , X2 where X2 is observed without error and the
error-free X1∗ is not observed. Instead, we observe X1 that is related to X1∗ as specified
below. The observed conditional distribution of Y given X1 , X2 is

 
  
f (y | x1 , x2 ; θ ) = f y  x1∗ , x2 ; θ g x1∗  x1 , x2 dx1∗
(102)
if X1∗ is continuous and the integral is replaced by a sum if X1∗ is discrete. To determine
the observed conditional distribution we need to specify or identify g(x1∗ | x1 , x2 ). We
show that this conditional density can be identified from a separate dataset that only
contains observations from the distribution of X1∗ , i.e. observations from the marginal
distribution of the error-free explanatory variable. Hence we have a sample A that contains Y, X1 , X2 and an independent sample B that contains only X1∗ .
We consider a special case that allows for a closed-form solution. In particular, we
assume that both X1∗ and X1 are 0–1 dichotomous variables. The relation between these
variables, the measurement error model, can be specified in a number of ways. We
only allow for measurement error models that are identified from observations from the
marginal distribution of X1 observed in sample A and the marginal distribution of X1∗ ,
observed in the independent sample B. An example of such a measurement error model
is classical measurement error which assumes






Pr X1 = 1  X ∗ = 1, X2 = Pr X1 = 0  X ∗ = 0, X2 = λ,
(103)
1

1

i.e. the probability of misclassification is independent of X1∗ . Moreover, (103) implies
that X1 is independent of X2 given X1∗ . Solving for λ we find
Pr(X1 = 1) + Pr(X1∗ = 1) − 1
.
λ=
(104)
2 Pr(X1∗ = 1) − 1
Hence, λ is indeed identified from the marginal distributions of X1 and X1∗ .

5512

G. Ridder and R. Moffitt

Note that (104) only gives solutions between 0 and 1 if


Pr(X1 = 1) < Pr X1∗ = 1

(105)

if Pr(X1∗ = 1) > 1/2, or if


Pr(X1 = 1) > Pr X1∗ = 1

(106)

if Pr(X1∗ = 1) > 1/2. This is equivalent to


 
Pr(X1 = 1) 1 − Pr(X1 = 1) = Var(X1 ) > Var X1∗




= Pr X1∗ = 1 1 − Pr X1∗ = 1 .

(107)

X1∗ ,

as is generally
In other words, the observed X has a larger variance than the true
true for classical measurement error models. This restriction on the observable marginal
distributions must be satisfied, if we want to consider the classical measurement error
model.
The second measurement error model assumes that misclassification only occurs if
X1∗ is equal to 1,17 maintaining the assumption that X1 is independent of X2 given X1∗ .
Hence



Pr X1 = 0  X1∗ = 0, X2 = 1,



Pr X1 = 1  X ∗ = 1, X2 = λ.
(108)
1

With this assumption we find
λ=

Pr(X1 = 1)
.
Pr(X1∗ = 1)

(109)

As in the case of classical measurement error, this measurement error model implies an observable restriction on the two observed marginal distributions, in the case
Pr(X1 = 1)  Pr(X1∗ = 1).
Both measurement error models are special cases of the general misclassification
error model



Pr X1 = 0  X1∗ = 0, X2 = λ0 ,



Pr X1 = 1  X ∗ = 1, X2 = λ1 .
(110)
1

Again we assume that X1 is independent of X2 given X1∗ . In this general model the
parameters λ0 , λ1 are not identified from the marginal distributions of X1 and X1∗ . Hence
we must fix one of these parameters or their ratio, as is done in the measurement error
models that we introduced in this section. We also assume that the misclassification is
independent of X2 .
Of course, it is not sufficient to identify the measurement error distribution. The
conditional density of Y given X1 , X2 , which is the basis for likelihood inference, is
17 The misclassification can also only occur if X ∗ is 0.
1

Ch. 75:

The Econometrics of Data Combination

5513

obtained from the density of Y given X1∗ , X2 , which contains the parameters of interest,
if we integrate the unobserved X1∗ with respect to the density of X1∗ given the observed
X1 , X2 (see (102)). Hence, the key is the identification of the distribution of X1∗ given
X1 , X 2 .
This conditional distribution is identified from the measurement error model that in
turn is identified from the marginal distributions of X1 and X1∗ and the joint distribution
of X1 , X2 . The solution depends on the measurement error model. Here we give the
solution, if we assume that the measurement error is classical, but the solution for other
(identified) measurement error models is analogous. In the sequel we use subscripts to
indicate the variables in the distribution.
Consider


 



gx1 ,x1∗ ,x2 x1 , x1∗ , x2 = gx1 x1  x1∗ , x2 gx1∗ ,x2 x1∗ , x2
  


= gx x1  x ∗ gx ∗ ,x x ∗ , x2
(111)
1

1

1

2

1

X1∗ .

After substitution of (103) we obtain
because X1 is independent of X2 given

∗
λgx1∗ ,x2 (x1 , x2 ),
x1 = x1∗ ,


gx1 ,x1∗ ,x2 x1 , x1∗ , x2 =
(112)
(1 − λ)gx1∗ ,x2 (x1∗ , x2 ), x1 = x1∗ .
The marginal distribution of X1 , X2 , which can be observed, is
gx1 ,x2 (x1 , x2 ) = λgx1∗ ,x2 (x1 , x2 ) + (1 − λ)gx1∗ ,x2 (1 − x1 , x2 ).

(113)

Solving for gx1∗ ,x2 (x1∗ , x2 ) we find

 (1 − λ)gx1 ,x2 (1 − x1∗ , x2 ) − λgx1 ,x2 (x1∗ , x2 )
gx1∗ ,x2 x1∗ , x2 =
(114)
.
1 − 2λ
Substitution in (112) gives the joint density of X1 , X1∗ , X2 . The conditional density of
X1∗ given X1 , X2 is obtained if we divide the result by gx1 ,x2 (x1 , x2 ).
With a dichotomous X1 we obtain a simple closed form solution. If X1 is continuous,
we can still identify the distribution of X1∗ given X1 , X2 if the measurement error model
is identified from the marginal distributions of X1 and X1∗ , as is the case if we assume
classical measurement error. Hu and Ridder (2003) show that the identification involves
two sequential deconvolution problems. They also develop the distribution theory of the
resulting estimator.

5. Repeated cross sections
5.1. General principles
Repeated cross sections consist of independent samples drawn from a population at
multiple points in time t = 1, . . . , T . There are many such data sets in the US and
other countries, and more than true panel data sets in some. In the US, the Current

5514

G. Ridder and R. Moffitt

Population Survey (CPS) is a leading example, as is the General Social Survey, and
even the Survey of Income and Program Participation, if data from different cohorts are
employed. There are also examples of firm-level data sets of this kind. In the UK, the
Family Expenditure Survey (FES) is a prominent example. In continental Europe, CPSlike cross sections are often used, as are repeated cross-sectional labor force surveys. In
developing countries, such labor force surveys are often available as well as several of
the World Bank LSMS surveys which have multiple waves.
Although repeated cross section (RCS) data have the obvious disadvantage relative to
panel data of not following the same individuals over time, they have certain advantages
over panel data. Attrition and nonresponse problems are generally much less severe,
for example, and often RCS data have much larger sample sizes than available panels.
In many cases RCS data are available farther back in calendar time than longitudinal
data because governments began collecting repeated cross sections prior to collecting
panel data. In some cases, RCS data are available for a broader and more representative
sample of the population than true panel data, at least in cases where the latter only
sample certain groups (e.g. certain cohorts as in the US NLS panels).
Although the cross sections can be pooled and cross-sectional models can be estimated on them, the more interesting question is whether they can be used to estimate
models of the type estimable with true panel data. To consider this question, assume
that in each cross section t we observe a sample from the distribution Wt , Zt where Wt
is a vector of variables that are only measured in each cross section and Zt is a vector of
variables which are measured in all cross sections, and hence can be used to match the
individuals across the different waves (individual subscripts i = 1, . . . , N are omitted
for now). Both Wt and Zt may contain variables which are identical at all t (i.e. time
invariant variables) although in most applications all time invariant variables will be
measured at all t and hence will be in Zt . We assume that the population is sufficiently
large and the sample sufficiently small that there are no common individuals in the cross
sections. Further, we assume that the population from which the samples are drawn is
closed,18 and thus we ignore out in- and out-migration, births, and mortality.
At issue is what distributions and what types of models can be identified from the set
of cross sections. The unconditional joint distribution of W1 , . . . , WT is not identified
except in the trivial case in which the elements are independent. Models which require
for identification only moments from each cross section, and which therefore do not
require knowledge of the joint distribution, are identified but do not make particular use
of the repeated cross section (RCS) nature of the data except perhaps for investigations
of time-varying parameters. The models of interest and under discussion here are those
which require identification of the joint distribution or of some aspect of it.
Identification necessarily requires restrictions. Non-parametric identification of conditional distributions f (Wt | Wτ ), t = τ , follows from the general principles and
18 This ensures that the relation between a dependent and independent variables does not change over time

due to in- and outflow from the population, and we can make this assumption, instead of that of a closed
population.

Ch. 75:

The Econometrics of Data Combination

5515

restrictions elucidated in Section 4.2.2 above, with the change of notation from Y to
Wt and from X to Wτ . With the common variable Zt available in each cross section and
used for matching, bounds on those conditional distributions can be established. If Zt
or some elements of it are excluded from the relation between Wt and Wτ , and Zt is
discrete, the conditional distributions are exactly identified provided a rank condition is
met which relates the number of points in the support of Zt to the number of conditional
distributions to be estimated.
We shall focus in this section primarily on parametric models for which independence
of W1 , . . . , WT is not assumed but which contain exclusion restrictions. While there are
in general many models which can be identified under different restrictions, we will
work with a model similar to that in Section 4.3.2 above:
f (Yt ; θ ) = g1 (Xt , Z0 ; θ ) + g2 (Yt−1 , Z0 ; θ ) + εt
and with associated GMM moment condition, following on (69), of:



E f (Yt ; θ ) − g1 (Xt , Z0 ; θ ) − g2 (Yt−1 , Z0 ; θ ) h(Z0 , Z1t ) = 0,

(115)

(116)

where f, g1 , g2 , and h are known (possibly up to parameters) functions and θ a vector of parameters. The vector Z0 is a vector of common time-invariant variables in the
cross sections which are included in the g1 and g2 relations.19 In most applications,
f (Yt ; θ ) = Yt . The function g1 contains only Xt and Z0 and hence appears to be estimable from a single cross section, but, as will be shown below, this is problematic
in fixed effects models because Xt is correlated with the error in that case. The functions g1 and g2 must be separable because Xt and Yt−1 do not appear in the same cross
section.
Individuals across cross sections are identified by variables Z0 and Z1t , with the latter
excluded from the relation between Yt and Xt , Yt−1 , Z0 . In most applications to date,
Z1t = t or a set of time dummies.20 The critical exclusion restriction in all RCS models
is that Z1t and its interactions with Z0 do not enter in g1 and g2 , and yet these variables
are correlated with those functions. For the Z1t = t case, this implies that variables
that change predictably with time, as individual age, year, unemployment duration, or
firm lifetimes (depending on the application) cannot enter g1 and g2 . Thus the essential
restriction in RCS estimation is that intertemporal stability exist in the true relationship.
Such a restriction is not needed when true panel data are available. Note as well that the
number of independent components in h must not be smaller than the dimension of θ
and, in most models, must be larger than the dimension of Xt , Yt−1 , and Z0 . This also
can be a fairly limiting condition in practice if the number of cross sections available is

19 These variables can be time-varying but this is rare in applications so we consider only the case where they

are time-constant. None of the results we discuss below are substantially changed by this restriction.
20 However, it is possible that some history information is available in each cross section which means that

these time-varying variables (e.g. employment or marital status histories in the case of household survey data;
ages of children are another) are potential additional instruments.

5516

G. Ridder and R. Moffitt

small relative to the number of parameters whose identification requires instrumenting
with functions of t.
In linear models the GMM estimator can be implemented as a two-step estimator.
First, project Xt and Yt−1 on h(Z0 , Z1t ), i.e. obtain E(Xt | h(Z0 , Z1t )) and E(Yt−1 |
h(Z0 , Z1t )).21 Second, regress Yt on these projections and on Z0 . If there are no Z0 in
the data and h(Z1t ) is a set of time dummies, this is equivalent to an aggregate timeseries regression where the time means of Yt are regressed upon the time means of Xt
and Yt−1 . In this case the number of cross sections has to be at least 3. Most interesting
cases arise however when Z0 variables are available; in household survey data, these
may be birth year (= cohort), education, race, sex, and so on. If these variables are
all discrete and h(Z1t , Z0 ) is assumed to be a vector of indicators for a complete crossclassification Z0 and time, estimation using (116) is equivalent to a regression of the cell
means of Yt on the cell means of Xt , Yt−1 , and the dummy variables Z0 . Note that in
that case we need fewer cross sections. However, if a parametric form of h is assumed,
this aggregation approach is not necessary, and if the model is nonlinear (including the
binary choice and related models), the two-step aggregation approach is not possible in
the first place. In that case the estimator is the possibly overidentified GMM estimator
defined by the moment conditions in (116).
Two leading examples fit into this framework. One is the linear first-order autoregression (with individual i subscripts now added)
Yit = α + βYi,t−1 + γ Xit + δZ0i + εit .

(117)

With time dummies as excluded variables the number of observations is equal to the
number of cross sections and this imposes restrictions on the time-variation of the parameters of (117). The restriction that the instrument must be relevant implies that the
mean of E(Yt−1 | Z0 , t) must vary with t. If Yt−1 is correlated with εt , an instrument
Z1t must be found which is orthogonal to εt .
A second example is the linear individual effects model
Yit = γ Xit + δZ0i + fi + εit ,

(118)

where f is an individual effect which is potentially correlated with Xt and Z0 . The
within-estimator commonly used with true panel data cannot be implemented with RCS
data because it requires knowledge of Yt at multiple t. RCS IV estimation using (116)
proceeds by using the elements of h as instruments for Xt , which again requires some
minimal time-invariance of the parameters of (118). Consistency (see below) is based
on the presumption that time-varying variables like those in Z1t must be orthogonal
to time-invariant variables like f . For instrument relevance, E(Xt | Z0 , t) must vary
with t.
Estimation of the model in (118) by the aggregation method mentioned previously
was proposed by Deaton (1985). He considers cohort data, so that time in his case is
21 Projections onto Z and Z directly are an alternative.
0
1t

Ch. 75:

The Econometrics of Data Combination

5517

age. Deaton considered Z0 to contain only birth year (= cohort) indicators and h to be
a set of all cohort-age indicators. He then proposed constructing a data set of cohort
profiles of mean Y and X (a ‘pseudo’ panel data set) and estimating (118) by regressing
the age-cohort means of Y on those of X and on cohort dummies (or by the withinestimator for fixed effects models applied to these aggregate observations).
5.2. Consistency and related issues
The conditions for consistency of moment estimators in the form (116) are well known
in general [Hansen (1982)]. The special form they take in the two sample case were
considered in Section 4.3.2 above, where weak consistency was proven. For the RCS
case, aside from the usual rank conditions and conditions on convergence of matrices
to positive definite forms, we have the condition that the instruments are not correlated
with the random error


E ηit h(Z0i , Zit ) = 0,
where ηit = f (Yit ; θ ) − g1 (Xit , Z0i ; θ ) − g2 (Yi,t−1 , Z0i ; θ ). If there is an individual
effect, we have that ηit = fi + εit and hence we require that E[εit h(Z0i , Zit )] = 0, and
E[fi h(Z0i , Zit )] = 0. The first assumption must hold even with the presence of Yt−1 in
the equation and represents an IV solution familiar to panel data models with dynamics
and lagged endogenous variables. However, with a lagged dependent variable in the
equation the errors in successive periods have a MA(1) structure because the errors
in not observing the same individuals in each cross section are correlated [McKenzie
(2004)].
The assumption on the individual effect fi that may be correlated with Xit is the
more problematic assumption. If h is a set of time dummies, then a sufficient condition
is that the (population) mean of fi does not change over time. If we have repeated cross
sections of size Nt in period t = 1, . . . , T , then this implies that22
ft =

Nt
1 
p
fi −
→ 0.
Nt
i=1

Hence, if min{N1 , . . . , NT } → ∞, then the limit of the time averaged regression without a lagged dependent variable is
Yt∗ = α + γ Xt∗ + εt∗
with εt∗ a common time shock in the εit and * indicating population averages of the
variables. OLS applied to this equation gives consistent estimators of the regression
parameters, and this establishes that the GMM estimator that uses moment condition
(116) is consistent if min{N1 , . . . , NT } → ∞, i.e. for large N asymptotics.
22 Without loss of generality we can take the common time constant limit equal to 0.

5518

G. Ridder and R. Moffitt

For the same model and assumptions on the random error, time dummies are not
valid instruments if Nt is fixed and T becomes large. Note that in this case the number
of instruments is equal to T and hence goes to infinity. The problem is obvious if we
consider the second stage regression that involves the projections on the instruments,
i.e. the averages in the repeated cross sections
Y t = α + γ Xt + f t + εt .
Hence
E[X t f t ] =

1
E[Xit fi ] = 0
Nt

for finite Nt .
There is another asymptotic that can be considered as well, which is an asymptotic
in the number of cohorts [Deaton (1985), Verbeek (1996)]. Up to this point we have
assumed that a single population of N individuals is followed over time for T periods,
which is equivalent to a single cohort (or a fixed set of birth years). Now let us consider increasing the number of such cohort groups (c) by moving over calendar time,
or possibly space, and increasing the number of pseudo-panels in the data. Each new
panel has N individuals and is observed for T periods. Once again, with fixed N, the
average individual effect will be correlated with the average covariate, so that the GMM
estimator is biased.
Deaton (1985) has proposed a modification of the estimator for the linear fixed effects
model which contains a bias adjustment for the finite, fixed N case and which is consistent for the large T case, an estimator that has been much discussed in the literature.
Deaton notes that estimation of the aggregated estimation equation
Y ct = γ Xct + δc + εct ,

(119)

where means are taking over observations within each cohort (c) and year (t) cell yields
biased estimates for finite N because f ct is correlated with Xct . Deaton instead considers the “population” equation
∗
∗
Yct∗ = γ Xct
+ δc + εct
,

(120)

where variables with asterisks represent population values, i.e. values that would obtain
if the cohort would be infinitely large. Note that δc absorbs a nonzero mean of the fi in
cohort c.
For the estimation of (120) Xct and Y ct must be inserted to proxy their population
counterparts but they do so with error. Deaton suggests that the measurement error for
each be estimated by the within-cell variances of X and Y using the individual data and
that a finite-sample adjustment be made when estimating the coefficient vector.
Deaton does not set up his model in the GMM framework but it can be done so.
Although he discusses his estimator as an errors-in-variables estimator, it is more in
line with our discussion to consider it as a finite N bias-corrected version of the GMM
estimator. To focus on the key issues, assume that only one cohort of N individuals is

Ch. 75:

The Econometrics of Data Combination

5519

observed for T periods. The individual model is
yit = δ + βxit + fit + εit .

(121)

The second stage equation when using time dummies as instruments is
y t = βx t + f t + εt .

(122)

Consequently,
Cov(y t , x t ) = β Var(x t ) + Cov(f t , x t ).

(123)

The bias term in (123) is
Cov(fi , xit )
.
(124)
N
This bias term is small if N is large or if the correlation between the regressor and the
individual effect is small. The Deaton finite sample adjustment can be derived by noting
that fit = yit −βxit −εit and that, therefore, Cov(fit , xit ) = Cov(yit , xit )−β Var(xit ).
Cov(f t , x t ) =

σ −βσ 2

Hence Cov(f t , x t ) = yx N x where σyx and σx2 are the covariance of x and y and the
variance of x for the individual observations in a time period. Inserting this into (123)
and solving for β, we obtain the Deaton estimator if we replace the population variances
and covariances with sample variances and covariances:
β̂ =

Cov(y t , x t ) −
Var(x t ) −

σyx
N

σx2
N

.

(125)

As N → ∞ the bias and the bias correction terms go to 0 and the least squares estimate
of the aggregate model is consistent. Deaton noted that the estimator is also consistent
as T → ∞ and Verbeek and Nijman (1992, 1993) show that this estimator is consistent as C → ∞ provided a minor change is made in the bias correction. Verbeek and
Nijman also note that the Deaton estimator increases variance at the same time that
it reduces bias, giving rise to a mean-squared error tradeoff that can be addressed by
not subtracting off the “full” bias correction in (125). Devereux (2003) shows that the
Deaton estimator is closely related to estimators which adjust for finite sample bias in
IV estimation and that, in fact, the estimator is equivalent to the Jacknife Instrumental
Variables estimator and is closely related to k-class estimators. Devereux also proposes a
modification of the Deaton estimator which is approximately unbiased but has a smaller
finite sample variance.
There have been some explorations in the literature seeking to determine how large
N must be for the finite sample adjustments to be avoided by Monte Carlo simulations.
Verbeek and Nijman (1992) suggest that cell sizes of 100 to 200 are sufficient, while
Devereux (2003) suggests that they should be higher, possibly 2000 or more. The necessary N is sensitive to the specification of the model. Devereux also conducts an exercise
which subsamples the available N in a model to gauge the degree of bias.

5520

G. Ridder and R. Moffitt

There has also been a discussion in the literature of how to divide the available data
into cohort groups, given that most data sets do not have sufficient samples to divide the
data completely by discrete values of birth year [Verbeek and Nijman (1992, 1993)]. Dividing the sample into more birth cohorts increases C while decreasing the sample size
per cohort. In the applied literature, groupings of birth cohorts and formation of cells
for the aggregated estimation has been, by and large, ad hoc. Moffitt (1993) suggests
that aggregation not be conducted at all but rather that the individual data be employed
and a parametric function of birth year and t be estimated to smooth the instrument to
achieve efficiency, but he does not present any formal criteria for how to do so. A better
framework for analyzing these issues is that which considers alternative specifications
of the instrument which trade off bias and variance. Donald and Newey (2001) present
one such analysis.
The literature has also addressed dynamic fixed effects models. In this case we are
interested in the individual model
Yit = α + βYi,t−1 + δZ0i + fi + εit

(126)

which is a combination of (117) and (118). The desirability of different instrument sets
Z1i depends once again on the asymptotics involved. But when asymptotics are taken
in N (the number of observations per cohort), the consistency properties of different
instrument sets are almost identical to those for true panel data [Sevestre and Trognon
(1996), Arellano and Honoré (2001)]. Using simple functions of t as instruments, for
example, will yield inconsistent estimates for the same reasons that conventional fixed
effects methods in true panel data yield inconsistent estimates in the presence of both
fixed effects and lagged regressors. As in the case of true panel data, additional instruments which generate first-differenced estimators and which use lagged values of the
dependent variable can yield consistent estimates.
Collado (1997) and McKenzie (2004) consider this model and discuss various applications of IV to the model, using the same principles in the literature on true panel
data, using lagged values of the dependent variable as instruments and possibly using
the larger instrument set implied by the Arellano–Bond estimator. Collado and McKenzie also propose Deaton-style bias-correction terms to correct for the finite N problem
discussed above. Collado shows that her estimator is consistent in C and, for a different
bias-correction, consistent in T . McKenzie considers a sequential asymptotic in which
N is first allowed to go to infinity conditional on fixed T and then limits are taken
w.r.t. T .
5.3. Binary choice models
In the binary choice model we return to (115) and let f (Yt ; θ ) = Yt∗ , Yt = I (Yt∗  0),
and F be the c.d.f. of −εt . Then Pr(Yt = 1 | Xt , Z0 , Yt−1 ; θ ) = F (g1 (Xt , Z0 ; θ ) +
g2 (Yt−1 , Z0 ; θ )) so that


Yit = F g1 (Xit , Z0i ; θ ) + g2 (Yi,t−1 , Z0i ; θ ) + νit ,
(127)

Ch. 75:

The Econometrics of Data Combination

5521

which does not fit into the framework of the moment condition in (117) because Xt and
Yt−1 are not separable. Let us therefore initially assume g2 = 0 and consider lagged
indicators below. Now (117) applies directly assuming the availability of a suitable exclusion restriction, as before. The moment conditions are a simple extension of those
shown in Equations (74)–(76). The method is applicable to the individual effects binary
choice model or to a binary choice model with endogenous Xt with the restrictions that
hold in the cross section case. For instance, in parametric estimation where the F distribution is assumed to be known, a distributional assumption is needed for the individual
effect in order to derive F , e.g., if f is the individual effect component of εt ,
fi = v(Z0i ; φ) + ηi ,

(128)

where v is assumed to be of known form and where ηi has a known parametric distribution from which the c.d.f. of the composite error ηi − εit can be derived.
If Xt is endogenous and if the instrument is a set of time dummies, possibly interacted
with Z0 , the nonlinearity of the conditional expectation function means that GMM is
not equivalent to any type of aggregate regression of cell means of Y on cell means of X
and Z. However, with a stronger assumption, a version of such an approach is possible
[Moffitt (1993)]. The necessary assumption, in addition to (128), is
Xit = w(Z0i , Z1it ; ψ) + ωit ,

(129)

where w is a function of known parametric form and ωit is an error term with a parametric distributional form that may be correlated with εit . The assumption that the exact
form of dependence of the endogenous variable on the instruments is known and that
the conditional distribution of the regressor follows a specific parametric form are very
strong. In the simplest case, g1 is linear in Xt and Z0 and w is linear in Z0 and Z1t , and
εt and ωt are assumed to be bivariate normal. Then a variety of estimating techniques
are possible, drawing on the literature on endogenous regressors in limited dependent variable models [Amemiya (1978), Heckman (1978), Nelson and Olsen (1978),
Rivers and Vuong (1988), Smith and Blundell (1986); see Blundell and Smith (1993)
for a review]. Options include replacing Xt in g1 with its predicted value from (129);
inserting an estimated residual from (129) into (127); and estimating (153) and (155) in
reduced form by inserting (129) into (127). In this approach, the parameters of (127) are
estimated by maximum likelihood, which implies that the instrument vector h in (116)
F
is the binary choice instrument vector that is equal to (1−F
)F times the derivative of the
argument of F w.r.t. θ .
To consider the model with Yt−1 let us first consider the case in which Xt = X is
time invariant, in which case it can be folded into Z0 and we can let g1 = 0 without loss
of generality. Then we have


E(Yit | Z0i , Yi,t−1 ) = F g2 (Yi,t−1 , Z0i ; θ ) ,
(130)
where we have assumed that εit is distributed independently of Yi,t−1 , i.e. there is no serial correlation. Instrumental variable estimation of (130) conducted by replacing Yt−1

5522

G. Ridder and R. Moffitt

by a predicted value and applying maximum likelihood to the resulting model is known
to be inconsistent because Yt−1 is binary and hence its prediction error follows a nonnormal, two-point discrete distribution. An alternative procedure is to integrate Yt−1 out
of the equation. Letting pt (Z0 ) be the marginal probability Pr(Yt = 1 | Z0 ), we have
E(Yt | Z0 ) = pt (Z0 )
= pt−1 (Z0 ) Pr(Yt = 1 | Z0 , Yt−1 = 1)


+ 1 − pt−1 (Z0 ) Pr(Yt = 1 | Z0 , Yt−1 = 0)

 
 

= pt−1 (Z0 )F g2 (1, Z0 ; θ ) + 1 − pt−1 (Z0 ) F g2 (0, Z0 ; θ )

 

= pt−1 (Z0 ) 1 − λ(Z0 ; θ ) + 1 − pt−1 (Z0 ) μ(Z0 ; θ )
= μ(Z0 ; θ ) + η(Z0 ; θ )pt−1 (Z0 ),

(131)

where λ(Z0 ; θ ) = Pr(Yt = 0 | Z0 , Yt−1 = 1) = F (g2 (1, Z0 ; θ )) is the exit rate from
Yt−1 = 1 to Yt = 0, μ(Z0 ; θ ) = Pr(Yt = 1 | Z0 , Yt−1 = 0) = F (g2 (0, Z0 ; θ ))
is the exit rate from Yt−1 = 0 to Yt = 1, and η(Z0 ; θ ) = 1 − λ(Z0 ; θ )μ(Z0 ; θ ).
Equation (131) is a familiar flow identity from renewal theory showing how the marginal probability at t − 1 is transformed by the two transition probabilities into the
marginal probability at t. It suggests a procedure by which the reduced form model
Yt = μ(Z0 ; θ ) + η(Z0 ; θ )pt−1 (Z0 ) + νt is estimated by nonlinear least squares (given
the nonlinearity of the two transition probabilities in θ ) or GMM using a first-stage estimate of pt−1 (Z0 ) similar to the case of a generated regressor. Because the marginals
at every t are estimable from the RCS data, such a first-stage estimate is obtainable.
Identification of the transition probabilities is achieved by restricting their temporal dependence (indeed, in (131) they are assumed to be time invariant); identification is lost
if the two transition probabilities vary arbitrarily with t [Moffitt (1993)]. The model
is equivalent to a two-way contingency table where the marginals are known; the data
furnish a sample of tables and the restrictions on how the joint distribution varies across
the sample yields identification.
The first-stage estimation of pt−1 (Z0 ) can be obtained from an approximation of the
function or the structure of the model can be used to recursively solve for pt−1 (Z0 )
back to the start of the process. Assuming that p0 = 0 and that the process begins with
t = 1, and continuing to assume time-invariant hazards,


t−2

t−1−τ
η(Z0 ; θ )
pt−1 (Z0 ) = μ(Z0 ; θ ) 1 +
τ =1

1 − η(Z0 ; θ )t−1
= μ(Z0 ; θ )
1 − η(Z0 ; θ )

(132)

which can be jointly estimated with (131) imposing the commonality of the functions23
Alternatively, (131) can be expressed in fully solved back form and estimated as well.
23 Alternatively an initial conditions can be specified as a marginal p in the first period.

Ch. 75:

The Econometrics of Data Combination

5523

Equation (131) has been used as the basis of RCS estimation at the aggregate level.
Miller (1952) considered estimation of (131) with time-series data on the proportions
of a variable, pt which is special case of RCS data. Without data on individual regressors Z0 , he suggested simple least squares estimation of
pt = μ + ηpt−1 + νt .

(133)

Madansky (1959) proved that the least squares estimators of the two hazards are
consistent for fixed N as T → ∞ and for fixed T as N → ∞. Lee, Judge and Zellner
(1970) and MacRae (1977) proposed various types of restricted least squares estimators
to ensure that the estimated hazards do not fall outside the unit interval. This problem
would not arise in the approach here, which specifies the hazards in proper probability
form.
Estimation of the Markov model with RCS data is considerably complicated if there
is serial correlation in the errors or if time-varying Xt are allowed. With serial correlation of the errors, the two transition probabilities require knowledge of the functional
dependence of εt on Yt−1 . The most straightforward approach would require replacing
the simple transition probabilities we have shown here with joint probabilities of the entire sequences of states Yt−1 , Yt−2 , . . . , Y1 which in turn would be a nonlinear function
of Z0 and the parameters of the assumed joint distribution of εt−1 , εt−2 , . . . , ε1 . This
treatment would be parallel to maximum likelihood estimation with true panel data in
random effects and similar models where the joint distribution is likewise integrated
out. With time-varying Xt , the approach in (131) is problematic because
E(Yt | Xt , Z0 ) = μ(Xt , Z0 ; θ ) + η(Xt , Z0 ; θ )pt−1 (Xt , Z0 ),

(134)

where μ(Xt , Z0 ; θ ) = F (g1 (Xt , Z0 ; θ ) + g2 (0, Z0 ; θ )) and λ(Xt , Z0 ; θ ) = 1 −
F (g1 (Xt , Z0 ; θ ) + g2 (1, Z0 ; θ )). The difficulty is that pt−1 (Xt , Z0 ) is not identified
from the data. Estimation would require the assumption of a Markov or other process
for Xt which could be used to formulate a function pt−1 (Xt , Z0 ) which could be identified from the data.
5.4. Applications
Despite the large number of RCS data sets in the US and abroad, the methods described in this section have been applied relatively infrequently. The vast majority of
uses of RCS data simply estimate pooled cross-sectional parameters without matching
individuals across waves by birth cohort, education, or other individual time-invariant
covariates. A rather large literature on program evaluation in the US uses RCS data with
area fixed effects in a period where policies differ across areas and over time and policy
effects are estimated from the cross-area covariation in the change in policies and in
the outcome (migration is ignored). This literature likewise does not make use of the
techniques discussed here.
Of the applications that have been conducted, virtually all have used the Deaton linear fixed effects aggregation approach rather than the more general GMM-IV approach

5524

G. Ridder and R. Moffitt

described here. Most of the applications have been to life cycle models, which is a natural area of application because age profiles are central to the theory and the Deaton
approach is explicit in formulating aggregate cohort profiles of that type. Browning,
Deaton and Irish (1985) estimated a life cycle model of labor supply and consumption using seven waves of the FES and were the first to demonstrate the estimation of
the fixed effects model, which arises naturally from the first order conditions of separable lifetime utility functions, by aggregation into cohort profiles. Subsequent FES
analyses include Blundell, Browning and Meghir (1994), who estimated Euler equations under uncertainty for aggregate cohort profiles of consumption, using instrumental
variables with lags to control for the endogeneity of lagged consumption; Attanasio and
Weber (1994), who estimated life cycle consumption profiles with aggregate cohort
means but allowed calendar-time varying effects in an attempt to explain macro trends
in UK consumption; and Alessie, Devereux and Weber (1997), who added borrowing
constraints to the model. Analyses using RCS methods to other data sets are small in
number. Attanasio (1998) used the US Consumer Expenditure Survey to construct aggregate cohort profiles of saving rates in an attempt to explain the decline in saving
rates in the US. Blow and Preston (2002) used a UK tax data set that did not contain
information on age to estimate the effect of taxes on earnings of the self-employed,
and followed the aggregation approach grouping on region of residence and occupation. Paxson and Waldfogel (2002) used the Deaton method but applied to state-specific
means over time in the US, regressing state-specific measures of measures of child mistreatment on a number of state-level variables and mean socioeconomic characteristics
obtained from the CPS as well as state and year fixed effects. The authors applied the
Deaton finite-sample correction to the regressor matrix containing the moments for the
aggregate CPS regressors and reported large increases in estimated coefficients as a result. Finally, Heckman and Robb (1985) showed that treatment effects models can be
estimated with RCS data even if information on who has been trained and who has
not is not available in post-training cross sections if the fraction who are trained is
known.
There have been a few applications of the Markov model described above. Pelzer,
Eisinga and Franses (2002, 2004) have implemented the maximum likelihood estimator
suggested in Moffitt (1993) and discussed above, adding unobserved heterogeneity, for
two applications. The papers also discuss alternative computational methods and algorithms. In the first application, the authors used a true panel data set with five waves
to estimate a Markov model for changes in voter intentions (Democrat vs Republican),
treating the panel as a set of repeated cross sections. They then validated the model by
estimating model on the true panel, and found that the coefficients on the regressor variables were quite similar in both methods but that the intercept was quite different. In
the second application, the authors examined transition rates in personal computer ownership in the Netherlands over a 16-year period, but again using a true panel data set
which was initially treated as a set of repeated cross sections. The authors again found
the regressor coefficients to be quite close in both cases. The authors also note that
the RCS Markov model is formally identical to problem of ecological inference, or the

Ch. 75:

The Econometrics of Data Combination

5525

problem of how to infer individual relationships from grouped data [Goodman (1953),
King (1997)]. In the ecological inference problem, a set of grouped observations furnishes data on the marginals of binary dependent and independent variables (the “aggregate” data) and restrictions on how the joint distribution (the “individual data”) varies
across groups is used for identification.
Güell and Hu (2003) studied the estimation of hazard functions for leaving unemployment using RCS data containing information on the duration of the spell, allowing
matching across cross sections on that variable. The authors used a GMM procedure
very similar to that proposed here. The similarity to the RCS Markov model discussed
here is superficial, however, for the matching on duration permits direct identification
of transition rates. The authors apply the method to quarterly Spanish labor force survey data, which recorded spell durations, over a 16 year period, and estimate how exit
rates from unemployment have changed with calendar time and what that implies for
the distribution of unemployment. A simpler but similar exercise by Peracchi and Welch
(1994) used matched CPS files in adjacent years over the period 1968–1990 to measure
labor force transitions between full-time, part-time, and no work, and then assemble the
transition rates into an RCS data set which they use to estimate transition rates by cohort
as a function of age, year, and other variables.

6. Combining biased samples and marginal information
6.1. Biased samples and marginal information
In the previous sections we combined random samples from the same population that
had (some) population members and/or variables in common. In this section we study
the combination of samples that are drawn from distinct, but possibly overlapping subpopulations. The most common case is that of a stratified sample. In a stratified sample
the population is divided into nonoverlapping subpopulations, the strata, and separate
random samples, usually with different sampling fractions, are drawn from these strata.
A stratified random sample usually achieves the same precision, as measured by the
variance of estimates, with a smaller sample size.
If the sampling fraction differs between strata, the members of the population have
an unequal probability of being observed. If the probability of observation depends on
the variable of interest, or on variables that are correlated with the variable of interest,
then the stratified sample gives a biased estimate of the distribution of the variable of
interest and any parameter defined for this distribution.
A stratified sample is a special case of a biased sample. A biased sample is a sample
in which the probability of observation depends on the variable(s) of interest. Let Y be
the vector of variables of interest. In a biased sample the probability of observation is
proportional with W (Y ). The function W is called the biasing function. The density of

5526

G. Ridder and R. Moffitt

Y in the sample is24
W (y)f (y)
.
−∞ W (v)f (v) dv

g(y) =  ∞

(135)

Special cases of biasing functions are W (y) = IS (y) with IS the indicator of the
subset S of the support of Y , i.e. a stratum of the population, and W (y) = y, i.e. the
probability of selection is proportional to Y . If Y is a duration, the second biased sample
is called a length-biased sample. A length-biased sample is a biased sample from the
full distribution and not a sample from a subpopulation. Estimation from a pure lengthbiased sample does not involve sample combination.
For biased samples the distinction between the dependent variable(s) Y and the independent variable(s) X is important. In particular, it makes a difference if the distribution
of interest is that of Y or the conditional distribution of Y given X. If the biasing function
W (y, x) is a function of x only, the joint density of Y given X in the sample is
W (x)f (y | x)f (x)
.
g(y, x) =  ∞
−∞ W (w)f (w) dw

(136)

The marginal distribution of X in the sample has density
W (x)f (x)
−∞ W (w)f (w) dw

g(x) =  ∞

(137)

so that the conditional density of Y given X in the sample is the population conditional
density f (y | x). Hence, if we are interested in the conditional distribution of Y given
X (or parameters defined for this distribution) and the biasing function is a function of
X only, the biased sample directly identifies the conditional distribution of Y given X.
In all other cases, we cannot ignore the fact that we have a biased sample.
In Section 6.2 we consider parametric and non-parametric identification in biased
samples. In leading cases parametric restrictions secure identification while there is
non-parametric underidentification. This precludes tests of these parametric restrictions.
Non-parametric identification requires that the biased samples are ‘overlapping’ (in
a sense that will be made precise). Necessary and sufficient conditions for the nonparametric identification of the distribution of Y or the joint distribution of Y, X are
given by Gill, Vardi and Wellner (1988). These conditions apply if the biased samples
have the same variables. However they cannot be used if some of the subsamples only
have a subset of the variables in Y, X. It is even possible that we do not observe the
subsample itself, but only moments of the variables in the subsample. In these cases
non-parametric identification has to be established on a case by case basis.
Efficient estimation from a combination of biased samples is considered in Section 6.3. First, we consider efficient non-parametric estimation of the cdf of Y or that
24 Here and in the sequel g and f are either pdf’s or mass functions, i.e. densities with respect to the counting

measure.

Ch. 75:

The Econometrics of Data Combination

5527

of Y, X from a combination of biased samples that non-parametrically identifies these
distributions. Next, we consider the special case of an endogenously stratified sample
and parametric inference on the conditional distribution of Y given X, if the parameters in this distribution are identified.25 Finally, we consider the case that a (possibly
biased) sample is combined with information from other samples that only specify selected moments of a subset of the variables in Y, X. If the main sample is a random
sample then the parameters are identified from this sample and the additional information overidentifies the parameters. The additional degrees of freedom can be used to
increase the precision of the estimates or they can be used to test the (parametric) model
for the conditional distribution of Y given X. If the additional information just identifies the parameters there is no gain in precision. Finally, the first sample and additional
information may not identify the parameters. In that case the combination may provide narrower bounds on these parameters. An alternative is to define a population that
is consistent with all available information and to estimate parameters defined for this
population. These parameters are equal to the population parameters in the identified
case [Imbens and Hellerstein (1999)].
This final approach has all the earlier efficient parametric estimators as special cases.
It also covers the combination of biased samples with samples that have marginal information on a subset of the variables in Y, X. An example is the contaminated sampling
problem considered by Lancaster and Imbens (1996) who consider the combination of
a sample from the distribution of X given Y = 1, Y is a 0–1 variable, with a random
sample from the marginal distribution of X.
The theory of biased samples is now fairly complete. The general theory of identification is summarized in Gill, Vardi and Wellner (1988) who also discuss efficient
non-parametric estimation of the marginal cdf of Y or the joint cdf of Y, X. In econometrics the emphasis has been on parametric inference in the conditional distribution of Y given X. The efficient MLE was developed by Imbens (1992). Imbens and
Lancaster (1996) consider the general case. The history of this problem is interesting, because the contributions were made by researchers with different backgrounds,
which reflects the prevalence of biased samples in different areas. Cox (1969) considered non-parametric inference in length-biased samples. This was followed by a
number of contributions by Vardi (1982, 1985), culminating in Gill, Vardi and Wellner (1988). In econometrics the problem was first studied in discrete-choice models
[Manski and Lerman (1977)]. Further contributions are Manski and McFadden (1981),
Cosslett (1981b, 1981a), Morgenthaler and Vardi (1986), and Imbens (1992). The case
that the dependent variable Y is continuous was studied by Hausman and Wise (1981)
and Imbens and Lancaster (1996). Related problems that will be considered in this
section are case-control studies [Prentice and Pyke (1979), Breslow and Day (1980)],
contaminated samples [Hsieh, Manski and McFadden (1985), Lancaster and Imbens

25 Parametric identification suffices, but preferably the conditional distribution of Y given X should be non-

parametrically identified, and for this the strata need to be overlapping.

5528

G. Ridder and R. Moffitt

(1996)] and the combination of micro and macro data [Imbens and Lancaster (1994),
Imbens and Hellerstein (1999)].
6.2. Identification in biased samples
General results on non-parametric identification of the population cdf from combined
biased samples are given by Vardi (1985) and Gill, Vardi and Wellner (1988). Initially,
we make no distinction between dependent and independent variables. Let the population distribution of the random vector Y have cdf F . Instead of a random sample from
the population with cdf F , we have K random but biased samples from distributions
with cdf’s Gk , k = 1, . . . , K. The relation between Gk and F is given by
y
Wk (y) dF (y)
.
Gk (y) = −∞
(138)
∞
−∞ Wk (v) dF (v)
In this expression Wk is a biasing function. This function is assumed to be known and
nonnegative (it may be 0 for some values of y). An obvious interpretation of this function is that it is proportional to the probability of selection. If f is the density of F , then
the probability of observing y in the kth biased sample is proportional to Wk (y)f (y).
Because we specify the probability of selection up to a multiplicative constant we must
divide by the integral of Wk (y)f (y) to obtain a proper cdf.
It is obvious that we can only recover the population cdf for values of y where at least
one of the weight functions is positive. The region where F is identified, S, is defined
by


K


S= y
(139)
Wk (y) > 0 .
k=1

If S is a strict subset of the support of Y we can only recover the conditional cdf of Y
given Y ∈ S. For values of y with Wk (y) > 0, the population pdf can be found from
gk (y) =

Wk (y)
f (y)
Wk

(140)

with
Wk =

∞
−∞

Wk (w) dF (w).

(141)

If f satisfies (140), then so does c.f for any positive constant c. Because f is a density,
the sum or integral over its support is 1, and this restriction determines the constant.
Let g(y) be the density of a randomly selected observation from the pooled sample.
If the subsample sizes are determined by a multinomial distribution with parameters λk ,
k = 1, . . . , K, and N (the size of the pooled sample), then we have a multinomial
sampling plan.
 The density of a randomly selected observation from the pooled sample
is g(y) = K
i=1 λk gk (y). In the case that the subsample sizes are fixed, we substitute

Ch. 75:

The Econometrics of Data Combination

5529

Nk
N

for λk to obtain the density of a randomly selected observation in the pooled sample. This implies that the identification results for multinomial sampling and for fixed
subsample sizes are identical.
From (140) we solve for f as a function of g
1

f (y) = K

Wk (y)
k=1 λk Wk

(142)

g(y).

This solution does not express f in terms of observable quantities, because it depends
on the unknown Wk ’s. The Wk , k = 1, . . . , K, are determined by the following system
of equations that is obtained by multiplying (142) by Wk (y) and by integrating the
resulting expression over y
1=

1
Wk

∞
−∞

Wk (y)
g(y) dy
K
Wl (y)
l=1 λl Wl

(143)

for k = 1, . . . , K. Note that this set of equations only determines the Wk ’s up to a
multiplicative factor. To obtain a solution we choose an arbitrary subsample, e.g. subsample 1, and we set W1 = 1.
By rewriting (142) (we divide by 1), we find
1
Wk (y)
λ
k=1 k W

K

f (y) =  ∞

g(y)

k

1
Wk (v)
k=1 λk W

−∞ K

g(v) dv

.

(144)

k

k
We see that f only depends on the ratios W
W1 , k = 2, . . . , K. We can now restate the
identification problem: The population pdf f (with cdf F ) is non-parametrically identified from the K biased samples if and only if the equation system (143) and (144) has
a unique solution for f and Wk , k = 2, . . . , K (in the equations we set W1 = 1). If
desired we can recover W1 from (141) with k = 1.
We consider the solution in more detail for the case of two biased samples, i.e. K = 2.
Define the set V12 by
 

V12 = y  W1 (y)W2 (y) > 0 .
(145)

Note that if the weight functions are stratum indicators, the V12 contains all y that are
common to both strata. For all y ∈ V12
W2
g1 (y) W2 (y)
=
.
W1
g2 (y) W1 (y)

(146)

Note that the functions on the right-hand side are all known or estimable from the biased
2
samples. Hence, the ratio W
W1 is (over)identified on V12 . This ratio can be substituted in
(144) to obtain f . If the set V12 is empty, then it is not possible to identify the ratio
and f .

W2
W1

5530

G. Ridder and R. Moffitt

If K  3 we look for biased samples k, l for which the set Vkl = {y |
Wk (y)Wl (y) > 0} is not empty, i.e. for which
∞
−∞

Wk (y)Wl (y) dF (y) > 0.

(147)

The same argument as for K = 2 shows that for such a pair of subsamples k, l
we can identify the conditional distribution of Y given that Y is in the set where
Wk (y) + Wl (y) > 0. Samples for which (147) holds are called connected. Because
the result holds for all pairs k, l we can characterize the region of identification of the
population distribution. Let Km , m = 1, . . . , M, be disjoint index sets of connected
subsamples. The union of these index sets is the set of all subsamples {1, . . . , K}.
The population
 distribution is identified on the regions Sm , m = 1, . . . , M, with
Sm = {y | k∈Km Wk (y) > 0}, i.e. we can identify the conditional distributions of
Y given that Y ∈ Sm . If there is only one region of identification that coincides with the
support of Y , the population distribution is identified on its support.
Until now we did not distinguish between the dependent variable(s) Y and independent variables X. The theory developed above applies directly if biased samples from
the joint distribution of Y, X are combined. The special case that the biasing function
only depends on X has already been discussed. There are however other possibilities,
e.g. that in some subsample only Y or only X is observed. A sample from the marginal
distribution of X or Y cannot be considered as a biased sample from the joint distribution of X, Y , so that the general theory cannot be used. A simple example illustrates
this point.26
Assume that X and Y are both discrete with 2 and K values and assume that we
have random samples from strata defined by Y . The biasing functions are Wk (y, x) =
Iy=k (y, x), k = 1, . . . , K. The subsamples are not connected and we cannot identify the
joint distribution of X, Y . Now assume that we have an additional random sample from
the distribution of X. It seems that the ‘biasing’ function for this sample is Ix=1,2 (y, x)
and this additional subsample is connected with each of the other subsamples. We conclude that the joint distribution is identified. This conclusion is not correct, because the
marginal density of X satisfies by the law of total probability
fX (1) =

K


f (1 | k)fY (k).

(148)

k=1

If K = 2 we can identify the marginal distribution of Y and therefore the joint distribution of Y, X from the biased samples and the marginal distribution of X. If K > 2, there
will be observationally equivalent solutions and we cannot identify the joint distribution.
If the additional sample is from the marginal distribution of Y we can identify the joint
distribution. Note that Wk = fY (k) so that this case corresponds to prior information
26 Although Gill, Vardi and Wellner (1988) do not claim that their identification theorem applies with mar-

ginal information, they give suggestive examples, e.g. their Example 4.4.

Ch. 75:

The Econometrics of Data Combination

5531

on the Wk ’s. In general, samples from marginal distributions provide prior information
on the Wk ’s, e.g. (148) imposes as many restrictions as the number of distinct values
taken by X. Currently there is no general theory of non-parametric identification with
marginal information that is comparable to the Gill, Vardi and Wellner (1988) theory.
We now consider some examples:
Endogenous stratification First, we consider the marginal distribution of Y . Let Sk ,
k = 1, . . . , K, be a partition of the support of Y , and let Wk (y) = ISk . The population
cdf of Y is not identified, because the biased samples are not connected. If we have a
supplementary random sample from the distribution of Y , the biased samples are connected and the cdf is identified. Next, consider the conditional distribution of Y given X.
If the subpopulations partition the support of the joint distribution of Y, X, then the joint
and conditional cdf are identified with a supplementary sample from the joint distribution. This conditional cdf is in general not identified if the supplementary sample is
from the marginal distribution of Y . If the subpopulations are defined as a partition of
the support of Y , then an additional random sample from the marginal distribution of Y
suffices for identification of the joint and conditional cdf of Y, X, because the Wk can
be obtained from the marginal distribution of Y . A special case is a case-control study
in which Y is 0–1 and the strata are defined by Y .
Case-control with contaminated controls Consider the case that Y is a 0–1 variable. We combine a random sample from the subpopulation defined by Y = 1, i.e.
a random sample from the conditional distribution of X given Y = 1, with random
samples from the marginal distributions of X and Y . By the law of total probability
f (x) = f (x | y = 1) Pr(Y = 1) + f (x | y = 0)(1 − Pr(Y = 1)). The marginal distribution of Y identifies Pr(Y = 1) and combining this with the marginal distribution of
X identifies f (x | y = 0). Hence, the joint distribution of X, Y is identified. A sample
from the marginal distribution of X does not identify the joint distribution of Y, X nor
the marginal distribution of Y given X.
Non-parametric identification of the conditional distribution of Y given X is desirable, even if we assume that the conditional cdf is a member of a parametric family
F (y | x; θ ). Often, parametric assumptions identify θ from a single biased sample.
Consider
f (y, x; θ ) = f (y | x; θ )h(x) = Wk

gk (y, x)
Wk (y, x)

(149)

for all (y, x), (y  , x  ) ∈ Sk with Sk = {(y, x) | Wk (y, x) > 0} we have
f (y | x; θ )
gk (y, x) Wk (y  , x  )
=
.
f (y  | x  ; θ )
gk (y  , x  ) Wk (y, x)

(150)

For instance, if the model is a probit model with Pr(Y = 1 | x; θ ) = Φ(θ0 + θ1 x) for
a dummy dependent Y , and Wk is the indicator of the stratum Y = 1, then θ0 , θ1 are
identified from this biased sample. To see this we consider the case that x is continuous

5532

G. Ridder and R. Moffitt

and that 0 and 1 are in the support of x. Fix x  in (150) and consider the derivative with
respect to x of the logarithm of the resulting expression. Evaluating the result fot x = 0
and x = 1 gives a (nonlinear) system of two equations in θ0 , θ1 that can be solved for
these two parameters. A more comprehensive discussion of parametric identification in
choice-based samples can be found in Lancaster (1992). We do not discuss this type of
identification any further, because it should be avoided.
Nonresponse in sample surveys or attrition in panel data also results in biased samples
from the underlying population. For conditional inference, the key question is whether
the response/attrition depends on Y . Note that in this case the biasing function is in general unknown. The large literature on sample selectivity goes back to Heckman (1979).
Sample combination can be used to put restrictions on the biasing function, in this case
the probability of response. Hirano et al. (2001) consider the combination of a panel
survey with selective attrition and a refreshment sample. Manski (2003, Section 1.4),
derives bounds on the population distribution under weak assumptions on the missing
data process. This type of biased samples is beyond the scope of this survey.
6.3. Non-parametric and efficient estimation in biased samples
6.3.1. Efficient non-parametric estimation in biased samples
The efficient non-parametric estimator of the population cdf from a set of biased samples was first derived by Vardi (1985). Gill, Vardi and Wellner (1988) give a rigorous
analysis of this estimator and prove that it is asymptotically efficient.27
Vardi’s estimator is the solution to the empirical counterparts of Equations (144) and
(143). The estimator of the cdf is
y
ˆ
1
Wk (v) dG(v)
0 K
F̂ (y) = 
∞

k=1 λk Ŵ
k

1
Wk (v)
k=1 λk

−∞ K

1=

1

∞

Ŵk

−∞

ˆ
dG(v)

(151)

,

Ŵk

Wk (y)
ˆ
dG(y),
K
Wl (y)
l=1 λl

k = 2, . . . , K.

(152)

Ŵl

In these equations λk = NNk . Integration with respect to the empirical cdf is just averaging over the combined sample.
If the cdf is non-parametrically identified, then the system of K −1 equations in K −1
unknowns (152) has a unique solution. This solution is substituted in (151) to obtain the
non-parametric estimator of the cdf.

27 In the sense that its limit process has a covariance function that reaches the lower bound for all regular

estimators.

Ch. 75:

The Econometrics of Data Combination

5533
1

Gill, Vardi and Wellner (1988) show that the empirical cdf is consistent (at rate n 2 )
and asymptotically normal with a covariance function that can be easily estimated.28
In the case of endogenous stratification we have Wk (y) = ISk (y) with Sk , k =
1, . . . , K, a partition of the set of values taken by Y . To ensure identification we have
an additional random sample and we call this stratum K + 1 with WK+1 (y) = 1 for
all y. We normalize with respect to this stratum so that in (152) we have K equations in
the unknowns Ŵ1 , . . . , ŴK . They are
1=

1
Ŵk

∞
−∞

K+1

Wk (y)

Wl (y)
l=1 λl Ŵl

+ λK+1

ˆ
dG(y),

k = 1, . . . , K.

(153)

ˆ is just averaging over the comBecause integration with respect to the empirical cdf G
plete data we obtain
1=

N
1 1 
Wk (yi )
K+1 Wl (yi )
Ŵk N
λl
+ λK+1
i=1

N
1 1 
=
Ŵk N i=1 λk

l=1

Ŵl

1
1
Ŵk

+ λK+1

ISk (yi ).

(154)

If Nk , k = 1, . . . , K + 1, is the sample size in the strata, N = N1 + · · · + NK+1 ,
and N̂K+1,k is the number of observations in the random sample that is in Sk , we have
N
i=1 ISk (yi ) = Nk + N̂K+1,k
1=

Nk + N̂K+1,k
Nk + NK+1 Ŵk

,

k = 1, . . . , K,

(155)

with solution
Ŵk =

N̂K+1,k
.
NK+1

(156)

Hence the non-parametric estimator of the empirical cdf is just the sum of the empirical
cdf of the random sample and the weighted empirical cdf in the strata with weights λk ,
Ŵk
i.e. the ratio of the fraction of stratum k in the sample and population.
6.3.2. Efficient parametric estimation in endogenously stratified samples
We restrict the discussion to parametric models that specify the conditional density
f (y | x; θ ). A special case is the discrete choice model where y is a categorical
28 If the dimension of y  2 the result applies to the empirical measure that counts the number of outcomes
in a set E ⊂ M with M the dimension of y. There are restrictions on the choice of E, e.g. the orthants y  c

will do, in order to obtain uniform convergence.

5534

G. Ridder and R. Moffitt

variable. The sample space Y × X is divided into strata Sk . These strata need not be
disjoint. Indeed the analysis in Section 6.2 shows that to ensure non-parametric identification of f (y | x) the strata should be overlapping. A special case occurs if Y is
discrete and Sy = {y} × X for y = 1, . . . , M. Such a sample is called a choicebased or response-based sample. In econometrics, estimation in endogenously stratified samples was first discussed in choice-based samples [Manski and Lerman (1977),
Manski and McFadden (1981), Cosslett (1981b)]. The surprisingly simple efficient estimator in such samples was also first discovered for choice-based samples [Imbens
(1992)] and later generalized to arbitrary endogenously stratified samples [Imbens and
Lancaster (1996)]. We use a suggestion by Lancaster (1992) who showed that in choicebased samples the efficient estimator is the Conditional Maximum Likelihood (CML)
estimator, if we substitute the observed stratum fractions, even if these fractions are
specified by the sample design. This is true in any endogenously stratified sample. This
simple result is similar to the observation of Wooldridge (1999) and Hirano, Imbens
and Ridder (2003) who show that in stratified sampling the estimated or observed sample weights are preferred over the weights computed from the sampling probabilities
that are used in the sample design. In the sequel we assume that the parameters in the
conditional distribution of Y given X are identified, preferably because this conditional
distribution is non-parametrically identified.
We assume that sampling is in two stages (i) a stratum Sk is selected with probability Hk , (ii) a random draw is obtained from f (y, x | (Y, X) ∈ Sk ) which we denote
as f (y, x | Sk ). This is called multinomial sampling. In stratified sampling the number
of observations in each stratum Sk is fixed in advance. Imbens and Lancaster (1996)
show that inferences for both sampling schemes are the same, because the associated
likelihood functions are proportional. Let S be the stratum indicator that is equal to k if
the observation is in Sk .
The joint density of Y, X, S in the sample is
f (y | x; θ )f (x)
g(s, y, x) = Hs f (y, x | Sk ) = Hs
(157)
Qs
with
Qs =

Sk

f (y | x; θ )f (x) dy dx,

where we implicitly assume that Y is continuous. If not, just replace integration by
summation. Now define

 
Sk (x) = y  (y, x) ∈ Sk
and







R(k, x, θ ) = Pr (Y, X) ∈ Sk  X = x = Pr Y ∈ Sk (x)  X = x
=

Sk (x)

f (y | x; θ ) dy.

Obviously Qk = E(R(k, X, θ )).

Ch. 75:

The Econometrics of Data Combination

5535

The marginal density of X in the sample is obtained from (157) by integration with
respect to y over Sk (x) (which may be an empty set for some x and k) and summation
over k
g(x) = f (x)

K

Hk
R(k, x, θ ).
Qk
k=1

The sample density of X depends on the parameters θ . In endogenously stratified samples this distribution contains information on X. The conditional density of S, Y given
X in the sample is
Hs
f (y | x; θ ) Q
s
g(s, y | x) = K H
.
k
k=1 Qk R(k, x, θ )

(158)

An obvious method to obtain an efficient estimator of θ is by maximizing the likelihood function based on (157)
ln L(θ) =

N


ln g(si , yi , xi ) =

i=1

N


ln f (yi | xi ; θ )f (xi ) + ln

i=1

Hs i
.
Qsi

This likelihood requires the evaluation of Qk that depends on θ and also on the marginal population density of X, f (x). This is computationally unattractive, and worse it
requires the specification of the density of the independent variables.
For that reason we consider an alternative method to obtain the MLE. This method
consists of three steps. First, we assume that the distribution of X is discrete with L
points of support, i.e.
Pr(X = xl ) = f (xl ) = πl ,

l = 1, . . . , L.

Next, we reparameterize from the discrete distribution of X in the population πl to
its discrete distribution in the sample λl . The stratum probabilities Qk can also be expressed in λl . After this reparametrization the log likelihood is the sum of the conditional
loglikelihood and the marginal loglikelihood of the observations on X. The first factor
depends on λl only through the stratum probabilities Qk .
The third step is that, if we maximize the conditional loglikelihood with respect to
H1 , . . . , HK and evaluate the first-order conditions at the MLE of these ‘parameters’,
the restrictions on the stratum probabilities Qk are satisfied. Hence maximizing the conditional loglikelihood with respect to θ and H1 , . . . , HK is equivalent to maximization
of the sample loglikelihood with respect to θ. This conclusion does not depend on the
assumption that X has a discrete distribution. Following Chamberlain (1987) we conclude that the CMLE is efficient. Note that this is true if we replace the multinomial
sampling probabilities Hk in the conditional loglikelihood by their sample values NNk
with Nk the number of observations in stratum k. The CMLE is not efficient if we use
the probabilities Hk that were actually used in the multinomial sampling.

5536

G. Ridder and R. Moffitt

The discrete distribution of X in the sample is

 K
 Hk
R(k, xl , θ) .
g(xl ) = λl = πl
Qk
k=1

Hence
Qk =

L


R(k, xl , θ)πl =

L


l=1

K

R(k, xl , θ)

λl
Hm
m=1 Qm R(m, xl , θ)

l=1

which can be written as a sample average
1=

N
R(k, xi , θ) Q1k
1 
.
 K Hm
N
R(m, xi , θ)
m=1

(159)

Qm

i=1

The conditional loglikelihood is
ln Lc (θ ) =

N


ln f (yi | xi ; θ ) −

i=1

N

i=1

K

K
 Hk

Hk
ln
R(k, xi , θ) +
Nk ln
.
Qk
Qk
k=1

k=1

The first-order condition for Hk is
N

R(k, xi , θ) Q1k
Nk
=
.
K Hm
Hk
R(m, xi , θ)
m=1
i=1

Qm

If we substitute the MLE Ĥk = NNk in this equation and in (159) we see that they are
identical and we conclude that the restrictions for Qk are satisfied at the MLE (but not
if we substitute Hk ).
Note that if (159)
 holds for all k = 1, . . . , K, multiplication by Hk and summation
over k gives that L
l=1 λl = 1. Again this condition is satisfied if the first-order conditions for maximization of the conditional loglikelihood with respect to H1 , . . . , HK are
evaluated at the MLE of these ‘parameters’.
Hence the efficient estimator of θ is found by maximizing the conditional loglikelihood with respect to θ and H1 , . . . , HK . The first order conditions are evaluated at the
MLE of H1 , . . . , HK and solved for θ and Q1 , . . . , QK . These first-order conditions set
the sample average of the following functions equal to 0
K
| x; θ )
k=1
− 
m1 (s, y, x; θ, Q) =
K
f (y | x; θ )
∂
∂θ f (y

Ĥk ∂
Qk ∂θ R(k, x, θ )

Ĥk
k=1 Qk R(k, x, θ )

m2k (s, y, x; θ, Q) = Qk − 
K

R(k, x, θ )

Ĥm
m=1 Qm R(m, x, θ)

,

,

(160)

(161)

Ch. 75:

The Econometrics of Data Combination

5537

for k = 1, . . . , K and Ĥk = NNk . Hence the efficient estimator is a GMM estimator
that satisfies moment conditions based on these moment functions. An additional moment function that gives Ĥk can be added, but the corresponding moment condition is
independent of the other moment conditions. Hence we can treat the Ĥk as given.
The variance of the efficient estimator can be found by the usual GMM formula. The
GMM formulation is convenient if we add additional sample information. This is just
another moment condition.
6.3.3. Efficient parametric estimation with marginal information
Random sample with marginal information First we consider the case that a random
sample Yi , Xi , i = 1, . . . , N, is combined with marginal information. The marginal information consists of moments E(h(Y, X)) = h with h a known function of dimension
K and h an K vector of constants. The expectation is over the population distribution
of X, Y . Hence we combine information in two random samples, one of which comprises the whole population. Although these random samples cannot be independent,
we can think of this as the combination of a relatively small random sample with a very
large one. The sampling variance in the second sample is negligible. This is the setup
considered by Imbens and Lancaster (1994).
Without loss of generality we set h equal to 0. The goal is to estimate the parameter vector θ in the conditional distribution of Y given X with conditional density
f (y | x; θ ). Because we have a random sample, identification is not an issue. However,
the additional moments overindentify the parameters, and these additional moment restrictions increase the precision of the estimation or can be used to create more powerful
specification tests.
The score vector is
m1 (y, x; θ ) =

∂ ln f (y | x; θ )
.
∂θ

(162)

Of course setting the sample average of the score equal to 0 gives the MLE that is an
efficient estimator without additional information. The additional information can be
expressed as


E h(Y, X) =

h(y, x)f (y | x; θ ) dy g(x) dx = 0.

(163)

This gives a restriction on θ. The efficient estimator that uses this restriction is the
restricted MLE that is obtained by maximizing the loglikelihood subject the constraint
in (163).
The implementation of the restricted MLE requires the specification of the marginal
density of X. Applied researchers are usually unwilling to make parametric assumptions
on this marginal distribution, and for that reason it is convenient that such a specification

5538

G. Ridder and R. Moffitt

is not needed. Rewrite (163) as an average over the sample
N
1 
N

h(y, Xi )f (y | Xi ; θ ) dy =

i=1

N
1 
m2 (Yi , Xi ; θ ).
N

(164)

i=1

Imbens and Lancaster (1994) show that the optimal GMM estimator with weight matrix
equal to the inverse of the variance matrix of the moment restrictions has an asymptotic
variance that is equal to that of the restricted MLE.29
Their simulation study and empirical example show that the efficiency gains can
be substantial. The precision of the estimator of the regression coefficient of Xj increases if the marginal information is the joint population distribution of grouped Y and
grouped Xk . For instance, if Y is the employment indicator and Xj is age, the joint
population distribution of employment status and age category (but no other variable) is
highly informative on the age coefficient in an employment probit or logit. If the model
has no interactions the pairwise population joint distributions of the dependent and the
independent variables reduce the variances of the regression coefficients. Also in the
case of a dummy dependent variable the marginal information is very useful if one of
the outcomes is rare.
The additional moments (164) involve an integral over y (if Y is continuous). If one
wants to avoid this integral one would be tempted to use the additional moment
N
N
1 
1 
h(Yi , Xi ) =
m3 (Yi , Xi ; θ )
N
N
i=1

(165)

i=1

instead of (164). The resulting GMM estimator is less efficient than the restricted MLE.
This can be seen if one considers the case without covariates X and a scalar h and θ . In
that case the moment condition in (164) restricts the parameter to its population value,
while the moment condition in (165) does not remove the sampling variation in the
restricted MLE. To achieve efficiency one should use (164) as the second set of moment
conditions.
In the case that the conditional density is not specified, the moment conditions in
(164) are not available and one if forced to use (165) together with the moment conditions based on m(y, x; θ ) that is a vector of moment conditions that identifies θ and
could be used to estimate the parameters if one only had the random sample from the
population. The moment conditions (164) do not depend on θ , but because they are correlated with the moment conditions in (164). Hence imposing them along with (164)
improves the precision of the estimators.
As noted the additional moments can be used for an often powerful test of the parametric model f (y | x; θ ). The obvious test is the GMM overidentification test based
on the moment conditions (162) and (164). The test statistic is the minimal value of the
optimal GMM minimand and it has under the null hypothesis of correct specification,
29 An alternative definition is the restricted MLE with (164) as the restriction.

Ch. 75:

The Econometrics of Data Combination

5539

a chi-squared distribution with K (dimension of h) degrees of freedom. It should be
noted that the test also rejects if the random sample is not from the same population that
is used to compute E(h(Y, X)). To deal with this one could consider a joint test based
on the moment conditions (162), (164) and (165) that tests both for the compatibility of
the information and the specification. This test statistic has 2K degrees of freedom.
Van den Berg and van der Klaauw (2001) consider the estimation of a model for
unemployment durations where the aggregate information consists of unemployment
rates. Their approach is a direct application of the restricted MLE with the additional
complication that they allow for measurement error in the aggregate data.
Biased samples with marginal information Imbens and Hellerstein (1999) show30 that
the optimal GMM estimator, based on (162) and (165), i.e. we consider the case that
the conditional density of Y given X is not specified, but θ is estimated from a set of
moment conditions, is equivalent to a weighted GMM estimator that solves
N


wi m1 (Yi , Xi ; θ ) = 0

(166)

i=1

with weights wi , i = 1, . . . , N , defined as the solution to
max

N


ln wi

s.t.

i=1

N


wi = 1,

i=1

N


wi h(Yi , Xi ) = h.

(167)

i=1

The weights are equal to
wi = w(Yi , Xi ) =

1
N (1 + λ̂ h(Yi , Xi ))

(168)

with λ̂ the Lagrange multiplier on the second restriction. It is the solution to
N
1 
h(Yi , Xi )
= 0.
N
1 + λ̂ h(Yi , Xi )
i=1

(169)

Now consider the case that a biased sample is combined with marginal information
from the population. As an illustration we consider the example of a 0–1 dependent
variable with conditional density f (y | x; θ ) = G(x  θ )y (1 − G(x  θ ))1−y . The endogenously stratified sample has strata S1 = 1×X and S2 = 0×X with X the support of X.
The multinomial sampling probabilities are H1 , H2 and the population fractions of the
two strata are Q1 , Q2 . Also h(y, x) = y − Q1 . In large samples λ̂ in (169) converges
to the solution to the equation that is obtained by replacing the sample average in (169)

30 To be precise, they only consider linear regression with additional moment restrictions, but their argument

applies generally.

5540

G. Ridder and R. Moffitt

by the corresponding expectation over the sample distribution
1

y=0

y − Q1
H1
G(x  θ )
1 + λ(y − Q1 ) Q1

y


H2 
1 − G(x  θ )
Q2

1−y

g(x) dx = 0.
(170)

The solution is
H1 − Q1
λ=
Q1 Q2

(171)

so that the weights that depend on the value of y only are
w(y, x) =

1
N 1+

1
H1 −Q1
Q1 Q2 (y

− Q1 )

=

1
N

Q1
H1

y

Q2
H2

1−y

(172)

.

These weights are used in the score based on the full sample to obtain the weighted
likelihood equation
N

i=1

w(Yi , Xi ) Yi

∂ ln(1 − G(Xi θ ))
∂ ln G(Xi θ )
+ (1 − Yi )
∂θ
∂θ

= 0.

(173)

This corresponds to the Weighted Exogenous Sampling MLE of Manski and Lerman
(1977). This estimator is not fully efficient because it does not use the parametric model
in the additional moment condition.
We conclude that if the additional population moments combined with the biased
sample identify the population parameters, then the weighted estimator proposed by
Imbens and Hellerstein (1999) is the efficient GMM that imposes the population moments. If the conditional density is specified, the estimator is not fully efficient. Hence
their weighted estimator provides a constructive method to combine biased samples
with population moments. Devereux and Tripathi (2004) consider the combination of
sample in which some of the variables in Y, X are censored or truncated with a sample
in which all these variables or fully observed. They show that the efficient GMM estimator is a weighted GMM estimator. For instance, in the case of Y censored at C the
I (Y =C)
weights are w = p+(1−p)I
(Y <C) with p the fraction of the combined sample with fully
observed Y . The assumption that in one of the samples all variables are fully observed
is restrictive.
If the combination of the biased sample(s) and the population moment does not identify the population parameters, the weighted GMM estimator converges to the solution
of
fs (y, x)
dy dx
m1 (y, x; θ )
(174)
1 + λ h(y, x)
with λ the solution of (169) if we replace the (biased) sample average by the (biased)
sample expected value. Hence the GMM estimator is consistent for the parameters in a
distribution that satisfies the population moments and is also consistent with the biased

Ch. 75:

The Econometrics of Data Combination

5541

sample. It is obtained from the distribution in the biased sample by weighting, which
is the general approach (see Section 6.3.2). The weights reproduce the population distribution if the parameters are identified. If not, they produce a GMM estimate that is
consistent with the available information. However, in that case the weight (and hence
the GMM estimator) are not unique. In the optimization problem (167) we can replace
ln wi by K(wi ) with K any concave function. This reflects the fact that the parameters
are not point identified.

Appendix A
T HEOREM 1. If assumptions (A1)–(A3) hold, then the 2SIV estimator is weakly consistent.
P ROOF. We have by adding and subtracting mN (θ0 )




mN (θ ) WN mN (θ ) = mN (θ ) − mN (θ0 ) WN mN (θ ) − mN (θ0 )


+ 2mN (θ0 ) WN mN (θ ) − mN (θ0 )
+ mN (θ0 ) WN mN (θ0 ).

(175)

By the mean value theorem
∂mN
(θ∗ )(θ − θ0 )
(176)
∂θ 
with θ∗ between θ and θ0 . Substitution in (175) and taking the limit N1 , N2 → ∞ gives
 



∂m
∂m

(θ − θ0 ) E
(θ∗ ) (θ − θ0 )
(θ∗ ) W E
∂θ
∂θ 








∂m
(θ∗ ) (θ − θ0 ) + E m(θ0 ) W E m(θ0 )
+ 2E m(θ0 ) W E
(177)
∂θ 
mN (θ ) = mN (θ0 ) +

and this limit is attained uniformly in θ. If (A1) holds, then E(m(θ0 )) = 0, so that the

last two terms on the right-hand side are equal to 0. Because E[ ∂m
∂θ (θ )] is continuous in
θ this matrix has full rank in a neighborhood of θ0 . In that neighborhood θ0 is the unique
minimizer. By Van der Vaart (1998, Theorem 5.7), this implies that the 2SIV estimator
converges in probability to θ0 .

T HEOREM 2. If assumptions (A1)–(A4) hold, then



d
N2 (θ̂N − θ0 ) −
→
N 0, V (θ0 )
with



∂m
∂m
(θ0 )
(θ0 ) W (θ0 )E
V (θ0 ) = E
∂θ
∂θ 

−1

(178)

5542

G. Ridder and R. Moffitt




∂m
(θ0 ) W (θ0 ) λ Var m1j (θ0 )
∂θ


∂m
(θ0 )
+ Var m2i (θ0 ) W (θ0 )E
∂θ 

−1
∂m
∂m
· E
(θ
)
.
(θ0 ) W (θ0 )E
0
∂θ
∂θ 

·E

(179)

P ROOF. The first-order conditions give
0=


∂mN
(θ̂N )WN N2 mN (θ̂N ).
∂θ

(180)

By the mean value theorem we have for some θ N between θ0 and θ̂N



∂mN
N2 mN (θ̂N ) = N2 mN (θ0 ) +
(θ
)
N2 (θ̂N − θ0 ).
N
∂θ 
√
Substitution in (180) and solving for N2 (θ̂N − θ0 ) gives

N2 (θ̂N − θ0 )
 
−1 

∂mN
∂mN
∂mN
=−
(θ
)
)W
N2 mN (θ0 ).
(θ̂N )WN
(
θ̂
N
N
N
∂θ
∂θ 
∂θ

(181)

(182)

N
The proof is completed by noting that ∂m
∂θ (θ ) is continuous in θ , and by using the
central
limit theorem for i.i.d. random variables to obtain the asymptotic distribution of
√
N2 mN (θ0 ).


d
→
χ 2 (dim(m) − dim(θ )).
T HEOREM 3. If (A1)–(A4) hold, then TN −

P ROOF. Substitution of (182) in (181) gives

N2 mN (θ̂N )

 
−1 

∂mN
∂mN
∂mN
∂mN
= I−
(θ
)
)W
(θ
)
)W
N2 mN (θ0 ).
(
θ̂
(
θ̂
N
N
N
N
N
N
∂θ 
∂θ
∂θ 
∂θ
(183)
∂m

Using the notation A(θ ) = ∂θN (θ ) and the assumption that this matrix is continuous
in θ, we have


−1


N2 mN (θ̂N ) = I − A(θ0 ) A(θ0 )W A(θ0 ) A(θ0 )W N2 mN (θ0 ) + op (1).
(184)
Upon substitution of (184) in (94)



−1

TN = N2 mN (θ0 ) I − W  A(θ0 ) A(θ0 )W A(θ0 ) A(θ0 ) W

Ch. 75:

The Econometrics of Data Combination

5543


−1


· I − A(θ0 ) A(θ0 )W A(θ0 ) A(θ0 )W N2 mN (θ0 ) + op (1)



−1

= N2 mN (θ0 ) W − W  A(θ0 ) A(θ0 )W A(θ0 ) A(θ0 )W N2 mN (θ0 )
+ op (1).

(185)
− 12

− 12

1

If W = M(θ0 )−1 , we can find a matrix M(θ0 ) with M(θ0 )−1 = M(θ0 ) M(θ0 )− 2 .
Then

1
TN = N2 mN (θ0 ) M(θ0 )− 2


−1
1
1
· I − M(θ0 )− 2 A(θ0 ) A(θ0 )M(θ0 )−1 A(θ0 ) A(θ0 )M(θ0 )− 2
1
· M(θ0 )− 2 N2 mN (θ0 ) + op (1).
(186)
√
1 d
Because N2 mN (θ0 ) M(θ0 )− 2 −
→ N (0, I ) and the matrix between [.] is idempotent

with rank equal to dim(mN ) − dim(θ ), the result follows.

References
Alessie, R., Devereux, M., Weber, G. (1997). “Intertemporal consumption, durables and liquidity constraints:
A cohort analysis”. European Economic Review 41, 37–59.
Amemiya, T. (1978). “The estimation of a simultaneous-equation generalized probit model”. Econometrica 46
(5), 1193–1205.
Amemiya, T. (1985). Advanced Econometrics. Harvard University Press, Cambridge, MA.
Angrist, J.D., Krueger, A.B. (1992). “The effect of age at school entry on educational attainment: An application of instrumental variables with moments from two samples”. Journal of the American Statistical
Association 87, 328–336.
Arellano, M., Honoré, B. (2001). “Panel data models: Some recent developments”. In: Heckman, J., Leamer,
E. (Eds.), Handbook of Econometrics, vol. 5. Elsevier, Amsterdam.
Arellano, M., Meghir, C. (1992). “Female labour supply and on-the-job-search: An empirical model estimated
using complementary data sets”. Review of Economic Studies 59, 537–557.
Attanasio, O. (1998). “A cohort analysis of saving behavior by US households”. Journal of Human Resources 33, 575–609.
Attanasio, O., Weber, G. (1994). “The UK consumption boom of the late 1980’s: Aggregate implications of
microeconometric evidence”. Economic Journal 104, 1269–1302.
Barr, R.S., Turner, J.S. (1978). “A new, linear programming approach to microdata file merging”. In: Compendium of Tax Research, 1978 ed. Office of Tax Analysis, Department of the Treasury, Washington, DC,
pp. 131–155.
Belin, T.R., Rubin, D.B. (1995). “A method for calibrating false-match rates in record linkage”. Journal of the
American Statistical Association 90, 694–707.
Blow, L., Preston, I. (2002). “Deadweight loss and taxation of earned income: Evidence from tax records of
the UK self-employed”. IFS Working Paper 15. London.
Blundell, R., Browning, M., Meghir, C. (1994). “Consumer demand and the lifecycle allocation of household
expenditures”. Review of Economic Studies 61, 57–80.
Blundell, R., Smith, R. (1993). “Simultaneous microeconometric models with censored and qualitative dependent variables”. In: Maddala, G.S., Rao, C.R., Vinod, H.D. (Eds.), Handbook of Statistics: Econometrics,
vol. 11. Elsevier, Amsterdam.
Box, G.E.P., Cox, D.R. (1964). “An analysis of transformations”. Journal of the Royal Statistical Society
B 26, 211–252.

5544

G. Ridder and R. Moffitt

Breslow, N.E., Day, N.E. (1980). Statistical Methods on Cancer Research, Volume 1: Case-control Studies.
International Agency for Cancer Research, Lyon.
Browning, M., Deaton, A., Irish, M. (1985). “A profitable approach to labor supply and commodity demands
over the life cycle”. Econometrica 53, 503–544.
Buehler, J.W., Prager, K., Hogue, C.J., Shamsuddin, K., Lieberman, E., Young, T.K., Kliewer, E., Blanchard,
O.J., Mayer, T. (2000). “The role of linked birth and infant death certificates in maternal and child health
epidemiology in the United States”. American Journal of Preventive Medicine 19, 3–11.
Burbidge, J.B., Magee, L., Robb, A.L. (1988). “Alternative transformations to handle extreme values of the
dependent variable”. Journal of the American Statistical Association 83, 123–127.
Card, D., Hildreth, A.K.G., Shore-Sheppard L.D. (2001). “The measurement of Medicaid coverage in the
SIPP: Evidence from California, 1990–1996”. Working Paper. NBER 8514.
Carroll, C.D., Weil, D.N. (1994). “Saving and growth: A reinterpretation”. Carnegie–Rochester Conference
Series on Public Policy 40, 133–191.
Carroll, C.D., Dynan, K.E., Krane, S.D. (1999). “Unemployment risk and precautionary wealth: Evidence
from household’s balance sheetd”. Finance and Economics Discussion Series, 1999-15. Federal Reserve
Board, Washington, DC.
Chamberlain, G. (1982). “Multivariate regression models for panel data”. Journal of Econometrics 18, 5–46.
Chamberlain, G. (1987). “Asymptotic efficiency in estimation with conditional moment restrictions”. Econometrica 34, 305–334.
Chen, X., Hong, H., Tamer, E. (2005). “Measurement error models with auxiliary data”. Review of Economic
Studies 72, 343–366.
Chen, X., Hong, H., Tarozzi, A. (2004). “Semiparametric efficiency in GMM models of nonclassical measurement error, missing data and treatment effects”.
Collado, L. (1997). “Estimating dynamic models from time series of independent cross-sections”. Journal of
the Econometrics 82, 37–62.
Copas, J.B., Hilton , F.J. (1990). “Record linkage: Statistical methods for matching computer records”. Journal
of the Royal Statistical Society A 153, 287–320.
Cosslett, S.R. (1981a). “Efficient estimation of discrete choice models”. In: Manski, C.F., McFadden, D.
(Eds.), Structural Analysis of Discrete Data. MIT Press, Cambridge, MA.
Cosslett, S.R. (1981b). “Maximum likelihood estimator for choice-based samples”. Econometrica 49 (5),
1289–1316.
Cox, D.R. (1969). “Some sampling problems in technology”. In: Johnson, N.L., Smith Jr., H. (Eds.), New
Developments in Survey Sampling. Wiley–Interscience, New York, pp. 506–527.
Cross, P.J., Manski, C.F. (2002). “Regressions, short and long”. Econometrica 70, 357–368.
Currie, J., Yelowitz, A. (2000). “Are public housing projects good for kids?”. Journal of Public Economics 75,
99–124.
Deaton, A. (1985). “Panel data from time series of cross sections”. Journal of Econometrics 30, 109–126.
Dee, T.S., Evans, W.N. (2003). “Teen drinking and educational attainment: Evidence from Two-Sample Instrumental Variables (TSIV) estimates”. Journal of Labor Economics 21, 178–209.
DeGroot, M.H., Goel, P.K. (1976). “The matching problem for multivariate normal data”. Sankhya 38, 14–29.
DeGroot, M.H., Goel, P.K. (1980). “Estimation of the correlation coefficient from a broken random sample”.
Annals of Statistics 8, 264–278.
DeGroot, M.H., Feder, P.I., Goel, P.K. (1971). “Matchmaking”. Annals of Mathematical Statistics 42, 578–
593.
Devereux, P.J. (2003). “Small sample bias in synthetic cohort models of labor supply”. Mimeo.
Devereux, P.J., Tripathi, G. (2004). “Combining datasets to overcome selection caused by censoring and
truncation in moment based models”. Mimeo. UCLA.
Donald, S., Newey, W. (2001). “Choosing the number of instruments”. Econometrica 69, 1161–1191.
Fair, M., Cyr, M., Wen, S.W., Guyon, G., MacDonald, R.C., Buehler, J.W., Prager, K., Hogue, C.J., Shamsuddin, K., Lieberman, E., Young, T.K., Kliewer, E., Blanchard, O.J., Mayer, T. (2000). “An assessment
of the validity of a computer system for probabilistic record linkage of birth and death records in Canada.
The fetal and infant health study group”. Chronic Diseases in Canada 21, 8–13.

Ch. 75:

The Econometrics of Data Combination

5545

Fellegi, I.P. (1999). “Record linkage and public policy”. In: Record Linkage Techniques-1997. National Academy Press, Washington, DC, pp. 3–12.
Fellegi, I.P., Sunter, A.B. (1969). “A theory of record linkage”. Journal of the American Statistical Association 64, 1183–1210.
Fréchet, M. (1951). “Sur les tableaux de correlation dont les marges sont données”. Annales de Université,
Lyons Sect. A 14, 53–77.
Gill, R.D., Vardi, Y., Wellner, J.A. (1988). “Large sample theory of empirical distributions in biased sampling
models”. Annals of Statistics 18, 1069–1112.
Goodman, L. (1953). “Ecological regressions and behavior of individuals”. American Sociological Review 18, 663–664.
Güell, M., Hu, L. (2003) “Estimating the probability of leaving unemployment using uncompleted spells from
repeated cross-section data”. Working Paper 473. Industrial Relations Section, Princeton.
Hajek, J., Sidak, Z. (1967). Theory of Rank Tests. Academic Press, New York.
Hansen, L.P. (1982). “Large sample properties of generalized method of moments estimators”. Econometrica 50, 1029–1054.
Hausman, J., Wise, D. (1981). “Stratification on endogenous variables and estimation: The Gary income
maintenance experiment”. In: Manski, C.F., McFadden, D. (Eds.), Structural Analysis of Discrete Data.
MIT Press, Cambridge, MA, pp. 364–391.
Heckman, J.J. (1978). “Dummy endogenous variables in a simultaneous equation system”. Econometrica 46
(6), 931–959.
Heckman, J.J. (1979). “Sample selection bias as a specification error”. Econometrica 47, 153–161.
Heckman, J.J., Robb, R. (1985). “Alternative methods for evaluating the impact of interventions”. In: Heckman, J., Singer, B. (Eds.), Longitudinal Analysis of the Labor Market. Cambridge University Press,
Cambridge.
Hirano, K., Imbens, G., Ridder, G. (2003). “Efficient estimation of average treatment effects using the estimated propensity score”. Econometrica 71, 1161–1189.
Hirano, K., Imbens, G.W., Ridder, G., Rubin, D.B. (2001). “Combining panel data with attrition and refreshment samples”. Econometrica 69, 1645–1659.
Horowitz, J., Manski, C.F. (1995). “Identification and robustness with contaminated and corrupted data”.
Econometrica 63, 281–302.
Horvitz, D.G., Thompson, D.J. (1952). “A generalization of sampling without replacement from a finite universe”. Journal of the American Statistical Association 47, 663–685.
Hsieh, D.A., Manski, C.F., McFadden, D. (1985). “Estimation of response probabilities from augmented
retrospective observations”. Journal of the American Statistical Association 80, 651–662.
Hu, Y., Ridder, G. (2003). “Estimation of nonlinear models with measurement errors using marginal information”. Working Paper. CLEO, University of Southern California.
Ichimura, H., Martinez-Sanchis, E. (2005). “Identification and estimation of GMM models by a combination
of two data sets”. Mimeo. University College London.
Imbens, G.W. (1992). “An efficient method of moments estimator for discrete choice models with choicebased sampling”. Econometrica 60, 1187–1214.
Imbens, G.W., Hellerstein, J. (1999). “Imposing moment restrictions from auxiliary data by weighting”. Review of Economics and Statistics 81, 1–14.
Imbens, G.W., Lancaster, T. (1994). “Combining micro and macro data in microeconometric models”. Review
of Economic Studies 61, 655–680.
Imbens, G.W., Lancaster, T. (1996). “Efficient estimation and stratified sampling”. Journal of Econometrics 74, 289–318.
Imbens G.W., Newey, W.K., Ridder, G. (2004). “Mean-squared-error calculations for Average Treatment
Effects”. Mimeo.
Kadane, J.B. (1978). “Some problems in merging data files”. In: Compendium of Tax Research, 1978 ed.
Office of Tax Analysis, Department of the Treasury, Washington, DC, pp. 159–179.
King, G. (1997). A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior from
Aggregate Data. Princeton University Press, Princeton.

5546

G. Ridder and R. Moffitt

Klevmarken, W.A. (1982). “Missing variables and two-stage least-squares estimation from more than one
data set”. In: 1981 Proceedings of the American Statistical Association, Business and Economic Statistics
Section. Pp. 156–161.
Lancaster, A.D. (1992). “A survey of inference in choice-based samples”. Working Paper. Department of
Economics, Brown University.
Lancaster, A.D., Imbens, G.W. (1996). “Case-control studies with contaminated controls”. Journal of Econometrics 71, 145–160.
Lee, T., Judge, G., Zellner, A. (1970). Estimating the Parameters of the Markov Probability Model from
Aggregate Time Series Data. North-Holland, Amsterdam.
Lusardi, A. (1996). “Permanent income, current income, and consumption: Evidence from two panel data
sets”. Journal of Business and Economic Statistics 14, 81–90.
MacRae, E.C. (1977). “Estimation of time-varying Markov processes with aggregate data”. Econometrica 45,
183–198.
Madansky, A. (1959). “Least squares estimation in finite Markov processes”. Psychometrika 17, 149–167.
Manski, C. (2003). Partial Identification of Probability Distributions. Springer-Verlag, New York.
Manski, C.F., Lerman, S. (1977). “The estimation of choice probabilities from choice based samples”. Econometrica 45, 1977–1988.
Manski, C.F., McFadden, D. (1981). “Alternative estimators and sample designs for discrete choice data”. In:
Manski, C.F., McFadden, D. (Eds.), Structural Analysis of Discrete Data. MIT Press, Cambridge, MA.
McKenzie, D. (2004). “Asymptotic theory for heterogeneous dynamic pseudo-panels”. Journal of Econometrics 120, 235–262.
Miller, G. (1952). “Finite Markov processes in psychology”. Psychometrika 24, 137–144.
Moffitt, R. (1993). “Identification and estimation of dynamic models with a time series of repeated cross
sections”. Journal of Econometrics 59, 99–123.
Morgenthaler, S., Vardi, Y. (1986). “Choice-based samples: A non-parametric approach”. Journal of Econometrics 32, 109–125.
Nelson, F., Olsen, L. (1978). “Specification and estimation of a simultaneous equation model with limited
dependent variables”. International Economic Review 19, 695–705.
Neter, J., Maynes, E.S., Ramanathan, R. (1965). “The effect of mismatching on the measurement of response
errors”. Journal of the American Statistical Association 60, 1005–1027.
Newcombe, H.B. (1988). Handbook of Record Linkage: Methods for Health and Statistical Studies, Administration and Business. Oxford University Press, Oxford.
Newcombe, H.B., Fair, M.E., LaLonde, P. (1992). “The use of names for linking personal records”. Journal
of the American Statistical Association 87, 1193–1208.
Newcombe, H.B., Kennedy, J.M., Axford, S.J., James, A.P. (1959). “Automatic linkage of vital records”.
Science 130, 954–959.
Newey, W.K., Powell, J.L. (2003). “Instrumental variables estimation for non-parametric models”. Econometrica 71, 1565–1578.
Okner, B.A. (1972). “Constructing a new data base from existing microdata sets: The 1966 merge file”. Annals
of Economic and Social Measurement 1, 325–362.
Pagan, A. (1984). “Econometric issues in the analysis of regressions with generated regressors”. International
Economic Review 25, 221–247.
Paxson, C., Waldfogel, J. (2002). “Work, welfare and child maltreatment”. Journal of Labor Economics 20,
435–474.
Pelzer, B., Eisinga, R., Franses, P.H. (2002). “Inferring transition probabilities from repeated cross sections”.
Political Analysis 18, 113–133.
Pelzer, B., Eisinga, R., Franses, P.H. (2004). “Ecological panel inference from repeated cross sections”. In:
King, G., Rosen, O., Tanner, M. (Eds.), Ecological Inference. New Methodological Strategies. Cambridge
University Press, Cambridge.
Perrachi, F., Welch, F. (1994). “Trends in labor force transitions of older men and women”. Journal of Labor
Economics 12, 210–242.

Ch. 75:

The Econometrics of Data Combination

5547

Prentice, R., Pyke, R. (1979). “Logistic disease incidence models and case-control studies”. Biometrika 66,
403–411.
Radner, D.B. (1974). “The statistical matching of microdata sets: The Bureau of Economic Analysis 1964
Current Population Survey-Tax model match”. PhD Thesis. Department of Economics, Yale University.
Radner, D.B., Allen, R., Gonzalez, M.E., Jabine, T.B., Muller, H.J. (1980). “Report on exact and statistical
matching techniques”. Statistical Policy Working Paper no. 5. US Department of Commerce, Washington
DC.
Raessler, S. (2002). Statistical Matching: A Frequentist Theory, Practical Applications, and Alternative
Bayesian Approaches. Springer, New York.
Rivers, D., Vuong, Q. (1988). “Limited information estimators and exogeneity tests for simultaneous probit
models”. Journal of Econometrics 39, 347–366.
Rodgers, W.L. (1984). “An evaluation of statistical matching”. Journal of Business and Economic Statistics 2,
91–102.
Rodgers, W., DeVol, E. (1982). “An evaluation of statistical matching”. In: 1981 Proceedings of the American
Statistical Association, Section on Survey Research Methods. Pp. 128–132.
Rubin, D.B. (1986). “Statistical matching using file concatenation with adjusted weights and multiple imputations”. Journal of Business and Economic Statistics 4, 87–94.
Ruggles, N., Ruggles, R. (1974). “A strategy for merging and matching microdata sets”. Annals of Economic
and Social Measurement 3, 353–371.
Ruggles, N., Ruggles, R., Wolff, E. (1977). “Merging microdata: Rationale, practice, and testing”. Annals of
Economic and Social Measurement 6, 407–429.
Scheuren, F., Winkler, W.E. (1993). “Regression analysis of data files that are computer matched”. Survey
Methodology 19, 39–58.
Sevestre, P., Trognon, A. (1996). “Dynamic linear models”. In: Mátyás, L., Sevestre, P. (Eds.), The Econometrics of Panel Data: A Handbook of the Theory with Applications, second ed. Kluwer, Dordrecht.
Sims, C.A. (1972). “Comments”. Annals of Economic and Social Measurement 1, 343–345.
Smith, R., Blundell, R. (1986). “An exogeneity test for a simultaneous equation Tobit model with an application to labor supply”. Econometrica 54, 679–685.
Tepping, B.J. (1968). “A model for optimal linkage of records”. Journal of the American Statistical Association 63, 1321–1332.
Van den Berg, G.J., van der Klaauw, B. (2001). “Combining micro and macro unemployment duration data”.
Journal of Econometrics 102, 271–309.
Van der Vaart, A. (1998). Asymptotic Statistics. Cambridge University Press, Cambridge, UK.
Vardi, Y. (1982). “Non-parametric estimation in the presence of length bias”. Annals of Statistics 10, 616–620.
Vardi, Y. (1985). “Empirical distributions in selection bias models”. Annals of Statistics 13, 178–203.
Verbeek, M. (1996). “Pseudo panel data”. In: Matyas, L., Sevestre, P. (Eds.), The Econometrics of Panel Data.
Kluwer.
Verbeek, M., Nijman, T. (1992). “Can cohort data be treated as genuine panel data?”. Empirical Economics 17,
9–23.
Verbeek, M., Nijman, T. (1993). “Minimum MSE estimation of a regression model with fixed effects from a
series of cross sections”. Journal of Econometrics 59, 125–136.
Wald, A. (1940). “The fitting of straight lines if both variables are subject to error”. Annals of Mathematical
Statistics 11, 284–300.
Wooldridge, J. (1999). “Asymptotic properties of weighted M-estimators for variable probability samples”.
Econometrica 67, 1385–1406.

