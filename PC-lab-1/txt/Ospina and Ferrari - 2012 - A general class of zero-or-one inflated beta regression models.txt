A general class of zero-or-one inflated beta regression models

arXiv:1103.2372v3 [stat.ME] 2 Nov 2011

Raydonal Ospina
Departamento de Estatı́stica/CCEN
Universidade Federal de Pernambuco
Cidade Universitária, Recife/PE 50740-540, Brazil.
e-mail: raydonal@de.ufpe.br
Silvia L.P. Ferrari
Departamento de Estatı́stica/IME
Universidade de São Paulo
Rua do Matão 1010, São Paulo/SP 05508-090, Brazil.
e-mail: silviaferrari.usp@gmail.com

Abstract
This paper proposes a general class of regression models for continuous proportions when the
data contain zeros or ones. The proposed class of models assumes that the response variable
has a mixed continuous-discrete distribution with probability mass at zero or one. The beta
distribution is used to describe the continuous component of the model, since its density has
a wide range of different shapes depending on the values of the two parameters that index
the distribution. We use a suitable parameterization of the beta law in terms of its mean and
a precision parameter. The parameters of the mixture distribution are modeled as functions
of regression parameters. We provide inference, diagnostic, and model selection tools for this
class of models. A practical application that employs real data is presented.
Keywords and Phrases: Continuous proportions; Zero-or-one inflated beta distribution;
Fractional data; Maximum likelihood estimation; Diagnostics; Residuals.

1

Introduction

Statistical modeling of continuous proportions has received close attention in the last few years.
Some examples of proportions measured on a continuous scale include the fraction of income
contributed to a retirement fund, the proportion of weekly hours spent on work-related activities,
the fraction of household income spent on food, the percentage of ammonia escaping unconverted
from an oxidation plant, etc. Usual regression models, such as normal linear or nonlinear regression
models, are not suitable for such situations. Different strategies have been proposed for modeling
continuous proportions that are perceived to be related to other variables. Regression models
that assume a beta distribution for the response variable are of particular interest.
It is well known that the beta distribution is very flexible for modeling limited range data,
since its density has different shapes depending on the values of the two parameters that index
1

the distribution: left-skewed, right-skewed, “U ,” “J,” inverted “J,” and uniform (see Johnson,
Kotz & Balakrishnan, 1995, §25.1). Beta regression models have been studied by Kieschnick &
McCullough (2003), Ferrari & Cribari-Neto (2004), Espinheira, Ferrari & Cribari-Neto (2008a,
2008b), Paolino (2001), Smithson & Verkuilen (2006), Korhonen et al. (2007), Simas, BarretoSouza & Rocha (2010), and Ferrari & Pinheiro (2010), among others.
Oftentimes, proportions data include a non-negligeable number of zeros and/or ones. When
this is the case, the beta distribution does not provide a satisfactory description of the data, since
it does not allow a positive probability for any particular point in the interval [0, 1]. A mixed
continuous-discrete distribution might be a better choice. This approach has been considered
by Ospina & Ferrari (2010), who used the beta law to define the continuous component of the
distribution. The discrete component is defined by a Bernoulli or a degenerate distribution at
zero or at one. The proposed distributions are usually referred to as zero-and-one inflated beta
distributions (mixture of a beta and a Bernoulli distribution) and zero-inflated beta distributions
or one-inflated beta distributions (mixture of a beta and a degenerate distribution at zero or at
one, depending on the case).
This paper is concerned with a general class of regression models for modeling continuous
proportions when zeros or ones appear in the data. Such class of models is tailored for the situation
where only one of the extremes (zero or one) is present in the dataset. We shall consider a mixture
of a beta distribution and a degenerate distribution at a fixed known point c, where c ∈ {0, 1}. The
beta distribution is conveniently parameterized in terms of its mean and a precision parameter.
We shall allow the mean and the precision parameter of the beta distribution and the probability
of a point mass at c to be related to linear or non-linear predictors through smooth link functions.
Inference, diagnostic, and selection tools for the proposed class of models will be presented. Closely
related to our work are the papers by Hoff (2007) and Cook, Kieschnick & McCullough (2008). Our
model, however, is more general and convenient than those proposed by these authors. It relaxes
linearity assumptions, allows all the parameters of the underlying distribution to be modeled as
functions of unknown parameters and covariates, and uses a suitable parameterization of the beta
law. Unlike their papers, our article offers a comprehensive framework for the statistical analysis
of continuous data observed on the standard unit interval with a point mass at one of its extremes.
The paper unfolds as follows. Section 2 presents a general class of zero-or-one inflated beta
regression models. Section 3 discusses maximum likelihood estimation. Section 4 is devoted to
diagnostic measures. Section 5 contains an application using real data and concluding remarks
are given in Section 6. Some technical details are collected in two appendices.

2

Zero-or-one inflated beta regression models

The beta distribution with parameters µ and φ (0 < µ < 1 and φ > 0), denoted by B(µ, φ), has
the density function
f (y; µ, φ) =

Γ(φ)
y µφ−1 (1 − y)(1−µ)φ−1 ,
Γ(µφ)Γ((1 − µ)φ)

y ∈ (0, 1),

(2.1)

where Γ(·) is the gamma function. The parameterization employed in (2.1) is not the usual one,
but it is suitable for modeling purposes.
If y ∼ B(µ, φ), then E(y) = µ and Var(y) = µ(1 − µ)/(φ + 1). Hence, µ is the distribution
mean and φ plays the role of a precision parameter, in the sense that, for a fixed µ, the larger
the value of φ, the smaller the variance of y. Since the beta distribution is very flexible, allowing
a wide range of different forms, it is an attractive choice for modeling continuous proportions. A
2

possible shortcoming is that it is not appropriate for modeling datasets that contain observations
at the extremes (either zero or one). Our focus is on the case where only one of the extremes
appears in the data—a situation often found in empirical research. It is then natural to model
the data using a mixture of two distributions: a beta distribution and a degenerate distribution
in a known value c, where c equals zero or one, depending on the case. Under this approach,
we assume that the probability density function of the response variable y with respect to the
measure generated by the mixture1 is given by
(
α,
if y = c,
(2.2)
bic (y; α, µ, φ) =
(1 − α)f (y; µ, φ), if y ∈ (0, 1),
where f (y; µ, φ) is the beta density (2.1). Note that α is the probability mass at c and represents
the probability of observing zero (c = 0) or one (c = 1). If c = 0, the density (2.2) is called a
zero-inflated beta distribution, and if c = 1, the density is called a one-inflated beta distribution.
The mean of y and its variance can be written as
E(y) = αc + (1 − α)µ,
µ(1 − µ)
Var(y) = (1 − α)
+ α(1 − α)(c − µ)2 .
φ+1
Note that E(y) is the weighted average of the mean of the degenerate distribution at c and the
mean of the beta distribution B(µ, φ) with weights α and 1 − α. Also, E(y | y ∈ (0, 1)) = µ and
Var(y | y ∈ (0, 1)) = µ(1 − µ)/(1 + φ). Other properties of this distribution can be found in
Ospina & Ferrari (2010).
A general class of zero-or-one inflated beta regression models can be defined as follows. Let
y1 , . . . , yn be independent random variables such that each yt , for t = 1, . . . , n, has probability
density function (2.2) with parameters α = αt , µ = µt , and φ = φt . We assume that αt , µt , and
φt are defined as
h1 (αt ) = η1t = f1 (vt , ρ),
h2 (µt ) = η2t = f2 (xt , β),

(2.3)

h3 (φt ) = η3t = f3 (zt , γ),
where ρ = (ρ1 , . . . , ρp )⊤ , β = (β1 , . . . , βk )⊤ and γ = (γ1 , . . . , γm )⊤ are vectors of unknown
regression parameters; (p + k + m < n), η1 = (η11 , . . . , η1n )⊤ , η2 = (η21 , . . . , η2n )⊤ , and
η3 = (η31 , . . . , η3n )⊤ are predictor vectors; and f1 (·, ·), f2 (·, ·), and f3 (·, ·) are linear or nonlinear twice continuously differentiable functions in the second argument, such that the derivative
matrices V = ∂η1 /∂ρ, X = ∂η2 /∂β, and Z = ∂η3 /∂γ have ranks p, k and m, respectively, for all
ρ, β, and γ.
Here vt = (vt1 , . . . , vtp′ ), xt = (xt1 , . . . , xtk′ ), and zt = (zt1 , . . . , ztm′ ) are observations on
′
p + k′ + m′ known covariates. We also assume that the link functions h1 : (0, 1) → R, h2 :
(0, 1) → R, and h3 : (0, +∞) → R are strictly monotonic and twice differentiable. Various
different link functions may be used. For µ and α one may choose h2 (µ) = log{µ/(1 − µ)} (logit
link); h2 (µ) = Φ−1 (µ) (probit link), where Φ(·) denotes the standard normal distribution function;
h3 (µ) = log{− log(1 − µ)} (complementary log-log link); and h2 (µ) = − log{− log(µ)} (log-log
1

The probability measure P, corresponding to this distribution, defined over the measure space ((0, 1) ∪ {c}, B),
where B is the class of all Borelian subsets of (0, 1)∪{c}, is such that P << λ+δc , with λ representing the Lebesgue
measure, and δc is such that δc (A) = 1 if c ∈ A and δc (A) = 0 if c ∈
/ A and A ∈ B.

3

√
link), among others. Possible specifications for φ are h3 (φ) = log φ (log link) and h3 (φ) = φ
(square-root link).
Model (2.2)-(2.3) has a number of interesting features. The variance of yt is a function of
(αt , µt , φt ) and, as a consequence, of the covariates values. Hence, non-constant response variances
are naturally accommodated by the model. Also, the role that the covariates and the parameters
play in the model is clear. For example, suppose c = 0. In this case, vt and ρ affect Pr(yt = 0), xt
and β control E(yt | yt ∈ (0, 1)), and zt and γ influence the precision of the conditional distribution
of yt , given that yt ∈ (0, 1). This feature can be very useful for modeling purposes. For instance,
if the response is the individual mobile communications expenditure proportion (MCEP), model
(2.2)-(2.3) allows the researcher to take into account that some individuals do not spend at all
on MCEP and to separately assess the effects of the heterogeneity among consumers and nonconsumers of mobile communications (in this connection, see Yoo, 2004).
Special cases. Model (2.2)-(2.3) embodies two general classes of models: the zero-inflated beta
regression model (c = 0) and the one-inflated beta regression model (c = 1), the first of which
is suitable when the data include zeros and the second, when ones appear in the dataset. Each
of them leads to a corresponding linear model when the predictors are linear functions of the
parameters. In this case, we have p = p′ , k = k′ , m = m′ , h1 (αt ) = vt⊤ ρ, h2 (µt ) = x⊤
t β and
⊤ )⊤ , and Z = (z ⊤ , . . . , z ⊤ )⊤ . Also, the
h3 (φt ) = zt⊤ γ. Here, V = (v1⊤ , . . . , vn⊤ )⊤ , X = (x⊤
,
.
.
.
,
x
n
n
1
1
nonlinear beta regression model (Simas, Barreto-Souza & Rocha, 2010, and Ferrari & Pinheiro,
2010) is a limiting case of our model obtained by setting αt = α → 0. If, in addition, the predictor
for µt is linear and φt is constant through the observations, we arrive at the beta regression model
defined by Ferrari & Cribari-Neto (2004).

3

Likelihood inference

The likelihood function for θ = (ρ⊤ , β ⊤ , γ ⊤ )⊤ based on a sample of n independent observations is
L(θ) =

n
Y

bic (yt ; αt , µt , φ) = L1 (ρ)L2 (β, γ),

(3.1)

t=1

where

L1 (ρ) =

n
Y

1l

αt {c}

(yt )

t=1

L2 (β, γ) =

Y

(1 − αt )1−1l{c} (yt ) ,

f (yt ; µt , φt ),

t:yt ∈(0,1)

with 1lA (yt ) being an indicator function that equals 1 if yt ∈ A, and 0, if yt ∈
/ A. Here, αt =
−1
−1
h−1
(η
),
µ
=
h
(η
),
and
φ
=
h
(η
),
as
defined
in
(2.3),
are
functions
of ρ, β, and γ,
1t
t
2t
t
3t
1
2
3
respectively. Notice that the likelihood function L(θ) factorizes in two terms, the first of which
depends only on ρ (discrete component), and the second, only on (β, γ) (continuous component).
Hence, the parameters are separable (Pace & Salvan, 1997, p. 128) and the maximum likelihood
inference for (β, γ) can be performed separately from that for ρ, as if the value of ρ were known,
and vice-versa.
The log-likelihood function is given by
ℓ(θ) = ℓ1 (ρ) + ℓ2 (β, γ) =

n
X
t=1

4

ℓt (αt ) +

X

t:yt ∈(0,1)

ℓt (µt , φt ),

(3.2)

where
ℓt (αt ) = 1l{c} (yt ) log αt + (1 − 1l{c} (yt )) log(1 − αt ),
ℓt (µt , φt ) = log Γ(φt ) − log Γ(µt φt ) − log Γ((1 − µt )φt ) + (µt φt − 1)yt∗ + (φt − 2)yt† ,

(3.3)
(3.4)

yt∗ = log{yt /(1 − yt )} and yt† = log(1 − yt ) if yt ∈ (0, 1), and yt∗ = 0 and yt† = 0 otherwise. We
have the following conditional moments of yt∗ and yt† :
µ∗t = E(yt∗ | yt ∈ (0, 1)) = ψ(µt φt ) − ψ((1 − µt )φt ),
µ†t = E(yt† | yt ∈ (0, 1)) = ψ((1 − µt )φt ) − ψ(φt ),

vt∗ = Var(yt∗ | yt ∈ (0, 1)) = ψ ′ (µt φt ) − ψ ′ ((1 − µt )φt ),
vt†
∗†

c

=
=

(3.5)

Var(yt† | yt ∈ (0, 1)) = ψ ′ ((1 − µt )φt ) − ψ ′ (φt ),
Cov(yt∗ , yt† | yt ∈ (0, 1)) = −ψ ′ ((1 − µt )φt ),

with ψ(·) denoting the digamma function.2 Notice that ℓ1 (ρ) represents the log-likelihood function
of a regression model for binary responses, in which the success probability for the tth observation
is αt = h−1
1 (η1t ) (McCullagh & Nelder 1989, §4.4.1). On the other hand, ℓ2 (β, γ) is the loglikelihood function for (β, γ) in a nonlinear beta regression model based on the observations
that fall in the interval (0, 1). Hence, the maximum likelihood (ML) estimation for this model
can be accomplished by separately fitting a binomial regression model to the indicator variables
1l{c} (yt ), for t = 1, . . . , n, and a nonlinear beta regression model to the observations yt ∈ (0, 1),
for t = 1, . . . , n.
The score function, obtained by the differentiation of the log-likelihood function with respect
to the unknown parameters (see (A.1) – (A.3); Appendix A), is given by
U (θ) = (Uρ (ρ)⊤ , Uβ (β, γ)⊤ , Uγ (β, γ)⊤ )⊤ ,
where

Uρ (ρ) = V ⊤ ADA∗ (y c − α),

Uβ (β, γ) = X ⊤ (In − Y c )T Φ(y ∗ − µ∗ ),
⊤

c

∗

∗

(3.6)
†

†

Uγ (β, γ) = Z (In − Y )H[M(y − µ ) + (y − µ )].

Here, y ∗ = (y1∗ , . . . , yn∗ )⊤ , y † = (y1† , . . . , yn† )⊤ , y c = (1l{c} (y1 ), . . . , 1l{c} (yn ))⊤ , µ∗ = (µ∗1 , . . . , µ∗n )⊤ ,
µ† = (µ†1 , . . . , µ†n )⊤ , and α = (α1 , . . . , αn )⊤ are n-vectors and M = diag(µ1 , . . . , µn ), A =
diag(1/α1 , . . . , 1/αn ),
A∗ = diag(1/(1 − α1 ), . . . , 1/(1 − αn )),
D = diag(1/h′1 (α1 ),
. . . , 1/h′1 (αn )), Φ = diag(φ1 , . . . , φn ), T = diag(1/h′2 (µ1 ), . . . , 1/h′2 (µn )), H = diag(1/h′3 (φ1 ),
. . . , 1/h′3 (φn )), and Y c = diag(1l{c} (y1 ), . . . , 1l{c} (yn )) are n × n diagonal matrices. Also, In represents the n × n identity matrix. The maximum likelihood estimators (MLEs) of ρ and (β ⊤ , γ ⊤ )⊤
are obtained as the solutions of the nonlinear systems Uρ (ρ) = 0 and (Uβ (β, γ)⊤ , Uφ (β, γ))⊤ = 0.
There are not closed form expressions for these estimators, and their computations should be
performed numerically using a nonlinear optimization algorithm, e.g., some form of Newton’s
method (Newton-Raphson, Fisher’s scoring, BHHH, etc.) or a quasi-Newton algorithm such as
BFGS. An iterative algorithm for maximum likelihood estimation is presented in Appendix B.
For more details on nonlinear optimization, see Press et al. (1992).
2

The digamma function is defined as ψ(x) = d log Γ(x)/dx, x > 0.

5

From the observed information matrix given in (A.4), it can be shown that the Fisher information matrix is


Kρρ
0
0
K(θ) =  0
Kββ Kβγ  ,
(3.7)
0
Kγβ Kγγ

⊤ = X ⊤ W Z, K
⊤
where Kρρ = V ⊤ W1 V, Kββ = X ⊤ W2 X , Kγβ = Kβγ
3
γγ = Z W4 Z with W1 =
(A∗ + A)D 2 , W2 = ΦT {V ∗ A∗−1 }T Φ, W3 = T {Φ(MV ∗ + C)A∗−1 }H, and W4 = H{(M2 V ∗ +
2MC + V † )A∗−1 }H. Notice that Kρβ = Kβρ ⊤ = 0 and Kργ = Kγρ ⊤ = 0, thus indicating that
the parameters γ and (β ⊤ , γ ⊤ )⊤ are globally orthogonal (Cox & Reid, 1987) and their MLEs, ρb
and (βb⊤ , γb⊤ )⊤ , are asymptotically independent.
The inverse of Fisher’s information matrix is useful for computing asymptotic standard errors
of MLEs. From (3.7) and a standard formula for the inverse of partitioned matrices (Rao, 1973,
p. 33), we have
 ρρ

K
0
0
K(θ)−1 =  0
K ββ K βγ  ,
(3.8)
0
K γβ K γγ

where K ρρ = (V ⊤ W1 V)−1 , K ββ = {X ⊤ (W2 −W3 Z(Z ⊤ W4 Z)−1 Z ⊤ W3 )X }−1 , K γβ = (K βγ )⊤ =
−(Z ⊤ W4 Z)−1 Z ⊤ W3 X (X ⊤ (W2
−
W3 Z(Z ⊤ W4 Z)−1 Z ⊤ W3 )X )−1 ,
and
K γγ
=
(Z ⊤ W4 Z)−1 + (Z ⊤ W4 Z)−1 Z ⊤ W3 X (X ⊤ (W2 − W3 Z(Z ⊤ W4 Z)−1 Z ⊤ W3 )X )−1
⊤
X W3 Z × (Z ⊤ W4 Z)−1 .
Fitting the model using the GAMLSS implementation. The zero-or-one inflated beta
distribution has been incorporated in the GAMLSS framework (Rigby & Stasinopoulos, 2005);
see Ospina (2006). GAMLSS allows the flexible modeling of each of the three parameters that
index the distribution using parametric terms involving linear or nonlinear predictors, smooth
nonparametric terms (e.g., cubic splines or loess), and random effects. Maximum (penalized)
likelihood estimation is approached through a Newton-Raphson or Fisher scoring algorithm with
the backfitting algorithm for the additive components. Our approach consists of an application of
the gamlss functions, which are fully documented in the gamlss package (Stasinopoulos & Rigby,
2007; see also http://www.gamlss.org). The structure of the gamlss functions is familiar to
readers who are used to the R (or S-Plus) syntax (the glm function, in particular). The set of
gamlss packages can be freely downloaded from the R library at http://www.r-project.org/.
Large sample inference. If the model specified by (2.2) and (2.3) is valid and the usual regularity conditions for maximum likelihood estimation are satisfied (Cox & Hinkley, 1974, p. 107),
b respectively, are consistent. Assuming that
the MLEs of θ and K(θ), θb = (b
ρ⊤ βb⊤ , γ
b)⊤ and K(θ),
√
D
I(θ) = limn→∞ {n−1 K(θ)} exists and is nonsingular, we have n(θb − θ) → Np+k+m(0, I(θ)−1 ),
D

where → denotes convergence in distribution. Note that, if θl denotes the lth component of θ, it
follows that
n
o−1/2
D
b ll
→ N (0, 1),
(θbl − θl ) K(θ)

b ll is the lth diagonal element of K(θ)
b −1 . A rigorous proof of the aforementioned
where K(θ)
asymptotic result can be obtained by extending arguments similar to those given in Fahrmeir &
Kaufmann (1985).
b rr , and K(b
Let K(b
ρ)ss , K(β)
γ )RR be the estimated sth, rth and Rth diagonal elements of K ρρ ,
K ββ , and K γγ , respectively. The asymptotic variances of ρbs , βbr , and γbR are estimated by K(b
ρ)ss ,
6

b rr , and K(b
K(β)
γ )RR , respectively. If 0 < ς < 1/2, and zς represents the ςth quantile of the N (0, 1)
b rr )1/2 , and γb ± z1−ς/2 (K(b
distribution, we have ρbs ± z1−ς/2 (K(b
ρ)ss )1/2 , βbr ± z1−ς/2 (K(β)
γ )RR )1/2
R
as the limits of asymptotic confidence intervals (ACI) for ρs , βr , and γR , respectively, all with
asymptotic coverage 100(1 − ς)%. Additionally, an approximate 100(1 − ς)% confidence interval
for µ• = E(y), the mean response for the given covariates v and x, can be computed as


c• − z1−ς/2 s.e.(µ
c• ), µ
c• + z1−ς/2 s.e.(µ
c• ) ,
µ
c• = cb
where c = 0 or c = 1, depending on the case; µ
α + (1 − α
b)b
µ, with α
b = h−1
η1 ); µ
b = h−1
η2 );
1 (b
2 (b
b
ηb1 = f1 (v; ρb); ηb2 = f2 (x; β); and
q

2
2
•
b ρρ v + (1 − α
b ββ x.
c
s.e.(µ ) =
(c − µ
b)/h′1 (b
b)/h′2 (b
α) v ⊤ K
µ ) x⊤ K

b
b ρρ and K
b ββ respectively equal K ρρ and K ββ , evaluated at θ.
Here, K
The likelihood ratio, Rao’s score, and Wald’s (W) statistics to test hypotheses on the parameters can be calculated from the log-likelihood function, the score vector, the Fisher information
matrix, and its inverse given above. Their null distributions are usually unknown and the tests
rely on asymptotic approximations. In large samples, a chi-squared distribution can be used as
an approximation to the true null distributions. For testing the significance of the ith regression
parameter that models µ, one can use the signed square root of Wald’s statistic, βbi /s.e.(βbi ), where
s.e.(βbi ) is the asymptotic standard error of the MLE of βi obtained from the inverse of Fisher’s
information matrix evaluated at the maximum likelihood estimates. The limiting null distribution
of the test statistic is standard normal. Significance tests on the γ’s and ρ’s can be performed in
a similar fashion.

Finite-sample performance of the MLEs. We now present the results of two Monte Carlo
simulation experiments in order to investigate the finite-sample performance of the MLEs. The
first experiment evaluates the impact of the magnitude of the probability of zero response on the
MLEs. Here, the sample size is n = 150 and we focus on the zero-inflated beta regression model
with logit(µ) = β0 + β1 x1 + β2 x2 + β3 x3 , log(φ) = γ0 + γ1 z1 + γ2 z2 + γ3 z3 , and α being constant for
all observations. The true parameter values were taken as β0 = −1, β1 = 1, β2 = −0.5, β3 = 0.5,
γ0 = 2, γ1 = 1, γ2 = 0.5, and γ3 = 0.5. Here, we consider four different values for the probability
of observing zero: α = 0.18, α = 0.32, α = 0.68, and α = 0.82.
The second experiment considers the zero-inflated beta regression model with logit(α) =
ρ0 + ρ1 v1 + ρ2 v2 + ρ3 v3 , logit(µ) = β0 + β1 x1 + β2 x2 + β3 x3 , and log(φ) = γ0 + γ1 z1 + γ2 z2 + γ3 z3 .
Here, we evaluate the performance of the MLEs when the sample size increases. The true values
of the β’s and the γ’s are the same as in the first experiment. The true values of the ρ’s are
ρ0 = −1, ρ1 = 1, ρ2 = −0.5, and ρ3 = 0.5. In this situation the sample sizes considered are
n = 50, 150, and 300.
The explanatory variables v1 , x1 , and z1 were generated as independent draws from a standard
normal distribution. The covariates v2 , x2 , and z2 were generated from the Poisson distribution
with unit mean, and v3 , x3 , and z3 were generated from the Binomial(0.2, 5) distribution. The
total number of Monte Carlo replications was set at 5000 for each sample size. All simulations
were carried out in R (Ihaka & Gentleman, 1996). Computations for fitting inflated beta regression
models were performed using the gamlss package. The MLEs were obtained by maximizing the
log-likelihood function using the RS algorithm (Rigby & Stasinopoulos,
√ 2005). In order to analyze
the results, we computed the bias and the root mean squared error ( MSE) of the estimates.
7

Table 1 summarizes the numerical
results of the first experiment. We note that, for a fixed
√
sample size, the bias and the MSE of the estimators of the continuous component (β’s and γ’s)
increase with the expected proportion of zeros (α). This is to be expected, since the expected
number
√ of observations in (0, 1) decreases as α increases. Also, it is noteworthy that the bias and
the MSE of MLEs corresponding to the dispersion covariates tend to be much more pronounced
when compared with the MLEs of the parameters that model the mean response.
Table 1: Simulation results for the first experiment.

Estimator

α = 0.18
√
Bias
MSE

α = 0.18
√
Bias
MSE

α = 0.18
√
Bias
MSE

α = 0.18
√
Bias
MSE

α
b

0.00052

0.03149

0.00022

0.03778

0.00086

0.03802

−0.00207

0.03097

βb0
βb1

−0.00095

0.05213

−0.00176

0.07724

0.00032

0.11219

−0.00226

0.19777

0.00176

0.04006

0.00207

0.04101

0.00354

0.07070

0.00452

0.11554

βb2
βb3

−0.00048

0.02711

−0.00076

0.04308

−0.00427

0.08532

−0.00328

0.11987

0.00014

0.03764

0.00024

0.05145

0.00073

0.08256

0.00089

0.12046

γ
b0

0.01241

0.21359

0.02668

0.25779

0.02892

0.43969

0.06437

0.71228

γ
b1

0.03158

0.13205

0.03203

0.14619

0.07302

0.30639

0.20369

0.51323

γ
b2

0.03088

0.12514

0.04288

0.16232

0.09854

0.30874

0.23140

0.68394

γ
b3

0.02745

0.15964

0.01527

0.17183

0.07067

0.32288

0.19891

0.69026

Table 2 presents simulation results for the second experiment. For the smallest sample size
considered (n = 50), the estimation algorithm failed to converge in 1.3% of the samples. For large
sample sizes the algorithm converged for all the samples. The estimated biases of the MLEs of
the ρ’s (parameters of the discrete component) are markedly high for small samples. This is not
surprising; in standard logistic regression, the MLEs are considerably biased in small samples.
For large samples, the bias of the ρ’s is negligible.
In the second experiment, the β’s and the γ’s are essentially estimated from the observations
in (0,1), which represent around 70% of the observations in our study. For all the sample sizes
b and γ
considered, the mean of the β’s
b’s are close to the corresponding true values. Also, all the
root mean square errors decrease when the sample size increases, as expected.

4

Diagnostics

Likelihood-based inference depends on parametric assumptions, and a severe misspecification of
the model or the presence of outliers may impair its accuracy. We shall introduce some types of
residuals for detecting departures from the postulated model and outlying observations. Additionally, we suggest some measures to assess goodness-of-fit.
Residuals. Residual analysis in the context of the zero-or-one inflated beta regression model
(2.1)-(2.3) can be split into two parts. First, we focus on the residual analysis for the discrete and
the continuous component of the model separately. For this purpose, we propose a standardized
Pearson residual based on Fisher’s scoring iterative algorithm for estimating ρ, β, and γ. Second,
8

Table 2: Simulation results for the second experiment.
n = 50
Estimator
ρb0
ρb1
ρb2
ρb3

Bias
−0.14554

−0.18009

−0.13892

n = 150
√

MSE

0.81179
0.56946
0.66011

0.09667

0.55895

βb0
βb1

0.00588
0.00588

βb2
βb3

n = 300
√

Bias
−0.02398

MSE

0.38136

−0.04623

0.25131

−0.03437

0.24178

Bias
−0.01263

−0.02501

√

MSE

0.26529
0.17076

−0.01803

0.16958

0.01093

0.16877

0.02113

0.24240

0.16190

0.00171

0.07288

0.00055

0.05272

0.10917

0.00146

0.04882

0.00027

0.03313

−0.00211

0.09761

0.00018

0.02872

0.12356

−0.00131

0.05057

0.00140

0.00104

0.05264

0.00063

0.03049

γ
b0

0.12628

0.45696

0.04208

0.25820

0.02472

0.18738

γ
b1

0.04197

0.29273

0.03485

0.15767

0.00959

0.10359

γ
b2

0.09771

0.39828

0.02333

0.17590

0.01153

0.10208

γ
b3

0.04934

0.36516

0.02266

0.18366

0.00473

0.11669

we define a randomized quantile residual as a global residual for the model, i.e., using information
of the discrete and the continuous component simultaneously.
To study departures from the error assumptions as well as the presence of outlying observations
of the discrete component for the zero-or-one inflated beta regression model we define a version
of the standardized Pearson residual as
D
rpt
=q

1l{c} (yt ) − α
bt

b tt )
α
bt (1 − α
bt )(1 − h

,

t = 1, . . . , n,

(4.1)

where htt is the tth diagonal element of the orthogonal projection matrix H defined in (B.5) and
D is a version of the ordinary residual obtained
“ b ” indicates evaluation at the MLEs. Note that rpt
in (B.4) that takes the leverage of the tth observation into account. Plots of the residuals against
the covariates or the fitted values should exhibit no systematic trend.
The conditional distribution of yt , given that yt ∈ (0, 1), is a beta distribution B(µt , φt ). Ferrari
and Cribari–Neto (2004) provided two different residuals (standardized and deviance) for the class
of beta regression models. Espinheira et al. (2008b) proposed two new beta regression residuals
and their numerical results favor one of the new residuals: the one that accounts for the leverages
of the different observations. Here, we follow Espinheira et al. (2008b) to define a weighted version
of the ordinary residual from Fisher’s scoring iterative algorithm for estimating β when γ is fixed
(see Appendix B). Our proposed residual is defined as
C
rpt
=q

yt∗ − µ
b∗t

b tt )
vbt∗ (1 − α
bt )(1 − P

,

t : yt ∈ (0, 1),

(4.2)

b tt is the tth diagonal element of the projection matrix P
b defined in (B.13). Note that r C
where P
pt
is a version of the standardized Pearson residual obtained in (B.14) that takes the leverage of the
tth observation into account.
9

To assess the overall adequacy of the zero-or-one inflated beta regression model to the data
at hand, we propose the randomized quantile residual (Dunn & Smyth, 1996). It is a randomized
version of the Cox & Snell (1968) residual and given by
rtq = Φ−1 (ut ),

t = 1, . . . , n,

(4.3)

where Φ(·) denotes the standard normal distribution function, ut is a uniform random variable
bt , µ
bt , φbt ) and bt = BIc (yt ; α
bt , µ
bt , φbt ). Here,
on the interval (at , bt ], with at = limy↑yt BIc (yt ; α
BIc (yt ; αt , µt , φt ) = αt 1l[c,1] (yt ) + (1 − αt )F (yt ; µt , φt ), where F (·; µt , φt ) is the cumulative distribution function of the beta distribution B(µt , φt ). In the zero-inflated beta regression model,
ut is a uniform random variable on (0, α
bt ] if yt = 0 and ut = BI0 (yt ; α
bt , µ
bt , φbt ) if yt ∈ (0, 1). On
the other hand, in the one-inflated beta regression model, ut is a uniform random variable on
[b
αt , 1) if yt = 1 and ut = BI1 (yt ; α
bt , µ
bt , φbt ) if yt ∈ (0, 1). Apart from sampling variability in α
bt , µ
bt ,
q
b
and φt the rt are exactly standard normal in (at , bt ] and the randomized procedure is introduced
in order to produce a continuous residual. The randomized quantile residuals can vary from one
realization to another. In practice, it is useful to make at least four achievements.
A plot of these residuals against the index of the observations (t) should show no detectable
pattern. A detectable trend in the plot of some residual against the predictors may be suggestive
of link function misspecification. Also, normal probability plots with simulated envelopes are a
helpful diagnostic tool (Atkinson, 1985). Simulation results not presented here indicated that
the randomized quantile residuals perform well in detecting whether the distribution assumption
is incorrect.
Global goodness-of-fit measure. A simple global goodness-of-fit measure is a pseudo R2 , say
Rp2 , defined by the square of the sample correlation coefficient between the outcomes, y1 , . . . , yn ,
• = E(y
[
c• , . . . , µ
c• , where µc
and their corresponding predicted values, µ
αt + (1 − α
bt )b
µt . A
t ) = cb
n

1

t

perfect agreement between the y’s and µ
b• ’s yields Rp2 = 1. Other pseudo R2 ’s are defined as
b log L
b 0 (McFadden, 1974) and R2 = 1 − (L
c0 /L)
b 2/n (Cox and Snell, 1989, p.
Rp2∗ = 1 − log L/
LR
b 0 and L
b are the maximized likelihood functions of the null model and the fitted
208-209), where L
model, respectively. The ratio of the likelihoods or log-likelihoods may be regarded as measures
of the improvement, over the model with only three parameters (α, µ, and φ), achieved by the
model under investigation.

Influence measures. A well-known measure of the influence of each observation on the regression parameter estimates is the likelihood displacement (Cook & Weisberg, 1982, Ch. 3). The
likelihood displacement that results from removing the tth observation from the data is defined
by
2 b
− ℓ(θb(t) )},
LDt = {ℓ(θ)
d
where d is the dimension of θ and θb(t) is the MLE of θ obtained after removing the tth observation
from the data. This definition does not consider that θ is actually split into two different types
of parameters: the parameters of the discrete component and the parameters of the continuous
component. Thus, it is more appropriate to consider the influence of the tth case on the estimation
of ρ and (β, γ) separately. Therefore, we propose the following statistics:
2
ρ) − ℓ1 (b
ρ(t) )},
t = 0, 1, . . . , n
LDtD = {ℓ1 (b
p
2
b γ
{ℓ2 (β,
b) − ℓ2 (βb(t) , γ
b(t) )},
t: yt ∈ (0, 1).
LDtC =
k+m
10

Simple approximations for LDtD and LDtC are given by
htt
(r D )2 ,
t = 0, 1, . . . , n
p(1 − htt ) pt
Ptt
(r C )2 ,
t: yt ∈ (0, 1),
LDtC ≈ cC
tt =
(k + m)(1 − Ptt ) pt

LDtD ≈ cD
tt =

see Cook and Weisberg (1982, p. 191) and Wei (1998, p. 102). The approximations above are
based on the iterative scheme for evaluating the MLE of ρ, β, and γ (Appendix B) and Taylor
b γ
expansions of ℓ1 (b
ρ(t) ) and ℓ2 (βb(t) , γ
b(t) ) around ρb and (β,
b), respectively. The quantities cD
tt and
C
ctt can be useful to highlight influential cases. In practice, we recommend the plotting of cD
tt and
C
ctt against t.
Model selection. Nested zero-or-one inflated beta regression models can be compared via
the likelihood ratio test, using twice the difference between the maximized log-likelihoods of a
full model and a restricted model whose covariates are a subset of the full model. Information
criteria, such as the generalized Akaike information criterion (GAIC), can be used for comparing
non-nested models. It is defined as
b + d℘ ,
GAIC = D

(4.4)

b = −2ℓb is the global fitted deviance (Rigby & Stasinopoulus, 2005), ℓb is the maximized
where D
log-likelihood and d is the dimension of θ. It is possible to interpret the first term on the righthand side of (4.4) as a measure of the lack of fit and the second term as a “penalty” for adding
d parameters. The model with the smallest GAIC is then selected. The Akaike information
criterion AIC (Akaike, 1974), the Schwarz Bayesian criterion SBC (Schwarz, 1978), and the
consistent Akaike information criterion (CAIC) are special cases of GAIC corresponding to ℘ = 2,
℘ = log(n), and ℘ = log(n) + 1, respectively.

5

An application

This section contains an application of the zero-inflated beta regression model to real data.
We analyze data on the mortality in traffic accidents of 200 randomly selected Brazilian municipal districts of the southeast region in the year 2002. The data were extracted from the
DATASUS database available at www.datasus.gov.bt and IPEADATA database available at
www.ipeadata.gov.br. The response variable y is the proportion of deaths caused by traffic
accidents. The explanatory variables are the logarithm of the number of inhabitants of the
municipality (lnpop), the proportion of the population living in the urban area (propurb), the
proportion of men in the population (propmen), the proportion of residents aged between 20 and
29 years (prop2029), and the human development index of education of the municipal district
(hdie). The main objective is to investigate the effect of the young population (prop2029) on
the proportion of deaths caused by traffic accidents after controlling for potential confounding
variables. We report the summary statistics on each of these variables in Table 3.
Figure 1 presents a histogram and a box-plot of the response variable. The clump-at-zero
(the bar with the dot above) in the histogram represents 39% of the data. We observe that
the distribution of the data in (0,1) is asymmetric with an inverted “J” shape. Also, the boxplot reveals the presence of some outliers. Visual inspection suggests that a zero-inflated beta
distribution may be a suitable model for the data.
11

Table 3: Sample summary statistics.
mean
9.4042
0.7173
0.5062
0.1660
0.8236

s.d.
1.3464
0.2044
0.0114
0.0136
0.0587

min.
7.3920
0.2042
0.4766
0.1322
0.6060

1st qu.
8.3997
0.5887
0.4989
0.1559
0.7955

median
9.0620
0.7445
0.5070
0.1661
0.8325

3rd qu.
10.0025
0.8862
0.5146
0.1750
0.8640

max.
16.1606
1.0000
0.5370
0.2042
0.9330

80

variable
lnpop
propurb
propmen
prop2029
hdie

40
0

20

Frequency

60

Mean: 0.05
Std. deviation: 0.07
Prop. zeros: 39 %

0.0

0.1

0.2

0.3

0.4

0.5

0.0

y

0.1

0.2

0.3

0.4

0.5

y

Figure 1: Frequency histogram and box-plot of the observed proportions of deaths caused by
traffic accidents.

We consider a zero-inflated beta regression model with the following specification:
logit(α) = ρ0 + ρ1 lnpop + ρ2 propurb + ρ3 propmen + ρ4 prop2029 + ρ5 hdei,
logit(µ) = β0 + β1 lnpop + β2 propurb + β3 propmen + β4 prop2029 + β5 hdei,
log(φ) = γ0 + γ1 lnpop + γ2 propurb + γ3 propmen + γ4 prop2029 + γ5 hdei.
Computations for fitting the model were carried out using the package gamlss (Stasinopoulus
& Rigby, 2007) in the R software package (Ihaka & Gentleman, 1996) and the BEZI distribution
implemented by Ospina (2006). The automatic procedure stepGAICAll.B() included in the
gamlss package was used to perform model selection using the AIC. The following final model
has been selected:
logit(α) = ρ0 + ρ1 lnpop + ρ4 prop2029 + ρ5 hdei,
logit(µ) = β0 + β1 lnpop + β4 prop2029 + β5 hdei,

(5.1)

log(φ) = γ0 + γ1 lnpop + γ4 prop2029 + γ5 hdei.
In fact, the value of the likelihood ratio statistic for testing H0 : θ = (ρ2 , ρ3 , β2 , β3 , γ2 , γ3 ) = 0
versus a two-side alternative is Λ = 4.89 (p-value = 0.56), i.e., H0 is not rejected at the usual
significance levels, and hence, propurb and propmen can be excluded from the model.
12

The parameter estimates and the corresponding standard errors for the final model are summa2 = 0.584, thus suggesting a reasonable
rized in Table 4. The pseudo R2 ’s are Rp2∗ = 0.597 and RLR
fit of the model to the data. It is noteworthy that prop2029, which represents the proportion
of the population aged between 20 to 29 years, has a positive effect on both the probability of
observing at least one death due to traffic accidents (1 − α) and the conditional mean proportion
of deaths due to this cause (µ).
Table 4: Parameter estimates (est.), standard errors (s.e.), and p-values (p).
α
intercept
lnpop
prop2029
hdei
µ
intercept
lnpop
prop2029
hdei
φ
intercept
lnpop
prop2029
hdei

est.
27.27
−1.17
−48.06
−11.34
estimate
−4.72
−0.53
27.68
3.10
estimate
9.46
0.47
−28.34
−6.70

s.e.
4.63
0.27
17.46
4.01
s.e.
1.20
0.05
6.37
1.44
s.e.
3.25
0.10
16.63
3.90

p
1.73e−08
2.07e−05
6.49e−03
5.13e−03
p
1.11e−04
7.14e−17
2.33e−05
3.28e−02
p
4.08e−03
8.54e−06
9.01e−02
8.78e−02

Figure 2 shows the plot of randomized quantile residuals against the case number (Figure
2(a)) and the quantile–quantile plot of the ordered randomized quantile residuals against the
corresponding quantiles of a standard normal distribution with a 95% envelope based on 100
simulations (Figure 2(b)). Figure 2(a) singles out observation 138 as possibly atypical, and
Figure 2(b) indicates that it lies in the threshold of the envelope. Case 138 is, therefore, worthy of
further investigation. Notice that Figure 2(b) clearly shows that the distribution of the randomized
quantile residuals is asymmetric, and the usual thresholds (−2, 2) or (−3, 3) should be used with
care.
D
Diagnostic plots for the discrete component are given in Figures 3(a)–3(c). The plot of rpt
against the case number (Figure 3(a)) suggests that the residuals appear to be randomly scattered
D against α
bt (Figure 3(b))
around zero and that there is no atypical observation. The plot of rpt
is not suggestive of a lack of fit. The lower and upper bounds, corresponding to responses equal
to zero and one, respectively, are typical of data with only two outcomes. Figure 3(c) shows the
plot of Cook statistics cD
tt against the case number, from which observation 196 (an observation
with the response variable equal to zero) is highlighted as influential.
Diagnostic plots for the continuous component are given in Figures 3(d)–3(f). Figure 3(d)
D against case number and suggests that the residuals are randomly scattered
shows the plot of rpt
D against µ
around zero. No observation is distinguished as atypical. The plot of rpt
bt (Figure
C
3(e)) is not suggestive of a lack of fit. Finally, the plot of ctt against the case number indicates
observations 2, 9, 85, and 138 as influential points for the continuous component.
We have reestimated the model after removing the following sets of observations: {138}, {196},
and {2, 9, 85, 138}. Our purpose is to investigate the impact of the exclusion of observations
13

(b)

3

(a)

138

*

q

* **

*
**** *
****
*
*
*
*
*
******
*****
*******
*
*
*
*
*
*
**
*******
*******
***
**********
*
*
*
*
*
*
**
***
*********
*
*
*
*
***

−3

−3

−2

−2

−1

−1

rt

0

rt

q

0

1

1

2

2

138

0

50

100

150

200

−3

t

−2

−1

0

1

2

3

normal quantiles

Figure 2: Quantile residual plots.
highlighted in the diagnostic plots on the inferences based on the estimated model. Table 5 gives
relative changes in the parameter estimates and the standard errors. Observation 138 alone is
clearly influential for the estimation of the continuous component of the model, while observation
196 solely impacts the inference on the discrete component, as expected. The joint exclusion of
cases 2, 9, 85, and 138 produces substantial changes in the parameter estimates of the continuous
component and only slight changes in the estimated model for the discrete part. Our findings
suggest that the proposed diagnostic tools are helpful for detecting atypical observations and
influential cases and are able to indicate the component of the model, discrete or continuous, that
is affected by each influential observation.

6

Concluding remarks

We developed a general class of zero-or-one inflated beta regression models that can be useful
for practitioners when modeling response variables in the standard unit interval, such as rates or
proportions, with the presence of zeros or ones. Explicit formulas for the score function, Fisher’s
information matrix, and its inverse are given. An iterative estimation procedure and its computational implementation are discussed. Interval estimation for different population quantities is
presented. We also proposed a set of diagnostic tools that can be employed to identify departures
from the postulated model and the atypical and influential observations. These tools include
pseudo R2 ’s, different residuals, influence measures, and model selection procedures. One particularly interesting feature of the proposed diagnostic analysis is that it allows the practitioner to
separately identify influential cases on the discrete and the continuous components of the model.
An application using real data was presented and discussed.

14

4

0.10

cDtt

2

196

−4

0.00

−2

0

rDpt

2
0
−4

−2

100

150

200

0.0

0.2

0.4

0.6

0.8

0

50

100

(e)

(f)

150

200

138
2
9

85

−4

0.00

−2

0.04

cCtt

rCpt

0
−4

−2

0.08

(d)
4

t

2

αt

4

t

2

50

0

rDpt

0

rCpt

(c)
0.20

(b)

4

(a)

0

50

100

150

200

0.00

0.10

0.20

0

50

100

µt

t

150

200

t

Figure 3: Diagnostic plots for the discrete component ((a)–(c)) and the continuous component
((d)–(f)).

Acknowledgements
We gratefully acknowledge partial financial support from FAPESP/Brazil and CNPq/Brazil.

A

Appendix A: Score vector and observed information matrix

Score vector. The elements of the score vector are given by
n

n

X 1l{c} (yt ) − αt 1 ∂η1t
∂ℓ(θ) X ∂ℓt (αt ) ∂αt ∂η1t
=
=
,
Us =
∂ρs
∂αt ∂η1t ∂ρs
αt (1 − αt ) h′1 (αt ) ∂ρs
t=1
t=1
Ur =

UR =

∂ℓ(θ)
=
∂γR

∂ℓ(θ)
=
∂βr

X ∂ℓt (µt , φt ) ∂µt ∂η2t
=
∂µt
∂η2t ∂βr

t:yt ∈(0,1)

X ∂ℓt (µt , φt ) ∂φt ∂η3t
=
∂φt
∂η3t ∂γR

t:yt ∈(0,1)

X

15

φt (yt∗ − µ∗t )

t:yt ∈(0,1)

1
h′2 (µt )

[µt (yt∗ − µ∗t ) + (yt† − µ†t )]

t:yt ∈(0,1)

for s = 1, . . . , p; r = 1, . . . , k; and R = 1, . . . , m.

X

(A.1)

∂η2t
,
∂βr
1

h′3 (φt )

∂η3t
,
∂γR

(A.2)

(A.3)

Table 5: Parameter estimates (est.), standard errors (s.e.), p-values (p), relative changes in estimates (rel. est.), and relative changes in standard errors (rel. s.e.) because of exclusions of
observations (in percentage).
obs.

138

196

2, 9, 85, 138

parameter
ρ0
ρ1
ρ4
ρ5
β0
β1
β4
β5
γ0
γ1
γ4
γ5
ρ0
ρ1
ρ4
ρ5
β0
β1
β4
β5
γ0
γ1
γ4
γ5
ρ0
ρ1
ρ4
ρ5
β0
β1
β4
β5
γ0
γ1
γ4
γ5

est.
27.26
−1.17
−48.06
−11.34
−5.05
−0.57
32.40
2.93
10.66
0.61
−43.46
−6.47
30.78
−1.38
−51.62
−12.68
−4.72
−0.53
27.68
3.10
9.47
0.48
−28.34
−6.70
26.97
−1.16
−47.28
−11.27
−3.98
−0.58
29.71
2.29
7.48
0.61
−29.35
−5.37

s.e
4.63
0.27
17.46
4.01
1.11
0.06
6.42
1.40
3.00
0.11
17.26
3.83
5.25
0.30
18.21
4.24
1.20
0.06
6.38
1.44
3.26
0.10
16.64
3.91
4.62
0.27
17.45
4.00
1.11
0.05
5.87
1.36
3.35
0.14
16.95
4.17

p
0.00
0.00
0.01
0.01
0.00
0.00
0.00
0.04
0.00
0.00
0.01
0.09
0.00
0.00
0.01
0.00
0.00
0.00
0.00
0.03
0.00
0.00
0.09
0.09
0.00
0.00
0.01
0.01
0.00
0.00
0.00
0.09
0.03
0.00
0.08
0.20

rel. est.
0.04
0.08
−0.01
0.04
−6.96
−6.85
−17.07
5.54
−12.60
−27.20
−53.34
3.52
−12.86
−18.15
−7.41
−11.80
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.11
1.17
1.63
0.67
15.69
−8.90
−7.33
26.14
20.96
−27.75
−3.57
19.85

rel. s.e
−0.06
−0.17
0.01
0.00
7.06
−1.48
−0.67
2.64
7.74
−7.02
−3.78
2.02
−13.34
−12.33
−4.30
−5.95
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.17
0.24
0.06
0.09
6.93
5.45
8.03
5.94
−2.82
−36.20
−1.86
−6.70

Observed information matrix. For s, s′ = 1, . . . , p; r, r′ = 1, . . . , k; and R, R′ = 1, . . . , m, we have
Jss′

Jrr′

(
#2
!"
n
X
−1l(0,1) (yt ) 1l{c} (yt )
∂ 2 ℓ(θ)
1
=−
=−
−
∂ρs ∂ρs′
(1 − αt )2
α2t
h′1 (αt )
t=1
!"
#"
#)
1l{c} (yt ) − αt
1
−h′′1 (αt )
∂η1t ∂η1t
+
αt (1 − αt )
h′1 (αt ) (h′1 (αt ))2
∂ρs ∂ρs′
(
"
#
)
n
X
1l{c} (yt ) − αt
1
∂ 2 η1t
−
,
′
αt (1 − αt ) h1 (αt ) ∂ρs ∂ρs′
t=1

(
!
)
X
1
1 ∂η2t ∂η2t
−h′′2 (µt )
∂ 2 ℓ(θ)
2 ∗
∗
∗
− φt vt ′
=−
+ φt (yt − µt )
=−
∂βr ∂βr′
h2 (µt )
(h′2 (µt ))2 h′2 (µt ) ∂βr ∂βr′
t:yt ∈(0,1)
(
)
2
X
1
∂
η
2t
φt (yt∗ − µ∗t ) ′
−
,
(h2 (µt )) ∂βr ∂βr′
t:yt ∈(0,1)

16

JrR

X
∂ 2 ℓ(θ)
=−
=−
∂βr ∂γR

(

[yt∗ − µ∗t − φt (µt vt∗ +

c∗†
t )]

t:yt ∈(0,1)

JRR′

X
∂ 2 ℓ(θ)
=−
=−
∂γR ∂γR′

(

[−µ2t vt∗ − 2µt c∗† − vt† ]

t:yt ∈(0,1)

)
1
∂η2t
∂η3t
1
,
(h′2 (µt )) ∂βr (h′3 (φt )) ∂γR

1
+ [µt (yt∗ − µ∗t ) + (yt† − µ†t )]
(h′3 (φt ))

!)
∂η3t ∂η3t
1
−h′′3 (φt )
×
(h′3 (φt ))2
(h′3 (φt )) ∂γR ∂γR′
(
)
X
1
∂ 2 η3t
†
†
∗
∗
[µt (yt − µt ) + (yt − µt )] ′
−
.
(h3 (φt )) ∂γR ∂γR′
t:yt ∈(0,1)

The observed Fisher information matrix can now be written as


Jρρ
0
0
J(θ) =  0 Jββ Jβγ  ,
0 Jγβ Jγγ

(A.4)

where

Jρρ = V ⊤ {(A∗2 (In − Y c ) + A2 Y c ) + A∗ A(Y c − α∗ )D∗ D}V ⊤ − [(y c − α)⊤ AD][V̇],

Jββ = X ⊤ {ΦT V ∗ + ST 2 (Y ∗ − M∗ )}(In − Y c )T ΦX + [(y ∗ − µ∗ )⊤ (In − Y c )T Φ][Ẋ ],
⊤
Jγβ = Jγβ
= −X ⊤ {(Y ∗ − M∗ ) − Φ(MV ∗ + C)}(In − Y c )T HZ,

Jγγ = Z ⊤ {H(M2 V ∗ + 2MC + V † ) + {M(Y ∗ − M∗ ) + (Y † − M† )}QH 2 }(In − Y c )HZ
+ [((y ∗ − µ∗ )⊤ M + (y † − µ† )⊤ )(In − Y c )H][Ż],

with α∗ = diag(α1 , . . . , αn ), Y ∗ = diag(y1∗ , . . . , yn∗ ), Y † = diag(y1† , . . . , yt† ), M∗ = diag(µ∗1 , . . . , µ∗n ), M† =
diag(µ†1 , . . . , µ†n ), and Q = diag(h′′3 (φ1 ), . . . , h′′3 (φn )); V̇ is an n×p×p array with faces Vt = ∂ 2 η1t /∂ρ∂ρ⊤ , Ẋ
is an n×k×k array with faces Xt = ∂ 2 η2t /∂β∂β ⊤ , and Ż is an n×m×m array with faces Zt = ∂ 2 η3t /∂γ∂γ ⊤,
for t = 1, . . . , n. The column multiplication for three-dimensional arrays is indicated by the bracket operator
[·][·] as defined by Wei (1998, p. 188).

B

Appendix B: Iterative algorithm for maximum likelihood estimation

MLEs for ρ and ϑ = (β ⊤ , φ)⊤ can be obtained by using a re-weighted least-squares algorithm. For ρ we
have
m = 0, 1, . . . ,
(B.1)
ρ(m+1) = (V (m)⊤ W1 (m) V (m) )−1 V (m) W1 (m) y1 (m) ,
where W1 is defined in (3.7) and
y1 = Vρ + W1 −1 ADA∗ (y c − α)

(B.2)

is a local modified dependent variable. This cycle is repeated until convergence is achieved. Rewriting this
equation as
(B.3)
y1 = η1 − τ1 + W1 −1 ADA∗ (y c − α),
where η1 = f 1 (V ; ρ) and τ1 = f 1 (V ; ρ) − Vρ, we can interpret (B.1) as an iterative process to fit a
generalized linear model with design matrix V, systematic part h1 (αt ) = η1t , tth diagonal element of the
variance function [W1 −1 ADA∗ ]tt , t = 1, . . . , n, and offset τ1 . The offset quantity is subtracted, at each
step, from the predictor η1 . The iterative process (B.1) may be performed, for instance, in the R package

17

by taking advantage of the library MASS (Venables & Ripley, 2002). This procedure allows us to extend
the diagnostic results of ordinary regression models to the discrete component of the zero-or-one inflated
b −1 V
b ⊤ z, where
b⊤W
c 1 V)
beta regression model. Upon the converge of iterative process (B.1), we have ρb = (V
b ρb + W
c −1 AbD
b Ab∗ (y c − α
z=V
b). The ordinary residual for this re-weighted regression is
1
b Ab∗ (y c − α
c 1/2 (z − ηb1 ) = W
c −1/2 AbD
b).
r∗ = W
1
1

Note that the tth element of r∗ is

1l{c} − α
bt
rt∗ = p
.
α
bt (1 − α
bt )

(B.4)

b ρ = V
b ⊤ z, it is possible to obtain the approximations E(r∗ ) ≈ 0 and Var(r∗ ) ≈
b⊤W
c 1 V)b
By writing (V
(In − H), where In is the n × n identity matrix and
H = W1 1/2 V(V ⊤ W1 V)−1 V ⊤ W1 1/2

(B.5)

is an orthogonal projection matrix onto the vector space spanned by the columns of V. The geometric
interpretation of H as a projection matrix is discussed by Moolgavark et al. (1984).
Now, let X , T, and W be the (2n × (k + n)) and (2n × 2n), (2n × 2n) dimensional matrices
∼ ∼
∼






X 0
(In − Y c )T Φ
0
W2 W3
,
(B.6)
=
,
=
,
=
X
T
W
∼
∼
∼
W3 W4
0 Z
0
(In − Y c )H
y ⊤ = ((y ∗ − µ∗ )⊤ , [M(y ∗ − µ∗ ) + (y † − µ† )]⊤ ) be an 1 × 2n auxiliary vector. The score
respectively, and ∼
vector corresponding to ϑ = (β ⊤ , γ ⊤ )⊤ can be written as
⊤ y
U (ϑ) = X
∼ T
∼∼ .

(B.7)

K(ϑ) = X ⊤ WX .
∼ ∼∼

(B.8)

ϑ(m+1) = (X (m)⊤ W(m) X (m) )−1 X (m) W(m) y ∗(m) ,
∼
∼
∼
∼
∼
∼

(B.9)

Fisher’s information matrix for the parameter vector ϑ is given by

Also, the iterative process for estimating ϑ takes the form

where y ∗(m) = X ϑ + W−1 T y and m = 0, 1, 2, . . . are the iterations that are performed until convergence,
∼
∼
∼ ∼∼
which occurs when the distance between ϑ(m+1) and ϑ(m) becomes smaller than a given, small constant.
The procedure is initialized by choosing suitable initial values for β and γ.
Assuming that γ is known, Fisher’s scoring iterative scheme used to estimating β can be written as
β (m+1) = (X (m)⊤ W2 (m) X (m) )−1 X (m) W2 (m) y2 (m) ,

(B.10)

where W2 is defined in (3.7) and y2 = X β + W2 −1 T Φ(y ∗ − µ∗ ), with m = 0, 1, 2, . . . . Upon convergence,
we have
c 2 Xb)−1 Xb⊤ τ,
(B.11)
βb = (Xb⊤ W

c −1 TbΦ(y
b ∗−µ
where τ = Xbβb + W
b∗ ). Then, βb in (B.11) can be viewed as the least-squares estimate of
2
β obtained by regressing τ on X with weighting matrix W2 . The ordinary residual of this re-weighted
least-squares regression is

Here,

b =W
b Tb(y ∗ − µ
b =W
c 1/2 (τ − Xbβ)
c −1/2 Φ
b∗ ).
r = (In − P)τ
2
2
c 1/2
b =W
c 1/2bX(Xb⊤ W
c 2 Xb)−1 Xb⊤ W
P
2
2

18

(B.12)

(B.13)

is a projection matrix. Note that if all the quantities are evaluated at the true parameter, E(r) = 0 and
Var(r) = (In − P). The P matrix is similar to the leverage matrix in standard linear regression models, and
hence, we refer to it as the generalized leverage matrix. It is possible to show that In − P is symmetric,
idempotent, and spans the residual r-space. This implies that a small 1 − Ptt indicates extreme points in
the design space of the continuous component of the zero-or-one inflated beta regression model. Note that
the tth element of r is
y∗ − µ
b∗
rt = p ∗
.
(B.14)
vbt (1 − α
bt )

References
Akaike, H. (1974). A new look at the statistical model identification. IEEE. Transactions on Automatic
Control, 19, 716–723.
Atkinson, A. C. (1985). Plots, Transformations and Regression: An Introduction to Graphical Methods
of Diagnostic Regression Analysis. New York: Oxford University Press.
Cook, R. D. & Weisberg, S. (1982). Residuals and Influence in Regression. London: Chapman and Hall.
Cook, D. O., Kieschnick, R. & McCullough, B. D. (2008). Regression analysis of proportions in finance
with self selection. Journal of Empirical Finance, 15, 860–867.
Cox, D. R. & Hinkley, D. V. (1974). Theoretical Statistics. London: Chapman and Hall.
Cox, D. R. & Reid, N. (1987). Parameter orthogonality and approximate conditional inference (with
discussion). Journal of the Royal Statistical Society B, 49, 1–39.
Cox, D. & Snell, E. (1968). A general definition of residuals. Journal of the Royal Statistical Society B,
30, 248–275.
Cox, D. R. & Snell, E. J. (1989). Analysis of Binary Data. London: Chapman and Hall.
Dunn, P. K. & Smyth, G. K. (1996). Randomized quantile residuals. Journal of Computational and
Graphical Statistics, 5, 236–244.
Espinheira, P. L., Ferrari, S. L. P. & Cribari–Neto, F. (2008a). Influence diagnostics in beta regression.
Computational Statistics and Data Analysis, 52, 4417–4431.
Espinheira, P. L., Ferrari, S. L. P. & Cribari–Neto, F. (2008b). On beta regression residuals. Journal of
Applied Statistics, 35, 407–419.
Fahrmeir, L. & Kaufmann, H. (1985). Consistency and asymptotic normality of the maximum likelihood
estimator in generalized linear models. Annals of Statistics, 13, 342–368.
Ferrari, S. L. P. & Cribari–Neto, F. (2004). Beta regression for modelling rates and proportions. Journal
of Applied Statistics, 7, 799–815.
Ferrari, S. L. P. & Pinheiro, E. C. (2010). Improved likelihood inference in beta regression. Journal of
Statistical Computation and Simulation. Available online. DOI: 10.1080/00949650903389993.
Hoff, A. (2007). Second stage DEA: Comparison of approaches for modelling the DEA score. European
Journal of Operational Research, 181, 425–435.
Ihaka, R. & Gentleman, R. (1996). R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5, 299–314.

19

Johnson, N., Kotz, S. & Balakrishnan, N. (1995). Continuous Univariate Distributions. 2nd ed. New
York: John Wiley and Sons.
Kieschnick, R. & McCullough, B. D. (2003). Regression analysis of variates observed on (0,1): percentages,
proportions, and fractions. Statistical Modelling, 3, 1–21.
Korhonen, L., Korhonen, K. T., Stenberg, P., Maltamo, M. & Rautiainen, M. (2007). Local models for
forest canopy cover with beta regression. Silva Fennica, 41, 671–685.
McCullagh, P. & Nelder, J. A. (1989). Generalized Linear Models, 2nd ed. London: Chapman and Hall.
McFadden, D. (1974). Conditional logit analysis of qualitative choice behavior. In: P. Zarembka, ed.,
Frontiers in Econometrics, 105–142. New York: Academic Press.
Ospina, R. (2006). The zero-inflated beta distribution for fitting a GAMLSS. Extra distributions to be
used for GAMLSS modelling. Available at gamlss.dist package. http://www.gamlss.org.
Ospina, R. & Ferrari, S. L. P. (2010). Inflated beta distributions. Statistical Papers, 51, 111–126.
Pace, L. & Salvan, A. (1997). Principles of Statistical Inference from a Neo-Fisherian Perspective. Advanced Series on Statistical Science and Applied Probability, Vol.4. Singapore: World Scientific.
Paolino, P. (2001). Maximum likelihood estimation of models with beta-distributed dependent variables.
Political Analysis, 9, 325–346.
Press, W. H., Teulosky, S. A., Vetterling, W. T. & Flannery, B. P. (1992). Numerical Recipes in C: The
Art of Scientific Computing. 2nd ed. Prentice Hall: London.
Rao, C. R. (1973). Linear Statistical Inference and Its Applications, 2nd ed. New York: Wiley.
Rigby, R. A. & Stasinopoulos, D. M. (2005). Generalized additive models for location, scale and shape
(with discussion). Applied Statistics, 54, 507–554.
Schwarz, G. (1978). Estimating the dimension of a mode. Annals of Statistics, 6, 461–464.
Simas, A. B., Barreto-Souza, W. & Rocha, A. V. (2010). Improved estimators for a general class of beta
regression models. Computational Statistics & Data Analysis, 54, 348–366.
Smithson, M. & Verkuilen, J. (2006). A better lemon squeezer? Maximum likelihood regression with beta
distributed dependent variables. Psychological Methods, 11, 54–71.
Stasinopoulos, D. M. & Rigby, R. A. (2007). Generalized additive models for location scale and shape
(GAMLSS) in R. Journal of Statistical Software, 23, 1–43.
Venables, W. N. & Ripley, B. D. (2002). Modern Applied Statistics with S, 4th ed. New York: Springer.
Wei, B. C. (1998). Exponential Family Nonlinear Models. Singapore: Springer.
Yoo, S. (2004). A note on an approximation of the mobile communications expenditures distribution
function using a mixture model. Journal of Applied Statistics, 31, 747–752.

20

