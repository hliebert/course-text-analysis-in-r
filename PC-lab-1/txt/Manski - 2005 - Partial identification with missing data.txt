International Journal of Approximate Reasoning
39 (2005) 151–165
www.elsevier.com/locate/ijar

Partial identiﬁcation with missing data:
concepts and ﬁndings
Charles F. Manski

*

Department of Economics and Institute for Policy Research, Northwestern University,
2001 Sheridan Road, Evanston, IL 60208, USA
Received 1 February 2004; accepted 1 October 2004
Available online 24 November 2004

Abstract
The traditional way to cope with missing data problems has been to combine the available
data with assumptions strong enough to point-identify the probability distribution describing
a population. However, such assumptions often are not well motivated. An alternative
approach is to ﬁrst determine what may be inferred using the empirical evidence alone and
then study the identifying power of credible assumptions. The generic result is that one may
partially identify the probability distribution of interest: an identiﬁcation region gives the
set of distributions generated by combining the available data with all possible distributions
of missing data. This expository article collects ﬁndings on partial identiﬁcation with missing
data. The focus is on identiﬁcation of means, quantiles, and other parameters that respect stochastic dominance. It is shown how distributional assumptions using instrumental variables
shrink the identiﬁcation regions for these parameters. Findings are given on conditional prediction with missing data on outcomes or covariates.
 2004 Elsevier Inc. All rights reserved.
Keywords: Bounds; Identiﬁcation; Instrumental variables; Missing data; Prediction

*

Tel.: +1 847 491 8223; fax: +1 847 491 7001.
E-mail address: cfmanski@northwestern.edu

0888-613X/$ - see front matter  2004 Elsevier Inc. All rights reserved.
doi:10.1016/j.ijar.2004.10.006

152

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

1. Introduction
Suppose that one wants to use data to draw conclusions about a population of
interest. Econometricians and statisticians have long found it useful to separately
study identiﬁcation problems and problems of statistical inference. Studies of identiﬁcation seek to characterize the conclusions that could be drawn if one were able
to observe an unlimited number of realizations of a speciﬁed sampling process. Studies of statistical inference seek to characterize the generally weaker conclusions that
can be drawn given a sample of positive but ﬁnite size. Koopmans, in [1], put it this
way in the article that introduced the term identiﬁcation (p. 132):
In our discussion we have used the phrase ‘‘a parameter that can be determined
from a suﬃcient number of observations’’. We shall now deﬁne this concept
more sharply, and give it the name identiﬁability of a parameter. Instead of reasoning, as before, from ‘‘a suﬃciently large number of observations’’ we shall
base our discussion on a hypothetical knowledge of the probability distribution
of the observations, as deﬁned more fully below. It is clear that exact knowledge of this probability distribution cannot be derived from any ﬁnite number
of observations. Such knowledge is the limit approachable but not attainable
by extended observation. By hypothesizing nevertheless the full availability
of such knowledge, we obtain a clear separation between problems of statistical
inference arising from the variability of ﬁnite samples, and problems of identiﬁcation in which we explore the limits to which inference even from an inﬁnite
number of observations is suspect.
Statistical and identiﬁcation problems limit in distinct ways the conclusions that
may be drawn in empirical research. Statistical problems are most severe when the
available sample is small. Identiﬁcation problems are most severe when the researcher knows little about the population under study and the sampling process yields
only weak data on the population.
A leading source of identiﬁcation problems is missing data. Suppose that each
member j of a population J has an outcome yj in a space Y. The population is a probability space (J, X, P) and y: J ! Y is a random variable with distribution P(y). Suppose that a sampling process draws persons at random from J. However, not all
realizations of y are observable. Let the realization of a binary random variable z
indicate observability; y is observable if z = 1 and not observable if z = 0.
By the Law of Total Probability
P ðyÞ ¼ P ðy j z ¼ 1ÞP ðz ¼ 1Þ þ P ðy j z ¼ 0ÞP ðz ¼ 0Þ:

ð1Þ

The sampling process reveals P(yjz = 1) and P(z), but is uninformative regarding
P(yjz = 0). Hence, the empirical evidence partially identiﬁes P(y). In particular, it
reveals that P(y) lies in the identiﬁcation region
H ½P ðyÞ

½P ðy j z ¼ 1ÞP ðz ¼ 1Þ þ cP ðz ¼ 0Þ;

c 2 CY ;

ð2Þ

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

153

where CY is the space of all probability measures on Y. This set of probability distributions has the same structure as the ones generated in other settings by the
gross-error model of [2] and the linear-vacuous mixture model of [3].
The identiﬁcation region H[P(y)] is a proper subset of CY whenever the probability P(z = 0) of missing data is less than one, and is a singleton when
P(z = 0) = 0. Hence, P(y) is partially identiﬁed when 0 < P(z = 0) < 1 and is pointidentiﬁed when P(z = 0) = 0. In any case, an empirical researcher having a random
sample of ﬁnite size N may use the empirical distributions PN(yjz = 1) and PN(z)
to estimate P(yjz = 1) and P(z) respectively. Hence, a natural estimate of H[P(y)]
is [PN(yjz = 1)PN(z = 1) + cPN(z = 0), c 2 CY].
The traditional way to cope with missing data problems has been to combine the
available data with assumptions strong enough to yield point identiﬁcation. Such
assumptions often are not well motivated, and empirical researchers debate their
validity. In contrast, I have recommended that researchers ﬁrst ask what may be inferred about the distribution of interest using the empirical evidence alone, and then
ask how the identiﬁcation region shrinks if certain assumptions are imposed.
Beginning with the empirical evidence alone establishes a domain of consensus
among researchers who may hold disparate beliefs about what assumptions are
appropriate [4]. It also makes plain The Law of Decreasing Credibility posed in
[5] (p. 1):
The Law of Decreasing Credibility: The credibility of inference decreases with the
strength of the assumptions maintained.
This principle implies that empirical researchers face a dilemma as they decide
what assumptions to maintain: stronger assumptions yield inferences that may be
more powerful but less credible.
This expository paper summarizes ﬁndings on partial identiﬁcation with missing
data collected in Chapters 1 through 3 of [5]. Section 2 shows how the identiﬁcation
region H[P(y)] generates identiﬁcation regions for means, quantiles, and other
parameters that respect stochastic dominance. Section 3 shows how some distributional assumptions using instrumental variables shrink these identiﬁcation regions. Section 4 studies conditional prediction when data on outcomes or
covariates may be missing. The numbering of propositions in the paper corresponds
to the numbering in [5], where all propositions are proved.
Many of the ﬁndings discussed here originally appeared in research articles published in economics and statistics journals; see [6–10]. Related ﬁndings not discussed
here are reported in [11–14]. Several articles related to partial identiﬁcation with
missing data have appeared recently in artiﬁcial intelligence and computing journals;
see [15–20].

2. Identiﬁcation of population parameters using the empirical evidence alone
The identiﬁcation region H[P(y)] states what may be learned about a probability
distribution P(y) when data are generated by random sampling and some outcome

154

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

realizations are not observable. A common objective of empirical research is to infer
particular parameters of P(y). Let s(Æ): CY ! T map probability distributions on Y
into a space T and consider inference on the parameter s[P(y)]. The identiﬁcation region for s[P(y)] is
H fs½P ðyÞg ¼ fsðgÞ; g 2 H ½P ðyÞg:

ð3Þ

In what follows, I characterize H{s[P(y)]} when s is the mean of a function of y
(Section 2.1) or a parameter that respects stochastic dominance (Section 2.2).
2.1. Means of functions of y
Let R [ 1, 1] be the extended real line, which closes the ordinary real line by
including its limits 1 and 1 as elements of R. Let G be the space of functions that
map Y into R and that attain their lower and upper bounds g0 infy2Yg(y) and
g1 supy2Yg(y). Let the problem of interest be to infer the expectation E[g(y)] using
only the empirical evidence. The Law of Iterated Expectations gives
E½gðyÞ ¼ E½gðyÞ j z ¼ 1P ðz ¼ 1Þ þ E½gðyÞ j z ¼ 0P ðz ¼ 0Þ:

ð4Þ

The sampling process reveals E[g(y)jz = 1] and P(z), but is uninformative regarding E[g(y)jz = 0], which can take any value in the interval [g0, g1]. We therefore have
the very simple
Proposition 1.1. Let g 2 G. Given the empirical evidence alone, the identification
region for E[g(y)] is the closed interval
H fE½gðyÞg ¼ ½E½gðyÞ j z ¼ 1P ðz ¼ 1Þ þ g0 P ðz ¼ 0Þ;
E½gðyÞ j z ¼ 1P ðz ¼ 1Þ þ g1 P ðz ¼ 0Þ:

ð5Þ

The interval H{E[g(y)]} is a proper subset of [g0, g1], and hence informative, whenever the probability P(z = 0) of missing data is less than one and g(Æ) has ﬁnite range.
The width of the region is (g1 g0)P(z = 0). Thus, the severity of the identiﬁcation
problem varies directly with the probability P(z = 0) of missing data.
The situation changes if g0 = 1 or g1 = 1. The identiﬁcation region is [ 1,
E[g(y)jz = 1]P(z = 1) + g1P(z = 0)] in the former case and [E[g(y)jz = 1]P(z = 1) +
g0P(z = 0),1] in the latter. In both cases, the region remains informative but has
inﬁnite length. The region is [ 1, 1] if g is unbounded from both below and above.
Thus, credible assumptions are a prerequisite for inference on the mean of an unbounded random variable if there is any missing data at all.
Proposition 1.1 has many applications. Perhaps the most far reaching is the identiﬁcation region it implies for the probability that y lies in any non-empty, proper set
B  Y. Let gB(Æ) be the indicator function gB(y) 1[y 2 B]; that is, gB (y) = 1 if y 2 B
and gB(y) = 0 otherwise. Then gB (Æ) attains its lower and upper bounds on Y, these

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

155

being 0 and 1. Moreover, E[gB(y)] = P(y 2 B) and E[gB(y)jz = 1] = P(y 2 Bjz = 1).
Hence, Proposition 1.1 has this corollary.
Corollary 1.1.1. Let B be a non-empty, proper subset of Y. Given the empirical
evidence alone, the identification region for P(y 2 B) is the closed interval
H ½P ðy 2 BÞ ¼ ½P ðy 2 B j z ¼ 1ÞP ðz ¼ 1Þ; P ðy 2 B j z ¼ 1ÞP ðz ¼ 1Þ þ P ðz ¼ 0Þ:
ð6Þ
Illustration. Horowitz and Manski [9] used data from the National Longitudinal
Survey of Youth (NLSY) to estimate the probability that a member of the surveyed
population is employed in 1991. In the 1979 base year of the survey, the NLSY
sought to interview a random sample of 6812 individuals and succeeded in obtaining
interviews from 6111 of the sample members. Data on employment status in 1991 are
available for 5556 of the 6111 individuals interviewed in the base year. The remaining 555 are nonrespondents, some because they declined to be interviewed in
1991 and some because they did not answer the employment-status question in their
1991 interviews. Table 1 presents these response statistics and the frequencies with
which diﬀerent outcome values are reported.
We use these frequencies to generate empirical probabilities of events and use
these empirical probabilities as ﬁnite-sample estimates of population quantities.
The empirical nonresponse rate, which takes account of sample members who were
never interviewed, is PN(z = 0) = 1256/6812 = 0.184. The empirical probability of
employment for the 5556 persons who responded to the 1991 employment-status
question is PN(y = 2jz = 1) = 4332/5556 = 0.780. The probability of employment
among nonrespondents can take any value in the interval [0, 1]. Hence, Corollary
1.1.1 yields this estimate of H[P(y = 2)]: [(0.780)(0.816), (0.780)(0.816) + (0.184)] =
[0.636, 0.820].
2.2. Parameters that respect stochastic dominance
Let CR denote the space of all probability distributions on R = [ 1, 1]. Let P
and P 0 be any two members of CR. Distribution P is said to stochastically dominate

Table 1
1991 Employment status of NLSY respondents
Employment status

Number of respondents

Employed (y = 2)
Unemployed (y = 1)
Out of labor force (y = 0)
Ever-interviewed non-respondents
Never-interviewed non-respondents

4332
297
927
555
701

Total

6812

156

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

P 0 if all left-tail probabilities of P are no larger than the corresponding left-tail probabilities of P 0 ; that is, if P[ 1, t] 6 P 0 [ 1, t] for all t 2 R. An extended real-valued
function D(Æ): CR ! R is said to respect stochastic dominance if D(P) P D(P 0 ) whenever P stochastically dominates P 0 .
The venerable concept of stochastic dominance formalizes the loose notion that
one probability distribution may be ‘‘larger’’ than another. Ref. [21] introduced
the class of parameters that respect stochastic dominance. Leading examples are
the mean and quantiles of real random variables. If distribution P stochastically
dominates P 0 , the mean of a random variable distributed P must be at least as large
as that of one distributed P 0 . Similarly, the a-quantile of a random variable distributed P must be at least as large as that of one distributed P 0 , for all a 2 (0, 1).
Spread parameters such as the variance or interquartile range do not respect stochastic dominance.
The reasoning underlying Proposition 1.1 extends to all parameters that respect
stochastic dominance. The smallest feasible value of such a parameter if obtained
by making the distribution P[g(y)jz = 0] of missing data as ‘‘small’’ as possible in
the sense of stochastic dominance; that is, by letting P[g(y)jz = 0] have all of its mass
at the point g0. Similarly, the largest value of the parameter is obtained by letting
P[g(y)jz = 0] have all of its mass at g1. This gives
Proposition 1.2. Let D respect stochastic dominance. Let g 2 G. Let
Rg [g(y), y 2 Y] be the range set of g. Let Cg be the space of probability
distributions on Rg. Let c0g 2 Cg and c1g 2 Cg be the degenerate distributions that place
all mass on g0 and g1 respectively. Given the empirical evidence alone, the smallest
and largest points in the identification region for D{P[g(y)]} are D{P[g(y)jz =
1]P(z = 1) + c0gP(z = 0)} and D{P[g(y)jz = 1]P(z = 1) + c1gP(z = 0)}.
This proposition determines sharp lower and upper bounds on D{P[g(y)]}, but it
does not assert that the identiﬁcation region is the entire interval connecting these
bounds. Proposition 1.1 showed that the identiﬁcation region is this interval if D
is the expectation parameter. However, the interval may contain non-feasible values
if D is another parameter that respects stochastic dominance. A particularly simple
example occurs when g(y) is a binary random variable and D is a quantile of P[g(y)].
A quantile must be an element of the range set Rg. Hence, D{P[g(y)]} cannot take a
value in the interior of the interval [0, 1].
More generally, the a-quantile of P[g(y)] is deﬁned to be Qa[g(y)] min t:
{P[g(y) 6 t] P a}. Proposition 1.2 shows that the smallest feasible value of Qa
[g(y)] is the a-quantile of P[g(y)jz = 1]P(z = 1) + c0gP(z = 0), and the largest is the
a-quantile of P[g(y)jz = 1]P(z = 1) + c1gP(z = 0). For any value of a, the lower and
upper bounds are generically informative if P(z = 1) > 1 a and P(z = 1) P a,
respectively. This holds whether or not g has ﬁnite range. This shows that the implications of missing data for inference on quantiles are quite diﬀerent from the implications for inference on means; the bound for a mean given in Proposition 1.1 is a
ﬁnite interval only if g has ﬁnite range.

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

157

3. Distributional assumptions using instrumental variables
Distributional assumptions may enable one to shrink identiﬁcation regions obtained using the empirical evidence alone. One may assert that the distribution
P(yjz = 0) of missing outcomes lies in some set C0Y  CY. Then the identiﬁcation region shrinks from H[P(y)] to
H 1 ½P ðyÞ

½P ðy j z ¼ 1ÞP ðz ¼ 1Þ þ cP ðz ¼ 0Þ;

c 2 C0Y :

ð7Þ

Or one may assert that the distribution of interest, P(y), lies in some set
H0[P(y)]  CY. Then the identiﬁcation region shrinks from H[P(y)] to
H 1 ½P ðyÞ

H 0 ½P ðyÞ \ H ½P ðyÞ:

ð8Þ

Assumptions of the former type are not empirically testable because, after all, the
empirical evidence reveals nothing about P(yjz = 0). Assumptions of the latter type
may be empirically testable: if the intersection of H0[P(y)] and H[P(y)] should be
empty, then P(y) cannot lie in H0[P(y)].
Many distributional assumptions make use of instrumental variables. Suppose
that each person j is characterized by a covariate vj in a space V. Let v: J ! V be
the random variable mapping persons into covariates and let P(y, z, v) denote the
joint distribution of (y, z, v). Suppose that all realizations of v are observable. Observability of v provides an instrument or tool that may help to identify the outcome distribution P(y). Thus v is said to be an instrumental variable.
The presence of an instrumental variable does not, per se, help to identify P(y).
However, observability of v may be useful when combined with distributional
assumptions. This section shows how. For simplicity, I assume that the space V is
ﬁnite, with P(v = v) > 0, all v 2 V.
Section 3.1 assumes that y is statistically independent of z conditional on v; that is,
outcomes are missing-at-random conditional on v.
Outcomes missing-at-random ðassumption MARÞ :
P ðy j v; z ¼ 0Þ ¼ P ðy j v; z ¼ 1Þ:
This assumption is sometimes stated as P(zjv, y) = P(zjv), but the equivalent form
given above is more convenient for present purposes. Section 3.2 assumes that y is
statistically independent of v; that is:
Statistical independence of outcomes and instrumentsðassumption SIÞ :
P ðy j vÞ ¼ P ðyÞ:
Other distributional assumptions that use instrumental variables are studied in [5],
Chapter 2.
3.1. Outcomes missing-at-random
Assumption MAR point-identiﬁes P(y) and is not empirically testable. Proposition 2.1 gives the result.

158

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

Proposition 2.1. Let assumption MAR hold. Then P(y) is point-identified with
X
P ðyÞ ¼
P ðy j v ¼ v; z ¼ 1ÞP ðv ¼ vÞ:

ð9Þ

v2V

Assumption MAR is not empirically testable.
P
To show (9), use the Law of Total Probability to write P ðyÞ ¼ v2V P ðy j v ¼
vÞP ðv ¼ vÞ. Assumption MAR states that P(yjv) = P(yjv, z = 1); hence, (9) holds.
Assumption MAR is not empirically testable because the empirical evidence reveals
nothing about P(yjv, z = 0).
Proposition 2.1 has long been well-known, so much so that it is unclear when the
idea originated. Whatever v may be, the credibility of assumption MAR is regularly
a matter of controversy. Empirical researchers sometimes assert that the assumption
becomes more credible as the instrumental variable partitions the population into
more reﬁned sub-populations. That is, if v1 and v2 are alternative speciﬁcations of
the instrumental variable, with P(v1jv2) degenerate, researchers may assert that v2
is a more credible instrumental variable than is v1. Unfortunately, this assertion typically is backed up by nothing more than the empty statement that v2 ‘‘controls for’’
more determinants of missing data than v1. In principle, assumption MAR could
hold for both, either, or neither of v1 and v2.
3.2. Statistical independence
Assumption SI usually does not point-identify P(y), but it is empirically testable.
Proposition 2.2 gives the basic result and two corollaries ﬂesh it out.
Proposition 2.2. (a) Let assumption SI hold. Then the identification region for P(y) is
\
H SI ½P ðyÞ ¼ fP ðy j v ¼ v; z ¼ 1ÞP ðz ¼ 1 j v ¼ vÞ
v2V

þ cv  P ðz ¼ 0 j v ¼ vÞ;

cv 2 CY g:

ð10Þ

(b) Let the set HSI[P(y)] be empty. Then assumption SI does not hold.
Part (a) holds because, for each v 2 V, the empirical evidence alone places
P(yjv = v) in the identiﬁcation region {P(yjv = v, z = 1)P(z = 1jv = v) + cv Æ P(z = 0jv
= v), cv 2 CY}. Assumption SI states that P(yjv = v) is the same for all values of v.
Hence, the common value P(y) must lie in the intersection of these regions.
The identifying power of assumption SI can range from point identiﬁcation of P(y)
to no power at all, depending on the nature of the instrumental variable. Point identiﬁcation occurs if there exists a v 2 V such that P(z = 1jv = v) = 1; then one of the sets
whose intersection is taken in (10) is a singleton. The assumption has no identifying
power if v is unrelated to (y, z) in the sense that (a) z is statistically independent of v
and (b) y is statistically independent of v conditional on the event {z = 1}; that is, if
P(zjv) = P(z) and P(yjv, z = 1) = P(yjz = 1). Then H[P(yjv = v)], v 2 V are all the same
as the identiﬁcation region obtained using the empirical evidence alone.

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

159

Part (b) shows that assumption SI is empirically testable. If HSI[P(y)] is empty,
the assumption logically cannot hold. Of course, non-emptiness of HSI[P(y)] does
not imply that the assumption is correct.
Proposition 2.2 is simple in form but is too abstract to communicate much about
the size and shape of the identiﬁcation region. Corollary 2.2.1 gives a useful alternative characterization of the region. Part (a) shows that a distribution is a feasible
value for P(y) if and only if the probability that it places on each subset of Y is no
less than an easily computed lower bound. This characterization further simpliﬁes
when Y is countable. Then, part (b) shows that one need only consider the probability placed on each atom of Y. This ﬁnding yields a simple necessary and suﬃcient
condition for existence of a unique feasible distribution, given in part (c). See [5], Section 2.4 for the proof of this corollary and of the subsequent Corollary 2.2.2.
Corollary 2.2.1. Let assumption SI hold. Let g 2 CY. For BY, define
pV ðBÞ max P ðy 2 B j v ¼ v; z ¼ 1ÞP ðz ¼ 1 j v ¼ vÞ:
v2V

ð11Þ

(a) Then g 2 HSI[P(y)] if and only if g(B) P pV (B), "BY.
(b) Let Y be countable. Then g 2P
HSI[P(y)] if and only if g(y) P pV(y), "y 2 Y.
(c) Let Y be countable. Let S V
y2Y pV ðyÞ. Then HSI[P(y)] contains multiple distributions if SV < 1 and a unique distribution if SV = 1. If SV = 1, the unique feasible
distribution is gV(y) pV(y), y 2 Y. If SV > 1, then assumption SI does not hold.
When Y is a countable subset of the real line, Corollary 2.2.1 implies simple characterizations of the identiﬁcation regions for parameters that respect stochastic dominance. Corollary 2.2.2 gives the result. Part (a) determines the endpoints of the
identiﬁcation region for any parameter that respects stochastic dominance. Part
(b) focuses on the expectation parameter and shows that its identiﬁcation region is
the closed interval connecting the endpoints determined in part (a).
Corollary 2.2.2. Let assumption SI hold. Let Y be a countable subset of R, and let Y
contain its lower and upper bounds y0 infy2Y and y1 supy2Y. Let g0 and g1 be
probability distributions on Y such that, for each y 2 Y,
g0 ðyÞ ¼ pV ðyÞ if y > y 0 and g0 ðy 0 Þ ¼ pV ðy 0 Þ þ ð1 S V Þ;
ð12aÞ
g1 ðyÞ ¼ pV ðyÞ if y < y 1 and g1 ðy 1 Þ ¼ pV ðy 1 Þ þ ð1

S V Þ:

ð12bÞ

(a) Let D respect stochastic dominance. Then the smallest and largest elements of
HSI{D[P(y)]} are D(g0) and D(g1).
(b) The closed interval
"
#
X
X
H SI ½EðyÞ ¼
ypV ðyÞ þ ð1 S V Þy 0 ;
ypV ðyÞ þ ð1 S V Þy 1
ð13Þ
y2Y

is the identification region for E(y).

y2Y

160

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

4. Conditional prediction with missing data
A large part of statistical practice aims to predict outcomes conditional on covariates. In research on artiﬁcial intelligence, a covariate may be called a feature or
attribute. When the outcome to be predicted is categorical, it may be called a class.
The prediction problem may be called pattern classiﬁcation.
Suppose that each member j of population J has an outcome yj in a space Y and a
covariate xj in a space X. Let the random variable (y, x): J ! Y · X have distribution
P(y, x). In general terms, the objective is to learn the conditional distributions
P(yjx = n), n 2 X. A particular objective may be to learn the conditional expectation
E(yjx = n), conditional median M(yjx = n), or another point predictor of y conditional on an event {x = n}.
Suppose that a sampling process draws persons at random from J and realizations
of (y, x) may be observable in whole, in part, or not at all. Two binary random variables (zy, zx) now indicate observability. A realization of y is observable if zy = 1 but
not if zy = 0; a realization of x is observable if zx = 1 but not if zx = 0. The sampling
process reveals the distributions P(zy, zx), P(y, xjzy = 1, zx = 1), P(yjzy = 1, zx = 0),
and P(xjzy = 0, zx = 1). The problem is to use this empirical evidence to infer
P(yjx = n), n 2 X.
In practice, empirical researchers may face complex patterns of missing data;
some sample members may have missing outcome data, others may have missing
covariate data, and others may have jointly missing outcomes and covariates. Nevertheless, it is instructive to study the polar cases in which all missing data are of the
same type. Section 4.1 brieﬂy reviews the case in which only outcome data are missing. Section 4.2 supposes that all sample members with missing data have jointly
missing outcomes and covariates. Section 4.3 supposes that only covariate data
are missing. With these polar cases understood, Section 4.4 gives an empirical illustration with a general pattern of missing data. Finally, Section 4.5 considers parametric prediction with missing data. For simplicity, I assume that the space X is
ﬁnite, with P(x = n) > 0, n 2 X.
The present discussion focuses on identiﬁcation using the empirical evidence
alone. [5], Chapter 3 presents ﬁndings using distributional assumptions as well.
4.1. Missing outcomes
Recall identiﬁcation of P(y) when some realizations of y are missing. The results
in Sections 2 and 3 apply immediately to P(yjx = n) if realizations of x are always
observable. One simply needs to redeﬁne the population of interest to be the subpopulation of J for which {x = n}. In particular, the identiﬁcation region using the
empirical evidence alone is
H ½P ðy j x ¼ nÞ ¼ ½P ðy j x ¼ n; zy ¼ 1ÞP ðzy ¼ 1 j x ¼ nÞ
þ cP ðzy ¼ 0 j x ¼ nÞ; c 2 CY :

ð14Þ

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

161

4.2. Jointly missing outcomes and covariates
Jointly missing outcomes and covariates is a regular occurrence in survey research. Realizations of (y, x) may be missing in their entirety when sample members
refuse to be interviewed or cannot be contacted by survey administrators. Joint missingness also occurs when outcomes are missing and the objective is to learn a distribution of the form P(yjy 2 B), where B  Y. If the outcome y is not observed, then
the conditioning event {y 2 B} necessarily is not observed.
Let zyx = 1, if zy = zx = 1 and zyx = 0 otherwise. Proposition 3.1 gives the identiﬁcation region using the empirical evidence alone.
Proposition 3.1. Let P(zy = zx = 1) + P(zy = zx = 0) = 1. Then
H ½P ðy j x ¼ nÞ ¼ fP ðy j x ¼ n; zyx ¼ 1ÞrðnÞ þ c½1

rðnÞ;

c 2 CY g;

ð15aÞ

where
rðnÞ

P ðx ¼ n j zyx ¼ 1ÞP ðzyx ¼ 1Þ
:
P ðx ¼ n j zyx ¼ 1ÞP ðzyx ¼ 1Þ þ P ðzyx ¼ 0Þ

ð15bÞ

The present identiﬁcation region for P(yjx = n) has the same form as the region
(14) obtained when only realizations of y are missing, except that r(n) here replaces
P(zy = 1jx = n) there. The quantity r(n) is the smallest feasible value of P(zyx = 1jx =
n). It is obtained by using Bayes theorem to write

P ðzyx ¼ 1 j x ¼ nÞ ¼

P ðx ¼ n j zyx ¼ 1ÞP ðzyx ¼ 1Þ
P ðx ¼ n j zyx ¼ 1ÞP ðzyx ¼ 1Þ þ P ðx ¼ n j zyx ¼ 0ÞP ðzyx ¼ 0Þ
ð16Þ

and by conjecturing that all missing covariate values equal n; that is, by setting
P(x = njzyx = 0) = 1. Thus, joint missingness of (y, x) exacerbates the identiﬁcation
problem produced by missingness of y alone.
With r(n) replacing P(z = 1jx = n) and zyx replacing z, all results obtained in Section 2 hold when realizations of (y, x) are jointly missing. In particular, Proposition
1.2 has this analog.
Proposition 3.2. Let D respect stochastic dominance. Let g 2 G. Let
P(zy = zx = 1) + P(zy = zx = 0) = 1. Then the smallest and largest points in the
identification region for D{P[g(y)jx = n]} are D{P[g(y)jzyx = 1]r(n) + c0g[1 r(n)]}
and D{P[g(y)jzyx = 1]r(n) + c1g[1 r(n)]}.

4.3. Missing covariates
Suppose now that realizations of y are always observed but realizations of x may
be missing. In this case, the identiﬁcation region does not have a simple form

162

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

comparable to those derived earlier. Proposition 3.5 gives the identiﬁcation region
for P(yjx = n) using the empirical evidence alone. See [5], Section 3.4 for the proof.
Proposition 3.5. Let P(zy = 1) = 1. Then

H ½P ðy jx ¼ nÞ ¼ [p2½0;1 P ðy j x ¼ n; zx ¼ 1Þ

P ðx ¼ n j zx ¼ 1ÞP ðzx ¼ 1Þ
P ðx ¼ n j zx ¼ 1ÞP ðzx ¼ 1Þ þ pP ðzx ¼ 0Þ

pP ðzx ¼ 0Þ
; g 2 CY ðpÞ ;
þg
ð17Þ
P ðx ¼ n j zx ¼ 1ÞP ðzx ¼ 1Þ þ pP ðzx ¼ 0Þ

where CY(p)

CY \ {[P(yjzx = 0)

c(1

p)]/p, c 2 CY}.

Missing covariates pose a less severe observational problem than do jointly missing outcomes and covariates. Hence, the identiﬁcation region derived in Proposition
3.5 necessarily is a subset of the one obtained in Proposition 3.1. Indeed, P(yjx = n)
may even be point identiﬁed. This happens if P(yjx = n, zx = 1) = P(yjzx = 0), and
these distributions are degenerate. Then region (17) contains only one element,
P(yjx = n, zx = 1).
4.4. General missing data patterns: an empirical illustration
A proposition covering general missing data patterns can be developed by combining the ﬁndings of Sections 4.2–4.4. See [5], Chapter 3 for details. Here I present
an empirical illustration drawn from [10], which analyzes data from a randomized
trial comparing treatments for hypertension.
In the trial in question, patients were randomly assigned to one of six antihypertensive drug treatments (treatments 1–6) or to a placebo (treatment 7). Treatment
was deemed by the investigators to be successful if diastolic blood pressure (DBP)
was less than 90 mmHg on two consecutive measurements in the ﬁrst phase of treatment and less than 95 mmHg in the second. Treatment was unsuccessful otherwise.
Thus, the outcome of interest was binary, with y = 1 if the criterion for success was
met and y = 0 otherwise.
Among the measured covariates, one was the biochemical indicator ‘‘renin response,’’ taking the values n = (low, medium, high). Renin-response data were missing for some patients in the trial. Moreover, some patients dropped out of the trial
before their outcomes could be determined. The observed outcomes and the pattern
of missing covariate and outcome data are shown in Table 2.
Table 3 uses these data to estimate the identiﬁcation regions for the success probabilities for each treatment. These estimates are computed using the empirical evidence alone, making no assumptions about the distribution of missing outcome
and covariate data.
Consider a physician who accepts the success criterion, observes renin response,
and has no other information on mean treatment response or the distribution of
missing data. Suppose that all treatments have the same cost. How might this physician choose treatments in a population similar to that studied in the trial?

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

163

Table 2
Data in the hypertension trial
Treatment

Number
randomized

Observed
successes

None
missing

Missing
only y

Missing
only x

Missing
y and x

1
2
3
4
5
6
7

188
178
188
178
185
188
187

100
106
96
110
130
97
57

173
158
169
159
164
164
178

4
11
6
5
6
12
3

11
9
13
13
14
10
6

0
0
0
1
1
2
0

Table 3
Identiﬁcation regions for success probabilities conditional on renin response
Renin
response

Treatment
1

2

3

4

5

6

7

Low
Medium
High

[0.54, 0.61]
[0.47, 0.62]
[0.28, 0.50]

[0.52, 0.62]
[0.60, 0.74]
[0.64, 0.86]

[0.43, 0.53]
[0.53, 0.68]
[0.56, 0.75]

[0.58, 0.66]
[0.50, 0.69]
[0.63, 0.84]

[0.66, 0.76]
[0.68, 0.85]
[0.55, 0.78]

[0.54, 0.65]
[0.41, 0.65]
[0.34, 0.59]

[0.29, 0.32]
[0.27, 0.32]
[0.28, 0.40]

Clearly, the physician should eliminate the dominated treatments; that is, the ones
that cannot possibly be optimal, whatever the distribution of missing data may be.
Table 3 shows that for patients with low renin response, treatments 1–4, 6, and 7
are all dominated by treatment 5, which has the greatest lower bound (0.66). For patients with medium renin response, treatments 1, 3, 6, and 7 are dominated by treatment 5, which again has the greatest lowest bound (0.68). For patients with high
renin response, treatments 1, 6, and 7 are dominated by treatment 2, which has
the greatest lowest bound in this case (0.64). Thus, without any knowledge of the
nature of the missing data, the physician can reject treatments 1, 6, and 7 for all patients, can reject treatment 3 for patients with medium renin response, and can determine that treatment 5 is optimal for patients with low renin response.
In the absence of assumptions about the distribution of missing data, it is not possible to give the physician guidance on how to choose among undominated treatments for patients with medium and high renin response. A physician using the
maximin rule would choose treatment 5 for patients with medium renin response
and treatment 2 for patients with high renin response. This is a reasonable treatment
rule, but one cannot say that it is an optimal rule.
4.5. Parametric prediction with missing data
The above discussion has concerned nonparametric prediction of outcomes conditional on covariates. Researchers often specify a parametric family of predictor

164

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

functions and seek to infer a member of this family that minimizes expected loss with
respect to some loss function. Let the outcome y be real-valued. Let H be the parameter space and f(Æ, Æ): X · H ! R be the family of predictor functions. Let L(Æ):
R ! [0, 1] be the loss function. Then the objective is to ﬁnd a h* 2 H such that
h 2 arg minh2H EfL½y

f ðx; hÞg:

ð18Þ

The function f(Æ, h*) is called a best f(Æ, Æ)-predictor of y given x under loss function
L. For example, in the familiar problem of best linear prediction under square loss,
f(x, h) = x 0 h and L[y f(x, h)] = (y x 0 h)2. As is well known, h* = E(xx 0 ) 1E(xy),
provided that E(xx 0 ) is non-singular.
The identiﬁcation region for h* is the set of parameter values that minimize expected loss under some feasible distribution for the missing data. This set can be
determined by brute-force solution of the best-prediction problem (18) for all possible data conﬁgurations. This gives
[
H ðh Þ ¼
farg minh2H P ðzyx ¼ 1ÞEfL½y f ðx; hÞ j zyx ¼ 1g
ðg10 ;g00 ;g01 Þ2C10 C00 C01

Z

þ P ðzx ¼ 1; zy ¼ 0Þ  L½y f ðx; hÞdg10 þ P ðzx ¼ 0; zy ¼ 0Þ
Z
Z
 L½y f ðx; hÞdg00 þ P ðzx ¼ 0; zy ¼ 1Þ  L½y f ðx; hÞdg01 g:

ð19Þ

Here P(zyx = 1) is the probability that (y, x) are both observed, P(zx = 1, zy = 0)
that x is observed but not y, P(zx = 0, zy = 0) that (y, x) are both missing, and
P(zx = 0, zy = 1) that y is observed but not x. C10 is the set of all distributions on
Y · X with x-marginal P(xjzx = 1, zy = 0), C00 is the set of all distributions on
Y · X, and C01 is the set of all distributions on Y · X with y-marginal
P(yjzx = 0, zy = 1).
The natural estimate of H(h*) is its sample analog, which uses the empirical distribution of the data to estimate P(zyx), P[(y, x)jzyx = 1], P(xjzx = 1,zy = 0), and
P(yjzx = 0, zy = 1). However, computation of this estimate can pose a considerable
challenge. This is so even in the relatively benign setting of best linear prediction
under square loss, where the sample analog of H(h*) is the set of least squares estimates produced by conjecturing all possible values for missing outcome and covariate data; see [18].

Acknowledgments
This expository article draws on the introduction and ﬁrst three chapters of my
monograph Partial Identiﬁcation of Probability Distributions (Springer-Verlag,
2003). The article expands on a tutorial I presented at the ISIPTA 03 conference,
July 2003. This work was supported in part by National Science Foundation grant
SES-0314312. I am grateful for the comments of Marco Zaﬀalon and two anonymous reviewers.

C.F. Manski / Internat. J. Approx. Reason. 39 (2005) 151–165

165

References
[1] T. Koopmans, Identiﬁcation problems in economic model construction, Econometrica 17 (1) (1949)
125–144.
[2] P. Huber, Robust estimation of a location parameter, Ann. Math. Stat. 35 (1) (1964) 73–101.
[3] P. Walley, Statistical reasoning with imprecise probabilities, Chapman & Hall, London, 1991.
[4] C. Manski, Identiﬁcation Problems in the Social Sciences, Harvard University Press, Cambridge,
MA, 1995.
[5] C. Manski, Partial Identiﬁcation of Probability Distributions, Springer-Verlag, New York, 2003.
[6] C. Manski, Anatomy of the selection problem, J. Human Res. 24 (3) (1989) 343–360.
[7] C. Manski, Nonparametric bounds on treatment eﬀects, Am. Econ. Rev. Papers Proc. 80 (1990)
319–323.
[8] C. Manski, The Selection Problem, in: C. Sims (Ed.), Advances in Econometrics, Sixth World
Congress, Cambridge University Press, Cambridge, 1994, pp. 143–170.
[9] J. Horowitz, C. Manski, Censoring of outcomes and regressors due to survey nonresponse:
identiﬁcation and estimation using weights and imputations, J. Econom. 84 (1) (1998) 37–58.
[10] J. Horowitz, C. Manski, Nonparametric analysis of randomized experiments with missing covariate
and outcome data, J. Am. Stat. Assoc. 95 (449) (2000) 77–84.
[11] C. Manski, J. Pepper, Monotone instrumental variables: with an application to the returns to
schooling, Econometrica 68 (4) (2000) 997–1010.
[12] C. Manski, E. Tamer, Inference on regressions with interval data on a regressor or outcome,
Econometrica 70 (2) (2002) 519–546.
[13] D. Scharfstein, C. Manski, J. Anthony, On the construction of bounds in prospective studies with
missing ordinal outcomes: application to the good behavior game trial, Biometrics 60 (2004) 154–164.
[14] M. Zaﬀalon, Exact credal treatment of missing data, J. Stat. Planning Inference 105 (1) (2002)
105–122.
[15] S. Acid, L. de Campos, J. Huete, Estimating probability values from an incomplete dataset, Int. J.
Approx. Reasoning 27 (2001) 183–204.
[16] G. de Cooman, M. Zaﬀalon, Updating beliefs with incomplete observations, Artiﬁ. Intell. 159 (1–2)
(2004) 75–125.
[17] P. Grunwald, J. Halpern, Updating probabilities, J. Artif. Intell. Res. 19 (2003) 243–278.
[18] J. Horowitz, C. Manski, M. Ponomareva, J. Stoye, Computation of bounds on population
parameters when the data are incomplete, Reliable Comput. 9 (6) (2003) 419–440.
[19] M. Ramoni, P. Sebastiani, Robust learning with missing data, Mach. Learn. 45 (2001) 147–170.
[20] M. Ramoni, P. Sebastiani, Robust Bayes classiﬁers, Artiﬁ. Intell. 125 (2001) 209–226.
[21] J. Horowitz, C. Manski, Identiﬁcation and robustness with contaminated and corrupted data,
Econometrica 63 (1) (1995) 281–302.

