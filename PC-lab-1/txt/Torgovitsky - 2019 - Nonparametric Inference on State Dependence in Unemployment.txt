Econometrica Supplementary Material

SUPPLEMENT TO “NONPARAMETRIC INFERENCE ON STATE DEPENDENCE
IN UNEMPLOYMENT”
(Econometrica, Vol. 87, No. 5, September 2019, 1475–1505)
ALEXANDER TORGOVITSKY
Department of Economics, University of Chicago
This supplement contains: (i) Discussions of extending the DPO model to discrete
outcomes or higher order state dependence; (ii) a brief survey of semiparametric dynamic binary response models; (iii) proofs for the propositions in the main text; (iv) a
discussion of the linearity of parameters and assumptions used in the DPO model; (v) a
discussion of dimension reduction strategies; (vi) a discussion of estimation and statistical inference; and (vii) additional empirical estimates.

S1. EXTENSION TO DISCRETE OUTCOMES
THE DPO MODEL EXTENDS READILY TO THE CASE where Yit assumes values in
{0 1     J} with J > 1, so that Yi assumes values in Y ≡ {0 1     J}T +1 . Applications
of such an extension to the dynamics of employment include Magnac (2000) and Prowse
(2012), who examined state dependence under finer categorizations (part-time, full-time,
etc.) of employment status. Irace (2018) applied the DPO model with J > 1 to study the
dynamics of hospital choice.
In this more general case, there are J + 1 potential outcomes {Uit (y)}Jy=0 for each t ≥ 1.
The observed outcome in period t is determined as
Yit =

J


1[Yi(t−1) = y]Uit (y)

y=0

The primitive P is a probability mass function for (Yi0  {Uit (0)     Uit (J)}Tt=1 ) and the
characterization of the identified set remains conceptually unchanged. Some parameters
and identifying assumptions that are appropriate for the J = 1 case are also appropriate for the J > 1 case, but others would require modification. A separate analysis seems
beyond the scope of this paper.
S2. EXTENSION TO HIGHER ORDER STATE DEPENDENCE
The discussion in the main text presumes that the analyst is interested in first-order
state dependence, that is, the causal effect of the immediately preceding period on the
current period. This is consistent with much of the empirical and theoretical literature
on state dependence. However, the DPO model can also be modified to consider the
causal effects of longer histories of past outcomes. In this section, I outline how one would
extend the model to enable the estimation of this type of higher order state dependence.
When Yit is binary, the generalization to state dependence of length K ≥ 2 is accomplished by introducing 2K potential outcomes {Uit (y)}y∈{01}K for each period t ≥ K. The
recursive relationship (1) is replaced by



Uit (y)1 (Yi(t−1)      Yi(t−K) ) = y for t ≥ K
(S1)
Yit =
y∈{01}K

Alexander Torgovitsky: atorgovitsky@gmail.com
© 2019 The Econometric Society

https://doi.org/10.3982/ECTA14138

2

ALEXANDER TORGOVITSKY

with the joint determination of periods t = 0 up to t = K − 1 not being modeled explicitly.
For example, with K = 2, (S1) would become
Yit = 1[Yi(t−1) = 0 Yi(t−2) = 0]Uit (0 0) + 1[Yi(t−1) = 0 Yi(t−2) = 1]Uit (0 1)
+ 1[Yi(t−1) = 1 Yi(t−2) = 0]Uit (1 0) + 1[Yi(t−1) = 1 Yi(t−2) = 1]Uit (1 1)
so that, for each t, there are four potential outcomes corresponding to the four potential
two-period histories immediately prior to period t. The primitive P is a probability mass
function for the random vector

T 

Yi0  Yi1      Yi(K−1)  Uit (y) : y ∈ {0 1}K t=K 
The identified set P  can be characterized through essentially the same argument as for
the first-order case.
S3. A BRIEF SURVEY OF SEMIPARAMETRIC IDENTIFICATION IN DBR MODELS
The most common way to implement (8) is to construct a finitely parameterized likelihood function by imposing a parametric distributional assumption (typically normality)
for both (Vi1      ViT ) and Ai , and by further assuming that these latent variables are independent of Xi , as well as independent of each other. Maximum likelihood estimates of
the parameters in (8) can be used with the maintained parametric distributional assumptions to form estimates of causal parameters like those discussed in Section 3. However,
consistency of these estimates depends critically on the validity of the parametric assumptions.
Honoré (2002) raised a number of additional criticisms of this reduced form approach.
Many of his points apply equally to parametric implementations of structural DC models
like (2). One frequently discussed criticism is the treatment of Ai as a normally distributed
random effect that is independent of other explanatory state variables. Since (8) is nonlinear, treating Ai as a fixed effect to be estimated leads to the well-known incidental
parameters problem when T is small.1 Nonlinear differencing arguments can be applied
in certain cases (Chamberlain (1984, 1985, 2010), Honoré and Kyriazidou (2000), and
Bartolucci and Nigro (2010)), but these depend on very specific functional forms and may
therefore amplify concerns about misspecification.2 Honoré and Lewbel (2002) showed
that if there exists an exogenous special regressor with a large amount of variation, then
Ai can be treated as a fixed effect while also relaxing distributional assumptions in (8).
However, their results only identify the parameter coefficients and not causal parameters
such as the ATE. Similarly, Pakes and Porter (2016) allowed Ai to be a fixed effect and
also removed parametric distributional assumptions on Vit , but their partial identification
results concern the index parameters, not the causal parameters that constitute the focus
of this paper.
Some researchers have developed point and partial identification results for nonparametric models of dynamic binary outcomes that depart from the threshold crossing form
of (8) entirely. Instead, these papers maintain assumptions that imply that Yit follows a
homogeneous, first-order Markov process, conditional on permanent latent variable, like
1

Fernández-Val (2009) argued that the bias on the ATE may be relatively small, even for small T , while
Carro (2007) argued that the bias can be mitigated by using a modified maximum likelihood estimator.
2
See also Bonhomme (2012) for related results and a unifying analysis.

3

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

Ai , and possibly also on the initial period, Yi0 . Under this type of assumption, Kasahara
and Shimotsu (2009) and Browning and Carro (2010, 2014) established point identification by imposing the additional assumption that Ai has sufficiently small finite support
relative to T , while Hu and Shum (2012) allowed Ai to be continuously distributed, but
imposed a high-level completeness condition. These types of conditions on the distribution of unobserved heterogeneity may be difficult to motivate or interpret in applications.
In addition, the first-order conditional Markov property assumed by all of these papers
may be unattractive in some settings (see Bhuller, Brinch, and Königs (2017) and Section 6 of Browning and Carro (2014)). The DPO model does not maintain this type of
first-order conditional Markov property, although it can be imposed if desired; see Section 4.4.
S4. PROOFS
PROOF OF PROPOSITION 1: Observe that if P ∈ P  , then


SD+t (P) = PP Yi(t−1) = 0 Uit (0) = 0 Uit (1) = 1


+ PP Yi(t−1) = 1 Uit (0) = 0 Uit (1) = 1




= PP Yi(t−1) = 0 Yit = 0 Uit (1) = 1 + PP Yi(t−1) = 1 Uit (0) = 0 Yit = 1
= P[Yi(t−1) = 0 Yit = 0] + P[Yi(t−1) = 1 Yit = 1]




− PP Yi(t−1) = 0 Yit = 0 Uit (1) = 0 − PP Yi(t−1) = 1 Uit (0) = 1 Yit = 1 
where the second equality follows because, under (1), [Yi(t−1) = 0 Uit (0) = 0] if and only
if [Yi(t−1) = 0 Yit = 0], and [Yi(t−1) = 1 Uit (1) = 1] if and only if [Yi(t−1) = 1 Yit = 1]. The
only restrictions implied on the second two terms are


0 ≥ −PP Yi(t−1) = 0 Yit = 0 Uit (1) = 0 ≥ −PP [Yi(t−1) = 0 Yit = 0]
(S2)


0 ≥ −PP Yi(t−1) = 1 Uit (0) = 1 Yit = 1 ≥ −PP [Yi(t−1) = 1 Yit = 1]
and there are no cross-equation restrictions between these terms. Thus, there exists a
P ∈ P  obtaining both of the upper bounds in (S2), and one obtaining both of the lower
bounds. The upper and lower bounds in (14) now follow from those in (S2). The bounds
in (15) follow from an analogous argument using the decomposition
SD−t (P) = P[Yi(t−1) = 0 Yit = 1] + P[Yi(t−1) = 1 Yit = 0]




− PP Yi(t−1) = 0 Yit = 1 Uit (1) = 1 − PP Yi(t−1) = 1 Uit (0) = 0 Yit = 0 
and the following analogous bounds on the second two terms:


0 ≥ −PP Yi(t−1) = 0 Yit = 1 Uit (1) = 1 ≥ −PP [Yi(t−1) = 0 Yit = 1]


0 ≥ −PP Yi(t−1) = 1 Uit (0) = 0 Yit = 0 ≥ −PP [Yi(t−1) = 1 Yit = 0]

(S3)

Observe that there are no restrictions preventing the terms in (S2) and (S3) from simultaneously achieving either their lower or upper bounds. Considering these two polar cases
shows that the sharp identified set for SDt ≡ SD+t + SD−t is equal to [0 1].
Q.E.D.

4

ALEXANDER TORGOVITSKY

PROOF OF PROPOSITION 2: If P † is closed and convex, then P  is also closed and convex, since P  is the set of P ∈ P † that satisfy the linear equalities (11) for all y and x. The
image of the continuous, real-valued function θ over this closed, convex, and nonempty
set is a closed, nonempty interval (e.g., Rudin (1976, Theorem 4.22)) with smallest value


, that is, Θ = [θlb  θub
].
Q.E.D.
θlb and largest value θub
PROOF OF PROPOSITION 3: Let P ∈ P denote a distribution that is consistent with the
stated conditions and fix y ∈ {0 1}. Since ϕ(s̄ ·) is weakly increasing for each s̄, it has a
generalized inverse, ϕ−1 (s̄ ·). By Theorem 3.1 of Fang, Hu, and Joe (1994), (S̃it  S̃i(t+t  ) )
is decreasing in the concordance ordering as a function of |t  |, conditional on S̄i . That is,
P[S̃it ≥ s̃1  S̃i(t+t  ) ≥ s̃2 |S̄i = s̄] is decreasing in |t  | for any s̃1 , s̃2 , and s̄. Thus, for any integers
t  , t  with |t  | < |t  |,


PP Uit (y) = 1 Ui(t+t  ) (y) = 1
 




= P ν̊ Sit (y) ≥ 0 ν̊ Si(t+t  ) (y) ≥ 0
 

= E P ϕ(S̄i  S̃it ) ≥ 0 ϕ(S̄i  S̃i(t+t  ) ) ≥ 0|S̄i
 

= E P S̃it ≥ ϕ−1 (S̄i  0) S̃i(t+t  ) ≥ ϕ−1 (S̄i  0)|S̄i

 
≥ E P S̃it ≥ ϕ−1 (S̄i  0) S̃i(t+t  ) ≥ ϕ−1 (S̄i  0)|S̄i


(S4)
= PP Uit (y) = 1 Ui(t+t  ) (y) = 1 
where the third equality follows because ϕ(s̄ ·) is right-continuous (e.g., Embrechts and
Hofert (2013, Proposition 1(5))), and the final equality reverses the steps of the first three.
As discussed in Appendix S5, (S4) implies Assumption DSC if Assumption ST is satisfied.
Q.E.D.
PROOF OF PROPOSITION 4: Let P ∈ P denote a distribution that is consistent with the
stated conditions. If A and B are two events and B occurs with probability strictly between
0 and 1, then P[A B] ≥ P[A]P[B] implies P[A|B] ≥ P[A|Bc ].3 From this, (19) follows
from conditions (i) and (ii) whenever P[Yi(t−1) = 1 Yi(t−2) = ỹ] ∈ (0 1):


PP Uit (y) = 1|Yi(t−1) = 1 Yi(t−2) = ỹ




 
= PP ν̊ Sit (y) ≥ 0|ν̊ Si(t−1) (ỹ) ≥ 0 Yi(t−2) = ỹ




 
≥ PP ν̊ Sit (y) ≥ 0|ν̊ Si(t−1) (ỹ) < 0 Yi(t−2) = ỹ


= PP Uit (y) = 1|Yi(t−1) = 0 Yi(t−2) = ỹ 
Q.E.D.
PROOF OF PROPOSITION 5: Note that Assumption TIV implies that
P[Uit = u|Yi(0:t−1)  Ai ] = P[Ui1 = u|Yi0  Ai ] = P[Uit  = u|Yi(0:t  −1)  Ai ]
almost surely, for any t, t  and u ∈ {0 1}2 . As a consequence, if t  > t ≥ 1, then
PP [Uit  = u Yi(0:t−1) = y]
3
This follows because P[A | B] ≡ P[A B]P[B]−1 ≥ P[A], while on the other hand, P[A | Bc ] = (P[A] −
P[A B])(1 − P[B])−1 ≤ P[A].

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

5



= E P[Uit  = u Yi(0:t−1) = y|Yi(0:t  −1)  Ai ]


= E 1[Yi(0:t−1) = y]P[Uit  = u|Yi(0:t  −1)  Ai ]


= E 1[Yi(0:t−1) = y]P[Uit = u|Yi(0:t−1)  Ai ]


= E P[Uit = u Yi(0:t−1) = y|Yi(0:t−1)  Ai ]
= PP [Uit = u Yi(0:t−1) = y]
for any u ∈ {0 1}2 and y ∈ {0 1}t , as claimed. Summing both sides of this equality over all
realizations y of Yi(0:t−1) shows that Assumption TIV implies Assumption ST with m = 0.
Q.E.D.
PROOF OF PROPOSITION 6: Let P ∈ P denote a distribution that is consistent with the
stated conditions. Then, for any u = (u0  u1 ) ∈ {0 1}2 ,
PP [Uit = u|Yi(0:t−1)  S̄i ]


= EP PP [Uit = u|Yi(0:t−1)  S̄i  S̃i(1:t−1) ]|Yi(0:t−1)  S̄i


= EP PP [Uit = u|Yi0  S̄i  S̃i(1:t−1) ]|Yi(0:t−1)  S̄i


= EP PP [Ui1 = u|Yi0  S̄i ]|Yi(0:t−1)  S̄i = PP [Ui1 = u|Yi0  S̄i ]
where the second equality follows because Yi(1:t−1) are fully determined by Yi0 , S̃i(1:t−1) , and
S̄i under (7) and (1). The third equality used condition (ii), since when Uit is determined
by (7), it is a function of (Sit (0) Sit (1)), which have here been split into (S̄i  S̃it ). The timeQ.E.D.
invariant variable S̄i serves the role of Ai in the statement of Assumption TIV.
PROOF OF PROPOSITION 7: Let P ∈ P denote a distribution that is consistent with the
stated conditions and fix a y ∈ {0 1}. Consider any (x0  x1 ), (x0  x̃1 ) in the support of
(Xit0  Xit1 ), with x̃1 ≥ x1 . Then


PP Uit (y) = 1|Xit0 = x0  Xit = x1
 


= PP ϕ x0  x1  Vit ≥ 0|Xit0 = x0  Xit1 = x1
 


= PP ϕ x0  x1  Vit ≥ 0|Xit0 = x0  Xit1 = x̃1
 


≤ PP ϕ x0  x̃1  Vit ≥ 0|Xit0 = x0  Xit1 = x̃1


= PP Uit (y) = 1|Xit0 = x0  Xit1 = x̃1 
where the second and third equalities used conditions (iii) and (iv).

Q.E.D.

PROOF OF PROPOSITION 8: Under Assumption MC, the worker’s Bellman equation is
given by (24). Let ν̊ denote the worker’s per-period objective function, as in (2). Profiling
the effort decision for a fixed employment decision y  gives




e Sit  y  ≡ arg max ν̊ y   e  Yi(t−1)  Ei(t−1)  Vit  Ai
e ∈E



 


= arg max −κ y   e  Ai + δE ν y   e  Vi(t+1)  Ai | Vit  Ai 
e ∈E

(S5)

6

ALEXANDER TORGOVITSKY

so that the effort decision in period t is a function of the (fixed) current period employment decision, y  , the current period wage shock, Vit , and time-invariant heterogeneity,
Ai . As a consequence, the counterfactual state in period t had the worker chosen y in
period t − 1 is


Sit (y) ≡ y e (Vi(t−1)  Ai  y) Vit  Ai 
which depends on the hypothesized previous period employment decision, y, the previous
and current period wage shocks, (Vi(t−1)  Vit ), and time-invariant heterogeneity, Ai .4 The
worker’s present-discounted net utility from choosing employment if the previous period’s
employment choice was y can therefore be written as




ν̊ Sit (y) = ω y e (Vi(t−1)  Ai  y) Ai  Vit − κ(Vit  Ai ) + γ(Vit  Ai )
(S6)
where κ and γ are shorthand for
 



κ(Vit  Ai ) = κ 1 e (Vit  Ai  1) Ai − κ 0 e (Vit  Ai  0) Ai 
 

γ(Vit  Ai ) = δE ν 1 e (Vit  Ai  1) Vi(t+1)  Ai



− ν 0 e (Vit  Ai  0) Vi(t+1)  Ai |Vit  Ai 

and

Statements (i)–(iv) can now be proven for the generated potential outcomes (7) by
using (S6) as follows.
(i) Notice that (Uit (0) Uit (1)) is fully determined by (ν̊(Sit (0)) ν̊(Sit (1))), and that
from (S6), the latter is only stochastic due to (Vi(t−1)  Vit ) and Ai . Thus, if Assumption W(a) is satisfied with m = 1, then since Ai is time-invariant, Assumption ST is also
satisfied with m = 0. More generally, (7) and (S6) imply that Ui(t−m:t) (0) and Ui(t−m:t) (1)
are stochastic only due to Vi(t−m−1:t) and Ai , so that Assumption W(a) with any m ≥ 1
implies Assumption ST with m = m − 1.
(ii) Under Assumption W(b),
 


 


E ν y   e  Vi(t+1)  Ai | Vit  Ai = E ν y   e  Vi(t+1)  Ai | Ai 
From (S5), it follows that e (Vit  Ai  y) = e (Ai  y) is only stochastic due to Ai . This
implies that Sit (y) = (y e (Ai  y) Vit  Ai ) is only stochastic due to Vit and Ai , and that
(S6) can be written as




(S7)
ν̊ Sit (y) = ω y e (Ai  y) Ai  Vit − κ(Ai ) + γ(Ai )
To see that the conditions of Proposition 3 are satisfied, fix a y, take S̄i ≡ Ai , and take
S̃it ≡ ω(y e (Ai  y) Ai  Vit ). Then (S7) can be written as


ν̊ Sit (y) ≡ S̃it − κ(S̄i ) + γ(S̄i ) ≡ ϕ(S̄i  S̃it )

(S8)

which is an increasing and continuous function of S̃it . Since Vit and Vi(t−1) are independent,
conditional on Ai , so too are S̃it and S̃i(t−1) , conditional on S̄i . Therefore, the stochastic
increasing condition of Proposition 3 is also satisfied.
4

As in Section 2.2, I am assuming here that the solution to this effort decision is unique.

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

7

(iii) As shown in (ii), Assumptions MC and W(b) imply that (Uit (0) Uit (1)) is stochastic
only due to Vit and Ai . Thus, the same argument as in (i) holds with m = m.
(iv) As shown in (ii), Assumptions MC and W(b) imply that the stochastic components of (Sit (0) Sit (1)) are (Vit  Ai ) ≡ (Vit  Āi  Yi0 ). Assumption MC further implies that
Vit |{Vit  }t  <t , Āi , Yi0 has the same distribution as Vit |Vi(t−1)  Āi  Yi0 . By Assumption W(b),
the latter has the same distribution as Vit |Āi  Yi0 , which by Assumption W(a) has the same
distribution as Vi1 |Āi  Yi0 . Thus, the conditions of Proposition 6 are satisfied with S̄i ≡ Āi
and S̃it ≡ Vit .
Q.E.D.
PROOF OF PROPOSITION (9): Under Assumptions MC and W(b), the worker’s net utility from employment can be written as (S7); see Proposition 8(ii). For shorthand, define


Ω(y Ai ) ≡ κ(Ai ) − γ(Ai ) − ω̄ y e (Ai  y) Ai 
Then, since ω(y e a v) = ω̄(y e a) + v,
 




P ν̊ Sit (y) ≥ 0 ν̊ Si(t−1) (ỹ) ≥ 0 | Yi(t−2) = ỹ
 


= E P Vit ≥ Ω(y Ai ) Vi(t−1) ≥ Ω(ỹ Ai ) | Yi(t−2) = ỹ Ai  {Vis }s≤t−2 | Yi(t−2) = ỹ
 


= E P Vit ≥ Ω(y Ai ) Vi(t−1) ≥ Ω(ỹ Ai ) | Ai  {Vis }s≤t−2 | Yi(t−2) = ỹ
 

= E P Vit ≥ Ω(y Ai ) | Ai



× P Vi(t−1) ≥ Ω(ỹ Ai ) | Ai | Yi(t−2) = ỹ 
(S9)
where the second equality follows because Yi(t−2) is fully determined by Ai and {Vis }s≤t−2
when Ai includes the initial conditions, (YiT  EiT ), and the third equality uses Assump¯
¯
independent,
tion W(b). Since Ai and Vit are assumed to be






P Vit ≥ Ω(y Ai ) | Ai = a = P Vit ≥ Ω(y a) ≡ Ft Ω(y a) 
so that (S9) can be written as
 




P ν̊ Sit (y) ≥ 0 ν̊ Si(t−1) (ỹ) ≥ 0 | Yi(t−2) = ỹ



 

= E Ft Ω(y Ai ) Ft−1 Ω(ỹ Ai ) | Yi(t−2) = ỹ 
Below, it will be shown that Ft (Ω(y a)) and Ft−1 (Ω(ỹ a)) are both increasing functions
of a. This implies that the covariance between Ft (Ω(y Ai )) and Ft−1 (Ω(ỹ Ai )) is positive
conditional on Yi(t−2) = ỹ (or on any other event); see, for example, Lehmann (1966).
Thus,
 




P ν̊ Sit (y) ≥ 0 ν̊ Si(t−1) (ỹ) ≥ 0 | Yi(t−2) = ỹ
 

 



≥ E Ft Ω(y Ai ) | Yi(t−2) = ỹ E Ft−1 Ω(ỹ Ai ) | Yi(t−2) = ỹ 
(S10)
By reversing the previous arguments, one has
 


 


E Ft Ω(y Ai ) | Yi(t−2) = ỹ = P ν̊ Sit (y) ≥ 0 | Yi(t−2) = ỹ  and


 




E Ft−1 Ω(ỹ Ai ) | Yi(t−2) = ỹ = P ν̊ Si(t−1) (ỹ) ≥ 0 | Yi(t−2) = ỹ 

8

ALEXANDER TORGOVITSKY

which upon substitution into (S10) implies that ν̊(Sit (y)) and ν̊(Si(t−1) (ỹ)) are positively quadrant dependent, locally at (0 0) and conditional on Yi(t−2) = ỹ. The result then
follows from Proposition 4.
It remains to be shown that Ft (Ω(y a)) and Ft−1 (Ω(ỹ a)) are both increasing functions
of a. By definition, both Ft (·) and Ft−1 (·) are decreasing functions, so it suffices to show
that Ω(y a) is also a decreasing function of a. For this, we remove the search effort
decision from the model (condition (iii)), under which
Ω(y a) = −γ(a) − ω̄(y a)
The second term is decreasing in a by condition (iv). As for the first term, under the given
assumptions, we have


γ(a) = δE ν(1 Vi(t+1)  a) − ν(0 Vi(t+1)  a) 
so to show that it is increasing in a (and therefore that −γ(y) is decreasing in a), it
suffices to show that ν(y v a) has the increasing differences (or supermodularity in this
simple setting) property in (y a), that is, that ν(1 v a) − ν(0 v a) is increasing as a
function of a for all v. Under the maintained assumptions, the Bellman equation is

 

  

V

ν(y v a) = max
y
ω̄(y
a)
+
v
+
δE
ν
y
i(t+1)  a

y ∈{01}

so that ν(y v a) will have increasing differences in (y a) for all v if ω̃(y   y a v) ≡
y  (ω̄(y a) + v) has increasing differences in (y   y) for all (a v), in (y   a) for all (y v),
and in (y a) for all (y   v); see Proposition 2 of Hopenhayn and Prescott (1992). To see
that these conditions are satisfied here, observe that by condition (iv),
ω̃(1 y a v) − ω̃(0 y a v) = ω̄(y a) + v
is increasing in both y and a, and






ω̃ y   1 a v − ω̃ y   0 a v = y  ω̄(1 a) − ω̄(0 a)
is increasing in a.

Q.E.D.

PROOF OF PROPOSITION 10: Under Assumptions MC and W(b), ν̊(Sit (y)) is given by
(S7). Assumption W(d) then implies that


 
P ν̊ Sit (0) ≥ 0


= P Wit (0) ≥ κ(Ai ) − γ(Ai )
 

= E P Wit (0) ≥ κ(Ai ) − γ(Ai ) | Ai
 



 
≤ E P Wit (1) ≥ κ(Ai ) − γ(Ai ) | Ai = P ν̊ Sit (1) ≥ 0 
which implies Assumption MATR when Uit (y) is determined by (7). Assumption MTR
follows similarly, since under Assumption W(e),






 
Q.E.D.
P ν̊ Sit (1) − ν̊ Sit (0) ≥ 0 = P Wit (1) ≥ Wit (0) = 1

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

9

S5. LINEARITY OF PARAMETERS AND ASSUMPTIONS
The parameters and assumptions discussed in the main text can be represented as linear functions of P = {P(u x) : u ∈ U  x ∈ X }. This section demonstrates this point. For
notational simplicity, I assume throughout that Xi is degenerate, but it is straightforward
to adjust the conditions to allow for Xi to be random by simply conditioning and then
averaging over all realizations of Xi . (Alternatively, it is also straightforward to modify
the parameters so that they are conditional on certain realizations of Xi .)
First, consider SD+t , which can be written as

 
SD+t (P) ≡ PP Uit (0) = 0 Uit (1) = 1 =
P(u)
u∈Ut+

where Ut+ is the set of u = (u0  u(0) u(1)) ∈ U such that ut (0) = 0 and ut (1) = 1. This is
a linear function of P. To see that SD+t (·|0) is linear, write it as

P(u)


+
P
(0)
=
0
U
(1)
=
1
Y
=
0
U
u∈
U
(0)
P
it
it
it
t
=

SD+t (P|0) =
P[Yit = 0]
P[Yit = 0]
where Ut+ (0) is the set of u ∈ U such that ut (0) = 0, ut (1) = 1, and Yit = 0 when computed
through the recursive relationship (1) with Yi0 = u0 , Uit (0) = ut (0), and Uit (1) = ut (1).
Similar equations follow for SD+t (P|1), SD+t (P|00), and SD+t (P|11).
To demonstrate linearity of the assumptions, consider Assumption MTR, which has
the simplest form. Assumption MTR can be written as PP [Uit (0) = 1 Uit (1) = 0] = 0 for
all t ≥ 1. Letting UtMTR denote the set of all u ∈ U such that ut (0) = 1 and ut (1) = 0,
Assumption MTR can also be written as

P(u) = 0
(S11)
u∈UtMTR

for all t ≥ 1. In terms of the ρ function, this equality constraint can be imposed with two
inequalities.5 Assumptions ST and its variations, TIV, MIV, and MATR, can be imposed
similarly by summing over the appropriate subsets of U . Assumption MTS can be imposed
using a construction similar to that for SD+t (P|0).
Finally, consider Assumption DSC, which has a different structure. In general, Assumption DSC is a nonlinear restriction, but if Assumption ST holds so that the distribution
of Uit (d) does not depend on t, then CorrP (Uit (d) Ui(t+s) (d)) is decreasing in |s| if and
only if CovP (Uit (d) Ui(t+s) (d)) is decreasing in |s|. Furthermore, under Assumption ST,
the latter is true if and only if EP [Uit (d)Ui(t+s) (d)], that is, PP [Uit (d) = 1 Ui(t+s) (d) = 1], is
decreasing in |s|. It is straightforward to show that PP [Uit (d) = 1 Ui(t+s) (d) = 1] is a linear
function of P using a construction like (S11).
S6. DIMENSION REDUCTION
S6.1. Computational Considerations
The optimization problem in Proposition 2 can be quite large. For example, if T = 6,
then the dimension of the variables in the problem, that is, of P = {P(u x) : u ∈ U  x ∈ X },
5
In practice, (S11) is always combined with the requirement that P(u) ≥ 0, and so it simply reduces to
P(u) = 0 for all u ∈ UtMTR .

10

ALEXANDER TORGOVITSKY

is 22T +1 = 213 = 8192, even without including any covariates. The number of constraints
in the problem—even without any identifying assumptions—is at least 2T +1 = 128 for the
observational equivalence conditions (11), plus 2 × 8192 constraints to ensure that P is
contained in the unit interval.
These dimensions are large for an unstructured optimization problem. However, if both
ρ and θ are linear so that the problems in Proposition 2 are linear programs, then these
dimensions are actually fairly modest. A standard desktop computer with sophisticated
linear programming solvers such as CPLEX (IBM (2010)) or Gurobi (Gurobi Optimization (2015)) can finish problems of this size in a matter of seconds. Nevertheless, increasing the length of the panel, T , or including rich, time-varying specifications of covariates
both increase the number of variables at an exponential rate. This can quickly become
computationally infeasible.
In the remainder of this section, I describe three dimension reduction strategies for
addressing this curse of dimensionality. The first strategy combines information across
multiple models of shorter time horizons. The second strategy applies a similar construction to the covariates. The third strategy imposes a simple semiparametric structure for
the covariates. All three strategies involve a natural and familiar trade-off between the
amount of information in the data that is utilized, and the difficulty (both computational
and statistical) of harnessing that information. They can be implemented separately or
combined together.
S6.2. Combining Shorter Models
The most immediate way in which the curse of dimensionality affects the DPO model
is through the dimension of the potential outcomes sequence, Ui , which increases exponentially with the time period T . This difficulty is common for models that do not impose
a conditional Markov restriction on the observed outcomes. For example, Hyslop (1999)
considered a parametric DBR model in which the idiosyncratic error follows an AR(1)
process. As observed by Heckman (1981) and Chamberlain (1984), this implies that the
observed outcomes are not Markov of any order. The resulting likelihood function for the
parametric DBR involves a T -dimensional integral, which is also difficult to approximate
when T is large.
In the DPO model, this difficulty can be addressed by constructing several shorter, overlapping models. To see how this works, fix a model length ML ∈ {2     T }. Then construct
a DPO model for the observed sequence (Yit0  Yi(t0 +1)      Yi(t0 +ML) ) at every initial period
t0 ∈ {0 1     T − ML}. Each of these shorter models relates potential outcomes to observed outcomes through (1) for t ∈ {t0      t0 + ML}. The case discussed throughout the
main text corresponds to setting ML = T .
The primitive object is now a collection of probability mass functions P ≡ {Pt0 }Tt0−ML
=0 ,
each of which describes the joint distribution of


Yit0  Ui(t0 +1) (0)     Ui(t0 +ML)  Ui(t0 +1) (1)     Ui(t0 +ML) (1) Xi 
Each Pt0 should satisfy (10), where U is now {0 1}2ML+1 , and each Pt0 is restricted to
lie in a parameter space Pt†0 , that can be specified to satisfy the same types of assumptions discussed in Section 4. The identified set contains all collections P = {Pt0 : Pt0 ∈
Pt†0 }Tt0−ML
of shorter models such that (11) is satisfied for each t0 , and all realizations of
=0
(Yit0  Yi(t0 +1)      Yi(t0 +ML)  Xi ). The target parameter, θ, is a function of the collection of
shorter models, P.

11

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

Since Pt0 and Pt0 +1 are distributions that encompass some of the same random variables, the identified set for P must satisfy an additional coherency condition. Mogstad,
Torgovitsky, and Walters (2019) described such a condition (in a different model) as logical consistency. In the DPO model, the logical consistency condition states that when two
overlapping models can both assign a probability to an event, this probability must be the
same. That is,
PPt0 [Yi(t0 +1) = y0  Ui(t0 +2:t0 +ML) = u] = PPt0 +1 [Yi(t0 +1) = y0  Ui(t0 +2:t0 +ML) = u]
for all (y0  u) ∈ {0 1}1+2(ML−1) 

(S12)

The identified set, P  , only contains collections P that satisfy (S12) for all t0 .6 Intuitively,
(S12) aggregates information across the shorter overlapping models.
The benefit of this approach is that it reduces the dimension of variables of optimization
from 22T +1 to (T − ML)22ML+1 , which no longer increases exponentially with the length
of the panel, T . However, it should also be noted that the coherency condition (S12)
constitutes an additional (T − ML)22(ML−1) constraints that are not present when ML = T .
As a consequence, values of ML close to T may not provide any computational gain, and
may in fact be costlier. For values of ML significantly smaller than T , however, the large
reduction in the number of variables, combined with a modest increase in the number of
constraints, can still net out to massive dimension reduction.
The cost of this approach is the loss of information from modeling less of the distribution of Yi .7 This manifests itself in two ways. First, there are fewer observational equivalence conditions. This is because a model of (Yit0  Yi(t0 +1)      Yi(t0 +ML) ) does not provide
a probability for an observable sequence of length greater than ML + 1. Second, it may
not be possible to impose certain identifying assumptions, such as Assumption ST with
m > ML − 1, for the related reason that a model of length ML does not provide statements about potential outcome sequences longer than ML − 1.
S6.3. Partitioned Covariates
Recall that X denotes the support of the covariates Xi . For each j = 1     J, let
Xj ≡ {Xj1      XjKj } denote a finite partition of X into Kj exhaustive and mutually excluK

j
sive sets (or bins), Xjk , that are specified by the researcher. Let Xij ≡ k=1
k1[Xi ∈ Xjk ]
denote the random variable that takes value k if Xi lands in bin Xjk .
Let Pj denote a probability mass function with support contained in U × Xj , and let Pj
denote the set of all such functions that sum to unity, as in (10). Every P ∈ P defines a
collection of Pj ∈ Pj for j = 1     J through the relationship

Pj : U × N → [0 1] : Pj (u k) =



P(u x)

(S13)

x∈Xjk

Given a parameter space, P † , relationship (S13) generates parameter spaces Pj† that each
Pj is restricted to lie in. Moreover, if P ∈ P  , then Pj must satisfy the following set of
6

Proposition 2 and the resulting methodology extend immediately, since these constraints are linear in each

Pt0 .

7

Formally, the identified set will be an outer identified set, rather than the sharp identified set.

12

ALEXANDER TORGOVITSKY

observational equivalence constraints:

P[Yi = y Xi ∈ Xjk ] =
P[Yi = y Xi = x]
x∈Xjk

=





P(u x) =

x∈Xjk u∈Uoeq (y)



Pj (u k)

for all y, j and k. (S14)

u∈Uoeq (y)

As in the previous section, there is an additional logical consistency constraint that can be
imposed across the smaller models {Pj }Jj=1 , namely,
Kj

k=1

Pj (u k) =


x∈X

j


K

P(u x) =

Pj (u k) for any j, j  and all u ∈ U 

(S15)

k=1

As long as the collection {Pj }Jj=1 is sufficient for evaluating the researcher’s target parameter, θ, one can work only with these lower-dimensional objects via (S13)–(S15), rather
than with P directly.
To see how this partitioning approach addresses the curse of dimensionality, suppose
that Xi = (Xi1      Xidx ) and that each Xij has K support points {xjk }Kk=1 . The full distribution P consists of 22T +1 × J K elements. This can be a large number even if J or K
are relatively small. However, suppose that the researcher specifies partitions Xj taken as
Xj = {{x1k }Kk=1      {xJk }Kk=1 }, so that each partition has K elements corresponding to the
jth component of Xi . The total dimension of {Pj }Jj=1 under this partition is 22T +1 × JK,
which can be dramatically smaller than 22T +1 × J K . The cost of this approach is a loss of
information. This occurs for the same reasons as for the strategy in the previous section:
Only a subset of the observational equivalence conditions are being met, and identifying
content contained in restrictions that would span across covariate partitions cannot be
exploited.
S6.4. A Semiparametric Specification
One natural response to the curse of dimensionality is to impose semiparametric restrictions. For doing this, it is more convenient to formulate the DPO model as one of the
conditional distribution of Yi given Xi , rather than of their joint distribution. This changes
most of the discussion in the paper in only obvious ways; the exception is the discussion
of statistical inference in Appendix S7, which would require some reworking. The primitive object of the model changes from a joint distribution to a collection of conditional
distributions, written (with mild abuse of notation) as P = {P(·|x) : x ∈ X }, each of which
has support contained in U ≡ {0 1}2T +1 .
With P as a conditional distribution, a natural semiparametric assumption is that


P † ⊆ P ∈ P : P(u|x) = h(x) βu for some βu ∈ Rdh , all u and x 
where h is a known, vector-valued function of length dh . This assumption says that for
each u, P(u|x) is a linear function of a known transformation of x with coefficient vector
βu .8 Under this assumption, each P is characterized by a set of parameters {βu : u ∈ U }
8

Note that unlike a linear probability model, here βu is still required to be such that P(u|x) ∈ [0 1].

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

13

that has dimension 22T +1 × dh . This dimension does not depend on the number of support points of Xi , and grows linearly with the dimension of the transformation vector, h,
thereby overcoming the curse of dimensionality, while preserving the linear programming
structure. The cost is the usual threat of potential misspecification.
When taken to the observational equivalence condition (11), the semiparametric model
also implies a lower-dimensional representation for the observed data distribution, since
PP [Yi = y|Xi = x] =



P(u|x) = h(x)

u∈Uoeq (y)



βu ≡ h(x) δy 

(S16)

u∈Uoeq (y)

Thus, it justifies estimating P[Yi = y|Xi = x] by a linear probability model. In practice,
one would want to ensure that the fitted probabilities are in the unit interval. One way to
do this is to estimate a constrained least squares regression
δ̂y ≡ arg min
δ∈R

dh

n



2

1[Yi = y] − h(Xi ) δ

s.t. 0 ≤ h(Xi ) δ ≤ 1 ∀i = 1     n

(S17)

i=1

as suggested by Domencich and McFadden (1975, p. 105) or Judge, Griffiths, Hill, Lütkepohl, and Lee (1985, p. 759). Alternatively, one can estimate P[Yi = y|Xi = x] using a
statistical model chosen for fit, and then run the resulting probabilities through the DPO
model.
S7. ESTIMATION AND STATISTICAL INFERENCE
In Section 3, I considered the distribution of observables as if it were known without
accounting for any sampling error. As a result, the identified set Θ was also known without error for a given parameter and given set of assumptions. In this section, I adjust the
discussion to account for the statistical variation that arises when modeling the data as an
i.i.d. sample from some underlying population distribution. First, I describe how to construct a consistent estimator of Θ . Second, I describe how to construct confidence regions
that contain (with probability at least 1 − α) the parameter θ0 = θ(P0 ) ∈ Θ corresponding
to the “true” P0 ∈ P  that generated the data. Third, I discuss a specification test that can
be used to falsify the hypothesis that such a P0 exists. Finally, I conduct a Monte Carlo
simulation to evaluate these procedures.
S7.1. The Criterion Function

Approaches based on direct sample analogs of θlb and θub
are unattractive for two important reasons. First, while these estimators are consistent under weak conditions, their

asymptotic distributions are highly nonstandard.9 Second, sample analogs of θlb and θub
10
might not exist even when the population identified set is nonempty. Both problems can

9

See Shapiro and Dentcheva (2014, Chapter 5), who derived the asymptotic distributions of these analog
estimators. The results of Andrews and Han (2009) imply that naively bootstrapping or subsampling empirical


and θub
will not lead to valid confidence regions.
analogs of θlb
10
Freyberger and Horowitz (2015) studied an instrumental variables model for which the identified set can
be represented through the solution to two linear programming problems. They proposed a modified bootstrap
procedure based on the sample analogs of the solutions to the linear programs, but their procedure assumes
that these sample analogs exist.

14

ALEXANDER TORGOVITSKY

be addressed by transforming the characterization of the identified set provided in Proposition 2 into a criterion function, and then using a sample analog of this criterion function
as the basis for estimation and statistical inference (e.g., Chernozhukov, Hong, and Tamer
(2007)).
Some additional notation is required. Let W ≡ supp(Yi  Xi ) denote the joint support
of the observable data Wi ≡ (Yi  Xi ). For each w ≡ (wy  wx ) ∈ W ⊂ RdW , define


moeqw (Wi  P) ≡ 1[Yi = wy  Xi = wx ] −

P(u wx )

(S18)

u∈Uoeq (wy )

Next, divide the restriction function ρ into a deterministic component ρd , and a stochastic
component ρs with dimension ds . The deterministic component, ρd : P → Rdρ −ds , is a
function defined on P that does not depend on the distribution of Wi . The stochastic
component, ρs : P → Rds , is a function defined on P that is assumed to be representable
as a moment condition. That is, it is assumed that there exists a function mρ : W × P →
Rds for which ρs (P) = Emρ (Wi  P). This condition is satisfied by all of the identifying
assumptions discussed in Section 4.
For example, Assumption ST would be part of ρd , since it is not a restriction that depends on the distribution of observables Wi . On the other hand, Assumption MTS would
be part of ρs , since it depends on the distribution of (Yi(t−1)  Yi(t−2) ).
Next, define Pd† ≡ {P ∈ P : ρd (P) ≥ 0} as the set of deterministic constraints on P. These
include not only ρd , but also the requirement that P ∈ P , that is, that P is a probability
mass function on U × X . Then

P  = P ∈ Pd† : Emoeqw (Wi  P) = 0 ∀w ∈ W

(S19)
and Emρs (Wi  P) ≥ 0 ∀s = 1     ds 
where mρs (Wi  P) denotes the sth component of mρ (Wi  P). Equation (S19) shows that
the DPO model can be viewed as a moment inequality model with parameter space Pd† ,
moment equalities {Emoeqw (Wi  P) = 0}w∈W , and moment inequalities {Emρs (Wi  P) ≥
s
0}ds=1
. Alternatively and equivalently, let η ∈ Rd+s denote a vector of nonnegative slackness
variables and define the identified set using only moment equalities as

R ≡ (P η) ∈ Pd† × Rd+s : Emoeqw (Wi  P) = 0 ∀w ∈ W

(S20)
and Emρs (Wi  P) − ηs = 0 ∀s = 1     ds 
Then P  is the projection of the first component of R , that is,


P  = P ∈ P : (P η) ∈ R for some η ∈ Rd+s 
s
m
and {moeqw }w∈W together as {mj }dj=1
where dm =
Write the moment functions {mρs }ds=1
dm
ds
ds + dW and the first ds components of {mj }j=1 correspond to {mρs }s=1 . A convenient
choice of population criterion function is

Q(P η) ≡

ds

j=1

Emj (Wi  P) − ηj +

dm

j=ds +1

Emj (Wi  P) 

(S21)

15

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

Notice that (P η) ∈ R if and only if Q(P η) = 0 and (P η) ∈ Pd† × Rd+s . Using an absolute value loss function instead of the more standard quadratic loss function is computationally convenient for the estimation and inference procedures discussed ahead. Other
choices of criterion function are possible in principle, but tend to create computational
obstacles.11 Given an i.i.d. sample {Wi }ni=1 of size n, a sample analog of Q is constructed by
replacing the population expectation with its (scaled) empirical counterpart:
Qn (P η) ≡

ds
dm


√
√
n m̄nj (P) − ηs +
n m̄nj (P) 

where

j=ds +1

j=1

1
m̄nj (P) ≡
mj (Wi  P)
n i=1

(S22)

n

for j = 1     dm 

S7.2. Estimation
An estimator of Θ can be constructed by restricting attention to P that come close to
minimizing the sample criterion (S22). Let
Q̄n ≡

min

d

(Pη)∈Pd† ×R+s

Qn (P η)

(S23)

denote the minimum value of Qn , and let


Pn ≡ P ∈ Pd† : Qn (P η) ≤ Q̄n (1 + τn ) for some η ∈ Rd+s
denote the collection of P ∈ Pd† that yield criterion values within τn % of the optimum.
Then define
θ̂lb ≡ min θ(Pn ) =
θ̂ub ≡ max θ(Pn ) =

min

d

(Pη)∈Pd† ×R+s

max

d

(Pη)∈Pd† ×R+s

θ(P)

s.t. Qn (P η) ≤ Q̄n (1 + τn )

θ(P)

s.t. Qn (P η) ≤ Q̄n (1 + τn )

and

Theorem S.1 of Mogstad, Santos, and Torgovitsky (2018) provides conditions under which
[θ̂lb  θ̂ub ] will be a consistent estimator of Θ in the Hausdorff metric. The result requires
Θ to be nonempty, so that the model is correctly specified. It also requires τn → 0. For the
empirical results in Section 5, I used τn = 025. This value was chosen because confidence
regions constructed using the method discussed in the next section require a similar tuning
parameter, and τn = 025 performed well in terms of size for the Monte Carlo simulations
discussed in Appendix S7.6.
11

In a previous draft of this paper, I used a quadratic criterion function. The finite sample performance
was slightly better, but the computation was significantly more difficult for reasons I discuss further in Appendix S7.5. Also, criterion functions that incorporate information on the covariance matrix for the moments
may have preferable statistical properties; see Andrews and Soares (2010) and Andrews and Barwick (2012).
However, Studentizing the moments introduces nonlinearities when the moments are not additively separable in P. This severely complicates computation, because the constraint sets for the optimization problems
proposed ahead become potentially non-convex.

16

ALEXANDER TORGOVITSKY

S7.3. Confidence Regions
As is common in the literature on inference under partial identification, I will construct
confidence regions through test inversion.12 The tests will be of null hypotheses taking the
form H0 : t ∈ Θ for conjectured scalar values t. A natural test statistic for this null is a
profiled version of Qn :
Q̄n (t) ≡

inf

d

(Pη)∈Pd† (t)×R+s

Qn (P η)

(S24)

where Pd† (t) ≡ {P ∈ Pd† : θ(P) = t}. Constructing a confidence region for Θ by inverting
these tests means collecting all t for which Q̄n (t) is not “too large.”
To operationalize such a test, one needs to determine how large is “too large” by approximating the distribution of Q̄n (t) under the null hypothesis. The asymptotic distribution of Qn (P η) is itself nonstandard due to the lack of point identification; see, for
example, Chernozhukov, Hong, and Tamer (2007), Andrews and Soares (2010), Bugni
(2010), and Canay (2010). There is an added difficulty here caused by the infimum in
the definition of Q̄n (t), which is introduced by the desire to conduct profile (or “subvector”) inference on Θ rather than P  . The two procedures I consider for this problem are
subsampling and the shape restriction approach of Chernozhukov, Newey, and Santos
(2015).13
The subsampling approach approximates the distribution of Q̄n (t) under the null hypothesis with the distribution of
Q̄bSS (t) ≡

inf

d

(Pη)∈Pd† (t)×R+s

QbSS (P η)

where QbSS (P η) is defined analogously to Qn (P η), but constructed instead using a subsample {Wi  }bi=1 of size b that is randomly drawn (without replacement) from {Wi }ni=1 . This
profiled subsampling procedure was first proposed in Romano and Shaikh (2008); see
also Chernozhukov, Hong, and Tamer (2007) and Romano and Shaikh (2010). In the following, I refer to the test that rejects H0 : t ∈ Θ when Q̄n (t) is greater than the 1 − α
quantile of Q̄bSS (t) based on B random subsamples as the SS test. A 1 − α SS confidence
region for Θ is the set of all t for which the SS test does not reject.
The Monte Carlo simulations in the next section suggest that the SS test can be quite
conservative in the DPO model. This leads to low power and excessively wide confidence
regions. The procedure for testing shape constraints proposed by Chernozhukov, Newey,
and Santos (2015, “CNS”) provides an alternative that turns out to be less conservative in
the DPO model. Their approach is based on a careful approximation of Q̄n (t) that takes
into account the shape of the constraint set Pd† (t) × Rd+s .
12

See Canay and Shaikh (2017) for a recent survey of the literature.
In a previous version of this paper, I also applied the method proposed by Bugni, Canay, and Shi (2017).
Monte Carlo results reported in that version of the paper suggest that this approach has low power in the
DPO model. Another recently proposed procedure for profile inference in partially identified models is Kaido,
Molinari, and Stoye (2016). Unfortunately, their approach is not computationally feasible for the dimension
of the nuisance parameters (P) considered here.
13

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

17

To describe their procedure, first define the function
Qn (P η g h) ≡

ds

j=1

+

1
∇mj (Wi  P)[g] − hj
n i=1
n


ξnj
(P) +

dm


1
∇mj (Wi  P)[g] 
n i=1
n


ξnj
(P) +

j=ds +1

Here, (g h) are parameters that serve as local deviations to (P η) and, correspondingly,
have dimensions 22T +1 and ds , respectively. The notation ∇mj (Wi  P)[g] stands for the
directional derivative of mj with respect to P, evaluated at P, in the direction g, that is,
∂

mj (Wi  P + κg)|κ=0 . The function ξnj
is defined for each j = 1     dm as
∂κ

1    
ξ (P) ≡ √
mj Wi  P − m̄nj (P) 
n i=1
n


nj

where {Wi  }ni=1 is a bootstrap sample drawn i.i.d. with replacement from {Wi }ni=1 . CNS then
approximated the distribution of Q̄n (t) with that of
Q̃n (t) ≡ min Qn (P η g h)
(Pηgh)

s.t. (P η) ∈ R (t) and
(P η) + n−1/2 (g h) ∈ Pd† (t) × Rd+s 

(S25)

The second constraint here restricts (g h) to be small deviations of (P η) that remain
inside the parameter space under the null hypothesis.14 The first constraint uses the definition


R (t) ≡ (P η) ∈ Pd† (t) × Rd+s : Qn (P η) ≤ Q̄n (t)(1 + τn ) 
which is the set of (P η) that approximately solve (S24). In the Monte Carlo simulations
in the next section, I find that τn = 025 works well in terms of size, so this is the value that
I use in the application in Section 5.
The distribution of Q̃n (t) can be approximated by redrawing {Wi  }ni=1 a large number
(B) of times and computing Q̃n (t) for each draw. I refer to the test that rejects H0 : t ∈ Θ
when Q̄n (t) is greater than the 1 − α quantile of these B values of Q̃n (t) as the CNS test.
A 1 − α CNS confidence region for Θ is the set of all t for which the CNS test does not
reject.
CNS provided a set of sufficient conditions under which their procedure controls size
uniformly (see their Theorem 6.3). Most of the sufficient conditions are satisfied immediately for the specifications used here because P is finite-dimensional and compact, and
both the moment conditions and constraints on P are linear in P. This includes CNS’s
Assumptions 2.1–2.2, 3.2–3.3, 4.1–4.2, 5.1–5.4, 6.1–6.5. Their Assumption 3.1 is satisfied
14

CNS included an additional tuning parameter that regulates the slackness of inequality constraints in
× Rd+s . This parameter is theoretically necessary; however, it introduces a non-convexity into (S25) that
renders the problem computationally intractable. The Monte Carlo simulations in Appendix S7.6 suggest the
CNS test performs well in the DPO model even when this parameter is omitted.
Pd† (t)

18

ALEXANDER TORGOVITSKY

when the sample is drawn i.i.d., and their Assumption 3.4 is satisfied because the objective function is unweighted. Assumption 6.6 is a rate condition on the tuning parameter
τn and the additional tuning parameter used by CNS that I am excluding for computational considerations; see footnote 14. The sufficient conditions also include a high-level
anti-concentration condition (their Assumption 6.7), which is mild in a finite-dimensional
setting.
S7.4. Testing for Misspecification
An attractive feature of a partial identification strategy is that it provides an immediate
specification test based on the nonemptyness of the identified set. Specifically, a rejection
of the null hypothesis H0 : P  = ∅ is evidence of the nonexistence of a P ∈ P † that is
consistent with the data, and hence evidence that some of the assumptions embodied in
P † are false, that is, that the model is misspecified. A natural statistic for such a test is the
overall minimum criterion value, Q̄n , defined in (S23). The results of CNS show that one
can approximate the distribution of Q̄n by simulating the distribution of a quantity that is
analogous to (S25), but which replaces Pd† (t) by Pd† throughout. A level-α misspecification
test rejects the null that the identified set is nonempty when Q̄n is greater than the 1 − α
quantile of this simulated distribution. Note that such a test always fails to reject when
the estimated identified set is nonempty, since in such cases Q̄n = 0.
S7.5. Computing Critical Values
In order to implement the SS and CNS tests, it is important to be able to reliably solve
the optimization problems that define Q̄n (t) and Q̃n (t). Reliability—in particular, ensuring that local optima are in fact global optima—is especially important here because each
problem needs to be solved a large number of times in the process of resampling and inverting hypothesis tests to construct confidence regions. Both problems are convex as long
as each m̄nj (P) is linear in P for every j and Pd† (t) is determined by the intersection of
linear equalities and inequalities. These conditions are satisfied by all of the assumptions
and parameters discussed in Section 2.
Under these conditions, the optimization problem in the definition of Q̄n (t) (and hence
Q̄bSS (t)) can be reformulated as a linear program, using a standard reformulation argument for the absolute value function. As a result, solving this problem is not significantly
harder than solving the linear programs used to directly estimate the bounds of the identified set. The optimization problem in the definition of Q̃n (t) can also be shown to be
a linear program, again by reformulating the absolute value function. This is the motivation for choosing the absolute loss function in (S21) rather than a quadratic loss function.
With quadratic loss, (S25) would be a (convex) quadratically-constrained quadratic program, due to the definition of R (t). While such programs can still be solved reliably using
widely available solvers, they are significantly more costly to solve than the corresponding
problem using an absolute loss function.15
15
In previous drafts of this paper, I used a quadratic loss function and solved the quadratically-constrained
quadratic programs. This procedure was substantially more computationally demanding.

19

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE
TABLE SI
FINITE SAMPLE PROPERTIES OF ESTIMATED BOUNDS IN COLUMN (6) OF TABLE IIa
θ̂lb

θ̂ub

Sample size

1718

3435

6870

1718

3435

6870

SD+
avg

true
mean
std
rmse
5/95%
min/max

0034
0047
0009
0015
0031
0017

0034
0043
0009
0013
0029
0020

0034
0040
0007
0010
0030
0020

0933
0931
0006
0006
0940
0947

0933
0931
0004
0005
0938
0942

0933
0932
0003
0003
0937
0941

SD+
avg (·|0)

true
mean
std
rmse
5/95%
min/max

0238
0319
0067
0105
0207
0120

0238
0300
0070
0093
0193
0123

0238
0281
0058
0072
0193
0119

0569
0546
0029
0037
0594
0622

0569
0550
0026
0032
0591
0615

0569
0554
0021
0026
0589
0605

SD+
avg (·|00)

true
mean
std
rmse
5/95%
min/max

0414
0553
0119
0183
0348
0193

0414
0521
0125
0165
0329
0214

0414
0487
0104
0127
0328
0202

0980
0942
0032
0050
0988
100

0980
0948
0030
0044
0993
100

0980
0956
0026
0035
0994
100

SD+
avg (·|1)

true
mean
std
rmse
5/95%
min/max

0016
0026
0007
0012
0016
0006

0016
0024
0006
0010
0014
0009

0016
0021
0005
0007
0014
0011

0963
0959
0005
0006
0966
0972

0963
0960
0004
0005
0966
0969

0963
0961
0003
0004
0966
0968

SD+
avg (·|11)

true
mean
std
rmse
5/95%
min/max

0017
0027
0007
0012
0016
0006

0017
0024
0006
0010
0015
0009

0017
0022
0005
0007
0014
0011

0993
0989
0004
0006
0995
0998

0993
0990
0004
0005
0995
0997

0993
0991
0003
0003
0995
0997

P[Θ = ∅ in sample]

0882

0624

0334

–

–

–

a The bounds are computed with T = 6 under Assumption ST with m = 4. The row 5/95% gives the 0.05 quantile across simulations
of θ̂lb and the 0.95 quantile of θ̂ub . Similarly, the row min/max gives the minimum across simulations of θ̂lb and the maximum across
simulations of θ̂ub . The final row shows the proportion of simulations in which the sample identified set is empty. The statistics are
based on 500 replications and the tuning parameter is set at τn = 025.

S7.6. Monte Carlo Simulations
In this section, I report the results of a Monte Carlo study that evaluates the procedures
discussed in the preceding sections. The data generating process in the study draws Yi
according to the empirical probabilities in the SIPP sample used in Section 5.
Table SI reports the finite sample properties of the estimated bounds, θ̂lb and θ̂ub , using
the same time horizon as in the application (T = 6), and maintaining Assumption ST with
m = 4 as in column (6) of Table II. The statistics are based on 500 simulation draws, and
the tuning parameter τn is set to 025. Comparing results across increasing sample sizes

20

ALEXANDER TORGOVITSKY

FIGURE S1.—Density of estimated bounds in column (6) of Table II. Notes: The plot shows kernel density
estimates of the directly estimated lower and upper bounds of SD+
avg (·|0) from the Monte Carlo reported in
Table SI. The gray dotted lines indicate the true lower and upper bounds of the identified set in the DGP. The
smoothing used a Gaussian kernel and Silverman’s rule-of-thumb bandwidth.

suggests that the bound estimators are consistent. Figure S1 plots the estimated density
of θ̂lb and θ̂ub when the target parameter is SD+avg (·|0). The distributions are nonnormal
and nonstandard, which accords with theoretical predictions; see, for example, Section 5
of Shapiro and Dentcheva (2014).
Table SII reports the rejection rates of the SS and CNS tests of H0 : t ∈ Θ at nominal
levels α = 001, 005, and 010 for several values of t. The data generating process is still
the SIPP sample, but I only use the first four periods (T = 3) in order to moderate the
computational burden. The maintained assumptions are Assumption ST with m = 1 and
Assumption MTR, with the latter assumption also being useful for easing computation.
The reported statistics are based on 500 simulation draws, 500 bootstrap draws or subsamples per simulation, and still uses the tuning parameter τn = 025. The sample size
TABLE SII
FINITE SAMPLE REJECTION PROBABILITIES OF SS AND CNS TESTSa
Rejection probability of H0 : t ∈ Θ for t = · · ·
level
0.01
0.05
0.10

test

0.090

0.150

0.210

0.270

0.300

0.338

0.445

0.480

0.510

0.540

0.600

0.660

SS
CNS
SS
CNS
SS
CNS

0.098
0.724
0.408
0.876
0.602
0.920

0.032
0.478
0.190
0.716
0.394
0.802

0.004
0.242
0.066
0.472
0.164
0.596

0.000
0.078
0.012
0.204
0.046
0.330

0.000
0.034
0.006
0.118
0.016
0.200

0.000
0.012
0.004
0.048
0.010
0.092

0.000
0.008
0.006
0.050
0.010
0.102

0.002
0.042
0.014
0.158
0.032
0.242

0.004
0.146
0.036
0.326
0.094
0.430

0.018
0.290
0.094
0.522
0.208
0.648

0.140
0.724
0.394
0.906
0.588
0.964

0.578
0.982
0.912
1.00
0.984
1.00

a The target parameter is SD+ (·|0) and Assumptions ST (with m = 1) and MTR are maintained. The time horizon is T = 3. The
avg
values of t in boxes indicate the lower and upper bound of the population identified set. The statistics are based on 500 replications,
500 bootstraps (for CNS) or subsamples (for SS, with b = n2/3 = 228), and the tuning parameter is set at τn = 025.

21

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE
TABLE SIII
FINITE SAMPLE PROPERTIES OF ESTIMATED BOUNDS FROM TABLE SIIa
θ̂lb

θ̂ub

Sample size

1718

3435

6870

1718

3435

6870

SD+
avg

true
mean
std
rmse
5/95%
min/max

0037
0030
0009
0011
0014
0001

0037
0032
0007
0008
0019
0010

0037
0034
0005
0006
0026
0017

0925
0925
0006
0006
0934
0942

0925
0925
0005
0005
0932
0938

0925
0925
0003
0003
0931
0935

SD+
avg (·|0)

true
mean
std
rmse
5/95%
min/max

0338
0274
0075
0099
0126
0006

0338
0295
0062
0075
0172
0077

0338
0316
0045
0050
0235
0168

0445
0457
0040
0042
0527
0595

0445
0454
0036
0037
0520
0587

0445
0448
0028
0028
0499
0548

SD+
avg (·|00)

true
mean
std
rmse
5/95%
min/max

0614
0500
0136
0177
0221
0011

0614
0537
0115
0138
0307
0141

0614
0575
0084
0093
0422
0286

0807
0828
0057
0060
0927
0962

0807
0821
0051
0053
0914
0989

0807
0812
0041
0041
0882
0930

SD+
avg (·|1)

true
mean
std
rmse
5/95%
min/max

0016
0013
0004
0005
0006
0000

0016
0014
0003
0004
0008
0004

0016
0015
0003
0003
0011
0006

0957
0957
0005
0005
0964
0968

0957
0957
0004
0004
0962
0967

0957
0957
0003
0003
0961
0965

SD+
avg (·|11)

true
mean
std
rmse
5/95%
min/max

0017
0014
0004
0005
0006
0000

0017
0015
0004
0004
0008
0004

0017
0016
0003
0003
0011
0007

0990
0990
0003
0003
0995
0998

0990
0990
0003
0003
0994
0996

0990
0990
0002
0002
0993
0996

P[Θ = ∅ in sample]

0428

0358

0290

–

–

–

a The bounds are computed with T = 3 under Assumption ST with m = 1 and Assumption MTR. See notes for Table SI.

is set at n = 3435, as in the application, and the subsample size is set to 288 ≈ n2/3 . The
target parameter is taken to be SD+avg (·|0). For comparison, Table SIII reports the finite
sample performance for the estimated bounds in this case.
The results for t at the boundary of the identified set suggest that the SS test is quite
conservative. This leads to low power when testing points outside of the identified set, and
thus large confidence regions when constructing these regions through test inversion. In
contrast, the CNS test maintains roughly the nominal level at the boundary of the identified set, and is much more powerful at points outside of the identified set. Table SIII
suggests that the CNS test may still have low power in this setting; for example, it rejects
t = 0150 only about 72% of the time, even though this point is more than three standard
deviations smaller than the lower bound of the identified set. These results provide reassurance that the CNS test works reasonably well for the DPO model, at least for the

22

ALEXANDER TORGOVITSKY

empirical setting considered here, and provides at least a conservative indication of the
impacts of statistical uncertainty.
S8. ADDITIONAL EMPIRICAL RESULTS
Table SIV contains estimates for the SIPP data under some specifications that maintain
Assumptions MATR and/or DSC.
TABLE SIV
ADDITIONAL ESTIMATES FROM THE DPO MODEL
(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

Assumptions
ST(m)
MTS(q)
MATR
DSC

2


2


2

2






2
2

2
2



Misspecification
Θ = ∅
in sample

No

No

No

No

No

No

No

No

p-value for
H0 : Θ = ∅
Bounds
SD+
avg

0000
0947

0000
0947

0025
0933

0025
0933

0027
0933

0027
0933

0025
0380

0027
0380

SD+
avg (·|0)

0000
0581

0000
0581

0193
0576

0193
0576

0193
0576

0193
0576

0193
0560

0193
0560

SD+
avg (·|00)

0000
100

0000
100

0335
0992

0335
0992

0335
0992

0335
0992

0335
0965

0335
0965

SD+
avg (·|1)

0000
0970

0000
0970

0010
0966

0010
0966

0010
0966

0010
0966

0010
0371

0010
0371

SD+
avg (·|11)

0000
100

0000
100

0010
0996

0010
0996

0010
0996

0010
0996

0010
0383

0010
0383

SDavg

0000
100

0000
100

0054
0976

0054
0976

0054
0976

0054
0976

0054
0423

0054
0423

REFERENCES
ANDREWS, D. W. K., AND P. J. BARWICK (2012): “Inference for Parameters Defined by Moment Inequalities:
A Recommended Moment Selection Procedure,” Econometrica, 80, 2805–2826. [15]
ANDREWS, D. W. K., AND S. HAN (2009): “Invalidity of the Bootstrap and the m out of n Bootstrap for Confidence Interval Endpoints Defined by Moment Inequalities,” Econometrics Journal, 12, S172–S199. [13]
ANDREWS, D. W. K., AND G. SOARES (2010): “Inference for Parameters Defined by Moment Inequalities
Using Generalized Moment Selection,” Econometrica, 78, 119–157. [15,16]

NONPARAMETRIC INFERENCE ON STATE DEPENDENCE

23

BARTOLUCCI, F., AND V.
√ NIGRO (2010): “A Dynamic Model for Binary Panel Data With Unobserved Heterogeneity Admitting a n-Consistent Conditional Estimator,” Econometrica, 78, 719–733. [2]
BHULLER, M., C. N. BRINCH, AND S. KÖNIGS (2017): “Time Aggregation and State Dependence in Welfare
Receipt,” The Economic Journal, 127, 1833–1873. [3]
BONHOMME, S. (2012): “Functional Differencing,” Econometrica, 80, 1337–1385. [2]
BROWNING, M., AND J. M. CARRO (2010): “Heterogeneity in Dynamic Discrete Choice Models,” Econometrics
Journal, 13, 1–39. [3]
(2014): “Dynamic Binary Outcome Models With Maximal Heterogeneity,” Journal of Econometrics,
178, 805–823. [3]
BUGNI, F. A. (2010): “Bootstrap Inference in Partially Identified Models Defined by Moment Inequalities:
Coverage of the Identified Set,” Econometrica, 78, 735–753. [16]
BUGNI, F. A., I. A. CANAY, AND X. SHI (2017): “Inference for Subvectors and Other Functions of Partially
Identified Parameters in Moment Inequality Models,” Quantitative Economics, 8, 1–38. [16]
CANAY, I. A. (2010): “EL Inference for Partially Identified Models: Large Deviations Optimality and Bootstrap
Validity,” Journal of Econometrics, 156, 408–425. [16]
CANAY, I. A., AND A. M. SHAIKH (2017): “Practical and Theoretical Advances in Inference for Partially Identified Models,” in Advances in Economics and Econometrics, ed. by B. Honore, A. Pakes, M. Piazzesi, and L.
Samuelson. Cambridge University Press, 271–306. [16]
CARRO, J. M. (2007): “Estimating Dynamic Panel Data Discrete Choice Models With Fixed Effects,” Journal
of Econometrics, 140, 503–528. [2]
CHAMBERLAIN, G. (1984): “Chapter 22 Panel Data,” in Handbook of Econometrics, Vol. 2, ed. by Z. Griliches
and M. D. Intriligator. Elsevier, 1247–1318. [2,10]
(1985): “Heterogeneity, Omitted Variable Bias, and Duration Dependence,” in Longitudinal Analysis
of Labor Market Data, ed. by J. Heckman and B. Singer. Cambridge University Press. [2]
(2010): “Binary Response Models for Panel Data: Identification and Information,” Econometrica, 78,
159–168. [2]
CHERNOZHUKOV, V., H. HONG, AND E. TAMER (2007): “Estimation and Confidence Regions for Parameter
Sets in Econometric Models,” Econometrica, 75, 1243–1284. [14,16]
CHERNOZHUKOV, V., W. NEWEY, AND A. SANTOS (2015): “Constrained Conditional Moment Restriction
Models,” CEMMAP Working Paper CWP 59/15. [16]
DOMENCICH, T., AND D. L. MCFADDEN (1975): Urban Travel Demand: A Behavioral Analysis. North-Holland
Publishing Co. [13]
EMBRECHTS, P., AND M. HOFERT (2013): “A Note on Generalized Inverses,” Mathematical Methods in Operations Research, 77, 423–432. [4]
FANG, Z., T. HU, AND H. JOE (1994): “On the Decrease in Dependence With Lag for Stationary Markov
Chains,” Probability in the Engineering and Informational Sciences, 8, 385–401. [4]
FERNÁNDEZ-VAL, I. (2009): “Fixed Effects Estimation of Structural Parameters and Marginal Effects in Panel
Probit Models,” Journal of Econometrics, 150, 71–85. [2]
FREYBERGER, J., AND J. L. HOROWITZ (2015): “Identification and Shape Restrictions in Nonparametric Instrumental Variables Estimation,” Journal of Econometrics, 189, 41–53. [13]
GUROBI OPTIMIZATION (2015): “Gurobi Optimizer Reference Manual.” [10]
HECKMAN, J. J. (1981): “Heterogeneity and State Dependence,” in Studies in Labor Markets, ed. by S. Rosen.
University of Chicago Press. [10]
HONORÉ, B. (2002): “Nonlinear Models With Panel Data,” Portuguese Economic Journal, 1, 163–179. [2]
HONORÉ, B. E., AND E. KYRIAZIDOU (2000): “Panel Data Discrete Choice Models With Lagged Dependent
Variables,” Econometrica, 68, 839–874. [2]
HONORÉ, B. E., AND A. LEWBEL (2002): “Semiparametric Binary Choice Panel Data Models Without Strictly
Exogeneous Regressors,” Econometrica, 70, 2053–2063. [2]
HOPENHAYN, H. A., AND E. C. PRESCOTT (1992): “Stochastic Monotonicity and Stationary Distributions for
Dynamic Economies,” Econometrica, 60, 1387–1406. [8]
HU, Y., AND M. SHUM (2012): “Nonparametric Identification of Dynamic Models With Unobserved State
Variables,” Journal of Econometrics, 171, 32–44. [3]
HYSLOP, D. R. (1999): “State Dependence, Serial Correlation and Heterogeneity in Intertemporal Labor
Force Participation of Married Women,” Econometrica, 67, 1255–1294. [10]
IBM (2010): “IBM ILOG AMPL Version 12.2,” International Business Machines Corporation. [10]
IRACE, M. (2018): “Patient Loyalty in Hospital Choice: Evidence From New York,” Working Paper. [1]
JUDGE, G. G., W. E. GRIFFITHS, R. C. HILL, H. LÜTKEPOHL, AND T.-C. LEE (1985): The Theory and Practice
of Econometrics (Second Ed.). Wiley. [13]

24

ALEXANDER TORGOVITSKY

KAIDO, H., F. MOLINARI, AND J. STOYE (2016): “Inference on Projections of Identified Sets,” Working Paper.
[16]
KASAHARA, H., AND K. SHIMOTSU (2009): “Nonparametric Identification of Finite Mixture Models of Dynamic Discrete Choices,” Econometrica, 77, 135–175. [3]
LEHMANN, E. L. (1966): “Some Concepts of Dependence,” The Annals of Mathematical Statistics, 37, 1137–
1153. [7]
MAGNAC, T. (2000): “Subsidised Training and Youth Employment: Distinguishing Unobserved Heterogeneity
From State Dependence in Labour Market Histories,” The Economic Journal, 110, 805–837. [1]
MOGSTAD, M., A. SANTOS, AND A. TORGOVITSKY (2018): “Using Instrumental Variables for Inference About
Policy Relevant Treatment Parameters,” Econometrica, 86, 1589–1619. [15]
MOGSTAD, M., A. TORGOVITSKY, AND C. R. WALTERS (2019): “Identification of Causal Effects With Multiple
Instruments: Problems and Some Solutions,” Working Paper. [11]
PAKES, A., AND J. PORTER (2016): “Moment Inequalities for Multinomial Choice With Fixed Effects,” Tech.
rep. [2]
PROWSE, V. (2012): “Modeling Employment Dynamics With State Dependence and Unobserved Heterogeneity,” Journal of Business & Economic Statistics, 30, 411–431. [1]
ROMANO, J. P., AND A. M. SHAIKH (2008): “Inference for Identifiable Parameters in Partially Identified Econometric Models,” Journal of Statistical Planning and Inference, 138, 2786–2807. [16]
(2010): “Inference for the Identified Set in Partially Identified Econometric Models,” Econometrica,
78, 169–211. [16]
RUDIN, W. (1976): Principles of Mathematical Analysis. New York: McGraw-Hill. [4]
SHAPIRO, A., AND D. DENTCHEVA (2014): Lectures on Stochastic Programming: Modeling and Theory, Vol. 16.
SIAM. [13,20]

Co-editor Liran Einav handled this manuscript.
Manuscript received 1 February, 2016; final version accepted 30 April, 2019; available online 6 May, 2019.

