Rejoinder: Demystifying Double Robustness: A Comparison of Alternative Strategies for
Estimating a Population Mean from Incomplete Data
Author(s): Joseph D. Y. Kang and Joseph L. Schafer
Source: Statistical Science, Vol. 22, No. 4 (Nov., 2007), pp. 574-580
Published by: Institute of Mathematical Statistics
Stable URL: https://www.jstor.org/stable/27645863
Accessed: 21-10-2019 14:59 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and
extend access to Statistical Science

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:24 UTC
All use subject to https://about.jstor.org/terms

Statistical Science
2007, Vol. 22, No. 4, 574-580

DOI: 10.1214/07-STS227REJ
Main article DOI: 10.1214/07-STS227
? Institute of Mathematical Statistics, 2007

Rejoinder: Demystifying Double
Robustness: A Comparison of Alternative
Strategies for Estimating a Population
Mean from Incomplete Data
Joseph D. Y. Kang and Joseph L. Sch?fer
1. CLARIFYING OUR POSITION ON DOUBLY

do we wish to cast clouds of suspicion over all DR esti
mators in all circumstances. In many situations, they do
work well. On the other hand, we still believe that pro

ROBUST ESTIMATORS

We are grateful to the editors for eliciting comments
from some of the most prominent researchers in this
exciting and rapidly developing field. After we drafted
our article, a number of important works on DR estima
tors appeared, including Tan's (2006) article on causal

cedures motivated by parametric y-models, when care
fully implemented, remain a viable option and should
not be categorically dismissed.

Under ignorability, the propensities re i = P(t? =
1 |x/), i ? 1,..., n, play no role in likelihood-based or
Bayesian inference about ? under a given y -model.
If we had absolute faith in one parametric form for

inference, the monograph by Tsiatis (2006) and the
recent articles and technical reports cited by Robins,

Sued, Lei-Gomez and Rotnitzky. The discussants' in

P(yi \xi)> then we could discard all information beyond

sightful remarks highlight these recent developments
and bring us up to date.

the sufficient statistics for that model. But the propensi

ties carry information that helps us evaluate the quality
of the y-model, and we ignore this extra information at

Our purpose in writing this article was to provide
unfamiliar readers with gentle introduction to DR es
timators without the language of influence functions,
using only simple concepts from regression analysis
and survey inference. We wanted to show that DR es
timators come in many different flavors. And, without

our peril, because no model is above criticism. No sen
sible statistician would argue that propensities should

not be examined. But reasonable persons may differ
over what role the propensities should play in formu
lating an estimator. Those who favor a semiparamet
ric approach devise influence functions that combine

minimizing the importance of the literature spawned by
Robins, Rotnitzky and Zhao (1994), we wanted to draw
attention to some older related methods from model

inverse-propensity weights with regression predictions

for y. Parametric modelers, on the other hand, may
well argue that if the propensities reveal weaknesses

assisted survey sampling which arrive at a similar po
sition from the opposite direction.

in the y -model, then that model should be revised and

Despite the good performance of ?ois in our simu

corrected. The latter view has been expressed by Elliott
and Little (2000) in the context of survey estimation,

lated example, we have not and would not argue that it
be used routinely and uncritically. The pitfalls of rely

where the selection probabilities are known, but paral
lels to uncontrolled nonresponse and causal inference

ing solely on outcome regression or y-modeling have
been well documented for causal inference, where the
rates of missing information are high and the impact of

selection bias can be dramatic (e.g., Rubin, 2001). Nor
Joseph D. Y. Kang is Research Associate, The Methodology
Center, 204 E. Calder Way, Suite 400, State College,

are obvious.

We believe that propensities are useful for model di
agnosis and estimation, but we are still not convinced
that they need to enter an influence function as inverse
propensity weights. The strength of weighting is that, if

done properly, it protects an estimate from bias regard

Pennsylvania 16801, USA (e-mail:
josephkang@stat.psu.edu). Joseph L. Schafer is Associate

less of how y is distributed. But this strength can also

Professor, The Methodology Center, The Pennsylvania State
University, 204 E. Calder Way, Suite 400, State College,

be a weakness, because such a high level of protection
is not always warranted. If the propensities are unre

Pennsylvania 16801, USA (e-mail: jls@stat.psu.edu).

lated to the linear predictors from a good y -model, then

574
This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:24 UTC
All use subject to https://about.jstor.org/terms

REJOINDER 575
weighting may be superfluous. If the propensities are
poorly estimated or extreme, then combining weights
with the regression predictions may do more harm than

good. And if the propensities do reveal weaknesses
in the y -model, inverse-propensity weights are not the

only way to correct them.

2. RESPONSE TO TSIATIS AND DAVIDIAN
In their illuminating discussion, Tsiatis and Davidian
demonstrate that a wide variety of estimators for ? can

We like the suggestion by Tsiatis and Davidian
of using a hybrid estimator that combines inverse
propensity weighting for cases with moderate propen
sity and regression predictions for cases with small
propensity, an idea echoed by van der Laan and Rubin
(2006). As an alternative to a hard threshold S at which
the change is made, one could opt for a smoother tran
sition by "weighting" each part of the influence func

tion more or less depending on the estimated propen

sity. We also agree with Tsiatis and Davidian that

based on an influence function. (One possible excep

estimators in the spirit of ?Xn.cov deserve more con
sideration even though they are not DR over M?U Mu
in the usual asymptotic sequence. In the simulations

tion is the class of estimators based on propensity-score

of our article, we expressed m, as a piecewise con

be expressed as the solution to an estimating equation

matching, which we have not discussed.) Influence

stant function of 717 with discontinuities at the sample

functions present interesting results on semiparametric

quintiles of ft i. Another version of ?X7l.cov that we have

efficiency, but we find them appealing for other reasons

found to work well in many situations uses a linear

as well. First, they show us how to compute a standard
error for whatever estimator we choose. Second, they

spline in fa ? log(7T?/(l ? ft i)) with knots at the quin

generalize nicely to finite-population sample surveys
with complex designs. Regression techniques for com
plex surveys, as implemented in software packages like

SUDAAN (Shah, Barnwell and Biler, 1997), are based
on weighted influence functions, so any of the estima
tors described by Tsiatis and Davidian can be extended
to surveys. Third, if we move on to causal inference, we
must address the thorny issue of the inestimable par
tial correlation between the potential outcomes. Any

estimator of an average causal effect makes a work
ing assumption about this correlation (e.g., setting it to
zero), but a standard error computed from an influence

function sandwich may still perform well when this
working correlation is incorrect.

Tsiatis and Davidian mention that our estimator

?n-cov, which incorporates propensity-related basis
functions into the OLS procedure, is not consistent un

der Mi U Mp unless the conditional mean of yi hap
pens to be a linear combination of the particular basis
functions for 7rf used in the OR model. This is cer
tainly true for the usual asymptotic sequence in which
the number of basis functions remains fixed as n -> oo.

But if we allow the basis to grow with the sample size

(e.g., as in a smoothing spline), then it may become

tiles.

3. RESPONSE TO TAN
Tan's important work on regression estimators con
nects the theory of influence functions to ideas of sur
vey regression estimators and the use of control vari

ates in importance sampling. His remarks and propo
sitions are very helpful for understanding the behavior
of IPW, OR and DR methods in realistic settings where
all the models are incorrect.

We were initially puzzled by several of Tan's points
but, upon further consideration, found them to be very
insightful. He states that it is more constructive to view

DR estimators as efficiency-enhanced versions of IPW
than as bias-corrected versions of OR. We find both
views helpful for understanding the nature and prop

erties of DR methods. But, as he explains, there are
theoretical reasons to expect that his carefully crafted
DR estimators may lead to greater improvement over

IPW than over a good OR model, because IPW is con
servative whereas OR is aggressive.
We are still unsure why Tan states that IPW extrapo
lates explicitly whereas OR extrapolates implicitly. To
us, fitting an OR model to respondents and using that
model to predict for nonrespondents is a very obvious

DR (Little and An, 2004). Given a large sample, a good

kind of extrapolation, especially if the leverage val

data analyst will tend to fit a richer model than with a

ues for some nonrespondents are large relative to those

small sample. If the analyst is allowed to build a rich
OR model that corrects for the kind of inadequacies

of the respondents. But his points about extrapolation
are well taken. All of our methods extrapolate. The as
sumption of ignorability is itself an extrapolation.
He also points out that estimating an average causal
effect is more subtle than simply estimating the mean

shown in our Figure 4, then the OLS procedure based
on the corrected OR model may be as good as any DR

procedure.

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:24 UTC
All use subject to https://about.jstor.org/terms

576 J. D. Y. KANG AND J. L. SCH?FER
of each potential outcome and taking the difference.

this trend by introducing the squared linear predictor

This distinction is important in a semiparametric ap

from the logit model fjj = (xf?)2 as one more covari

proach. A semiparametric method that is optimal for
estimating two means may not be optimal for estimat
ing the mean difference. Similarly, a method that is op
timal for estimating a population average causal effect
may not be optimal for estimating the average effect
among the treated, or for estimating differences in av
erage causal effects between subpopulations. As para
metric assumptions about the OR model are discarded,
it becomes important to tailor the estimation procedure
to the estimand, which his regression estimators appar
ently do.
In Tan's simulations, his alternative model in which

the analyst sees X4 = (Z3 + Z4 + 20)2 presents an
interesting situation where OLS predicts the y?'s for
the respondents almost perfectly (R2 ^ 0.99), but the
extrapolated linear predictions for the nonrespondents

are biased because the unseen true values of y? turn
sharply away from those predictions in the region of
low propensity. This is a perfect illustration of how

the uncritical use of ?jlols can lead us astray. But in
this example, propensity-based diagnostics reveal ob
vious deficiencies in the linear model. Taking the ini
tial sample of n = 200 observations from our article,
we fit the linear model to the respondents and a logistic

propensity model to all cases given X\, X2, X3, and
Tan's alternative version of X4. A plot of the observed
residuals from the 3;-model versus the estimated logit
propensities from the ix -model is shown in Figure 1.
The loess curve clearly shows that the OLS predictions
are biased in the region of high propensity (where it
does not really matter) and in the region of low propen
sity (where it matters very much). If we account for

ate in the y-model, the performance of ?Xols greatly
improves. Even better performance is obtained with
splines, which tend to predict better than ordinary poly
nomials over the whole range of fji 's. We created a lin
ear spline basis for fji with four knots located at the
sample quintiles of fji. That is, we added the four co

variates

(1)

(fa -k\)+, (fa -?2)+,
(fji -/c3)+, (m ~k4)+

to the y model, where (?)+ = max(0, z) and k\, k^, &3>

k4 are the knots. Over 1000 samples, we found that
this new version of ?Xols (which, in our article, we
would have called ?Xn.cov) performed as well as any
of Tan's estimators in the scenario where both models

were incorrect. With n = 200, we obtained bias = 0.16,

% bias - 5.70, RMSE = 2.78 and MAE = 1.78. With
n = 1000, we obtained bias = 0.30, % bias = 24.6,
RMSE = 1.27 and MAE = 0.88. The performance of
Tan's regression estimators in these simulations is im
pressive. The performance of ?Xols is equally impres
sive if we allow the analyst to make a simple correction
to adjust for the y-model's obvious lack of fit.

4. RESPONSE TO RIDGEWAY AND MCCAFFREY
Ridgeway and McCaffrey correctly observe that, for
estimating propensity scores, there are many good al
ternatives to logistic regression. In addition to their

work on the generalized boosted model (GBM), some
have been estimating propensities using classification

trees (Luellen, Shadish and Clark, 2005) and neural
networks (King and Zeng, 2002).
A rich propensity model should improve the per
formance of the weighted estimator. The advantage
of procedures like classification trees and GBM is
that they allow us to search through a large space of
7t -models, accounting for the effects of many covari
ates and their interactions, thereby reducing bias in the
resulting estimator regardless of how y i is distributed.

These procedures may also reduce variance, because,
as explained by Tan, in a sequence of increasingly rich
propensity models, the asymptotic variance of an aug
linear predictor

FlG. 1. Scatterplot of raw residuals from linear y-model fit to
respondents in Tan's alternative model, versus the linear predictors
from a logistic it-model, with local polynomial (loess) fit.

mented IPW estimator decreases to the semiparamet
ric bound. In principle, one could apply similar pro
cedures like regression trees to create a rich y-model.
But, as Ridgeway and McCaffrey point out, this raises
the possibility of data snooping. As we search through

larger and more complicated spaces to find the best

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:24 UTC
All use subject to https://about.jstor.org/terms

REJOINDER 577
j-model, it becomes increasingly difficult to compute
honest standard errors.

When Bang and Robins (2005) recommended cer

Ridgeway and McCaffrey's simulations with the ex

tain DR procedures for routine use, they did so with
out qualifications or cautionary statements. Now they

tra interaction term again reveal the dangers of un
critically relying on ?ois- This interaction increases

quote a passage from another article published five
years earlier, which Bang and Robins (2005) did not

the degree to which the additive and linear y -model

cite, to demonstrate that this was not what they had in

is misspecified, so in this scenario we would expect

mind. Readers cannot react to what they have in mind,

the performance of ?ois to worsen. The final columns
of their Tables 1 and 2 show that, when this interac

but only to what they write. Dr. Robins and his col

tion is present, propensity-based and DR estimators

carry considerable weight. The fact that they knew that
these estimators sometimes misbehave but failed to ac

strongly outperform ?oLS- Using the wrong covariates

leagues are eminent researchers, and their statements

ible GBM procedure. But one could argue that these
comparisons between GBM and ?ois are unfair in the
following sense: They resemble a situation where the

knowledge it makes their blanket recommendations in
2005 even more troubling.
For the record, we will clarify how we came up with
our simulated example. As mentioned in our Section 4,

analyst is allowed to fit a rich and flexible n -model but

we were trying to loosely mimic a quasi-experiment

in the propensity model does little harm to the flex

is given no leeway to improve the j-model. We exam

ined many samples of n ? 200 from this new popu
lation and found X\X2 to be a strong and highly sig
nificant predictor of y in every sample. If we add this
one interaction to the y-model, the bias in ?jlols nearly
vanishes, and its RMSE becomes comparable to that of
the best DR estimators that Ridgeway and McCaffrey
tried. Other interactions are often significant as well.
We have not examined the performance of ??ols when
these other interactions are included; doing so would
be an interesting exercise.
Our point here is not to argue for the superiority of

?OLS over the DR procedures. Either can work well
if applied carefully with appropriate safeguards. And
either can be made to fail if we, through the design of a

simulation, impose artificial restrictions that force the

analyst to ignore clear evidence in the observed data
that the procedure is flawed.

5. RESPONSE TO ROBINS, SUED, LEI-GOMEZ
AND ROTNITZKY
The comments by Robins et al. contain many use

to assess the average causal effect of dieting on body

mass index among adolescent girls. We decided be
forehand that yi should be predicted from the ob
served Xi with R2 ^ 0.80, as in the actual data. We
decided that the distributions of the estimated propen
sity scores should resemble those in our Figure 3(e),

as in the actual data. We decided that the linear pre
dictors from the y-model and n -model should have a
correlation of at least 0.5, as in the actual data, so that

y\ = J2i Uyi/Hi U would be a strongly biased as an
estimator of ?. We decided that the covariates in jc?

should not be normally distributed, but they should not
be so heavily skewed that a data analyst would need to
transform them to reduce the leverage of a few large
values. We decided that jcz- must be a one-to-one trans
formation of the unseen true covariates Zi over the ef

fective support of the Zi (without this condition, non
response would not be ignorable given jc?). Finally, we
decided that the linear regression of yi and the logistic
regression of ti on xz would be misspecified to about
the same extent, in the sense that the correlations be
tween the linear predictors from each model and the
corresponding true linear predictors would be about

ful observations and helpful references. Their simula
tions that reverse the roles of t? and 1 ? t? are instruc

0.9.

tive. However, in the process of arguing that we mis

one example that met all of these criteria. As we ran
our simulations, we were truly surprised to see ?Xols
perform as well as it did, consistently beating all com
petitors. We expected that at least some of the DR es
timators would improve upon ?Xols> but none did. In
fact, we were tempted to look for a different example
that would demonstrate some of the benefits of DR, but

understood the message of Bang and Robins (2005),
they have apparently misunderstood ours. Their insinu
ations of cherry-picking might be understandable if we
had been arguing for the superiority of ?jlols, but that is

not what we have done. Quite honestly, we began this
investigation fully expecting to demonstrate the ben
efits of dual modeling when neither model is exactly
true.

After considerable trial and error, we came up with

we decided against it precisely because we wanted to
avoid cherry-picking.

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:24 UTC
All use subject to https://about.jstor.org/terms

578 J. D. Y. KANG AND J. L. SCHAFER

As Robins et al. deconstruct our simulated exam
ple, they suggest that our misspecified linear model

E(yi) = xj? is so close to being true that ??ols is

virtually guaranteed to outperform all competitors. If

that were so, then why did the DR estimators ?wis
and ?jlbc-ols not perform as well, as those estimators

were given the same opportunity to take advantage
of this nearly correct y-model? And, if that were so,
why would ??ols perform so poorly in their simulations
when the roles of t? and 1 ? *,- were reversed?

X~???^i?-1-r~
100 200 300 400

The first plot in Figure 1 by Robins et al. reveals
that (a) the model for y? given the vector of true co

variates Zi is a linear with very high R2 and (b) the
nonresponse is ignorable, so that P(y? | Zi,U = 1) and
P(yi I Zi,ti = 0) are the same. This plot implies that
conditions where the analyst is allowed to see the zi 's
are unrealistic, because knowing zi is essentially equiv
alent to knowing y?. But this plot says nothing about
the performance of ??ols or any other estimator when

Zi is hidden and the analyst sees only jc?, which is
the only scenario that we have claimed is realistic. [In
fact, the first simulated example published by Bang and

Robins (2005) yields a similar picture, because their
true data-generating mechanism is also linear and their

R2 is 0.94.] The conditional variance V(y? \ Zi) was
one of many parameters that we had to adjust to cre
ate an example that satisfied all of the criteria that we
have mentioned. We tried to set V(y? \ Zi) to larger val
ues, but doing so decreased the signal-to-noise ratio in
the observed data to the point where we no longer saw
meaningful biases in any estimators when n = 200.

With their Figure 2, Robins et al. purport to show

Predicted value

FlG. 2. Residuals versus predicted values for respondents
(ti = 1) (black dots) and nonrespondents (t? = 0) (gray dots) from
one sample of n ? 1000 from our original simulation, with local
polynomial (loess) trends for each group. For visual clarity, only
20% of the sampled points are shown.

points are displayed, but the loess trends are estimated
from the full sample.) For each group, the least-squares
regression model strongly underpredicts near the cen
ter and overpredicts at the extremes. The reason why
?Xols performs well in this example is not that the linear

model is approximately true, but that the positive and
negative residuals in the nonrespondent group approx

imately cancel out. The average value of y? ? xj ? for
respondents is exactly zero (a consequence of OLS),
and the average value of y? ? xj ? for nonrespondents
is close to zero. Over 1000 simulated samples, the av
erage of y i ? xj ? among nonrespondents was 1.68.
Multiplying this by ?0.5 (because the average nonre

well that the predicted values xj ? are essentially un

sponse rate is 50%) gives ?0.84, the estimated bias for
?OLS reported in our Table 3.
Figure 2 also reveals why ?Xols was not beaten in this

can see that the predicted values of the nonrespon

example by any of the dual-modeling methods. The
differences between the two loess curves in Figure 2
are not large, showing that the OLS predictions have

that our misspecified linear regression model fits so
biased predictions of the missing y,-'s, which guaran
tees excellent performance for ?jlols- They state, "We
dents are reasonably centered around the straight line
even for those points with predicted values far from the

predicted values of the respondents." On the contrary,

our linear model E(y?) = xj? does not give unbiased

similar patterns of bias for respondents and nonrespon

dents. When the predictions from a j-model are bi
ased, and the biases are similar when t? = 1 and ti = 0,
they are not easily corrected by an estimated propensity

predictions for nonrespondents or respondents, espe
cially not in the region of extrapolation. To illustrate,

model.

we took one simulated sample of n = 1000 observa
tions, regressed y? on X[ among the respondents, and

al. have done, the situation dramatically changes. Tak
ing the same sample of n = 1000, we regressed y i on

computed the regression predictions xj ? and resid

xi when ti = 0 and predicted the responses for both

uals y i ? xj ? for both groups. A plot of the resid
uals versus the regression predictions is displayed in
Figure 2, along with local polynomial (loess) trends.

groups. Residuals versus predicted values from this re

Respondents are shown in black, and nonrespondents
are shown in gray. (For visual clarity, only 20% of the

If we reverse the roles of t\ and 1 ? t?, as Robins et

verse fit are shown in Figure 3. (Once again, for vi
sual clarity, only 20% of the sampled points are shown,
but the loess trends are estimated from the full sam

ple.) For the *,- = 0 group, the linear model underpre

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:24 UTC
All use subject to https://about.jstor.org/terms

REJOINDER 579
the linear y-model to overpredict when P(ti = 1) is
low or high. To correct this bias, we created a spline
basis as in expression (1), with knots at the sample

0 *

u

quintiles, and included the four extra terms as predic
tors in the linear y-model. The performance of ?Xols
(which we would now call ?n-cov) improved dramati
cally, and the new estimator worked better than any of
the dual-modeling methods reported by Robins et al.
The performance statistics in the both-models-wrong
i
--r

150 200

250

300

350

Predicted value

scenario were Bias = 2.21, Var. = 12.61 and MSE =
17.46 when n = 200, and Bias = 2.40, Var. = 1.88,
and MSE = 7.66 when n = 1000, which compare fa

vorably to the results shown by Robins et al. in their

Fig. 3. Plot analogous to Figure 2, with the roles oft[ and 1 ? i; Table
reversed. Cases with t{ = 0 and t[ = 1 are denoted by black and
gray dots, respectively, with local polynomial (loess) trends shown
for each group. For visual clarity, only 20% of the sampled points

are shown.

diets at the center and overpredicts at the extremes,

and the average value y i = xj ? is zero. But for the
/,- = 1 group, the linear model consistently overpredicts

across the entire range, introducing a strong upward

bias into ?oLS

This alternative simulation by Robins et al. is a clas
sic example where patterns of bias in a linear y-model

cause ?ois to perform poorly. But because the pat
terns are dramatically different when t? = 0 and f,- = 1,

it is also a classic example where the failure can be
readily diagnosed and corrected by fitting a n -model.

2.

6. CONCLUDING REMARKS

As statisticians devise newer and fancier methods,
we hope to find one that is foolproof, yielding good
results no matter when and how it is applied. But the
search for a foolproof method is quixotic and futile.
Some procedures are, on balance, better than others,
but each one requires many subjective inputs, and none
should be applied routinely or uncritically. As we de
velop better estimators, we should also strive to give

potential users a healthy dose of intuition about how
the procedures work, their limitations, sound recom

mendations about their use, and diagnostics that can
help users decide when a procedure is trustworthy and
when it is not.

model is shown in Figure 4. The plot, which is based

In conclusion, we believe that propensity modeling
is prudent and even necessary when rates of missing
information are high. But we are still not convinced
that estimated inverse propensities must always be used

only on (*;, f,-, (1 ? U)y?)9 shows a strong tendency for

as weights.

A plot of the residuals y? ? xj ? for the t? = 0 group
versus the linear predictors from a logistic propensity

ACKNOWLEDGMENT
This research was supported by National Institute on

Drug Abuse Grant P50-DA10075.

REFERENCES
Bang, H. and Robins, J. M. (2005). Doubly robust estimation in
missing data and causal inference models. Biometrics 61 962

972.MR2216189
Elliott, M. R. and Little, R. J. A. (2000). Model-based al

ternatives to trimming survey weights. J. Official Statistics 16

191-209.

linear predictor

FIG. 4. Scatterplot of residuals from a linear y-model fit to t? = 0
cases, versus linear predictors from a logistic it -model, with local
polynomial (loess) fit in one sample ofn = 1000 from the alterna
tive simulation study by Robins et al.

King, G. and ZENG, L. (2002). Improving forecasts of state fail
ure. World Politics 53 623-658.

Little, R. J. A. and An, H. (2004). Robust likelihood-based
analysis of multivariate data with missing values. Statist. Sinica

14 949-968. MR2089342
Luellen, J. K., Shadish, W. R. and Clark, M. H. (2005).

Propensity scores: An introduction and experimental test. Eval

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:24 UTC
All use subject to https://about.jstor.org/terms

580 J. D. Y. KANG AND J. L. SCH?FER
uation Review 29 530-558.

Robins, J. M., Rotnitzky, A. and Zhao, L. P. (1994). Es
timation of regression coefficients when some regressors are

not always observed. J. Amer. Statist. Assoc. 89 846-866.

MR1294730

RUBIN, D. B. (2001). Using propensity scores to help design
observational studies: Applications to the tobacco litigation.
Health Services and Outcomes Research Methodology 2 169

188.

Shah, B. V., Barnwell, B. G. and Biler, G. S. (1997). SU
DAAN User's Manual, Release 7.5. Research Triangle Park, Re
search Triangle Institute, NC.

Tan, Z. (2006). A distributional approach for causal inference us
ing propensity scores. J. Amer. Statist. Assoc. 101 1619-1637.

MR2279484

TSIATIS, A. A. (2006). Semiparametric Theory and Missing Data.

Springer, New York. MR2233926

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:24 UTC
All use subject to https://about.jstor.org/terms

