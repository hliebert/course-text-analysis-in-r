Econometrica Supplementary Material

SUPPLEMENT TO “FIXED-EFFECT REGRESSIONS ON NETWORK DATA”
(Econometrica, Vol. 87, No. 5, September 2019, 1543–1560)
KOEN JOCHMANS
Faculty of Economics, University of Cambridge
MARTIN WEIDNER
Department of Economics, University College London

S.1. ADDITIONAL ILLUSTRATIONS
RECALL THAT OUR MEASURE OF GLOBAL CONNECTIVITY OF THE GRAPH G is λ2 , the second smallest eigenvalue of the normalized Laplacian matrix. In the following discussion,
we provide some concrete examples of graphs for which λ2 can be explicitly calculated,
and we discuss the implications of our variance bound in Theorem 2.
Our first example illustrates that even if λ2 → 0 with the sample size, we may still have
that var(α̂i )  di−1 .
EXAMPLE S.1—Hypercube graph: Consider the N-dimensional hypercube, where each
of n = 2N vertices is involved in N edges; see the left-hand side of Figure S.1. This is an
N-regular graph—that is, di = hi = N for all i—with the total number of edges in the
graph equalling 2N−1 . Here,


2
= O (ln n)−1 
N
Thus, λ2 hi is constant in n. An application of Theorem 2 yields
λ2 =

1 + o(1) ≤

N var(α̂i ) 3
≤ + o(1)
2
σ2

From this, we obtain the convergence rate result (α̂i − αi ) = Op ((ln n)−1/2 ).
Theorem 2 allows us to establish the convergence rate for the hypercube, but the conditions are too stringent to obtain (12). The reason is that hi does not increase fast enough
to ensure that λ2 hi → ∞. The following example deals with an extended hypercube and
illustrates that, despite λ2 → 0, we still have λ2 hi → ∞ in this case.
EXAMPLE S.2—Extended hypercube graph: Start with the N-dimensional hypercube
G from the previous example and add edges between all path-2 neighbors in G ; see the
right-hand side of Figure S.1 for an example. The resulting graph still has n = 2N vertices,
but now has N(N + 1)2N−1 edges. Here
di = h i =

N(N + 1)

2

λ2 =

4

N +1

so that λ2 hi → ∞ holds, despite λ2 → 0 as n → ∞. Theorem 2 therefore implies (12) in
this example.
Koen Jochmans: kj345@cam.ac.uk
Martin Weidner: m.weidner@ucl.ac.uk
© 2019 The Econometric Society

https://doi.org/10.3982/ECTA14605

2

K. JOCHMANS AND M. WEIDNER

FIGURE S.1.—Three-dimensional hypercube (left) and extended hypercube (right).

The next example shows that our bound can still be informative if hi is finite.
EXAMPLE S.3—Star graph: Consider a star graph around the central vertex 1, that is,
the graph with n vertices and edges


E = (1 j) : 2 ≤ j ≤ n ;
see the left-hand side of Figure S.2. Here, λ2 = 1 for any n, while d1 = n − 1, h1 = 1 and
di = 1, hi = n − 1 for i = 1. For i = 1, one finds that the bounds in Theorem 2 imply that
var(α̂1 ) = O(n−1 ), and so


(α̂1 − α1 ) = Op n−1/2 
In contrast, for i = 1, we find λ2 hi → ∞ and thus, although (12) holds, these αi cannot be
estimated consistently as di = 1.
The previous example also illustrates that λ2 can be large despite having many vertices
with small degrees. It is largely due to this property that we prefer to measure global
connectivity by λ2 and not by the “algebraic connectivity” (the second smallest eigenvalue
of L; see, e.g., Chung 1997), which has been studied more extensively.
Our last example shows the effect on the upper bound in Theorem 2 when neighbors
themselves are more strongly connected.

FIGURE S.2.—Star graph (left) and wheel graph (right) for n = 8.

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

3

EXAMPLE S.4—Wheel graph: The wheel graph is obtained by combining a star graph
centered at vertex 1 with a cycle graph on the remaining n − 1 vertices; see the right-hand
side of Figure S.2. Thus, a wheel graph contains strictly more edges than the underlying
star graph, although none of these involves the central vertex directly. From Butler (2016),
we have
 

2
2π
4

λ2 = min  1 − cos
3
3
n
which satisfies λ2 ≥ 1 only for n ≤ 4, and converges to 1/3 at an exponential rate. However, while, as in the star graph, d1 = n − 1, we now have that hi = 3 for all i = 1. Hence,
λ2 h1 > 1 for any finite n and the upper bound in Theorem 2 is strictly smaller than in the
star graph.
The last two examples also illustrate that adding edges to the graph (in this case, to
obtain the wheel graph from the star graph) can result in a decrease of our measure of
global connectivity λ2 . This is not a problem, however, for our results, as we only require
that λ2 be sufficiently different from zero. The wheel graph with λ2 ≥ 1/3, for example,
clearly describes a very well globally connected graph by that measure.
S.2. VARIANCE BOUNDS FOR DIFFERENCES
Our focus in the main text has been inference on the αi , under the constraint in (3),
i di αi = 0. An alternative to normalizing the parameters that may be useful in certain
applications is to focus directly on the differences αi − αj for all i = j. An example where
this is the case is Finkelstein, Gentzkow, and Williams (2016). We give a corresponding
version of Theorem 2 here.
Let dij := k∈V (A)ik (A)jk for an unweighted graph dij = |[i] ∩ [j]|, the number of vertices that are neighbors of both i and j. Write
⎧
 (A)ik (A)jk −1
⎪
⎨ 1
for dij = 0
dij k∈V
dk
hij :=
⎪
⎩
∞
for dij = 0
for the corresponding harmonic mean of the degrees of the vertices k ∈ [i] ∩ [j]. We have
the following theorem.
THEOREM S.1—First-order bound for differences: Let G be connected. Then


2(A)ij
1
1
σ2
+ −
di dj
di dj




2(A)ij
2dij
1
σ2
1
1
1
2
+

+ −
+
−
≤ var(α̂i − α̂j ) ≤ σ
di dj
di dj
λ2 di hi dj hj di dj hij
For a simple graph G , when [i] = [j] but i ∈
/ [j] and i ∈
/ [j], that is, when vertices i and j
share exactly the same neighbors and are not connected themselves, the theorem implies


1
1
2

(S.1)
+
var(α̂i − α̂j ) = σ
di dj

4

K. JOCHMANS AND M. WEIDNER

as, in that case, both (A)ij and the second term in the upper bound in Theorem S.1 are
zero.
S.3. ALTERNATIVE NORMALIZATION
If we change the normalization constraint in the least-squares minimization problem
(4) to
n


αi = 0

i=1

we obtain the estimator α̂ = M ι α̂, where M ι = I n − n−1 ιn ιn is the projector orthogonal
to ιn . We then have var(α̂ ) = σ 2 L† , because this variance needs to satisfy var(α̂ )ιn = 0,
and the Moore–Penrose pseudoinverse guarantees that the null space of L equals the
null space of L† . Thus, changing the normalization corresponds to changing the particular
pseudoinverse of L that features in the expression for the variance. From α̂ = M ι α̂, we
find
 
var α̂ = M ι var(α̂)M ι 
−1
†
which thus also shows that L† = M ι L M ι . We have L ≤ λ−1
2 D , and, therefore, L ≤
−1
†
−1
−1
−1 2
2
λ2 M ι D M ι . We thus find var(α̂i ) = σ ei L ei ≤ λ2 σ ei M ι D M ι ei , and evaluating the
last expression gives the following theorem.

THEOREM S.2—Global bound under alternative normalization: Let G be connected.
Then


  1 σ2
di
1+

var α̂i ≤
di λ2
nh
2

Notice that di /(nh) ≤ 1/ h ≤ 1 and, therefore, var(α̂i ) ≤ d2i σλ . For the estimator α̂i
2
obtained under the normalization in the main text, we immediately find from (6) and
1 σ2
(S† )ii ≤ λ−1
2 that var(α̂i ) ≤ di λ . Thus, for sequences of growing networks, we find the
2

p

p

pointwise consistency results (α̂i − αi ) → 0 and (α̂i − αi ) → 0 for both estimators, under
the sufficient condition λ2 di → ∞.
Analogously one can extend Theorem 2 from α̂i to α̂i as follows.
THEOREM S.3—First-order bound under alternative normalization: Let G be connected. Then






  σ2
σ2
2σ 2
1
2
1
σ2 2
− (2) ≤ var α̂i ≤
+

1−
1+
+
di
n
di
λ2 hi
h n λ2 H
nhi
1
where h(2)
i = ( di

j∈[i]

(A)ij −1
dj

) , and h and H are defined in the main text.

Analogous to (12) in the main text, we thus find
 
  σ2
+ o di−1 
var α̂i =
di

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

5

provided that λ2 hi → ∞ and nh/di → ∞ and nh(2)
i /di → ∞ and λ2 hH/di → ∞ as n →
∞. Therefore, under plausible assumptions on the sequence of growing networks, we find
the same asymptotic properties for α̂i as for α̂i . The particular choice of normalization in
the main text is not necessary for our main results, but it makes all derivations as well as
the presentation of the results more convenient.
S.4. PROOFS
PROOF OF THEOREM 1 (EXISTENCE): The estimator is defined by the constraint minimization problem in (4). For convenience, we express the constraint in quadratic form,
(a d)2 = 0. By introducing the Lagrange multiplier λ > 0, we can write
 2
α̌ = arg min(y − Ba) M X (y − Ba) + λ a d 
a∈Rn

Solving the corresponding first-order condition we obtain

−1
α̌ = B M X B + λdd B M X y

−1
= D−1/2 SX + λψψ D−1/2 B y

(S.2)

where SX := D−1/2 B M X BD−1/2 and ψ := D1/2 ιn = D−1/2 d. Since we assume that the
graph is connected, we have di > 0 for all i, that is, D is invertible. Our assumption
rank((X B)) = p + n − 1 implies that rank(B M X B) = n − 1, that is, the zero eigenvalue of B M X B has multiplicity 1. By construction of B, we have Bιn = 0, that is, the zero
eigenvector of B M X B is given by ιn . It follows that the zero eigenvalue SX has multiplicity 1 and eigenvector ψ. This explains why the matrix SX + λψψ is invertible, which we
already used in (S.2). Furthermore, the matrices SX and ψψ commute, and by properties
of the Moore–Penrose inverse, we thus have
−1

†

SX + λψψ
= S†X + λ−1 ψψ 
(S.3)
We furthermore have


ψψ

†

= m−2 ψψ 

(S.4)

where m = ψ ψ is the total number of observations. Because Bιn = 0, the contribution
from (ψψ )† drops out of (S.2), and we obtain


α̌ = D−1/2 S†X D−1/2 B y = B M X B B y
according to the definition of the pseudoinverse  in the main text. Notice that α̌ given in
the last display does not depend on λ, and automatically satisfies the constraint d α̌ = 0,
that is, any value of λ can be chosen in the above derivation.
Q.E.D.
PROOF OF THEOREMS 2 AND S.1 (VARIANCE BOUNDS): We first show that if G is connected, then

 σ 2 −1

D AD−1 AD−1 
0 ≤ var(α̂) − σ 2 D−1 + D−1 AD−1 − 2m−1 ιn ιn ≤
λ2

(S.5)

6

K. JOCHMANS AND M. WEIDNER

Theorems 2 and S.1 will then follow readily. Analogous to (S.3), we also have (S +
λψψ )−1 = S† + λ−1 (ψψ )† . Using this and (S.4), we find

−1 

I n = S + λψψ
S + λψψ



= S† + λ−1 m−2 ψψ S + λψψ 
and since Sψ = 0 and ψ ψ = m, we thus find that S† S = I n − m−1 ψψ , which is simply the
idempotent matrix that projects orthogonally to ψ. We thus find L L = D−1/2 S† SD1/2 =
I n − m−1 ιn d . Plugging in L = D − A and then solving for L gives
L = D−1 + L AD−1 − m−1 ιn ιn 

(S.6)

The Laplacian is symmetric, and so transposition gives
L = D−1 + D−1 AL − m−1 ιn ιn 

(S.7)

Replacing L on the right-hand side of (S.6) by the expression for L given by (S.7), and
also using that D−1 Aιn = ιn yields
L = D−1 + D−1 AD−1 + D−1 AL AD−1 − 2m−1 ιn ιn 

(S.8)

Rearranging this equation allows us to write


L − D−1 + D−1 AD−1 − 2m−1 ιn ιn = D−1 AL AD−1 

−1 −1
From L∗ = D−1/2 S† D−1/2 and 0 ≤ S† ≤ λ−1
2 I n , we obtain 0 ≤ L ≤ λ2 D , and therefore,
−1
−1
−1
0 ≤ D−1 AL AD−1 ≤ λ−1
2 D AD AD 

Put together this yields


−1
−1
−1
0 ≤ L − D−1 + D−1 AD−1 − 2m−1 ιn ιn ≤ λ−1
2 D AD AD 
and multiplication with σ 2 gives the bounds stated in (S.5).
To show Theorems 2 and S.1, we calculate, for i = j,
ei D−1 ei = di−1 
ei D−1 ej = 0
ei D−1 AD−1 ei = 0

ei D−1 AD−1 AD−1 ei = di−1 h−1
i 
ei D−1 AD−1 AD−1 ej = di−1 dj−1 dij h−1
ij 
ei ιn ιn ei = 1

ei D−1 AD−1 ej = di−1 dj−1 (A)ij 

ei ιn ιn ej = 1

where ei is the vector that has 1 as its ith entry and 0s elsewhere. Combining these results
with (S.5) gives the bounds on, respectively, var(α̂i ) = ei var(α̂)ei and var(α̂i − α̂j ) = (ei −
Q.E.D.
ej ) var(α̂)(ei − ej ) stated in the theorems.
−1
PROOF OF THEOREMS S.2 AND S.3: Using that L∗ ≤ λ−1
we find that
2 D
 
 
var α̂i = ei var α̂ ei = ei M ι var(α̂)M ι ei = σ 2 ei M ι L∗ M ι ei
−1
2
≤ λ−1
2 σ ei M ι D M ι ei 

7

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

and we calculate
2
1
ei M ι D−1 M ι ei = ei D−1 ei − ei D−1 ιn + 2 ιn D−1 ιn
n
n
1
2
1

= −
+
di ndi nh

(S.9)

Combing those results gives the statement of Theorem S.2
Next, multiplying M ι from the left and right to the matrix bounds (S.5) and using
var(α̂ ) = M ι var(α̂)M ι gives

  σ2
  
M ι D−1 AD−1 AD−1 M ι 
0 ≤ var α̂ − σ 2 M ι D−1 + D−1 AD−1 M ι ≤
λ2
and, therefore,


 σ2
  
e M ι D−1 AD−1 AD−1 M ι ei 
0 ≤ var α̂i − σ 2 ei M ι D−1 + D−1 AD−1 M ι ei ≤
λ2 i
We already calculated ei M ι D−1 M ι ei in (S.9) above. We furthermore have
2
1
ei M ι D−1 AD−1 M ι ei = ei D−1 AD−1 ei − ei D−1 AD−1 ιn + 2 ιn D−1 AD−1 ιn
n
n
n
2  (A)ij
1  (A)jk
=0−
+ 2

ndi j∈[i] dj
n jk=1 dj dk
(A)jk
jk dj dk

and by applying the Cauchy–Schwarz inequality, we find

≤

(A)jk
jk

dj2

=

1
j dj

,

and, therefore,
−

2
nh

(2)
i

≤ ei M ι D−1 AD−1 M ι ei ≤

1

nh

Similarly, ei M ι D−1 AD−1 AD−1 M ι ei ≥ 0 contains three terms, for which we have
ei D−1 AD−1 AD−1 ei =

1

d i hi

2
2  (A)ij  (A)jk
− ei D−1 AD−1 AD−1 ιn = −
≤ 0
n
ndi j∈[i] dj k∈[j] dk
1
1  (A)ij (A)jk
1  (A)ij
1 1
1
−1
−1
−1

ι
D
AD
AD
ι
=
≤
=
=
n
n
2
2
2
2
n i di hi hH
n
n ijk di dj dk
n ijk di dj
2

where, in the last line, we again applied the Cauchy–Schwarz inequality, and the definitions of the harmonic means h and H in the main text. Combining the above gives the
statement of Theorem S.3.
Q.E.D.

8

K. JOCHMANS AND M. WEIDNER

PROOF OF THEOREM 3 (COVARIATES): Define the n × n matrix


−1

C := B B B X X X X B
Let λi (C) denote the ith eigenvalue of C, arranged in ascending order. The matrix C is
similar to the positive semidefinite matrix


XX

−1/2




−1/2
XB BB BX XX


and since similar matrices share the same eigenvalues, we have λ1 (C) ≥ 0. The matrix C
is also similar to the matrix


−1

B BB BX XX X
which is the product of two projection matrices, whose spectral norm is thus bounded
by 1. Hence, λn (C) ≤ 1. In addition, we must have λi (C) = 1 for any 1 < i < n because,
otherwise, rank(I n − C) < n, which implies that rank(B M X B) < n − 1, contradicting our
non-collinearity assumption (since the graph is connected, we have rank(B B) = n − 1,
which together with the non-collinearity assumption rank((X B)) = p+n−1 implies that
rank(B M X B) = n − 1). We, therefore, have C 2 < 1, implying that I m − C is invertible.
Using (S.3) and (S.4) with λ = m−1 , we find that (B M X B + m−1 Dιn ιn D)−1 =
(B M X B) + m−1 ιn ιn or, equivalently,
B M X B + m−1 Dιn ιn D =




−1
B M X B + m−1 ιn ιn 

and analogously we have
B B + m−1 Dιn ιn D =




−1
B B + m−1 ιn ιn 

(S.10)

Subtracting the expressions in the last two displays gives

−1



−1 
−1
B X X X X B = B B + m−1 ιn ιn − B M X B + m−1 ιn ιn 
and by multiplying with [(B B) + m−1 ιn ιn ] from the left and [(B M X B) + m−1 ιn ιn ] from
the right, and using Bιn = 0, we obtain


−1

 
 


B B B X X X X B B MXB = B MXB − B B 
which can equivalently be expressed as (I m − C)(B M X B) = (B B) . We have already
argued that (I m − C) is invertible and, therefore,




B M X B = (I m − C)−1 B B 
Since C

2

< 1, we can expand (I m − C)−1 in powers of C, as
∞

 

B MXB =
Cr B B 



r=0

Defining the p × p matrix
−1/2



−1/2


XB BB BX XX

C := X X

(S.11)

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

9

we can rewrite (S.11) as


∞

 
 


−1/2 



r 
 X X −1/2 X B B B  
B MXB = B B + B B B X X X
C
r=0

The parameter ρ defined in the main text satisfies

−1/2

−1/2 
 = Ip − 
ρ= XX
X M BX X X
C
2

2

 2
=1− C

 2 = 1 − ρ, and since 
C is symmetric and semidefinite, this can equivathat is, we have C

lently be written as C ≤ (1 − ρ)I p . Therefore,
∞


r

C ≤

r=0

∞

(1 − ρ)r I p = ρ−1 I p 
r=0

We thus have

∞

 
 


−1/2 




r 
 X X −1/2 X B B B 
C
B MXB − B B = B B B X X X
r=0

≤



−1


1
BB BX XX XB BB
ρ

(S.12)

and, therefore,
 
 

var(α̌i ) − var(α̂i ) = σ 2 ei B M X B − B B ei
≤



−1

 
σ 2 
ei B B B X X X X B B B ei 
ρ

Using the expressions (S.6) and (S.7) for (B B) = L , we obtain


−1



ei B B B X X X X B B B ei

−1
= ei L B X X X X BL ei




−1

= ei D−1 + D−1 AL B X X X X B D−1 + L AD−1 ei

≤ Ti(1) + Ti(2) + 2 Ti(1) Ti(2) 
where

−1
Ti(1) := ei D−1 B X X X X BD−1 ei 

−1
Ti(2) := ei D−1 AL B X X X X BL AD−1 ei 
and we used the Cauchy–Schwarz inequality to bound the mixed term. Again, because
similar matrices have the same eigenvalues, we have
  1/2

−1
 1/2 
 L
 2 =1−ρ
B X X X X B L  2 = C

10

K. JOCHMANS AND M. WEIDNER

and, therefore,
 1/2   1/2

−1
 1/2   1/2
Ti(2) = ei D−1 A L
B X X X X B L
AD−1 ei
L
L
≤ (1 − ρ)ei D−1 AL AD−1 ei
≤

1−ρ
e D−1 AD−1 AD−1 ei
λ2 i

=

1−ρ

λ2 di hi

where, in the last step, we used ei D−1 AD−1 AD−1 ei = (di hi )−1 . Using our definitions xi =
X BD−1 ei and Ω = X X/m, we obtain

−1
1
Ti(1) = ei D−1 B X X X X BD−1 ei = xi Ω−1 xi 
m
Combining the above results, we find


σ 2  (1)
Ti + Ti(2) + 2 Ti(1) Ti(2)
ρ



1
1−ρ
1
−
ρ
σ2 1
−1
−1

x Ω xi +
+2
x Ω xi
≤
ρ m i
λ2 di hi
m i
λ2 di hi

var(α̌i ) − var(α̂i ) ≤

√
For any a b ≥ 0, we have a + b + 2 ab ≤ 2(a + b). Thus, a slightly cruder but simpler
bound is given by

−1
2


var(α̌i ) − var(α̂i ) ≤ 2σ xi Ω xi + 1 − ρ 
ρ
m
λ2 di hi
where we also used that var(α̌i ) ≥ var(α̂i ), because adding regressors can only increase
the variance of the least-squares estimator under homoskedasticity.
Q.E.D.
PROOF OF THEOREM 4 (FIRST-ORDER REPRESENTATION): Remember that we treat B
and X as fixed (i.e., nonrandom) throughout. Let β̌ := (X M B X)−1 X M B y. Using the
model for y, we find β̌ − β = (X M B X)−1 X M B u. Using our assumptions E(u) = 0 and
Σ ≤ I m σ 2 , we find E(β̌ − β) = 0 and

−1

−1
 
E (β̌ − β)(β̌ − β) = X M B X X M B ΣM B X X M B X
−1

−1

≤ σ 2 X M BX X M BI mM BX X M BX
−1

= σ 2 X M BX 

(S.13)

The result in (S.10) can be rewritten as

−1
− m−1 ιn ιn 
L = L + m−1 dd

(S.14)

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

11

The constrained least-squares estimator in (4) can be expressed as
α̌ = arg min

a∈{a∈Rn :d a=0}

y − X β̌ − Ba 2 

(S.15)

and analogous to Theorem 1, we then find α̌ = L B (y − X β̌) = (L + m−1 dd )−1 B (y −
X β̌). Multiplying by (L + m−1 dd ) from the left and using our normalization d α̌ = 0
gives
Lα̌ = B (y − X β̌)
Plugging L = D − A and y = Bα + Xβ + u into the last display, multiplying from the left
with D−1 , and rearranging terms, we obtain
α̌ − α = D−1 B u +  + 
˜

(S.16)

where
 := D−1 A(α̌ − α)

˜ := −D−1 B X(β̌ − β)

We have E(β̌ − β) = 0 and E(α̌ − α) = 0, and, therefore, also E() = 0 and E()
˜ = 0.
The definition ρ = (X X)−1 X M B X 2 can equivalently be written as ρX X ≥ X M B X,
and, therefore, ρ−1 (X X)−1 ≤ (X M B X)−1 . Using this and (S.13), we obtain

−1
 
E ˜ ˜ ≤ σ 2 D−1 B X X M B X X BD−1
≤


−1
σ 2 −1
D B X X X X BD−1 
ρ

Using α̌ − α = (B M X B) B M X u and the assumption Σ ≤ σ 2 I n , we calculate




 
E  = D−1 A B M X B B M X ΣM X B B M X B AD−1




≤ σ 2 D−1 A B M X B B M X B B M X B AD−1


= σ 2 D−1 A B M X B AD−1





−1


σ2
≤ σ 2 D−1 A B B AD−1 + D−1 A B B B X X X X B B B AD−1 
ρ
where, in the last step, we used (S.12). Since furthermore X(X X)−1 X ≤ I m and (B B) =
−1
L ≤ λ−1
2 D , we obtain






 
σ2
E  ≤ σ 2 D−1 A B B AD−1 + D−1 A B B B B B B AD−1
ρ
=


σ 2 (1 + ρ) −1 
D A B B AD−1
ρ

≤

σ 2 (1 + ρ) −1
D AD−1 AD−1 
λ2 ρ

12

K. JOCHMANS AND M. WEIDNER

Denote the elements of  and ˜ by

i

and ˜ i . Equation (S.16) can then be written as

α̌i − αi =

bi u
+
di

i

+ ˜i

and we have
E

 2
i

≤

σ 2 (1 + ρ)
σ 2 (1 + ρ) 1
ei D−1 AD−1 AD−1 ei =
λ2 ρ
λ2 ρ
d i hi

and
  σ2

−1
1 σ2
ei D−1 B X X X X BD−1 ei =
E ˜ 2i ≤
x Ω−1 xi 
ρ
m ρ i
where we used our definitions xi = X bi /di = X BD−1 ei and Ω := X X/m.

Q.E.D.

PROOF OF THEOREM 5 (ASYMPTOTIC DISTRIBUTION): We have ρ ≤ 1 by definition.
Together with the assumptions σ 2 = O(1), λ2 hi → ∞, and the conditions in (13), this implies that E( 2i ) ≤ σ 2 (1 + ρ)/(ρdi λ2 hi ) = o(di−1 ) and E(˜ 2i ) ≤ σ 2 xi Ω−1 xi /(ρm) = o(di−1 ).
By Markov’s inequality, we thus have i = op (di−1/2 ) and ˜ i = op (di−1/2 ), and applying Theorem 4 gives, as di → ∞,
p

(α̌i − αi ) →

bi u
1  
=
νεe i 
di
di j∈[i] e∈E

νεe i := (B)εe i uεe 

(ij)

The number of terms νεe i summed over in the last display grows to infinity asymptotically, because we assume that di = j∈[i] e∈E(ij) we → ∞, while the weights we = (B)2εe i
are bounded. Our assumptions furthermore guarantee that the νεe i are independent and
satisfy E(νεe i ) = 0, E(νε2e i ) ≥ c1 > 0 and E(|νεe i |3 ) ≤ c2 < ∞ for constants c1 and c2 . Thus,
the Lyapunov condition is satisfied, and the statement of the theorem then follows from
a standard application of Lyapunov’s central limit theorem.
Q.E.D.
REFERENCES
BUTLER, S. (2016): “Algebraic Aspects of the Normalized Laplacian,” in Recent Trends in Combinatorics, ed.
by A. Beveridge, J. R. Griggs, L. Hogben, G. Musiker, and P. Tetali. Springer, 295–315. [3]
CHUNG, F. R. K. (1997): Spectral Graph Theory. CBMS Regional Conference Series in Mathematics, Vol. 92.
American Mathematical Society. [2]
FINKELSTEIN, A., M. GENTZKOW, AND H. WILLIAMS (2016): “Sources of Geographic Variation in Health
Care: Evidence From Patient Migration,” Quarterly Journal of Economics, 131, 1681–1726. [3]

Co-editor Ulrich K. Müller handled this manuscript.
Manuscript received 4 August, 2016; final version accepted 3 April, 2019; available online 10 April, 2019.

