Econometrica Supplementary Material

SUPPLEMENT TO â€œFIXED-EFFECT REGRESSIONS ON NETWORK DATAâ€
(Econometrica, Vol. 87, No. 5, September 2019, 1543â€“1560)
KOEN JOCHMANS
Faculty of Economics, University of Cambridge
MARTIN WEIDNER
Department of Economics, University College London

S.1. ADDITIONAL ILLUSTRATIONS
RECALL THAT OUR MEASURE OF GLOBAL CONNECTIVITY OF THE GRAPH G is Î»2 , the second smallest eigenvalue of the normalized Laplacian matrix. In the following discussion,
we provide some concrete examples of graphs for which Î»2 can be explicitly calculated,
and we discuss the implications of our variance bound in Theorem 2.
Our first example illustrates that even if Î»2 â†’ 0 with the sample size, we may still have
that var(Î±Ì‚i )  diâˆ’1 .
EXAMPLE S.1â€”Hypercube graph: Consider the N-dimensional hypercube, where each
of n = 2N vertices is involved in N edges; see the left-hand side of Figure S.1. This is an
N-regular graphâ€”that is, di = hi = N for all iâ€”with the total number of edges in the
graph equalling 2Nâˆ’1 . Here,


2
= O (ln n)âˆ’1 
N
Thus, Î»2 hi is constant in n. An application of Theorem 2 yields
Î»2 =

1 + o(1) â‰¤

N var(Î±Ì‚i ) 3
â‰¤ + o(1)
2
Ïƒ2

From this, we obtain the convergence rate result (Î±Ì‚i âˆ’ Î±i ) = Op ((ln n)âˆ’1/2 ).
Theorem 2 allows us to establish the convergence rate for the hypercube, but the conditions are too stringent to obtain (12). The reason is that hi does not increase fast enough
to ensure that Î»2 hi â†’ âˆž. The following example deals with an extended hypercube and
illustrates that, despite Î»2 â†’ 0, we still have Î»2 hi â†’ âˆž in this case.
EXAMPLE S.2â€”Extended hypercube graph: Start with the N-dimensional hypercube
G from the previous example and add edges between all path-2 neighbors in G ; see the
right-hand side of Figure S.1 for an example. The resulting graph still has n = 2N vertices,
but now has N(N + 1)2Nâˆ’1 edges. Here
di = h i =

N(N + 1)

2

Î»2 =

4

N +1

so that Î»2 hi â†’ âˆž holds, despite Î»2 â†’ 0 as n â†’ âˆž. Theorem 2 therefore implies (12) in
this example.
Koen Jochmans: kj345@cam.ac.uk
Martin Weidner: m.weidner@ucl.ac.uk
Â© 2019 The Econometric Society

https://doi.org/10.3982/ECTA14605

2

K. JOCHMANS AND M. WEIDNER

FIGURE S.1.â€”Three-dimensional hypercube (left) and extended hypercube (right).

The next example shows that our bound can still be informative if hi is finite.
EXAMPLE S.3â€”Star graph: Consider a star graph around the central vertex 1, that is,
the graph with n vertices and edges


E = (1 j) : 2 â‰¤ j â‰¤ n ;
see the left-hand side of Figure S.2. Here, Î»2 = 1 for any n, while d1 = n âˆ’ 1, h1 = 1 and
di = 1, hi = n âˆ’ 1 for i = 1. For i = 1, one finds that the bounds in Theorem 2 imply that
var(Î±Ì‚1 ) = O(nâˆ’1 ), and so


(Î±Ì‚1 âˆ’ Î±1 ) = Op nâˆ’1/2 
In contrast, for i = 1, we find Î»2 hi â†’ âˆž and thus, although (12) holds, these Î±i cannot be
estimated consistently as di = 1.
The previous example also illustrates that Î»2 can be large despite having many vertices
with small degrees. It is largely due to this property that we prefer to measure global
connectivity by Î»2 and not by the â€œalgebraic connectivityâ€ (the second smallest eigenvalue
of L; see, e.g., Chung 1997), which has been studied more extensively.
Our last example shows the effect on the upper bound in Theorem 2 when neighbors
themselves are more strongly connected.

FIGURE S.2.â€”Star graph (left) and wheel graph (right) for n = 8.

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

3

EXAMPLE S.4â€”Wheel graph: The wheel graph is obtained by combining a star graph
centered at vertex 1 with a cycle graph on the remaining n âˆ’ 1 vertices; see the right-hand
side of Figure S.2. Thus, a wheel graph contains strictly more edges than the underlying
star graph, although none of these involves the central vertex directly. From Butler (2016),
we have
 

2
2Ï€
4

Î»2 = min  1 âˆ’ cos
3
3
n
which satisfies Î»2 â‰¥ 1 only for n â‰¤ 4, and converges to 1/3 at an exponential rate. However, while, as in the star graph, d1 = n âˆ’ 1, we now have that hi = 3 for all i = 1. Hence,
Î»2 h1 > 1 for any finite n and the upper bound in Theorem 2 is strictly smaller than in the
star graph.
The last two examples also illustrate that adding edges to the graph (in this case, to
obtain the wheel graph from the star graph) can result in a decrease of our measure of
global connectivity Î»2 . This is not a problem, however, for our results, as we only require
that Î»2 be sufficiently different from zero. The wheel graph with Î»2 â‰¥ 1/3, for example,
clearly describes a very well globally connected graph by that measure.
S.2. VARIANCE BOUNDS FOR DIFFERENCES
Our focus in the main text has been inference on the Î±i , under the constraint in (3),
i di Î±i = 0. An alternative to normalizing the parameters that may be useful in certain
applications is to focus directly on the differences Î±i âˆ’ Î±j for all i = j. An example where
this is the case is Finkelstein, Gentzkow, and Williams (2016). We give a corresponding
version of Theorem 2 here.
Let dij := kâˆˆV (A)ik (A)jk for an unweighted graph dij = |[i] âˆ© [j]|, the number of vertices that are neighbors of both i and j. Write
âŽ§
 (A)ik (A)jk âˆ’1
âŽª
âŽ¨ 1
for dij = 0
dij kâˆˆV
dk
hij :=
âŽª
âŽ©
âˆž
for dij = 0
for the corresponding harmonic mean of the degrees of the vertices k âˆˆ [i] âˆ© [j]. We have
the following theorem.
THEOREM S.1â€”First-order bound for differences: Let G be connected. Then


2(A)ij
1
1
Ïƒ2
+ âˆ’
di dj
di dj




2(A)ij
2dij
1
Ïƒ2
1
1
1
2
+

+ âˆ’
+
âˆ’
â‰¤ var(Î±Ì‚i âˆ’ Î±Ì‚j ) â‰¤ Ïƒ
di dj
di dj
Î»2 di hi dj hj di dj hij
For a simple graph G , when [i] = [j] but i âˆˆ
/ [j] and i âˆˆ
/ [j], that is, when vertices i and j
share exactly the same neighbors and are not connected themselves, the theorem implies


1
1
2

(S.1)
+
var(Î±Ì‚i âˆ’ Î±Ì‚j ) = Ïƒ
di dj

4

K. JOCHMANS AND M. WEIDNER

as, in that case, both (A)ij and the second term in the upper bound in Theorem S.1 are
zero.
S.3. ALTERNATIVE NORMALIZATION
If we change the normalization constraint in the least-squares minimization problem
(4) to
n


Î±i = 0

i=1

we obtain the estimator Î±Ì‚ = M Î¹ Î±Ì‚, where M Î¹ = I n âˆ’ nâˆ’1 Î¹n Î¹n is the projector orthogonal
to Î¹n . We then have var(Î±Ì‚ ) = Ïƒ 2 Lâ€  , because this variance needs to satisfy var(Î±Ì‚ )Î¹n = 0,
and the Mooreâ€“Penrose pseudoinverse guarantees that the null space of L equals the
null space of Lâ€  . Thus, changing the normalization corresponds to changing the particular
pseudoinverse of L that features in the expression for the variance. From Î±Ì‚ = M Î¹ Î±Ì‚, we
find
 
var Î±Ì‚ = M Î¹ var(Î±Ì‚)M Î¹ 
âˆ’1
â€ 
which thus also shows that Lâ€  = M Î¹ L M Î¹ . We have L â‰¤ Î»âˆ’1
2 D , and, therefore, L â‰¤
âˆ’1
â€ 
âˆ’1
âˆ’1
âˆ’1 2
2
Î»2 M Î¹ D M Î¹ . We thus find var(Î±Ì‚i ) = Ïƒ ei L ei â‰¤ Î»2 Ïƒ ei M Î¹ D M Î¹ ei , and evaluating the
last expression gives the following theorem.

THEOREM S.2â€”Global bound under alternative normalization: Let G be connected.
Then


  1 Ïƒ2
di
1+

var Î±Ì‚i â‰¤
di Î»2
nh
2

Notice that di /(nh) â‰¤ 1/ h â‰¤ 1 and, therefore, var(Î±Ì‚i ) â‰¤ d2i ÏƒÎ» . For the estimator Î±Ì‚i
2
obtained under the normalization in the main text, we immediately find from (6) and
1 Ïƒ2
(Sâ€  )ii â‰¤ Î»âˆ’1
2 that var(Î±Ì‚i ) â‰¤ di Î» . Thus, for sequences of growing networks, we find the
2

p

p

pointwise consistency results (Î±Ì‚i âˆ’ Î±i ) â†’ 0 and (Î±Ì‚i âˆ’ Î±i ) â†’ 0 for both estimators, under
the sufficient condition Î»2 di â†’ âˆž.
Analogously one can extend Theorem 2 from Î±Ì‚i to Î±Ì‚i as follows.
THEOREM S.3â€”First-order bound under alternative normalization: Let G be connected. Then






  Ïƒ2
Ïƒ2
2Ïƒ 2
1
2
1
Ïƒ2 2
âˆ’ (2) â‰¤ var Î±Ì‚i â‰¤
+

1âˆ’
1+
+
di
n
di
Î»2 hi
h n Î»2 H
nhi
1
where h(2)
i = ( di

jâˆˆ[i]

(A)ij âˆ’1
dj

) , and h and H are defined in the main text.

Analogous to (12) in the main text, we thus find
 
  Ïƒ2
+ o diâˆ’1 
var Î±Ì‚i =
di

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

5

provided that Î»2 hi â†’ âˆž and nh/di â†’ âˆž and nh(2)
i /di â†’ âˆž and Î»2 hH/di â†’ âˆž as n â†’
âˆž. Therefore, under plausible assumptions on the sequence of growing networks, we find
the same asymptotic properties for Î±Ì‚i as for Î±Ì‚i . The particular choice of normalization in
the main text is not necessary for our main results, but it makes all derivations as well as
the presentation of the results more convenient.
S.4. PROOFS
PROOF OF THEOREM 1 (EXISTENCE): The estimator is defined by the constraint minimization problem in (4). For convenience, we express the constraint in quadratic form,
(a d)2 = 0. By introducing the Lagrange multiplier Î» > 0, we can write
 2
Î±ÌŒ = arg min(y âˆ’ Ba) M X (y âˆ’ Ba) + Î» a d 
aâˆˆRn

Solving the corresponding first-order condition we obtain

âˆ’1
Î±ÌŒ = B M X B + Î»dd B M X y

âˆ’1
= Dâˆ’1/2 SX + Î»ÏˆÏˆ Dâˆ’1/2 B y

(S.2)

where SX := Dâˆ’1/2 B M X BDâˆ’1/2 and Ïˆ := D1/2 Î¹n = Dâˆ’1/2 d. Since we assume that the
graph is connected, we have di > 0 for all i, that is, D is invertible. Our assumption
rank((X B)) = p + n âˆ’ 1 implies that rank(B M X B) = n âˆ’ 1, that is, the zero eigenvalue of B M X B has multiplicity 1. By construction of B, we have BÎ¹n = 0, that is, the zero
eigenvector of B M X B is given by Î¹n . It follows that the zero eigenvalue SX has multiplicity 1 and eigenvector Ïˆ. This explains why the matrix SX + Î»ÏˆÏˆ is invertible, which we
already used in (S.2). Furthermore, the matrices SX and ÏˆÏˆ commute, and by properties
of the Mooreâ€“Penrose inverse, we thus have
âˆ’1

â€ 

SX + Î»ÏˆÏˆ
= Sâ€ X + Î»âˆ’1 ÏˆÏˆ 
(S.3)
We furthermore have


ÏˆÏˆ

â€ 

= mâˆ’2 ÏˆÏˆ 

(S.4)

where m = Ïˆ Ïˆ is the total number of observations. Because BÎ¹n = 0, the contribution
from (ÏˆÏˆ )â€  drops out of (S.2), and we obtain


Î±ÌŒ = Dâˆ’1/2 Sâ€ X Dâˆ’1/2 B y = B M X B B y
according to the definition of the pseudoinverse  in the main text. Notice that Î±ÌŒ given in
the last display does not depend on Î», and automatically satisfies the constraint d Î±ÌŒ = 0,
that is, any value of Î» can be chosen in the above derivation.
Q.E.D.
PROOF OF THEOREMS 2 AND S.1 (VARIANCE BOUNDS): We first show that if G is connected, then

 Ïƒ 2 âˆ’1

D ADâˆ’1 ADâˆ’1 
0 â‰¤ var(Î±Ì‚) âˆ’ Ïƒ 2 Dâˆ’1 + Dâˆ’1 ADâˆ’1 âˆ’ 2mâˆ’1 Î¹n Î¹n â‰¤
Î»2

(S.5)

6

K. JOCHMANS AND M. WEIDNER

Theorems 2 and S.1 will then follow readily. Analogous to (S.3), we also have (S +
Î»ÏˆÏˆ )âˆ’1 = Sâ€  + Î»âˆ’1 (ÏˆÏˆ )â€  . Using this and (S.4), we find

âˆ’1 

I n = S + Î»ÏˆÏˆ
S + Î»ÏˆÏˆ



= Sâ€  + Î»âˆ’1 mâˆ’2 ÏˆÏˆ S + Î»ÏˆÏˆ 
and since SÏˆ = 0 and Ïˆ Ïˆ = m, we thus find that Sâ€  S = I n âˆ’ mâˆ’1 ÏˆÏˆ , which is simply the
idempotent matrix that projects orthogonally to Ïˆ. We thus find L L = Dâˆ’1/2 Sâ€  SD1/2 =
I n âˆ’ mâˆ’1 Î¹n d . Plugging in L = D âˆ’ A and then solving for L gives
L = Dâˆ’1 + L ADâˆ’1 âˆ’ mâˆ’1 Î¹n Î¹n 

(S.6)

The Laplacian is symmetric, and so transposition gives
L = Dâˆ’1 + Dâˆ’1 AL âˆ’ mâˆ’1 Î¹n Î¹n 

(S.7)

Replacing L on the right-hand side of (S.6) by the expression for L given by (S.7), and
also using that Dâˆ’1 AÎ¹n = Î¹n yields
L = Dâˆ’1 + Dâˆ’1 ADâˆ’1 + Dâˆ’1 AL ADâˆ’1 âˆ’ 2mâˆ’1 Î¹n Î¹n 

(S.8)

Rearranging this equation allows us to write


L âˆ’ Dâˆ’1 + Dâˆ’1 ADâˆ’1 âˆ’ 2mâˆ’1 Î¹n Î¹n = Dâˆ’1 AL ADâˆ’1 

âˆ’1 âˆ’1
From Lâˆ— = Dâˆ’1/2 Sâ€  Dâˆ’1/2 and 0 â‰¤ Sâ€  â‰¤ Î»âˆ’1
2 I n , we obtain 0 â‰¤ L â‰¤ Î»2 D , and therefore,
âˆ’1
âˆ’1
âˆ’1
0 â‰¤ Dâˆ’1 AL ADâˆ’1 â‰¤ Î»âˆ’1
2 D AD AD 

Put together this yields


âˆ’1
âˆ’1
âˆ’1
0 â‰¤ L âˆ’ Dâˆ’1 + Dâˆ’1 ADâˆ’1 âˆ’ 2mâˆ’1 Î¹n Î¹n â‰¤ Î»âˆ’1
2 D AD AD 
and multiplication with Ïƒ 2 gives the bounds stated in (S.5).
To show Theorems 2 and S.1, we calculate, for i = j,
ei Dâˆ’1 ei = diâˆ’1 
ei Dâˆ’1 ej = 0
ei Dâˆ’1 ADâˆ’1 ei = 0

ei Dâˆ’1 ADâˆ’1 ADâˆ’1 ei = diâˆ’1 hâˆ’1
i 
ei Dâˆ’1 ADâˆ’1 ADâˆ’1 ej = diâˆ’1 djâˆ’1 dij hâˆ’1
ij 
ei Î¹n Î¹n ei = 1

ei Dâˆ’1 ADâˆ’1 ej = diâˆ’1 djâˆ’1 (A)ij 

ei Î¹n Î¹n ej = 1

where ei is the vector that has 1 as its ith entry and 0s elsewhere. Combining these results
with (S.5) gives the bounds on, respectively, var(Î±Ì‚i ) = ei var(Î±Ì‚)ei and var(Î±Ì‚i âˆ’ Î±Ì‚j ) = (ei âˆ’
Q.E.D.
ej ) var(Î±Ì‚)(ei âˆ’ ej ) stated in the theorems.
âˆ’1
PROOF OF THEOREMS S.2 AND S.3: Using that Lâˆ— â‰¤ Î»âˆ’1
we find that
2 D
 
 
var Î±Ì‚i = ei var Î±Ì‚ ei = ei M Î¹ var(Î±Ì‚)M Î¹ ei = Ïƒ 2 ei M Î¹ Lâˆ— M Î¹ ei
âˆ’1
2
â‰¤ Î»âˆ’1
2 Ïƒ ei M Î¹ D M Î¹ ei 

7

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

and we calculate
2
1
ei M Î¹ Dâˆ’1 M Î¹ ei = ei Dâˆ’1 ei âˆ’ ei Dâˆ’1 Î¹n + 2 Î¹n Dâˆ’1 Î¹n
n
n
1
2
1

= âˆ’
+
di ndi nh

(S.9)

Combing those results gives the statement of Theorem S.2
Next, multiplying M Î¹ from the left and right to the matrix bounds (S.5) and using
var(Î±Ì‚ ) = M Î¹ var(Î±Ì‚)M Î¹ gives

  Ïƒ2
  
M Î¹ Dâˆ’1 ADâˆ’1 ADâˆ’1 M Î¹ 
0 â‰¤ var Î±Ì‚ âˆ’ Ïƒ 2 M Î¹ Dâˆ’1 + Dâˆ’1 ADâˆ’1 M Î¹ â‰¤
Î»2
and, therefore,


 Ïƒ2
  
e M Î¹ Dâˆ’1 ADâˆ’1 ADâˆ’1 M Î¹ ei 
0 â‰¤ var Î±Ì‚i âˆ’ Ïƒ 2 ei M Î¹ Dâˆ’1 + Dâˆ’1 ADâˆ’1 M Î¹ ei â‰¤
Î»2 i
We already calculated ei M Î¹ Dâˆ’1 M Î¹ ei in (S.9) above. We furthermore have
2
1
ei M Î¹ Dâˆ’1 ADâˆ’1 M Î¹ ei = ei Dâˆ’1 ADâˆ’1 ei âˆ’ ei Dâˆ’1 ADâˆ’1 Î¹n + 2 Î¹n Dâˆ’1 ADâˆ’1 Î¹n
n
n
n
2  (A)ij
1  (A)jk
=0âˆ’
+ 2

ndi jâˆˆ[i] dj
n jk=1 dj dk
(A)jk
jk dj dk

and by applying the Cauchyâ€“Schwarz inequality, we find

â‰¤

(A)jk
jk

dj2

=

1
j dj

,

and, therefore,
âˆ’

2
nh

(2)
i

â‰¤ ei M Î¹ Dâˆ’1 ADâˆ’1 M Î¹ ei â‰¤

1

nh

Similarly, ei M Î¹ Dâˆ’1 ADâˆ’1 ADâˆ’1 M Î¹ ei â‰¥ 0 contains three terms, for which we have
ei Dâˆ’1 ADâˆ’1 ADâˆ’1 ei =

1

d i hi

2
2  (A)ij  (A)jk
âˆ’ ei Dâˆ’1 ADâˆ’1 ADâˆ’1 Î¹n = âˆ’
â‰¤ 0
n
ndi jâˆˆ[i] dj kâˆˆ[j] dk
1
1  (A)ij (A)jk
1  (A)ij
1 1
1
âˆ’1
âˆ’1
âˆ’1

Î¹
D
AD
AD
Î¹
=
â‰¤
=
=
n
n
2
2
2
2
n i di hi hH
n
n ijk di dj dk
n ijk di dj
2

where, in the last line, we again applied the Cauchyâ€“Schwarz inequality, and the definitions of the harmonic means h and H in the main text. Combining the above gives the
statement of Theorem S.3.
Q.E.D.

8

K. JOCHMANS AND M. WEIDNER

PROOF OF THEOREM 3 (COVARIATES): Define the n Ã— n matrix


âˆ’1

C := B B B X X X X B
Let Î»i (C) denote the ith eigenvalue of C, arranged in ascending order. The matrix C is
similar to the positive semidefinite matrix


XX

âˆ’1/2




âˆ’1/2
XB BB BX XX


and since similar matrices share the same eigenvalues, we have Î»1 (C) â‰¥ 0. The matrix C
is also similar to the matrix


âˆ’1

B BB BX XX X
which is the product of two projection matrices, whose spectral norm is thus bounded
by 1. Hence, Î»n (C) â‰¤ 1. In addition, we must have Î»i (C) = 1 for any 1 < i < n because,
otherwise, rank(I n âˆ’ C) < n, which implies that rank(B M X B) < n âˆ’ 1, contradicting our
non-collinearity assumption (since the graph is connected, we have rank(B B) = n âˆ’ 1,
which together with the non-collinearity assumption rank((X B)) = p+nâˆ’1 implies that
rank(B M X B) = n âˆ’ 1). We, therefore, have C 2 < 1, implying that I m âˆ’ C is invertible.
Using (S.3) and (S.4) with Î» = mâˆ’1 , we find that (B M X B + mâˆ’1 DÎ¹n Î¹n D)âˆ’1 =
(B M X B) + mâˆ’1 Î¹n Î¹n or, equivalently,
B M X B + mâˆ’1 DÎ¹n Î¹n D =




âˆ’1
B M X B + mâˆ’1 Î¹n Î¹n 

and analogously we have
B B + mâˆ’1 DÎ¹n Î¹n D =




âˆ’1
B B + mâˆ’1 Î¹n Î¹n 

(S.10)

Subtracting the expressions in the last two displays gives

âˆ’1



âˆ’1 
âˆ’1
B X X X X B = B B + mâˆ’1 Î¹n Î¹n âˆ’ B M X B + mâˆ’1 Î¹n Î¹n 
and by multiplying with [(B B) + mâˆ’1 Î¹n Î¹n ] from the left and [(B M X B) + mâˆ’1 Î¹n Î¹n ] from
the right, and using BÎ¹n = 0, we obtain


âˆ’1

 
 


B B B X X X X B B MXB = B MXB âˆ’ B B 
which can equivalently be expressed as (I m âˆ’ C)(B M X B) = (B B) . We have already
argued that (I m âˆ’ C) is invertible and, therefore,




B M X B = (I m âˆ’ C)âˆ’1 B B 
Since C

2

< 1, we can expand (I m âˆ’ C)âˆ’1 in powers of C, as
âˆž

 

B MXB =
Cr B B 



r=0

Defining the p Ã— p matrix
âˆ’1/2



âˆ’1/2


XB BB BX XX

C := X X

(S.11)

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

9

we can rewrite (S.11) as


âˆž

 
 


âˆ’1/2 



r 
 X X âˆ’1/2 X B B B  
B MXB = B B + B B B X X X
C
r=0

The parameter Ï defined in the main text satisfies

âˆ’1/2

âˆ’1/2 
 = Ip âˆ’ 
Ï= XX
X M BX X X
C
2

2

 2
=1âˆ’ C

 2 = 1 âˆ’ Ï, and since 
C is symmetric and semidefinite, this can equivathat is, we have C

lently be written as C â‰¤ (1 âˆ’ Ï)I p . Therefore,
âˆž


r

C â‰¤

r=0

âˆž

(1 âˆ’ Ï)r I p = Ïâˆ’1 I p 
r=0

We thus have

âˆž

 
 


âˆ’1/2 




r 
 X X âˆ’1/2 X B B B 
C
B MXB âˆ’ B B = B B B X X X
r=0

â‰¤



âˆ’1


1
BB BX XX XB BB
Ï

(S.12)

and, therefore,
 
 

var(Î±ÌŒi ) âˆ’ var(Î±Ì‚i ) = Ïƒ 2 ei B M X B âˆ’ B B ei
â‰¤



âˆ’1

 
Ïƒ 2 
ei B B B X X X X B B B ei 
Ï

Using the expressions (S.6) and (S.7) for (B B) = L , we obtain


âˆ’1



ei B B B X X X X B B B ei

âˆ’1
= ei L B X X X X BL ei




âˆ’1

= ei Dâˆ’1 + Dâˆ’1 AL B X X X X B Dâˆ’1 + L ADâˆ’1 ei

â‰¤ Ti(1) + Ti(2) + 2 Ti(1) Ti(2) 
where

âˆ’1
Ti(1) := ei Dâˆ’1 B X X X X BDâˆ’1 ei 

âˆ’1
Ti(2) := ei Dâˆ’1 AL B X X X X BL ADâˆ’1 ei 
and we used the Cauchyâ€“Schwarz inequality to bound the mixed term. Again, because
similar matrices have the same eigenvalues, we have
  1/2

âˆ’1
 1/2 
 L
 2 =1âˆ’Ï
B X X X X B L  2 = C

10

K. JOCHMANS AND M. WEIDNER

and, therefore,
 1/2   1/2

âˆ’1
 1/2   1/2
Ti(2) = ei Dâˆ’1 A L
B X X X X B L
ADâˆ’1 ei
L
L
â‰¤ (1 âˆ’ Ï)ei Dâˆ’1 AL ADâˆ’1 ei
â‰¤

1âˆ’Ï
e Dâˆ’1 ADâˆ’1 ADâˆ’1 ei
Î»2 i

=

1âˆ’Ï

Î»2 di hi

where, in the last step, we used ei Dâˆ’1 ADâˆ’1 ADâˆ’1 ei = (di hi )âˆ’1 . Using our definitions xi =
X BDâˆ’1 ei and Î© = X X/m, we obtain

âˆ’1
1
Ti(1) = ei Dâˆ’1 B X X X X BDâˆ’1 ei = xi Î©âˆ’1 xi 
m
Combining the above results, we find


Ïƒ 2  (1)
Ti + Ti(2) + 2 Ti(1) Ti(2)
Ï



1
1âˆ’Ï
1
âˆ’
Ï
Ïƒ2 1
âˆ’1
âˆ’1

x Î© xi +
+2
x Î© xi
â‰¤
Ï m i
Î»2 di hi
m i
Î»2 di hi

var(Î±ÌŒi ) âˆ’ var(Î±Ì‚i ) â‰¤

âˆš
For any a b â‰¥ 0, we have a + b + 2 ab â‰¤ 2(a + b). Thus, a slightly cruder but simpler
bound is given by

âˆ’1
2


var(Î±ÌŒi ) âˆ’ var(Î±Ì‚i ) â‰¤ 2Ïƒ xi Î© xi + 1 âˆ’ Ï 
Ï
m
Î»2 di hi
where we also used that var(Î±ÌŒi ) â‰¥ var(Î±Ì‚i ), because adding regressors can only increase
the variance of the least-squares estimator under homoskedasticity.
Q.E.D.
PROOF OF THEOREM 4 (FIRST-ORDER REPRESENTATION): Remember that we treat B
and X as fixed (i.e., nonrandom) throughout. Let Î²ÌŒ := (X M B X)âˆ’1 X M B y. Using the
model for y, we find Î²ÌŒ âˆ’ Î² = (X M B X)âˆ’1 X M B u. Using our assumptions E(u) = 0 and
Î£ â‰¤ I m Ïƒ 2 , we find E(Î²ÌŒ âˆ’ Î²) = 0 and

âˆ’1

âˆ’1
 
E (Î²ÌŒ âˆ’ Î²)(Î²ÌŒ âˆ’ Î²) = X M B X X M B Î£M B X X M B X
âˆ’1

âˆ’1

â‰¤ Ïƒ 2 X M BX X M BI mM BX X M BX
âˆ’1

= Ïƒ 2 X M BX 

(S.13)

The result in (S.10) can be rewritten as

âˆ’1
âˆ’ mâˆ’1 Î¹n Î¹n 
L = L + mâˆ’1 dd

(S.14)

FIXED-EFFECT REGRESSIONS ON NETWORK DATA

11

The constrained least-squares estimator in (4) can be expressed as
Î±ÌŒ = arg min

aâˆˆ{aâˆˆRn :d a=0}

y âˆ’ X Î²ÌŒ âˆ’ Ba 2 

(S.15)

and analogous to Theorem 1, we then find Î±ÌŒ = L B (y âˆ’ X Î²ÌŒ) = (L + mâˆ’1 dd )âˆ’1 B (y âˆ’
X Î²ÌŒ). Multiplying by (L + mâˆ’1 dd ) from the left and using our normalization d Î±ÌŒ = 0
gives
LÎ±ÌŒ = B (y âˆ’ X Î²ÌŒ)
Plugging L = D âˆ’ A and y = BÎ± + XÎ² + u into the last display, multiplying from the left
with Dâˆ’1 , and rearranging terms, we obtain
Î±ÌŒ âˆ’ Î± = Dâˆ’1 B u +  + 
Ëœ

(S.16)

where
 := Dâˆ’1 A(Î±ÌŒ âˆ’ Î±)

Ëœ := âˆ’Dâˆ’1 B X(Î²ÌŒ âˆ’ Î²)

We have E(Î²ÌŒ âˆ’ Î²) = 0 and E(Î±ÌŒ âˆ’ Î±) = 0, and, therefore, also E() = 0 and E()
Ëœ = 0.
The definition Ï = (X X)âˆ’1 X M B X 2 can equivalently be written as ÏX X â‰¥ X M B X,
and, therefore, Ïâˆ’1 (X X)âˆ’1 â‰¤ (X M B X)âˆ’1 . Using this and (S.13), we obtain

âˆ’1
 
E Ëœ Ëœ â‰¤ Ïƒ 2 Dâˆ’1 B X X M B X X BDâˆ’1
â‰¤


âˆ’1
Ïƒ 2 âˆ’1
D B X X X X BDâˆ’1 
Ï

Using Î±ÌŒ âˆ’ Î± = (B M X B) B M X u and the assumption Î£ â‰¤ Ïƒ 2 I n , we calculate




 
E  = Dâˆ’1 A B M X B B M X Î£M X B B M X B ADâˆ’1




â‰¤ Ïƒ 2 Dâˆ’1 A B M X B B M X B B M X B ADâˆ’1


= Ïƒ 2 Dâˆ’1 A B M X B ADâˆ’1





âˆ’1


Ïƒ2
â‰¤ Ïƒ 2 Dâˆ’1 A B B ADâˆ’1 + Dâˆ’1 A B B B X X X X B B B ADâˆ’1 
Ï
where, in the last step, we used (S.12). Since furthermore X(X X)âˆ’1 X â‰¤ I m and (B B) =
âˆ’1
L â‰¤ Î»âˆ’1
2 D , we obtain






 
Ïƒ2
E  â‰¤ Ïƒ 2 Dâˆ’1 A B B ADâˆ’1 + Dâˆ’1 A B B B B B B ADâˆ’1
Ï
=


Ïƒ 2 (1 + Ï) âˆ’1 
D A B B ADâˆ’1
Ï

â‰¤

Ïƒ 2 (1 + Ï) âˆ’1
D ADâˆ’1 ADâˆ’1 
Î»2 Ï

12

K. JOCHMANS AND M. WEIDNER

Denote the elements of  and Ëœ by

i

and Ëœ i . Equation (S.16) can then be written as

Î±ÌŒi âˆ’ Î±i =

bi u
+
di

i

+ Ëœi

and we have
E

 2
i

â‰¤

Ïƒ 2 (1 + Ï)
Ïƒ 2 (1 + Ï) 1
ei Dâˆ’1 ADâˆ’1 ADâˆ’1 ei =
Î»2 Ï
Î»2 Ï
d i hi

and
  Ïƒ2

âˆ’1
1 Ïƒ2
ei Dâˆ’1 B X X X X BDâˆ’1 ei =
E Ëœ 2i â‰¤
x Î©âˆ’1 xi 
Ï
m Ï i
where we used our definitions xi = X bi /di = X BDâˆ’1 ei and Î© := X X/m.

Q.E.D.

PROOF OF THEOREM 5 (ASYMPTOTIC DISTRIBUTION): We have Ï â‰¤ 1 by definition.
Together with the assumptions Ïƒ 2 = O(1), Î»2 hi â†’ âˆž, and the conditions in (13), this implies that E( 2i ) â‰¤ Ïƒ 2 (1 + Ï)/(Ïdi Î»2 hi ) = o(diâˆ’1 ) and E(Ëœ 2i ) â‰¤ Ïƒ 2 xi Î©âˆ’1 xi /(Ïm) = o(diâˆ’1 ).
By Markovâ€™s inequality, we thus have i = op (diâˆ’1/2 ) and Ëœ i = op (diâˆ’1/2 ), and applying Theorem 4 gives, as di â†’ âˆž,
p

(Î±ÌŒi âˆ’ Î±i ) â†’

bi u
1  
=
Î½Îµe i 
di
di jâˆˆ[i] eâˆˆE

Î½Îµe i := (B)Îµe i uÎµe 

(ij)

The number of terms Î½Îµe i summed over in the last display grows to infinity asymptotically, because we assume that di = jâˆˆ[i] eâˆˆE(ij) we â†’ âˆž, while the weights we = (B)2Îµe i
are bounded. Our assumptions furthermore guarantee that the Î½Îµe i are independent and
satisfy E(Î½Îµe i ) = 0, E(Î½Îµ2e i ) â‰¥ c1 > 0 and E(|Î½Îµe i |3 ) â‰¤ c2 < âˆž for constants c1 and c2 . Thus,
the Lyapunov condition is satisfied, and the statement of the theorem then follows from
a standard application of Lyapunovâ€™s central limit theorem.
Q.E.D.
REFERENCES
BUTLER, S. (2016): â€œAlgebraic Aspects of the Normalized Laplacian,â€ in Recent Trends in Combinatorics, ed.
by A. Beveridge, J. R. Griggs, L. Hogben, G. Musiker, and P. Tetali. Springer, 295â€“315. [3]
CHUNG, F. R. K. (1997): Spectral Graph Theory. CBMS Regional Conference Series in Mathematics, Vol. 92.
American Mathematical Society. [2]
FINKELSTEIN, A., M. GENTZKOW, AND H. WILLIAMS (2016): â€œSources of Geographic Variation in Health
Care: Evidence From Patient Migration,â€ Quarterly Journal of Economics, 131, 1681â€“1726. [3]

Co-editor Ulrich K. MÃ¼ller handled this manuscript.
Manuscript received 4 August, 2016; final version accepted 3 April, 2019; available online 10 April, 2019.

