Appendix

Appendix
Direct and indirect treatment eﬀects - causal chains
and mediation analysis with instrumental variables
Markus Frölich* and Martin Huber**
*Center for Evaluation and Development C4ED and University of Mannheim
**University of Fribourg, Dept. of Economics
20 October 2016

1

1

Causal graphs on Assumption 1 versus condition (6) in main
text

In this appendix we illustrate a few examples in the form of Directed Acyclic Graphs (DAG) to
indicate that Assumption 1 is (a little bit) weaker than the stronger condition given in (6). To
simplify the exposition, all other covariates X have been dropped. Hence, all examples satisfy
that
Assumption 1: IV independence
(Z1 , Z2 )⊥⊥(U, V )|T

and

Z1 ⊥
⊥(U, V, T )|Z2 ,

but they do not satisfy the stronger condition, which was given in condition (6) of the main paper
as:
(Z1 , Z2 )⊥
⊥(U, V, W ).
Remember that the type T is uniquely determined by the value of W because treatment status
D is deﬁned as D = 1 ( χ(Z1 ) ≥ W ) for Z1 ∈ {0, 1}.
Although the diﬀerences between the two assumptions are likely to be of limited relevance
for applied work, the main diﬀerence is that Assumption 1 permits the type T and Z2 to be
correlated whereas condition (6) would not. Since the fraction of compliers conditional on Z2 is
identiﬁed, condition (6) imposes a testable restriction whereas Assumption 1 does not. This could
be relevant in applications because ﬁnding a covariation between the probability of compliance
and Z2 does not imply that the identiﬁcation conditions of Theorem 1 would be violated, because
only Assumption 1 is needed and not condition (6).
The following ﬁgures show various examples of DAGs. Note that in all these graphs there are
arrows such as
W

−→ T

−→

D
↑
Z1

because, in these examples where all other covariates X have been dropped in order to simplify
the exposition, the type T is uniquely determined by W , i.e. it is a mapping of W into the three
diﬀerent types, and D is uniquely determined by T and Z1 .

2

The ﬁgures (a) and (b) show examples of a DAG where W and Z2 are correlated due to
further common unobservables. Hence, the instrument Z2 itself depends on unobservables related
to W , which implies that Z2 and the type T are correlated. In these examples, Assumption 1 is
nevertheless satisﬁed. Note, though, that in these examples W must be independent of U and V ,
as seen in the DAGs. The reason for why dependence between W and (U, V ) is not permitted
in these graphs, in contrast to the later graphs, is that Assumption 1 requires Z2 ⊥
⊥(U, V )|T .
Suppose there was an arrow between W and U, then by conditioning on W we would make
(U, V ) and Z2 dependent. The same applies when conditioning on T because T is a child of W .

U

U

Y
W

Y
W

T

D

T

D

M
Z1

Z2

M

V

Z1

(a)

Z2

V

(b)

The next examples consider cases where W directly inﬂuences Z2 . Hence, these DAGs contain
an arrow from W to Z2 , such that again Z2 and the type are correlated and the stronger condition
(6) is not satisﬁed. Note that in these examples there are no common unobservables of Z2 and
W (in contrast to (a) and (b)) and now we can permit arbitrary dependence between W and the
unobservables U and V . {In principle, we could also have presented further examples, where an
arrow emanates from Z2 towards W , i.e. where Z2 inﬂuences W , but such examples appear less
interesting at least for applications in economics as one would like to permit that the unobservables
W could be immanent characteristics of an individual that are not (easily) malleable by external
(instrumental) variables.}
In example (c), W inﬂuences Z2 only indirectly via treatment status D, whereas in example (e),
W inﬂuences Z2 only directly. In example (d), both arrows are permitted, i.e. the unobservable
W can inﬂuence the instrumental variable Z2 directly and/or via treatment status D. Example
3

(f) is an extension of example (e) where a further arrow from Z2 to Z1 is included, i.e. where the
instrument Z2 also causally inﬂuences Z1 . {Such an extension is not possible for (c) and (d) as
it would introduce a cycle in the DAG.}
In all these examples, Q represents some further unobservable that introduces variation in Z2
to ensure that Z2 is not deterministically determined by W and D. Such further variation is also
required for Z1 in example (f), but is omitted in the optical presentation as it should be obvious
that Z1 and Z2 must not have a correlation of one.
These examples are not exhaustive and merely illustrate possible structures that would permit
correlation between Z2 and the type T but still admit identiﬁcation of the potential outcomes.

U

U

T

T

Y

W

Y

W
D

D
M

Z1

M

Z2

Z1

V
Q

Z2

V
Q

(c)

(d)

4

U

T

U

T

Y

W

Y

W
D

D
M

Z1

M

Z2

V

Z1

Q

Z2

V
Q

(e)

(f)

5

2

Proof of Lemma 1

Lemma 1: Under Assumptions 1, 3 and 4 it follows that
Part a)
Ci = FM|D,Z2 ,X,T =co (Mi , Di , Z2i , Xi ) = FV |X=Xi ,T =co (Vi )

(1)

Vi = FV−1
|X=Xi ,T =co (Ci )

(2)

M⊥
⊥U|C, X, T = co.

(3)

Part b)

Part c)

2.1

Main proof

Proof of Part a:
We ﬁrst proof the ﬁrst equality of (1). First note that
FM|D,Z2 ,X,T =co (m, d, z2 , x) = FM|Z1 ,Z2 ,X,T =co (m, d, z2 , x)
because D = Z1 for compliers. Now inserting the results of Section 2.4 we obtain
=

E [(d + D − 1) · (Z1 − π̄(z2 , x)) |M ≤ m, Z2 = z2 , X = x]
FM|Z2 ,X (m, z2 , x).
E [D · (Z1 − π̄(z2 , x)) |Z2 = z2 , X = x]

(4)

Hence,
Ci = FM|D,Z2 ,X,T =co (Mi , Di , Z2i , Xi )
=

E [(Di + D − 1) · (Z1 − π̄(Z2i , Xi )) |M ≤ Mi , Z2 = Z2i , X = Xi ]
FM|Z2 ,X (Mi , Z2i , Xi ) .
E [D · (Z1 − π̄(Z2i , Xi )) |Z2 = Z2i , X = Xi ]

––––––––––––––––––––––––––––––
We now proof the second equality of (1), i.e. we need to show that
FM|D,Z2 ,X,T =co (Mi , Di , Z2i , Xi ) = FV |X,T =co (Vi , Xi ) .
Consider an individual i with observed values Mi , Di , Z2i , Xi and consider the following expression (treating Mi , Di , Z2i , Xi as given i.e. non-stochastic):
FM|D,Z2 ,X,T =co (Mi , Di , Z2i , Xi )
= Pr (M ≤ Mi |D = Di , Z2 = Z2i , X = Xi , T = co)
= Pr (ζ(D, Z2 , X, V ) ≤ ζ(Di , Z2i , Xi , Vi ) |D = Di , Z2 = Z2i , X = Xi , T = co)
= Pr (ζ(Di , Z2i , Xi , V ) ≤ ζ(Di , Z2i , Xi , Vi ) |D = Di , Z2 = Z2i , X = Xi , T = co) .
6

Now we make use of the strict monotonicity of ζ to obtain
= Pr (V ≤ Vi |D = Di , Z2 = Z2i , X = Xi , T = co)
and using the fact that D = Z1 for compliers we obtain
= Pr (V ≤ Vi |Z1 = Di , Z2 = Z2i , X = Xi , T = co)
= FV |Z1 ,Z2 ,X,T =co (Vi , Di , Z2i , Xi ) .
Noting that V ⊥⊥(Z1 , Z2 )|X, T = co by Assumption 1 we have
= FV |X,T =co (Vi , Xi ) = FV |X=Xi ,T =co (Vi ) .
––––––––––––––––––––––––––––––
Proof of Part b:
As we asssumed that FV |X=x,T =co (v) is strictly increasing by Assumption 4, we obtain that
Vi = FV−1
|X=Xi ,T =co (Ci ) .
Hence, conditional on X, Vi is a one-to-one function of Ci and thus V is identiﬁed. That means
conditioning on C is equivalent to conditioning on V , as long as we condition on X throughout. Because we always condition on X in the subsequent derivations, conditioning on V and
conditioning on FV |X=x (V ) is equivalent.
––––––––––––––––––––––––––––––
Proof of Part c:
We need to show that
M⊥
⊥U|C, X, T = co.
Examine
FM|U,C,X,T =co (a)
= Pr (M ≤ a|U, C, X, T = co)
= Pr (ζ(D, Z2 , X, V ) ≤ a|U, C, X, T = co)
where we inserted the equation for M . Now we use that D = Z1 for compliers and the result of
Part b, i.e. that Vi = FV−1
|X=Xi ,T =co (Ci ) to obtain
= Pr ζ(Z1 , Z2 , X, FV−1
|X,T =co (C)) ≤ a|U, C, X, T = co .
7

The only stochastic components in the probability expression are Z1 and Z2 which are independent
of U conditional on C, X, T = co by Assumption 1. Hence, the probability does not depend on
the value of U such that
= Pr ζ(Z1 , Z2 , X, FV−1
|X,T =co (C)) ≤ a|C, X, T = co
= FM|C,X,T =co (a)
which completes the proof.

2.2

Additional calculations

2.3

Probability mass of compliers

We ﬁrst show that the fraction of compliers is identiﬁed by Z1 ⊥⊥T |X
E

D Z1 − Π
= Pr(T = co).
Π 1−Π

E

D Z1 − Π̄
= Pr(T = co)
Π̄ 1 − Π̄

(5)

It is also identiﬁed as

if Z1 ⊥
⊥T |Z2 , X.
To show this, by iterated expectations we can write the previous expression via iterated
expectations as
DZ1 D(1 − Z1 )
−
Π
1−Π
DZ1 D(1 − Z1 )
= E E
−
|X
Π
1−Π
= E [E [D|X, Z1 = 1] − E [D|X, Z1 = 0]]
E

and partition by the type, where we use that D = 0 for never-takers
= E [E [D|X, Z1 = 1, T = at] Pr (T = at|X, Z1 = 1) + E [D|X, Z1 = 1, T = co] Pr (T = co|X, Z1 = 1)]
−E [E [D|X, Z1 = 0, T = at] Pr (T = at|X, Z1 = 0) + E [D|X, Z1 = 0, T = co] Pr (T = co|X, Z1 = 0)]
= E [Pr (T = at|X, Z1 = 1) − Pr (T = at|X, Z1 = 1) + Pr (T = co|X, Z1 = 1)]
noting that Pr (T = at|X, Z1 = 1) = Pr (T = at|X, Z1 = 0) by Z1 ⊥
⊥T |X we obtain
= E [Pr (T = co|X, Z1 = 1)] = E [Pr (T = co|X)] = Pr (T = co) .
8

Similarly, we can derive:
Pr (T = co|Z2 , X) = E

D Z1 − Π̄
|Z2 , X
Π̄ 1 − Π̄

(6)

if Z1 ⊥
⊥T |Z2 , X. By similar derivations as before we obtain that
Pr (T = co) = E

2.4

D Z1 − Π̄
.
Π̄ 1 − Π̄

(7)

Complier distribution functions

Note that we do not use Assumption 2.
In the following we derive the distribution function FM|Z1 =z1 ,Z2 ,X,T =co . We derive the results
separately for z1 = 1 and for z1 = 0. We begin with z1 = 1 and show that
FM|Z1 =1,Z2 ,X,T =co (m, z2 , x) =

E D Z1 − Π̄ |M ≤ m, Z2 = z2 , X = x
· FM|Z2 ,X (m, z2 , x). (8)
E D Z1 − Π̄ |Z2 = z2 , X = x

We start with
Z1 − π̄(Z2 , X)
|M ≤ m, Z2 , X · FM|Z2 ,X
π̄(Z2 , X) (1 − π̄(Z2 , X))
DZ1
D(1 − Z1 )
= E
−
|M ≤ m, Z2 , X · FM|Z2 ,X
π̄(Z2 , X) 1 − π̄(Z2 , X)
Pr (D = 1, Z1 = 1|M ≤ m, Z2 , X) · FM|Z2 ,X
Pr (D = 1, Z1 = 0|M ≤ m, Z2 , X) · FM|Z2 ,X
=
−
π̄(Z2 , X)
1 − π̄(Z2 , X)
E D

=

Pr (D = 1, Z1 = 1|M ≤ m, Z2 , X) · FM|Z2 ,X (m) Pr (D = 1, Z1 = 0|M ≤ m, Z2 , X) · FM|Z2 ,X (m)
−
Pr (Z1 = 1|Z2 , X)
Pr (Z1 = 0|Z2 , X)

= FM|D=1,Z1 =1,Z2 ,X (m) Pr (D = 1|Z1 = 1, Z2 , X)−FM|D=1,Z1 =0,Z2 ,X (m) Pr (D = 1|Z1 = 0, Z2 , X)

= E [1 (M ≤ m) |D = 1, Z1 = 1, Z2 , X] Pr (D = 1|Z1 = 1, Z2 , X)
− E [1 (M ≤ m) |D = 1, Z1 = 0, Z2 , X] Pr (D = 1|Z1 = 0, Z2 , X)
= E [1 (M ≤ m) D|Z1 = 1, Z2 , X] − E [1 (M ≤ m) D|Z1 = 0, Z2 , X]

9

and partition by type (at,nt,co) and calculate each term to obtain:
= E [1 (M ≤ m) D|Z1 = 1, Z2 , X, T = at] Pr (T = at|Z1 = 1, Z2 , X)
+E [1 (M ≤ m) D|Z1 = 1, Z2 , X, T = nt] Pr (T = nt|Z1 = 1, Z2 , X)
+E [1 (M ≤ m) D|Z1 = 1, Z2 , X, T = co] Pr (T = co|Z1 = 1, Z2 , X)
−E [1 (M ≤ m) D|Z1 = 0, Z2 , X, T = at] Pr (T = at|Z1 = 0, Z2 , X)
−E [1 (M ≤ m) D|Z1 = 0, Z2 , X, T = nt] Pr (T = nt|Z1 = 0, Z2 , X)
−E [1 (M ≤ m) D|Z1 = 0, Z2 , X, T = co] Pr (T = co|Z1 = 0, Z2 , X)
= E 1 M 1 ≤ m |Z1 = 1, Z2 , X, T = at Pr (T = at|Z1 = 1, Z2 , X)
+E 1 M 1 ≤ m |Z1 = 1, Z2 , X, T = co Pr (T = co|Z1 = 1, Z2 , X)
−E 1 M 1 ≤ m |Z1 = 0, Z2 , X, T = at Pr (T = at|Z1 = 0, Z2 , X)
now we use that M 1 ⊥⊥Z1 |Z2 , X, T = co and T ⊥
⊥Z1 |Z2 , X to obtain
= E 1 M 1 ≤ m |Z2 , X, T = at Pr (T = at|Z2 , X)
+E 1 M 1 ≤ m |Z1 = 1, Z2 , X, T = co Pr (T = co|Z1 = 1, Z2 , X)
−E 1 M 1 ≤ m |Z2 , X, T = at Pr (T = at|Z2 , X)

= E [1 (M ≤ m) |Z1 = 1, Z2 , X, T = co] Pr (T = co|Z1 = 1, Z2 , X)
= FM|Z1 =1,Z2 ,X,T =co (m) Pr (T = co|Z1 = 1, Z2 , X)
= FM|Z1 =1,Z2 ,X,T =co (m) Pr (T = co|Z2 , X) .

Note further that
Pr (T = co|Z2 , X) = E

D
Z1 − π̄ (Z2 , X)
|Z2 , X .
π̄ (Z2 , X) 1 − π̄ (Z2 , X)

(9)

Combining these results we obtain
E D

Z1 − π̄(Z2 , X)
D
Z1 − π̄ (Z2 , X)
|M ≤ m, Z2 , X ·FM|Z2 ,X = FM|Z1 =1,Z2 ,X,T =co ·E
|Z2 , X
π̄(Z2 , X) (1 − π̄(Z2 , X))
π̄ (Z2 , X) 1 − π̄ (Z2 , X)

and thus
FM|Z1 =1,Z2 ,X,T =co =

E [D (Z1 − π̄ (Z2 , X)) |M ≤ m, Z2 , X]
· FM|Z2 ,X
E [D (Z1 − π̄ (Z2 , X)) |Z2 , X]
10

FM|Z1 =1,Z2 ,X,T =co (m, z2 , x) =

E [D (Z1 − π̄ (Z2 , X)) |M ≤ m, Z2 = z2 , X = x]
·FM|Z2 ,X (m, z2 , x)
E [D (Z1 − π̄ (Z2 , X)) |Z2 = z2 , X = x]
(10)

––––––––––––––––––––––––––––––––––
Analogously we can calculate the cdf for z1 = 0. With similar derivations as before we obtain
E

1 − D Z1 − π̄(Z2 , X)
|M ≤ m, Z2 , X FM|Z2 ,X = −FM|Z1 =0,Z2 ,X,T =co Pr (T = co|Z2 , X)
π̄(Z2 , X) 1 − π̄(Z2 , X)

and inserting the expression for Pr (T = co|Z2 , X) we have
FM|Z1 =0,Z2 ,X,T =co (m, z2 , x) = −

E [(1 − D) (Z1 − π̄ (Z2 , X)) |M ≤ m, Z2 = z2 , X = x]
·FM|Z2 ,X (m, z2 , x) .
E [D (Z1 − π̄ (Z2 , X)) |Z2 = z2 , X = x]
(11)

Now we combine the results for FM|Z1 =1,Z2 ,X,T =co and FM|Z1 =0,Z2 ,X,T =co as
FM|Z1 =z1 ,Z2 ,X,T =co = z1 · FM|Z1 =1,Z2 ,X,T =co + (1 − z1 ) · FM|Z1 =0,Z2 ,X,T =co
and insert the previous expressions, which gives equation (4).

3

Lemma 2: Supplementary result for Theorem 5

In the main paper in Theorem 5 in Section 3.5 of the main paper we examined identiﬁcation with
discrete instruments. We examined the model
Y

= ϕ(D, M, U) + ψ(D, X),

(12)

M = ζ(D, Z2 , X, V ),
D = 1 ( χ(Z1 , X, W ) ≥ 0 ) ,
where both Z1 and Z2 are discrete and X contains (at least) one continuous variable. We normalized the intercept of ψ such that for some value x0 in the support of X
ψ(1, x0 ) = 0,
and further derived the relationship
E [Y D (Z1 − Π) |M = m, C = c, X = x]
= Ξ(m, c) + ψ(1, x),
E [D (Z1 − Π) |M = m, C = c, X = x]
11

(13)

for some values m, c, x in the support of M, C, X.
Consider now two triplets (m, c, x1 ) and (m, c, x0 ) in the support of M, C, X. We obtain
E [Y D (Z1 − Π) |M = m, C = c, X = x1 ] E [Y D (Z1 − Π) |M = m, C = c, X = x0 ]
−
= ψ(1, x1 ).
E [D (Z1 − Π) |M = m, C = c, X = x1 ]
E [D (Z1 − Π) |M = m, C = c, X = x0 ]
(14)
because ψ(1, x0 ) has been normalized to be zero. Similarly, consider two triplets (m′ , c′ , x2 ) and
(m′ , c′ , x1 ) in the support of M, C, X:
E [Y D (Z1 − Π) |M = m′ , C = c′ , X = x2 ] E [Y D (Z1 − Π) |M = m′ , C = c′ , X = x1 ]
−
= ψ(1, x2 )−ψ(1, x1 ),
E [D (Z1 − Π) |M = m′ , C = c′ , X = x2 ]
E [D (Z1 − Π) |M = m′ , C = c′ , X = x1 ]
and as ψ(1, x1 ) is identiﬁed by (46), so is ψ(1, x2 ). We can thus identify ψ(1, x1 ), ψ(1, x2 ), and
so forth.
However, identiﬁcation of the entire function ψ(1, x) for all x in the support of X is more
diﬃcult. We discussed identiﬁcation of ψ if ψ is a parametric function. On the other hand, if
ψ is a non-parametric function, identiﬁcation conditions are more diﬃcult to characterize and
require more than smoothness assumptions. If we do not want to impose any structure on ψ
(apart from continuity), it would be required that for any x1 in the support of X, there exists
(at least) one value of m and c, respectively, in the supports of M and C which allows applying
(46). This would be unproblematic if m did not appear on the right-hand-side of (45), because
C and X are independent by Assumption 10. However, in the T = co, Z1 = 1 subpopulation, M
is a deterministic function of Z2 , X and C only, such that M, C, and X are closely related and
the relationship is non-deterministic only because of Z2 .
Consider as an example the most cumbersome case where both Z1 and Z2 are binary. M takes
only two diﬀerent values conditional on X and C and being in the T = co, Z1 = 1 subpopulation.
It is diﬃcult to ﬁnd general identiﬁcation conditions. One special case applies if the function
M = ζ(D, Z2 , X, V ) happens to be invertible in x, i.e. in its third argument. Since there is a oneto-one mapping between V and C (as established in Lemma 1), we can also ﬁnd a function ζ̄ such
−1

that M can be expressed in terms of C instead of V , that is M = ζ̄(D, Z2 , X, C). Let ζ̄ (3) denote
−1

the inverse function with respect to the third argument of ζ̄, such that X = ζ̄ (3) (D, Z2 , M, C).
Consider some value c in the support of C. For x0 and c given, two diﬀerent values of m can
be observed, depending on whether Z2 takes the value 0 or 1. (Note that we are always in the
T = co, Z1 = 1 subpopulation.) Suppose z2 = 1. Then, there exists a diﬀerent value x1 which in
combination with z2 = 0 would deliver the same value of m, i.e. ζ̄(1, z2 = 0, x1 , c) = ζ̄(1, z2 =
12

−1

1, x0 , c). This value x1 is given by x1 = ζ̄ (3) (d = 1, z2 = 0, ζ̄(d = 1, z2 = 1, x0 , c), c). Similarly, for
the same x0 and c, one could observe Z2 = 0. This would give us a value, say, m̄. Now, there
exists a diﬀerent value x2 which in combination with z2 = 1 would deliver the same value m̄.
−1

This value x2 is given by x2 = ζ̄ (3) (d = 1, z2 = 1, ζ̄(d = 1, z2 = 0, x0 , c), c). These two values x1
and x2 are observable with positive density if 0 < Pr(Z2 = 1|C, X) < 1 so that both values of Z2
actually occur for given c and x.1 Hence, for each value of c there exist two values x1 and x2 at
which ψ(1, x) is identiﬁed.2 Identiﬁcation of ψ(1, x̄) at a general x̄ requires the existence of a c
in the support of C, such that either of the two mappings delivers x̄. This is formally stated in
the following lemma.
Lemma 2: For Z1 and Z2 being binary random variables, the function ψ(1, x) is
identiﬁed at x if there is a value c in the support of C such that either
−1

x = ζ̄ (3) (1, 1, ζ̄(1, 0, x0 , c), c)
or
−1

x = ζ̄ (3) (1, 0, ζ̄(1, 1, x0 , c), c),
under Assumptions 1, 2, 3, 4, 10 and the following assumptions:
i) ζ(D, Z2 , X, V ) is invertible in its third argument,
ii) 0 < Pr(Z2 = 1|C, X) < 1.

4

Lemma 3: Monotonicity calculations

Consider a function ϕ(z, v) that is continuous and strictly monotonously increasing in each argument. Deﬁne ϕ−1 as the inverse function with respect to its ﬁrst argument, i.e. for ϕ(z, v) = a
we have ϕ−1 (a, v) = z. Then this inverse function ϕ−1 (., .) is strictly monotonously increasing in
its ﬁrst argument and strictly monotonously decreasing in its second argument. (Note: We could
also derive a similar expression for weak monotonicity, but we will only need the result for strict
monotonicity for the main proof.)
1

Obviously, they are only identiﬁed in the support of X among compliers, but we anyhow do not require the

function ψ(1, x) to be identiﬁed outside this support.
2
This result applies to a Z2 having only two mass-points. If Z2 was discrete with k mass-points, we could
identify k · (k − 1) values for each c.

13

Proof:
The ﬁrst part, i.e. "then this inverse function ϕ−1 (., .) is monotonously increasing in its ﬁrst
argument", is trivial since the inverse function was deﬁned with respect to the ﬁrst argument
of ϕ. We thus only have to show the second part, i.e. that the "inverse function ϕ−1 (., .) is
monotonously decreasing in its second argument". Consider two values v and v̄ with v< v̄. Using
¯
¯
that ϕ is increasing in its second argument, we have
v̄ > v
¯

=⇒

ϕ(ϕ−1 (z, v), v̄) > ϕ(ϕ−1 (z, v), v)
¯
¯ ¯

ϕ(ϕ−1 (z, v), v̄) > z
¯

=⇒

because z = ϕ(ϕ−1 (z,v),v) and now using z = ϕ(ϕ−1 (z, v̄), v̄)
¯ ¯
=⇒ ϕ(ϕ−1 (z, v), v̄) > ϕ(ϕ−1 (z, v̄), v̄)
¯
which implies because ϕ is increasing in its ﬁrst argument
=⇒ ϕ−1 (z, v) > ϕ−1 (z, v̄).
¯
This shows that ϕ−1 is monotonously decreasing in the second argument.
Note: In the previous derivations we had assumed that ϕ(z, v) is strictly monontonous in both
arguments. An analogous result can be obtained if ϕ is assumed to be only weakly monontonous
in v, as long as ϕ is strictly monontonous in z such that the inverse function ϕ−1 is well deﬁned.
As a further remark, note that in the main text we apply this Lemma to the function
ζ (z1 , z2 , v), which is a function of three arguments. There, we will keep z1 ﬁxed throughout such
that the above Lemma applies with z1 being kept implicit (i.e. in the deﬁnition of the function
ϕ) and where the instrument z2 ﬁgures as the z argument.

5

Theorem 1

Before we prove Theorem 1, we ﬁrst repeat Theorem 1 of the main text, expressed in a slightly
diﬀerent way to make it easier to follow the proofs.
Theorem 1:
E Y

1,M(0)

E
|T = co =

Y DZ1
Π

E

14

−

DZ1
Π

Y D(1−Z1 )
1−Π

−

D(1−Z1 )
1−Π

Ω
(15)

where
Ω = ω(M, C, X) ≡ 1 −

E [Z1 |M, C, X] − π(X)
E [DZ1 |M, C, X] − E [D|M, C, X] · π(X)

and Π = π(X) with π(x) = Pr (Z1 = 1|X = x) = E [Z1 |X = x].

5.1

Main proof

Below we prove (15). By Lemma 1 we have that C is a one-to-one function of V , conditional on
X. That means conditioning on C is equivalent to conditioning on V , as long as we also condition
on X. In the calculations below we will make use of (19), where it is shown that the weights Ω
are equivalent to
ω(M, C, X) = 1 −

dFM,C|Z1 =0,X,T =co
E [Z1 |M, C, X] − π(X)
=
.
E [DZ1 |M, C, X] − E [D|M, C, X] · π(X)
dFM,C|Z1 =1,X,T =co

Note that the numerator of (15) can be written as
E YD

1 Z1 − Π
Ω
Π 1−Π

Y D Z1 − Π
Ω|T = at Pr (T = at)
Π 1−Π
Y D Z1 − Π
+E
Ω|T = co Pr (T = co)
Π 1−Π

= E

because D is zero for the never-takers. In Section 5.2.2 we show that the term for the alwaystakers is zero. In Section 5.2.1 we show that the denominator of (15) identiﬁes the fraction of
compliers Pr (T = co). We thus obtain that (15) equals
=E

Y D Z1 − Π
Ω|T = co
Π 1−Π

and because D = Z1 for compliers the previous expression equals
=
=
=

Y D Z1 − Π
Ω|X, T = co dFX|T =co
Π 1−Π
Y
E
Ω|Z1 = 1, X, T = co Pr (Z1 = 1|X, T = co) dFX|T =co
Π
E

E [Y · Ω|Z1 = 1, X, T = co] dFX|T =co
15

because of Z1 ⊥
⊥T |X. Now using iterated expectations with respect to M and C we obtain
=

E [Y · Ω|Z1 = 1, M, C, X, T = co] dFM,C|Z1 =1,X,T =co dFX|T =co

Now we insert the weights Ω = ω(M, C, X) which are equivalent to (19) and thus obtain
=

E [Y |Z1 = 1, M, C, X, T = co] ·
=

=

dFM,C|Z1 =0,X,T =co
· dFM,C|Z1 =1,X,T =co dFX|T =co
dFM,C|Z1 =1,X,T =co

E [ϕ(D, M, X, U)|Z1 = 1, M, C, X, T = co] · dFM,C|Z1 =0,X,T =co · dFX|T =co

ϕ(1, m, x, U)dFU |Z1 =1,M=m,C=c,X=x,T =co

· dFM,C|Z1 =0,X,T =co (m, c, x) · dFX|T =co (x)

now we use U⊥
⊥Z1 , M|C, X, T = co which is implied by U⊥⊥(Z1 , Z2 )|V, X, T to obtain
=

ϕ(1, m, x, u)dFU |C,X,T =co (u, c, x) · dFM,C|Z1 =0,X,T =co (m, c, x) · dFX|T =co (x).

We further note that FM,C|Z1 =0,X,T =co

=

FM 0 ,C|Z1 =0,X,T =co

=

FM 0 ,C|X,T =co because

Z1 ⊥⊥M 0 , C|X, T = co which is implied by Z1 ⊥
⊥(Z2 , V )|X, T to obtain
=

ϕ(1, m, x, u)dFU |C,X,T =co (u, c, x) · dFM 0 ,C|X,T =co (m, c, x) · dFX|T =co (x).
=

ϕ(1, m, x, u)dFU |C,X,T =co (u, c, x) · dFM 0 |C,X,T =co (m, c, x)dFC,X|T =co (c, x).

Finally, we make use of M 0 ⊥
⊥U|C, X, T = co which is implied by U⊥
⊥Z2 |V, X, T to obtain
=

ϕ(1, m, x, u)dFM 0 ,U |C,X,T =co (m, u, c, x)dFC,X|T =co (c, x)

=

ϕ(1, M 0 , X, U) · dFM 0 ,U|C,X,T =co · dFC,X|T =co

=

ϕ(1, M 0 , X, U) · dFM 0 ,X,U,C|T =co =

ϕ(1, M 0 , X, U) · dFM 0 ,X,U |T =co

= E ϕ(1, M 0 , X, U)|T = co = E Y 1,M(0) |T = co .

5.2
5.2.1

Additional calculations
Probability mass of compliers

As shown before we have E

D Z1 −Π̄
Π̄ 1−Π̄

= Pr(T = co) and by Assumption 2 we have Π̄ = Π. We

therefore obtain that the fraction of compliers is identiﬁed by
E

DZ1 D(1 − Z1 )
−
= Pr(T = co).
Π
1−Π
16

(16)

5.2.2

Cancellation of terms for the always-takers

We need to show that
E

Y D Z1 − Π
Ω|T = at = 0.
Π 1−Π

Apply iterated expectations with respect to X
Y D Z1 − Π
Ω|X, T = at
Π 1−Π
Y D Z1 − Π
= E
Ω|X, T = at, Z1 = 1 Pr (Z1 = 1|X, T = at)
Π 1−Π
Y D Z1 − Π
+E
Ω|X, T = at, Z1 = 0 Pr (Z1 = 0|X, T = at) .
Π 1−Π
E

Now using Z1 ⊥
⊥T |X we obtain
= E [Y Ω|X, T = at, Z1 = 1] − E [Y Ω|X, T = at, Z1 = 0] .
Now we note that the weights Ω = ω(M, C, X) and entering the equations for Y and M
= E [ϕ(1, ζ(1, Z2 , X, V ), X, U) · ω(ζ(1, Z2 , X, V ), C, X)|X, T = at, Z1 = 1]
− E [ϕ(1, ζ(1, Z2 , X, V ), X, U) · ω(ζ(1, Z2 , X, V ), C, X)|X, T = at, Z1 = 0]
=0
because Z1 ⊥
⊥(Z2 , U, V )|X, T .
5.2.3

Densities for the compliers

(A) Calculation of fM,C|Z1 =1,X,T =co
Consider the following expression
FM,C|D=1,Z1 =1,X (m, c) · Pr (D = 1|Z1 = 1, X) − FM,C|D=1,Z1 =0,X (m, c) · Pr (D = 1|Z1 = 0, X)
= E [1 (M ≤ m, C ≤ c) |D = 1, Z1 = 1, X] · Pr (D = 1|Z1 = 1, X)
−E [1 (M ≤ m, C ≤ c) |D = 1, Z1 = 0, X] · Pr (D = 1|Z1 = 0, X)
= E [1 (M ≤ m, C ≤ c) D|Z1 = 1, X] − E [1 (M ≤ m, C ≤ c) D|Z1 = 0, X]
and partition by type (at,nt,co) and using Z1 ⊥
⊥(Z2 , V, T )|X to obtain:
= E [1 (M ≤ m, C ≤ c) |Z1 = 1, X, T = co] · Pr (T = co|X)
= FM,C|Z1 =1,X,T =co (m, c) · Pr (T = co|X) .
17

From this we obtain the density function by diﬀerentiation on the left and right hand side:
dFM,C|D=1,Z1 =1,X (m, c) · Pr (D = 1|Z1 = 1, X) − dFM,C|D=1,Z1 =0,X (m, c) · Pr (D = 1|Z1 = 0, X)
= dFM,C|Z1 =1,X,T =co (m, c) · Pr (T = co|X) .
By some calculations, making use of Bayes law, we obtain
dFM,C|Z1 =1,X,T =co ·Pr (T = co|X) =

Pr (D = 1, Z1 = 0|M, C, X)
Pr (D = 1, Z1 = 1|M, C, X)
dFM,C|X −
dFM,C|X
Pr (Z1 = 1|X)
Pr (Z1 = 0|X)
D Z1 − Π
|M, C, X dFM,C|X
=E
Π 1−Π

and thus
dFM,C|Z1 =1,X,T =co = E

dFM,C|X
D Z1 − Π
|M, C, X
.
Π 1−Π
Pr (T = co|X)

(17)

(B) Calculation of fM,C|Z1 =0,X,T =co
Consider the expression
FM,C|D=0,Z1 =1,X (m, c) Pr (D = 0|Z1 = 1, X) − FM,C|D=0,Z1 =0,X (m, c) Pr (D = 0|Z1 = 0, X)
= E [1 (M ≤ m, C ≤ c) (1 − D) |Z1 = 1, X] − E [1 (M ≤ m, C ≤ c) (1 − D) |Z1 = 0, X]
and partition by type (at,nt,co) and using Z1 ⊥
⊥(Z2 , V, T )|X to obtain:
= −E [1 (M ≤ m, C ≤ c) |Z1 = 0, X, T = co] · Pr (T = co|X)
= −FM,C|Z1 =0,X,T =co (m, c) · Pr (T = co|X) .
From this we obtain the density function by diﬀerentiation on the left and right hand side:
dFM,C|D=0,Z1 =1,X (m, c) Pr (D = 0|Z1 = 1, X) − dFM,C|D=0,Z1 =0,X (m, c) Pr (D = 0|Z1 = 0, X)
= −dFM,C|Z1 =0,X,T =co (m, c) · Pr (T = co|X)
and after simpliﬁcations we obtain
dFM,C|Z1 =0,X,T =co = E (1 − D)

dFM,C|X
Π − Z1
|M, C, X
(1 − Π)Π
Pr (T = co|X)

18

and ﬁnally
dFM,C|X
D − 1 Z1 − Π
|M, C, X
.
Π 1−Π
Pr (T = co|X)

dFM,C|Z1 =0,X,T =co = E

(18)

(C) Calculation of the density ratio
We need to derive the density ratio
ω(M, C, X) =

dFM,C|Z1 =0,X,T =co
dFM,C|Z1 =1,X,T =co

inserting the previous results we obtain
=

Π−Z1
|M, C, X dFM,C|X
E (1 − D) Π(1−Π)
Z1 −Π
E D Π(1−Π)
|M, C, X dFM,C|X

E [(D − 1) (Z1 − Π) |M, C, X]
E [D (Z1 − Π) |M, C, X]
E [Z1 |M, C, X] − π(X)
= 1−
E [DZ1 |M, C, X] − E [D|M, C, X] · π(X)
=

dFM,C|Z1 =0,X,T =co
E [Z1 |M, C, X] − π(X)
=1−
dFM,C|Z1 =1,X,T =co
E [DZ1 |M, C, X] − E [D|M, C, X] · π(X)

6

(19)

Proof of Theorem 2

In the following we prove Theorem 2 in Section 6.1 below. We need Assumptions 1, 3, 4 and 6,
but we do not need Assumption 2, i.e. we permit that Z1 and Z2 may be dependent.
In our discussion paper we also examine the case if we additionally also impose Assumption 2.
There we ﬁnd that under this additional assumption we obtain an analytically simpler expression:
Theorem 2b: Under Assumptions 1, 2, 3, 4 and 6
E Y 1,m |T = co =

1
Pr (T = co)

E YD

Z1 − Π
|C, X, M = m · Ω · dFC,X
1−Π

(20)

with weights
Ω = ω(C, X) =

1
E [D (Z1 − Π) |C, X]
.
Π E [D (Z1 − Π) |M = m, C, X]

(21)

This simpler expression in Theorem 2b is obtained, because we obtain an explicit expression
for the complier density when imposing Assumption 2, namely
fM|Z1 =1,X,C,T =co =

E [D (Z1 − Π) |M, X, C]
· fM|X,C .
E [D (Z1 − Π) |X, C]
19

(22)

This is not possible for the main Theorem 2 without Assumption 2, where one would need to
condition on Z2 and C, which would imply a degenerate distribution function. With Assumption
2, on the other hand, the weights in (21) may therefore also be expressed as
Ω=

fM|X,C (m)
1
.
Π fM|Z1 =1,X,C,T =co (m)

In the following, however, we only discuss the case without Assumption 2, which will lead to
the less tractable expression given in the main text.

6.1

Main proof of Theorem 2

Consider the expression
1
Pr (T = co)

E YD

Z1 − Π̄
ω(C, X)|X, M = m̄ · dFX
1 − Π̄

(23)

1
Z1 − Π̄
E YD
ω(C, X)|X, M = m̄, T = at Pr (T = at|X, M = m̄) · dFX
Pr (T = co)
1 − Π̄
1
Z1 − Π̄
ω(C, X)|X, M = m̄, Z1 = 1, T = co Pr (T = co, Z1 = 1|X, M = m̄) · dFX .
+
E YD
Pr (T = co)
1 − Π̄

=

In Subsection 6.2.2 we show that the term for the always-takers is zero. We thus obtain
=

1
Pr (T = co)

E [Y ω(C, X)|Z1 = 1, X, M = m̄, T = co] Pr (T = co, Z1 = 1|X, M = m̄) dFX

and using iterated expectations with respect to C
=

1
Pr (T = co)

E [Y ω(C, X)|Z1 = 1, C, X, M = m̄, T = co] dFC|Z1 =1,X,M=m̄,T =co
× Pr (T = co, Z1 = 1|X, M = m̄) dFX

=

1
Pr (T = co)

E [ϕ(1, m̄, X, U)|Z1 = 1, C, X, M = m̄, T = co] · ω(C, X)
×

fM|C,Z1 =1,X,T =co (m̄) · fC|Z1 =1,X,T =co
Pr (Z1 = 1, T = co|X) dFX .
fM|X (m̄)

Now using that fC|Z1 =1,X,T =co = fC|X,T =co because Z1 ⊥
⊥V |X, T = co and using that
Pr (T = co|X) dFX = dFX|T =co Pr (T = co) gives
=

ϕ(1, m̄, X, U)dFU |Z1 =1,C,X,M=m̄,T =co ·ω(C, X)×
20

fM|C,X,Z1 =1,T =co (m̄)
Pr (Z1 = 1|X, T = co)·dFC,X|T =co
fM|X (m̄)

and noting that U⊥
⊥(Z1 , Z2 )|V, X, T = co implies U⊥
⊥(Z1 , M)|C, X, T = co gives
=

ϕ(1, m̄, X, U)dFU |C,X,T =co ·ω(C, X)×

fM|C,Z1 =1,X,T =co (m̄)
Pr (Z1 = 1|X, T = co)·dFC,X|T =co
fM|X (m̄)

and entering the weights ω(C, X)
=

ϕ(1, m̄, X, U)dFU |C,X,T =co dFC,X|T =co

=

ϕ(1, m̄, X, U)dFU,C,X|T =co = E [ϕ(1, m̄, X, U)|T = co] = E Y 1,m̄ |T = co

where we used that the weights can be written equivalently as
ω(C, X) =

=

fM|X (m̄)
1
Pr (Z1 = 1|X, T = co) fM|C,X,Z1 =1,T =co (m̄)
Z1 −Π̄
Pr (Z1 = 1|X, T = co) · E D
|C, X
fM|X (m̄)
Π̄ 1−Π̄
Z1 −Π̄
∂
Pr (Z1 = 1|X, T = co)
∂m E 1 (M ≤ m̄) · D 1−Π̄ |C, X

E
= fM|X (m̄)

∂
∂m E

D Z1 −Π̄
|C, X
Π̄ 1−Π̄

1 −Π̄
1 (M ≤ m̄) · D Z1−
|C, X
Π̄

where we entered the density expression (28).

6.2
6.2.1

Additional calculations
Independence of Z1 and M

As a preliminary calculation we examine the joint distribution
Pr (Z1 = 1, M ≤ m̄|Z2 , X, T = at)
= Pr (Z1 = 1, ζ(D, Z2 , X, V ) ≤ m̄|Z2 , X, T = at)
= Pr (Z1 = 1, ζ(1, Z2 , X, V ) ≤ m̄|Z2 , X, T = at)
= Pr (Z1 = 1|Z2 , X, T = at) × Pr (M ≤ m̄|Z2 , X, T = at)
because of the independence Z1 ⊥
⊥V |Z2 , X, T by Assumption 1.

(24)

Hence, we ﬁnd that

Z1 ⊥⊥M|Z2 , X, T = at.
6.2.2

Cancellation of terms among always-takers

Suppose we apply iterated expectations with respect to Z2 to the conditional expectation term
in the ﬁrst line below (23). We show that after additionally conditioning on Z2 the term is zero,
21

which implies that the expecation is zero for the always-takers. Thus consider
Z1 − Π̄
ω(C, X)|Z2 , X, M = m̄, T = at
1 − Π̄
= E [Y ω(C, X)|Z1 = 1, Z2 , X, M = m̄, T = at] Pr (Z1 = 1|Z2 , X, M = m̄, T = at)
−Π̄
+E Y
ω(C, X)|Z1 = 0, Z2 , X, M = m̄, T = at Pr (Z1 = 0|Z2 , X, M = m̄, T = at)
1 − Π̄
E YD

now using (24) we obtain
= E [Y ω(C, X)|Z1 = 1, Z2 , X, M = m̄, T = at] Pr (Z1 = 1|Z2 , X, T = at)
Π̄
−E Y
ω(C, X)|Z1 = 0, Z2 , X, M = m̄, T = at Pr (Z1 = 0|Z2 , X, T = at)
1 − Π̄
now we use Z1 ⊥⊥T |Z2 , X by Assumption 1
= E [Y ω(C, X)|Z1 = 1, Z2 , X, M = m̄, T = at] Π̄
−E [Y ω(C, X)|Z1 = 0, Z2 , X, M = m̄, T = at] Π̄.
Now we show that the conditional expectation does not depend on Z1 , which thus implies that
the previous expression cancels to zero. To see this note that
E [Y ω(C, X)|Z1 = z1 , Z2 , X, M = m̄, T = at]
= E [ϕ(1, m̄, x, U)ω(x, C (m̄, 1, z2 , x))|ζ(1, z2 , x, V ) = m̄, Z1 = z1 , Z2 = z2 , X = x, T = at]
where we inserted M = ζ(D, Z2 , X, V ) and also noted that C is a function of Ci =
C (Mi , Di , Z2i , Xi )
= E [ϕ(1, m̄, x, U)ω(x, C (m̄, 1, z2 , x))|ζ(1, z2 , x, V ) = m̄, Z2 = z2 , X = x, T = at]
because Z1 ⊥
⊥(U, V )|Z2 , X, T = at. Hence, the previous expression does not depend on Z1 and
the terms in the always-taker population thus cancel out.
6.2.3

Derivation of complier densities

We need to derive the density fM|C,X,Z1 =1,T =co (m̄).
Consider ﬁrst
E 1 (M ≤ m) · D

22

Z1 − Π̄
|C, X .
1 − Π̄

(25)

This term can be written as
= E 1 (M ≤ m) · D

Z1 − Π̄
|C, X, T = co Pr (T = co|C, X)
1 − Π̄
Z1 − Π̄
+ E 1 (M ≤ m) · D
|C, X, T = at Pr (T = at|C, X) .
1 − Π̄

One can show that the term for the always taker is zero, via making use of iterated expectations
with respect to Z2 and using Z1 ⊥
⊥(V, T )|Z2 , X by Assumption 1.
We thus have
Z1 − Π̄
|C, X, T = co Pr (T = co|C, X)
1 − Π̄
= E [1 (M ≤ m) |Z1 = 1, C, X, T = co] Pr (Z1 = 1|C, X, T = co) Pr (T = co|C, X)
= E 1 (M ≤ m) · D

= E [1 (M ≤ m) |Z1 = 1, C, X, T = co] Pr (Z1 = 1|X, T = co) Pr (T = co|C, X)
D Z1 − Π̄
= FM|Z1 =1,C,X,T =co (m) · Pr (Z1 = 1|X, T = co) · E
|C, X
Π̄ 1 − Π̄

(26)
(27)

where we used Z1 ⊥⊥V |X, T and that
Pr (T = co|C, X) = E

D Z1 − Π̄
|C, X .
Π̄ 1 − Π̄

We thus obtain the density function through diﬀerentiation as
fM|Z1 =1,C,X,T =co (m) =

∂
∂m E

1 −Π̄
1 (M ≤ m) · D Z1−
|C, X
Π̄

Pr (Z1 = 1|X, T = co) · E

D Z1 −Π̄
|C, X
Π̄ 1−Π̄

.

(28)

Comment: In the discussion paper we obtain the more concise expression in Theorem 2b
through an explicit expression for the conditional density of M. This is not the case here because
for simplifying (28) we would need to use iterated expectations with respect to Z2 in order to
cancel Π̄, which however would lead to a degenerate distribution function, i.e. after having
conditioned on C, X, Z1 , Z2 , T , and thus not permit direct calculation of the density function. To
see this suppose we take (25) and split into the Z1 = 1 and Z1 = 0 subpopulation to obtain
E 1 (M ≤ m) · D

Z1 − Π̄
|C, X, Z1 = 1 Pr (Z1 = 1|C, X)
1 − Π̄
Z1 − Π̄
+ E 1 (M ≤ m) · D
|C, X, Z1 = 0 Pr (Z1 = 0|C, X)
1 − Π̄

23

= FM|D=1,C,X,Z1 =1 (m) Pr (D = 1, Z1 = 1|C, X)
− E 1 (M ≤ m) · D

Π̄
|C, X, Z1 = 0 Pr (Z1 = 0|C, X) .
1 − Π̄

While the ﬁrst term can be written as a cdf multiplied with some term that does not depend on
m, such that the density can be obtained directly by diﬀerentiation, this is not possible for the
second term because Π̄ also depends on Z2 . On the other hand, using iterated expectations with
respect to Z2 leads to a degenerate distribution function, thereby not permitting diﬀerentiation.
I.e. inserting M = ζ(D, Z2 , X, V ) and Π̄ = Pr(Z1 = 1|Z2 , X) and using iterated expectations
with respect to Z2 one can write
= FM|D=1,C,X,Z1 =1 (m) Pr (D = 1, Z1 = 1|C, X)
−

E [1 (ζ(1, Z2 , X, V ) ≤ m) |D = 1, Z2 , C, X, Z1 = 0] ·

Pr(Z1 = 1|Z2 , X)
Pr(Z1 = 0|Z2 , X)

Pr (D = 1|Z2 , C, X, Z1 = 0) dFZ2 |C,X,Z1 =0 Pr (Z1 = 0|C, X) ,
which does not permit simple diﬀerentiation of the second term with respect to m.

7

Proof of Theorem 3

Theorem 3 did not impose Assumption 2, i.e. it is permitted that Z1 and Z2 may be dependent.

7.1

Preliminaries

In Section 3.2.2 of the main paper we made use of the two equations:
FU |Q,Z1 ,Z2 ,X,T =co (u, q, 1, q, x) = FU |M=m,Z1 =1,Z2 =q,X=x,T =co (u)
FQ|X,T =co (q, x) = 1 − FM|Z1 =1,Z2 ,X,T =co (m, q, x) .

24

and

In the following we show how to derive these equations. First consider
FU |Q,Z1 ,Z2 ,X,T =co (u, q, 1, q, x)
= Pr (U ≤ u|Q = q, Z1 = 1, Z2 = q, X = x, T = co)
= Pr U ≤ u|ζ −1 (1, m, X, V ) = q, Z1 = 1, Z2 = q, X = x, T = co
= Pr (U ≤ u|m = ζ(1, q, X, V ), Z1 = 1, Z2 = q, X = x, T = co)
= Pr (U ≤ u|m = ζ(D, Z2 , X, V ), Z1 = 1, Z2 = q, X = x, T = co)
= Pr (U ≤ u|m = M, Z1 = 1, Z2 = q, X = x, T = co)
= FU |M=m,Z1 =1,Z2 =q,X=x,T =co (u).
Secondly, concerning dFQ|X,T =co , note that conditional on X, the only stochastic component in
Q is V . Because V is independent of Z1 , Z2 conditional on X, so is Q.
Second, consider
FQ|X,T =co (q, x) = FQ|Z1 ,Z2 ,X,T =co (q, 1, q, x)
= Pr (Q ≤ q|Z1 = 1, Z2 = q, X = x, T = co)
= Pr ζ −1 (1, m, X, V ) ≤ q|Z1 = 1, Z2 = q, X = x, T = co
= Pr (m ≤ ζ(1, q, X, V )|Z1 = 1, Z2 = q, X = x, T = co)
= Pr (m ≤ ζ(D, Z2 , X, V )|Z1 = 1, Z2 = q, X = x, T = co)
= Pr (m ≤ M|Z1 = 1, Z2 = q, X = x, T = co)
= 1 − FM|Z1 =1,Z2 ,X,T =co (m, q, x) .

7.2

Main proof

As we have derived in the main text we can write the object of interest as
E Y 1,m̄ |T = co
=

(29)

E [Y |M = m̄, Z1 = 1, Z2 = z2 , X = x, T = co] −

∂FM|Z1 =1,Z2 ,X,T =co (m̄, z2 , x)
fX|T =co (x)dz2 dx.
∂z2

We thus need to ﬁnd an expression that delivers (29).

Consider the following expression
1
Pr (T = co)

E YD

Z1 − Π̄
|Z2 , X, M = m̄ · ω(Z2 , X) · dFZ2 ,X .
1 − Π̄
25

(30)

1 −Π̄
Note that the term E Y D Z1−
|Z2 , X, M = m̄ is zero in the always-taker subpopulation if
Π̄

Z1 ⊥⊥(U, V, T )|Z2 , X.
Therefore the previous expression is equal to
=

1
Pr (T = co)
=
=

E YD

1
Pr (T = co)
1
Pr (T = co)

Z1 − Π̄
|Z2 , X, M = m̄, Z1 = 1, T = co ·Pr (Z1 = 1, T = co|Z2 , X, M = m̄) ω(Z2 , X)·dFZ2 ,X
1 − Π̄

E [Y |Z1 = 1, Z2 , X, M = m̄, T = co] Pr (Z1 = 1, T = co|Z2 , X, M = m̄) · ω(Z2 , X)dFZ2 ,X
E [Y |Z1 = 1, Z2 , X, M = m̄, T = co]

fM|Z2 ,X,Z1 =1,T =co (m̄) Pr (Z1 = 1, T = co|Z2 , X)
ω(Z2 , X)dFZ2 ,X .
fM|Z2 ,X (m̄)

If we deﬁne the weights as
ω(z2 , x) = −

∂FM|Z1 =1,Z2 ,X,T =co (m̄, z2 , x)
∂z2

we obtain the expression (29).

fM|Z2 ,X (m̄)fX|T =co
Pr (T = co)
Pr (Z1 = 1, T = co|Z2 , X) fM|Z2 ,X,Z1 =1,T =co (m̄)fZ2 ,X

By Bayes theorem we can re-write the weights and using

Z1 ⊥⊥T |Z2 , X we obtain
ω(z2 , x) =

∂FM|Z1 =1,Z2 ,X,T =co (m̄, z2 , x)
∂z2
fM|Z2 ,X (m̄, z2 , x)
1
.
fM|Z2 ,X,Z1 =1,T =co (m̄, z2 , x) Π̄fZ2 |X,T =co (z2 , x)
−

(31)

Now we also use that by Bayes theorem and using (34)
fZ2 |X,T =co

Z1 −Π̄
E D
|Z2 , X
Pr (T = co|Z2 , X) fZ2 |X
Π̄ 1−Π̄
=
=
fZ2 |X
Z1 −Π̄
Pr (T = co|X)
E D
|X
Π̄ 1−Π̄

we obtain
ω(z2 , x) =

−

∂FM|Z1 =1,Z2 ,X,T =co (m̄, z2 , x)
∂z2

fM|Z2 ,X (m̄, z2 , x)
1
fM|Z2 ,X,Z1 =1,T =co (m̄, z2 , x) Π̄fZ2 |X (z2 , x) E

E

D Z1 −Π̄
|X
Π̄ 1−Π̄

D Z1 −Π̄
|Z2
Π̄ 1−Π̄

=x
(32)

= z2 , X = x

and inserting the expressions for FM|Z2 ,X,T =co,Z1 =1 and fM|Z2 ,X,T =co,Z1 =1 from (35) and (36) we
obtain as weights
ω(z2 , x) = −

∂
∂z2

E D Z1 − Π̄ |M ≤ m̄, Z2 = z2 , X = x
FM|Z2 ,X (m̄, z2 , x)
E D Z1 − Π̄ |Z2 = z2 , X = x

Z1 −Π̄
E D
|X = x
1
Π̄ 1−Π̄
×
.
fZ2 |X (z2 , x) E D Z1 −Π̄ |M = m̄, Z2 = z2 , X = x
1−Π̄

26

7.3
7.3.1

Additional calculations
Derivation of complier densities

We need to derive FM|Z2 ,X,Z1 =1,T =co and fM|Z2 ,X,Z1 =1,T =co
Start with
E 1 (M ≤ m) · D

Z1 − Π̄
|Z2 , X .
1 − Π̄

This term can be written as
Z1 − Π̄
|Z2 , X, T = co, Z1 = 1 Pr (T = co, Z1 = 1|Z2 , X)
1 − Π̄
Z1 − Π̄
+E 1 (M ≤ m) · D
|Z2 , X, T = at Pr (T = at|Z2 , X) .
1 − Π̄

= E 1 (M ≤ m) · D

One can show that the last row is zero in the always-taker population because Z1 ⊥
⊥(V, T )|Z2 , X.
We thus obtain
= E [1 (M ≤ m) |Z2 , X, T = co, Z1 = 1] Pr (T = co, Z1 = 1|Z2 , X)
= FM|Z2 ,X,T =co,Z1 =1 (m) Pr (T = co|Z2 , X) Pr (Z1 = 1|Z2 , X)
= FM|Z2 ,X,T =co,Z1 =1 (m) · Π̄ · Pr (T = co|Z2 , X) .

Alternatively, we can also write
Z1 − Π̄
|Z2 , X
1 − Π̄
Z1 − Π̄
= E 1 (M ≤ m) · D
|Z2 , X, Z1 = 1 Pr (Z1 = 1|Z2 , X)
1 − Π̄
Z1 − Π̄
+E 1 (M ≤ m) · D
|Z2 , X, Z1 = 0 Pr (Z1 = 0|Z2 , X)
1 − Π̄
E 1 (M ≤ m) · D

= E [1 (M ≤ m) · D|Z2 , X, Z1 = 1] · Π̄ − E [1 (M ≤ m) · D|Z2 , X, Z1 = 0] · Π̄

1 − Π̄
1 − Π̄

= E [1 (M ≤ m) |D = 1, Z2 , X, Z1 = 1] · Pr (D = 1|X, Z1 = 1) · Π̄
− E [1 (M ≤ m) |D = 1, Z2 , X, Z1 = 0] · Pr (D = 1|X, Z1 = 0) · Π̄

= FM|D=1,Z2 ,X,Z1 =1 (m) · Pr (D = 1|Z2 , X, Z1 = 1) · Π̄
− FM|D=1,Z2 ,X,Z1 =0 (m) · Pr (D = 1|Z2 , X, Z1 = 0) · Π̄
27

We thus have obtained
FM|Z2 ,X,T =co,Z1 =1 (m) · Π̄ · Pr (T = co|Z2 , X)

(33)

= FM|D=1,Z2 ,X,Z1 =1 (m) · Pr (D = 1|Z2 , X, Z1 = 1) · Π̄
−FM|D=1,Z2 ,X,Z1 =0 (m) · Pr (D = 1|Z2 , X, Z1 = 0) · Π̄.
Via Bayes theorem and a few calculations we can also re-write the previous expression as
FM|Z2 ,X,T =co,Z1 =1 (m) · Pr (T = co|Z2 , X) = E

D Z1 − Π̄
|M ≤ m, Z2 , X FM|Z2 ,X .
Π̄ 1 − Π̄

Now we also use that by Z1 ⊥
⊥T |Z2 , X
Pr (T = co|Z2 , X) = E

D Z1 − Π̄
|Z2 , X
Π̄ 1 − Π̄

(34)

to obtain
FM|Z2 ,X,T =co,Z1 =1 (m) =

E D Z1 − Π̄ |M ≤ m, Z2 , X
FM|Z2 ,X .
E D Z1 − Π̄ |Z2 , X

(35)

To obtain the density function, we take derivatives with respect to m on the left and right
hand side of (33) and via Bayes theorem and a few calculations we obtain
dFM|Z2 ,X,T =co,Z1 =1 (m) =

E D Z1 − Π̄ |M = m, Z2 , X
dFM|Z2 ,X .
E D Z1 − Π̄ |Z2 , X

(36)

(Hence, as in previous results, in the expression for the cdf we need to condition on M ≤ m
whereas we condition on M = m in the pdf.)

8

Construction of the model for Theorem 4

The initial model for binary M was:
Y

= ϕ(D, M, X, U),

(37)

M = 1 (ζ(D, Z2 , X, V ) ≥ 0 ) ,
D = 1 ( χ(Z1 , X, W ) ≥ 0 ) .
To develop the intuition for identiﬁcation based on monotonicity of M in z2 and v (due to the
lack of identiﬁcation of V ), consider for a moment a simpliﬁed version of model (37), in which X
is dropped (or kept implicit) for notational convenience. The mediator is then given by
M = 1 (ζ(D, Z2 , V ) ≥ 0 ) .
28

Deﬁne ζ −1 to be the inverse function with respect to z2 . Since ζ is monotonically increasing in z2 ,
so is ζ −1 . Applying the inverse function on both sides, the mediator equation can be rewritten as
M = 1 Z2 ≥ ζ −1 (D, 0, V ) .
Furthermore, deﬁne ξ d (v) ≡ ξ(d, v) ≡ ζ −1 (d, 0, v). Note that while ζ −1 is strictly monotonically
increasing in its second argument z2 , it is strictly monotonically decreasing in its third argument
v, see Lemma 3 in the appendix. Therefore, also ξ(d, v) is strictly monotonically decreasing in v.
We may write
M = 1 (ξ(D, V ) ≤ Z2 ) ,

(38)

or alternatively,
M = 1 (ξ D (V ) ≤ Z2 ) ,
where we use the notation ξ d (v) ≡ ξ(d, v), with d indexing the function. The latter is convenient
because D only takes values 0 and 1.
In a next step, we examine
Pr (M = 0|D = d, Z2 = z2 , T = co)
= Pr (ξ(D, V ) > Z2 |D = d, Z2 = z2 , T = co)
= Pr (ξ(d, V ) > z2 |Z1 = d, Z2 = z2 , T = co)
= Pr (ξ d (V ) > z2 |Z1 = d, Z2 = z2 , T = co)
= Pr (ξ d (V ) > z2 |T = co) ,
where we made use of the facts that Z1 = D for compliers and that V ⊥
⊥(Z1 , Z2 )|X, T = co by
Assumption 1. Since ξ d (v) is strictly monotonically decreasing in v, its inverse function ξ −1
d exists
and is also strictly monotonically decreasing in v such that the inverse function can be applied
on both sides (where the inequality sign changes because ξ −1
d is a decreasing function) to obtain
−1
= Pr V ≤ ξ −1
d (z2 ) |T = co = FV |T =co ξ d (z2 ) .

Now, we make conditioning on X explicit again and deﬁne the function µd,x (z2 ) as
µd,x (z2 ) = Pr (M = 0|D = d, Z2 = z2 , X = x, T = co) .

29

(39)

Repeating the previous derivations yields
M = 1 ξ D,X (V ) ≤ Z2
with
µd,x (z2 ) = FV |X=x,T =co ξ −1
d,x (z2 ) .

(40)

Further, µd,x (z2 ) is identiﬁed by Assumptions 1 to 3 as
E [(1 − M)D (Z1 − E [Z1 |X = x]) |Z2 = z2 , X = x]
,
E [D (Z1 − E [Z1 |X = x]) |Z2 = z2 , X = x]
E [(1 − M) (1 − D) (Z1 − E [Z1 |X = x]) |Z2 = z2 , X = x]
µ0,x (z2 ) =
,
E [(1 − D) (Z1 − E [Z1 |X = x]) |Z2 = z2 , X = x]

µ1,x (z2 ) =

which can be shown by similar arguments as before.
Since FV |X,T =co is strictly increasing by Assumption 8 and ξ d,x is monotonic as discussed
above, the relationship in (40) can be inverted. Let µ−1
d,x denote the inverse function of µd,x (z2 ),
i.e. with respect to z2 . (40) implies that µd,x (z2 ) and ξ −1
d,x (z2 ) are both strictly monotonically
decreasing. Using the shortcut notation FV |x,co ≡ FV |X=x,T =co and denoting its inverse function
by FV−1
|x,co , we note the following relationships:
µd,x (z2 ) = FV |x,co ξ −1
d,x (z2 ) ,
−1
ξ −1
d,x (z2 ) = FV |x,co µd,x (z2 ) ,

ξ d,x (v) = µ−1
d,x FV |x,co (v) .
Therefore, the model can be rewritten as
Y

= ϕ(D, M, X, U),

M = 1 µ−1
D,X FV |X,co (V ) ≤ Z2

,

D = 1 ( χ(Z1 , X, W ) ≥ 0 ) ,
−1
where the function µ−1
D,X is identiﬁed and µd,x (v) is strictly monotonically decreasing in v.

9

Proof of Theorem 4

Consider
E

Y DZ1
Π

−

Y D(1−Z1 )
1−Π

E

DZ1
Π

−

Ω {M + (1 − M)}
D(1−Z1 )
1−Π

30

(41)

where the weights are a function of Z2 and X
Ω = ω(z2 , x) =

fZ2 |X=x,T =co µ−1
0,x µ1,x (z2 )
fZ2 |X=x,T =co (z2 )

·

µ′1,x (z2 )
µ′0,x (µ−1
0,x (µ1,x (z2 )))

.

Because the weights Ω only depend on z2 and x, one can show (using similar derivations as
previously) that (41) is zero in the always-taker subpopulation. We thus obtain that (41) equals
E
=E

Y DZ1
{ΩM + Ω(1 − M)} |T = co
Π

Y
{ΩM + Ω(1 − M)} |T = co, Z1 = 1 Pr (Z1 = 1|T = co)
Π

now we insert the model and note that D = 1 in the T = co, Z1 = 1 subpopulation.
ϕ(1, M, X, U)
{ΩM + Ω(1 − M)} dFU,M,V,Z2 |X,T =co,Z1 =1 dFX|T =co,Z1 =1 ·Pr (Z1 = 1|T = co)
π(X)

=

We further note that M is uniquely determined by D, Z2 , X, V such that M is nonstochastic once
we condition on Z2 , X, V and D = 1. We also use dFX|T =co,Z1 =1 =

Pr(Z1 =1|X,T =co)dFX|T =co
Pr(Z1 =1|T =co)

to

obtain
=

ϕ(1, 1, X, U) · Ω · 1 (M = 1) · dFU,V,Z2 |X,T =co,Z1 =1 · dFX|T =co
+

ϕ(1, 0, X, U) · Ω · 1 (M = 0) · dFU,V,Z2 |X,T =co,Z1 =1 · dFX|T =co

ϕ(1, 1, X, U) · Ω · 1 µ−1
1,X (FV |X,co (V )) ≤ Z2 · dFU,V,Z2 |X,T =co,Z1 =1 · dFX|T =co

=
+

ϕ(1, 0, X, U) · Ω · 1 µ−1
1,X (FV |X,co (V )) > Z2 · dFU,V,Z2 |X,T =co,Z1 =1 · dFX|T =co

where we inserted the equation for M. Now by Assumptions 1 and 2 we have (U, V )⊥⊥Z2 |X, T, Z1
and (U, V )⊥
⊥Z1 |X, T and Z2 ⊥
⊥Z1 |X, T such that we can write
dFU,V,Z2 |X,T =co,Z1 =1 = dFU,V |X,T =co · dFZ2 |X,T =co
and thus obtain
ϕ(1, 1, X, U) · dFU|V,X,T =co · Ω · 1 µ−1
1,X (FV |X,co (V )) ≤ Z2 · dFZ2 |X,T =co · dFV,X|T =co

=
+

ϕ(1, 0, X, U) · dFU |V,X,T =co · Ω · 1 µ−1
1,X (FV |X,co (V )) > Z2 · dFZ2 |X,T =co · dFV,X|T =co

31

=

ϕ(1, 1, x, u)dFU |V,X,T =co (u|v, x)

ω(z2 , x) · 1 µ−1
1,x (FV |x,co (v)) ≤ z2 dFZ2 |X,T =co (z2 |x) · dFV,X|T =co (v, x)
=Pr(M=1|V,X,T =co,Z1 =0)

+

ϕ(1, 0, x, u)dFU|V,X,T =co (u|v, x)

ω(z2 , x) · 1 µ−1
1,x (FV |x,co (v)) > z2 dFZ2 |X,T =co (z2 |x) dFV,X|T =co (v, x)
=Pr(M=0|V,X,T =co,Z1 =0)

where the terms in brackets equal Pr (M = 1|V, X, T = co, Z1 = 0) and Pr (M = 0|V, X, T = co, Z1 = 0),
respectively, as shown in (42) and (43). We thus obtain
=

ϕ(1, 1, x, u)dFU |V,X,T =co (u|v, x) Pr (M = 1|V = v, X = x, T = co, Z1 = 0) · dFV,X|T =co (v, x)
+

ϕ(1, 0, x, u)dFU|V,X,T =co (u|v, x) Pr (M = 0|V = v, X = x, T = co, Z1 = 0) dFV,X|T =co (v, x)

=

ϕ(1, 0, X, U)dFU |V,X,T =co · Pr (M = 0|V, X, T = co, Z1 = 0) · dFV,X|T =co
+

ϕ(1, 1, X, U)dFU |V,X,T =co · Pr (M = 1|V, X, T = co, Z1 = 0) · dFV,X|T =co

now using M 0 ⊥
⊥Z1 |V, X, T by Assumption 2
ϕ(1, 0, X, U)dFU|V,X,T =co · Pr M 0 = 0|V, X, T = co · dFV,X|T =co

=
+
=

ϕ(1, 1, X, U)dFU |V,X,T =co · Pr M 0 = 1|V, X, T = co · dFV,X|T =co
ϕ(1, M 0 , X, U) · dFU |V,X,T =co · dFM 0 |V,X,T =co · dFV,X|T =co

now we use that M 0 ⊥⊥U|V, X, T = co by Assumption 1 to obtain
=

ϕ(1, M 0 , X, U) · dFM 0 ,U|V,X,T =co · dFV,X|T =co

=

ϕ(1, M 0 , X, U) · dFM 0 ,X,U,V |T =co
0

= E ϕ(1, M 0 , X, U)|T = co = E Y 1,M |T = co .

9.1

Additional calculations

We need to show
ω(z2 , x) · 1 µ−1
1,x (FV |x,co (v)) > z2 dFZ2 |X,T =co (z2 |x) = Pr (M = 0|V = v, X = x, T = co, Z1 = 0)
(42)

32

and
ω(z2 , x) · 1 µ−1
1,x (FV |x,co (v)) ≤ z2 dFZ2 |X,T =co (z2 |x) = Pr (M = 1|V = v, X = x, T = co, Z1 = 0)
(43)

Consider ﬁrst the left-hand side of (42). Note that the integration is with respect to z2 for
given values of v and x. To ease notation we drop the x subscript in the following, i.e. keep x
implicit in the notation:
ω(z2 ) · 1 µ−1
1 (FV |co (v)) > z2 dFZ2 |T =co (z2 )
z2 <µ−1
1 (FV |co (v))

=

ω(z2 ) · dFZ2 |T =co (z2 )
−∞

now inserting the weights (and keeping x implicit) gives
z2 <µ−1
1 (FV |co (v))

=
−∞

fZ2 µ−1
µ′ (z2 )
0 (µ1 (z2 ))
· ′ −11
· dFZ2 |T =co (z2 ) .
fZ2 (z2 )
µ0 (µ0 (µ1 (z2 )))

−1
Now deﬁne a = µ−1
0 (µ1 (z2 )) and note that µ0 (µ1 (·)) is a monotonously increasing function

(because both µ−1
and µ1 are monotonously decreasing functions). Note further that z2 =
0
µ−1
1 (µ0 (a)) and that
function is given as

−1 ′
dz2
′
da = (µ1 ) (µ0 (a)) · µ0 (a) and by noting that the derivative of the inverse
µ′0 (a)
dz2
1
′
µ−1
1 (a) = µ′ (µ−1 (a)) we have that da = µ′ (µ−1 (µ (a))) . We thus obtain
1

1

1

a<µ−1
0 (FV |co (v))

=

fZ2 |T =co (a) ·
−∞

1

0

µ′1 (µ−1
µ′0 (a)
1 (µ0 (a)))
·
da
−1
−1
′
µ′0 (µ−1
0 (µ1 (µ1 (µ0 (a))))) µ1 µ1 (µ0 (a))

a<µ−1
0 (FV |co (v))

=

fZ2 |T =co (a) da
−∞

= FZ2 |T =co µ−1
0 (FV |co (v) .
Now making the x subscript explicit again
= FZ2 |X=x,T =co µ−1
0,x (FV |x,co (v)

33

now using Z2 ⊥⊥(V, W, Z1 )|X by Assumptions 1 and 2
= FZ2 |V =v,X=x,T =co,Z1 =0 µ−1
0,x (FV |x,co (v)
= FZ2 |V =v,X=x,T =co,Z1 =0 µ−1
0,x (FV |x,co (v)
= Pr Z2 < µ−1
0,x (FV |x,co (v) |V = v, X = x, T = co, Z1 = 0
= Pr µ−1
D,X (FV |X,co (V )) > Z2 |V = v, X = x, T = co, Z1 = 0
= Pr (M = 0|V = v, X = x, T = co, Z1 = 0) ,
which completes the proof of (42).
With similar derivations, one can proof equation (43).

10

Discussion of identiﬁcation based on Theorem 5

Identiﬁcation proceeds in three steps, requiring Assumptions 1, 2, 3, 4, 9, and 10. First, we
identify the function ψ(D, X). Second, we subtract the function ψ(D, X) from Y , and then
identify E[ϕ(1, M 0 , U)|T = co]. Third, we identify E[ψ(1, X)|T = co], and then combine the
0

previous results for the identiﬁcation of E[Y 1,M |T = co].
Consider the following expression for some values m, c, x in the support of M, C, X:
1 −Π
|M = m, C = c, X = x
E Y D Z1−Π

Pr (T = co, Z1 = 1|M = m, C = c, X = x)

.

(44)

As shown further below in this appendix, this term is zero for the always takers, such that we
obtain
= E [Y |T = co, Z1 = 1, M = m, C = c, X = x]
= E [ϕ(1, m, U) + ψ(1, x)|T = co, Z1 = 1, M = m, C = c, X = x]
= E [ϕ(1, m, U)|T = co, Z1 = 1, M = m, C = c, X = x] + ψ(1, x)
= E [ϕ(1, m, U)|T = co, C = c] + ψ(1, x)
= Ξ(m, c) + ψ(1, x)
where the last equations made use of U⊥
⊥M|C, Z1 , X, T = co due to U⊥
⊥Z2 |V, Z1 , X, T = co
and U⊥
⊥Z1 |C, X, T = co, and ﬁnally U⊥⊥X|C, T = co by Assumption 10, and where we deﬁned
Ξ(m, c) ≡ E [ϕ(1, m, U)|T = co, C = c] as an unknown function of m and c only.
34

We can further simplify the denominator of (49) by using an auxiliary result of the proof of
Theorem 1. Combining all results we obtain for some values m, c, x in the support of M, C, X
E [Y D (Z1 − Π) |M = m, C = c, X = x]
= Ξ(m, c) + ψ(1, x).
E [D (Z1 − Π) |M = m, C = c, X = x]

(45)

Consider now two triplets (m, c, x1 ) and (m, c, x0 ) in the support of M, C, X. We obtain
E [Y D (Z1 − Π) |M = m, C = c, X = x1 ] E [Y D (Z1 − Π) |M = m, C = c, X = x0 ]
−
= ψ(1, x1 ).
E [D (Z1 − Π) |M = m, C = c, X = x1 ]
E [D (Z1 − Π) |M = m, C = c, X = x0 ]
(46)
because ψ(1, x0 ) has been normalized to be zero. Similarly, consider two triplets (m′ , c′ , x2 ) and
(m′ , c′ , x1 ) in the support of M, C, X:
E [Y D (Z1 − Π) |M = m′ , C = c′ , X = x2 ] E [Y D (Z1 − Π) |M = m′ , C = c′ , X = x1 ]
−
= ψ(1, x2 )−ψ(1, x1 ),
E [D (Z1 − Π) |M = m′ , C = c′ , X = x2 ]
E [D (Z1 − Π) |M = m′ , C = c′ , X = x1 ]
and as ψ(1, x1 ) is identiﬁed by (46), so is ψ(1, x2 ). We can thus identify ψ(1, x1 ), ψ(1, x2 ), and so
forth. However, identiﬁcation of the entire function ψ(1, x) for all x in the support of X requires
further conditions as we need to ﬁnd triplets with identical m and c, but x2 = x1 . Since M is a
function of Z2 , X, V among compliers, it is only through variation of z2 that identical values of
m and c for diﬀerent x may be obtained.
If ψ is a parametric function of, say, a k-dimensional parameter vector β, it generally suﬃces
to identify ψ(1, x) ≡ ψ1 (x; β) for k diﬀerent values of x. In practice, one may for instance consider
the following regression approach. Let Ŷi be a (non-parametric) estimate of the left-hand side of
(49). We estimate the model
Ŷi = Ξ(Mi , Ci ) + ψ 1 (Xi ; β) + ǫi ,

(47)

with ǫi being the regression error, Ξ an unknown two-dimensional nonparametric function, and
ψ1 (x; β) a parametric function, using partially linear semiparametric regression.
On the other hand, if ψ is a non-parametric function, identiﬁcation conditions are more
diﬃcult to characterize. There exist diﬀerent scenarios that guarantee identiﬁcation. One possible
scenario is discussed in Lemma 2 of this appendix.
In the following, we suppose that the function ψ(1, x) is identiﬁed, without specifying exactly
how (e.g. via a parametric approach or by Lemma 2 or by some alternative assumptions). Based
on ψ(1, x), we can then also identify E[ϕ(1, M 0 , U)|T = co]. To show this, we deﬁne the weights
Ω = ω(M, C) =

dFM,C|Z1 =0,T =co
dFM,C|Z1 =1,T =co

35

and consider the following expression, where we insert the outcome equation
1 −Π
E (Y − ψ(1, X)) · Ω · D Z1−Π

Pr (T = co) Pr (Z1 = 1)

=

1 −Π
E ϕ(D, M, U) · Ω · D Z1−Π

Pr (T = co) Pr (Z1 = 1)

= E ϕ(1, M 0 , U)|T = co

where the last expression is derived further below in this appendix. We thus have identiﬁed
E ϕ(1, M 0 , U)|T = co .
Finally, we need E[ψ(1, X)|T = co]. By iterated expectations and using Z1 ⊥
⊥T |X we obtain
E[ψ(1, X)|T = co] =

1
D Z1 − Π
E ψ(1, X) ·
=
Pr (T = co)
Π 1−Π

ψ(1, X)dFX|T =co .

All these results are combined in Theorem 5, see the subsequent proof.

11

Proof of Theorem 5

11.1

Part 1: Identiﬁcation of ψ(D, X)

Below we will make use of
Pr (Z1 = 1|M, C, X, T = at) = Pr (Z1 = 1|X) .

(48)

This result follows, ﬁrst because Z1 ⊥
⊥M|C, X, T = at because of the independence Z1 ⊥⊥Z2 |V, X, T =
at by Assumptions 1 and 2, and second because Z1 ⊥
⊥(V, T )|X.
Now, we need to show that the following expression is zero in the always-taker subpopulation.
Consider
E YD

Z1 − Π
|M, C, X
1−Π

(49)

which we can write as a weighted average across always-takers and compliers. Here we only show
that the term for always-takers is zero:
E YD

Z1 − Π
|M, C, X, T = at
1−Π

Z1 − Pr (Z1 = 1|X)
|Z1 = 1, M, C, X, T = at Pr (Z1 = 1|M, C, X, T = at)
1 − Pr (Z1 = 1|X)
Z1 − Pr (Z1 = 1|X)
+E Y
|Z1 = 0, M, C, X, T = at Pr (Z1 = 0|M, C, X, T = at)
1 − Pr (Z1 = 1|X)

= E Y

and inserting (48) we obtain
= {E [Y |Z1 = 1, M, C, X, T = at] − E [Y |Z1 = 0, M, C, X, T = at]} · Pr (Z1 = 1|X) .
36

This expression is zero if
(Z2 , U)⊥
⊥Z1 |V, X, T = at
which is satisﬁed by Assumption 1.

Part 2: Identiﬁcation of E [ϕ(1, M 0 , U )|T = co]

11.2

In Section 11.3.2 we show that the weights can be expressed equivalently as
Ω = ω(M, C) =

dFM,C|Z1 =0,T =co
E [(D − 1) {Z1 − Pr (Z1 = 1)} |M, C]
=
.
dFM,C|Z1 =1,T =co
E [D {Z1 − Pr (Z1 = 1)} |M, C]

Now, consider the expression
Z1 − Π
1−Π
Z1 − Π
= E ϕ(D, M, U) · Ω · D
1−Π
E (Y − ψ(1, X)) · Ω · D

Z1 − Π
|T = at Pr (T = at)
1−Π
Z1 − Π
+ E ϕ(D, M, U) · Ω · D
|T = co, Z1 = 1 Pr (T = co, Z1 = 1) .
1−Π

= E ϕ(D, M, U) · Ω · D

The ﬁrst term is zero as shown in Section 11.3.1. We thus obtain
= E [ϕ(1, M, U)ω(M, C)|T = co, Z1 = 1] · Pr (T = co, Z1 = 1)
=

E [ϕ(1, M, U)|M, C, T = co, Z1 = 1] ω(M, C)dFM,C|T =co,Z1 =1 Pr (T = co, Z1 = 1)

and inserting the weights ω(M, C) =
=
=

dFM,C|Z1 =0,T =co
dFM,C|Z1 =1,T =co

we obtain

E [ϕ(1, M, U)|M, C, T = co, Z1 = 1] dFM,C|Z1 =0,T =co · Pr (T = co, Z1 = 1)
ϕ(1, m, u)dFU|M,C,T =co,Z1 =1 (u, m, c) dFM,C|Z1 =0,T =co (m, c) · Pr (T = co, Z1 = 1)

now using U⊥
⊥(M, Z1 )|C, T = co which is implied by U⊥
⊥(Z1 , Z2 , X)|V, T = co we obtain
=

ϕ(1, m, u)dFU |C,T =co (u, c) dFM,C|Z1 =0,T =co (m, c) · Pr (T = co, Z1 = 1) .

37

We note that FM,C|Z1 =0,T =co = FM 0 ,C|Z1 =0,T =co = FM 0 ,C|T =co if Z1 ⊥
⊥(M 0 , C)|T = co which is
implied by Z1 ⊥
⊥(Z2 , X, V )|T = co, and obtain
=

ϕ(1, m, u)dFU|C,T =co (u, c) dFM 0 ,C|T =co (m, c) · Pr (T = co, Z1 = 1)

=

ϕ(1, m, u)dFU|C,T =co (u, c) dFM 0 |C,T =co (m, c)dFC|T =co (c) · Pr (T = co, Z1 = 1) .

Now using U⊥
⊥M 0 |C, T = co we obtain
=

ϕ(1, m, u)dFU,M 0 |C,T =co (u, m, c)dFC|T =co (c) · Pr (T = co, Z1 = 1)

= E ϕ(1, M 0 , U)|T = co · Pr (T = co, Z1 = 1)

11.3
11.3.1

Additional calculations
Cancellation of term for always-takers

We need to show that
E ϕ(D, M, U) · Ω · D

Z1 − Π
|T = at
1−Π

is zero. Apply iterated expectations with respect to X and inspect the inner term
E ϕ(D, M, U) · Ω · D

Z1 − Π
|X, T = at
1−Π

Z1 − Π
|Z1 = 1, X, T = at Pr (Z1 = 1|X, T = at)
1−Π
Z1 − Π
+E ϕ(D, M, U) · ω(M, C) · D
|Z1 = 0, X, T = at Pr (Z1 = 0|X, T = at)
1−Π

= E ϕ(D, M, U) · ω(M, C) · D

and with Z1 ⊥
⊥T |X we obtain
= E [ϕ(1, M, U) · Ω|Z1 = 1, X, T = at] Pr (Z1 = 1|X)
− E [ϕ(1, M, U) · Ω|Z1 = 0, X, T = at] Pr (Z1 = 1|X) .
This expression is zero if (M, U, C)⊥
⊥Z1 |X, T = at which is implied by Z1 ⊥
⊥ (Z2 , V, U) |X, T = at.
11.3.2

Densities for the compliers

(A) Calculation of fM,C|Z1 =1,T =co

38

Consider the following expression
FM,C|D=1,Z1 =1 (m, c) · Pr (D = 1|Z1 = 1) − FM,C|D=1,Z1 =0 (m, c) · Pr (D = 1|Z1 = 0)
= E [1 (M ≤ m, C ≤ c) |D = 1, Z1 = 1] · Pr (D = 1|Z1 = 1)
−E [1 (M ≤ m, C ≤ c) |D = 1, Z1 = 0] · Pr (D = 1|Z1 = 0)
= E [1 (M ≤ m, C ≤ c) D|Z1 = 1] − E [1 (M ≤ m, C ≤ c) D|Z1 = 0]
and partition by type and noting that terms cancel for the always-takers because Z1 ⊥
⊥(Z2 , X, V )|T =
at and Z1 ⊥
⊥T to obtain:
= E [1 (M ≤ m, C ≤ c) |Z1 = 1, T = co] · Pr (T = co)
= FM,C|Z1 =1,T =co (m, c) · Pr (T = co) .
From this we obtain the density function by diﬀerentiation on the left and right hand side:
dFM,C|D=1,Z1 =1 (m, c) · Pr (D = 1|Z1 = 1) − dFM,C|D=1,Z1 =0 (m, c) · Pr (D = 1|Z1 = 0)
= dFM,C|Z1 =1,T =co (m, c) · Pr (T = co) .
By some calculations, making use of Bayes law, we obtain
dFM,C|Z1 =1,T =co ·Pr (T = co) =

Pr (D = 1, Z1 = 1|M, C)
Pr (D = 1, Z1 = 0|M, C)
dFM,C −
dFM,C
Pr (Z1 = 1)
Pr (Z1 = 0)

and thus
dFM,C|Z1 =1,T =co = E [D {Z1 − Pr (Z1 = 1)} |M, C]

dFM,C
1
.
Pr (Z1 = 1) Pr (Z1 = 0) Pr (T = co)

(B) Calculation of fM,C|Z1 =0,T =co
Consider the expression
FM,C|D=0,Z1 =1 (m, c) Pr (D = 0|Z1 = 1) − FM,C|D=0,Z1 =0 (m, c) Pr (D = 0|Z1 = 0)
= E [1 (M ≤ m, C ≤ c) (1 − D) |Z1 = 1] − E [1 (M ≤ m, C ≤ c) (1 − D) |Z1 = 0]
and partition by type to obtain:
= −E [1 (M ≤ m, C ≤ c) |Z1 = 0, T = co] · Pr (T = co)
= −FM,C|Z1 =0,T =co (m, c) · Pr (T = co) .
39

From this we obtain the density function by diﬀerentiation on the left and right hand side:
dFM,C|D=0,Z1 =1 (m, c) Pr (D = 0|Z1 = 1) − dFM,C|D=0,Z1 =0 (m, c) Pr (D = 0|Z1 = 0)
= −dFM,C|Z1 =0,T =co (m, c) · Pr (T = co)
and after simpliﬁcations we obtain
dFM,C|Z1 =0,T =co = E [(D − 1) {Z1 − Pr (Z1 = 1)} |M, C]

dFM,C
1
.
Pr (Z1 = 1) Pr (Z1 = 0) Pr (T = co)

(C) Calculation of the density ratio
Combining the previous results we obtain the density ratio
ω(M, C) =

12

dFM,C|Z1 =0,T =co
E [(D − 1) {Z1 − Pr (Z1 = 1)} |M, C]
=
dFM,C|Z1 =1,T =co
E [D {Z1 − Pr (Z1 = 1)} |M, C]
E [Z1 |M, C] − E [Z1 ]
=1−
.
E [DZ1 |M, C] − E [D|M, C] · E [Z1 ]

Controlled direct eﬀects with discrete mediator: Discussion
of Theorem 6

In the main text in Section 3.4 of the main text we brieﬂy discussed identiﬁcation of the controlled
eﬀect for discrete mediator. The details are derived in the following. Consider the model
Y

= ϕ(D, M, X, U),

(50)

M = 1 (ζ(D, Z2 , X, V ) ≥ 0 ) ,
D = 1 ( χ(Z1 , X, W ) ≥ 0 ) ,
and impose Assumption 7, assuming that ζ is monotonically increasing in z2 . Our object of interest is E Y 1,m |T = co for m ∈ {0, 1}. Identiﬁcation requires some support condition similar
to Assumption 6, with the density function being replaced by a probability because M is now assumed to be discrete. For the identiﬁcation of E Y 1,0 |T = co , the following support assumption
is needed:
Assumption 6’
Pr (M = 0|X, V, Z1 = 1, T = co) > 0
40

a.s.

(51)

To see how identiﬁcation is achieved, consider expression (51), which is equivalent to requiring
Pr (ζ(1, Z2 , X, V ) < 0 |X, V, Z1 = 1, T = co) > 0

a.s.

or
Pr Z2 < ζ −1 (1, 0, X, V ) |X, V, Z1 = 1, T = co > 0

a.s.,

where ζ −1 is the inverse function with respect to z2 . Hence, for almost every v and x there exists
a value zv,x ≡ ζ −1 (1, 0, x, v), such that M takes the value 0 for every Z2 <zv,x . By Assumption
¯
¯
6’, these values of Z2 have positive probability mass. Now, for a given x, consider the minimum
of these values zv,x in the support of V :
¯
z ≡
inf
z ≡
inf
ζ −1 (1, 0, x, v).
¯x v∈Supp(V |X=x)¯v,x v∈Supp(V |X=x)

(52)

Consider a value z̃ which satisﬁes z̃ <zx , for a given x. The previous considerations imply
¯
Pr (M = 0|X = x, Z2 = z̃, Z1 = 1, T = co) = 1,

(53)

whily by Assumption 6’, such values z̃ <zx exist with positive density. Hence, by only using
¯
those observations i with Z2i <zXi , where zXi is deﬁned by (52) for x taking the value of the
¯
¯
observed Xi , there is no endogeneity problem. On the other hand, for observations with Z2i ≥zXi ,
¯
observing Mi = 0 implies a dependence between Vi and Z2i which would lead to an improper
weighting of Ui . This idea is exploited in the following theorem:
Theorem 6: Under Assumptions 1, 3, 6’, 7
E Y 1,0 |T = co =

Pr (Z2 < zX )
Z1 − Π̄
· Ω10 |Z2 < zX ,
¯ E YD
¯
Pr (T = co)
1 − Π̄

with weights
E
Ω10 = ω 10 (X) =

D Z1 −Π̄
|X
Π̄ 1−Π̄

1 −Π̄
E 1 (Z2 < zX ) D Z1−
|X
Π̄
¯

,

where zX is given by (52).
¯
Concerning the ﬁrst result of Theorem 6, only the outcome information of observations satisfying Z2i <zXi is used. In practice, zx is unknown and needs to be estimated. According to (53),
¯
¯
41

the largest value satisfying that the probability of observing M = 0 is one (or very close to one)
for all values of Z2 below this threshold should be chosen for zx . Note that the left hand side of
¯
(53), which is a conditional probability among compliers, is identiﬁed as
Pr (M = 0|X, Z2 , Z1 = 1, T = co) =
=

1 −Π̄
|X, Z2
E (1 − M) D Z1−
Π̄

Pr (T = co, Z1 = 1|X, Z2 )
1 −Π̄
E (1 − M) D Z1−
|X, Z2
Π̄

Π̄E
=

D Z1 −Π̄
|X, Z2
Π̄ 1−Π̄

E (1 − M) D Z1 − Π̄ |X, Z2
,
E D Z1 − Π̄ |X, Z2

because Z1 ⊥
⊥T |X, Z2 and V ⊥
⊥Z1 |X, Z2 , T = at and Pr (T = co|X, Z2 ) = E

D Z1 −Π̄
|X, Z2
Π̄ 1−Π̄

. With

this relationship, Pr (M = 0|X, Z2 , Z1 = 1, T = co) is estimable and one may ﬁnd those values of
z2 where this probability is one (or close to one) in order to estimate zx .
¯

13

Proof of Theorem 6

In the following we suppose that the event Z2 <zX has positive probability, i.e. Pr (Z2 < zX ) > 0.
¯
¯
As we show in Section 13.1, we can write the weights
E
Ω = ω(X) =

D Z1 −Π̄
|X
Π̄ 1−Π̄

1 −Π̄
E 1 (Z2 < zX ) D Z1−
|X
Π̄
¯

=

1
Pr (Z2 < zX , Z1 = 1|X, T = co)
¯

.

Now consider
Pr (Z2 < zX )
Z1 − Π̄
· Ω|Z2 < zX
¯ E YD
Pr (T = co)
¯
1 − Π̄
=

Pr (Z2 < zX )
Z1 − Π̄
¯ E YD
· Ω|Z2 < zX , T = at Pr (T = at|Z2 < zX )
Pr (T = co)
¯
¯
1 − Π̄
Pr (Z2 < zX )
Z1 − Π̄
· Ω|Z2 < zX , T = co, Z1 = 1 Pr (T = co, Z1 = 1|Z2 < zX ) .
+
¯ E YD
¯
¯
Pr (T = co)
1 − Π̄

Analogously to previous sections, one can show that ﬁrst term is zero via using iterations expectations with respect to X and Z2 and using Z1 ⊥
⊥T |Z2 , X and (U, V )⊥⊥Z1 |Z2 , X, T = at as implied
by Assumption 1, and noting that the weights Ω = ω(X) are a function of X only. Hence
Pr (Z2 < zX )
= E [Y · Ω|Z2 < zX , T = co, Z1 = 1] Pr (T = co, Z1 = 1|Z2 < zX )
¯
¯
¯
Pr (T = co)
Pr (Z2 < zX )
= E [ϕ(1, 0, X, U) · ω(X)|Z2 < zX , T = co, Z1 = 1] Pr (T = co, Z1 = 1|Z2 < zX )
¯
¯
¯
Pr (T = co)
42

because Z2 <zX implies M = 0 with certainty.
¯
=

Pr (Z2 < zX )
ϕ(1, 0, X, U)ω(X)dFX,U |Z2 <zX ,T =co,Z1 =1 Pr (T = co, Z1 = 1|Z2 < zX )
¯
¯
Pr (T = co)
¯

=

Pr (Z2 < zX )
ϕ(1, 0, X, U)ω(X)dFU |X,Z2 <zX ,T =co,Z1 =1 dFX|Z2 <zX ,T =co,Z1 =1 Pr (T = co, Z1 = 1|Z2 < zX )
¯ .
¯
Pr (T = co)
¯
¯

Now we use U⊥
⊥(Z1 , Z2 )|X, T = co by Assumption 1
=

Pr (Z2 < zX )
ϕ(1, 0, X, U)ω(X)dFU |X,T =co dFX|Z2 <zX ,T =co,Z1 =1 Pr (T = co, Z1 = 1|Z2 < zX )
¯
¯
Pr (T = co)
¯

and by Bayes theorem using dFX|Z2 <zX ,T =co,Z1 =1 =
¯

=

Pr(Z2 <z X ,Z1 =1|X,T =co)dFX|T =co
¯
Pr(Z2 <zX ,Z1 =1|T =co)
¯

we obtain

Pr (Z2 < zX , Z1 = 1|X, T = co) dFX|T =co
¯
Pr (Z2 < zX , Z1 = 1|T = co)
¯
Pr (Z2 < zX )
Pr (T = co, Z1 = 1|Z2 < zX )
¯
¯
Pr (T = co)
ϕ(1, 0, X, U)ω(X)dFU |X,T =co

now entering the weights ω(X) we obtain

13.1

Pr (T = co, Z1 = 1|Z2 < zX ) Pr (Z2 < zX )
¯
¯
Pr (Z2 < zX , Z1 = 1|T = co) Pr (T = co)
¯
= E [ϕ(1, 0, X, U)|T = co] = E Y 1,0 |T = co .

=

ϕ(1, 0, X, U)dFU |X,T =co dFX|T =co

=

ϕ(1, 0, X, U)dFU,X|T =co

Additional calculations

Consider the expression
Z1 − Π̄
|X
E 1 (Z2 < zX ) D
¯
1 − Π̄
Z1 − Π̄
= E 1 (Z2 < zX ) D
|X, T = at Pr (T = at|X)
¯
1 − Π̄
Z1 − Π̄
+E 1 (Z2 < zX ) D
|X, T = co, Z1 = 1 Pr (T = co, Z1 = 1|X) .
¯
1 − Π̄
Similar to previous results one can show that the term for the always-takers is zero via iterated
expectations with respect to Z2 and using Z1 ⊥
⊥T |Z2 , X. We thus obtain
= Pr (Z2 < zX |X, T = co, Z1 = 1) Pr (T = co, Z1 = 1|X)
¯
= Pr (Z2 < zX , T = co, Z1 = 1|X)
¯
= Pr (Z2 < zX , Z1 = 1|X, T = co) Pr (T = co|X) .
¯
43

We also note that by similar derivations the fraction of compliers is identiﬁed as
D Z1 − Π̄
|X .
Π̄ 1 − Π̄

Pr (T = co|X) = E
Putting these results together we obtain:
Pr (Z2 < zX , Z1 = 1|X, T = co) =
¯
=

14

1 −Π̄
E 1 (Z2 < zX ) D Z1−
|X
Π̄
¯
Pr (T = co|X)
1 −Π̄
|X
E 1 (Z2 < zX ) D Z1−
Π̄
¯
Z1 −Π̄
E D
|X
Π̄ 1−Π̄

Descriptive statistics for the empirical illustrations
Table 1: Descriptive statistics for the application based on Theorem 1

D=1

D=0

M > 12, 000 GBP

M ≤ 12, 000 GBP

mean

std.dev

mean

std.dev

mean

std.dev

mean

std.dev

gender (binary)

0.544

0.498

0.577

0.494

0.349

0.477

0.760

0.427

Scotland (binary)

0.045

0.208

0.059

0.237

0.044

0.204

0.053

0.225

social functioning (Y )

8.155

1.742

7.795

2.141

8.297

1.577

7.847

2.059

school leaving age 16 years (Z1 ) (binary)

0.559

0.497

0.453

0.498

0.544

0.498

0.528

0.499

windfall income in 1000 GBP (Z2 )

0.457

3.548

0.453

4.976

0.654

4.399

0.252

3.314

# of observations

2671

757

1742

1686

The ﬁrst application in the main text is based on data from the British Household Panel
Survey (BHPS) as an illustration for estimation using the results of Theorem 1. Table 1 presents
descriptive statistics (means and standard deviations) for the evaluation sample containing n =
3428 observations. The descriptives are provided for the covariates X (gender and a regional
dummy for Scotland), the outcome Y (social functioning), and the instruments Z1 (one if school
leaving age is 16 years) and Z2 (windfall income), separately by treatment D (one if more than
lower secondary education) and by an indicator for the mediator M (annual individual income)
being larger than 12,000 GBP.
The second application in the main text is based on data from the U.S. Job Corps
experimental study as an illustration for estimation using the results of Theorem 5. Table
44

2 presents descriptive statistics (means and standard deviations) for the evaluation sample
containing n = 4, 603 observations.

The descriptives are provided for the covariates X

(education, race, age, labor market state and school attendance prior to randomization, and
dependence on AFDC or foodstamps), the outcome Y (weekly earnings), and the instruments
Z1 (program randomization) and Z2 (number of children), separately by treatment D (program
participation) and mediator M (hours worked). It is noteworthy that the means of several
covariates diﬀer importantly across D and M. Education, age, and going to school in the year
before randomization have statistically signiﬁcant correlations (at the 5% level) with both the
treatment and the mediator, whereas ethnicity, having a job in year before randomization, and
the receipt of food stamps/AFDC are signiﬁcantly correlated with the mediator only.
Table 2: Descriptive statistics for the application based on Theorem 5

D=1

D=0

M ≥ 35

M < 35

mean

std.dev

mean

std.dev

mean

std.dev

mean

std.dev

high school degree at rand. (binary)

0.226

0.418

0.253

0.435

0.324

0.468

0.214

0.410

at least some college at rand. (binary)

0.032

0.176

0.034

0.182

0.056

0.230

0.026

0.158

black (binary)

0.541

0.498

0.535

0.499

0.489

0.500

0.553

0.497

Hispanic (binary)

0.195

0.396

0.180

0.384

0.189

0.391

0.186

0.389

age

18.489

2.180

18.666

2.168

19.070

2.221

18.427

2.136

in school in year before rand. (binary)

0.661

0.473

0.626

0.484

0.607

0.489

0.653

0.476

had a job in year before rand. (binary)

0.615

0.487

0.631

0.483

0.754

0.431

0.581

0.493

AFDC before randomization (binary)

0.413

0.492

0.431

0.495

0.373

0.484

0.439

0.496

food stamps before randomization (binary)

0.538

0.499

0.559

0.497

0.497

0.500

0.567

0.496

year after rand. (Y )

143.3

134.4

134.9

143.2

312.4

134.7

81.6

81.6

assignment to Job Corps (Z1 ) (binary)

0.992

0.088

0.361

0.481

0.668

0.471

0.638

0.481

# of kids < 6 in 3rd year after rand. (Z2 )

0.765

0.898

0.772

0.900

0.617

0.819

0.819

0.918

1.140

1.248

1.163

1.267

0.907

1.117

1.233

1.292

weekly earnings 3

# kids < 15 in 3

rd

rd

year after rand. (Z2 )
# of observations

2,074

45

2,529

1,139

3,464

