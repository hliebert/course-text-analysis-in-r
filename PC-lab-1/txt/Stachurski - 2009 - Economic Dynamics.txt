Economic Dynamics

Economic Dynamics
Theory and Computation

John Stachurski

The MIT Press
Cambridge, Massachusetts
London, England

c 2009 Massachusetts Institute of Technology


All rights reserved. No part of this book may be reproduced in any form by any electronic or
mechanical means (including photocopying, recording, or information storage and retrieval)
without permission in writing from the publisher.
MIT Press books may be purchased at special quantity discounts for business or sales
promotional use. For information, please email special_sales@mitpress.mit.edu or write to
Special Sales Department, The MIT Press, 55 Hayward Street, Cambridge, MA 02142.
This book was typeset in LATEX by the author and was printed and bound in the United States
of America.
Library of Congress Cataloging-in-Publication Data
Stachurski, John, 1969–
Economic dynamics : theory and computation / John Stachurski.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-262-01277-5 (hbk. : alk. paper) 1. Statics and dynamics (Social sciences)—
Mathematical models. 2.
Economics—Mathematical models. I. Title.
HB145.S73 2009
330.1’519—dc22
2008035973
10 9 8 7 6 5 4 3 2 1

To Cleo

Contents
Preface

xiii

Common Symbols

xvii

1

Introduction

1

I

Introduction to Dynamics

9

2

Introduction to Programming
2.1 Basic Techniques
2.1.1 Algorithms
2.1.2 Coding: First Steps
2.1.3 Modules and Scripts
2.1.4 Flow Control
2.2 Program Design
2.2.1 User-Deﬁned Functions
2.2.2 More Data Types
2.2.3 Object-Oriented Programming
2.3 Commentary

11
11
11
14
19
21
25
25
27
29
33

3

Analysis in Metric Space
3.1 A First Look at Metric Space
3.1.1 Distances and Norms
3.1.2 Sequences
3.1.3 Open Sets, Closed Sets
3.2 Further Properties
3.2.1 Completeness
3.2.2 Compactness

35
35
36
38
41
44
44
46
vii

viii

Contents

3.3

3.2.3 Optimization, Equivalence
3.2.4 Fixed Points
Commentary

48
51
54

4

Introduction to Dynamics
4.1 Deterministic Dynamical Systems
4.1.1 The Basic Model
4.1.2 Global Stability
4.1.3 Chaotic Dynamic Systems
4.1.4 Equivalent Dynamics and Linearization
4.2 Finite State Markov Chains
4.2.1 Deﬁnition
4.2.2 Marginal Distributions
4.2.3 Other Identities
4.2.4 Constructing Joint Distributions
4.3 Stability of Finite State MCs
4.3.1 Stationary Distributions
4.3.2 The Dobrushin Coefﬁcient
4.3.3 Stability
4.3.4 The Law of Large Numbers
4.4 Commentary

55
55
55
59
62
66
68
68
72
76
80
83
83
88
90
93
96

5

Further Topics for Finite MCs
5.1 Optimization
5.1.1 Outline of the Problem
5.1.2 Value Iteration
5.1.3 Policy Iteration
5.2 MCs and SRSs
5.2.1 From MCs to SRSs
5.2.2 Application: Equilibrium Selection
5.2.3 The Coupling Method
5.3 Commentary

99
99
99
102
105
107
107
110
112
116

6

Inﬁnite State Space
6.1 First Steps
6.1.1 Basic Models and Simulation
6.1.2 Distribution Dynamics
6.1.3 Density Dynamics
6.1.4 Stationary Densities: First Pass
6.2 Optimal Growth, Inﬁnite State

117
117
117
122
125
129
133

Contents

6.3

6.4

II

6.2.1 Optimization
6.2.2 Fitted Value Iteration
6.2.3 Policy Iteration
Stochastic Speculative Price
6.3.1 The Model
6.3.2 Numerical Solution
6.3.3 Equilibria and Optima
Commentary

Advanced Techniques

ix
133
135
142
145
145
150
154
156

157

7

Integration
7.1 Measure Theory
7.1.1 Lebesgue Measure
7.1.2 Measurable Spaces
7.1.3 General Measures and Probabilities
7.1.4 Existence of Measures
7.2 Deﬁnition of the Integral
7.2.1 Integrating Simple Functions
7.2.2 Measurable Functions
7.2.3 Integrating Measurable Functions
7.3 Properties of the Integral
7.3.1 Basic Properties
7.3.2 Finishing Touches
7.3.3 The Space L1
7.4 Commentary

159
159
159
163
166
168
171
171
173
177
178
179
180
183
186

8

Density Markov Chains
8.1 Outline
8.1.1 Stochastic Density Kernels
8.1.2 Connection with SRSs
8.1.3 The Markov Operator
8.2 Stability
8.2.1 The Big Picture
8.2.2 Dobrushin Revisited
8.2.3 Drift Conditions
8.2.4 Applications
8.3 Commentary

187
187
187
189
195
197
197
201
204
207
210

x
9

Contents
Measure-Theoretic Probability
9.1 Random Variables
9.1.1 Basic Deﬁnitions
9.1.2 Independence
9.1.3 Back to Densities
9.2 General State Markov Chains
9.2.1 Stochastic Kernels
9.2.2 The Fundamental Recursion, Again
9.2.3 Expectations
9.3 Commentary

211
211
211
215
216
218
218
223
225
227

10 Stochastic Dynamic Programming
10.1 Theory
10.1.1 Statement of the Problem
10.1.2 Optimality
10.1.3 Proofs
10.2 Numerical Methods
10.2.1 Value Iteration
10.2.2 Policy Iteration
10.2.3 Fitted Value Iteration
10.3 Commentary

229
229
229
231
235
238
238
241
244
246

11 Stochastic Dynamics
11.1 Notions of Convergence
11.1.1 Convergence of Sample Paths
11.1.2 Strong Convergence of Measures
11.1.3 Weak Convergence of Measures
11.2 Stability: Analytical Methods
11.2.1 Stationary Distributions
11.2.2 Testing for Existence
11.2.3 The Dobrushin Coefﬁcient, Measure Case
11.2.4 Application: Credit-Constrained Growth
11.3 Stability: Probabilistic Methods
11.3.1 Coupling with Regeneration
11.3.2 Coupling and the Dobrushin Coefﬁcient
11.3.3 Stability via Monotonicity
11.3.4 More on Monotonicity
11.3.5 Further Stability Theory
11.4 Commentary

247
247
247
252
254
257
257
260
263
266
271
272
276
279
283
288
293

Contents
12 More Stochastic Dynamic Programming
12.1 Monotonicity and Concavity
12.1.1 Monotonicity
12.1.2 Concavity and Differentiability
12.1.3 Optimal Growth Dynamics
12.2 Unbounded Rewards
12.2.1 Weighted Supremum Norms
12.2.2 Results and Applications
12.2.3 Proofs
12.3 Commentary

III

Appendixes

xi
295
295
295
299
302
306
306
308
311
312

315

A Real Analysis
A.1 The Nuts and Bolts
A.1.1 Sets and Logic
A.1.2 Functions
A.1.3 Basic Probability
A.2 The Real Numbers
A.2.1 Real Sequences
A.2.2 Max, Min, Sup, and Inf
A.2.3 Functions of a Real Variable

317
317
317
320
324
327
327
331
334

B Chapter Appendixes
B.1 Appendix to Chapter 3
B.2 Appendix to Chapter 4
B.3 Appendix to Chapter 6
B.4 Appendix to Chapter 8
B.5 Appendix to Chapter 10
B.6 Appendix to Chapter 11
B.7 Appendix to Chapter 12

339
339
342
344
345
347
349
350

Bibliography

357

Index

367

Preface
The aim of this book is to teach topics in economic dynamics such as simulation, stability theory, and dynamic programming. The focus is primarily on stochastic systems
in discrete time. Most of the models we meet will be nonlinear, and the emphasis is
on getting to grips with nonlinear systems in their original form, rather than using
crude approximation techniques such as linearization. As we travel down this path,
we will delve into a variety of related ﬁelds, including ﬁxed point theory, laws of large
numbers, function approximation, and coupling.
In writing the book I had two main goals. First, the material would present the
modern theory of economic dynamics in a rigorous way. I wished to show that sound
understanding of the mathematical concepts leads to effective algorithms for solving
real world problems. The other goal was that the book should be easy and enjoyable to read, with an emphasis on building intuition. Hence the material is driven
by examples—I believe the fastest way to grasp a new concept is through studying
examples—and makes extensive use of programming to illustrate ideas. Running
simulations and computing equilibria helps bring abstract concepts to life.
The primary intended audience is graduate students in economics. However, the
techniques discussed in the book add some shiny new toys to the standard tool kit
used for economic modeling, and as such they should be of interest to researchers
as well as graduate students. The book is as self-contained as possible, providing
background in computing and analysis for the beneﬁt of those without programming
experience or university-level mathematics.
Part I of the book covers material that all well-rounded graduate students should
know. The style is relatively mathematical, and those who ﬁnd the going hard might
start by working through the exercises in appendix A. Part II is signiﬁcantly more
challenging. In designing the text it was not my intention that all of those who read
part I should go on to read part II. Rather, part II is written for researchers and graduate students with a particular aptitude for technical problems. Those who do read the
majority of part II will gain a very strong understanding of inﬁnite-horizon dynamic
programming and (nonlinear) stochastic models.
xiii

xiv

Preface

How does this book differ from related texts? There are several books on computational macroeconomics and macrodynamics that have some similarity. In comparison,
this book is not speciﬁc to macroeconomics. It should be of interest to (at least some)
people working in microeconomics, operations research, and ﬁnance. Second, it is
more focused on analysis and techniques than on applications. Even when numerical
methods are discussed, I have tried to emphasize mathematical analysis of the algorithms, and acquiring a strong knowledge of the probabilistic and function-analytic
framework that underlies proposed solutions.
The ﬂip-side of the focus on analysis is that the models are more simplistic than
in applied texts. This is not so much a book from which to learn about economics
as it is a book to learn about techniques that are useful for economic modeling. The
models we do study in detail, such as the optimal growth model and the commodity
pricing model, are stripped back to reveal their basic structure and their links with
one another.
The text contains a large amount of Python code, as well as an introduction to
Python in chapter 2. Python is rapidly maturing into one of the major programming
languages, and is a favorite of many high technology companies. The fact that Python
is open source (i.e., free to download and distribute) and the availability of excellent
numerical libraries has led to a surge in popularity among the scientiﬁc community.
All of the Python code in the text can be downloaded from the book’s homepage, at
http://johnstachurski.net/book.html.
MATLAB aﬁcionados who have no wish to learn Python can still read the book.
All of the code listings have MATLAB counterparts. They can be found alongside the
Python code on the text homepage.
A word on notation. Like any text containing a signiﬁcant amount of mathematics, the notation piles up thick and fast. To aid readers I have worked hard to keep
notation minimal and consistent. Uppercase symbols such as A and B usually refer
to sets, while lowercase symbols such as x and y are elements of these sets. Functions
use uppercase and lowercase symbols such as f , g, F, and G. Calligraphic letters such
as A and B represent sets of sets or, occasionally, sets of functions. Python keywords
are typeset in boldface. Proofs end with the symbol .
I provide a table of common symbols on page xvii. Furthermore, the index begins
with an extensive list of symbols, along with the number of the page on which they
are deﬁned.
In the process of writing this book I received invaluable comments and suggestions
from my colleagues. In particular, I wish to thank Robert Becker, Toni Braun, Roger
Farmer, Onésimo Hernández-Lerma, Timothy Kam, Takashi Kamihigashi, Noritaka
Kudoh, Vance Martin, Sean Meyn, Len Mirman, Tomoyuki Nakajima, Kevin Reffett,
Ricard Torres, and Yiannis Vailakis.
Many graduate students also contributed to the end product, including Real Arai,

Preface

xv

Rosanna Chan, Katsuhiko Hori, Murali Neettukattil, Jenö Pál, and Katsunori Yamada.
I owe particular thanks to Dimitris Mavridis, who made a large number of thoughtful
suggestions on both computational and theoretical content; and to Yu Awaya, who—
in a remarkable feat of brain-power and human endurance—read part II of the book
in a matter of weeks, solving every exercise, and checking every proof.
I have beneﬁted greatly from the generous help of my intellectual elders and betters. Special thanks go to John Creedy, Cuong Le Van, Kazuo Nishimura, and Rabee
Tourky. Extra special thanks go to Kazuo, who has helped me at every step of my
personal random walk.
The editorial team at MIT Press has been ﬁrst rate. I deeply appreciate their enthusiasm and their professionalism, as well as their gentle and thoughtful criticism:
Nothing is more valuable to the author who believes he is always right—especially
when he isn’t.
I am grateful to the Department of Economics at Melbourne University, the Center
for Operations Research and Econometrics at Université Catholique de Louvain, and
the Institute for Economic Research at Kyoto University for providing me with the
time, space, and facilities to complete this text.
I thank my parents for their love and support, and Andrij, Nic, and Roman for the
same. Thanks also go to my extended family of Aaron, Kirdan, Merric, Murdoch, and
Simon, who helped me weed out all those unproductive brain cells according to the
principles set down by Charles Darwin. Dad gets an extra thanks for his long-running
interest in this project, and for providing the gentle push that is always necessary for
a task of this size.
Finally, I thank my beautiful wife Cleo, for suffering with patience and good humor the absent-minded husband, the midnight tapping at the computer, the highs
and the lows, and then some more lows, until the job was ﬁnally done. This book is
dedicated to you.

Common Symbols
IID

∼F
N (μ, σ2 )
∼F
P( A )
x p
d p ( x, y)
bS
 f ∞
d∞ ( f , g)
bcS
ibS
ibcS
B(; x )
B

B (S)
P (S)
δx
sS
mS
bS
L1 (S, S , μ)
L1 (S, S , μ)
 f 1
d1 ( f , g )
D (S)
bM (S)
bS
d FM

independent and identically distributed according to F
the normal distribution with mean μ and variance σ2
distributed according to F
the set of all subsets of A
p
the norm (∑ik=1 xi )1/p on k
the distance  x − y p on k
the set of bounded functions mapping S into
the norm supx∈S | f ( x )| on bS
the distance  f − g∞ on bS
the continuous functions in bS
the increasing (i.e., nondecreasing) functions in bS
the continuous functions in ibS
the -ball centered on x
the indicator function of set B
the Borel subsets of S
the distributions on S
the probability measure concentrated on x
the simple functions on measure space (S, S )
the measurable real-valued functions on (S, S )
the bounded functions in mS
the μ-integrable functions on (S, S )
the metric space generated by L1 (S, S , μ)
the norm μ(| f |) on L1 (S, S , μ)
the distance  f − g1 on L1 (S, S , μ)
the densities on S
the ﬁnite signed measures on (S, B (S))
the bounded, Lipschitz functions on metric space S
the Fortet–Mourier distance on P (S)

xvii

Chapter 1

Introduction
The teaching philosophy of this book is that the best way to learn is by example.
In that spirit, consider the following benchmark modeling problem from economic
dynamics: At time t an economic agent receives income yt . This income is split into
consumption ct and savings k t . Savings is used for production, with input k t yielding
output
yt+1 = f (k t , Wt+1 ),
t = 0, 1, 2, . . .
(1.1)
where (Wt )t≥1 is a sequence of independent and identically distributed shocks. The
process now repeats, as shown in ﬁgure 1.1. The agent gains utility U (ct ) from consumption ct = yt − k t , and discounts future utility at rate ρ ∈ (0, 1). Savings behavior
is modeled as the solution to the expression

max

( k t ) t ≥0



∞

∑ ρt U (yt − k t )

t =0

subject to yt+1 = f (k t , Wt+1 ) for all t ≥ 0, with y0 given
This problem statement raises many questions. For example, from what set of possible
paths is (k t )t≥0 to be chosen? And how do we choose a path at the start of time such
that the resource constraint 0 ≤ k t ≤ yt holds at each t, given that output is random?
Surely the agent cannot choose k t until he learns what yt is. Finally, how does one go
about computing the expectation implied by the symbol ?
A good ﬁrst step is to rephrase the problem by saying that the agent seeks a savings
policy. In the present context this is a map σ that takes a value y and returns a number
σ (y) satisfying 0 ≤ σ (y) ≤ y. The interpretation is that upon observing yt , the agent’s
response is k t = σ (yt ). Next period output is then yt+1 = f (σ (yt ), Wt+1 ), next period
1

2

Chapter 1

Wt+1
k t +1

kt

f (k t, Wt+1 ) = yt+1

yt

c t +1

ct

t+1

t

Figure 1.1 Timing

savings is σ (yt+1 ), and so on. We can evaluate total reward as


∞

∑ ρt U (yt − σ(yt ))

t =0

where

yt+1 = f (σ(yt ), Wt+1 ), with y0 given

(1.2)

The equation yt+1 = f (σ (yt ), Wt+1 ) is called a stochastic recursive sequence (SRS),
or stochastic difference equation. As we will see, it completely deﬁnes (yt )t≥0 as a
sequence of random variables for each policy σ. The expression on the left evaluates
the utility of this income stream.
Regarding the expectation , we know that in general expectation is calculated by
integration. But how are we to understand this integral? It seems to be an expectation
over the set of nonnegative sequences (i.e., possible values of the path (yt )t≥0 ). High
school calculus tells us how to take an integral over an interval, or perhaps over a
subset of n-dimensional space n . But how does one take an integral over an (inﬁnitedimensional) space of sequences?
Expectation and other related issues can be addressed in a very satisfactory way,
but to do so, we will need to know at least the basics of a branch of mathematics called
measure theory. Almost all of modern probability theory is deﬁned and analyzed in
terms of measure theory, so grasping its basics is a highly proﬁtable investment. Only
with the tools that measure theory provides can we pin down the meaning of (1.2).1
Once the meaning of the problem is clariﬁed, the next step is considering how to
solve it. As we have seen, the solution to the problem is a policy function. This is
1 The

golden rule of research is to carefully deﬁne your question before you start searching for answers.

Introduction

3

rather different from undergraduate economics, where solutions are usually numbers,
or perhaps vectors of numbers—often found by differentiating objective functions. In
higher level applied mathematics, many problems have functions as their solutions.
The branch of mathematics that deals with problems having functions as solutions
is called “functional analysis.” Functional analysis is a powerful tool for solving realworld problems. Starting with basic functional analysis and a dash of measure theory,
this book provides the tools necessary to optimize (1.2), including algorithms and
numerical methods.
Once the problem is solved and an optimal policy is obtained, the income path
(yt )t≥0 is determined as a sequence of random variables. The next objective is to
study the dynamics of the economy. What statements can we make about what will
“happen” in an economy with this kind of policy? Might it settle down into some sort
of equilibrium? This is the ideal case because we can then make ﬁrm predictions. And
predictions are the ultimate goal of modeling, partly because they are useful in their
own right and partly because they allow us to test theory against data.
To illustrate analysis of dynamics, let’s specialize our model so as to dig a little
further. Suppose now that U (c) = ln c and f (k, W ) = kα W. For this very special case,
no computation is necessary: pencil and paper can be used to show (e.g., Stokey and
Lucas 1989, §2.2) that the optimal policy is given by σ (y) = θy, where θ := αρ. From
(1.2) the law of motion for the “state” variable yt is then
yt+1 = (θyt )α Wt+1 ,

t = 0, 1, 2, . . .

(1.3)

To make life simple, let’s assume that ln Wt ∼ N (0, 1). Here N (μ, v) represents the
normal distribution with mean μ and variance v, and the notation X ∼ F means that
X has distribution F.
If we take the log of (1.3), it is transformed into the linear system
xt+1 = b + αxt + wt+1 , where xt := ln yt , wt+1 ∼ N (0, 1), and b := α ln θ

(1.4)

This system is easy to analyze. In fact every xt is normally distributed because x1 is
normally distributed (x0 is constant and constant plus normal equals normal), and
moreover xt+1 is normally distributed whenever xt is normally distributed.2
One of the many nice things about normal distributions is that they are determined
by only two parameters, the mean and the variance. If we can ﬁnd these parameters,
then we know the distribution. So suppose that xt ∼ N (μt , vt ), where the constants
μt and vt are given. If you are familiar with manipulating means and variances, you
will be able to deduce from (1.4) that xt+1 ∼ N (μt+1 , vt+1 ), where
μt+1 = b + αμt
2 Recall

and

v t +1 = α 2 v t + 1

that linear combinations of normal random variables are themselves normal.

(1.5)

4

Chapter 1
0.5
0.4
0.3
0.2
0.1
0

xt ∼ N (−2, 0.8)

−3

−2

−1

0

1

Figure 1.2 Sequence of marginal distributions

Paired with initial conditions μ0 and v0 , these laws of motion pin down the sequences
(μt )t≥0 and (vt )t≥0 , and hence the distribution N (μt , vt ) of xt at each point in time. A
sequence of distributions starting from xt ∼ N (−2, 0.8) is shown in ﬁgure 1.2. The
parameters are α = 0.4 and ρ = 0.75.
In the ﬁgure it appears that the distributions are converging to some kind of limiting distribution. This is due to the fact that α < 1 (i.e., returns to capital are diminishing), which implies that the sequences in (1.5) are convergent (don’t be concerned
if you aren’t sure how to prove this yet). The limits are
μ∗ := lim μt =
t→∞

b
1−α

and

v∗ := lim vt =
t→∞

1
1 − α2

(1.6)

Hence the distribution N (μt , vt ) of xt converges to N (μ∗ , v∗ ).3 Note that this “equilibrium” is a distribution rather than a single point.
All this analysis depends, of course, on the law of motion (1.4) being linear, and
the shocks being normally distributed. How important are these two assumptions
in facilitating the simple techniques we employed? The answer is that they are both
critical, and without either one we must start again from scratch.
3 What do we really mean by “convergence” here? We are talking about convergence of a sequence of
functions to a given function. But how to deﬁne this? There are many possible ways, leading to different notions of equilibria, and we will need to develop some understanding of the deﬁnitions and the differences.

Introduction

5

Figure 1.3 Stationary distribution

To illustrate this point, let’s brieﬂy consider the threshold autoregression model

A1 Xt + b1 + Wt+1 if Xt ∈ B ⊂ n
X t +1 =
(1.7)
A2 Xt + b2 + Wt+1 otherwise
Here Xt is n × 1, Ai is n × n, bi is n × 1, and (Wt )t≥1 is an IID sequence of normally
distributed random n × 1 vectors. Although for this system the departure from linearity is relatively small (in the sense that the law of motion is at least piecewise linear),
analysis of dynamics is far more complex. Through the text we will build a set of tools
that permit us to analyze nonlinear systems such as (1.7), including conditions used
to test whether the distributions of ( Xt )t≥0 converge to some stationary (i.e., limiting)
distribution. We also discuss how one should go about computing the stationary distributions of nonlinear stochastic models. Figure 1.3 shows the stationary distribution
of (1.7) for a given set of parameters, based on such a computation.
Now let’s return to the linear model (1.4) and investigate its sample paths. Figure 1.4 shows a simulated time series over 250 periods. The initial condition is x0 = 4,
and the parameters are as before. The horizontal line is the mean μ∗ of the stationary distribution. The sequence is obviously correlated, and not surprisingly, shows
no tendency to settle down to a constant value. On the other hand, the sample mean
x̄t := 1t ∑it=1 xi seems to converge to μ∗ (see ﬁgure 1.5).
That this convergence should occur is not obvious. Certainly it does not follow
from the classical law of large numbers, since ( xt )t≥0 is neither independent nor identically distributed. Nevertheless, the question of whether sample moments converge
to the corresponding moments of the stationary distribution is an important one, with
implications for both theory and econometrics.

6

Chapter 1

Figure 1.4 Time series

Figure 1.5 Sample mean of time series

Introduction

7

For example, suppose that our simple model is being used to represent a given
economy over a given period of time. Suppose further that the precise values of the
underlying parameters α and ρ are unknown, and that we wish to estimate them from
the data.4 The method of moments technique proposes that we do this by identifying
the ﬁrst and second moments with their sample counterparts. That is, we set
ﬁrst moment = μ∗ (α, ρ) =

1 t
xt
t i∑
=1

second moment = v∗ (α, ρ) + μ∗ (α, ρ)2 =

1 t 2
xt
t i∑
=1

The right-hand side components 1t ∑it=1 xt and 1t ∑it=1 xt2 are collected from data, and
the two equalities are solved simultaneously to calculate values for α and ρ.
The underlying assumption that underpins this whole technique is that sample
means converge to their population counterparts. Figure 1.5 is not sufﬁcient proof
that such convergence does occur. We will need to think more about how to establish
these results. More importantly, our linear normal-shock model is very special. Does
convergence of sample moments occur for other related economies? The question is a
deep one, and we will have to build up some knowledge of probability theory before
we can tackle it.
To end our introductory comments, note that as well as studying theory, we will
be developing computer code to tackle the problems outlined above. All the major
code listings in the text can be downloaded from the text homepage, which can be
found at http://johnstachurski.net/book.html. The homepage also collects other
resources and links related to our topic.

4 We

have also used parameters b and θ, but b = α ln θ, and θ = αρ.

Part I

Introduction to Dynamics

9

Chapter 2

Introduction to Programming
Some readers may disagree, but to me computers and mathematics are like beer and
potato chips: two ﬁne tastes that are best enjoyed together. Mathematics provides the
foundations of our models and of the algorithms we use to solve them. Computers
are the engines that run these algorithms. They are also invaluable for simulation and
visualization. Simulation and visualization build intuition, and intuition completes
the loop by feeding into better mathematics.
This chapter provides a brief introduction to scientiﬁc computing, with special emphasis on the programming language Python. Python is one of several outstanding
languages that have come of age in recent years. It features excellent design, elegant syntax, and powerful numerical libraries. It is also free and open source, with a
friendly and active community of developers and users.

2.1

Basic Techniques

This section provides a short introduction to the fundamentals of programming: algorithms, control ﬂow, conditionals, and loops.

2.1.1

Algorithms

Many of the problems we study in this text reduce to a search for algorithms. The
language we will use to describe algorithms is called pseudocode. Pseudocode is an informal way of presenting algorithms for the beneﬁt of human readers, without getting
tied down in the syntax of any particular programming language. It’s a good habit to
begin every program by sketching it ﬁrst in pseudocode.
11

12

Chapter 2
Our pseudocode rests on the following four constructs:
if–then–else,

while,

repeat–until,

and

for

The general syntax for the if–then–else construct is
if condition then
ﬁrst sequence of actions
else
second sequence of actions
end

The condition is evaluated as either true or false. If found true, the ﬁrst sequence
of actions is executed. If false, the second is executed. Note that the else statement
and alternative actions can be omitted: If the condition fails, then no actions are performed. A simple example of the if–then–else construct is
if there are cookies in the jar then
eat them
else
go to the shops and buy more
eat them
end

The while construct is used to create a loop with a test condition at the beginning:
while condition do
sequence of actions
end

The sequence of actions is performed only if the condition is true. Once they are
completed, the condition is evaluated again. If it is still true, the actions in the loop
are performed again. When the condition becomes false the loop terminates. Here’s
an example:
while there are cookies in the jar do
eat one
end

The algorithm terminates when there are no cookies left. If the jar is empty to begin
with, the action is never attempted.
The repeat–until construct is similar:

Introduction to Programming

13

repeat
sequence of actions
until condition

Here the sequence of actions is always performed once. Next the condition is
checked. If it is true, the algorithm terminates. If not, the sequence is performed
again, the condition is checked, and so on.
The for construct is sometimes called a deﬁnite loop because the number of repetitions is predetermined:
for element in sequence do
do something
end

For example, the following algorithm computes the maximum of a function f over
a ﬁnite set S using a for loop and prints it to the screen.1
set c = −∞
for x in S do
set c = max{c, f ( x )}
end
print c

In the for loop, x is set equal to the ﬁrst element of S and the statement “set
c = max{c, f ( x )}” is executed. Next x is set equal to the second element of S, the
statement is executed again, and so on. The statement “set c = max{c, f ( x )}” should
be understood to mean that max{c, f ( x )} is ﬁrst evaluated, and the resulting value is
assigned to the variable c.
Exercise 2.1.1 Modify this algorithm so that it prints the maximizer rather than the
maximum. Explain why it is more useful to know the maximizer.
Let’s consider another example. Suppose that we have two arrays A and B stored
in memory, and we wish to know whether the elements of A are a subset of the elements of B. Here’s a prototype algorithm that will tell us whether this is the case:
set subset = True
for a in A do
if a ∈
/ B then set subset = False
end
print subset
1 That is, it displays the value to the user. The term “print” dates from the days when sending output to
the programmer required generating hardcopy on a printing device.

14

Chapter 2

Exercise 2.1.2 The statement “a ∈
/ B” may require coding at a lower level.2 Rewrite
the algorithm with an inner loop that steps through each b in B, testing whether a = b,
and setting subset = False if no matches occur.
Finally, suppose we wish to model ﬂipping a biased coin with probability p of
heads, and have access to a random number generator that yields uniformly distributed variates on [0, 1]. The next algorithm uses these random variables to generate
and print the outcome (either “heads” or “tails”) of ten ﬂips of the coin, as well as the
total number of heads.3
set H = 0
for i in 1 to 10 do
draw U from the uniform distribution on [0, 1]
if U < p then
print “heads”
H = H+1
else
print “tails”
end
end
print H

// With probability p
// With probability 1 − p

Note the use of indentation, which helps maintaining readability of our code.
Exercise 2.1.3 Consider a game that pays $1 if three consecutive heads occur in ten
ﬂips and zero otherwise. Modify the previous algorithm to generate a round of the
game and print the payoff.
Exercise 2.1.4 Let b be a vector of zeros and ones. The vector corresponds to the
employment history of one individual, where 1 means employed at the associated
point in time, and 0 means unemployed. Write an algorithm to compute the longest
(consecutive) period of employment.

2.1.2

Coding: First Steps

When it comes to programming, which languages are suitable for scientiﬁc work?
Since the time it takes to complete a programming project is the sum of the time spent
writing the code and the time that a machine spends running it, an ideal language
would minimize both these terms. Unfortunately, designing such a language is not
2 Actually, in many high-level languages you will have an operator that tests whether a variable is a
member of a list. For the sake of the exercise, suppose this is not the case.
3 What is the probability distribution of this total?

Introduction to Programming

15

easy. There is an inherent trade-off between human time and computer time, due to
the fact that humans and computers “think” differently: Languages that cater more to
the human brain are usually less optimal for the computer and vice versa.
Using this trade-off, we can divide languages into (1) robust, lower level languages
such as Fortran, C/C++, and Java, which execute quickly but can be a chore when it
comes to coding up your program, and (2) the more nimble, higher level “scripting”
languages, such as Python, Perl, Ruby, Octave, R, and MATLAB. By design, these
languages are easy to write with and debug, but their execution can be orders of magnitude slower.
To give an example of these different paradigms, consider writing a program that
prints out “Hello world.” In C, which is representative of the ﬁrst class of languages,
such a program might look like this:
#i n c l u d e < stdio .h >
i n t main ( i n t argc , c h a r * argv []) {
printf ( " Hello world \ n " );
r e t u r n 0;
}

Let’s save this text ﬁle as hello.c, and compile it at the shell prompt4 using the gcc
(GNU project C) compiler:
gcc -o hello hello . c

This creates an executable ﬁle called hello that can be run at the prompt by typing its
name.
For comparison, let’s look at a “Hello world” program in Python—which is representative of the second class of languages. This is simply
print ( " Hello world " )

and can be run from my shell prompt as follows:
python hello . py

What differences do we observe with these two programs? One obvious difference is
that the C code contains more boilerplate. In general, C will be more verbose, requiring us to provide instructions to the computer that don’t seem directly relevant to our
task. Even for experienced programmers, writing boilerplate code can be tedious and
error prone. The Python program is much shorter, more direct, and more intuitive.
Second, executing the C program involves a two-step procedure: First compile,
then run. The compiler turns our program into machine code speciﬁc to our operating system. By viewing the program as a whole prior to execution, the compiler can
4 Don’t

worry if you aren’t familiar with the notion of a shell or other the details of the program. We are
painting with broad strokes at the moment.

16

Chapter 2

optimize this machine code for speed. In contrast, the Python interpreter sends individual instructions to the CPU for processing as they are encountered. While slower,
the second approach is more interactive, which is helpful for testing and debugging.
We can run parts of the program separately, and then interact with the results via the
interpreter in order to evaluate their performance.
In summary, the ﬁrst class of languages (C, Fortran, etc.) put more burden on the
programmer to specify exactly what they want to happen, working with the operating system on a more fundamental level. The second class of languages (Python,
MATLAB, etc.) shield the programmer from such details and are more interactive, at
the cost of slower execution. As computers have become cheaper and more powerful,
these “scripting” languages have naturally become more popular. Why not ask the
computer to do most of the heavy lifting?
In this text we will work exclusively with an interpreted language, leaving the ﬁrst
class of languages for those who wish to dig deeper. However, we note in passing that
one can often obtain the best of both worlds in the speed versus ease-of-use trade-off.
This is achieved by mixed language programming. The idea here is that in a typical
program there are only a few lines of code that are executed many times, and it is
these bottlenecks that should be optimized for speed. Thus the modern method of
numerical programming is to write the entire program in an interpreted language
such as Python, proﬁle the code to ﬁnd bottlenecks, and outsource those (and only
those) parts to fast, compiled languages such as C or Fortran.5
The interpreted language we work with in the text is Python. However, MATLAB
code is provided on the text homepage for those who prefer it. MATLAB has a gentler
learning curve, and its numerical libraries are better documented. Readers who are
comfortable with MATLAB and have no interest in Python can skip the rest of this chapter.
Python is a modern and highly regarded object-oriented programming language
used in academia and the private sector for a wide variety of tasks. In addition to
being powerful enough to write large-scale applications, Python is known for its minimalist style and clean syntax—designed from the start with the human reader in
mind. Among the common general-purpose interpreted languages (Perl, Visual Basic, etc.), Python is perhaps the most suitable for scientiﬁc and numerical applications,
with a large collection of MATLAB-style libraries organized primarily by the SciPy
project (scipy.org).6 The power of Python combined with these scientiﬁc libraries
makes it an excellent choice for numerical programming.
Python is open source, which means that it can be downloaded free of charge,7
and that we can peruse the source code of the various libraries to see how they work
5 Another promising option is Cython, which is similar to Python but generates highly optimized C code.
6 See

also Sage, which is a mathematical tool kit built on top of Python.
main Python repositories are at python.org. Recently several distributions of Python that come
bundled with various scientiﬁc tools have appeared. The book home page contains suggestions and links.
7 The

Introduction to Programming

17

(and to make changes if necessary).
Rather than working directly with the Python interpreter, perhaps the best way to
begin interacting with Python is by using IDLE.8 IDLE is a free, cross-platform development environment for Python that comes bundled with most Python distributions.
After you start IDLE, you will meet an interface to the interpreter that is friendlier than
the standard one, providing color syntax highlighting, tab completion of commands,
and more.9 At the IDLE prompt you can start typing in commands and viewing the
results:
>>> 10 * 10
100
>>> 10**2 # exponentiation
100

The result of the calculations is printed when you hit the return key. Notice that with
the second calculation we added a comment, which is the hash symbol followed by
text. Anything to the right of a hash is ignored by the interpreter. Comments are only
for the beneﬁt of human readers.
Continuing on with our tour of Python, next let’s try assigning values to variables.
Variables are names for values stored in memory. Here is an example:
>>>
>>>
>>>
3
>>>
>>>
>>>

x = 3
y = 2.5
x

# Bind x to integer 3
# Bind y to floating point number 2.5
# Query the value of x

z = x * y
x = x + 1
a, b = 1, 2

# Bind z to x * y = 7.5
# Rebind x to integer 4
# Bind a to 1 and b to 2

Names (x, y, etc.) are also called identiﬁers, and the values assigned to them are called
objects. You can think of the identiﬁers as “pointers” to the location in memory where
their values are stored. Identiﬁers are case sensitive (X is different from x), must start
with a letter or an underscore, and cannot be one of Python’s keywords.10
Observe that assignment of names to values (identiﬁers to objects) is accomplished
via the “=” symbol. Assignment is also called binding: setting x = 3 binds the identiﬁer x to the integer object 3. Passed the statement z = x * y, the interpreter evaluates
8 See http://johnstachurski.net/book.html

for more information on how to get started with Python.
is not the best development environment for Python, but it is the easiest to get started with. A
more powerful alternative is the IPython shell combined with a good text editor (such as Emacs or Vim).
The following discussion assumes that you are using IDLE—if you are using something else, then you
probably know what you are doing and hence need less instruction.
10 These are: and, del, from, not, while, as, elif, global, or, with, assert, else, if, pass, yield, break, except,
import, class, exec, in, raise, continue, ﬁnally, is, return, def, for, lambda, try, True, False, and None.
9 IDLE

18

Chapter 2

the expression x * y on the right-hand side of the = sign to obtain 7.5, stores this result in memory, and binds the identiﬁer speciﬁed on the left (i.e., z) to that value.
Passed x = x + 1, the statement executes in a similar way (right to left), creating a
new integer object 4 and rebinding x to that value.
Objects stored in memory have different types. The type of our objects can be
queried using the built-in function type():
>>> type ( x )
< type ’ int ’ >
>>> type ( y )
< type ’ float ’ >

# x = 4
# y = 2.5

The identiﬁer x is bound to integer 4, while y is bound to ﬂoating point number 2.5. A
ﬂoating point number is a number with a decimal point. Like most programming languages, Python distinguishes between ﬂoats and integers because integers are more
efﬁcient in terms of operations such as addition and multiplication.
Another common type of object is a string:
>>>
>>>
>>>
>>>
>>>
>>>
...
...
...

s = " godzilla "
# Single or double quotes
s . count ( " g " )
# How many g ’s ? Returns 1
s . startswith ( " god " ) # Returns True
s . upper ()
# Returns " GODZILLA "
s . replace ( " l " , " m " ) # Returns " godzimma "
s2 = " " "
Triple quotes can be used to create multi - line
strings , which is useful when you want to
record a lot of text . " " "

We are using some of Python’s string methods (e.g., count(), upper()) to manipulate
the string "godzilla". Before discussing methods, let’s introduce a fourth data type,
called lists. Lists are containers used to store a collection of objects.
>>> X = [20 , 30 , 40]
>>> sum ( X )
>>> max ( X )

# Bind X to a list of integers
# Returns 90
# Returns 40

We can extract elements of the list using square brackets notation. Like most programming languages, the ﬁrst index is 0 rather than 1, so X[0] references the ﬁrst element
of the list (which is 20), X[1] references the second element, and so on. In this context
the integers 0, 1, 2 are called the indexes of the list. The list can be modiﬁed using
indexes as follows:
>>> X [0] = " godzilla "
>>> d e l X [1]

# Now X = [" godzilla " , 30 , 40]
# Now X = [" godzilla " , 40]

Lists can be unpacked into variables containing their elements:

Introduction to Programming
>>> x , y = [ " a " , " b " ]

19
# Now x = " a " and y = " b "

We saw that Python has methods for operations on strings, as in s.count("g") above.
Here s is the variable name (the identiﬁer), bound to string "godzilla", and count()
is the name of a string method, which can be called on any string. Lists also have
methods. Method calls on strings, lists and other objects follow the general syntax
identifier . methodName ( arguments )

# e . g . , s . count (" g ")

For example, X.append(3) appends 3 to the end of X, while X.count(3) counts the
number of times that 3 occurs in X. In IDLE you can enter X. at the prompt and then
press the TAB key to get a list of methods that can be applied to lists (more generally,
to objects of type type(X)).

2.1.3

Modules and Scripts

There are several ways to interact with the Python interpreter. One is to type commands directly into the prompt as above. A more common way is to write the commands in a text ﬁle and then run that ﬁle through the interpreter. There are many
ways to do this, and in time you will ﬁnd one that best suits your needs. The easiest is
to use the editor found in IDLE: Open up a new window under the ’File’ menu. Type
in a command such as print("Hello world") and save the ﬁle in the current directory as hello.py. You can now run the ﬁle by pressing F5 or selecting ’Run Module’.
The output Hello world should appear at the command prompt.
A text ﬁle such as hello.py that is run through an interpreter is known as a script.
In Python such ﬁles are also known as modules, a module being any ﬁle with Python
functions and other deﬁnitions. Modules can be run through the interpreter using
the keyword import. Thus, as well as executing hello.py using IDLE’s ’Run Module’
command as above, we can type the following at the Python prompt:
>>> i m p o r t hello
’ Hello world ’

# Load the file hello . py and run

When you ﬁrst import the module hello, Python creates a ﬁle called hello.pyc, which
is a byte-compiled ﬁle containing the instructions in hello.py. Note that if you now
change hello.py, resave, and import again, the changes will not be noticed because
hello.pyc is not altered. To affect the changes in hello.py, use reload(hello), which
rewrites hello.pyc.
There are vast libraries of Python modules available,11 some of which are bundled
with every Python distribution. A useful example is math:
>>> i m p o r t math
11 See,

# Module math

for example, http://pypi.python.org/pypi.

20

Chapter 2

>>> math . pi
>>> math . sqrt (4)

# Returns 3.1415926535897931
# Returns 2.0

Here pi is a ﬂoat object supplied by math and sqrt() is a function object. Collectively
these objects are called attributes of the module math.
Another handy module in the standard library is random.
>>>
>>>
>>>
>>>
>>>

i m p o r t random
X = ["a", "b", "c"]
random . choice ( X )
random . shuffle ( X )
random . gammavariate (2 , 2)

# Returned " b "
# X is now shuffled
# Returned 3.433472

Notice how module attributes are accessed using moduleName.identifier notation.
Each module has it’s own namespace, which is a mapping from identiﬁers to objects in
memory. For example, pi is deﬁned in the namespace of math, and bound to the ﬂoat
3.14 · · · 1. Identiﬁers in different namespaces are independent, so modules mod1 and
mod2 can both have distinct attribute a. No confusion arises because one is accessed
as mod1.a and the other is accessed as mod2.a.12
When x = 1 is entered at the command line (i.e., Python prompt), the identiﬁer
x is registered in the interactive namespace.13 If we import a module such as math,
only the module name is registered in the interactive namespace. The attributes of
math need to be accessed as described above (i.e, math.pi references the ﬂoat object
pi registered in the math namespace).14 Should we wish to, we can however import
attributes directly into the interactive namespace as follows:
>>> from math i m p o r t sqrt , pi
>>> pi * sqrt (4)

# Returns 6.28...

Note that when a module mod is run from within IDLE using ’Run Module’, commands are executed within the interactive namespace. As a result, attributes of mod
can be accessed directly, without the preﬁx mod. The same effect can be obtained at the
prompt by entering
>>> from mod i m p o r t *

# Import everything

In general, it is better to be selective, importing only necessary attributes into the
interactive namespace. The reason is that our namespace may become ﬂooded with
variable names, possibly “shadowing” names that are already in use.
12 Think of the idea of a namespace as like a street address, with street name being the namespace and
street number being the attribute. There is no confusion if two houses have street number 10, as long as we
supply the street names of the two houses.
13 Internally, the interactive namespace belongs to a top-level module called __main__.
14 To view the contents of the interactive namespace type vars() at the prompt. You will see some stuff we
have not discussed yet (__doc__, etc.), plus any variables you have deﬁned or modules you have imported.
If you import math and then type vars(math), you will see the attributes of this module.

Introduction to Programming

21

Modules such as math, sys, and os come bundled with any Python distribution.
Others will need to be installed. Installation is usually straightforward, and documented for each module. Once installed, these modules can be imported just like
standard library modules. For us the most important third-party module15 is the scientiﬁc computation package SciPy, which in turn depends on the fast array processing
module NumPy. The latter is indispensable for serious number crunching, and SciPy
provides many functions that take advantage of the facilities for array processing in
NumPy, based on efﬁcient C and Fortran libraries.16
Documentation for SciPy and NumPy can be found at the SciPy web site and the
text home page. These examples should give the basic idea:
>>>
>>>
>>>
>>>
>>>
>>>

from scipy i m p o r t *
integral , error = integrate . quad ( sin , -1 , 1)
minimizer = optimize . fminbound ( cos , 0 , 2 * pi )
A = array ([[1 , 2] , [3 , 4]])
determinant = linalg . det ( A )
eigenvalues = linalg . eigvals ( A )

SciPy functions such as sin() and cos() are called vectorized (or universal) functions, which means that they accept either numbers or sequences (lists, NumPy arrays,
etc.) as arguments. When acting on a sequence, the function returns an array obtained
by applying the function elementwise on the sequence. For example:
>>> cos ([0 , pi , 2 * pi ]) # Returns array ([ 1. , -1. ,

1.])

There are also modules for plotting and visualization under active development.
At the time of writing, Matplotlib and PyX are popular and interact well with SciPy.
A bit of searching will reveal many alternatives.

2.1.4

Flow Control

Conditionals and loops can be used to control which commands are executed and the
order in which they are processed. Let’s start with the if/else construct, the general
syntax for which is
i f < expression >:
< statements >
else:
< statements >

# If < expression > is true , then
# this block of code is executed
# Otherwise , this one

The else block is optional. An expression is any code phrase that yields a value when
executed, and conditionals like if may be followed by any valid expression. Expressions are regarded as false if they evaluate to the boolean value False (e.g., 2 < 1),
15 Actually
16 For

a package (i.e., collection of modules) rather than a module.
symbolic (as opposed to numerical) algebra see SymPy or Sage.

22

Chapter 2

to zero, to an empty list [], and one or two other cases. All other expressions are
regarded as true:
>>> i f 42 and 99: print ( " foo " )
>>> i f [] o r 0.0: print ( " bar " )

# Both True , prints foo
# Both False

As discussed above, a single = is used for assignment (i.e., binding an identiﬁer to an
object), rather than comparison (i.e., testing equality). To test the equality of objects,
two equal signs are used:
>>> x = y = 1
>>> i f x == y : print ( " foobar " )

# Bind x and y to 1
# Prints foobar

To test whether a list contains a given element, we can use the Python keyword in:
>>> 1 i n [1 , 2 , 3]
>>> 1 n o t i n [1 , 2 , 3]

# Evaluates as True
# Evaluates as False

To repeat execution of a block of code until a condition fails, Python provides the
while loop, the syntax of which is
w h i l e < expression >:
< statements >

Here a while loop is used to create the list X = [1,...,10]:17
X = []
i = 1
w h i l e len ( X ) < 10:
X . append ( i )
i += 1
print ( " Loop completed " )

#
#
#
#
#

Start with an empty list
Bind i to integer object 1
While length of list X is < 10
Append i to end of list X
Equivalent to i = i + 1

At ﬁrst X is empty and i = 1. Since len(X) is zero, the expression following the while
keyword is true. As a result we enter the while loop, setting X = [1] and i = 2. The
expression len(X) < 10 is now evaluated again and, since it remains true, the two
lines in the loop are again executed, with X becoming [1, 2] and i taking the value 3.
This continues until len(X) is equal to 10, at which time the loop terminates and the
last line is executed.
Take note of the syntax. The two lines of code following the colon, which make up
the body of the while loop, are indented the same number of spaces. This is not just
to enhance readability. In fact the Python interpreter determines the start and end of code
blocks using indentation. An increase in indentation signiﬁes the start of a code block,
17 In

the following code, the absence of a Python prompt at the start of each line means that the code is
written as a script (module) and then run.

Introduction to Programming

23

whereas a decrease signiﬁes its end. In Python the convention is to use four spaces to
indent each block, and I recommend you follow it.18
Here’s another example that uses the break keyword. We wish to simulate the
random variable T := min{t ≥ 1 : Wt > 3}, where (Wt )t≥1 is an IID sequence of
standard normal random variables:
from random i m p o r t normalvariate
T = 1
w h i l e 1:
# Always true
X = normalvariate (0 , 1)
# Draw X from N (0 ,1)
i f X > 3:
# If X > 3 ,
print ( T )
# print the value of T ,
break
# and terminate while loop .
T += 1
# Else T = T + 1 , repeat

The program returns the ﬁrst point in time t such that Wt > 3.
Another style of loop is the for loop. Often for loops are used to carry out operations on lists. Suppose, for example, that we have a list X, and we wish to create a
second list Y containing the squares of all elements in X that are strictly less than zero.
Here is a ﬁrst pass:
Y = []
f o r i i n range ( len ( X )):
i f X [ i ] < 0:
Y . append ( X [ i ]**2)

# For all indexes of X

This is a traditional C-style for loop, iterating over the indexes 0 to len(X)-1 of the list
X. In Python, for loops can iterate over any list, rather than just sequences of integers
(i.e., indexes), which means that the code above can be simpliﬁed to
Y = []
f o r x i n X:
# For all x in X , starting with X [0]
i f x < 0:
Y . append ( x **2)

In fact, Python provides a very useful construct, called a list comprehension, that allows
us to achieve the same thing in one line:
Y = [ x **2 f o r x i n X i f x < 0]

A for loop can be used to code the algorithm discussed in exercise 2.1.2 on page 14:
subset = True
f o r a i n A:
18 In

text ﬁles, tabs are different to spaces. If you are working with a text editor other than IDLE, you
should conﬁgure the tab key to insert four spaces. Most decent text editors have this functionality.

24

Chapter 2

i f a not i n B:
subset = F a l s e
print ( subset )

Here A and B are expected to be lists.19
We can also code the algorithm on page 14 along the following lines:
from random i m p o r t uniform
H , p = 0 , 0.5
f o r i i n range (10):
U = uniform (0 , 1)
i f U < p:
print ( " heads " )
H += 1
else:
print ( " tails " )
print ( H )

# H = 0 , p = 0.5
# Iterate 10 times
# U is uniform on (0 , 1)

# H = H + 1

Exercise 2.1.5 Turn the pseudocode from exercise 2.1.3 into Python code.
Python for loops can step through any object that is iterable. For example:
from urllib i m p o r t urlopen
webpage = urlopen ( " http :// johnstachurski . net " )
f o r line i n webpage :
print ( line )

Here the loop acts on a “ﬁle-like” object created by the call to urlopen(). Consult the
Python documentation on iterators for more information.
Finally, it should be noted that in scripting languages for loops are inherently
slow. Here’s a comparison of summing an array with a for loop and summing with
NumPy’s sum() function:
i m p o r t numpy , time
Y = numpy . ones (100000)
t1 = time . time ()
s = 0
f o r y i n Y:
s += y
t2 = time . time ()
s = numpy . sum ( Y )
t3 = time . time ()
print (( t2 - t1 )/( t3 - t2 ))

# NumPy array of 100 ,000 ones
# Record time
# Sum elements with for loop
# Record time
# NumPy ’s sum () function
# Record time

19 Actually they are required to be iterable.

this test for you. The details are omitted.

Also note that Python has a set data type, which will perform

Introduction to Programming

25

On my computer the output is about 200, meaning that, at least for this array of numbers, NumPy’s sum() function is roughly 200 times faster than using a for loop. The
reason is that NumPy’s sum() function passes the operation to efﬁcient C code.

2.2

Program Design

The material we have covered so far is already sufﬁcient to solve useful programming
problems. The issue we turn to next is that of design: How to construct programs
so as to retain clarity and readability as our projects grow. We begin with the idea of
functions, which are labeled blocks of code written to perform a speciﬁc operation.

2.2.1

User-Deﬁned Functions

The ﬁrst step along the road to good program design is learning to break your program up into functions. Functions are a key tool through which programmers implement the time-honored strategy of divide and conquer: problems are broken up into
smaller subproblems, which are then coded up as functions. The main program then
coordinates these functions, calling on them to do their jobs at the appropriate time.
Now the details. When we pass the instruction x = 3 to the interpreter, an integer
“object” with value 3 is stored in the memory and assigned the identiﬁer x. In a similar
way we can also create a set of instructions for accomplishing a given task, store the
instructions in memory, and bind an identiﬁer (name) that can be used to call (i.e., run)
the instructions. The set of instructions is called a function. Python supplies a number
of built-in functions, such as max() and sum() above, as well as permitting users to
deﬁne their own functions. Here’s a fairly useless example of the latter:
d e f f (x , y ):
print ( x + y )

# Bind f to a function that
# prints the value of x + y

After typing this into a new window in IDLE, saving it and running it, we can then
call f at the command prompt:
>>> f (2 ,3)
>>> f ( " code " , " monkey " )

# Prints 5
# Prints " code monkey "

Take note of the syntax used to deﬁne f. We start with def, which is a Python keyword
used for creating functions. Next follows the name and a sequence of arguments in
parentheses. After the closing bracket a colon is required. Following the colon we
have a code block consisting of the function body. As before, indentation is used to
delimit the code block.
Notice that the order in which arguments are passed to functions is important. The
calls f("a", "b") and f("b", "a") produce different output. When there are many

26

Chapter 2

arguments, it can become difﬁcult to remember which argument should be passed
ﬁrst, which should be passed second, and so on. In this case one possibility is to use
keyword arguments:
d e f g ( word1 = " Charlie " , word2 = " don ’t " , word3 = " surf . " ):
print ( word1 + word2 + word3 )

The values supplied to the parameters are defaults. If no value is passed to a given
parameter when the function is called, then the parameter name is bound to its default
value:
>>> g ()
>>> g ( word3 = " swim " )

# Prints " Charlie don ’t surf "
# Prints " Charlie don ’t swim "

If we do not wish to specify any particular default value, then the convention is to use
None instead, as in x=None.
Often one wishes to create functions that return an object as the result of their
internal computations. To do so, we use the Python keyword return. Here is an
example that computes the norm distance between two lists of numbers:
d e f normdist (X , Y ):
Z = [( x - y )**2 f o r x , y i n zip (X , Y )]
r e t u r n sum ( Z )**0.5

We are using the built-in function zip(), which allows us to step through the x, y
pairs, as well as a list comprehension to construct the list Z. A call such as
>>> p = normdist (X , Y )

# X , Y are lists of equal length

binds identiﬁer p to the value returned by the function.
It’s good practice to include a doc string in your functions. A doc string is a string
that documents the function, and comes at the start of the function code block. For
example,
d e f normdist (X , Y ):
" Computes euclidean distance between two vectors . "
Z = [( x - y )**2 f o r x , y i n zip (X , Y )]
r e t u r n sum ( Z )**0.5

Of course, we could just use the standard comment notation, but doc strings have
certain advantages that we won’t go into here. In the code in this text, doc strings are
used or omitted depending on space constraints.
Python provides a second way to deﬁne functions, using the lambda keyword.
Typically lambda is used to create small, in-line functions such as
f = lambda x , y : x + y

# E . g . f (1 ,2) returns 3

Introduction to Programming

27

Note that functions can return any Python object, including functions. For example, suppose that we want to be able to create the Cobb–Douglas production function
f (k) = Akα for any parameters A and α. The following function takes parameters
( A, α) as arguments and creates and returns the corresponding function f :
d e f cobbdoug (A , alpha ):
r e t u r n lambda k : A * k ** alpha

After saving and running this, we can call cobbdoug at the prompt:
>>> g = cobbdoug (1 , 0.5)

2.2.2

# Now g ( k ) returns 1 * k **0.5

More Data Types

We have already met several native Python data types, such as integers, lists and
strings. Another native Python data type is the tuple:
>>> X = (20 , 30 , 40)

# Parentheses are optional

Tuples behave like lists, in that one can access elements of the tuple using indexes.
Thus X[0] returns 20, X[1] returns 30, and so on. There is, however, one crucial difference: Lists are a mutable data type, whereas tuples (like strings) are immutable. In
essence, mutable data types such as lists can be changed (i.e., their contents can be altered without creating a new object), whereas immutable types such as tuples cannot.
For example, X[0] = 3 raises an exception (error) if X is a tuple. If X is a list, then the
same statement changes the ﬁrst element of X to 3.
Tuples, lists, and strings are collectively referred to as sequence types, and they
support a number of common operations (on top of the ability to access individual
elements via indexes starting at zero). For example, adding two sequences concatenates them. Thus (1, 2) + (3, 4) creates the tuple (1, 2, 3, 4), while "ab" +
"cd" creates "abcd". Multiplication of a sequence by an integer n concatenates with
n copies of itself, so [1] * 3 creates [1, 1, 1]. Sequences can also be unpacked: x, y
= (1, 2) binds x to 1 and y to 2, while x, y = "ab" binds x to "a" and y to "b".
Another useful data type is a dictionary, also known as a mapping, or an associative array. Mathematically, a dictionary is just a function on a ﬁnite set, where the
programmer supplies the domain of the function plus the function values on elements
of the domain. The points in the domain of a dictionary are referred to as its keys. A
dictionary d is created by specifying the key/value pairs. Here’s an example:
>>> d = { " Band " : " AC / DC " , " Track " : " Jailbreak " }
>>> d [ " Track " ]
" Jailbreak "

28

Chapter 2
x = y = [1]

Mutable case:

[1]

x
y

y[0] = 2

y

y = 2

x = y = 1
x

[2]

x

1

x

1

y

2

Immutable case:
y

Figure 2.1 Mutable and immutable types

Just as lists and strings have methods that act on them, dictionaries have dictionary
methods. For example, d.keys() returns the list of keys, and d.values() returns the
list of values. Try the following piece of code, which assumes d is as deﬁned above:
>>> f o r key i n d . keys (): print ( key , d [ key ])

Values of a dictionary can be any object, and keys can be any immutable object.
This brings us back to the topic of mutable versus immutable. A good understanding of the difference between mutable and immutable data types is helpful when
trying to keep track of your variables. At the same time the following discussion is
relatively technical, and can probably be skipped if you have little experience with
programming.
To begin, consider ﬁgure 2.1. The statement x = y = [1] binds the identiﬁers x
and y to the (mutable) list object [1]. Next y[0] = 2 modiﬁes that same list object,
so its ﬁrst (and only) element is 2. Note that the value of x has now changed, since the
object it is bound to has changed. On the other hand, x = y = 1 binds x and y to an
immutable integer object. Since this object cannot be altered, the assignment y = 2
rebinds y to a new integer object, and x remains unchanged.20
A related way that mutable and immutable data types lead to different outcomes
is when passing arguments to functions. Consider, for example, the following code
segment:
20 You

can check that x and y point to different objects by typing id(x) and id(y) at the prompt. Their
unique identiﬁer (which happens to be their location in memory) should be different.

Introduction to Programming

29

d e f f ( x ):
x = x + 1
return x
x = 1
print ( f ( x ) , x )

This prints 2 as the value of f(x) and 1 as the value of x, which works as follows: After
the function deﬁnition, x = 1 creates a global variable x and binds it to the integer object
1. When f is called with argument x, a local namespace for its variables is allocated in
memory, and the x inside the function is created as a local variable in that namespace
and bound to the same integer object 1. Since the integer object is immutable, the
statement x = x + 1 creates a new integer object 2 and rebinds the local x to it. This
reference is now passed back to the calling code, and hence f(x) references 2. Next,
the local namespace is destroyed and the local x disappears. Throughout, the global x
remains bound to 1.
The story is different when we use a mutable data type such as a list:
d e f f ( x ):
x [0] = x [0] + 1
return x
x = [1]
print ( f ( x ) , x )

This prints [2] for both f(x) and x. Here the global x is bound to the list object [1].
When f is called with argument x, a local x is created and bound to the same list
object. Since [1] is mutable, x[0] = x[0] + 1 modiﬁes this object without changing
its location in memory, so both the local x and the global x are now bound to [2]. Thus
the global variable x is modiﬁed by the function, in contrast to the immutable case.

2.2.3

Object-Oriented Programming

Python supports both procedural and object-oriented programming (OOP). While any
programming task can be accomplished using the traditional procedural style, OOP
has become a central part of modern programming design, and will reward even the
small investment undertaken here. It succeeds because its design pattern ﬁts well
with the human brain and its natural logic, facilitating clean, efﬁcient code. It ﬁts
well with mathematics because it encourages abstraction. Just as abstraction in mathematics allows us to develop general ideas that apply in many contexts, abstraction
in programming lets us build structures that can be used and reused in different programming problems.
Procedural programming is based around functions (procedures). The program
has a state, which is the values of its variables, and functions are called to act on these

30

Chapter 2

data according to the task. Data are passed to functions via function calls. Functions
return output that modiﬁes the state. With OOP, however, data and functions are
bundled together into logical entities called abstract data types (ADTs). A class deﬁnition is a blueprint for such an ADT, describing what kind of data it stores, and what
functions it possesses for acting on these data. An object is an instance of the ADT; an
individual realization of the blueprint, typically with its own unique data. Functions
deﬁned within classes are referred to as methods.
We have already met objects and methods. Recall that when the Python interpreter
receives the instruction X = [1, 2], it stores the data [1, 2] in memory, recording it
as an object of type list. The identiﬁer X is bound to this object, and we can use it to call
methods that act on the data. For example, X.reverse() changes the data to [2, 1].
This method is one of several list methods. There are also string methods, dictionary
methods, and so on.
What we haven’t done so far is create our own ADTs using class deﬁnitions. You
will probably ﬁnd the class deﬁnition syntax a little ﬁddly at ﬁrst, but it does become
more intuitive as you go along. To illustrate the syntax we will build a simple class to
represent and manipulate polynomial functions. The data in this case are the coefﬁcients ( a0 , . . . , a N ), which deﬁne a unique polynomial
p ( x ) = a0 + a1 x + a2 x 2 + · · · a N x N =

N

∑ an x n

n =0

(x ∈

)

To manipulate these data we will create two methods, one to evaluate the polynomial
from its coefﬁcients, returning the value p( x ) for any x, and another to differentiate
the polynomial, replacing the original coefﬁcients ( a0 , . . . , a N ) with the coefﬁcients of
p.
Consider, ﬁrst, listing 2.1, which sketches a class deﬁnition in pseudo-Python. This
is not real Python code—it is intended to give the feeling of how the class deﬁnition
might look, while omitting some boilerplate. The name of the class is Polynomial, as
speciﬁed after the keyword class. The class deﬁnition consists of three methods. Let’s
discuss them in the order they appear.
The ﬁrst method is called initialize(), and represents a constructor, which is
a special method most languages provide to build (construct an instance of) an object from a class deﬁnition. Constructor methods usually take as arguments the data
needed to set up a speciﬁc instance, which in this case is the vector of coefﬁcients
( a0 , . . . , a N ). The function should be passed a list or tuple, to which the identiﬁer coef
is then bound. Here coef[i] represents ai .
The second method evaluate() evaluates p( x ) from x and the coefﬁcients. We are
using the built-in function enumerate(), which allows us to step through the i, X[i]
pairs of any list X. The third method is differentiate(), which modiﬁes the data

Introduction to Programming

31

Listing 2.1 (polyclass0.py) A polynomial class in pseudo-Python
c l a s s Polynomial :
d e f initialize ( coef ) :
" " " Creates an instance p of the Polynomial class ,
where p ( x ) = coef [0] x ^0 + ... + coef [ N ] x ^ N . " " "
d e f evaluate ( x ) :
y = sum ( a * x ** i f o r i , a i n enumerate ( coef ) )
return y
d e f differentiate () :
new_coef = [ i * a f o r i , a i n enumerate ( coef ) ]
# Remove the first element , which is zero
d e l new_coef [0]
# And reset coefficients data to new values
coef = new_coef

of a Polynomial instance, rebinding coef from ( a0 , . . . , a N ) to ( a1 , 2a2 , . . . , Na N ). The
modiﬁed instance represents p .
Now that we have written up an outline of a class deﬁnition in pseudo-Python,
let’s rewrite it in proper Python syntax. The modiﬁed code is given in listing 2.2.
Before working through the additional syntax, let’s look at an example of how to use
the class, which is saved in a ﬁle called polyclass.py in the current working directory:
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>

from polyclass i m p o r t
data = [2 , 1 , 3]
p = Polynomial ( data )
p . evaluate (1)
p . coef
p . differentiate ()
p . coef
p . evaluate (1)

Polynomial
#
#
#
#
#
#

Creates instance of Polynomial class
Returns 6
Returns [2 , 1 , 3]
Modifies coefficients of p
Returns [1 , 6]
Returns 7

The ﬁlename polyclass.py becomes the name of the module (with the “.py” extension omitted), and from it we import our class Polynomial. An instance p is created
by a call of the form p = Polynomial(data). Behind the scenes this generates a call
to the constructor method, which realizes the instance as an object stored in memory,
and binds the name p to this instance. As part of this process a namespace for the
object is created, and the name coef is registered in that namespace and bound to the

32

Chapter 2

Listing 2.2 (polyclass.py) A polynomial class, correct syntax
c l a s s Polynomial :
d e f __init__ ( self , coef ) :
" " " Creates an instance p of the Polynomial class ,
where p ( x ) = coef [0] x ^0 + ... + coef [ N ] x ^ N . " " "
self . coef = coef
d e f evaluate ( self , x ) :
y = sum ( a * x ** i f o r i , a i n enumerate ( self . coef ) )
return y
d e f differentiate ( self ) :
new_coef = [ i * a f o r i , a i n enumerate ( self . coef ) ]
# Remove the first element , which is zero
d e l new_coef [0]
# And reset coefficients data to new values
self . coef = new_coef

data [2, 1, 3].21 The attributes of p can be accessed using p.attribute notation,
where the attributes are the methods (in this case evaluate() and differentiate())
and instance variables (in this case coef).
Let’s now walk through the new syntax in listing 2.2. First, the constructor method
is given its correct name, which is __init__. The double underscore notation reminds
us that this is a special Python method—we will meet another example in a moment.
Second, every method has self as its ﬁrst argument, and attributes referred to within
the class deﬁnition are also preceded by self (e.g., self.coef).
The idea with the self references is that they stand in for the name of any instance
that is subsequently created. As one illustration of this, note that calling p.evaluate(1)
is equivalent to calling
>>> Polynomial . evaluate (p , 1)

This alternate syntax is more cumbersome and not generally used, but we can see how
p does in fact replace self, passed in as the ﬁrst argument to the evaluate() method.
And if we imagine how the evaluate() method would look with p instead of self,
our code starts to appear more natural:
21 To

view the contents of this namespace type p.__dict__ at the prompt.

Introduction to Programming

33

d e f evaluate (p , x ):
y = sum ( a * x ** i f o r i , a i n enumerate ( p . coef ))
return y

Before ﬁnishing, let’s brieﬂy discuss another useful special method. One rather ungainly aspect of the Polynomial class is that for a given instance p corresponding
to a polynomial p, the value p( x ) is obtained via the call p.evaluate(x). It would
be nicer—and closer to the mathematical notation—if we could replace this with the
syntax p(x). Actually this is easy: we simply replace the word evaluate in listing 2.2
with __call__. Objects of this class are now said to be callable, and p(x) is equivalent
to p.__call__(x).

2.3

Commentary

Python was developed by Guido van Rossum, with the ﬁrst release in 1991. It is now
one of the major success stories of the open source model, with a vibrant community
of users and developers. van Rossum continues to direct the development of the language under the title of BDFL (Benevolent Dictator For Life). The use of Python has
increased rapidly in recent years.
There are many good books on Python programming. A gentle introduction is
provided by Zelle (2003). A more advanced book focusing on numerical methods is
Langtangen (2008). However, the best place to start is on the Internet. The Python
homepage (python.org) has links to the ofﬁcial Python documentation and various
tutorials. Links, lectures, MATLAB code, and other information relevant to this chapter can be found on the text home page, at http://johnstachurski.net/book.html.
Computational economics is a rapidly growing ﬁeld. For a sample of the literature,
consult Amman et al. (1996), Heer and Maussner (2005), Kendrick et al. (2005), or
Tesfatsion and Judd (2006).

Chapter 3

Analysis in Metric Space
Metric spaces are sets (spaces) with a notion of distance between points in the space
that satisﬁes certain axioms. From these axioms we can deduce many properties relating to convergence, continuity, boundedness, and other concepts needed for the study
of dynamics. Metric space theory provides both an elegant and powerful framework
for analyzing the kinds of problems we wish to consider, and a great sandpit for playing with analytical ideas: A careful read of this chapter should strengthen your ability
to read and write proofs.
The chapter supposes that you have at least some exposure to introductory real
analysis or advanced calculus. A review of this material is given in appendix A. On
the other hand, if you are already familiar with the fundamentals of metric spaces,
then the best approach is to skim through this chapter quickly and return as necessary.

3.1

A First Look at Metric Space

Consider the set k , a typical element of which is x = ( x1 , . . . , xk ), where xi ∈ .
These elements are also called vectors. There are a number of important “topological”
notions we need to introduce for k . These notions concern sets and functions on or
into such space. In order to introduce them, it is convenient to begin with the concept
of euclidean distance between vectors: Deﬁne d2 : k × k → by

d2 ( x, y) :=:  x − y2 :=

k

∑ ( xi − yi )

1/2
2

(3.1)

i =1

Doubtless you have met with this notion of distance before. You might know that it
satisﬁes the following three conditions:
35

36

Chapter 3
1. d2 ( x, y) = 0 if and only if x = y,
2. d2 ( x, y) = d2 (y, x ), and
3. d2 ( x, y) ≤ d2 ( x, v) + d2 (v, y).

for any x, y, v ∈ k . The ﬁrst property says that a point is at zero distance from
itself, and also that distinct points always have positive distance. The second property
is symmetry, and the third—the only one that is not immediately apparent—is the
triangle inequality.
These three properties are fundamental to our understanding of distance. In fact
if you look at the proofs of many important results—for example, the proof that every
continuous function f from a closed bounded subset of k to has a maximizer and
a minimizer—you will notice that no other properties of d2 are actually used.
Now it turns out that there are many other “distance” functions we can impose
on k that also satisfy properties 1–3. Any proof for the euclidean (i.e., d2 ) case that
only uses properties 1–3 continues to hold for other distances, and in certain problems
alternative notions of distance are easier to work with. This motivates us to generalize
the concept of distance in k .
While we are generalizing the notion of distance between vectors in k , it is worth
thinking about distance between other kinds of objects. If we could deﬁne the distance
between two (inﬁnite) sequences, or between a pair of functions, or two probability
distributions, we could then give a deﬁnition for things like the “convergence” of
distributions discussed informally in chapter 1.

3.1.1

Distances and Norms

Here is the key deﬁnition:
Deﬁnition 3.1.1 A metric space is a nonempty set S and a metric or distance ρ : S × S →
such that, for any x, y, v ∈ S,
1. ρ( x, y) = 0 if and only if x = y,
2. ρ( x, y) = ρ(y, x ), and
3. ρ( x, y) ≤ ρ( x, v) + ρ(v, y).
Apart from being nonempty, the set S is completely arbitrary. In the context of
a metric space the elements of the set are usually called points. As in the case of
euclidean distance, the third axiom is called the triangle inequality.
An immediate consequence of the axioms in deﬁnition 3.1.1 (which are sometimes
referred to as the Hausdorff postulates) is that ρ( x, y) ≥ 0 for any x, y ∈ S. To see this,

Analysis in Metric Space

37

note that if x and y are any two points in S, then 0 = ρ( x, x ) ≤ ρ( x, y) + ρ(y, x ) =
ρ( x, y) + ρ( x, y) = 2ρ( x, y). Hence ρ( x, y) ≥ 0 as claimed.
The space ( k , d2 ) is a metric space, as discussed above. The most important case
is k = 1, when d2 ( x, y) reduces to | x − y| for x, y ∈ . The notation ( , | · |) will be used
to denote this one-dimensional space.
Many additional metric spaces on k are generated by what is known as a norm:
Deﬁnition 3.1.2 A norm on
x, y ∈ k and any γ ∈ ,

k

k

is a mapping

x → x ∈

such that, for any

1.  x  = 0 if and only if x = 0,
2. γx  = |γ| x , and
3.  x + y ≤  x  + y.
Each norm  ·  on

k

generates a metric ρ on

k

via ρ( x, y) :=  x − y.

Exercise 3.1.3 Show that ρ is indeed a metric, in the sense that it satisﬁes the three
axioms in deﬁnition 3.1.1.
Exercise 3.1.4 Let  ·  be a norm on
y| ≤  x − y.

k.

Show that for any x, y ∈

k

we have | x  −

The most important norm on k is  x 2 := (∑ik=1 xi2 )1/2 , which generates the
euclidean distance d2 . A class of norms that includes  · 2 as a special case is the
family  ·  p deﬁned by

1/p

 x  p :=

k

∑ | xi | p

(x ∈

i =1

k

)

(3.2)

where p ≥ 1. It is standard to admit p = ∞ in this family, with  x ∞ := max1≤i≤k | xi |.
Proving that  ·  p is indeed a norm on k for arbitrary p ≥ 1 is not difﬁcult, but
neither is it entirely trivial. In particular, establishing the triangle inequality requires
the services of Minkowski’s inequality. The latter is found in any text covering norms
and is omitted.
Exercise 3.1.5 Prove that  ·  p is a norm on

k

for p = 1 and p = ∞.

The class of norms  ·  p gives rise to the class of metric spaces ( k , d p ), where
d p ( x, y) :=  x − y p for all x, y ∈ k .
So far all our spaces have involved different metrics on ﬁnite-dimensional vector
space. Next let’s consider an example of a “function space.” Let U be any set, let bU
be the collection of all bounded functions f : U → (i.e., supx∈U | f ( x )| < ∞), and let
d∞ ( f , g) :=:  f − g∞ := sup | f ( x ) − g( x )|
x ∈U

(3.3)

38

Chapter 3
x1
x2

{y | ρ( x, y) < }

xN

x

Figure 3.1 Limit of a sequence

The space (bU, d∞ ) is a metric space. The reader can verify the ﬁrst two properties
of the deﬁnition of a metric space. The triangle inequality is veriﬁed as follows. Let
f , g, h ∈ bU. Pick any x ∈ U. We have

| f ( x ) − g( x )| ≤ | f ( x ) − h( x )| + |h( x ) − g( x )| ≤ d∞ ( f , h) + d∞ (h, g)
Since x is arbitrary, we obtain d∞ ( f , g) ≤ d∞ ( f , h) + d∞ (h, g).1

3.1.2

Sequences

Let S = (S, ρ) be a metric space. A sequence ( xn ) ⊂ S is said to converge to x ∈ S
if, for all  > 0, there exists an N ∈
such that n ≥ N implies ρ( xn , x ) < . In
other words ( xn ) converges to x if and only if the real sequence ρ( xn , x ) → 0 in as
n → ∞ (see §A.2 for more on real sequences). If this condition is satisﬁed, we write
limn→∞ xn = x, or xn → x. The point x is referred to as the limit of the sequence.
Figure 3.1 gives an illustration for the case of two-dimensional euclidean space.
Theorem 3.1.6 A sequence in (S, ρ) can have at most one limit.
Proof. You might like to try a proof by contradiction as an exercise. Here is a direct
proof. Let ( xn ) be an arbitrary sequence in S, and let x and x be two limit points. We
have
0 ≤ ρ( x, x ) ≤ ρ( x, xn ) + ρ( xn , x )
∀n ∈
1 As an aside, you may have noticed that the metric space ( bU, d ) seems to be deﬁned by a “norm”
∞
 f ∞ := supx∈U | f ( x )|. This is not a norm in the sense of deﬁnition 3.1.2, as that deﬁnition requires that the
underlying space is k , rather than bU. However, more general norms can be deﬁned for abstract “vector
space,” and  · ∞ is a prime example. The details are omitted: See any text on functional analysis.

Analysis in Metric Space

39

f +

f

f −

a

b

Figure 3.2 An -ball for d∞

From theorems A.2.12 and A.2.13 (page 330) we have ρ( x, x ) = 0. Therefore x = x .
(Why?)
Exercise 3.1.7 Let ( xn ) and (yn ) be sequences in S. Show that if xn → x ∈ S and
ρ( xn , yn ) → 0, then yn → x.
One of the most important creatures deﬁned from the distance function is the humble open ball. The open ball or -ball B(; x ) centered on x ∈ S with radius  > 0 is the
set
B(; x ) := {z ∈ S : ρ(z, x ) < }
In the plane with ρ = d2 the -ball is a circle; in
visualization of the -ball around f ∈ (b[ a, b], d∞ ).

3

it is a sphere. Figure 3.2 gives a

Exercise 3.1.8 Let ( xn ) ⊂ S and x ∈ S. Show that xn → x if and only if for all  > 0,
the ball B(; x ) contains all but ﬁnitely many terms of ( xn ).
A subset E of S is called bounded if E ⊂ B(n; x ) for some x ∈ S and some (suitably
large) n ∈ . A sequence ( xn ) in S is called bounded if its range { xn : n ∈ } is a
bounded set.
Exercise 3.1.9 Show that every convergent sequence in S is also bounded.
Given sequence ( xn ) ⊂ S, a subsequence is deﬁned analogously to the case of real
sequences: (yn ) is called a subsequence of ( xn ) if there is a strictly increasing function
f:
→ such that yn = x f (n) for all n ∈ . It is common to use the notation ( xnk )
to denote a subsequence of ( xn ).

40

Chapter 3

Exercise 3.1.10 Show that for ( xn ) ⊂ S, xn → x for some x ∈ S if and only if every
subsequence of ( xn ) converges to x.
For the euclidean space (

k, d

2)

we have the following result:

Lemma 3.1.11 A sequence ( xn ) = ( xn1 , . . . , xnk ) in ( k , d2 ) converges to x = ( x1 , . . . , x k ) ∈
k if and only if x j → x j in
= ( , | · |) for all j in 1, . . . , k.
n
j

Proof. For j in 1, . . . , k we have | xn − x j | ≤ d2 ( xn , x ). (Why?) Hence if d2 ( xn , x ) → 0,
j
then | xn − x j | → 0 for each j. For the converse, ﬁx  > 0 and choose for each j in
√
j
1, . . . , k an N j ∈
such that n ≥ N j implies | xn − x j | < / k. Now n ≥ max j N j
implies d2 ( xn , x ) ≤ . (Why?)
Lemma 3.1.11 is important, and you should try sketching it for the case k = 2 to
build intuition. We will see that in fact the same result holds not just for d2 , but for the
metric induced by any norm on k .
Let S and Y be two metric spaces. Parallel to §A.2.3, deﬁne f : S ⊃ A → Y to be
continuous at a ∈ A if for every sequence ( xn ) in A converging to a we have f ( xn ) →
f ( a) in Y, and continuous on A whenever it is continuous at every a ∈ A. For the same
f : A → Y and for a ∈ A, we say that y = limx→ a f ( x ) if f ( xn ) → y for every sequence
( xn ) ⊂ A with xn → a. Clearly, f is continuous at a if and only if limx→a f ( x ) = f ( a).
Example 3.1.12 Let S be a metric space, and let x̄ be any given point in S. The map
S x → ρ( x, x̄ ) ∈
is continuous on all of S. To see this, pick any x ∈ S, and any
( xn ) ⊂ S with xn → x. Two applications of the triangle inequality yield
ρ( x, x̄ ) − ρ( xn , x ) ≤ ρ( xn , x̄ ) ≤ ρ( xn , x ) + ρ( x, x̄ )

∀n ∈

Now take the limit (i.e., apply theorem A.2.12 on page 330).
Exercise 3.1.13 Let f ( x, y) = x2 + y2 . Show that f is a continuous function from
( 2 , d2 ) into ( , | · |).2
Throughout the text, if S is some set, f : S → , and g : S → , then f + g denotes
the function x → f ( x ) + g( x ) on S, while f g denotes the function x → f ( x ) g( x ) on S.
Exercise 3.1.14 Let f and g be as above, and let S be a metric space. Show that if f
and g are continuous, then so are f + g and f g.
Exercise 3.1.15 A function f : S → is called upper-semicontinuous (usc) at x ∈ S if,
for every xn → x, we have lim supn f ( xn ) ≤ f ( x ); and lower-semicontinuous (lsc) if,
for every xn → x, we have lim infn f ( xn ) ≥ f ( x ). Show that f is usc at x if and only if
− f is lsc at x. Show that f is continuous at x if and only if it is both usc and lsc at x.
2 Hint:

Use lemma 3.1.11.

Analysis in Metric Space

3.1.3

41

Open Sets, Closed Sets

Arbitrary subsets of arbitrary spaces can be quite unruly. It is useful to identify
classes of sets that are well-behaved, interacting nicely with common functions, and
co-operating with attempts to measure them, or to represent them in terms of simpler
elements. In this section we investigate a class of sets called the open sets, as well as
their complements the closed sets.
Let’s say that x ∈ S adheres to E ⊂ S if, for each  > 0, the ball B(; x ) contains at
least one point of E;3 and that x is interior to E if B(; x ) ⊂ E for some  > 0.4 A set E ⊂
S is called open if all points in E are interior to E, and closed if E contains all points that
adhere to E. In the familiar metric space ( , | · |), canonical examples are the intervals
( a, b) and [ a, b], which are open and closed respectively.5 The concepts of open and
closed sets turn out to be some of the most fruitful ideas in all of mathematics.
Exercise 3.1.16 Show that a point in S adheres to E ⊂ S if and only if it is the limit of
a sequence contained in E.
Theorem 3.1.17 A set F ⊂ S is closed if and only if for every convergent sequence entirely
contained in F, the limit of the sequence is also in F.
Proof. Do the proof as an exercise if you can. If not, here goes. Suppose that F is
closed, and take a sequence in F converging to some point x ∈ S. Then x adheres to F
by exercise 3.1.16, and is therefore in F by deﬁnition. Suppose, on the other hand, that
the limit of every convergent sequence in F belongs to F. Take any x ∈ S that adheres
to F. By exercise 3.1.16, there is a sequence in F converging to it. Therefore x ∈ F, and
F is closed.
Open sets and closed sets are closely related. In fact we have the following fundamental theorem:
Theorem 3.1.18 A subset of an arbitrary metric space S is open if and only if its complement
is closed, and closed if and only if its complement is open.
Proof. The proof is a good exercise. If you need a start, here is a proof that G open
implies F := G c closed. Take ( xn ) ⊂ F with xn → x ∈ S. We wish to show that x ∈ F.
In fact this must be the case because, if x ∈
/ F, then x ∈ G, in which case there is an
 > 0 such that B(, x ) ⊂ G. (Why?) Such a situation is not possible when ( xn ) ⊂ F
and xn → x. (Why?)
3 In

some texts, x is said to be a contact point of E.
sketching some examples for the case of ( 2 , d2 ).
5 If you ﬁnd it hard to verify this now, you won’t by the end of the chapter.
4 Try

42

Chapter 3

We call D (; x ) := {z ∈ S : ρ(z, x ) ≤ } the closed -ball centered on x. Every
D (; x ) ⊂ S is a closed set, as anticipated by the notation. To see this, take ( an ) ⊂
D (; x ) converging to a ∈ S. We need to show that a ∈ D (; x ) or, equivalently, that
ρ( a, x ) ≤ . Since ρ( an , x ) ≤  for all n ∈ , since limits preserve orders and since
y → ρ(y, x ) is continuous, we have ρ( a, x ) = lim ρ( an , x ) ≤ .
Exercise 3.1.19 Likewise every open ball B(; x ) in S is an open set. Prove this directly,
or repeat the steps of the previous example applied to B(; x )c .
You will not ﬁnd it difﬁcult to convince yourself that if (S, ρ) is any metric space,
then the whole set S is itself both open and closed. (Just check the deﬁnitions carefully.) This can lead to some confusion. For example, suppose that we consider the
metric space (S, | · |), where S = (0, 1). Since (0, 1) is the whole space, it is closed. At
the same time, (0, 1) is open as a subset of ( , | · |). The properties of openness and
closedness are relative rather than absolute.
Exercise 3.1.20 Argue that for any metric space (S, ρ), the empty set ∅ is both open
and closed.
Exercise 3.1.21 Show that if (S, ρ) is an arbitrary metric space, and if x ∈ S, then the
set { x } is always closed.
Theorem 3.1.22 If F is a closed, bounded subset of ( , | · |), then sup F ∈ F.
Proof. Let s := sup F. Since F is closed it is sufﬁcient to show there exists a sequence
( xn ) ⊂ F with xn → s. (Why?) By lemma A.2.19 (page 332) such a sequence exists.
Exercise 3.1.23 Prove that a sequence converges to a point x if and only if the sequence
is eventually in every open set containing x.
Exercise 3.1.24 Prove: If { Gα }α∈ A are all open, then so is ∪α∈ A Gα .
Exercise 3.1.25 Show that if A is ﬁnite and { Gα }α∈ A is a collection of open sets, then
∩α∈ A Gα is also open.
In other words, arbitrary unions and ﬁnite intersections of open sets are open. But
be careful: An inﬁnite intersection of open sets is not necessarily open. For example,
consider the metric space ( , | · |). If Gn = (−1/n, 1/n), then ∩n∈ Gn = {0} because
x ∈ ∩n Gn ⇐⇒ −

1
1
<x<
n
n

∀n ∈

⇐⇒ x = 0

Exercise 3.1.26 Show that ∩n∈ ( a − 1/n, b + 1/n) = [ a, b].
Exercise 3.1.27 Prove that if { Fα }α∈ A are all closed, then so is ∩α∈ A Fα .

Analysis in Metric Space

43

Exercise 3.1.28 Show that if A is ﬁnite and { Fα }α∈ A is a collection of closed sets, then
∪α∈ A Fα is closed. On the other hand, show that the union ∪n∈ [ a + 1/n, b − 1/n] =
( a, b). (Why is this not a contradiction?)
Exercise 3.1.29 Show that G ⊂ S is open if and only if it can be formed as the union
of an arbitrary number of open balls.
The closure of E is the set of all points that adhere to E, and is written cl E. In view
of exercise 3.1.16, x ∈ cl E if and only if there exists a sequence ( xn ) ⊂ E with xn → x.
The interior of E is the set of its interior points, and is written int E.
Exercise 3.1.30 Show that cl E is always closed. Show in addition that for all closed
sets F such that F ⊃ E, cl E ⊂ F. Using this result, show that cl E is equal to the
intersection of all closed sets containing E.
The last exercise tells us that the closure of a set is the smallest closed set that
contains that particular set. The next one shows us that the interior of a set is the
largest open set contained in that set.
Exercise 3.1.31 Show that int E is always open. Show also that for all open sets G such
that G ⊂ E, int E ⊃ G. Using this result, show that int E is equal to the union of all
open sets contained in E.
Exercise 3.1.32 Show that E = cl E if and only if E is closed. Show that E = int E if
and only if E is open.
Open sets and continuous functions interact very nicely. For example, we have the
following fundamental theorem.
Theorem 3.1.33 A function f : S → Y is continuous if and only if the preimage f −1 ( G ) of
every open set G ⊂ Y is open in S.
Proof. Suppose that f is continuous, and let G be any open subset of Y. If x ∈ f −1 ( G ),
then x must be interior, for if it is not, then there is a sequence xn → x where xn ∈
/
−
1
f ( G ) for all n. But, by continuity, f ( xn ) → f ( x ), implying that f ( x ) ∈ G is not
interior to G. (Why?) Contradiction.
Conversely, suppose that the preimage of every open set is open, and take any
{ xn }n≥1 ∪ { x } ⊂ S with xn → x. Pick any -ball B around f ( x ). The preimage f −1 ( B)
is open, so for N sufﬁciently large we have xn ∈ f −1 ( B) for all n ≥ N, in which case
f ( xn ) ∈ B for all n ≥ N.
Exercise 3.1.34 Let S, Y, and Z be metric spaces, and let f : S → Y and g : Y → Z.
Show that if f and g are continuous, then so is h := g ◦ f .

44

Chapter 3

Exercise 3.1.35 Let S = k , and let ρ∗ ( x, y) = 0 if x = y and 1 otherwise. Prove
that ρ∗ is a metric on k . Which subsets of this space are open? Which subsets are
closed? What kind of functions f : S → are continuous? What kinds of sequences
are convergent?

3.2

Further Properties

Having covered the fundamental ideas of convergence, continuity, open sets and
closed sets, we now turn to two key concepts in metric space theory: completeness
and compactness. After stating the deﬁnitions and covering basic properties, we will
see how completeness and compactness relate to existence of optima and to the theory
of ﬁxed points.

3.2.1

Completeness

A sequence ( xn ) in metric space (S, ρ) is said to be a Cauchy sequence if, for all  > 0,
there exists an N ∈ such that ρ( x j , xk ) <  whenever j ≥ N and k ≥ N . A subset A
of a metric space S is called complete if every Cauchy sequence in A converges to some
point in A. Often the set A of interest is the whole space S, in which case we say that
S is a complete metric space. As discussed in §A.2, the set of reals ( , | · |) has this
property. Many other metric spaces do not.
Notice that completeness is intrinsic to a given set A and a metric ρ on A. Either
every Cauchy sequence in ( A, ρ) converges or there exists a Cauchy sequence that
does not. On the other hand, openness and closedness are relative properties. The set
A := [0, 1) is not open as a subset of ( , | · |), but it is open as a subset of ( + , | · |).
The signiﬁcance of completeness is that when searching for the solution to a problem, it sometimes happens that we are able to generate a Cauchy sequence whose limit
would be a solution if it does in fact exist. In a complete space we can rest assured
that our solution does exist as a well-deﬁned element of the space.
Exercise 3.2.1 Show that a sequence ( xn ) in metric space (S, ρ) is Cauchy if and only
if limn→∞ supk≥n ρ( xn , xk ) = 0.
Exercise 3.2.2 Show that if a sequence ( xn ) in metric space (S, ρ) is convergent, then
it is Cauchy. Show that if ( xn ) is Cauchy, then it is bounded.
Which metric spaces are complete? Observe that while = ( , | · |) is complete,
subsets of may not be. For example, consider the metric space (S, ρ) = ((0, ∞), | ·
|). Some manipulation proves that while ( xn ) = (1/n) is Cauchy in S, it converges
to no point in S. On the other hand, for (S, ρ) = ( + , | · |) the limit point of the

Analysis in Metric Space

45

sequence (1/n) is in S. Indeed this space is complete, as is any closed subset of
More generally,

.

Theorem 3.2.3 Let S be a complete metric space. Subset A ⊂ S is complete if and only if it
is closed as a subset of S.
Proof. Let A be complete. To see that A is closed, let ( xn ) ⊂ A with xn → x ∈ S.
Since ( xn ) is convergent it must be Cauchy (exercise 3.2.2). Because A is complete we
have x ∈ A. Thus A contains its limit points, and is therefore closed. Conversely,
suppose that A is closed. Let ( xn ) be a Cauchy sequence in A. Since S is complete,
( xn ) converges to some x ∈ S. As A is closed, the limit point x must be in A. Hence A
is complete.
The euclidean space (

k, d

2)

is complete. To see this, observe ﬁrst that

Lemma 3.2.4 A sequence ( xn ) = ( xn1 , . . . , xnk ) in (
j
component sequence xn is Cauchy in = ( , | · |).

k, d

2)

is Cauchy if and only if each

The proof of lemma 3.2.4 is an exercise.6 The lemma is important because it implies
that ( k , d2 ) inherits the completeness of (axiom A.2.4, page 328):
Theorem 3.2.5 The euclidean space (

k, d

2)

is complete.

Proof. If ( xn ) is Cauchy in ( k , d2 ), then each component is Cauchy in = ( , | · |),
and, by completeness of , converges to some limit in . It follows from lemma 3.1.11
that ( xn ) is convergent in ( k , d2 ).
Recall that (bU, d∞ ) is the bounded real-valued functions f : U → , endowed
with the distance d∞ deﬁned on page 37. This space also inherits completeness from
:
Theorem 3.2.6 Let U be any set. The metric space (bU, d∞ ) is complete.
Proof. Let ( f n ) ⊂ bU be Cauchy. We claim the existence of a f ∈ bU such that
d∞ ( f n , f ) → 0. To see this, observe that for each x ∈ U we have supk≥n | f n ( x ) −
f k ( x )| ≤ supk≥n d∞ ( f n , f k ) → 0, and hence ( f n ( x )) is Cauchy (see exercise 3.2.1). By
the completeness of , ( f n ( x )) is convergent, and we deﬁne a new function f ∈ bU
by f ( x ) = limn→∞ f n ( x ).7
To show that d∞ ( f n , f ) → 0, ﬁx  > 0, and choose N ∈ such that d∞ ( f n , f m ) <
/2 whenever n, m ≥ N. Pick any n ≥ N. For arbitrary x ∈ U we have | f n ( x ) −
f m ( x )| < /2 for all m ≥ n, and hence, taking limits with respect to m, we have
| f n ( x ) − f ( x )| ≤ /2. Since x was arbitrary, d∞ ( f n , f ) ≤ /2 < .
6 Hint:
7 Why

You might like to begin by rereading the proof of lemma 3.1.11.
is f ∈ bU (i.e., why is f bounded on U)? Consult exercise 3.2.2.

46

Chapter 3

This is a good opportunity to brieﬂy discuss convergence of functions. A sequence
converges pointwise to f : U →
if
of functions ( f n ) from arbitrary set U into
| f n ( x ) − f ( x )| → 0 as n → ∞ for every x ∈ U; and uniformly if d∞ ( f n , f ) → 0.
Pointwise convergence is certainly important, but it is also signiﬁcantly weaker than
convergence in d∞ . For example, suppose that U is a metric space, that f n → f , and
that all f n are continuous. It might then be hoped that the limit f inherits continuity
from the approximating sequence. For pointwise convergence this is not generally
true,8 while for uniform convergence it is:
Theorem 3.2.7 Let ( f n ) and f be real-valued functions on metric space U. If f n is continuous
on U for all n and d∞ ( f n , f ) → 0, then f is also continuous on U.
Proof. Take ( xk ) ⊂ U with xk → x̄ ∈ U. Fix  > 0. Choose n ∈ such that | f n ( x ) −
f ( x )| < /2 for all x ∈ U. For any given k ∈ the triangle inequality gives

| f ( xk ) − f ( x̄ )| ≤ | f ( xk ) − f n ( xk )| + | f n ( xk ) − f n ( x̄ )| + | f n ( x̄ ) − f ( x̄ )|
∴

| f ( xk ) − f ( x̄ )| ≤ | f n ( xk ) − f n ( x̄ )| + 

(k ∈

)

From exercise A.2.28 (page 333) we have 0 ≤ lim supk | f ( xk ) − f ( x̄ )| ≤ . Since  is
arbitrary, lim supk | f ( xk ) − f ( x̄ )| = limk | f ( xk ) − f ( x̄ )| = 0.
Now let’s introduce another important metric space.
Deﬁnition 3.2.8 Given any metric space U, let (bcU, d∞ ) be the continuous functions
in bU endowed with the same metric d∞ .
Theorem 3.2.9 The space (bcU, d∞ ) is always complete.
Proof. This follows from theorem 3.2.3 (closed subsets of complete spaces are complete), theorem 3.2.6 (the space (bU, d∞ ) is complete) and theorem 3.2.7 (which implies
that the space bcU is closed as a subset of bU).

3.2.2

Compactness

Now we turn to the notion of compactness. A subset K of S = (S, ρ) is called precompact in S if every sequence contained in K has a subsequence that converges to a point
of S. The set K is called compact if every sequence contained in K has a subsequence
that converges to a point of K. (Thus every compact subset of S is precompact in S,
and every closed precompact set is compact.) Compactness will play a major role in
our analysis. As we will see, the existence of a converging subsequence often allows
us to track down the solution to a difﬁcult problem.
8A

counterexample is U = [0, 1], f n ( x ) = x n , f ( x ) = 0 on [0, 1) and f (1) = 1.

Analysis in Metric Space

47

As a ﬁrst step, note that there is another important characterization of compactness, which at ﬁrst sight bears little resemblance to the sequential deﬁnition above. To
state the theorem, recall that for a set K in S, an open cover is a collection { Gα } of open
subsets of S such that K ⊂ ∪α Gα . The cover is called ﬁnite if it consists of only ﬁnitely
many sets.
Theorem 3.2.10 A subset K of an arbitrary metric space S is compact if and only if every
open cover of K can be reduced to a ﬁnite cover.
In other words, a set K is compact if and only if, given any open cover, we can discard all but a ﬁnite number of elements and still cover K. The proof of theorem 3.2.10
can be found in any text on real analysis.
Exercise 3.2.11 Exhibit an open cover of k that cannot be reduced to a ﬁnite subcover.
Construct a sequence in k with no convergent subsequence.
Exercise 3.2.12 Use theorem 3.2.10 to prove that every compact subset of a metric
space S is bounded (i.e., can be contained in an open ball B(n; x ) for some x ∈ S and
some (suitably large) n ∈ ).
Exercise 3.2.13 Prove that every compact subset of a metric space is closed.
On the other hand, closed and bounded subsets of metric spaces are not always
compact.
Exercise 3.2.14 Let (S, ρ) = ((0, ∞), | · |), and let K = (0, 1]. Show that although K is a
closed, bounded subset of S, it is not precompact in S.
Exercise 3.2.15 Show that every subset of a compact set is precompact, and every
closed subset of a compact set is compact.
Exercise 3.2.16 Show that in any metric space the intersection of an arbitrary number
of compact sets and the union of a ﬁnite number of compact sets are again compact.
Exercise 3.2.17 For a more advanced exercise, you might like to try to show that
the closure of a precompact set is compact. It follows that every precompact set is
bounded. (Why?)
When it comes to precompactness and compactness, the space (
special. For example, the Bolzano–Weierstrass theorem states that
Theorem 3.2.18 Every bounded sequence in euclidean space (
vergent subsequence.

k, d

2)

k, d

2)

is rather

has at least one con-

48

Chapter 3

Proof. Let’s check the case k = 2. Let ( xn ) = ( xn1 , xn2 ) ⊂ ( 2 , d2 ) be bounded. Since
( xn1 ) is itself bounded in (why?), we can ﬁnd a sequence n1 , n2 , . . . =: (n j ) such that
( xn1 j ) converges in (theorem A.2.10, page 330). Now consider ( xn2 j ). This sequence is
also bounded, and must itself have a convergent subsequence, so if we discard more
terms from n1 , n2 , . . . =: (n j ) we can obtain a subsubsequence (ni ) ⊂ (n j ) such that
( xn2 i ) converges. Since (ni ) ⊂ (n j ), the sequence ( xn1 i ) also converges. It follows from
lemma 3.1.11 (page 40) that ( xni ) converges in ( k , d2 ).
The next result (called the Heine–Borel theorem) follows directly.
Theorem 3.2.19 A subset of ( k , d2 ) is precompact in (
and compact if and only if it is closed and bounded.

k, d

2)

if and only if it is bounded,

As we have seen, some properties of ( k , d2 ) carry over to arbitrary metric spaces,
while others do not. For example, we saw that in an arbitrary metric space, closed
and bounded sets are not necessarily compact. (This has important implications for
the theory of Markov chains developed below.) However, we will see in §3.2.3 that
any metric d on k induced by a norm (see deﬁnition 3.1.2 on page 37) is “equivalent”
to d2 , and that, as a result, subsets of ( k , d) are compact if and only if they are closed
and bounded.

3.2.3

Optimization, Equivalence

Optimization is important not only to economics, but also to statistics, numerical computation, engineering, and many other ﬁelds of science. In economics, rationality is
the benchmark assumption for agent behavior, and is usually imposed by requiring
agents solve optimization problems. In statistics, optimization is used for maximum
likelihood and other related procedures, which search for the “best” estimator in some
class. For numerical methods and approximation theory, one often seeks a simple representation f n of a given function f that is the “closest” to f in some suitable metric
sense.
In any given optimization problem the ﬁrst issue we must confront is whether or
not optima exist. For example, a demand function is usually deﬁned as the solution
to a consumer optimization problem. It would be awkward then if no solution to
the problem exists. The same can be said for supply functions, or for policy functions,
which return the optimal action of a “controller” faced with a given state of the world.
Discussions of existence typically begin with the following theorem:
Theorem 3.2.20 Let f : S → Y, where S and Y are metric spaces and f is continuous. If
K ⊂ S is compact, then so is f (K ), the image of K under f .

Analysis in Metric Space

49

Proof. Take an open cover of f (K ). The preimage of this cover under f is an open
cover of K (recall theorem 3.1.33 on page 43). Since K is compact we can reduce this to
a ﬁnite cover (theorem 3.2.10). The image of this ﬁnite cover under f contains f (K ),
and hence f (K ) is compact.
Exercise 3.2.21 Give another proof of theorem 3.2.20 using the sequential deﬁnitions
of compactness and continuity.
The following theorem is one of the most fundamental results in optimization theory. It says that in the case of continuous functions on compact domains, optima
always exist.
Theorem 3.2.22 (Weierstrass) Let f : K → , where K is a subset of arbitrary metric space
(S, ρ). If f is continuous and K is compact, then f attains its supremum and inﬁmum on K.
In other words, α := sup f (K ) exists, and, moreover, there is an x ∈ K such that
f ( x ) = α. A corresponding result holds for the inﬁmum.
Proof. Regarding suprema, the result follows directly from theorem 3.2.20 combined
with theorem 3.1.22 (page 42). By these theorems you should be able to show that
α := sup f (K ) exists, and, moreover, that α ∈ f (K ). By the deﬁnition of f (K ), there is
an x ∈ K such that f ( x ) = α. This proves the assertion regarding suprema. The proof
of the assertion regarding inﬁma is similar.
In general, for f : S → , a value y ∈
is called the maximum of f on A ⊂ S if
f ( x ) ≤ y for all x ∈ A and f ( x̄ ) = y for some x̄ ∈ A. At most one maximum exists.
The maximizers of f on A are the points
argmax f ( x ) := { x ∈ A : f ( x ) = y} = { x ∈ A : f (z) ≤ f ( x ) for all z ∈ A}
x∈ A

Minima and minimizers are deﬁned in a similar way.
With this notation, we can restate theorem 3.2.22 as follows: If K is compact and
f: K →
is continuous, then K contains at least one maximizer and one minimizer
of f on K. (Convince yourself that this is so.)
Exercise 3.2.23 Let f : K → , where K is compact and f is continuous. Show that if
f is strictly positive on K, then inf f (K ) is strictly positive.
As an application of theorem 3.2.22, let’s show that all norms on k induce essentially the same metric space. We begin with a deﬁnition: Let S be a nonempty set,
and let ρ and ρ be two metrics on S. We say that ρ and ρ are equivalent if there exist
constants K and J such that
ρ( x, y) ≤ Kρ ( x, y) and ρ ( x, y) ≤ Jρ( x, y)

for any x, y ∈ S

(3.4)

50

Chapter 3

The notion of equivalence is important because equivalent metrics share the same
convergent sequences and Cauchy sequences, and the metric spaces (S, ρ) and (S, ρ )
share the same open sets, closed sets, compact sets and bounded sets:
Lemma 3.2.24 Let ρ and ρ be equivalent on S, and let ( xn ) ⊂ S. The sequence ( xn ) ρconverges to x ∈ S if and only if it ρ -converges to x.9
Proof. If ρ( xn , x ) → 0, then ρ ( xn , x ) ≤ Jρ( xn , x ) → 0, and so forth.
Exercise 3.2.25 Let ρ and ρ be equivalent on S, and let ( xn ) ⊂ S. Show that ( xn ) is
ρ-Cauchy if and only if it is ρ -Cauchy.10
Exercise 3.2.26 Let ρ and ρ be equivalent on S, and let A ⊂ S. Show that A is ρcomplete if and only if it is ρ -complete.
Exercise 3.2.27 Let ρ and ρ be equivalent on S. Show that (S, ρ) and (S, ρ ) share the
same closed sets, open sets, bounded sets and compact sets.
Exercise 3.2.28 Let ρ and ρ be equivalent on S, and let f : S →
that f is ρ-continuous if and only if it is ρ -continuous.

= ( , | · |). Show

Exercise 3.2.29 Let S be any nonempty set, and let ρ, ρ , and ρ be metrics on S.
Show that equivalence is transitive, in the sense that if ρ is equivalent to ρ and ρ is
equivalent to ρ , then ρ is equivalent to ρ .
Theorem 3.2.30 All metrics on

k

induced by a norm are equivalent.

Proof. The claim is that if  ·  and  ·  are any two norms on k (see deﬁnition 3.1.2
on page 37), and ρ and ρ are deﬁned by ρ( x, y) :=  x − y and ρ ( x, y) :=  x − y ,
then ρ and ρ are equivalent. In view of exercise 3.2.29, it is sufﬁcient to show that any
one of these metrics is equivalent to d1 . To check this, it is sufﬁcient (why?) to show
that if  ·  is any norm on k , then there exist constants K and J such that

 x  ≤ K  x 1 and  x 1 ≤ J  x 

for any x ∈

k

(3.5)

To check the ﬁrst inequality, let e j be the j-th basis vector in k (i.e., the j-th component
of vector e j is 1 and all other components are zero). Let K := max j e j . Then for any
x ∈ k we have

 x  =  x 1 e1 + · · · x k e k  ≤
9 Here
10 Hint:

k

k

j =1

j =1

∑  x j e j  = ∑ |x j |e j  ≤ K x1

ρ-convergence means convergence in (S, ρ), etc., etc.
Try a proof using exercise 3.2.1 (page 44).

Analysis in Metric Space
To check the second inequality in (3.5), observe that x →  x  is continuous on (
because if xn → x in d1 , then

51
k, d

1)

| xn  −  x | ≤  xn − x  ≤ K  xn − x 1 → 0 (n → ∞)
Now consider the set E := { x ∈ k :  x 1 = 1}. Some simple alterations to theorem 3.2.19 (page 48) and the results that lead to it show that, just as for the case
of ( k , d2 ), closed and bounded subsets of ( k , d1 ) are compact.11 Hence E is d1 compact. It now follows from theorem 3.2.22 that x →  x  attains its minimum on E,
in the sense that there is an x ∗ ∈ E with  x ∗  ≤  x  for all x ∈ E. Clearly,  x ∗  = 0.
(Why?) Now observe that for any x ∈ k we have


 x 

  x 1 ≥  x ∗  x 1
x = 
 x 1 
Setting J := 1/ x ∗  gives the desired inequality.

3.2.4

Fixed Points

Next we turn to ﬁxed points. Fixed point theory tells us how to ﬁnd an x that solves
Tx = x for some given T : S → S.12 Like optimization it has great practical importance. Very often the solutions of problems we study will turn out to be ﬁxed points
of some appropriately constructed function. Of the theorems we treat in this section,
one uses convexity and is due to L. E. J. Brouwer while the other two are contraction
mapping arguments: a famous one due to Stefan Banach and a variation of the latter.
Incidentally, ﬁxed point and optimization problems are closely related. When we
study dynamic programming, an optimization problem will be converted into a ﬁxed
point problem—in the process yielding an efﬁcient means of computation. On the
other hand, if T : S → S has a unique ﬁxed point in metric space (S, ρ), then ﬁnding
that ﬁxed point is equivalent to ﬁnding the minimizer of g( x ) := ρ( Tx, x ).
So let T : S → S, where S is any set. An x ∗ ∈ S is called a ﬁxed point of T on S if
∗
Tx = x ∗ . If S is a subset of , then ﬁxed points of T are those points in S where T
meets the 45 degree line, as illustrated in ﬁgure 3.3.
Exercise 3.2.31 Show that if S =
and T : S → S is decreasing (x ≤ y implies
Tx ≥ Ty), then T has at most one ﬁxed point.
11 Alternatively, you can show directly that ( k , d ) and ( k , d ) are equivalent by establishing (3.5) for
2
1
 ·  =  · 2 . The ﬁrst inequality is already done, and the second follows from the Cauchy–Schwartz
inequality (look it up).
12 It is common in ﬁxed point theory to use upper case symbols like T for the function, and no brackets
around its argument. One reason is that S is often a space of functions, and standard symbols like f and g
are reserved for the elements of S.

52

Chapter 3
45 degree line

T

x ∗ = Tx ∗

Figure 3.3 Fixed points in one dimension

A set S ⊂ k is called convex if for all λ ∈ [0, 1] and a, a ∈ S we have λa + (1 − λ) a ∈
S. Here is Brouwer’s ﬁxed point theorem:
Theorem 3.2.32 Consider the space ( k , d), where d is the metric induced by any norm.13
Let S ⊂ k , and let T : S → S. If T is continuous and S is both compact and convex, then T
has at least one ﬁxed point in S.
The proof is neither easy nor central to these notes, so we omit it14 and move on
to contraction mappings. Let (S, ρ) be a metric space, and let T : S → S. The map T is
called nonexpansive on S if
ρ( Tx, Ty) ≤ ρ( x, y)

∀ x, y ∈ S

(3.6)

∀ x, y ∈ S with x = y

(3.7)

It is called contracting on S if
ρ( Tx, Ty) < ρ( x, y)

It is called uniformly contracting on S with modulus λ if 0 ≤ λ < 1 and
ρ( Tx, Ty) ≤ λρ( x, y)

∀ x, y ∈ S

(3.8)

Exercise 3.2.33 Show that if T is nonexpansive on S then it is also continuous on S
(with respect to the same metric ρ).
13 All

such metrics are equivalent. See theorem 3.2.30.
might like to sketch the case S = [0, 1] to gain some intuition.

14 You

Analysis in Metric Space

53

Exercise 3.2.34 Show that if T is a contraction on S, then T has at most one ﬁxed point
in S.
the notation T n refers to the n-th composition of T with itself, so T n x
For n ∈
means apply T to x, apply T to the result, and so on for n times. By convention, T 0 is
the identity map x → x.15
Exercise 3.2.35 Let T be uniformly contracting on S with modulus λ, and let x0 ∈ S.
Deﬁne xn := T n x0 for n ∈ . Use induction to show that ρ( xn+1 , xn ) ≤ λn ρ( x1 , x0 )
for all n ∈ .
The next theorem is one of the cornerstones of functional analysis:
Theorem 3.2.36 (Banach) Let T : S → S, where (S, ρ) is a complete metric space. If T is a
uniform contraction on S with modulus λ, then T has a unique ﬁxed point x ∗ ∈ S. Moreover
for every x ∈ S and n ∈
we have ρ( T n x, x ∗ ) ≤ λn ρ( x, x ∗ ), and hence T n x → x ∗ as
n → ∞.
Proof. Let λ be as in (3.8). Let xn := T n x0 , where x0 is some point in S. From exercise 3.2.35 we have ρ( xn , xn+1 ) ≤ λn ρ( x0 , x1 ) for all n ∈ , suggesting that the
sequence is ρ-Cauchy. In fact with a bit of extra work one can show that if n, k ∈
and n < k, then ρ( xn , xk ) ≤ ∑ik=−n1 λi ρ( x0 , x1 ).

∴

ρ( xn , xk ) <

λn
ρ ( x0 , x1 )
1−λ

(n, k ∈

with n < k)

Since ( xn ) is ρ-Cauchy, this sequence has a limit x ∗ ∈ S. That is, T n x0 → x ∗ ∈ S. Next
we show that x ∗ is a ﬁxed point of T. Since T is continuous, we have T ( T n x0 ) → Tx ∗ .
But T ( T n x0 ) → x ∗ clearly also holds. (Why?) Since sequences in a metric space have
at most one limit, it must be that Tx ∗ = x ∗ .
Regarding uniqueness, let x and x be ﬁxed points of T in S. Then
ρ( x, x ) = ρ( Tx, Tx ) ≤ λρ( x, x )

∴

ρ( x, x ) = 0, and hence x = x

The estimate ρ( T n x, x ∗ ) ≤ λn ρ( x, x ∗ ) in the statement of the theorem is left as an
exercise.
If we take away uniformity and just have a contraction, then Banach’s proof of
stability does not work, and indeed a ﬁxed point may fail to exist. Under the action
of a uniformly contracting map T, the motion induced by iterating T slows down at a
geometric rate. The limit of this process is a ﬁxed point. On the other hand, with a
15 In

other words, T 0 := { x → x } and T n := T ◦ T n−1 for n ∈

.

54

Chapter 3

contraction we know only that the process slows down at each step, and this is not
enough to guarantee convergence. Imagine a particle that travels at speed 1 + 1/t at
time t. Its motion slows down at each step, but the particle’s speed is bounded away
from zero.
Exercise 3.2.37 Let S := + with distance | · |, and let T : x → x + e− x . Show that T is
a contraction on S, and that T has no ﬁxed point in S.
However, if we add compactness of S to the contractiveness of T the problem is
rectiﬁed. Now our particle cannot diverge, as that would violate the existence of a
convergent subsequence.
Theorem 3.2.38 If (S, ρ) is compact and T : S → S is contracting, then T has a unique ﬁxed
point x ∗ ∈ S. Moreover T n x → x ∗ for all x ∈ S.
The proof is consigned to the appendix in order to maintain continuity.

3.3

Commentary

The French mathematician Maurice Fréchet (1878–1973) introduced the notion of metric space in his dissertation of 1906. The name “metric space” is due to Felix Hausdorff
(1868–1942). Other important spaces related to metric spaces are topological spaces (a
generalization of metric space) and normed linear spaces (metric spaces with additional algebraic structure). Good references on metric space theory—sorted from elementary to advanced—include Light (1990), Kolmogorov and Fomin (1970), Aliprantis and Burkinshaw (1998), and Aliprantis and Border (1999). For a treatment with
economic applications, see Ok (2007).
This chapter’s discussion of ﬁxed points and optimization only touched the surface of these topics. For a nice treatment of optimization theory, see Sundaram (1996).
Various extensions of Brouwer’s ﬁxed point theorem are available, including Kakutani’s theorem (for correspondences, see McLennan and Tourky 2005 for an interesting proof) and Schauder’s theorem (for inﬁnite-dimensional spaces). Aliprantis and
Border (1999) is a good place to learn more.

Chapter 4

Introduction to Dynamics
4.1

Deterministic Dynamical Systems

Having covered programming and metric spaces in some depth, we now possess ample tools for analysis of dynamics. After starting with deterministic dynamical systems, setting up the basic theory and the notion of stability, we turn to stochastic
models, where evolution of the state variable is affected by noise. While deterministic
systems are clearly a kind of stochastic system (with zero-variance noise), we will see
that the converse is also true: Stochastic models can be embedded in the deterministic framework. Through this embedding we can study the dynamic properties of
stochastic systems using our knowledge of the deterministic model.

4.1.1

The Basic Model

Suppose that we are observing the time path of some variable x in a metric space S. At
t, the current state of the system is denoted by xt . Assume that from the current state xt
we can compute the time t + 1 value xt+1 by applying a map h. That is, xt+1 = h( xt ).
The two primitives that make up this system are S and h:
Deﬁnition 4.1.1 A dynamical system is a pair (S, h), where S = (S, ρ) is an arbitrary
metric space and h is a map from S into itself.
By the n-th iterate of x ∈ S under h we mean hn ( x ). It is conventional to set h0 ( x ) :=
x. The trajectory of x ∈ S under h is the sequence (ht ( x ))t≥0 . As before, x ∗ ∈ S is
a ﬁxed point of h in S if h( x ∗ ) = x ∗ . Fixed points are also said to be stationary or
invariant under h.1 Figure 4.1 illustrates the dynamics of one particular map h on 2
1 Similar

terminology applies to sets. For example, if h( A) ⊂ A, then A is said to be invariant under h.

55

56

Chapter 4

Figure 4.1 A dynamical system

by showing an arrow from x to h( x ) for x ∈ a grid of points.
Exercise 4.1.2 Show that if (S, h) is a dynamical system, if x ∈ S is the limit of some
trajectory (i.e., ht ( x ) → x as t → ∞ for some x ∈ S), and if h is continuous at x , then
x is a ﬁxed point of h.2
Exercise 4.1.3 Prove that if h is continuous on S and h( A) ⊂ A (i.e., h maps A → A),
then h(cl A) ⊂ cl A.
Let x ∗ be a ﬁxed point of h on S. By the stable set Λ( x ∗ ) of x ∗ we refer to all x ∈ S
such that limt→∞ ht ( x ) = x ∗ . Clearly, Λ( x ∗ ) is nonempty. (Why?) The ﬁxed point
x ∗ is said to be locally stable, or an attractor, whenever there exists an open set G with
x ∗ ∈ G ⊂ Λ( x ∗ ). Equivalently x ∗ is locally stable whenever there exists an -ball
around x ∗ such that every trajectory starting in that ball converges to x ∗ :
Exercise 4.1.4 Prove that x ∗ is locally stable if and only if there exists an  > 0 such
that B(, x ∗ ) ⊂ Λ( x ∗ ).
In this book we will be interested primarily in global stability:
Deﬁnition 4.1.5 A dynamical system (S, h) is called globally stable or asymptotically
stable if
1. h has a unique ﬁxed point x ∗ in S, and
2. ht ( x ) → x ∗ as t → ∞ for all x ∈ S.
Let xt := ht ( x ), so xt → x . Consider the sequence (h( xt ))t≥1 . Argue that h( xt ) → h( x ). Now
show that h( xt ) → x also holds. (Why?!) What do you conclude?
2 Hint:

Introduction to Dynamics

57

Figure 4.2 Global stability

Exercise 4.1.6 Prove that if x ∗ is a ﬁxed point of (S, h) to which every trajectory converges, then x ∗ is the only ﬁxed point of (S, h).
Figure 4.2 helps to visualize the concept of global stability, plotting 20 individual
trajectories of a stable map h on 2 . Figure 4.3 also illustrates global stability, in this
case for the one-dimensional system (S, h), where S := (0, ∞) and h(k ) := sAkα with
s ∈ (0, 1], A > 0 and α ∈ (0, 1). The system represents a simple Solow–Swan growth
model, where next period’s capital stock h(k) is the savings rate s times current output
Akα . The value A is a productivity parameter and α is the capital intensity. Figure 4.3
is called a 45 degree diagram. When the curve h lies above (resp., below) the 45 degree
line we have h(k ) > k (resp., h(k) < k), and hence the trajectory moves to the right
(resp., left). Two trajectories are shown, converging to the unique ﬁxed point k∗ .
Regarding local stability of (S, h) when S is an open subset of , it is well-known
that
Lemma 4.1.7 If h is a map with continuous derivative h and x ∗ is a ﬁxed point of h with
|h ( x ∗ )| < 1, then x ∗ is locally stable.
The most enthusiastic readers might like to attempt a proof, although the lemma
is not particularly central to what follows.
Example 4.1.8 Consider a growth model with “threshold” nonconvexities of the form
k t+1 = sA(k t )kαt , where s ∈ (0, 1] and k → A(k) is some increasing function with

58

Chapter 4
kt+1

k∗

kt

Figure 4.3 45 degree diagram

A(k) > 0 when k > 0. Suppose, for example, that A is a step function of the form

A1 if 0 < k < k b
A(k) =
A2 if k b ≤ k < ∞
Here k b is a “threshold” value of capital stock, and 0 < A1 < A2 . Let k∗i be the
solution to k = sAi kα for i = 1, 2 when it exists. A plot is given in ﬁgure 4.4 for the
case where k∗1 < k b < k∗2 . The two ﬁxed points k∗1 and k∗2 are local attractors, as can be
veriﬁed from lemma 4.1.7. Long-run outcomes depend on initial conditions, and for
this reason the model is said to exhibit path dependence.
Exercise 4.1.9 A dynamical system (S, h) is called Lagrange stable if every trajectory
is precompact in S. In other words, the set { hn ( x ) : n ∈ } is precompact for every
x ∈ S.3 Show that if S is a closed and bounded subset of n , then (S, h) is Lagrange
stable for any choice of h.
Exercise 4.1.10 Give an example of a dynamical system (S, h) where S is unbounded
but (S, h) is Lagrange stable.

→ be an increasing function, in the sense
Exercise 4.1.11 Let S = , and let h :
that if x ≤ y, then h( x ) ≤ h(y). Show that every trajectory of h is a monotone sequence
in (either increasing or decreasing).
3 Equivalently

every subsequence of the trajectory has a convergent subsubsequence.

Introduction to Dynamics

59

kt+1

k1∗

kb

k2∗

kt

Figure 4.4 Threshold externalities

Exercise 4.1.12 Now order points in n by setting x ≤ y whenever xi ≤ yi for i in
{1, . . . , n} (i.e., each component of x is dominated by the corresponding component
of y). Let S = n , and let h : S → S be monotone increasing. (The deﬁnition is the
same.) Show that the same result no longer holds—h does not necessarily generate
monotone trajectories.

4.1.2

Global Stability

Global stability will be a key concept for the remainder of the text. Let’s start our
investigation of global stability by looking at linear (more correctly, afﬁne) systems in
one dimension.
Exercise 4.1.13 Let S = ( , | · |) and h( x ) = ax + b. Prove that
t −1

h t ( x ) = a t x + b ∑ ai
i =0

( x ∈ S, t ∈

)

(Hint: Use induction.) From this expression, prove that (S, h) is globally stable whenever | a| < 1, and exhibit the ﬁxed point.
Exercise 4.1.14 Show that the condition | a| < 1 is also necessary, in the sense that if
| a| ≥ 1, then (S, h) is not globally stable. Show, in particular, that ht ( x0 ) converges to
x ∗ := b/(1 − a) only if x0 = x ∗ .

60

Chapter 4

In exercise 4.1.13 we found a direct proof of global stability for our afﬁne system
when | a| < 1. For more complex systems direct methods are usually unavailable,
and we must deploy more powerful machinery, such as Banach’s ﬁxed point theorem
(theorem 3.2.36 on page 53).
Exercise 4.1.15 Let (S, h) be as in exercise 4.1.13. Using theorem 3.2.36, prove that
(S, h) is globally stable whenever | a| < 1.
Exercise 4.1.16 Let S := (0, ∞) with ρ( x, y) := | ln x − ln y|. Prove that ρ is a metric
on S and that (S, ρ) is a complete metric space.4 Consider the growth model k t+1 =
h(k t ) = sAkαt in ﬁgure 4.3, where s ∈ (0, 1], A > 0 and α ∈ (0, 1). Convert this into a
dynamical system on (S, ρ), and prove global stability using theorem 3.2.36.
Next we consider linear systems in n . In general, a function h :
linear if
∀ x, y ∈ n ∀ α, β ∈
h(αx + βy) = αh( x ) + βh(y)

n

→

n

is called
(4.1)

It can be shown that every such h is continuous. If E is an n × n matrix, then the map
on n deﬁned by x → Ex is linear. In fact it can be shown that for all linear maps
h : n → n there exists a matrix Eh with h( x ) = Eh x for all x ∈ n . An afﬁne system
on n is a map h : n → n given by
h( x ) = Ex + b where E is an n × n matrix and b ∈
To investigate this system, let  ·  be any norm on
λ := max{ Ex  : x ∈

n

n,

n

and deﬁne

,  x  = 1}

(4.2)

Exercise 4.1.17 If you can, prove that the maximum exists. Using the properties of
norms and linearity of E, show that  Ex  ≤ λ x  for all x ∈ n . Show in addition
that if λ < 1, then ( n , h) is globally stable.
Let’s look at an application of these ideas. In a well-known paper, Long and
Plosser (1983) studied the modeling of business cycles using multisector growth models. After solving their model, they ﬁnd a system for log output given by yt+1 =
Ayt + b. Here A = ( aij ) is a matrix of input/output elasticities across sectors, and yt
is a 6 × 1 vector recording output in agriculture, mining, construction, manufacturing, transportation and services.5 Using cost share data and the hypothesis of perfect
4 Hint: Take a Cauchy sequence ( x ) in ( S, ρ ), and map it into (ln x ), which is a sequence in ( , | · |).
n
n
Prove this sequence is Cauchy in ( , | · |), and therefore converges to some y ∈ . Now prove that xn → ey
in (S, ρ).
5 In their model b is random, but let’s ignore this complication for now.

Introduction to Dynamics

61

competition, the authors calculate A to be given by
⎛
0.45 0.00 0.01 0.21
⎜ 0.00 0.09 0.04 0.17
⎜
⎜ 0.00 0.01 0.00 0.42
A = ( aij ) = ⎜
⎜ 0.06 0.03 0.01 0.46
⎜
⎝ 0.00 0.00 0.02 0.12
0.02 0.02 0.06 0.20

0.10
0.05
0.12
0.06
0.10
0.09

0.16
0.49
0.09
0.13
0.32
0.38

⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠

Exercise 4.1.18 Prove that Long and Plosser’s system is stable in the following way:
Let A = ( aij ) be an n × n matrix where the sum of any of the rows of A is strictly less
than 1 (i.e., maxi αi < 1, where αi := ∑ j | aij |). Using the norm  · ∞ in (4.2), show that
for A we have λ < 1. Now argue that in Long and Plosser’s model, (yt ) converges
to a limit y∗ , which is independent of initial output y0 , and, moreover, is the unique
solution to the equation y∗ = Ay∗ + b.6
Exercise 4.1.19 Let B = (bij ) be an n × n matrix where the sum of any of the columns
of B is strictly less than 1 (i.e., max j β j < 1, where β j := ∑i |bij |). Using the norm  · 1
in (4.2), show that for B we have λ < 1. Conclude that if h( x ) = Bx + b, then ( n , h)
is globally stable.
The following results will be needed later in the text:
Exercise 4.1.20 Suppose that h is uniformly contracting on complete space S, so (S, h)
is globally stable. Prove that if A ⊂ S is nonempty, closed and invariant under h (i.e.,
h( A) ⊂ A), then the ﬁxed point of h lies in A.
Lemma 4.1.21 Let (S, h) be a dynamical system. If h is nonexpansive and (S, h N ) is globally
stable for some N ∈ , then (S, h) is globally stable.
Proof. By hypothesis, h N has a unique ﬁxed point x ∗ in S, and hkN ( x ) → x ∗ as k → ∞
for all x ∈ S. Pick any  > 0, and choose k ∈ so that ρ(hkN (h( x ∗ )), x ∗ ) < . Then
ρ(h( x ∗ ), x ∗ ) = ρ(h(hkN ( x ∗ )), x ∗ ) = ρ(hkN (h( x ∗ )), x ∗ ) < 
It follows that x ∗ is a ﬁxed point of h. (Why?)
Stability: Fix x ∈ S and  > 0. Choose k ∈
nonexpansiveness implies that for each n ≥ kN,

so that ρ(hkN ( x ), x ∗ ) < . Then

ρ(hn ( x ), x ∗ ) = ρ(hn−kN (hkN ( x )), x ∗ ) ≤ ρ(hkN ( x ), x ∗ ) < 
In other words, (S, h) is globally stable.
6 You

are proving d∞ -convergence of trajectories, but this is equivalent to d2 -convergence by theorem 3.2.30.

62

Chapter 4

Figure 4.5 Trajectory of the quadratic map

4.1.3

Chaotic Dynamic Systems

In this section we make a very brief foray into complex dynamical systems. Chaotic
dynamics is an area that has suffered from excessive hype but is nonetheless interesting for any student of dynamics. We will not be covering much theory, but we will
get some practice with programming.
To begin, consider ﬁrst the dynamical system (S, h) deﬁned by
h( x ) = 4x (1 − x )

( x ∈ S := [0, 1])

(4.3)

The function h is called the quadratic (or logistic) map, as is often found in biological
models related to population dynamics. Readers can check that h maps S into itself.
In the previous section we deﬁned global stability. For stable systems the trajectories converge to a single point, and long series will have an average value close to
that point. Other systems can have several attractors, so the point where the trajectory
settles down to depends on the initial condition. We will see that for (4.3) dynamics
are still more complicated.
Figure 4.5 shows one trajectory starting at initial condition 0.11. Code used to
generate the ﬁgure is given in listing 4.1. It uses Matplotlib’s pylab, a module for
producing graphs in Python.
Listing 4.2 provides an alternative method of generating trajectories using OOP
(see §2.2.3 for an introduction to OOP design). Dynamical systems are naturally represented as objects because they combine data (current state) and action (iteration of

Introduction to Dynamics

63

Listing 4.1 (quadmap1.py) Trajectories of the quadratic map
from pylab i m p o r t plot , show

# Requires Matplotlib

datapoints = []
x = 0.11
f o r t i n range (200) :
datapoints . append ( x )
x = 4 * x * (1 - x )

# Stores trajectory
# Initial condition

plot ( datapoints )
show ()

the map). Here we deﬁne a class DS that is an abstraction of a dynamical system. The
class deﬁnition is relatively self-explanatory. Although the use of OOP is perhaps excessive for this simple problem, one can see how abstraction facilitates code reuse: DS
can be used to generate trajectories for any dynamical system. An example of usage
is given in listing 4.3.
Notice that in ﬁgure 4.5 the trajectory continues to traverse through the space without settling down. Some experimentation shows that this happens for many initial
conditions (but not all—does the map have any ﬁxed points?). Moreover a slight
variation in the initial condition typically leads to a time series that bears no clear
resemblance to the previous one.
Science and mathematics are all about simpliﬁcation and reduction. For example,
with a globally stable system we can usually focus our attention on the steady state.
(How does this state ﬁt with the data?) From this perspective ﬁgure 4.5 is a little
distressing. Unless the initial conditions are very special and can be known exactly, it
seems that long run outcomes cannot be predicted.7 However, this conclusion is too
pessimistic, as the next exercise illustrates.
Exercise 4.1.22 Using your preferred plotting tool, histogram some trajectories generated by the quadratic map, starting at different initial conditions. Use relatively long
trajectories (e.g., around 5,000 points), and a ﬁne histogram (about 40 bins). What
regularities can you observe?
Incidentally, the time series in ﬁgure 4.5 looks quite random, and in exercise 4.1.22
we treated the trajectory in a “statistical” way, by computing its histogram. Is there in
7 Which

models?

is problematic for a scientiﬁc study—what falsiﬁable implications can be drawn from these

64

Chapter 4

Listing 4.2 (ds.py) An abstract dynamical system
c l a s s DS :
d e f __init__ ( self , h =None , x =None) :
" " " Parameters : h is a function and x is a number
in S representing the current state . " " "
self .h , self . x = h , x
d e f update ( self ) :
" Update the state of the system by applying h . "
self . x = self . h ( self . x )
d e f trajectory ( self , n ) :
" " " Generate a trajectory of length n , starting
at the current state . " " "
traj = []
f o r i i n range ( n ) :
traj . append ( self . x )
self . update ()
r e t u r n traj

Listing 4.3 (testds.py) Example application
from ds i m p o r t DS

# Import from listing 4.2

d e f quadmap ( x ) :
r e t u r n 4 * x * (1 - x )
q = DS ( h = quadmap , x =0.1)
T1 = q . trajectory (100)

# Create an instance q of DS
# T1 holds trajectory from 0.1

q . x = 0.2
T2 = q . trajectory (100)

# Reset current state to 0.2
# T2 holds trajectory from 0.2

Introduction to Dynamics

65

Figure 4.6 Quadratic maps, r ∈ [0, 4]

fact any formal difference between this kind of complex dynamics and the dynamics
produced in systems perturbed by random variables?
One answer is that proposed by Kolmogorov, who suggested measuring the “randomness” of a string of numbers by the size of the shortest computer program that can
replicate it.8 The upper bound of this measure is the size of the string itself because, if
necessary, one can simply enumerate the string. This upper bound is associated with
complete randomness. On the other hand, our code used to produce the time series
for the quadratic map was only a few lines, and therefore has a low Kolmogorov score.
In this sense we can differentiate it from a random string.
How does the quadratic map behave when we let the multiplicative parameter
take values other than 4? Consider the more general map x → rx (1 − x ), where
0 ≤ r ≤ 4. A subset of these maps is plotted in ﬁgure 4.6, along with a 45 degree line.
More curvature corresponds to greater r. It turns out that for some values of r this
system is globally stable. For others, like 4, the behavior is highly complex.
The bifurcation diagram shown in ﬁgure 4.7 helps to give an understanding of the
dynamics. On the x-axis the parameter r ranges from 2.7 to 4. The y-axis corresponds
to the state space S. For each value r in a grid over [2.7, 4], a trajectory of length
1000 was generated. The ﬁrst 950 points were discarded, and the last 50 were plotted.
8 Put

differently, by how much can we compress such a string of numbers?

66

Chapter 4

Figure 4.7 Bifurcation diagram

For
√ r ≤ 3, interior points converge to a unique interior steady state. For r ∈ (3, 1 +
6], the state eventually oscillates between two “periodic attractors.” From there
the number of periodic attractors increases rapidly, and the behavior of the system
becomes correspondingly more “chaotic.”
Exercise 4.1.23 Reproduce ﬁgure 4.7 using Python to generate the data and your preferred plotting tool.

4.1.4

Equivalent Dynamics and Linearization

In general, nonlinear models are much more difﬁcult to analyze than linear models,
leading researchers to approximate nonlinear models with linearized versions. The
latter are usually obtained by a ﬁrst-order Taylor expansion. Since ﬁxed points are the
natural focus of analysis, it is standard to take expansions around ﬁxed points.
Let us see how this is done in the one-dimensional case. Let (S, h) be a dynamical
system where S is an open subset of , and h is continuously differentiable, with
derivative h on S. Pick any a ∈ S. The ﬁrst-order Taylor expansion around a is the
map h1 deﬁned by
(4.4)
h1 ( x ) = h( a) + h ( a)( x − a)
Notice that h1 is an afﬁne function on

with h( a) = h1 ( a). Clearly, h1 approximates

Introduction to Dynamics

67

h in some sense when | x − a| is small. For this reason it is regarded as a “linear”
approximation to h around a.
Now let x ∗ be a ﬁxed point of h, so
h1 ( x ) = x ∗ + h ( x ∗ )( x − x ∗ )

(4.5)

You can check that x ∗ is also a ﬁxed point of the approximating map h1 . Note also that
x ∗ will be stable for h1 whenever |h ( x ∗ )| < 1. But this is precisely the condition for
x ∗ to be a local attractor for h (lemma 4.1.7). So it seems that we can learn something
about how ht ( x ) will behave when | x − x ∗ | is small by studying the simple afﬁne map
h1 and the trajectory h1t ( x ) that it generates.
The well-known Hartman–Grobman theorem formalizes this idea. To state the
theorem, it is necessary to introduce the abstract but valuable notion of topological
conjugacy. First, let S and Ŝ be two metric spaces. A function τ from S to Ŝ is called
a homeomorphism if it is continuous, a bijection, and its inverse τ −1 is also continuous.
Two dynamical systems (S, g) and (Ŝ, ĝ) are said to be topologically conjugate if there
exists a homeomorphism τ from S into Ŝ such that g and ĝ commute with τ in the
sense that ĝ = τ ◦ g ◦ τ −1 everywhere on Ŝ. In other words, shifting a point x̂ ∈ Ŝ to
ĝ( x̂ ) using the map ĝ is equivalent to moving x̂ into S with τ −1 , applying g, and then
moving the result back using τ:
g

x −−−−→ g( x )

⏐
⏐ −1
⏐τ
⏐τ

ĝ

x̂ −−−−→ ĝ( x̂ )
Exercise 4.1.24 Let S := ((0, ∞), | · |) and Ŝ := ( , | · |). Let g( x ) = Ax α , where A > 0
and α ∈ , and let ĝ( x̂ ) = ln A + α x̂. Show that g and ĝ are topologically conjugate
under τ := ln.
Exercise 4.1.25 Show that if (S, g) and (Ŝ, ĝ) are topologically conjugate, then x ∗ ∈ S
is a ﬁxed point of g on S if and only if τ ( x ∗ ) ∈ Ŝ is a ﬁxed point of ĝ on Ŝ.
Exercise 4.1.26 Let x ∗ ∈ S be a ﬁxed point of g and let x be any point in S. Show that
limt→∞ gt ( x ) = x ∗ if and only if limt→∞ ĝt (τ ( x )) = τ ( x ∗ ).
Exercise 4.1.27 Let x ∗ ∈ S be a ﬁxed point of g. Show that if x ∗ is a local attractor for
(S, g), then τ ( x ∗ ) is a local attractor for (Ŝ, ĝ). Show that if (S, g) is globally stable,
then (Ŝ, ĝ) is globally stable.
We can now state the theorem of Hartman and Grobman:

68

Chapter 4

Theorem 4.1.28 (Hartman–Grobman) Let S be an open subset of , let h : S → S be a
continuously differentiable function, let x ∗ ∈ S be a ﬁxed point of h, and let h1 be the Taylor
approximation in (4.5). If |h ( x ∗ )| = 1, then there exists an open set G containing x ∗ such
that h and h1 are topologically conjugate on G.9
Be careful when applying this theorem, which is one of the most misused mathematical results in all of economics. It provides only a neighborhood of S such that
behavior of the approximation is qualitatively similar to that of the original system. As
it stands, the Hartman–Grobman theorem provides no basis for quantitative analysis.

4.2

Finite State Markov Chains

Next we start our journey into the world of stochastic dynamics. Stochastic dynamics
is a technically demanding ﬁeld, mainly because any serious attempt at probability
theory on general state spaces requires at least a modicum of measure theory. For
now we concentrate on ﬁnite state spaces, where measure theory is unnecessary. However, our treatment of ﬁnite state stochastic dynamics is geared in every way toward
building understanding, intuition, notation and tools that will be used in the general
(uncountable state) case.

4.2.1

Deﬁnition

We begin with a state space S, which is a ﬁnite set { x1 , . . . , x N }. A typical element
of S is usually denoted by x, rather than a symbol such as xi or xn . This makes our
notation more consistent with the continuous state theory developed below. The set
of distributions on S will be denoted P (S), and consists of all functions φ : S → with
φ( x ) ≥ 0 for all x ∈ S, and ∑ x∈S φ( x ) = 1. In general, φ( x ) will correspond to the
probability attached to the point x in the state space under some given scenario.10
A quick digression: Although φ has been introduced as a function from S to ,
one can also think of it as a vector under the one-to-one correspondence
P (S)

φ ↔ (φ( x )) x∈S := (φ( x1 ), . . . , φ( x N )) ∈

N

(4.6)

Under the correspondence (4.6), the collection of functions P (S) becomes a subset of
the vector space N —in particular, the elements of N that are nonnegative and sum
to one. This set is called the unit simplex, and is illustrated for the case of N = 3 in
ﬁgure 4.8.
The basic primitive for a discrete time Markov process on S is a stochastic kernel,
the deﬁnition of which is as follows.
9 To

see why |h ( x ∗ )| = 1 is important, consider the case of h( x ) = arctan( x ).
we call a distribution here is also referred to as a probability mass function.

10 What

Introduction to Dynamics

69

Figure 4.8 The unit simplex with N = 3

Deﬁnition 4.2.1 A stochastic kernel p is a function from S × S into [0, 1] such that
1. p( x, y) ≥ 0 for each ( x, y) in S × S, and
2. ∑y∈S p( x, y) = 1 for each x ∈ S.
In other words, the function S y → p( x, y) ∈ is an element of P (S) for all x ∈ S.
This distribution is represented by the symbol p( x, dy) in what follows.
As well as being a function, the distribution p( x, dy) can be viewed as a row11
vector ( p( x, x1 ), . . . , p( x, x N )) in N , located in the unit simplex, and these rows can
be stacked horizontally to produce an N × N matrix with the property that each row
is nonnegative and sums to one:
⎛
⎞ ⎛
⎞
p( x1 , dy)
p ( x1 , x1 ) · · · p ( x1 , x N )
⎜
⎟ ⎜
⎟
..
..
..
(4.7)
p=⎝
⎠=⎝
⎠
.
.
.
p( x N , dy)

p ( x N , x1 )

···

p( x N , x N )

Conversely, any square N × N matrix that is nonnegative and has all rows summing
to one deﬁnes a stochastic kernel. However, when we move on to inﬁnite state spaces
there is no concept of matrices, and hence most of the theory is stated in terms of
kernels.
11 When

treating distributions as vectors it is traditional in the Markov chain literature to regard them as
row vectors.

70

Chapter 4
p ( x1 , x1 )

x1
p ( x1 , x3 )

p ( x2 , x1 )

p ( x3 , x1 )

p ( x1 , x2 )
p ( x2 , x3 )

x2

p ( x3 , x2 )

p ( x2 , x2 )

x3

p ( x3 , x3 )

Figure 4.9 Finite Markov chain

In this chapter we are going to study a sequence of random variables ( Xt )t≥0 ,
where each Xt takes values in S. The sequence updates according to the following
rule: If Xt = x, then, in the following period Xt+1 takes the value y with probability
p( x, y). In other words, once the current state Xt is realized, the probabilities for Xt+1
are given by p( Xt , dy). Figure 4.9 depicts an example of a simple Markov system,
where S = { x1 , x2 , x3 }, and p( xi , x j ) is the probability that Xt moves from state xi at
time t to x j at time t + 1.
The transition probabilities at each time depend on nothing other than the current location of the state. This is the “Markov” assumption. Moreover the transition
probabilities do not depend on time. This is called time homogeneity. These are assumptions, which ﬁt the motion of some models well and others poorly. Although
they may seem restrictive, it turns out that, with some manipulation, a large class of
systems can be embedded in the basic Markov framework.
An example of a stochastic kernel is the one used in a recent study by Hamilton
(2005), who investigates a nonlinear statistical model of the business cycle based on
US unemployment data. As part of his calculation he estimates the kernel
⎛
⎞
0.971 0.029
0
p H := ⎝ 0.145 0.778 0.077 ⎠
(4.8)
0
0.508 0.492
Here S = { x1 , x2 , x3 } = { NG, MR, SR}, where NG corresponds to normal growth,

Introduction to Dynamics

71

MR to mild recession, and SR to severe recession. For example, the probability of
transitioning from severe recession to mild recession in one period is 0.508. The length
of the period is one month.
For another example of a Markov model, consider the growth dynamics study of
Quah (1993), who analyzes the evolution of real GDP per capita relative to the world
average for a “typical” country (e.g., Xt = 2 implies that income per capita for the
country in question is twice the world average at time t). A natural state space is + ,
but to simplify matters Quah discretizes this space into ﬁve bins that correspond to
values for relative GDP of 0 to 0.25, 0.25 to 0.5, 0.5 to 1, 1 to 2 and 2 to ∞ respectively.
He then calculates the stochastic kernel by setting p( x, y) equal to the fraction of times
that a country, ﬁnding itself in state x, subsequently makes the transition to state y.12
The result of this calculation is
⎛
⎞
0.97 0.03 0.00 0.00 0.00
⎜ 0.05 0.92 0.03 0.00 0.00 ⎟
⎜
⎟
⎟
pQ := ⎜
(4.9)
⎜ 0.00 0.04 0.92 0.04 0.00 ⎟
⎝ 0.00 0.00 0.04 0.94 0.02 ⎠
0.00 0.00 0.00 0.01 0.99
For example, the probability of our typical country transitioning from the lowest bin
to the second lowest bin in one year is 0.03.
Algorithm 4.1 Simulation of a Markov chain
draw X0 ∼ ψ and set t = 0
while True do
draw Xt+1 ∼ p( Xt , dy)
set t = t + 1
end

// "while True" means repeat forever

Let us now try to pin down the deﬁnition of a Markov chain ( Xt )t≥0 corresponding
to a given stochastic kernel p. It is helpful to imagine that we wish to simulate ( Xt )t≥0
on a computer. First we draw X0 from some predetermined initial condition ψ ∈ P (S).
As p( x, dy) gives the transition probabilities for Xt+1 conditional on Xt = x, we now
draw X1 from p( X0 , dy). Taking the result X1 , we then draw X2 from p( X1 , dy), and
so on. This is the content of algorithm 4.1, as well as the next deﬁnition.
Deﬁnition 4.2.2 Let ψ ∈ P (S). A sequence of S-valued random variables ( Xt )t≥0 is
called Markov-( p, ψ) if
12 His data span the period 1962 to 1984, and have a sample of 118 countries. The transitions are over a
one year period. The model is assumed to be stationary (transition probabilities do not vary with time), so
all of the transitions (1962 to 1963, 1963 to 1964, etc.) can be pooled when calculating transition probabilities.

72

Chapter 4
3

2.5

2

1.5

1

0

200

400

600

800

1000

Figure 4.10 Simulation of the chain

1. at time zero, X0 is drawn from ψ, and
2. at time t + 1, Xt+1 is drawn from p( Xt , dy).
If ψ( x ) = 1 for some x ∈ S, then ( Xt )t≥0 is called Markov-( p, x ).
Let’s try simulating a Markov-( p, ψ) chain. One method is given in listing 4.4.
The function sample() deﬁned at the top is used to generate draws from a given
ﬁnite distribution using the inverse transform algorithm.13 Next we deﬁne a class MC,
which is similar to the class DS in listing 4.2 (page 64). An instance is initialized with
a stochastic kernel p and an initial state X. The kernel p should be such that p[x] is a
sequence representing p( x, dy)—see listing 4.5 for an example. The method update()
updates the current state using sample(), drawing Xt+1 from p( Xt , dy).14
In listing 4.5 an instance h of MC is created for Hamilton’s stochastic kernel p H ,
and the sample_path() method is used to produce some time series. In the code,
genfinitemc is the name of the ﬁle in listing 4.4. Figure 4.10 shows a simulated series.

4.2.2

Marginal Distributions

Let ( Xt )t≥0 be Markov-( p, ψ). For every t ∈ , let ψt ∈ P (S) denote the distribution
of Xt . That is, ψt (y) is the probability that Xt = y, given that X0 is draw from initial
13 There are existing libraries that can do this job, but the technique used in sample() is something we
will revisit later on (see algorithm 5.3 on page 109), and we present it here for future reference. Don’t be
concerned if the logic is not yet clear.
14 All of this code is written for clarity rather than speed, and is suitable only for small state spaces. See
the text home page for more efﬁcient methods.

Introduction to Dynamics

Listing 4.4 (genfinitemc.py) Finite Markov chain
from random i m p o r t uniform
d e f sample ( phi ) :
" " " Returns i with probability phi [ i ] , where phi is an
array ( e . g . , list or tuple ) . " " "
a = 0.0
U = uniform (0 ,1)
f o r i i n range ( len ( phi ) ) :
i f a < U <= a + phi [ i ]:
return i
a = a + phi [ i ]

c l a s s MC :
" " " For generating sample paths of finite Markov chains
on state space S = {0 ,... , N -1}. " " "
d e f __init__ ( self , p =None , X =None) :
" " " Create an instance with stochastic kernel p and
current state X . Here p [ x ] is an array of length N
for each x , and represents p (x , dy ) .
The parameter X is an integer in S. " " "
self .p , self . X = p , X
d e f update ( self ) :
" Update the state by drawing from p (X , dy ) . "
self . X = sample ( self . p [ self . X ])
d e f sample_path ( self , n ) :
" " " Generate a sample path of length n , starting from
the current state . " " "
path = []
f o r i i n range ( n ) :
path . append ( self . X )
self . update ()
r e t u r n path

73

74

Chapter 4

Listing 4.5 (testgenfinitemc.py) Example application
from genfinitemc i m p o r t sample , MC

# Import from listing 4.4

pH = ((0.971 , 0.029 , 0.000) ,
(0.145 , 0.778 , 0.077) ,
(0.000 , 0.508 , 0.492) )
psi = (0.3 , 0.4 , 0.3)
h = MC ( p = pH , X = sample ( psi ) )
T1 = h . sample_path (1000)

# Initial condition
# Create an instance of class MC
# Series is Markov -( p , psi )

psi2 = (0.8 , 0.1 , 0.1)
h . X = sample ( psi2 )
T2 = h . sample_path (1000)

# Alternative initial condition
# Reset the current state
# Series is Markov -( p , psi2 )

distribution ψ, and that the chain subsequently follows Xt+1 ∼ p( Xt , dy). This distribution is sometimes called the marginal or unconditional distribution of Xt . We can
understand it as follows: Generate n independent realizations of Xt , and calculate the
fraction that takes the value y. Call this number ψtn (y). The probability ψt (y) can be
thought of as the limit of ψtn (y) as n → ∞.
A method for computing the fraction ψtn (y) is given in algorithm 4.2. In the algorithm, the instruction draw X ∼ p( X, dy) should be interpreted as: Draw a random
variable Y according to the distribution p( X, dy) and then set X = Y. Also, { Xti = y}
is an indicator function, equal to one when Xti = y and zero otherwise.
Algorithm 4.2 Approximate marginal distribution
for i in 1 to n do
draw X ∼ ψ
for j in 1 to t do
draw X ∼ p( X, dy)
end
set Xti = X
end
return (1/n) ∑in=1 { Xti = y}

Introduction to Dynamics

75

Exercise 4.2.3 Implement algorithm 4.2 for Hamilton’s Markov chain.15 Set ψ =
(0, 0, 1), so the economy starts in severe recession with probability one. Compute
an approximation to ψt (y), where t = 10 and y = NG. For sufﬁciently large n you
should get an answer close to 0.6.
Exercise 4.2.4 Rewrite algorithm 4.2 using a counter that increments by one whenever
the output of the inner loop produces a value equal to y instead of recording the value
of each Xti .
Now consider again a Markov-( p, ψ) chain ( Xt )t≥0 for arbitrary stochastic kernel
p and initial condition ψ. As above, let ψt ∈ P (S) be the marginal distribution of Xt .
From ψt and our complete description of the dynamics in p, it seems possible that we
will be able to calculate the distribution of Xt+1 . That is to say, we might be able to
link ψt and ψt+1 using p. That we can in fact construct such a recursion is one of the
most fundamental and important properties of Markov chains.
To begin, pick any y ∈ S. Using the law of total probability (see §A.1.3), we can
decompose the probability that Xt+1 = y into conditional parts as follows:

{ X t +1 = y } =

∑

x ∈S

{ X t +1 = y | X t = x } · { X t = x }

Rewriting this statement in terms of our marginal and conditional probabilities gives
ψt+1 (y) =

∑ p(x, y)ψt (x)

x ∈S

(y ∈ S)

(4.10)

This is precisely the kind of recursion we are looking for. Let’s introduce some additional notation to help manipulate this expression.
Deﬁnition 4.2.5 Given stochastic kernel p, the Markov operator corresponding to p is
the map M sending P (S) ψ → ψM ∈ P (S), where ψM is deﬁned by
ψM(y) =

∑ p(x, y)ψ(x)

x ∈S

(y ∈ S)

(4.11)

The notation appears unusual, in the sense that we normally write M(ψ) instead of
ψM for the image of ψ under a mapping M. However, such notation is traditional in
the Markov literature. It reminds us that applying the Markov operator to a distribution ψ ∈ P (S) is just postmultiplication of the row vector (ψ( x )) x∈S by the stochastic
kernel (viewed as a matrix).
15 Ideally you should add the extra functionality to the class MC rather than writing it speciﬁcally for
Hamilton’s chain. Note that in Python, if Y is a sequence containing observations of Xt , then Y.count(y)
gives the number of elements equal to y.

76

Chapter 4
Combining (4.10) and (4.11), we obtain the fundamental recursion
ψt+1 = ψt M

(4.12)

Check this carefully until you feel comfortable with the notation.
This representation (4.12) is easy to manipulate. For example, suppose that we
want to calculate ψj+k from ψj . Clearly,
ψ j + k = ψ j + k −1 M = ( ψ j + k −2 M ) M = ψ j + k −2 M 2 = · · · = ψ j M k
where Mm is the m-th composition of the map M with itself. In particular, setting j = 0
and k = t, we have Xt ∼ ψMt when X0 ∼ ψ. Let’s state these results as a theorem:
Theorem 4.2.6 Let ( Xt )t≥0 be Markov-( p, ψ), and let M be the Markov operator corresponding to p. If ψt is the marginal distribution of Xt for each t, then ψt+1 = ψt M and ψt = ψMt .
To illustrate these ideas, consider again the kernel p Q calculated by Danny Quah,
and let MQ be the Markov operator. We can evaluate probabilities of different outcomes for a given country over time by iteratively applying MQ to an initial condition
60
ψ, generating the sequence (ψMtQ ). Figure 4.11 shows the elements ψM10
Q , ψM Q , and
ψM160
Q of this sequence. In the top graph, the country in question is initially in the
poorest group, so ψ = (1, 0, 0, 0, 0). The bottom graph shows the corresponding elements when the initial condition is reset to ψ = (0, 0, 0, 1, 0).

4.2.3

Other Identities

Let’s think a bit more about the iterates of the Markov operator M. To begin, ﬁx a
kernel p with Markov operator M and deﬁne the t-th order kernel pt by
p1 := p,

pt ( x, y) :=

∑ pt−1 (x, z) p(z, y)

z∈S

(( x, y) ∈ S × S, t ∈

Exercise 4.2.7 Show that pt is in fact a stochastic kernel on S for each t ∈
Use induction.)

)
. (Hint:

To interpret pt , we can use the following lemma:
Lemma 4.2.8 If M is the Markov operator deﬁned by stochastic kernel p on S, then its t-th
iterate Mt is the Markov operator deﬁned by pt , the t-th order kernel of p. In other words, for
any ψ ∈ P (S) we have
ψMt (y) =

∑ pt (x, y)ψ(x)

x ∈S

(y ∈ S)

Introduction to Dynamics

77

0.8
0.6
0.4
0.2

0.2

0.2

0.0

0.0

0.4

0.6

0.8

t = 160

0.2

0.4

0.6

0.8

t = 60

0.0

0.4

0.6

0.8

t = 10

t = 160

0.0

0.2

0.4

0.6

0.8

t = 60

0.0

0.0

0.2

0.4

0.6

0.8

t = 10

Figure 4.11 Top: X0 = 1. Bottom: X0 = 4

78

Chapter 4

We prove only the case t = 2 here, and leave the full proof for the reader. (Hint:
Use induction.) Pick any ψ ∈ P (S) and any y in S. Then
ψM2 (y) = ((ψM)M)(y) =

=
=

∑ p(z, y)ψM(z)

z∈S

∑ p(z, y) ∑ p(x, z)ψ(x)

z∈S

x ∈S

∑ ∑ p(x, z) p(z, y)ψ(x) = ∑ p2 (x, y)ψ(x)

x ∈S z∈S

x ∈S

Now let δx ∈ P (S) be the distribution that puts all mass on x ∈ S (i.e., δx (y) = 1 if
y = x and zero otherwise). Applying lemma 4.2.8 with ψ = δx , we obtain δx Mt (y) =
pt ( x, y) for all y ∈ S. In other words, the distribution pt ( x, dy) is precisely δx Mt , which we
know is the distribution of Xt when X0 = x. More generally, pk ( x, y) is the probability
that the state moves from x now to y in k steps:
pk ( x, y) =

{ Xt + k = y | Xt = x }

( x, y ∈ S, k ∈

)

and pk ( x, dy) is the conditional distribution of Xt+k given Xt = x.
Exercise 4.2.9 Let t ∈ . Show that if p is interpreted as the matrix in (4.7), then
pt ( x, y) is the ( x, y)-th element of its t-th power.
Now let’s introduce another operation for the Markov operator M. So far we have
M acting on distributions to the left, as in ψM(y) = ∑ x p( x, y)ψ( x ). We are also going
to let M act on functions to the right, as in
Mh( x ) =

∑ p(x, y)h(y)

( x ∈ S)

y∈S

(4.13)

where h : S → is any function. Thus M takes a given function h on S and sends it
into a new function Mh on S. In terms of matrix algebra, this is pre-multiplication of
the column vector (h(y))y∈S by the matrix (4.7).
To understand (4.13), recall that if Y is a random variable on S with distribution
φ ∈ P (S) (i.e., {Y = y} = φ(y) for all y ∈ S) and h is a real-valued function on
S, then the expectation h(Y ) of h(Y ) is the sum of all values h(y) weighted by the
probabilities {Y = y}:
h (Y ) : =

∑ h(y)

y∈S

{Y = y } =

∑ φ(y)h(y)

y∈S

In terms of vectors we are just computing inner products.
It is now clear that Mh( x ) = ∑y∈S p( x, y)h(y) should be interpreted as the expectation of h( Xt+1 ) given Xt = x. Analogous to the result in lemma 4.2.8, we have
Mt h( x ) =

∑ pt (x, y)h(y)

y∈S

( x ∈ S, t ∈

)

(4.14)

Introduction to Dynamics

79

Since pt ( x, dy) is the distribution of Xt given X0 = x, it follows that Mt h( x ) is the
expectation of h( Xt ) given X0 = x.
Exercise 4.2.10 Using induction, conﬁrm the claim in (4.14).
Now the ﬁnishing touches. Fix an initial condition ψ ∈ P (S), a function h as
above and a k ∈ . Deﬁne
ψMk h :=

∑ ∑ pk (x, y)ψ(x)h(y)

(4.15)

y∈S x ∈S

In terms of linear algebra, this expression can be thought of as the inner product of
ψMk and h. Since ψMk is the distribution of Xt+k when Xt ∼ ψ, (4.15) gives us the
expectation of h( Xt+k ) given Xt ∼ ψ. In symbols,
ψMk h =

[ h ( Xt + k ) | Xt ∼ ψ ]

(4.16)

Exercise 4.2.11 Conﬁrm the Chapman–Kolmogorov equation, which states that for any
k, j ∈ ,
pk+ j ( x, y) = ∑ p j (z, y) pk ( x, z)
(( x, y) ∈ S × S)
z∈S

The listing below provides suggested code for implementing the operator ψ →
ψM given a kernel p. (The map h → Mh can be implemented in similar fashion.)
From NumPy we import dot(), which performs matrix multiplication (NumPy must
be installed for this to work). The second line imports the class DS from listing 4.2.
Next we set up Hamilton’s kernel and use it to deﬁne the corresponding Markov
operator as a function acting on distributions. Using the class DS, we compute 100
elements of the sequence (ψMt )t≥0 .
from numpy i m p o r t dot
from ds i m p o r t DS
pH = ((0.971 , 0.029 , 0.000) ,
(0.145 , 0.778 , 0.077) ,
(0.000 , 0.508 , 0.492))
psi = (0.3 , 0.4 , 0.3)
M = lambda phi : dot ( phi , pH )
markovds = DS ( h =M , x = psi )
T = markovds . trajectory (100)

# Matrix multiplication
# Dynamical system class
# Hamilton ’s kernel

#
#
#
#

Initial condition
Define Markov operator
Instance of class DS
Compute trajectory

Exercise 4.2.12 Suppose that the business cycle evolves according to Hamilton’s kernel p H on S = { NG, MR, SR}, and that a ﬁrm makes proﬁts {1000, 0, −1000} in these
three states. Compute expected proﬁts at t = 5, given that the economy starts in NG.
How much do proﬁts change when the economy starts in SR?

80

Chapter 4

Exercise 4.2.13 Compute expected proﬁts at t = 1000 for each of the three possible
initial states. What do you notice?
Exercise 4.2.14 Suppose now that the initial state will be drawn according to ψ =
(0.2, 0.2, 0.6). Compute expected proﬁts at t = 5 using (4.16).

4.2.4

Constructing Joint Distributions

Let’s now consider the joint distributions of a Markov-( p, ψ) process ( Xt )t≥0 . We
would like to understand more about probabilities not just for individual elements of
the sequence such as Xt , but rather for a collection of elements. For example, how do
we compute the probability that ( Xt , Xt+1 ) = ( x, y), or that X j ≤ x for j ≤ t?
Consider ﬁrst the pair ( X0 , X1 ), which can be thought of as a single bivariate random variable taking values in S2 := S × S. Thus the joint distribution is an element of
P (S2 ). A typical element of S2 is a pair ( x0 , x1 ), where xi ∈ S.16 We wish to ﬁnd the
probability { X0 = x0 , X1 = x1 }.
To begin, pick any ( x0 , x1 ) ∈ S2 , and let
q2 ( x 0 , x 1 ) : =

{ X0 = x 0 , X1 = x 1 } =

{ X0 = x 0 } ∩ { X1 = x 1 }

From (A.2) on page 325, for any events A and B we have
It follows that
q2 ( x 0 , x 1 ) =

( A ∩ B) =

( A ) ( B | A ).

{ X0 = x 0 } { X1 = x 1 | X0 = x 0 } = ψ ( x 0 ) p ( x 0 , x 1 )

Similarly, the distribution q3 ∈ P (S3 ) of ( X0 , X1 , X2 ) is
q3 ( x 0 , x 1 , x 2 ) =

=

{ X0 = x 0 , X1 = x 1 , X2 = x 2 }
{ X0 = x 0 , X1 = x 1 } { X2 = x 2 | X0 = x 0 , X1 = x 1 }

= ψ( x0 ) p( x0 , x1 ) p( x1 , x2 )
Notice that we are using { X2 = x2 | X0 = x0 , X1 = x1 } = p( x1 , x2 ). This is reasonable because, if X1 = x1 , then X2 ∼ p( x1 , dy).
Continuing along the same lines yields the general expression
q T +1 ( x 0 , . . . , x T ) = ψ ( x 0 )

T −1

∏ p ( x t , x t +1 )

(4.17)

t =0

To evaluate the probability of a given path x0 , . . . , x T for some stochastic kernel p and
initial condition ψ, we can use a function such as
word on notation: Superscripts represent time, so x0 ∈ S is a typical realization of X0 , x1 ∈ S is a
typical realization of X1 , and so on.
16 A

Introduction to Dynamics

81

d e f path_prob (p , psi , X ): # X a sequence giving the path
prob = psi [ X [0]]
f o r t i n range ( len ( X ) - 1):
prob = prob * p [ X [ t ] , X [ t +1]]
r e t u r n prob

Here p is such that p[x, y] corresponds to p( x, y),17 while psi[x] represents the
probability of initial state x according to ψ, and X is a sequence representing the path
we wish to evaluate.
Exercise 4.2.15 Show that for Hamilton’s kernel p H and ψ = (0.2, 0.2, 0.6), the probability of path ( NG, MR, NG ) is 0.000841.
Exercise 4.2.16 Note that q T +1 ( x0 , . . . , x T ) = q T ( x0 , . . . , x T −1 ) p( x T −1 , x T ) also holds
for each T and each path. Readers familiar with recursive function calls can try rewriting path_prob() using recursion.
From our expression for q T +1 in (4.17) we can also compute the probabilities of
more complex events. By an event is meant any subset B of S T +1 . For example,
B := {( x0 , . . . , x T ) ∈ S T +1 : x t ≤ x t+1 for t = 0, . . . , T − 1}
is an event. It consists of all paths ( x0 , . . . , x T ) in S T +1 that are increasing (i.e., nondecreasing). To obtain the probability of any such event B we just sum q T +1 ( x0 , . . . , x T )
over all distinct paths in B.
One important special case is events of the form
D0 × · · · × D T = {( x0 , . . . , x T ) ∈ S T +1 : x t ∈ D t for t = 0, . . . , T }
where D t ⊂ S for each t. Then {( X0 , . . . , XT ) ∈ D0 × · · · × D T } =
and for this kind of event the following lemma applies:

∩ t ≤ T { Xt ∈ D t },

Lemma 4.2.17 If D0 , . . . , D T is any collection of subsets of S, then

∩ t ≤ T { Xt ∈ D t } =

∑

ψ( x0 )

x0 ∈ D0

∑

∑

( x0 ,...,x T )∈ D0 ×···× D T

∑

x0 ∈ D0

···

p ( x T −1 , x T )

∩t≤T { Xt ∈ D t } can be computed by

Proof. For any such sets
the probability
summing over distinct paths:

=

∑

xT ∈DT

x1 ∈ D1

Dt ,

∩ t ≤ T { Xt ∈ D t } =

p( x0 , x1 ) · · ·

∑

xT ∈DT

q T +1 ( x 0 , . . . , x T )

q T +1 ( x 0 , . . . , x T )

The last step now follows from the expression for q T +1 in (4.17).
17 For

example, p might be implemented as a 2D NumPy array. Alternatively, you can use a sequence of
sequences (e.g., list of lists), in which case change p[x, y] to p[x][y].

82

Chapter 4

Exercise 4.2.18 Returning to Hamilton’s kernel p H , and using the same initial condition ψ = (0.2, 0.2, 0.6) as in exercise 4.2.15, compute the probability that the economy starts and remains in recession through periods 0, 1, 2 (i.e., that x t = NG for
t = 0, 1, 2). [Answer: 0.704242]
Another way to compute this probability is via Monte Carlo:
Exercise 4.2.19 Generate 10,000 observations of ( X0 , X1 , X2 ), starting at the same initial condition ψ = (0.2, 0.2, 0.6). Count the number of paths that do not enter state NG
and divide by 10,000 to get the fraction of paths that remain in recession. This fraction
converges to the probability of the event, so you should get approximately the same
number as you found in exercise 4.2.18.
Now let’s think a little bit about computing expectations. Recall the ﬁrm in exercise 4.2.12. If the ﬁrm operates up until period T, and if the interest rate is equal to r,
then the net present value (NPV) of the ﬁrm is the expected sum of discounted proﬁts
Π ( X0 , . . . , X T )

Π ( X0 , . . . , X T ) : =

where

T

∑ ρ t h ( Xt )

t =0

and ρ := 1/(1 + r ). Expectations for ﬁnite state spaces are found by summing values
weighted by probabilities. In this case,
Π ( X0 , . . . , X T ) =

∑ Π ( x 0 , . . . , x T ) q T +1 ( x 0 , . . . , x T ) = : ∑ Π ( x ) q T +1 ( x )

where the sum is over all x ∈ S T +1 .
Exercise 4.2.20 Compute the NPV when T = 2 and r = 0.05. Take the same initial
condition as in exercise 4.2.15. [Answer: −396.5137]
For larger T and S this kind of computation is problematic. For example, if S has
ten elements and T = 100, then we must sum Π(x)q T +1 (x) over 10100 paths.
Exercise 4.2.21 If the computer can evaluate one billion (109 ) paths per second, how
may years will it take to evaluate all of the paths? Compare this with current estimates
of the age of the universe.
In high dimensions, expectations are often best evaluated via Monte Carlo.
Exercise 4.2.22 Redo exercise 4.2.20 using Monte Carlo. Generate 10,000 observations
of the path ( X0 , X1 , X2 ) and evaluate average proﬁts.
Actually the computational problem we have been discussing can be greatly simpliﬁed in this particular case by linearity of expectations, which gives


Π=

T

∑ ρ t h ( Xt )

t =0

=

T

∑ ρt

t =0

h ( Xt ) =

T

∑ ρt ψMt h

t =0

Introduction to Dynamics

83

The second equality (linearity of ) can be proved from the deﬁnition of the joint distribution, but we treat it in much greater generality below. The third equality follows
from (4.16) on page 79.
Exercise 4.2.23 Redo exercise 4.2.20 using this last expression, taking parameters T, r,
and ψ as given in that exercise. Next plot expected proﬁts against T. After how many
periods (for what value of T) will the ﬁrm’s expected proﬁts be positive?

4.3

Stability of Finite State MCs

In chapter 1 we investigated a Markovian model where the distribution for log income
converges to a unique distribution N (μ∗ , v∗ ), independent of initial conditions. This
behavior means that knowledge of the limiting distribution gives us a great deal of
predictive power in terms of likelihoods for long-run outcomes. In fact stability also
gives us a number of statistical properties that are central to time series econometrics.
As a result, we are motivated to study when one does observe stability, beginning
with the case of ﬁnite state Markov chains.
To start the ball rolling, consider again the sequences of distributions in ﬁgure 4.11
(page 77). What happens if we extend the time horizon? In other words, what sort of
limiting properties, if any, do these sequences possess? Figure 4.12 repeats the same
distribution projections, but this time for dates t = 160, t = 500, and t = 1, 000.
Looking at the top graph for starters, note that after about t = 500 there seems to be
very little change in ψt . In other words, it appears that the sequence (ψt ) is converging.
Interestingly, the sequence in the bottom graph seems to be converging to the same
limit.
Perhaps we are again observing a form of global stability? It turns out that we are,
but to show this we must ﬁrst deﬁne stability for Markov chains and derive theorems
that allow us to establish this property.

4.3.1

Stationary Distributions

Recall that a dynamical system (U, h) consists of a metric space U and a map h : U →
U. Recall also the deﬁnition of the Markov operator M corresponding to a given
stochastic kernel p: Given ψ ∈ P (S), the operator M is a map sending ψ into ψM,
where ψM(y) = ∑ x∈S p( x, y)ψ( x ) for each y ∈ S. What we are going to do now is
view (P (S), M) as a dynamical system in its own right (recalling that trajectories of
the form (ψMt )t≥0 correspond to the sequence of marginal distributions for a Markov( p, ψ) process ( Xt )t≥0 ; see page 76). To do this, we need to introduce a metric on P (S),
and also establish that M does indeed send P (S) into itself.

84

Chapter 4

0.8
0.6
0.4
0.2

0.2

0.2

0.0

0.0

0.4

0.6

0.8

t = 1000

0.2

0.4

0.6

0.8

t = 500

0.0

0.4

0.6

0.8

t = 160

t = 1000

0.0

0.2

0.4

0.6

0.8

t = 500

0.0

0.0

0.2

0.4

0.6

0.8

t = 160

Figure 4.12 Top: X0 = 1. Bottom: X0 = 4

Introduction to Dynamics

85

Exercise 4.3.1 Conﬁrm that ψM ∈ P (S) whenever ψ ∈ P (S).
To set P (S) up as a metric space, we deﬁne

 ψ 1 : =

∑ |ψ(x)| for each ψ ∈ P (S),

x ∈S

and

d1 (ψ, ψ ) := ψ − ψ 1

If one views P (S) as the unit simplex in N rather than as a space of functions (see
the correspondence (4.6) on page 68), then our norm and distance are just the regular
 · 1 norm (see page 37) and d1 distance on N . Viewed in this way, P (S) is a closed
and bounded subset of ( N , d1 ), and therefore both compact and complete.18
Exercise 4.3.2 Let ψ1 , ψ2 ∈ P (S), and for each A ⊂ S let Ψi ( A) := ∑ x∈ A ψi ( x ) =
the probability of A ⊂ S according to distribution ψi . Show that ψ1 − ψ2 1 =
2 sup A⊂S |Ψ1 ( A) − Ψ2 ( A)|.19
To illustrate the dynamical system (P (S), M) and its trajectories, consider Hamilton’s kernel p H and the corresponding operator M H . Here P (S) can be visualized as
the unit simplex in 3 . Figure 4.13 shows four trajectories (ψMtH ) generated by iterating M H on four different initial conditions ψ. All trajectories converge toward the
bottom right-hand corner. Indeed, we will prove below that (P (S), M H ) is globally
stable.
Exercise 4.3.3 Let M be the Markov operator determined by an arbitrary stochastic
kernel p. Show that M is d1 -nonexpansive on P (S), in the sense that for any ψ, ψ ∈
P (S) we have d1 (ψM, ψ M) ≤ d1 (ψ, ψ ).
Let us now turn to the existence of ﬁxed points for the system (P (S), M). For
Markov chains, ﬁxed points are referred to as stationary distributions:
Deﬁnition 4.3.4 A distribution ψ∗ ∈ P (S) is called stationary or invariant for M if
ψ∗ M = ψ∗ . In other words, ψ∗ is a stationary distribution for M if it is a ﬁxed point of
the dynamical system (P (S), M).
If ψ∗ is stationary for M, if M corresponds to kernel p, if ( Xt )t≥0 is Markov-( p, ψ),
and if Xt has distribution ψ∗ for some t, then Xt+1 has distribution ψt+1 = ψt M =
18 Interested readers are invited to supply the details of the argument. The connection between the function space (P (S), d1 ) and the unit simplex in ( N , d1 ) can be made precise using the concept of isomorphisms. Metric spaces (S, ρ) and (S , ρ ) are said to be isometrically isomorphic if there exists a bijection
τ : S → S such that ρ( x, y) = ρ (τ ( x ), τ (y)) for all x, y ∈ S. In our case, the bijection in question is (4.6)
on page 68. If (S, ρ) and (S , ρ ) are isometrically isomorphic, then (S, ρ) is complete if and only if (S , ρ ) is
complete, and compact if and only if (S , ρ ) is compact.
19 Hint: The set that attains the supremum in this expression is A : = { x ∈ S : ψ ( x ) ≥ ψ ( x )}. Make
2
1
use of the fact that ∑ x∈ A (ψ1 ( x ) − ψ2 ( x )) = ∑ x∈ Ac (ψ2 ( x ) − ψ1 ( x )). (Why?) Now decompose the sum in
ψ1 − ψ2 1 into a sum over A and Ac .

86

Chapter 4

Figure 4.13 Trajectories of (P (S), M H )

ψ∗ M = ψ∗ . In fact iteration shows that Xt+k has distribution ψ∗ for every k ∈ ,
so probabilities are stationary over time. Moreover if ( Xt )t≥0 is Markov-( p, ψ∗ ), then
Xt ∼ ψ∗ for all t, and the random variables ( Xt )t≥0 are identically distributed (but not
IID —why?).
On the other hand, stationary distributions are just ﬁxed points of a dynamical
system (P (S), M). This is convenient for analysis because we already know various
techniques for studying ﬁxed points and stability properties of deterministic dynamical systems. For example, suppose that we view P (S) as the unit simplex in N ,
and ψ → ψM as postmultiplication of vector ψ ∈ N by the matrix corresponding to
p. This mapping is d1 -nonexpansive (recall exercise 4.3.3), and hence d1 -continuous
(exercise 3.2.33, page 52). The unit simplex is a compact, convex subset of ( N , d1 ).
(Proof?) Applying Brouwer’s theorem (theorem 3.2.32, page 52) we obtain our ﬁrst
major result for Markov chains:
Theorem 4.3.5 Every Markov chain on a ﬁnite state space has at least one stationary distribution.
There may, of course, be many stationary distributions, just as other dynamical
systems can have many ﬁxed points.
Exercise 4.3.6 For which kernel p is every ψ ∈ P (S) stationary?
Let’s consider a technique for computing ﬁxed points using matrix inversion. In
terms of linear algebra, row vector ψ ∈ P (S) is stationary if and only if ψ(I N − p) = 0,

Introduction to Dynamics

87

Listing 4.6 (fphamilton.py) Computing stationary distributions
from numpy i m p o r t ones , identity , transpose
from numpy . linalg i m p o r t solve
pH = ((0.971 , 0.029 , 0.000) ,
(0.145 , 0.778 , 0.077) ,
(0.000 , 0.508 , 0.492) )

# Hamilton ’s kernel

I = identity (3)
Q , b = ones ((3 , 3) ) , ones ((3 , 1) )
A = transpose ( I - pH + Q )
print ( solve (A , b ) )

# 3 by 3 identity matrix
# Matrix and vector of ones

where I N is the N × N identity matrix, and p is the matrix in (4.7). One idea would
be to try to invert (I N − p). However, this does not impose the restriction that the
solution ψ is an element of P (S). That restriction can be imposed in the following
way.
Exercise 4.3.7 Let N be the 1 × N row vector (1, . . . , 1). Let
matrix of ones. Show that if ψ is stationary, then
N

= ψ(I N − p +

N×N

be the N × N

N×N )

(4.18)

Explain how this imposes the restriction that the elements of ψ sum to 1.
Taking the transpose of (4.18) we get (I N − p + N × N ) ψ = 
N . This is a linear
system of the form Ax = b, which can be solved for x = A−1 b. (The solution is not
necessarily unique. We return to the issue of uniqueness below.) Listing 4.6 shows
how to do this in Python using NumPy.
Exercise 4.3.8 Use this technique to solve for the stationary distribution of Quah’s
kernel p Q .20 Plot it as a bar plot, and compare with the t = 1000 distributions in
ﬁgure 4.12.
Exercise 4.3.9 Recall the ﬁrm introduced on page 79. Compute expected proﬁts at
the stationary distribution. Compare it with proﬁts at t = 1000, as computed in exercise 4.2.13.
20 We

prove below that the ﬁxed point is unique.

88

Chapter 4

Exercise 4.3.10 According to the deﬁnition of the stationary distribution, if ψ∗ is stationary for p H and if X0 ∼ ψ∗ , then Xt ∼ ψ∗ for all t. Using algorithm 4.2 (page 74),
check this for p H by drawing X0 from the stationary distribution calculated in listing 4.6 and then computing an approximation to the distribution ψT of XT when
T = 20.

4.3.2

The Dobrushin Coefﬁcient

Now let’s consider convergence to the stationary distribution. We continue to impose on P (S) the distance d1 and study the dynamical system (P (S), M). By deﬁnition 4.1.5, the system (P (S), M) is globally stable if
1. it has a unique ﬁxed point (stationary distribution) ψ∗ ∈ P (S), and
2. d1 (ψMt , ψ∗ ) := ψMt − ψ∗ 1 → 0 as t → ∞ for all ψ ∈ P (S).
The second condition implies that if ( Xt )t≥0 is Markov-( p, ψ) for some ψ ∈ P (S),
then the distribution of Xt converges to ψ∗ .21
Exercise 4.3.11 Exercise 4.3.6 asked you to provide an example of a kernel where
global stability fails. Another is the “periodic” Markov chain


0 1
p=
1 0
Show that ψ∗ := (1/2, 1/2) is the unique stationary distribution. Give a counterexample to the claim ψMt − ψ∗ 1 → 0 as t → ∞, ∀ψ ∈ P (S).
How might one check for stability of a given kernel p and associated dynamical
system (P (S), M)? Exercise 4.3.3 suggests the way forward: M is nonexpansive on
P (S), and if we can upgrade this to a uniform contraction then Banach’s ﬁxed point
theorem (page 53) implies that (P (S), M) is globally stable, and that convergence to
equilibrium takes place at a geometric rate.
Which kernels will we be able to upgrade? Intuitively, stable kernels are those
where current states have little inﬂuence on future states. An extreme example is
where the distributions p( x, dy) are all equal: p( x, dy) = q ∈ P (S) for all x ∈ S. In
this case the current state has no inﬂuence on tomorrow’s state—indeed, the resulting
process is IID with Xt ∼ q for all t. The Markov operator satisﬁes ψM = q for all
ψ ∈ P (S) (check it), and (P (S), M) is globally stable.
A less extreme case is when the distributions p( x, dy) are “similar” across x ∈
S. One similarity measure for two distributions p( x, dy) and p( x , dy) is ∑y p( x, y) ∧
21 In

this context, global stability is sometimes referred to as ergodicity.

Introduction to Dynamics

89

p( x , y), where a ∧ b := min{ a, b}. If p( x, dy) = p( x , dy) then the value is one. If the
supports22 of p( x, dy) and p( x , dy) are disjoint, then the value is zero. This leads us
to the Dobrushin coefﬁcient, which measures the stability properties of a given kernel
p.
Deﬁnition 4.3.12 Given stochastic kernel p, the Dobrushin coefﬁcient α( p) is deﬁned
by


α( p) := min

∑ p(x, y) ∧ p(x , y)

y∈S

: ( x, x ) ∈ S × S

(4.19)

Exercise 4.3.13 Prove that 0 ≤ α( p) ≤ 1 always holds.
Exercise 4.3.14 Show that α( p) = 1 if and only if p( x, dy) is equal to a constant distribution q ∈ P (S) for every x ∈ S.
Exercise 4.3.15 Show that α( p) = 0 for the periodic kernel in exercise 4.3.11, and for
p corresponding to the identity matrix.
Exercise 4.3.16 Distributions φ and ψ are said to overlap if there exists a y such that
φ(y) > 0 and ψ(y) > 0. Show that α( p) > 0 if and only if for each pair ( x, x ) ∈ S × S
the distributions p( x, dy) and p( x , dy) overlap.
The following result links the Dobrushin coefﬁcient to stability via Banach’s ﬁxed
point theorem (page 53).
Theorem 4.3.17 If p is a stochastic kernel on S with Markov operator M, then

φM − ψM1 ≤ (1 − α( p))φ − ψ1

∀ φ, ψ ∈ P (S)

Moreover this bound is the best available, in the sense that if λ < 1 − α( p), then there exists
a pair φ, ψ in P (S) such that φM − ψM1 > λφ − ψ1 .
The ﬁrst half of the theorem says that if α( p) > 0, then M is uniformly contracting
(for the deﬁnition see page 52) with modulus 1 − α( p). Since (P (S), d1 ) is complete,
Banach’s ﬁxed point theorem then implies global stability of (P (S), M). The second
part of the theorem says that this rate 1 − α( p) is the best available, which in turn suggests that the Dobrushin coefﬁcient is a good measure of the stability properties of M.
For example, if α( p) = 0, then we can be certain M is not a uniform contraction.
Some intuition for theorem 4.3.17 and it’s stability implications was discussed
above. The coefﬁcient is large (close to one) when all distributions p( x, dy) are similar
across x, and the current state has little inﬂuence on future states. This is the stable
case. The coefﬁcient is zero when there exists states x and x such that p( x, dy) and
22 The

support of φ ∈ P (S) is {y ∈ S : φ(y) > 0}.

90

Chapter 4

p( x , dy) have disjoint support, as with the identity kernel and the periodic kernel.
More intuition on the link between positivity of α( p) and stability is given in the next
section.
The proof of theorem 4.3.17 is given in the appendix to this chapter. The fact that
1 − α( p) is the best rate possible may suggest to you that the proof is not entirely
trivial. Indeed this is the case. We have to do better than crude inequalities. All but
the most enthusiastic readers are encouraged to skip the proof and move to the next
section.

4.3.3

Stability

Let p be a stochastic kernel on S. If α( p) > 0, then (P (S), M) is globally stable by
Banach’s ﬁxed point theorem. In fact we can say a bit more. We now present our main
stability result for ﬁnite chains, which clariﬁes the relationship between the Dobrushin
coefﬁcient and stability.
Theorem 4.3.18 Let p be a stochastic kernel on S with Markov operator M. The following
statements are equivalent:
1. The dynamical system (P (S), M) is globally stable.
2. There exists a t ∈

such that α( pt ) > 0.

Another way to phrase the theorem is that (P (S), M) is globally stable if and only
if there is a t ∈
such that, given any pair of states x, x , one can ﬁnd at least one
state y such that pt ( x, y) and pt ( x , y) are both positive. Thus, if we run two Markov
chains from any two starting points x and x , there is a positive probability that the
chains will meet. This is connected with global stability because it rules out the kind of
behavior seen in example 4.1.8 (page 57), where initial conditions determine long-run
outcomes.
Exercise 4.3.19 Consider the periodic kernel in exercise 4.3.11. Show that α( pt ) = 0
for every t ∈ .
Exercise 4.3.20 Prove that if minx∈S pt ( x, ȳ) =:  > 0 for some ȳ ∈ S, then α( pt ) ≥ ,
and hence (P (S), M) is globally stable.
Exercise 4.3.21 Stokey and Lucas (1989, thm. 11.4) prove that (P (S), M) is globally
stable if there exists a t ∈ such that ∑y∈S minx∈S pt ( x, y) > 0. Show how this result
is implied by theorem 4.3.18.
Exercise 4.3.22 Prove theorem 4.3.18. To show that (2) implies (1), use Lemma 4.2.8
(page 76) and Theorem 4.3.17. To show that (1) implies (2), let ψ∗ be the stationary

Introduction to Dynamics

91

distribution. Note that ∃ȳ ∈ S with ψ∗ (ȳ) > 0. Argue that pt ( x, ȳ) → ψ∗ (ȳ) for any x.
Using ﬁniteness of S, show that there is a t ∈
with minx∈S pt ( x, ȳ) > 0. Conclude
t
that α( p ) > 0.
Let’s consider how to apply theorem 4.3.18. In view of exercise 4.3.20, if there
exists a y with p( x, y) > 0 for all x ∈ S, then α( p) > 0 and global stability holds. A
case in point is Hamilton’s kernel (4.8) on page 70, which is globally stable as a result
of the strict positivity of column two.
Next consider Quah’s kernel pQ (page 71). We know from theorem 4.3.5 that at
least one stationary distribution exists, and we calculated a stationary distribution in
exercise 4.3.8. We should now check that there are not many stationary distributions—
otherwise exhibiting one of them is not very interesting. Also, the stationary distribution becomes a better predictor of outcomes if we know that all trajectories converge
to it.
Exercise 4.3.23 Show that the Dobrushin coefﬁcient α( pQ ) is zero.
Since α( pQ ) = 0, let’s look at the higher order iterates. In his study Quah calculates
the 23rd-order kernel
⎛
⎞
0.61 0.27 0.09 0.03 0.00
⎜ 0.37 0.32 0.20 0.09 0.02 ⎟
⎜
⎟
⎜ 0.14 0.23 0.31 0.25 0.07 ⎟
p23
(4.20)
=
Q
⎜
⎟
⎝ 0.04 0.11 0.25 0.39 0.22 ⎠
0.00

0.01

0.04

0.12

0.82

Exercise 4.3.24 Show that α( p23
Q ) > 0.
Exercise 4.3.25 As (P (S), MQ ) is globally stable, we can iterate MQ on any initial
condition ψ to calculate an approximate ﬁxed point ψ∗ . Take ψ = (1, 0, 0, 0, 0) as your
initial condition and iterate until d1 (ψMtQ , ψMtQ+1 ) < 0.0001. Compare your result
with that of exercise 4.3.8.
Exercise 4.3.26 Code a function that takes a kernel p as an argument and returns α( p).
Write another function that repeatedly calls the ﬁrst function to compute the smallest
t ≥ 1 such that α( pt ) > 0, and prints that t along with the value α( pt ). Include a
maximum value T such that if t reaches T the function terminates with a message that
α( pt ) = 0 for all t ≤ T. Now show that the ﬁrst t such that α( ptQ ) > 0 is 2.
One interesting fact regarding stationary distributions is as follows: Let p be a
kernel such that (P (S), M) is globally stable, and let ψ∗ be the unique stationary
distribution. Let ( Xt )t≥0 be Markov-( p, x ), where ψ∗ ( x ) > 0. The return time to x is
deﬁned as the random variable
τ ( x ) := inf{t ≥ 1 : Xt = x }

92

Chapter 4

It turns out that for τ ( x ) so deﬁned we have τ ( x ) = 1/ψ∗ ( x ). We will skip the proof
(see Norris, 1997, thm. 1.7.7), but let’s try running a simulation. The pseudocode in
algorithm 4.3 indicates how one might go about estimating τ ( x ).23
Algorithm 4.3 Computing the mean return time
for i in 1 to n do
set t = 0
set X = x
repeat
draw X ∼ p( X, dy)
set t = t + 1
until X = x
set τi = t
end
return n−1 ∑in=1 τi

// n is the number of replications

Exercise 4.3.27 Implement algorithm 4.3 for Hamilton’s Markov chain. Examine
whether for ﬁxed x ∈ S the output converges to 1/ψ∗ ( x ) as n → ∞.
Finally, let’s consider a slightly more elaborate application, which concerns socalled (s, S) inventory dynamics. Inventory management is a major topic in operations research that also plays a role in macroeconomics due to the impact of inventories on aggregate demand. The discrete choice ﬂavor of (s, S) models accord well with
the data on capital investment dynamics.
Let q, Q ∈ {0} ∪ with q ≤ Q, and consider a ﬁrm that, at the start of time t, has
inventory Xt ∈ {0, . . . , Q}. Here Q is the maximum level of inventory that the ﬁrm is
capable of storing. (We are studying (q, Q) inventory dynamics because the symbol
S is taken.) If Xt ≤ q, then the ﬁrm orders inventory Q − Xt , bringing the current
stock to Q. If Xt > q then the ﬁrm orders nothing. At the end of the period t demand
Dt+1 is observed, and the ﬁrm meets this demand up to its current stock level. Any
remaining inventory is carried over to the next period. Thus

max{ Q − Dt+1 , 0} if Xt ≤ q
Xt +1 =
max{ Xt − Dt+1 , 0} if Xt > q
If we adopt the notation x + := max{ x, 0} and let { x ≤ q} be one when x ≤ q and
zero otherwise, then this can be rewritten more simply as
X t +1 = ( X t + ( Q − X t ) { X t ≤ q } − Dt +1 ) +
ψ∗ ( x ) > 0, then ( Xt )t≥0 returns to x (inﬁnitely often) with probability one, so the algorithm terminates in ﬁnite time with probability one.
23 If

Introduction to Dynamics

93

or, if hq ( x ) := x + ( Q − x ) { x ≤ q} is the stock on hand after orders for inventory are
completed, as
X t + 1 = ( h q ( X t ) − Dt +1 ) +
We assume throughout that ( Dt )t≥1 is an IID sequence taking values in {0} ∪
cording to distribution b(d) := { Dt = d} = (1/2)d+1 .

ac-

Exercise 4.3.28 Let S = {0, 1, . . . , Q}. Given an expression for the stochastic kernel
p( x, y) corresponding to the restocking policy q.24
Exercise 4.3.29 Let Mq be the corresponding Markov operator. Show that (P (S), Mq )
is always globally stable independent of the precise values of q and Q. Let ψq∗ denote the
stationary distribution corresponding to threshold q. Show numerically that if Q = 5,
then
ψ2∗ = (0.0625, 0.0625, 0.125, 0.25, 0.25, 0.25)
Now consider proﬁts of the ﬁrm. To minimize the number of parameters, suppose
that the ﬁrm buys units of the product for zero dollars and marks them up by one dollar. Revenue in period t is min{hq ( Xt ), Dt+1 }. Placing an order for inventory incurs
ﬁxed cost C. As a result proﬁts for the ﬁrm at time t are given by
πq ( Xt , Dt+1 ) = min{ hq ( Xt ), Dt+1 } − C { Xt ≤ q}
If we now sum across outcomes for Dt+1 taking Xt = x as given, then we get
gq ( x ) : =

[πq ( x, Dt+1 )] =

∞

∞

πq ( x, d)
d +1
d =0 2

∑ πq (x, d)b(d) = ∑

d =0

which is interpreted as expected proﬁts in the current period when the inventory state
Xt is equal to x.
Exercise 4.3.30 One common performance measure for an inventory strategy (in this
case, a choice of q) is long-run average proﬁts, which is deﬁned here as gq ( X ) when
X ∼ ψq∗ (i.e., ∑ x∈S gq ( x )ψq∗ ( x )). Show numerically that according to this performance
measure, when Q = 20 and C = 0.1, the optimal policy is q = 7.

4.3.4

The Law of Large Numbers

Let’s end our discussion of stability by investigating some probabilistic properties of
sample paths. In particular, we investigate the law of large numbers (LLN) in the
context of Markov chains.
24 Hint:

If a is any integer and D has the distribution b deﬁned above, then what is the probability that
( a − D )+ = y? You might ﬁnd it convenient to ﬁrst consider the case y = 0 and then the case y > 0.

94

Chapter 4

In algorithm 4.2 (page 74) we computed an approximation to the marginal distribution ψt via Monte Carlo. The basis of Monte Carlo is that if we sample independently from a ﬁxed probability distribution and count the fraction of times that an
event happens, that fraction converges to the probability of the event (as determined
by this probability distribution). This is more or less the frequentist deﬁnition of probabilities, but it can also be proved from the axioms of probability theory. The theorem
in question is the law of large numbers (LLN), a variation of which is as follows:
Theorem 4.3.31 If F is a cumulative
distribution function on

is a measurable function with |h( x )| F (dx ) < ∞, then
1
n

n

∑ h ( Xi ) →

i =1

h ( X1 ) : = :



h( x ) F (dx )

IID

, ( Xt )t≥1 ∼ F, and h :

as n → ∞ with probability one

→

(4.21)

This result is fundamental to statistics. It states that for IID sequences, sample
means converge to means as the sample size gets large. Later we will give a formal deﬁnition of independence and prove a version of the theorem. At that time the
term “measurable function” and the nature of probability one convergence will be
discussed. Sufﬁce to know that measurability of h is never a binding restriction for
the problems we consider.
Example 4.3.32 If ( Xi )in=1 are independent standard normal random variates, then
according to theorem 4.3.31 we should ﬁnd that n−1 ∑in=1 Xi2 → 1. (Why?) You can try
this out in Python using some variation of
>>> from random i m p o r t normalvariate
>>> Y = [ normalvariate (0 , 1)**2 f o r i i n range (10000)]
>>> sum ( Y ) / 10000

Another use of the LLN: Suppose that we wish to compute h( X ), where h is some
real function. One approach would
be to use pen and paper plus our knowledge of
∞
calculus to solve the integral −∞ h( x ) F (dx ). In some situations, however, this is not
so easy. If instead we have access to a random number generator that can generate
independent draws X1 , X2 , . . . from F, then we can produce a large number of draws,
take the mean of the h( Xi ) terms, and appeal to (4.21).
In (4.21) the sequence of random variables is IID. In some situations the LLN extends to sequences that are neither independent nor identically distributed. For example, we have the following result concerning stable Markov chains:
Theorem 4.3.33 Let S be ﬁnite, let ψ ∈ P (S), let p be a stochastic kernel on S with α( pt ) >
0 for some t ∈ , and let h : S → . If ( Xt )t≥0 is Markov-( p, ψ), then
1
n

n

∑ h ( Xt ) → ∑ h ( x ) ψ ∗ ( x )

t =1

x ∈S

as n → ∞ with probability one

(4.22)

Introduction to Dynamics

95

where ψ∗ is the unique stationary distribution of p.
The left-hand side is the average value of h( Xt ), and the right-hand side is the
expectation of h( X ) when X ∼ ψ∗ . Note that the result holds for every initial condition
ψ ∈ P ( S ).
The proof of theorem 4.3.33 requires more tools than we have in hand.25 The intuition is that when the chain is globally stable, Xt is approximately distributed according to ψ∗ for large t. In addition the stability property implies that initial conditions
are unimportant, and for the same reason Xt has little inﬂuence on Xt+k for large k.
Hence there is a kind of asymptotic independence in the chain. Together, these two
facts mean that our chain approximates the IID property that drives the LLN.
If h( x ) = 1 if x = y and zero otherwise (i.e., h( x ) = { x = y}), then (4.22) becomes
1
n

n

1

n

∑ h ( Xt ) = n ∑

t =1

t =1

{ Xt = y} → ψ∗ (y) as n → ∞

(4.23)

This provides a new technique for computing the stationary distribution, via Monte
Carlo. Exercise 4.3.34 illustrates.
Exercise 4.3.34 Let p H be Hamilton’s kernel, and let h( x ) = 1 if x = NG and zero
otherwise. Take any initial condition, and draw a series of length 50,000. Compute the
left-hand side of (4.22). Compare it with the right-hand side, which was calculated in
listing 4.6 on page 87.
When the state space is small, this is a less efﬁcient technique for computing the
stationary distribution than the algebraic method used in listing 4.6. However, the
computational burden of the algebraic method increases rapidly with the size of the
state space. For large or inﬁnite state spaces, a variation of the LLN technique used in
exercise 4.3.34 moves to center stage. See §6.1.3 for details.26
The importance of theorem 4.3.33 extends beyond this new technique for computing stationary distributions. It provides a new interpretation for the stationary distribution: If we turn (4.23) around, we get
ψ∗ (y)  fraction of time that ( Xt ) spends in state y
This is indeed a new interpretation of ψ∗ , although it is not generally valid unless the
chain in question is stable (in which case the LLN applies).
Exercise 4.3.35 Give an example of a kernel p and initial condition ψ where this interpretation fails.
25 A

weak version of theorem 4.3.33 is proved in §11.1.1.

26 The look-ahead method introduced in §6.1.3 concerns inﬁnite state spaces, but it can be applied to ﬁnite

state spaces with the obvious modiﬁcations.

96

Chapter 4

In the preceding discussion, h was an indicator function, which reduced the discussion of expectations to one of probabilities. Now let’s consider more general expectations.
Exercise 4.3.36 Recall the ﬁrm introduced on page 79. Extending exercise 4.3.34, approximate expected proﬁts at the stationary distribution using theorem 4.3.33. Compare your results to those of exercise 4.3.9.
Thus the LLN provides a new way to compute expectations with respect to stationary distributions. However, as was the case with probabilities above, it also provides
a new interpretation of these expectations when the Markov chain is stationary. For
example, if h denotes proﬁts as above, then we have

∑ h( x )ψ∗ ( x ) 

long-run average proﬁts

x ∈S

Again, this interpretation is valid when the chain in question is stationary, but may
not be valid otherwise.

4.4

Commentary

Regarding deterministic, discrete-time dynamical systems, a good mathematical introduction is provided by Holmgren (1996), who treats elementary theory, topological
conjugacy, and chaotic dynamics. For dynamics from an economic perspective, see,
for example, Stokey and Lucas (1989), Azariadis (1993), de la Fuente (2000), Turnovsky
(2000), Shone (2003), Ljungqvist and Sargent (2004), Caputo (2005), or Gandolfo (2005).
The threshold externality model in example 4.1.8 is a simpliﬁed version of Azariadis and Drazen (1990). See Durlauf (1993) for a stochastic model with multiple equilibria.
Our discussion of chaotic dynamics lacked any economic applications, but plenty
exist. The Solow–Swan model produces chaotic dynamics with some minor modiﬁcations (e.g., Böhm and Kaas 2000). Moreover rational behavior in inﬁnite-horizon,
optimizing models can lead to chaos, cycles, and complex dynamics. See, for example, Benhabib and Nishimura (1985), Boldrin and Montrucchio (1986), Sorger (1992),
Nishimura et al. (1994), Venditti (1998), or Mitra and Sorger (1999). For more discussion of complex economic dynamics, see Grandmont (1985), Chiarella (1988), Galor (1994), Medio (1995), Brock and Hommes (1998), Barnett and Serletis (2000), or
Kikuchi (2008).
Two good references on ﬁnite state Markov chains are Norris (1997) and Häggström (2002). These texts provide a more traditional approach to stability of Markov
chains based on irreducibility and aperiodicity. It can be shown that every irreducible

Introduction to Dynamics

97

and aperiodic Markov chain is globally stable, and as a result satisﬁes the conditions
of theorem 4.3.18 (in particular, α( pt ) > 0 for some t ∈ ). The converse is not true,
so theorem 4.3.18 is more general.
The Dobrushin coefﬁcient was introduced by Dobrushin (1956). For an alternative
discussion of the Dobrushin coefﬁcient in the context of ﬁnite state Markov chains, see
Bremaud (1999).
The treatment of (s, S) dynamics in §4.3.3 is loosely based on Norris (1997). For
another discussion of inventory dynamics see Stokey and Lucas (1989, sec. 5.14). An
interesting analysis of aggregate implications is Nirei (2008). A modern treatment of
discrete adjustment models can be found in Stokey (2008).

Chapter 5

Further Topics for Finite MCs
We have now covered the fundamental theory of ﬁnite state Markov chains. Next let
us turn to more applied topics. In §5.1 below we consider the problem of dynamic
programming, that is, of controlling Markov chains through our actions in order to
achieve a given objective. In §5.2 we investigate the connection between Markov
chains and stochastic recursive sequences.

5.1

Optimization

In this section we take our ﬁrst look at stochastic dynamic programming. The term
“dynamic programming” was coined by Richard Bellman in the early 1950s, and pertains to a class of multistage planning problems. Because stochastic dynamic programming problems typically involve Markov chains, they are also called Markov
decision problems, Markov control problems, or Markov control processes. We will
focus on solving a simple example problem, and defer the more difﬁcult proofs until
chapter 10.

5.1.1

Outline of the Problem

Our objective is to model the behavior of Colonel Kurtz, who can be found on a small
island in the Nung river living on catﬁsh. The local catﬁsh bite only at dawn, and the
Colonel bags a random quantity W ∈ {0, . . . , B}, W ∼ φ. Catﬁsh spoil quickly in the
tropics, lasting only if refrigerated. The Colonel has limited refrigeration facilities: A
maximum of M ﬁsh can ﬁt into his freezer. We let Xt be the stock of ﬁsh at noon on
day t, from which a quantity Ct is consumed. The remainder Rt = Xt − Ct is frozen.
Following the next morning’s ﬁshing trip, the state is Xt+1 = Rt + Wt+1 .
99

100

Chapter 5

Being economists, we do not hesitate for a second to model Colonel Kurtz as a
rational intertemporal maximizer. We estimate his discount factor to be ρ, and his
period utility function to be U. We assume that Kurtz follows a ﬁxed policy function σ:
On observing state Xt , the Colonel saves quantity Rt = σ ( Xt ). For the state space we
take S := {0, . . . , M + B}, as Xt ≤ M + B always holds. (Why?) The function σ is a
map from S into {0, . . . , M} that satisﬁes the feasibility constraint 0 ≤ σ ( x ) ≤ x for all
x ∈ S. We denote the set of all feasible policies by Σ. For given σ ∈ Σ the state evolves
according to
Xt+1 = σ ( Xt ) + Wt+1 ,

IID

(Wt )t≥1 ∼ φ,

X0 = x ∈ S

(5.1)

Exercise 5.1.1 For each a ∈ {0, . . . , M}, let γ( a, dy) ∈ P (S) be the distribution of
a + W. That is, { a + W = y} = γ( a, y) for y ∈ S. Check that S × S
( x, y) →
γ(σ ( x ), y) ∈ is a stochastic kernel on S.
Using this deﬁnition of γ, we can now see that the distribution of Xt+1 given Xt is
γ(σ ( Xt ), dy), and that ( Xt )t≥0 in (5.1) is Markov-( pσ , x ), where pσ ( x, y) := γ(σ ( x ), y).
Let Mσ be the Markov operator corresponding to pσ . For any given h : S →
we
have (see (4.14) on page 78)
Mtσ h( x ) =

∑ ptσ (x, y)h(y)

y∈S

( t ≥ 0)

(5.2)

What’s important to remember from all this is that when Colonel Kurtz chooses a
policy he chooses a Markov chain on S as well.
We model the optimization problem of Colonel Kurtz as


∞

max
σ∈Σ

∑ ρt U (Xt − σ(Xt ))

t =0

for ( Xt )t≥0 given by (5.1)

(5.3)

Let’s try to understand this objective function as clearly as possible. For each ﬁxed σ,
the Markov chain is determined by (5.1). As discussed further in chapter 9, random
variables are best thought of as functions on some underlying space Ω that contains
the possible outcomes of a random draw. At the start of time, nature selects an element ω ∈ Ω according to a given “probability” . The shocks (Wt )t≥1 are functions
of this outcome, so the draw determines the path for the shocks as (Wt (ω ))t≥1 .1 From
the rule Xt+1 (ω ) = σ ( Xt (ω )) + Wt+1 (ω ) and X0 (ω ) = x we obtain the time path
( Xt (ω ))t≥0 for the state. In turn each path gives us a real number Yσ (ω ) that corresponds to the value of the path:
Yσ (ω ) =

∞

∑ ρt U (Xt (ω ) − σ(Xt (ω )))

t =0

(ω ∈ Ω)

(5.4)

that although all random outcomes are determined at the start of time by the realization of ω, the
time t value Wt (ω ) is regarded as “unobservable” prior to t.
1 Note

Further Topics for Finite MCs

101

The value Yσ is itself a random variable, being a function of ω. The objective function
is the expectation of Yσ .
For probabilities on a ﬁnite set Ω, the expectation of a random variable Y is given
by the sum ∑ω ∈Ω Y (ω ) {ω }. However, in the present case it turns out that Ω must be
uncountable (see deﬁnition A.1.4 on page 322), and this sum is not deﬁned. We will
have to wait until we have discussed measure theory before a general deﬁnition of
expectation on uncountable spaces can be constructed. We can, however, approximate
(5.3) by truncating at some (large but ﬁnite) T ∈ . This takes us back to a ﬁnite
scenario treated above: For given σ ∈ Σ, the chain ( Xt )tT=0 is Markov-( pσ , x ), and
we can construct its joint probabilities via (4.17) on page 80. This joint distribution
is deﬁned over the ﬁnite set S T +1 , and hence expectations with respect to it can be
computed with sums. In particular, if
F : S T +1

x := ( x0 , . . . , x T ) →

T

∑ ρt U (xt − σ(xt )) ∈

t =0

and q T +1 is the joint distribution of ( Xt )tT=0 , then


T

∑ ρt U (Xt − σ(Xt ))

∑

=

t =0

x ∈ S T +1

F ( x ) q T +1 ( x )

(5.5)

In fact we can simplify further using linearity of expectations:


T

∑ ρt U (Xt − σ(Xt ))

t =0

=

T

∑ ρt

t =0

U ( Xt − σ ( Xt ))

Letting rσ ( x ) := U ( x − σ( x )) and using (5.2), we can write
U ( Xt − σ ( Xt )) =


∴

r σ ( Xt ) =

∑ ptσ (x, y)rσ (y) = Mtσ rσ (x)

y∈S



T

∑ ρ U (Xt − σ(Xt ))
t

t =0

=

T

∑ ρt Mtσ rσ (x)

(5.6)

t =0

With some measure theory it can be shown (see chapter 10) that the limit of the righthand side of (5.6) is equal to the objective function in the inﬁnite horizon problem
(5.3). In particular, if vσ ( x ) denotes total reward under policy σ when starting at initial
condition x ∈ S, then


vσ ( x ) :=

∞

∑ ρt U (Xt − σ(Xt ))

t =0

=

∞

∑ ρt Mtσ rσ (x)

t =0

(5.7)

102

5.1.2

Chapter 5

Value Iteration

The term vσ ( x ) in (5.7) gives the expected discounted reward from following the policy σ. Taking x as given, our job is to ﬁnd a maximizer of vσ ( x ) over the set of policies
Σ. The ﬁrst technique we discuss is value iteration. To begin, deﬁne the value function
v∗ ( x ) := sup{vσ ( x ) : σ ∈ Σ}

( x ∈ S)

(5.8)

The value function satisﬁes a restriction known as the Bellman equation. Letting
Γ( x ) := {0, 1, . . . , x ∧ M }

x ∧ M := min{ x, M}

be the set of all feasible actions (number of ﬁsh that can be frozen) when the current
state is x, the Bellman equation can be written as


v∗ ( x ) = max

a∈Γ( x )

U ( x − a) + ρ

B

∑ v∗ ( a + z)φ(z)

z =0

( x ∈ S)

(5.9)

The idea behind (5.9)—which we later prove in some detail—is that if one knows the
values of different states in terms of maximum future rewards, then the best action is
found by trading off the two effects inherent in choosing an action: current reward and
future reward after transitioning to a new state next period (the transition probabilities
being determined by the action). The result of making this trade-off optimally is the
maximum value from the current state, which is the left-hand side of (5.9).
Given w : S → , we say that σ ∈ Σ is w-greedy if


σ( x ) ∈ argmax U ( x − a) + ρ
a∈Γ( x )

B

∑ w( a + z)φ(z)

z =0

( x ∈ S)

(5.10)

Further, a policy σ ∈ Σ is called optimal if vσ = v∗ , which is to say that the value
obtained from following σ is the maximum possible. A key result of chapter 10 is
that a policy σ∗ is optimal if and only if it is v∗ -greedy. Hence computing an optimal
policy is trivial if we know the value function v∗ , since we need only solve (5.10) for
each x ∈ S, using v∗ in place of w.
So how does one solve for the value function? Equation (5.9) is a tail-chasing
equation: if we know v∗ , then we can substitute it into the right-hand side and ﬁnd v∗ .
When it comes to such equations involving functions, Banach’s ﬁxed point theorem
(page 53) can often be used to unravel them. Let bS be the set of functions w : S → ,2
and deﬁne the Bellman operator bS v → Tv ∈ bS by


Tv( x ) = max

a∈Γ( x )

2 This

U ( x − a) + ρ

B

∑ v( a + z)φ(z)

z =0

( x ∈ S)

(5.11)

is our usual notation for the bounded functions from S to . Since S is ﬁnite, all real-valued functions on S are bounded, and bS is just the real-valued functions on S.

Further Topics for Finite MCs

103

As will be proved in chapter 10, T is a uniform contraction of modulus ρ on (bS, d∞ ),
where d∞ (v, w) := supx∈S |v( x ) − w( x )|. By construction, Tv∗ ( x ) = v∗ ( x ) for all x ∈ S
(check it), so v∗ is a ﬁxed point of T. From Banach’s ﬁxed point theorem, v∗ is the only
ﬁxed point of T in bS, and d∞ ( T n v, v∗ ) → 0 as n → ∞ for any given v ∈ bS.3
Algorithm 5.1 Value iteration algorithm
pick any v ∈ bS
repeat
compute Tv from v
set e = d∞ ( Tv, v)
set v = Tv
until e is less that some tolerance
solve for a v-greedy policy σ

This suggests the value iteration algorithm presented in algorithm 5.1.4 If the tolerance is small, then the algorithm produces a function T n v that is close to v∗ . Since
v∗ -greedy policies are optimal, and since T n v is almost equal to v∗ , it seems likely that
T n v-greedy policies are almost optimal. This intuition is correct, and will be conﬁrmed
in §10.2.1.
The Bellman operator T is implemented in listing 5.1. In the listing, the utility
function is U (c) = c β , and the distribution φ is uniform on {0, . . . , B}. The operator
T is implemented as a function T(). It takes as input a sequence (list, etc.) v, which
corresponds to a function v on S, and returns a list Tv representing the image of v
under T. The outer loop steps through each x ∈ S, computing the maximum on the
right-hand side of (5.11) and assigning it to Tv[x]. The inner loop steps through each
feasible action a ∈ Γ( x ) to ﬁnd the maximum at x.5
Exercise 5.1.2 Complete the implementation of algorithm 5.1 in listing 5.1, computing
an (approximately) optimal policy. For the initial condition you might like to use v
deﬁned by v( x ) = U ( x ).6
Exercise 5.1.3 Using the code you wrote in exercise 4.3.26 (page 91), show that ( Xt )t≥0
is stable under the optimal policy (in particular, show numerically that α( pσ∗ ) > 0).
Compute the stationary distribution.
that (bS, d∞ ) is complete (see theorem 3.2.6 on page 45).
is reminiscent of the iterative technique for computing stationary distributions explored in exercise 4.3.25 on page 91.
5 As usual, the code is written for clarity, not speed. For large state spaces you will need to rewrite this
code using libraries for fast array processing such as SciPy and NumPy, or rewrite the inner loop in C or
Fortran, and then call it from Python.
6 You will have an opportunity to check that the policy you compute is correct in the next section.
3 Recall

4 Which

104

Chapter 5

Listing 5.1 (kurtzbellman.py) Bellman operator
beta , rho , B , M = 0.5 , 0.9 , 10 , 5
S = range ( B + M + 1) # State space = 0 ,... , B + M
Z = range ( B + 1)
# Shock space = 0 ,... , B
def U(c):
" Utility function . "
r e t u r n c ** beta
d e f phi ( z ) :
" Probability mass function , uniform distribution . "
r e t u r n 1.0 / len ( Z ) i f 0 <= z <= B e l s e 0
d e f Gamma ( x ) :
" The correspondence of feasible actions . "
r e t u r n range ( min (x , M ) + 1)
def T(v):
" " " An implementation of the Bellman operator .
Parameters : v is a sequence representing a function on S .
Returns : Tv , a list . " " "
Tv = []
f o r x i n S:
# Compute the value of the objective function for each
# a in Gamma ( x ) , and store the result in vals
vals = []
f o r a i n Gamma ( x ) :
y = U ( x - a ) + rho * sum ( v [ a + z ]* phi ( z ) f o r z i n Z )
vals . append ( y )
# Store the maximum reward for this x in the list Tv
Tv . append ( max ( vals ) )
r e t u r n Tv

Further Topics for Finite MCs

5.1.3

105

Policy Iteration

Another common technique for solving dynamic programming problems is policy iteration, as presented in algorithm 5.2.7 This technique is easy to program, and is often faster than value iteration. It has the nice feature that for ﬁnite state problems
the optimal policy is computed exactly (modulo numerical error) in ﬁnite time (theorem 10.2.6, page 244).
Algorithm 5.2 Policy iteration algorithm
pick any σ ∈ Σ
repeat
compute vσ from σ
compute a vσ -greedy policy σ
set e = σ − σ
set σ = σ
until e = 0

First, an arbitrary policy σ is chosen. Next, one computes the value vσ of this
policy. From vσ a vσ -greedy policy σ is computed:


σ ( x ) ∈ argmax

0≤ a ≤ x ∧ M

U ( x − a) + ρ

B

∑ vσ ( a + z)φ(z)

z =0

( x ∈ S)

and e records the deviation between σ and σ . If the policies are equal the loop terminates. Otherwise we set σ = σ and iteration continues.
The most difﬁcult part of coding this algorithm is to compute the value of a given
policy (i.e., compute vσ from σ). Listing 5.2 gives one method for accomplishing this,
based on evaluating the right-hand side of (5.6) with large T. It starts with two import
statements. First we import zeros, dot and array from numpy. The ﬁrst is for creating
arrays of zeros, the second is for matrix multiplication, and the third is for converting
other data types such as lists into NumPy arrays. Second, we import some primitives
from kurtzbellman, which is listing 5.1.
Next we deﬁne the function value_of_policy(), which takes an array sigma representing a feasible policy σ ∈ Σ as its argument, and returns a NumPy array v_sigma,
which represents vσ . At the start of the function we create a 2D array p_sigma corresponding to the stochastic kernel pσ ( x, y) := γ(σ( x ), y) deﬁned above. From that
array we create a function M_sigma() corresponding to Mσ . In addition we create
the array r_sigma(), which corresponds to rσ . Finally, we step through 50 terms of
ρt Mtσ rσ , adding each one to the return value.
7 Sometimes

called Howard’s policy improvement algorithm.

106

Chapter 5

Listing 5.2 (kurtzvsigma.py) Approximation of vσ
from numpy i m p o r t zeros , dot , array
from kurtzbellman i m p o r t S , rho , phi , U

# From listing 5.1

d e f value_of_policy ( sigma ) :
" Computes the value of following policy sigma . "
# Set up the stochastic kernel p_sigma as a 2 D array :
N = len ( S )
p_sigma = zeros (( N , N ) )
f o r x i n S:
f o r y i n S:
p_sigma [x , y ] = phi ( y - sigma [ x ])
# Create the right Markov operator M_sigma :
M_sigma = lambda h : dot ( p_sigma , h )
# Set up the function r_sigma as an array :
r_sigma = array ([ U ( x - sigma [ x ]) f o r x i n S ])
# Reshape r_sigma into a column vector :
r_sigma = r_sigma . reshape (( N , 1) )
# Initialize v_sigma to zero :
v_sigma = zeros (( N ,1) )
# Initialize the discount factor to 1:
discount = 1
f o r i i n range (50) :
v_sigma = v_sigma + discount * r_sigma
r_sigma = M_sigma ( r_sigma )
discount = discount * rho
r e t u r n v_sigma

Further Topics for Finite MCs

107

Exercise 5.1.4 Complete the implementation of algorithm 5.2. The resulting policy
should be the same as the one you computed in exercise 5.1.2.

5.2

MCs and SRSs

In this section we investigate the connection between Markov chains generated by
stochastic kernels on one hand, and stochastic recursive sequences (stochastic difference equations) on the other. We will see that to each Markov chain there corresponds
at least one stochastic recursive sequence (in fact there are many). This provides us
with new ways of analyzing and simulating Markov chains.

5.2.1

From MCs to SRSs

When we start studying processes on inﬁnite state spaces we will often be interested
in stochastic recursive sequences (SRSs). In the ﬁnite case, a typical SRS has the form
Xt+1 = F ( Xt , Wt+1 ),

X0 ∼ ψ ∈ P ( S ) ,

F: S × Z → S

(5.12)

where (Wt )t≥1 is a sequence of independent shocks taking values in arbitrary set Z.
As discussed in §5.1.1, the shocks Wt are to be thought of as functions on a common
space Ω. At the start of time, nature selects an ω ∈ Ω according to the probability
. This provides a complete realization of the path (Wt (ω ))t≥1 . The value ω also
determines X0 , with {ω : X0 (ω ) = xi } = ψ( xi ). Given (Wt (ω ))t≥1 and X0 (ω ), we
construct the corresponding time path ( Xt (ω ))t≥0 by
X1 (ω ) = F ( X0 (ω ), W1 (ω )),

X2 (ω ) = F ( X1 (ω ), W2 (ω )),

etc.

The idea that all uncertainty is realized at the start of time by a single observation ω
from Ω is a convenient mathematical ﬁction. It’s as if we could draw an entire path
(Wt (ω ))t≥1 by a single call to our random number generator. However, you can mentally equate the two approaches by thinking of ω as being realized at the beginning
of our simulation. Generating n random numbers can be thought of as observing the
ﬁrst n observations of the sequence. The default behavior of most random number
generators is to produce (quasi) independent sequences of shocks.
Given the SRS (5.12) we obtain a stochastic kernel p on S by
p( x, y) =

{ F ( x, Wt ) = y} :=

{ω ∈ Ω : F ( x, Wt (ω )) = y}

In fact, we can also go the other way, representing any Markov-( p, ψ) process by an
SRS such as (5.12). This is useful for simulation, and for gaining a deeper understanding of the probabilistic structure of dynamics.

108

Chapter 5

I ( x1 ; φ )

0

I ( x2 ; φ )

φ ( x1 )

I ( x3 ; φ )

φ ( x1 ) + φ ( x2 )

1

Figure 5.1 Partition ( I ( x; φ)) x∈S created by φ

To begin, let W be uniformly distributed on (0, 1]. Thus, for any a ≤ b ∈ (0, 1],
we have { a < W ≤ b} = b − a, which is the length of the interval ( a, b].8 Given a
distribution φ ∈ P (S), let us try to construct a function z → τ (z; φ) from (0, 1] to S
such that τ (W; φ) has distribution φ:

{τ (W; φ) = x } = φ( x )

( x ∈ S)

One technique is as follows: Divide the unit interval (0, 1] into N disjoint subintervals,
one for each x ∈ S. A typical subinterval is denoted I ( x; φ) and is chosen to have
length φ( x ). As a concrete example we could take
I ( xi ; φ) := (φ( x1 ) + · · · + φ( xi−1 ), φ( x1 ) + · · · + φ( xi )]
with I ( x1 ; φ) = (0, φ( x1 )]. Figure 5.1 gives the picture for S = { x1 , x2 , x3 }.
Now consider the function z → τ (z; φ) deﬁned by
τ (z; φ) :=

∑x

x ∈S

{z ∈ I ( x; φ)}

(z ∈ (0, 1])

(5.13)

where {z ∈ I ( x; φ)} is one when z ∈ I ( x; φ) and zero otherwise.
Exercise 5.2.1 Prove: ∀ x ∈ S, τ (z; φ) = x if and only if z ∈ I ( x; φ).
The random variable τ (W; φ) has distribution φ. To see this, pick any x ∈ S,
and observe that the τ (W; φ) = x precisely when W ∈ I ( x; φ). The probability of
this event is the length of the interval I ( x; φ), which, by construction, is φ( x ). Hence
{τ (W; φ) = x } = φ( x ) for all x ∈ S as claimed. An implementation of the function
z → τ (z; φ) is given in algorithm 5.3.9
8 The
9 This

probability is the same whether inequalities are weak or strict.
algorithm was used previously in listing 4.4 (page 73).

Further Topics for Finite MCs

109

Algorithm 5.3 The function z → τ (z; φ)
read in z
set a = 0
for x in S do
if a < z ≤ a + φ( x ) then return x
a = a + φ( x )
end

With these results we can represent any Markov-( p, ψ) chain as an SRS. By deﬁnition, such a chain obeys X0 ∼ ψ and Xt+1 ∼ p( Xt , dy) for t ≥ 0. To write this as a
stochastic recursive sequence, let (Wt )t≥0 be a IID and uniform on (0, 1], and let
X0 = τ (W0 ; ψ),

Xt+1 = τ (Wt+1 ; p( Xt , dy))

(5.14)

The second equality can be rewritten as
Xt+1 = F ( Xt , Wt+1 )

where

F ( x, z) := τ (z; p( x, dy))

(5.15)

You should convince yourself that if W is uniform on (0, 1], then F ( x, W ) has distribution p( x, dy), and that the sequence ( Xt )t≥0 generated by (5.14) and (5.15) obeys
X0 ∼ ψ and Xt+1 ∼ p( Xt , dy) for t ≥ 0.
Listing 5.3 provides some code for creating F from a given kernel p. The kernel p is represented as a sequence of sequences (see, e.g., the deﬁnition of the kernel on page 79). A call such as F = createF(p) assigns to F the function F ( x, z) =
τ (z; p( x, dy)).
Exercise 5.2.2 Use listing 5.3 to reimplement Hamilton’s Markov chain as a stochastic
recursive sequence. Verify that the law of large numbers still holds by showing that
1 n
∗
n ∑t=1 { Xt = y } → ψ ( y ) as n → ∞. (The right-hand side of this equality can be
calculated by listing 4.6. Now calculate the left using listing 5.3 and compare.)
Incidentally, SRSs are sometimes referred to as iterated function systems. In this
framework one thinks of updating the state from Xt to Xt+1 by the random function
FWt+1 := F (·, Wt+1 ). Although we are now dealing with “random functions,” which
sounds rather fancy, in practice the only change is a notational one: Xt+1 = FWt+1 ( Xt )
as compared to (5.12). The main advantage is that we can now write
Xt = FWt ◦ FWt−1 ◦ · · · ◦ FW1 ( X0 ) = FWt ◦ FWt−1 ◦ · · · ◦ FW1 (τ (W0 ; ψ))
We see that Xt is just a ﬁxed function of the shocks up to time t.

110

Chapter 5

Listing 5.3 (p2srs.py) Creating F for kernel p
d e f createF ( p ) :
" " " Takes a kernel p on S = {0 ,... , N -1} and returns a
function F (x , z ) which represents it as an SRS .
Parameters : p is a sequence of sequences , so that p [ x ][ y ]
represents p (x , y ) for x , y in S .
Returns : A function F with arguments (x , z ) . " " "
S = range ( len ( p [0]) )
d e f F (x , z ) :
a = 0
f o r y i n S:
i f a < z <= a + p [ x ][ y ]:
return y
a = a + p [ x ][ y ]
return F

5.2.2

Application: Equilibrium Selection

In this section we consider an application where a ﬁnite state Markov chain naturally arises as an SRS. The context is equilibrium selection in games. We look at how
so-called stochastically stable equilibria are identiﬁed in games with multiple Pareto
ranked Nash equilibria.
The application we consider is a coordination game with N players. The players
cooperate on a project that involves the use of computers. The agents choose as their
individual operating system (OS) either an OS called U or a second OS called W. For
this project, OS U is inherently superior. At the same time, cooperation is enhanced
by the use of common systems, so W may be preferable if enough people use it.
Speciﬁcally, we assume that the individual one-period rewards for using U and W
are given respectively by
x
N−x
u and Πw ( x ) :=
w
(0 < w < u )
N
N
where x is the number of players using U. Players update their choice of operating
system according to current rewards. As a result of their actions the law of motion for
the number of players using U is xt+1 = B( xt ), where the function B is deﬁned by
⎧
⎪
⎨ N if Πu ( x ) > Πw ( x )
B( x ) := x
if Πu ( x ) = Πw ( x )
( ⇐⇒ x = N (1 + u/w)−1 )
⎪
⎩
0
if Πu ( x ) < Πw ( x )
Πu ( x ) :=

Further Topics for Finite MCs

111

●

12

●

●

●

●

●

●

●

10

8

6

●

4

2

0

●

0

●

●

2

●

4

6

8

10

12

Figure 5.2 Best response dynamics

The 45 degree diagram for B is shown in ﬁgure 5.2 when N = 12, u = 2 and w = 1.
There are three ﬁxed points: x = 0, x = xb := N (1 + u/w)−1 = 4 and x = N.
The point xb is the value of x such that rewards are exactly equal. That is, Πu ( xb ) =
Π w ( x b ).
Under these deterministic dynamics, the long-run outcome for the game is determined by the initial condition x0 , which corresponds to the number of players originally using U. Notice that a larger fraction of initial conditions lead to coordination
on U, which follows from our assumption that U is inherently superior (i.e., u > w).
So far the dynamics are characterized by multiple equilibria and path dependence
(where long-run outcomes are determined by initial conditions). Some authors have
sought stronger predictions for these kinds of coordination models (in the form of
unique and stable equilibria) by adding learning, or “mutation.”
Suppose, for example, that after determining the choice of OS via the best response
function B, players switch to the alternative OS with independent probability  > 0.
Thus each of the B( x ) users of U switches to W with probability , and each of the
N − B( x ) users of W switches to U with the same probability. Using Xt to denote the
(random) number of U users at time t, the dynamics are now
Xt+1 = B( Xt ) + Vtu − Vtw

(5.16)

where Vtu and Vtw are independent and binomially distributed with probability  and

112

Chapter 5

sizes N − B( Xt ) and B( Xt ) respectively.10 Here Vtu is the number of switches from W
to U, while Vtw is switches from U to W.
With the addition of random “mutation,” uniqueness and stability of the steady
state is attained:
Exercise 5.2.3 Let p( x, y) := { Xt+1 = y | Xt = x } be the stochastic kernel corresponding to the SRS (5.16), let M be the Markov operator, and let S := {0, . . . , N }.
Argue that for any ﬁxed  ∈ (0, 1), the system (P (S), M) is globally stable.
Let ψ∗ be the unique stationary distribution for  ∈ (0, 1). It has been shown (see
Kandori, Mailath, and Rob 1993) that as  → 0, the distribution ψ∗ concentrates on the
Pareto dominant equilibrium N (i.e., ψ∗ ( N ) → 1 as  → 0). The interpretation is that
for low levels of experimentation or mutation, players rarely diverge from the most
attractive equilibrium.
This concentration on N can be observed by simulation: Let ( Xt )nt=0 be a time
series generated for some ﬁxed  ∈ (0, 1). Then the law of large numbers (theorem 4.3.33, page 94) implies that for large n,
n −1

n

∑ {Xt = N }  ψ∗ ( N )

t =1

Figure 5.3 shows a simulation that gives n−1 ∑nt=1 { Xt = N } as  ranges over the
interval [0.001, 0.1]. The parameters are N = 12, u = 2 and w = 1. The series length n
used in the simulation is n = 10, 000. The ﬁgure shows that steady state probabilities
concentrate on N as  → 0.
Exercise 5.2.4 Replicate ﬁgure 5.3. Generate one time series of length n = 10, 000 for
each , and plot the fraction of time each series spends in state N.

5.2.3

The Coupling Method

Much of the modern theory of Markov chains is based on probabilistic methods (as
opposed to analytical techniques such as ﬁxed point theory). A prime example is
coupling. Found in many guises, coupling is a powerful and elegant technique for
studying all manner of probabilistic phenomena. It has been used to prove stability of
Markov processes since the masterful work of Wolfgang Doeblin (1938).
We will use coupling to prove global stability of Markov chains for which the Dobrushin coefﬁcient is strictly positive, without recourse to the contraction mapping
argument employed in theorem 4.3.18 (page 90). Our main aim is to provide the basic
10 A

binomial random variable with probability p and size n counts the number of successes in n binary
trials, each with independent success probability p.

113

1.0

Further Topics for Finite MCs

n


t=1 {Xt

= N}

0.0

0.2

0.4

0.6

0.8

1
n

0.001

0.011

0.021

0.031

0.041

0.051

0.061

0.071

0.081

0.091



Figure 5.3 Fraction of time spent at N as  → 0

feel of the coupling method. When we turn to stability of Markov chains on inﬁnite
state spaces, the intuition you have developed here will be valuable. Note, however,
that the topic is technical, and those who feel they have learned enough about stability
for now can move on without much loss of continuity.
To begin, consider a stochastic kernel p with α( pt ) > 0. To simplify the argument,
we are going to assume that t = 1. (The general case is a bit more complicated but
works along the same lines.) A little thought will convince you that α( p) > 0 is
equivalent to strict positivity of


 := min

∑ p(x, y) · p(x , y)

y∈S

: ( x, x ) ∈ S × S

(5.17)

The condition  > 0 can be understood as follows: If we run two independent chains
( Xt )t≥0 and ( Xt )t≥0 , both updated with kernel p, then the kernel for the joint process
(( Xt , Xt ))t≥0 on S × S is p( x, y) p( x , y ). If Xt = x and Xt = x , then the probability
both chains hit the same state next period (i.e., the probability that Xt+1 = Xt+1 ) is
∑y∈S p( x, y) p( x , y). Hence  > 0 means that, regardless of the current state, there is a
positive (≥ ) probability the chains will meet next period. This in turn is associated
with stability, as it suggests that initial conditions are relatively unimportant.
To make this argument more concrete, ﬁx ψ ∈ P (S) and consider two inde-

114

Chapter 5

pendent Markov chains ( Xt )t≥0 and ( Xt∗ )t≥0 , where ( Xt )t≥0 is Markov-( p, ψ) and
( Xt∗ )t≥0 is Markov-( p, ψ∗ ) for some stationary distribution ψ∗ ∈ P (S).11 It follows
that Xt ∼ ψMt and Xt∗ ∼ ψ∗ . Now consider a third process ( Xt )t≥0 , which follows
( Xt )t≥0 until ν := min{t ≥ 0 : Xt = Xt∗ }, and then switches to following ( Xt∗ )t≥0 . In
other words, Xt = Xt for t ≤ ν and Xt = Xt∗ for t ≥ ν. (The random variable ν is
known as the coupling time.) A recipe for generating these three processes is given in
algorithm 5.4.
Algorithm 5.4 Coupling two Markov chains
generate independent draws X0 ∼ ψ and X0∗ ∼ ψ∗
set X0 = X0
for t ≥ 0 do
draw Xt+1 ∼ p( Xt , dy) and Xt∗+1 ∼ p( Xt∗ , dy) independently
if Xt = Xt∗ then
set Xt+1 = Xt∗+1
else
set Xt+1 = Xt+1
end
end

We claim that the distributions of Xt and Xt are equal for all t, from which it follows
that Xt ∼ ψMt . To verify the latter it is sufﬁcient to show that ( Xt )t≥0 is Markov( p, ψ). And indeed ( Xt )t≥0 is Markov-( p, ψ) because at time zero we have X0 = X0 ∼
ψ, and subsequently Xt+1 ∼ p( Xt , dy).
That Xt+1 is drawn from p( Xt , dy) at each t ≥ 0 can be checked by carefully working through algorithm 5.4. Another way to verify that Xt+1 ∼ p( Xt , dy) is to cast both
( Xt )t≥0 and ( Xt∗ )t≥0 as stochastic recursive sequences of the form
Xt+1 = F ( Xt , Wt+1 ),

X0 ∼ ψ,

Xt∗+1 = F ( Xt∗ , Wt∗+1 ),

X0∗ ∼ ψ∗

where the random variables (Wt )t≥0 and (Wt∗ )t≥0 are all independent and uniform
on (0, 1], and F is determined in (5.15). Now we create ( Xt )t≥0 by setting X0 = X0 ,
Xt+1 = F ( Xt , Wt+1 ) for t < ν and Xt+1 = F ( Xt , Wt∗+1 ) for t ≥ ν. By switching the
source of shocks at the coupling time, Xt changes course and starts to follow ( Xt∗ )t≥0 .
Nevertheless, ( Xt )t≥0 is always updated by F (·, W ) for some uniformly distributed
independent W, which means that Xt+1 ∼ p( Xt , dy) at every step, and hence Xt ∼
ψMt as claimed.
The next step of the proof uses the following coupling inequality.
11 At

least one must exist by theorem 4.3.5, page 86.

Further Topics for Finite MCs

115

Lemma 5.2.5 If X and Y are any random variables taking values in S and having distributions φX and φY respectively, then

φX − φY ∞ := max |φX ( x ) − φY ( x )| ≤
x ∈S

{ X = Y }

Intuitively, if the probability that X and Y differ is small, then so is the distance
between their distributions. An almost identical proof is given later in the book so we
omit the proof here.12
Let’s apply lemma 5.2.5 to Xt and Xt∗ . Since Xt ∼ ψMt and Xt∗ ∼ ψ∗ ,

ψMt − ψ∗ ∞ ≤

{ Xt = Xt∗ }

We wish to show that the right-hand side of this inequality goes to zero, and this is
where the reason for introducing Xt becomes clear. Not only does it have the distribution ψMt , just as Xt does, but also we know that if Xt is distinct from Xt∗ , then X j and
X j∗ are distinct for all j ≤ t. Hence { Xt = Xt∗ } ≤ ∩ j≤t { X j = X j∗ }.13 Therefore

ψMt − ψ∗ ∞ ≤

∩ j≤t { X j = X j∗ }

(5.18)

Thus, to show that ψMt converges to ψ∗ , it is sufﬁcient to demonstrate that the probability of X j and X j∗ never meeting prior to t goes to zero as t → ∞. And this is where
positivity of  in (5.17) comes in. It means that there is an  chance of meeting at each
time j, independent of the locations of X j−1 and X j∗−1 . Hence the probability of never
meeting converges to zero. Speciﬁcally,
Proposition 5.2.6 We have

∩ j≤t { X j = X j∗ } ≤ (1 − )t for all t ∈

.

It follows from proposition 5.2.6 and (5.18) that if  is strictly positive, then ψMt −
∞ → 0 at a geometric rate.

ψ∗ 

Proof of proposition 5.2.6. The process ( Xt , Xt∗ )t≥0 is a Markov chain on S × S. A typical
element of S × S will be denoted by ( x, s). In view of independence of ( Xt )t≥0 and
( Xt∗ )t≥0 , the initial condition of ( Xt , Xt∗ )t≥0 is ψ × ψ∗ (i.e., {( X0 , X0∗ ) = ( x, s)} =
(ψ × ψ∗ )( x, s) := ψ( x )ψ∗ (s)), while the stochastic kernel is
k(( x, s), ( x , s )) = p( x, x ) p(s, s )
To simplify notation let’s write ( x, s) as x so that k(( x, s), ( x , s )) can be expressed
more simply as k (x, x ), and set D := {( x, s) ∈ S × S : x = s}. Evidently

∩ j≤t { X j = X j∗ } =
12 See

∩ j≤t {( X j , X j∗ ) ∈ D c }

lemma 11.3.2 on page 273.
A and B are two events with A ⊂ B (i.e., occurrence of A implies occurrence of B), then
See chapter 9 for details.
13 If

( A) ≤

( B ).

116

Chapter 5

In view of lemma 4.2.17 this probability is equal to

∑

(ψ × ψ∗ )(x0 )

x0 ∈ D c

∑

k ( x0 , x1 ) · · ·

∑

k ( x t −2 , x t −1 )

x t −1 ∈ D c

x1 ∈ D c

∑

k ( x t −1 , x t )

(5.19)

xt ∈ D c

Now consider the last term in this expression. We have

∑

k ( x t −1 , x t ) = 1 −

xt ∈ D c

∑

k ( x t −1 , x t )

xt ∈ D

But from the deﬁnitions of k and D we obtain

∑

xt ∈ D

k ( x t −1 , x t ) =

p ( x t −1 , x t ) p ( s t −1 , s t ) =

∑

( x t ,st )∈ D

∴

∑

∑ p ( x t −1 , y ) p ( s t −1 , y )

y∈S

k ( x t −1 , x t ) ≥ 

xt ∈ D

∴

∑

k ( x t −1 , x t ) ≤ 1 − 

xt ∈ D c

Working back through (5.19) and applying the same logic to each term shows that
(5.19) is less than (1 − )t . This proves the proposition.

5.3

Commentary

Much of the early theory of dynamic programming is due to Bellman (1957). A good
introduction to dynamic programming in discrete state environments can be found
in Puterman (1994). An overview with applications is given in Miranda and Fackler
(2002, ch. 7). Further references can be found in the commentary to chapters 6 and 10.
The representation of Markov chains as stochastic recursive sequences in §5.2.1 is
loosely based on Häggström (2002, ch. 3). The application in §5.2.2 is from Kandori,
Mailath, and Rob (1993). The approach to coupling in §5.2.3 is somewhat nonstandard. More information can be found in the commentary to chapter 11.

Chapter 6

Inﬁnite State Space
In this chapter we begin working with stochastic systems on inﬁnite state space. While
a completely rigorous treatment of this area requires measure theory (chapter 7 and
onward), we can build a good understanding of the key topics (dynamics, optimization, etc.) by heuristic arguments, simulation, and analogies with the ﬁnite case.
Along the way we will meet some more challenging programming problems.

6.1

First Steps

In this section we study dynamics for stochastic recursive sequences (SRSs) taking values in . Our main interest is in tracking the evolution of probabilities over time, as
represented by the marginal distributions of the process. We will also look at stationary distributions—the inﬁnite state analogue of the stationary distributions discussed
in §4.3.1—and how to calculate them.

6.1.1

Basic Models and Simulation

Our basic model is as follows: Let the state space S be a subset of
Let F : S × Z → S be a given function, and consider the SRS
Xt+1 = F ( Xt , Wt+1 ),

X0 ∼ ψ,

IID

IID

(Wt )t≥1 ∼ φ

and let Z ⊂

.

(6.1)

Here (Wt )t≥1 ∼ φ means that (Wt )t≥1 is an IID sequence of shocks with cumulative
distribution function φ. In other words, {Wt ≤ z} = φ(z) for all z ∈ Z. Likewise ψ
is the cumulative distribution function of X0 , and X0 is independent of (Wt )t≥1 . Note
that Xt and Wt+1 are independent, since Xt depends only on the initial condition and
the shocks W1 , . . . , Wt , all of which are independent of Wt+1 .
117

118

Chapter 6

Example 6.1.1 Consider a stochastic version of the Solow–Swan growth model, where
output is a function f of capital k and a real-valued shock W. The sequence of producIID
tivity shocks (Wt )t≥1 is ∼ φ. Capital at time t + 1 is equal to that fraction s of output
that was saved last period, plus undepreciated capital, giving law of motion
k t+1 = F (k t , Wt+1 ) := s f (k t , Wt+1 ) + (1 − δ)k t

(6.2)

Consumption is given by ct = (1 − s) f (k t , Wt+1 ). The production function satisﬁes
f : 2+ → + and f (k, z) > 0 whenever k > 0 and z > 0. For the state space we can
choose either S0 = + or S = (0, ∞), while Z := (0, ∞).
Exercise 6.1.2 Show that if k ∈ S0 (resp., S) and z ∈ Z, then next period’s stock F (k, z)
is in S0 (resp., S).
Example 6.1.3 Let Z = S =
gression (STAR) model

, and consider the smooth transition threshold autore-

Xt+1 = g( Xt ) + Wt+1 ,

IID

(Wt )t≥1 ∼ φ

(6.3)

g( x ) := (α0 + α1 x )(1 − G ( x )) + ( β 0 + β 1 x ) G ( x )
Here G : S → [0, 1] is a smooth transition function, such as the logistic function, satisfying G > 0, limx→−∞ G ( x ) = 0 and limx→∞ G ( x ) = 1.
Code for simulating time series from an arbitrary SRS is given in listing 6.1. The
listing deﬁnes a class SRS implementing the canonical SRS in (6.1).1 The behavior of
the class is similar to that of the class MC in listing 4.4 (page 73), and the methods are
explained in the doc strings.
Listing 6.2 provides an example of usage. The code creates an instance of SRS,
corresponding to the Solow model (6.2) when f (k, W ) = kα W and ln Wt ∼ N (0, σ2 ).
The two sample paths generated by this code are shown in ﬁgure 6.1.
Exercise 6.1.4 Repeat exercise 5.2.2 on page 109 using the class SRS.
Returning to the SRS (6.1), let’s consider the distribution of Xt for arbitrary t ∈ .
This distribution will be denoted by ψt , and you can think of it for now as a cumulative
distribution function (i.e., ψt ( x ) is the probability that Xt ≤ x). It is also called the
marginal distribution of Xt ; conceptually it is equivalent to its discrete state namesake
that we met in §4.2.2.
In order to investigate ψt via simulation, we need to sample from this distribution.
The simplest technique is this: First draw X0 ∼ ψ and generate a sample path stopping at time t. Now repeat the exercise, but with a new set of draws X0 , W1 , . . . , Wt ,
1 The

beneﬁt of designing an abstract class for SRSs is code reuse: The class can be used to study any
system. Readers are encouraged to add functionality to the class as they do the exercises in the chapter.

Inﬁnite State Space

119

Listing 6.1 (srs.py) Simulation of SRSs
c l a s s SRS :
d e f __init__ ( self , F =None , phi =None , X =None) :
" " " Represents X_ { t +1} = F ( X_t , W_ { t +1}) ; W ~ phi .
Parameters : F and phi are functions , where phi ()
returns a draw from phi . X is a number representing
the initial condition . " " "
self .F , self . phi , self . X = F , phi , X
d e f update ( self ) :
" Update the state according to X = F (X , W ) . "
self . X = self . F ( self .X , self . phi () )
d e f sample_path ( self , n ) :
" Generate path of length n from current state . "
path = []
f o r i i n range ( n ) :
path . append ( self . X )
self . update ()
r e t u r n path

Listing 6.2 (testsrs.py) Example application
from srs i m p o r t SRS
# Import from listing 6.1
from random i m p o r t lognormvariate
alpha , sigma , s , delta = 0.5 , 0.2 , 0.5 , 0.1
# Define F (k , z ) = s k ^ alpha z + (1 - delta ) k
F = lambda k , z : s * ( k ** alpha ) * z + (1 - delta ) * k
lognorm = lambda : lognormvariate (0 , sigma )
solow_srs = SRS ( F =F , phi = lognorm ,
P1 = solow_srs . sample_path (500)
solow_srs . X = 60
P2 = solow_srs . sample_path (500)

X =1.0)
# Generate path from X = 1
# Reset the current state
# Generate path from X = 60

120

Chapter 6

Figure 6.1 Time series plot

leading to a new draw of Xt that is independent of the ﬁrst. If we do this n times, we
get n independent samples Xt1 , . . . , Xtn from the target distribution ψt . Algorithm 6.1
contains the pseudocode for this operation. Figure 6.2 is a visualization of the algorithm after 3 iterations of the outer loop.
Exercise 6.1.5 Investigate the mean of ψt for the Solow–Swan model when f (k, W ) =
kα W and ln Wt ∼ N (0, σ2 ). Carry out a simulation with k0 = 1, t = 20, δ = 0.1,
s = 1/2, σ2 = 0.2 and α = 0.3. Draw n = 1000 samples. Compute k t using the
statistic n−1 ∑in=1 kit . Explain how theorem 4.3.31 on page 94 justiﬁes your statistic.
Exercise 6.1.6 Repeat exercise 6.1.5, but now setting s = 3/4. How does your estimate
change? Interpret.
Exercise 6.1.7 Repeat exercise 6.1.5, but now set k0 = 5, k0 = 10, and k0 = 20. To the
extent that you can, interpret your results.
Exercise 6.1.8 Repeat exercise 6.1.5, but now set t = 50, t = 100, and t = 200. What
happens to your estimates? Interpret.
Exercise 6.1.9 Repeat exercise 6.1.7, but with t = 200 instead of t = 20. Try to interpret
your results.

Inﬁnite State Space

121

Algorithm 6.1 Draws from the marginal distribution
for i in 1 to n do
draw X from the initial condition ψ
for j in 1 to t do
draw W from the shock distribution φ
set X = F ( X, W )
end
set Xti = X
end
return ( Xt1 , . . . , Xtn )

Xt1

Xt2

Xt3

0

time

t

Figure 6.2 Sampling from the marginal distribution

122

Chapter 6

Recall that if X1 , . . . , Xn is a sample of IID random variables, then the sample mean
is deﬁned as X̄n := n1 ∑in=1 Xi , while the sample variance is
σ̂n2 :=

n
1
( X − X̄n )2
∑
n − 1 i =1 i

Assuming that the second moment of Xi is ﬁnite, the central limit theorem and a
convergence result often referred to as Slutsky’s theorem give
√

n( X̄n − X1 ) d
→ N (0, 1) as n → ∞, where σ̂n := σ̂n2
σ̂n
Exercise 6.1.10 Based on this fact, construct a 95% conﬁdence interval for your estimate of k t . (Use the parameters from exercise 6.1.5.)
Exercise 6.1.11 Consider the same model as in exercise 6.1.5, but now set δ = 1. The
classical golden rule optimization problem is to choose the savings rate s in the Solow–
Swan model to maximize steady state consumption. Let’s consider the stochastic analogue. The simplest criterion is to maximize expected steady state consumption. For
this model, by t = 100 the distribution of c j varies little for j ≥ t (we’ll learn more
about this later on). As such, let’s consider c100 as expected steady state consumption. Compute n = 5, 000 observations of c100 , and take the sample average to obtain
an approximation of the expectation. Repeat for s in a grid of values in (0, 1). Plot the
function, and report the maximizer.

6.1.2

Distribution Dynamics

While the mean conveys some information about the random variable k t , at times we
wish to know about the entire (cumulative) distribution ψt . How might one go about
computing ψt by simulation?
The standard method is with the empirical distribution function, which, for independent samples ( Xi )in=1 of random variable X ∈ , is given by
Fn ( x ) :=

1
n

n

∑

i =1

{ Xi ≤ x }

(x ∈

)

(6.4)

Thus Fn ( x ) is the fraction of the sample that falls below x. The LLN (theorem 4.3.31,
page 94) can be used to show that if X has cumulative distribution F, then Fn ( x ) →
F ( x ) with probability one for each x as n → ∞.2 These results formalize the fundamental idea that empirical frequencies converge to probabilities when the draws are
independent.
2 Later

we will cover how to do these kinds of proofs. In this case much more can be proved—interested
readers should refer to the Glivenko–Cantelli theorem.

Inﬁnite State Space

123

Listing 6.3 (ecdf.py) Empirical distribution function
c l a s s ECDF :
d e f __init__ ( self , observations ) :
self . observations = observations
d e f __call__ ( self , x ) :
counter = 0.0
f o r obs i n self . observations :
i f obs <= x :
counter += 1
r e t u r n counter / len ( self . observations )

An implementation of the empirical distribution function is given in listing 6.3,
based on a class called ECDF.3 The class is initialized with samples ( Xt )nt=1 stored in
a sequence (list or tuple) called observations, and an instance is created with a call
such as F = ECDF(data). The method __call__ evaluates Fn ( x ) for a given value of x.
(Recall from §2.2.3 that __call__ is a special method that makes an instance F callable,
so we can evaluate Fn ( x ) using the simple syntax F(x).) Here is an example:
from ecdf i m p o r t ECDF
# Import from listing 6.3
from random i m p o r t uniform
samples = [ uniform (0 , 1) f o r i i n range (10)]
F = ECDF ( samples )
F (0.5) # Returned 0.29
F . observations = [ uniform (0 , 1) f o r i i n range (1000)]
F (0.5) # Returned 0.479

Figure 6.3 gives four plots of the empirical distribution function corresponding to the
time t distribution of the Solow–Swan model. The plots are for n = 4, n = 25, n = 100,
and n = 5, 000. The parameters are k0 = 1, t = 20, δ = 0.1, s = 1/2, σ2 = 0.2, and
α = 0.3.
Exercise 6.1.12 Add a method to the ECDF class that uses Matplotlib to plot the empirical distribution over a speciﬁed interval. Replicate the four graphs in ﬁgure 6.3
(modulo randomness).
Consider now a variation of our growth model with additional nonlinearities. In
3 As

usual, the code is written for clarity rather than speed. See the text home page for more optimized
solutions.

124

Chapter 6
n=4

n = 25

1.0

1.0

0.0

0.0
10

15

20

25

10

15

k

k

n = 100

n = 5000

1.0

20

25

20

25

1.0

0.0

0.0
10

15

20

k

25

10

15

k

Figure 6.3 Empirical distribution functions

example 4.1.8 (page 57) we looked at a model with “threshold” nonconvexities. A
stochastic version is
(6.5)
k t+1 = sA(k t )kαt Wt+1 + (1 − δ)k t
where the shock is assumed to be lognormally distributed (and independent), and A
is the step function

A1 if 0 < k < k b
A ( k ) = A1 {0 < k < k b } + A2 { k b ≤ k < ∞ } =
A2 if k b ≤ k < ∞
with k b ∈ S = (0, ∞) interpreted as the threshold, and 0 < A1 < A2 .
Figures 6.4 and 6.5 each show two time series generated for this model, with initial
conditions k0 = 1 and k0 = 80. The parameters for ﬁgure 6.4 are set at α = 0.5,
s = 0.25, A1 = 15, A2 = 25, σ2 = 0.02, and k b = 21.6, while for ﬁgure 6.5, k b =
24.1. Notice how initial conditions tend to persist, although time series occasionally
cross the threshold k b —what might be referred to in physics as a “phase transition.”
Informally, the state variable moves from one locally attracting region of the state
space to another.
Exercise 6.1.13 Compute the empirical distribution functions at t = 100 for the two

Inﬁnite State Space

125

Figure 6.4 Persistence in time series
sets of parameters used in ﬁgures 6.4 and 6.5. Plot the functions and interpret their
shapes.
Aside from computing the distributions, another interesting question is: How long
do we expect it to take on average for the transition (crossing of the threshold k b ) to
occur for an economy with initial condition k0 = 1? More mathematically, what is the
expectation of τ := inf{t ≥ 0 : k t > k b } when regarded as a random variable on ?
(Here τ is usually called the ﬁrst passage time of (k t )t≥0 to (k b , ∞).)
Exercise 6.1.14 Using α = 0.5, s = 0.25, A1 = 15, A2 = 25, σ2 = 0.02, k0 = 1 and
k b = 21.6, compute an approximate expectation of τ by sample mean. (Set n = 5, 000.)
Do the same for k b = 24.1, which corresponds to ﬁgure 6.5. How does your answer
change? Interpret.

6.1.3

Density Dynamics

Now let’s look more deeply at distribution dynamics for SRSs, with an emphasis on
density dynamics. In reading this section, you should be aware that all densities create
distributions but not all distributions are created by densities. If f is a density function
x
on , then F ( x ) := −∞ f (u)du is a cumulative distribution function. However, if F is
a cumulative distribution function with jumps—corresponding to positive probability

126

Chapter 6

Figure 6.5 Persistence in time series
mass on individual points—then there exists no density f with F ( x ) :=
for all x ∈ . (More about this later on.)
For the sake of concreteness, let’s focus on a model of the form
Xt+1 = g( Xt ) + Wt+1 ,

X0 ∼ ψ,

IID

(Wt )t≥1 ∼ φ

x

−∞

f (u)du

(6.6)

where Z = S = , and both ψ and φ are density functions on . For this model, the
distribution of Xt can be represented by a density ψt for any t ≥ 1, and ψt and ψt+1
are linked by the recursion
ψt+1 (y) =



p( x, y)ψt ( x )dx,

where p( x, y) := φ(y − g( x ))

(6.7)

Here p is called the stochastic density kernel corresponding to (6.3). It represents the
distribution of Xt+1 = g( Xt ) + Wt+1 given Xt = x (see below). The left-hand side of
(6.7) is a continuous state version of (4.10) on page 75. It links the marginal densities
of the process from one period to the next. In fact it deﬁnes the whole sequence of
densities (ψt )t≥1 for the process once an initial condition is given.
Let’s try to understand why (6.7) holds, leaving fully rigorous arguments until
later. First we need the following lemma.
Lemma 6.1.15 If W ∼ φ, then Y := g( x ) + W has density φ(y − g( x ))dy.4
4 Here

the symbol dy indicates that φ(y − g( x )) is a density in y rather than in x.

Inﬁnite State Space

127

Proof. Let F be the cumulative distribution function (cdf) of Y, and let Φ be the cdf
corresponding to φ (i.e., Φ = φ). We have
F (y) =

{ g( x ) + W ≤ y} =

{W ≤ y − g( x )} = Φ(y − g( x ))

The density of Y is F (y) = φ(y − g( x )) as claimed.
Returning to (6.7), recall that if X and Y are random variables with joint density
p X,Y ( x, y), then their marginal densities satisfy
pX (x) =



p X,Y ( x, y)dy,

pY ( y ) =



p X,Y ( x, y)dx

Moreover the conditional density pY |X ( x, y) of Y given X = x is given by
pY |X ( x, y) =

p X,Y ( x, y)
pX (x)

( x, y ∈ S)

Some simple manipulations now yield the expression
pY ( y ) =



pY |X ( x, y) p X ( x )dx

(y ∈ S)

We have almost established (6.7). Letting Xt+1 = Y and Xt = X, we have
ψt+1 (y) =



p Xt+1 |Xt ( x, y)ψt ( x )dx

(y ∈ S)

The function p Xt+1 |Xt ( x, y) is the density of g( Xt ) + Wt+1 given Xt = x, or, more simply, the density of g( x ) + Wt+1 . By lemma 6.1.15, this is φ(y − g( x )) =: p( x, y), conﬁrming (6.7).
Now let’s look at the dynamics implied by the law of motion (6.7). The initial
condition is ψ0 = ψ, which is the density of X0 (regarded as given). From this initial
condition, (6.7) deﬁnes the entire sequence (ψt )t≥0 . There are a couple of ways that we
can go about computing elements of this sequence. Oneis numerical integration. For
example, ψ1 could be calculated by evaluating ψ1 (y) = p( x, y)ψ( x )dx at each y ∈ S.
Come to think of it, though, this is impossible: there is an inﬁnity of such y. Instead,
we would have to evaluate on a ﬁnite grid, use our results to form an approximation
ψ̂1 of ψ1 , then do the same to obtain ψ̂2 , and so on.
Actually this process is not very efﬁcient, and it is difﬁcult to obtain a measure of
accuracy. So let’s consider some other approaches. Say that we wish to compute ψt ,
where t is a ﬁxed point in time. Since we know how to compute empirical distribution
functions by simulation, we could generate n observations of Xt (see algorithm 6.1),
compute the empirical distribution function Ftn , and differentiate Ftn to obtain an approximation ψtn to ψt .

128

Chapter 6

It turns out that this is not a good plan either. The reason is that Ftn is not differentiable everywhere on S. And although it is differentiable at many points in S, at those
points the derivative is zero. So the derivative of Ftn contains no information about
ψt .5
Another plan would be to generate observations of Xt and histogram them. This
is a reasonable and common way to proceed—but not without its ﬂaws. The main
problem is that the histogram converges rather slowly to its target ψt . This is because
histograms have no notion of neighborhood, and do not make use of all knowledge
we have at hand. For example, if the number of bins is large, then it is often the case
that no points from the sample will fall in certain bins. This may happen even for bins
close to the mean of the distribution. The result is a “spiky” histogram, even when the
true density is smooth.6
We can include the prior information that the density of ψt is relatively smooth
using Parzen windows, or nonparametric kernel density estimates. The kernel density
estimate f n of unknown density f from observations (Yi )in=1 is deﬁned as


n
x − Yi
1
f n ( x ) :=
K
(x ∈ )
(6.8)
n · δn i∑
δn
=1
where K is some density on , and δn is either a parameter or a function of the data,
usually referred to as the bandwidth.
Essentially, f n is a collection of n “bumps,” one centered on each data point Yi .
These are then summed and normalized to create a density.
Exercise 6.1.16 Using a suitable change of variables in the integral, show that f n is a
density for every n.
The bandwidth parameter plays a role similar to the number of bins used in the
histogram: A high value means that the densities we place on each data point are ﬂat
with large tails. A low value means they are concentrated around each data point,
and f n is spiky.
Exercise 6.1.17 Implement the nonparametric kernel density estimator (6.8) using a
standard normal density for K. Base your code on the empirical distribution function
class in listing 6.3. Generate a sample of 100 observations from the standard normal
distribution and plot the density esimate for bandwidth values 0.01, 0.1, and 0.5.
Although the nonparametric kernel density estimator produces good results in a
broad range of environments, it turns out that for the problem at hand there is a better
5 Readers familiar with the theory of ill-posed problems will have a feel for what is going on here. The
density computation problem is ill-posed!
6 We get ψ by integrating ψ
t
t−1 with respect to a kernel φ ( y − g ( x )), and functions produced in this way
are usually smooth rather than spiky.

Inﬁnite State Space

129

way: The look-ahead estimator ψtn of ψt is deﬁned by generating n independent draws
( Xt1−1 , . . . , Xtn−1 ) of Xt−1 and then setting
ψtn (y) :=

1
n

n

∑ p(Xti−1 , y)

i =1

(y ∈

)

(6.9)

where p( x, y) = φ(y − g( x )).7 This estimator has excellent asymptotic and ﬁnite sample properties. While we won’t go into them too deeply, note that
Lemma 6.1.18 The look-ahead estimator ψtn is pointwise unbiased for ψt , in the sense that
ψtn (y) = ψt (y) for every y ∈ S. Moreover ψtn (y) → ψt (y) as n → ∞ with probability one.
Proof. Fix y ∈ S, and consider the random variable Y := p( Xt−1 , y). The look-ahead
estimator n1 ∑in=1 p( Xti −1 , y) is the sample mean of IID copies of Y, while the mean is
Y=

p( Xti −1 , y) =



p( x, y)ψt−1 ( x )dx = ψt (y)

(The last equality is due to (6.7)). The desired result now follows from the fact that the
sample mean of an IID sequence of random variables is an unbiased and consistent
estimator of the mean.8
Figure 6.6 shows a sequence of densities from the STAR model (6.3), computed
using the look-ahead estimator with 1,000 observations per density. The transition
function G is the cumulative distribution function for the standard normal distribution, α0 = 1, α1 = 0.4, β 0 = 10, and β 1 = 0.8. The density φ is standard normal.
Exercise 6.1.19 Implement the look-ahead estimator for the STAR model using the
same parameters used for ﬁgure 6.6. Replicate the ﬁgure (modulo the inﬂuence of
randomness).

6.1.4

Stationary Densities: First Pass

The sequence of densities (ψt )t≥0 in ﬁgure 6.6 appears to be converging.9 Indeed it
can be shown (see chapter 8) that there is a limiting distribution ψ∗ to which (ψt )t≥0
is converging, and that the limit ψ∗ is independent of the initial condition ψ0 . The
density ψ∗ is called a stationary density, and it satisﬁes
ψ∗ (y) =



p( x, y)ψ∗ ( x )dx

(y ∈

)

(6.10)

independent draws ( Xt1−1 , . . . , Xtn−1 ) can be obtained via algorithm 6.1 on page 121.
you don’t know the proof of this fact then try to do it yourself. Consistency follows from the law of
large numbers (theorem 4.3.31 on page 94).
9 See ﬁgure 6.12 on page 142 for another sequence of densities converging to a limit.
7 The
8 If

130

Chapter 6

ψ5

ψ20

ψ15

ψ10

0

20

40

60

Figure 6.6 Density sequence

More generally, a density ψ∗ on is called stationary for the SRS (6.6) if (6.10) holds,
where the density kernel p satisﬁes p( x, y) = φ(y − g( x )). The SRS is called globally
stable if there exists one and only one such density on , and the sequence of marginal
distributions (ψt )t≥0 converges to it as t → ∞. (A more formal deﬁnition is given in
chapter 8.)
You will recall that in the ﬁnite case a distribution ψ∗ is called stationary if ψ∗ =
∗
ψ M, or equivalently, ψ∗ (y) = ∑ x∈S p( x, y)ψ∗ ( x ) for all y ∈ S. The expression (6.10)
simply replaces the sum with an integral, and the basic idea is the same: if the current
marginal density is stationary, then updating to the next period leaves probabilities
unchanged. Note, however, that when the state space is inﬁnite a stationary density
may fail to exist. You will be asked to give an example in exercise 8.2.2.
Recall that in the ﬁnite state case, when the stochastic kernel p is globally stable,
each Markov chain generated by the kernel satisﬁes a law of large numbers (theorem 4.3.33, page 94). Here we have an analogous result. As shown in theorem 8.2.15
(page 206), given global stability and a function h such that |h( x )|ψ∗ ( x )dx is ﬁnite,
we have

1 n
h( Xt ) → h( x )ψ∗ ( x )dx as n → ∞
(6.11)
n t∑
=1
with probability one, where ( Xt )t≥0 is a time series generated by the model.
Exercise 6.1.20 Consider the STAR model (6.3) with α0 = β 0 = 0 and α1 = β 1 = a,

Inﬁnite State Space

131

where a is a constant with | a| < 1. Suppose that φ is standard normal. We will see later
that this is a stable parameter conﬁguration, and ψ∗ = N (0, 1/(1 − a2 )) is stationary
for this kernel. From (6.11) we have
1
n

n

1

∑ Xt2  1 − α2

for large n

t =1

Write a simulation that compares these two expressions for large n.
The LLN gives us a method to investigate the steady state distribution ψ∗ for globally stable systems. For example, we can form the empirical distribution function
Fn ( x ) :=

n

1
n

∑

t =1

{ Xt ≤ x } =

1
n

n

∑

t =1

(−∞,x ] ( Xt )

(x ∈

)

where (−∞,x] (y) = 1 if y ≤ x and zero otherwise and ( Xt )t≥0 is a simulated time series generated by the model. The empirical distribution
function was discussed previ
∗
ously in §6.1.2. We will see in what follows that
(−∞,x ] (y )ψ (y )dy is the probability
∗
∗
that a draw from ψ falls below x. In other words, F ( x ) :=
(−∞,x ] (y )ψ (y )dy is the
cumulative distribution function associated with ψ∗ . Setting h = (−∞,x] in (6.11), we
then have F n ( x ) → F ( x ) with probability one, ∀ x ∈ , and the empirical distribution
function is consistent for F.
Exercise 6.1.21 Use the empirical distribution function to compute an estimate of F
for (6.3) under the same parameters used in ﬁgure 6.6.
There is, however, a more powerful technique for evaluating ψ∗ when global stability holds. Taking our simulated time series ( Xt )nt=1 , deﬁne
ψn∗ (y) :=

1
n

n

∑ p ( Xt , y )

t =1

(y ∈

)

(6.12)

This expression is almost identical to the look-ahead estimator developed in §6.1.3
(see (6.9) on page 129), with the difference being that the random samples are now a
single time series rather than repeated draws at a ﬁxed point in time. To study the
properties of ψn∗ , observe that for any ﬁxed y ∈ S, the LLN (6.11) gives us
ψn∗ (y) :=

1
n

n

∑

t =1

p ( Xt , y ) →



p( x, y)ψ∗ ( x )dx = ψ∗ (y)

where the last equality is by (6.10). Thus ψn∗ (y) is consistent for ψ∗ (y).
In fact much stronger results are true, and ψn∗ is an excellent estimator for ψ∗ (see,
e.g., Stachurski and Martin 2008). The reason is that while estimators such as F n use

132

Chapter 6

10

20

30

40

50

60

k

Figure 6.7 Look-ahead estimator

only the information contained in the sampled time series ( Xt ), the look-ahead estimator ψn∗ also incorporates the stochastic kernel p, which encodes the entire dynamic
structure of the model.
Exercise 6.1.22 Use the look-ahead estimator (6.12) to compute an estimate of ψ∗ for
(6.3) under the same parameters used in ﬁgure 6.6.
Here is a second application. Consider the nonconvex growth model in (6.5) on
page 124 with δ = 1. We will prove below that the stochastic density kernel for this
model is


y
1
( x, y > 0)
(6.13)
p( x, y) = φ
sA( x ) x α sA( x ) x α
and that the model is globally stable. From stability we obtain the LLN (6.11), and
hence the look-ahead estimator (6.12) is consistent for the unique stationary density
ψ∗ . Figure 6.7 shows a realization of ψn∗ when A is the step function
A ( k ) : = A1 { k ≤ k b } + A2 { k > k b }

( k > 0)

ξ

and Wt = et , where ξ t ∼ N (0, σ2 ). The parameters are α = 0.5, s = 0.25, A1 = 15,
A2 = 25, σ2 = 0.02, k b = 22.81, and k0 = k b .
Exercise 6.1.23 Replicate ﬁgure 6.7. Sample a time series (k t )t≥0 from the model, and
implement ψn∗ with (k t )t≥0 and the kernel in (6.13).10
10 You

will require a value of n around 100,000 to get a reasonable estimate, and even then some variation

Inﬁnite State Space

6.2

133

Optimal Growth, Inﬁnite State

Let’s now look at a simple optimal growth model on an inﬁnite state space. We will
compute the optimal policy for the model numerically using value iteration and policy
iteration. We also study via simulation the dynamics of the model under that policy.

6.2.1

Optimization

Consider again the optimal growth model discussed in chapter 1. At time t an agent
receives income yt , which is split into consumption ct and savings k t . Given k t , output
at t + 1 is yt+1 = f (k t , Wt+1 ), where (Wt )t≥1 is IID and takes values in Z := (0, ∞)
according to density φ. The agent’s behavior is speciﬁed by a policy function σ, which
is a map from S := + to
satisfying 0 ≤ σ (y) ≤ y for all y ∈ S. The value
σ (y) should be interpreted as the agent’s choice of savings when income = y, while
0 ≤ σ (y) ≤ y is a feasibility constraint ensuring that savings is nonnegative and does
not exceed income. The set of all such policies will be denoted by Σ.
As with the ﬁnite state case, choice of a policy function σ ∈ Σ also determines an
SRS for the state variable, given by
yt+1 = f (σ (yt ), Wt+1 ),

IID

(Wt )t≥1 ∼ φ,

y0 = y

(6.14)

where y is initial income. Letting U be the agent’s utility function and ρ ∈ (0, 1) be
the discount factor, the agent’s decision problem is

max vσ (y),
σ∈Σ

where

vσ (y) :=



∞

∑ ρ U (yt − σ(yt ))
t

(6.15)

t =0

Here vσ (y) is the expected discounted value of following policy σ when initial income
is y0 = y. For now we assume that U : + → + is bounded and continuous, and
that f : + × Z → + is continuous.
As discussed in the ﬁnite case—see §5.1.1, and in particular the discussion surrounding (5.4) on page 100—a rigorous deﬁnition of the expectation in (6.15) requires
measure theory. The details are deferred until chapter 10. Among other things, we
will see that the expectation can be passed through the sum to obtain
vσ (y) =

∞

∑ ρt

t =0

U (yt − σ (yt ))

(y ∈ S =

+)

(6.16)

will be observable over different realizations. This is due to the nonlinearity in the model and resulting
slow convergence.

134

Chapter 6

This expression is simpler to interpret, with each term U (yt − σ (yt )) deﬁned in terms
of integrals over . Speciﬁcally, we integrate the function y → U (y − σ (y)) with
respect to the marginal distribution ψt of yt , where yt is deﬁned recursively in (6.14).
Given vσ in (6.16), we can deﬁne the value function v∗ in exactly the same way
as (5.8) on page 102: v∗ (y) := sup{vσ (y) : σ ∈ Σ}.11 Just as in §5.1.2, the value
function satisﬁes a Bellman equation: Letting Γ(y) := [0, y] be the feasible savings
choices when income is y, we have



∗
∗
(y ∈ S)
(6.17)
v (y) = max U (y − k) + ρ v ( f (k, z))φ(z)dz
k∈Γ(y)

The intuition behind (6.17) is similar to that for the ﬁnite state Bellman equation on
page 102, and won’t be repeated here. A proof that v∗ satisﬁes (6.17) will be provided
via theorem 10.1.11 on page 234. In the same theorem it is shown that v∗ is continuous.
Recall that bcS is the set of continuous bounded real-valued functions on S. Given
a w ∈ bcS, we say that σ ∈ Σ is w-greedy if



σ (y) ∈ argmax U (y − k) + ρ w( f (k, z))φ(z)dz
(y ∈ S)
(6.18)
k∈Γ(y)

Later in the text we will see that continuity of w implies continuity of the objective
function in (6.18), and since Γ(y) is compact, the existence of a maximizer σ(y) for
each y is guaranteed by theorem 3.2.22 (page 49).
We will also prove that a policy σ∗ is optimal in terms of maximizing expected discounted rewards if and only if it is v∗ -greedy (theorem 10.1.11). In view of continuity
of v∗ and the previous comment regarding existence of maximizers, this result shows
that at least one optimal policy exists. Moreover we can compute σ∗ by ﬁrst solving
for v∗ and then obtaining σ∗ as the maximizer in (6.18) with v∗ in place of w.
In order to compute v∗ , we deﬁne the Bellman operator T, which maps w ∈ bcS
into Tw ∈ bcS via



Tw(y) = max U (y − k) + ρ w( f (k, z))φ(z)dz
(y ∈ S)
(6.19)
k∈Γ(y)

We prove in chapter 10 that T is a uniform contraction of modulus ρ on the metric
space (bcS, d∞ ), where d∞ (v, w) := supy∈S |v(y) − w(y)|. In view of Banach’s ﬁxed
point theorem (page 53), T then has a unique ﬁxed point v̄ ∈ bcS, and T n v → v̄ in d∞
as n → ∞ for all v ∈ bcS. Moreover it is immediate from the deﬁnition of T and the
Bellman equation that Tv∗ (y) = v∗ (y) for all y ∈ S, so v̄ = v∗ . We conclude that all
trajectories of the dynamical system (bcS, T ) converge to v∗ .
boundedness of U it can be shown that this supremum is taken over a bounded set, and hence v∗
is well deﬁned. See exercise 10.1.8 on page 233. We treat unbounded rewards in §12.2.
11 From

Inﬁnite State Space

135

These observations suggest that to solve for an optimal policy we can use the value
iteration technique presented in algorithm 5.1 on page 103, replacing bS with bcS for
the set from which the initial condition is chosen. The algorithm returns a v-greedy
policy σ, computed from a function v ∈ bcS that is close to v∗ . If v is close to v∗ , then
v-greedy policies are “almost optimal.” See §10.2.1 for details.

6.2.2

Fitted Value Iteration

Let’s turn to numerical techniques. With regard to value iteration, the fact that the
state space is inﬁnite means that implementing the sequence of functions generated
by the algorithm on a computer is problematic. Essentially, the issue is that if w is an
arbitrary element of bcS, then to store w in memory we need to store the values w(y)
for every y ∈ S. For inﬁnite S this is not generally possible.
At the same time, some functions from S to can be stored on a computer. For
example, if w is a polynomial function such as w(y) = ∑in=−01 ai yi , then to store w in
memory, we need only store the n coefﬁcients ( ai )in=−01 and the instructions for obtaining w(y) from these coefﬁcients. Functions that can be recorded in this way (i.e., with
a ﬁnite number of parameters) are said to have ﬁnite parametric representation.
Unfortunately, iterates of the Bellman operator do not naturally present themselves in ﬁnite parametric form. To get Tv from v, we need to solve a maximization
problem at each y and record the result. Again, this is not possible when S is inﬁnite.
A common kludge is discretization, where S is replaced with a grid of size k, and the
original model with a “similar” model that evolves on the grid. This is rarely the best
way to treat continuous state problems, since a great deal of useful information is discarded, and there is little in the way of theory guaranteeing that the limiting policy
converges to the optimal policy as k → ∞.12
Another approach is ﬁtted value iteration, as described in algorithm 6.2. Here F
is a class of functions with ﬁnite parametric representation. The map v → w deﬁned
by the ﬁrst two lines of the loop is, in effect, an approximate Bellman operator T̂,
and ﬁtted value iteration is equivalent to iteration with T̂ in place of T. A detailed
theoretical treatment of this algorithm is given in §10.2.3. At this stage let us try to
grasp the key ideas, and then look at implementation.
The ﬁrst thing to consider is the particular approximation scheme to be used in the
step that sends Tv into w ∈ F . A number of schemes have been used in economic
modeling, from Chebychev polynomials to splines and neural nets. In choosing the
best method we need to consider how the scheme interacts with the iteration process
12 One reason is that the resulting policy is not an element of the original policy space Σ, making it difﬁcult to discuss the error induced by approximation. As an aside, some studies actually treat discrete state
problem using continuous approximations in order to reduce the number of parameters needed to store the
value function.

136

Chapter 6

Algorithm 6.2 Fitted value iteration
initialize v ∈ bcS
repeat
sample the function Tv at ﬁnite set of grid points (yi )ik=1
use the samples to construct an approximation w ∈ F of Tv
set e = d∞ (v, w)
set v = w
until e is less that some tolerance
solve for a v-greedy policy σ

used to compute the ﬁxed point v∗ . A scheme that approximates individual functions
well with respect to some given criterion does not always guarantee good dynamic
properties for the sequence ( T̂ n v)n≥1 .
To try to pin down a suitable technique for approximation, let’s decompose T̂ into
the action of two operators L and T. First T is applied to v—in practice Tv is evaluated
only at ﬁnitely many points—and then an approximation operator L sends the result
into w = T̂v ∈ F . Thus, T̂ = L ◦ T. Figure 6.8 illustrates iteration of T̂.
We aim to choose L such that (1) the sequence ( T̂ n v)n≥1 converges, and (2) the
collection of functions F is sufﬁciently rich that the limit of this sequence (which
lives in F ) can be close to the ﬁxed point v∗ of T (which lives in bcS).13 The richness
of F depends on the choice of the approximation scheme and the number of grid
points k in algorithm 6.2. In the formal results presented in §10.2.3, we will see that
the approximation error depends on d∞ ( Lv∗ , v∗ ), which indicates how well v∗ can be
approximated by an element of F .
Returning to point (1), any serious attempt at theory requires that the sequence
( T̂ n v)n≥1 converges in some sense as n → ∞. In this connection, note the following
result.
Exercise 6.2.1 Let M and N be operators sending metric space (U, d) into itself. Show
that if N is a uniform contraction with modulus ρ and M is nonexpansive, then M ◦ N
is a uniform contraction with modulus ρ.
As T is a uniform contraction on (bcS, d∞ ), we see that T̂ is uniformly contracting
whenever L is nonexpansive on (bcS, d∞ ). While for some common approximation
architectures this fails, it does hold for a number of useful schemes. When attention
is restricted to these schemes the sequence ( T̂ n v)n≥1 is convergent by Banach’s ﬁxed
point theorem, and we can provide a detailed analysis of the algorithm.
13 More

correctly, the limit of the sequence lives in cl F ⊂ bcS.

Inﬁnite State Space

137

bcS
T

F

v

L

T

L
T̂

T̂

Figure 6.8 The map T̂ := L ◦ T

v

Lv
Lw
w

x1

x2

x3

Figure 6.9 Approximation via linear interpolation

138

Chapter 6

Listing 6.4 (lininterp.py) An interpolation class
from scipy i m p o r t interp
c l a s s LinInterp :
" Provides linear interpolation in one dimension . "
d e f __init__ ( self ,
" " " Parameters :
containing the
self .X , self . Y

X, Y):
X and Y are sequences or arrays
(x , y ) interpolation points . " " "
= X, Y

d e f __call__ ( self , z ) :
" " " If z is a float ( or integer ) returns a float ;
if z is a sequence or array returns an array . " " "
i f isinstance (z , int ) o r isinstance (z , float ) :
r e t u r n interp ([ z ] , self .X , self . Y ) [0]
r e t u r n interp (z , self .X , self . Y )

Let’s move on to implementation, deferring further theory until §10.2.3. The approximation scheme we will use is piecewise linear interpolation, as shown in ﬁgure 6.9. (Outside the set of grid points, the approximations are constant.) With reference to the ﬁgure, it is not difﬁcult to see that for any v, w ∈ bcS, and any x in the
domain, we have

| Lv( x ) − Lw( x )| ≤ sup |v( xi ) − w( xi )| ≤ v − w∞
1≤ i ≤ k

Taking the supremum over x gives  Lv − Lw∞ ≤ v − w∞ , so L is nonexpansive on
bcS.
Consider the class LinInterp in listing 6.4. This class provides an interface to
SciPy’s interp() function, which implements linear interpolation. The latter is called
using syntax interp(Z, X, Y), where X and Y are arrays of equal length providing
the ( x, y) interpolation points, and Z is an array of arbitrary points where the interpolant is to be evaluated. The call returns an array of length len(Z) containing these
evaluations.
The class LinInterp makes two modiﬁcations. First, an object of class LinInterp
stores the interpolation points X and Y internally, so after instantiation of the object
with these arrays, evaluations of the interpolant can be obtained without having to
pass the interpolation points each time. Second, we may wish to evaluate the inter-

Inﬁnite State Space

139

Listing 6.5 (fvi.py) Fitted value iteration
from scipy i m p o r t linspace , mean , exp , randn
from scipy . optimize i m p o r t fminbound
from lininterp i m p o r t LinInterp
# From listing 6.4
theta , alpha , rho = 0.5 , 0.8 , 0.9
d e f U ( c ) : r e t u r n 1 - exp ( - theta * c )
d e f f (k , z ) : r e t u r n ( k ** alpha ) * z
W = exp ( randn (1000) )

#
#
#
#

Parameters
Utility
Production
Draws of shock

gridmax , gridsize = 8 , 150
grid = linspace (0 , gridmax **1 e -1 , gridsize ) **10
d e f maximum (h , a , b ) :
r e t u r n h ( fminbound ( lambda x : -h ( x ) , a , b ) )
d e f bellman ( w ) :
" " " The approximate Bellman operator .
Parameters : w is a vectorized function ( i . e . , a
callable object which acts pointwise on arrays ) .
Returns : An instance of LinInterp .
"""
vals = []
f o r y i n grid :
h = lambda k : U ( y - k ) + rho * mean ( w ( f (k , W ) ) )
vals . append ( maximum (h , 0 , y ) )
r e t u r n LinInterp ( grid , vals )

polant at a single point (ﬂoat or integer) and receive a single number in return (rather
than always having to pass and receive arrays). The if statement in the __call__
method checks the type of the call parameter z and deals with it appropriately.14
The main part of the code is in listing 6.5. The utility function is set to U (c) =
1 − e−θc , where the risk aversion parameter θ determines the curvature of U. The
production function is f (k, z) = kα z. The shock is assumed to be lognormal; W = eξ ,
where ξ is standard normal.
Stepping through the logic of listing 6.5, the grid is formed by a call to SciPy’s
linspace() function, which returns an evenly spaced sequence on [0, 80.1 ]. Algebraic
14 For

discussion of the __call__ method see §2.2.3.

140

Chapter 6
1.5

1.25

1

0.75

0.5

0.25

0

0

0.25

0.5

0.75

1

1.25

1.5

1.75

2

Figure 6.10 FVI algorithm iterates

operations on SciPy (NumPy) arrays are performed elementwise, so appending **10
to the end of the line raises each element of the grid to the power of 10. The overall
effect is to create a grid on [0, 8] such that most grid points are close to zero. This is desirable since most the value function’s curvature is close to zero, and more curvature
requires closer grid points to achieve the same level of error.
Next we deﬁne the function maximum() that takes a function h and two points a and
b, and returns the maximum of h on the interval [ a, b]. As deﬁned here, maximum() is
just a wrapper for SciPy’s fminbound() function. The latter computes minimizers (not
minima), and our wrapper uses this functionality to compute maxima. Speciﬁcally,
we exploit the fact that the maximizer of h on [ a, b] is the minimizer x ∗ of −h on [ a, b].
Hence h( x ∗ ) is the maximum.
The function bellman() is the approximate Bellman operator T̂ = L ◦ T, taking a
function w and returning a new function T̂w. The for loop steps through each grid
point yi , computing Tw(yi ) as deﬁned in (6.19) and recording this value in vals.
Note
how the expression mean(w(f(k,W))) is used to approximate the expection in

w( f (k, z))φ(z)dz. We are using the fact that w is vectorized (i.e., acts elementwise on
NumPy arrays), so w(f(k,W)) is an array produced by applying w( f (k, ·)) to each element of the shock array W. Vectorized operations are typically faster than for loops.15
After collecting the values Tw(yi ) in the for loop, the last line of the function deﬁnition returns an instance of LinInterp, which provides linear interpolation between
the set of points (yi , Tw(yi )). This object corresponds to T̂w = L( Tw). Figure 6.10
shows convergence of the sequence of iterates, starting from initial condition U.
15 Alternatively,

the integral can be computed using a numerical integration routine.

Inﬁnite State Space

141

4

45 degrees
y → σ(y)
y → f (σ(y), m)
3

2

1

0

0

0.5

1

1.5

2

2.5

3

3.5

4

Figure 6.11 Approximate optimal policy

We can now compute a greedy policy σ from the last of these iterates, as shown
in ﬁgure 6.11; along with the function y → f (σ (y), m), where m is the mean of the
shock. If the shock is always at its mean, then from every positive initial condition the
income process (yt )t≥0 would converge to the unique ﬁxed point at  1.25. Notice
that when income is low, the agent invests all available income.
Of course, what happens when the shock is at its mean gives us little feel for the
true dynamics. To generate the sequence of densities corresponding to the process
yt+1 = f (σ (yt ), Wt+1 ), we can use the look-ahead estimator (6.9) on page 129. To use
the theory developed there it is easiest to operate in logs. Given our parameterization
of f , taking logs of both sides of yt+1 = f (σ (yt ), Wt+1 ) = σ (yt )α Wt+1 yields
xt+1 = α ln σ (exp( xt )) + wt+1 := g( xt ) + wt+1
where xt := ln yt , wt := ln Wt and g( x ) := α ln σ (exp( x )). This SRS is of the form (6.6)
on page 126, and the look-ahead estimator of the density ψt of xt can be computed via
ψtn (y) :=

1
n

n

∑ p(xti −1 , y)

i =1

(y ∈

)

(6.20)

where p( x, y) = φ(y − g( x )) and ( xti −1 )in=1 is n independent draws of xt−1 starting
from a given initial condition x0 . Here φ is the density of wt —in this case it’s N (0, 1).
Figure 6.12 shows the densities ψ1 to ψ15 starting at x0 ≡ −7.5 and using n = 1, 000.
Exercise 6.2.2 Using your preferred plotting utility and extending the code in listing 6.5, replicate ﬁgures 6.10 through 6.12. Starting from initial condition U, about

142

Chapter 6
0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
−10

−7.5

−5

−2.5

0

2.5

5

Figure 6.12 Densities of the income process

30 iterates of T̂ produces a good approximation of v∗ . When you compute the densities via the look-ahead estimator, try experimenting with different initial conditions.
Observe how the densities always converge to the same limit.

6.2.3

Policy Iteration

Recall that in §5.1.3 we solved the ﬁnite state problem using a second algorithm, called
policy iteration. (See in particular algorithm 5.2 on page 105.) We can do the same
thing here, although we will need to use approximation techniques similar to those
we used for ﬁtted value iteration. The basic idea is presented in algorithm 6.3. (In
practice, the functions vσ and σ will have to be approximated at each step.)
Algorithm 6.3 Policy iteration algorithm
pick any σ ∈ Σ
repeat
compute vσ from σ
solve for a vσ -greedy policy σ
set σ = σ
until until some stopping condition is satisﬁed

Inﬁnite State Space

143

The theory behind policy iteration is presented in §10.2.2. In this section our interest will be in implementation. When considering implementation the most difﬁcult
part is to compute vσ from σ (i.e., to calculate the value of a given policy). Let’s think
t
about how we might do this given the deﬁnition vσ (y) = ∑∞
t=0 ρ U ( yt − σ ( yt )) in
(6.16).
Fix y and consider the evaluation of vσ (y). We need to sum the terms U (yt −
σ (yt )) discounted by ρt , at least for all t ≤ T where T is large. How should we
evaluate U (y j − σ (y j )) for ﬁxed j? One idea would be to use Monte Carlo. The initial
condition is y, and from there the process (yt )t≥0 follows the SRS in (6.14). We can
generate n independent observations y1j , . . . , ynj of y j using a version of algorithm 6.1

(page 121). The mean n−1 ∑in=1 U (yij − σ (yij )) is close to U (y j − σ (y j )) for large n.16
While Monte Carlo methods can be useful for high-dimensional problems, this
approach is not the easiest to implement. The reason is that even if we obtain good
approximations to U (yt − σ (yt )) for each t and then sum them (discounting by ρt ) to
obtain vσ (y), we have only obtained an evaluation of the function vσ at a single point
y. But for algorithm 6.3 we need to evaluate vσ at every point y (or at least on a grid
of points so we can approximate vσ ).
Rather than us going down this path, consider the following iterative technique.
For given σ ∈ Σ, deﬁne the operator Tσ that sends w ∈ bcS into Tw ∈ bcS by
Tσ w(y) = U (y − σ (y)) + ρ



w( f (σ(y), z))φ(z)dz

(y ∈ S)

(6.21)

For us the pertinent features of Tσ are summarized in the following result.
Lemma 6.2.3 For each σ ∈ Σ, the operator Tσ is a uniform contraction on (bcS, d∞ ), and the
unique ﬁxed point of Tσ in bcS is vσ .
The proof is not overly difﬁcult but does involve some measure theory. As such
we defer it until §10.1.3. What is important for us now is that from any initial guess
v ∈ bcS we have Tσn v → vσ so, by iterating with Tσ , we can obtain an approximation
to vσ . In doing so we need to approximate at each iteration, just as we did for ﬁtted
value iteration (algorithm 6.2, but with Tσ in place of T). Listing 6.6 provides an
implementation of this scheme, along with some additional routines.
The ﬁrst line of the listing imports everything from the module fvi, which is
the ﬁle in listing 6.5. This provides the primitives of the model and the grid. Next
some functions are imported from scipy. The ﬁrst function maximizer() is similar to
maximum() in listing 6.5 but returns maximizers rather than maxima.
16 An alternative Monte Carlo strategy would be to use the densities computed by the look-ahead estimator (see ﬁgure 6.12). Numerical integration of y → U (y − σ (y)) with respect to the density of y j then gives
another approximation to U (y j − σ (y j )).

144

Chapter 6

Listing 6.6 (fpi.py) Fitted policy iteration
from fvi i m p o r t * # Import all definitions from listing 6.5
from scipy i m p o r t absolute a s abs
d e f maximizer (h , a , b ) :
r e t u r n fminbound ( lambda x : -h ( x ) , a , b )
d e f T ( sigma , w ) :
" Implements the operator L T_sigma . "
vals = []
f o r y i n grid :
Tw_y = U ( y - sigma ( y ) ) + rho * mean ( w ( f ( sigma ( y ) , W ) ) )
vals . append ( Tw_y )
r e t u r n LinInterp ( grid , vals )
d e f get_greedy ( w ) :
" Computes a w - greedy policy . "
vals = []
f o r y i n grid :
h = lambda k : U ( y - k ) + rho * mean ( w ( f (k , W ) ) )
vals . append ( maximizer (h , 0 , y ) )
r e t u r n LinInterp ( grid , vals )
d e f get_value ( sigma , v ) :
" " " Computes an approximation to v_sigma , the value
of following policy sigma . Function v is a guess .
"""
tol = 1e -2
# Error tolerance
w h i l e 1:
new_v = T ( sigma , v )
err = max ( abs ( new_v ( grid ) - v ( grid ) ) )
i f err < tol :
r e t u r n new_v
v = new_v

Inﬁnite State Space

145

The function T takes functions sigma and w, representing σ and w respectively, and
returns Tσ w as an instance of the class LinInterp deﬁned in listing 6.4. The code is
relatively self-explanatory, as is that for the function get_greedy(), which takes as its
argument a function w and returns a w-greedy policy as an instance of LinInterp.
The function get_value is used to calculate vσ from σ. It takes as arguments the
functions sigma and v, which represent σ and a guess for vσ . We then iterate on this
guess with Tσ (actually L ◦ Tσ where L is the linear interpolation operator) to obtain
an approximation to vσ . Iteration proceeds until the maximum distance over the grid
points between the new and previous iterate falls below some tolerance.
Exercise 6.2.4 Compute an approximate optimal policy from listing 6.6. Check that
this policy is similar to the one you computed in exercise 6.2.2.17

6.3

Stochastic Speculative Price

This section applies some of the ideas we have developed to the study of prices in a
commodity market with consumers and speculators. After specifying and solving the
model, we will also investigate how the solution can be obtained using the optimal
growth model of §6.2. In this way, we will see that the optimal growth model, which
at ﬁrst pass seems rather limited, can in fact be applied to the study of decentralized
economies with a large number of agents.

6.3.1

The Model

Consider a market for a single commodity, whose price is given at t by pt . The “harvest” of the commodity at time t is Wt . We assume that the sequence (Wt )t≥1 is IID
with common density function φ. The harvests take values in S := [ a, ∞), where
a > 0. The commodity is purchased by “consumers” and “speculators.” We assume
that consumers generate demand quantity D ( p) corresponding to price p. Regarding
the inverse demand function D −1 =: P we assume that
Assumption 6.3.1 The function P : (0, ∞) → (0, ∞) exists, is strictly decreasing and
continuous, and satisﬁes P( x ) ↑ ∞ as x ↓ 0.
Speculators can store the commodity between periods, with It units purchased in
the current period yielding αIt units in the next, α ∈ (0, 1). For simplicity, the risk free
interest rate is taken to be zero, so expected proﬁt on It units is
t p t +1

· αIt − pt It = (α

t p t +1

− pt ) It

Suppose that σn is the policy computed at the n-th iteration. A good initial condition for the guess
of vσn in get_value() is vσn−1 .
17 Hint:

146

Chapter 6

Here t pt+1 is the expectation of pt+1 taken at time t. Speculators are assumed to be
risk neutral. Nonexistence of arbitrage requires that
α

t p t +1

− pt ≤ 0

(6.22)

Proﬁt maximization gives the additional condition
α

t p t +1

− pt < 0 implies It = 0

(6.23)

We also require that the market clears in each period. Supply Xt is the sum αIt−1 +
Wt of carryover by speculators and the current harvest, while demand is D ( pt ) + It
(i.e., purchases by consumers and speculators). The market equilibrium condition is
therefore
αIt−1 + Wt =: Xt = D ( pt ) + It
(6.24)
The initial condition X0 ∈ S is treated as given.
Now to ﬁnd an equilibrium. Constructing a system ( It , pt , Xt )t≥0 for investment,
prices, and supply that satisﬁes (6.22)–(6.24) is not trivial. Our path of attack will be
to seek a system of prices that depend only on the current state. In other words, we take a
function p : S → (0, ∞) and set pt = p( Xt ) for every t. The vector ( It , pt , Xt )t≥0 then
evolves as
pt = p( Xt ), It = Xt − D ( pt ), Xt+1 = αIt + Wt+1
(6.25)
For given X0 and exogenous process (Wt )t≥1 , the system (6.25) determines the time
path for ( It , pt , Xt )t≥0 as a sequence of random variables. We seek a p such that (6.22)
and (6.23) hold for the corresponding system (6.25).18
To this end, suppose that there exists a particular function p∗ : S → (0, ∞) satisfying
 

∗
∗
( x ∈ S)
(6.26)
p ( x ) = max α p (αI ( x ) + z)φ(z)dz, P( x )
where

I ( x ) := x − D ( p∗ ( x ))

( x ∈ S)

(6.27)

It turns out that such a p∗ will sufﬁce, in the sense that (6.22) and (6.23) hold for the
corresponding system (6.25). To see this, observe ﬁrst that19
t p t +1

=

tp

∗

( X t +1 ) =

tp

∗

(αI ( Xt ) + Wt+1 ) =



p∗ (αI ( Xt ) + z)φ(z)dz

(6.25) we have Xt = It + D ( pt ), so (6.24) automatically holds.
the manipulations here are not obvious don’t be concerned—we will treat random variables in detail
later on. The last inequality usesthe fact that if U and V are independent and V has density φ then the
expectation of h(U, V ) given U is h(U, z)φ(z)dz.
18 Given
19 If

Inﬁnite State Space

147

Thus (6.22) requires that
α



p∗ (αI ( Xt ) + z)φ(z)dz ≤ p∗ ( Xt )

This inequality is immediate from (6.26). Second, regarding (6.23), suppose that
α



p∗ (αI ( Xt ) + z)φ(z)dz < p∗ ( Xt )

Then by (6.26) we have p∗ ( Xt ) = P( Xt ), whence D ( p∗ ( Xt )) = Xt , and It = I ( Xt ) = 0.
(Why?) In conclusion, both (6.22) and (6.23) hold, and the system ( It , pt , Xt )t≥0 is an
equilibrium.
The only issue remaining is whether there does in fact exist a function p∗ : S →
(0, ∞) satisfying (6.26). This is not obvious, but can be answered in the afﬁrmative
by harnessing the power of Banach’s ﬁxed point theorem. To begin, let C denote the
set of decreasing (i.e., nonincreasing) continuous functions p : S →
with p ≥ P
pointwise on S.
Exercise 6.3.2 Argue that C ⊂ bcS, the set of all bounded continuous functions from
S into .
Exercise 6.3.3 Show that if (hn ) ⊂ C and d∞ (hn , h) → 0 for some function h ∈ bcS,
then h is decreasing and dominates P.
Lemma 6.3.4 The metric space (C , d∞ ) is complete.
Proof. By theorem 3.2.3 on page 45 (closed subsets of complete spaces are complete)
and the completeness of bcS (theorem 3.2.9 on page 46) we need only show that C is
closed as a subset of bcS. This follows from exercise 6.3.3.
As C is complete, it provides a suitable space in which we can introduce an operator from C to C and—appealing to Banach’s ﬁxed point theorem—show the existence
of an equilibrium. The idea is to construct the operator such that (1) any ﬁxed point
satisﬁes (6.26), and (2) the operator is uniformly contracting on C . The existence of an
operator satisfying conditions (1) and (2) proves the existence of a solution to (6.26).
So let p be a given element of C , and consider the new function on S constructed
by associating to each x ∈ S the real number r satisfying
 

r = max α p(α( x − D (r )) + z)φ(z)dz, P( x )
(6.28)
We denote the new function by T p, where T p( x ) is the r that solves (6.28), and regard
T as an operator sending elements of C into new functions on S. It is referred to below
as the pricing functional operator.

148

Chapter 6

Theorem 6.3.5 The following results hold:
1. The pricing functional operator T is well-deﬁned, in the sense that T p( x ) is a uniquely
deﬁned real number for every p ∈ C and x ∈ S. Moreover
 

( x ∈ S)
P( x ) ≤ T p( x ) ≤ v( x ) := max α p(z)φ(z)dz, P( x )
2. T maps C into itself. That is, T (C ) ⊂ C .
The proof is only sketched. You might like to come back after reading up on measure theory and ﬁll out the details. To start, let p ∈ C and x ∈ S. Deﬁne
 

h x (r ) := max α p(α( x − D (r )) + z)φ(z)dz, P( x )
( P( x ) ≤ r ≤ v( x ))
Although we skip the proof, this function is continuous and decreasing on the interval [ P( x ), v( x )]. To establish the claim that there is a unique r ∈ [ P( x ), v( x )] satisfying
(6.28), we must show that h x has a unique ﬁxed point in this set. As h x is decreasing, uniqueness is trivial.20 Regarding existence, it sufﬁces to show that there exist
numbers r1 ≤ r2 in [ P( x ), v( x )] with
r1 ≤ h x (r1 )

and

h x (r2 ) ≤ r2

Why does this sufﬁce? The reason is that if either holds with equality then we are
done, and if both hold inequalities are strict, then we can appeal to continuity of h x
and the intermediate value theorem (page 335).21
A suitable value for r1 is P( x ). (Why?) For r2 we can use v( x ), as
 

h x (r2 ) = max α p(α( x − D (r2 )) + z)φ(z)dz, P( x )
 

≤ max α p(z)φ(z)dz, P( x ) = v( x ) = r2
The claim is now established, and with it part 1 of the theorem.
To prove part 2, we must show that T p (1) dominates P, (2) is decreasing on S, and
(3) is continuous on S. Of these, (1) is implied by previous results, while (2) and (3)
hold but proofs are omitted—we won’t cover the necessary integration theory until
chapter 7.22
20 This

was discussed in exercise 3.2.31 on page 51.
you see why? Apply the theorem to g(r ) = r − h(r ).
22 The proof of (3) uses theorem B.1.4 on page 341.
21 Can

Inﬁnite State Space

149

Exercise 6.3.6 Verify that if p∗ is a ﬁxed point of T, then it solves (6.26).
Theorem 6.3.7 The operator T is a uniform contraction of modulus α on (C , d∞ ).
It follows from theorem 6.3.7 that there exists a unique p∗ ∈ C with T p∗ = p∗ . In
view of exercise 6.3.6, p∗ satisﬁes (6.26), and we have solved our existence problem.
Thus it only remains to conﬁrm theorem 6.3.7, which can be proved using Blackwell’s
sufﬁcient condition for a uniform contraction. To state the latter, consider the metric space ( M, d∞ ), where M is a subset of bU, the bounded real-valued functions on
arbitrary set U.
Theorem 6.3.8 (Blackwell) Let M be a subset of bU with the property that u ∈ M and
γ ∈ + implies u + γ U ∈ M. If T : M → M is monotone and

∃ λ ∈ [0, 1)

s.t.

T (u + γ

U)

≤ Tu + λγ

U

∀ u ∈ M and γ ∈

+

(6.29)

then T is uniformly contracting on ( M, d∞ ) with modulus λ.
Monotonicity means that if u, v ∈ M and u ≤ v, then Tu ≤ Tv, where all inequalities are pointwise on U. The proof of theorem 6.3.8 is given in on page 344, and we
now return to the proof of theorem 6.3.7.
Exercise 6.3.9 Let h1 and h2 be decreasing functions on S with (necessarily unique)
ﬁxed points x1 and x2 . Show that if h1 ≤ h2 , then x1 ≤ x2 .
Using this exercise it is easy to see that T is a monotone operator on C : Pick any
p, q ∈ C with p ≤ q, and any x ∈ S. Let r → h p (r ) be deﬁned by
 

h p (r ) := max α p(α( x − D (r )) + z)φ(z)dz, P( x )
and let hq (r ) be deﬁned analogously. Clearly, T p( x ) is the ﬁxed point of r → h p (r ),
as is Tq( x ) the ﬁxed point of r → hq (r ). Since h p (r ) ≤ hq (r ) for all r, it must be that
T p( x ) ≤ Tq( x ). As x was arbitrary we have T p ≤ Tq.
To apply Blackwell’s condition, we need to show in addition that if p ∈ C and
γ ∈ + , then (1) p + γ S ∈ C , and (2) there exists a λ < 1 independent of p and γ
and having the property
T ( p + γ S ) ≤ T p + λγ S
(6.30)
Statement (1) is obviously true. Regarding statement (2), we make use of the following
easy lemma:
Lemma 6.3.10 Let a, b, and c be real numbers with b ≥ 0. We have
max{ a + b, c} ≤ max{ a, c} + b

150

Chapter 6

If you’re not sure how to prove these kinds of inequalities, then here is how they
are done: Observe that both
a + b ≤ max{ a, c} + b

∴

and

c ≤ max{ a, c} + b

max{ a + b, c} ≤ max{ a, c} + b

To continue, let p and γ be as above, and let q := p + γ S . Pick any x ∈ S. Let r p stand
for T p( x ) and let rq stand for Tq( x ). We have
 

rq = max α q(α( x − D (rq )) + z)φ(z)dz, P( x )
 

≤ max α q(α( x − D (r p )) + z)φ(z)dz, P( x )
 

= max α p(α( x − D (r p )) + z)φ(z)dz + αγ, P( x )
 

≤ max α p(α( x − D (r p )) + z)φ(z)dz, P( x ) + αγ

= r p + αγ
Here the ﬁrst inequality follows from the fact that r p ≤ rq (since p ≤ q and T is
monotone), and the second from lemma 6.3.10.
We have show that T ( p + γ S )( x ) ≤ T p( x ) + αγ. Since x is arbitrary and α < 1,
the inequality (6.30) is established with λ := α.

6.3.2

Numerical Solution

In this section we compute the rational expectations pricing functional p∗ numerically
via Banach’s ﬁxed point theorem. To start, recall that in §6.3.1 we established the
existence of a function p∗ : S → (0, ∞) in C satisfying (6.26). In the proof, p∗ was
shown to be the ﬁxed point of the pricing operator T : C
p → T p ∈ C . In view of
Banach’s theorem we have d∞ ( T n p, p∗ ) → 0 as n → ∞ for any p ∈ C , so a natural
approach to computing p∗ is by iterating on an arbitrary element p of C (such as P).
In doing so, we will need to approximate the iterates T n p at each step, just as for ﬁtted
value iteration (algorithm 6.2, page 136). As before we use linear interpolation, which
is nonexpansive with respect to d∞ .
So suppose that p ∈ C and x ∈ S are ﬁxed, and consider the problem of obtaining T p( x ), which, by deﬁnition, is the unique r ∈ [ P( x ), v( x )] such that (6.28) holds.
Regarding this r,

Exercise 6.3.11 Show that r = P( x ) whenever α p(z)φ(z)dz ≤ P( x ).

Inﬁnite State Space

151

Exercise 6.3.12 Show that if α



r=α

p(z)φ(z)dz > P( x ), then r satisﬁes


p(α( x − D (r )) + z)φ(z)dz

Algorithm 6.4 Computing T p( x )

evaluate y = α p(z)φ(z)dz
if y ≤ P( x ) then return P( x )
else

deﬁne h(r ) = α p(α( x − D (r )) + z)φ(z)dz
return the ﬁxed point of h in [ P( x ), y]
end

Together, exercises 6.3.11 and 6.3.12 suggest the method for ﬁnding r presented in
algorithm 6.4, which returns T p( x ) given p and x. An implementation of the algorithm is given in listing 6.7. The demand curve D is set to 1/x, while for the shock we
assume that Wt = a + cBt , where Bt is beta with shape parameters (5, 5). The function
fixed_point() computes ﬁxed points inside a speciﬁed interval using SciPy’s brentq
root-ﬁnding algorithm. The function T() implements algorithm 6.4.23
Once we can evaluate T p( x ) for each p and x, we can proceed with the iteration
algorithm, as shown in algorithm 6.5. A sequence of iterates starting at P is displayed
in ﬁgure 6.13.
Algorithm 6.5 Computing the pricing function
set p = P
repeat
sample T p at ﬁnite set of grid points ( xi )ik=1
use samples to construct linear interpolant q of T p
set p = q
until a suitable stopping rule is satisﬁed

Exercise 6.3.13 Implement algorithm 6.5 and replicate ﬁgure 6.13.24
23 Although the else statement from algorithm 6.4 is omitted in the deﬁnition of T(), it is unnecessary
because the last two lines in the deﬁnition of T() are only executed if the statement y <= P(x) is false. Note
also that the function p() passed to T() must be vectorized.
24 Hint: You can use the LinInterp class in listing 6.4 (page 138) for interpolation.

152

Chapter 6

Listing 6.7 (cpdynam.py) Computing T p( x )
from scipy i m p o r t mean
from scipy . stats i m p o r t beta
from scipy . optimize i m p o r t brentq
alpha , a , c = 0.8 , 5.0 , 2.0
W = beta (5 , 5) . rvs (1000) * c + a
D = P = lambda x : 1.0 / x

# Shock observations

d e f fix_point (h , lower , upper ) :
" " " Computes the fixed point of h on [ upper , lower ]
using SciPy ’s brentq routine , which finds the
zeros ( roots ) of a univariate function .
Parameters : h is a function and lower and upper are
numbers ( floats or integers ) . " " "
r e t u r n brentq ( lambda x : x - h ( x ) , lower , upper )
d e f T (p , x ) :
" " " Computes Tp ( x ) , where T is the pricing functional
operator .
Parameters : p is a vectorized function ( i . e . , acts
pointwise on arrays ) and x is a number . " " "
y = alpha * mean ( p ( W ) )
i f y <= P ( x ) :
r e t u r n P(x)
h = lambda r : alpha * mean ( p ( alpha *( x - D ( r ) ) + W ) )
r e t u r n fix_point (h , P ( x ) , y )

Inﬁnite State Space

153

Figure 6.13 The trajectory T n P

Given p∗ , we have a dynamic system for quantities deﬁned by
Xt+1 = αI ( Xt ) + Wt+1 ,

IID

(Wt )t≥1 ∼ φ

(6.31)

where I ( x ) := x − D ( p∗ ( x )). As shown in §6.1.3, the distribution ψt of Xt is a density
for each t ≥ 1, and the densities satisfy
ψt+1 (y) =



p( x, y)ψt ( x )dx

(y ∈ S)

where p( x, y) = φ(y − αI ( x )), and φ is the density of the harvest Wt .25
Exercise 6.3.14 We will see later that the process (6.31) is stable, with a unique stationary density ψ∗ , and that the look-ahead estimator given in (6.12) on page 131 can
be used to estimate it. Using this estimator, show graphically that for these particular
parameter values, speculators do not affect long-run probabilities for the state, in the
sense that ψ∗  φ.26
25 We

regard φ as deﬁned on all of , and zero off its support. Hence, if y < αI ( x ), then p( x, y) = 0.
latter is the distribution that prevails without speculation.

26 The

154

6.3.3

Chapter 6

Equilibria and Optima

In §6.3.1 we used Banach’s ﬁxed point theorem to show the existence of a pricing
functional p∗ such that the resulting system for prices and quantities was a competitive equilibrium. There is another way we can obtain the same result using dynamic
programming and the optimal growth model. Solving the problem this way illustrates
one of the many fascinating links between decentralized equilibria and optimality.
Before getting started we are going to complicate the commodity pricing model
slightly by removing the assumption that the interest rate is zero. With a positive and
constant interest rate r, next period returns must be discounted by ρ := 1/(1 + r ). As
a result the no arbitrage and proﬁt maximization conditions (6.22) and (6.23) become
ρα
ρα

t p t +1

− pt ≤ 0

(6.32)

− pt < 0 implies It = 0

(6.33)

t p t +1

As in §6.3.1 we seek a pricing function p∗ such that the system
p t = p ∗ ( Xt ),

It = Xt − D ( pt ),

Xt+1 = αIt + Wt+1

(6.34)

satisﬁes (6.32) and (6.33). This can be accomplished directly using the ﬁxed point arguments in §6.3.1, making only minor adjustments to incorporate ρ. However, instead
of going down this path we will introduce a ﬁctitious planner who solves the optimal
growth model described in §6.2.1. Through a suitable choice of primitives, we show
that the resulting optimal policy can be used to obtain such a p∗ . Since the optimal
policy exists and can be calculated, p∗ likewise exists and can be calculated.
Regarding the planner’s primitives, the production function f is given by f (k, z) =
αk
+
 c z; the discount factor is ρ := 1/(1 + r ); the utility function U is deﬁned by U (c) :=
0 P ( x ) dx, where P is the inverse demand function for the commodity pricing model;
and the distribution φ of the shock is the distribution of the harvest.
We assume that P is such that U is bounded on + . By the fundamental theorem
of calculus we have U = P. The conditions of assumption 6.3.1 on page 145 also
hold, and as a result the function U is strictly increasing, strictly concave and satisﬁes
U (c) ↑ ∞ as c ↓ 0. We remove the assumption in §6.3.1 that the shock is bounded
away from zero, as this restriction is not needed.
Using the arguments in §6.2.1, we know that there exists at least one optimal policy.
In fact concavity of the primitives implies that there is only one such policy. (For the
proof, see §12.1.2.) The policy, denoted simply by σ, is v∗ -greedy, which is to say that

σ( x ) = argmax U ( x − k) + ρ
0≤ k ≤ x



∗

v ( f (k, z))φ(z)dz


(6.35)

Inﬁnite State Space

155

for all x. One can also show that v∗ is differentiable with (v∗ ) ( x ) = U ( x − σ ( x )), and
that the objective function in (6.35) is likewise differentiable. Using these facts and
taking into account the possibility of a corner solution, it can be shown that σ satisﬁes
U ◦ c( x ) ≥ ρ



U ◦ c[ f (σ ( x ), z)] f (σ ( x ), z)φ(z)dz

∀x ∈ S

(6.36)

Moreover if the inequality is strict at some x > 0, then σ( x ) = 0. Here c( x ) := x − σ ( x )
and f (k, z) is the partial derivative of f with respect to k. This is the famous Euler
(in)equality, and full proofs of all these claims can be found via propositions 12.1.23
and 12.1.24 in §12.1.2.
The main result of this section is that by setting p∗ equal to marginal utility of
consumption we obtain an equilibrium pricing functional for the commodity pricing
model.
Proposition 6.3.15 If p∗ is deﬁned by p∗ ( x ) := U ◦ c( x ) :=: U (c( x )), then the system
deﬁned in (6.34) satisﬁes (6.32) and (6.33).
Proof. Substituting the deﬁnition of p∗ into (6.36) we obtain
p∗ ( x ) ≥ ρ



p∗ [ f (σ ( x ), z)] f (σ ( x ), z)φ(z)dz

∀x ∈ S

with strict inequality at x implying that σ( x ) = 0. Using f (k, z) = αk + z this becomes
p∗ ( x ) ≥ ρα



p∗ (ασ( x ) + z)φ(z)dz

∀x ∈ S

Now observe that since c( x ) = x − σ ( x ), we must have
p∗ ( x ) = U ( x − σ ( x )) = P( x − σ ( x ))

∴

D ( p∗ ( x )) = x − σ ( x )

Turning this around, we get σ( x ) =
I ( x ). Hence σ = I, and we have
ρα



∀x ∈ S

∀x ∈ S

x − D ( p∗ ( x )), and the right-hand side is precisely

p∗ (αI ( x ) + z)φ(z)dz − p∗ ( x ) ≤ 0

∀x ∈ S

with strict inequality at x implying that I ( x ) = 0. Since this holds for all x ∈ S, it
holds at any realization Xt ∈ S, so
ρα



p∗ (αI ( Xt ) + z)φ(z)dz − p∗ ( Xt ) ≤ 0

with strict inequality implying I ( Xt ) = 0. Substituting It and pt from (6.34), and using
the fact that

p
=
p∗ (αI ( Xt ) + z)φ(z)dz
t t +1
as shown in §6.3.1, we obtain (6.32) and (6.33).

156

6.4

Chapter 6

Commentary

Further theory and applications of stochastic recursive sequences in economics and
ﬁnance can be found in Sargent (1987), Stokey and Lucas (1989), Farmer (1999), Dufﬁe
(2001), Miranda and Fackler (2002), Adda and Cooper (2003), or Ljungqvist and Sargent (2004). The simulation-based approach to computing marginal and stationary
densities of stochastic recursive sequences in §6.1.3 and §6.1.4 was proposed by Glynn
and Henderson (2001), and a detailed analysis of the technique and its properties can
be found in Stachurski and Martin (2008).
The one-sector, inﬁnite horizon, stochastic optimal growth model in §6.2 is essentially that of Brock and Mirman (1972). Related treatments can be found in Mirman
and Zilcha (1975), Donaldson and Mehra (1983), Stokey and Lucas (1989), Amir (1996),
and Williams (2004). A survey of the ﬁeld is given in Olsen and Roy (2006). For
discussion of the stability properties of the optimal growth model, see §12.1.3. The
commentary to that chapter contains additional references.
We saw in §6.3.3 that optimal policies for the growth model coincide with the market equilibria of certain decentralized economies. For more discussion of the links between dynamic programming and competitive equilibria, see Stokey and Lucas (1989,
ch. 16), or Bewley (2007). Seminal contributions to this area include Samuelson (1971),
Lucas and Prescott (1971), Prescott and Mehra (1980), and Brock (1982).
Under some variations to the standard environment (incomplete markets, production externalities, distortionary taxes, etc.), equilibria and optima no longer conincide,
and the problem facing the researcher is to ﬁnd equilibria rather than optimal policies. For a sample of the literature, see Huggett (1993), Aiyagari (1994), Greenwood
and Huffman (1995), Rios-Rull (1996), Krusell and Smith (1998), Kubler and Schmedders (2002), Reffett and Morand (2003), Krebs (2004), Datta et al. (2005), Miao (2006),
and Angeletos (2007).
The commodity price model studied in §6.3 is originally due to Samuelson (1971),
who connected equilibrium outcomes with solutions to dynamic programming problems. Our treatment in §6.3.1 and §6.3.2 follows Deaton and Laroque (1992), who
were the ﬁrst to derived the equilibrium price directly via Banach’s ﬁxed point theorem. The technique of iterating on the pricing functional is essentially equivalent to
Coleman’s algorithm (Coleman 1990). For more on the commodity pricing model, see,
for example, Scheinkman and Schectman (1983) or Williams and Wright (1991).

Part II

Advanced Techniques

157

Chapter 7

Integration
Measure and integration theory are among the foundation stones of modern mathematics, and particularly those ﬁelds of concern to us. Measure theory also has a reputation for being difﬁcult, and indeed it is both abstract and complex. However, with
a little bit of effort and attention to the exercises, you will ﬁnd that measure-theoretic
arguments start to seem quite natural, and that the theory has a unique beauty of its
own.
Before attempting this chapter you should have a good grounding in basic real
analysis. Anyone who has solved most of the exercises in appendix A should be up
to the task.

7.1

Measure Theory

In this ﬁrst section we give a brisk introduction to measure theory. The longer proofs
are omitted, although a ﬂavor of the arguments is provided. If you read this section
carefully you will have a good feel for what measure theory is about, and for why
things are done the way that they are.

7.1.1

Lebesgue Measure

To understand integration, we need to know about Lebesgue measure. The basic
problem of Lebesgue measure is how to assign to each subset of k (each element
of P( k )) a real number that will represent its “size” (length, area, volume) in the
most natural sense of the word.1 For a set like ( a, b] ⊂ 1 there is no debate: The
1 Recall

that P( A) denotes the set of all subsets of the set A.

159

160

Chapter 7

( I ) = (b1 − a1 ) × (b2 − a2 )
b2

I = ( a1, b1 ] × ( a2, b2 ]

a2

a1

b1

Figure 7.1 The measure of a rectangle I ⊂

2

length is b − a. Indeed, for a rectangle such as ×ik=1 ( ai , bi ] = { x ∈ k : ai < xi ≤
bi , i = 1, . . . , k}, the “measure” of this set is the product of the sides ∏ik=1 (bi − ai ). But
for an arbitrary set? For example, how large is , the set of rational numbers, when
taken as a subset of ? And how about the irrational numbers?
A natural approach is to try to extend the notion of size from sets we do know how
to measure to sets we don’t know how to measure. To begin, let J be the set of all
left-open, right-closed rectangles in k :
J := {×ik=1 ( ai , bi ] ∈ P(

k

) : a i , bi ∈

, a i ≤ bi }

Here we admit the possibility that ai = bi for some i, in which case the rectangle
×ik=1 ( ai , bi ] is the empty set. (Why?) Now let  be the map

: J

I = ×ik=1 ( ai , bi ] → ( I ) :=

k

∏ ( bi − a i ) ∈

i =1

+

(7.1)

which assigns to each rectangle its natural measure, with (∅) := 0 (see ﬁgure 7.1).
We want to extend the domain of  to all of P( k ). Our extension of  to P( k ) will be
denoted by λ.
The ﬁrst problem we must address is that there are many possible extensions. For
example, λ( A) = 42 for any A ∈
/ J is a possible—albeit not very sensible—extension
of . How will we know whether a given extension is the right one?

Integration

161

Figure 7.2 A covering of A by elements of J

The solution is to cross-check against our intuition. Our intuition says that size
should be nonnegative. Does our extension λ always give nonnegative values? In
addition λ should obey the fundamental principle that—at least when it comes to
and A :=
measurement—the whole is the sum of its parts. For example, if k =
( a, b] ∪ (c, d], where b ≤ c, then we must have λ( A) = b − a + d − c. More generally,
if A and B are disjoint, then one would expect, from our basic intuition about length
or area, that λ( A ∪ B) = λ( A) + λ( B). This property is called additivity, and we will
not be satisﬁed with our deﬁnition of λ unless it holds.
With this in mind, let’s go ahead and attempt an extension of  to P( k ). Given
arbitrary A ∈ P( k ), let C A be the set of all countable covers of A. That is,
C A := {( In )n≥1 ⊂ J : ∪n In ⊃ A}
Figure 7.2 shows a (necessarily ﬁnite) covering of A by elements of J . Now we deﬁne


λ( A) := inf

∑ ( In )

n ≥1

: ( In )n≥1 ∈ C A

( A ∈ P(

k

))

(7.2)

Thus we are approximating our arbitrary set A by covering it with sets we already
know how to measure, and taking the inﬁmum of the value produced by all such
covers.2 The set function λ is called Lebesgue outer measure. If ∑n≥1 ( In ) = ∞ for all
( In )n≥1 ∈ C A , then we set λ( A) = ∞.
Exercise 7.1.1 (Monotonicity) Show that if A ⊂ B, then λ( A) ≤ λ( B).3
2 We are using half-open rectangles here, but it turns out that other kinds of rectangles (closed, open, etc.)
produce the same number.
3 Hint: Apply lemma A.2.25 on page 333.

162

Chapter 7

Exercise 7.1.2 (Sub-additivity) Show that if A and B are any two subsets of
λ ( A ∪ B ) ≤ λ ( A ) + λ ( B ) .4

k,

then

Exercise 7.1.3 Extend sub-additivity to countable sub-additivity if you can: Show that
for any ( An ) ⊂ P( ) we have λ(∪n An ) ≤ ∑n λ( An ).5
Before we go on, we need to consider whether Lebesgue outer measure is actually
an extension of —the function deﬁned on J —to P( k ). Clearly, λ is a well deﬁned
function on P( k ) (why?), but does it actually agree with the original function  on
J ? In other words, we need to check that λ assigns “volume” to rectangles.
Lemma 7.1.4 λ : P(

k)

→ [0, ∞] deﬁned in (7.2) agrees with  on J .

Although the result seems highly likely, the proof is not entirely trivial. It can be
found in any text on measure theory.
Now the task is to see whether λ agrees with our intuition in the ways that we discussed above (nonnegativity, additivity, etc.). Nonnegativity is obvious, but when it
comes to additivity we run into problems: Additivity fails. In 1905 G. Vitali succeeded
in constructing sets A, B ∈ P( ) that are so nasty and intertwined that λ( A) + λ( B) >
λ( A ∪ B).6
Well, we said at the start that we were not prepared to accept the validity of our
extension unless it preserves additivity. So must (7.2) be abandoned? It might seem
so, but no obvious alternatives present themselves.7 The solution of Henri Lebesgue
was to restrict the domain of the set function λ to exclude those nasty sets that cause
additivity to break down. The method succeeds because the sets remaining in the
domain after this exclusion process turn out to be all those sets we will ever need in day
to day analysis.
The actual restriction most commonly used in modern texts is due to the Greek
mathematician Constantin Carathéodory, who considers the class of sets A ∈ P( k )
satisfying
λ( B) = λ( B ∩ A) + λ( B ∩ Ac ) for all B ∈ P( k )
(7.3)
This collection of sets is denoted by L and called the Lebesgue measurable sets. The
restriction of λ to L is called Lebesgue measure.
Exercise 7.1.5 Show that k ∈ L and ∅ ∈ L . Using monotonicity and sub-additivity,
show that if N ⊂ and λ( N ) = 0, then N ∈ L .
Fix  > 0 and choose covers ( InA )n≥1 and ( InB )n≥1 of A and B respectively such that ∑n ( InA ) ≤
λ( A) + /2 and ∑n ( InB ) ≤ λ( B) + /2. Now consider (∪n InA ) ∪ (∪n InB ) as a cover of A ∪ B.
5 Hint: Fix  > 0 and choose for each A a cover ( I n )
n
−n
n
j j≥1 such that ∑ j ( I j ) ≤ λ ( An ) + 2 . Now
n
consider ∪n ∪ j Ij as a cover of ∪n An .
6 His construction uses the dreaded Axiom of Choice.
7 We could try approximating sets from the inside (“inner” measure rather than outer measure), but this
is less convenient and it turns out that the same problem reappears.
4 Hint:

Integration

163

Our ﬁrst important observation is that Lebesgue measure is additive on L . In fact
one of the central facts of measure theory is that, restricted to this domain, λ is not just
additive, but countably additive (deﬁnition below). This turns out to be crucial when
trying to show that λ interacts well with limiting operations. Equally important, L is
very large, containing the open sets, closed sets, countable sets, and more.8
Let’s state without proof the countable additivity property of λ on L .
Theorem 7.1.6 (Countable additivity) If ( An )n≥1 is a disjoint sequence in L , in that
Ai ∩ A j = ∅ whenever i = j, then λ(∪n An ) = ∑n≥1 λ( An ).9
Exercise 7.1.7 Show that countable additivity implies (ﬁnite) additivity.10
Exercise 7.1.8 Prove that for any A, B ∈ L with A ⊂ B and λ( B) < ∞ we have
λ ( B \ A ) = λ ( B ) − λ ( A ).
To learn a bit more about the properties of λ, consider the following exercise: We
know that { x } ∈ L for all x ∈ k because L contains all closed sets. Let’s show that
λ({ x }) = 0 for any point x ∈ . It is enough to show that for any  > 0, we can ﬁnd
a sequence ( In )n≥1 ⊂ J containing x and having ∑n≥1 ( In ) ≤ . (Why?) So pick
such an , and take a cover ( In )n≥1 such that the ﬁrst rectangle I1 satisﬁes I1 x and
( I1 ) ≤ , and then In = ∅ for n ≥ 2. Now ∑n≥1 ( In ) ≤  as required.
Exercise 7.1.9 Show that λ(

k)

= ∞.11

Exercise 7.1.10 Using countable additivity and the fact that singletons { x } have zero
measure, show that countable sets have zero measure.12
You may be wondering if there are uncountable sets of measure zero. The answer
is yes. An often cited example is the Cantor set, which can be found in any text on
measure theory.

7.1.2

Measurable Spaces

We mentioned above that the set of Lebesgue measurable sets L contains all of the
sets we typically deal with in analysis, In addition it has nice “algebraic” properties.
In particular, it is a σ-algebra:
8 For

the deﬁnition of countable (and uncountable) sets see page 322.
value +∞ is permitted here. Also, readers might be concerned that for an arbitrary sequence
( An )n≥1 ⊂ L the union ∪n An may not be in L , in which case λ(∪n An ) is not deﬁned. In fact the union is
always in L as we will see below.
10 Hint: We must show that λ (∪ N A ) =
∑nN=1 λ( An ) for any ﬁnite collection of disjoint sets ( An )nN=1 .
n =1 n
Try thinking about the countable sequence ( Bn )n≥1 where Bn = An for n ≤ N and Bn = ∅ for n > N.
11 Hint: Using lemma 7.1.4 and monotonicity (exercise 7.1.1), show that λ ( k ) is bigger than any real
number.
12 This implies that in
we must have λ( ) = 0. By additivity, then, λ( ) = λ( c ). In this sense there
are “many more” irrational numbers than rational numbers.
9 The

164

Chapter 7

Deﬁnition 7.1.11 Let S be any nonempty set. A family of sets S ⊂ P(S) is called a
σ-algebra if
1. S ∈ S ,
2. A ∈ S implies Ac ∈ S , and
3. if ( An )n≥1 is a sequence with An in S for all n, then ∪n An ∈ S .
The pair (S, S ) is called a measurable space, and elements of S are called measurable sets. The second and third properties are usually expressed by saying that the
collection S is “closed” or “stable” under complementation and countable unions, in
the sense that these operations do not take us outside the collection S . From De Morgan’s law (∩n An )c = ∪n Acn , we see that S is also stable under countable intersections.
(Why?) Finally, note that ∅ ∈ S .
An example of a σ-algebra on S is P(S). This is true for every set S. For example,
if A ∈ P(S), then Ac := { x ∈ S : x ∈
/ A} is also a subset of S by its very deﬁnition.
On the other hand, the collection O of open subsets of is not a σ-algebra because it
is not stable under the taking of complements.
Incidentally, the concept of σ-algebras plays a major role in measure theory, and
these collections of sets sometimes seem intimidating and abstract to the outsider.
But the use of σ-algebras is less mysterious than it appears. When working with a
σ-algebra, we know that if we start with some measurable sets, take unions, then
complements, then intersections, and so on, the new sets we create are still measurable sets. By deﬁnition, σ-algebras are stable under the familiar set operations, so we
need not worry that we will leave our safe environment when using these standard
operations.
Exercise 7.1.12 Check that {∅, S} is a σ-algebra on S for any set S. Show that, on the
other hand, J is not a σ-algebra on k .
Exercise 7.1.13 If {Sα }α∈Λ is any collection of σ-algebras on S, then their intersection
∩α Sα is all B ⊂ S such that B ∈ Sα for every α ∈ Λ. Show that ∩α Sα is itself a
σ-algebra on S.
One of the most common ways to deﬁne a particular σ-algebra is to take a collection C of subsets of S, and consider the smallest σ-algebra that contains this collection.
Deﬁnition 7.1.14 If S is any set and C is any collection of subsets of S, then the σalgebra generated by C is the smallest σ-algebra on S that contains C , and is denoted by
σ (C ). More precisely, σ(C ) is the intersection of all σ-algebras on S that contain C . In
general, if σ (C ) = S , then C is called a generating class for S .

Integration

165

Exercise 7.1.15 Show that if C is a σ-algebra, then σ (C ) = C , and that if C and D are
two collections of sets with C ⊂ D, then σ (C ) ⊂ σ (D ).
Now let’s return to the Lebesgue measurable sets. As discussed above, it can be
shown that L is a σ-algebra, and that it contains all the sets we need in day-to-day
analysis.13 In fact L contains more sets than we actually need, and is not easily abstracted to more general spaces. As a result we will almost invariably work with a
smaller domain called the Borel sets, and denoted B ( k ). The collection B ( k ) is
just the σ-algebra generated by the open subsets of k .14 More generally,
Deﬁnition 7.1.16 Let S be any metric space. The Borel sets on S are the sets B (S) :=
σ (O ), where O is the open subsets of S.
Exercise 7.1.17 Explain why B (S) must contain the closed subsets of S.
In the case of S = k this collection B ( k ) is surprisingly large. In fact it is quite
difﬁcult to construct a subset of k that is not in B ( k ). Moreover B ( k ) is a subset
of L , and hence all of the nice properties that λ has on L it also has on B ( k ). In
particular, λ is countably additive on B ( k ).
Exercise 7.1.18 Show that

∈ B ( ).15

The following theorem gives some indication as to why the Borel sets are so natural
and important to analysis.
Theorem 7.1.19 Let O, C and K be the open, closed and compact subsets of
We have
B ( k ) : = σ (O ) = σ (C ) = σ (K ) = σ (J )

k

respectively.

Let’s just show that B ( k ) = σ (K ). To see that B ( k ) ⊃ σ (K ), note that B ( k )
is a σ-algebra containing all the closed sets, and hence all the compact sets. (Why?) In
which case it also contains σ(K ). (Why?) To show that B ( k ) ⊂ σ (K ), it sufﬁces to
prove that σ (K ) contains C . (Why?) To see that σ (K ) contains C , pick any C ∈ C
and let Dn := { x ∈ k :  x  ≤ n}. Observe that Cn := C ∩ Dn ∈ K for every n ∈
(why?), and that C = ∪n Cn . Since Cn ∈ K for all n, we have C = ∪n Cn ∈ σ (K ), as
was to be shown.
Exercise 7.1.20 Let A be the set of all open intervals ( a, b) ⊂
B ( ).16

. Show that σ(A ) =

13 To prove that L is a σ-algebra, one can easily check from the deﬁnition (7.3) that
∈ L and that
A ∈ L implies Ac ∈ L . To show that L is stable under countable unions is a bit more subtle and the proof
is omitted.
14 The open subsets of k are determined by the euclidean metric d . But any metric deﬁned from a norm
2
on k gives us the same open sets (theorem 3.2.30), and hence the same Borel sets.
15 Hint: Singletons are closed, and
= a countable union of singletons.
16 You should be able to show that σ (A ) ⊂ σ (O ). To show that σ (O ) ⊂ σ (A ) it is sufﬁcient to prove that

166

7.1.3

Chapter 7

General Measures and Probabilities

Measure theory forms the foundations of modern probability. Before we study this in
earnest, let’s think a little bit about why it might be fruitful to generalize the concept of
“measures.” To start, imagine a space S, which you might like to visualize as a subset
of the plane 2 . Like a satellite photograph of the earth at night, the space is sprinkled
with lots of tiny glowing particles. If we take a region E of the space S, you might ask
how many of these particles are contained in E, or alternatively, what fraction of the
total quantity of the particles are in E?
Let μ be a set function on P(S) such that μ( E) is the fraction of particles in E. It
seems that μ is going to be nonnegative, monotone (E ⊂ F implies μ( E) ≤ μ( F )), and
additive (E, F disjoint implies μ( E ∪ F ) = μ( E) ∪ μ( F )), just as the Lebesgue measure
λ was. So perhaps μ is also some kind of “measure,” and can be given a neat treatment
by using similar ideas.
These considerations motivate us to generalize the notion of Lebesgue measure
with an abstract deﬁnition. As is always the case in mathematics, abstracting in a
clever way will save us saying things over and over again, and lead to new insights.
Deﬁnition 7.1.21 Let (S, S ) be a measurable space. A measure μ on (S, S ) is a function from S to [0, ∞] such that
1. μ(∅) = 0, and
2. μ is countably additive: If ( An ) ⊂ S is disjoint, then μ(∪n An ) = ∑n μ( An ).
The triple (S, S , μ) is called a measure space.
Exercise 7.1.22 Show that if there exists an A ∈ S with μ( A) < ∞, then (2) implies
(1).17
Exercise 7.1.23 Show that (2) implies monotonicity: If E, F ∈ S and E ⊂ F, then
μ ( E ) ≤ μ ( F ).
Exercise 7.1.24 Show that a measure μ on S is always sub-additive: If A and B are any
elements of S (disjoint or otherwise), then μ( A ∪ B) ≤ μ( A) + μ( B).
Let ( An )n≥1 ⊂ S have the property that An+1 ⊂ An for all n ∈ , and let A :=
∩n An . We say that the sequence ( An )n≥1 decreases down to A, and write An ↓ A.
On the other hand, if An+1 ⊃ An for all n ∈
and A := ∪n An , then we say that
( An )n≥1 increases up to A, and write An ↑ A. For such “monotone” sequences of sets,
an arbitrary measure μ on (S, S ) has certain continuity properties, as detailed in the
next exercise.
σ(A ) contains the open sets. (Why?) To do so, use the fact that every open subset of can be expressed as
the countable union of open intervals.
17 In this sense (1) is just to rule out trivial cases, and (2) is the real condition of interest.

Integration

167

Exercise 7.1.25 Let ( An )n≥1 be a sequence in S . Show that
1. if An ↑ A, then μ( An ) ↑ μ( A),18 and
2. if μ( A1 ) < ∞ and An ↓ A, then μ( An ) ↓ μ( A).19
Example 7.1.26 Consider the measurable space ( , P( )) formed by the set of natural numbers
and the set of all its subsets. On this measurable space deﬁne the
counting measure c, where c( B) is simply the number of elements in B, or +∞ if B is
inﬁnite. With some thought you will be able to convince yourself that c is a measure
on ( , P( )).20
Exercise 7.1.27 Let ( an ) ⊂ + . Let μ be deﬁned on ( , P( )) by μ( A) = ∑n∈ A an .
Show that μ is a measure on ( , P( )).
Let’s now specialize this to “probability measures,” which are the most important
kind of measures for us after Lebesgue measure. In what follows (S, S ) is any measurable space.
Deﬁnition 7.1.28 A probability measure μ is a measure on (S, S ) such that μ(S) = 1.
The triple (S, S , μ) is called a probability space. The set of all probability measures on
(S, S ) is denoted by P (S, S ). When S is a metric space, elements of P (S, B (S)) are
called Borel probability measures. For brevity we will write P (S, B (S)) as P (S).
In the context of probability theory, a set E in S is usually called an event, and
μ( E) is interpreted as the probability that when uncertainty is realized the event E
occurs. Informally, μ( E) is the probability that x ∈ E when x is drawn from the set S
according to μ. The empty set ∅ is called the impossible event, and S is called the certain
event.
Deﬁning μ on a σ-algebra works well in terms of probabilistic intuition. For example, the impossible and certain events are always in S . Also, we want Ec to be in S
whenever E is: If the probability of E occurring is deﬁned, then so is the probability
of E not occurring. And if E and F are in S , we want E ∩ F ∈ S so that we can
talk about the probability that both E and F occur, and so forth. These properties are
assured by the deﬁnition of σ-algebras.
Given measurable space (S, S ) and point x ∈ S, the Dirac probability measure δx ∈
P (S, S ) is the distribution that puts all mass on x. More formally, δx ( A) = A ( x ) for
all A ∈ S .
Exercise 7.1.29 Conﬁrm that δx is a probability measure on (S, S ).
18 Hint: Let B = A and B = A \ A
k
n
n
1
1
n−1 for n ≥ 2. Show that ( Bn ) is a disjoint sequence with ∪n=1 Bn =
Ak and ∪n Bn = A. Now apply countable additivity.
19 Hint: Use part 1 of the exercise.
20 When we come to integration, series of real numbers will be presented as integrals of the counting
measure. Repackaging summation as integration may not sound very useful, but actually it leads to a
handy set of results on passing limits through inﬁnite sums.

168

7.1.4

Chapter 7

Existence of Measures

Suppose that we release at time zero a large number of identical, tiny particles into
water at a location deﬁned as zero, and then measure the horizontal distance of the
particles from the origin at later point in time t. As Albert Einstein pointed out, the
independent action of the water molecules on the particles and the central limit theorem tell us that, at least in this idealized setting, and at least for E = ( a, b] ∈ J , the
fraction of the total mass contained in E should now be approximately
μ( E) = μ(( a, b]) =

 b
a

√

1
2πt

exp

− x2
dx
2t

(7.4)

One can think of μ( E) as the probability that an individual particle ﬁnds itself in E at
time t.
In (7.4) we have a way of computing probabilities for intervals ( a, b] ∈ J , but
no obvious way of measuring the probability of more complex events. For example,
what is μ( ), where is the rational numbers? Measuring intervals is all well and
good, but there will certainly be times that we want to assign probabilities to more
complex sets. In fact we need to do this to develop a reasonable theory of integration.
How to extend μ from J to a larger class of subsets of ? Taking our cue from
the process for Lebesgue measure, we could assign probability to an arbitrary set A
by setting
μ∗ ( A) := inf

∑

n ≥1

μ( In ) = inf

∑

 bn

n ≥1 a n

√

1
2πt

exp

− x2
dx
2t

(A ⊂

)

(7.5)

where the inﬁmum is over all sequences of intervals ( In )n≥1 , with In := ( an , bn ] ∈ J
for each n, and with the sequence covering A (i.e., ∪n In ⊃ A). This extension would
be suitable if (1) it agrees with μ on J , and (2) it is a measure, at least when restricted
to a nice subset of P( ) such as B ( ). Being a measure implies attractive properties
such as nonnegativity, monotonicity and additivity.21
Instead of dealing directly with μ, let’s look at this extension process in an abstract
setting. As with all abstraction, the advantage is that we can cover a lot of cases in one
go. The disadvantage is that the statement of results is quite technical. It is provided
mainly for reference purposes, rather than as material to be worked through step by
step. The idea is to formulate a system for constructing measures out of set functions
that behave like measures (i.e., are countably additive) on small, concrete classes of
sets called semi-rings.
21 For probabilities, monotonicity should be interpreted as follows: A ⊂ B means that whenever A happens, B also happens. In which case B should be at least as likely to occur as A, or μ∗ ( A) ≤ μ∗ ( B).
Additivity is familiar from elementary probability. For example, the probability of getting an even number
when you roll a dice is the probability of getting a 2 plus that of getting a 4 plus that of getting a 6. (Note
that monotonicity is implied by nonnegativity and additivity—see exercise 7.1.23).

Integration

169

Deﬁnition 7.1.30 Let S be a nonempty set. A nonempty collection of subsets R is
called a semi-ring if, given arbitrary sets I and J in R, we have
1. ∅ ∈ R,
2. I ∩ J ∈ R, and
3. I \ J can be expressed as a ﬁnite union of elements of R.
The deﬁnition is not particularly attractive, but all we need to know at this stage
is that J , the half-open rectangles ×ik=1 ( ai , bi ] in k , form a semi-ring. Although the
proof is omitted, a little thought will convince you that J is a semi-ring when k = 1.
You might like to give a sketch of the proof for the case of 2 by drawing pictures.
We now give a general result for existence of measures. Let S be any nonempty
set, and let R be a semi-ring on S. A set function μ : R → [0, ∞] is called a pre-measure
on R if μ(∅) = 0 and μ(∪n In ) = ∑n μ( In ) for any disjoint sequence ( In )n≥1 ⊂ R with
∪n In ∈ R. For any A ⊂ S, let C A be the set of all countable covers of A formed from
elements of R. That is,
C A := {( In )n≥1 ⊂ R : ∪n In ⊃ A}
Now deﬁne the outer measure generated by μ as

∗

μ ( A) := inf

∑ μ( In )

n ≥1

: ( In )n≥1 ∈ C A



( A ∈ P(S))

(7.6)

The restriction of μ∗ to σ(R ) turns out to be a measure, and is typically denoted simply
by μ. Formally,
Theorem 7.1.31 Let S be any nonempty set and let R be a semi-ring on S. If μ is a premeasure on R, then the outer measure (7.6) agrees with μ on R and is a measure on (S, σ (R )).
If there exists a sequence ( In ) ⊂ R with ∪n In = S and μ( In ) < ∞ for all n, then the
extension is unique in the sense that if ν is any other pre-measure that agrees with μ on R,
then its extension agrees with μ on all of σ (R ).
The proof is quite similar to the construction of Lebesgue measure sketched in
§7.1.1. First one deﬁnes the outer measure μ∗ by (7.6). Since μ∗ is not necessarily
additive over all of P(S) (think of the case of Lebesgue measure), we then restrict
attention to those sets S that satisfy Carathéodory’s condition: All A ∈ P(S) such
that
μ∗ ( B) = μ∗ ( B ∩ A) + μ∗ ( B ∩ Ac ) for all B ∈ P(S)
(7.7)
It can be proved that S is a σ-algebra containing R, and that μ∗ is a measure on S .
Evidently σ (R ) ⊂ S (why?), and the restriction of μ∗ to σ (R ) is simply denoted by
μ.

170

Chapter 7

Let’s consider applications of theorem 7.1.31. One is Lebesgue measure on k . For
the semi-ring we take J . It can be shown that  deﬁned on J by (7.1) on page 160 is
a pre-measure. As a result  extends uniquely to a measure on σ (J ) = B ( k ). This
gives us Lebesgue measure on B ( k ).
A second application is probabilities on , such as the Gaussian probability deﬁned in (7.4). Recall that F :
→
is called a cumulative distribution function on
if it is nonnegative, increasing, right-continuous, and satisﬁes limx→−∞ F ( x ) = 0
and limx→∞ F ( x ) = 1. We imagine F ( x ) represents the probability that random variable X takes values in (−∞, x ]. More generally, for interval ( a, b] the probability that
X ∈ ( a, b] is given by F (b) − F ( a).
Fix any distribution function F, and let J be the semi-ring of all intervals ( a, b],
where a ≤ b. Although the proof is not trivial, one can show using the properties of F
that μ F : J → + deﬁned by
μ F (( a, b]) = F (b) − F ( a)
is a pre-measure on J . Clearly, there exists a sequence ( In ) ⊂ J with ∪n In = and
μ F ( In ) < ∞ for all n. As a result there exists a unique extension of μ F to a measure
on σ (J ) = B ( ) with μ F ( A) := inf ∑n≥1 ( F (bn ) − F ( an )) for all A ∈ B ( ). The
inﬁmum is over all sequences of intervals ( In ) with In := ( an , bn ] ∈ J for each n and
∪n In ⊃ A.
It follows that to each cumulative distribution function on there corresponds a
unique Borel probability measure. Conversely, suppose that μ ∈ P ( ), and let F be
deﬁned by F ( x ) = μ((−∞, x ]) for x ∈ .
Exercise 7.1.32 Show that F is a cumulative distribution function on

.

Putting this together and ﬁlling in some details, one can show that
Theorem 7.1.33 There is a one-to-one pairing between the collection of all distribution functions on and P ( ), the set of all Borel probability measures on . If F is a distribution
function, then the corresponding probability μ F satisﬁes
μ F ((−∞, x ]) = F ( x )

(x ∈

)

Let’s look back and see what we have accomplished. The main result here is theorem 7.1.31, which helps us construct measures. To understand it’s importance, suppose that we propose a would-be measure such as (7.4). To check that this is indeed
a measure on the Borel sets is a tough ask. After all, what does an arbitrary Borel set
b
2
look like? But to show that μ(( a, b]) = a (2πt)−1/2 e− x /2t dx is a pre-measure on the
nice semi-ring J of intervals ( a, b] is much easier. Once this is done theorem 7.1.31
can be applied.

Integration

7.2

171

Deﬁnition of the Integral

Elementary calculus courses use the Riemann deﬁnition of integrals. As a result of its
construction the Riemann integral is inherently limited, in terms of both its domain
of deﬁnition and its ability to cope with limiting arguments. We want to construct an
integral that extends the Riemann integral to a wider domain, and has nice analytical
properties to boot. With this goal in mind, let’s start to develop a different theory of
integration (the Lebesgue theory), beginning with the case of functions from to ,
and working up to more abstract settings.

7.2.1

Integrating Simple Functions

Let’s start with the easiest case. A simple function is any real-valued function taking
only ﬁnitely many different values. Consider a simple function s :
→ that takes
values α1 , . . . , α N on a corresponding disjoint intervals I1 , . . . , IN in J . Exploiting the
assumption that the intervals are disjoint, the function s can be expressed as a linear
combination of indicator functions: s = ∑nN=1 αn In .22 Take a moment to convince
yourself of this.
Since our integral is to be an extension of the Riemann integral, and since the Riemann integral of s is well deﬁned and equal to the sum over n of αn times the length
of In , the Lebesgue integral must also be
λ(s) :=:



sdλ :=

N

N

n =1

n =1

∑ αn (bn − an ) = ∑ αn λ( In )

(7.8)

where λ on the right-hand
side is the Lebesgue measure.


The symbol sdλ is reminiscent of the traditional notation s( x )dx. Using dλ
reminds us that we are integrating with respect to Lebesgue measure. Later, more
general integrals are deﬁned. The alternative notation λ(s) for the integral of function
s with respect to measure λ is also common. It reminds us that we are deﬁning a map
that sends functions into numbers.
Little effort is needed to shift our theory up from to k . For a function s : k →
deﬁned by s = ∑nN=1 αn In , where each In = ×ik=1 ( ai , bi ] is an element of J ⊂
P( k ) and the rectangles are disjoint, we set
λ(s) :=:



sdλ :=

N

∑ αn λ( In )

Here λ on the right-hand side is Lebesgue measure on
ple of such a function s on 2 .
22 In

other words, s( x ) = ∑nN=1 αn

In ( x )

for every x ∈

(7.9)

n =1

, where

k.

In ( x )

Figure 7.3 shows an exam= 1 if x ∈ In and zero otherwise.

172

Chapter 7

Figure 7.3 A simple function on the plane

Having deﬁned an integral for simple functions that are constant on rectangles,
the next step is to extend the deﬁnition to the B ( k )-simple functions sB ( k ), each
of which takes only ﬁnitely many values, but on Borel sets rather than just rectangles.
More succinctly, sB ( k ) is all functions of the form ∑nN=1 αn Bn , where the Bn ’s are
disjoint Borel sets. For now let’s think about nonnegative simple functions (αn ≥ 0 for
all n), the set of which we denote sB ( k )+ . A natural extension of our integral (7.9)
to sB ( k )+ is given by
λ(s) :=:



sdλ :=

N

∑ αn λ( Bn )

(7.10)

n =1

This is already a generalization of the Riemann integral. For example, the Riemann
integral is not deﬁned for
, which is an element of sB ( )+ . Note also that λ(s) =
∞ is a possibility, and we do not exclude this case.
Exercise 7.2.1 Explain why the integral of

is zero.

So far we have deﬁned integrals of (ﬁnite-range) functions deﬁned over k , where
integration was with respect to Lebesgue measure. Next, just as we abstracted from
Lebesgue measure to arbitrary measures, let us now introduce integrals of simple
functions using general measures.
Suppose that we have a measure μ on an arbitrary measurable space (S, S ). We
can deﬁne the real-valued simple functions sS on (S, S ) in the same way that we

Integration

173

deﬁned the Borel simple functions sB ( k ) on k , replacing B ( k ) with S in the
deﬁnition. In other words, sS is those functions of the form s = ∑nN=1 αn An , where
the sets A1 , . . . , A N are disjoint and An ∈ S for all n. The set sS + is the nonnegative
functions in sS .
By direct analogy with (7.10), the integral of s ∈ sS + is deﬁned as
μ(s) :=:



sdμ :=

N

∑ αn μ( An )

(7.11)

n =1

To give an illustration of (7.11), consider an experiment where a point ω is selected
from some set Ω according to probability measure . Here is deﬁned on some σalgebra F of subsets of Ω, and ( E) is interpreted as the probability that ω ∈ E for
each E ∈ F . Suppose that we have a discrete random variable X taking ω ∈ Ω and
sending it into one of N values. Speciﬁcally, X sends points in An ∈ F into αn ∈ ,
where A1 , . . . , A N is a partition of Ω. Intuitively, the expectation of X is then
N

N

N

n =1

n =1

n =1

∑ αn Prob{X = αn } = ∑ αn Prob{ω ∈ An } = ∑ αn

( An )

Comparing the right-hand side of this expression with
(7.11), it becomes clear that the

expectation of X is precisely the integral ( X ) :=: Xd . A more traditional notation
is X. We will come back to expectations later on.
Returning to general (S, S , μ), integrals of simple functions have some useful
properties.
Proposition 7.2.2 For s, s ∈ sS + and γ ≥ 0, the following properties hold:
1. γs ∈ sS + and μ(γs) = γμ(s).
2. s + s ∈ sS + and μ(s + s ) = μ(s) + μ(s ).
3. If s ≤ s pointwise on S, then μ(s) ≤ μ(s ).
We say that on sS + the integral μ is positive homogeneous, additive, and monotone respectively. The full proofs are a bit messy and not terribly exciting so we omit
them.
Exercise 7.2.3 Prove part 1 of proposition 7.2.2. Prove parts 2 and 3 in the special case
where s = α A and s = β B .

7.2.2

Measurable Functions

So far we have extended the integral to sS + . This is already quite a large class of
functions. The next step is to extend it further by a limiting operation (a method of

174

Chapter 7

deﬁnition so common in analysis!). To do this, we need to deﬁne a class of functions
that can be approximated well by simple functions. This motivates the deﬁnition of a
measurable function:
Deﬁnition 7.2.4 Let (S, S ) and ( R, R ) be two measurable spaces, and let f : S → R.
The function f is called S , R-measurable if f −1 ( B) ∈ S for all B ∈ R. If ( R, R ) =
( , B ( )), then f is called S -measurable. If, in addition, S is a metric space and
S = B (S), then f is called Borel measurable.
While this deﬁnition is very succinct, it is also rather abstract, and the implications
of measurability are not immediately obvious. However, we will see that—for the
kinds of functions we want to integrate—measurability of a function f is equivalent
to the existence of a sequence of simple functions (sn )n≥1 that converges to f in a
suitable way (see lemma 7.2.11 below). We will then be able to deﬁne the integral of f
as the limit of the integrals of the sequence (sn )n≥1 .
Exercise 7.2.5 Show that if (S1 , S1 ), (S2 , S2 ) and (S3 , S3 ) are any three measurable
spaces, f : S1 → S2 is S1 , S2 -measurable and g : S2 → S3 is S2 , S3 -measurable, then
h := g ◦ f : S1 → S3 is S1 , S3 -measurable.
With measure theory the notation keeps piling up. Here is a summary of the notation we will use for functions from (S, S ) into :
• mS is deﬁned to be the S -measurable functions on S,
• mS + is deﬁned to be the nonnegative functions in mS , and
• bS is deﬁned to be the bounded functions in mS .
Exercise 7.2.6 Let S be any set. Argue that every f : S →
only the constant functions are {S, ∅}-measurable.

is P(S)-measurable, while

Exercise 7.2.7 Let (S, S ) be any measurable space. Show that sS ⊂ mS .
The following lemma is very useful when checking measurability. The proof is
typical of measure-theoretic arguments.
Lemma 7.2.8 Let ( E, E ) and ( F, F ) be two measurable spaces, and let f : E → F. Let G be
a generator of F , in the sense that σ (G ) = F . Then f is E , F -measurable if and only if
f −1 ( B) ∈ E for all B ∈ G .
Proof. Necessity is obvious. Regarding sufﬁciency, let
M : = { B ∈ F : f −1 ( B ) ∈ E }

Integration

175

It is left to the reader to verify that M is a σ-algebra containing G .23 But then F =
σ (G ) ⊂ σ(M ) = M . (Why?) Hence F ⊂ M , which is precisely what we wish to
show.
In other words, to check measurability of a function, we need only check measurability
on a generating class. For example, to verify measurability of a function into ( , B ( )),
we need only check that the preimages of open sets are measurable. (Why?)
Exercise 7.2.9 Let S be any metric space. Show that if f : S →
is Borel measurable (i.e., in mB (S)).24

is continuous, then it

In fact one can show that families such as

[ a, b] with a ≤ b,

( a, ∞) with a ∈

,

(−∞, b] with b ∈

all generate B ( ). So for f : S → to be in S -measurable (given σ-algebra S on S),
it is sufﬁcient that, for example, { x ∈ S : f ( x ) ≤ b} ∈ S for all b ∈ . To get a feel for
why this is useful consider the next example:
Example 7.2.10 Let (S, S ) be a measurable space, and let ( f n ) ⊂ mS . If f : S →
is a function satisfying f ( x ) = supn f n ( x ) for x ∈ S, then f ∈ mS because, given any
b ∈ , we have { x ∈ S : f ( x ) ≤ b} = ∩n { x ∈ S : f n ( x ) ≤ b} ∈ S .25
When deﬁning integrals of measurable functions, our approach will be to approximate them with simple functions, which we saw how to integrate in §7.2.1. While the
deﬁnition of measurability is rather abstract, it turns out that functions are measurable
precisely when they can be well approximated by simple functions. In particular,
Lemma 7.2.11 A function f : S → + is S -measurable if and only if there is a sequence
(sn )n≥1 in sS + with sn ↑ f pointwise on S.
That the existence of such an approximating sequence is sufﬁcient for measurability follows from example 7.2.10. (Why?) Let’s sketch the proof of necessity in the case
of S = and S = B ( ). Figure 7.4 might help with intuition. In this case the function f is bounded above by c. The range space [0, c] is subdivided into the intervals
[0, a), [ a, b), and [b, c]. Using f , this partition also divides the domain (the x-axis) into
the sets f −1 ([0, a)), f −1 ([ a, b)) and f −1 ([b, c]). We can now deﬁne a simple function s
by
s = 0 × f −1 ([0,a)) + a × f −1 ([a,b)) + b × f −1 ([b,c])
23 Hint:

See lemma A.1.2 on page 321.
Use theorem 3.1.33 on page 43.
25 Below we often write { g ≤ b } for the set { x ∈ S : g ( x ) ≤ b }.
24 Hint:

176

Chapter 7

c

f

b

s

a

f −1 ([0, a))

f −1 ([ a, b))

f −1 ([b, c])

Figure 7.4 A measurable function

Notice that as drawn, s ∈ sB ( )+ , because s takes only ﬁnitely many values on
ﬁnitely many disjoint sets, and these sets f −1 ([0, a)), f −1 ([ a, b)), and f −1 ([b, c]) are all
intervals, which qualiﬁes them as members of B ( ). Notice also that s lies below f .
By looking at the ﬁgure, you can imagine that if we reﬁne our partition of the range
space, we would get another function s that dominates s but still lies below f , and
is again an element of sB ( )+ . Continuing in this way, it seems that we can indeed
approximate f from below by an increasing sequence of elements of sB ( )+ .
The function f we chose was a bit special—in particular, it is increasing, which
means that the sets f −1 ([0, a)), f −1 ([ a, b)) and f −1 ([b, c]) are intervals, and therefore elements of B ( ). Were they not elements of B ( ), we could not say that
s ∈ sB ( )+ . This is where the deﬁnition of Borel measurability comes in. Even if
f is not increasing, we require in lemma 7.2.11 that it is at least Borel measurable. In
which case sets like f −1 ([ a, b)) are always elements of B ( ), because [ a, b) is a Borel
set. As a result the approximating simple functions are always in sB ( )+ .
For arbitrary (S, S ), elements of mS play nicely together, in the sense that when
standard algebraic and limiting operations are applied to measurable functions the
resulting functions are themselves measurable:
Theorem 7.2.12 If f , g ∈ mS , then so is α f + βg for any α, β ∈ . The product f g is
also in mS . If ( f n )n≥1 is a sequence in mS with f n → f pointwise, where f : S → , then
f ∈ mS . If f ∈ mS , then | f | ∈ mS .

Integration

177

Exercise 7.2.13 Show that if f ∈ mS , then | f | ∈ mS .26

7.2.3

Integrating Measurable Functions

Now we are ready to extend our notion of integral from simple functions to measurable functions. Let (S, S , μ) be any measure space and consider ﬁrst integration of a
nonnegative measurable function f : S → + (i.e., an element of mS + ). We deﬁne the
integral of f on S with respect to μ by
μ( f ) :=:



f dμ := lim μ(sn ) where (sn )n≥1 ⊂ sS + with sn ↑ f
n→∞

(7.12)

We are appealing to lemma 7.2.11 for the existence of at least one sequence (sn )n≥1 ⊂
sS + with sn ↑ f . Note that μ(sn ) always converges in [0, ∞] as a result of monotonicity (see proposition 7.2.2).
Regarding notation, all of the following are common alternatives for the integral
μ ( f ):


μ( f ) :=:

f ( x )μ(dx )

In the case of Lebesgue measure we will also use f ( x )dx:
λ( f ) :=:



f dμ :=:

f dλ :=:



f ( x )λ(dx ) :=:



f ( x )dx

Isn’t it possible that the number we get in (7.12) depends on the particular approximating sequence (sn )n≥1 that we choose? The answer is no. If (sn )n≥1 and (sn )n≥1
are two sequences in sS + with sn ↑ f and sn ↑ f , then μ(sn ) and μ(sn ) always have
the same limit.27 The number given by taking any of these limits is in fact equal to


(7.13)
sup μ(s) : s ∈ sS + , 0 ≤ s ≤ f ∈ [0, ∞]
Example 7.2.14 Recall the counting measure c on ( , P( )) introduced on page 167.
A nonnegative function f :
→ + is just a nonnegative sequence, and we emphasize this by writing f as ( f n ). Since
is paired with its power set (the set of
all subsets), there is no issue with measurability—all such functions (sequences) are
measurable.
The function ( f n ) is simple if it takes only ﬁnitely many values. Suppose in particular that f n = 0 for all n ≥ N ∈ . Then, by the deﬁnition of the integral on simple
functions,
c( f ) :=:



f dc =

N

∑

n =1

f n c(n) =

N

∑

n =1

fn

Use the fact that g ∈ mS whenever { g ≤ b} ∈ S for all b ∈ .
reason that the approximating sequence in (7.12) is required to be monotone is to ensure that this
independence holds. The proof is not difﬁcult but let’s take it as given.
26 Hint:
27 The

178

Chapter 7

so integration with respect to c is equivalent to summation.
Now consider the case of a general nonnegative sequence f = ( f n ). For simple
functions converging up to f we can take f N := ( f nN ), which is deﬁned as equal to f n
if n ≤ N and to zero if n > N. In light of (7.12) we have
c( f ) :=:



f dc = lim



N →∞

N

∑
N →∞

f N dc = lim

n =1

fn

which is the standard deﬁnition of the inﬁnite series ∑n f n . Again, integration with
respect to c is equivalent to summation.
So far we have only deﬁned the integral of nonnegative measurable functions.
Integration of general measurable functions is also straightforward: Split the function
f into its positive part f + := max{0, f } and its negative part f − := max{0, − f }, so
f = f + − f − . Then set
μ( f ) := μ( f + ) − μ( f − )
(7.14)
The only issue here is that we may end up with the expression ∞ − ∞, which is deﬁnitely not allowed—in this case the integral is not deﬁned.
Deﬁnition 7.2.15 Let (S, S , μ) be a measure space, and let f ∈ mS . The function f is
called integrable if both μ( f + ) and μ( f − ) are ﬁnite. If f is integrable, then its integral
μ( f ) is given by (7.14). The set of all integrable functions on S is denoted L1 (S, S , μ),
or simply L1 (μ).
How do we deﬁne integration of a function f ∈ L1 (μ) over a subset E of S, rather
than over the whole space? The answer is by setting


E

f dμ :=



E f dμ

:=: μ(

Ef)

Here the function E f evaluated at x is the product E ( x ) · f ( x ).
Although we omit the proof, if f is a continuous real function on
with the
property that f = 0 outside an interval [ a, b]—so that the Riemann integral is welldeﬁned—then λ( f ) is precisely the Riemann integral of f on [ a, b]. We can go ahead
and integrate f as our high school calculus instinct tells us to. Hence we have succeeded in extending the elementary integral to a larger class of functions.

7.3

Properties of the Integral

Having deﬁned the abstract Lebesgue integral, let’s now look at some of its properties. As we will see, the integral has nice algebraic properties, and also interacts well
with limiting operations. Section §7.3.1 focuses on nonnegative functions, while §7.3.2
treats the general case.

Integration

7.3.1

179

Basic Properties

Recall that functions constructed from measurable functions using standard algebraic
and limiting operations are typically measurable (theorem 7.2.12). This leads us to
consider the relationship between the integrals of the original functions and the integrals of the new functions. For example, is the integral of the sum of two measurable
functions equal to the sum of the integrals? And is the integral of the limit of measurable functions equal to the limit of the integrals? Here is a summary of the key
results:
Theorem 7.3.1 Given an arbitrary measure space (S, S , μ), the integral has the following
properties on mS + :
M1. If A ∈ S and f :=
M2. If f =

∅

A,

then μ(

A)

= μ ( A ).

≡ 0, then μ( f ) = 0.

M3. If f , g ∈ mS + and α, β ∈

+,

then μ(α f + βg) = αμ( f ) + βμ( g).

M4. If f , g ∈ mS + and f ≤ g pointwise on S, then μ( f ) ≤ μ( g).
M5. If ( f n )n≥1 ⊂ mS + , f ∈ mS + and f n ↑ f , then μ( f n ) ↑ μ( f ).
Property M1 is immediate from the deﬁnition of the integral, and M2 is immediate
from M1. Properties M3 and M4 can be derived as follows:
Exercise 7.3.2 Using proposition 7.2.2 (page 173), show that if γ ∈ + and f ∈ mS + ,
then μ(γ f ) = γμ( f ). Show further that if f , g ∈ mS + , then μ( f + g) = μ( f ) + μ( g).
Combine these two results to establish M3.
Exercise 7.3.3 Prove M4.28
Property M5 is fundamental to the success of Lebesgue’s integral, and is usually
referred to as the monotone convergence theorem (although we give a more general
result with that name below). Note that μ( f ) = ∞ is permitted. The proof of M5 is
based on countable additivity of μ, and is one of the reasons that countable additivity
(as opposed to ﬁnite additivity) is so useful. You can consult any book on measure
theory and integration to see the proof.
The next result clariﬁes the relationship between measures and integrals.
B →
Theorem 7.3.4 Let (S, S ) be any measurable space. For each measure μ : S
μ( B) ∈ [0, ∞], there exists a function μ : mS +
f → μ( f ) ∈ [0, ∞] with properties M1–
M5. Conversely, each function μ : mS + → [0, ∞] satisfying properties M2–M5 creates a
unique measure on (S, S ) via M1.
28 Hint:

Use the expression for the integral in (7.13).

180

Chapter 7

One can think of a measure μ on (S, S ) as a kind of “pre-integral,” deﬁned on the
subset of mS + that consists of all indicator functions (the integral of A being μ( A)).
The process of creating an integral on mS + via simple functions and then monotone
limits can be thought of as one that extends the domain of μ from indicator functions in
mS + to all functions in mS + .
Exercise 7.3.5 Show that if μ : mS + → [0, ∞] satisﬁes M2–M5, then the map μ̂ : S →
[0, ∞] deﬁned by μ̂( A) = μ( A ) is a measure on S .29
Exercise 7.3.6 Use M1–M5 to prove the previously stated result (see exercise 7.1.25 on
page 167) that if ( En ) ⊂ S with En ⊂ En+1 for all n, then μ(∪n En ) = limn→∞ μ( En ).30
Lemma 7.3.7 Let A, B ∈ S , and let f ∈ mS + . If A and B are disjoint, then


f dμ =

A∪ B

Proof. We have

A∪ B f

=(

A

+

B) f

=



f dμ +

A
Af

+


B

B f.

f dμ

Now apply M3.

One of the most important facts about the integral is that integrating over sets of
zero measure cannot produce a positive number:

Theorem 7.3.8 If f ∈ mS + , E ∈ S , and μ( E) = 0, then E f dμ = 0.
Proof. Since f ∈ mS + , there is a sequence (sn )n≥1 ⊂ sS + with sn ↑ f , and hence
31
E sn ↑ E f . But
E sn

∴

μ(

=

K

∑ αk (

k =1

E sn ) =

E

·

Ak )

K

K

∑ αk

k =1

∑ αk μ( E ∩ Ak ) = 0

k =1

since μ( E) = 0. By property M5, we have μ(

7.3.2

=

Ef)

E∩ Ak

∀n ∈

= limn→∞ μ(

E sn )

= 0.

Finishing Touches

Let (S, S , μ) be any measure space. By using the ﬁve fundamental properties M1–M5,
one can derive the classical theorems about integrals on L1 (μ) := L1 (S, S , μ). The
next few results show that many results from the previous section that hold for nonnegative functions also hold for the (not necessarily nonnegative) elements of L1 (μ).
Regarding countable additivity, if ( An ) ⊂ S are disjoint then ∪n An = ∑n An .
Use the fact that ∪n En = limn→∞ En pointwise on S.
31 That is, the convergence
E ( x ) sn ( x ) → E ( x ) sn ( x ) holds at each point x ∈ S, and that
progressively larger with n for each x ∈ S.
29 Hint:
30 Hint:

E ( x )sn ( x )

gets

Integration

181

To state the results we introduce the concept of properties holding “almost everywhere.” Informally, if f , g ∈ mS and P( x ) is a statement about f and g at x (e.g.,
f ( x ) = g( x ) or f ( x ) ≤ g( x )), then we will stay f and g have property P μ-almost
everywhere (μ-a.e.) whenever the set of all x such that P( x ) fails has μ-measure zero.
For example, if the set of x ∈ S such that f ( x ) = g( x ) has measure zero then f and g
are said to be equal μ-a.e. In addition we say that f n → f μ-a.e. if lim f n = f μ-a.e.
This sounds a bit complicated, but the basic idea is that null sets don’t matter when
it comes to integration, so it’s enough for properties to hold everywhere except on a
null set.
Theorem 7.3.9 Let f , g ∈ L1 (μ), and let α, β ∈

. The following results hold:

1. α f + βg ∈ L1 (μ) and μ(α f + βg) = αμ( f ) + βμ( g).

2. If E ∈ S with μ( E) = 0, then E f dμ = 0.
3. If f ≤ g μ-a.e., then μ( f ) ≤ μ( g).
4. | f | ∈ L1 (μ), and |μ( f )| ≤ μ(| f |).
5. μ(| f |) = 0 if and only if f = 0 μ-a.e.
This list is not minimal. For example, part 2 follows from parts 4 and 5. Part
1 follows from the deﬁnitions and M3 (i.e., linearity of the integral on the space of
nonnegative measurable functions). See, for example, Dudley (2002, thm. 4.1.10). Part
2 can also be obtained from the identity f = f + − f − , part 1 and theorem 7.3.8:
μ(

Ef)

= μ(

Ef

+

−

Ef

−

) = μ(

Ef

+

) − μ(

Ef

Exercise 7.3.10 Prove that if μ( E) = 0 and f ∈ L1 (μ), then
f , g ∈ L1 (μ) with f = g μ-a.e., then μ( f ) = μ( g).32

−



) = 0−0

Ec

f dμ =



f dμ, and if

Exercise 7.3.11 Prove part 3 using properties M1–M5.33
Exercise 7.3.12 Prove part 4 using the identity | f | = f + + f − .
Regarding part 5, suppose that f = 0 on a set E with μ( E) > 0. We will prove that
μ(| f |) > 0. To see this, deﬁne En := { x : | f ( x )| > 1/n}. Observe that En ⊂ En+1
for each n, and that E = ∪n En . From exercise 7.1.25 (page 167) there is an N with
μ( EN ) > 0. But then μ(| f |) ≥ μ( EN | f |) ≥ μ( EN )/N > 0. The converse implication
follows from exercise 7.3.10.
Now we come to the classical convergence theorems for integrals, which can be
derived from M1–M5. They are among the foundation stones of modern real analysis.
Let E be the set on which f and g disagree. Then Ec f = Ec g.
Convert to a statement about nonnegative functions. Note that if f ≤ g μ-a.e., then
−
+
−
Ec g ≤ Ec g + Ec f , where E is all x such that f ( x ) > g ( x ).
32 Hint:
33 Hint:

Ec

f+ +

182

Chapter 7

Theorem 7.3.13 (Monotone convergence theorem) Let (S, S , μ) be a measure space, and
let ( f n )n≥1 be a sequence in mS . If f n ↑ f ∈ mS μ-almost everywhere and μ( f 1 ) > −∞,
then limn→∞ μ( f n ) = μ( f ).34
Theorem 7.3.14 (Dominated convergence theorem) Let (S, S , μ) be a measure space, let
g ∈ L1 (μ) and let ( f n )n≥1 ⊂ mS with | f n | ≤ g for all n. If f n → f μ-almost everywhere,
then f ∈ L1 (μ) and limn→∞ μ( f n ) = μ( f ).
It’s almost impossible to overemphasize what a useful result the dominated convergence theorem is, and the proof can be found in any text on measure theory. For a
neat little illustration, consider
Corollary 7.3.15 Consider the collection of real sequences
a = ( a1 , a2 , . . . ),

ak = ( a1k , a2k , . . .)

(k ∈

)

Suppose that
is dominated pointwise by a sequence b = (bn ) for all k, in the sense that
| akn | ≤ bn for all k, n. Suppose further that limk→∞ akn = an for each n. If ∑n bn < ∞, then
ak

lim

akn = ∑ an
∑ akn = ∑ klim
→∞

k → ∞ n ≥1

n ≥1

n ≥1

Proof. Apply the dominated convergence theorem with (S, S , μ) = ( , P( ), c),
where c is the counting measure (recall example 7.2.14).
Let’s conclude with the topic of image measures. To deﬁne image measures, let
(S, S , μ) be any measure space, let (S , S ) be a measurable space, and let T : S → S
be S , S -measurable. If E is some element of S , then T −1 ( E) ∈ S , so μ ◦ T −1 ( E) =
μ( T −1 ( E)) is well deﬁned. In fact E → μ ◦ T −1 ( E) is a measure on (S , S ), called the
image measure of μ under T. Figure 7.5 provides a picture. The value of μ ◦ T −1 ( E) is
obtained by pulling E back to S and evaluating with μ.
Exercise 7.3.16 Show that μ ◦ T −1 is indeed a measure on (S , S ).35
The following result shows how to integrate with image measures:36
Theorem 7.3.17 Let (S, S , μ) be any measure space, let (S , S ) be a measurable space, let
T : S → S be a measurable function, and let μ ◦ T −1 be the image measure of μ under T.
If w : S →
is S -measurable and either w is nonnegative or μ(|w ◦ T |) is ﬁnite, then
μ ◦ T −1 (w) = μ(w ◦ T ), where the ﬁrst integral is over S and the second is over S.
One application of this theorem is when μ is Lebesgue measure, and μ ◦ T −1 is
more complex. If we don’t know how to integrate with respect to this new
measure,

we can use the change of variable to get back to an integral of the form f dμ.
34 In

fact, for this theorem to hold the limiting function f need not be ﬁnite everywhere (or anywhere) on
S. See Dudley (2002, thm. 4.3.2).
35 You might ﬁnd it useful to refer to lemma A.1.2 on page 321.
36 A full proof can be found in Dudley (2002, thm. 4.1.11).

Integration

183
T

S

S

E

T −1 ( E )

μ lives here

μ ◦ T −1 lives here

Figure 7.5 Image measure

7.3.3

The Space L1

In this section we specialize to the case (S, S , μ) = (S, B (S), λ), where S is a Borel
subset of k . Most of the results we discuss hold more generally, but such extra generality is not needed here. Our interest is in viewing the space of integrable functions as
a metric space. To this end, we deﬁne the “distance” d1 on L1 (λ) := L1 (S, B (S), λ)
by

d1 ( f , g ) : =

| f − g|dλ :=: λ(| f − g|)

(7.15)

Alternatively, we can set
d1 ( f , g ) : =  f − g 1

where

h1 := λ(|h|)

From the pointwise inequalities | f − g| ≤ | f | + | g| and | f + g| ≤ | f | + | g| plus linearity
and monotonicity of the integral, we have

 f − g 1 ≤  f 1 +  g 1

and

 f + g 1 ≤  f 1 +  g 1

The ﬁrst inequality tells us that d1 ( f , g) is ﬁnite for any f , g ∈ L1 (λ). From the second
we can show that d1 satisﬁes the triangle inequality on L1 (λ) using add and subtract:

 f − g1 = ( f − h) + (h − g)1 ≤  f − h1 + h − g1

184

Chapter 7

Since d1 satisﬁes the triangle inequality it seems plausible that d1 is a metric (see the
deﬁnition on page 36) on L1 (λ). However, there is a problem: We may have f =
g and yet d1 ( f , g) = 0, because functions that are equal almost everywhere satisfy
and 0 := ∅ are
| f − g|dλ = 0. (Why?) For example, when S = , the functions
at zero distance from one another. Hence (L1 (λ), d1 ) fails to be a metric space. Rather
it is what’s called a pseudometric space:
Deﬁnition 7.3.18 A pseudometric space is a nonempty set M and a function ρ : M ×
M → such that, for any x, y, v ∈ M,
1. ρ( x, y) = 0 if x = y,
2. ρ( x, y) = ρ(y, x ), and
3. ρ( x, y) ≤ ρ( x, v) + ρ(v, y).
In contrast to a metric space, in a pseudometric space distinct points are permitted
to be at zero distance from one another.
Exercise 7.3.19 On the space 2 consider the function ρ( x, y) = | x1 − y1 |, where x1
and y1 are the ﬁrst components of x = ( x1 , x2 ) and y = (y1 , y2 ) respectively. Show
that ( 2 , ρ) is a pseudometric space.
It is not difﬁcult to convert a pseudometric space into a metric space: We simply
regard all points at zero distance from each other as the same point. In other words,
we partition the original space into equivalence classes of points at zero distance from
one another, and consider the set of these classes as a new space. Figure 7.6 illustrates
for the space in exercise 7.3.19.
The distance between any two equivalence classes is just the distance between
arbitrarily chosen members of each class. This value does not depend on the particular
members chosen: If x and x are equivalent, and y and y are equivalent, then ρ( x, y) =
ρ( x , y ) because
ρ( x, y) ≤ ρ( x, x ) + ρ( x , y ) + ρ(y , y)

= ρ( x , y ) ≤ ρ( x , x ) + ρ( x, y) + ρ(y, y ) = ρ( x, y)
The space of equivalence classes and the distance just described form a metric space.
In particular, distinct elements of the derived space are at positive distance from one
another (otherwise they would not be distinct).
The metric space derived from the pseudometric space (L1 (λ), d1 ) is traditionally
denoted ( L1 (λ), d1 ), and has a major role to play in the rest of this book.37 Since two
functions in L1 (λ) are at zero distance if and only if they are equal almost everywhere,
37 To

simplify notation, we are using the symbol d1 to represent distance on both spaces.

Integration

185
x2

equivalence classes

x1

Figure 7.6 Equivalence classes for (

2 , ρ)

the new space ( L1 (λ), d1 ) consists of equivalences classes of functions that are equal
almost everywhere.
A density on S is a nonnegative measurable function that integrates to one. We
are interested in describing the set of densities as a metric space, with the intention of
studying Markov chains such that their marginal distributions evolve in the space of
densities. The densities are embedded into L1 (λ) as follows:
Deﬁnition 7.3.20 The space of densities on S is written as D (S) and deﬁned by
D (S) := { f ∈ L1 (λ) : f ≥ 0 and  f 1 = 1}
In the deﬁnition, f is actually an equivalence class of functions f , f , etc., that are
all equal almost everywhere. The statement f ≥ 0 means that all of these functions
are nonnegative almost everywhere, while  f 1 = 1 means that all integrate to one.
(More generally, if f ∈ L1 (λ), then  f 1 is the integral of the absolute value of any
element in the equivalence class.)
Theorem 7.3.21 The spaces ( L1 (λ), d1 ) and ( D (S), d1 ) are both complete.
A proof of completeness of ( L1 (λ), d1 ) can be found in any good text on measure
theory. Completeness of ( D (S), d1 ) follows from the fact that D (S) is closed as a subset
of ( L1 (λ), d1 ) and theorem 3.2.3 (page 45). The proof that D (S) is closed is left as an
exercise for enthusiastic readers.
Densities are used to represent distributions of random variables. Informally, the
statement that X has density f ∈ D (S) means that X is in B ⊂ S with probability

186

Chapter 7




f ( x )dx :=: B f dλ, where f is some member of the equivalence class. Note that
it does not matter which member we pick, as all give the same value here. In this sense
it is equivalence classes that represent distributions rather than individual densities.
Finally, Scheffés identity provides a nice quantitative interpretation of d1 distance
between densities: For any f and g in D (S),
B

 f − g1 = 2 × sup

B ∈B ( S )


B

f ( x )dx −


B

g( x )dy

(7.16)

It follows that if  f − g1 ≤ , then for any event B of interest, the deviation in the
probability assigned to B by f and g is less than /2.38

7.4

Commentary

The treatment of measure theory and integration in this chapter is fairly standard
among modern expositions. There are many excellent references with which to round
out the material. Good starting points are Williams (1991) and Taylor (1997). Aliprantis and Burkinshaw (1998) is more advanced, and contains many exercises. The books
by Pollard (2002), Dudley (2002), and Schilling (2005) are also highly recommended.

38 A

proof of this identity is given later in a more general context (see lemma 11.1.29).

Chapter 8

Density Markov Chains
In this chapter we take an in-depth look at Markov chains on state space S ⊂ n with
the property that conditional (and hence marginal) distributions can be represented
by densities. These kinds of processes were previously studied in chapter 6. Now that
we have measure theory under our belts, we will be able to cover a number of deeper
results.
Not all Markov chains ﬁt into the density framework (see §9.2 for the general case).
For those that do, however, the extra structure provided by densities aids us in analyzing dynamics and computing distributions. In addition densities have a concreteness
that abstract probability measures lack, in the sense that they are easy to represent visually. This concreteness makes them a good starting point when building intuition.

8.1

Outline

We start with the basic theory of Markov chains with density representations. After
deﬁning density Markov chains, we will illustrate the connection to stochastic recursive sequences (stochastic difference equations). In §8.1.3 we introduce the Markov
operator for the density case, and show how the marginal distributions of a density
Markov chain can be generated by iterating on the Markov operator. This theory
closely parallels the ﬁnite case, as discussed in §4.2.2.

8.1.1

Stochastic Density Kernels

We met some examples of density kernels in chapter 6. Let’s now give the formal
deﬁnition of a density kernel.
187

188

Chapter 8

Deﬁnition 8.1.1 Let S be a Borel subset of n . A stochastic density kernel on S is a Borel
measurable function p : S × S → + such that


p( x, y)dy :=:



p( x, y)λ(dy) :=: λ( p( x, ·)) = 1

for all x ∈ S

(8.1)

In particular, the function y → p( x, y) is a density for each x ∈ S. We can think
of p as a family of density functions, one for each point in the state space. In what
follows, we will use the notation p( x, y)dy to represent the density function y → p( x, y).
A second point is that S × S is a subset of 2n , and Borel measurability of p refers to
Borel subsets of this space. In practice, one rarely encounters stochastic kernels where
Borel measurability is problematic.
To illustrate the deﬁnition, consider the kernel p deﬁned by


(y − ax − b)2
1
p( x, y) = √
exp −
2
2π

(( x, y) ∈ S × S =

×

)

In other words, p( x, y)dy = N ( ax + b, 1). The kernel is presented visually in ﬁgure 8.1.
Each point on the x-axis picks out a distribution N ( ax + b, 1), which is represented as
a density running along the y-axis. In this case a is positive, so an increase in x leads to
an increase in the mean of the corresponding density p( x, y)dy, and the density puts
probability mass on larger y.
From an initial condition ψ ∈ D (S) and a density kernel p, we can generate a
Markov chain ( Xt )t≥0 . Here is a deﬁnition paralleling the ﬁnite case (page 71):
Deﬁnition 8.1.2 Let ψ ∈ D (S). A random sequence ( Xt )t≥0 on S is called Markov( p, ψ) if
1. at time zero, X0 is drawn from ψ, and
2. at time t + 1, Xt+1 is drawn from p( Xt , y)dy.
In the case of the kernel p( x, y)dy = N ( ax + b, 1), we draw X0 from some given
ψ and then, at each time t, draw Xt+1 ∼ N ( aXt + b, 1). Listing 8.1 generates one
observation (of length 100) for the process when ψ = N (0, 1). An observation (sample
path) is shown in ﬁgure 8.2.
There is another way to visualize the dynamics associated with our stochastic kernel. Recall the 45 degree diagram technique for studying univariate deterministic
dynamic systems we introduced in ﬁgure 4.3 (page 58). Now consider ﬁgure 8.3, each
panel of which shows a series generated by the kernel p( x, y)dy = N ( ax + b, 1). The
kernel itself is represented in the graphs by shading. You should understand this
shading as a “contour” representation of the 3D graph in ﬁgure 8.1, with lighter areas
corresponding to higher probability. For each graph the sequence of arrows traces out

Density Markov Chains

189

prob

dy

x

Figure 8.1 The stochastic kernel p( x, y)dy = N ( ax + b, 1)

an individual time series. The initial condition is X0 = −4, and X1 is then drawn from
N ( aX0 + b, 1). We trace this value back to the 45 degree line to obtain the distribution
for X2 ∼ N ( aX1 + b, 1), and so on.
When most of the probability mass lies above the 45 degree line, the value of the
state tends to increase. When most is below the 45 degree line, the value tends to
decrease. The actual outcome, however, is random, depending on the sequence of
draws that generate the time series.

8.1.2

Connection with SRSs

Suppose that we wish to investigate a stochastic recursive sequence (SRS) where the
state space S is a Borel subset of n , Z is a Borel subset of k , F : S × Z → S is a given
function, and
Xt+1 = F ( Xt , Wt+1 ),

X0 ∼ ψ ∈ D ( S ) ,

IID

(Wt )t≥1 ∼ φ ∈ D ( Z )

(8.2)

We would like to know when there exists a stochastic density kernel p on S that represents (8.2), in the sense that ( Xt )t≥0 deﬁned in (8.2) is Markov-( p, ψ). Put differently,
we wish to know when there exists a density kernel p such that, for all x ∈ S,
p( x, y)dy = the density of F ( x, W ) when W ∼ φ

(8.3)

190

Chapter 8

Listing 8.1 (ar1.py) Simulation of ( Xt )t≥0 for p( x, y)dy = N ( ax + b, 1)
from random i m p o r t normalvariate a s N
a , b = 0.5 , 1
X = {}
X [0] = N (0 , 1)

# Parameters
# An empty dictionary to store path
# X_0 has distribution N (0 , 1)

f o r t i n range (100) :
X [ t +1] = N ( a * X [ t ] + b , 1)

Figure 8.2 Time series

191

0
−4

−2

dy

2

4

Density Markov Chains

−4

−2

0

2

4

2

4

0
−2
−4

dy

2

4

x

−4

−2

0
x

Figure 8.3 Two time series

192

Chapter 8

Such a p will do the job for us because, if the state Xt arrives at any x ∈ S, then
drawing Xt+1 from p( x, y)dy is—by deﬁnition of p—probabilistically equivalent to
drawing Wt+1 from φ and setting Xt+1 = F ( x, Wt+1 ).
The reason for our interest in this question is that the theory of Markovian dynamics is more easily developed in the framework of stochastic kernels than in that
of SRSs such as (8.2). This is largely because the stochastic density kernel captures
the stochastic law of motion for the system in one single object. To apply the theory
developed below, it is necessary to be able to take a given model of the form (8.2) and
obtain its density kernel representation.
The existence of a stochastic density kernel p satisfying (8.3) is not guaranteed, as
not all random variables have distributions that can be represented by densities. Let
us try to pin down simple sufﬁcient conditions implying that F ( x, W ) can be represented by a density.
Here is a more general question: If Y is a random variable on S, when does there
exist a density
that φ represents Y in the sense that, for every B ∈ B (S),
 φ ∈ D (S) such

the number B φdλ :=: B φ( x )dx gives the probability that Y ∈ B? In essence the
answer is that Y must not take values in any Lebesgue null set with positive probability. To
see this, suppose that N ∈ B (S) with λ( N ) = 0 and Y ∈ N with positive probability.
Then regardless of which φ ∈ D (S) we choose, theorem 7.3.8 implies that N φ( x )dx =
0, and hence φ does not represent Y.
Now let us consider the distribution of Y = F ( x, W ), W ∼ φ ∈ D ( Z ). The following theorem, though not as general as some, will be sufﬁcient for our purposes:
Theorem 8.1.3 Let W be a random variable on n with density φ, let γ ∈ n , and let Γ be
an n × n matrix. If det Γ = 0, then Y = γ + ΓW has density φY on n , where
φY (y) := φ(Γ−1 (y − γ))| det Γ−1 |

(y ∈

n

)

Why is det Γ = 0 required? If det Γ = 0 then Y = γ + ΓW takes values in a
subspace of dimension less than n. In n , such subspaces have Lebesgue measure
zero. Hence Y takes values in a Lebesgue null set with positive probability, in which
case it cannot be represented by a density.
Before looking at the proof, let’s see how we can use the theorem. In §8.1.1 we
looked at a process ( Xt )t≥0 deﬁned by kernel p( x, y)dy = N ( ax + b, 1). This corresponds to the SRS
Xt+1 = aXt + b + Wt+1 ,

IID

(Wt )t≥1 ∼ φ = N (0, 1)

In other words, p( x, y)dy = N ( ax + b, 1) is the density of Y = ax + b + W when
W ∼ φ. This claim is easily veriﬁed. From theorem 8.1.3 the density φY of Y is φY (y) =

Density Markov Chains

193

φ(y − ax − b). Since φ = N (0, 1), this becomes


(y − ax − b)2
1
exp −
= N ( ax + b, 1)
φY (y) = √
2
2π
Exercise 8.1.4 Consider the

n -valued

SRS

Xt+1 = AXt + b + Wt+1 ,

IID

(Wt )t≥1 ∼ φ ∈ D (

n

)

(8.4)

where A is an n × n matrix and b is an n × 1 vector. Show that the stochastic density
kernel corresponding to this model is p( x, y) = φ(y − Ax − b).
Exercise 8.1.5 Consider the well-known threshold autoregression model (Chan and
Tong 1986). The model is a nonlinear AR(1) process
X t +1 =

K

∑ ( A k X t + bk )

k =1

Bk ( Xt ) + Wt+1 ,

IID

(Wt )t≥1 ∼ φ ∈ D (

n

)

(8.5)

where Xt takes values in n , the family of sets ( Bk )kK=1 is a (measurable) partition of
n , and ( A )K
K
k k =1 and ( bk )k =1 are n × n-dimensional matrices and n × 1-dimensional
vectors respectively. The idea is that when Xt is in the region of the state space Bk ,
the state variable follows the law of motion Ak Xt + bk . Show that the corresponding
density kernel is


p( x, y) = φ y −

K

∑ ( A k x + bk )

k =1

Bk ( x )

(8.6)

Example 8.1.6 Consider again the Solow–Swan model. Set δ = 1 and f (k, W ) =
f (k)W. In other words, the state evolves according to
k t+1 = s f (k t )Wt+1 ,

IID

(Wt )t≥1 ∼ φ

(8.7)

Suppose that s > 0 and f (k) > 0 whenever k > 0, and take S = Z = (0, ∞). We wish
to determine the stochastic density kernel p( x, y)dy; equivalently, we wish to ﬁnd the
density φY of the random variable Y = s f ( x )W when x ∈ S is ﬁxed and W ∼ φ.
The only obstacle to applying theorem 8.1.3 in this case is that Z is a proper subset
of , not itself. Hence φ is not necessarily deﬁned on all of . However, we can get
around this easily enough by setting φ = 0 on the complement (−∞, 0] of Z.
When x ∈ S is ﬁxed, s f ( x ) is a strictly positive constant, and from theorem 8.1.3
the density of Y = s f ( x )W is


y
1
p( x, y) = φ
(8.8)
s f (x) s f (x)

194

Chapter 8

Exercise 8.1.7 Consider again example 8.1.6, but this time with δ < 1. In other words,
(k t )t≥0 evolves according to
k t+1 = s f (k t )Wt+1 + (1 − δ)k t ,

IID

(Wt )t≥1 ∼ φ

Show that the stochastic density kernel is now given by


y − (1 − δ ) x
1
p( x, y) = φ
s f (x)
s f (x)

(8.9)

Notice that if y < (1 − δ) x, then φ is evaluated at a negative number. This is why
we need to extend φ to all of , with φ(z) = 0 for all z ≤ 0.
Exercise 8.1.8 In §6.1.2 we considered a stochastic threshold externality model with
multiple equilibria, with law of motion for capital given by
k t+1 = sA(k t )kαt Wt+1 ,

IID

(Wt )t≥1 ∼ φ

(8.10)

Here k → A(k) is any function with A(k) > 0 when k > 0. Set Z = S = (0, ∞). Derive
the stochastic density kernel p corresponding to this model.
Now let’s sketch the proof of theorem 8.1.3. We will use the following standard
change-of-variable result:
Theorem 8.1.9 Let A and B be open subsets of
f ∈ L1 ( A, B ( A), λ), then

A

f ( x )dx =


B

k,

and let T : B → A be a C1 bijection. If

f ◦ T (y) · | det JT (y)|dy

(8.11)

Here JT (y) is the Jacobian of T evaluated at y, and C1 means that T is continuously
differentiable on B.1
Example 8.1.10 Let A = , let B = (0, ∞), and let Tx = ln( x ). Then | det JT (y)| =
1/y, and for any measurable f on with ﬁnite integral we have


f ( x )dx =


(0,∞)

1
f (ln y) dy
y

Now suppose we have a random variable W with density φ ∈ D ( n ), and we
transform W with some function h to create a new random variable Y = h(W ). In
the case of theorem 8.1.3 the transformation h is the linear function z → γ + Γz. The
following generalization of theorem 8.1.3 holds:
1 theorem

8.1.9 is actually a special case of theorem 7.3.17 on page 182.

Density Markov Chains

195

Theorem 8.1.11 Let S and T be open subsets of n , and let W be a random vector on S,
distributed according to density φ on S. Let Y = h(W ), where h : S → T is a bijection and
the inverse h−1 is a C1 function. In that case Y is a random vector on T with density φY ,
where
φY (y) := φ(h−1 (y)) · | det Jh−1 (y)|
(y ∈ T )
(8.12)
The proof can be constructed along the
 following lines: The statement that φY is
the density of Y means that {Y ∈ B} = B φY (y)dy holds for every B ∈ B ( T ). By an
application of theorem 8.1.9 we get

{Y ∈ B } =
=

8.1.3



{W ∈ h−1 ( B)}
h −1 ( B )

φ( x )dx =


B

φ(h

−1

(y))| det Jh−1 (y)|dy =


B

φY (y)dy

The Markov Operator

Let p be a density kernel on S ∈ B ( n ), let ψ ∈ D (S) be an initial condition, and
let ( Xt )t≥0 be Markov-( p, ψ). As usual, let ψt be the (marginal) distribution of Xt
for each t ≥ 0. In §6.1.3 we saw that if X and Y are random variables on S with
marginals p X and pY , and if pY |X is the conditional density of Y given X, then pY (y) =

pY |X ( x, y) p X ( x )dx for all y ∈ S. Letting Xt+1 = Y and Xt = X, we obtain
ψt+1 (y) =



p( x, y)ψt ( x )dx

(y ∈ S)

(8.13)

Equation (8.13) is just (6.7) on page 126, translated to a more general setting. It is the
continuous state version of (4.10) on page 75 and the intuition is roughly the same:
The probability of being at y tomorrow is the probability of moving from x today to y
tomorrow, summed over all x and weighted by the probability ψt ( x )dx of observing
x today.
Now deﬁne an operator M sending ψ ∈ D (S) into ψM ∈ D (S) by
ψM(y) =



p( x, y)ψ( x )dx

(y ∈ S)

(8.14)

This operator is called the Markov operator corresponding to stochastic density kernel
p, and parallels the deﬁnition of the Markov operator for the ﬁnite state case given on
page 75. As in that case, M acts on distributions (densities) to the left rather than the
right, and is understood as updating the distribution of the state: If the state is currently distributed according to ψ then next period its distribution is ψM. In particular,
(8.13) can now be written as
ψt+1 = ψt M
(8.15)

196

Chapter 8

This a density version of (4.12) on page 76. Iterating backward we get the representation ψt = ψMt for the distribution of Xt given X0 ∼ ψ.
Technical note: An element of D (S) such as ψ is actually an equivalence class of
functions on S that are equal almost everywhere—see §7.3.3. This does not cause
problems for the deﬁnition of the Markov operator, since applying M to any element
of the equivalence class yields the same function: If ψ and ψ are equal off a null set
E, then the integrands in (8.14) are equal off E for every y, and hence both integrate to
the same number. Thus ψM in (8.14) is a well-deﬁned function on S, and we embed
it in D (S) by identifying it with the equivalence class of functions to which it is equal
almost everywhere.
That M does in fact map D (S) into itself can be veriﬁed by showing that ψM is
nonnegative and integrates to 1. That ψM integrates to 1 can be seen by changing the
order of integration:
"

 
 !
ψM(y)dy =
p( x, y)ψ( x )dxdy =
p( x, y)dy ψ( x )dx



Since p( x, y)dy = 1 and ψ( x )dx = 1, the proof is done. Regarding nonnegativity,
ﬁx any y ∈ S. Since p ≥ 0 and since ψ ≥ 0 almost everywhere, the integrand in (8.14)
is nonnegative almost everywhere, and ψM(y) ≥ 0.
Before moving on let us brieﬂy investigate the iterates of the Markov operator M.
Recall that when we studied ﬁnite Markov chains, the t-th order kernel pt ( x, y) was
deﬁned by
p1 := p, pt ( x, y) := ∑ pt−1 ( x, z) p(z, y)
z∈S

Analogously, let p be a stochastic density kernel p, and deﬁne a sequence ( pt )t≥1 of
kernels by

(8.16)
p1 := p, pt ( x, y) := pt−1 ( x, z) p(z, y)dz
Below pt is called the t-th order density kernel corresponding to p. As in the ﬁnite state
case, pt ( x, y)dy can be interpreted as the distribution (density) of Xt when X0 = x.
Exercise 8.1.12 Using induction, verify that pt is density kernel on S for each t ∈

.

Lemma 8.1.13 If M is the Markov operator associated with stochastic density kernel p on S,
then Mt is the Markov operator associated with pt . In other words, for any ψ ∈ D (S), we
have

ψMt (y) = pt ( x, y)ψ( x )dx
(y ∈ S)
This lemma is the continuous state version of lemma 4.2.8 on page 76. Essentially
it is telling us what we claimed above: that pt ( x, y)dy is the distribution of Xt given
X0 = x. The proof is an exercise.

Density Markov Chains

197

Given any kernel p on S, the Markov operator M is always continuous on D (S)
(with respect to d1 ). In fact it is nonexpansive. To see this, observe that for any φ, ψ ∈
D (S), we have

φM − ψM1 =
≤
=

8.2





 
 

p( x, y)(φ( x ) − ψ( x ))dx dy
p( x, y)|φ( x ) − ψ( x )|dxdy
p( x, y)dy|φ( x ) − ψ( x )|dx = φ − ψ1

Stability

Let’s now turn to the topic of stability. The bad news is that for density Markov chains
on inﬁnite state spaces, the theory is considerably more complex than for the ﬁnite
case (see §4.3.3). The good news is that we can build on the intuition gained from
studying the ﬁnite case, and show how the concepts can be extended to cope with inﬁnite state spaces. After reviewing the density analogue of the Dobrushin coefﬁcient,
we look at drift conditions that keep probability mass within a “bounded” region
of the state space as the Markov chain evolves. Combining drift conditions with a
concept related to positivity of the Dobrushin coefﬁcient, we obtain a rather general
sufﬁcient condition for stability of density Markov chains, and apply it to several applications.

8.2.1

The Big Picture

Before plunging into the formal theory of stability, we are going spend some time
building intuition. In particular, we would like to know under what circumstances
stability will fail, with the aim of developing conditions that rule out these kinds of
circumstances. This section considers these issues in a relatively heuristic way. We
begin with the deﬁnitions of stationary densities and global stability.
Let p be a stochastic density kernel on S, and let M be the corresponding Markov
operator. As before, S is a Borel subset of n endowed with the standard euclidean
metric d2 . Since M sends D (S) into D (S), and since D (S) is a well-deﬁned metric
space with the distance d1 (see §7.3.3), the pair ( D (S), M) is a dynamical system in the
sense of chapter 4. The trajectory (ψMt )t≥0 of a point ψ ∈ D (S) corresponds to the
sequence of marginal distributions for a Markov-( p, ψ) process ( Xt )t≥0 .
Now consider the stability properties of the dynamical system ( D (S), M). We are
interested in existence of ﬁxed points and global stability. A ﬁxed point ψ∗ of M is

198

Chapter 8

also called a stationary density, and, by deﬁnition, satisﬁes
ψ∗ (y) =



p( x, y)ψ∗ ( x )dx

(y ∈ S)

Exercise 8.2.1 Consider the linear AR(1) model
Xt+1 = aXt + Wt+1 ,

IID

(Wt )t≥1 ∼ φ = N (0, 1)

(8.17)

with | a| < 1. The corresponding density kernel is p( x, y)dy = N ( ax, 1). Using pencil
and paper, show that the normal density N (0, 1/(1 − a2 )) is stationary for this kernel.
In the ﬁnite state case every Markov chain has a stationary distribution (theorem 4.3.5, page 86). When S is not ﬁnite, however, stationary distributions can easily fail to exist. Consider, for example, the model (8.17) with α = 1, which is called
a random walk. With a bit of thought, you will be able to convince yourself that
Xt ∼ N ( X0 , t), and hence
pt ( x, y) = √

1
2πt


exp

−(y − x )2
2t



(( x, y) ∈

×

)

Exercise 8.2.2 Show that p has no stationary distribution by arguing that if ψ∗ is stationary, then

∗
ψ (y) = pt ( x, y)ψ∗ ( x )dx
(t ∈ , y ∈ )
in which case the dominated convergence theorem implies that ψ∗ (y) = 0 for all
y ∈ . (Contradicting what?)
Translated to the present context, global stability of ( D (S), M) is equivalent to the
existence of a unique stationary density ψ∗ such that
ψMt → ψ∗ in d1 as t → ∞ for every ψ ∈ D (S)
Let’s try to work out when such stability can be expected. First, we have to rule out the
kind of behavior exhibited by the random walk above. In this case the density of Xt
becomes more and more spread out over . In fact ψt converges to zero everywhere
because

(t → ∞)
ψt (y) = (ψMt )(y) = pt ( x, y)ψ( x )dx → 0
for all y ∈ by the dominated convergence theorem.
A similar problem arises when densities are diverging off to the right or the left, as
illustrated in ﬁgure 8.4. In either case probability mass is escaping from the “center”
of the state space. In other words, it is not concentrating in any one place.

Density Markov Chains

199

diverging

Figure 8.4 Divergence to +∞

K
Figure 8.5 Nondiverging sequence

What we need then is to ensure that densities concentrate in one place, or that
“most” of the probability mass stays in the “center” for all densities in the trajectory.
Another way of putting this is to require that, for each density, most of the mass is on
a bounded set K, as in ﬁgure
 8.5. More mathematically, we require the existence of a
bounded set K such that K ψt ( x )dx  1 for all t. (Requiring that all mass stays on K
is too strict because there will always be tails of the densities poking out.)
Now, having said that K must be bounded, the truth of the matter is that boundedness is not enough. Suppose that we are studying a system not on S = , but rather
on S = (−1, 1). And suppose, for example, that the densities are shifting all their mass
toward 1.2 Now this is also an example of instability (after all, there is no limiting density for such a sequence to converge to), and it has a similar feel to the divergence in
ﬁgure 8.4 (in the sense that probability mass is leaving the center of the state space).
But we cannot rule this problem out by requiring that probability remains on some
bounded set K ⊂ (−1, 1); it already stays on the bounded set (−1, 1). To keep probability mass in the center of the state space, what we really need is an interval [ a, b]
with [ a, b] ⊂ (−1, 1) and most of the mass remaining on [ a, b].
A suitable condition, then, is to require that K be not only bounded but also com2 We

can imagine this might be the case if we took the system in ﬁgure 8.4 and transformed it via a
change of variables such as y = arctan( x ) into a system on (−1, 1).

200

Chapter 8
kt+1
bh
h
ah

kt

Figure 8.6 Multiple invariant sets

pact. That is, we require that most of the probability mass stays on a compact subset
of S. For example,
 we might require that, given any  > 0, there is a compact set
K ⊂ S such that K ψt ( x )dx ≥ 1 −  for all t. Indeed this is precisely the deﬁnition of
tightness, and we will see it plays a crucial role in what follows.
Tightness is necessary for stability, but it is not sufﬁcient. For example, consider
the model represented in ﬁgure 8.6, which is a stochastic version of the model in example 4.1.8 (page 57). The deterministic model is k t+1 = h(k t ) := sA(k t )kαt , and
the function h is the bold curve in ﬁgure 8.6. The stochastic version is given by
k t+1 = Wt+1 h(k t ), where the shock sequence (Wt )t≥1 is supported on a bounded
interval [ a, b]. The functions k → ah(k) and k → bh(k) are represented by dashed
lines.
A little thought will convince you that the two intervals marked in the ﬁgure are
invariant sets, which is to say that if the state enters either of these sets, then it cannot
escape—it remains there with probability one. As a result global stability fails, even
though tightness appears likely to hold. (It does, as we’ll see later on.)
The problem here is that there is insufﬁcient mixing for global stability to obtain.
What we need, then, is a condition to ensure sufﬁcient mixing, as well as a condition
for tightness. Finally, we need a minor technical condition called uniform integrability
that ensures trajectories do not pile up on a set of measure zero—as does, for example,
the sequence of densities N (0, 1/n)—in which case the limiting distribution cannot be
represented by a density, and hence does not exist in D (S).
We now turn to a formal treatment of these conditions for stability.

Density Markov Chains

8.2.2

201

Dobrushin Revisited

On page 89 we introduced the Dobrushin coefﬁcient for the ﬁnite case. Replacing
sums with integrals, we get
α( p) := inf




p( x, y) ∧ p( x , y)dy : ( x, x ) ∈ S × S

For densities f and g the pointwise minimum f ∧ g is sometimes called the afﬁnity
between f and g, with a maximum of 1 when f = g and a minimum of zero when
f and g have disjoint supports. The Dobrushin coefﬁcient reports the inﬁmum of the
afﬁnities for all density pairs in the stochastic kernel.
The proof of theorem 4.3.17 (page 89) carries over to the density case almost unchanged, replacing sums with integrals at each step. That is to say,

φM − ψM1 ≤ (1 − α( p))φ − ψ1

∀ φ, ψ ∈ D (S)

and, moreover, this bound is the best available, in the sense that

∀ λ < 1 − α( p), ∃ φ, ψ ∈ D (S) such that φM − ψM1 > λφ − ψ1

(8.18)

From completeness of D (S), Banach’s ﬁxed point theorem, and lemma 4.1.21 (see
page 61), it now follows (supply the details) that ( D (S), M) is globally stable whenever there exists a t ∈ such that α( pt ) > 0.
On the other hand, positivity of α( pt ) for some t is no longer necessary for global
stability.3 This is fortunate because in many applications we ﬁnd that α( pt ) = 0 for
all t ∈ . To give an example, consider the stochastic density kernel p given by
p( x, y)dy = N ( ax, 1), which corresponds to the AR(1) process (8.17). If | a| < 1 then
this process is globally stable, as was shown in chapter 1 using elementary arguments.
However, it turns out that α( pt ) = 0 for every t ∈ . Indeed, for ﬁxed t, pt ( x, y)dy =
N (cx, d) for some constants c and d. Choosing x, x so that cx = n ∈ and cx = −n,
the integral of pt ( x, y) ∧ pt ( x , y) is the area of the two tails shown in ﬁgure 8.7. This
integral can be made arbitrarily small by choosing n sufﬁciently large.
Thus, in view of (8.18), the Markov operator M associated with the AR(1) process
is not uniformly contracting, and neither is any iterate Mt . Hence Banach’s ﬁxed point
theorem does not apply.
Fortunately we can get around this negative result and produce a highly serviceable sufﬁcient condition for stability. However, a bit of fancy footwork is required.
The rest of this section explains the details.
3 If

you did the proof of necessity in exercise 4.3.22 (page 90) you will understand that ﬁniteness of S is
critical.

202

Chapter 8



pt ( x, y) ∧ pt ( x , y)dy

−n

0

n

Figure 8.7 Dobrushin coefﬁcient is zero

First, even when Mt fails to be a uniform contraction on D (S), it may still be a
contraction, in the sense that

φMt − ψMt 1 < φ − ψ1 whenever φ = ψ

(8.19)

In fact the following result holds (see the appendix to this chapter for a proof.)
Lemma 8.2.3 Let t ∈ , let p be a stochastic density kernel on S, and let M be the Markov
operator corresponding to p. If


pt ( x, y) ∧ pt ( x , y)dy > 0 for all x, x ∈ S

(8.20)

then Mt is a contraction on D (S); that is, (8.19) holds.
The existence of a t such that (8.20) holds is a mixing condition. (The need for
mixing was discussed in §8.2.1.) A simple but important special case is when p is
strictly positive on S × S (an example is the kernel p associated with the AR(1) process
in exercise 8.2.1), in which case p( x, y) ∧ p( x , y) > 0 for each y. Integrating a positive
function over a set of positive measure produces a positive number (theorem 7.3.9,
page 181), and condition (8.20) is satisﬁed.
Contractiveness can be used to prove global stability when paired with compactness of the state space. In particular, if h : U → U is contracting and U is compact,
then the dynamical system (U, h) is globally stable (see theorem 3.2.38 on page 54). In
our case this result is potentially helpful, but does not immediately apply. The reasons

Density Markov Chains

203

is that when S is inﬁnite, D (S) = ( D (S), d1 ) is not compact. The next two exercises
help to illustrate why this is so.
Exercise 8.2.4 Let S = , and let (φn )n≥1 ⊂ D (S) be given by φn := [n,n+1) . Show
that d1 (φn , φm ) = 2 whenever n = m. Conclude that this sequence has no subsequence
converging to a point in D (S).
Exercise 8.2.5 Let S = (0, 1), and let (φn )n≥1 ⊂ D (S) be given by φn := n · (0,1/n) .
Suppose that d1 (φn , φ) → 0 for some φ ∈ D (S). Using your measure-theoretic bag of
tricks, show that λ(φ) = 0, contradicting φ ∈ D (S). You have now shown that the
sequence itself cannot converge to any point in D (S). Argue that the same is true for
any subsequence.
Fortunately there is a way around this problem created by lack of compactness
of D (S). Recall from exercise 4.1.9 (page 58) that a dynamical system (U, h) is called
Lagrange stable if every trajectory is precompact in U. Lagrange stability is weaker
than compactness of U,4 but it turns out that if (U, h) is contracting and Lagrange stable
then it is globally stable.5
So suppose that (8.20) holds for some t ∈ , and hence Mt is contracting. If
we can prove that all trajectories of ( D (S), M) are precompact, then all trajectories
of ( D (S), Mt ) are also precompact (subsets of precompact sets are precompact), and
( D (S), Mt ) is globally stable. Finally, lemma 4.1.21 (page 61) implies that if ( D (S), Mt )
is globally stable, then so is ( D (S), M). Let’s record this as a theorem.
Theorem 8.2.6 Let ( D (S), M) be Lagrange stable. If Mt is a contraction for some t ∈
then ( D (S), M) is globally stable.

,

But how to prove precompactness of trajectories? We need the following two definitions:
Deﬁnition 8.2.7 Let M be a subset of D (S). The collection of densities M is called
tight if


∀  > 0, ∃ a compact set K ⊂ S such that sup

ψ ∈M

It is called uniformly integrable if

∀  > 0, ∃ δ > 0 such that λ( A) < δ implies sup

ψ ∈M

Kc

ψ( x )dx ≤ 


A

ψ( x )dx ≤ 

U is compact then (U, h) is always Lagrange stable (why?), but the converse is not true (example?).
Fix x ∈ U, and deﬁne Γ( x ) to be the closure of {hn ( x ) : n ∈ }. The set Γ( x ) is a compact subset
of U. (Why?) Moreover h sends Γ( x ) into itself (see exercise 4.1.3, page 56). Hence (Γ( x ), h) is a dynamical
system where h is contracting and Γ( x ) is compact, implying the existence of a unique ﬁxed point x ∗ ∈ Γ( x )
with hn ( x ) → x ∗ (theorem 3.2.38, page 54). Finally, h has at most one ﬁxed point in U by contractiveness
over U. Therefore x ∗ does not depend on x, and (U, h) is globally stable.
4 If

5 Proof:

204

Chapter 8

Here λ is Lebesgue measure. Essentially, tightness rules out the violation of compactness seen in exercise 8.2.4, while uniform integrability rules out that seen in exercise 8.2.5. Tightness and uniform integrability are important to us because of the
following result:
Theorem 8.2.8 Let p be a stochastic density kernel on S, and let M be the corresponding
Markov operator. Let ψ ∈ D (S). If the sequence (ψMt )t≥0 is both tight and uniformly
integrable, then it is also precompact in D (S).
While the proof is omitted, those readers familiar with functional analysis will understand that tightness and uniform integrability together imply weak precompactness in L1 . Further, the fact that M is an integral operator means it is sufﬁciently
smoothing that it sends weakly precompact sets into (strongly) precompact sets. The
proof of the latter result can be found in Lasota (1994, thm. 4.1).

8.2.3

Drift Conditions

How might one verify the tightness and uniform integrability of a given trajectory
(ψMt )t≥0 ? Tightness is usually established using drift inequalities. We will use a
variety that rely on the concept of norm-like functions (Meyn and Tweedie 1993):
Deﬁnition 8.2.9 A measurable function w : S → + is called norm-like if all of its
sublevel sets (i.e., sets of the form Ca := { x ∈ S : w( x ) ≤ a}, a ∈ + ) are precompact
in (S, d2 ).
Example 8.2.10 Let S = n , and let w( x ) :=  x , where  ·  is any norm on n (definition 3.1.2, page 37). This function w is norm-like on S because the sublevel sets of
w are bounded with respect to the metric induced by  · , and hence bounded for d2
(theorem 3.2.30, page 50). For subsets of ( n , d2 ), boundedness implies precompactness (theorem 3.2.19, page 48).
We can now state our drift condition:
Deﬁnition 8.2.11 Let p be a stochastic density kernel on S. We say that p satisﬁes
geometric drift to the center if there exists a norm-like function w on S and positive
constants α < 1 and β < ∞ such that


w(y) p( x, y)dy ≤ αw( x ) + β

( x ∈ S)

As we will see, this condition is often easy to check in applications. Moreover
Proposition 8.2.12 If p satisﬁes geometric drift to the center, then (ψMt ) is tight for every
ψ ∈ D ( S ).

Density Markov Chains

205

The proof is given in the appendix
to the chapter, but the intuition is not difﬁcult:

Under geometric drift, the ratio w(y) p( x, y)dy/w( x ) is dominated by α + β/w( x ).
Norm-like functions tend to get large as x moves away from the center of the state
space,
so if x is sufﬁciently far from the center, then α + β/w( x ) < 1. In which case

w(y) p( x, y)dy, the expectation of w( Xt+1 ) given Xt = x, is less than w( x ). This
in turn means that probability mass is moving back toward the center, where w is
smaller. Keeping probability mass in the center of the state space is the essence of
tightness.
Let’s conclude this section by showing that the AR(1) model
Xt+1 = aXt + b + Wt+1 ,

IID

(Wt )t≥1 ∼ φ = N (0, 1),

| a| < 1

is Lagrange stable. Since its stochastic kernel
p( x, y) = φ(y − ax − b),

1
exp(−z2 /2)
φ(z) := √
2π

is strictly positive on × (and hence its Markov operator is contracting), this implies global stability.
Regarding tightness, let w be the norm-like function | · |. The change of variable
z = y − ax − b gives


|y| p( x, y)dy =
=




|y|φ(y − ax − b)dy
| ax + b + z|φ(z)dz ≤ α| x | + β,

α : = | a |, β : = | b | +



|z|φ(z)dz

Since | a| < 1 the geometric drift condition is satisﬁed, and, in view of proposition
8.2.12, every trajectory is tight.
Only uniform integrability of trajectories remains to be checked. To do this, pick
any ψ ∈ D (S). Observe there is a constant K such that p( x, y) ≤ K for every x, y.
Hence, for any A ∈ B ( ), t ∈ ,
"

 !
p( x, y)ψMt−1 ( x )dx dy
ψMt (y)dy =
A
A
"
 !

=
p( x, y)dy ψMt−1 ( x )dx ≤ Kλ( A)ψMt−1 ( x )dx = Kλ( A)
A


Now ﬁx  > 0. If λ( A) < /K, then A ψMt (y)dy < , independent of t. Hence
uniform integrability of (ψMt )t≥0 is established. Theorem 8.2.8 now tells us that
( D (S), M) is Lagrange stable, and hence globally stable.
We will see in the next section that these ideas can be used to prove the stability
of much more complex models. Before discussing applications in earnest, let’s try to

206

Chapter 8

package our results in a simple format. On the Lagrange stability side, we can make
our life easier with the following result:
Proposition 8.2.13 Let ψ ∈ D (S), let p be a stochastic density kernel on S, and let M be
the corresponding Markov operator. If the sequence (ψMt )t≥0 is tight, and in addition there
exists a continuous function m : S →
such that p( x, y) ≤ m(y) for all x, y ∈ S, then
t
(ψM )t≥0 is also uniformly integrable.
The proof is an extension of the proof of uniform integrability of the AR(1) system
above, where we used the fact that p is bounded above by a constant (which is certainly a continuous function). It is given in the appendix to the chapter and should be
skipped on ﬁrst pass.
Now let’s put this all together:
Theorem 8.2.14 Let p be a stochastic density kernel on S, and let M be the corresponding
Markov operator. If

1. ∃ t ∈ such that pt ( x, y) ∧ pt ( x , y)dy > 0 for all ( x, x ) ∈ S × S,
2. p satisﬁes geometric drift to the center, and
3. ∃ a continuous m : S →

such that p( x, y) ≤ m(y) for all x, y ∈ S,

then the dynamical system ( D (S), M) is globally stable.
Proof. In view of theorem 8.2.6, we need only show that (ψMt )t≥0 is precompact for
every ψ ∈ D (S). So pick any ψ ∈ D (S). Since p satisﬁes geometric drift, (ψMt )t≥0 is
tight. By proposition 8.2.13 it is also uniformly integrable, and therefore precompact
(theorem 8.2.8).
Just as for the ﬁnite state case, stability is connected with the law of large numbers
(recall theorem 4.3.33 on page 94). For example, take the stochastic recursive sequence
Xt+1 = F ( Xt , Wt+1 ),

X0 ∼ ψ,

IID

(Wt )t≥1 ∼ φ

(8.21)

where the state space S is a Borel subset of n , Z is a Borel subset of k , φ ∈ D ( Z )
and ψ ∈ D (S). Let kernel p represent this SRS on S in the sense of (8.3) on page 189.
Let M be the corresponding Markov operator. We have the following result:
Theorem 8.2.15 Let h : S → be a Borel measurable function, and let ( Xt )t≥0 , the kernel
p and the Markov operator M be as above. If ( D (S), M) is globally stable with stationary
distribution ψ∗ , then
1
n

n

∑ h ( Xt ) →

t =1

with probability one whenever





h( x )ψ∗ ( x )dx

|h( x )|ψ∗ ( x )dx is ﬁnite.

as n → ∞

Density Markov Chains

207

The meaning of probability one convergence will be discussed later, but for now
you can understand it as it sounds: The probability of generating a path (Wt )t≥1 such
that this convergence fails is zero. Notice that convergence holds independent of the
initial condition ψ.
The proof of the theorem is beyond the scope of this book. See Nummelin (1984,
prop. 6.3) and Meyn and Tweedie (1993, thm. 17.1.7). Note that theorem 8.2.15 justiﬁes
the stationary density look-ahead estimator introduced in §6.1.4, at least when stability holds. Also, as we saw in the ﬁnite state case, the LLN leads to a new interpretation
of the stationary density that is valid in the globally stable case:

B

ψ∗ ( x )dx  the fraction of time that ( Xt )t≥0 spends in B

for any B ∈ B (S). To see this, take h =
1
n

8.2.4

n

1

n

∑ h ( Xt ) = n ∑

t =1

t =1

B.

B ( Xt )

Then theorem 8.2.15 gives

→


B

ψ∗ ( x )dx

as n → ∞

(8.22)

Applications

Let’s turn to applications. To start, recall the model of commodity dynamics with
speculation discussed in §6.3.2. The state evolves according to
Xt+1 = αI ( Xt ) + Wt+1 ,

IID

(Wt )t≥1 ∼ φ

where I is the equilibrium investment function deﬁned in (6.27), page 146. Suppose
for now that φ is a lognormal density. The stochastic kernel is
p( x, y) = φ(y − αI ( x ))

(( x, y) ∈ S × S)

where φ(z) = 0 when z < 0. The state space is S = + .
This model is easily seen to be globally stable. Regarding condition 1 of theorem 8.2.14, pick any x, x ∈ S. Let E be all y ∈ S such that y > αI ( x ) and y > αI ( x ).
On E the function y → p( x, y) ∧ p( x , y) is strictly positive, and integrals of strictly
positive functions on sets of positive measure are positive (theorem 7.3.9, page 181).
Hence condition 1 holds for t = 1.
Regarding condition 2, let w( x ) = x. This function is norm-like on S. (Proof?)
Moreover geometric drift to the center holds because, using the change of variable
z = y − αI ( x ),


yp( x, y)dy =



yφ(y − αI ( x ))dy = αI ( x ) +



zφ(z)dz ≤ αx +



zφ(z)dz

208

Chapter 8

Condition 3 is trivial because p( x, y) ≤ K for some constant K, and constant functions
are continuous. Hence global stability holds.
Next, recall the STAR model of example 6.1.3, where Z = S = , and the state
evolves according to
IID

Xt+1 = g( Xt ) + Wt+1 ,

(Wt )t≥1 ∼ φ ∈ D ( )

(8.23)

with g( x ) := (α0 + α1 x )(1 − G ( x )) + ( β 0 + β 1 x ) G ( x ). Here G : S → [0, 1] is a smooth
transition function satisfying G > 0, limx→−∞ G ( x ) = 0, and limx→∞ G ( x ) = 1.
Suppose that φ is bounded, everywhere positive on ,
γ := |α1 | ∨ | β 1 | < 1,



and

|z|φ(z)dz < ∞

Exercise 8.2.16 Show that under these assumptions there exists a constant c such that
| g( x )| ≤ γ| x | + c for all x ∈ S = .
Since the stochastic density kernel p( x, y) = φ(y − g( x )) is strictly positive on
S × S, condition 1 of theorem 8.2.14 holds. Regarding condition 2, set w( x ) = | x |. The
change of variable z = y − g( x ) gives


|y| p( x, y)dy =



|y|φ(y − g( x ))dy =



| g( x ) + z|φ(z)dz ≤ γ| x | + c +



|z|φ(z)dz

Since γ < 1, condition 2 is satisﬁed.
Condition 3 is trivial because φ and hence p are bounded by some constant K.
Setting m(y) = K for all y gives a continuous upper bound.
As another application, consider again the threshold autoregression model, where
S = Z = n , and
Xt +1 =

K

∑ ( A k X t + bk )

k =1

IID

Bk ( Xt ) + Wt+1 ,

(Wt )t≥1 ∼ φ ∈ D (

n

)

In exercise 8.1.5 you showed that the stochastic kernel is given by

p( x, y) = φ y −

K

∑ ( A k x + bk )

k =1


Bk ( x )

(( x, y) ∈ S × S)

(8.24)


Assume that φ is strictly positive on n , bounded, and that zφ(z)dz < ∞ for some
norm  ·  on n . Conditions 1 and 3 of theorem 8.2.14 can be veriﬁed in much the
same way as the previous example. Regarding condition 2, let γk be a real number
such that  Ak x  ≤ γk  x  for all x. Assume that γ := maxk γk < 1. Then, using a

Density Markov Chains

209

change of variable again,


y p( x, y)dy =
≤
≤



 ∑ ( A k x + bk )
 k =1

  K

K

∑  A k x + bk 

k =1
K

∑ γk  x 

k =1

≤ γ x  + β,




(
x
)
+
z
 φ(z)dz
Bk


Bk ( x ) +

Bk ( x ) +

K



zφ(z)dz

∑  bk  +

k =1
K

β :=

∑

k =1



 bk  +



zφ(z)dz
zφ(z)dz

Since γ < 1 and  ·  is norm-like, condition 2 is also satisﬁed, and the model is
globally stable.
Before starting the next application, let’s think a little more about norm-like functions (i.e., nonnegative functions with precompact sublevel sets). For the metric space
( n , d2 ) we can equate precompactness with boundedness (theorem 3.2.19, page 48).
For S ⊂ n , bounded subsets of (S, d2 ) are not necessarily precompact (e.g., see exercise 3.2.14 on page 47), making precompactness and hence the norm-like property
harder to check. The next result gives some guidance when S is an open interval in .
The proof is an exercise.
Lemma 8.2.17 If S = (u, v), where u ∈ {−∞} ∪ and v ∈ {+∞} ∪
is norm-like if and only if limx→u w( x ) = limx→v w( x ) = ∞.6

, then w : S →

+

As a consequence w( x ) := | ln x | is a norm-like function on S = (0, ∞). We exploit
this fact below.
Now for the last application. In exercise 8.1.8 you derived the stochastic kernel
for the nonconvex growth model k t+1 = sA(k t )kαt Wt+1 , where S = Z = (0, ∞) and
(Wt )t≥1 is IID with density φ ∈ D ( Z ). It has the form


y
1
(( x, y) ∈ S × S)
(8.25)
p( x, y) = φ
sA( x ) x α sA( x ) x α
Suppose that A takes values in [ a1 , a2 ] ⊂ S and that α < 1. Regarding the density
φ, assume that φ is strictly positive on (0, ∞), that | ln z|φ(z)dz is ﬁnite, and that
φ(z)z ≤ M for some M < ∞ and all z ∈ (0, ∞). For example, the lognormal density
satisﬁes all of of these conditions.
6 Here lim
, there exists an N ∈
such that
x → a f ( x ) = ∞ means that for any xn → a and any M ∈
n ≥ N implies f ( xn ) ≥ M. Hint: Show that K ⊂ S is precompact in S if and only if no sequence in K
converges to either u or v.

210

Chapter 8

Now let’s check the conditions of theorem 8.2.14. Condition 1 holds because p is
strictly positive on S × S. Regarding condition 2, set w( x ) = | ln x |, so


w(sA( x ) x α z)φ(z)dz =



| ln s + ln A( x ) + α ln x + ln z|φ(z)dz

≤ | ln s| + | ln A( x )| + α| ln x | +
Setting β := | ln s| + max{| ln a1 |, | ln a2 |} +






| ln z|φ(z)dz

| ln z|φ(z)dz we obtain

w(sA( x ) x α z)φ(z)dz ≤ α| ln x | + β = αw( x ) + β

Since w is norm-like on (0, ∞), condition 2 is proved.
Finally, consider condition 3. Given any x and y in S we have


y
y
y
1
M
≤
p( x, y) = p( x, y) = φ
y
sA( x ) x α sA( x ) x α y
y
Since m(y) := M/y is continuous on S, condition 3 is satisﬁed.

8.3

Commentary

The material in this chapter draws heavily on Lasota and Mackey (1994), and also
on a slightly obscure but fascinating paper of Lasota (1994). Theorem 8.2.14 is from
Mirman, Reffett, and Stachurski (2005). More details on the theory and an application
to optimal growth can be found in Stachurski (2002). See also Stachurski (2003).

Chapter 9

Measure-Theoretic Probability
In the ﬁrst few decades of the twentieth century, mathematicians realized that through
measure theory it was possible to place the slippery subject of probability in a completely sound and rigorous framework, where manipulations are straightforward and
powerful theorems can be proved. This integration of probability and measure yielded
a standard language for research in probability and statistics shared by mathematicians and other scientists around the world. A careful read of this chapter will provide
sufﬁcient ﬂuency to understand and participate in their conversation.

9.1

Random Variables

The language of probability begins with random variables and their distributions.
Let’s start with a detailed treatment of these topics, beginning with basic deﬁnitions
and moving on to key concepts such as expectations and independence.

9.1.1

Basic Deﬁnitions

In probability theory, the term random variable is just another way of saying F measurable real-valued function on some measure space (Ω, F ). In other words, a
random variable on (Ω, F ) is a map X : Ω → with the property that X −1 ( B) ∈ F
for all B ∈ B ( ). For historical reasons random variables are typically written with
upper-case symbols such as X and Y, rather than lower-case symbols such as f and
g. The measurable space (Ω, F ) is usually paired with a probability measure (see
page 167), which assigns probabilities to events E ∈ F .
Why restrict attention to F -measurable functions? Well, suppose that is a probability on (Ω, F ). We can think of a draw from as an experiment that results in a
211

212

Chapter 9

nonnumerical outcome ω ∈ Ω, such as “three heads and then two tails.” In order to
make the outcome of this experiment more amenable to analysis, we specify a function X : Ω → that maps outcomes into numbers. Suppose further that we wish to
evaluate the probability that X ≥ a, or

{ω ∈ Ω : X (ω ) ≥ a} :=:

{ X ≥ a} :=:

X −1 ([ a, ∞))

Since is only deﬁned on the sets in F , this requires X −1 ([ a, ∞)) ∈ F . The latter is
guaranteed by F -measurability of X.
Actually, the deﬁnition of random variables as real-valued functions is not general
enough for our purposes. We need to consider random “objects,” which are like (realvalued) random variables except that they take values in other spaces (such as n , or
abstract metric space). Some authors use the term “random object,” but we will call
them all random variables:
Deﬁnition 9.1.1 Let (Ω, F , ) be a probability space, and let (S, S ) be any measurable space. An S-valued random variable is a function X : Ω → S that is F , S measurable: X −1 ( B) ∈ F whenever B ∈ S . The distribution of X is the unique
measure μ X ∈ P (S, S ) deﬁned by
μ X ( B) :=

( X −1 ( B)) =

{ω ∈ Ω : X (ω ) ∈ B}

(B ∈ S )

Note that μ X , which gives the probability that X ∈ B for each B ∈ S , is just
the image measure ◦ X −1 of
under X (see page 183 for a discussion of image
measures). Distributions play a central role in probability theory.
A quick but important point on notation: In probability theory it is standard to use
the abbreviation { X has property P} for the set {ω ∈ Ω : X (ω ) has property P}. We
will follow this convention. Similarly, { X ∈ A} is the indicator function for the set
{ ω ∈ Ω : X ( ω ) ∈ A }.
Exercise 9.1.2 Let S be a metric space, and let (Ω, F ) be any measurable space. Let
f : Ω → S be F , B (S)-measurable, and let g : S → be a continuous function. Show
that X := g ◦ f is a random variable on (Ω, F ).
The integral of a real-valued random variable X on (Ω, F , ) is called its expectation, and written ( X ) or just X. That is,
X :=



Xd

:=:



X (ω ) (dω ) :=:

(X)

The deﬁnition on the far right is the linear functional notation for the integral, which
I personally prefer, as it eliminates the need for the new symbol , and forces us to
specify the underlying probability whenever we want to take expectations. However,

Measure-Theoretic Probability

213

the notation is more traditional, and is used in all of what follows—albeit with some
resentment on my part.
Our deﬁnition of has a sound probabilistic interpretation. If X is simple, taking
values α1 , . . . , α N on A1 , . . . , A N respectively, then the expectation X = ∑n αn ( An ),
which is the sum of all possible values taken by the random variable X multiplied by
the probability that each such value occurs. If X is not simple, then to calculate expectation we approximate X by simple functions (see page 177), in which case similar
intuition applies.
Exercise 9.1.3 Consider the probability space (S, S , δz ), where δz is the degenerate
probability
measure introduced in §7.1.3. The expectation of f ∈ mS + is f :=

f dδz . Intuitively, f = f (z), since we are sure that δz will pick out the point z.
Conﬁrm this intuition.
There is a close connection between distributions and expectations:
Theorem 9.1.4 If X is an S-valued random variable on (Ω, F , ) with distribution μ X ∈
P (S, S ), and if w ∈ mS is nonnegative or |w( X )| < ∞, then
w( X ) :=



w◦Xd

=



w dμ X :=: μ X (w)

(9.1)

This is just a special case of theorem 7.3.17 on page 182, which gives


w dμ X :=



w d(

◦ X −1 ) =



w◦Xd

Let (Ω, F , ) be a probability space, let X be a real-valued random variable on
this space, and let k ∈ . The k-th moment of X is ( X k ), which may or may not exist
as an expectation. By deﬁnition, existence of the expectation requires that | X |k < ∞.
The elementary inequality a j ≤ ak + 1 for all j ≤ k and a ≥ 0 can be used to prove
that if j ≤ k and the k-th moment is ﬁnite, then so is the j-th: By the previous bound
we have | X | j ≤ | X |k + Ω , where the inequality is understood to hold pointwise on
Ω (i.e., for all ω ∈ Ω). Referring to properties M1–M5 of the integral (page 179), we
conclude that | X | j ≤ | X |k + 1 < ∞.
Let X and Y be real-valued random variables with ﬁnite second moment. The
variance of X is the real number Var( X ) := [( X − X )2 ], while the covariance of X
and Y is
Cov( X, Y ) := [ ( X − X )(Y − Y ) ]
Exercise 9.1.5 Show that if X has ﬁnite second moment and if a and b are constants,
then Var( aX + b) = a2 Var( X ). Show in addition that if Cov( X, Y ) = 0, then Var( X +
Y ) = Var( X ) + Var(Y ).

214

Chapter 9

In the results that follow, we will often need to say something along the lines of “let
X be a random variable on (Ω, F , ) taking values in (S, S ) and having distribution
μ.” It is fortunate then that:
Theorem 9.1.6 Given any measurable space (S, S ) and μ ∈ P (S, S ), there exists a probability space (Ω, F , ) and a random variable X : Ω → S such that X has distribution μ.
Exercise 9.1.7 Prove theorem 9.1.6 by setting (Ω, F , ) = (S, S , μ) and X = the
identity map on S. Show, in particular, that X is measurable and has distribution μ.
Sometimes it’s useful to construct a supporting probability space a little more explicitly. Consider an arbitrary distribution in P ( ), which can be represented by a
cumulative distribution function H.1 For simplicity, we assume that H is strictly increasing (a discussion of the general case can be found in Williams 1991, ch. 3). Let
X : (0, 1) →
be the inverse of H: X = H −1 . Then X is a random variable on
(Ω, F , ) = ((0, 1), B (0, 1), λ) with distribution H. (Here λ is Lebesgue measure.)
Leaving measurability as an exercise, let’s conﬁrm that X has distribution H. Fix
z ∈ . Note that for each u ∈ (0, 1) we have X (u) ≤ z if and only if u ≤ H (z), and
hence

{ X ≤ z} := {u ∈ (0, 1) : X (u) ≤ z}
= {u ∈ (0, 1) : u ≤ H (z)} = λ((0, H (z)]) = H (z)
Thus X has distribution H, as was to be shown.
Next we introduce Chebychev’s inequality, which allows us to bound tail probabilities in terms of expectations—the latter being easier to calculate in many applications.
Theorem 9.1.8 Let X be an S-valued random variable on (Ω, F , ). If h : S →
measurable function and δ ∈ , then δ { h( X ) ≥ δ} ≤ h( X ).

+

is a

Proof. Observe that h( X ) ≥ h( X ) {h( X ) ≥ δ} ≥ δ {h( X ) ≥ δ} pointwise on Ω.
Now integrate, using properties M1–M5 as required.
Two special cases are used repeatedly in what follows: First, if X is a nonnegative
random variable then setting h( x ) = x gives

{ X ≥ δ} ≤

X
δ

Second, application of the bound to Y := X −

{| X − X | ≥ δ} =

( δ > 0)
X with h( x ) = x2 gives

{( X − X )2 ≥ δ2 } ≤

Var( X )
δ2

As we will see, (9.3) can be used to prove a law of large numbers.
1 Recall

theorem 7.1.33 on page 170.

(9.2)

( δ > 0)

(9.3)

Measure-Theoretic Probability

9.1.2

215

Independence

We have already used the concept of independence repeatedly in the text. It’s now
time for a formal deﬁnition.
Deﬁnition 9.1.9 Random variables X and Y taking values in (S, S ) and ( T, T ) respectively are said to be independent whenever

{ X ∈ A } ∩ {Y ∈ B } =

{ X ∈ A} · {Y ∈ B} for all A ∈ S and B ∈ T

More generally, a ﬁnite collection of random variables X1 , . . . , Xn with Xi taking values in (Si , Si ) is called independent if
n

n

i =1

i =1

∩in=1 { Xi ∈ Ai } = ∏ { Xi ∈ Ai } = ∏ μ Xi ( Ai )

(9.4)

for any sets with Ai ∈ Si . An inﬁnite collection of random variables is called independent if any ﬁnite subset of the collection is independent.
In (9.4), the right-hand side is the product of the marginal distributions. Hence
the joint distributions of independent random variables are just the product of their
marginals. The next result extends this “independence means multiply” rule from
probabilities to expectations.
Theorem 9.1.10 If X and Y are independent real-valued random variables with
and |Y | < ∞, then | XY | < ∞ and ( XY ) = X Y.

|X| < ∞

Exercise 9.1.11 Show that if X and Y are independent, then Cov( X, Y ) = 0.
One important consequence of independence is the following result
Theorem 9.1.12 (Fubini) Let X and Y be as above, and let S and T be subsets of n and k
respectively. Let h ∈ bB (S × T ) or mB (S × T )+ . In other words, h is a either a bounded or
a nonnegative Borel measurable function on the space where the pair ( X, Y ) takes values. If X
and Y are independent, then
h( X, Y ) =

 

h( x, y)μ X (dx )μY (dy) =

 

h( x, y)μY (dy)μ X (dx )

Things start to get interesting when we have an inﬁnite number of random variables on the one probability space indexed by a value that often represents time. These
collections of random variables are called stochastic processes. A formal deﬁnition
follows.

216

Chapter 9

Deﬁnition 9.1.13 Let (S, S ) be a measurable space, and let (Ω, F , ) be a probability
space. By an S-valued stochastic process we mean a tuple

(Ω, F , , ( Xt )t∈ )
where is an index set such as or
variables Xt all deﬁned on (Ω, F , ).

and ( Xt )t∈

is a family of S-valued random

With stochastic processes the ideas is that, at the “start of time,” a point ω is selected by “nature” from the set Ω according to the probability law (i.e., ( E) is the
probability that ω ∈ E). This is a once-off realization of all uncertainty, and Xt (ω )
simply reports the time t outcome for the variable of interest, as a function of that
realization.
The simplest kind of stochastic processes are the IID processes:
Deﬁnition 9.1.14 An S-valued stochastic process (Ω, F , , ( Xt )t∈ ) is called independent and identically distributed (IID) if the sequence ( Xt )t∈ is independent and
each Xt has the same distribution, in the sense that

{ Xt ∈ B } =

{ Xs ∈ B} for any s, t ∈

and any B ∈ S

For an IID process, any event that occurs at each t with nonzero probability occurs
eventually with probability one. To see this, suppose that (Ω, F , , ( Xt )t∈ ) is a IID,
and that the common distribution of each Xt is μ ∈ P (S, S ). Consider a set A ∈ S
with μ( A) > 0.
Exercise 9.1.15 Show that { Xt ∈
/ A, ∀t ∈ } ⊂ ∩t≤T { Xt ∈
/ A} for all T ∈ .2
Using this relation, show that { Xt ∈
/ A, ∀t ∈ } ≤ (1 − μ( A)) T for all T ∈ .
Conclude that this probability is zero, and hence that Xt ∈ A for at least one t ∈
with probability one.

9.1.3

Back to Densities

We have mentioned a few times that some but not all distributions can be represented
by densities. Let’s now clarify exactly when distributions do have density representations, as well as collecting some miscellaneous facts about densities.
n . Recall that a density on S is a function φ ∈ mB ( S )+
Let S be a Borel subset
 of

with the property that φ( x )dx :=: φdλ :=: λ(φ) = 1. The set of all densities on
S is denoted
 by D (S). Each density φ ∈ D (S) creates a distribution μφ ∈ P (S) via
μφ ( B) = B φ( x )dx.
2 Remember

that these are subsets of Ω.

Measure-Theoretic Probability

217

Exercise 9.1.16 Conﬁrm that this μφ is countably additive.3
Sometimes we can go the other way, from distributions to associated density. In
particular, suppose that S ∈ B ( n ), and let μ ∈ P (S). The distribution μ is said to
have a density representation φ if φ ∈ D (S) and
μ( B) =


B

φ( x ) dx

( B ∈ B (S))

(9.5)

However, the pairing of D (S) and P (S) in (9.5) is not a one-to-one correspondence.
Every density creates a distribution, but there are distributions in P (S) without such
an “integral” representation by an element of D (S). Here is an example:
Exercise 9.1.17 Let a ∈ , and let δa be the element of P ( ) that puts unit mass on a.
That is, δa ( B)= 1 if a ∈ B and zero otherwise. Argue that there is no φ ∈ D ( ) such
that δa ( B) = B φ( x )dx for all B ∈ B ( ).4
So when do density representations exist? The following rather fundamental theorem answers that question. The proof is omitted, but you can ﬁnd it in any text on
measure theory.
Theorem 9.1.18 (Radon–Nikodym) Let μ ∈ P (S), where S ∈ B ( n ), and let λ be the
Lebesgue measure. The distribution μ has a density representation if and only if μ( B) = 0
whenever B ∈ B (S) and λ( B) = 0.
When densities exist, they can make our life much easier. The next theorem indicates how density representations can be used to compute expectations by changing
the measure used to integrate from a given distribution to Lebesgue measure. Often
the transformation results in a standard Riemann integral, which can be solved using
calculus.
Theorem 9.1.19 Let S ∈ B ( n ). If distribution μ ∈ P (S) has density representation
φ ∈ D (S), and if h ∈ bB (S) or h ∈ mB (S)+ , then
μ(h) =



h( x )φ( x )dx

(9.6)

Proof. The proof follows a very standard argument, and is probably worth reading
through at least once. Let’s focus on the case of h ∈ bB (S). Suppose ﬁrst that h = B ,
where B ∈ B (S). For such an h the equality (9.6) holds by (9.5). Now suppose that
∞
3 Hint: If ( B ) is a disjoint sequence of sets, show that
n
∪ Bn = ∑n
properties M1–M5 of the integral.

4 Hint: Recall from theorem 7.3.9 that if λ ( B ) = 0, then
B φ ( x ) dx = 0.

Bn .

Complete the proof using

218

Chapter 9

h is a simple function: h ∈ sB (S), h = ∑nN=1 αn Bn , Bn ∈ B (S). Since the integral is
linear, we have



 N
N
N
N
μ ∑ αn Bn = ∑ αn μ( Bn ) = ∑ αn
Bn ( x ) φ ( x ) dx =
∑ αn Bn (x)φ(x)dx
n =1

n =1

n =1

n =1

In other words, (9.6) holds for h ∈ sB (S). Now let h ∈ bB (S) with h ≥ 0. By
lemma 7.2.11 (page 175) there is a sequence (sk ) ⊂ sB (S)+ with sk ↑ h. Since (9.6)
holds for each sk , we have
μ(sk ) =



sk ( x )φ( x )dx

(k ∈

)

Taking limits with respect to k and using the monotone convergence theorem gives
(9.6). Finally, for general h ∈ bB (S), we have h = h+ − h− , and another application
of linearity completes the proof.

9.2

General State Markov Chains

It’s time to develop a general theory of Markov chains on uncountably inﬁnite state
spaces.5 In chapter 8 we covered uncountable state spaces when the stochastic (density) kernel was a family of densities p( x, y)dy, one for each x in the state space. We
now drop the assumption that these distributions can be represented as densities and
permit them to be arbitrary probability measures.

9.2.1

Stochastic Kernels

For discrete time Markov chains of all shapes and forms, the most important primitive
is the stochastic kernel.6 You have already met some stochastic kernels: The ﬁrst was
the ﬁnite kernel p, living on a ﬁnite set S, with the property that p( x, y) ≥ 0 and
∑y∈S p( x, y) = 1. The second was the density kernel p( x, y)dy on a Borel subset S of
k . Here is the general (i.e., probability measure) case:
Deﬁnition 9.2.1 Let S be a Borel subset of n . A stochastic kernel on S is a family of
probability measures
P( x, dy) ∈ P (S)
( x ∈ S)
where x → P( x, B) is Borel measurable for each B ∈ B (S).7
5 In order to be consistent with earlier theory and what lies ahead, we stick to state spaces that are subsets
k.

The theory for abstract measure spaces differs little.
kernels are also called Markov kernels, or transition probability functions.
7 This last property is just a regularity condition to make sure that various integrals we want to use will
make sense.
of

6 Stochastic

Measure-Theoretic Probability

219

Each ﬁnite kernel p on ﬁnite S deﬁnes a general kernel P on S by
P( x, B) =

∑ p(x, y)

y∈ B

Each density kernel p on Borel set S ⊂
P( x, B) =


B

n

( x ∈ S, B ⊂ S)

deﬁnes a general kernel P on S by

p( x, y)dy

( x ∈ S, B ∈ B (S))

The next deﬁnition provides a link between Markov chains and kernels.
Deﬁnition 9.2.2 Let ψ ∈ P (S). A stochastic process ( Xt )t≥0 on S is called Markov( P, ψ) if
1. at time zero, X0 is drawn from ψ, and
2. at time t + 1, Xt+1 is drawn from P( Xt , dy).
If ψ = δx for some x ∈ S, then we say ( Xt )t≥0 is Markov-( P, x ).
While this deﬁnition is intended to parallel the ﬁnite and density case deﬁnitions
given on pages 71 and 188 respectively, it is time to address an issue that needs clariﬁcation: If the Markov-( P, ψ) process ( Xt )t≥0 is to be regarded a stochastic process, then,
by deﬁnition of stochastic processes (see page 216) it must be a sequence of S-valued
random variables, all deﬁned on a common probability space (Ω, F , ). In the deﬁnition above no probability space is mentioned, and it is not clear how ( Xt )t≥0 is deﬁned
as a sequence of functions from Ω to S.
While construction of the underlying probability space can be undertaken without
any additional assumptions—interested readers are referred to Pollard (2002, §4.8)
or Shiryaev (1996, p. 249)—the construction is usually redundant in economic applications because Markov chains typically present themselves in the form of stochastic recursive sequences (SRSs). Such representations simultaneously determine the
stochastic kernel P, provide the probability space (Ω, F , ), and furnish us with the
random variables ( Xt )t≥0 living on that space. Let’s see how this works, starting with
the following deﬁnition.
Deﬁnition 9.2.3 Let S ∈ B ( n ), Z ∈ B ( k ), φ ∈ P ( Z ), and ψ ∈ P (S). Let F : S ×
Z → S be Borel measurable. The canonical stochastic recursive sequence ( Xt )t≥0 is
deﬁned by
IID
Xt+1 = F ( Xt , Wt+1 ), (Wt )t≥1 ∼ φ, X0 ∼ ψ
(9.7)
The random variables (Wt )t≥1 and X0 are deﬁned on a common probability space
(Ω, F , ) and are jointly independent.

220

Chapter 9

In the deﬁnition, each Xt is deﬁned as a random variable on (Ω, F , ) as follows:
Given ω ∈ Ω, we have (Wt (ω ))t≥1 and X0 (ω ). From these, ( Xt (ω ))t≥0 is recursively
determined by
Xt+1 (ω ) = F ( Xt (ω ), Wt+1 (ω ))
Note that according to the deﬁnition, Xt is a function only of X0 and W1 , . . . , Wt . Hence
Xt and the current shock Wt+1 are independent.
There is a unique stochastic kernel P on S that represents the dynamics implied
by F and φ. To deﬁne P, we need to specify P( x, B) for arbitrary x ∈ S and B ∈
B (S), corresponding to the probability that Xt+1 ∈ B given Xt = x. Since Xt+1 =
F ( Xt , Wt+1 ), we get
P( x, B) =

{ F ( x, Wt+1 ) ∈ B} =

B [ F ( x, Wt+1 )]

(Recall that the expectation of an indicator function is equal to the probability of the
event it refers to.) Since Wt+1 is distributed according to φ ∈ P ( Z ), this becomes
P( x, B) =



B [ F ( x, z )] φ ( dz )

( x ∈ S, B ∈ B (S))

(9.8)

The integral is over the space Z on which the shock is deﬁned.
In what follows, whenever we introduce a Markov-( P, ψ) process ( Xt )t≥0 , it will be
implicitly assumed that P is derived from the canonical SRS via (9.8), and that ( Xt )t≥0
is the sequence of random variables deﬁned recursively in (9.7). This way, ( Xt )t≥0
is always a well-deﬁned stochastic process living on the probability space (Ω, F , )
that supports the shocks (Wt )t≥1 and initial condition X0 .8
Example 9.2.4 In §6.1 we introduced a stochastic Solow–Swan growth model where
output is a function f of capital k and a real-valued shock W. The sequence of productivity shocks (Wt )t≥1 is IID with distribution φ ∈ P ( ). Capital at time t + 1 is equal
to that fraction s of output saved last period, plus undepreciated capital. As a result
k t follows the law
k t+1 = s f (k t , Wt+1 ) + (1 − δ)k t
(9.9)
Let f :
We set

+×

→

+.

A suitable state space is S =

+ , and the shock space is Z

=

.

F ( x, z) = s f ( x, z) + (1 − δ) x
8 Two technical notes: Given a distribution φ on Z, there always exists a probability space ( Ω, F , ) and
an independent sequence of random variables (Wt )t≥1 on (Ω, F , ) such that the distribution of Wt is φ
(i.e., ◦ Wt−1 = φ) for each t. See, for example, Pollard (2002, §4.8). Second, there is no loss of generality in
assuming the existence of an SRS representation for a given kernel P on S. In fact every kernel on S can be
shown to have such a representation. See Bhattacharya and Majumdar (2007, §3.8) for details.

Measure-Theoretic Probability

221

which clearly maps S × Z into S. Using (9.8), the “Solow-Swan stochastic kernel” is
given by

P( x, B) =
B ( s f ( x, z ) + (1 − δ ) x ) φ ( dz )
Example 9.2.5 Consider the deterministic model Xt+1 = h( Xt ). Since P( x, B) is the
probability that Xt+1 ∈ B given Xt = x, we can set P( x, B) = B (h( x )) = h−1 ( B) ( x ).9
Example 9.2.6 Consider the linear model with correlated shocks given by
Yt+1 = αYt + ξ t+1
ξ t+1 = ρξ t + Wt+1
where all variables take values in
and (Wt )t≥1 is IID according to φ ∈ P ( ).
Although (Yt )t≥0 is not itself a Markov chain, the bivariate process given by Xt :=
(Yt , ξ t ) is Markov on 2 . It is a special case of the canonical SRS deﬁned in (9.7), with
S = 2 , Z = and


αy + ρξ + z
F ( x, z) = F [(y, ξ ), z] =
ρξ + z
If max{|α|, |ρ|} < 1, then the model has certain stability properties elaborated on
below.
Figures 9.1–9.3 give some idea of the dynamics for (Yt )t≥0 that can arise in the
linear correlated shock model. In ﬁgure 9.1 the shocks Wt are identically zero and the
parameters α and ρ are negative, causing oscillation. In ﬁgures 9.2 and 9.3 the shock is
N (0, 0.25) and the parameters α and ρ are nonnegative. In ﬁgure 9.2 the coefﬁcient ρ
is relatively large, leading to strong autocorrelation, while in 9.3 we set ρ = 0. In this
case the shocks are IID, (Yt )t≥0 is Markovian, and the autocorrelation is weaker.
Example 9.2.7 This next example is the so-called AR(p) model. It demonstrates that
Markov models are more general than they ﬁrst appear. Suppose that the state variable X takes values in , that (Wt ) is an independent and identically distributed sequence in , and that
Xt+1 = a0 Xt + a1 Xt−1 + · · · + a p−1 Xt− p+1 + Wt+1
Deﬁne Yt := ( Xt , Xt−1 , . . . , Xt− p+1 ), and consider the system
⎞
⎛
⎛ ⎞
a0
a1
...
a p −2
a p −1
1
⎟
⎜1
⎜0⎟
0
.
.
.
0
0
⎟
⎜
⎜ ⎟
Yt+1 = ⎜ .
⎟ Yt + ⎜ . ⎟ Wt+1
⎠
⎝ ..
⎝ .. ⎠
0

0

...

1

0

(9.10)

(9.11)

0

path to the same conclusion is by considering Xt+1 = h( Xt ) + Wt+1 where Wt = 0 with
probability one and then appealing to (9.8).
9 Another

Chapter 9

0
−5

Y

5

222

0

20

40

60

80

100

Time

Y

−20

−15

−10

−5

0

5

Figure 9.1 Correlated shocks, α = −0.9, ρ = −0.9, Wt ≡ 0

0

20

40

60

80

100

Time

Figure 9.2 Correlated shocks, α = 0.9, ρ = 0.9, Wt ∼ N (0, 0.25)

223

1
−1

0

Y

2

3

Measure-Theoretic Probability

0

20

40

60

80

100

Time

Figure 9.3 Correlated shocks, α = 0.9, ρ = 0.0, Wt ∼ N (0, 0.25)

The process (9.11) is an SRS with a well-deﬁned stochastic kernel. At the same time,
the ﬁrst element of Yt follows the process in (9.10).

9.2.2

The Fundamental Recursion, Again

On page 68 we showed that for a ﬁnite state Markov chain ( Xt )t≥0 with Markov operator M, the sequence of marginal distributions (ψt )t≥0 satisﬁes ψt+1 = ψt M. An analogous recursion was obtained for the density case by deﬁning the density Markov
operator ψM(y) = p( x, y)ψ( x )dx. Since a measure kernel P generalizes the ﬁnite
and density kernels, perhaps we can identify the general rule.
To begin, let S be a Borel subset of n , let P be a stochastic kernel on S, and let
( Xt )t≥0 be Markov-( P, ψ) for some ψ ∈ P (S). Writing ψt ∈ P (S) for the distribution
of Xt , we claim that the sequence (ψt )t≥0 ⊂ P (S) satisﬁes
ψt+1 ( B) =



P( x, B)ψt (dx )

( B ∈ B (S))

(9.12)

The intuition is the same as for the ﬁnite case: The probability that Xt+1 ∈ B is the
probability that Xt goes from x into B, summed across all x ∈ S, weighted by the
probability ψt (dx ) that Xt takes the value x.

224

Chapter 9

In verifying (9.12), let us assume that P is deﬁned by the canonical SRS (9.7). Picking any h ∈ bB (S), independence of Xt and Wt+1 plus theorem 9.1.12 (page 215) give
h ( X t +1 ) =

h[ F ( Xt , Wt+1 )] =

Specializing to the case h =
B ( X t +1 )

=

B

 

h[ F ( x, z)]φ(dz)ψt (dx )

∈ bB (S) gives

 

B [ F ( x, z )] φ ( dz ) ψt ( dx )

=



P( x, B)ψt (dx )

where the second inequality is due to (9.8). But
{ X t +1 ∈ B } =
B ( X t +1 ) =
ψt+1 ( B), conﬁrming (9.12).
When we studied ﬁnite and density Markov chains, we made extensive use of the
Markov operator. In both cases this operator was deﬁned in terms of the stochastic kernel and satisﬁed ψt+1 = ψt M for all t. Now let’s consider the general measure setting: Given stochastic kernel P, deﬁne the Markov operator M by as the map
P (S) φ → φM ∈ P (S), where
φM( B) :=



P( x, B)φ(dx )

( B ∈ B (S))

(9.13)

The next lemma veriﬁes that φM is a probability measure, and shows how to compute integrals of the form (φM)(h).
Lemma 9.2.8 If Q is any stochastic kernel on S and μ ∈ P (S), then the set function ν
deﬁned by

ν( B) =

Q( x, B)μ(dx )

( B ∈ B (S))

is an element of P (S), and for any h ∈ bB (S), we have
"

 !
h(y) Q( x, dy) μ(dx )
ν(h) :=: hdν =

(9.14)

Proof. Clearly, ν(S) = 1. Countable additivity of ν can be checked using either the
dominated or the monotone convergence theorem. The proof of (9.14) can be obtained
along the same lines as that of theorem 9.1.19 (page 217) and is left as an exercise.
As before, M acts on distributions to the left rather than the right. Using the
Markov operator, we can write the recursion (9.12) as ψt+1 = ψt M, which exactly
parallels our expression for the ﬁnite case (see (4.12) on page 76) and the density case
(see (8.15) on page 195). An inductive argument now conﬁrms that if X0 ∼ ψ, then
Xt ∼ ψMt .

Measure-Theoretic Probability

225

Example 9.2.9 In the case of the deterministic dynamical system Xt+1 = h( Xt ), recall
that P( x, B) = B (h( x )). Now suppose that X0 = x̄ (i.e., X0 ∼ δx̄ ). Our intuition tells
us that the distribution of X1 must then be δh( x̄) , and indeed
δx̄ M( B) =



B ( h ( x )) δx̄ ( dx )

∴

=

B ( h ( x̄ ))

= δh(x̄) ( B)

δx̄ M = δh( x̄)

Iterating forward, we ﬁnd that Xt ∼ δx̄ Mt = δht ( x̄) , as expected.
Given a kernel P, the higher order kernels ( Pt )t≥1 are deﬁned by
P1 := P,

Pt ( x, B) :=



P(z, B) Pt−1 ( x, dz)

( x ∈ S, B ∈ B (S))

These kernels are deﬁned so that Pt ( x, B) gives the probability of moving from x into
B in t steps, and Pt ( x, dy) is the distribution of Xt given X0 = x. To see this, observe
that the distribution of Xt given X0 = x is precisely δx Mt , so we are claiming that
δx Mt ( B) = Pt ( x, B)

( x ∈ S, B ∈ B (S), t ∈

)

This claim is a special case of the more general statement
φMt ( B) =



Pt ( x, B)φ(dx )

( φ ∈ P ( S ), B ∈ B ( S ), t ∈

)

which says that Mt , the t-th iterate of M, is the Markov operator corresponding to the
t-th-order kernel Pt .10 For the proof we refer to theorem 9.2.15 below.

9.2.3

Expectations

As before, let bB (S) be all the bounded measurable functions on S. We now introduce
a second operator, also called the Markov operator and also denoted by M, which
sends h ∈ bB (S) into Mh ∈ bB (S), where
Mh( x ) :=



h(y) P( x, dy)

( x ∈ S)

(9.15)

Intuitively, Mh( x ) represents the expectation of h( Xt+1 ) given Xt = x. This is the
same interpretation as the ﬁnite case, where we deﬁned Mh( x ) = ∑y∈S h(y) p( x, y).
We now have M acting on measures to the left and functions to the right. This is
analogous to the ﬁnite state notation, where ψM is the row vector ψ postmultiplied
10 This (unsurprising) result is the measure analogue of the density case stated in lemma 8.1.13 (page 196).

226

Chapter 9

by M, and Mh is the column vector h premultiplied by M. Stochastic kernels and the
operators ψ → ψM and h → Mh are in one-to-one correspondence via the identity
P( x, B) = δx M( B) = M

B (x)

( x ∈ S, B ∈ B (S))

(9.16)

In (9.15), h is restricted to be bounded so that the integral and hence Mh are well
deﬁned. On occasion it will be convenient to use the same operator notation when h is
unbounded. For example, if h is nonnegative and measurable, then the integral is well
deﬁned (although possibly inﬁnite) and the notation Mh is useful. In the remainder
of this section, however, M always acts on functions in bB (S).
The next few exercises show that the operator (9.15) has certain well-deﬁned properties. Knowing these properties makes manipulating expectations of functions of
( Xt )t≥0 straightforward.
Exercise 9.2.10 Verify that if h : S →
Exercise 9.2.11 Show that M
M).

S

=

is bounded (resp., nonnegative), then so is Mh.
S

pointwise on S (i.e., that

S

is a ﬁxed point of

Exercise 9.2.12 Show that M is monotone, in the sense that if h, g ∈ bB (S) and h ≤ g,
then Mh ≤ Mg (inequalities are pointwise on S).11
Exercise 9.2.13 Show that M is linear, in the sense that if h, g ∈ bB (S) and α, β ∈ ,
then M(αh + βg) = αMh + βMg.12

Often we will be dealing with the kernel P( x, B) =
B [ F ( x, z )] φ ( dz ) generated
by the canonical SRS deﬁned in (9.7) on page 219. In this case Mh takes the form
Mh( x ) :=



h(y) P( x, dy) =



h[ F ( x, z)]φ(dz)

(9.17)

This expression is intuitive because Mh( x ) represents the expectation of h( Xt+1 ) given
Xt = x, and h( Xt+1 ) = h[ F ( Xt , Wt+1 )]. Hence
Mh( x ) =

h[ F ( x, Wt+1 )] =



h[ F ( x, z)]φ(dz)

(9.18)

Here’s another way to get the same answer:
Exercise 9.2.14 Verify (9.17) using theorem 7.3.17 on page 182.13
11 Hint:

Use theorem 7.3.9 on page 181.
In other words, show that the function on the left-hand side equals the function on the right-hand
side at every point x∈ S. Use theorem 7.3.9 on page 181.
13 Hint: P ( x, B ) =
B [ F ( x, z )] φ ( dz ) is the image measure of φ under z → F ( x, z ).
12 Hint:

Measure-Theoretic Probability

227

The t-th iterate Mt h of h under M can be represented in terms of Pt :
Mt h( x ) =



h(y) Pt ( x, dy)

( x ∈ S)

(9.19)

We state a more general result immediately below (theorem 9.2.15). Before doing so,
note that since Pt ( x, dy) is the distribution of Xt given X0 = x, it follows from (9.19)
that Mt h can be interpreted as the conditional expectation
Mt h( x ) =

[ h ( X t ) | X0 = x ]

(9.20)

Theorem 9.2.15 Let P be a stochastic kernel on S. If M is the corresponding Markov operator,
then for every φ ∈ P (S), h ∈ bB (S) and t ∈ , we have
"
 !
(φMt )(h) = φ(Mt h) =
h(y) Pt ( x, dy) φ(dx )
We are using the linear functional notation for the ﬁrst two integrals. In traditional
notation,

(φMt )(h) =



h(y)(φMt )(dy)

and

φ(Mt h) =



(Mt h)( x )φ(dx )

Theorem 9.2.15 connects the iterates of φ → φM and those of h → Mh.14
Proof of theorem 9.2.15. Consider the case t = 1. We have
"

 !
h(y) P( x, dy) φ(dx ) = φM(h)
φ(Mh) = Mh( x )φ(dx ) =
where the ﬁnal equality is due to (9.14). It is an exercise for the reader to extend this
to general t using induction.

9.3

Commentary

The ﬁrst monograph to exposit the measure-theoretic foundations of probability is
Kolmogorov (1956)—originally published in 1933—which provides excellent historical perspective, and is still well worth reading. For general references on measuretheoretic probability, see Williams (1991), Breiman (1992), Shiryaev (1996), Durrett
(1996), Taylor (1997), Pollard (2002), Dudley (2002), and Schilling (2005).
For further background on general state Markov chains see Breiman (1992, ch. 7),
Meyn and Tweedie (1993, ch. 3), Durrett (1996, ch. 5), and Taylor (1997, ch. 3). For
a reference with economic applications see Stokey and Lucas (1989, ch. 8). Stability
of general state Markov chains is discussed in chapter 11, and the commentary at the
end of the chapter contains more pointers to the literature.
14 There

is an obvious parallel with the ﬁnite case. See (4.16) on page 79.

Chapter 10

Stochastic Dynamic
Programming
In this chapter we continue our study of intertemporal decision problems begun in
§5.1 and §6.2, working our way through a rigorous treatment of stochastic dynamic
programming. Intertemporal problems are challenging because they involve optimization in high dimensions; in fact the objective function is often deﬁned over a
space of inﬁnite dimension.1 We will see that studying the theory behind dynamic
programming is valuable not only for the understanding it provides, but also for developing numerical solution methods. Value iteration and policy iteration are the
most common techniques, and convergence of the algorithms is covered in some detail.

10.1

Theory

Our ﬁrst step is to give a careful deﬁnition of the problem. Once the deﬁnition is in
place we will go on to state and prove the basic principle of optimality for (inﬁnite
horizon, stationary) stochastic dynamic programs.

10.1.1

Statement of the Problem

For an inﬁnite horizon stochastic dynamic program (SDP), the scenario is one where
actions taken by an agent affect the future path of a state variable. Actions are spec1 We

have not deﬁned inﬁnite-dimensional space, which is an algebraic concept, but spaces of sequences
and spaces of functions typically have this property.

229

230

Chapter 10

iﬁed in terms of a policy function, which maps the current state of the system into a
given action. Each policy induces a Markov process on the state space, and different
processes give different levels of expected reward.
Each SDP has a “state” space S ∈ B ( n ), an “action” space A ∈ B ( m ) and a
nonempty correspondence Γ mapping x ∈ S into B ( A). The set Γ( x ) will be interpreted as the collection of all feasible actions for the agent when the state is x. We
set
gr Γ := {( x, u) ∈ S × A : u ∈ Γ( x )}
Below gr Γ (“graph” of Γ) is called the set of feasible state/action pairs.
Next we introduce a measurable “reward” function r : gr Γ →
and a discount
factor ρ ∈ (0, 1). Finally, let Z ∈ B ( k ) be a shock space, let (Wt )t≥1 be a sequence of
IID shocks with distribution φ ∈ P ( Z ), and let
F : gr Γ × Z

( x, u, z) → F ( x, u, z) ∈ S

be a measurable “transition function,“ which captures the dynamics. At the start of
time t the agent observes the state Xt ∈ S and responds with action Ut ∈ Γ( Xt ) ⊂ A.
After choosing Ut , the agent receives a reward r ( Xt , Ut ), and the state is updated
according to Xt+1 = F ( Xt , Ut , Wt+1 ). The whole process then repeats, with the agent
choosing Ut+1 , receiving reward r ( Xt+1 , Ut+1 ) and so on.
If our agent cared only about present rewards, the best action would be to choose
Ut = argmaxu∈Γ(Xt ) r ( Xt , u) at each date t. However, the agent cares about the future
too, and must therefore trade off maximizing current rewards against positioning the
state optimally in order to reap good rewards in future periods. The optimal decision
depends on how much he or she cares about the future, which is in turn parameterized
by the discount factor ρ. The role of ρ is clariﬁed below.
Example 10.1.1 Consider again the accumulation problem treated in §6.2. At the start
of time t, an agent has income yt , which is divided between consumption ct and savings k t . From consumption c the agent receives utility U (c), where U : + → .
Savings is added to the existing capital stock. For simplicity, assume that depreciation
is total: current savings and the stock of capital are equal. After the time t investment
decision is made, shock Wt+1 is observed. Production then takes place, yielding
yt+1 = f (k t , Wt+1 ),

IID

(Wt )t≥1 ∼ φ ∈ P ( Z ), Z ∈ B ( )

(10.1)

(See also ﬁgure 1.1 on page 2 for a depiction of the timing.) This ﬁts our SDP framework, with y ∈ S := + the state variable and k ∈ A := + the control. Γ is the
map S
y → [0, y] ⊂ A that deﬁnes feasible savings given income y. The reward
function r (y, k ) on gr Γ is U (y − k ). The transition function is F (y, k, z) = f (k, z). For
the present model it is independent of the state.

Stochastic Dynamic Programming

231

Clearly, some states in S are more attractive than others. High income positions us
well in terms of future consumption. Hence a trade off exists between consuming now,
which gives current reward, and saving, which places us at a more attractive point in
the state space tomorrow. Such trade-offs are the essence of dynamic programming.
Example 10.1.2 Now consider the same model but with correlated shocks. That is,
yt+1 = f (k t , ηt+1 ), where ηt+1 = g(ηt , Wt+1 ), g : + × + → + . This also ﬁts
the SDP framework, with the only modiﬁcation being that the state space has two
elements (y, η ) ∈ S := + × + , and the transition function F is


f (k, g(η, z))
F : (y, η, k, z) →
g(η, z)
The feasible correspondence Γ is the map sending (y, η ) into [0, y].
Returning to the general case, at minimum we need some regularity assumptions
on the primitives that will ensure that at least one solution to the SDP exists:
Assumption 10.1.3 The map r : gr Γ →

is continuous and bounded.

Assumption 10.1.4 Γ : S → B ( A) is continuous and compact valued.2
Assumption 10.1.5 gr Γ

( x, u) → F ( x, u, z) ∈ S is continuous, ∀z ∈ Z.

These continuity and compactness assumptions are all about guaranteeing existence of maximizers. For us the important implication of assumption 10.1.5 is that for
any w ∈ bcS, the function
gr Γ

( x, u) →



w[ F ( x, u, z)]φ(dz) ∈

is continuous. To see this, take any ( xn , un ) ⊂ gr Γ converging to some arbitrary
( x, u) ∈ gr Γ, and any w ∈ bcS. We need to show that


w[ F ( xn , un , z)]φ(dz) →



w[ F ( x, u, z)]φ(dz)

(n → ∞)

You can verify this using the dominated convergence theorem (page 182).

10.1.2

Optimality

In order to construct a sensible optimization problem, we will restrict the agent to
policies in the set of stationary Markov policies.3 For such a policy, the agent makes exactly the same decision after observing Xt = x as after observing Xt = x at some later
2 For
3 In

the deﬁnition of continuity for correspondences and a simple sufﬁcient condition see page 339.
fact it can be shown that every reasonable optimal policy is of this type.

232

Chapter 10

date t . This is intuitive because when looking toward the inﬁnite future, the agent
faces exactly the same trade-off (i.e., maximizing current rewards versus positioning
the state attractively next period), independent of whether the time is t or t .
Under a stationary Markov policy, the agent’s behavior is described by a Borel
measurable function σ mapping each possible x ∈ S into a feasible action u ∈ Γ( x ).
The interpretation is that if the current state is x ∈ S, then the agent responds with
action σ ( x ) ∈ Γ( x ). We let Σ denote the set of all Borel measurable σ : S → A with
σ( x ) ∈ Γ( x ) for all x ∈ S. In what follows we refer to Σ simply as the set of feasible
policies.
For each σ ∈ Σ, we obtain a stochastic recursive sequence
Xt+1 = F ( Xt , σ ( Xt ), Wt+1 ),

IID

(Wt )t≥1 ∼ φ

(10.2)

for the state ( Xt )t≥0 , and hence a stochastic kernel Pσ ( x, dy) on S given by
Pσ ( x, B) :=



B [ F ( x, σ ( x ), z )] φ ( dz )

( x ∈ S, B ∈ B (S))

We denote by Mσ the corresponding Markov operator. It is also convenient to deﬁne
the function
rσ : S x → r ( x, σ( x )) ∈
so that rσ ( x ) is the reward at x when the agent follows policy σ. Using operator
notation, expected rewards next period under policy σ can be expressed as
Mσ rσ ( x ) =



rσ (y) Pσ ( x, dy) =



rσ [ F ( x, σ( x ), z)]φ(dz)

( x ∈ S)

where the last equality follows from (9.17) on page 226.
The shocks (Wt )t≥1 are deﬁned on a ﬁxed probability space (Ω, F , ). Each ω ∈ Ω
picks out a sequence (Wt (ω ))t≥1 . Combining this sequence with an initial condition
X0 = x ∈ S and a policy σ yields a path ( Xt (ω ))t≥0 for the state:
Xt+1 (ω ) = F ( Xt (ω ), σ ( Xt (ω )), Wt+1 (ω )),

X0 ( ω ) = x

The reward corresponding to this path is random variable Yσ : Ω →
Yσ (ω ) :=

∞

∑ ρt rσ (Xt (ω ))

t =0

,

(ω ∈ Ω)

Exercise 10.1.6 Using boundedness of r, prove that this random variable is well deﬁned, in the sense that the sum converges for each ω ∈ Ω.4
4 Hint:

if ( xn ) is any sequence in

and ∑n | xn | converges, then so does ∑n xn .

Stochastic Dynamic Programming

233

The optimization problem for the agent is maxσ∈Σ




vσ ( x ) :=

∞

∑ ρ t r σ ( Xt )

Yσ :=:

t =0

and deﬁne the value function v∗ : S →

:=:

∞

Yσ . More precisely, if we set


∑ ρt rσ (Xt (ω ))

t =0

(dω )

as

v∗ ( x ) = sup vσ ( x )
σ∈Σ

( x ∈ S)

(10.3)

then a policy σ∗ ∈ Σ is called optimal if it attains the supremum in (10.3) for every
x ∈ S. In other words, σ∗ ∈ Σ is optimal if and only if vσ∗ = v∗ .
Exercise 10.1.7 Using the dominated convergence theorem, show that we can exchange limit and integral to obtain


vσ ( x ) :=

∞

∑ ρ t r σ ( Xt ) =

t =0

∞

∑ ρt

t =0

r σ ( Xt )

Now if h ∈ bB (S), then we can express h( Xt ) as Mtσ h( x ), where Mtσ is the t-th
iterate of the Markov operator Mσ and X0 = x (see (9.20) on page 227). As a result vσ
can be written as
vσ ( x ) =

∞

∑ ρt Mtσ rσ (x)

t =0

( x ∈ S)

(10.4)

As an aside, note that by theorem 9.2.15 (page 227), we have
Mtσ rσ ( x ) = (δx Mtσ )(rσ ) =



rσ (y) Pσt ( x, dy)

( x ∈ S)

(10.5)

Thus each policy σ ∈ Σ creates a Markov chain ( Xt )t≥0 starting at x, with corresponding marginal distributions (δx Mtσ )t≥0 . By integrating each distribution with rσ , the
reward corresponding to σ, and computing the discounted sum, we obtain a value for
the policy. This is our objective function, to be maximized over σ ∈ Σ.
Exercise 10.1.8 Show that the sup in (10.3) is well-deﬁned for each x ∈ S.
Deﬁnition 10.1.9 Given w ∈ bB (S), we deﬁne σ ∈ Σ to be w-greedy if



( x ∈ S)
σ ( x ) ∈ argmax r ( x, u) + ρ w[ F ( x, u, z)]φ(dz)

(10.6)

u∈Γ( x )

Lemma 10.1.10 Let assumptions 10.1.3–10.1.5 hold. If w ∈ bcS, then Σ contains at least
one w-greedy policy.

234

Chapter 10

The proof of this lemma is harder than it looks: On one hand, Since w ∈ bcS,
assumption 10.1.5 implies that the objective function on the right-hand side of (10.6)
is continuous with respect to u for each x. Since the constraint set Γ( x ) is compact, a
solution to the maximization problem exists. Thus for every x we can ﬁnd at least one
u∗x that attains the maximum, and the map x → u∗x certainly deﬁnes a function σ from
S → A satisfying (10.6). On the other hand, for this “policy” to be in Σ it must be Borel
measurable. Measurability is not immediately clear.
Fortunately there are “measurable selection” theorems stating that under the current assumptions we can ﬁnd at least one such x → u∗x that is measurable. We omit
the details, referring interested readers to Aliprantis and Border (1999, §17.3).5
We are now ready to state our main result on dynamic programming:
Theorem 10.1.11 Under assumptions 10.1.3–10.1.5, the value function v∗ is the unique
function in bB (S) that satisﬁes



∗
∗
( x ∈ S)
(10.7)
v ( x ) = sup r ( x, u) + ρ v [ F ( x, u, z)]φ(dz)
u∈Γ( x )

Moreover v∗ ∈ bcS. A feasible policy is optimal if and only if it is v∗ -greedy. At least one such
policy exists.
Before turning to the proof let’s make some brief comments and discuss an easy
application. As a preliminary observation, note that in view of (10.7), a policy σ ∈ Σ
is v∗ -greedy if and only if
v∗ ( x ) = r ( x, σ ( x )) + ρ



v∗ [ F ( x, σ ( x ), z)]φ(dz)

( x ∈ S)

(10.8)

In operator notation, this translates to v∗ = rσ + ρMσ v∗ .
Next let’s discuss how theorem 10.1.11 can be applied. One use is as a sufﬁcient
condition: We will see below that v∗ can be computed using value iteration. With
v∗ in hand, one can then compute a v∗ -greedy policy. Assuming Borel measurability,
we have found an optimal policy. The second way that we can use the theorem is as a
necessary condition for optimality. For example, suppose that we want to know about
the properties of optimal policies. We know that if σ∗ is optimal, then it satisﬁes (10.8).
We can (and do) use this to deduce facts about σ∗ .
As an application of theorem 10.1.11, consider again the optimal growth example
discussed on page 230. Recall that the state variable is income y ∈ S := + , the
control is savings k ∈ A := + , the feasible correspondence is Γ(y) = [0, y], the
reward function is r (y, k ) = U (y − k ), and the transition function is F (y, k, z) = f (k, z).
many of the applications considered here, solutions of the form x → u∗x described above are easily
seen to be measurable (being either continuous or monotone).
5 In

Stochastic Dynamic Programming

235

The shocks (Wt )t≥1 are independent and take values in Z ⊂
P ( Z ).
Assumption 10.1.12 The map U :
tion f is measurable and maps
k → f (k, z) is continuous.

according to φ ∈

→ + is bounded and continuous. The func× Z into + . For each ﬁxed z ∈ Z, the map

+
+

A feasible savings policy σ ∈ Σ is a Borel function from S to itself such that σ (y) ∈
[0, y] for all y. Every σ ∈ Σ deﬁnes a process for income via
yt+1 = f (σ (yt ), Wt+1 )

(10.9)

The corresponding stochastic kernel Pσ on S is given by
Pσ (y, B) =



B [ f ( σ ( y ), z )] φ ( dz )

(y ∈ S, B ∈ B (S))

Proposition 10.1.13 Under assumption 10.1.12, the value function v∗ is continuous, and is
the unique function in bB (S) that satisﬁes



∗
∗
(y ∈ S)
v (y) = max U (y − k) + ρ v [ f (k, z)]φ(dz)
0≤ k ≤ y

At least one optimal policy exists. Moreover a policy σ∗ is optimal if and only if it satisﬁes
∗

∗

v (y) = U (y − σ (y)) + ρ



v∗ [ f (σ∗ (y), z)]φ(dz)

(y ∈ S)

Exercise 10.1.14 Verify proposition 10.1.13 using assumption 10.1.12. In particular,
show that assumptions 10.1.3–10.1.5 on page 231 all hold.

10.1.3

Proofs

Let’s turn to the proof of theorem 10.1.11. As a preliminary step, we introduce two
important operators and investigate their properties. Many proofs in dynamic programming can be reduced to simple manipulations using these maps.
Deﬁnition 10.1.15 The operator Tσ : bB (S) → bB (S) is deﬁned for all σ ∈ Σ by
Tσ w( x ) = r ( x, σ ( x )) + ρ



w[ F ( x, σ ( x ), z)]φ(dz)

Further, the Bellman operator T : bB (S) → bB (S) is deﬁned by



Tw( x ) = sup r ( x, u) + ρ w[ F ( x, u, z)]φ(dz)
u∈Γ( x )

( x ∈ S)

( x ∈ S)

236

Chapter 10

Using the Bellman operator we can restate the ﬁrst part of theorem 10.1.11 as: v∗
is the unique ﬁxed point of T in bB (S).
Exercise 10.1.16 Show that both Tσ and T do in fact send bB (S) into itself. (Only
advanced students need worry about measurability. Just show that the images of
bounded functions are bounded.) If you need a hint on how to tackle T, refer to
lemma A.2.31 on page 334.
t t
Recalling the deﬁnition vσ := ∑∞
t=0 ρ Mσ rσ , our ﬁrst result is as follows:

Lemma 10.1.17 For every σ ∈ Σ, the operator Tσ is uniformly contracting on (bB (S), d∞ ),
with
 Tσ w − Tσ w ∞ ≤ ρw − w ∞
∀ w, w ∈ bB (S)
(10.10)
and the unique ﬁxed point of Tσ in bB (S) is vσ . In addition Tσ is monotone on bB (S), in the
sense that if w, w ∈ bB (S) and w ≤ w , then Tσ w ≤ Tσ w .
Here inequalities such as w ≤ w are pointwise inequalities on S.
Proof. The proof that Tσ is monotone is not difﬁcult, and is left to the reader (you
might want to use monotonicity of Mσ , as in exercise 9.2.12, page 226). Regarding the
claim that Tσ vσ = vσ , pointwise on S we have
vσ =

∞

∑ ρt Mtσ rσ = rσ +

t =0

∞

∑ ρt Mtσ rσ = rσ + ρMσ

t =1

∞

∑ ρt Mtσ rσ = rσ + ρMσ vσ = Tσ vσ

t =0

The only tricky part of this argument is passing Mσ through the limit in the inﬁnite
sum. Justifying this is a good exercise for the reader who wants to improve his or her
familiarity with the dominated convergence theorem.
The proof that Tσ is uniformly contracting is easy. Pick any w, w ∈ bB (S). Making
use of the linearity and monotonicity of Mσ , we have

| Tσ w − Tσ w | = |ρMσ w − ρMσ w | = ρ|Mσ (w − w )|
≤ ρMσ |w − w | ≤ ρMσ w − w ∞ S = ρw − w ∞
pointwise on S. The inequality (10.10) now follows.
Next we turn to the Bellman operator.
Lemma 10.1.18 The operator T is uniformly contracting on (bB (S), d∞ ), with

 Tw − Tw ∞ ≤ ρw − w ∞

∀ w, w ∈ bB (S)

(10.11)

In addition T is monotone on bB (S), in the sense that if w, w ∈ bB (S) and w ≤ w , then
Tw ≤ Tw .

Stochastic Dynamic Programming

237

Proof. The proof of the second claim (monotonicity) is easy and is left to the reader.
Before starting the proof of (10.11), we make the following observation: If w, w ∈
bB (S), then
| sup w − sup w | ≤ sup |w − w | =: w − w ∞
(10.12)
To see this, pick any such w, w . We have
sup w = sup(w − w + w ) ≤ sup(w − w ) + sup w ≤ sup |w − w | + sup w

∴

sup w − sup w ≤ sup |w − w |

The same argument reversing the roles of w and w ﬁnishes the job.
Now consider (10.11). For any w, w ∈ bB (S) and any x ∈ S, the deviation
| Tw( x ) − Tw ( x )| is equal to






sup r ( x, u) + ρ w[ F ( x, u, z)]φ(dz) − sup r ( x, u) + ρ w [ F ( x, u, z)]φ(dz)
u

u

Using (10.12), we obtain

| Tw( x ) − Tw ( x )| ≤ ρ sup
u

≤ ρ sup
u

≤ ρ sup
u





{w[ F ( x, u, z)] − w [ F ( x, u, z)]}φ(dz)
|w[ F ( x, u, z)] − w [ F ( x, u, z)]|φ(dz)
w − w ∞ φ(dz) = ρw − w ∞

Taking the supremum over x ∈ S gives the desired inequality.6
Exercise 10.1.19 Give an alternative proof that T is a uniform contraction of modulus
ρ by applying theorem 6.3.8 (page 149).
Now we turn to the ﬁrst claim in theorem 10.1.11. In operator notation, this translates to the following assertion:
Lemma 10.1.20 The value function v∗ is the unique ﬁxed point of T in bB (S). Moreover
v∗ ∈ bcS.
Proof. Since T is uniformly contracting on the complete space (bB (S), d∞ ), it follows
from Banach’s ﬁxed point theorem (theorem 3.2.36) that T has one and only one ﬁxed
point w∗ in bB (S).7 It remains to show that w∗ ∈ bcS and w∗ = v∗ .
6 This

proof is due to Hernández-Lerma and Lasserre (1996, lmm. 2.5).
∞ ) complete? Since ( bS, d ∞ ) is complete (theorem 3.2.6) and closed subsets of complete
spaces are complete (theorem 3.2.3), one need only show that bB (S) is a closed subset of (bS, d∞ ). This can
be proved via theorem 7.2.12 on page 176.
7 Why is ( bB ( S ), d

238

Chapter 10

To show that w∗ ∈ bcS, it is sufﬁcient to prove that bcS is a closed subset of bB (S),
and that T maps bcS into itself (see exercise 4.1.20 on page 61). That bcS is a closed subset of bB (S) was proved in theorem 3.2.7 (page 46). That T sends bcS into bcS follows
from Berge’s theorem. (Given w ∈ bcS, Tw is continuous by assumptions 10.1.3–10.1.5
and Berge’s theorem (page 340). Boundedness is clear.) Hence w∗ ∈ bcS as claimed.
Next we show that w∗ = v∗ . To begin, note that by lemma 10.1.10 there exists a
policy σ ∈ Σ satisfying Tw∗ = Tσ w∗ . (Why?) For this policy σ we have w∗ = Tw∗ =
Tσ w∗ . But vσ is the only ﬁxed point of Tσ , so w∗ = vσ . In which case w∗ ≤ v∗ , since
vσ ≤ v∗ for any σ ∈ Σ. It remains only to check that w∗ ≥ v∗ also holds.
To do this, pick an arbitrary σ ∈ Σ, and note that w∗ = Tw∗ ≥ Tσ w∗ . Iterating on
this inequality and using the monotonicity of Tσ , we obtain w∗ ≥ Tσk w∗ for all k ∈ .
Taking limits and using the fact that Tσk w∗ → vσ uniformly and hence pointwise, we
have w∗ ≥ vσ . Since σ is arbitrary it follows that w∗ ≥ v∗ . (Why?) Therefore w∗ = v∗
as claimed.
Our next task is to verify the claim that policies are optimal if and only if they are
v∗ -greedy.
Lemma 10.1.21 A policy σ ∈ Σ is optimal if and only if it is v∗ -greedy.
Proof. Recall that σ is v∗ -greedy if and only if it satisﬁes (10.8), which in operator
notation becomes v∗ = Tσ v∗ . This is equivalent to the statement vσ = v∗ , since vσ is
the unique ﬁxed point of Tσ . But vσ = v∗ says precisely that σ is optimal.
The last claim in theorem 10.1.11 is that at least one optimal policy exists. This now
follows from lemma 10.1.10.

10.2

Numerical Methods

Numerical solution of dynamic programming problems is challenging and, at the
same time, of great practical signiﬁcance. In earlier chapters we considered techniques for solving SDPs numerically, such as value iteration and policy iteration. In
this section we look more deeply at the theory behind these iterative methods. The
algorithms are shown to converge globally to optimal solutions.

10.2.1

Value Iteration

Consider the SDP deﬁned in §10.1. The fact that the Bellman operator is a uniform
contraction on bcS for which v∗ is the ﬁxed point gives us a natural way to approximate v∗ : Pick any v0 ∈ bcS and iterate the Bellman operator until T n v0 is close to

Stochastic Dynamic Programming

239

Algorithm 10.1 Value iteration algorithm
read in initial v0 ∈ bcS and set n = 0
repeat
set n = n + 1
set vn = Tvn−1 , where T is the Bellman operator
until a stopping rule is satisﬁed
solve for a vn -greedy policy σ (cf., deﬁnition 10.1.9)
return σ

v∗ . This suggests the algorithm for computing (approximately) optimal policies presented in algorithm 10.1.
Algorithm 10.1 is essentially the same as the earlier value iteration algorithms presented on page 103, although we have added the index n in order to keep track of
the iterates. Since vn = T n v0 converges to v∗ , after sufﬁciently many iterations the
resulting policy σ should have relatively good properties, in the sense that vσ  v∗ .8
Two obvious questions arise: First, what stopping rule should be used in the loop?
We know that vn → v∗ , but v∗ is not observable. How can we measure the distance
between v∗ and vn for given n? Second, for given vn , how close to being optimal is the
vn -greedy policy σ that the algorithm produces? These questions are answered in the
next theorem.
Theorem 10.2.1 Let v0 ∈ bcS. Fix n ∈
operator. If σ ∈ Σ is vn -greedy, then

v∗ − vσ ∞ ≤

, and let vn := T n v0 , where T is the Bellman
2ρ
 v n − v n −1  ∞
1−ρ

(10.13)

The next corollary follows directly (the proof is an exercise).
Corollary 10.2.2 Let (vn )n≥0 be as in theorem 10.2.1. If (σn )n≥0 is a sequence in Σ such
that σn is vn -greedy for each n ≥ 0, then v∗ − vσn ∞ → 0 as n → ∞.
The proof of theorem 10.2.1 is given at the end of this section. Before turning to it,
let us make some comments on the theorem.
First, theorem 10.2.1 bounds the deviation between the vn -greedy policy σ and the
optimal policy σ∗ in terms of their value. The value of σ∗ is given by vσ∗ , which by
deﬁnition is equal to v∗ . As a result we can say that given any initial condition x,
vσ∗ ( x ) − vσ ( x ) = |vσ∗ ( x ) − vσ ( x )| ≤ v∗ − vσ ∞ ≤
8 Of

course if vn is exactly equal to v∗ , then σ is optimal, and vσ = v∗ .

2ρ
 v n − v n −1  ∞
1−ρ

240

Chapter 10

Of course one can also seek to bound some kind of geometric deviation between σ
and σ∗ , but in applications this is usually less important than bounding the difference
between their values.
Second, the usefulness of this theorem comes from the fact that vn − vn−1 ∞ is
observable. In particular, it can be measured at each iteration of the algorithm. This
provides a natural stopping rule: Iterate until vn − vn−1 ∞ is less than some tolerance
, and then compute a vn -greedy policy σ. The policy satisﬁes v∗ − vσ ∞ ≤ 2ρ/(1 −
ρ ).
Third, the bound (10.13) is often quite conservative. From this perspective, theorem 10.2.1 might best be viewed as a guarantee that the output of the algorithm
converges to the true solution—such a guarantee is indispensable for numerical algorithms in scientiﬁc work.
Finally, on a related point, if you wish to supply bounds for a particular solution
you have computed, then relative optimality bounds are easier to interpret. A relative
bound establishes that an approximate optimal policy earns at least some fraction
(say, 95%) of maximum value. Here is an example: Suppose that the reward function
is nonnegative, so vσ and v∗ are nonnegative on S.9 Suppose further that we choose
v0 to satisfy 0 ≤ v0 ≤ v∗ , in which case 0 ≤ vn ≤ v∗ for all n ∈ by monotonicity of
T. By (10.13) and the fact that vn ≤ v∗ , we have

vn − vn−1 ∞ ≤ η =⇒

v∗ ( x ) − vσ ( x )
2ρ
η
·
≤
=: αn (η, x )
∗
v (x)
1 − ρ vn ( x )

(10.14)

In other words, if one terminates the value iteration at vn − vn−1 ∞ ≤ η, then the
resulting policy σ obtains at least (1 − αn (η, x )) × 100% of the total value available
when the initial condition is x.
Let’s ﬁnish with the proof of theorem 10.2.1:
Proof of theorem 10.2.1. Note that

v∗ − vσ ∞ ≤ v∗ − vn ∞ + vn − vσ ∞

(10.15)

First let’s bound the ﬁrst term on the right-hand side of (10.15). Using the fact that v∗
is a ﬁxed point of T, we get

v∗ − vn ∞ ≤ v∗ − Tvn ∞ +  Tvn − vn ∞ ≤ ρv∗ − vn ∞ + ρvn − vn−1 ∞
∴

v∗ − vn ∞ ≤

ρ
 v n − v n −1  ∞
1−ρ

(10.16)

9 Since r is already assumed to be bounded, there is no loss of generality in taking r as nonnegative, in
the sense that adding a constant to r produces a monotone transformation of the objective function vσ ( x ),
and hence does not alter the optimization problem.

Stochastic Dynamic Programming

241

Now consider the second term on the right-hand side of (10.15). Since σ is vn -greedy,
we have Tvn = Tσ vn , and

vn − vσ ∞ ≤ vn − Tvn ∞ +  Tvn − vσ ∞ ≤  Tvn−1 − Tvn ∞ +  Tσ vn − Tσ vσ ∞
∴

 v n − v σ  ∞ ≤ ρ  v n −1 − v n  ∞ + ρ  v n − v σ  ∞
ρ
∴ vn − vσ ∞ ≤
 v n − v n −1  ∞
1−ρ
Together, (10.15), (10.16), and (10.17) give us (10.13).

10.2.2

(10.17)

Policy Iteration

Aside from value function iteration, there is another iterative procedure called policy
iteration, which we ﬁrst met in §5.1.3. In this section we describe policy iteration, and
some of its convergence properties. The basic algorithm is presented in algorithm 10.2.
Algorithm 10.2 Policy iteration algorithm
read in an initial policy σ0 ∈ Σ
set n = 0
repeat
t t
evaluate vσn := ∑∞
t=0 ρ Mσn rσn
compute a vσn -greedy policy σn+1 ∈ Σ
set n = n + 1
until a stopping rule is satisﬁed
return σn

It turns out that the sequence of functions vσn produced by algorithm 10.2 converges to v∗ , and, with a sensible stopping rule, the resulting policy is approximately
optimal. Let’s clarify these ideas, starting with the following observation:
Lemma 10.2.3 If (σn )n≥0 is a sequence in Σ generated by the policy iteration algorithm, then
vσn ≤ vσn+1 holds pointwise on S for all n.
Proof. Pick any x ∈ S and n ∈

. By deﬁnition,



σn+1 ( x ) ∈ argmax r ( x, u) + ρ vσn [ F ( x, u, z)]φ(dz)
u∈Γ( x )

From this and the fact that vσ = Tσ vσ for all σ ∈ Σ, we have
vσn ( x ) = r ( x, σn ( x )) + ρ



≤ r ( x, σn+1 ( x )) + ρ

vσn [ F ( x, σn ( x ), z)]φ(dz)



vσn [ F ( x, σn+1 ( x ), z)]φ(dz)

242

Chapter 10

Rewriting in operator notation, this inequality becomes vσn ≤ Tσn+1 vσn . Since Tσn+1
is monotone (lemma 10.1.17), iteration with Tσn+1 yields vσn ≤ Tσkn+1 vσn for all k ∈ .
Taking limits, and using the fact that Tσkn+1 vσn → vσn+1 uniformly and hence pointwise
on S, we obtain the conclusion of the lemma.
It turns out that just as the value iteration algorithm is globally convergent, so too
is the policy iteration algorithm.
Theorem 10.2.4 If (σn )n≥0 ⊂ Σ is a sequence generated by the policy iteration algorithm,
then vσn − v∗ ∞ → 0 as n → ∞.
Proof. Let wn := T n vσ0 , where T is the Bellman operator, and, as usual, T 0 is the
identity map. Since vσn ≤ v∗ for all n ≥ 0 (why?), it is sufﬁcient to prove that wn ≤ vσn
for all n ≥ 0. (Why?) The claim is true for n = 0 by deﬁnition. Suppose that it is true
for arbitrary n. Then it is true for n + 1, since
wn+1 = Twn ≤ Tvσn = Tσn+1 vσn ≤ Tσn+1 vσn+1 = vσn+1
You should not have too much trouble verifying these statements.
The rest of this section focuses mainly on policy iteration in the ﬁnite case, which
we treated previously in §5.1.3. It is proved that when S and A are ﬁnite, the exact
optimal policy is obtained in ﬁnite time.
Lemma 10.2.3 tells us that the value of the sequence (σn ) is nondecreasing. In fact,
if σn+1 cannot be chosen as equal to σn , then the value increase is strict. On the other
hand, if σn+1 can be chosen as equal to σn , then we have found an optimal policy. The
next lemma makes these statements precise.
Lemma 10.2.5 Let (σn ) be a sequence of policies generated by the policy iteration algorithm.
If σn+1 cannot be chosen as equal to σn , in the sense that there exists an x ∈ S such that

/ argmax r ( x, u) + ρ
σn ( x ) ∈
u∈Γ( x )




vσn [ F ( x, u, z)]φ(dz)

then vσn+1 ( x ) > vσn ( x ). Conversely, if σn+1 can be chosen as equal to σn , in the sense that

σn ( x ) ∈ argmax r ( x, u) + ρ
u∈Γ( x )

then vσn = v∗ and σn is an optimal policy.




vσn [ F ( x, u, z)]φ(dz)

∀x ∈ S

Stochastic Dynamic Programming

243

Proof. Regarding the ﬁrst assertion, let x be a point in S with
r ( x, σn+1 ( x )) + ρ



vσn [ F ( x, σn+1 ( x ), z)]φ(dz)

> r ( x, σn ( x )) + ρ



vσn [ F ( x, σn ( x ), z)]φ(dz)

Writing this in operator notation, we have Tσn+1 vσn ( x ) > Tσn vσn ( x ) = vσn ( x ). But
lemma 10.2.3 and the monotonicity of Tσn+1 now yield
vσn+1 ( x ) = Tσn+1 vσn+1 ( x ) ≥ Tσn+1 vσn ( x ) > vσn ( x )
Regarding the second assertion, suppose that



σn ( x ) ∈ argmax r ( x, u) + ρ vσn [ F ( x, u, z)]φ(dz)
u∈Γ( x )

∀x ∈ S

It follows that


vσn ( x ) = r ( x, σn ( x )) + ρ vσn [ F ( x, σn ( x ), z)]φ(dz)



= max r ( x, u) + ρ vσn [ F ( x, u, z)]φ(dz)
u∈Γ( x )

for every x ∈ S. In other words, vσn is the ﬁxed point of the Bellman operator. In
which case vσn = v∗ , and the proof is done.

Algorithm 10.3 Policy iteration, ﬁnite case
read in initial σ0 ∈ Σ
set n = 0
repeat
set n = n + 1
t t
evaluate vσn−1 = ∑∞
t=0 ρ Mσn−1 rσn−1
taking σn = σn−1 if possible, compute a vσn−1 -greedy policy σn
until σn = σn−1
return σn

Algorithm 10.3 adds a stopping rule to algorithm 10.2, which is suitable for the
ﬁnite case. The algorithm works well in the ﬁnite state/action case because it always
terminates in ﬁnite time at an optimal policy. This is the content of our next theorem.

244

Chapter 10

Theorem 10.2.6 If S and A are ﬁnite, then the policy iteration algorithm always terminates
after a ﬁnite number of iterations, and the resulting policy is optimal.
Proof. First note that if the algorithm terminates at n with σn+1 = σn , then this policy is optimal by the second part of lemma 10.2.5. Next suppose that the algorithm
never terminates, generating an inﬁnite sequence of policies (σn ). At each stage n the
stopping rule implies that σn+1 cannot be chosen as equal to σn , and the ﬁrst part of
lemma 10.2.5 applies. Thus vσn < vσn+1 for all n (i.e., vσn ≤ vσn+1 and vσn = vσn+1 ).
But the set of maps from S to A is clearly ﬁnite, and hence so is the set of functions
{vσ : σ ∈ Σ}. As a result such an inﬁnite sequence is impossible, and the algorithm
always terminates.

10.2.3

Fitted Value Iteration

In §6.2.2 we began our discussion of ﬁtted value iteration and presented the main
algorithm. Recall that to approximate the image Tv of a function v we evaluated
Tv at ﬁnite set of grid points ( xi )ik=1 and then used these samples to construct an
approximation to Tv. In doing so, we decomposed T̂ into the two operators L and T:
First T is applied to v at each of the grid points, and then an approximation operator
L sends the result into a function w = T̂v = L( Tv). Thus T̂ := L ◦ T :=: LT. We saw
that LT is uniformly contracting whenever L is nonexpansive with respect to d∞ .
In general, we will consider a map L : bB (S) → F ⊂ bB (S) where, for each
v ∈ bB (S), the approximation Lv ∈ F is constructed based on a sample (v( xi ))ik=1 on
grid points ( xi )ik=1 . In addition, L is chosen to be nonexpansive:

 Lv − Lw∞ ≤ v − w∞

∀ v, w ∈ bB (S)

(10.18)

Example 10.2.7 (Piecewise constant approximation) Let ( Pi )ik=1 be a partition of S, in
that Pm ∩ Pn = ∅ when m = n and S = ∪ik=1 Pi . Each Pi contains a single grid point xi .
For any v : S → we deﬁne v → Mv by
Mv( x ) =

k

∑ v ( xi )

i =1

Pi ( x )

( x ∈ S)

Exercise 10.2.8 Show that for any w, v ∈ bB (S) and any x ∈ S we have

| Mw( x ) − Mv( x )| ≤ sup |w( xi ) − v( xi )|
1≤ i ≤ k

Using this result, show that the operator M is nonexpansive on (bB (S), d∞ ).

Stochastic Dynamic Programming

245

Example 10.2.9 (Continuous piecewise linear interpolation) Let’s focus on the onedimensional case. Let S = [ a, b], and let the grid points be increasing:
x1 < . . . < x k ,

x1 = a and xk = b

Let N be the operator that maps w : S → into its continuous piecewise afﬁne interpolant deﬁned by the grid. That is to say, if x ∈ [ xi , xi+1 ] then
Nw( x ) = λw( xi ) + (1 − λ)w( xi+1 )

where

λ :=

x i +1 − x
x i +1 − x i

Exercise 10.2.10 Show that for any w, v ∈ bB (S) and any x ∈ S we have

| Nw( x ) − Nv( x )| ≤ sup |w( xi ) − v( xi )|
1≤ i ≤ k

Using this result, show that N is nonexpansive.
Algorithm 10.4 FVI algorithm
read in initial v0 ∈ bB (S) and set n = 0
repeat
set n = n + 1
sample Tvn−1 at a ﬁnite set of grid points
compute T̂vn−1 = LTvn−1 from the samples
set vn = T̂vn−1
until the deviation vn − vn−1 ∞ falls below some tolerance
solve for a vn -greedy policy σ

Our algorithm for ﬁtted value iteration is given in algorithm 10.4. It terminates
in ﬁnite time for any strictly positive tolerance, since T̂ is a uniform contraction. The
policy it produces is approximately optimal, with the deviation given by the following
theorem. (The proof is given in the appendix to this chapter.)
Theorem 10.2.11 Let v0 ∈ F and let vn := T̂ n v0 , where T̂ := LT and L : bB (S) → F is
nonexpansive. If σ ∈ Σ is vn -greedy, then

v∗ − vσ ∞ ≤

2
× (ρvn − vn−1 ∞ +  Lv∗ − v∗ ∞ )
(1 − ρ )2

Most of the comments given after theorem 10.2.1 (page 239) apply to theorem
10.2.11. In particular, the bound is conservative, but it shows that the value of σ
can be made as close to that of σ∗ as desired, provided that L can be chosen so that

246

Chapter 10

 Lv∗ − v∗ ∞ is arbitrarily small. In the case of continuous piecewise linear interpolation on S = [ a, b] this is certainly possible.10

10.3

Commentary

A ﬁrst-rate theoretical treatment of stochastic dynamic programming can be found
in the two monographs of Hernández-Lerma and Lasserre (1996, 1999). Also recommended are Bertsekas (1995) and Puterman (1994). From the economics literature see,
for example, Stokey and Lucas (1989) or Le Van and Dana (2003). All these sources
contain extensive references to further applications.
Additional discussion of ﬁtted value iteration with nonexpansive approximators
can be found in Gordon (1995) and Stachurski (2008). For alternative discussions of
value iteration, see, for example, Santos and Vigo-Aguiar (1998) or Grüne and Semmler (2004).
Value iteration and policy iteration are two of many algorithms proposed in the
literature. Other popular techniques include projection methods, Taylor series approximation (e.g., linearization), and parameterized expectations. See, for example,
Marcet (1988), Tauchen and Hussey (1991), Judd (1992), Den Haan and Marcet (1994),
Rust (1996), Judd (1998), Christiano and Fisher (2000), McGrattan (2001), Uhlig (2001),
Maliar and Maliar (2005), or Canova (2007). Marimon and Scott (2001) is a useful
survey, while Santos (1999) and Aruoba et al. (2006) provide numerical comparisons.
Dynamic programming has many interesting applications in economics that we
will have little chance to discuss. Two unfortunate omissions are industrial organization (e.g., Green and Porter 1984, Hopenhayn 1992, Ericson and Pakes 1995, or
Pakes and McGuire 2001) and search theory (see McCall 1970 for an early contribution and Rogerson et al. 2005 for a recent survey). For some earlier classics the
macroeconomics literature, try Lucas and Prescott (1971), Hall (1978), Lucas (1978),
Kydland and Prescott (1982), Brock (1982), or Mehra and Prescott (1985). Dechert and
O’Donnell (2006) provide a nice application of dynamic programming to environmental economics.

10 This is due to continuity of v∗ , which always holds under our assumptions. When v∗ is continuous on
[ a, b] it is also uniformly continuous, which is to say that for any  > 0 there is a δ > 0 such that |v∗ ( x ) −
v∗ (y)| <  whenever | x − y| < δ. From this property it is not too difﬁcult to show that given any  > 0,
a sufﬁciently ﬁne grid will yield a continuous piecewise afﬁne interpolant Lv∗ such that v∗ − Lv∗ ∞ < .
See, for example, Bartle and Sherbet (1992).

Chapter 11

Stochastic Dynamics
It’s now time to give a treatment of stability for general Markov chains on uncountably
inﬁnite state spaces. Although the stability theory we used to study the ﬁnite case
(chapter 4) and the density case (chapter 8) does not survive the transition without
some modiﬁcation, the underlying ideas are similar, and connections are drawn at
every opportunity. Throughout this chapter we take S to be a Borel subset of n .

11.1

Notions of Convergence

Before considering the dynamics of general state Markov chains, we need to develop
notions of convergence that apply to the measure (as opposed to ﬁnite probability or
density) setting. Along the way we will cover some fundamental results in asymptotic
probability, including the weak and strong laws of large numbers for IID sequences.

11.1.1

Convergence of Sample Paths

Recall that an S-valued stochastic process is a tuple (Ω, F , , ( Xt )t∈ ) where (Ω, F )
is a measurable space, is a probability on (Ω, F ), is an index set such as or ,
and ( Xt )t∈ is a family of S-valued random variables on (Ω, F ). Various notions of
convergence exist for stochastic processes. We begin with almost sure convergence.
Deﬁnition 11.1.1 Let (Ω, F , , ( Xt )t≥1 ) be an S-valued stochastic process and let X
be an S-valued random variable on (Ω, F , ). We say that ( Xt )t≥1 converges almost
surely (alternatively, with probability one) to X if




lim Xt = X :=
ω ∈ Ω : lim Xt (ω ) = X (ω ) = 1
t→∞

t→∞

247

248

Chapter 11

In the language of §7.3.2, almost sure convergence is just convergence
erywhere.

-almost ev-

Almost sure convergence plays a vital role in probability theory, although you may
wonder why we don’t just require that convergence occurs for every ω ∈ Ω, instead
of just those ω in a set of probability one. The reason is that convergence for every
path is too strict: In almost all random systems, aberrations can happen that cause
such convergence to fail. Neglecting probability zero events allows us to obtain much
more powerful conclusions.
Exercise 11.1.2 Expectations are not always very informative measures of stochastic
processes dynamics. For example, consider a stochastic process with probability space
((0, 1), B (0, 1), λ) and random variables Xn := n2 (0,1/n) . Show that Xn → 0 almost
surely, while Xn ↑ ∞.
Here is another important notion of convergence of random variables:
Deﬁnition 11.1.3 Let (Ω, F , , ( Xt )t≥1 ) and X be as above. We say that ( Xt )t≥1 converges in probability to X if, for all  > 0,
lim

t→∞

{ Xt − X  ≥ } := lim

t→∞

{ω ∈ Ω :  Xt (ω ) − X (ω ) ≥ } = 0

Convergence in probability is weaker than almost sure convergence:
Lemma 11.1.4 If Xt → X almost surely, then Xt → X in probability.
Proof. Fix  > 0. Since Xt → X almost surely, { Xt − X  ≥ } → 0 -almost
everywhere on Ω. (Why?) An application of the dominated convergence theorem
(page 182) now gives the desired result.
The converse is not true. It is an optional (and nontrivial) exercise for you to construct a stochastic process that converges to some limit in probability, and yet fails to
converge to that same limit almost surely.
Exercise 11.1.5 The sets in deﬁnition 11.1.3 are measurable. Show, for example, that if
S = and  > 0, then {| Xn − X | ≥ } ∈ F .
Let’s discuss some applications of almost sure convergence and convergence in
probability. Let ( Xt )t≥1 be a real-valued stochastic process on (Ω, F , ) with common
expectation m. Deﬁne X̄n := n−1 ∑nt=1 Xt . The (weak) law of large numbers (WLLN)
states that under suitable assumptions the sample mean X̄n converges in probability
to m as n → ∞. We begin with the case m = 0.

Stochastic Dynamics

249

Exercise 11.1.6 We will make use of the following identity, the proof of which is left
as an exercise (use induction):
2

for all ( a1 , . . . , an ) ∈

n

n

,

∑ ai

i =1

=

∑

1≤i,j≤n

ai a j =

n

n

∑ ∑ ai a j

j =1 i =1

Exercise 11.1.7 Suppose the real, zero-mean sequence ( Xt )t≥1 satisﬁes
1. Cov( Xi , X j ) = 0 for all i = j, and
2. Cov( Xi , Xi ) = Var( Xi ) =

Xi2 ≤ M for all i ∈

.

Show that Var( X̄n ) ≤ M/n for all n ∈ . (Regarding the ﬁrst property, we usually
say that the sequence is pairwise uncorrelated.)
Exercise 11.1.8 Now prove that the WLLN holds for this sequence. As a ﬁrst step,
show that {| X̄n | ≥ } ≤ M/(n2 ).1 Conclude that X̄n converges to zero in probability as n → ∞.
This result can be extended to the case m = 0 by considering the zero-mean sequence Yt := Xt − m. This gives us
Theorem 11.1.9 Let (Ω, F , , ( Xt )t≥1 ) be a real-valued stochastic process with common
mean m. If ( Xt )t≥1 is pairwise uncorrelated and Var ( Xt ) ≤ M for all t, then X̄n → m in
probability. In particular,

{| X̄n − m| ≥ } ≤

M
n2

( > 0, n ∈

)

(11.1)

When the sequence is independent a stronger result holds:
Theorem 11.1.10 Let (Ω, F , , ( Xt )t≥1 ) be a real-valued stochastic process. If ( Xt )t≥1 is
IID and | X1 | < ∞, then X̄n → m almost surely.
Theorem 11.1.10 is called the strong law of large numbers (SLLN). A version of this
theorem was stated previously on page 94. For a proof, see Dudley (2002, thm. 8.3.5).
How about stochastic processes that are correlated, rather than IID? We have already presented some generalizations of the SLLN that apply to Markov chains (theorem 4.3.33 on page 94 and theorem 8.2.15 on page 206). Although the proofs of these
strong LLNs are beyond the scope of this book, let’s take a look at the proof of a weak
LLN for correlated processes. (Readers keen to progress can skip to the next section
without loss of continuity.) We will see that the correlations Cov( Xi , Xi+k ) must converge to zero in k sufﬁciently quickly. The following lemma will be useful:
1 Hint:

Use the Chebychev inequality (see page 214).

250

Chapter 11

Lemma 11.1.11 Let ( β k )k≥1 be a sequence in + , and let (Ω, F , , ( Xt )t≥1 ) be a realvalued stochastic process such that Cov( Xi , Xi+k ) ≤ β k for all i ≥ 1. If ( β k )k≥0 satisﬁes
∑k≥1 β k < ∞, then Var( X̄n ) → 0 as n → ∞.
Proof. We have

1
Var
n



n

∑ Xi

i =1



∴

Var

1
n

=

1
n2

=

1
n2

≤

2
n2

n

∑ Xi

i =1

∑

1≤i,j≤n

Cov( Xi , X j )

n

2

∑ Cov(Xi , Xi ) + n2

i =1

∑

1≤ i ≤ j ≤ n



≤

2
n2

∑

1≤ i < j ≤ n

Cov( Xi , X j ) =

n −1

∑ (n − k) β k ≤

k =0

2
n2

2
n

Cov( Xi , X j )

n −1 n − k

∑ ∑ Cov(Xi , Xi+k )

k =0 i =1

n −1

∑ βk ≤

k =0

2
n

∞

∑ βk → 0

k =0

Now we can give a weak LLN for correlated, non-identically distributed random
variables:
Theorem 11.1.12 Let (Ω, F , , ( Xt )t≥1 ) be a real-valued stochastic process. If
1. Cov( Xi , Xi+k ) ≤ β k for all i ≥ 1 where ∑k≥0 β k < ∞, and
2.

Xn → m ∈

as n → ∞,

then X̄n → m in probability as n → ∞.
Proof. Note that Xn → m implies X̄n → m. Now ﬁx  > 0 and choose N ∈
that | X̄n − m| ≤ /2 whenever n ≥ N. If n ≥ N, then

such

{| X̄n − m| ≥ } ⊂ {| X̄n − X̄n | + | X̄n − m| ≥ } ⊂ {| X̄n − X̄n | ≥ /2}
∴

{| X̄n − m| ≥ } ≤

This is sufﬁcient because | X̄n −
the Chebychev inequality

{| X̄n − X̄n | ≥ /2}

(n ≥ N )

X̄n | → 0 in probability when n → ∞ as a result of

{| X̄n − X̄n | ≥ } ≤
and the fact that Var( X̄n ) → 0 by lemma 11.1.11.

Var( X̄n )
2

Stochastic Dynamics

251

Theorem 11.1.12 can be used to prove a weak version of the SLLN stated in theorem 4.3.33 (page 94). Let S be a ﬁnite set, let p be a stochastic kernel on S such that the
Dobrushin coefﬁcient α( p) is strictly positive, let ψ∗ be the unique stationary distribution for p, and let h : S → be any function.
Exercise 11.1.13 Show that if T is a uniform contraction with modulus γ and ﬁxed
point x ∗ on metric space (U, ρ), then for any given x ∈ U we have ρ( T k x, x ∗ ) ≤
γk ρ( x, x ∗ ) . Conclude that there exist two constants M < ∞ and γ ∈ [0, 1) satisfying

∑ | pk (x, y) − ψ∗ (y)| ≤ Mγk for any k ∈

y∈S

and any x ∈ S

Exercise 11.1.14 Let m := ∑y∈S h(y)ψ∗ (y) be the mean of h with respect to ψ∗ . Using
exercise 11.1.13, show that there are constants K < ∞ and γ ∈ [0, 1) such that

∑ h(y) pk (x, y) − m

y∈S

≤ Kγk for any k ∈

and any x ∈ S

Now consider a Markov-( p, ψ∗ ) chain ( Xt )t≥0 , where the initial condition has been
set to the stationary distribution in order to simplify the proof. In particular, we have
h( Xt ) = m for all t. Regarding the covariances,
Cov(h( Xi ), h( Xi+k )) =

=
=

∑ ∑ [h(x) − m][h(y) − m]

x ∈S y∈S

{ Xi+k = y, Xi = x }

∑ ∑ [h(x) − m][h(y) − m] pk (x, y)ψ∗ (x)

x ∈S y∈S

∑ [h(x) − m]ψ∗ (x) ∑ [h(y) − m] pk (x, y)

x ∈S

y∈S

where the second equality is due to

{ Xi+k = y, Xi = x } =

{ Xi + k = y | Xi = x } { Xi = x }

Exercise 11.1.15 Using these calculations and the result in exercise 11.1.14, show that
there are constants J < ∞ and γ ∈ [0, 1) such that

| Cov(h( Xi ), h( Xi+k ))| ≤ Jγk for any i, k ≥ 0
Exercise 11.1.16 Show that the process (h( Xt ))t≥0 satisﬁes the conditions of theorem 11.1.12.

252

Chapter 11

11.1.2

Strong Convergence of Measures

Now let’s turn to another kind of convergence: convergence of the distribution of Xn
to the distribution of X. Since distributions are measures, what we are seeking here
is a metric (and hence a concept of convergence) deﬁned on spaces of measures. In
this section we discuss so-called strong convergence (or total variation convergence)
of measures. The next section discusses weak convergence.
When deﬁning strong convergence, it is useful to consider not only standard nonnegative measures but also signed measures, which are countably additive set functions taking both positive and negative values.
Deﬁnition 11.1.17 Let S be a Borel measurable subset of n . A (Borel) signed measure
μ on a S is a countably additive set function from B (S) to : Given any pairwise
disjoint sequence ( Bn ) ⊂ B (S), we have μ(∪n B) = ∑n μ( Bn ). The set of all signed
measures on S will be denoted by bM (S).2
Exercise 11.1.18 Show that for any μ ∈ bM (S) we have μ(∅) = 0.
Addition and scalar multiplication of signed measures is deﬁned setwise, so (αμ +
βν)( B) = αμ( B) + βν( B) for μ, ν ∈ bM (S) and scalars α, β. Notice that the difference
μ − ν of any two ﬁnite (nonnegative) measures is a signed measure. It turns out that
every signed measure can be represented in this way:
Theorem 11.1.19 (Hahn–Jordan) For each μ ∈ bM (S) there exists sets S− and S+ in
B (S) with S− ∩ S+ = ∅, S− ∪ S+ = S,
• μ( B) ≥ 0 whenever B ∈ B (S) and B ⊂ S+ , and
• μ( B) ≤ 0 whenever B ∈ B (S) and B ⊂ S− .
As a result, μ can be expressed as the difference μ+ − μ− of two ﬁnite nonnegative measures
μ+ and μ− , where
• μ+ ( B) := μ( B ∩ S+ ) for all B ∈ B (S), and
• μ− ( B) := −μ( B ∩ S− ) for all B ∈ B (S).
The set S+ is called a positive set for μ and S− is called a negative set. They are unique
in the sense that if A and B are both positive (negative) for μ, then the set of points in A
or B but not both has zero μ-measure. The ﬁrst part of the theorem (decomposition of
S) is called the Hahn decomposition, while the second (decomposition of μ) is called
the Jordan decomposition. The proof of the Hahn decomposition is not overly difﬁcult
and can be found in almost every text on measure theory. The Jordan decomposition
follows from the Hahn decomposition is a straightforward way:
2 Notice

that our signed measures are required to take values in
bM (S) the set of ﬁnite (or bounded) signed measures.

. For this reason some authors call

Stochastic Dynamics

253

Exercise 11.1.20 Show that μ = μ+ − μ− holds setwise on B (S).
Exercise 11.1.21 Verify that μ+ and μ− are measures on (S, B (S)). Show that the
equalities μ(S+ ) = maxB∈B (S) μ( B) and μ(S− ) = minB∈B (S) μ( B) both hold.
Exercise 11.1.22 Show that if S+ (resp., S− ) is positive (resp., negative) for μ, then S−
(resp., S+ ) is positive (resp., negative) for −μ.
Exercise 11.1.23 Let f ∈ mB (S) with λ(| f |) < ∞. Let μ( B) := λ( B f ). Show that μ ∈
bM (S). Show that if S+ := { x ∈ S : f ( x ) ≥ 0} and S− := { x ∈ S : f ( x ) < 0}, then S+
and S− form a Hahn decomposition of S with respect to μ; and that μ+ ( B) = λ( B f + )
and μ− ( B) = λ( B f − ). Show that the L1 norm of f is equal to μ+ (S) + μ− (S).
The ﬁnal result of the last exercise suggests the following generalization of L1 distance from functions to measures:
Deﬁnition 11.1.24 The total variation norm of μ ∈ bM (S) is deﬁned as

μ TV := μ+ (S) + μ− (S) = μ(S+ ) − μ(S− )
where S+ and S− are as in theorem 11.1.19. The function
d TV (μ, ν) := μ − ν TV

(μ, ν in bM (S))

is a metric on bM (S), and (bM (S), d TV ) is a metric space.
Exercise 11.1.25 Prove that for μ ∈ bM (S), the norm μ TV = maxπ ∈Π ∑ A∈π |μ( A)|,
where Π is the set of ﬁnite measurable partitions of S.3
Exercise 11.1.26 Verify that d TV is a metric on bM (S).
One of the nice things about the Jordan decomposition is that we can deﬁne integrals with respect to signed measures with no extra effort:
Deﬁnition 11.1.27 Let h ∈ mB (S), let μ ∈ bM (S) and let μ+ , μ− be the Jordan decomposition of μ. We set μ(h) := μ+ (h) − μ− (h) whenever at least one term is ﬁnite.
For given stochastic kernel P, we can deﬁne the Markov operator as a map over
the space of signed measures in the obvious way:
μM( B) =



P( x, B)μ(dx ) =



P( x, B)μ+ (dx ) −



P( x, B)μ− (dx )

where μ ∈ bM (S) and B ∈ B (S). The next lemma can be used to show that M is
nonexpansive on (bM (S), d TV ).
3 Hint:

The maximizer is the partition π := {S+ , S− }.

254

Chapter 11

Lemma 11.1.28 We have μM TV ≤ μ TV for any μ ∈ bM (S).
Proof. If S+ and S− are the positive and negative sets for μM, then

μM TV = μM(S+ ) − μM(S− ) =



P( x, S+ )μ(dx ) −



P( x, S− )μ(dx )

By the deﬁnition of the integral with respect to μ, this becomes


P( x, S+ )μ+ (dx ) −

∴



P( x, S+ )μ− (dx ) −

μM TV ≤





P( x, S− )μ+ (dx ) +

P( x, S+ )μ+ (dx ) +





P( x, S− )μ− (dx )

P( x, S− )μ− (dx )

The last term is dominated by μ+ (S) + μ− (S) =: μ TV .
Total variation distance seems rather abstract, but for probabilities it has a very
concrete alternative expression.
Lemma 11.1.29 If φ and ψ are probability measures, then
d TV (φ, ψ) := φ − ψ TV = 2 sup |φ( B) − ψ( B)|
B ∈B ( S )

This equivalence makes the total variation distance particularly suitable for quantitative work. The proof is in the appendix to this chapter.
Theorem 11.1.30 The metric spaces (bM (S), d TV ) and (P (S), d TV ) are both complete.
Proof. See, for example, Stokey and Lucas (1989, lem. 11.8).

11.1.3

Weak Convergence of Measures

Although total variation distance is pleasingly quantitative and important for the
analysis of Markov chains, it does not cover all bases. We will also consider another
type of convergence, known to probabilists as weak convergence. The next exercise
starts the ball rolling.
Exercise 11.1.31 Let S = , let φn = δ1/n and let φ = δ0 . Show that for each n ∈
we have supB∈B (S) |φn ( B) − φ( B)| = 1. Conclude that d TV (φn , φ) → 0 fails for this
example.
This negative result is somewhat unfortunate in that δ1/n seems to be converging
to δ0 . To make the deﬁnition of convergence more accommodating, we can abandon
uniformity, and simply require that φn ( B) → φ( B) for all B ∈ B (S). This is usually

Stochastic Dynamics

255

known as setwise convergence. However, a little thought will convince you that the
sequence treated in exercise 11.1.31 still fails to converge setwise.
Thus we need to weaken the deﬁnition further by requiring that φn ( B) → φ( B)
holds only for a certain restricted class of sets B ∈ B (S): We say that φn → φ weakly
if φn ( B) → φ( B) for all sets B ∈ B (S) such that φ(cl B \ int B) = 0. In fact this is
equivalent to the following—more convenient—deﬁnition.
Deﬁnition 11.1.32 The sequence (φn ) ⊂ P (S) is said to converge to φ ∈ P (S) weakly
if φn (h) → φ(h) for every h ∈ bcS, the bounded continuous functions on S. If ( Xn ) is
a sequence of random variables on S, then Xn → X weakly (or, in distribution; or, in
law) means that the distribution of Xn converges weakly to that of X.
Weak convergence is more in tune with the topology of S than setwise convergence. The next exercise illustrates.
Exercise 11.1.33 Show that δ1/n → δ0 holds for weak convergence.
Exercise 11.1.34 Convergence in probability implies convergence in distribution.4
Show that the converse is not true.5
Actually, when testing weak convergence, we don’t in fact need to test convergence at every h ∈ bcS. There are various “convergence determining classes” of functions, convergence over each element of which is sufﬁcient for weak convergence. One
example is ibcS, the increasing functions in bcS:
Theorem 11.1.35 Let (φn ) and φ be elements of P (S). We have φn → φ weakly if and only
if φn (h) → φ(h) for all h ∈ ibcS.6
Readers may be concerned that limits under weak convergence are not unique, in
the sense that there may exist a sequence (φn ) ⊂ P (S) with φn → φ and φn → φ
weakly, where φ = φ . In fact this is not possible, as can be shown using the following
result:
Theorem 11.1.36 Let φ, ψ ∈ P (S). The following statements are equivalent:
1. φ = ψ
2. φ(h) = ψ(h) for all h ∈ bcS
3. φ(h) = ψ(h) for all h ∈ ibcS
4 The

proof is not trivial. See, for example, Dudley (2002, prop. 9.3.5).
What happens in the case of an IID sequence?
6 See Torres (1990, cor. 6.5).
5 Hint:

256

Chapter 11

If it can be shown that the third statement implies the ﬁrst, then the rest of the
theorem is easy. A proof that such an implication holds can be found in Torres (1990,
thms. 3.7 and 5.3).
Exercise 11.1.37 Using theorem 11.1.36, show that if (φn ) ⊂ P (S) is a sequence with
φn → φ and φn → φ weakly, then φ = φ .
When S = , weak convergence implies convergence of distribution functions in
the following sense:
Theorem 11.1.38 Let (φn ) and φ be elements of P ( ), and let ( Fn ) and F be their respective
distribution functions. If φn → φ weakly, then Fn ( x ) → F ( x ) in for every x ∈
such
that F is continuous at x.7
One reason weak convergence is so important in probability theory is the magnificent central limit theorem, which concerns the asymptotic distribution of the sample
mean X̄n := n−1 ∑nt=1 Xt .8
Theorem 11.1.39 Let ( Xt )t≥1 be an IID sequence of real-valued random variables. If μ :=
X1 and σ2 := Var X1 are both ﬁnite, then n1/2 ( X̄n − μ) converges weakly to N (0, σ2 ).
While not obvious from the deﬁnition, it turns out that weak convergence on P (S)
can be metrized, in the sense that there exists a metric ρ on P (S) with the property
that φn → φ weakly if and only if ρ(φn , φ) → 0. Actually there are several, and all are
at least a little bit complicated. We will now described one such metric, known as the
Fortet–Mourier distance.
Recall that a function h : S →
is called Lipschitz if there exists a K ∈
such
that |h( x ) − h(y)| ≤ Kd2 ( x, y) for all x, y ∈ S. Let bS be the collection of bounded
Lipschitz functions on S, and set

hb := sup |h( x )| + sup
x ∈S

x =y

|h( x ) − h(y)|
d2 ( x, y)

(11.2)

The Fortet–Mourier distance between φ and ψ in P (S) is deﬁned as
d FM (φ, ψ) := sup{|φ(h) − ψ(h)| : h ∈ bS, hb ≤ 1}

(11.3)

It can be shown that d FM so constructed is indeed a metric, and does metrize weak
convergence as claimed.9
7 For a proof see, for example, Taylor (1997, prop. 6.2.3). The converse is not true in general, but if
Fn ( x ) → F ( x ) in for every x ∈ such that F is continuous at x then φn (h) → φ(h) for every h ∈ bcS such
that h(S \ K ) = {0} for some compact K ⊂ S.
8 For a proof see, for example, Taylor (1997, thm. 6.7.4.).
9 See, for example, Dudley (2002, thm. 11.3.3).

Stochastic Dynamics

257

There is a large and elegant theory of weak convergence, most of which would take
us too far aﬁeld. We will content ourselves with stating Prohorov’s theorem. It turns
out that P (S) is d FM -compact if and only if S is compact. Prohorov’s theorem can be
used to prove this result, and also provides a useful condition for (pre)compactness of
subsets of P (S) when S is not compact.
Deﬁnition 11.1.40 A subset M of P (S) is called tight if, for each  > 0, there is a
compact K ⊂ S such that φ(S \ K ) ≤  for all φ ∈ M .10
Theorem 11.1.41 (Prohorov) The following statements are equivalent:
1. M ⊂ P (S) is tight.
2. M is a precompact subset of the metric space (P (S), d FM ).
For a proof, see Pollard (2002, p. 185) or Dudley (2002, ch. 11).

11.2

Stability: Analytical Methods

We are now ready to tackle some stability results for general state Markov chains. In
this section we will focus on analytical techniques related to the metric space theory
of chapter 3. In §11.3 we turn to more probabilistic methods.

11.2.1

Stationary Distributions

When we discussed distribution dynamics for a stochastic kernel p on a ﬁnite state
space, we regarded the Markov operator M corresponding to p (see page 75) as providing a dynamical system of the form (P (S), M), where P (S) was the set of distributions on S. The interpretation was that if ( Xt )t≥0 is Markov-( p, ψ), then ψMt is the
distribution of Xt . A ﬁxed point of M was called a stationary distribution.
A similar treatment was given for the density case (chapter 8), where we considered the dynamical system ( D (S), M). Trajectories correspond to sequences of
marginal densities, and a ﬁxed point of M in D (S) is called a stationary density.
For the general (i.e., measure) case, where S is a Borel subset of n , P is an arbitrary
stochastic kernel and M is the corresponding Markov operator (see page 224), we
consider the dynamical system (P (S), M), with P (S) denoting the Borel probability
measures on S. The metric imposed on P (S) is either d TV or d FM (see §11.1.2 and
§11.1.3 respectively). A distribution ψ∗ ∈ P (S) is called stationary if ψ∗ M = ψ∗ ;
equivalently

P( x, B)ψ∗ (dx ) = ψ∗ ( B)

10 This

( B ∈ B (S))

is a generalization of the deﬁnition given for densities on page 203.

258

Chapter 11

Exercise 11.2.1 Consider the deterministic model Xt+1 = Xt . Show that for this model
every ψ ∈ P (S) is stationary.11
In this section we focus on existence of stationary distributions using continuity
and compactness conditions. Regarding continuity,
Deﬁnition 11.2.2 Let P be a stochastic kernel on S, and let M be the corresponding
Markov operator. We say that P has the Feller property if Mh ∈ bcS whenever h ∈ bcS.
The Feller property is usually easy to check in applications. To illustrate, recall our
canonical SRS deﬁned in (9.7) on page 219.
Lemma 11.2.3 If x → F ( x, z) is continuous on S for all z ∈ Z, then P is Feller.
Proof. Recall from (9.17) on page 226 that for any h ∈ bB (S), and in particular for
h ∈ bcS, we have

Mh( x ) =

h[ F ( x, z)]φ(dz)

( x ∈ S)

So ﬁx any h ∈ bcS. We wish to show that Mh is a continuous bounded function.
Verifying boundedness is left to the reader. Regarding continuity, ﬁx x0 ∈ S and take
some xn → x0 . For each z ∈ Z, continuity of x → F ( x, z) and h gives us h[ F ( xn , z)] →
h[ F ( x0 , z)] as n → ∞. Since h is bounded the conditions of the dominated convergence
theorem (page 182) are all satisﬁed. Therefore
Mh( xn ) :=



h[ F ( xn , z)]φ(dz) →



h[ F ( x0 , z)]φ(dz) =: Mh( x0 )

As x0 was arbitrary, Mh is continuous on all of S.
The Feller property is equivalent to continuity of ψ → ψM in (P (S), d FM ):
Lemma 11.2.4 A stochastic kernel P with Markov operator M is Feller if and only if ψ →
ψM is weakly continuous as a map from P (S) to P (S).
Proof. Suppose ﬁrst that M is Feller. Take any (ψn ) ⊂ P (S) with ψn → ψ ∈ P (S)
weakly. We must show that ψn M(h) → ψM(h) for every h ∈ bcS. Pick any such h.
Since Mh ∈ bcS, theorem 9.2.15 (page 227) gives
ψn M(h) = ψn (Mh) → ψ(Mh) = ψM(h)

(n → ∞)

The reverse implication is left as an exercise.12
We can now state the well-known Krylov–Bogolubov existence theorem, the proof
of which is given in the appendix to this chapter.
11 Hint:
12 Hint:

See example 9.2.5 on page 221.
Try another application of theorem 9.2.15.

Stochastic Dynamics

259

Theorem 11.2.5 (Krylov–Bogolubov) Let P be a stochastic kernel on S, and let M be the
corresponding Markov operator. If P has the Feller property and (ψMt )t≥0 is tight for some
ψ ∈ P (S), then P has at least one stationary distribution.13
Remark 11.2.6 If S is compact, then every subset of P (S) is tight (why?), and hence
every kernel with the Feller property on S has at least one stationary distribution. This
is theorem 12.10 in Stokey and Lucas (1989).
There are many applications of theorem 11.2.5 in economic theory, particularly for
the case where the state space is compact. In fact it is common in economics to assume
that the shocks perturbing a given model are bounded above and below, precisely
because the authors wish to obtain a compact state space. (Actually such strict restrictions on the shocks are usually unnecessary: we can deal with unbounded shocks
using drift conditions as discussed below.)
Example 11.2.7 Consider again the commodity pricing model, which was shown to
be stable when the shock is lognormal in §8.2.4. The law of motion for the model is
Xt+1 = αI ( Xt ) + Wt+1 with shock distribution φ ∈ P ( Z ). The Feller property holds
because I is continuous (see the deﬁnition on page 146). Suppose now that Z := [ a, b]
for positive constants a ≤ b.14 Deﬁne S := [ a, b/(1 − α)]. It is an exercise to show
that if x ∈ S and z ∈ Z, then αI ( x ) + z ∈ S. Hence the compact set S can be chosen
as a state space for the model, and, by the Krylov–Bogolubov theorem, at least one
stationary distribution ψ∗ exists. It satisﬁes
"
 !
[
αI
(
x
)
+
z
]
φ
(
dz
)
( B ∈ B (S))
(11.4)
ψ∗ ( B) =
ψ∗ (dx )
B
If S is not compact, then to establish existence via the Krylov–Bogolubov theorem,
we need to show that at least one trajectory of M is tight. Fortunately we already know
quite a bit about ﬁnding tight trajectories, at least in the density case. For example, under geometric drift to the center every trajectory is tight (proposition 8.2.12, page 204).
In the general (i.e., measure rather than density) case a similar result applies:
Lemma 11.2.8 Let M be a subset of P (S). If there exists a norm-like function15 w on S such
that supψ∈M ψ(w) < ∞, then M is tight.16
Proof. Let M := supψ∈M ψ(w), and ﬁx  > 0. Pick any ψ ∈ M . We have
ψ{ x ∈ S : w( x ) > k} ≤

ψ(w)
M
≤
k
k

∀k ∈

is, M has at least one ﬁxed point in P (S).
Z is compact, one often says that φ has compact support.
15 Recall that a function w : S →
+ is called norm-like when all sublevel sets are precompact. See
deﬁnition 8.2.9 on page 204.
16 Actually the converse is also true. See Meyn and Tweedie (1993, lem. D.5.3).
13 That

14 Since

260

Chapter 11

where the ﬁrst inequality follows from w ≥ k { x ∈ S : w( x ) > k}. Since ψ is arbitrary,
sup ψ{ x ∈ S : w( x ) > k} ≤

ψ ∈M

M
k

∀k ∈

For sufﬁciently large k the left-hand side is less than . Deﬁning C := w−1 ([0, k ]),
we can write this as supψ∈M ψ(C c ) < . Since w is norm-like, we know that C is
precompact. By exercise 3.2.17 on page 47, there is a compact set K with C ⊂ K, or
K c ⊂ C c . But then ψ(K c ) ≤ ψ(C c ) ≤  for all ψ ∈ M .
The easiest way to apply lemma 11.2.8 is via a drift condition:
Lemma 11.2.9 Let P be a stochastic kernel on S with Markov operator M, and let ψ ∈ P (S).
If there exists a norm-like function w on S and constants α ∈ [0, 1) and β ∈ + with
Mw( x ) ≤ αw( x ) + β

( x ∈ S)

(11.5)

then there exists a ψ ∈ P (S) such that the trajectory (ψt ) := (ψMt ) is tight.
The intuition is similar to that for proposition 8.2.12 (page 204).
Proof. Let ψ := δx for some x ∈ S. Using theorem 9.2.15 (page 227) and then monotonicity property M4 of the integral, we have
ψt (w) = (ψt−1 M)(w) = ψt−1 (Mw) ≤ ψt−1 (αw + β) = αψt−1 (w) + β
From this bound one can verify (use induction) that
ψt (w) ≤ αt ψ(w) +

β
β
= αt w( x ) +
1−α
1−α

∀t ∈

Tightness of (ψt ) now follows from lemma 11.2.8.

11.2.2

Testing for Existence

Let’s investigate how one might use lemma 11.2.9 in applications. First we can repackage the results of the previous section as a corollary that applies to the canonical SRS
given in (9.7) on page 219. The proof is an exercise.
Corollary 11.2.10 If x → F ( x, z) is continuous on S for each z ∈ Z, and there exists a
norm-like function w on S and constants α ∈ [0, 1) and β ∈ + with


w[ F ( x, z)]φ(dz) ≤ αw( x ) + β

then at least one stationary distribution exists.

( x ∈ S)

(11.6)

Stochastic Dynamics

261

Example 11.2.11 Let S = Z = n , and let F ( x, z) = Ax + b + z, where A is an n × n
matrix and b is an n × 1 vector. Let φ ∈ P ( Z ). Suppose thatfor some norm  ·  on
S we have λ := sup{ Ax  :  x  = 1} < 1, and in addition zφ(dz) < ∞. Using
exercise 4.1.17 (page 60), we have


 Ax + b + zφ(dz) ≤  Ax  + b +



zφ(dz) ≤ λ x  + b +



zφ(dz)


Setting α := λ and β := b + zφ(dz) gives the drift condition (11.6). It is left as
an exercise to show that the Feller property holds. Since  ·  is norm-like on S (see
example 8.2.10), a stationary distribution exists.
Example 11.2.12 Let S = Z = n , and let F ( x, z) = G ( x ) + z, where G : n → n is a
continuous function with the property that for some M < ∞ and some α < 1 we have
 G ( x ) ≤ α x  whenever  x  > M. In other words, when x is sufﬁciently far from
the origin, G ( x ) is closer to the origin than x. Also note that L := sup x≤ M  G ( x ) is
ﬁnite because continuous functions map compact sets into compact sets. By considering the two different cases  x  ≤ M and  x  > M, you should be able to show that
 G ( x ) ≤ α x  + L for every x ∈ n . As a result


 G ( x ) + zφ(dz) ≤  G ( x ) +



zφ(dz) ≤ α x  + L +



zφ(dz)


If zφ(dz) < ∞, then the drift condition (11.6) holds. As the Feller property clearly
holds a stationary distribution must exist.
Exercise 11.2.13 Show that example 11.2.11 is a special case of example 11.2.12.
Exercise 11.2.14 Consider the log-linear Solow–Swan model k t+1 = skαt Wt+1 . Set
S = Z = (0, ∞), and assume α < 1 and | ln Wt | < ∞. One way to show existence of a stationary distribution is by way of taking logs and converting our loglinear system into a linear one. Example 11.2.11 then applies. However, this still
leaves the task of showing that existence of a stationary distribution for the linear
model implies existence of a stationary distribution for the original model. Instead of
log-linearizing, prove the existence of a stationary distribution directly, by applying
corollary 11.2.10.17
Finally, let’s treat the problem of existence in a more involved application. The
application requires an extension of lemma 11.2.9 on page 260. A proof can be found
in Meyn and Tweedie (1993, thm. 12.1.3).
17 Hint:

Use w( x ) = | ln x | as the norm-like function.

262

Chapter 11

Lemma 11.2.15 Let P be a stochastic kernel on S with Markov operator M, and let ψ ∈
P (S). If P has the Feller property, and in addition there exists a norm-like function w on S
and constants α ∈ [0, 1) and β ∈ + with
Mt w( x ) ≤ αw( x ) + β
for some t ∈

( x ∈ S)

(11.7)

, then P has at least one stationary distribution.

As an application, consider the SRS with correlated shocks deﬁned by
X t +1 = g ( X t ) + ξ t +1

and

ξ t+1 = Aξ t + Wt+1

(11.8)

Here Xt and ξ t both take values in k , and (Wt )t≥1 is an k valued IID sequence
with distribution φ ∈ P ( k ). The matrix A is k × k, while g : k → k is Borel
measurable. Although ( Xt )t≥0 is not generally Markovian, the joint process ( Xt , ξ t )t≥0
is Markovian in S := k × k . The associated stochastic kernel is
P(( x, ξ ), B) =



B [ g( x ) +

Aξ + z, Aξ + z]φ(dz)

(( x, ξ ) ∈ S, B ∈ B (S))

If g is a continuous function, then in view of lemma 11.2.15 a stationary distribution
will exist for P whenever the drift condition (11.7) holds. That this drift condition does
hold under some restrictions is the content of the next proposition.
Proposition 11.2.16 Let  ·  be any norm on k . Let ρ be a constant such that A satisﬁes
 Ax  ≤ ρ x  for all x ∈ k , and let w be the norm-like function w( x, ξ ) =  x  + ξ . Set
μ := W1 . If μ < ∞, ρ < 1, and there exists constants λ ∈ [0, 1) and L ∈ + such that

 g( x ) ≤ λ x  + L
then there exists constants t ∈

(x ∈

, α ∈ [0, 1) and β ∈

+

k

)

such that (11.7) holds.

Proof. Consider the joint process ( Xt , ξ t )t≥0 from constant initial condition ( x0 , ξ 0 ) ∈
S. From the deﬁnition of the SRS and the growth condition on g, we have

 Xt+1  ≤ λ  Xt  + L + ξ t+1  and

 ξ t +1  ≤ ρ  ξ t  + μ

From these bounds one can see (use induction) that for any t ≥ 0,

 Xt  ≤ λ t  x0  +
and

t −1
L
+ ∑ λi  ξ t −i 
1 − λ i =0

ξ t  ≤ ρt ξ 0  +

μ
1−ρ

(11.9)

(11.10)

Stochastic Dynamics

263

Substituting (11.10) into (11.9) and rearranging gives

 Xt  ≤ λ t  x0  +

t −1
L
μ
+
+ ∑ λi ρ t −i  ξ 0 
1 − λ (1 − λ)(1 − ρ) i=0

(11.11)

Adding (11.10) and (11.11), we obtain
t −1

 Xt  +  ξ t  ≤ λ t  x0  + ρ t  ξ 0  + ∑ λ i ρ t −i  ξ 0  + β
i =0

−1 i t − i
where β is a constant. Since limt→∞ ∑it=
= 0, we can choose a t ∈
0λ ρ

ρt +

such that

t −1

∑ λi ρ t −i < 1

i =0

Letting α be the maximum of this term and λt , we obtain

 Xt  +  ξ t  ≤ α  x0  + α  ξ 0  + β
This inequality is equivalent to the claim in the proposition.

11.2.3

The Dobrushin Coefﬁcient, Measure Case

Now let’s turn to the problem of uniqueness and stability of stationary distributions.
As a ﬁrst step we discuss a contraction mapping approach based on the Dobrushin coefﬁcient. As we saw in §8.2.2, for unbounded state spaces this approach is not always
successful. Nevertheless, it provides a useful departure point, and the basic ideas will
later be extended to handle more general models.
Let S be a Borel subset of n . For stochastic kernel P with a density representation
p (i.e., P( x, dy) = p( x, y)dy for all x ∈ S), the Dobrushin coefﬁcient was deﬁned in
§8.2.2 as


α( p) := inf



p( x, y) ∧ p( x , y)dy : ( x, x ) ∈ S × S

(11.12)

The corresponding Markov operator is a uniform contraction of modulus 1 − α( p) on
( D (S), d1 ) whenever α( p) > 0.
The concept of the Dobrushin coefﬁcient can be extended to kernels without density
 representation. To do so we need a notion equivalent to the afﬁnity measure
f ∧ g between densities f and g used in (11.12). Since f ∧ g is the largest function
less than both f and g, it is natural to extend this idea by considering the largest measure less than two given measures μ and ν.18 Such a measure is called the inﬁmum of
μ and ν, and denoted by μ ∧ ν.
18 The

ordering is setwise: μ ≤ ν if μ( B) ≤ ν( B) for all B ∈ B (S).

264

Chapter 11

Things are not quite as simple as the density case. In particular, it is not correct
to deﬁne μ ∧ ν as the set function m : B → μ( B) ∧ ν( B). The reason is that m is not
always additive, and hence fails to be a measure (example?) However, the inﬁmum of
two measures does always exists:
Lemma 11.2.17 If μ ∈ bM (S) and ν ∈ bM (S), then there exists a unique element of
bM (S), denoted here by μ ∧ ν, such that
1. both μ ∧ ν ≤ μ and μ ∧ ν ≤ ν, and
2. if κ ∈ bM (S) and both κ ≤ μ and κ ≤ ν, then κ ≤ μ ∧ ν.
Proof. Let S+ be a positive set for μ − ν, and let S− be a negative set (for deﬁnitions,
see page 252). It follows that if B ∈ B (S) and B ⊂ S+ , then μ( B) ≥ ν( B), while if
B ⊂ S− , then ν( B) ≥ μ( B). Now set

(μ ∧ ν)( B) := μ( B ∩ S− ) + ν( B ∩ S+ )
Evidently μ ∧ ν is countably additive. That μ ∧ ν ≤ μ is immediate:
μ( B) = μ( B ∩ S− ) + μ( B ∩ S+ ) ≥ μ( B ∩ S− ) + ν( B ∩ S+ )
The proof that μ ∧ ν ≤ ν is similar. To check the second claim, let κ ∈ bM (S) with
κ ≤ μ and κ ≤ ν. Then
κ ( B) = κ ( B ∩ S− ) + κ ( B ∩ S+ ) ≤ μ( B ∩ S− ) + ν( B ∩ S+ )
Hence κ ≤ μ ∧ ν, as was to be shown.
Exercise 11.2.18 Show that if μ is a probability with density f , and ν has density g,
then μ ∧ ν has density f ∧ g.
For probabilities μ and ν, the value (μ ∧ ν)(S) is sometimes called the afﬁnity between μ and ν, and is a measure of similarity.
Exercise 11.2.19 Show that (μ ∧ ν)(S) = minπ ∑ A∈π μ( A) ∧ ν( A) for any μ and ν in
P (S), where the minimum is over all ﬁnite measurable partitions π of S.19 Show that
the afﬁnity between μ and ν has a maximum value of 1, which is attained if and only
if μ = ν.
We can now deﬁne the Dobrushin coefﬁcient for general kernel P.
μ( A) ∧ ν( A) is a simple inﬁmum in , rather than in bM (S). Hint: For the minimizer consider
π = {S+ , S− }, where S+ and S− are as in the proof of lemma 11.2.17.
19 Here

Stochastic Dynamics

265

Deﬁnition 11.2.20 Let P be a stochastic kernel on S. Writing Px ∧ Px for the inﬁmum
measure P( x, dy) ∧ P( x , dy), the Dobrushin coefﬁcient of P is deﬁned as
α( P) := inf { ( Px ∧ Px )(S) : ( x, x ) ∈ S × S }
When P( x, dy) = p( x, y)dy, this reduces to (11.12) above, while if S is ﬁnite and P
is deﬁned by a ﬁnite kernel p (i.e., P( x, B) = ∑y∈ B p( x, y) for all x ∈ S and B ⊂ S),
then it reduces to the deﬁnition given on page 89.
As for the ﬁnite and density cases, the Dobrushin coefﬁcient is closely connected
to stability. In fact the following theorem holds:
Theorem 11.2.21 Let P be a stochastic kernel on S with Markov operator M. For every pair
φ, ψ in P (S) we have

φM − ψM TV ≤ (1 − α( P))φ − ψ TV
Moreover this bound is the best available, in the sense that if λ < 1 − α( P), then there exists
a pair φ, ψ in P (S) such that φM − ψM TV > λφ − ψ TV .
This result closely parallels the result in theorem 4.3.17, page 89. The proof is
similar to the ﬁnite case (i.e., the proof of theorem 4.3.17) and as such is left to the
enthusiastic reader as an exercise. The intuition behind the theorem is also similar to
the ﬁnite case: The afﬁnity ( Px ∧ Px )(S) is a measure of the similarity of the kernels
P( x, dy) and P( x , dy). If all the kernels are identical then α( P) = 1 and M is a constant
map—the ultimate in global stability. More generally, high values of α( P) correspond
to greater similarity across the kernels, and hence more stability.
The ﬁrst half of theorem 11.2.21 says that if α( P) > 0, then M is a uniform contraction with modulus 1 − α( P) on P (S). Since (P (S), d TV ) is a complete metric space
(theorem 11.1.30), it follows that (P (S), M) is globally stable whenever α( Pt ) > 0 for
some t ∈ .20 Let us record these ﬁndings as a corollary.
Corollary 11.2.22 Let P be a stochastic kernel on S, and let M be the associated Markov operator. If α( Pt ) > 0 for some t ∈ , then (P (S), M) is globally stable with unique stationary
distribution ψ∗ . Moreover if h : S → is a measurable function satisfying ψ∗ |h| < ∞ and
ψ ∈ P (S), then any Markov-( P, ψ) chain ( Xt )t≥0 satisﬁes
1
n

n

∑ h ( Xt ) → ψ ∗ ( h )

t =1

with probability one as n → ∞

(11.13)

The second part of the corollary, which is a law of large numbers for Markov
chains, follows from global stability. The proof is omitted, but interested readers can
consult Meyn and Tweedie (1993, ch. 17).
20 This

follows from Banach’s ﬁxed point theorem, nonexpansiveness of M with respect to d TV (see
lemma 11.1.28 on page 254) and lemma 4.1.21 on page 61.

266

Chapter 11

Example 11.2.23 Consider the well-known stability condition

∃m ∈

, ν ∈ P ( S ),  > 0

such that

Pm ( x, dy) ≥ ν

∀x ∈ S

(11.14)

Here Pm ( x, dy) ≥ ν means that Pm ( x, B) ≥ ν( B) for all B ∈ B (S). If this condition
holds, then α( Pm ) > 0 and (P (S), M) is globally stable, as the next exercise asks you
to conﬁrm.
Exercise 11.2.24 Show that if (11.14) holds, then α( Pm ) ≥ .
The condition (11.14) is particularly easy to verify when Pm ( x, dy) puts uniformly
positive probability mass on a point z ∈ S, in the sense that

∃ z ∈ S, γ > 0 such that Pm ( x, {z}) ≥ γ ∀ x ∈ S

(11.15)

It turns out that if (11.15) holds, then (11.14) holds with ν := δz and  := γ. To see this,
pick any x ∈ S and any B ∈ B (S). If z ∈ B, then
Pm ( x, B) ≥ Pm ( x, {z}) ≥ γ = ν( B)
On the other hand, if z ∈
/ B, then Pm ( x, B) ≥ 0 = ν( B). Thus Pm ( x, B) ≥ ν( B) for
all x ∈ S and B ∈ B (S) as claimed.
Example 11.2.25 Stokey and Lucas (1989, p. 348) use the following condition for stability, which they refer to as condition M: There exists a m ∈ and an  > 0 such that
for any A ∈ B (S), either Pm ( x, A) ≥  for all x ∈ S or Pm ( x, Ac ) ≥  for all x ∈ S.
This condition is stricter than α( Pm ) > 0, as the next exercise asks you to conﬁrm.
Exercise 11.2.26 Show that if condition M holds, then α( Pm ) ≥ .21
We saw in §8.2.2 that when the state space is unbounded, existence of a t ∈ such
that α( Pt ) > 0 often fails (recall the discussion of the AR(1) model in that section). In
that case the method for establishing global stability discussed in this section cannot
be applied. However, as we will see, the basic ideas can be extended to a wide variety
of problems—including those on unbounded spaces.

11.2.4

Application: Credit-Constrained Growth

In this section we apply the stability condition in corollary 11.2.22 to a model of economic growth under credit constraints due to Matsuyama (2004), which tries to reconcile classical and structuralist views on the effects of global ﬁnancial market integration. The classical view is that such integration fosters growth of developing countries
For any x, x ∈ S we have ( Px ∧ Px )(S) = Pm ( x, S− ) + Pm ( x , S+ ), where S− and S+ are negative
and positive for Pm ( x, dy) − Pm ( x , dy) respectively. Now use the fact that S+ = (S− )c .
21 Hint:

Stochastic Dynamics

267

by giving them access to scarce capital. Structuralists argue that poorer economies
would not be able to compete with richer countries in global ﬁnancial markets, and
that the gap between rich and poor may even be magniﬁed.
We will not delve into the many economic ideas that are treated in the paper. Instead our focus will be on technical issues, in particular, on analyzing the dynamics of
a small open economy model constructed by Matsuyama. The model has been slightly
modiﬁed to better suit our purposes.
To begin, consider a small open economy populated by agents who live for two periods. Agents supply one unit of labor when young and consume only when old. Each
successive generation has unit mass. At the start of time t a shock ξ t is realized and
production takes place, combining the current aggregate stock of capital k t supplied
by the old with the unit quantity of labor supplied by the young.22 The resulting output is yt = f (k t )ξ t , where the production function f : + → + is increasing, strictly
concave, differentiable, and f ( x ) ↑ ∞ as x ↓ 0. The shocks (ξ t )t≥0 are IID on + . For
convenience we set ξ t = 1.23
Factor markets are competitive, paying young workers the wage wt := w(k t )ξ t ,
where
w(k) := f (k) − k f (k)
(k ∈ + )
and a gross return on capital given by f (k t )ξ t . Since the old supply k t units of capital
to production, the sum of factor payments exhausts aggregate income.24
After production and the distribution of factor payments, the old consume and
disappear from the model, while the young take their wage earnings and invest them.
In doing so, the young have two choices:
1. a loan to international investors at the risk free gross world interest rate R, or
2. an indivisible project, which takes one unit of the consumption good and returns
in the next period Q units of the capital good.
The gross rate of return on the second option, measured in units of the consumption
good, is Q f (k t+1 )ξ t+1 . In this expression, k t+1 is the outcome of investment in the
project by the young. Factors of production are not internationally mobile, and FDI is
ruled out.
Agents are assumed to be risk neutral, and as a result they invest in the project
until the expected rate of return Q f (k t+1 ) is equal to the risk-free rate. Thus k t+1 is
determined by the equation
R = Q f ( k t +1 )
(11.16)
22 Capital

depreciates fully between periods, so capital stock is equal to investment.
the mean differs from one then this constant can be absorbed into the production function. Thus
ξ t = 1 is essentially a ﬁnite ﬁrst-moment assumption.
24 That is, k f ( k ) ξ + w ( k ) ξ = y .
t
t t
t t
t
23 If

268

Chapter 11

We assume that Q f ( Q) ≤ R so that if every young agent starts a project, then the
return to projects is driven below that of the risk-free rate.
We have already constructed a dynamic model, with (k t )t≥0 converging immediately to the (constant) solution to (11.16). To make matters more interesting, however,
let’s investigate how the model changes when capital markets are imperfect. The imperfection we consider is a constraint on borrowing, where lending is dependent on
the provision of collateral.
When wt < 1, young agents who start projects must borrow 1 − wt at the risk
free rate R. As a result their obligation at t + 1 is given by R(1 − wt ). Against this
obligation, borrowers can only credibly pledge a fraction λ ∈ [0, 1] of their expected
earnings Q f (k t+1 ). This results in the borrowing constraint
R(1 − wt ) ≤ λQ f (k t+1 )
This constraint is binding only if 1 − wt > λ, or, as a restriction on wt , if wt < 1 −
λ; otherwise, agents are able to choose the unconstrained equilibrium value of k t+1
deﬁned in (11.16). If the constraint is in fact binding, then it holds with equality
R=

λ
Q f ( k t +1 )
1 − wt

Combining this with (11.16), we can write the equation that determines k t+1 as

λ/(1 − w) if w < 1 − λ
R = θ ( w t ) Q f ( k t +1 ),
θ (w) :=
(11.17)
1
otherwise
The function w → θ (w) is shown in ﬁgure 11.1. It is monotone increasing and
takes values in the interval [λ, 1]. The determination of next period’s capital stock
k t+1 is depicted in ﬁgure 11.2 as the value of k at which the curve k → θ (wt ) Q f (k)
intersects the horizontal line R. This value is the solution to (11.17).
Let g := ( f )−1 be the inverse function of f . The constants a and b in the ﬁgure
are deﬁned by

 

R
R
a := g
and b := g
λQ
Q
The lower bound a is the quantity of domestic capital at t + 1 when wt = 0. In this
case the entire cost of the project must be ﬁnanced by borrowing, and k t+1 solves
R = λQ f (k t+1 ). The upper bound b is the unconstrained solution (11.16). As is clear
from these two ﬁgures, higher wages increases θ, which increases k t+1 .
Using g, we can write the stochastic law of motion for (k t )t≥0 as the SRS


R
k t +1 = g
(11.18)
θ (w(k t )ξ t ) Q
A suitable state space for this SRS is provided by the interval S = [ a, b]:

Stochastic Dynamics

269

θ
1

θ (w)

λ

w
1−λ

0

Figure 11.1 The function θ (w)

Return on investment

R
Q f (k)
θ ( wt ) Q f ( k )
λQ f (k )
a

k t +1

b

k
Q

Figure 11.2 Determination of k t+1

270

Chapter 11

Exercise 11.2.27 Show that if k t ∈ S and ξ t ∈

+,

then k t+1 ∈ S.

The corresponding stochastic kernel P on S is deﬁned by
! 
"

R
( x ∈ S, B ∈ B (S))
P( x, B) =
φ(dz)
B g
θ (w( x )z) Q
where φ is the common distribution of the shocks (ξ t )t≥0 .
Global stability holds whenever α( P) > 0 (corollary 11.2.22). To show that α( P) >
0, we can use the condition

∃ z ∈ S, γ > 0 such that P( x, {z}) ≥ γ ∀ x ∈ S
presented in (11.15) on page 266. For the z in this expression we use b, the upper
bound of S = [ a, b]. For γ we set


1−λ
γ := {w( a)ξ t ≥ 1 − λ} = φ z ∈ + : z ≥
f ( a) − a f ( a)
We assume φ is such that γ > 0. It remains to be shown that P( x, {b}) ≥ γ for all x ∈
S. To see that this is the case, note that by monotonicity we have P( x, {b}) ≥ P( a, {b})
for all x ∈ S. The latter quantity P( a, {b}) is the probability of jumping from the
lowest state a to the highest in state b (the unconstrained equilibrium) in one period.
This occurs whenever θ (w( a)ξ t ) = 1, which in turn holds when w( a)ξ t ≥ 1 − λ. The
probability of this event is precisely γ, and P( x, {b}) ≥ γ for all x is now veriﬁed.
Although global stability holds, this general result masks important differences in
the dynamics that occur when the parameters are varied. To gain some understanding
of these differences let us compute the stationary distribution ψ∗ and see how it is
affected by variation in parameters.
The stationary distribution is not a density (it puts positive mass on b) and hence
one cannot use the look-ahead estimator (6.12) introduced on page 131. Instead we
use the empirical cumulative distribution function
Fn∗ ( x ) :=

1
n

n

∑

t =1

{k t ≤ x } =

1
n

n

∑

t =1

[0,x ] (k t )

( x ∈ S)

where (k t )t≥0 is a time series generated from (11.18). From the LLN result (11.13) on
page 265, we have
lim F ∗ ( x )
n→∞ n

=



[0,x ] (y )ψ

∗

(dy) = ψ∗ {y : y ≤ x } =: F ∗ ( x )

with probability one, where the far right-hand side function F ∗ is deﬁned to be the
cumulative distribution corresponding to the probability measure ψ∗ . Thus the estimator Fn∗ of F ∗ converges at each point in the state space with probability one.

Stochastic Dynamics

271

1
λ = 0.44
λ = 0.48

λ = 0.5

λ = 0.52

0
a

b
S

Figure 11.3 Estimates of F ∗ for different λ

Figure 11.3 shows four observations of Fn∗ , each generated with individual time
series of length 5,000. The four observations correspond to different values of the
credit constraint parameter λ, as shown in the ﬁgure.25 Notice that the stationary
distributions are very sensitive to λ, with probability mass rapidly shifting toward
the unconstrained equilibrium state b as λ increases.
Exercise 11.2.28 Use the LLN to estimate ψ∗ ({b}) for different values of λ. Graph
these values against λ as λ varies over [0.4, 0.6].26

11.3

Stability: Probabilistic Methods

So far our techniques for treating stability of Markov chains have been mainly analytical. Now it’s time to treat probabilistic methods, which are perhaps even more
fundamental to modern Markov chain theory. As you will see, the ﬂavor of the proofs
is quite different. This unusual taste has limited the diffusion of probabilistic methods
into economics. However, a bit of study will illustrate how powerful—and beautiful—
these ideas can be.
25 The model is otherwise deﬁned by f ( k ) = k α , α = 0.6, R = 1, Q = 2 and ln ξ ∼ N ( μ, σ2 ), where
t
σ = 0.1 and μ = −σ2 /2 (which gives ξ t = 1).
26 In view of the LLN, ψ∗ ({ b }) can be interpreted as the fraction of time that ( k )
t t≥0 spends at the unconstrained equilibrium b over the long run.

272

Chapter 11

Underlying most probabilistic methods is the notion of coupling. Coupling is used
to make assertions about a collection of distributions by constructing random variables on a common probability space that (1) have these distributions, and (2) also
have certain properties useful for establishing the assertion one wishes to prove. In
the case of Markov chains the distributions in question are usually the stationary distribution ψ∗ and the marginal distribution ψMt , and the assertion one seeks to prove
is that ψMt − ψ∗  → 0 as t → ∞.
At this stage you might like to review the material in §5.2.3, which uses coupling
to prove the stability of ﬁnite state Markov chains. However, the next section can be
read independently of §5.2.3, and despite the inﬁnite state space, is perhaps a little
easier to follow.

11.3.1

Coupling with Regeneration

Let us begin by reconsidering stability of the commodity pricing model. This model
has a regenerative structure making it particularly well suited to illustrating the fundamentals of coupling. The commodity pricing model was shown to be globally stable
in §8.2.4 when the harvest (i.e., shock) process is lognormally distributed. Let us now
prove that global stability holds when the harvest is distributed according to a general
Borel probability measure φ (as opposed to a density).
In §6.3.1 we assumed that the shock is bounded away from zero in order to set up
a contraction mapping argument in a space of bounded functions. Now we assume
that Wt has compact support [0, b]. The contraction mapping arguments of §6.3.1 can
be maintained if P(0) < ∞, where P is the inverse demand function. Assume this to
be the case.
The law of motion for the commodity pricing model is of the form
Xt+1 = αI ( Xt ) + Wt+1 ,

IID

(Wt )t≥1 ∼ φ,

X0 ∼ ψ

where I is the equilibrium investment function in (6.27), page 146. As usual, we assume that X0 is independent of (Wt )t≥1 . The shocks (Wt )t≥1 and the initial condition
X0 are random variables on some probability space (Ω, F , ). The stochastic kernel
P is given by
P( x, B) :=



B [ αI ( x ) + z ] φ ( dz )

( x ∈ S, B ∈ B (S))

and the Markov operator M is deﬁned in the usual way. In example 11.2.7 (page 259)
we saw that S := [0, s̄] is a valid state space for this model when s̄ := b/(1 − α), and
that a stationary distribution ψ∗ ∈ P (S) always exists. When discussing stability, we
will use the metric
μ − ν := sup |μ( B) − ν( B)|
(11.19)
B ∈B ( S )

Stochastic Dynamics

273

on P (S), which is proportional to total variation distance (see page 254).
Theorem 11.3.1 If φ(z0 ) > 0 whenever z0 > 0, then (P (S), M) is globally stable. In fact
there exists a k ∈ and a δ < 1 such that

ψMt×k+1 − ψ∗  ≤ δt

for all ψ ∈ P (S), t ∈

(11.20)

It is an exercise to show that (11.20) implies global stability of (P (S), M).27 The
proof of (11.20) is based on the following inequality, a version of which was previously
discussed in lemma 5.2.5 (page 115):
Lemma 11.3.2 If X and Y are two random variables on (Ω, F , ) with X ∼ ψX ∈ P (S)
and Y ∼ ψY ∈ P (S), then
ψX − ψY  ≤ { X = Y }
(11.21)
Intuitively, if the probability that X and Y differ is small, then so is the distance
between their distributions. The beauty of lemma 11.3.2 is that it holds for any X and
Y with the appropriate distributions, and careful choice of these random variables can
yield a tight bound.
Proof. Pick any B ∈ B (S). We have

{ X ∈ B} =

{ X ∈ B} ∩ { X = Y } + { X ∈ B} ∩ { X = Y }, and

{Y ∈ B } =

{Y ∈ B } ∩ { X = Y } + {Y ∈ B } ∩ { X  = Y }

Since { X ∈ B} ∩ { X = Y } = {Y ∈ B} ∩ { X = Y }, we have

{ X ∈ B } − {Y ∈ B } =
∴

{ X ∈ B } ∩ { X  = Y } − {Y ∈ B } ∩ { X  = Y }

{ X ∈ B } − {Y ∈ B } ≤

{ X ∈ B} ∩ { X = Y } ≤

{ X = Y }

Reversing the roles of X and Y gives

| { X ∈ B} − {Y ∈ B}| ≤

{ X = Y }

Since B is arbitrary, we have established (11.21).
Our strategy for proving theorem 11.3.1 is as follows: Given the harvest process
(Wt )t≥1 , let ( Xt )t≥0 and ( Xt∗ )t≥0 be deﬁned by
Xt+1 = αI ( Xt ) + Wt+1 , X0 ∼ ψ
27 Hint:

and

Xt∗+1 = αI ( Xt∗ ) + Wt+1 , X0∗ ∼ ψ∗

Use nonexpansiveness. See lemma 11.1.28 on page 254.

274

Chapter 11

Notice that Xt∗ ∼ ψ∗ for all t. Hence, by (11.21), we have

ψMt − ψ∗  ≤

{ Xt = Xt∗ }

(t ∈

)

(11.22)

Thus to bound ψMt − ψ∗ , it is sufﬁcient to bound { Xt = Xt∗ }. In other words,
we need to show that the probability Xt and Xt∗ remain distinct converges to zero in
t—or, conversely, that Xt and Xt∗ are eventually equal with high probability.
Critical to the proof are two facts. One is that ( Xt )t≥0 and ( Xt∗ )t≥0 are driven by
the same sequence of harvests (Wt )t≥1 . As a result, if the two processes meet, then they
remain equal: if X j = X j∗ for some j, then Xt = Xt∗ for all t ≥ j. Second, there exists an
xb > 0 such that I ( x ) = 0 for all x ≤ xb (see lemma 11.3.3 below). As a result, if both
X j ≤ xb and X j∗ ≤ xb , then I ( X j ) = I ( X j∗ ) = 0, in which case X j+1 = X j∗+1 = Wj+1 .
As a consequence of these two properties, for Xt = Xt∗ to hold, it is sufﬁcient that
both X j ≤ xb and X j∗ ≤ xb for some j < t. Moreover this will occur whenever there
is a sufﬁciently long sequence of sufﬁciently small harvests. We will show that the
probability such a sequence has occurred at least once prior to t converges to one as
t → ∞; and hence { Xt = Xt∗ } → 0.
An illustration of the coupling of ( Xt )t≥0 and ( Xt∗ )t≥0 is given in ﬁgure 11.4, which
shows simulated paths for these two time series.28 At t = 4 both Xt and Xt∗ are below
the threshold xb at which investment becomes zero. As a result Xt = Xt∗ for all t ≥ 5.
The two processes are said to couple at t = 5, and that date is referred as the coupling
time.
Now let’s turn to details, beginning with the following lemma. (To maintain continuity, the proof is given in the appendix to this chapter.)
Lemma 11.3.3 There exists an xb > 0 such that x ≤ xb implies I ( x ) = 0.
The processes ( Xt )t≥0 and ( Xt∗ )t≥0 couple at j + 1 if { X j ≤ xb } ∩ { X j∗ ≤ xb } occurs.
To check for occurrence of this event, it is convenient to deﬁne a third process that acts
as an upper bound for ( Xt )t≥0 and ( Xt∗ )t≥0 :
Xt+1 = αXt + Wt+1 ,

X0 = s̄

Exercise 11.3.4 Show by induction that the inequalities X j ≤ X j and X j∗ ≤ X j hold
pointwise on Ω, and as a consequence

{ X j ≤ xb } ⊂ { X j ≤ xb } ∩ { X j∗ ≤ xb }
for all j ≥ 0. (We are, of course, referring to subsets of Ω here.)
the simulation, the primitives are α = 0.9, ξ ∼ cU where c = 4 and U is Beta(5,5), and P( x ) = s̄e− x .
As above, s̄ = b(1 − α)−1 . Since U ≤ 1, we have b = 4, and s̄ = 40.
28 In

Stochastic Dynamics

275

Xt

Xt∗
xb

5

10

15

20

25

time

Figure 11.4 Coupling of ( Xt )t≥0 and ( Xt∗ )t≥0 at t = 5
Given that if ( Xt )t≥0 and ( Xt∗ )t≥0 meet they remain equal, and given that X j ≤ xb
implies X j+1 = X j∗+1 , we have
X j ≤ xb for some j ≤ t =⇒ Xt+1 = Xt∗+1
In terms of subsets of Ω, this can be stated as

∪ j≤t { X j ≤ xb } ⊂ { Xt+1 = Xt∗+1 }
∴

∪ j≤t { X j ≤ xb } ≤

{ Xt+1 = Xt∗+1 }

∴

{ Xt+1 = Xt∗+1 } ≤

∩ j≤t { X j > xb }

The probability of the event ∩ j≤t { X j > xb } can be bounded relatively easily. Indeed
suppose that harvests Wn+1 to Wn+k are all below z0 , where k and z0 are chosen to
satisfy
1 − αk
αk s̄ + z0
≤ xb
(11.23)
1−α
Since the harvests are all below z0 , we have
X j ≤ αX j−1 + z0

j = n + 1, . . . , n + k

Combining these k inequalities gives
Xn + k ≤ α k Xn + z0

1 − αk
1 − αk
≤ αk s̄ + z0
≤ xb
1−α
1−α

276

Chapter 11

where the last inequality follows from (11.23). Thus a sequence of k harvests below z0
forces ( Xt )t≥0 below xb by the end of the sequence. For dates of the form t × k there
are t nonoverlapping sequences of k consecutive harvests prior to t × k. Let Ei be the
event that the i-th of these t sequences has all harvests below z0 :
k
Ei = ∩ij×
{Wj ≤ z0 }
=k×(i −1)+1

i = 1, . . . , t

If X j never falls below xb in the period up to t × k, then none of the events Ei has
occurred.
∴ ∩ j≤t×k { X j > xb } ⊂ ∩it=1 Eic

∴

{ Xt×k+1 = Xt∗×k+1 } ≤

∩ j≤t×k { X j > xb } ≤

∩it=1 Eic

Since the sequences of harvests that make up each Ei are nonoverlapping, these events
are independent, and ∩it=1 Eic = ∏it=1 (1 − ( Ei )).

∴

t

{ Xt×k+1 = Xt∗×k+1 } ≤ ∏(1 − ( Ei )) = (1 − φ(z0 )k )t
i =1

We have now established (11.20) and hence theorem 11.3.1.

11.3.2

Coupling and the Dobrushin Coefﬁcient

Let S ∈ B ( n ), and let P be an arbitrary stochastic kernel on S. In the previous
section S was a subset of , and P had the attractive property that for any x, x in
[0, xb ] ⊂ S, P( x, dy) = P( x , dy). Such a set is called an atom of P. Existence of an atom
to which the state returns regularly makes coupling particularly simple: Whenever
the two chains ( Xt )t≥0 and ( Xt∗ )t≥0 in §11.3.1 enter the atom simultaneously they can
be coupled.
Unfortunately, most Markov chains studied in economic applications fail to have
this structure. However, it turns out that with a little bit of trickery one can construct
couplings for many chains without using atoms. In this section we discuss the case
where P has a positive Dobrushin coefﬁcient. As we show, positivity of the Dobrushin
coefﬁcient is very closely connected with the possibility of successful coupling.
Understanding the connection between coupling and the Dobrushin coefﬁcient
is satisfying because the latter plays such an important role in analytical proofs of
stability. Coupling will shine a light on the role of the Dobrushin coefﬁcient from a
new angle. More importantly, the basic idea behind the coupling proof we use here
can be generalized to a large number of different models.
As in §11.3.1, we will endow P (S) with the metric deﬁned by (11.19), which is
proportional to total variation distance. Our main result is as follows:

Stochastic Dynamics

277

Proposition 11.3.5 Let ψ, ψ ∈ P (S). If α( P) is the Dobrushin coefﬁcient for P and M is
the corresponding Markov operator, then

ψMt − ψ Mt  ≤ (1 − α( P))t

(t ∈

)

If α( P) = 0, then this inequality is trivial. On the other hand, if α( P) > 0, then
for any initial conditions ψ and ψ , we have ψMt − ψ Mt  → 0 at a geometric rate.
In particular, if ψ = ψ∗ is stationary, then ψMt − ψ∗  → 0. Note also that we are
proving nothing new here. Theorem 11.2.21 yields the same result. What is new is the
proof—which is radically different.
Since the proposition is trivial when α( P) = 0, we assume in all of what follows
that α( P) is strictly positive. To make the proof, we will build two processes ( Xt )t≥0
and ( Xt )t≥0 such that the distribution ψt of Xt is ψMt and the distribution ψt of Xt is
ψ Mt . In view of lemma 11.3.2 (page 273), we then have

ψMt − ψ Mt  ≤

{ Xt  = Xt }

(11.24)

Given (11.24) it is sufﬁcient to show that { Xt = Xt } ≤ (1 − α( P))t . The trick to
the proof is constructing ( Xt )t≥0 and ( Xt )t≥0 in a rather special way. In particular,
we build these processes so that there is an independent α( P) probability of meeting
at every step, and, moreover, once the processes meet (i.e., X j = X j for some j) they
remain coupled (i.e., Xt = Xt for all t ≥ j). It then follows that if Xt = Xt , then the
two processes have never met, and the probability of this is less than (1 − α( P))t .
While ( Xt )t≥0 and ( Xt )t≥0 are constructed in a special way, care must be taken that
ψt = ψMt and ψt = ψ Mt remains valid. This requires that X0 ∼ ψ, X0 ∼ ψ , and,
recursively, Xt+1 ∼ P( Xt , dy) and Xt+1 ∼ P( Xt , dy). That such is the case does not
appear obvious from the construction, but at the end of our proof we will verify that
it is.
In order to construct the processes ( Xt )t≥0 and ( Xt )t≥0 , it is convenient to introduce some additional notation. First, let
γ( x, x ) := ( Px ∧ Px )(S)

(( x, x ) ∈ S × S)

be the afﬁnity between P( x, dy) and P( x , dy), so α( P) = infx,x γ( x, x ). Evidently
γ( x, x ) ≥ α( P) > 0 for every x and x .
Next let us deﬁne three new functions from S × S × B (S) to [0, 1] by
ν( x, x , B) :=
μ( x, x , B) :=

( Px ∧ Px )( B)
γ( x, x )

P( x, B) − ( Px ∧ Px )( B)
1 − γ( x, x )

278

Chapter 11
μ ( x, x , B) :=

P( x , B) − ( Px ∧ Px )( B)
1 − γ( x, x )

Crucially, ν( x, x , dy), μ( x, x , dy) and μ ( x, x , dy) are all probability measures on S. In
particular, ν( x, x , B), μ( x, x , B) and μ ( x, x , B) are nonnegative for all B ∈ B (S); and
ν( x, x , S) = μ( x, x , S) = μ ( x, x , S) = 1. The next exercise asks you to show that this
is the case.
Exercise 11.3.6 Prove that ν( x, x , dy), μ( x, x , dy) and μ ( x, x , dy) are probability measures on S for every ( x, x ) ∈ S × S such that γ( x, x ) < 1.29
Algorithm 11.1 Coupling algorithm
draw X0 ∼ ψ and X0 ∼ ψ∗
set t = 0
while True do
if Xt = Xt then
draw Z ∼ P( Xt , dy) and set Xt+1 = Xt+1 = Z
else
draw Ut+1 independently from Uniform(0, 1)
if Ut+1 ≤ γ( Xt , Xt ) then
// with probability γ( Xt , Xt )
draw Z ∼ ν( Xt , Xt , dy) and set Xt+1 = Xt+1 = Z
else
// with probability 1 − γ( Xt , Xt )
draw Xt+1 ∼ μ( Xt , Xt , dy)
draw Xt+1 ∼ μ ( Xt , Xt , dy)
end
end
set t = t + 1
end

We are now ready to build the processes ( Xt )t≥0 and ( Xt )t≥0 . This is done in
algorithm 11.1. The while loop in the algorithm is an inﬁnite loop. If at time t we have
Xt = Xt , then a uniform random variable Ut+1 is drawn to determine the next action.
The probability that Ut+1 ≤ γ( Xt , Xt ) is γ( Xt , Xt ), and if this occurs, then both Xt+1
and Xt+1 are set equal to a single draw from ν( Xt , Xt , dy). Otherwise, they are drawn
from μ( Xt , Xt , dy) and μ ( Xt , Xt , dy) respectively. All random variables are assumed
to live on probability space (Ω, F , ).
Our ﬁrst claim is that for the processes ( Xt )t≥0 and ( Xt )t≥0 we have

{ Xt = Xt } ≤ (1 − α( P))t
29 Note

that γ( x, x ) > 0 by assumption.

(t ∈

)

(11.25)

Stochastic Dynamics

279

The proof is not difﬁcult: Fix any t ∈ . Observe that if Uj ≤ γ( X j−1 , X j−1 ) for just
one j ≤ t, then the two processes couple and Xt = Xt . So, if Xt = Xt , then we must
have Uj > γ( X j−1 , X j−1 ) for all j ≤ t. Since γ( X j−1 , X j−1 ) ≥ α( P), and since (Ut )t≥1
is IID and uniformly distributed on (0, 1), the probability of this event is no more than
(1 − α( P))t . As t is arbitrary, the proof of (11.25) is now done.
In view of (11.25), proposition 11.3.5 will be established if we can verify (11.24); in
other words, we need to show that

ψMt − ψ Mt  ≤

{ Xt  = Xt }

As discussed above, this is implied by lemma 11.3.2 (page 273), provided that the
distributions of Xt and Xt are ψMt and ψ Mt respectively. Since X0 ∼ ψ and X0 ∼ ψ
certainly hold, we need only show that Xt+1 ∼ P( Xt , dy) and Xt+1 ∼ P( Xt , dy) at
each step.
To see that this is the case, suppose that at time t we have ( Xt , Xt ) = ( x, x ). We
claim that after the next iteration of algorithm 11.1, the probability that Xt+1 ∈ B is
P( x, B), while the probability that Xt+1 ∈ B is P( x , B). Let’s focus for now on the
claim that the probability that Xt+1 ∈ B is P( x, B).
Suppose ﬁrst that x = x . If γ( x, x ) = 1, then Xt+1 is drawn from ν( x, x , dy)
with probability one. But γ( x, x ) = 1 implies that P( x, dy) = P( x , dy), and hence
ν( x, x , dy) = P( x, dy) = P( x , dy). In particular, the probability that Xt+1 ∈ B is
P( x, B). On the other hand, if γ( x, x ) < 1, then Xt+1 is drawn from ν( x, x , dy) with
probability γ( x, x ), and from μ( x, x , dy) with probability 1 − γ( x, x ). As a result the
probability that Xt+1 ∈ B is
γ( x, x )ν( x, x , B) + (1 − γ( x, x ))μ( x, x , B)
A look at the deﬁnitions of ν and μ conﬁrms that this is precisely P( x, B).
The argument that Xt+1 ∈ B with probability P( x , B) is almost identical.
Finally, suppose that x = x . Then Xt+1 ∼ P( x, dy), so clearly Xt+1 ∈ B with
probability P( x, B). Also Xt+1 = Xt+1 , so Xt+1 ∈ B with probability P( x, B). But since
x = x, we have P( x , B) = P( x, B). Hence the probability that Xt+1 ∈ B is P( x , B).
The proof is done.

11.3.3

Stability via Monotonicity

Economic models often possess a degree of monotonicity in the laws of motion. If
a nice property such as monotonicity holds, then we would like to exploit it when
considering stability. The question of when monotone stochastic systems are stable is
the topic of this section.

280

Chapter 11

Consider again our canonical SRS introduced on page 219. To reiterate, the state
space S is a Borel subset of n and the shock space Z is a Borel subset of k . The
SRS is described by a (Borel) measurable function F : S × Z → S and a distribution
φ ∈ P ( Z ), where
Xt+1 = F ( Xt , Wt+1 ),

IID

(Wt )t≥1 ∼ φ,

X0 ∼ ψ ∈ P ( S )

(11.26)

The sequence of shocks (Wt )t≥1 and the initial condition X0 live on a common probability space (Ω, F , ) and are mutually independent. The stochastic kernel for (11.26)
is given by

P( x, B) =

B [ F ( x, z )] φ ( dz )

( x ∈ S, B ∈ B (S))

(11.27)

Let M be the corresponding Markov operator, so ψ∗ is stationary for (11.26) if and
only if ψ∗ M = ψ∗ .
Deﬁnition 11.3.7 The SRS (11.26) is said to be monotone increasing if, for all z ∈ Z,30
F ( x, z) ≤ F ( x , z) whenever x ≤ x

(11.28)

Example 11.3.8 Consider the one-dimensional linear system
Xt+1 = αX + Wt+1 ,
where S = Z =
if α ≥ 0.

IID

(Wt )t≥1 ∼ φ ∈ P ( )

(11.29)

and F ( x, z) = αx + z. This SRS is monotone increasing if and only

Exercise 11.3.9 Suppose that (11.26) is monotone increasing, and h : S →
that if h ∈ ibS, then Mh ∈ ibS.31

. Show

Exercise 11.3.10 A set B ⊂ S is called an increasing set if x ∈ B and x ∈ S with
x ≤ x implies x ∈ B. Show that B ⊂ S is an increasing set if and only if B ∈ ibS.
(Since P( x, B) = M B ( x ), it follows that x → P( x, B) is increasing whenever B is an
increasing set. What is the intuition?)
Returning to the general SRS (11.26), let’s assume that at least one stationary distribution ψ∗ exists, and see what conditions we might need to obtain uniqueness and
stability of ψ∗ under the hypothesis of monotonicity. In this section stability of ψ∗ will
have the slightly nonstandard deﬁnition

∀ ψ ∈ P (S), (ψMt )(h) → ψ∗ (h) as t → ∞ for all h ∈ ibS
order on n is the usual one: ( xi )in=1 ≤ (yi )in=1 if xi ≤ yi for all i with 1 ≤ i ≤ n.
usual, ibS denotes the increasing bounded functions h : S → .

30 Our
31 As

(11.30)

Stochastic Dynamics

281

To understand condition (11.30), recall that a sequence (μn ) ⊂ P (S) converges to
μ ∈ P (S) weakly if μn (h) → μ(h) as n → ∞ for all h ∈ bcS. Moreover, to verify weak
convergence, it sufﬁces to check μn (h) → μ(h) only for h ∈ ibcS (theorem 11.1.35,
page 255). Since ibcS ⊂ ibS, the convergence in (11.30) is stronger than ψMt → ψ∗
weakly. Finally, (11.30) implies uniqueness of ψ∗ , as follows from the next exercise.
Exercise 11.3.11 Show that if ψ∗∗ ∈ P (S) satisﬁes ψ∗∗ M = ψ∗∗ and (11.30) holds,
then ψ∗∗ and ψ∗ must be equal.32
Now let (Wt )t≥1 and (Wt )t≥1 be jointly independent IID processes on Z, both distributed according to φ and deﬁned on the common probability space (Ω, F , ). Using these two independent shock processes, we can introduce a condition that is sufﬁcient for stability of monotone systems.
Deﬁnition 11.3.12 The SRS (11.26) is called order mixing if, given any two independent
initial conditions X0 and X0 , the processes ( Xt )t≥0 and ( Xt )t≥0 deﬁned by Xt+1 =
F ( Xt , Wt+1 ) and Xt+1 = F ( Xt , Wt+1 ) satisfy

∪ t ≥0 { X t ≤ X t } =
Exercise 11.3.13 Prove:

∪ t ≥0 { X t ≤ X t } = 1

∪t≥0 { Xt ≤ Xt } = 1 iff limT →∞

(11.31)

∩t≤T { Xt  Xt } = 0.33

Paired with monotonicity, order mixing is sufﬁcient for stability. In particular, we
have the following result:
Theorem 11.3.14 Suppose that (11.26) has at least one stationary distribution ψ∗ ∈ P (S).
If it is monotone increasing and order mixing, then ψ∗ is the only stationary distribution, and
moreover ψ∗ is globally stable in the sense of (11.30).
The proof is given at the end of §11.3.4. For now let us consider how to apply this
result. To do so, it is necessary to develop sufﬁcient conditions for order mixing that
are easy to check in applications. One set of conditions that implies order mixing is
that introduced by Razin and Yahav (1979), and extended and popularized by Stokey
and Lucas (1989) and Hopenhayn and Prescott (1992). Following Stokey and Lucas
(1989, assumption 12.1), suppose that S = [ a, b] := { x ∈ n : a ≤ x ≤ b} for ﬁxed
a, b ∈ n , and that

∃m∈

, c ∈ S,  > 0 such that Pm ( a, [c, b]) ≥  and Pm (b, [ a, c]) ≥ 

(11.32)

Let’s consider this condition in our setting, where P is the kernel (11.27) and (11.26) is
monotone increasing.
32 Hint:
33 Hint:

See theorem 11.1.36 on page 255.
Use exercise 7.1.25 on page 167. Note that all sets are subsets of Ω.

282

Chapter 11

Exercise 11.3.15 Show that under (11.32) the kernel P satisﬁes Pm ( x, [c, b]) ≥  and
Pm ( x, [ a, c]) ≥  for all x ∈ S.34
If (11.26) satisﬁes (11.32), then it is order mixing. To get a feel for the proof, suppose
that (11.32) holds with m = 1, and consider the probability that there exists a t ≥ 0
with Xt ≤ Xt . In light of exercise 11.3.13, to show this probability is one, it is sufﬁcient
to show that limT →∞ ( ET ) = 0, where ET := ∩t≤T { Xt  Xt }. The probability of
Xt ≤ Xt given ( Xt−1 , Xt−1 ) = ( x, x ) is

{ F ( x, Wt ) ≤ F ( x , Wt )} ≥
=

{ F ( x, Wt ) ≤ c} ∩ { F ( x , Wt ) ≥ c}
{ F ( x, Wt ) ≤ c} { F ( x , Wt ) ≥ c}

= P( x, [ a, c]) P( x , [c, b]) ≥ 2
Hence for each t the probability of Xt ≤ Xt occurring is at least 2 , independent of the
lagged value of the state. One can then show that the probability ( ET ) of this event
never occurring prior to T is ≤ (1 − 2 ) T → 0.35 From exercise 11.3.13 we then have
∪t≥0 { Xt ≤ Xt } = 1. A similar argument establishes ∪t≥0 { Xt ≥ Xt } = 1, and
hence order mixing.
Condition (11.32) can be restrictive, as the state space must be of the form { x ∈
n : a ≤ x ≤ b }. To devise a weaker set of sufﬁcient conditions, we introduce the
concept of order inducing sets and order norm-like functions.
Deﬁnition 11.3.16 A C ∈ B (S) is called order inducing for kernel P if there exists a
c ∈ S and an m ∈ such that
inf Pm ( x, {z : z ≤ c}) > 0 and inf Pm ( x, {z : z ≥ c}) > 0

x ∈C

x ∈C

A measurable function v : S →
of v is order inducing for P.36

+

is called order norm-like for P if every sublevel set

Example 11.3.17 Continuing with the model (11.29) in example 11.3.8, suppose that
{Wt ≤ d} > 0 and {Wt ≥ d} > 0 for all d ∈ S = . Then every set of the form
[−b, b] is order inducing with m = 1 and c = 0. To see this, pick any b ≥ 0. For
x ∈ [−b, b] we have
P( x, {z : z ≤ 0}) =

{αx + W ≤ 0} =

{W ≤ −αx } ≥

{W ≤ −αb}

which is positive. The proof that infx∈C P( x, {z : z ≥ 0}) > 0 is similar.
Since all sets of the form [−b, b] are order inducing, it follows that v( x ) = | x | is
order norm-like for this model. (Why?)
34 Hint:

Use exercise 11.3.10.
complete proof can be made along the lines of of proposition 5.2.6, or using conditional expectations.
See also Kamihigashi and Stachurski (2008).
36 Recall that the sublevel sets of v are sets of the form { x ∈ S : v ( x ) ≤ a } for a ∈
+.
35 A

Stochastic Dynamics

283

Exercise 11.3.18 Show that all measurable subsets of order inducing sets are order
inducing. Show that v : S → + is order norm-like if and only if there exists a K ∈ +
such that { x ∈ S : v( x ) ≤ a} is order inducing for all a ≥ K.
We can now state the following sufﬁcient condition for order mixing:
Theorem 11.3.19 If there exists an order norm-like function v for (11.26) and constants
α ∈ [0, 1) and β ∈ + such that


v[ F ( x, z)]φ(dz) ≤ αv( x ) + β

( x ∈ S)

(11.33)

then (11.26) is order mixing.
The intuition is that under (11.33) there is drift back to (sufﬁciently large) sublevel sets of v, which are order inducing. When two independent chains ( Xt )t≥0 and
( Xt )t≥0 enter such an order inducing set, there is a positive probability of the orderings
Xt ≤ Xt and Xt ≥ Xt occurring within m periods. Repeated over an inﬁnite horizon,
these orderings eventually occur with probability one. For a proof, see Kamihigashi
and Stachurski (2008).
Example 11.3.20 Continuing with example 11.3.17, we saw that v( x ) = | x | is order
norm-like for this model. If |Wt | < ∞ and |α| < 1, then the model is order mixing,
since for any x ∈ S,


11.3.4

v[ F ( x, z)]φ(dz) =



|αx + z|φ(dz) ≤ |α|v( x ) +



|z|φ(dz)

More on Monotonicity

Let’s now turn to a less trivial example of how monotonicity and order mixing can
be used to establish stability.37 Recall the stochastic Solow–Swan growth model discussed in example 9.2.4, page 220, with law of motion
k t+1 = s f (k t )Wt+1 + (1 − δ)k t

(11.34)

and increasing production function f : + → + with f (k) > 0 for all k > 0. The productivity shock Wt and the capital stock k t take values in Z := S := (0, ∞), while the
parameters satisfy s ∈ (0, 1) and δ ∈ (0, 1]. The model (11.34) is monotone increasing
in the sense of deﬁnition 11.3.7.
Let’s look for conditions under which (11.34) is order mixing. For simplicity we assume that δ = 1.38 We suppose further that limk→0 f (k)/k = ∞, and limk→∞ f (k)/k =
37 See
38 The

also §12.1.3, which treats the optimal growth model using monotonicity.
case δ < 1 can be accommodated at the cost of a longer proof.

284

Chapter 11

0. (More traditional Inada conditions can be used if f is assumed to be differentiable.)
Regarding the distribution φ, we assume that both Wt and (1/Wt ) are ﬁnite, and
that {Wt ≤ x } and {Wt ≥ x } are strictly positive for every x ∈ S.39
First let’s show that all closed intervals [ a, b] ⊂ S are order inducing for this model.
To do so, pick any a ≤ b. Fix any c ∈ S. It sufﬁces to show that infa≤ x≤b P( x, [c, ∞)) >
0 and likewise for (0, c]. Given our assumptions on φ, for any x ∈ [ a, b] we have
P( x, [c, ∞)) =

=

{s f ( x )Wt+1 ≥ c}
{Wt+1 ≥ c/(s f ( x ))} ≥

{Wt+1 ≥ c/(s f ( a))} > 0

A similar argument gives infa≤ x≤b P( x, (0, c]) > 0.
Now consider the function v( x ) := 1/x + x. Since sublevel sets of v are closed
intervals in S, the function v is order norm-like on S.
Exercise 11.3.21 Suppose that f is unbounded, and f (0) = 0. Show that neither x → x
nor x → 1/x are order norm-like for (11.34).
To complete the proof of stability, we must show that the drift condition (11.33)
holds for v. That is, we need to establish the existence of an α ∈ [0, 1) and a β ∈ +
such that
1
(11.35)
Mv ≤ αv + β when v( x ) = x +
x

where M is the Markov operator deﬁned by Mh( x ) = h[s f ( x )z]φ(dz). To this end,
suppose that we can establish the existence of α1 , α2 ∈ [0, 1) and β 1 , β 2 ∈ + with
Mv1 ≤ α1 v1 + β 1

and

Mv2 ≤ α2 v2 + β 2

(11.36)

where v1 ( x ) = x and v2 ( x ) = 1/x. Then since v = v1 + v2 , adding across the two
inequalities in (11.36) gives the desired inequality (11.35) because
Mv = M(v1 + v2 ) = Mv1 + Mv2 ≤ α1 v1 + β 1 + α2 v2 + β 2 ≤ αv + β
when α := max{α1 , α2 } and β := β 1 + β 2 . In other words, to establish the drift
condition (11.35) and hence order mixing, it is sufﬁcient to establish separately the two
drift conditions in (11.36). Intuitively, the drift condition on v1 prevents divergence to
+∞, which the condition on v2 prevents divergence to zero.
Let’s attempt to establish the two inequalities in (11.36), starting with the left-hand
side (i.e., the inequality Mv1 ≤ α1 v1 + β 1 ).
39 We

require a lot of mixing for global stability because f is not assumed to be concave. In fact f can be
rather arbitrary, and the deterministic model (i.e., Wt ≡ 1 for all t) can have many ﬁxed points.

Stochastic Dynamics

285

Exercise 11.3.22 Fix any constant α1 ∈ (0, 1). Using the assumption limx→∞ f ( x )/x =
0, establish the existence of a γ ∈ S satisfying
s f ( x ) W1 ≤ α1 x

∀x > γ

Using monotonicity of f , show further that there is a β 1 ∈
s f ( x ) W1 ≤ β 1

+

with

∀x ≤ γ

Based on these two inequalities, conclude that
Mv1 ( x ) = s f ( x ) W1 ≤ α1 x + β 1 = α1 v1 ( x ) + β 1

∀x ∈ S

The last exercise establishes the ﬁrst of the two inequalities in (11.36). The next
exercise establishes the second inequality.
Exercise 11.3.23 Fix any constant α2 ∈ (0, 1). Using the assumption limx→0 f ( x )/x =
∞, establish the existence of a γ ∈ S satisfying
!
"
1
1
≤ α2
∀x < γ
s f ( x )W1
x
Using monotonicity of f , show there exists a β 2 ∈ + with
!
"
1
≤ β2
∀x ≥ γ
s f ( x )W1
Based on these two inequalities, conclude that
!
"
1
1
≤ α2 + β 2 = α2 v2 ( x ) + β 2
Mv2 ( x ) =
s f ( x )W1
x

∀x ∈ S

Thus the second inequality in (11.36) also holds, and theorem 11.3.19 implies that
under our assumptions the stochastic Solow–Swan growth model is order mixing.
Since it is also monotone increasing, theorem 11.3.14 implies that should any stationary distribution ψ∗ ∈ P (S) exist, that stationary distribution would be unique and
globally stable in the sense of (11.30). If f is continuous, then since v is also norm-like
(as well as order norm-like, see lemma 8.2.17 on page 209), existence of ψ∗ follows
immediately from corollary 11.2.10.
To complete this section, let’s give the proof of theorem 11.3.14. Using the two independent series (Wt )t≥1 and (Wt )t≥1 in the deﬁnition of order mixing, algorithm 11.2
deﬁnes four processes, denoted by ( Xt )t≥0 , ( Xt )t≥0 , ( XtL )t≥0 and ( XtU )t≥0 .40 The process ( Xt )t≥0 is the original SRS in (11.26), with initial condition ψ. The process ( Xt )t≥0
40 Formally, ω ∈ Ω is drawn at the start of time according to , thereby determining the initial conditions
X0 (ω ) and X0 (ω ), and the shock realizations (Wt (ω ))t≥1 and (Wt (ω ))t≥1 . These in turn determine the
realizations of all other random variables.

286

Chapter 11

X0 = X0L

(Xt)

(Xt )
(XtL)

X0

0

time

TL

Figure 11.5 The process ( XtL )t≥0
has the same law of motion, is driven by independent shocks (Wt )t≥1 , and starts at the
stationary distribution ψ∗ . Clearly, Xt ∼ ψ∗ for all t ≥ 0. The process ( XtL )t≥0 starts
off equal to X0 , and updates with shock Wt+1 if XtL ≤ Xt and with Wt+1 otherwise.
The process ( XtU )t≥0 also starts off equal to X0 , and updates with Wt+1 if XtU ≥ Xt
and with Wt+1 otherwise. An illustration of the process ( XtL )t≥0 is given in ﬁgure 11.5.
To help clarify the algorithm, we introduce two random variables:
T L := min{t ≥ 0 : Xt ≤ Xt }

and

T U := min{t ≥ 0 : Xt ≥ Xt }

with the usual convention that min ∅ = ∞. Three properties of ( XtL )t≥0 and ( XtU )t≥0
are pertinent. The ﬁrst is that ( XtL )t≥0 and ( XtU )t≥0 are identical to ( Xt )t≥0 until T L
and T U respectively. This just follows from the logic of the algorithm. Second, for
t ≥ T L we have XtL ≤ Xt ; and XtU ≥ Xt for t ≥ T U . To see this, consider the case of
( XtL )t≥0 . Note that XTL L = XT L ≤ XT L by the deﬁnition of T L , and then
XTL L +1 = F ( XTL L , WT L +1 ) ≤ F ( XT L , WT L +1 ) = XT L +1
by monotonicity. Continuing in this way, we have XtL ≤ Xt for all t ≥ T L . The
argument for ( XtU )t≥0 is similar. A third property is that both XtL and XtU have the
same distribution ψMt as Xt for all t. The reason is that X0L and X0U are drawn from ψ,
and both processes are then updated by the same SRS as the original process ( Xt )t≥0 ,
even though the source of shocks switches from (Wt )t≥1 to (Wt )t≥1 at T L and T U
respectively.41
41 A

more formal argument can be made via the so-called strong Markov property. See also Kamihigashi
and Stachurski (2008).

Stochastic Dynamics

287

Algorithm 11.2 Four ( F, φ) processes
generate independent draws X0 ∼ ψ and X0 ∼ ψ∗
set X0L = X0U = X0
for t ≥ 0 do
draw Wt+1 ∼ φ and Wt+1 ∼ φ independently
set Xt+1 = F ( Xt , Wt+1 ) and Xt+1 = F ( Xt , Wt+1 )
if XtL ≤ Xt then
set XtL+1 = F ( XtL , Wt+1 )
else
set XtL+1 = F ( XtL , Wt+1 )
end
if XtU ≥ Xt then
set XtU+1 = F ( XtU , Wt+1 )
else
set XtU+1 = F ( XtU , Wt+1 )
end
end

Now to the proof. Pick any h ∈ ibS. We wish to show that (ψMt )(h) → ψ∗ (h)
as t → ∞. Order mixing tells us precisely that { T L < ∞} = 1, or { T L ≤ t} → 1
as t → ∞. Note that T L ≤ t implies XtL ≤ Xt , and since h is increasing, this implies
h( XtL ) ≤ h( Xt ). Therefore
h( XtL ) { T L ≤ t} ≤ h( Xt ) { T L ≤ t}

∴
Using

{T L

h( XtL ) { T L ≤ t} ≤

h ( Xt ) { T L ≤ t }

(11.37)

≤ t} → 1 and taking the limit superior now gives
lim sup h( XtL ) ≤ lim sup h( Xt )
t→∞

t→∞

(11.38)

where the precise derivation of (11.38) is left until the end of the proof. To continue,
since Xt ∼ ψ∗ for all t, the right-hand side is just ψ∗ (h). And since XtL ∼ ψMt for all t,
we have proved that
lim sup(ψMt )(h) ≤ ψ∗ (h)
t→∞

By a similar argument applied to XtU instead of XtL , we obtain
ψ∗ (h) ≤ lim inf(ψMt )(h)
t→∞

288

Chapter 11

It now follows that limt→∞ (ψMt )(h) = ψ∗ (h), and since h is an arbitrary element of
ibS, the claim in (11.30) is established.
To end the section, let’s see how (11.38) is derived. We have
lim sup h( XtL ) ≤ lim sup h( XtL ) { T L ≤ t} + lim sup h( XtL ) { T L > t}
t→∞

t→∞

t→∞

Since h is bounded the last term is zero, and hence
lim sup h( XtL ) ≤ lim sup h( XtL ) { T L ≤ t} ≤ lim sup h( Xt ) { T L ≤ t} = ψ∗ (h)
t→∞

t→∞

t→∞

where the second inequality is due to (11.37), and the ﬁnal equality holds because
h ( Xt ) { T L ≤ t } =

h ( Xt ) −

and boundedness of h gives

11.3.5

h ( Xt ) { T L > t } = ψ ∗ ( h ) −

h ( Xt ) { T L > t }

h( Xt ) { T L > t} → 0 as t → ∞.

Further Stability Theory

In the theory covered so far, we have illustrated how stability problems in unbounded
state spaces can be treated using drift conditions. Drift conditions were used in §8.2.3
for existence, uniqueness, and stability in the density case, in §11.2.1 for existence in
the general (i.e., measure) case, and in §11.3.3 for stability in the general case under
monotonicity. It would be nice to add a stability result suitable for unbounded spaces
(i.e., using drift) that requires neither density assumptions nor monotonicity. Let us
now address this gap, providing a result for the general case that gives existence,
uniqueness, and stability without speciﬁcally requiring continuity, monotonicity or
densities. While full proofs are beyond the scope of this text, an intuitive explanation
based on coupling and the Dobrushin coefﬁcient is provided.
Let P be a stochastic kernel on S ∈ B ( n ). In §11.3.2 we showed how processes
on S can be coupled when the Dobrushin coefﬁcient is strictly positive, and how this
coupling can be used to prove stability. However, we know that on unbounded state
space the Dobrushin coefﬁcient α( P) of P is often zero.42 The problem is that α( P) is
deﬁned as the inﬁmum of the afﬁnities γ( x, x ) := ( Px ∧ Px )(S) over all ( x, x ) pairs
in S × S. The afﬁnity is close to one when P( x, dy) and P( x , dy) put probability mass
in similar areas, and converges to zero as their supports diverge. If S is unbounded,
then as the distance between x and x increases without bound, it is likely that the
supports of P( x, dy) and P( x , dy) also diverge from one another, and γ( x, x ) can be
made arbitrarily small. The end result is α( P) = 0.
42 Recall

the example shown in ﬁgure 8.7 on page 202.

Stochastic Dynamics

289

If α( P) = 0, then the coupling result in §11.3.2 fails. To see this, recall that the proof
is based on the bound
ψMt − ψ Mt  ≤ { Xt = Xt }
(11.39)
where ( Xt )t≥0 and ( Xt )t≥0 are processes with X0 ∼ ψ, X0 ∼ ψ , Xt+1 ∼ P( Xt , dy)
and Xt+1 ∼ P( Xt , dy). To show that { Xt = Xt } → 0 as t → ∞, we constructed the
sequences ( Xt )t≥0 and ( Xt )t≥0 using algorithm 11.1, where the probability of coupling
at t + 1 is γ( Xt , Xt ) ≥ α( P). This gave the bound { Xt = Xt } ≤ (1 − α( P))t .
Of course, if α( P) = 0, then this bound has no bite. However, there is a way
to extend our stability result to such cases. The basic idea is as follows: While the
inﬁmum of γ( x, x ) may be zero when taken over all of S × S, it may well be a positive
value  when taken over C × C, where C is some bounded subset of S. If such a C
exists, then we can modify our strategy by attempting to couple the chains only when
both are in C. In this case the probability of coupling at t + 1 is γ( Xt , Xt ) ≥ .
This idea is illustrated in algorithm 11.3. (Compare with algorithm 11.1.)
Algorithm 11.3 Coupling with drift
draw X0 ∼ ψ, X0 ∼ ψ∗ and set t = 0
while True do
if Xt = Xt then
draw Z ∼ P( Xt , dy) and set Xt+1 = Xt+1 = Z
else
if Xt ∈ C and Xt ∈ C then
draw Ut+1 from the uniform distribution on (0, 1)
if Ut+1 ≤ γ( Xt , Xt ) then
// with probability γ( Xt , Xt )
draw Z ∼ ν( Xt , Xt , dy) and set Xt+1 = Xt+1 = Z
else
// with probability 1 − γ( Xt , Xt )
draw Xt+1 ∼ μ( Xt , Xt , dy) and Xt+1 ∼ μ ( Xt , Xt , dy)
end
else
draw Xt+1 ∼ P( Xt , dy) and Xt+1 ∼ P( Xt , dy)
end
end
set t = t + 1
end

The algorithm can be made to work along the following lines: Suppose that the
kernel P and the set C are such that ( Xt )t≥0 and ( Xt )t≥0 return to C inﬁnitely often
with probability one. Each time both chains return to C simultaneously, there is an 
probability of coupling. From this it can be shown that { Xt = Xt } → 0 as t → ∞,

290

Chapter 11

which implies stability via (11.39).
The formal arguments are not trivial, and rather than attempting such a proof
here, we present instead some standard results that can be understood using the same
intuition. In particular, we will be looking for (1) a set C ⊂ S such that the inﬁmum
of γ( x, x ) on C × C is positive, and (2) some kind of drift condition ensuring the
chain returns to C inﬁnitely often. These conditions capture the essence of stability on
unbounded state spaces: mixing at the center of the state sufﬁcient to rule out multiple
local equilibria, and drift back to the center sufﬁcient to rule out divergence.
Before presenting these standard results, it might be helpful to consider an example for which there exists a set C ⊂ S such that the inﬁmum of γ( x, x ) on C × C is
positive:
Example 11.3.24 Consider again the STAR model discussed in §8.2.4, where Z = S =
, and the state evolves according to
IID

Xt+1 = g( Xt ) + Wt+1 ,

(Wt )t≥1 ∼ φ ∈ D (S)

(11.40)

The kernel P corresponding to this model is of the form P( x, dy) = p( x, y)dy, where
p( x, y) = φ(y − g( x )). The afﬁnities are given by
γ( x, x ) =



p( x, y) ∧ p( x , y)dy =



φ(y − g( x )) ∧ φ(y − g( x ))dy

Suppose that φ and g are both continuous, and that φ is strictly positive on
case
S × S ( x, y) → p( x, y) = φ(y − g( x )) ∈

, in which

is continuous and positive on S × S. It follows that for any compact set C, the inﬁmum
of ( x, y) → p( x, y) on C × C is greater than some δ > 0. But then
γ( x, x ) =



p( x, y) ∧ p( x , y)dy ≥


C

p( x, y) ∧ p( x , y)dy ≥ δλ(C ) =: 

for any ( x, x ) ∈ C × C. If λ(C ) > 0, then inf( x,x )∈C×C γ( x, x ) ≥  > 0.
The set C is called a small set. The traditional deﬁnition is as follows:
Deﬁnition 11.3.25 Let ν ∈ P (S) and let  > 0. A set C ∈ B (S) is called (ν, )-small
for P if for all x ∈ C we have
P( x, A) ≥ ν( A)

( A ∈ B (S))

C is called small for P if it is (ν, )-small for some ν ∈ P (S) and  > 0.

Stochastic Dynamics

291

This deﬁnition works for us because if C is small, then the inﬁmum of γ on C × C
is strictly positive. In particular, if C is (ν, )-small, then the inﬁmum is greater than
. The proof is left as an exercise:
Exercise 11.3.26 Show that if C is (ν, )-small for kernel P, then γ( x, x ) := ( Px ∧
Px )(S) ≥  for any x, x in C.
Exercise 11.3.27 Show that every measurable subset of a small set is small.
Exercise 11.3.28 Show that if P( x, dy) = p( x, y)dy for some density kernel
 p, then C ∈
B (S) is small for P whenever there exists a measurable g : S → + with g(y)dy > 0
and p( x, y) ≥ g(y) for all x ∈ C and y ∈ S.
Exercise 11.3.29 Consider the stochastic kernel P( x, dy) = p( x, y)dy = N ( ax, 1),
which corresponds to a linear AR(1) process with normal shocks. Show that every
bounded C ⊂ is a small set for this process.
Now let’s introduce a suitable drift condition.
Deﬁnition 11.3.30 The kernel P satisﬁes drift to a small set if there exists a small set C,
a measurable function v : S → [1, ∞) and constants λ ∈ [0, 1) and L ∈ + such that
Mv( x ) ≤ λv( x ) + L

C (x)

( x ∈ S)

(11.41)

Drift to a small set is not dissimilar to our other drift conditions. In this case, if the
current state Xt is at some point x ∈
/ C, then since λ < 1, the next period expectation
[v( Xt+1 ) | Xt = x ] of the “Lyapunov function” v is a fraction of the current value
v( x ). Since v ≥ 1, this cannot continue indeﬁnitely, and the state tends back toward
C.43
Sometimes the following condition is easier to check than (11.41).
Lemma 11.3.31 Suppose that there exists a measurable function v : S →
α ∈ [0, 1) and β ∈ + such that
Mv( x ) ≤ αv( x ) + β

( x ∈ S)

+

and constants
(11.42)

If all sublevel sets of v are small, then P satisﬁes drift to a small set.44
Exercise 11.3.32 Prove lemma 11.3.31. Show that we can assume without loss of generality that v in the lemma satisﬁes v ≥ 1.45 Now show that if we pick any λ ∈ (α, 1),
set C := { x : v( x ) ≤ β/(λ − α)} and L := β, then v, C, λ, and L satisfy the conditions
of deﬁnition 11.3.30.
43 Condition

(11.41) corresponds to (V4) in Meyn and Tweedie (1993, §15.2.2).
that the sublevel sets of v are sets of the form { x ∈ S : v( x ) ≤ K }, K ∈ + .
45 Show that if the lemma holds for some v, α and β then it also holds for the function v : = v + 1 and
constants α := α and β = β + 1.
44 Recall

292

Chapter 11

Finally, to present the main result, we need two deﬁnitions that appear frequently
in classical Markov chain theory. The ﬁrst is aperiodicity:
Deﬁnition 11.3.33 A kernel P is called aperiodic if P has a (ν, )-small set C with
ν(C ) > 0.46
Exercise 11.3.34 Continuing
 exercise 11.3.28, suppose there is a C ∈ B (S) and measurable g : S → + with g(y)dy > 0 and p( x, y) ≥ g(y) for all x ∈ C and y ∈ S.
Show that if C g( x )dx > 0, then P is aperiodic.
Deﬁnition 11.3.35 Let μ ∈ P (S). The kernel P is called μ-irreducible if, for all x ∈ S
and B ∈ B (S) with μ( B) > 0, there exists a t ∈
with Pt ( x, B) > 0. P is called
irreducible if it is μ-irreducible for some μ ∈ P (S).
Let P be a stochastic kernel on S. Let M be the corresponding Markov operator, and
let P (S) be endowed with the total variation metric. We can now state the following
powerful stability result:
Theorem 11.3.36 If P is irreducible, aperiodic and satisﬁes drift to a small set, then the system
(P (S), M) is globally stable with unique stationary distribution ψ∗ ∈ P (S). Moreover if
v is the function in deﬁnition 11.3.30 and h : S →
is any measurable function satisfying
|h| ≤ v, then
1 n
h( Xt ) → ψ∗ (h) with probability one as n → ∞
(11.43)
n t∑
=1
Finally, if h2 ≤ v, then there is a σ ∈ + with


√
1 n
∗
n
h( Xt ) − ψ (h) → N (0, σ2 )
n t∑
=1

weakly as n → ∞

(11.44)

For a proof of theorem 11.3.36, the reader is referred to Meyn and Tweedie (1993,
thms. 16.0.1 and 17.0.1), or Roberts and Rosenthal (2004, thms. 9 and 28). The proofs
in the second reference are closer to the coupling intuition provided in algorithm 11.3.
Remark 11.3.37 Under the conditions of theorem 11.3.36 it is known that ψ∗ (v) < ∞,
so |h| ≤ v implies ψ∗ (|h|) < ∞. Actually this last restriction is sufﬁcient for h to satisfy
the LLN (11.43).
Remark 11.3.38 Why is aperiodicity required in theorem 11.3.36? To gain some intuition we can refer back to algorithm 11.3. Under the conditions of theorem 11.3.36 we
have drift back to a small set, which corresponds to C in algorithm 11.3. Each time
46 Our

ch. 5).

deﬁnition corresponds to what is usually called strong aperiodicity. See Meyn and Tweedie (1993,

Stochastic Dynamics

293

the chains both return to C there is an opportunity to couple, and with enough such
opportunities the probability of not coupling prior to t converges to zero in t. At issue
here is that the chains must not only drift back to C, they must also return to C at the
same time. With periodic chains this can be problematic because starting at different
points in the cycle can lead to inﬁnitely many visits to C by both chains that never
coincide.
Let’s try to apply theorem 11.3.36 to the STAR model in (11.40). We assume that
the function g in (11.40) is continuous and satisﬁes

| g( x )| ≤ α| x | + c

(x ∈ S =

)

for some α < 1 and c ∈ + , that the density φ is continuous and everywhere positive
on , and that |W1 | = |z|φ(z)dz < ∞.
Exercise 11.3.39 Show that the corresponding kernel P is irreducible on S =

.47

Next let’s prove that every compact C ⊂ S is small for P. Since measurable subsets
of small sets are themselves small (exercise 11.3.27), we can assume without loss of
generality that the Lebesgue measure λ(C ) of C is strictly positive. (Why?) Now set
δ := min{ p( x, y) : ( x, y) ∈ C × C }. The quantity δ is strictly positive. (Why?) Finally,
let g := δ C .
Exercise 11.3.40 Show that C and g satisfy the all of the conditions in exercises 11.3.28
and 11.3.34.
From exercise 11.3.40 we conclude that all compact sets are small, and, moreover,
that P is aperiodic. To verify theorem 11.3.36, it remains only to check drift to a small
set holds. In view of lemma 11.3.31, it is sufﬁcient to exhibit a function v : S → + and
constants α ∈ [0, 1), β ∈ + such that all sublevel sets of v are compact and the drift
in (11.42) holds. The condition (11.42) has already been shown to hold for v( x ) :=
| x | in §8.2.4. Moreover the sublevel sets of v are clearly compact. The conditions of
theorem 11.3.36 are now veriﬁed.

11.4

Commentary

The outstanding reference for stability of Markov chains in general state spaces is
Meyn and Tweedie (1993). (At the time of writing, a second edition is reportedly
in production through Cambridge University Press.) Another good reference on the
topic is Hernández-Lerma and Lasserre (2003). Bhattacharya and Majumdar (2007)
Show ﬁrst that if x ∈ S and B ∈ B (S) has positive Lebesgue measure, then P( x, B) > 0. Now let
f ∈ D (S), and let μ ∈ P (S) be deﬁned by μ( B) = B f ( x )dx. Show that P is μ-irreducible.
47 Hint:

294

Chapter 11

give a general treatment of Markov chains with extensive discussion of stability. For
an application of Meyn and Tweedie’s ideas to time series models, see Kristensen
(2007).
I learned about the Dobrushin coefﬁcient and its role in stability through reading lecture notes of Eric Moulines, and about coupling via Lindvall (1992), Rosenthal
(2002), and Roberts and Rosenthal (2004). The idea of using coupling to prove stability
of Markov chains is due to the remarkable Wolfgang Doeblin (1938). The link between
the Dobrushin coefﬁcient and coupling in §11.3.2 is my own work, extending similar
studies by the authors listed above. While I was unable to ﬁnd this line of argument
elsewhere, I certainly doubt that it is new.
The monotonicity-based approach to stability introduced in §11.3.3 is due to Kamihigashi and Stachurski (2008). For other monotonicity-based treatments of stability,
see Razin and Yahav (1979), Bhattacharya and Lee (1988), Hopenhayn and Prescott
(1992), or Zhang (2007).
In this chapter we gave only a very brief discussion of the central limit theorem for
Markov chains. See Meyn and Tweedie (1993) for standard results, and Jones (2004)
for a survey of recent theory.

Chapter 12

More Stochastic Dynamic
Programming
In this chapter we treat some extensions to the fundamental theory of dynamic programming, as given in chapter 10. In §12.1 we investigate how additional structure
can be used to obtain new results concerning the value function and the optimal policy. In §12.2 we show how to modify our earlier optimality results when the reward
function is not bounded.

12.1

Monotonicity and Concavity

In many economic models we have more structure at our disposal than continuity and
compactness (assumptions 10.1.3–10.1.5, page 231), permitting sharper characterizations of optimal actions and more efﬁcient numerical algorithms. Often this additional
structure is in the form of monotonicity or convexity. We begin by discussing monotonicity.

12.1.1

Monotonicity

Let’s recall some deﬁnitions. Given vectors x = ( x1 , . . . , xn ) and y = (y1 , . . . , yn ) in
n , we say that x ≤ y if x ≤ y for 1 ≤ i ≤ n; and x < y if, in addition, we have
i
i
x = y. A function w : n ⊃ E →
is called increasing on E if, given x, x ∈ E with
x ≤ x , we have w( x ) ≤ w( x ); and strictly increasing if x < x implies w( x ) < w( x ).
A correspondence Γ from E to any set A is called increasing on E if Γ( x ) ⊂ Γ( x )
whenever x ≤ x . A set B ⊂ E is called an increasing subset of E if B is an increasing
295

296

Chapter 12

function on E; equivalently, if x ∈ B and x ∈ E with x ≤ x implies x ∈ B. It is called
a decreasing subset of E if x ∈ B and x ∈ E with x ≤ x implies x ∈ B. For convex
E ⊂ n , a function w : E → is called concave if,
λw( x ) + (1 − λ)w(y) ≤ w(λx + (1 − λ)y)

∀ λ ∈ [0, 1] and x, y ∈ E

and strictly concave if the inequality is strict for all x = y and all λ ∈ (0, 1).
In all of this section, S ∈ B ( n ) and A ∈ B ( k ). Furthermore the set S is assumed
to be convex. We will be working with an arbitrary SDP (r, F, Γ, φ, ρ) that obeys our
usual assumptions (page 231). The state space is S, the action space is A, and the
shock space is Z. Finally, ibcS is the increasing bounded continuous functions on S.
Exercise 12.1.1 Show that ibcS is a closed subset of (bcS, d∞ ).1 The same is not true
for the strictly increasing bounded continuous functions. (Why?)
Our ﬁrst result gives sufﬁcient conditions for the value function associated with
this SDP to be increasing on S.
Theorem 12.1.2 The value function v∗ is increasing on S whenever Γ is increasing on S and,
for any x, x ∈ S with x ≤ x , we have
1. r ( x, u) ≤ r ( x , u) for all u ∈ Γ( x ), and
2. F ( x, u, z) ≤ F ( x , u, z) for all u ∈ Γ( x ), z ∈ Z.
Proof. In the proof of lemma 10.1.20 (page 237) we saw that the Bellman operator T
satisﬁes T : bcS → bcS. Since ibcS is a closed subset of bcS and since v∗ is the ﬁxed
point of T, we need only show that T : ibcS → ibcS. (Recall exercise 4.1.20, page 61.)
To do so, take any x and x in S with x ≤ x and ﬁx w ∈ ibcS. Let σ be w-greedy
(deﬁnition 10.1.9, page 233) and let u∗ = σ ( x ). From w ∈ ibcS and our hypotheses we
obtain
Tw( x ) = r ( x, u∗ ) + ρ




w[ F ( x, u∗ , z)]φ(dz)

≤ r ( x , u∗ ) + ρ w[ F ( x , u∗ , z)]φ(dz)



≤ max r ( x , u) + ρ w[ F ( x , u, z)]φ(dz) =: Tw( x )
Γ( x )

where the second inequality follows from the assumption that Γ is increasing. (Why?)
We conclude that Tw ∈ ibcS, and hence so is v∗ .
1 Hint:

Recall that if f n → f uniformly, then f n → f pointwise.

More Stochastic Dynamic Programming

297

Exercise 12.1.3 Show that if, in addition to the hypotheses of the theorem, x < x
implies r ( x, u) < r ( x , u), then v∗ is strictly increasing.2
Exercise 12.1.4 Recall the optimal growth model, with S = A = + , Γ(y) = [0, y]
and gr Γ = all pairs (y, k ) with k ∈ [0, y]. The reward function is given on gr Γ by
r (y, k ) := U (y − k ), where U : + → . The transition function is F (y, k, z) = f (k, z).
The shocks (Wt )t≥1 are independent and takes values in Z ⊂
according to φ ∈
P ( Z ). Let the conditions of assumption 10.1.12 be satisﬁed (see page 235). Show that
the value function v∗ is increasing whenever U is, and strictly increasing when U is.
(Notice that monotonicity of the production function plays no role.)
Next let’s consider parametric monotonicity. The question is as follows: Suppose
that an objective function has maximizer u. If one now varies a given parameter in
the objective function and maximizes again, a new maximizer u is determined. Does
the maximizer always increase when the parameter increases? The connection to dynamic programming comes when the state variable is taken to be the parameter and
the corresponding optimal action is the maximizer. We wish to know when the optimal action is monotone in the state. Monotone policies are of interest not only for
their economic interpretation but also because they can speed up algorithms for approximating optimal policies.
Deﬁnition 12.1.5 Let Γ and gr Γ be as above. A function g : gr Γ → satisﬁes increasing differences on gr Γ if, whenever x, x ∈ S with x ≤ x and u, u ∈ Γ( x ) ∩ Γ( x ) with
u ≤ u , we have
g( x, u ) − g( x, u) ≤ g( x , u ) − g( x , u)
(12.1)
The function is said to satisfy strictly increasing differences on gr Γ if the inequality (12.1)
is strict whenever x < x and u < u .
Intuitively, the impact of increasing the argument from u to u has more effect on
g when the parameter x is larger. The requirement u, u ∈ Γ( x ) ∩ Γ( x ) ensures that g
is properly deﬁned at all the points in (12.1).
Example 12.1.6 Consider the optimal growth model of exercise 12.1.4. If U is strictly
concave, then r (y, k ) = U (y − k) has strictly increasing differences on gr Γ. To see this,
pick any y, y , k, k ∈ + with y < y , k < k and k, k ∈ Γ(y) ∩ Γ(y ) = [0, y]. We are
claiming that
U (y − k ) − U (y − k) < U (y − k ) − U (y − k)
or alternatively,
U (y − k ) + U (y − k) < U (y − k ) + U (y − k)

(12.2)

2 Hint: Show that the result Tw ( x ) ≤ Tw ( x ) in the proof of the theorem can be strengthened to Tw ( x ) <
Tw( x ). Hence T sends ibcS into the strictly increasing bounded continuous functions. Since v∗ ∈ ibcS it
follows that Tv∗ is strictly increasing. But then v∗ is strictly increasing. (Why?!)

298

Chapter 12

It is left as an exercise for the reader to show that strict concavity of U implies that for
each λ ∈ (0, 1) and each pair x, x with 0 ≤ x < x we have
U ( x ) + U ( x ) < U (λx + (1 − λ) x ) + U (λx + (1 − λ) x )
This yields (12.2) when
λ :=

k −k
,
y −y+k −k

x := y − k ,

x := y − k

Exercise 12.1.7 Show that if g : gr Γ →
satisﬁes strictly increasing differences on
gr Γ and h : gr Γ →
satisﬁes increasing differences on gr Γ, then g + h satisﬁes
strictly increasing differences on gr Γ.
We now present a parametric monotonicity result in the case where the action
space A is one-dimensional. Although not as general as some other results, it turns
out to be useful, and the conditions in the theorem are relatively easy to check in
applications. In the statement of the theorem we are assuming that A ⊂ , g : gr Γ →
, and argmaxu∈Γ( x) g( x, u) is nonempty for each x ∈ S.
Theorem 12.1.8 Suppose that g satisﬁes strictly increasing differences on gr Γ, that Γ is
increasing on S, and that Γ( x ) is a decreasing subset of A for every x ∈ S. Let x, x ∈ S with
x ≤ x . If u is a maximizer of a → g( x, a) on Γ( x ) and u is a maximizer of a → g( x , a) on
Γ( x ), then u ≤ u .
Proof. When x = x the result is trivial, so take x < x . Let u and u be as in the
statement of the theorem. Suppose to the contrary that u > u . Since Γ is increasing,
we have Γ( x ) ⊂ Γ( x ), and hence both u and u are in Γ( x ). Also, since u < u ∈ Γ( x ),
and since Γ( x ) is a decreasing set, both u and u are in Γ( x ). It then follows from
strictly increasing differences that
g( x , u) − g( x , u ) > g( x, u) − g( x, u )
However, from u ∈ Γ( x ), u ∈ Γ( x ) and the deﬁnition of maxima,
g( x , u ) − g( x , u) ≥ 0 ≥ g( x, u ) − g( x, u)

∴

g( x , u) − g( x , u ) ≤ g( x, u) − g( x, u )

Contradiction.
We can now derive a general parametric monotonicity result for SDPs.

More Stochastic Dynamic Programming

299

Corollary 12.1.9 Let (r, F, Γ, φ, ρ) deﬁne an SDP satisfying the conditions in theorem 12.1.2.
If in addition Γ( x ) is a decreasing subset of A for every x ∈ S, r satisﬁes strictly increasing
differences on gr Γ, and, ∀ w ∈ ibcS,
gr Γ

( x, u) →



w[ F (u, x, z)]φ(dz) ∈

satisﬁes increasing differences on gr Γ, then every optimal policy is monotone increasing on S.
Proof. Let v∗ be the value function for this SDP, and set
g( x, u) := r ( x, u) + ρ



v∗ [ F ( x, u, z)]φ(dz)

on gr Γ. If a policy is optimal, then it maximizes a → g( x, a) over Γ( x ) for each x ∈ S.
Hence we need only verify the conditions of theorem 12.1.8 for our choice of g and Γ.
The only nontrivial assertion is that g satisﬁes strictly increasing differences on gr Γ.
This follows from exercise 12.1.7 and the fact that v∗ ∈ ibcS (see theorem 12.1.2).
From corollary 12.1.9 it can be shown that investment in the optimal growth model
is monotone increasing whenever U is increasing and strictly concave. In particular,
no shape restrictions on f are necessary.
Exercise 12.1.10 Verify this claim.

12.1.2

Concavity and Differentiability

Next we consider the role of concavity and differentiability in dynamic programs.
This topic leads naturally to the Euler equation, which holds at the optimal policy
whenever the solution is interior and all primitives are sufﬁciently smooth. Although
we focus on the growth model, many other models in economics have Euler equations, and they can be derived using steps similar to those shown below. Detailed
proofs are provided, although most have been consigned to the appendix to this chapter.
To begin, let C ibcS denote the set of all concave functions in ibcS, where the latter
is endowed as usual with the supremum norm d∞ .
Exercise 12.1.11 Show that the set C ibcS is a closed subset of ibcS.
Our ﬁrst result gives conditions under which the value function v∗ is concave.
Here v∗ corresponds to our canonical SDP (r, F, Γ, φ, ρ) that obeys the standard assumptions (page 231).
Theorem 12.1.12 Let the conditions of theorem 12.1.2 hold. If, in addition,

300

Chapter 12

1. gr Γ is convex,
2. r is concave on gr Γ, and
3. ( x, u) → F ( x, u, z) is concave on gr Γ for all z ∈ Z,
then the value function v∗ is concave. In particular, we have v∗ ∈ C ibcS.
Proof. By theorem 12.1.2, T : ibcS → ibcS and v∗ ∈ ibcS. We wish to show additionally
that v∗ ∈ C ibcS. Analogous to the proof of theorem 12.1.2, since C ibcS is a closed
subset of ibcS, it sufﬁces to show that T maps C ibcS into itself. So let w ∈ C ibcS.
Since Tw ∈ ibcS, we need only show that Tw is also concave. Let x, x ∈ S, and let
λ ∈ [0, 1]. Set x := λx + (1 − λ) x . Let σ be a w-greedy policy, let u := σ( x ), and let
u := σ ( x ). Deﬁne u := λu + (1 − λ)u . Condition 1 implies that u ∈ Γ( x ), and
hence

Tw( x ) ≥ r ( x , u ) + ρ w[ F ( x , u , z)]φ(dz)
Consider the two terms on the right-hand side. By condition 2,
r ( x , u ) ≥ λr ( x, u) + (1 − λ)r ( x , u )
By condition 3 and w ∈ C ibcS,


w[ F ( x , u , z)]φ(dz) ≥



≥λ
∴

w[λF ( x, u, z) + (1 − λ) F ( x , u , z)]φ(dz)



w[ F ( x, u, z)]φ(dz) + (1 − λ)



w[ F ( x , u , z)]φ(dz)

Tw( x ) ≥ λTw( x ) + (1 − λ) Tw( x )

Hence Tw is concave on S, Tw ∈ C ibcS, and v∗ is concave.
Exercise 12.1.13 Show that if, in addition to the hypotheses of the theorem, r is strictly
concave on gr Γ, then v∗ is strictly concave.3
Exercise 12.1.14 Consider again the stochastic optimal growth model (see exercise
12.1.4 on page 297 for notation and assumption 10.1.12 on page 235 for our assumptions on the primitives). Suppose that the utility function U is strictly increasing and
strictly concave, in which case v∗ is strictly increasing (exercise 12.1.4) and any optimal
investment policy is increasing (exercise 12.1.10). Show that if, in addition, k → f (k, z)
is concave on + for each ﬁxed z ∈ Z, then v∗ is also strictly concave.
Under the present assumptions one can show that for the optimal growth model
the optimal policy is unique. Uniqueness in turn implies continuity. The details are
left for you:
3 Hint:

The argument is similar to that of exercise 12.1.3.

More Stochastic Dynamic Programming

301

Exercise 12.1.15 Let [ a, b] ⊂ , where a < b, and let g : [ a, b] → . Show that if g
is strictly concave, then g has at most one maximizer on [ a, b]. Show that under the
conditions of exercise 12.1.14, there is one and only one optimal policy for the growth
model. Show that in addition it is continuous everywhere on S.4
Now let’s turn to the Euler equation. First we need to strengthen our assumptions;
in particular, we need to ensure that our primitives are smooth.
Assumption 12.1.16 For each z ∈ Z, the function k → f (k, z) is concave, increasing, and differentiable, while z → f (k, z) is Borel measurable for each k ∈ + . The
utility function U is bounded, strictly increasing, strictly concave and differentiable.
Moreover
lim f (k, z) > 0 ∀ z ∈ Z,
and
lim U (c) = ∞
c ↓0

k ↓0

Here and below, f (k, z) denotes the partial derivative of f with respect to k. Since
U is bounded we can and do assume that U (0) = 0.5
Under the conditions of assumption 12.1.16, we know that the value function is
strictly concave and strictly increasing, while the optimal policy is unique, increasing,
and continuous. More can be said. A preliminary result is as follows.
Proposition 12.1.17 Let assumption 12.1.16 hold, and let w ∈ C ibcS. If σ is w-greedy, then
σ(y) < y for every y > 0.
The proof can be found in the appendix to this chapter. We are now ready to state
a major differentiability result.
Proposition 12.1.18 Let w ∈ C ibcS and let σ be w-greedy. If assumption 12.1.16 holds,
then Tw is differentiable at every y ∈ (0, ∞), and moreover

( Tw) (y) = U (y − σ(y))

( y > 0)

Concavity plays a key role in the proof, which can be found in the appendix to the
chapter.6
Corollary 12.1.19 Let σ be the optimal policy. If assumption 12.1.16 holds, then v∗ is differentiable and (v∗ ) (y) = U (y − σ(y)) for all y > 0.
That corollary 12.1.19 follows from proposition 12.1.18 is left as an exercise.
4 Hint:

Regarding continuity, see Berge’s theorem (page 340).
a constant to an objective function (in this case the function σ → vσ (y)) affects the maximum
but not the maximizer.
6 The argument is one of the class of so-called “envelope theorem” results. There is no one envelope
theorem that covers every case of interest, so it is worth going over the proof to get a feel for how the bits
and pieces ﬁt together.
5 Adding

302

Chapter 12

Exercise 12.1.20 Show using corollary 12.1.19 that optimal consumption is strictly
increasing in income.
There is another approach to corollary 12.1.19, which uses the following lemma
from convex analysis.
Lemma 12.1.21 If g : + →
is concave, and on some neighborhood N of y0 ∈ (0, ∞)
there is a differentiable concave function h : N →
with h(y0 ) = g(y0 ) and h ≤ g on N,
then g is differentiable at y0 and g (y0 ) = h (y0 ).
Exercise 12.1.22 Prove proposition 12.1.18 using lemma 12.1.21.
We have been working toward a derivation of the Euler (in)equality. In our statement of the result, σ := σ∗ is the optimal policy and c(y) := y − σ (y) is optimal
consumption.
Proposition 12.1.23 Let y > 0. Under assumption 12.1.16, we have
U ◦ c(y) ≥ ρ



U ◦ c[ f (σ (y), z)] f (σ (y), z)φ(dz)

( y > 0)

(12.3)

When is the Euler inequality an equality? Here is one answer:
Proposition 12.1.24 Under the additional assumption f (0, z) = 0 for all z ∈ Z, we have
σ(y) > 0 for all y > 0, and the Euler inequality always holds with equality. On the other
hand, if the inequality is strict at some y > 0, then σ(y) = 0.
The proofs of these propositions are in the appendix to this chapter.

12.1.3

Optimal Growth Dynamics

Next we consider dynamics of the growth model when agents follow the optimal
policy. We would like to know whether the system is globally stable, and, in addition, whether the resulting stationary distribution is nontrivial in the sense that it is
not concentrated on zero. The problem is not dissimilar to that for the Solow–Swan
model treated in §11.3.4. However, the fact that the savings rate is endogenous and
nonconstant means that we will have to work a little harder. In particular, we need to
extract any necessary information about savings from the Euler equation.
Throughout this section we will suppose that the conditions of assumption 12.1.16
all hold, and moreover that f (0, z) = 0 for all z ∈ Z. Together, these conditions,
proposition 12.1.17, and proposition 12.1.24 give us interiority of the optimal policy
and the Euler equality
U ◦ c(y) = ρ



U ◦ c[ f (σ (y), z)] f (σ (y), z)φ(dz)

( y > 0)

More Stochastic Dynamic Programming

303

We study the process (yt )t≥0 generated by the optimal law of motion
yt+1 = f (σ (yt ), Wt+1 )

IID

(Wt )t≥1 ∼ φ ∈ P ( Z )

(12.4)

For our state space S we will use (0, ∞) rather than + . The reason is that when
S = + the degenerate measure δ0 ∈ P (S) is stationary for (12.4). Hence any proof
of existence based on a result such as the Krylov–Bogolubov theorem is entirely redundant. Moreover global convergence to a nontrivial stationary distribution is impossible because δ0 will never converge to such a distribution. Hence global stability
never holds. Third, if we take S := (0, ∞), then any stationary distribution we can
obtain must automatically be nontrivial.
To permit (0, ∞) to be the state space, we require that f (k, z) > 0 whenever k > 0
and z ∈ Z. (For example, if f (k, z) = kα z and Z = (0, ∞) then this assumption holds.)
Observe that since σ (y) > 0 for all y ∈ S = (0, ∞), we then have f (σ (y), z) ∈ S for
all y ∈ S and z ∈ Z. Hence S is a valid state space for the model. Observe also that if
we permit f (k, z) = 0 independent of k for z in a subset of Z with φ-measure  > 0,
then {yt = 0} ≤ (1 − )t for all t, and yt → 0 in probability. (Why?) Under such
conditions a nontrivial steady state cannot be supported.
To keep our assumptions clear let’s now state them formally.
Assumption 12.1.25 All the conditions of assumption 12.1.16 hold. Moreover for any
z ∈ Z, f (k, z) = 0 if, and only if, k = 0.
Let us begin by considering existence of a (nontrivial) stationary distribution. We
will use the Krylov–Bogolubov theorem; in particular, corollary 11.2.10 on page 260.
The corollary requires that y → f (σ (y), z) is continuous on S for each z ∈ Z, and that
there exists a norm-like function w on S and nonnegative constants α and β with α < 1
and

Mw(y) = w[ f (σ (y), z)]φ(dz) ≤ αw(y) + β
∀y ∈ S
(12.5)
Since σ is continuous on S (see exercise 12.1.15 on page 301), y → f (σ (y), z) is continuous on S for each z ∈ Z. Thus it remains only to show that there exists a norm-like
function w on S and nonnegative constants α and β with α < 1 such that (12.5) holds.
In this connection recall from lemma 8.2.17 on page 209 that w : S → + is normlike on S if and only if limx→0 w( x ) = limx→∞ w( x ) = ∞. So suppose that we have two
nonnegative real-valued functions w1 and w2 on S with the properties limx→0 w1 ( x ) =
∞ and limx→∞ w2 ( x ) = ∞. Then the sum w := w1 + w2 is norm-like on S. (Why?) If,
in addition,
Mw1 ≤ α1 w1 + β 1 and Mw2 ≤ α2 w2 + β 2 pointwise on S

(12.6)

for some α1 , α2 , β 1 , β 2 with αi < 1 and β i < ∞, then w = w1 + w2 satisﬁes (12.5), as
the next exercise asks you to conﬁrm.

304

Chapter 12

Exercise 12.1.26 Using linearity of M and assuming (12.6), show that w satisﬁes (12.5)
for α := max{α1 , α2 } and β := β 1 + β 2 .
The advantage of decomposing w in this way stems from the fact that we must
confront two rather separate problems. We are trying to show that at least one trajectory of distributions is tight, keeping almost all of its mass on a compact K ⊂ S. A
typical compact subset of S is a closed interval [ a, b] with 0 < a < b < ∞. Thus we
require positive a such that almost all probability mass is above a, and ﬁnite b such
that almost all mass is less than b. In other words, income must neither collapse toward zero nor drift out to inﬁnity. The existence of a w1 with limx→0 w1 ( x ) = ∞ and
Mw1 ≤ α1 w1 + β 1 prevents drift to zero, and requires that the agent has sufﬁcient incentives to invest at low income levels. The existence of a w2 with limx→∞ w2 ( x ) = ∞
and Mw2 ≤ α2 w2 + β 2 prevents drift to inﬁnity, and requires sufﬁciently diminishing
returns.
Let’s start with the ﬁrst (and hardest) problem, that is, guaranteeing that the agent
has sufﬁcient incentives to invest at low income levels. We need some kind of Inada
condition on marginal returns to investment. In the deterministic case (i.e., f (k, z) =
f (k)) a well-known condition is that limk↓0 ρ f (k) > 1, or limk↓0 1/ρ f (k) < 1. This
motivates the following assumption:
Assumption 12.1.27 Together, φ, ρ and f jointly satisfy


lim
k ↓0

1
φ(dz) < 1
ρ f (k, z)

With this assumption we can obtain a function w1 with the desired properties, as
shown in the next lemma. (The proof is straightforward but technical, and can be
found in the appendix of this chapter.)
Lemma 12.1.28 For w1 := (U ◦ c)1/2 there exist positive constants α1 < 1 and β 1 < ∞
such that Mw1 ≤ α1 w1 + β 1 pointwise on S.
Now let’s turn to the second problem, which involves bounding probability mass
away from inﬁnity via diminishing returns. We assume that
Assumption 12.1.29 There exists constants a ∈ [0, 1) and b ∈


f (k, z)φ(dz) ≤ ak + b

∀k ∈ S

+

such that
(12.7)

This is a slightly nonstandard and relatively weak diminishing returns assumption. Standard assumptions forcing marginal returns to zero at inﬁnity can be shown
to imply assumption 12.1.29.
We now establish the complementary result for a suitable function w2 .

More Stochastic Dynamic Programming

305

Lemma 12.1.30 For w2 (y) := y there exist positive constants α2 < 1 and β 2 < ∞ such that
Mw2 ≤ α2 w2 + β 2 pointwise on S.
Exercise 12.1.31 Prove lemma 12.1.30 using assumption 12.1.29.
Since limx→0 w1 ( x ) = ∞ and limx→∞ w2 ( x ) = ∞, we have proved the following
result:
Proposition 12.1.32 Under assumptions 12.1.25–12.1.29 there exists a norm-like function
w on S and nonnegative constants α and β with α < 1 and β < ∞ such that Mw ≤ αw + β
pointwise on S. As a result the optimal income process (12.4) has at least one nontrivial
stationary distribution ψ∗ ∈ P (S).
Having established existence, let us now consider the issue of global stability. This
property can be obtained quite easily if the shocks are unbounded (for example, multiplicative, lognormal shocks). If the shocks are bounded the proofs are more ﬁddly,
and interested readers should consult the commentary at the end of this chapter.7
Assumption 12.1.33 For each k > 0 and each c ∈ S both
{ f (k, W ) ≤ c} are strictly positive.

{ f (k, W ) ≥ c} and

Proposition 12.1.34 If, in addition to the conditions of proposition 12.1.32, assumption
12.1.33 holds, then the optimal income process is globally stable.
Proof. We will show that the function w in proposition 12.1.32 is also order norm-like
(see deﬁnition 11.3.16 on page 282). Since y → f (σ (y), z) is monotone increasing this
is sufﬁcient for the proof (see theorems 11.3.14 and 11.3.19 in §11.3.3). As a ﬁrst step,
let’s establish that all intervals [ a, b] ⊂ S are order inducing for the growth model. To
do so, pick any a ≤ b. Fix any c ∈ S. In view of assumption 12.1.33,

∀ y ∈ [ a, b],

{ f (σ(y), Wt+1 ) ≥ c} ≥
∴

{ f (σ( a), Wt+1 ) ≥ c} > 0

inf P(y, [c, ∞)) > 0

a≤y≤b

A similar argument shows that infa≤y≤b P(y, (0, c]) > 0. Therefore [ a, b] is order inducing. Now since any sublevel set of w is contained in a closed interval [ a, b] ⊂ S
(why?), and since subsets of order inducing sets are order inducing, it follows that
every sublevel set of w is order inducing. Hence w is order norm-like, as was to be
shown.
7 Don’t be afraid of assuming unbounded shocks—this is just a modeling assumption that approximates
reality. For example, the normal distribution is often used to model human height, but no one is claiming
that a 20 meter giant is going to be born.

306

12.2

Chapter 12

Unbounded Rewards

One weakness of the dynamic programming theory provided in chapter 10 is that the
reward function is must be bounded. This constraint is violated in many applications.
The problem of a potentially unbounded reward function can sometimes be rectiﬁed
by compactifying the state space so that the (necessarily continuous) reward function
is automatically bounded on the state (despite perhaps being unbounded on a larger
domain). In other situations such tricks do not work, or are ultimately unsatisfying in
terms of the model they imply.
Unfortunately, there is no really general theory of dynamic programming with
unbounded rewards. Different models are tackled in different ways, which is timeconsuming and intellectually unrewarding. Below we treat perhaps the most general
method available, for programs with reward and value functions that are bounded
when “weighted” by some function κ. We travel to the land of weighted supremum
norms, ﬁnding an elegant technique and the ability to treat quite a large class of models.
Should you seek to use this theory for a given application, you will quickly discover that while the basic ideas are straightforward, the problem of choosing a suitable
weighting function can be quite tricky. We give some indication of how to go about
this using our benchmark example: the optimal growth model.

12.2.1

Weighted Supremum Norms

To begin, let κ be a function from S to
deﬁne the κ-weighted supremum norm

vκ := sup
x ∈S

such that κ ≥ 1. For any other v : S →

|v( x )| 
v
= 
κ (x)
κ ∞

,

(12.8)

Deﬁnition 12.2.1 Let bκ S be the set of all v : S → such that vκ < ∞. We refer to
these functions as the κ-bounded functions on S. On bκ S deﬁne the metric
dκ (v, w) := v − wκ = v/κ − w/κ ∞
Deﬁne also bκ B (S) := bκ S ∩ mB (S) and bκ cS := bκ S ∩ cS. In other words, bκ B (S)
is the κ-bounded functions on S that are also (Borel) measurable, and bκ cS is the κbounded functions on S that are also continuous.
Exercise 12.2.2 Show that v ∈ bκ S if and only if v/κ ∈ bS.
Exercise 12.2.3 Show that bB (S) ⊂ bκ B (S) and bcS ⊂ bκ cS.
Exercise 12.2.4 Conﬁrm that (bκ S, dκ ) is a metric space.

More Stochastic Dynamic Programming

307

It is a convenient fact that dκ -convergence implies pointwise convergence. Precisely, if (wn ) is a sequence in bκ S and dκ (wn , w) → 0 for some w ∈ bκ S, then wn ( x ) →
w( x ) for every x ∈ S. To see this, pick any x ∈ S. We have

|wn ( x )/κ ( x ) − w( x )/κ ( x )| ≤ wn − wκ → 0
∴

|wn ( x ) − w( x )| ≤ wn − wκ κ ( x ) → 0

The next lemma states that the usual pointwise ordering on bκ S is “closed” with
respect to the dκ metric. The proof is an exercise.
Lemma 12.2.5 If (wn ) is a dκ -convergent sequence in bκ S with wn ≤ w ∈ bκ S for all n ∈
then lim wn ≤ w.

,

The space (bκ S, dκ ) and its closed subspaces would not be of much use to us should
they fail to be complete. Fortunately all the spaces of κ-bounded functions are complete under reasonable assumptions.
Theorem 12.2.6 The space (bκ S, dκ ) is a complete metric space.
Proof. Let (vn ) be a Cauchy sequence in (bκ S, dκ ). It is left to the reader to show that
(vn /κ ) is then Cauchy in (bS, d∞ ). Since the latter space is complete, there exists some
function v̂ ∈ bS with vn /κ − v̂∞ → 0. We claim that v̂ · κ ∈ bκ S and vn − v̂ · κ κ →
0, in which case the completeness of (bκ S, dκ ) is established. That v̂κ ∈ bκ S follows
from boundedness of v̂. Moreover,

vn − v̂κ κ = vn /κ − (v̂κ )/κ ∞ = vn /κ − v̂∞ → 0

(n → ∞)

The completeness of (bκ S, dκ ) is now veriﬁed.
Exercise 12.2.7 Show that if κ is Borel measurable, then bκ B (S) is a closed subset of
(bκ S, dκ ), and that if κ is continuous, then bκ cS is a closed subset of (bκ S, dκ ).8 Now
prove the following theorem:
Theorem 12.2.8 If κ is measurable, then (bκ B (S), dκ ) is complete. If κ is continuous, then
(bκ cS, dκ ) is complete.
The following result is a useful extension of Blackwell’s sufﬁcient condition, which
can be used to establish that a given operator is a uniform contraction on bκ S.
Theorem 12.2.9 Let T : bκ S → bκ S be a monotone operator, in the sense that v ≤ v implies
Tv ≤ Tv . If, in addition, there is a λ ∈ [0, 1) such that
T (v + aκ ) ≤ Tv + λaκ

for all v ∈ bκ S and a ∈

+

(12.9)

then T is uniformly contracting on (bκ S, dκ ) with modulus λ.
8 Hint:

Pointwise limits of measurable functions are measurable. Uniform limits of continuous functions
are continuous.

308

Chapter 12

Exercise 12.2.10 Prove this result by modifying the proof of theorem 6.3.8 (page 344)
appropriately.

12.2.2

Results and Applications

Let S, A, r, F, Γ, and ρ again deﬁne an SDP, just as in §10.1. Let gr Γ have its previous
deﬁnition. However, instead of assumptions 10.1.3–10.1.5 on page 231, we assume the
following:
Assumption 12.2.11 The reward function r is continuous on gr Γ.
Assumption 12.2.12 Γ : S → B ( A) is continuous and compact valued.

( x, u) → F ( x, u, z) ∈ S is continuous for all z ∈ Z.

Assumption 12.2.13 gr Γ

The only difference so far is that r is not required to be bounded. For our last
assumption we replace boundedness of r by
Assumption 12.2.14 There exists a continuous function κ : S → [1, ∞) and constants
R ∈ + and β ∈ [1, 1/ρ) satisfying the conditions
sup |r ( x, u)| ≤ Rκ ( x )

u∈Γ( x )



sup
u∈Γ( x )

∀x ∈ S

κ [ F ( x, u, z)]φ(dz) ≤ βκ ( x )

In addition the map ( x, u) →



∀x ∈ S

(12.10)

(12.11)

κ [ F ( x, u, z)]φ(dz) is continuous on gr Γ.

Remark 12.2.15 Actually it is sufﬁcient to ﬁnd a continuous nonnegative function κ satisfying the conditions of assumption 12.2.14. The reason is that if κ is such a function,
then κ̂ := κ + 1 is a continuous function that is greater than 1 and satisﬁes the conditions of the assumption with the same constants R and β. You may want to check this
claim as an exercise.
In applications the difﬁculty is in constructing the required function κ. The following example illustrates how this might be done:
Example 12.2.16 Consider again the stochastic optimal growth model, this time satisfying all of the conditions in assumption 10.1.12 (page 235) apart from boundedness
of U. Instead U is required to be nonnegative. In addition we assume that
κ (y) :=

∞

∑ δt

t =0

U (ŷt ) < ∞

(y ∈ S)

(12.12)

More Stochastic Dynamic Programming

309

Here δ is a parameter satisfying ρ < δ < 1, and (ŷt )t≥0 is deﬁned by
ŷt+1 = f (ŷt , Wt+1 )

ŷ0 = y

and

(12.13)

The process (ŷt ) is an upper bound for income under the set of feasible policies. It is
the path for income when consumption is zero in each period.
We claim that the function κ in (12.12) satisﬁes all the conditions of assumption
12.2.14 for the optimal growth model.9 In making the argument, it is useful to deﬁne
N to be the Markov operator corresponding to (12.13). Hopefully it is clear to you that
for this operator we have U (ŷt ) = Nt U (y) for each t ≥ 0, so κ can be expressed as
∑t δt Nt U.
Lemma 12.2.17 The function κ in (12.12) is continuous and increasing on

+.

The proof can be found in the appendix to this chapter. Let us show instead that the
conditions of assumption 12.2.14 are satisﬁed, beginning with (12.10). In the optimal
growth model r ( x, u) = U (y − k) and Γ( x ) = Γ(y) = [0, y]. Since U is increasing and
nonnegative, we have
sup |r ( x, u)| = sup U (y − k ) ≤ U (y) ≤ κ (y)
0≤ k ≤ y

u∈Γ( x )

Thus (12.10) holds with R = 1. Now let’s check that (12.11) also holds. Observe that


sup

0≤ k ≤ y

κ ( f (k, z))φ(dz) ≤



κ ( f (y, z))φ(dz) = Nκ (y)

where we are using the fact that κ is increasing on S. Now
∞

Nκ = N ∑ δt Nt U =
t =0

∴

sup

0≤ k ≤ y



∞

∞

t =0

t =0

∑ δt Nt+1 U = (1/δ) ∑ δt+1 Nt+1 U ≤ (1/δ)κ

κ ( f (k, z))φ(dz) ≤ βκ (y),

where β := 1/δ

Since δ was chosen to satisfy ρ < δ < 1, we have 1 ≤ β < 1/ρ as desired.
Finally,to complete the veriﬁcation of assumption 12.2.14, we need to check that
( x, u) → κ [ F ( x, u, z)]φ(dz) is continuous, which in the present case amounts to
showing that if (yn , k n ) is a sequence with 0 ≤ k n ≤ yn and converging to (y, k ),
then


κ ( f (k n , z))φ(dz) → κ ( f (k, z))φ(dz)
Evidently z → κ ( f (k n , z)) is dominated by κ ( f (ȳ, z)), where ȳ := supn k n , and an
application of the dominated convergence theorem completes the proof.
9 In

view of remark 12.2.15 we need not verify that κ ≥ 1.

310

Chapter 12
Returning to the general case, as in chapter 10 we deﬁne the function vσ by

vσ ( x ) :=

∞

∑ ρt rσ (Xt ) for x ∈ S, where Xt+1 = F(Xt , σ(Xt ), Wt+1 ) with X0 = x

t =0

Unlike the situation where r is bounded, this expression is not obviously ﬁnite, or
t
even well deﬁned. Indeed it is not clear that ∑∞
t=0 ρ rσ ( Xt ( ω )) is convergent at each
ω ∈ Ω. And even if this random variable is well deﬁned and ﬁnite, the expectation
may not be.10
To start to get a handle on the problem, let’s prove that
Lemma 12.2.18 For all x ∈ S and all σ ∈ Σ we have

|rσ ( Xt )| ≤ Rβt κ ( x ).

Proof. That |rσ ( Xt )| = Mtσ |rσ | ≤ Rβt κ pointwise on S can be proved by induction.
For t = 0 we have |rσ ( x )| ≤ Rκ ( x ) by (12.10). Suppose in addition that Mtσ |rσ | ≤ Rβt κ
holds for some arbitrary t ≥ 0. Then
Mσt+1 |rσ | = Mσ Mtσ |rσ | ≤ Mσ Rβt κ = Rβt Mσ κ ≤ Rβt ( βκ ) = Rβt+1 κ
where the second inequality follows from (12.11).
Lemma 12.2.19 For each σ ∈ Σ and x ∈ S we have

t
∑∞
t=0 ρ |rσ ( Xt )| < ∞.

Proof. Pick any σ ∈ Σ and x ∈ S. Using the monotone convergence theorem followed
by lemma 12.2.18, we obtain
∞

∞

t =0

t =0

∑ ρt |rσ (Xt )| = ∑ ρt

|rσ ( Xt )| ≤

∞

∑ ρt Rβt κ (x)

t =0

Since ρ · β < 1 the right-hand side is ﬁnite, as was to be shown.
This lemma implies that limT →∞ ∑tT=0 ρt rσ ( Xt (ω )) exists (and hence the inﬁnite
sum is well deﬁned) for -almost every ω ∈ Ω. The reason is that a real-valued series
∑t at converges whenever it converges absolutely, that is, when ∑t | at | < ∞. This absolute convergence is true of ∑t ρt rσ ( Xt (ω )) for -almost every ω by lemma 12.2.19
and the fact that a random variable with ﬁnite expectation is ﬁnite almost everywhere.11 It also implies via the dominated convergence theorem (why?) that we can
pass expectation through inﬁnite sum to obtain our previous expression for vσ :


vσ ( x ) :=

∞

∑ ρ t r σ ( Xt )

t =0

=

∞

∑ ρt Mtσ rσ (x)

t =0

We will need the following lemma, which is proved in the appendix to this chapter:
10 It

may be inﬁnite or it may involve an expression of the form ∞ − ∞.
a proof of this last fact, see Schilling (2005, cor. 10.13).

11 For

More Stochastic Dynamic Programming

311

Lemma 12.2.20
Let assumptions 12.2.11–12.2.14 all hold. If w ∈ bκ cS, then the mapping

( x, u) → w[ F ( x, u, z)]φ(dz) is continuous on gr Γ.
Parallel to deﬁnition 10.1.9 on page 233, for w ∈ bκ B (S) we say that σ ∈ Σ is
w-greedy if



( x ∈ S)
(12.14)
σ ( x ) ∈ argmax r ( x, u) + ρ w[ F ( x, u, z)]φ(dz)
u∈Γ( x )

Lemma 12.2.21 Let assumptions 12.2.11–12.2.14 hold. If w ∈ bκ cS, then Σ contains at least
one w-greedy policy.
The proof follows from lemma 12.2.20, and is essentially the same as that of lemma
10.1.10 on page 233. We can now state the main result of this section.
Theorem 12.2.22 Under assumptions 12.2.11–12.2.14, the value function v∗ is the unique
function in bκ B (S) satisfying



( x ∈ S)
(12.15)
v∗ ( x ) = sup r ( x, u) + ρ v∗ [ F ( x, u, z)]φ(dz)
u∈Γ( x )

v∗

∈ bκ cS. A feasible policy is optimal if and only if it is v∗ -greedy. At least one
Moreover
such policy exists.

12.2.3

Proofs

Let’s turn to the proof of theorem 12.2.22. In parallel to §10.1, let Tσ : bκ B (S) →
bκ B (S) be deﬁned for all σ ∈ Σ by
Tσ w( x ) = r ( x, σ ( x )) + ρ



w[ F ( x, σ ( x ), z)]φ(dz) = rσ ( x ) + ρMσ w( x )

and let the Bellman operator T : bκ B (S) → bκ B (S) be deﬁned by



( x ∈ S)
Tw( x ) = sup r ( x, u) + ρ w[ F ( x, u, z)]φ(dz)
u∈Γ( x )

Exercise 12.2.23 Show that both Tσ and T do in fact send bκ B (S) into itself. (In particular, show that the images of κ-bounded functions are κ-bounded.) Lemma A.2.31
on page 334 might be useful in the case of T.
Lemma 12.2.24 Let γ := ρβ. For every σ ∈ Σ, the operator Tσ is uniformly contracting on
the metric space (bκ B (S), dκ ), with

 Tσ w − Tσ w κ ≤ γw − w κ

∀ w, w ∈ bκ B (S)

(12.16)

and the unique ﬁxed point of Tσ in bκ B (S) is vσ . In addition Tσ is monotone on bκ B (S), in
the sense that if w, w ∈ bB (S) and w ≤ w , then Tσ w ≤ Tσ w .

312

Chapter 12

Proof. The proof that Tσ is monotone is left to the reader. The proof that Tσ vσ =
vσ is identical to the proof in §10.1 for bounded r. The proof that Tσ is a uniform
contraction goes as follows: Pick any w, w ∈ bκ B (S). Making use of the linearity and
monotonicity of Mσ , we have

| Tσ w − Tσ w | = |ρMσ w − ρMσ w | = ρ|Mσ (w − w )|
≤ ρMσ |w − w | ≤ ρw − w κ Mσ κ ≤ ρβw − w κ κ
The rest of the argument is an exercise.
Next we turn to the Bellman operator.
Lemma 12.2.25 The operator T is uniformly contracting on (bκ B (S), dκ ), with

 Tw − Tw κ ≤ γw − w κ

∀ w, w ∈ bκ B (S)

(12.17)

where γ := ρβ. In addition T is monotone on bκ B (S), in the sense that if w, w ∈ bκ B (S)
and w ≤ w , then Tw ≤ Tw .
Exercise 12.2.26 Prove lemma 12.2.25. In particular, prove that T is uniformly contracting with modulus γ by applying theorem 12.2.9 (page 307).
The proof of theorem 12.2.22 now follows from lemmas 12.2.24 and 12.2.25 in an
almost identical fashion to the bounded case (see §10.1.3). The details are left to the
reader.

12.3

Commentary

Monotonicity in parameters is a major topic in mathematical economics and dynamic
programming. Useful references include Lovejoy (1987), Puterman (1994), Topkis
(1998), Hopenhayn and Prescott (1992), Huggett (2003), Amir (2005), and Mirman et
al. (2008). See Amir et al. (1991) for an early analysis of monotone dynamic programming and optimal growth.
Our treatment of concavity and differentiability is standard. The classic reference
is Stokey and Lucas (1989). Corollary 12.1.19 is due to Mirman and Zilcha (1975). The
connection between lemma 12.1.21 and differentiability of the value function is due
to Benveniste and Scheinkman (1979), and is based on earlier results in Rockafellar
(1970).
Global stability of the stochastic optimal growth model under certain Inada-type
conditions was proved by Brock and Mirman (1972). See also Mirman (1970, 1972,
1973), Mirman and Zilcha (1975), Hopenhayn and Prescott (1992), Stachurski (2002),
Nishimura and Stachurski (2004), Zhang (2007), Kamihigashi (2007), or Chatterjee and

More Stochastic Dynamic Programming

313

Shukayev (2008). The techniques used here closely follow Nishimura and Stachurski
(2004).
Our discussion of unbounded dynamic programming in §12.2 closely follows the
theory developed in Hernández-Lerma and Lasserre (1999, ch. 8). Boyd (1990) is an
early example of the weighted norm approach in economics, with an application to
recursive utility. See also Becker and Boyd (1997). Le Van and Vailakis (2005) is a
more recent treatment of the same topic. Stokey and Alvarez (1998) use weighted
norm techniques for dynamic programs with certain homogeneity properties. See
also Rincon-Zapatero and Rodriguez-Palmero (2003).

Part III

Appendixes

315

Appendix A

Real Analysis
This appendix reviews some bits and pieces from basic real analysis that are used
in the book. If you lack background in analysis, then it’s probably best to parse the
chapter brieﬂy and try some exercises before starting the main body of the text.

A.1

The Nuts and Bolts

We start off our review with fundamental concepts such as sets and functions, and
then move on to a short discussion of probability on ﬁnite sample spaces.

A.1.1

Sets and Logic

Pure mathematicians might tell you that everything is a set, or that sets are the only
primitive (i.e., the only mathematical objects not deﬁned in terms of something else).
We won’t take such a purist view. For us a set is just a collection of objects viewed as a
whole. Functions are rules that associate elements of one set with elements of another.
Examples of sets include , , and , which denote the natural numbers (i.e.,
positive integers), the integers, and the rational numbers respectively. The objects
that make up a set are referred to as its elements. If a is an element of A we write
a ∈ A. The set that contains no elements is called the empty set and denoted by ∅. Sets
A and B are said to be equal if they contain the same elements. Set A is called a subset
of B (written A ⊂ B) if every element of A is also an element of B.1 Clearly, A = B if
and only if A ⊂ B and B ⊂ A.
1 Something

to ponder: In mathematics any logical statement that cannot be tested is regarded as (vacuously) true. It follows that ∅ is a subset of every set.

317

318

Appendix A

If S is a given set, then the collection of all subsets of S is itself a set. We denote it
by P(S).
The intersection A ∩ B of two sets A and B consists of all elements found in both A
and B; A and B are called disjoint if A ∩ B = ∅. The union of A and B is the set A ∪ B
consisting of all elements in at least one of the two. The set-theoretic difference A \ B is
deﬁned as
A \ B := { x : x ∈ A and x ∈
/ B}
In the case where the discussion is one of subsets of some ﬁxed set A, the difference
A \ B is called the complement of B and written Bc .
If A is an arbitrary “index” set so that {Kα }α∈ A is a collection of sets, then we
deﬁne
∩α∈ A Kα := { x : x ∈ Kα for all α ∈ A}
and

∪α∈ A Kα := { x : there exists an α ∈ A such that x ∈ Kα }
The same collection {Kα }α∈ A is called pairwise disjoint if any pair Kα , K β with α = β is
disjoint.
The following two equalities are known as de Morgan’s laws:
1. (∪α∈ A Kα )c = ∩α∈ A Kαc
2. (∩α∈ A Kα )c = ∪α∈ A Kαc
Let’s see how we prove these kinds of set equalities by going through the proof of the
ﬁrst one slowly. Let A := (∪α∈ A Kα )c and B := ∩α∈ A Kαc . Take some arbitrary element
x ∈ A. Since x ∈ A, it must be that x is not in Kα for any α. In other words, x ∈ Kαc for
every α. But if this is true, then, by the deﬁnition of B, we see that x ∈ B. Since x was
arbitrary, we have A ⊂ B. Similar reasoning shows that B ⊂ A, and hence A = B.
As we go along, fewer of these steps will be spelled out, so read the proof through
a few times if the method does not seem obvious. You will get used to this way of
thinking.
The Cartesian product of sets A and B is the set of ordered pairs
A × B := {( a, b) : a ∈ A, b ∈ B}
For example, if A is the set of outcomes for a random experiment (experiment A), and
B is the set of outcomes for a second experiment (experiment B), then A × B is the
set of all outcomes for the experiment C, which consists of ﬁrst running A and then
running B. The pairs ( a, b) are ordered, so ( a, b) and (b, a) are not in general the same
point. In the preceding example this is necessary so that we can distinguish between
the outcomes for the ﬁrst and second experiment.

Real Analysis

319

Inﬁnite Cartesian products are also useful. If ( An ) is a collection of sets, one for
each n ∈ , then
×n≥1 An := {( a1 , a2 , . . .) : an ∈ An }
If An = A for all n, then ×n≥1 A is often written as A .
So much for sets. Now let’s very brieﬂy discuss logic and the language of mathematics. We proceed in a “naive” way (rather than axiomatic), with the idea of quickly
introducing the notation and its meaning. If you are not very familiar with formal
mathematics, I suggest that you skim through, picking up the basic points and coming back if required.
Logic starts with the notion of mathematical statements, which we denote with
capital letters such as P or Q. Typical examples are
P = the area of a rectangle is the product of its two sides
Q = x is strictly positive
Next we assign truth values to these statements, where each statement is labeled either
“true” or “false.” A starting point for logic is the idea that every sensible mathematical
statement is either true or false. The truth value of “maybe” is not permitted.
In general, mathematical statements should not really be thought of as inherently
true or false. For example, you might think that P above is always a true statement.
However, it is better to regard P as consistent with the natural world in certain ways,
and therefore a useful assumption to make when performing geometric calculations.
At the same time, let’s not rule out the possibility of assuming that P is false in order
to discover the resulting implications.
Simply put, a lot of mathematics is about working out the consistency of given
truth values assigned to collections of mathematical statements. This is done according to the rules of logic, which are indeed quite logical. For example, if a statement P
is labeled as true, then its negation ∼ P is false, and ∼ (∼ P) must have the same truth
value as P.
Statements can be combined using the elementary connectives “and” and “or.” Statement “P and Q” is true if both P and Q are true, and false otherwise. Statement
“P or Q” is false if both P and Q are false, and true otherwise. You might try to convince yourself that

∼ ( A or B) ≡ (∼ A) and (∼ B) &

∼ ( A and B) ≡ (∼ A) or (∼ B)

where the notation P ≡ Q means that P and Q are logically equivalent (i.e., always have
the same truth value).
Another form of relationship between statements is implication. For example, suppose that we have sets A and B with A ⊂ B. Let P be the statement x ∈ A and Q be

320

Appendix A

the statement x ∈ B. If P is labeled as true, then Q must also be true, since elements
of A are also elements of B. We say that P implies Q (alternatively: if P, then Q), and
write P =⇒ Q.
Sometimes it is not so easy to see that P =⇒ Q. Mathematical proofs typically
involve creating a chain of statements R1 , . . . , Rn with
P =⇒ R1 =⇒ · · · =⇒ Rn =⇒ Q
Often this is done by working forward from P and backward from Q, and hoping that
you meet somewhere in the middle. Another strategy for proving that P =⇒ Q is to
show that ∼ Q =⇒ ∼ P.2 For if the latter holds then so must P =⇒ Q be valid, for
when P is true Q cannot be false (if it were, then P could not be true).
The universal quantiﬁer ∀ (for all) and the existential quantiﬁer ∃ (there exists) are
used as follows. If P(α) is statement about an object α, then

∀ α ∈ Λ, P(α)
means that for all elements α of the set Λ, the statement P(α) holds.

∃ α ∈ Λ such that P(α)
means that P(α) is true for at least one α ∈ Λ. The following equivalences hold:

∼ [∀ α ∈ Λ, P(α)] ≡ ∃ α ∈ Λ such that ∼ P(α),

and

∼ [∃ α ∈ Λ such that P(α)] ≡ ∀ α ∈ Λ, ∼ P(α)

A.1.2

Functions

A function f from set A to set B, written A
x → f ( x ) ∈ B or f : A → B, is a rule
associating to each and every one of the elements a in A one and only one element
b ∈ B.3 The point b is also written as f ( a), and called the image of a under f . For
C ⊂ A, the set f (C ) is the set of all images of points in C, and is called the image of C
under f . Formally,
f (C ) := {b ∈ B : f ( a) = b for some a ∈ C }
Also, for D ⊂ B, the set f −1 ( D ) is all points in A that map into D under f , and is
called the preimage of D under f . That is,
f −1 ( D ) : = { a ∈ A : f ( a ) ∈ D }
2 The

latter implication is known as the contrapositive of the former.
writers refer to a function f by the symbol f ( x ), as in “the production function f ( x ) is increasing. . . ,” or similar. Try not to follow this notation. The symbol f ( x ) represents a value, not a function.
3 Some

Real Analysis

321

When D consists of a single point b ∈ B we write f −1 (b) rather than f −1 ({b}). In
general, f −1 (b) may contain many elements of A or none.
Let S be any set. For every A ⊂ S, let S x → A ( x ) ∈ {0, 1} be the function that
takes the value 1 when x ∈ A and zero otherwise. This function is called the indicator
function of A.
Exercise A.1.1 Argue that Ac = S − A holds pointwise on S (i.e., Ac ( x ) = S ( x ) −
A ( x ) at each x ∈ S). In what follows we usually write S simply as 1. Argue further
that if A1 , . . . , An is a collection of subsets, then maxi Ai = 1 − ∏i Ac .
i

A function f : A → B is called one-to-one if distinct elements of A are always
mapped into distinct elements of B, and onto if every element of B is the image under
f of at least one point in A. A function that is both one-to-one and onto is called a
bijection.
You will be able to verify that f : A → B is a bijection if and only if f −1 (b) consists
of precisely one point in A for each b ∈ B. In this case f −1 deﬁnes a function from B
to A by setting f −1 (b) equal to the unique point in A that f maps into b. This function
is called the inverse of f . Note that f ( f −1 (b)) = b for all b ∈ B, and that f −1 ( f ( a)) = a
for all a ∈ A.
New functions are often deﬁned from old functions by composition: If f : A → B
and g : B → C, then g ◦ f : A → C is deﬁned at x ∈ A by ( g ◦ f )( x ) := g( f ( x )). It is
easy to check that if f and g are both one-to-one and onto, then so is g ◦ f .
Preimages and set operations interact nicely. For example, if f : A → B, and E and
F are subsets of B, then
f −1 ( E ∪ F ) = f −1 ( E ) ∪ f −1 ( F )
To see this, suppose that x ∈ f −1 ( E ∪ F ). Then f ( x ) ∈ E ∪ F, so f ( x ) ∈ E or f ( x ) ∈ F
(or both). Therefore x ∈ f −1 ( E) or x ∈ f −1 ( F ), whence x ∈ f −1 ( E) ∪ f −1 ( F ). This
proves that f −1 ( E ∪ F ) ⊂ f −1 ( E) ∪ f −1 ( F ). A similar argument shows that f −1 ( E ∪
F ) ⊃ f −1 ( E) ∪ f −1 ( F ), from which equality now follows.
More generally, we have the following results. (Check them.)
Lemma A.1.2 Let f : A → B, and let E and { Eγ }γ∈C all be arbitrary subsets of B.4 We have
1. f −1 ( Ec ) = [ f −1 ( E)]c ,
2. f −1 (∪γ Eγ ) = ∪γ f −1 ( Eγ ), and
3. f −1 (∩γ Eγ ) = ∩γ f −1 ( Eγ ).
The forward image is not as well behaved as the preimage.
4 Here

C is any “index” set.

322

Appendix A

Exercise A.1.3 Construct an example of sets A, B, C, D, with C, D ⊂ A, and function
f : A → B, where f (C ∩ D ) = f (C ) ∩ f ( D ).
Using the concept of bijections, let us now discuss some different notions of inﬁnity. To start, notice that it is not always possible to set up a bijection between two
sets. (Consider the case where one set has two elements and the other has one—try to
ﬁnd a bijection.) When a bijection does exist, the two sets are said to be in one-to-one
correspondence, or have the same cardinality. This notion captures the idea that the two
sets “have the same number of elements,” but in a way that can be applied to inﬁnite
sets.
Deﬁnition A.1.4 A nonempty set A is called ﬁnite if it has the same cardinality as the
set {1, 2, . . . , n} for some n ∈ . Otherwise, A is called inﬁnite. If A is either ﬁnite
or in one-to-one correspondence with , then A is called countable. Otherwise, A is
called uncountable.5
The distinction between countable and uncountable sets is important, particularly
for measure theory. In the rest of this section we discuss examples and results for these
kinds of properties. The proofs are a little less than completely rigorous—sometimes
all the cases are not covered in full generality—but you can ﬁnd formal treatments in
almost all textbooks on real analysis.
An example of a countable set is E := {2, 4, . . .}, the even elements of . We can
set up a bijection f :
→ E by letting f (n) = 2n. The set O := {1, 3, . . .} of odd
elements of is also countable, under f (n) = 2n − 1. These examples illustrate that
for inﬁnite sets, a proper subset can have the same cardinality as the original set.
Theorem A.1.5 Countable unions of countable sets are countable.
Proof. Let An := ( a1n , a2n , . . .) be a countable set, and let A := ∪n≥1 An . For simplicity
we assume that the sets ( An ) are all inﬁnite and pairwise disjoint. Arranging the
elements of A into an inﬁnite matrix, we can count them in the following way:
a11
a12
↓
a13
..
.

→ a21

a22

a31

···

..
.

This system of counting provides a bijection with
5 Sets

→ ···

.

we are calling countable some authors refer to as at most countable.

Real Analysis

323

Exercise A.1.6 Show that

:= {. . . , −1, 0, 1, . . .} is countable.

Theorem A.1.7 Finite Cartesian products of countable sets are countable.
Proof. Let’s just prove this for a pair A and B, where both A and B are inﬁnite. In this
case, the Cartesian product can be written as

( a1 , b1 ) → ( a1 , b2 )

( a2 , b1 )
( a2 , b2 )
↓
..
( a3 , b1 )
.
..
.

( a1 , b3 ) → · · ·
···

Now count as indicated.
Theorem A.1.8 The set of all rational numbers

is countable.

Proof. The set
= { p/q : p ∈ , q ∈ , q = 0} can be put in one-to-one correspondence with a subset of × = {( p, q) : p ∈ , q ∈ }, which is countable by
theorem A.1.7. Subsets of countable sets are countable.
Not all sets are countable. In fact countable Cartesian products of countable sets
may be uncountable. For example, consider {0, 1} , the set of all binary sequences
( a1 , a2 , . . .), where ai ∈ {0, 1}. If this set were countable, then it could be listed as
follows:
1 ↔ a1 , a2 , a3 , . . .
2 ↔ b1 , b2 , b3 , . . .
..
..
.
.
where the sequences on the right-hand side are binary sequences. Actually such a
list is never complete: We can always construct a new binary sequence c1 , c2 , . . . by
setting c1 to be different from a1 (zero if a1 is one, and one otherwise), c2 to be different
from b2 , and so on. This differs from every element in our supposedly complete list
(in particular, it differs from the n-th sequence in that their n-th elements differ); a
contradiction indicating that {0, 1} is uncountable.6
The cardinality of the set of binary sequences is called the power of the continuum.
The assertion that there are no sets with cardinality greater than countable and less
than the continuum is called the Continuum Hypothesis, and is a rather tricky problem to say the least.
6 This

is Cantor’s famous diagonal argument.

324

Appendix A

A.1.3

Basic Probability

In this section we brieﬂy recall some elements of probability on ﬁnite sets. Consider
a ﬁnite set Ω, a typical element of which is ω. A probability on Ω is a function from
P(Ω), the set of all subsets of Ω, into [0, 1] with properties
1.

(Ω) = 1, and

2. if A, B ⊂ Ω and A ∩ B = ∅, then

( A ∪ B) =

( A ) + ( B ).

The pair (Ω, ) is sometimes called a ﬁnite probability space. Subsets of Ω are also called
events. The elements ω that make up Ω are the primitive events, while general B ⊂ Ω
is a composite event, consisting of M ≤ #Ω primitive events.7 The number ( B) is the
“probability that event B occurs.” In other words, ( B) represents the probability that
when uncertainty is resolved and some ω ∈ Ω is selected by “nature,” the statement
ω ∈ B is true.
Exercise A.1.9 Let p : Ω → [0, 1], where ∑ω ∈Ω p(ω ) = 1, and let

( B) :=

∑

ω∈B

p(ω )

Show that properties (1) and (2) both hold for

( B ⊂ Ω)

(A.1)

deﬁned in (A.1).

The next few results follow easily from the deﬁnition of a probability.
Lemma A.1.10 If A ⊂ Ω, then

( A c ) = 1 − ( A ).

Proof. Here of course Ac := Ω \ A. The proof is immediate from (1) and (2) above
because 1 = (Ω) = ( A ∪ Ac ) = ( A) + ( Ac ).
Exercise A.1.11 Prove that (∅) = 0. Prove that if A ⊂ B, then
( A). Prove further that if A ⊂ B, then ( A) ≤ ( B).

( B \ A) =

( B) −

The idea that if A ⊂ B, then ( A) ≤ ( B) is fundamental. Event B occurs whenever A occurs, so the probability of B is larger. Many crucial ideas in probability boil
down to this one point.
Exercise A.1.12 Prove that if A and B are (not necessarily disjoint) subsets of Ω, then
( A ∪ B) ≤ ( A) + ( B). Construct an example of a probability and subsets A, B
such that this inequality is strict. Show that in general, ( A ∪ B) = ( A) + ( B) −
( A ∩ B ).
7 If

A is a set, then #A denotes the number of elements in A.

Real Analysis

325

( B) > 0, then the conditional probability of A given

If A and B are two events and
B is

( A | B) :=

( A ∩ B)
( B)

(A.2)

It represents the probability that A will occur, given the information that B has occurred.
What is the justiﬁcation for the expression (A.2)? Informally, the probability (C )
of an event C can be thought of as the fraction of times that C occurs in n independent
and identical experiments, as n → ∞. Letting ωn be the outcome of the n-th trial and
#A be the number of elements in set A, we can write this as

(C ) = lim

n→∞

#{ n : ω n ∈ C }
n

The conditional ( A | B) is (approximately) the number of times both A and B occur
over a large number of observations, expressed as a fraction of the number of occurrences of B:
#{n : ωn ∈ A and B}
( A | B) 
#{ n : ω n ∈ B }
Dividing through by n gives and taking the limit gives

( A | B) 

#{n : ωn ∈ A and B}/n
→
#{n : ωn ∈ B}/n

( A ∩ B)
( B)

Events A and B are called independent if ( A ∩ B) = ( A) ( B). You will ﬁnd it easy
to conﬁrm that if A and B are independent, then the conditional probability of A given
B is just the probability of A.
We will make extensive use of the law of total probability, which says that if A ⊂ Ω
and B1 , . . . , B M is a partition of Ω (i.e., Bm ⊂ Ω for each m, the Bm ’s are mutually disM B = Ω) with ( B ) > 0
joint in the sense that Bj ∩ Bk is empty when j = k, and ∪m
m
=1 m
for all j, then

( A) =

M

∑

m =1

( A | Bm ) · ( Bm )

The proof is quite straightforward, although you should check that the manipulations
of intersections and unions work if you have not seen them before:

( A) =

M
( A ∩ ∪m
=1 Bm ) =

=

M
(∪m
=1 ( A ∩ Bm ))
M

∑

m =1

( A ∩ Bm ) =

M

∑

m =1

( A | Bm ) · ( Bm )

326

Appendix A

Now consider a random variable taking values in some collection of numbers S. Formally, a random variable X is a function from the sample space Ω into S. The idea is
that “nature” picks out an element ω in Ω according to some probability. The random
variable now sends this ω into X (ω ) ∈ S. We can think of X as “reporting” the outcome of the draw to us in a format that is more amenable to analysis. For example,
Ω might be a collection of binary sequences, and X translates these sequences into
(decimal) numbers.
Each probability on Ω and X : Ω → S induces a distribution8 φ on S via
φ( x ) =

{ω ∈ Ω : X (ω ) = x }

( x ∈ S)

(A.3)

Exercise A.1.13 Show that φ( x ) ≥ 0 and ∑ x∈S φ( x ) = 1.
In what follows we will often write the right-hand side of (A.3) simply as { X =
x }. Please be aware of this convention. We say that X is distributed according to φ,
and write X ∼ φ.
An aside: If you stick to elementary probability, then you may begin to feel that the
distinction between the underlying probability and the distribution φ of X is largely
irrelevant. Why can’t we just say that X is a random variable with distribution φ, and
Y is another random variable with distribution ψ? The meaning of these statements
seems clear, and there is no need to introduce and Ω, or to think about X and Y as
functions.
The short answer to this question is that it is often useful to collect different random variables on the one probability space deﬁned by Ω and . With this construct
one can then discuss more complex events, such as convergence of a sequence of random variables on (Ω, ) to yet another random variable on (Ω, ).
Next we deﬁne expectation. Let X : Ω → S and let be a probability on Ω. The
expectation X of X is given by
X :=

∑

ω ∈Ω

X (ω ) {ω }

(A.4)

Exercise A.1.14 Prove that if X ∼ φ, then X = ∑ x∈S xφ( x ).9 Prove the more general
result that if Y = h( X ) for some real-valued function h of X (i.e., h : S → ), then
Y :=
8 What
9 Hint:

∑

ω ∈Ω

h( X (ω )) {ω } =

∑ h( x )φ( x )

x ∈S

we call a distribution here is often referred to as a probability mass function.
Divide Ω into sets Bx for x ∈ S, where Bx := {ω ∈ Ω : X (ω ) = x }.

(A.5)

Real Analysis

A.2

327

The Real Numbers

As usual, denotes the so-called real numbers, which you can visualize as the “continuous” real line. We will make use of several of its properties. One property worth
mentioning before we start is that the set is uncountable. This can be proved by showing that is in one to one correspondence with the set of all binary sequences—which
were shown to be uncountable in §A.1.2. (For the correspondence, think of the way
that computers represent numbers in binary form.) also has certain algebraic, order,
and completeness properties, which we now detail.

A.2.1

Real Sequences

In what follows, if x ∈
then | x | denotes its absolute value. For any x, y ∈
triangle inequality | x + y| ≤ | x | + |y| holds.

the

Exercise A.2.1 Show that if a, b and x are any real numbers, then

| a − b| ≤ | a − x | + | x − b| and | | x | − | a| | ≤ | x − a|

(A.6)

A subset A of
is called bounded if there is an M ∈
such that | x | ≤ M, all
x ∈ A. The -ball or -neighborhood around a ∈
is the set of points x ∈
such
10
that | a − x | < .
An X-valued sequence is a function from the natural numbers
:= {1, 2, . . .} to nonempty set X, traditionally denoted by notation such as ( xn ). It
is called a real sequence when X ⊂ . A real sequence ( xn ) called bounded if its range
is a bounded set (i.e., ∃ M ∈ such that | xn | ≤ M for all n ∈ ).
A real sequence ( xn ) is said to be convergent if there is an x ∈
such that, given
any  > 0, we can ﬁnd an N ∈
with the property | xn − x | <  whenever n ≥
N. This property will often be expressed by saying that ( xn ) is eventually in any
-neighborhood of x. The point x is called the limit of the sequence, and we write
limn→∞ xn = x or xn → x as n → ∞.
This deﬁnition of convergence can be a little hard to grasp at ﬁrst. One way is
to play the “, N game.” If I claim that a sequence is convergent, then, for every neighborhood you give me, I commit to providing you with an index N such that
all points further along the sequence than the N-th one (i.e., points x N , x N +1 , . . .) are
in that -neighborhood. For example, consider xn = 1/n2 . I claim xn converges to
zero. When you give me  = 1/3, I can give you N = 2, because n ≥ 2 implies
xn = 1/n2 ≤ 1/4 < . In fact I can give you an
√ “algorithm” for generating such an N:
Given  > 0, take any N ∈ greater than 1/ .
Sometimes the “points” ∞ and −∞ can are regarded as limits of sequences. In
what follows, we will say that xn → ∞, or limn xn = ∞, if for each M ∈ there is an
10 This

“ball” will look more ball-like once we move to higher dimensional spaces.

328

Appendix A

such that xn ≥ M whenever n ≥ N. Similarly we say that xn → −∞ if, for
N ∈
each M ∈ there is an N ∈ such that xn ≤ − M whenever n ≥ N. Also, sequence
( xn ) is called monotone increasing (resp., decreasing) if xn ≤ xn+1 (resp, xn+1 ≤ xn ) for
all n ∈ . If ( xn ) is monotone increasing (resp., decreasing) and converges to some
x ∈ , then we write xn ↑ x (resp., xn ↓ x).
Lemma A.2.2 Let ( xn ) be a sequence in
| xn − x | → 0.

, and let x ∈

. Then xn → x if and only if

Proof. The ﬁrst statement says that we can make | xn − x | less than any given  > 0 by
choosing n sufﬁciently large. The second statement says that we can make || xn − x | −
0| less than any given  > 0 by choosing n sufﬁciently large. Clearly, these statements
are equivalent.
Lemma A.2.3 Each real sequence has at most one limit.
Proof. Let xn → a and xn → b. Suppose that a = b. By choosing  small enough,
we can take -balls Ba and Bb around a and b that are disjoint.11 By the deﬁnition of
convergence, ( xn ) is eventually in Ba and eventually in Bb . In which case there must
be an N such that x N ∈ Ba and x N ∈ Bb . But this is impossible. Hence a = b.
While most of the numbers that we deal with in every day life can be expressed in
terms of integers or rational numbers, for more sophisticated mathematics does not
sufﬁce. Simple equations using rational numbers may not have rational solutions, and
sequences of rational numbers that seem to converge to something may not converge
to any rational number. The real numbers “complete” the rational numbers, in the
sense that sequences of rationals—or reals—that “appear to converge” will have a
limit within the set .
To make this precise, recall that a sequence ( xn ) in is called Cauchy if, for any
 > 0, there exists an N ∈ such that for any n and m greater than N, | xn − xm | < .
Now Cauchy sequences seem to be converging to something, so we can express the
idea of completeness of —as opposed to —by saying that every Cauchy sequence
in does converge to an element of .
Axiom A.2.4 (Completeness of

) Every Cauchy sequence on the real line is convergent.

There are formal constructions of the real numbers from the rationals by which this
statement can be proved, but we will take it as axiomatic. This completeness property
of is one of the most important and fundamental ideas of real analysis. For example, it allows us to deﬁne a solution to a particular problem as the limit of a Cauchy
sequence of numbers generated by some approximation process, without fearing the
11 Using

(A.6), show that if  < | a − b|/2, then x ∈ Ba and x ∈ Bb is impossible.

Real Analysis

329

embarrassment that would result should such a limit point fail to exist. This is important because many types of sequences are Cauchy. The following example is extremely
useful in both theory and applications:
Theorem A.2.5 Every bounded monotone sequence in

is convergent.

Proof. We prove the case where ( xn ) is increasing (xn+1 ≥ xn for all n). By axiom A.2.4,
it sufﬁces to show that ( xn ) is Cauchy. Suppose that it is not. Then we can ﬁnd an
0 > 0 such that, given any N ∈ , there is a pair n, m ∈
with N ≤ n < m and
xm − xn ≥ 0 . But then ( xn ) cannot be bounded above. (Why?) Contradiction.12
Exercise A.2.6 Prove that if ( xn ) ⊂

is convergent, then ( xn ) is bounded.13

Now we introduce the notion of subsequences. Formally, a sequence (yn ) is called
a subsequence of another sequence ( xn ) if there is a strictly increasing function f : →
such that yn = x f (n) for all n ∈ . To put it more simply, (yn ) is the original sequence ( xn ) but with some points omitted. The function f picks out a strictly increasing sequence of positive integers n1 < n2 < · · · that are to make up the subsequence,
in the sense that y1 = xn1 , y2 = xn2 , and so forth. Often one writes this new sequence
as ( xnk ).
Exercise A.2.7 Show that if ( xn ) ⊂
quence.

converges to x ∈

Exercise A.2.8 Show that ( xn ) converges to x ∈
( xn ) has a subsubsequence that converges to x.

, then so does every subse-

if and only if every subsequence of

Theorem A.2.9 Every real sequence has a monotone subsequence.
Proof. Call an element xk in ( xn ) dominant if all the following elements are less than
or equal to it. If there are inﬁnitely many such dominant elements, then we can select
these to be our monotone subsequence (which is decreasing). If not, let xm be the last
dominant term. Since xm+1 is not dominant, there is a j > m + 1 such that x j > xm+1 .
Since x j is not dominant there is an i > j such that xi > x j . Continuing in this way, we
can pick out a monotone subsequence (which is increasing).
Now we have the following crucial property of . Usually called the Bolzano–
Weierstrass theorem, it also extends to higher dimensional space (see theorem 3.2.18
on page 47) and forms the foundations of countless results in analysis.
12 How are you going with proof by contradiction? After a while you will become familiar with the style
of argument. The assertion that ( xn ) is not Cauchy led to a contradiction—in this case of the hypothesis
that ( xn ) is bounded. We are forced to conclude that this assertion (i.e., that ( xn ) is not Cauchy) is false. In
other words, ( xn ) is Cauchy.
13 Hint: How many points are there outside a given -ball around the limit?

330

Appendix A

Theorem A.2.10 Every bounded sequence in

has a convergent subsequence.

Proof. Take a given sequence in . By theorem A.2.9, the sequence has a monotone
subsequence, which is a sequence in its own right. Evidently this sequence is also
bounded. By theorem A.2.5, every bounded monotone sequence converges.
The next result is important in practice, and the proof is an exercise.
Theorem A.2.11 Let ( xn ) and (yn ) be two sequences in
If xn ≤ yn for all n ∈ , then x ≤ y.14

, with lim xn = x and lim yn = y.

Often when we use this result one sequence will be a constant. For example, if
xn ≤ b for all n ∈ , then lim xn ≤ b. Note that taking limits does not preserve strict
ordering! For example, 1/n > 0 for all n, but limn 1/n > 0 is false.
Theorem A.2.12 Let ( xn ), (yn ) and (zn ) be three sequences in
all n ∈ . If xn → a and zn → a both hold, then yn → a.

, with xn ≤ yn ≤ zn for

Proof. Fix  > 0. We can choose an N ∈
such that if n ≥ N, then xn > a −  and
zn < a + . (Why?) For such n we must have |yn − a| < .
You might have thought it would be simpler to argue that, since xn ≤ yn ≤ zn for
all n, we have lim xn ≤ lim yn ≤ lim zn from theorem A.2.11. But this is not permissible because we did not know at the start of the proof that lim yn exists. Theorem A.2.11
expressly requires that the limits exist. (This is an easy mistake to make.)
Theorem A.2.13 Let ( xn ) and (yn ) be real sequences. If xn → x and yn → y, then xn +
yn → x + y.
Proof. Fix  > 0. By the triangle inequality,

|( xn + yn ) − ( x + y)| ≤ | xn − x | + |yn − y|

(A.7)

Choose N ∈
such that | xn − x | < /2 whenever n ≥ N, and N ∈
such that
|yn − y| < /2 whenever n ≥ N . For n ≥ max{ N, N }, the right-hand side of (A.7) is
less than .
Exercise A.2.14 Show that if a ∈

and xn → x, then axn → ax.

Theorem A.2.15 Let ( xn ) and (yn ) be real sequences. If xn → x and yn → y, then xn yn →
xy.
for the proof: Suppose that x > y. Take -balls around each point that do not intersect. (Convince
yourself that this is possible.) Now try to contradict xn ≤ yn for all n.
14 Hint

Real Analysis

331

Proof. In view of exercise A.2.6, there is a positive integer M such that | xn | ≤ M for
all n ∈ . By the triangle inequality,

| xn yn − xy| = | xn yn − xn y + xn y − xy| ≤ | xn yn − xn y| + | xn y − xy|
= | xn ||yn − y| + |y|| xn − x | ≤ M|yn − y| + |y|| xn − x | < 
The result now follows from exercise A.2.14 and theorem A.2.13.
If ( xn ) is a sequence in , the term ∑n≥1 xn or ∑n xn is deﬁned, when it exists,
as the limit of the sequence (sk ), where sk := ∑kn=1 xn . If sk → ∞, then we write
∑n xn = ∞. Of course the limit may fail to exist entirely, as for xn = (−1)n .
Lemma A.2.16 Let ( xn ) ⊂

+.

If ∑n xn < ∞, then xn → 0.

Proof. Suppose instead that xn → 0 fails. Then ∃  > 0 such that xn >  inﬁnitely
often. (Why?) Hence ∑n xn = ∞. (Why?) Contradiction.

A.2.2

Max, Min, Sup, and Inf

Let x and y be any two real numbers. We will use the notation x ∨ y for the maximum
of x and y, while x ∧ y is their minimum. The following equalities are bread and
butter:
Lemma A.2.17 For any x, y ∈

and any a ≥ 0 we have the following identities:

1. x + y = x ∨ y + x ∧ y.
2. | x − y| = x ∨ y − x ∧ y.
3. | x − y| = x + y − 2( x ∧ y).
4. | x − y| = 2( x ∨ y) − x − y.
5. a( x ∨ y) = ( ax ) ∨ ( ay).
6. a( x ∧ y) = ( ax ) ∧ ( ay).
To see that x + y = x ∨ y + x ∧ y, pick any x, y ∈ . Suppose without loss of
generality that x ≤ y. Then x ∨ y + x ∧ y = y + x, as was to be shown. The remaining
equalities are left as exercises.
Exercise A.2.18 Show that if xn → x in , then | xn | → | x |. (Hint: Use (A.6) on
page 327.) Using this result and identities 3 and 4 in lemma A.2.17, argue that if
xn → x and yn → y, then xn ∧ yn → x ∧ y and xn ∨ yn → x ∨ y.

332

Appendix A

If A ⊂ , the maximum of A, when it exists, is a number m ∈ A with a ≤ m for all
a ∈ A. The minimum is deﬁned analogously. For any ﬁnite collection of real numbers,
the maximum and minimum always exist. For inﬁnite collections this is not the case.
To deal with inﬁnite sets we introduce the notion of suprema and inﬁma.
Given a set A ⊂ , an upper bound of A is any number u such that a ≤ u for all
a ∈ A. If s ∈ is an upper bound for A and also satisﬁes s ≤ u for every upper bound
u of A, then s is called the supremum of A. You will be able to verify that at most one
such s exists. We write s = sup A.
Lemma A.2.19 Suppose that s is an upper bound of A. The following statements are then
equivalent:
1. s = sup A.
2. s ≤ u for all upper bounds u of A.
3. ∀  > 0, ∃ a ∈ A with a > s − .
4. There exists a sequence ( an ) ⊂ A with an ↑ s.
Exercise A.2.20 Prove lemma A.2.19.
Exercise A.2.21 Show that sup(0, 1) = 1 and sup(0, 1] = 1. Show that if a set A
contains one of its upper bounds u, then u = sup A.
Theorem A.2.22 Every nonempty subset of

that is bounded above has a supremum in

.

The proof is omitted, but this is in fact equivalent to axiom A.2.4. Either one can be
treated as the axiom. They assert the “completeness” of the real numbers.
If A is not bounded above, then it is conventional to set sup A := ∞. With this
convention, the following statement is true:
Lemma A.2.23 If A, B ⊂

with A ⊂ B, then sup A ≤ sup B.

Proof. If sup B = ∞ the result is trivial. Suppose instead that B is bounded above,
and let b̄ := sup B, ā = sup A. By lemma A.2.19, there is a sequence ( an ) ⊂ A with
an ↑ ā. But b̄ is an upper bound for A (why?), so an ≤ b̄ for all n. It now follows from
theorem A.2.11 that ā = lim an ≤ b̄.
For A ⊂
a lower bound of A is any number l such that a ≥ l for all a ∈ A. If
i ∈
is an lower bound for A and also satisﬁes i ≥ l for every lower bound l of A,
then i is called the inﬁmum of A. At most one such i exists. We write i = inf A. Every
nonempty subset of bounded from below has an inﬁmum.
Exercise A.2.24 Let A be bounded below. Show that i = inf A if and only if i is a lower
bound of A and, for each  > 0, there is an a ∈ A with a < i + .

Real Analysis

333

Lemma A.2.25 If A, B ⊂

with A ⊂ B, then inf A ≥ inf B.

Proof. The proof is an exercise.
For ( xn ) ⊂

we set

lim inf xn := lim inf xk
n→∞ k≥n

and

lim sup xn := lim sup xk
n→∞

k≥n

If ( xn ) is bounded, then both lim inf xn and lim sup xn always exist in

. (Why?)

Exercise A.2.26 For A a bounded subset of , let − A be all b ∈ such that b = − a
for some a ∈ A. Show that − sup A = inf(− A). Let ( xn ) be a bounded sequence of
real numbers. Show that − lim sup xn = lim inf − xn .
Exercise A.2.27 Let ( xn ) be a sequence of real numbers, and let x ∈
limn xn = x if and only if lim supn xn = lim infn xn = x.

. Show that

Exercise A.2.28 Let ( xn ), (yn ), and (zn ) be sequences of real numbers with xn ≤
yn + zn for all n ∈ . Show that the following inequality always holds:
lim sup xn ≤ lim sup yn + lim sup zn
Exercise A.2.29 Show that ( xn ) ⊂
Let f : A →

+

and lim sup xn = 0 implies lim xn = 0.15

, where A is any nonempty set. We will use the notation
sup f :=: sup f ( x ) := sup{ f ( x ) : x ∈ A}
x∈ A

Also, if g : A → , then f + g is deﬁned by ( f + g)( x ) = f ( x ) + g( x ), while | f | is
deﬁned by | f |( x ) = | f ( x )|.
Lemma A.2.30 Let f , g : A →

, where A is any nonempty set. Then

sup( f + g) ≤ sup f + sup g
Proof. We can and do suppose that sup f and sup g are ﬁnite. (Otherwise the result is
trivial.) For any x ∈ A, f ( x ) ≤ sup f and g( x ) ≤ sup g.

15 Hint:

∴

f ( x ) + g( x ) ≤ sup f + sup g

∴

sup( f + g) ≤ sup f + sup g

A neat argument follows from theorem A.2.12.

334

Appendix A

Lemma A.2.31 If f : A →

, then | sup f | ≤ sup | f |.

Proof. We can and do suppose that sup | f | < ∞. Evidently sup f ≤ sup | f |.16 To
complete the proof, we need only show that − sup f ≤ sup | f | also holds. This is the
case because
0 = sup(− f + f ) ≤ sup(− f ) + sup f ≤ sup | f | + sup f

Exercise A.2.32 Show via counterexample that the statement | sup f | = sup | f | does
not hold in general.
Let A ⊂ . A function f : A → is called monotone increasing on A if, whenever
x, y ∈ A and x ≤ y, we have f ( x ) ≤ f (y). It is called monotone decreasing if, whenever
x ≤ y, we have f ( x ) ≥ f (y). We say strictly monotone increasing or strictly monotone
decreasing if the previous inequalities can be replaced with strict inequalities.
Exercise A.2.33 Let S be any set, let g : S → , and let x̄ be a maximizer of g on S,
in the sense that g( x̄ ) ≥ g( x ) for all x ∈ S. Prove that if f :
→
is monotone
increasing, then x̄ is a maximizer of f ◦ g on S.

A.2.3

Functions of a Real Variable

Let’s recall some basics about functions when send subsets of
into . Below we
deﬁne such concepts as continuity, differentiability, convexity, and concavity. If you
are rusty on these deﬁnitions, then it is probably worth skim-reading this section and
completing a few of the exercises.
and let f , g : A → . As usual, the sum of f and g is the function
Let A ⊂
f + g deﬁned by ( f + g)( x ) := f ( x ) + g( x ). Similarly, the product f g is deﬁned by
( f g)( x ) := f ( x ) g( x ). The product of real number α and f is the function (α f )( x ) :=
α f ( x ). Recall that f is called bounded if its range is a bounded set (i.e., ∃ M ∈ such
that | f ( a)| ≤ M for all a ∈ A).
Exercise A.2.34 Show that if f and g are bounded and α ∈
are also bounded functions.

, then f + g, f g, and α f

Function f : A → is said to be continuous at a ∈ A if for every sequence ( xn ) in
A converging to a we have f ( xn ) → f ( a). (Sketch it.) It is called continuous on A (or
just continuous) whenever it is continuous at every a ∈ A. Continuity of functions
captures the idea that small changes to the input do not lead to sudden jumps in the
16 Pick

any x ∈ A. Then f ( x ) ≤ | f ( x )| ≤ sup | f |. Since x is arbitrary, sup f ≤ sup | f |.

Real Analysis

335

output. Notice that in requiring that f ( xn ) → f ( a) for each xn → a, we require that not
only does f ( xn ) actually converge for each choice of xn → a, but all these sequences
converge to the same limit, and moreover that limit is f ( a).
Exercise A.2.35 Prove carefully that the functions f ( x ) = x + 1 and g( x ) = x2 are
continuous. Give an example of a function that is not continuous, showing how it
fails the deﬁnition.
More generally, for the same f : A → and for a ∈ A, we say that y = limx→ a f ( x )
if f ( xn ) → y for every sequence ( xn ) ⊂ A with xn → a. Note that limx→ a f ( x ) may
not exist. It may be the case that different sequences converging to a yield different
limits for the sequence f ( xn ), or indeed that f ( xn ) does not converge at all. But this
new notation is useful because we can now say that f is continuous at a if and only if
limx→ a f ( x ) exists and is equal to f ( a).
Exercise A.2.36 Show that if f and g are continuous functions and α ∈
f g and α f are also continuous.

, then f + g,

is said to be continuous from the left at x ∈ A
Exercise A.2.37 A function f : A →
if f ( xn ) → f ( x ) for every sequence xn ↑ x; and continuous from the right at x ∈ A
if f ( xn ) → f ( x ) for every sequence xn ↓ x. Clearly, a function continuous at x is
both continuous from the left at x and continuous from the right at x. Show that the
converse also holds.17
One of the many delightful results concerning continuous functions is the intermediate value theorem:
Theorem A.2.38 Let f : [ a, b] → , where a < b. If f is continuous on [ a, b] and f ( a) <
0 < f (b), then there exists an s ∈ ( a, b) with f (s) = 0.
Proof. Let A := { x ∈ [ a, b] : f ( x ) < 0}, and let s be the supremum of this set. (Why
can we be sure that such a supremum exists?) We claim that f (s) = 0. To see why
this must be the case, observe that since s = sup A there exists a sequence ( xn ) with
f ( xn ) < 0 and xn ↑ s. (Why?) By continuity of f , we have lim f ( xn ) = f (s). But
f ( xn ) < 0 for all n, so lim f ( xn ) ≤ 0. Hence f (s) ≤ 0. On the other hand, since s is an
upper bound of A, we know that x > s implies x ∈
/ A, in which case f ( x ) ≥ 0. Take
a strictly decreasing sequence ( xn ) in (s, b] with xn ↓ s. (Convince yourself that such
a sequence does exist.) As f ( xn ) ≥ 0 for all n it follows that lim f ( xn ) = f (s) ≥ 0.
Therefore f (s) = 0.
Exercise A.2.39 Using theorem A.2.38 (the result, not the proof), show that the same
result holds when f (b) < 0 < f ( a).
17 Hint:

You might like to make use of exercise A.2.8 and theorem A.2.9.

336

Appendix A

Let’s brieﬂy review differentiability. Let f : ( a, b) → , and let x ∈ ( a, b). The
function f is said to be differentiable at x if, for every sequence (hn ) converging to zero
and satisfying hn = 0 and x + hn ∈ ( a, b) for each n, the sequence
f ( x + hn ) − f ( x )
hn
converges, and the limit is independent of the choice of (hn ). If such a limit exists, it
is denoted by f ( x ). The function f is called differentiable if it is differentiable at each
point in its domain, and continuously differentiable if, in addition to being differentiable,
x → f ( x ) is continuous everywhere on the domain of f .

→

Exercise A.2.40 Let f :
x∈ .

be deﬁned by f ( x ) = x2 . Prove that f ( x ) = 2x for any

A function f from an interval I to

is called convex (resp., strictly convex) if

λ f ( x ) + (1 − λ) f (y) ≥ f (λx + (1 − λ)y)
for all λ ∈ [0, 1] and x, y ∈ I (resp., for all x = y and all λ ∈ (0, 1)), and concave (resp.,
strictly concave) if
λ f ( x ) + (1 − λ) f (y) ≤ f (λx + (1 − λ)y)
for all λ ∈ [0, 1] and x, y ∈ I (resp., for all x = y and all λ ∈ (0, 1)). Since f is concave
if and only if − f is convex, we can think of convexity is the fundamental property;
concavity is merely a shorthand way of referring to convexity of − f .
There are numerous connections between continuity, differentiability, and convexity. For example, if f : [ a, b] → is convex, then it is continuous everywhere on ( a, b).
Also you are no doubt aware that if f is twice differentiable, then nonnegativity of
f on ( a, b) is equivalent to convexity on ( a, b). These facts can be proved from the
deﬁnitions above.
Finally, let’s consider right and left derivatives. Let f : ( a, b) → . For ﬁxed x ∈
( a, b) we deﬁne
D ( x, h) :=

f ( x + h) − f ( x )
h

(h = 0 and x + h ∈ ( a, b))

If for each sequence hn ↓ 0 the limit limn→∞ D ( x, hn ) exists and is equal to the same
number, we call that number the right-hand derivative of f at x, and denote it by
f + ( x ). If for each sequence hn ↑ 0 the limit limn→∞ D ( x, hn ) exists and is equal to
the same number, we call that number the left-hand derivative of f at x, and denote
it by f − ( x ). It turns out that f is differentiable at x if and only if both the left- and
right-hand derivatives exist at x and are equal. The proof is not too difﬁcult if you feel
like doing it as an exercise.
The following lemma collects some useful facts:

Real Analysis

337

Lemma A.2.41 If f is concave on ( a, b), then f + and f − exist everywhere on ( a, b). For each
x ∈ ( a, b),
f + ( x ) = sup D ( x, h) and f − ( x ) = inf D ( x, h)
h >0

h <0

Moreover f + ≤ f − everywhere on ( a, b). If f + ( x ) = f − ( x ) at some point x ∈ ( a, b), then f
is differentiable at x, and f ( x ) = f + ( x ) = f − ( x ).
Exercise A.2.42 Prove lemma A.2.41. First show that when f is concave, D ( x, h) is decreasing in h. Next apply existence results for limits of monotone bounded sequences.

Appendix B

Chapter Appendixes
B.1

Appendix to Chapter 3

Let us brieﬂy discuss the topic of parametric continuity. The question we address is
whether or not the solution to a given optimization problem varies continuously with
the parameters that deﬁne the problem. The classic theorem in this area is Berge’s
theorem of the maximum. You should familiarize yourself at least with the statement
of the theorem.
To begin, let A and B be two sets. A function Γ from A into P( B) (i.e., into the
subsets of B) is called a correspondence from A to B. Correspondences are often used
to deﬁne constraint sets. For example, a ∈ A might be the price of a commodity, or
a level of wealth, and Γ( a) ⊂ B is the budget set associated with that value of the
parameter.
Now suppose that A and B are metric spaces, and let Γ be a correspondence from
A to B. We say that Γ is compact-valued if Γ( a) is a compact subset of B for every
a ∈ A, and nonempty if Γ( a) = ∅ for every a ∈ A. A nonempty compact-valued
correspondence Γ from A to B is called upper-hemicontinuous at a ∈ A if, for each
sequence ( an ) ⊂ A with an → a, and each sequence (bn ) ⊂ B with bn ∈ Γ( an ) for
all n ∈ , the sequence (bn ) has a convergent subsequence whose limit is in Γ( a).
It is called lower-hemicontinuous at a if, for each ( an ) ⊂ A with an → a and each
b ∈ Γ( a), there is a sequence (bn ) ⊂ B with bn ∈ Γ( an ) for all n ∈ , and bn → b
as n → ∞. Finally, Γ is called continuous at a if it is both upper-hemicontinuous and
lower-hemicontinuous at a. It is called continuous if it is continuous at a for each
a ∈ A.
The following lemma treats an important special case:

339

340

Appendix B

Lemma B.1.1 Let A ⊂
P( ) be deﬁned by

, let g and h be continuous functions from A to

Γ( x ) = {y ∈

: g( x ) ≤ y ≤ h( x )}

, and let Γ : A →

( x ∈ A)

If g and h are continuous functions, then the correspondence Γ is also continuous.
Proof. Pick any a ∈ A. First let’s check upper-hemicontinuity at a. Let ( an ) ⊂ A,
an → a, and let (bn ) ⊂ B, bn ∈ Γ( an ) for all n. We claim the existence of a subsequence
(bn j ) and a b ∈ Γ( a) with bn j → b as j → ∞.
To see why—ﬁll in any gaps in the argument to your own satisfaction—note that
( an ) is bounded and C := { a} ∪ { an }n∈ is closed, from which it follows that G :=
infx∈C g( x ) and H = supx∈C h( x ) exist (see theorem 3.2.20 on page 48). But as G ≤
bn ≤ H for all n, the sequence bn is bounded and hence contains a convergent subsequence bn j → b. Observing that g( an j ) ≤ bn j ≤ h( an j ) for all j ∈ , we can take the
limit to obtain g( a) ≤ b ≤ h( a). In other words, b ∈ Γ( a), as was to be proved.
Regarding lower-hemicontinuity at a, given ( an ) with an → a and b ∈ Γ( a), we
claim there is a sequence (bn ) ⊂ B with bn ∈ Γ( an ) for all n ∈ and bn → b. To see
this, suppose ﬁrst that b = g( a). Setting bn = g( an ) gives the desired convergence.
The case of b = h( a) is treated similarly. Suppose instead that g( a) < b < h( a). It
follows that for N sufﬁciently large we have g( an ) < b < h( an ) whenever n ≥ N.
Taking b1 , . . . , b N −1 arbitrary and bn = b for all n ≥ N gives a suitable sequence
bn → b.
Exercise B.1.2 Let Γ : A → B be a correspondence such that Γ( a) is a singleton {ba }
for each a ∈ A. Show that if Γ is a continuous correspondence, then a → ba is a
continuous function.
Now we can state Berge’s theorem.
Theorem B.1.3 Let Θ and U be two metric spaces, let Γ be a correspondence from Θ to U,
and let
gr Γ := {(θ, u) ∈ Θ × U : u ∈ Γ(θ )}
If f : gr Γ →
function

is continuous, and Γ is nonempty, compact-valued and continuous, then the
g: Θ

θ → max f (θ, u) ∈
u∈Γ(θ )

is continuous on Θ. The correspondence of maximizers
M: Θ

θ → argmax f (θ, u) ⊂ U
u∈Γ(θ )

is compact-valued and upper-hemicontinuous on Θ. In particular, if M (θ ) is single-valued,
then it is continuous.

Chapter Appendixes

341

In the theorem, continuity of f on gr Γ means that if (θ, u) ∈ gr Γ and (θn , un ) is a
sequence in gr Γ with (θn , un ) → (θ, u), then f (θn , un ) → f (θ, u). (This is a stronger
requirement than assuming f is continuous in each individual argument while the
other is held ﬁxed.) The theorem is well-known to economists, and we omit the proof.
See Aliprantis and Border (1999, thm. 16.31), or Stokey and Lucas (1989, thm. 3.6).
The next result is a direct implication of Berge’s theorem B.1.3 but pertains to parametric continuity of ﬁxed points.
Theorem B.1.4 Let Θ, U and Γ be as in theorem B.1.3. Let g : gr Γ → U, and let
F (θ ) := {u ∈ U : u = g(θ, u)}

(θ ∈ Θ)

If F (θ ) is nonempty for each θ ∈ Θ, g is continuous on gr Γ, and Γ is nonempty, compactvalued and continuous, then θ → F (θ ) is compact-valued and upper-hemicontinuous on Θ.
In particular, if F (θ ) is single-valued, then it is continuous.
Proof. Continuity of g on gr Γ means that if (θ, u) ∈ gr Γ and (θn , un ) is a sequence in
gr Γ with (θn , un ) → (θ, u), then g(θn , un ) → g(θ, u). Let f : gr Γ → be deﬁned by
f (θ, u) = −ρ(u, g(θ, u))
where ρ is the metric on U. You will be able to show that f is also continuous on
gr Γ. Theorem B.1.3 then implies that θ → M (θ ) is compact-valued and upper-hemicontinuous, where M (θ ) is the set of maximizers argmaxu∈Γ(θ ) f (θ, u).
Now pick any θ ∈ Θ. As F (θ ) is assumed to be nonempty, the set of maximizers
of f and ﬁxed-points of g coincide. That is, M(θ ) = F (θ ). Since θ is arbitrary, M
and F are the same correspondence on Θ, and θ → F (θ ) is also compact-valued and
upper-hemicontinuous.
Next we turn to the
Proof of theorem 3.2.38. Uniqueness is by exercise 3.2.34. To prove existence, deﬁne
r : S → by r ( x ) = ρ( Tx, x ). It is not too difﬁcult to show that r is continuous (with
respect to ρ). Since S is compact, r has a minimizer x ∗ . But then Tx ∗ = x ∗ must hold,
because otherwise
r ( Tx ∗ ) = ρ( TTx ∗ , Tx ∗ ) < ρ( Tx ∗ , x ∗ ) = r ( x ∗ )
contradicting the deﬁnition of x ∗ .
Next we show that T n x → x ∗ as n → ∞ for all x ∈ S. To see this, pick any x ∈ S and
consider the real sequence (αn ) deﬁned by αn := ρ( T n x, x ∗ ). Since T is contracting,
the sequence (αn ) is monotone decreasing, and therefore converges (why?) to some
limit α ≥ 0. I claim that α = 0.

342

Appendix B

To see this, we can argue as follows: By compactness of S the sequence ( T n x ) has
a subsequence ( T n(k) x ) with T n(k) x → x for some x ∈ S. It must be the case that
ρ( x , x ∗ ) = α. The reason is that y → ρ(y, x ∗ ) is continuous as a map from S to
(example 3.1.12 on page 40). Hence ρ( T n(k) x, x ∗ ) → ρ( x , x ∗ ). But ρ( T n(k) x, x ∗ ) → α
and sequences have at most one limit, so ρ( x , x ∗ ) = α.
It is also the case that ρ( Tx , x ∗ ) = α. To see this, note that by continuity of T we
have
T ( T n(k) x ) = T n(k)+1 x → Tx
Since y → ρ(y, x ∗ ) is continuous, we have ρ( T n(k)+1 x, x ∗ ) → ρ( Tx , x ∗ ). At the same
time, ρ( T n(k)+1 x, x ∗ ) → α is also true, so ρ( Tx , x ∗ ) = α.
We have established the existence of a point x ∈ S such that both ρ( x , x ∗ ) and
ρ( Tx , x ∗ ) are equal to α. If α > 0 the points x and x ∗ are distinct, implying
α = ρ( x , x ∗ ) > ρ( Tx , Tx ∗ ) = ρ( Tx , x ∗ ) = α
Contradiction.

B.2

Appendix to Chapter 4

We now provide the proof of theorem 4.3.17 on page 89. To begin our proof, consider
the following result:
Lemma B.2.1 If φ and ψ are elements of P (S) and h : S →

∑ h( x )φ( x ) − ∑ h( x )ψ( x )

x ∈S

x ∈S

≤

+,

then

1
sup |h( x ) − h( x )| · φ − ψ1
2 x,x

Proof. Let ρ( x ) := φ( x ) − ψ( x ), ρ+ ( x ) := ρ( x ) ∨ 0, ρ− ( x ) := (−ρ( x )) ∨ 0. It is left to
the reader to show that ρ( x ) = ρ+ ( x ) − ρ− ( x ), that |ρ( x )| = ρ+ ( x ) + ρ− ( x ), and that
∑ x∈S ρ+ ( x ) = ∑ x∈S ρ− ( x ) = (1/2)ρ1 .
In view of the equality | a − b| = a ∨ b − a ∧ b (lemma A.2.17, page 331), we have

∑ hφ − ∑ hψ

=

∑ hρ

∑ hρ+ −%∑ hρ−
#
$ #
$ #
$&#
$
= ∑ hρ+
∑ hρ− − ∑ hρ+
∑ hρ−
=

Consider the two terms to the right of the last equality. If sup h := supx∈S h( x ) and
inf h := infx∈S h( x ), then the ﬁrst term satisﬁes
#
$%#
$ #
$%#
$
sup h ∑ ρ−
∑ hρ+
∑ hρ− ≤ sup h ∑ ρ+

= sup h

#

∑ ρ+

$%#

∑ ρ−

$

= sup h

 ρ 1
2

Chapter Appendixes

343

while the second satisﬁes
$&#
$ #
$&#
$
#
inf h ∑ ρ−
∑ hρ+
∑ hρ− ≥ inf h ∑ ρ+

= inf h

#

∑ ρ+

$&#

∑ ρ−

$

= inf h

 ρ 1
2

Combining these two bounds, we get

∑ hφ − ∑ hψ

≤ (sup h − inf h)

 ρ 1
2

This is the same bound as given in the statement of the lemma.
We need two more results to prove theorem 4.3.17, both of which are straightforward.
Lemma B.2.2 Let p, M, φ, and ψ be as in theorem 4.3.17. Then1

φM − ψM1 ≤

1
sup  p( x, dy) − p( x , dy)1 · φ − ψ1
2 x,x

Proof. In the proof of this lemma, if φ ∈ P (S) and A ⊂ S, we will write φ( A) as a
shorthand for ∑y∈ A φ( x ). So pick any A ⊂ S. In view of lemma B.2.1, we have

∑ P(x, A)φ(x) − ∑ P(x, A)ψ(x)

x ∈S

x ∈S

≤

1
sup | P( x, A) − P( x , A)| · φ − ψ1
2 x,x

Applying the result of exercise 4.3.2, we obtain

∑ P(x, A)φ(x) − ∑ P(x, A)ψ(x)

x ∈S

x ∈S

≤

1
sup  p( x, dy) − p( x , dy)1 · φ − ψ1
4 x,x

which is another way of writing

|φM( A) − ψM( A)| ≤
∴

1
sup  p( x, dy) − p( x , dy)1 · φ − ψ1
4 x,x

sup |φM( A) − ψM( A)| ≤
A⊂S

1
sup  p( x, dy) − p( x , dy)1 · φ − ψ1
4 x,x

Using exercise 4.3.2 again, we obtain the bound we are seeking.
1 Here

 p( x, dy) − p( x , dy)1 is to be interpreted as ∑y∈S | p( x, y) − p( x y)|.

344

Appendix B
To prove the ﬁrst claim in theorem 4.3.17, it remains only to show that
1
sup  p( x, dy) − p( x , dy)1 = 1 − inf ∑ p( x, y) ∧ p( x , y)
2 x,x
x,x y∈S

It is sufﬁcient (why?) to show that  p( x, dy) − p( x , dy)1 /2 = 1 − ∑y∈S p( x, y) ∧
p( x , y) for any pair x, x . Actually this is true for any pair of distributions, as shown
in the next and ﬁnal lemma.
Lemma B.2.3 For any pair μ, ν ∈ P (S), we have μ − ν1 /2 = 1 − ∑y∈S μ(y) ∧ ν(y).
Proof. From lemma A.2.17 (page 331) one can show that given any pair of real numbers a and b, we have | a − b| = a + b − 2a ∧ b. Hence for each x ∈ S we obtain

|μ( x ) − ν( x )| = μ( x ) + ν( x ) − 2μ( x ) ∧ ν( x )
Summing over x gives the identity we are seeking.
The ﬁrst claim in theorem 4.3.17 is now established. Regarding the second claim,
we have
1 − α( p) =

1
sup  p( x, dy) − p( x , dy)1
2 x,x

= sup
x = x

 p( x, dy) − p( x , dy)1
μM − νM1
≤ sup
δx − δx 1
 μ − ν 1
μ=ν

The claim now follows from the deﬁnition of the supremum.

B.3

Appendix to Chapter 6

Proof of theorem 6.3.8. Pick any u and v in bU. Observe that
u = u + v − v ≤ v + |u − v| ≤ v + u − v∞
where (in)equalities are pointwise on U. By the monotonicity property of T, we have
Tu ≤ T (v + u − v∞ ). Applying (6.29), we have Tu − Tv ≤ λu − v∞ . Reversing
the roles of u and v gives Tv − Tu ≤ λu − v∞ . These two inequalities are sufﬁcient
for the proof. (Why?)

Chapter Appendixes

B.4

345

Appendix to Chapter 8

First let us prove lemma 8.2.3, beginning with some preliminary discussion: We can
extend M to act on all functions f in L1 (S) by setting f M(y) := p( x, y) f ( x )dx. The
inequality  f M1 ≤  f 1 always holds. Under the following condition it is strict:
Lemma B.4.1 Let f ∈ L1 (S). Then  f M1 <  f 1 if and only if
λ[( f + M) ∧ ( f − M)] > 0
Proof. In view of lemma A.2.17 (page 331) the pointwise inequality

| f M | = | f + M − f − M | = f + M + f − M − 2( f + M ) ∧ ( f − M )
holds. Integrating over S and making some simple manipulations, we have

 f M1 = λ( f + ) + λ( f − ) − 2λ[( f + M) ∧ ( f − M)]
∴

 f M1 =  f 1 − 2λ[( f + M) ∧ ( f − M)]

The proof is done.
Proof of lemma 8.2.3. Note that it is sufﬁcient to prove the stated result for the case
t = 1 because if it holds at t = 1 for an arbitrary kernel q and its associated Markov
operator N, then it holds for q := pt , and the Markov operator associated with pt is
Mt (lemma 8.1.13, page 196).
So choose any distinct φ, ψ ∈ D (S), and let f := φ − ψ. In view of lemma B.4.1 we
will have φM − ψM1 < φ − ψ whenever
 
"
 !
&
+
−
p( x, y) f ( x )dx
p( x, y) f ( x )dx
dy > 0
(B.1)
With a little bit of effort one can show that, for each y ∈ S, we have
 


&
p( x, y) f + ( x )dx
p( x , y) f − ( x )dx

≥

 

p( x, y) ∧ p( x , y) f + ( x ) f − ( x )dxdx

Integrating over y shows that (B.1) dominates
"
  !
p( x, y) ∧ p( x , y)dy f + ( x ) f − ( x )dxdx
Since the inner integral is always positive by hypothesis, and both f + and f − are
nontrivial (due to distinctness of φ and ψ), this term is strictly positive. Lemma 8.2.3
is now established.

346

Appendix B

Proof of proposition 8.2.12. In lemma 11.2.9 it is shown that if λ(w · φ) < ∞ and the
geometric drift condition holds then (φMt )t≥0 is tight. It remains to establish this
result for general ψ ∈ D (S). We establish it via the following two claims:
1. The set D0 of all φ ∈ D (S) with λ(w · φ) < ∞ is dense in D (S).2
2. If there exists a sequence (φn ) ⊂ D (S) such that (φn Mt )t≥0 is tight for each
n ∈ and d1 (φn , ψ) → 0, then (ψMt )t≥0 is tight.
The two claims are sufﬁcient because if claim 1 holds, then there is a dense subset
D0 of D (S) such that trajectories starting from D0 are all tight. Since D0 is dense, the
existence of a sequence with the properties in claim 2 is assured.
Regarding the ﬁrst claim, let Cn := { x : w( x ) ≤ n} and pick any φ ∈ D (S).
Deﬁne φn := an Cn φ, where an is the normalizing constant 1/λ( Cn φ). It can be veriﬁed that the sequence (φn ) lies in D0 and converges pointwise to φ. Scheffè’s lemma
(see Taylor, 1997, p. 186) implies that for densities pointwise convergence implies d1
convergence. Since φ is an arbitrary density, D0 is dense.
Regarding claim 2, pick any  > 0, and choose n such that d1 (φn , ψ) ≤ /2. Nonexpansiveness of M implies that d1 (φn Mt , ψMt ) ≤ /2 for all t. Since (φn Mt ) is tight,
there exists a compact set K such that λ( Kc φn Mt ) ≤ /2 for all t. But then
λ(

K c ψM

for all t ∈

t

) = λ(

K c | ψM

t

− φn Mt + φn Mt |) ≤ d1 (ψMt , φn Mt ) + λ(

K c φn M

t

)≤

. Hence (ψMt )t≥0 is tight as claimed.

Proof
of proposition 8.2.13. Fix  > 0. We claim the existence of a δ > 0 such that

t ( x ) dx <  whenever λ ( A ) < δ. Since ( ψMt )
ψM
t≥0 is tight, there exists a compact
A
set K such that


λ( Kc ψMt ) :=:
ψMt dλ <
∀t ∈
(B.2)
2
Kc
For any Borel set A ⊂ S the decomposition

A

ψMt dλ =


A∩K

ψMt dλ +


A∩K c

ψMt dλ

(B.3)

holds. Consider the ﬁrst term in the sum. We have
!
"


(ψMt )( x )λ(dx ) =
p( x, y)(ψMt−1 )( x )λ(dx ) λ(dy)
A∩K
A∩K
"
 !
=
p( x, y)λ(dy) (ψMt−1 )( x )λ(dx )
A∩K

2A

subset A of a metric space U is called dense in U if every element of U is the limit of a sequence in A.

Chapter Appendixes

347

But by the hypothesis p ≤ m and the fact that the image of the continuous function m
is bounded on K by some constant N < ∞,


∴



A∩K

p( x, y)λ(dy) ≤

ψM dλ =
t

A∩K



 !
A∩K

A∩K

m(y)λ(dy) ≤ N · λ( A)
"

p( x, y)λ(dy) ψMt−1 dλ ≤ Nλ( A)

(B.4)

Combining (B.2), (B.3), and (B.4), we obtain the bound

A

(ψMt )( x )λ(dx ) ≤ N · λ( A) +


2

for any t and any A ∈ B (S). Setting δ := /(2N ) now gives the desired result.

B.5

Appendix to Chapter 10

Next we give the proof of theorem 10.2.11. To simplify notation, we write  · ∞ as
 · . The theorem claims that if σ is the policy generated by the approximate value
iteration algorithm, then

v∗ − vσ  ≤

2
(ρvn − vn−1  + v∗ − Lv∗ )
(1 − ρ )2

This result follows immediately from the next two lemmas.
Lemma B.5.1 The vn -greedy policy σ satisﬁes

v∗ − vσ  ≤
Proof. We have

2
vn − v∗ 
1−ρ

v∗ − vσ  ≤ v∗ − vn  + vn − vσ 

(B.5)

(B.6)

The second term on the right-hand side of (B.6) satisﬁes

vn − vσ  ≤ vn − Tvn  +  Tvn − vσ 

(B.7)

Consider the ﬁrst term on the right-hand side of (B.7). Observe that for any w ∈
bB (S), we have

w − Tw ≤ w − v∗  + v∗ − Tw ≤ w − v∗  + ρv∗ − w = (1 + ρ)w − v∗ 
Substituting in vn for w, we obtain

vn − Tvn  ≤ (1 + ρ)vn − v∗ 

(B.8)

348

Appendix B

Now consider the second term on the right-hand side of (B.7). Since σ is vn -greedy,
we have Tvn = Tσ vn , and

 Tvn − vσ  =  Tσ vn − vσ  =  Tσ vn − Tσ vσ  ≤ ρvn − vσ 
Substituting this bound and (B.8) into (B.7), we obtain

vn − vσ  ≤ (1 + ρ)vn − v∗  + ρvn − vσ 
∴

vn − vσ  ≤

1+ρ
 v n − v ∗ .
1−ρ

This inequality and (B.6) together give

v∗ − vσ  ≤ v∗ − vn  +

1+ρ
vn − v∗ 
1−ρ

Simple algebra now gives (B.5).
Lemma B.5.2 For every n ∈

, we have

(1 − ρ)v∗ − vn  ≤ v∗ − Lv∗  + ρvn − vn−1 
Proof. Let v̂ be the ﬁxed point of T̂. By the triangle inequality,

v∗ − vn  ≤ v∗ − v̂ + v̂ − vn 

(B.9)

Regarding the ﬁrst term on the right-hand side of (B.9), we have

v∗ − v̂ ≤ v∗ − T̂v∗  +  T̂v∗ − v̂
= v∗ − Lv∗  +  T̂v∗ − T̂ v̂ ≤ v∗ − Lv∗  + ρv∗ − v̂
∴

(1 − ρ)v∗ − v̂ ≤ v∗ − Lv∗ 

(B.10)

Regarding the second term in the sum (B.9), we have

v̂ − vn  ≤ v̂ − T̂ n+1 v0  +  T̂ n+1 v0 − T̂ n v0  ≤ ρv̂ − vn  + ρvn − vn−1 
∴

(1 − ρ)v̂ − vn  ≤ ρvn − vn−1 

Combining (B.9), (B.10), and (B.11) gives the bound we are seeking.

(B.11)

Chapter Appendixes

B.6

349

Appendix to Chapter 11

Proof of lemma 11.1.29. It is an exercise to show that if φ, ψ ∈ P (S), then

φ − ψ TV = 2(φ − ψ)+ (S) = 2(φ − ψ)− (S) = 2(φ − ψ)(S+ )

(B.12)

where S+ is a positive set for the signed measure φ − ψ. Now suppose that S+ is a
maximizer of |φ( B) − ψ( B)| over B (S). In this case, we have
sup |φ( B) − ψ( B)| = |φ(S+ ) − ψ(S+ )| = (φ − ψ)(S+ )

B ∈B ( S )

and the claim in lemma 11.1.29 follows from (B.12). Hence we need only show that
S+ is indeed a maximizer. To do so, pick any B ∈ B (S), and note that

|φ( B) − ψ( B)| = |(φ − ψ)+ ( B) − (φ − ψ)− ( B)|
= (φ − ψ)+ ( B) ∨ (φ − ψ)− ( B) − (φ − ψ)+ ( B) ∧ (φ − ψ)− ( B)
where the second equality follows from lemma A.2.17 on page 331.

∴

|φ( B) − ψ( B)| ≤ (φ − ψ)+ ( B) ∨ (φ − ψ)− ( B) ≤ (φ − ψ)+ (S)

But (φ − ψ)+ (S) = (φ − ψ)(S+ ) by deﬁnition, so S+ is a maximizer as claimed.
Next let’s prove theorem 11.2.5. In the proof, P is a stochastic kernel and M is the
Markov operator. By assumption, Mh ∈ bcS whenever h ∈ bcS.
Proof of theorem 11.2.5. Let ψ be as in the statement of the theorem, so (ψMt )t≥1 is
tight. Let νn := n1 ∑nt=1 ψMt . The sequence (νn )n≥1 is also tight (proof?), from which it
follows (see Prohorov’s theorem, on page 257) that there exists a subsequence (νnk ) of
(νn ) and a ν ∈ P (S) such that d FM (νnk , ν) → 0 as k → ∞. It is not hard to check that,
for all n ∈ , we have
ψMn+1 − ψM
νn M − νn =
n
We aim to show that d FM (νM, ν) = 0, from which it follows that ν is stationary for
M. From the deﬁnition of the Fortet–Mourier distance (see page 256), it is sufﬁcient
to show that for any bounded Lipschitz function h ∈ bS with  hb ≤ 1 we have
|νM(h) − ν(h)| = 0.
So pick any such h. Observe that

|νM(h) − ν(h)| ≤ |νM(h) − νn M(h)| + |νn M(h) − νn (h)| + |νn (h) − ν(h)|

(B.13)

350

Appendix B

for all n ∈ . All three terms on the right-hand side of (B.13) converge to zero along
the subsequence (nk ), which implies |νM(h) − ν(h)| = 0. To see that this is the case,
consider the ﬁrst term. We have

|νM(h) − νnk M(h)| = |ν(Mh) − νnk (Mh)| → 0
where the equality is from the duality property in theorem 9.2.15 (page 227), and convergence is due to the fact that Mh is bounded and continuous, and d FM (νnk , ν) → 0
as k → ∞.
Consider next the second term in (B.13). That |νnk M(h) − νnk (h)| converges to zero
as k → ∞ follows from the bound

|νnk M(h) − νnk (h)| =

1
2
|ψMnk +1 (h) − ψM(h)| ≤
nk
nk

That the ﬁnal term in the sum (B.13) converges to zero along the subsequence (nk ) is
trivial, and this completes the proof of theorem 11.2.5.

Proof of lemma 11.3.3. The xb that solves P( xb ) = α p∗ (z)φ(dz) satisﬁes D (αP(0)) ≤
xb because P( xb ) = α p∗ (z)φ(dz) ≤ αp∗ (0) = αP(0). Here we are using the fact that
p∗ (0) = P(0).3 Also D (αP(0)) > 0 because D ( P(0)) = 0 and D is strictly decreasing.
Since xb ≥ D (αP(0)), we have shown that xb > 0.
We claim in addition that if x ≤ xb , then p∗ ( x ) = P( x ) and I ( x ) = 0. That p∗ ( x ) =
P( x ) implies I ( x ) = 0 is immediate from the deﬁnition: I ( x ) = x − D ( p∗ ( x )) (see
page 146). Hence we need only prove that when x ≤ xb we have p∗ ( x ) = 0. But if
x ≤ xb , then P( xb ) ≤ P( x ), and hence
P( x ) ≥ α



∗

p (z)φ(dz) ≥ α



p∗ (αI ( x ) + z)φ(dz)

That p∗ ( x ) = P( x ) is now clear from the deﬁnition of p∗ .4

B.7

Appendix to Chapter 12

Proof of proposition 12.1.17. Pick any y > 0 and deﬁne the h : [0, y] →
h ( k ) : = U ( y − k ) + W ( k ),

W (k) := ρ



by

w[ f (k, z)]φ(dz)

3 To prove this, one can show via (6.28) on page 147 that if p ≤ P (0), then T p ≤ P (0), from which it
follows that p∗ = limn T n P ≤ P(0). Therefore p∗ (0) ≤ P(0). On the other hand, p∗ ≥ P, so p∗ (0) ≥ P(0).
Hence p∗ (0) = P(0).
4 For the deﬁnition refer to (6.26) on page 146.

Chapter Appendixes

351

Given any  > 0, we have
h(y) − h(y − )
U () W (y) − W (y − )
=−
+




(B.14)

If σ (y) = y, then h(y) ≥ h(y − ), and (B.14) is nonnegative for all  > 0. But this
is impossible: On one hand, W (k) is concave and its left-hand derivative exists at
y (lemma A.2.41, page 337), implying that the second term on the right-hand side
converges to a ﬁnite number as  ↓ 0. On the other hand, the assumption U (0) = ∞
implies that the ﬁrst term converges to −∞.
Proof of proposition 12.1.18. Fix w ∈ C ibcS and y > 0. Deﬁne
W ( x, k ) := U ( x − k) + ρ



w( f (k, z))φ(dz)

( x > 0 and k ≤ x )

From proposition 12.1.17 we have σ(y) < y. From this inequality one can establish
the existence of an open neighborhood G of zero with 0 ≤ σ (y) ≤ y + h for all h ∈ G.

∴

W (y + h, σ (y)) ≤ Tw(y + h) = W (y + h, σ (y + h))

∀h ∈ G

It then follows that for all h ∈ G,
Tw(y + h) − Tw(y) ≥ W (y + h, σ (y)) − W (y, σ (y)) = U (y − σ (y) + h) − U (y − σ (y))
Take hn ∈ G, hn > 0, hn ↓ 0. Since hn > 0, we have
U (y − σ (y) + hn ) − U (y − σ (y))
Tw(y + hn ) − Tw(y)
≥
hn
hn

∀n ∈

Let DTw+ denote the right derivative of Tw, which exists by concavity of Tw. Taking
limits gives DTw+ (y) ≥ U (y − σ (y)).
Now take hn ∈ G, hn < 0, hn ↑ 0. Since hn < 0, we get the reverse inequality
Tw(y + hn ) − Tw(y)
U (y − σ (y) + hn ) − U (y − σ (y))
≤
hn
hn

∀n ∈

and taking limits gives DTw− (y) ≤ U (y − σ (y)). Thus
DTw− (y) ≤ U (y − σ (y)) ≤ DTw+ (y)
But concavity of Tw and lemma A.2.41 imply that DTw+ (y) ≤ DTw− (y) also holds.
In which case the left and right derivatives are equal (implying differentiability of Tw
at y), and their value is U (y − σ (y)).

352

Appendix B

Proof of proposition 12.1.23. Fix y > 0. Let k∗ := σ (y) be optimal investment, and let
v := v∗ be the value function. In light of proposition 12.1.17 we have k∗ < y. Let’s
assume for now that
h(k) := U (y − k) + ρ



v( f (k, z))φ(dz)

is differentiable on [0, y), and that
h ( k ) = −U ( y − k ) + ρ



U ◦ c( f (k, z)) f (k, z)φ(dz)

(B.15)

(If k = 0, then by differentiability we mean that the right-hand derivative h+ (0) exists,
although it is permitted to be +∞.)
The inequality (12.3) is then equivalent to h (k∗ ) ≤ 0. This must hold because k∗
is a maximizer and k∗ < y, in which case h (k∗ ) > 0 is impossible. Thus it remains
only to show that h is differentiable on [0, y), and that h is given by (B.15). In view of
corollary 12.1.19, it sufﬁces to show that
h ( k ) = −U ( y − k ) + ρ



∂
v( f (k, z))φ(dz)
∂k

(0 < k < y )

The only difﬁcultly in the preceding set of arguments is in showing that
d
dk



v( f (k, z))φ(dz) =

To see this, deﬁne
g(k) :=





∂
v( f (k, z))φ(dz)
∂k

v( f (k, z))φ(dz)

(B.16)

( k > 0)

and consider the derivative at ﬁxed k > 0. Let h0 < 0 be such that k + h0 > 0. For all
h > h0 it is not hard to show that
"
 !
v( f (k + h, z)) − v( f (k, z))
g(k + h) − g(k)
=
φ(dz)
h
h
Since k → v( f (k, z)) is concave for each z, the inequality
v( f (k + h0 , z)) − v( f (k, z))
v( f (k + h, z)) − v( f (k, z))
≤
:= M(z)
h
h0
holds for all z (see exercise A.2.42 on page 337). The function M is bounded and
therefore φ-integrable. As a result the dominated convergence theorem implies that

Chapter Appendixes

353

for hn → 0 with hn > h0 and hn = 0 we have
"
 !
v( f (k + hn , z)) − v( f (k, z))
g(k) = lim
φ(dz)
n→∞
hn
!
"

v( f (k + hn , z)) − v( f (k, z))
=
lim
φ(dz)
n→∞
hn

∂
v( f (k, z))φ(dz)
=
∂k
The last equality is due to the fact that k → v( f (k, z)) is differentiable in k for each
z.
Proof of proposition 12.1.24. Starting with the second claim, in the notation of the proof
of proposition 12.1.23, we are claiming that if h (k∗ ) < 0, then k∗ = 0. This follows
because if k∗ > 0, then k∗ is interior, in which case h (k∗ ) = 0.
The ﬁrst claim will be established if, whenever f (0, z) = 0 for each z ∈ Z, we have
0 < σ (y) for all y > 0. (Why?) Suppose instead that σ(y) = 0 at some y > 0, so
v(y) = U (y) + ρ



v[ f (0, z)]φ(dz) = U (y)

where we have used U (0) = 0. Deﬁne also
vξ := U (y − ξ ) + ρ



(B.17)

v[ f (ξ, z)]φ(dz)

(B.18)

where ξ is a positive number less than y. Using optimality and the fact that U (y) =
v(y), we get
0≤


v(y) − vξ
U (y) − U (y − ξ )
v[ f (ξ, z)]
φ(dz)
=
−ρ
ξ
ξ
ξ

∀ξ < y

Note that the ﬁrst term on the right-hand side of the equal sign converges to the ﬁnite
constant U (y) as ξ ↓ 0. We will therefore induce a contradiction if the second term
(i.e., the integral term) converges to plus inﬁnity. Although our simple version of the
monotone convergence theorem does not include this case, it is sufﬁcient to show that
the integrand converges to inﬁnity as ξ ↓ 0 for each ﬁxed z; interested readers should
consult, for example, Dudley (2002, thm. 4.3.2).5 To see that this is so, observe that for
any z ∈ Z,
lim
ξ ↓0

v[ f (ξ, z)]
v[ f (ξ, z)] f (ξ, z)
U [ f (ξ, z)] f (ξ, z)
= lim
≥ lim
→∞
ξ
f (ξ, z)
ξ
f (ξ, z)
ξ
ξ ↓0
ξ ↓0

We have used here the fact that v ≥ U pointwise on S.
are using the fact that the integrand increases monotonically as ξ ↓ 0 for each ﬁxed z, as follows
from concavity of f in its ﬁrst argument, and the fact that the value function is increasing.
5 We

354

Appendix B

Proof of lemma 12.1.28. Set w1 := (U ◦ c)1/2 , as in the statement of the proposition. We
have
"1/2

 !
f ( σ ( y ), z )
w1 [ f (σ(y), z)]φ(dz) =
φ(dz)
U ◦ c[ f (σ (y), z)]
f ( σ ( y ), z )
To break up this expression, we make use of the fact that if g and h are positive real
functions on S, then by the Cauchy–Schwartz inequality (Dudley 2002, cor. 5.1.4),
1/2



1/2
( gh) dφ ≤
g dφ · h dφ
(B.19)
It follows that


w1 [ f (σ (y), z)]φ(dz)
!
"1/2 !
U ◦ c[ f (σ(y), z)] f (σ (y), z)φ(dz)
≤

1
φ(dz)
f ( σ ( y ), z )

"1/2

Substituting in the Euler equation, we obtain
!
"1/2
"1/2 !

U ◦ c(y)
1
w1 [ f (σ (y))z]φ(dz) ≤
φ(dz)
ρ
f ( σ ( y ), z )
Using the deﬁnition of w1 , this expression can be rewritten as
!
"1/2

1
w1 [ f (σ (y))z]φ(dz) ≤
φ(dz)
w1 ( y )
ρ f ( σ ( y ), z )
From assumption 12.1.27 one can deduce the existence of a δ > 0 and an α1 ∈ (0, 1)
such that
"1/2
!
1
φ(dz)
< α1 < 1 for all y < δ
ρ f ( σ ( y ), z )

∴



w1 [ f (σ(y), z)]φ(dz) ≤ α1 w1 (y)

On the other hand, if y ≥ δ, then


w1 [ f (σ (y), z)]φ(dz) ≤ β 1 :=



(y < δ)

w1 [ f (σ (δ), z)]φ(dz)

The last two inequalities together give the bound


w1 [ f (σ (y), z)]φ(dz) ≤ α1 w1 (y) + β 1

This completes the proof of lemma 12.1.28.

(y ∈ S)

Chapter Appendixes

355

Proof of lemma 12.2.17. Our ﬁrst observation
 is that if w is any continuous and increasing function on + with Nw(y) = w( f (y, z))φ(dz) < ∞ for every y ∈ + ,
then Nw is also continuous and increasing on + . Monotonicity of Nw is obvious. Continuity holds because
 if yn → y, then (yn ) is bounded by some ȳ, and
w( f (yn , ·)) ≤ w( f (ȳ, ·)). As w( f (ȳ, z))φ(dz) < ∞, the dominated convergence theorem gives us limn→∞ Nw(yn ) → Nw(y).
A simple induction argument now shows that Nt U is increasing and continuous
on + for every t ≥ 0. The fact that κ is monotone increasing on + is immediate
from this result, given that κ = ∑t δt Nt U. (Why?) Continuity of κ on + can be
established by corollary 7.3.15 on page 182. Take yn → y and note again the existence
of a ȳ such that yn ≤ ȳ for every n ∈ . Thus δt Nt U (yn ) ≤ δt Nt U (ȳ) for every n and
every t. Moreover δt Nt U (yn ) → δt Nt U (y) as n → ∞ for each t. Corollary 7.3.15 now
gives
lim κ (yn ) =

n→∞

∞

∞

t =0

t =0

δt Nt U (yn ) = ∑ δt Nt U (y) = κ (y)
∑ nlim
→∞

Proof of lemma 12.2.20. Pick v ∈ bκ cS, ( x, u) ∈ gr Γ and ( xn , un ) → ( x, u). Let v̂ :=
v + vκ κ. Observe that v̂ is both continuous and nonnegative. Let v̂k be a sequence
of bounded continuous nonnegative functions on S with v̂k ↑ v̂ pointwise. (Can you
give an explicit example of such a sequence?) By the dominated convergence theorem,
for each k ∈ we have


lim inf
n

v̂[ F ( xn , un , z)]φ(dz) ≥ lim inf



n

v̂k [ F ( xn , un , z)]φ(dz) =

Taking limits with respect to k gives


lim inf
n

v̂[ F ( xn , un , z)]φ(dz) ≥

It follows that
ĝ( x, u) :=







v̂k [ F ( x, u, z)]φ(dz)

v̂[ F ( x, u, z)]φ(dz)

v̂[ F ( x, u, z)]φ(dz)

is lower-semicontinuous (lsc) on gr Γ. And if ĝ is lsc, then so is
g( x, u) :=



v[ F ( x, u, z)]φ(dz)


as g( x, u) = ĝ( x, u) − vκ κ [ F ( x, u, z)]φ(dz).
Since v was an arbitrary element of bκ cS, and since −v is also in bκ cS, we can also
conclude that − g is lsc on gr Γ—equivalently, g is usc on gr Γ (recall exercise 3.1.15 on
page 40). But if g is both lsc and usc on gr Γ, then g is continuous on gr Γ, as was to be
shown.

Bibliography
Adda, J., and R. Cooper. 2003. Dynamic Economics: Quantitative Methods and Applications. Cambridge: MIT Press.
Aiyagari, S. R. 1994. Uninsured idiosyncratic risk and aggregate saving. Quarterly Journal of
Economics 109: 659–684.
Aliprantis, C. D., and K. C. Border. 1999. Inﬁnite Dimensional Analysis. New York: Springer.
Aliprantis, C. D., and O. Burkinshaw. 1998. Principles of Real Analysis. London: Academic Press.
Alvarez, F., and N. L. Stokey. 1998. Dynamic programming with homogeneous functions.
Journal of Economic Theory 82: 167–89.
Amir, R. 1997. A new look at optimal growth under uncertainty. Journal of Economic Dynamics
and Control 22: 67–86.
Amir, R. 2005. Supermodularity and complementarity in economics: An elementary survey.
Southern Economic Journal 71: 636–60.
Amir, R., L. J. Mirman, and W. R. Perkins. 1991. One-sector nonclassical optimal growth: Optimality conditions and comparative dynamics. International Economic Review 32: 625–44.
Amman, H. M., D. A. Kendrick, and J. Rust, eds. 1990. Handbook of Computational Economics.
Burlington, MA: Elsevier.
Angeletos, G-M. 2007. Uninsured idiosyncratic investment risk and aggregate saving. Review
of Economic Dynamics 10: 1–30.
Aruoba, S. B., J. Fernàndez-Villaverde, and J. Rubio-Ramírez. 2006. Comparing solution methods for dynamic equilibrium economies. Journal of Economic Dynamics and Control 30: 2447–508.
Azariadis, C. 1993. Intertemporal Macroeconomics. New York: Blackwell.
Azariadis, C., and A. Drazen. 1990. Threshold externalities in economic development. Quarterly
Journal of Economics 105: 501–26.
Barnett, W. A., and A. Serletis. 2000. Martingales, nonlinearity, and chaos. Journal of Economic
Dynamics and Control 24: 703–24.
Bartle, R., and D. Sherbet. 1992. Introduction to Real Analysis. New York: Wiley.

357

358

Bibliography

Becker, R. A., and J. H. Boyd. 1997. Capital Theory, Equilibrium Analysis and Recursive Utility.
New York: Blackwell.
Bellman, R. E. 1957. Dynamic Programming. Princeton: Princeton University Press.
Benhabib, J., and K. Nishimura. 1985. Competitive equilibrium cycles. Journal of Economic
Theory 35: 284–306.
Benveniste, L. M. , and J. A. Scheinkman. 1979. On the differentiability of the value function in
dynamic models of economics. Econometrica 47: 727–32.
Bertsekas, D. P. 1995. Dynamic Programming and Optimal Control. New York: Athena Scientiﬁc.
Bewley, T. 2007. General Equilibrium, Overlapping Generations Models, and Optimal Growth Theory.
Cambridge: Harvard University Press.
Bhattacharya, R. N., and O. Lee. 1988. Asymptotics of a class of Markov processes which are
not in general irreducible. Annals of Probability 16: 1333–47.
Bhattacharya, R., and M. Majumdar. 2007. Random Dynamical Systems: Theory and Applications.
Cambridge: Cambridge University Press.
Böhm, V., and L. Kaas. 2000. Differential savings, factor shares, and endogenous growth cycles.
Journal of Economic Dynamics and Control 24: 965–80.
Boldrin, M., and L. Montrucchio. 1986. On the indeterminacy of capital accumulation paths.
Journal of Economic Theory 40: 26–39.
Boyd, J. H. 1990. Recursive utility and the Ramsey problem. Journal of Economic Theory 50:
326–45.
Breiman, L. 1992. Probability. SIAM Classics in Applied Mathematics, Philadelphia: SIAM.
Bremaud, P. 1999. Markov Chains. New York: Springer.
Brock, W. A., and L. J. Mirman. 1972. Optimal economic growth and uncertainty: The discounted case. Journal of Economic Theory 4: 479–513.
Brock, W. A. 1982. Asset prices in a production economy. In Economics of Information and Uncertainty. J. J. McCall, ed. Chicago: University of Chicago Press, pp. 1–43.
Brock, W. A., and C. H. Hommes. 1998. Heterogeneous beliefs and routes to chaos in a simple
asset pricing model. Journal of Economic Dynamics and Control 22: 1235–74.
Canova, F. 2007. Methods for Applied Macroeconomic Research. Princeton: Princeton University
Press.
Caputo, M. R. 2005. Foundations of Dynamic Economic Analysis: Optimal Control Theory and Applications. Cambridge: Cambridge University Press.
Chan, K. S., and H. Tong. 1986. On estimating thresholds in autoregressive models. Journal of
Time Series Analysis 7: 179–90.
Chatterjee, P., and M. Shukayev. 2008. Note on a positive lower bound of capital in the stochastic growth model. Journal of Economic Dynamics and Control 32: 2137–47.

Bibliography

359

Chiarella, C. 1988. The cobweb model its instability and the onset of chaos. Economic Modelling
5: 377–84.
Christiano, L. J., and J. D. M. Fisher. 2000. Algorithms for solving dynamic models with occasionally binding constraints. Journal of Economic Dynamics and Control 24: 1179–232.
Coleman, W. J. 1990. Solving the stochastic growth model by policy-function iteration. Journal
of Business and Economic Statistics 8: 27–29.
Datta, M., L. J. Mirman, O. F. Morand, and K. Reffett. 2005. Markovian equilibrium in inﬁnite horizon economies with incomplete markets and public policy. Journal of Mathematical
Economics 41: 505–44.
Deaton, A., and G. Laroque. 1992. On the behavior of commodity prices. Review of Economic
Studies 59: 1–23.
Dechert, W. D., and S. I. O’Donnell. 2006. The stochastic lake game: A numerical solution.
Journal of Economic Dynamics and Control 30: 1569–87.
Den Haan, W. J., and A. Marcet. 1994. Accuracy in simulations. Review of Economic Studies 61:
3–17.
Dobrushin, R. L. 1956. Central limit theorem for nonstationary Markov chains. Theory of Probability and its Applications 1: 65–80.
Doeblin, W. 1938. Exposé de la theorie des chaîns simples constantes de Markov à un nombre
ﬁni d’états. Revue Mathematique de l’Union Interbalkanique 2: 77–105.
Donaldson, J. B., and R. Mehra. 1983. Stochastic growth with correlated production shocks.
Journal of Economic Theory 29: 282–312.
Dudley, R. M. 2002. Real Analysis and Probability. Cambridge: Cambridge University Press.
Dufﬁe, D. 2001. Dynamic Asset Pricing Theory. Princeton: Princeton University Press.
Durlauf, S. 1993. Nonergodic economic growth. Review of Economic Studies 60: 349–66.
Durrett, R. 1996. Probability: Theory and Examples. New York: Duxbury Press.
Ericson, R., and A. Pakes. 1995. Markov-perfect industry dynamics: A framework for empirical
work. Review of Economic Studies 62: 53–82.
Farmer, R. E. A. 1999. The Macroeconomics of Self-Fulﬁlling Prophecies. Cambridge: MIT Press.
de la Fuente, A. 2000. Mathematical Methods and Models for Economists. Cambridge: Cambridge
University Press.
Galor, O. 1994. A two-sector overlapping generations model: A global characterization of the
dynamical system. Econometrica 60: 1351–86.
Gandolfo, G. 2005. Economic Dynamics: Study Edition. New York: Springer.
Glynn, P. W., and S. G. Henderson. 2001. Computing densities for Markov chains via simulation. Mathematics of Operations Research 26: 375–400.
Gordon, G. J. 1995. Stable function approximation in dynamic programming. Mimeo. Carnegie–
Mellon University.

360

Bibliography

Grandmont, J-M. 1985. On endogenous competitive business cycles. Econometrica 53: 995–1046.
Green, E. J., and R. H. Porter. 1984. Noncooperative collusion under imperfect price information. Econometrica 52: 87–100.
Greenwood, J., and G. W. Huffman. 1995. On the existence of nonoptimal equilibria. Journal of
Economic Theory 65: 611–23.
Grüne, L., and W. Semmler. 2004. Using dynamic programming with adaptive grid scheme for
optimal control. Journal of Economic Dynamics and Control 28: 2427–56.
Häggström, O. 2002. Finite Markov Chains and Algorithmic Applications. Cambridge: Cambridge
University Press.
Hall, R. E. 1978. Stochastic implications of the life cycle-permanent income hypothesis: Theory
and evidence. Journal of Political Economy 86: 971–87.
Hamilton, J. D. 2005. What’s real about the business cycle? Federal Reserve Bank of St. Louis
Review. July–August: 435–52.
Heer, B., and A. Maussner. 2005. Dynamic General Equilibrium Modelling. New York: Springer.
Hernández-Lerma, O., and J. B. Lasserre. 1996. Discrete Time Markov Control Processes: Basic
Optimality Criteria. New York: Springer.
Hernández-Lerma, O., and J. B. Lasserre. 1999. Further Topics on Discrete Time Markov Control
Processes. New York: Springer.
Hernández-Lerma, O., and J. B. Lasserre. 2003. Markov Chains and Invariant Probabilities. Boston:
Birkhäuser.
Holmgren, R. A. 1996. A First Course in Discrete Dynamical Systems. New York: Springer.
Hopenhayn, H. A. 1992. Entry, exit, and ﬁrm dynamics in long run equilibrium. Econometrica
60: 1127–50.
Hopenhayn, H. A., and E. C. Prescott. 1992. Stochastic monotonicity and stationary distributions for dynamic economies. Econometrica 60: 1387–1406.
Huggett, M. 1993. The risk-free rate in heterogeneous-agent incomplete-insurance economies.
Journal of Economic Dynamics and Control 17: 953–969.
Huggett, M. 2003. When are comparative dynamics monotone? Review of Economic Dynamics 6:
1–11.
Jones, G. L. 2004. On the Markov chain central limit theorem. Probability Surveys 1: 299–320.
Judd, K. L. 1992. Projection methods for solving aggregate growth models. Journal of Economic
Theory 58: 410–52.
Judd, K. L. 1998. Numerical Methods in Economics. Cambridge: MIT Press.
Kamihigashi, T. 2007. Stochastic optimal growth with bounded or unbounded utility and with
bounded or unbounded shocks. Journal of Mathematical Economics 43: 477–500.
Kamihigashi, T., and J. Stachurski. 2008. Asymptotics of stochastic recursive economies under
monotonicity. Mimeo. Kyoto University.

Bibliography

361

Kandori, M., G. J. Mailath, and R. Rob. 1993. Learning, mutation and long run equilibria in
games. Econometrica 61: 29–56.
Kendrick, D. A., P. R. Mercado, and H. M. Amman. 2005. Computational Economics. Princeton:
Princeton University Press.
Kikuchi, T. 2008. International asset market, nonconvergence, and endogenous ﬂuctuations.
Journal of Economic Theory 139: 310–34.
Kolmogorov, A. N. 1955. Foundations of the Theory of Probability. Chelsea, NY: Nathan Morrison.
Kolmogorov, A. N., and S. V. Fomin. 1970. Introductory Real Analysis. New York: Dover Publications.
Krebs, T. 2004. Non-existence of recursive equilibria on compact state spaces when markets are
incomplete. Journal of Economic Theory 115: 134–50.
Kristensen, D. 2007. Geometric ergodicity of a class of Markov chains with applications to time
series models. Mimeo. University of Wisconsin.
Krusell, P., and A. Smith. 1998. Income and wealth heterogeneity in the macroeconomy. Journal
of Political Economy 106: 867–96.
Krylov, N., and N. Bogolubov. 1937. Sur les properties en chaine. Comptes Rendus Mathematique
204: 1386–88.
Kubler, F., and K. Schmedders. 2002. Recursive equilibria in economies with incomplete markets. Macroeconomic Dynamics 6: 284–306.
Kydland, F., and E. C. Prescott. 1982. Time to build and aggregate ﬂuctuations. Econometrica 50:
1345–71.
Langtangen, H. P. 2008. Python Scripting for Computational Science. New York: Springer.
Lasota, A. 1994. Invariant principle for discrete time dynamical systems. Universitatis Iagellonicae Acta Mathematica 31: 111–27.
Lasota, A., and M. C. Mackey. 1994. Chaos, Fractals and Noise: Stochastic Aspects of Dynamics.
New York: Springer.
Le Van, C., and R-A. Dana. 2003. Dynamic Programming in Economics. New York: Springer.
Le Van, C., and Y. Vailakis. 2005. Recursive utility and optimal growth with bounded or unbounded returns. Journal of Economic Theory 123: 187–209.
Light, W. 1990. Introduction to Abstract Analysis. Oxford, UK: Chapman and Hall.
Lindvall, T. 1992. Lectures on the Coupling Method. New York: Dover Publications.
Ljungqvist, L., and T. Sargent. 2004. Recursive Macroeconomic Theory. Cambridge: MIT Press
Long, J., and C. Plosser. 1983. Real business cycles. Journal of Political Economy 91: 39–69.
Lovejoy, W. 1987. Ordered solutions for dynamic programs. Mathematics of Operations Research
12: 269–76.
Lucas, R. E., Jr., and E. C. Prescott. 1971. Investment under uncertainty. Econometrica 39: 659–81.

362

Bibliography

Lucas, R. E., Jr. 1978. Asset prices in an exchange economy. Econometrica 46: 1429–45.
Maliar, L., and S. Maliar. 2005. Solving nonlinear dynamic stochastic models: An algorithm
computing value function by simulations. Economics Letters 87: 135–40.
Marcet, A. 1988. Solving nonlinear models by parameterizing expectations. Mimeo. Carnegie
Mellon University.
Marimon, R., and A. Scott, eds. 2001. Computational Methods for the Study of Dynamic Economies.
Oxford: Oxford University Press.
Matsuyama, K. 2004. Financial market globalization, symmetry-breaking, and endogenous inequality of nations. Econometrica 72: 853–84.
McCall, J. J. 1970. Economics of information and job search. Quarterly Journal of Economics 84:
113–26.
McGrattan, E. R. 2001. Application of weighted residual methods to dynamic economic models.
In Computational Methods for the Study of Dynamic Economies. R. Marimon and A. Scott, eds.
Oxford: Oxford University Press, pp. 114–43.
McLennan, A., and R. Tourky. 2005. From imitation games to Kakutani. Mimeo. University of
Melbourne.
Medio, A. 1995. Chaotic Dynamics: Theory and Applications to Economics. Cambridge: Cambridge
University Press.
Mehra, R., and E. C. Prescott. 1985. The equity premium: A puzzle. Journal of Monetary Economics 15: 145–61.
Meyn, S. P., and R. L. Tweedie. 1993. Markov Chains and Stochastic Stability. London: Springer.
Miao, J. 2006. Competitive equilibria of economies with a continuum of consumers and aggregate shocks. Journal of Economic Theory 128: 274–98.
Miranda, M., and P. L. Fackler. 2002. Applied Computational Economics and Finance. Cambridge:
MIT Press.
Mirman, L. J. 1970. Two essays on uncertainty and economics. PhD Thesis, University of
Rochester.
Mirman, L. J., 1972. On the existence of steady state measures for one sector growth models
with uncertain technology. International Economic Review 13: 271–86.
Mirman, L. J. 1973. The steady state behavior of a class of one sector growth models with
uncertain technology. Journal of Economic Theory 6: 219–42.
Mirman, L. J., and I. Zilcha. 1975. On optimal growth under uncertainty. Journal of Economic
Theory 11: 329–39.
Mirman, L. J., O. F. Morand, and K. L. Reffett. 2008. A qualitative approach to Markovian
equilibrium in inﬁnite horizon economies with capital. Journal of Economic Theory 139: 75–98.
Mirman, L. J., K. Reffett, and J. Stachurski. 2005. Some stability results for Markovian economic
semigroups. International Journal of Economic Theory 1: 57–72.

Bibliography

363

Mitra, T., and G. Sorger. 1999. Rationalizing policy functions by dynamic optimization. Econometrica 67: 375–92.
Nirei, M. 2008. Aggregate ﬂuctuations of discrete investments. Mimeo. Carleton University.
Nishimura, K., G. Sorger, and M. Yano. 1994. Ergodic chaos in optimal growth models with
low discount rates. Economic Theory 4: 705–17.
Nishimura, K., and J. Stachurski. 2005. Stability of stochastic optimal growth models: A new
approach. Journal of Economic Theory 122: 100–18.
Norris, J. R. 1997. Markov Chains. Cambridge: Cambridge University Press.
Nummelin, E. 1984. General Irreducible Markov Chains and Nonnegative Operators. Cambridge:
Cambridge University Press.
Ok, E. A. 2007. Real Analysis with Economic Applications. Princeton: Princeton University Press.
Olsen, L., and S. Roy. 2006. Theory of stochastic optimal growth. In Handbook of Optimal Growth,
vol. 1. C. Le Van, R-A. Dana, T. Mitra and K. Nishimura, eds. New York: Springer, pp. 297–335.
Pakes, A., and P. McGuire. 2001. Stochastic algorithms, symmetric Markov perfect equilibria
and the curse of dimensionality. Econometrica 69: 1261–81.
Pollard, D. 2002. A User’s Guide to Measure Theoretic Probability. Cambridge: Cambridge University Press.
Prescott, E. C., and R. Mehra. 1980. Recursive competitive equilibrium: The case of homogeneous households. Econometrica 48: 1365–79.
Puterman, M. 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming. New
York: Wiley.
Quah, D. T. 1993. Empirical cross-section dynamics in economic growth. European Economic
Review 37: 426–34.
Razin, A., and J. A. Yahav. 1979. On stochastic models of economic growth. International Economic Review 20: 599–604.
Reffett, K., and O. F. Morand. 2003. Existence and uniqueness of equilibrium in nonoptimal
unbounded inﬁnite horizon economies. Journal of Monetary Economics 50: 1351–73.
Rios-Rull, V. 1996. Life-cycle economies with aggregate ﬂuctuations. Review of Economic Studies
63: 465–90.
Rincon-Zapatero, J. P., and C. Rodriguez-Palmero. 2003. Existence and uniqueness of solutions
to the Bellman equation in the unbounded case. Econometrica 71: 1519–55.
Roberts, G. O., and J. S. Rosenthal. 2004. General state space Markov chains and MCMC algorithms. Probability Surveys 1: 20–71.
Rockafellar, R. T. 1970. Convex Analysis. Princeton: Princeton University Press.
Rogerson, R., R. Shimer, and R. Wright. 2005. Search-theoretic models of the labor market: A
survey. Journal of Economic Literature 43: 959–88.

364

Bibliography

Rosenthal, J. S. 2002. Quantitative convergence rates of Markov chains: A simple account.
Electronic Communications in Probability 7: 123–28.
Rust, J. 1996. Numerical dynamic programming in economics. In Handbook of Computational
Economics. H. Amman, D. Kendrick, and J. Rust, eds. Burlington, MA: Elsevier, pp. 619–729.
Sargent, T. J. 1987. Dynamic Macroeconomic Theory. Cambridge: Harvard University Press.
Santos, M. S., and J. Vigo-Aguiar. 1998. Analysis of a numerical dynamic programming algorithm applied to economic models. Econometrica 66: 409–26.
Santos, M. 1999. Numerical solutions of dynamic economic models. In Handbook of Macroeconomics, vol. 1A. J. B. Taylor and M. Woodford, eds. Burlington, MA: Elsevier, pp. 311–86.
Scheinkman, J. A., and J. Schectman. 1983. A simple competitive model with production and
storage. Review of Economic Studies 50: 427–41.
Schilling, R. L. 2005. Measures, Integrals and Martingales. Cambridge: Cambridge University
Press.
Shiryaev, A. N. 1996. Probability. New York: Springer.
Shone, R. 2003. Economic Dynamics: Phase Diagrams and their Economic Application. Cambridge:
Cambridge University Press.
Stachurski, J. 2002. Stochastic optimal growth with unbounded shock. Journal of Economic Theory 106: 40–65.
Stachurski, J. 2003. Economic dynamical systems with multiplicative noise. Journal of Mathematical Economics 39: 135–52.
Stachurski, J., and V. Martin. 2008. Computing the distributions of economics models via simulation. Econometrica 76: 443–50.
Stachurski, J. 2008. Continuous state dynamic programming via nonexpansive approximation.
Computational Economics 31: 141–60.
Samuelson, P. A. 1971. Stochastic speculative price. Proceedings of the National Academy of Science
68: 335–7.
Sorger, G. 1992. On the minimum rate of impatience for complicated optimal growth paths.
Journal of Economic Theory 56: 160–79.
Stokey, N. L. 2008. The Economics of Inaction. Princeton: Princeton University Press.
Stokey, N. L., and R. E. Lucas, with E. C. Prescott. 1989. Recursive Methods in Economic Dynamics.
Cambridge: Harvard University Press.
Sundaram, R. K. 1996. A First Course in Optimization Theory. Cambridge: Cambridge University
Press.
Tauchen, G., and R. Hussey. 1991. Quadrature-based methods for obtaining approximate solutions to nonlinear asset pricing models. Econometrica 59: 371–96.
Taylor, J. C. 1997. An Introduction to Measure and Probability. New York: Springer.

Bibliography

365

Tesfatsion, L., and K. L. Judd, eds. 2006. Handbook of Computational Economics, Volume 2: AgentBased Computational Economics. Burlington, MA: Elsevier.
Topkis, D. 1998. Supermodularity and Complementarity. Princeton: Princeton University Press.
Torres, R. 1990. Stochastic dominance. Mimeo. Northwestern University.
Turnovsky, S. 2000. Methods of Macroeconomic Dynamics. Cambridge: MIT Press.
Uhlig, M. 2001. A toolkit for analysing nonlinear dynamic stochastic models easily. In Computational Methods for the Study of Dynamic Economies. R. Marimon and A. Scott, eds. Oxford: Oxford
University Press, pp. 30–62.
Venditti, A. 1998. Indeterminacy and endogenous ﬂuctuations in a two-sector optimal growth
model with externalities. Journal of Economic Behavior and Organization 33: 521–42.
Williams, D. 1991. Probability with Martingales. Cambridge, UK: Cambridge University Press.
Williams, N. 2004. Small noise asymptotics for a stochastic growth model. Journal of Economic
Theory 119: 271–98.
Williams, J. C., and B. C. Wright. 1991. Storage and Commodity Markets. Cambridge: Cambridge
University Press.
Zelle, J. M. 2003. Python Programming: An Introduction to Computer Science. Wilsonville, OR:
Franklin Beedle and Associates.
Zhang, Y. 2007. Stochastic optimal growth with a non-compact state space. Journal of Mathematical Economics 43: 115–29.

Index
A \ B, 318
B(; x ), 39
D (S), 185
D (; x ), 42
L1 , 184
Px ∧ Px , 265
S+ , S− , etc., 252
T, Bellman op., 235
Tσ , 235
A , B , etc., 321
, 317
, 317
Σ, set of policies, 232
, 317
B (S), B ( ), etc., 165
C ibcS, 299
δx , δz , etc., 78, 167
-ball, 39
∃, 320
∀, 320
γ( x, x ), 277
gr Γ, 230
=⇒ , 319
κ-bounded, 306
L , Lebesgue measurable sets, 162
L1 , 178
μ-a.e., 181
μ ( x, x , dy), 277
μ( f ), λ( f ), etc., 177
μ( x, x , dy), 277
μ+ , μ− , etc., 252

ν( x, x , dy), 277
P (S), 68, 167
P(U ), P(S), etc., 318
σ-algebra, 164
σ (C ), σ (O ), etc., 164
bU, bS, etc., 37
bS, 256
bM (S), 252
bS , bB (S), etc., 174
bκ S, 306
bκ B (S), 306
bκ cS, 306
bcU, bcS, etc., 46
d FM , 256
d∞ , 38
dκ , 306
f ◦ g, 321
f + , f − , etc., 178
ibcS, 255
mS , mB (S), etc., 174
mS + , 174
rσ , 232
sS , sB (S), etc., 172
sS + , sB (S)+ , etc., 172
w-greedy, 102, 134, 233, 311
x ∨ y, 331
x ∧ y, 331
45 degree diagram
deterministic model, 57
stochastic model, 188
Additivity, 161
367

368
Adherence, 41
Afﬁnity
between densities, 201
between measures, 264
Almost everywhere, 181
Aperiodicity, 96, 292
AR(p) model, 221
Atom, 276
Attractor, 56, 62
Attribute, 19
Banach’s ﬁxed point theorem, 53
Banach, S., 51
Bellman equation, 102, 134, 234, 311
Bellman operator, 102, 134, 235, 311
Bellman, R., 99
Berge’s theorem, 340
Bifurcation diagram, 65
Bijection, 321
Binding, 17
Blackwell’s condition, 149, 307
Bolzano–Weierstrass thm., 47, 329
Borel measurable, 174
Borel sets, 165
Boundedness
in , 327
in metric space, 39
break, 23
Brouwer’s ﬁxed point thm., 52
Brouwer, L.E.J., 51
C language, 15
__call__, 33
Carathéodory’s condition, 162
Cardinality, 322
Cartesian product, 318
Cauchy sequence, 44, 328
Central limit theorem
IID , 122, 256
Markov, 292, 294

Index
Change-of-variable, 194
Chaotic dynamics, 62
Chapman–Kolmogorov eq., 79
Chebychev’s inequality, 214
class, 30
Class, Python, 29
Closed ball, 42
Closed set, 41
Closure, 43
Compact-valued, 339
Compactness
and ﬁxed points, 52, 54
and optima, 49
and small sets, 293
and the Borel sets, 165
deﬁnition, 46
in ( k , d2 ), 48
of state space, 259
Complement, 318
Completeness
deﬁnition, 44
of (C , d∞ ), 147
of (P (S), d TV ), 254
of ( k , d2 ), 45
of (bcU, d∞ ), 46
of (bU, d∞ ), 45
of ( D (S), d1 ), 185
of ( L1 (λ), d1 ), 185
of , 328
Concatenation, 27
Concavity, 295, 336
Condition M, 266
Conditional probability, 325
Constructor, 30
Continuity
in metric space, 40
of correspondences, 339
of real functions, 334
open sets deﬁnition, 43
Continuum, 323

Index
Contracting, 52
Contrapositive, 320
Convergence
almost sure, 247
in , 327
in metric space, 38
in probability, 248
pointwise, 46, 307
setwise, 255
uniform, 46
weak, 255
with prob. one, 247
Convex
function, 336
set, 52
Coordination game, 110
Correlated shocks, 221, 231, 262
Correspondence, 339
Countable, 322
Countable additivity, 163
Countable cover, 161, 169
Counting measure, 167, 177
Coupling, 272
ﬁnite case, 112
inequality, 114, 273
time, 114
Covariance, 213
Credit constraints, 266
Cython, 16
de Morgan’s laws, 318
Decreasing
function, 295
set, 295
def, 25
Density, 185
existence of, 217
expectation w.r.t., 217
kernel, see Stochastic kernel
Dictionary, Python, 27

369
Differentiability, 336
Diminishing returns, 4, 304
Dirac probability, 167
Distance, 36
Distribution
ﬁnite, 68
function, 170
joint, 80
marginal, 72, 76, 118, 223
of a random variable, 212
stationary, see Stationary
unconditional, 72
Dobrushin coefﬁcient
and coupling, 276
density case, 201
ﬁnite case, 89
measure case, 265
Doc string, 26
Doeblin, W., 112, 294
Dominated convergence thm., 182
Drift condition
density case, 204
for monotone systems, 283
measure case, 260
to a small set, 291
DS (class name), 62
Dynamical system, 55
Empirical distribution, 122, 131, 270
Equivalence class, 184
Equivalence of metrics, 49
Euclidean distance, 35
Euler (in)equality, 302
Expectation
ﬁnite, 326
general, 212
Expression, Python, 21
Feasible policy, see Policy
Feller property, 258

370
Financial markets, 266
First passage time, 125
Fitted value iteration, 135, 244
Fixed point, 51, 55
for, 23
Fortet–Mourier distance, 256
Fréchet, M., 54
Fubini’s theorem, 215
Function
mathematical, 320
Python, 25
Generating class, 164, 175
Geometric drift, see Drift cond.
Greedy policy, see w-greedy
Hahn–Jordan theorem, 252
Hamilton’s kernel, 70
Hartman–Grobman theorem, 67
Hausdorff, F., 36, 54
Heine–Borel thm., 48
Hemicontinuity, 339
Homeomorphism, 67
Identiﬁer, 17
IDLE, 17
if/else, 21
IID , 216
Image (of a set), 320
Image measure, 182, 212
Immutable, 27
Imperfect markets, 268
Inada condition, 284, 304
Increasing
correspondence, 295
function, 58, 295
set, 280, 295
Increasing differences, 297
Independence, 215
Indicator function, 321
Inﬁmum

Index
of a subset of , 332
of two measures, 264
__init__, 32
Instance, 29
Integrable function, 178
Interior, 43
Interior Point, 41
Intermediate value thm., 335
Intersection, 318
Invariant
point, 55
set, 55
Inventory dynamics, 92
Inverse function, 321
IPython, 17
Irreducibility, 96, 292
Iterate, 55
Iterated function system, 109
Jordan decomposition, 252
Krylov–Bogolubov thm., 259
Lagrange stability, see Stability
lambda, 26
Law of large numbers
correlated, 250
IID , 94, 249
Markov, density, 206
Markov, ﬁnite, 94, 251
Markov, general, 265, 292
Law of Total Probability, 325
Lebesgue
integral, 177, 178
measurable set, 162
measure, 162, 169
outer measure, 161
Left derivative, 336
Liminf, 333
Limit
in , 327

Index
in metric space, 38
Limsup, 333
Linear systems, 59
Linearization, 66
LinInterp (class name), 138
Lipschitz function, 256
List comprehension, 23
List, Python, 18
LLN, see Law of large numbers
Logic, 319
Look-ahead estimator
marginal distribution, 128, 141
stationary distribution, 131
Loop
for, 13, 23
while, 12, 22
Lower bound, 332
Lower-semicontinuity, 40
Markov chain
density case, 188
ﬁnite, 71
general case, 219
periodic, 88
Markov operator
continuity w.r.t. d FM , 258
contraction properties, 89, 201, 265
density case, 195
ﬁnite state, 75, 78
for SRS, 226
general case, 224, 225
math, Python module, 19
MATLAB, 16
Matplotlib, 21, 62
Matsuyama, K., 266
Maximizer, 49
Maximum, 49
MC (class name), 72
Measurable
function, 174, 176

371
selection, 234
set, 164
space, 164
Measure, 166
probability, 167
signed, 252
Measure space, 166
Method, 29
Metric, 36
Metric space, 36
Minimizer, 49
Minimum, 49
Mixing, 200, 202
Module, 19
Monotone convergence thm., 182
Monotone sequence, 327
Monotone subseq., 329
Monotonicity
and stability, 279
in parameters, 297
of an SRS, 280
of functions, 58
of measures, 166
of optimal policies, 299
of the Bellman operator, 236
Mutable, 27
Namespace, 20
Negative set, 252
Nonconvex growth model
density kernel, 194
deterministic, 57
simulation, 124
stability, 209
Nonexpansiveness, 52
and approximation, 136, 244
of M w.r.t. d1 , 85, 196
of M w.r.t. d TV , 253
Norm, 37
Norm-like function, 204, 259

372
NumPy, 21
Object, Python, 17, 29
Object-oriented programming, 29
One-to-one, 321
Onto, 321
Open ball, 39
Open cover, 47
Open set, 41
Optimal growth
concavity of v∗ , 300
drift condition, 303
monotonicity, 297, 299
optimization, 230, 234
outline, 133
stability, 302, 305
uniqueness of policy, 301
Optimal policy
deﬁnition, 102, 134, 233
monotonicity of, 299
Order inducing, 282
Order mixing, 281, 283
Order norm-like, 282
Outer measure, 169
Parametric continuity, 339
Parametric monotonicity, 298
Path dependence, 58, 111
Policy
feasible, 232
interiority, 301
optimal, see Optimal policy
savings, 1, 133
stationary Markov, 231
Policy iteration, 105, 142, 241
Positive set, 252
Pre-measure, 169
Precompactness
and Lagrange stability, 58
and norm-like functions, 204

Index
and Prohorov’s theorem, 257
deﬁnition, 46
of densities, 204
Preimage
deﬁnition, 320
rules concerning, 321
Probability
ﬁnite, 324
measure, 167
Probability space, 167
Prohorov’s theorem, 256
Pseudocode, 11
Pseudometric space, 184
Pylab, 62
PyX, 21
Quadratic map, 62
Quah’s kernel, 71
random, Python module, 20
Random variable, 212
Real
numbers, 327
sequence, 327
Regeneration, 272
return, 26
Return time, 91
Reward function, 230
Riemann integral, 171, 178
Right derivative, 336
Sage, 16
Scheffés identity, 186
SciPy, 21
Script, 19
SDP, 229
self, 32
Semi-ring, 169
Sequence, 38, 327
Signed measure, see Measure
Simple function, 171, 172

Index
Small set, 290
Solow–Swan
density kernel, 193
deterministic, 57
simulation, 118
stationary distribution, 261
stochastic kernel, 220
stochastic stability, 283
SRS, see Stochastic recursive seq.
SRS (class name), 118
Stability
global, 56, 88, 90, 198
Lagrange, 58, 203
local, 56
Stationary
density, 129, 197
distribution, 85, 257
point, 55
Stochastic kernel
density, 126, 188
ﬁnite, 68
for SRS, 220
general case, 218
higher order, 76, 196, 225
Stochastic process, 216, 247
Stochastic recursive sequence
and density kernel, 189
canonical, 219
ﬁnite case, 107
on , 117
simulation of, 118
Strict concavity, 295, 301
Strictly increasing diff., 297
String, 18
Sub-additivity, 162, 166
Subsequence
in , 329
in metric space, 39
Supremum, 332

373
Taylor expansion, 66
Threshold autoregression
density kernel, 193
stability, 208
STAR, 118, 208, 290, 293
Tightness
of densities, 200, 203
of measures, 257, 259
Topological conjugacy, 67
Total variation norm, 253
Trajectory, 55
Transition function, 230
Triangle inequality, 36, 183
Tuple, Python, 27
Unbounded rewards, 306
Uncountable, 322
Uniform integrability, 203
Uniformly contracting, 52
Union, 318
Unit simplex, 68
Unpacking, 18
Upper bound, 332
Upper-semicontinuity, 40
Value function
concavity of, 299
deﬁnition, 102, 134, 233
differentiability of, 301
monotonicity of, 296
Value iteration, 103, 239
Variable, Python, 17
Variance, 213
Vector, 35
Vectorized function, 21
Weak convergence, see Convergence
Weierstrass theorem, 49
Weighted supremum norm, 306
while, 22

