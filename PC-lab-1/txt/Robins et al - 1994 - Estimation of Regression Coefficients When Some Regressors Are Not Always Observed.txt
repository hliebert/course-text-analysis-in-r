Estimation of Regression Coefficients When Some Regressors Are Not Always Observed
Author(s): James M. Robins, Andrea Rotnitzky and Lue Ping Zhao
Source: Journal of the American Statistical Association, Vol. 89, No. 427 (Sep., 1994), pp.
846-866
Published by: Taylor & Francis, Ltd. on behalf of the American Statistical Association
Stable URL: https://www.jstor.org/stable/2290910
Accessed: 19-02-2019 22:46 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

American Statistical Association, Taylor & Francis, Ltd. are collaborating with JSTOR to
digitize, preserve and extend access to Journal of the American Statistical Association

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Estimation of Regression Coefficients When Some
Regressors Are Not Always Observed
James M. ROBINS, Andrea ROTNITZKY, and Lue Ping ZHAO*

In applied problems it is common to specify a model for the conditional mean of a response given a set of regressors. A subset of

the regressors may be missing for some study subjects either by design or happenstance. In this article we propose a new class of
semiparametric estimators, based on inverse probability weighted estimating equations, that are consistent for parameter vector a0
of the conditional mean model when the data are missing at random in the sense of Rubin and the missingness probabilities are
either known or can be parametrically modeled. We show that the asymptotic variance of the optimal estimator in our class attains
the semiparametric variance bound for the model by first showing that our estimation problem is a special case of the general problem
of parameter estimation in an arbitrary semiparametric model in which the data are missing at random and the probability of
observing complete data is bounded away from 0, and then deriving a representation for the efficient score, the semiparametric

variance bound, and the influence function of any regular, asymptotically linear estimator in this more general estimation problem.

Because the optimal estimator depends on the unknown probability law generating the data, we propose locally and globally adaptive
semiparametric efficient estimators. We compare estimators in our class with previously proposed estimators. We show that each
previous estimator is asymptotically equivalent to some, usually inefficient, estimator in our class. This equivalence is a consequence
of a proposition stating that every regular asymptotic linear estimator of a0 is asymptotically equivalent to some estimator in our
class. We compare various estimators in a small simulation study and offer some practical recommendations.

KEY WORDS: Cox proportional hazards model; Linear regression; Logistic regression; Measurement error; Missing covariates;
Missing data; Nonlinear regression; Semiparametric efficiency; Survey sampling; Two-stage case-control studies;
Validation study.

implies that iri may depend on subject i's observed data,

1. INTRODUCTION

including Yi, but not on the missing data. Restriction (b)
In applied problems it is common to specify a model
guarantees the existence of n1 /2-consistent estimates of aO.
g(X*; a) for the conditional mean of the response Yi of a
Restrictions (a)-(c) plus the conditional mean model
subject i given a set of regressors X*, where a is an unknown
g(X*; a) constitute a semiparametric model for the data.
parameter vector and g(X*; a) is a known function such
Define a semiparametric estimator to be one that is guaras a'X* or [1 + exp {-a'X* }]-. A subset Xi of the reanteed to be asymptotically normal and unbiased for aO ungressors X* (X', V')' may be missing for some study
der the sole restrictions imposed by the model. A second
subjects, either by design or by happenstance. For example,
goal of this article is to show that the optimal estimator in
Xi may be very expensive to measure and thus can be obour class has the minimum possible asymptotic variance
tained only on a subsample of subjects, the validation sample.
among all regular semiparametric estimators of ao. That is,
As a second example, Xi may represent responses to a set of
the asymptotic variance of the optimal estimator attains the
personal questions that some subjects may refuse to answer.
semiparametric variance bound in the sense of Begun et al.
If the probability iri that Xi is completely observed depends
(1983). Regularity is a technical condition that prohibits
only on the vector Vi of other regressors, asymptotically unsuper-efficient estimators by specifying that the convergence
biased estimates of the true value a0 of a may be obtained
of the estimator to its limiting distribution is locally uniform.
by a complete case analysis; that is, by the (possibly nonlinear,
A third goal is to compare estimators in our class with
possibly weighted) least squares regression of Yi on X*
previously proposed estimators, many of which were reamong subjects with complete data. If iri depends on both
viewed by Little (1993). Even when iri depends only on Vi,
Yi and Vi, then the complete case estimator may be inconthe
optimal estimator in our class is more efficient than the
sistent.
complete case estimator, because the optimal estimator exOne goal of this article is to propose a new class of estitracts information available from subjects with incomplete
mators, based on inverse probability weighted estimating
data. In this setting, Dagenais (1973), Gourieroux and
equations, that are asymptotically normal and unbiased for
- Montfort (1 98 1), Beale and Little (1 975), Pepe and Fleming
ao when (a) the data are missing at random in the sense of
(1991), and Carroll and Wand (1991) have previously proRubin (1976), (b) iri is bounded away from 0, and (c) the
posed estimators that extract information from subjects with
iri are either known (as in a designed study) or can be paraincomplete data. The Dagenais (1973), Beale and Little
metrically modeled as in Rosenbaum (1987). Restriction (a)
(1975), and Gourieroux and Montfort (1981) estimators assumed that g(X*; a) was a'X*, whereas the Pepe and
* James M. Robins is Professor of Epidemiology and Biostatistics, and
Fleming
(1991) and Carroll and Wand (1991) estimators
Andrea Rotnitzky is Assistant Professor of Biostatistics, Harvard School of
Public Health, Boston, MA 02115. Lue Ping Zhao is Associate Professor,
allowed for nonlinear regression functions. In Sections 4 and
Department of Epidemiology, Fred Hutchinson Cancer Center, Seattle, WA
6 we show that these previous estimators are asymptotically
98104. The research was partially supported by National Institutes of Health
equivalent
to inefficient estimators in our class.
Grants 2 P30 ES00002, 1-R29-GM48704-OIAI, ROIAI32475, RO1ES03405, K04-ESOO1 80, and GM-29745. The authors are grateful to Mark
Van der Laan, Richard Gill, Whitney Newey, Fushing Hsieh, Daniel Rabinowitz, Donald Blevins and Sander Greenland for their help. Andrea Rotnitzky was additionally supported in part by a Mellon Foundation Faculty
Award.

? 1994 American Statistical Association
Journal of the American Statistical Association
September 1994, Vol. 89, No. 427, Theory and Methods

846

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 847

by design, on a known
complex
function
of pre-treatment
When Yi is Bernoulli, Vi is discrete,
andbutiri
may
depend
predictor
variables (Robins, Mark,
and Newey 1992;proRobins
on both Yi and Vi, we show that
estimators
previously
posed by Horvitz and Thompson (1952), Manski and Lerand Morgenstern 1987).
man (1977), Manski and McFadden (1981), Cosslett (1981),
In Section 6 we show that we can improve on the efficiency
Kalbfleisch and Lawless (1988), Breslow and Cain (1988),
of inefficient estimators in our class by estimating the selecImbens (1992), Flanders and Greenland (1991), and Zhao
tion probabilities iri even when the iri are known. In Sections
and Lipsitz (1992) are, with the exception of efficient but
4 and 6 we consider the relationship between our estimators
computationally challenging estimator of Cosslett (1981),
and previously proposed estimators. In Sections 1-6.2, we
asymptotically equivalent to some inefficient estimator in
assume that the data vectors are independent and identically
our class. These equivalences are corollary to Proposition
distributed across subjects; in Section 6.3 we generalize our
3.2 of Section 3, which states that every regular asymptotiresults to include the nonindependent "fixed fraction" sam-

cally linear estimator of ao is asymptotically equivalent to
some estimator in our class, where an estimator a^ is asymp-

pling design considered by Breslow and Cain (1988). In Sec-

tions 1-6.3, we assume that the iri are known as in a designed
totically linear if n1/2 ( a^ - ao) is asymptotically equivalent study. In Section 6.4 we allow the data to be missing by
to an average of independent and identically distributed ranhappenstance and assume a parametric model for the undom variables.

known rir. In Sections 1-6 we assume that Xi is complete

Breslow and Cain (1988), Flanders and Greenland (1991),

observed or completely unobserved. In Section 7 we gener-

Weinberg and Wacholder (1993), Zhao and Lipsitz (1992),

alize our previous results to arbitrary missing data patterns,

Carroll, Wang, and Wang (1993), and Carroll, Gail, and
Lubin (1993) have considered the estimation of logistic
regression models in two-stage case control designs. In Sec-

thereby allowing different components of Xi to be missing
on different subjects. In Section 8 we note that the semipara-

tion 6 we show that the optimal estimator in our class attains

the general problem of estimating the parameters of an ar-

the semiparametric variance bound in this class of designs

bitrary semiparametric model in the presence of data missing

even in the presence of the nonrandom differential mea-

at random. In Proposition 8.1, following Robins and Rot-

surement error considered by Carroll et al. (1993).

The article is organized as follows. In Section 2 we for-

metric problem that we are considering is a special case of

nitzky (1992), we provide representations for the efficient
score, the semiparametric variance bound, and the influence

malize our semiparametric model, propose a class of estifunction of any regular asymptotically linear estimator in
mators, study their performance in a small simulation study,this general estimation problem (provided that the probability
and provide some practical recommendations for applica7ri of observing complete data is bounded away from 0). We
tions. In Section 3 we show that the optimal estimator in

then prove our major propositions by specializing Proposi-

our class attains the semiparametric variance bound.

tion 8.1 to the semiparametric model characterized by the

In Sections 4 and 5 we note that the optimal estimator

depends on the true but unknown probability distribution

model g(X*; a) for the conditional mean of Yi given X*.
We conclude with a discussion of how our methods can be

generating the data and thus is not available for data analysis; extended to other semiparametric models, such as the Cox
we thus propose locally and globally adaptive semiparametric

proportional hazards model, with data missing at random.

efficient estimators.

If we impose additional mild regularity conditions, such

2. THE MODEL AND A CLASS OF ESTIMATORS

as bounds on higher order moments, then even when Yi has
2.1 The Regression Model
many continuous components and -xi is a complex function
of Yi, prior knowledge of the selection probabilities -xi can
We develop our results in the context of the following
be exploited to construct a locally semiparametric efficient
example modeled on a large ongoing epidemiologic study,
"inverse-probability weighted" estimator that should perform the Nurses Health Study (Stampfer et al. 1985). A prospective
well in moderate sized samples (in the sense that, under all
5-year follow-up study of 100,000 previously healthy women
data generating processes, i.e. probability laws, allowed by
was undertaken. At start of follow-up, a blood serum sample
our semiparametric model, the estimator will be approxiwas obtained from each study subject and frozen for later
mately normal and centered on ao); in contrast, due to the
analysis. A co-investigator wishes to study the effect of the
"curse of dimensionality," if the iri are completely unknown,antioxidants serum vitamin A (XI) and vitamin E (X2) reno estimator of ao exists with reasonable performance in the
corded at start of follow-up on the subsequent development
moderate sized samples occurring in practice. Yet, when data of coronary artery disease. Examples of possible outcome
are missing at random, the likelihood principle implies that variables Y of interest include (a) Y is a Bernoulli random
inference on ao should not depend on whether the -xi are
variable that takes the value 1 if a subject develops a myoknown or completely unknown. With -xi known, our inversecardial infarction (i.e., a heart attack) over the 5-year followprobability weighted estimators perform well but violate the up period and 0 otherwise; (b) Y = (Y1, . . ., Y5)' is a vector
likelihood principle; in contrast, globally efficient estimators of yearly electrocardiogram (EKG) results dichotomized as
which ignore prior knowledge concerning iri and maximize normal or abnormal, a multivariate discrete outcome; and
a smoothed version of the likelihood function do (approxi(c) Y = ( Y1,. .., Ys5)' is a vector of yearly measurements of
mately) satisfy the likelihood principle but perform poorly

the width in millimeters of a subject's left main stem coronary

in moderate samples. Analogous issues arise in randomized

artery based on a noninvasive dye study, a continuous mul-

studies in which the probability of the treatment depends,

tivariate outcome. Due to the high cost of laboratory analyses

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

848 Journal of the American Statistical Association, September 1994

and to the small amount of stored serum per subject, the

a second example, it is standard epidemiologic practice not

other investigators will only permit the stored serum of

to adjust in model (1) for any covariate Vt (e.g., serum cho-

roughly 2% of the study subjects to be thawed and assayed

lesterol) that is measured after start of follow-up (a) if the

for the exposures X1 and X2 of interest. On all subjects, error-

covariate is an intermediate variable on the causal pathway

prone estimates, V1 = (V 1, V12) ', of X = (X1, X2)' were

from vitamin exposure to disease (Y) or (b) more generally,

obtained based on the results of a dietary questionnaire. The

if the covariate is affected by vitamin exposure (even if not

subjects with X recorded are said to be validation sample

an intermediate variable) (Robins 1987; Robins, Blevins,

members. The investigator randomly selects subjects into

Ritter, and Wulfsohn 1992; Rosenbaum 1984; Weinberg

the validation sample with selection probabilities that may

1992). In the absence of missing data, according to Propo-

depend on the surrogates V1, other confounding factors V2

sition 3.1, efficient estimation of the parameters of model

such as age and race, and the outcome Y. Such selection

(1) will not use data on Vt. In the presence of missing data,

rules offer efficiency advantages by overrepresenting subjects

we show in Proposition 4.2 that it is useful to collect data

with extreme or rare values of Y, V1, and V2 in the validation

on the extraneous surrogates Vt, because when Vt is cor-

sample (Breslow and Cain 1988). We summarize the data

related with X conditional on (Y, V), data on Vt will increase

in the following notation:

the precision with which we can estimate the parameters of

n = total number of study subjects;

(1). We adopt the notation

Vt = extraneous surrogates; wt = (Y', Vt')';

Y= outcome variable;
X= true exposures;

W = (Wt', V')'; L = (W',X')'.

V1 = mismeasured surrogates for X;
V2 = a vector of confounding variables, such as age and
race, with first component the constant 1;

V= (VI, V2)'

Throughout we suppose that X is a continuous multivariate

exposure, although our results also hold for discrete X. But
Wmay have all continuous, all discrete, or mixed continuous

X (X', V')'; and

and discrete components. In particular, this implies that X

A= 1 if X is fully observed (validation sample) and A =

and its surrogate V1 may be measured on different scales.

0 otherwise.

2.3 The Missing Data Mechanism

Our goal is to estimate the parameters a0 of the regression
model for the conditional mean of Y,

Y= g(X*; ao) + e, E[elX*] = 0, (1)
where g(X*; a) is a known smooth function with dimension
equal to that of Y and a0 is a q vector of unknown parameters.
Under model (1), when Y is Bernoulli, it is common to take

We assume until Section 7 that with probability 1, either

X, and X2 are both observed or X1 and X2 are both unobserved. We suppose until Section 6.3 that (Ai, Xi, Wi), i
= 1, ..., n are independently and identically distributed
random vectors with i indexing study subjects. Because in
our study the selection probabilities did not depend on X,
we shall assume that

g(X*; a) = {l + exp(- a'X*)} -1 (2)

Pr[A = lIW,X] = Pr[A = 1IW], (3)

where the supposed
i subscript has been
suppressed. Equation (3) imwith a'X*= a 'X + a?V2 + a 'V1 . It is often
that

V1 is a pure surrogate for X in the sense
that
Y missing
and V1
are in the sense of Rubin (1976).
plies
that Xis
at random
independent given X and V2 and thus, for example, that a3
Write 7r(W) = Pr[ A = 1 I W]. Because the selection prob= 0 in model (2). But it might occasionally be inappropriate
abilities are under the control of the investigator in our exto impose the restriction a3 = 0 in (2) a priori, because V1
ample, we assume until Section 6.4 that
may happen to capture other aspects of the diet correlated

wr(w) is a known function. (4)

with X that need to be adjusted for in the analysis.
2.2 Extraneous Surrogates

Further, we assume that

r(W) > a > 0 with probability 1, (5)

In epidemiologic studies it is very common to obtain data

so that probability of being selected into the valid
selves, of no interest in the sense that the scientific goal re-is bounded away from 0. A semiparametric model is characterized both by the available data and by restrictions on
mains the estimation of the conditional mean of Ygiven X*
the joint distribution of that data. For each subject, we shall
rather than the conditional mean of Y given (X *, Vt). As
on additional extraneous surrogates vt that are, in them-

an example, suppose that at end of follow-up or at time of

call

diagnosis of myocardial infarction (whichever comes first),
all subjects are readministered the dietary questionnaire. Let
Vt be the vitamin A and E measurements based on the sec-

L

=

(W',

X')'

(6)

the full data and

ond questionnaire. We would not adjust for vt in our regres(A, Lobs) (7)
sion model (1), because the error in vt will likely be nonthe observed data, where, following Little and Rubin (1987),
random (i.e., differential) as the cases' (Y-= 1) concern over
their health status may make them more motivated than
Lobs is the observed component of L so Lobs = L if A = 1
controls (Y = 0) to accurately recall their dietary habits. As
and Lobs = Wif A - 0. Let the semiparametric model "full"

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 849

be characterized by restriction (1) and data (6). Let the semi-

b. The asymptotic variance of n12 {a&(h, 4) - a0o}

parametric model "obs" be characterized by (1), (3)-(5),

can be consistently estimated by { K( h) } -Q( h,

and data (7). We shall propose a class of semiparametric

K){k(h)'}J, where K(h) = -n- -' aDiD(a, h, ?)/d

estimators for the parameters a0 of model "obs." To motivate and Q(h,k)= n-' i Di (a, h, 0)Di (a, h, 0)'evaluated
these estimators, we first review well-known estimators of a0
at o(h, k).
in model "full."

The fundamental identity used in the proof of Proposi-

2.4 A Class of Estimators

tion 2.2 in Appendix B is E[D(h, 0)] = E[DF(h)] = 0 because, by (3) and (5), for any b(L), E[Ab(L)/r]
The estimators a^F( h) of a0 in model "full" will be indexed

by a q X t function of X*, h(X* ), satisfying the local iden-

tification condition E{h(X* )ag(X*; ao )/aa' } nonsingular,
with t the dimension of Y and q the dimension of a0. Here

= E[E{A I L}b(L)/ir] = E[b(L)].

2.5 A Simulation Study and Some Practical
Recommendations

and throughout a superscript F will denote estimators and

statistics based on the full data (6). Specifically, ^F( h) solves
To demonstrate various properties of the estimators in
O = BF(a(, h) n-1 1i Df(a, h) with
our class, we conducted three small simulation experiments.
We provide some practical advice for applications guided by
DF(a, h) = h(X*)e(a), e (a) = Y-g(X*; a). (8)
Our semiparametric estimators &(h, 0) of a0 in model
"obs" will depend on h(X*) and on an arbitrary fixed q X 1
function of w, +(w), satisfying Et[(W)' (W)] < oo. Let
r and 0 denote the random variables ir(W) and 0(W). Then

a&(h, k) solves 0 = D (a, h, c) a n-' Ii Di (a, h, + j, where
D(a, h, X) = ADF(a, h)/r -A(),

A(X) (, (/- r)01. (9)

the results of these experiments. As in example 2 of Pepe

and Fleming (1991), we generated for each of n subjects a
normally distributed exposure X - N(0, 1); a dichotomous
surrogate V1 = I[X + v > 0] with v - N(0, 1), v independent
of X, and I[A] = 1 if A is true and 0 otherwise; a nonrandom
intercept V2 1; and an outcome Y from the logistic model
(2) with a3 = 0, a2 =-1, and a 1 = 0, 1, or 2 depending on
the experiment. Subjects were randomly selected into the
validation sample with probability .10. The sample size n
was 2,000. In this section we assume that data on extraneous

Before studying the properties of a&(h, 0), it will be convesurrogates Vt were not generated, so W = (Y, V')'. Each
nient to review the well-known asymptotic properties of
experiment
is based on 1,000 replications. Rows 1-3 of Table
a&F(h). It will be useful to define an asymptotically linear
estimator (Newey 1990a). An estimator a' of a0 is asymptotically linear with influence function B if

n1/2(a^ _- ao) = n- /2 Bi + op(l),

1 provide Monte Carlo averages and estimated asymptotic

relative efficiencies (ARE) for three different estimators of

a1 in our class. The estimated ARE of any estimator 'a, is
calculated as the ratio of the Monte Carlo variance of the

semiparametric efficient estimator reported in row 3 to that

of ,1. Following Pepe and Fleming (1991), in all our analyses
E(B) = 0, E(B'B) < oo. If 'a is asymptotically linear, then
we
by the central limit theorem and Slutsky's theorem, n1 /2 ( a^assumed that it is known that V1 is a pure surrogate and
- ao) is asymptotically normal with mean 0 and variance

thus a3 = 0, so only a 1 and a2 are estimated. Thus a in

model (2) was redefined to be (a1, a2) ',so h(X*) is of length
E[BB']. Asymptotically linear estimators a (1) and a (2) with

2. The estimator a I(hFff, 0) of row 1 uses h(X*) equal to
in the sense that n 1a2(&1 - (2)) = op(l). Conversely, twoh'f(X-*) (1, X)' and 4'(W) 0. The choice of h'f was

the same influence function are asymptotically equivalent

motivated by the fact that (a) in the absence of missing data,

asymptotically linear estimators that are asymptotically
equivalent must have the same influence function. The following result is well known (see, for example, Manski 1988)

the choice h ff(X*) is efficient because a '((her) is maximum

under regularity conditions (1)-(9) provided in Appendix

0) is maximum likelihood were data on nonvalidation sub-

likelihood, and (b) in the presence of missing data, a, (h'e,

jects unavailable. When ir is constant, ?& (h ' , 0) is algebraically equivalent to the estimators of Manski and Lerman
Proposition 2.1. Under model "full" with probability
(1977), Kalbfleisch and Lawless (1988), and Manski and
approaching 1, there exists a unique solution a&F(h) to
McFadden (1981) discussed in Section 6.2. The estimator
IF(a, h) = 0 such that F( h) is asymptotically linear with inal(h'ff, ^hff) of row 2 uses Xh(W) = E[D (a, h) IW],
fluence function { K( h )}l-DF(fh ) where DF(h) )DF( ao,

B, which we henceforth assume to be true.

h) with K(h) - E[h(X*)ag(X*; ao)/da']. Further K(h)
=-~E[dDF(ao, h)/aat'].
The corresponding properties of e( h, q$) are as follows.

with

E(BIW= w) z AiBiI(IWi = W)/E A4I(Wi = w)

Proposition 2.2.

(10)

a. Under model "obs," with probability approaching 1

as the sample average of Bi among validation sample memthere exists a unique solution &e(h, 0) to D(ae, h, k)
bers for whom W1 = w and & = oi(heFff, 0). Reading from
- 0 such that mo(h, X) is asymptotically linear with inTable 1, we note that &1(heFff, Xhe) is more efficient than
fluence function {K(h)}'lAD(h, 1) where D(h, 7 )
&t (heFr, 0). This reflects the fact that according to Proposi= D(ato, h, 4i).

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

850 Journal of the American Statistical Association, September 1994
Table 1. Results of a Simulation Study

Monte Carlo average of &1 Estimated ARE of t1
a1

Row

Estimator

Previously

proposed

by

a1

0

1

2

0

1

2

1 &(h F4, 0) Manski and Lerman (1977) -.02 1.01 2.03 .24 .47 .76
Manski and McFadden (1981)
Kalbfleisch and Lawless (1988)

2

3

ef(hff,

&(hle,

ff)

hfr)*

-.01

.01

1.02

1.01

2.04

2.01

.34

1.00

.58

1.00

.84

1.00

4 a(hf, 0) Flanders and Greenland (1991) -.01 1.01 2.03 .34 .58 .84

5 aPFCW Pepe and Fleming (1991) .01 1.01 2.00 1.00 .97 .74
Carroll and Wand (1991)

6 aMM(qMM) Breslow and Cain (1988) -.01 1.02 2.03 .34 .58 .85
7
&(he,
0,
4/1)
.00
.65
8
&(hFf
0,
(2))
.00
1.33
9

&(h

F

0,

4(3))

.00

1.10

NOTE: The estimators in rows 7-9 use data on extraneous surrogates Vt.

* Semiparametric efficient estimator in model "obs" when W = (Y, V')'.

when Y is Bernoulli, estimation of the unknown function
tions 2.3 and 2.4, for a fixed h, the asymptotic variance of

a&I(h, 0) is minimized at X*(W) = E[DF(h)I W] and that
var(e I X *) in Equation (17) will be necessary.
a(h, X*) and a(h, X*) are asymptotically equivalent.
Comparing rows 2 and 3 shows that in the presence of

In certain instances one might choose to forego weighted
estimators. For example, suppose that in model "obs" ir is

missing data, h ff is no longer efficient. Rather, the optimal

constant, data on Vt were not obtained, V2 3 1, g(X*; ao)

h(X*), heff( X*), is characterized in Equations (26)-(29) in
Section 5.2 and depends both on the data through the surrogate V1 and on the probability law generating the data.

is linear in X and does not depend on the surrogate V1, and
Y is continuous and univariate. Thus

= Y, g(X*; ao) = ao0o + a',1X,
For example, in Section 5.3 we show that heff( X*) = (1, .1 Wt
X
+ (1 - . l)E(X I V))' when ao,I = 0. In Section 5.2 we describe
and ir is a constant p. ( 11)
how to compute the consistent estimator heff(X*) of
Then a reasonable practical approach to estimating ao is to
heff( X*) used in row 3. In Sections 4 and 5 we prove that
compute the parametric maximum likelihood estimator
al (heff, heff) attains the semiparametric variance bound for

our model "obs" and that &al(heff, *heff) is asymptotically (MLE), OaMLE, of ao in the fully parametric model that imposes the additional assumption that the joint distribution
equivalent to a&l(heff, heff) and is thus semiparametric effiof (e, X, V) was generated by the "normal-normal meacient. Rows 4-6 of Table 1, discussed in Section 6, compare

the performance of previously proposed inefficient estimators surement error model"
with that of the efficient estimator of row 3. The estimators

reported in rows 7-9 illustrate the efficiency advantage of
collecting data on extraneous surrogates and are discussed
in Section 6.4.
Practical Recommendations. The foregoing simulation
results help motivate the following practical advice. The es-

V - N(,u, 1), X I V - N(y'V, Q),

c I x, v - N(O, o-2), (12)
with (Au, 1, y, Q, a 2) unknown parameters to be estimated.
We show in Section 5.1 that 'MLE iS (a) asymptotically normal and unbiased for ao of (11) even if (12) is false and (b)

the semiparametric variance bound if (12) is true.
timator a( heff , 0) is simplest to compute. But if efficiencyattains
is

Result (a) depends critically on the linearity of g(X*; ao)
of concern, then one can compute a(heff, Whe) with only
in
(I 1).
slightly greater effort. If even further increases in efficiency
are required, then with some additional effort one can use
the formulas in Section 5.2 to compute the efficient estimator2.6 Efficiency for Fixed h
a( heff, Xheff). These recommendations assume that, as in our Proposition 2.3. For fixed h, the asymptotic variance of
simulation experiment, Wt = (Y', Vt')' was discrete. But
a(h, 4) is uniquely minimized at the asymptotic variance
when Wt has continuous components, estimation of the op- of a(h, oh) with
timal function heff( X * ) can be computationally challenging
requiring iterative calculations, as discussed in Section 4.2.
h E[DF(h)I W] (13)

Thus for nondiscrete Wt, although the estimator a&(heff,
Proof It follows from Proposition 2.2 that the oh minheff) provided in Section 4.2 can (with difficulty) be comimizing the asymptotic variance of a&(h, 0) minimizes the

puted, in practice the more easily computed estimator

variance of D(h, 4). Now, because ADF((a, h)/ir = DF((a,
a(hff, heFff ) will suffice when efficiency is not of major con-

h) + (A - vr)DF(a, h)/ir, we have

cern. Here heFff and X h are the generalizations given in Equation (17) and Section 2.7 of h ff and Xh defined earlier. Except

D(a, h, 4) = DF(a, h) + J(a, h, 4) (14)

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 851

where J(a, h, ) (A - )r)[DF(a, h) - 4]/r. Further,

3. ASYMPTOTIC EFFICIENCY AND

by (3) and (5), for all a, (A - 7r)[DF(a, h) - 0q]/ r has

ASYMPTOTIC EQUIVALENCE

mean 0 given (W, X) and thus is uncorrelated with

In this section we show that our class of estimators a^(h,

DF(a, h). Hence var[D((a, h, 5 )] = var[DF(a, h)] + E[(1
k) contains a member whose asymptotic variance attains
- ir)-'E{ [DF(a, h) - 4 ] 2 I W} ], which is minimized, in the semiparametric variance bound for model "obs" and

the positive definite sense, at = E[DF((a, h) I W], where
A 02 = AA'.

The decomposition (14) provides insight into our esti-

mation problem. Because J( a, h, k) has mean 0 for all a
(not just for ao) and is uncorrelated with the full data esti-

mating function DF( a, h), J( a, h, 0) cannot help in iden-

that any regular asymptotic linear estimator of ao has the

same influence function as a member of our class. We first
formally define the semiparametric variance bound for the
class of regular estimators following Begun et al. (1983),

Bickel, Klaassen, Ritov, and Wellner (1993), and Newey
(1990a). Suppose that the data consist of n independent re-

tification or estimation of ao. That is, as far as the estimation

alizations of a random variable Z. Let L (a, 0; Z) be the

of ao is concerned, J( a, h, X ) is just random noise added to

likelihood function for a single subject in a semiparametric

DF( a, h), representing the penalty we pay for not having

model indexed by a q-dimensional parameter a and a nui-

observed Xi and thus D (a, h) for all subjects i. The variance
sance parameter 0 taking values in an infinite-dimensional
of J( a, h, X) is a quantitative measure of this penalty that

set. Let (ao , 0O) index the distribution generating Z. For

is minimized at oh for fixed h.

example, in model "full" Z is L = (X', W')' and ?L(a, 0;
Z) = LF(a, 0; L), where

2.7 Adaptive Estimation of oh

LF(a, 0; L)= f V; 03)f(XI V; 02)f [e(a)I X, V; 01]

The estimator a^( h, X h) is not feasible, because X h depends

X f[VtIY, VJX;04] (16a)

on the unknown population quantity E[DF(h) I W]. Hence

0= (01, 02, 03, 04), restricted only by f t dF(t I X
we shall study the properties of estimators a^( h, X h),and
where

qh is an estimate of the unknown function oh computed as
follows. Suppose that we have specified a regression model

E(DF(h) I W) = I(W; Xo), (15)
where l(W; X) is a known regression function smooth in a

= 0 because the error has mean 0. In model "obs" Z

= (A, Lobs) and ?L(a, 0; Z) ? L(a, 0; A, Lobs) with
?L(a, 0; A, Lobs) = F(a, 0; X, W)A

x F LF(a, 0; Xi, X2, W) dx1 dx2} Xr (1 -r)lA

finite dimensional parameter X. We set 0 h (W) equal to l( W;
(16b)
A), where X is the possibly nonlinear least squares estimator
Define a regular parametric submodel to be a regular fully
of X0 from the regression of D( a^, h) on Wi in the validation

parametric model with parameters (a, 77) and likelihood
?L(a, i7; Z) with true values (ao, f7o), where the subprefix
refers
model in W; then *h(W) = E[DF(&a, h) I W], as defined
in to the fact that for each 71, the distribution L (a, n; Z)

sample and a^ = a&(h, 0) for some preliminary 4(W). For
W discrete, we take l(W; X0) to be a saturated regression

(10). Under the mild regularity conditions in Appendix B,

is a distribution L (a, 0; Z) allowed by the semiparametric

we prove the following proposition.

model. An estimator is regular in a regular parametric model

if locally it converges uniformly to its limiting distribution.
Proposition 2.4. Under model "obs," a&( h, X h) is asymp- A more precise definition of a regular estimator has been
totically equivalent to a&(h, Ot), where Ot( w) is the proba- given by Bickel et al. (1993) and Newey (1990a). An estimator is regular in a semiparametric model if it is regular in
bility limit of X h( w). The asymptotic variance of a( h, X h)
every regular parametric submodel. Because LeCam and
can be consistently estimated as in Proposition 2.2, with

Xh(W) in place of O(W).

Hajek proved that in a regular parametric model, the

Hence when (15) is correctly specified, kt( w) equals Oh(W)
Cramer-Rao variance bound for ao is a lower bound for th
and a&(h, h) is efficient for fixed h. If (15) is misspecified,
asymptotic variance of any regular estimator of ao0, it fol
that in a semiparametric model, the supremum of the
then a( h, h ) remains asymptotically normal and unbiased
Cramer-Rao bounds for ao over all regular parametric subfor ao. Although the asymptotics are the same, one can often
improve finite sample performance by using &(h h, )rather models is a lower bound for the asymptotic variance of any
than a&(h, Xh), where &(h, X h) is the limit of (i.e., iteratesregular estimator (Begun, Hall, Huang, and Wellner 1983;
Bickel et al. 1993; Newey 1990a). The supremum is referred
to convergence) the a^ ( (h, X h(J)) j = 1, 2, .. ., where
to
Me1l(h, h(1)) = o(h, Xh) and (j+ )(h, Xh(J+l)) is defined as the semiparametric variance bound of the semiparametric model. Since, by missing the at random assumption,
like a^(h, 5h) but with a in DT(a, h) equal to a^()(h,
4 (I)). Finally, when Whas only one or two continuousthe
comterm (1 - 7r) - is not contained within the integral in
(16b), it follows that, for likelihood-based inference conponents, we could have taken Xh(w) to be the predicted
value at w from a nonparametric (e.g., kernel or series
regression) of D_J ( I, h) on W1 in the validation sample.

cerning ao0, it does not matter whether ir is completely known
or unknown. Therefore, the semiparametric variance bound

for a0O in model "obs" is the same whether or not ir is known.
We now motivate our principal result for the missing data
which the resulting estimator of a0o is consistent, asymptot-

Newey (1 993a, 1 993b) provided regularity conditions under
ically normal.

case with the following full-data result.

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

852 Journal of the American Statistical Association, September 1994

Proposition 3.1. Suppose that aF is a regular, asymp-

set of functions of the observed data that have mean 0 con-

ditional on the full data.
totically linear (RAL) estimator of ao in the semiparametric

model "full." Then (a) its influence function lies in the set

{[K(h)] -lDF(h)}, and (b) there exists a unique h ff such

that {var[DF(heF)] }hF equals the semiparametric variance
bound. In addition, K(h ff) = var[DF(f f) I, so the asymp-

totic variance of a(heff) attains the bound.
Proposition 3.2. Suppose that a is a RAL estimator of
a0O in the semiparametric model "obs"; then (a) its influence

function lies in the set {[K(h)]-'D(h, k)}, and (b) there

4. EFFICIENCY CALCULATIONS
AND COMPARISONS

In Section 4.2 we derive heff and qeff, propose some adaptive locally semiparametric efficient estimators, and compare
the efficiency of our proposed estimators to that of some

previously proposed estimators. To motivate these results,
we first study the simpler problem of estimating the "best

linear predictor" of Y given X *.
exists a unique heff and 4eff such that {var[D(heff, Oeff)] } -1
equals the semiparametric variance bound for the model. In

4.1 Global and Local Efficient Estimation of Best
addition, K(heff) = var[D( heff, eff )] f, so the asymptotic variLinear Predictors
ance of A( 4eff heff) attains the bound.
The proofs of these propositions are deferred to Section
Suppose that Y is univariate and ao and X* are vectors
8. Proposition 3.1 is a corollary of results provided by Newey
of dimension q. Let i(X*) be the identity function; that is,

( 1 990a), Chamberlain ( 1987), and Newey and Powell ( 1990).

Proposition 3.2 implies that any RAL estimator a0O in model

i(X*) = X*. The asymptotic variance of a&(i, 4f') generally
will not attain the semiparametric variance bound in model

"obs" must be asymptotically equivalent to an estimator

"obs," because heff( X * ) will not equal i (X* ). Nevertheless,
&(h, X5) in our class. DF(heFff) and D(heff, 44wef) are called
the
the
asymptotic variance of &(i, Xi) attains the efficiency

efficient scores in their respective semiparametric models.
Chamberlain (1987) showed that

heFf(X* ) = [dg(X*; ao0)/dt] {var(ej X* )}1- (17)
in model "full." We derive heff and D5 Oefor model "obs" in

bound for estimation of the best linear predictor of Y given
X*. Formally, let models "full b" and "obs b" be defined
like their counterpart models "full" and "obs" with g(X*;

ao) = a 'X*, except that E[cIX*] = 0 is replaced by the

weaker condition that E[X*e] = 0. Then a' equals
the following section. Propositions 3.1 and 3.2 imply that
E[YX*'] { E[X*X*'] } -'. We call a 'X* the best linear prevar { DF[helf] } -var { D[herv, heff] } iS the information about
dictor because ao minimizes E[(Y - a'X*)(Y -a'X*)'].

aog lost due to missing data.
Model "full b" does not restrict the distribution of the dat
We next provide an interesting restatement of Proposition
(6) and thus can be viewed as "saturated." In the Appendix

3.2a. Consider any correctly specified model 7r(Df) for
the the following proposition.
we prove
known missingness process irnPr[ / = 11 W]; that is,
X= w(1/o), (18)

Proposition 4.1. The asymptotic variance of ai(i, X') attains the semiparametric variance bound for ao in model

"obs b."
with ir( + ) = r( W; + ) a smooth function of W and a fi
Bickel et al. (1993) independently derived the bound for
dimensional parameter i whose range lies in (O, 1P]. Let S
model "obs b" when ir( W) is constant. It follows from Prop= S( Coi) be the score for t evaluated at m. Note that Sfr th)
osition 2.4 that if Wi is discrete and we use (10), then
n logw[nL mis(g)e]/ po equals Ar(, Pr ), where
a&(i, 4i) is semiparametric efficient in model "obs b." If W,
has continuous components, then a&(i, &i) will be locally

ird logit =r(to)/d' (19)
Conversely, given +(W) taking values in R, A(?I) is S p in
the model logit 7r(i/) = logit ir + if/'q/r, with 4'o = 0. Furthermore, we noted previously that A ( ) has mean 0 given

(W, X). Conversely, according to Proposition 8.2, any ran-

semiparametric efficient in model "obs b" at restriction (15)
with h(X*) = i(X*), where we have used the following
definition due to Bickel et al. (1993).
Definition. Given a semiparametric model, say A, and

an additional restriction R on the joint distribution of the

data not imposed by the model, we say that an estimator a^
is locally semiparametric efficient in model A at R if a^ is a
with mean 0 conditional on the full data L = (W', X')' has
semiparametric estimator in model A whose asymptotic
the representation A (q5) for some function 4. Hence we obvariance attains the semiparametric variance bound for
tain the following result.
model A when R is true. Informally, a^ is the most efficient
dom variable B that is a function of the observed data (7)

Restatement of Proposition 3.2a. The influence function

of any RAL estimator ao in the missing data model "obs"
lies in the set whose elements are formed as follows. First,

possible estimator of ao when model A and restriction R are
both true that is guaranteed to remain asymptotically normal

take the influence function of an arbitrary RAL estimator

and unbiased for ao when A is true but R is false.
It is of interest to compare a&(i, Xi) to some previously

in the model "full," multiply by the indicator variable for

proposed estimator of ao in model "obs b." Gourieroux and

full (i.e., complete) data, and divide by the probability of

Montfort ( 198 1) discussed a semiparametric estimator of a0O
in model "obs b" when ir( W) was a constant p. They pro-

having full data; then add an arbitrary element from the set

posed computing a weighted regression of Y1 on (Xi, Vi ), i
of scores S* for correctly specified regular parametric models
= 1, . .., n, except they first estimated the unobserved Xi

for the known missingness process or, equivalently, from the

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 853

for nonvalidation sample subject i by its predicted value

- Vi and then used an estimated weight , W < 1, to down-

where t(X*) = {E[e'/Ir I X*] }1, r(B) is the operator
(1-7r) 7r E[BI W] and, again, qheff E[heff(X*)EI W].

weight the nonvalidation sample observations. Here y is the We use bold lowercase letters to denote operators.
ordinary least squares (OLS) estimator from the multivariate

Equation (23) is not directly useful for data analysis, be-

regression of X on V in the validation sample; that is, &GM

cause (a) the solution heff( X * ) is a function of the unknown
true distribution generating the data, and (b) even were the

(&GM, 1, a GM,2) solves

true distribution known, except in special cases such as those

O = /2 DGM,i(a ) = n1/2 AiX"i e(a)
i

discussed in Section 5, heff( X* ) will not exist in closed form

i

in the sense that it cannot be explicitly represented as a function of the true distribution. But as shown in Appendix A,

with a' = (al', a'). Dagenais (1973) and Beale and Little
(1976) proposed estimators that differ from the Gourieroux

and Montfort estimator only in the choice of weight W. In
Appendix C, we show that &GM has the same influence func-

tion as a( i, 5kGM) with

for any distribution allowed by model "full" with likelihood
LF(a, 0; L) given by (16a), Equation (23) can be solved
iteratively by the method of successive approximation. That

is, given the mth iterate hm (X* ), hm+I (X* ) is obtained by
evaluating the right side of (23) at hm(X* ), with expectations

computed with respect to LF('a, 0; L). hm(X* ) will converge
to the unique solution h(X*; a, 0) of (23) under LF(a, 0;

GM = {(yoV)'(Y- a,1yo V -a,2V), V'(Y
- a?,1yoV- a',2V)} {(1 - p) + pw-'}-', (21a)
where a'X* = a' = E[XV+ ]E(VV )} =

L) as m -> oo for any initial function ho(X*) (Kress 1989).
This implies a quite general approach to obtaining locally

efficient adaptive estimators in model "obs," as follows:

and w is the limit of ^. The influence function of &GM, in

contrast to that of a^(i, i'), is always linear in Yi . Thus one 1. Specify a fully parametric model L F(a, 7; X, W).
would not expect &GM to be efficient. In fact &GM wil be

efficient if and only if OGM = X'. But 9'3 E[X*eI W]'

2. Estimate q by 1 solving -x A7i Aa log LF(&a, q; X,
W) /77 = 0, with a^ a&(h, 0) a preliminary estimate of ao.

equals

3. Solve (23), by successive approximation if necessary,
and evaluate (24) under the law LF(a&, i; X, W) to obtain

({E(X I V, Y)Y -E(XX' | Y, V)ao,l l-E(X I V, Y)ao',2V } ',

heff = h(&^, r), keff = 0(, ^)-

V'(Y-ao',E(XI V, Y)-ao',2V)). (21b)

Then, by a proof analogous to that of Proposition 2.4 given

Comparing (21 a) and (2 1b), 5kGM = Xi if and only if (a) win Appendix B, a^(heff, keff) will be asymptotically equivalent
= 1, E[XI V, Y] = yoJV, and either ao,I = 0 or var[X I Y, V] to a&(ht, 4 t), where ht(X* ) and 4t(W) are the probability
= 0, or(b) = Oandvar(e) = 0.
Let a MLE be the MLE of ao = (a?',, ? a',2)' in the fully

parametric multivariate normal model defined by (12) and

Y = Wt, E(YIX*) = a?o, X + a0,2V,
ir is a constant p. (22)

limits of heff (X*) and eff( W) if h(X*; a, 11) and O(W; a,

71) have, for example, bounded second derivatives with re-

spect to (a, 71) with probability 1. In particular, if LF(a, n;
X, W) is correctly specified with true values (ao, o10), then
(a, 7) will be consistent for (ao, flo), ht = heff, and &t = keff.
Hence a&(heff, 5keff) is locally semiparametric efficient at the

parametric model L F( a, q; X, W). In fact, because V is
Results of Gourieroux and Montfort (1981) and Little (1993)
imply that & MLE is a semiparametric estimator in model "obs ancillary, ht = heff and X t = 4eff even if the model f( V; 113)
for the law of V is misspecified. In Appendix D we describe
b." Thus a^mLE is locally semiparametric efficient and
asymptotically equivalent to a^(i, Xi) at the joint restriction"simulation" estimators for the conditional expectations on
the right sides of Equations (23) and (24) that avoid the need
(12) and (22). ^ MLE will generally be inefficient and thus not
for numerical integration. In moderate sized samples, the
asymptotically equivalent to a( i, O') when either (12) or (22)
is false.

estimator a( heff 5keff ) should perform well (in the sense that
it will be approximately normal and centered on ao) provided

4.2 Functional Equations for heff and Oeff in Model the dimension of 7 is not too large.
"obs"

Equations (23) and (24) allow us to deduce conditions
under which data on an extraneous surrogate Vt that is not

In Section 8 we prove the following proposition.

adjusted for in model (1) will not provide additional infor-

Proposition 4.2. In model "obs," heff(X* ) is the unique
solution to the functional (integral) equation

mation about ao. Suppose that the selection probability ir(W)
does not depend on the extraneous surrogates Vt, so Vt
may be ignored without introducing bias. From (23) and

h(X*) = {ag(X*; ao)/a}at(X*)

(24), it follows that the efficient score D(heff, 5 eff) will depend

+ E[r {h(X*)e} e'I X*] t(X*) (23)

on Vt unless E[heff(X*)cI W] does not depend on Vt or ir
is equal to 1 almost surely. It follows that a sufficient condition for the efficient score not to depend on vt is that vt

and

and X are conditionally independent given (Y, V). Furkeff

-

~jffi

(24)

thermore, if W is a perfect surrogate for X in the sense that

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

854 Journal of the American Statistical Association, September 1994

W = c(W) with probability 1 for some fixed function c(.),

Hence if we obtain a closed-form expression for 4eff, (26)

then h' (X*) solves (23), a&(heF, a/jX) = &F(hF ) and

becomes a closed-form expression for heff( X* ). Now, after

asymptotically no information is lost due to missing data.

right multiplying Equation (26) by e and then taking conditional expectations given W, we obtain that 4eff solves

5. SPECIAL CASES WHERE heff AND 4eff EXIST
IN CLOSED FORM

m(W)

= X-E{E[(1 - r)Kr-i4'IX*]t(X*)e W}, (27)

5.1 Normal-Normal Measurement Error Model
Consider model "obs" satisfying (11) and suppose in truth

with m(W) E[ {ag(X*; ao)/aa} t(X* )e I W]. In Appendix
A we show that 4eff iS the unique solution to (27).

that (12) is satisfied for some unknown 77 = n70, where 77'When Wt is discrete, (27) is a finite-dimensional matrix
= (771', 72', 773), 17 = Or2772 = (,y Q), and n3 = (A, 1). Thenequation and admits a closed-form solution as follows. Dewith q replacing 0, let L (a, r; A, Lobs) be the missing-note the Ith components of the q vectors 4(W) and m(W)

data likelihood (16b) corresponding to the fully parametric

multivariate normal model defined by the restrictions of

(11 )-(12) and the model "obs." The parametric efficient
score for a in this fully parametric model is Sa,eff

Sa - E[SaS'] { E[S-,S'] }-1Sx, with Sa 8 log L(ao, no;

by 01(W) and m1(W). Write 01(W) 3I(Wt, V). Then for
Wt discrete with S levels, say wt, ..., wt, we will (in a
slight abuse of notation) define OI(V) to be the S vectorvalued function of Vwith sth component 01(wI , V). The S
vector m( V) is defined analogously in terms of mi( W). Then,

A, Lobs)/aa and S, 8 log L(ao, n0; A, Lobs)/an. Then
by (27), we obtain the closed-form solution
heff (X*)

keff,/(V) = { Isxs - q(V) } -lm1(V) (28)
=E[Sa,effI Y= 1, X*] -E[Sa,effI Y= O ,X*] (25)

where Isxs is the S X S identity matrix; q(V) is the S X S
matrix with k, s entry, qks(V), equal to

is a closed-form expression for heff( X * ) in terms of the pa-

E[Ej (1- 7r)7r-l I (Wt = wst)[Y -g(X5 V; ago)] ' JX, VI
rameters (ao, f70) of(1 1) and (12). We obtained (25) without
explicitly solving (23) by arguing as follows. Let (a&MLE,

nMLE) maximize ri L (a, n; Ai, Lobs,i). In Appendix C, we

X t(X, V){yk- g(X, V; ao)} I wt = Wk, V], (29)

prove that S MLE is a semiparametric RAL estimator of aoand
in ys and Yk are the realizations of Y corresponding to Wt
model "obs" of (11), even if (12) is false. Hence because it
is also a parametric MLE, a^MLE is locally semiparametric
efficient in model "obs" at restriction (12) with influence

= wt and Wt = wk.

Adaptive Estimation for Discrete Wt. When Wt is dis-

crete, adaptive local semiparametric efficient estimation
function E[Sa,effSa',eff] -1 Sa,eff. Equation (25) is then thebased
spe- on a parametric model ?F( a, 77; L) proceeds as in
cial case corresponding to Y univariate of part (b) of the
Section 4.2, except in Step 3 we now evaluate (28) and then
following corollary to Proposition 3.2a.
(26) under ?F(a^, 7i; L) to obtain keff and hef. But when V
Corollary 5.1. If B is the influence function of a RAL

estimator in model "obs," then B = D(h, 0) with (a) 0(W)
= b2(W), where b2(W) is determined by B through the
unique decomposition B = Ab1 (L) + (1 - A)b2(W), and

is also discrete, we modify Steps 1 and 2 in that we leave

f( X I V) completely unrestricted, so that 112 02 iS "infinite
dimensional"; we estimate f(X I V) by the nonparametric
unsmoothed estimator

(b) h(X*) = (h (X* ),. . . , ht(X* )), with hj(X*) = E[BIf(X
Y IV; 02) f(X I V; n2)
= ej, X *] -E[B I Y = 0, X*] and ej is the vector of the same
dimension t as Y, with the jth component equal to 1 and

- Ai'I (Vi = V)I (Xi = X)

all other components 0. Without loss of generality, we have

+ i ATiII(Vi = V); (30)

assumed that ej forj = (1, . . . , t) and the 0 vector are in the
support of Y.

But when E(YIX*) a a'(1, X')' and thus (11) is false,

and we estimate ii* = (71 5, qX)' by a"' solving
a log ?F(&, 112, n*; Li)/8n7* = 0. A mean value

a MLE( 1, X')' is not a consistent estimator of the best linear

predictor of Y given (1, X')'; that is, of E[Y(1, X')] E[(1,

X')'(1, X')]f'(1, X')'. Compare with the properties of
a MLE described in the final paragraph of Section 4.1.

shows that a( heff eff ) remains asymptotically equ
a&(ht, 0t); thus a(heff, kef) will attain the bound if

metric modelsf[elX, V; mq] andf[Vtl Y, X*; '74
rectly specified.

5.2 Wt Discrete

We next show that, as in our simulation experiment, if
wt = (Y', Vt')' is discrete, then heff(X*) always exists

in closed form. We first note that because 4eff

Suppose that, as in our simulation experiment, Y is Bernoulli and data on extraneous covariates Vt were not collected, so Y = Wt. Then Equation (1) completely specifies
the conditional law of Y given X* (X', V')', and the pa-

rameters 01 and O are not present in ?F(a, 0; L). Thus if
= E[heff( X* )C I W], Equation (23) can be rewritten as

Vis discrete, then &e( hef, kef) will be globally semiparametric

heff( X* ) = {8ag(X*; aot)/8t} t(X* )

+ E[(1 - l)-lKiefe'IX*] t(X* ). (26)

efficient, provided we estimate f( XI V) by (30). In fact, it

can be shown that &e( hef, kef) will be globally efficient even
if heff and kefs are obtained by estimating conditional ex-

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 855

pectations given W using Equation (10) rather than the law

LF(& w; L)A

h, 0) as an estimator of the unobservable "full" data esti-

mating function DF(a, h) = n-l >i h(X* )ei(a). Finally,

5.3 X Independent of Y Given V
When X is independent of Y given V and Y = Wt, it is

straightforward to check that (23) is solved by hind(X*)

= hF (X*)Cr2(V)t(V) + E[h F(X*) IV]{ 1 - o2(V)t(V)}

because n-1dD(ao, h, 0)/aa and n`'aD(ao, h, O)/aa both
converge to the same limit, - K(h), a(h, 0) will be more
efficient than a&(h, 0). The preference for a&(h, 0) can also
be viewed in terms of conditional bias rather than unconditional efficiency. Specifically, a&(h, 0), in contrast to a&(h,
0), is asymptotically biased conditional on the approximate

and fkhind(W) = E[h ff(X*) I V]e, where t(V) = E[ee'/
ancillary statistic n" /2(i- - r)/varA {n l/2(T _i - }).
i7rl V]-', 2(V)-E[ee' l V], and heff(X*) is given by (17).
Thus if g(X*; a) = l(ao + a1X + a2V) for known 1(*),

6.2 Comparison with Previously Proposed
then a( hindd, 'khind) will be locally semiparametric efficient
at
Estimators
the joint restriction that ao, I = 0 and that e is independent
6.2.1 Missing Completely at Random. A number of
of X given V.
alternative estimators of a0 in model "obs" have been pro-

6. ESTIMATION OF THE SELECTION PROBABILITIES,
COMPARISONS WITH PREVIOUS ESTIMATORS,
AND DATA MISSING BY HAPPENSTANCE
6.1 Estimation of Selection Probabilities
We will show that we can improve on the efficiency of

inefficient estimators in our class by estimating the selection

probabilities ir( W) even when they are known. Related phenomena have been noted by Robins and Morgenstern (1987),
Rosenbaum (1987), Lancaster (1990), Robins and Rotnitzky
(1992), Robins, Mark, and Newey (1992), and Imbens

(1992). Suppose that W' = (Wt', V') is discrete, and let

posed in the special case considered in our simulation ex-

periment, where Yis Bernoulli, Y = Wt, and ir is a constant
p. Pepe and Fleming (1991) and Carroll and Wand (1991)

proposed estimating ao by 'aPFCw that maximizes 1Ii ?(a,
0; Ai, Lobs,i) of (1 6b), except that a nonparametric estimate
calculated from the validation sample data is substituted for

f( X I V; 02). For V discrete, Pepe and Fleming proposed the
estimator (30). If V has continuous components, then the
kernel estimators proposed by Carroll and Wand are used.
In Appendix C we use Corollary 5.1 to show that, when g( *;

a) is logistic, aPFcW is asymptotically equivalent to

a&(hpFcw, 4PFCW), with 4PFCW = E[X*eI W], hpFcw(X
= pX* + (1-p){E[X*eI Y= 1, V]-E[X*e Y = 0, VI}.

X(W) 3 i AiI (W, = w)/ i I (WV = w) be the empirical
probability of selection into the validation sample given W

= w. Set gr = 7^(W), and let D(a, h, 0) = Ah(X*)e(a)/1
- (i\ - 7 / 7r. A corollary to Proposition 6.1 below is the

following.
Corollary 6.1. Under model "obs" with Wdiscrete, with

When Yis independent ofX given V, hpFcw and OPFCW

hind and 0khind of Section 5.3, and thus a&PFCw will be e
(Pepe and Fleming 199 1). Comparing rows 3 and 5 of Table

1 reveals that when a0, I 0, apFCW is inefficient. To understand why aPFCW is inefficient, suppose that X were also dis-

probability approaching 1 there exists a unique solution

crete, so that 02 in the modelf( X I V; 02) is a finite vector of
parameters. Then the MLE of a is obtained by maximizing

a(h, 0) to D(a, h, 4) n- ji Di(a, h, ) = 0. Further,
a(h, 0) is asymptotically equivalent to a^(h, h)*

MLE of 02 given a, obtained by maximizing (16b) over 02

As predicted by Corollary 6.1, reading from rows 2 and 4

for fixed a. 02(a) will depend on data from both the vali-

11i ?(a, 02(a); Ai, Lobs,i), where 02(a) is the restricted

of Table 1 we observe that &a j(h ff, 0) and a^, (h F, h.f) aredation and nonvalidation sample members and is efficient
essentially equally efficient in our simulation experiment.
for 02 when a = ao is known. In contrast, when ao,1 I 0,
Equation (30) is inefficient for 02. As a result, aPFCW is inefIndeed, Corollary 6.1 implies that a(h, 0) is always more
ficient for a0.
efficient than a^(h, 0) unless h = 4". In fact Corollary 6.1
implies that a(heff, 0) is a pseudo-complete-case estimator

that attains the efficiency bound, where we define a pseudocomplete case estimator of a0 to be an estimator that uses
the nonvalidation sample (i.e., the cases with incomplete

6.3 Missing at Random and Two-Stage CaseControl Designs

Until the final two paragraphs of this subsection, we restr

data) only to estimate the selection probabilities 7r( w). attention to the special case considered in Section 5.2 and
We now present a heuristic argument as to why replacing
our simulation experiment, in which Y = Wt, Yis Bernoulli,
the known probability ir by an estimate X can improve the and V is discrete. We allow selection to depend on W = (Y,
V')'. In this setting, Manski and Lerman (1977) and
efficiency of an inefficient estimator. Suppose that in the
Kalbfleisch
and Lawless (1988) generalized an idea of Horvitz
example of Section 2.1, there were 100 subjects with Wi = w
for a particular realization wand that 7r( w) = .5. Also suppose and Thompson (1952) and proposed the complete case esthat due to sampling variability, only 40 of the 100 subjects
were selected into the validation sample, so that 7i(w) = .4.

timator & ML( IML) solving E i Ai DML,i (IML, a) = 0, where for
any l(X*, a), DML(l, a) = I(X*, a)e(a)/ir and IML(X*,

Then using the complete case estimating equation 0 = D(a,

a)- {9g(X*; a)Id6a}[g(X*; a){1 -g(X*; a)}f-'.Flan-

ders and Greenland (1991) proposed the pseudo-completeh, 0) = n - Ei Aih(X* )ei (a)/iri is tantamount to assuming

that 40/.5 = 80 subjects had W, = w. On the other hand,
using D(a, h, 0) = n-l S Aih(Xiw )ci(ae)/X is tantamount
to correctly assuming that 40/.4 = 100 subjects had_ W1
= w. Therefore, D( a, h, 0) will be more precise than D( a,

case estimator axML(/ML) which replaces ir by the empirical
selection probability gr in the definition of DML( I, ae). It is
straightforward to show that &ML( I) and aeML(l) are asymptotically equivalent to t( h, 0) and t( h, 0), with h (X*)

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

856 Journal of the American Statistical Association, September 1994

= I(X*, ao). This becomes an exact algebraic equivalence

"obs-complete" by imposing the additional assumption that

when, as in our simulation study (row 4), g(X*; a) is logistic

the marginal distribution of W, f( W), is known. Then the

since then IML(X*, a) = X*. It follows that the Flanders-

efficiency bound in the two strengthened models will be

Greenland estimator aML( ML) can be made semiparametric

identical, because the nonvalidation sample data can be use-

efficient by replacing IML(X*, a) by heff(X* ) of Section 5.2.

ful only for estimatingf( J) butf( W) is now known. But

In the same setting, Manski and McFadden proposed the

Cosslett (1981, sec. 2.20) showed that aCssett, a MM( qMM)X

complete-case estimator aMM(qMM), where for any Q( a)

and a val, j = 1, 2, are asymptotically equivalent and semi-

= q(Y, X*, a), "aMM(q) solves 0 = ,i AiDMM,i(q, a), with

parametric efficient with ir either known or unknown in the

DMM(q, a) = { Q(a) - E[7rQ(a)IX*I/Ea[ rIX*I } and

strengthened model "obs-complete." Here aval maximizes

proposed the pseudo-complete-case estimator aMM( qMM)

mators must be efficient in the less restrictive model "obs."

that replaced 7r by 7r. As is evident from Table 1,

Weinberg and Wacholder (1992) obtained this result for X

qMM(Y, X*, a) = IML(X*, a)e((a). Breslow and Cain (1988) 1li A'LiF(a, 0; Li). Hence these same complete-case est

aMM(qMM) in row 6 is more efficient than aMM(qMM) in row

discrete. Finally, Cosslett (1981, sec. 2.21) proposed an es-

1, but neither is semiparametric efficient. In Appendix C we

timator based on a nonparametric maximum likelihood es-

prove that aMM(q) and aMM(q) are asymptotically equivalent

timate of the law of X given V that uses nonvalidation data

to a&(h, 0) and a&(h, 0), where h(X*) satisfies q(Y, X*, a)

and proved that this estimator is semiparametric efficient in

= i7r-lh(X*)e(a) or, equivalently,

h(X*)= {7r(1, V)q(1, X *, ao) - 7r(O, V)q(O, X *, ao)}

-{r(1, V) - -7r(O, V)}E[Q(ao)IA = 1,X*]. (31)
It follows that the Breslow-Cain estimator could have been

model "obs." Cosslett (1981) and Imbens (1992) noted that

this estimator may be difficult to compute, although Weinberg and Wacholder (1992) recently proposed using the EM

algorithm to simplify the computation.
Two-Stage Case-Control Studies. We now extend our
results to include two-stage case-control designs with g(X *;

made semiparametric efficient by replacing qMM(Y, X*, a)
a) logistic, Y Bernoulli. Data on extraneous surrogates vt

with r - 1heff ( X * )e( a ). Further, when, as in our simulation
may again be available. In the first stage, we select n subjects.
experiments, ir is constant, the Breslow-Cain and FlandersGreenland estimators are asymptotically equivalent. Zhao
and Lipsitz (1992) discuss estimators in the class aML(1)

and a^MM (q).
As mentioned in Section 3, the efficiency bound in model
"obs" is unchanged when the assumption (4) that 7r is known
is not imposed. Indeed, a(heff, O) attains the bound and yet

Specifically, for i = 1, .. ., n, with probability 0o , subject i

is randomly selected from the cases (Y = 1), and with prob-

ability (1 - yo), subject i is randomly selected from the
controls (Y = 0), and Wi is recorded. In the second stage,
first-stage subjects are independently selected with prob-

ability ir( Wi) to have Xi recorded. The design described in

Section 2.1 is the special case of this two-stage design with
does not depend on 7r, where heff is heff of Section 5.2 with
7 replacing 7r. In contrast, as we now argue, the asymptotic
variance of the optimal complete case estimator will differ

depending on whether ir is or is not known. To be precise,
let model "obs-complete" be the model "obs" with all nonvalidation sample data discarded and ir possibly unknown.
Imbens and Lancaster (1991) referred to model "obs-

complete" as the "Bernoulli sampling" model and showed
that with 7r unknown, (a) this model is equivalent to (i.e.,

,yo = Pr(Y = 1). Let model "obst" be model "obs," except

that we now allow yo and Pr(Y = 1) to differ and yo is
unknown, and write the logistic model (2) as g(X*; a)

= [1 + exp{-[a + b'X_l] }]-l, where ao = (ao, bo)' and
X is X* less the component that is the constant 1. Write

a&(h, 0)' = (a(h, q5), b(h, /)'). In Appendix C we prove
the following lemma.

Lemma (6.1). In model "obst" with g(X*; a) given by

has the same likelihood function as) the stratified sampling

(2), (a 1) any RAL estimator of bo is asymptotically equivalen

discrete choice model previously considered by Manski and

to some b(h, q5); (a2) the estimator given in Proposition
2.2b is consistent for varA ( n+1/2 [b(h, ) - bo]); (b)

Lerman (1977), Manski and McFadden (1981), Cosslett
(1981), and others, and (b) aCosslett proposed by Cosslett
(1981, sec. 2.14) and aImbens proposed by Imbens (1992, sec.

b(heff, lieff) as defined in Section 4.2 for Wt nondiscrete and
in Section 5.2 for Wt discrete is locally semiparametric ef-

a logistic model saturated with respect to a discrete V
there exist complete-case estimators that are efficient for

ficient for b (even were yo known); (c) if fixed fractions oy*
and 1 - y* of the n subjects are randomly sampled from
the cases (Y = 1) and controls (Y = 0), then parts (a) and
(b) remain true.
When V is discrete and the selection probabilities ir(W)
do not depend on the differentially misclassified measurements Vt, the b component of the Flanders-Greenland
(1991) and Breslow-Cain estimators (1988) are consistent

the slope parameters for X. Specifically, suppose that V

for bo but inefficient. Carroll et al. (1993) and Carroll, Wang,

3.2) are semiparametric efficient in this model. In Appendix
A we prove that Manski-McFadden estimator aMM( qMM) is
semiparametric efficient in the model "obs-complete" when
ir is known.

Although no complete-case estimator will be semiparametric efficient in model "obs" for all components of a0, in

has T levels, (1, 2, ..., T), and g(X*; a) = 1/[1

and Wang (1993) proposed estimators of bo in settings in

? exp {-E Ti aP(lX I (V =t) ? a (2)X2I (V = t) ? a43)J

which the selection probabilities may depend on Vt; how-

( V = t) }], with a' - ( a (l)', of 2) , a (3)') and a j =(of(J

ever, in contrast to our estimators b( h, /), for consistency

the estimators proposed by these authors require a correctly
... ., at(T) 'and a (1) and a (2) are the slope parameters. Suppose
that for the moment we strengthen both model "obs" and

specified parametric model for the law of vt given (Y, X* ).

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 857

6.4 Arbitrary W

verse of the (weighted) information matrix and thus will be

We now return to the general case of model "obs"; in
particular, Wmay now again have continuous components.
We suppose that we have a correctly specified parametric

invalid.

Although our proof of Proposition 6.1 assumes ir( ) to
be n1 2-consistent for ir, results of Newey (1993a) suggest

that it is necessary and sufficient for i7r( f) to be greater than

selection model for ir; that is, Equation (18) holds. Let 4t
n1/4 consistent for ir, which limits the number of free pamaximize H,i ?p is (() so that 0 = L, S+,,i (t). Define D(a,
rameters in model ir(4f). The effect of moderate overpah, 0, i) like D(a, h, /), but with the true selection probarameterization of ir(4t) is illustrated by the fact that the
bilities replaced by those specified by the parameter it in the
estimated asymptotic relative efficiency (ARE) (i.e., Monte

selection model. Let a&(h, 0, st) solve Ii Di (a, h, 0, it) = 0
Carlo efficiency) in row 9 is less than that in row 8, even
so a^(h, q, ito) = a^(h, 0). Define A02 = AA'. In Appendix
though t'{3)) added the terms 5 VIt2 + t6V 2 + t7v12y
B we prove the following.
+ {8 V2Y to model r(t (2)) of row 8. Thus the finite sample

Proposition 6.1. Given a correctly specified model for

efficiencies recorded in the table conflict with the theoretical

asymptotic efficiencies of Proposition 6. id. In our experience,
the missingness process with score Sf = A (0k) in model
"obs," (a) with probability approaching 1, a^(h, +, 1) exists

moderate overparameterization of ir(4 1) produces significant

finite sample bias in our estimated variance of a but little
and is unique; (b) a&(h, 0, 41) is a regular asymptotically

linear estimator of ao with influence function { K(h) } 'Resid
bias in a itself, suggesting that in this setting, inference (e.g.,
intervals) should be based on bootstrap estimates
{D(h, 0), S4,} = {K(h)}J-D(h, 0*), where Resid(A, confidence
B)

= A - E(AB'){var(B) } ` B is the residual from the popula-of the variability of a rather than on the variance estimator

tion least squares regression of A on B, and 1-

of Proposition 6. le.

+ E[D(h, )SJ]{var(S4)}J-'0; (c) varA[nh/2{a(h,
0)
Nonindependent

Sampling. Proposition 6.1 can be ex-

- ao}I ? varA[nl/2{&a(h, 1, ao) - Oo}I 2 varA[nl/2{a(h,
to settings in which the assumption that the (Yi, Xi,
0h) - ao } ], where the first inequality is stricttended
unless

Ai , Vi ), i = (1, ... , n), are independent random vectors is
E[D (h, 0 ) S$ ] = 0, and the second inequality is strict unless
inappropriate. As an example, suppose that W- (W*',
= =oh (i.e., unless b04 = h - 0 for some matrix b); (d)
* ')', W * is discrete with S levels w*, ..., w * and, folgiven J nested correctly specified models, j = 1, . . ., J, for
lowing Breslow and Cain ( 1988), the investigator uses a "fixed
the missingness process ordered by the increasing dimenfraction" sampling design in which he or she selects a fraction
sion of the parameter vectors / (i), the asymptotic variance
of a^(h, 0, A (I)) is nonincreasing with j; and (e) the asymp- ps without replacement from the N, subjects in level w5* of
W* into the validation sample for s = 1, ..., S. Because

totic variance of n-1/2 { a( h , 0 ) ao } can be consistently
one cannot sample a fractional person, we let ps=ps(Ns)
estimated by {K(h, t)}`Q(h, 0, t){K(h, 1)}`; K(h, 1)

- n-' li oD(a, h, 0, 0 )/Oaa'; Q(h, 0, f) = n-1

depend on Ns; however, we assume constants 7r(wS*) > 0

such that ps(Ns) - 7r(w*) is Op(N-1). For example, given
i Residi {D(h, 50), S41,}?2, where Residi {D(h, 5/), S4,} is

constants
ir( w5*), we could set ps(Ns)Ns to be the smallest
the residual for subject i from the least squares regression
of
integer greater than or equal to 7r(w* )Ns. We continue to
the Di(aA, h, 5, ii) on S4,,i (t) for i = 1, ... , n.
assume that the Li, i = ( 1, ... , n), are independent. On the
To illustrate the efficiency advantage of collecting data on
extraneous surrogates, in our simulation experiment we gen-

other hand, the Ai are not independent conditional on
{ Wi; i = 1, .. ., n }, because the sampling design ensures

erated extraneous surrogates Vt = (V 1, V t)' according to
that N- Ai I (W" = w * ) = ps. In Appendix B, we prove

V t I Y,X* - N(X-.12 + .24Y, a2), with U2 = U2,= 1
the following lemma.
cov(VI1, Vt I Y, X*) = 0. {(l), used in row 7 of Table 1,

was obtained by fitting the model

Lemma 6.2. In a model characterized by (1), data (7),
and a "fixed fraction" sampling design, the estimators a (h,

j=1 m=1

logit (7r (1)) - I 4'jmI{ (Y5 V) = (j, m)}
j=O m=O

5, if) remain asymptotically normal and unbiased for ao
with asymptotic variance that can still be consistently esti-

mated as in Proposition 6. le, provided that the model ir(4)
+ ? iV1 + ?t2VIt Y. (32)

used to estimate ir has nested within it the saturated model
Note that the true values of {l and 412 are 0, because X
.1
in =W*,

is constant. i (2) of row 8 added i13 V 2 + 4 V 2 Y to model

(32). Comparison of the efficiencies of row 4 with row 7 and

S

logit 7r(W; I*) = 4 1,*II(W* = w*),

of row 7 with row 8 illustrates Proposition 6. Id, because row

s=1

4 is based on fitting the model (32) with {,l and 412 set to 0
a priori. Note that although the row 3 estimator is semipara5' ? * . .. X'5* 4 ) '- (33)
metric efficient when W' = (Y, V), it is less efficient than
the row 8 estimator based on W' = (Y, V, Vt, Vi), which
6.5 Data Missing by Happenstance
illustrates the efficiency advantage of collecting data on extraneous surrogates. The estimators in rows 4 and 7-9 can

In many epidemiologic studies data are missing by hap-

be computed by a canned logistic regression program that

penstance rather than design, and thus the missingness

ir( W) is not known. For example, a subset of
allows for individual weights (i.e., Agi r1 (i,) 1l), althoughmechanism
the
standard errors output by the program are based on the in-

subjects may simply refuse to have blood drawn for Vitamin

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

858 Journal of the American Statistical Association, September 1994

A and E measurements. In this setting suppose that we were

Rk = 0 otherwise, with RO 1. Let R = (RI, ... , RK)' be

willing to continue to assume that the restrictions of model

the K vector of missing data indicators with realization r.

"obs" held except for the assumption that the missingness

Let L(r) be the vector of observed components of L when R

mechanism is completely known, but that we correctly spec-

ify a parametric model for the missing data mechanism with

= r and let lj, j = (0, ... , K), be the K vector in which the
first j components are 1 and the last K - j components are

score St('). Denote the resulting semiparametric model by
"obs*". Then in Appendixes A and B we prove the following.

as in our sequential design, the observed data for each subject

Proposition 6.2. In model "obs*" (a) the conclusions of

0. We shall say there is a monotone missing-data pattern if,
i is

{R, L(R)}, (34a)

Proposition 6.1 remain true, (b) the influence function of

any RAL estimator of ao lies in the set {Resid [K(h)1 D(h,
/), SJ }, and (c) the efficient score remains D(heff, 1teff).

with

R = lj for some j E {O, . .. , K} with probability 1.

Part (b) of Proposition 6.2 can be restated as follows: When

the missingness process is known only up to an unknown

(34b)

parameter i,0, the influence of any RAL estimator lies in the
Equation (34b) implies that RK = 1 if and only if I (R = 1)
set whose elements consist of the residual from the popu= 1, where 1 1K isthe K vector of ls and I (R = 1) is the
lation regression of an arbitrary "influence function of an
indicator for complete data on L. Data (7) was the special
RAL estimator when the missingness process is completely

known" on the "score S1, for the missingness process." case
As of monotone missingness, in which with probability 1,
eitherR = 1Mt orR = 1 with A = I(R = 1)and Lobs = L(R)an alternative to replacing 7r by 7r(f) in defining the estimator
The data in our sequential design were missing at random
a(h, 0, st), we could have estimated ir by a nonparametric

in the sense of Rubin (1976). That is, the probability of obregression estimator 7r to protect against misspecification bias.
serving R = r depends only on the observed components
Newey (1993a) considered nonparametric regression estimates based on series and provided regularity conditions

L(r) of L,

under which the resulting estimator of ao would be asymptotically equivalent to a&(h, oh). Of course, if the dimension

7r(r) Pr[R = rjL] = Pr[R = rIL(r)] (35)

of W is large, such asymptotic results are of little practical
relevance due to the "curse of dimensionality," and some

When (35) is true, we write 7r(r) as 7r(r, L(r)) when
to make explicit the dependence on L(r). Suppose that

(not necessarily parametric) model for ir( W) will be required.
7. OTHER MISSING DATA PATTERNS
We have assumed that any subjects with incomplete data

7r(1) > a > 0 with probability 1, (36)

so each subject has a positive probability of having
data, and that

have exactly the same set of covariates missing; that is, XI
and X2 were either both observed or both unobserved. In
this section this assumption is relaxed.

7.1 Monotone Missing-Data Patterns

lr(r) is known. (37)

Redefine the semiparametric model "obs" to be the model
characterized by (1), (35)-(37), and the observed data (34a).
To obtain consistent estimates of ao in this model, let Lk

--(Lo, L1, ... , Lk-1)' be shorthand for L(r) with r = l(k- 1)
Hence LK+l = L. Put Lo = 1. Define 1rk = Pr[Rk =I I Rk-I
(X2) than Vitamin A (XI). Then the following sequential
= 1 LkI, Xrk = m rm, ir0 = = 1. When, as we assume
design might be used in the study of Section 2.1. An initial
in this subsection, (34b) holds and thus missingness is
sample of subjects is randomly selected to have XI assayed.
monotone, it can be verified that 7r( 1) = 7rK and that (35) is
Suppose that it were more expensive to assay Vitamin E

A subsample of this initial sample is then randomly selected
equivalent to Pr[Rk = 1 IRk- = 1, L] = Pr[Rk = 1 iLk,
to have X2 assayed. The probability of selection into the initial
Rk-l = 11 ]7rk. Now redefine
sample may depend on W. The probability of selection into

the subsample may depend on both Wand Xi. a( h, q5), as
previously defined, may now be inconsistent since X may
no longer be independent of A given W, so E[D (a, h, 0)]

# E[DF(h)] = 0. Further, if selection does not depend on

D(a, h, 0) =I (R = 1)DF(a, h)/lr(1)-A(O) (38)
and
K

A (+)- (Rk - 17rkRk-1 )7r k (t)k(Lk)5 (39)

Xi, then a(heff, Oeff), although consistent, may now be ink= 1
efficient, because it ignores data on XI when XI is observed
with S (q1 , ..., 5K) and qt)k--(tk(Lk) taking values in R".
but X2 is missing. Note that we can generalize this sequential

design to a vector X = (XI, . .. , XM) of M ? 2 exposures.Furthermore, redefine ?mis(4/) and 0q5 of equation (19) as
Indeed, it will be both more general and more convenient

CM's (410) = HK1 {7rk( 40)R{1 - 7rk(14/0)} } , ,k

to consider a sequential design that would allow the com-

= 7rka logit 7rk(10)/d1, where Irk(1I) is a correctly specified
model for Irk; that is,

ponents of W as well as X to be missing. To do so, write
K = Mt + M with each component Lk and Wk of L and

W univariate. Thus L1 = W1, LMt = WMt, LMt+1 =Xl

and Irk(14) = I(Lk, 14) iS a known function taking values in

and LK = XM. Set LO-0. Let Rk = 1 if Lk is observed and

(0, 1] . In Section 8 and Appendixes A and B, we prove that

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 859

with these redefinitions, Propositions 2.2 and 3.2 and the
restatement of Proposition 3.2 remain true. Furthermore,

Propositions 2.3, 4.2, 6.1, and 6.2 remain true when we re-

define oh = (oh, . o. , q) and r(B) by

/(Lk) = E[DF(h) I Lk (41)
and

with m(B*) -r -r(r)E[B* I L(r)] and (2)
r[h(X*)cI]= {lr(1) l7r(r)E[B* IL(r)]. (46)
r*1

Note that with nonmonotone missing data, oh (the optimal

choice of 0 for fixed h) no longer exists in closed form. Also
Corollary 5.1 remains true, except with the condition r(L(r))
= b(r, L(r)) for r # 1 replacing condition (a) where B

K

r(B) = (1 - 7rk)1r kE[BILk (42)
k= 1

= IrI(R = r)b(r, L(r)). Note that even if the law of L
been known, because with arbitrary missing data patterns
the solution heff(X* ) to (23) depends on B* heff through (46)

and replace 7r by i7r(1) in the definition of t(X*) following
equation (24). Set t(B) = RKB/7rK. In view of the identity

and B* heff depends on heff( X* ) through (45), more work is
required to obtain heff and q$ff . In Appendix A, we show that

B* heff solves
E(B I Lk) = E[ (B) I Lk, Rk-l = 1l1rk-i given in Proposition

8.2b, an adaptive estimator a&(h, jh) can be constructed

B* = {ag(X*; ao)/aa} {var(e I X*) } -1e

as in Section 2.7, with 14k obtained by multiplying rk-l
by the estimated predicted value from the regression of

-{ m(B*) - B* } + E[m(B*) - B* I X*] var(e I X* )-e.

j[DF(a^, h)] on functions of Lk among subjects with
Rk-l = 1.

(47)
We prove in Appendix A that the solution Be*ff B* heff is

7.2 Missing at Random with Arbitrary MissingData Patterns

We next consider estimating the parameter ao in model
"obs" with arbitrary nonmonotone missing-data patterns.

unique and can be obtained by successive approximation.

Then Oeff has elements Or,eff = E[B,*ff I L(r)], r # 1. Further, by (45), heff = (heff,1 , . .., heff,t), with heff,j(X*)

= E[m(Be*ff) I Y = ej, X*] - E[m(Be*ff) I Y = 0, X*], with
ej as in Corollary 5.1. As in Section 4.2 we can obtain locally

As an example, suppose that in the study of Section 2.1 an

semiparametric efficient estimates by computing expecta-

initial sample of approximately 4% was selected, in which

tions under L?F( a^, $l; L).

one-half of the subjects had their Vitamin A (XI) assayed
When the data are missing by happenstance and there are
and one-half had their Vitamin E (X2) assayed. Suppose,

arbitrary missing-data patterns, it is necessary to specify and
further, that approximately 50% of subjects in whom Vitamin
fit models 7r(r, L(r), t1) for 7r(r, L(r)). Robins, Greenland,

A had been assayed were selected to have their Vitamin E

and Rotnitzky (1994) have provided appropriate models and

assayed as well (with the selection probability depending on

their observed Vitamin A levels) and vice versa. Then the

fitting algorithms.
Finally suppose that only data on X are missing but there

missing data pattern will be nonmonotone and yet satisfy

are arbitrary missing data patterns in X and (35) is false, so

(35)-(37). Redefine

the data are not missing at random in the sense of Rubin

A(+) = {7r(1)}J-I(R = 1)

by (38) and (43), can be inconsistent for ao. But suppose

(1 976). Then a (h, 5 ), using D (a, h, 5 ) and A ( ) as given
that (3)-(5) remain true. Then, even with arbitrary missing-

X ( r(r)Or(L(r))1- z I(R = r)4r(L(r)), (43)
data patterns
lr1

rJ

1

where the sum is over all possible realizations of R other

in X, a^(h, 5) with D (a, h, 5) and A(+) based

on (9) is a semiparametric estimator in the model defined
by restrictions (1) and (3)-(5). Furthermore, a proof anal-

ogous to that of theorem 3 of Robins and Rotnitzky (1995)
than 1, / = {Or; r # 1 }, and Or = 1r(L(r)) is a function
of

L(r) taking values in Rq. That is, A(+) = 7r(1)-1I(R

implies that a&(heff X eff), with heff and /eff as initially defined

in Section 4.2, will attain the semiparametric variance bound

= 1)E[O(R, L(R)) IL] - /(R, L(R)). Similarly, redefine

for this model.

fris(4,0) = ir(R, 1o) and 14,r = a log{ r(r, iI0)}/ /o9, where

ir(r, it) is a correctly specified parametric model for ir(r)
satisfying (35); that is,

8. ESTIMATION IN ARBITRARY SEMIPARAMETRIC
MODELS WITH MISSING DATA

7r(r)=7r(r; 42o) (44)

In Proposition 8.1 we provide representations for the efficient score and the influence function of any RAL estimator

and 7r(r; { ) = 7r(r, L(r), i ) is a function of if taking values
in an arbitrary semiparametric model with data missing at
in (0, 1 ] satisfying 2.r r(r, i ) = 1 for each if with probability
random. We prove Propositions 2.3, 3.1, 3.2, 4.2, and 6.2
1. In Section 8 and Appendixes A and B, we prove that with
by specializing to the case in which the "full data" semiparthese redefinitions, (a) Propositions 2.2 and 3.2, and the reametric model is characterized by the conditional mean restatement of Proposition 3.2a are true, and (b) Propositions
striction (1).
2.3, 4.1, 6.1, and 6.2 are true (1) when rkh has elements rhh
-E(B*hIL(r)), where B*h is defined to be the unique q
vector-valued function of L satisfying

m(B*h*) = h(X* )e, (45)

8.1 An Arbitrary Semiparametric Model
Consider the generic semiparametric model with likeli-

hood ?(a, 0; Z ) of Section 3. We define the nuisance tangent

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

860 Journal of the American Statistical Association, September 1994

to be A(') = {A(E E Rq; there exists bjSp,j with limj1-.,
random vectors bS,, where S, is the score for q in
[IIA3
some
"j- b1S+,II 2] = 0}, where Sp is the score at the tr
regular parametric submodel r( r, L(r); 4) of the model
regular parametric submodel (usually, S, = d log ? for
(ao,a i7o;

space A to be the mean squared closure of the set of all

L(r); y) and the bj are constant matrices. We shall assume
Z)/)77) and b is a conformable constant matrix with q7r(r,
rows

A (3) is linear. Define A (2) = {A(2) = a(2)(R, L(R))E Rq;
(i.e., A = {A E Rq: E[ ||Al| 2] < oo), and there existsthat
bjSa,j
E[A (2) 1 L] = O)} to be the space of functions of the observed
with limj,,o E[ IIA-bjS j1II 2] = 0}, where each bj is a matrix
with
of constants and IIAll 2 = A 'A. We shall considerdata
A as
a mean 0 given the full data L. In Appendix A we
subset of the Hilbert space of q X 1 random vectors H with

inner product E[H' H2] and E[H'H] < oo. In our examples

prove the following.

Lemma 8.2.

A is a linear subspace. The projection ll(HI A), of any vector
H on a closed linear space such as A exists and is the unique

a. A c A

vector A E A minimizing E[(H - A)'(H - A)]. HI is the

b. If the model (49) is completely nonparametric in the

projection operator. Hl(H I A) is also the unique element of

sense that it is unrestricted except for the condition

A satisfying E[{H- l(HIA)} 'A] = 0 for all A E A. The

1r 7r(r, L(r); y) = 1, then A2) =A

semiparametric variance bound equals the inverse of the

variance of Seff -l[ Sa I A'], where Sa is the score for a

(usually, Sa = a log L(ao, Oo; Z)/aa) and "l" denotes an
orthogonal complement (Begun et al. 1983; Bickel et al.
1993). Seff is called the efficient score.

Lemma 8.3. If (36) is true, then with A(+) as in (43),

(a) E[A(1)I L] = 0 and (b) given any a2)(R, L(R)) E ,

a(2)(R, L(R)) = A(+), with -9r(L(r)) = a(2)(r, L(r)) for r
= 1. Hence A(2) = {A(0)} .
Henceforth let B = b(R, L(R)) and D = d(R, L(R)) rep-

Definition. For any set 5 of random variables, let W ( be

the subset with mean 0. Let A' (A')O.

resent generic functions of (R, L(R)) and let B* = b*(L),
and D* = d*(L) represent generic functions of the full
data L. Define the operators g, m, u, and v by g(B*)

Lemma 8. 1.

a. In any semiparametric model, the influence func-

tion of any RAL estimator of ao is in A'* - {E

[AS' -'A; A E Ao }{A E Ao; E[AS'] = Iqxq}-

b. Further, A E A4 implies E(AS'ff) = E(AS'). Theorem (2.2) of Newey (1 990a) implies Lemma 8. 1.

Even if Ao* contains nonzero elements, Lemma 8.1 does
not guarantee that any RAL estimator exists: however, in
sufficiently smooth models, including all those studied in

-Jr I(R = r)E(B* I L(r) R = r), m(B*) )-E[g(B*) I L]
= -r 7r(r)E(B*I L(r), R = r), u(B*) = {7r(1)}1-I(R
= 1)B*, and v(B*, A(2)) = u(B*) + A(2) - H[u(B*)

+ A (2)1 A (3)]. When the data are missing at random
(35) is true), E(B* l L(r), R = r) = E(B* l L(r)) and g(B*)
and m(B*) simplify. Our fundamental result is the following
rather abstract proposition, a verbal description of which is
provided in Remark 1.
Proposition 8.1. In model "obs," with A ( ) as in (43):

this paper, Ao* can be identified with the space of influence
al. If B E Ao, then the decomposition B = u {E(B I L)}
functions of RAL estimators (Bickel and Ritov 1990; Newey

+ [B - u{E(BIL)}] satisfies E(BIL) C AO' and

1990a).

We now specialize the foregoing general results to "full

B - u {E(B I L)} = A () E A with 1r(L(r))
= b(r, L(r)) for r 1.

data" and "missing data" models. Again let L' = (LI,....
a2. Further, if B E AO'*, then E(B I L) C AOn* .
LK) be a multivariate random variable with each Lk uni- b. A' {v(B*, A(2)); B* E AoF, A(2) E A(2)}

variate, let

cl. If B* E AF," A2 A (2), then E[v(B*, A(2))S Sf]
LF(a,

be

the

0;

likelihood

L)

(48)

for

a

in a semiparametric model "full" indexed by a E Rq and
an infinite-dimensional nuisance parameter 0, and let SF,

= E[B*S"].

c2. Ao* = {v(B*, A(2)), B* E AO*, A(2) E A(2)}
single subject when L is full

= {[E(B*Seff)]-lv(B* A(2)); B* C As" A(2)

E A (2)}.

d. ll[u(D*)IA(2)] = u(D*) - g{m-(D*)} = A(O*),

AF, Seff, and AO* be the score for a, the nuisance tangent
where m- (D*) is the unique B* solving m(B*) = D*
space, the efficient score, and the space of influence functions
in model "full." Let the semiparametric model "obs" with

likelihood ? (a, 0; R, L(R)) now be characterized by the
observed data (34a), restriction (48) on the law of L, the
missing-at-random assumption (35), restriction (36), and the
additional restriction that

and /r* (L(r)) = E[m- (D*) I L(r)] for r # 1.
el. Seff = u(De*ff) - Il[u(De*ff)IA (2)] = g[m-I(De*ff)],
with De*ff the unique D* E AF1 solving ll[m(D*) I AFu e sff
e2. Be*ff m-'(De*ff) is the unique B* satisfying B*

= eff- [m(B*)-B*IAF] which can be solved

r(r, L(r)) E {r(r, L(r); y): Y E 'Y}, (49)

by successive approximation.

1. When l7r(r) is known and A (3) is thus empty,
where, for each -y, 7r(r, L(r); NY) is a density for Pr[R = rI Remark
L]
part c2 of Proposition 8.1 is equivalent to the restatement
satisfying (35) and y may be infinite dimensional. Henceof Proposition 3.2a, except that to be precise, we should have
forth let S,a, A, Seff, and A&* be the score for ag, the nuisance
tangent space, the efficient score, and the space of influence

referred to the set of "mean square limits of scores" rather

functions in the missing data model "obs." Define the

than simply to the set of "scores." When model (49) is a

tangent space for the model (49) for the missing process

parametric model with score S, part c2 of Proposition 8.1

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 861

is equivalent to the restatement of Proposition 6.2b, because

D(a, h, ) given by (38), Proposition 3.2a is Proposition
F,l

in that case for any B, B - Il(B I A (3)) is the residual from 8. 1c2 with A0o* as in Proposition 8.3e. Proposition 3.2b
follows from (a) Seff E A & so Seff = D(heff, (keff) for some
the population least squares regression of B on Si,. Propo-

heff and keff by Proposition 8.3b and (b) K(heff)
sition 8.1el and Eq. (36) implies both that var(Seff) is invertible and that Seff does not depend on A (3) and thus on = E[DF(heff)Sf] = E[D(heff, (keff)Seff] = var(Seff) by
Proposition 8.3d and Proposition 8.1 ci. If missingness is
the model (49) for the missingness process. It follows that,
even if models (48) for the full data and (49) for the missingmonotone, then, by Proposition 8.2c, Proposition 3.2 is true
at random mechanism are completely unrestricted, the law

of L and the response probabilities ir(r, L(r)) are locally
identified. Proposition 8.1 e2 is used to prove that the so-

with A(X) as defined in (39).

Proof of Propositions 2.3, 4.1, and 4.2. To prove Prop-

osition 2.3, for arbitrary missing-data patterns, note that by
lutions to (23) and (47) are unique. The remaining parts
Proposition 2.2, the asymptotic variance of a^(h, k) is
of Proposition 8.1 are needed for the proofs of Propositions
2.3, 4.1, and 4.2. It can be seen from the proof provided

in Appendix A, that when lr(r) Pr[R = r I L] is completely known (i.e., A (3) is empty), Proposition 8.1 remains
true even without the missing at random assumption (35).

Van der Laan (1993, pp. 29-30) proved that B*
= ml (D*) can be obtained by solving B* = D* + [B*
-m(B*)] by successive approximation. The following
proposition specializes parts of Proposition 8.1 to the case
of monotone missingness.

var{ [K(h)]-'D(h, k)} . Thus with h fixed, we'need to min-

imize var { D (h, (k) var [ u {DF(h ) A(X)] over A ( X)
E A (2). By the definition of a projection, the minimizer will

be ll[U{DF(h)A} IA2)], which by Proposition 8.1d equals
A(ph) with 0 h _ E[B* hI L(r)] because, by its definition (45),
B*h = m-' {DF(h) }, proving Proposition 2.3. Proposition
4.1 then follows from the fact that in the saturated model
"full b," the OLS estimator a^F(i) is known to be RAL, and

AO*' consists of a single element (Bickel et al. 1993; Newey

1990a). Turning now to Proposition 4.2, Propositions 8. le
and
8.3b imply that heff(X*) solves SFff = Il[m-i(D*)I
Proposition 8.2. In model "obs," if missingness is
AFI],
monotone [i.e., Eq. (34b) is true], then, with A ( k) given bywith D* = h(X*)e. Hence Propositions 8.3a and

(39):

al. g(D*) = RKD* + K I (Rkl - Rk)E(D* Lk).
a2. m(D*) = rKD* ? (1 - 7k)7rk-1E(D* ILk)

b. E(D*ILk) = E(D*ILk, Rk-l = 1) = E(RKD*/
XKILk, Rk-1 = l)0k-i-

c. A (2) ={A(f)

d. ll[u(D*)IA(2)] A(P*) with (P* = E(D*ILk).

8.3c and definition (45) of B* h imply that heff( X* ) solv
dg(X*; aoe)/dae = E[B* he I lX*] . But definitions (45) and

(46) imply that B* h = {1r(1) }I -h(X*)e - r[h(X*)e]. Thus

heff(X*) solves dg(X*; ao)/da = h(X*){t(X*)}-'

- E[r {h(X* )e } e 'IX*], which gives (23) . Equation (24)

follows from Proposition 2.3. Uniqueness is proved in
Appendix A.

To prove Propositions 2.3, 4.1, and 4.2 when the miss-

e. m (D*) = D*/ irK- k=1 - rk)rkE(D* Lk)

ing data pattern is monotone, note by Proposition
8.2d, 0 h is as in (41) with A(X) now as in (39). Further,
Propositions 8.1 and 8.2 are proved in Appendix A.
after substituting the right side of the identity in Prom-' (D*) and ll[u(D*) I A (2)] do not in general exist in closed
position 8.2e for m- (D*) in the identity Seff
form with arbitrary patterns of missingness. But according
= ll[m'l (D* ) I AFl], we again obtain (23), but now with
to Propositions 8.2d and 8.2e, they do exist in closed
r(h(X* )e) as in (42) .
form when the missing pattern is monotone. The rightmost expression in part b is a useful representation
Proof of Propositions 6.2b and 6.2c. Proposition 6.2b

of E[D*lLk] as an explicit function of the observables
follows from Proposition 3.2 and the discussion in
Remark 1. Proposition 6.2c follows from Proposition
(R, L(R))8.2 Specialization to the Conditional Mean
Model (1)

8. el.

8.3 Additional Considerations

The following proposition concerning the full data
In model
any semiparametric model in which data are missing
(1) is needed for the proofs of Propositions 2.3, 3.1,
3.2, 4.1,
at random,
the probability of observing full (complete)
and 4.2 and is proved in Appendix A.

data is bounded away from 0, and the response probabil-

II(D*IAOF,) = E[D*e/'IX*]{var(eIX*)}j-e, (b) AOt

ities are known or can be modelled, Proposition 8.1 can
be used to construct a class of estimators that contains
both an estimator whose asymptotic variance attains the
semiparametric variance bound and an estimator asymp-

(17), (d) E[DF(h)Seff] = E[h(X*){dg(X*; ao)/doa}]

totically equivalent to any given RAL estimator. As an
example, consider the Cox proportional hazards model,

Proposition 8.3. In the model "full" characterized by

restriction (1) with likelihood LF( a, 0; L) of (16a), (a)
= {DF(h) }, (c) Sff = DF(h hff) with heff as in equation

aK(h), and (e)Ao* = {[K(h)I-lDF(h)}.

Proposition 3.1 is an immediate corollary of Lemma 8.1

and Proposition 8.3.

Proof of Proposition 3.2. Consider first an arbitrary

missing data pattern. Then, with A (+) given by (43) and

XT(tIlX*) = Xo(t)exp(a50X* ), (50)

where, in the example of Section 2.1, Tis time to
infarction, XT(tlX*) is the hazard of TXat time t
and V no longer includes the constant 1. We assume cen-

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

862 Journal of the American Statistical Association, September 1994

APPENDIX A: SEMIPARAMETRIC EFFICIENCY

soring C, defined as the minimum of time to death without
myocardial infarction and end of follow-up, is independent

In Appendix A, we prove Lemmas 8.2-8.3, Propositions 8.1-

of T given X*. Redefine W = (T* -min(T, C), Y-I(T
= T*), Vt', V')'. Now let model "full" be defined by data

8.3, that Equations (23), (27) and (47) have unique solutions, that
Equations (23) and (47) can be solved by successive approximation,

L = (W',X')' and restriction (50). Let model "obs" now be

and that the Manski-McFadden estimator of Section 6 is semipar-

characterized by (50), (3)- (5), and data (7); that is, ( A, Lobs) .

ametric efficient in model "obs-complete" with 7r(W) known.

Pugh, Robins, Lipsitz, and Harrington (1992) proved that,

Proof of Lemma 8.2

similar to Proposition 2.2, the influence function of a&(h, k)

solving Ji Di(a, h, k) = 0 equals K(h) l { ABF(h)/1r

A (3) C A (2), because (a) any score So is in A (2) by the conditional

-A(4)} where K(h) `BF(h) is the influence function of

mean 0 property of scores, and (b) A (2) is closed because it is the

aF(h) solving i D (a, h) = 0. Here A(fk) is as in (9) and

inverse image of the closed set of {O } under the continuous mapping

h h(t, X*), DYf(a, h) = I(Tj < Ci){h(T*, Xi) E{ * I L }. To prove part b, we note that for any bounded A (2) in
A (2), the submodel ir(R, L(R )(1 + {'A(2)) defined on a sufficiently
-j e"}ejI(Tj* ? T* )h(Ti*, Xj*)/1j ea'xI(Tj* ? Tj*
)

Di(a, h, ) = A1irT'I(TI < Ci){h( T* , X*) - [Zj ea ;I

small open ball around i10 = 0 is regular with score A (2) by lemma

C.4 of Newey (1 990b). But any function in A (2) can be approximated
in mean square by bounded functions.

A}ir-'] -Ai(0), K(h) = E{BF(h)BF(h hff)}', heff(t, X*)
= X*, BF(h) = f dM(u) {h(u, X*) - E[h(u, X*) IT

Proof of Lemma 8.3

= u] }, dM(u) = dN(u) - Xo(u)exp(aoX* )I( T* > u) du,
and N(u) = I[T* < u, T* = T]. Note that aF(heff) is the
usual Cox partial likelihood estimator. Ritov and Wellner

Part (a) is an easy calculation. For part (b), note that 0 = E[a

(R, L(R)) I L] = Er .7r(r)a(2)(r, L(r)) implies a(2)(1, L) = {7r(l)
Zr*1 .7(r)a (2) (r, L(r)).-

(1988) proved that any RAL estimator of ao in model "full" The proof of Proposition 8.1 requires a series of lemmas con-

is asymptotically equivalent to a&F(h) for some h; thus, by

cerning model "obs" of Section 8.

Proposition 8.1c2 (i.e., by the restatement of Proposition

3.2i), any RAL estimator of ao in model "obs" is asymptotically equivalent to some estimator a^(h, k). Further, by

Lemma A.1. In model "obs," A is the closure of {g(AF) + A(3);

A E CA , A FE AF } and Sa = g(Sa) . Lemma A. 1 states that the

a proof analogous to our proofs of Propositions 2.3, 4.1, and

score for a in model "obs" is the conditional expectation of the
score for a in model "full" given the observed data.

4.2, it can be shown that the asymptotic variance of a&(heff,
Oeff) attains the semiparametric variance bound for model

Proof Lemma A. 1 follows from proposition A5.5 of Bickel et

al. (1993) concerning scores in missing-data models, Lemma C.4

"obs," where Oeff = E[BF(heff)I W] and heff(u, X*) solves
h(u, X*) = E[h(u, X*)IT = u] + v(u, X*){X*
? E[-xKlBF(h) - (1 - r)Kr-E{BF(h) I W} IX*, T* > u]

? E[{Ir-'(u, 1, Vt, V) - 1}E{BF(h)I T= u, Vt, V} IX*] }

of Newey (1990b) and the fact that, by the missing-at-random as-

sumption (35), g(A(3)) = A(3).
Lemma A.2. If (36) is true and B = b(R, L(R)), then (a) B

with v(u, X*) = E[ ir -'(u , 1, Vt, V) I X*]. The case cohort
design (Prentice, 1986) is a special case of model "obs" with
ir(u, 1, Vt, V) = 1 and ir(u, 0, Vt, V) = p, with p the

-u{E(BIL)} = A(X), with &r(L(r)) = -b(r, L(r)), and (b) B

subcohort sampling fraction. The nested case-control design

from part a either by Lemma 8.3 or directly from E[u {E(B I L) } I L]

-u{E(BIL)} EA(

Proof Part a is a straightforward calculation. Part b follows

= E(B IL).
of Thomas (1977) is also a special case of model "obs" modified so as to allow for nonindependent sampling as in Section Lemma A.3. For all B* and allA (2) E A (2) E[g(B* )A (2)] = 0.
(6.3). Proposition 8. la can be used to show that, even when

data on the extraneous surrogates Vt are not obtained, the

estimators of Prentice (1986) and Lin and Ying (1993) for
the case-cohort design and the partial likelihood estimator

Proof E[g(B*)A(2)'] = E[B*A (2)] = E[B*E{A(2) IL}']

= E[B*O'] = 0.

Lemma A.4. In model "obs," A (1) and A (3) are mutually or-

of Thomas (1977) for the nested case-control design are gen- thogonal closed linear spaces, where A (1)={ g(AF); AF E AF}.
erally inefficient. Similar results have been obtained by Rob-Further, A = A (1) ED A (3), where ED denotes the direct sum of two
ins, Hsieh, and Newey (1995) and Rotnitzky and Robins

spaces.

(1993) for parametric models for the conditional density of

Proof A(3) 1 A(') by Lemmas A.3 and 8.2. A(3) is closed by
definition and linear by assumption. Linearity of A (') follows from
the assumed linearity of AF. To prove that A (') is closed, we note
that it is the image of the closed set AF under the linear operator

Y given X* with missing covariates or outcome data, by

Robins and Rotnitzky and Zhao (1995) for parametric models for the conditional mean of Y given X* with missing

g( * ). Hence it suffices to chow that g( * ) has a continuous inverse.
data on Y, and by Robins and Rotnitzky (1992) and Robins
But g( * ) has a continuous inverse by part a of Proposition A 1.7 of
(1993a,b) in the accelerated failure time model and in the
Bickel et al. (1993), because E[ llg(AF) 112] 2 E[I (R = 1)AF'AF]
Cox proportional hazards model in the presence of dependent
= E[LlI(1)AF'AF] 2 oE[ IIAFI 2], where the final inequality is by
censoring attributable to time-dependent covariates that si(36). Hence A = A ") ED A (3) by Lemma A. 1.
multaneously predict failure and censoring. Analogues of
Lemma A.5. In model "obs," if B* E AOFs and A(2) is in A (2)
Propositions 8.1 and 8.2 hold when the data are coarsened
at random in the sense of Heitjan and Rubin (1991) allowing

then v(B*, A(2)) E A&.

extension of our results to models with missingness (e.g.,

Proof: Because A = A(') G A(3) and E[v(B*, A(2))] = 0, it
is sufficient to show that v(B*, A(2)) is contained in A (1), and
A(3)". By its definition, v(B*, A(2)) E A(3)". That v(B*, A(2))

censoring) in continuous time (Robins and Rotnitzky 1992;
Robins 1 993a,b).

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhoo: Regression With Missing Regressors 863

E A ('" follows from E[v(B*, A(2))g(AF)'] = E[u(B*)g(AF)']
= E[B*A F'], where the first equality is by Lemma A.3 and the
second equality is by E[u(B*)g(AF)'] = E[{7r(1)}-'I (R
= 1)B*A F'].

Lemma A.6. In model "obs," B E A(1) 1 if and only if
E AOF,'.

(/4 r = 8 ( 1 rk ) rk ( Lk) -)
k=O

with r = lj, 0 ' j < K. (A.1)
1) can be reexpressed as the following recursive formula for the
E(B(A.
I L)

O(/k(Lk) in terms of,,t:

Proof The lemma follows from E[Bg(A F)1] = E[u {E

Oj+I(Lj+l) = {-Or + , (1 -rk)rk k(Lk)}i,
(B I L) } g(A F) '] = E[E(BIL)A FI], where the first equality is by
k= 1

Lemmas A.2 and A.3.

0

with r = lj, j = (0, ... , K), where, by convention, z-0.

Proof of Proposition 8.1

k= 1

Because B E A' == B E A part al follows from Lemmas

A.6 and A.2. Part a2 follows from B E AO'* implies Iqxq
= E[BS'] = E[u {E(B I L) } g(SF)']= E[E(B I L)SaF] where the
second equality is by Sa = g(SaF), and Lemmas A.2-A.3. Part b
follows immediately from part al and Lemma A.5. Turn next to

(A.2)

Part c now follows from (A.1), (A.2), and Lemma 8.3. To prove

part d, note that by part c, A (0* ) E A (2) . Also, one can write u(D*

= D* + I (R, - kRk-l)ikk D*, so that u(D*)-A (X*) = D*

+ k= (Rk - rkRkl)ik {D* - E[D* ILk, Rk-l = 1]}. Using
part cl. Because B* E AOF' implies that v(B*, A(2)) E A' by Lemma
this representation, one can use iterated conditional expectations
A.5, in view of Lemma 8.1b it suffices to note E[v(B*, A 2)Sa]
toshowthatE[{u(D*)-A(?**)}A(X)']=0forallA(X)(-A (),
= E[B* SF'], because E[v(B*, A (2))S] = E[u(B*)g(SF)] by Sce
= g(S ) and Lemma A.3. Part c2 is a consequence of parts b and
cI and Lemma 8. lb. Turn now to part d. Van der Laan (1993, pp.

proving part d. Part e follows at once from part d, Proposition 8. ld
and the identity E[,rK-I (RK = 1)g{m-l(D*)} I L] = m-l(D*).

29-30) proved that m-' exists as a bounded linear operator. A modProof of Proposition 8.3

ified version of his proof follows. [The proof in Robins and Rot-

nitzky (1992) unnecessarily and incorrectly argued that the operator

To prove part a, following arguments essentially identical to those

ir(l)-lm(B*) - B* was always compact.] Let i be the identity

in lemma A.5 of Newey and Powell (1990), we have AF = {Al + A2

+ A3: Al = a, (X*), A2 = a2(X*, e), and A3 = a3(L), with E(A1) =
operator. An operator a is a contraction if SUPIIB*I1=, E[ IIa(B*) 112]
0, E(A2IX*) = 0, E(eA IX*) = 0, and E(A3Ie, X*) = 0}.
< c < 1. Now m-' is a bounded linear operator if i - m is a conThe restriction E(eA2 IX*) = 0 is a consequence of E[ eX*] = 0.
traction, since m = i - (i - m) and the identity minus a contraction
has a bounded inverse (Kress, 1989, p. 16). But m and thus i - m

are self-adjoint, because E[m(B*)'D* ] = E[B* 'm(D*)]. By i - m

Further, it is easy to show that A' - {AI }, A F {A2}, and A F
{A3 } are mutually orthogonal. Consider any D* with E(D*) = 0.

Then
self-adjoint, SUpIIB*I1=1 E[ || [i - m](B*) 112] = SUP,iB*11=1 E[B*
'[i II(D*IAF) = II(D*IAF) + II(D*IA F) + 1l(D*I AF).
Further, it is easy to verify that ll(D* I A F) = D* - E(D*I X*, e),
-m](B*)] (Kress 1989, thm. 15.9). But SUpIIB*11=1 E[B*'[i
l(D*IA F) = E(D*IX*, e) - E(D*IX*) - E (D*c'I

-m](B*)] < 1 - r, since E[B*'m(B*)] = E[g(B*)'g(B*)]

X*)E(ee'IX*)-'e,
and lI(D*IA F) = E(D*IX*). So finally,
2 E[ir(1) JI B*II 2]> uE[ 11 B*II 2], where the last inequality
uses (36).
It remains to show ll[u(D* ) A (2)s1j = g(B*) with B* 3 m-(D*).
ll(D* I A F) = D* - E(D*'lI X*)E(ee'I X*)-1e, proving part a. Par
b follows directly from a; c follows from a and the identity
Now g(B*) E A (2) 1 by Lemma A.3. Further, because D* = m(B*)
E(SFe' IX*) = dg(X*; ao)/da or from Chamberlain (1987). Part d
= E[g(B*) I L], u(D*) - g(B*) = A(0*) E A (2) by Lemma A.2,
completing the proof. To prove part el, note that Seff Sa

is a straightforward calculation, and e follows from b.

l-(S I AA) = Sa -(S I A (')), because Sa = g(SF) I A (3) by A3

c A(2) and Lemma A.3. So with A F E A F solving II(SaIA('))

=g(A F) and Be*ff = SF - AF, Seff = g(Be*ff) E A' and

Uniqueness of the Solutions to Equations (23), (27),

and (47)

Since
Equation (47) is a special case of the equation in Proposi
[Be*ff lAF,1] = ll[SFlAF,?I ] 3 SeF. Hence by part al,
Seff

8.1 e2, it has a unique solution. Further, because heff( X *), solv
= u{m(B*ff)} + [g(Be*ff) - u{m(Be*ff)}], with Deff 3 m(Be*ff)
(23)
implies
E AF,' and g(Be*ff) - u{m(Be*ff)j} = -1[u(Deff)IA (2) ] by
part
d, m-1(heff(X*)e) solves (47), heff(X*) is unique by
which proves part e 1 except for uniqueness. To prove part m-'
e 1, ()
first
injective. To prove that (27) has a unique solution, let >j,
1, 2 be two solutions to (27). Let hj(X*) be defined by (26),
note Be*ff = 5 - A a solves the equation in part e2j =because
g(Be*ff) E A' implies m(Be*ff) E AF? by Lemma A.6. To
prove
with
4j substituted for (/eff. Then E[hj(X * )c I W] = 4j by qj satisfying

uniqueness, let B* be the difference between two solutions B*

to the equation in part (e.2), so 0 = m(B*) + lI[B*

(27), and thus hj(X*) satisfies (23). By uniqueness of the solution

to (23), h1(X*) = h2(X*). Hence by E[hj(X*)eI W] = oj, we

-m(BO*)IAF]. Thus m(B*) E AF,1. Hence 0 = II[Bo* IAFI],
conclude that 0 1 = 102so B* E AF. Thus 0 = E[m(B*)B*'] 2 rE[B*Bt'] by (36).
Proof
Hence B* = 0 with probability 1. This implies uniqueness in
partThat the Equation in Proposition (8.1e2), Equations
(23) and (47) Can Be Solved by Successive
e 1 as well. That the equation in (e.2) can be solved by successive
Approximation

approximation is proved below.

Let a be a bounded linear operator from a Hilbert space to itself.

Proof of Proposition 8.2

The solution B to B = D - a(B) can be obtained by successive

Parts al and a2 are easy calculations. To prove part b, note that

(35) and (36) implies that f [LILk, R = 1k -] = f[LILk], which
implies E[D* I Lk] = E[D* Lk, R = 1k-1 ] = E[D* Lk, Rkl = 1] Also, E[D* I Lk] = E[RKD*/7rKILk] = E[RKD*/7iKILk, Rk_l
= 1]Pr[Rk_l = I lLk]. But Pr[Rk-l = I14L] = X-

approximation if a is a contraction (Kress 1989, p. 17). The operator
ll[m - ii AF] in Proposition 8. 1e2 is a contraction, since m - i is
a contraction (Van der Laan 1993) and H[l- IAF] is a projection
operator. Hence Equation (47), as a special case of Proposition
8. 1 e2, can also be solved by successive approximation. Finally, the

To prove part c, one can calculate that A (1k), defined as in (39), operator acting on h(X* ) on the right side of (23) can be shown to
equals A (qt) as defined in (43) if

be a contraction by applying the Cauchy-Schwartz inequality twice.

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

864 Journal of the American Statistical Association, September 1994

Proof That a^MM(qMM) is Semiparametric Efficient in Model

that by Proposition 8. 1 c 1, Proposition 8.3d, and Lemma 8. 1 b, K(h)

"obs-complete" with r(W) Known

= E[D(h, q5)Sa] completes the proof of Propositions 6. la and 6. lb.

inequality in Proposition 6. 1 c follows from Proposition
In this model the likelihood is ?C (a, 0; Y, X, V) = fThe
[Y,second
X, V
I A

= 1; a, 0]. Redefine Sa= a logf( YIX*; ao)/aa = hf (X* ) and

2.3; the first inequality is a special case of Proposition 6. Id. To

prove
6. Id, we note that S41 0) is the first p(J) compo
AF = {A = a(X, V); E(A I A = 1) = O}. Let Sa, Seff, and
AProposition
be the
score for a, efficient score, and nuisance tangent space in model

"obs-complete" with 7r(W) known. Then it is straightforward to

show Sa = S' - E[S' I A = 1] and A = {A - E(AIA = 1); A
E AF}. Hence it is sufficient to show DMM(qMM) is Seff. To show

of the p(j+l) vector So(,+'), where So (,, is the score for model j

p(j) is the dimension of 1 ('). By standard least squares theory, the

variance of the residual from a population regression does not increase as the number of regressors increases, proving Proposition

6. 1d.
DMM(qMM) E A', note that DMM(qMM) = S' - E[S | X, V,
AvFinally,

=1], so E[DMM(qMM){A-E(A I A= 1)}'I A = 1] = O for all A
E AF. Finally, Sa - DMM(qMM) = E[Sa |X, V, Av = 1]

- E[SF IA = 1] E A. Hence DMM(qMM) = Seff
APPENDIX B: ASYMPTOTIC NORMALITY
In this Appendix, we prove Lemma 6.2 and Propositions 2.2,
2.4, 6.1, and 6.2a.
We first prove Proposition 6.1 and then Proposition 2.4. Proposition 2.2 is a special case of 6. 1, and the proof of Proposition 6.2a

is identical to that of Proposition 6.1. We consider the case of arbitrary missing-data patterns so that S#(l,6) is based on a model

lr(r, ,6') satisfying (44), D(a, h, / ) is given by (38), and A( 0) is
given by (43). Further specialization to the case of monotone missing
data is straightforward.

Let H(y)' = (DF(h)', S#(lt)'), y' = (a', it"), and zy = a X 4',
where a and 4' are the parameter spaces of a and it. We prove our
propositions under the following nine regularity conditions:

1. oy lies in the interior of a compact set y.

2. except in the proof of Lemma 6.2 (Li, Ri), i = (1, ..., n)
are independently and identically distributed.

3. ir(1,i,6)> c> Oforallit'e4'forsomec.

4. E[H(-y)] # O if -y yo.
5. var[H(^y0)] is finite and positive definite.

6. E[dH(^yo)/doy'] exists and is invertible.

Proposition 6.1 e follows under our regularity con-

ditions, from Theorem 4.5 in Newey and McFadden (1993).

Proof of Proposition 2.4

Redefine y' (a[ 4a, '), where a1I and a2 are of the dimension
of a0, H*(^y)' (D{ai, h, 1(W; X)}', D(a2, h, 0)', G(a2, X)')
with 0 = ?(W) a given function, G(a, X) A[DF(a, h) - l(W;

X)] {Il(W; X)/Xa} . We
that regularity conditions
Define -y' = (ac', a', Xt'),
and note that E[H*( -yo)] =

prove Proposition 2.4 under the assumpt
1-9 hold, with H*(-y) replacing H(^Y).
where Xt satisfies E[G(ao, Xt)] = 0,
0 even if (15) is misspecified. Thus by

theorem 3.4 of Newey and McFadden (1993) or corollary 1 of
Manski (1988, chap. 8), with probability approaching 1 the solution

jy to L, Hl* (ey) = 0 is asymptotically linear with influence function

{-E[dH*(yo)/}y]j}-'H*(-yo). But j' is precisely (&(h, kh)',

a&(h, )%',X'), defined in Section 2.7. Hence &(h, Xh) is asymptotically linear with influence function - {E[kDF(ao, h)/da']}-`D(ao,
h, Ot), with ot(w) = l(w; Xt), because 0 = E[dD(al, h, 1(W;

O= E[D(al, h, 1(W; X))/da2'] 1,0 and E[dD(al,
h, 1(W; X))/Iaa]I = E[dDF(ao, h)/doa']. The consistency of

the variance estimator follows from theorem 4.5 of Newey and
McFadden (1993).

Proof of Lemma 6.2

7. a neighborhood N of -yo such that E[sup,ENIIH(-y)
11],
It is straightforward
to show that the probability distribution genE[SUP8ENII dH(^y)/doy' II], and E[5upffN||H(^y)H(^y)' erated
11] are
by all
the finite,
"fixed sampling plan" is equal to the conditional

where I|A|II|-{ LE Ai2} 1/2 for any matrix A with elements A11.

8. f( L, R; ay) is a regular parametric model with score Sky)

distribution generated by an independent sample plan with ir(W)
= 7r(W*) conditional on U = 0, where U = (U,, ..., Us)', Us

= a logfNL, R; H y)/&y, where[f(UL, R; E y) is a density that differs
from the true density f(L, R) =af(L, R; Y) only in that s y replaces ^Yo*

= n '/2 (Nval/NI - p,(N5) }, and Nval = E Ai I (W * = w *

fore, we can restrict attention to conditional properties of the inde-

pendent sampling design given U = 0. Because Us = n'/2 {Nval/
9. For all y) in a neighborhood N of u ao, El*[H(sy*)] and
Ns - 7r(W *)} - n'/2{ps(Ns) - ir(W*)}, ps(Ns) - -r(W*)
E=lsupgLNRH(^y)'H(ywRy)] is bounded where t refers to expec= Op(N-') by assumption, and logit { Nval/Ns } is the MLE of ,6

tation with respect to the densityf( L, R; y ot).
Proof of Proposition 6.1

under model (33), conditioning on U = 0 is asymptotically equiv-

alent to conditioning on n-/2 1i S,*,i = 0, where S#* is the scor

for and
model (33). Thus it suffices to show that the asymptotic conFirst, note that E[D(yo, h, X, a g or )] = by (1), (35), (36),
ditional distribution of n 1/2 { t( h, X, ,6' )- a }, given n1-l 2 z, Sa** ,,
(44). Let H*(^y)' = (D(at, h, X, {,t)', S4it')'). Then, by E[D(aoe, h,
equals its unconditional asymptotic distribution. By Proposition
E, y,U0)] = and r(y1, t') bounded below by c, regularity conditions

1-9 hold, with H*(ry) replacing H(Ly). Theorems 2.6 and 3.4 of
Newey and McFadden ( 1993) or corollary 1 of Manski ( 1988, chap.

6. la and the conditional properties of the multivariate normal dis-

tribution, the limiting conditional distribution of n 1/2 { &:(h, X, ,6')

- ao } is equivalent to that of { K(h) } -'nn2 Ii Residi {Resid[D(h,
?k), =
S4j,
proaching 1, there exists a unique solution 6 to 1 Hi* (^y)
0 S* }. But because, by assumption, model (33) with score

8) imply that if H*( y) satisfies 1-7 then, with probability ap-

such that jy is asymptotically linear with influence function

S* is nested within model ir(i/') with score S*, we have S*

= bSJ, for some matrix b, so that Resid{Resid[D(h, w)i Sc],
-E [dH*( yoy) /d8y 'f]-H*( yoy) . By definition of H*(oy ), jy' = (o(h
S } = Resid[D(h, o), Smb. The lemma then follows by Theorem
, 6t)', ,t"). Hence E(h, , ' ) and t' are asymptotically linear with
6.la and 6.le.
influence functionsH-E[D(a,h h, S)/dai'f1 {D(h, )- E[dD(ao,

h, X, ito)/dit"]E[dS4t'>(o)/dit"] -15,J } and -E[dS*(t'o)/dit"]S#*, with
APPENDIX C: ASYMPTOTIC EQUIVALENCE WITH
S,= SVt'= 0). Now, under regularity conditions 6, 8, and 9,
Lemma c.3 of Newey (1990b) implies that -E[H*( ^Yo)/a^~Y']

= E[H*(^yo)Si (yo)], which by theorem 2.2 of Newey (1990a)
implies that j is regular and that -E[7D(ao, h, (a), prb)/litya]

= E[D(h, ka)S+], -E[dD(ato, h, X, i/O)/dat'] = E[D(h, k)S'],

PREVIOUS ESTIMATORS
In this Appendix, we prove Lemma 6.1, that &SMLE of Section 5.1
is a semiparametric estimator in model "obs" and derive the influ-

ence
andc-E[dSi(g ,o)/dit] = E[StS,]. Substituting these identities into

functions of the previously proposed estimators of Sections 4

the expression for the influence function of &(h, X, /) and notingand 6.

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

Robins, Rotnitzky, and Zhao: Regression With Missing Regressors 865

replacing ?F('a,
0; L)a&(hPFCW,
by ?F(b, p; L). ?F(b, p; kPFCW)
L) and L(b, p;
Asymptotic Equivalence of aPFcW
and
In their appendixes, Pepe and Fleming (1991) and Carroll and

A, Lobs) are simply reparameterizations of ?F(a, 0; L) and ?(a,

6; A, Lobs) of(16a) and (16b). Now the semiparametric model "obst
Wand (1991) showed that &apFCw has influence function K - 'BpFcw,
also has likelihood function ?(b, PI, P2, P3; A, Lobs) Thus the

with BPFCW = AX{X - -(1 - p)p'E[OpFCw IX*] } + (1 - A)OPFcW.
asymptotic properties of the semiparametric estimator b(h, 0) and

K = E[BPFCWS'] by Lemma 8.1. Applying Corollary 5.1, we obtain

its variance estimator, as well as the form of the efficient score for

K-'BpFcw = K-'D(h, k), with h(X*) = hpFcw(X*) and ?(W)
b and of the orthogonal complement A' to the nuisance tangent
= OPFcw( W). But, by Proposition 2.2, K-1 D(h, 0) is the influspace for b, do not depend on whether true value of pi is plo

ence function of aY(hpFcw, OPFCW)-

Asymptotic Equivalence of a&MM(q) and a(h, 0)
Clearly, 'aMM(q) has influence function K-'ADMM(q, ao), where

K = E[ADMM(q, ao)Sj]; so, by Proposition 2.2, it suffices to

--Pr(Y = 1) or yo. It follows that parts a and b of Lemma 6.1 hold

for model "obst." Part b for -Yo known and part c follow from { Y1;
i = 1, ..., n} ancillary for b in the parameterization L(b, p;

A, Lobs).

show that D(h, 0) = ADMM(q, ao). Now, given h(X*), D(h, 0)

APPENDIX D: COMPUTATIONALLY CONVENIENT

= ADMM(q, ao), with q(Y, X*, a) -xl-h(X*)e because

SIMULATION ESTIMATORS

E[7rQ(ao) I X*] = 0. Conversely, given q(Y, X*, a), we apply Corollary 5.1 to obtain h(X*) as given by (31) and +(W) 0. Proof

We provide, for any B = b(W, X), easily computed simulation

of the asymptotic equivalence of &MM(q) and a&(h, 0) follows
from Ea(BI W) and Ea,I( BIX*) of Ea,7( BI W) and
estimates

Corollary 5.1 after using a Taylor expansion to show that aMM(q)

has influence function K { ADMM(q, ao)} -(A -) { E[DMM(q,

ao)I W]}.

Ea,,,( B I X*), where E,,,,[* I II denotes expectations with respect to

a law ?F(a, q; L) allowed by the model "full" characterized by
(1). Our estimators Ea,4. I * are based on the identities E[B I X

Proof of the Asymptotic Equivalence of &GM
and a(i, kGM)

= x* =-E[b(Wt, x* )q( Wt, x*)]/E[q( Wt, x*)], with q(Wt,

x*) f(VtIY, x*)f(YIx*)If(Vt, Y), and E(BIW = w)
= E[b(X, w)p(X, w)]/E[p(X, w)], where p(X, w) =f(vt IX, y,

v)ft(yIX,
v)I (V = v), for V discrete and, if V has continuous
A Taylor expansion around -yo and c gives n-"/2DGM,j (ao,
Y)
p(X, w) = f(vt IX, y, v)f(yI X, v)f(X I v)/f(X).
n-1/2 i DGM,i(ao, -yo) - (1 - p)w( yo, 1)'ao,1E(VV')n 1/2 components,
( ^y

-Yo) + op(l) = n / El HGM,j + op(l), with HGM = DGM(aO,These
-yo)identities motivate generating a large number, say n*, of
-Aw(1 - p)p-'(,yo, 1)'ao,1V(X - yOV), where we have used
independent draws L. from ?F(a, i7; L) and defining (a) E,,[B I X*

- /2(- yo) = n- /2P-1 i Ai {E(VV')}-'Vi(Xi - yoVi) + op(l). = j= _ ni b(Wt x*)q( j, X**)/ n*i q(WFj, X*), where

Hence, by a Taylor expansion around ao and Lemma 8.1, 'GM will
q( t j* )- f[vjt I y,j,X*;q74 If[ [Yj I x*; at, ni ]II( W i; a0hf7, 774),
have influence function K-'HGM, K = E[HGMS' I. Because Corollary
f[YjJx*; a, '1iI fjYj - g(x*; a)Ix*; a, '71VI,(Wj; a, n11, '14)
5.1 is also true for model "obs b," we have K -'HGM = K-'D(h, k),

where h(X*) = E(HGMI Y =,X*)-E(HGMl Y= O,X*) = bX*
with b = (b,, b2), b, = (p, 0)', b2 = [(1- p)O, (1 - p) p',
and X = (,yo, 1)'V[Y- ao, 1yoV- ao,2V]. Hence, by Proposition
2.2 and Lemma 8.1, a&(h, k) and aGM have the same influence

-k=: f [VW I Yj, Xk*; n4]f [yjIX *; a, ni]/n*, and (b)Ea,,(BI W
= W) = zJ=_ b(Xj, W)p(X;, w)I>4* 3(Xj, w), where fi(Xj, w)

_f(vtIXj, y, v; n4)f(yIXj, v; a, n7)I (Vj = v) for V discrete
and 3(Xj, w) f(vtlXj, y, v; N4)f(YlXj, v; a, n1)f(XjIv; '12)/
f(Xj; '72) for V with continuous components, with f(Xj; '72)

functions. But because b is constant, &(h, k) = a(i, b `0) andkb-`0
I f(XJ I Vk; '72)/n*. By the central limit theorem, E&,[B I W
= w] and Ea,[BBI* X= x] converge to Ea&,(BI W = w) and
=GME&,dB I X* = x]* at a ( rate. These simulation estimators were
Proof that aMLE of Section 5.1 is a Semiparametric
motivated by a similar estimator proposed by Daniel Rabinowitz
Estimator in Model "obs" When (11) is True

in an unpublished manuscript.

Let 77 = (at Qt, a2t,i;, st), where t E[XV']{E[ VV']} -1

Qt = E[(X- ayt V)?2], a2t = E[(Y- aoo + ao,iX)2], It = E(V),

[Received December 1990. Revised September 1993.1

and t = E[(V- lit)(V- it)'], where expectations are with respect

REFERENCES

to the true distribution of the data. Straightforward but tedious

calculation gives E[d log ?(ao, i7t; A, Lob5)/d(a, ii)] = 0 even ifBeale,

E. M. L., and Little, R. J. A. (1975), "Missing Values in Multivariate
Analysis," Journal of the Royal Statistical Society, Ser. B, 37, 129-145.
Begun, J. M., Hall, W. J., Huang, W. M., and Wellner, J. A. (1983), "In-

(12) is false, where ?( a, 7; A, L0b,) is the likelihood assuming that
(12) were also true. Hence, under our regularity conditions,

formation
(aMLE, 71MLE) will be RAL estimators of (ao, 7t) in model "obs"

when (1 1) is true even if (12) is false.

Proof of Lemma 6.1

and Asymptotic Efficiency in Parametric-nonparametric Mod-

els," The Annals of Statistics, 11, 432-452.
Bickel, P., Klaassen, C. A. J., Ritov, Y., and Wellner, J. A. (1993), Efficient
and Adaptive Inference in Semiparametric Models, Baltimore: Johns
Hopkins University Press.

To prove Lemma 6.1, recall that a (a, b')' and define the odds

ratio function or(X*1) = f(X*1 I Y = l)/f(X*1 = 0 1 Y = 1)/
{f(X-* I Y= O)/f(X * = 0I Y= 0)}, where we assume that 0 is

Breslow, N. E., and Cain, K. C. (1988), "Logistic Regression for Two-Stage
Case-Control Data," Biometrika, 75, 11-20.
Carroll, R. J., Gail, M. H., and Lubin, J. H. (1993), "Case-Control Studies
With Errors in Covariates," Journal oftheAmerican StatisticalAssociation,

in the support of X *_ . Prentice and Pyke (I1979) pointed out that,88, 185-199.

Carroll, R. J., and Wand, M. P. (1991), "Semi-Parametric Estimation in
Logistic Measurement Error Models," Journal of the Royal Statistical
model (2) is equivalent to a semiparametric model indexed by b
Society, Ser. B, 53, 573-587.
and an infinite-dimensional parameter p with the likelihood conCarroll, R. J., Wang, S., and Wang, C. Y. (1993), "Asymptotics for Prospective Analysis of Stratified Case-Control Studies," submitted to Journal
tribution for a single subject, LF(b, p; L) = (p,)Y(l -po)'-y
f( X1 I Y; b, P2)f [Vt iX *,, Y; p3], characterized by the sole of the American Statistical Association.
Chamberlain, G. (1987), "Asymptotic Efficiency in Estimation With Conrestriction that f(X*1 I Y; b, P2) satisfies log{or(X*,)}
ditional Moment Restrictions," Journal of Econometrics, 34, 305-324.
= b'X*,. Thus model "obs" is equivalent to the semiparametric
Cosslett, S. R. (1981), "Efficient Estimation of Discrete Choice Models," in
Structural Analysis of Discrete Data With Econometric Applications, eds.
model with likelihood S( b, p; A, Lobs) given by (16) modified by

with the intercept a unrestricted, model "full" based on the logistic

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

866 Journal of the American Statistical Association, September 1994
C. F. Manski and D. McFadden, Cambridge, MA: MIT Press, pp. 51111.

Dagenais, M. G. (1973), "The Use of Incomplete Observations and Multiple
Regression Analysis: A Generalized Least Squares Approach," Journal
of Econometrics, 1, 317-328.

Ritov, Y., and Wellner, J. A. (1988), "Censoring, Martingales, and the Cox
Model," in Contemporary Mathematics: Statistical Inferencefor Stochastic
Processes (Vol. 80), ed. N. U. Prabhu, Providence, RI: American Mathematical Society, 191-220.
Robins, J. M. (1987), "A Graphical Approach to the Identification and

Flanders, W. D., and Greenland, S. (1991), "Analytic Methods for Two-

Estimation of Causal Parameters in Mortality Studies With Sustained

Stage Case-Control Studies and Other Stratified Designs," Statistics in
Medicine, 10, 739-747.
Gourieroux, C., and Montfort, A. (1981), "On the Problem of Missing Data
in Linear Models," Review of Econometric Study, xlviii, 579-586.
Heitjan, D. F., and Rubin, D. B. (1991), "Ignorability and Coarse Data,"
The Annals of Statistics, 19, 2244-2253.
Horvitz, D. G., and Thompson, D. J. (1952), "A Generalization of Sampling
Without Replacement From a Finite Universe," Journal of the American
Statistical Association, 47, 663-685.
Huber, P. (1985), "Projection Pursuit," The Annals of Statistics, 13, 435-

Exposure Periods," Journal of Chronic Diseases, 40, Supplement 2, 139s161s.
Robins, J. M., and Morgenstern, H. (1987), "The Foundations of Con-

474.

Imbens, G. W. (1992), "An Efficient Method of Moments Estimator for
Discrete Choice Models With Choice-Based Sampling," Econometrica,
60, 1187-1214.
Imbens, G. W., and Lancaster, T. (1991), "Efficient Estimation and Stratified
Sampling," Discussion Paper 1545, Harvard Institute of Economic Research.

Kalbfleisch, J. D., and Lawless, J. F. (1988), "Likelihood Analysis of MultiState Models for Disease Incidence and Mortality," Statistics in Medicine,
7, 149-160.

founding in Epidemiology," Computers and Mathematics with Applications, 14, 869-916.
Robins, J. M., Blevins, D., Ritter, G., and Wulfsohn, M. (1992), "GEstimation of the Effect of Prophylaxis Therapy for Pneumocystis carinii
Pneumonia on the Survival of AIDS Patients," Epidemiology, 3, 319336.
Robins, J. M., Hsieh, F., and Newey, W. (1995), "Semiparametric Efficient
Estimation of a Conditional Density With Missing or Mismeasured Covariates," submitted to the Journal of the Royal Statistical Society,
Sen. B.
Robins, J. M., Mark, S. D., and Newey, W. K. (1992), "Estimating Exposure

Effects by Modelling the Expectation of Exposure Conditional on Confounders," Biometrics, 48, 479-495.
Robins, J. M., and Rotnitzky, A. (1992), "Recovery of Information and
Adjustment for Dependent Censoring Using Surrogate Markers," in AIDS

Epidemiology-Methodological Issues, eds. N. Jewell, K. Dietz, and V.
Farewell, Boston: Birkhauser, pp. 297-331.

Kress, R. (1989), Linear Integral Equations, Berlin: Springer-Verlag.
Robins, J. M. (1993a), "Information Recovery and Bias Adjustment in ProLancaster, T. (1990), "A Paradox in Choice-Base Sampling," working paper,
portional Hazards Regression Analysis of Randomized Trials Using SurBrown University.
rogate Markers," Proceedings of the Biopharmaceutical Section, American
Lin, D. Y., and Ying, Z. (1993), "Cox Regression With Incomplete Covariate
Statistical Association, pp. 24-33.
Measurements," Journal ofthe American Statistical Association, 88, 1341 (1993b), "Analytic Methods for HIV Treatment and Cofactor Ef1349.
fects," in Methodological Issues ofAIDS Behavioral Research, eds. D. G.
Little, R. J. A. (1993), "Regression With Missing X's: A Review. Journal
Ostrow and R. Kessler, New York: Plenum Press, pp. 213-287.
of the American Statistical Association, 87, 1227-1237.
Robins, J. M., Greenland, S., and Rotnitzky, A. (1992), "Parametric Models
Little, R. J. A., and Rubin, D. (1987), Statistical Analysis With Missing
for Nonmonotone Missing Data Processes," Technical Report, Harvard
Data, New York: John Wiley.
School of Public Health, Dept. of Epidemiology.
Manski, C. F. (1988), Analog Estimation Methods in Econometrics, New
Robins, J. M., and Rotnitzky, A. (1994), "Semiparametric Efficiency in
York: Chapman and Hall.
Multivariate Regression Models With Missing Data," Journal of the
American Statistical Association.
Manski, C. F., and Lerman, S. (1977), "The Estimation of Choice ProbaRobins, J. M., Rotnitzky, A., and Zhao, L. P. (1994), "Analysis of Semibilities From Choice-Based Samples," Econometrica, 45, 1977-1988.
Manski, C. F., and McFadden, D. (1981), "Alternative Estimators and Samparametric Regression Models for Repeated Outcomes in the Presence
ple Designs for Discrete Choice Analysis," in Structural Analysis ofDiscrete of Missing Data," Journal of the American Statistical Association.
Data With Econometric Applications, eds. C. F. Manski and D. McFadden, Rosenbaum, P. R. (1984), "The Consequences of Adjustment for a ConCambridge, MA: MIT Press, pp. 2-50.
commitant Variable That Has Been Adversely Affected by Treatment,"
Newey, W. K. (1990a), "Semiparametric Efficiency Bounds," Journal of
Journal of the Royal Statistical Society, Ser. A, 147, 656-666.
Applied Econometrics, 5, 99-135.
(1987), "Model-Based Direct Adjustment," Journal of the American
(1990b), "Efficient Estimation of Tobit Models Under Conditional
Statistical Association, 82, 387-394.
Symmetry," in Semiparametric and Nonparametric Methods in EconoRotnitzky, A., and Robins, J. M. (1993), "Efficient Semiparametric Estimetrics and Statistics, eds. W. Barnett, J. Powell, and G. Tauchen, Cammation With Missing Outcomes and Surrogate Data," technical report,
Harvard School of Public Health, Dept. of Epidemiology.
bridge: Cambridge University Press, pp. 291-336.
(1993a), "The Asymptotic Variance of Semiparametric Estimators,"
Rubin, D. B. (1976), "Inference and Missing Data," Biometrika, 63, 581submitted to Econometrica.
592.
(1993b), "Series Estimation of Regression Functionals," MIT mimeo.
Stampfer, M. J., Willett, W. C., Colditz, G. A., Rosner, B., Speizer, F. E.,
Newey, W. K., and McFadden, D. (1993), "Estimation in Large Samples,"
and Hennekens, C. H. (1985), "A Prospective Study of Postmenopausal
Estrogen Therapy and Coronary Heart Disease," New England Journal
in Handbook of Econometrics, Vol. 4, eds. D. McFadden and R. Engler,

of Medicine, 313, 1044-1049.

Amsterdam: North-Holland.

Newey, W. K., and Powell, J. (1990), "Efficient Estimation of Linear and
Type I Censored Regression Models Under Conditional Quantile Restrictions," Econometric Theory, 6, 295-317.
Pepe, M. S., and Fleming, T. R. (1991), "A Nonparametric Method for
Dealing With Mismeasured Covariate Data," Journal of the American

Statistical Association, 86, 108-113.
Prentice, R. L., and Pyke, R. (1979), "Logistic Disease Incidence Models
and Case-Control Studies," Biometrika, 66, 403-41 1.
Prentice, R. L. (1986), "A Case-cohort Design for Epidemiologic Cohort
Studies and Disease Prevention Trials," Biometrika, 73, 1-11.
Pugh, M., Robins, J. M., Lipsitz, S., and Harrington, D. (1992), "Inference
in the Cox Proportional Hazards Model With Missing Covariates," Technical Report, Harvard School of Public Health, Dept. of Biostatistics.

Thomas, D. C. (1977), "In Appendix to Liddell, F. D. K., McDonald,

J. C., and Thomas, D. C.," Journal of the Royal Statistical Society, Ser.
B, 140, 469-490.

Van der Laan, M. J. (1993), Efficient and Inefficient Estimation in Semiparametric Models, Doctoral Dissertation, University of Utrecht, The
Netherlands.

Weinberg, C. R. (1993), "Towards a Clearer Definition of Confounding,"

American Journal of Epidemiology, 137, 1-3.
Weinberg, C. R., and Wacholder, S. (1993), "Prospective Analysis of CaseControl Data Under General Multiplicative-Intercept Risk Models," submitted to Biometrika.

Zhao, L. P., and Lipsitz, S. (1992), "Design and Analysis of Two-Stage
Studies," Statistics in Medicine, 11, 769-782.

This content downloaded from 206.253.207.235 on Tue, 19 Feb 2019 22:46:58 UTC
All use subject to https://about.jstor.org/terms

