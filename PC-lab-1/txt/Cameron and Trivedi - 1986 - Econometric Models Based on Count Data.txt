Econometric Models Based on Count Data: Comparisons and Applications of Some Estimators
and Tests
Author(s): A. Colin Cameron and Pravin K. Trivedi
Source: Journal of Applied Econometrics, Vol. 1, No. 1 (Jan., 1986), pp. 29-53
Published by: John Wiley & Sons
Stable URL: http://www.jstor.org/stable/2096536
Accessed: 08/10/2009 12:46
Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at
http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless
you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you
may use content in the JSTOR archive only for your personal, non-commercial use.
Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
http://www.jstor.org/action/showPublisher?publisherCode=jwiley.
Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed
page of such transmission.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact support@jstor.org.

John Wiley & Sons is collaborating with JSTOR to digitize, preserve and extend access to Journal of Applied
Econometrics.

http://www.jstor.org

JOURNAL OF APPLIED ECONOMETRICS,VOL. 1, 29-53 (1986)

ECONOMETRIC MODELS BASED ON COUNT DATA:
COMPARISONS AND APPLICATIONS OF SOME
ESTIMATORS AND TESTS
A. COLIN CAMERON
Department of Economics, Ohio State University, U.S.A.

AND
PRAVIN K. TRIVEDI
Department of Economics, Faculty of Economics and Commerce, Australian National University, Canberra, Australia
2600

SUMMARY
This paper deals with specification, estimation and tests of single equation reduced form type equations
in which the dependent variable takes only non-negative integer values. Beginning with Poisson and
compound Poisson models, which involve strong assumptions, a variety of possible stochastic models and
their implications are discussed. A number of estimators and their properties are considered in the light
of uncertainty about the data generation process. The paper also considers the role of tests in sequential
revision of the model specification beginning with the Poisson case and provides a detailed application of
the estimators and tests to a model of the number of doctor consultations.

1. INTRODUCTION

The objective of this paper is to discuss the estimation and testing of count data models from
the viewpoint

of an applied econometrician

who may be concerned

predominantly

with

practical aspects of their use. A number of articles on these models, in which the dependent
variable takes only non-negative integer values corresponding to the number of events
occurring in a given interval, have appeared recently in the econometrics literature. These
include those by Gilbert (1979) and Hausman, Hall and Griliches (1984). We believe that, the
apparent novelty of count data models notwithstanding, many of the insights and lessons learnt
from standard applied econometric researchcarry over to this area.
In particular, the idea that applied econometric analysis involves an iterative modelling cycle

consisting of specification, evaluation, comparison and eventual model revision is now widely
accepted (Hendry and Wallis, 1984). It is therefore useful to discuss count data models
from the viewpoint of an econometrician who wishes to go through this process. For the
normal linear regression model a great deal has been learnt about the process of model
estimation and post-estimation model evaluation. Since most count data models can be
accommodated within an extended or generalized linear model framework, many of these
insights for the normal linear regression model carry over to count data models. McCullagh
and Nelder (1983) draw many parallels between the regression model on the one hand and
count data models and limited dependent variable models on the other.
The modelling of random counts is widespread and long established in the biometric
literature (see Patil (1970) for an extensive introduction). However, as Gilbert (1979) and
Hausman, Hall and Griliches (1984), henceforth referred to as HHG (1984), have pointed
0883-7252/86/010029-25$02.50
Â© 1986 by John Wiley & Sons, Ltd.

Received February 1985
Final revision June 1985

30

A. C. CAMERON AND P. K. TRIVEDI

out, there is also much scope for application of count data models in econometric research.
Examples are the number of purchases per period (Gilbert), the number of patents applied for
(HHG, 1984) the number of visits to doctors, admissions to hospital and medicines taken
(Cameron, Trivedi, Milne and Piggott (1984), henceforth referred to as Cameron et al.
(1984)), the number of spells of unemployment, the number of strikes in a month and so forth.
In some cases the nature of data available to econometricians will dictate the use of count
data models, regardless of economic theory. In other cases the nature of economic decision
processes may actually lead to econometric models of variables naturally measured as counts.
An example is provided by models which lead to corner solutions and hence to discrete choice,
but where the choice may be made several times in the time interval studied, or the number of
times the choice is exercised is itself optimally chosen at the beginning of the period. Yet
another possibility is to view counted data as a discrete categorical proxy variable for an
unobserved continuous variable. In these contexts it is natural to attempt to model the
probability that the economic event will occur n = 0,1,2, . . . times in a given time interval.

Whatever the ability of economic theory to suggest variables that explain the count data of
interest, it will generally offer little guidance in selection of a stochastic model for the count
data. Furthermore, the extensive probabilistic literature on modelling discrete random events
will usually be inappropriate. This is to be contrasted with biometrics where there may be
strong reasons (such as genetics theory) for using a particular stochastic model. Thus, whereas
the common practice of using the Poisson distribution as the starting point may be reasonable
in biometric applications, the use of this distribution for modelling non-negative
integer-valued economic events involves quite strong and empirically questionable
assumptions (restrictions). A similar warning could be given to the econometrician using
continuous data, who often regards the choice of the normal linear regression model as
uncontentious (with effort directed more towards controlling for possible heteroscedasticity,
autocorrelation or endogeneity). But when count data are used, considerably more attention
needs to be paid to model specification and evaluation. Section 2 of the paper therefore
considers the specification question in the context of a small number of empirically interesting
count data models.
Many count data models are non-linear, though they can be usefully viewed, a la Nelder and
Wedderburn (1972), as 'generalized linear models' and their extensions. The discussion of
estimation of these models is closely related to the general theory of estimation of possibly
misspecified non-linear models. Section 3 of the paper exposits the application of this theory
to count data models and draws out its implications for applied econometric work. Section 4
considers how one may evaluate certain aspects of an estimated model using diagnostic tests
that will reveal its weaknesses and point to desirable respecifications. Section 5 provides an
illustrative application of a sequential modelling strategy that is espoused in the earlier
sections of the paper and a comparison of some estimators. Section 6 concludes.

2. SPECIFICATION ISSUES
In the analysis of count data it is natural to consider discrete models, including not only the
Poisson and compound Poisson distributions but also a large number of others which are
extensively discussed by Patil (1970). However, certain categorical models such as the ordinal
probit model (McKelvey and Zavoina, 1975) and even some continuous models may be
appropriate in some cases. We concentrate on discrete data models, but do consider the
ordinal probit model in Section 5 as well as models based on the normal distribution.

ECONOMETRIC MODELS OF COUNT DATA

31

2.1. The Basic Poisson Model
Let Y, denote the number of occurrences, for the ith of N individuals, of an event of interest in
a given interval of time, Yi = 0,1,2,... Let y(t, t + dt) denote the number of events observed
in the interval (t, t + dt). If
Pr[y(t,t

+ dt) = 0] = 1 - Adt + O(dt)

and
Pr[y(t, t + dt) = 1] = Adt + O(dt)

so that
=
O(dt),
Pr[y(t, t + dt) > 2]

as dt -O 0

then the number of events in an interval of a given length is Poisson distributed with the
probability density
Pr(Y, = y,) = e- A/y!

,

y,

= 0,1,2, . . .,

i = 1,2, . . . ,N

(1)

where y, is the realized value of the random variable. This is a one-parameter distribution with
mean and variance of Y, equal to A,. To incorporate

exogenous

including a constant, the parameter A,is specified to bet
, = exp(X,P)

variables X,(j = 1, . . . K),

(2)

In applied work the Poisson regression model is restrictive in several ways. First, itis based
on the assumption that events occur independently over time. The independence assumption
may break down in several ways. There may be a form of dynamic dependence between the
occurrence of successive events. Prior occurrence of an event, such as an accident or illness,
may raise the probability of subsequent occurrence of the same or similar event. In the context
of unemployment spells this form of dynamic interdependence has been dubbed occurrence
dependence by Heckman and Borjas (1980). In the context of biometric literature the
contagion model considers the possibility that an occurrence of an event, such as an accident or
illness, may subsequently modify the probability of occurrence of a similar event. Xekalaki

(1983) provides several references to this literature. Yet another mode of dynamic
interdependence is inherent in the notion that events occur in 'spells' and the spells themselves
occur according to some probability law, whereas the events within a given spell, which occur
according to a different probability law, may be dependent. This model is discussed by
Cresswell and Froggatt (1963) and Xekalaki (1983). For example, the event that A saw his
doctor on Tuesday may not be independent of the event that he also saw the doctor on
Monday, if both events arise out of the occurrence of a single spell of illness. Independence
implies that Pr (A sees doctor on Tuesday | A saw doctor on Monday) = Pr (A sees doctor on
Tuesday | A did not see doctor on Monday). The breakdown of the independence assumption
raises fundamental issues which are discussed further later in the section.
Secondly, the assumption that the conditional mean and variance of Y, given X, are equal
may also be too strong and hence fail to account for the overdispersion-the variance exceeds
the mean-that characterizes many data sets.t In applied work it is desirable to test the
Poisson restriction and to relax it if appropriate. Inappropriate imposition of this restriction
may produce spuriously small estimated standard errors of p.
tThe exponential function is used to ensure the non-negativity of v,. Gilbert (1979) alternatively specified a linear
function but this can cause obvious computational difficulties.
tSee Gourieroux, Monfort and Trognon (1984b, p. 703, footnote 3).

32

A. C. CAMERON AND P. K. TRIVEDI

2.2. Compound Poisson Models
One way to relax this restriction is to allow for unexplained randomness in A,by replacing (2)
by the stochastic equation
In Ai = X,p + ,l

(3)

where the error term could reflect a specification error such as unobserved omitted exogenous
variables, as suggested by Gourieroux, Montfort and Trognon (1984a,b), henceforth referred
to as GMT (1984-), or more straightforwardly simply intrinsic randomness, as in HHG
(1984). Let g (e,) denote the probability density function for e. Then the marginal density of Y,
can be obtained by integrating with respect to e,:
Pr[Y, = yj]

JPr[ Y, = y, IX,,6,]g(()de,
ee-exp(Xi+exp(X1p

+ e)YI g(e)de

(4)

Expression (4) defines a compound Poisson distribution whose precise form depends upon the
specific choice of g(ei). For certain parametric forms, such as the gamma which we shall
examine in the next section, a closed form expression for (4) can be obtained; but for other
choices, such as the standard normal density, the resultant compound Poisson might not have a
closed form and hence be computationally cumbersome.
Compound Poisson distributions provide a natural generalization of the basic Poisson
models. Often they are motivated by a desire for greater flexibility, specifically a desire to
account for frequently observed overdispersion in data and to provide a better fit. This is
achieved essentially by appealing to aggregation over a heterogeneous population as the
source of overdispersion. But unless the parametric form of g(e,) can be motivated from
fundamental considerations there is some arbitrariness involved here and it becomes relevant
to explore the consequences of misspecifying the model in this important respect. GMT
(1984a) have provided a theoretical analysis that can be applied to this issue.
2.3. Negative Binomial Models
Although the negative binomial model can be motivated in a number of different ways, see
Boswell and Patil (1970), its representation as a specific compound Poisson distribution has
been common (HHG, 1984, pp. 921-922). One way is to allow for inter-person heterogeneity
by allowing Ai to vary randomly according to a probability law. For example, if g(c), or
equivalently f(A,), is assumed to be a gamma distribution, then the integration in (4) leads to
the negative binomial. The specific parameterization of the resulting form is determined by the
parameterization of the gamma distribution. In the i.i.d. case where no exogenous variables
appear in the model the resulting difference is of no consequence, but in the regression case
the differences in parameterization deserve some comment. We use the 'index'
parameterization of the gamma distribution. Then A,- Gamma (/<, v,) with density (for A, > 0,
q, > 0, v, > 0)
f(A);t,
E[Ai] -/,

-

) ( Â¢, ) exp
and Var(A,)=

1

2

(5)
(6)

33

ECONOMETRIC MODELS OF COUNT DATA

The parameter < is the mean and v, is called the index or precision parameter. It is readily
shown that

y
Pr[Y, yJ

f Pr[YI = y
+

F(yi

I A,]f(/)di
vi

vi)

f(yi + 1)r(vi) Vi+

v~

i

Yi

vv+ <

with
E[Y]

=

(8)

and
Var(Y) - X

+

- ^
VI

(9)

(7) is one possible parameterization of the negative binomial distribution. Since Xl > 0 and
vi > 0 it is clear that the variance exceeds the mean, so the model allows for overdispersion
Different negative binomial regression models can be generated by linking the parameters
to the explanatory variables X, in different ways.
i and v, of the underlying distribution for 2Ai
the
mean
a natural specification is E[Yj] - exp(X,P),
of
to
ensure
non-negativity
Again
obtained by letting 0, = exp(Xip). A wide range of variance-mean relationships can be
obtained by letting v, = (1/a)(exp(XiP))k, for o > 0 and arbitrary constant k. Then Var( Y) =
exp(X/iP) + a exp((2 - k)X,I)

-= E[Y,]

+

(E[Y]j)2-k.

The model we call Negbin I is obtained by setting k =- 1. Then Var(Y,) - (1 + a)E[ Y]j,
implying a constant variance-mean ratio. This model is given by McCullagh and Nelder (1983,
p. 132) and is used by HHG (1984, p. 922).t The model we call Negbin II is obtained by
setting k - 0. Then Var(Y1) = E[Y](1 + aE[YY]), so that the variance-mean ratio is linear in
the mean. This model is given by McCullagh and Nelder (1983, p. 194) and is used by GMT
(1984b) and HHG (1984, p. 917). Clearly there are many more possibilities.
The two parameterizations imply different assumptions about the functional form of
heteroscedasticity-a point which is not emphasized in the literature-and hence in general
will lead to different estimates of the parameter P.
The two alternative specifications of the gamma heterogeneity distribution amount to
different parameterizations in the univariate model, but where a regression component is
present they lead to different models. This difference is also relevant when we consider the test
of the null hypothesis that the distribution of Y, is Poisson against the alternative that it is
negative binomial.
2.4. Towards More General Count Data Models
The negative binomial model is simply one example of a generalised Poisson model.
Generalization can proceed along two routes which are by no means mutually exclusive.
First, the strong independence assumption of the Poisson model may be relaxed by choosing
one of the alternative models used in the biometric literature. These include the
tHHG use a different parameterization of the gamma distribution to obtain a negative binomial model with
E[Yj] = (1/6)exp(X,0) and Var(Y) = (1 + 1/6)(1/6)exp(X,O). We suppose that X, includes a constant. Then
partitioning XiO = 00 + X1l01 and XlP = fo + X1l,ii, the model of HHG is the same as that in the text with a = (1/6),
o = 00 log and l1 = 01.

34

A. C. CAMERON AND P. K. TRIVEDI

above-mentioned 'true contagion' and 'spells' models. Yet another possibility would be to
retain the independence assumption but choose a distribution other than the Poisson.
The negative binomial model as developed and applied in the econometric literature is
essentially the 'apparent contagion' model of biometrics. According to this model individuals
have constant but unequal probability of experiencing an event. A different model is the 'true
contagion' model where all individuals initially have the same probability of experiencing an
event but this is modified by prior occurrence of events. Another model is the 'proneness'
model according to which individuals are heterogeneous in respect of their proneness to
certain events, with this heterogeneity attributed to individual and/or environmental factors.
Yet another alternative mentioned earlier is a 'spells' model according to which events occur in
clusters and are dependent.
A second alternative approach to generalization consists of retaining the basic Poisson
model of events and making alternative more general heterogeneity assumptions. This simply
means that one attempts to employ increasingly general compound Poisson distributions to fit
the data. The statistical literature.is replete with examples of compound Poisson distributions;
see Johnson and Kotz (1972) for a survey.
Depending upon which of the two approaches considered above is followed, we are led to
problems of model testing and discrimination which might be dealt with differently according
to the aims and objectives of the investigator. Using standard econometric terminology one
may say that the problem of discriminating between alternative models of events for a given
individual involves a comparison of alternative structural hypotheses, whereas the problem of
discriminating between alternative (compound Poisson) models for a given sample of
individuals involves a comparison of different reduced forms. The problem of discriminating
between structural hypotheses of the kind mentioned above is essentially a problem of
identification and may not be solvable for any one cross-section data set. An example is given
below. The problem of choosing between alternative reduced form models essentially on the
basis of goodness of fit is a less complex problem. However, from an empirical viewpoint it is
desirable to understand the effects of both kinds of misspecification if only to motivate
different tests of misspecification. The issue is reconsidered in Section 4.
3. ESTIMATION
3.1. Overview
Leading references on the estimation of count data models specified from a viewpoint
congenial to that of the applied econometrician are Nelder and Wedderburn (1972),
Wedderburn (1974), McCullagh (1983), McCullagh and Nelder (1983), GMT (1984a, 1984b,
1984c) and HHG (1984). We assume that the applied econometrician will be interested in
choosing from a range of feasible estimators on the basis of their theoretical properties, the
effect of specification errors on the properties of the selected estimator and computational
cost. Our task here is to summarize briefly some of the known results relating to these issues,
avoiding excessive detail but providing sufficient background to make comprehensible the
discussion in the later sections of this paper. To some extent this task has been fulfilled by
GMT (1984c).
The essential choice in estimation is between maximum likelihood methods on the one
hand, based on strong distributional assumptions, and on pseudo-maximum-likelihood or
maximum quasi-likelihood methods on the other, based on weaker assumptions.
If the probability distribution of the variable y, is known to belong to a specified parametric

ECONOMETRIC MODELS OF COUNT DATA

35

family, that is, the data generation process is known, and the likelihood function is
well-behaved, maximum likelihood (ML) is the obvious estimation procedure.
If the specified probability distribution is not necessarily the 'true' one and the ML method
is applied as if the 'true' distribution had been specified, then following GMT (1984a,b,c), we
shall refer to the method as 'pseudo' ML (PML). In general there is no reason to believe that
the PML estimator will be consistent-model
misspecification can lead to inconsistent
estimators. But, in the special case where the specified distribution for y, lies in the linear
exponential family, GMT (1984a) show that, regardless of the true distribution for y,, the PML
estimator is consistent provided only that the mean is correctly specified.t
Some commonly used distributions are members of the linear exponential family with an
additional nuisance parameter. If this nuisance parameter can be related to the mean and
variance in a certain way, GMT (1984a) propose the quasi-generalized pseudomaximum-likelihood (QGPML) estimator. This estimator is consistent if both the mean
and variance are correctly specified.t
The results of GMT (1984a) are closely related to those of McCullagh (1983), who derived
the asymptotic distribution for the maximum quasi-likelihood (MQL) estimator defined by
Wedderburn (1974). The quasi-likelihood is defined implicitly as the sum of N subfunctions,
each subfunction having derivative proportional to (y, minus E [y]) divided by the variance of
y1. It can be shown that the PMLE is a special case of the MQLE, and the QGPMLE permits
more complex mean-variance relationships than does the MQLE.
The main advantages of the PML and QGPML estimators are that they require fewer
distributional assumptions and may be computationally less burdensome than ML estimators.
The disadvantages are that if the complete distribution for the data can be correctly specified
the PMLE will be (usually) less efficient than the QGPMLE which in turn will be (usually) less
efficient than the fully efficient MLE, and that for model uses such as prediction knowledge of
the mean and (possibly) the variance may be much less informative than knowledge of the
complete distribution.
3.2. Properties of Estimators
The presentation here is based on GMT (1984a), who use the results of Burguette, Gallant
and Souza (1982) on the properties of extremum estimators which are obtained by maximizing
a stochastic objective function.Â§ For completeness we present results for the multivariate case
where y, is a G-dimensional vector, though in our applications to count data y, will be scalar.
ML estimator
The maximum likelihood (ML) estimator 6 maximizes the likelihood function, or
tA well-known example is PML estimation of Io based on assuming thaty, is distributed as Normal (X,(), a') with 2
known. Then
p

N
E
^xix,

-1

N
E

XTy

is consistent for Io even if y has some other distribution, as long as E(y,) = X,Po.
tA classic example is regression analysis when the variance is proportional to the square of the mean. Then y, is
specified to be distributed as Normal (X,po, ,(XIJo)2), and the QGPMLE for hois an empirical (or quasi) generalized
least squares estimator.
Â§Extremum estimators can be viewed as generalizing the results of Jennrich (1969) and are given an excellent
exposition by Amemiya (1985). A special feature of the study of Burguette, Gallant and Souza (1982) is that {X,} are
assumed to be i.i.d. random variables, so that asymptotic variance expressions involve the expectation Ex.

36

A. C. CAMERON AND P. K. TRIVEDI

equivalently maximized
N

E

i= 1

log l(yi, X1, 0)

(10)

where l(y, X, 00) is the specified conditional probability density for y given X.
If the specified density is the true density, then it can be shown under very general
-do) .A'(0, /o 1), where
conditions that 6 is consistent for 00, and \/N(6

-EXE0

log l(y,X,0)
r=ExEf~oa

-_2

is the infor

mat

o

rix, and Eo is the expectation with respect to the density l(y, X, 00).

PML estimators
When the specified density is n'ot the true density, Huber (1967) and White (1982) show
that 0 converges to 0* which maximizes ExE [ log (y, X, 0) ], where E* denotes the expectation
with respect to the true conditional density of y given X, and x/N(O - 0,) 4 .A/(0, -1 fy-1)
Where
-

i2 logl(y,X,0)
E *0I*0T

and
log (y,X,0)

a

-=EE
*

a

0, -

Â»0.

-

t

log (y,X,0)

Clearly 0* 0 00 necessarily, and 0 may be inconsistent for 00.
The fundamental contribution of GMT (1984a) is to give conditions under which 0, = 00.
We partition 0 into (11,a) and suppose that (a) the conditional mean of y, is f(X,, P0), (b) the
remaining parameters a are specified and need not be estimated and (c) the specified density
l(y, X, P0)is of the form l(y,f(X, Po)), where l(y, tt)is a member of a linear exponential family
(LEF) with mean parameterization, so that
l(y, ,t) = exp{A(it)

+ B(y) + C(it)y}

(11)

where it can be shown that
E[y] = t=

-

a()

(C

and

Var(y) = (,C()-1

The pseudo-maximum-likelihood (PML) estimator i based on an LEF maximizes the
likelihood function of the model defined by (a), (b) and (c), or equivalently maximizes
N

[A (f(Xi,j)) + C(f(X1,P))y1]

E
i=

where E[yi

IX]

(12)

1

= f(X,, P0).

tWhite (1982) calls b a 'quasi' MLE. We follow GMT in calling . a 'pseudo' MLE, and use 'quasi' in a different sense
below.

37

ECONOMETRIC MODELS OF COUNT DATA

If the conditional mean is correctly specified, i.e. E* [y, | X] = f(Xi, Po), then p is consistent
for Po, and VN(# - Po) A4 IV(0, J AIJ '), where
'
af "af
af=
I = Ex J "Ex xs-1
~,

=

_T
aE

_ap
a

apT-,

f _Af(XPI)
alP

^a1

a
(_/C(f(x,p))
pO'

af(x,IP)

apT

iPO,

E is the specified conditional variance-covariance matrix for y, and Q* is the true one.

The essential result is that regardless of other properties of the true conditional distribution
of y, provided the conditional mean is correctly specified, the PMLE is consistent. The
asymptotic distribution of the PMLE will, however, depend on other properties of the true
distribution for y, since I depends on Q*. If Q* is unknown we can extend the results of Eicker
(1967) and White (1980) for regression models based on the normal distribution to those
based on an LEF and consistently estimate I by

I

N

- f(\R
-(f/x?
l(Yv-t(xip)

1El
N1
B

apS

apT

i=

where

and ti

ifi =af(x,,l)

=

c(f((X,p)) )-1

Alternatively, if the true Q* = .(Xi, P0)we replace (Yi - f(Xi, p))2 in I by Qg(Xi,P0). In either
case a consistent estimate for J is

N/i

=ap

apT

QGPML estimators
The LEF includes the normal (with a2 specified), Poisson, negative binomial (with v in (7)
specified), binomial (with n given) and gamma (with v in (5) specified) distributions. Some of
these distributions include a nuisance parameter. PML estimators based on these distributions
will be consistent for Poregardless of the value specified for the nuisance parameter. However,
if the nuisance parameter could be estimated from the data, rather than arbitrarily specified a
value, a more efficient estimator for Po may be possible.
GMT (1984a) propose a method to do this. We partition 0 into (P, a) and suppose that (a)
the conditional mean of y, is f(Xi, po); (b) the conditional variance of yi is Q(Xi, po, ao); (c) the
specified density /(y, X, Po, ao) is of the form l(y,f(Xi, Po0), (f(Xi, PIo), (Xi, Po, ao))), where
1(y, ,u, ) is a member of an LEF with nuisance parameter (and mean parameterization), so that
l(y, it, ) = exp{A(tt, n) + B(y, n) + C(t, q)y}
(13)
=
p
it
and
where can be shown that E[y]
and Var(y)
(3C(t, l)/iu) 1;
(d) q
4(A, Q) is a
q
function
of
and
for
a
differentiable
defines,
any given ,, one-to-one relationship
p and Q
between q and Q.
The quasi-generalized pseudo-maximum-likelihood (QGPML) estimator A based on LEF

38

A. C. CAMERON AND P. K. TRIVEDI

with nuisance parameter is obtained by maximizing the likelihood of the model defined by (a),
(b), (c) and (d), with ll(Xi, Po, ao)-=-(f(Xi, Po), fl(Xi, Po, ao)) replaced by ll(Xi, , a) where P
and a are strongly consistent estimates of Po and ao of order O(N-~). Equivalently, j
maximizes
N

> [A(f(X,

i= 1

P), rl(Xi,,da))

+ C(f(Xi,b),

(14)

qi(X,,a))y,]

where
E[yi

I X] = f(Xi,lo) and Var(y, I X) = Q(Xi,Po0,0) = (

(Xj,oo0)

If both the conditional mean and variance of y, are correctly specified, then p is consistent
for P0and \/N(p - P0) 4>. (0, K-1), where

K

Et

a-f E_

af

af=

Af(x,b")

and El

(x,,P0 ,0)

A consistent estimate for K is given by

A

N
N1
f

The initial consistent estimate p for P0 may be the PMLE based on any LEF. From this a
consistent estimator & for a may be formed, see below for examples.
Comparison of estimators
The QGPMLE p has a variance-covariance matrix which depends only on the functional
forms for the conditional mean and variance of y, so a very large number of QGPML
estimators based on different objective functions of the form (14) and different initial
consistent parameter estimates will have the same asymptotic distribution.
If the specified conditional mean and variance of y, are the true ones, the QGPMLE for P0
attains the lower bound for the variance-covariance matrix of the PMLE for P0. However,
although the QGPMLE is then more efficient than the PMLE it is not necessarily efficient. In
the special case where the true density is itself a member of an LEF of form (13), with
t = f(X, P) and 11= ql(X, a) (equal to those specified for the QGPMLE), the QGPMLE for P0
is asymptotically equivalent to the MLE for P0 (obtained by maximizing the true likelihood
function jointly with respect to P and a), and hence is fully efficient. Note that for a generalized
linear model ql(X, P, a) is of the simpler form Tl(X, a).t
tGeneralized linear models (GLM) are given a comprehensive treatment by McCullagh and Nelder (1983). The scalar
dependent variable y is assumed to have a density of the form
l+(y,/,j)

= exp

(a) (yr

-

b(r)) + c(y,a)

where it can be shown that ,/ = E[y] =b'(7r). For a given, l+(y, 7r) is a member of a LEF with 'natural'
=
parameterization. To link this with GMT who use the 'mean' parameterization, invert b'(r) = / to obtain 7r d(p)
and hence
d(+)
+ c(y'oc) + y
a(a)
a(a) +
For x a freely varying parameter this is a special case of the LEF with nuisance parameter defined in (13), with q a
function of x alone.
1'(yv,jx) = exp-l+(y

b(d(0))

ECONOMETRIC MODELS OF COUNT DATA

39

Now define the pseudo joint MLE (PJMLE) to be the estimator obtained by maximizing a
possibly misspecified likelihood function with respect to pIand a.
The PJMLE for Pois consistent and fully efficient if the specified conditional mean, variance
and distribution of yi are the true ones. If the distribution is misspecified the PJMLE may be
inconsistent. One trivial example where it is still consistent is when there is no nuisance
parameter, so that maximization of the likelihood is with respect to P alone, and the specified
is
case for Poisson regression models.
likelihood function is a member of an LEF-this the
Another example of consistency of the PJMLE forP0 despite incorrect specification of the
distribution is when the specified distribution is a member of a quadratic exponential family
normal distribution but not for the
case fore
(GMT, 1984a, p. 691)-this
thcas
this is
If
binomial
distribution.
negative
additionally i1(X, f, a) = q(X, a), it can be shown that in this
case the QGPMLE will be asymptotically equivalent to the PJMLE though both may be
inefficient.t
4. SOME MODEL EVALUATION PROCEDURES
As in standard regression analysis, in modelling count data it is desirable to supplement
estimation with additional tests to determine whether the fitted model is adequate and
whether a specific deficiency of any initially entertained model can be removed by progression
to a less restrictive model. Alternatively one may wish to begin with a relatively unrestricted
model and proceed to a more restrictive, but data coherent, model on grounds of parsimony
and/or statistical efficiency. This section considers a number of tests which are intended to help
such a specification search.
4.1. Discrimination Between Rival Structural Hypotheses
If the objective of the researcher is to evaluate the adequacy of the independence assumption
of the Poisson model against alternative assumptions which allow for interdependence
between events, then generally it will not be possible to do so on the basis of goodness of fit
tests alone applied to the reduced form compound Poisson models. An example illustrating the
fundamental identification problem involved was provided by Irwin (1941) who showed that a
univariate negative binomial distribution for accidents can be derived as a compound Poisson
distribution with a gamma compounder, or as a true contagion model in which the probability
of an individual having an accident depends upon the number of previous accidents sustained.
A more recent example is provided by Xekalaki (1983), also in the context of accident theory.
The author considers a particular three-parameter distribution for accidents, the 'univariate
generalized Waring distribution', UGWD, and shows that it is consistent with 'proneness',
'contagion' and 'spells' models. So a good fit of the UGWD to data is of no help in
discriminating between rival structural hypotheses. (Note that both these examples refer to
models without a regression component.)
A necessary condition for discriminating between compound Poisson models and rival
models which incorporate dependence between events is that we have available more detailed
data, possibly panel data. Intuitively this should be obvious, since an assumption about
dynamic interdependence between events implies something about the time intervals between
successive events. It is well known that if the process generating events is a Poisson process
then the time between successive events has an exponential distribution with a constant hazard
tAn example is that POLS is asymptotically equivalent to the estimator for Poobtained by; maximizing jointly w.r.t. p
and a2 the likelihood function based on assuming yi ~V(Xi, Po, as), even if the normality assumption is incorrect.

40

A. C. CAMERON AND P. K. TRIVEDI

function and, furthermore, the times between events are independent. Therefore the Poisson
process implies that the times between successive events are i.i.d. exponential random
variables. Of course these times may be i.i.d. but not necessarily exponential. It seems
appropriate, therefore, to first test whether the process is a renewal process before testing
whether it is a Poisson renewal process. (A process in which intervals between events are i.i.d.
is a renewal process.) Tests for renewal processes
for generally
and
Poisson
renewal
process
specifically, based on information on intervals between events, are discussed by Cox and Lewis
(1966) at length and by Lawless (1982, Chapter 10.2) more briefly. The cases discussed there
do not include models with a regression component and do not allow for heterogeneity, but
extensions and generalizations may be possible. To implement such tests one requires more
data than a single cross-section. For example, one test is based on the first serial correlation
coefficient between lengths of successive time intervals. Under the null hypothesis that the
process is a renewal process its value would be zero. Clearly, to test this one requires panel
data.
That the availability of panel data constitutes a necessary condition for discriminating
between 'true' and 'spurious' (or 'apparent') contagion has been recognized at least since the
work of Bates and Neyman (1952). In the context of the problem of discrimination between
'true' and 'spurious' (unemployment) duration dependence, Heckman and Singer (1984) have
also emphasized the value of panel data and provided conditions for identifiability. Extension
of this analysis to count data models with a regression component remains an area for future
research.t
4.2. Tests for Poisson Model
The discussion above concerned specification tests. We now consider misspecification tests
which are designed to highlight inadequacy of the maintained model is specific directions. In
this subsection we consider the problem of choosing between Poisson models and more
general models, given that the investigator has correctly specified the mean function
Pt-E=[yi |X, P].
A natural basis for testing the adequacy of the Poisson model is the relationship between
Var(yi | Xi, P) and E[y, | Xi, P]. We propose tests of
HO y,

Poisson (,-

f(X,, P))

against
HA: E[yi IX,]

Var(yi IXi, p)=

H

+ t4l, for given I

where the distribution for y, under HA may not necessarily be specified. The model under HA
nests many potentially interesting models as special cases. For example, if y, is specified to be
distributed as the negative binomial under HA, the value 1 = 1 corresponds to the Negbin I
model and 1 = 2 to the Negbin II model.
Since the variance of y, equals y, when y, is Poisson distributed, tests of Ho against HA are
t Models for panel count data are beyond the scope of this paper. HHG (1984) take a similar approach to Mossiman
(1970) to control for 'apparent' contagion by allowing an individual specific effect in a panel data analogue of (3)ln Ait= XitP + er-to have both a random component which is controlled for along the lines of (4) and a fixed
component which is controlled for by conditioning for each individual on Etyit. GMT (1984b) propose an error
component model with both individual specific and time specific effects, but this will require a very long panel for
consistent estimation of the parameters. Neither set of authors considers 'true' contagion.

ECONOMETRIC MODELS OF COUNT DATA

41

based on tests for a = 0. We consider tests using the familiar principles of the Wald, the
likelihood ratio and score (Lagrange multiplier) tests.
Score tests
We follow Lee (1984) and consider score tests of the Poisson model against the alternative
that the distribution belongs to a system of distributions studied by Katz (1963) which contains
as special cases the Poisson, negative binomial and binomial distributions.t An advantage of
this system of distributions is that it not only allows for overdispersion (in which case it defines
the negative binomial distribution) but it also permits underdispersion.
We specify the mean and variance of the Katz distribution under HA^to be respectively ,l
and (pi + ayl4).t The Poisson distribution is obtained when a = 0, so a natural test for Ho
against HA is a score test for a -= 0. By similar manipulations to those of Lee we obtain the
efficient score under H0:

N

LZ

N

;

-

-

l 1

(16)

2

where fi = f(X,, P) and p is the MLE for P in the null hypothesis Poisson model. Under
When testing specifically against the negative binomial distribution, i.e.
Ho, TL -I.(O,1).
against overdispersion, a one-sided normal test based on TLshould be used.
Since VarHo(l/,/{(y,
= 2, an asymptotically equivalent test statistic under H( is
M)2 - y})
1

N
llN
1f

1

{(Yl--/1t)

-A

{(Y -

--

Yl}
~~

-~

-y)2_ Y})

E

(^I- 1)2

tThe system of discrete distributions studied by Katz (Katz, 1963; Johnson and Katz, 1972) is defined by the recursive
relationship
Pr{Y = y + 1} = (A+ y)Pr{Y

},

A+ yyO 0

0
, A + yy < 0
=
>
<
variance A/(1 -_ y)2, and includes as special
and
.
.
.
This
has
mean
where A 0, y 1 andy
0,1,2,
y)
/(
system
binomial
cases the negative binomial
( < y < 1), Poisson (y = 0), binomial (y < 0 and -X/y an integer) and other (y < 0 and
(0
-A/y not an integer) distributions.
tLee (1984) specifies i = exp(X,P) and considers two different specifications for the variance under the alternative
hypothesis,
,u) and ( + -+ l
^ l
Â±N
which with the obvious reparameterization correspond to 1 = 1 and / = 2 above.

42

A. C. CAMERON AND P. K. TRIVEDI

The statistic is easily computed as it is the square root of N times the centred R2 from the least
- y,} against ,u-1.
squares regression of (1//i){(y,j)2
Overdispersion tests
The above tests require that the actual distribution of y, rather than just the mean and
variance, be specified under HA. Cox (1983) proposes a test statistic which does not require
specification of the distribution of y, under HA. This is possible by considering only local
alternatives to the null hypothesis Poisson distribution of y, and Cox calls his test a test for
'modest amounts of overdispersion'.
In the general framework of Cox we consider independently distributed observations
Y....
, YN. Each observation Yi has density f(y, Ai),where the scalar parameter A,is itself the
realization of a random variable A, distributed with mean i, = f(X,, J) and variance r,z2.
Under Ho, rz2= 0, whereas the alternative overdispersed model permits r' > 0 and possibly a
function of Mu.The restriction that Cox makes is to suppose that z2is 0(1/\YN), and it is in this
sense that Cox considers modest amounts of overdispersion.
Under certain regularity conditions (Cox and Hinkley, 1974, Chapter 9.2), by taking a
second-order Taylor series expansion of f(y, A) about A = u and taking expectations with
respect to A Cox obtains the following 0(1/N) approximation to the density of the
overdispersed model
f* (y,/,

r2) =f(y,/)

2h(y,/u))/a(y,/u, 2)

exp(

(18)

where
h(y,)

^alogf(y,)0

2

32 log

(19)

f(y,i)

and a(y,u, r2) is a normalizing constant and we have omitted the subscript i on y,,, and r,2.
For his test Cox considers the case where r2 does not depend on Mp.More generally we
suppose r2 = (1/VN)a/x', for given 1. Then a test for overdispersion is simply a test of the null
hypothesis a = 0 against the alternative a > 0. A natural test is based on the score statistic

=

EaaOS
a(y,,W,,(./.-)2)

,iEh(y,

,)

(20)

where i = f(Xl, 1) and fi is the MLE for Piunder H0.
We now apply these results to the case where yl - Poisson (A,-= l) under H0. First note that
under the alternative hypothesis Var(y) = E[ Var(y | A)] + Var,(E[y I A]) = E[A ] + Var,(A)
= Â±p+ (1//lN)a/1. This is the same as HA given earlier, except now only local alternatives
((1/v/N)a) are considered. Secondly, for the Poisson h(y, p) = (1/A2){(y - A)2 _ y}. Therefore
(20) becomes
1
1 N
2VN= 25NIl

P11 1(2
{I

-

t1)2--

Yl}

which is equivalent to (15) and clearly yields the test statistic TL given in (16). So the same test
statistic is obtained by considering either local alternatives to the Poisson or alternatives to the
Poisson in the Katz system of distributions.t
tSee Cameron and Trivedi (1985) for more detailed analysis of the commonalities between various test procedures. In
addition to the above tests, White's information matrix test and its obvious similarity with Cox's test and regression
based tests are discussed.

ECONOMETRIC MODELS OF COUNT DATA

43

Wald test
The advantage of the score test is that the model need only be estimated under Ho.
However, if the model is estimated under the alternative hypothesis, a Wald test may be easy
to compute. For example, if the Negbin I model is estimated to yield the MLE oc and
asymptotic

variance var(a) for a, the Wald test of HO: Poisson

(pi) against HA: negative

binomial with mean i, and variance (pi + acpu)is based on the usual T-statistic &/v[var(&)],
which is asymptotically distributed as J (0, 1). Note that a one-sided test is appropriate.
Likelihood ratio test
If both the model under Ho and the model under HA are estimated by maximum likelihood,
and the model under Ho can be derived from the model under HA by a single parametric
restriction (on a), the quantity 2 times log-likelihood ratio is asymptotically distributed as X2(1)
under Ho. For example, we can form log-likelihood ratio tests for the Negbin I model or
Negbin II model against the Poisson model.
4.3. Implications for Sequential Modelling
In common with the use of score tests in many other areas, several of the above tests have a
certain drawback from an applied viewpoint: they are helpful in rejecting the maintained
model but less so in directing the user to a better class of models. Cox's approach indicates
that, regardless of the alternative considered, the test will be the same whenever the 0(1/N)
approximation to the density is the same. Misspecification tests do not therefore indicate a
unique alternative model. The question then arises: what should an investigator do if the
Poisson model is found to be data incoherent? It is clear that tests based on specific parametric
alternatives are more helpful in this regard. If the test indicates overdispersion, the negative
binomial seems an obvious alternative, whereas if underdispersion is the problem then the
binomial or truncated Poisson could be appropriate. Even if a specific parametric alternative
to the Poisson is not adopted, it is useful to find out whether the Poisson model fits the data
adequately to guard against the invalid imposition of strong restrictions which will produce
artifically low variances.
Suppose, as is likely in many empirical situations, the negative binomial model is preferred.
Then, of course, it too should be subjected to additional tests against other more flexible
alternatives. A random effects negative binomial is potentially more flexible and can be
generated beginning with (7) and treating the quantity v, as a random variable with a beta
density, dB(a, b), with parameters a and b. This is the approach followed in HHG (1984,
p. 927); also see Ord (1972, pp. 74 and 85). We do not consider such models here. It is clear
from section 2.3 that the negative binomial already allows great flexibility for modelling the
relationship between Var(yi) and E(y).
It is more convenient to proceed as follows. Following the rejection of the Poisson model,
re-estimate the model under the negative binomial assumption and by QGPML method. If the
two sets of results differ substantially, then this probably indicates the failure of the negative
binomial assumption and the investigator should prefer QGPMLE. However, this still leaves
open the question of whether a random effects negative binomial may be preferred to a
Poisson model with specification errors. We do not pursue this question here.
5. APPLICATION
5.1. The Data and The Model
This section contains an application of the sequential modelling strategy advocated in the
earlier sections. It is based on and related to a much more extensive study of the demand for

44

A. C. CAMERON AND P. K. TRIVEDI

health insurance and the use of health care services in Australia. See Trivedi et al. (1984) and
Cameron etal. (1984) for a discussion of the substantive economic issues. Here we shall
confine discussion to matters of econometric methodology leaving the interested reader to
pursue other details in the previous papers.
The data
the
for the application are derived from the Australian Health Survey 1977-78 which
contains information on the number of consultations with a doctor or specialist, denoted
NOCNSLT, in the two-week period before an interview. No information was obtained on
intervals between consultations. The sample consists of 5190 individuals (heads-only) over 18
years old who answered all the questions essential to our analysis. The explanatory variables in
the model comprise, in addition to a constant denoted ONE, thirteen variables, namely SEX,
AGE, AGESQ (age-squared), INCOME, ILLNESS (recent illnesses), ACTDAYS (number
of reduced activity days), HSCORE (general health questionnaire score), CHCOND1,
CHCOND2 (number and type of chronic condition s) and dummy variables for four levels of
health insurance cover, denoted, respectively, as MEDLEVY, LEVYPLUS, FREEPOOR
and FREEREPAT. The omitted dummy variable is MEDLEVY (medibank levy).
LEVYPLUS represents a higher level of insurance cover, whereas FREEPOOR and
FREEREPAT represent a basic level of insurance cover supplied free of charge to certain
individuals on grounds of, respectively, low income and pensioner or repatriation status.
The major focus of the empirical studies mentioned above was on the nexus between
insurance level and health care use, that is whether a high insurance level caused individuals to
'consume' more health care services, and on the role of income. The NOCNSLT variable for
which we report the results here is only one of seven variables that were analysed using count
data models.
Some of the characteristics of the raw data on NOCNSLT are as follows: 79-8 per cent of
5190 interview respondents had zero consultations, 15-1 per cent had one consultation and
the remainder had up to a maximum of nine consultations. The sample mean is 0-302, the
sample variance is 0-637, indicating substantial overdispersion in raw terms. The frequency
distribution of NOCNSLT is unimodal.
5.2. Estimators for Count Data Models
A number of models for count data are defined in Table I. These models vary along three
dimensions: different functional forms may be specified for the objective function (i.e. the
'assumed' distribution), the mean and the variance of y.
First, the objective functions are based on the normal, Poisson or negative binomial
distributions.
Secondly, the mean is almost always parameterized as f(X, P) = exp(XP) to ensure that it is
positive. The exception is the OLS model where we let f(X, P) = Xp for the normal
Table I. Some models for count data

1.
2.
3.
4.
5.

Model

Distribution

Mean

OLS
Normal
Poisson
Negbin I
Negbin II

Normal
Normal
Poisson
Negativebinomial
Negativebinomial

exp(X)
exp(X )
exp(Xp)
exp(Xp)

Xp

Variance
a
a
exp(X1)
(1 + a)exp(Xp)
exp(Xp)(1 + axexp(X))

ECONOMETRIC MODELS OF COUNT DATA

45

Table II. Some estimators for count data
Model
1.

OLS

2.

Normal

3.

Poisson

4.

Negbin I

5.

Negbin II

la
lb
1c
2a
2b
2c
3a
3b
3c
4a
4b
4c
5a
5b
5c

Estimator

P

Var(p)

ML
QGPML
PML
ML
QGPML
PML
ML
QGPML
PML
ML
QGPML
PML
ML
QGPML
PML

la
Same as Ia
Same as la
2a
Same as 2a
Same as 2a
3a
Does not exist
Same as 3a
4a
4b
4c
5a
5b
Same as 4c

la
Same as la
1c
2a
Same as 2a
2c
3a
3c
4a
4b
4c
5a
5b
Same as 4c

distribution to include OLS estimation. A positive mean is required for ML (though not PML
and QGPML) estimation based on the Poisson and negative binomial distributions.
Thirdly, the variance varies according to the distribution specified and the ways in which
overdispersion might be modelled. For OLS and Normal models the customary specification
of constant variance is made. For the Poisson model, by definition the variance equals the
mean. Negbin I and Negbin II have been discussed in section 2.3.
For each model three types of estimators for f may be possible. The MLE assumes that the
distribution, mean and variance are correctly specified. The PMLE assumes that only the
mean may be correctly specified, and sets the nuisance parameter a equal to an arbitrary
constant. The properties of these estimators have been discussed in section 3.2. However,
some of these estimators overlap, leading to a considerable economy in computation and
reporting of results. This is summarized in Table II.
First, note that a QGPMLE cannot be constructed for the Poisson model because there is no
nuisance parameter. Secondly, OLS, Normal and Poisson are all generalized linear models, see
section 3.2. It can be shown that for these models the solution for fIdoes not depend upon a, so
ML, QGPML and PML estimators of P for any one model are equivalent and the asymptotic
variance-covariance matrices of the MLE and QGPMLE will be equivalent. Thirdly, Negbin I
and Negbin II are not generalized linear models, so there is no overlap in estimation. Fourthly,
the PML estimators for Negbin I and Negbin II are equivalent since only the knowledge of the
specified mean is used.
For the QGPML estimators we need consistent estimators of the nuisance parameters for
two reasons: (1) to form the actual QGPMLE for fIfor those models which are not generalized
linear models in the sense of McCullagh and Nelder (1983), and (2) to form the estimated
variance-covariance matrix of the QGPMLE for P. We proceed as follows. First, obtain the
- f(X,,
=
PMLE jlfor the model under consideration. Secondly, since E[(yi
a),
[P))2]
(X,, ,
=
,
we can estimate a by regression based on the relation (y (Xi, l,))2
(X, ,a) + e,, where
ei is an independent error with zero mean. For
fI, = a we obtain
Q(Xi, oa)
i~(Yi-

f(X

,0))2

46

A. C. CAMERON AND P. K. TRIVEDI

and specializing to the case f(Xi, P) = exp(X,P), for Q(X, P, ) = exp(X,P)(l + acexp(X,P))
we obtain
N

f (exp(X, ))2{(y, - exp(X/i))2 - exp(X,i)}

i=l

N

E

, = 1

(exp(X, P))4

whereas for g(Xi, ), c) = (1 + a) exp(X, ) we obtain
N

f

exp(X,i){(y,-

OC
~~N

exp(XI,))2-

exp(X,p)}

=

E

(exp(X, P))2

When an estimate of oc is needed for the asymptotic variance-covariance matrix of the
QGPMLE, we again use the above formulae for the estimator for ac, but base the actual
estimate on the QGPMLE for P rather than the PMLE for P.t
Column (1) of Table III gives OLS estimates and standard errors corresponding to la and
lb of Table II. Column (2) corresponds to estimators 3a and 3b of Table II; columns (3) and
(4) to 4a and 5a and column (5) to the QGPML estimator 5b. Column (7) gives the standard
errors of the PML estimator for the OLS model. These are the same as Whites (1980)
heteroscedasticity consistent standard errors. Column (8) corresponds to estimators 2a and 2b.
Column (6) refers to the ordinal probit model which is discussed later in this section. We do
not report results for estimators 2c, 3c and 4c.
5.3. Analysis of Results
The estimates in column (1) of Table III are of interest because the first step in the analysis is
often estimation by ordinary least squares. Though this regression is significant as a whole it
shows a negligible role for either INCOME or the three insurance level dummies. In the spirit
of residual analysis consider the least squares residuals u, = y, - X, which are used to
estimate the following regressions (standard errors in parenthesis are obtained using
Eicker-White heteroscedastic consistent estimator):t

U= 2 2158 (X,J)

(21)

(0-2034)
^2

(- = 1.1962 + 0.6717 (X,J)
v(X) (0-2737) (0-3996)

(22)

tThe estimators here follow suggestions by GMT (1984b). Alternatively when the model is a GLM we could follow
the method of McCullagh and Nelder (1983) and use an estimate of a based on the generalized Pearson x2 statistic.
When SI(X,, I, a) = a we get the same estimate as above, but when QI(X,, I, a) = (1 + x)exp(X,P), we obtain
N
(1
(y, f(X,))2
(1 a)
N-K,
(X, I)
tThese tests serve only as a crude guide. X,p possibly less than zero causes obvious problems. Even abstracting from
this, the use of P rather than JIin (21) and (22) will lead to disturbances which are not only heteroscedastic but also
correlated. For a more rigorous treatment see Cameron and Trivedi (1985).

Table III. Alternative estimates for NOCNSLT equation
(1)

(2)

(3)

(4)

OLS

Poisson

Negbin
I

Negbin
II

-2-2244
(0-1443)
0-1570
(0-0406)
1-0547
(0-7499)
-0-8466
(0-8092)
-0-2048
(0-0619)
0-1230
(0-0560)
-0-4412
(0-1163)
0-0799
(0-0701)
0-1870
(0-0142)
0-1268
(0-0035)
0-0301
(0-0074)
0-1142
(0-0515)
0-1417
(0-0586)

-0-6270
(0-2253)
0-1638
(0-0602)
0-2769
(1-1257)
0-0223
(1.1190)
-0-1345
(0-0957)
0-2127
(0-0842)
-0-5379
(0-2093)
0-2086
(0-1038)
0-1959
(0-0206)
0-1123
(0-0056)
0-0358
(0-0105)
0-1326
(0-0746)
0-1742
(0-0890)

-2-1902
(0-2224)
0-2164
(0-0659)
-0-2207
(1-2334)
0-6137
(1-3801)
-0-1422
(0-0976)

One

0-0276

SEX

0-0338
(0-0216)
0-2032
(0-4100)
-0-0621
(0-4587)
-0-0573
(0-0331)
0-0352
(0-0249)
-0-1033
(0-0525)
0-0332
(0-0382)
0-0599
(0-0084)
0-1032
(0-0036)
0-0170
(0-0052)
0-0044
(0-0237)
0-0416
(0-0359)

AGE
AGESQ
INCOME
LEVYPLUS
FREEPOOR
FREEREPAT
ILLNESS
ACTDAYS
HSCORE
CHCOND1
CHCOND2
Variance
parameter or
-log L
Iter; Time
R2

0-4551

3355-542
12;467

(0-0405)
3226-589
10;592

0-1191

(0-0849)
-0-4978
(0-1750)
0-1458
(0-1174)
0-2145
(0-0257)
0-1437
(0-0075)
0-0381
(0-0143)
0-0997
(0-0766)
0-1905
(0-0948)
1-0766
(0-0984)
3198-744
12;563

(5)
QGPML
based on
Negbin II

(6)
Ordinal
Probitt

-2-1958
(0-1833)
0-1992
(0-0541)
0-2535
(1.0057)
0-0680
(1-1152)
-0.1613
( 0-807)
0-1124
(0-0699)
-0-4731
(0-1448)
0-1139
(0-0948)
0-2035
(0-0202)
0-1359
(0-0053)
0-0354

-1-390
(0-1480)
0-1301
(0-0436)
-0-5784
(0-8226)
0-9159

(0-0110)

(0-0091)

0-1013
(0-0644)
0-1691
(0-0771)
0-4899

0-0624
(0-0499)
0-1402
(0-0657)

3024-437

(0-9009)

-0-0537
(0-0663)
0-1418
(0-0558)
-0-3347
(0-1257)
0-1874
(0-0757)
0-1523
(0-0163)
0-0998
(0-0055)
0-0821

3140-6
20;686

0-20

Estimated ancillary parameters and their standard errors for the ordinal probit model are: ,Mi2= 0-9417 (0-0313); i3
= 3-299 (0-4376). tTo obtain slopes of
=
(0-0627); i5 = 1-9420 (0-0806); P6 = 2.079 (0-0942); P7 2-330 (0-1029); /9
regressor means, multiply slopes by 1-3526.

48

A. C. CAMERON AND P. K. TRIVEDI

The first regression clearly rejects the hypothesis that the coefficient of (Xi$) is unity which
is what we would expect under the Poisson model and suggests that variance is a multiple of
the mean which is consistent with the negative binomial model. The second regression is a
rough test of whether variance is linear in the mean to suggest whether Negbin I or Negbin II
is preferable. The coefficient of (Xi$) is now positive and just significant on a one-sided test at
5 per cent significance level and the intercept is not significantly different from unity,
indicating that there is some support for the second over the first parameterization.
Next the Poisson model was estimated notwithstanding that preliminary analysis favoured
the negative binomial. These results are given in column (2). The estimated Poisson model was
used to apply Lee's score test of the null hypothesis of Poisson versus the alternatives of a
number of the general Katz family of frequency distributions (including the negative binomial)
with variance exp(X13)(1 + a exp(XP)). The estimated T-statistic was 29 89 compared with
the critical value at 5 per cent significance level of 1-96, which strongly rejects the Poisson.
Which parameterization of the negative binomial should one choose? Though we recognize
that issues of power of tests need to be investigated, proceeding in a data-analytic spirit, we
used Poisson estimates to calculate parallel regression equations to (21) and (22). Define
E(y,) =- Ai and V(y)

= (

- Ai)2. We obtained
V(y)

-

2-2180 A,

(23)

(0-069)
..L

Al

= 1-0569 + 0 888 A,

(24)

(0-1051) (0-212)

(23) again confirms that the negative binomial is preferred to the Poisson and (24) suggests,
more positively than did (22), that the second rather than the first parameterization of the
negative binomial is appropriate. We have indicated already our preference for the second
parameterization on grounds of computational efficiency.
All tests which we have applied lead to the rejection of the Poisson model. Although this
does not imply that the Negbin I and II are automatic alternative choices, they are favoured as
preferred alternatives to the Poisson model.
The negative binomial model estimates are given in columns (3) and (4). The Wald statistics
(A'(0, 1)) for testing Poisson against Negbin I and Negbin II are respectively 11-23
(=0-4551/0-0405) and 10-84. The corresponding likelihood ratio test statistics (x2 (1)) are,
respectively, 257-90 (=2 x (3355-54 - 3226-59)) and 313-58. Again the Poisson model is
strongly rejected. Note that squaring the score and Wald tests (so that x2 (1)) yields score > LR
> Wald, which is the reverse of the well-known analytical results for the normal linear
regression model. The log-likelihood associated with Negbin II is higher. Negbin II is also
slightly more efficient computationally, something that was independently confirmed on other
samples. We think this favours the second over the first parameterization of the negative
binomial and we return to this issue later.
We now compare the actual parameter estimates in Columns (1)-(4). We are particularly
interested in whether a higher insurance level is associated with more visits to a doctor.
Although both sets of estimates yield similar answers, and are notably different from those
produced by the OLS results, there are differences of detail. The LEVYPLUS dummy
coefficient is almost twice as large for Negbin I as for Negbin II.
The similarity of the Poisson point estimates to those in the Negbin models is rather striking,
but note that the estimated variances under the Poisson assumption are generally substantially

ECONOMETRIC MODELS OF COUNT DATA

49

smaller, reflecting the consequences of imposing on the data the equality of conditional mean
and variance. This example confirms that the major impact of the distributional assumption is
on estimated variances rather than point estimates of parameters. Comparison of OLS with
Poisson and Negbin estimates is different because the former model has mean XP and the
latter models have mean exp(XP).
We next relax the negative binomial assumption in the Negbin II model to the assumption
that only the mean and variance are correctly specified. QGPMLE based on Negbin II uses the
2-step procedure given in section 5.2. This leads to a higher value of 'likelihood' and the
estimated standard errors are somewhat smaller than under the Negbin assumptions.
However, most of the substantive inferences, for example, regarding the role of insurance
status, would be the same in either case. So in some respects the choice between the two
estimators is not critical.
In column (7) we give the PMLE for the OLS model as an example in which the MLE may be
consistent despite missspecification-the parameter estimates in columns (1) and (7) are
identical-but the standard errors are inconsistently estimated-all but two standard errors in
column (1) are understated (by roughly 10-20 per cent).
The estimates obtained under the assumption of a Normal model, given in column (8), are
in certain respects markedly different from those for Negbin II. This is especially so in the case
of coefficients of INCOME, FREEPOOR and CHCOND2 variables.
In terms of computing time, estimation is cheapest under the Poisson assumption and least
so under the Normal assumption. Savings in computer time resulting from the Poisson
assumption could be greater still if an efficient special purpose computer program such as
GLIM were to be employed (see the Appendix). The overall impression obtained from this
example is that, at least with 5190 observations, relatively few coefficients are sensitive to the
choice of estimator and they are the ones with relatively small 't-ratios'.
5.4. Ordinal Probit as an Alternative to Count Data Models
Although we could have stopped at this stage, we felt that the sensitivity of our conclusions to
the choice of the model needed further probing. Our motivation for this is somewhat specific
to our example but will extend to other situations in which count data models are used. This is
that even though NOCNSLT seems a natural cardinally measured variable it could be validly
treated as an ordinal measure of the use of doctor's services.t Thus, for example, three
doctor-consultations represents a higher level of doctor usage than two consultations, but not
necessarily 50 percent more. In the context of the HHG (1984) paper, n patents may
represent a higher level of R & D activity but not necessarily double that associated with n/2
patents. Inverting an argument due to McKelvey and Zavoina (1975, p. 103), we might regard
an observed variable which is of count form as reflecting a methodological limitation in
collecting data, and being no more than a proxy, measured on a crude ordinal scale, for the
true unobserved variable which the model is intended to predict. (In our case we are interested
in the use of doctor's services but not the number of consultations per se). From this viewpoint
the emphasis on embedding the model in a parametric family of discrete distributions appears
somewhat misplaced. It may be better to use a statistical model suitable for analysing ordinal
level dependent variables. McKelvey and Zavoina (1975) have developed such a model,
known as the ordinal probit model, as an extension of the dichotomous probit model. See
Maddala (1983) for an exposition.
tThis suggestion was made to us by James Heckman.

50

A. C. CAMERON AND P. K. TRIVEDI

In using this model for our purposes we treat the observed counted variable Y,, as a proxy for
the variable of theoretical interest, Y*, which by assumption is assumed to be distributed as
.kf(Xip, a2). Y,(i = 1, .. ., N) is treated as a categorical variable with M response categories
R1, . .. , RM, related to the unobserved variable, Y*, as follows. Let ,u/, /, . . ,M denote
M + 1 real numbers with 0 = -ox, ,m = + o and ,i/0o
, m, such that
I ...
1-tk
YEe Rk : 1k-I < Y*
for 1 k M.
Since Y is treated as ordinal (categorical) it can be represented as a series of dummy
variables by defining

y lkk=lifY,eRk

l0 otherwise

wherei = 1, .. .,N; k = 1, ..., M.
Denoting the cumulative standard normal density by <D(t)and imposing the identifying
restrictions /1u= 0, a = 1, the ordinal probit model leads to the probability function
Pr[ Y,k = 1] =

D[k

-

XP]

-

D[1k-1

-

XP]

1 and the
which forms the basis of maximum likelihood estimation of parameters ,uI, . . .,
vector 1P.
Finally, therefore, it is interesting to compare the earlier estimates of our model for
NOCNSLT with those based on maximum likelihood estimation of the ordinal probit model.
The estimate of f is in column (6) and the estimate of puis at the bottom of Table III. As they
stand, columns (2)-(5) are not directly comparable with column (6). However, the inferences
regarding the qualitative influence of sex, income, health insurance status, health status and
chronic conditions on NOCNSLT would be almost exactly the same as those based on the
estimated count data models. This does increase our confidence in the robustness of our
substantive conclusions and hence seems a worthwhile final check of the results. But note also
that the ordinal probit estimation has provided additional interesting information on the Pu
parameters, which all have rather small standard errors. The observed counts Y, range from 0
to 9 whereas the corresponding unobserved theoretical values Y* are calibrated by the

estimated PIk(k =- 0, . . . , 9) which range between 0 (which is a normalized value) and 3-30

That is, individuals falling below u1-= 0 record a zero count, those between PuI= 0 and
u2 =0-9417 record one count, those between 2 = 0-9417 and 3 = 1-512 record two counts
and so forth. This provides a further useful calibration of differential propensities of
individuals for consultations with doctors. (Since the ordinal probit model involves an
additional eight parameters, comparison of fit between it and other alternatives should be
based on a modified log-likelihood ratio such as that proposed by Akaike.)
6. CONCLUSIONS
In common with many econometric problems, the specification of count data models raises
fundamental questions about the underlying objectives of the analysis; that is, whether a
reduced form or a structural interpretation is desired. Within the context of single period
cross-section data, only reduced form type models would appear to be feasible. From the
viewpoint of estimation a number of available models can be usefully treated as non-linear
regression models with specific conditional mean-variance or heteroscedasticity structures. To
some extent flexibility in specification can be achieved through the specification of such
heteroscedasticity structures. If the matter is viewed this way, then it seems possible to develop

ECONOMETRICMODELSOF COUNT DATA

51

a sequential modelling strategy based on some simple tests, in which one can proceed to
increasingly flexible and data-coherent models, beginning with the basic Poisson model. We
have provided a detailed application of this modelling strategy to illustrate both its simplicity
and feasibility. Our results are broadly supportive of the QGPMLE procedure advocated by
GMT but we also advocate further exploration of suitable categorical variable mnodelsas an
alternative approach. Additional work remains to be done on the power of various test
procedures and the extension of the results on non-nested model comparisons to count data
models.
ACKNOWLEDGEMENTS

This paper originated in an applied econometric project on the demand for health
insurance and health care in Australia. We are grateful to the Commonwealth Department of
Health and the A.N.U. Faculties Research Fund for financial support and to James Heckman,
Tony Lancaster, Lung-Fei Lee, Grayham Mizon, Hashem Pesaran and anonymous referees
for this journal for helpful comments and suggestions for improvements.
APPENDIX
The negative binomial models were estimated using the optimization routine for user-defined
functions in the LIMDEP package of Greene (1983). This package also allowed estimation by
NLLS and ordinal probit methods. The iterative method of Berndt et al. (1974) was used with
analytical first derivatives provided. Considerable computational savings arise by using the
recursion relations for the gamma and digamma functions, rather than the power series
expansions referred to by Gilbert (1979), Hausman et al. (1984) and Gourieroux et al. (1984).
For example, the ratio r(v, + y,)/r(v,) in (7) equals Il,'1(v, +j - 1). Note also that the
remaining gamma term r(y, + 1) is easily computed since yi + 1 is an integer, and does not
need to be recalculated at each iteration. A number of other models were also estimated under
the negative binomial assumption (Cameron et al., 1984). Computational time on a
Univac 1100/82 for N = 5190 and 13 parameters was between 480 and 640 CPU seconds,
with I/O time one to two times CPU time. Computational time per iteration doubled as N
doubled and roughly doubled as M quadrupled. When ordinal probit was used instead CPU
time was between 25 and 40 per cent greater.
Program LIMDEP was also used for the Poisson model. The Poisson model took fewer
iterations and about two-thirds the CPU time of comparable negative binomial models. In
addition, the Poisson model was estimated using program GLIM. Computational time was
considerably less-about one-third that of LIMDEP-because GLIM takes advantage of the
special structure of exponential family distributions such as the Poisson to estimate the model
by iterative weighted least squares (Nelder and Wedderburn, 1972; McCullagh and
Nelder, 1983). Although GLIM macros exist for the negative binomial model, they are
unfortunately not computationally efficient because the negative binomial distribution is only
a member of the exponential family when v is known.
REFERENCES
Amemiya,T. AdvancedEconometrics(1985), HarvardUniversityPress, Cambridge.
Bates, G. E. and J. Newman(1952), 'Contributionsto the theory of accidentproneness',Universityof
California Publications in Statistics, 1, 215-275.
Berndt, E. R., B. H. Hall, R. E. Hall and J. A. Hausman (1974), 'Estimation and Inference in Nonlinear
Structural Models', Annals of Economic and Social Measurement, 3, 653-666.

A. C. CAMERONAND P. K. TRIVEDI

52

Boswell, M. T. and G. P. Patil (1970), 'Chance mechanisms generating the negative binomial
distributions', in G. Patil (ed.), Random Counts in Models and Structures: Volume 1, The Pennsylvania
State University Press, Pennsylvania.
Burguette, J., R. Gallant and G. Souza (1982), 'On unification of the asymptotic theory of nonlinear
econometric models', Econometric Reviews, 1, 151-190.
Cameron, A. C. and P. K. Trivedi (1985), 'Regression based tests for overdispersion', Technical Report
No. 9, Econometric Workshop, Stanford University.
Cameron, A. C., P. K. Trivedi, F. Milne and J. Piggott (1984), 'Microeconometric models of the demand
for health insurance and health care in Australia: II', Australian National University, Working Papers
in Economics and Econometrics, No. 106.
Cox, D. R. (1983), 'Some remarks on overdispersion', Biometrika, 70, 269-274.
Cox, D. R. and D. V. Hinkley (1974), Theoretical Statistics, Chapman and Hall, London.
Cox, D. R. and P. A. W. Lewis (1966), The Statistical Analysis of Series of Events, Methuen, London.
Cresswell, W. L. and P. Froggatt (1963), The Causation of Bus Drivers Accidents, Cambridge University
Press, London.
Eicker, F. (1967), 'Limit theorems for regressions with unequal and dependent errors', Fifth Berkeley
Symposium on Mathematical Statistics and Probability, University of California Press, Berkeley and
Los Angeles.
Gilbert, C. L. (1979), 'Econometric models for discrete economic processes', Discussion Paper,
University of Oxford, presented at the Econometric Society European Meeting, Athens.
Gourieroux, C., A. Monfort and A. Trognon (1984), 'Pseudo maximum likelihood methods: theory',
Econometrica, 52, 681-700.
Gourieroux, C., A. Monfort and A. Trognon (1984), 'Pseudo maximum likelihood methods: applications
to Poisson models', Econometrica, 52, 701-720.
Gourieroux, C., A. Monfort and A. Trognon (1984), 'Pseudo-likelihood methods: a Survey', Document
de travail ENSAE/INSEE, No. 8406.
Greene, W. H. (1983), 'LIMDEP: a program for estimating the parameters of qualitative and limited
dependent variables', The American Statistician, 37, 170.
Hausman, J., B. H. Hall and Z. Griliches (1984), 'Econometric models for count data with an application
to the patents-R

& D relationship',

Econometrica,

52, 909-938.

Heckman, J. and B. Singer (1984), 'Econometric models of duration', Journal of Econometrics, 24,
63-132.
Heckman, J. and G. Borjas (1980), 'Does unemployment cause future unemployment? Definitions,
questions and answers from a continuous time model for heterogeneity and state dependence',
Econometrica, 47, 247-283.
Hendry, D. D. and K. F. Wallis, (eds) (1984), Econometrics and Quantitative Economics, Basil
Blackwell, Oxford and New York.
Huber, P. (1967), 'The behaviour of maximum likelihood estimates under nonstandard conditions', in
Fifth Berkeley Symposium on Mathematical Statistics and Probability, 1, University of California Press,
Berkeley and Los Angeles.
Irwin, J. 0. (1941), 'Discussion on Chambers and Yule's paper', Journal of the Royal Statistical Society,
Supplement 7, 101-109.
Jennrich, R. (1969), 'Asymptotic properties of nonlinear least squares estimators', The Annals of
Mathematical Statistics, 40, 633-643.
Johnson, N. L. and S. Kotz (1972), Discrete Distributions, Wiley, New York.
Katz, L. (1963), 'Unified treatment of a broad class of discrete probability distributions', Proceedings of
the International Symposium on Discrete Distributions, Montreal, pp. 172-182.
Lawless, J. F. (1982), Statistical Models and Methods for Lifetime Data, Wiley, New York.
Lee, L-F. (1984), 'Specification tests for Poisson regression models', Department of Economics,
University of Minnesota, unpublished paper.
Maddala, G. S. (1983), Limited-dependent and Qualitative Variables in Econometrics, Cambridge
University Press.
McCullagh, P. (1983), 'Quasi-likelihood functions', The Annals of Statistics, 11, 59-67.
McCullagh, P. and J. A. Nelder (1983), Generalised Linear Models, Chapman and Hall, London.
McKelvey, R. D. and W. Zavoina (1975), 'A statistical model for the analysis of ordinal level dependent
variables', Journal of Mathematical Sociology, 4, 103-120.
Mossiman, J. E. (1970), 'Compound multinomial distributions', in G. P. Patil (ed.), Random Counts in
Scientific Work, 3, Pennsylvania State University Press, Pennsylvania, Chapter 1.

ECONOMETRICMODELSOF COUNT DATA

53

Nelder, J. A. and R. W. Wedderburn (1972), 'Generalised linear models', Journal of the Royal Statistical
Society, Series B, 135, 370-384.
Ord, J. K. (1972), Families of Frequency Distributions, Griffin, London.
Patil, G. P. (1970), Random Counts in Models and Structures: Volume 1, The Pennsylvania State
University Press, Pennsylvania.
Trivedi, P. K., C. Cameron, F. Milne and J. Piggott (1984), 'Microeconometric models of the demand for
health insurance and health care in Australia: I', Australian National University, Working Papers in
Economics and Econometrics, No. 105.
Wedderburn, R. W. M. (1974), 'Quasi-likelihood functions, generalised linear models and the
Gauss-Newton method', Biometrika, 61, 439-447.
White, H. (1980), 'A heteroscedasticity-consistent covariance matrix estimator and a direct test for
heteroscedasticity', Econometrica, 48, 817-838.
White, H. (1982), 'Maximum likelihood estimation of misspecified models', Econometrica, 50, 1-25.
Xekalaki, E. (1983), 'The univariate generalised Waring distribution in relation to accident theory:
proneness, spells or contagion?', Biometrics, 39, 887-895.

