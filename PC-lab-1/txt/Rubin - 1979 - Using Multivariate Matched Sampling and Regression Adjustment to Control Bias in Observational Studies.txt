Using Multivariate Matched Sampling and Regression Adjustment to Control Bias in
Observational Studies
Author(s): Donald B. Rubin
Source: Journal of the American Statistical Association, Vol. 74, No. 366 (Jun., 1979), pp.
318-328
Published by: Taylor & Francis, Ltd. on behalf of the American Statistical Association
Stable URL: https://www.jstor.org/stable/2286330
Accessed: 11-09-2018 09:05 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

American Statistical Association, Taylor & Francis, Ltd. are collaborating with JSTOR to
digitize, preserve and extend access to Journal of the American Statistical Association

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

Using Multivariate Matched Sampling and Regression
Ad justment to Control Bias

in Observational Studies
DONALD B. RUBIN*

Monte Carlo methods are used to study the efficacy of multivariate
matched sampling and regression adjustment for controlling bias due
to specific matching variables X when dependent variables are
moderately nonlinear in X. The general conclusion is that nearest
available Mahalanobis metric matching in combination with regression adjustment on matched pair differences is a highly effective plan
for controlling bias due to X.

Y can be recorded only on a limited number of units (e.g.,

Y is obtained by an expensive medical examination in the
matched samples).
A major problem with matching methods is that in
practice it is rare that enough matched pairs of treatment

and control units with identical values of X can be found,

KEY WORDS: Covariance adjustment; Nonrandomized studies;
Quasi-experiments.

and then the matching does not perfectly control for X.
A major problem with regression adjustment is that the

1. INTRODUCTION

linear model relating Y to X may be wrong, and then the

adjustment being applied may not be entirely appro-

Our objective is to study the utility of matched

priate. We study cases with imperfect matches and Y

sampling and regression adjustment (covariance ad-

moderately nonlinear in X.

justment) for controlling specific matching variables in

Cochran and Rubin (1973) summarize work on the

observational studies. This introduction is brief; we

efficacy of univariate matching and regression adjust-

assume that the reader is familiar with the literature on

ment with quantitative Y and X. The general conclusions

matching and covariance adjustment in observational

of these univariate investigations are that (a) a very

studies (e.g., Althauser and Rubin 1970); Billewicz 1964,

simple and easy-to-use pair-matching method known as

1965; Campbell and Erlebacher 1970; Cochran 1953,

nearest available pair matching (order the treatment

1968; Cochran and Rubin 1973; Gilbert, Light, and

units and sequentially choose as a match for each treat-

Mosteller 1975; Greenberg 1953; Lord 1960; McKinlay

ment unit the nearest unmatched control unit) seems to

1974, 1975a,b; and Rubin 1974, 1977, 1978a). In par-

be an excellent matching method; and (b) the combina-

ticular, this work is a natural extension of earlier Monte

tion of regression adjustment on matched samples is

Carlo work on one matching variable (Rubin 1973a,b)
and theoretical work on multivariate matching methods

usually superior to either method alone.

We extend this work with quantitative Y and X to the

(Rubin 1976a,b).

case of bivariate X. The two main questions to be ad-

Matched sampling refers to the selection of treatment

dressed are (a) Which of two multivariate nearest avail-

units (e.g., smokers) and control units (e.g., nonsmokers)

able pair-matching methods (discriminant, Mahalanobis

that have similar values of matching variables, X (e.g.,

metric) is preferable? and (b) Which of three regression

age, weight), whereas regression adjustment refers to a

adjustments (no adjustment, pooled estimate, estimate

statistical procedure that adjusts estimates of the treatment effects by estimating the relationship between the

based on matched pair differences) is preferable? Section
2 introduces terminology and notation, and Section 3

dependent variable Y (e.g., blood pressure) and X in each

defines the conditions of our Monte Carlo study. Section

treatment group. Hence, matched sampling and regres-

4 presents results on the ability of regression adjustment

sion adjustment may be used alone or in combination,

to control bias in random samples. Section 5 presents

that is, samples may be random or matched, and regres-

results for matched samples, with and without regression

sion adjustment may or may not be performed. Our use

adjustment. The broad conclusion is that nearest avail-

of the term matching excludes methods that discard units

able Mahalanobis metric pair-matching coupled with

with Y recorded; thus, our matching methods should be

regression adjustment on the matched pairs is a quite

thought of as choosing units on which to record Y when

effective general plan for controlling the bias due to

matching variables, and this combination is clearly
* Donald B. Rubin is Chairman, Statistical Research Group,
superior to regression adjustment on random samples.

Educational Testing Service, Princeton, NJ 08541. Research was
partially supported by the National Institutes of Education, NIEC-74-0126, and facilitated by a Guggenheim Fellowship. The author
would like to thank D.T. Thayer for excellent and extensive programming support and editors and referees for helpful editorial

? Journal of the American Statistical Association

June 1979, Volume 74, Number 366

suggestions.

Theory and Methods Section

318

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

Rubin: Multivariate Matched Sampling and Regression Adjustment 319
2. TERMINOLOGY AND NOTATION

Let P1 be a population of treatment units and let P2
be a population of control units. Random samples are

obtained from P1 and P2; these samples, G1 and G2,

consist of N and rN units (r > 1) with recorded values of
the matching variables, X. Matched samples are created

by assigning to each G1 unit a G2 unit having similar
values of X; the algorithm used to make the assignments

cause both matching methods are easy to implement

using commonly available computer programs for sorting
and calculating a pooled covariance matrix and because
both matching methods have an appealing statistical
property discussed in Rubin (1976a,b; 1978b).
2.2 The Treatment Effect

is the matching method. The dependent variable Y is

For the expected value of Y given X in Pi we write
ai + Wi (X). This expectation is often called the response

then recorded on all 2N units in the matched samples,

surface for Y in Pi. The difference in expected values of

and the effect of the treatment is estimated. Regression

Y for P1 and P2 units with the same value of X is thus

adjustments may be performed by fitting a linear model

al - a2 + Wl(X) - W2(X); when P1 and P2 represent

to the conditional expectation of Y given X. These

two treatment populations such that the variables in X

regressions are estimated from the matched samples and

are the only ones that affect Y and have different dis-

not the random samples because Y is only recorded in

tributions in Pi and P2, then this difference is the effect
of the treatment at X. If W1(X) = W2(X) = W(X) for

the matched samples. Of course, if r = 1, the matched
samples are simply random samples of size N with pairing

all X, the response surfaces are called parallel, and

of G1 and G2 units.

a- a2 iS the effect of the treatment for all values of the
matching variables X.

2.1 Matching Methods to Be Studied

Nonparallel response surfaces are not studied here in

We will study two matching methods: nearest available
pair matching on the estimated best linear discriminant
and nearest available pair matching using the Mahalanobis metric to define distance. Nearest available pair-

order to limit the number of conditions in the Monte

Carlo experiment and because a straightforward argument suggests that matching must have beneficial effects
when the response surfaces are nonparallel and the

average treatment effect in P1 is desired. The expected

matching methods first order the G1 units and then have
each G1 unit choose in turn the closest match from the yet

treatment effect over population P1 is

unmatched G2 units; that is, the first G1 unit selects the

ElEal- a2 + W1(X) - W2(X)]

closest G2 unit, the second G1 unit selects the closest G2
unit from the rN - 1 not yet matched, and so on, until
all G1 units are matched. These matching methods are
fully defined once we specify the order for matching the

G1 units and the precise meaning of closest. Since previous univariate work (Rubin 1973a) indicated that
random ordering is usually satisfactory, we will study
random order, nearest available matching methods.
Closest is clearly defined for one matching variable but
not for more than one.

= El[al + W1(X)] - El[a2 + W2(X)] (2.3)
where E1 is the expectation over the distribution of X in
P1. An unbiased estimate of the first expectation in (2.3)

is simply pi., the average observed Y in G1. If we knew
the P2 response surface, an unbiased estimate of the
second term in (2.3) would be
N

E [a2 + W2(xj)]/NI , (2.4)
j=l

Let xi be the Ni X p data matrix of X in Gi (where
N1 = N, N2 = rN), let xi be the 1 X p sample mean
vector in Gi, and let

that is, the average value of the P2 response surface across

S = [(XlTXl - N1x1iT1) + (X2TX2 - N222T22)

into the region of G1 data. When the P2 response surface

the values of X in the G1 sample. Expression (2.4) implies
that in order to estimate the expected treatment effect in

Pi, we must extrapolate the P2 response surface, W2(.),

(N1 + N2- 2)

is estimated from G2 data, this extrapolation can be

be the pooled within-sample covariance matrix of X

subject to great error unless the sample from P2 used to

based on the random samples G1 and G2. Mahalanobis

metric matching calculates the distance between a G,
unit with score X1 and a G2 unit with score X2 as

(X1 - X2)S-1(X1 - X2)T . (2.1)
Discriminant matching calculates each unit's score on the

estimated discriminant as XDT where D = (xl-x2)S-1,
and then matches on this variable; equivalently, it
defines the distance between a G1 unit with score X1 and
G2 unit with score X2 as

estimate W2(.) has values of X similar to values in G1,
that is, unless the sample from P2 is matched to G1. See

Billewicz (1965) and Rubin (1973a, 1977) for further discussion of nonparallel response surfaces.

Henceforth, we will assume W1(X) = W2(X) = W(X)

so that the treatment effect is r = ail- a2.
2.3 Estimators of T

We will consider three estimators of r, all of the form
A= (7. - y2.) - (i1 -

(X1 - X2)DTD (X1 - X2)T . (2.2)
We study these two particular matching methods be-

where Yi and fi. are the means of Y and X in th
samples, and i is an estimated regression coefficient of

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

320 Journal of the American Statistical Association, June 1979
1. Estimators of the Response Surface Difference:

7=Yi 2 -Y2 -(V1 - )
Estimators of T: z Estimators of 83: i

for any response surface and all of our estimators. If
the response surfaces were parallel and linear, then the
percentage reduction in expected squared bias would be
100 for the regression adjusted estimates (Tp and Td)

whether random or matched samples were used. But in
2 N

Tp AP = Sx S-xy; SXU = E (Xij - x)TUi
i=1 j=1
2 N

general with imperfect matches and nonlinear response
surfaces, the percentage reduction in expected squared
bias will be less than 100.

td Ad = Sxx SxY; Sxu = E (Xij - Xi - 2 j + X )TU
i=1 j=1

Y on X. The differences between the estimators are thus
confined to estimating the regression coefficient and are

summarized in Table 1: T, is simply the difference of Y
means in the matched samples, Tp is the analysis of co-

variance estimator of r from the two-group design ignoring the paired structure of the matched samples, and Td
is the analysis of covariance estimator of r using the two
group by N matched-pair structure of the matched

samples (equivalently forming matched-pair differences,

3. MONTE CARLO COMPARISONS OF
PROCEDURES - CONDITIONS
Except for the cases noted at the end of Section 2.2,
the computations of percentage reductions in expected
squared bias in matched samples appear to be analytically intractable. Hence we turn to Monte Carlo techniques. Our study can be compactly described as a

2 X 3 X 6 X 4 X 3 X 3 X 8 factorial study with one

summary value (percentage reduction in expected
squared bias) per cell. This summary value was in fact
obtained by a covariance adjustment on 100 replications

Ydi = Ylj- Y2j and Xdj = Xlj- X2j, d iS the estimate of using the first, second, and third moments of X in each
3 found from regressing Ydj on Xdj). Note that Td is the
of the random samples G1, G2 as nine covariates. The

only estimator that requires matched pairs to be assigned
in the matched samples.

Simple algebra shows that the conditional bias of T

given the xij is

w?. - w2. - (x1. - 2.) (Sxx-'Sxw) (2.5)
where wij =W(xij), i= 1, 2, j= 1, ..., N, and for
rT, S 0w 0. With multivariate X and moderate N, the
variance of this conditional bias can be substantial, and
then the expected value of the conditional bias may not
be a good indicator of the utility of a procedure. Hence,
we will use the expected value of the squared conditional

bias to measure the utility of a procedure:

E*El. - f2.V-2(- 21- .-2-) (S Sw)]2 (2.6)
where E* is the expectation over the distribution of X in
matched samples. When r = 1, the expected squared
bias of T is

[EE(zvi.) - E2(z7V1.)]2 + [V1(1l.) + V2(11.)]
= [E1(W(X)) -E2(W(X))]2
+ [V1W(X) + V2W(X)]/N

resultant precision of the value is roughly equivalent to

that obtained with 300 replications and yields standard
errors usually less than 1 percent, although larger in
cases with smaller percentage reductions in expected
squared bias. The Appendix provides details of the design.
The factors in this study are
Factor 1: matching method: metric matching, discriminant matching.

Factor 2: regression adjustment: T Tp, Td-.
Factor 3: ratio of sample sizes, r: 1, 2, 3, 4, 6, 9
(N = 50 for all conditions).

Factor 4: bias along discriminant, B: 14, , 4,11
Factor 5: ratio of variances along discriminant, o2: 1, 2.
Factor 6: ratio of variances orthogonal to discriminant,

02. ' 1, 2.

Factor 7: response surfaces W(X); curvature along and
orthogonal to discriminant: + +, +0, + -,
0+, 0-, -+, -0, --; see Equation (3.2).
The first three factors define the procedures that we
study. The next three factors specify the distributions
of the matching variables in P1 and P2; we assume that
X has the following normal distributions:

where Ei is the expectation and Vi the variance over the
distribution of X in Pi. It follows that the percentage
reduction in expected squared bias resulting from matching and/or regression adjustment is
100 1

Inl Pi X -N P() y [ 0 y]
(3.1)

where

E*[?vl.-1 -22. - (X1. -x2.) (Sx'SXw) ]2 1

[EE(W(x)) -E2(W(x))32+[VlW(x) + V2W(x)]/N,
If the matches were always perfect (x13 = X23, j = 1,

In P2 X -N (0) X0 1]

B - /(1+ = 4 2, 4, 1;
ur2 2l1,2; and 422,,

..,N), then xl. = x2. and wl = 9v2.; hence, the per-

The last factor defines the nonlinear response surface

centage reduction in expected squared bias would be 100

W(X). The + + notation for the eight levels of W(X)

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

Rubin: Multivariate Matched Sampling and Regression Adjustment 321
refers to nonlinearity along the discriminant and non-

where

linearity orthogonal to the discriminant. Specifically,

Ai= 2a2[1 + 0.2(2i-3)] + 2b2[1 + ,2(2i-3)]

we let

It follows that the percentage of variance of W(X) that

W(X) = W(u,v)

can be attributed to the linear regression on X varies
between 70 and 92 percent across all conditions of this

- exp [a ((u - n-q2) + b ( -) v] (3.2)

Monte Carlo study.

where (a,b) = (+2, +2), (+X,O), (+2,-2)
(+- 1 +2)
4. REGRESSION ADJUSTMENTS

(-.1, -2), -2, ?+) (-I, 0), (-2, I 1). Values of a

WITH

RANDOM SAMPLES

were set to 4. 1 instead of 0 in order to avoid cases in

which T is unbiased in random samples; q 2, and V In
appractice, it is not uncommon for researchers to corn-

pear in (3.2) so that the response surfaces (3.2) with the

duct observational studies without any matching. Random samples from P1 and P2 are chosen and regression

distributions of X given by (3.1) are equivalent to the re-

sponse surfaces W(u, v) = exp (au + bv) with the distributions of X standardized so that E1 (X) + E2 (X) = 0 and

adjustments are used to control the X variables. We begin
our study of the Monte Carlo results by considering

2[V1(X) + V2(X)] = .

estimators with r = 1.

The response surfaces given by (3.2) are moderately

When r = 1, T yields no reduction in squared bias with
either matching procedure, and T is the same for both

nonlinear for the distributions given by (3.1). In order
to justify the use of the phrase moderately nonlinear to

matching procedures because it is the usual analysis of
covariance estimator with two groups and no blocking;

describe these response surfaces, we calculate the percentage of the variance of W(X) that can be attributed

therefore, when r = 1, only three of the six possible
estimators defined by the first two factors are of interest.
Although when r = 1 Td metric matched and Td disRi2 = C,(X, W(X))TV,(X)XCi(X, W(X))/Vi(W(X)),
criminant matched use the same units they in general
where C&(, *) is the covariance in Pi. Straightforward
yield different percentage reductions in expected squared
algebra using (3.1), (3.2), and the fact that if t - N(O, 4),
bias because they pair the units in different ways before

to the linear regression on X in Pi:

then

performing the regression adjustment on matched pair

E[exp (yt)] = exp [0>y + 7y2 /2]

differences.

and

Table 2 presents the Monte Carlo percentage reduc-

E[t.exp (-yt)] = (6 + -y4)E[exp (-yt)]

tions in expected squared bias for Tp. Although T, does
quite well in many conditions, especially when o2 = ,2
= 1, in other conditions it does quite poorly, especially

shows that

R =2 = Ai/[exp (Ai) - 1]

2. Percentage Reduction in Expected Squared Bias; Monte Carlo Values for Tp in Random Samples of Size 50
a2

1/2

a2

=12

2

Response

Surface
++
+0

2

2

1/4

1/2

/4

1

1/4

1/2

/4

1/4

1/2

/4

1

93
96

95
98

96
99

87

94

97

97

26
78
96
94
95

25
82
97
97
96

97
98
97
89
89
90
94
85

98
99
98
90
91
95
97
92

++ 80 89 93 95 82 92 95 97 74 88 94
+0 40 81 93 97 96 99 99 100 87 96 98
+
82
91
95
97
79
90
95
97
70
86
93
=2 0+ 64 71 77 81 63 71 77 81 63 71 77
0- 43 34 29 30 41 30 23 24 38 26 18
-+
91
96
97
97
60
79
89
93
-12
02
55
-0 88 96 99 99 96 99 99 100 39 81 94
-- 85 93 95 96 42 69 84 90 -43 -26 42

97
99
97
81
17
79
97
72

1/2
0-+
-0
--

-52 -55 17 54 43 65 80 87
40 81 93 97 96 99 99 100

1

87
87

+-

2

B=

-27

-13

0+ 37
65
72
74
87
88
96
75
88

47

75

51

74

87

92

22 10 05 40 28 18 16 43 33
78
82
66
73
78
82
67
73
93
96
83
91
95
97
87
93
99
99
96
99
99
100
38
81
94
96
83
92
95
97
82
91

++
35
60
77
86
84
92
95
96
85
+0 40 81 93 97 96 99 99 100 87
+48
73
87
93
84
93
96
98
82
=1 0+ 82 84 85 87 83 85 87 89 84
086
88
90
91
86
87
89
91
85
-+
86
94
97
98
89
95
97
98
58
-0
88
96
99
99
96
99
99
100
39
-85
93
97
97
83
92
95
97
39

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

94
96
93
87
87
79
81
68

322 Journal of the American Statistical Association, June 1979
3. Percentage Reduction in Expected Squared Bias for Td, Metric Matching With r = 1
a

/2

2

2

=2

Response

Surface B= 1/4 1/2 /4 1 1/4 1/2 /4 1 1/4 1/2 /4

++ -12 -43 24 66 54 49 87 86 91
+0 53 87 96 99 97 99 99 99 92
+ - -09 12 63 84 60 79 90 93 91
1/2 0+ 47 47 47 44 56 52 67 60 64
0- 74 84 89 92 79 87 93 94 78
-+ 84 94 97 96 89 94 96 97 86
-0 94 98 99 98 97 99 99 99 47
-- 83 93 94 95 87 93 96 97 72

92
98
93
64
87
93
88
86

93
99
94
68
93
95
97
95

93
98
94
65
94
96
99
97

++ 47 70 77 86 88
+0 60 89 96 98 96
+- 60 80 92 95 86
21 0+ 86 87 85 89 89
0- 88 90 92 93 91
-+ 92 97 97 97 91
-0 95 98 99 98 96
-- 88 93 94 94 86

=2

91
99
95
89
91
96
98
89

95
99
97
90
93
97
99
94

95
99
97
89
94
97
99
96

91
92
91
89
91
64
48
47

96
98
97
90
92
83
88
76

94
99
98
91
95
93
97
91

95
98
97
93
95
96
99
96

93
98
94

96
99
97

93
99
97

86
92
81

94
98
94

94
99
97

94
98
97

0- 55 52 54 56 58 59 57 59 54 53
-+ 91 94 95 95 63 81 90 93 06 26
-0 94 98 99 98 96 98 99 99 51 88
-- 82 88 91 92 48 75 88 91 -19 07

56
73
97
65

57
88
99
87

++ 87 89 88 93
+0 57 89 96 98
+- 84 93 96 97

20+

73

77

84

88
96
83

86

75

83

88

87

76

83

88

89

minimum difference is - 15.6, the maximum difference
when o-2 = 02 = 2 and o-2 = 42 = 2. The negative values
is
58.1, 219 differences are positive, and only three
in Table 2 indicate that the regression adjustment acdifferences
are less than -10 (a2 = 2 42= 2, B = tually increases bias. These results show that Ir based on
random samples cannot be counted on to control the
++; a-2 = 1 02 = 2 B = 2, ++; 2 = 2, 02 = B = 4,-) X-variables when the response surfaces are nonlinear.
Some insight into the problem with Ir can be achieved Even though the results for Td are som
by considering the large sample case. In large samples, than for rp, in cases in which rp does po

Of course, with real data we could try fittin
rp is approximately E1(W(X)) - E2(W(X)) - qc where

c is the pooled slope of W(X) on the discriminant, that
the first component of [C1(X, W(X)) + C2(X, W(X))]/
(1 + a-2). From (3.1) and (3.2), we can write 7c
= aB[El(W(X))(a-2/(1 + a-2)) + E2(W(X))/(l + a-2)];
because W (X) and B are positive, 7c has the same
as a. If -2 = 2= 1, then the initial bias, E1(W(X))
- E2(W (X)), has the same sign as a, implying that the

(e.g., quadratic) terms, although the non
is,
be difficult to detect because these respo
only moderately nonlinear. Our study do
quadratic terms in the regression adjust
include matched sampling. Hence, we turn
sign
r > 1 to see if matched sampling improve
of r.

adjustment - qc is in the correct direction. However, if
$12 X 1 and/or 02 $ 1, then the adjustment may be in the
wrong direction and actually increase bias.

5. RESULTS WITH r > 1

Somewhat surprising, estimating the regression coeffi- We now consider the utility of matched sampling, alone
and in combination with regression adjustment. Because
cient from matched-pair differences in the random
the analyses of the 7-factor Monte Carlo study are somesamples can result in better estimates. Table 3 presents
what
involved, we begin in Section 5.1 by presenting
Monte Carlo values for the percentage reduction in
specific
results for two procedures in order to convey the
expected squared bias for Td metric matched with r = 1;
flavor
of
our conclusions. The remainder of Section 5
Td is superior to rp in 209 of 288 cases, and in only two
presents
detailed
analyses of the results of the Monte
cases (a-2 = 1 2 = 1, B = 2, ++ and a-2 = 2, 2 = 1,
Carlo
study
with
r > 1. Section 5.2 presents an analysis
B-4, ) do the results favor Tp by more than 5
of
variance
(ANOVA)
of the study. Section 5.3 shows
percent. The fact that Id is usually better than Ir is conthat
although
the
difference
between metric and dissistent with Monte Carlo results and analytic consideracriminant
matching
varies
with
the estimator, the distions presented in Rubin (1973b) for the univariate case.
tribution
of
X,
and
the
response
surface,
metric matching
The results for Td discriminant matched are similar but
is
clearly
superior
to
discriminant
matching.
Section 5.4
inferior to the results for Ird metric matched. The mean of
focuses
on
the
results
for
metric
matching
and
concludes
the 288 metric minus discriminant differences in perthat
rd
is
the
best
regression
adjustment
procedure
that
centage reduction in expected squared bias is 5.4, the

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

Rubin: Multivariate Matched Sampling and Regression Adjustment 323
we have considered. Section 5.5 displays results for Td

metric matched that can be used to suggest ratios of
sample sizes needed to remove nearly all of the bias for a
variety of nonlinear response surfaces.

5.2 An ANOVA of the Results When r > 1
Table 6 presents an ANOVA of the 7-factor study

where factor 3 has five levels r = 2, 3, 4, 6, 9. For

simplicity of display, the response surface factor and the

three distribution of X factors have been collapsed into

5.1 Two Specific Estimators

one "distribution" factor with 288 levels. In fact, little

Tables 4 and 5 present percentage reductions in ex-

pected squared bias for metric matched samples with
r = 2 for Ir and Td. By comparing Tables 3 and 5, we
immediately see advantages to matching even with

r = 2. The estimator -rd metric matched with r = 2
usually removes most of the squared bias and is clearly
better than Td (or Ir) with r = 1. Of the 288 differences

between the percentage reductions in expected squared
bias for Td metric matched with r = 2 and Td metric
matched with r = 1, only two are negative; of the 288

differences between the percentage reductions in expected
squared bias for ITd metric matched with r = 2 and A
metric matched with r = 1, only seven are negative; and

the most negative of these nine differences is only -1.
In the next sections we will see that in those cases that
are difficult for matching (e.g., o-2 = = 2), larger ratios
result in even better estimates.
By comparing Tables 4 and 5 we see that in all cases

except with a-2 = 2= 2, there is an advantage to using
regression adjustment on matched samples. When

a-2 - = 2, T is superior to rd in a few cases, but Td is
usually better. Without knowledge of the response surface, Td is to be preferred to I; with such knowledge, a
more appropriate regression adjustment can be used. We
will see that Td is in this sense always preferable to

information was lost by collapsing the distributional
factors because of numerous large higher-order interac-

tions among the distributional factors and because larger
interactions between procedure factors and distributional
factors tended to involve higher-order interactions among

the distributional factors. The purpose of this ANOVA
is simply to see which are the large sources of variation.
Table 7 summarizes the procedure factors by the
average value of the percentage reduction in expected
squared bias over the 288 distributional conditions. If

there were no interactions between procedures and condi-

tions, then Table 7 would be an adequate summary for
our Monte Carlo study; that is, there would be good and
bad procedures and easy and hard distributional conditions, but comparisons between procedures would be
the same in each distributional condition. However,

there are nontrivial interactions between procedures and
distributions in the sense that if we fit the procedure-

plus-distribution additive model to the 7-factor study, we
are left with some large residuals to explain. Although
Table 7 is not an entirely adequate summary for our
study, we will see in Sections 5.3 through 5.5 that most

trends displayed there are not misleading. The major
trends in Table 7 are that
1. Metric matching is superior to discriminant
matching.

T. (or Tp).

4. Percentage Reduction in Expected Squared Bias for Toy Metric Matching With r = 2
02

=1/2

2

2

2

Response

Surface

B=

1/4

1/2

/4

1

1/4

1/2

/4

1

1/4

1/2

++
95
88
88
82
83
77
73
67
50
49
+0 95 96 94 87 89 85 79 71 65 61
+
95
91
92
87
87
82
77
70
60
54
=1/2 0+ 98 94 90 80 96 93 88 78 92 87
0- 98 98 95 91 98 97 93 77 95 94
-+
99
99
99
98
99
98
98
96
93
96
-0 99 99 99 98 98 98 97 95 85 93
-99
99
99
97
99
98
97
95
91
96
++
80
83
84
80
71
74
71
65
+0
96
96
94
88
89
87
81
72
+
91
90
89
84
82
80
76
68
2=1 0+ 83 83 79 73 79 76 73 68
090
91
93
92
90
92
91
90
-+
99
99
99
98
95
98
98
96
-0
99
99
99
98
98
98
97
95
-98
99
99
98
93
98
98
96

2

+
+
56
70
75
71
60
+0
95
95
94
88
89
+
71
78
79
75
69
20+
56
60
58
57
56
055
50
51
55
60
+
89
96
98
99
72
-0
99
99
98
97
97
-89
96
98
98
75

64
86
71
56
56
93
97
92

63
80
67
55
56
97
97
96

50
67
63
70
88
81
85
77

57
45
72
67
61
54
51
51
63
60
97
54
95
84
96
51

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

/4

1

49
46
57 51
52
48
78 67
89 82
93
90
90 87
93
90

53
50
64
58
59
54
67 62
89
87
93
93
93
91
92
91

45
51
48
57
83
90
88
89

45
63
53
48
63
82
93
80

39
50
43
42
66
91
87
88

43
57
48
47
64
91
90
90

324 Journal of the American Statistical Association, June 1979
5. Percentage Reduction in Expected Squared Bias for Td, Metric Matching With r = 2
O"2

=1/2

s2=

r2

=2

Response

Surface

B=

1/4

1/2

3/4

1

1/4

1/2

3/4

1

1/4

1/2

3/4

1

++
99
97
96
97
98
96
98
98
95
97
97
97
+0 99 100 99 100 99 100 100 100 96 99 99 99
+98
97
97
96
97
97
98
97
95
97
97
96
21/2 0+ 99 98 97 95 99 98 98 96 98 98 97 96
0- 100 99 99 99 100 100 99 99 99 99 99 99
-+ 100 100 100 100 99 100 100 100 93 97 99 100
-0 100 100 100 100 99 100 100 100 79 95 99 100
-- 100 100 100 100 99 99 100 100 89 95 98 99
++
98
98
98
+0 99 100 100

97
98
100 99

98
99
99
95
98
96
98
100 100 100 96 99 99 99

+-

98

99

98

99

99

98

99

98

95

98

97

97

21

0+
97
97
97
96
98
96
97
97
96
96
95
97
098
98
98
98
98
98
98
98
97
97
98
98
- + 99 100 100 100 98 99 100 100 80 90 97 99
-0 100 100 100 100 99 100 100 100 79 95 99 100
-- 99 100 100 100 97 98 99 100 74 86 95 98
++
93
96
98
97
93
97
98
98
92
97
96
98
+0 98 99 100 100 99 99 100 100 96 99 99 99
+
92
96
98
98
93
97
98
98
91
96
98
98

2

0+

88

90

92

93

89

92

93

92

88

92

080
76
78
80
81
81
80
79
78
77
-+
94
97
99
99
84
93
97
99
51
62
-0 100 100 100 100 99 100 100 100 77 94
-93
97
99
99
80
90
96
98
41
50

93

96

77
79
87
95
99 100
82
93

2.
Td
iS
superior
to
r,
especially
X and response surfaces, using larger ratios for matching
samples, and both Td and Ir are superior to 0
especially for smaller r.

3. Larger ratios of sample sizes are better, although
only modest benefits accrue when moving from
r = 2 to larger ratios, the benefit being largest

can result in substantial improvements.

7. Percentage Reduction in Expected Squared Bias
Averaging Over Distributional Conditions

with Ir and smallest with Td.

Discriminant Metric
Matching Matching

Further analysis will show that the conclusion from 1.

and 2., to the effect that Td metric matched is the best
combination of matching method and regression adjustment, is correct. However, the conclusion from 3., to the
effect that ratios larger than 2 are not needed, is not

always true; with some combinations of distributions of
6. Analysis of Variance of 7-Factor
Monte Carlo Study With r > 1

Source

Degrees of Mean
Freedom Square

Matching Method (metric, discriminant) 1 36.615

Regression Adjustment (To'tp'td) 2 7.414
Ratio (r = 2,3,4,6,9) 4 .705
Distributiona 287 .442
M.M.
x
R.A.
2
.430
M.M.
x
Ratio
4
.214

M.M. x Distribution 287 .241
R.A.
x
Ratio
8
.114
R.A. x Distribution 574 .027
Ratio x Distribution 1148 .004
M.M. x R.A. x Ratio 8 .051
M.M. x R.A. x Distribution 514 .007
M.M. x Ratio x Distribution 1148 .002
R.A. x Ratio x Distribution 2296 .002
M.M. x R.A. x Ratio x Distribution 2296 .001
a Factors 4, 5, 6, and 7 defined in Section 3.

Ratio
1
2

To

tp

00" 78
71
83

7.]

78

to

tp

td

84

00" 78
81
91

96

84

3
4

74
75

84
84

85
85

88
90

94
95

97
98

6
9

74
75

84
85

85
86

94
95

96
97

99
99

Theoretical values

5.3 Metric Matching vs. Discriminant Matching
Table 6 indicates that the matching method factor and
its interactions with distributional factors make a large

contribution to the variation in the Monte Carlo study.
Table 7 suggests that metric matching is on the average

superior to discriminant matching. But these tables do

not tell us whether the interaction between matching
method and distribution is the result of a varying superiority of metric matching over discriminant matching or
an occasional superiority of discriminant matching. If
discriminant mnatching were better than metric matching

for only some distributions of X, then we should decide
which matching method to use on the basis of the observed distribution of X in G1 and G2. Our results clearly

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

Rubin: Multivariate Matched Sampling and Regression Adjustment 325
strong prior knowledge suggests a response surface that

8. Summary of Differences in Percentage Reduction in
Expected Squared Bias for Each Estimator: Metric

has curvature increasing as one moves from the P2 range

Matching Minus Discriminant Matchinga

of X to the PI range of X, and even then little can be lost
by metric matching especially if r > 4.

Ratio
r=

2

3

4

6

This conclusion holds for Ao even when the response

9

surface is linear in the discriminant (i.e., W(X)
= X(1, 0)T). Of the 3(a2) X 3( 2) X 4(B) X 5(r) = 180

r,, (metric) min -19.9 -16.0 -13.0 -7.8 -6.1
- ,, (discrim) max 124.5 120.2 137.6 139.3 175.6
mean 9.9 13.3 15.4 19.2 20.6
# > 0 194 219 232 245 254

differences in percentage reduction in expected squared

5p (metric) min -2.0 -.7 -.7 -.4 -.5
- Tp (discrim) max 102.0 73.4 90.2 83.1 82.9

cent, and of these, 6 favored discriminant matching (five

bias for To (Mahalanobis metric matched) minus To
(discriminant matched), only 26 were greater than 2 per-

mean 8.9 9.8 10.9 12.0 12.7
# >0 262 260 256 259 262

3 percent differences, one 4 percent difference), whereas

rd (metric) min -1.2 -1.3 -.6 -.4 -.2
- Td (discrim) max 107.6 79.1 90.1 83.9 81.6

differences, six 4 percent, two 5 percent, four 6 percent,

mean 11.5 12.0 12.9 13.2 13.2
# > 0 269 264 264 271 273

20 favored Mahalanobis metric matching (four 3 percent

three 7 percent, and one 9 percent difference).
Because there seems to be no reason to recommend
discriminant matching over metric matching, we restrict

a 288 differences for each estimator, one for each distributional condition in Monte
Carlo study.

further investigation of the Monte Carlo study to results
obtained by metric matching.

show metric matching to be superior to discriminant
matching for all distributions of X considered.

Focus on one estimator, that is, one adjustment

5.4 Comparing Regression Adjustments
Table 10 compares the regression adjustments (based

on metric
matched samples) for each ratio across the 288
( rol Td, Tp) and one ratio (r = 2, 3, 4, 6, 9); for
each of
the 288 distributional conditions, take the difference bedistributional conditions. The comparison of Td with Tp
shows that although there is usually not much difference
tween the percentage reduction in expected squared bias
nant matching. If all 288 differences were positive, we

between the adjustments, Td iS clearly superior to T, the
most negative difference in percentage reductions in

would know that metric matching was superior to dis-

expected squared bias being -2.2 percent when r = 2.

obtained by metric matching and obtained by discrimi-

The comparison of Td with r, in Table 10 shows that
although Td is usually substantially better than To, To iS
better than Td in a few cases. All five cases having differences in percentage reductions in expected squared bias
tional conditions. With Td and TAp there are essentially nofavoring To by 10 percent or more occur when o2 = = 2,
three with r = 2 and two with r = 3. Tables 4 and 5
cases where discriminant matching is to be preferred to
provide the results for ro and Td when r = 2 and show
metric matching; the most negative difference is only
that without strong prior knowledge of the response sur-2 percent. With To,? metric matching is usually superior

criminant matching for that estimator (e.g., A with

r = 2).
Table 8 summarizes the 3 X 5 = 15 metric minus
discriminant sets of differences across the 288 distribu-

to discriminant matching, but further study of these
differences is enlightening. All cases where the differences
are < -10 percent occur when a2 = 2 and 42 = 2: five

cases when r = 2, three cases when r = 3, and two cases
when r = 4. Table 9 displays the metric minus dis-

criminant differences for To with r = 2, 3, 4 for U2 = 2
and 42 = 1. Clearly, even for these distributions of X,
metric matching is to be preferred unless exceptionally

face, Td is better than ro even when c2 = t2 = 2; a
researcher having rather specific knowledge of the response surface should be fitting a model relevant to that
response surface and should not be using a linear approximation (Td or T) or no adjustments (u).
The conclusion thus far is simple: Use Td based on
metric matched samples. It remains for us to summarize
the efficacy of using different ratios for the matching.

9. Differences in Percentage Reduction in Expected Squared Bias: To (Metric Matched) Minus To (Discriminan
Matched) When Distributions of X Are Relatively Favorable to Discriminant Matching (0-2 = 2, e2 = 1/2)
r=2
B=

1/4

1/2

++ -08 -15 -18
+0 -01 -04 -03
+- -04 -20 -13
0+ 63 71 35
0- 52 49 30
-+ 18 16 10
-0 11 -01 -01
-- 32 26 10

/4

1

-12
-01
-08
24
20
11
-01
06

r=3

1/4

1/2

/4

r=4
1

-03 -16 -15 -10
-03 -04 -03 -02
-01 -07 -09 -08
64 64 57 57
55 73 36 30
17 16 08 07
24 01 -01 -01
27 26 11 06

1/4

05
01
16
74
61
21
20
29

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

1/2

?4

-05 -13
-03 -03
-01 -08
94 88
45 31
13 09
00 -01
15 09

1

-12
-02
-06
61
29
06
-00
06

326 Journal of the American Statistical Association, June 1979
10. Summary of Differences in Percentage Reduction

within the context of our Monte Carlo study, these repre-

in Expected Squared Bias for Estimators Based

sent the worst results that can be obtained by using Td
metric matched. We see that increases in r result in im-

on Metric Matched Samples

proved estimation. When r = 4, Td metric matched removes 73 percent of the expected squared bias in the

Ratio

r=

2

3

4

6

9

worst case and usually removes more than 90 percent

even when a2 = V = 2. The most difficult cases are t

rd (metric) min -2.2 -.6 -.0 -.1 -.7

- Tp (metric) max 34.1 26.3 20.9 16.2 12.6
mean

4.2

3.4

3.0

2.2

with small initial bias. Using a ratio equal to 2a2'2 for

1.8

matching usually removes most of the bias.

# > 0 275 281 285 284 283

6. DISCUSSION

Td (metric) min -29.8 -16.1 -9.5 -9.3 -6.0

- To (metric) max 59.7 45.0 37.2 26.9 21.6
mean

14.9

9.2

7.5

5.0

The general conclusion from our analyses of the

3.8

# > 0 270 259 270 271 270

Monte Carlo study is that the best procedure we have

considered is Td (the regression-adjusted estimator based
on matched pair differences) using large r (large ratio of

5.5 The Effect of Using Different Ratios for Matching

size of reservoir to size of matched sample) and metric

Tables 2 through 5 give specific results for the estima-

matched samples (specifically, using the Mahalanobis

tors T (r = 1), Td(r = 1, metric), T (r = 2, metric), and
Td (r = 2, metric). These tables show that in some cases

metric, (2.1)). Obtaining G2 with large r can be expensive
in practice, however, and we have seen that the im-

any of these estimators can remove nearly all of the

provements that accrue from using r = 9 rather than

squared bias, whereas in other cases even the best of

r = 2 are modest, except when the spread of the X

them, Td with r = 2, removes less than 50 percent of

distribution is larger in P1 than P2. Tentative advice

squared bias. The results are sensitive to the distribution

would be to metric match using a ratio of 2a202 (i.e.,

of X and the response surface, especially when r = 1.

perhaps twice the determinant of N1 12-' where li is the

Table 5 shows that if a2 < 1 and T2 < 1, d metric

covariance of X in Pi) and perform regression adjustments on matched pair differences. Our results demon-

matched with r = 2 can be counted on to remove most

of the bias, and in most cases with either a2 > 1 or t2 > 1

strate quite clearly that matching can dramatically im-

usually removes more than 90 percent of the bias. The

prove estimation.

clearest need for improved estimation occurs when both

Of course, a realistic criticism of this work is that we

a2 > 1 and 42 > 1. Increasing r yields better estimates.

have not considered other procedures, ones that carefully

In order to indicate the advantages of increasing r, we

try to search for nonlinear components in the response

present Table 11, which gives "pessimistic" results for

surfaces or try to perform sophisticated Bayesian or

Td metric matched. Pessimistic means that for each ratio

empirical Bayesian analyses that average over a variety

and each distribution of X defined by a value of (B, a2, 42),

of nonlinear models for the response surfaces. Our reaction

we have produced the minimum percentage reduction in

to this criticism is that although we hope that in any real

squared bias over the eight response surfaces. Hence,

data analysis such techniques would be applied, con-

11. Pessimistic Percentage Reductions in Expected Squared Bias for Td, Metric Matched
O'2
r

(2

B=?

?/4

?/2

=1/2

3/4

1

?/4

0,2

?'2

?/4

=
1

?

(2
?

?

=2

1/2

/4

1

1/2
-12
-43
24
44
54
49
67
60
47
64
68
1
47
70
77
86
86
89
90
89
47
76
91
2
55
52
54
56
48
59
57
59
-19
07
56

2

1/2
1
2

3

4

98

97
80

97

97
76

96

97
78

95
96
80

97
97
80

96
96
81

98
97
80

96
97
79

79
74
41

1/2
99
99
99
98
97
98
98
98
85
1
99
99
99
99
94
98
98
98
82
2
88
84
83
83
87
85
85
85
61

65
93
57

95

97

96

86
50

95
77

97
79

96
88
65

98
96
82

97
97
85

1/2
100
99
100
99
98
98
99
99
92
97
98
1
99
99
99
99
98
98
98
99
88
92
97
2
92
89
90
88
90
92
91
89
73
77
89

98
97

89

6

1/2 99
1
99
2
94

100 100 100 99 99 100 100 94 98 99 98
99
99
99
98
99
99
99
94
94
98
97
94
94
94
94
94
94
92
80
81
92
92

9

1/2 100 100 99 100 100 99 100 100 97 99 99 99
1
99
99
100
100
99
97
99
100
95
99
99
99
2
96
96
96
96
95
96
96
96
85
85
95
95

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

Rubin: Multivariate Matched Sampling and Regression Adjustment 327
sidering all of them in a Monte Carlo study is impossible

using the first three moments in G1 and G2 as simula-

because good data-analytic techniques must be, by

tion covariates resulted in the most cost-effective plan.

nature, conditional on the observed data. Presumably,

More covariates would have been more expensive be-

such sophisticated estimators would do at least as well

cause extra storage would have been required in core,

as Td, and so the results for Td (metric matched) can be

and fewer covariates would have been more expensive

thought of as minimums for methods that try to estimate

because additional simulations would have been needed

the response surface by more than a linear fit. We feel

to obtain the same precision. Roughly speaking, our use

that the general benefits from obtaining matched samples

of simulation covariates allowed us to reduce the number

would hold for more sophisticated estimators because

of simulations by a factor of 3. Typically, the squared

with matched samples the sensitivity of the regression

multiple correlation between BIAS2 and the simulation

adjustment to model specification is reduced.

In conclusion, because we feel that results similar to

covariates was about .6 and higher when the standard
errors were higher; hence, without the covariance ad-

ours would be obtained for more than two matching

justment we would have needed roughly NSIM = 300

variables, for nonnormal matching variables, and other

in order to obtain the precision that was obtained using

nonlinear response surfaces, we feel that an effective plan

NSIM = 100 and simulation covariates.

for controlling matching variables in observational studies

The issues of minimizing the number of random num-

is to perform regression adjustments on matched samples

bers generated in order to save cost and correlating the

obtained by nearest available Mahalanobis metric pair

estimates across conditions to increase precision of com-

matching. Of course this summary of advice assumes the

parisons are really handled in the same manner. First,

sampling situation described in Section 2, with dependent

focus on one distributional condition and consider a fixed

variables recorded in the matched samples but not

ratio r and a fixed matching method. Then we want to

recorded in the initial random samples.

compare the three adjustments ,O, rp, T dwith respec
BIAS2. By calculating all adjustments on the same

APPENDIX: DISCUSSION OF MONTE CARLO STUDY
The Monte Carlo study is a 7-factor study as described
in Section 3. The first three factors define 36 procedures,

and the last four factors define 288 distributional conditions. For the moment, focus on one condition and one

procedure: Over repeated matched samples we wish to
know the expected value of BIAS2, expression (2.6).

Letting NSIM be the number of sampling replications
in each condition, we could simply perform NSIM replica-

tions in each condition, drawing samples independently
across the conditions and independently for each pro-

cedure. Such a sampling scheme would be more expensive
than necessary for several reasons. First, it would generate

many more random numbers than needed. Second, it
would not provide efficient comparisons of procedures

within conditions or a procedure across conditions because the independent sampling would not have created
correlated estimates of BIAS2. Third, no attempt would
have been made to increase the precision of the study by

using simulation covariates, quantities that are defined
for each procedure and condition and correlated with
BIAS2, but whose expectations we know from analytic
considerations, for example, the moments of X in the
random samples G1 and G2.

First consider the third point, using simulation covariates to increase precision. In our study with normal
random variables, we know the expectations of all
sample moments in G1 and G2, and these should be related
to the ease of obtaining well-matched samples and the

utility of regression adjustment. For example, G1 and G2
samples with means farther apart than usual should
imply that the resultant matched samples will have

means farther apart than usual. We can let the data

estimat~e these relationships between sample moments
in G1 and G2 and BIAS2. Pilot studies indicated that

matched sample, we make the estimates of BIAS2 correlated across adjustments and hence make comparisons

more precise. Now let us consider different matching
methods with the same ratio; we cannot use the same

matched samples from G2 but we can and do use the same
random samples (G1, G2) for matching. For example,

BIAS2 for To r = 2 metric and To r = 2 discriminant are
calculated in matched samples obtained from the same
random samples. Using the same random samples cor-

relates the metric and discriminant results, increasing the
precision when comparing metric matching with dis-

criminant matching for each estimator. Finally, consider

different ratios for matching; we cannot use the same
random samples G2, but we can and do use the same

random samples G1 and overlapping random samples G2
(i.e., the r = 2 G2 sample includes the r = 1 G2 sample,

the r = 3 G2 sample includes the r = 2 G2 sample, etc.).
Using overlapping random samples correlates the results
for different ratios and hence increases the precision of
comparisons between estimators using different ratios.
Furthermore, we can correlate results across response
surfaces and distributions of X, thereby increasing precision of comparisons of procedures between distributional conditions and reducing computational costs by

reducing the number of random deviates that must be
generated. Correlating results across response surfaces is
trivial because all nine response surfaces can be studied
from the same matched sample. Correlating results across
distributions of X is done by having the 4 X 3 X 3 cases
use the same N(0, 1) deviates. Only the G1 sample needs
to be linearly transformed in accordance with equation
(3.1).
The N (0, 1) deviates were generated by Marsaglia's
rectangle-wedge-tail method described in Knuth (1969).

[Received January 1978. Revised December 1978.]

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

328 Journal of the American Statistical Association, June 1979
REFERENCES
Althauser, R.P., and Rubin, D.B. (1970), "The Computerized Construction of a Matched Sample," American Journal of Sociology,
76, 325-346.
Billewicz, W.Z. (1964), "Matched Samples in Medical Investiga-

Lord, F.M. (1960), "Large-Sample Covariance Analysis When the
Control Variable Is Fallible," Journal of the American Statistical
Association, 55, 307-321.
McKinlay, S.M. (1974), "The Expected Number of Matches and Its

Variance for Matched-Pair Designs," Applied Statistics, 23,

372-383.
tions," British Journal of Preventative Social Medicine, McKinlay,
18, 167-173.
S.M. (1975a), "The Design and Analysis of the ObservaBillewicz, W.Z. (1965), "The Efficiency of Matched Samples: An
tional Study-A Review," Journal of the American Statistical
Empirical Investigation," Biometrics, 21, 623-643.
Association, 70, 503-520.
Campbell, D.T., and Erlebacher, A. (1970), "How Regression ArtiMcKinlay, S.M. (1975b), "The Effect of Bias on Estimators of
facts in Quasi-Experimental Evaluations Can Mistakenly Make
Relative Risk for Pair-Matched and Stratified Samples," Journal
Compensatory Education Look Harmful," in The Disadvantaged
of the American Statistical Association, 70, 859-864.
Child, Volume 3, Compensatory Education: A National Debate, ed.
Rubin, D.B. (1973a), "Matching to Remove Bias in Observational
J. Hellmuth, New York: Brunner/Mazel.
Studies," Biometrics, 29, 159-183.
Carpenter, R.G. (1977), "Matching When Covariables Are Normally
Rubin, D.B. (1973b), "The Use of Matched Sampling and RegresDistributed," Biometrika, 64, 299-307.
sion Adjustment to Remove Bias in Observational Studies,"
Cochran, W.G. (1953), "Matching in Analytical Studies," American
Biometrics, 29, 185-203.
Journal of Public Health, 43, 684-691.
Rubin, D.B. (1974), "Estimating Causal Effects of Treatments in

Cochran, W.G. (1963), Sampling Techniques, New York: John
Wiley & Sons.
Cochran, W.G. (1968), "The Effectiveness of Adjustment by Subclassification in Removing Bias in Observational Studies," Biometrics, 24, 295-313.

, and Rubin, D.B. (1973), "Controlling Bias in Observational
Studies: A Review," Sankhya-A, 35, 417-446.
Gilbert, J.P., Light, R.J., and Mosteller, F. (1975), "Assessing Social
Innovation: An Empirical Base for Policy," in Evaluation and
Experiment: Some Critical Issues in Assessing Social Programs, ed.
A.R. Lumsdaine and C.A. Bennett, New York: Academic Press.
Greenberg, B.G. (1953), "The Use of Analysis of Covariance and
Balancing in Analytical Surveys," American Journal of Public
Health, 43, 692-699.

Knuth, D.E. (1969), Seminumerical Algorithms, Volume 2, Reading,
Mass.: Addison-Wesley.

Randomized and Nonrandomized Studies," Journal of Educational
Psychology, 66, 688-701.
Rubin, D.B. (1976a), "Multivariate Matching Methods That Are
Equal Percent Bias Reducing, I: Some Examples," Biometrics, 32,
109-120.

Rubin, D.B. (1976b), "Multivariate Matching Methods That Are
Equal Percent Bias Reducing, II: Maximums on Bias Reduction
for Fixed Sample Sizes," Biometrics, 32, 121-132.

Rubin, D.B. (1977), "Assignment to Treatment Group on the Basis
of a Covariate," Journal of Educational Statistics, 2, 1-26.
Rubin, D.B. (1978a), "Bayesian Inference for Causal Effects: The
Role of Randomization," The Annals of Statistics, 7, 34-58.
Rubin, D.B. (1978b), Bias Reduction Using Mahalanobis Metric
Matching, Research Bulletin 78-17, Princeton, N.J.: Educational
Testing Service.

This content downloaded from 130.82.31.210 on Tue, 11 Sep 2018 09:05:54 UTC
All use subject to https://about.jstor.org/terms

