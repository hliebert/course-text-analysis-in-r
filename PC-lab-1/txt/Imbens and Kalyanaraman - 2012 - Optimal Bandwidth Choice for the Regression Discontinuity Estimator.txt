Review of Economic Studies (2012) 79, 933–959
doi:10.1093/restud/rdr043
© The Author 2011. Published by Oxford University Press on behalf of The Review of Economic Studies Limited.
Advance access publication 24 November 2011

GUIDO IMBENS
Harvard University

and
KARTHIK KALYANARAMAN
University College London
First version received August 2009; final version accepted August 2011 (Eds.)
We investigate the choice of the bandwidth for the regression discontinuity estimator. We focus
on estimation by local linear regression, which was shown to have attractive properties (Porter, J. 2003,
“Estimation in the Regression Discontinuity Model” (unpublished, Department of Economics, University
of Wisconsin, Madison)). We derive the asymptotically optimal bandwidth under squared error loss.
This optimal bandwidth depends on unknown functionals of the distribution of the data and we propose
simple and consistent estimators for these functionals to obtain a fully data-driven bandwidth algorithm.
We show that this bandwidth estimator is optimal according to the criterion of Li (1987, “Asymptotic
Optimality for C p , C L , Cross-validation and Generalized Cross-validation: Discrete Index Set”, Annals
of Statistics, 15, 958–975), although it is not unique in the sense that alternative consistent estimators for
the unknown functionals would lead to bandwidth estimators with the same optimality properties. We
illustrate the proposed bandwidth, and the sensitivity to the choices made in our algorithm, by applying
the methods to a data set previously analysed by Lee (2008, “Randomized Experiments from Non-random
Selection in U.S. House Elections”, Journal of Econometrics, 142, 675–697) as well as by conducting a
small simulation study.
Key words: Optimal bandwidth selection, Local linear regression, Regression discontinuity designs,
Cross-validation
JEL Codes: C13, C14, C21

1. INTRODUCTION
Regression discontinuity (RD) designs for evaluating causal effects of interventions, where assignment to the intervention is (partly) determined by the value of an observed covariate exceeding a threshold, were introduced by Thistlewaite and Campbell (1960). See Shadish, Campbell
and Cook (2002) and Cook (2008) for a historical perspective. A recent surge of applications in
economics includes studies of the impact of financial aid offers on college acceptance (Van Der
Klaauw, 2002), school quality on housing values (Black, 1999), class size on student achievement (Angrist and Lavy, 1999), air quality on health outcomes (Chay and Greenstone, 2005),
incumbency on re-election (Lee, 2008), and many others. Recent important theoretical work has
dealt with identification issues (Hahn, Todd and Van Der Klaauw, 2001, HTV from hereon), optimal estimation (Porter, 2003), tests for validity of the design (McCrary, 2008), quantile effects
933

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

Optimal Bandwidth Choice for
the Regression Discontinuity
Estimator

934

REVIEW OF ECONOMIC STUDIES

2. BASIC MODEL
In the basic RD setting, researchers are interested in the causal effect of a binary treatment.
In the setting, we consider that we have a sample of N units, drawn randomly from a large
population. For unit i, for i = 1, . . . , N , using Rubin’s (1974) potential outcome notation, the
variable Yi (1) denotes the potential outcome for unit i given treatment and Yi (0) denotes the
potential outcome without treatment. For unit i, we observe the treatment received, Wi , equal to
1 if unit i was exposed to the treatment and 0 otherwise, and the outcome corresponding to the
treatment received:
(
Yi (0) if Wi = 0,
Yi = Yi (Wi ) =
Yi (1) if Wi = 1.
We also observe for each unit a scalar covariate, called the forcing variable, denoted by X i . In
Section 5, we discuss the case with additional covariates. Define
m(x) = E[Yi |X i = x],
to be the conditional expectation of the outcome given the forcing variable. The idea behind the
sharp regression discontinuity (SRD) design is that the treatment Wi is determined solely by the
value of the forcing variable X i being on either side of a fixed and known threshold c or:
Wi = 1 X i ≥c .
1. Matlab and Stata software for implementing this bandwidth rule is available on the Web site http://www.
economics.harvard.edu/faculty/imbens/imbens.html.

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

(Frandsen, 2008; Frölich and Melly, 2008), and the inclusion of covariates (Frölich, 2007).
General surveys include Imbens and Lemieux (2008), Van Der Klaauw (2008), and Lee and
Lemieux (2010).
In RD settings, analyses typically focus on the average effect of the treatment for units with
values of the forcing variable close to the threshold, using local linear, or global polynomial
series estimators. Fan and Gijbels (1992) and Porter (2003) show that local linear estimators are
rate optimal and have attractive bias properties. A key decision in implementing local methods
is the choice of bandwidth. In current practice researchers use a variety of ad hoc approaches
for bandwidth choice, such as standard plug-in and cross-validation methods from the general
non-parametric regression literature (e.g. Fan and Gijbels, 1992, Härdle, 1992, Wand and Jones,
1994). These are typically based on objective functions which take into account the performance
of the estimator of the regression function over the entire support and do not yield optimal
bandwidths for the problem at hand. There are few papers in the literature that use bandwidths
which focus specifically on the RD setting (Ludwig and Miller, 2007; DesJardins and McCall,
2008; see discussion later in the paper), and none with optimality properties. In this paper, we
build on this literature by (i) deriving the asymptotically optimal bandwidth under squared error
loss, taking account of the special features of the RD setting, and (ii) providing a fully datadependent method for choosing the bandwidth that is asymptotically optimal in the sense of Li
(1987).1 Although optimal in large samples, the proposed algorithm involves initial bandwidth
choices and is not unique. We analyse the sensitivity of the results to these choices. We illustrate
our proposed algorithm using a data set previously analysed by Lee (2008) and compare our
procedure to global methods and other local methods based on other error criteria. Simulations
indicate that our proposed algorithm works well in realistic settings.

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

935

In Section 5, we extend the SRD setup to the case with additional covariates and to the fuzzy
regression discontinuity (FRD) design, where the probability of receiving the treatment jumps
discontinuously at the threshold for the forcing variable, but not necessarily from zero to one.
In the SRD design, the focus is on average effect of the treatment for units with covariate
values equal to the threshold:

Now suppose that the conditional distribution functions FY (0)|X (y|x) and FY (1)|X (y|x) are continuous in x for all y and that the conditional first moments E[Yi (1)|X i = x] and E[Yi (0)|X i = x]
exist and are continuous at x = c. Then
τSRD = μ+ − μ− , where μ+ = lim m(x)
x↓c

and

μ− = lim m(x).
x↑c

Thus, the estimand is the difference of two regression functions evaluated at boundary points.
We focus on estimating τSRD by separate local linear regressions on both sides of the threshold. We view local non-parametric methods as attractive in this setting compared to methods
based on global approximations to the regression function (e.g. higher-order polynomials applied to the full data set) because local methods build in robustness by ensuring that observations with values for the forcing variable far away from the threshold do not affect the point
estimates. Furthermore, in the RD setting, local linear regression estimators are preferred to the
standard Nadaraya–Watson kernel estimator because local linear methods have attractive bias
properties in estimating regression functions at the boundary (Fan and Gijbels, 1992) and enjoy
rate optimality (Porter, 2003).
To be explicit, we estimate the regression function m(∙) at x as
(
α̂− (x) if x < c,
m̂ h (x) =
(1)
α̂+ (x) if x ≥ c,
where
(α̂− (x), β̂− (x)) = arg min
α,β

N
X
i=1

2

1 X i <x ∙ (Yi − α − β(X i − x)) ∙ K




Xi − x
,
h

where K (∙) is a kernel function described later, and h is the bandwidth, and,


N
X
Xi − x
2
(α̂+ (x), β̂+ (x)) = arg min
1 X i >x ∙ (Yi − α − β(X i − x)) ∙ K
.
α,β
h
i=1

Then, we can write the estimator for τSRD as the difference in two regression estimators,
τ̂SRD = μ̂+ − μ̂− ,
where the two regression estimators are
μ̂− = lim m̂ h (x) = α̂− (c)
x↑c

and μ̂+ = lim m̂ h (x) = α̂+ (c).
x↓c

The focus in this paper is on the optimal choice for the bandwidth h.
3. ERROR CRITERION AND INFEASIBLE OPTIMAL BANDWIDTH CHOICE
In this section, we discuss the objective function and derive the optimal bandwidth h opt under
that criterion.

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

τSRD = E[Yi (1) − Yi (0)|X i = c].

936

REVIEW OF ECONOMIC STUDIES

3.1. Error criteria

x

where f (x) is the density of the forcing variable. This criterion is not directly relevant for the
problem at hand: we wish to choose a bandwidth that is optimal for estimating τSRD . This estimand has two special features that are not captured in the MISE criterion. First, τSRD depends
on m(x) only through two values and specifically their difference. Second, both these values are
boundary values.
Our proposed criterion is based on the expectation of the asymptotic expansion, around h = 0,
of the squared error (τ̂SRD − τSRD )2 . First, define the mean squared error:
MSE(h) = E[(τ̂SRD − τSRD )2 ] = E[((μ̂+ − μ+ ) − (μ̂− − μ− ))2 ],

(2)

and let h ∗ be the optimal bandwidth that minimizes this criterion:
h ∗ = arg min MSE(h).
h

(3)

This criterion is difficult to work with directly. The problem is that in many cases even as the
sample sizes become infinite, the optimal bandwidth h ∗ will not converge to zero. This is because
biases in different parts of the regression function away from the threshold may be offsetting.2
In such cases, the optimal bandwidth h ∗ can be very sensitive to the actual distribution and
regression function. Moreover, it does not seem appropriate to base estimation on global criteria
when identification is local. We therefore follow the standard bandwidth choice literature in
statistics by focusing on the bandwidth that minimizes a first-order approximation to MSE(h),
what we call the asymptotic mean squared error or AMSE(h).
A second comment concerns our focus on a single bandwidth. Because the estimand, τSRD ,
is a function of the regression function at two points, an alternative would be to allow for a
different bandwidth for these two points, h − for estimating μ− , and h + for estimating μ+ and
focus on an objective function that is an approximation to
MSE(h − , h + ) = E[((μ̂+ (h + ) − μ+ ) − (μ̂− (h − ) − μ− ))2 ],

(4)

instead of focusing on an approximation to MSE(h). Doing so would raises an important issue.
We focus on minimizing mean squared error, equal to variance plus bias squared. Suppose that
for both estimators, the biases, E[μ̂− (h − )] − μ− and E[μ̂+ (h + )] − μ+ , are strictly increasing
2. To be explicit, consider a simple example where we are interested in estimating a regression function g(x)
at a single point, say g(0). Suppose the covariate X has a uniform distribution on [0, 1]. Suppose the regression
2
function
is g(x)
h, equal to
P = (x − 1/4) − 1/16. With a uniform kernel, the estimator for2g(0) is, for a bandwidth P
P
X
/
1.
As
a function of the bandwidth h, the bias is equal to h /3 − h/4, conditional on i:X i <h 1.
i
i:X i <h
i:X i <h
Thus, the bias is zero at h = 3/4, and if we minimize the expected squared error, the optimal bandwidth will converge
to 3/4 as the sample size gets large.

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

The primary question studied in this paper concerns the optimal choice of the bandwidth h. In the
current empirical literature, researchers often choose the bandwidth by either cross-validation or
ad hoc methods. See Härdle (1992), Fan and Gijbels (1992), and Wand and Jones (1994) for
textbook discussions of cross-validation and related methods, and Ludwig and Miller (2007) for
a specific implementation in the RD settings. Conventional cross-validation yields a bandwidth
that is optimal for fitting a curve over the entire support of the data. Typically, it leads to a
bandwidth choice that minimizes an approximation to the mean integrated squared error criterion
(MISE),
Z

2
MISE(h) = E
(m̂ h (x) − m(x)) f (x)d x ,

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

937

3.2. An asymptotic expansion of the expected error
The next step is to derive an asymptotic expansion of MSE(h) given equation (2) and formally
define the asymptotic approximation AMSE(h). First, we state the key assumptions. Not all
these will be used immediately, but for convenience, we state them all here.
Assumption 3.1.

(Yi , X i ), for i = 1, . . . , N , are independent and identically distributed.

Assumption 3.2. The marginal distribution of the forcing variable X i , denoted f (∙), is continuous and bounded away from zero at the threshold c.
Assumption 3.3. The conditional mean m(x) = E[Yi |X i = x] has at least three continuous
derivatives in an open neighbourhood of X = c. The right and left limits of the kth derivative of
(k)
(k)
m(x) at the threshold c are denoted by m + (c) and m − (c).
Assumption 3.4. The kernel K (∙) is non-negative, bounded, differs from zero on a compact
interval [0, a], and is continuous on (0, a).
Assumption 3.5. The conditional variance function σ 2 (x) = Var(Yi |X i = x) is bounded in an
open neighbourhood of X = c and right and left continuous at c. The right and left limit at the
threshold are denoted by σ+2 (c) and σ−2 (c), respectively, σ+2 (c) > 0 and σ−2 (c) > 0.
Assumption 3.6. The second derivatives from the right and the left differ at the threshold:
(2)
(2)
m + (c) 6= m − (c).
Now define the AMSE as a function of the bandwidth h:
AMSE(h) = C1 ∙ h

4

(2)
(2)
∙ (m + (c) − m − (c))2 +

!
σ+2 (c) σ−2 (c)
C2
∙
+
.
N ∙h
f (c)
f (c)

The constants C1 and C2 in this approximation are functions of the kernel:
!2
ν22 π0 − 2ν1 ν2 π1 + ν12 π2
1 ν22 − ν1 ν3
C1 =
and
C
=
,
2
4 ν2 ν0 − ν12
(ν2 ν0 − ν12 )2
where
νj =

Z

∞
0

j

u K (u)du

and π j =

Z

∞

(5)

(6)

u j K 2 (u)du.

0

The first term in equation (5) corresponds to the square of the bias and the second term corresponds to the variance. The expression for AMSE(h) clarifies the role that Assumption 3.6 will
play. The leading term in the expansion of the bias is of order h 4 if the left and right limits of the
second derivative differ. If these two limits are equal, the bias converges to zero faster, allowing for estimation of τSRD at a faster rate of convergence. It is difficult to exploit the improved

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

(or both strictly decreasing) functions of the bandwidth. Then, there is a function h + (h − ) such
that the bias of the RD estimate, that is the difference between the above biases cancel out:
(E[μ̂− (h − )] − μ− ) − (E[μ̂+ (h + (h − )) − μ+ ]) = 0. Hence, we can minimize the mean squared
error by letting h − get large (the variance is generally a decreasing function of the bandwidth)
and choosing h + = h + (h − ). Even if this does not hold exactly, the point is that a problem may
arise that even for large bandwidths, the difference in bias may be close to zero. In practice, it
is unlikely that one can effectively exploit the cancellation of biases for large bandwidths. This
would make it difficult to construct practical bandwidth algorithms. Therefore, in order to avoid
this problem, we focus in this discussion on a single bandwidth choice.

938

REVIEW OF ECONOMIC STUDIES

Lemma 3.1 (Mean Squared Error Approximation and Optimal Bandwidth).
(i) Suppose Assumptions 3.1–3.5 hold. Then,


1
4
.
MSE(h) = AMSE(h) + o p h +
N ∙h
(ii) Suppose that also Assumption 3.6 holds. Then,
h opt = arg min AMSE(h) = C K ∙
h

σ+2 (c) + σ−2 (c)
(2)

(2)

f (c) ∙ (m + (c) − m − (c))2

!1/5

∙ N −1/5 ,

(7)

where C K = (C2 /(4 ∙ C1 ))1/5 , indexed by the kernel K (∙).
For the edge kernel, with K (u) = 1|u|≤1 (1 − |u|), shown by Cheng, Fan and Marron (1997) to
have optimality properties for boundary estimation problems, the constant is C K ,edge ≈ 3∙4375.
For another commonly used kernel, the uniform kernel with K (u) = 1|u|≤1/2 , the constant is
approximately C K ,uniform ≈ 5∙40.
4. FEASIBLE OPTIMAL BANDWIDTH CHOICE
In this section, we develop an estimator for the bandwidth and discuss its asymptotic properties.
The proposed bandwidth estimator is fully data driven and based on substituting consistent estimators for the various components of the optimal bandwidth given in equation (7). It involves a
number of choices for initial smoothing parameters in order to estimate these components. As is
typically the case with plug-in estimators, these choices are not unique and can be replaced by
others without affecting the asymptotic optimality of the procedure.
4.1. A simple plug-in bandwidth
A natural choice for the estimator for the optimal bandwidth estimator is to replace the six
unknown quantities in the expression for the optimal bandwidth h opt in equation (7) by consistent
estimators, leading to
h̃ opt = C K ∙

σ̂−2 (c) + σ̂+2 (c)

(2)
2
fˆ(c)∙(m̂ (2)
+ (c) − m̂ − (c))

!1/5

∙ N −1/5 .

(8)

One potential concern here, however, is that the first-order bias may be extremely small or vanish in finite samples. This could happen for instance in the constant additive treatment effects

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

convergence rate that would result from this, in practice, because it would be difficult to establish sufficiently fast that two second derivatives are indeed equal, and therefore, we focus on
optimality results given Assumption 3.6. Note, however, that even if the second derivatives are
identical, our proposed estimator for τSRD will be consistent.
An alternative approach would be to focus on a bandwidth choice that is optimal if the second
derivatives from the left and right are identical. It is possible to construct such a bandwidth choice
and still maintain consistency of the resulting estimator for τSRD irrespective of the difference
in second derivatives. However, such an bandwidth choice would generally not be optimal if
the difference in second derivatives is non-zero. Thus, there is a choice between a bandwidth
(2)
choice that is optimal under m (2)
+ (c) 6= m − (c) and a bandwidth choice that is optimal under
(2)
(2)
m + (c) = m − (c). In the current paper, we choose to focus on the first case.

IMBENS & KALYANARAMAN
(2)

OPTIMAL BANDWIDTH CHOICE

939

(2)

4.1.1. Regularization. Motivated by the above concern that due to the error in the estimation of the true curvature, the error in the estimation of its squared reciprocal could potentially
be large, leading to very large and ill-performing bandwidths, we modify the bandwidth estimator using ideas from the regularization literature.5 A simple calculation establishes that the bias
in the plug-in estimator for the reciprocal of the squared difference in second derivatives is
"
#
1
1
E
− (2)
(2)
(2)
(2)
(m̂ + (c) − m̂ − (c))2 (m + (c) − m − (c))2
!
(2)
(2)
3∙(V(m̂ + (c)) + V(m̂ − (c)))
+ o(N −2α ).
=
(2)
(2)
4
(m + (c) − m − (c))
(2)
(2)
This implies that, for r = 3∙(V(m̂ − (c)) + V(m̂ + (c))), the bias in the modified estimator for the
reciprocal of the squared difference in second derivatives is of lower order:
"
#
1
1
E
= o(N −2α ).
− (2)
(2)
(2)
(2)
2
2
(m̂ + (c) − m̂ − (c)) + r (m + (c) − m − (c))

This in turn motivates the modified bandwidth estimator
ĥ opt = C K ∙

σ̂−2 (c) + σ̂+2 (c)

(2)
(2)
fˆ(c)((m̂ + (c) − m̂ − (c))2 + r̂− + r̂+ )

!1/5

∙ N −1/5 ,

(9)

where
(2)

r̂− = 3∙V̂(m̂ − (c))

(2)

and r̂+ = 3∙V̂(m̂ + (c)).

Note that this bandwidth will not become infinite even in the cases when the difference in curvatures at the threshold is zero.
3. This problem is not unique to our specific estimator. In the general case of estimating a regression at an interior
point, this occurs when the second derivative at that point is zero
4. As an aside, the same formal argument applies to the estimator of the density. If the estimated density is close
to zero, the bandwidth estimator might become unstable. However, in practice that is rarely a concern: if the true density
is so close to zero that one cannot estimate the density accurately at the threshold, it is unlikely that any estimates of
the discontinuity will be precise enough to be of interest. We therefore focus on the complications arising from the
difference in second derivatives being estimated to be close to zero.
5. Kalyanaraman (2008) has developed some theory about regularization in bandwidth selection in the different
context of estimated smooth regression functionals.

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

(CATEs) case where m + (x) = m − (x) for any x. In this case, the bandwidth that minimizes
first-order mean squares is infinite (the denominator term is zero in equation (8))3 .
More generally, even if the true value of the bias term is not zero, the precision with which
(2)
(2)
we estimate the second derivatives m + (c) and m − (c) is likely to be low. Thus, the estimated
optimal bandwidth h̃ opt will occasionally be very large, even when the data are consistent with
a substantial degree of curvature. Thus, estimates of the bandwidth will be very imprecise and
will have a large variance across repeated data sets. Moreover, such a bandwidth may lead to
estimators for τSRD with poor properties because the true finite sample bias depends on global
properties of the regression function that are not captured by the asymptotic approximation used
to calculate the bandwidth.4

940

REVIEW OF ECONOMIC STUDIES

μ̂ j,h,+ =

1
Nh,+

X

c≤X i ≤c+h

(X i − X ) j ,

where X =

1
Nh,+

X

Xi ,

c≤X i ≤c+h

be the jth (centred) moment of the X i in the interval [c, c + h]. We can derive the following
explicit formula for three times the conditional variance of the curvature on the left, denoted by
r+ , in terms of these moments:
!
σ+2 (c)
12
∙
.
r+ =
Nh,+ μ̂4,h,+ − (μ̂2,h,+ )2 − (μ̂3,h,+ )2 /μ̂2,h,+
However, because fourth moments are difficult to estimate precisely, we approximate this expression exploiting the fact that for small h, the distribution of the forcing variable can be approximated by a uniform distribution on [c, c + h], so that μ2,h,+ ≈ h 2 /12, μ3,h,+ ≈ 0, and
μ4,h,+ ≈ h 4 /60. After substituting σ̂−2 (c) for σ−2 (c) and σ̂+2 (c) for σ+2 (c), this leads to
r̂+ =

2160∙σ̂+2 (c)
,
Nh,+ ∙h 4

and similarly r̂− =

2160∙σ̂−2 (c)
.
Nh,− ∙h 4

The proposed bandwidth is now obtained by adding the regularization term r̂ = r̂− + r̂+ to the
squared difference-in-curvature term in the bias term of MSE expansion:
ĥ opt = C K ∙

σ̂−2 (c) + σ̂+2 (c)

(2)
(2)
fˆ(c)((m̂ + (c) − m̂ − (c))2 + r̂− + r̂+ )

!1/5

∙N −1/5 .

(10)

To operationalize this proposed bandwidth, we need specific estimators fˆ(c), σ̂−2 (c), σ̂+2 (c),
(2)
m̂ (2)
− (c), and m̂ + (c). In the next section, we discuss a specific way of doing so, leading to
a completely data-driven bandwidth choice. This bandwidth estimator will be shown to have
certain optimality properties. It should be noted though that our proposed bandwidth estimator
is not unique in having these optimality properties. Any combination of consistent estimators
(2)
(2)
for f (c), σ−2 (c), σ+2 (c), m − (c), and m + (c) substituted into expression (10), with or without
the regularity terms, will have the same optimality properties. Within this class, our proposed
estimator is relatively simple, but the more important point is that it is a specific estimator, in the
same spirit as the Silverman rule-of-thumb bandwidth for non-parametric density estimation:
it gives a convenient starting point and benchmark for doing a sensitivity analyses regarding
bandwidth choice.
In addition, we will address the sensitivity of our bandwidth estimator to the choices made
in our algorithm in a simulation study. In general, we find the bandwidth selection algorithm to
be relatively robust to these choices. This is not surprising given that the presence of the power
1/5 in the expression for the optimal bandwidth: for example, doubling the estimates for both
σ−2 (c) and σ+2 (c) only increases the estimated bandwidth by a factor 21/5 ≈ 1∙18.

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

4.1.2. Implementing the regularization. Consider first a simplification of the regularization term r = r− +r+ , where r− and r+ are three times the variance of the estimated curvatures
(2)
on the left and the right, respectively. To be explicit, we estimate the second derivative m + (c)
by fitting a quadratic function to the observations with X i ∈ [c, c + h]. The initial bandwidth
h here will be different from the bandwidth ĥ opt used in the estimation of τSRD , and its choice
will be discussed in Section 4.2. Let Nh,+ be the number of units with covariate values in this
interval. We assume homoskedasticity with error variance σ 2 (c) in this interval. Let

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

941

4.2. An algorithm for bandwidth selection
(2)

2
2
Step 1. Estimation of density f (c) and conditional variances σP
− (c) and σ+ (c).
2
2
First, calculate the sample variance of the forcing variable, S X = (X i − X ) /(N − 1). We now
use the Silverman rule to get a pilot bandwidth for calculating the density and variance at c.
The standard Silverman rule of h = 1∙06∙S X ∙N −1/5 is based on a normal kernel and a normal
reference density. We modify this for the uniform kernel on [−1, 1] and the normal reference
density and calculate the pilot bandwidth h 1 as follows:

h 1 = 1∙84∙S X ∙N −1/5 .
We assess the sensitivity of the choice of a uniform kernel in the final simulations. We choose
the uniform kernel because we are interested in a simple estimate of density, that is proportion
of observations near the threshold (which is a kernel density estimate with a uniform kernel).
Using alternative kernels would not affect the optimality properties in Theorem 4.1.
Calculate the number of units on either side of the threshold, and the average outcomes on
either side as
N
N
X
X
Nh 1 ,− =
1c−h 1 ≤X i <c , Nh 1 ,+ =
1c≤X i ≤c+h 1 ,
i=1

Y h 1 ,− =

1

Nh 1 ,−

i=1

X

Yi ,

i:c−h 1 ≤X i <c

and

Y h 1 ,+ =

1

Nh 1 ,+

X

Yi .

i:c≤X i ≤c+h 1

Now estimate the density of X i at c as
Nh 1 ,− + Nh 1 ,+
fˆ(c) =
,
2∙N ∙h 1

(11)

and estimate the limit of the conditional variances of Yi given X i = x, at x = c, from the left and
the right, as
X
1
σ̂−2 (c) =
(Yi − Y h 1 ,− )2 ,
(12)
Nh 1 ,− − 1
i:c−h 1 ≤X i <c

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

The reference bandwidth ĥ opt is a function of estimates for f (c), σ−2 (c), σ+2 (c), m − (c), and
(2)
m + (c) and the kernel K (∙). Here, we give a specific algorithm for implementation. In practice, we recommend using the theoretically optimal edge kernel, where K (u) = 1|u|≤1 ∙(1 − |u|),
which also has consistently superior performance in simulations, although the algorithm is easily
modified for other kernels by changing the kernel-specific constant C K . To calculate the bandwidth, we also need estimators for the density at the threshold, f (c), the conditional variances
at the threshold, σ−2 (c) and σ+2 (c), and the limits of the second derivatives at the threshold from
(2)
(2)
the right and the left, m − (c), m + (c). (The other components of equation (10), r̂− and r̂+ , are
functions of these four components.) The first three functionals are calculated in Step 1, the last
two in Step 2. Step 3 puts these together with the appropriate kernel constant C K to produce the
reference bandwidth ĥ opt .
We make the following choices in this algorithm. First, an initial bandwidth h 1 and a kernel
to estimate the density f X (c) and the conditional outcome variances σ−2 (c) and σ+2 (c). Second,
(2)
(2)
a pair of bandwidths h 2,− and h 2,+ for estimating the second derivatives m − (c), m + (c). We
choose these two bandwidths h 2,− and h 2,+ optimally given the third derivative, which in turn we
estimate globally. The choices for h 1 , the initial kernel, and the estimator for the third derivative
do not affect the asymptotic optimality properties of the bandwidth estimator, but they do affect
the finite sample properties.

942

REVIEW OF ECONOMIC STUDIES

and
σ̂+2 (c) =

X

1
Nh 1 ,+ − 1

i:c≤X i ≤c+h 1

(Yi − Y h 1 ,+ )2 .

(13)

(2)

(2)

Step 2. Estimation of second derivatives m̂ + (c) and m̂ − (c).
First, we need pilot bandwidths h 2,− and h 2,+ . We base this on a simple, not necessarily consistent, estimator of the third derivative of m(∙) at c. Fit a third-order polynomial to the data,
including an indicator for X i ≥ 0. Thus, estimate the regression function
Yi = γ0 + γ1 ∙1 X i ≥c + γ2 ∙(X i − c) + γ3 ∙(X i − c)2 + γ4 ∙(X i − c)3 + εi ,
m (3) (c)

(14)

m̂ (3) (c)

as
= 6∙γ̂4 . This will be our estimate of the third derivative of the
and estimate
regression function. Note that m̂ (3) (c) is in general not a consistent estimate of m (3) (c) but will
converge to some constant at a parametric rate.
However, we do not need a consistent estimate of the third derivative at c here to obtain a
consistent estimator for the second derivative. Calculate h 2,+ , using the σ̂−2 (c), σ̂+2 (c) and fˆ(c)
from Step 1, as
!1/7
σ̂+2 (c)
−1/7
h 2,+ = 3∙56∙
∙ N+
(15)
fˆ(c)∙(m̂ (3) (c))2

and

h 2,− = 3∙56∙

σ̂−2 (c)

fˆ(c)∙(m̂ (3) (c))2

!1/7

−1/7

∙ N−

,

where N− and N+ are the number of observations to the left and right of the threshold, respectively. These bandwidths, h 2,− and h 2,+ , are estimates of the optimal bandwidth for calculation
of the second derivative at a boundary point using a local quadratic and a uniform kernel. See the
Appendix for details. Again alternative consistent estimators for these second derivatives would
also lead to optimality for the corresponding bandwidth estimator ĥ opt .
(2)
Given the pilot bandwidth h 2,+ , we estimate the curvature m + (c) by a local quadratic fit. To
be precise, temporarily discard the observations other than the N2,+ observations with c ≤ X i ≤
c + h 2,+ . Label the new data Ŷ+ = (Y1 , . . . , Y N2,+ ) and X̂+ = (X 1 , . . . , X N2,+ ) each of length
N2,+ . Fit a quadratic to the new data. That is, let T = [ι T1 T2 ], where ι is a column vector of
ones, and T0j = ((X 1 − c) j , . . . , (X N2,+ − c) j ), for j = 1, 2. Estimate the regression coefficients
(2)

λ̂ = (T0 T)−1 T0 Ŷ. Calculate the curvature as m̂ + (c) = 2∙λ̂3 . This is a consistent estimate of
(2)
(2)
m + (c). To estimate m − (c), follow the same procedure using the data with c − h 2,− ≤ X i < c.
Step 3. Calculation of regularization terms r̂− and r̂+ and calculation of ĥ opt .
Given the previous steps, the regularization terms are calculated as follows:
r̂+ =

2160∙σ̂+2 (c)
N2,+ ∙h 42,+

and r̂− =

2160∙σ̂−2 (c)
N2,− ∙h 42,−

.

(16)

We now have all the pieces to calculate the proposed bandwidth:
ĥ opt = C K ∙

σ̂−2 (c) + σ̂+2 (c)

(2)
(2)
fˆ(c)∙((m̂ + (c) − m̂ − (c))2 + (r̂+ + r̂− ))

!1/5

∙ N −1/5 ,

(17)

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

The main property we will need for these estimators is that they are consistent for the density and
the conditional variance, respectively. They need not be efficient for the optimality properties in
Theorem 4.1. Because the bandwidth goes to zero at rate N −1/5 , Assumptions 3.2 and 3.5 are
sufficient for consistency of these estimators.

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

943

where C K is, as in Lemma 3.1, a constant that depends on the kernel used. For the edge kernel,
with K (u) = (1 − |u|)∙1|u|≤1 , the constant is C K ≈ 3∙4375.
Given the bandwidth ĥ opt , we estimate τSRD as follows:
τ̂SRD = lim m̂ ĥ opt (x) − lim m̂ ĥ opt (x),
x↓c

x↑c

4.3. Properties of algorithm
For the bandwidth choice based on this algorithm, we establish some asymptotic properties.
First, the resulting RD estimator τ̂SRD is consistent at the best rate for non-parametric regression
functions at a point (Stone, 1982). Second, the estimated constant term in the reference bandwidth converges to the best constant. Third, we have a Li (1987) type optimality result for the
mean squared error and consistency at the optimal rate for the RD estimate. The optimality result
implies that asymptotically the procedure with the estimated bandwidth ĥ opt performs as well as
the infeasible procedure with the optimal bandwidth h opt .
Theorem 4.1 (Properties of ĥopt ).
Suppose Assumptions 3.1–3.5 hold. Then:
(i) (consistency) if Assumption 3.6 holds, then
τ̂SRD − τSRD = O p (N −2/5 ).

(18)

(ii) (consistency) if Assumption 3.6 does not hold, then
τ̂SRD − τSRD = O p (N −3/7 ).

(19)

ĥ opt − h opt
= o p (1),
h opt

(20)

MSE(ĥ opt ) − MSE(h opt )
= o p (1).
MSE(h opt )

(21)

(iii) (convergence of bandwidth)

and (iv) (Li’s optimality):

Note that when Assumption 3.6 holds, the convergence rate (N −2/5 ) for τ̂SRD is slower than
when Assumption 3.6 does not hold (namely N −3/7 ). This is because failure of Assumption
(3.6) implies that the second derivatives from the left and right are equal, implying in turn that
the leading term of the bias vanishes, which, as one might expect, would improve convergence.
4.4. DesJardins–McCall bandwidth selection
DesJardins and McCall (2008) use an alternative method for choosing the bandwidth. They
focus separately on the limits of the regression function to the left and the right rather than on
the difference in the limits. This implies a focus on minimizing an objective criterion based on
the sum of the squared differences between μ̂− and μ− and between μ̂+ and μ+ :
E[(μ̂+ − μ+ )2 + (μ̂− − μ− )2 ],

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

where m̂ h (x) is the local linear regression estimator defined in equation (1).

944

REVIEW OF ECONOMIC STUDIES

instead of our criterion, which focuses on the squared difference between (μ̂+ − μ̂− ) and
(μ+ − μ− ),
E[((μ̂+ − μ+ ) − (μ̂− − μ− ))2 ] = E[(τ̂SRD − τSRD )2 ].

This will in large samples lead to a smaller bandwidth than our proposed bandwidth choice if the
second derivatives are of the same sign. DesJardins and McCall actually use different bandwidths
on the left and the right and also use a Epanechnikov kernel instead of the optimal edge kernel.
In the simulations and bandwidth comparisons below, we use the better performing edge kernel
to facilitate the comparison with our proposed bandwidth ĥ opt .
4.5. Ludwig–Miller cross-validation
In this section, we briefly describe the cross-validation method used by Ludwig and Miller (2005,
LM from hereon), which we compare to our proposed bandwidth in the application and simulations. The LM bandwidth is the only cross-validation bandwidth selection procedure in the
literature that is specifically aimed at the RD setting. Let N− and N+ be the number of observations with X i < c and X i ≥ c, respectively. For δ ∈ (0, 1), let θ− (δ) and θ+ (δ) be the δth quantile
of the X i among the subsample of observations with X i < c and X i ≥ c, respectively, so that
(
!
)
N
X
θ− (δ) = arg min a
1 X i ≤a ≥ δ∙N−
a

and

i=1

(

θ+ (δ) = arg min a
a

N
X
i=1

1c≤X i ≤a

!

)

≥ δ∙N+ .

Now the LM cross-validation criterion we use is of the form
C Vδ (h) =

N
X
i=1

1θ− (1−δ)≤X i ≤θ+ (δ) ∙(Yi − m̂ h (X i ))2 .

(In fact, LM use a slightly different criterion function, where they sum up over all observations within a distance h 0 from the threshold.) The estimator for the regression function here is
m̂ h (x) defined in equation (1). A key feature of m̂ h (x) is that for values of x < c, it only uses
observations with X i < x to estimate m(x) and for values of x ≥ c, it only uses observations
with X i > x to estimate m(x), so that m̂ h (X i ) does not depend on Yi , as is necessary for crossvalidation. By using a value for δ close to zero, we only use observations close to the threshold
to evaluate the cross-validation criterion. Apart from the choice on needs to make of δ, the concern is that by using too small value of δ, we may not get a precisely estimated cross-validation
bandwidth. In a minor modification of the LM proposal, we use the edge kernel instead of the
Epanechnikov kernel they suggest. In our calculations, we use δ = 0∙5.
Any fixed value for δ is unlikely to lead to an optimal bandwidth in general, as it is implicitly based on a criterion function that is appropriate for fitting the entire regression function
between the (1 − δ)-quantile for the observations on the left and the δ-quantile for observations
on the right. Moreover, the criterion focuses implicitly on minimizing a criterion more akin to

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

The single optimal bandwidth based on the DesJardins and McCall criterion is
!1/5
σ+2 (c) + σ−2 (c)
h DM = C K ∙
∙ N −1/5 .
(2)
(2)
f (c)∙(m + (c)2 + m − (c)2 )

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

945

E[(μ̂+ − μ+ )2 + (μ̂− − μ− )2 ] (with the errors in estimating μ− and μ+ squared before adding
them up), rather than MSE(h) = E[((μ̂+ − μ+ ) − (μ̂− − μ− ))2 ] where the error in the difference
μ+ − μ− is squared. As a result, even letting δ → 0 with the sample size in the cross-validation
procedure will not result in an optimal bandwidth.
5. EXTENSIONS

5.1. The fuzzy regression design
In the FRD design , the treatment Wi is not a deterministic function of the forcing variable.
Instead, the probability Pr(Wi = 1|X i = x) changes discontinuously at the threshold c. The
focus is on the ratio
τFRD =

limx↓c E[Yi |X i = x] − limx↑c E[Yi |X i = x]
.
limx↓c E[Wi |X i = x] − limx↑c E[Wi |X i = x]

In an important theoretical paper, Hahn, Todd and Van Der Klaauw (2001) discuss identification
in this setting and show that in settings with heterogenous effects, the estimand has an interpretation as a local average treatment effect (Imbens and Angrist, 1994). In the FRD case, we need
to estimate two regression functions, each at two boundary points: the expected outcome given
the forcing variable E[Yi |X i = x] to the right and left of the threshold c and the expected value
of the treatment variable given the forcing variable E[Wi |X i = x] again both to the right and
left of c. Again, we focus on a single bandwidth, now the bandwidth that minimizes the mean
squared error to this ratio. Define
τY = lim E[Yi |X i = x] − lim E[Yi |X i = x] and
x↓c

x↑c

τW = lim E[Wi |X i = x] − lim E[Wi |X i = x],
x↓c

x↑c

with τ̂Y and τ̂W denoting the corresponding estimators, so that τFRD = τY /τW and τ̂FRD =
τ̂Y /τ̂W . In large samples, we can approximate the difference τ̂FRD − τFRD by
1
τY
(τ̂Y − τY ) − 2 (τ̂W − τW ) + o p ((τ̂Y − τY ) + (τ̂W − τW )).
τW
τW
This is the basis for the asymptotic approximation to the MSE around h = 0:
τ̂FRD − τFRD =

AMSEFRD (h) = C1 h

4

1
τY
(2)
(2)
(2)
(2)
(m Y,+ (c) − m Y,− (c)) − 2 (m W,+ (c) − m W,− (c))
τW
τW

+

C2
N h f (c)

−

2τY
3
τW

!2

(22)

τ2 2
1
2
2
2
(σY,+
(c) + σY,−
(c)) + Y4 (σW,+
(c) + σW,−
(c))
2
τW
τW
!

(σY W,+ (c) + σY W,− (c)) .

In this expression, the constants C1 and C2 are the same as before in equation (6). The second
(2)
(2)
(2)
(2)
derivatives of the regression functions, m Y,− (c), m Y,+ (c), m W,− (c), and m W,+ (c), are now defined separately for the treatment W and the outcome Y . In addition, the conditional variances

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

In this section, we discuss two extensions. First, we consider the FRD design and second, we
allow for the presence of covariates.

946

REVIEW OF ECONOMIC STUDIES

are indexed by the treatment and outcome. Finally, the AMSE also depends on the right and left
limit of the covariance of W and Y conditional on the forcing variable, at the threshold, denoted
by σY W,+ (c) and σY W,− (c), respectively.
The bandwidth that minimizes the AMSE in the fuzzy design is
h opt,FRD = C K ∙N −1/5
2 (c) + σ 2 (c)) + τ 2 (σ 2 (c) + σ 2 (c)) − 2τ
(σY,+
FRD (σY W,+ (c) + σY W,− (c))
FRD W,+
Y,−
W,−
(2)

(2)

(2)

(2)

f (c)∙((m Y,+ (c) − m Y,− (c)) − τFRD (m W,+ (c) − m W,− (c)))2

.

The analogue of the bandwidth proposed for the SRD is
ĥ opt,FRD = C K ∙N −1/5
×

(24)

2 (c) + σ̂ 2 (c)) + τ̂ 2 (σ̂ 2 (c) + σ̂ 2 (c)) − 2τ̂
(σ̂Y,+
FRD (σ̂Y W,+ (c) + σ̂Y W,− (c))
FRD W,+
Y,−
W,−

(2)
(2)
(2)
(2)
fˆ(c)∙(((m̂ Y,+ (c) − m̂ Y,− (c)) − τ̂FRD (m̂ W,+ (c) − m̂ W,− (c)))2 + r̂Y,+ + r̂Y,− + τ̂FRD (r̂ W,+ + r̂ W,− ))

!1/5

.

We can implement this as follows. First, using the algorithm described for the SRD case sepa2 , σ̂ 2 , σ̂ 2 , σ̂ 2 ,
rately for the treatment indicator and the outcome, calculate τ̂FRD , fˆ(c), σ̂Y,+
Y,− W,+ W,−
(2)
(2)
(2)
(2)
m̂ Y,+ (c), m̂ Y,− (c), m̂ W,+ (c), m̂ W,− (c), r̂Y,+ , r̂Y,− , r̂ W,+ , and r̂ W,− . Second, using the initial Silverman bandwidth use the deviations from the means to estimate the conditional covariances
σ̂Y W,+ (c) and σ̂Y W,− (c). Then, substitute everything into the expression for the bandwidth. By
the same argument as for the SRD case, the resulting bandwidth has the asymptotic no-regret
property.
In practice, this often leads to bandwidth choices similar to those based on the optimal bandwidth for estimation of only the numerator of the RD estimand. One may therefore simply wish
to use the basic algorithm ignoring the fact that the regression discontinuity design is fuzzy.
5.2. Additional covariates
Typically, the presence of additional covariates does not affect the RD analyses very much. In
most cases, the distribution of the additional covariates does not exhibit any discontinuity around
the threshold for the forcing variable, and as a result, those covariates are approximately independent of the treatment indicator for samples constructed to be close to the threshold. In that
case, the covariates only affect the precision of the estimator, and one can modify the previous
analysis using the conditional variance of Yi given all covariates at the threshold, σ−2 (c|x) and
σ+2 (c|x) instead of the variances σ−2 (c) and σ+2 (c) that condition only on the forcing variable.
In practice, this modification does not affect the optimal bandwidth much unless the additional
covariates have great explanatory power (recall that the variance enters to the power 1/5), and
the basic algorithm is likely to perform adequately even in the presence of covariates. For example, if the conditional variances are half the size of the unconditional ones, using the basic
algorithm with unconditional variances will mean that the bandwidth will be off only by a factor
(1 − 1/21/5 ) or approximately 0∙17.
6. AN ILLUSTRATION AND SOME SIMULATIONS
6.1. Data
To illustrate the implementation of these methods, we use a data set previously analysed by Lee
(2008) in a recent influential application of RD designs. Lee studies the incumbency advantage

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

×

(23)
!1/5

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

947

in elections. His identification strategy is based on the discontinuity generated by the rule that
the party with a majority vote share wins. The forcing variable X i is the difference in vote share
between the Democratic and Republican parties in one election, with the threshold c = 0. The
outcome variable Yi is vote share at the second election. There are 6558 observations (districts)
in this data set, 3818 with X i > 0, and 2740 with X i < 0. The average difference in voting
percentages at the last election for the Democrats was 0∙13, with a standard deviation of 0∙46.
Figure 1 plots the density of the forcing variable, in bins with width 0∙05. Figure 2 plots the
average value of the outcome variable, in 40 bins with width 0∙05, against the forcing variable.
The discontinuity is clearly visible in the raw data, lending credibility to any positive estimate
of the incumbency effect. The vertical line indicate the optimal bandwidth calculated below.
6.2. Imbens-Kalyanaraman (IK) algorithm on Lee data
In this section, we implement our proposed bandwidth on the Lee data set. For expositional
reasons, we gave all the intermediate steps.
Step 1. Estimation of density f (0) and conditional variance σ 2 (0).
We start with the modified Silverman bandwidth,
h 1 = 1∙84 ∙ S X ∙ N −1/5 = 1∙84 ∙ 0∙4553 ∙ 6558−1/5 = 0∙1445.

F IGURE 2
Regression function for democratic vote share

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

F IGURE 1
Density for forcing variable

948

REVIEW OF ECONOMIC STUDIES

There are Nh 1 ,− = 836 units with values for X i in the interval [−h 1 , 0), with an average outcome
2
of Y h 1 ,− = 0∙4219 and a sample variance of SY,h
= 0∙10472 , and Nh 1 ,+ = 862 units with
1 ,−
values for X i in the interval [0, h 1 ], with an average outcome of Y h 1 ,+ = 0∙5643 and a sample
2
variance of SY,h
= 0∙12022 . This leads to
1 ,+

and
2
σ̂−2 (0) = SY,h
= 0∙10472
1 ,−

2
and σ̂+2 (0) = SY,h
= 0∙12022 .
1 ,+
(2)

(2)

Step 2. Estimation of second derivatives m̂ + (0) and m̂ − (0).
To estimate the curvature at the threshold, we first need to choose bandwidths h 2,+ and h 2,− .
We choose these bandwidths based on an estimate of m̂ (3) (0) obtained by fitting a global cubic
with a jump at the threshold:
Yi = γ0 + γ1 ∙1 X i ≥c + γ2 ∙(X i − c) + γ3 ∙(X i − c)2 + γ4 ∙(X i − c)3 + εi .
The least squares estimate for γ4 is γ̂4 = −0∙1686, and thus, the third derivative at the threshold
is estimated as m̂ (3) (0) = 6∙γ̂4 = −1∙0119. This leads to the two bandwidths
σ̂+2 (0)

h 2,+ = 3∙56 ×

fˆ(0) × (m̂ (3) (0))2

!1/7

−1/7

× N+

= 0∙6057 and

h 2,− = 0∙6105.

The two pilot bandwidths are used to fit two quadratics. The quadratic to the right of 0 is fitted
(2)
on [0, 0∙6057], yielding m̂ + (0) = 0∙0455 and the quadratic to the left is fitted on [−0∙6105, 0]
(2)
yielding m̂ − (0) = −0∙8471.
Step 3. Calculation of regularization terms r̂− and r̂+ and calculation of ĥ opt .
Next, the regularization terms are calculated. We obtain
r̂+ =

2160 × σ̂+2 (0)
N2,+ × h 42,+

=

2160 × 0∙12022
= 0.0825
2814 × 0∙60574

and r̂− =

2160 × σ̂−2 (0)
N2,− × h 42,−

= 0∙0675.

Now we have all the ingredients to calculate the optimal bandwidth under different kernels and
the corresponding RD estimates. Using the edge kernel with C K = 3∙4375, we obtain
ĥ opt = C K

σ̂−2 (0) + σ̂+2 (0)

(2)
(2)
fˆ(0) ∙ ((m̂ +
(0) − m̂ − (0))2 + (r̂+ + r̂− ))

!1/5

N −1/5 = 0∙2939.

6.3. Thirteen estimates for the Lee data
Here, we calculate 13 estimates of the ultimate object of interest, the size of the discontinuity
in m(x) at zero. The first eight are based on local linear regression and the last five on global
polynomial regressions. The first is based on our proposed bandwidth. The second drops the regularization terms. The third uses a normal kernel and the corresponding Silverman bandwidth
for estimating the density function at the threshold (h 1 = 1∙06 ∙ Sx ∙ N −1/5 ). The fourth estimates
separate cubic regressions on the left and the right of the threshold to derive the bandwidth for
estimating the second derivatives. The fifth estimates the conditional variance at the threshold
assuming its left and right limit are identical. The sixth uses a uniform kernel on [−1/2, 1/2]

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

836 + 862
Nh 1 ,− + Nh 1 ,+
=
= 0∙8962
fˆ(0) =
2∙N ∙h 1
2 × 6558 × 0∙1445

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

949

TABLE 1
RD estimates and bandwidths for Lee data
Procedure

Linear
Quadratic
Cubic
Quartic
Quintic

τ̂SRD

(Standard error)

0∙2939
0∙3042
0∙2938
0∙2546
0∙2940
0∙4617
0∙3105
0∙9750

0∙0799
0∙0802
0∙0799
0∙0774
0∙0799
0∙0806
0∙0804
0∙0788

0∙0083
0∙0082
0∙0083
0∙0089
0∙0083
0∙0087
0∙0081
0∙0056

Global
Global
Global
Global
Global

0∙1182
0∙0519
0∙1115
0∙0766
0∙0433

0∙0056
0∙0071
0∙0093
0∙0113
0∙0132

instead of the optimal edge kernel. The seventh bandwidth is based on the DesJardin–McCall criterion, where we modify the procedure to use the edge kernel instead of the Epanechikov kernel
that DesJardin–McCall use. The eighth bandwidth is based on the LM cross-validation criterion.
The last five estimates for τSRD are based on global linear, quadratic, cubic, quartic, and quintic
regressions. The point estimates and robust standard errors are presented in Table 1. To investigate the overall sensitivity of the point estimates to the bandwidth choice, Figure 3 plots the RD
estimates τ̂SRD (h), and the associated 95% confidence intervals, as a function of the bandwidth,
for h between 0 and 1. The solid vertical line indicates the optimal bandwidth (ĥ opt = 0∙2939).
6.4. A small simulation study
Next, we conduct a small Monte Carlo study to assess the properties of the proposed bandwidth
selection rule in practice. We consider four designs, the first based on the Lee data, the second
on a very simple low-order polynomial, and the third and fourth on a case of constant average
treatment effect.

F IGURE 3
RD estimates and confidence intervals by bandwidth

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

ĥ opt
No regularization
f (c) estimated using normal kernel
Third-order polynomial separate on left and right
Homoskedastic variance
Uniform kernel
DesJardin–McCall
LM cross-validation (δ = 0∙5)

h

950

REVIEW OF ECONOMIC STUDIES

The error variance is σε2 = 0∙12952 . We use data sets of size 500 (smaller than the Lee data set
with 6558 observations, but more in line with common sample sizes).
In the second design, we use the same distribution for the forcing variable as in the first
design. We again have 500 observations per sample, and the true regression function is quadratic
both to the left and to the right of the threshold, but with different coefficients:
( 2
3x
if x < 0,
m quad (x) =
4x 2 if x ≥ 0,
implying the data-generating process is close to the point where the bandwidth h opt is fairly large
(because the left and right limit of the second derivative are 6 and 8, respectively), and one may
expect some effect from the regularization. The error variance is the same as in the first design,
σε2 = 0∙12952 .
Under the third design, we have a constant average treatment effect, and consequently,
the second derivatives on both sides of the threshold are equal. Here, one might expect the
DesJardins–McCall bandwidth to work particularly well because it assumes equality of the second derivatives. We base the design on the Lee data, where we use the following regressions,
where note that the regression for the treated group (right of the threshold) is an additive shift
(of 0∙1, approximately the discontinuity in the original sample) of the treatment effect regression
for the control (left of threshold). In other words, we test a scenario where the treatment effect
is constant across values of the forcing variable.
m CATE(1) (x) = 0∙42 + 0∙1 ∙ 1x≥0 + 0∙84x − 3∙00x 2 + 7∙99x 3 − 9∙01x 4 + 3∙56x 5 .
Our fourth design is a modification of the above. We look at the constant additive treatment
effect case where the curvature at the threshold is zero on both sides (for instance, in locally
linear regression functions). To do this, we simply use m CATE(1) (x), but set the coefficients on
the squared term to zero:
m CATE(2) (x) = 0∙42 + 0∙1 ∙ 1x≥0 + 0∙84x + 7∙99x 3 − 9∙01x 4 + 3∙56x 5 .
The other parameters for the data generating process are set as in the simulations based on
the Lee data. In the last two cases, one might expect substantial effects from regularization
because the infeasible optimal bandwidth in both cases is infinite. Moreover, in the last case, even
methods that are based on separately estimating left and right end points will need regularization.
In Tables 2 and 3, we report results for the same estimators as we reported in Table 1 for the
real data. We include one additional bandwidth choice, namely the infeasible optimal bandwidth
h opt , which can be derived because we know the data generating process. In Tables 2 and 3,
we present for both designs in each case the mean (Mean) and standard deviation (S.D.) of the
bandwidth choices and the bias (Bias) and the root mean squared error (RMSE) of the estimator
for τ .

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

In the first design, based on the Lee data, we use a Beta distribution for the forcing variable.
Let Z have a beta distribution with parameters α = 2 and β = 4, then the forcing variable is
X = 2∙Z − 1. The regression function is a fifth-order polynomial, with separate coefficients for
X i < 0 and X i > 0, with the coefficients estimated on the Lee data (after discarding observations
with past vote share differences greater than 0∙99 and less than −0∙99), leading to
(
0∙48 + 1∙27x + 7∙18x 2 + 20∙21x 3 + 21∙54x 4 + 7∙33x 5 if x < 0,
m Lee (x) =
0∙52 + 0∙84x − 3∙00x 2 + 7∙99x 3 − 9∙01x 4 + 3∙56x 5
if x ≥ 0.

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

951

TABLE 2
Simulations, 5000 replications
τ̂SRD

ĥ

Quadratic design
h opt (infeasible)
ĥ opt
No regularization
f (c) estimated using normal kernel
Third-order polynomial separate on left and right
Homoskedastic variance
Uniform kernel
DesJardins–McCall
LM cross-validation (δ = 0 ∙ 5)
Linear
Quadratic
Cubic
Quartic
Quintic

S.D.

0∙166
0∙480
0∙757
0∙480
0∙336
0∙478
0∙377
0∙556
0∙423

0∙058
0∙680
0∙058
0∙037
0∙058
0∙046
0∙134
0∙115
Global
Global
Global
Global
Global

0∙418
0∙422
0∙473
0∙422
0∙372
0∙421
0∙332
0∙223
0∙220

0∙070
0∙268
0∙070
0∙060
0∙070
0∙055
0∙010
0∙023
Global
Global
Global
Global
Global

Bias

RMSE

0∙017
0∙040
0∙037
0∙040
0∙038
0∙041
0∙034
0∙037
0∙037
0∙048
–0∙019
0∙087
0∙028
0∙001

0∙060
0∙054
0∙051
0∙054
0∙056
0∙054
0∙056
0∙051
0∙054
0∙055
0∙043
0∙100
0∙068
0∙074

0∙003
0∙006
0∙015
0∙006
0∙003
0∙006
–0∙041
–0∙002
–0∙002
0∙245
–0∙000
–0∙000
–0∙000
–0∙000

0∙037
0∙036
0∙045
0∙036
0∙040
0∙036
0∙067
0∙049
0∙050
0∙251
0∙037
0∙048
0∙060
0∙073

First, consider the design motivated by the Lee data. All feasible bandwidth selection methods combined with local linear estimation perform fairly similarly under this design as far as
τ̂SRD is concerned and close to the infeasible h opt . There is considerably more variation in the
performance of the global polynomial estimators. The quadratic estimator performs very well,
but adding a third-order term increases both bias and RMSE. The quintic approximation does
very well in terms of bias, not surprising given the regression that generated the data was a
fifth-order polynomial but has a higher RMSE than the local methods.
In the second design, the regularization matters, and the bandwidth choices based on different criterion functions perform worse than the proposed bandwidth in terms of RMSE, increasing it by about 35%. The global quadratic estimator obviously performs well here because
it corresponds to the data generating process, but it is interesting that the local linear estimator
based on ĥ opt has a RMSE very similar to that for the global quadratic estimator.
In the third and forth designs, as expected, regularization matters even more. Again the bandwidth choices based on different criterion functions perform worse. In particular, in the case
where the regression function has no curvature at the threshold, methods based on estimating

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

Lee design
h opt (infeasible)
ĥ opt
No regularization
f (c) estimated using normal kernel
Third-order polynomial separate on left and right
Homoskedastic variance
Uniform kernel
DesJardins–McCall
LM cross-validation (δ = 0 ∙ 5)
Linear
Quadratic
Cubic
Quartic
Quintic

Mean

952

REVIEW OF ECONOMIC STUDIES
TABLE 3
Simulations, 5000 replications
τ̂SRD

ĥ

CATE(2), zero curvature
h opt (infeasible)
ĥ opt
No regularization
f (c) estimated using normal kernel
Third-order polynomial separate on left and right
Homoskedastic variance
Uniform kernel
DesJardins–McCall
LM cross-validation (δ = 0 ∙ 5)
Linear
Quadratic
Cubic
Quartic
Quintic

Mean

S.D.

∞
0∙174
0∙257
0∙174
0∙164
0∙175
0∙137
0∙206
0∙113

0∙016
0∙206
0∙016
0∙013
0∙016
0∙013
0∙045
0∙013
Global
Global
Global
Global
Global

∞
0∙173
0∙252
0∙173
0∙163
0∙172
0∙135
0∙239
0∙120

0∙016
0∙184
0∙016
0∙013
0∙016
0∙013
0∙073
0∙011
Global
Global
Global
Global
Global

Bias

RMSE

–3∙758
–0∙008
–0∙067
–0∙008
–0∙007
–0∙009
0∙003
–0∙015
–0∙003
–3∙758
1∙367
–0∙207
0∙015
–0∙000

3∙767
0∙058
0∙303
0∙058
0∙059
0∙058
0∙069
0∙065
0∙073
3∙767
1∙373
0∙214
0∙062
0∙074

–3∙453
–0∙007
–0∙055
–0∙007
–0∙006
–0∙007
–0∙003
–0∙026
–0∙004
–3∙453
1∙365
–0∙209
0∙015
–0∙000

3∙462
0∙057
0∙260
0∙057
0∙058
0∙057
0∙068
0∙095
0∙069
3∙462
1∙371
0∙216
0∙061
0∙073

end points separately perform poorly (RMSE nearly the size of the RD estimate itself). This
is partly explained by the fact that in this case, these bandwidth choices would benefit from
regularization as well. Note that across all four simulations, the standard deviation of the estimated bandwidth with regularization is lower than that of the bandwidth without regularization,
sometimes by a factor 10. This is because regularization has the added benefit of reducing the
instability of the estimated bandwidth.

7. CONCLUSION
In this paper, we propose a fully data-driven, asymptotically optimal bandwidth choice for RD
settings. Although this choice has asymptotic optimality properties, it still relies on somewhat
arbitrary initial bandwidth choices. Rather than relying on a single bandwidth, we therefore
encourage researchers to use this bandwidth choice as a reference point for assessing sensitivity
to bandwidth choice in RD settings. The bandwidth selection procedures commonly used in
this literature are typically based on different objectives, for example on global measures, not

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

CATE(1), non-zero curvature
h opt (infeasible)
ĥ opt
No regularization
f (c) estimated using normal kernel
Third-order polynomial separate on left and right
Homoskedastic variance
Uniform kernel
DesJardins–McCall
LM cross-validation (δ = 0 ∙ 5)
Linear
Quadratic
Cubic
Quartic
Quintic

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

953

tailored to the specific features of the RD setting. We compare our proposed bandwidth selection
procedure to these and find that our proposed method works well in realistic settings, including
one motivated by data previously analysed by Lee (2008).
APPENDIX

Lemma A1 (MSE for Estimation of a Regression Function at the Boundary). Suppose (i) we have N pairs
(Yi , X i ), independent and identically distributed, with X i ≥ 0, (ii) m(x) = E[Yi |X i = x] is three times continuously
differentiable, (iii) the density of X i , f (x), is continuously differentiable at x = 0, with f (0) > 0, (iv) the conditional
variance σ 2 (x) = Var(YiR|X i = x) > 0 is bounded, and continuous at x = 0, (v) we have a kernel K : R+ 7→ R, with
K (u) = 0 for u ≥ u, and 0u K (u)du = 1, and define K h (u) = K (u/ h)/ h. Define μ = m(0), and
(μ̂h , β̂h ) = arg min
μ,β

N
X
i=1

(Yi − μ − β∙X i )2 ∙K h (X i ).

Then
1/2

E[μ̂|X 1 , . . . , X N ] − μ = C1 m (2) (0)h 2 + o p (h 2 ),
V(μ̂|X 1 , . . . , X N ) = C2

σ 2 (0)
+ op
f (c)N h





1
,
Nh

(A.1)
(A.2)

and
E[(μ̂ − μ)2 |X 1 , . . . , X N ] = C1 (m (2) (0))2 h 4 + C2



σ 2 (0)
1
+ o p h4 +
,
f (0)N h
Nh

(A.3)

where the kernel-specific constants C1 and C2 are those given in Lemma 31.
Before proving Lemma A1, we state and prove two preliminary results.
PN
j
Lemma A2. Define F j = N1 i=1
K h (X i )X i . Under the assumptions in Lemma A1|, (i) for non-negative integer j,
F j = h j f (0)ν j + o p (h j ) ≡ h j (F j∗ + o p (1)),

R
with ν j = 0∞ t j K (t)dt and F j∗ ≡ f (0)ν j and (ii) if j ≥ 1, F j = o p (h j−1 ).

Proof. F j is the average of independent and identically distributed random variables, so
F j = E[F j ] + O p (Var(F j )1/2 ).

The mean of F j is, using a change of variables from z to x = z/ h,
Z ∞
Z ∞
1 z j
z f (z)dz = h j
K (x)x j f (hx)d x
K
h
0 h
0
Z ∞
Z ∞
f (hx) − f (0)
dx
= hj
K (x)x j f (0)d x + h j+1
K (x)x j+1
hx
0
0

E[F j ] =

= h j f (0)ν j + O(h j+1 ).

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

To obtain the MSE expansions for the RD estimand, we first obtain the bias and variance estimates from estimating a regression function at a boundary point. Fan and Gijbels (1992) derive a version of Lemma A1 under different assumptions
(such as thin tailed rather than compact kernels) and hence, their proof is less transparent and not easily generalizable
to multiple dimensions and derivatives. The proof we outline is based on Ruppert and Wand (1994) but since they only
cursorily indicate the approach for a boundary point in multiple dimensions, we provide a simple proof for our case.

954

REVIEW OF ECONOMIC STUDIES

The variance of F j can be bounded by
1
1
2j
E[(K h (X i ))2 X i ] =
E
N
N h2

"  
#
Z ∞   
2
Xi
1
z 2 2j
2j
K
∙X i
=
K
∙z f (z)dz.
2
h
h
Nh 0

By a change of variables from z to x = z/ h, this is equal to

h 2 j−1
N

Hence,
k
Lemma A3.

!



hj
= o
h N 1/2

!2 

 = o((h j )2 ).

F j = E[F j ] + o p (h j ) = h j f (0)ν j + o p (h j ) = h j ∙( f (0)ν j + o p (1)).
PN
j
Let G j = N1 i=1
K h2 (X i )X i σ 2 (X i ). Under the assumptions from Lemma A1,
Z ∞
t j K 2 (t)dt.
G j = h j−1 σ 2 (0) f (0)π j (1 + o p (1)), with π j =
0

Proof. This claim is proved in a manner exactly like Lemma A1, here using in addition the continuity of the conditional
variance function. k
Proof of Lemma A1. Define R = [ιX ], where ι is a N -dimensional column of ones, define the diagonal weight matrix W
with (i, i)th element equal to K h (X i ) and define e1 = (1 0)0 . Then
m̂(0) = μ̂ = e10 (R 0 W R)−1 R 0 W Y .

The conditional bias is B = E[m̂(0)|X 1 , . . . , X N ] − m(0). Note that E(m̂(0)|X ) = e10 (R 0 W R)−1 R 0 W M, where M =
(m(X 1 ), . . . , m(X N ))0 . Let m (k) (x) denote the kth derivative of m(x) with respect to x. Using Assumption (ii) in Lemma
A1, a Taylor expansion of m(X i ) yields
1
m(X i ) = m(0) + m (1) (0)X i + m (2) (0)X i2 + Ti ,
2
where

|Ti | ≤ sup |m (3) (x)|∙|X i3 |.
x

Thus, we can write the vector M as
M=R




m(0)
+S+T,
m (1) (0)

where the vector S has ith element equal to Si = m (2) (0)X i2 /2, and the vector T has typical element Ti . Therefore, the
bias can be written as
B = e10 (R 0 W R)−1 R 0 W M − m(0) = e10 (R 0 W R)−1 R 0 W (S + T ).
Using Lemma A2, we have


−1
1 0
R WR
=
N

F0 F1
F1 F2



=



!−1

=

1
F0 F2 − F12

F2 −F1
−F1 F0


!

F2∗
F1∗
+ o p (1)
− h1
F0∗ F2∗ −(F1∗ )2
F0∗ F2∗ −(F1∗ )2



F0∗
F1∗
1
+ o p (1)
− h1
∗
∗
∗
2
2
h
F0 F2 −(F1 )
F0∗ F2∗ −(F1∗ )2


+ o p (1) 


+ o p (1)

ν2
ν1
+ o p (1) −
+ op
(ν0 ν2 −ν12 ) f (0)h
 (ν0 ν2 −ν12 ) f (0)


 
=
ν1
−
+ o p h1
O p 12
h
(ν0 ν2 −ν12 ) f (0)h




=

 
O p (1) O p h1

 
 .
1
1
Op h Op 2
h

 
1
h 



Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

Z
h 2 j−1 ∞
(K (x))2 ∙x 2 j f (hx)d x = O
N
0

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

Next,
F3

1 0
R W T = sup m (3) (x)∙
N
x

F4

!

Thus,

B = e10 (R 0 W R)−1 R 0 W S + o p (h 2 ).

 P


N
1
2
ν2 h 2 + o p (h 2 )
1 0
1 (2)  N i=1 K h (X i )X i  1 (2)
.
(R W S) = m (0)
= m (0) f (0) 
1 P N K (X )X 3
N
2
2
ν h 3 + o (h 3 )
N

Therefore,

 
1
∙o p (h 3 ) = o p (h 2 ),
h

h

i=1

i

B = e10 (R 0 W R)−1 R 0 W S + o p (h 2 ) =

p

3

i

ν 2 − ν 3 ν1
1 (2)
m (0) 2
2
ν0 ν2 − ν12

!

h 2 + o p (h 2 ).

This finishes the proof for the first part of the result in Lemma A1, equation (A.1).
Next, we consider the expression for the conditional variance in equation (A.2).
V = V(m̂(0)|X 1 , . . . , X N ) = e10 (R 0 W R)−1 R 0 W 6W R(R 0 W R)−1 e1 ,
where 6 is the diagonal matrix with (i, i)th element equal to σ 2 (X i ).
Consider the middle term


1 P K 2 (X )σ 2 (X ) 1 P K 2 (X )X σ 2 (X )
i
i
i h i
i h i i
1 0
N
N
=
R W 6W R =  P
P 2
1
N
K 2 (X )X σ 2 (X ) 1
K (X )X 2 σ 2 (X )
N

i

h

i

i

i

i

n

h

i

G0 G1
G1 G2

i

i

!

.

Thus, we have

NV =
=

1
(F0 F2 − F12 )2

e10

F2 −F1
−F1 F0

F22 G 0 − 2F1 F2 G 1 + F12 G 2
(F0 F2 − F12 )2

!

G0 G1
G1 G2

!

F2 −F1
−F1 F0

!

e1

.

Applying Lemmas A2 and A3, this leads to
V=

ν 2 π0 − 2ν1 ν2 π1 + ν12 π2
σ 2 (0)
∙ 2
f (0)N h
(ν0 ν2 − ν 2 )2
1

!

+ op




1
.
Nh

This finishes the proof for the statement in equation (A.2). The final result in equation (A.3) follows directly from the
first two results. k
Proof of Lemma 3.1. Applying Lemma A1 to the N+ units with X i ≥ c implies that
1/2 (2)

E[μ̂+ − μ+ |X 1 , . . . , X N ] = C1 m + (c)h 2 + o p (h 2 ),
and
V(μ̂+ − μ+ |X 1 , . . . , X N ) = C2

2 (c)
σ+

f X |X ≥c (c)N+ h

+ op



1
N+ h



.

Because N+ /N = pr(X i ≥ c) + O p (1/N ), and f X |X ≥c (x) = f (x)/Pr(X i ≥ c), it follows that
V(μ̂+ − μ+ |X 1 , . . . , X N ) = C2

2 (c)
σ+

f (c)N h

+ op




1
.
Nh

Conditional on X 1 , . . . , X N , the covariance between μ̂+ and μ̂− is zero, and thus, combining the results from applying
Lemma A1 also to the units with X i < c, we find

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

Similarly,



 
o p h2
.
≤
o p (h 3 )

e10 (R 0 W R)−1 R 0 W T = O p (1)∙o p (h 2 ) + O p
implying

955

956

REVIEW OF ECONOMIC STUDIES
E[(τ̂SRD − τSRD )2 |X 1 , . . . , X N ] = E[(μ̂+ − μ̂− − (μ+ − μ− ))2 |X 1 , . . . , X N ]
= E[(μ̂+ − μ+ )2 |X 1 , . . . , X N ] + E[(μ̂− − μ− )2 |X 1 , . . . , X N ]

−2∙E[μ̂+ − μ+ |X 1 , . . . , X N ]∙E[μ̂− − μ− |X 1 , . . . , X N ]
!


2 (c)

2
σ 2 (c) σ−
C
1
(2)
(2)
+
= C1 ∙h 4 ∙ m + (c) − m − (c) + 2 ∙ +
+ o p h4 +
,
N ∙h
f (c)
f (c)
N ∙h

2 (c)
σ+

(2)
(2)
h opt = arg min C1 h 4 (m + (c) − m − (c))2 + C2
h

which leads to
h opt =



f (c)N h

+

2 (c)
σ−

f (c)N h

!!

,


1/5
2 (c)
2 (c)
σ+
σ−

+
C2 1/5 
f (c)
f (c)

−1/5 .
 (2)
 N
(2)
4C1
(m + (c) − m − (c))2

k
Motivation for the bandwidth choice in equation (15) in Step 2 of bandwidth algorithm:
Fan and Gijbels (1996, Theorem 3.2) give an asymptotic approximation to the MSE for an estimator of the νth derivative
of a regression function at a boundary point using a pth order local polynomial (using the notation in Fan and Gijbels).
Specializing this to our case, with the boundary point c, a uniform one-sided kernel K (t) = 10≤t≤1 and interest in the
second derivative using a local quadratic approximation (ν = p = 2), their MSE formula simplifies to
MSE =

!
2 (c)
1 2 (3)
1 σ+
2
2
K (m (c)) h + 4K 2
(1 + o p (1)).
9 1 +
N h 5 f (c)

Here,
K1 =
where

so that


 0 μ0
0 
K ∗ (t) =  0  
 μ1
1
μ2

Z

t 3 K ∗ (t)dt

μ1 μ2

−1


μ2 μ3 

μ3 μ4

and

K2 =




1
 t  ∙K (t),
t2

Z

(K ∗ (t))2 dt,

with μk =

Z

q k K (q)dq =

1
,
(k + 1)

 0 
−1  
1
0
1 1/2 1/3
 t  ∙K (t) = (30 − 180t + 180t 2 )∙1[0,1] ,
1
1/3 1/4 1/5
t2

K ∗ (t) =  0   1/2 1/3 1/4 

and therefore, K 1 = 1.5 and K 2 = 180. Thus,

1 (3)
1
(m (c))2 h 2 + 720
4 +
N h5

MSE =

2 (c)
σ+

f + (c)

!

(1 + o p (1)).

Minimizing this over h leads to
h 2,+ = 72001/7 ∙
Proof of Theorem 4.1.
Write

2 (c)
σ+
(3)

f (c)(m + (c))2

!1/7

−1/7

N+

≈ 3∙56∙

2 (c)
σ+
(3)

f (c)(m + (c))2

!1/7

−1/7

N+

.

Before directly proving the three claims in the theorem, we make some preliminary observations.

h opt = Copt ∙N −1/5 ,




with Copt = C K ∙ 


2 (c) + σ 2 (c)
σ−
+

f (c)∙



2
(2)
(2)
m + (c) − m − (c)

1/5





Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

proving the first result in Lemma 31.
For the second part of Lemma 3.1, solve

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

and
ĥ opt = Ĉopt

∙N −1/5 ,

with Ĉopt = C K ∙

2 (c) + σ̂ 2 (c)
σ̂−
+

(2)
(2)
fˆ(c)∙((m̂ + (c) − m̂ − (c))2 + r̂+ + r̂− )

!1/5

957

.

h 2,+ = o p (1)

and

(N h 52,+ )−1 = o p (1).

(A.4)

Let m 3 be the probability limit of m̂ (3) (c). This probability limit need not be equal to m (3) (c), but it will exist under the
assumptions in Theorem 4.1. As long as this probability limit differs from zero, then h 2,+ = O p (N −1/7 ), so that the
(2)
(2)
two conditions in equation (A.4) are satisfied and m̂ + (c) is consistent for m + (c).
2
4
2 (c). The denominator is
Fourth, consider r̂+ = 2160σ̂+ (c)/(N2,+ h 2,+ ). The numerator converges to 2160σ+
4
−4/7
2/7
N2,+ ∙h 2,+ = C∙(N ∙h 2,+ )∙N
(1 + o p (1)) = C∙N (1 + o p (1)), so that the ratio is C∙N −2/7 (1 + o p (1)) = o p (1).
A similar result holds for r̂− .
Now we turn to the statements in Theorem 4.1. We will prove (iii), then (iv), and then (i) and (ii). First, consider
(2)
(2)
(2)
(2)
(iii). If m + (c) − m − (c) differs from zero, then Copt is finite. Moreover, in that case (m̂ + (c) − m̂ − (c))2 + r̂+ + r̂−
(2)
(2)
converges to (m̂ + (c) − m̂ − (c))2 , and Ĉopt converges to Copt . These two implications in turn lead to the result that
(ĥ opt − h opt )/ h opt = (Ĉopt − Copt )/Copt = o p (1), finishing the proof for (iii).
Next, we prove (iv). Because h opt = Copt ∙N −1/5 , it follows that


1
MSE(h opt ) = AMSE(h opt ) + o p h 4opt +
= AMSE(h opt ) + o p (N −4/5 ).
N ∙h opt
Because ĥ opt = (Ĉopt /Copt )∙Copt N −1/5 and Ĉopt /Copt → 1 it follows that
MSE(ĥ opt ) = AMSE(ĥ opt ) + o p (N −4/5 ).
Therefore,
and

N 4/5 ∙(MSE(ĥ opt ) − MSE(h opt )) = N 4/5 ∙(AMSE(ĥ opt ) − AMSE(h opt )) + o p (1),
MSE(ĥ opt ) − MSE(h opt )
N 4/5 ∙(MSE(ĥ opt ) − MSE(h opt ))
=
MSE(h opt )
N 4/5 ∙MSE(h opt )
=
=

N 4/5 ∙(AMSE(ĥ opt ) − AMSE(h opt )) + o p (1)
N 4/5 ∙AMSE(h opt ) + o p (1)

N 4/5 ∙(AMSE(ĥ

opt ) − AMSE(h opt ))

N 4/5 ∙AMSE(h opt )

+ o p (1).

Because N 4/5 ∙AMSE(h opt ) converges to a non-zero constant, all that is left to prove in order to establish (iv) is that
N 4/5 ∙(AMSE(ĥ opt ) − AMSE(h opt )) = o p (1).
Substituting in, we have
(2)

(2)

N 4/5 ∙(AMSE(ĥ opt ) − AMSE(h opt )) = C1 ∙(m + (c) − m − (c))2 ∙((N 1/5 h opt )4 − N 1/5 ĥ opt )4 )
+

C2
C2
−
N 1/5 ∙h opt N 1/5 ∙ĥ opt

!

∙

2 (c)
σ+

σ 2 (c)
+ −
f (c)
f (c)

!

= o p (1)

because N 1/5 h opt − N 1/5 ĥ opt = Copt − Ĉopt = o p (1), so that equation (A.5) holds, and therefore, (iv) holds.

(A.5)

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

2 (c), σ̂ 2 (c), fˆ(c), m̂ (2) (c) and m̂ (2) (c) converge
First, we show that the various estimates of the functionals in Ĉopt , σ̂−
+
+
−
(2)
(2)
2
2
to their counterparts in Copt , σ− (c), σ+ (c), f (c), m + (c) and m − (c). Consider fˆ(c). This is a histogram estimate of
density at c, with bandwidth h = C N −1/5 . Hence, fˆ(c) is consistent for f (c) if f − (c) = f + (c) = f (c), if the left- and
right-hand limit are equal and for ( f − (c) + f + (c))/2 if they are different.
2 (c) (and σ̂ 2 (c)). Because it is based on a bandwidth h = C∙N −1/5 that converges to zero, it is
Next, consider σ̂−
+
2 (c) if σ 2 (c) = σ 2 (c) = σ 2 (c).
consistent for σ−
−
+
(2)
Third, consider m̂ + (c). This is a local quadratic estimate using a one-sided uniform kernel. From (Fan and Gijbels,
(2)
(2)
1996, Theorem 3.2), it follows that to guarantee consistency of m̂ + (c) for m + (c), we need both

958

REVIEW OF ECONOMIC STUDIES

Now we turn to (i). If Assumption 3.6 holds, ĥ opt = Ĉopt N −1/5 , with Ĉopt → Copt , a non-zero constant. Then, Lemma
−4/5 ) so that τ̂
−2/5 ). Next con3.1 implies that MSE(ĥ opt ) is O p (ĥ 4opt + N −1 ĥ −1
SRD − τSRD = O p (N
opt ) = O p (N
(2)

(2)

sider (ii). If Assumption 36 does not hold and m + (c) − m + (c) = 0. Because h 2,+ = C N −1/7 , it follows that r+ =
−1 −4
C N2,+
h = C N −2/7 (1 + o p (1)) (with each time different constants C), it follows that ĥ opt = C(N 2/7 )1/5 N −1/5 =
−1/7
CN
, so that the MSE(h) = C N −6/7 + C̃ N −6/7 = C N −6/7 (note that the leading bias term is now O(h 3 ) so that
the square of the bias is O(h 6 ) = O(N −6/7 )) and thus τ̂SRD − τSRD = O p (N −3/7 ), and thus the result holds. k

REFERENCES
ANGRIST, J. and LAVY, V. (1999) “Using Maimonides’ Rule to Estimate the Effect of Class Size to Estimate the Effect
of Class Size on Scholastic Achievement”, Quarterly Journal of Economics, 114, 533–575.
BLACK, S., (1999), “Do Better Schools Matter? Parental Valuation of Elementary Education”, Quarterly Journal
of Economics, 114, 577–599.
CHAY, K. and GREENSTONE, M. (2005) “Does Air Quality Matter? Evidence from the Housing Market”, Journal
of Political Economy, 103, 376–424.
CHENG, M.-Y., FAN, J. and MARRON, J. S. (1997), “On Automatic Boundary Corrections”, Annals of Statistics, 25,
1691–1708.
COOK, T. (2008), “Waiting for Life to Arrive: A History of the Regression-Discontinuity Design in Psychology, Statistics and Economics”, Journal of Econometrics, 142, 636–654.
DESJARDINS, S. and MCCALL, B. (2008), “The Impact of the Gates Millennium Scholars Program on the Retention,
College Finance- and Work-Related Choices, and Future Educational Aspirations of Low-Income Minority Students”
(Unpublished Manuscript).
FAN, J. and GIJBELS, I. (1992), “Variable Bandwidth and Local Linear Regression Smoothers”, Annals of Statistics,
20, 2008–2036.
FAN, J. and GIJBELS, I.. (1996), Local Polynomial Modeling and its Implications, Monographs on Statistics and Applied Probability no. 66 (Boca Raton: Chapman and Hall/CRC).
FRANDSEN, B. (2008), “A Nonparametric Estimator for Local Quantile Treatment Effects in the Regression Discontinuity Design” (Unpublished Working Paper, Department of Economics, MIT).
FRÖLICH, M. (2007), “Regression Discontinuity Design with Covariates” (IZA Discussion Paper No. 3024, Bonn).
FRÖLICH, M. and Melly, B. (2008), “Quantile Treatment Effects in the Regression Discontinuity Design” (IZA Discussion Paper No. 3638, Bonn).
HAHN, J., TODD, P. and VAN DER KLAAUW, W. (2001), “Regression Discontinuity”, Econometrica, 69, 201–209.
HÄRDLE, W. (1992), Applied Nonparametric Regression (Cambridge: Cambridge University Press).
IMBENS, G. and ANGRIST J. (1994), “Identification and Estimation of Local Average Treatment Effects ”, Econometrica, 61, 467–476.
IMBENS, G. and LEMIEUX, T. (2008), “Regression Discontinuity esigns”, Journal of Econometrics, 142, 615–635.
KALYANARAMAN, K. (2008), “Bandwidth Selection for Linear Functionals of the Regression Function” (unpublished, Department of Economics, University College London).
LEE, D. (2008), “Randomized Experiments from Non-random Selection in U.S. House Elections”, Journal of Econometrics, 142, 675–697.
LEE, D. and LEMIEUX, T. (2010), “Regression Discontinuity Designs in Economics”, Journal of Economic Literature,
48, 281–355.
LI, K.-C. (1987), “Asymptotic Optimality for C p , C L , Cross-validation and Generalized Cross-validation: Discrete
Index Set”, Annals of Statistics, 15, 958–975.
LUDWIG, J. and MILLER, D. (2005), “Does Head Start Improve Children’s Life Chances? Evidence from a Regression
Discontinuity Design” (NBER Working Paper No. 11702).
LUDWIG, J. and MILLER, D. (2007), “Does Head Start Improve Children’s Life Chances? Evidence from a Regression
Discontinuity Design”, Quarterly Journal of Economics, 122, 159–208.
MCCRARY, J. (2008), “Manipulation of the Running Variable in the Regression Discontinuity Design: A Density Test”,
Journal of Econometrics, 142, 698–714.
PORTER, J. (2003), “Estimation in the Regression Discontinuity Model” (unpublished, Department of Economics,
University of Wisconsin, Madison).
RUBIN, D. (1974), “Estimating Causal Effects of Treatments in Randomized and Non-randomized Studies”, Journal of
Educational Psychology, 66, 688–701.
RUPPERT, D. and WAND, M. P. (1994), “Multivariate Locally Weighted Least Squares Regression”, Annals of Statistics, 22, 1346–1370.

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

Acknowledgment. Financial support for this research was generously provided through National Science Foundation grants 0631252, 0820361, and 0961707. We are grateful to David Lee for making his data available and to
Joshua Angrist, Tom Cook, Tom Lemieux, Doug Miller, Fernando Yu, Fanyin Zheng, a co-editor, and three referees for
comments.

IMBENS & KALYANARAMAN

OPTIMAL BANDWIDTH CHOICE

959

Downloaded from https://academic.oup.com/restud/article-abstract/79/3/933/1533189 by guest on 18 October 2019

SHADISH, W., CAMPBELL, T. and COOK, D. (2002), Experimental and Quasi-experimental Designs for Generalized
Causal Inference (Boston: Houghton and Mifflin).
STONE, C. (1982), “Optimal Global Rates of Convergence for Nonparametric Regression”, Annals of Statistics, 10,
1040–1053.
THISTLEWAITE, D. and CAMPBELL, D. (1960), “Regression-Discontinuity Analysis: An Alternative to the Ex-post
Facto Experiment”, Journal of Educational Psychology, 51, 309–317.
VAN DER KLAAUW, W. (2002), “A Regression–discontinuity Evaluation of the Effect of Financial Aid Offers on
College Enrollment”, International Economic Review, 43, 1249–1287.
VAN DER KLAAUW, W. (2008), “Regression–Discontinuity Analysis: A Survey of Recent Developments in Economics”, Labour, 22, 219–245.
WAND, M. and JONES, M. (1994), Kernel Smoothing (Boca Raton, FL: Chapman and Hall).

