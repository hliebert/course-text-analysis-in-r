A Least Squares Correction for Selectivity Bias
Author(s): Randall J. Olsen
Source: Econometrica, Vol. 48, No. 7 (Nov., 1980), pp. 1815-1820
Published by: The Econometric Society
Stable URL: https://www.jstor.org/stable/1911938
Accessed: 17-10-2019 15:51 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

The Econometric Society is collaborating with JSTOR to digitize, preserve and extend access
to Econometrica

This content downloaded from 206.253.207.235 on Thu, 17 Oct 2019 15:51:21 UTC
All use subject to https://about.jstor.org/terms

Econometrica, Vol. 48, No. 7 (November, 1980)
NOTES AND COMMENTS

A LEAST SQUARES CORRECTION FOR SELECTIVITY BIAS
BY RANDALL J. OLSEN'
1. INTRODUCTION

WHEN ESTIMATING REGRESSION MODELS it is very nearly always assumed that the
sample is random. The recent literature has begun to deal with the problems which arise
when estimating a regression model with samples which may not be random. The most
general case in which one only has access to a single nonrandom sample has not been
addressed since it is a very imposing problem. The case which has been addressed starts

with a random sample but considers the problem of missing values for the dependent
variable of a regression. If the determination of which values are to be observed is related to
the unobservable error term in the regression, then methods such as ordinary least squares

are in general inappropriate. By constructing a joint model which represents both the
regression model to be estimated and the process determining when the dependent variable
is to be observed, some progress can be made towards taking into account nonrandomness
for the observed values of the dependent variable. The actual techniques employed fall into
two rough groups, full information maximum likelihood models, and limited information
methods which are more easily estimated. In the full information category are two
methods. One model combines the probit and the normal regression models, and the other
combines the Tobit or limited dependent variable model with the normal regression model.
The form of the probit regression model is
y,=1 iff u1>-Zy,

y1=0 iff ul1<-Zy,
Y2 = Xf3 +u2,

Y2 observed iff Yi = 1.

It is assumed u1 and u2 have a bivariate normal distribution with var (u,) = 1 imposed as a
normalizing assumption. The likelihood solution is obtained by iterative methods.
The maximum likelihood model based upon the Tobit model has the following general
form:

Y,=0 iff u1<-Z'y,
y, = Z-y + u otherwise,
Y2 = X8 +? U2,

Y2 observed iff y, > 0.
The equation for y, performs the function of the probit model in the first example by
indicating when the dependent variable of the regression is missing. This model (with some

alterations) was estimated in the seminal work by Heckman [4].
The desire for a method which would be easier to compute than these maximum
likelihood models led to the Mill's ratio method. This method requires only the estimation
of a simple probit model in order to construct a new regressor which, when included in the
original regression model, corrects for the possibility of selection bias or nonrandom
sampling with respect to the observed dependent variable. Below we show how with a slight
alteration in the derivation of the Mill's ratio method a more general solution can be
obtained. In particular, we show how one may start with a linear probability model and
derive a similar correction. The advantage of this latter method is that it only requires
regression techniques so that an iterative probit is not a necessary first step. The general

1 This research was supported by the Institution for Social and Policy Studies at Yale. I have
benefited from helpful suggestions by anonymous referees.
1815

This content downloaded from 206.253.207.235 on Thu, 17 Oct 2019 15:51:21 UTC
All use subject to https://about.jstor.org/terms

1816

RANDALL

J.

OLSEN

method developed here has been extended by Hay [3] to the polytomous logit model in a
generalization of the standard selection model.

In Section 2 we derive this simple estimator and show its similarity to the Mill's ratio
method. In Section 3 we discuss briefly what these linear correction methods imply about
the distribution of the error terms and the role the distribution of the error term plays in

identifying the effects of selection. An example is given which compares the results
obtained with both methods. Section 4 summarizes.

2. DERIVATION OF THE ESTIMATOR

The versatility of the Mill's ratio method as a correction for specification error has been

shown by Heckman [5, 6] and no attempt will be made here to repeat his arguments in the
entirety. However, much of what is done below closely parallels his derivations. The major

difference is that we will not assume bivariate normality. As we will see, this extends the
generality of his results for selection and allows us to obtain a different correction method.
Let us assume the regression model of interest is

(1) Yi = XV+ui
and that yi is observed if and only if Ii = 1 where

Ii = 1 iff vi < Zjy,
I =O iff vi:Ziy.

We will assume Xi and Zi are row vectors which conform to the column vector
coefficients 1 and y, respectively. The X's and Z's are exogenous. Assume
E(ui) = 0, E(vi) =

E(uiu,) = u i =j,
= 0 otherwise,

E[(vi - ,u.)(vi - ,u. )] = 0a2Usi j
= 0 otherwise,

E(uiv1) = PUuov, i = j,
= 0 otherwise,

E(uilvi) = p(vi - Av)SuSv-

By assuming the conditional expectation of ui given vi is linear in vi we can use
decomposition

(2) Ui = P ( - Julav + Ei

where var (Ei) = U(1 _-p2) with Ei uncorrelated with vi. When we estimate th
(1) for the subsample with observed y's we seek to calculate the conditional mean

E(yijXi, Ii = 1) or E(yijXi, vi <Zi^y). Since

yi = X, + P (vi -1v)u/v + gi
it follows

E(yilXi, vi < Ziy) = XiI3 +pouE(vilvi < Ziy)/v -pouuvl/v.
TFhe conditional variance of this conditional expectation is

var (yilXi, Vi < Ziy) = p20r2 var (viIvi < Ziy)/02v + o2 (1 _ p2).

This content downloaded from 206.253.207.235 on Thu, 17 Oct 2019 15:51:21 UTC
All use subject to https://about.jstor.org/terms

LEAST

SQUARES

CORRECTION

1817

Up to this point we have assumed the conditional expectation of ui given vi is linear in vi,
but if we also assume that v is standard normal, then

E(yi IXi, vi < Zy) = Xf3 + SA ,

Ai = f(zy) = E(vi I vi < Zy),
F(Ziy)

var (yXi, vi <Zy) = o_I{1 +p2Ai[Ziy_-Aij},
where f(.) and F(*) are the standard normal pdf and cdf, respectively. Without assuming
normality for the u's we have arrived at Heckman's specification:

Yi = Xi + 8Ai + 7mi,
var (1i) = oTE2 + p 2o-2U [1 +Al (Ziy -Ai)],

so his result does not require bivariate normality, only the normality of the v's and the
linearity of the conditional expectation of u given v. Bivariate normality is a sufficient
condition for these results to hold, but not necessary. Of course the Ai's are unknown, so it is

necessary to estimate a probit model where pr (Ii = 1) = F(Z,y) in order to derive estimates
for the vector y and then construct Ai based upon Zi^y.
Suppose instead the v's were uniformly distributed over [0, 1]. Putting last things first, we

note that pr (vi < Ziy) = Ziy svo the y vector can be consistently estimated using the linear
probability model. Using the uniform distribution we find

E(yiXi, vi < Ziy) = Xi3 +po-uV13(Ziy - 1),
var (yjXi,,vi < Zi^y) = o_2 (1 _ p2) + p2(ZiY)2,
so we estimate

Yi =Xi + 8(Ziy - 1) + mi,

(3) 8 =pou V3,
var (7i) = SE + p 2U2 (Ziy)2.
For preliminary work equation (3) will be very simple to work with.
Since y is not typically known, in (3) we must use 'y from the linear probability model so
that the model estimated is

Yi= Xi + (Zi A_- 1) + pi,
Vi -,8Zi (eY -,) +ni.
The covariance matrix of the error terms vi is

(4) j2Z, + Z'2+ I + 8 * D[(Zi_Y)/ ]|3
where

IZ
Z.

and D[(Ziy)2] is a diagonal matrix with the ith term being (Zy)2. I is the varian

covariance matrix of the jy. The expected value of the sum of squared residuals is the t
of the matrix in (4), and so we can estimate a E by setting the sum of squared residuals equal
to the trace in (4). Using the degrees of freedom as our divisor yields

A 2 v S > *tr(Z:Z')-S2 - -trD ]/3

O.-,= ~--

This content downloaded from 206.253.207.235 on Thu, 17 Oct 2019 15:51:21 UTC
All use subject to https://about.jstor.org/terms

1818

RANDALL

J.

OLSEN

where . is the estimate2 of ., k is the number of regressors in Xi, and n is the number of

observations with observed yi. Now o-2 = o-2(1 -p2) and =pou-3, so we may calculate
A

A

V( 1P 2) A13

and solve for 4. These results are slight modifications of the established results using A.

3. THE DISTRIBUTION OF THE RESIDUALS AND IDENTIFICATION

A natural question is whether the assumption that v, is uniformly rather than normally
distributed together with the assumption that the conditional expectation of ui given vi is
linear in vi implicitly imposes an outrageous distribution upon ui. Returning briefly to
equation (2), if we assume Ei is normal, then assuming vi is uniform implies ui cannot be
normal. The distribution of ui will be the convolution of a uniform and a normal density
which means the density function for ui will be symmetric but with a broader peak and
narrower tails. Only when lP I exceeds 0.5 does this hybrid density function differ noticeably

from the normal. In the extreme case where lpI = 1 the density of u becomes uniform which
is a most unlikely distribution for a regression residual. Heckman [5] has shown p = 1 arises
when correcting for truncation, and so the P- 1 method would be unseemly (although
technically correct) in this extreme case.

If the distribution of Ei is not normal, then ui will follow a nonnormal distribution even
vi is normal. While the assumption that Ei is normal is convenient, there is no reason to
believe this assumption is usually correct. On purely a priori grounds there is a little reason
to prefer either the A or P - 1 method because of the distributional restrictions implied. In
fact, if selection is present the only regression residuals we can look at will not be

representative of the population of u's, since we will be dealing with the distributi
given vi < Ziy. In the bivariate normal case it has been shown by Olsen [11] that sel
causes the observed residuals to follow nonidentical skewed density functions formed by

the convolution of independent normal and truncated normal densities. If ei is normal and
vi uniform it can be shown the observed residuals have nonidentical density functions
formed by the convolution of uniform and normal random variables. The distribution of the
residuals contains substantial information on selection; this approach is pursued in more
detail in Olsen [12].
On a more practical level, the most important distinction between the two models is the

apparent difference in the conditions required to identify the effect of sample selection. Th

probit method requires the matrix (XA) to have full rank whereas the linear probability
method requires this rank condition to hold for (XP- 1). Because the Mill's ratio is a
nonlinear function of the exogenous variables in the probit model, the same set of
regressors may be used in the regression and probit models without encountering perfect
collinearity when using the A correction.3 The use of the P- 1 correction requires the
presence of a regressor in the linear probability model which does not appear in the
regression model. In order to identify the effect of sample selection one could include
higher order powers and cross products in the linear probability model but exclude them
from the regression equation.

The use of nonlinearities in the exogenous variables to identify a relationship is rather

unappealing. We usually have some intuition about the proper set of explanatory variables
to be included in a regression, but we rarely have any intuition about the proper functional

2 If correct standard errors are desired generalized least squares should be applied to the linear
probability model. This should also be done if au and p are desired. This requires the predicted
probabilities to be between zero and one.

3 The Mill's ratio method breaks down if Z is composed of mutually exclusive and exhaustive set
dummy variables and Z is contained in X. See Gronau [2].

This content downloaded from 206.253.207.235 on Thu, 17 Oct 2019 15:51:21 UTC
All use subject to https://about.jstor.org/terms

LEAST SQUARES CORRECTION 1819

form for the exogenous variables. Including these same higher order powers and cross
products in the regression equation as well as in the linear probability model will leave the
effect of selection again unidentified when using P - 1.

A similar result holds using the A method. When the values of X enter linearly in both the
probit and regression models identification is achieved since A is nonlinear in X. As higher
order powers and interactions of the X's are added to the regression equation,
identification is not lost in the sense of a failure of the rank condition, but rather
progressively more severe multicollinearity sets in. The coefficients on A and the other
coefficients become very unstable and inference becomes very difficult. While the conditions for identification of the A and P - 1 methods appear to differ, in a fundamental sense
they are the same. If there are no a priori exclusionary restrictions4 which can be imposed

upon the regression equation, it is unlikely that there exist a priori restrictions upon the
functional form of the exogenous variables in the regression. If the effect of selection is not
identified using the P - 1 method, careful thought should precede the use of A as a
substitute technique.

In practice the two correction methods produce very similar results. As an illustration we
use data from the Survey of Economic Opportunity for white wives 25-29 whose spouse is
present and working. Of the 1239 wives, 546 worked in the survey week. In Table I we
show the estimates of their wage equation using least squares as well as the A and P - 1
correction methods. t ratios formed for these estimates would be correct under the null
hypothesis that selection is absent. The effect of selection is identified by excluding the
number of children in various age categories from the wife's wage equation but including

TABLE I

ALTERNATIVE CORRECTIONS FOR SELECTIONa

P-1
Explanatory

Variable

Constant

OLS

0.786

Method

0.833

Method

0.835

(0.565) (0.563) (0.560)
City 250,000-500,000 -0.089 -0.090 -0.090
(0.122) (0.122) (0.120)

City over 500,000 -0.026 -0.002 0.003
(0.093) (0.067) (0.075)
Not in SMSA -0.204 -0.190 -0.188
In

South

Age

-0.105

-0.003

Education
Disabled

(0.094) (0.095) (0.094)
-0.097 -0.097

(0.117) (0.117) (0.117)
0.003 0.005

(0.018) (0.019) (0.021)
0.130 0.122 0.121

-0.002

(0.012) (0.013) (0.013)
-0.022 -0.025

(0.100) (0.122) (0.114)
In South when 16 -0.078 -0.112 -0.118
(0.116) (0.119) (0.119)

Selection
R2
a
be

Term

0.224

-

0.182

0.363

(0.098) (0.166)
0.229
0.231

Standard
errors
are
below
correct
under
the
null
h

the logarithm of the wage rate.

4Linear restrictions on the coefficients in the regression model may be used to achieve identification
in the place of exclusionary restrictions.

This content downloaded from 206.253.207.235 on Thu, 17 Oct 2019 15:51:21 UTC
All use subject to https://about.jstor.org/terms

1820

RANDALL

J.

OLSEN

them in the probability of working model. The effect of selection in this example is to
increase the estimated population mean of the wage offer distribution by about 20 per cent,
which is evidence of considerable selection. We see that the two correction methods give
very similar results. This similarity was held for other applications. Although R2 is
meaningless if selection is present, both corrections explain about the same share of the

variance indicating the issue of linearity in P - 1 versus nonlinearity in A is not important in
this application.
Yale University
Manuscript received May, 1979; revision received February, 1980.

REFERENCES

[1] GRILICHES, Z., B. HALL, AND J. HAUSMAN: "Missing Data and Self Selection in Large
Panels," Harvard University Discussion Paper 573, September, 1977.

[2] GRONAU, REUBEN: "Wage Comparisons-A Selectivity Bias," Journal of Political Economy,
82 (1974), 1119-1143.

[3] HAY, JOEL W.: "Physicians' Specialty Choice and Specialty Wage Structure," Mimeographed,
New Haven, Conn., 1979.

[4] HECKMAN, JAMES: "Shadow Prices, Market Wages, and Labor Supply," Econometrica, 42
(1974), 679-694.

[5] : "The Common Structure of Statistical Models of Truncation, Sample Selection, and
Limited Dependent Variables and a Simple Estimator for Such Models," Annals of Economic
and Social Measurement, 5 (1976), 475-492.

[6] : "Sample Bias as a Specification Error," Econometrica, 47 (1979), 153-162.
[7] LEE, LUNG-FEI: "Unionism and Wage Rates: A Simultaneous Equations Model with Qualita
tive and Limited Dependent Variables," International Economic Review, 19 (1978), 415-433.
[8] LEE, LUNG-FET, AND ROBERT TROST: "Estimation of Some Limited Dependent Variable
Models with Application to Housing Demand," Journal of Econometrics, 8 (1978), 357-382.
[9] LEWIS, H. G.: "Comments on Selectivity Biases in Wage Comparisons," Journal of Political
Economy, 82 (1974), 1145-1155.

[10] NELSON, FORREST D.: "Censored Regression Models with Unobserved, Stochastic Censoring
Thresholds," Journal of Econometrics, 6 (1977). 309-327.

[11] OLSEN, RANDALL J.: "An Econometric Model of Family Labor Supply," Ph.D. Dissertation,
University of Chicago, December, 1977.

[12] : "Tests for the Presence of Selectivity Bias and Their Relation to Specifications of
Functional Form and Error Distribution," Mimeographed, New Haven, Conn., 1979.

This content downloaded from 206.253.207.235 on Thu, 17 Oct 2019 15:51:21 UTC
All use subject to https://about.jstor.org/terms

