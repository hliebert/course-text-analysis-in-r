Statistics & Probability Letters 48 (2000) 253 â€“ 259

Robust simulation-based estimation
Marc G. Gentona; âˆ— , Xavier de Lunab
a Department

b Department

of Mathematics, Massachusetts Institute of Technology, Cambridge, MA 02139-4307, USA
of Statistical Science, University College London, Gower Street, London, WC1E 6BT, UK
Received July 1999; received in revised form September 1999

Abstract
The simulation-based inferential method called indirect inference was originally proposed for statistical models whose
likelihood is dicult or even impossible to compute and=or to maximize. In this paper, indirect estimation is proposed
as a device to robustify the estimation for models where this is not possible or dicult with classical techniques such as
M-estimators. We derive the in uence function of the indirect estimator, and present results about its gross-error sensitivity
and asymptotic variance. Two examples from time series are used for illustration. c 2000 Elsevier Science B.V. All rights
reserved
Keywords: Asymptotic variance; B-robustness; Gross-error sensitivity; In uence function; M-estimator; Indirect inference;
Time series

1. Introduction
Suppose a set of observations x1 ; : : : ; x n has been collected, and are assumed to have been generated from a
probability model F(Ã‚), where Ã‚ âˆˆ  is an unknown parameter. Maximum likelihood is then commonly used
to estimate Ã‚. However, the latter is notoriously not robust to the presence of outliers, and robust inferential
methods have been developed for that reason. A theory of robustness (Huber, 1981; Hampel et al., 1986) has
developed around simple models for contaminated data. For instance, when observations are independently
distributed, contamination models are of the form (1 âˆ’ )F(Ã‚) + x , with  small and where x is the measure
putting all its mass at x. For dependent data more complex contaminations may occur, see Martin and Yohai
(1986). The properties of di erent inferential methods can be studied under the original assumed model
F(Ã‚), and a contaminated version. Maximum likelihood is asymptotically optimal under the former while
having often poor performances under the latter. For simple models (typically linear in the parameter Ã‚ and
assuming independence), robust inferential methods are readily available (Hampel et al., 1986). However, for
more complex situations such robust inferential methods and their theoretical properties may be intricate if not
impossible to obtain. Indirect inference (Gourieroux et al., 1993) may then be a solution as it was rst noticed
âˆ— Corresponding author.
E-mail addresses: genton@math.mit.edu (M.G. Genton), xavier@stats.ucl.ac.uk (X. de Luna)

0167-7152/00/$ - see front matter c 2000 Elsevier Science B.V. All rights reserved
PII: S 0 1 6 7 - 7 1 5 2 ( 0 0 ) 0 0 0 0 4 - 3

254

M.G. Genton, X. de Luna / Statistics & Probability Letters 48 (2000) 253 â€“ 259

in de Luna and Genton (1998), where it was used to robustify the estimation of autoregressive and moving
average (ARMA) models in a time series context. An indirect estimator of Ã‚ is available when it is possible
(and simple) to draw pseudo-observations from F(Ã‚) and if there is a simple auxiliary reparameterization
FÌƒ( ) of F(Ã‚), where is an unknown auxiliary parameter. This auxiliary parameter is chosen such that it
is easier to estimate than Ã‚ (the original idea behind the indirect inference proposal), or easier to estimate
robustly (the studied concept here). The auxiliary parameter is thus estimated with the observed data yielding
Ë†, and with simulated data (at least as many as were observed) from F(Ã‚), yielding âˆ— , a function of Ã‚ by
construction. The indirect estimator is then de ned as
Ã‚Ì‚ = arg min ( Ë† âˆ’
Ã‚

âˆ— 0

)

(Ë†âˆ’

âˆ—

);

(1)

where
can be chosen in order to maximize eciency. Note that the pseudo-observations drawn from F(Ã‚)
are based on the same random generator seed for all Ã‚. For the indirect estimator to be consistent and
asymptotically normal, the binding function h(Ã‚) = needs to be locally injective around the true value of
Ã‚ (Gourieroux et al., 1993). In the latter article, the two estimators Ë† and âˆ— were assumed to be identical,
although this does not need to be the case as long as they are consistent, see de Luna and Genton (1998). For
instance, a robust estimator Ã‚Ì‚ can be obtained with a robust estimator Ë†. On the other hand, âˆ— is evaluated
on outlier-free simulated data and thus is most conveniently chosen to be ecient under the uncontaminated
model.
Indirect inference was originally introduced and used by econometricians faced with complex dynamic
models for which more conventional inferential methods, typically based on a likelihood or on moments,
are not applicable because the corresponding estimating criterion is not available neither analytically nor
algorithmically; various such examples are discussed in Gourieroux and Monfort (1996). In the absence of
a tractable analytical estimating criterion, (1) is constructed with the help of a data simulator. An intuitive
motivation for the minimization of criterion (1) is that it corresponds to looking for Ã‚ such that the distance
between âˆ— and Ë† is minimum.
In this paper we study robust indirect estimators in a general framework. For this purpose, we start by
de ning in Section 2 the in uence function, an important analytical tool used to compare robustness properties
of estimators. The in uence function for the indirect estimator is then derived under general conditions,
allowing us to indicate how robust indirect estimation can be obtained. The in uence function of some
indirect estimators is studied in Section 3 for a moving average model, and in Section 4 for an asymmetric
moving average model. Section 5 concludes the paper.
2. In uence function for indirect estimators
The in uence function (Hampel, 1974) is a tool used to describe the robustness properties of an estimator.
Its importance lies in its appealing heuristic interpretation: it measures the asymptotic bias caused by an
in nitesimal contamination of the observations. For a multidimensional statistical functional T at a distribution
F, the in uence function IF(; T; F) at the probability measure  (de ning the contamination process) is de ned
by the limit:
T (F ) âˆ’ T (F)
;
â†’0+


IF(; T; F) = lim

(2)

in those  where this limit exists. As  â†’ 0, the distribution of the contaminated process F â†’ F. Hampelâ€™s
original de nition was aimed at the independent and identically distributed case, with  = x the measure that
puts all its mass at x and F = (1 âˆ’ )F + x . Its extension to the time-series setting has rst been proposed by
Kunsch (1984). Martin and Yohai (1986) consider a di erent contaminated distribution F which they argue
is more realistic because it corresponds to usual contaminated models, e.g. additive or replacement outliers.

M.G. Genton, X. de Luna / Statistics & Probability Letters 48 (2000) 253 â€“ 259

255

The general in uence function de ned by (2) can be derived for the indirect estimator Ã‚Ì‚ from the in uence
function of the auxiliary estimator Ë†. They are proportional so that the boundedness of the latter implies the
boundedness of the former.
Denoted by A and T the statistical functionals corresponding, respectively, to the estimators Ë† and Ã‚Ì‚
de ned in the previous section. Thus, A is such that Ë† = A(Fn ) for any n and Fn (or Ë† tends in probability
towards A(F)), where Fn is the empirical distribution of the sample, and F the generating distribution function
(Hampel et al., 1986, p. 82). In the sequel Fisher consistency is assumed, i.e. A(F(Ã‚)) = h(Ã‚), for all Ã‚ âˆˆ .
The functional T is characterized more precisely in the proof below. Let IFaux (; A; F) be the vector in uence
function of the auxiliary estimator Ë†, and IFind (; T; F) be the vector in uence function of the indirect estimator
Ã‚Ì‚. We have the following result.
Theorem 1. Let Ã‚Ì‚ be the indirect estimator (1) based on Ë† and âˆ— ; the latter a consistent estimator
of h(Ã‚) for all Ã‚ âˆˆ . Assume further that at T (F); h is locally injective and the Jacobian matrix D of h
exists. If the in uence function of the auxiliary estimator exists; then the in uence function of the indirect
estimator is
IFind (; T; F) = P(T (F)) IFaux (; A; F);
âˆ’1

where P(T (F)) = [D(T (F))0 D(T (F))]

(3)

D(T (F))0 .

Proof. We need rst to characterize the functional T . Because âˆ— is consistent, i.e. it converges towards h(Ã‚)
for all Ã‚ âˆˆ , either in probability or with probability one, the criterion minimized in (1) converges uniformly
in probability towards
(A(F) âˆ’ h(Ã‚))0 (A(F) âˆ’ h(Ã‚)):

(4)

Therefore, Ã‚Ì‚ converges in probability towards the argument of the minimum of (4). This argument is thus
T (F). The score corresponding to (4) is equal to zero at Ã‚ = T (F):
D(T (F))0 (A(F) âˆ’ h(T (F))) = 0;

(5)

where D(Â·) is the Jacobian matrix which is of full-column rank by the injectivity assumption. Finally, replacing
F by F in (5), di erentiating with respect to  and evaluating at  = 0 yields the theoremâ€™s result.
Therefore, IFind is bounded if IFaux is bounded, and in particular, we can replace IFaux by either Hampelâ€™s
(1974) in uence function, or Martin and Yohaiâ€™s (1986) in uence function. The former can usually be used
to compute the asymptotic variance as
Z
(6)
V (T; F) = IF(x; T; F)IF(x; T; F)0 dF(x):
We have the following relationship between V (T; F)â€™s:
Vind (T; F) = P(T (F))Vaux (A; F)P(T (F))0 :

(7)

Note, however, that Vind (T; F) does not describe the whole asymptotic variance of the indirect estimator,
because it does not contain the variability due to the simulations. The complete expression can be found in
de Luna and Genton (1998).
The worst e ect of a contamination on the estimator is described by the gross-error sensitivity âˆ— (T; F) =
sup {kIF(; T; F)k}, where kÂ·k denotes the Euclidean norm. If the gross-error sensitivity is nite, the estimator
is said to be B-robust, i.e. robust with respect to the bias. It is often of interest to standardize the gross-error
sensitivity (Hampel et al., 1986) in order to make it invariant under one-to-one parameter transformations.

256

M.G. Genton, X. de Luna / Statistics & Probability Letters 48 (2000) 253 â€“ 259

The self-standardized sensitivity is de ned in the metric given by the asymptotic variance, i.e. âˆ—s (T; F) =
sup {IF(; T; F)0 V (T; F)âˆ’1 IF(; T; F)}1=2 , whereas the information-standardized sensitivity is de ned in the
local metric given by the Fisher information J (T (F)), i.e. âˆ—i (T; F) = sup {IF(; T; F)0 J (T (F))IF(; T; F)}1=2 .
He and Simpson (1992) give arguments for such standardizations. Here, we have the following result.
Corollary 1.1. Under the hypothesis of Theorem 1; if Ë† is B-robust; then Ã‚Ì‚ is also B-robust. Moreover; if
dim( ) = dim(Ã‚); then âˆ—s; ind (T; F) = âˆ—s; aux (A; F) and âˆ—i; ind (T; F) = âˆ—i; aux (A; F).
The breakdown point is another important feature of reliability of an estimator (Huber, 1981, 1984; Hampel
et al., 1986). It indicates how many data points need to be replaced by arbitrary values to destroy the
estimator, i.e. to bring the estimator to the boundaries of the parameter space. Note that this concept can
also be applied to a dependent data setting; see, e.g., Lucas (1997) and Ma and Genton (2000) for a time
series context, as well as Genton (1998) for a spatial random eld situation. The indirect estimator of Ã‚ will
inherit the breakdown point of the auxiliary estimator of , because the simulated data are outlier-free. In fact,
one can expect that other robustness properties of the auxiliary estimator will be transmitted to the indirect
estimator of Ã‚.
3. Example I: rst-order moving average model
In de Luna and Genton (1998) it was proposed to use linear predictors as auxiliary parameterization to
robustify the inference of autoregressive and moving average models. Here we study the in uence function
for the simple situation of a moving average model of order one, i.e.
Xt = tâˆ’1 + t ;

(8)

where the sequence {t } is independently and identically distributed with mean zero and variance 2 . The
model is identi able when || Â¡ 1. The indirect estimation of this model can be based on the auxiliary
parameterization (Gourieroux et al., 1993; de Luna and Genton, 1998)
Xt = Xtâˆ’1 + Ut ;

(9)

where is such that E(Ut Xtâˆ’1 ) = 0, i.e. Xtâˆ’1 is the best linear predictor of order 1 for Xt . Higher order
predictors, i.e. 1 Xtâˆ’1 + Â· Â· Â· + r Xtâˆ’r may be used, although here for simplicity we focus on the use of (9),
further assuming that 2 is known.
In order to compute the in uence function of the indirect estimator of  based on (9), we need the
binding function, which here is h() = =(1 +  2 ) and the in uence function of . The latter is not uniquely
de ned. In the sequel, we follow Martin and Yohai (1986) and work with the in uence curve IC() obtained
by using a degenerate measure for  in Eq. (2), where  represents the value of the contamination. For
GM-estimators of , the in uence curve IC () is given in Theorem 5:2 in Martin and Yohai (1986), for
additive outliers. We consider Hampelâ€“Kraskerâ€“Welsch type of GM-estimators, with Bisquare and Huber
-functions, where the tuning constants are respectively 9.36 and 2.52, in order to reach 95% eciency at
the standard normal distribution. As a particular case, the least squares (LS) estimator of has an explicit
in uence curve IC () = âˆ’ (1 âˆ’ 2 )2 . The matrix P in Theorem 1 is a scalar in our simple example and
equals P() = (1 +  2 )2 =(1 âˆ’  2 ). Therefore, the in uence curve for the indirect estimator of , based on a
least squares estimator of , is:
( 4 +  2 + 1) 2
:
(10)
4 âˆ’1
In Fig. 1, the in uence curves IC () of the indirect estimators of  based on the two GM-estimators with
Bisquare (dashed curve) and Huber (dotted curve) -functions, as well as on the least squares estimators
IC () =

M.G. Genton, X. de Luna / Statistics & Probability Letters 48 (2000) 253 â€“ 259

257

Fig. 1. In uence curves IC () of the indirect estimator for (a)  = 0:5, (b)  = 0:9, and in uence curve IC () of the auxiliary estimator
for (c) = 0:4 = h(0:5). The scalar matrix P() is depicted in (d).

(solid curve), are depicted for  = 0:5 and 0.9. As expected, the in uence curves for the two GM-estimators
are bounded, whereas the one for the least squares estimator is not. Figs. 1(a) and (b) may directly be
compared to Figs. 3(a) and (b) in Martin and Yohai (1986) where in uence curves for direct estimators
are displayed. Thus, we note that the least squares in uence curves are identical. On the other hand, the
indirect estimators of  have a smaller absolute size in in uence curve than the corresponding Martin and
Yohaiâ€™s estimators. Moreover, the latter when based on a Huber-type GM-criterion, are not B-robust while
the indirect estimators are. Fig. 1(c) describes the in uence curve IC () of the auxiliary estimator of , for
= 0:4 = h(0:5). Comparing Figs. 1(c) and (a), we can see that B-robustness is preserved through the indirect
inference procedure, although the unstandardized gross-error sensitivity becomes larger. The factor in ating
the in uence curve of the auxiliary estimator is the scalar matrix P(), depicted in Fig. 1(d).
4. Example II: rst-order asymmetric moving average model
In this second example we look at a non-linear moving average model introduced by Wecker (1981). The
asymmetric moving average model of order one is of the form
âˆ’
+
+  âˆ’ tâˆ’1
+ t ;
Xt =  + tâˆ’1

(11)

where t+ = max(0; t ), tâˆ’ = min(0; t ), and the sequence {t } is independently and identically distributed with
mean zero and variance 2 . The model is identi able for | + |; | âˆ’ | Â¡ 1 (Wecker, 1981). Note also that when
 + =  âˆ’ we obtain the linear moving average model (8). Indirect estimation of this model was proposed and
studied in Brannas and de Luna (1998), based on auxiliary parameterizations de ned by best linear predictors.

258

M.G. Genton, X. de Luna / Statistics & Probability Letters 48 (2000) 253 â€“ 259

Fig. 2. Four components of the matrix P(Ã‚) for  + = 0:5 (a) and  + = 0:9 (b). On the x-axes we have âˆ’1 Â¡  âˆ’ Â¡ 1.

On the contrary of the previous example, the stochastic process de ned by (11) may have a mean di erent
from zero, and the auxiliary parameterization takes the form
(Xt âˆ’

0)

=

1 (Xtâˆ’1 âˆ’

0)

+ Ut ;

(12)

where = ( 0 ; 1 )0 is such that E(Ut (1; Xtâˆ’1 )0 ) = 0, i.e. 1 (Xtâˆ’1 âˆ’ 0 ) is the best linear predictor of order
one for (Xt âˆ’ 0 ). Again higher-order linear predictors can be used. Brannas and de Luna (1998) found
this reparameterization to yield inecient indirect estimators when  + âˆ’  âˆ’ was far from zero. This is not
surprising because then linear predictors are not optimal in a mean squared prediction error sense. On the
other hand, a Wald test of the null hypothesis  + âˆ’  âˆ’ = 0 based on the indirect estimator was shown to
have good power properties.
For the indirect estimator of Ã‚ = ( + ;  âˆ’ )0 based on (12), the binding function is given by (Wecker, 1981)
0

=

+ âˆ’âˆ’
âˆš
;
2

1

=

+ +âˆ’
;
2

=1+

1 +
( + )2 + ( âˆ’ )2
âˆ’
( âˆ’  âˆ’ )2 ;
2
2

where
is the variance of {Xt }. We have set 2 = 1. The Jacobian matrix D(Ã‚) and the matrix P(Ã‚),
can be deduced from these formulae. In Fig. 2 the four components of P are plotted for  + = 0:5, 0.9
and âˆ’1 Â¡  âˆ’ Â¡ 1. We can see that these matrices keep bounded, although, for  + = 0:9 and values of
 âˆ’ approaching one, the multiplicative factor may be quite large. This situation is similar to the case of a
symmetric moving average model with coecient close to one, and indeed in Fig. 1(d) we observe a similar
phenomenon when this coecient is approaching one.
Finally, in uence functions could be obtained as in Section 3, once P(Ã‚) is known. For instance, robustness
is achieved by using GM-estimators for (12).

M.G. Genton, X. de Luna / Statistics & Probability Letters 48 (2000) 253 â€“ 259

259

5. Discussion
In the examples of Sections 3 and 4, the binding function h(Â·) and its Jacobian could be computed
analytically. This, however, does not need to be the case to perform the type of analysis presented. An
example where the function h(Â·) is not possible to retrieve analytically arises when using an indirect estimator
for spatial autoregressive models as it was proposed in de Luna and Genton (1999). The link function can be
estimated with simulated data, indeed hÌ‚(Ã‚) = âˆ— is a consistent estimator of h(Ã‚). The Jacobian can then be
approximated numerically, for instance for the one-dimensional parameter case by {hÌ‚( + =2) âˆ’ hÌ‚( âˆ’ =2)}=,
with  small.
The methodology introduced in this article allows us to perform robust inference for models where classical
methods such as M-type estimators are not directly implementable. This is achieved by robustifying the indirect
estimator of Gourieroux et al. (1993). Furthermore, previous situations for which indirect inference has proven
useful may take advantage of the robust estimator herein proposed. Such instances include inference for
spatial processes (de Luna and Genton, 1999), stochastic di erential equation models (e.g. models for option
pricing and interest rate modeling), latent variable models (e.g. stochastic volatility models), and complex
macroeconomics models (non-linear simultaneous equations), see Gourieroux and Monfort (1996).
Acknowledgements
The authors thank an anonymous referee for valuable comments.
References
Brannas, K., de Luna, X., 1998. Generalized method of moment and indirect estimation of the ARASMA model. Comput. Statist.
13, 485â€“494.
de Luna, X., Genton, M.G., 1998. Simulation-based robust estimation of ARMA models. Research Report, 198, Dept. of Statistical
Science, University College London.
de Luna, X., Genton, M.G., 1999. Indirect inference for spatio-temporal autoregression models. Proceedings of Spatial-temporal modeling
and its application, Leeds, UK, 61â€“ 64.
Genton, M.G., 1998. Spatial breakdown point of variogram estimators. Math. Geol. 30, 853â€“871.
Gourieroux, C., Monfort, A., Renault, A.E., 1993. Indirect inference. J. Appl. Econometrics 8, 85â€“118.
Gourieroux, C., Monfort, A., 1996. Simulation-Based Econometric Methods. Oxford University Press, Oxford.
Hampel, F.R., 1974. The in uence curve and its role in robust estimation. J. Amer. Statist. Assoc. 69, 383â€“393.
Hampel, F.R., Ronchetti, E.M., Rousseeuw, P.J., Stahel, W.A., 1986. Robust Statistics: The Approach based on In uence Functions.
Wiley, New York.
He, X., Simpson, D.G., 1992. Robust direction estimation. Ann. Statist. 20, 351â€“369.
Huber, P.J., 1981. Robust Statistics. Wiley, New York.
Huber, P.J., 1984. Finite sample breakdown point of M- and P-estimators. Ann. Statist. 12, 119â€“126.
Kunsch, H., 1984. In nitesimal robustness for autoregressive processes. Ann. Statist. 12, 843â€“863.
Lucas, A., 1997. Asymptotic robustness of least median of squares for autoregressions with additive outliers. Comm. Statist. Therory
Methods 26, 2363â€“2380.
Ma, Y., Genton, M.G., 2000. Highly robust estimation of the autocovariance function. J. Time Ser. Anal. (to appear).
Martin, R.D., Yohai, V.J., 1986. In uence functionals for time series (with Discussion). Ann. Statist. 14, 781â€“855.
Wecker, W.E., 1981. Asymmetric time series. J. Amer. Statist. Assoc. 76, 16â€“21.

