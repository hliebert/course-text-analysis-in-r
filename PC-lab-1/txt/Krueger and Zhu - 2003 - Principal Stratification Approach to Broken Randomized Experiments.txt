Principal Stratification Approach to Broken Randomized Experiments: A Case Study of
School Choice Vouchers in New York City [with Comment]
Author(s): Alan B. Krueger and Pei Zhu
Source: Journal of the American Statistical Association, Vol. 98, No. 462 (Jun., 2003), pp.
314-318
Published by: Taylor & Francis, Ltd. on behalf of the American Statistical Association
Stable URL: https://www.jstor.org/stable/30045240
Accessed: 27-02-2019 21:30 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

American Statistical Association, Taylor & Francis, Ltd. are collaborating with JSTOR to
digitize, preserve and extend access to Journal of the American Statistical Association

This content downloaded from 206.253.207.235 on Wed, 27 Feb 2019 21:30:16 UTC
All use subject to https://about.jstor.org/terms

314 Journal of the American Statistical Association, June 2003

Brown, C. H.,
and Liao,
J. (1999), "Principles
for Designing
Randomi
question of whether school choice
will
produce
better
achieve

ventive Trials in Mental Health: An Emerging Developmental Epidemi
ment outcomes for children in
an urban public school system
Perspective," American Journal of Community Psychology, 27, 673-7
The randomized lottery provides
an exceptionally
powerful
Jo, B. (2002a),
"Model Misspecification Sensitivity
Analysis in to
Estim

Causal
Effects of Interventions With
Noncompliance,"
Statistics
in Me
for examining the impact of a
program-far
more
useful
than

21, 3161-3181.

observational studies that have causal change intertwined hop

(2002b), "Sensitivity of Causal Effects Under Ignorable an

lessly with self-selection factors.
Statisticians
areavailable
just
now in
tent Ignorable
Missing-Data Mechanisms,"
at www.statmodel

vestigating variations in such mplus/examples/jo/.
principal strata analyses, that i

(2002c), "Estimation of Intervention Effects With Noncomplianc
those involving latent classes formed
as a function of random

ternative Model Specifications" (with discussion), Journal of Educ

ized trials involving intervention
invitations
(such as vouchand Behavioral
Statistics, 27, 385-420.
(2002a), "Beyond
SEM: designs
General Latent Variable
Modeling
ers), encouragement designs,Muthen,
andB. field
trial
involvin
haviormetrika, 29, 81-117.
more than one randomization
(Brown and Liao 1999). The la-

Muthen, B., and Brown, C. H. (2001), "Non-Ignorable Missing Data in

tent categories in this article,
labels
"complier,
eral which
Latent VariableBFHR
Modeling Framework,"
presented
at the annual m
of and
the Society
for Prevention Research,
Washington, DC,
June 2001.
"never-taker," "always-taker,"
"defier,"
represent
only
on

Brown, C. H., Masyn, K., Jo, B., Khoo, S. T., Yang, C. C
type of design. Other terms Muth6n,
mayB.,
be
more relevant to the sci

C. P., Kellam, S., Carlin, J., and Liao, J. (2002), "General Growth M

entific questions underlying Modeling
trials
which
subjects
are
ran
for in
Randomized
Preventive
Interventions,"
Biostatistics,
domly assigned to different 475.
levels of invitation (e.g., Angri
Muthen, L., and Muth6n, B. (1998-2002), Mplus User's Guide, Los Angeles:
and Imbens 1995), or different
levels of implementation. Such
authors.
trials not only have great potential
for
examining
questions
Muthen, B., and Shedden,
K. (1999),
"Finite Mixture Modeling
With Mixture
Outcomes
Using the
EM Algorithm," Biometrics,
55, 463-446.
of effectiveness, sustainability,
and
scalability,
but
also requir
Seltzer, M. H., Frank, K. than
A., and Bryk,
A. S. (1993), "The Metric Matterms more consistent with adherence
compliance.
Again
ters: The Sensitivity of Conclusions About Growth in Student Achieve-

we congratulate the authors on
important
addition
to
ment toan
Choice of
Metric," Educational Evaluation
and Policy Analysis,
16, th
methodological literature that41-49.
we predict will have lasting im
Shadish, W. R., Cook, T. D., and Campbell, D. T. (2002), Experimental
pact.

and Quasi-Experimental Designs for Generalized Causal Inference, Boston

Houghton Mifflin.

Slavin, R. E. (2002), "Evidence-Based Education Policies: Transforming Edu
cational Practice and Research," Educational Researcher, 31, 15-21.
Angrist, J. D., and Imbens, G. W. (1995), "Two-Stage Least Squares
EstimaWest, S.
G., and Sagarin, B. J. (2000), "Participant Selection and Loss in Ran

ADDITIONAL REFERENCES

tion of Average Causal Effects in Models With Variable Treatment
Intensity,"
domized
Experiments," in Research Design, ed. L. Bickman, Thousand Oaks,
Journal of the American Statistical Association, 90, 431-442.
CA: Sage, pp. 117-154.

Comment
Alan B. KRUEGER and Pei ZHU

In our comment, we use the more comprehensive sample beThe New York City school voucher experiment provides
results for this sample are more informative, but we highsome of the strongest evidence on the effect of private cause
school
light
where
differences arise from using the sample analyzed by
vouchers on student test achievement yet available. Barnard
Barnard
et al. provide a useful reevaluation of the experiment, and
someet al.
Three
of the authors were integrally involved in the design of the
ex- themes emerge from our analysis. First, simplicity and

transparency
are under appreciated virtues in statistical analyperiment. We will leave it to other discussants to comment
on
sis.
Second,
it
is desirable to use the most recent, comprehenthe Bayesian methodological advances in their paper, and instead comment on the substantive lessons that can be learned

sive data, for the widest sample possible. Third, there is no sub-

stitute for probing the definitions and concepts that underlie the
from the experiment, and the practical lessons that emerge from

the novel design of the experiment.

data.

We have the advantage of having access to more complete

1. RANDOM ASSIGNMENT
data than Barnard et al. used in preparing their paper. This inAs Barnard et al. explain, the voucher experiment entaile
cludes more complete baseline test information, data on multi-

a complicated block design, and different random assignm
child families as well as single-child families, and three years

procedures were used in the first and second set of blocks. In t
of follow-up test data instead of just one year of follow-up data.

first block, a propensity matched-pairs design (PMPD) meth

was used. In this block, far more potential control families w

available
than was money to follow them up. Rather than s
Alan B. Krueger is Professor and Pei Zhu is graduate student,
Ecolect
a random sample of controls for follow up after random
nomics Department, Princeton University, Princeton, NJ 08544
(E-mail:
akrueger@princeton.edu). In part, this comment extends and summarizes results of Krueger and Zhu (2003). Readers are invited to read that article for a
more in-depth analysis of many of the issues raised in this comment. Some of
the results presented here differ slightly from those in our earlier article, however, because the definition of family size in this comment corresponds to the
June
one used by Barnard et al. rather than to the definition in our earlier article.

@ 2003 American Statistical Association
Journal of the American Statistical Association

2003, Vol. 98, No. 462, Applications and Case Studi

This content downloaded from 206.253.207.235 on Wed, 27 Feb 2019 21:30:16 UTC
All use subject to https://about.jstor.org/terms

DOI 10.1198/016214503000099

Krueger
Table

1.

and

Zhu:

Comment

Efficiency

315

Comparison

Random

of

Tw

Assignment

Relative Sample-size-

Propensity score Randomized blocks sampling adjusted relative
match subsample subsample sampling adjusted
relative
variance sampling variance
Year Model Coefficient Number of observations Coefficient Number of observations (RB/PMPD) (RB/PMPD)
All students

Year

1

Control

Omit
Year

3

for

baseline

.33

721

(1.40) (1.38)
baseline .32 1,048 -1.02
(1.40) (1.58)

Control

for

baseline

(1.56)
Omit

baseline

1.02

2.10

734

1,032

613

.67

.972

1.274

637

.989

1.254

1.244

1.293

(1.74)

-.31

(1.57)

900

-.45

901

1.285

302

8.10

321

3.19

447

1.082

1.287

(1.78)

African-American students

Year

1

Control

for

baseline

(1.88)
Omit

baseline

2.10

(1.99)
Year

3

Control

for

NOTE:
the

as

Standard

relative

and

sampling

non-Hispanic,

selecting

1.109

4.21

247

6.57

272

.693

.764

347

3.05

386

.875

.973

(2.31)

errors

dummies,

.879

(2.09)

2.26

(2.47)
strata

436

baseline

baseline

.827

(2.07)

(2.51)
Omit

.32

(1.71)

in

are

in

parentheses.

indicated

variances

of

models
the

Treatment

baseline

treatment

test

effec

scores

effects

for

Black/African-American.

treatments,
the
research
matching
should
lea
to subgroups
align controls
with
t
in the
tre
used a nearest-available-neighbor
Mah
covariate balance
is
specific paired control
families
for
fol
PMPD
than in
the
r
for the differences
between treatments and controls
for the basePMPD design should
improve
the
prec
by reducing the chance
imbalances
line covariates. One possibility
is that because Black students th
randomized block could
design.
A standard
have been matched to non-Black
students in the PMPD
was used in the remaining
four
blocks.
design, this subsample was actually less balanced
than the subBarnard et al. emphasize
that
the
PMP
sample of Blacks from the more conventional randomized
block
anced for 15 of the
21
baseline
variab
design.
But the key question
is
the
to
The increase in
power from
the PMPD extent
design is relatively
the estimates was improved, not the c
modest, even in the full sample. Of much more practical imboth blocks yield unbiased estimates.
portance is the fact that the complexity of the PMPD derelatively little of the outcome variable
sign caused Mathematica to initially calculate incorrect basevery little. In Table 1 we provide stand
line sample weights. The baseline weights were designed to
estimates-that is, the difference in m
make the follow-up sample representative of all eligible aptreatments and controls, conditional on
plicants for vouchers. The initial weights assigned much too
dom assignment (family size by block
little importance to the control sample in the PMPD block. Beschool)-and, more importantly, their
cause these families represented many unselected controls, they
rately for the PMPD subsample and ran
should have been weighted heavily. The revised weights inple. The outcome variable is the avera
creased the weight on the PMPD controls by 620%. In contrast,
centile rank on the math and reading s
the weight increased by just 18% for the rest of the sample.
for Basic Skills, taken either 1 year or
error hadresults
grave consequences for theare
initial infer-prese
signment. Two setsThisof
ences
and
analyses
of
the
data.
First,
using
the
faulty
for baseline test scores for the weights,
subsam
Peterson, Myers,
and Howell (1998) reported highly statistiand the other without
controlling
for sc
cally significant
differences in baseline test
scores between
the
ple. The last column
adjusts
the
relativ
sity

score

the

model

treatment and control
groups, withbetween
control group members scordifferences in sample
size
the
ing higher on both theA),
math and reading
exams. After
the misFor all students (panel
the
standar
take was discovered
and the weights were revised,
however, the B
smaller in the PMPD
sample.
For

the baseline differences
became small and statistically
the standard
errors
are insignifessenti
icant. (We
note, however, that if we limit
attention to the findin
stularger in the PMPD
sample.
This
dents from single-children (2000)
families that Barnard et al.
use, and
Hill, Rubin, and Thomas
predict
groups would have
use themore
more complete baselinepower
test score data, then there
in
is
the

however,

This content downloaded from 206.253.207.235 on Wed, 27 Feb 2019 21:30:16 UTC
All use subject to https://about.jstor.org/terms

316 Journal of the American Statistical Association, June 2003

2. INTENT-TO-TREAT
ESTIMATION
RESULTS
a statistically significant difference
in baseline
reading
score
between treatments and controls, even with the revised ba
Barnard et al. devote much attention to addressing poten
line weights. If we pool together students from single-child a
problems caused by missing data for the sample of studen
multichild families, then the baseline difference is insignifican
grades 1-4 at baseline; this follows the practice of earlier
This is another reason why we think it is more appropriate
ports by Mathematica, which restricted the analysis of stu
use the broader sample that includes children from all familie
outcomes (but not parental responses such as satisfaction
Second, because of the inaccurate inference that there was
those enrolled in grades 1-4 at baseline. In our opinion, a m
difference in baseline abilitymore
between
treatments
and
contro
important substantive
problem results
from
the excl

Mathematica's researchers were
discouraged
from
analyzing
of students
in the kindergarten
cohort, who
were categori
(or at least from presenting)dropped
results
that
did
not
condition
o
because they were not given baseline tests. Becau
baseline scores. This is unfortunate,
because
conditioning
on
assignment to treatment status was random (within stra
baseline scores caused the researchers
drop
from
theand
sam
simple comparisonto
of means
between
treatments
con
ple all the students who were
initially
in
kindergarten
(becau
without conditioning on baseline scores provides an unbi
these students were not given
baseline
tests)
and
11%
ofas st
estimate
of the average
treatment
effect.
Moreover,
Bar
dents initially in grades 1-4.et Including
students
with
missin
al. and others show, treatments and controls were well
baseline scores increases the anced
sample
more
than 40%,
in terms by
of baseline
characteristics,
so there isand
no re
expands the population to which
the
can was
besomehow
generaliz
to suspect
that results
random assignment
corrupte

As explained later and in an article
by regression
Krueger
and
Zhu
(200
Table 2 presents
estimates
of the
ITT effect
u

qualitatively different results various
are found
if students
with
samples. Because
we cannot replicate
the missi
Bayesian

timates
without
knowing the
propensity score,
we present
baseline scores are included in
the
sample.
Because
assignme

ventional
ITT estimates.
For each
entry in the
first colum
to receive a voucher was random
within
lottery
strata,
estimate
that do not condition on baseline scores are nonetheless unbi-

we regressed the test score percentile rank on a voucher o

dummy, 30 dummies indicating lottery strata (block x fam
ased. Moreover, it is inefficient to exclude students with missing

baseline scores (most of whom had follow-up test scores),size
andx high/low school), and baseline test scores. The samp
limited to those with baseline test data. Our results differ from
such a sample restriction can potentially cause sample selection
the ITT estimates in Table 5 of Barnard et al., because we use

bias.

Of lesser importance is the fact that the PMPD design also the revised weights and a more comprehensive sample that also
complicates the calculation of standard errors. The matching ofincludes students from multichild families. Barnard et al. report
treatments and controls on selected covariates creates a depen-that their Bayesian estimates are "generally more stable" than
the conventional ITT estimates, "which in some cases are not
dence between paired observations. Moreover, if the propensity
score model is misspecified, then the equation error also cre-even credible," but the extreme ITT estimates that they refer to
ates a dependence between observations. Researchers have notfor 4th graders from high-achieving schools are based on only
taken this dependence into account in the calculation of stan-15 students. For the samples that pool students across grades,
the results of the conventional ITT estimates are a priori credidard errors. We suspect, however, that this is likely to cause
ble and probably not statistically different from their Bayesian
only a small understatement of the standard errors, because the
covariates do not account for much of the residual variance of

estimates.

In the second column we use the same sample as in column 1,
test scores, and because the PMPD sample is only about half of
but omit the baseline test score from the model. In the third

the overall sample. (We tried to compute the propensity score
column we expand the sample to include those with missing
to adjust the standard errors ourselves, but were unable to replibaseline scores. In the fourth column we continue to include

cate the PMPD model because it was not described in suffi-

students with missing baseline scores, but restrict the sample to
cient detail and because the computer programs are proprietary.
those initially in low-achieving public schools.
This also prevented us from replicating estimates in Table 4 of
We focus mainly on the results for Black students and stuBarnard et al.)
dents from low-achieving public schools, because the effect of
The problems created by the complicated experimental deoffering a voucher on either reading or math scores for the oversign, particularly concerning the weights, lead us to reiterate
all sample is always statistically insignificant, and because most
the advice of Cochran and Cox (1957): "A good working rule is
public policy attention has focused on these two groups.
to use the simplest experimental design that meets the needs
of
Although
it has not been made explicit in previous studies of

the occasion." It seems to us that in this case, the PMPDthese
design
data, Black students have been defined as children with
introduced unnecessary complexity that inadvertently led
to a
a non-Hispanic,
Black/African-American mother. We use this
consequential mistake in the computation of weights, with
very
definition
and a broader one to probe the sensitivity of the relittle improvement in efficiency. This was not the fault ofsults.
the ar-

chitects of the PMPD design, who did not compute the weights,
Omitting baseline scores has little qualitative effect on the
but it was related to the complexity of the design. Simplicity
estimates when the same sample is used (compare columns 1
and transparency-in designs, methods, and procedures-can
and 2); the coefficient typically changes by no more than half
help avoid mistakes down the road, which are almost inevitable
a standard error. For year 3 scores for Black students, for exin large-scale empirical studies. Indeed, Mathematica recently
ample, the treatment effect on the composite score is 5.2 points
informed us that they still do not have the baseline weights
ex(t = 3.2)
when controlling for baseline scores and 5.0 points
actly correct, 5 years after random assignment.
(t = 2.6) when not controlling. This stability is expected in a

This content downloaded from 206.253.207.235 on Wed, 27 Feb 2019 21:30:16 UTC
All use subject to https://about.jstor.org/terms

Krueger
Table

2.

and

Zhu:

Estimated

Comment
Treatment

Regression

for

317

Effects

Various

Subsample Subsample Subsample
with baseline with baseline applicant school:
scores controls for scores omits Full sample Low omits
Test Sample baseline scores baseline scores omits baseline scores baseline scores
First follow-up test

Composite

Overall 1.27 .42 -.33 -.60
(.96) (1.28) (1.06) (1.06)
Mother Black, Non-Hispanic 4.31 3.64 2.66 1.75
(1.28) (1.73) (1.42) (1.57)
Either parent Black, Non-Hispanic 3.55 2.52 1.38 .53
(1.24) (1.73) (1.42) (1.55)

Reading

1.11 .03 -1.13 -1.17
(1.04) (1.36) (1.17) (1.18)
Mother Black, Non-Hispanic 3.42 2.60 1.38 .93
(1.59) (1.98) (1.74) (1.84)
Either parent Black, Non-Hispanic 2.68 1.49 -.09 -.53
(1.50) (1.97) (1.71) (1.81)

Math

Overall

Overall

1.44

.80

.47

-.03

(1.17) (1.45) (1.17) (1.15)
Mother Black, Non-Hispanic 5.28 4.81 4.01 2.68
(1.52) (1.93) (1.54) (1.63)
Either parent Black, Non-Hispanic 4.42 3.54 2.86 1.59
(1.48) (1.90) (1.51) (1.63)
Third

follow-up

Composite

test

Overall .90 .36 -.38 .10
(1.14) (1.40) (1.19) (1.16)
Mother Black, Non-Hispanic 5.24 5.03 2.65 3.09
(1.63) (1.92) (1.68) (1.67)
Either parent Black, Non-Hispanic 4.70 4.27 1.87 1.90
(1.60) (1.92) (1.68) (1.67)

Reading

Overall .25 -.42 -1.00 -.08
(1.26) (1.51) (1.25) (1.24)
Mother Black, Non-Hispanic 3.64 3.22 1.25 2.06
(1.87) (2.19) (1.83) (1.83)
Either parent Black, Non-Hispanic 3.37 2.75 .76 1.23
(1.86) (2.18) (1.83) (1.82)

Math

Overall

1.54

1.15

.24

.29

(1.30) (1.53) (1.33) (1.28)
Mother Black, Non-Hispanic 6.84 6.84 4.05 4.13
(1.86) (2.09) (1.86) (1.85)
Either parent Black, Non-Hispanic 6.04 5.78 2.97 2.58
(1.83) (2.09) (1.85) (1.82)
NOTE:

Standard

dummies;
Bootstrap

errors

are

in

parentheses.

Dependent

variable

is

test

sco

models in the first column also control for baseline math and
standard errors are robust to correlation in residuals among st

Sample

sizes

in

year

1

for

subsample

with

baseline

scores/or

without

ar

Sample

sizes

in

year

3

for

subsample

with

baseline

scores/or

without

ar

Sample

sizes

in

year

1

for

subsample

from

low

schools

without

baseline

Sample

sizes

in

year

3

for

subsample

from

low

schools

without

baseline

randomized experiment.
inferred
When from
the sample
parents'
is e
r
clude those with missing
baseline
scores inone
colu
Black
students-and
the ITT effect falls almost
finition
in half
used
in year
in governm
3 for B
2.65

points

with

a

t

research-would
include those
who have a
Black father
ratio
ofalso
1.58.
The
effect

on
in the sample as
of Black
students. According
to the
1990 Cens
is statistically significant,
Barnard
et
al.
em
race are
was reported
as Black forif
85% of
the children with aco
Bl
tatively similar results
found
baseline
mother's education and
father
family
and a Hispanic mother
income,
(the overwhelming
are non-Blac
inc
sors (see Krueger and portion
Zhu
2003).
of the
sample) in the New York metropolitan region
The classification of Hence
Black
we also present
students
results for a sample
used
in which eith
in
somewhat idiosyncratic.
parent is
Race
listed as "Black,
and
non-Hispanic."
ethnicity
This increases
w
sampleparental
of Black students by survey-contr
about 10%, and the results are e
a single question in the
guidelines for government
weaker for this
surveys-so
sample. For example, the
it
ITT is
estimate
imp
usin
tify Blacks of Hispanic
(of
which
there
the origin
more comprehensive
sample
of Black students
enrolled
ber in New York). In addition,
the
survey
did
grades K-4 at baseline
is 1.87 points
(t = 1.11) on
the thin
about the child's race year
or
ethnicity,
so even
children
follow-up
composite test, although
with this sam

This content downloaded from 206.253.207.235 on Wed, 27 Feb 2019 21:30:16 UTC
All use subject to https://about.jstor.org/terms

318 Journal of the American Statistical Association, June 2003

the effect on math is significant
at question
the that
.10one
level
in in
year
1,
depends on the
is interested
answering.
Barnard et al. have found, and
almost
significant
year
3.
Because
most interest
in the experimentin
for policy
purposes
we conclude that the effect of
the
opportunity
to use
a priva
centers
on the
impact of offering vouchers
on achievement
school voucher on the composite
score for
the
comprehe
not on compelling
students
to usemost
vouchers-we
think the IT
sive sample of Black students estimates,
is insignificantly
different
which reflect inevitable
partial usage offrom
vouchers
although it is possible that there
initially
a 1998;
small
benefic
are mostwas
relevant
(see also Rouse
Angrist
et al. 2003

effect on the math score for Black students.

Moreover, New York had a higher voucher take-up rate than
In the final column we present results for the subsample of the experiments in Dayton and the District of Columbia, so
students originally enrolled in schools with average test scores one could argue that the New York experiment provides an
below the median score in New York City. In each case, the upper bound estimate of the effect of offering vouchers. O
results for this subsample are quite similar to those for the the other hand, if the goal is to use the experiment to estifull sample, and a formal test of the null hypothesis that stu- mate the effect of attending private school on achievement
dents from low- and high-achieving schools benefit equally then such methods as instrumental variables or those used b
from vouchers is never close to rejecting. It also appears veryBarnard et al. are necessary to estimate the parameter of inte
unlikely that the differences between the treatment effects for
est. We consider this of secondary interest in this case, how
applicants from low- and high-achieving schools in Barnard ever.
et al.'s Table 4 are statistically significant either.
ADDITIONAL REFERENCES
3. WHAT WAS BROKEN?
Angrist, J., Bettinger, E., Bloom, E., King, E., and Kremer, M. (2003),

ers for Private Schooling in Colombia: Evidence from a Randomized N
We agree with Barnard et al. that the experiment was broken

Experiment," American Economic Review, 92, 1535-1558.
in the sense that attrition and missing data were common. Previ-

Cochran, W., and Cox, G. (1957), Experimental Designs (2nd ed.), Ne

ous analyses were also strained, if not broken, for their neglect
Wiley.
of the cohort of students originally in kindergarten who were
inA., and Zhu, P. (2003), "Another Look at the New York City
Krueger,
School Voucher Experiment," American Behavioral Scientist, forthcoming.
third grade at the end of the experiment and whose follow-up
Also available from Industrial Relation Section, Princeton University, at

test scores were ignored. Including these students qualitatively
http://www.irs.princeton.edu.
alters the results for Black students. The experiment was
also
Peterson,
P., Myers, D., and Howell, W. (1998), "An Evaluation of the
New York City Scholarships Program: The First Year," Mathematica Policy
broken in the sense that years passed before correct baseline
Research, Inc., available at http://www.ksg.harvard.edu/pepg/pdf/nylrpt.pdf.
weights were computed.
Rouse, C. (1998), "Private School Vouchers And Student Achievement: An
We disagree, however, with the interpretation that the experiEvaluation Of The Milwaukee Parental Choice Program," The Quarterly
Journal
ment was broken because compliance was less than 100%.
Thisof Economics, 13, 553-602.

Comment
Richard A. BERK and Hongquan Xu

1. INTRODUCTION

concretely with reference to the phenomena being studied,

uated empirically whenever possible, and, at a minimum,
The article by Barnard, Frangakis, Hill, and Rubin (hereafter
jected to sensitivity tests.

BFHR) is a virtuoso performance. By applying the NeymanAs battle-tested academics, we can certainly quibble her
Rubin model of causal effects and building on a series
of about
im- some aspects of the article. For example, migh
there
portant works on the estimation and interpretation ofnot
casual
haveefmade sense to construct priors from the educator

fects (e.g., Angrist, Imbens, and Rubin 1996), BFHR manage
to who were responsible for designing the inter
economists

extract a reasonable set of findings for a "broken" randomized
tion? Moreover, might it not have made sense to interpret th

experiment. The article more generally underscores the
imporsults
using a yardstick of treatment effects that are large en
tant difference between estimating relationships in a to
consistent
matter in practical terms? Nonetheless, we doubt that

allobtainwe could have done as well as BFHR did. Moreover, m
manner and interpreting those estimates in causal terms;

ing consistent estimates is only part of the enterprise.
of Finally,
our concerns depend on features of the data that canno

the article emphasizes that assumptions made so that known
proper from
esa distance. We would have had to carefully e
timates may be obtained are not just technical moves
conineof
the
data ourselves. As a result, we focus on why a virt
venience. Rather, they are statements about how theperformance
empirical
was needed to begin with. Why was this trip
world is supposed to work. As such, they need to be essary?
examined

b 2003 American Statistical Association

Journal of the American Statistical Association
Richard A. Berk is Professor and Hongquan Xu is Assistant Professor,
Department of Statistics, University of California, Los Angeles, CA 90095
June 2003, Vol. 98, No. 462, Applications and Case Studies

DOI 10.1198/016214503000107

(E-mail: berk@ stat. ucla.edu).

This content downloaded from 206.253.207.235 on Wed, 27 Feb 2019 21:30:16 UTC
All use subject to https://about.jstor.org/terms

