Review of Economic Studies (2009) 76, 1071–1102
c 2009 The Review of Economic Studies Limited


0034-6527/09/00000000$02.00

DAVID S. LEE
Princeton University and NBER
First version received October 2005; final version accepted August 2008 (Eds.)
This paper empirically assesses the wage effects of the Job Corps program, one of the largest
federally funded job training programs in the U.S. Even with the aid of a randomized experiment, the
impact of a training program on wages is difficult to study because of sample selection, a pervasive problem in applied microeconometric research. Wage rates are only observed for those who are employed,
and employment status itself may be affected by the training program. This paper develops an intuitive
trimming procedure for bounding average treatment effects in the presence of sample selection. In contrast to existing methods, the procedure requires neither exclusion restrictions nor a bounded support
for the outcome of interest. Identification results, estimators, and their asymptotic distribution are presented. The bounds suggest that the program raised wages, consistent with the notion that the Job Corps
raises earnings by increasing human capital, rather than solely through encouraging work. The estimator is generally applicable to typical treatment evaluation problems in which there is nonrandom sample
selection/attrition.

1. INTRODUCTION
For decades, many countries around the world have administered government-sponsored
employment and training programs, designed to help improve the labour market outcomes
of the unemployed or economically disadvantaged.1 To do so, these programs offer a number
of different services, ranging from basic classroom education and vocational training to various
forms of job search assistance. The key question of interest to policymakers is whether or not
these programs are actually effective, sufficiently so to justify the cost to the public. The evaluation of these programs has been the focus of a large substantive and methodological literature in
economics. Indeed, Heckman, LaLonde and Smith (1999) observe that “[f]ew U.S. government
programs have received such intensive scrutiny, and been subject to so many different types of
evaluation methodologies, as governmentally-supplied job training”.
Econometric evaluations of these programs typically focus on their reduced-form impacts
on total earnings, a first-order issue for cost–benefit analysis. Unfortunately, exclusively studying
the effect on total earnings leaves open the question of whether any earnings gains are achieved
through raising individuals’ wage rates (price effects) or hours of work (quantity effects). That
is, a training program may lead to a meaningful increase in human capital, thus raising participants’ wages. Alternatively, the program may have a pure labour supply effect: through career
counselling and encouragement of individuals to enter the labour force, a training program may
simply be raising incomes by increasing the likelihood of employment, without any increase in
wage rates.
1. See Heckman et al. (1999) for figures on expenditures on active labour market programs in OECD countries.
See also Martin (2000).
1071

Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Training, Wages, and Sample
Selection: Estimating Sharp
Bounds on Treatment Effects

1072

REVIEW OF ECONOMIC STUDIES

2. In the 2004 fiscal year, the U.S. Department of Labor’s Employment and Training Administration spent $1·54
billion for the operation of the Job Corps. By comparison, it spent about $893 million on “Adult Employment and
Training Activities” (job search assistance for anyone and job training available to anyone if such training is needed for
obtaining or retaining employment) and about $1·44 billion on “Dislocated Workers Employment and Training Activities”
(employment and training services for unemployment and underemployed workers) (U.S. Department of Labor, 2005a).
3. A summary of services provided and costs can be found in Burghardt, Schochet, McConnell, Johnson, Gritz,
Glazerman, Homrighausen and Jackson (2001).
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

But assessing the impact of training programs on wage rates is not straightforward, due
to the well-known problem of sample selection, which is pervasive in applied microeconometric research. That is, wages are only observed for individuals who are employed. Thus, even
if there is random assignment of the “treatment” of a training program, there may not only
be an effect on wages but also on the probability that a person’s wage will even be observed.
Even a randomized experiment cannot guarantee that treatment and control individuals will be
comparable conditional on being employed. Indeed, standard labour supply theory predicts that
wages will be correlated with the likelihood of employment, resulting in sample selection bias
(Heckman, 1974). This missing data problem is especially relevant for analysing public job training programs, which typically target individuals who have low employment probabilities.
This paper empirically assesses the wage effects of the Job Corps program, one of the largest
federally funded job training programs in the U.S.2 The Job Corps is a comprehensive program
for economically disadvantaged youth aged 16–24 years and is quite intensive: the typical participant will live at a local Job Corps centre, receiving room, board, and health services while
enrolled, for an average of about 8 months. During the stay, the individual can expect to receive
about 1100 hours of vocational and academic instruction, equivalent to about 1 year in high
school. The Job Corps is also expensive, with the average cost at about $14,000 per participant.3
This paper uses data from the National Job Corps Study, a randomized evaluation funded by the
U.S. Department of Labor.
Standard parametric or semiparametric methods for correcting for sample selection require
exclusion restrictions that have little justification in this case. As shown below, the data include
numerous baseline variables, but all those that are found to be related to employment probabilities
(i.e., sample selection) could also potentially have a direct impact on wage rates.
Thus, this paper develops an alternative method, a general procedure for bounding the treatment effects. The method amounts to first identifying the excess number of individuals who are
induced to be selected (employed) because of the treatment and then “trimming” the upper and
lower tails of the outcome (e.g., wage) distribution by this number, yielding a worst-case scenario
bound. The assumptions for identifying the bounds are already assumed in conventional models
for sample selection: (1) the regressor of interest is independent of the errors in the outcome
and selection equations and (2) the selection equation can be written as a standard latent variable binary response model. In the case of an experiment, random assignment ensures that the
first assumption holds. It is proven that the trimming procedure yields the tightest bounds for the
average treatment effect that are consistent with the observed data. No exclusion restrictions are
required, nor is a bounded support for the outcome variable.
√
An estimator for the bounds is introduced and shown to be n consistent and asymptotically normal with an intuitive expression for its asymptotic variance. It not only depends on the
variance of the trimmed outcome variable but also on the trimming threshold, which is an estimated quantile. There is also an added term that accounts for the estimation of which quantile
(the 10th, 11th, 12th, etc. percentile) of the distribution to use as the trimming threshold.
For the analysis of Job Corps, the trimming procedure is instrumental to measuring the
wage effects, producing bounds that are somewhat narrow. For example, at week 90 after random assignment, the estimated interval for the treatment effect is 4·2–4·3%, even when wages

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1073

2. THE NATIONAL JOB CORPS STUDY AND SAMPLE SELECTION
This section describes both the Job Corps program and the data used for the analysis, replicates
the main earnings results of the recently completed randomized evaluation, and illustrates the
nature of the sample selection problem. It is argued below that standard sample selection correction procedures are not appropriate for this context. Also, to provide an initial benchmark, the
approach of Horowitz and Manski (2000a) is used to provide bounds on the Job Corps’ effect
on wages. They are to be compared with the “trimming” bounds presented in Section 5, which
implements the estimator developed in Sections 3 and 4.
2.1. The Job Corps program and the randomized experiment
The U.S. Department of Labor describes the Job Corps program today as “a no-cost education
and vocational training program . . . that helps young people ages 16 through 24 get a better
job, make more money and take control of their lives” (U.S. Department of Labor, 2005b). To
be eligible, an individual must be a legal resident of the U.S., be between the ages of 16 and
24, and come from a low-income household (Schochet, Burghardt and Glazerman, 2001). The
administration of the Job Corps is considered to be somewhat uniform across the 110 local Job
Corps centres in the U.S.
Perhaps the most distinctive feature of the program is that most participants live at the local
Job Corps centre while enrolled. This residential component of the program includes formal social skills training, meals, and a dormitory-style life. During the stay, with the help of counsellors,
the participants develop individualized, self-paced programs, which will consist of a combination of remedial high school education, including consumer and driver education, as well as
vocational training in a number of areas, including clerical work, carpentry, automotive repair,
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

are missing for about 54% of individuals. By the end of the 4-year follow-up period, the interval is still somewhat informative, statistically rejecting effects more negative than −3·7% and
more positive than 11·2%. By comparison, the assumption-free, “worst-case scenario” bounds
proposed by Horowitz and Manski (2000a) produce a lower bound of −75% effect and an upper
bound of 80%.
Adjusting for the reduction in potential work experience likely caused by the program, the
evidence presented here points to a positive causal effect of the program on wage rates. This is
consistent with the view that the Job Corps program represents a human capital investment rather
than a means to improve earnings through raising work effort alone.
The proposed trimming procedure is neither specific to this application nor to randomized
experiments. It will generally be applicable to treatment evaluation problems when outcomes are
missing, a problem that often arises in applied research. Reasons for missing outcomes range
from survey nonresponse (e.g., students not taking tests), to sample attrition (e.g., inability to follow individuals over time), to other structural reasons (e.g., mortality). Generally, this estimator
is well suited for cases where the researcher is uncomfortable imposing exclusion restrictions in
the standard two-equation sample selection model and when the support of the outcome variable
is too wide to yield informative bounds on treatment effects.
This paper is organized as follows. It begins, in Section 2, with a description of the Job Corps
program, the randomized experiment, and the nature of the sample selection problem. After this
initial analysis, the proposed bounding procedure is described in Sections 3 and 4. Section 3
presents the identification results, while Section 4 introduces a consistent and asymptotically
normal estimator of the bounds and discusses inference. Section 5 reports the results from the
empirical analysis of the Job Corps. Section 6 concludes.

1074

REVIEW OF ECONOMIC STUDIES

4. Missing values for each of the baseline variables were imputed with the mean of the variable. The analysis
below uses this imputed data.
5. Although the analysis here abstracts from the nonresponse problem, there is some evidence that it is a secondorder issue, as mentioned in Remark 2 of Subsection 3.1.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

building and apartment maintenance, and health-related work. On average, enrollees can expect
to receive about 440 hours of academic instruction and about 700 hours of vocational training,
over an average of 30 weeks. Centres also provide health services as well as job search assistance
upon the students’ exit from the Job Corps.
In the mid-1990’s, three decades after the creation of Job Corps, the U.S. Department of
Labor funded a randomized evaluation of the program, which was carried about by Mathematica
Policy Resarch, Inc. Persons who applied for the program for the first time between November
1994 and December 1995 and were found to be eligible (80,883 persons) were randomized into
a “program” group and a “control” group. The control group of 5977 individuals was essentially
embargoed from the program for 3 years, while the remaining applicants could enrol in the Job
Corps as usual. Since those who were still eligible after randomization were not compelled to
participate, the differences in outcomes between program and control group members represent
the reduced-form effect of eligibility or the “intent-to-treat” effect. This treatment effect is the
focus of the empirical analysis presented below. Throughout the paper, when I use the phrase
“effect of the program”, I am referring to this reduced-form treatment effect.
Of the program group, 9409 applicants were randomly selected to be followed for data
collection. The research sample of 15,386 individuals was interviewed at random assignment,
and at three subsequent points in time: 12, 30, and 48 months after random assignment. Due
to programmatic reasons, some subpopulations were randomized into the program group with
differing, but known, probabilities. Thus, analysing the data requires the use of the design weights
(the variable DSGN_WGT as described in Schochet, Cao, Glazerman, Grady, Gritz, McConnell,
Johnson and Burghardt, 2003).
This paper uses the public-release data of the National Job Corps Study. Table 1 provides descriptive statistics for the data used in the analysis below. For baseline as well as postassignment
variables, it reports the treatment and control group means, S.D., proportion of the observations
with nonmissing values for the specified variable, as well as the difference in the means and associated S.E. The table shows that the proportion nonmissing and the means for the demographic
variables (the first 12 rows), education and background variables (the next 4 rows), income at
baseline (the next 9 rows), and employment information (the next 6 rows) are quite similar. For
only one of the variables—usual weekly hours on the most recent job at the baseline—is the
difference (0·91 hours) statistically significant. A logit of the treatment indicator on all baseline
characteristics in Table 1 was estimated; the chi-square test of all coefficients equalling zero
yielded a p value of 0·577.4 The overall comparability between the treatment and the control
groups is consistent with successful randomization of the treatment.
It is important to note that the analysis in this paper, abstracts from missing values due
to interview nonresponse and sample attrition over time. Thus, only individuals who had nonmissing values for weekly earnings and weekly hours for every week after the random assignment are used; the estimation sample is thus somewhat smaller (9145 vs. 15,386). It will become
clear below that the trimming procedure could be applied exclusively to the attrition/nonresponse
problem, which is a mechanism for sample selection that is quite distinct from the selection into
employment status. More intensive data collection can solve the attrition/nonresponse problem
but not the problem of sample selection on wages caused by employment. For this reason, the
analysis below focuses exclusively on the latter problem and analyses the data conditional on
individuals having continuously valid earnings and hours data.5

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1075

TABLE 1
Summary statistics, by treatment status, National Job Corps Study
Control

Female
Age at baseline
White, non-Hispanic
Black, non-Hispanic
Hispanic
Other race/ethnicity
Never married
Married
Living together
Separated
Has child
Number of children
Education
Mother’s education
Father’s education
Ever arrested
Household income
<3000
3000–6000
6000–9000
9000–18,000
>18,000
Personal income
<3000
3000–6000
6000–9000
>9000
At baseline
Have job
Months employed, previous year
Had job, previous year
Earnings, previous year
Usual hours/week
Usual weekly earnings
After random assignment
Week 52 weekly hours
Week 104 weekly hours
Week 156 weekly hours
Week 208 weekly hours
Week 52 weekly earnings
Week 104 weekly earnings
Week 156 weekly earnings
Week 208 weekly earnings
Total earnings (4 years)
Number of observations

Program

Difference

Proportion of
nonmissing

Mean

S.D.

Proportion of
nonmissing

Mean

S.D.

Difference

S.E.

1·00
1·00
1·00
1·00
1·00
1·00
0·98
0·98
0·98
0·98
0·99
0·99
0·98
0·81
0·61
0·98

0·458
18·351
0·263
0·000
0·172
0·074
0·916
0·023
0·040
0·021
0·193
0·268
10·105
11·461
11·540
0·249

0·498
2·101
0·440
0·500
0·377
0·262
0·278
0·150
0·197
0·144
0·395
0·640
1·540
2·589
2·789
0·432

1·00
1·00
1·00
1·00
1·00
1·00
0·98
0·98
0·98
0·98
0·99
0·99
0·98
0·82
0·62
0·98

0·452
18·436
0·266
0·493
0·169
0·072
0·917
0·020
0·039
0·024
0·189
0·270
10·114
11·483
11·394
0·249

0·498
2·159
0·442
0·500
0·375
0·258
0·275
0·139
0·193
0·154
0·392
0·650
1·562
2·562
2·853
0·432

−0·006
0·085
0·002
0·003
−0·003
−0·002
0·002
−0·003
−0·002
0·003
−0·004
0·002
0·009
0·022
−0·146
−0·001

0·011
0·045
0·009
0·011
0·008
0·006
0·006
0·003
0·004
0·003
0·008
0·014
0·033
0·061
0·077
0·009

0·65
0·65
0·65
0·65
0·65

0·251
0·208
0·114
0·245
0·182

0·434
0·406
0·317
0·430
0·386

0·63
0·63
0·63
0·63
0·63

0·253
0·206
0·117
0·245
0·179

0·435
0·405
0·321
0·430
0·383

0·002
−0·002
0·003
0·000
−0·003

0·012
0·011
0·008
0·011
0·010

0·92
0·92
0·92
0·92

0·789
0·131
0·046
0·034

0·408
0·337
0·209
0·181

0·92
0·92
0·92
0·92

0·789
0·127
0·053
0·031

0·408
0·334
0·223
0·174

−0·001
−0·003
0·007
−0·003

0·009
0·007
0·005
0·004

0·98
1·00
0·98
0·93
1·00
1·00

0·192
3·530
0·627
2810·482
20·908
102·894

0·394
4·238
0·484
4435·616
20·704
116·465

0·98
1·00
0·98
0·94
1·00
1·00

0·198
3·596
0·635
2906·453
21·816
110·993

0·398
4·249
0·482
6401·328
21·046
350·613

0·006
0·066
0·007
95·971
0·908*
8·099

0·009
0·091
0·010
117·097
0·446
5·093

1·00
1·00
1·00
1·00
1·00
1·00
1·00
1·00
1·00
3599

17·784
21·977
23·881
25·833
103·801
150·407
180·875
200·500
30,007

23·392
26·080
26·151
26·250
159·893
210·241
224·426
230·661
26,894

1·00
1·00
1·00
1·00
1·00
1·00
1·00
1·00
1·00
5546

15·297
22·645
25·879
27·786
91·552
157·423
203·714
227·912
30,800

22·680
26·252
26·574
25·745
149·282
200·266
239·802
250·222
26,437

−2·487*
0·668
1·997*
1·953*
−12·249*
7·015
22·839*
27·412*
794

0·495
0·560
0·563
0·558
3·335
4·417
4·936
5·106
572

Notes: N = 9145. Computations use design weights. Chi-square test of all coefficients equalling zero, from a logit of
the treatment indicator on all baseline characteristics (where mean values were imputed for missing values) yields 24·95;
associated p value from a chi-squared (27df) distribution is 0·577.
*Indicates difference is statistically significant from 0 at the 5% (or less) level.

The bottom of Table 1 shows that the only set of variables that show important (and statistically significant) differences between treatment and control are the postassignment labour market
outcomes. The treatment group has lower weekly hours and earnings at week 52 but higher hours
and earnings at the 3-year and 4-year marks. At week 208, the earnings gain is about $27, with the
control mean of about $200. This is consistent with Mathematica’s final report, which showed that
the program had about a 12% positive effect on earnings by the fourth year after enrolment and
suggested that lifetime gains in earnings could very well exceed the program’s costs (Burghardt
et al., 2001). The effect on weekly hours at that time is a statistically significant 1·95 hours.
Figure 1 illustrates the treatment effects on earnings for each week subsequent to random
assignment. It shows an initial negative impact on earnings for the first 80 weeks, after which
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Variable

1076

REVIEW OF ECONOMIC STUDIES

Impact of Job Corps on weekly earnings

point a positive treatment effect appears and grows. The estimates in the bottom of Table 1 and
plotted in Figure 1 are similar qualitatively and quantitatively to the impact estimates reported in
Schochet et al. (2001).6
2.2. The effect on wages and the sample selection problem
It seems useful to assess the impact of the program on wage rates, as distinct from total earnings,
which is a product of both the price of labour (the wage) and labour supply (whether the person
works, and if so, how many hours). Distinguishing between price and quantity effects is important
for better understanding the mechanism through which the Job Corps leads to more favourable
labour market outcomes.
On the one hand, one of the goals of the Job Corps is to encourage work and self-sufficiency;
thus, participants’ total earnings might rise simply because the program succeeds in raising the
likelihood that they will be employed, while at the same time leaving the market wage for their
labour unaffected. On the other hand, the main component of the Job Corps is significant academic and vocational training, which could be expected to raise wages. There is a great deal of
empirical evidence to suggest a positive causal effect of education on wages (see Card, 1999).
Unfortunately, even though the National Job Corps study was a randomized experiment, one
cannot use simple treatment–control differences to estimate the effect of the program on wage
rates. This is because the effective “prices” of labour for these individuals are only observed
to the econometrician when the individuals are employed. This gives rise to the classic sample
selection problem (e.g., see Heckman, 1979).
Figure 2 suggests that sample selection may well be a problem for the analysis of wage
effects of the Job Corps. It reports employment rates (the proportion of the sample that has
positive work hours in the week) for both treated and control individuals, for each week following
6. In Schochet et al. (2001), the reported estimates used a less stringent sample criterion. Instead of requiring
nonmissing values for 208 consecutive weeks, individuals only needed to complete the 48-month interview (11,313
individuals). Therefore, for that sample, some weeks’ data will be missing. Despite the difference in the samples, the
levels, impact estimates, and time profile reported in Schochet et al. (2001) are also quite similar to those found in
Figures 2 and 3 (below).
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

F IGURE 1

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1077

random assignment. The results show that the program had a negative impact on employment
propensities in the first half of the follow-up and a positive effect in the latter half. This shows
that the Job Corps itself affected whether individuals would have a nonmissing wage rate.
Put another way, Figure 2 illustrates that even though proper random assignment will imply
that the treatment and control groups are comparable at the baseline, they may well be systematically different conditional on being employed in a given period subsequent to the random
assignment. As a result, the treatment–control difference in mean log hourly wages, as plotted in
Figure 3 (with pointwise 95% confidence intervals), may not represent the true causal effect of
the program.7
There are two other reasons why sample selection can potentially be important in this case.
As shown in Figure 2, a large fraction of individuals are not employed: employment rates start at
about 20% and grow to at most 60% at the 4-year mark. Second, nonemployed and employed individuals appear to be systematically different on a number of important observable dimensions.
Table 2 reports log-odds coefficients from a logit of employment in week 208 on the treatment
dummy and the baseline characteristics listed in Table 1. As might be expected, gender, race,
education, criminal history, and employment status at the baseline are all very strong predictors
of employment in week 208.
The problem of nonrandom sample selection is well understood in the training literature; it
may be one of the reasons why most evaluations of job training programs focus on total earnings,
including zeros for those without a job, rather than on wages conditional on employment. Of the
24 studies referenced in a survey of experimental and nonexperimental studies of U.S. employment and training programs (Heckman et al., 1999), most examine annual, quarterly, or monthly
earnings without discussing the sample selection problem of examining wage rates.8 As for the
7. Hourly wage is computed by dividing weekly earnings by weekly hours worked, for the treatment and control
groups. Note the pattern of “kinks” that occur at the 12- and 30-month marks, which is also apparent in Figure 1.
This could be caused by the retrospective nature of the interviews that occur at 12-, 30-, and 48-months postrandom
assignment. This pattern would be found if there were systematic overestimation of earnings on employment that was
further away from the interview date. The lines would “connect” if respondents were reminded of their answer from the
previous interview. Note that these potential errors do not seem to be too different between the treatment and the control
groups, as there are no obvious kinks in the difference (solid squares).
8. The exceptions include Kiefer (1979), Hollister, Kemper and Maynard (1984), and Barnow (1987). The sources
from tables 22 and 24 in Heckman et al. (1999) were surveyed.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

F IGURE 2
Impact of Job Corps on employment rates

1078

REVIEW OF ECONOMIC STUDIES

Job Corps, when reporting results on hourly wages for the working, Schochet et al. (2001) is
careful to note that because of the selection into employment, the treatment–control differences
cannot be interpreted as impact estimates.
2.3. Existing approaches
Currently, there are two general approaches to addressing the sample selection problem. The
first is to explicitly model the process determining selection. The conventional setup, following
Heckman (1979), models the wage determining process as:
Y ∗ = Dβ + Xπ1 + U

(1)

Z ∗ = Dγ + Xπ2 + V


Y = 1 Z ∗ ≥ 0 · Y ∗,
where Y ∗ is the offered market wage as of a particular point in time (e.g., week 208 after randomization), D is the indicator variable of receiving the treatment of being given access to the Job
Corps program, and X is a vector of baseline characteristics. Z ∗ is a latent variable representing
the propensity to be employed. γ represents the causal effect of the treatment on employment
propensities, while β is the (constant) treatment effect of interest.9 Both Y ∗ and Z ∗ are unobserved, but the wage conditional on employment Y is observed, where 1 [·] is the indicator
variable. (U, V ) are assumed to be jointly independent of the regressors (D, X).10 Within a standard labour supply framework, it is easy to imagine the possibility that job training could raise
the market wage for individuals, leading to a positive β , and at the same time raise the probability of participating in the labour force (γ > 0) since a higher wage will more likely exceed the
reservation wage for participating.11
9. In this specification, the treatment effect is constant.
10. This assumption, which is stronger than necessary, is invoked now for expositional purposes. It will be shown
below that what is required is instead independence of (U, V ) and D, conditional on X.
11. Of course, it should be noted that since the goal here is to estimate a reduced-form treatment effect, we do not
adopt a particular labour supply model or prohibit ways in which the treatment could affect participation. For example, γ
could be positive if the program’s job search assistance component was important.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

F IGURE 3
Differences in log(hourly wage), conditional on employment

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1079

TABLE 2
Logit of employment in week 208 on baseline characteristics
Variable

Estimate
0·172* (0·046)
−0·253* (0·051)
0·027 (0·014)
−0·471* (0·060)
−0·225* (0·077)
−0·412* (0·099)
−0·193 (0·175)
0·106 (0·130)
−0·261 (0·165)
0·121 (0·114)
−0·031 (0·070)
0·104* (0·019)
0·007 (0·012)
−0·006 (0·012)
−0·223* (0·055)
0·033 (0·085)
0·213* (0·104)
0·149 (0·086)
0·103 (0·095)
0·105 (0·080)
0·180 (0·127)
0·197 (0·162)
0·218* (0·071)
0·049* (0·011)
0·306* (0·091)
0·012 (0·120)
−26·580 (19·508)
0·845 (1·990)
−1·288* (0·285)

Notes: N = 9145. Robust S.E. are given in parentheses. Table reports are
(log-odds) coefficients from a logit of employment (positive hours) in week
208 on treatment status and baseline characteristics.
*Indicates statistically significant at the 0·05 (or less) level.

As in Heckman (1979), sample selection bias can be seen as specification error in the conditional expectation




E Y |D, X, Z ∗ ≥ 0 = Dβ + Xπ1 + E U |D, X, V ≥ −Dγ − Xπ2 .
One modelling approach is to assume that data are missing at random, perhaps conditional
on a set of covariates (Rubin, 1976). This amounts to assuming that U and V are independent of
one another or that employment status is unrelated to the determination of wages. This assumption is strictly inconsistent with standard models of labour supply that account for the participation decision (e.g., see Heckman, 1974).
A more common modelling assumption is that some of the exogenous variables determine
sample selection but do not have their own direct impact on the outcome of interest; i.e., some
of the elements of π1 are zero, while corresponding elements of π2 are nonzero. Such exclusion
restrictions are used in parametric and semiparametric models of the censored selection process
(e.g., Heckman, 1979, 1990; Ahn and Powell, 1993; Andrews and Schafgans, 1998; Das, Newey
and Vella, 2003).
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Treatment status
Female
Age at baseline
Black, non-Hispanic
Hispanic
Other race/ethnicity
Married
Living together
Separated
Has child
Number of children
Education
Mother’s education
Father’s education
Ever arrested
Household income
3000–6000
6000–9000
9000–18,000
>18,000
Personal income
3000–6000
6000–9000
>9000
At baseline
Have job
Months employed , previous year
Had job, previous year
Earnings, previous year (*10,000)
Usual hours/week (*10,000)
Usual weekly earnings (*10,000)
Constant

1080

REVIEW OF ECONOMIC STUDIES
TABLE 3
Bounds on treatment effects for week 208 ln(wage) using bounds of support
(Horowitz and Manski)
Observations
Employment rate
Mean log(wage)
Upper bound
Lower bound

3599
0·566
1·997
2·332
1·520

Observations
Employment rate
Mean log(wage)
Upper bound
Lower bound

5546
0·607
2·031
2·321
1·586

Upper bound: (ix) − (v)
Lower bound: (x) − (iv)

0·802
−0·746

Notes: 0·90 and 2·77 are the lower and upper bounds of the support of
ln(hourly wage) in week 208 after random assignment; (iv) = (ii)×(iii) +
[1−(ii)]×2·77; (v) = (ii)×(iii) + [1−(ii)]×(0·90). Rows (ix) and (x) are defined analogously.

The practical limitation to relying on exclusion restrictions for the sample selection problem
is that there may not exist credible “instruments” that can be excluded from the outcome equation.
This seems to be true for an analysis of the Job Corps experiment. There are many variables
available to the researcher from the Job Corps evaluation and many of the key variables are listed
in Tables 1 and 2. But for each of the variables in Table 2 that have significant associations
with employment, there is a well-developed literature suggesting that those variables may also
influence wage offers. For example, race, gender, education, and criminal histories all could
potentially impact wages. Household income and past employment experiences are also likely to
be correlated with unobserved determinants of wages.
Researchers’ reluctance to rely upon specific exclusion restrictions motivates a second, general approach to addressing the sample selection problem: the construction of worst-case scenario
bounds of the treatment effect. When the support of the outcome is bounded, the idea is to impute
the missing data with either the largest or the smallest possible values to compute the largest and
smallest possible treatment effects consistent with the data that are observed. Horowitz and Manski (2000a) use this notion to provide a general framework for constructing bounds for treatment
effect parameters when outcome and covariate data are nonrandomly missing in an experimental
setting.12 This strategy is discussed in detail in Horowitz and Manski (2000a), which shows that
the approach can be useful when Y is a binary outcome.
This imputation procedure cannot be used when the support is unbounded. Even when the
support is bounded, if it is very wide, so too will be the width of the treatment effect bounds. In
the context of the Job Corps program, the bounds are somewhat uninformative. Table 3 computes
the Horowitz and Manski (2000a) bounds for the treatment effect of the Job Corps program on
log wages in week 208. Specifically, it calculates the upper bound of the treatment effect as:

12. An early example of sensitivity analysis that imputed missing values is found in the work of Smith and Welch
(1986). Others (Balke and Pearl, 1997; Heckman and Vytlacil, 1999, 2000a,b) have constructed such bounds to address
a very different problem—that of imperfect compliance of the treatment, even when “intention” to treat is effectively
randomized (Bloom, 1984; Robins, 1989; Angrist et al., 1996).
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Control group
(i)
(ii)
(iii)
(iv)
(v)
Treatment group
(vi)
(vii)
(viii)
(ix)
(x)
Difference
(xi)
(xii)

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1081





Pr Z ∗ ≥ 0|D = 1 E [Y |D = 1] + Pr Z ∗ < 0|D = 1 Y UB
 




− Pr Z ∗ ≥ 0|D = 0 E [Y |D = 0] + Pr Z ∗ < 0|D = 0 Y LB ,

3. IDENTIFICATION OF BOUNDS ON TREATMENT EFFECTS
This section first uses a simple case to illustrate the intuition behind the main identification result
and then generalizes it for a very unrestrictive sample selection model.
Consider the case where there is only the treatment indicator, with no other covariates. That
is, X is a constant, so that π1 and π2 will be intercept terms. It will become clear that the result
below is also valid conditional on any value of X. Describing the identification result in this
simple case makes clear that the proposed procedure does not rely on exclusion restrictions.14
In addition, this section and the next assume that U (and hence Y ) has a continuous distribution.
Doing so will simplify the exposition; it can be shown that the proposed procedure can be applied
to discrete outcome variables as well (see Lee, 2002). Without loss of generality, assume γ > 0,
so that the treatment causes an increase in the likelihood of the outcome being observed.
From Equation (1), the observed population means for the control and treatment groups can
be written as


E Y |D = 0, Z ∗ ≥ 0 = π1 + E [U |D = 0, V ≥ −π2 ]
(2)
and





E Y |D = 1, Z ∗ ≥ 0 = π1 + β + E U |D = 1, V ≥ −π2 − γ ,

(3)

respectively. This shows that when U and V are correlated, the difference in the means will
generally be different from β.
13. The wage variable was transformed before being analysed to minimize the effect of outliers so that the Horowitz
and Manski (2000a) bounds would not have to rely on these outliers. Specifically, the entire observed wage distribution
was split into 20 categories, according to the 5th, 10th, 15th, . . . 95th percentile wages, and the individual was assigned
the mean wage within each of the 20 groups. Thus, the upper “bound” of the support, e.g., is really the mean log wage for
those earning more than the 95th percentile. The same data are used for the trimming procedure described below. Strictly
speaking, the Horowitz and Manski (2000a) bounds would use the theoretical bounds of the support of the population
log-wage distribution. Since these population maximums and minimums are not observed, one could instead use the log
of the minimum and maximum log-wage observed in the sample. It is clear that doing so would produce wider bounds
than that given by the implementation here.
14. Note that while existing procedures for point identification require an instrument that satisfies an exclusion
restriction, the existence of such an instrument is not sufficient for identification. For example, a single binary instrument
will not allow identification without imposing further assumptions.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

where all population quantities can be estimated, and Y UB and Y LB are the upper and lower
bounds of the support of log wages. As reported in the table, Y UB and Y LB are taken to be 2·77
and 0·90 ($15·96 and $2·46 an hour), respectively.13
Table 3 shows that the lower bound for the treatment effect on week 208 log wages is −0·75
and the upper bound is 0·80. Thus, the interval is almost as consistent with extremely large
negative effects as it is with extremely large positive effects. The reason for this wide interval is
that more than 40% of the individuals are not employed in week 208. In this context, imputing
the missing values with the maximal and minimal values of Y is so extreme as to yield an interval
that includes effect sizes that are arguably implausible. Nevertheless, the Horowitz and Manski
(2000a) bounds provide a useful benchmark and highlight that some restrictions on the sample
selection process are needed to produce tighter bounds (Horowitz and Manski, 2000b).
The procedure proposed below is a kind of “hybrid” of the two general approaches to the
sample selection problem. It yields bounds on the treatment effect, even when the outcome is
unbounded. It does so by imposing some structure on the sample selection process but without
requiring exclusion restrictions.

1082

REVIEW OF ECONOMIC STUDIES

Identification of β would be possible if we could estimate
E [Y |D = 1, V ≥ −π2 ] = π1 + β + E [U |D = 1, V ≥ −π2 ]

(4)

Pr −π2 −γ ≤V <−π2]
. The observed treatment mean is a weighted average of (4) and the
where p = [Pr[−π
2 −γ ≤V ]
mean for a subpopulation of “marginal” individuals (−π2 − γ ≤ V < −π2 ) that are induced to
be selected into the sample because of the treatment.


E [Y |D = 1, V ≥ −π2 ] is therefore bounded above by E Y |D = 1, Z ∗ ≥ 0, Y ≥ y p , where
y p is the pth quantile of the treatment group’s observed Y distribution. This is true because among
the selected population with V ≥ −π2 − γ, D = 1, no subpopulation with proportion (1 − p) can
have a mean that is larger than the average of the largest (1 − p) values of Y .
Put another way, we cannot identify which observations are inframarginal (V ≥ −π2 ) and
which are marginal (−π2 − γ ≤ V < −π2 ). But the worst-case scenario is that the smallest p
values of Y belong to the marginal group and the largest 1 − p values belong to the inframarginal
group. Thus, by trimming the lower tail of the Y distribution by the proportion p, we obtain
an upper bound
group’s mean in (4). Consequently, E[Y | D = 1, Z ∗ ≥ 0,
 for the inframarginal

∗
Y ≥ y p ] − E Y |D = 0, Z ≥ 0 is an upper bound for β. Note that the trimming proportion p is
equal to




Pr Z ∗ ≥ 0|D = 1 − Pr Z ∗ ≥ 0|D = 0
,
Pr [Z ∗ ≥ 0|D = 1]
where each of these probabilities is identified by the data.
To summarize, a standard latent variable sample selection model implies that the observed
outcome distribution for the treatment group is a mixture of two distributions: (1) the distribution for those who would have been selected irrespective of the treatment (the inframarginal
group) and (2) the distribution for those induced into being selected because of the treatment (the
marginal group). It is possible to quantify the proportion of the treatment group that belongs to
this second group, using a simple comparison of the selection probabilities of the treatment and
control groups. Although it is impossible to identify specifically which treated individuals belong
to the second group, worst-case scenarios can be constructed by assuming that they are either
at the very top or at the very bottom of the distribution. Thus, trimming the data by the known
proportion of excess individuals should yield bounds on the mean for the inframarginal group.

3.1. Identification under a generalized sample selection model
This identification result applies to a much wider class of sample selection models. It depends
neither on a constant treatment effect nor on homoskedasticity, which are both implicitly assumed
in Equation (1).
To see this, consider a general sample selection model that allows for heterogeneity in treatment effects:
(Y1∗ , Y0∗ , S1, S0, D) is i.i.d. across individuals
S = S1 D + S0 (1 − D)
c 2009 The Review of Economic Studies Limited


(5)

Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

because (2) could be subtracted to yield the effect β (since D is independent of (U, V ) by assumption). But the mean in (4) is not observed.
But this mean can be bounded. This is because all observations on Y needed to compute this
mean are a subset of the selected population (V ≥ −π2 − γ ). For example, we know that




E Y |D = 1, Z ∗ ≥ 0 = (1 − p) E [Y |D = 1, V ≥ −π2 ] + p E Y |D = 1, −π2 − γ ≤ V < −π2 ,

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1083



Y = S · Y1∗ D + Y0∗ (1 − D)
(Y, S, D) is observed,

Assumption 1.



(Independence): Y1∗ , Y0∗ , S1, S0 is independent of D.

This assumption corresponds to the independence of (U, V ) and (D, X) in the previous
section. In the context of experiments, random assignment will ensure this assumption will hold.
Assumption 2a. (Monotonicity): S1 ≥ S0 with probability 1.
This assumption implies that treatment assignment can only affect sample selection in “one
direction”. Some individuals will never be observed, regardless of treatment assignment (S0 =
S1 = 0), others will always be observed (S0 = 1, S1 = 1), and others will be selected into the
sample because of the treatment (S0 = 0, S1 = 1). This assumption is commonly invoked in
studies of imperfect compliance of treatment (Imbens and Angrist, 1994; Angrist, Imbens and
Rubin, 1996); the difference is that in those studies, monotonicity is for how an instrument affects
treatment status. Here, the monotonicity is for how treatment affects sample selection.
In the context of the Job Corps program, the monotonicity assumption essentially limits the
degree of heterogeneity in the effect of the program on labour force participation. It does not
allow, e.g., the job search assistance services provided by Job Corps to induce some to become
employed while simultaneously causing others to drop out of the labour force. A negative impact
could occur, e.g., if the job search counselling induced some to pursue further education (and
hence drop out of the labour force). Similar to the case of LATE, with only information on the
outcome, treatment status, and selection status, the monotonicity assumption is fundamentally
untestable. It should be noted that monotonicity has been shown to be equivalent to assuming
a latent variable threshold-crossing model (Vytlacil, 2002), which is the basis for virtually all
sample selection models in econometrics.
Proposition 1a. Let Y0∗ and Y1∗ be continuous random variables. If Assumptions 1 and
UB
2ahold then LB
0 and 0  are sharp lower and upper bounds for the average treatment effect
∗
∗
E Y1 − Y0 |S0 = 1, S1 = 1 , where


LB
0 ≡ E Y |D = 1, S = 1, Y ≤ y1− p0 − E [Y |D = 0, S = 1]


UB
0 ≡ E Y |D = 1, S = 1, Y ≥ y p0 − E [Y |D = 0, S = 1]
yq ≡ G −1 (q) , with G the c.d.f. of Y , conditional on D = 1, S = 1
p0 ≡

Pr [S = 1|D = 1] − Pr [S = 1|D = 0]
.
Pr [S = 1|D = 1]
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

where D, S, S0 , and S1 are all binary indicator variables. D denotes treatment status; S1 and
S0 are “potential” sample selection indicators for the treated and control states, respectively. For
example, when an individual has S1 = 1 and S0 = 0, this means that there will be nonmissing
data on the outcome (S = 1) if treatment is given and there will be missing data on the outcome
(S = 0) if treatment is denied. The second line highlights the fact that for each individual, we
only observe S1 or S0. Y1∗ and Y0∗ are latent potential outcomes for the treated and control states,
and the third line points out that we observe only one of the latent outcomes Y1∗ or Y0∗ and
only if the individual is selected into the sample S = 1. It is assumed throughout the paper that
E [S|D = 1] , E [S|D = 0] > 0.

1084

REVIEW OF ECONOMIC STUDIES

UB
The bounds are sharp in the sense that LB
lower
0 (0 ) is the largest (smallest)

 (upper) bound
UB
,

that is consistent with the observed data. Furthermore, the interval LB
is contained in
0
0
any other valid bounds that impose the same assumptions. (If S0 ≥ S1 with probability 1 then the
control group’s, rather than the treatment group’s, outcome distribution must be trimmed.)

Remark 1. The sharpness of the bound UB
0 means that it is the “best” upper bound that
is consistent with the data. A specific example of where this proposition can be applied is in
Krueger and Whitmore (2001), who study the impact of the Tennessee STAR (Student Teacher
Achievement Ratio) class-size experiment. In that study, students are randomly assigned to a
regular or small class and the outcome of interest is the SAT (or ACT) scores, but not all students
take the exam. Krueger and Whitmore (2001, p. 25) use Assumptions 1 and 2a to derive a different
upper bound, given by B ≡ E[Y | D = 1, S = 1] · Pr[S=1|D=1]
Pr[S=1|D=0] − E[Y | D = 0, S = 1]. Proposition
1a implies that this bound B, like any other proposed bound using these assumptions, cannot be
15
smaller than UB
0 .
Remark 2. An important practical implication of Assumptions 1 and 2a is that as p0
vanishes, so does the sample selection bias.16 The intuition is that if p0 = 0 then under the
monotonicity assumption, both treatment and control groups are comprised of individuals whose
sample selection was unaffected by the assignment to treatment, and therefore, the two groups
are comparable. These individuals can be thought of as the “always-takers” subpopulation
(Angrist et al., 1996), except that “taking” is not the taking of the treatment, but rather selection
into the sample. It follows that when analysing randomized experiments, if the sample selection
rates in the treatment and control groups are similar and if the monotonicity condition is believed
to hold then a comparison of the treatment and control means is a valid estimate of an average
treatment effect.17 As an example, the proportion of control group individuals, at week 90, that
have continuously nonmissing earnings and hours data is 0·822 and the proportion is 0·003
smaller (S.E. of 0·006) for the treatment group. Thus, if the above assumptions are invoked to
examine the nonresponse/attrition problem (as opposed to the focus of this study, missing wages
due to nonemployment) then the data suggest little bias due to nonresponse/attrition.
Remark 3. Assumptions 1 and 2a are minimally sufficient for computing the bounds.
First, the independence assumption is also important since it is what justifies the contrast between
the trimmed treatment group and the control group.
Second, monotonicity ensures that the sample-selected control group consists only of those
individuals with S0 = 1, S1 = 1. Without monotonicity, the control group could consist solely of
observations with S0 = 1, S1 = 0 and the treatment group solely of observations with S0 = 0,
15. Thus, in the context of Krueger and Whitmore (2001), Proposition 1a implies that computing the bound B is
unnecessary after already computing a very different estimate T , their “linear truncation” estimate. They justify T under a
different set of assumptions that (1) the additional small-class students induced to take the ACT exam are from the left tail
of the distribution and (2) if attending a small class did not change the ranking of students in small classes. Their estimate
T is mechanically equivalent to the bound UB
0 . Therefore, Proposition 1a implies that their estimate T is actually the
sharp upper bound given the mild assumptions that were used to justify their bound B.
16. A vanishing p corresponds to individuals with the same value of the sample selection correction term, and it
is well known that there is no selection bias, conditional on the correction term (see, e.g., Heckman and Robb, 1986;
Heckman, 1990; Ahn and Powell, 1993; Angrist, 1997).
17. Note that p0 here is proportional to the difference in the fraction that are sample selected between the treatment
and the control groups. Thus, the notion of a vanishing p should not be confused with “identification at infinity” in
Heckman (1990), in which the bias term vanishes as the fraction that is selected into the sample tends to 1.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Obviously, this result is equally valid if one were to assume monotonicity in the opposite
direction (S0 ≥ S1 with probability 1).

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1085

Remark 4. When p0 = 0 in a randomized experimental setting, there is a limited test
of whether monotonicity holds (and therefore whether the simple difference in means in the
outcome suffers from sample selection bias). If p0 = 0 and monotonicity holds then the selected subsets of both the treatment and the control groups will consist solely of individuals with
(S0 = 1, S1 = 1). Under randomization, the treatment control difference in the outcome should
represent a causal effect. In addition, the distribution of the Xs should be the same in the treatment
and control groups, conditional on being selected. This can be tested empirically.
In order for this test to have power, the two subpopulations (S0 = 0, S1 = 1) and (S0 = 1,
S1 = 0), need to have different distributions of baseline characteristics X. Recall that without
monotonicity, the selected treated group will be comprised of two subpopulations, (S0 = 1,
S1 = 1) and (S0 = 0, S1 = 1), while the selected control group will be comprised of the
groups (S0 = 1, S1 = 1) and (S0 = 1, S1 = 0). So if the distribution of the X is the same for
(S0 = 0, S1 = 1) and (S0 = 1, S1 = 0) then the selected treatment and control groups will have
the same distribution of X, whether or not monotonicity holds.
Finally, the trimming procedure described above places sharp bounds on the average treatment effect for a particular subpopulation—those individuals who will be selected irrespective of
the treatment assignment (S0 = 1, S1 = 1). It should be noted, however, that this subpopulation
is the only one for which it is possible to learn about treatment effects, given Assumptions 1 and
2a (at least, in this missing data problem). For the marginal (S0 = 0, S1 = 1) observations, the
outcomes are missing in the control regime. For the remaining (S0 = 0, S1 = 0) observations,
outcomes are missing in both the treatment and the control regimes. It would still be possible
to appeal to the bounds of Horowitz and Manski (2000a) to construct bounds on this remaining
population of the “never observed”, but this interval (whose width would be two times the width
of the outcome variable’s support) would not require any data. Whether or not the subpopulation
18. For example, the procedure will not work if 49% of the treatment group is missing and 52% of the control group
is missing.
19. Although E [S| D = 0] + E [S| D = 1] > 1 is not formally stated as an assumption in Zhang and Rubin (2003)
or in Zhang et al. (2008), it is clear that it is a necessary one to produce informative bounds. Using the notation of Zhang
and Rubin (2003), PC G and PT G are equivalent to E [S| D = 0] and E [S| D = 1], respectively. If PC G + PT G < 1, this
means that π DG is bounded above by PC G (the line below their equation (12)), which means that their equations (11)
and (12) yield (−∞, ∞) as bounds (if the dependent variable has unbounded support).
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

S1 = 1. Since the two subpopulations do not “overlap”, the difference in the means could not be
interpreted as a causal effect.
An interesting exception to this arises in the special case that E [S|D = 0] + E [S|D = 1] >
1, in which case informative bounds can be constructed without invoking monotonicity, as
demonstrated in Zhang and Rubin (2003). There, the insight is that the proportion of those who
are S0 = 1, S1 = 0 can be no larger than the proportion in the treatment group who have missing
values, 1 − E [S|D = 1]. It follows that within the control group, the fraction of S0 = 1, S1 = 1
individuals cannot be less than E [S|D = 0] − (1 − E [S|D = 1]), which is positive, as assumed.
It thus follows that, e.g., the upper bound for the mean of Y0∗ for S0 = 1, S1 = 1 is the mean after
[S|D=1]
trimming the bottom 1−E
E [S|D=0] fraction of the observed control group distribution. A symmetric
argument can be made for bounding the mean of Y1∗ for S0 = 1, S1 = 1. This idea is formalized
in Zhang and Rubin (2003) and also discussed in Zhang, Rubin and Mealli (2008). It should
be noted, however, that the procedure of Zhang and Rubin (2003) will not produce informative
bounds for a general sample selection model, as the assumption E [S|D = 0] + E [S|D = 1] > 1
is crucial.18 Specifically, if E [S|D = 0] + E [S|D = 1] ≤ 1 then the worst-case scenario would
involve trimming all the observed treatment and control observations, resulting in noninformative
(or “vacuous”) bounds.19

1086

REVIEW OF ECONOMIC STUDIES

of the “always observed” is of interest will depend on the context. In the case of the Job Corps
program, e.g., it is useful to assess the impact of the program on wage rates for those whose
employment status was not affected by the program.

A straightforward extension to the above analysis is to produce bounds of the treatment effect,
stratified by observed “baseline” characteristics X (those determined prior to the assignment of
treatment). Examples of such covariates in the case of Job Corps might include gender or race. It
is clear that the above analysis can all be conditioned on covariates X. It is possible to estimate
bounds for the average treatment effect for each value of X.
Alternatively, one can use these covariates to reduce the width of the bounds for the same
estimand that has been discussed so far (the average treatment effect for those who would always
be observed). To gain intuition for this, suppose half of the workers in the treatment group earns
the wage w H , while the other half earns the lower wage of w L . The trimming procedure described
in the previous sections suggests removing only low-wage individuals, by a proportion p0 to
obtain an upper bound of the mean for the “inframarginally” selected. The trimmed mean will
necessarily be larger.
Suppose now there is a baseline covariate X that perfectly predicts whether an individual
will earn w H or w L . Then, due to the random assignment of treatment, Assumptions 1 and 2a
also hold conditional on X. Therefore, the results in the previous section can be applied separately
for the two types of workers. If, for both groups, the same proportion of observations is trimmed,
the overall mean will not be altered by this trimming procedure.
More formally, consider the following alternative to Assumption 1,


Assumption 3. (Independence): Let X be a vector of covariates, and let Y1∗, Y0∗, S1, S0, X
be independent of D.
As an example, this would hold in the case of the Job Corps experiment due to random
assignment.
Proposition 1b. Let Y0∗ and Y1∗ be continuous random variables. If Assumptions 3 and
2ahold then 0LB and UB
0  are sharp lower and upper bounds for the average treatment effect
E Y1∗ − Y0∗ |S0 = 1, S1 = 1 , where

LB
≡
LB
x d H (x)
0

UB
≡
UB
x d H (x), where H is the c.d.f. of X conditional on D = 0, S = 1
0


LB
x ≡ E Y |D = 1, S = 1, Y ≤ y1− p x , X = x − E [Y |D = 0, S = 1, X = x]


UB
x ≡ E Y |D = 1, S = 1, Y ≥ y p x , X = x − E [Y |D = 0, S = 1, X = x]
yq ≡ G −1
x (q), with G x the c.d.f. of Y , conditional on D = 1, S = 1, X = x
Pr [S = 1|D = 1, X = x] − Pr [S = 1|D = 0, X = x]
px ≡
.
Pr [S = 1|D = 1, X = x]
The bounds are sharp in the sense that 0LB (UB
0 ) is the largest (smallest) lower (upper)
UB
UB
bound that is consistent with the observed data. Furthermore, 0LB ≥ LB
0 and 0 ≤ 0 .
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

3.2. Narrowing bounds using covariates

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1087

4. ESTIMATION AND INFERENCE
This
√section proposes and discusses an estimator for the bounds. The estimator can be shown to
be n consistent and asymptotically normal. The asymptotic variance comprises three components, reflecting (1) the variance of the trimmed distribution, (2) the variance of the estimated
trimming threshold, and (3) the variance in the estimate of how much of the distribution to trim.
To minimize redundancies, the discussion below continues to consider the case that S1 ≥ S0 with
probability 1 (from Assumption 2a); the results are also analogously valid for the reverse case of
S0 ≥ S1 .
4.1. Estimation
The estimates of the bounds are sample analogs to the parameters defined in Proposition 1a. First,
the trimming proportion p is estimated by taking the treatment control difference in the proportion with nonmissing outcomes and dividing by the proportion that is selected in the treatment
group. Next, the pth (or the (1 − p)th) quantile of the treatment group’s outcome distribution is
calculated. Finally, these quantiles are used to trim the data for the treatment group’s outcomes
and compute the bounds LB and UB .
Formally, we have
Definition of estimator:


Y · S· D·1 Y ≤ 
y1− p
Y · S · (1 − D)
LB

 −
 ≡
(6)
S · (1 − D)
S· D·1 Y ≤ 
y1− p


Y · S · D · 1 Y ≥ yp
Y · S · (1 − D)
UB

 −
 ≡
S · (1 − D)
S · D · 1 Y ≥ yp
S · D · 1[Y ≤ y]
yq ≡ min y :
≥q
S·D
 

S · (1 − D)
S·D
−
p≡
D
(1 − D)


S·D
,
D

where the summation is over the entire sample of size n.
4.2. Consistency, asymptotic normality, variance estimation, and inference
UB
The estimators LB and UB are consistent for LB
0 and 0 under fairly standard conditions:

Proposition 2. (Consistency): Let Y have bounded support (i.e. ∃ finite L , U such that
p
Pr [Y ≤ L] and Pr [Y ≥ U ] = 0), and suppose E [S|D = 0] > 0 and p0 ≥ 0, then LB → LB
0
p
and UB → UB
.
0
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

The first part of the proposition follows from applying Proposition 1a conditionally on
X = x. The second claim, that the width of the bounds must be narrower after using the covariates, is seen by noting that any treatment effect that is consistent with an observed population
distribution of (Y, S, D, X), must also be consistent with the data after throwing away information on X, and observing only the distribution of (Y, S, D). This necessity is strictly inconsistent
UB
with UB
0 > 0 .

1088

REVIEW OF ECONOMIC STUDIES

Proposition 3. (Asymptotic normality): Define μLB ≡ E[Y | D = 1, S = 1, Y ≤ y1− p0 ]
and μUB ≡ E[Y | D = 1, S = 1, Y ≥ y p0 ]. In addition
to the conditions
in Proposition 2,
 d


√  LB
LB
→ N 0, V LB + VC and
assume E [S|D = 0] < E [S|D = 1] < 1. Then, n  − 0



√  UB
d
n  − UB
→ N 0, V UB + VC , where
0

 
2
y1− p0 − μLB p0
V ar Y |D = 1, S = 1, Y ≤ y1− p0
+
V =
E [S D] (1 − p0)
E [S D] (1 − p0)

2
y1− p0 − μLB
+
·Vp
1 − p0
 
2

y p0 − μUB p0
V ar Y |D = 1, S = 1, Y ≥ y p0
UB
V =
+
E [S D] (1 − p0)
E [S D] (1 − p0)


2
y p0 − μUB
+
·Vp
1 − p0

⎛ 
⎞
1 − 1−α0p0
(1
−
α
)
0
⎠
+

V p = (1 − p0)2 ⎝
(1 − E [D]) α0
E [D] α0
LB

(7)

1− p0

and VC = V ar [Y |D = 0, S = 1] /E [S (1 − D)].21
Consider the three terms in V LB . The first term would be the variance of the estimate if
1
the trimming threshold y1− p0 were known. The term E [S D](1−
p0) exists because n is the size of
the entire sample (both treatment and control, and all observations including those with missing
outcomes). The second term reflects the fact that the threshold is a quantile that needs to be
estimated. Taken together, the first two terms are exactly equivalent to the expression given in
Stigler (1973), which derives the asymptotic distribution of a one-sided “ p0 -trimmed” mean,
when p0 is known. But p0 is not known, and must be estimated, which is reflected in the third
term. The third term itself includes the asymptotic variance of p̂ multiplied by the square
of


α

the gradient of the population trimmed mean with respect to p0. Note that

1−α0
α0

and

1− 1−0p
0


α0
1− p0

are the odds of an observation being missing conditional on being in the control group and the
treatment group, respectively. The Appendix contains the proposition’s proof, which involves
applying theorem 7.2 of Newey and McFadden (1994), an asymptotic normality result for GMM
estimators when the moment function is not smooth.
20. Recall that boundedness of the support of Y is unnecessary for identification. Furthermore, consistency can be
proven without boundedness (see Lee, 2005).
21. Note that the usual asymptotic variance of the estimated mean for the control group is divided by E [S (1 − D)]
because n here is the total number of observations (selected and nonselected and treated and control).
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

As shown in the Appendix, the proof involves showing that the estimator is a solution to a
GMM (Generalized Method of Moments) problem, showing that the moment function vector is,
UB
with probability 1, continuous at each possible value of LB
0 and 0 and applying theorem 2.6
20
of Newey and McFadden (1994).
The estimators LB and UB are also asymptotically normal, with an intuitive expression
for the variance.

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1089

4.3. Inference with unknown sgn( p0 )
The discussion to this point has presumed that p0 > 0 and therefore the procedure described so
far is appropriate when the researcher has reason to impose the assumption that the treatment
status has a (strictly) positive impact on the outcome being observed. But a researcher may want
to remain agnostic about the sign of p0. Specifically, we have so far assumed that S1 ≥ S0 with
probability 1. But the researcher—still concerned about sample selection—may instead want to
adopt the following assumption.
Assumption 2b.
ability 1.

(Monotonicity): Either S1 ≥ S0 with probability 1 or S0 ≥ S1 with prob-

This means that monotonicity is maintained, but the direction in which treatment affects
selection is unknown.
The above identification, estimation, and inference procedure readily generalizes to this
case. First, from an identification standpoint, it is clear that the sharp lower bound is given by



 
LB
0 ≡ 1 p0 ≥ 0 E Y |D = 1, S = 1, Y ≤ y1− p0 − E [Y |D = 0, S = 1]




+1 p0 < 0 E [Y |D = 1, S = 1] − E Y |D = 0, S = 1|Y ≥ y p0∗ ,
 LB LB



UB + 1·96σ UB > UB is equivalent to Pr  −0 <
22. To see this, note that Pr LB − 1·96σ LB < LB
0 ,
0
σ LB

 LB LB



 LB LB

UB
UB
UB
UB
 −0
 −0
 −0
 −0
> −1·96 = 1 − Pr
> 1·96 − Pr
< −1·96 + Pr
>
1·96,
σ UB
σ LB
σ UB
σ LB





UB
LB
UB
UB −0
LB −0
UB −0
< −1·96 , which is equal to 1 − 0·025 − 0·025+ Pr
> 1·96,
< −1·96 ,
1·96,
UB
LB
UB
σ

UB
UB
LB −LB
0 ,  −0
is standard bivariate normal.
when
σ LB
σ UB

σ

σ

c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Estimation of the variances is easily carried out by replacing all of the above quantities
(e.g., E [S D] , y p0 ) with either of their sample analogs (e.g., 1n S D, y p ). After assuming a
finite second moment for Y , consistency follows because the resulting estimator is a continuous
function of consistent estimators for each part.
There are two simple ways to compute confidence
 intervals. First,one can compute the in





terval [LB −1·96 σ√LBn , UB + 1·96 · σ√UBn ], σLB ≡ V LB , σUB ≡ V UB . This interval


UB
22
will asymptotically contain the region LB
and
with at least 0·95 probability.
0 , 0
 ∗ Imbens
Manski (2004) point out that this same interval will contain the parameter E Y1 − Y1∗ |S0 = 1,
S1 = 1] with an even greater probability, suggesting that the confidence interval for the parameter will be narrower for the same coverage rate. The results of Imbens and Manski (2004) imply
that a (smaller) interval of [LB −C n · σ√LBn , UB + C n σ√UBn ], where C n satisfies




√ UB − LB
−  −C n = 0·95,
 Cn + n
max (σLB , σUB)


can be computed, and will contain the parameter E Y1∗ − Y1∗ |S0 = 1, S1 = 1 with a probability
of at least 0·95.
The interval of Imbens and Manski (2004) is more appropriate here since the object of interest is the treatment effect and not the region of all rationalizable treatment effects. Nevertheless,
for completeness, both intervals are reported in the presentation of the results.

1090

REVIEW OF ECONOMIC STUDIES




 ∗
UB ,
UB = 1 p̂ ≥ 0 · UB + 1 p̂ < 0 · 


LB∗ and 
UB∗ are the analogous bounds when the control groups are trimmed.23 As long
where 

LB∗ ,
as p0 = 0, LB is consistent because it is a function of consistent estimators p̂,LB , and 
and the function is continuous at the true parameter values of those estimators.
It follows from the delta method that the above estimator is also asymptotically normal with








√  LB
d
n  − LB
→ N 0, 1 p0 ≥ 0 V LB + VC + 1 p0 < 0 VT + VCUB
0
 d







√  UB
n  − UB
→ N 0, 1 p0 ≥ 0 V UB + VC + 1 p0 < 0 VT + VCLB ,
0
where the variance for the untrimmed treatment mean VT is analogous to VC defined previously
and VCUB and VCLB use the analogous expressions in Proposition 3 but for the control group.
To summarize, suppose the researcher is unsure about the sign of p0 , but knows that p0 is
nonzero. As an overall procedure, it is asymptotically valid to estimate p, and if positive, trim the
treatment group and conduct inference as discussed in Subsections 4.1 and 4.2. And if negative,
trim the control group instead and conduct inference using the same formulas (i.e., let D ∗ = 1 − D
and replace D everywhere with D ∗ ). The intuition behind this is that as sample size increases,
and the sampling variability of p shrinks, the probability that the “wrong” group (treatment or
control) is trimmed, leading to the wrong asymptotic variance being used, vanishes.
It is useful to consider the asymptotic behaviour of this estimator when p0 = 0. In the Appendix, the estimator is shown to remain consistent, even without bounded support. Intuitively,
the amount of trimming vanishes with sample size and so the trimmed mean converges to the
(unbiased) untrimmed mean. On the other hand, it is clear that conventional first-order asymptotics will not apply. Close inspection of the above expressions reveals that keeping all other
parameters constant, the asymptotic variance of either of the bounds is in general discontinuous
at p0 = 0. Specifically, when p0 approaches zero from the right, the third component of the variance of the trimmed treatment mean will in general converge to a quantity that differs from the
third component that must appear for the variance of the trimmed control mean when p0 becomes
negative.
This leads to two practical implications. First, when the researcher knows p0 to be exactly
zero, the above asymptotic expressions do not apply. Second, in the case when p0 = 0 , even

23. That is,



more formally,



∗
Y ·S·(1−D)·1 Y ≤ y
!
∗
1− p


∗
S·(1−D)·1 Y ≤ y
!
∗




LB∗



≡

Y ·S·D
−
S·D



Y ·S·(1−D)·1 Y ≥ y ∗!∗
p


S·(1−D)·1 Y ≥ y ∗!∗


UB∗ ≡
and 

Y ·S·D
−
S·D

p



,

with yq∗ ≡ min y :

1− p

S·(1−D)
.
(1−D)

c 2009 The Review of Economic Studies Limited


S·(1−D)·1[Y ≤y]
≥q
S·(1−D)



and !
p∗ ≡



S·(1−D)
−
(1−D)

S·D
D

"

Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

where y p0∗ is the p0∗ t h quantile of the control group’s observed distribution of Y . In other words,
when p0 > 0, the upper tail of the treatment group’s Y distribution is trimmed, as described above;
but when p0 < 0, the lower tail of the control group is trimmed for exactly same reasoning as
described in the previous section. There is an analogous expression for UB
0 .
Replacing the above population quantities with their sample analogues, an estimator for the
bounds in this less restrictive model becomes



 ∗
LB
LB = 1 p̂ ≥ 0 · LB + 1 p̂ < 0 · 

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1091

5. EMPIRICAL RESULTS
This section uses the trimming estimator to compute bounds on the treatment effect of the Job
Corps on wage rates. The procedure is first employed for wages at week 208, 4 years after the
date of random assignment. The widths of the bounds are reasonably narrow and are suggestive
of positive wage effects of the program. The bounds for the effect at week 208 do contain zero,
but the bounds at week 90 do not. Overall, the evidence presented below points towards a positive
treatment effect, but not significantly more than a 10% effect.
5.1. Main results at week 208
Table 4 reports the estimates of the bounds of the treatment effect on wages at week 208. The
construction of the bounds and their S.E. are illustrated in the table. Rows (iii) and (vi) report the
means of log wages for the treated and control groups. Rows (ii) and (v) report that about 61% of
the treated group have nonmissing wages, while about 57% of the control group have nonmissing
wages. This implies a trimming proportion of about 6·8% of the treated group sample. The pth
quantile is about 1·64, and therefore, the upper bound for the treated group is the mean after
24. As can be seen from the asymptotic expressions above, the discontinuity in the asymptotic variance disappears
when the treatment and control groups have similar scale, in the sense that y − μ T for the treatment group is equal to
μC − y for the control group, where μ T and μC are the untrimmed treatment and control means, and y and y are the
population maximum and minimum for the treatment and control groups, respectively.
25. It should also be recalled that the untrimmed estimator lies between the point estimators of the two bounds
with probability 1, and therefore, it may well be with many applications and sample sizes the untrimmed confidence
interval may be contained in the trimmed confidence interval with high probability, meaning that inferences based on the
trimming bounds would be too conservative.
26. See, e.g., Staiger and Stock (1997) and Andrews, Moreira and Stock (2007) and the references therein. Although
there are some similarities, the trimming problem presented here is quite distinct from the IV case. For one, the bounds
are still identified and the proposed estimator is still consistent (with bounded support) even when p0 = 0.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

though coverage rates for confidence intervals are asymptotically correct, a large discontinuity in
the asymptotic variance suggests coverage rates may be inaccurate when sample sizes are small
and p0 is “close” to zero, which would imply that the “wrong” group is being trimmed with
nontrivial probability in repeated samples.24
It is useful to note, however, that for any finite sample size, as p0 approaches zero, the confidence interval constructed from the untrimmed estimator will have coverage for the parameter
of interest that approaches the correct rate since the bias (the difference between the untrimmed
population mean and the population trimmed mean) is continuous in p0 and equal to zero at
p0 = 0. Therefore, the untrimmed estimator for the treatment effect may have better coverage
rates in a finite sample, even though its coverage will be zero asymptotically. Thus, at a minimum, it seems worthwhile for the researcher to additionally report the untrimmed estimator and
S.E. A simple, conservative approach to combining the trimmed and untrimmed intervals is to
compute their union. In repeated finite samples, at p0 arbitrarily close to zero, this guarantees at
least nominal coverage.25
The issue of the estimator’s finite sample behaviour when p0 is close to zero has some similarities to that regarding inference in instrumental variables when the first-stage coefficient is
close to zero. Just as instrumental variables presumes the existence of a first stage, here, we presume that there is a nontrivial selection problem ( p0 nonzero). In both cases, first-order asymptotic approximations may be inadequate in finite samples when the nuisance parameter (here, p0 )
is close to zero. The problem for instrumental variables is indeed nontrivial and has motivated a
number of theoretical papers focusing on inference with weak instruments.26

1092

REVIEW OF ECONOMIC STUDIES
TABLE 4
Bounds on treatment effects for ln(wage) in week 208 using trimming procedure

Control
3599
0·566
1·997

S.E.

(iv) Number of observations
(v) Proportion nonmissing
(vi) Mean ln(wage) for employed

5546
0·607
2·031

Treatment upper bound S.E.
Component 1
0·0053
Component 2
0·0021
Component 3
0·0083
Total
0·0100

p = [(v)−(ii)]/(v)
(vii) pth quantile
(viii) Trimmed mean: E[Y |Y > y p

0·068
1·636
2·090

0·0082

Treatment lower bound S.E.
Component 1
0·0058
Component 2
0·0037
(ix) (1− p)th quantile
2·768 Component 3
0·0144
(x) Trimmed mean: E[Y |Y < y1− p ]
1·978 Total
0·0159
Effect
Effect
(xi) Upper bound estimate = (viii) − (iii) 0·093 (xiii) Upper bound S.E. 0·0130
(xii) Lower bound estimate = (x) − (iii) −0·019 (xiv) Lower bound S.E. 0·0179
Confidence interval 1 = [(xii)−1·96 × (xiv), (xi)+1·96 × (xiii)]
[−0·055 to 0·119]
Confidence interval 2 (Imbens and Manski) = [(xii)−1·645 × (xiv), (xi)+1·645 × (xiii)]
[−0·049 to 0·114]
Heckman two-step estimator 0·0148 (0·0117)
Das et al. (2003)
0·0140 (0·0122)

Notes: Before trimming, there are 3371 nonmissing observations in the treatment group. After trimming, there are 3148
(3142) observations remaining in the treatment group after trimming the lower p (upper 1−p) of the distribution (these
numbers are not equal due to using the design weights). For the upper bound S.E., component 1 is the usual S.E. of the
mean, using the trimmed sample. Component 2 is the square root of (1/3371)×( p/(1 − p))×{(viii) − (vii)}2 . Component 3 is the square root of {((viii) − (vii))/(1 − p)}2 × Var(p), where Var( p) = (1 − p)2 ×{(1/5546)×((1−(v))/(v)) +
(1/3599)×((1−(ii))/(ii))}. “Total” refers to the square root of the sum the squared components. The entries for the treatment lower bound S.E. are defined analogously. (xiii) and (xiv) are the square root of the sum of the squared S.E. for
the treatment upper bound (or lower bound) and control group. For the Imbens and Manski confidence interval 1·645
satisifies F (1·645 + ((xi) − (xii))/(max((xiii), (xiv))) − F (−1·645) = 0·95, where F is the standard normal c.d.f. See
Imbens and Manski (2004) for details. The Heckman two-step estimator uses months employed in the previous year and
treatment status in the first-stage probit. The Das et al. (2003) estimator is described in the text.

trimming the tail of the distribution below 1·64.27 After trimming, the resulting mean is about
2·09, and so the upper bound of the treatment effect UB is 0·093 (row (xi)). A symmetric
procedure yields LB of −0·019 (row (xii)).
The width of these bounds is about 0·11. Note that this is 1/14th the width of the bounds
yielded by existing “imputation” procedures as reported in Table 3 (calculate 1·55 from rows (xi)
and (xii)). The much larger interval in Table 3 is clearly driven by the relatively wide support
of the outcome variable.28 The difference between the two sets of bounds makes an important
difference in gauging the magnitude of the effects of the program. From Table 3, the negative
region covered by the bounds is almost as large as the positive region contained by the bounds.
In this sense, the bounds from Table 3 are almost as consistent with large negative effects as they
are with large positive effects.
27. The procedure can be easily adapted to the case of a dependent variable with discrete support, which can
generate “ties” in the data. After sorting the data by the dependent variable, unique ranks can be imposed (i.e., so that
individuals with the exact same wage level all have different ranks). The correct proportion of data can be trimmed based
on those ranks, before calculating the trimmed mean, which is based on the remaining data. This procedure was used here,
with the slight modification that the design weights were used, so the observations were dropped until the accumulated
sum of the weights equalled the trimming proportion times the total sum of the weights in the treatment group.
28. For a detailed theoretical discussion of how the imputation bounds (e.g., Table 3) compare to the trimming
bounds (e.g., Table 4) when the outcome is binary, see Lee (2002).
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Treatment

Control S.E.
(i) Number of observations
(ii) Proportion of nonmissing
(iii) Mean ln(wage) for employed

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1093

29. From Figure 2, there appears to be about 40% nominal wage growth more than 4 years. Inflation over that
length of time in the late 1990’s was about 9% (CPI-U (Consumer Price Index for all Urban Consumers) for 1995: 152·4
and for 1999: 166·6). Schochet et al. (2001) find that the Job Corps impact on time spent in any education and training
programs amounted to about 1 school year per participant. The estimated impact per eligible applicant was 28% lower.
30. Specifically, it is the square root of the sample analog of T R1I M V ar[Y | D = 1, S = 1, Y ≥ y p0 ], where n T RI M
n
is the number of observations after trimming.


y p0 −μUB

2

p0

, where nU N T RI M is the number of
31. It is the square root of the sample analog of U N T1 R I M
(1− p0 )
n
nonmissing observations before trimming.


α
2 1 1− 1−0p0

1 1−α0 , where n T and nC are
32. It is the square root of the sample analog of y p0 − μUB
+
α0
T
C α
n

1− p0

n

0

the number of treatment and control observations (missing and nonmissing) in the sample.
33. Specifically, for the Heckman two-step estimator, selection status was the dependent variable in a first-step
probit including the treatment status and months employed. The predicted inverse Mill’s ratio was used as an additional
regressor in a regression of wages at week 208 on treatment status. For the estimator of Das et al. (2003), the probability
of selection was predicted from a regression of selection status on treatment months employed, their interaction and
the square of months employed. The second-stage regressed wages at week 208 on treatment status and the predicted
probability. As in Das et al. (2003), the orders of the polynomials and interactions for both first and second stages were
determined by cross-validation.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

The width of the trimming bounds in Table 4 is also narrow enough to rule out plausible
effect sizes. For example, suppose the training component of the Job Corps program was ineffective at raising the marketable skills of the participants. We would then expect Job Corps to
have a negative impact on wages, insofar as the time spent in the program caused a delay in
accumulating labour market experience.
Suppose annual wage growth is about 8% a year, and the program group spent more time
in education and training programs than the control group by an amount equivalent to 0·72 of a
school year.29 If a full school year in training causes a year delay in earnings growth, this would
imply Job Corps impact of about −0·058. The lower bound in Table 4 is −0·019. Thus, the
scenario described above is ruled out by the trimming bounds computed in Table 4. By contrast,
an impact of −0·058 is easily contained by the support-dependent interval [−0·746, 0·802] of
Table 3.
An impact of −0·058 is also outside the interval after accounting for sampling errors of
the estimated bounds. The right side of Table 4 illustrates the construction of these S.E. For the
estimate of the upper bound for the treatment group, component 1 is the S.E. associated with
the first term in Equation (7).30 Component 2 reflects sampling error in estimating the trimming
threshold.31 Component 3 reflects sampling error in estimating the trimming proportion.32 In
this case, the largest source of the variance in the upper bound comes from the estimation of the
trimming proportion. The total of 0·010 is the square root of the sum of the squared components.
Doing a similar calculation for the lower bound, and then using the S.E. on the mean for the
control group, yields S.E. for UB and LB of 0·0130 and 0·0179, as shown in the bottom of
Table 4. These S.E. can then be used to compute two types of 95% confidence intervals. The first
covers the entire set of possible treatment effects with at least 0·95 probability, while the second
interval, using the result from Imbens and Manski (2004), covers the true treatment effect at least
95% of the time. A plausible negative impact of −0·058 is outside both of these intervals.
As argued previously, the Job Corps data do not seem to include a plausible instrument for
selection. Nevertheless, it is useful to compare the bounding inference to conventional parametric
and nonparametric sample selection estimators that do rely on exclusion restrictions. The bottom
of Table 4 presents both a Heckman two-step estimator and the nonparametric estimator of
Das et al. (2003). Both use the “Months Employed in Previous Year” variable to predict sample
selection.33

1094

REVIEW OF ECONOMIC STUDIES
TABLE 5
Bounds on treatment effects for ln(wage) in week 208 trimming procedure using baseline covariates
Lower bound for treatment mean
Estimate

1
2
3
4
5
Total
Effect

1·795
1·938
1·934
2·025
2.121
1·985

S.E.

Observation

0·030
343
0·052
248
0·020
931
0·028
745
0.025
712
0·013
2979
Lower bound for effect
−0·0118
0·0151

Estimate
1·979
1·963
2·051
2·127
2.204
2·086

S.E.

Observation

0·025
348
0·065
250
0·017
935
0·020
748
0.022
715
0·012
2996
Upper bound for effect
0·0889
0·0142

Weight
0·107
0·131
0·291
0·238
0.234
1·000

Notes: Trimming procedure from Table 3 applied separately to each group (defined in text). “Total” estimates are
means of the five groups using the “Weight” as weights. Asymptotic variance for “Total” is computed according
to Chamberlain (1993): it is the (weighted, using “Weight”) average of the asymptotic variance for each group
(each group’s sampling variance times the number of observations for the group) plus the (weighted by “Weight”)
average squared deviation of each group’s estimate from the “Total” mean. Control mean, (iii) in Table 4, is then
subtracted to obtain bounds on the treatment effect.

5.2. Using covariates to narrow bounds
The construction of bounds that use the baseline covariates, as presented in Proposition 1b, is
illustrated using a variable that splits the sample into five mutually exclusive groups, based on
their observed baseline characteristics. Any baseline covariate will do, as will any function of
all the baseline covariates. In the analysis here, a single baseline covariate—which is meant to
be a proxy for the predicted wage potential for each individual—is constructed from a linear
combination of all observed baseline characteristics. This single covariate is then discretized, so
that effectively five groups are formed according to whether the predicted wage is within intervals
defined by $6·75, $7, $7·50, and $8·50.34
Then, a trimming analysis is conducted for each of the five groups separately. Note that for
each of the five groups, there is a different trimming proportion. The lower and upper bounds of
the treatment group means, by each of the five groups, are given in the left and right columns
of Table 5, respectively. The lower bounds range from 1·80 to 2·12, while the upper bounds
range from 1·96 to 2·20. The S.E. are computed for each group separately in the same manner as
in Table 4.


To compute the bounds for the overall average E Y1∗ |S0 = 1, S1 = 1 , the group-specific

bounds must be averaged, weighted by the proportions Pr Group J |S0 = 1, S1 = 1 . This is provided in the row labelled “Total”.35 This leads to an interval of [−0·0118, 0·0889]. This interval
is about 11% narrower than that reported in Table 4. The estimated asymptotic variance for these
overall averages is the sum of (1) a weighted average of the group-specific variances and (2)
the (weighted-) mean squared deviation of the group-specific estimates from the overall mean.
This second term takes into account the sampling variability of the weights, as described in

34. Specifically, the coefficients from the linear combination of the Xs are the coefficients from a regression of
week 208 wages on all baseline characteristics in Table 1. The coefficients were then applied to all individuals to impute
a predicted wage.
35. There are slight differences in the number of observations in each group after trimming, for the upper and lower
bounds. This is due to the use of the design weights.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Group

Upper bound for treatment

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1095

TABLE 6
Treatment effect estimates and bounds, by week
Fraction nonmissing

Effect

Treatment

Trimming proportion

Untrimmed

Lower bound

Upper bound

Week 45

0·4223

0·3424

Week 90

0·4600

0·4601

Week 135

0·5173

0·5451

Week 180

0·5403

0·5825

0·1892
(0·0219)
0·0003
(0·0232)
0·0509
(0·0192)
0·0724
(0·0177)

0·022
(0·011)
0·043
(0·011)
0·028
(0·011)
0·026
(0·011)

−0·074
(0·014)
0·042
(0·024)
−0·016
(0·021)
−0·033
(0·019)

0·127
(0·015)
0·043
(0·025)
0·076
(0·014)
0·087
(0·013)

Notes: (N = 9145 for each row). S.E. are given in parentheses. S.E. for trimming proportion given by formula
in note to Table 4. Bounds computed according to Table 4. See text for details.

Chamberlain (1994).36 These sampling errors lead to a 95% Imbens–Manski interval of
[−0·037, 0·112].
By statistically ruling out any effect more negative than −0·037, this suggests that after 4
years, the Job Corps enabled program group members to offset at least 35% (and perhaps more)
of the potential 0·058 loss in wages due to lost labour market experience that could have been
caused by the program.
5.3. Effects by time horizon and testable implications
An analysis of the bounds at different time horizons provides further evidence that the Job Corps
program had a positive impact on wage rates. The analysis of Table 4 was performed for impacts
on wage rates at weeks 45, 90, 135, and 180, and these results are reported in Table 6.
As would be expected, the widths of the intervals are directly related to the treatment control
difference in the proportion missing. When the proportion is the largest, as at week 45, the range
is [−0·074, 0·127]. At week 180, when the proportion is 0·0724, the interval is [−0·033, 0·087].
At week 90, the estimated trimming proportion is close to zero, and the resulting bounds are
given by the interval [0·042, 0·043]. Maintaining the assumption that the true p0 = 0, we note
that the S.E. are larger for these bounds, even though they are quite similar to the untrimmed
treatment control difference. This is partly due to the sampling error in the trimming proportion.
Using these S.E. and the Imbens and Manski (2004) confidence interval for the treatment effect,
parameter is computed to be [−0·004, 0·092]. As noted above, if the true trimming proportion
p0 is arbitrarily close to zero then the untrimmed confidence interval will have almost accurate
coverage in a finite sample. This untrimmed treatment effect confidence interval is [0·020, 0·065].
Thus, both procedures can rule out effects more negative than −0·004 at conventional levels of
significance.
If we were to alternatively assume that p0 = 0 at week 90 then one can provide limited evidence on the plausibility of the monotonicity condition (Assumption 2b). If at week
90, E [S|D = 1] − E [S|D = 0] is truly zero then the average causal effect on sample selection
36. The weighted mean of the five group-specific means can be seen as a minimum distance estimator where
the weights are the estimated proportions in each group. Chamberlain (1994) gives the asymptotic variance for this
estimator even when the moment vector is misspecified, as would be the case if the group-specific means are different.
The asymptotic variance is the sum of two components: (1) the (weighted) average of the asymptotic variance for each
group ( 1 in Chamberlain, 1994) and (2) the (weighted) average squared deviation of each group’s estimate from the
“Total” mean ( 2 in Chamberlain, 1994).
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Control

1096

REVIEW OF ECONOMIC STUDIES

6. CONCLUSIONS: IMPLICATIONS AND APPLICATIONS
This paper focuses on an important issue in evaluating the impact of a job training program
on wage rates—the sample selection problem. It is a serious issue even when the treatment of
a training program is believed to be independent of all other factors, as was the case in the
randomized experimental evaluation of the U.S. Job Corps. Existing sample selection correction
methods are infeasible due to the absence of plausible exclusion restrictions, and in this case, one
cannot rely upon the boundedness of the outcome variable’s support to yield informative bounds
on the treatment effect of interest.
To estimate the impact of the Job Corps on wages, this paper develops a new method for
bounding treatment effects in the presence of sample selection in the outcome. An appealing feature of the method is that the assumptions for identification, independence, and monotonicity are
typically already assumed in standard models of the sample selection process, such as in Equation (1). In the case of randomized experiments, the independence assumption is satisfied, and
as illustrated in the previous section, the existence of baseline characteristics suggests a limited
test of monotonicity. More importantly, the bounding approach does not require any exclusion
restrictions for the outcome equation. Nor do the trimming bounds rely on the bounds of the
support of the outcome variable.
The analysis using the proposed “trimming” bounds points to two substantive conclusions
about the Job Corps. First, the evidence casts doubt on the notion that the program only raised
earnings through raising labour force participation. Effects more negative than −0·037 can be
37. If S1 = S0 with less than probability 1 then
 therewould be a nonzero probability of S1 < S0 and it would be
equal to the probability of S0 > S1 (in order for E S1 − S0 = 0). This would contradict monotonicity.
38. This is a valid test since in this context, Pr[S = 1| D = 1, X = x] = Pr[S = 1| D = 0, X = x] for all x and is
equivalent to the test Pr[D = 1| S = 1, X = x]/ Pr[D = 0| S = 1, X = x] = Pr [D = 0]/ Pr [D = 1].
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

E [S1 − S0] is zero. If monotonicity holds then this can only be true if S1 = S0 with probability 1.37
If the only observed data are the triple (Y, S, D) then it is impossible to test this monotonicity
assumption. On the other hand, if there exist baseline characteristics X, as in the case of the Job
Corps experiment, then it is possible to test whether S0 = S1 with probability 1. That is, it is
possible to test whether for each value of X, Pr[S = 1|D = 1, X = x] = Pr[S1 = 1|X = x] is equal
to Pr[S = 1|D = 0, X = x] = Pr[S0 = 1|X = x], which should be the case for all x if S0 = S1 with
probability 1. Intuitively, if it was found that for some values of X, the treatment caused wages to
be observed, while for other values of X, the treatment was found to cause wages to be missing,
then Assumption 2a must not hold.
By Bayes’ Rule and independence (Assumption 1), Pr[S = 1|D = 1, X = x] = Pr[S = 1|D =
0, X = x] for all x implies that the distribution of X conditional on S = 1, D = 1 should be the
same as the distribution conditional on S = 1, D = 0. This is because the density of X, conditional
on D, does not depend on the value of D, and the probability of S = 1 conditional on D also
does not depend on D, by assumption.
A simple way to check this empirically is to examine the means of the variables in
Table 1, but conditional on having nonmissing wages. This is done for week 90 and is reported in the Appendix, Table A1. The differences between the treatment and the control means
for each variable are small and consistently statistically insignificant. A joint test of significance
is given by a logistic regression of the treatment indicator on the baseline characteristics X, using
a sample of all those with nonmissing wages at week 90.38 The resulting test of all coefficients
equalling zero yields a p value of 0·851. Thus, the data are consistent with the monotonicity
condition holding at week 90.

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1097

MATHEMATICAL APPENDIX
∗
Lemma 1. Let Y be a continuous random variable and a mixture of two random variables,
 withc.d.f.s M (y) and
N ∗ (y), and a known mixingproportion p ∗ ∈ [0, 1), so that we have F ∗ (y) = p ∗ M ∗ (y) + 1 − p ∗ N ∗ (y). Consider
∗ (y)− p ∗
#∞
, which is the c.d.f. of Y after truncating the p ∗ lower tail of Y . Then, −∞
yd G ∗ (y) ≥
G ∗ (y) = max 0, F 1−
p∗
#∞
#
#∞
∞
∗
∗
∗
−∞ yd N (y). −∞ yd G (y) is a sharp (in the sense of Horowitz and Manski, 1995) upper bound for −∞ yd N (y).

39. A recent example of the use of more restrictive assumptions that rule out extreme or arguably unintuitive
(negative) correlations in the labour supply context is found in the analysis of Blundell, Gosling, Ichimura and Meghir
(2007).
40. See table 4 in Card (1999).
41. But it should be noted that since the baseline characteristics X would no longer be independent of the treatment,
one could no longer use Remark 4 to test the monotonicity assumption.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

statistically ruled out. If there were literally no wage effect, one might expect to see a more
negative impact (perhaps around a −0·058 effect) due to lost labour market experience since
these young applicants are on the steep part of their wage profile.
Another reason to interpret the evidence as pointing to positive wage effects is that the lower
bound is based on an extreme and unintuitive assumption—that wage outcomes are perfectly
negatively correlated with the propensity to be employed. From a purely theoretical standpoint,
a simple labour supply model suggests that, all other things equal, those on the margin of being
employed will have lowest wages not the highest wages (i.e., the “reservation wage” will be
the smallest wage that draws the individual into the labour force).39 In addition, the empirical
evidence in Table 2 suggests that there is positive selection into employment: those who are
predicted to have higher wages are more likely to be employed (i.e., U and V are positively
correlated). If this is true, it seems relatively more plausible to trim the lower rather than the
upper tail of the distribution to get an estimate of the treatment effect.
Second, the intervals provided here are comparable to rates of return found in the returns to
education literature. At week 208, the point estimates an interval of [−0·0118, 0·0889]. Program
participants may be lagging behind their control counterparts by as much as 8 months in labour
market experience due to enrolment in the program. As argued above, this could translate to as
much as a 5·8% wage disadvantage even 4 years after random assignment because many of the
individuals in this sample are still on the steep part of their age-earnings profiles. Projecting to
ages when the wage profile flattens leads to an interval of [0·047, 0·145]. A similar adjustment
for week 90 wages yields an interval tightly centred around 0·10. As found in a survey of studies
that exploit institutional features of school systems (Card, 1999), point estimates of the return
to a single year of schooling range from 0·060 to 0·153.40 Thus, the magnitudes found in this
analysis of the Job Corps are roughly consistent with viewing the program as a human capital
investment of 1 year of schooling.
It should be emphasized that the trimming bounds introduced here are specific neither to selection into employment nor to randomized experiments. For example, outcomes can be missing
due to survey nonresponse (e.g., students not taking tests), sample attrition (e.g., inability to follow individuals over time), or other structural reasons (e.g., mortality). As long as the researcher
believes that the sample selection process can be written as a model like Equation (1) or (5), the
same trimming method can be applied.
 Also, the basis for matching estimators for evaluations is
the weaker assumption that Y1∗ , Y0∗ is independent of D, conditional on X, rather than Assumption
clear that the trimming bounds proposed here can be applied even when
 ∗ 3. ∗It is immediately

Y1 , Y0 , S0, S1 is independent of D, but only conditional on X, as long as Assumption 2b holds
conditional on X. In this situation, the procedure described in Subsection 5.2 can be applied.41

1098

REVIEW OF ECONOMIC STUDIES
TABLE A1

Summary statistics, by treatment status, National Job Corps Study conditional on positive earnings in week 90
Control

Program

Difference

Proportion of nonmissing Mean Proportion of nonmissing Mean Difference

Female
Age at baseline
White, non-Hispanic
Black, non-Hispanic
Hispanic
Other race/ethnicity
Never married
Married
Living together
Separated
Has child
Number of children
Education
Mother’s education
Father’s education
Ever arrested
Household income
<3000
3000–6000
6000–9000
9000–18,000
>18,000
Personal income
<3000
3000–6000
6000–9000
>9000
At baseline
Have job
Months employed, previous year
Had job, previous year
Earnings, previous year
Usual hours/week
Usual weekly earnings
After random assignment
Week 90 ln(wage)
Number of observations

S.E.

1·00
1·00
1·00
1·00
1·00
1·00
0·99
0·99
0·99
0·99
0·99
0·99
0·99
0·83
0·66
0·99

0·429
18·691
0·310
0·447
0·171
0·072
0·909
0·030
0·039
0·022
0·188
0·247
10·381
11·506
11·644
0·238

1·00
1·00
1·00
1·00
1·00
1·00
0·99
0·99
0·99
0·99
1·00
0·99
0·98
0·84
0·67
0·99

0·419
18·729
0·328
0·443
0·167
0·063
0·909
0·023
0·045
0·022
0·178
0·241
10·371
11·579
11·458
0·232

−0·009
0·038
0·018
−0·004
−0·004
−0·009
0·000
−0·007
0·006
0·001
−0·009
−0·007
−0·010
0·072
−0·186
−0·006

0·016
0·068
0·015
0·016
0·012
0·008
0·009
0·005
0·006
0·005
0·012
0·019
0·050
0·090
0·111
0·013

0·68
0·68
0·68
0·68
0·68

0·188
0·188
0·116
0·289
0·219

0·66
0·66
0·66
0·66
0·66

0·202
0·182
0·119
0·270
0·227

0·014
−0·006
0·003
−0·019
0·008

0·015
0·015
0·012
0·017
0·016

0·95
0·95
0·95
0·95

0·726
0·164
0·065
0·045

0.93
0·93
0·93
0·93

0·732
0·154
0·068
0·047

0·005
−0·010
0·003
0·002

0·014
0·012
0·008
0·007

0·98
1·00
0·99
0·94
1·00
1·00

0·251
4·572
0·725
3783·940
24·600
125·147

0·98
1·00
0·99
0·94
1·00
1·00

0·254
4·558
0·727
3699·524
25·165
126·297

1·00
1660

1·827

1·00
2564

1·870

0·002
0·014
−0·013
0·143
0·002
0·014
−84·416 159·333
0·565
0·642
1·150
3·838
0·043*

0·011

Notes: N = 4224. Computations use design weights. Chi-square test of all coefficients equalling zero, from a logit of
the treatment indicator on all baseline characteristics (where mean values were imputed for missing values) yields 19·50;
associated p value from a chi-squared (27 df) distribution is 0·851.
*Indicates difference is statistically significant from 0 at the 5% level.

Proof of Lemma.

See Horowitz and Manski (1995), corollary 4.1.



UB ≡ E Y | D = 1, S = 1, Y ≥ y
Proof
p0 is a sharp upper bound
 ∗ of Proposition 1a. It suffices to show that μ
for E Y1 |S0 = 1, S1 = 1 . A similar argument for the sharp lower bound would follow. Assumptions 1 and 2a imply
Pr[ S0 =0,S1 =1|D=1]
=
. Let F (y) be the c.d.f. of Y conditional on D = 1, S = 1.
that p0 = Pr[S=1|D=1]−Pr[S=1|D=0]
Pr[S=1|D=1]
Pr[S=1|D=1]
Assumption 2a implies that F (y) = p0 M (y) + (1 − p0) N (y), where M (y) denotes the c.d.f. of Y1∗ , conditional on
D = 1, S0 = 0, S1 = 1, and N (y) denotes the c.d.f. of Y1∗ , conditional on D = 1, S0 = 1, S1 = 1. By Assumption 1,
#
#∞
yd F (y) ≥ −∞
yd N (y) =
N (y) is also the c.d.f. of Y1∗ , conditional on S0 = 1, S1 = 1. By the lemma, μUB ≡ 1−1p y∞
p0
0

 ∗
E Y1 |S0 = 1, S1 = 1 .


To show that μUB equals the maximum possible value for E Y1∗ |S0 = 1, S1 = 1 that is consistent with the distribution of the observed data on (Y, S, D), it must be shown that (1) conditional on p0 , μUB is a sharp upper bound and (2)
p0 is uniquely determined by the data. (1) follows from the lemma. (2) is true because the data yield a unique probability
function Pr [S = s, D = d], s, d =
 0, 1, which uniquely determines p0.
UB is contained in any other valid bounds that impose the same assumptions, it sufTo show that LB
0 , 0


UB cannot be ruled out by the observed data, note
fices to show that any  strictly within the interval LB
0 , 0
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

Variable

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1099



that UB
0 ≥E [Y | D = 1, S = 1] − E [Y | D = 0, S = 1] ≥E Y | D = 1, S = 1, Y < y p0 − E [Y | D = 0, S = 1]. ThereUB
fore, for any  between E [Y | D = 1, S = 1]−E [Y | D = 0, S = 1] and 0 , there exists λ ∈ [0, 1] such that  =


λUB
0 + (1 − λ){E Y | D = 1, S = 1, Y < y p0 −E [Y | D = 0, S = 1]}. With this λ , we can construct (1) a density of
∗
∗
Y
1 conditional onS0 = 1, S1 = 1 as λg (y) +(1 − λ)h (y) and (2) a density of Y1 conditional on S0 = 0, S1 = 1 being
observed density of Y conditional on D = 1, S = 1; furthermore, by construction the mean of the constructed density
of Y1∗ conditional on S0 = 1, S1 = 1 minus the control mean yields the proposed . A symmetric argument can be
LB
  in between E [Y | D = 1, S = 1]−E [Y | D = 0, S = 1] and 0 . Therefore, each  within the interval
made about any
UB cannot be ruled out by the observed data.
LB
0 , 0

Q.E.D.

Proof of Proposition 2. It is sufficient to prove consistency for the trimmed mean for the treatment group, and only
for the lower bound, since a symmetric argument will follow for the upper bound. Denote μ0 ≡ E [Y | D = 1, S = 1, Y ≤
y1− p0 ] as the true lower bound of interest. Consistency follows from applying theorem 2.6 of Newey and McFadden
(1994), which applies to GMM estimators. Define the moment function

⎞
(Y − μ) S D · 1 Y ≤ y1− p


⎜  
⎟
⎜ 1 Y > y1− p − p S D ⎟
⎜
⎟


g (z, θ ) ≡ ⎜
⎟,
⎜
⎟
S − α 1−1 p D
⎝
⎠
(S − α) (1 − D)
⎛





where θ = μ, y1− p , p, α , θ0 = μ0 , y1− p0 , p0, α0 , α0 ≡ Pr [S = 1| D = 0], and z = (Y, S, D) . The estimator of μ0 ,

 


 ∗
g (z, θ ) ·
g (z, θ ) .
the lower bound of E Y1 |S0 = 1, S1 = 1 , as provided in Equation (6), is a solution to minθ
From theorem 2.6, (i) holds because as long as E [S| D = 0] > 0 , this just-identified system yields only one solution, (ii)
holds if we take the parameter space to be the bounds of the support for the trimmed mean and quantiles, and [0, 1] to be
the parameter space for the two probabilities α and p , (iii) continuity holds, and bounded support implies (iv). Q.E.D.
Proof of Proposition 3. As in the proof above, it is sufficient to focus only on the asymptotic properties of the
estimator of μ0 . This estimator will be independent of that for the (untrimmed) control group mean. The proof follows
by showing that the conditions
of

 theorem 7.2 of Newey and McFadden (1994) are satisfied.
Define g0 (θ ) ≡ E g (z, θ ) and gn (θ ) ≡ n−1 g (z, θ ). (i) of theorem 7.2 holds. (iii) holds because by assumption,
each of the parameters is in the interior of the parameter space defined in Proposition 2. (iv) holds by the central limit
theorem. Let G be the derivative of g0 (θ ) at θ = θ0. An explicit expression for G, a square matrix, is given below and
will be shown to be nonsingular; hence, (ii) holds as well.
The stochastic equicontinuity condition in (v) can be shown to hold using theorem 1 of Andrews (1994). Assumption C of this theorem holds, and Assumption A holds with envelope M = |Y − Dμ0 | + | D| supμ ||μ0 − μ|| for the first
element and 1 for the remaining elements of g (z, θ ). Boundedness of the support implies E |Y |2+δ < ∞ for some δ > 0,
& &2+δ
< ∞ for some δ > 0, and therefore, Assumption B holds as well.
which implies that E & M &
 −1
, where is
From theorem 7.2 of Newey and McFadden (1994), the asymptotic variance is V LB = G −1 G


(θ
).
After
letting
γ
≡
μ,
y
and
δ
≡
(
p,
α)
,
it
can
be
shown
that
G
can
be
written as
the asymptotic variance
of
g
n
0
1−
p




Gγ Gδ
0
1
LB
the partitioned matrix
can then
and can be partitioned as
. The upper left 2 × 2 block of V
0
Mδ
0


 2 


−1
−1
−1 + G −1 G M
be shown to be equal to G −1
G δ G −1
. The first term contains the variance
δ δ
1 Gγ
2 · Mδ
γ
γ
γ
of the trimmed mean if the trimming proportion p0 is known. The second term captures the variance due to the estimation
of the trimming proportion.
Consider the first term. After computing g0 (θ ), G γ can be shown to equal
E [S D]


− (1 − p0)
0



 

y1− p0 − μ0 f y1− p0
,


− f y1− p0

where f (·) is the density of Y conditional on D = 1, S = 1. 1 is equal to
# y1− p
0
−∞

(y − μ0)2 f (y) d y · E [S D]
0

0



p0 (1 − p0) E [S D]

.

c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

1− p0
1− p0
1− p0
p0 −
p0 λ g (y) + 1 − p0 (1 − λ) h (y), where g (y) is the density of Y conditional on Y ≥ y p0 and h (y)
is the density of Y conditional on Y < y p0 . The mixture of these two latent densities, by construction, replicates the

1100

REVIEW OF ECONOMIC STUDIES



−1 is
It follows that the upper left element of G −1
1 Gγ
γ


 
2 
1
V ar Y | D = 1, S = 1, Y ≤ y1− p0 + y1− p0 − μ0 p0 ,
E [S D](1 − p0)

2=


 α 
α0
0
E [D]
1− p 1 − 1− p
0

0

0

0

−E [D] 1−1p



0

− (1 − E [D])


α0 (1 − α0) (1 − E [D])

.





−1
−1 G G −1 is equal to
After simplifying terms, it follows that the upper left element of G −1
2 Mδ
γ G δ Mδ
γ
δ

⎛ 
⎞
α
1 − 1−0p

2
)
(1
−
α
0
0
⎠,
+

y1− p0 − μ0 ⎝
α0
(1 − E [D]) α0
E [ D]
1− p0

as stated in Equation (7), after substituting in V p .



Finally, direct computation of the upper left element of Mδ−1 2 Mδ−1 yields the expression for V p .

Q.E.D.

Proof of Consistency When p0 = 0. Assume p0 = 0. We know that as long as E |Y | < ∞, the untrimmed treatment
to &show that for any
effect estimator
 converges
to the true treatment&effect 0 &. It isthus sufficient
&

&
&
 δ> 0, we have

&
&
&
&
&
&
lim Pr &LB − & < δ = 1. First, note that Pr &LB − & < δ = Pr &LB − & < δ|0 ≤ p̂ ≤ p Pr 0 ≤ p̂ ≤ p +
n→∞
&
&
&
&
&
 

 
&


&
&
&
&
&
&
Pr &LB − & < δ| p̂ > p Pr p̂ > p + Pr &LB − & < δ| 0 > p̂ ≥ p ∗ Pr 0 > p̂ ≥ p ∗ + Pr &LB − & < δ| p̂ < p ∗


Pr p̂ < p ∗ . Since p0 = 0, for any positive p and negative p ∗ , the second and fourth terms converge to zero.
LB
Now consider the first term. Let p be any positive value such that 0 − LB
p < δ, where  p is the population
p.
Now
note
that
for
any
sample
indexed
by N, we have
trimmed
mean
after
trimming
the
top
tail
by
the
proportion
&
&

 #
&
&
&
&
&
&
p
Pr &LB − & < δ|0 ≤ p̂ ≤ p = 0 Pr &LB − & < δ| p̂ = p d F N ( p), where F N is the c.d.f. of p̂ conditional on
&
&
&
&

&
&
&
&
0 ≤ p̂ ≤ p. For any realization of the data, &LB − & is nondecreasing in p̂. Therefore, Pr &LB − & < δ| p̂ = p is
&
&


# p &&
# p &&
&
&
nonincreasing in p̂. It follows that 0 Pr &LB − & < δ| p̂ = p d F N ( p) ≥ 0 Pr &LB − & < δ| p̂ = p d F N ( p)
&
&
&
&
= Pr[&LB − & < δ| p̂ = p], which converges to 1, by construction of p. Pr[0 ≤ p̂ ≤ p] converges to 0·5 and therefore
so does the first term above. A parallel argument shows the third term converges to 0·5 as well. Q.E.D.

Acknowledgements. Earlier drafts of this paper were circulated as “Trimming for Bounds on Treatment Effects
with Missing Outcomes”, Center for Labor Economics Working Paper No. 51, March 2002, and NBER Technical Working Paper No. 277, June 2002, as well as a revision with the above title, as NBER Working Paper No. 11721, October
2005. Emily Buchsbaum, Vivian Hwa, Xiaotong Niu, and Zhuan Pei provided excellent research assistance. I thank David
Card, Guido Imbens, Justin McCrary, Marcelo Moreira, Enrico Moretti, Jim Powell, Jesse Rothstein, Mark Watson, and
Edward Vytlacil for helpful discussions and David Autor, Josh Angrist, John DiNardo, Jonah Gelbach, Alan Krueger,
Doug Miller, Aviv Nevo, Jack Porter, Diane Whitmore, and participants of the UC Berkeley Econometrics and Labor
Lunches for useful comments and suggestions.

REFERENCES
AHN, H. and POWELL, J. (1993), “Semiparametric Estimation of Censored Selection Models with a Nonparametric
Selection Mechanism”, Journal of Econometrics, 58, 3–29.
ANDREWS, D. W. K. (1994), “Empirical Process Methods in Econometrics”, in R. F. Engle and D. L. McFadden (eds.)
Handbook of Eonometrics, Vol. 4 (Amsterdam: North Holland) 2248–2296.
ANDREWS, D., MOREIRA, M. J. and STOCK, J. H. (2007), “Performance of Conditional Wald Tests in IV Regression
with Weak Instruments”, Journal of Econometrics, 139, 116–132.
ANDREWS, D. and SCHAFGANS, M. (1998), “Semiparametric Estimation of the Intercept of a Sample Selection
Model”, Review of Economic Studies, 65, 497–517.
ANGRIST, J. (1997), “Conditional Independence in Sample Selection Models”, Economics Letters, 54, 103–112.
c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

as stated in Equation (7).
Consider the second term. Direct calculation of G δ , Mδ , and 2 yields



1
0 0
−E [D] α0
(1− p0 )2
G δ = E [S D]
, Mδ =
−1 0
0

LEE

ESTIMATING SHARP BOUNDS ON TREATMENT EFFECTS

1101

c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

ANGRIST, J., IMBENS, G. and RUBIN, D. (1996), “Identification of Causal Effects Using Instrumental Variables”,
Journal of the American Statistical Association, 91, 444–455.
BALKE, A. and PEARL, J. (1997), “Bounds on Treatment Effects from Studies with Imperfect Compliance”, Journal
of the American Statistical Association, 92, 1171–1177.
BARNOW, B. (1987), “The Impact of CETA on the Post-Program Earnings of Participants”, Journal of Human Resources, 22, 157–193.
BLOOM, H. (1984), “Accounting for No-Shows in Experimental Evaluation Designs”, Evaluation Review, 8, 225–246.
BLUNDELL, R., GOSLING, A., ICHIMURA, H. and MEGHIR, C. (2007), “Changes in the Distribution of Male and
Female Wages Accounting for Employment Composition Using Bounds”, Econometrica, 75, 323–363.
BURGHARDT, J., SCHOCHET, P. Z., MCCONNELL, S, JOHNSON, T., GRITZ, R. M., GLAZERMAN, S.,
HOMRIGHAUSEN, J. and JACKSON, R. (2001), Does Job Corps Work? Summary of the National Job Corps
Study, Report (Washington, DC: Mathematica Policy Research, Inc.).
CARD, D. (1999), “The Causal Effect of Education on Earnings”, in O. Ashenfelter and D. Card (eds.) Handbook of
Labor Economics, Vol. 3A (Amsterdam: North Holland) 1801–1863.
CHAMBERLAIN, G. (1994), “Quantile Regression, Censoring, and the Structure of Wages”, in C. A. Sims (ed.) Advances in Econometrics, Sixth World Congress, Vol. 1 (Cambridge: Cambridge University Press) 171–220.
DAS, M., NEWEY, W. K. and VELLA, F. (2003), “Nonparametric Estimation of Sample Selection Models”, Review of
Economic Studies, 70, 33–58.
HECKMAN, J. J. (1974), “Shadow Prices, Market Wages, and Labor Supply”, Econometrica, 42, 679–694.
HECKMAN, J. J. (1979), “Sample Selection Bias as a Specification Error”, Econometrica, 47, 153–161.
HECKMAN, J. J. (1990), “Varieties of Selection Bias”, American Economic Review, 80, 313–318.
HECKMAN, J. J., LALONDE, R. J. and SMITH, J. A. (1999), “The Economics and Econometrics of Active Labor
Market Programs”, in O. Ashenfelter and D. Card (eds.) Handbook of Labor Economics, Vol. 3A (Amsterdam:
North Holland) 1865–2097.
HECKMAN, J. J. and ROBB, R. (1986), “Alternative Methods for Solving the Problem of Selection Bias in Evaluating
the Impact of Treatments on Outcomes”, in H. Wainer (ed.) Drawing Inferences from Self-selected Samples (New
York: Springer) 63–107.
HECKMAN, J. J. and VYTLACIL, E. (1999), “Local Instrumental Variables and Semiparametric Estimation and
Latent Variable Models for Identifying and Bounding Treatment Effects”, Proceedings of the National Academy
of Sciences, 96, 4730–4734.
HECKMAN, J. J. and VYTLACIL, E. (2000a), “Instrumental Variables, Selection Models, and Tight Bounds on the
Average Treatment Effect” (Technical Working Paper No. 259, National Bureau of Economic Research).
HECKMAN, J. J. and VYTLACIL, E. (2000b), “Local Instrumental Variables” (Technical Working Paper No. 252,
National Bureau of Economic Research).
HOLLISTER, R., KEMPER, P. and MAYNARD, R. (1984), The National Supported Work Demonstration (Madison,
WI: University of Wisconsin Press).
HOROWITZ, J. L. and MANSKI, C. F. (1995), “Identification and Robustness with Contaminated and Corrupted Data”,
Econometrica, 63, 281–302.
HOROWITZ, J. L. and MANSKI, C. F. (2000a), “Nonparametric Analysis of Randomized Experiments with Missing
Covariate and Outcome Data”, Journal of the American Statistical Association, 95, 77–84.
HOROWITZ, J. L. and MANSKI, C. F. (2000b), “Rejoinder: Nonparametric Analysis of Randomized Experiments with
Missing Covariate and Outcome Data”, Journal of the American Statistical Association, 95, 87.
IMBENS, G. W. and ANGRIST, J. (1994), “Identification and Estimation of Local Average Treatment Effects”, Econometrica, 62, 467–476.
IMBENS, G. W. and Manski, C. F. (2004), “Confidence Intervals for Partially Identified Parameters”, Econometrica, 72,
1845–1857.
KIEFER, N. (1979), The Economic Benefits of Four Employment and Training Programs (New York: Garland
Publishing).
KRUEGER, A. B. and WHITMORE, D. M. (2001), “The Effect of Attending a Small Class in the Early Grades on
College-Test Taking and Middle School Test Results: Evidence from Project STAR”, Economic Journal, 111,
1–28.
LEE, D. S. (2002), “Trimming for Bounds on Treatment Effects with Missing Outcomes” (Center for Labor Economics
Working Paper No. 38, University of California, Berkeley).
LEE, D. S. (2005), “Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects” (NBER
Working Paper No. 11721, National Bureau of Economic Research).
MARTIN, J. P. (2000), “What Works among Active Labour Market Policies: Evidence from OECD Countries’ Experiences”, OECD Economic Studies, 30, 79–113.
NEWEY, W. K. and MCFADDEN, D. (1994), “Large Sample Estimation and Hypothesis Testing”, in R. F. Engle and D.
L. McFadden (eds.) Handbook of Eonometrics, Vol. 4 (Amsterdam: North Holland) 2113–2245.
ROBINS, J. (1989), “The Analysis of Randomized and Non-Randomized AIDS Treatment Trials Using a New Approach
to Causal Inference in Longitudinal Studies”, in L. Sechrest, H. Freeman and A. Mulley (eds.) Health Service
Research Methodology: A Focus on AIDS (Washington, DC: U.S. Public Health Service) 113–159.
RUBIN, D. (1976), “Inference and Missing Data”, Biometrika, 63, 581–592.

1102

REVIEW OF ECONOMIC STUDIES

c 2009 The Review of Economic Studies Limited


Downloaded from https://academic.oup.com/restud/article-abstract/76/3/1071/1590707 by Harvard Library user on 20 February 2019

SCHOCHET, P. Z., BURGHARDT, J. and GLAZERMAN, S. (2001), “National Job Corps Study: The Impacts of
Job Corps on Participants’ Employment and Related Outcomes”, Report (Washington, DC: Mathematica Policy
Research, Inc.).
SCHOCHET, P. Z., CAO, J. B. R.-J., GLAZERMAN, S., GRADY, A., GRITZ, M., MCCONNELL, S., JOHNSON, T.
and BURGHARDT, J. (2003), “National Job Corps Study: Data Documentation and Public Use Files, Volume I,”
Documentation (Washington, DC: Mathematica Policy Research, Inc.).
SMITH, J. P. and WELCH, F. R. (1986), Closing the Gap: Forty Years of Economic Progress for Blacks (Santa Monica,
CA: Rand Corporation).
STAIGER, D. and STOCK, J. H. (1997), “Instrumental Variables regression with Weak Instruments”, Econometrica, 65,
557–586.
STIGLER, S. M. (1973), “The Asymptotic Distribution of the Trimmed Mean”, Annals of Statistics, 1, 472–477.
U.S. DEPARTMENT OF LABOR (2005a), “Summary of Budget Authority: Fiscal Years 2004-2005” (Table, Employment and Training Administration 2).
U.S. DEPARTMENT OF LABOR (2005b), “What is Job Corps?” (Web Page, Employment and Training Administration,
http://jobcorps. doleta.gov/about.cfm).
VYTLACIL, E. (2002), “Independence, Monotonicity, and Latent Index Models: An Equivalence Result”, Econometrica, 70, 331–341.
ZHANG, J. L. and RUBIN, D. B. (2003), “Estimation of Causal Effects via Principal Stratification When Some Outcomes
Are Truncated by ‘Death’”, Journal of Educational and Behavioral Statistics, 28, 353–368.
ZHANG, J. L., RUBIN, D. B. and MEALLI, F. (2008), “Evaluating the Effects of Job Training Programs on Wages
through Principal Stratification”, in D. Millimet, J. Smith and E. Vytlacil (eds.) Modelling and Evaluating Treatment Effects in Econometrics, Vol. 21 (Amsterdam: Elsevier) 119–147.

