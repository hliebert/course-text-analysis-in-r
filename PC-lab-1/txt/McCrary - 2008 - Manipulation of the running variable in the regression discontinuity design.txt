ARTICLE IN PRESS

Journal of Econometrics 142 (2008) 698–714
www.elsevier.com/locate/jeconom

Manipulation of the running variable in the regression
discontinuity design: A density test
Justin McCrary
University of Michigan, 735 S. State St. # 5220, Ann Arbor, MI 48109, USA
Available online 29 May 2007

Abstract
Standard sufﬁcient conditions for identiﬁcation in the regression discontinuity design are continuity of the conditional
expectation of counterfactual outcomes in the running variable. These continuity assumptions may not be plausible if
agents are able to manipulate the running variable. This paper develops a test of manipulation related to continuity of the
running variable density function. The methodology is applied to popular elections to the House of Representatives, where
sorting is neither expected nor found, and to roll call voting in the House, where sorting is both expected and found.
r 2007 Elsevier B.V. All rights reserved.
JEL classification: C14
Keywords: Regression discontinuity design; Local linear density estimator

1. Introduction
One reason for the increasing popularity in economics of regression discontinuity applications is the
perception that the identifying assumptions are quite weak. However, while some applications of the design
can be highly persuasive, many are subject to the criticism that public knowledge of the treatment assignment
rule may invalidate the continuity assumptions at the heart of identiﬁcation.
Consider a hypothetical example. A doctor plans to randomly assign heart patients to a statin and a placebo
to study the effect of the statin on heart attack within 10 years. The doctor randomly assigns patients to two
different waiting rooms, A and B, and plans to give those in A the statin and those in B the placebo. If some of
the patients learn of the planned treatment assignment mechanism, we would expect them to proceed to
waiting room A. If the doctor fails to divine the patients’ contrivance and follows the original protocol,
random assignment of patients to separate waiting rooms may be undone by patient sorting after random
assignment. In the regression discontinuity context, an analogous evaluation problem may occur in the
common case where the treatment assignment rule is public knowledge (cf., Lee, 2007).
In this paper, I propose a formal test for sorting of this type. The test is based on the intuition that, in the
example above, we would expect for waiting room A to become crowded. In the regression discontinuity
Tel.: +1 734 615 7549; fax: +1 734 763 9181.

E-mail address: jmccrary@umich.edu
0304-4076/$ - see front matter r 2007 Elsevier B.V. All rights reserved.
doi:10.1016/j.jeconom.2007.05.005

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

699

context, this is analogous to expecting the running variable to be discontinuous at the cutoff, with surprisingly
many individuals just barely qualifying for a desirable treatment assignment and surprisingly few failing to
quality. This test will be informative when manipulation of the running variable is monotonic, in a sense to be
made speciﬁc below.
The proposed test is based on an estimator for the discontinuity at the cutoff in the density function of the
running variable. The test is implemented as a Wald test of the null hypothesis that the discontinuity is zero.
The estimator, which is a simple extension of the local linear density estimator (Cheng et al., 1997), proceeds in
two steps. In the ﬁrst step, one obtains a ﬁnely gridded histogram. In the second step, one smooths the
histogram using local linear regression, separately on either side of the cutoff. To efﬁciently convey sensitivity
of the discontinuity estimate to smoothing assumptions, one may augment a graphical presentation of the
second-step smoother with the ﬁrst-step histogram, analogous to presenting local averages along with an
estimated conditional expectation.
This test complements existing speciﬁcation checks in regression discontinuity applications. Authors
routinely report on the smoothness of pre-determined characteristics around the cutoff (e.g., DiNardo and
Lee, 2004). If the particular pre-determined characteristics the researcher has at disposal are relevant to the
problem, this method should be informative about any sorting around the discontinuity. However, in some
applications pre-determined characteristics are either not available, or those which are available are not
relevant to the outcome under study. By way of contrast, the density test may always be conducted, since data
on the running variable is required for any analysis. The method is also useful in applications where a
discontinuous density function is itself the object of interest. For example, Saez (1999, 2002) measures tax
avoidance using the discontinuity in the density of income reported to the Internal Revenue Service.
To show how the estimator works in practice, I apply the methodology to two distinct settings. The ﬁrst
setting is popular elections to the United States House of Representatives, considered in Lee’s (2001, 2007)
incumbency study. In this context, it is natural to assume that the density function of the democratic vote
share is continuous at 50%. The data do not reject this prediction.1 The second setting is roll call votes in the
House. In this context, the vote tally for a given bill is expected to be subject to manipulation. Although
the number of representatives would seem to make coordination between members difﬁcult, these problems
are overcome by a combination of the repeated game aspect of roll call votes and the fact that a
representative’s actual vote becomes public knowledge, enabling credible commitments and vote contracting.
In this setting, the density test provides strong evidence of manipulation.
The remainder of the paper is organized as follows. Section 2 deﬁnes manipulation and distinguishes
between partial and complete manipulation. Section 3 describes the estimator and discusses smoothing
parameter methods and inference procedures. Section 4 motivates the manipulation problem with a
hypothetical job training program. Section 5 presents the results of a small simulation study. Section 6
presents the empirical analysis, and Section 7 concludes. Appendix A gives a proof of the proposition of
Section 3, and Appendix B describes the data.
2. Identiﬁcation under partial and complete manipulation
Let Y i denote an outcome and Di a binary treatment. The outcome depends on treatment according to
Y i ¼ ai þ bi Di ¼ a þ bDi þ i ,

(1)

where ai and bi are random variables with means a and b, respectively, and i ¼ ai  a þ ðbi  bÞDi (cf.,
appendices of Card, 1999). In counterfactual notation, ai ¼ Y i0 and bi ¼ Y i1  Y i0 , where Y i0 is the outcome
that would obtain, were Di ¼ 0, and Y i1 is the outcome that would obtain, were Di ¼ 1. Eq. (1) is viewed as a
structural equation, in the sense that the manner in which i is induced into participation in the program does
not affect ðY i0 ; Y i1 Þ under exogeneity.2 As noted by Hahn et al. (2001, hereinafter HTV), and following
Imbens and Angrist (1994), the average bi for a speciﬁc subpopulation is identiﬁable under continuity of the
1

However, see Snyder (2005).
This is Heckman’s (2005, p. 11) assumption (A-2). In the statistics literature, this is subsumed under the stable unit treatment value
assumption (SUTVA). See Rubin (1980, 1986). I also abstract from general equilibrium effects.
2

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

700

conditional expectations of ai and bi , given an underlying index. This index is here termed the ‘‘running
variable’’ and denoted Ri .
Underlying Ri is an unobservable index Ri0 that is the running variable that would obtain, were there no
program. This may be different from Ri . The running variable is manipulated when Ri aRi0 . Although Ri0 is
not observed, it is well-deﬁned conceptually. For example, van der Klaauw (2002) studies the impact of
scholarships on students’ enrollment decisions, where scholarships are assigned discontinuously on the basis of
a linear combination of SAT and high school grade point average (GPA). It is straightforward to
conceptualize of the linear combination of the ith student’s SAT and GPA that would obtain, if the university
in question did not run such a scholarship program.
I interpret the identiﬁcation results of HTV as holding under continuity assumptions pertaining to the
unobservable index Ri0 . Formally, throughout the paper I assume that
E½ai jRi0 ¼ r;

E½bi jRi0 ¼ r;

and

f Ri0 ðrÞ are continuous in r,

(A0)

where f Ri0 ðrÞ is the density of Ri0 . Although this assumption is very weak, it is sufﬁcient for a regression
discontinuity estimator based on the index Ri0 to identify a local average treatment effect.3 The conditional
expectation restrictions in (A0) are HTV’s identiﬁcation assumptions, but (A0) is stronger than their
assumptions because of the additional restriction that the density function be continuous. For most settings in
which continuity of the conditional expectations is plausible, continuity of the density will be plausible.
If there is no manipulation, then (A0) holds with Ri replacing Ri0 , and identiﬁcation of meaningful
parameters can be obtained. Sufﬁcient conditions for lack of manipulation include timing, such as when the
program is announced simultaneously with implementation, and lack of agent interest in obtaining any
particular training assignment, for example. However, when individuals know of the selection rule for
treatment, are interested in being treated, and have time to adjust their behavior accordingly, manipulation
can be important. In Section 4, below, I give an example of a job training program where manipulation is
expected and show how manipulation leads to erroneous inferences. The density test detects manipulation
easily in this setting. In the example, the identiﬁcation problem arises because the incentives of the program
lead to sorting on the running variable. Generally, manipulation can lead E½ai jRi ¼ r and E½bi jRi ¼ r to be
discontinuous at the cutoff, despite continuity of E½ai jRi0 ¼ r and E½bi jRi0 ¼ r.
Only some varieties of manipulation lead to identiﬁcation problems. I draw a distinction between partial
and complete manipulation. Partial manipulation occurs when the running variable is under the agent’s
control, but also has an idiosyncratic element. Typically, partial manipulation of the running variable does not
lead to identiﬁcation problems. Examples of regression discontinuity settings where partial manipulation is
arguably plausible include van der Klaauw (2002) and DiNardo and Lee (2004), for example.4 Complete
manipulation occurs when the running variable is entirely under the agent’s control. Typically, complete
manipulation of the running variable does lead to identiﬁcation problems. Examples of regression
discontinuity settings in which complete manipulation is a potential threat to validity include Hahn et al.
(1999) and Jacob and Lefgren (2004), for example.5
3

For discussion of the local average treatment effect parameter, see Angrist et al. (1996) and Heckman et al. (2006), for example.
van der Klaauw (2002) studies the effect of scholarships on enrollment for a college that assigns scholarships discontinuously using an
index that is a linear combination of SAT score and high school grade point average (p. 1255). van der Klaauw does not state whether
students could have had prior knowledge of the formula used, but it seems plausible that even if they had, it would be difﬁcult to control
precisely the value of such an index. Similarly, it might be difﬁcult to control one’s grade point average perfectly. DiNardo and Lee (2004)
study the impact of unionization on establishment outcomes. Firms become unionized based on a majority vote of the employees. While
ﬁrms and unions certainly attempt to manipulate the vote tally, it would be difﬁcult for either to do so perfectly, particularly since union
certiﬁcation elections are secret ballot.
5
Hahn et al. (1999) study the impact of equal employment opportunity laws on employment of racial minorities, taking advantage of the
fact that the 1964 Civil Rights Act, as amended, covers only those ﬁrms with 15 or more employees. Employers presumably maintain
perfect control over labor inputs. This raises the possibility that a ﬁrm owner with a taste for discrimination, who would otherwise ﬁnd it
proﬁt-maximizing to employ 15, 16, or 17 employees, for example, would elect to employ 14 employees to preclude the possibility of
litigation alleging violations of the Civil Rights Act (cf., Becker, 1957). Jacob and Lefgren (2004) study the impact of summer school and
grade retention on test scores, where the treatments depend discontinuously on separate pre-tests. In that context, because the treatment
assignment rule is public knowledge, it is possible that those grading the pre-test would be motivated to inﬂuence a student’s treatment
assignment by strategically mismeasuring the student’s actual score (see authors’ discussion, p. 231).
4

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

701

Propositions 2 and 3 of Lee (2007) establish that, under mild regularity conditions, identiﬁcation of
meaningful parameters can be obtained under partial manipulation. As Lee notes, the critical assumption
underlying both propositions is that the conditional density function f RjW ðrjwÞ be continuous in r, where W
represents potential confounders (‘‘types’’). This is an intuitive identifying assumption: if the running variable
has a continuous density conditional on type, then for every type of person the chance of a running variable
draw just above the cutoff is equal to the chance of a running variable draw just below the cutoff. The
assumption is not directly testable, since types are unobserved. However, Lee stresses the important idea that
this assumption implies continuity of the conditional expectation of any baseline characteristic in the running
variable. It is thus easy to test the identifying assumption using standard estimators for conditional
expectations, such as local linear regression or global polynomial regression. Such tests are already commonly
reported in applications.
This paper develops a complementary testing procedure. The idea of the test is that continuity in r of the
conditional density f RjW ðrjwÞ implies continuity of f R ðrÞ, the density of the running variable. Thus, a natural
speciﬁcation test in applications is a test of the continuity of the running variable density function.
The density test may not be informative unless the existence of the program induces agents to adjust the
running variable in one direction only. Manipulation is monotonic if either Ri XRi0 for all i or Ri pRi0 for all i.
Consider a hypothetical example based on the Jacob and Lefgren (2004) study, in which the probability of
summer school is a discontinuous function of test scores, and teachers are in charge of grading examinations
for summer school. Assume students attend summer school if and only if assigned to attend, so that in the
absence of manipulation, the local average treatment effect equals the average treatment effect (ATE). Let the
ATE be zero, but assume students have heterogeneous treatment effects of summer school; summer school
helps half and harms half. Teachers discern these treatment effects, and manipulate the scores of those who
would be helped and who just barely passed, so that they fail and have to go to summer school. Similarly,
teachers manipulate the scores of those who would be harmed and who just barely failed, so that they pass and
avoid going to summer school. Estimated treatment effects of the program would be positive, because of
teacher manipulation of scores. However, because the manipulation is non-monotonic, and because those
whose scores are adjusted up are equally numerous as those whose scores are adjusted down, the density test
will fail to detect manipulation.
The density test could also fail, even when there is no failure of identiﬁcation. Assume teachers give bonus
points to some of those who just barely fail the exam (perhaps to reduce the size of summer school classes),
and subtract points from no student. Then the density test would suggest a failure of identiﬁcation. However,
if teachers select at random which students receive bonus points, then an ATE would be identiﬁed. These
examples clarify that a running variable with a continuous density is neither necessary nor sufﬁcient for
identiﬁcation except under auxiliary assumptions.6
3. Estimation and inference procedures
To estimate potentially discontinuous density functions, economists have used either traditional histogram
techniques (DiNardo and Lee, 2004; Saez, 2002), or kernel density estimates which smooth over the point of
potential discontinuity (DiNardo et al., 1996; Saez, 1999; Jacob and Lefgren, 2004). Neither procedure allows
for point estimation or inference. One could estimate a kernel density function separately for points to the left
and right of the point of discontinuity, but at boundaries a kernel density estimator is badly biased, as is wellknown (e.g., Marron and Ruppert, 1994).
One method that corrects for boundary bias is the local linear density estimator developed by Cheng et al.
(1993) and Cheng (1994).7,8 The grounds for focusing on the local linear density estimator are theoretical and
6

I thank the editors for their emphasis of this important point.
Published papers describing the local linear density approach include Fan and Gijbels (1996), Cheng (1997a, b), and Cheng et al.
(1997). The general idea of ‘‘pre-binning’’ the data before density estimation, and the conclusion that estimators based on pre-binned data
do not suffer in terms of practical performance despite theoretical loss of information, are both much older than the idea of local linear
density estimation; see, for example, Jones (1989) and references therein.
8
Competing estimators for estimating a density function at a boundary are also available. Estimators from the statistics literature
include modiﬁed kernel methods (see, e.g., Chu and Cheng, 1996; Cline and Hart, 1991) and wavelet methods (for references, see Hall et
7

ARTICLE IN PRESS
702

J. McCrary / Journal of Econometrics 142 (2008) 698–714

practical. Theoretically, the estimator weakly dominates other proposed methods. Cheng et al. (1997) show
that for a boundary point the local linear method is 100% efﬁcient among linear estimators in a minimax
sense.9 Practically, the ﬁrst-step histogram is of interest in its own right, because it provides an analogue to the
local averages typically accompanying conditional expectation estimates in regression discontinuity
applications. Moreover, among nonparametric methods showing good performance at boundaries, local
linear density estimation is simplest.

3.1. Estimation
Implementing the local linear density estimator involves two steps. The ﬁrst step is a very undersmoothed
histogram. The bins for the histogram are deﬁned carefully enough that no one histogram bin includes points
both to the left and right of the point of discontinuity. The second step is local linear smoothing of the
histogram. The midpoints of the histogram bins are treated as a regressor, and the normalized counts of
the number of observations falling into the bins are treated as an outcome variable. To accommodate the
potential discontinuity in the density, local linear smoothing is conducted separately for the bins to the right
and left of the point of potential discontinuity, here denoted c.
The ﬁrst-step histogram is based on the frequency table of a discretized version of the running variable,




Ri  c
b
b
b
b
b
b
b
gðRi Þ ¼
(2)
b þ þ c 2 ...;c  5 ;c  3 ;c  ;c þ ;c þ 3 ;c þ 5 ;... ,
b
2
2
2
2
2
2
2
where bac is the greatest integer in a.10,11 Deﬁne an equi-spaced grid X 1 ; X 2 ; . . . ;P
X J of width b covering the
support of gðRi Þ and deﬁne the (normalized) cellsize for the jth bin, Y j ¼ ð1=nbÞ ni¼1 1ðgðRi Þ ¼ X j Þ.12,13 The
ﬁrst-step histogram is the scatterplot ðX j ; Y j Þ. The second step smooths the histogram using local linear
b , where ðf
b ;f
b Þ minimize Lðf ; f ; rÞ ¼
regression. Formally, the density estimate at r is given by fbðrÞ ¼ f
1
1
2
1
2
PJ
2
fY

f

f
ðX

rÞg
KððX

rÞ=hÞf1ðX
4cÞ1ðrXcÞ
þ
1ðX
ocÞ1ðrocÞg,
KðÞ
is
a
kernel
function,
here
j
j
j
j
j
1
2
j¼1
chosen as the triangle kernel KðtÞ ¼ maxf0; 1  jtjg, and h is the bandwidth, or the window width deﬁning
which observations are included in the regression.14 In words, the second step smooths the histogram by
estimating a weighted regression using the bin midpoints to explain the height of the bins, giving most weight
(footnote continued)
al., 1996). Among the better-known methods, one with good properties is Rice (1984). Boundary folding methods are also used (see, for
example, Schuster, 1985), but their properties are not favorable. Marron and Ruppert (1994) give a three-step transformation method. An
older method with favorable properties is the smoothed histogram approach developed by Gawronski and Stadtmüller (1980, 1981) and
recently explored by Bouezmarni and Scaillet (2005). These last authors also discuss the use of asymmetric kernels for circumventing the
boundary bias of kernel estimators. Bouezmarni and Scaillet appear to be the ﬁrst authors in economics to estimate a density function at a
boundary using a nonparametric method, but they do not discuss local linear density estimation. Parametric models involving
discontinuous density functions have been studied extensively in economics; see Aigner et al. (1976) for an early paper and Chernozhukov
and Hong (2004) for references.
9
Fan and Gijbels (1996) give a good discussion of this result and discuss further results regarding deeper senses of efﬁciency.
10
The greatest integer in a is the unique integer k such that kpaok þ 1 (‘‘round to the left’’). In software, this is typically known as the
floor function, which is not the same as the int function, because negatives are handled differently.
11
Eq. (2) will result in observations with Ri ¼ c being assigned to the bin c þ b=2, which is valid if ties are assigned to treatment. If ties
are assigned to control, re-deﬁne gðRi Þ ¼ dðRi  cÞ=beb  b=2 þ c, where dae is 1 plus the greatest integer in a.
12
Deﬁning a grid covering the support of gðRi Þ is necessary to account for ‘‘zero count’’ bins.
13
Note that these values of X j are deterministic in the sense that c and b are treated as constants. The endpoint X 1 (X J ) may always be
chosen arbitrarily small (large) so that it is well beyond the support of gðRi Þ with no consequences for estimation of the overall density
anywhere within ½R þh; R  h, where ½R; R is the support of the original Ri . Thus, without loss of generality, we may also deﬁne
X j ¼ l þ ðj  1Þb, where l ¼ bðRmin  cÞ=bcb þ ðb=2Þ þ c, and J ¼ bðRmax  Rmin Þ=bc þ 2. However, if global polynomial ﬁtting is used, as
in the automatic bandwidth selector discussed in Section 3.2, below, then the grid should fall strictly in the range ½R; R. This is not
necessary if modeling a density with unbounded support.
14
Given its generally minimal role in performance, the kernel function may be chosen on the basis of convenience. However, the triangle
kernel is boundary optimal (Cheng et al., 1997). At interior points, where the Epanechnikov kernel KðtÞ ¼ maxf0; 0:75ð1  t2 Þg is optimal,
the local linear density estimator is primarily used for graphical purposes and informal inference. Hence there is little cost to using the
triangle kernel everywhere, and this is the convention I adopt for Sections 4–6, below.

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

703

to the bins nearest where one is trying to estimate the density. It is straightforward to estimate the entire
density function, f ðrÞ, by looping over evaluation points r.
Deﬁne the parameter of interest to be the log difference in height, or
y ¼ ln lim f ðrÞ  ln lim f ðrÞ  ln f þ  ln f  .
r#c

r"c

(3)

While one can estimate f þ and f  using fbðrÞ for r just above and below c, respectively, it is easier and more
accurate to estimate two separate local linear regressions, one on either side of c, with X j  c as regressor. The
log difference of the coefﬁcients on the intercepts then estimates y. Formally,
b
y  ln fbþ  ln fb
8
9
< X X  c S þ  Sþ ðX  cÞ =
j
j
n;2
n;1
¼ ln
K
Yj
2
þ þ
:X 4c
;
h
S n;2 Sn;0  ðS þ
n;1 Þ
j
8
9
< X X  c S   S  ðX  cÞ =
j
j
n;2
n;1
 ln
K
Yj ,
 
 2
:X oc
;
h
S

ðS
S
n;2 n;0
n;1 Þ
j

P
k
 cÞ=hÞðX j  cÞk and S 
n;k ¼
X j oc KððX j  cÞ=hÞðX j  cÞ . Under standard nonparametric regularity conditions, b
y is consistent and asymptotically normal.

where S þ
n;k ¼

P

ð4Þ

X j 4c KððX j

Proposition. Let f ðÞ be a density function which, everywhere except at c, has three continuous and bounded
derivatives.
Let KðtÞ ¼ maxf0; 1  jtjg be the triangle kernel, and suppose that h ! 0, nh ! 1, b=h ! 0, and
pﬃﬃﬃﬃﬃ
h2 nh!H 2 ½0; 1Þ. Then if R1 ; R2 ; . . . ; Rn is a random sample with density f ðrÞ,





pﬃﬃﬃﬃﬃ
24 1
1
H f þ00 f 00
d
b
nhðy  yÞ ! N B;
þ
  .
where B ¼
5 fþ f
20 f þ
f
The proof, given in Appendix A, builds on an unpublished proof of Cheng (1994).
The proposition implies an approximate standard error for b
y of
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ﬃ
!
u
u 1 24 1
1
t
b
.
sy ¼
þ
nh 5 fbþ fb

(5)

As shown in the simulation study in Section 5, below, t-tests constructed using this standard error are very
nearly normally distributed under the null hypothesis.
However, the normal distribution in question is not quite centered at zero if the bandwidth is of order n1=5 ,
the rate which minimizes the asymptotic mean-squared error. This is typical of a nonparametric setting; a
tuning parameter that is good for estimation purposes is not necessarily good for testing purposes (Pagan and
Ullah, 1999). Practically, this means that a conﬁdence region for b
y constructed using the standard error above
b
will give good coverage accuracy for the probability limit of y, as opposed to good coverage accuracy for y.
Two approaches are taken in the literature to circumvent this problem. First, relative to a bandwidth which is
believed to minimize the mean-squared error, one can choose a bandwidth smaller than that. The hope is that
the bias is thereby sufﬁciently reduced that it may be ignored. Second, one can estimate the bias.15 This bells
the cat in that it requires choosing another bandwidth. Following Horowitz (2001) and Hall (1992), I focus on
undersmoothing. A simple undersmoothing method is to take a reference bandwidth and to divide it by 2

15

In their survey, Härdle and Linton (1994) discuss only undersmoothing. Pagan and Ullah (1999) discuss a variety of procedures, but
do not provide recommendations. In the related context of local linear regression, Fan and Gijbels (1996) recommend estimating the bias
using a two-step procedure; a pilot bandwidth is required for this procedure. Davison and Hinkley (1997) suggest the use of the bootstrap
to estimate the bias of the kernel density estimator, but Hall (1992) shows that this method performs badly.

ARTICLE IN PRESS
704

J. McCrary / Journal of Econometrics 142 (2008) 698–714

(Hall, 1992). Section 5 presents simulation evidence on the success of this strategy in connection with the
reference bandwidth described in the next subsection.
3.2. Binsize and bandwidth selection
For a ﬁxed bandwidth, the estimator described above is robust to different choices of binsize provided that
h=b410, say. To understand this robustness, decompose fbþ as




pﬃﬃﬃﬃﬃ
X j  c w2  w1 ðX j  cÞ pﬃﬃﬃﬃﬃ
1 X
1
p
nb
Y
nhðfb þ f þ Þ ¼ pﬃﬃﬃﬃﬃﬃﬃﬃ
K

j
b j
h
w2 w0  ðw1 Þ2
h=b X j 4c




X j  c w2  w1 ðX j  cÞ pﬃﬃﬃﬃﬃ 1
1 X
þ
p
nh
K

f
þ
h=b X 4c
b j
h
w2 w0  ðw1 Þ2
j

 An þ E½fbþ  f þ ,
ð6Þ
R
P
1=2
where wk ¼ ð1=ðh=bÞÞ X j 4c KððX j  cÞ=hÞðX j  cÞk , k ¼ 0; 1; 2 and ð1=bÞpj ¼ 1=2 f ðX j þ buÞ du.16 As shown
formally in Appendix A, An tends towards a normal distribution. The quality of the normal approximation
does not turn on the magnitude of b. Intuitively, the second-step smoother averages over the Y j , which are
themselves averages. If b is small, then the Y j are not particularly normal, but the second-step smoothing
compensates. If b is large, then the Y j are very nearly normal, and not much averaging needs to happen in the
second step. The second sum in this decomposition gives the ﬁnite-sample bias of the estimator. Two Taylor
approximations and the algebra of regressions show that
o
Xb
w  w1 htj pﬃﬃﬃﬃﬃn 2 2 00þ
3
2
Kðtj Þ 2
E½fbþ  f þ  ¼
nh
h
t
f
þ
Oðh
Þ
þ
Oðb
Þ
,
(7)
j
h
w2 w0  ðw1 Þ2
X j 4c
where tj ¼ ðX j  cÞ=h. Since the tj sequence is b=h apart, this is a Riemann approximation to the area under a
pﬃﬃﬃﬃﬃ
curve. The height of the curve in question is dominated by the h2 term since h4b. The analysis for nhðfb  f  Þ
is symmetric. Thus, good performance of b
y does not appear to require a careful choice of binsize. This point is
substantiated in the simulation study in Section 5, below.
Good performance of b
y does require a good choice of bandwidth, however. Probably the best method of
bandwidth selection is visual inspection of the ﬁrst-step histogram and the second-step local linear density
function estimate, under a variety of choices for b and h. With software, it is easy to inspect both functions
within a few seconds.17 One of the practical advantages of the two-step estimation method described here is
visual. Suppose that as part of a pilot investigation, one has estimated the ﬁrst-step histogram using binsize b and
the second-step local linear smoother using bandwidth h. Graphically, superimposing the local linear smoother
on the scatterplot ðX j ; Y j Þ reveals rapidly the likely consequences of choosing a different bandwidth. The
effectiveness of subjective bandwidth choice has been noted in related contexts by Pagan and Ullah (1999) and
Deaton (1997), for example.
Less subjective methods include cross-validation (Stone, 1974, 1977) and plug-in estimators. Cheng (1997a)
proposes a plug-in bandwidth selector tailored to local linear density estimation, analogous to the Sheather
and Jones (1991) selector that is popular in standardR density estimation settings. Her method requires
estimating the integral of the squared second
ðf ð2Þ ðrÞÞ2 dr. As is standard in the literature, she uses
R ð2Þderivative,
2
a bandwidth other thanR h to estimate ðf ðrÞÞ dr; to ﬁnd the optimal bandwidth for this ancillary task
requires approximating f ð2Þ ðrÞf ð4Þ ðrÞ dr, and we are back where we started. Cheng (1994, Section 4.5.2) notes
that the method fares poorly in the boundary setting, where the integrals are (particularly) hard to estimate
with any accuracy, and suggests further modiﬁcations.
16
An analogous decomposition can be used to motivate an estimator that replaces takes the log of the histogram counts before
smoothing. Due to the covariance structure of the Y j and the nonlinearity of lnðÞ, a rigorous demonstration of asymptotic normality does
not appear straightforward unless one ﬁxes b and redeﬁnes the parameter of interest. Nonetheless, such an estimator is consistent whenever
b
y is, and has the same asymptotic variance as b
y, provided nb ! 1.
17
Software (STATA version 9) is available from the author for a period of 3 years from the date of publication.

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

705

To be practical, bandwidth selection rules need to be easy to implement. My own view is that the best
method is subjective choice, guided by an automatic procedure, particularly if the researcher agrees to report
how much the chosen bandwidth deviates from the recommendations of the automatic selector. Here is a
simple automatic bandwidth selection procedure that may be used as a guide:
1. Compute the ﬁrst-step histogram using the binsize bb ¼ 2b
sn1=2 , where b
s is the sample standard deviation of
the running variable.
2. Using the ﬁrst-step histogram, estimate a global 4th order polynomial separately on either side of the
P 00
cutoff. For each side, compute k½s 2 ðb  aÞ= f ðX j Þ2 1=5 , and set hb equal to the average of these two
:
quantities, where k¼3:348, s 2 is the mean-squared error of the regression, b  a equals X J  c for the right00
hand regression and c  X 1 for the left-hand regression, and f ðX j Þ is the estimated second derivative
implied by the global polynomial model.18
The second step of this algorithm is based on the rule-of-thumb bandwidth selector of Fan and Gijbels (1996,
Section 4.2). After implementing this selector, displaying the ﬁrst-step histogram based on bb and the curve fbðrÞ
based on hb provides a very detailed sense of the distribution of the running variable, upon which subjective
methods can be based. The selection method outlined in the above algorithm is used in the simulation study in
Section 5, below, where an automatic method is needed. In the empirical work in Section 6, where subjective
methods are feasible, this selection method is used as a guide.

4. Theoretical example
To motivate the potential for identiﬁcation problems caused by manipulation, consider a simple labor
supply model. Agents strive to maximize the present discounted value of utility from income over two periods.
Each agent chooses to work full- or part-time in each period. Part-time work requires supplying a fraction
f i of full-time labor supply and receiving a fraction f i of full-time income. Each worker has a different
fraction f i , which is determined unilaterally by the employer prior to period 1 on the basis of production
technology. Earnings in period 1 are given by Ri ¼ ai H i , where H i ¼ 1 if the individual works full-time and
H i ¼ f i if the individual works part-time. Between periods 1 and 2, a job training program takes place.
Agents are eligible for participation if they pass a means test based on period 1 income: program
participation is indicated by Di ¼ 1ðRi pcÞ, where c is the earnings threshold. Earnings in period 2 are given
by Y i ¼ ai þ bi Di , as in Eq. (1).
If the program did not exist, agents would supply full labor in both periods. In the notation of Section 2,
above, this means that Ri0 ¼ ai . However, the existence of the program raises the possibility that agents will
manipulate the running variable, withholding labor supply to meet the means test and gain access to job
training. Schematically, the decision problem can be represented as in Fig. 1, where d is the discount factor.
For well-paid agents with ai 4c=f i , the model predicts H i ¼ 1; for such an agent, reducing labor supply is
never worth it, because even under part-time work, the agent will not satisfy the means test. For poorly paid
agents with ai pc, the model similarly predicts H i ¼ 1, but for a different reason: such an agent satisﬁes the
means test for the program, even if working full-time. The remaining agents, those with latent wages satisfying
coai pc=f i , may ﬁnd it worthwhile to reduce labor supply, because otherwise they will fail the means test.
These agents reduce labor supply in response to the program if and only if uðf i ai Þ þ duðai þ bi Þ4uðai Þ þ duðai Þ.
There will always exist a value bi large enough to induce an agent to select H i ¼ f i . If bi and ai are correlated,
as would be expected in the general case, then this leads the conditional expectation of counterfactual
outcomes in Ri to be discontinuous. A necessary condition for the utility inequality above to hold is bi 40.
Under concave utility a sufﬁcient condition is bi 4ðuðai Þ  uðf i ai ÞÞ=du0 ðai Þ.Under linear utility, this condition is
18
The constant k is based on various integrals of the kernel used in the second step. The standard formula (see Fan and Gijbels, 1996,
Eq. (4.3)) does not apply to the boundary case (see Fan and Gijbels, Eqs. (3.20) and (3.22)). The constant cited is speciﬁc to the triangle
kernel in the boundary case.

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

706

Fig. 1. The agent’s problem.

0.50
Conditional Expectation
Estimate

Conditional Expectation
Estimate

0.50
0.30
0.10
-0.10
-0.30
-0.50

0.30
0.10
`
-0.10
-0.30
-0.50

5

10

15

20

5

10

0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
5

10

15
Income

15

20

Income

Density Estimate

Density Estimate

Income

20

0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
5

10

15

20

Income

Fig. 2. Hypothetical example: gaming the system with an income-tested job training program: (A) conditional expectation of returns to
treatment with no pre-announcement and no manipulation; (B) conditional expectation of returns to treatment with pre-announcement
and manipulation; (C) density of income with no pre-announcement and no manipulation; (D) density of income with pre-announcement
and manipulation.

also necessary, and we may characterize those who reduce their labor supply as those with coai pc=f i and
bi 4ai ð1  f i Þ=d.
Fig. 2 shows the implications of these behavioral effects using a simulated data set on 50,000 agents with
linear utility. The simulation takes ðai ; bi Þ to be distributed as independent normals, with E½ai  ¼ 12, V ½ai  ¼ 9,
E½bi  ¼ 0, and V ½bi  ¼ 1, and the f i distribution to be uniform on ½0; 1 and independent of ðai ; bi Þ. The
earnings threshold is set at c ¼ 14.
This data generating process is consistent with (A0). If the program did not exist, then period 1 earnings
would be Ri0 ¼ ai . The conditional expectation of ai given Ri0 is thus just the 45 line, which is continuous; the
conditional expectation of bi given Ri0 is ﬂat, which is likewise continuous; and the density of Ri0 is the normal
density, hence continuous. Panel A of Fig. 2 is a local linear regression estimate of the conditional expectation
of bi given Ri0 . The smoothness of the conditional expectation indicates the validity of (A0).
However, even though (A0) is satisﬁed, agents’ endogenous labor supply creates an identiﬁcation problem.
The actual running variable is not Ri0 , but Ri , which is manipulated by those agents who ﬁnd it worthwhile to
do so. Panel B gives a local linear regression estimate of the conditional expectation of bi given Ri . This panel

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

707

highlights the identiﬁcation problem. The estimated curve is strongly discontinuous near the earnings
threshold—those agents who stand to gain from the program self-select to supply less labor and hence are
displaced from just to the right of the earnings threshold to just to the left, leading to sample selection effects
which operate discontinuously at the earnings threshold.
In empirical work, it is not possible to estimate conditional expectations such as those in panels A and B,
because bi ¼ Y i1  Y i0 is unobservable. However, it is possible to carry out a density test. Panel C presents an
estimate of the density function of Ri0 , estimated using the local linear density estimation technique described
in Section 3, above. The density function is estimated and plotted for evaluation points r ¼ X 1 ; X 2 ; . . . ; X J .
The bandwidth and binsize were chosen subjectively following inspection of the automatic choices delivered by
the algorithm outlined in Section 3.2, above.19 The density estimate is consistent with continuity at the
earnings threshold, as expected.
Panel D instead gives the density function of Ri .20 In contrast with panel C, the estimated curve is strongly
discontinuous at the earnings threshold. The graph furnishes evidence of the economic behavior described by
the model above: agents self-select into the job training program by manipulating the value of the running
variable that will determine treatment assignment. This leads there to be slightly too few agents just above the
means test threshold, and slightly too many agents just below.

5. Simulation evidence
Table 1 presents the results of a small simulation study on the performance of b
y as an estimator and as part
of a testing procedure. In the table, ‘‘Design I’’ corresponds to the data generating process underlying panel C
from Fig. 2—50,000 independent draws from the Nð12; 3Þ distribution. There are 1000 replication data sets
used. For each data set, I calculate b
y using the binsize and bandwidth produced by the algorithm speciﬁed in
Section 3.2 (‘‘A. Basic, Basic’’). In addition to the ‘‘basic’’ implementation of the algorithm, I consider
a modiﬁed rule that undersmooths the bandwidth, setting it equal to half the size of the basic bandwidth
(‘‘B. Basic, Half’’). This allows assessment of the bias reduction that comes with undersmoothing. Finally, I
consider two non-basic binsizes, corresponding to half the basic binsize width (‘‘C. Half, Basic’’) and twice the
basic binsize width (‘‘D. Twice, Basic’’). This is to assess the robustness of the estimator to binsize choices.
The simulation corroborates the good performance suggested by the theoretical work of Section 3. The
estimator has generally small bias which declines as the bandwidth shrinks. As well, the standard error
suggested by the proposition represents well the approximate underlying standard deviation of the estimator.
Importantly, t-tests using the proposition
pﬃﬃﬃﬃﬃ have size of roughly 6%.
Approximating the distribution of nhðb
y  yÞ with a normal distribution is highly accurate. Fig. 3 presents
the normal Q–Q plot for the t-test of the (true) null hypothesis of continuity, where the t-tests stem from the
1000 replications reported in rows A and B of Table 1. Panel A (B) corresponds to row A (B). It is clear from
the ﬁgure that the quality of the ﬁt is quite good, even far out into the tails where it is most relevant for testing.
Comparing panels A and B in the ﬁgure, we see that undersmoothing nearly eliminates the estimator’s bias.
Perhaps surprisingly, these happy results carry over to much smaller samples. Design II reports results for
1000 replications of data sets with only 1000 observations from the same data generating process as Design I.
The bias of the estimator remains manageable, and the accuracy of the variance estimates is striking. The size
of tests using the estimation scheme proposed, even with such small sample sizes, is roughly 4–7%. Space
precludes the presentation of any further normal Q–Q plots, but these are similar to those shown in Fig. 3, in
that neither skewness nor fat tails is indicated.
Finally, these results also carry over to much more challenging density functions with multiple modes.
Design III reports results for 1000 replications of data sets with 10,000 observations from a 75–25 mixture of
normals with mean 0 and variance 1 and mean 4 and variance 1. The cutoff point was taken to be at 2. This is
a challenging point for local linear density estimation in this setting, because it is just to the left of a local
minimum of the true density, where the density function is strongly quadratic. However, the estimator
19

The recommended binsize and bandwidth were b ¼ 0:03 and h ¼ 1:5, and I chose b ¼ 0:05 and h ¼ 0:9.
In the interest of maintaining comparability, the binsize and bandwidth are kept the same in panels C and D.

20

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

708
Table 1
Simulation results
Design

I

II

III

Rule for
binsize, bandwidth

A. Basic, Basic
B. Basic, Half
C. Half, Basic
D. Twice, Basic
A. Basic, Basic
B. Basic, Half
C. Half, Basic
D. Twice, Basic
A. Basic, Basic
B. Basic, Half
C. Half, Basic
D. Twice, Basic

Range of
binsizes ðbÞ

Range of
bandwidths ðhÞ

½0:027; 0:027
½0:027; 0:027
½0:013; 0:013
½0:053; 0:054
½0:182; 0:196
½0:183; 0:196
½0:091; 0:098
½0:366; 0:393
½0:040; 0:040
½0:040; 0:040
½0:020; 0:020
½0:080; 0:081

½1:45; 1:56
½0:73; 0:78
½1:45; 1:54
½1:46; 1:61
½2:44; 3:45
½1:22; 1:72
½2:46; 3:44
½2:35; 3:46
½0:851; 1:01
½0:426; 0:506
½0:812; 0:950
½0:912; 1:11

Estimator

Proposition standard error

Bias

Std. Dev.

Mean

Size, t-test

0.0064
0.0018
0.0063
0.0066
0.0420
0.0059
0.0424
0.0423
0.0252
0.0011
0.0222
0.0307

0.0353
0.0513
0.0354
0.0351
0.1800
0.2564
0.1793
0.1809
0.1598
0.2079
0.1608
0.1575

0.0345
0.0489
0.0346
0.0343
0.1763
0.2532
0.1757
0.1775
0.1484
0.2010
0.1516
0.1440

0.0630
0.0600
0.0640
0.0600
0.0580
0.0430
0.0670
0.0670
0.0650
0.0560
0.0610
0.0690

Notes: Simulation results for three different data generating processes and four different binsize and bandwidth selection rules. See text for
details.

Quantile of t-test Distribution

4
3
2
1
0
-1
-2
-3
-4
-4

-3

-2

-4

-3

-2

-1
0
1
2
Quantile of Normal Distribution

3

4

3

4

Quantile of t-test Distribution

4
3
2
1
0
-1
-2
-3
-4
-1
0
1
Quantile of Normal Distribution

2

Fig. 3. Quality of normal approximation: (A) t-test based on proposition standard error, no undersmoothing; (B) t-test based on
proposition standard error, undersmoothing.

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

709

continues to enjoy bias of small magnitude, and t-tests using the estimator and its estimated standard error
lead to size of 5–7%.
6. Empirical example
One of the better examples of the regression discontinuity design is the incumbency study of Lee (2001).
Political scientists have postulated that there is an incumbency advantage for both parties and individual
candidates, whereby having won the election once makes it easier to win the election subsequently. Credibly
establishing the magnitude of any incumbency advantage is challenging because of strong selection effects. Lee
notes that in a two-party system with majority rule, incumbency is assigned discontinuously at 50% on the
basis of the popular vote and uses the regression discontinuity design to assess the party incumbency effect for
popular elections to the United States House of Representatives.
The complete manipulation phenomena described in Section 2 seems unlikely to occur in this instance,
because voters are unlikely to be able to coordinate to manipulate the vote tally, and because democratic
safeguards are presumably sufﬁcient to prevent vote fraud.21 Thus, a natural expectation is for the density
function of the vote share to be smooth. I test this notion formally using the techniques outlined above.
Speciﬁcally, using data on the votes cast for each candidate in contested elections to the US House of
Representatives involving a democratic candidate, 1900–1990, I estimate the density function of the
‘‘democratic margin’’, deﬁned as the fraction of all votes (vote share) received by the democratic candidate,
less the largest vote share received by any other candidate in the election.22 Deﬁned in this way, the democratic
candidate wins the election if and only if the democratic margin is positive.23
Fig. 4 gives an estimate of the density function of the democratic margin. The curve was estimated using the
estimator outlined in Section 3, with evaluation points r ¼ X 1 ; X 2 ; . . . ; X J . The binsize and bandwidth were
chosen subjectively after using the automatic procedure outlined in Section 3.2 as a pilot estimate. The automatic
procedure in this case seems to oversmooth at the mode in this setting.24 The estimated curve gives little indication
of strong discontinuity near zero. Indeed, the density appears generally quite smooth. Importantly, the ﬁrst-step
histogram reveals that this is not the result of oversmoothing. The estimated parameter b
y is presented in Table 2,
along with the proposition standard error. As expected, a t-test of the null hypothesis of continuity fails to reject.
The complete manipulation problem described in Section 2 is unlikely to occur in a fair popular election,
because coordination of voters is difﬁcult and there is little discretion in measuring the vote tally. However, in
other election contexts, coordination is feasible and complete manipulation may be a concern.
A leading example of this type of coordination is roll call voting in the House of Representatives.
Coordination is expected in this context. First, the volume of bills before the House and the long tenure of
most representatives conspire to create a repeated game. Second, a representative’s vote is public knowledge,
allowing for credible commitments to contracts over voting. In such a context, side payments for a
representative’s vote do not have to involve (illegal) monetary compensation, but may pertain simply to votes
on future bills. Riker’s (1962) size principle then implies that the most likely bills to be put to vote on the
House ﬂoor are those expected to narrowly pass.
Fig. 5 presents an estimated density function for the percent voting ‘‘yeay’’ on all roll call votes in the House
from 1857–2004.25,26 The curve was estimated using the estimator outlined in Section 3, with evaluation points
21
Democratic safeguards may not always be sufﬁcient. Greenberg (2000) discusses the famously contested 1960 presidential election
between Richard Nixon and John F. Kennedy. See also Snyder (2005), who uses the estimator described here to analyze close elections to
the United States House involving an incumbent.
22
1591 elections during this period involve a single candidate. Of the contested elections, 95.3% involve a Democratic candidate, and
92.5% involve a Republican candidate.
23
This deﬁnition of the running variable is slightly different from that in Lee (2007), but differs little as a practical matter, particularly
for the post-1948 period pertaining to Lee’s study.
24
I use a binsize of b ¼ 0:004 and a bandwidth of h ¼ 0:02. The automatic procedure would select b ¼ 0:004 and h ¼ 0:13.
25
Stratifying the votes into before and after 1900 subperiods results in highly similar estimates with less precision.
26
The density estimator is allowed to be discontinuous at 50% but nowhere else, despite the existence of bills which require a supermajority
vote for passage (e.g., two-thirds approval for constitutional amendments and veto overrides), so 50% is not the cutoff for passage for all bills.
However, bills requiring a supermajority for passage are rare, and the data do not allow me to determine the cutoff for the given bill.
Consequently, I focus on the potential discontinuity at 50%, viewing this as being slightly attenuated due to the unobserved supermajority bills.

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

710

150
1.60
1.40
1.20
90

1.00
0.80

60
0.60

Density Estimate

Frequency Count

120

0.40

30

0.20
0

0.00
-1

-0.8

-0.6

-0.4

-0.2
0
0.2
Democratic Margin

0.4

0.6

0.8

1

Fig. 4. Democratic vote share relative to cutoff: popular elections to the House of Representatives, 1900–1990.

Table 2
Log discontinuity estimates

N

Popular elections

Roll call votes

0.060
(0.108)
16,917

0.521
(0.079)
35,052

Note: Standard errors in parentheses. See text for details.

300

Frequency Count

2.00

200
150

1.50

100

1.00

50

0.50

0

Density Estimate

2.50

250

0.00
0

0.1

0.2
0.3
0.4
0.5
0.6
0.7
0.8
Percent Voting in Favor of Proposed Bill

0.9

1

Fig. 5. Percent voting yeay: roll call votes, U.S. House of Representatives, 1857–2004.

r ¼ X 1 ; X 2 ; . . . ; X J . The binsize and bandwidth were again chosen subjectively after using the automatic
procedure. Much more so than the vote share density, the roll call density exhibits very speciﬁc features near
the cutoff point that are hard for any automatic procedure to identify.27
The ﬁgure strongly suggests that the underlying density function is discontinuous at 50%. Outcomes within
a handful of votes of the cutoff are much more likely to be won than lost; the ﬁrst-step histogram indicates
that the passage of a roll call vote by 1–2 votes is 2.6 times more likely than the failure of a roll call vote by 1–2
27

I use a binsize of b ¼ 0:003 and a bandwidth of h ¼ 0:03. The automatic procedure would select b ¼ 0:0025 and h ¼ 0:114.

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

711

votes. Although the magnitude of the effect is not as extreme, the second-step smoother corroborates the
suggestion of the ﬁrst-step histogram. Table 2 presents the estimated log discontinuity in the density, which is
a large 52%. The effect is precisely estimated, with a t-ratio of 6.6.
These empirical results are consistent with a manipulation hypothesis. In particular, the results suggest that
it would be a mistake to view the majority vote election procedure in the US House of Representatives as
generating quasi-random assignment of policy decisions emerging from the House.
7. Conclusion
This paper describes identiﬁcation problems encountered in the regression discontinuity design pertaining to
manipulation of the running variable and describes a simple test for manipulation. The test involves
estimation of the discontinuity in the density function of the running variable at the cutoff. Consistency
and asymptotic normality of the log discontinuity in the density at the cutoff was demonstrated theoretically,
and inference procedures discussed. The methodology was applied to two distinct settings, one in which
manipulation is unexpected and is not detected, and another in which manipulation is expected and
demonstrated.
The context of most regression discontinuity applications is such that the treatment assignment rule
is public knowledge. I have argued that this will often make it plausible that the agents under study engage
in manipulation of the running variable in order to obtain desirable treatment assignments, and I have
emphasized that manipulation will often lead to violations of the assumptions necessary for identiﬁcation.
The standard speciﬁcation test used currently in regression discontinuity applications is a test for continuity of
the conditional expectation of pre-determined characteristics in the running variable at the cutoff. Such tests are
a natural and powerful way to assess the plausibility of the identifying assumptions. The density test proposed
here complements these methods and is expected to be powerful when manipulation is monotonic, as discussed
above. The density test may be particularly important for applications where pre-determined characteristics are
not available, or are not relevant to the substantive topic studied.
Acknowledgments
I thank two anonymous referees for comments, the editors for multiple suggestions that substantially
improved the paper, Jack Porter, John DiNardo, and Serena Ng for discussion, Jonah Gelbach for computing
improvements, and Ming-Yen Cheng for manuscripts. Any errors are my own.
Appendix A. Proof of proposition
Because of the linearity of Y j ¼ ð1=nbÞ
fbþ ¼
¼

þ
Sþ
n;2 T n;0
þ
Sþ
n;2 S n;0
J
X




Pn

i¼1 1ðgðRi Þ

¼ X j Þ, we have

þ
Sþ
n;1 T n;1
þ
Sþ
n;1 S n;1

Kðtj Þ1ðtj 40Þ

j¼1

þ
Sþ
n;2  S n;1 h tj
þ
þ þ Yj
Sþ
n;2 S n;0  S n;1 S n;1

¼

þ
n X
J
Sþ
1X
1
n;2  S n;1 htj
1ðgðRi Þ ¼ X j Þ
Kðtj Þ1ðtj 40Þ þ þ
þ þ
n i¼1 j¼1
Sn;2 S n;0  S n;1 Sn;1 b



n X
J
1X
Z ijn
n i¼1 j¼1



n
1X
Z in ,
n i¼1

ðA:1Þ

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

712

k PJ
k PJ
þ
k
k
where tj ¼ ðX j  cÞ=h, S þ
j¼1 Kðtj Þ1ðtj 40Þtj , and T n;k  h
j¼1 Kðtj Þ1ðtj 40Þtj Y j , and analogously for
n;k  h
fb . The proof proceeds by calculating E½fbþ  and V ½fbþ  and verifying the skewness condition of the Lyapunov
central limit theorem (Rao, 1965, p. 107), which applies since Z in is independent of Z i0 n for i0 ai. Independence
follows since Z in is just a transformation of Ri and since X 1 ; X 2 ; . . . ; X J are constants (see footnote 13). By
kþ1
k1
=bÞS þ
bÞ,
Riemann approximation (see Cheng, 1994, Lemma 4, for example), we have S þ
n;k ¼ ðh
k þ Oðh
R
1
þ
þ
k
1 1
1
,
where S k ¼ 0 t KðtÞ dt, k ¼ 0; 1; 2; . . . For the triangle kernel with k ¼ 0; 1; 2, Sk is equal to 2, 6, and 12
respectively. We have
 2
þ
Sþ
b
b
n;2  S n;1 htj
6ð1

2t
¼
Þ
þ
O
.
(A.2)
j
þ
þ þ
h
Sþ
S

S
S
h2
n;2 n;0
n;1 n;1

Using Taylor and Riemann approximation we have
E½fbþ  ¼ E½Z in  ¼

J
X
b
j¼1

h

Kðtj Þ1ðtj 40Þ6ð1  2tj Þf ðc þ htj Þ þ O

 
b
þ Oðb2 Þ
h

 
b
ð1  tÞ6ð1  2tÞf ðc þ htÞ dt þ O
þ Oðb2 Þ
h
0
 
1 1 þ00
b
þ
f þ Oðh3 Þ þ O
¼ f  h2
þ Oðb2 Þ,
2 10
h
Z

1

¼

!
J X
J
X
1
1
1
E½Zijn Zikn   E2 ½Z in 
V ½fbþ  ¼ V ½Z in  ¼ ðE½Z 2in   E 2 ½Z in Þ ¼
n
n
n j¼1 k¼1
!
 2
J
1 X
b2 2
b
2 1
2
¼
K
ðt
Þ1ðt
40Þ36ð1

2t
Þ
p

E
½Z

þ
O
j
j
j
in
j
n j¼1 h2
b2
nh2
Z 1

 
 2
1
b
b
2
2
2
¼
ð1  tÞ 36ð1  2tÞ f ðc þ htÞ dt  hE ½Z in  þ O
þO
2
nh 0
n
nh
 
1 24 þ
1
f þO
¼
nh 5
n

ðA:3Þ

ðA:4Þ

since E½Y j  ¼ f ðX j Þ þ Oðb2 Þ, where the only terms from the double summation which matter are those for
which j ¼ k since the histogram bins are mutually exclusive. For the Lyapunov condition, calculate
"
#
J X
J X
J
X
3
3
E½jZ in  E½Z in j p8E½jZ in j p8E
jZ ijn j  jZikn j  jZ iln j
j¼1 k¼1 l¼1

¼8

J
X

E½jZ ijn j3 

j¼1

 
J
1X
b 3
b
3
3
K ðtj Þ1ðtj 40Þ6 j1  2tj j f ðc þ htj Þ þ O
¼8 2
h
h
h j¼1


 
Z
1 1
b
1
3 3
3
ð1  tÞ 6 j1  2tj f ðc þ htÞ dt þ O
¼8 2
¼O 2 .
h
h 0
h

ðA:5Þ

Combining the expression for the variance with the skewness bound, we have
P
ð ni¼1 E½jZ in  E½Zin j3 Þ1=3 ðOðn=h2 ÞÞ1=3
p
¼ OððnhÞ1=6 Þ
P
ð ni¼1 V ½Z in Þ1=2
ðOðn=hÞÞ1=2

ðA:6Þ

ARTICLE IN PRESS
J. McCrary / Journal of Econometrics 142 (2008) 698–714

so that the Lyapunov condition is satisﬁed since nh ! 1. Thus, and by symmetry,




pﬃﬃﬃﬃﬃ þ
pﬃﬃﬃﬃﬃ 
24
24
d
d
nhðfb  f þ Þ ! N Bþ ; f þ
and
nhðfb  f  Þ ! N B ; f  ,
5
5

713

(A.7)

00

1 þ
1 00
f
and B ¼ H 20
f . To strengthen this result to joint asymptotic normality, deﬁne
where Bþ ¼ H 20
 

þ
l
Z
,
where
the
Z
from
above is redeﬁned to be Z þ
U in ¼ lþ Z þ
in
in
in
in and Z in denotes the analogous quantity
to the left of c. Observe that U in is independent of U i0 n for all i0 ai. Then we have E½U in  ¼ lþ f þ þ l f  þ
þ 2 þ
 2 
þ

24
Oðh2 Þ and V ½U in  ¼ ð24
5 Þðl Þ ðf =hÞþ ð 5 Þðl Þ ðf =hÞ þ oð1=hÞ, where the latter follows since C½Z in ; Z in  ¼
þ

2

3
3
E½Zþ
in E½Z in  ¼ f f þ Oðh Þ. Using the results from above, we have E½jU in  E½U in j p8E½jU in j p
þ 3
 3
2
þ 3
 3
8jl j E½jZ in j  þ 8jl j E½jZ in j  ¼ Oð1=h Þ and it is then straightforward to verify the Lyapunov condition as
above. Since this holds for every vector ðlþ ; l Þ, the Cramér–Wold device (White, 2001, p. 114) implies joint
asymptotic normality with a diagonal asymptotic variance matrix. Deﬁne tðf þ ; f  Þ ¼ ln f þ  ln f  ¼ y, note
that rt ¼ ð1=f þ ; 1=f  Þ0 , and apply the delta method to conclude



pﬃﬃﬃﬃﬃ
24 1
1
d
b
þ
nhðy  yÞ ! N B;
,
(A.8)
5 fþ f

where B ¼ Bþ =f þ  B =f  .
Appendix B. Data
Data on popular elections to the US House of Representatives are taken from ICPSR Study # 7757. These
are the same data used by Lee (2001, 2007), but I have engaged in neither the data augmentation nor the
cleaning procedures he conducted. Data on roll call votes are taken from http://www.voteview.com/
partycount.htm, a website maintained by Keith T. Poole of the University of California, San Diego. This same
website is the basis for the data on DW-Nominate scores. Finally, data on party control of the House are
taken from http://arts.bev.net/roperldavid/politics/congress.htm, a website maintained by L. David Roper of
the Virginia Polytechnic Institute and State University.
All data and programs are available from the author for a period of 3 years from the date of publication.
References
Aigner, D.J., Amemiya, T., Poirier, D.J., 1976. On the estimation of production frontiers: maximum likelihood estimation of the
parameters of a discontinuous density function. International Economic Review 17 (2), 377–396.
Angrist, J.D., Imbens, G.W., Rubin, D.B., 1996. Identiﬁcation of causal effects using instrumental variables. Journal of the American
Statistical Association 91 (434), 444–455.
Becker, G.S., 1957. The Economics of Discrimination. University of Chicago Press, Chicago.
Bouezmarni, T., Scaillet, O., 2005. Consistency of asymmetric kernel density estimators and smoothed histograms with application to
income data. Econometric Theory 21 (2), 390–412.
Card, D.E., 1999. The causal effect of education on earnings. In: Ashenfelter, O., Card, D.E. (Eds.), The Handbook of Labor Economics,
vol. 3A. Elsevier, Amsterdam.
Cheng, M.-Y., 1994. On boundary effects of smooth curve estimators (dissertation). Unpublished manuscript Series # 2319, Institute for
Statistics, University of North Carolina.
Cheng, M.-Y., 1997a. A bandwidth selector for local linear density estimators. Annals of Statistics 25 (3), 1001–1013.
Cheng, M.-Y., 1997b. Boundary aware estimators of integrated density products. Journal of the Royal Statistical Society, Series B 59 (1),
191–203.
Cheng, M.-Y., Fan, J., Marron, J.S., 1993. Minimax efﬁciency of local polynomial ﬁt estimators at boundaries. Unpublished manuscript
Series # 2098, Institute for Statistics, University of North Carolina.
Cheng, M.-Y., Fan, J., Marron, J.S., 1997. On automatic boundary corrections. The Annals of Statistics 25 (4), 1691–1708.
Chernozhukov, V., Hong, H., 2004. Likelihood estimation and inference in a class of nonregular econometric models. Econometrica 72
(5), 1445–1480.
Chu, C., Cheng, P., 1996. Estimation of jump points and jump values of a density function. Statistica Sinica 6 (1), 79–96.
Cline, D.B., Hart, J.D., 1991. Kernel estimation of densities with discontinuities or discontinuous derivatives. Statistics 22 (1), 69–84.

ARTICLE IN PRESS
714

J. McCrary / Journal of Econometrics 142 (2008) 698–714

Davison, A.C., Hinkley, D.V., 1997. Bootstrap Methods and their Application. Cambridge University Press, New York.
Deaton, A., 1997. The Analysis of Household Surveys: A Microeconomic Approach to Development Policy. World Bank, Washington,
DC.
DiNardo, J., Fortin, N., Lemieux, T., 1996. Labor market institutions and the distribution of wages, 1973–1992: a semi-parametric
approach. Econometrica 64 (5), 1001–1044.
DiNardo, J.E., Lee, D.S., 2004. Economic impacts of new unionization on private sector employers: 1984–2001. Quarterly Journal of
Economics 119 (4), 1383–1441.
Fan, J., Gijbels, I., 1996. Local Polynomial Modelling and its Applications. Chapman & Hall, New York.
Gawronski, W., Stadtmüller, U., 1980. On density estimation by means of Poisson’s distribution. Scandinavian Journal of Statistics 7 (2),
90–94.
Gawronski, W., Stadtmüller, U., 1981. Smoothing histograms by means of lattice- and continuous distributions. Metrika 28 (3), 155–164.
Greenberg, D., 2000. Was nixon robbed? The Legend of the Stolen 1960 Presidential Election. Slate.
Hahn, J., Todd, P., van der Klaauw, W., 1999. Identiﬁcation and estimation of treatment effects with a regression discontinuity design.
NBER Working Paper # 7131.
Hahn, J., Todd, P., van der Klaauw, W., 2001. Identiﬁcation and estimation of treatment effects with a regression discontinuity design.
Econometrica 69 (1), 201–209.
Hall, P., 1992. Effect of bias estimation on coverage accuracy of bootstrap conﬁdence intervals for a probability density. The Annals of
Statistics 20 (2), 675–694.
Hall, P., McKay, I., Turlach, B.A., 1996. Performance of wavelet methods for functions with many discontinuities. Annals of Statistics 24
(6), 2462–2476.
Härdle, W., Linton, O., 1994. Applied nonparametric methods. In: Engle, R.F., McFadden, D.L. (Eds.), The Handbook of Econometrics,
vol. 4. Elsevier, New York, pp. 2297–2341.
Heckman, J.J., 2005. The scientiﬁc model of causality. Sociological Methodology 35 (1), 1–98.
Heckman, J.J., Urzua, S., Vytlacil, E., 2006. Understanding instrumental variables in models with essential heterogeneity. Review of
Economics and Statistics 88 (3), 389–432.
Horowitz, J.L., 2001. The bootstrap. In: Heckman, J.J., Leamer, E. (Eds.), The Handbook of Econometrics, vol. 5. Elsevier, New York,
pp. 3463–3568.
Imbens, G.W., Angrist, J.D., 1994. Identiﬁcation and estimation of local average treatment effects. Econometrica 62 (2), 467–475.
Jacob, B.A., Lefgren, L., 2004. Remedial education and student achievement a regression-discontinuity analysis. Review of Economics
and Statistics 86 (1), 226–244.
Jones, M.C., 1989. Discretized and interpolated kernel density estimates. Journal of the American Statistical Association 84 (407),
733–741.
Lee, D.S., 2001. The electoral advantage to incumbency and voters’ valuation of politicians’ experience a regression discontinuity analysis
of elections to the U.S. House. NBER Working Paper # 8441.
Lee, D.S., 2007. Randomized experiments from non-random selection in U.S. House Elections. Journal of Econometrics.
Marron, J.S., Ruppert, D., 1994. Transformations to reduce boundary bias in kernel density estimation. Journal of the Royal Statistical
Society, Series B 56 (4), 653–671.
Pagan, A., Ullah, A., 1999. Nonparametric Econometrics. Cambridge University Press, New York.
Rao, C.R., 1965. Linear Statistical Inference and its Applications, ﬁrst ed. Wiley, New York.
Rice, J., 1984. Boundary modiﬁcation for kernel regression. Communications in Statistics, A 13 (7), 893–900.
Riker, W.H., 1962. The Theory of Political Coalitions. Yale University Press, New Haven.
Rubin, D.B., 1980. Randomization analysis of experimental data: the ﬁsher randomization test: comment. Journal of the American
Statistical Association 75 (371), 591–593.
Rubin, D.B., 1986. Statistics and causal inference: which ifs have causal answers. Journal of the American Statistical Association 81 (396),
961–962.
Saez, E., 1999. Do taxpayers bunch at kink points? NBER Working Paper # 7366.
Saez, E., 2002. Do taxpayers bunch at kink points? Unpublished manuscript, University of California, Berkeley.
Schuster, E.F., 1985. Incorporating support constraints into nonparametric estimators of densities. Communications in Statistics, A 14 (5),
1123–1136.
Sheather, S.J., Jones, M.C., 1991. A reliable data-based bandwidth selection method for kernel density estimation. Journal of the Royal
Statistical Society, Series B 53 (3), 683–690.
Snyder, J., 2005. Detecting manipulation in U.S. House elections. Unpublished manuscript, Haas School of Business, University of
California, Berkeley.
Stone, M., 1974. Cross-validation and multinomial prediction. Biometrika 61 (3), 509–515.
Stone, M., 1977. Asymptotics for and against cross-validation. Biometrika 64 (1), 29–35.
van der Klaauw, W., 2002. Estimating the effect of ﬁnancial aid offers on college enrollment: a regression-discontinuity approach.
International Economic Review 43 (4), 1249–1287.
White, H., 2001. Asymptotic Theory for Econometricians. Academic Press, San Diego.

