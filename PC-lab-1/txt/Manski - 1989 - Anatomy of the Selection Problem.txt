The Board of Regents of the University of Wisconsin System

Anatomy of the Selection Problem
Author(s): Charles F. Manski
Reviewed work(s):
Source: The Journal of Human Resources, Vol. 24, No. 3 (Summer, 1989), pp. 343-360
Published by: University of Wisconsin Press
Stable URL: http://www.jstor.org/stable/145818 .
Accessed: 13/01/2013 09:56
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at .
http://www.jstor.org/page/info/about/policies/terms.jsp

.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact support@jstor.org.

.

University of Wisconsin Press and The Board of Regents of the University of Wisconsin System are
collaborating with JSTOR to digitize, preserve and extend access to The Journal of Human Resources.

http://www.jstor.org

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

Anatomy of the Selection Problem

Charles F. Manski

ABSTRACT

This article considers anew the problem of estimating a regression E(ylx) when realizations of(y, x) are sampled randomly
but y is observed selectively. The central issue is the failure of
the sampling process to identify E(y x). The problem faced by
the researcher is to find correct prior restrictions which, when
combined with the data, identify the regression.
Two kinds of restrictions are examined here. One, which has
not been studied before, is a bound on the support of y. Such a
bound implies a simple, useful bound on E(y x). The other,
which has received much attention, is a separability restriction
derived from a latent variable model.

I. Introduction
This article seeks to expose the essence of a problem that
has drawnmuch attentionin the past fifteen years: estimationof a regression from selectively observed random sample data.
Suppose that each memberof a populationis characterizedby a triple
(y, z, x), where y is a real number,z is a binary indicator,and x is a real
The author is a professor of economics at the Universityof Wisconsin-Madisonand is
affiliatedwith the Institutefor Research on Poverty. This research is supportedby National Science FoundationGrantSES-8808276and byfunding to the Institutefor Research on Povertyfrom the U.S. Departmentof Health and Human Services. The author
is grateful to Irving Piliavinfor posing the empiricalissue that stimulatedhim to take a
fresh look at the selection problem. He has benefitedfrom discussions withArthurGoldbergerandfrom the opportunityto present this workin a conferenceat Wingspreadand
in seminars at Brown, Cornell, and HarvardUniversity.
THE JOURNAL OF HUMAN RESOURCES * XXIV * 3

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

344 The Journalof HumanResources
vector. A researcherobserves a randomsample of realizationsof (z, x)
and, moreover, observes the realizationsof y when z = 1. I shall assume
that the researcherwants to learn the regressionfunction E(y x) on the
supportof the conditioningvariablex.
The central issue is identification.The samplingprocess identifiesthe
regressions E(ylx, z = 1) and E(zlx) = P(z = llx). Given minimal

regularity,these functions of x can be estimatedconsistently. The literature on nonparametricregression analysis offers numerousapproaches.
The samplingprocess does not identify E(y x, z = 0) nor
(1) E(y|x) = E(ylx, z = 1)P(z = l x) + E(y x, z = 0)P(z = OIx).

On the other hand, E(y x) may be identifiedif one can combine the data
with suitable prior restrictions on the population distributionof (y, z)
conditionalon x. The problemfaced by the researcheris to find restrictions which are both correct and useful.
Until the early 1970s, researchers almost universally assumed that,
conditionalon x, y is mean independentof z. That is,
(2) E(y x) = E(ylx, z = 1) = E(y x, z = 0).

As the samplingprocess identifiesE(y x, z = 1), restriction(2) identifies
E(ylx). The plausibilityof (2) has subsequentlybeen questionedsharply,
especially by researchers who use latent variablemodels to explain the
determinationof (y, z). See Gronau(1974).
I shall examine two alternatives to conditional mean independence.
Section II poses a weak restriction that has not been studied before,
namely a bound on the supportof y conditionalon x. I show that such a
bound implies a simple, useful bound on E(y x) and present an empirical
illustration.
Section III examines separabilityrestrictionsderivedfrom latent variable models. Leading cases include the familiarnormal-linearmodel and
recently developed index models.
Section IV draws conclusions.

II. Bound on the Conditional Support of y
Suppose it is known that, conditionalon x and on z = 0, the
distributionof y is concentratedin a given interval[Kox,K1x],where Kox'
K1x.That is,
(3) P{y E [Kox, Klx]lx, z = 0} = 1.

Then we may derive an estimablebound on E(y Ix). To obtainthe bound,
observe that

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

Manski 345
(4) P{y E [Kox, Kllxlx, z = 0} = I = Ko, < E(yx,

z = O) - K1,.

Apply this inequalityto the right-handside of Equation(1). The result is
(5) E(ylx, z = 1)P(z = l x) + KoxP(z = OIx) < E(ylx)
- E(y x, z = l)P(z = l|x) + KlxP(z = O0x).
Thus the lower bound is the value E(ylx) takes if, in the nonselected

subpopulation, y always equals Ko. The upper bound is the value of
E(y Ix) if all the nonselected y equal K1.
This boundon E(y Ix)is determinedby the bound [Kox,Klx] on y, which
is known, and by the regressions E(yjx, z = 1) and P(zlx), which are
identifiedby the samplingprocess. So the boundcan be madeoperational.
Methods for estimating the bound from sample data will be provided in
Section II.C.
The bound is informativeif P(z = 0Ox)< 1 and if the bound [Kox,K1x]
on the conditionalsupportof y is nontrivial.Its widthis (Klx - Kox)P(z=
0 x). Thus the width does not depend on E(ylx, z = 1). The boundwidth
varies with x in proportionto the two quantities(Kix - Kox)and P(z =
Ofx). This behavior is intuitive. The wider the bound on the conditional
supportof y, the less priorinformationone has. The largeris P(z = Olx),
the smaller is the probabilitythat y is observed.
It is useful to consider the bound width as a fractionof the width of the
originalbound on y. The fractionalwidth is P(z = Olx),the probabilityof
not being selected conditional on x. Thus the fractionalwidth does not
depend on the variable y whose regression on x is sought. Researchers
facing selection problems routinely estimate selection probabilities. So
they may easily determinehow informativethe bound (5) will be for any
choice of y and at any value of x.
It is of some historicalinterestto ask why the literatureon selection has
not previously recognized the identifying power of a bound on y. The
explanationmay have several parts.
Timingmay have played a role. The literatureon selection developed in
the 1970s, a period when the frontier of econometrics was nonlinear
parametricanalysis. At that time, nonparametricregressionanalysis was
just beginningto be formalizedby statisticians. Economists were generally unawarethat consistent nonparametricregression was possible.
It may be that the historical fixation of econometrics on point identificationhas inhibitedappreciationof the potentialusefulness of bounds.
Econometricianshave occasionally reporteduseful bounds on quantities
that are not point-identified;see, for example, McFadden(1975),Klepper
and Leamer (1984), Varian (1985), and Manski (1988a). But the conventional wisdom has been that bounds are hardto estimate and rarelyinfor-

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

346

The Journalof Human Resources
mative. Whateverthe validity of this conventionalwisdom in other contexts, it does not apply to the bound (5).
Perhaps the preoccupationof researcherswith the estimationof wage
equations has been a factor. The typical wage regressiondefines y to be
the logarithm of wage. This variable has no obvious upper bound, althoughminimumwage legislationmay enforce a lower bound. Whetheror
not the logarithm of wage is bounded, wage distributionsare always
boundable. This is shown in Section A.
A. Binary y

When y is a binary indicator variable, the bound takes an especially
simple form. Here y is definitionally bounded, with Ko, = 0 and K1x = 1
for all x. Moreover, E(y x) = P(y = 1 ix) and E(ylx, z = 1) = P(y = l x,
z = 1). Hence (5) reduces to
(6) P(y = lx, z = l)P(z = lIx) c P(y = l x)
< P(y = llx, z = l)P(z = lIx) + P(z = OIx).

The binary indicatorcase may seem special. Actually it has very general application;it provides a bound for any conditionalprobability.To
see this, let w be a randomvariabletakingvalues in a space W. Let A be
any subset of W. Suppose that a researcherwants to boundthe probability that w is in A, conditionalon x. To do so, one need only observe that
(7) P(w E A x) = E{l [w E A] |x},

where 1[*] is the indicatorfunction takingthe value one if the bracketed
logical condition holds and zero otherwise. So the bound (6) applies with
y -l[w

E A].

For example, let w be the logarithmof a worker's wage. Suppose that
the support of w conditional on x is unbounded.Then the bound (5) on
E(w Ix) is the trivial (- oo,oo).But one can obtain an informative bound on
the conditional probability P(w - T Ix) for any real number T. Just define
y - l[w < T] and apply (6). Varying 7, one may bound the distribution

function of w conditionalon x.
It may seem surprisingthat one shouldbe able to boundthe distribution
function of a randomvariablebut not its mean. The explanationis a fact
that is widely appreciatedby researchersin the field of robust statistics:
the mean of a randomvariableis not a continuousfunctionof its distribution function. Hence small perturbationsin a distributionfunction can
generate large movements in the mean. See Huber (1981).
To obtain some intuition for this fact, consider the following thought
experiment.Let w be a randomvariablewith 1 - Eof its probabilitymass

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

Manski 347
in the interval (- o, T] and E mass at some point S > T. Suppose w is
perturbed by moving the mass at S to some S1 > S. Then P(w - T)
remains unchanged for T < S and falls by at most E for T
v
S. But E(w)
increases by the amount e(S1 - S). Now let S1 -> oo. The perturbed

distributionfunctionremainswithinan e-boundof the originalone but the
mean of the perturbedrandomvariableconverges to infinity.
B. Bounding the Effect of a Change in x

The objective of a regressionanalysis is sometimes not to learnE(y Ix) at
a given value of x but ratherto learnhow E(y x) moves as x changes. One
can use (5) to bound the magnitudeof this movement. In some cases one
can bound its direction.
Suppose that one wants to learnE(y x = ) - E(y x = p), where5 and
are
given points in the supportof x. The bound (5) implies that E(y x =
p
~)
E(y x = p) is bounded from below by the difference between
the lower bound on E(ylx = ~) and the upper bound on E(ylx = p).
Similarly, E(ylx = ~) - E(ylx = p) is bounded from above by the dif-

ference between the upper bound on E(y Ix = ~) and the lower bound on
E(ylx = p). That is,
(8) E(yIx = i, z = 1)P(z = lIx = )
- E(ylx = p, z = l)P(z = l x = p) + KoP(z = Ox = )

-E(yIx

- KgpP(z = Oix = p) C E(yix = ~) - E(yix = p)
= ~,z = 1)P(z = 1|x = 5) - E(ylx = p,z = l)P(z = l|x = p)
+ KrlP(z = Olx = ) - KopP(z = Ox = p).

The width of this bound is the sum of the widths of the bounds on E(y Ix
= ~) and on E(y x = p). Dependingon the case, the bound may or may
not lie entirely to one side of the origin.
The foregoingconcerns a finitechangein x. On occasion one would like
to learn the derivativeOE(yx)/lx. A boundon y does not by itself restrict
this derivative. A bound on y combined with one on aE(y x, z = 0)/dx
does.
The argumentextends that leading to (5). It follows from (1) that
(9) aE(y x)/dx
= P(z = lIx)[aE(ylx, z = 1)/ax] + E(yIx, z = l)[aP(z = lIx)/dx]
+ P(z = Oix)[dE(yix, z = 0)/ax] + E(ylx, z = O)[aP(z = Olx)/lx],

provided that these derivatives exist. Of the quantitieson the right-hand
side of (9), all but E(y Ix, z = 0) and aE(y Ix, z = 0)/ax are identifiedby the

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

348 The Journalof Human Resources
samplingprocess. Suppose that (3) holds. Moreover,let it be knownthat,
for a given [Dox, DIx],
(10) aE(ylx, z = 0)/ax E [Dox, Dlx].

Then the unidentifiedquantitiesare both bounded. The result is a bound
on aE(ylx)/lx, namely
(11) P(z = lIx)[8E(ylx, z = 1)/ax] + E(ylx, z = 1)[aP(z = lIx)/lx]

+ P(z = O|X)Do, + Ko,[aP(z = Ox)/lx] - aE(ylx)/ax
- P(z=

l|x)[aE(y|x,

z = l)/ax] + E(y x, z = l)[aP(z = l|x)/ax]
+ P(z = Olx)Dlx + Klx[aP(z = Oix)/ax].

I shall not discuss this bound further.The knowledgeneeded to obtain
it is much less readilyavailablethanthatwhich sufficesto boundthe finite
difference E(ylx = Q) - E(ylx = p). The support of y is often defi-

nitionallybounded. The derivative aE(y x, z = 0)/ax is rarely so.
C. Estimation of the Bound

A simpleway to estimate the boundon E(y ix) is to estimateE(y Ix, z = 1)
and P(zlx), both of which are identifiedby the samplingprocess. I shall
present an equivalent approach whose statistical properties are a bit
easier to derive.
First rewrite (5) in an equivalentform. Observe that
(12) E(yzlx) = E(yzIx, z = 1)P(z = l|x) + E(yzlx, z = 0)P(z = OIx)
E(y x, z=

1)P(z = lIx).

Also observe that
(13) P(z = Ox) = E(1 - zlx).

It follows from (12) and (13) that (5) may be rewrittenas
(5') E(yzlx) + KoxE(1 - zlx) - E(yix)<

E(yzlx) + KjxE(1 - zlx).

The above shows that to estimate the bound, it sufficesto estimatetwo
linear combinationsof E(yz lx) and E(1 - z x). I shall firstpose a simple
methodthat works at values of x havingpositive probabilityin the population. I shall then extend the method to make it work at any point in the
supportof x.
Consider a point e such that P(x = i) > 0. Let N denote the sample
size. The natural estimates for E(yzIx = t) and E(1 - z|x = t) are the

sample averages of yz and 1 - z across those observationsfor which x =

5, namely

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

Manski 349
N

YiZi

E

(14) bN-

l[

=

i=

> 1[x,=

j=i

and
N

3
i= 1

(15)

(1 - zi) l[x-

CN=

=

]

N

>
j=1

IW
j=

Note that bNg is computableeven thoughYiis not always observed;if z =
0, then yizi = 0. Given (5'), (14), and (15), we may estimate the bound at
5 by
(16)

[bNg + KogCNg, bN4 + K1CNJ].

This estimate is consistent. The strong law of large numbersimplies

that as N -> oo,
(17)

(bNt,

CN-)

-

[E(yzX

= ), E(1 - zx

=

)]

almost surely. Hence (16) converges almost surely to the true bound.
The estimate has a limitingnormaldistributionif, conditionalon x = ~,
the bivariaterandomvariable (yz, 1 - z) has finite variancematrix. Let

Ve denote the variance matrix. Let 1e = V/P(x = ). The central limit
theorem implies that as N -> oo,
(18)

vN[(bNt, CNt) - {E(yzlx =

E),E(1 - zlx = )}] -- N(,S(

)

in distribution.Hence IN times the difference between (bNq + KOtCNt,
bN + K1gCNt)and the true endpoints of the bound has a limiting normal distributionwith mean zero and variance matrix
O
[1 Kl
g I
Now consider the problem of estimatingthe bound at values of x that
have probabilityzero in the populationbut are in the supportof x. Thatis,
let |I |Idenote a norm and consider
< 8] > 0 for all 6 > 0.

e

such that P(x = i) = 0 but P[lJx - ~||

EstimatingE(yzjx = ~) and E(1 - zlx = /) by (14) and (15) clearly will
not work;with probabilityone there are no sampleobservationsfor which
x = t. On the other hand, it seems reasonableto estimate these quantities

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

350 The Journalof Human Resources
by the sample averages of yz and 1 - z across those observations for
which x is "close" to e, provided that one tightens the criterion of
closeness appropriatelyas the sample size increases. This intuitive idea
does work; it is the basis of nonparametricregressionanalysis.
To formalize the idea, let WNi(Q),i = 1, . . , N be chosen weights that
sum to one and redefine (bNe, CNe) to be estimates of the form
N

i() 1 zii.

(19)[LCN~
ZNgi=1i

The earlier definitions of (bNe,

special case in which

(20)

CNE)

are subsumed by (19); they are the

I[xi
X -= ~]

W(Ni()

l[xj=
j=1

A large menu of nonparametricregression estimates having the form
(19) are availablefor application.Perhapsthe simplestis the "histogram"
method. Here the researcherselects a "bandwidth"8 > 0 and lets
(21)

WNi()

N

lllx

-

3 l[llix -

< 6]

11< a]

j=1

If the weights are chosen as in (21) and if 8 is fixed, we have a consistent
1< 8), E(1 - z lix - 11< 8)]. On the other hand,
if the researcherlets 8 vary with the samplein a way that makes 8 -> 0 as
estimate of [E(yz IIx -

N -

oo, it is plausible that we obtain a consistent estimate of [E(yzlx),

E(1 - zlx)]. This turns out to be so, provided that the rule used to
choose 8 makes 8 -> 0 sufficiently slowly as N -

oo.

Two classes of nonparametricregression methods that have drawn
much attention are the "kernel" estimatorsand the "nearest-neighbor"
estimators.Both classes have the form (19);they choose the weights Win
differentways. The histogramestimatoris a memberof the kernel class.
Bierens (1987) and Hardle (1988)provide excellent expositions of kernel
regression, complete with numericalexamples. PrakasaRao (1983) and
Hardle (1988) cover the nearest-neighborapproach.Manski (1988b)introduces a close cousin of the kernel and nearest-neighbormethods,
called "smallest neighborhood"estimation.
It would carryus too far afieldto survey here the asymptoticproperties
and operationalcharacteristicsof the many availableprocedures.A few
general remarkswill suffice.

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

Manski 351
First, almost any intuitivelyreasonableestimatorof the form (19) provides a consistent estimate of [E(yz Ix),E(1 - z Ix)].Most estimatorshave
limitingnormaldistributions,althoughnot always centered at zero. The
rate of convergence is generally slower than the /N rate obtainable in
classical estimation problems. Bierens (1987) introduces an easily computed kernel type estimate that converges as rapidly as is possible and
that has a limitingnormaldistributioncentered at zero.
Second, some researchersfind it uncomfortablethat so many different
choices of the weights WNi(t), i = 1, . ... , N yield estimates with similar

asymptoticproperties.Simplyput, the problemis that the availablestatistical theory gives the researcher too little guidance on choosing the
weights in practice. Many researchersadvocate use of "cross-validation"
to select an estimator. In cross-validation,one computes alternativeestimates on subsamples and uses them to predict y on the complementary
subsamples. One selects the estimate which has the best predictive
power.
Third, there is consensus among practitionersthat nonparametricregression methodsusually work well when the regressorvariablex has low
dimension. On the other hand, it is common to find that, in samples of
realistic size, performanceis poor when the dimensionof x is high. This
phenomenon has led some researchers to develop approaches that impose dimension-reducingrestrictions on the regression but remain nonparametricin part. Section III.C. will describeapplicationsto the analysis
of selection problems.
D. An Empirical Example: Attrition in a Survey of the Homeless

To illustrate the bound, I consider a selection problem that arose in a
recent study of exit from homelessness undertakenby Piliavinand Sosin
(1988). These researcherswished to learn the probabilitythat an individual who is homeless at a given date has a home six monthslater. Thus the
populationof interest is the set of people who are homeless at the initial
date. The variabley is binary, with y = 1 if the individualhas a home six
months later and y = 0 if he or she remainshomeless. The regressorsx
are individualbackgroundattributes.The objective is to learn E(y x) =

P(y = llx).

The investigatorsintervieweda randomsampleof the people who were
homeless in Minneapolis in late December 1985. Six months later they
attemptedto reinterviewthe originalrespondentsbut succeeded in locating only a subset. So the selection problem is attritionfrom the sample:
z = 1 if a respondent is located for reinterview,z = 0 otherwise.
Let us first estimate the bound on a very simple regression, that in
which x is the respondent's sex. Considerthe males. First interviewdata

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

352 The Journalof Human Resources
were obtainedfrom 106 men, of whom 64 were located six monthslater.
Of the latter group, 21 had exited from homelessness. So the estimate of
E(yz male) is bNmale = 21/106 and that of E(1 - z male) is cNmale = 42/
106. The estimate of the bound on P(y = 1Imale) is [21/106,63/106] =
[.20, .59].
Now consider the females. Data were obtained from 31 women, of
whom 14 were located six months later. Of these, 3 had exited from
homelessness. So bNfemale = 3/31 and CNfemale = 17/31. The estimated
bound on P(y = |female) is [3/31, 20/31] = [.10, .65].
Interpretationof these estimates should be cautious, given the small
sample size. Takingthe results at face value, we have a tighterbound on
P(y = II male) than on P(y = 1 female). The attrition frequencies,
hence bound widths, are .39 for men and .55 for women. The important
point is that both bounds are informative.Havingimposedno restrictions
on the selection process, we are nevertheless able to place meaningful
bounds on the probabilitythat a personwho is homeless on a given date is
no longer homeless six months later.
The foregoingillustratesestimationof the bound when the regressoris
a discrete variable. To provide an example in which x is continuous, I
regress y on sex and an income variable. The latter is the respondent's
response, expressed in dollars per week, to the question "Whatwas the
best job you ever had? How much did thatjob pay?"
Usable responses to the income question were obtained from 89 men
and from 22 women. The sample of women is too smallto allow meaningful nonparametricregression analysis so I shall restrict attention to the
men. To keep the analysis simple, I ignore the selection problemimplied
by the fact that 17 of the 106men did not respondto the income question.
Let x be the bivariate regressor (male, income). Figure 1 graphs a
nonparametricestimate of P(z = 01x) on the sample income data. This
and the estimate of E(yzlx) were obtained by cross-validatedlogistic
kernel regression, using the programNPREG described in Manski and
Thompson (1987). Observe that the estimated attrition probability increases smoothly over the income rangewhere the data are concentrated
but seems to turn downwardin the high income rangewhere the data are
sparse.
1
The lower
Figure 2 graphs the estimate of the bound on P(y = lx).
bound is the estimate of E(yz x), which is flat on the income rangewhere
the data are concentrated but turns downward eventually. The upper
bound is the sum of the estimates for E(yzlx) and for P(z = 0Ix).
Observe that the estimated bound is tightest at the low end of the
income domainand spreadsas income increases. The intervalis [.24, .55]
at income $50 and [.23, .66] at income $600. This spreadingreflects the

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

Manski 353
0.50-

0.45-

;

0.40*

4-

<
0.35-

0.30

/

'
0

300

'

'

'

600

900

1200

Males, highest weekly income
Figure 1
P(z = Olx)
fact, shown in Figure 1, that the estimated probabilityof attrition increases with income.

III. Separability Restrictions Derived from Latent
Variable Models
Prevailingpracticein the econometricliteratureon selection
is to identify E(y x) by assumingthat E(y x, z = 1) is the sum of E(y x)
and another function that is distinguishablefrom E(ylx). Suppose it is
known that E(y x) and E(y x, z = 1) have the forms
(22a) E(y x) = gl(x)
(22b) E(y x, z = 1) = gl(x) + g2(x)

for some g1 E G1 and g2 E G2, where G1and G2are specifiedfamilies of
functions mappingx into the real line. The samplingprocess identifies
E(ylx, z = 1); hence g1(*) + g2(*) is identified.The functions gl and g2

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

l

1500

354 The Journalof HumanResources
1.00 -

0.90

0.80 0.70 . 60

0.50-

? 0.400.30 _MUSS_

I +N

+

*
.

..

0.20 0.10 0.00

300

0

600

900

1200

Males, highest weekly income

1500

Figure 2
Bound on P(y = l|x)
can be separately identified if knowledge of g1(*) + g(*) is combined
with prior restrictionson G, and G2.

The literatureprovidesvariousspecifications
for (G1,G2)thatsuffice.
Thesespecifications
havebeenmotivatedby referenceto the latentvariablemodel
(23) y =f(x) + ul
(24) E(uljx) = 0
(25)

z = l[f2x)

+

2 > 0],

where [f (*),f2(*)] are real functionsof x and (ul, U2) are unobservedreal

randomvariables.Condition(24) normalizeslocationif fi(*) is unrestrictedbut is an assumptionotherwise.
The latentvariablemodelimpliesthat
(26a) E(y x) = f (x)
(26b) E(ylx, z = 1) = fi(x) + E[u |x, f2(x) + U2 > 0].

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

Manski 355
So (22) holds with
(27a) gl(x) = fi(x)
(27b) g2(x) = E[ul x,f2(x) + u2 > 0].

Restrictionsimposed on fi (*) translatedirectly into a specificationof G1.
Restrictionsimposedonf2(*) and on the distributionof (ul, u2)conditional
on x induce a specificationof G2.
Sections III.A. throughIII.C. consider three restrictionsthat have received considerable attention. In each case I give the resulting specification of (G1, G2). The restrictionsto be discussed are neither nested
nor mutuallyexclusive. A latent variablemodel may impose any combination of the three.
A. The Model with Conditionally Independent Disturbances

The early literatureassumed that ul and u2 are statisticallyindependent
conditionalon x. This and (24) imply that
(28) E(ul x,f2(x) + u2 > 0) = E(u1 x) = 0.

So G2 contains only the function g2(x) = 0. In other words, the conditional mean independencerestriction(2) holds.
The model with conditionally independent disturbances imposes no
restrictions on fi(*). Hence this model has no implicationsbeyond (2),
which just identifies E(ylx). In practice, researchershave typically imposed supplementaryrestrictionson fi(*); most of the applied literature
makesfi (*) linear.
B. Parametric Models

A second type of restrictionbecame prominentin the middle 1970s. Suppose thatfi(*) is known up to a finite dimensional parameter P1,f2(*) up to

a finite dimensionalparameter 32, and the distributionof (u1, u2) conditional on x up to a finite dimensionalparametery. Then (27) implies that
G1is a finitedimensionalfamilyof functionsparameterizedby p1and G2is
a finite dimensional family parameterized by (12, Y). So we may write
(29a) E(ylx) = gl(x, 13)

(29b) E(yIx, z = 1) = gl(x, pi) + g2[x, (22,

'Y)].

Sufficiently strong parametricrestrictions identify pi, hence E(yIx).

One widely applied model makes fi(*) and f2(*) linear functions, (ul, u2)

statisticallyindependentof x, and the distributionof (ul, u2) normalwith

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

356

The Journalof Human Resources
mean zero. See Heckman (1976). In this case,
(30a) E(y|x) = x'1i
(30b) E(yjx, z = 1) = x'3

+ 'y(x',P2)/I)(x'r2),

where rb(*)and 4(*) are the standardnormal density and distribution
functions and where y = E(u1, u2). Identificationof pI hinges on the fact
that the linear function x'fp and the nonlinear'y(x'f2)/I(x'T2) affect
E(ylx, z = 1) in differentways.
There is a commonperceptionthat the normal-linearmodel generalizes
the model with conditionallyindependentdisturbances.Barros(1988)observes that the two models are, in fact, not nested. The normal-linear
model permits ul and u2 to be dependentbut assumes linearityof [fi(*),
f2(*)], normality of (ul, u2), and independence of (ul, u2) from x. The
model with conditionallyindependentdisturbancesassumes ul and u2 to
be independentconditional on x but restricts neither the form of [fi(*),
f2(*)], the distribution of ul conditional on x, nor the distribution of u2

conditionalon x. Given this, Barrosarguesthat the model with conditionally independentdisturbanceswarrantsrenewed attention.
C. Index Models

Parametricmodels have increasingly been criticized for their fragility;
seemingly small misspecificationsmay generatelargebiases in estimates
of E(ylx). Several articles have reported that estimates obtained under
the normal-linearmodel are sensitive to misspecification.Hurd(1979)has
shown the consequences of heteroskedasticity.Arabmazarand Schmidt
(1982) and Goldberger(1983) have described the effect of nonnormality.
The lack of robustnessof parametricmodels is particularlysevere when
the x componentsthat enter g2 are the same as those that determinegl. In
this case, identificationof g1 hinges entirely on the imposed functional
formrestrictions.Recognitionof this has led to the recent developmentof
a thirdclass of models, one which weakens functionalformrestrictionsat
the cost of imposing exclusion restrictions.
Let hl(x) and h2(x)be "indices" of x; that is, many-to-onefunctionsof
x. Suppose thatfi (x) is known to vary with x only throughhl(x). Suppose
thatf2(x) and the distributionof (u1, u2) conditionalon x are known to
vary with x only through h2(x). Then G1 is a family of functions that
dependon x only throughhl(x) and G2is a familyof functionsthat depend
on x only throughh2(x). So we may write
(31a) E(ylx) = gl[hi(x)]
(31b) E(ylx, z = 1) = g1[hl(x)] + g2[h2(x)].

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

Manski 357
An example is the model in whichfi(x) = fi [h(x)], f2(x) = f2[h2(x)],
and (ul, u2) is statistically independent of x. This model weakens the
assumptionsof the normal-linearmodel in some respects but strengthens
them in others. The index model does not force fi andf2 to be linear nor
the distributionof (ul, u2)to be normal.On the otherhand, it assumes that
fi andf2 are determinedby distinctindices, a conditionnot imposedby the
normal-linearmodel.
Whencombinedwith restrictionson the familyGI of feasible regression
functions, index restrictionscan identify gl. Powell (1987)expresses the
basic idea, which is to difference-outthe function g2 as in fixed effects
analyses of panel data.
Let (~, p) denote a pair of points in the supportof x such that h2()
h2(p)but hi(7) $7hi(p). For each such pair, (31b) implies that
(32) E(ylx = t, z = 1) - E(ylx = p, z = 1) = gl[hl(})] - g[hi(p)].

The left-handside of (32) is identifiedby the samplingprocess. The righthand side is determined by the function of interest gl and not by the
"nuisance" function g2. Hence (32) restricts gl. Identificationhinges on
whether the support of x contains enough pairs (t, p) for (32) to pin gl
down to a single function within the family of feasible functions G1.
The statistics literatureon "projectionpursuit" regression offers approaches to the estimation of gl when the family G1 is restricted only
qualitatively.See Huber (1985). Econometriciansstudyingindex models
have typically assumed that gl is linear. See Ichimuraand Lee (1988),
Powell (1987),and Robinson(1988)for alternativeestimationapproaches.
The first two papers are concerned with an extension of the index model
in which the form of the index function h2 is not known but is estimable.
As the dates of the foregoing citations indicate, the literatureon index
models is young. The work so far has been entirelytheoretical.Empirical
applicationshave yet to appear.
D. Latent Variable Models and the Bound Restriction

It is of interest to juxtapose the restrictionson E(ylx) implied by latent
variablemodels with those impliedby a boundon the conditionalsupport
of y. For purposesof this discussion, I shall supposethat the boundon y is
specified properly. This is an assumption in some applicationsbut is a
truism when y is definitionallybounded.
Consider a researcherwho has specified a latent variablemodel. The
researchercan check whether the hypothesis [E(y x) = fi(x)] is consistent with the bound on E(y x). I use the informalterm "check" rather
than the formal one "test" intentionally; sampling theory for these
bounds-checks remains to be developed.

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

358 The Journalof Human Resources
For example, suppose that the researcher has specified the normallinearmodel. Normalityof ul impliesthaty has unboundedsupportconditional on x. Hence acceptance of the normal-linearmodel impliesthat the
bound on E(y x) is ineffective. But the boundon the conditionaldistribution P(y < T|x), T E R1 is effective, as was shown in Section II.A. The
normal-linearmodel implies that
(33) P(y ' tIx) = (D[(T - X'P1)/r]

T

E R1,

where rl is the standarddeviation of ul. So we may check the validityof
the model by estimating (PI, tl1), computing the estimate of (33), and
comparingthe result with an estimate of the bound on the conditional
distributionfunction.
It may be thought that bounds-checks of latent variable models are
impracticalin those applicationswhere the dimensionof x is high. The
ostensible reason, stated in Section II.C., is that nonparametricregression estimation tends to perform poorly in high dimensional settings.
Nevertheless, informativechecks are practical, as follows.
Consider E(y x E A), where A is any region in x-space such that
P(x E A) > 0. The bound on E(ylx E A) is easily estimatedby (16). Let
a latent variable model be specified. The model implies that E(ylx) =
fi(x). Hence it implies that
(34) E(ylx E A) = E(y l[x E A])/P(x E A)
= Ex{E(ylx) l[x E A]}/P(x E A)
= Ex{f1(x) l[x E A]}/P(x E A).

LetfiN(x) be an estimate offi (x). Then the latent variablemodel implies
the following estimate of E(y x E A):
N

L

l[xi E A]

flN(Xi)

(35) EN(Ylx E A)

i=1

l[xj E A]
j=l

One may check the latent variable model by comparing(35) with the
estimate of the bound on E(y Ix E A).

IV. Conclusion
Fifteen years ago few economists paid attentionto the fact
that selective observation of random sample data has implicationsfor

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

The Journalof Human Resources
empiricalanalysis. Then the professionbecame sensitizedto the selection
problem. The heretofore maintainedassumption,conditionalmean independence of y and z, became a standardobject of attack. For a while the
normal-linearlatent variable model became the standard"solution" to
the selection problem. But researchers soon became aware that this
model does not solve the selection problem. It trades one set of assumptions for another.
Today there is no conventional wisdom. Some applied researchers,
such as LaLonde (1986), have leaped from disenchantmentwith the normal-linearmodel to the conclusion that econometricanalysis is incapable
of interpretingobservationsof naturalpopulations.In rebuttal,Heckman
and Hotz (1988) argue that latent variable models are useful empirical
tools provided that applied researcherstake seriously the task of model
specification.
Econometriciansare seeking to widen the menu of separableregression
specifications derived from latent variable models. The recent work on
index models weakens the parametricassumptionsof the normal-linear
model at the cost of requiringexclusion assumptions. There is also a
revival of interest in the model with conditionally independent disturbances.
I find the currentdiversity of opinionunsurprising.Moreover, I expect
it to persist. Selection creates an identificationproblem. Identification
always depends on the priorknowledgea researcheris willingto assert in
the applicationof interest. As researchersare heterogeneous, so must be
their perspectives on the selection problem.
Econometricianscan assist empiricalresearchersby clarifyingthe nature of the selection problem and by wideningthe menu of prior restrictions for which estimation methods are available. Work on restrictions
derived from latent variable models is welcome. I also believe that researchers should routinely estimate the simple bound developed in Section II. To bound E(y x) one need only be able to bound the variabley.
One need not accept the latent variable model.

References
Arabmazar,A., and Schmidt, P. 1982. "An Investigationof the Robustness of
the Tobit Estimatorto Non-Normality."Econometrica50:1055-63.
Barros, R. 1988. "NonparametricEstimationof CausalEffects in
ObservationalStudies." Chicago:Economic Research Center, NORC,
University of Chicago.
Bierens, H. 1987. "Kernel Estimatorsof RegressionFunctions." In Advances
in Econometrics, volume I, ed. T. Bewley. New York: CambridgeUniversity
Press.

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

359

360

The Journal of Human Resources
Goldberger,A. 1983. "AbnormalSelection Bias." In Studies in Econometrics,
Time Series, and Multivariate Statistics, ed. T. Amemiya and I. Olkin.

Orlando:Academic Press.
Gronau,R. 1974. "Wage Comparisons-A Selectivity Bias." Journalof
Political Economy 82:1119-43.
Hardle, W. 1988. Applied Nonparametric Regression. Bonn, West Germany:

Rheinische-Friedrich-Wilhelms
Universitat.
Heckman, J. 1976. "The CommonStructureof StatisticalModels of
Truncation,Sample Selection, and LimitedDependentVariablesand a
Simple Estimator for Such Models. Annals of Economic and Social
Measurement 5:479-92.

Heckman, J., and Hotz, J. 1988. "ChoosingamongNonexperimentalMethods
for Estimatingthe Impact of Social Programs:The Case of Manpower
Training." Journal of the American Statistical Association. Forthcoming.

Huber, P. 1981. Robust Statistics. New York: Wiley.
. 1985. "Projection Pursuit." Annals of Statistics 13:435-75.

Hurd, M. 1979. "Estimationin TruncatedSamples When There Is
Heteroskedasticity." Journal of Econometrics 11:247-58.

Ichimura,H., and Lee, L. 1988. "SemiparametricEstimationof Multiple
Indices Models: Single EquationEstimation."Minneapolis:Departmentof
Economics, University of Minnesota.
Klepper, S., and Leamer, E. 1984. "ConsistentSets of Estimatesfor
Regressionswith Errorsin All Variables."Econometrica52:163-83.
LaLonde, R. 1986. "Evaluatingthe EconometricEvaluationsof Training
Programswith ExperimentalData." AmericanEconomicReview 76:604-20.
Manski, C. 1988a. "Identificationof Binary Response Models." Journalof the
American Statistical Association 83:729-38.
. 1988b. Analog Estimation Methods in Econometrics. London:

Chapmanand Hall.
Manski, C., and Thompson, S. 1987. "MSCOREwith NPREG:Documentation
for Version 1.4." Madison:Departmentof Economics, University of
Wisconsin.
McFadden,D. 1975. "Tchebyscheff Bounds for the Space of Agent
Characteristics." Journal of Mathematical Economics 2:225-42.

Piliavin, I., and Sosin, M. 1988. "Exiting Homelessness: Some Recent
EmpiricalFindings." Madison:Institutefor Researchon Poverty, University
of Wisconsin. In preparation.
Powell, J. 1987. "SemiparametricEstimationof BivariateLatent Variable
Models." Social Systems Research InstitutePaper8704. Madison:University
of Wisconsin.
Prakasa Rao, B. L. S. 1983. Nonparametric Functional Estimation. Orlando:

Academic Press.
Robinson, P. 1988. "Root-N-ConsistentSemiparametricRegression."
Econometrica 56:931-54.

Varian,H. 1985. "NonparametricAnalysis of OptimizingBehaviorwith
Measurement Error." Journal of Econometrics 30:445-58.

This content downloaded on Sun, 13 Jan 2013 09:56:35 AM
All use subject to JSTOR Terms and Conditions

