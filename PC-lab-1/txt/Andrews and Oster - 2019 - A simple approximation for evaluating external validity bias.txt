Economics Letters 178 (2019) 58–62

Contents lists available at ScienceDirect

Economics Letters
journal homepage: www.elsevier.com/locate/ecolet

A simple approximation for evaluating external validity bias✩
∗

Isaiah Andrews a,c , , Emily Oster b,c
a

Department of Economics, Harvard University, 1805 Cambridge St, Cambridge, MA 02138, 617-496-2720, United States
Department of Economics, Brown University, 64 Waterman Street, Providence, RI 02912, United States
c
NBER, United States
b

highlights
•
•
•
•
•

Experimental estimates may not be externally valid.
We provide an approximation that may be used to evaluate external validity bias.
The approximation relates the external validity bias to two terms.
The first measures bias from selection on observables.
The second measures the role of treatment effect heterogeneity in driving selection.

article

info

Article history:
Received 20 December 2018
Received in revised form 16 February 2019
Accepted 22 February 2019
Available online 25 February 2019

a b s t r a c t
We develop a simple approximation that relates the total external validity bias in randomized trials to
(i) bias from selection on observables and (ii) a measure for the role of treatment effect heterogeneity
in driving selection into the experimental sample.
© 2019 Elsevier B.V. All rights reserved.

JEL classification:
C1
Keywords:
External validity
Randomized trials

1. Introduction
External validity has drawn attention in economics with the
growth of randomized trials. Randomized trials provide an unbiased estimate of the average treatment effect in the experimental
sample, but this may differ from the average treatment effect in
the population of interest. We will refer to such differences as
‘‘external validity bias’’.
One reason such differences may arise is because individuals
(or other treatment units) actively select into the experiment. For
example, Bloom et al. (2015) report results from an evaluation of
✩ This paper was previously circulated under the title ‘‘Weighting for External
Validity.’’ Sophie Sun provided excellent research assistance. We thank Nick
Bloom, Guido Imbens, Matthew Gentzkow, Peter Hull, Larry Katz, Ben Olken,
Jesse Shapiro, Andrei Shleifer seminar participants at Brown University, Harvard
University and University of Connecticut, and an anonymous reviewer for
helpful comments. Andrews gratefully acknowledges support from the Silverman
(1978) Family Career Development chair at MIT, and from the National Science
Foundation under grant number 1654234.
∗ Corresponding author at: Department of Economics, Harvard University,
1805 Cambridge St, Cambridge, MA 02138, 617-496-2720, United States.
E-mail address: iandrews@fas.harvard.edu (I. Andrews).
https://doi.org/10.1016/j.econlet.2019.02.020
0165-1765/© 2019 Elsevier B.V. All rights reserved.

working from home in a Chinese firm. Workers at the firm were
asked to volunteer for the experiment, and the study randomized
among eligible volunteers. External validity bias arises here if the
effects of the treatment on the volunteers differ from effects in
the overall population.
Papers that report experimental results often comment qualitatively on differences between the sample and some population
of interest, and sometimes provide a table comparing means
between groups (e.g. Bloom et al., 2015; Attanasio et al., 2011;
Muralidharan et al., 2018). Similar means are taken as reassuring.
This comparison of means does not fully address external
validity concerns. First, external validity bias depends on both
the difference in characteristics across groups and the extent to
which treatment effects vary based on these characteristics. Second, this approach does not rule out differences in unobservable
characteristics across groups.1 To partially address the first concern, we can formally adjust for observed differences in covariates
1 For example, individuals could sort directly on their treatment effects,
which in an instrumental variables context Heckman et al. (2006) describe as
‘‘essential heterogeneity’’.

I. Andrews and E. Oster / Economics Letters 178 (2019) 58–62

59

(e.g. Hotz et al., 2005; Stuart et al., 2011; Dehejia et al., 2015).
This is rarely done in practice, however, and does not address
differences on unobserved dimensions.2
Our goal is to provide a framework in which to consider selection on unobservables when studying external validity. Under a
simple model for selection and treatment effects, if the degree of
selection is small, external validity bias is approximately equal to
the bias due to selection on observables multiplied by a measure
of the role of treatment effect heterogeneity in driving selection.
We suggest that researchers report formal corrections for selection on observables and then use our result to benchmark how
much selection on unobservables would be required to overturn
their findings. This approach does not provide a definitive estimate of external validity bias, but offers a tractable language to
frame the question.
There are alternatives to our approach. One could adopt an
approach directly grounded in economic theory, and estimate
an model for the unobserved factors driving selection into the
sample (perhaps fitting a Roy model as in e.g. Borjas, 1987). This
has intuitive appeal, but in many settings it may be challenging
to model selection. As a result, many existing approaches to this
problem do not directly model the selection decision.
For example, Nguyen et al. (2017) assume bounds on the bias
from unobservables, while Gechter (2016) restricts the level of
dependence between the individual outcomes in the treated and
untreated states. If data from multiple experimental sites is available, Meager (2019) and Vivalt (2019) use Bayesian hierarchical
approaches to assess external validity under the assumption that
the effects at observed and future sites are exchangeable.
These alternatives have the advantage that, unlike our approach, they do not rely on the assumption that the degree of
selection is small. At the same time, Meager (2019) and Vivalt
(2019) require data from multiple sites, while the approaches
of Nguyen et al. (2017) and Gechter (2016) require assumptions
on, respectively, the bias from unobservables and the dependence
of treated and untreated potential outcomes. Our approach also
requires that researchers have a view on what degree of selection
on unobservables (relative to observables) they think is plausible
in a given setting, but we think that this will often be the case.

We do not observe the distribution of all variables in the
target population, and so cannot in general correct this bias. We
assume, however that we know the target-population-mean for
a set of covariates Ci , EP [Ci ], where Ci is also observed in the trial
population.4

2. A simple approximation to external validity bias

so the conditional average treatment effect given covariates and
unobservables is the same in the trial and target populations. The
external validity bias is thus

2.1. Setup

2.2. A simple model
We next adopt a simple model for treatment effects and
selection.
Assumption 1. For a set of unobservables Ui ,
TEi = α + Ci′ γ + Ui′ δ + εi
where EP [εi |Ci , Ui ] = 0 and Cov (Ci , Ui ) = 0.
Without additional restrictions on the unobservables Ui , this
assumption is without loss of generality. In practice, however,
we will typically want to assume that Ui consists of particular
known (but unobserved) variables, which makes this restriction
substantive.
Assumption 2. For the same set of unobservables Ui ,
Si = 1 Ci′ κ + Ui′ τ − vi ≥ 0

{

}

where vi is independent of (Ci , Ui , εi ) under P and has support
equal to R. The distribution function Fv of vi is twice continuously
differentiable with a bounded second derivative.

(Assumption
) 2 is equivalent (to assuming
) that EP [Si |C∂i 2, Ui ] =
Fv Ci′ κ + Ui′ τ where 0 < Fv Ci′ κ + Ui′ τ < 1 and ∂v
2 Fv (v)
is bounded. This implies that all values (Ci , Ui ) that arise in the
target population also sometimes arise in the trial population.
2.3. Corrections for selection on observables
Assumptions 1 and 2 imply that
EPS [TEi |Ci , Ui ] = EP [TEi |Ci , Ui ] = α + Ci′ γ + Ui′ δ,

)′

(1)

)′

We are interested in the effect of a binary treatment Di ∈
{0, 1} on an outcome Yi . Adopting the standard potential outcomes framework (see e.g. Imbens and Rubin (2015)) we write
the outcomes of unit i in the untreated and treated states as Yi (0),
Yi (1), respectively.3
We observe an iid sample from a randomized experiment
in a trial population with distribution PS , where treatment Di
is randomly assigned. The experiment allows us to unbiasedly
estimate the average treatment effect in the trial population,
EPS [TEi ] = EPS [Yi (1) − Yi (0)].
The trial population is a potentially non-representative subset
of a larger target population, and we are interested in inference
on the average treatment effect in the target population. Let Si be
a dummy equal to one if individual i in the target population is a
member of the trial population. For P the distribution in the target
population, our object of interest is EP [TEi ], while our experiment
estimates EPS [TEi ] = EP [TEi |Si = 1]. The ‘‘external validity bias’’ is
EPS [TEi ] − EP [TEi ].

EPS [TEi ] − EP [TEi ] = EPS [Ci ] − EP [Ci ] γ + EPS [Ui ] − EP [Ui ] δ.

2 A notable exception is Allcott (2015).
3 We assume throughout that all random variables considered have bounded
fourth moments.

4 For example, C could contain demographic or geographic variables.
i
5 Many antecedents for (2) exist in the literature. See for example Nguyen
et al. (2017).

(

(

(2)
Hence, the external validity bias depends on (a) the shift in the
mean of (Ci , Ui ) between the trial and target populations and (b)
the importance of (Ci , Ui ) for predicting treatment effects.5
If δ = 0, so the unobservables do not predict treatment
effects, then external validity bias depends only on the difference
in means for the covariates, EPS [Ci ] − EP [Ci ], and the coefficient
γ . As discussed in the introduction the difference of means is
sometimes reported, but the coefficient γ is rarely discussed. We
can, however,
γ as the difference in coefficients γ̂ =
( estimate
)
γ̂1 − γ̂0 for γ̂0 , γ̂1 calculated from the regression
Yi = (1 − Di ) α0 + (1 − Di ) Ci′ γ0 + Di α1 + Di Ci′ γ1 + ui
of Yi on Ci in the treatment and control groups. If we assume that
δ = 0 we can easily estimate (and correct) external validity bias.

60

I. Andrews and E. Oster / Economics Letters 178 (2019) 58–62

2.4. Small-selection approximation
If we do not assume δ = 0, the external validity bias depends
on terms we cannot estimate. To make progress we consider
settings where the degree of selection is small, and in particular
consider behavior as (κ, τ ) → 0. We then relate the external
validity bias to the bias estimated by assuming that δ = 0. Given
that it is common not to formally address external validity at all,
we think that considering the case where the degree of selection
is small is a natural first step.
Let γS denote the probability limit of our estimate γ̂ obtained
from regression (1) in the trial population. (The probability limit
)′
of our bias estimate based on observables is EPS [Ci ] − EP [Ci ] γS .
This estimated bias bears an intuitive relationship to the true bias
when the degree of selection is small.
Proposition 1. Under Assumptions 1 and 2, for (κ, τ ) = λ · (κ̃, τ̃ )
and (κ̃, τ̃ ) fixed, as λ → 0
EPS [TEi ] − EP [TEi ]

(

)′

EPS [Ci ] − EP [Ci ] γS

−

γ ′ ΣC κ + δ ′ ΣU τ
→ 0,
γ ′ ΣC κ

provided γ ′ ΣC κ̃ ̸ = 0, where ΣC = VarP (Ci ) and ΣU = VarP (Ui ).
This is our main result, and links the (estimable) selection-onobservables bias to the true external validity bias.
Validity of approximation. Proposition 1 discusses behavior as
(κ, τ ) → 0. This can be interpreted as an approximation result,
and shows that
EPS [TEi ] − EP [TEi ] ≈

)′
γ ′ ΣC κ + δ ′ ΣU τ (
EPS [Ci ] − EP [Ci ] γS
γ ′ ΣC κ

in the sense that the difference is of lower order for (κ, τ ) small.
Since in practice we are interested in settings with a nonzero degree of selection, it is reasonable to ask when this approximation
will be reliable. The proof of Proposition 1 in Appendix
( proceeds)
by (i) taking a first-order Taylor approximation of Fv Ci′ κ + Ui′ τ
and (ii) approximating γS by γ . We expect that the result of
Proposition
a reasonable approximation so long
( 1 will provide
)
as (a) Fv Ci′ κ + Ui′ τ is not overly nonlinear over the region
containing most realizations of (Ci , Ui ) and (b) γS is close to γ .
Interpretation:. The key unknown term in Proposition 1 is the
selection ratio

Ψ =

γ ′ ΣC κ + δ ′ ΣU τ
.
γ ′ ΣC κ

This ratio measures the relative importance of treatment effect
heterogeneity in explaining the observed and unobserved drivers
of selection. In particular,
CovP TEi , τ ′ Ui

(

Ψ =1+

)

CovP (TEi , κ ′ Ci )

,

where we can interpret κ ′ Ci and τ ′ Ui as the observed and unobserved drivers of selection.
To develop intuition, we consider four special cases.
Special Case 1 δ = 0: unobservables are unrelated to treatment effects, so Ψ = 1 and the correction for observable
differences discussed above is valid.
Special Case 2 τ = 0: unobservables may predict treatment effects but play no role in selection. We again have Ψ = 1, so
the correction for observable differences is (approximately)
valid.

Special Case 3 δ ̸ = 0, τ ̸ = 0, but δ ′ ΣU τ = 0: unobservables predict both treatment effects and selection, but the
unobserved drivers of selection and treatment effects are
unrelated. Hence, Ψ = 1 and the correction for observable
differences is (approximately) valid.
Special Case 4 (γ , δ) ∝ (κ, τ ): the same combinations of observables and unobservables matter for both selection and
treatment effects. Hence,

Ψ =

R2C ,U
R2C

for R2X the R-squared from the regression of TEi on Xi .
Here, Ψ can be interpreted as the proportional increase in
R2 from including the unobservables Ui in an (infeasible)
regression of TEi on covariates. This implies that Ψ ≥ 1, so
the correction for observable differences is a lower bound
on the true bias.
The fourth special case and the general case are likely to be of the
most interest, since they do not assume away bias from selection
on unobservables. The result in the fourth special case delivers
sharper conclusions, since we get a lower bound on the external
validity bias, but the result in the general case is more widely
applicable.
3. Illustrative application
To illustrate, we apply our results to data from Bloom et al.
(2015). Workers at a Chinese call center were given an opportunity to volunteer for a work-from-home program. Approximately
50% volunteered, and treatment was randomized among eligible volunteers. The results suggest substantial productivity gains
from working from home.
A follow-up question is whether it would be productivityenhancing to have many or all eligible call center employees
work from home. If the ATE estimated in the experiment is valid
for the entire workforce, the answer is likely yes. Given the
sample construction, however, it seems plausible that the ATE
for the experimental sample is not representative of the whole
population.
Target population data. The natural target population is the set
of all eligible workers. Bloom et al. (2015) collect some basic
characteristics for this population, which are compared to characteristics of the volunteers in Table 1. There are some differences:
the volunteers have longer commutes, are more likely to be male,
and are more likely to have children.
Correcting for observables. We first correct for selection on observables. We estimate γ as the difference γ̂ = γ̂1 − γ̂0 in
coefficients from the regression
(1), and
(
)′ estimate the selection
on observables bias as ÊPS [Ci ] − ÊP [Ci ] γ̂S , where we use Ê to
denote the sample average.6 Results are reported in Table 2,
which shows that correcting for observable differences slightly
increases the estimated effect, from 0.271 to 0.289.
Accounting for unobservables. We next consider the scope for
further bias due to selection on unobservables. We bound the
target population average treatment effect under the assumption
that Ψ ∈ [1, 2], so bias due to unobservables operates in the
same direction as, and is no larger than, bias due to observables.
Estimates are reported in column three of Table 2. These are
similar to the baseline results.
6 We take C to include all of the variables reported in Table 1 and, for
i
non-binary variables, their squares.

I. Andrews and E. Oster / Economics Letters 178 (2019) 58–62
Table 1
Observable Characteristics, Bloom et al. (2015).
Variable

Population: Mean (SD)

Sample: Mean (SD)

Age
Gross Wage
Any Children
Married
Male
At Least Tertiary Educ
Commute Time (Min)
Job Tenure

24.4 (3.30)
3.13 (0.84)
0.155 (0.362)
0.265 (0.442)
0.385 (0.487)
0.456 (0.498)
96.9 (61.1)
32.4 (19.7)

24.7 (3.65)
3.09 (0.78)
0.201 (0.402)
0.310 (0.463)
0.438 (0.497)
0.399 (.490)
111.7 (62.7)
31.2 (20.6)

Notes: This table reports moments for the sample and target population in
Bloom et al. (2015).
Table 2
Application: Bloom et al. (2015).
Outcome

Baseline
Effect

Observable
Adjusted

Bounds,
Ψ ∈ [1, 2]

Ψ (0)

Job Performance

0.271
(0.22, 0.32)

0.289
(0.23,0.34)

[0.289, 0.309]

-14.7

Notes: Bootstrapped 95% confidence intervals are reported below the baseline
and observables-adjusted estimates. One can also calculate confidence sets for
the last two columns, but for brevity we do not explore this possibility here.

and
EPS [TEi ] − EP [TEi ]

)′

EPS [Ci ] − EP [Ci ] γ

We then ask what value Ψ (0) of the selection ratio Ψ would
yield an average treatment effect of zero in the target population.
This value is equal −14.7, so the bias from unobservables would
have to be much larger than the estimated bias from observables,
and operate in the opposite direction, in order to overturn the
main result.
Since the observables in this application include variables
(e.g. commute time, whether the worker has children) that seem
likely to play an important role in the decision to work from
home, both approaches suggest to us that the results of Bloom
et al. (2015) are robust to a wide range of plausible assumptions
about the role of unobservables.
4. Conclusion
This paper considers the problem of external validity, and
derives an approximation which relates the total external validity
bias to the bias from observables.
Our application to Bloom et al. (2015) is representative of a
class of applications in which participants select in to a study
(e.g. Attanasio et al., 2011; Gelber et al., 2016; Muralidharan et al.,
2018). Our approach applies more broadly, however, including
to settings where researchers select a set of areas or treatment
units for their experiments (i.e. Muralidharan and Sundararaman,
2011; Olken et al., 2014; Allcott, 2015).7 The only additional
data requirement to implement our approach is knowledge of
some characteristics of the target population. In many cases one
could use demographic variables, where moments in the target
population may be available from public datasets.
Appendix. Proof of Proposition 1
Note that under Assumptions 1 and 2, as in Olsen et al. (2013),
EPS [TEi ] − EP [TEi ] = EP [(Wi − 1) TEi ] = CovP (Wi , TEi )

(
(

)
.
)]

Fv Ci′ κ+Ui′ τ
EP [Fv Ci′ κ+Ui′ τ

7 Note that when the selection occurs at a different level than treatment,
Si will not be iid across units i but our results continue to apply provided we
define Ci and Ui to vary at same level as the selection decision.

)

c1 = )fv (0) /EP [Fv (0)] ̸ = 0. By Assumption 1, CovP
(where
′
′
′
′
(Ci′κ + Ui τ ,′TEi )= γ (Σ2C)κ + δ ΣU τ , so CovP (Wi , TEi ) = λc1
γ ΣC κ̃ + δ ΣU τ̃ + O λ . By the same argument
(
)
EPS [Ci ] − EP [Ci ] = CovP (Wi , Ci ) = λc1 CovP Ci′ κ + Ui′ τ , Ci
)
(
= λc1 κ̃ ′ ΣC + O λ2 ,

(

for Wi =

61

( )
value theorem Fv Ci′ κ + Ui′ τ = F (0) + fv vi∗
( ′ By the′ mean
)
Ci κ + Ui τ for fv (·) the density of vi and vi∗ an intermediate
value. Since fv (·) is continuously differentiable with a bounded
derivative it is Lipschitz, and |f⏐v (v) − fv (0)| (≤ K v for )some
constant K and all v . As a result, ⏐Fv (0) + fv (0) Ci′ κ + Ui′ τ − Fv
( ′
)⏐
(
)2
Ci κ + Ui′ τ ⏐ ≤ K · Ci′ κ + Ui′ τ . Hence, for (κ, τ ) = λ · (κ̃, τ̃ ),
CovP (Wi , TEi ) is equal to
(
)
(
)
( )
Fv (0) + fv (0) Ci′ κ + Ui′ τ
[
(
)] , TEi + O λ2
CovP
EP Fv (0) + fv (0) Ci′ κ + Ui′ τ
)
( )
(
= λc1 CovP Ci′ κ + Ui′ τ , TEi + O λ2
(

=

( )
γ ′ ΣC κ̃ + δ ′ ΣU τ̃
+ O λ2 ,
′
γ ΣC κ̃

where the denominator on the right hand side is nonzero by
assumption.
This nearly completes the proof, except that the proposition
replaces the γ on the left hand side by γS . Note, however, that
random assignment implies γS = VarPS (Ci )−1 CovPS (Ci , TEi ). By
arguments along the same lines as above, VarPS (Ci ) → VarP (Ci ) =
O(λ) and CovPS (Ci , TEi ) → CovP (Ci , TEi ) = O(λ) as λ → 0. Hence

)′

EPS [Ci ] − EP [Ci ] (γS − γ ) = o λ2 ,

(

( )

from which the result follows immediately. □
References
Allcott, Hunt, 2015. Site selection bias in program evaluation. Q. J. Econ. 130 (3),
1117–1165.
Attanasio, Orazio, Kugler, Adriana, Meghir, Costas, 2011. Subsidizing vocational
training for disadvantaged youth in Colombia: evidence from a randomized
trial. Amer. Econ. J. Appl. Econ. 3 (3), 188–220.
Bloom, Nicholas, Liang, James, Roberts, John, Ying, Zhichun Jenny, 2015. Does
working from home work? Evidence from a Chinese experiment. Q. J. Econ.
165, 218.
Borjas, George J., 1987. Self-selection and the earnings of immigrants. Amer.
Econ. Rev. 531–553.
Dehejia, Rajeev, Pop-Eleches, Cristian, Samii, Cyrus, 2015. From Local to Global:
External Validity in a Fertility Natural Experiment. In: Working Paper Series,
(vol. 21459), National Bureau of Economic Research.
Gechter, Michael, 2016. Generalizing the results from social experiments: theory
and evidence from Mexico and India. In: Manuscript. Pennsylvania State
University.
Gelber, Alexander, Isen, Adam, Kessler, Judd B., 2016. The effects of youth
employment: evidence from New York City lotteries. Q. J. Econ. 131 (1),
423–460.
Heckman, James, Urzua, Sergio, Vytlacil, Edward, 2006. Understanding instrumental variables in models with essential heterogeneity. Rev. Econ. Stat. 88
(3), 389–432.
Hotz, Joseph, Imbens, Guido W., Mortimer, Julie H., 2005. Predicting the efficacy
of future training programs using past experiences at other locations. J.
Econometrics 125 (1–2), 241–270.
Imbens, Guido, Rubin, Don, 2015. Causal Inference for Statistics, Social Science and Biomedical Sciences: An Introduction. Cambridge University Press,
Cambridge.
Meager, Rachael, 2019. Understanding the average impact of microcredit expansions: a bayesian hierarchical analysis of seven randomized experiments.
Amer. Econ. J. Appl. Econ. 11 (1), 57–91.
Muralidharan, Karthik, Singh, Abhijeet, Ganimian, Alejandro, 2018. Disrupting
education? Experimental evidence on technology-aided instruction in India.
In: Working Paper.
Muralidharan, Karthik, Sundararaman, Venkatesh, 2011. Teacher performance
pay: experimental evidence from India. J. Polit. Econ. 119 (1), 39–77.

62

I. Andrews and E. Oster / Economics Letters 178 (2019) 58–62

Nguyen, Trang Quynh, Ebnesajjad, Cyrus, Cole, Stephen R., Stuart, Elizabeth A., 2017. Sensitivity analysis for an unobserved moderator in
rct-to-target-population generalization of treatment effects. Ann. Appl. Stat..
Olken, Benjamin A., Onishi, Junko, Wong, Susan, 2014. Should aid reward
performance? Evidence from a field experiment on health and education
in Indonesia. Amer. Econ. J. Appl. Econ. 6 (4), 1–34.

Olsen, Robert B., Orr, Larry L., Bell, Stephen H., Stuart, Elizabeth A., 2013. External
validity in policy evaluations that choose sites purposively. J. Policy Anal.
Manag. 32 (1), 107–121.
Stuart, Elizabeth A., Cole, Stephen R., Bradshaw, Catherine P., Leaf, Philip J., 2011.
The use of propensity scores to assess the generalizability of results from
randomized trials. J. R. Stat. Soc. A 174 (2), 369–386.
Vivalt, Eva, 2019. How much can we generalize from impact evaluations? In:
Working Paper.

