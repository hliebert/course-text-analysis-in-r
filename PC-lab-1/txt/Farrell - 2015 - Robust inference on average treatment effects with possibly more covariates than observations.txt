Journal of Econometrics 189 (2015) 1–23

Contents lists available at ScienceDirect

Journal of Econometrics
journal homepage: www.elsevier.com/locate/jeconom

Robust inference on average treatment effects with possibly more
covariates than observations✩
Max H. Farrell ∗
University of Chicago Booth School of Business, 5807 South Woodlawn Avenue, Chicago, IL 60637, United States

article

info

Article history:
Received 30 October 2013
Received in revised form
3 April 2015
Accepted 7 June 2015
Available online 25 June 2015
JEL classification:
C21
C31
C52

abstract
This paper concerns robust inference on average treatment effects following model selection. Under
selection on observables, we construct confidence intervals using a doubly-robust estimator that are
robust to model selection errors and prove their uniform validity over a large class of models that allows
for multivalued treatments with heterogeneous effects and selection amongst (possibly) more covariates
than observations. The semiparametric efficiency bound is attained under appropriate conditions. Precise
conditions are given for any model selector to yield these results, and we specifically propose the group
lasso, which is apt for treatment effects, and derive new results for high-dimensional, sparse multinomial
logistic regression. Both a simulation study and revisiting the National Supported Work demonstration
show our estimator performs well in finite samples.
© 2015 Elsevier B.V. All rights reserved.

Keywords:
High-dimensional sparse model
Heterogeneous treatment effects
Uniform inference
Model selection
Doubly-robust estimator
Unconfoundedness
Group lasso

1. Introduction
Model selection has always had a place in empirical economics,
whether or not it is formally acknowledged. A key problem in
modern empirical work is that researchers face datasets with
large numbers of variables, sometimes more than observations.
A complementary problem is that economic theory and prior
knowledge may mandate controlling for certain variables, but are
generally silent regarding functional form. These two problems
force researchers to search for a model that is simultaneously

✩ An online supplement contains additional proofs and simulations. I am deeply
grateful to Matias Cattaneo for advice and support. I am indebted to Xuming He,
Lutz Kilian, and Jeffrey Smith for thoughtful feedback and discussions. I thank
Victor Chernozhukov for pointing to the relevant latest results, obtained in joint
work Alexandre Belloni and Christian Hansen, and the latter two authors for
conversations in the early stages of this project. I benefited from discussions with
Rosa Matzkin, Blaise Melly, and Jack Porter. I also thank the co-editor, Han Hong,
and two reviewers for their detailed comments and suggestions that improved the
paper.
∗ Tel.: +1 773 834 0161.
E-mail address: max.farrell@chicagobooth.edu.
URL: http://faculty.chicagobooth.edu/max.farrell/.

http://dx.doi.org/10.1016/j.jeconom.2015.06.017
0304-4076/© 2015 Elsevier B.V. All rights reserved.

parsimonious and adequately flexible. Many formal methods are
computationally infeasible with a large number of variables. A
typical response to this challenge is to iteratively search over
a small set of alternative specifications, guided only by the
researcher’s taste and intuition. But no matter the approach
used, subsequent inference almost never takes accounts for this
‘‘specification search’’ and the resulting confidence intervals are
not robust to model selection mistakes, and hence are unreliable
in empirical work.
This problem is particularly important in estimating average
treatment effects under selection on observables, because in this
framework using the right covariates is crucial for identification
and correct inference. In this context, we provide an easy-toimplement and objective method for covariate selection and postselection inference on average treatment effects.1 We establish
four main results for multivalued treatments effects with arbitrary

1 Treatment effects, missing data, measurement error, and data combination
models are equivalent under selection on observables. Thus, all our results
immediately apply to those contexts. For reviews of these literatures, see Tsiatis
(2006), Heckman and Vytlacil (2007), Imbens and Wooldridge (2009), and
Wooldridge (2010).

2

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

heterogeneity in observables and heteroskedasticity. First, we
show that a doubly-robust estimator is robust to model selection
errors. These estimators were initially developed for robustness
to parametric misspecification, but are now known to be robust
to selection.2 By taking explicit account of the model selection
stage and its inherent selection errors, we derive precise conditions
required for any model selector to deliver confidence intervals
for average treatment effects that are uniformly valid over a
large class of data-generating processes. Second, we show that
a simple refitting procedure allows researchers to augment
variables chosen according economic theory with data-driven
selection to deliver flexible inference that remains uniformly valid.
Third, we prove that our estimator is asymptotically linear, and
standard conditions imposed in the program evaluation literature,
semiparametrically efficient bound. Fourth, we derive new results
for multinomial (and binary) logistic regression, the most widely
used model for treatment assignment.
Inference following model selection is notoriously difficult. In
a sequence of papers, Leeb and Pötscher (2005, 2008a,b), Pötscher
and Leeb (2009) have shown that inference relying too heavily on
model selection cannot be made uniformly valid. Loosely speaking,
uniform validity of a confidence interval captures the idea that
the interval should have the same quality (coverage) for many
data-generating processes. This theoretical property is practically
important because it implies greater reliability in applications. Our
proposed methods for post model selection inference build upon
the path-breaking recent work of Belloni et al. (2014).
The crucial insight that leads to uniform inference is to change
the goal of model selection away from perfect covariate selection
(the oracle property) and to high-quality approximation of the
underlying functions. This fundamental shift in focus allows us
to circumvent, without contradicting, the impossibility results of
Leeb and Pötscher. Valid post-selection inference has attracted
considerable attention during the preparation of this paper: in
contexts and with methods quite different from ours, contributions
have been made by Belloni et al. (2013), Berk et al. (2013), Zhang
and Zhang (2014), Efron (2014), van de Geer et al. (2014),
and Belloni et al. (2014), among others.
Our approach, based on the doubly-robust estimator, has several key features. The name ‘‘doubly-robust’’ reflects that it is
robust to misspecification of either the treatment equation
(propensity score) or the outcome equation, a property obtained
by combining inverse probability weighting and regression imputation. First, we show that this robustness extends to model
selection, enabling us to allow for selection errors in both equations without impacting inference. Second, we capture arbitrary
treatment effect heterogeneity (dependence of the effect on an individual’s observed characteristics), which is crucial in empirical
work. With such heterogeneity, the average treatment effect and
the treatment on the treated differ, and hence we present results
for both. Third, the doubly-robust estimator also stems from the
semiparametric efficient moment conditions, and hence we obtain
the semiparametric efficiency bound, even under heteroskedasticity, under standard additional conditions. Thus, Pötscher’s (2009)
result that sparse estimators have large confidence sets is also circumvented. Taking all these features together enables us to obtain
uniform inference over such a large class of treatment effects models.
In recent independent work, Belloni et al. (2014), propose a
similar approach. Their main focus is inference on the linear part
of a partially linear model, which motivates an estimator quite

2 Doubly-robust estimation and its role in program evaluation is discussed by
Robins and Rotnitzky (1995), van der Laan and Robins (2003), Kang and Schafer
(2007, with discussion), Tan (2010), and references therein.

different from ours, but it will recover the average treatment
effect in the special case of a binary treatment where the effect is
constant across observables. However, their Section 5, developed
independently from our work, considers heterogeneous effects and
proposes an estimator based on the efficient influence function,
similar to ours. There are two broad differences. First, we allow for
multivalued treatments, which offers a larger set of estimands and
can thus enhance the understanding of program impacts.3 In this
context we propose a group lasso based approach that naturally
exploits the already-present structure of treatment effects data to
improve model selection by pooling information across treatment
levels. This is particularly natural in the multivalued case, but even
in the binary case there is still a grouped structure in the outcome
regressions, though not in treatment assignment (i.e., in propensity
score estimation). Second, although in both cases the doublyrobust estimator is used for average treatment effects4 (following
a quite different model selection step), we show that this estimator
has two benefits: (i) it may require weaker conditions on the
first stage (see Assumption 3); and (ii) it does not require using
variables selected for the treatment equation in the outcome
model, and vice versa (‘‘post double selection’’), and indeed, doing
may require additional assumptions (see Assumption 5).
Our analysis is conducted under selection on observables,
which has a long tradition and remains quite popular in empirical
economics.5 Covariates play three crucial roles in this framework.
First, using more observed covariates as proxies, and more flexibly,
may help account for unobserved confounding and hence increase
the plausibility of unconfoundedness. Second, some observed
variables may not be part of the causal mechanism under study,
and should be excluded. Third, the efficient conditioning set are
those variables that drive the outcome, not necessarily those
important for treatment assignment. This reasoning mandates
contradicting goals for practitioners: a large, rich set of controls
on the one hand, and parsimony on the other. Our approach is a
formal, theory-driven attempt to reconcile this contradiction.
A special feature of our analysis is that we match the empirical
realities of large datasets by considering selection from amongst
(possibly) more covariates than observations, so-called highdimensional data. The goal of variable selection is to find a small
model that is nonetheless sufficiently flexible to capture unknown
features of the data-generating process required for inference. If a
small model can perfectly capture the unknown feature it is said
to be exactly sparse. More realistic is approximate sparsity, when
the bias from using a small model is well-controlled, but nonzero.
Sparsity is a natural framework for thinking about model selection.
Indeed, any time only a few of the available variables are used,
a sparsity assumption has effectively been made. It is common
empirical practice to report results from several small models, but
for these results to be valid one must assume these specifications
give high-quality, sparse representations of the unknown features.
The alternative we provide involves selecting a sparse, yet flexible,
model from among a large set of variables. Results may then be
compared with more traditional methods.
With the aim of mimicking common empirical practice we
estimate the propensity score with multinomial logistic regression,
coupled with group lasso selection (Yuan and Lin, 2006). Our

3 Discussion and applications may be found in, for example Imbens (2000), Lechner (2001), Imai and van Dyk (2004), Abadie (2005), Cattaneo (2010), and Cattaneo
and Farrell (2011).
4 They use different asymptotic variance estimators, and for treatment effects on
the treated they do not exploit the simplification discussed in Remark 1.
5 For other approaches and reviews of the literature, see, e.g., Holland (1986),
Hahn (1998), Horowitz and Manski (2000), Chen et al. (2004, 2008), Bang and Robins
(2005), Abadie and Imbens (2006), Wooldridge (2007), and references therein.

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

3

Table 1
Analysis of NSW demonstration: treatment effects on the treated and confidence intervals for various specifications.
Specifications:

Experimental Benchmark
Doubly-Robust Estimates
Specification 1 (No Selection)
DW02 (Informal Selection)
Refitting after Group Lasso Selection

Sample sizesc

Number of variables
Before selectiona

After selectionb

–

–

N/A
??
171

11
15
20/6

Control

ATT

95% CI

Treated

260

185

1794

[110, 3479]

1211
1058
1735

185
185
185

1664
2528
1737

[−276, 3604]
[149, 4908]
[33, 3441]

Notes: All analyses use the DW99 subsample and PSID comparison group. Specifications vary, but all estimates and standard errors of from the method defined in Section 5
with the exception of the partially linear model.
a
Not counting the intercept. The total set of variables considered by DW02 is not known.
b
For the group lasso estimators, the two numbers given are for those used in the outcome regressions and propensity score, respectively. For other doubly-robust
estimators all variables are used in the propensity score and outcome models.
c
The full sample begins with 2490 comparisons and 185 treated units. Control observations outside the range of estimated propensity scores in the treated sample are
discarded.

results are stated in the language of treatment effects, but apply
to general data structures and are of independent interest in the
high-dimensional literature.6 Much of the literature has focused
on linear models (see Buhlmann and van de Geer (2011) for a
survey), while prior studies of nonlinear models often assume
exact sparsity or present limited results.7 Furthermore, these
studies often use high-level conditions that can be hard to verify. In
contrast, we obtain sharp results for logistic regression under the
same simple and intuitive conditions used for linear modeling by
exploiting mathematical techniques of self-concordant functions
put forth by Bach (2010). We also provide extensions to prior
work on linear models needed to apply them in treatment effect
estimation.
Finally, we offer numerical evidence on the finite sample performance of our procedure. In a small simulation study we find
that our procedure delivers very accurate coverage of confidence
intervals even for models where covariate selection is difficult, either because of a low signal-to-noise ratio or lack of sparsity, thus
highlighting the uniform validity of inference. We also apply our
method to the widely-used National Supported Work Demonstration data (LaLonde, 1986) and find very accurate estimates and
tight confidence intervals (see Table 1).
The paper proceeds as follows. Section 2 gives short, selfcontained overview. Section 2.3 collects notation. Section 3
describes the treatment effect models. Sparse models are discussed
in Section 4, which shows how several commonly used models
fit in this framework. Section 5 presents our estimation method
and complete results on treatment effect inference. Theoretical
results for the group lasso are in Section 6. Section 7 presents
the numerical evidence and Section 8 concludes. The main proofs
are presented in Appendix, while the remainder are available in a
supplement (see Appendix D).

2.1. Treatment effects and results on post-selection inference
We consider a multivalued treatment, with status indicated by
D ∈ {0, 1, . . . , T }. Interest lies in mean effects of the treatment
on a scalar outcome Y . Let {Y (t )}Tt =0 be the (latent) potential
outcomes: Y (t ) is the outcome a unit would have under
TD = t and
{D =
is only observed for units with D = t; that is, Y =
t =0
t }Y (t ). Many interesting parameters combine means of potential
outcomes, and having multivalued treatments allows for a wider
range of estimands. Define the mean of one potential outcome as
µt = E[Y (t )]. To fix ideas, µ1 − µ0 is the average treatment effect
in the binary case (D ∈ {0, 1}). Sections 3 and 5 consider more
general average effects, including effects on treated groups. For
simplicity, in this section we focus on a single µt .
We use the selection on observables framework to identify µt .
For a vector of covariates X , define the generalized propensity score
and conditional outcome regressions as
pt (x) = P[D = t |X = x]

and µt (x) = E[Y |D = t , X = x].

For identification it is sufficient to assume that E[Y (t )|D, X ] =
E[Y (t )|X ] (mean independence) and pt (X ) is bounded away
from zero (overlap) for all treatment levels. Broadly, these two
assumptions imply that units from one treatment group are good
proxies for other treatments and that there are always such proxies
available (see Section 3).
For an i.i.d. sample {(yi , di , x′i )}ni=1 and model-selection-based
estimators p̂t (xi ) and µ̂t (xi ), we estimate µt with

µ̂t =

n
1

n i=1




{di = t }(yi − µ̂t (xi ))
+ µ̂t (xi ) .
p̂t (xi )

6 Our techniques build on prior studies, in particular Bickel et al. (2009), Lounici
et al. (2011), Obozinski et al. (2011), Belloni and Chernozhukov (2011), Belloni et al.
(2012), Belloni and Chernozhukov (2013), and Belloni et al. (2014).
7 Examples include van de Geer (2008) and Negahban et al. (2012), whose

This doubly-robust estimator combines regression imputation and
inverse probability weighting, and remains consistent if either
the model pt (x) or µt (x) is misspecified. Following widespread
empirical practice, we estimate p̂t (xi ) with multinomial logistic
regression and µ̂t (xi ) linearly (see Section 6). The choice of
covariates in p̂t (xi ) and µ̂t (xi ) impacts consistency, efficiency,
and finite sample performance. Covariate selection based on ad
hoc, iterative searches is common in empirical work, but is not
formal, objective, or replicable. Balancing tests are also common,
but have the additional drawback of assuming the same covariates
are important for outcomes and treatment assignment, and more
generally do not weight the covariates by their importance for bias.
On the other hand, our proposed procedure gives practitioners
an easy to implement, fully objective tool to perform data-driven
covariate selection and treatment effect inference, with replicable
results.8 Importantly, we do not preclude the addition of variables

bounds do not imply our results. Bach (2010) only gives an error bound on
coefficients in exactly sparse logistic regression, which cannot yield our results;
and does not consider prediction error or post-selection estimation. In independent
work, Kwemou (2012) and Belloni et al. (2013) also apply Bach’s (2010) tools, but
are focused on different goals. Vincent and Hansen (2014) apply the group lasso to
multinomial logistic regression, but do not derive any theoretical results.

8 For the final step, the doubly-robust estimator is available in STATA and
the package of Cattaneo et al. (2013). The covariate selection stage is easily
implemented in R.

2. Overview of results and notation
Here we give an overview of the paper, including treatment
effect inference (Section 2.1), our new results for the group lasso
(Section 2.2), and notation used throughout (Section 2.3).

4

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

known to be important from economic theory or prior knowledge.
Our procedure is intended to supplement these variables with
a flexible set of controls, guarding against misspecification or
overfitting.
The following theorem is an example of the more general results
presented in Section 5.2, wherein we also define Vt and V̂t .
Theorem 1. Consider a sequence {Pn } of data-generating processes
that obey, for each n, Assumptions 1 and 2. If the first stage obeys
i=1 (p̂t


(xi ) − pt (xi ))2 /n = oPn (1) and ni=1 (µ̂t (xi ) − µt (xi ))2
/n = oPn (1) and
n
1/2 n
(ii)
{di = t }(p̂t (xi ) − pt (xi ))2 /n
{d i = t }
i=1
i=1
1/2
= oPn (n−1/2 ),
(µ̂t (xi ) − µt (xi ))2 /n
√
then n(µ̂t − µt ) →d N (0, Vt ) and V̂t /Vt →Pn 1. For each n, let Pn
n

(i)

be the set of data-generating processes obeying Assumptions 1 and
2 and (i) and (ii) above. Then for cα = Φ −1 (1 − α/2)

 







sup PP µt ∈ µ̂t ± cα V̂t /n
− (1 − α) → 0.
P ∈P
n

This result establishes the uniform validity of an asymptotic
confidence interval for µt , overcoming all the post model
selection inference challenges: robustness to model selection
errors, selecting a model that is small but flexible enough to
capture the features of the underlying data generating process,
and still retaining efficiency under standard conditions (see
Section 5.3). Intuitively, this is similar to (but distinct from)
overcoming pretesting bias in other contexts. Also, although our
discussion is in terms of covariate selection in high-dimensional,
sparse models, the inference result is generic for any first stage
estimator.
The two conditions placed on the first stage are analogous to
the commonly-used, high-level requirement in semiparametrics
that first stage components converge faster than n−1/4 . However
exploiting features of the doubly-robust estimator yields weaker
conditions. The first is a mild consistency requirement. The second
requires a rate on the product of errors and is thus easier to satisfy if
one function is easier to estimate, e.g. more smooth or more sparse.
In model selection, the rates for the first stage depend on the
sample size, the number of covariates considered, and the sparsity
level. Importantly, the rate will depend on the total number of
covariates only logarithmically, allowing for a large number. We
propose to use the group lasso and prove that these estimators
satisfy (i) and (ii).
2.2. Model selection stage
We propose refitting following group lasso selection, and show
that it meets all requirements on the model selector. The group
lasso is well-suited to program evaluation applications because
covariates are penalized according to their overall contribution in
all treatment groups. This has two consequences. First, information
from all treatments is pooled when doing selection, and hence
a weaker signal may be extracted, which improves the selection
properties. Second, the selected variables are common to all
treatment levels. From a practical point of view this is desirable,
as interest rarely lies in a single µt , but rather a collection, and
substantial commonality is expected in the variables important for
different treatment levels.
We consider high-dimensional, sparse models for pt (x) and
µt (x). These are defined by a p-dimensional vector X ∗ based on
the original variables X . The X ∗ may consist of any combination of
the original variables, interactions, flexible parametric transformations, and/or nonparametric series terms (such as splines or polynomials). A model is approximately sparse if there are s < n of

these terms that yield a good approximation (s → ∞ is allowed).
To build intuition, suppose that µt (x) obeys a p-dimensional
linear model. Then the sparsity assumption is that there is an
s-dimensional submodel with sufficiently small specification bias.
In the nonparametric case, sparsity is weaker than (but analogous
to) the familiar assumption that a small set of basis functions can
approximate the unknown objects well. In practice researchers
employ a hybrid of these approaches, which is covered by our results. Section 4 gives more details and examples.
We form p̂t (x) and µ̂t (x) in two steps (complete details in Section 6). First, the group lasso is applied separately to multinomial
logistic and least squares regression to select covariates from X ∗ .
We then estimate pt (x) and µt (x) by refitting unpenalized models using the selected variables, possibly augmented with controls
suggested by prior work or economic theory. It is not desirable for
a model selector to discard theory and prior work, and our procedure explicitly avoids this. We also allow for using logistic-selected
variables in the linear model refitting and vice versa, but this is not
necessary for uniformity nor efficiency.
Our main results give precise bounds for the number of
covariates selected and the estimation error, both for the penalized
and unpenalized estimates. Section 6 results gives nonasymptotic
bounds, with exact constants. Such results are complex and so we
give the following intuitive, asymptotic result. (The notation OPn is
defined in Section 2.3.)
Corollary 1. Suppose the biases from the best sd - and sy -term ap
√
proximations to pt (x) and µt (x) are order sd /n and sy /n, respectively. Then under the assumptions in Section 6, and δ > 0 described
therein, with high probability we have:
2
−1
3/2+δ
1.
ni=1 (p̂t (xi ) − pt (xi )) /2 n = OPn n −s1d log(p ∨ n) 3/2+δ and
2.
(
µ̂
(
x
)
−
µ
(
x
))
/
n
=
O
n
s
log
(
p
∨
n
)
.
t i
t i
Pn
y
i=1
These two results for our proposed group lasso estimators can
be directly used to verify the high-level conditions in Theorem 1.
Specifically, if sd sy log(p)3+2δ = o(n), conditions (i) and (ii) of
Theorem 1 are met (requiring s2 = o(n), up to log factors, as
found in other results in the literature). Further, it is clear how
the doubly-robust estimator can help: if one function is more
smooth or more sparse, sd or sy will be lower, easing the restriction.
Section 6.3 gives further results: showing that the number of
variables selected is the same order as the sparsity level, and
provides bounds on the logistic and linear coefficients directly.
Both these results are important for certain steps in treatment
effect estimation that are not reflected in the simple statement
of Theorem 1. These results appear to be entirely new for the
multinomial logistic regression, for any version of the lasso. From
a practical point of view, these results provide formal justification
for using multinomial logistic regression, coupled with group lasso
selection and post-selection refitting.

n





2.3. Notation
We collect here notation to be used for the rest of the paper.
The data generating process (DGP) is denoted by Pn and is defined
by the joint law of the random variables (Y , D, X ′ )′ . For a given n,
{(yi , di , x′i )′ }ni=1 constitute draws from Pn . The DGP may vary with n,
along with features such as parameters, distributions, and so forth,
as discussed in Section 4.2. This is generally suppressed for clarity.
We adopt the following conventions.
Treatments. Define the treatment sets NT = {0, 1, 2, . . . , T }
and NT = {1, 2, . . . , T }. No order is assumed in the
treatments. For each unit i, di indicates treatment
assignn
t
ment, and define dti =
{di = t }. Let nt =
i =1 d i
be the number of individuals with treatment t and define n = mint ∈NT nt and n = maxt ∈NT nt . Further define

T = T + 1.

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

Vectors. Define Np = {1, 2, . . . , p}. For a doubly-indexed collection of scalars {δt ,j : t ∈ NT , j ∈ Np }, define δ·,j ∈ RT as
the vector that collects over all t for fixed j; δt ,· ∈ Rp col-

lects over j ∈ Np for fixed t; and δ·,· ∈ Rp×T the concatenation of all δt ,· . For simplicity, we write δt for δt ,· . When
considering the multinomial logistic model, t will vary
only over NT but the notation will be maintained. For a
set S ⊂ Np , let δt ,S ∈ Rcard(S ) be the vector of {δt ,j : j ∈ S }

for fixed t and similarly let δ·,S ∈ R|S |×T = {δt ,j : t ∈
N T , j ∈ S }.
Norms. Single bars will be either absolute value or cardinality of
a set, and will be clear from the context. For a vector v ,
let ∥v∥1 and ∥v∥2 denote the ℓ1 and ℓ2 norms, respectively.
group lasso, define the mixed ℓ2 /ℓ1 norm as
 For the
δ·,·  =
j∈Np ∥δ·,j ∥2 . It will always be the case that
2 ,1
the (‘‘outer’’) ℓ1 norm is over the covariates and the (‘‘inner’’) ℓ2 norm is over the treatments (in our application).
When discussing the multinomial logistic model, treatments will be restricted to NT with no change in notation.
Data-Generating Processes. The set of all Pn considered is Pn . For
sequences, {Pn } = {Pn : n ≥ 1, Pn ∈ Pn }. Expectations
and probabilities are taken against Pn , though notationally suppressed. For asymptotic arguments dependence
on n is explicit, so that OPn (·) and oPn (·) have their usual
meaning with the understanding that the measure Pn is
used for each n.

{mt }Tt =1 , let p̂t ({mt }NT ) = exp(mt )[1 +
 For a set of scalars
−1
denote the multinomial logit function. Emt ∈NT exp(mt )]
n
pirical expectation will be denoted En [wi ] =
i=1 wi /n and

n t
En,t [wi ] = i∈It wi /nt = i=1 di wi /nt .
3. Treatment effects model
In this section we formally define the treatment effects model
and the parameters of interest. Recall that D ∈ {0, 1, . . . , T } indicates treatment status, {Y (t )}t ∈NT are the (latent) potential outcomes,
and Y (t ) is only observed for units with D = t; that is,
Y =
t ∈NT Y (t ). The building blocks of many general estimands
are the averages

µt = E[Y (t )],

t ∈ NT ,

t , t ∈ NT × NT .
′

and

This assumption is a form of ‘‘ignorability’’ coined by Rosenbaum
and Rubin (1983). This model allows arbitrary treatment effect heterogeneity in observables, but not unobservables. This assumption
is standard in the program evaluation literature, and its plausibility has been discussed at length, so we omit a general discussion
(see, e.g., Imbens (2004), Wooldridge (2010, Chapter 21), and references therein). However, in the context of model selection, three
remarks are warranted.
First, in place of Assumption 1(a), it is more common to
instead assume full conditional independence: Y
D|X . However,
as observed by Heckman et al. (1997), the weaker mean
independence is sufficient. For our purposes, the ‘‘gap’’ between
the two assumptions is important. Suppose full independence
holds only conditional on a set of variables strictly larger than the
variables entering the mean functions (e.g. the excess variables
affect higher moments). In this case, because mean independence
is still sufficient, we need not aim to select the larger set.
Full independence is important for the efficiency discussed in
Section 5.3.
Second, the covariates may, in general, include instruments
for treatment status, but they are not known as such. This
is standard, but left implicit, in discussions of ignorability. If
instruments are present, and selected for estimation, efficiency
suffers but unbiasedness is not harmed. Efficiency bounds in this
context typically (implicitly) assume there are no instruments in
X . Assumption 1(b) rules out perfect predictors. Section 5.3 offers
further discussion.
Finally, the main drawback of Assumption 1(a) is that it does not
give identification of average effects on transformations of Y (t ).
However, we are expressly interested in model selection on the
mean function of the level of Y (t ), and hence Assumption 1(a)
is more natural. To operationalize model selection, structure
must be placed on E[Y (t )|X = x], and hence functional form
conditions tied to mean independence are not limiting per se. If the
parameter of interest is changed, say to E[log(Y (t ))], and a sparsity
assumption is made for E[log(Y (t ))|X = x], then our method
applies.
Assumption 1 yields identification of µt and µt ,t ′ using either
inverse weighting or regression, and double robustness follows
from combining the two strategies. Recall the notation pt (x) =
P[D = t |X = x] and µt (x) = E[Y |D = t , X = x]. Applying
Assumption 1 we find that

E ψt Y , D, µt (X ), pt (X ), µt



µt ,t ′ = E[Y (t )|D = t ],
′




(1)

In the binary case, the average treatment effect is µ1 − µ0 and
the treatment on the treated is µ1,1 − µ0,1 . A multivalued treatment allows for a large range of interesting estimands. To fix
ideas, we keep as running examples two leading cases. First,
the so-called dose–response function: the (T + 1)-vector µ =
(µ0 , µ1 , . . . , µT )′ . Second, define τ as the T -vector with element
t given by µt ,t −µ0,t . This gives the effect of each treatment relative
to the baseline t = 0, only for those who received that treatment.
These are by no means the only interesting estimands constructed
from µt and µt ,t ′ ; many others are given by Lechner (2001), Heckman and Vytlacil (2007), and others.
The following two conditions are sufficient to identify µt and
µt ,t ′ .
Assumption 1 (Identification). For all t ∈ NT and almost surely X ,
Pn obeys:
(a) (Mean independence) E[Y (t )|D, X = x] = E[Y (t )|X = x], and
(b) (Overlap) P[D = t |X = x] ≥ pmin > 0 for all t ∈ NT .

5

=E



{D = t }Y
+ µt ( X ) −
pt ( X )


{D = t }µt (X )
− µt = 0
pt (X )

(2)

and



E ψt ,t ′ Y , D, µt (X ), pt (X ), pt ′ (X ), µt ,t ′


{D = t ′ }µt (X )
pt ′ (X ) {D = t }(Y − µt (X ))
=E
+
− µt ,t ′
pt ′
pt ′
pt (X )

= 0,

(3)

where pt = P[D = t ]. The moment condition (2) holds if either
pt (x) or µt (x) is misspecified. For µt ,t ′ , if µt (x) is misspecified,
both pt (X ) and pt ′ (X ) must be correctly specified, while if µt (x) is
correct, both propensity scores may be misspecified. It is important
to note that the forms of ψt (·) and ψt ,t ′ (·) are fixed, so the function
itself does not depend on the sample size even if its arguments do.
Our estimator is a plug-in version of this moment condition.
Remark 1 (Simplifications for µt ,t ). Identification of µt ,t does not
require Assumption 1. Y (t ) is fully observed for the sub-population
of interest and so a simple average will deliver µt ,t = E[ {D =
t }Y ]/pt . Note that (3) reduces to this when t = t ′ . For τ this means

6

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

we must only estimate the function µt (xi ) for t = 0. Intuitively,
we must use comparison group observations to proxy for treated
units, but not the other way around. Thus, for certain parameters
of interest, Assumption 1 can be weakened to hold only for the
comparison group. However, we cover generic estimands, without
necessarily specifying a comparison group, and so we maintain
Assumption 1 for simplicity, rather than keeping track of hosts of
special cases. 
Remark 2 (Efficient Influence Functions). The efficient influence
functions in this model are exactly ψt (·) and ψt ,t ′ (·), and so our
estimators have the interpretation of being plug-in versions of
these, and indeed, will be asymptotically linear with this influence
function (see Section 5.3). 
4. Approximately sparse models
We now formalize approximate sparsity. Let XY∗ and XD∗ be
p-dimensional transformations of the covariates X , with p > n
allowed. These transformations are specific to the outcome and
treatment models, but may overlap. They do not vary with t, nor
depend on the DGP. Some examples are given below in Section 4.1.
For the multinomial logistic model it is 
convenient to work with
the log-odds ratio. We take p0 (x) = 1 − t ∈NT pt (x) and write


log

pt (x)
p0 (x)



= x∗D ′ γt∗ + BDt ,

t ∈ NT .

(4)

Similarly, write the outcome regressions as

µt (x) = x∗Y ′ βt∗ + BYt ,

t ∈ NT .

(5)

The terms BDt = BDt (x) and BYt = BYt (x) are bias terms arising from
the parametric specification. As discussed below, these encompass
the usual nonparametric bias as well. Approximate sparsity
requires that only a small
number of the X ∗ are needed
 to make the
∗
∗
Y
bias small. Define S∗D =
NT supp(βt ),
NT supp(γt ) and S∗ =
so that these sets capture all variables important for treatment and
outcomes, respectively. We assume that there are some sd < n and
sy < n, such that for |S∗D | = sd and |S∗Y | = sy , the biases BDt and BYt
are sufficiently small. This is made precise by defining the bounds:

En [(p̂t ({x∗i γt∗ }NT ) − pt (xi ))2 ]1/2 ≤ bds
′

and

En [BYt (xi )2 ]1/2 ∨ En,t [BYt (xi )2 ]1/2 ≤ bys .

(6)

Note that the former bias bound is placed directly on the propensity score because it is the ultimate object of interest, rather than
on the linearization of the log-odds.
While a great deal of overlap is expected, in practice it is likely
that a few covariates will be more or less important for different
treatments, and so we do not require that the supports of γt∗ , t ∈
NT or βt∗ , t ∈ NT are constant over t, nor that S∗D overlaps with
S∗Y . Instead, it may be better to think of Np \ S∗D and Np \ S∗Y as the
‘‘common nonsupports’’ of the treatment and outcome equations.
When it is clear from the context we will abbreviate both XD∗ and
XY∗ by X ∗ (and their realizations by x∗i ) and refer to them generically
as ‘‘covariates’’, and further write s for either sd or sy . We assume
En [(x∗i,j )2 ] = 1 without loss of generality (see Remark 4).
4.1. Parametric and nonparametric examples
To concretize the sparse model idea, we now discuss how
several models commonly used in practice fit into this framework.
These include parametric and nonparametric models for pt (x) and
µt (x), and hybrids of these. A common theme to all examples
will be comparison to the oracle model: the model that knows

the true support in advance. Our uniform inference results include
all these examples as special cases because, loosely speaking, we
obtain uniformity over DGPs where pt (x) and µt (x) have sparse
representations. We aim for an accessible discussion of each model,
and defer technicalities to the literature (Raskutti et al., 2010;
Rudelson and Zhou, 2013; Belloni et al., 2014).
Example 1 (Oracle Parametric Model). Assume models (4) and (5)
hold with BDt = BYt = 0 and XD∗ = XY∗ = X . Let p = s = dim(X ).
All covariates are used in all modeling. If dimension is fixed this
is the textbook parametric model, see for example Wooldridge
(2010). Alternatively, the dimension can be diverging, but more
slowly than n. We are not aware of any work which covers this
case explicitly, though for the first stage, He and Shao (2000) cover
linear and logistic regression, and their results easily extend to
multinomial logistic models.
The vast majority of treatment effect studies adopt this model
(with dimension fixed), taking the set of covariates as given. In
our framework, this is equivalent to the researcher having prior
knowledge of which covariates are important and which are not.
Such knowledge no doubt plays an important role, but it cannot
cover all situations or all variables. Furthermore, as more data
become available, the researcher does not increase the complexity
of their model. 
Example 2 (Exactly Sparse Parametric Model). Retain the exact
parametric structure of the prior example, but let dim(X ) = p be
possibly larger than n, and assume that S∗Y and S∗D are unknown
sets of cardinality less than n. Model selection must be performed.
Often, researchers (implicitly) rely on the oracle property, that S∗Y
and S∗D can be found with probability approaching one, and conduct
inference conditioning on this event. This approach cannot be
made uniformly valid and has poor finite sample properties, as
shown by Leeb and Pötscher (2005, 2008a,b) and Pötscher and Leeb
(2009). 
Example 3 (Approximately Sparse Parametric Model). Again suppose a purely parametric model, so that XD∗ = XY∗ = X and
dim(X ) = p, possibly greater than n. Suppose that there exist
0
0
coefficients γ·,·
and β·,·
such that log[pt (x)/p0 (x)] = x∗D ′ γt0 and

µt (x) = x′ βt0 exactly, but instead of any coefficients being precisely zero, suppose they may be ordered such that |γt0,j | ∝ j−αγ

and |βt0,j | ∝ j−αβ , with αγ and αγ at least one. Then, there exist
sd and sy that are o(n) such that Eqs. (4) and (5), and other conditions needed, are satisfied for γt∗,j = γt0,j for j ≤ sd and βt∗,j = βt0,j
for j ≤ sy and the rest truncated to zero. That is S∗D and S∗Y collect

0
the largest coefficients and BDt =
Np \S D xj γt ,j , and similarly for
∗

BYt .



Example 4 (Semiparametric Model). Assume pt (x) and µt (x) are
unknown functions that can be well-approximated by a linear
combination of sd and sy basis functions, respectively (e.g. are suf∗
∗
ficiently smooth). In (4) and (5), γ·,·
and β·,·
are the coefficients of
these approximations, while BDt and BYt are the usual nonparametric biases. XD∗ = RD (X ) and XY∗ = RY (X ) are series terms used in
the approximation. Standard semiparametric analyses, such as Hirano et al. (2003), Imbens et al. (2007), or Cattaneo (2010), can
be viewed in this context as oracle models that know in advance
which terms yield the best approximation, typically assumed to be
the first terms. Instead, we only require that some sd (or sy ) of a set
of p series terms give good approximations. This allows for greater
flexibility in applications, where there is no knowledge of which
series terms to use, and the researcher may want to mix terms from
different bases. 

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

Example 5 (Mixed Parametric and Semiparametric Model). Partition
X = (X1 , X2 ). Suppose that the true log-odds function satisfies
log[pt (x)/p0 (x)] = x′1 γt1 + ht (x2 ) + B1t (x), where B1t (x) is a
specification bias and ht (·) is a smooth unknown function. For a
set of basis functions RD (x2 ), there will exist coefficients γt2 such
that ht (x2 ) = RD (x2 )′ γt2 + B2t (x2 ) and so


log

pt (x)



p0 (x)
1′
t

= x∗D ′ γt∗ + BDt ,
2′ ′
t

γt = (γ , γ ) , and
∗

BDt

=

x∗D = (x′1 , RD (x2 )′ )′ ,
B1t

+

B2t

.

We require that some collection of variables and series terms give
a good, sparse approximation, without placing explicit conditions
on how many of either. Implicitly, one will restrict the other. For
example, if the dimension of the parametric part is large, then we
require that ht (·) can be more easily approximated. We treat µt (x)
the same. This example is closest to actual practice, where some
variables (e.g. dummies) enter in a known way and should not be
considered part of a nonparametric object, while other covariates
must be considered flexibly. 
It is important to note that misspecification of the type
guarded against by double robustness can arise in any type of
model. In parametric cases, this is most often functional form
misspecification. While this type of misspecification does not occur
in nonparametrics, others are possible, such as shape restrictions
or separability assumptions being incorrect, or omitting relevant
variables. None of these errors disappear asymptotically, and all of
them are guarded against by use of the doubly-robust estimator.
4.2. Conceptual considerations in n-varying DGPs
Much of the DGP, including parameters and distributions, is
allowed to depend on n. Perhaps the most salient features that do
not depend on n are the set of treatments and the functions ψt and
ψt ,t ′ . It is likely that our results can be extended to accommodate a
growing number of treatments, but that is beyond the scope of our
∗
∗
study. In the models (4) and (5), X ∗ , γ·,·
, and β·,·
must depend on
n by construction. Our results on estimation of these models are
nonasymptotic. For treatment effect inference, we use triangular
array asymptotics to retain the dependence on n of the DGP. The
interpretation of the results does, and should, change depending
on what is assumed about the DGP. To illustrate, let us return to
Examples 2 and 4.
First, consider the simple parametric models of Example 2. In
this case, µt = E[E[Y (t )|X ]] = E[X ′ ]βt∗ , which depends on n by
construction, as the dimension is diverging. It may seem unnatural
that the parameter to be estimated depends on n, as we typically
think of ‘‘true’’ parameters being features of a (large) fixed study
population. However, with a diverging number of covariates, there
(n )
is no fixed DGP. Indeed, if we estimate µt = µt 1 based upon
n1 observations, and then proceed to gather n2 more observations,
(n +n )
(n )
when we re-estimate our target is now µt 1 2 ̸= µt 1 . One
possible resolution is as follows. First, the parameter of interest is
µ(∞)
= E[Y (t )], which is defined without reference to covariates.
t
We can view each successive n-dependent µt as an approximation
(∞)
of µt based upon p = pn covariates. Note well that in our thought
experiment, pn1 ̸= pn1 +n2 , and so additional variables should have
been collected for all n1 + n2 samples.
Contrast this with the semiparametric model in Example 4. It
is common to assume the population DGP is fixed over n. The
treatment effects may be constructed in terms of the underlying
(∞)
variables, e.g. µt
= E[Y (t )] = E[E[Y (t )|X ]], with X ∗ serving
only the purpose of aiding in approximating the regression functions. Model selection is performed on series terms, not under∗
∗
lying variables, to estimate the coefficients γ·,·
and β·,·
. If µt =

7

E[XY∗ ′ ]βt∗ + E[BYt ] does not depend on n, the bias term, by definition,
exactly compensates for the n-dependence in E[XY∗ ′ ]βt∗ . We emphasize that our inference results allow for general n-dependence
in the DGP, and interpretation by the econometrician must take
careful account of any conceptual assumptions.
5. Main results on treatment effect estimation and inference
In this section we present results on uniformly valid treatment
effect inference. We first present the estimators and conditions
required for a generic first stage to yield uniform inference.
Although our focus is on model selection and sparsity, our results
are more general, showcasing the benefits of doubly robust
estimation for any model in Section 4 where Assumption 3 (which
does not refer to selection or sparsity) can be satisfied.
5.1. Estimation procedure with a generic model selector
The moment functions ψt (·) and ψt ,t ′ (·) of Eqs. (2) and (3) have
fixed and known form, and so for estimators p̂t (x) and µ̂t (x), we
can define

µ̂t =

n
1



dti (yi − µ̂t (xi ))
p̂t (xi )

n i=1


+ µ̂t (xi )

(7)

and

µ̂t ,t ′ =

n
1

n i =1



′

dti µ̂t (xi )
p̂t ′

+

p̂t ′ (xi ) dti (yi − µ̂t (xi ))
p̂t ′

p̂t (xi )


,

(8)

where p̂t = nt /n. By combining these estimators appropriately we
can construct estimators µ̂ and τ̂ for the dose–response function µ
and the vector τ , respectively, and any other estimand. Notice that
when t = t ′ µ̂t ,t is an average over the appropriate subpopulation:
µ̂t ,t = En,t [yi ].
Although in this section we allow for generic estimates p̂t (x)
and µ̂t (x), it is important to distinguish between estimates based
upon selected sets that have no ‘‘additional randomness’’ and
those that do. Model selection based estimation will naturally
have two steps: first data-driven selection and then refitting to
ameliorate the shrinkage bias and allow the researcher to augment
the selected variables. Let S̃ D and S̃ Y be the selected sets and Ŝ D and
Ŝ Y be the final sets of variables used in the refitting. We will say that
these contain no ‘‘additional randomness’’ if the added variables
(i.e. Ŝ \ S̃, for Y or D) are nonrandomly selected, such as from
economic theory or prior knowledge. On the other hand, the added
variables may be selected from a random process beyond that
included in S̃. The leading example would be using logistic-selected
variables in the regressions or vice versa. Then the variables used
in µ̂t (xi ) depend not only on the randomness of S̃ Y , but also on that
of S̃ D , and hence on {di }ni=1 . Additional conditions are required for
the estimators with additional randomness.
The choice of method is in part dependent on the assumptions
of the underlying model. To illustrate, first, return to Example 2,
where we have a purely parametric model with X = XD∗ =
XY∗ . The researcher may want to set Ŝ D ⊃ S̃ D ∪ S̃ Y , in order to
have a better chance that S∗Y ⊂ Ŝ D . The set Ŝ D now contains
additional randomness due to S̃ Y . Conversely, consider Example 4.
It is natural to include ‘‘low-order’’ basis functions for each
underlying covariate, say linear and quadratic polynomials. Thus,
the researcher may want to include these in Ŝ, whether or not
selected by the group lasso. However, there is no reason that the
series terms useful for approximating the functions µt (x) would
be useful for pt (x), or vice versa, and no additional randomness is
injected.

8

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

We now state the sufficient conditions used for treatment
effect estimation and inference. For exposition, we present these in
three groups: those concerning the underlying DGP, requirements
of p̂t (x) and µ̂t (x) in the ‘‘no additional randomness’’ case, and
finally the additional conditions to allow for ‘‘additionally random’’
selected sets. Begin with conditions on the DGP. Let U ≡ Y (t ) −
µt (X ) and impose the following conditions.
Assumption 2 (Data Generating Process). Pn obeys the following,
with bounds uniform in n.
(a) {(yi , di , x′i )′ }ni=1 is an i.i.d. sample from (Y , D, X ′ )′ .
(b) The covariates X ∗ have bounded support, with maxj∈Np |Xj∗ | ≤
X < ∞. Transformations may depend on n but not the
underlying data generating process.
(c) E[|U |4 | X ] ≤ U4 .
(d) minj∈Np , t ∈NT E[Xj∗ 2 U 2 ] ∧ E[Xj∗ 2 ( {D = t } − pt (X ))2 ] is bounded away from zero.
(e) For some r > 0: E[|µt (xi )µt ′ (xi )|1+r ] and E[|ui |4+r ] are
bounded.
These conditions are mild and intuitive, and not unique to highdimensional models or model selection. Assumption 2(a) restricts
attention to cross-sectional applications. The condition of bounded
covariates is unlikely to be a limitation in practice. Any X ∗ that
are underlying variables will naturally be bounded in applications.
This condition is automatically satisfied for most common choices
of basis functions employed in nonparametric estimation. The rest
are moment conditions on the potential outcome models, including allowing the errors to be heteroskedastic and non-Gaussian.
The uniform bounds in n are needed for array asymptotics.
We now give precise conditions on the model selector sufficient
for uniformly valid inference.
Assumption 3 (First Stage Restrictions). The estimators p̂t (x) and
µ̂t (x) obey the following for a sequence {Pn }, uniformly in t ∈ NT .


(a) En [(p̂t (xi ) − pt (xi ))2 ] = oPn (1) and En (µ̂t (xi ) − µt (xi ))2 =
oPn (1).
(b) En [(µ̂t (xi ) − µt (xi ))2 ]1/2 En [(p̂t (xi ) − pt (xi ))2 ]1/2 = oPn (n−1/2 ).
These two collectively play the same role as the commonly-used,
high-level requirement in semiparametrics that each first-step
component separately converges at n−1/4 at least.9 Indeed, Belloni
et al. (2014) employ just such a condition for each component.
However, by making use of the doubly-robust property we have
the weaker conditions shown.10 The first is a mild consistency
requirement. The second requires an explicit rate on the product
of errors, and hence if one function is relatively easy to estimate
Assumption 3(b) can be satisfied even if the other does not
converge at n−1/4 . This formalizes the benefit of doubly-robust
estimation in general. In high-dimensional, sparse modeling
specifically the rates for the first stage depend on the sample size,
the number of covariates considered, and the sparsity level. Thus,
if one function requires fewer covariates to estimate, i.e. smaller
p or s, then greater complexity can be allowed for in the other
(capturing, in particular, their relative smoothness).
The so-called ‘‘additional-randomness’’ estimators are more
specific to the (approximately) sparse model context, and so we
now codify the sparsity requirements of Section 4 and then give
the additional conditions required for these estimators.

9 See Newey and McFadden (1994) and Chen (2007), and references therein.
10 Many studies in the semiparametric literature relax or do not rely on the n1/4
condition, allowing the nonparametric portion to converge at a slower rate, at any
rate, or in some cases be inconsistent; examples include Powell et al. (1989), Newey
(1990), Robins et al. (2008), Cattaneo et al. (2014a, 2013, 2014b), among others.

Assumption 4 (Sparsity). For each n, Pn obeys (4)–(6), with |S∗Y | =
sd and |S∗D | = sy .
Assumption 5 (Regularity Conditions for Union Estimators). For a
sequence {Pn }, log(p) = o(n1/3 ) and the estimators pt (x) and µ̂t (x)
obey the following, uniformly t ∈ NT :



max |ui | En [(p̂t (xi ) − pt (xi ))2 ] = oPn (n−1/2 ) and





i∈It



γ̂t − γ ∗  ∨ ∥β̂t − β ∗ ∥1 = oP (log(p ∨ n)−1/2 ).
n
t 1
t
These conditions are needed to apply bounds for self-normalized
sums (de la Peña et al., 2009). Belloni et al. (2012) were the first
to use these techniques in high-dimensional, sparse models. The
first condition is high-level, but can be verified with conditions on
the errors and a bound for estimation. For the former, Belloni et al.
(2012) assume that maxi∈Nn |ui | = OPn (n1/q ) for some q > 2. A
larger q eases the restriction in Assumption 5 but at the expense
of stronger conditions on the noise distribution. For example, if ui
are assumed Gaussian, q can be taken to be any (large) positive
number.
Remark 3 (Linear Probability Models). Our results cover use of a linear probability model for pt (x), instead of the multinomial logistic
form. All we require is a sufficiently high-quality approximation of
the unknown function, and hence if Assumptions 3 and 5 if appropriate,11 are met then uniform inference is possible using a linear
probability model. Our group lasso results (Theorems 7 and 8) can
be used directly to verify these conditions. In the same vein, multinomial logistic regression can be used to estimate µt (x) if the outcome Y is discretely valued. 
5.2. Theoretical results
We now come to our main results on inference on average
treatment effects. Most of our discussion will concern µt and µ;
similar points apply to results for µt ,t ′ and τ . Our first result
formalizes consistency of our estimates under misspecification.
Theorem 2 (Double Robustness). Consider a sequence {Pn } of datagenerating processes. Suppose that for some p0t (x) and µ0t (x),
En [(p̂t (xi )− p0t (xi ))2 ] = oPn (1) and En [(µ̂t (xi )−µ0t (xi ))2 ] = oPn (1).
Let Assumptions 1 and 2 hold for each n, with the regularity conditions
also holding
for p0t (x) and µ0t (x). If p0t (x) = pt (x) or µ0t (x) = µt (x),


then µ̂t − µt  = oPn (1).
This theorem formalizes the double-robustness property of
our estimators: the propensity score or regression may be
misspecified if the limiting objects are well-behaved. Compare to
Assumption 3(a). The nearly identical result for µt ,t ′ is omitted to
save space.
We now turn to our main inference results. First we demonstrate a Bahadur representation of a generic µ̂t or µ̂t ,t ′ . These are
shown to be equivalent to a sample average of the moment functions ψt (·) and ψt ,t ′ (·), respectively, after proper centering and
scaling, evaluated at the true pt (xi ) and µt (xi ). Using these results,
asymptotic normality can be obtained for general estimands. We
state explicit results for the leading examples µ and τ .
An asymptotic variance formula is needed to state the results.
Define the conditional variance of the potential outcomes σt2 (x) =

11 Assumption 5 can be slightly weakened in this case due to the linear link
function.

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

E[U 2 |D = t , X = x] and the T -square matrix Vµ with elements


σt2 (X )
Vµ [ t , t ′ ] = { t = t ′ } E
pt (X )
+ E [(µt (X ) − µt )(µt ′ (X ) − µt ′ )]
≡ VµW (t ) + VµB (t , t ′ ).


V̂µ (t ) = En
W



dti (yi − µ̂t (xi ))2


and

p̂t (xi )2

V̂µB (t , t ′ ) = En (µ̂t (xi ) − µ̂t )(µ̂t ′ (xi ) − µ̂t ′ ) .





Our first result gives the asymptotic behavior of µ̂t and µ̂ for a
sequence of DGPs.
Theorem 3 (Estimation of Average Treatment Effects). Consider a
sequence {Pn } of data-generating processes that obey Assumptions 1–
3 for each n. If µ̂t (xi ) and p̂t (xi ) do not have additional randomness
in the estimated supports, we have:
n
√
√
t
1. n(µ̂t −µt ) =
i=1 ψt (yi , di , µt (xi ), pt (xi ), µt )/ n + oPn (1);
√
−1/2
2. Vµ
n(µ̂ − µ) →d N (0, IT ); and
3. V̂µW (t ) − VµW (t ) = oPn (1) and V̂µB (t , t ′ ) − VµB (t , t ′ ) = oPn (1).
If, in addition, Assumptions 4 and 5 hold, then the same is true when
the supports contain additional randomness.
Theorem 3 itself may appear standard, but what is nonstandard
is that the model selection step of the estimation has been
explicitly accounted for. This immediately gives the following
uniform inference results.
Corollary 2 (Uniformly Valid Inference). Let Pn be the set of datagenerating processes satisfying the conditions of Theorem 3 for a given
n and G : RT → R be a fixed, twice uniformly continuously differentiable function with gradient ∇G such that lim infn→∞ ∥∇G (µ)∥2 is
bounded away from zero. Then for cα = Φ −1 (1 − α/2), we have:






sup PP G(µ) ∈



P ∈Pn

G(µ̂) ± cα



∇G (µ̂)′ V̂µ ∇G (µ̂)/n





− (1 − α)

→ 0.
Corollary 2 shows that these procedures are uniformly valid
over the class of DGPs we consider, and hence will be reliable in
applications. The crucial insight that leads to uniform inference
is to change the goal of model selection away from perfect
covariate selection (the oracle property) and to high-quality
approximation of the underlying functions (here pt (·) and µt (·)).
This fundamental shift in focus allows us to avoid the uniformity
problems demonstrated by Leeb and Pötscher. Assumption 3
formalizes exactly the quality of approximation needed. Such an
approximation can be found for any element in Pn , and hence
inference is uniformly valid over that class. This method of proving
uniformity follows Belloni et al. (2014) and Romano (2004), and is
distinct from the approach of Andrews and Guggenberger (2009).
Results for the treatment effects on the treated are similar. The
variance formula for τ is slightly more cumbersome. Define the
T -square matrix Vτ with elements
Vτ [ t , t ′ ] =

− µt ,t



pt (X ) 

σt2 (X ) + (µt (X ) − µ0 (X )



2 
pt (X )pt ′ (X ) 2
+E
σ0 (X )
+ µ0,t
pt pt ′ p0 (X )

{t = t ′ }E

Straightforward plug-in estimators for these two components are
given by
V̂τW (t ) = En

Straightforward plug-in estimators for these two components are
given by12

p2t

≡ VτW (t ) + VτB (t , t ′ ).
12 Estimators can also be based on sample averages of outer products of influence
functions, which would include the covariance term that vanishes in expectation.

9



V̂τB (t , t ′ ) = En


2 
dti 
y
−
µ̂
(
x
)
−
µ̂
+
µ̂
i
0
i
t
,
t
0
,
t
p̂2t


p̂t (xi )p̂t ′ (xi )
p̂t p̂t ′ p̂0 (xi )2

and



d0i (yi − µ̂0 (xi ))2 .

Note that we need not estimate µt (x) and σt2 (x), due to the
simplification in Remark 1. With this notation, we have the
following results. Proofs are so similar to those for Theorem 3 and
Corollary 2 that we omit them.
Theorem 4 (Estimation of Treatment Effects on Treated Groups). Consider a sequence {Pn } of data-generating processes that obey Assumptions 1–3 for each n. Then under Pn , as n → ∞, if µ̂t (xi ) and p̂t (xi )
do not have additional randomness in the estimated supports:
1.

√

n(µ̂t ,t ′ − µt ,t ′ ) =
√
µt ,t ′ )/ n + oPn (1);

n

i =1

ψt ,t ′ (yi , dti , µt (xi ), pt (xi ), pt ′ (xi ),

−1/2 √

2. Vτ
n(τ̂ − τ) →d N (0, IT ); and
3. V̂τW (t ) − VτW (t ) = oPn (1) and V̂τB (t , t ′ ) − VτB (t , t ′ ) = oPn (1).
If, in addition, Assumptions 4 and 5 hold, then the same is true when
the supports contain additional randomness.
Corollary 3 (Uniformly Valid Inference). Let Pn be the set of datagenerating processes satisfying the conditions of Theorem 4 for a given
n and G : RT → R be a fixed, twice uniformly continuously differentiable function with gradient ∇G such that lim infn→∞ ∥∇G (τ)∥2 is
bounded away from zero. Then for cα = Φ −1 (1 − α/2), we have:






sup PP G(τ) ∈

P ∈Pn



G(τ̂) ± cα



∇G (τ̂)′ V̂τ ∇G (τ̂)/n





− (1 − α)

→ 0.
5.3. Efficiency considerations
The prior theoretical results are aimed at delivering robust
inference. In this section, we briefly discuss the efficiency of our
estimator according to two criteria: semiparametric efficiency and
oracle efficiency. To put each on sound conceptual footing we
separate discussion and restrict to an appropriate set of models.
For semiparametric efficiency, pt (x) and µt (x) are nonparametric objects, as in Example 4, X are fixed-dimension variables
and the DGP does not vary with n. If we ‘‘upgrade’’ the mean independence of Assumption 1(a) to full, namely {Y (t )}NT
D|X ,
then Theorems 3 and 4 immediately yield asymptotic linearity and
semiparametric efficiency, attaining Hahn’s (1998) or Cattaneo’s
(2010) bounds. This requires there be no (known) instruments for
treatment status in X , as implicitly assumed in those works, else
the bound may change (Hahn, 2004).
Turning to oracle efficiency, an alternative to our robust
approach is to prove that the true support can be found with
probability approaching one (the oracle property), then conduct
inference conditioning on this event. This approach cannot be
made uniformly valid, but may be of interest in the exactly sparse
models of Example 2 (there is no ‘‘true’’ support in approximately
sparse models), because discovering the true support is equivalent
to finding the variables in the causal mechanism (White and
Lu, 2011), if one exists. This may be interesting in its own right,
or for future applications by way of hypothesis generation. The
post oracle selection estimator is made efficient by using only the
variables important for µt (xi ) = E[Y |D = t , xi ]. This amounts to
entirely removing the instrumental variables indexed by S∗D \ S∗Y ,
whose inclusion would, in general, reduce efficiency, though not

10

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

increase bias. Further, S∗Y \ S∗D are excluded from propensity score
estimation.
Perfect selection requires two strong conditions: (i) an orthogonality condition on the Gram matrices that restricts the correlation between the variables in and out of the true support (Bach,
2008), and (ii) a beta-min condition bounding the nonzero coefficients away from zero. Intuitively, highly correlated variables cannot be distinguished, nor can coefficients sufficiently close to zero
be found with certainty. Both bounds may depend on n, and in particular the lower bound on the coefficients may vanish at an appropriate rate. Under such conditions, it is straightforward to show
that S∗Y and S∗D can be found with probability approaching one.
6. Group lasso selection and estimation
We now give details for group lasso model selection and estimation, and make the refitting precise. Section 6.1 discusses penalty
choices and implementation. Restricted and sparse eigenvalues,
key quantities in our bounds, are discussed in Section 6.2. Our main
nonasymptotic results are stated in Section 6.3. These results are
of interest more generally in the literature on high-dimensional
sparse models. Finally, Section 6.4 gives asymptotic rates and verifies the conditions of Section 5.
We first select covariates by applying the group lasso penalty
to the multinomial logistic loss (for the propensity scores) and to
least squares loss (to estimate the outcome regression). The loss
functions are defined as

M(γ·,· ) =






′
En −dti log p̂t ({x∗i γt }NT )

and

Remark 4 (Weighted Penalties). The group lasso penalty can be
weighted
 in two ways. First, one may weight the ℓ2 portion, as
in λD j∈Np ∥Xj γ·,j ∥2 , where Xj is the design matrix for covariate
j, across all the treatments. Other weight matrices are possible,
but with this choice, the estimate is invariant to within group
(treatment) reparameterizations, and is thus scale invariant for
each covariate. We therefore assume En [(x∗i,j )2 ] = 1 without loss
of generality.
Second, the
 ℓ1 norm can be weighted to give a penalty of
the form λD j∈Np wj ∥γ·,j ∥2 . Two common choices for wj are the
number of variables in group j or an adaptive penalty from a
pilot estimate. Our groups are equally sized, and although adaptive
procedures may improve oracle properties (Zou, 2006; Wei and
Huang, 2010), our goal is not perfect selection. 
6.1. Choice of penalty
We must specify choices of λD and λY for programs (9). From a
theoretical point of view, these must be chosen so that the penalty
dominates the noise, which is captured by the magnitude of the
score in the dual of the |||·|||2,1 norm, with high probability. To
achieve this, we set

√

λY =

4XU T

√

n

√



En,t [(yi − x∗i βt )2 ].
′

t ∈NT

Then, the group lasso estimates for the propensity score coefficients, denoted γ̃·,· , and the regression coefficients, β̃·,· , respectively solve


  
γ̃·,· = arg min M(γ·,· ) + λD γ·,· 2,1
γ·,· ∈RpT


  
β̃·,· = arg min E (β·,· ) + λY β·,· 2,1 ,

and
(9)


1+

1/2
and

√

T
log(p ∨ n)3/2+δY

√

T

1/2

(11)

,

4 log(2p)(1 + 64 log(12p)2 )



where
  λD and λY are the penalty parameters discussed below and
γ·,·  is the mixed ℓ2 /ℓ1 norm.
2 ,1

To ameliorate the downward bias induced by the penalty
and to allow for researcher-added variables, we refit unpenalized
models.13 Let S̃ D = {j : ∥γ̃·,j ∥2 > 0} and S̃ Y = {j : ∥β̃·,j ∥2 > 0} be
the selected covariates and Ŝ D and Ŝ Y those used in refitting.14 We
require Ŝ ⊃ S̃ and |Ŝ | ≤ s for D and Y (we will prove that |S̃ | ≤ s
in both cases). The refitting estimators solve
arg min

M(γ·,· )





γ·,· , supp(γt )=Ŝ D

β̂·,· =

log(p ∨ n)3/2+δD

for some δD > 0 and δY > 0. With these choices, λD > 2 maxj∈Np
∥En [(pt (xi ) − dti )x∗i,j ]∥2 and λY > 4 maxj∈Np ∥En,t [ui x∗i,j ]∥2 with
probability 1 − P for a small (and shrinking) P . In generic terms,
λ is of the form Λ(1 + rn ), where Λ is an upper bound on the
true score and rn is a rate that depends on n and p.15 The specific
rate chosen serves to balance the rate of convergence against
the concentration effect: a smaller rn would increase the rate of
convergence, but at the expensive of lowering the concentration
probability 1 − P . In Appendix we show that (for appropriate δ
and n or n) the concentration probability is given by

β·,· ∈RpT

γ̂·,· =

1+

√

n

t ∈NT

E (β·,· ) =



λD =

2X T

arg min

E (β·,· ) .





and
(10)

β·,· , supp(βt )=Ŝ Y

13 The bias is away from the pseudo-true coefficients of the sparse parametric
∗
∗
representation, γ·,·
and β·,·
. There is no relation to specification biases BDt and BYt .
14 When supp(γ ∗ ) and supp(β ∗ ) do not vary much over t, the group lasso is
t

t

known to have better properties than the ordinary lasso in terms of selection and
convergence. Obozinski et al. (2011) give a sharp bound on the overlap necessary to
yield improvements, while Huang and Zhang (2010), Kolar et al. (2011), and Lounici
et al. (2011) also demonstrate advantages of the group lasso approach. These works
show, among other things, that the group lasso advantage increases with large T ,
and with the group structure, may perform better with smaller samples. We defer
to the works cited for a formal discussion.

P =

log(p ∨ n)3/2+δ

.

(12)

There are two practical methods to make these choices for
feasible for implementation. When p̂t (x) and µ̂t (x) are used to
estimate average treatment effects, the decreased sensitivity of
the final estimate to the first stage, thanks to the doubly-robust
estimator, in turn results in less sensitivity to the choice of
penalty (through the sparsity).16 The first option is an iterative
procedure to estimate the unknown X and U in λY and λD , as
employed by Belloni et al. (2012) (validity of this procedure may
be established along the same lines as in that study). We use
maxi≤n maxj∈Np |x∗i,j | for X and estimate U by iteration: given an
(0)

(k−1)

initial estimate µ̂t (x), set Û(k) = En [(yi − µ̂t
(xi ))4 ]1/4 , where
(k)
µ̂t (xi ), k > 0, is based on Eq. (10). In implementation we found
10 iterations more than sufficient, and based the initial estimate

15 The slight differences in the two are as follows. The full sample has information
on the logistic coefficients, so n appears instead of n. No error bound appears in λD
because the errors are bounded by one. The multiple 4 for λY , instead of 2, can be
traced to the quadratic loss. These forms are determined at heart by the maximal
inequality of Lounici et al. (2011).
16 To our knowledge, no formal results exist on ‘‘optimal’’ penalty parameter
choices for inference in high-dimensional problems nor are any procedures free of
user-specified choices.

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

on ridge regression (with penalty chosen by cross validation). A
second option is to select λY and λD directly by cross-validation.
This has the appealing feature that the precise forms of Eq.
(11) need not be characterized and estimated. If interest lies
in the underlying functions pt (x) and µt (x), cross validation is
appropriate as it minimizes a relevant loss function. Formal results
establishing the validity of cross-validation are not available, but it
performs well in practice.

11

Remark 5. Often, invertibility of Q and Qt relies on their convergence to nonsingular population counterparts.18 Some of the papers cited use this approach and our results can be restated in this
way by conditioning on the event that Q and Qt are close to their
counterparts in the appropriate sense, and adjusting the probability with which the conclusions hold. We instead take bounds to be
infinite if the minimum eigenvalues are zero. 
6.3. Finite sample theoretical results

6.2. Restricted eigenvalues
The local behavior of optimizations (9) and (10) is captured
by their respective Hessians, which involve the second moment
matrix of the covariates. The eigenvalues of such matrices will be
explicit in our bounds. We are interested in finite sample bounds,
and so we will only discuss the empirical Gram matrices (see
Remark 5). Define
′

Q = En [x∗i x∗i ]

and

′

Qt = En,t [x∗i x∗i ].

(13)

In high-dimensional data, both are singular, and so we use restricted eigenvalues and sparse eigenvalues (Bickel et al., 2009).
For the multinomial logistic regression, the minimal restricted
eigenvalue is defined by

  ′
δt Q δt


 




t ∈NT




2
pT
. (14)
κD ≤ min
: δ ∈ R \ {0}, δ·,{S∗D }c  ≤ 4 δ·,S∗D 
2
δ 
2,1 
2 ,1

 ∥δ·,SD∗ ∥2

We now have the necessary notation and assumptions to state
our theoretical results on group lasso estimation, beginning with
multinomial logistic regression, followed by a terse treatment of
linear models. Corollary 1 is a special case of the results in this
section, see Section 6.4.
Our first result is a nonasymptotic bound on the group lasso
estimates from (9).
Theorem 5 (Group Lasso Estimation of Multinomial Logistic Models). Suppose Assumptions 1(b), 2(a), 2(b), 2(c), and 4 hold. Define
Ap = pmin /(0 ∨ (pmin − bds )) and

 

RM = Ap pmin

(15)

∗
∗
In contrast, the refitting errors (γ̂·,· − γ·,·
) and (β̂·,· − β·,·
)
from (10) may not obey the cone constraint, but are sparse by
construction. This motivates the use of sparse eigenvalues. For a
set S ⊂ Np and a p × p matrix Q̃ , define

φ{Q̃ , S } =
2

min

δ ′ Q̃ δ
∥δ∥22

max

δ ′ Q̃ δ
.
∥δ∥22

δ∈Rp , supp(δ)=S

δ∈Rp , supp(δ)=S



√ 

,
√

for AK > 2κD2 {κD2 − (2/3)X T (30λD |S∗ | + 100 |S∗ |κD bds
1 −1
+ 80κD2 (bds )2 T λ−
. Then with probability 1 − P , we have
D )}

√
T

1. max En [(p̂t ({x∗i ′ γ̃t }NT ) − pt (xi ))2 ]1/2 ≤ RM + bds ,
t ∈NT



|S̃ D ∪ SD∗ | φ{Q , S̃ D ∪ SD∗ },
t ∈NT


3. and |S̃ D | ≤ 8sLn min{φ(Q , m) : m ∈ NDQ } ,


where NDQ = m ∈ {1, 2, . . . , n} : m > 8sLn φ(Q , m) and Ln =

 √ 2
T (RM + bds ) (λD s) .
2. max γ̃t − γt∗ 1 ≤ RM

Note that Q appears for κD , whereas the Qt are used in κY . The
restricted set, or cone constraint, requires the magnitude of δ·,· off
the true support be small relative to the true support, measured in
∗
∗
the group lasso norm.17 We will show that (γ̃·,· −γ·,·
) and (β̃·,· −β·,·
)
obey the respective constraints.

φ{Q̃ , S }2 =



T AK 6λD |S∗ |κD−1 + 8bds T

√



For least squares estimation we instead use

  ′
δt Qt δt



 



t
∈
N




T
.
κY2 ≤ min
: δ ∈ RpT \ {0}, δ·,{S∗Y }c  ≤ 3 δ·,S∗Y 
2
δ 
2,1 
2,1

 ∥δ·,SY∗ ∥2

T

and
(16)

Finally, it will be useful to define a bound on φ{Q̃ , S } over all
subsets of a certain size. To this end, for any integer m, define

φ(Q̃ , m) = maxS ⊂Np , |S |≤m φ{Q̃ , S }.
We take these quantities to be primitive, and defer to the
literature. For example, van de Geer and Buhlmann (2009), Huang
and Zhang (2010), Raskutti et al. (2010), Rudelson and Zhou (2013),
and Belloni et al. (2014). In particular, Huang and Zhang (2010)
show that the group lasso may need fewer observations to satisfy
conditions on φ{Q̃ , S }.

17 The multiplier of 4 in the constraint for κ is traceable to the nonlinear model.
D



This theorem is new to the literature, to the best of our knowledge.
Much of the detail involves capturing the finite sample behavior
of the Hessian and Gram matrices. We discuss the features of this
result in the following remarks.

• The Hessian of M(γ·,· ) is En [Hi ⊗x∗i x∗i ′ ] for a T -square matrix Hi

that depends on the coefficients and x∗i through the estimated
probabilities p̂t ({x∗i ′ γt }NT ). The error RM depends on how wellcontrolled is this matrix. The factors pmin , Ap , and AK capture the
behavior of Hi and κD−1 accounts for the rest. Under overlap, the

T
true probabilities are bounded below by pmin , and hence p−
min
captures the nonsingularity of the population version of Hi . To
get to this point requires two steps. First, the sparse parametric
representations p̂t ({x∗i ′ γt∗ }NT ) must also be bounded away from
zero, leading to the factor of Ap . This is essentially a bias
condition, which in the asymptotic case holds trivially: Ap may
be chosen arbitrarily close to one as bds → 0. Second, AK controls
the neighborhood in which p̂t ({x∗i ′ γ̃t }NT ) is also bounded away
from zero. Intuitively (and asymptotically), the estimate will
be in a small (shrinking) neighborhood of the p̂t ({x∗i ′ γt∗ }NT ).
In asymptotics AK may be chosen arbitrarily close to 2, which
stems from the factor of 1/2 in a quadratic expansion of M (·). A

18 This is standard in fixed-dimension models, and has been used for divergingdimensions parametric models (He and Shao, 2000) and nonparametrics (Newey,
1997; Huang, 2003; Cattaneo and Farrell, 2013; Belloni et al., 2015; Chen and
Christensen, forthcoming). The eigenvalue assumptions employed in those works
are conceptually the same as the restricted eigenvalues used here, only restricted
to the p < n case.

12

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

lower bound on AK is required in finite samples to ensure that
p̂t ({x∗i ′ γ̃t }NT ) is positive, and hence the two-term expansion is
valid. This is analogous to Belloni and Chernozhukov’s (2011)
‘‘restricted nonlinear impact coefficient’’ approach, also used
by Belloni et al. (2014) with a central difference that AK is
captured in our bound directly.
• The maximal sparse eigenvalues are crucial to the bound
on |S̃ D |. In many prior results, the latter is bounded using
the largest eigenvalue of Q itself, i.e. φ(Q , n). Adapting the
technique of Belloni and Chernozhukov (2013) to the present
case, we are able to find a tighter bound, which yields sparsity
proportional to s under weaker conditions. This is crucial for
refitting.
• For the linear model the constants in the group lasso bounds
can offset the (logarithmic) suboptimality in rate (Huang and
Zhang, 2010; Lounici et al., 2011), and this may be true here as
well. This is application dependent however.
The error bounds for post-selection estimation are more complex and depend in part on the good properties of the initial group
lasso fit. The following theorem gives our results.
Theorem 6 (Post-Selection Multinomial Logistic Regression). Suppose the conditions of Theorem 5 hold. To save notation, let SD =
ŜD ∪ SD∗ and φ = φ{Q , ŜD ∪ SD∗ }. Then for

φ2


AK > 2

√

Then with probability 1 − P , maxt ∈NT En [(p̂t ({xi γ̂t }NT ) − pt


1/2


(xi ))2 ]1/2 ≤ R′′M + bds , and maxt ∈NT γ̂t − γt∗ 1 ≤ |S D |/φ

R′′M .
It is not readily discernible if these bounds improve upon the initial
fit. This will depend on the DGP, the selection success of the initial
fit, and any added variables. In this result, further lower bounds on
AK are required to handle the sparse eigenvalues, compared to the
restricted version in Theorem 5. The role played by AK is the same
in both cases, as with the other factors.
It is worth noting that, despite the complexity of multinomial
logistic regression, the conditions for Theorems 5 and 6 are simple
and intuitive, and match those used for linear models.
We now give our results for group lasso estimation of the
conditional outcome regressions. In computing µt (xi ) for dti ̸=
1 we are performing out of sample prediction, which slightly
complicates the bounds. Our first result is on the initial group lasso
fit.
Theorem 7 (Group Lasso Estimation of Linear Models). Suppose Assumptions 1(b), 2(a)–2(c), and 4 hold. To save notation, let SY =
S̃ Y ∪ SY∗ . Define
RE =

3λY

√

κY

s



+ 2bys .

Then with probability 1 − P , we have







2. maxt ∈NT β̃t − βt∗ 

≤

1

SY })1/2 RE ,

1/2
 

(φ{Q , SY } φ{Qt ,
|SY | φ{Q , SY }



3. and |S̃ Y | ≤ 32sLn minm∈NY



Q

where NYQ

=



y

and Ln = (RE + bs ) (λY



t ∈NT


φ(Qt , m) ,

m ∈ {1, 2, . . . , n} : m > 32sLn



√ 2
s) .



t ∈NT

φ(Qt , m)



This theorem generalizes Lounici et al. (2011) to the nonparametric, approximately sparse case, improves the sparsity bound,
and gives out of sample prediction (imputation) results. The analogous generalization for within sample prediction loss (e.g. multitask learning), En,t [(x∗i ′ β̃t − µt (xi ))2 ]1/2 , may be found in the
Supplement (see Appendix D).
For refitting, we are predicting for the entire sample and so we
utilize the general results given by Belloni et al. (2012) for postselection estimation of least squares. The following result is a direct
implication of their Lemma 7 and our Theorem 7.
Theorem 8 (Post-Selection Linear Regression). Suppose log(p) =
o(n1/3 ) in addition to the conditions of Theorem 7. Then for constants
A1 –A4 not depending on n nor the DGP:



∗′


1/2

φ{Q , SY } φ{Qt , SY }

y

RE + bs ,



√ √
φ 2 − X T (λD |SD | + bds φ T |SD |)


φ
∨
√ √
φ − 2RM X T |SD |
 √
√ 

T
define R′M = Ap /pmin T AK λD |SD |φ −1 /2 + bds T and

1/2 

 
T
′′
2
′
′
RM = {RM } ∨ RM + RM RM + Ap pmin T AK RM
.



1. maxt ∈NT En [(x∗i ′ β̃t − µt (xi ))2 ]1/2 ≤

2 1/2

En [(xi β̂t − µt (xi )) ]
′

≤ A1

s(T ∧ log(sT ))
nφ{Q , SY∗ }




 |ŜY \S ∗ | log(pT )
Y
+ A2 
+
A
En [(x∗i ′ β̃t − µt (xi ))2 ]
3
nφ{Q , SYFP }
and maxt ∈NT ∥β̂t −βt∗ ∥1 ≤ A4 (|ŜY ∪SY∗ |En [(x′i β̂t −µt (xi ))2 ] φ{Q , ŜY



∪ SY∗ })1/2 .

As above, the performance of the refitting procedure depends in
part on the success of the initial group lasso fit. Indeed, the middle
term is dropped if the true support union is found. The constants
Ak , k = 1, 2, 3, 4 are not given explicitly but are known to be
absolute bounds (de la Peña et al., 2009) under Assumption 2. This
result is less precise than Theorems 5 and 6, but sufficient to verify
Assumptions 3 and 5.
6.4. Asymptotic analysis and verification of high-level conditions
This section derives rates of convergence for the group lasso
estimates and uses these results to verify Assumptions 3 and 5
in Section 5. For simplicity, we only state results for the postselection estimators that we recommend in practice. In reducing
the finite sample results of Theorems 6 and 8 to rates we retain
the dependence on n, p, s, and the bias. Note that the number of
treatments is fixed, and the overlap assumption ensures that all
nt ∝ n. Further, the various (restricted and sparse) eigenvalues
are commonly taken to be bounded (or bounded away from zero)
in asymptotic analyses. This accounts for the remaining factors
in the bounds. For multinomial logistic regression, we obtain the
following result.
Corollary 4 (Asymptotics for Multinomial Logistic Regression). Suppose the conditions of Theorem 6 hold and further that (i) λD sd =
o(1), (ii) κD is bounded away from zero, and (iii) minS :|S |=O(s) φ{Q , S }
is bounded away from zero and φ(Q , ·) is bounded, uniformly in NDQ .
Then

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

1. |S̃ D | = OPn (sd ),
2. En [(p̂t ({x∗i ′ γ̂t }NT ) − pt (xi ))2 ] = OPn (n−1 sd log(p ∨ n)3/2+δD +
(bds )2 ), and
3. ∥γ̂t − γt ∥1 = OPn
∗



n− 1 s 2
d

3/2+δD

log(p ∨ n)

+

bds

β00 = ρβ (−1, 1, −1, 2−αβ , −3−αβ , . . . , j−αβ , . . . , p−αβ )′ ,

sd .

γ 0 = ργ (1, −1, 1, −2−αγ , 3−αγ , . . . , j−αγ , . . . , −p−αγ )′ ,

Similarly, we have the following for the linear models.
Corollary 5 (Asymptotics for Linear Regression). Suppose the con√
ditions of Theorem 8 hold and further that (i) λY sy = o(1),

(ii) κY is bounded away from zero, and (iii) uniformly in NT ,
minS :|S |=O(s) φ{Qt , S } ∧ φ{Q , S } is bounded away from zero and
φ(Q , ·) ∨ φ(Qt , ·) is bounded uniformly in NYQ . Then
1. |S̃ Y | = OPn (sy ),

2. En [(µ̂t (xi ) − µt (xi ))2 ] = OPn n−1 sy log(p ∨ n)3/2+δY + (bs )2 ,
and


y



3. ∥β̃t − βt∗ ∥1 = OPn

y

n−1 s2y log(p ∨ n)3/2+δY + bs

coefficient vectors β00 , β10 , and γ 0 , which are defined to vary with
the positive scalars ρβ , ργ , αβ , and αγ , as follows:



√



√

s .

It is now straightforward to verify the requirements of Section 5.
Assumption 3(b) requires

(n−1 sd log(p ∨ n)3/2+δD + (bds )2 )(n−1 sy log(p ∨ n)3/2+δY + (bys )2 )
 
= o n−1 .
√
Under the common assumption that bs = O( s/n), we require
3+δD +δY
sd sy log(p ∨ n)
= o(n). Both this, and the display above,
clearly show how the sparsity and smoothness of the two functions
interact due to the double robustness. Assumption 5 can be verified
similarly.
These rates of convergence (i.e. part 2 of each corollary)
are optimal up to factor log(p ∨ n)1/2+δ . At heart, this loss
appears to stem from the maximal inequality used to establish the
concentration probability of (12). In practice, this is unlikely to be
a limitation. As mentioned above, the use of group lasso can yield
improvements in the constants if the data obey a grouped sparsity
pattern, as is expected for treatment effects data, and may even
yield improvements in the detection of the sparse signal, further
offsetting the suboptimal log factor (see for example Lounici et al.
(2011) or Obozinski et al. (2011)). Alternative methods could, in
principle, yield a rate improvement. Chief among these would be
lasso-penalized linear probability models (see also Remark 3) or
separate logistic regressions. The group lasso approach adopted
here reflects common practice, and so it may be preferred. In any
case, the log factors do not impact the treatment effect inference.
7. Numerical and empirical evidence
7.1. Simulation study
We conducted a Monte Carlo exercise to study how our
estimator behaves as the propensity score and regression functions
change, and the model selection problem becomes more or less
difficult.19 For simplicity we focus on the average effect of a binary
treatment. We generated 1000 observations (yi , di , x′i )′ from the
models in Example 3, using both p = 1000 and p = 1500. The
covariates include an intercept, with the remainder drawn from
N (0, Σ ), with covariance Σ [j1 , j2 ] = 2−|j1 −j2 | , 2 ≤ j1 , j2 ≤ p.
Errors are standard Normal. The crucial aspects of the DGP are the

19 The supplemental Appendix contains the additional results (see Appendix D).

13

with β10 = −β00 . The ρ multipliers affect the signal-to-noise ratio,
but not the sparsity. For smaller values distinguishing the large
and small coefficients is more difficult for a given sample. The
exponents α control the sparsity, where a sparse representation
is not possible for small values.
Fig. 1 shows the empirical coverage rates of 95% confidence
intervals for µ1 − µ0 for different DGPs, for p = 1000 and 1500.
Panels (a) and (c) show coverage as the multipliers ρβ and ργ range
over 0.01 (weak signal) to 1 (strong), with αβ = αγ = 2. Panels (b)
and (d) vary the sparsity exponents αβ and αγ over 1/8 (not sparse)
to 4 (very sparse), with ρβ = ργ = 1. Of 1000 observations total,
the (mean) size of the comparison group declines from roughly 500
to 300 as ργ increases and 450 to 300 as αγ increases, over their
given ranges. Coverage is accurate over all signal strengths, and
breaks down only when neither µt (xi ) nor pt (xi ) is sparse, which
is exactly when Assumption 3(b) (or condition (ii) of Theorem 1)
cannot be satisfied. Note that coverage accuracy is retained when
only one function is sparse, showcasing the double-robustness
property.
The penalty parameters λD and λY are chosen using the iterative
procedure described in Section 6.1, with δD = 4.5 and δY = 5
throughout. Different DGPs exhibit different sensitivity to these
values. Results using penalties chosen via 10-fold cross-validation
appear in Fig. 2, which also exhibits excellent coverage across all
sparse designs.20
7.2. Empirical application
To illustrate the role that model selection can play in a realworld application, we revisit the National Supported Work (NSW)
demonstration. The NSW has been analyzed numerous times
since LaLonde (1986). Our aim is a simple study of model selection,
not a comprehensive or conclusive evaluation of the NSW. We
focus on the subsample used by Dehejia and Wahba (1999) and the
Panel Study of Income Dynamics (PSID) comparison sample, taking
as given their data definitions, sample selection, and trimming
rules. Detailed discussion of these choices, and the NSW program
may be found in Dehejia and Wahba (1999, 2002) (hereafter
DW99 and DW02) and Smith and Todd (2005), and references
therein. Briefly, the outcome of interest is earnings following a job
training program. The dataset includes a treatment indicator, posttreatment earnings (1978), two years of pre-treatment earnings
(197421 and 1975), as well as age, education, a marital status,
and indicators for Black and Hispanic. Thus, X consists of seven
variables. We will keep the estimator fixed: all estimates will be
based on the doubly-robust estimator with standard errors from
Section 5.2. We will compare the following specifications for X ∗ :
1. No Selection: X , (earn1974)2 , (earn1975)2 , (age)2 , and (educ)2 .
2. Informally Selected: The above, plus
{educ < HS},
{earn1974 = 0}, {earn1975 = 0}, and ( {earn1974 = 0}×
Hispanic). This specification was selected by DW02 using an
informal balance test.

20 The R routines appear unstable for nonsparse designs, thus the analogues to
panels (b) and (d) of Fig. 1 are omitted. See the supplement for limited versions.
This will be explored for future software development.
21 This naming follows DW99, but the variable may be measured outside 1974,
see discussion in the works cited.

14

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

µ(x) signal

µ(x) sparsity

pt(x) signal

1.0

pt(x) sparsity

1.0

0.5

0.5

0.0

0.0

(a) p = 1000, varying signal strength.

µ(x) signal

(b) p = 1000, varying sparsity.

µ(x) sparsity

pt(x) signal

1.0

pt(x) sparsity

1.0

0.5

0.5

0.0

0.0

(c) p = 1500, varying signal strength.

(d) p = 1500, varying sparsity.

Fig. 1. Empirical coverage of 95% confidence intervals, varying signal strength and sparsity of pt (x) and µt (x).

µ(x) signal

µ(x) signal

pt(x) signal

1.0

pt(x) signal

1.0

0.5

0.5

0.0

0.0

(a)1000 Covariates.

(b)1500 Covariates.

Fig. 2. Empirical coverage of 95% confidence intervals, penalty chosen with cross-validation, varying signal strength of pt (x) and µt (x).

3. Group Lasso Selection: X ,

{educ < HS},

{earn1974 = 0},

{earn1975 = 0}, all possible first-order interactions, and all

polynomials up to order five of the continuous covariates (age,
educ, earn1974, earn1975).

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

For specifications 1 and 2, the same covariates are in the outcome
and treatment models. All specifications include an intercept and
we include education and pre-treatment income in the refitting
step following model selection. We follow DW99 and DW02 and
trim comparisons with estimated propensity score larger (smaller)
than the maximum (minimum) in the treated sample.22
Table 1 presents results from these three specifications, and
includes the experimental arm of the NSW. The group lasso based
estimate performs very well: the point estimate is accurate and the
interval is tight. Selecting from 171 possible covariates allows for
a great deal of flexibility, but the sparsity of the estimate keeps
the variance well-controlled. The no-selection point estimate is
accurate, but fails to yield significance, while the specification
of DW02 yields a significant, but overly high estimate and wide
confidence interval. The benefits of explicit model selection are
clear.

This paper proposed a method that achieves uniformly valid
inference on mean effects of a multivalued treatment even
after model selection among possibly more covariates than
observations. We demonstrated robustness to model selection
errors, misspecification, and heterogeneous effects in observables.
To accomplish this, a doubly-robust estimator was employed and
shown to have excellent properties following model selection. We
proved new results on group lasso estimation, which we argue is
natural for treatment effects data. Multinomial logistic regression
was studied in some detail. Numerical evidence shows that our
method is quite promising for applications.
A key outstanding question in this work and in the highdimensional, sparse modeling literature more generally, is penalty
parameter choice. Very little work has been done in this area,
which is a crucial gap in implementability of these techniques. We
plan to develop a formal choice for the penalty parameter that is
appropriately optimal. Tuning parameter selection in semi- and
nonparametric analysis, and its impact on estimation and inference, is becoming better understood, and parallel developments
must take place in model selection contexts.

The proofs in this section are asymptotic. Order symbols hold
for the sequence being considered, as a shorthand for the more
formal versions given in e.g. Assumption 3. C will denote a generic
positive constant, which may be a matrix. Define the set of indexes
It = {i : di = t }. The online supplement contains much greater
detail. We make frequent use of the linearization

a

1
b

+

b−a
ab

=

1
b

+

b−a
b2

+

(b − a)2
ab2

.

(A.1)

Proof of Theorem 2. See supplemental Appendix.



Proof of Theorem 3.1 without Additional
Randomness.
With
√
√
ψt (·) defined in Eq. (2), we have n(µ̂t − µt ) = nEn [ψt (yi , dti ,
µt (xi ), pt (xi ), µt )] + R1 + R2 , where
n
1 

R1 = √
n i=1

dti

n
dti
1 
(µ̂t (xi ) − µt (xi )) 1 −
R2 = √
p̂t (xi )
n i=1





.

The proof proceeds by showing that both R1 and R2 are oPn (1).
Applying the first equality in Eq. (A.1), we rewrite R1 as
n
1  t
R1 = √
di ui
n i=1



pt (xi ) − p̂t (xi )



p̂t (xi )pt (xi )

.

Applying Assumptions 1(b) and 2(c) and the first-stage consistency
condition of Assumption 3(a):

E R21 |{xi , di }ni=1 = En





dti σt2 (xi )



p̂t (xi )2 pt (xi )2

2

pt (xi ) − p̂t (xi )





≤ C En [(pt (xi ) − p̂t (xi ))2 ] = oPn (1).

(yi − µt (xi ))



1
p̂t (xi )

−

1

n
1 
R21 = √
(µ̂t (xi ) − µt (xi ))
n i =1



pt (xi ) − dti



pt (xi )

22 A formal treatment of trimming is beyond the scope of the present study. The
goal of our analysis is illustrative, and hence we take DW99’s trimming as given.
This issue is discussed by DW99, DW02, and Smith and Todd (2005).



pt (xi )

and
n
1 
(µ̂t (xi ) − µt (xi ))(p̂t (xi ) − pt (xi ))
R22 = √
n i =1



dti



p̂t (xi )pt (xi )

.

For the first term E R221 |{xi }ni=1 ≤ C En (µ̂t (xi ) − µt (xi ))2 =
oPn (1), by the first-stage consistency condition of Assumption 3(a).
Next,



√
|R22 | ≤


n max
i≤n



1







p̂t (xi )pt (xi )



× En [(µ̂t (xi ) − µt (xi ))2 ]En [(p̂t (xi ) − pt (xi ))2 ] = oPn (1).
by Hölder’s inequality, Assumption 1(b) and the rate condition of
Assumption 3(b). 
Proof of Theorem 3.1 with Additional Randomness. We must
reconsider the remainders R1 and R2 . For the former, applying
Eq. (A.1), we find R1 = R11 + R12 , where
n

1  dti ui 
pt (xi ) − p̂t (xi )
R11 = √
2
n i=1 pt (xi )

Appendix A. Proofs for treatment effect inference

=

and

Next, again using Eq. (A.1), we have R2 = R21 + R22 , where

8. Discussion

1

15

and

n

2
1 
dti ui
p̂t (xi ) − pt (xi ) .
R12 = √
2
n i=1 pt (xi ) p̂t (xi )

For R11 , we first add and subtract the parametric representation to
get R11 = R111 + R112 , where,
n

1  dti ui 
′
′
R111 = √
p̂t ({x∗i γt∗ }NT ) − p̂t ({x∗i γ̂t }NT )
n i=1 pt (xi )2

and

n

1  dti ui 
′
R112 = √
pt (xi ) − p̂t ({x∗i γt∗ }NT ) .
2
n i=1 pt (xi )

By a two-term mean-value expansion R111 = R111a + R111b , with
n
1  dti ui  
′
′
R111a = √
p̂t ({x∗i γt∗ }NT )(1 − p̂t ({x∗i γt∗ }NT ))
n i=1 pt (xi )2 t ∈NT



× x∗i ′ (γ̂t − γt∗ )
n
1  dti ui ′
and R111b = √
vi H̄ vi ,
2 n i=1 pt (xi )2

where vi = {x∗i ′ (γ̂t − γt∗ )}NT and H = H ({x∗i ′ γt∗ + mt x∗i ′ γ̂t }NT )
for appropriate scalars mt and the T -square Hessian matrix
H ({x∗i ′ γt }NT ) (defined in Appendix B).

16

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

For R111a , consider
each term in the sum over NT one at a time;

′
let R111a =
t ∈NT R111a (t ). Let t denote the original treatment
under consideration. Define

Σt , j = E



(x∗i,j )2 σt2′ (xi )p̂t ({x∗i ′ γt∗ }NT )2

2
3
∗′ ∗

Then proceed as follows:

=

n
1 

√

n i =1

j∈ŜD
1/2
Σt , j



t′

d
x∗i,j i

ui p̂t ({x∗i ′ γt∗ }NT )(1 − p̂t ({x∗i ′ γt∗ }NT ))



γt∗,j )

√
t ∈NT

n(max |ui |)En |x∗i (γ̂t − γt∗ )|2
′





i∈It

n(max |ui |) max

1


En





pt (xi ) − dti

pt (xi ) − dti



pt (xi )

.




pt (xi )

x∗i,j (β̂t ,j − βt∗,j )

j∈ŜY


t
∗
n
1  xi,j (pt (xi ) − di )/pt (xi )
1/2
=
Σ̃t ,j (β̂t ,j − βt∗,j )
√
1/2
n i=1
Σ̃t ,j
j∈ŜY



t
∗
n
1  xi,j (pt (xi ) − di )/pt (xi )
1/2
≤ max Σ̃t ,j
max √
1/2
j∈Np
j∈Np
n i =1
Σ̃t ,j




× β̂t − βt∗ 
1




= O(1)OPn (log(p)) β̂t − βt∗  = oPn (1),
1

where the final line follows exactly as above. A variance bound may
be applied to R212 as in the previous proof, and we have |R212 | =
OPn (bs ) = oPn (1) by Markov’s inequality. 
Proof of Theorem 3.2. This follows from the prior result and
Assumption 2(e). 
Proof of Theorem 3.3. We begin with V̂W (t ). Expanding the
square and using Eq. (A.1), rewrite V̂µW (t ) = En [dti u2i pt (xi )−2 ] +
RW ,1 + RW ,2 + RW ,3 where
dti u2i


RW ,1 = En

p̂t (xi )2 pt (xi )2


R W ,2 = E n

p̂t (xi ) − pt (xi )



dti (µt (xi ) − µ̂t (xi ))2




p̂t (xi ) + pt (xi )



,



, and

dti ui (µt (xi ) − µ̂t (xi ))
.
p̂t (xi )2
p̂t (xi )2


RW ,3 = 2En

p̂t (xi ) + pt (xi )

2 

i∈It

where the rate follows from Assumptions 1(b), 2 and 3, and this
tends to zero by Assumption 5.
As in the prior proof, write R2 = R21 + R22 . The same bound
is used for R22 . However, for R21 , add and subtract the pseudotrue



En [dti |ui |4 ]1/2
p̂t (xi )2 pt (xi )2
× En [dti (p̂t (xi ) − pt (xi ))2 ]1/2 = oPn (1),
max
i∈It



p̂t (xi ) − pt (xi )

i∈It pt (xi )2 p̂t (xi )

2 
√
≤ OPn (1) n(max |ui |)En p̂t (xi ) − pt (xi )
= oPn (1),
i∈It





R W ,1 ≤

by the union bound and Assumption 5, using Assumptions 1(b) and
3(a) to apply Eq. (B.15) with the inequality reversed.
A variance bound may be applied to R112 as in the previous
proof, and we have |R112 | = OPn (bs ) = oPn (1) by Markov’s inequality.
Next, R12 is simply bounded by

√

and

pt (xi )










× |p̂t ({x∗i ′ γ̂t }NT ) − p̂t ({x∗i ′ γt∗ }NT )|2  = oPn (1),


|R12 | ≤



Using Hölder’s inequality, Assumptions 1(b), 2(e) and 3(a), we have
the following:


√
≤ C T max  n(max |ui |)En
t ∈NT
i∈It



n
1 

R211 = √
n i =1

1/2

pt ′ (xi )2 Σt ,j

Convergence follows under Assumption 5. For the penultimate
equality, it follows from Assumptions 1(b), 2(b) and 2(c) that
maxj∈Np Σt ,j = O(1). Finally, the center factor is shown to be
OPn (log(p)) by applying the moderate deviation theory for selfnormalized sums of de la Peña et al. (2009, Theorem 7.4) and in
particular Belloni et al. (2012, Lemma 5). To apply this lemma, first
note that the summand of the center factor has bounded third moment and second moment bounded away from zero, from Assumptions 1(b), 2(b), 2(c), and the requirements of Assumptions 3 and 5.
Σt ,j normalizes the second moment, and the lemma applies under
Assumption 4 and the first restriction of Assumption 5.
For R111b , the results of Tanabe and Sagae (1992) coupled with
Assumption 3 give vi′ H̄ vi ≤ C ∥vi ∥22 . Thus, using Assumption 1(b)
to bound maxi≤n pt (xi )−2 < C , we find R111b may be bounded as
follows:

|R111b | ≤ C

pt (xi ) − dti



(γ̂t ,j −

1/2
≤ max Σt ,j
j∈Np


′
n
1  ∗ dti ui p̂t ({x∗i ′ γt∗ }NT )(1 − p̂t ({x∗i ′ γt∗ }NT ))
xi,j
× max √
1/2
j∈Np
n i=1
pt ′ (xi )2 Σt ,j


× γ̂t − γt∗ 1


= O(1)OPn (log(p)) γ̂t − γt∗ 1 = oPn (1).
×




For the first term, define Σ̃t ,j = E (x∗i,j )2 (dti − pt (xi ))2 /pt (xi )2 and
then proceed as follows:

R111a (t )



R211

n
1  ∗′
= √
(xi β̂t − x∗i βt∗ )
n i =1

n
1  ∗ ∗
R212 = √
(xi βt − µt (xi ))
n i =1

× (1 − p̂t ({xi γt }NT )) /pt ′ (xi ) .



values to get R21 = R211 + R212 , where

R W ,2 ≤

max
i∈It



1
p̂t (xi )2

En [dti (µ̂t (xi ) − µt (xi ))2 ] = oPn (1),



1



En [dti |ui |2 ]1/2
p̂t (xi )2
× En [dti (µ̂t (xi ) − µt (xi ))2 ]1/2 = oPn (1),

and, RW ,3 ≤ 2 max
i∈It

where En [|ui |4 ] = OPn (1) from the inequality of von Bahr
and Esseen (1965). From the same inequality it follows that
En [dti u2i pt (xi )−2 ] − VµW (t )| = oPn (1), under Assumptions 1(b) and
2(c).
Next consider the ‘‘between’’ variance estimator, V̂µB . For any
t NT and t ′ ∈ NT , define
RB,1 (t , t ′ ) = En (µ̂t (xi ) − µt (xi ))(µ̂t ′ (xi ) − µt ′ (xi )) ,





RB,2 (t , t ′ ) = µ̂t En µ̂t ′ (xi ) − µt ′ (xi ) ,





and

RB,3 (t , t ) = En µt (xi )(µ̂t ′ (xi ) − µt ′ (xi )) .
′





M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

17

From Hölder’s inequality, Assumption 3(a), Theorem 3.2, the von
Bahr and Esseen inequality, and Assumptions 2(c) and 2(e) it follows that RB,k (t , t ′ ) = oPn (1) for k ∈ N3 and all pairs (t , t ′ ) ∈ N2t .
With this in mind, we decompose

Again using Lemma 9.1 of Lounici et al. (2011), and Assumptions 2(a) and 2(b), we bound the expectation in the second term
above as follows:

V̂µ (t , t )

E max En [vt ,i x∗i,j ]

′

B



4




≤

64 log(12p)2 X4
n2

j∈Np

= En [µt (xi )µt ′ (xi )] − µ̂t En [µt ′ (xi )] − µ̂t ′ En [µt (xi )] + µ̂t µ̂t ′
+ RB,1 (t , t ′ ) + RB,2 (t , t ′ )
+ RB,2 (t ′ , t ) + RB,3 (t , t ′ ) + RB,3 (t ′ , t ).

Collecting these results proves the Lemma.



Lemma B.2 (Estimate Sparsity). With probability at least 1 − P

Consistency of V̂µ (t , t ) now follows from the von Bahr and Esseen
inequality and Theorem 3.2. 
′

B

.

|S̃ D | ≤

4

λ

2
D



φ{Q , S̃ D }

En (p̂t ({x∗i γ̃t }NT ) − pt (xi ))2 .
′





t ∈NT

Proof of Corollary 2. Suppose the result did not hold. Then, there
would exist a subsequence Pm ∈ Pm , for each m, such that






lim PPm G(µ) ∈

m→∞



G(µ̂) ± cα





∇G (µ̂)V̂ ∇G′ (µ̂)/n − (1 − α)

> 0.

Proof. From the Karush–Kuhn–Tucker conditions for (9), for all
t ∈ NT , if γ̃·,j ̸= 0 it must satisfy

En [x∗i,j (p̂t ({x∗i γ̃t }NT ) − dti )] = λD
′

But this contradicts Theorem 3, under which (∇G (µ̂)V̂ ∇G′ (µ̂)/
n)−1/2 (G(µ̂) − G(µ)) is asymptotically standard normal under the
sequence Pm . 
Appendix B. Proofs for group Lasso selection and estimation of
multinomial logistic models
This section is nonasymptotic. We use generic notation X ∗ , δ ,
etc. The online supplement has greater detail (see Appendix D).

Taking the ℓ2 -norm over t ∈ NT for fixed j ∈ S̃ D , adding and
subtracting the true propensity score, using the triangle inequality,
the score bound (B.1), collecting terms, squaring both sides, and
summing over j ∈ S̃ D (i.e. applying ∥ · ∥22 over j ∈ S̃ D to both sides)
yields





λ2D ≤ 4

En [x∗i,j (p̂t ({x∗i γ̃t }NT ) − pt (xi ))]2
′

j∈S̃ D t ∈NT

j∈S̃ D

≤ 4φ{Q , S̃ D }

B.1. Lemmas



En (p̂t ({x∗i γ̃t }NT ) − pt (xi ))2 .
′





t ∈NT

The following three lemmas are needed for the proofs of
Theorems 5 and 6. Due to space considerations, only a short sketch
of the proofs will be given, highlight the main ideas in each. Full
details are available in the online supplement (see Appendix D).
Lemma B.1 (Score Bound). For λD and P defined in Eqs. (11) and
(12) we have



P max ∥En [(pt (xi ) −
j∈Np

dti

)x∗i,j ]∥2

≥

λD



2



P max ∥En [(pt (xi ) − dti )x∗i,j ]∥2 ≥
j∈Np




≤ P max

ξ t ,j ≥

X2 T rn



n

t ∈NT

λD



≤ 4 log(2p)



2 ,1
2 ,1
√

√
1 d
∗′
2 1/2
|S∗ |, 2λ−
b
T
E
[∥{
x
δ̃
}
∥
.
n
t
N
T 2]
s
D
i



En (pt (xi ) − dti )x∗i δ̃t
′



t ∈NT

≤


j∈Np

En (pt (xi ) −



dti

2
)x∗i,j



t ∈NT

δ̃t2,j

t ∈NT


 
   
λD  
 
≤ max En (pt (xi ) − dti )x∗i,j 2
δ̃·,j  ≤
δ̃·,· 
j∈Np

j∈Np

2 ,1

2

2

,

(B.1)

2



 
n


≤ E max 
ξt ,j 
2
 X T rn
j∈Np 
t ∈N


T

with probability at least 1 − P . Applying the Cauchy–Schwarz inequality, the bias condition of Assumption 4, and Cauchy–Schwarz
again yields





En (p̂t ({x∗i γt∗ }NT ) − pt (xi ))x∗i δ̃t
′

′



t ∈NT

≤



1/2

where an := max κD−1



where final line follows from Markov’s inequality. Next, applying
Lemma 9.1 of Lounici et al. (2011), Jensen’s inequality, and
Assumption 2(c), we find that



 


E max 
ξ t ,j 

j∈Np 
t ∈NT




Lemma B.3 (Bounds in ℓ2 /ℓ1 norm).
 1−
 P the
  With probability


 
∗
vector δ̃·,· = γ̃·,· − γ·,· satisfies δ̃·,·  ≤ 5an and δ̃·,S∗  ≤ an



and the definition of X, we find that E ∥En [vt ,i x∗i,j ]∥22 ≤ X2 T /n,
uniformly in j ∈ Np . Define the mean-zero random variables ξt ,j =
(En [vt ,i x∗i,j ])2 − 1n E[Vt2 Xj∗ 2 ] and set rn = T −1/2 log(p ∨ n)3/2+δ . Then



The result now follows, as the left-hand side is equal to |S̃ D |λ2D .

Proof. By the Cauchy–Schwarz inequality and Lemma B.1,

≤ P.

Proof. The residuals vt ,i = pt (xi ) − dti are conditionally mean-zero
by definition and satisfy E[vt2,i |xi ] ≤ 1. Using this, Assumption 2(a),

j∈Np

γ̃t ,j
.
∥γ̃·,j ∥2



En (p̂t ({x∗i γt∗ }NT ) − pt (xi ))2
′



1/2



En (x∗i δ̃t )2
′

1/2

t ∈NT

 X4

t ∈NT

n2

+


t ∈NT




4
E max En [vt ,i x∗i,j ]
j∈Np

1/2

≤ bds
.





En (x∗i δ̃t )2
′

1/2

t ∈NT

√
≤ bds T En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 .

(B.2)

18

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

Combining Eqs. (B.1) and (B.2), we have, probability at least
1 − P,









En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̃t =
′

′



′
En (pt (xi ) − dti )x∗i δ̃t

of κD2 . This implies

 
 
δ̃·,· 





≤ 5 δ̃·,S∗ 

2,1

+





En (p̂t ({x∗i γt∗ }NT ) − pt (xi ))x∗i δ̃t
′

′



t ∈NT

λD  
≤
δ̃·,· 

2 ,1

2

√
d

+ bs T En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 .

(B.3)







∗
By the optimality of δ̃·,· , M (γ·,·
+ δ̃·,· ) + λD γ·,·∗ + δ̃·,· 

M (γ·,·∗ ) + λD γ·,·∗ 2,1 , and so





2 ,1

≤



λD





√

5 bds T
15 

′
En [∥{x∗i δ̃t }NT ∥22 ]1/2
≤
δ̃·,S∗  +
2,1
4
2 λD







Combining

 the right hand side of the first line with third lines yields

 c 
δ̃·,S∗ 

√

≤ 8bds T En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 λD . Plugging this back

2,1

into the last line we obtain the bound



 
 
δ̃·,· 

2 1/2
2



5 

δ̃·,S∗c 
2,1
4

√

En [∥{xi δ̃t }NT ∥ ]



2,1

≤



2,1

≤ 10

bds

√
T

λD

En [∥{x∗i δ̃t }NT ∥22 ]1/2 ,
′

(B.7)

while instead, plugging it into the failure of the cone constraint
yields



1 
1 


= δ̃·,S∗  + δ̃·,S∗c  + γ·,∗S∗ 2,1
2,1
2,1
2
2


 
 
δ̃·,· 

5 bds T
15 

′
En [∥{x∗i δ̃t }NT ∥22 ]1/2 .
≤
δ̃·,S∗c  +
2,1
16
2 λD

Dividing through λD and decomposing the supports, we find that

 


 ∗ 
1  
 − γ ∗ + δ̃·,· 
0 ≤ δ̃·,·  + γ·,·
·,·
2
,
1
2,1
2,1
2




2,1


  λ  
 ∗ 
D 

 ∗







0 ≤ λD
γ·,· 2,1 − γ·,· + δ̃·,· 
+
δ̃·,· 
2,1
2,1
2
√
d
∗′
2 1/2
+ bs T En [∥{xi δ̃t }NT ∥2 ] .

+

(B.6)

Eq. (B.4) for the second, we have



∗′

2,1

√
|S∗ |
′
En [∥{x∗i δ̃t }NT ∥22 ]1/2 .
≤
κD

<
2 ,1








1
δ̃·,S∗c  . Using this for the first and third inequalities, and
4

applying the convexity of M . Using the bound in Eq. (B.3) and
rearranging we find that

T





δ̃·,S∗ 




t ∈NT

bs

by the Cauchy–Schwarz inequality, the restricted eigenvalue defi
nition of Eq. (14), and noting that t ∈NT δ̃t′ Q δ̃t = En [∥{x∗i ′ δ̃t }NT ∥22 ].
Collecting across the second and third inequalities yields

On the other hand, if the cone constraint fails, then δ̃·,S∗ 



 
 


λD γ·,·∗ 2,1 − γ·,·∗ + δ̃·,· 
≥ M(γ·,·∗ + δ̃·,· ) − M(γ·,·∗ )
2,1

 
′
′
En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̃t ,
≥

√
d

2,1

√



5 |S∗ |


′
En [∥{x∗i δ̃t }NT ∥22 ]1/2 , (B.5)
≤ 5 |S∗ | δ̃·,S∗  ≤
2
κD

t ∈NT

t ∈NT



bds

√









− γ·,∗S∗ + δ̃·,S∗  − δ̃·,S∗c 
2 ,1
2 ,1
√
d





δ̃·,S∗ 

+

Combining Eqs. (B.5) and (B.7) gives the first claim of the lemma
and Eqs. (B.6) and (B.8) give the second. 

bs

T

λD

En [∥{x∗i δ̃t }NT ∥22 ]1/2 ,
′

because γ·,∗S c = 0. Collecting terms and applying the triangle in∗
equality yields







 


1 
1 




δ̃·,S∗c  ≤ δ̃·,S∗  + γ·,∗S∗ 2,1 − γ·,∗S∗ + δ̃·,S∗  
2,1
2,1
2 ,1
2
2
√
d
+

bs

T

En [∥{x∗i δ̃t }NT ∥22 ]1/2
′

λD





1 



≤ δ̃·,S∗  + γ·,∗S∗ − γ·,∗S∗ + δ̃·,S∗ 
2,1
2,1
2
+

bds





En [∥{xi δ̃t }NT ∥ ]

λD




1 

= δ̃·,S∗ 
2

2,1





+ δ̃·,S∗ 

2,1

+

2 ,1





≤ 3 δ̃·,S∗ 

2 ,1

+

2bds

′

(B.8)

B.2. Proof of Theorem 5
∗
Define δ̃·,· = γ̃·,· − γ·,·
. By the optimality of δ̃·,· , we have







M (γ·,·∗ + δ̃·,· ) + λD γ·,·∗ + δ̃·,· 

2 ,1

 
≤ M(γ·,·∗ ) + λD γ·,·∗ 2,1 .



En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̃t
′



′

t ∈NT

bds

√
T

λD

En [∥{x∗i δ̃t }NT ∥22 ]1/2 .
′

T


 
 


≤ λD γ·,·∗ 2,1 − γ·,·∗ + δ̃·,· 
2,1
 
′
∗′ ∗
−
En (p̂t ({xi γt }NT ) − dti )x∗i δ̃t .

(B.9)

t ∈NT

√

λD

En [∥{x∗i δ̃t }NT ∥22 ]1/2 .



Therefore with probability at least 1 − P



 c 
δ̃·,S∗ 

λD

M (γ·,·∗ + δ̃·,· ) − M (γ·,·∗ ) −

2 1/2
2

∗′

T

Rearranging and subtracting the score, we have

√

T

2,1

≤2

En [∥{x∗i δ̃t }NT ∥22 ]1/2 .
′

(B.4)

Consider two cases based on the upper bound in (B.4). First, suppose that δ̃·,· obeys the cone constraint of Eq. (14) in the definition

The proof proceeds by deriving a further upper bound to the right
and a quadratic lower bound of the left. The combination of these
will yield a bound on En [(x∗i ′ δ̃t )2 ]1/2 .
Begin with the right side of
 Eq. (B.9).

For
 the penalized

difference of coefficients we have γ·,∗S c 





∗

2,1



− γ·,∗S c + δ̃·,S∗c 
∗

2 ,1

=

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23



 c 
δ̃·,S∗  , because γ·,∗S c = 0. Therefore,
2,1

for any r ̸= s ̸= t:

∗

∂ H (w)t ,t /∂wt = p̂t (w)(1 − p̂t (w))(1 − 2p̂t (w))



 ∗ 
γ  − γ ∗ + δ̃·,· 
·,· 2,1
·,·
2,1




 ∗ 
 ∗









= γ·,S∗ 2,1 − γ·,S∗ + δ̃·,S∗  − δ̃·,S∗c 
2,1
2 ,1


 ∗ 
 ∗







≤ γ·,S∗ 2,1 − γ·,S∗ + δ̃·,S∗ 
2,1


 
 ∗ 


≤ γ·,S∗ 2,1 − γ·,∗S∗ + δ̃·,S∗  
2 ,1





 ∗



∗
≤ γ·,S∗ − γ·,S∗ + δ̃·,S∗  = δ̃·,S∗  ,
2 ,1

= H (w)t ,t (1 − 2p̂t (w))
∂ H (w)t ,t /∂wr = −p̂t (w)p̂r (w)(1 − p̂t (w)) + p̂t (w)2 p̂r (w)
= H (w)t ,t (p̂t (w)p̂r (w)(1 − p̂t (w))−1 − p̂r (w))
∂ H (w)t ,s /∂wt = −p̂t (w)p̂s (w)(1 − 2p̂t (w))
= H (w)t ,s (1 − 2p̂t (w))
∂ H (w)t ,s /∂wr = −p̂t (w)p̂s (w)(−2p̂r (w))
= H (w)t ,s (−2p̂r (w)).

2,1

where the first inequality reflects dropping the nonpositive final
term (the norm is nonnegative) and the third inequality follows
from the triangle inequality. Using this result for the first term and
the bound (B.3) for the second, the right side of Eq. (B.9) is bounded
by



√
λD  


′
λD δ̃·,S∗  +
δ̃·,·  + bds T En [∥{x∗i δ̃t }NT ∥22 ]1/2
2,1
2 ,1
2
 √
 √
√ 
√ 
|S∗ | 2bds T
λD 5 |S∗ | 10bds T
≤ λD
∨
+
∨
κD
λD
2
κD
λD

√
+ bds T En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2
√

√
λD |S∗ |
+ 8bds T En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 ,
≤ 6
κD


(B.10)

where the second inequality applies Lemma B.3 and the third
bounds the maximum by the sum.
Now turn to the left side of Eq. (B.9). Our goal is to show that
this is bounded below by a quadratic function. We apply
the bounds for Bach’s (2010) modified self-concordant functions. To show that M (·) belongs to this class, we must
bound the third derivative in terms

 Recall
of the Hessian.
that p̂t ({x∗i ′ γt }NT ) = exp{x∗i ′ γt }/ 1 + NT exp{x∗i ′ γt } and the

T -square matrix H ({xi γt }NT ) has (t , t ) ∈ NT entry given by
∗′

H ({x∗i γt }NT )[t ,t ′ ] =
′



′

2

p̂t ({x∗i γt }NT )(1 − p̂t ({x∗i γt }NT ))

if t = t ′

−p̂t ({x∗i γt }NT )p̂t ′ ({x∗i γt }NT )

if t ̸= t ′ .

′

′

′

′

First, note that M (γ·,· ) can be written as

 


′
′
M(γ·,· ) = En log 1 +
exp{x∗i γt } −
dti (x∗i γt ) .




t ∈NT

Each derivative returns the same Hessian element multiplied by
term bounded by 2 in absolute value. Let ar represent this factor.
Then we bound




  ∂v ′ H (w̃)v 



g (α) = 
vr


r ∈N
∂w
r
w̃=w+αv 
T






′
=
vr v H (w + αv)v ar 
r ∈N

T


≤
v ′ H (w + αv)v|vr ||ar | ≤ 2v ′ H (w + αv)v
|vr |
′′′

r ∈NT


Define F : RT → R as F (w) = log 1 + t ∈NT exp(wt ) , so



that M (γ·,· ) = En F (wi ) − t ∈NT dti wi,t , where wi,t = x∗i ′ γt
and wi = {wi,t }NT . Then for any w ∈ RT , v ∈ RT , and scalar
α , define g (α) = F (w + αv) : R → R. We verify the conditions of Bach (2010, Lemma 1) for this g (α) and F (w). This involves finding the third derivative of g (α), and bounding it in


= 2∥v∥1 g (α) ≤ 2 T ∥v∥2 g (α).

g (α) = v F (w + αv) =



vt p̂t (w + αv) and

t ∈NT

g (α) = v F (w + αv)v = v H (w + αv)v.
′′

′ ′′

′

To bound g ′′′ (α), we again use the derivatives of p̂t ({x∗i ′ γt }NT ) to
find the derivatives of elements H (w). Routine calculations give,

′′

Applying Bach’s (2010) Lemma 1 to each observation, as in Belloni
et al. (2013), with wi = {x∗i ′ γt∗ }NT and vi = {x∗i ′ δ̃t }NT we get the
lower bound
∗
M (γ·,·
+ δ̃·,· ) − M(γ·,·∗ ) −



En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̃t
′



′

t ∈NT


v ′ H ({x∗i ′ γt }NT )vi  −2∥vi ∥2
e
+ 2∥vi ∥2 − 1
≥ En i
2
4T ∥vi ∥2


 ′
vi H ({x∗i ′ γt }NT )vi
4
3
2
2∥vi ∥2 − ∥vi ∥2
,
≥ En
3
4T ∥vi ∥22




(B.11)

where the second inequality follows from Belloni et al. (2013,
Lemma 9).
Tanabe and Sagae (1992, Theorem 1) give H ({x∗i ′ γt∗ }NT ) ≥
φmin {H ({x∗i ′ γt∗ }NT )}IT , in the positive definite sense, where
φmin (A) denotes the smallest eigenvalue of A and IT is the T × T
identity matrix. Then

φmin {H ({x∗i γt∗ }NT )} ≥ det{H ({x∗i ′ γt }NT )}


 T
′
p̂t ({x∗i γt∗ }NT ) ≥ pmin Ap ,
=
t ∈NT

where p0 ({xi γt }NT ) = 1 − t ∈NT p̂t ({x∗i ′ γt∗ }NT ) and the first inequality is also due to Tanabe and Sagae (1992). These results imply that vi′ H ({x∗i ′ γt }NT )vi ≥ (pmin /Ap )T vi′ IT vi = (pmin /Ap )T ∥vi ∥22
and therefore
∗′

∗





vi′ H ({x∗i ′ γt }NT )vi
4
2
3
2
∥v
∥
−
∥v
∥
i 2
i 2
3
4T ∥vi ∥22



 T 1
4
≥ pmin Ap
En 2∥vi ∥22 − ∥vi ∥32


En

terms of the second (i.e. the Hessian). To this end, note that the
multinomial function has the property that ∂ p̂t ({x∗i ′ γt }NT )/∂γt =
p̂t ({x∗i ′ γt }NT )(1 − p̂t ({x∗i ′ γt }NT ))x∗i and ∂ p̂t ({x∗i ′ γt }NT )/∂γt ′ ,· =
−p̂t ({x∗i ′ γt }NT )p̂t ′ ({x∗i ′ γt }NT )x∗i . From these, we find
′ ′

r ∈NT

√

′′

t ∈NT



′

19

4T



3

 T 1 En [∥vi ∥22 ]

= pmin Ap

T

2


1−

2 En [∥vi ∥32 ]
3 En [∥vi ∥22 ]



.

(B.12)

Recall that vi = {x∗i ′ δ̃t }NT . To prove a quadratic lower bound,
consider two cases, depending on whether
1
2


1−

2 En [∥{x∗i ′ δ̃t }NT ∥32 ]
3 En [∥{x∗i ′ δ̃t }NT ∥22 ]



20

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

is above or below 1/AK . In the first case, combining equations
(B.11) and (B.12) gives



M(γ·,·∗ + δ̃·,· ) − M(γ·,·∗ ) −

En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̃t
′

′



t ∈NT

(B.13)

AK

Now consider the second case, where this bound does not hold.
By Assumption 2(b), the Cauchy–Schwarz inequality, and the
conclusion of Lemma B.3

 

 
  
√
 

 
∥{x∗i ′ δ̃t }NT ∥1 =
x∗i,j δ̃t ,j  ≤ X δ̃·,·  ≤ T X δ̃·,· 

2 ,1

1

t ∈NT j∈Np

 √
√ 
√
5 |S∗ |
10bds T
′
∨
En [∥{x∗i δ̃t }NT ∥22 ]1/2 .
≤ TX
κD
λD
Hence, by subadditivity (to bound the ℓ2 norm by the ℓ1 norm),

En [∥{x∗i δ̃t }NT ∥32 ] ≤ En [∥{x∗i δ̃t }NT ∥22 ∥{x∗i δ̃t }NT ∥1 ]
′

′

′

 √
√ 
√
5 |S∗ |
10bds T
∗′
2 3/2
≤ En [∥{xi δ̃t }NT ∥2 ]
TX
∨
.
κD
λD

1

>

AK


1−

2

2 En [∥{x∗i ′ δ̃t }NT ∥32 ]



3 En [∥{x∗i ′ δ̃t }NT ∥22 ]


√ 
1
2X T 
≥
1−
5λD |S∗ | + 10κD bds T
2
3 κD λD

2 1/2
2

× En [∥{xi δ̃t }NT ∥ ]
∗′



AK

,


√ −1
3 κD λD 
:= rn .
5λD |S∗ | + 10κD bds T
√
2X T

∗
Because M (γ·,·
+ δ·,· ) − M(γ·,· ) −



En (p̂t ({x∗i ′ γt∗ }NT ) − dti )



t ∈NT

x∗i ′ δt is convex in δ·,· , and hence any line segment lies above the
function, we know that En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 > rn , so we have



M(γ·,·∗ + δ̃·,· ) − M(γ·,· ) −



En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̃t



′

′

t ∈NT

En [∥{xi δ̃t }NT ∥22 ]1/2
∗′

≥ rn2 ≥ rn2

rn

= rn En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 .
Combining this result with Eqs. (B.9) and (B.10), we have


1−

2
AK



3 κD λD

√

2X T

max En [(x∗i δ̃t )2 ]1/2 ≤ En [∥{x∗i δ̃t }NT ∥22 ]1/2
′

′

t ∈NT

√


√
 
T
λD |S∗ |
≤ Ap pmin T AK 6
+ 8bds T .
κD

(B.14)

To bound the propensity score error, we apply the mean value
theorem and the form of ∂ p̂t ({x∗i ′ γt }NT )/∂γt . We must linearize
with respect to t only (recall that p̂t ({x∗i ′ γ̃t }NT ) depends on all of
γ̃·,· ). To this end, define Mt as the T -vector with entry t given by
x∗i ′ γt∗ + m̃t x∗i ′ γ̃t for a scalar m̃t ∈ [0, 1] and entries t ′ ∈ NT \ {t }
equal to x∗i ′ γt ′ . Then we have

≤ En [(p̂t ({x∗i ′ γ̃t }NT ) − p̂t ({x∗i ′ γt∗ }NT ))2 ]1/2

′

2

Thus, dividing through and applying the union bound we find that

′

En [∥{x∗i δ̃t }NT ∥22 ]1/2

> 1−

AK

En [(p̂t ({x∗i γ̃t }NT ) − pt (xi ))2 ]1/2

which is equivalent to



T

√

√
λD |S∗ |
≤ 6
+ 8bds T En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 .
κD

Using this result coupled with the triangle inequality, the bias
condition, and Eq. (B.14), we find

√



 T 1 En [∥{x∗i ′ δ̃t }NT ∥22 ]

pmin Ap





p̂t ({x∗ ′ γ̃t }N ) − p̂t ({x∗ ′ γ ∗ }N ) = p̂t (Mt )[1 − p̂t (Mt )]x∗ ′ δ̃t 
T
T
i
i
t
i




(B.15)
≤ x∗i ′ δ̃t  .

Thus
1






 T 1 En [∥{x∗i ′ δ̃t }NT ∥22 ]
≥ pmin Ap
.
T

we find that


√  −1
5λD |S∗ | + 10κD bds T



× En [∥{x∗i ′ δt }NT ∥22 ]1/2
√


√
λD |S∗ |
≤ 6
+ 8bds T En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 ,
κD
which is impossible under the restriction on AK . Therefore,
Eq. (B.13) must hold.23 Combining this with Eqs. (B.9) and (B.10),

23 This analysis is conceptually similar to using Belloni and Chernozhukov’s (2011)
restricted nonlinearity impact coefficient, but our characterization is different.

+ En [(p̂t ({x∗i ′ γt∗ }NT ) − pt (xi ))2 ]1/2

1/2
≤ En (x∗i ′ δ̃t )2
+ bds
√


√
 
T
λD |S∗ |
d
+ 8bs T + bds .
≤ Ap pmin T AK 6
κD
The ℓ1 bound follows from Eq. (B.14), the Cauchy–Schwarz
inequality, and Eq. (16):






γ̃t − γ ∗  ≤ |S̃ D ∪ S ∗ | γ̃t − γ ∗ 
t 1
t 2 ,p
D

1/2
|S̃ D ∪ SD∗ |
′
En [(x∗i (γ̃t − γt∗ ))2 ]1/2 .
≤
φ{Q , S̃ D ∪ SD∗ }
Finally, we bound the size of the selected set of coefficients.
First, note that optimality of γ̃·,· ensures that |S̃ D | ≤ n. Then, restating the conclusion Lemma B.2 using the notation of the Theorem and the rate result (B.14), then bounding φ by φ we find that

|S̃ D | ≤ |SD∗ |4Ln φ{Q , |S̃ D |}.
The argument now parallels that used by Belloni and Chernozhukov (2013), relying on their result on the sublinearity of
sparse eigenvalues. Let ⌈m⌉ be the ceiling function and note that
⌈m⌉ ≤ 2m. For any m ∈ NDQ , suppose that |S̃ D | > m. Then,

|S̃ D | ≤ |SD∗ |4Ln φ{Q , m(|S̃ D |/m)}


≤ |S̃ D |/m |SD∗ |4Ln φ{Q , m}
≤ (|S̃ D |/m)|SD∗ |8Ln φ{Q , m}.

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

≤ |SD∗ |8Ln φ{Q , m} whence m ̸∈ NDQ .

Rearranging gives m
Minimizing over

NDQ

gives the result.



B.3. Proof of Theorem 6

21

where the first inequality follows from Eq. (B.3) and the same steps
as in (B.15) while the second applies (B.16) with δ̃·,· and mt ≤ 1.24
Collecting the bounds of (B.17) and (B.18), and the definition of
RM gives



M(γ·,·∗ + δ̂·,· ) − M(γ·,·∗ ) −

En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̂t
′

′



t ∈NT

∗
Define δ̂·,· = γ̂·,· − γ·,·
. Many of the arguments parallel those for
Theorem 5. The key differences are that

 a quadratic lower bound

∗
for M (γ·,·
+ δ̂·,· )− M(γ·,·∗ )− t ∈NT En (p̂t ({x∗i ′ γt∗ }NT ) − dti )x∗i ′ δ̂t

may occur, but is not necessary, and δ̂·,· may not belong to the
cone of the restricted eigenvalues, but obeys the sparse eigenvalue
constraints.
∗
We first give a suitable upper bound for M (γ·,·
+ δ̂·,· )− M(γ·,·∗ )−

En (p̂t ({x∗i ′ γt∗ }NT ) − dti )xi δ̂t . By the Cauchy–Schwarz
inequality and the definition of the sparse eigenvalues of Eq. (16),


∗′





t ∈NT

 
 
δ̂·,· 

2 ,1

≤


  


δ̂t2,j
ŜD ∪ SD∗ 


≤


|ŜD ∪ SD∗ |

λD

√
d

2 φ{Q , ŜD ∪ S ∗ }
D



+ bs T 



× En [∥{x∗i ′ δ̂t }NT ∥22 ]1/2 + RM + R2M .

(B.19)

Next, we turn to a lower bound. Consider the same two cases as
in the proof of Theorem 5. In the first case, we have the quadratic
lower bound:



∗
M (γ·,·
+ δ̂·,· ) − M(γ·,·∗ ) −

En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̂t
′

′



t ∈NT


 T En [∥{xi δ̂t }NT ∥22 ]
≥ pmin Ap
.
∗′

t ∈NT j∈Ŝ ∪S ∗
D
D

(B.20)

T AK


  
− 2

∗
≤ ŜD ∪ SD 
φ Q , ŜD ∪ SD∗
δ̂t′ Q δ̂t

In the other case, this bound may not hold. Arguing as in the proof
of Theorem 5, but applying Eq. (B.16), we get


 
− 1


′
= ŜD ∪ SD∗ φ Q , ŜD ∪ SD∗
En [∥{x∗i δ̂t }NT ∥22 ]1/2 .

∥{x∗i ′ δ̂t }NT ∥1

√
≤ T X |ŜD ∪ SD∗ |φ{Q , ŜD ∪ SD∗ }−1 En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2 .

t ∈NT

(B.16)

Therefore, as above, we find
Following identical steps to Eqs. (B.1)–(B.3), but with δ̂·,· in place of
δ̃·,· , and then using the above bound, we have

M(γ·,·∗ + δ̂·,· ) − M(γ·,· ) −

(B.17)

selection estimator M (γ̂·,· ) ≤ M (γ̃·,· ), as S̃ ⊂ ŜD by construction,




+ δ̃·,· ) −

 
′
′
′
=
En (dti − p̂t ({x∗i γt∗ + mt x∗i δ̃t }))x∗i δ̃t

=

≤

En (

− p̂t ({xi γt }NT ))xi δ̃t
∗′

 λD

2 φ{Q , ŜD ∪ S ∗ }
D

+



′

′

′

λD

|ŜD ∪ SD∗ |

2 φ{Q , ŜD ∪ S }
D
∗

+ En [∥{x∗i ′ δ̃t }NT ∥22 ],



′

Canceling the rn and solving yields En [∥{x∗i ′ δ̂t }NT ∥22 ]1/2 ≤ RM . On
the other hand, if the quadratic term is the minimum, define
R′M = Ap pmin

 


×

t ∈NT

≤

(B.22)



≤ (rn /3) En [∥{x∗i ′ δ̂t }NT ∥22 ]1/2 + 2RM .



√
λD  
′
≤
δ̃·,·  + bds T En [∥{x∗i δ̃t }NT ∥22 ]1/2
2,1
2

 
′
+
En mt (x∗i δ̃t )2 .


+ bs T 



′

t ∈NT





rn En [∥{x∗i δ̂t }NT ∥22 ]1/2 ≤ (rn /3) En [∥{x∗i δ̂t }NT ∥22 ]1/2 + RM + R2M



En (p̂t ({x∗i γt∗ }NT ) − p̂t ({x∗i γt∗ + mt x∗i δ̃t }))x∗i δ̃t ,
′

√
d



∧ rn En [∥{x∗i ′ δ̂t }NT ∥22 ]1/2



× En [∥{x∗i ′ δ̂t }NT ∥22 ]1/2 + RM + R2M .

t ∈NT



AK

T


|ŜD ∪ SD∗ |





Suppose the linear term is the minimum. The restrictions on AK
imply, algebraically, that Eq. (B.22) yields

t ∈NT

∗

 T 1 En [∥{x∗i ′ δ̂t }NT ∥22 ]

pmin Ap

M(γ·,·∗ )

∗′

(B.21)

Collecting the upper bound of (B.19) and the lower bounds (B.20)
and (B.21) we have

∗
and hence M (γ·,·
+ δ̂·,· )− M(γ·,·∗ ) ≤ M(γ̃·,· )− M(γ·,·∗ ). By the mean
value theorem, for scalars {mt ∈ [0, 1]}NT we have

dti

′

≥ rn En [∥{xi δ̂t }NT ∥ ] ,


φ{Q , ŜD ∪ SD∗ }
2
3
1−
.
with rn =
√ 
2
AK
X T |ŜD ∪ SD∗ |

D



′



t ∈NT

∗
Next we turn to M (γ·,·
+ δ̂·,· )− M(γ·,·∗ ). By optimality of the post



En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̂t

2 1/2
2

∗′



 




′
′
En (p̂t ({x∗i γt∗ }NT ) − dti )x∗i δ̂t 

t ∈N

T






√
λD  
′
≤
δ̂·,·  + bds T En [∥{x∗i δ̂t }NT ∥22 ]1/2
2,1
2



∗
|
Ŝ
∪
S
|
√
D
λ
D
D
+ bds T  En [∥{x∗i ′ δ̂t }NT ∥22 ]1/2 .
≤
2 φ{Q , ŜD ∪ S ∗ }
D

M(γ·,·∗



−1

2



T

T AK

λD |ŜD ∪ SD |/φ{Q , ŜD ∪ SD } +
∗

∗

bds

√
T



.



√
+ bds T  En [∥{x∗i ′ δ̃t }NT ∥22 ]1/2
24 Applying the steps of Eq. (B.16) to δ̃ is preferred to using the results of
·,·

(B.18)

Lemma B.3 because it leads to the tidier expression involving φ{Q , ŜD ∪ SD∗ }, but
the latter method could be substituted.

22

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23

With this notation and the quadratic term being the minimum,
Eq. (B.22) becomes

En [∥{x∗i δ̂t }NT ∥22 ]
′

T
 
≤ R′M En [∥{x∗i ′ δ̂t }NT ∥22 ]1 /2 + R′M RM + Ap pmin T AK R2M .
√
Then, because a2 ≤ ab + c implies that a ≤ b + c, we have

1/2
 
T
′
En [∥{x∗i δ̂t }NT ∥22 ]1/2 ≤ R′M + R′M RM + Ap pmin T AK R2M
.
Combining the bounds on En [∥{x∗i ′ δ̂t }NT ∥22 ]1/2 from the two cases
gives

En [∥{x∗i δ̂t }NT ∥22 ]1/2
′



1/2 
 
T
≤ {RM } ∨ R′M + R′M RM + Ap pmin T AK R2M
.
From this bound on the log-odds, we bound the propensity
score and the ℓ1 rate:
max En [(p̂t ({x∗i γ̂t }NT ) − pt (xi ))2 ]1/2
′

t ∈NT



1/2 
 
T
≤ {RM } ∨ R′M + R′M RM + Ap pmin T AK R2M
+ bds ;



max γ̂t − γt∗ 1 ≤
t ∈NT

|S̃ D ∪ SD∗ |

1/2

{R M }
φ{Q , S̃ D ∪ SD∗ }

1/2 

 
T
2
′
′
,
∨ RM + RM RM + Ap pmin T AK RM

by arguments parallel to those used in the proof of Theorem 5.



Appendix C. Proofs for group lasso selection and estimation of
linear models
See supplemental Appendix.
Appendix D. Supplementary data
Supplementary material related to this article can be found
online at http://dx.doi.org/10.1016/j.jeconom.2015.06.017.
References
Abadie, A., 2005. Semiparametric difference-in-differences estimators. Rev.
Econom. Stud. 72, 1–19.
Abadie, A., Imbens, G.W., 2006. Large sample properties of matching estimators for
average treatment effects. Econometrica 74, 235–267.
Andrews, D.W. K., Guggenberger, P., 2009. Incorrect asymptotic size of subsampling
procedures based on post-consistent model selection estimators. J. Econometrics 152, 19–27.
Bach, F.R., 2008. Consistency of the group lasso and multiple kernel learning.
J. Mach. Learn. Res. 9, 1179–1225.
Bach, F.R., 2010. Self-concordant analysis for logistic regression. Electron. J. Stat. 4,
384–414.
Bang, H., Robins, J.M., 2005. Doubly robust estimation in missing data and causal
inference models. Biometrics 61, 962–972.
Belloni, A., Chen, D., Chernozhukov, V., Hansen, C., 2012. Sparse models and
methods for optimal instruments with an application to eminent domain.
Econometrica 80, 2369–2429.
Belloni, A., Chernozhukov, V., 2011. ℓ1 -Penalized quantile regression in highdimensional sparse models. Ann. Statist. 39, 82–130.
Belloni, A., Chernozhukov, V., 2013. Least squares after model selection in highdimensional sparse models. Bernoulli 19, 521–547.
Belloni, A., Chernozhukov, V., Chetverikov, D., Kato, K., 2015. Some new asymptotic
theory for least squares series: Pointwise and uniform results. J. Econometrics
186, 345–366.
Belloni, A., Chernozhukov, V., Fernandez-Val, I., Hansen, C., 2014, Program
Evaluation with High-Dimensional Data. Arxiv preprint arXiv:1311:2645.
Belloni, A., Chernozhukov, V., Hansen, C., 2014. Inference on treatment effects after
selection amongst high-dimensional controls. Rev. Econom. Stud. 81, 608–650.

Belloni, A., Chernozhukov, V., Wei, Y., 2013, Honest Confidence Regions for Logistic
Regression with a Large Number of Controls. arXiv:1304.3969.
Berk, R., Brown, L., Buja, A., Zhang, K., Zhao, L., 2013. Valid post-selection inference.
Ann. Statist. 4, 802–837.
Bickel, P.J., Ritov, Y., Tsybakov, A.B., 2009. Simultaneous analysis of LASSO and
dantzig selector. Ann. Statist. 37, 1705–1732.
Buhlmann, P., van de Geer, S., 2011. Statistics for High-Dimensional Data.
In: Springer Series in Statistics, Springer-Verlag, Berlin.
Cattaneo, M.D., 2010. Efficient semiparametric estimation of multi-valued treatment effects under ignorability. J. Econometrics 155, 138–154.
Cattaneo, M.D., Crump, R.K., Jansson, M., 2013. Generalized Jackknife estimators of
weighted average derivatives. J. Amer. Statist. Assoc. 108, 1243–1256.
Cattaneo, M.D., Drukker, D.M., Holland, A.D., 2013. Estimation of multivalued
treatment effects under conditional independence. The Stata J. 13, 407–450.
Cattaneo, M.D., Farrell, M.H., 2011. Efficient estimation of the dose response function under ignorability using subclassification on the covariates. In: Drukker, D.
(Ed.), Advances in Econometrics: Missing Data Methods, vol. 27A. Emerald
Group Publishing Limited, pp. 93–127.
Cattaneo, M.D., Farrell, M.H., 2013. Optimal convergence rates, Bahadur representation, and asymptotic normality of partitioning estimators. J. Econometrics 174,
127–143.
Cattaneo, M.D., Jansson, M., Newey, W.K., 2014a, Alternative Asymptotics and the
Partially Linear Model with Many Regressors. Working Paper.
Cattaneo, M.D., Jansson, M., Newey, W.K., 2014b. Small bandwidth asymptotics for
density-weighted average derivatives. Econometric Theory 30, 176–200.
Chen, X., 2007. Large sample sieve estimation of semi-nonparametric models.
In: Heckman, J., Leamer, E. (Eds.), Handbook of Econometrics, vol. 6B. Elsevier
(Chapter 76).
Chen, X., Christensen, T.M., 2015. Optimal uniform convergence rates and
asymptotic normality for series estimators under weak dependence and weak
conditions. J. Econometrics (forthcoming).
Chen, X., Hong, H., Tarozzi, A., 2004, Semiparametric Efficiency in GMM Models of
Nonclassical Measurament Errors, Missing Data and Treatment Effects. Cowles
Foundation Discussion Paper No. 1644.
Chen, X., Hong, H., Tarozzi, A., 2008. Semiparametric efficiency in GMM models with
auxiliary data. Ann. Statist. 36, 808–843.
de la Peña, V.H., Lai, T.L., Shao, Q.-M., 2009. Self-Normalized Processes: Limit Theory
and Statistical Applications, Probability and Its Applications, Springer.
Dehejia, R.H., Wahba, S., 1999. Causal effects in nonexperimental studies:
Reevaluating the evaluation of training programs. J. Amer. Statist. Assoc. 94,
1053–1062.
Dehejia, R.H., Wahba, S., 2002. Propensity score-matching methods for nonexperimental causal studies. Rev. Econ. Stat. 84, 151–161.
Efron, B., 2014. Estimation and accuracy after model selection. J. Amer. Statist.
Assoc. 109, 991–1007.
Hahn, J., 1998. On the role of the propensity score in efficient semiparametric
estimation of average treatment effects. Econometrica 66, 315–331.
Hahn, J., 2004. Functional restriction and efficiency in causal inference. Rev. Econ.
Stat. 84, 73–76.
He, X., Shao, Q.-M., 2000. On parameters of increasing dimensions. J. Multivariate
Anal. 73, 1201–1235.
Heckman, J., Ichimura, H., Todd, P., 1997. Matching as an econometric evaluation
estimator: evidence from evaluating a job training programme. Rev. Econom.
Stud. 64, 605–654.
Heckman, J., Vytlacil, E.J., 2007. Econometric evaluation of social programs, Part I.
In: Heckman, J., Leamer, E. (Eds.), Handbook of Econometrics, vol. VIB. Elsevier
Science B.V., pp. 4780–4874.
Hirano, K., Imbens, G.W., Ridder, G., 2003. Efficient estimation of average treatment
effects using the estimated propensity score. Econometrica 71, 1161–1189.
Holland, P.W., 1986. Statistics and causal inference. J. Amer. Statist. Assoc. 81,
945–960.
Horowitz, J.L., Manski, C.F., 2000. Nonparametric analysis of randomized experiments with missing covariate and outcome data. J. Amer. Statist. Assoc. 95,
77–84.
Huang, J.Z., 2003. Local asymptotics for polynomial spline regression. Ann. Statist.
31, 1600–1635.
Huang, J., Zhang, T., 2010. The benefit of group sparsity. Ann. Statist. 38, 1978–2004.
Imai, K., van Dyk, D.A., 2004. Causal inference with general treatment regimes:
generalizing the propensity score. J. Amer. Statist. Assoc. 99, 854–866.
Imbens, G.W., 2000. The role of the propensity score in estimating dose–response
functions. Biometrika 87, 706–710.
Imbens, G.W., 2004. Nonparametric estimation of average treatment effects under
exogeneity: A review. Rev. Econ. Stat. 86, 4–29.
Imbens, G.W., Newey, W.K., Ridder, G., 2007, Mean-Squared-Error Calculations for
Average Treatment Effects. Working Paper.
Imbens, G.W., Wooldridge, J.M., 2009. Recent developments in the econometrics of
program evaluation. J. Econ. Lit. 47, 5–86.
Kang, J.D. Y., Schafer, J.L., 2007. Demystifying double robustness: a comparison of
alternative strategies for estimating a population mean from incomplete data.
Statist. Sci. 22, 523–539.
Kolar, M., Lafferty, J., Wasserman, L., 2011. Union support recovery in multi-task
learning. J. Mach. Learn. Res. 12, 2415–2435.
Kwemou, M., 2012, Non-asymptotic Oracle Inequalities for the Lasso and Group
Lasso in High Dimensional Logistic Model. Arxiv preprint arXiv:1206.0710.
LaLonde, R.J., 1986. Evaluating the econometric evaluations of training programs
with experimental data. Am. Econ. Rev. 76, 604–620.

M.H. Farrell / Journal of Econometrics 189 (2015) 1–23
Lechner, M., 2001. Identification and estimation of causal effects of multiple
treatments under the conditional independence assumption. In: Lechner, M.,
Pfeiffer, E. (Eds.), Econometric Evaluations of Active Labor Market Policies.
Physica. Heidelberg, pp. 43–58.
Leeb, H., Pötscher, B.M., 2005. Model selection and inference: facts and fiction.
Econometric Theory 21, 21–59.
Leeb, H., Pötscher, B.M., 2008a. Can one estimate the unconditional distribution of
post-model-selection estimators? Econometric Theory 24, 338–376.
Leeb, H., Pötscher, B.M., 2008b. Sparse estimators and the oracle property, or the
return of Hodges’ estimator. J. Econometrics 142, 201–211.
Lounici, K., Pontil, M., van de Geer, S., Tsybakov, A.B., 2011. Oracle inequalities and
optimal inference under group sparsity. Ann. Statist. 39, 2164–2204.
Negahban, S.N., Ravikumar, P., Wainwright, M.J., Yu, B., 2012. A unified
framework for high-dimensional analysis of M-estimators with decomposable
regularizers. Statist. Sci. 27, 538–557.
Newey, W.K., 1990. Efficient instrumental variables estimation of nonlinear
models. Econometrica 58, 809–837.
Newey, W.K., 1997. Convergence rates and asymptotic normality for series
estimators. J. Econometrics 79, 147–168.
Newey, W.K., McFadden, D.L., 1994. Large sample estimation and hypothesis
testing. In: Engle, R.F., McFadden, D. (Eds.), Handbook of Econometrics.
In: Handbook of Econometrics, vol. 4. Elsevier, pp. 2111–2245 (Chapter 36).
Obozinski, G., Wainwright, M.J., Jordan, M.I., 2011. Support union recovery in highdimensional multivariate regression. Ann. Statist. 39, 1–47.
Pötscher, B.M., 2009. Confidence sets based on sparse estimators are necessarily
large. Sankhyā 71-A, 1–18.
Pötscher, B.M., Leeb, H., 2009. On the distribution of penalized maximum likelihood
estimators: The LASSO, SCAD, and thresholding. J. Multivariate Anal. 100,
2065–2085.
Powell, J.L., Stock, J.H., Stoker, T.M., 1989. Semiparametric estimation of index
coefficients. Econometrica 57, 1403–1430.
Raskutti, G., Wainwright, M.J., Yu, B., 2010. Restricted eigenvalue properties for
correlated Gaussian designs. J. Mach. Learn. Res. 11, 2241–2259.
Robins, J., Li, L., Tchetgen, E., van der Vaart, A., 2008. Higher order influence
functions and minimax estimation of nonlinear functionals. In: Nolan, D.,
Speed, T. (Eds.), Probability and Statistics: Essays in Honor of David A.
Freedman, vol. 2. Institute of Mathematical Statistics, Beachwood, Ohio, USA.
Robins, J.M., Rotnitzky, A., 1995. Semiparametric efficiency in multivariate
regression models with missing data. J. Amer. Statist. Assoc. 90, 122–129.
Romano, J.P., 2004. On non-parametric testing, the uniform behaviour of the t-test,
and related problems. Scand. J. Stat. 31, 567–584.

23

Rosenbaum, P.R., Rubin, D.B., 1983. The central role of the propensity score in
observational studies for causal effects. Biometrika 70, 41–55.
Rudelson, M., Zhou, S., 2013. Reconstruction from anisotropic random measurements. IEEE Trans. Inform. Theory 59, 3434–3447.
Smith, J.A., Todd, P.E., 2005. Does matching overcome LaLonde’s critique of
nonexperimental estimators? J. Econometrics 125, 305–353.
Tan, Z., 2010. Bounded, efficient and doubly robust estimation with inverse
weighting. Biometrika 97, 661–682.
Tanabe, K., Sagae, M., 1992. An exact Cholesky decomposition and the generalized
inverse of the variance–covariance matrix of the multinomial distribution, with
applications. J. R. Stat. Soc. Ser. B Stat. Methodol. 54, 211–219.
Tsiatis, A.A., 2006. Semiparametric Theory and Missing Data. Springer, New York.
van de Geer, S., 2008. High-dimensional generalized linear models and the Lasso.
Ann. Statist. 36, 614–645.
van de Geer, S., Buhlmann, P., 2009. On the conditions used to prove oracle results
for the Lasso. Electron. J. Stat. 3, 1360–1392.
van de Geer, S., Buhlmann, P., Ritov, Y., Dezeure, R., 2014. On asymptotically optimal
confidence regions and tests for high-dimensional models. Ann. Statist. 42,
1166–1202.
van der Laan, M., Robins, J.M., 2003. Unified Methods for Censored Longitudinal
Data and Causality. Springer-Verlag.
Vincent, M., Hansen, N.R., 2014. Sparse group lasso and high dimensional
multinomial classification. Comput. Statist. Data Anal. 71, 771–786.
von Bahr, B., Esseen, C.-G., 1965. Inequalities for the rth absolute moment of a sum
of random variables, 1 5 r 5 2. Ann. Math. Stat. 36, 299–303.
Wei, F., Huang, J., 2010. Consistent group selection in high-dimensional linear
regression. Bernoulli 16, 1369–1384.
White, H., Lu, X., 2011. Causal diagrams for treatment effect estimation with
application to efficient covariate selection. Rev. Econ. Stat. 93, 1453–1459.
Wooldridge, J.M., 2007. Inverse probability weighted estimation for general
missing data problems. J. Econometrics 141, 1281–1301.
Wooldridge, J.M., 2010. Econometric Analysis of Cross Section and Panel Data,
second ed.. MIT Press, Cambridge.
Yuan, M., Lin, Y., 2006. Model selection and estimation in regression with grouped
variables. J. R. Stat. Soc. Ser. B Stat. Methodol. 68, 46–67.
Zhang, C.-H., Zhang, S.S., 2014. Confidence intervals for low dimensional
parameters in high dimensional linear models. J. R. Stat. Soc. Ser. B 76, 217–242.
Zou, H., 2006. The adaptive Lasso and its oracle properties. J. Amer. Statist. Assoc.
101, 1418–1429.

