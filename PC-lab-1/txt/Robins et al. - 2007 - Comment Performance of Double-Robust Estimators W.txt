Statistical Science
2007, Vol. 22, No. 4, 544â€“559
DOI: 10.1214/07-STS227D
Main article DOI: 10.1214/07-STS227
Â© Institute of Mathematical Statistics, 2007

Comment: Performance of Double-Robust
Estimators When â€œInverse Probabilityâ€
Weights Are Highly Variable
James Robins, Mariela Sued, Quanhong Lei-Gomez and Andrea Rotnitzky
all candidate estimators that depend on a linear logistic
maximum likelihood estimate of the propensity score,
including all the DR estimators considered by the authors.
Near the end of their Section 1, the authors state
that their simulation example â€œappears to be precisely
the type of situation for which the DR estimators of
Robins et al. were developed.â€ They then suggest that
their simulation results imply that the cited quotation
from Bang and Robins (2005) is incorrect or, at the
very least, misguided. We disagree with both the authorsâ€™ statement and suggestion. First, the cited quote
neither claims nor implies that when a linear logistic
model for the propensity score and a linear model for
the mean of Y given X are moderately misspecified,
DR estimators always outperform estimatorsâ€”such as
regression, maximum likelihood, or parametric (multiple) imputation estimatorsâ€”that do not depend on the
estimated propensity score. Indeed, Robins and Wang
(2000) in their paper â€œInference for Imputation Estimatorsâ€ stated the following:

1. GENERAL CONSIDERATIONS

We thank the editor Ed George for the opportunity to
discuss the paper by Kang and Schaeffer.
The authorsâ€™ paper provides a review of doublerobust (equivalently, double-protected) estimators of
(i) the mean Î¼ = E(Y ) of a response Y when Y is missing at random (MAR) (but not completely at random)
and of (ii) the average treatment effect in an observational study under the assumption of strong ignorability. In our discussion we will depart from the notation
in Kang and Schaeffer (throughout, K&S) and use capital letters to denote random variables and lowercase
letter to denote their possible values.
In the missing-data setting (i), one observes n i.i.d.
copies of O = (T , X, T Y ), where X is a vector of always observed covariates and T is the indicator that the
response Y is observed. An estimator of Î¼ is doublerobust (throughout, DR) if it remains consistent and
asymptotically normal (throughout, CAN) when either (but not necessarily both) a model for the propensity score Ï€(X) â‰¡ P (T = 1|X) = P (T = 1|X, Y ) or
a model for the conditional mean m(X) â‰¡ E(Y |X) =
E(Y |X, T = 1) is correctly specified, where the equalities follow from the MAR assumption. The authors
demonstrate, via simulation, that when a linear logistic model for the propensity score and a linear model
for the mean of Y given X are both moderately misspecified, there exists a joint distribution under which
the OLS regression estimator Î¼Ì‚OLS of Î¼ outperforms

If nonresponse is ignorable, a locally semiparametric efficient estimator is doubly protected; i.e., it is consistent if either a model
for nonresponse or a parametric model for
the complete data can be correctly specified. On the other hand, consistency of
a parametric multiple imputation estimator
requires correct specification of a parametric model for the complete data. However,
in cases in which the variance of the â€˜inverse probabilityâ€™ weights is very large, the
sampling distribution of a locally semiparametric efficient (augmented inverse probability of response weighted) estimator can
be markedly skew and highly variable, and
a parametric imputation estimator may be
preferred.

James Robins is Professor, Department of Epidemiology,
Harvard School of Public Health, Boston, Massachusetts
02115, USA (e-mail: akaris@hsph.harvard.edu). Mariela
Sued is Assistant Professor, Facultad de Ciencias Exactas y
Naturales, Universidad de Buenos Aires and CONICET,
Argentina. Quanhong Lei-Gomez is a Graduate Student,
Department of Biostatistics, Harvard School of Public
Health, Boston, Massachusetts 02115, USA. Andrea
Rotnitzky is Professor, Department of Economics, Di Tella
University, Buenos Aires, Argentina.

The just-quoted cautionary message of Robins and
Wang (2000) is not far from K&Sâ€™s take-home mes544

COMMENT

sage. In Section 5 we show that, in the authorsâ€™ simulation example, the variance of the estimated â€œinverse
probabilityâ€ weights is very large and the sampling distribution of their candidate DR estimators is skewed
and highly variable. It follows that their example is
far from the settings Bang and Robins had in mind
when recommending the â€œroutine use of DR estimators.â€ Rather, their example falls squarely into the class
for which Robins and Wang (2000) cautioned that a
parametric imputation estimator may be preferable to
DR estimators.
Even prior to Robins and Wang (2000), Robins, Rotnitzky and colleagues had published extensive warnings about, and simulation studies of, the hazards of
highly variable â€œinverse probabilityâ€ weights (Robins,
Rotnitzky and Zhao, 1995, pages 113â€“115; Scharfstein, Rotnitzky and Robins, 1999, pages 1108â€“1113),
although not specifically for DR estimators. Due to the
fact that the problem of highly variable weights was
not the focus of their paper and had already been discussed extensively in earlier papers by Robins and colleagues, Bang and Robins (2005) did not repeat Robins
and Wangâ€™s (2000) cautionary message. In retrospect,
had they done so or had the authors been aware of
the Robins and Wang article, a misunderstanding could
perhaps have been averted.
Whenever the â€œinverse probabilityâ€ weights are
highly variable, as in K&Sâ€™s simulation experiment,
a small subset of the sample will have extremely large
weights relative to the remainder of the sample. In this
setting, no estimator of the marginal mean Î¼ = E(Y )
can be guaranteed to perform well. That is why, in such
settings, some â€œargue that inference about the mean
E(Y ) in the full population should not be attempted,â€
to quote from the authorsâ€™ discussion. Yet, surprisingly, in the authorsâ€™ simulation experiment, the regression estimator Î¼Ì‚OLS performed very well with a mean
squared error (MSE) less than any of their candidate
DR estimators, all of which estimated the propensity
score by maximum likelihood under a linear logistic
model. The explanation is that, whether due to unusual
luck or to â€œcherry-picking,â€ the chosen data-generating
distribution was as if optimized to have Î¼Ì‚OLS perform
well. Indeed, in Section 5, we â€œdeconstructâ€ the chosen distribution and show that it possesses a number
of specific, some rather unusual, features that together
served to insure Î¼Ì‚OLS would perform well even under
K&Sâ€™s misspecified models.
Now, even were the chosen joint distribution of
(Y, T , X) optimized to have Î¼Ì‚OLS perform extremely
well as an estimator of E(Y ) on data (T Y, T , X) in

545

which Y is observed only when T = 1, such optimization would not guarantee that Î¼Ì‚OLS would also perform
well on the data ((1 âˆ’ T )Y, T , X) in which Y is observed only when T = 0. Based on this insight, in Section 5, we repeat K&Sâ€™s simulation study, except based
on data ((1 âˆ’ T )Y, T , X) rather than data (T Y, T , X),
and show that, indeed, Î¼Ì‚OLS is now outperformed by
all candidate DR estimators in terms of both bias and
MSE.
In the analysis of real, as opposed to simulated data,
we do not know a priori whether the features of the
joint distribution of (Y, T , X) do or do not favor Î¼Ì‚OLS .
Furthermore, with highly variable â€œinverse probabilityâ€ weights, we generally cannot learn the answer
from the data, owing to poor power. This suggests
that, with highly variable weights, a single estimator,
whether Î¼Ì‚OLS or a single DR estimator of the mean Î¼ is
never adequate even with MAR data; rather an analyst
should either â€œnot attempt to make inference about the
meanâ€ or else provide a sensitivity analysis (in which
models for both the propensity score and the regression
of Y on X and estimators of Î¼ are varied). In Section 6,
we sketch a possible approach to sensitivity analysis.
In this discussion we ask the following question: can
we find DR estimators that, under the authorsâ€™ chosen joint distribution for (Y, T , X), both perform almost as well as Î¼Ì‚OLS applied to data (T Y, T , X) and
yet perform better than Î¼Ì‚OLS when applied to data
((1 âˆ’ T )Y, T , X). In Section 4 we describe the principles we used to search among the set of possible DR
estimators and discuss the expected performance of
various candidates. We define a general class of DR
estimators, which we refer to as â€œbounded,â€ that contains the DR estimators that perform best in the setting
of highly variable â€œinverse probabilityâ€ weights. We
further subdivide the class of bounded DR estimators
into two subclassesâ€”bounded Horvitzâ€“Thompson DR
estimators and regression DR estimators. We then describe various scenarios which favor one subclass over
the other. We also explain why certain DR estimators
perform particularly poorly in settings with highly variable â€œinverse probabilityâ€ weights. The performance
of our estimators is examined in the simulations reported in Section 5, which both mimics the simulations in S&K and also repeats it but now using data
((1 âˆ’ T )Y, T , X).
A major point emphasized by K&S was that, in
their simulations, the regression estimator Î¼Ì‚OLS outperformed any DR estimator when both their model for
the propensity score and for the regression of Y on X
(from now on referred to as the â€œoutcome modelâ€) were

546

J. ROBINS, M. SUED, Q. LEI-GOMEZ AND A. ROTNITZKY

misspecified. However, they restricted attention to linear logistic propensity score models. In Section 3, we
show that Î¼Ì‚OLS is CAN for Î¼ when either (but not necessarily both) a linear model for the inverse propensity
score, 1/Ï€(X) = X T Î±, or a linear model X T Î² for the
conditional mean E(Y |X) is correctly specified. That
is, by definition, Î¼Ì‚OLS itself is a DR estimator of Î¼
when the inverse linear model Ï€(X) = 1/(XT Î±) for
the propensity score is substituted for K&Sâ€™s linear logistic model!
In K&Sâ€™s simulation experiment, the linear model
X T Î² and the model Ï€(X) = 1/(XT Î±) are both misspecified. Yet, under their scenario, Î¼Ì‚OLS did not â€œoutperform any DR estimator that is CAN when either the
regression model XT Î² or the model Ï€(x) = 1/(x T Î±)
is correctly specified,â€ precisely because Î¼Ì‚OLS is one
such DR estimator! Of course, the model Ï€(X) =
1/(X T Î±) would rarely, if ever, be used in practice as
the model does not naturally constrain Ï€(X) to lie in
[0, 1]. Nonetheless, understanding that Î¼Ì‚OLS is a DR
estimator provides important insight into the meaning
and theory of double-robustness.

Most of the DR estimators of Î¼ discussed by K&S
are of the general form

2. GENERAL FORM OF DOUBLE-ROBUST
ESTIMATORS

where Ï€(x; Î±) is a known function, for example,
Ï€(x; Î±) = {1 + exp(âˆ’x T Î±)}âˆ’1 as in S&K and, (ii) a
working parametric model for m(X) â‰¡ E(Y |X) of the
form

The authors note that many different DR estimators
exist and give a number of explicit examples. The authors restrict attention to MAR data with missing response. In this setting Robins (2000), Robins (2002),
Tan (2006) and van der Laan and Robins (2003) had
previously proposed a rather wide variety of DR estimators, in addition to the DR estimators of Robins
and colleagues considered by the authors. Moreover,
Scharfstein et al. (1999) and van der Laan and Robins
(2003) provided general methods for the construction
of DR estimators in models with MAR or coarsened
at random (CAR) data. Robins and Rotnitzky (2001)
described a general approach to the construction of DR
estimators (when they exist) in a very large model class
that includes all MAR and CAR models as well as certain nonignorable (i.e., non-CAR) missing data models. Recently, van der Laan and Rubin (2006) have developed a general approach called â€œtargeted maximum
likelihoodâ€ that has overlap with methods in Scharfstein et al. (1999), Robins (2000, 2002) and Bang and
Robins (2005) in the setting of missing response data.
We will use the general methods of Robins and Rotnitzky (2001) to find a candidate set of DR estimators
among which we then search for ones that perform in
simulations as well as or better than those discussed by
S&K.



Î¼Ì‚DR (Ï€Ì‚, mÌ‚) = Pn {mÌ‚(X)} + Pn



T
{Y âˆ’ mÌ‚(X)}
Ï€Ì‚(X)

or
(1)

Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚) = Pn {mÌ‚(X)}
+

Pn [T /Ï€Ì‚(X){Y âˆ’ mÌ‚(X)}]
,
Pn {T /(Ï€Ì‚(X))}

where throughout, Pn (A) is a shortcut for nâˆ’1 Â·
i=1 Ai . Robins, Sued, Lei-Gomez and Rotnitzky
(2007) show that these estimators are solutions to
particular augmented inverse probability weighted
(AIPW) estimating equations. The AIPW estimating
equations are obtained by applying the general methods of Robins and Rotnitzky (2001) to the simple
missing-data model considered by K&S.
Quite generally, to construct mÌ‚ and Ï€Ì‚ we specify
(i) a â€œworkingâ€ parametric submodel for the propensity score Ï€(X) â‰¡ Pr(T = 1|X) of the form
n

(2)

(3)

Ï€(Â·) âˆˆ {Ï€(Â·; Î±) : Î±âˆˆRq },

m(Â·) âˆˆ {m(Â·; Î²) : Î²âˆˆRp },

where m(x; Î²) is a known function, for example,
m(x; Î²) = x T Î² as in S&K. We then obtain estimators Î±Ì‚ and Î²Ì‚ which converge at rate n1/2 to some constant vectors Î± âˆ— and Î² âˆ— , which are, respectively, equal
to the true value of Î± and/or Î² when the corresponding working model is correctly specified, and define
mÌ‚(x) â‰¡ m(x; Î²Ì‚) and Ï€Ì‚(x) â‰¡ Ï€(x; Î±Ì‚). [In fact, under
mild additional regularity conditions, the rate n1/2 can
be relaxed to nÎ¶ , Î¶ > 1/4, a fact which is critical when
the dimensions p and q of Î² and Î± are allowed to increase with n as nÏ , Ï < 1/2.]
Under regularity conditions, if either (but not necessarily both) (2) or (3) is correct, Î¼Ì‚DR (Ï€Ì‚, mÌ‚) and
Î¼Ì‚B -DR (Ï€Ì‚, mÌ‚) are consistent and asymptotically normal
(CAN) estimators of Î¼.
In the special case in which (a) Ï€(x; Î±) = {1 +
exp(âˆ’x T Î±)}âˆ’1 and m(x; Î²) = (x T Î²) where  is a
known canonical inverse link function, and (b) Î±Ì‚ is
the MLE of Î± and Î²Ì‚ is the iteratively reweighted least
squares estimator of Î² among respondents (throughout
denoted as Î²Ì‚REG ) satisfying
(4)

Pn [T X{Y âˆ’ (XT Î²Ì‚REG )}] = 0,

547

COMMENT

we shall denote mÌ‚ with mÌ‚REG and the resulting DR estimators as Î¼Ì‚DR (Ï€Ì‚ , mÌ‚REG ) and Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG ). When
 is the identity, Î²Ì‚REG is thus the OLS estimator of Î².
In such case, Î¼Ì‚DR (Ï€Ì‚ , mÌ‚REG ) is the estimator denoted
Î¼Ì‚BC-OLS in S&K and Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG ) is the estimator
in the display following (8) in S&K.
3. Î¼Ì‚OLS AS A DR ESTIMATOR UNDER A LINEAR
INVERSE PROPENSITY MODEL

As anticipated in Section 1, in this section we will
argue that the regression estimator Î¼Ì‚OLS is indeed a
DR estimator with respect to specific working models.
Suppose that we postulate the linear inverse propensity
model, that is, in (2) we take Ï€(x; Î±) = 1/(Î± T x). It
follows from (4) that for any Î± âˆˆ ,


(5)

Pn



T
{Y âˆ’ mÌ‚REG (X)} = 0.
Ï€(X; Î±)

Thus, the regression estimator Î¼Ì‚OLS is indeed equal
to Î¼Ì‚DR (Ï€(Â·; Î±), mÌ‚REG ) for any Î± and therefore DR
with respect to the linear inverse propensity model
Psub,inv and the outcome model (x T Î²) = x T Î².
To estimate Î± in model Psub,inv we may use either the estimator Î±Ì‚inv or the estimator Î±Ì‚Ë† inv that, respectively, minimize the log-likelihood Pn [T log{Ï€(X;
Î±)} + {1 âˆ’ T } log{1 âˆ’ Ï€(X; Î±)}] or squared norm
T
âˆ’ 1}X]2 both subject to the constraints
Pn [{ Ï€(X;Î±)
Ï€(Xi ; Î±) â‰¥ 0, i = 1, . . . , N. Under regularity conditions, Î±Ì‚inv and Î±Ì‚Ë† inv converge in probability to quanâˆ— and Î± âˆ—âˆ— with the property that when model
tities Î±inv
inv
âˆ— ) and Ï€(X; Î± âˆ—âˆ— )
Psub,inv is correctly specified, Ï€(X; Î±inv
inv
are equal to the propensity score P (T = 1|X).
4. DOUBLE-ROBUST ESTIMATORS WITH
DESIRABLE PROPERTIES
4.1 Boundedness

We would like to have DR estimators of Î¼ = E(Y )
with the â€œboundednessâ€ property that, when the sample space of Y is finite, they fall in the parameter space
for Î¼ with probability 1. Neither Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG ) nor
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG ) has this property. We consider two
separate ways to guarantee the â€œboundednessâ€ property.
First, suppose that we found DR estimators that
could be written in the IPW form
(6)

Pn {Y T /Ï€Ì‚ (X)}/Pn {T /Ï€Ì‚(X)}

for some nonnegative Ï€Ì‚(Â·). Then the property would
hold for such estimators. Specifically, the quantity in

the last display is a convex combination of the observed Y -values and thus always lies in the interval
[Ymin , Ymax ] with endpoints the minimum and maximum observed Y -values. But, [Ymin , Ymax ] is included
in the parameter space for Î¼ because Î¼ is the population mean of Y .
Note that division by Pn {T /Ï€Ì‚ (X)} is essential to
ensure that (6) is in [Ymin , Ymax ]. In particular, the
Horvitzâ€“Thompson estimator Î¼Ì‚HT = Pn {Y T /Ï€Ì‚ (X)}
does not satisfy this property. For example, if Y is
Bernoulli, (6) lies in [0, 1] but Î¼Ì‚HT may lie outside
[0, 1]. For instance, this will be the case if in the sample
there exists a unit with T = 1, Ï€Ì‚(X) < 1/n and Y = 1
since then Î¼Ì‚HT will be greater than 1. Indeed, when
we carried out 1000 Monte Carlo replications of Kang
and Schaefferâ€™s simulation experiment for sample size
n = 1000 using their misspecified analysis model for
Ï€(x), we found in one particularly anomalous replication, a simulated unit with T = 1 but with the unusually
small estimated propensity Ï€Ì‚(X) < 1/17,000. Thus,
had we simulated Y from a Bernoulli rather than from
a normal distribution, we would have had Î¼Ì‚HT > 17 for
this anomalous replication!
The desire that an estimator falls in the interval
[Ymin , Ymax ] is in conflict with the desire that it be
unbiased, as we now show. Suppose the propensity
score function Ï€(x) were known. The set of exactly
unbiased estimators of Î¼ (that are invariant to permutations of the index i labeling the units) are contained in the set {Pn [T {Y âˆ’ q(X)}/Ï€(X) + q(X)]}
as q(X) varies. It follows that no unbiased estimator
of Î¼ exists for Y Bernoulli that is guaranteed to fall
in [Ymin , Ymax ]. Taking q(X) identically zero, we obHT = Pn {Y T /Ï€(X)}. Suppose that in an actual
tain Î¼
study of 1000 subjects, a rare fluctuation had occurred
and there was a subject with T = 1 whose propensity
HT > 17. We
score Ï€(X) was less than 1/17,000 so Î¼
doubt any scientist could be convinced to publish the
HT for the mean of a
logically impossible estimate Î¼
Bernoulli Y , with the argument that only then would
his estimator of the mean be exactly unbiased over hypothetical repetitions of the study that, of course, neither have occurred nor will occur. Exactly analogous
difficulties arise for any other choice of q(X). With
highly variable weights, â€œboundednessâ€ trumps unbiasedness.
Second, the boundedness property also holds for DR
estimators that can be written in the regression form
(7)

Pn {mÌ‚(X)}

with mÌ‚(x) = (x T Î²Ì‚ + h(x)T Î³Ì‚ ) for some specified
function h(Â·) and an inverse link function  satisfying

548

J. ROBINS, M. SUED, Q. LEI-GOMEZ AND A. ROTNITZKY

inf Y â‰¤ (u) â‰¤ sup Y for all u, where Y is the sample
space of Y. This follows because (i) Pn {mÌ‚(X)} falls
in the interval [mÌ‚min , mÌ‚max ], with mÌ‚min and mÌ‚max the
minimum and maximum values of mÌ‚(X) among the n
sample units and, (ii) the above choice of (Â·) guarantees that [mÌ‚min , mÌ‚max ] is contained in the parameter
space for Î¼.
Neither the estimator Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG ) nor the estimator Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG ) of Section 2 satisfies the â€œboundednessâ€ property. Note, however, that Î¼Ì‚B -DR (Ï€Ì‚, mÌ‚)
satisfies |Î¼Ì‚B -DR (Ï€Ì‚, mÌ‚)| < maxi=1,...,n |Yi âˆ’ mÌ‚(Xi )| +
maxi=1,...,n |mÌ‚(Xi )|. Thus when Y is Bernoulli and
m(X) = (XT Î²) for  any inverse link with range in
[0, 1], we have that |Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚)| < 2, so it is within
a factor of 2 of lying in the parameter space. In contrast, when Y is Bernoulli, Î¼Ì‚DR (Ï€Ì‚, mÌ‚), like Î¼Ì‚HT , can
be extremely large.
In the next sections, we describe general approaches
to constructing DR estimators that can be written in the
form (6) or the form (7). Thus it is important to determine whether DR estimators that satisfy (6) perform
better or worse than those satisfying (7) when models for both m(Â·) and Ï€(Â·) are wrong. Unfortunately,
no general recommendation can be given because the
answer will depend on the specific data generating
process and models used to estimate m(Â·) and Ï€(Â·). For
example, suppose that as in Kang and Schaefferâ€™s simulation experiment, Y is continuous, var(Y |X) = Ïƒ 2
does not depend on X,  is the identity link and we
estimate the model XT Î² for m(X) = E(Y |X) by OLS.
When (i) Ïƒ 2 / Var(Y ) is near zero and (ii) there exist
a number of nonrespondent units j (i.e., units j with
Tj = 0) whose values xj of X lie far outside the convex hull of the set of values of X for the subsample
of respondents, then yj and m(xj ) will be close to
one another but not to mÌ‚REG (xj ) except if, by luck,
the model E(Y |X) = XT Î² is so close to being correct that the fit of the model to the subsample of respondents allows successful linear extrapolation to Xâ€™s
far from those fitted. As we shall see, it is precisely
such â€œluckâ€ that explains the good performance of
Î¼Ì‚OLS in the authorsâ€™ simulation experiment. Without
such luck, the estimator Pn {mÌ‚REG (X)} may perform
poorly compared to Pn {Y T /Ï€Ì‚ (X)}/Pn {T /Ï€Ì‚(X)}, owing to unsuccessful linear extrapolation. On the other
hand, when Ïƒ 2 / Var(Y ) is close to 1 and very few
units with T = 0 have values of X far outside the convex hull of the set of Xâ€™s in the respondents subsample, Pn {mÌ‚REG (X)} will generally perform better than
Pn {Y T /Ï€Ì‚ (X)}/Pn {T /Ï€Ì‚ (X)},as the latter may be dominated by the large weights 1/Ï€Ì‚ (X) assigned to respon-

dents who have both very small values of Ï€Ì‚(X) and
large residuals Y âˆ’ m(X).
4.1.1 Regression double-robust estimators. We refer to DR estimators satisfying (7) as regression DR
estimators. They are obtained by replacing mÌ‚REG in
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG ) with mÌ‚(X) satisfying


(8)

Pn



T
{Y âˆ’ mÌ‚(X)} = 0.
Ï€Ì‚(X)

Here we describe three such estimators, though others
exist (Robins et al., 2007).
The first one, proposed in Scharfstein et al. (1999)
and discussed further in Bang and Robins (2005), is
the estimator in K&Sâ€™s Table 8. To compute this estimator one considers an extended outcome model of
the form (X T Î² + Ï• Ï€Ì‚ (X)âˆ’1 ) adding the covariate
Ï€Ì‚(X)âˆ’1 (i.e., the inverse of the fitted propensity score).
Ï•
) satisfyOne then jointly estimates (Î², Ï•) with (Î²,

âˆ’1
T
âˆ’1
Ï€Ì‚
(X)

Ï€Ì‚(X) )}
ing Pn [T {Y âˆ’ (X Î² + Ï•
] = 0.
X
The first row of this last equation is precisely (8)
with mÌ‚(X) replaced with the fitted regression function
Ï€Ì‚(X)âˆ’1 ). Consequently,
mÌ‚EXT -REG (X) = (XT Î² + Ï•
Î¼Ì‚DR (Ï€Ì‚, mÌ‚EXT -REG ) = Pn {mÌ‚EXT -REG (X)}. This estimator is CAN provided either the model Ï€(x; Î±) for
the propensity score Ï€(x) or the model (x T Î²) for
E(Y |X = x) is correct. In fact, it is CAN even if model
(x T Î²) is incorrect provided the model (X T Î² +
Ï•{Ï€(X; Î± âˆ— )}âˆ’1 ) is correct, where Î± âˆ— is the probability
limit of the estimator Î±Ì‚ of Î±. In particular, as indicated
in the previous subsection, if Y is Bernoulli and  is
the inverse logit link, Î¼Ì‚DR (Ï€Ì‚ , mÌ‚EXT -REG ) is always in
[0, 1].
Nonetheless, when Y is continuous and  is the
identity, |Î¼Ì‚DR (Ï€Ì‚, mÌ‚EXT -REG )| can be disastrously large
when the estimated inverse probability weights Ï€Ì‚
(X)âˆ’1 are highly variable. Specifically, when Ï€Ì‚(X)âˆ’1
is highly variable, it could very well happen that in
most repeated samples the largest value of Ï€Ì‚ âˆ’1 among
nonrespondents is manyfold greater than the largest
value among the respondent subsample. [E.g., a typical Monte Carlo replication of Kang and Schaeffer
under the wrong propensity score model had a largest
Ï€Ì‚(X)âˆ’1 of 80 in the respondent subsample but a largest
Ï€Ì‚(X)âˆ’1 of 1800 in the nonrespondent subsample.] In
such cases, enormously greater extrapolation would be
required with model {XT Î² + Ï• Ï€Ì‚ (X)âˆ’1 } than with
model (XT Î²) to obtain fitted values for Y in the nonrespondent subsample, clearly a problem if the extrapolation model {XT Î² + Ï• Ï€Ì‚ (X)âˆ’1 } is also wrong. This
phenomenon explains the disastrous performance of

549

COMMENT

Î¼Ì‚DR (Ï€Ì‚, mÌ‚EXT -REG ) observed in K&Sâ€™s Table 8 when
both the model for the propensity score and the outcome model are wrong.
A second DR estimator with the regression form (7)
is immediately obtained by estimating the parameter
Î² of the model E(Y |X) = (X T Î²) with the weighted
least squares estimator Î²Ì‚WLS that uses weights 1/Ï€Ì‚ (X).
By definition, the estimator Î²Ì‚WLS satisfies


Pn



T
{Y âˆ’ (XT Î²Ì‚WLS )}X = 0
Ï€Ì‚ (X)

and consequently (8) is immediately true for mÌ‚(X)
equal to mÌ‚WLS (X) = (XT Î²Ì‚WLS ) when, as we always assume in this discussion, the first component
of X is the constant 1. It therefore follows that when
model (X T Î²) has an intercept, Î¼Ì‚DR (Ï€Ì‚ , mÌ‚WLS ) =
Pn (mÌ‚WLS ). The estimator Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS ) is called
Î¼Ì‚WLS in K&S.
With highly variable Ï€Ì‚(X)âˆ’1 and incorrect models for both Ï€(X) and E(Y |X), we would expect
Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS ) to outperform Î¼Ì‚DR (Ï€Ì‚, mÌ‚EXT -REG ) because it does not have the severe extrapolation problem
of Î¼Ì‚DR (Ï€Ì‚, mÌ‚EXT -REG ). This expectation is dramatically
borne out in K&Sâ€™s simulations.
Some years ago, Marshall Joffe pointed out to us
that Î¼Ì‚DR (Ï€Ì‚ , mÌ‚WLS ) was double-robust and asked us if
it had advantages compared to Î¼Ì‚DR (Ï€Ì‚ , mÌ‚EXT -REG ). At
the time we had not realized that Î¼Ì‚DR (Ï€Ì‚, mÌ‚EXT -REG )
would perform so very poorly in settings with highly
variable Ï€Ì‚ (X)âˆ’1 , so we told him that it probably offered no particular advantage. Based on our bad advice,
Joffe never published a paper on Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS ) as a
DR estimator. To our knowledge, Kang and Schaeffer
are the first to do so. We note that Kang and Schaeffer
do not consider Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS ) to be an AIPW DR estimator. However, the above derivation shows otherwise.
Even Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS ) may not perform well in some
instances. For example, if Var(Y |X) = Ïƒ 2 is constant,
Ïƒ 2 / Var(Y ) is near 1 and a number of nonrespondents
have X lying far outside the convex hull of the respondentsâ€™ X values, then Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS ) may perform
poorly. This is because the subjects who have the greatest Ï€Ì‚(X)âˆ’1 in the respondentsâ€™ subsample will have
enormous leverage which can force their residual to
be nearly zero, which is a problem particularly when
Ïƒ 2 / Var(Y ) is near 1 and the model (X T Î²) is misspecified, as then extrapolation to the Xâ€™s far from the
convex hull will be poor.
The third DR regression type estimator is an extension of the estimator Î¼Ì‚IPW -NR in Kang and Schaeffer.
To compute this estimator we extend the regression

model (XT Î²) by adding the covariate Ï€Ì‚(X) (rather
than its inverse) to obtain {XT Î² + Ï• Ï€Ì‚(X)} and then
Ï•
) satisjointly estimate (Î², Ï•) with the estimator (Î²,
fying



T 
Ï€Ì‚(X)
T
Ï€Ì‚(X)
Y âˆ’ X Î² +Ï•
= 0.
Pn
X
Ï€Ì‚(X)
Because we have assumed the vector X has one component equal to the constant 1, (8) is satisfied with
Ï€Ì‚(X)}.
mÌ‚(X) equal to mÌ‚DR-IPW -NR (X) = {XT Î² + Ï•
Thus, Î¼Ì‚DR (Ï€Ì‚, mÌ‚DR-IPW -NR ) = Pn (mÌ‚DR-IPW -NR ). Furthermore, since by construction, Pn [T {Y âˆ’
mÌ‚DR-IPW -NR (X)}] = 0, then Î¼Ì‚DR (Ï€Ì‚, mÌ‚DR-IPW -NR ) is
also equal to Pn {T Y + (1 âˆ’ T )mÌ‚DR-IPW -NR (X)}. Because Ï€Ì‚(X) is bounded between 0 and 1, adding
the covariate Ï€Ì‚ (X) to model (XT Î²), in contrast to
adding Ï€Ì‚(X)âˆ’1 , does not induce model extrapolation
problems like the ones discussed above for Î¼Ì‚DR (Ï€Ì‚,
mÌ‚EXT -REG ). We speculate that Î¼Ì‚DR (Ï€Ì‚, mÌ‚DR-IPW -NR )
will behave much better than Î¼Ì‚DR (Ï€Ì‚, mÌ‚EXT -REG ) and
possibly similarly to Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS ) when Ï€Ì‚(X)âˆ’1 has
high variance. Indeed, we have observed this behavior
in the simulation study of Section 5; however, due to
space limitations, results for Î¼Ì‚DR (Ï€Ì‚, mÌ‚EXT -REG ) were
not reported as they were qualitatively similar to those
reported in K&S.
Finally, the estimator Î¼Ì‚DR (Ï€Ì‚ , mÌ‚EXT -REG ) with  the
identity link is also an example of a DR targeted maximum likelihood estimator of the marginal mean Î¼ in
the sense of van der Laan and Rubin (2006). We thus
conclude from the above discussion that with highly
variable Ï€Ì‚(X)âˆ’1 and incorrect parametric models for
Ï€(X) and E(Y |X), certain targeted maximum likelihood estimators can perform much worse than the ad
hoc estimator Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS ).
4.1.2 Bounded Horvitzâ€“Thompson double-robust
estimators. We refer to DR estimators satisfying (6)
as bounded Horvitzâ€“Thompson DR estimators. They
are obtained by replacing Ï€Ì‚ in Î¼Ì‚B -DR (Ï€Ì‚, mÌ‚REG ) with
Ï€Ì‚EXT satisfying


T
(9) Pn mÌ‚REG (X) âˆ’ Î¼Ì‚REG
âˆ’1 =0
Ï€Ì‚EXT (X)
where Î¼Ì‚REG = Pn [mÌ‚REG (X)]. We can obtain such a
Ï€Ì‚EXT (Â·) by considering the extended logistic model
Ï€EXT (X) = expit{Î± T X + Ï•h(X)} with h(X) a userPROP-GREED
supplied function and estimating Ï• with Ï•
solving


T
Pn
âˆ’
1
expit(Î±Ì‚ T X + Ï•h(X))


Â· {mÌ‚REG (X) âˆ’ Î¼Ì‚OLS } = 0

550

J. ROBINS, M. SUED, Q. LEI-GOMEZ AND A. ROTNITZKY

where Î±Ì‚ is the MLE of Î± in the model Ï€(X) =
expit(Î± T X). Then Ï€Ì‚EXT (X) = expit{Î±Ì‚ T X +
PROP-GREED h(X)} satisfies (9) and consequently
Ï•
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG ) is of the form (6). A default
choice for h(X) would be mÌ‚REG (X) âˆ’ Î¼Ì‚OLS .
Interestingly, the OLS estimator Î¼Ì‚OLS can be viewed
not only as a DR estimator, as seen in Section 3, but
also as a bounded Horvitzâ€“Thompson DR estimator!
Specifically, suppose that Î±Ì‚Ë† inv is used to estimate Î± in
the propensity model Ï€(X; Î±) as in Section 3, except
that without imposing the constraints, so that indeed,
T
âˆ’ 1}X]. Then
Î±Ì‚Ë† inv solves 0 = Pn [{ Ï€(X;Î±)


Pn

T
Ï€(X; Î±Ì‚Ë† inv )



âˆ’ 1 mÌ‚REG (X) âˆ’ Î¼Ì‚OLS



=0

and Î¼Ì‚OLS is equal to the inverse probability weighted
estimator

 

T
T
Y Pn
.
Pn
Ï€(X; Î±Ì‚Ë† inv )
Ï€(X; Î±Ì‚Ë† inv )
Robins (2001) and van der Laan and Rubin (2006)
describe particular bounded Horvitzâ€“Thompson DR
estimators Î¼Ì‚(âˆ) that are obtained by iterating to
convergence a sequence Î¼Ì‚(j ) of estimators that are
not themselves bounded Horvitzâ€“Thompson estimators. However, the Robins (2001) estimator performed
poorly in our simulations (results not shown) and no
simulation study of the van der Laan and Rubin (2006)
estimator has been published to our knowledge with
highly variable inverse probability weights. In fact, van
der Laan and Rubin (2006) describe an estimator Î¼Ì‚(âˆ)
that is simultaneously a bounded Horvitzâ€“Thompson
and a regression DR estimator that is obtained by iterating to convergence a sequence Î¼Ì‚(j ) of estimators
without this dual property. Again we do not know of a
simulation study showing that the estimator generally
performs well in practice with highly variable inverse
probability weights.
5. SIMULATION STUDIES

To investigate the nature of the surprisingly good
performance of the regression estimator Î¼Ì‚OLS in the
simulation study of K&S and to evaluate the performance of the additional estimators described in Section 4, we replicated the simulation study of K&S.
Table 1 reports the Monte Carlo bias, variance and
mean squared error for twelve different estimators of
Î¼ = 210, sample sizes n = 200 and 1000 and the
four possible combinations of model specifications for
the propensity score and the conditional mean of the

response given the covariates (the latter referred to
throughout as the outcome model).
The estimators reported in rows 1 and 9, rows 3
and 11, rows 4, 12, 17 and 22 and rows 5, 13, 18 and
23 are, respectively, the estimators Î¼Ì‚OLS , Î¼Ì‚IPW -POP ,
Î¼Ì‚BC-OLS and Î¼Ì‚WLS investigated by K&S. Throughout we use the notational conventions of Sections
2â€“4, and thus we rename Î¼Ì‚BC-OLS and Î¼Ì‚WLS with
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG ) and Î¼Ì‚DR (Ï€Ì‚ , mÌ‚WLS ), respectively. The
estimator Î¼Ì‚HT is the Horvitzâ€“Thompson type estimator
Pn {T Y/Ï€Ì‚(X)}. All remaining estimators are DR estimators of Î¼ and are defined in Sections 2 and 4.
When both working models are correct, theory
indicates that all DR estimators are CAN, asymptotically equivalent, and efficient in the class of estimators that are CAN even if the outcome model is
incorrect. We were not surprised then to find that all
DR estimators reported in rows 4â€“8 of Table 1 performed identically and were more efficient than the inefficient IPW estimators Î¼Ì‚IPW -POP and Î¼Ì‚HT . However,
the near-identical behavior of the regression estimator
Î¼Ì‚OLS caught our attention. The estimator Î¼Ì‚OLS is the
maximum likelihood estimator of Î¼, and hence efficient, in a semiparametric model that assumes a parametric form for the conditional mean of Y given the
covariates. Thus, we would have expected it to have
smaller variance than that of the DR estimators of Î¼,
because when both the propensity score model and
the regression model are correct, the latter attains the
semiparametric variance bound in the less restrictive
(nonparametric) model that does not impose restrictions on the conditional mean of Y. A closer examination of the data generating process used by K&S
explains this unusual behavior. Under S&K data generating process Yi = 210 + 13.7Ziâˆ— + Îµi , where Ziâˆ— =

2Z1i + 4j =2 Zj i , with Zj i , j = 1, . . . , 4, and Îµi mutually independent N(0, 1) random variables. But under
this process Ziâˆ— , and hence Zi = (Z1i , Z2i , Z3i , Z4i ),
is an essentially perfect predictor of Yi : the residual
variance var(Yi |Ziâˆ— ) is equal to var(Yi )/195. This striking feature of K&S data generating process is illustrated in Figure 1. The figure shows a scatterplot of Y
versus the predicted values from the fit of the correct
outcome model to the respondents in a random sample
of n = 200 units. Dark dots correspond to data points
of respondents. White dots correspond to the simulated
missing outcomes Yi of the nonrespondents plotted
against the predicted values Zi Î²Ì‚. The white dots follow
nearly perfectly a straight line through the origin and

551

COMMENT
TABLE 1
Results for simulation study as in K&S
Sample size 200
Row

Estimator

Sample size 1000

Bias

Var

MSE

Bias

0.13
âˆ’0.08
âˆ’0.06
0.13
0.13
0.13
0.13
0.13

5.97
148.92
14.12
5.96
5.97
5.97
5.96
5.97

5.98
148.92
14.13
5.98
5.98
5.98
5.98
5.98

âˆ’0.03
0.17
âˆ’0.03
âˆ’0.03
âˆ’0.03
âˆ’0.03
âˆ’0.03
âˆ’0.03

1.41
26.46
3.43
1.41
1.41
1.41
1.41
1.41

1.41
26.49
3.43
1.41
1.41
1.41
1.41
1.41

âˆ’0.39
16.87
1.67
âˆ’4.90
âˆ’2.01
âˆ’1.76
âˆ’3.82
âˆ’2.25

10.91
4110.86
73.39
145.93
10.70
11.82
40.07
11.77

11.06
4395.39
76.17
169.91
14.74
14.90
54.65
16.82

âˆ’0.83
38.97
4.81
âˆ’13.91
âˆ’2.98
âˆ’2.49
âˆ’8.03
âˆ’3.33

2.19
39933
108.86
6853.68
2.20
1.81
128.61
3.44

2.88
41452
131.95
7047.12
11.08
8.02
193.13
14.54

Ï€ -model right, outcome model wrong
17
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
18
Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS )
19
Î¼Ì‚DR (Ï€Ì‚, mÌ‚DR-IPW -NR )
20
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG )
21
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG )

0.55
0.65
0.06
0.56
0.53

11.82
8.82
7.39
11.51
9.41

12.12
9.24
7.40
11.83
9.69

0.07
0.16
âˆ’0.10
0.08
0.11

2.81
1.90
1.58
2.79
2.08

2.82
1.93
1.59
2.80
2.09

Ï€ -model wrong, outcome model right
22
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
23
Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS )
24
Î¼Ì‚DR (Ï€Ì‚, mÌ‚DR-IPW -NR )
25
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG )
26
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG )

0.14
0.13
0.13
0.13
0.13

5.95
5.97
5.97
5.96
5.96

5.97
5.98
5.98
5.97
5.98

âˆ’0.03
âˆ’0.03
âˆ’0.03
âˆ’0.02
âˆ’0.02

1.77
1.41
1.41
1.43
1.42

1.77
1.41
1.41
1.43
1.42

Both models right
1
Î¼Ì‚OLS
2
Î¼Ì‚HT
3
Î¼Ì‚IPW -POP
4
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
5
Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS )
6
Î¼Ì‚DR (Ï€Ì‚, mÌ‚DR-IPW -NR )
7
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG )
8
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG )
Both models wrong
9
Î¼Ì‚OLS
10
Î¼Ì‚HT
11
Î¼Ì‚IPW -POP
12
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
13
Î¼Ì‚DR (Ï€Ì‚, mÌ‚WLS )
14
Î¼Ì‚DR (Ï€Ì‚, mÌ‚DR-IPW -NR )
15
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG )
16
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG )

with slope 1: the predicted values are essentially perfect predictors of the missing outcomes! When the outcome and propensity score models are correctly specified, the asymptotic variance of the DR estimator is
equal to var(Y ) + var[Ï€(Z){1 âˆ’ Ï€(Z)}âˆ’1 var(Y |Z)].
When Z is a perfect predictor of Y, this variance reduces to var(Y ), the variance of the standardized distribution of Î¼Ì‚FULL , the sample mean of Y of respondents and nonrespondents. This is not surprising because it is well known that, when the outcome model
is correctly specified, a DR estimator asymptotically
extracts all the information available in Z to predict
Y. Since the regression estimator Î¼Ì‚OLS cannot be more
efficient than Î¼Ì‚FULL , we conclude that Î¼Ì‚OLS and the
DR estimators should have nearly identical variance
when Z is an almost perfect predictor of Y and indeed
this variance should be also almost the same as that

Var

MSE

of the infeasible estimator Î¼Ì‚FULL . In our study we had
simulated the outcomes of the nonrespondents. Thus,
we were indeed able to compute Î¼Ì‚FULL and its Monte
Carlo variance. As expected, the Monte Carlo variance
of Î¼Ì‚FULL was essentially the same as that of Î¼Ì‚OLS for
both sample sizes.
Theory also indicates that the IPW estimators
Î¼Ì‚IPW -POP and Î¼Ì‚HT of rows 2 and 3 should be CAN.
However, in our simulations, these estimators were
nearly unbiased but their sampling distribution was
skewed to the right and had very large variance. Figure 2 shows smooth density estimators for these sampling distributions for sample sizes n = 200 and n =
1000. The skewness and large variance of the IPW estimators were caused by few samples which had respondents with large values of Y and very large weights
1/Ï€Ì‚ . Specifically, in most samples, the true Ï€ values of

552

J. ROBINS, M. SUED, Q. LEI-GOMEZ AND A. ROTNITZKY

F IG . 1. K&S simulation experiment. Outcomes vs predicted values. Sample size 200. Top: correct y model. Bottom: wrong y model. Dashed
line is the line Y = X. Dark dots: respondents. White dots: nonrespondents.

COMMENT

the respondents were not too small, and consequently
the weights 1/Ï€Ì‚ not too large, precisely because by
the very definition of Ï€, having a respondent with a
small Ï€ is a rare event. In the data generating process
of K&S, Ï€(Z) is negatively correlated with Y ; the correlation is roughly equal to âˆ’0.6. Thus, in most samples, the 1/Ï€Ì‚ -weighted mean of the Y values of the
respondents tended to be smaller than Î¼. However, in a
few samples, some anomalous respondent had a small
value of Ï€. In the computation of Î¼Ì‚HT , this anomalous
respondent carried an unusually large weight 1/Ï€Ì‚ and
because his Y value tended to be larger than the mean
Î¼, the estimator Î¼Ì‚HT in those rare samples tended to be
substantially larger than Î¼. The skewness lessens as the
sample size increases because with large sample sizes,
the number of samples which have respondents with
small values of Ï€ also increases. The skewness is also
substantially less severe for Î¼Ì‚IPW -POP compared to that
of Î¼Ì‚HT also as expected since, as discussed in Section 4.1, in any given sample, |Î¼Ì‚IPW -POP | is bounded
by the largest observed |Y | value.
Although the Monte Carlo sampling distribution of
the IPW estimators gives a rough idea of the shape of
the true sampling distribution of these estimators, neither the Monte Carlo bias nor the Monte Carlo variance
should be trusted. One thousand replications are not
enough to capture the tail behavior of highly skewed
sampling distributions, and as such cannot produce reliable Monte Carlo estimates of bias, much less of variance.
Turn now to the case in which the propensity score
model is correct but the outcome model is incorrect.
Theory indicates that the DR estimators of rows 17 to
21 of Table 1 should be CAN. However, in our simulations nearly all the DR estimators were slightly biased
upward. Nevertheless, all DR estimators performed as
well as or better, in terms of MSE, than the OLS estimator of row 9.
Consider now the case in which the propensity score
model is incorrect but the outcome model is correct.
Once again, the almost identical performance of all DR
estimators in rows 22â€“26 of Table 1 with that of the
OLS estimator of row 1 is no surprise after recalling
that Z is a perfect predictor of Y . Specifically, the fact
that Ziâˆ— is a nearly perfect predictor of Yi implies that
mÌ‚(Zi ) is almost identical to the outcome Yi regardless
of whether unit i is a respondent or a nonrespondent
and regardless of whether mÌ‚(Zi ) was fit by ordinary
least squares or by weighted least squares. Thus, the
average of mÌ‚(Zi ) is essentially the same as Î¼Ì‚FULL and

553

the sum of Ti Ï€Ì‚iâˆ’1 (Yi âˆ’ mÌ‚(Zi )) is almost zero regardless of the model under which Ï€Ì‚ was computed. Consequently, all DR estimators must be nearly the same
as the infeasible full data sample mean Î¼Ì‚FULL .
Finally, turn to the case in which both propensity
score and the outcome models are wrong. The performance of the IPW estimators is disastrous as well as
that of the DR estimator in row 12 and, to a lesser extent, that of the estimator in row 15. Figure 3 shows
smooth density estimators of the sampling distribution of these four estimators when the sample size is
1000. The estimators Î¼Ì‚HT and Î¼Ì‚IPW -POP have distributions heavily skewed to the right while the estimators Î¼Ì‚DR (Ï€Ì‚ , mÌ‚REG ) and Î¼Ì‚B -DR (Ï€Ì‚, mÌ‚REG ) have
distributions heavily skewed to the left. The skewness is far more dramatic for the estimators Î¼Ì‚HT and
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG ) than for their counterparts Î¼Ì‚IPW -POP
and Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG ), reflecting the fact that Î¼Ì‚IPW -POP
and Î¼Ì‚B -DR (Ï€Ì‚, mÌ‚REG ) are bounded in the sense described in Section 4.1 while Î¼Ì‚HT and Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
are unbounded. [Indeed, to avoid distortions, in constructing the density plots of Î¼Ì‚HT and Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
we have omitted the extreme values of 5873 and
âˆ’2213, respectively, from one simulation replication.]
Rows 12 and 14 of Table 1 report that the Monte Carlo
bias and variance indeed are even larger for n = 1000
than for n = 200. The extreme distribution skewness
and the increase in bias and variance with sample size
are explained as follows. As noted earlier, even when
the Ï€ â€™s are estimated from a correct model, the distribution of Î¼Ì‚HT and Î¼Ì‚IPW -POP will tend to be skewed
to the right when 1/Ï€ is positively correlated with Y
because of the presence of a few unusual samples with
anomalous respondents with large Y values and small
Ï€ values. Now, because of the nature of the wrong analytic propensity score model used in the simulations,
the estimated Ï€Ì‚ â€™s corresponding to the anomalous units
in the unusual samples were many times smaller than
the true Ï€ â€™s. As a consequence the, usually large, values of Y of the anomalous units essentially determined
the values of Î¼Ì‚HT and Î¼Ì‚IPW -POP in the unusual samples
and consequently, exacerbated even more the skewness of the Monte Carlo sampling distribution of the
IPW estimators. The larger bias and variance when
n = 1000 than when n = 200 were due to two replications with sample size 1000 in which the values of
the estimators were extreme [specifically, Î¼Ì‚HT = 1475
and 2884, and Î¼Ì‚DR (Ï€Ì‚ , mÌ‚REG ) = âˆ’2213 and âˆ’175].
These outlying values were caused by one anomalous
nonrespondent in each sample with large values of Y
(the second largest Y values in one sample and the

554

J. ROBINS, M. SUED, Q. LEI-GOMEZ AND A. ROTNITZKY

F IG . 2.

Distributions of Î¼Ì‚HT and Î¼Ì‚IPW -POP under correct propensity score models.

largest in the other). For these units, the 1/Ï€ values
were 38.7 and 50.9 but 1/Ï€Ì‚ were 17,068 and 399, respectively. When the two samples with these anomalous units were removed, the variance of the estimators Î¼Ì‚HT and Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG ) decreased to 6729 and
890, respectively. The paradoxical increase in Monte
Carlo variance with sample size is but another proof
that the Monte Carlo variance in simulations with 1000
replications is not a reliable estimator of the true variance for estimators with highly skewed distributions.

The different directionality of the skewness of the IPW
and DR estimators is explained as follows. In the computation of Î¼Ì‚DR (Ï€Ì‚ , mÌ‚REG ) and Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG ) we
inverse probability weight the values of (mÌ‚REG âˆ’ Y ).
Consequently since, as indicated below, under K&Sâ€™s
wrong analytic outcome model, mÌ‚REG was reasonably
bounded; thus, in the few unusual samples, the anomalous units with small Ï€ â€™s had large and negative values of (mÌ‚REG âˆ’ Y ) and produced extremely small values of the DR estimators.

COMMENT

F IG . 3.

555

Distributions of Î¼Ì‚HT , Î¼Ì‚IPW -POP , Î¼Ì‚DR (Ï€Ì‚ , mÌ‚REG ) and Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG ) under incorrect propensity score and outcome models.

The performance of the remaining DR estimators in
rows 13, 14 and 16 is heterogeneous. Some, though
still biased, have bias and variance orders of magnitude smaller than the variance of the estimators
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG ) and Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG ).
In a second simulation experiment described below,
the relative performance of the DR estimators was
somewhat different than in this simulation study and,
as we explain later, better than that of the regression estimator Î¼Ì‚OLS . This attests to the obvious fact that when
the propensity score and outcome models are both in-

correct we cannot expect to find a single clear winner.
The relative performance of the estimators will very
much depend on the data generating process and the
nature of the model misspecifications.
To understand why the regression estimator Î¼Ì‚OLS
performed so remarkably well when both models were
wrong, we first note that because the outcome model
was a linear regression model with an intercept fitted
by ordinary least squares in the respondent subsample, the sum of the predicted values X Î²Ì‚ and the sum
of Y in the respondent subsample are the same. Thus,

556

J. ROBINS, M. SUED, Q. LEI-GOMEZ AND A. ROTNITZKY

F IG . 4. Y vs predicted values in one sample of size 1000 generated under K&S experiment. Dashed line is the line Y = X. Dark dots:
respondents. White dots: nonrespondents.

Î¼Ì‚OLS = (nobs /n)Y obs + (nmiss /n) (X Î²Ì‚)miss , where
(X Î²Ì‚)miss is the average of the predicted values for
the missing outcomes. The bias of Î¼Ì‚OLS therefore depends on the bias of (X Î²Ì‚)miss as an estimator of the
mean of Y in the nonrespondent subpopulation. If, due
either to good luck or â€œcherry picking,â€ the prediction function x Î²Ì‚ from a misspecified regression model
x Î² successfully extrapolates to the covariates of nonrespondents, even when these are far from the convex hull of covariates in the respondent subsample,
(X Î²Ì‚)miss âˆ’ Y miss will be roughly centered around 0,
and consequently Î¼Ì‚OLS will be a nearly unbiased estimator of the mean of Y . We now show this phenomenon explains the excellent performance of Î¼Ì‚OLS in
Kang & Schaefferâ€™s simulation. In Figure 4 we plotted the outcomes Y versus the predicted values X Î²Ì‚ in
the previously mentioned unusual sample of size 1000
with both the propensity and outcome models misspecified, where Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG ), Î¼Ì‚B -DR (Ï€Ì‚, mÌ‚REG ) and
the IPW estimators did disastrously due to the presence of one anomalous unit with extremely small Ï€Ì‚.
The dark dots correspond to the observed data values
of the respondents. White dots correspond to the actual simulated missing outcomes Y of the nonrespondents plotted against the predicted values X Î²Ì‚. We can

see that the predicted values of the nonrespondents are
reasonably centered around the straight line even for
those points with predicted values far from the predicted values of the respondents. In this sample, Î¼Ì‚OLS
was 205.78, a far more reasonable value than those obtained for the IPW and just-mentioned DR estimators.
To demonstrate that Î¼Ì‚OLS can have a substantially
worse performance than the DR estimators, we conducted a second simulation experiment. This second
experiment, like our first, redid K&Sâ€™s simulation by
generating the data (Y, T , X) from K&Sâ€™s chosen distributions. However, in our second experiment we analyzed the data ((1 âˆ’ T )Y, T , X) in which Y is observed
only when T = 0, rather than the data (T Y, T , X) that
was analyzed by us in our first experiment and by
K&S in their paper. [To do so, since the data ((1 âˆ’
T )Y, T , X) can be recoded as ((1 âˆ’ T )Y, 1 âˆ’ T , X),
we simply recompute each of the estimators reported
in Table 1 except now we everywhere replace Ï€Ì‚ and
T by 1 âˆ’ Ï€Ì‚ and 1 âˆ’ T .] The results are displayed in
Table 2. We observe that, with both models wrong, the
bias and MSE of Î¼Ì‚OLS now exceed those of any DR
estimator!
As in our first experiment, due to the extreme variability in the estimated â€œinverse probabilityâ€ weights,
the DR estimators appear to have considerable finite

557

COMMENT
TABLE 2
Results for simulation study as in K&S but with the roles of T and 1 âˆ’ T reversed
Sample size 200
Row

Estimator

Sample size 1000

Bias

Var

MSE

Bias

Var

MSE

0.12
âˆ’0.46
0.45
0.12
0.12
0.12
0.12
0.12

5.96
49.14
14.76
5.96
5.96
5.96
5.96
5.96

5.98
49.36
14.96
5.98
5.97
5.97
5.97
5.97

âˆ’0.03
âˆ’0.24
0.05
âˆ’0.02
âˆ’0.02
âˆ’0.02
âˆ’0.02
âˆ’0.02

1.41
8.45
3.11
1.41
1.41
1.40
1.41
1.40

1.41
8.51
3.12
1.41
1.41
1.41
1.41
1.41

4.97
0.55
3.92
3.33
3.17
3.11
3.32
3.30

7.97
40.27
9.67
8.79
8.21
8.21
8.70
8.68

32.68
40.57
25.03
19.90
18.24
17.90
19.69
19.55

4.97
0.39
3.68
3.07
2.81
2.64
3.04
3.01

1.91
6.27
2.22
2.12
1.97
1.97
2.10
2.10

26.62
6.43
15.79
11.53
9.84
8.94
11.34
11.16

Ï€ -model right, outcome model wrong
17
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
18
Î¼Ì‚DR (Ï€Ì‚ , mÌ‚WLS )
19
Î¼Ì‚DR (Ï€Ì‚ , mÌ‚DR-IPW -NR )
20
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG )
21
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG )

0.71
0.99
0.71
0.75
0.86

12.60
8.04
7.26
11.21
10.38

13.11
9.02
7.76
11.76
11.12

0.14
0.23
0.18
0.14
0.18

2.96
1.92
1.72
2.76
2.71

2.98
1.97
1.75
2.78
2.74

Ï€ -model wrong, outcome model right
22
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
23
Î¼Ì‚DR (Ï€Ì‚ , mÌ‚WLS )
24
Î¼Ì‚DR (Ï€Ì‚ , mÌ‚DR-IPW -NR )
25
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG )
26
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG )

0.12
0.12
0.12
0.12
0.12

5.96
5.96
5.96
5.96
5.96

5.97
5.97
5.97
5.97
5.97

âˆ’0.02
âˆ’0.02
âˆ’0.02
âˆ’0.02
âˆ’0.02

1.40
1.40
1.40
1.40
1.40

1.41
1.41
1.41
1.41
1.41

Both models right
1
Î¼Ì‚OLS
2
Î¼Ì‚HT
3
Î¼Ì‚IPW -POP
4
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
5
Î¼Ì‚DR (Ï€Ì‚ , mÌ‚WLS )
6
Î¼Ì‚DR (Ï€Ì‚ , mÌ‚DR-IPW -NR )
7
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG )
8
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG )
Both models wrong
9
Î¼Ì‚OLS
10
Î¼Ì‚HT
11
Î¼Ì‚IPW -POP
12
Î¼Ì‚DR (Ï€Ì‚, mÌ‚REG )
13
Î¼Ì‚DR (Ï€Ì‚ , mÌ‚WLS )
14
Î¼Ì‚DR (Ï€Ì‚ , mÌ‚DR-IPW -NR )
15
Î¼Ì‚B -DR (Ï€Ì‚ , mÌ‚REG )
16
Î¼Ì‚B -DR (Ï€Ì‚EXT , mÌ‚REG )

sample bias, especially at the smaller sample size of
200, when the propensity model is correct but the outcome model is wrong. In fact, this bias is larger than
it was in the first simulation experiment, which was to
be expected as the variability in the estimated â€œinverse
probabilityâ€ weights was greater in the second than the
first experiment (data not shown).
6. SENSITIVITY ANALYSIS

Consider again the missing-data setting with the
mean Î¼ of Y as the parameter of interest. When the
covariate vector X is high dimensional, one cannot be
certain, owing to lack of power, that a chosen model for
the propensity score is nearly correct, even if it passes
standard goodness-of-fit tests. Therefore a large number of models for the propensity score with different

subsets of the covariates, different orders of interactions and different dimensions of the parameter vector should be fit to the data. Similarly, many different
outcome models should be fit. This raises the question:
once fit, how should these many candidate models be
used in the estimation of the mean of Y ?
One approach is to use modern techniques of model
selection to choose a single propensity and outcome
model. Specifically, there has been a recent outpouring
of work on model selection in regression. This work
has shown that one can use cross-validation and/or penalization to empirically choose, from among a large
number of candidates, a model whose predictive risk
for the response variable in the regression is close
to that of the best candidate model. In fact, van der
Laan (2005) has proposed that k-fold cross-validation
should be routinely employed to select the model for

558

J. ROBINS, M. SUED, Q. LEI-GOMEZ AND A. ROTNITZKY

the propensity score and for the outcome regression
that are to be used in the construction of a DR estimator.
An alternative approach which we are currently
studying is the following. Suppose one has fit Jp
propensity score models and Jo outcome models. For
a favorite DR estimator Î¼Ì‚, define Î¼Ì‚ij as the DR estimator that uses the fitted values from the ith propensity model and the j th outcome model. Now, if the
ith propensity model is correct, all Jo estimators in
the set Ep,i â‰¡ {Î¼Ì‚ij ; j = 1, . . . , Jo } will be CAN estimators of Î¼. Thus, an Î±-level test of the homogeneity hypothesis Hpi : E A (Î¼Ì‚i1 ) = E A (Î¼Ì‚ij ) for all
j âˆˆ {2, . . . , Jo } [where E A (Â·) stands for large sample
mean, i.e., the probability limit of Â·] is also an Î±-level
goodness-of-fit test for the propensity model that is directly relevant to its use in a DR estimator of Î¼. Similarly if the j th outcome model is correct, all Jp estimators in the set Eo,j â‰¡ {Î¼Ì‚ij ; i = 1, . . . , Jp } will be
CAN for Î¼ and a test of the homogeneity hypothesis
Hoj : E A (Î¼Ì‚1j ) = E A (Î¼Ì‚ij ) for all i âˆˆ {2, . . . , Jp } is a
test of fit for the outcome model. This suggests that
one could choose as a final estimator of Î¼ the DR estimator Î¼Ì‚i âˆ— j âˆ— , where i âˆ— is the i for which the test of the
hypothesis Hpi gave the largest p-value and j âˆ— is the j
for which the test of the hypothesis Hoj gave the largest
p-value. However, this method of selecting i âˆ— and j âˆ—
is nonoptimal for two reasons. First, it could easily select a misspecified propensity model i for which the
power of the test of the hypothesis Hpi is particularly
poor and similarly for the outcome regression. This remark implies that some measure of the spread of the
elements of Ep,i and Eo,j should also contribute to
the selection of i âˆ— and j âˆ— . Second, the method does
not exploit the fact that if i âˆ— and j âˆ— are correct, then
E A (Î¼Ì‚ij âˆ— ) = E A (Î¼Ì‚i âˆ— j ) for all i and j, suggesting that
an optimal method should select i âˆ— and j âˆ— jointly. Alternative approaches for selecting i âˆ— and j âˆ— will be
reported elsewhere. In any case, the very fact that input
to the selection algorithm requires the matrix Î¼Ì‚ij provides an informal sensitivity analysis; we directly observe the sensitivity of our DR estimator to the choice
of propensity and outcome regression model.
The approach just described could also be combined
with the model selection approach. Specifically, one
first uses cross-validation to choose not one but rather
Jp and Jo propensity and outcome models (the ones
with the Jp and Jo lowest cross-validated risk estimates) out of a much larger number of candidate models and next, one uses these Jp +Jo models as input for
the approach described above. Sensitivity to the choice

of the particular DR estimator might be included by
using a number of different DR estimators and selecting among or averaging over DR estimators that give
similar estimates Î¼Ì‚i âˆ— j âˆ— .
van der Laan (2005) has proposed some new approaches to model selection for DR estimation that
go beyond his above-mentioned approach, which we
do not discuss here due to space limitations. Finally,
Wang, Petersen, Bangsberg and van der Laan (2006)
have proposed using the parametric bootstrap to study
the sensitivity of DR estimates to highly variable â€œinverse probabilityâ€ weights.
7. FURTHER CONSIDERATIONS
Estimation of Causal Effects

K&S briefly touch on the problem of estimating the
difference of the outcome means corresponding to two
treatments in an observational study under ignorability. This difference is often referred to as the average
causal effect (ACE). K&S view the problem of estimating ACE essentially as two missing-data problems,
each one regarding the outcomes of subjects that do not
follow the treatment of concern as missing. The difference of the DR estimators of the separate means serves
as an estimator of the mean difference ACE. However,
the difference of the two DR estimators will have poor
small sample behavior if there is incomplete overlap
of the estimated propensity scores in the treated and
untreated. In fact, in the presence of incomplete overlap, most investigators argue against trying to estimate
ACE and in favor of estimating the causal effect in
the subpopulation of subjects with overlapping propensity scores. However, assuming the ACE parameter is
of some substantive interest, Robins et al. (2007) suggest an alternative to reporting the difference of two
DR estimators of the separate means. Their approach
is based on fitting a linear semiparametric regression
model for the unknown conditional effect function encoding the dependence of the conditional treatment effect on the baseline covariates X. Their model has the
property that it is guaranteed to be correctly specified
under the null hypothesis that the conditional effect
function is the zero function. Robins et al. (2007) show
that this strategy results in estimators of the ACE that
greatly outperform any estimator based on the difference of double-robust estimators, whenever the model
for the conditional effect function is correctly specified; in particular, when the aforementioned null hypothesis is true.

559

COMMENT

Multiple Robustness

Consider again the MAR missing-data model with X
very high dimensional (say 20â€“100 continuous covariates) so we cannot possibly hope to model the propensity score or the outcome regression nonparametrically.
Double-robust estimators of the mean Î¼ of Y are n1/2 consistent if either one of two parametric models is
correct but inconsistent if both models are misspecified. This property of DR estimators seems unsatisfactory, as it means that one does very, very well if one of
the two models is correct but can do very, very poorly
when both are incorrect. Might we do better?
Define an estimator to be m-robust for Î¼ at rate nÎ±
if the estimator is nÎ± -consistent for Î¼ when any one of
m parametric models is correct, but inconsistent if all
m models are misspecified. A DR estimator is then a
2-robust estimator with Î± = 1/2. Our view is that an
m-robust estimator with m large, even though this may
require Î± to be much smaller than 1/2 and so entail
a much slower rate of convergence, would usually be
preferable to a DR estimator for the following two reasons. First, if one uses an m-robust rather than a DR
estimator, one is more likely to be using a consistent
estimator of Î¼ (as it is always more likely that at least
one of m, rather than one of two, models is correct).
Second, the slower rate of convergence (under the assumption one of the m models is correct) will result
in wider nominal confidence intervals than the usual
nominal intervals of length 1/n1/2 associated with a
DR estimator. Such a wide interval seems to us a more
appropriate measure of the actual uncertainty about Î¼,
more accurately reflecting the fact that our estimator
could even be inconsistent if all m models are incorrect.
These observations raise the following questions.
Do m-robust estimators exist for arbitrarily large m
if we are willing to sacrifice n1/2 -consistency for nÎ± consistency with Î± perhaps much smaller than 1/2?
What is the maximum value of Î± we can achieve for
a given m? If m-robust estimators exist for m > 2,
how do we construct them? Answers to these questions can be found in Robins, Li, Tchetgen and van
der Vaart (2007), where it is shown that, under weak
assumptions, (i) m-robust estimators exist for all m,
(ii) m-robust estimators are (m âˆ’ 1) dimensional Ustatistics, for which explicit closed-form expressions
are given, and (iii) the maximal possible Î± is often less than 1/2 and sometimes much less. However, the finite sample properties of m-robust estimators have yet to be studied even by simulation.
Thus we will have to wait to see if they are as useful in practice as theory would indicate they should
be.

REFERENCES
BANG , H. and ROBINS , J. M. (2005). Doubly robust estimation in
missing data and causal inference models. Biometrics 61 962â€“
972. MR2216189
ROBINS , J. M. (2000). Robust estimation in .sequentially ignorable missing data and casual inference models. Proc. Amer. Statist. Assoc. Section on Bayesian Statistical Science 1999 6â€“10.
ROBINS , J. M. (2002). Commentary on â€œUsing inverse weighting
and predictive inference to estimate the effects of time-varying
treatments on the discrete-time hazardâ€, by Dawson. and Lavori.
Statistics in Medicine 21 1663â€“1680.
ROBINS , J. M. and ROTNITZKY, A. (2001). Comment on â€œInference for semiparametric models: Some questions and an answer,â€ by P. J. Bickel and J. Kwon. Statist. Sinica 11 920â€“936.
MR1867326
ROBINS , J. M. and WANG , N. (2000). Inference for imputation
estimators. Biometrika 87 113â€“124. MR1766832
ROBINS , J. M., ROTNITZKY, A. and Z HAO L.-P. (1995). Analysis
of semiparametric regression models for repeated outcomes in
the presence of missing data. J. Amer. Statist. Assoc. 90 106â€“
121. MR1325118
ROBINS , J. M., L I , L., T CHETGEN , F. and VAN DER VAART,
A. W. (2007). Higher order influence functions and minimax
estimation of nonlinear functionals. In IMS Lecture Notesâ€“
Monograph Series Probability and Statistics Models: Essays in
Honor of David A. Freedman 2 335â€“421.
ROBINS , J. M., S UED , M., L EI -G OMEZ , Q. and ROTNITZKY, A.
(2007). Double-robustness with improved efficiency in missing
and causal inference models. Technical report, Harvard School
of Public Health.
ROSENBAUM , P. R. (2002). Observational Studies, 2nd ed.
Springer, New York. MR1899138
S CHARFSTEIN , D. O., ROTNITZKY, A. and ROBINS , J. M.
(1999). Adjusting for nonignorable drop-out using semiparametric non-response models (with discussion). J. Amer. Statist.
Assoc. 94 1096â€“1146. MR1731478
TAN , Z. (2006). A distributional approach for causal inference using propensity scores. J. Amer. Statist. Assoc. 101 1619â€“1637.
MR2279484
VAN DER L AAN , M. J. and ROBINS , J. M. (2003). Unified Methods for Censored Longitudinal Data and Causality. Springer,
New York. MR1958123
VAN DER L AAN , M. J. and RUBIN , D. (2006). Targeted maximum likelihood learning. U.C. Berkeley Division of Biostatistics Working Paper Series. Available at http://www.bepress.
com/ucbbiostat/paper213/.
VAN DER L AAN , M. (2005). Statistical inference for variable
importance. U.C. Berkeley Division of Biostatistics Working
Paper Series. Available at http://www.bepress.com/ucbbiostat/
paper188/.
WANG , Y., P ETERSEN , M., BANGSBERG , D. and VAN DER
L AAN , M. (2006). Diagnosing bias in the inverse probability of
treatment weighted estimator resulting from violation of experimental treatment assignment. U.C. Berkeley Division of Biostatistics Working Paper Series. Available at http://www.bepress.
com/ucbbiostat/paper211/.

