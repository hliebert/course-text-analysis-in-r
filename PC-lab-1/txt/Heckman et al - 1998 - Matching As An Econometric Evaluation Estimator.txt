0034-6527/98/00120261$02.00

Review of Economic Studies (1998) 65, 261-294
© 1998 The Review of Economic Studies Limited

Matching As An Econometric
Evaluation Estimator
JAMES J. HECKMAN
University of Chicago

University of Pittsburgh

and

PETRA TODD
University of Pennsylvania
First version received October 1994;.final version accepted October 1997 (Eds.)

This paper develops the method of matching as an econometric evaluation estimator. A
rigorous distribution theory for kernel-based matching is presented. The method of matching is
extended to more general conditions than the ones assumed in the statistical literature on the topic.
We focus on the method of propensity score matching and show that it is not necessarily better,
in the sense of reducing the variance of the resulting estimator, to use the propensity score method
even if propensity score is known. We extend the statistical literature on the propensity score by
considering the case when it is estimated both parametrically and nonparametrically. We examine
the benefits of separability and exclusion restrictions in improving the efficiency of the estimator.
Our methods also apply to the econometric selection bias estimator.

1. INTRODUCTION
Matching is a widely-usedmethod of evaluation. It is based on the intuitively attractive idea
of contrasting the outcomes of programme participants (denoted Yd with the outcomes of
"comparable" nonparticipants (denoted Yo). Differences in the outcomes between the two
groups are attributed to the programme.
Let 10 and 11 denote the set of indices for nonparticipants and participants, respectively. The following framework describes conventional matching methods as well as the
smoothed versions of these methods analysed in this paper. To estimate a treatment effect
for each treated person ie I«, outcome Y li is compared to an average of the outcomes YOj
for matched persons jElo in the untreated sample. Matches are constructed on the basis
of observed characteristics X in Rd. Typically, when the observed characteristics of an
untreated person are closer to those of the treated person iel., using a specific distance
measure, the untreated person gets a higher weight in constructing the match. The estimated gain for each person i in the treated sample is
(1)

where WNO,Nt(i,j) is usually a positive valued weight function, defined so that for each
iel«, LjE[, WNo,N.(i,j) = 1, and No and N, are the number of individuals in 10 and I\,
respectively. The choice of a weighting function reflects the choice of a particular distance
measure used in the matching method, and the weights are based on distances in the X
261

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

HIDEHIKO ICHIMURA

262

REVIEW OF ECONOMIC STUDIES

space. For example, for each iel, the nearest-neighbour method selects one individual
je10 as the match whose X, is the "closest" value to Xi, in some metric. The kernel methods
developed in this paper construct matches using all individuals in the comparison sample
and downweighting "distant" observations.
The widely-used evaluation parameter on which we focus in this paper is the mean
effect of treatment on the treated for persons with characteristics X
E(Y1 - YoID= I,X),

(P-I)

(2)

where different values of WNO,N\(i) may be used to select different domains f!( or to account
for heteroskedasticity in the treated sample. Different matching methods are based on
different weighting functions {WNO,N\(i)} and {WNO,N\(i,j)}.
The method of matching is intuitively appealing and is often used by applied statisticians, but not by economists. This is so for four reasons. First, it is difficult to determine
if a particular comparison group is truly comparable to participants (i.e. would have
experienced the same outcomes as participants had they participated in the programme).
An ideal social experiment creates a valid comparison group. But matching on the
measured characteristics available in a typical nonexperimental study is not guaranteed
to produce such a comparison group. The published literature presents conditional independence assumptions under which the matched group is comparable, but these are far
stronger than the mean-independence conditions typically invoked by economists. Moreover, the assumptions are inconsistent with many economic models of programme participation in which agents select into the programme on the basis of unmeasured components
of outcomes unobserved by the econometrician. Even if conditional independence is
achieved for one set of X variables, it is not guaranteed to be achieved for other sets of
X variables including those that include the original variables as subsets. Second, if a valid
comparison group can be found, the distribution theory for the matching estimator remains
to be established for continuously distributed match variables X. 1
Third, most of the current econometric literature is based on separability between
observables and unobservables and on exclusion restrictions that isolate different variables
that determine outcomes and programme participation. Separability permits the definition
of parameters that do not depend on unobservables. Exclusion restrictions arise naturally
in economic models, especially in dynamic models where the date of enrollment into the
programme differs from the dates when consequences of the programme are measured.
The available literature on matching in statistics does not present a framework that
incorporates either type of a priori restriction.
Fourth, matching is a data-hungry method. With a large number of conditioning
variables, it is easy to have many cells without matches. This makes the method impractical
or dependent on the use of arbitrary sorting schemes to select hierarchies of matching
variables. (See, e.g. Westat (1980, 1982, 1984).) In an important paper, Rosenbaum and
Rubin (1983) partially solve this problem. They establish that if matching on X is valid,
so is matching solely on the probability of selection into the programme Pr (D = IIX) =
I. When the match variables are discrete, the matching estimator for each cell is a mean and consistency
and asymptotic normality of the matching estimator are easily established.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

where D= I denotes programme participation. Heckman (1997) and Heckman and Smith
(1998) discuss conditions under which this parameter answers economically interesting
questions. For a particular domain f!( for X, this parameter is estimated by

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

263

P(X). Thus a multidimensional matching problem can be recast as a one-dimensional

E(YoID= I,X)=E(YoID=O,X),

whenever both sides of this expression are well defined. In order for both sides to be well
defined simultaneously for all X it is usually assumed that 0 < P(X) < 1 so that
Supp (XID= 1) =Supp (XID=O). As Heckman, Ichimura, Smith, Todd (1998), Heckman,
Ichimura, Smith and Todd (1996b) and Heckman, Ichimura and Todd (1997) point out,
this condition is not appropriate for important applications of the method. In order to
meaningfully implement matching it is necessary to condition on the support common to
both participant and comparison groups S, where
S=Supp (XID= 1) n Supp (XID=O),
and to estimate the region of common support. Equality of the supports need not hold
a priori although most formal discussions of matching assumes that it does. Heckman,
Ichimura, Smith and Todd (1998) and Heckman, Ichimura and Todd (1997) report the
empirical relevance of this point for evaluating job training programmes. Invoking
assumptions that justify the application of nonparametric kernel regression methods to
estimate programme outcome equations, maintaining weaker mean independence
assumptions compared to the conditional independence assumptions used in the literature,
and conditioning on S, we produce an asymptotic distribution theory for matching estimators when regressors are either continuous, discrete or both. This theory is general enough
to make the Rosenbaum-Rubin theorem operational in the commonly-encountered case
where P(X) is estimated either parametrically or nonparametrically.
With a rigorous distribution theory in hand, we address a variety of important questions that arise in applying the method of matching: (I) We ask, if one knew the propensity
score, P(X), would one want to use it instead of matching on X? (2) What are the effects
on asymptotic bias and variance if we use an estimated value of P? We address this
question both for the case of parametric and nonparametric P(X). Finally, we ask (3)
what are the benefits, if any, of econometric separability and exclusion restrictions on the
bias and variance of matching estimators?
The structure of this paper is as follows. Section 2 states the evaluation problem and
the parameters identified by the analysis of this paper. Section 3 discusses how matching
solves the evaluation problem. We discuss the propensity score methodology of Rosenbaum and Rubin (1983). We emphasize the importance of the common support condition
2. They term P(X) the propensity score. For the relationship between propensity score methods and
selection models, see Heckman and Robb (1986) or Heckman, Ichimura, Smith and Todd (1998).

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

problem and a practical solution to the curse of dimensionality for matching is possible.'
Several limitations hamper the practical application of their theoretical result. Their
theorem assumes that the probability of selection is known and is not estimated. It is also
based on strong conditional independence assumptions that are difficult to verify in any
application and are unconventional in econometrics. They produce no distribution theory
for their estimator.
In this paper we first develop an econometric framework for matching that allows us
to incorporate additive separability and exclusion restrictions. We then provide a sampling
theory for matching from a nonparametric vantage point. Our distribution theory is
derived under weaker conditions than the ones currently maintained in the statistical
literature on matching. We show that the fundamental identification condition of the
matching method for estimating (P-I) is

264

REVIEW OF ECONOMIC STUDIES

assumed in the literature and develop an approach that does not require it. Section 4
contrasts the assumptions used in matching with the separability assumptions and exclusion restrictions conventionally used in econometrics. A major goal of this paper is to
unify the matching literature with the econometrics literature. Section 5 investigates a
central issue in the use of propensity scores. Even if the propensity score is known, is it
better, in terms of reducing the variance of the resulting matching estimator, to condition
on X or P(X)? There is no unambiguous answer to this question. Section 6 presents a
basic theorem that provides the distribution theory for kernel matching estimators based
on estimated propensity scores. In Section 7, these results are then applied to investigate
the three stated questions. Section 8 summarizes the paper.

Each person can be in one of two possible states, 0 and I, with associated outcomes
(Yo, Y 1) , corresponding to receiving no treatment or treatment respectively. For example,
"treatment" may represent participation in the social programme, such as the job training
programme evaluated in our companion paper where we apply the methods developed in
this paper. (Heckman, Ichimura and Todd (1997).) Let D= 1 if a person is treated; D=
o otherwise. The gain from treatment is A= Y1 - Yo. We do not know A for anyone
because we observe only Y = D Y1 + (1- D) Yo, i.e. either Yo or Y1 •
This fundamental missing data problem cannot be solved at the level of any individual.
Therefore, the evaluation problem is typically reformulated at the population level. Focusing on mean impacts for persons with characteristics X, a commonly-used parameter of
interest for evaluating the mean impact of participation in social programmes is (P-I).
It is the average gross gain from participation in the programme for participants with
characteristics X. If the full social cost per participant is subtracted from (P-I) and the
no treatment outcome for all persons closely approximates the no programme outcome,
then the net gain informs us of whether the programme raises total social output compared
to the no programme state for the participants with characteristics X. 3
The mean E(YtlD= I,X) can be identified from data on programme participants.
Assumptions must be invoked. to identify the counterfactual mean E( Yo ID = I, X), the
no-treatment outcome of programme participants. In the absence of data from an ideal
social experiment, the outcome of self-selected nonparticipants E( Yo ID = 0, X) is often
used to approximate E( Yo ID = I, X). The selection bias that arises from making this
approximation is
B(X)=E(YoID= I,X)- E(YoID=O,X).

Matching on X, or regression adjustment of Yo using X, is based on the assumption that
B(X) = 0 so conditioning on X eliminates the bias.
Economists have exploited the idea of conditioning on observables using parametric or
nonparametric regression analysis (Barnow, Cain and Goldberger (1980), Barros (1986),
Heckman and Robb (1985, 1986». Statisticians more often use matching methods, pairing
treated persons with untreated persons of the same X characteristics (Cochrane and Rubin
(1973».
The literature on programme evaluation gives two distinct responses to the problem
of estimating (P-I) with continuous conditioning variables. The first borrows from the
3. See Heckman (1997) or Heckman and Smith (1998) for a precise statement of when this parameter
answers an interesting economic evaluation question.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

2. THE EVALUATION PROBLEM AND THE PARAMETERS OF INTEREST

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

265

M(S) =E(Y1 - YoID= 1, XES),

(P-2)

1 2
/

with a well-defined N- distribution theory where S is a subset ofSupp (XID= 1). There
is considerable interest in estimating impacts for groups so (P-2) is the parameter of
interest in conducting an evaluation. In practice both pointwise and setwise parameters
may be of interest. Historically, economists have focused on estimating (P-I) and statisticians have focused on estimating (P-2), usually defined over broad intervals of X values,
including Supp (XID= 1). In this paper, we invoke conditions sufficiently strong to consistently estimate both (P-I) and (P-2).
3. HOW MATCHING SOLVES THE EVALUATION PROBLEM
Using the notation of Dawid (1979) let
(Yo, Y1) llDIX,

(A-I)

denote the statistical independence of (Yo, Y1) and D conditional on X. An equivalent
formulation of this condition is
Pr (D= 11 Yo, Y1 , X) =Pr (D= IIX).
This is a non-causality condition that excludes the dependence between potential
outcomes and participation that is central to econometric models of self selection. (See
Heckman and Honore (1990).) Rosenbaum and Rubin (1983), henceforth denoted RR,
establish that, when (A-I) and

0< P(X) < I,

(A-2)

are satisfied, (Yo, Y1) .ll.DIP(X), where P(X) =Pr (D= IIX). Conditioning on P(X) balances the distribution of Yo and Y1 with respect to D. The requirement (A-2) guarantees
that matches can be made for all values of X. RR called condition (A-I) an "ignorability"
condition for D, and they call (A-I) and (A-2) together a "strong ignorability" condition.
When the strong ignorability condition holds, one can generate marginal distributions
of the counterfactuals

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

kernel regression literature. It uses a smoothing procedure that borrows strength from
adjacent values of a particular value of X = x and produces uniformly consistent estimators
of (P-I) at all points of the support for the distributions of X given D = 1 or D = O. (See
Heckman, Ichimura, Smith and Todd (I996a) or Heckman, Ichimura and Todd (1997).)
Parametric assumptions about E( Yo ID = 1, X) play the same role as smoothing
assumptions, and in addition allow analysts to extrapolate out of the sample for X. Unless
the class of functions to which (P-I) may belong is restricted to be smaller than the finiteorder continuously-differentiable class of functions, the convergence rate of an estimator
of (P-I) is governed by the number of continuous variables included in X (Stone (1982».
The second response to the problem of constructing counterfactuals abandons estimation of (P-I) at any point of X and instead estimates an average of (P-I) over an interval
of X values. Commonly-used intervals include Supp (X ID = 1), or subintervals of the
support corresponding to different groups of interest. The advantage of this approach is
that the averaged parameter can be estimated with rate N- 1/ 2 , where N is sample size,
regardless of the number of continuous variables in X when the underlying functions are
smooth enough. Averaging the estimators over intervals of X produces a consistent estimator of

266

REVIEW OF ECONOMIC STUDIES

E( Yo ID = I, XeS) = E[E( Yo ID = 1, X) ID = 1, XES]

= E[E( YoID=O, X) ID= 1, XeS],
so E( Yo ID = 1, XeS) can be recovered from E( Yo ID = 0, X) by integrating over X using
the distribution of X given D = 1, restricted to S. Note that, in principle, both E( Yo IX, D =
O} and the distribution of X given D = 1 can be recovered from random samples of participants and nonparticipants.
It is important to recognize that unless the expectations are taken on the common
support of S, the second equality does not necessarily follow. While E(YoID=O, X) is
always measurable with respect to the distribution of X given D=O, (J1(XID=O», it may
not bemeasurable with respect to the distribution of X given D = 1, (J1 (X ID = 1)). Invoking
assumption (A-2) or conditioning on the common support S solves the problem because
J1 (X ID = 0) and J1 (X ID = I), restricted to S, are mutually absolutely continuous with
respect to each other. In general, assumption (A-2) may not be appropriate in many
empirical applications. (See Heckman, Ichimura, and Todd (1997) or Heckman, Ichimura,
Smith and Todd (1996a, b, 1998).)
The sample counterpart to the population requirement that estimation should be over
a common support arises when the set S is not known. In this case, we need to estimate
S. Since the estimated set, S, and S inevitably differ, we need to make sure that asymptotically the points at which we evaluate the conditional mean estimator of E( Yo ID = 0, X)
are in S. We use the "trimming" method developed in a companion paper (Heckman,
Ichimura, Smith, Todd (I996a) to deal with the problem of determining the points in S.
Instead of imposing (A-2), we investigate regions S, where we can reasonably expect to
learn about E(Yt - YoID= 1, S).
Conditions (A-I) and (A-2) which are commonly invoked to justify matching, are
stronger than what is required to recover E(Yt - YoID= 1, X) which is the parameter of
4. Heckman, Smith and Clements (1997) and Heckman and Smith (1998) analyse a variety of such
assumptions.
5. Thus, the implications of 0 < Pr (D = IIX) < 1 is that conditional measures of X given D = 0 and that
given D= 1 are absolutely continuous with respect to each other. These dominating measure conditions are
standard in the matching literature.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

but one cannot estimate the joint distribution of (Yo, Yt), F(yo, YtID, X), without making
further assumptions about the structure of outcome and programme participation
equations."
If P(X) = 0 or P(X) = I for some values of X, then one cannot use matching conditional on those X values to estimate a treatment effect. Persons with such X characteristics
either always receive treatment or never receive treatment, so matches from both D = I
and D = 0 distributions cannot be performed. Ironically, missing data give rise to the
problem of causal inference, but missing data, i.e. the unobservables producing variation
in D conditional on X, are also required to solve the problem of causal inference. The
model predicting programme participation should not be too good so that P(X) = I or 0
for any X. Randomness, as embodied in condition (A-2), guarantees that persons with
the same characteristics can be observed in both states. This condition says that for any
measurable set A, Pr (X e A ID = I) > 0 if and only if Pr (X e A ID = 0) > 0, so the comparison
of conditional means is well defined." A major finding in Heckman, Ichimura, Smith, Todd
(1996a, b, 1998) is that in their sample these conditions are not satisfied, so matching is
only justified over the subset Supp (XID= 1) n Supp (XID=O).
Note that under assumption (A-I)

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

267

interest in this paper. We can get by with a weaker condition since our objective is
construction of the counterfactual E( Yo IX, D = I)
(A-3)

YollDIX,

E(YoID= I,X)=E(YoID=O,X)

for XES.

(A-I')

Under this assumption, we can identify E(YoID= 1, X) for XES, the region of common
support.' Mean independence conditions are routinely invoked in the econometrics
literature. 8
Under conditions (A-I) and (A-2), conceptually different parameters such as the
mean effect of treatment on the treated, the mean effect of treatment on the untreated, or
the mean effect of randomly assigning persons to treatment, all conditional on X, are the
same. Under assumptions (A-3) or (A-I'), they are distinct."
Under these weaker conditions, we demonstrate below that it is not necessary to make
assumptions about specific functional forms of outcome equations or distributions of
unobservables that have made the empirical selection bias literature so controversial. What
is controversial about these conditions is the assumption that the conditioning variables
available to the analyst are sufficiently rich to justify application of matching. To justify
the assumption, analysts implicitly make conjectures about what information goes into
the decision sets of agents, and how unobserved (by the econometrician) relevant information is related to observables. (A-I) rules out dependence of D on Yo and Y1 and so is
inconsistent with the Roy model of self selection. See Heckman (1997) or Heckman and
Smith (1998) for further discussion.

4. SEPARABILITY AND EXCLUSION RESTRICTIONS
In many applications in economics, it is instructive to partition X into two not-necessarily
mutually exclusive sets of variables, (T, Z), where the T variables determine outcomes
Yo=go(T) + o;

(3a)

Y1 =gl(T)+ U1 ,

(3b)

and the Z variables determine programme participation
Pr (D= l\X) =Pr (D=

uz: =P(Z).

(4)

6. By symmetric reasoning, if we postulate the condition Y1.lL D IX and (A-2), then Pr (D = II Y., X) =
Pr (D = lIX), so selection could occur on Yo or A, and we can recover Pr ( Y 1 < tiD = 0, X). Since Pr ( Yo < tiD =
0, X) can be consistently estimated, we can recover E( Y1 - Yo ID = 0, X).
7. We can further identify E(Y1 - YoID=O) if we assume E(YdD= I,X)=E(YdD=O, X) for X in S.
8. See, for example, Barnow, Cain and Goldberger (1980) or Heckman and Robb (1985, 1986).
9. See Heckman (1990, 1997) for a discussion of the three parameters. See also Heckman and Smith
(1998).

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

which implies that Pr(Yo<tID=I,X)=Pr(Yo<tID=O,X) for XES.
In this case, the distribution of Yo given X for participants can be identified using
data only on nonparticipants provided that XES. From these distributions, one can recover
the required counterfactual mean E(YoID=I,X) for XES. Note that condition (A-3)
6
does not rule out the dependence of D and Y1 or on J1. = Y1 - Yo given X.
For identification of the mean treatment impact parameter (P-I), an even weaker
mean independence condition suffices

268

REVIEW OF ECONOMIC STUDIES

E(YoID= I, X) =go(T) + E(UoIP(Z)),

where Z and T contain some distinct regressors. This representation reduces the dimension
of the matching or nonparametric regression if the dimension of Z is two or larger.
Currently-available matching methods do not provide a way to exploit such information
about the additive separability of the model or to exploit the information that Z and T
do not share all of their elements in common.
This paper extends the insights of Rosenbaum and Rubin (1983) to the widely-used
model of programme participation and outcomes given by equations (3a), (3b) and (4).
Thus, instead of (A-I) or (A-3), we consider the case where
Uo11.DIX.

(A-4a)

Invoking the exclusion restrictions P(X) = P(Z) and using an argument analogous to
Rosenbaum and Rubin (1983), we obtain
E{DI

o: P(Z)} =E{E(DI Uo,X)1 v; P(Z)}
= E{P(Z) I o: P(Z)} = P(Z) = E{D IP(Z)},

so that

Uo 11. DIP(Z).

(A-4b)

Under condition (A-4a) it is not necessarily true that (A-I) or (A-3) are valid but it is
obviously true that
[Yo - go(T)] 11. D IP(Z).

In order to identify the mean treatment effect on the treated, it is enough to assume that
E(UoID= I, P(Z))=E(UoID=O, P(Z)),

(A-4b')

instead of (A-4a) or (A-4b).
Observe that (A-4a), (A-4b), and (A-4b') do not imply that E(UoIP(Z)) = 0 or that
E( UII P(Z)) = o. They only imply that the distributions of the unobservables are the same
in populations of D= 1 and D=O, once one conditions on P(Z). Yo and YI must be
adjusted to eliminate the effects of T on outcomes. Only the residuals can be used to
exploit the RR conditions. Thus P(Z) is not, in general, a valid instrumental variable.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Thus in a panel data setting Y 1 and Yo may be outcomes measured in periods after
programme participation decisions are made, so that Z and T may contain distinct variables, although they may have some variables in common. Different variables may determine participation and outcomes, as in the labour supply and wage model of Heckman
(1974).
Additively-separable models are widely used in econometric research. A major advantage of such models is that any bias arising from observing Yo or Y 1 by conditioning on
D is confined to the "error term" provided that one also conditions on T, e.g. E(YoID=
1, X)=go(T)+ E(UoID= 1, Z) and E(YIID= 1, X) =gI(T) + E(UtlD= 1, Z). Another
major advantage of such models is that they permit an operational definition of the effect
of a change in T holding U constant. Such effects are derived from the go and gI functions.
The Rosenbaum-Rubin Theorem (1983) does not inform us about how to exploit
additive separability or exclusion restrictions. The evidence reported in Heckman,
Ichimura, Smith and Todd (I996a), reveals that the no-training earnings of persons who
chose to participate in a training programme, Yo, can be represented in the following way

269

MATCHING AS AN ESTIMATOR

HECKMAN ET AL.

In order to place these results in the context of classical econometric selection models,
consider the following index model setup
Yo=go(T) +
D= I

=

°

o:

if yt(Z) -

v~o;

otherwise.

If Z and v are independent, then P(Z)=Fv(yt(Z», where F v( . ) is the distribution
function of v. In this case identification condition (A-4b') implies
E[UoID= 1, Fv(yt(Z»] = E[UoID=O, Fv(yt(Z»],

oo

f"'(z)

-00

-00

f

=

Uof(Uo, vlyt(Z»dvdUo/Fv(yt(Z»

rr
t:
-00

Uo/(

v; v I'I'(Z))dvdUo/[1 - r, ( 'I'(Z»].

",(Z)

If, in addition, ",(Z) is independent of (Uo , v), and E(Uo) =0, condition (*) implies

r~Z) Uo/(Uo• v)dvdUo=O.

for any ",(Z), which in tum implies E(Uolv=s)=O for any s when ",(Z) traces out the
entire real line. Hence under these conditions our identification condition implies there is
no selection on unobservables as defined by Heckman and Robb (1985, 1986). However,
",(Z) may not be statistically independent of (Uo, v). Thus under the conditions assumed
in the conventional selection model, the identification condition (A-4b') mayor may not
imply selection on unobservables depending on whether ",(Z) is independent of (Uo , v)
or not.
5. ESTIMATING THE MEAN EFFECT OF TREATMENT: SHOULD ONE
USE THE PROPENSITY SCORE OR NOT?
Under (A-I') with S=Supp (XID= 1) and random sampling across individuals, if one
knew E( Yo ID = 0, X = x), a consistent estimator of (P-2) is
1

'Ax=N 1

Li Ef l [YIi- E(YoID=O, X=Xd],

where II is the set of i indices corresponding to observations for which D, = 1. If we assume
E(YoID= 1, P(X»=E(YoID=O, P(X» for XeSupp (P(X)ID= 1),

(A-I")

which is an implication of (A-I), and E( Yo ID = 0, P(X) = p) is known, the estimator

Ap=N11 LiEf! [YIi-E(YoID=O, P(X)=P(Xd)]
is consistent for E(~ID= 1).
We compare the efficiency of the two estimators, 'Ap and 'Ax. We show that neither
is necessarily more efficient than the other. Neither estimator is feasible because both
assume the conditional mean function and P(X) are known whereas in practice they need

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

or when F; is strictly increasing,

270

REVIEW OF ECONOMIC STUDIES

to be estimated. However, the analysis of this case is of interest because the basic intuition
from the simple theorem established below continues to hold when the conditional mean
function and P(X) are estimated.

Theorem 1. Assume:
(i) (A-I') and (A-I") holdfor S=Supp (XID= 1);
(ii) {Yli , X, hell are independent and identically distributed;

and
(iii) 0 < E( Y~) . E( Yi) < 00.

Vx=E[Var (Y1ID= 1, X) ID= 1]+ Var [E(Y1- YoID= I,X)ID= 1],

and
Vp=E[Var (Y1ID= 1, P(X» ID= 1]+ Var [E(Y1- YoID= 1, P(X» ID= 1].
The theorem directly follows from the central limit theorem for iid sampling with
finite second moment and for the sake of brevity its proof is deleted.
Observe that
E[Var (Y1ID= 1, X) ID= 1] ~E[Var (Y1ID= 1, P(X» ID= I],
because X is in general a better predictor than P(X) but
Var [E(Y1 - YoID= I, X) ID= 1]~Var [EI(Y1- Yo) ID= 1, P(X» ID= 1],
because vector X provides a finer conditioning variable than P(X). In general, there are
both costs and benefits of conditioning on a random vector X rather than P(X). Using
this observation, we can construct examples both where Vx~ Vp and whereVx~ V p •
Consider first the special case where the treatment effect is constant, that is
E(Y1- YoID= 1, X) is constant. An iterated expectation argument implies that
E(Y1- YoID= 1, P(X» is also constant. Thus, the first inequality, Vx~ Vp holds in this
case. On the other hand, if Y1=m(P(X» + U for some measurable function m( . ) and U
and X are independent, then
Vx - Vp=Var [E(YoID= I,X) ID= 1]- Var [E(YoID= 1, P(X» ID= 1],
which is non-negative because vector X provides a finer conditioning variable than P(X).
So in this case Vx~ Vp •
When the treatment effect is constant, as in the conventional econometric evaluation
models, there is only an advantage to conditioning on X rather than on P(X) and there
is no cost. 10 When the outcome Y 1 depends on X only through P(X), there is no advantage
to conditioning on X over conditioning on P(X).
Thus far we have assumed that P(X) is known. In the next section, we investigate
the more realistic situation where it is necessary to estimate both P(X) and the conditional
10. Heckman (1992), Heckman and Smith (1998) and Heckman, Smith and Clements (1997) discuss the
central role ofthe homogeneous response assumption in conventional econometric models ofprogram evaluation.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Then Ax and Ap are both consistent estimators of (P-2) with asymptotic distributions
that are normal with mean 0 and asymptotic variances Vx and Vp , respectively, where

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

271

6. ASYMPTOTIC DISTRIBUTION THEORY FOR KERNEL-BASED
MATCHING ESTIMATORS
We present an asymptotic theory for our estimator of treatment effect (P-2) using either
identifying assumption (A-I') or (A-4b'). The proof justifies the use of estimated P values
under general conditions about the distribution of X.
We develop a general asymptotic distribution theory for kemel-regression-based and
local-polynomial-regression-based matching estimators of (P-2). Let T and Z be not necessarily mutually exclusivesubvectors of X, as before. When a function depends on a random
variable, we use corresponding lower case letters to denote its argument, for example,
g(t,p)=E(YoID=I, T=t,P(Z)=p) and P(z)=Pr(D=IIZ=z). Although not explicit
in the notation, it is important to remember that g(t, p) refers to the conditional expectation
conditional on D= I as well as T=t and P(Z)=p. We consider estimators of ~(t, P(z))
where P(z) must be estimated. Thus we consider an estimator g(t, P(z)), where P(z) is an
estimator of P(z). The general class of estimators of (P-2) that we analyse are of the form

~

L\ =

NIl Lo

1

[Yli - g(Ti, P(Zi))]I(XiES)
~

-I

IE J

N

I

(6)

LiefII(XiES)

where I(A) = 1 if A holds and =0 otherwise and S is an estimator of S, the region of
overlapping support, where S=Supp {XI D= I} n Supp {XI D=O}.
To establish the properties of matching estimators of the form! based on different
estimators of P(z) and g(t, P(z)), we use a class of estimators which we call asymptotically
11. If we knew E(Y1ID= 1, P(X) =p) as well, the estimator
NIl

L;ef

l

[E(Y1ID= 1, P(X)=P(X;»- E(YoID=O, P(X)=P(X;»]

would be more efficient than ~p. In practical applications, we do not know either E(YdD= 1, P(x)=p) or
E( Yo ID = 0, P(X) = p) so this point is only a theoretical curiosum and is not investigated further.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

means. In this more realistic case, the trade-off between the two terms in Vx and Vp
persists. II
When we need to estimate P(X) or E( Yo I D = 0, X), the dimensionality of the X is a
major drawback to the practical application of the matching method or to the use of
conventional nonparametric regression. Both are data-hungry statistical procedures. For
high dimensional X variables, neither method is feasible in samples of the size typically
available to social scientists. Sample sizes per cell become small for matching methods
with discrete X's. Rates of convergence slow down in high-dimensional nonparametric
methods. In a parametric regression setting, one may evade this problem by assuming
functional forms for E( UoIX) (see e.g. Barnow, Cain and Goldberger (1980) and the
discussion in Heckman and Robb (1985, 1986)), but this approach discards a major advantage of the matching method because it forces the investigator to make arbitrary
assumptions about functional forms of estimating equations.
Conditioning on the propensity score avoids the dimensionality problem by estimating
the mean function conditional on a one-dimensional propensity score P(X). However, in
practice one must estimate the propensity score. If it is estimated nonparametrically, we
again encounter the curse of dimensionality. The asymptotic distribution theorem below
shows that the bias and the asymptotic variance of the estimator of the propensity score
affects the asymptotic distribution of the averaged matching estimator more the larger the
effect of a change in the propensity score on the conditional means of outcomes.

272

REVIEW OF ECONOMIC STUDIES

linear estimators with trimming. We analyse their properties by proving a series of lemmas
and corollaries leading up to Theorem 2. With regard to the estimators P(z) and get, p),
we only assume that they can be written as an average of some function of the data plus
residual terms with appropriate convergence properties that are specified below. We start
by defining the class of asymptotically linear estimators with trimming.

r.7=t

-; 2r.7=t

An estimator

pof f3 is called asymptotically linear if
P- f3 =n- tr.7=t ",(Zi) +oin- t/2),

holds." Definition 1 is analogous to the conventional definition, but extends it in five ways
to accommodate nonparametric estimators. First, since the parameter B(x) that we estimate is a function evaluated at a point, we need a notation to indicate the point x at
which we estimate it. Conditions (i)-(iv) are expressed in terms of functions of x. Second,
for nonparametric estimation, asymptotic linearity only holds over the support of X-the
region where the density is bounded away from zero. To define the appropriate conditions
for this restricted region, we introduce a trimming function I(xES) that selects observations only if they lie in S and discards them otherwise.
Third, nonparametric estimators depend on smoothing parameters and usually have
bias functions that converge to zero for particular sequences of smoothing parameters.
We introduce a subscript n to the ",-function and consider it to be an element of a class
of functions \}In, instead of a fixed function, in order to accommodate smoothing parameters. For example, in the context of kernel estimators, if we consider a smoothing parameter of the form a(x) . hn, different choices of h; generate an entire class of functions \}In
indexed by a function a(' ) for any given kernel." We refer to the function "'n as a score
function. The stochastic term hex) is the bias term arising from estimation. For parametric
cases, it often happens that h(x) =0.
Fourth, we change the notion of the residual term being "small" from oin-1/2) to
the weaker condition (iv). We will demonstrate that this weaker condition is satisfied by
some nonparametric estimators when the stronger condition op(n-t/2) is not. Condition
(iii) is required to restrict the behaviour of the bias term. The bias term has to be reduced
to a rate o(n- t/2) in order to properly centre expression (i) asymptotically. For the case
of d-dimensional nonparametric model with p-times continuously differentiable functions,
Stone (1982) proves that the optimal uniform rate of convergence of the nonparametric
regression function with respect to mean square error is (n/log n)-pl(2p+ d) . His result implies
that some undersmoothing, compared to this optimal rate, is required to achieve the
desired rate of convergence in the bias term alone. Note that the higher the dimension of
the estimand, the more adjustment in smoothing parameters to reduce bias is required.
12. See Newey and McFadden (1994) p. 2142.

13. See Ichimura (1996).

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Definition 1. An estimator O(x) of B(x) is an asymptotically linear estimator with
trimming I(xES) if and only if there is a function "'nE\}In, defined over some subset of a
finite-dimensional Euclidean space, and stochastic terms hex) and R(x) such that for
sample size n:
(i) [O(x)-B(x)]I(xES)=n- t
"'n(Xi, ~;x)+h(x)+R(x);
(ii) E{"'n(Xi, ~;X)IX...= x}=O;
(iii) plim n --+ cx:l n
b(Xi ) =b< 00;
(iv) n- t/2r.7=t R(Xi)=op(I).

HECKMAN ET AL.

273

MATCHING AS AN ESTIMATOR

This is the price that one must pay to safeguard against possible misspecificationsof g(t, p)
or P(z). It is straightforward to show that parametric estimators of a regression function
are asymptotically linear under some mild regularity conditions. In the Appendix we
establish that the local polynomial regression estimator of a regression function is also
asymptotically linear.
A typical estimator of a parametric regression function m(x; /3) takes the form
m(x; p), where m is a known function and p is an asymptotically linear estimator, with
I
p-p=n- 1 :L;=1 ",(Xi, Y i)+op(n- / 2) . In this case, by a Taylor expansion,

~[m(x, p)-m(x, p)]=n- 1/2:L;=1[om(x, f3)/oP] vcx., Y i)

p.

fJ lies on a line segment between p and
When E{ ",(X;, Y;}} =0 and
12
E{",(X;, Y;}",(X;, Y;)'l<oo, under iid sampling, for ~xample, n- / :L;=1",(X;, Y;)=
Op(l) and plimn ---+oof3 =f3 so that plimn ---+oo I om(x, P)/op-om(x, f3)/apl =op(l) if
am(x, f3)/af3 is Holder continuous at 13. 14

where

Under these regularity conditions
~[m(x, p) - m(x, 13)] =n- 1/ 2 :L;=l [om(x,

/3)/oP] ",(X;,

Yi ) +op(l).

The bias term of the parametric estimator m(x, p) is b(x) = 0, under the conditions we
have specified. The residual term satisfies the stronger condition that is maintained in the
traditional definition of asymptotic linearity.

(a) Asymptotic linearity of the kernel regression estimator

We now establish that the more general kernel regression estimator for nonparametric
functions is also asymptotically linear. Corollary I stated below is a consequence of a
more general theorem proved in the Appendix for local polynomial regression models
used in Heckman, Ichimura, Smith and Todd (1998) and Heckman, Ichimura and Todd
(1997). We present a specialized result here to simplify notation and focus on main ideas.
To establish this result we first need to invoke the following assumptions.
Assumption 1. Sampling of {Xi, Y;} is i.i.d., Xi takes values in

K' and

Y; in R, and

Var (Y;) < 00.
When a function is p-times continuously differentiable and its p-th derivative satisfies
Holder's condition, we call the function p-smooth. Let m(x) = E{ Y;I X;=x}.
Assumption 2. m(x) is p-smooth, where p>d.
14. A function is Holder continuous at X=xo with constant 0< a ~ 1 if 19'<x, 9) - 9'<xo, 9)1 ~C' \Ix-xo\la
for some C> 0 for all x and 9 in the domain of the function 9'<'. -). Usually Holder continuity is defined for
a function with no second argument 9. We assume that usual Holder continuity holds uniformly over 9 whenever
there is an additional argument.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

+ [om(x, p)/oP - om(x, p)/of3]n- 1/ 2 :L;=1 ",(Xi, Y i) + op(1),

274

REVIEW OF ECONOMIC STUDIES
We also allow for stochastic bandwidths:

Assumption 3. Bandwidth sequence an satisfies plimn -+ oo ani hn= ao>O for some
deterministic sequence {hn} that satisfies nh:/log n-.oo and nh~P -.c< 00 for some c~O.

This assumption implies 2p> d but a stronger condition is already imposed in Assumption 2.15
Assumption 4. Kernel function K( . ) is symmetric, supported on a compact set, and
is Lipschitz continuous.

Assumption 5. Trimming is p-nice on S.

In order to control the bias of the kernel regression estimator, we need to make
additional assumptions. Certain moments of the kernel function need to be 0, the underlying Lebesgue density of Xi,fx(x), needs to be smooth, and the point at which the
function is estimated needs to be an interior point of the support of Xi. It is demonstrated
in the Appendix that these assumptions are not necessary for p-th order local polynomial
regression estimator.
Assumption 6.

Kernel function K ( . ) has moments of order 1 through p - 1 that are

equal to zero.
Assumption 7. fx(x) is p-smooth.
Assumption 8. A point at which m( . ) is being estimated is an interior point of the

support of Xi.
The following characterization of the bias is a consequence of Theorem 3 that is
proved in the Appendix.
Corollary 1. Under Assumptions 1-8, if K(UI' ... ,Ud) =k(uI) ... k(ud) where k(·)
is a one dimensional kernel, the kernel regression estimator mo(x) of m(x) is asymptotically
15. Assumption 3 implies h; -+ 0 and log ntion implies h; -+O.

«r: -+0. These two imply 2p>d. Also notice that theassump-

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

The assumption of compact support can be replaced by a stronger assumption on the
distribution of Xi so that all relevant moments exist. Since we can always choose K(·),
but we are usually not free to pick the distribution of Xi, we invoke compactness.
In this paper we consider trimming functions based on Sand S that have the following
~tructure. Letfx(x) be the Lebesgue density of Xi, S= {xERd;fx(x) ~qo}, and S= {xER d;
fx(x) ~qo}, where supxEslfx(x) - fx(x)1 converges almost surely to zero, andfx(x) is psmooth. We also require thatfx(x) has a continuous Lebesgue density Ii in a neighbourhood of qo withjj(qo) >0. We refer to these sets Sand S as p-nice on S. The smoothness
of fx(x) simplifies the analysis and hence helps to establish the equicontinuity results we
utilize.

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

275

linear with trimming, where, writing G;= Y;- E{ Y;I X;}, and
1fIn(X;, Y;; x) = (naoh:)-l G;K«X;- x)/(aohn))I(xeS)/fx(x) ,

b(x) = (aoh.Y' . [fx(X)'

x

~~,

f

K(u)du

[U u~K(u)du

r

~~, [s!(p-s)W'

!Wm(X)/(OX.)'j' [o{p-'Yx(x)/(oxdfi-')j }(XES).

(b) Extensions to the case of local polynomial regression
In the Appendix, we consider the more general case in which the local polynomial regression estimator for g(t, p) is asymptotically linear with trimming with a uniformly consistent
derivative. The latter property is useful because, as the next lemma shows, if both P(:) and
g(t,p) are asymptotically linear, and if iJg(t,p)/iJp is uniformly consistent, theng(t, P(z)) is
also asymptotically linear under some additional conditions. We also verify in the Appendix that these additional conditions are satisfied for the local polynomial regression
estimators.
Let Pt(z) be a function that is defined by a Taylor's expansion of g(t, P(z)) in the
neighbourhood of P(z), i.e. g(t, P(z)) =g(t, P(z)) + iJg(t, Pt(z))/iJp . [P(z) - P(z)].
Lemma 1.

Suppose that:

(i) Both P(z) and g(t, p) are asymptotically linear with trimming where

[P(z) - P(z)]I(xeS) = n- 1 r.;= 11fInp(Dj, Zj; z) + bp(z) + Rp(z),
[g(t,p) - g(t,p)]I(xeS) =n- Ir.;=1 1fIng( lj, Tj, P(Zj); t,p) + bg(t,p) + Rg(t, p);
(ii) iJg(t,p)/iJp and P(z) are uniformly consistent and converge to iJg(t,p)/iJp and
P(z), respectively and iJg(t,p)/iJp is continuous;
(iii) plimn-+<Xl n- I/22:.;=1 bg(T;, P(Z;)) =bg and
plimn-+<Xl n- I/22:.;=1 iJg(T;, P(Z;))/iJp· bp(T;, P(Z;)) =b gp;
(iv) plimn-+<Xl n- I/22:.;=1 [iJg(T;, PT;(Z;))/iJp-iJg(T;, P(Z;))/iJp]· Rp(Z;)=O;
(v) plim, .... co n- 3/ 2 2:.;=12:.;=1 [iJg(T;, PT;(Z;})/iJp-iJg(T;, P(Z;))/iJp]
. 1fInp(Dj,Zj;Z;)=O.
A

then g(t, P(z)) is also asymptotically linear where
[g(t, P(z))-g(t, P(z))]I(xeS)=n- 1 2:.;=1 [1fIng(lj,

r., P(Zj); t, P(z))

+ iJg(t, P(z))/iJp . 1fInp(Dj, Zj; z)] + b(x) + R(x),
and plimn-+<Xl n-1/2 'L:= 1 b(X;) = bg+ bgp.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Our use of an independent product form for the kernel function simplifies the expression
for the bias function. For a more general expression without this assumption see the
Appendix. Corollary I differs from previous analyses in the way we characterize the
residual term.

276

REVIEW OF ECONOMIC STUDIES

n-

3/2r.;=1r.;=1 [og(Tj, PT;(Zj))/op-og(Tj, P(Zj))/op]· ljIp(Dj, z., Zj)
=n-

I I;=I [og(Tj, PT;(Zj))/op -

I 2
og(Tj, P(Zj))/op]ljIpl(Z;}n- / I;=I V'p2(Dj, Zj),

so condition (v) follows from an application of the central limit theorem and the uniform
consistency of the derivative of g(t,p).
For the case of nonparametric estimators, ljInp does not factor and the double summation does not factor as it does in the case of parametric estimation. For this more general
case, we apply the equicontinuity results obtained by Ichimura (1995) for general Ustatistics to verify the condition. We verify all the conditions for the local polynomial
regression estimators in the Appendix. Since the derivative of g(t, p) needs to be defined
we assume
Assumption 9.

K(·) is I-smooth.

Lemma 1 implies that the asymptotic distribution theory of ! can be obtained for
those estimators based on asymptotically linear estimators with trimming for the general
nonparametric (in P and g) case. Once this result is established, it can be used with lemma
1 to analyze the properties of two stage estimators of the form g(t, P(z)).
(c) Theorem 2: The asymptotic distribution of the matching estimator under general
conditions

Theorem 2 enables us to produce the asymptotic distribution theory of a variety of estimators! of the treatment effect under different identifying assumptions. It also produces the
asymptotic distribution theory for matching estimators based on different estimators of
g(t,p) and P(z). In Sections 3 and 4, we presented various matching estimators for the
mean effect of treatment on the treated invoking different identifying assumptions. An
alternative to matching is the conventional index-sufficient selection estimators that can
be used to construct estimators of E (Yo ID = 1, X), as described in our companion paper
Heckman, Ichimura and Todd (1997) and in Heckman, Ichimura, Smith and Todd
(1996a, 1998). Our analysis is sufficiently general to cover the distribution theory for that
case as well provided an exclusion restriction or a distributional assumption is invoked.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

An important property of this expression which we exploit below is that the effect of
the kernel function ljInp(Dj, Zj; z) always enters multiplicatively with og(t, P(z))/op. Thus
both the bias and variance of P(z) depend on the slope to g with respect to p. Condition
(ii) excludes nearest-neighbour type matching estimators with a fixed number of neighbours. With conditions (ii)-(v), the proof of this lemma is just an application of Slutsky's
theorem and hence the proof is omitted.
In order to apply the theorem, however, we need to verify the conditions. We sketch
the main arguments for the case of parametric estimator P(z) of P(z) here and present
proofs and discussion of the nonparametric case in the Appendix.
Under the regularity conditions just presented, the bias function for a parametric
P(z) is zero. Hence condition (iii) holds if g(t,p) is asymptotically linear and its derivative
is uniformly consistent for the true derivative. Condition (iv) also holds since IRp(Z;} 1=
I 2)
op(n- / and the derivative of g(t,p) is uniformly consistent. Condition (v) can be verified
by exploiting the particular form of score function obtained earlier. Observing that
ljIp(Dj, Zj; Zj) = ljIpl(Zj) . ljIp2(Dj, Zj), we obtain

HECKMAN ET AL.

277

MATCHING AS AN ESTIMATOR

Denote the conditional expectation or variance given that X is in S by E s ( .) or
Vars('), respectively. Let the number of observations in sets 10 and II be No and N I,
respectively, where N = No + N I and that 0 < limN_oo Nt!No = e < 00.

Theorem 2. Under the following conditions:
(i) {YOi' Xi }iEfO and {Yli , Xi }iEft are independent and within each group they are i.i.d.
and YOi for iel« and Yufor iE/I each has afinite second moment;
(ii) The estimator g(x) of g(x) = E{ Yo;ID i= 1, X i= x} is asymptotically linear with
trimming, where

o LiEfo V'ONoNI( YOi, Xi; x)
l

-I

A.

A.

+N I LiEft V'INoNI(Yli,Xi;x)+bg(x)+Rg(x)

and the scorefunctions V'dNoNl(Yd'X; x) for d=O and 1, the bias term bg(x), and
the trimming function satisfy:
(ii-a) E {V'dNoNI (Ydi, Xi; X) IDi=d, X, D= I)} =0 for d=O and 1, and
Var {V'dNON\( Ydi, Xi; X)} = o(N) for each iEIo u II ;
l 2
(ii-b) plimNt_ OO N t / LiEfl b(Xi ) =b;
(ii-c) plimNt_ oo Var {E[V'ONON\(YOi, Xi; X) I YOi, Di=O, Xi, D= 1] ID= I} = Vo<
plimNI_oo Var {E[V'INoNt(Yli ' Xi; X) I Yli , Di= 1, Xi, D= 1] ID= 1} = VI <

00
00,

and
limNI_oo E{[ Yli - g(Xi)]I(XiES) .
E[V'INoNI( Yli , Xi; X) I Yli , Di> 1, Xi, D = 1] ID= I} = COVI ;

Sare p-nice on S, where p > d, where d is the number of regressors in X
and lex) is a kernel density estimator that uses a kernel function that satisfies
Assumption 6.

(ii-d) Sand

Then under (A-I') the asymptotic distribution of
1/2

NI

[Nt

l

LiEfl [Yli - g(Xi)]I(XiES)
]
-I
E s (YI- YoID= 1)

NI

~. t I
L.
'E

A.

I(XiES)

is normal with mean (b/Pr (XESID= 1» and asymptotic variance
Pr (XESID= l)-I{Vars [E s (YI- Yol T, P(Z), D= 1) ID= 1]

+ E s [Vars (YII T, P(Z), D= 1) ID= I]}
+ Pr (XESID= 1)-2{ VI +2· COVI + eVo}.
Proof

See the Appendix.

II

Theorem 2 shows that the asymptotic variance consists of five components. The first
two terms are the same as those previously presented in Theorem 1. The latter three terms
are the contributions to variance that arise from estimating g(x) = E{ Y;jDi= 1, Xi=x}.
The third and the fourth terms arise from using observations for which D = 1 to estimate
g(x). If we use just observations for which D = 0 to estimate g(x), as in the case of the
simple matching estimator, then these two terms do not appear and we only acquire the

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

[g(x) - g(x)]I {XES} = N

278

REVIEW OF ECONOMIC STUDIES

Es (YoID= I,X)=Es (YoIX)=E s (YoID=O,X).
Strong ignorability conditions given by (A-I), (A-2) or (A-3), while conventional in the
matching literature, are not needed but they obviously imply these equalities.
Theorem 2 can be combined with the earlier results to obtain an asymptotic distribution theory for estimators that use g(t, P(z)). One only needs to replace functions
'l'ONoNt(YO;,X;; x) and 'l'INoN.(YIi , X;; x) and the bias term by those obtained in Lemma 1.
7. ANSWERS TO THE THREE QUESTIONS OF SECTION 1 AND MORE
GENERAL QUESTIONS CONCERNING THE VALUE OF
A PRIORI INFORMATION
Armed with these results, we now investigate the three questions posed in the Section 1.
(1) Is it better to match on P(X) or X

if you know P(X)?

Matching on X, Ax involves d-dimensional nonparametric regression function estimation
whereas matching on P(X), Ap only involves one dimensional nonparametric regression
function estimation. Thus from the perspective of bias, matching on P(X) is better in the
sense that it allows IN-consistent estimation of (P-2) for a wider class of models than is
possible if matching is performed directly on X. This is because estimation of higherdimensional functions requires that the underlying functions be smoother for bias terms
to converge to zero. If we specify parametric regression models, the distinction does not
arise if the model is correctly specified.
When we restrict consideration to models that permit IN-consistent estimation either
by matching on P(X) or on X, the asymptotic variance of Ap is not necessarily smaller
than that of Ax. To see this, consider the case where we use a kernel regression for the
D = 0 observations i.e. those with iel«. In this case the score function
'l'INoNt (YIi , X;; x) =0 and

y; X.
'l'ONONt (

0;,

i s

- E;K«X;- x)jaNo)I(xeS)
x) - atofx(xID=O) K(u)du'

J

where E;= Yo;- E{ YodX;, D;=O} and we writefx(xID=O) for the Lebesgue density of X;
given D;=O. (We use analogous expressions to denote various Lebesgue densities.) Clearly
VI and Cov I are zero in this case. Using the score function we can calculate Vo when we
match on X. Denoting this variance by Vox,
16. An earlier version of the paper assumed that only observations for which D=O are used to estimate
g(x).

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

fifth term." We consider the more general case with all five terms. If No is much larger
than N 1 , then the sampling variation contribution of the D = 0 observations is small as ()
is small.
Condition (i) covers both random and choice-based sampling and enables us to avoid
degeneracies and to apply a central limit theorem. Condition (ii) elaborates the asymptotic
linearity condition for the estimator of g(x). We assume p-nice trimming. The additional
condition on the trimming function is required to reduce the bias that arises in estimating
the support.
Note that there is no need for g(x) to be smooth. A smoothness condition on g(x)
is used solely to establish asymptotic linearity of the estimator of g(x). Also note that the
sampling theory above is obtained under mean independence

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

279

Vox= lim Var {E['l/ONON\(YOi, Xi, X) I Yo;, Di=O, X;, D= 1]ID= I}
No-+oo

= lim Var {E[Gi;«Xi-x)/aNo)I(XeS) I YOi, Di=O,Xi, D= 1]ID= I}.

f

aNo!x(XID=O) K(u)du

No-+oo

Now observe that conditioning on Xi and YOi, e, is given, so that we may write the last
expression as
Var {G.E[K«Xi-X)/aNo)I(XES) ID·=O X· D= I]ID= I}.
a~o!x(XID=O)JK(u)du

I

I

'"

E[K«Xi-X)/aNo)I(XES)ID.=O X· D=
a~o!x(XID=O)JK(u)du
I
'"

1]
,

can be written in the following way, making the change of variable (Xi-X)/aNo=w:

f

K (W)I ([ X i - aNoW] ES ) !(Xi-aNowID= 1) dw.

JK(u)du

!(Xi-aNowID=O)

Taking limits as No -+ 00, and using assumptions 3, 4 and 7, so we can take limits inside
the integral
lim E[KJ(Xi-x)/aNo)I(XES)IDi=O,Xi, D= I]=!(XiI D= 1) I(XiES),
No-+oo

aNo/x(XID=O)JK(u)du

!(X;jD=O)

since aNo -+0 and JK(w)dw/JK(u)du= 1. Thus, since we sample the Xi for which Di=O,

v; =E [var (YoiIXi, Di=O)/i(XdDi= 1) ID.=O] Pr {X.eSID.=O}.
ox
s
!i(XiIDi=O)
I
I
I

Hence the asymptotic variance of!x is, writing A=Pr {XeSID=O}/Pr (XeSID= 1),
Pr (XeSID= 1)-1 {Var, [E, (Y1- YO IX, D= 1) ID= 1]+ Es [Vars (Y1IX, D= 1) ID= 1]
+ABEs [Var (YoIX, D=O)!i(x/D= 1)//i(XID=O)/D=O]).
Similarly for !p, Vop is
Pr (XESID= 1)-1 {Vars [Es (Y1- YoIP(X), D= 1) ID= 1]
+ Es [Vars (Y1IP(X), D= 1) ID= 1]
+ ABEs [Var (YoIP(X), D=O)
x!; (P(X) ID= 1)/!;(P(X) ID=O)ID=O]}.
The first two terms for both variance expressions are the same as those that appear in Vx
and Vp in Theorem 1. To see that one variance is not necessarily smaller than the other,
consider the case where Ix(X ID = 1) = Ix(X ID = 0) and B = I. Clearly in this case
h(P(X) ID= 1) = h(P(X) ID=O). Propensity score matching has smaller variance if and

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Now

280

REVIEW OF ECONOMIC STUDIES

only if
Es {Es (YIIP(X), D= I)Es (YoIP(X), D= I) ID= I}

> Es {E; (YIIX, D= I)Es (Yo IX, D= I) ID= I}.
Since the inequality does not necessarily hold, the propensity score matching estimator in
itself does not necessarily improve upon the variance of the regular matching estimator. 17,18
(2) What are the effects on asymptotic bias and variance if we use an estimated value of P?

If/dNONlg( Ydj, P(Zj); P(z» + og(P(z»jop· If/NP(Dj, Zj; z),
for ie1d, d=O, I, where If/dNoNlg are the scores for estimating g(p) and If/NP is the score for
estimating P(z). By assumption (ii-a) they are not correlated with og(P(z»jop·
If/N~(Dj, Zj; z), and hence the variance of the sum of the scores is the sum of the variances
of each score. So the variance increases by the variance contribution of the score
og(P(z»jop· If/NP(Dj, Zj; z) when we use estimated, rather than known, P(z). Even with
the additional term, however, matching on X does not necessarily dominate matching on
P(X) because the additional term may be arbitrarily close to zero when og(P(z»jop is
close to zero.
(3) What are the benefits, if any, of econometric separability and exclusion restrictions on
the bias and variance of matching estimators?
We first consider exclusion restrictions in the estimation of P(x). Again we derive the
asymptotic variance formulae explicitly using a kernel regression estimator. Using Corollary I, the score function for estimating P(x) is
If/NP

j(X . D.· x) = ujK«X x)jaN )I(xeS)
J'
J'
a'fvfx(x)JK(u)du'

where Uj= Dj- E{Dj IXj}. Hence the variance contribution of estimation of P(z) without
imposing exclusion restrictions is

v2X = lim

Var {E[og(P(Z»jop

N-+oo

x al/ujK«Xj- X)jaN)I(XeS)j[fx(X)

JK(u)du] IDj, x.. D= I]}

= Es[Var (DjIXj)[og(P(Zj»jop]2. f~(XjID= l)jf~(Xj)][Pr {XjeS} r

l

.

Analogously, we define the variance contribution of estimating P(z) imposing exclusion
restrictions by V2Z • Observe that when Z is a subset of the variables in X, and when there
17.
18.
shows in
estimator

For example Es (Y] IX, D= 1)=Es (yo IX, D= I) or Es (Y.IX, D= 1)= -Es (YoIX, D= I) can hold.
In a later analysis, Hahn (1996) considers a special case of the models considered in this paper and
a model with no exclusion restrictions that when P is not known, the estimated propensity score
is efficient and that knowledge of P improves efficiency.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

When P(x) is estimated nonparametrically, the smaller bias that arises from matching on
the propensity score no longer holds true if estimation of P(x) is ad-dimensional nonparmetric estimation problem where d> 1. In addition, estimation of P(x) increases the
asymptotic variance. Lemma l.informs us that the score, when we use estimated P(z)
but no other conditioning variables, is

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

281

are exclusion restrictions so P(X)=P(Z) then one can show that V2Z~ V 2X • Thus, exclusion restrictions in estimating P(X) reduce the asymptotic variance of the matching estimator-an intuitively obvious result.
To show this first note that in this case Var (DIX)=Var (DIZ). Thus
[V2X - V2Z ] . Pr

{XES}

=Es{Var (DIZ)[og(P(Z»/opf· llfi(XID= 1)/ fi(X)] - [f~(ZID= 1)/f~(Z)]]}
=E {var (DIZ)[o (P(Z»/o ]2.
s
g
11

[f~(ZID= I)]. [E(fi(XIZ, D= 1) IZ)-l]}
f~(Z)

\

fi(XIZ)

=0.
Since the other variance terms are the same, imposing the exclusion restriction helps to
reduce the asymptotic variance by reducing the estimation error due to estimating the
propensity score. The same is true when we estimate the propensity score by a parametric
method. It is straightforward to show that, holding all other things constant, the lower
the dimension of Z, the less the variance in the matching estimator. Exclusion restrictions
in T also reduce the asymptotic variance of the matching estimator.
By the same argument, it follows that E{[fi(XID= 1)/ fi(XID=O)] -IID=O} ~O.
This implies that under homoskedasticity for Yo, the casef(XID=I)=f(XID=O) yields
the smallest variance.
We next examine the consequences of imposing an additive separability restriction
on the asymptotic distribution. We find that imposing additive separability does not necessarily lead to a gain in efficiency. This is so, even when the additively separable variables
are independent. We describe this using the estimators studied by Tjostheim and Auestad
(1994) and Linton and Nielsen (1995).19
They consider estimation of gl(X 1 ) , g2(X2) in

where x = (x I , X2). There are no overlapping variables among X I and X 2. In our context,
E(YIX=x) = g(t) + K(P(z» and E(YIX=x) is the parameter of interest. In order to focus
on the effect of imposing additive separability, we assume P(z) to be known so that we
write P for P(Z).
Their estimation method first estimates E{ YI T= t, P= p} = g(t) + K(p) non-parametrically, say by ~ {YIT=t, P=p}, and then integrates ~ {YIT=t, P=p} over p using
an estimated marginal distribution of P. Denote the estimator by g(t). Then under additive
separability, g(t) consistently estimates g(t) + E{K(P)}. Analogously one can integrate
~ {YI T= t, P= p} - g(t) over t using an estimated marginal distribution of T to obtain a
consistent estimator of K(p) - E{ K(P)}. We add the estimators to obtain the estimator
of E( YIX = x) that imposes additive separability. The contribution of estimation of the
regression function to asymptotic variance when T and P are independent and additive
19. The derivative of E{ YI T= t, P= p} with respect to p only depends on p if it is additively separable.
Fan, Hardle and Mammen (1996) exploits this property in their estimation. Using this estimator does not lead
to an improvement in efficiency, either.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

~Es {var (DIZ)[og(P(Z))/Op]2. f~~~~; I)} [Erxj.I(;I~; 1)14-1]}

282

REVIEW OF ECONOMIC STUDIES

separability is imposed, is Pr (XeS ID = I) -I times

OE {var (]I; IT P D=O)[f(PID= I) +f(TID= I)
S
so"
f(PID=O) f(TID=O)

1]2}.20

When additive separability is not used, it is Pr (XeSID= 1)-1 times

OE {var (]I; IT P D=O)ff(PID=I) .f(TID=I)]2}.
S
so"
l](PID=O)' f(TID=O)

8. SUMMARY AND CONCLUSION
This paper examines matching as an econometric method for evaluating social programmes. Matching is based on the assumption that conditioning on observables eliminates
selective differences between programme participants and nonparticipants that are not
correctly attributed to the programme being evaluated.
We present a framework to justify matching methods that allows analysts to exploit
exclusion restrictions and assumptions about additive separability. We then develop a
sampling theory for kernel-based matching methods that allows the matching variables to
be generated regressors produced from either parametric or nonparametric estimation
methods. We show that the matching method based on the propensity score does not
20. The derivation is straightforward but tedious. Use the asymptotic linear representation of the kernel
regression estimator and then obtain the asymptotic linear expression using it.
21. Let a(P) =f(PID= 1)/f(PID=O) and beT) =f(TID= 1)/f(TID=O) and define an interval B(T) =
[[l-b(T»)/[I+b(T»),1] when b(T)<1. If whenever b(T»I, a(P) > 1 and whenever b(T)<l,a(P)eB(T)
holds, imposing additive separability improves efficiency. On the other hand, if whenever b(T) > I, a( P) < I and
whenever beT) < I, a(P) lies outside the interval B(T), then imposing additive separability using the available
methods worsens efficiency even if the true model is additive.
22. The expression above implies that the same can be said for the estimator that is constructed without
imposing additive separability. However that result is an artifact of assuming independence of P(Z) and T.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Note that the first expression is not necessarily smaller, sincef(PID= I) . f(TID= I) can
be small without bothf(PID= I) andf(TID= I) being simultaneously small."
Imposing additive separability per se does not necessarily improve efficiency. This is
in contrast to the case of exclusion restrictions where imposing them always improved
efficiency. Whether there exists a method that improves efficiency by exploiting additive
separability is not known to us.
Note that whenf(PID= 1)=f(PID=O) andf(TID= 1)=f(TID=O) both hold, the
variance for the additively separable case and for the general case coincide. Under homoskedasticityof Yo, the most efficientcase arises when the distributions of (T, P(Z» given
D= 1 and (T, P(Z» given D=O coincide. In the additively separable case, only the marginal distributions of P(Z) and T respectively have to coincide, but the basic result is the
same.22
Note that nearest neighbour matching "automatically" imposes the restriction of
balancing the distributions of the data whereas kernel matching does not. While our
theorem does not justify the method of nearest neighbour matching, within a kernel
matching framework we may be able to reweight the kernel to enforce the restrictions that
the two distributions be the same. That is an open question which we will answer in our
future research. Note that we clearly need to reweight so that the homoskedasticity condition holds.

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

283

APPENDIX
In this Appendix we prove Corollary 1 by proving a more general result, Theorem 3 stated below, verify the
conditions of Lemma 13, for the case of a local polynomial regression estimator, and prove Theorem 2. We first
establish the property that local polynomial regression estimators are asymptotically linear with trimming.

A.l. Theorem 3
Theorem 3 will show that local polynomial regression estimators are asymptotically linear with trimming. Lemma
1 follows as a corollary.
The local polynomial regression estimator of a function and its derivatives is based on an idea of approximating the function at a point by a Taylor's series expansion and then estimating the coefficients using data in a
neighbourhood of the point. In order to present the results, therefore, we first develop a compact notation to write
a multivariate Taylor series expansion. Let X=(XI, ... , Xd) and q=(ql,"" qd)eR d where fJ.i (j= 1, ... , d) are
nonnegative integers. Also let x'!=x1' . ..x:¥/(ql! ... qd!)' Note that we include (qd ... qd!) in the definition. This
enables us to study the derivative of x q without introducing new notation; for example, ox'!/ ox. = xii where ij =
q
(ql - I, ... , qd), if ql ~ I and 0 otherwise. When the sum of the elements of q is s, x corresponds to a Taylor
series polynomial associated with the term irm(x)/(ox1 1 •• • ox:¥). In order to consider all polynomials that
correspond to s-th order derivatives we next define a vector whose elements are themselves distinct vectors of
nonnegative integers whose elements sum to s. We denote this row vector by Q(s) = «ql' ... ,qd»ql+'" +qd=S;
that is Q(s) is a row vector of length (s+d-I)!/[s!(d-I)!] whose typical element is a row vector (q., ... , qd),
which has arguments that sum to s. For concreteness we assume that {(q., ... , qd)} are ordered according to
j
the magnitude of
IOd- qj from largest to smallest. We define a row vector xQ(S) = (X(ql .....qd\I+···+qd- S' This
row vector corresponds to the polynomial terms of degree s. Let x Qp= (xQ(S»SE(O.I .... ,p}. This row vector represents
the whole polynomial up to degree p from lowest to the highest.
Also let m(S)(x) for s~ 1 denote a row vector whose typical element is irm(x)/(ox1' .. . ox:¥) where
ql + ... +qd=S and the elements are ordered in the same way {(ql' ... ,qd)} are ordered. We also write m(O)(x) =
m(x). Let P: (xo) = (m(O)(xo), ... , m(P)(xo»'. In this notation, Taylor's expansion of m(x) at Xo to order p without
the remainder term can now be written as (x-xo) QPf3:(xo).
We now define the local polynomial regression estimator with a global smoothing parameter ah.; where
ae[ao- 0, ao+ 0] for some ao>O and ao> 0>0. We denote d= [ao- 0, ao+ 0). Let Kh(s) = (ahn)-dK(s/(ah n»
and KhO(s) = (aohn)-d. K(s/(aohn», and let f3= (P'o, ... ,P~)" where P;is conformable with m(I)(xo),for 1=0, ... .p.

r:=1

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

necessarily reduce the asymptotic bias or the variance of estimators of M(S) compared
to traditional matching methods.
The advantage of using the propensity score is simplicity in estimation. When we use
the method of matching based on propensity scores, we can estimate treatment effects in
two stages. First we build a model that describes the programme participation decision.
Then we construct a model that describes outcomes. In this regard, matching mimics
features of the conventional econometric approach to selection bias. (Heckman and Robb
(1986) or Heckman, Ichimura, Smith and Todd (1998).)
A useful extension of our analysis would consider the small sample properties of
alternative estimators. In samples of the usual size in economics, cells will be small if
matching is made on a high-dimensional X. This problem is less likely to arise when
matching is on a single variable like P. This small sample virtue of propensity score
matching is not captured by our large sample theory. Intuitively, it appears that the less
data hungry propensity score method would be more efficient than a high dimensional
matching method.
Our sampling theory demonstrates the value of having the conditional distribution
of the regressors the same for D = 0 and D = I. This point is to be distinguished from the
requirement of a common support that is needed to justify the matching estimator.
Whether a weighting scheme can be developed to improve the asymptotic variance remains
to be investigated.

284

REVIEW OF ECONOMIC STUDIES

Also let Y=(Y1, ... , Yn)', W(xo)=diag (Kh(X1-XO),"" Kh(Xn-Xo», and
X I- .XO)QP)
Xp(xo) =

:

(

.

(Xn-Xo)Qp

Then the local polynomial regression estimator is defined as the solution to
min 'l.;=1 [Y,-(X;- xo)QPjJJ 2Kh(X;- xo),
p

or, more compactly
pp(xo)= arg min (Y- X, (xo)fJ)'W(xo)( Y - X p (xo)fJ).
p

m=(m(X1), ... , m(Xn»', rp(xo) = (rp(X), xo), ... , rp(X n, xo»', and S=(SI, ... , Sn)"
Let Mpn(xo) be the square matrix of size ~=o [(q+d-l)!/q! (d-l)!] denoting the expectation of Mpn(xo),
where the s-th row, r-th column "block" of Mpn(xo) matrix is, for O~s, t~p,

Ex {[«X - xo)/(ahn»Q(s-I)]'[«X - xo)/(ahn»Q(I-I)]Kh(X - xo)}.
Let limn.... 00 Mpn(xo)=Mp·f(xo)P+I. Note that M; only depends on K(·) when Xo is an interior point of the
support of X. Also write I;=I{X;ES} and I;=I{X;ES}, Io=I{xoES} and Io=I{xoES}. We prove the following
theorem.
Theorem 3. Suppose Assumptions 1-4 hold.
estimator of order p, mp(x), satisfies

If M;

is non-singular, then the local polynomial regression

[mj.i(xo) -m(xo)]Io=n- 1 I,7-1 S;K;h(X;- xo)Io+ b(xo) + R(xo),
where b(xo) =o(h~), n- I/2 'l.;= I R(X;) =0p(1), and
K;h(X;-XO)=(l, 0, ... ,0)' [Mj.in(xo)r '· [«X;-xo)/(aohn»Qp]'Kho(X;-xo).
Furthermore, suppose that Assumptions 5-7 hold. Then the local polynomial regression estimator of order
mp(x), satisfies
[mp(xo) -m(xo)]Io=n- 1
S;K;h(X;- xo)Io+ b(xo) + R(xo),

O~p<p,

r;=1

where
b(xo) = (aohnt el . [Mpn(XO)]-1

~=p+

I

[[f uQ(O) . uQ(S)m(S)(xo) . uQ{P-S)K(u)du,

...,f

uQ(P)· uQ(S)m(S)(xo) . uQ{P-S)K(u)du]f<P-S)(xo)']Io,

Fan (1993), Ruppert and Wand (1994), and Masry (1995) prove pointwise or uniform convergence property of the estimator. As for any other nonparametric estimator the convergence rate in this sense is slower than
I 2
n- / -rate. We prove that the averaged pointwise residuals converges to zero faster than n- I / 2-rate.
We only specify that M p is nonsingular because one can find different conditions on K(' ) to guarantee it.
For example, assuming that Ktu., ... , ud)=k(UI)'" k(Ud), if Jl.pk(s)ds> 0, then M p is nonsingular.
23. Ruppert and Wand (1994) develop multivariate version of the local linear estimator. Masry (1995)
develops multivariate general order local polynomial regression.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Clearly the estimator equals [X~(xo)W(xo)Xp(xo)rIX~(xo)W(xo)Y when the inverse exists. When p=O, the
estimator is the kernel regression estimator and when p = I the estimator is the local linear regression estimator. 23
Let H=diag (1, (ahn)-I ,d, ... , (ahn)-P'(p+d-I)!/(p!(d-I)!), where Is denotes a row vector of size s with I in
all arguments. Then Pp(xo)=H[Mpn(xo)-I]n-IH'X~(xo)W(xo)Y, where Mpn(xo)=n-IH'X~(xo)W(xo)Xp(xo)H.
Note that by Taylor's expansion of order p?:.p at Xo, m(X;) = (X;- xo)QPfJ*(xo) + rp(X;, xo), where
rp(X;, xo)=(X;-xo)Q(jil(m{P)(x;)-m{P)(xo)] and X; lies on a line between X; and xo. Write

285

MATCHING AS AN ESTIMATOR

HECKMAN ET AL.

To prove the theorem, note first that Y=m+ e=Xp(xo)P;(xo)+rp(Xi, xo)+e. We wish to consider the
situation where the order of polynomial terms included,p, is less than the underlying smoothness of the regression
function, p. For this purpose let Xp(xo) = [Xp(xo),Xp(xo)] and partition P; conformably: P;(xo)=
[P;(xo)', p;(xo)']', thus note that·
[Pp(xo) - P; (xo)]/o= H[Mpn (xo)rln-IH'X~(xo) W(xo)e .

/0

(A-3)

+ H[Mpn(xo)rln-IH'X~(xo) W(xo)Xp(xo)p; (xo)·

+H[Mpn(xo)rln-IH'X~(xo)W(xo)rp(xo)'

10

10 •

(B-3)
(C-3)

Lemma 2 (Term (A-3».

Under the assumptions of Theorem 3, the right-hand side of

(A-3)=el' [Mpn(Xo)rln-IH'X~(xo)W(xo)e'[o+RI(xo),
where el = (1,0, ... ,0) and n-

I/ 2

r,;=

I

RI(Xi) =op(l).

Proof. We first define neighbourhoods of functions
denote them by F,; jf, J, and d, respectively, where

el .

[Mpn(x)rl,fx(x), Iixe S), and point ao. We

f n = {rn(x); sup Irn (x) -el . [Mpn(x)rll ~ s.},
XES

for some small er>O, jf= {f(x);

SUPXES

If(x) - fx(x) I ~ Ef} for some small ef>O,

J= {[(xeS); S= {x; f(x) ~qo} for somef(x)ejf(x)},
and d=[ao-oa, ao+oa], where 0<Oa<ao.

24

Vsing the neighbourhoods we next define a class of functions ~In as follows:

~In= {gn; gn(Ei' Xi' Xj) =n- 3 / 2 • rn(Xj) . [[(X i- Xj)/(ahn)]Qp]'EiK,,(Xi- Xj) . t}
where it is indexed by a row vector-valued function rn(x)ef n, aed, which is also implicit in K,,('), and an
l
indicator function teJ. Let rnO(Xj)=el . [Mpn(Xj)r\ Yn(Xj)=el . [Mpn(Xj)r ,
gnO(ei, Xi' Xj) =n-

3 2
/ •

rnO(Xj) . [[(X i- Xj)!(aohn)]Qp]' eiK"O(Xi- Xj) . I j ,

and

K"(Xi- Xj) . i;

Kn(ej, Xi, Xj) =n- 3 / 2 • rn(Xj) . [[(X i- Xj)/(ahn)]Qp]' Ei

»

».

where we denote KhQ(Xi- Xj) = (aohn)-dK«Xi- Xj)/(aoh n and K,,(Xi- Xj) = (ahn)-dK«X j- Xj)!(ah n Then
since RI(xo)=gn(ej,Xi, xO)-gnO(ei'Xi, Xo), the result follows if two conditions are met: (1) equicontinuity of
the process
gn(ei' Xi' Xj) over ~In in a neighbourhood of gnO(ei'Xi' Xj) and (2) that, with probability
approaching 1, gn(ei'Xi,Xj) lies within the neighbourhood over which we establish equicontinuity. We use the
2'2- norm to examine (1). We verify both of these two conditions in turn.
We verify the equicontinuity condition (1) using a lemma in Ichimura (1995).25 We first define some
notation in order to state his lemma. For r = I and 2, let fIr denote the r-fold product space of ff' c R d and
define a class of functions IFn defined over fEr. For any 'l'ne'l'n, write 'l'n.;, as a short hand for either 'l'n(Xi)
or 'I'n (Xii' Xi2)' where i l #:lz- We define Un 'I'n= r,. 'IIn.; , where
denotes the summation over all permutations
of r elements of {XI, ... ,xn} for r= I or 2. The~r U; ~n is called V-process over 'l'ne'l'n. For r= 2 we assume
that 'lin( Xi, Xj) = v;(Xj, Xi)' Note that a normalizing constant is included as a part of v; A V-process is

r,;=1 'i;=1

r,.

"a

24. Note that a calculation using change of variables and the Lebesgue dominated convergence theorem
shows that on S, [Mpn(x)r l converges to a nonsingular matrix which only depends of K(·) times [1!f(x)Y+ I.
l
Hence, on S, each element of [Mpn(x)r is uniformly bounded. Thus use of the sup-norm is justified.
25. The result extends Nolan and Pollard (1987), Pollard (1990), Arcones and Gine (1993), and Sherman
(1994) by considering V-statistics of general order r~ I under inid sampling, allowing IF to depend on n. When
IF depends on n, we need to assume condition (ii) in the lemma below as noted by Pollard (1990) when r = I.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Note that if we use a p-th order polynomial when m(x) is p-th order continuously differentiable, that is p=p,
the second term (B-3) is zero.
Denote the first element of P(xo) by mp(xo). We establish asymptotic linearity of mp(xo) and uniform
consistency of its derivative. Lemma 2 shows that the first term (A-3) determines the asymptotic distribution,
Lemma 7 shows that the second term (B-3) determines the bias term, and the right hand side of 8 shows that
the third term (C-3) is of sufficiently small order to be negligible. Together, these lemmas prove the theorem.

286

REVIEW OF ECONOMIC STUDIES

called degenerate if all conditional expectations given other elements are zero. When r = 1, this condition is
defined so that E( '"II) = O.
We assume that '¥"c"p2(9'), where "p 2 (9 ' ) denotes the "p 2.space defined over X' using the product
measure of 9,9'. We denote the covering number using Z2-norm, 1\' liz, by N 2(s, 9, '¥,,).26
Lemma 3 (Equkontinuity). Let {Xi }7= I be an iid sequence of random variables generated by 9. For a
degenerate Usprocess {U""',,} over a separable class offunctions '¥"c Z2(9') suppose the following assumptions
hold (let II v- liz =
r E{"'''';r} ]IIZ);
(i) There exists an F"eZ 2(9') such that for any ",,,e'¥,,, I",,,I <F" such that lim sup".... co
L E{F~,;J < 00;
(ii) F;r each s>O, lim".... oo L E{F~,;r 1{F".ir> s}} =0;
(iii) There exists A( s) and E;0 such that for each e > 0 less than E,

[L

~

and

S: [log A. (X)]'/2dx < 00. Then for any s>O, there exists 0 >0 such that
lim Pr {

sup

I U,,("'I,,- "'2,,)1 > s} =0.

IllJIl.- ¥,:z,,1I2;:;;cS

Following the literature we call a function F"';r an envelope function of §i" if for any ",,,e§i,,, "''',lr~F''.ir holds.
In order to apply the lemma to the process L;= 1 L;=l s, (s., X/, Xj ) over r§I" in a neighbourhood of
g"o(S/, Xi' Xj), we first split the process into two parts; the process Lrg,,(s;, XI' Xj) =Lrg~(s;, XI' Sj, Xj), where
g~(s/, XI, Sj, Xj)= [g,,(£/, XI' Xj) +g,,(£jo Xj,X/)l/2,

and the process L;'-I g,,(£/,X;,Xi). Note that g,,(£;,Xi,Xi)=n-3/2. r,,(X/)' ei' e.: (ah,,)-dK(O)' 1; is a order
one process and has mean zero, hence it is a degenerate process. On the other hand g~ (s., X;, £j, Xj) is a order
two process and not degenerate, although it has mean zero and is symmetric. Instead of studying
g~(ei' Xj, ej, Xj) we study a sum of degenerate Ll-processes following Hoeffding (1961).27 Write Z/=
(Sj,X j), 4>,,(Z/)=E{g~(Zi' z)IZ;} =E {g~(z, Z/)IZj}, and
g~(Zi' Zj)=g~(Zj, Zj) - 4>,,(Z;)- tP,,(Zj)'

Then

where g~(Zj,~) and 2· (n-l) . tP,,(Zi) are degenerate Ll-processes. Hence we study the three degenerate Vprocesses: g~(Z/,Zj),2' (n-l)' q,,,(Z/), and g,,(£;, X;, X;}, by verifying the three conditions stated in the
equicontinuity lemma.
We start by verifying conditions (i) and (ii). An envelope function for g~(Zi' Zj) can be constructed by
the sum of envelope functions for gIl( e., Xi, X;) and gIl(s., Xi' X, ). Similarly an envelope function for
g~(Zi' Zj) can be constructed by the sum of envelope functions for g~(Z/, Zj) and 2· tP,,(Z;). Thus we only
need to construct envelope functions that satisfy conditions (i) and (ii) for gIl(£i' X;, X;), gIl (s., X;, X j), and
2· n- tP,,(Zi)'
Let n=l{fx(X;)~qo-2sf} for some qO>2ef>0. Since sUPxeslf(x)-fx(x)l~ef' I~~1; holds for any
teJ. Also for any neighbourhood of [Mp,,(x)r l defined by the sup-norm, there exists a C>O such that
I[Mp " (x)r l I ~C so that Ig"(£i,X;,X;)1 ~n-3IZ. C'le;! . [(ao-oa)h"rdK(O)' and the second moment of
the right hand side times n is uniformly bounded over n since the second moment of e, is finite and nh: -+ 00.
Hence condition (i) holds. Condition (ii) holds by an application of Lebesgue dominated convergence theorem
since nh: -+ 00.

n

26. For each e>O, the covering number Nr(e, 9, §i) is the smallest value of m for which there exist
functions gl, ... .s; (not necessarily in F) such that min, [E{lf-gjl'} I/r] ~ e for each fin F. If such m does
not exist then set the number to be 00. When the sup-norm is used to measure the distance in calculating the
covering number, we write Noo(s, §i).
27. See also Serfling (1980).

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

supN2 (s, 9 , '¥,,)~A(S)

HECKMAN ET AL.

287

MATCHING AS AN ESTIMATOR

Note that any element of [[(Xi- Xj)/(ah n)]Qp)'Kh(Xi - Xj) is bounded by C1 • [(ao- oa)hnr d
I{IIXi-Xjll ~C2' hn} for some C\ and C2. Thus
\gn(ei, Xi,Xj) \ ~n-3/2. I e;1 'C' C1 • [(ao- oa)hnrdl {IIXi - Xjll ~ C2· hn} .

tt .

Therefore, analogous to the previous derivation, conditions (i) and (ii) hold for gn(e;, Xi, Xj)'
Note further that since the density of x is bounded on S by some constant, say, C3>O, I tPn(ei,Xi)1 ~
n- 3/ 2'1 ed . C' C\ . C3 and hence 2· n- I tPn(ei' X;)\ has an envelope function n- I / 22 ' led' C' C\ . C3 that satisfies the two conditions.
To verify condition (iii), first note the following. Write Jh(Xi- Xj) = [[(Xi- Xj)/(ahn)]Qp)'Kh(Xi - Xj) and
JhQ(X;-Xj) = [[(Xi-Xj)/(aohn)]QP)'KhQ(Xi-Xj)' Using this notation

Ign(ei, Xi, Xj) -

gnO(e;,Xi, Xj)1
3 2
/ 1

ed 'Irn(Xj)' Jh(Xi-Xj)' ~-rnO(Xj)' JhQ(Xi-Xj) 'Ijl

~n-3/21 ed . Irn(Xj)- rnO(Xj) I . C\ . [(ao- Oa)hnrdI{IIXi- Xjll ~C2' hn} . Ij

+n- 3/ 2 1 e;I'lrnO(Xj)1

. I Jh(X;-Xj)-JhQ(Xi-Xj) I ·Ij•

+ n- 3/2I e;I·lrnO(Xj)I·IJhO(X;-Xj)I·I~-~I.

(L-3)

For gn(ei, X;, Xi) the right hand side is bounded by some C>O,

n- 3/ 2(hn)- dl ed' C' [Irn(Xi)-rnO(X;}I' (ao-Oa)-d!1+1 rnO(Xi)I·la-d-aodl·I~+lrnO(X;)I·lii-ld]].
Since nh~ -+ 00, the ~2-covering number for the class of functions denoted by gn(ei, Xi, Xi) for gne~\n can be
bounded above by the product of the covering numbers of r n, .91, and J. Since it is the log of the covering
number that needs to be integrable, if each of three spaces satisfy condition (iii), then this class will also. Clearly
.91 satisfies condition (iii). To see that T, and J do also, we use the following result by Kolmogorov and
Tihomirov (1961).28 First they define a class of functions for which the upper bound of the covering number is
obtained.
Definition 2. A function in 'I'(K) has smoothness q>O, where q=p+a with integer p and
for any xeK and x+heK, we have
yt(x+h) =~=o (k!)-IBk(h, x)

O<a~l,

if

+ R.,(h, x),

where Bk(h, x) is a homogenous form of degree k in hand IR.,(h, x)1 ~ C1lhll q , where C is a constant.

'1':

Let
(C) = {yte'l'(K): IR., (h, x) I ~ Cllhllq } .
If a function defined on K is p-times continuously differentiable and the p-th derivative satisfies Holder
(C) for
continuity with the exponent 0 < a ~ 1, then a Taylor expansion shows that the function belongs to
some C, where q=p+a.

'1':

Lemma 4 (K-T). For every set Ac'l':(C), where KcR d, we have, for O<d, q< 00,
log, Noo(e, A)~L(d, q, C, K)(1/e)d/ q
for some constant L(d, q, C, K»O.

Hence, because diq « 1, condition (iii) holds for I', and J. Analogously we can verify condition (iii) for
the remaining U-processes. Hence all three processes are equicontinuous.
The remaining task is to verify that gn(ei,Xi, Xj) lies in the neighbourhood of gnO(ei, X;, Xj) over which
we showed equicontinuity. By the inequality in Lemma 3 (L-3), this follows from Assumptions 3 and 5, and by
verifying that almost surely
sup

IIMpn(x) - Mpn(x)II-+O,

xeS,aed

where limn _ oo infx e s det (Mpn(x» >0. 29 The latter follows directly from the nonsingularity of matrix M, and the
trimming rule. Hence the following lemma completes the proof.
28. See pp. 308-314. Kolmogorov and Tihomirov present their result using the concept of packing number
instead of covering number.
29. Recall from the discussion of Theorem 3 that Mpn(x) depends on the parameter a.
A

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

=n-

288

REVIEW OF ECONOMIC STUDIES
Lemma S.

Under the assumptionsof Theorem 3, almost surely,
IIMpn(x)-Mpn(x)II-+O.

sup
xeS.aed

Proof

Note that any element of the matrix difference Mpn(x) - Mpn(x) has the form
n-

II;=I [(ahn)-dG«X j- xo)/(ahn» - E{ (ahn)-dG«X j- xo)/(ahn»} ],

where in the notation introduced in Section A.l the kernel function G(s) =sq· s' K(s) for some vectors q and r
whose elements are integers and they sum to p or less, where q and r depend on the element of M being
examined. To construct a proof, we use the following lemma of Pollard (1984).30
Lemma 6 (pollard).

For each n, let 'I' n be a separable class offunctions whose coveringnumbers satisfy
for

0< E< 1

fJ'

with constants A and W not depending on n. Let {~n} be a non-increasing sequence ofpositive numbersfor which
oo.lf Iytl ~ 1 and (E{ yt2})1/2~'nfor each yt in 'I'n, then, almost surely,

limn~oo n'~~;/log n =

sup In-I
'1'.

E'=

I

[yt(X j) - E{ yt(X j)} ]I/('~~n) -+ O.

To use this lemma, we need to calculate NdE,~, 'I'n). Let C1 ~SUPseR G(s), C2 is a Lipschitz constant for
G, and C3 is a number greater than the radius of a set that includes the support of G. In our application, recall
from our proof of Theorem 3 that ,r;;/= [ao- 8 a, ao + 8 a], where 0 < Oa < ao so that

E {laldG«x- xOl)/(a. hn» - aidG«x- x02)/(a 2hn» I}

s I«< aidl
~I

. E{ IG«x- xOI)/(al hn»l} + aid. E{ I G«x- xOI)/(al hn» - G«x- X(2)/(a2hn» I}

ald-aidl . C1

+ (ao- 8 a )- d . [C2 . (1- ada2) . [(ao+ 8a )/(ao- 8 a ) ] • C3 } • h+ C2 . IIxOl - x0211/(ao- 8 a ) ].
The upper bound of the right hand side does not depend on ~. Moreover, the right hand side can be made less
than E' C for some C> 0 by choosing I a 1- a21 ~ E and I XOI - x021 ~ E. Since Sand ,r;;/ are both bounded subsets
of a finite dimensional Euclidean space, the uniform covering number condition holds. To complete the proof
of Lemma 5, note that we are free to choose gn= 1 and 'n= C' h~/2 in our application of the lemma. II
Next we examine the second term (B-3).

Lemma 7 (Term (B-3».

Under the assumptionsof Theorem 3
(B-3) = b(xo) + R2(xo),

where
b(xo)=(aohntel' [Mp(XO)]-1 x

~=p+ .[fuQ(O). uQ(S)m(s)(xo)' uQ(ji-s)K(u)du, ... , f uQ(P) . uQ(s)m(S)(xo)' uQ(ji-s)K(u)du ]r-S)(xor]io,
n- I / 2

I ;=1R2(X ;}= op(l ), and R2(xo) is defined as the difference between Term (B-3) and bn(xo).
Proof

Note that

(B-3)=el' [Mpn(xo)rln-·H'X~(xo)W(xo).Xp(xo)p-;(xo)·
l,
I
l
=el' [Mpn(xo)r ~=P+I n- I;-I [[(Xj-xo)/(ahn)]Qp]'(Xj-xo)Q(S)m(s)(xo)Kh(Xj-XO)' i

o

=el' [Mpn(xo)r l I~_p+1 n- I L;_I [[[(Xj-xo)/(ahn)]Qp]'(Xj-xo)Q(S)Kh(Xj-XO)
- E ([[(X;- xo)/(ahn)]Qp]'(X;-xo)Q(S)Kh(X;- xo) Ixo} ]m(S)(xo)' i o
+el' [Mpn(xo)r l ~=P+I E{[[(X;-xo)/(ahn)]Qp]'(X;-xo)Q(s)Kh(X;-xo)lxo}m(S)(xo)'

(L-7A)

i;

(L-7B)

30. His lemma only requires that !F n is a permissible class of functions. In our applications !F n is always
separable. His Example 38, pp. 35-36, gives a similar result. We provide a proof here for completeness and for
later reference.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

sup NI(E,~, 'I'n)~AE-w

HECKMAN ET AL.

term (L-7A) as R2 1 (Xo). We apply the same method as in Lemma
L:=l R 2 1 (Xd =op(1). Instead of ~ln, define the class of functions ~2n as follows:

Define
n-

I 2
/

~2n= {gn; gn(X;, Xj) =n- 3 / 2

289

MATCHING AS AN ESTIMATOR

•

2 to

show

that

Yn(Xj) . [[[(X;- Xj)/(ahn)]Qp]'(X;- x, )Q(S)«, (X;- Xj)

- E {[[(X;- Xj)/(ahn)]Qp]'(X;- Xj)Q(s)Kh(X;- Xj) IXj} ]m(s)(Xj) . ~,

where it is indexed by a row vector-valued function Yn(x)ern, aed, which is also implicit in K h(·), and an
indicator function ~eJ. Let
gnO(X;, Xj) =n- 3 / 2 • YnO(Xj) . [[(X;- Xj)/(aohn)]Qp]'(X;- Xj)Q(S)m(S)(Xj)KhO(X;-X j) . ~,

and
gn(X;, Xj) =n- 3 / 2

•

rn(Xj) . [[(X;- Xj)/(ahn)]Qp]'(X;- Xj)Q(s)m(s)(Xj )Kh(X;- X j) .

i;

assumption that the density of x is p-times continuously differentiable and its p-th derivative satisfies a Holder
condition. Using a change of variable calculation and the Lebesgue dominated convergence theorem, and Lemma
5 the result follows. I
Note that this term converges to zero with the specified rate only if Xo is an interior point. Thus for kernel
regression estimators for higher dimensions, we need to introduce a special trimming method to guarantee this.
Use of higher order local polynomial regression alleviates the problem for higher dimensional problems, but at
the price of requiring more data locally.

Lemma 8 (Tenn (C-3».

Under the assumptions of Theorem 3,

(C-3) = op(h~).
Proof

Recall that the third term equals el . [Mpn(xo)]-ln-lH'X~(xo)W(xo)rp(xo)'

10 • Note that

n-lIIH'X~(xo)W(xo)rp(xo) . i, II
=n- ilL;: I [(X;- xo)/(ahn)]Qp[(X;- xo)/hn]Q{P)[m(fi)(x;) -m(.ii)(xo)]Kh(xo- Xi)h~ II
I

~n-lL:=1

II [(X;-xo)/(ahn)]QP I[(X;-xo)/hn]Q(fi) I 'IKh(xo-X;)lllo(h~)

=op(h~),

where the inequality follows from the HOlder condition on m(fi)(xo) and the compact support condition on K( . ),
and the last equality follows from the same reasoning used to prove Lemma 5. The conclusion follows from
Lemma 5 and the assumption of nonsingularity of Mp(xo). I
Lemmas 2-8 verify Theorem 3. Corollary 1 in the text follows as a special case for p = o.

A.3.

Verifying the assumptions of Lemma 1

Five assumptions in Lemma 1 are as follows:
(i) Both P(z) and g(t,p) are asymptotically linear with trimming where
[P(z) - P(z)]I(xeS) =n[t(t,p)-g(t,p)]I(xeS)=n-

1

I

r.;= IY'np(DJ> Zj; z) + bp(z) + Rp(z),

r.;_1 Y'ng(lj, 1j, P(Zj); t,p)+bg(t,p)+ Rg(t,p);

(ii) og(t,p)/op and P(z) converge uniformly to og(t,p)/op and P(z), respectively, and that og(t,p)/op is
continuous for all t and p;
I 2
I 2
(iii) plim, co n- /
bg(T;, X;) =bg and plim,.... oo n- /
og(T;, P(Z;»/op . bp(T;, P(Z;» = bgp ;

r.:=1
r.:=1
I/2r.:_
1[og(T;, PTj(Z;»/op-og(T;, P(Z;»/op]' Rp(Z;)=O;
n- r.:=1 r.;=1 [og(T;, PTi(Z;»/op- og(T;, P(Zd)/op] . Y'np(Dj, z., Z;)=O.

(iv) plim, ""n-

(v) plim,

co

3 2
/

We verify these assumptions for the local polynomial regression estimator. Condition (i) is just a consequence of Theorem 3. We next show that the derivative of the local polynomial regression estimator converges
uniformly to the derivative of the limit of the local polynomial regression estimator.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

To prove that term (L-7B) equals b(xo)+o(h~) we use the assumption that all the moments of K(·) of order
p + 1 and higher up to p have mean zero, that Xo is an interior point of the support of x that is more than
(ao + 8 )hn C interior to the closest edge of the support, where C is the radius of the support of K(' ), and the

290

REVIEW OF ECONOMIC STUDIES
11Ieorem 4.

If Assumptions 1-4, and 8 hold, then og(t,p)/op is uniformly consistentfor og(t,p)/op.

Proof For convenience we drop the subscriptsp,p, and the argument Xo of Xp(xo), Pp(xo), P;(xo), and
W(xo) here so that X=Xp(xo), P= Pp(xo), P* = P; (xo), and W= W(xo). Also denote the derivative with respect
to the first argument of Xo by V. Note that X'WY=X'WXp. Hence by the chain rule,
VP=(X'WX)-I{V(X'W)Y-[V(X'WX)]P}
= (X'WX)-I {V(X'W) - [V(X'WX)](X'WX)-IX'W} Y
= (X'WX)-I {V(X'W) - [VX'WX)](X'WX)-IX'W}(Xp* + r+ E).

Since
{V(X'W)-[V(X'WX)](X'WX)-IX'W}XP*

= -X'W(VX)p*,

we obtain

Vp= -(X'WX)-I(X'WVX)P*
+ (X'WX)-I{V(X'W)-[V(X'WX)](X'WX)-IX'W}r
+(X'WX)-I{V(X'W)-[V(X'WX)](X'WX)-IX'W}E.

Note that for

s~ 1,

Vex- xo)Q(S) = (V(x - XO)(ql .....qd»ql + ... +qr s
= «x- xo)(%····qd)l(ql ~ l»ql + ... +qd-s - I
= [«x-XO)(ql .....qd»ql+···+qd- s -

h

0, ... ,0],

where the second equality follows from our convention on the order of the elements.
Thus
X = (l
VX=-(O

(x - xo)Q(I) (x - xo)Q(Z)

1 0

...

(x - xo)Q(P»

... 0 (x-XO)Q(I) 0 ... 0 ...

(x-XO)Q(P-I)

0 ... 0).

Note that each column of VX is either a column of X or the column with all elements being O. Hence there
exists a matrix J such that VX= -XJ, where J is a matrix that selects appropriate column of X or the zero
column. Without being more specific about the exact form of J we can see that
- (X'WX)-I(X'WVX)P* =JP*, and also that
(X'WX)-I{V(X'W)-[V(X'WX)](X'WX)-IX'W}
= (X' WX)-I{(VX') W+ X'(VW)

- (VX') WX(X' WX) -IX' W - X'(V W)X(X' WX)-IX' W - X' W(VX)(X' WX)-IX' W}
=(X'WX)-I{ -J'X'W+X'(VW)
+J'X'W- X'(VW)X(X'WX)-IX'W+ X'WXJ(X'WX)-IX'W}
= (X'WX)-I {X'(VW) - [X'(VW)X](X'WX)-IX'W} +J(X'WX)-IX'W}.

Next, to simplify the last expression, we use some specific properties of matrix J. The key properties of J we
use are that all elements of the first column are 0 and that the first element of the second column of J is 1. That
is, the first column of AJ is always the O-vector, regardless of A and the second column of AJ is the first column
of A. Since the first column is chosen by J exactly once, the preceding observations also imply that the first row
of J is ez, where ez= (0,1,0, ... ,0) if p~ 1 and 0, if p=O. Therefore
el . Vp=Vmp(x)=el . JP*

+(el' (X'WX)-I{(X'(VW)-[X'(VW)X](X'WX)-IX'W} <e«: X'W)r
+(el' (X'WX)-I{(X'(VW)-[X'(VW)X](X'WX)-IX'W}

r

ez: X'W)E,

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

=V(X'W)Xp* -V(X'WX)p*
= {(V X') WX + X'(V W)X - (VX') WX - X'(V W)X - X' W(VX)} P*

MATCHING AS AN ESTIMATOR

HECKMAN ET AL.

291

where el . Jp. = Vm(x). That the remaining two terms converge uniformly to zero can be shown analogously as
in Lemma 5. II
Condition (iii) of Lemma I clearly holds under an i.i.d. assumption given the bias function defined in
Theorem 3. In order to verify condition (iv) of the lemma recall the definition of the residual terms and use the
same equicontinuity argument as in the proof of Theorem 3.
Finally condition (v) can be verified by invoking the equicontinuity lemma. This is where the additional
smoothness condition is required.
Armed with these results, we finally turn to the proof of key Theorem 2.

A.3. Proof of Theorem 2
Note first that, writing I;=I(X;ES),
N- I

I

E(Y _ Yo ID= I)]
S

[.

"

I

L.;e/l •

I

0

We first consider the numerator and then turn to the denominator of the expression. Note that the numerator
of the right-hand side of (T-I) is the sum of three terms (TR-I)-(TR-3): writinggl(x)=Es(YdD=I,X=x),
(TR_I)=N I I / 2 "L..e. I I [YIi-gl(X;)]I;,
(TR-2)=N I I / 2 L;e/l {[gl(X;} -g(X;)] - Es(YI - YoID= I)}I;,
(TR-3)=N I I / 2 "L...e I I [g(X;)-g(X;)]I;.
Terms (TR-I) and (TR-2) are analogous to terms we examined in Theorem 1. Term (TR-3) is the additional
term that arises from estimating g(x). However, the first two terms from Theorem I have to be modified to
allow for the trimming function introduced to control the impact of the estimation error of g(x).
Central limit theorems do not apply directly to the sums in (TR-I) and (TR-2) because the trimming
function depends on all data and this creates correlation across all i. Instead, writing 1;=I(X;ES), we show that
these terms can be written as

and

respectively. One can use the equicontinuity lemma and our assumption of p-nice trimming to show the result
for term (TR-l). The same method does not apply for term (TR-2), however. This is because when we take an
indicator function t, from J, where #= I;, then

t.

It is necessary to recenter this expression to adjust for the bias that arises from using t;
In order to achieve this we observe that, writing As (X;) = [gl (X;) - g(X;)] - Es ( YI - Yo ID = I),
NI

I 2
/ " . I

i-J' E

I

As(X;)' I;=N I I / 2 "~'E. I I As(X;)' I;

+NII/2L;e/l As(X/)' [6(X/)r l
-1/2"

+N I

A

L.;e/l As(X;)' [u(X;}]

-I

•

K_~(:~~~qO)[J(X;)-f(X;}]I{](X;»f(X;)}
- (f(X;)-qo)
. K+ 6(X;) [f(X/)-f(X;)]I{f(X;)~f(X;}},
A

A

where ~(X;)= I](X;) - /(X;)I, K_(s) = I if -I ~s<O and 0 otherwise, and K+(s) = I ifO~s< I and 0 otherwise.
Since f(X;) = (Noan)-dLje/o K«Xj - X;}/a n), the latter two terms can be expressed as double sums. We then
apply an equicontinuity argument to the expressions
N- I / 2(N, a )-d"
IOn

"

L./e/, L.je/o

A (X). [6(X)]-1 .
S,

•

K_ (f(X;)
- qo)
\
6(X;)

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

N I / 2 [NIl Lie'l [YIi-!(Xi)]I;

292

REVIEW OF ECONOMIC STUDIES

and
N- 1/2(N, a )-d".
IOn

".

i....e/l i...Je/o

Ii (X.)· [u(X.)]-1 . K (f(X;)-qo)
S I
I
+ \ u(X;)

x [K«Xj- X;)/a n)- E{K«Xj- X;)/a n) IX;}]I {J(X;)~f(X;)}

and control the bias by ft-smoothness of f(X;).
Finally, term (TR-3) can be written as the sum of three terms
Nlt/2Not L;e/, Lje/0 V'ONoNl(YOj,Xj;X;)+NI3/2L;e/l Lje/! V'INoNl(YOj,Xj;X;),
-1/2

Nt

-

L;eIl bg(X;),

(TR-3-1)
(TR-3-2)

and

Terms (TR-3-2) and (TR3-3) are op(1) by the definition of asymptotic linearity of g(X;). Term (TR-3-1) is a
U-statistic and a central limit theorem can be obtained for it using the lemmas of Hoeffding (1948) and Powell,
Stock, and Stoker (1989) and a two-sample extension of the projection lemma as in Serfting (1980). We first
present the Hoeffding, Powell, Stock, and Stoker result.

We also make use of the results of Serfting (1980).

to (Serfling).

Suppose {Zo.;};e/oand {Zt,;}ie/t are independent and within each group they are i.i.d.,
Uno. n! V'no.nl = (no' nt)-I L;e/o Lje/, V'no.n, (Zo;, Ztj), andE{ V'no,nl (Zo;, ZIj)} =O,and
Uno .nl V'no.nl =nol L;e/oPOno.nl (Zo;) + nIl LjeIl Ptno,lll (ZIj), wherefor k=O, I, pkno,nl (Zkj) = E{V'no,nl (Zo;, ZI;) IZk;}'
If O<lim n _ oo n./no= 71<00, where n=no+nl
and
E{V'no,llI(Zo;,ZIj)2}=0(no)+o(nt),
then
2
nE[( Uno,nl V'no.nl - Uno,nl V'no,nl) ] = 0(1).
Lemma

In order to apply the lemmas to term (TR-3-1), note that it can be written as
3/2
N I L;e/l Lje/ l •j ..; '1'1 NoNI (Ylj, X j; X;)
3/2
+ N I L;eI! V'INoNl (YI;,X;; X;)
1/2 I
+N 1 N O L;e/l Lje/o V'ONoNl (YOj,Xj; X;).
I/2
Term (TR-3-la) can be rewritten as N I Lie/ l Lje/lJ ..i V'?NoNl (YIj, X, ;Xi; Y\i,X; ;Xj) where

(TR-3-la)
(TR-3-lb)
(TR-3-lc)

V'?NoNI (YIj,Xj; X;; Y Ii, X;; Xj)=[V'INoNl (Ytjo Xj; X;)+ V'tNoNl (YIi, Xi; Xj )l/2.

Thus by Lemma 9 and assumption (ii-a), term (TR-3-la) is asymptotically equivalent to
I/2
N I L;e/, E {lfIINoNI (Y\i, X;; Xj) I YJj, Xi}'
By assumption (ii-a), term (TR-3-1b) is op(l), By Lemma 10 and assumption (ii-a), term (TR-3-lc) is asymptotically equivalent to
2N i
NV o Lje/oE{V'ONoNl (YOi' Xi; Xi) I YOj, Xi}'
Hence putting these three results together, term (TR-3-1) is asymptotically equivalent to
t/2
2N i
N l L;e/l E{ '1'1 NoN1 (YJj, X;; XJ> I YIi,X;} + NV o LieIo E{ V'ONoN1 (YO
i' x, ;Xi)1 YOj> Xi}'
Collecting all these results, we have established the asymptotic normality of the numerator.
I I (/;-1;). The
We next examine the denominator of (T-l). Note that NIl L·Ie I I li=N I I ".
i...,e I 1 li+NI ".
i.../e 1
first term on the right-hand side converges in probability to E(I) by the law of large numbers. To see that the
second term is op(1), note that NltILie/!(/;-I;)I~NltLie/llli-/d. The Markov inequality implies for any

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

(TR-3-3)

HECKMAN ET AL.

MATCHING AS AN ESTIMATOR

293

e>O, Pr {Nil Lid! 'Ii - Iii> s} ~E{I Ii - IiI}/ e. Hence assumption (ii-d) implies that the second term is op(l).
This result, in conjunction with our result for the denominator, proves Theorem 2. II

REFERENCES
ARCONES, M. A. and GINE, E. (1993), "Limit Theorems for If-processes", Annals of Probability, 21, 14941542.
BARNOW, B., CAIN, G. and GOLDBERGER, A. (1980), "Issues in the Analysis of Selectivity Bias," in E.
Stromsdorfer and G. Farkas (eds.), Evaluation Studies Review Annual, Volume 5 (San Francisco: Sage).
BARROS, R. (1986), "Nonparametric Estimation of Causal Effects in Observational Studies" (University of
Chicago, mimeo).
COCHRANE, W. G. and RUBIN, D. B. (1973), "Controlling Bias In Observational Studies", Sankyha, 3S,
417-446.
DAWID, A. P. (1979), "Conditional Independence in Statistical Theory", Journal of The Royal Statistical
Society Series B, 41, 1-31.
FAN, J. (1993), "Local Linear Regression Smoothers and Their Minimax Efficiencies", The Annals ofStatistics,
21, 196-216.
FAN, J., HARDLE, W. and MAMMEN, E. (1996), "Direct Estimation of Low Dimensional Components in
Additive Models" (Working paper).
HAHN, J. (1996), "On the Role of the Propensity Score in the Efficient Semiparametric Estimation of Average
Treatment Effects" (unpublished manuscript, University of Pennsylvania).
HECKMAN, J. (1974), "Shadow Prices, Market Wages, and Labor Supply", Econometrica, 42, 679-694.
HECKMAN, J. (1990), "Varieties of Selection Bias", American Economic Review, SO, 313-318.
HECKMAN, J. (1997), "Instrumental Variables: A Study of the Implicit Assumptions Underlying One Widely
Used Estimator for Program Evaluations", Journal of Human Resources, 32, 441-462.
HECKMAN, J., ICHIMURA, H., SMITH, J. and TODD, P. (1996a), "Nonparametric Characterization of
Selection Bias Using Experimental Data, Part II: Econometric Theory and Monte Carlo Evidence"
(Unpublished manuscript, University of Chicago).
HECKMAN, J., ICHIMURA, H., SMITH, J. and TODD, P. (l996b), "Sources of Selection Bias in Evaluating
Programs: An Interpretation of Conventional Measures and Evidence on the Effectiveness of Matching
As A Program Evaluation Method", Proceedings of The National Academy of Sciences, 93, 13416-13420.
HECKMAN, J., ICHIMURA, H., SMITH, 1. and TODD, P. (1998), "Characterizing Selection Bias Using
Experimental Data", Econometrica (forthcoming).
HECKMAN, J., ICHIMURA, H. and TODD, P. (1997), "Matching As An Econometric Evaluation Estimator:
Evidence from Evaluating a Job Training Program", Review of Economic Studies, 64,605-654.
HECKMAN, J. and ROBB, R. (1985), "Alternative Methods For Evaluating The Impact of Interventions", in
J. Heckman and B. Singer (eds.), Longitudinal Analysis of Labor Market Data (Cambridge: Cambridge
University Press).
HECKMAN, J. and ROBB, R. (1986), "Alternative Method For Solving the Problem of Selection Bias in
Evaluating The Impact of Treatments on Outcomes," in H. Wainer (ed.), Drawing Inferences from SelfSelected Samples (New York: Springer-Verlag).
HECKMAN, J. and SMITH, J. (1998), "Evaluating The Welfare State", Frisch Centenary, Econometric Monograph Series (Cambridge: Cambridge University Press), forthcoming.
HECKMAN, 1., SMITH, J. and CLEMENTS, N. (1997), "Making the Most Out of Social Experiments:
Reducing the Intrinsic Uncertainty in Evidence From Randomized Trials With An Application to the
National JTPA Experiment", Review of Economic Studies, 64, 487-535.
ICHIMURA, H. (1993), "Semiparametric Least Squares (SLS) and Weighted SLS Estimation of Single Index
Models", Journal of Econometrics, S8, 71-120.
ICHIMURA, H. (1995), "Asymptotic Distribution of Nonparametric and Semiparametric Estimators with
Data-Dependent Smoothing Parameters" (Unpublished manuscript, University of Chicago).
KOLMOGOROV, A. N. and TIHOMIROV, V. M. (1961), "s-Entropy and s-Capacity of Sets in Functional
Spaces", American Mathematical Society Translations, series 2, 17,277-364.

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

Acknowledgements. The work reported here is a distant outgrowth of numerous conversations with
Ricardo Barros, Bo Honore, and Richard Robb. We thank Manuel Arellano and three referees for helpful
comments. An earlier version of this paper "Matching As An Evaluation Estimator: Theory and Evidence on
Its Performance applied to the ITPA Program, Part I. Theory and Methods" was presented at the Review of
Economic Studies conference on evaluation research in Madrid in September, 1993. This paper was also presented
by Heckman in his Harris Lectures at Harvard, November, 1995, at the Latin American Econometric Society
meeting in Caracas, Venezuela, August, 1994; at the Rand Corporation, U.C. Irvine, U.S.C., U.C. Riverside,
and U.C. San Diego September, 1994; at Princeton, October, 1994; at UCL London, November, 1994 and
November, 1996, and at Texas, Austin, March, 1996 and at the Econometric Society meetings in San Francisco,
January, 1996.

294

REVIEW OF ECONOMIC STUDIES

Downloaded from http://restud.oxfordjournals.org/ at University Freiberg on November 20, 2012

LINTON, O. and NIELSON, J. P. (1995), "A Kernel Method of Estimating Structural Nonparametric Regression Based on Marginal Integration", Biometrika, 82, 93-100.
NOLAN, D. and POLLARD, D. (1987), "U-processes: Rates of Convergence", Annal of Statistics, 15, 405414.
NEWEY, W. K. and McFADDEN, D. L. (1994), "Large Sample Estimation And Hypothesis Testing", in R.
F. Engle and D. L. McFadden (eds.), Handbook of Econometrics. Vol. IV. (Amsterdam: Elsevier).
MASRY, E. (1995), "Multivariate Local Polynomial Regression for Time Series" (Unpublished manuscript).
POLLARD, D. (1990), Empirical Processes: Theory and Applications (Hayward: IMS).
POWELL, J. L., STOCK, J. H. and STOKER, I. M. (1989), "Semiparametric Estimation oflndex Coefficients",
Econometrica, 57, 1403-1430.
ROSENBAUM, P. and RUBIN, D. B. (1983), "The Central Role of the Propensity Score in Observational
Studies for Causal Effects", Biometrika, 70, 41-55.
RUPPERT, D. and WAND, M. P. (1994), "Multivariate Locally Weighted Least Squares Regression", The
Annals of Statistics, 22, 1346-1370.
SERFLING, R. J. (1980), Approximation Theorems of Mathematical Statistics (New York: Wiley).
SHERMAN, R. P. (1994), "Maximal Inequalities for Degenerate U-processes with Applications to Optimization
Estimators", Annal of Statistics, 22,439--459.
STONE, C. (1982), "Optimal Global Rates of Convergence For Nonparametric Regression", Annals ofStatistics,
10, 1040-1053.
TJOSTHEIM, D. and AUESTAD, B. H. (1994), "Nonparametric Identification of Nonlinear Time Series:
Projections", Journal of the American Statistical Association, 89, 1398-1409.
WESTAT, INC. (1980), "Net Impact Report No.1, Impact on 1977 Earnings of New FY 1976 CETA Enrollees
in Selected Program Activities" (Rockville, Maryland, Westat Inc.).
WESTAT, INC. (1982), "CLMS Follow up Report No.7 (18 Months After Entry), Postprogram Experiences
and Pre/Post Comparisons For Terminees Who Entered CETA During July 1976 Through September
1977" (Rockville, Maryland, Westat Inc.).
WESTAT, INC. (1984), "Summary of Net Impact Results" (Rockville, Maryland, Westat Inc.).

