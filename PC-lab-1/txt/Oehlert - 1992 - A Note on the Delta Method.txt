A Note on the Delta Method
Author(s): Gary W. Oehlert
Source: The American Statistician, Vol. 46, No. 1 (Feb., 1992), pp. 27-29
Published by: Taylor & Francis, Ltd. on behalf of the American Statistical Association
Stable URL: https://www.jstor.org/stable/2684406
Accessed: 15-11-2019 19:29 UTC
REFERENCES
Linked references are available on JSTOR for this article:
https://www.jstor.org/stable/2684406?seq=1&cid=pdf-reference#references_tab_contents
You may need to log in to JSTOR to access the linked references.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

Taylor & Francis, Ltd., American Statistical Association are collaborating with JSTOR to
digitize, preserve and extend access to The American Statistician

This content downloaded from 206.253.207.235 on Fri, 15 Nov 2019 19:29:19 UTC
All use subject to https://about.jstor.org/terms

A Note on the Delta Method
GARY W. OEHLERT*

1. THE DELTA METHOD

The delta method is an intuitive technique for approxi-

mating the moments of functions of random variables.
This note reviews the delta method and conditions under
which delta-method approximate moments are accurate.
KEY WORDS: Approximate moments; Asymptotic approximation; Taylor series.

For linear functions g of a random variable X, we have

We begin with some notation for observations and
moments. We have X, a random variable with distri-

bution F, and xl, x2, ..., xn, an iid sample from F. Let

,Uj be the jth population moment of X, pUj = E[Xi], where
the mean may be denoted by ,u l or just ,ut. Let ,j7 be the

jth population central moment of X, Aji = E[(XDenote the jth sample moment by mj,,,, mj,, = I
xjln, and the jth sample central moment by ni, Mii> n
i= (xi - x)In. The second subscript showing the sam-

that
E[g(X)] = g(E[X]) (1)

(assuming that X has a finite expectation). A common

mistake for beginning statistics students is to assume that
Equation (1) holds for all functions g. This is not true,
of course, but students quickly learn that exact compu-

tation of E[g(X)] is often possible only for the simplest
functions g and random variables X. The delta method
is a technique for approximating expected values of
functions of random variables when direct evaluation of

the expectation is not feasible.
The delta method approximates the expectation of g(X)
by taking the expectation of a polynomial approximation
to g(X). The polynomial is usually a truncated Taylor
series centered at the mean of X. This kind of approximation is intuitively appealing and often gives good results. However, good results are not guaranteed. Section

ple size may be dropped from time to time when no confusion will result.
The first fairly rigorous delta method appeared in

Cramer (1946 p. 353). Cramer stated the result for

g(Qiin, i n), a function of two sample central moments
that depends on the sample size n only through the sample moments themselves, but Cramer noted that the same

proof can be used for functions of any number of central

moments or of xi. Cramer assumed that g is twice continuously differentiable in a neighborhood of the popu-

lation moments ji and ji- and bounded by CnP for positive constants C and p. Cramer did not list explicit

assumptions about X, but earlier in the chapter he stated
that X will be assumed to have enough finite moments
so that the formulas are correct. Cramer's conclusion was
that

E[g(Mi,l, ii,i)] = g(jiZ, ,iy) + O(n-'). (2)

1 of this note reviews some of the conditions under which

the delta method will give accurate results. We conclude
with some examples in Section 2.
We note that the term delta method is also used for a

Thus, up to an error of order 1 /n, the order of the function g and the expectation operator E may be inter-

changed.

related technique, wherein we compute the moments of

Hurt (1976) extended Cramer's results by (a) allowing

an approximating asymptotic distribution. See, for ex-

more general random variables as arguments to g, (b)

ample, Rao (1965 p. 319) or Bishop, Fienberg, and Hol-

allowing the function g to depend on n, and (c) taking

land (1975 p. 486). In this note, however, we are con-

more terms in the Taylor series approximation of g. Hurt

cerned with approximating the moments of a random

considered a sequence of random variables W,n consistent

variable. This is not the same as finding the moments of

for w such that

an approximating distribution.
The delta method approximates the expected value of

a function by the expected value of an approximation to

the function. Alternatively, we could approximate the
expected value by approximating the distribution with
respect to which the expectation is taken. This second

ELW, - W12(q+l)= =(n-(q + l))
Note, in particular, that this is true for sample moments

mi,, provided that X has 2j(q + 1) finite moments (Loeve
1977 p. 276):

E|mjn -jl2(q+ l) - 0(n-(qn l))

approach is the subject of asymptotic expansions [see

The function g is allowed to depend on the sample size

Bhattacharya and Rao (1976) for a thorough treatment

n but must still be smooth and bounded. Hurt assumed

of this subject]. The results given here can be proved

that g(n, Wn) is q + 1 times differentiable with respect

using asymptotic expansion techiques (see Oehlert 1981),

to W, in an interval around w, that g is bounded, and

but the proofs are not as straightforward as those given

that the first q + 1 derivatives of g are bounded in a

here.

neighborhood of w. Hurt's conclusion was that

E[g(n, W,,)] = g(n, w)
*Gary W. Oehlert is Associate Professor, Department of Applied
Statistics, University of Minnesota, St. Paul, 55108. The author thanks
two anonymous referees for comments that improved the readability
of this article.

(C 1992 American Statistical Association

+ E g(n, w) E(W W)
j=1 J!

+ 0(n-(q+1)/2 (3)
The American Statistician, February 1992, Vol. 46, No. 1 27

This content downloaded from 206.253.207.235 on Fri, 15 Nov 2019 19:29:19 UTC
All use subject to https://about.jstor.org/terms

Proof. The proof is straightforward, given the fact that

Cramer's result corresponds to the case where q = 1 and

if a sequence of random variables converges to zero in

g does not depend on n.

Lehmann (1983 p. 106, Theorem 5.1) gave the special

probability and the sequence of (1 + E)th absolute mo-

case of Hurt's result where W,, = xi, q = 3, and g does

ments is bounded for some e greater than zero, then the

not depend on n. (Theorem 5. lb on page 109 allows a

sequence of random variables converges to zero in L1

limited form of dependence on n.)

(see Loeve 1977, p. 166). By the Minkowski inequality,

we only need to show that E|uP|I +e is bounded over n for

All the results to this point are limited in that they are
restricted to bounded functions. While some bounded

some positive E and all PEPB. Taking the absolute values

functions are of interest, there are many unbounded

inside the power, it suffices to bound

functions, for example, squared error loss functions, for

E[Ju [,n( +E)Iu2 P2(0 +E) ... IUJ PJ(l +E)]

which we would like to be able to compute approximate

Holder's inequality) to showing that E[Iu
bounded over n for 1 ' j ? J. By our second assumption
on t, it suffices to show that E[|uj|t/j] is bounded over n.

expectations. Lehmann (1983 p. 109, Theorem 5.1b)
showed that

Loeve (1977, p. 276) showed that if Yl, Y2, *.., Yn are

Eg- g (_) + g L2t)o2/(2n) + O(n 2) (4)

iid with E[yj] = 0 and E[|yjjs] finite for s > 2, then

need not be bounded, but it will be bounded by a poly-

EE=ls c= cn12 E|yi|s, for some constant c. Identify yi
with (x4, - j) and s with t/j; then, by our first assumption on t and the Loeve result, we have that E[|ujlt/j] is

nomial in its argument. This polynomial bounding is the

bounded over n and the L1 convergence is proved. Our

key. [Withers (1987, App. E) gives a result analogous

second assumption on t insures that the expected value

to (4). However, his theorem as stated is incorrect, be-

of Aj(u,) exists, and the theorem is proved.

when g has k derivatives (k ' 3), the kth derivative is

bounded, and X has at least k finite moments. Such a g

cause it omits any bounding conditions on the function

The basic point of the theorem is that when the func-

g or its derivatives. See the examples.]

tion we are approximating is polynomially bounded in

The following theorem extends the previous delta-

the random variables, then the naive Taylor series ap-

method theorems. The extensions are that the approxi-

proximation will yield the correct asymptotic approxi-

mating polynomial need not be a truncated Taylor series

mation to the expected value of the function provided

(though it generally will be) and that the polynomial

only that the underlying sequences of random variables

bounding is put directly on the approximation error. We

have enough bounded moments.

prove the theorem for functions in the normalized sam-

ple moments uj, = -1j=I (xii - 0/\/n, but it can be

2. EXAMPLES

extended to other random variables as well.

Example 1: Computation of the mean squxared error

We will use the following notation for polynomials in

of an adaptive shrinkage estimator of the mean. Let ,i

the first J normalized sample moments. Let p = (P1, P2,

= x x x2/(x2 + s2/n), where x and 52 are the sample

pJ)' be a vector of powers, and let u P = u=
PI

P2

...

Pi

UI,n

U2,,

combination of a finite number of the uP. The sets PA

mean and variance. This estimator was introduced by

*UJn.

and PB are finite sets of powers that define the approximating and bounding polynomials.

a distribution with finite tth moment. Suppose that there
are approximating and bounding polynomials

studied by a number of authors, including Mehta and

may be expanded, after some algebra, as

u1 n1/ o-- 2_o-2u1/ + u2-2,ul

A- =--1/ n3/2Ai
n"2

-_t _ ul/n'1 2) + u1(u2 + 2stul)

+

An(un) = EanpupI
PE PA

and

poly

Srinivasan (1971) and Oehlert (1981). The error ,I -,u

Theorem. Let the random variables ui,n be the normalized sample moments of an iid sample of size n from

A

Thompson (1968); variants and improvements have been

2

2

-s

2S4

Xs (,2 4 2/,2 2
-2 2) (u s - 3 2

n(nx + s

B(un) = bpuP
pEPB

such that

2S4 ( 2 U1 + n1/t2)2

n|Jg(n, un) - An(un)j -P> 0

+ ~~~2) ~(5)

and

nt'g(n, u,) - AJ(uJ)| B(Un)
for all n sufficiently large. If t > 2J and t > max

P&PBUPA Ej=1 jpj, then

where ul and u2 are the normalized sam
fined in the preceding section. We are interested in the

mean squared error of ,2, so our g function is the squared

error (SE) (,i - 1c) , or equivalently, the right side of

n~E|g(n, un) -A,l(u,l)l- 0,

(5) squared. Let T be the first two terms on the right side
of (5); our approximating polynomial A,, will be T2. This

and consequently,

E[g(n, u,l)] = E[A,I(u,l)] + o(n13).

approximate squared error satisfies n2|SE -A -4 0. The
difference n2|SE -T2| is equal to n'12Q2 -1u. + T)n312(ui

28 The American Statistician, February 1992, Vol. 46, No. I

This content downloaded from 206.253.207.235 on Fri, 15 Nov 2019 19:29:19 UTC
All use subject to https://about.jstor.org/terms

- , - T); each factor in this product can be polyno-

Example 4: The delta method approximation can fail

mially bounded by noting that terms of the form ab/(a2

to hold when the moment conditions are not met.

+ b) (where b is positive) are bounded. The resulting

Suppose that xl, x2, . . ., xn are iid observations from the

bounding polynomial has largest order term U4, SO if

distribution with density f(x) = (3/x4)I [x ? 1 ] (which

has mean 1.5 and variance .75 but no moments of order
E[X|8'+ is finite for some positive 8, then the mean squared
three or higher). Apply approximation (4) to the function

error of ,i can be expressed as
-2

g(X) = X without checking the moment conditions (which

1

MSE(f) = + 2 2 (3o4 - 2/4A3) + o(n2)

are not met!). We obtain the approximate expectation _t

The remaining examples give some idea of what can

[Received June 1990. Revised November 1990.]

n n ,ut

+ 3o2_t/n, but the exact expectation is infinite.

happen when the conditions of the theorem are not met.
REFERENCES

Example 2. Convergence in probability is not sufficient to get convergence in mean without further con-

Bhattacharya, R. N., and Rao, R. R. (1976), Normal Approximation

and Asymptotic Expansions, New York: John Wiley.

ditions. Let xl, x2, ..., x, be iid unit normal random

Bishop, Y. M. M., Fienberg, S. E., and Holland, P. W. (1975),

variables, take g(n, u,j) to be exp(nx 2/2)/[\/n(1 +
x2)], and let An = 1 /\/n (the first term in the Taylor

Discrete Multivariate Analysis, Cambridge, MA: MIT Press.
Cramer, H. (1946), Mathematical Methods of Statistics, Princeton,

series expansion of g) be the approximating polynomial.

NJ: Princeton University Press.
Hurt, J. (1976), "Asymptotic Expansions of Functions of Statistics,"

Now, both g and Ig - An converge in probability to zero,

Aplikace
yet g has constant expectation equal to 7-/2. Thus the

polynomial boundedness condition on g cannot be disregarded with impunity.

Matematiky, 21, 444-456.

Lehmann, E. L. (1983), Theory of Point Estimation, New York: John
Wiley.

Loeve, M. (1977), Probability Theory I (4th ed.), New York: Springer-

Example 3: The polynomial boundedness condition is
not necessary. For the iid unit normals in Example 2,

Verlag.

Mehta, J. S., and Srinivasan, R. (1971), "Estimation of the Mean by
Shrinkage to a Point," Journal of the American Statistical Associ-

ation,
let g(n, Un) = exp(x3) = exp(ul/\Fn). Using facts about

the lognormal distribution, we have that

66, 86-90.

Oehlert, G. W. (1981), "Estimating the Mean of A Positive Random
Variable," unpublished Ph.D. dissertation, Yale University, Dept.

E[exp(x)] = exp(l/(2n)) 1 + - + o(n-),
2n

which agrees with the delta-method approximation based

of Statistics.

Rao, C. R. (1965), Linear Statistical Inference and Its Applications,
New York: John Wiley.
Thompson, J. R. (1968), "Some Shrinkage Techniques for Estimating
the Mean," Journal of the American Statistical Association, 63,

on the truncated Taylor series An = 1 + u1/\/'- +113-122.
Iu21(2n), even though the function g is not polynomially
Withers,
bounded.

C. S. (1987), "Bias Reduction by Taylor Series, Com-

munications in Statistics-Theory and Methods, 16, 2369-2383.

Exponential Families and Variance Component Models
DENNIS L. CLASON and LEIGH W. MURRAY*

This article shows that two slightly differential expo-

families. Graybill (1976) defines the k-parameter expo-

nential families are needed for commonly used linear

nential family in definition 2.6.3 as

models. The first family is the usual textbook definition.

The second family, due to a lemma by Gautschi, is needed
for some cross-classified variance component models.

f(Y:O) = H(O)G(Y) exp E Ti(Y)Qi(O)1, (1)

KEY WORDS: Completeness; Lehmann-Scheffe theorem; Random components; Sufficiency.

with some regularity conditions on H, G, T, and Q. (This
definition is similar to that of Rohatgi 1976, Roussas

Uniformly Minimum Variance Unbiased Estimators

1973, and Lehmann 1959). The form of the exponential

(UMVUE) are usually obtained by appeals to the Leh-

family (1) is not, however, met by some cross-classified

mann-Scheffe theorem and the properties of exponential
*Dennis L. Clason is Assistant Professor and Leigh W. Murray is
Associate Professor, both in the Department of Experimental Statis-

tics, New Mexico State University, Las Cruces, 88003. This is Jour-

variance component models. These models are members

of a slightly different exponential family, whose completeness was first proven in a lemma by Gautschi (1959)
and subsequently reproven for the normal, twofold crossed

nal Article 1617, New Mexico Agricultural Experiment Station. The

model with an interaction component by Arnold (1981).

authors thank the referees and the editors for helpful comments.

Gautschi's lemma is cited by Hocking (1985, app. B).

(C 1992 American Statistical Association

The American Statistician, February 1992, Vol. 46, No. 1 29

This content downloaded from 206.253.207.235 on Fri, 15 Nov 2019 19:29:19 UTC
All use subject to https://about.jstor.org/terms

