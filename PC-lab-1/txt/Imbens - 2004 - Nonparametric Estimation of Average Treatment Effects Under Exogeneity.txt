NONPARAMETRIC ESTIMATION OF AVERAGE TREATMENT EFFECTS
UNDER EXOGENEITY: A REVIEW*
Guido W. Imbens
Abstract—Recently there has been a surge in econometric work focusing
on estimating average treatment effects under various sets of assumptions.
One strand of this literature has developed methods for estimating average
treatment effects for a binary treatment under assumptions variously
described as exogeneity, unconfoundedness, or selection on observables.
The implication of these assumptions is that systematic (for example,
average or distributional) differences in outcomes between treated and
control units with the same values for the covariates are attributable to the
treatment. Recent analysis has considered estimation and inference for
average treatment effects under weaker assumptions than typical of the
earlier literature by avoiding distributional and functional-form assumptions. Various methods of semiparametric estimation have been proposed,
including estimating the unknown regression functions, matching, methods using the propensity score such as weighting and blocking, and
combinations of these approaches. In this paper I review the state of this
literature and discuss some of its unanswered questions, focusing in
particular on the practical implementation of these methods, the plausibility of this exogeneity assumption in economic applications, the relative
performance of the various semiparametric estimators when the key
assumptions (unconfoundedness and overlap) are satisfied, alternative
estimands such as quantile treatment effects, and alternate methods such
as Bayesian inference.

I.

Introduction

S

INCE the work by Ashenfelter (1978), Card and Sullivan (1988), Heckman and Robb (1984), Lalonde
(1986), and others, there has been much interest in econometric methods for estimating the effects of active labor
market programs such as job search assistance or classroom
teaching programs. This interest has led to a surge in
theoretical work focusing on estimating average treatment
effects under various sets of assumptions. See for general
surveys of this literature Angrist and Krueger (2000), Heckman, LaLonde, and Smith (2000), and Blundell and CostaDias (2002).
One strand of this literature has developed methods for
estimating the average effect of receiving or not receiving a
binary treatment under the assumption that the treatment
satisfies some form of exogeneity. Different versions of this
assumption are referred to as unconfoundedness (Rosenbaum & Rubin, 1983a), selection on observables (Barnow,
Cain, & Goldberger, 1980; Fitzgerald, Gottschalk, & Moffitt, 1998), or conditional independence (Lechner, 1999). In
the remainder of this paper I will use the terms unconfoundReceived for publication October 22, 2002. Revision accepted for
publication June 4, 2003.
* University of California at Berkeley and NBER
This paper was presented as an invited lecture at the Australian and
European meetings of the Econometric Society in July and August 2003.
I am also grateful to Joshua Angrist, Jane Herr, Caroline Hoxby, Charles
Manski, Xiangyi Meng, Robert Moffitt, and Barbara Sianesi, and two
referees for comments, and to a number of collaborators, Alberto Abadie,
Joshua Angrist, Susan Athey, Gary Chamberlain, Keisuke Hirano, V.
Joseph Hotz, Charles Manski, Oscar Mitnik, Julie Mortimer, Jack Porter,
Whitney Newey, Geert Ridder, Paul Rosenbaum, and Donald Rubin for
many discussions on the topics of this paper. Financial support for this
research was generously provided through NSF grants SBR 9818644 and
SES 0136789 and the Giannini Foundation.

edness and exogeneity interchangeably to denote the assumption that the receipt of treatment is independent of the
potential outcomes with and without treatment if certain
observable covariates are held constant. The implication of
these assumptions is that systematic (for example, average
or distributional) differences in outcomes between treated
and control units with the same values for these covariates
are attributable to the treatment.
Much of the recent work, building on the statistical
literature by Cochran (1968), Cochran and Rubin (1973),
Rubin (1973a, 1973b, 1977, 1978), Rosenbaum and Rubin
(1983a, 1983b, 1984), Holland (1986), and others, considers
estimation and inference without distributional and functional form assumptions. Hahn (1998) derived efficiency
bounds assuming only unconfoundedness and some regularity conditions and proposed an efficient estimator. Various alternative estimators have been proposed given these
conditions. These estimation methods can be grouped into
five categories: (i) methods based on estimating the unknown regression functions of the outcome on the covariates (Hahn, 1998; Heckman, Ichimura, & Todd, 1997, 1998;
Imbens, Newey, & Ridder, 2003), (ii) matching on covariates (Rosenbaum, 1995; Abadie and Imbens, 2002) (iii)
methods based on the propensity score, including blocking
(Rosenbaum & Rubin, 1984) and weighting (Hirano, Imbens, & Ridder, 2003), (iv) combinations of these approaches, for example, weighting and regression (Robins &
Rotnizky, 1995) or matching and regression (Abadie &
Imbens, 2002), and (v) Bayesian methods, which have
found relatively little following since Rubin (1978). In this
paper I will review the state of this literature—with particular emphasis on implications for empirical work—and
discuss some of the remaining questions.
The organization of the paper is as follows. In section II
I will introduce the notation and the assumptions used for
identification. I will also discuss the difference between
population- and sample-average treatment effects. The recent econometric literature has largely focused on estimation of the population-average treatment effect and its counterpart for the subpopulation of treated units. An alternative,
following the early experimental literature (Fisher, 1925;
Neyman, 1923), is to consider estimation of the average
effect of the treatment for the units in the sample. Many of
the estimators proposed can be interpreted as estimating
either the average treatment effect for the sample at hand, or
the average treatment effect for the population. Although the
choice of estimand may not affect the form of the estimator,
it has implications for the efficiency bounds and for the
form of estimators of the asymptotic variance; the variance
of estimators for the sample average treatment effect are

The Review of Economics and Statistics, February 2004, 86(1): 4–29
© 2004 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology

AVERAGE TREATMENT EFFECTS

generally smaller. In section II, I will also discuss alternative estimands. Almost the entire literature has focused on
average effects. However, in many cases such measures
may mask important distributional changes. These can be
captured more easily by focusing on quantiles of the distributions of potential outcomes, in the presence and absence
of the treatment (Lehman, 1974; Docksum, 1974; Firpo,
2003).
In section III, I will discuss in more detail some of the
recently proposed semiparametric estimators for the average
treatment effect, including those based on regression,
matching, and the propensity score. I will focus particularly
on implementation, and compare the different decisions
faced regarding smoothing parameters using the various
estimators.
In section IV, I will discuss estimation of the variances of
these average treatment effect estimators. For most of the
estimators introduced in the recent literature, corresponding
estimators for the variance have also been proposed, typically requiring additional nonparametric regression. In practice, however, researchers often rely on bootstrapping, although this method has not been formally justified. In
addition, if one is interested in the average treatment effect
for the sample, bootstrapping is clearly inappropriate. Here
I discuss in more detail a simple estimator for the variance
for matching estimators, developed by Abadie and Imbens
(2002), that does not require additional nonparametric estimation.
Section V discusses different approaches to assessing the
plausibility of the two key assumptions: exogeneity or
unconfoundedness, and overlap in the covariate distributions. The first of these assumptions is in principle untestable. Nevertheless a number of approaches have been proposed that are useful for addressing its credibility (Heckman
and Hotz, 1989; Rosenbaum, 1984b). One may also wish to
assess the responsiveness of the results to this assumption
using a sensitivity analysis (Rosenbaum & Rubin, 1983b;
Imbens, 2003), or, in its extreme form, a bounds analysis
(Manski, 1990, 2003). The second assumption is that there
exists appropriate overlap in the covariate distributions of
the treated and control units. That is effectively an assumption on the joint distribution of observable variables. However, as it only involves inequality restrictions, there are no
direct tests of this null. Nevertheless, in practice it is often
very important to assess whether there is sufficient overlap
to draw credible inferences. Lacking overlap for the full
sample, one may wish to limit inferences to the average
effect for the subset of the covariate space where there exists
overlap between the treated and control observations.
In Section VI, I discuss a number of implementations of
average treatment effect estimators. The first set of implementations involve comparisons of the nonexperimental
estimators to results based on randomized experiments,
allowing direct tests of the unconfoundedness assumption.
The second set consists of simulation studies—using data

5

created either to fulfill the unconfoundedness assumption or
to fail it a known way—designed to compare the applicability of the various treatment effect estimators in these
diverse settings.
This survey will not address alternatives for estimating
average treatment effects that do not rely on exogeneity
assumptions. This includes approaches where selected observed covariates are not adjusted for, such as instrumental
variables analyses (Björklund & Moffit, 1987; Heckman &
Robb, 1984; Imbens & Angrist, 1994; Angrist, Imbens, &
Rubin, 1996; Ichimura & Taber, 2000; Abadie, 2003a;
Chernozhukov & Hansen, 2001). I will also not discuss
methods exploiting the presence of additional data, such as
difference in differences in repeated cross sections (Abadie,
2003b; Blundell et al., 2002; Athey and Imbens, 2002) and
regression discontinuity where the overlap assumption is
violated (van der Klaauw, 2002; Hahn, Todd, & van der
Klaauw, 2000; Angrist & Lavy, 1999; Black, 1999; Lee,
2001; Porter, 2003). I will also limit the discussion to binary
treatments, excluding models with static multivalued treatments as in Imbens (2000) and Lechner (2001) and models
with dynamic treatment regimes as in Ham and LaLonde
(1996), Gill and Robins (2001), and Abbring and van den
Berg (2003). Reviews of many of these methods can be
found in Shadish, Campbell, and Cook (2002), Angrist and
Krueger (2000), Heckman, LaLonde, and Smith (2000), and
Blundell and Costa-Dias (2002).
II.

Estimands, Identification, and Efficiency Bounds

A. Definitions

In this paper I will use the potential-outcome notation that
dates back to the analysis of randomized experiments by
Fisher (1935) and Neyman (1923). After being forcefully
advocated in a series of papers by Rubin (1974, 1977,
1978), this notation is now standard in the literature on both
experimental and nonexperimental program evaluation.
We begin with N units, indexed by i ⫽ 1, . . . , N,
viewed as drawn randomly from a large population. Each
unit is characterized by a pair of potential outcomes, Y i (0)
for the outcome under the control treatment and Y i (1) for
the outcome under the active treatment. In addition, each
unit has a vector of characteristics, referred to as covariates,
pretreatment variables, or exogenous variables, and denoted
by X i . 1 It is important that these variables are not affected by
the treatment. Often they take their values prior to the unit
being exposed to the treatment, although this is not sufficient for the conditions they need to satisfy. Importantly,
this vector of covariates can include lagged outcomes.
1 Calling such variables exogenous is somewhat at odds with several
formal definitions of exogeneity (e.g., Engle, Hendry, & Richard, 1974),
as knowledge of their distribution can be informative about the average
treatment effects. It does, however, agree with common usage. See for
example, Manski et al. (1992, p. 28). See also Frölich (2002) and Hirano
et al. (2003) for additional discussion.

6

THE REVIEW OF ECONOMICS AND STATISTICS

Finally, each unit is exposed to a single treatment; W i ⫽ 0
if unit i receives the control treatment, and W i ⫽ 1 if unit
i receives the active treatment. We therefore observe for
each unit the triple (W i , Y i , X i ), where Y i is the realized
outcome:

再

Y i 共0兲 if Wi ⫽ 0,
Y i ⬅ Y i 共W i 兲 ⫽ Y 共1兲 if W ⫽ 1.
i
i
Distributions of (W, Y, X) refer to the distribution induced
by the random sampling from the superpopulation.
Several additional pieces of notation will be useful in the
remainder of the paper. First, the propensity score (Rosenbaum and Rubin, 1983a) is defined as the conditional
probability of receiving the treatment,
e共 x兲 ⬅ Pr共W ⫽ 1兩X ⫽ x兲 ⫽ ⺕关W兩X ⫽ x兴.
Also, define, for w 僆 {0, 1}, the two conditional regression
and variance functions
␮ w 共 x兲 ⬅ ⺕关Y共w兲兩X ⫽ x兴,

␴ w2 共 x兲 ⬅ ⺦共Y共w兲兩X ⫽ x兲.

Finally, let ␳( x) be the conditional correlation coefficient of
Y(0) and Y(1) given X ⫽ x. As one never observes Y i (0)
and Y i (1) for the same unit i, the data only contain indirect
and very limited information about this correlation coefficient.2
B. Estimands: Average Treatment Effects

In this discussion I will primarily focus on a number of
average treatment effects (ATEs). This is less limiting than
it may seem, however, as it includes averages of arbitrary
transformations of the original outcomes. Later I will return
briefly to alternative estimands that cannot be written in this
form.
The first estimand, and the most commonly studied in the
econometric literature, is the population-average treatment
effect (PATE):
␶ P ⫽ ⺕关Y共1兲 ⫺ Y共0兲兴.
Alternatively we may be interested in the populationaverage treatment effect for the treated (PATT; for example,
Rubin, 1977; Heckman & Robb, 1984):
␶ TP ⫽ ⺕关Y共1兲 ⫺ Y共0兲兩W ⫽ 1兴.
Heckman and Robb (1984) and Heckman, Ichimura, and
Todd (1997) argue that the subpopulation of treated units is
often of more interest than the overall population in the
context of narrowly targeted programs. For example, if a
program is specifically directed at individuals disadvantaged in the labor market, there is often little interest in the
2 As Heckman, Smith, and Clemens (1997) point out, however, one can
draw some limited inferences about the correlation coefficient from the
shape of the two marginal distributions of Y(0) and Y(1).

effect of such a program on individuals with strong labor
market attachment.
I will also look at sample-average versions of these two
population measures. These estimands focus on the average
of the treatment effect in the specific sample, rather than in
the population at large. They include the sample-average
treatment effect (SATE)
␶S ⫽

1
N

冘 关Y 共1兲 ⫺ Y 共0兲兴,
N

i

i

i⫽1

and the sample-average treatment effect for the treated
(SATT)
␶ TS ⫽

1
NT

冘

关Y i 共1兲 ⫺ Y i 共0兲兴,

i:W i ⫽1

N
where N T ⫽ ¥ i⫽1
W i is the number of treated units. The
SATE and the SATT have received little attention in the
recent econometric literature, although the SATE has a long
tradition in the analysis of randomized experiments (for
example, Neyman, 1923). Without further assumptions, the
sample contains no information about the PATE beyond the
SATE. To see this, consider the case where we observe the
sample (Y i (0), Y i (1), W i , X i ), i ⫽ 1, . . . , N; that is, we
observe both potential outcomes for each unit. In that case
␶ S ⫽ ¥ i [Y i (1) ⫺ Y i (0)]/N can be estimated without error.
Obviously, the best estimator for the population-average
effect ␶ P is ␶ S . However, we cannot estimate ␶ P without
error even with a sample where all potential outcomes are
observed, because we lack the potential outcomes for those
population members not included in the sample. This simple
argument has two implications. First, one can estimate the
SATE at least as accurately as the PATE, and typically more
so. In fact, the difference between the two variances is the
variance of the treatment effect, which is zero only when the
treatment effect is constant. Second, a good estimator for
one ATE is automatically a good estimator for the other. One
can therefore interpret many of the estimators for PATE or
PATT as estimators for SATE or SATT, with lower implied
standard errors, as discussed in more detail in section IIE.
A third pair of estimands combines features of the other
two. These estimands, introduced by Abadie and Imbens
(2002), focus on the ATE conditional on the sample distribution of the covariates. Formally, the conditional ATE
(CATE) is defined as

␶共X兲 ⫽

1
N

冘 ⺕关Y 共1兲 ⫺ Y 共0兲兩X 兴,
N

i

i

i

i⫽1

and the SATE for the treated (CATT) is defined as
␶共X兲 T ⫽

1
NT

冘

i:W i ⫽1

⺕关Y i 共1兲 ⫺ Y i 共0兲兩X i 兴.

AVERAGE TREATMENT EFFECTS

Using the same argument as in the previous paragraph, it
can be shown that one can estimate CATE and CATT more
accurately than PATE and PATT, but generally less accurately than SATE and SATT.
The difference in asymptotic variances forces the researcher to take a stance on what the quantity of interest is.
For example, in a specific application one can legitimately
reach the conclusion that there is no evidence, at the 95%
level, that the PATE is different from zero, whereas there
may be compelling evidence that the SATE and CATE are
positive. Typically researchers in econometrics have focused on the PATE, but one can argue that it is of interest,
when one cannot ascertain the sign of the population-level
effect, to know whether one can determine the sign of the
effect for the sample. Especially in cases, which are all too
common, where it is not clear whether the sample is representative of the population of interest, results for the sample
at hand may be of considerable interest.
C. Identification

We make the following key assumption about the treatment assignment:
ASSUMPTION 2.1 (UNCONFOUNDEDNESS):
共Y共0兲, Y共1兲兲 ⬜ W兩X.
This assumption was first articulated in this form by
Rosenbaum and Rubin (1983a), who refer to it as “ignorable
treatment assignment.” Lechner (1999, 2002) refers to this
as the “conditional independence assumption.” Following
work by Barnow, Cain, and Goldberger (1980) in a regression setting it is also referred to as “selection on observables.”
To see the link with standard exogeneity assumptions,
suppose that the treatment effect is constant: ␶ ⫽ Y i (1) ⫺
Y i (0) for all i. Suppose also that the control outcome is
linear in X i :
Y i 共0兲 ⫽ ␣ ⫹ X⬘i ␤ ⫹ ε i ,
with ε i ⬜ X i . Then we can write
Y i ⫽ ␣ ⫹ ␶ 䡠 W i ⫹ X⬘i ␤ ⫹ ε i .
Given the assumption of constant treatment effect, unconfoundedness is equivalent to independence of W i and ε i
conditional on X i , which would also capture the idea that W i
is exogenous. Without this assumption, however, unconfoundedness does not imply a linear relation with (mean-)independent errors.
Next, we make a second assumption regarding the joint
distribution of treatments and covariates:
ASSUMPTION 2.2 (OVERLAP):
0 ⬍ Pr共W ⫽ 1兩X兲 ⬍ 1.

7

For many of the formal results one will also need smoothness assumptions on the conditional regression functions
and the propensity score [␮ w ( x) and e( x)], and moment
conditions on Y(w). I will not discuss these regularity
conditions here. Details can be found in the references for
the specific estimators given below.
There has been some controversy about the plausibility of
Assumptions 2.1 and 2.2 in economic settings, and thus
about the relevance of the econometric literature that focuses on estimation and inference under these conditions for
empirical work. In this debate it has been argued that
agents’ optimizing behavior precludes their choices being
independent of the potential outcomes, whether or not
conditional on covariates. This seems an unduly narrow
view. In response I will offer three arguments for considering these assumptions.
The first is a statistical, data-descriptive motivation. A
natural starting point in the evaluation of any program is a
comparison of average outcomes for treated and control
units. A logical next step is to adjust any difference in
average outcomes for differences in exogenous background
characteristics (exogenous in the sense of not being affected
by the treatment). Such an analysis may not lead to the final
word on the efficacy of the treatment, but its absence would
seem difficult to rationalize in a serious attempt to understand the evidence regarding the effect of the treatment.
A second argument is that almost any evaluation of a
treatment involves comparisons of units who received the
treatment with units who did not. The question is typically
not whether such a comparison should be made, but rather
which units should be compared, that is, which units best
represent the treated units had they not been treated. Economic theory can help in classifying variables into those
that need to be adjusted for versus those that do not, on the
basis of their role in the decision process (for example,
whether they enter the utility function or the constraints).
Given that, the unconfoundedness assumption merely asserts that all variables that need to be adjusted for are
observed by the researcher. This is an empirical question,
and not one that should be controversial as a general
principle. It is clear that settings where some of these
covariates are not observed will require strong assumptions
to allow for identification. Such assumptions include instrumental variables settings where some covariates are assumed to be independent of the potential outcomes. Absent
those assumptions, typically only bounds can be identified
(as in Manski, 1990, 2003).
A third, related argument is that even when agents choose
their treatment optimally, two agents with the same values
for observed characteristics may differ in their treatment
choices without invalidating the unconfoundedness assumption if the difference in their choices is driven by differences
in unobserved characteristics that are themselves unrelated
to the outcomes of interest. The plausibility of this will
depend critically on the exact nature of the optimization

8

THE REVIEW OF ECONOMICS AND STATISTICS

process faced by the agents. In particular it may be important that the objective of the decision maker is distinct from
the outcome that is of interest to the evaluator. For example,
suppose we are interested in estimating the average effect of
a binary input (such as a new technology) on a firm’s
output.3 Assume production is a stochastic function of this
input because other inputs (such as weather) are not under
the firm’s control: Y i ⫽ g(W, ε i ). Suppose that profits are
output minus costs (␲ i ⫽ Y i ⫺ c i 䡠 W i ), and also that a firm
chooses a production level to maximize expected profits,
equal to output minus costs, conditional on the cost of
adopting new technology,
W i ⫽ arg max ⺕关␲共w兲兩ci 兴
w僆兵0,1其

⫽ arg max ⺕关 g共w, εi 兲 ⫺ ci 䡠 w兩ci 兴,
w僆兵0,1其

implying
W i ⫽ 1兵⺕关 g共1, ε兲 ⫺ g共0, ε i 兲 ⱖ c i 兩c i 兴其 ⫽ h共c i 兲.
If unobserved marginal costs c i differ between firms, and
these marginal costs are independent of the errors ε i in the
firms’ forecast of production given inputs, then unconfoundedness will hold, as
共 g共0, ε兲, g共1, ε i 兲兲 ⬜ c i .
Note that under the same assumptions one cannot necessarily identify the effect of the input on profits, for (␲ i (0),
␲(1)) are not independent of c i . For a related discussion, in
the context of instrumental variables, see Athey and Stern
(1998). Heckman, LaLonde, and Smith (2000) discuss alternative models that justify unconfoundedness. In these
models individuals do attempt to optimize the same outcome that is the variable of interest to the evaluator. They
show that selection-on-observables assumptions can be justified by imposing restrictions on the way individuals form
their expectations about the unknown potential outcomes. In
general, therefore, a researcher may wish to consider, either
as a final analysis or as part of a larger investigation,
estimates based on the unconfoundedness assumption.
Given the two key assumptions, unconfoundedness and
overlap, one can identify the average treatment effects. The
key insight is that given unconfoundedness, the following
equalities hold:
␮ w 共 x兲 ⫽ ⺕关Y共w兲兩X ⫽ x兴 ⫽ ⺕关Y共w兲兩W ⫽ w, X ⫽ x兴
⫽ ⺕关Y兩W ⫽ w, X ⫽ x兴,
3 If we are interested in the average effect for firms that did adopt the
new technology (PATT), the following assumptions can be weakened
slightly.

and thus ␮ w ( x) is identified. Thus one can estimate the
average treatment effect ␶ by first estimating the average
treatment effect for a subpopulation with covariates X ⫽ x:
␶共 x兲 ⬅ ⺕关Y共1兲 ⫺ Y共0兲兩X ⫽ x兴 ⫽ ⺕关Y共1兲兩X ⫽ x兴
⫺ ⺕关Y共0兲兩X ⫽ x兴 ⫽ ⺕关Y共1兲兩X ⫽ x, W ⫽ 1兴
⫺ ⺕关Y共0兲兩X ⫽ x, W ⫽ 0兴 ⫽ ⺕关Y兩X, W ⫽ 1兴
⫺ ⺕关Y兩X, W ⫽ 0兴;
followed by averaging over the appropriate distribution of x.
To make this feasible, one needs to be able to estimate the
expectations ⺕[Y兩X ⫽ x, W ⫽ w] for all values of w and
x in the support of these variables. This is where the second
assumption enters. If the overlap assumption is violated at
X ⫽ x, it would be infeasible to estimate both ⺕[Y兩X ⫽ x,
W ⫽ 1] and ⺕[Y兩X ⫽ x, W ⫽ 0], because at those values
of x there would be either only treated or only control units.
Some researchers use weaker versions of the unconfoundedness assumption (for example, Heckman, Ichimura,
and Todd, 1998). If the interest is in the PATE, it is sufficient
to assume that
ASSUMPTION 2.3 (MEAN INDEPENDENCE):
⺕关Y共w兲兩W, X兴 ⫽ ⺕关Y共w兲兩X兴,
for w ⫽ 0, 1.
Although this assumption is unquestionably weaker, in
practice it is rare that a convincing case is made for the
weaker assumption 2.3 without the case being equally
strong for the stronger version 2.1. The reason is that the
weaker assumption is intrinsically tied to functional-form
assumptions, and as a result one cannot identify average
effects on transformations of the original outcome (such as
logarithms) without the stronger assumption.
One can weaken the unconfoundedness assumption in a
different direction if one is only interested in the average
effect for the treated (see, for example, Heckman, Ichimura,
& Todd, 1997). In that case one need only assume
ASSUMPTION 2.4 (UNCONFOUNDEDNESS

FOR

CONTROLS):

Y共0兲 ⬜ W兩X.
and the weaker overlap assumption
ASSUMPTION 2.5 (WEAK OVERLAP):
Pr共W ⫽ 1兩X兲 ⬍ 1.
These two assumptions are sufficient for identification of
PATT and SATT, because the moments of the distribution of
Y(1) for the treated are directly estimable.
An important result building on the unconfoundedness
assumption shows that one need not condition simulta-

AVERAGE TREATMENT EFFECTS

neously on all covariates. The following result shows that
all biases due to observable covariates can be removed by
conditioning solely on the propensity score:
Lemma 2.1 (Unconfoundedness Given the Propensity
Score: Rosenbaum and Rubin, 1983a): Suppose that assumption 2.1 holds. Then
共Y共0兲, Y共1兲兲 ⬜ W兩e共X兲.
Proof: We will show that Pr(W ⫽ 1兩Y(0), Y(1), e(X)) ⫽
Pr(W ⫽ 1兩e(X)) ⫽ e(X), implying independence of (Y(0),
Y(1)) and W conditional on e(X). First, note that
Pr共W ⫽ 1兩Y共0兲, Y共1兲, e共X兲兲 ⫽ ⺕关W ⫽ 1兩Y共0兲, Y共1兲, e共X兲兴
⫽ ⺕关⺕关W兩Y共0兲, Y共1兲, e共X兲, X兴兩Y共0兲, Y共1兲, e共X兲兴
⫽ ⺕关⺕关W兩Y共0兲, Y共1兲, X兴兩Y共0兲, Y共1兲, e共X兲兴
⫽ ⺕关⺕关W兩X兴兩Y共0兲, Y共1兲, e共X兲兴
⫽ ⺕关e共X兲兩Y共0兲, Y共1兲, e共X兲兴 ⫽ e共X兲,
where the last equality follows from unconfoundedness. The
same argument shows that
Pr共W ⫽ 1兩e共X兲兲 ⫽ ⺕关W ⫽ 1兩e共X兲兴 ⫽ ⺕关⺕关W ⫽ 1兩X兴兩e共X兲兴
⫽ ⺕关e共X兲兩e共X兲兴 ⫽ e共X兲.
■
Extensions of this result to the multivalued treatment case
are given in Imbens (2000) and Lechner (2001). To provide
intuition for Rosenbaum and Rubin’s result, recall the textbook formula for omitted variable bias in the linear regression model. Suppose we have a regression model with two
regressors:
Y i ⫽ ␤ 0 ⫹ ␤ 1 䡠 W i ⫹ ␤⬘2 X i ⫹ ε i .
The bias of omitting X from the regression on the coefficient
on W is equal to ␤⬘2␦, where ␦ is the vector of coefficients on
W in regressions of the elements of X on W. By conditioning on the propensity score we remove the correlation
between X and W, because X ⬜ W兩e(X). Hence omitting X
no longer leads to any bias (although it may still lead to
some efficiency loss).
D. Distributional and Quantile Treatment Effects

Most of the literature has focused on estimating ATEs.
There are, however, many cases where one may wish to
estimate other features of the joint distribution of outcomes.
Lehman (1974) and Doksum (1974) introduce quantile
treatment effects as the difference in quantiles between the
two marginal treated and control outcome distributions.4

9

Bitler, Gelbach, and Hoynes (2002) estimate these in a
randomized evaluation of a social program. In instrumental
variables settings Abadie, Angrist, and Imbens (2002) and
Chernozhukov and Hansen (2001) investigate estimation of
differences in quantiles of the two marginal potential outcome distributions, either for the entire population or for
subpopulations.
Assumptions 2.1 and 2.2 also allow for identification of
the full marginal distributions of Y(0) and Y(1). To see this,
first note that we can identify not just the average treatment
effect ␶( x), but also the averages of the two potential
outcomes, ␮ 0 ( x) and ␮ 0 ( x). Second, by these assumptions
we can similarly identify the averages of any function of the
basic outcomes, ⺕[ g(Y(0))] and ⺕[ g(Y(1))]. Hence we can
identify the average values of the indicators 1{Y(0) ⱕ y}
and 1{Y(1) ⱕ y}, and thus the distribution function of the
potential outcomes at y. Given identification of the two
distribution functions, it is clear that one can also identify
quantiles of the two potential outcome distributions. Firpo
(2002) develops an estimator for such quantiles under unconfoundedness.
E. Efficiency Bounds and Asymptotic Variances for
Population-Average Treatment Effects

Next I review some results on the efficiency bound for
estimators of the ATEs ␶ P , and ␶ TP . This requires both the
assumptions of unconfoundedness and overlap (Assumptions 2.1 and 2.2) and some smoothness assumptions on the
conditional expectations of potential outcomes and the treatment indicator (for details, see Hahn, 1998). Formally, Hahn
(1998) shows that for any regular estimator for ␶ P , denoted
by ␶ˆ , with

冑N 䡠 共␶ˆ ⫺ ␶P兲

d

3 ᏺ共0, V兲,

it must be that

冋

册

␴ 02 共X兲
␴ 12 共X兲
⫹
⫹ 共␶共X兲 ⫺ ␶ P 兲 2 .
Vⱖ⺕
e共X兲
1 ⫺ e共X兲
Knowledge of the propensity score does not affect this
efficiency bound.
Hahn also shows that asymptotically linear estimators
exist with such variance, and hence such efficient estimators
can be approximated as
␶ˆ ⫽ ␶P ⫹

冘

1 N
␺共Yi , Wi , Xi , ␶P 兲 ⫹ op 共N ⫺1/ 2 兲,
N i⫽1

where ␺⵺ is the efficient score:

4

In contrast, Heckman, Smith, and Clemens (1997) focus on estimation
of bounds on the joint distribution of (Y(0), Y(1)). One cannot without
strong untestable assumptions identify the full joint distribution, since one

can never observe both potential outcomes simultaneously, but one can
nevertheless derive bounds on functions of the two distributions.

10

THE REVIEW OF ECONOMICS AND STATISTICS

冉

冊

共1 ⫺ w兲 y
wy
⫺
␺共 y, w, x, ␶ 兲 ⫽
e共 x兲 1 ⫺ e共 x兲
␮ 0 共 x兲
␮ 1 共 x兲
⫹
⫺ ␶P ⫺
关w ⫺ e共 x兲兴.
e共 x兲
1 ⫺ e共 x兲

冉

P

冊

⺕关␺共Y, W, X, ␶ P 兩Y共0兲, Y共1兲, X兲兴 ⫽ Y共1兲 ⫺ Y共0兲 ⫺ ␶ P ,
(1)

and thus
N
⺕关␶˜ 兩共Yi 共0兲, Yi 共1兲, Xi 兲i⫽1
兴 ⫽ ⺕关␺៮ 兴 ⫹ ␶P

Hahn (1998) also reports the efficiency bound for ␶ ,
both with and without knowledge of the propensity score.
For ␶ TP the efficiency bound given knowledge of e(X) is

冘

1 N
⫽
共Y 共1兲 ⫺ Yi 共0兲兲.
N i⫽1 i

P
T

冋

e共X兲 Var共Y共1兲兩X兲 e共X兲2 Var共Y共0兲兩X兲
⺕
⫹
⺕关e共X兲兴2
⺕关e共X兲兴2 共1 ⫺ e共X兲兲

Hence
N
兴
⺕关␶˜ ⫺ ␶S 兩共Yi 共0兲, Yi 共1兲, Xi 兲i⫽1

册

e共X兲2
.
⫹ 共␶共X兲 ⫺ ␶ 兲
⺕关e共X兲兴2
P 2
T

⫽

If the propensity score is not known, unlike the bound for
␶ P , the efficiency bound for ␶ TP is affected. For ␶ TP the bound
without knowledge of the propensity score is
⺕

冋

e共X兲 Var共Y共1兲兩X兲 e共X兲2 Var共Y共0兲兩X兲
⫹
⺕关e共X兲兴2
⺕关e共X兲兴2 共1 ⫺ e共X兲兲
⫹ 共␶共X兲 ⫺ ␶TP 兲2

册

冋

Next, consider the normalized variance:
V P ⫽ N 䡠 ⺕关共␶˜ ⫺ ␶S 兲2 兴 ⫽ N 䡠 ⺕关共␺៮ ⫹ ␶P ⫺ ␶S 兲2 兴.
Note that the variance of ␶˜ as an estimator of ␶ P can be
expressed, using the fact that ␺⵺ is the efficient score, as
N 䡠 ⺕关共␶˜ ⫺ ␶P 兲2 兴 ⫽ N 䡠 ⺕关共␺៮ 兲2 兴 ⫽

e共X兲
,
⺕关e共X兲兴2

N 䡠 ⺕关共␺៮ 共Y, W, X, ␶P 兲 ⫹ 共␶P ⫺ ␶S 兲 ⫺ 共␶P ⫺ ␶S 兲兲2 兴.

which is higher by
⺕ 共␶共X兲 ⫺ ␶ TP 兲 2 䡠

冘

1 N
共Y 共1兲 ⫺ Yi 共0兲兲 ⫺ ␶S ⫽ 0.
N i⫽1 i

册

e共X兲共1 ⫺ e共X兲兲
.
⺕关e共X兲兴 2

The intuition that knowledge of the propensity score affects
the efficiency bound for the average effect for the treated
(PATT), but not for the overall average effect (PATE), goes
as follows. Both are weighted averages of the treatment
effect conditional on the covariates, ␶( x). For the PATE the
weight is proportional to the density of the covariates,
whereas for the PATT the weight is proportional to the
product of the density of the covariates and the propensity
score (see, for example, Hirano, Imbens, and Ridder, 2003).
Knowledge of the propensity score implies one does not
need to estimate the weight function and thus improves
precision.
F. Efficiency Bounds and Asymptotic Variances for
Conditional and Sample Average Treatment Effects

Consider the leading term of the efficient estimator for
the PATE, ␶˜ ⫽ ␶ P ⫹ ␺៮ , where ␺៮ ⫽ (1/N) ¥ ␺(Y i , W i , X i ,
␶ P ), and let us view this as an estimator for the SATE,
instead of as an estimator for the PATE. I will show that,
first, this estimator is unbiased, conditional on the covariates
and the potential outcomes, and second, it has lower variance as an estimator of the SATE than as an estimator of the
PATE. To see that the estimator is unbiased note that with
the efficient score ␺( y, w, x, ␶) given in equation (1),

Because
⺕关共␺៮ 共Y, W, X, ␶P 兲 ⫹ 共␶P ⫺ ␶S 兲兲 䡠 共␶P ⫺ ␶S 兲兴 ⫽ 0
[as follows by using iterated expectations, first conditioning
on X, Y(0), and Y(1)], it follows that
N 䡠 ⺕关共␶˜ ⫺ ␶P 兲2 兴 ⫽ N 䡠 ⺕关共␶˜ ⫺ ␶S 兲2 兴 ⫹ N 䡠 ⺕关共␶S ⫺ ␶P 兲2 兴
⫽ N 䡠 ⺕关共␶˜ ⫺ ␶S 兲2 兴 ⫹ N 䡠 ⺕关共Y共1兲 ⫺ Y共0兲 ⫺ ␶P 兲2 兴.
Thus, the same statistic that as an estimator of the population average treatment effect ␶ P has a normalized variance
equal to V P , as an estimator of ␶ S has the property

冑N共␶˜ ⫺ ␶S兲

d

3 ᏺ共0, VS 兲,

with
V S ⫽ V P ⫺ ⺕关共Y共1兲 ⫺ Y共0兲 ⫺ ␶ P 兲 2 兴.
As an estimator of ␶ S the variance of ␶˜ is lower than its
variance as an estimator of ␶ P , with the difference equal to
the variance of the treatment effect.
The same line of reasoning can be used to show that

冑N共␶˜ ⫺ ␶共X兲兲

d

3 ᏺ共0, V ␶共X兲 兲,

with
V ␶共X兲 ⫽ V P ⫺ ⺕关␶共X兲 ⫺ ␶ P 兲 2 ],

AVERAGE TREATMENT EFFECTS

and
V S ⫽ V ␶共X兲 ⫺ ⺕关共Y共1兲 ⫺ Y共0兲 ⫺ ␶共X兲兲 2 兴.
An example to illustrate these points may be helpful.
Suppose that X 僆 {0, 1}, with Pr(X ⫽ 1) ⫽ p x and
Pr(W ⫽ 1兩X) ⫽ 1/ 2. Suppose that ␶( x) ⫽ 2x ⫺ 1, and
␴ 2w ( x) is very small for all x and w. In that case the average
treatment effect is p x 䡠 1 ⫹ (1 ⫺ p x ) 䡠 (⫺1) ⫽ 2p x ⫺ 1.
The efficient estimator in this case, assuming only unconfoundedness, requires separately estimating ␶( x) for x ⫽ 0
and 1, and averaging these two by the empirical distribution
of X. The variance of 公N(␶ˆ ⫺ ␶ S ) will be small because
␴ 2w ( x) is small, and according to the expressions above, the
variance of 公N(␶ ⫺ ␶ P ) will be larger by 4p x (1 ⫺ p x ). If
p x differs from 1/2, and so PATE differs from 0, the
confidence interval for PATE in small samples will tend to
include zero. In contrast, with ␴ 2w ( x) small enough and N
odd [and both N 0 and N 1 at least equal to 2, so that one can
estimate ␴ 2w ( x)], the standard confidence interval for ␶ S will
exclude 0 with probability 1. The intuition is that ␶ P is much
more uncertain because it depends on the distribution of the
covariates, whereas the uncertainty about ␶ S depends only
on the conditional outcome variances and the propensity
score.
The difference in asymptotic variances raises the issue of
how to estimate the variance of the sample average treatment effect. Specific estimators for the variance will be
discussed in section IV, but here I will introduce some
general issues surrounding their estimation. Because the
two potential outcomes for the same unit are never observed
simultaneously, one cannot directly infer the variance of the
treatment effect. This is the same issue as the nonidentification of the correlation coefficient. One can, however,
estimate a lower bound on the variance of the treatment
effect, leading to an upper bound on the variance of the
estimator of the SATE, which is equal to V ␶共X兲 . Decomposing the variance as
⺕关共Y共1兲 ⫺ Y共0兲 ⫺ ␶ P 兲 2 兴 ⫽ ⺦共⺕关Y共1兲 ⫺ Y共0兲 ⫺ ␶ P 兩X兴兲
⫹ ⺕关⺦共Y共1兲 ⫺ Y共0兲 ⫺ ␶ P 兩X兲兴,
⫽ ⺦共␶共X兲 ⫺ ␶ P 兲 ⫹ ⺕关␴ 12 共X兲 ⫹ ␴ 02 共X兲
⫺ 2␳共X兲␴ 0 共X兲␴ 1 共X兲兴,
we can consistently estimate the first term, but generally say
little about the second other than that it is nonnegative. One
can therefore bound the variance of ␶˜ ⫺ ␶ S from above by
⺕关␺共Y, W, X, ␶ P 兲 2 兴 ⫺ ⺕关共Y共1兲 ⫺ Y共0兲兲 ⫺ ␶ P 兲 2 ]
ⱕ ⺕关␺共Y, W, X, ␶ P 兲 2 兴 ⫺ ⺕关共␶共X兲 ⫺ ␶ P 兲 2 兴
⫽⺕

冋

册

␴ 02 共X兲
␴ 12 共X兲
⫹
⫽ V ␶共X兲 ,
e共X兲
1 ⫺ e共X兲

11

and use this upper-bound variance estimate to construct
confidence intervals that are guaranteed to be conservative.
Note the connection with Neyman’s (1923) discussion of
conservative confidence intervals for average treatment effects in experimental settings. It should be noted that the
difference between these variances is of the same order as
the variance itself, and therefore not a small-sample problem. Only when the treatment effect is known to be constant
can it be ignored. Depending on the correlation between the
outcomes and the covariates, this may change the standard
errors considerably. It should also be noted that bootstrapping methods in general lead to estimation of ⺕[(␶˜ ⫺ ␶ P ) 2 ],
rather than ⺕[(␶˜ ⫺ ␶(X)) 2 ], which are generally too big.
III.

Estimating Average Treatment Effects

There have been a number of statistics proposed for
estimating the PATE and PATT, all of which are also
appropriate estimators of the sample versions (SATE and
SATT) and the conditional average versions (CATE and
CATT). (The implications of focusing on SATE or CATE
rather than PATE only arise when estimating the variance,
and so I will return to this distinction in section IV. In the
current section all discussion applies equally to all estimands.) Here I review some of these estimators, organized
into five groups.
The first set, referred to as regression estimators, consists
of methods that rely on consistent estimation of the two
conditional regression functions, ␮ 0 ( x) and ␮ 1 ( x). These
estimators differ in the way that they estimate these elements, but all rely on estimators that are consistent for these
regression functions.
The second set, matching estimators, compare outcomes
across pairs of matched treated and control units, with each
unit matched to a fixed number of observations with the
opposite treatment. The bias of these within-pair estimates
of the average treatment effect disappears as the sample size
increases, although their variance does not go to zero,
because the number of matches remains fixed.
The third set of estimators is characterized by a central
role for the propensity score. Four leading approaches in
this set are weighting by the reciprocal of the propensity
score, blocking on the propensity score, regression on the
propensity score, and matching on the propensity score.
The fourth category consists of estimators that rely on a
combination of these methods, typically combining regression with one of its alternatives. The motivation for these
combinations is that although in principle any one of these
methods can remove all of the bias associated with the
covariates, combining two may lead to more robust inference. For example, matching leads to consistent estimators
for average treatment effects under weak conditions, so
matching and regression can combine some of the desirable
variance properties of regression with the consistency of
matching. Similarly, a combination of weighting and regression, using parametric models for both the propensity score

12

THE REVIEW OF ECONOMICS AND STATISTICS

and the regression functions, can lead to an estimator that is
consistent even if only one of the models is correctly
specified (“doubly robust” in the terminology of Robins &
Ritov, 1997).
Finally, in the fifth group I will discuss Bayesian approaches to inference for average treatment effects.
Only some of the estimators discussed below achieve the
semiparametric efficiency bound, yet this does not mean
that these should necessarily be preferred in practice—that
is, in finite samples. More generally, the debate concerning
the practical advantages of the various estimators, and the
settings in which some are more attractive than others, is
still ongoing, with as of yet no firm conclusions. Although
all estimators, either implicitly or explicitly, estimate the
two unknown regression functions or the propensity score,
they do so in very different ways. Differences in smoothness
of the regression function or the propensity score, or relative
discreteness of the covariates in specific applications, may
affect the relative attractiveness of the estimators.
In addition, even the appropriateness of the standard
asymptotic distributions as a guide towards finite-sample
performance is still debated (see, for example, Robins &
Ritov, 1997, and Angrist & Hahn, 2004). A key feature that
casts doubt on the relevance of the asymptotic distributions
is that the 公N consistency is obtained by averaging a
nonparametric estimator of a regression function that itself
has a slow nonparametric convergence rate over the empirical distribution of its argument. The dimension of this
argument affects the rate of convergence for the unknown
function [the regression functions ␮ w ( x) or the propensity
score e( x)], but not the rate of convergence for the estimator of the parameter of interest, the average treatment effect.
In practice, however, the resulting approximations of the
ATE can be poor if the argument is of high dimension, in
which case information about the propensity score is of
particular relevance. Although Hahn (1998) showed, as
discussed above, that for the standard asymptotic distributions knowledge of the propensity score is irrelevant (and
conditioning only on the propensity score is in fact less
efficient than conditioning on all covariates), conditioning
on the propensity score involves only one-dimensional nonparametric regression, suggesting that the asymptotic approximations may be more accurate. In practice, knowledge
of the propensity score may therefore be very informative.
Another issue that is important in judging the various
estimators is how well they perform when there is only
limited overlap in the covariate distributions of the two
treatment groups. If there are regions in the covariate
space with little overlap (propensity score close to 0 or
1), ATE estimators should have relatively high variance.
However, this is not always the case for estimators based
on tightly parametrized models for the regression functions, where outliers in covariate values can lead to
spurious precision for regression parameters. Regions of
small overlap can also be difficult to detect directly in

high-dimensional covariate spaces, as they can be
masked for any single variable.
A. Regression

The first class of estimators relies on consistent estimation of ␮ w ( x) for w ⫽ 0, 1. Given ␮ˆ w ( x) for these
regression functions, the PATE, SATE, and CATE are estimated by averaging their differences over the empirical
distribution of the covariates:

冘

1 N
关␮ˆ 共X 兲 ⫺ ␮ˆ 0 共Xi 兲兴.
␶ˆ reg ⫽
N i⫽1 1 i

(2)

In most implementations the average of the predicted
treated outcome for the treated is equal to the average
observed outcome for the treated [so that ¥ i W i 䡠 ␮ˆ 1 (X i ) ⫽
¥ i W i 䡠 Y i ], and similarly for the controls, implying that ␶ˆ reg
can also be written as
1
N

冘 W 䡠 关Y ⫺ ␮ˆ 共X 兲兴 ⫹ 共1 ⫺ W 兲 䡠 关␮ˆ 共X 兲 ⫺ Y 兴.
N

i

i

0

i

i

1

i

i

i⫽1

For the PATT and SATT typically only the control regression function is estimated; we only need predict the outcome under the control treatment for the treated units. The
estimator then averages the difference between the actual
outcomes for the treated and their estimated outcomes under
the control:
␶ˆ reg,T ⫽

冘

1 N
W 䡠 关Yi ⫺ ␮ˆ 0 共Xi 兲兴.
NT i⫽1 i

(3)

Early estimators for ␮ w ( x) included parametric regression functions—for example, linear regression (as in Rubin,
1977). Such parametric alternatives include least squares
estimators with the regression function specified as
␮ w 共 x兲 ⫽ ␤⬘x ⫹ ␶ 䡠 w,
in which case the average treatment effect is equal to ␶. In
this case one can estimate ␶ directly by least squares
estimation using the regression function
Y i ⫽ ␣ ⫹ ␤⬘X i ⫹ ␶ 䡠 W i ⫹ ε i .
More generally, one can specify separate regression functions for the two regimes:
␮ w 共 x兲 ⫽ ␤⬘w x.
In that case one can estimate the two regression functions
separately on the two subsamples and then substitute the
predicted values in equation (2). These simple regression
estimators may be very sensitive to differences in the
covariate distributions for treated and control units. The

AVERAGE TREATMENT EFFECTS

reason is that in that case the regression estimators rely
heavily on extrapolation. To see this, note that the regression
function for the controls, ␮ 0 ( x), is used to predict missing
outcomes for the treated. Hence on average one wishes to
predict the control outcome at X៮ T , the average covariate
value for the treated. With a linear regression function, the
average prediction can be written as Y៮ C ⫹ ␤ˆ ⬘(X៮ T ⫺ X៮ C ).
With X៮ T very close to the average covariate value for the
controls, X៮ C , the precise specification of the regression
function will not matter very much for the average prediction. However, with the two averages very different, the
prediction based on a linear regression function can be very
sensitive to changes in the specification.
More recently, nonparametric estimators have been proposed. Hahn (1998) recommends estimating first the three
conditional expectations g 1 ( x) ⫽ ⺕[WY兩X ], g 0 ( x) ⫽
⺕ [(1 ⫺ W )Y兩X ], and e( x) ⫽ ⺕ [W兩X] nonparametrically
using series methods. He then estimates ␮ w ( x) as
␮ˆ 1 共x兲 ⫽

ĝ1 共x兲
,
ê共x兲

␮ˆ 0 共x兲 ⫽

ĝ0 共x兲
,
1 ⫺ ê共x兲

and shows that the estimators for both PATE and PATT
achieve the semiparametric efficiency bounds discussed in
section IIE (the latter even when the propensity score is
unknown).
Using this series approach, however, it is unnecessary to
estimate all three of these conditional expectations
( ⺕ [YW兩X], ⺕ [Y(1 ⫺ W)兩X], and ⺕ [W兩X]) to estimate
␮ w ( x). Instead one can use series methods to directly
estimate the two regression functions ␮ w ( x), eliminating
the need to estimate the propensity score (Imbens, Newey,
and Ridder, 2003).
Heckman, Ichimura, and Todd (1997, 1998) and Heckman, Ichimura, Smith, and Todd (1998) consider kernel
methods for estimating ␮ w ( x), in particular focusing on
local linear approaches. The simple kernel estimator has the
form
␮ˆ w 共x兲 ⫽

冘

i:W i ⫽w

冉 冊冒 冘 冉 冊

Yi 䡠 K

Xi ⫺ x
h

K

i:W i ⫽w

Xi ⫺ x
,
h

with a kernel K⵺ and bandwidth h. In the local linear
kernel regression the regression function ␮ w ( x) is estimated
as the intercept ␤0 in the minimization problem
min
␤ 0 ,␤ 1

冘

i:W i ⫽w

冉 冊

关Yi ⫺ ␤0 ⫺ ␤⬘1 共Xi ⫺ x兲兴2 䡠 K

Xi ⫺ x
.
h

In order to control the bias of their estimators, Heckman,
Ichimura, and Todd (1998) require that the order of the
kernel be at least as large as the dimension of the covariates.
That is, they require the use of a kernel function K( z) such
that 兰 z z r K( z) dz ⫽ 0 for r ⱕ dim (X), so that the kernel
must be negative on part of the range, and the implicit
averaging involves negative weights. We shall see this role

13

of the dimension of the covariates again for other estimators.
For the average treatment effect for the treated (PATT), it
is important to note that with the propensity score known,
the estimator given in equation (3) is generally not efficient,
irrespective of the estimator for ␮ 0 ( x). Intuitively, this is
because with the propensity score known, the average ¥
W i Y i /N T is not efficient for the population expectation
⺕ [Y(1)兩W ⫽ 1]. An efficient estimator (as in Hahn, 1998)
can be obtained by weighting all the estimated treatment
effects, ␮ˆ 1 (X i ) ⫺ ␮ˆ 0 (X i ), by the probability of receiving the
treatment:

冘 e共X 兲 䡠 关␮ˆ 共X 兲 ⫺ ␮ˆ 共X 兲兴冒 冘 e共X 兲.
N

␶˜ reg,T ⫽

N

i

i⫽1

1

i

0

i

i

(4)

i⫽1

In other words, instead of estimating ⺕ [Y(1)兩W ⫽ 1] as ¥
W i Y i /N T using only the treated observations, it is estimated
using all units, as ¥ ␮ˆ 1 (X i ) 䡠 e(X i )/¥ e(X i ). Knowledge of
the propensity score improves the accuracy because it allows one to exploit the control observations to adjust for
imbalances in the sampling of the covariates.
For all of the estimators in this section an important issue
is the choice of the smoothing parameter. In Hahn’s case,
after choosing the form of the series and the sequence, the
smoothing parameter is the number of terms in the series. In
Heckman, Ichimura, and Todd’s case it is the bandwidth of
the kernel chosen. The evaluation literature has been largely
silent concerning the optimal choice of the smoothing parameters, although the larger literature on nonparametric
estimation of regression functions does provide some guidance, offering data-driven methods such as cross-validation
criteria. The optimality properties of these criteria, however,
are for estimation of the entire function, in this case ␮ w ( x).
Typically the focus is on mean-integrated-squared-error
criteria of the form 兰 x [␮ˆ w ( x) ⫺ ␮ w ( x)] 2 f X ( x) dx, with
possibly an additional weight function. In the current problem, however, one is interested specifically in the average
treatment effect, and so such criteria are not necessarily
optimal. In particular, global smoothing parameters may be
inappropriate, because they can be driven by the shape of
the regression function and distribution of covariates in
regions that are not important for the average treatment
effect of interest. LaLonde’s (1986) data set is a well-known
example of this where much of probability mass of the
nonexperimental control group is in a region with moderate
to high earnings where few of the treated group are located.
There is little evidence whether results for average treatment effects are more or less sensitive to the choice of
smoothing parameter than results for estimation of the
regression functions themselves.
B. Matching

As seen above, regression estimators impute the missing
potential outcomes using the estimated regression function.

14

THE REVIEW OF ECONOMICS AND STATISTICS

Thus, if W i ⫽ 1, then Y i (1) is observed and Y i (0) is missing
and imputed with a consistent estimator ␮ˆ 0 (X i ) for the
conditional expectation. Matching estimators also impute
the missing potential outcomes, but do so using only the
outcomes of nearest neighbors of the opposite treatment
group. In that respect matching is similar to nonparametric
kernel regression methods, with the number of neighbors
playing the role of the bandwidth in the kernel regression. A
formal difference is that the asymptotic distribution is derived conditional on the implicit bandwidth, that is, the
number of neighbors, which is often fixed at one. Using
such asymptotics, the implicit estimate ␮ˆ w ( x) is (close to)
unbiased, but not consistent for ␮ w ( x). In contrast, the
regression estimators discussed in the previous section rely
on the consistency of ␮ w ( x).
Matching estimators have the attractive feature that given
the matching metric, the researcher only has to choose the
number of matches. In contrast, for the regression estimators discussed above, the researcher must choose smoothing
parameters that are more difficult to interpret: either the
number of terms in a series or the bandwidth in kernel
regression. Within the class of matching estimators, using
only a single match leads to the most credible inference with
the least bias, at most sacrificing some precision. This can
make the matching estimator easier to use than those estimators that require more complex choices of smoothing
parameters, and may explain some of its popularity.
Matching estimators have been widely studied in practice
and theory (for example, Gu & Rosenbaum, 1993; Rosenbaum, 1989, 1995, 2002; Rubin, 1973b, 1979; Heckman,
Ichimura, & Todd, 1998; Dehejia & Wahba, 1999; Abadie &
Imbens, 2002). Most often they have been applied in settings with the following two characteristics: (i) the interest
is in the average treatment effect for the treated, and (ii)
there is a large reservoir of potential controls. This allows
the researcher to match each treated unit to one or more
distinct controls (referred to as matching without replacement). Given the matched pairs, the treatment effect within
a pair is then estimated as the difference in outcomes, with
an estimator for the PATT obtained by averaging these
within-pair differences. Since the estimator is essentially the
difference between two sample means, the variance is calculated using standard methods for differences in means or
methods for paired randomized experiments. The remaining
bias is typically ignored in these studies. The literature has
studied fast algorithms for matching the units, as fully
efficient matching methods are computationally cumbersome (see, for example, Gu and Rosenbaum, 1993; Rosenbaum, 1995). Note that in such matching schemes the order
in which the units are matched may be important.
Abadie and Imbens (2002) study both bias and variance
in a more general setting where both treated and control
units are (potentially) matched and matching is done with
replacement (as in Dehejia & Wahba, 1999). The AbadieImbens estimator is implemented in Matlab and Stata (see

Abadie et al., 2003).5 Formally, given a sample, {(Y i , X i ,
N
W i )} i⫽1
, let ᐉ m (i) be the index l that satisfies W l ⫽ W i and

冘

1兵储X j ⫺ X i 储 ⱕ 储X l ⫺ X i 储其 ⫽ m,

j兩W j ⫽W i

where 1{䡠} is the indicator function, equal to 1 if the
expression in brackets is true and 0 otherwise. In other
words, ᐉ m (i) is the index of the unit in the opposite treatment group that is the m th closest to unit i in terms of the
distance measure based on the norm 储 䡠 储. In particular, ᐉ 1 (i)
is the nearest match for unit i. Let ᏶ M (i) denote the set of
indices for the first M matches for unit i: ᏶ M (i) ⫽
{ᐉ 1 (i), . . . , ᐉ M (i)}. Define the imputed potential outcomes
as

冦

Yi
Ŷ i 共0兲 ⫽ 1
M j僆᏶

if Wi ⫽ 0,

冘

Yj if Wi ⫽ 1,

冘

Y j if Wi ⫽ 0,

M 共i兲

and

冦

1
M
Ŷ i 共1兲 ⫽
Yi

j僆᏶ M 共i兲

if Wi ⫽ 1.

The simple matching estimator discussed by Abadie and
Imbens is then
␶ˆ sm
M ⫽

冘

1 N
关Ŷ 共1兲 ⫺ Ŷi 共0兲兴.
N i⫽1 i

(5)

They show that the bias of this estimator is O(N ⫺1/k ), where
k is the dimension of the covariates. Hence, if one studies
the asymptotic distribution of the estimator by normalizing
by 公N [as can be justified by the fact that the variance of
the estimator is O(1/N)], the bias does not disappear if the
dimension of the covariates is equal to 2, and will dominate
the large sample variance if k is at least 3.
Let me make clear three caveats to Abadie and Imbens’s
result. First, it is only the continuous covariates that should
be counted in this dimension, k. With discrete covariates the
matching will be exact in large samples; therefore such
covariates do not contribute to the order of the bias. Second,
if one matches only the treated, and the number of potential
controls is much larger than the number of treated units, one
can justify ignoring the bias by appealing to an asymptotic
sequence where the number of potential controls increases
faster than the number of treated units. Specifically, if the
number of controls, N 0 , and the number of treated, N 1 ,
satisfy N 1 /N 4/k
3 0, then the bias disappears in large
0
samples after normalization by 公N 1. Third, even though
5 See Becker and Ichino (2002) and Sianesi (2001) for alternative Stata
implementations of estimators for average treatment effects.

AVERAGE TREATMENT EFFECTS

the order of the bias may be high, the actual bias may still
be small if the coefficients in the leading term are small.
This is possible if the biases for different units are at least
partially offsetting. For example, the leading term in the
bias relies on the regression function being nonlinear, and
the density of the covariates having a nonzero slope. If one
of these two conditions is at least close to being satisfied, the
resulting bias may be fairly limited. To remove the bias,
Abadie and Imbens suggest combining the matching process with a regression adjustment, as I will discuss in
section IIID.
Another point made by Abadie and Imbens is that matching estimators are generally not efficient. Even in the case
where the bias is of low enough order to be dominated by
the variance, the estimators are not efficient given a fixed
number of matches. To reach efficiency one would need to
increase the number of matches with the sample size. If
M 3 ⬁, with M/N 3 0, then the matching estimator is
essentially like a regression estimator, with the imputed
missing potential outcomes consistent for their conditional
expectations. However, the efficiency gain of such estimators is of course somewhat artificial. If in a given data set
one uses M matches, one can calculate the variance as if this
number of matches increased at the appropriate rate with the
sample size, in which case the estimator would be efficient,
or one could calculate the variance conditional on the
number of matches, in which case the same estimator would
be inefficient. Little is yet known about the optimal number
of matches, or about data-dependent ways of choosing this
number.
In the above discussion the distance metric in choosing
the optimal matches was the standard Euclidean metric:
d E 共 x, z兲 ⫽ 共 x ⫺ z兲⬘共 x ⫺ z兲.
All of the distance metrics used in practice standardize the
covariates in some manner. Abadie and Imbens use the
diagonal matrix of the inverse of the covariate variances:
d AI 共 x, z兲 ⫽ 共 x ⫺ z兲⬘ diag 共⌺X⫺1 兲 共x ⫺ z兲,
where ⌺ X is the covariance matrix of the covariates. The
most common choice is the Mahalanobis metric (see, for
example, Rosenbaum and Rubin, 1985), which uses the
inverse of the covariance matrix of the pretreatment variables:
d M 共 x, z兲 ⫽ 共 x ⫺ z兲⬘⌺ X⫺1 共 x ⫺ z兲.
This metric has the attractive property that it reduces differences in covariates within matched pairs in all directions.6 See for more formal discussions Rubin and Thomas
(1992).
6 However, using the Mahalanobis metric can also have less attractive
implications. Consider the case where one matches on two highly correlated covariates, X 1 and X 2 with equal variances. For specificity, suppose
that the correlation coefficient is 0.9 and both variances are 1. Suppose
that we wish to match a treated unit i with X i1 ⫽ X i2 ⫽ 0. The two

15

Zhao (2004), in an interesting discussion of the choice of
metrics, suggests some alternatives that depend on the
correlation between covariates, treatment assignment, and
outcomes. He starts by assuming that the propensity score
has a logistic form
e共 x兲 ⫽

exp共x⬘␥兲
,
1 ⫹ exp共x⬘␥兲

and that the regression functions are linear:
␮ w 共 x兲 ⫽ ␣ w ⫹ x⬘␤.
He then considers two alternative metrics. The first weights
absolute differences in the covariates by the coefficient in
the propensity score:

冘 兩x ⫺ z 兩 䡠 兩␥ 兩,
K

d Z1 共 x, z兲 ⫽

k

k

k

k⫽1

and the second weights them by the coefficients in the
regression function:

冘 兩x ⫺ z 兩 䡠 兩␤ 兩,
K

d Z2 共 x, z兲 ⫽

k

k

k

k⫽1

where x k and z k are the k th elements of the K-dimensional
vectors x and z respectively.
In light of this discussion, it is interesting to consider
optimality of the metric. Suppose, following Zhao (2004),
that the regression functions are linear with coefficients ␤ w .
Now consider a treated unit with covariate vector x who will
be matched to a control unit with covariate vector z. The
bias resulting from such a match is ( z ⫺ x)⬘␤ 0 . If one is
interested in minimizing for each match the squared bias,
one should choose the first match by minimizing over the
control observations ( z ⫺ x)⬘␤ 0 ␤⬘0 ( z ⫺ x). Yet typically
one does not know the value of the regression coefficients,
in which case one may wish to minimize the expected
squared bias. Using a normal distribution for the regression
errors, and a flat prior on ␤0, the posterior distribution for ␤0
2
is normal with mean ␤ˆ 0 and variance ⌺ ⫺1
X ␴ /N. Hence the
expected squared bias from a match is
potential matches are unit j with X j1 ⫽ X j2 ⫽ 5 and unit k with X k1 ⫽ 4
and X k2 ⫽ 0. The difference in covariates for the first match is the vector
(5, 5)⬘, and the difference in covariates for the second match is (4, 0)⬘.
Intuitively it may seem that the second match is better: it is strictly closer
to the treated unit than the first match for both covariates. Using the
Abadie-Imbens metric diag (⌺ X⫺1), this is in fact true. Under that metric
the distance between the second match and the treated unit is 16,
considerably smaller than 50, the distance between the first match and the
treated unit. Using the Mahalanobis metric, however, the distance for the
first match is 26, and the distance for the second match is much higher at
84. Because of the correlation between the covariates in the sample, the
difference between the matches is interpreted very differently under the
two metrics. To choose between the standard and the Mahalanobis metric
one needs to consider what the appropriate match would be in this case.

16

THE REVIEW OF ECONOMICS AND STATISTICS

⺕关共 z ⫺ x兲⬘␤ 0 ␤⬘0 共 z ⫺ x兲兴 ⫽ 共 z ⫺ x兲⬘共␤ˆ 0 ␤ˆ 0⬘ ⫹ ␴2 ⌺X⫺1 /N兲
⫻ 共z ⫺ x兲.
In this argument the optimal metric is a combination of the
sample covariance matrix plus the outer product of the
regression coefficients, with the former scaled down by a
factor 1/N:
⫺1
/N兲共z ⫺ x兲.
d*共 z, x兲 ⫽ 共 z ⫺ x兲⬘共␤ˆ w ␤ˆ w⬘ ⫹ ␴w2 ⌺X,w

A clear problem with this approach is that when the regression function is misspecified, matching with this particular
metric may not lead to a consistent estimator. On the other
hand, when the regression function is correctly specified, it
would be more efficient to use the regression estimators
than any matching approach. In practice one may want to
use a metric that combines some of the optimal weighting
with some safeguards in case the regression function is
misspecified.
So far there is little experience with any alternative
metrics beyond the Mahalanobis metric. Zhao (2004) reports the results of some simulations using his proposed
metrics, finding no clear winner given his specific design,
although his findings suggest that using the outcomes in
defining the metric is a promising approach.

two conditional expectations ␮ w ( x), they require instead the
equally high-dimensional nonparametric regression of the
treatment indicator on the covariates. In practice the relative
merits of these estimators will depend on whether the
propensity score is more or less smooth than the regression
functions, and on whether additional information is available about either the propensity score or the regression
functions.
Weighting: The first set of propensity-score estimators
use the propensity scores as weights to create a balanced
sample of treated and control observations. Simply taking
the difference in average outcomes for treated and controls,
␶ˆ ⫽

i i

i

i

i

i

is not unbiased for ␶ P ⫽ ⺕ [Y(1) ⫺ Y(0)], because,
conditional on the treatment indicator, the distributions of
the covariates differ. By weighting the units by the reciprocal of the probability of receiving the treatment, one can
undo this imbalance. Formally, weighting estimators rely on
the equalities
⺕

C. Propensity Score Methods

Since the work by Rosenbaum and Rubin (1983a) there
has been considerable interest in methods that avoid adjusting directly for all covariates, and instead focus on adjusting
for differences in the propensity score, the conditional
probability of receiving the treatment. This can be implemented in a number of different ways. One can weight the
observations using the propensity score (and indirectly also
in terms of the covariates) to create balance between treated
and control units in the weighted sample. Hirano, Imbens,
and Ridder (2003) show how such estimators can achieve
the semiparametric efficiency bound. Alternatively one can
divide the sample into subsamples with approximately the
same value of the propensity score, a technique known as
blocking. Finally, one can directly use the propensity score
as a regressor in a regression approach.
In practice there are two important cases. First, suppose
the researcher knows the propensity score. In that case all
three of these methods are likely to be effective in eliminating bias. Even if the resulting estimator is not fully
efficient, one can easily modify it by using a parametric
estimate of the propensity score to capture most of the
efficiency loss. Furthermore, since these estimators do not
rely on high-dimensional nonparametric regression, this
suggests that their finite-sample properties are likely to be
relatively attractive.
If the propensity score is not known, the advantages of
the estimators discussed below are less clear. Although they
avoid the high-dimensional nonparametric regression of the

冘 W Y ⫺ 冘 共1 ⫺ W 兲Y ,
冘W 冘1⫺W

冋 册 冋
冋

册 冋 冋 冏 册册
册

WY
WY共1兲
WY共1兲
⫽⺕
⫽⺕ ⺕
X
e共X兲
e共X兲
e共X兲
⫽⺕

e共X兲 䡠 ⺕关Y共1兲兩X兴
⫽ ⺕关Y共1兲兴,
e共X兲

using unconfoundedness in the second to last equality, and
similarly
⺕

冋

册

共1 ⫺ W兲Y
⫽ ⺕关Y共0兲兴,
1 ⫺ e共X兲

implying
␶P ⫽ ⺕

冋

册

W 䡠 Y 共1 ⫺ W兲 䡠 Y
⫺
.
e共X兲
1 ⫺ e共X兲

With the propensity score known one can directly implement this estimator as

冘冉

冊

1 N Wi Yi 共1 ⫺ Wi 兲Yi
⫺
.
␶˜ ⫽
N i⫽1 e共Xi 兲
1 ⫺ e共Xi 兲

(6)

In this particular form this is not necessarily an attractive
estimator. The main reason is that, although the estimator
can be written as the difference between a weighted average
of the outcomes for the treated units and a weighted average
of the outcomes for the controls, the weights do not necessarily add to 1. Specifically, in equation (6), the weights for
the treated units add up to [¥ W i /e(X i )]/N. In expectation
this is equal to 1, but because its variance is positive, in any

AVERAGE TREATMENT EFFECTS

given sample some of the weights are likely to deviate
from 1.
One approach for improving this estimator is simply to
normalize the weights to unity. One can further normalize
the weights to unity within subpopulations as defined by the
covariates. In the limit this leads to an estimator proposed
by Hirano, Imbens, and Ridder (2003), who suggest using a
nonparametric series estimator for e( x). More precisely,
they first specify a sequence of functions of the covariates,
such as power series h l ( x), l ⫽ 1, . . . , ⬁. Next, they
choose a number of terms, L(N), as a function of the sample
size, and then estimate the L-dimensional vector ␥ L in
Pr共W ⫽ 1兩X ⫽ x兲 ⫽

exp关共h1 共x兲, . . . , hL 共x兲兲␥L 兴
,
1 ⫹ exp关共h1 共x兲, . . . , hL 共x兲兲␥L 兴

by maximizing the associated likelihood function. Let ␥ˆ L be
the maximum likelihood estimate. In the third step, the
estimated propensity score is calculated as
ê共 x兲 ⫽

contribution for unit i by the propensity score e( x i ). If the
propensity score is known, this leads to

冘

冘
N

⫺

i⫽1

冒冘

Wi
ê共X
i兲
i⫽1

共1 ⫺ W i 兲 䡠 Y i
1 ⫺ ê共X i 兲

冒冘
N

i⫽1

i

(7)
1 ⫺ Wi
.
1 ⫺ ê共X i 兲

Hirano, Imbens, and Ridder show that with a nonparametric
estimator for e( x) this estimator is efficient, whereas with
the true propensity score the estimator would not be fully
efficient (and in fact not very attractive).
This estimator highlights one of the interesting features of
the problem of efficiently estimating average treatment
effects. One solution is to estimate the two regression
functions ␮ w ( x) nonparametrically, as discussed in Section
IIIA; that solution completely ignores the propensity score.
A second approach is to estimate the propensity score
nonparametrically, ignoring entirely the two regression
functions. If appropriately implemented, both approaches
lead to fully efficient estimators, but clearly their finitesample properties may be very different, depending, for
example, on the smoothness of the regression functions
versus the smoothness of the propensity score. If there is
only a single binary covariate, or more generally if there are
only discrete covariates, the weighting approach with a fully
nonparametric estimator for the propensity score is numerically identical to the regression approach with a fully
nonparametric estimator for the two regression functions.
To estimate the average treatment effect for the treated
rather than for the full population, one should weight the

i

i

i

i

i

i⫽1

冘 共1 ⫺ W 兲 䡠 Y 䡠 1 ⫺e共Xê共X兲 兲冒 冘 共1 ⫺ W 兲 1 ⫺e共Xê共X兲 兲 ,
N

⫺

N

i

i

i

i

i

i

i⫽1

i

i⫽1

where the propensity score enters in some places as the true
score (for the weights to get the appropriate estimand) and
in other cases as the estimated score (to achieve efficiency).
In the unknown propensity score case one always uses the
estimated propensity score, leading to
␶ˆ weight,tr ⫽

冋

冘

1
Y
N1 i : W ⫽1 i

⫺

N

N

i

i⫽1

Finally they estimate the average treatment effect as
Wi 䡠 Yi
␶ˆ weight ⫽
ê共Xi 兲
i⫽1

e共X 兲
冘 W 䡠 Y 䡠 ê共X
冒兲 冘 W ê共Xe共X 兲兲
N

␶ˆ weight,tr ⫽

exp关共h1 共x兲, . . . , hL 共x兲兲␥ˆ L 兴
.
1 ⫹ exp关共h1 共x兲, . . . , hL 共x兲兲␥ˆ L 兴

N

17

i

冋

冘

i : W i ⫽0

册

Yi 䡠

ê共X i 兲
1 ⫺ ê共X i 兲

冒冘

i : W i ⫽0

册

ê共X i 兲
.
1 ⫺ ê共X i 兲

One difficulty with the weighting estimators that are
based on the estimated propensity score is again the problem of choosing the smoothing parameters. Hirano, Imbens,
and Ridder (2003) use series estimators, which requires
choosing the number of terms in the series. Ichimura and
Linton (2001) consider a kernel version, which involves
choosing a bandwidth. Theirs is currently one of the few
studies considering optimal choices for smoothing parameters that focuses specifically on estimating average treatment effects. A departure from standard problems in choosing smoothing parameters is that here one wants to use
nonparametric regression methods even if the propensity
score is known. For example, if the probability of treatment
is constant, standard optimality results would suggest using
a high degree of smoothing, as this would lead to the most
accurate estimator for the propensity score. However, this
would not necessarily lead to an efficient estimator for the
average treatment effect of interest.
Blocking on the Propensity Score: In their original
propensity-score paper Rosenbaum and Rubin (1983a) suggest the following blocking-on-the-propensity-score estimator. Using the (estimated) propensity score, divide the sample into M blocks of units of approximately equal
probability of treatment, letting J im be an indicator for unit
i being in block m. One way of implementing this is by
dividing the unit interval into M blocks with boundary
values equal to m/M for m ⫽ 1, . . . , M ⫺ 1, so that
J im ⫽ 1

再

m⫺1
m
⬍ e共X i 兲 ⱕ
M
M

冎

18

THE REVIEW OF ECONOMICS AND STATISTICS

for m ⫽ 1, . . . , M. Within each block there are N wm
observations with treatment equal to w, N wm ⫽ ¥ i 1{W i ⫽
w, J im ⫽ 1}. Given these subgroups, estimate within each
block the average treatment effect as if random assignment
held:
␶ˆ m ⫽

冘

冘

1 N
1 N
Jim Wi Yi ⫺
J 共1 ⫺ Wi 兲Yi .
N1m i⫽1
N0m i⫽1 im

Then estimate the overall average treatment effect as

冘 ␶ˆ
M

␶ˆ block ⫽

m

䡠

m⫽1

N1m ⫹ N0m
.
N

If one is interested in the average effect for the treated, one
will weight the within-block average treatment effects by
the number of treated units:

冘 ␶ˆ
M

␶ˆ T,block ⫽

m⫽1

m

䡠

N1m
.
NT

Blocking can be interpreted as a crude form of nonparametric regression where the unknown function is approximated by a step function with fixed jump points. To establish asymptotic properties for this estimator would require
establishing conditions on the rate at which the number of
blocks increases with the sample size. With the propensity
score known, these are easy to determine; no formal results
have been established for the unknown propensity score
case.
The question arises how many blocks to use in practice.
Cochran (1968) analyzes a case with a single covariate and,
assuming normality, shows that using five blocks removes at
least 95% of the bias associated with that covariate. Since
all bias, under unconfoundedness, is associated with the
propensity score, this suggests that under normality the use
of five blocks removes most of the bias associated with all
the covariates. This has often been the starting point of
empirical analyses using this estimator (for example,
Rosenbaum and Rubin, 1983b; Dehejia and Wahba, 1999)
and has been implemented in Stata by Becker and Ichino
(2002).7 Often, however, researchers subsequently check
the balance of the covariates within each block. If the true
propensity score per block is constant, the distribution of the
covariates among the treated and controls should be identical, or, in the evaluation terminology, the covariates should
be balanced. Hence one can assess the adequacy of the
statistical model by comparing the distribution of the covariates among treated and controls within blocks. If the
distributions are found to be different, one can either split
the blocks into a number of subblocks, or generalize the
7 Becker and Ichino also implement estimators that match on the
propensity score.

specification of the propensity score. Often some informal
version of the following algorithm is used: If within a block
the propensity score itself is unbalanced, the blocks are too
large and need to be split. If, conditional on the propensity
score being balanced, the covariates are unbalanced, the
specification of the propensity score is not adequate. No
formal algorithm has been proposed for implementing these
blocking methods.
An alternative approach to finding the optimal number of
blocks is to relate this approach to the weighting estimator
discussed above. One can view the blocking estimator as
identical to a weighting estimator, with a modified estimator
for the propensity score. Specifically, given the original
estimator ê( x), in the blocking approach the estimator for
the propensity score is discretized to
ẽ共 x兲 ⫽

1
M

冘 1再 Mm ⱕ ê共 x兲冎 .
M

m⫽1

Using ẽ( x) as the propensity score in the weighting estimator leads to an estimator for the average treatment effect
identical to that obtained by using the blocking estimator
with ê( x) as the propensity score and M blocks. With
sufficiently large M, the blocking estimator is sufficiently
close to the original weighting estimator that it shares its
first-order asymptotic properties, including its efficiency.
This suggests that in general there is little harm in choosing
a large number of blocks, at least with regard to asymptotic
properties, although again the relevance of this for finite
samples has not been established.
Regression on the Propensity Score: The third method
of using the propensity score is to estimate the conditional
expectation of Y given W and e(X). Define
␯ w 共e兲 ⫽ ⺕关Y共w兲兩e共X兲 ⫽ e兴.
By unconfoundedness this is equal to ⺕ [Y兩W ⫽ w, e(X) ⫽
e]. Given an estimator ␯ˆ w (e), one can estimate the average
treatment effect as

冘

1 N
关␯ˆ 共e共Xi 兲兲 ⫺ ␯ˆ 0 共e共Xi 兲兲兴.
␶ˆ regprop ⫽
N i⫽1 1
Heckman, Ichimura, and Todd (1998) consider a local linear
version of this for estimating the average treatment effect
for the treated. Hahn (1998) considers a series version and
shows that it is not as efficient as the regression estimator
based on adjustment for all covariates.
Matching on the Propensity Score: Rosenbaum and
Rubin’s result implies that it is sufficient to adjust solely for
differences in the propensity score between treated and
control units. Since one of the ways in which one can adjust
for differences in covariates is matching, another natural

AVERAGE TREATMENT EFFECTS

way to use the propensity score is through matching. Because the propensity score is a scalar function of the covariates, the bias results in Abadie and Imbens (2002) imply
that the bias term is of lower order than the variance term
and matching leads to a 公N-consistent, asymptotically
normally distributed estimator. The variance for the case
with matching on the true propensity score also follows
directly from their results. More complicated is the case
with matching on the estimated propensity score. I do not
know of any results that give the variance for this case.
D. Mixed Methods

A number of approaches have been proposed that combine two of the three methods described in the previous
sections, typically regression with one of its alternatives.
The reason for these combinations is that, although one
method alone is often sufficient to obtain consistent or even
efficient estimates, incorporating regression may eliminate
remaining bias and improve precision. This is particularly
useful in that neither matching nor the propensity-score
methods directly address the correlation between the covariates and the outcome. The benefit associated with combining methods is made explicit in the notion developed by
Robins and Ritov (1997) of double robustness. They propose a combination of weighting and regression where, as
long as the parametric model for either the propensity score
or the regression functions is specified correctly, the resulting estimator for the average treatment effect is consistent.
Similarly, matching leads to consistency without additional
assumptions; thus methods that combine matching and regressions are robust against misspecification of the regression function.
Weighting and Regression: One can rewrite the weighting estimator discussed above as estimating the following
regression function by weighted least squares:
Y i ⫽ ␣ ⫹ ␶ 䡠 W i ⫹ ε i,
with weights equal to
␭i ⫽

冑

1 ⫺ Wi
Wi
⫹
.
e共X i 兲 1 ⫺ e共X i 兲

Without the weights the least squares estimator would not
be consistent for the average treatment effect; the weights
ensure that the covariates are uncorrelated with the treatment indicator and hence the weighted estimator is consistent.
This weighted-least-squares representation suggests that
one may add covariates to the regression function to improve precision, for example,
Y i ⫽ ␣ ⫹ ␤⬘X i ⫹ ␶ 䡠 W i ⫹ ε i ,

19

with the same weights ␭ i . Such an estimator, using a more
general semiparametric regression model, was suggested by
Robins and Rotnitzky (1995), Robins, Roznitzky, and Zhao
(1995), and Robins and Ritov (1997), and implemented by
Hirano and Imbens (2001). In the parametric context Robins
and Ritov argue that the estimator is consistent as long as
either the regression model or the propensity score (and thus
the weights) are specified correctly. That is, in Robins and
Ritov’s terminology, the estimator is doubly robust.
Blocking and Regression: Rosenbaum and Rubin
(1983b) suggest modifying the basic blocking estimator by
using least squares regression within the blocks. Without the
additional regression adjustment the estimated treatment
effect within blocks can be written as a least squares
estimator of ␶ m for the regression function
Y i ⫽ ␣ m ⫹ ␶ m 䡠 W i ⫹ ε i,
using only the units in block m. As above, one can also add
covariates to the regression function
Y i ⫽ ␣ m ⫹ ␤⬘m X i ⫹ ␶ m 䡠 W i ⫹ ε i ,
again estimated on the units in block m.
Matching and Regression: Because Abadie and Imbens
(2002) have shown that the bias of the simple matching
estimator can dominate the variance if the dimension of the
covariates is too large, additional bias corrections through
regression can be particularly relevant in this case. A number of such corrections have been proposed, first by Rubin
(1973b) and Quade (1982) in a parametric setting. Following the notation of section IIIB, let Ŷ i (0) and Ŷ i (1) be the
observed or imputed potential outcomes for unit i; the
estimated potential outcomes equal the observed outcomes
for some unit i and for its match ᐉ(i). The bias in their
comparison, ⺕ [Ŷ i (1) ⫺ Ŷ i (0)] ⫺ [Y i (1) ⫺ Y i (0)], arises
from the fact that the covariates X i and X ᐉ(i) for units i and
ᐉ(i) are not equal, although they are close because of the
matching process.
To further explore this, focusing on the single-match
case, define for each unit

再

Xi
if Wi ⫽ 0,
X̂ i 共0兲 ⫽ X
ᐉ 1 共i兲 if Wi ⫽ 1
and
X̂ i 共1兲 ⫽

再

X ᐉ 1共i兲 if Wi ⫽ 0,
Xi
if Wi ⫽ 1.

If the matching is exact, X̂ i (0) ⫽ X̂ i (1) for each unit. If not,
these discrepancies may lead to bias. The difference
X̂ i (1) ⫺ X̂ i (0) will therefore be used to reduce the bias of
the simple matching estimator.

20

THE REVIEW OF ECONOMICS AND STATISTICS

Suppose unit i is a treated unit (W i ⫽ 1), so that Ŷ i (1) ⫽
Y i (1) and Ŷ i (0) is an imputed value for Y i (0). This imputed
value is unbiased for ␮ 0 (X ᐉ 1(i) ) (since Ŷ i (0) ⫽ Y ᐉ(i) ), but
not necessarily for ␮ 0 (X i ). One may therefore wish to adjust
Ŷ i (0) by an estimate of ␮ 0 (X i ) ⫺ ␮ 0 (X ᐉ 1(i) ). Typically these
corrections are taken to be linear in the difference in the
covariates for unit i and its match, that is, of the form
␤⬘0 [X̂ i (1) ⫺ X̂ i (0)] ⫽ ␤⬘0 (X i ⫺ X ᐉ 1(i) ). Rubin (1973b)
proposed three corrections, which differ in how ␤0 is estimated.
To introduce Rubin’s first correction, note that one can
write the matching estimator as the least squares estimator
for the regression function
Ŷ i 共1兲 ⫺ Ŷ i 共0兲 ⫽ ␶ ⫹ ε i .
This representation suggests modifying the regression function to
Ŷ i 共1兲 ⫺ Ŷ i 共0兲 ⫽ ␶ ⫹ 关X̂ i 共1兲 ⫺ X̂ i 共0兲兴⬘␤ ⫹ ε i ,
and again estimating ␶ by least squares.
The second correction is to estimate ␮ 0 ( x) directly by
taking all control units, and estimate a linear regression of
the form
Y i ⫽ ␣ 0 ⫹ ␤⬘0 X i ⫹ ε i
by least squares. [If unit i is a control unit, the correction
will be done using an estimator for the regression function
␮ 1 ( x) based on a linear specification Y i ⫽ ␣ 1 ⫹ ␤⬘1 X i
estimated on the treated units.] Abadie and Imbens (2002)
show that if this correction is done nonparametrically, the
resulting matching estimator is consistent and asymptotically normal, with its bias dominated by the variance.
The third method is to estimate the same regression
function for the controls, but using only those that are used
as matches for the treated units, with weights corresponding
to the number of times a control observations is used as a
match (see Abadie and Imbens, 2002). Compared to the
second method, this approach may be less efficient, as it
discards some control observations and weights some more
than others. It has the advantage, however, of only using the
most relevant matches. The controls that are discarded in the
matching process are likely to be outliers relative to the
treated observations, and they may therefore unduly affect
the least squares estimates. If the regression function is in
fact linear, this may be an attractive feature, but if there is
uncertainty over its functional form, one may not wish to
allow these observations such influence.
E. Bayesian Approaches

Little has been done using Bayesian methods to estimate
average treatment effects, either in methodology or in application. Rubin (1978) introduces a general approach to
estimating average and distributional treatment effects from

a Bayesian perspective. Dehejia (2002) goes further, studying the policy decision problem of assigning heterogeneous
individuals to various training programs with uncertain and
variable effects.
To my knowledge, however, there are no applications
using the Bayesian approach that focus on estimating the
average treatment effect under unconfoundedness, either for
the whole population or just for the treated. Neither are there
simulation studies comparing operating characteristics of
Bayesian methods with the frequentist methods discussed in
the earlier sections of this paper. Such a Bayesian approach
can be easily implemented with the regression methods
discussed in section IIIA. Interestingly, it is less clear how
Bayesian methods would be used with pairwise matching,
which does not appear to have a natural likelihood interpretation.
A Bayesian approach to the regression estimators may be
useful for a number of reasons. First, one of the leading
problems with regression estimators is the presence of many
covariates relative to the number of observations. Standard
frequentist methods tend to either include those covariates
without any restrictions, or exclude them entirely. In contrast, Bayesian methods would allow researchers to include
covariates with more or less informative prior distributions.
For example, if the researcher has a number of lagged
outcomes, one may expect recent lags to be more important
in predicting future outcomes than longer lags; this can be
reflected in tighter prior distributions around zero for the
older information. Alternatively, with a number of similar
covariates one may wish to use hierarchical models that
avoid problems with large-dimensional parameter spaces.
A second argument for considering Bayesian methods is
that in an area closely related to this process of estimated
unobserved outcomes—that of missing data with the missing at random (MAR) assumption—Bayesian methods have
found widespread applicability. As advocated by Rubin
(1987), multiple imputation methods often rely on a Bayesian approach for imputing the missing data, taking account
of the parameter heterogeneity in a manner consistent with
the uncertainty in the missing-data model itself. The same
methods could be used with little modification for causal
models, with the main complication that a relatively large
proportion—namely 50% of the total number of potential
outcomes—is missing.
IV.

Estimating Variances

The variances of the estimators considered so far typically involve unknown functions. For example, as discussed
in section IIE, the variance of efficient estimators of the
PATE is equal to

冋

册

␴ 02 共X兲
␴ 12 共X兲
⫹
⫹ 共␮ 1 共X兲 ⫺ ␮ 0 共X兲 ⫺ ␶兲 2 ,
V ⫽⺕
e共X兲
1 ⫺ e共X兲
P

involving the two regression functions, the two conditional
variances, and the propensity score.

AVERAGE TREATMENT EFFECTS

There are a number of ways we can estimate this asymptotic variance. The first is essentially by brute force. All five
components of the variance, ␴ 20 ( x), ␴ 21 ( x), ␮ 0 ( x), ␮ 1 ( x),
and e( x), are consistently estimable using kernel methods
or series, and hence the asymptotic variance can be estimated consistently. However, if one estimates the average
treatment effect using only the two regression functions, it is
an additional burden to estimate the conditional variances
and the propensity score in order to estimate V P . Similarly,
if one efficiently estimates the average treatment effect by
weighting with the estimated propensity score, it is a considerable additional burden to estimate the first two moments of the conditional outcome distributions just to estimate the asymptotic variance.
A second method applies to the case where either the
regression functions or the propensity score is estimated
using series or sieves. In that case one can interpret the
estimators, given the number of terms in the series, as
parametric estimators, and calculate the variance this way.
Under some conditions that will lead to valid standard errors
and confidence intervals.
A third approach is to use bootstrapping (Efron and
Tibshirani, 1993; Horowitz, 2002). There is little formal
evidence specific for these estimators, but, given that the
estimators are asymptotically linear, it is likely that bootstrapping will lead to valid standard errors and confidence
intervals at least for the regression and propensity score
methods. Bootstrapping may be more complicated for
matching estimators, as the process introduces discreteness
in the distribution that will lead to ties in the matching
algorithm. Subsampling (Politis and Romano, 1999) will
still work in this setting.
These first three methods provide variance estimates for
estimators of ␶ P . As argued above, however, one may
instead wish to estimate ␶ S or ␶(X), in which case the
appropriate (conservative) variance is
VS ⫽ ⺕

冋

册

␴ 02 共X兲
␴ 12 共X兲
⫹
.
e共X兲
1 ⫺ e共X兲

As above, this variance can be estimated by estimating the
conditional moments of the outcome distributions, with the
accompanying inherent difficulties. V S cannot, however, be
estimated by bootstrapping, since the estimand itself
changes across bootstrap samples.
There is, however, an alternative method for estimating
this variance that does not require additional nonparametric
estimation. The idea behind this matching variance estimator, as developed by Abadie and Imbens (2002), is that even
though the asymptotic variance depends on the conditional
variance ␴ 2w ( x), one need not actually estimate this variance
consistently at all values of the covariates. Rather, one needs
only the average of this variance over the distribution,
weighted by the inverse of either e( x) or its complement
1 ⫺ e( x). The key is therefore to obtain a close-to-unbiased
estimator for the variance ␴ 2w ( x). More generally, suppose

21

we can find two treated units with X ⫽ x, say units i and j.
In that case an unbiased estimator for ␴ 21 ( x) is
␴ˆ 12 共x兲 ⫽ 共Yi ⫺ Yj 兲2 /2.
In general it is again difficult to find exact matches, but
again, this is not necessary. Instead, one uses the closest
match within the set of units with the same treatment
indicator. Let v m (i) be the mth closest unit to i with the
same treatment indicator (W v m(i) ⫽ W i ), and

冘

1兵储X l ⫺ x储 ⱕ 储X v m共i兲 ⫺ x储其 ⫽ m.

l兩W l ⫽W i ,l⫽i

Given a fixed number of matches, M, this gives us M units
with the same treatment indicator and approximately the
same values for the covariates. The sample variance of the
outcome variable for these M units can then be used to
estimate ␴ 21 ( x). Doing the same for the control variance
function, ␴ 20 ( x), we can estimate ␴ 2w ( x) at all values of the
covariates and for w ⫽ 0, 1.
Note that these are not consistent estimators of the conditional variances. As the sample size increases, the bias of
these estimators will disappear, just as we saw that the bias
of the matching estimator for the average treatment effect
disappears under similar conditions. The rate at which this
bias disappears depends on the dimension of the covariates.
The variance of the estimators for ␴ 2w (X i ), namely at specific values of the covariates, will not go to zero; however,
this is not important, as we are interested not in the variances at specific points in the covariates distribution, but in
the variance of the average treatment effect, V S . Following
the process introduce above, this last step is estimated as
1
V̂ ⫽
N
S

冘冉
N

i⫽1

冊

␴ˆ 02 共Xi 兲
␴ˆ 12 共Xi 兲
⫹
.
ê共Xi 兲
1 ⫺ ê共Xi 兲

Under standard regularity conditions this is consistent for
the asymptotic variance of the average treatment effect
estimator. For matching estimators even estimation of the
propensity score can be avoided. Abadie and Imbens show
that one can estimate the variance of the matching estimator
for SATE as:
1
V̂ ⫽
N
E

冘冉
N

i⫽1

冊

K M 共i兲 2 2
1⫹
␴ˆ W i共Xi 兲,
M

where M is the number of matches and K M (i) is the number
of times unit i is used as a match.
V.

Assessing the Assumptions

A. Indirect Tests of the Unconfoundedness Assumption

The unconfoundedness assumption relied upon throughout this discussion is not directly testable. As discussed

22

THE REVIEW OF ECONOMICS AND STATISTICS

above, it states that the conditional distribution of the
outcome under the control treatment, Y(0), given receipt of
the active treatment and given covariates, is identical to the
distribution of the control outcome given receipt of the
control treatment and given covariates. The same is assumed for the distribution of the active treatment outcome,
Y(1). Because the data are completely uninformative about
the distribution of Y(0) for those who received the active
treatment and of Y(1) for those who received the control,
the data cannot directly reject the unconfoundedness assumption. Nevertheless, there are often indirect ways of
assessing this assumption, a number of which are developed
in Heckman and Hotz (1989) and Rosenbaum (1987). These
methods typically rely on estimating a causal effect that is
known to equal zero. If the test then suggests that this causal
effect differs from zero, the unconfoundedness assumption
is considered less plausible. These tests can be divided into
two broad groups.
The first set of tests focuses on estimating the causal
effect of a treatment that is known not to have an effect,
relying on the presence of multiple control groups (Rosenbaum, 1987). Suppose one has two potential control groups,
for example, eligible nonparticipants and ineligibles, as in
Heckman, Ichimura, and Todd (1997). One interpretation of
the test is to compare average treatment effects estimated
using each of the control groups. This can also be interpreted as estimating an “average treatment effect” using
only the two control groups, with the treatment indicator
now a dummy for being a member of the first group. In that
case the treatment effect is known to be zero, and statistical
evidence of a nonzero effect implies that at least one of the
control groups is invalid. Again, not rejecting the test does
not imply the unconfoundedness assumption is valid (as
both control groups could suffer the same bias), but nonrejection in the case where the two control groups are likely to
have different potential biases makes it more plausible that
the unconfoundedness assumption holds. The key for the
power of this test is to have available control groups that are
likely to have different biases, if any. Comparing ineligibles
and eligible nonparticipants as in Heckman, Ichimura, and
Todd (1997) is a particularly attractive comparison. Alternatively one may use different geographic controls, for
example from areas bordering on different sides of the
treatment group.
One can formalize this test by postulating a three-valued
indicator T i 僆 {⫺1, 0, 1} for the groups (e.g., ineligibles,
eligible nonparticipants, and participants), with the treatment indicator equal to W i ⫽ 1{T i ⫽ 1}. If one extends the
unconfoundedness assumption to independence of the potential outcomes and the group indicator given covariates,
Y i 共0兲, Y i 共1兲 ⬜ T i 兩X i ,
then a testable implication is
Y i ⬜ 1兵T i ⫽ 0其兩X i , T i ⱕ 0.

An implication of this independence condition is being
tested by the tests discussed above. Whether this test has
much bearing on the unconfoundedness assumption depends on whether the extension of the assumption is plausible given unconfoundedness itself.
The second set of tests of unconfoundedness focuses on
estimating the causal effect of the treatment on a variable
known to be unaffected by it, typically because its value is
determined prior to the treatment itself. Such a variable can
be time-invariant, but the most interesting case is in considering the treatment effect on a lagged outcome. If this is
not zero, this implies that the treated observations are
distinct from the controls; namely, that the distribution of
Y i,⫺1 for the treated units is not comparable to the distribution of Y i,⫺1 for the controls. If the treatment is instead zero,
it is more plausible that the unconfoundedness assumption
holds. Of course this does not directly test this assumption;
in this setting, being able to reject the null of no effect does
not directly reflect on the hypothesis of interest, unconfoundedness. Nevertheless, if the variables used in this
proxy test are closely related to the outcome of interest, the
test arguably has more power. For these tests it is clearly
helpful to have a number of lagged outcomes.
To formalize this, let us suppose the covariates consist of
a number of lagged outcomes Y i,⫺1 , . . . , Y i,⫺T as well as
time-invariant individual characteristics Z i , so that X i ⫽
(Y i,⫺1 , . . . , Y i,⫺T , Z i ). By construction only units in the
treatment group after period ⫺1 receive the treatment; all
other observed outcomes are control outcomes. Also suppose that the two potential outcomes Y i (0) and Y i (1) correspond to outcomes in period zero. Now consider the
following two assumptions. The first is unconfoundedness
given only T ⫺ 1 lags of the outcome:
Y i 共1兲, Y i 共0兲 ⬜ W i 兩Y i,⫺1 , . . . , Y i,⫺共T⫺1兲 , Z i ,
and the second assumes stationarity and exchangeability:
f Y i,s共0兲兩Y i,s⫺1共0兲, . . . ,Y i,s⫺共T⫺1兲共0兲,Z i,W i共 y s 兩y s⫺1 , . . . , y s⫺共T⫺1兲 , z, w兲
does not depend on i and s. Then it follows that
Y i,⫺1 ⬜ W i 兩Y i,⫺2 , . . . , Y i,⫺T , Z i ,
which is testable. This hypothesis is what the test described
above tests. Whether this test has much bearing on unconfoundedness depends on the link between the two assumptions and the original unconfoundedness assumption. With a
sufficient number of lags, unconfoundedness given all lags
but one appears plausible, conditional on unconfoundedness
given all lags, so the relevance of the test depends largely on
the plausibility of the second assumption, stationarity and
exchangeability.
B. Choosing the Covariates

The discussion so far has focused on the case where the
covariates set is known a priori. In practice there can be two

AVERAGE TREATMENT EFFECTS

issues with the choice of covariates. First, there may be
some variables that should not be adjusted for. Second, even
with variables that should be adjusted for in large samples,
the expected mean squared error may be reduced by ignoring those covariates that have only weak correlation with
the treatment indicator and the outcomes. This second issue
is essentially a statistical one. Including a covariate in the
adjustment procedure, through regression, matching or otherwise, will not lower the asymptotic precision of the
average treatment effect if the assumptions are correct. In
finite samples, however, a covariate that is not, or is only
weakly, correlated with outcomes and treatment indicators
may reduce precision. There are few procedures currently
available for optimally choosing the set of covariates to be
included in matching or regression adjustments, taking into
account such finite-sample properties.
The first issue is a substantive one. The unconfoundedness assumption may apply with one set of covariates but
not apply with an expanded set. A particular concern is the
inclusion of covariates that are themselves affected by the
treatment, such as intermediate outcomes. Suppose, for
example, that in evaluating a job training program, the
primary outcome of interest is earnings two years later. In
that case, employment status prior to the program is unaffected by the treatment and thus a valid element of the set of
adjustment covariates. In contrast, employment status one
year after the program is an intermediate outcome and
should not be controlled for. It could itself be an outcome of
interest, and should therefore never be a covariate in an
analysis of the effect of the training program. One guarantee
that a covariate is not affected by the treatment is that it was
measured before the treatment was chosen. In practice,
however, the covariates are often recorded at the same time
as the outcomes, subsequent to treatment. In that case one
has to assess on a case-by-case basis whether a particular
covariate should be used in adjusting outcomes. See Rosenbaum (1984b) and Angrist and Krueger (2000) for more
discussion.
C. Assessing the Overlap Assumption

The second of the key assumptions in estimating average
treatment effects requires that the propensity score—the
probability of receiving the active treatment—be strictly
between zero and one. In principle this is testable, as it
restricts the joint distribution of observables; but formal
tests are not necessarily the main concern. In practice, this
assumption raises a number of questions. The first is how to
detect a lack of overlap in the covariate distributions. A
second is how to deal with it, given that such a lack exists.
A third is how the individual methods discussed in section
III address this lack of overlap. Ideally such a lack would
result in large standard errors for the average treatment
effects.
The first method to detect lack of overlap is to plot
distributions of covariates by treatment groups. In the case

23

with one or two covariates one can do this directly. In
high-dimensional cases, however, this becomes more difficult. One can inspect pairs of marginal distributions by
treatment status, but these are not necessarily informative
about lack of overlap. It is possible that for each covariate
the distributions for the treatment and control groups are
identical, even though there are areas where the propensity
score is 0 or 1.
A more useful method is therefore to inspect the distribution of the propensity score in both treatment groups, which
can directly reveal lack of overlap in high-dimensional
covariate distributions. Its implementation requires nonparametric estimation of the propensity score, however, and
misspecification may lead to failure in detecting a lack of
overlap, just as inspecting various marginal distributions
may be insufficient. In practice one may wish to undersmooth the estimation of the propensity score, either by
choosing a bandwidth smaller than optimal for nonparametric estimation or by including higher-order terms in a series
expansion.
A third way to detect lack of overlap is to inspect the
quality of the worst matches in a matching procedure. Given
a set of matches, one can, for each component k of the
vector of covariates, inspect maxi 兩x i,k ⫺ x ᐉ 1(i),k 兩, the
maximum over all observations of the matching discrepancy. If this difference is large relative to the sample
standard deviation of the kth component of the covariates,
there is reason for concern. The advantage of this method is
that it does not require additional nonparametric estimation.
Once one determines that there is a lack of overlap, one
can either conclude that the average treatment effect of
interest cannot be estimated with sufficient precision, and/or
decide to focus on an average treatment effect that is
estimable with greater accuracy. To do the latter it can be
useful to discard some of the observations on the basis of
their covariates. For example, one may decide to discard
control (treated) observations with propensity scores below
(above) a cutoff level. The desired cutoff may depend on the
sample size; in a very large sample one may not be concerned with a propensity score of 0.01, whereas in small
samples such a value may make it difficult to find reasonable comparisons. To judge such tradeoffs, it is useful to
understand the relationship between a unit’s propensity
score and its implicit weight in the average-treatment-effect
estimation. Using the weighting estimator, the average outcome under the treatment is estimated by summing up
outcomes for the control units with weight approximately
equal to 1 divided by their propensity score (and 1 divided
by 1 minus the propensity score for treated units). Hence
with N units, the weight of unit i is approximately 1/{N 䡠
[1 ⫺ e(X i )]} if it is a treated unit and 1/[N 䡠 e(X i )] if it is
a control. One may wish to limit this weight to some
fraction, for example, 0.05, so that no unit will have a
weight of more than 5% in the average. Under that approach, the limit on the propensity score in a sample with

24

THE REVIEW OF ECONOMICS AND STATISTICS

200 units is 0.1; units with a propensity score less than 0.1
or greater than 0.9 should be discarded. In a sample with
1000 units, only units with a propensity score outside the
range [0.02, 0.98] will be ignored.
In matching procedures one need not rely entirely on
comparisons of the propensity score distribution in discarding the observations with insufficient match quality.
Whereas Rosenbaum and Rubin (1984) suggest accepting
only matches where the difference in propensity scores is
below a cutoff point, alternatively one may wish to drop
matches where individual covariates are severely mismatched.
Finally, let us consider the three approaches to inference—regression, matching, and propensity score methods—and assess how each handles lack of overlap. Suppose
one is interested in estimating the average effect on the
treated, and one has a data set with sufficient overlap. Now
suppose one adds a few treated or control observations with
covariate values rarely seen in the alternative treatment
group. Adding treated observations with outlying values
implies one cannot estimate the average treatment effect for
the treated very precisely, because one lacks suitable controls against which to compare these additional units. Thus
with methods appropriately dealing with limited overlap
one will see the variance estimates increase. In contrast,
adding control observations with outlying covariate values
should have little effect, since such controls are irrelevant
for the average treatment effect for the treated. Therefore,
methods appropriately dealing with limited overlap should
in this case show estimates approximately unchanged in
bias and precision.
Consider first the regression approach. Conditional on a
particular parametric specification for the regression function, adding observations with outlying values of the regressors leads to considerably more precise parameter estimates;
such observations are influential precisely because of their
outlying values. If the added observations are treated units,
the precision of the estimated control regression function at
these outlying values will be lower (since few if any control
units are found in that region); thus the variance will
increase, as it should. One should note, however, that the
estimates in this region may be sensitive to the specification
chosen. In contrast, by the nature of regression functions,
adding control observations with outlying values will lead
to a spurious increase in precision of the control regression
function. Regression methods can therefore be misleading
in cases with limited overlap.
Next, consider matching. In estimating the average treatment effect for the treated, adding control observations with
outlying covariate values will likely have little affect on the
results, since such observations are unlikely to be used as
matches. The results would, however, be sensitive to adding
treated observations with outlying covariate values, because
these observations would be matched to inappropriate con-

trols, leading to possibly biased estimates. The standard
errors would largely be unaffected.
Finally, consider propensity-score estimates. Estimates of
the probability of receiving treatment now include values
close to 0 and 1. The values close to 0 for the control
observations would cause little difficulty because these units
would get close to zero weight in the estimation. The control
observations with a propensity score close to 1, however,
would receive high weights, leading to an increase in the
variance of the average-treatment-effect estimator, correctly
implying that one cannot estimate the average treatment
effect very precisely. Blocking on the propensity score
would lead to similar conclusions.
Overall, propensity score and matching methods (and
likewise kernel-based regression methods) are better designed to cope with limited overlap in the covariate distributions than are parametric or semiparametric (series) regression models. In all cases it is useful to inspect
histograms of the estimated propensity score in both groups
to assess whether limited overlap is an issue.
VI.

Applications

There are many studies using some form of unconfoundedness or selection on observables, ranging from simple
least squares analyses to matching on the propensity score
(for example, Ashenfelter and Card, 1985; LaLonde, 1986;
Card and Sullivan, 1988; Heckman, Ichimura, and Todd,
1997; Angrist, 1998; Dehejia and Wahba, 1999; Lechner,
1998; Friedlander and Robins, 1995; and many others).
Here I focus primarily on two sets of analyses that can help
researchers assess the value of the methods surveyed in this
paper: first, studies attempting to assess the plausibility of
the assumptions, often using randomized experiments as a
yardstick; second, simulation studies focusing on the performance of the various techniques in settings where the
assumptions are known to hold.
A. Applications: Randomized Experiments as Checks on
Unconfoundedness

The basic idea behind these studies is simple: to use
experimental results as a check on the attempted nonexperimental estimates. Given a randomized experiment, one can
obtain unbiased estimates of the average effect of a program. Then, one can put aside the experimental control
group and attempt to replicate these results using a nonexperimental control. If one can successfully replicate the
experimental results, this suggests that the assumptions and
methods are plausible. Such investigations are of course not
generally conclusive, but are invaluable in assessing the
plausibility of the approach. The first such study, and one
that made an enormous impact in the econometrics literature, was by LaLonde (1986). Fraker and Maynard (1987)
conducted a similar investigation, and many more have
followed.

AVERAGE TREATMENT EFFECTS

LaLonde (1986) took the National Supported Work program, a fairly small program aimed at particularly
disadvantaged people in the labor market (individuals with
poor labor market histories and skills). Using these data, he
set aside the experimental control group and in its place
constructed alternative controls from the Panel Study of
Income Dynamics (PSID) and Current Population Survey
(CPS), using various selection criteria depending on prior
labor market experience. He then used a number of methods—ranging from a simple difference, to least squares
adjustment, a Heckman selection correction, and differenceindifferences techniques—to create nonexperimental estimates of the average treatment effect. His general conclusion was that the results were very unpredictable and that no
method could consistently replicate the experimental results
using any of the six nonexperimental control groups constructed. A number of researchers have subsequently tested
new techniques using these same data. Heckman and Hotz
(1989) focused on testing the various models and argued
that the testing procedures they developed would have
eliminated many of LaLonde’s particularly inappropriate
estimates. Dehejia and Wahba (1999) used several of the
semiparametric methods based on the unconfoundedness
assumption discussed in this survey, and found that for the
subsample of the LaLonde data that they used (with two
years of prior earnings), these methods replicated the experimental results more accurately—both overall and within
subpopulations. Smith and Todd (2003) analyze the same
data and conclude that for other subsamples, including those
for which only one year of prior earnings is available, the
results are less robust. See Dehejia (2003) for additional
discussion of these results.
Others have used different experiments to carry out the
same or similar analyses, using varying sets of estimators
and alternative control groups. Friedlander and Robins
(1995) focus on least squares adjustment, using data from
the WIN (Work INcentive) demonstration programs conducted in a number of states, and construct control groups
from other counties in the same state, as well as from
different states. They conclude that nonexperimental methods are unable to replicate the experimental results. Hotz,
Imbens, and Mortimer (2003) use the same data and consider matching methods with various sets of covariates,
using single or multiple alternative states as nonexperimental control groups. They find that for the subsample of
individuals with positive earnings at some date prior to the
program, nonexperimental methods work better than for
those with no known positive earnings.
Heckman, Ichimura, and Todd (1997, 1998) and Heckman, Ichimura, Smith, and Todd (1998) study the national
Job Training Partnership Act (JPTA) program, using data
from different geographical locations to investigate the
nature of the biases associated with different estimators, and
the importance of overlap in the covariates, including labor
market histories. Their conclusions provide the type of

25

specific guidance that should be the aim of such studies.
They give clear and generalizable conditions that make the
assumptions of unconfoundedness and overlap—at least
according to their study of a large training program—more
plausible. These conditions include the presence of detailed
earnings histories, and control groups that are geographically close to the treatment group—preferably groups of
ineligibles, or eligible nonparticipants from the same location. In contrast, control groups from very different locations are found to be poor nonexperimental controls. Although such conclusions are only clearly generalizable to
evaluations of social programs, they are potentially very
useful in providing analysts with concrete guidance as to the
applicability of these assumptions.
Dehejia (2002) uses the Greater Avenues to INdependence (GAIN) data, using different counties as well as
different offices within the same county as nonexperimental
control groups. Similarly, Hotz, Imbens, and Klerman
(2001) use the basic GAIN data set supplemented with
administrative data on long-term quarterly earnings (both
prior and subsequent to the randomization date), to investigate the importance of detailed earnings histories. Such
detailed histories can also provide more evidence on the
plausibility of nonexperimental evaluations for long-term
outcomes.
Two complications make this literature difficult to evaluate. One is the differences in covariates used; it is rare that
variables are measured consistently across different studies.
For instance, some have yearly earnings data, others quarterly, others only earnings indicators on a monthly or quarterly basis. This makes it difficult to consistently investigate
the level of detail in earnings history necessary for the
unconfoundedness assumption to hold. A second complication is that different estimators are generally used; thus any
differences in results can be attributed to either estimators or
assumptions. This is likely driven by the fact that few of the
estimators have been sufficiently standardized that they can
be implemented easily by empirical researchers.
All of these studies just discussed took data from actual
randomized experiments to test the “true” treatment effect
against the estimators used on the nonexperimental data. To
some extent, however, such experimental data are not required. The question of interest is whether an alternative
control group is an adequate proxy for a randomized control
in a particular setting; note that this question does not
require data on the treatment group. Although these questions have typically been studied by comparing experimental with nonexperimental results, all that is really relevant is
whether the nonexperimental control group can predict the
average outcomes for the experimental control. As in Heckman, Ichimura, Smith, and Todd’s (1998) analysis of the
JTPA data, one can take two groups, neither subject to the
treatment, and ask the question whether—using data on the
covariates for the first control group in combination with
outcome and covariate information for the second—one can

26

THE REVIEW OF ECONOMICS AND STATISTICS

predict the average outcome in the first. If so, this implies
that, had there been an experiment on the population from
which the first control group was drawn, the second group
would provide an acceptable nonexperimental control.
From this perspective one can use data from many different
surveys. In particular, one can more systematically investigate whether control groups from different counties, states,
or regions or even different time periods make acceptable
nonexperimental controls.
B. Simulations

A second question that is often confounded with that of
the validity of the assumptions is that of the relative performance of the various estimators. Suppose one is willing
to accept the unconfoundedness and overlap assumptions.
Which estimation method is most appropriate in a particular
setting? In many of the studies comparing nonexperimental
with experimental outcomes, researchers compare results
for a number of the techniques described here. Yet in these
settings we cannot be certain that the underlying assumptions hold. Thus, although it is useful to compare these
techniques in such realistic settings, it is also important to
compare them in an artificial environment where one is
certain that the underlying assumptions are valid.
There exist a few studies that specifically set out to do
this. Frölich (2000) compares a number of matching estimators and local linear regression methods, carefully formalizing fully data-driven procedures for the estimators
considered. To make these comparisons he considers a large
number of data-generating processes, based on eight different regression functions (including some highly nonlinear
and multimodal ones), two different sample sizes, and three
different density functions for the covariate (one important
limitation is that he restricts the investigation to a single
covariate). For the matching estimator Frölich considered a
single match with replacement; for the local linear regression estimators he uses data-driven optimal bandwidth
choices based on minimizing the mean squared error of the
average treatment effect. The first local linear estimator
considered is the standard one: at x the regression function
␮( x) is estimated as ␤0 in the minimization problem

冘 关Y ⫺ ␤ ⫺ ␤ 䡠 共X ⫺ x兲兴 䡠 K冉X h⫺ x冊,
N

min

2

␤ 0 ,␤ 1 i⫽1

i

0

1

i

i

with an Epanechnikov kernel. He finds that this has computational problems, as well as poor small-sample properties. He therefore also considers a modification suggested by
Seifert and Gasser (1996, 2000). For given x, define x៮ ⫽ ¥
X i K((X i ⫺ x)/h)/¥ K((X i ⫺ x)/h), so that one can write
the standard local linear estimator as
␮ˆ 共x兲 ⫽

T0 T1
⫹ 共x ⫺ x៮ 兲,
S0 S2

where, for r ⫽ 0, 1, 2, one has S r ⫽ ¥ K((X i ⫺
x)/h)(X i ⫺ x) r and T r ⫽ ¥ K((X i ⫺ x)/h)(X i ⫺ x) r Y i . The
Seifert-Gasser modification is to use instead
␮ˆ 共x兲 ⫽

T1
T0
共x ⫺ x៮ 兲,
⫹
S0 S2 ⫹ R

where the recommended ridge parameter is R ⫽ 兩x ⫺
x៮ 兩[5/(16h)], given the Epanechnikov kernel k(u) ⫽ 43 (1 ⫺
u 2 )1{兩u兩 ⬍ 1}. Note that with high-dimensional covariates,
such a nonnegative kernel would lead to biases that do not
vanish fast enough to be dominated by the variance (see the
discussion in Heckman, Ichimura, and Todd, 1998). This is
not a problem in Frölich’s simulations, as he considers only
cases with a single covariate. Frölich finds that the local
linear estimator, with Seifert and Gassert’s modification,
performs better than either the matching or the standard
local linear estimator.
Zhao (2004) uses simulation methods to compare matching and parametric regression estimators. He uses metrics
based on the propensity score, the covariates, and estimated
regression functions. Using designs with varying numbers
of covariates and linear regression functions, Zhao finds
there is no clear winner among the different estimators,
although he notes that using the outcome data in choosing
the metric appears a promising strategy.
Abadie and Imbens (2002) study their matching estimator
using a data-generating process inspired by the LaLonde
study to allow for substantial nonlinearity, fitting a separate
binary response model to the zeros in the earnings outcome,
and a log linear model for the positive observations. The
regression estimators include linear and quadratic models
(the latter with a full set of interactions), with seven covariates. This study finds that the matching estimators, and in
particular the bias-adjusted alternatives, outperform the linear and quadratic regression estimators (the former using 7
covariates, the latter 35, after dropping squares and interactions that lead to perfect collinearity). Their simulations also
suggest that with few matches—between one and four—
matching estimators are not sensitive to the number of
matches used, and that their confidence intervals have actual
coverage rates close to the nominal values.
The results from these simulation studies are overall
somewhat inconclusive; it is clear that more work is required. Future simulations may usefully focus on some of
the following issues. First, it is obviously important to
closely model the data-generating process on actual data
sets, to ensure that the results have some relevance for
practice. Ideally one would build the simulations around a
number of specific data sets through a range of datagenerating processes. Second, it is important to have fully
data-driven procedures that define an estimator as a function
N
, as seen in Frölich (2000). For the
of (Y i , W i , X i ) i⫽1
matching estimators this is relatively straightforward, but
for some others this requires more care. This will allow

AVERAGE TREATMENT EFFECTS

other researchers to consider meaningful comparisons
across the various estimators.
Finally, we need to learn which features of the datagenerating process are important for the properties of the
various estimators. For example, do some estimators
deteriorate more rapidly than others when a data set has
many covariates and few observations? Are some estimators
more robust against high correlations between covariates
and outcomes, or high correlations between covariates and
treatment indicators? Which estimators are more likely to
give conservative answers in terms of precision? Since it is
clear that no estimator is always going to dominate all
others, what is important is to isolate salient features of the
data-generating processes that lead to preferring one alternative over another. Ideally we need descriptive statistics
summarizing the features of the data that provide guidance
in choosing the estimator that will perform best in a given
situation.
VII.

Conclusion

In this paper I have attempted to review the current state
of the literature on inference for average treatment effects
under the assumption of unconfoundedness. This has recently been a very active area of research where many new
semi- and nonparametric econometric methods have been
applied and developed. The research has moved a long way
from relying on simple least squares methods for estimating
average treatment effects.
The primary estimators in the current literature include
propensity-score methods and pairwise matching, as well as
nonparametric regression methods. Efficiency bounds have
been established for a number of the average treatment
effects estimable with these methods, and a variety of these
estimators rely on the weakest assumptions that allow point
identification. Researchers have suggested several ways for
estimating the variance of these average-treatment-effect
estimators. One, more cumbersome approach requires estimating each component of the variance nonparametrically.
A more common method relies on bootstrapping. A third
alternative, developed by Abadie and Imbens (2002) for the
matching estimator, requires no additional nonparametric
estimation. There is, as yet, however, no consensus on
which are the best estimation methods to apply in practice.
Nevertheless, the applied researcher has now a large number
of new estimators at her disposal.
Challenges remain in making the new tools more easily
applicable. Although software is available to implement
some of the estimators (see Becker and Ichino, 2002;
Sianesi, 2001; Abadie et al., 2003), many remain difficult to
apply. A particularly urgent task is therefore to provide fully
implementable versions of the various estimators that do not
require the applied researcher to choose bandwidths or other
smoothing parameters. This is less of a concern for matching methods and probably explains a large part of their
popularity. Another outstanding question is the relative

27

performance of these methods in realistic settings with large
numbers of covariates and varying degrees of smoothness in
the conditional means of the potential outcomes and the
propensity score.
Once these issues have been resolved, today’s applied
evaluators will benefit from a new set of reliable, econometrically defensible, and robust methods for estimating the
average treatment effect of current social policy programs
under exogeneity assumptions.
REFERENCES
Abadie, A., “Semiparametric Instrumental Variable Estimation of Treatment Response Models,” Journal of Econometrics 113:2 (2003a),
231–263.
Abadie, A., “Semiparametric Difference-in-Differences Estimators,”
forthcoming, Review of Economic Studies (2003b).
Abadie, A., J. Angrist, and G. Imbens, “Instrumental Variables Estimation
of Quantile Treatment Effects,” Econometrica 70:1 (2002), 91–
117.
Abadie, A., D. Drukker, H. Herr, and G. Imbens, “Implementing Matching
Estimators for Average Treatment Effects in STATA,” Department
of Economics, University of California, Berkeley, unpublished
manuscript (2003).
Abadie, A., and G. Imbens, “Simple and Bias-Corrected Matching Estimators for Average Treatment Effects,” NBER technical working
paper no. 283 (2002).
Abbring, J., and G. van den Berg, “The Non-parametric Identification of
Treatment Effects in Duration Models,” Free University of Amsterdam, unpublished manuscript (2002).
Angrist, J., “Estimating the Labor Market Impact of Voluntary Military
Service Using Social Security Data on Military Applicants,”
Econometrica 66:2 (1998), 249–288.
Angrist, J. D., and J. Hahn, “When to Control for Covariates? PanelAsymptotic Results for Estimates of Treatment Effects,” NBER
technical working paper no. 241 (1999).
Angrist, J. D., G. W. Imbens, and D. B. Rubin, “Identification of Causal
Effects Using Instrumental Variables,” Journal of the American
Statistical Association 91 (1996), 444–472.
Angrist, J. D., and A. B. Krueger, “Empirical Strategies in Labor Economics,” in A. Ashenfelter and D. Card (Eds.), Handbook of Labor
Economics vol. 3 (New York: Elsevier Science, 2000).
Angrist, J., and V. Lavy, “Using Maimonides’ Rule to Estimate the Effect
of Class Size on Scholastic Achievement,” Quarterly Journal of
Economics CXIV (1999), 1243.
Ashenfelter, O., “Estimating the Effect of Training Programs on Earnings,” this REVIEW 60 (1978), 47–57.
Ashenfelter, O., and D. Card, “Using the Longitudinal Structure of
Earnings to Estimate the Effect of Training Programs,” this REVIEW
67 (1985), 648–660.
Athey, S., and G. Imbens, “Identification and Inference in Nonlinear
Difference-in-Differences Models,” NBER technical working paper no. 280 (2002).
Athey, S., and S. Stern, “An Empirical Framework for Testing Theories
about Complementarity in Organizational Design,” NBER working
paper no. 6600 (1998).
Barnow, B. S., G. G. Cain, and A. S. Goldberger, “Issues in the Analysis
of Selectivity Bias,” in E. Stromsdorfer and G. Farkas (Eds.),
Evaluation Studies vol. 5 (San Francisco: Sage, 1980).
Becker, S., and A. Ichino, “Estimation of Average Treatment Effects Based
on Propensity Scores,” The Stata Journal 2:4 (2002), 358–377.
Bitler, M., J. Gelbach, and H. Hoynes, “What Mean Impacts Miss:
Distributional Effects of Welfare Reform Experiments,” Department of Economics, University of Maryland, unpublished paper
(2002).
Björklund, A., and R. Moffit, “The Estimation of Wage Gains and Welfare
Gains in Self-Selection Models,” this REVIEW 69 (1987), 42–49.
Black, S., “Do Better Schools Matter? Parental Valuation of Elementary
Education,” Quarterly Journal of Economics CXIV (1999), 577.

28

THE REVIEW OF ECONOMICS AND STATISTICS

Blundell, R., and Monica Costa-Dias, “Alternative Approaches to Evaluation in Empirical Microeconomics,” Institute for Fiscal Studies,
Cemmap working paper cwp10/02 (2002).
Blundell, R., A. Gosling, H. Ichimura, and C. Meghir, “Changes in the
Distribution of Male and Female Wages Accounting for the Employment Composition,” Institute for Fiscal Studies, London, unpublished paper (2002).
Card, D., and D. Sullivan, “Measuring the Effect of Subsidized Training
Programs on Movements In and Out of Employment,” Econometrica 56:3 (1988), 497–530.
Chernozhukov, V., and C. Hansen, “An IV Model of Quantile Treatment
Effects,” Department of Economics, MIT, unpublished working
paper (2001).
Cochran, W., “The Effectiveness of Adjustment by Subclassification in
Removing Bias in Observational Studies,” Biometrics 24, (1968),
295–314.
Cochran, W., and D. Rubin, “Controlling Bias in Observational Studies: A
Review,” Sankhya៮ 35 (1973), 417–446.
Dehejia, R., “Was There a Riverside Miracle? A Hierarchical Framework
for Evaluating Programs with Grouped Data,” Journal of Business
and Economic Statistics 21:1 (2002), 1–11.
“Practical Propensity Score Matching: A Reply to Smith and
Todd,” forthcoming, Journal of Econometrics (2003).
Dehejia, R., and S. Wahba, “Causal Effects in Nonexperimental Studies:
Reevaluating the Evaluation of Training Programs,” Journal of the
American Statistical Association 94 (1999), 1053–1062.
Doksum, K., “Empirical Probability Plots and Statistical Inference for
Nonlinear Models in the Two-Sample Case,” Annals of Statistics 2
(1974), 267–277.
Efron, B., and R. Tibshirani, An Introduction to the Bootstrap (New York:
Chapman and Hall, 1993).
Engle, R., D. Hendry, and J.-F. Richard, “Exogeneity,” Econometrica 51:2
(1974), 277–304.
Firpo, S., “Efficient Semiparametric Estimation of Quantile Treatment
Effects,” Department of Economics, University of California,
Berkeley, PhD thesis (2002), chapter 2.
Fisher, R. A., The Design of Experiments (Boyd, London, 1935).
Fitzgerald, J., P. Gottschalk, and R. Moffitt, “An Analysis of Sample
Attrition in Panel Data: The Michigan Panel Study of Income
Dynamics,” Journal of Human Resources 33 (1998), 251–299.
Fraker, T., and R. Maynard, “The Adequacy of Comparison Group
Designs for Evaluations of Employment-Related Programs,” Journal of Human Resources 22:2 (1987), 194–227.
Friedlander, D., and P. Robins, “Evaluating Program Evaluations: New
Evidence on Commonly Used Nonexperimental Methods,” American Economic Review 85 (1995), 923–937.
Frölich, M., “Treatment Evaluation: Matching versus Local Polynomial
Regression,” Department of Economics, University of St. Gallen,
discussion paper no. 2000-17 (2000).
“What is the Value of Knowing the Propensity Score for Estimating Average Treatment Effects,” Department of Economics, University of St. Gallen (2002).
Gill, R., and J. Robins, “Causal Inference for Complex Longitudinal Data:
The Continuous Case,” Annals of Statistics 29:6 (2001), 1785–
1811.
Gu, X., and P. Rosenbaum, “Comparison of Multivariate Matching Methods: Structures, Distances and Algorithms,” Journal of Computational and Graphical Statistics 2 (1993), 405–420.
Hahn, J., “On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects,” Econometrica 66:2
(1998), 315–331.
Hahn, J., P. Todd, and W. Van der Klaauw, “Identification and Estimation
of Treatment Effects with a Regression-Discontinuity Design,”
Econometrica 69:1 (2000), 201–209.
Ham, J., and R. LaLonde, “The Effect of Sample Selection and Initial
Conditions in Duration Models: Evidence from Experimental Data
on Training,” Econometrica 64:1 (1996).
Heckman, J., and J. Hotz, “Alternative Methods for Evaluating the Impact
of Training Programs” (with discussion), Journal of the American
Statistical Association 84:804 (1989), 862–874.
Heckman, J., H. Ichimura, and P. Todd, “Matching as an Econometric
Evaluation Estimator: Evidence from Evaluating a Job Training
Program,” Review of Economic Studies 64 (1997), 605–654.

“Matching as an Econometric Evaluation Estimator,” Review of
Economic Studies 65 (1998), 261–294.
Heckman, J., H. Ichimura, J. Smith, and P. Todd, “Characterizing Selection Bias Using Experimental Data,” Econometrica 66 (1998),
1017–1098.
Heckman, J., R. LaLonde, and J. Smith, “The Economics and Econometrics of Active Labor Markets Programs,” in A. Ashenfelter and D.
Card (Eds.), Handbook of Labor Economics vol. 3 (New York:
Elsevier Science, 2000).
Heckman, J., and R. Robb, “Alternative Methods for Evaluating the
Impact of Interventions,” in J. Heckman and B. Singer (Eds.),
Longitudinal Analysis of Labor Market Data (Cambridge, U.K.:
Cambridge University Press, 1984).
Heckman, J., J. Smith, and N. Clements, “Making the Most out of
Programme Evaluations and Social Experiments: Accounting for
Heterogeneity in Programme Impacts,” Review of Economic Studies 64 (1997), 487–535.
Hirano, K., and G. Imbens, “Estimation of Causal Effects Using Propensity Score Weighting: An Application of Data on Right Ear Catheterization,” Health Services and Outcomes Research Methodology
2 (2001), 259–278.
Hirano, K., G. Imbens, and G. Ridder, “Efficient Estimation of Average
Treatment Effects Using the Estimated Propensity Score,” Econometrica 71:4 (2003), 1161–1189.
Holland, P., “Statistics and Causal Inference” (with discussion), Journal of
the American Statistical Association 81 (1986), 945–970.
Horowitz, J., “The Bootstrap,” in James J. Heckman and E. Leamer (Eds.),
Handbook of Econometrics, vol. 5 (Elsevier North Holland, 2002).
Hotz, J., G. Imbens, and J. Klerman, “The Long-Term Gains from GAIN:
A Re-analysis of the Impacts of the California GAIN Program,”
Department of Economics, UCLA, unpublished manuscript (2001).
Hotz, J., G. Imbens, and J. Mortimer, “Predicting the Efficacy of Future
Training Programs Using Past Experiences,” forthcoming, Journal
of Econometrics (2003).
Ichimura, H., and O. Linton, “Asymptotic Expansions for Some Semiparametric Program Evaluation Estimators,” Institute for Fiscal Studies, cemmap working paper cwp04/01 (2001).
Ichimura, H., and C. Taber, “Direct Estimation of Policy Effects,” Department of Economics, Northwestern University, unpublished
manuscript (2000).
Imbens, G., “The Role of the Propensity Score in Estimating DoseResponse Functions,” Biometrika 87:3 (2000), 706–710.
“Sensitivity to Exogeneity Assumptions in Program Evaluation,”
American Economic Review Papers and Proceedings (2003).
Imbens, G., and J. Angrist, “Identification and Estimation of Local
Average Treatment Effects,” Econometrica 61:2 (1994), 467–476.
Imbens, G., W. Newey, and G. Ridder, “Mean-Squared-Error Calculations
for Average Treatment Effects,” Department of Economics, UC
Berkeley, unpublished manuscript (2003).
LaLonde, R. J., “Evaluating the Econometric Evaluations of Training
Programs with Experimental Data,” American Economic Review
76 (1986), 604–620.
Lechner, M., “Earnings and Employment Effects of Continuous Off-theJob Training in East Germany after Unification,” Journal of Business and Economic Statistics 17:1 (1999), 74–90.
Lechner, M., “Identification and Estimation of Causal Effects of Multiple
Treatments under the Conditional Independence Assumption,” in
M. Lechner and F. Pfeiffer (Eds.), Econometric Evaluations of
Active Labor Market Policies in Europe (Heidelberg: Physica,
2001).
“Program Heterogeneity and Propensity Score Matching: An
Application to the Evaluation of Active Labor Market Policies,”
this REVIEW 84:2 (2002), 205–220.
Lee, D., “The Electoral Advantage of Incumbency and the Voter’s Valuation of Political Experience: A Regression Discontinuity Analysis
of Close Elections,” Department of Economics, University of
California, unpublished manuscript (2001).
Lehman, E., Nonparametrics: Statistical Methods Based on Ranks (San
Francisco: Holden-Day, 1974).
Manski, C., “Nonparametric Bounds on Treatment Effects,” American
Economic Review Papers and Proceedings 80 (1990), 319–323.
Manski, C., G. Sandefur, S. McLanahan, and D. Powers, “Alternative
Estimates of the Effect of Family Structure During Adolescence on

AVERAGE TREATMENT EFFECTS
High School,” Journal of the American Statistical Association
87:417 (1992), 25–37.
Partial Identification of Probability Distributions (New York:
Springer-Verlag, 2003).
Neyman, J., “On the Application of Probability Theory to Agricultural
Experiments. Essay on Principles. Section 9” (1923), translated
(with discussion) in Statistical Science 5:4 (1990), 465–480.
Politis, D., and J. Romano, Subsampling (Springer-Verlag, 1999).
Porter, J., “Estimation in the Regression Discontinuity Model,” Harvard
University, unpublished manuscript (2003).
Quade, D., “Nonparametric Analysis of Covariance by Matching,” Biometrics 38 (1982), 597–611.
Robins, J., and Y. Ritov, “Towards a Curse of Dimensionality Appropriate
(CODA) Asymptotic Theory for Semi-parametric Models,” Statistics in Medicine 16 (1997), 285–319.
Robins, J. M., and A. Rotnitzky, “Semiparametric Efficiency in Multivariate Regression Models with Missing Data,” Journal of the American Statistical Association 90 (1995), 122–129.
Robins, J. M., Rotnitzky, A., Zhao, L.-P., “Analysis of Semiparametric
Regression Models for Repeated Outcomes in the Presence of
Missing Data,” Journal of the American Statistical Association 90
(1995), 106–121.
Rosenbaum, P., “Conditional Permutation Tests and the Propensity Score
in Observational Studies,” Journal of the American Statistical
Association 79 (1984a), 565–574.
“The Consequences of Adjustment for a Concomitant Variable
That Has Been Affected by the Treatment,” Journal of the Royal
Statistical Society, Series A 147 (1984b), 656–666.
“The Role of a Second Control Group in an Observational Study”
(with discussion), Statistical Science 2:3 (1987), 292–316.
“Optimal Matching in Observational Studies,” Journal of the
American Statistical Association 84 (1989), 1024–1032.
Observational Studies (New York: Springer-Verlag, 1995).
“Covariance Adjustment in Randomized Experiments and Observational Studies,” Statistical Science 17:3 (2002), 286–304.
Rosenbaum, P., and D. Rubin, “The Central Role of the Propensity Score
in Observational Studies for Causal Effects,” Biometrika 70
(1983a), 41–55.
“Assessing the Sensitivity to an Unobserved Binary Covariate in
an Observational Study with Binary Outcome,” Journal of the
Royal Statistical Society, Series B 45 (1983b), 212–218.
“Reducing the Bias in Observational Studies Using Subclassification on the Propensity Score,” Journal of the American Statistical Association 79 (1984), 516–524.

29

“Constructing a Control Group Using Multivariate Matched
Sampling Methods That Incorporate the Propensity Score,” American Statistician 39 (1985), 33–38.
Rubin, D., “Matching to Remove Bias in Observational Studies,” Biometrics 29 (1973a), 159–183.
“The Use of Matched Sampling and Regression Adjustments to
Remove Bias in Observational Studies,” Biometrics 29 (1973b),
185–203.
“Estimating Causal Effects of Treatments in Randomized and
Non-randomized Studies,” Journal of Educational Psychology 66
(1974), 688–701.
“Assignment to Treatment Group on the Basis of a Covariate,”
Journal of Educational Statistics 2:1 (1977), 1–26.
“Bayesian Inference for Causal Effects: The Role of Randomization,” Annals of Statistics 6 (1978), 34–58.
“Using Multivariate Matched Sampling and Regression Adjustment to Control Bias in Observational Studies,” Journal of the
American Statistical Association 74 (1979), 318–328.
Rubin, D., and N. Thomas, “Affinely Invariant Matching Methods with
Ellipsoidal Distributions,” Annals of Statistics 20:2 (1992), 1079–
1093.
Seifert, B., and T. Gasser, “Finite-Sample Variance of Local Polynomials:
Analysis and Solutions,” Journal of the American Statistical Association 91 (1996), 267–275.
“Data Adaptive Ridging in Local Polynomial Regression,” Journal of Computational and Graphical Statistics 9:2 (2000), 338–
360.
Shadish, W., T. Campbell, and D. Cook, Experimental and Quasiexperimental Designs for Generalized Causal Inference (Boston:
Houghton Mifflin, 2002).
Sianesi, B., “psmatch: Propensity Score Matching in STATA,” University
College London and Institute for Fiscal Studies (2001).
Smith, J. A., and P. E. Todd, “Reconciling Conflicting Evidence on the
Performance of Propensity-Score Matching Methods,” American
Economic Review Papers and Proceedings 91 (2001), 112–118.
“Does Matching Address LaLonde’s Critique of Nonexperimental
Estimators,” forthcoming, Journal of Econometrics (2003).
Van der Klaauw, W., “A Regression-Discontinuity Evaluation of the Effect
of Financial Aid Offers on College Enrollment,” International
Economic Review 43:4 (2002), 1249–1287.
Zhao, Z., “Using Matching to Estimate Treatment Effects: Data Requirements, Matching Metrics, and Monte Carlo Evidence,” this REVIEW
this issue (2004).

This article has been cited by:
1. Halbert White, Karim Chalak. 2013. Identification and Identification Failure for Treatment Effects Using Structural Systems.
Econometric Reviews 32:3, 273-317. [CrossRef]
2. JOHN AMMER, SARA B. HOLLAND, DAVID C. SMITH, FRANCIS E. WARNOCK. 2012. U.S. International Equity
Investment. Journal of Accounting Research 50:5, 1109-1139. [CrossRef]
3. Jose Galdo, Alberto Chong. 2012. Does the quality of public-sponsored training programs matter? Evidence from bidding
processes data. Labour Economics 19:6, 970-986. [CrossRef]
4. Robert Girtz. 2012. The Effects of Personality Traits on Wages: A Matching Approach. LABOUR 26:4, 455-471. [CrossRef]
5. VLADIMIR ATANASOV, VLADIMIR IVANOV, KATE LITVAK. 2012. Does Reputation Limit Opportunistic Behavior
in the VC Industry? Evidence from Litigation against VCs. The Journal of Finance 67:6, 2215-2246. [CrossRef]
6. Ulf Rinne, Arne Uhlendorff, Zhong Zhao. 2012. Vouchers and caseworkers in training programs for the unemployed.
Empirical Economics . [CrossRef]
7. Steven Elías Alvarado, Ruth N. López Turley. 2012. College-bound friends and college application choices: Heterogeneous
effects for Latino and White students. Social Science Research 41:6, 1451-1468. [CrossRef]
8. Fitsum Hagos, Gayathri Jayasinghe, Seleshi Bekele Awulachew, Mekonnen Loulseged, Aster Denekew Yilma. 2012.
Agricultural water management and poverty in Ethiopia. Agricultural Economics 43, 99-111. [CrossRef]
9. Gregory N. Price. 2012. Hurricane Katrina as an Experiment in Housing Mobility and Neighborhood Effects: Were the
Relocated Poor Black Evacuees Better-Off?. The Review of Black Political Economy . [CrossRef]
10. Galina Besstremyannaya. 2012. The impact of Japanese hospital financing reform on hospital efficiency: A difference-indifference approach. Japanese Economic Review n/a-n/a. [CrossRef]
11. Sylvain Chabé-Ferret, Julie Subervie. 2012. How much green for the buck? estimating additional and windfall effects of
french agro-environmental schemes by did-matching. Journal of Environmental Economics and Management . [CrossRef]
12. Colette Grey, Konstantinos Stathopoulos, Martin Walker. 2012. The impact of executive pay on the disclosure of alternative
earnings per share figures. International Review of Financial Analysis . [CrossRef]
13. Kent Eliasson, Pär Hansson, Markus Lindvert. 2012. Do firms learn by exporting or learn to export? Evidence from small
and medium-sized enterprises. Small Business Economics 39:2, 453-472. [CrossRef]
14. Sarah B. Hunter, Rajeev Ramchand, Beth Ann Griffin, Marika J. Suttorp, Daniel McCaffrey, Andrew Morral. 2012. The
effectiveness of community-based delivery of an evidence-based treatment for adolescent substance use. Journal of Substance
Abuse Treatment 43:2, 211-220. [CrossRef]
15. Manuel Gomes, Richard Grieve, Richard Nixon, Edmond S.-W. Ng, James Carpenter, Simon G. Thompson. 2012.
METHODS FOR COVARIATE ADJUSTMENT IN COST-EFFECTIVENESS ANALYSIS THAT USE CLUSTER
RANDOMISED TRIALS. Health Economics 21:9, 1101-1118. [CrossRef]
16. Fédes van Rijn, Kees Burger, Eefje den Belder. 2012. Impact assessment in the Sustainable Livelihood Framework.
Development in Practice 22:7, 1019-1035. [CrossRef]
17. Annelies Deuss. 2012. The Economic Growth Impacts of Sugarcane Expansion in Brazil: An Inter-regional Analysis. Journal
of Agricultural Economics 63:3, 528-551. [CrossRef]
18. D. P. Green, H. L. Kern. 2012. Modeling Heterogeneous Treatment Effects in Survey Experiments with Bayesian Additive
Regression Trees. Public Opinion Quarterly 76:3, 491-511. [CrossRef]
19. Chi Wang, Giovanni Parmigiani, Francesca Dominici. 2012. Rejoinder: Bayesian Effect Estimation Accounting for
Adjustment Uncertainty. Biometrics 68:3, 680-686. [CrossRef]
20. Steven E. Sexton. 2012. Paying for Pollution? How General Equilibrium Effects Undermine the “Spare the Air” Program.
Environmental and Resource Economics . [CrossRef]
21. Miwako Okamoto, Hideaki Ishigami, Kumiko Tokimoto, Megumi Matsuoka, Ryoko Tango. 2012. Early Parenting Program
as Intervention Strategy for Emotional Distress in First-Time Mothers: A Propensity Score Analysis. Maternal and Child
Health Journal . [CrossRef]
22. Perry Singleton. 2012. Earnings of rejected applicants to the Social Security Disability Insurance program. Economics Letters
116:2, 147-150. [CrossRef]
23. Emilia Del Bono, Andrea Weber, Rudolf Winter-Ebmer. 2012. CLASH OF CAREER AND FAMILY: FERTILITY
DECISIONS AFTER JOB DISPLACEMENT. Journal of the European Economic Association 10:4, 659-683. [CrossRef]

24. Florian Lehmer. 2012. Dient die Arbeitnehmerüberlassung für Langzeitarbeitslose als Brücke in nachhaltige Beschäftigung?.
Sozialer Fortschritt 61:8, 190-197. [CrossRef]
25. Gabriel Pons Rotger, Mette Gørtz, David J. Storey. 2012. Assessing the effectiveness of guided preparation for new venture
creation and performance: Theory and practice. Journal of Business Venturing 27:4, 506-521. [CrossRef]
26. Jung Hur, Cheolbeom Park. 2012. Do Free Trade Agreements Increase Economic Growth of the Member Countries?. World
Development 40:7, 1283-1294. [CrossRef]
27. S. Chernenko, A. Sunderam. 2012. The Real Consequences of Market Segmentation. Review of Financial Studies 25:7,
2041-2069. [CrossRef]
28. B. S. Graham, C. C. De Xavier Pinto, D. Egel. 2012. Inverse Probability Tilting for Moment Condition Models with Missing
Data. The Review of Economic Studies 79:3, 1053-1079. [CrossRef]
29. Song Xi Chen, Jing Qin, Cheng Yong Tang. 2012. Mann-Whitney test with adjustments to pretreatment variables for missing
values and observational study. Journal of the Royal Statistical Society: Series B (Statistical Methodology) no-no. [CrossRef]
30. Hiroki Uematsu, Ashok K. Mishra. 2012. Organic farmers or conventional farmers: Where's the money?. Ecological
Economics 78, 55-62. [CrossRef]
31. Lamin Dibba, Simon C. Fialor, Aliou Diagne, Fred Nimoh. 2012. The impact of NERICA adoption on productivity and
poverty of the small-scale rice farmers in the Gambia. Food Security 4:2, 253-265. [CrossRef]
32. A.A. Akinola, N.A. Sofoluwe. 2012. Impact of mulching technology adoption on output and net return to yam farmers in
Osun State, Nigeria. Agrekon 51:2, 75-92. [CrossRef]
33. Alberto Abadie, Guido W. Imbens. 2012. A Martingale Representation for Matching Estimators. Journal of the American
Statistical Association 107:498, 833-843. [CrossRef]
34. E. J. O. Rao, B. Brummer, M. Qaim. 2012. Farmer Participation in Supermarket Channels, Production Technology, and
Efficiency: The Case of Vegetables in Kenya. American Journal of Agricultural Economics . [CrossRef]
35. Jeremy A. Rassen, Abhi A. Shelat, Jessica Myers, Robert J. Glynn, Kenneth J. Rothman, Sebastian Schneeweiss. 2012. Oneto-many propensity score matching in cohort studies. Pharmacoepidemiology and Drug Safety 21, 69-80. [CrossRef]
36. Daniel L. Millimet, Rusty Tchernis. 2012. Estimation of Treatment Effects without an Exclusion Restriction: with an
Application to the Analysis of the School Breakfast Program. Journal of Applied Econometrics n/a-n/a. [CrossRef]
37. Ranjula Bali Swain, Maria Floro. 2012. Assessing the Effect of Microfinance on Vulnerability and Poverty among Low
Income Households. Journal of Development Studies 48:5, 605-618. [CrossRef]
38. E. A. Ramalho, R. J. Smith. 2012. Discrete Choice Non-Response. The Review of Economic Studies . [CrossRef]
39. Steven F. Lehrer, Gregory Kordas. 2012. Matching using semiparametric propensity scores. Empirical Economics . [CrossRef]
40. Barry T. Hirsch, Edward J. Schumacher. 2012. Underpaid or Overpaid? Wage Analysis for Nurses Using Job and Worker
Attributes. Southern Economic Journal 78:4, 1096-1119. [CrossRef]
41. Jochen Kluve, Hilmar Schneider, Arne Uhlendorff, Zhong Zhao. 2012. Evaluating continuous training programmes by using
the generalized propensity score. Journal of the Royal Statistical Society: Series A (Statistics in Society) 175:2, 587-617.
[CrossRef]
42. Varouj A. Aivazian, Simiao Zhou. 2012. Is Chapter 11 Efficient?. Financial Management 41:1, 229-253. [CrossRef]
43. Andreas Peichl, Nico Pestel, Sebastian Siegloch. 2012. The politicians’ wage gap: insights from German members of
parliament. Public Choice . [CrossRef]
44. Yosuke Fujii, Masayuki Henmi, Toshiharu Fujita. 2012. Evaluating the interaction between the therapy and the treatment in
clinical trials by the propensity score weighting method. Statistics in Medicine 31:3, 235-252. [CrossRef]
45. Peter C. Austin. 2012. Using Ensemble-Based Methods for Directly Estimating Causal Effects: An Investigation of TreeBased G-Computation. Multivariate Behavioral Research 47:1, 115-135. [CrossRef]
46. Paul Hutchinson, Thomas W. Carton, Marsha Broussard, Lisanne Brown, Sarah Chrestman. 2012. Improving adolescent
health through school-based health centers in post-Katrina New Orleans. Children and Youth Services Review 34:2, 360-368.
[CrossRef]
47. E. J. Williamson, R. Morley, A. Lucas, J. R. Carpenter. 2012. Variance estimation for stratified propensity score estimators.
Statistics in Medicine n/a-n/a. [CrossRef]
48. Martin Binder, Alex Coad. 2012. Life satisfaction and self-employment: a matching approach. Small Business Economics
. [CrossRef]

49. P. M. Dontsop Nguezet, V. O. Okoruwa, A. I. Adeoti, K. O. Adenegan. 2012. Productivity Impact Differential of Improved
Rice Technology Adoption Among Rice Farming Households in Nigeria. Journal of Crop Improvement 26:1, 1-21. [CrossRef]
50. H. Spencer Banzhaf, Garima Bhalla. 2012. Do Households Prefer Small School Districts? A Natural Experiment. Southern
Economic Journal 78:3, 819-841. [CrossRef]
51. Lamin Dibba, Simon C. Fialor, Aliou Diagne, Fred Nimoh. 2012. The impact of NERICA adoption on productivity and
poverty of the small-scale rice farmers in the Gambia. Food Security . [CrossRef]
52. K. John McConnell, Samuel H. N. Gast, Bentson H. McFarland. 2012. The Effect of Comprehensive Behavioral Health Parity
on Choice of Provider. Medical Care 1. [CrossRef]
53. Adam N. Glynn. 2012. The Product and Difference Fallacies for Indirect Effects. American Journal of Political Science 56:1,
257-269. [CrossRef]
54. Fabrizia Mealli, Barbara Pacini, Donald B. RubinStatistical Inference for Causal Effects 171-192. [CrossRef]
55. Tu N. Dang. 2011. Evaluating the effectiveness of exchange rate bands in reducing inflation. Macroeconomics and Finance
in Emerging Market Economies 1-20. [CrossRef]
56. Xiao Mei Li, Linda Yueh. 2011. Does Incorporation Improve Firm Performance?. Oxford Bulletin of Economics and Statistics
73:6, 753-770. [CrossRef]
57. Yuyu Chen, Ginger Zhe Jin. 2011. Does Health Insurance Coverage Lead to Better Health and Educational Outcomes?
Evidence from Rural China. Journal of Health Economics . [CrossRef]
58. Di Mo, Hongmei Yi, Linxiu Zhang, Yaojiang Shi, Scott Rozelle, Alexis Medina. 2011. Transfer paths and academic
performance: The primary school merger program in china. International Journal of Educational Development . [CrossRef]
59. Kim Manturuk, Sarah Riley, Janneke Ratcliffe. 2011. Perception vs. reality: The relationship between low-income
homeownership, perceived financial stress, and financial hardship. Social Science Research . [CrossRef]
60. N. Hemken, C. Schusterschitz, M. Thöni. 2011. Optional deductibles in GKV (statutory German health insurance): do they
also exert an effect in the medium term?. Journal of Public Health . [CrossRef]
61. Use of Response Propensities 385-418. [CrossRef]
62. J. Hainmueller. 2011. Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced
Samples in Observational Studies. Political Analysis . [CrossRef]
63. X. De Luna, I. Waernbaum, T. S. Richardson. 2011. Covariate selection for the nonparametric estimation of an average
treatment effect. Biometrika . [CrossRef]
64. Marco Di Cintio, Emanuele Grassi. 2011. Internal migration and wages of Italian university graduates*. Papers in Regional
Science no-no. [CrossRef]
65. Robert I. Griffiths, Richard L. Barron, Michelle L. Gleeson, Mark D. Danese, Anthony O#Hagan, Victoria M. Chia, Jason
C. Legg, Gary H. Lyman. 2011. Granulocyte-Colony Stimulating Factor Use and Medical Costs after Initial Adjuvant
Chemotherapy in Older Patients with Early-Stage Breast Cancer. PharmacoEconomics 1. [CrossRef]
66. Martin Huber. 2011. Testing for covariate balance using quantile regression and resampling methods. Journal of Applied
Statistics 1-19. [CrossRef]
67. Ruth Vargas Hill, Angelino Viceisza. 2011. A field experiment on the impact of weather shocks and insurance on risky
investment. Experimental Economics . [CrossRef]
68. Massimo G. Colombo, Evila Piva. 2011. Firms’ genetic characteristics and competence-enlarging strategies: A comparison
between academic and non-academic high-tech start-ups. Research Policy . [CrossRef]
69. Romina Cavatassi, Mario González-flores, Paul Winters, Jorge Andrade-Piedra, Patricio Espinosa, Graham Thiele. 2011.
Linking Smallholders to the New Agricultural Economy: The Case of the Plataformas de Concertación in Ecuador. Journal
of Development Studies 1-29. [CrossRef]
70. Florencia Torche. 2011. The Effect of Maternal Stress on Birth Outcomes: Exploiting a Natural Experiment. Demography
. [CrossRef]
71. S. M. Iacus, G. King, G. Porro. 2011. Causal Inference without Balance Checking: Coarsened Exact Matching. Political
Analysis . [CrossRef]
72. Ildefonso Méndez, Jose M. Abellán Perpiñán, Fernando I. Sánchez Martínez, Jorge E. Martínez Pérez. 2011. Inverse
probability weighted estimation of social tariffs: An illustration using the SF-6D value sets. Journal of Health Economics
. [CrossRef]

73. Deven Carlson, Robert Haveman, Tom Kaplan, Barbara Wolfe. 2011. Long-term earnings and employment effects of housing
voucher receipt. Journal of Urban Economics . [CrossRef]
74. Anirban Basu, Daniel Polsky, Willard G. Manning. 2011. Estimating treatment effects on healthcare costs under exogeneity:
is there a ‘magic bullet’?. Health Services and Outcomes Research Methodology . [CrossRef]
75. Taro Esaka. 2011. Do hard pegs avoid currency crises? An evaluation using matching estimators. Economics Letters .
[CrossRef]
76. Benedito Cunguara, Ika Darnhofer. 2011. Assessing the impact of improved agricultural technologies on household income
in rural Mozambique. Food Policy 36:3, 378-390. [CrossRef]
77. Leah Brooks, William C. Strange. 2011. The micro-empirics of collective action: The case of business improvement districts.
Journal of Public Economics . [CrossRef]
78. Olivier Dagnelie, Philippe Lemay-Boucher. 2011. Rosca Participation in Benin: A Commitment Issue*. Oxford Bulletin of
Economics and Statistics no-no. [CrossRef]
79. Tamara G. J. Leech, Janice Johnson Dias. 2011. Risky Sexual Behavior: A Race-specific Social Consequence of Obesity.
Journal of Youth and Adolescence . [CrossRef]
80. Patrick Kline. 2011. Oaxaca-Blinder as a Reweighting Estimator. American Economic Review 101:3, 532-537. [CrossRef]
81. Peter Austin. 2011. An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational
Studies. Multivariate Behavioral Research 46:3, 399-424. [CrossRef]
82. Jennifer Hill, Christopher Weiss, Fuhua Zhai. 2011. Challenges With Propensity Score Strategies in a High-Dimensional
Setting and a Potential Alternative. Multivariate Behavioral Research 46:3, 477-513. [CrossRef]
83. Thomas G. Blomberg, William D. Bales, Karen Mann, Alex R. Piquero, Richard A. Berk. 2011. Incarceration, education and
transition from delinquency. Journal of Criminal Justice . [CrossRef]
84. Pao-Li Chang, Myoung-Jae Lee. 2011. The WTO trade effect. Journal of International Economics . [CrossRef]
85. Mitchell Marsden, Cathleen D. Zick, Robert N. Mayer. 2011. The Value of Seeking Financial Advice. Journal of Family and
Economic Issues . [CrossRef]
86. Nianwen Shi, Liisa Palmer, Bong-Chul Chu, Julie P. Katkin, Caroline B. Hall, Anthony S. Masaquel, Parthiv J. Mahadevia.
2011. Association of RSV lower respiratory tract infection and subsequent healthcare use and costs: a Medicaid claims analysis
in early-preterm, late-preterm, and full-term infants. Journal of Medical Economics 335-340. [CrossRef]
87. K. M. Esterling, M. A. Neblo, D. M. J. Lazer. 2011. Estimating Treatment Effects in the Presence of Noncompliance and
Nonresponse: The Generalized Endogenous Treatment Model. Political Analysis . [CrossRef]
88. Michael Lechner, Ruth Miquel, Conny Wunsch. 2011. LONG-RUN EFFECTS OF PUBLIC SECTOR SPONSORED
TRAINING IN WEST GERMANY. Journal of the European Economic Association no-no. [CrossRef]
89. John C. Ham, Xianghong Li, Patricia B. Reagan. 2011. Matching and semi-parametric IV estimation, a distance-based measure
of migration, and the wages of young men#. Journal of Econometrics 161:2, 208-227. [CrossRef]
90. Gregory N. Price, William Spriggs, Omari H. Swinton. 2011. The Relative Returns to Graduating from a Historically Black
College/University: Propensity Score Matching Estimates from the National Survey of Black Americans. The Review of Black
Political Economy . [CrossRef]
91. CHUNLING LU, YUANLI LIU, JIAN SHEN. 2011. DOES CHINA'S RURAL COOPERATIVE MEDICAL SYSTEM
ACHIEVE ITS GOALS? EVIDENCE FROM THE CHINA HEALTH SURVEILLANCE BASELINE SURVEY IN 2001.
Contemporary Economic Policy no-no. [CrossRef]
92. Claudia Schmoor, Christine Gall, Susanne Stampf, Erika Graf. 2011. Correction of confounding bias in non-randomized
studies by appropriate weighting. Biometrical Journal n/a-n/a. [CrossRef]
93. Karim Chalak, Halbert White. 2011. Viewpoint: An extended class of instrumental variables for the estimation of causal
effects. Canadian Journal of Economics/Revue canadienne d'économique 44:1, 1-51. [CrossRef]
94. Liisa Palmer, Caroline B. Hall, Julie P. Katkin, Nianwen Shi, Anthony S. Masaquel, Kimmie K. McLaurin, Parthiv J.
Mahadevia. 2011. Respiratory outcomes, utilization and costs 12 months following a respiratory syncytial virus diagnosis
among commercially insured late-preterm infants. Current Medical Research and Opinion 27:2, 403-412. [CrossRef]
95. HENRY E. BRADY, JOHN E. MCNULTY. 2011. Turning Out to Vote: The Costs of Finding and Getting to the Polling
Place. American Political Science Review 105:01, 115-134. [CrossRef]
96. Michael Lechner, Rosalia Vazquez-Alvarez. 2011. The effect of disability on labour market outcomes in Germany. Applied
Economics 43:4, 389-412. [CrossRef]

97. E. Galasso, N. Umapathi, J. Yau. 2011. Nutritional Gains from Extended Exposure to a Large-scale Nutrition Programme.
Journal of African Economies . [CrossRef]
98. Michael Lechner. 2011. A Note on the Relation of Inverse-Probability-Weighting and Matching Estimators. Communications
in Statistics - Theory and Methods 40:4, 674-683. [CrossRef]
99. Christian Volpe Martincus, Jeronimo Carballo, Pablo Garcia. 2011. Public programmes to promote firms' exports in
developing countries: are there heterogeneous effects by size categories?. Applied Economics 1-21. [CrossRef]
100. Alberto Abadie, Guido W. Imbens. 2011. Bias-Corrected Matching Estimators for Average Treatment Effects. Journal of
Business and Economic Statistics 29:1, 1-11. [CrossRef]
101. Peter Austin. 2011. A Tutorial and Case Study in Propensity Score Analysis: An Application to Estimating the Effect of InHospital Smoking Cessation Counseling on Mortality. Multivariate Behavioral Research 46:1, 119-151. [CrossRef]
102. Francesco Bravo, David Jacho-Chavez. 2011. Empirical Likelihood for Efficient Semiparametric Average Treatment Effects.
Econometric Reviews 30:1, 1-24. [CrossRef]
103. Michael Lechner. 2011. The Relation of Different Concepts of Causality Used in Time Series and Microeconometrics.
Econometric Reviews 30:1, 109-127. [CrossRef]
104. David Bloom, David Canning, Erica Shenoy. 2011. The effect of vaccination on children's physical and cognitive development
in the Philippines. Applied Economics 1-7. [CrossRef]
105. Suzanne E. Graham, Michal Kurlaender. 2011. Using Propensity Scores in Educational Research: General Principles and
Practical Applications. The Journal of Educational Research 104:5, 340-353. [CrossRef]
106. Paul J. Ferraro, Merlin M. Hanauer. 2011. Protecting Ecosystems and Alleviating Poverty with Parks and Reserves: ‘WinWin’ or Tradeoffs?. Environmental and Resource Economics 48:2, 269. [CrossRef]
107. Francisco Henriquez, Bernardo Lara, Alejandra Mizala, Andrea Repetto. 2011. Effective schools do exist: low-income
children's academic performance in Chile. Applied Economics Letters 1-7. [CrossRef]
108. Gabriel V. Montes-Rojas. 2011. Nonparametric Estimation of ATE and QTE: An Application of Fractile Graphical Analysis.
Journal of Probability and Statistics 2011, 1-23. [CrossRef]
109. Pilar García-Gómez. 2011. Institutions, health shocks and labour market outcomes across Europe. Journal of Health
Economics 30:1, 200-213. [CrossRef]
110. Giovanni Valentini. 2011. Measuring the effect of M&A on patenting quantity and quality. Strategic Management Journal
n/a-n/a. [CrossRef]
111. Matias D. Cattaneo, Max H. FarrellEfficient Estimation of the Dose–Response Function Under Ignorability Using
Subclassification on the Covariates 27, 93-127. [CrossRef]
112. Daniel L. MillimetThe Elephant in the Corner: A Cautionary Tale about Measurement Error in Treatment Effects Models
27, 1-39. [CrossRef]
113. Peter C. Austin. 2011. Comparing paired vs non-paired statistical methods of analyses when making inferences about absolute
risk reductions in propensity-score matched samples. Statistics in Medicine n/a-n/a. [CrossRef]
114. Daniel E. Ho, Donald B. Rubin. 2010. Credible Causal Inference for Empirical Legal Studies. Annual Review of Law and
Social Science 7:1, 110301100413081. [CrossRef]
115. Stefanie Behncke, Markus Frölich, Michael Lechner. 2010. A Caseworker Like Me - Does The Similarity Between The
Unemployed and Their Caseworkers Increase Job Placements?*. The Economic Journal 120:549, 1430-1459. [CrossRef]
116. Richard Berk. 2010. What You Can and Can’t Properly Do with Regression. Journal of Quantitative Criminology 26:4,
481-487. [CrossRef]
117. James Fenske. 2010. THE CAUSAL HISTORY OF AFRICA: A RESPONSE TO HOPKINS. Economic History of
Developing Regions 25:2, 177-212. [CrossRef]
118. Hala Abou-Ali, Hesham El-Azony, Heba El-Laithy, Jonathan Haughton, Shahid Khandker. 2010. Evaluating the impact of
Egyptian Social Fund for Development programmes. Journal of Development Effectiveness 2:4, 521-555. [CrossRef]
119. Marcela Munizaga, Sergio Jara-Díaz, Javiera Olguín, Jorge Rivera. 2010. Generating twins to build weekly time use data
from multiple single day OD surveys. Transportation . [CrossRef]
120. Romina Cavatassi, Lina Salazar, Mario González-Flores, Paul Winters. 2010. How do Agricultural Programmes Alter Crop
Production? Evidence from Ecuador. Journal of Agricultural Economics no-no. [CrossRef]

121. P. C. Austin. 2010. Statistical Criteria for Selecting the Optimal Number of Untreated Subjects Matched to Each Treated
Subject When Using Many-to-One Matching on the Propensity Score. American Journal of Epidemiology 172:9, 1092-1097.
[CrossRef]
122. Niklas Hanes, Erik Norlin, Magnus Sjostrom. 2010. THE CIVIL RETURNS OF MILITARY TRAINING: A STUDY OF
YOUNG MEN IN SWEDEN. Defence and Peace Economics 21:5, 547-565. [CrossRef]
123. Patrycja Scioch. 2010. The Impact of Cleansing Procedures and Coding Decisions for Overlaps on Estimation Results –
Evidence from German Administrative Data. Schmollers Jahrbuch 130:4, 485-512. [CrossRef]
124. Richard E. Just. 2010. Behavior, Robustness, and Sufficient Statistics in Welfare Measurement. Annual Review of Resource
Economics 3:1, 110301102409075. [CrossRef]
125. Peter C. Austin. 2010. The performance of different propensity-score methods for estimating differences in proportions (risk
differences or absolute risk reductions) in observational studies. Statistics in Medicine 29:20, 2137-2148. [CrossRef]
126. Bruno Cassiman, Elena Golovko. 2010. Innovation and internationalization through exports. Journal of International Business
Studies . [CrossRef]
127. Jennifer L. Hill. 2010. Bayesian Nonparametric Modeling for Causal Inference. Journal of Computational and Graphical
Statistics 100903001709005-24. [CrossRef]
128. Markus Frölich. 2010. Exploiting Regional Treatment Intensity for the Evaluation of Labor Market Policies. Journal of the
American Statistical Association 105:491, 1014-1029. [CrossRef]
129. Murillo Campello, John R. Graham, Campbell R. Harvey. 2010. The real effects of financial constraints: Evidence from a
financial crisis#. Journal of Financial Economics 97:3, 470-487. [CrossRef]
130. Phuong Nguyen-Hoang. 2010. Fiscal effects of budget referendums: evidence from New York school districts. Public Choice
. [CrossRef]
131. Liisa Palmer, Caroline B. Hall, Julie P. Katkin, Nianwen Shi, Anthony S. Masaquel, Kimmie K. McLaurin, Parthiv
J. Mahadevia. 2010. Healthcare costs within a year of respiratory syncytial virus among medicaid infants. Pediatric
Pulmonology 45:8, 772-781. [CrossRef]
132. Michael Lechner, Ruth Miquel. 2010. Identification of the effects of dynamic treatments by sequential conditional
independence assumptions. Empirical Economics 39:1, 111-137. [CrossRef]
133. D. W. Gingerich. 2010. Understanding Off-the-Books Politics: Conducting Inference on the Determinants of Sensitive
Behavior with Randomized Response Surveys. Political Analysis 18:3, 349-380. [CrossRef]
134. Martin Huber, Michael Lechner, Conny Wunsch, Thomas Walter. 2010. Do German Welfare-to-Work Programmes Reduce
Welfare Dependency and Increase Employment?. German Economic Review no-no. [CrossRef]
135. M. Fan. 2010. Do Food Stamps Contribute to Obesity in Low-Income Women? Evidence from the National Longitudinal
Survey of Youth 1979. American Journal of Agricultural Economics 92:4, 1165-1180. [CrossRef]
136. Sara K. Pasquali, Matthew Hall, Jennifer S. Li, Eric D. Peterson, James Jaggers, Andrew J. Lodge, Jeffrey P. Jacobs, Marshall
L. Jacobs, Samir S. Shah. 2010. Safety of Aprotinin in Congenital Heart Operations: Results from a Large Multicenter
Database. The Annals of Thoracic Surgery 90:1, 14-21. [CrossRef]
137. Gerhard Krug. 2010. Paradoxe Folgen finanzieller Anreize zur Arbeitsaufnahme für die Beschäftigungsstabilität. KZfSS
Kölner Zeitschrift für Soziologie und Sozialpsychologie 62:2, 191-217. [CrossRef]
138. Javier Gil-Bazo, Pablo Ruiz-Verdú, André A. P. Santos. 2010. The Performance of Socially Responsible Mutual Funds: The
Role of Fees and Management Companies. Journal of Business Ethics 94:2, 243-263. [CrossRef]
139. Laura Hartman, Linus Liljeberg, Oskar Nordström Skans. 2010. Stepping-stones, dead-ends, or both? An analysis of Swedish
replacement contracts. Empirical Economics 38:3, 645-668. [CrossRef]
140. Søren Leth-Petersen. 2010. Intertemporal Consumption and Credit Constraints: Does Total Expenditure Respond to an
Exogenous Shock to Credit?. American Economic Review 100:3, 1080-1103. [CrossRef]
141. Markus Gangl. 2010. Causal Inference in Sociological Research. Annual Review of Sociology 36:1, 21-47. [CrossRef]
142. Haitao Wu, Shijun Ding, Sushil Pandey, Dayun Tao. 2010. Assessing the Impact of Agricultural Technology Adoption on
Farmers' Well-being Using Propensity-Score Matching Analysis in Rural China. Asian Economic Journal 24:2, 141-160.
[CrossRef]
143. Peter Egger, Marko Koethenbuerger, Michael Smart. 2010. Do fiscal transfers alleviate business tax competition? Evidence
from Germany. Journal of Public Economics 94:3-4, 235-246. [CrossRef]

144. Bruno Arpino, Arnstein Aassve. 2010. Estimating the causal effect of fertility on economic wellbeing: data requirements,
identifying assumptions and estimation methods. Empirical Economics . [CrossRef]
145. Katrina Mullan, Andreas Kontoleon, Timothy M. Swanson, Shiqiu Zhang. 2010. Evaluation of the Impact of the Natural
Forest Protection Program on Rural Household Livelihoods. Environmental Management 45:3, 513-525. [CrossRef]
146. Kenneth A Couch, Dana W Placzek. 2010. Earnings Losses of Displaced Workers Revisited. American Economic Review
100:1, 572-589. [CrossRef]
147. 2010. A Data-Generation Process for Data with Specified Risk Differences or Numbers Needed to Treat. Communications
in Statistics - Simulation and Computation 39:3, 563-577. [CrossRef]
148. Núria Rodríguez-Planas, Benus Jacob. 2010. Evaluating active labor market programs in Romania. Empirical Economics
38:1, 65-84. [CrossRef]
149. TERESA D. HARRISON. 2010. DO MERGERS REALLY REDUCE COSTS? EVIDENCE FROM HOSPITALS. Economic
Inquiry . [CrossRef]
150. Stefanie Behncke, Markus FrÃ¶lich, Michael Lechner. 2010. Unemployed and their caseworkers: should they be friends or
foes?. Journal of the Royal Statistical Society: Series A (Statistics in Society) 173:1, 67-92. [CrossRef]
151. C. D. Mayen, J. V. Balagtas, C. E. Alexander. 2010. Technology Adoption and Technical Efficiency: Organic and
Conventional Dairy Farms in the United States. American Journal of Agricultural Economics 92:1, 181-195. [CrossRef]
152. Lorenzo Guarcello, Fabrizia Mealli, Furio Camillo Rosati. 2010. Household vulnerability and child labor: the effect of shocks,
credit rationing, and insurance. Journal of Population Economics 23:1, 169-198. [CrossRef]
153. Martin Huber, Michael Lechner, Conny Wunsch. 2010. Does leaving welfare improve health? Evidence for Germany. Health
Economics n/a-n/a. [CrossRef]
154. A. N. Glynn, K. M. Quinn. 2010. An Introduction to the Augmented Inverse Propensity Weighted Estimator. Political Analysis
18:1, 36-56. [CrossRef]
155. Marcela Munizaga, Sergio Jara-Díaz, Javiera Olguín, Jorge Rivera. 2010. Generating twins to build weekly time use data
from multiple single day OD surveys. Transportation . [CrossRef]
156. Merlin M. Hanauer, Paul J. Ferraro. 2010. Protecting Ecosystems and Alleviating Poverty with Parks and Reserves: ‘WinWin’ or Tradeoffs?. Environmental and Resource Economics . [CrossRef]
157. Peter C. Austin. 2010. Optimal caliper widths for propensity-score matching when estimating differences in means and
differences in proportions in observational studies. Pharmaceutical Statistics n/a-n/a. [CrossRef]
158. Jessica Erin Todd, Paul Winters, Tom Hertz. 2010. Conditional Cash Transfers and Agricultural Production: Lessons from
the Oportunidades Experience in Mexico. Journal of Development Studies 46:1, 39-67. [CrossRef]
159. Takako Nomi. 2010. The Effects of Within-Class Ability Grouping on Academic Achievement in Early Elementary Years.
Journal of Research on Educational Effectiveness 3:1, 56-92. [CrossRef]
160. Peter C. Austin. 2010. Different measures of treatment effect for different research questions. Journal of Clinical
Epidemiology 63:1, 9-10. [CrossRef]
161. Ahmed Khwaja, Gabriel Picone, Martin Salm, Justin G. Trogdon. 2010. A comparison of treatment effects estimators using
a structural model of AMI treatment choices and severity of illness information from hospital charts. Journal of Applied
Econometrics n/a-n/a. [CrossRef]
162. Xavier d’Haultfoeuille. 2010. A new instrumental method for dealing with endogenous selection#. Journal of Econometrics
154:1, 1-15. [CrossRef]
163. Ali Mehryar Karim, Timothy Williams, Leslie Patykewich, Disha Ali, Charlotte E. Colvin, Jessica Posner, Gideon
Rutaremwa. 2009. The Impact of the African Youth Alliance Program on the Sexual Behavior of Young People in Uganda.
Studies in Family Planning 40:4, 289-306. [CrossRef]
164. Michael Lechner, Stephan Wiehler. 2009. Kids or courses? Gender differences in the effects of active labor market policies.
Journal of Population Economics . [CrossRef]
165. ANDREW C. EGGERS, JENS HAINMUELLER. 2009. MPs for Sale? Returns to Office in Postwar British Politics. American
Political Science Review 103:04, 513. [CrossRef]
166. R. D. Plotnick. 2009. Childlessness and the Economic Well-being of Older Americans. The Journals of Gerontology Series
B: Psychological Sciences and Social Sciences 64B:6, 767-776. [CrossRef]
167. Andreas C. Drichoutis, Rodolfo M. Nayga, Jr., Panagiotis Lazaridis. 2009. Can Nutritional Label Use Influence Body Weight
Outcomes?. Kyklos 62:4, 500-525. [CrossRef]

168. Richard Berk. 2009. Can't tell. Criminology & Public Policy 8:4, 845-851. [CrossRef]
169. R. Carriero, J. Ghysels, C. van Klaveren. 2009. Do Parents Coordinate Their Work Schedules? A Comparison of Dutch,
Flemish, and Italian Dual-Earner Households. European Sociological Review 25:5, 603-617. [CrossRef]
170. J. E. McNulty, C. M. Dowling, M. H. Ariotti. 2009. Driving Saints to Sin: How Increasing the Difficulty of Voting Dissuades
Even the Most Motivated Voters. Political Analysis 17:4, 435-455. [CrossRef]
171. G. Robinson, J. E. McNulty, J. S. Krasno. 2009. Observing the Counterfactual? The Search for Political Experiments in
Nature. Political Analysis 17:4, 341-357. [CrossRef]
172. Bradford F. Mills, Joachim Schleich. 2009. Profits or preferences? Assessing the adoption of residential solar thermal
technologies. Energy Policy 37:10, 4145-4154. [CrossRef]
173. Susan M. Allen, Susan Wieland, Jane Griffin, Pedro Gozalo. 2009. Continuity in provider and site of care and preventive
services receipt in an adult Medicaid population with physical disabilities. Disability and Health Journal 2:4, 180-187.
[CrossRef]
174. Ted Joyce, Andrew Racine, Cristina Yunzal-Butler. 2009. Reassessing the WIC effect: Evidence from the Pregnancy Nutrition
Surveillance System. Journal of Policy Analysis and Management 27:2, 277-303. [CrossRef]
175. Loren W. Tauer. 2009. Estimation of Treatment Effects of Recombinant Bovine Somatotropin Using Matching Samples.
Review of Agricultural Economics 31:3, 411-423. [CrossRef]
176. Søren Leth-Petersen, Gabriel Pons Rotger. 2009. Long-term labour-market performance of whiplash claimants. Journal of
Health Economics 28:5, 996-1011. [CrossRef]
177. G. Krug. 2009. In-work Benefits for Low-wage Jobs: Can Additional Income Reduce Employment Stability?. European
Sociological Review 25:4, 459-474. [CrossRef]
178. Pablo Fajnzylber, William Maloney, Gabriel Montes-Rojas. 2009. Releasing Constraints to Growth or Pushing on a String?
Policies and Performance of Mexican Micro-Firms. Journal of Development Studies 45:7, 1027-1047. [CrossRef]
179. Elizabeth A. Stuart, Sue M. Marcus, Marcela V. Horvitz-Lennon, Robert D. Gibbons, Sharon-Lise T. Normand, C. Hendricks
Brown. 2009. Using Non-Experimental Data to Estimate Treatment Effects. Psychiatric Annals 39:7, 719-728. [CrossRef]
180. Daniel L. Millimet, Rusty Tchernis. 2009. On the Specification of Propensity Scores, With Applications to the Analysis of
Trade Policies. Journal of Business and Economic Statistics 27:3, 397-415. [CrossRef]
181. Michael Lechner. 2009. Long-run labour market and health effects of individual sports activities. Journal of Health Economics
28:4, 839-854. [CrossRef]
182. ALFONSO FLORES-LAGUNES, ARTURO GONZALEZ, TODD NEUMANN. 2009. LEARNING BUT NOT EARNING?
THE IMPACT OF JOB CORPS TRAINING ON HISPANIC YOUTH. Economic Inquiry . [CrossRef]
183. David T. Butry. 2009. Fighting fire with fire: estimating the efficacy of wildfire mitigation programs using propensity scores.
Environmental and Ecological Statistics 16:2, 291-319. [CrossRef]
184. Jasjeet S. Sekhon. 2009. Opiates for the Matches: Matching Methods for Causal Inference. Annual Review of Political Science
12:1, 487-508. [CrossRef]
185. Liane Faltermeier, Awudu Abdulai. 2009. The impact of water conservation and intensification technologies: empirical
evidence for rice farmers in Ghana. Agricultural Economics 40:3, 365-379. [CrossRef]
186. M. Beblo, S. Bender, E. Wolf. 2009. Establishment-level wage effects of entering motherhood. Oxford Economic Papers
61:Supplement 1, i11-i34. [CrossRef]
187. Guido W Imbens, Jeffrey M Wooldridge. 2009. Recent Developments in the Econometrics of Program Evaluation. Journal
of Economic Literature 47:1, 5-86. [CrossRef]
188. Ronald Mincy, Jennifer Hill, Marilyn Sinkewicz. 2009. Marriage: Cause or mere indicator of future earnings growth?. Journal
of Policy Analysis and Management 28:3, 417-439. [CrossRef]
189. Emanuela Galasso, Nithin Umapathi. 2009. Improving nutritional status through behavioural change: lessons from
Madagascar. Journal of Development Effectiveness 1:1, 60-85. [CrossRef]
190. Kosuke Imai. 2009. Statistical analysis of randomized experiments with non-ignorable missing binary outcomes: an
application to a voting experiment. Journal of the Royal Statistical Society: Series C (Applied Statistics) 58:1, 83-104.
[CrossRef]
191. S BAIER, J BERGSTRAND. 2009. Estimating the effects of free trade agreements on international trade flows using matching
econometrics. Journal of International Economics 77:1, 63-76. [CrossRef]

192. Hong Wang, Winnie Yip, Licheng Zhang, William Hsiao. 2009. The impact of rural mutual health care on health status:
evaluation of a social experiment in rural China. Health Economics n/a-n/a. [CrossRef]
193. Marco Caliendo. 2009. Start-up subsidies in East Germany: finally, a policy that works?. International Journal of Manpower
30:7, 625-647. [CrossRef]
194. Katherin Barg, Miriam Beblo. 2009. Does marriage pay more than cohabitation?. Journal of Economic Studies 36:6, 552-570.
[CrossRef]
195. Stefano Mazzuco. 2009. Another look into the effect of premarital cohabitation on duration of marriage: an approach based
on matching. Journal of the Royal Statistical Society: Series A (Statistics in Society) 172:1, 255-273. [CrossRef]
196. Michael Lechner. 2009. Sequential Causal Models for the Evaluation of Labor Market Programs. Journal of Business and
Economic Statistics 27:1, 71-83. [CrossRef]
197. Marian Huhman, Lance Potter, Mary Jo Nolin, Dave Judkins. 2009. Evaluation of the VERB™ Campaign: Challenges and
Solutions. Communication Methods and Measures 3:1, 47-60. [CrossRef]
198. Sean Reardon, Jacob Cheadle, Joseph Robinson. 2009. The Effect of Catholic Schooling on Math and Reading Development
in Kindergarten Through Fifth Grade. Journal of Research on Educational Effectiveness 2:1, 45-87. [CrossRef]
199. Richard E. Just. 2008. Distinguishing Preferences from Perceptions for Meaningful Policy Analysis. American Journal of
Agricultural Economics 90:5, 1165-1175. [CrossRef]
200. Juxin Liu, Paul Gustafson. 2008. On Average Predictive Comparisons and Interactions. International Statistical Review 76:3,
419-432. [CrossRef]
201. Kosuke Imai, Gary King, Olivia Lau. 2008. Toward a Common Framework for Statistical Analysis and Development. Journal
of Computational and Graphical Statistics 17:4, 892-913. [CrossRef]
202. K. S. Andam, P. J. Ferraro, A. Pfaff, G. A. Sanchez-Azofeifa, J. A. Robalino. 2008. Measuring the effectiveness of protected
area networks in reducing deforestation. Proceedings of the National Academy of Sciences 105:42, 16089-16094. [CrossRef]
203. Peter Fredriksson, Per Johansson. 2008. Dynamic Treatment Assignment. Journal of Business and Economic Statistics 26:4,
435-445. [CrossRef]
204. Markus Frölich. 2008. Parametric and Nonparametric Regression in the Presence of Endogenous Control Variables.
International Statistical Review 76:2, 214-227. [CrossRef]
205. K. John McConnell, Neal T. Wallace, Charles A. Gallia, Jeanene A. Smith. 2008. Effect of Eliminating Behavioral Health
Benefits for Selected Medicaid Enrollees. Health Services Research 43:4, 1348-1365. [CrossRef]
206. V. S. Harder, E. A. Stuart, J. C. Anthony. 2008. Adolescent Cannabis Problems and Young Adult Depression: Male-Female
Stratified Propensity Score Analyses. American Journal of Epidemiology 168:6, 592-601. [CrossRef]
207. Stephen L. Morgan, Jennifer J. Todd. 2008. A DIAGNOSTIC ROUTINE FOR THE DETECTION OF CONSEQUENTIAL
HETEROGENEITY OF CAUSAL EFFECTS. Sociological Methodology . [CrossRef]
208. P BARROS, M MACHADO, A SANZDEGALDEANO. 2008. Moral hazard and the demand for health services: A matching
estimator approach. Journal of Health Economics 27:4, 1006-1025. [CrossRef]
209. Hans J. Baumgartner, Marco Caliendo. 2008. Turning Unemployment into Self-Employment: Effectiveness of Two Start-Up
Programmes. Oxford Bulletin of Economics and Statistics 70:3, 347-373. [CrossRef]
210. Jennifer Hill. 2008. Discussion of research using propensity-score matching: Comments on ‘A critical appraisal of propensityscore matching in the medical literature between 1996 and 2003’ by Peter Austin,Statistics in Medicine. Statistics in Medicine
27:12, 2055-2061. [CrossRef]
211. Qi Li, Jeffrey S Racine, Jeffrey M Wooldridge. 2008. Estimating Average Treatment Effects with Continuous and Discrete
Covariates: The Case of Swan-Ganz Catheterization. American Economic Review 98:2, 357-362. [CrossRef]
212. Marco Caliendo, Reinhard Hujer, Stephan Thomsen. 2008. Identifying effect heterogeneity to improve the efficiency of job
creation schemes in Germany. Applied Economics 40:9, 1101-1122. [CrossRef]
213. Kosuke Imai, Gary King, Elizabeth A. Stuart. 2008. Misunderstandings between experimentalists and observationalists about
causal inference. Journal of the Royal Statistical Society: Series A (Statistics in Society) 171:2, 481-502. [CrossRef]
214. Deon Filmer, Norbert Schady. 2008. Getting Girls into School: Evidence from a Scholarship Program in Cambodia. Economic
Development and Cultural Change 56:3, 581-617. [CrossRef]
215. Andrea Ichino, Fabrizia Mealli, Tommaso Nannicini. 2008. From temporary help jobs to permanent employment: what can
we learn from matching estimators and their sensitivity?. Journal of Applied Econometrics 23:3, 305-327. [CrossRef]

216. X GONZALEZ, C PAZO. 2008. Do public subsidies stimulate private R&D spending?. Research Policy 37:3, 371-389.
[CrossRef]
217. Daniel J. Henderson, Daniel L. Millimet. 2008. Is gravity linear?. Journal of Applied Econometrics 23:2, 137-172. [CrossRef]
218. Pandej Chintrakarn. 2008. Estimating the Euro Effects on Trade with Propensity Score Matching*. Review of International
Economics 16:1, 186-198. [CrossRef]
219. Pedro L. Gozalo, Susan C. Miller, Orna Intrator, Janet P. Barber, Vincent Mor. 2008. Hospice Effect on Government
Expenditures among Nursing Home Residents. Health Services Research 43:1p1, 134-153. [CrossRef]
220. Marco Caliendo, Sabine Kopeinig. 2008. SOME PRACTICAL GUIDANCE FOR THE IMPLEMENTATION OF
PROPENSITY SCORE MATCHING. Journal of Economic Surveys 22:1, 31-72. [CrossRef]
221. E BATTISTIN, E RETTORE. 2008. Ineligibles and eligible non-participants as a double comparison group in regressiondiscontinuity designs. Journal of Econometrics 142:2, 715-730. [CrossRef]
222. G IMBENS, T LEMIEUX. 2008. Regression discontinuity designs: A guide to practice. Journal of Econometrics 142:2,
615-635. [CrossRef]
223. Marco Caliendo, Reinhard Hujer, Stephan L. ThomsenThe employment effects of job-creation schemes in Germany: A
microeconometric evaluation 21, 381-428. [CrossRef]
224. Rafael Lalive, Jan C. Van Ours, Josef Zweimüller. 2008. The Impact of Active Labour Market Programmes on The Duration
of Unemployment in Switzerland*. The Economic Journal 118:525, 235-257. [CrossRef]
225. Michael LechnerMatching estimation of dynamic treatment models: Some practical issues 21, 289-333. [CrossRef]
226. Jennifer Brown, Justine Hastings, Erin T. Mansur, Sofia B. Villas-Boas. 2008. Reformulating competition? Gasoline content
regulation and wholesale gasoline prices. Journal of Environmental Economics and Management 55:1, 1-19. [CrossRef]
227. Martin Boehm. 2008. Determining the impact of internet channel use on a customer's lifetime. Journal of Interactive
Marketing 22:3, 2-22. [CrossRef]
228. Stephen Senn, Erika Graf, Angelika Caputo. 2007. Stratification for the propensity score compared with linear regression
techniques to assess the effect of treatment or exposure. Statistics in Medicine 26:30, 5529-5544. [CrossRef]
229. Jennie E. Brand, Yu Xie. 2007. IDENTIFICATION AND ESTIMATION OF CAUSAL EFFECTS WITH TIME?VARYING
TREATMENTS AND TIME-VARYING OUTCOMES*. Sociological Methodology 37:1, 393-434. [CrossRef]
230. Michael Lechner, Ruth Miquel, Conny Wunsch. 2007. The Curse and Blessing of Training the Unemployed in a Changing
Economy: The Case of East Germany After Unification. German Economic Review 8:4, 468-509. [CrossRef]
231. D VANDEWALLE, R MU. 2007. Fungibility and the flypaper effect of project aid: Micro-evidence for Vietnam#. Journal
of Development Economics 84:2, 667-685. [CrossRef]
232. OZKAN EREN. 2007. Measuring the Union–Nonunion Wage Gap Using Propensity Score Matching. Industrial Relations
46:4, 766-780. [CrossRef]
233. Aliou Diagne, Matty Demont. 2007. Taking a new look at empirical models of adoption: average treatment effect estimation
of adoption rates and their determinants. Agricultural Economics 37:2-3, 201-210. [CrossRef]
234. Patricia M. Danzon, Andrew Epstein, Sean Nicholson. 2007. Mergers and acquisitions in the pharmaceutical and biotech
industries. Managerial and Decision Economics 28:4-5, 307-328. [CrossRef]
235. HOLGER GÖRG, ERIC STROBL. 2007. The Effect of R&D Subsidies on Private R&D. Economica 74:294, 215-234.
[CrossRef]
236. A WAGSTAFF, S YU. 2007. Do health sector reforms have their intended impacts?The World Bank's Health VIII project in
Gansu province, China. Journal of Health Economics 26:3, 505-535. [CrossRef]
237. Boris Augurzky, Jochen Kluve. 2007. Assessing the performance of matching algorithms when selection into treatment is
strong. Journal of Applied Econometrics 22:3, 533-557. [CrossRef]
238. Pedro L. Gozalo, Susan C. Miller. 2007. Hospice Enrollment and Evaluation of Its Causal Effect on Hospitalization of Dying
Nursing Home Patients. Health Services Research 42:2, 587-610. [CrossRef]
239. Ricky N. Bluthenthal, Greg Ridgeway, Terry Schell, Rachel Anderson, Neil M. Flynn, Alex H. Kral. 2007. Examination of
the association between syringe exchange program (SEP) dispensation policy and SEP client-level syringe coverage among
injection drug users. Addiction 102:4, 638-646. [CrossRef]
240. David R. Judkins, David Morganstein, Paul Zador, Andrea Piesse, Brandon Barrett, Pushpal Mukhopadhyay. 2007. Variable
selection and raking in propensity scoring. Statistics in Medicine 26:5, 1022-1033. [CrossRef]

241. Sergio Firpo. 2007. Efficient Semiparametric Estimation of Quantile Treatment Effects. Econometrica 75:1, 259-276.
[CrossRef]
242. Hartmut Lehmann, Jonathan WadsworthWage Arrears and Inequality in the Distribution of Pay: Lessons from Russia 26,
125-155. [CrossRef]
243. James J. Heckman, Edward J. VytlacilChapter 71 Econometric Evaluation of Social Programs, Part II: Using the Marginal
Treatment Effect to Organize Alternative Econometric Estimators to Evaluate Social Programs, and to Forecast their Effects
in New Environments 6, 4875-5143. [CrossRef]
244. Martin RavallionChapter 59 Evaluating Anti-Poverty Programs 4, 3787-3846. [CrossRef]
245. Susanne Rässler. 2006. Der Einsatz von Missing Data Techniken in der Arbeitsmarktforschung des IAB. Allgemeines
Statistisches Archiv 90:4, 527-552. [CrossRef]
246. Markus Frölich. 2006. Non-parametric regression for binary dependent variables. The Econometrics Journal 9:3, 511-540.
[CrossRef]
247. Matthew Potoski, Aseem Prakash. 2006. Covenants with weak swords: ISO 14001 and facilities' environmental performance.
Journal of Policy Analysis and Management 24:4, 745-769. [CrossRef]
248. Paul Hutchinson, Jennifer Wheeler. 2006. Advanced Methods for Evaluating the Impact of Family Planning Communication
Programs: Evidence from Tanzania and Nepal. Studies in Family Planning 37:3, 169-186. [CrossRef]
249. Jennifer Hill, Jerome P. Reiter. 2006. Interval estimation for treatment effects using propensity score matching. Statistics in
Medicine 25:13, 2230-2256. [CrossRef]
250. L BAUWENS, H PETERBOSWIJK, J URBAIN. 2006. Causality and exogeneity in econometrics. Journal of Econometrics
132:2, 305-309. [CrossRef]
251. Marco Caliendo, Reinhard Hujer. 2006. The microeconometric estimation of treatment effects—An overview. Allgemeines
Statistisches Archiv 90:1, 199-215. [CrossRef]
252. Eva Cantoni, Xavier De Luna. 2006. Non-parametric adjustment for covariates when estimating a treatment effect. Journal
of Nonparametric Statistics 18:2, 227-244. [CrossRef]
253. Alberto Abadie, Guido W. Imbens. 2006. Large Sample Properties of Matching Estimators for Average Treatment Effects.
Econometrica 74:1, 235-267. [CrossRef]
254. Han Vries, Marc N. Elliott, Kimberly A. Hepner, San D. Keller, Ron D. Hays. 2005. Equivalence of Mail and Telephone
Responses to the CAHPSR Hospital Survey. Health Services Research 40:6p2, 2120-2139. [CrossRef]
255. BETH A. SIMMONS, DANIEL J. HOPKINS. 2005. The Constraining Power of International Treaties: Theory and Methods.
American Political Science Review 99:04. . [CrossRef]
256. Amelia M. Haviland, Daniel S. Nagin. 2005. Causal inferences with group based trajectory models. Psychometrika 70:3,
557-578. [CrossRef]
257. Michael E. Sobel. 2005. Discussion: 'The Scientific Model of Causality'. Sociological Methodology 35:1, 99-134. [CrossRef]
258. James J. Heckman. 2005. Rejoinder: Response to Sobel*. Sociological Methodology 35:1, 135-162. [CrossRef]
259. RICHARD A. BERK. 2005. KNOWING WHEN TO FOLD 'EM: AN ESSAY ON EVALUATING THE IMPACT OF
CEASEFIRE, COMPSTAT, AND EXILE*. Criminology <html_ent glyph="@amp;" ascii="&"/> Public Policy 4:3,
451-465. [CrossRef]
260. Richard Harris. 2005. ECONOMICS OF THE WORKPLACE: SPECIAL ISSUE EDITORIAL. Scottish Journal of Political
Economy 52:3, 323-343. [CrossRef]
261. Richard Blundell, Lorraine Dearden, Barbara Sianesi. 2005. Evaluating the effect of education on earnings: models, methods
and results from the National Child Development Survey. Journal of the Royal Statistical Society: Series A (Statistics in
Society) 168:3, 473-512. [CrossRef]
262. Peter Thompson, Melanie Fox-Kean. 2005. Patent Citations and the Geography of Knowledge Spillovers: A Reassessment:
Reply. American Economic Review 95:1, 465-466. [CrossRef]
263. Thomas A. DiPrete, Markus Gangl. 2004. Assessing Bias in the Estimation of Causal Effects: Rosenbaum Bounds on
Matching Estimators and Instrumental Variables Estimation with Imperfect Instruments. Sociological Methodology 34:1,
271-310. [CrossRef]

