Journal of Research on Educational Effectiveness

ISSN: 1934-5747 (Print) 1934-5739 (Online) Journal homepage: https://www.tandfonline.com/loi/uree20

Partially Identified Treatment Effects for
Generalizability
Wendy Chan
To cite this article: Wendy Chan (2017) Partially Identified Treatment Effects for
Generalizability, Journal of Research on Educational Effectiveness, 10:3, 646-669, DOI:
10.1080/19345747.2016.1273412
To link to this article: https://doi.org/10.1080/19345747.2016.1273412

View supplementary material

Accepted author version posted online: 12
Jan 2017.
Published online: 21 Mar 2017.
Submit your article to this journal

Article views: 194

View related articles

View Crossmark data

Citing articles: 2 View citing articles

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=uree20

JOURNAL OF RESEARCH ON EDUCATIONAL EFFECTIVENESS
2017, VOL. 10, NO. 3, 646–669
https://doi.org/10.1080/19345747.2016.1273412

METHODOLOGICAL STUDIES

Partially Identiﬁed Treatment Effects for Generalizability
Wendy Chana
ABSTRACT

KEYWORDS

Recent methods to improve generalizations from nonrandom samples
typically invoke assumptions such as the strong ignorability of sample
selection, which is challenging to meet in practice. Although
researchers acknowledge the difﬁculty in meeting this assumption,
point estimates are still provided and used without considering
alternative assumptions. We compare the point identifying assumption
of strong ignorability of sample selection with two alternative
assumptions—bounded sample variation and monotone treatment
response—that partially identify the parameter of interest, yielding
interval estimates. Additionally, we explore the role that population
data frames play in contributing identifying power for the interval
estimates. We situate the comparison around causal generalization
with nonrandom samples by applying the assumptions to a cluster
randomized trial in education. Bounds on the population average
treatment effect are derived under the alternative assumptions and
the case when no assumptions are made on the data. While
comparing the bounds, we discuss the plausibility of each alternative
assumption and the practical trade-offs. We highlight the importance
of thoughtfully considering the role that assumptions play in causal
generalization by illustrating the differences in inferences from
different assumptions.

causal inference
generalizability
partial identiﬁcation
bounds

Policymakers have become increasingly interested in the extent to which inferences from
experimental studies apply to target populations of inference. For example, will the results of
a new mathematics curriculum from a sample of schools be applicable when implemented
in a larger group of schools? This interest in the external validity or generalizability of results
reﬂects the growing use of well-designed evaluations to inform decisions in evidence-based
policymaking. The generalizability of experimental results to target populations of inference
requires both treatment randomization and probability sampling. When probability sampling is used to select the experimental sample, estimates of the treatment effect are both
unbiased for the sample and for the population, and generalizations can be made without
modeling assumptions. Because the information from experimental studies is typically used
to inform policy decisions, particularly in education and medicine, designing evaluations
that strengthen both the internal validity and the generalizability of inferences has been particularly important to policymakers and researchers.
CONTACT Wendy Chan
wechan@upenn.edu
Graduate School of Education, University of Pennsylvania, 3700
Walnut Street, Philadelphia, PA 19104-6216, USA.
a
University of Pennsylvania, Philadelphia, Pennsylvania, USA
Supplemental data for this article can be accessed at https://doi.org/10.1080/19345747.2016.1273412.
© 2017 Taylor & Francis Group, LLC

In social science research, a common challenge to the generalizability of experimental
results is that the samples used in experimental studies are typically not randomly selected;
in other words, they do not employ probability sampling (Greenberg & Shroder, 2004;
Olsen, Orr, Bell, & Stuart, 2013). The populations of interest are often not speciﬁed beforehand or may not be part of the population that is available during the time of the experiment
(O’Muircheartaigh & Hedges, 2014). Without probability sampling, generalizing treatment
effects is challenging because the bias induced from self-selection no longer allows for
model-free estimation of treatment effects (Keiding & Louis, 2016). Statisticians have
recently developed methods to improve generalizations from nonprobability samples using
propensity scores (O’Muircheartaigh & Hedges, 2014; Stuart, Cole, Bradshaw & Leaf, 2011;
Tipton, 2013). Propensity score methods match experimental samples to an inference population based on observable characteristics so that the matched groups are compositionally
similar (Rosenbaum & Rubin, 1984). This application extends previous work on the use of
propensity scores in quasi-experimental and observational studies that addressed treatment
selection bias (Rosenbaum & Rubin, 1983). These methods also extend those used to adjust
for nonresponse in survey sampling (Little, 1986). Post hoc adjustments using propensity
scores include inverse propensity weighting (IPW; Stuart et al., 2011) and subclassiﬁcation
(O’Muircheartaigh & Hedges, 2014; Tipton, 2013), both of which reweight the sample to
obtain bias-reduced estimates of the average treatment effect.
Although propensity score methods have signiﬁcantly contributed to causal generalization, the assumptions required for these methods are often difﬁcult to meet and controversial. In particular, the generalizability of treatment effects requires that sample selection be
strongly ignorable given the propensity scores when the sample is not randomly selected.
Strong ignorability of sample selection requires that two conditions be met. First, the propensity scores must contain information on all possible covariates that explain treatment
effect variation and affect sample selection. If this is met, when propensity scores are
included in the analysis, the sample can be considered like a probability sample. Second,
every unit in the population must have a “similar unit” in the experimental sample in which
the comparability is based on the covariates used in the propensity score model. Situations
in which strong ignorability of sample selection fails for either one or both conditions can
occur, and it is important for researchers to consider the applicability of results when core
assumptions do not hold.
In practice, assumptions such as strong ignorability of sample selection are made in order
to get point estimates (single values) of average treatment effects. However, an important
concern is that although researchers acknowledge the challenges in meeting core assumptions and empirically validating them, point estimates are still given and used in spite of the
challenges (Stuart et al., 2011; Tipton, Hedges, Vaden-Kiernan, Borman, Sullivan, & Caverly,
2014). Importantly, alternative assumptions to strong ignorability of sample selection may
provide different perspectives on the same problem. Because different assumptions can lead
to different inferences, it is essential for researchers to consider the plausibility of different
assumptions and the role that assumptions play in the resulting inferences. For generalization in particular, if strong ignorability of sample selection is not met, researchers should discuss alternative assumptions and their trade-offs.
The goal of this article is twofold. First, we illustrate that some assumptions, not necessarily strong ignorability of sample selection, are necessary for generalizations to be informative
in determining the direction of the expected treatment effect. This is particularly true for
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

647

generalizations from small experimental studies in which the results are applied to a population that is at least 20 times larger than the sample. We demonstrate that this is also the case
even if auxiliary sources of data on the population (as is commonly used for generalizations)
are available. Second, we explore and compare alternative assumptions to strong ignorability
of sample selection and consider their practical trade-offs. In particular, we focus on
bounded sample variation and monotone treatment response, which differ from strong ignorability of sample selection in that they are insufﬁcient to point identify the average treatment effect and instead lead to interval estimates of the population parameter. The fact that
these assumptions lead to a range of possible values and impose few restrictions on the distribution of outcomes has led researchers to refer to them as weaker assumptions (Manski,
2009).
We center our discussion of these alternative assumptions around a completed cluster
randomized trial (CRT) in education. We focus on this CRT because the process by which
the schools were recruited is common among educational CRTs, the features of the study
(sample to population size ratio) are typical of many educational studies, and initial analyses
of the sample and population data suggest that strong ignorability of sample selection may
not hold. The preliminary analysis of the results was completed in Konstantopoulous, Miller,
and van der Ploeg (2013) and the generalizability of point estimates of the treatment effect
was considered in Tipton, Hallberg, Hedges, and Chan (2016). In order to understand the
roles and tradeoffs of different assumptions, we pose three important questions. First, are
interval estimates of average treatment effects informative when no assumptions are made
on the experimental data? This question is addressed by examining the interval estimates
from the CRT solely based on the data and the randomized nature of the treatment assignment. Second, can auxiliary sources of data improve upon the interval estimates of treatment
effects in causal generalization? This question is addressed by considering differences in the
interval estimates of treatment effects when covariate information from the population is
used to contribute identifying power. Third, what inferences can be made when bounded
sample variation and monotone treatment response are imposed, and what are the tradeoffs among these assumptions? To investigate this issue, we present the bounds for the CRT
under the two alternative assumptions, discuss the plausibility of each assumption for the
CRT, and compare the interval estimates to the ones under the no-assumptions framework.
We also compare each set of interval estimates with the point estimates derived under strong
ignorability of sample selection.
Although standard errors are provided for the point estimates, they are not provided for
the interval estimates for two reasons. First, because our discussion centers on identiﬁcation
of treatment effects when strong ignorability of sample selection is not met, we focus our
comparison between the nonparametric interval estimates and the point estimates. Second,
measures of statistical precision require speciﬁcation of the sampling process that generated
the data, which is unknown in the case of generalization studies from nonprobability samples (Manski & Pepper, 2015). We instead compare the inferences from different assumptions by assessing the differences in magnitudes of the estimates.
The article is organized as follows. In the ﬁrst section, we present the CRT example and
discuss the generalization problem of interest. In the second section, we introduce the notation and assumptions needed for point identiﬁcation of average treatment effects and discuss
the plausibility of strong ignorability of sample selection. In the third section, we introduce
the frameworks for deriving bounds and provide the bounds for the no-assumptions case
648

W. CHAN

(under treatment randomization), bounded sample variation, and monotone treatment
response. Additionally, a “fusion” approach combining the bounding methods with propensity score subclassiﬁcation is explored. In the ﬁnal two sections, we apply the bounds to the
CRT, discuss the plausibility of each alternative assumption and the trade-offs, and compare
these bounds with the point estimates of the average treatment effect. We conclude with a
discussion of the role of assumptions in causal generalization and ideas for future research.

CRT Example
In 2006, the Indiana Department of Education and the Indiana State Board of Education
managed the implementation of a new assessment system to measure annual student growth
and to provide feedback to teachers (Konstantopoulos et al., 2013). During the 2009–2010
academic year, 56 K–8 (elementary to middle) schools from the state of Indiana volunteered
to implement the new system, of which 34 were randomly assigned to the state’s new assessment system, while 22 served as control schools. In the treatment schools, students were
given four diagnostic assessments that were aligned with the Indiana state test, and their
teachers received online reports on their performance to dynamically guide their instruction
in the periods leading up to the state exam. The effectiveness of the assessment system was
measured using the Indiana Statewide Testing for Educational Progress-Plus (ISTEPC)
scores in English Language Arts (ELA) and mathematics. For each study school, the ISTEPC
scores were discretized using the minimum cutoff scores from the Indiana Department of
Education and aggregated as either “Pass” or “Not Pass.”
A natural question emerges from this study: If every school in Indiana were to implement
this system, what is the expected impact on student achievement? In other words, to what
extent do the results from the Indiana CRT generalize to the entire state? If the study was
planned with generalization in mind, both treatment randomization and probability sampling would be implemented to facilitate causal generalization by directly estimating the
average treatment effect for the population of Indiana schools. However, a key concern for
generalization in this example is whether this sample of volunteer schools is “representative”
of the 1,514 K–8 schools in Indiana during that year (Kruskal & Mosteller, 1980). If the
schools that self-selected into this study differed from the schools that did not volunteer (for
example, on the demographic composition of its students or in the schools’ past performance), and these differences moderated potential treatment effects, any estimate of the
treatment effect of the benchmark assessment system will be biased for the population of
Indiana schools. By making assumptions about the expected treatment effects among the
volunteer schools and the schools not in the experimental study, it is possible to generate
bias-reduced estimates of the average treatment effect for the population. In the next section,
we introduce notation and formally deﬁne the assumptions needed to get point estimates of
the average treatment effect.

Notation and Assumptions
The estimation of causal treatment effects is framed using Rubin’s Causal Model (Rubin,
1974, 1977, 1980, 1986). Let P denote the population of inference consisting of N schools, of
which n schools are selected into the sample. Let W be an indicator of treatment assignment
where W D 1 if a school was assigned to implement the assessment system (treatment) and
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

649

W D 0 if a school was not assigned to implement the system (control). For each school in P,
let Y(W) denote the binary potential outcome of whether a school received a “Pass” score or
not under the respective treatment condition (W D 0,1). Finally, let Z be an indicator of
sample selection where Z D 1 if a school was in the experimental study and Z D 0 otherwise.
To estimate treatment effects, the stable unit treatment value assumption (SUTVA) for
the sample and population is required (Rubin, 1978, 1980, 1990; Tipton, 2013). Under
SUTVA for the sample, there is only one version of the treatment, and the potential outcomes Y(0), Y(1) of each school depend only on the treatment received by that school and
not on the treatment received by another school. Under SUTVA for the population, this
condition must also hold for the sample selection process, in which the potential outcomes
do not depend on the proportion of schools selected into the experiment and the potential
treatment effects do not depend on being involved in the experimental study. Additionally,
SUTVA requires that there is no interference between schools, both between the treatment
and control schools in the sample, and between the volunteer and non-volunteer schools in
the population.
Assuming that SUTVA holds, the treatment effect for each school in the sample and population (the volunteer and non-volunteer schools) is deﬁned as t D Y(1) ¡ Y(0), and because
P consists of schools that were selected and not selected into the experimental study, we can
deﬁne two average treatment effects
ðSampleÞ : t SATE D Eðt j Z D 1Þ
ðPopulationÞ : t PATE D Eðt j Z D 1Þ



PðZ D 1Þ C Eðt j Z D 0Þ



ð1 ¡ PðZ D 1ÞÞ

where tSATE is the expected sample average treatment effect (SATE) and tPATE is the
expected population average treatment effect (PATE). For generalization, the PATE is the
parameter of interest. Note that the PATE deﬁned here is the measure of impact for all
schools in the state of Indiana, but a different PATE can be estimated for populations or
groups that change over time and space. Importantly, the SATE and PATE are equivalent
when E(tjZ D 1) D E(tjZ D 0), the case under probability sampling, when the potential treatment effects are constant, when every school in the population is in the study (P(Z D 1) D 1),
or when sample selection and heterogeneity in treatment effects are independent (Imai, King,
& Stuart, 2008; Rubin, 1974). Otherwise, estimates of the SATE are considered to be na€ıve
and biased estimates of the PATE.
For each school in P, we assume that a vector of characteristics or covariates, X, is
observed where X may include both categorical and continuous measures. This covariate
information is used to compare the volunteer with the non-volunteer schools in the study
and can be obtained from sources such as the Common Core of Data or state longitudinal
data systems. For each school, the sampling propensity score (for a ﬁnite population) is
deﬁned as:
sðXÞ D PðZ D 1 j XÞ
Propensity scores model the probability of sample membership as a function of X and
have the advantage of being balancing scores where matching by the propensity score is
equivalent to matching by the covariates in the propensity score model (Rosenbaum &
Rubin, 1983). A common method of estimating s(X) is with a logistic regression model with
650

W. CHAN

an intercept term b0 ,
logðsðXÞ=ð1 ¡ sðXÞÞÞ D b0 C b1 X1 C b2 X2 C . . . C bp Xp
based on X D (X1, X2, …, Xp) covariates.
To obtain bias-reduced point estimates of the PATE using propensity scores, several
assumptions, in addition to SUTVA, are needed.
First, treatment assignment must be strongly ignorable given the propensity scores (Stuart
et al., 2011; Tipton, 2013):
Y ð1Þ; Y ð0Þ?W j Z D 1; sðXÞ and 0 < PðW D 1 j Z D 1; sðXÞÞ < 1
Among schools selected into the study (Z D 1), the potential outcomes are conditionally
independent of treatment assignment, and every school in the study must have some probability of being assigned to the treatment condition. This condition is typically met in randomized experiments such as the Indiana CRT (Rosenbaum & Rubin, 1983; Stuart et al., 2011).
Second, unconfounded sample selection must hold where sample selection is conditionally independent of the treatment effects (Stuart et al., 2011; Tipton, 2013):
ðt D Y ð1Þ ¡ Y.0/Þ?Z j sðXÞ
Finally, strong ignorability of sample selection is needed to fully identify the PATE.
Because this assumption is the focus of this article, we refer to this assumption as simply
sampling ignorability hereafter. This assumption requires both unconfounded sample selection and that the distribution of covariates X in the sample and population share common
support (Tipton, 2013, 2014):
t?Z j sðXÞ and 0 < sðXÞ  1
The ﬁrst stipulation, unconfounded sample selection, requires that X (and consequently
s(X)) includes all covariates that explain the potential variation in treatment effects and sample selection. The second stipulation requires that every school in the population must have a
relevant comparison school in the sample so that no school should have s(X) D 0.

Sampling Ignorability in the Indiana CRT
Whether the sampling ignorability assumption is credible and plausible in practice is a controversial topic. At the heart of the matter, sampling ignorability is an invariance assumption in
which the effect of the assessment system is the same (invariant) for students, on average,
regardless of whether the school volunteered to be in the Indiana CRT, once the propensity
scores are taken into account. In other words, if sampling ignorability holds, self-selection
does not matter because any differences between the volunteer and non-volunteer schools are
explained by the propensity scores. Conceivably, sampling ignorability may not hold for a few
reasons. For example, schools that respond differently to the assessment system may have a
strong support base from parents, which may not be measured and would therefore be omitted from X. Additionally, some schools that chose not to volunteer in the Indiana CRT may
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

651

have an assessment system already in place, and the impact of the CRT system may be different for these schools compared with schools that did not have such a system or such resources
to begin with for their students. If these characteristics of schools are not included in X, sampling ignorability does not hold. Note that the concern here is that these potential covariates
may explain treatment effect variability, but they are not included in the propensity score
model. Alternatively, if the Indiana sample consisted of all single-gender schools, but generalization was made to a population of coeducational schools, sampling ignorability does not
hold. Coeducational schools in the population would not have appropriate matches in the
sample so that these schools would have a sampling propensity score s(X)  0.

Partial Identiﬁcation of the PATE
Examples in which sampling ignorability fails, as described above, can occur in practice.
Manski (2009) ﬁrst recommended that researchers begin analyses by considering what can
be learned from the data alone, without any assumptions, so that a “domain of consensus” is
the established starting point. If sampling ignorability is unlikely to hold, other assumptions
that partially identify the PATE, yielding interval estimates, can provide alternatives. Partial
identiﬁcation methods were ﬁrst developed in response to the concern that the credibility of
assumptions was compromised by the use of strong point-identifying assumptions, many of
which were clearly violated in practice (Manski, 1990). Instead of using point-identifying
assumptions, partial identiﬁcation analyzes the extent to which alternative assumptions yield
potentially informative bounds.
To explore these partially identifying assumptions, we ﬁrst decompose the estimator of
the PATE and consider the role that assumptions play in contributing identifying power.
Because our empirical example is a CRT, we frame this analysis around a randomized experiment and assume SUTVA, strongly ignorable treatment assignment, and perfect compliance. The average treatment effect, E(t) D E(Y(1) ¡ Y(0)) D E(Y(1)) ¡ E(Y(0)), is the
difference of two expected potential outcomes. Using the law of iterated expectations, the
SATE, a function of the treatment assignment indicator W, is decomposed as follows:
EðY.1/Þ D EðY ð1Þ j W D 1Þ  PðW D 1Þ C EðY ð1Þ j W D 0Þ  PðW D 0Þ




EðY.0/Þ D EðY ð0Þ j W D 1Þ PðW D 1Þ C EðY ð0Þ j W D 0Þ PðW D 0Þ

(1)
(2)

The PATE, a function of both W and sample selection indicator Z, is decomposed as follows:
EðY.1/Þ D EðY ð1Þ j W D 1; Z D 1Þ  PðW D 1; Z D 1Þ C EðY ð1Þ j W D 0; Z D 1Þ


PðW D 0; Z D 1Þ C EðY ð1Þ j W D 1; Z D 0Þ  PðW D 1; Z D 0Þ
C EðY ð1Þ j W D 0; Z D 0Þ  PðW D 0; Z D 0Þ

(3)

EðY.0/Þ D EðY ð0Þ j W D 1; Z D 1Þ  PðW D 1; Z D 1Þ C EðY ð0Þ j W D 0; Z D 1Þ


PðW D 0; Z D 1Þ C EðY ð0Þ j W D 1; Z D 0Þ  PðW D 1; Z D 0Þ
C EðY ð0Þ j W D 0; Z D 0Þ  PðW D 0; Z D 0Þ

(4)

Because each study school is assigned to at most one treatment, the quantities in Equations (1 through 4) cannot be identiﬁed, a premise of the Fundamental Problem of Causal
652

W. CHAN

Inference (Holland, 1986). The terms E(Y(1)jW D 0) and E(Y(0)jW D 1) are unobservable
counterfactuals because they refer to the expected outcome under treatment (control) when
assigned to control (treatment), which are unknown (Dawid, 2000; Greenland, Pearl, & Robins, 1999). We refer to these quantities as treatment counterfactuals.
The PATE is a function of the same treatment counterfactuals so it, too, cannot be identiﬁed. However, the decomposition of the PATE differs from that of the SATE in two important
ways. First, the potential outcomes in (3) and (4) necessarily include additional counterfactual
terms because the PATE requires information about sample selection, given by Z. Second, the
PATE includes four additional counterfactuals, E(Y(1)jW D 1, Z D 0), E(Y(1)jW D 0, Z D 0),
E(Y(0)jW D 1, Z D 0), and E(Y(0)jW D 0, Z D 0), which will be referred to as the sample counterfactuals. These are the potential outcomes under a treatment condition for schools not
selected into the experimental study. Note that the treatment and sample counterfactuals are
unobservable for different reasons. The goal of any causal inference study is to identify both
types of counterfactuals, whether through the design stage of the study or through the use of
assumptions on the distribution of these potential outcomes (Rubin, 2005).
Because the Indiana CRT was a randomized experiment, treatment assignment is
strongly ignorable so that Y(1), Y(0) ? W, E(YjW D 1) D E(Y(1)jW D 1) D E(Y(1)) and
E(YjW D 0) D E(Y(0)jW D 0) D E(Y(0)) and the SATE is point identiﬁed (Rubin, 1978).
Under treatment randomization, the distribution of unobservable treatment counterfactuals
is equivalent to that of the realized potential outcomes, which allows for model-free estimation of the SATE (Imai, King, & Stuart, 2008; Rubin, 1974). For the PATE, however,
the four sample counterfactuals remain unobservable in the absence of probability sampling, so that assumptions on the distribution of these counterfactuals are needed in order
to achieve point identiﬁcation.
In the following sections, we exclude sampling ignorability and begin with estimates of
the PATE based on the data and known features of the experimental outcome. Because the
ISTEPC scores in the Indiana example were aggregated into “Pass” and “Not Pass,” the subsequent bounds are derived using a binary Y. The potential outcomes Y(0), Y(1), therefore
share the same lower and upper bound, {0,1}, for all units in P and the expectations E(Y(1)),
E(Y(0)) become the probabilities P(Y(1) D 1), P(Y(0) D 1). These bounds can easily be
extended to bounded continuous Y, such as test scores, where the lower and upper bounds
of Y are used in place of {0,1}. Cases in which Y is bounded on one side but unbounded on
the other have been discussed in Manski (2009), though their focus is on experimental studies and not causal generalization.

Two Frameworks for Estimating Bounds on the PATE
For generalizability, a population data frame is required in order to estimate the propensity
score of being selected into the experimental sample for every school in P (Tipton et al.,
2014). The population data frame used in the Indiana CRT, sourced from the Common
Core of Data and the Indiana Department of Education, contains demographic information
on students and schools as well as test scores over several years. Because these data frames
enumerate all schools in P, they provide information on the sample counterfactuals in the
decomposition of (3) and (4). Whereas propensity score methods use the population data to
model the selection probability, we propose using the data frame to present two frameworks
for estimating bounds of the PATE.
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

653

We introduce the full-interval framework and the reduced-interval framework, which differ by the extent to which the population data frame is useful for providing information to
tighten the bounds. The full-interval framework is solely based on the experimental sample
data and makes no assumptions on the sample counterfactuals. Under this framework, the
only observable quantities are the realized potential outcomes E(Y(1)jW D 1, Z D 1) and
E(Y(0)jW D 0, Z D 1), while the unobservable treatment and sample counterfactuals are
replaced by known bounds on the outcome. The reduced-interval framework uses the
empirical evidence from the experimental sample and the population data frame (that is,
both the study data and population data from the Common Core of Data) to identify the
sample counterfactual E(Y(0)jW D 0, Z D 0). This counterfactual represents the expected
outcome under the control condition for schools that were assigned to the control group
(W D 0) and that were not in the experimental sample (Z D 0).
A rationale for the use of the reduced-interval framework lies in the idea that the
control condition in educational experiments may be a “business as usual” condition,
where control schools continue implementing existing curricula or programs. The
reduced-interval framework considers the distribution of potential outcomes among
schools not selected into the experiment (that is, Z D 0) to be identiﬁed by the population data frame if the control condition was “business as usual.” Because this was the
case for the Indiana CRT, we argue that the nonsample schools in the population were
similarly exposed to the control condition so that their potential outcomes under control are identiﬁed by the population data frame. We compare the widths and magnitudes of the estimated bounds of the PATE under these two frameworks to assess the
identifying power of the population data frame on the interval estimates. Note that the
reduced interval framework requires additional information, speciﬁcally on the probabilities P(W D 1jZ D 0), P(W D 0jZ D 0), and P(Z D 0). These three probabilities are
all functions of P(Z D 0), which is estimated by the proportion of the population not
selected into the experimental sample.

Bounds Under Treatment Randomization
Using the full- and reduced-interval frameworks, we begin by estimating the “worst case”
bounds for binary outcomes under treatment randomization using the data alone (Manski,
2009). From (3) and (4), the lower and upper bounds for the potential outcomes are derived
by replacing the unobservable sample counterfactuals with 0 and 1, respectively. No substitution is made for the treatment counterfactuals because the potential outcomes are statistically independent of the treatment indicator W.
Full-Interval Framework
Under this framework, the bounds associated with treatment w, where w 2 W, are:
EðY.w/Þ½EL ðY.w/Þ; EU ðY.w/Þ
EL ðY.w/Þ D EðY ðwÞ j W D w; Z D 1Þ  PðZ D 1Þ
EU ðY.w/Þ D EL ðY.w/Þ C ð1 ¡ PðZ D 1ÞÞ

654

W. CHAN

(5)

Reduced-Interval Framework
When the population data frame identiﬁes the sample counterfactual E(Y(0)jW D 0, Z D 0),
no replacements are made for this potential outcome. Because this sample counterfactual is
the expected outcome under control, the lower and upper bounds for E(Y(1)) remain the
same, but the bounds for E(Y(0)) become
EðY.0/Þ½EL ðY.0/Þ; EU ðY.0/Þ
EL ðY.0/Þ D EðY ð0Þ j W D 0; Z D 1Þ  PðZ D 1Þ C EðY ð0Þ j W D 0; Z D 0Þ  PðW D 0; Z D 0Þ
EU ðY.0/Þ D EL ðY.0/Þ C ð1 ¡ PðZ D 1Þ ¡ PðW D 0; Z D 0ÞÞ

(6)

Bounds on the PATE
For both frameworks, the lower and upper bound of the PATE are given by the differences
PATEL D EL ðYð1ÞÞ ¡ EU ðYð0ÞÞ
PATEU D EU ðYð1ÞÞ ¡ EL ðYð0ÞÞ

(7)

Under the given information structure, the bounds in (7) under treatment randomization
are sharp (see the online supplementary materials for proof). The width of the bound in (5)
is 2 P(Z D 0), which is strictly smaller than 1 when P(Z D 0) < 0.5. The width of the bound
using (6) changes to P(Z D 0) C P(W D 1, Z D 0) under the reduced-interval framework,
which is smaller than and at most equal to the width for the full-interval case. Because of the
smaller width, the population data frame has the potential to contribute identifying power
and tighten bounds.
Although the bounds in (5) and (6) provide the simplest (nonparametric) interval estimates of the PATE, they are rarely informative for identifying the sign of the treatment
effect. In studies such as the Indiana CRT, the sample is generalized to a population at least
20 times larger, so that the probability of not being in the sample, P(Z D 0), is likely to be
greater than 0.5, resulting in bounds that include zero (signifying an insigniﬁcant PATE).
Assumptions are thus necessary for generalizations from studies that exhibit similar sample
size ratios. In the next sections, we introduce the bounded sample variation and monotone
treatment response assumptions and compare the widths of the resulting bounds to that of
(5) and (6) when no assumptions are made.

Bounded Sample Variation and Treatment Randomization
Bounded variation assumptions were ﬁrst introduced in Manski (2015) and have been discussed in Manski and Pepper (2015) with applications to the impact of right-to-carry laws
on crime rates. Unlike sampling ignorability, bounded sample variation is an assumption
made on the expected sample counterfactuals, not treatment effects. In particular, this
assumption stipulates that the outcomes E(Y(1)jW D 1, Z D 0), E(Y(1)jW D 0, Z D 0),
E(Y(0)jW D 1, Z D 0), and E(Y(0)jW D 0, Z D 0) are “similar” to the observable, realized
outcomes among the sample schools. We quantify this similarity for randomized
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

655

experiments as follows:
j EðY ðwÞ j W D w; Z D 0Þ ¡ EðY ðwÞ j W D w; Z D 1Þ j  λ
j EðY ðwÞ j W 6¼ w; Z D 0Þ ¡ EðY ðwÞ j W 6¼ w; Z D 1Þ j  λ for w 2 W

(8)

Here, λ  [0,1] is a constant that represents the largest magnitude of the absolute difference (Manski, 2016). Note that the condition in (8) is applied to both sets of sample counterfactuals, and no substitution is made for the treatment counterfactuals under treatment
randomization. By design, bounded sample variation yields interval estimates of the PATE
where the width of the intervals is based on λ. For the Indiana CRT, if bounded sample variation holds, the proportion of “Pass” schools differs by at most a constant λ between the volunteer and non-volunteer schools for each respective treatment condition.
Under this assumption, for w 2 W, the bounds of the PATE are given by
EL ðY.w/Þ D EðY ðwÞ j W D w; Z D 1Þ  PðZ D 1Þ C .EðY ðwÞ j W D w; Z D 1/ ¡ λÞ  PðZ D 0Þ
EU ðY.w/Þ D EðY ðwÞ j W D w; Z D 1Þ  PðZ D 1Þ C .EðY ðwÞ j W D w; Z D 1/ C λÞ  PðZ D 0Þ
(9)
for the full-interval framework and
EL ðY.0/Þ D EðY ð0Þ j W D 0; Z D 1Þ  PðZ D 1Þ C EðY ð0Þ j W D 0; Z D 0Þ  p
C .EðY ð0Þ j W D 0; Z D 1/ ¡ λÞ  ð1 ¡ PðZ D 1Þ ¡ pÞ
EU ðY.0/Þ D EðY ð0Þ j W D 0; Z D 1Þ  PðZ D 1Þ C EðY ð0Þ j W D 0; Z D 0Þ  p
C .EðY ð0Þ j W D 0; Z D 1/ C λÞ  ð1 ¡ PðZ D 1Þ ¡ pÞ

(10)

for the reduced-interval framework, where p D P(W D 0, Z D 0). Like the previous
frameworks, the lower and upper bounds of the PATE are given by (7). Because Y is
binary, the constant λ lies in the interval [0,1], as it represents a difference in estimated
proportions. However, λ can be any positive constant when Y is continuous. Larger values of λ imply larger differences between the expected potential outcomes, thus widening the interval estimates and weakening the assumption further by allowing more
ﬂexibility in the absolute differences in potential outcomes. The bounds in (9) improve
upon the worst-case bounds in (7) for binary Y and are sharp (see online supplementary materials) if
EðY ð1Þ j W D 1; Z D 1Þ ¡ EðY ð0Þ j W D 0; Z D 1Þ C 2λ < 1 and
EðY ð1Þ j W D 1; Z D 1Þ ¡ EðY ð0Þ j W D 0; Z D 1Þ ¡ 2λ > ¡ 1
Because each sample counterfactual E(1)jW D 1, Z D 0), E(Y(1)jW D 0, Z D 0), E(Y(0)j
W D 1, Z D 0), and E(Y(0)jW D 0, Z D 0) is replaced by an expression based on λ rather
than zero (one) for the lower (upper) bound, the bounds under bounded sample variation
are narrower for certain ranges of λ.
Importantly, when l D 0 is chosen for both treatment conditions, the bounds reduce
to a single value of the PATE, and point estimation is recovered. Choosing λ to be zero
656

W. CHAN

implies that the expected proportion of “Pass” schools under treatment (or control) is
the same, regardless of whether the school was selected into the study. This choice of λ
also implies that the distributions of the expected outcome among schools in the sample and schools not in the sample are equivalent for each treatment condition. Note
that bounded sample variation and sampling ignorability are not nested. Bounded sample variation allows the potential outcomes among sample and nonsample schools to
vary under the assumption that the outcomes are similar in magnitude but not necessarily equal.
It is helpful to conceptualize the plausibility of this assumption in the context of matching. When an experimental sample is matched to a population P in causal generalization,
the goal is to achieve balance among observable covariates so that the resulting differences
in distributions is minimized (see, e.g., Hansen [2004] for examples of matching). Here, balance is quantiﬁed as attaining the smallest standardized mean difference among covariates
between the two groups. Conceivably, the difference in expected potential outcomes is
smaller with matched samples if the potential outcomes Y(1),Y(0) are a function of the covariates that are balanced between the groups. The plausibility of bounded sample variation
lies in the assumption that there is sufﬁcient overlap between the distributions of potential
outcomes among sample and nonsample schools to facilitate the derivation of informative
bounds.
Choice of l
Bounded sample variation assumptions require the researcher to choose λ. In this section, we provide several data-based suggestions for this parameter. Previous work on
bounded variation assumptions estimate λ based on prior outcome data (Manski &
Pepper, 2015). However, estimates of λ from prior data are difﬁcult in our example as
it represents the difference between the volunteer and non-volunteer schools’ outcomes
in the Indiana CRT, which is speciﬁc to this study. Because bounded sample variation
is related to the goals of matching methods, one natural choice is the absolute standardized mean difference (ASMD) of observable covariates. In particular, let l D
jmP ¡ X S j/s P , where X is a pretreatment covariate, mP is the mean covariate value for
schools in the population, X S is the mean covariate value for the sample schools, and s P is
the standard deviation of X across all schools in the population. This choice of parameter
reﬂects the assumption that λ effectively measures the degree to which the sample and population is balanced on a speciﬁc covariate. As a result, larger values of the ASMD suggest
larger differences between the sample and population distributions of the covariate, which
is then reﬂected in wider intervals. To give an example, suppose Y is a test score outcome.
One possible choice for X is a pretest score that is strongly correlated with Y. The interval
estimate of the population parameter is then determined by the extent to which the sample
and nonsample schools are balanced on the pretest scores X. Alternatively, because the
ASMD is typically estimated using multiple covariates, other choices of λ are the average
ASMD of several covariates or the maximum of the ASMD to use as a conservative estimate. Using the ASMD for λ allows researchers to base the parameter on empirically
derived balance statistics.
A caveat of using the ASMD for λ is that values greater than one are possible. In these
cases, the bounds under bounded sample variation and treatment randomization may
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

657

not improve upon the worst-case bounds so that the direction of the PATE is again
unidentiﬁed. However, it is important to compare the bounds from different choices of λ
when the bounds are tighter than those under the no-assumptions case. Comparing the
bounds from different values of λ to the case when l D 0 allows researchers to assess
how balance between the sample and population changes the inferences from the interval
estimates.
Aside from the ASMD, another possible choice for λ is based on the actual variation of the
realized potential outcomes in the experimental study. In particular, let λ D 2x(Var(Yj Z D
1)) where λ is analogous to the margin of error in the construction of conﬁdence intervals for
normally distributed data. When Y is binary, the variance is a function of the proportion of
“successes” in the sample (that is, the proportion of “Pass” schools). Choosing λ in this way
offers a conservative limit on the difference in E(Y) by setting the difference to be no more than
two standard deviations from the empirical distribution of outcomes. Note that this choice of λ
is based on the realized potential outcomes in the sample. If outcome data is available for all
schools in the population, λ can be estimated using information from all of the schools. Alternatively, if λ is based on the sample alone and the variance estimates differ among the treatment
groups, a conservative choice of λ is the maximum of the variance estimates.
Importantly, bounded sample variation is not necessarily validated using these
choices of λ. We provide these suggestions as a starting point for applications of this
assumption if the assumption is plausible in a given study. The suggestions for λ here
are not exhaustive and additional choices based on the balance between propensity
score logits (logit(s(X)) D log(s(X)/(1 ¡ s(X)))) offer other options when invoking
bounded sample variation assumptions.

Monotone Treatment Response
In some cases, the researcher may have prior knowledge on an intervention and be conﬁdent
in its positive effect on outcomes of interest. If the principal investigators of the Indiana CRT
were conﬁdent in and believed that the benchmark assessment system improved student
outcomes, this would lead to a different assumption with a different set of bounds. Under
monotone treatment response (MTR), the response variable Y is said to vary monotonically
if, given two treatments w, w 0 2 W, the following condition holds:
w  w 0 ! Y ðwÞ  Y ðw 0 Þ
We assume that this condition holds for all schools in the population (that is, those in the
sample (Z D 1) and not in the sample (Z D 0)). For the Indiana CRT, let w D 1 and w’ D 0
so that under MTR, the proportion of “Pass” schools in the treatment group is assumed to
be at least as large as the proportion of “Pass” schools in the control group. MTR is related
to the idea that the benchmark assessment system at least “does no harm” and at worst, that
the expected outcome under treatment is not signiﬁcantly different from the expected outcome under “business as usual.”
Like bounded sample variation, MTR differs from sampling ignorability because it is not
an assumption on the treatment effect. Instead, MTR is an assumption made on the response
function Y. MTR differs from both bounded sample variation and sampling ignorability on
two important aspects. First, the treatment indicator W now denotes an ordered set of
658

W. CHAN

treatments. Second, the bounds under MTR are derived using realized values of Y across all
schools in the study. The original MTR framework proposed in Manski (1997) assumed that
outcomes at different levels of the treatment were observable, which contributed identifying
power even if some levels of the treatment were not realized. A common application of this
framework is in labor economics, where each individual’s wage function in the labor market
is a monotonic function of years of schooling (Manski, 2009). The sharpness of the bounds
under MTR is a consequence of the monotonic nature of the response function. Although
the focus here is on weakly increasing response functions Y, the results can easily be generalized to weakly decreasing functions.
If MTR holds, this assumption, combined with SUTVA, strongly ignorable treatment
assignment, and perfect compliance, yields a new set of bounds of the PATE with analogous
extensions to the bounds of the prior frameworks. If Y is weakly increasing in W, the lower
bound of the PATE is zero by design. For binary Y and W, the upper bound is a function of
the proportion of “Pass” and “Not Pass” schools in the sample and is given by
PATEU D PðY D 0 j W D 0; Z D 1Þ  PðW D 0; Z D 1Þ C PðY D 1 j W D 1; Z D 1Þ


PðW D 1; Z D 1Þ C PðY D 0 j W D 0; Z D 0Þ  PðW D 0; Z D 0Þ

C PðY D 1 j W D 1; Z D 0Þ  PðW D 1; Z D 0Þ

(11)

The bounds [0, PATEU] are tight under the MTR assumption where Y is a weakly
increasing function of the treatment W for all schools in the population (see the online
supplementary materials). The upper bound is the sum of two components. If Y is monotone in treatment, the largest feasible upper bound is the sum of the upper bound for Y(1)
and the lower bound for Y(0). This is given in the ﬁrst term, P(Y D 0jW D 0, Z D 1) 
P(W D 0, Z D 1) C P(Y D 1jW D 1, Z D 1)  P(W D 1, Z D 1), which is the sum of the
proportion of “Not Pass” schools (Y D 0) among the sample schools assigned to control
(W D 0) and the proportion of “Pass” schools (Y D 1) among the sample schools assigned
to treatment (W D 1). The second term, P(Y D 0jW D 0, Z D 0)  P(W D 0, Z D 0) C
P(Y D 1jW D 1, Z D 0)  P(W D 1, Z D 0), is the sum of the analogous proportions
among the nonsample schools (Z D 0).
Because MTR is based on realized values Y, additional consideration must be taken for
the upper bound since it is a function of schools not selected into the experimental sample.
If the population data frame contributes identifying power, as proposed under the previous
reduced-interval framework, the term P(Y D 0jW D 0, Z D 0) is identiﬁed using the empirical evidence. However, the proportion P(Y D 1jW D 1, Z D 0) is still an unobservable sample
counterfactual, and the only information that contributes identifying power are the known
bounds {0,1} for the outcome Y. Because the upper bound depends on this sample counterfactual, an interval of values for PATEU can be derived by substituting 0 (1) for the minimum (maximum) upper PATE bound. Although we provide both bounds for PATEU in the
results section, we focus on the minimum of the PATEU as it is derived based on the data
alone using the realized values Y. This lower bound of the PATEU is a “best case” bound
because it represents the smallest feasible value of PATEU under the MTR framework. Thus,
the smallest PATEU is derived by substituting 0 for the unobserved sample counterfactuals
and the largest PATEU is derived by substituting the value 1. Like the full- and reducedinterval cases, two MTR bounds, denoted as the “sample MTR bound” and the “population
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

659

MTR bound,” can be written:
Sample MTR Bound
PATEU D PðY D 0 j W D 0; Z D 1Þ  PðW D 0; Z D 1Þ C PðY D 1 j W D 1; Z D 1Þ


PðW D 1; Z D 1Þ

Population MTR Bound
PATEU D PðY D 0 j W D 0; Z D 1Þ




PðW D 0; Z D 1Þ C PðY D 1 j W D 1; Z D 1Þ

PðW D 1; Z D 1Þ C PðY D 0 j W D 0; Z D 0Þ



PðW D 0; Z D 0Þ

(12)

The bounds in (12) differ by the probability P(Y D 0jW D 0, Z D 0), which is identiﬁed by
the population data frame in the “population MTR bound.” As a result, if MTR holds and
the treatment is believed to at least “do no harm,” the interval estimate of the PATE shrinks
to lie to one side of zero.

Bounds by Propensity Score Stratum
The bounds estimated thus far are based on the entire experimental sample and population
data frame (the latter for the reduced-interval and population MTR cases). In generalizations
with nonrandom samples, a primary goal in the study is to match the sample and population. Subclassiﬁcation is a common matching method in which the population is partitioned
into smaller subclasses or strata using quintiles of the propensity score distribution (Tipton,
2013). Figure 1 shows the distribution of propensity score logits (logit(s(X)) D log(s(X)/1-s
(X))) for the Indiana CRT. The experimental sample permitted only three equally sized

Figure 1. Distribution of propensity score logits for Indiana CRT. S1: Stratum 1; S2: Stratum 2; S3: Stratum 3.
660

W. CHAN

strata. These strata represent coarse matches between the volunteer and non-volunteer
schools based on the covariates used to estimate the propensity scores. As a statistical tool,
subclassiﬁcation shares the same advantages with stratiﬁcation methods by improving the
precision of estimates when schools in the same stratum are more similar in the matched
covariates than between strata.
Because the bounds are estimated nonparametrically, they can also be computed for subclasses of the data to derive stratum-speciﬁc interval estimates. In our ﬁnal framework, we
propose a combined approach where the previous no-assumptions, bounded sample variation
with treatment randomization and MTR frameworks are applied with subclassiﬁcation to
derive stratum-speciﬁc interval estimates of the PATE. Note that sampling ignorability is not
invoked here because the extended application is based on subclassiﬁcation as a matching
method. Given k propensity score strata, the bounds [PATEjL, PATEjU] are now estimated
using the empirical distribution (Yj, Wj, Zj), for j D 1, …, k. Because the outcome Y is bounded
by the same parameters in each stratum, the interval estimates have the same form as the
bounds based on the original sample. Furthermore, the nonparametric method of deriving
the bounds offers a ﬂexibility that can easily be extended to any number of subclasses.

Application to the Indiana CRT
We now apply the assumptions frameworks to the Indiana CRT. Tables 1 and 2 provide
the bounds under the two alternative assumptions and the no-assumptions cases using the
experimental and stratiﬁed Indiana CRT samples, respectively. The PATE is deﬁned as
Table 1. Bounds on PATE for Indiana CRT.
ELA
Identifying assumptions

Full-interval

Reduced-interval

Treatment randomization
Bounded sample variation
λ D 0.3
λ D 0.5

[¡0.93, 0.96]

[¡0.89, 0.54]

[¡0.31, 0.83]
[¡0.69, 0.99]

[¡0.43, 0.43]
[¡0.71, 0.71]

Monotone treatment response

Sample MTR

Population MTR

y

[0.00, 0.02]
[0.00, 0.97]yy

[0.00, 0.07]y
[0.00, 0.54]yy

Math
Identifying assumptions

Full-interval

Reduced-interval

Treatment randomization
Bounded sample variation
λ D 0.1
λ D 0.6

[¡0.93, 0.96]

[¡0.87, 0.55]

[0.02, 0.40]
[¡0.93, 0.99]

[¡0.18, 0.11]
[¡0.89, 0.82]

Monotone treatment response

Sample MTR

Population MTR

y

[0.00, 0.02]
[0.00, 0.97]yy

[0.00, 0.09]y
[0.00, 0.56]yy


y

Reduced-Interval bounds derived using P(W D 0jZ D 0) D 0.5.
Smallest value of PATEU is used.
Largest value of PATEU is used.

yy

PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

661

Table 2. Bounds on PATE for Indiana CRT by stratum.
Randomized treatment

Full interval

Reduced interval
Sample MTR

Population
MTR

S1 [¡0.90, 0.95] [¡0.86, 0.52] [¡0.22, 0.89] [¡0.59, 0.99] [¡0.36, 0.48] [¡0.63, 0.75] [0.00, 0.03]
[0.00, 0.96]
S2 [¡0.94, 0.95] [¡0.92, 0.50] [¡0.50, 0.63] [¡0.88, 0.99] [¡0.61, 0.24] [¡0.89, 0.53] [0.00, 0.02]
[0.00, 0.96]
S3 [¡0.99, 0.99] [¡0.89, 0.60] [¡0.10, 0.99] [¡0.49, 0.99] [¡0.34, 0.55] [¡0.64, 0.85] [0.00, 0.004]
[0.00, 0.996]
Math S1 [¡0.90, 0.95] [¡0.85, 0.53] [0.12, 0.49] [¡0.80, 0.99] [¡0.11, 0.17] [¡0.80, 0.86] [0.00, 0.03]
[0.00, 0.96]
S2 [¡0.95, 0.94] [¡0.90, 0.51] [¡0.25, 0.13] [¡0.99, 0.99] [¡0.40, 0.11] [¡0.99, 0.60] [0.00, 0.01]
[0.00, 0.96]
S3 [¡0.99, 0.99] [¡0.85, 0.64] [0.30, 0.70] [¡0.59, 0.99] [¡0.006, 0.29] [¡0.75, 0.99] [0.00, 0.004]
[0.00, 0.996]

[0.00, 0.07]
[0.00, 0.53]
[0.00, 0.04]
[0.00, 0.51]
[0.00, 0.11]
[0.00, 0.60]
[0.00, 0.08]
[0.00, 0.54]
[0.00, 0.06]
[0.00, 0.53]
[0.00, 0.14]
[0.00, 0.64]

Full interval
ELA

Reduced
interval

Monotone treatment
response

Bounded sample variation

l D l1z

l D l2z

l D l1z

l D l2z



Reduced Interval bounds derived using P(W D 0jZ D 0) D 0.5.
For ELA, λ1 D 0.3 and λ2 D 0.5. For Math, λ1 D 0.1 and λ2 D 0.6.
The top MTR bounds are based on the smallest value of PATEU while the bottom MTR bounds are based on the largest value of
PATEU.
z

P(Y(1) D 1) – P(Y(0) D 1), the difference between the proportion of “Pass” schools if all
schools would have implemented the assessment system and the proportion of “Pass”
schools if all schools would not have implemented the assessment system. To illustrate this
application, fourth-grade scores were used so that the bounds were estimated using
N D 1,029 fourth-grade-serving schools, a subset of the original 1,514 K–8 schools. From
the experimental sample, the conditional probabilities of treatment assignment were estimated as P(W D 1jZ D 1) D 34/56 D 0.61 and P(W D 0jZ D 1) D 22/56 D 0.39. The
probability of selection into sample is given by P(Z D 1) D 56/1029  0:05 and P(Z D 0)
D 1 – P(Z D 1). The reduced-interval framework requires additional information to estimate
P(W D 0, Z D 0) D P(W D 0jZ D 0)P(Z D 0). For the purpose of illustrating the bounds
and for comparing the assumptions, we let P(W D 0jZ D 0) D 0.5, signifying that schools
not in the sample would be randomly assigned to a treatment condition if they participated
in the study. Although the probability of receiving either treatment condition was not 0.5 in
the actual experimental sample, we use 0.5 for the nonsample schools as a plausible value for
this probability in randomized studies with two treatment groups.
A natural starting place for estimating the PATE is to consider the inferences when no
assumptions beyond SUTVA, strongly ignorable treatment assignment, and perfect compliance are made. The ﬁrst set of bounds in Table 1 shows the interval estimates under “Treatment Randomization.” Here, we estimate the PATE solely using the bounded outcome Y
and the randomized nature of the treatment assignment. As shown, the bounds are distinctly
uninformative, with the interval nearly spanning the [¡1, 1] range for both ELA and math
so that the sign of the PATE cannot be identiﬁed. Without any additional information, we
cannot determine if the benchmark assessment system had an impact on student achievement. Under the reduced-interval case, incorporating the population data frame narrows the
interval estimate. For ELA, the interval shrinks from [¡0.93, 0.96] under the full-interval
framework to [¡0.89, 0.54] under the reduced-interval framework, a 25% shrinkage in
662

W. CHAN

width, so that the expected difference in proportion of “Pass” schools is now between ¡0.89
and 0.54 in the latter framework. Analogous results are seen for math. Because the probability of sample selection is small, with P(Z D 1) D 0.05, the intervals include zero.
Some assumptions are thus needed to determine if the benchmark assessment system signiﬁcantly impacted changes in achievement scores for the study and whether the results generalize to the population of Indiana schools. An important question is, which assumption (if
any) is plausible for the Indiana CRT? Sampling ignorability requires that s(X) contain all
covariates that moderate treatment effects and affect sample selection. In addition, every
school in the population must have a comparable school in the CRT example. Although the
ﬁrst part of the assumption is difﬁcult to check, Figure 1 suggests that the second part of the
assumption may not be met. In particular, Stratum 3 illustrates that there are some population schools whose sampling propensity scores lie beyond the range of the propensity scores
in the sample. This suggests that there exist some schools in the population that may not
have comparable schools in the sample and that the plausibility of sampling ignorability is
suspect.
We then turn to the ﬁrst of the alternative assumptions, bounded sample variation, and
discuss its plausibility in the Indiana CRT and its trade-offs. Bounded sample variation
assumes that the average difference in expected outcomes between the sample and population is bounded. Determining this assumption’s plausibility becomes a question of how well
the sample matches the population and assuming that the similarity between sample and
population translates into small differences in expected potential outcomes. This similarity
has been studied in the literature using balance statistics among covariates (as mentioned in
the “Choice of λ” section), the distribution of propensity scores (Stuart et al., 2011), through
a generalizability index based on the distribution of propensity scores (Tipton, 2014) and by
comparing these measures of similarity to what one would expect in probability samples
(Tipton et al., 2016). How similar is the CRT sample to the Indiana population? We refer to
the results in Tipton et al. (2016) because measures of similarity between the sample and
population were calculated for the same Indiana CRT. For this example, Tipton et al. (2016)
compared the balance statistics of the sample with simulated random samples and found
that, among the 14 covariates studied, only ﬁve had unusually large ASMD when compared
to a random sample. Comparisons of the overlap in the propensity score distribution and
the generalizability index illustrated that the Indiana CRT sample was not very different
from a random sample of similar size. From these assessments, the Indiana CRT sample was
considered to be “like” a random sample from the population despite the volunteer nature
of the sample selection.
Because the CRT sample was considered “like” a probability sample, it potentially shares
the advantages of probability sampling. In particular, as a probability sample, we would not
expect systematic differences in balance among the covariates, both observable and unobservable. Bounded sample variation is therefore a plausible assumption for the Indiana CRT
given that, being close to a probability sample, differences between the expected outcomes
among the sample and the population would be small on average.
Assuming bounded sample variation, we estimate the bounds for the PATE under the
condition that the differences in expected outcomes are small and bounded. We choose two
values of λ corresponding to the variance of Y and the ASMD of a pretest covariate, as suggested in the “Choice of l” section. These choices give us the bounds in the second rows for
each subject in Table 1. For ELA, λ D 0.3 corresponds to the variance of Y and λ D 0.5
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

663

corresponds to the ASMD of the pretest scores. The values λ D 0.1 and 0.6 were chosen similarly for math. For both subjects, the smaller values of λ represent a smaller difference in the
expected potential outcomes between the volunteer and non-volunteer schools. When λ D
0.3, the expected difference in proportion of “Pass” schools ranges from ¡0.31 to 0.83 for
ELA, a much tighter interval compared to the treatment randomization case. For math, the
smaller λ value of 0.1 actually allows us to identify the sign of the PATE, with the interval
estimate ranging from 0.02 to 0.40 so that a positive treatment effect is estimated. This interval suggests that the difference in expected proportions of “Pass” schools between the treatment and control schools ranges from 0.02 to 0.40 so that the benchmark assessment system
appears to have a positive impact. These intervals are signiﬁcantly widened when the value
of λ increases to 0.5 and 0.6 for ELA and math, respectively. Since λ affects both the lower
and upper bounds of P(Y(1)) and P(Y(0)), the impact on the bounds on the PATE is more
pronounced after taking the difference. For the reduced-interval framework, the upper
bound PATEU is much smaller than that of the full-interval framework since the difference
P(Y(1))L – P(Y(0))U is now taken with a larger P(Y(0))U. With exception to math with λ D
0.1, the interval estimates under bounded sample variation still suggest an insigniﬁcant
PATE.
Bounded sample variation offers a ﬂexibility in estimating the PATE where the difference
in expected outcomes between the volunteer and non-volunteer schools is not restricted to
be zero. This assumption offers more credibility to inferences if researchers determine that
the sample is similar enough to the population or similar enough to a probability sample
that the difference in expected outcomes is small. However, like sampling ignorability,
bounded sample variation cannot be veriﬁed empirically and its application relies on the
choice of l. Although prior data has been used in studies such as Manski and Pepper (2015)
to empirically choose λ to be consistent with the assumption, it is difﬁcult to use prior data
in studies such as the Indiana CRT in which the study was run for one year. Furthermore,
determining the plausibility of this assumption with balance statistics is based on the observable covariates and, consequently, has similar concerns with sampling ignorability.
Is MTR a plausible assumption for the Indiana CRT? If MTR holds, the benchmark
assessment system improves student outcomes and at worst, has no impact. For the Indiana
CRT, we argue that MTR is also plausible but more appropriate than bounded sample variation. Although MTR, too, cannot be empirically validated, its plausibility can be suggested
from the logic model for the intervention and even from evidence from pilot studies. In our
empirical example, the Indiana State Board of Education planned to use the assessment system to “encourage the advanced and gifted child, drive progress in the student who is ready,
and accelerate progress for the student whose learning reﬂects gaps in preparation and readiness” (Indiana State Board of Education, 2006, pp. 11–12). Like other studies in the use of
interim assessments, the system used in the Indiana CRT was designed to identify areas of
improvement and use the information to implement instructional strategies to bring about
improvement in student outcomes (Konstantopoulos et al., 2013). Given that the benchmark
assessment system was conceived on the idea that the interim assessments would be instrumental in bringing about improvement in academic outcomes, MTR is a plausible assumption. Because the literature on the use of interim assessments supports the belief that these
interventions at least do no harm, we argue that MTR is more appropriate than bounded
sample variation in this example. During the pilot year of the CRT, Konstantopoulos et al.
664

W. CHAN

(2013) found positive, though insigniﬁcant, treatment effects for a majority of the grades
among the experimental schools, which lends additional support for MTR.
Under MTR, the expected difference in proportion of “Pass” schools is given in the third
rows for ELA and math in Table 1. Note that all the MTR bounds lie to one side of zero by
assumption. The upper bounds using the largest value of PATEU are largely similar to those
under treatment randomization and illustrate that in the absence of information on the sample counterfactuals, the range of values for the PATE is wide. With the MTR bounds using
the smallest value of PATEU, the PATE is 0.02 based on the experimental data for both subjects. This upper bound (based on the smallest PATEU) increases to 0.07 and 0.09 for ELA
and math, respectively, when it includes P(Y(0) D 0jW D 0, Z D 0). However, because the
difference between 0.02 and 0.07 for ELA is small, P(Y(0) D 0jW D 0, Z D 0) is small, which
implies that the proportion of “Not Pass” schools in the population is small. Importantly,
because the PATE is a function of P(Z D 1), if this proportion is small, the bound PATEU
will also be small. Although the MTR bounds include zero by design, the magnitude of the
smallest PATEU for both subjects suggests that, using the realized outcomes, large values of
the PATE can be ruled out even if small insigniﬁcant treatment effects cannot be excluded.
MTR would be more appropriate for interventions that are theoretically intended to produce positive impacts. However, there are two trade-offs. First, unlike bounded sample variation, the bounds under MTR contain zero by assumption so that an insigniﬁcant PATE is
never ruled out. For this reason, the extent to which the bounds are informative involves
comparing the upper bound PATEU. Second, for generalization problems in particular, the
bounds are still a function of the unobservable sample counterfactuals so that additional
assumptions are needed to tighten the range of values for PATEU.

Indiana CRT Bounds by Stratum
Thus far, the interval estimates under each framework suggest an insigniﬁcant treatment
effect so that the difference in proportion of “Pass” schools is not signiﬁcantly different
between treatment and control schools. We now consider how these interval estimates compare under the two alternative assumptions when they are estimated in each of the propensity score strata in which the sample and nonsample schools are matched on the observable
covariates. Combining the three assumptions frameworks with subclassiﬁcation provides a
way of observing differences in inferences among schools in individual subclasses. We estimated the propensity scores in this example using a logistic regression model based on covariates from the Indiana population data frame, which included continuous variables such as
pretest measures and binary measures such as Title I status. Table 2 provides the bounds for
the PATE under the three frameworks of treatment randomization, bounded sample variation, and MTR for ELA and math. Note that the stratum-speciﬁc sample sizes are smaller
under this approach, with Stratum 3 containing only two experimental schools, one treatment and one control.
Because subclassiﬁcation creates matched subgroups of schools under this “fusion”
approach, the improved matches strengthen the plausibility of bounded sample variation.
Within each subclass, the average difference in observable covariates between the sample
and nonsample is minimized. If this translates into small differences in expected outcomes,
bounded sample variation may yield bounds that are potentially more informative than
those based on the sample as a whole.
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

665

Table 3. Point estimates of the PATE for Indiana CRT.
Method
ELA

No weighting
IPW
Subclassiﬁcation
No weighting
IPW
Subclassiﬁcation

Math

Estimate

Standard error

0.048
0.128
0.056
0.095
0.158
0.097

0.038
0.116
0.056
0.051
0.117
0.063

From Table 2, the stratum-speciﬁc bounds under treatment randomization are largely
similar to those based on the entire experimental sample and again are uninformative for
both subjects. The reduced-interval bounds under treatment randomization have similar
widths as under the original sample, but slight differences can be seen among the strata. For
example, the difference in proportion of “Pass” schools for ELA and math in Stratum 1
ranges from approximately ¡0.85 to 0.53 under the reduced-interval framework, but the
lower bound decreases to about ¡0.90 in Stratum 2.
The differences among bounds are seen more distinctly with bounded sample variation
and MTR. Using the same bounding parameters as with the original sample, the interval
estimates under bounded sample variation suggest larger differences among the strata. In
math, for example, the sign of the PATE is identiﬁed in Stratum 1 when λ D 0.1 with an
interval estimate of [0.12, 0.49]. This implies that the expected difference in proportion of
“Pass” schools among the treatment and control schools is between 0.12 and 0.49 for schools
in this stratum. However, the bounds in Stratum 2 imply an insigniﬁcant PATE, which illustrate potential differences in inferences among the strata.
As a comparison, Table 3 gives the point estimates of the PATE under “no weighting,”
inverse propensity weighting (IPW), and subclassiﬁcation for ELA and math. These point
estimates are given assuming that all of the conditions of sampling ignorability hold. The
“no weighting” case refers to the estimate of the SATE, which is likely biased as an estimate
for the PATE in the absence of probability sampling. The point estimates for ELA and math
are all insigniﬁcant, a result that is largely consistent with the bounds provided. Whereas the
interval estimates for math show a positive PATE under λ D 0.1 with bounded sample variation, the lower bound of 0.02 suggests that small insigniﬁcant treatment effects are possible.
With exception to IPW, the point estimates under “no weighting” and subclassiﬁcation are
similar in magnitude to the smallest PATEU under MTR, further supporting insigniﬁcant
results.
In the original analysis, Konstantopoulos et al. (2013) found signiﬁcant treatment effects
for fourth-grade ELA using a two-level hierarchical linear model with covariates. Using standardized continuous ISTEPC scores, a signiﬁcant PATE of 0.135 (0.057 standard error) was
found based on the experimental sample with a model that included school- and student-level
covariates. Importantly, the PATE using the model with treatment alone was not signiﬁcant
(PATE D 0.087 with 0.111 standard error). Although this point estimate is not directly comparable to the bounds based on binary outcomes provided here, it is important to note that
the signiﬁcance of the estimate and the resulting inferences depended on the choice of model.
Several items should be noted following these comparisons. First, assumptions play a signiﬁcant role in determining the extent to which estimates are informative in generalization
studies with nonprobability samples. Bounded sample variation alone tightened the interval
666

W. CHAN

estimate of [¡0.93, 0.96] to [¡0.31, 0.83] under the full interval framework for ELA. Second,
differences in assumptions are reﬂected in the various magnitudes of the interval estimates,
which can lead to different inferences. If treatment is assumed to have a positive impact
(under MTR), the smallest PATEU is clearly different from the lower bounds under bounded
sample variation where the differences in expected outcomes are allowed to be positive or
negative. Lastly, different methods using the same assumptions may lead to different estimates of the parameter. Table 3 illustrates that the three methods of estimating the PATE
give slightly different values even though sampling ignorability assumptions are made. Additionally, although an insigniﬁcant PATE is the consistent result among the interval estimates
and point estimates in our example, it is important to note that this may not be true in other
studies. From our comparisons, we highlight the need for thoughtful consideration of
assumptions, their plausibility, and their implications for inference.

Discussion and Conclusion
This paper compares sampling ignorability with two alternative assumptions in the context of
causal generalization from nonprobability samples. Sampling ignorability is one of several key
assumptions needed to point identify the PATE, but as our empirical example suggests, situations in which it is violated may occur in practice. Our comparison of sampling ignorability with
bounded sample variation and MTR illustrates that data alone are not sufﬁciently informative of
the PATE and that there are trade-offs to invoking different assumptions. For example, bounded
sample variation may be plausible based on measures of balance between the sample and population, but the assumption involves choosing a value for λ, which can be challenging.
A common difﬁculty among each of the assumptions (sampling ignorability, bounded
sample variation, and MTR) is that they are all untestable by the data. With MTR, the data
and the logic model for the intervention can at most suggest its plausibility but cannot provide validation. The researcher is then left to decide which assumptions are plausible based
on which seem most credible and consistent with the data at hand and with the theoretically
proposed impact. Our focus on MTR and its plausibility in the Indiana CRT, for example,
was motivated by the Indiana State Board of Education’s proposal to use interim assessments
to improve academic outcomes. Prior information and theoretical evidence of the impact of
an intervention are important in deciding the plausibility of assumptions, a task that would
otherwise be difﬁcult in practice.
Generalizations with nonprobability samples inevitably involve a discussion of the extent
to which a self-selected sample differs from the population. The role that all assumptions in
generalization problems plays is to make conjectures about this difference. The literature on
the performance of estimators when assumptions do not hold is extensive and has been
explored in simulation (see Kern, Stuart, Hill, & Green, 2016). However, it is much more
challenging to design studies to assess whether sampling ignorability or bounded sample variation hold in practice. The comparison presented in this paper combines two perspectives of
inference by assessing the differences between point identifying and partially identifying
assumptions. Although interval estimates do not substitute for the point estimates used to
inform policy, we present their application as a way to highlight the importance of considering the plausibility and credibility of different assumptions, particularly when they lead to
potentially different inferences. The assumptions explored in this article are only a subset of
the alternatives to sampling ignorability. Future work should continue to explore other
PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

667

alternative assumptions, some of which may impose different constraints than sampling
ignorability, and assess their plausibility and their effect on inferences. Additionally, assumptions that specify different distributional conditions on the treatment effects should also be
explored, possibly through simulation.
Though not addressed here, it is important to note that the interval estimates provided in this
article do not incorporate standard errors. Previous research has explored asymptotically valid
conﬁdence intervals (Imbens & Manski, 2004) and estimation methods for intersection bounds
(interval estimates) that provide asymptotically valid inferences (Chernozhukov, Lee, & Rosen,
2013). However, the focus of these studies was on the theoretical development of large sample
inferential methods and not necessarily on sampling error. Future research should explore
methods of incorporating standard errors for partially identiﬁed parameters, particularly when
interval estimates are used for inference. This incorporation will be important in ﬁelds such as
meta-analysis where knowledge of sampling errors affects the synthesis of results.

ARTICLE HISTORY
Received 30 March 2016
Revised 11 December 2016
Accepted 12 December 2016

References
Chernozhukov, V., Lee, S., & Rosen, A. M. (2013). Intersection bounds: Estimation and inference.
Econometrica, 81(2), 667–737.
Dawid, A. P. (2000). Causal inference without counterfactuals. Journal of the American Statistical
Association, 95(450), 407–424.
Greenburg, D., & Shroder, M. (2004). The digest of social experiments (3rd ed.) Washington, DC:
Urban Institute Press.
Greenland, S., Pearl, J., & Robins, J. M. (1999). Causal diagrams for epidemiologic research. Epidemiology, 10, 37–48.
Hansen, B. B. (2004). Full matching in an observational study of coaching for the SAT. Journal of the
American Statistical Association, 99(467), 609–618.
Holland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association,
81(396), 945–960.
Imai, K., King, G., & Stuart, E. A. (2008). Misunderstandings between experimentalists and observationalists about causal inference. Journal of the Royal Statistical Society: Series A (Statistics in Society), 171(2), 481–502.
Imbens, G. W., & Manski, C. F. (2004). Conﬁdence intervals for partially identiﬁed parameters. Econometrica, 72(6), 1845–1857.
Indiana State Board of Education. (2006). A long-term assessment plan for Indiana: Driving student
learning. Indianapolis, IN: Author.
Keiding, N., & Louis, T. A. (2016). Perils and potentials of selfselected entry to epidemiological studies and surveys. Journal of the Royal Statistical Society: Series A (Statistics in Society), 179(2), 319–
376.
Kern, H. L., Stuart, E. A., Hill, J., & Green, D. P. (2016). Assessing methods for generalizing experimental impact estimates to target populations. Journal of Research on Educational Effectiveness, 9
(1), 103–127.
Konstantopoulos, S., Miller, S. R., & van der Ploeg, A. (2013). The impact of Indiana’s system of
interim assessments on mathematics and reading achievement. Educational Evaluation and Policy
Analysis, 35(4), 481–499.
668

W. CHAN

Kruskal, W., & Mosteller, F. (1980). Representative sampling, IV: The history of the concept in statistics, 1895–1939. International Statistical Review/Revue Internationale de Statistique, 48(2), 169–
195.
Little, R. J. (1986). Survey nonresponse adjustments. International Statistical Review, 54(1), 3.
Manski, C. F. (1990). Nonparametric bounds on treatment effects. The American Economic Review, 80
(2), 319–323.
Manski, C. F. (1997). Monotone treatment response. Econometrica: Journal of the Econometric Society,
65(6), 1311–1334.
Manski, C. F. (2009). Identiﬁcation for prediction and decision. Cambridge, MA: Harvard University
Press.
Manski, C. F. (2016). Credible interval estimates for ofﬁcial statistics with survey nonresponse. Journal
of Econometrics, 191(2), 293–301.
Manski, C. F., & Pepper, J. V. (2015). How do right-to-carry laws affect crime rates? Coping with ambiguity using bounded-variation assumptions (No. w21701). Cambridge, MA: National Bureau of
Economic Research.
Olsen, R. B., Orr, L. L., Bell, S. H., & Stuart, E. A. (2013). External validity in policy evaluations that
choose sites purposively. Journal of Policy Analysis and Management, 32(1), 107–121.
O’Muircheartaigh, C., & Hedges, L. V. (2014). Generalizing from unrepresentative experiments: A
stratiﬁed propensity score approach. Journal of the Royal Statistical Society: Series C (Applied Statistics), 63(2), 195–210.
Rosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1), 41–55.
Rosenbaum, P. R., & Rubin, D. B. (1984). Reducing bias in observational studies using subclassiﬁcation on the propensity score. Journal of the American Statistical Association, 79(387), 516–524.
Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology, 66(5), 688.
Rubin, D. B. (1977). Assignment to treatment group on the basis of a covariate. Journal of Educational
and Behavioral Statistics, 2(1), 1–26.
Rubin, D. B. (1978). Bayesian inference for causal effects: The role of randomization. The Annals of
Statistics, 6, 34–58.
Rubin, D. B. (1980). Randomization analysis of experimental data: The Fisher randomization test
comment. Journal of the American Statistical Association, 75(371), 591–593.
Rubin, D. B. (1986). Statistics and causal inference—Comment: Which ifs have causal answers. Journal
of the American Statistical Association, 81, 961–962.
Rubin, D. B. (1990). Formal mode of statistical inference for causal effects. Journal of Statistical Planning and Inference, 25(3), 279–292.
Rubin, D. B. (2005). Causal inference using potential outcomes: Design, modeling, decisions. Journal
of the American Statistical Association, 100(469), 322–331.
Stuart, E. A., Cole, S. R., Bradshaw, C. P., & Leaf, P. J. (2011). The use of propensity scores to assess the
generalizability of results from randomized trials. Journal of the Royal Statistical Society: Series A
(Statistics in Society), 174(2), 369–386.
Tipton, E. (2013). Improving generalizations from experiments using propensity score subclassiﬁcation: Assumptions, properties, and contexts. Journal of Educational and Behavioral Statistics, 38(3),
239–266.
Tipton, E. (2014). How generalizable is your experiment? Comparing a sample and population
through a generalizability index. Journal of Educational and Behavioral Statistics, 39(6), 478–501.
Tipton, E., Hallberg, K., Hedges, L. V., & Chan, W. (2016). Implications of small samples for generalization adjustments and rules of thumb. Evaluation Review. Advance online publication.
doi:10.1177/0193841X16655665
Tipton, E., Hedges, L., Vaden-Kiernan, M., Borman, G., Sullivan, K., & Caverly, S. (2014). Sample
selection in randomized experiments: A new method using propensity score stratiﬁed sampling.
Journal of Research on Educational Effectiveness, 7(1), 114–135.

PARTIALLY IDENTIFIED TREATMENT EFFECTS FOR GENERALIZABILITY

669

