Seán M. Muller
The ability to generalize effects estimated from randomized experiments is critical for
their relevance to policy. Framing that problem in terms of causal interaction reveals the
extent to which the literature to date has failed to adequately address external validity.
An analogy with matching estimators illustrates the current inconsistency in approaches
to estimating causal relationships and generalizing these estimates to other populations
and contexts. Contrary to some claims, atheoretic replication is not a plausible solution.
Better knowledge of, and more information on, interacting factors is required for credible, formal extrapolation. In the absence of that, modesty is recommended.

Randomized evaluations have become widespread in development economics in
recent decades, largely due to the promise of identifying policy-relevant causal
effects. A number of concerns have been raised in response, among them that:
randomized control trials (RCTs) and quasi-experimental methods may not identify causal relationships of interest (“internal validity”) in practice; these methods
are not suitable for addressing important development questions at the macroeconomic level; therefore, analysis using RCTs is not obviously superior to alternative
approaches using observational data. A final concern, which is the subject of the
present contribution, is that current research based on experimental methods does
not adequately address the problem of extrapolating from empirical findings to
policy claims relating to other populations (“external validity”).1 In order to
Seán M. Muller is an economic analyst for the Parliamentary Budget Office, Cape Town, South Africa
and an affiliate of the Southern Africa Labour and Development Research Unit; his email address is:
smmuller@parliament.gov.za.
This paper has benefited from comments at the Annual Bank Conference on Development Economics
and presentation of related work at the Economic Society of South Africa Conference, University of
Stellenbosch, University of Cape Town (2011), the Evidence and Causality in the Sciences (ECitS)
conference (2012) and the CEMAPRE Workshop on the Economics and Econometrics of Education
(2013). I am grateful to Martin Wittenberg for detailed comments, David McKenzie for inputs as a
discussant on a lengthier version and the Development Policy Research Unit for enabling my attendance at
ABCDE. The views expressed are those of the author and not official positions of the Parliamentary
Budget Office. All errors and omissions remain my own.
1. Muller (2014) provides a first, detailed review of the contributions to methodological debates
regarding RCTs and the cross-disciplinary literature on external validity.
THE WORLD BANK ECONOMIC REVIEW, VOL. 29, SUPPLEMENT, pp. S217– S225
doi:10.1093/wber/lhv027
Advance Access Publication April 15, 2015
# The Author 2015. Published by Oxford University Press on behalf of the International Bank for
Reconstruction and Development / THE WORLD BANK. All rights reserved. For permissions,
please e-mail: journals.permissions@oup.com.

S217

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

Causal Interaction and External Validity: Obstacles
to the Policy Relevance of Randomized Evaluations

S218

THE WORLD BANK ECONOMIC REVIEW

INTERACTIO N

AND

E X T E R N A L VA L I D I T Y

The term “external validity” was coined by Campbell and Stanley (1966) and in
a detailed analysis of the problem Cook and Campbell (1979) argue that “all of
the threats to external validity [can be represented] in terms of statistical interaction effects” (Cook and Campbell 1979, 73). A similar point has been made
more recently by Leamer (2010) in his discussion of what he calls “interactive
confounders.” Cook and Campbell’s (1979) analysis is informal and therefore
somewhat ill-defined for econometric purposes, but it is straightforward to
extend their basic insight into the formal (“Neyman-Rubin”) statistical framework of counterfactuals, that is now widely used in econometrics. In that notation, Yi is the outcome variable for individual i, which becomes Yi ð1Þ ¼ Y1i
denoting the outcome state associated with receiving treatment (Ti ¼ 1) and
Yi ð0Þ ¼ Y0i denoting the outcome state associated with not receiving treatment
(Ti ¼ 0). The effect of treatment for any individual is Di ¼ Y1i  Y0i , and the researcher’s interest is typically in the average treatment effect E½Y1i  Y0i .
The basic conception of external validity utilized by Cook and Campbell
(1979) is that the treatment effect estimated in one population is the same as the
effect that would occur under an identical intervention in another population.
Following Hotz, Imbens, and Mortimer (2005), define a dummy variable D that
indicates whether a given individual is in the experimental sample (Di ¼ 0) or in

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

isolate the challenges posed by extrapolation, I assume away the other problems.
Specifically, the analysis that follows assumes that the research question is of
genuine policy interest, that researchers have been able to conduct an “ideal experiment”—with no problems of experimenter effects, noncompliance or anything else that would compromise internal validity—and that the identical policy
would be implemented in a new population.
Combining insights from prior literature on experimental methods in social
science (Cook and Campbell 1979) and econometric formulations of external validity (Hotz, Imbens, and Mortimer 2005) yields three important insights. First,
that plausibly attaining external validity requires ex ante knowledge of covariates
that influence the treatment effect along with empirical information on these variables in the experimental and policy populations. This, in turn, implies that
“atheoretical” replication-based resolutions to the external validity problem are
unlikely to be successful except for extremely simple causal relations, or very homogeneous populations, of a kind that appear unlikely in social science. Finally,
the formal requirements for external validity are conceptually analogous to the
assumptions needed for causal identification using observational data. Together
these imply a much more modest interpretation of the policy relevance of past
work that has not addressed these issues. Furthermore, the resultant challenges
for making policy claims premised on randomized evaluations are substantial, if
not insurmountable, in many cases of interest.

Muller

S219

Definition Simple external validity
E½Yi ð1Þ  Yi ð0ÞjDi ¼ 1 ¼ E½Yi ð1Þ  Yi ð0ÞjDi ¼ 0:

ð1Þ

Given this, it is straightforward to show in what sense interaction is an obstacle
to external validity. Where some other variable, W, interacts with T in the causal
relation producing Y, then simple external validity will fail if the mean of W
differs across the two populations. Equations (2) and (3) capture a simple case in
the counterfactual framework.
Y0i ¼ t0 þ Xi b þ Wi g þ u0i

ð2Þ

Y1i ¼ t1 þ Xi b þ Wi ðd þ gÞ þ u1i :

ð3Þ

For the purposes of estimation there is nothing particularly remarkable about interactive functional forms of this kind; it is possible to obtain unbiased estimates of
the ATE from a least-squares regression even if the interaction effect is omitted.2
For extrapolation, however, neglect of interaction may lead to entirely misleading conclusions about the ATE outside the experimental sample. Given (2) and
(3) we can write the ATE as:
E½Y1i  Y0i  ¼ ðt1  t0 Þ þ E½Wi jT ¼ 1d:

ð4Þ

This now depends in part on the mean value of the covariate(s) (W) in the
population.3
As an illustrative example–invoked in related contributions by Imbens (2010),
Angrist and Pischke (2010), and Pritchett and Sandefur (2013)–consider the
case of random or quasi-random evaluations of the effect of school class sizes on
student test scores. Virtually all contributions to this literature assume an additive educational production function, implying that the effect of class size (T) is
independent of other variables. Assume instead that this remarkably strong assumption is false and, for example, teacher quality (W) partly determines the
effect of class size on test scores. Then the above result implies that, for the
simple model in (2) and (3), the estimated average treatment effect from an RCT
will only be a reliable predictor of the effect of this intervention in another population where average teacher quality is similar.
2. This follows from the general result that E½YjX; W; f ðX; WÞ ¼ E½YjX; W, provided E½YjX; W is
linear in the parameters.
3. Similar formulations have recently been used in the context of discussions of external validity by
Allcott and Mullainathan (2012) and Pritchett and Sandefur (2013), although without explicit
recognition of the key role played by interactions that we develop here.

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

the population of policy interest (Di ¼ 1), in which case we can represent ‘simple
external validity’ as:

S220

THE WORLD BANK ECONOMIC REVIEW

Definition Conditional external validity
E½Yi ð1Þ  Yi ð0ÞjDi ¼ 1 ¼ EW ½E½Yi jT1 ; Di ¼ 0; Wi   E½Yi jT0 ; Di ¼ 0; Wi jDi ¼ 1

ð5Þ
This states that the ATE in the population of interest can be expressed in terms
of an expectation of the covariate-varying treatment effect in the experimental
sample (Di ¼ 0), taken across the covariate (W) distribution in the population of
interest (Di ¼ 1).4
Hotz, Imbens, and Mortimer (2005) show that given independence of treatment
assignment and outcomes in the experimental sample—as produced by a successful experiment—two further conditions are sufficient for (5) to hold. “Location
independence” states that potential outcomes do not vary across locations except
as a result of differences between individuals in values of the variables in W.
Assumption 1.1. Location independence
Di ? ðYi ð0Þ; Yi ð1ÞÞjWi

ð6Þ

The assumption of overlapping support (assumption 1.2) states that there is a
non-zero probability of being in either location for any realized values of the
covariates (Wi ¼ w).
4. A related contribution is the analysis by Angrist and Fernandez-Vál (2013), which examines the
extrapolation problem in the more complicated case (relative to an ideal experiment) of estimating a local
average treatment effect.

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

Besides the important, but relatively neglected, methodological work of Hotz,
Imbens, and Mortimer (2005) and some recent working papers that address the
importance of implementer characteristics (Allcott and Mullainathan 2012; Bold
et al. 2013), the empirical literature has addressed this issue only symptomatically
through ad hoc analysis of “heterogeneity” in estimated treatment effects. The
credibility of such ex post testing for significance across covariates has been
criticised in other disciplines—for example, Rothwell (2005) in the context of
medical trials—and in a few recent contributions to the econometric literature.
Crump et al. (2008), for instance, have proposed a more systematic approach to
this kind of testing. As those authors note, however, this kind of approach can
only tell us—if significant heterogeneity is found—that simple external validity
will fail. It does not provide a positive basis for extrapolation, precisely because it
does not address the source of the problem as identified by Cook and Campbell
(1979).
To go further requires explicitly formulating external validity in terms of covariates. Hotz, Imbens, and Mortimer (2005) develop such a formulation which
they refer to as “conditional external validity”.

Muller

S221

Assumption 1.2. Overlapping support

d . 0 and for all w [ W:

ð7Þ

While (5) only loosely informs Hotz, Imbens, and Mortimer’s (2005) empirical
analysis, in the presence of interaction it implies—I argue—clear formal and empirical requirements for obtaining external validity that are comparable to the
well-known sets of alternative assumptions that must be satisfied to obtain internal validity.
Empirical Requirements for External Validity
What are the implications, then, of interaction for empirical analysis? Consider
the simplest case where there is one, dichotomous interacting variable
W [ f0; 1g and the experiment allows us to identify E½DjW ¼ 0; D ¼ 0 and
E½DjW ¼ 1; D ¼ 0, where:
E½DjD ¼ 0 ¼ PrðW ¼ 0jD ¼ 0ÞE½DjW ¼ 0; D ¼ 0
þ ð1  PrðW ¼ 0jD ¼ 0ÞÞE½DjW ¼ 1; D ¼ 0

ð8Þ

In our previous example, W might now denote low- and high-quality teachers.
If we then know the distribution of W in the target population, the average treatment effect of policy interest can be expressed in terms of these estimated values:
E½DjD ¼ 1 ¼ PrðW ¼ 0jD ¼ 1ÞE½DjW ¼ 0; D ¼ 0
þ ð1  PrðW ¼ 0jD ¼ 1ÞÞE½DjW ¼ 1; D ¼ 0:

ð9Þ

To implement the above procedure in practice a number of conditions need to be
satisfied. The researcher must know what the interacting variable is. Assuming
that is the case, the empirical distribution of W in the policy population (D ¼ 1)
must be observed. It must also be possible, in terms of having data on W and sufficient power from the experimental sample, to obtain unbiased and accurate estimates of the conditional average treatment effect. These in themselves are very
demanding requirements.
Furthermore, in order to collect the necessary data, researchers will need to
know, or anticipate, the identity of all interacting variables at the experimental
design stage. In the class size case, few studies have collected data on teacher
quality. If that variable interacts with class size to any appreciable degree then
formal extrapolation of the kind implied by (5) is likely to be impossible.
Lastly, it must be the case that such variables are meaningfully comparable
across the two populations. This means, first-and-foremost, having empirical
measures that are comparable. It also requires that the variables in question are

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

8w; d , PrðDi ¼ 1jWi ¼ wÞ , 1  d;

S222

THE WORLD BANK ECONOMIC REVIEW

Resolution through Sampling?
The above challenges have not been addressed in the empirical literature to date,
which is problematic given that many contributions to that literature are intended to inform policy in populations beyond the experimental sample. However, in
some instances there may be an alternative to formal extrapolation of this kind.
Much as Cook and Campbell (1979) identify interaction as the primary challenge to external validity, they argue that sampling is the basis for solving the extrapolation problem. Two approaches are noteworthy from an econometric
perspective: random sampling from the population of policy interest; and, “sampling for heterogeneity.” The former presumes the existence of a known “target
population” prior to conducting the experiment, in which case random sampling
will assure—in the limit—that E½Wij jDi ¼ 0 ¼ E½Wij jDi ¼ 1 for all Wj [ W. In
many cases of interest, however, this is either not practically feasible, or researchers have the more ambitious aim of generalising beyond a single, prespecified
population.
The second approach, I suggest, is best understood in terms of the overlapping
support assumption in (7). The idea is that while the experimental sample of any
given study may not be representative—in terms of W—of the population of interest, researchers may be able to conduct individual studies across a wide
enough range of experimental populations so that when combined they satisfy
this requirement.
On the face of it this may appear to support proposals (e.g., Duflo,
Glennerster, and Kremer 2006; Angrist and Pischke 2010) that replication is the
appropriate way to address external validity. However, further to Keane’s (2010)
more general critique of RCTs, there is clearly no basis for determining the
formal prospects of “atheoretic” replication: if the interacting factors are
unknown it is not possible to deliberately sample for heterogeneity, so researchers must rely on an unsubstantiated presumption that the extent of interaction is
limited enough to be revealed through uninformed replication.5
Even in the event that the interacting factors are known, we require a formal
method for integrating evidence from multiple experiments for the purpose of extrapolation. The most widely used procedure for integrating evidence from experiments in the social and health sciences is metaanalysis, but methods for
dealing with heterogeneity are in their infancy and the determination of relevance to other populations remains a qualitative exercise.
5. Related points are made by Deaton (2010, 30) and Rodrik (2008, 21). Imbens (2010, 420)
mentions the possibility of approximating the conditional average treatment effect using multiple
experiments but does not explain how the relevant covariates would be determined ex ante.

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

conceptually comparable across contexts, something implicit in the overlapping
support assumption. For example, if social institutions vary across contexts due
to history, they may be fundamentally incomparable; where such institutions interact with the treatment variable extrapolation may therefore be impossible.

Muller

S223

Equivalences between Assumptions for Matching and External Validity

Assumption 1.3. Unconfoundedness
Ti ? ðY0i ; Y1i ÞjXi

ð10Þ

But notice that the structure of (10) is identical to the location independence assumption in (6) required for extrapolation. This has two important implications.
First, consider the case where researchers directly apply results from a single
RCT to populations other than the experimental sample, thereby assuming
simple external validity holds. For that to be correct, interacting factors must
either not exist or be balanced across the experimental and policy populations.
However, a similar assumption across “treatment” and “nontreatment” populations in observational data would imply that internal validity could be obtained
without randomization.
Alternatively, consider the second, more sophisticated, case where researchers
implement an empirical procedure for extrapolation premised on Hotz, Imbens,
and Mortimer’s (2005) notion of conditional external validity. As we have seen,
that requires a “location independence” assumption (6), which rests on the use
of the correct set of covariates (interacting factors) and in form is identical to
the unconfoundedness assumption in (10). The first difference between these
assumptions is in the two populations: the experimental sample and policy
population for conditional external validity versus the treatment recipient and

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

Practitioners favouring randomized evaluations may take the view that interacting variables can be identified, and results extrapolated, using a researcher’s “expertise.” Indeed, as the preceding analysis should make clear, this is implicit in
most extant policy recommendations premised on RCTs. However, there is an
inherent inconsistency in this stance.
Consider methods for estimating causal effects with observational data using
matching estimators. The intuition for these is that the researcher matches every
individual with T ¼ 1 not obtained through random assignment with a maximally similar individual who has T ¼ 0, where neither state was obtained through
random assignment. Where enough individuals across the two groups are sufficiently similar on all dimensions that matter for the effect of treatment and are
matched accordingly, it is possible to obtain an unbiased estimate of the average
treatment effect even with observational data.
If a researcher takes the view that randomized evaluations are inherently superior to approaches based on matching, then this implies—leaving other practicalities aside—that they do not believe it is possible to identify and observe all
characteristics that are important for selection bias and heterogeneity of treatment effects. Formally, they do not believe it is possible to choose X such that the
“selection on observables” assumption in (10) holds.

S224

THE WORLD BANK ECONOMIC REVIEW

REFERENCES
Allcott, H., and S. Mullainathan. 2012. “External Validity and Partner Selection Bias.” NBER Working
Paper 18373.
Angrist, J., and I. Fernandez-Vál. 2013. “ExtrapoLATE-ing: External Validity and Overidentification in
the LATE Framework.” In D. Acemoglu, M. Arellano, and E. Dekel, eds., Advances in Economics and
Econometrics: Theory and Applications, Econometric Society Monographs, Tenth World Congress.
Vol. III. Cambridge: Cambridge University Press.
Angrist, J. D., and J.-S. Pischke. 2010. “The Credibility Revolution in Empirical Economics: How
Better Research Design Is Taking the Con out of Econometrics.” Journal of Economic Perspectives 24
(2): 3–30.
Bold, T., M. Kimenyi, G. Mwabu, A. Ngángá, and J. Sandefur. 2013. “Scaling Up What Works:
Experimental Evidence on External Validity in Kenyan Education.” Center for Global Development
Working Paper 321.
Campbell, D. T., and J. C. Stanley. 1966. Experimental and Quasi-experimental Designs for Research.
Chicago: Rand McNally College Publishing.
Cartwright, N. 2010. “What Are Randomised Controlled Trials Good For?” Philosophical Studies 147:
59 –70.
Cook, T. D., and D. T. Campbell. 1979. Quasi-Experimentation: Design and Analysis Issues for Field
Settings. Belmont, CA: Wadsworth.
Crump, R. K., V. J. Hotz, G. W. Imbens, and O. A. Mitnik. 2008. “Nonparametric Tests for Treatment
Effect Heterogeneity.” Review of Economics and Statistics 90 (3): 389– 405.
Deaton, A. 2010. “Instruments, Randomization, and Learning about Development.” Journal of
Economic Literature 48 (2): 424–55.
Duflo, E., R. Glennerster, and M. Kremer. 2006. “Using Randomization in Development Economics
Research: A Toolkit.” In T. Schultz, and J. Strauss, eds., Handbook of Development Economics.
Vol. 4. Amsterdam: Elsevier.

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

non-recipient populations for matching estimators. The second difference is in
the vector of conditioning variables. The vector of factors influencing whether
individuals have T ¼ 1 or T ¼ 0 absent random assignment may differ from ones
representing factors that determine presence in either the experimental sample or
policy population. Nevertheless, to date no author has provided any reason to
believe, ex ante, that one assumption is more plausible than the other. In which
case it is not clear why even formal extrapolation of estimated treatment effects
from ideal experiments is any more likely to be successful than the use of matching estimators to identify causal effects using observational data.
The analogous nature of the assumptions required should put to rest the
notion that policy-oriented economic analysis must use random or quasi-random
variation to obtain internal validity, but can rely on weakly substantiated, subjective assessments of external validity. This inconsistency has previously been emphasised by, among others, Cartwright (2010), Deaton (2010), and Manski
(2013). There is no doubt that the role of theory is, at least in part, to inform empirical analysis in this way and that “atheoretic” replication cannot plausibly
suffice to resolve the problem. Whether theory can provide ex ante knowledge to
an extent sufficient to produce confident extrapolation of results from one
context to another remains, for now, a wholly open question.

Muller

S225

Hotz, V., G. W. Joseph, Imbens, and J. H. Mortimer. 2005. “Predicting the Efficacy of Future Training
Programs Using Past Experiences at Other Locations.” Journal of Econometrics 125: 241–70.

Keane, Michael P. 2010. “Structural vs. Atheoretic Approaches to Econometrics.” Journal of
Econometrics 156 (1): 3 –20.
Leamer, Edward. 2010. “Tantalus on the Road to Asymptopia.” Journal of Economic Perspectives 24 (2):
31– 46.
Manski, Charles F. 2013. Public Policy in an Uncertain World: Analysis and Decisions. Cambridge, MA:
Harvard University Press.
Muller, S. M. 2014. “Randomised Trials for Policy: A Review of the External Validity of Treatment
Effects.” Southern Africa Labour and Development Research Unit Working Paper 127.
Pritchett, L., and J. Sandefur. 2013. “Context Matters for Size.” Center for Global Development Working
Paper 336.
Rodrik, D. 2008. “The New Development Economics: We Shall Experiment, but How Shall We Learn?”
Harvard Kennedy School Working Paper, RWP08–055.
Rothwell, Peter M. 2005. “Subgroup Analysis in Randomised Controlled Trials: Importance, Indications,
and Interpretation.” Lancet 365: 176 –86.

Downloaded from https://academic.oup.com/wber/article-abstract/29/suppl_1/S217/1689262 by Harvard Law School Library user on 25 February 2019

Imbens, G. W. 2010. “Better LATE than Nothing: Some Comments on Deaton (2009) and Heckman and
Urzua (2009).” Journal of Economic Literature 48 (2): 399–423.

