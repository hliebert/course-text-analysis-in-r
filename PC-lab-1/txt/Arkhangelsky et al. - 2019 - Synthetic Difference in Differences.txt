Synthetic Difference In Differences∗
arXiv:1812.09970v2 [stat.ME] 31 Jan 2019

Dmitry Arkhangelsky†

Susan Athey‡

Guido W. Imbens¶

David A. Hirshberg§

Stefan Wagerk

First version December 24, 2018, current version February 1, 2019

Abstract
We present a new perspective on the Synthetic Control (SC) method as a weighted least
squares regression estimator with time fixed effects and unit weights. This perspective suggests a generalization with two way (both unit and time) fixed effects, and both unit and
time weights, which can be interpreted as a unit and time weighted version of the standard
Difference In Differences (DID) estimator. We find that this new Synthetic Difference In
Differences (SDID) estimator has attractive properties compared to the SC and DID estimators. Formally we show that our approach has double robustness properties: the SDID
estimator is consistent under a wide variety of weighting schemes given a well-specified
fixed effects model, and SDID is consistent with appropriately penalized SC weights when
the basic fixed effects model is misspecified and instead the true data generating process
involves a more general low-rank structure (e.g., a latent factor model). We also present
results that justify standard inference based on weighted DID regression. Further generalizations include unit and time weighted factor models.

Keywords: Synthetic Controls, Causal Effects, Panel Data, Difference In Differences, LowRank Confounders
∗

We are grateful for conversations with Avi Feller and Yinchu Zhu, and for comments from seminar
participants at UC San Diego. This research was generously supported by ONR grant N00014-17-1-2131
and the Sloan Foundation.
†
Assistant Professor, CEMFI, Madrid, darkhangel@cemfi.es.
‡
Professor of Economics, Graduate School of Business, Stanford University, and NBER,
athey@stanford.edu.
§
Postdoctoral Fellow,
Department of Statistics and SIEPR, Stanford University,
davidahirshberg@stanford.edu.
¶
Professor of Economics, Graduate School of Business, and Department of Economics, Stanford
University, SIEPR, and NBER, imbens@stanford.edu.
k
Assistant Professor of Operations, Information and Technology, Graduate School of Business, and
of Statistics (by courtesy), Stanford University, swager@stanford.edu.

1

Introduction

Synthetic Control (SC) methods, introduced in a seminal series of papers by Abadie and coauthors [Abadie and Gardeazabal, 2003, Abadie, Diamond, and Hainmueller, 2010, 2015, Abadie
and L’Hour, 2016], have quickly become one of the most popular methods for estimating treatment effects in panel settings. By using data-driven weights to balance pre-treatment outcomes
for treated and control units, the SC method imputes post-treatment control outcomes for the
treated unit(s) by constructing a synthetic version of the treated unit(s) equal to a convex
combination of control units.
In the current paper, we build on these ideas to provide a different perspective on the SC
approach and to propose a new estimator with improved bias properties. First, we show that the
SC estimator can be viewed as a weighted least squares regression estimator with unit-specific
weights, where the regression model includes time fixed effects. We then propose adding unit
fixed effects to this regression representation of the standard SC set up to add flexibility, as
well as time weights to ensure that the weighted periods resemble more closely the period(s) for
which we are imputing the counterfactual. We show that this leads to a doubly weighted, or
local, version of the standard Difference In Differences (DID) estimator [e.g., Bertrand, Duflo,
and Mullainathan, 2004, Card, 1990]. We then establish that the resulting estimator, which
we call the Synthetic Difference In Differences (SDID) estimator, has attractive bias properties
compared to both the SC and DID estimators. In particular the estimator satisfies a form of
double robustness that both the SC and DID estimators lack: the estimator is consistent if
either the model is correctly specified, or if the weights are well choosen, but consistency does
not require both those conditions.
Consider the simplest case of a balanced panel with N units and T time periods, where
outcomes are denoted by Yit , and exposure to the binary treatment is denoted by Wit ∈ {0, 1}.
Initially suppose that Wit = 0 unless (i, t) = (N, T ), so that only unit N is treated, and only
in period T . Suppose also that there are no covariates. In that case, the SC estimator for the
causal effect is τ̂ sc = YN T − ŶNscT where ŶNscT is a weighted average of the period T outcomes
P −1 sc
sc
for the control units, ŶNscT = N
i=1 ω̂i YiT , with the weights ω̂i chosen to make the weighted
average of the controls in the pre-treatment period approximate the corresponding value for
P
the treated unit, i ω̂isc Yit ≈ YN t for all t = 1, . . . , T − 1. In this paper, we introduce a novel

2

characterization of the SC estimator τ̂ sc as a weighted least squares regression estimator:

sc

(µ̂, β̂, τ̂ ) = arg min
µ,β,τ

N X
T
X

(Yit − µ − βt − Wit τ )2 ω̂isc .

(1.1)

i=1 t=1

The regression has time fixed effects and unit-specific weights. In comparison, the standard DID
estimator τ̂ did for the treatment effect is

(µ̂, α̂, β̂, τ̂

did

) = arg min

N X
T
X

α,β,µ,τ

(Yit − µ − αi − βt − Wit τ )2 .

(1.2)

i=1 t=1

Our characterizations (1.1) and (1.2) make clear that relative to the SC estimator, the DID
estimator adds a unit fixed effect to the specification of the regression function, but it removes
the (unit) weights in the estimation. Contrasting the SC and DID estimators in this way suggests
a natural modification. Specifically, we propose the SDID estimator τ̂ sdid , formally defined as:

(µ̂, α̂, β̂, τ̂

sdid

) = arg min

α,β,µ,τ

N X
T
X

(Yit − µ − αi − βt − Wit τ )2 ω̂i λ̂t .

(1.3)

i=1 t=1

The regression in (1.3) includes both unit and time fixed effects as well as weights, where the
weights are the product of unit weights ω̂i and time weights λ̂t , with both sets of weights are
derived from the data. In the spirit of the SC approach, these time weights λ̂t could be chosen
so that within a unit, the weighted average outcomes across periods approximate the target
P
period, Tt=1 λ̂t Yit ≈ YiT for all i = 1, . . . , N − 1. Alternatively, one may wish to choose the time
weights partly to put more emphasis on recent periods. Thus, the proposed SDID estimator
differs from the DID estimator by allowing for both unit and time weights, and it differs from
the SC estimator by including unit-fixed effects and allowing for time weights.
Many approaches to SC settings, including Abadie, Diamond, and Hainmueller [2010, 2015],
Doudchenko and Imbens [2016], Xu [2017], Athey, Bayati, Doudchenko, Imbens, and Khosravi
[2017], Carvalho, Masini, and Medeiros [2018], Li and Bell [2017], can be thought of as either
focusing on constructing balancing weights, or focusing on modeling the conditional outcomes.
Ben-Michael, Feller, and Rothstein [2018] is an interesting exception. Their Augmented Synthetic Control (ASC) estimator uses a model for the conditional expectation of the last period’s
outcome YiT in terms of the lagged outcomes, in combination with the SC balancing weights,
3

in the spirit of unconfoundedness type methods, and in particular residual balancing methods
[Robins, Rotnitzky, and Zhao, 1994, Athey, Imbens, and Wager, 2018]. Their method also has
double robustness properties, but it cannot be characterized as a weighted regression estimator.
The importance of combining such outcome modeling and balancing/weighting and the associated double robustness are prominent features of the general program evaluation literature [e.g.,
Chernozhukov, Escanciano, Ichimura, Newey, and Robins, 2018b, Hirshberg and Wager, 2018,
Imbens and Rubin, 2015, Newey, Hsieh, and Robins, 2004, Scharfstein, Rotnitzky, and Robins,
1999], and most of the currently recommended estimators in that literature combine them.
An attraction of our regression set up is that it generalizes naturally to the case with multiple
treated units and multiple treated periods. We can in that case choose the unit weights for the
control units to balance the average of the treated units during the pre-treatment period, and
the time weights for the pre-treatment periods to balance the average post-treatment outcomes
for the control units. The regression set up can also easily accomodate covariates that vary by
unit and time by including them in the regression function. Unit-specific but time-invariant
covariates, which cannot be accomodated in the standard DID set up can be accomodated here
by adjusting the unit weights so the weights also balance these unit-specific covariates, and
similarly for time-varying covariates common to all units.
In the second half of the paper, we establish asymptotic properties of the SDID estimator
in a regime where both N and T are large. Establishing formal asymptotic properties for
estimators has been a major challenge in the SC literature. Throughout, we take the perspective,
common in panel data settings, that Y is a noisy estimate of an underlying signal matrix L, i.e.,
Yit = Lit +Wit τ +εit , where Wit denotes treatment assignment and ε is noise. The matrix L could
have a simple two-way fixed effect form, or have more generic low-rank structure (e.g., interactive
fixed effects, latent factor models) as in Athey, Bayati, Doudchenko, Imbens, and Khosravi
[2017], Bai and Ng [2002], Bai [2009], Bonhomme and Manresa [2015], Li and Bell [2017], and
Xu [2017]. We prove a variety of consistency results under different assumptions to highlight
the double robustness properties of our proposed estimator. One consistency result makes weak
assumptions on the weights but relatively strong assumptions on the outcome model L, while
another makes weak assumptions on the conditional outcome model but stronger assumptions
on the weights.
Ideally, the weights ω̂ and λ̂ would balance out the rows and columns of the underlying
signal matrix L in a way that eliminates bias, and moreover the weights would not depend
4

on the noise ε. This is essentially what occurs in the analysis of balancing methods under
unconfoundedness, where pre-treatment covariates are taken to be noiseless [Athey, Imbens, and
Wager, 2018, Graham, de Xavier Pinto, and Egel, 2012, Hainmueller, 2012, Imai and Ratkovic,
2014, Zubizarreta, 2015]. Here, however, the weights ω̂ and λ̂ are optimized to balance Y , not
L, and have a rich dependence on the noise ε that cannot be eliminated via sample splitting.
In Section 4.3, we use tools from modern empirical risk minimization theory to address both
challenges and to show that, despite being optimized to balance the observed Y , and despite the
fact that balancing the Y is a major challenge because the number of units is of the same order
as the number of time periods, the weights ω̂ and λ̂ balance the unobserved L well enough to
achieve consistency. In addition to proving consistency of the SDID estimator, our results also
allow us to establish conditions under which the original SC estimator is consistent given a lowrank L. The conditions on the weights for consistency of the SC estimator are stronger than
those needed for consistency of SDID because the latter has a double bias removal property
thanks to the time weights. Our asymptotic results require that we modify the original SC
weights using penalization to ensure that the number of units with positive weights increases in
large samples.
Finally, we present conditions that justify calculating the standard error for τ̂ sdid using standard robust inference methods for DID regressions; we show that the standard robust standard
errors are valid despite the fact that they take the weights as fixed, that is, they do not algorithmically account for dependence of the weights on the data.

2

Synthetic Panel Methods

Suppose we have a balanced panel with observations on an outcome Yit , i = 1, . . . , N , t =
1, . . . , T , with some units treated in some periods, and the binary treatment indicator denoted
by Wit ∈ {0, 1}. Throughout this paper, we assume that the outcomes Yit are generated as Yit =
Lit + Wit τ + εit , where εit is a noise term (potentially with correlation over time, that is, within
rows of the matrix Y ), and L is a baseline expected response matrix that may be correlated with
Wit . The methods proposed here can also be generalized to allow for heterogeneity in τ , e.g.,
perhaps treatment intensity depends on the number of time periods for which a unit has been
exposed to treatment. We refer to Athey and Imbens [2018] for a design-based interpretation of

5

these estimands using potential outcomes.
As motivation for our approach, consider models that parametrize the conditional expectation of the full set of unit and time period pairs: L = g(θ), where g : Θ 7→ RN × RT models the
conditional expectation of the control outcomes in terms of an unknown parameter θ. Examples
of such panel data models include
g(θ)it = θ,

(constant)

g(µ, α, β)it = µ + αi + βt ,
g(A, B)it =

R
X

Air Btr ,

θ = (µ, α, β),

(two-way fixed effect)

θ = (A, B),

(factor model).

r=1

(In the two-way fixed effect and factor models we also need some normalizations, e.g., α1 =
β1 = 0.) Now, given such a g(·), a natural approach would be to fit θ and τ on all the data:




θ̂, τ̂ = arg min
θ

N X
T 
X

Yit − g(θ)it − τ Wit

2

.

(2.1)

i=1 t=1

In the case with g(θ)it = µ + αi + βt , this leads us to the basic difference in differences estimation
strategy.
A challenge with this approach, however, is that if both N and T are large it may be difficult
to find a simple specification of g(·) that fits well over the entire panel. But accurately estimating
the entire matrix L is more difficult than the actual challenge: in order to estimate τ , we only
need the expectation Lit locally, that is, for the control potential outcomes in the treated cell
or cells. Here, we propose addressing potential misspecification of g(·) through weighting. We
apply weights ω̂i to the units and λ̂t to the time periods to form a “synthetic panel” on which
the model g(·)it is approximately unbiased for Lit , and then we build a model of the conditional
expectation on this weighted panel:


θ̂

sdid

, τ̂

sdid



= arg min
θ

T 
N X
X

Yit − g(θ)it − τ Wit

2

ω̂i λ̂t .

(2.2)

i=1 t=1

We can qualitatively think of the SDID estimator as relaxing the parallel trends assumption
in the DID estimator. Instead of assuming parallel trends for all units and all time periods,
6

the SDID estimator assumes that there exist unit and time weights such that the averaged
treated unit and the weighted average of the control units satisfy a parallel trends assumption,
and satisfy it not for all time periods but only for the averaged post-treatment period and the
weighted average of the pre-treatment periods.
One of our main findings is that, if we use synthetic control weights ω̂i and λ̂t , then relying on a simple two-way fixed effect model for g(·) in (2.2) allows for consistent (in the large
N and large T sense) estimation of τ , even if the two-way fixed effects model may be badly
misspecified over the full panel. This finding is in line with a key insight from the program
evaluation literature is that often methods that combine weighting/balancing the treated and
control units with modeling the control outcome distribution outperform methods that only
model the outcomes, as well as methods that only balance treated and control units. “Better” here includes both formal bias properties, as well as simulation evidence. A key formal
property is that of double robustness, where misspecification of only the balancing weights or
the conditional outcome model does not lead to inconsistency of τ̂ [Athey, Imbens, and Wager,
2018, Belloni, Chernozhukov, and Hansen, 2014, Chernozhukov, Escanciano, Ichimura, Newey,
and Robins, 2018b, Hirshberg and Wager, 2018, Imbens and Rubin, 2015, Newey, Hsieh, and
Robins, 2004, Scharfstein, Rotnitzky, and Robins, 1999].
Before discussing our choice of weights further below, we note that (2.2) not only allows for
practical estimation of τ , but can also be used to build confidence intervals for τ . In the case of
two-way fixed effects with covariates,


µ̂, α̂, β̂, γ̂, τ̂

sdid



= arg min

α,β,µ,τ

N X
T
X

(Yit − µ − αi − βt − Xit γ − Wit τ )2 ω̂i λ̂t ,

(2.3)

i=1 t=1

there is a wide variety of standard error estimates that that have been studied in the literature;
see Arellano [2003], Bertrand, Duflo, and Mullainathan [2004], Hansen [2007], Liang and Zeger
[1986], and Stock and Watson [2008] for examples and discussions. We present both formal
and simulation evidence that we can obtain valid confidence intervals for τ by applying these
methods directly to (2.2), treating the weights ω̂ and λ̂ as fixed, despite the fact that these
weights depend on the Yit ; see Section 5 for details.

7

2.1

Weighting for Synthetic Panels

In this paper, we focus on weighting in a generalization of the basic synthetic control setting of
Abadie, Diamond, and Hainmueller [2010]. We assume that exposed units i > N0 get treated
in time periods t > T0 , i.e Wit = 1 {i > N0 and t > T0 }. The weighting component of our
approach focuses on weights to balance the sample towards the treated unit / time period pairs.
A critical feature of our approach, appropriate to our treatment pattern W , is that we impose a
factor structure on the weights: γit = ωi λt . In addition to the factor structure we impose some
restrictions on the unit and time period weights. The weights are non-negative, and weight
groups ω1:N0 , ω(N0 +1):N , λ1:T0 , λ(T0 +1):T all sum to one. Moreover, we give equal weight to all the
exposed units and time periods. Formally, the set of weights we consider satisfy
(

N0
X

1
W = ω ∈ R ωi ≥ 0;
ωi = 1; ωN0 +1 , ..., ωN =
N − N0
i=1
(
)
T0
X
1
L = λ ∈ RT λt ≥ 0;
λi = 1; λT0 +1 , ..., λT =
.
T − T0
t=1
N

)
,
(2.4)

One possible choice for the weights is the SC weights Abadie, Diamond, and Hainmueller [2010,
2015], which Doudchenko and Imbens [2016] show, for the case without covariates, can be written
as (in the basic form we consider, synthetic control analyses only have one exposed unit, i.e.,
N0 = N − 1 and only one exposed time period, T0 = T − 1; however, the generalization is
immediate)

ω̂ sc = arg min
ω∈W

T −1
X

N
−1
X

t=1

i=1

!2
ωi Yit − YN t

.

(2.5)

In the current paper we modify these weights slightly by putting an L2 (ridge) penalty on the
weights to ensure that in larger samples there will be many units with non-zero weights, which is
important for the asymptotic properties of the estimator. Note that using an L1 (lasso) penalty
does not work because the weights are nonnegative and sum to one. We also consider the time
equivalent of the SC weights, which do not appear to have been considered in this literature:

sc

λ̂ = arg min
λ∈L

N
−1
X

T −1
X

i=1

t=1

!2
λt Yit − YiT

.

(2.6)

8

The time weights play somewhat of a different role than the unit weights. In some cases one may
wish to explicitly put more weights on recent periods than on distant periods, and not solely
have these weights determined by the similarity, in terms of outcomes, to the current period.
We may also wish to regularize the time weights to avoid putting most of the weight on a very
small number of units or time periods.
In cases where the data exhibit substantial trends, the SC time weights would tend to
concentrate on the most recent values. One modification in that case is to allow for an intercept
in the regression, and solve

λ̂isc = arg min

N
−1
X

λ0 ∈R, λ∈L i=1

λ0 +

T −1
X

!2
λt Yit − YiT

.

(2.7)

t=1

The intercept λ̂0 is not needed for weighting, as the time dummies βt will be able to absorb any
time trends during the modeling stage. We refer to these weights as the intercept weights λ̂isc
t ,
and note that these weights are invariant to adding in any global time trend to our observations,
Yit ← Yit + f (t).
An alternative, for both the unit and time weights, is to use kernel weights. For example, if
there is only one exposed unit / time period, we could use
ω̂ikernel


∝K

Yi(1:T −1) − YN (1:T −1)
hω



λ̂kernel
t

,


∝K

Y(1:N −1)t − Y(1:N −1)T
hλ


,

(2.8)

for some kernel function K(·), e.g., K(a) = exp(−a> a). We allow the tuning parameter to be
different for the unit and time dimension. We also consider nearest neigbor weights, where we
given constant weights to the Kω nearest units and the Kλ neast time periods [e.g., Abadie and
Imbens, 2006].
Using nearest neighbor methods to construct weights stresses the challenges in obtaining
formal large sample properties for the resulting estimators, and this explains partly the limited
nature of large sample results in the SC literature. If N is large, it is impossible to obtain a
“close” match for Y(1:N −1)T because we are matching on N −1 variables with only T −1 potential
matches (e.g., Abadie and Imbens [2006]). Similarly, if T is large it is impossible to obtain close
matches for YN (1:T −1) because there are only N − 1 potential matches and T − 1 variables to
match on. With both N and T large it is impossible to obtain close matches in either direction.
9

Nevertheless, under some assumptions on the outcome model, the closest matches may be good
enough, in the sense that they match closely on the relevant underlying variables. For example,
if the data are generated by a two-way fixed effect model for L, matching on all the lagged
outcomes will not give a close match in terms of all the lagged outcomes. But, in large N and
large T such matching methods will lead to matches that are close in terms of the unit-fixed
effects, which is what matters. Our formal results show that this holds in general factor models
for L.

3

Panel Estimators as Bias Reduction Methods

In the previous section, we introduced SDID as a flexible approach to estimating causal effects in
panels where both N and T are moderately large. To gain further intuition about the method,
we focus here on the behavior of the SDID estimator in the case where we fit a two-way fixed
effects model without any additional covariates, and only unit N in time period T gets treated.
In this case, the SDID estimator allows for a simple, closed form solution that allows for a
transparent comparison with the SC estimator.
We use the following notation. Partition the N × T matrix of observed outcomes Y , and
other comformable matrices, by treatment group and pre/post treatment period:

Y =

Y::

Y:T

YN : YN T


,

where Y:: , Y:T , YN : , and YN T are (N − 1) × (T − 1), (N − 1) × 1, 1 × (T − 1), and 1 × 1 matrices
respectively. Also define Yi: to be a T − 1 dimensional row vector and define Y:t to be a N − 1
dimensonal column vector, each with typical element Yit . Define the averages for the three sets
of control outcomes,

Y

c,pre

N
−1 X
T −1
X
1
Yit ,
=
(N − 1)(T − 1) i=1 t=1

Y

10

c,post

N −1
1 X
=
YiT ,
N − 1 i=1

and
T −1

Y

t,pre

=

1 X
YN t .
T − 1 t=1

In this setting, the basic difference in difference estimator (1.2) can be written as
τ̂ did = YN T − ŶNdid
T (0),
ŶNdid
T (0)

= µ̂ + α̂N + β̂T = Y

c,pre



+ Y

t,pre

−Y

c,pre





+ Y

c,post

−Y

c,pre



We can see the DID estimator as doubly bias-adjusting the simple average Y
bias adjustment, Y

t,pre

−Y

c,pre

(3.1)
.
c,pre

, with the first

, taking into account stable differences between the treated unit

and the control units and the second bias adjustment, Y

c,post

−Y

c,pre

, taking into account stable

differences over time for the control group. The main weakness of this basic DID estimator,
however, is of course that it is only valid under a well-specified two-way fixed effects model,
which is a very strong assumption when both N and T are moderately large.

3.1

Synthetic Control as a Single Bias Reduction Method

The main idea of the SC approach is to re-weight the control rows i = 1, ..., N − 1 of the matrix
Y with weights ω̂isc such as to make the time trends among the weighted controls and the treated
unit track each other. In the spirit of (3.1), we can write the SC estimator (1.1) as a weighted
bias-reduced estimator:
τ̂ sc

N −1 T −1
N
−1
X
1 X X sc
sc
sc
= YN T − ŶN T (0), ŶN T (0) =
ω̂ Yit +
ω̂isc
T − 1 i=1 t=1 i
i=1

YiT

!
T −1
1 X
−
Yit . (3.2)
T − 1 t=1

The bias adjustment uses a weighted average of the post-treatment control outcomes, with
weights ω̂isc minus a doubly weighted average of the pre-treatment control outcomes.
The SC estimator presents an obvious improvement over the DID estimator in its use of
weights to address potential misspecification of the basic two-way fixed effects model. However,
unlike (3.1), the estimator (3.2) appears to be “missing” a second bias correction term of the
P −1
PN −1 sc
form 1/(T − 1) Tt=1
(YN t − i=1
ω̂i Yit ) that seeks to correct for a potential systematic failure
of the weights ω̂isc to achieve balance in the pre-treatment periods. It is interesting to note that
11

if the weights ω̂isc were to balance the pre-treatment periods perfectly, so that

YN t −

N
−1
X

ω̂isc Yit = 0,

for all t = 1, . . . , T − 1,

(3.3)

i=1

then the second bias correction term would be numerically zero, and so SC could implicitly be
seen as a double-bias reduction method.1 Typically, however, perfect balance as in (3.3) does
not hold, and so the lack of this second bias-correction term may affect the properties of the SC
estimator.

3.2

Synthetic Difference In Differences as a Double Bias Reduction
Method

The SDID estimator addresses the case where the synthetic control adjustment does not completely balance the underlying signal in the pre-treatment periods. In the special case with only
unit N treated in period T , the SDID estimator (1.3) can be thought of as bias-adjusting the
SC estimator based on the pre-treatment discrepancies, weighted by λ̂sc
t :
sdid
sc
τ̂ sdid = YN T − ŶNsdid
T (0), ŶN T (0) = ŶN T (0) +

T −1
X

λ̂sc
t

YN t −

t=1

N
−1
X

!
ω̂isc Yit .

i=1

We can also write the SDID estimator as a symmetric version of (3.2), with

ŶNsdid
T (0) =

N
−1 X
T −1
X

ω̂isc λ̂sc
t Yit +

i=1 t=1

T −1
X

λ̂sc
t

YN t −

t=1

N
−1
X

!
ω̂isc Yit +

N
−1
X

i=1

ω̂isc

YiT −

T −1
X

!
λ̂sc
t Yit

. (3.4)

t=1

i=1

That is, compared to the simple weighting estimator ŶNweight
there are two (weighted) bias
T
adjustments,
T −1
X
t=1

λ̂sc
t

YN t −

N
−1
X
i=1

!
ω̂isc Yit

and

N
−1
X

ω̂isc

i=1

1

YiT −

T −1
X

!
λ̂sc
t Yit

,

t=1

An equivalent statement of this fact is that if (3.3) were to hold, then adding row fixed effects to to the
synthetic control estimator (1.1) would not change the point estimate τ̂ .

12

whereas the SC estimator has only one bias adjustment (the second one), similar to the way the
DID estimator has two bias adjustments in the unweighted case.
The problem of turning synthetic controls into a double-bias removal style estimator has also
been recently considered by Ben-Michael, Feller, and Rothstein [2018]. Their main proposal,
the augmented synthetic control (ASC) estimator, involves fitting a model for the conditinoal
expectation m(·) for YiT in terms of the lagged outcomes Yi(1:(T −1)) , and then using this fitted
model to “augment” the basic synthetic control estimator

ŶNasc
T (0) =

N
−1
X

ω̂isc YiT +

m̂(YN : ) −

i=1

= m̂(YN : ) +

N
−1
X

!
ω̂isc m̂(Yi(1:(T −1)) )

i=1
N
−1
X

ω̂isc



!

Yit − m̂(Yi(1:(T −1)) )
.

(3.5)

i=1

The first representation of the ASC estimator emphasizes its interpretation as a modification
of the SC estimator. It uses a cross-section model for the last period’s outcome to remove
biases from the standard SC estimator. The second representation stesses the connections to the
unconfoundedness literature. The starting point is a model for the potential outcomes in the last
period as a function of lagged outcomes. On its own this would suggest the estimator m̂(YN : ); the
ASC estimator then robustifies this using a weighted average of the residuals. This construction
is related to the residual balancing estimators in the original double robust literature [Robins,
Rotnitzky, and Zhao, 1994] or in high-dimensional settings [Athey, Imbens, and Wager, 2018],
where now the SC weights can be interpreted as a type of covariate-balancing inverse-propensity
weights. Such adjustments make the estimator doubly robust under appropriate conditions. A
more recent paper, Chernozhukov, Wuthrich, and Zhu [2018c], takes a similar approach to BenMichael, Feller, and Rothstein [2018]; however, they swap the role of units and time periods and
present formal results under strong time-homogeneity assumptions that, in particular, rule out
the low-rank model Yit = Lit + Wit τ + εit .
The second representation of the ASC estimator makes clear that its formal justification
would be standard under a unconfoundedness assumption with the lagged outcomes playing the
role of the pre-treatment variables, given a fixed number of pretreatment periods and large N .
By the same token it would make the justification more difficult under general factor structures
and with large T . This representation also highlights the feature of this estimator that it includes

13

the lagged outcomes in exactly the same way that pre-treatment variables would be included in
unconfoundedness-type analyses.2 This is in contrast to many panel data models such as fixed
effect and factor models that incorporate lagged outcomes in the model in a way that is similar
to the way the last period outcomes are treated, namely as noisy measures of the underlying
unobserved components that are critical for prediction.
Despite their different motivations, ASC and SDID share an interesting connection: In the
special case with a single treated unit / period and with a linear m̂(·) model, the SDID estimator
in (3.4) and the ASC estimator in (3.5) are very similar. In fact, they would be equivalent
if we impose the additional restriction on the ASC estimator that the slope coefficients are
nonnegative, positive and to sum to one. This connection suggests that weighted double biasremoval methods are a natural way of working with panels where we do not believe the basic DID
approach to be appropriate. This being said we emphasize that, once we move past the most
basic model, e.g., we have covariates or multiple treated units and periods, or we use more flexible
specifications for m(·), then the connection between the two methods breaks down. Moreover,
as Ben-Michael, Feller, and Rothstein [2018] motivate their estimator using unconfoundedness
type arguments, they do not provide consistency results for the type of factor models considered
here. In addition the ASC estimator does not have a weighted least squares interpretation,
which is helpful in accommodating covariates.

4

Formal Results

In this section, we develop the properties of the SDID estimator. First, we consider properties
that hold when the DID model is correctly specified; second, we discuss robustness properties
provided by weighting. For simplicity, in this section we focus on the single exposed unit / time
period setting. In this case, we can write the SDID treatment effect estimate τ̂ as
bN T , L
bN T = µ̂ + α̂N + β̂T ,
τ̂ = YN T − L

(4.1)

2
Ben-Michael, Feller, and Rothstein [2018] also propose a suite of methods that can be used when both
unit-specific covariates and lagged outcomes are available. For example, they propose first projecting out the
component of the outcomes that can be explained using the unit-specific covariates, and then running ASC on
the residuals.

14

where the parameters µ̂, α̂ and β̂ are as defined in (1.3). Here, we provide several results
bN T to LN T , implying that the error of τ̂ is asymptotically fully
establishing convergence of L
determined by the intrinsic noise in YN T . In the following section, we then build on these results
to provide methods for inference about τ in settings with more than one treated unit. Recall
that we assume that Yit is generated as below, and we follow the convention that “:” always
indexes over unexposed units or time periods.
Assumption 1. We have N, T → ∞, and there is a deterministic matrix L such that Yit =
Lit + Wit τ + εit with Wit = 1 {i = N, t = T } and εit ∼ N (0, σ 2 ), independently for each cell
(i, t).3
We consider two distinct sets of conditions: First, we examine the case where the two-way
fixed effects model is well specified, i.e., Lit = µ+αi +βt , and show that SDID is consistent under
very flexible conditions. The main point here is that using data-adaptive weights ω̂ and λ̂ does
not break DID when the outcome model is well specified. Second, we consider the generalized
fixed effects model, i.e., where we make only weak assumptions on L. Here, basic DID is
inconsistent; however, we show that SDID with penalized SC weights is consistent whenever
L is well-approximated by a matrix of rank r  min(N, T ) and can be consistent at the rate
p
log(T )/ min(N, T ) if it is well-approximated by a matrix of fixed rank.

4.1

Properties in the Well-Specified Two-Way Fixed Effects Model

Our first result shows that SDID is consistent and asymptotically normal in the well-specified
model, that is, where Lit = µ + αi + βt . Here we consider the following kernel weights:

ω̂i = P

1
i6=N



1
T −1

1



kYi: − YN : k22 ≤ cω

1
T −1



kYi: − YN : k22 ≤ cω

, λ̂t = P

1

t6=T



1
N −1

1



kY:t − Y:T k22 ≤ cλ

1
N −1



kY:t − Y:T k22 ≤ cλ

 , (4.2)

for i = 1, ..., N − 1 and t = 1, ..., T − 1, where cω and cλ are tuning parameters. For our result,
we also make generative assumptions that let us characterize the behavior of nearest neighbor
matching with noisy data; see Bonhomme, Lamadon, and Manresa [2017] for related results on
the behavior of clustering panel data.
3

We make this assumption for simplicity of exposition only. In Section 8.3 of the appendix we state and prove
results for heteroskedastic and auto-correlated subgaussian errors, and also for choices of weights ω̂, λ̂ that we
do not consider here.

15

Assumption 2. Lit = µ + αi + βt ; δα,i := |αi − αN | and δβ,t := |βT − βt | are i.i.d. random
variables such that corresponding densities fδα and fδβ are bounded at zero.
Theorem 1. Suppose Assumptions 1 and 2 hold and lim N/T = ρ ∈ (0, 1); then for the weights
bN T is consistent, that is,
ω̂i and λ̂t defined above we have the following: L
bN T − LN T →p 0,
L
and
1
q



b N T − LN T
L



→ N (0, σ 2 )

(4.3)

kω̂: k2 + kλ̂: k2

)
)
√
√
, cω = σ 2 +o(1), aN,T → ∞, and cλ = σ 2 +bN,T log(T
, cλ = σ 2 +o(1),
provided cω = σ 2 +aN,T log(N
T
N

bN,T → ∞
Note that matching discrepancies cλ and cω in (4.2) do not go to zero, instead they go to σ 2 ,
because with N and T large all rows and columns of Y will have distances that concentrate at
σ 2 away. We also note that the weighting function considered here is approximately equivalent
√
to k-nearest neighbors weighting, where kω = T cω − σ 2 is approximately the number of units
√
that we average over and kλ = T cλ − σ 2 is the approximate number of used time periods.

4.2

Double Robustness Part I: The Fixed Effects Model with General Weights

Next we show that, if fixed-effects model is correct, then our method is consistent essentially
regardless of the weights we use. Instead of requiring a specific functional form for the weights,
we only ask that we not use the T -th time period when picking the row weights ω̂, and we
not use the N -th row when picking λ̂, and that the weights are not too concentrated on a few
units or time periods. All algorithms considered in this paper, ranging from synthetic control
weighting to nearest neighbor matching, satisfy this condition.
Assumption 3. We choose weights such that ω̂: ⊥
⊥ Y:T and λ̂: ⊥
⊥ YN : .

16

Theorem 2. Under Assumption 1, suppose moreover that Lit = µ + αi + βt . Then, provided we
use weights ω̂ and λ̂ satisfying Assumption 3 such that
p
max{N, T }kω̂: k2 kλ̂: k2 →p 0,

kω̂: k2 , kλ̂: k2 →p 0,

(4.4)

bN T − LN T →p 0.
we have L

4.3

Double Robustness Part II: The Approximate Factor Model with
SC Weights

In this section we relax the modeling assumptions from the above section, and simply require
that L be approximable by a low-rank matrix. This type of model was used to motivate the
SC approach by Abadie, Diamond, and Hainmueller [2010], and has also been studied in other
contexts by, e.g., Athey, Bayati, Doudchenko, Imbens, and Khosravi [2017] and Bai [2009]. Our
goal is to show that, with well chosen weights, SDID remains consistent. Here, we focus on a
form of penalized synthetic control weights:

ω̂ sc (aω ) = arg min


T −1
X

ω∈W 

λ̂sc (aλ ) = arg min
λ∈L


−1
N
X


YN t −

t=1

i=1

N
−1
X

!2
ωi Yit

: kω: k2 ≤ aω




,



!2
T −1

X
λt Yit
: kλ: k2 ≤ aλ ,
−

i=1

YiT

(4.5)

t=1

where aλ and aω are tuning parameters. The penalization is important to ensure that in large
samples there will be many units and time periods with positive weights.
The key difficulty in showing that these SC weights ω̂ and λ̂ were chosen to balance rows and
columns of Y ; however, what we really need for useful inference with an approximately low-rank
L is for ω̂ and λ̂ to balance the the rows and columns of L. Furthermore, the weights defined in
(4.5) have a complicated dependence on the noise ε = Y −L, and the panel structure means that
we cannot address this challenge via sample splitting as in, e.g., Chernozhukov, Chetverikov,
Demirer, Duflo, Hansen, Newey, and Robins [2018a]. Here, we establish conditions under which
our SDID estimator with data-dependent weights (4.5) is consistent in the approximately lowrank model. As an additional benefit, we also prove that the basic SC estimator is consistent in
17

the motivating model from Section 2.2 of Abadie, Diamond, and Hainmueller [2010].
In order to spell out our result, we first define infeasible SC weights that balance the underlying effect matrix L rather than the observed matrix Y :

ω ? (aω ) = arg min
ω∈W

λ? (aλ ) = arg min
λ∈L


T −1
X

t=1

−1
N
X


LN t −

N
−1
X

!2
ωi Lit

: kω: k2 ≤ aω




,



!2
T −1

X
−
λt Lit
: kλ: k2 ≤ aλ ,

i=1

LiT

i=1

(4.6)

t=1

We then the following identification assumption in terms of these weights. Specifically we ask
that these population weights succeed in obtaining balance, i.e., the last row and column of the
matrix can in fact be usefully represented via a convex combination of other rows. Given this
assumption, SDID with penalized SC weights is consistent.
Theorem 3. Given Assumption 1, and that we choose weights via (4.5) with aω and aλ satisfying
δω = kLN : − ω:? (aω )0 L:: k;
δλ = kL:T − L:: λ?: (aλ )k;
δsdid = |LN T − (ω:? (aω )0 L:T + LN : λ?: (aλ )) − ω:? (aω )0 L:: λ?: (aλ )| .
Then for rλ , rω defined in Lemma 4,
b N T − LN T
L


h
p
√ i
= OP δsdid + aλ δω + σ min{ log(N ), aω N }
h
p
√ i
+ aω δλ + σ min{ log(T ), aλ T }

+ min(aω rλ , aλ rω ) .

In the case that N/T → κ ∈ (0, ∞), σ = O(1), L has exact (rather than approximate) low rank,
√
and we choose aλ , aω , = O(1/ N ), this bound simplifies to
r
bN T − LN T = OP
L

δsdid +

max {log(N ), rank(L), δω , δλ }
N

18

!
.

The key technical result underlying Theorem 3 is the following lemma, which establishes
convergence of the feasible SC weights (4.5) that balance Y to the infeasible weights (4.6) that
balance L. We emphasize that our result does not rely on the weights ω̂: and λ̂: converging to
ω̂:? and λ̂?: respectively at a particularly fast rate. In our analysis, we only use the trivial bounds
kω̂: − ω̂:∗ k ≤ kω̂: k + kω̂:? k, etc., and in fact the weights ω̂: and λ̂: do not appear to be particularly
stable empirically. Rather, our result only relies on the feasible and oracle weights having similar
“balancing” properties, i.e., for L0:: (ω̂: − ω:? ) and L:: (λ̂: − λ?: ) to be small as established below.
Lemma 4. Given Assumption 1, choose weights via (4.5) with aω , aλ . Then in terms of δω , δλ
defined in Theorem 3,
kL0:: (ω̂: − ω:? (aω ))k2 = OP (rω ) and kL:: (λ̂: − λ?: (aλ ))k2 = OP (rλ ),
where

p
rω = max δω , σ approx-rankω ,
r

√ √

np
√ o
√
4
σ aω max
N , N T log(N ), σ T aω , σ min
log(N ), aω N
;

p
rλ = max δλ , σ approx-rankλ ,
r

√ √

np
√
√ o
4
σ aλ max
T , N T log(T ), σ N aλ , σ min
log(T ), aλ T
.
Here approx-rankλ and approx-rankω are lower bounds on the rank of L:: that ignore small
nonzero singular values,




np
o 
√
s2k , sr+1 min
log(N ), aω N  ,
approx-rankω = min
r ≥ σ −1 min aω
r∈1,2,... 

k>r



sX

np
o 
√
approx-rankλ = min
r ≥ σ −1 min aλ
s2k , sr+1 min
log(T ), aλ T  ,
r∈1,2,... 



sX

k>r

where s1 , s2 , . . . is the decreasing sequence of singular values of L:: .
Finally, as discussed above, we can also use Lemma 4 to prove consistency of SC estimation
19

in the approximately low-rank model under the assumptions of Theorem 3. The main difference
between Theorem 5 and Theorem 3 above is that the error depends on the performance of an
oracle SC estimator rather than that of the the oracle SDID estimator.
Theorem 5. Given Assumption 1, choose ω̂ via (4.5) with constant aω . Then in terms of
δsc = |ω:∗ (aω ) · L:T − LN T | and rω defined in Lemma 4,

|ω̂: (aω ) · Y:T − LN T | = Op

4.4

h
i
δsc + aω + min rω kλ̃k2 + aω kL:T − L:: λ̃k2 .

(4.7)

λ̃∈RT −1

Asymptotic Properties with Multiple Exposed Units and Time
Periods

We will now consider the problem of inference in a simple setting with N1 exposed units and
T1 exposed time periods, in which all exposed units start treatment at the same time T0 + 1.
Relative to the setting of the previous section, we also consider autocorrelated errors.
Assumption 4. We have N0 , T0 → ∞, and there is a deterministic N0 + N1 × T0 + T1 matrix L
such that Yit = Lit + Wit τ + εit with Wit = 1 {i > N0 , t > T0 } and the rows of ε are independent
and distributed according to the gaussian AR(1) process εi,t+1 = ρεi,t + ξi,t+1 with ρ ∈ [0, 1) and
iid shocks ξi,t ∼ N (0, σξ2 ).
In this setting, our essential result is that our estimator τ̂ is asymptotically normal and
unbiased when our total number N1 T1 of treated observations is small relative to the number of
untreated observations and our number of treated units N1 is small relative to our number of
post-treatment periods T1 . The following theorem formalizes this. Its proof, as well as a more
complete discussion of our estimator in this setting, appears in Section 8.5 of the appendix.
Theorem 6. Under Assumption 4, let τ̂ be defined as in (4.1) with weights

ω̂ = arg min


T0
X

ω∈W 

λ̂ = arg min
λ∈L

t=1


−1
N
X


i=1

1
N1

N1
X

Yit −

N0
X

!2
ωi Yit

: kω: k2 ≤ aω





i=1
i=N0 +1

!2
T0

X
X
1
Yit −
λt Yit
: kλ: k2 ≤ aλ .

T1 i=T +1
t=1
0

20

,

In terms of the correponding oracle weights ω ? , λ? defined by substituting L for Y in the definitions above, let

δω =

kN1−1

NX
0 +N1

Li: − ω?0 L:: k,

i=N0 +1

δλ = kT1−1

TX
0 +T1

L:t − L:: λ? k,

t=T0 +1

δsdid = (N1 T1 )

−1

NX
0 +T1
0 +N1 TX

Lit − (ω?0 L:T + LN : λ? − ω?0 L:: λ? ) .

i=N0 +1 t=T0 +1
−1/2

If we choose aω . N0

−1/2

and aλ . T0

, then

NX
0 +N1 TX
0 +T1
p
1
√
N1 T1 (τ̂ − τ ) =
εit + op (1),
N1 T1 i=N +1 t=T +1
0

(4.8)

0

provided the following conditions hold:



T0
N0
N1 T1  min 2 ,
,
;
δsdid max {δλ2 , 1} max {δω2 , 1}
)
(
1/2
N
N
0
1/2
0
;
,
N1 T1  min
1/2
log(T
0)
T0 log(T0 )




N0
 .

N1  min T1 ,
−1/2 

approx-rank L , T
1

::

1

The asymptotic characterization (4.8) implies that under the stated conditions, our estimator
is asymptotically normal with variance Vτ on the order of (N1 T1 )−1 .

5

Large-Sample Inference of Treatment Effects

In the literature on synthetic controls, the dominant approach to uncertainty quantification is
via placebo tests [Abadie, Diamond, and Hainmueller, 2010, 2015]. The main idea is to consider
the behavior of synthetic control estimation when we replace the unit that was in fact exposed to
21

the treatment with different units that were not exposed. Such a placebo test is closely connected
to permutation tests in randomization inference. However, in many applications of synthetic
controls, the exposed unit was not chosen at random, in which case placebo tests do not have the
formal properties of randomization tests [Firpo and Possebom, 2018, Hahn and Shi, 2016], and
so may need to be interpreted via a more qualitative lens. Here, we take a different perspective,
and consider inferential methods motivated by large sample asymptotics. Our proposal builds on
methods for robust inference in large panels that were originally developed for the well-specified
two-way fixed effects models [e.g., Arellano, 2003, Hansen, 2007, Liang and Zeger, 1986].
As shown in Theorem 6, the synthetic difference in differences estimator is asymptotically
Gaussian under appropriate conditions,
(τ̂ − τ )



Vτ1/2 ⇒ N (0, 1) ,

(5.1)

where the asymptotic variance Vτ is determined by the sampling errors {εit : i > N0 , t > T0 } of
the observations under treatment and does not depend on the noise in the synthetic control
weights ω̂ or λ̂. The uphot is that we can estimate Vτ and build confidence intervals for τ using
standard methods for large-sample inference for weighted panels, while treating ω̂ and λ̂ as fixed.
Here, we focus on estimating Vτ by applying the jackknife [Miller, 1974, Efron and Stein,
1981] to the weighted regression (2.3)—again, with ω̂ and λ̂ treated as fixed. Following Bertrand,
Duflo, and Mullainathan [2004], we seek robustness to errors that may be correlated within rows,
and so we repeatedly run the regression (2.3) with one row i omitted at a time to get τ̂ (−i) , and
use the variation of these τ̂ (−i) to get an estimate the variance Vτ of the original treatment effect
estimate τ̂ . Although we do not do so here, one could also estimate Vτ via other methods for
heteroskedasticity-consistent variance estimation [Efron, 1982, MacKinnon and White, 1985].
In terms of connections to the literature, we note that a qualitatively result was used by
Bonhomme and Manresa [2015] to provide large-sample inference for panels with grouped fixed
effects: They rely on clustering to discover groups, but then show that inference remains asymptotically valid while ignoring the effect of clustering. Meanwhile, Chernozhukov, Wuthrich, and
Zhu [2017] propose a method for inference in synthetic control problems with a single treated
unit that relies on the prediction residuals Yit − L̂it over the control units being representative
of the counterfactual untreated residuals Lit + εit − L̂it over the treated units.
Finally, we note that although the above discuss has focused on inference that is robust to
22

within-row correlations, our jackknife-based procedure can be flexibly adapted to reflect different
sampling assumptions. If we believe that the εit were all independent, we could also apply a
cell-wise jackknife (i.e., where the jackknife omits one cell rather than one row at a time),
potentially allowing for power gains when there are few treated units. Meanwhile, if we believe
the εit may be correlated within rows but that the noise process eventually mixes (i.e., there are
no long-range correlations), we could consider and intermediate solution that divides each row
into blocks and then applies a block-wise jackknife [Kunsch, 1989].

6

Empirical Evaluation

6.1

Placebo Evaluation: Predicting the Prevalence of Smoking

In one of the original studies on synthetic control methods, Abadie, Diamond, and Hainmueller
[2010] focus on estimating the causal effect of anti-smoking legislation in California (Proposition
99). As discussed above, when only a single cell (N, T ) is treated, synthetic control methods
can be understood as producing a prediction L̂N T of what the unit-N time-T outcome would
have been without treatment, and then estimating τ̂ = YN T − L̂N T . This suggests a simple
placebo procedure for evaluating various synthetic control methods in a realistic environment:
If we run synthetic control methods while singling out as “treated” a cell (n, t) that did not
actually receive treatment, we should expect L̂nt to be a good prediction of the realized outcome
Ynt . Here, we benchmark difference in differences, synthetic controls, and synthetic differences
in differences against each other by running such a placebo analysis, and comparing the errors
of each method in predicting Ynt .
The original dataset of Abadie, Diamond, and Hainmueller [2010] had observations for 39
states (including California) from 1970 through 2000, where California is treated from 1989
onwards. We follow Abadie, Diamond, and Hainmueller [2010] in using per capita smoking as
the outcome. Here, we focus only on time periods 1970-1988, in which none of the units were
treated, and seek to predict the outcome of a focal cell using all data from earlier years as well
as data from different states in the target year by running different methods with the focal cell
considered as “treated”. For example, when predicting the outcome for Arizona in 1985, we run
the methods under comparison with the other 38 states used as the “control states” and the
years 1970-1984 used as the “pre-treatment years” (and we do not used any data from 1986 or
23

50

●

20
10

●

●
●
●
●
● ●

5

synthetic control RMSE

●

●
●

●

●●
●
●●●
● ●

●

2

●
●

●

●
● ●
●●
●
●
● ●
●
●

●

● ●

●

2

4

6

8

synthetic diff−in−diff RMSE

Figure 1: Comparison of the per-state root-mean squared error for SDID and SC. California
is highlighted in blue.
later).
Given this setup, we make predictions for all states in years 1980-1988 (i.e., we run DID, SC
and SDID separately for each focal state-year pair), and calculate the square root of the average
squared error:
v
u 1988 
2
u1 X
RMSEi = t
Yi,t − L̂i,t ,
9 t=1980
for all 39 states. In this example, we use L2 -penalized SC weights

ω̂ sc = arg min
ω∈W




1
T − 1

T −1
X
t=1

YN t −

N
−1
X

!2
ωi Yit

+ ζ kωk22




,

(6.1)



i=1

where we set ζ to be the average of (Yi,t+1 − Yi,t )2 over the pre-treatment data. For time weights
λ̂, we use an analogously penalized version of the intercept weights λisc
t to deal with the trends
in smoking rates.
We report the results on the RMSE in Figure 1 for each state and the average over all 39
states; Table 3 in the Appendix has detailed results. We find that the SDID method does
substantially better than the SC and DID method in terms of predictive accuracy, with the
SC outperforming the DID method: in Figure 1 almost all the pairs of RMSE lie above the 45
24

degree line, showing that the average RMSE based on the SDID estimator is lower than that
based on the SC estimator, for almost every state. The median improvement of the state-wise
root-mean-squared error of SDID over the state-wise root-mean-squared error of SC is 15% (and
the corresponding improvement over DID is 50%).
We can gain further insight into the behavior of different methods by comparing the onestep-ahead predicted trajectories L̂i,t to the true ones Yi,t . We see that SC struggle when a
state doesn’t fit neatly within the convex hull of other states (e.g., in the case of Utah), whereas
difference-in-differences does poorly when the temporal pattern of a state doesn’t match the
average temporal pattern (e.g., in Alabama). Of course, it is unlikely that practitioners would
use SC to study a state that does not fit within the convex hull of other states, as is the case of
Utah here, as standard goodness of fit checks would flag SC as an inappropriate method to use
here. However, we find that SDID out-performs SC in states where the latter are appropriate
(such as California), and remain robust in cases where the latter are not (such as Utah).

6.2

Simulation Results: Point Estimation

In this section we assess the properties of the proposed SDID estimator relative to the DID
and SC estimators in finite samples using a simulation study. As in the above placebo study,
we consider a setting with no treatment effect, and run all methods as though a single unit in
cell (N, T ) had been treated; then, we measure the accuracy of L̂N T as an estimate for LN T .
In all our examples, the data is drawn as Yit ∼ N (Lit , σ 2 ), independently for each (i, t) pair.
Meanwhile, the N ×T signal matrix L is low rank, L = U V > , where U ∈ RN ×R and V ∈ RT ×R
for a rank parameter R.
The key choice is in how we generate this low-rank matrix L. First, we consider a simulation
where the N -th row and the N -th column of L are “typical”; formally, we generate L via an
exchangeable process, such that Uil ∼ Exp(1) and Vtl ∼ Exp(1) independently for each (i, l)
and (t, l). Second, we consider a case where the focal row and column are not “typical”, and
p
in particular the rows and columns are not exchangeable. Here, we draw Uil ∼ Pois( i/N ) for
p
each (i, l), and Vtl ∼ Pois( t/T ) for each (t, l). Note that the N -th row and T -th column will
on average have relatively large observations.
In all our simulations, we use consider penalized SC weights as in (6.1), with ζ set to the
sample variance of the Yit . Below, we first generate a random L, and then simulate Y 20 times
25

130
120
110
90

100

smoking [packs per capita]

120
115
110
105

smoking [packs per capita]

100
95
1980

1982

1984

1986

1988

1980

1982

year

1984

1986

1988

1986

1988

year

California

90
80

160

60

70

smoking [packs per capita]

240
220
200
180

smoking [packs per capita]

260

100

280

Alabama

1980

1982

1984

1986

1988

1980

year

1982

1984
year

New Hampshire

Utah

Figure 2: Predictions for per capita smoking rates for selected states, using as training data
all years prior to the year indicated on the x-axis. The true yearly per-capita smoking Yi,t is in
black. SDID estimates are in red. SC estimates are in blue. DID estimates are in teal.

26

given this L. This lets us separate the contributions of bias and variance to the error. We
report for the two designs, for different values of σ 2 and the rank R, and for different pairs
of (N, T ), the root-mean-squared-error and mean-absolute-bias for the three estimators, DID,
SC, and SDID. We report results in Tables 1 and 2. In the Appendix, we also show results for
unpenalized SC (ζ = 0), in Tables 4 and 5. We find that in all cases the SDID estimator has
substantially better bias properties than the DID and SC estimators, and in most cases also has
better root-mean-squared-error.

6.3

Simulation Results: Confidence Intervals

Finally, we study the properties of confidence intervals derived via the weighted regression perspective of SDID. We work in the same data-generating distribution as for the “non-exchangeable”
example in Section 6.2, except now with multiple treated units. Units 1 : . . . , N0 are control
units, and units N0 + 1, . . . , N0 + N1 = N are treated from period T0 + 1 onwards. Writing Wit
for the treatment indicator, we draw data as

Yit = Lit + Wit τ + εit , εit ∼ N 0, σ 2 .
We use weights ω̂i = 1/N1 for i = N0 + 1, .., N , and

sc
ω̂1:N
0


T0
1 X
= arg min
 T0
t=1

1
N1

N
X
j=N0 +1

Yjt −

N0
X

!2
ωi Yit

ζ
kωk22 : ωi ≥ 0,
N1

+

i=1

N0
X
i=1



wi = 1 , (6.2)


and pick λ̂ analogously. As above, we set ζ to the sample variance of the Yit The, given these
weights, we estimate

τ̂ = arg min

(
X

)
(Yit − µ − αi − βt − Wit τ )2 ω̂i λ̂t

.

(6.3)

i, t

We perform inference via heteroskedasticity-consistent standard error as provided in the R package sandwich [Zeileis, 2004]. We estimate variance via the jackknife [Miller, 1974], which corresponds to HC3 standard errors of MacKinnon and White [1985]. We run the weighted regression
as though ω̂i and λ̂t were deterministic and did not depend on the data.
27

N
50
50
50
50
50
50
50
50
200
200
200
200

T
50
50
50
50
200
200
200
200
200
200
200
200

root-mean sq. error mean absolute bias
σ rank DID SC SDID DID SC SDID
0.5
2
1.56 0.47 0.24
0.86 0.21 0.07
0.5
5
1.96 1.10 0.57
1.40 0.70 0.33
2
2
1.45 1.04 0.89
0.87 0.44 0.24
2
5
2.20 1.54 1.15
1.52 0.92 0.52
0.5
2
1.22 0.39 0.17
0.79 0.11 0.04
0.5
5
2.09 0.65 0.44
1.46 0.41 0.22
2
2
1.22 0.64 0.68
0.75 0.26 0.16
2
5
2.40 1.17 1.04
1.52 0.66 0.42
0.5
2
1.38 0.29 0.11
0.87 0.11 0.02
0.5
5
2.19 0.77 0.30
1.56 0.44 0.13
2
2
1.27 0.51 0.53
0.81 0.21 0.11
2
5
2.38 1.12 0.72
1.64 0.58 0.24

Table 1: Simulation study with an exchangeable distribution for L and penalized SC weights.
Results are aggregated over 400 draws of the low-rank L matrix and 25 draws of Y for each L
(for a total of 10,000 simulation replications).

N
50
50
50
50
50
50
50
50
200
200
200
200

T
50
50
50
50
200
200
200
200
200
200
200
200

root-mean sq. error mean absolute bias
σ rank DID SC SDID DID SC SDID
0.5
2
1.70 0.58 0.31
1.08 0.29 0.09
0.5
3
1.99 0.95 0.46
1.35 0.51 0.20
2
2
1.55 1.07 1.00
1.03 0.58 0.33
2
3
1.76 1.30 1.14
1.22 0.75 0.44
0.5
2
1.40 0.30 0.19
1.01 0.14 0.05
0.5
3
2.08 0.65 0.37
1.37 0.29 0.12
2
2
1.49 0.83 0.83
0.98 0.40 0.25
2
3
2.02 1.07 0.96
1.42 0.62 0.37
0.5
2
1.86 0.67 0.18
1.14 0.17 0.03
0.5
3
2.07 0.53 0.21
1.35 0.24 0.06
2
2
1.63 0.70 0.64
1.09 0.35 0.16
2
3
1.89 0.88 0.68
1.31 0.49 0.21

Table 2: Simulation study with an non-exchangeable distribution for L and penalized SC
weights. Results are aggregated over 400 draws of the low-rank L matrix and 25 draws of Y
for each L (for a total of 10,000 simulation replications).

28

3

●

6

●

●

●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●

2
1
0

Sample Quantiles

●

−4

−1
−2
●

−3

2
0

Sample Quantiles

4

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●

−2
−4

●

●

●
●●

−2

0

2

4

●

−4

Theoretical Quantiles

standardized DID errors

−2

0

2

4

Theoretical Quantiles

standardized SDID errors

d [τ̂ ]1/2 , for the
Figure 3: Standard Gaussian QQ-plot of the standardized errors (τ̂ − τ )/Var
independent design for both DID and SDID, aggregated across 10,000 simulation replications.
Points along the diagonal (dashed) would indicate perfectly calibrated Gaussian standard errors.
Points along a centered line with a slope shallower than 45 degrees indicate that confidence
intervals are conservative.
We generated data as in the non-exchangeable case above, with N = 100, N1 = 20, T =
120, T1 = 5, σ = 2, τ = 1, and rank set to 2. It appears that SDID confidence intervals
were well calibrated albeit slightly conservative: Nominal 95% confidence intervals achieved
98% coverage. The slight conservativeness may be due to the well-known mild upward bias of
jackknife variance estimates [Efron and Stein, 1981]. In contrast, a basic difference-in-differences
regression (6.3) but without weights ω̂ and λ̂ did poorly: Nominal 95% confidence intervals
achieved 82% coverage. Figure 3 shows a Gaussian QQ-plot of the standardized errors of both
DID and SDID, mirroring the observation that SDID confidence intervals are well calibrated
where DID ones are not.
To address a well-known critique of Bertrand et al. [2004] we also consider a design with
dependent errors. The data is generated in the same way as above, but now the errors are
correlated:
E[εit εil ] = ρ|t−l|
We set ρ = 0.7 leaving all other parameters the same. To deal with the correlation in errors, we
29

●

4
−2

0

2

●●
●●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●

−4

●

−4

●

●

Sample Quantiles

2
0
−2
−4

Sample Quantiles

4

●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●

●

●●

●

−2

0

2

4

−4

Theoretical Quantiles

standardized DID errors

−2

0

2

4

Theoretical Quantiles

standardized SDID errors

d [τ̂ ]1/2 , for the
Figure 4: Standard Gaussian QQ-plot of the standardized errors (τ̂ − τ )/Var
correlated design for both DID and SDID, aggregated across 10,000 simulation replications.
Points along the diagonal (dashed) would indicate perfectly calibrated Gaussian standard errors.
estimate the variance using row-based jackknife. We run the weighted regression as though ω̂i
and λ̂t were deterministic and did not depend on the data.
Nominal 95% confidence intervals based on SDID now achieve 93% coverage, while those
based on simple DID estimator achieve 88%. Figure 4 shows a Gaussian QQ-plot of the standardized errors of both DID and SDID, indicating that again SDID is better calibrated than
DID.

7

Conclusion

We present a new estimator in a Synthetic Control setting, he synthetic difference in differences (SDID) estimator, which can be interpreted as a doubly weighted DID estimator. We find
that the new estimator has attractive double robustness properties compared to the SC and
DID estimators, both in simulations, in an application, and based on formal large N and large
T symptotic results. By putting the new estimator as well as the original SC estimator in a
weighted regression framework it allows us to connect the SC methodology to regression methods, which suggests alternative ways for accomodating time-invariant as well as time-varying
covariates, as well as generalizations from two-way fixed effect models to factor models.
30

References
Alberto Abadie and Javier Gardeazabal. The economic costs of conflict: A case study of the
basque country. American Economic Review, 93(-):113–132, 2003.
Alberto Abadie and Guido W Imbens. Large sample properties of matching estimators for
average treatment effects. Econometrica, 74(1):235–267, 2006.
Alberto Abadie and Jérémy L’Hour. A penalized synthetic control estimator for disaggregated
data, 2016.
Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for comparative case studies: Estimating the effect of California’s tobacco control program. Journal
of the American Statistical Association, 105(490):493–505, 2010.
Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Comparative politics and the synthetic
control method. American Journal of Political Science, pages 495–510, 2015.
Manuel Arellano. Panel data econometrics. Oxford university press, 2003.
Susan Athey and Guido W Imbens. Design-based analysis in difference-in-differences settings
with staggered adoption. Technical report, National Bureau of Economic Research, 2018.
Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi.
Matrix completion methods for causal panel data models. arXiv preprint arXiv:1710.10251,
2017.
Susan Athey, Guido W Imbens, and Stefan Wager. Approximate residual balancing: debiased
inference of average treatment effects in high dimensions. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 80(4):597–623, 2018.
Jushan Bai. Panel data models with interactive fixed effects. Econometrica, 77(4):1229–1279,
2009.
Jushan Bai and Serena Ng. Determining the number of factors in approximate factor models.
Econometrica, 70(1):191–221, 2002.

31

Alexandre Belloni, Victor Chernozhukov, and Christian Hansen. High-dimensional methods and
inference on structural and treatment effects. The Journal of Economic Perspectives, 28(2):
29–50, 2014.
Eli Ben-Michael, Avi Feller, and Jesse Rothstein. New perspectives on the synthetic control
method. Technical report, UC Berkeley, 2018.
Marianne Bertrand, Esther Duflo, and Sendhil Mullainathan. How much should we trust
differences-in-differences estimates?

The Quarterly journal of economics, 119(1):249–275,

2004.
Stéphane Bonhomme and Elena Manresa. Grouped patterns of heterogeneity in panel data.
Econometrica, 83(3):1147–1184, 2015.
Stéphane Bonhomme, Thibaut Lamadon, and Elena Manresa. Discretizing unobserved heterogeneity. Technical report, IFS Working Papers, 2017.
David Card. The impact of the mariel boatlift on the miami labor market. Industrial and Labor
Relation, 43(2):245–257, 1990.
Carlos Carvalho, Ricardo Masini, and Marcelo C Medeiros. Arco: an artificial counterfactual
approach for high-dimensional panel time-series data. Journal of Econometrics, 207(2):352–
380, 2018.
Victor Chernozhukov, Kaspar Wuthrich, and Yinchu Zhu. An exact and robust conformal
inference method for counterfactual and synthetic controls. arXiv preprint arXiv:1712.09089,
2017.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural
parameters. The Econometrics Journal, 21(1):C1–C68, 2018a.
Victor Chernozhukov, Juan Carlos Escanciano, Hidehiko Ichimura, Whitney K Newey, and
M Robins. Locally robust semiparametric estimation. arXiv preprint arXiv:1608.00033, 2018b.
Victor Chernozhukov, Kaspar Wuthrich, and Yinchu Zhu. Inference on average treatment effects
in aggregate panel data settings. arXiv preprint arXiv:1812.10820, 2018c.
32

Nikolay Doudchenko and Guido W Imbens. Balancing, regression, difference-in-differences and
synthetic control methods: A synthesis. Technical report, National Bureau of Economic
Research, 2016.
Bradley Efron. The jackknife, the bootstrap, and other resampling plans, volume 38. Siam,
1982.
Bradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statistics,
pages 586–596, 1981.
Sergio Firpo and Vitor Possebom. Synthetic control method: Inference, sensitivity analysis and
confidence sets. Journal of Causal Inference, 6(2), 2018.
Bryan S Graham, Cristine Campos de Xavier Pinto, and Daniel Egel. Inverse probability tilting
for moment condition models with missing data. The Review of Economic Studies, 79(3):
1053–1079, 2012.
Jinyong Hahn and Ruoyao Shi. Synthetic control and inference. Available at UCLA, 2016.
Jens Hainmueller. Entropy balancing for causal effects: A multivariate reweighting method to
produce balanced samples in observational studies. Political Analysis, 20(1):25–46, 2012.
Christian B Hansen. Asymptotic properties of a robust variance matrix estimator for panel data
when t is large. Journal of Econometrics, 141(2):597–620, 2007.
David A Hirshberg and Stefan Wager. Augmented minimax linear estimation. arXiv preprint
arXiv:1712.00038, 2018.
Kosuke Imai and Marc Ratkovic. Covariate balancing propensity score. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 76(1):243–263, 2014.
Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical
Sciences. Cambridge University Press, 2015.
Hans R Kunsch. The jackknife and the bootstrap for general stationary observations. The
Annals of Statistics, pages 1217–1241, 1989.

33

Guillaume Lecué and Shahar Mendelson. Learning subgaussian classes: Upper and minimax
bounds. arXiv preprint arXiv:1305.4825, 2013.
Kathleen T Li and David R Bell. Estimation of average treatment effects with panel data:
Asymptotic theory and implementation. Journal of Econometrics, 197(1):65–75, 2017.
Kung-Yee Liang and Scott L Zeger. Longitudinal data analysis using generalized linear models.
Biometrika, 73(1):13–22, 1986.
Christopher Liaw, Abbas Mehrabian, Yaniv Plan, and Roman Vershynin. A simple tool for
bounding the deviation of random matrices on geometric sets. In Geometric aspects of
functional analysis, pages 277–299. Springer, 2017.
James G MacKinnon and Halbert White. Some heteroskedasticity-consistent covariance matrix
estimators with improved finite sample properties. Journal of econometrics, 29(3):305–325,
1985.
Shahar Mendelson. Learning without concentration. In Conference on Learning Theory, pages
25–39, 2014.
Rupert G Miller. The jackknife-a review. Biometrika, 61(1):1–15, 1974.
Whitney K Newey, Fushing Hsieh, and James M Robins. Twicing kernels and a small bias
property of semiparametric estimators. Econometrica, 72(3):947–962, 2004.
Juan Peypouquet. Convex optimization in normed spaces: theory, methods and examples.
Springer, 2015.
James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients
when some regressors are not always observed. Journal of the American statistical Association,
89(427):846–866, 1994.
Daniel O Scharfstein, Andrea Rotnitzky, and James M Robins. Adjusting for nonignorable
drop-out using semiparametric nonresponse models. Journal of the American Statistical
Association, 94(448):1096–1120, 1999.
James H Stock and Mark W Watson. Heteroskedasticity-robust standard errors for fixed effects
panel data regression. Econometrica, 76(1):155–174, 2008.
34

William F Trench. Asymptotic distribution of the spectra of a class of generalized kac–murdock–
szegö matrices. Linear algebra and its applications, 294(1-3):181–192, 1999.
Roman Vershynin. High-dimensional probability: An introduction with applications in data
science, volume 47. Cambridge University Press, 2018.
Yiqing Xu. Generalized synthetic control method: Causal inference with interactive fixed effects
models. Political Analysis, 25(1):57–76, 2017.
Achim Zeileis. Econometric computing with hc and hac covariance matrix estimators. Journal
of Statistical Software, Articles, 11(10):1–17, 2004. doi: 10.18637/jss.v011.i10. URL https:
//www.jstatsoft.org/v011/i10.
José R Zubizarreta. Stable weights that balance covariates for estimation with incomplete
outcome data. Journal of the American Statistical Association, 110(511):910–922, 2015.

35

8

Appendix

Notation. Throughout the proofs section, we omit the “:” subscript from ω and λ when there
is no risk of ambiguity. In addition, we write ω? and λ? with the same meaning as ω ? and λ?
where conventient. We will use c to denote a universal constant, which may differ in value in
each instance. Many of our bounds are phrased in terms of the gaussian width of a set S ⊆ Rn ,
w(S) = E sups∈S hg, si where g ∈ Rn is a vector of iid standard gaussians, as well as the radius
rad(S) = sups∈S ktk and diameter diam(S) = sups,s0 ∈§ ks − s0 k. We will use results which, in
our references,
q may be phrased in terms of variants of gaussian width, γ(S) = E sups∈S |hg, si|
and h(S) = E sups∈S hg, si2 , and will express them here without comment in terms of either
w(S) or w(S − S) for S − S := {s − s0 : s, s0 ∈ S} using equivalences discussed in Vershynin
[2018, Section 7.6]. Unless otherwise specified, kvk will mean the euclidean norm kvk2 for a
vector v and kAk will mean the operator norm kAkop = supkvk2 ≤1 kAxk2 for a matrix A. The
L2 norm, subgaussian norm, and subexponential norms for a scalar random variable Z will
be written kZkL2 , kZkψ2 and kZkψ1 , and we extend them to random vectors Z by defining
kZkL2 = supkxk=1 k∠Z, xkL2 and the others analogously as in Vershynin [2018].

8.1

Proof of Theorem 2

When the fixed effects model is correctly specified, we can check that synthetic difference in
differences perfectly captures the signal for any set of weights, and the error depends only on
the noise ε:
bN T − LN T = ω̂ · ε:T + λ̂ · εN : − ω̂ 0 ε:: λ̂.
L
Next, by Assumption 3, we know that

ω̂ · ε:T ω̂ ∼ N 0, σ 2 kω̂k22 ,
and so by the first part of (4.4) the term ω̂ · ε:T converges in probability to 0; the same argument
also applies to λ̂ · εn: . Finally, for the last term, we invoke Cauchy-Schwarz to check that
ω̂ 0 ε:: λ̂ ≤ kω̂k2 kε:: kop kλ̂k2 = OP (kω̂k2 kλ̂k2

36

p
max{N, T }),

Alabama
Arkansas
California
Colorado
Connecticut
Delaware
Georgia
Idaho
Illinois
Indiana
Iowa
Kansas
Kentucky
Louisiana
Maine
Minnesota
Mississippi
Missouri
Montana
Nebraska
Nevada
New Hampshire
New Mexico
North Carolina
North Dakota
Ohio
Oklahoma
Pennsylvania
Rhode Island
South Carolina
South Dakota
Tennessee
Texas
Utah
Vermont
Virginia
West Virginia
Wisconsin
Wyoming

DID
12.95
16.24
8.79
7.18
6.25
3.89
12.68
7.60
2.40
6.31
4.45
6.29
9.24
5.42
4.25
6.43
8.09
5.98
6.98
2.84
27.34
42.52
1.75
30.35
6.98
9.59
8.11
8.55
6.58
8.74
3.44
17.22
7.93
4.26
6.49
2.18
4.34
5.57
12.27

SC SDID
3.41
2.46
5.03
2.81
3.37
1.81
4.66
3.81
2.79
2.40
5.26
3.04
3.61
2.42
2.55
2.24
3.07
3.08
4.36
3.46
4.77
5.12
3.92
3.59
18.52 4.62
2.71
2.48
5.01
5.62
3.56
3.72
2.31
1.88
2.14
1.82
4.31
3.41
1.31
1.40
8.10
7.90
48.37 8.72
2.38
2.65
9.96
5.10
5.37
4.15
2.58
1.33
4.88
4.27
2.47
2.32
6.90
6.73
2.69
2.24
2.28
2.41
5.94
3.15
4.21
3.58
23.59 3.89
3.85
4.05
2.51
2.39
4.13
3.42
3.36
3.30
8.15
6.87

Table 3: Root-mean squared error for one-step-ahead predictions made by difference in differences regression, SCs, and SDID. Results are averaged over the time period 1980-1988.
37

n
50
50
50
50
50
50
50
50
200
200
200
200

T
50
50
50
50
200
200
200
200
200
200
200
200

root-mean sq. error mean absolute bias
σ rank DID SC SDID DID SC SDID
0.5
2
1.56 0.32 0.33
0.86 0.09 0.05
0.5
5
1.96 0.82 0.47
1.40 0.38 0.14
2
2
1.45 1.15 1.20
0.87 0.39 0.25
2
5
2.20 1.51 1.42
1.52 0.73 0.43
0.5
2
1.22 0.38 0.27
0.79 0.07 0.04
0.5
5
2.09 0.50 0.42
1.46 0.19 0.09
2
2
1.22 0.84 0.98
0.75 0.25 0.18
2
5
2.40 1.22 1.24
1.52 0.54 0.35
0.5
2
1.38 0.27 0.21
0.87 0.06 0.04
0.5
5
2.19 0.58 0.30
1.56 0.19 0.05
2
2
1.27 0.63 0.79
0.81 0.18 0.14
2
5
2.38 1.10 0.96
1.64 0.45 0.21

Table 4: Simulation study with an exchangeable distribution for L and unpenalized SC
weights. Results are aggregated over 400 draws of the low-rank L matrix and 25 draws of Y for
each L (for a total of 10,000 simulation replications).

n
50
50
50
50
50
50
50
50
200
200
200
200

T
50
50
50
50
200
200
200
200
200
200
200
200

root-mean sq. error mean absolute bias
σ rank DID SC SDID DID SC SDID
0.5
2
1.70 0.47 0.36
1.08 0.16 0.07
0.5
3
1.99 0.80 0.46
1.35 0.32 0.12
2
2
1.55 1.15 1.26
1.03 0.51 0.32
2
3
1.76 1.36 1.38
1.22 0.65 0.41
0.5
2
1.40 0.30 0.28
1.01 0.09 0.05
0.5
3
2.08 0.57 0.40
1.37 0.17 0.08
2
2
1.49 0.97 1.06
0.98 0.37 0.25
2
3
2.02 1.16 1.17
1.42 0.56 0.34
0.5
2
1.86 0.61 0.24
1.14 0.12 0.04
0.5
3
2.07 0.42 0.27
1.35 0.13 0.05
2
2
1.63 0.75 0.84
1.09 0.30 0.17
2
3
1.89 0.89 0.88
1.31 0.41 0.20

Table 5: Simulation study with an non-exchangeable distribution for L and unpenalized
SC weights. Results are aggregated over 400 draws of the low-rank L matrix and 25 draws of
Y for each L (for a total of 10,000 simulation replications).

38



recalling that, under Assumption 1, it is known that E kε:: k2op = O(max{N, T }).

8.2

Proof of Theorem 1

We start with the following high-level lemma.
Lemma 7. Suppose that Assumption 1 is satisfied, further assume that the following conditions
hold:
kω? k2 = o(1)
kλ? k2 = o(1)
kλ̂ − λ? k2 = op (kλ? k2 )

(8.1)

kω̂ − ω? k2 = op (kω? k2 )
p
kω̂ − ω? k1 = op (1/ log(T ))
p
kλ̂ − λ? k1 = op (1/ log(N ))
Then we have the following result:
ω̂ 0 ε:: λ̂ = op (max{kω? k2 , kλ? k2 })

(8.2)

Proof. We decompose ω̂ T Σ:: λ̂ into a sum of four terms ξ1 + ξ2 + ξ3 + ξ4 and bound each term:
ω̂ T ε:: λ̂ = ω?0 ε:: λ? + ω?0 ε:: (λ̂ − λ? ) + (ω̂ − ω? )0 ε:: λ + (ω̂ − ω? )0 ε:: (λ̂ − λ? ) (8.3)
Our goal is to show that these terms are negligible:
ξk = op (max{kω? k2 , kλ? k2 })

(8.4)

For the first term we get the following:
ξ1 ∼ N (0, kω? k22 kλ? k22 ) ⇒ ξ1 = Op (kω? k2 kλ? k2 ) = op (max{kω? k2 , kλ? k2 })

(8.5)

The second term ξ2 is bounded, via Hölder’s inequality, by kω?0 ε:: k∞ kλ̂ − λ? k1 . The first factor
39

is the maximum of T − 1 independent mean-zero gaussians with variance σ 2 kωk22 , which is
p
p
Op (kω? k2 log(T )), and the second is op (1/ log(T )) by assumption, so the product is op (kω? k2 ).
The third term ξ3 is analogously op (kλ? k2 ).
To bound the fourth term ξ4 , we use Chevet’s inequality [Vershynin, 2018, Theorem 8.7.1],
E sup x0 εy ≤ rad(X) w(Y ) + w(X) rad(Y ).
x∈X,y∈Y

In essence, this is a uniform version of the same Hölder’s inequality bound, allowing us to bound
the simultaneous supremum over X and Y as if either x ∈ X were a constant vector of length
rad(X) or y ∈ Y were a constant vector of length rad(Y ). Here we can take X to be a set
p
of the form {x : kxk1 ≤ a/ log(T ), kxk2 ≤ bkω? k2 }, for a → 0, which will contain ω̂ − ω?
with high probability under our assumptions, and define Y analogously in terms of λ̂ and λ? .
p
p
Then w(X) . a/ log(T ) · log(T ) → 0 and rad(X) . kω? k and analogously w(Y ) → 0 and
rad(Y ) . kλ? k, which shows that ξ4 = op (max{kω? k2 , kλ? k2 }).
We now move to prove the claimed result. Define deterministic weights:
ωi? = P

1 ({(βt − βT )2 ≤ c̃λ })
1 ({(αi − αN )2 ≤ c̃ω })
?
P
,
λ
=
,
t
2
2
i6=N 1 ({(αi − αN ) ≤ c̃ω })
t6=T 1 ({(βt − βT ) ≤ c̃λ })

(8.6)

where c̃ω = cω − σ 2 and c̃λ = cλ − σ 2 .
First we verify that conditions for Lemma 7 hold for ω̂ and ω ? . Results for λ̂ and λ? follow
in the same way. Define the following random variables:
K=

X

1



(αi − αN )2 ≤ c̃ω



i6=N

X 
K̂ =
1
i6=N

1
kYi: − YN : k22 ≤ cω
T −1



(8.7)

By definition we have the following:
ωj∗ =
ω̂j =

1{δj2 ≤ c̃ω )}
K
2
1{δ̂j ≤ cω }

(8.8)

K̂

40

where δ̂j2 =

1
T −1

kYi: − YN : k22 = δj2 + σ 2 + ξj , where ξj is a mean-zero random variable. Define

the following random variable:
l = kω ? − ω̂k0

(8.9)

By definition l is the sum of n − 1 i.i.d. binary terms thus:
l = Op (µN )
i
h
2
2
6 1{δ̂j ≤ cω }}
µN := (N − 1)E 1{1{δj ≤ c̃ω )} =
Since

δ̂j2

=

δj2

2

+ σ + ξj , where ξj = Op


fδ2 (c̃ω )
µN = O (N − 1) √
T
Since fδ2 (x) =

l = Op

√
fδ ( x)
√
,
x

N −1
√
c̃ω T



√1
T



(8.10)

we get the following:


(8.11)

and using the fact that c̃ω = o(1) we get:


(8.12)

By construction we have the following:


K = Op (N − 1)Fδj2 (c̃ω )

(8.13)

√
)
√
and since c̃ω = o(1) and c̃ω = aN,T log(N
we
have
K
=
O
((N
−
1)
c̃ω ) → ∞. This implies the
p
T
following:
1
kω ? k2 = √ = op (1)
K

(8.14)

We have the following relationship:
|K − K̂| ≤ l


l
1
= Op √
= op (1)
K
T c̃ω

(8.15)

41

that implies


K̂
l
= Op 1 +
= Op (1 + op (1)) = Op (1)
K
K

(8.16)

As a result, we get that kω̂k2 = Op (kω ? k2 ). Define the following weights (different normalization):
{δ̂j2 ≤ cω }
ω̃j =
K

(8.17)

We have bounds on the squared norms:
 
1
l
kω̃ −
=
= op (kω ? k22 )
K K

2
 2
1
1
1
l
2
kω̃ − ω̂k2 = K̂
−
≤
= op (kω̃ − ω ? k22 )
K K̂
K̂ K
ω ? k22

(8.18)

Finally, we have the following:
p
kω̂ − ω k2 kω̂ − ω ? k0 log(N ) = Op
?

As k·k1 ≤ k·k2



l
log(N )
K




= Op

log(N )
√
c̃ω T


= op (1)

(8.19)

p
k·k0 , this implies that the conditions of Lemma 1 are satisfied. Thus, our

estimator has the following decomposition:
bN T − LN T = ω̂ · ε:T + λ̂ · εN : − ω̂ 0 ε:: λ̂
L
= ω ? · ε:T + λ? · εN : + (ω̂ − ω ? ) · ε:T + (λ̂ − λ? ) · εN : + op (max{kω ? k2 , kλ? k2 })
= ω ? · ε:T + λ · εN : + op (max{kω ? k2 , kλ? k2 })
where the last equality uses the fact that kω ? − ω̂k2 = op (kω ? k2 ), kλ? − λ̂k2 = op (kλ? k2 ) and the
fact that ω̂ is independent of ε:T and λ̂ is independent of εN : . This proves the result.

42

8.3

Generalizations of Theorem 3 and Lemma 4

In this section, we will replace Assumption 1 with the following generalization, which allows
for heteroskedastic and autocorrelated errors. In this setting, we consider the behavior of our
synthetic difference-in-difference estimator when we use least squares weights ω̂, λ̂ subject to
arbitrary constraints.
Assumption 5. Y = L + ε is an N × T matrix where L is deterministic and E ε = 0; the rows
of ε are independent and subgaussian; and E ε0i: εi: = Σ for all i ≤ N − 1. Here subscripting by :
takes the rows or columns for i < N, j < T .
Theorem 8. Under Assumption 5, consider the least squares estimators
ω̂ = arg minkω 0 Y:: − YN : k22 and λ̂ = arg minkY:: λ − Y:T k22
ω∈Ω

λ∈Λ

and the oracle estimators ω? , λ? defined analogously with L substituted for Y and define
δω = kLN : − ω?0 L:: k;
δλ = kL:T − L:: λ? k;
δsdid = |LN T − (ω?0 L:T + LN : λ? − ω?0 L:: λ? )| .
Then, for rλ defined in Lemma 9,


bN T − LN T
L



1/2
≤ diam(Λ) δω + Op kΣk + w(Ω) maxkεi: kψ2
i<N



1/2
+ diam(Ω) δλ + Op max Var[εiT | εi: ] + w(Λ) maxkεi: kψ2
i<N

i<N

+δsdid + Op (diam(Ω)rλ + kω?0 ε:: kψ2 w(Λ) + [kε:: λ? kψ2 + kE[ε:T | ε:: ]kψ2 ] w(Ω))
+ |ε0N : λ? + ω?0 ε:T − ω?0 ε:: λ? |
If the elements of ε:: are independent and identically distributed, we may substitute
min{diam(Ω)rλ , diam(Λ)rω } for diam(Ω)rλ , where rω is defined analogously to rλ , as the bound
established by Lemma 9 on k(ω̂ − ω? )0 L:: k.

43

Lemma 9. Under Assumption 5, for any subset Λ of RT −1 , the least squares estimator and
oracle least squares estimator
λ̂ = minkY:: λ − Y:T k22 and λ? = minkL:: λ − L:T k22
λ∈Λ

λ∈Λ

satisfy the bound kL:: (λ̂ − λ? )k2 = OP (rλ ) where


rλ = max kL:: λ? − L:T k,
p
x approx-rank(L:: , x) for x = kε:: λ? kψ2 + kε:T kψ2 ,
r
sup |γ̄ 0 δ|,
δ∈Λ

r
√ √

4
diam(Λ) max
T , N T log(T ) maxkεiT kψ2 kεij kψ2 ,
j

p
p
2
−1
N kΣk diam(Λ) + kεi: kψ2 kΣ k kΣk w(Λ) .
Here γ̄ = (N −1)−1

PN −1
i=1

E εiT εi: ; w(Λ) is the gaussian width of the set Λ; and approx-rank(L:: , x)

is an approximation to the rank of L:: that ignores small nonzero singular values, defined

approx-rank(L:: , x) := min






sX

r ≥ x−1 min diam(Λ)
s2k , sr+1 w(Λ)



r ∈ 1, 2, . . .

k>r

in terms of the decreasing sequence of singular values s1 , s2 , . . . of L:: .

44

8.3.1

Proof of Theorem 8

Our estimator’s error is the difference between our estimator and the corresponding infeasible
estimator, plus the infeasible estimator’s error δsdid , i.e.
b N T − LN T
L
h
i
= YN : λ̂ + ω̂ 0 Y:T − ω̂ 0 Y:: λ̂ − [(YN : − εN : )λ? + ω?0 (Y:T − ε:T ) − ω?0 (Y:: − ε:: )λ? ] + δsdid
h
i
0
0
0
0
= YN : (λ̂ − λ? ) + (ω̂ − ω? ) Y:T − (ω̂ − ω? ) Y:: (λ̂ − λ? ) + ω? Y:: (λ̂ − λ? ) + (ω̂ − ω? ) Y:: λ?
+ δsdid − ε0N : λ? − ω?0 ε:T + ω?0 ε:: λ?
= (YN : − ω?0 Y:: )(λ̂ − λ? ) + (ω̂ − ω? )0 (Y:T − Y:: λ? ) − (ω̂ − ω? )0 Y:: (λ̂ − λ? )
+ δsdid − ε0N : λ? − ω?0 ε:T + ω?0 ε:: λ?
= (L:N − ω?0 L:: )(λ̂ − λ? ) + (ω̂ − ω? )0 (L:T − L:: λ? ) + δsdid
+ (εN : − ω?0 ε:: )(λ̂ − λ? ) + (ω̂ − ω? )0 (ε:T − ε:: λ? ) − (ω̂ − ω? )0 ε:: (λ̂ − λ? )
− ε0N : λ? − ω?0 ε:T + ω?0 ε:: λ?
− (ω̂ − ω? )0 L:: (λ̂ − λ? ).
We bound each line.
1. The first line is bounded by δω diam(Λ) + δλ diam(Ω) + δsdid , which follows by applying
Cauchy-Schwarz to the first two terms.
2. The second line is






1/2

Op diam(Λ) kΣk + w(Ω) maxkεi: kψ2
i<N


1/2
+ diam(Ω) max Var[εiT | εi: ] + w(Λ) maxkεi: kψ2
i<N

i<N

+kω?0 ε:: kψ2 w(Λ) + [kε:: λ? kψ2 + kE[ε:T | ε:: ]kψ2 ] w(Ω)
(a) The first term is the sum of two pieces, ε0N : (λ̂ − λ? ) and −ω?0 ε:: (λ̂ − λ? ). The first
piece is Op (kΣk1/2 diam(Λ)). Because the row εN : is independent of the noise submatrices ε:: , ε:T that are used to define λ̂, it has mean zero and variance bounded by
45

kΣk diam(Λ)2 conditional on λ̂. The second piece is Op (kω?0 ε:: kψ2 w(Λ)), as the vector ω?0 ε:: is subgaussian, and by Talagrand’s comparison inequality [Vershynin, 2018,
Corollary 8.6.3],
E sup |ω?0 ε:: δ| ≤ cK w(Λ − λ? );
δ∈Λ−λ?

K0 =

sup kω?0 ε:: (x − y)kψ2 /kx − yk ≤ kω?0 ε:: kψ2 .

x,y∈Λ−λ?

(b) The second term is the sum of two pieces, (ω̂ − ω? )0 ε:T and (ω̂ − ω? )0 ε:: λ? . The second
piece, by Talagrand’s comparison inequality as above, is Op (kε:: λ? kψ2 w(Ω)). The
first piece is

Op diam(Ω) max Var[εiT | εi: ]

1/2

i<N


+ w(Ω)kE[ε:T | ε:: ]kψ2

with terms bounding those in the decomposition
(ω̂ − ω? )0 ε:T = (ω̂ − ω? )0 (ε:T − E[ε:T | ε:: ]) + (ω̂ − ω? ) E[ε:T | ε:: ].
The bound on the second of these terms follows from Talagrand’s comparison inequality as above, and the first of them has a conditional Chebyshev bound
P (|(ω̂ − ω? )0 (ε:T − E[ε:T | ε:: ])| > t | ε:: , εN : )
≤ t−2

N
−1
X

(ω̂ − ω? )2i Var[εiT | ε:: ] ≤ t−2 diam(Ω)2 max Var[εiT | εi: ].
i<N

i=1

(c) The third term is Op (maxi<N kεi: kψ2 [diam(Ω) w(Λ) + diam(Λ) w(Ω)]). This follows
from Chevet’s inequality for random matrices with iid subgaussian rows,
E sup x0 εy ≤ c maxkεi: kψ2 [rad(X) w(Y ) + w(X) rad(Y )] .
x∈X,y∈Y

i<N

The proof of Vershynin [2018, Theorem 8.7.1], which addresses the case of random
matrices with iid subgaussian elements, can be adapted for iid subgaussian rows by
applying Hoeffding’s inequality (i.e. Vershynin [2018, Proposition 2.6.1]) to row sums
46

rather than elementwise when bounding the increments of this subgaussian process.
3. The third line is included in the bound.
4. The fourth line is Op (diam(Ω)rλ ). This follows from the Cauchy-Schwarz bound kω̂ −
ω? kkL:: (λ̂ − λ? )k and Lemma 9. If the elements of ε:: are iid, then Lemma 9 implies a
bound k(ω̂ − ω? )0 L:: k ≤ rω , and we can also bound this term by k(ω̂ − ω? )0 L:: kkλ̂ − λ? k, so
the fourth line will be Op (min{diam(Ω)rλ , diam(Λ)rω }).
Collecting all of these results yields our claimed bound.
8.3.2

Proof of Lemma 9

To simplify our notation in this proof, we will write N and T for the dimensions of Y:: , which
are called N − 1 and T − 1 in the lemma statement.
Our proof is based on the well-known isomorphic bounds argument in empirical risk minimization [see e.g. Lecué and Mendelson, 2013, Mendelson, 2014].
0 ≥ kY:: λ̂ − Y:T k22 − kY:: λ? − Y:T k22
= kY:: λ̂k22 − kY:: λ? k22 − 2Y:T0 Y:: (λ̂ − λ? )
≥ kY:: (λ̂ − λ? )k22 + 2(Y:: λ? − Y:T )0 Y:: (λ̂ − λ? ) + 1{Λ=conv(Λ)} · 2(L:T − L:: λ? )0 L:: (λ̂ − λ? )
≥ kY:: (λ̂ − λ? )k22 +2 (Y:: λ? − Y:T )0 Y:: (λ̂ − λ? ) − 1{Λ=conv(Λ)} (L:: λ? − L:T )0 L:: (λ̂ − λ? ) .
|
{z
}
|
{z
}
Q(λ̂−λ? )
M (λ̂−λ? )
(8.20)
Here the addition of the term 1{Λ=conv(Λ)} ·2(L:T −L:: λ? )0 L:: (λ̂−λ? ) in the third line is justified by
its negativity. This is a consequence of the convexity of the set L:: Λ when the term is nonzero:
L:T − L:: λ? points ‘outward’ from the projection of L:T onto L:: Λ to L:T itself, whereas
L:: (λ̂−λ? ) points ‘inward’ toward another element of L:: Λ [see e.g. Peypouquet, 2015, Proposition
1.37].
Let Λ − λ? be the set of possible deviations from λ? ; δ̂ = λ̂ − λ? be the realized deviation; and
r2 (δ) = kL:: δk2 . We will show that, with probability tending to one, (8.20) cannot be satisfied
if r(δ̂) ≥ r? . This implies our claimed bounds.
47

To do this, we will show that with probability tending to one, we have a uniform quadratic
lower bound on Q(δ) and a corresponding upper bound on M (δ̂) ,

inf

Q(δ)
≥ 1 − η,
r2 (δ)

(8.21)

sup

|M (δ)|
< (1 − η)/2.
r2 (δ)

(8.22)

δ∈Λ−λ?
r(δ)≥r?

δ∈Λ−λ?
r(δ)≥r?

These bounds are implied by simpler bounds of the form
inf Q(δ) ≥ (1 − η)r?2 ,

(8.23)

sup |M (δ)| < (1 − η)/2 · r?2 .

(8.24)

δ∈Λ?

δ∈Λ?

where Λ? = {δ ∈ [0, 1](Λ − λ? ) : r(δ) = r? }. To see this, consider δ ∈ Λ − λ? with r(δ) ≥ r, and
observe that because Q is quadratic and L is linear and δr? /r(δ) ∈ Λ? , (8.23) and (8.24) imply
that
Q(δ) = (r(δ)/r? )2 Q(δr? /r(δ)) ≥ (r(δ)/r? )2 · (1 − η)r?2 = (1 − η)r(δ)2 ;
|M (δ)| = (r(δ)/r? ) |M (δr? /r(δ))| ≤ (r(δ)/r? ) · (1 − η/2)r?2 ≤ (1 − η/2)r(δ)2 .
We now move on to the core of our proof, which involves proving these bounds (8.23) and (8.24).
The lower bound (8.21). Much of our proof will rely on ε:: δ being small relative to L:: δ.
For ε:: having independent rows with the same correlation structure, i.e. E ε0i: εi: = Σ with
kεi: kψ2 ≤ K, we can establish this using the matrix deviation inequality of Liaw et al. [2017],
E sup kε:: δk −

p
√ √
N k Σδk ≤ ckΣ−1 k kΣkK 2 w(Λ? ).

(8.25)

δ∈Λ?

For simplicity, we use Markov’s inequality rather than subgaussian concentration to derive a tail
bound from this, letting it hold on an event A.
By the triangle inequality, this controls the degree to which r? = kL:: δk can exceed kY:: δk,

48

as
kL:: δk − kY:: δk ≤ kL:: δ − Y:: δk = kε:: δk.
In particular, every δ ∈ Λ? satisfying
√

p
√
N k Σδk + c(1 − P(A))−1 K 2 kΣ−1 k kΣk w(Λ? ) ≤ η/2 · r?

, will also satisfy the bound kL:: δk − kY:: δk ≤ η/2 · r? . For
r? ≥ 2η −1

p
p
N kΣk rad(Λ? ) + c(1 − P(A))−1 η −1 K 2 kΣ−1 k kΣk w(Λ? ),

(8.26)

this latter bound will be satisfied for all δ ∈ Λ? . And rearranging it gives (1−η/2)kL:: δk ≤ kY:: δk,
which implies (8.23). Thus, (8.21) holds on the event A for r? above.
The upper bound (8.22).
M (δ) = (Y:: λ? − Y:T )0 Y:: δ − 1{Λ=conv(Λ)} (L:: λ? − L:T )0 L:: δ
= 1{Λ6=conv(Λ)} (L:: λ? − L:T )0 L:: δ

(8.27)

+ (L:: λ? − L:T )0 ε:: δ

(8.28)

+ λ0? ε0:: L:: δ

(8.29)

+ λ0? ε0:: ε:: δ

(8.30)

− ε0:T L:: δ.

(8.31)

− ε0:T ε:: δ.

(8.32)

The first term (8.27) is deterministic, and has the Cauchy-Schwarz bound
1{Λ6=conv(Λ)} kL:: λ? − L:T kr? for δ ∈ Λ? .

49

The second term (8.28) has straightforward bound based Cauchy-Schwarz and (8.25). On A,
|(L:: λ? − L:T )0 ε:: δ| ≤ kL:: λ? − L:T k · (η/2)r? for δ ∈ Λ? .
Similarly, Cauchy-Schwarz, (8.25), and the analogous bound
E kελ? k −

√

p
p
√
N k Σλ? k ≤ ckΣ−1 k kΣkK 2 w({0, λ? }) ≤ ckΣ−1 k kΣkK 2 kλ? k

(8.33)

suffice to bound (8.30). They imply that for δ ∈ Λ? , on the intersection of A and the analogous
event on which the bound above holds with probability P(A),
|λ0? ε0:: ε:: δ| ≤ kε:: λ? kkε:: δk
h√
i
p
≤
N kΣ1/2 kkλ? k + (1 − P(A))−1 ckΣ−1 k kΣkK 2 kλ? k [(η/2) · r? ]
≤ (η/2)2 r?2 kλ? k/ rad(Λ? ).
The third term (8.29) is the supremum of the inner product of a subgaussian random vector
ε:: λ? and a vector L:: δ in the intersection of the image of Λ? under L:: and the k·k2 ball of radius
r? . and via Talagrand’s comparison inequality [Vershynin, 2018, Corollary 8.6.3],
E sup |(ε:: λ? )0 x| ≤ cK 0 w(L:: λ? )
x∈L:: Λ?

K0 =

sup k(ε:: λ? )0 (x − y)kψ2 /kx − ykL2 ≤ kε:: λ? kψ2 .
x,y∈L:: Λ?

To bound the gaussian width w(L:: Λ? ), we split L:: into two pieces, a low rank approximation
P
L̃R defined as the sum of the first R terms in the singular value decomposition L:: = k σk uk vk0 ,
and the remainder. L̃R δ is contained in a R-dimensional ball of radius r? , so in terms of a
standard gaussian vector g
√
w(L:: λ? ) = E sup g 0 L:: δ ≤ E sup g 0 L̃R δ + E sup g 0 (L:: − L̃R )δ ≤ c Rr? + w((L:: − L̃R )Λ? ).
δ∈Λ?

δ∈Λ?

δ∈Λ?

The fifth term (8.31) is analogous, with the difference that we substitute ε:T for ε:: λ? , so in
the corresponding bound we have K 0 = kε:T kψ2 .
50

To bound the sixth term, kε0:T ε:: δk, we begin by characterizing the vector ε0:T ε:: . Because the
P
rows of ε are independent, ε0:T ε:j = i εiT εij is a sum of independent subexponential random
variables with kεiT εij kψ1 ≤ kεiT kψ2 kεij kψ2 [Vershynin, 2018, Lemma 2.7.7]. In terms of the
P
averaged autocorrelation γ̄j = N −1 i EεiT εij , ε0:T ε:: = N γ̄ +Z where Zj = ε0:T ε:j −N γ̄j is a sum
of independent subexponential random variables with mean zero and kZi kψ1 ≤ ckεiT kψ2 kεij kψ2 .
By the Triangle inequality and Cauchy-Schwartz,
|ε0:T ε:: δ| ≤ N |γ̄ 0 δ| + |Z 0 δ| ≤ N |γ̄ 0 δ| + 2 rad(Λ? )kZk,
and by Bernstein’s inequality [Vershynin, 2018, Theorem 2.8.1],

P(|Zj | ≥ tj ) ≤ 2 exp −c min

t2j
tj
P
,
2
2
i kεiT kψ2 kεij kψ2 maxi kεiT kψ2 kεij kψ2

so we have a bound of the form 2 rad(Λ? )kZk < ξr?2 by the union bound for

!!
,

2
j tj

P

≤ ξ 2 r?4 /(4 rad(Λ? )2 ).

Simply taking t2j = ξ 2 r?4 /(4 rad(Λ? )2 T ) yields
P(2 rad(Λ? )kZk ≥ ξr?2 )
≤2

X

exp −c min

j

≤ 2T exp −c min

ξ 2 r?4
ξr?2
P
√
,
rad(Λ? )2 T i kεiT k2ψ2 kεij k2ψ2 rad(Λ? ) T maxi kεiT kψ2 kεij kψ2

!!

ξ 2 r?4
ξr?2
√
,
rad(Λ? )2 T N maxij kεiT k2ψ2 kεij k2ψ2 rad(Λ? ) T maxij kεiT kψ2 kεij kψ2

!!

Putting everything together, our claims on M (δ), as well as our claims on Q(δ) from the

51

.

previous section, hold with high probability when η ≤ min{1, 4 rad(Λ? )/kλ? k} and r? satisfies

r? ≥ c max [1{Λ6=conv(Λ)} + η](1 − η)−1 kL:: λ? − L:T k,
r
h√
i
−1
(1 − η) [kε:: λ? kψ2 + kε:T kψ2 ] Rr? + w((L:: − L̃R )Λ) ,
r
(1 − η)−1 N sup |γ̄ 0 δ|,
δ∈Λ?

r
√ √

4
(1 − η)−1 rad(Λ? ) max
T , N T maxkεiT kψ2 kεij kψ2 log(T ),
ij

p
√
−1
−1
2
η
N kΣk rad(Λ? ) + maxkεi: kψ2 kΣ k kΣk w(Λ? ) .
i

Here the constraint on η comes from (8.30), the first line in r? comes from (8.27) and (8.28),
the second from (8.29) and (8.31), the third and fourth from (8.32), and the fifth incorporates
(8.26) from the lower bound section.
Simplifications This is a fixed point condition, as r? appears in the second line of the right
side explicitly in the third line and implicitly throughout, as Λ? is a function of r? . To eliminate
the dependence of the right side on r? through Λ? , we simply substitute for w(Λ? ) and rad(Λ? ) the
upper bounds w(Λ) and diam(Λ). This also allows us to drop the constraint η ≤ 4 rad(Λ? )/kλ? k,
as because λ? ∈ Λ, it becomes vacuous if we perform this substitution.4
This leaves us with an expression
q √on the right that depends on r? only through the second
line, which will have the form c x[ Rr? + w(R)] for x = (1 − η)−1 [kε:: λ? kψ2 + kε:T kψ2 ] and
w(R) = w((L:: − L̃R )Λ). To obtain the claimed result, we simplify this fixed point condition to
p
√
a bound, observing that r?2 ≥ cx[ Rr? + w(R)] if r? ≥ 2cx max(R, x−1 w(R)). In addition, we
bound w(R) = w((L:: − L̃R )Λ) by the minimum of kL:: − L̃R k w(Λ) and the gaussian width of the
pP
2
ellipse with axes diam(Λ)σR+1 , diam(Λ)σR+2 , . . ., which is proportional to diam(Λ)
k>R σk
[see e.g Vershynin, 2018, Section 7.6.1], and we minimize this bound over R.
4

To justify this, we can check that this constraint would not have arisen from our bound on (8.30) had we
made this substitution earlier.

52

8.4

Specializing Theorem 8 and Lemma 9

In this section, we will prove more concrete versions of the results of the previous section. This
includes Theorem 3 and Lemma 4 from Section 4, which make the assumption that εit ∼ N (0, σ 2 )
is independent for each cell (i, t), and allowing us to dramatically simplify our bounds. We also
prove Theorem 5 from Section 4, a variant of Theorem 3 which characterizes the synthetic
control estimator under the same assumptions.
Proof of Lemma 4. In the bound of Lemma 9, we simplify several expressions under Assumpp
tion 1: kε:: λ? kψ2 = kεi: λ? kψ2 = σkλ? k, kε:T kψ2 = kεiT kψ2 = σ, kεiT kψ2 kεij kψ2 = σ 2 , kΣk = σ,
p
kεi: k2ψ2 kΣ−1 k kΣk = σ 2 σ −2 σ = σ, and γ̄ = 0. With these simplifications, as well as the bounds
p
p
√
√
2aω ≥ diam(Ω), 2aλ ≥ diam(Λ), and min{ log(N ), aω N } & w(Ω), min{ log(T ), aλ T } &
w(Λ), the bound of Lemma 9 reduces to that of Lemma 4.
Proof of Theorem 3. In the bound of Theorem 8 for iid noise, we substitute E[ε:T | ε:: ] =
0; ω̃ = ω̂; kΣk1/2 = maxi<N kVar[εiT | εi: k1/2 = σ; kε:: λ? kψ2 = σkλ? k and kω?0 ε:: kψ2 =
p
√
σkω? k; aω & diam(Ω) + kω? k and aλ & diam(Λ) + kλ? k; min{ log(N ), aω N } & w(Ω) and
p
√
min{ log(T ), aλ T } & w(Λ); and ε0N : λ? + ω?0 ε:T − ω?0 ε:: λ? = Op (σkλ? k + σkω? k). We do
not have an Op (σkω? kkλ? k) term in our bound corresponding to the third term ω?0 ε:: λ? in this
last expression because the bounds for the other terms suffice: kλ? k ≤ 1 for λ? ∈ Λ ⊆ L, so
σkω? kkλ? k ≤ σkω? k.
Proof of Theorem 5. We can express our estimator as
ω̂ · Y:T − LN T = (ω?0 L:T − LN T ) + (ω̂ − ω? )0 L:T + ω̂ 0 ε:T .
The first term above is simply δsc , while the last is Op (aω ), as it is gaussian with standard
deviation kω̂k ≤ aω conditional on ε:: , εN : . It remains to bound the middle term. To do so, note
that for any λ̃,


(ω̂ − ω? )0 L:T = (ω̂ − ω? )0 L:: λ̃ + (ω̂ − ω? )0 L:T − L:: λ̃
≤ kL0:: (ω̂ − ω? )k2 λ̃ + kω̂ − ω? k2 L:T − L:: λ̃
2


= Op rω λ̃ + aω L:T − L:: λ̃ ,
2

53

where the last line follows from Lemma 4.

8.5

Proof of Theorem 6

We will now consider the problem of inference in a simple setting with N1 treated units and T1
treated time periods, in which all units with i > N0 start treatment at the same time T0 + 1.
Our estimator (4.1) is, in this setting, of essentially the same form as those we’ve discussed
for a single treated unit and time period. Having observed such an N0 + N1 × T0 + T1 matrix
Ỹ = L̃ + ε̃, we define a N0 + 1 × T0 + 1 variant Y = L + ε in which treated units and rows are
averaged,

Y =

T1−1



P

Ỹ1:N0 ,1:T0
t>T0 Ỹ1:N0 ,t
.
P
P
N1−1 i>N0 Ỹi,1:T0 (N1 T1 )−1 n>N0 ,t>T0 Ỹit

(8.34)

In these terms, letting N = N0 + 1 and T = T0 + 1, τ̂ = YN T − L̂N T . Thus, the problem
of estimating the averaged unobserved control potential outcome in this setting is essentially
the same as the problem we’ve considered in the previous section, in which we had only one
unobserved potential outcome. It differs only in that this averaging results in heteroskedasticity
in the errors ε, making the elements of εN : and ε:T small relative to those of ε:: . Furthermore,
because averaging drives εN T to zero, it is possible for our estimator τ̂ to be consistent in
this setting, whereas in the previous section the constant order variance of YN T made this
impossible. Our core result is that, under Assumption 4 and some restrictions on N1 and T1 , τ̂
is approximately normal with bias negligible relative to variance Var(τ̂ ) ≈ Var(εN T ).
Our first step is to establish a formal equivalence between estimation of τ̂ under Assumption 4
on Ỹ and under a specialization of Assumption 5 on Y . This is straightforward.
Proposition 10. If Ỹ satisfies Assumption 4, then Y defined in (8.34) satisfies Assumption 6
with σ 2 = σξ2 /(1 − ρ2 ) and the estimator τ̂ (4.1) based on Ỹ is the same as the one based on Y .
Assumption 6. We have N = N0 + 1, T = T0 + 1 → ∞, and there is a N × T deterministic
matrix L such that Yit = Lit + Wit τ + εit with Wit = 1 {i = N, t = T }, and the rows of ε are
independent gaussian vectors with, for i < N ,
1. Cov (εij , εik ) = σ 2 ρ`−k for j ≤ k < T ,
54

2. Cov (εij , εiT ) = T1−1 σ 2
3. Var (εiT ) = T1−2

PT1

k=1

PT1 PT1
k=1

`=1

ρT0 +k−j = T1−1 σ 2 ρT0 +1−j (1 − ρT1 )/(1 − ρ) for j < T ,

σ 2 ρ|k−`| ,

for ρ ≥ 0 and, for i = N , the corresponding terms are N1−1 times these quantities.
Thus, as our estimator’s error is
τ̂ − τ = (YN T − LN T ) + (LN T − L̂N T ) = εN T + Op (LN T − L̂N T ),
a specialized version of Theorem 8 under Assumption 6 is sufficient to establish conditions under
which the second term is negligible relative to the first, i.e. conditions under the latter term is
op ((N1 T1 )−1/2 ). Our general result, of which Theorem 6 is a corollary, follows.
Lemma 11. Under Assumption 6, consider the least squares estimators
ω̂ = arg minkω 0 Y:: − YN : k22 and λ̂ = arg minkY:: λ − Y:T k22
ω∈Ω

λ∈Λ

and the oracle estimators ω? , λ? defined analogously with L substituted for Y and define
δω = kLN : − ω?0 L:: k;
δλ = kL:T − L:: λ? k;
δsdid = |LN T − (ω?0 L:T + LN : λ? − ω?0 L:: λ? )| .
bN T − LN T =
If σ = O(1) and lim supN,T →∞ ρ < 1, and we take diam(Ω) = O(1), then L

55

op ((N1 T1 )−1/2 ) if

N1 T1  min

1
2
δsdid

,

1
,
max {δλ2 , w2 (Λ)}
1
,
2
diam(Λ) max {δω2 , w2 (Ω)}

1
;
diam(Ω)2 diam(Λ)2 N0
1
1/2
 √
N1 T1 
;
2
diam(Ω) diam(Λ) max T0 , N0 T0 log(T0 )


1
T1
o .

n
,
N1  min
w(Ω)2 diam(Ω)2 approx-rank L , max kλ k, T −1/2
diam(Ω)2

::

?

1

This follows from Corollary 13 and Corollary 14 below by a straightforward calculation.
These corollaries are specializations of Lemma 9 and Theorem 8 respectively, which follow from
our more general results with a few calculations collect in Lemma 12 below.
Lemma 12. Under Assumption 6 for ρ > 0, letting Σ be the covariance matrix E εi: ε0i: and γ
be the vector of autocovariances E ε0:T ε:: ,
1. kΣk ≤ σ 2 1+ρ
1−ρ
2. kΣ−1 k ≤ σ −2 1+ρ
1−ρ
3. kγk ≤ T1−1 σ 2 ρ2 (1 − ρ)−1 (1 − ρ2 )−1/2
4. kεi: kψ2 = kΣk1/2 ≤

√
2σ(1 − ρ)−1/2 .

5. kε:: λ? kψ2 = kΣk1/2 kλ? k ≤

√
2σ(1 − ρ)−1/2 kλ? k

6. kω?0 ε:: kψ2 = σkω? k
7. kε:T kψ2 = kεiT kψ2 ≤

√

−1/2

2T1

σ(1 − ρ)−1/2

8. kE[ε:T | ε:: ]kψ2 = kE[εiT | ε:: ]kψ2 ≤ T1−1 ρ1/2 (1 − ρ)−1/2 σ
56

1/2

9. Var[εiT | εi: ]
10. εN : λ? +

ω?0 ε:

= Op

T−



−1/2
T1 σ(1

ω?0 ε:: λ?

= Op

−1/2

− ρ)

h



−1/2
N1 kλ? k

+

−1/2
T1 kω? k

i

−1/2

+ kω? kkλ? k σ(1 − ρ)



.

Corollary 13. Under Assumption 6, for any subset Λ of RT −1 , the least squares estimator and
oracle least squares estimator
λ̂ = minkY:: λ − Y:T k22 and λ? = minkL:: λ − L:T k22
λ∈Λ

λ∈Λ

satisfy the bound kL:: (λ̂ − λ? )k2 = OP (rλ ) where, for approx-rank defined in Lemma 9,


rλ = max kL:: λ? − L:T k,
n
o
p
−1/2
x approx-rank(L:: , x) for x = max kλ? k, T1
σ(1 − ρ)−1/2 ,
−1/2

diam1/2 (Λ) · σρ(1 − ρ)−1/2 (1 − ρ2 )−1/4 ,
r

p p
−1/4
T1
diam(Λ) max
T0 , 4 N0 T0 log(T0 ) · σ(1 − ρ)−1/4 ,

T1

1/2

diam(Λ) · σ(1 − ρ)−1/2 ,

3/2 
1+ρ
w(Λ) · σ
.
1−ρ

N0

Corollary 14. Under Assumption 6, consider the least squares estimators
ω̂ = arg minkω 0 Y:: − YN : k22 and λ̂ = arg minkY:: λ − Y:T k22
ω∈Ω

λ∈Λ

and the oracle estimators ω? , λ? defined analogously with L substituted for Y and define
δω = kLN : − ω?0 L:: k;
δλ = kL:T − L:: λ? k;
δsdid = |LN T − (ω?0 L:T + LN : λ? − ω?0 L:: λ? )| .

57

Then, for rλ defined in Corollary 13,


bN T − LN T ≤ diam(Λ) δω + Op σ(1 − ρ)−1/2 max {1, w(Ω)}
L



+ diam(Ω) δλ + Op σ(1 − ρ)−1/2 max T1−1 , w(Λ)

+δsdid + Op diam(Ω)rλ + σ(1 − ρ)−1/2 ρ1/2 T1−1 w(Ω) .

8.6

Proof of Lemma 12

Because all random variables involved are gaussian, the subgaussian norm k·kψ2 is equivalent
to k·kL2 . Furthermore, for a random vector v of identically distributed elements vi , kvkL2 =
pP
2
2
supkxk=1
i xi E vi = kvi kL2 .
1,2. Our bounds (i-ii) are determined by upper and lower bounds on the maximal and minimal
eigenvalues of the correlation matrix Σ/σ 2 respectively, which for our AR(1) process are
no larger than

1+ρ
1−ρ

and no smaller than

1−ρ
1+ρ

respectively [see e.g. Trench, 1999, Section 1].

3.
v

u T0
T1 uX

1−ρ t
ρ2(T0 +1−j)
1−ρ
j=1
s
1 − ρ2T0
1 − ρT1
ρ2
= T1−1 σ 2 ρ
1−ρ
1 − ρ2

kγk = T1−1 σρ

≤ T1−1 σ 2 ρ2 (1 − ρ)−1 (1 − ρ2 )−1/2 .
4,5,6. For any vector v, kε:: vkψ2 = kεi: Σ−1/2 Σ1/2 vkL2 = kΣ1/2 vk ≤

√

2σ(1 − ρ)−1/2 kvk, reducing

our problem to one about an identically distributed vector and in the last step using our
bound (i) and substituting 2 > 1 + ρ. Similarly, ku0 ε:: kψ2 = ku0 ε:: Σ−1/2 ΣkL2 ≤ kΣkkuk ≤
√
2σ(1 − ρ)−1/2 kuk.

58

7. kε:T kψ2 = kεiT kL2 , as it’s a vector of identically distributed gaussian elements, and
kεiT k2L2

=

=
≤

T1−2

T1 X
T1
X

σ 2 ρ|k−`|

k=1 `=1
!
TX
1 −1
T1−2 σ 2
2(T1 − s)ρs − T1
s=0
TX
1 −1
−2 2
ρs ≤ 2T1−1 σ 2 (1 −
T1 σ · 2T1
s=0

ρ)−1 .

8. For theq
same reason, kE[ε:T | ε:: ]kψ2 = kE[εiT | εi: ]kL2 , and kE[εiT | εi: ]kL2 = kT1−1
T1−1

T1

σ ≤ T1−1 ρ1/2 (1 − ρ)−1/2 σ.
ρ 1−ρ
1−ρ

9. E Var[εiT | εi: ] ≤ E ε2iT ≤ 2T1−1 σ 2 (1 − ρ)−1 .

10.
√ −1/2
kΣ1/2 λ? k ≤ 2N1 kλ? kσ(1 − ρ)−1/2
√ −1/2
kω?0 ε̃:T kL2 = kω? kkε̃iT kL2 ≤ 2T1 kω? kσ(1 − ρ)−1/2
√
kω?0 ε̃:: λ? kL2 = kω? kkΣ1/2 λ? k ≤ 2kω? kkλ? kσ(1 − ρ)−1/2 .
−1/2

kε̃0N : λ? kL2 = N1

59

PT1

k=1

ρk εi,T0 kL2 =

8.6.1

Proof of Lemma 11

Substituting the bound of Corollary 13 into that of Corollary 14,
bN T − LN T =δsdid +
L

Op
diam(Λ) max {δω , w(Ω)}
+ diam(Ω) max {δλ , w(Λ)}
n
or

n
o
−1/2
−1/2
+ diam(Ω) max kλ? k, T1
approx-rank L:: , max kλ? k, T1
−1/2

+ diam(Ω)T1
+

−1/4
diam(Ω)T1

diam1/2 (Λ)
r


p p
4
diam(Λ) max
T0 , N0 T0 log(T0 )

1/2

+ diam(Ω)N0 diam(Λ)

−1
+ T1 w(Ω) .
Several of the terms in this bound can be ignored, as they are bounded by others:
r

o

n
1/2
−1/2
. diam(Ω)N0 diam(Λ);
approx-rank L:: , max kλ? k, T1
r
o

n
−1/2
−1/2
−1/2
1/2
diam(Ω)T1
diam (Λ) . diam(Ω)T1
.
approx-rank L:: , max kλ? k, T1
diam(Ω)kλ? k

Under the stated conditions on N1 and T1 , each of the remaining terms is o((N1 T1 )−1/2 ).
1. The condition on N1 T1 in the lemma statement arises from terms in the expression above
with no factors of N1 , T1 . It is equivalent to the condition
−1/2

(N1 T1 )


 max δsdid ,
diam(Λ) max {δω , w(Ω)} ,
diam(Ω) max {δλ , w(Λ)} ,

1/2
diam(Ω) diam(Λ)N0
.

60

1/2

2. The condition on N1 T1
−1/4

leading factor of T1

in the lemma statment arises from the term above with the

.
−1/2

3. The condition on N1 arises from the terms above with leading factors of T1

61

and T1−1 .

