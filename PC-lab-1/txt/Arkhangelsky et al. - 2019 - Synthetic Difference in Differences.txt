Synthetic Difference In Differences‚àó
arXiv:1812.09970v2 [stat.ME] 31 Jan 2019

Dmitry Arkhangelsky‚Ä†

Susan Athey‚Ä°

Guido W. Imbens¬∂

David A. Hirshberg¬ß

Stefan Wagerk

First version December 24, 2018, current version February 1, 2019

Abstract
We present a new perspective on the Synthetic Control (SC) method as a weighted least
squares regression estimator with time fixed effects and unit weights. This perspective suggests a generalization with two way (both unit and time) fixed effects, and both unit and
time weights, which can be interpreted as a unit and time weighted version of the standard
Difference In Differences (DID) estimator. We find that this new Synthetic Difference In
Differences (SDID) estimator has attractive properties compared to the SC and DID estimators. Formally we show that our approach has double robustness properties: the SDID
estimator is consistent under a wide variety of weighting schemes given a well-specified
fixed effects model, and SDID is consistent with appropriately penalized SC weights when
the basic fixed effects model is misspecified and instead the true data generating process
involves a more general low-rank structure (e.g., a latent factor model). We also present
results that justify standard inference based on weighted DID regression. Further generalizations include unit and time weighted factor models.

Keywords: Synthetic Controls, Causal Effects, Panel Data, Difference In Differences, LowRank Confounders
‚àó

We are grateful for conversations with Avi Feller and Yinchu Zhu, and for comments from seminar
participants at UC San Diego. This research was generously supported by ONR grant N00014-17-1-2131
and the Sloan Foundation.
‚Ä†
Assistant Professor, CEMFI, Madrid, darkhangel@cemfi.es.
‚Ä°
Professor of Economics, Graduate School of Business, Stanford University, and NBER,
athey@stanford.edu.
¬ß
Postdoctoral Fellow,
Department of Statistics and SIEPR, Stanford University,
davidahirshberg@stanford.edu.
¬∂
Professor of Economics, Graduate School of Business, and Department of Economics, Stanford
University, SIEPR, and NBER, imbens@stanford.edu.
k
Assistant Professor of Operations, Information and Technology, Graduate School of Business, and
of Statistics (by courtesy), Stanford University, swager@stanford.edu.

1

Introduction

Synthetic Control (SC) methods, introduced in a seminal series of papers by Abadie and coauthors [Abadie and Gardeazabal, 2003, Abadie, Diamond, and Hainmueller, 2010, 2015, Abadie
and L‚ÄôHour, 2016], have quickly become one of the most popular methods for estimating treatment effects in panel settings. By using data-driven weights to balance pre-treatment outcomes
for treated and control units, the SC method imputes post-treatment control outcomes for the
treated unit(s) by constructing a synthetic version of the treated unit(s) equal to a convex
combination of control units.
In the current paper, we build on these ideas to provide a different perspective on the SC
approach and to propose a new estimator with improved bias properties. First, we show that the
SC estimator can be viewed as a weighted least squares regression estimator with unit-specific
weights, where the regression model includes time fixed effects. We then propose adding unit
fixed effects to this regression representation of the standard SC set up to add flexibility, as
well as time weights to ensure that the weighted periods resemble more closely the period(s) for
which we are imputing the counterfactual. We show that this leads to a doubly weighted, or
local, version of the standard Difference In Differences (DID) estimator [e.g., Bertrand, Duflo,
and Mullainathan, 2004, Card, 1990]. We then establish that the resulting estimator, which
we call the Synthetic Difference In Differences (SDID) estimator, has attractive bias properties
compared to both the SC and DID estimators. In particular the estimator satisfies a form of
double robustness that both the SC and DID estimators lack: the estimator is consistent if
either the model is correctly specified, or if the weights are well choosen, but consistency does
not require both those conditions.
Consider the simplest case of a balanced panel with N units and T time periods, where
outcomes are denoted by Yit , and exposure to the binary treatment is denoted by Wit ‚àà {0, 1}.
Initially suppose that Wit = 0 unless (i, t) = (N, T ), so that only unit N is treated, and only
in period T . Suppose also that there are no covariates. In that case, the SC estimator for the
causal effect is œÑÃÇ sc = YN T ‚àí YÃÇNscT where YÃÇNscT is a weighted average of the period T outcomes
P ‚àí1 sc
sc
for the control units, YÃÇNscT = N
i=1 œâÃÇi YiT , with the weights œâÃÇi chosen to make the weighted
average of the controls in the pre-treatment period approximate the corresponding value for
P
the treated unit, i œâÃÇisc Yit ‚âà YN t for all t = 1, . . . , T ‚àí 1. In this paper, we introduce a novel

2

characterization of the SC estimator œÑÃÇ sc as a weighted least squares regression estimator:

sc

(¬µÃÇ, Œ≤ÃÇ, œÑÃÇ ) = arg min
¬µ,Œ≤,œÑ

N X
T
X

(Yit ‚àí ¬µ ‚àí Œ≤t ‚àí Wit œÑ )2 œâÃÇisc .

(1.1)

i=1 t=1

The regression has time fixed effects and unit-specific weights. In comparison, the standard DID
estimator œÑÃÇ did for the treatment effect is

(¬µÃÇ, Œ±ÃÇ, Œ≤ÃÇ, œÑÃÇ

did

) = arg min

N X
T
X

Œ±,Œ≤,¬µ,œÑ

(Yit ‚àí ¬µ ‚àí Œ±i ‚àí Œ≤t ‚àí Wit œÑ )2 .

(1.2)

i=1 t=1

Our characterizations (1.1) and (1.2) make clear that relative to the SC estimator, the DID
estimator adds a unit fixed effect to the specification of the regression function, but it removes
the (unit) weights in the estimation. Contrasting the SC and DID estimators in this way suggests
a natural modification. Specifically, we propose the SDID estimator œÑÃÇ sdid , formally defined as:

(¬µÃÇ, Œ±ÃÇ, Œ≤ÃÇ, œÑÃÇ

sdid

) = arg min

Œ±,Œ≤,¬µ,œÑ

N X
T
X

(Yit ‚àí ¬µ ‚àí Œ±i ‚àí Œ≤t ‚àí Wit œÑ )2 œâÃÇi ŒªÃÇt .

(1.3)

i=1 t=1

The regression in (1.3) includes both unit and time fixed effects as well as weights, where the
weights are the product of unit weights œâÃÇi and time weights ŒªÃÇt , with both sets of weights are
derived from the data. In the spirit of the SC approach, these time weights ŒªÃÇt could be chosen
so that within a unit, the weighted average outcomes across periods approximate the target
P
period, Tt=1 ŒªÃÇt Yit ‚âà YiT for all i = 1, . . . , N ‚àí 1. Alternatively, one may wish to choose the time
weights partly to put more emphasis on recent periods. Thus, the proposed SDID estimator
differs from the DID estimator by allowing for both unit and time weights, and it differs from
the SC estimator by including unit-fixed effects and allowing for time weights.
Many approaches to SC settings, including Abadie, Diamond, and Hainmueller [2010, 2015],
Doudchenko and Imbens [2016], Xu [2017], Athey, Bayati, Doudchenko, Imbens, and Khosravi
[2017], Carvalho, Masini, and Medeiros [2018], Li and Bell [2017], can be thought of as either
focusing on constructing balancing weights, or focusing on modeling the conditional outcomes.
Ben-Michael, Feller, and Rothstein [2018] is an interesting exception. Their Augmented Synthetic Control (ASC) estimator uses a model for the conditional expectation of the last period‚Äôs
outcome YiT in terms of the lagged outcomes, in combination with the SC balancing weights,
3

in the spirit of unconfoundedness type methods, and in particular residual balancing methods
[Robins, Rotnitzky, and Zhao, 1994, Athey, Imbens, and Wager, 2018]. Their method also has
double robustness properties, but it cannot be characterized as a weighted regression estimator.
The importance of combining such outcome modeling and balancing/weighting and the associated double robustness are prominent features of the general program evaluation literature [e.g.,
Chernozhukov, Escanciano, Ichimura, Newey, and Robins, 2018b, Hirshberg and Wager, 2018,
Imbens and Rubin, 2015, Newey, Hsieh, and Robins, 2004, Scharfstein, Rotnitzky, and Robins,
1999], and most of the currently recommended estimators in that literature combine them.
An attraction of our regression set up is that it generalizes naturally to the case with multiple
treated units and multiple treated periods. We can in that case choose the unit weights for the
control units to balance the average of the treated units during the pre-treatment period, and
the time weights for the pre-treatment periods to balance the average post-treatment outcomes
for the control units. The regression set up can also easily accomodate covariates that vary by
unit and time by including them in the regression function. Unit-specific but time-invariant
covariates, which cannot be accomodated in the standard DID set up can be accomodated here
by adjusting the unit weights so the weights also balance these unit-specific covariates, and
similarly for time-varying covariates common to all units.
In the second half of the paper, we establish asymptotic properties of the SDID estimator
in a regime where both N and T are large. Establishing formal asymptotic properties for
estimators has been a major challenge in the SC literature. Throughout, we take the perspective,
common in panel data settings, that Y is a noisy estimate of an underlying signal matrix L, i.e.,
Yit = Lit +Wit œÑ +Œµit , where Wit denotes treatment assignment and Œµ is noise. The matrix L could
have a simple two-way fixed effect form, or have more generic low-rank structure (e.g., interactive
fixed effects, latent factor models) as in Athey, Bayati, Doudchenko, Imbens, and Khosravi
[2017], Bai and Ng [2002], Bai [2009], Bonhomme and Manresa [2015], Li and Bell [2017], and
Xu [2017]. We prove a variety of consistency results under different assumptions to highlight
the double robustness properties of our proposed estimator. One consistency result makes weak
assumptions on the weights but relatively strong assumptions on the outcome model L, while
another makes weak assumptions on the conditional outcome model but stronger assumptions
on the weights.
Ideally, the weights œâÃÇ and ŒªÃÇ would balance out the rows and columns of the underlying
signal matrix L in a way that eliminates bias, and moreover the weights would not depend
4

on the noise Œµ. This is essentially what occurs in the analysis of balancing methods under
unconfoundedness, where pre-treatment covariates are taken to be noiseless [Athey, Imbens, and
Wager, 2018, Graham, de Xavier Pinto, and Egel, 2012, Hainmueller, 2012, Imai and Ratkovic,
2014, Zubizarreta, 2015]. Here, however, the weights œâÃÇ and ŒªÃÇ are optimized to balance Y , not
L, and have a rich dependence on the noise Œµ that cannot be eliminated via sample splitting.
In Section 4.3, we use tools from modern empirical risk minimization theory to address both
challenges and to show that, despite being optimized to balance the observed Y , and despite the
fact that balancing the Y is a major challenge because the number of units is of the same order
as the number of time periods, the weights œâÃÇ and ŒªÃÇ balance the unobserved L well enough to
achieve consistency. In addition to proving consistency of the SDID estimator, our results also
allow us to establish conditions under which the original SC estimator is consistent given a lowrank L. The conditions on the weights for consistency of the SC estimator are stronger than
those needed for consistency of SDID because the latter has a double bias removal property
thanks to the time weights. Our asymptotic results require that we modify the original SC
weights using penalization to ensure that the number of units with positive weights increases in
large samples.
Finally, we present conditions that justify calculating the standard error for œÑÃÇ sdid using standard robust inference methods for DID regressions; we show that the standard robust standard
errors are valid despite the fact that they take the weights as fixed, that is, they do not algorithmically account for dependence of the weights on the data.

2

Synthetic Panel Methods

Suppose we have a balanced panel with observations on an outcome Yit , i = 1, . . . , N , t =
1, . . . , T , with some units treated in some periods, and the binary treatment indicator denoted
by Wit ‚àà {0, 1}. Throughout this paper, we assume that the outcomes Yit are generated as Yit =
Lit + Wit œÑ + Œµit , where Œµit is a noise term (potentially with correlation over time, that is, within
rows of the matrix Y ), and L is a baseline expected response matrix that may be correlated with
Wit . The methods proposed here can also be generalized to allow for heterogeneity in œÑ , e.g.,
perhaps treatment intensity depends on the number of time periods for which a unit has been
exposed to treatment. We refer to Athey and Imbens [2018] for a design-based interpretation of

5

these estimands using potential outcomes.
As motivation for our approach, consider models that parametrize the conditional expectation of the full set of unit and time period pairs: L = g(Œ∏), where g : Œò 7‚Üí RN √ó RT models the
conditional expectation of the control outcomes in terms of an unknown parameter Œ∏. Examples
of such panel data models include
g(Œ∏)it = Œ∏,

(constant)

g(¬µ, Œ±, Œ≤)it = ¬µ + Œ±i + Œ≤t ,
g(A, B)it =

R
X

Air Btr ,

Œ∏ = (¬µ, Œ±, Œ≤),

(two-way fixed effect)

Œ∏ = (A, B),

(factor model).

r=1

(In the two-way fixed effect and factor models we also need some normalizations, e.g., Œ±1 =
Œ≤1 = 0.) Now, given such a g(¬∑), a natural approach would be to fit Œ∏ and œÑ on all the data:




Œ∏ÃÇ, œÑÃÇ = arg min
Œ∏

N X
T 
X

Yit ‚àí g(Œ∏)it ‚àí œÑ Wit

2

.

(2.1)

i=1 t=1

In the case with g(Œ∏)it = ¬µ + Œ±i + Œ≤t , this leads us to the basic difference in differences estimation
strategy.
A challenge with this approach, however, is that if both N and T are large it may be difficult
to find a simple specification of g(¬∑) that fits well over the entire panel. But accurately estimating
the entire matrix L is more difficult than the actual challenge: in order to estimate œÑ , we only
need the expectation Lit locally, that is, for the control potential outcomes in the treated cell
or cells. Here, we propose addressing potential misspecification of g(¬∑) through weighting. We
apply weights œâÃÇi to the units and ŒªÃÇt to the time periods to form a ‚Äúsynthetic panel‚Äù on which
the model g(¬∑)it is approximately unbiased for Lit , and then we build a model of the conditional
expectation on this weighted panel:


Œ∏ÃÇ

sdid

, œÑÃÇ

sdid



= arg min
Œ∏

T 
N X
X

Yit ‚àí g(Œ∏)it ‚àí œÑ Wit

2

œâÃÇi ŒªÃÇt .

(2.2)

i=1 t=1

We can qualitatively think of the SDID estimator as relaxing the parallel trends assumption
in the DID estimator. Instead of assuming parallel trends for all units and all time periods,
6

the SDID estimator assumes that there exist unit and time weights such that the averaged
treated unit and the weighted average of the control units satisfy a parallel trends assumption,
and satisfy it not for all time periods but only for the averaged post-treatment period and the
weighted average of the pre-treatment periods.
One of our main findings is that, if we use synthetic control weights œâÃÇi and ŒªÃÇt , then relying on a simple two-way fixed effect model for g(¬∑) in (2.2) allows for consistent (in the large
N and large T sense) estimation of œÑ , even if the two-way fixed effects model may be badly
misspecified over the full panel. This finding is in line with a key insight from the program
evaluation literature is that often methods that combine weighting/balancing the treated and
control units with modeling the control outcome distribution outperform methods that only
model the outcomes, as well as methods that only balance treated and control units. ‚ÄúBetter‚Äù here includes both formal bias properties, as well as simulation evidence. A key formal
property is that of double robustness, where misspecification of only the balancing weights or
the conditional outcome model does not lead to inconsistency of œÑÃÇ [Athey, Imbens, and Wager,
2018, Belloni, Chernozhukov, and Hansen, 2014, Chernozhukov, Escanciano, Ichimura, Newey,
and Robins, 2018b, Hirshberg and Wager, 2018, Imbens and Rubin, 2015, Newey, Hsieh, and
Robins, 2004, Scharfstein, Rotnitzky, and Robins, 1999].
Before discussing our choice of weights further below, we note that (2.2) not only allows for
practical estimation of œÑ , but can also be used to build confidence intervals for œÑ . In the case of
two-way fixed effects with covariates,


¬µÃÇ, Œ±ÃÇ, Œ≤ÃÇ, Œ≥ÃÇ, œÑÃÇ

sdid



= arg min

Œ±,Œ≤,¬µ,œÑ

N X
T
X

(Yit ‚àí ¬µ ‚àí Œ±i ‚àí Œ≤t ‚àí Xit Œ≥ ‚àí Wit œÑ )2 œâÃÇi ŒªÃÇt ,

(2.3)

i=1 t=1

there is a wide variety of standard error estimates that that have been studied in the literature;
see Arellano [2003], Bertrand, Duflo, and Mullainathan [2004], Hansen [2007], Liang and Zeger
[1986], and Stock and Watson [2008] for examples and discussions. We present both formal
and simulation evidence that we can obtain valid confidence intervals for œÑ by applying these
methods directly to (2.2), treating the weights œâÃÇ and ŒªÃÇ as fixed, despite the fact that these
weights depend on the Yit ; see Section 5 for details.

7

2.1

Weighting for Synthetic Panels

In this paper, we focus on weighting in a generalization of the basic synthetic control setting of
Abadie, Diamond, and Hainmueller [2010]. We assume that exposed units i > N0 get treated
in time periods t > T0 , i.e Wit = 1 {i > N0 and t > T0 }. The weighting component of our
approach focuses on weights to balance the sample towards the treated unit / time period pairs.
A critical feature of our approach, appropriate to our treatment pattern W , is that we impose a
factor structure on the weights: Œ≥it = œâi Œªt . In addition to the factor structure we impose some
restrictions on the unit and time period weights. The weights are non-negative, and weight
groups œâ1:N0 , œâ(N0 +1):N , Œª1:T0 , Œª(T0 +1):T all sum to one. Moreover, we give equal weight to all the
exposed units and time periods. Formally, the set of weights we consider satisfy
(

N0
X

1
W = œâ ‚àà R œâi ‚â• 0;
œâi = 1; œâN0 +1 , ..., œâN =
N ‚àí N0
i=1
(
)
T0
X
1
L = Œª ‚àà RT Œªt ‚â• 0;
Œªi = 1; ŒªT0 +1 , ..., ŒªT =
.
T ‚àí T0
t=1
N

)
,
(2.4)

One possible choice for the weights is the SC weights Abadie, Diamond, and Hainmueller [2010,
2015], which Doudchenko and Imbens [2016] show, for the case without covariates, can be written
as (in the basic form we consider, synthetic control analyses only have one exposed unit, i.e.,
N0 = N ‚àí 1 and only one exposed time period, T0 = T ‚àí 1; however, the generalization is
immediate)

œâÃÇ sc = arg min
œâ‚ààW

T ‚àí1
X

N
‚àí1
X

t=1

i=1

!2
œâi Yit ‚àí YN t

.

(2.5)

In the current paper we modify these weights slightly by putting an L2 (ridge) penalty on the
weights to ensure that in larger samples there will be many units with non-zero weights, which is
important for the asymptotic properties of the estimator. Note that using an L1 (lasso) penalty
does not work because the weights are nonnegative and sum to one. We also consider the time
equivalent of the SC weights, which do not appear to have been considered in this literature:

sc

ŒªÃÇ = arg min
Œª‚ààL

N
‚àí1
X

T ‚àí1
X

i=1

t=1

!2
Œªt Yit ‚àí YiT

.

(2.6)

8

The time weights play somewhat of a different role than the unit weights. In some cases one may
wish to explicitly put more weights on recent periods than on distant periods, and not solely
have these weights determined by the similarity, in terms of outcomes, to the current period.
We may also wish to regularize the time weights to avoid putting most of the weight on a very
small number of units or time periods.
In cases where the data exhibit substantial trends, the SC time weights would tend to
concentrate on the most recent values. One modification in that case is to allow for an intercept
in the regression, and solve

ŒªÃÇisc = arg min

N
‚àí1
X

Œª0 ‚ààR, Œª‚ààL i=1

Œª0 +

T ‚àí1
X

!2
Œªt Yit ‚àí YiT

.

(2.7)

t=1

The intercept ŒªÃÇ0 is not needed for weighting, as the time dummies Œ≤t will be able to absorb any
time trends during the modeling stage. We refer to these weights as the intercept weights ŒªÃÇisc
t ,
and note that these weights are invariant to adding in any global time trend to our observations,
Yit ‚Üê Yit + f (t).
An alternative, for both the unit and time weights, is to use kernel weights. For example, if
there is only one exposed unit / time period, we could use
œâÃÇikernel


‚àùK

Yi(1:T ‚àí1) ‚àí YN (1:T ‚àí1)
hœâ



ŒªÃÇkernel
t

,


‚àùK

Y(1:N ‚àí1)t ‚àí Y(1:N ‚àí1)T
hŒª


,

(2.8)

for some kernel function K(¬∑), e.g., K(a) = exp(‚àía> a). We allow the tuning parameter to be
different for the unit and time dimension. We also consider nearest neigbor weights, where we
given constant weights to the Kœâ nearest units and the KŒª neast time periods [e.g., Abadie and
Imbens, 2006].
Using nearest neighbor methods to construct weights stresses the challenges in obtaining
formal large sample properties for the resulting estimators, and this explains partly the limited
nature of large sample results in the SC literature. If N is large, it is impossible to obtain a
‚Äúclose‚Äù match for Y(1:N ‚àí1)T because we are matching on N ‚àí1 variables with only T ‚àí1 potential
matches (e.g., Abadie and Imbens [2006]). Similarly, if T is large it is impossible to obtain close
matches for YN (1:T ‚àí1) because there are only N ‚àí 1 potential matches and T ‚àí 1 variables to
match on. With both N and T large it is impossible to obtain close matches in either direction.
9

Nevertheless, under some assumptions on the outcome model, the closest matches may be good
enough, in the sense that they match closely on the relevant underlying variables. For example,
if the data are generated by a two-way fixed effect model for L, matching on all the lagged
outcomes will not give a close match in terms of all the lagged outcomes. But, in large N and
large T such matching methods will lead to matches that are close in terms of the unit-fixed
effects, which is what matters. Our formal results show that this holds in general factor models
for L.

3

Panel Estimators as Bias Reduction Methods

In the previous section, we introduced SDID as a flexible approach to estimating causal effects in
panels where both N and T are moderately large. To gain further intuition about the method,
we focus here on the behavior of the SDID estimator in the case where we fit a two-way fixed
effects model without any additional covariates, and only unit N in time period T gets treated.
In this case, the SDID estimator allows for a simple, closed form solution that allows for a
transparent comparison with the SC estimator.
We use the following notation. Partition the N √ó T matrix of observed outcomes Y , and
other comformable matrices, by treatment group and pre/post treatment period:
Ô£´
Y =Ô£≠

Y::

Y:T

YN : YN T

Ô£∂
Ô£∏,

where Y:: , Y:T , YN : , and YN T are (N ‚àí 1) √ó (T ‚àí 1), (N ‚àí 1) √ó 1, 1 √ó (T ‚àí 1), and 1 √ó 1 matrices
respectively. Also define Yi: to be a T ‚àí 1 dimensional row vector and define Y:t to be a N ‚àí 1
dimensonal column vector, each with typical element Yit . Define the averages for the three sets
of control outcomes,

Y

c,pre

N
‚àí1 X
T ‚àí1
X
1
Yit ,
=
(N ‚àí 1)(T ‚àí 1) i=1 t=1

Y

10

c,post

N ‚àí1
1 X
=
YiT ,
N ‚àí 1 i=1

and
T ‚àí1

Y

t,pre

=

1 X
YN t .
T ‚àí 1 t=1

In this setting, the basic difference in difference estimator (1.2) can be written as
œÑÃÇ did = YN T ‚àí YÃÇNdid
T (0),
YÃÇNdid
T (0)

= ¬µÃÇ + Œ±ÃÇN + Œ≤ÃÇT = Y

c,pre



+ Y

t,pre

‚àíY

c,pre





+ Y

c,post

‚àíY

c,pre



We can see the DID estimator as doubly bias-adjusting the simple average Y
bias adjustment, Y

t,pre

‚àíY

c,pre

(3.1)
.
c,pre

, with the first

, taking into account stable differences between the treated unit

and the control units and the second bias adjustment, Y

c,post

‚àíY

c,pre

, taking into account stable

differences over time for the control group. The main weakness of this basic DID estimator,
however, is of course that it is only valid under a well-specified two-way fixed effects model,
which is a very strong assumption when both N and T are moderately large.

3.1

Synthetic Control as a Single Bias Reduction Method

The main idea of the SC approach is to re-weight the control rows i = 1, ..., N ‚àí 1 of the matrix
Y with weights œâÃÇisc such as to make the time trends among the weighted controls and the treated
unit track each other. In the spirit of (3.1), we can write the SC estimator (1.1) as a weighted
bias-reduced estimator:
œÑÃÇ sc

N ‚àí1 T ‚àí1
N
‚àí1
X
1 X X sc
sc
sc
= YN T ‚àí YÃÇN T (0), YÃÇN T (0) =
œâÃÇ Yit +
œâÃÇisc
T ‚àí 1 i=1 t=1 i
i=1

YiT

!
T ‚àí1
1 X
‚àí
Yit . (3.2)
T ‚àí 1 t=1

The bias adjustment uses a weighted average of the post-treatment control outcomes, with
weights œâÃÇisc minus a doubly weighted average of the pre-treatment control outcomes.
The SC estimator presents an obvious improvement over the DID estimator in its use of
weights to address potential misspecification of the basic two-way fixed effects model. However,
unlike (3.1), the estimator (3.2) appears to be ‚Äúmissing‚Äù a second bias correction term of the
P ‚àí1
PN ‚àí1 sc
form 1/(T ‚àí 1) Tt=1
(YN t ‚àí i=1
œâÃÇi Yit ) that seeks to correct for a potential systematic failure
of the weights œâÃÇisc to achieve balance in the pre-treatment periods. It is interesting to note that
11

if the weights œâÃÇisc were to balance the pre-treatment periods perfectly, so that

YN t ‚àí

N
‚àí1
X

œâÃÇisc Yit = 0,

for all t = 1, . . . , T ‚àí 1,

(3.3)

i=1

then the second bias correction term would be numerically zero, and so SC could implicitly be
seen as a double-bias reduction method.1 Typically, however, perfect balance as in (3.3) does
not hold, and so the lack of this second bias-correction term may affect the properties of the SC
estimator.

3.2

Synthetic Difference In Differences as a Double Bias Reduction
Method

The SDID estimator addresses the case where the synthetic control adjustment does not completely balance the underlying signal in the pre-treatment periods. In the special case with only
unit N treated in period T , the SDID estimator (1.3) can be thought of as bias-adjusting the
SC estimator based on the pre-treatment discrepancies, weighted by ŒªÃÇsc
t :
sdid
sc
œÑÃÇ sdid = YN T ‚àí YÃÇNsdid
T (0), YÃÇN T (0) = YÃÇN T (0) +

T ‚àí1
X

ŒªÃÇsc
t

YN t ‚àí

t=1

N
‚àí1
X

!
œâÃÇisc Yit .

i=1

We can also write the SDID estimator as a symmetric version of (3.2), with

YÃÇNsdid
T (0) =

N
‚àí1 X
T ‚àí1
X

œâÃÇisc ŒªÃÇsc
t Yit +

i=1 t=1

T ‚àí1
X

ŒªÃÇsc
t

YN t ‚àí

t=1

N
‚àí1
X

!
œâÃÇisc Yit +

N
‚àí1
X

i=1

œâÃÇisc

YiT ‚àí

T ‚àí1
X

!
ŒªÃÇsc
t Yit

. (3.4)

t=1

i=1

That is, compared to the simple weighting estimator YÃÇNweight
there are two (weighted) bias
T
adjustments,
T ‚àí1
X
t=1

ŒªÃÇsc
t

YN t ‚àí

N
‚àí1
X
i=1

!
œâÃÇisc Yit

and

N
‚àí1
X

œâÃÇisc

i=1

1

YiT ‚àí

T ‚àí1
X

!
ŒªÃÇsc
t Yit

,

t=1

An equivalent statement of this fact is that if (3.3) were to hold, then adding row fixed effects to to the
synthetic control estimator (1.1) would not change the point estimate œÑÃÇ .

12

whereas the SC estimator has only one bias adjustment (the second one), similar to the way the
DID estimator has two bias adjustments in the unweighted case.
The problem of turning synthetic controls into a double-bias removal style estimator has also
been recently considered by Ben-Michael, Feller, and Rothstein [2018]. Their main proposal,
the augmented synthetic control (ASC) estimator, involves fitting a model for the conditinoal
expectation m(¬∑) for YiT in terms of the lagged outcomes Yi(1:(T ‚àí1)) , and then using this fitted
model to ‚Äúaugment‚Äù the basic synthetic control estimator

YÃÇNasc
T (0) =

N
‚àí1
X

œâÃÇisc YiT +

mÃÇ(YN : ) ‚àí

i=1

= mÃÇ(YN : ) +

N
‚àí1
X

!
œâÃÇisc mÃÇ(Yi(1:(T ‚àí1)) )

i=1
N
‚àí1
X

œâÃÇisc



!

Yit ‚àí mÃÇ(Yi(1:(T ‚àí1)) )
.

(3.5)

i=1

The first representation of the ASC estimator emphasizes its interpretation as a modification
of the SC estimator. It uses a cross-section model for the last period‚Äôs outcome to remove
biases from the standard SC estimator. The second representation stesses the connections to the
unconfoundedness literature. The starting point is a model for the potential outcomes in the last
period as a function of lagged outcomes. On its own this would suggest the estimator mÃÇ(YN : ); the
ASC estimator then robustifies this using a weighted average of the residuals. This construction
is related to the residual balancing estimators in the original double robust literature [Robins,
Rotnitzky, and Zhao, 1994] or in high-dimensional settings [Athey, Imbens, and Wager, 2018],
where now the SC weights can be interpreted as a type of covariate-balancing inverse-propensity
weights. Such adjustments make the estimator doubly robust under appropriate conditions. A
more recent paper, Chernozhukov, Wuthrich, and Zhu [2018c], takes a similar approach to BenMichael, Feller, and Rothstein [2018]; however, they swap the role of units and time periods and
present formal results under strong time-homogeneity assumptions that, in particular, rule out
the low-rank model Yit = Lit + Wit œÑ + Œµit .
The second representation of the ASC estimator makes clear that its formal justification
would be standard under a unconfoundedness assumption with the lagged outcomes playing the
role of the pre-treatment variables, given a fixed number of pretreatment periods and large N .
By the same token it would make the justification more difficult under general factor structures
and with large T . This representation also highlights the feature of this estimator that it includes

13

the lagged outcomes in exactly the same way that pre-treatment variables would be included in
unconfoundedness-type analyses.2 This is in contrast to many panel data models such as fixed
effect and factor models that incorporate lagged outcomes in the model in a way that is similar
to the way the last period outcomes are treated, namely as noisy measures of the underlying
unobserved components that are critical for prediction.
Despite their different motivations, ASC and SDID share an interesting connection: In the
special case with a single treated unit / period and with a linear mÃÇ(¬∑) model, the SDID estimator
in (3.4) and the ASC estimator in (3.5) are very similar. In fact, they would be equivalent
if we impose the additional restriction on the ASC estimator that the slope coefficients are
nonnegative, positive and to sum to one. This connection suggests that weighted double biasremoval methods are a natural way of working with panels where we do not believe the basic DID
approach to be appropriate. This being said we emphasize that, once we move past the most
basic model, e.g., we have covariates or multiple treated units and periods, or we use more flexible
specifications for m(¬∑), then the connection between the two methods breaks down. Moreover,
as Ben-Michael, Feller, and Rothstein [2018] motivate their estimator using unconfoundedness
type arguments, they do not provide consistency results for the type of factor models considered
here. In addition the ASC estimator does not have a weighted least squares interpretation,
which is helpful in accommodating covariates.

4

Formal Results

In this section, we develop the properties of the SDID estimator. First, we consider properties
that hold when the DID model is correctly specified; second, we discuss robustness properties
provided by weighting. For simplicity, in this section we focus on the single exposed unit / time
period setting. In this case, we can write the SDID treatment effect estimate œÑÃÇ as
bN T , L
bN T = ¬µÃÇ + Œ±ÃÇN + Œ≤ÃÇT ,
œÑÃÇ = YN T ‚àí L

(4.1)

2
Ben-Michael, Feller, and Rothstein [2018] also propose a suite of methods that can be used when both
unit-specific covariates and lagged outcomes are available. For example, they propose first projecting out the
component of the outcomes that can be explained using the unit-specific covariates, and then running ASC on
the residuals.

14

where the parameters ¬µÃÇ, Œ±ÃÇ and Œ≤ÃÇ are as defined in (1.3). Here, we provide several results
bN T to LN T , implying that the error of œÑÃÇ is asymptotically fully
establishing convergence of L
determined by the intrinsic noise in YN T . In the following section, we then build on these results
to provide methods for inference about œÑ in settings with more than one treated unit. Recall
that we assume that Yit is generated as below, and we follow the convention that ‚Äú:‚Äù always
indexes over unexposed units or time periods.
Assumption 1. We have N, T ‚Üí ‚àû, and there is a deterministic matrix L such that Yit =
Lit + Wit œÑ + Œµit with Wit = 1 {i = N, t = T } and Œµit ‚àº N (0, œÉ 2 ), independently for each cell
(i, t).3
We consider two distinct sets of conditions: First, we examine the case where the two-way
fixed effects model is well specified, i.e., Lit = ¬µ+Œ±i +Œ≤t , and show that SDID is consistent under
very flexible conditions. The main point here is that using data-adaptive weights œâÃÇ and ŒªÃÇ does
not break DID when the outcome model is well specified. Second, we consider the generalized
fixed effects model, i.e., where we make only weak assumptions on L. Here, basic DID is
inconsistent; however, we show that SDID with penalized SC weights is consistent whenever
L is well-approximated by a matrix of rank r  min(N, T ) and can be consistent at the rate
p
log(T )/ min(N, T ) if it is well-approximated by a matrix of fixed rank.

4.1

Properties in the Well-Specified Two-Way Fixed Effects Model

Our first result shows that SDID is consistent and asymptotically normal in the well-specified
model, that is, where Lit = ¬µ + Œ±i + Œ≤t . Here we consider the following kernel weights:

œâÃÇi = P

1
i6=N



1
T ‚àí1

1



kYi: ‚àí YN : k22 ‚â§ cœâ

1
T ‚àí1



kYi: ‚àí YN : k22 ‚â§ cœâ

, ŒªÃÇt = P

1

t6=T



1
N ‚àí1

1



kY:t ‚àí Y:T k22 ‚â§ cŒª

1
N ‚àí1



kY:t ‚àí Y:T k22 ‚â§ cŒª

 , (4.2)

for i = 1, ..., N ‚àí 1 and t = 1, ..., T ‚àí 1, where cœâ and cŒª are tuning parameters. For our result,
we also make generative assumptions that let us characterize the behavior of nearest neighbor
matching with noisy data; see Bonhomme, Lamadon, and Manresa [2017] for related results on
the behavior of clustering panel data.
3

We make this assumption for simplicity of exposition only. In Section 8.3 of the appendix we state and prove
results for heteroskedastic and auto-correlated subgaussian errors, and also for choices of weights œâÃÇ, ŒªÃÇ that we
do not consider here.

15

Assumption 2. Lit = ¬µ + Œ±i + Œ≤t ; Œ¥Œ±,i := |Œ±i ‚àí Œ±N | and Œ¥Œ≤,t := |Œ≤T ‚àí Œ≤t | are i.i.d. random
variables such that corresponding densities fŒ¥Œ± and fŒ¥Œ≤ are bounded at zero.
Theorem 1. Suppose Assumptions 1 and 2 hold and lim N/T = œÅ ‚àà (0, 1); then for the weights
bN T is consistent, that is,
œâÃÇi and ŒªÃÇt defined above we have the following: L
bN T ‚àí LN T ‚Üíp 0,
L
and
1
q



b N T ‚àí LN T
L



‚Üí N (0, œÉ 2 )

(4.3)

kœâÃÇ: k2 + kŒªÃÇ: k2

)
)
‚àö
‚àö
, cœâ = œÉ 2 +o(1), aN,T ‚Üí ‚àû, and cŒª = œÉ 2 +bN,T log(T
, cŒª = œÉ 2 +o(1),
provided cœâ = œÉ 2 +aN,T log(N
T
N

bN,T ‚Üí ‚àû
Note that matching discrepancies cŒª and cœâ in (4.2) do not go to zero, instead they go to œÉ 2 ,
because with N and T large all rows and columns of Y will have distances that concentrate at
œÉ 2 away. We also note that the weighting function considered here is approximately equivalent
‚àö
to k-nearest neighbors weighting, where kœâ = T cœâ ‚àí œÉ 2 is approximately the number of units
‚àö
that we average over and kŒª = T cŒª ‚àí œÉ 2 is the approximate number of used time periods.

4.2

Double Robustness Part I: The Fixed Effects Model with General Weights

Next we show that, if fixed-effects model is correct, then our method is consistent essentially
regardless of the weights we use. Instead of requiring a specific functional form for the weights,
we only ask that we not use the T -th time period when picking the row weights œâÃÇ, and we
not use the N -th row when picking ŒªÃÇ, and that the weights are not too concentrated on a few
units or time periods. All algorithms considered in this paper, ranging from synthetic control
weighting to nearest neighbor matching, satisfy this condition.
Assumption 3. We choose weights such that œâÃÇ: ‚ä•
‚ä• Y:T and ŒªÃÇ: ‚ä•
‚ä• YN : .

16

Theorem 2. Under Assumption 1, suppose moreover that Lit = ¬µ + Œ±i + Œ≤t . Then, provided we
use weights œâÃÇ and ŒªÃÇ satisfying Assumption 3 such that
p
max{N, T }kœâÃÇ: k2 kŒªÃÇ: k2 ‚Üíp 0,

kœâÃÇ: k2 , kŒªÃÇ: k2 ‚Üíp 0,

(4.4)

bN T ‚àí LN T ‚Üíp 0.
we have L

4.3

Double Robustness Part II: The Approximate Factor Model with
SC Weights

In this section we relax the modeling assumptions from the above section, and simply require
that L be approximable by a low-rank matrix. This type of model was used to motivate the
SC approach by Abadie, Diamond, and Hainmueller [2010], and has also been studied in other
contexts by, e.g., Athey, Bayati, Doudchenko, Imbens, and Khosravi [2017] and Bai [2009]. Our
goal is to show that, with well chosen weights, SDID remains consistent. Here, we focus on a
form of penalized synthetic control weights:

œâÃÇ sc (aœâ ) = arg min

Ô£±
T ‚àí1
Ô£≤X

œâ‚ààW Ô£≥

ŒªÃÇsc (aŒª ) = arg min
Œª‚ààL

Ô£±
‚àí1
Ô£≤N
X
Ô£≥

YN t ‚àí

t=1

i=1

N
‚àí1
X

!2
œâi Yit

: kœâ: k2 ‚â§ aœâ

Ô£º
Ô£Ω

,

Ô£æ
Ô£º
!2
T ‚àí1
Ô£Ω
X
Œªt Yit
: kŒª: k2 ‚â§ aŒª ,
‚àí
Ô£æ
i=1

YiT

(4.5)

t=1

where aŒª and aœâ are tuning parameters. The penalization is important to ensure that in large
samples there will be many units and time periods with positive weights.
The key difficulty in showing that these SC weights œâÃÇ and ŒªÃÇ were chosen to balance rows and
columns of Y ; however, what we really need for useful inference with an approximately low-rank
L is for œâÃÇ and ŒªÃÇ to balance the the rows and columns of L. Furthermore, the weights defined in
(4.5) have a complicated dependence on the noise Œµ = Y ‚àíL, and the panel structure means that
we cannot address this challenge via sample splitting as in, e.g., Chernozhukov, Chetverikov,
Demirer, Duflo, Hansen, Newey, and Robins [2018a]. Here, we establish conditions under which
our SDID estimator with data-dependent weights (4.5) is consistent in the approximately lowrank model. As an additional benefit, we also prove that the basic SC estimator is consistent in
17

the motivating model from Section 2.2 of Abadie, Diamond, and Hainmueller [2010].
In order to spell out our result, we first define infeasible SC weights that balance the underlying effect matrix L rather than the observed matrix Y :

œâ ? (aœâ ) = arg min
œâ‚ààW

Œª? (aŒª ) = arg min
Œª‚ààL

Ô£±
T ‚àí1
Ô£≤X
Ô£≥
t=1
Ô£±
‚àí1
Ô£≤N
X
Ô£≥

LN t ‚àí

N
‚àí1
X

!2
œâi Lit

: kœâ: k2 ‚â§ aœâ

Ô£º
Ô£Ω

,

Ô£æ
Ô£º
!2
T ‚àí1
Ô£Ω
X
‚àí
Œªt Lit
: kŒª: k2 ‚â§ aŒª ,
Ô£æ
i=1

LiT

i=1

(4.6)

t=1

We then the following identification assumption in terms of these weights. Specifically we ask
that these population weights succeed in obtaining balance, i.e., the last row and column of the
matrix can in fact be usefully represented via a convex combination of other rows. Given this
assumption, SDID with penalized SC weights is consistent.
Theorem 3. Given Assumption 1, and that we choose weights via (4.5) with aœâ and aŒª satisfying
Œ¥œâ = kLN : ‚àí œâ:? (aœâ )0 L:: k;
Œ¥Œª = kL:T ‚àí L:: Œª?: (aŒª )k;
Œ¥sdid = |LN T ‚àí (œâ:? (aœâ )0 L:T + LN : Œª?: (aŒª )) ‚àí œâ:? (aœâ )0 L:: Œª?: (aŒª )| .
Then for rŒª , rœâ defined in Lemma 4,
b N T ‚àí LN T
L


h
p
‚àö i
= OP Œ¥sdid + aŒª Œ¥œâ + œÉ min{ log(N ), aœâ N }
h
p
‚àö i
+ aœâ Œ¥Œª + œÉ min{ log(T ), aŒª T }

+ min(aœâ rŒª , aŒª rœâ ) .

In the case that N/T ‚Üí Œ∫ ‚àà (0, ‚àû), œÉ = O(1), L has exact (rather than approximate) low rank,
‚àö
and we choose aŒª , aœâ , = O(1/ N ), this bound simplifies to
r
bN T ‚àí LN T = OP
L

Œ¥sdid +

max {log(N ), rank(L), Œ¥œâ , Œ¥Œª }
N

18

!
.

The key technical result underlying Theorem 3 is the following lemma, which establishes
convergence of the feasible SC weights (4.5) that balance Y to the infeasible weights (4.6) that
balance L. We emphasize that our result does not rely on the weights œâÃÇ: and ŒªÃÇ: converging to
œâÃÇ:? and ŒªÃÇ?: respectively at a particularly fast rate. In our analysis, we only use the trivial bounds
kœâÃÇ: ‚àí œâÃÇ:‚àó k ‚â§ kœâÃÇ: k + kœâÃÇ:? k, etc., and in fact the weights œâÃÇ: and ŒªÃÇ: do not appear to be particularly
stable empirically. Rather, our result only relies on the feasible and oracle weights having similar
‚Äúbalancing‚Äù properties, i.e., for L0:: (œâÃÇ: ‚àí œâ:? ) and L:: (ŒªÃÇ: ‚àí Œª?: ) to be small as established below.
Lemma 4. Given Assumption 1, choose weights via (4.5) with aœâ , aŒª . Then in terms of Œ¥œâ , Œ¥Œª
defined in Theorem 3,
kL0:: (œâÃÇ: ‚àí œâ:? (aœâ ))k2 = OP (rœâ ) and kL:: (ŒªÃÇ: ‚àí Œª?: (aŒª ))k2 = OP (rŒª ),
where

p
rœâ = max Œ¥œâ , œÉ approx-rankœâ ,
r

‚àö ‚àö

np
‚àö o
‚àö
4
œÉ aœâ max
N , N T log(N ), œÉ T aœâ , œÉ min
log(N ), aœâ N
;

p
rŒª = max Œ¥Œª , œÉ approx-rankŒª ,
r

‚àö ‚àö

np
‚àö
‚àö o
4
œÉ aŒª max
T , N T log(T ), œÉ N aŒª , œÉ min
log(T ), aŒª T
.
Here approx-rankŒª and approx-rankœâ are lower bounds on the rank of L:: that ignore small
nonzero singular values,
Ô£±
Ô£≤

Ô£∂Ô£º
np
o Ô£Ω
‚àö
s2k , sr+1 min
log(N ), aœâ N Ô£∏ ,
approx-rankœâ = min
r ‚â• œÉ ‚àí1 min Ô£≠aœâ
r‚àà1,2,... Ô£≥
Ô£æ
k>r
Ô£±
Ô£´
Ô£∂Ô£º
sX
Ô£≤
np
o Ô£Ω
‚àö
approx-rankŒª = min
r ‚â• œÉ ‚àí1 min Ô£≠aŒª
s2k , sr+1 min
log(T ), aŒª T Ô£∏ ,
r‚àà1,2,... Ô£≥
Ô£æ
Ô£´

sX

k>r

where s1 , s2 , . . . is the decreasing sequence of singular values of L:: .
Finally, as discussed above, we can also use Lemma 4 to prove consistency of SC estimation
19

in the approximately low-rank model under the assumptions of Theorem 3. The main difference
between Theorem 5 and Theorem 3 above is that the error depends on the performance of an
oracle SC estimator rather than that of the the oracle SDID estimator.
Theorem 5. Given Assumption 1, choose œâÃÇ via (4.5) with constant aœâ . Then in terms of
Œ¥sc = |œâ:‚àó (aœâ ) ¬∑ L:T ‚àí LN T | and rœâ defined in Lemma 4,

|œâÃÇ: (aœâ ) ¬∑ Y:T ‚àí LN T | = Op

4.4

h
i
Œ¥sc + aœâ + min rœâ kŒªÃÉk2 + aœâ kL:T ‚àí L:: ŒªÃÉk2 .

(4.7)

ŒªÃÉ‚ààRT ‚àí1

Asymptotic Properties with Multiple Exposed Units and Time
Periods

We will now consider the problem of inference in a simple setting with N1 exposed units and
T1 exposed time periods, in which all exposed units start treatment at the same time T0 + 1.
Relative to the setting of the previous section, we also consider autocorrelated errors.
Assumption 4. We have N0 , T0 ‚Üí ‚àû, and there is a deterministic N0 + N1 √ó T0 + T1 matrix L
such that Yit = Lit + Wit œÑ + Œµit with Wit = 1 {i > N0 , t > T0 } and the rows of Œµ are independent
and distributed according to the gaussian AR(1) process Œµi,t+1 = œÅŒµi,t + Œæi,t+1 with œÅ ‚àà [0, 1) and
iid shocks Œæi,t ‚àº N (0, œÉŒæ2 ).
In this setting, our essential result is that our estimator œÑÃÇ is asymptotically normal and
unbiased when our total number N1 T1 of treated observations is small relative to the number of
untreated observations and our number of treated units N1 is small relative to our number of
post-treatment periods T1 . The following theorem formalizes this. Its proof, as well as a more
complete discussion of our estimator in this setting, appears in Section 8.5 of the appendix.
Theorem 6. Under Assumption 4, let œÑÃÇ be defined as in (4.1) with weights

œâÃÇ = arg min

Ô£±
T0
Ô£≤X

œâ‚ààW Ô£≥

ŒªÃÇ = arg min
Œª‚ààL

t=1

Ô£±
‚àí1
Ô£≤N
X
Ô£≥

i=1

1
N1

N1
X

Yit ‚àí

N0
X

!2
œâi Yit

: kœâ: k2 ‚â§ aœâ

Ô£º
Ô£Ω

Ô£æ
i=1
i=N0 +1
Ô£º
!2
T0
Ô£Ω
X
X
1
Yit ‚àí
Œªt Yit
: kŒª: k2 ‚â§ aŒª .
Ô£æ
T1 i=T +1
t=1
0

20

,

In terms of the correponding oracle weights œâ ? , Œª? defined by substituting L for Y in the definitions above, let

Œ¥œâ =

kN1‚àí1

NX
0 +N1

Li: ‚àí œâ?0 L:: k,

i=N0 +1

Œ¥Œª = kT1‚àí1

TX
0 +T1

L:t ‚àí L:: Œª? k,

t=T0 +1

Œ¥sdid = (N1 T1 )

‚àí1

NX
0 +T1
0 +N1 TX

Lit ‚àí (œâ?0 L:T + LN : Œª? ‚àí œâ?0 L:: Œª? ) .

i=N0 +1 t=T0 +1
‚àí1/2

If we choose aœâ . N0

‚àí1/2

and aŒª . T0

, then

NX
0 +N1 TX
0 +T1
p
1
‚àö
N1 T1 (œÑÃÇ ‚àí œÑ ) =
Œµit + op (1),
N1 T1 i=N +1 t=T +1
0

(4.8)

0

provided the following conditions hold:



T0
N0
N1 T1  min 2 ,
,
;
Œ¥sdid max {Œ¥Œª2 , 1} max {Œ¥œâ2 , 1}
)
(
1/2
N
N
0
1/2
0
;
,
N1 T1  min
1/2
log(T
0)
T0 log(T0 )
Ô£±
Ô£º
Ô£≤
Ô£Ω
N0
 .

N1  min T1 ,
‚àí1/2 Ô£æ
Ô£≥
approx-rank L , T
1

::

1

The asymptotic characterization (4.8) implies that under the stated conditions, our estimator
is asymptotically normal with variance VœÑ on the order of (N1 T1 )‚àí1 .

5

Large-Sample Inference of Treatment Effects

In the literature on synthetic controls, the dominant approach to uncertainty quantification is
via placebo tests [Abadie, Diamond, and Hainmueller, 2010, 2015]. The main idea is to consider
the behavior of synthetic control estimation when we replace the unit that was in fact exposed to
21

the treatment with different units that were not exposed. Such a placebo test is closely connected
to permutation tests in randomization inference. However, in many applications of synthetic
controls, the exposed unit was not chosen at random, in which case placebo tests do not have the
formal properties of randomization tests [Firpo and Possebom, 2018, Hahn and Shi, 2016], and
so may need to be interpreted via a more qualitative lens. Here, we take a different perspective,
and consider inferential methods motivated by large sample asymptotics. Our proposal builds on
methods for robust inference in large panels that were originally developed for the well-specified
two-way fixed effects models [e.g., Arellano, 2003, Hansen, 2007, Liang and Zeger, 1986].
As shown in Theorem 6, the synthetic difference in differences estimator is asymptotically
Gaussian under appropriate conditions,
(œÑÃÇ ‚àí œÑ )



VœÑ1/2 ‚áí N (0, 1) ,

(5.1)

where the asymptotic variance VœÑ is determined by the sampling errors {Œµit : i > N0 , t > T0 } of
the observations under treatment and does not depend on the noise in the synthetic control
weights œâÃÇ or ŒªÃÇ. The uphot is that we can estimate VœÑ and build confidence intervals for œÑ using
standard methods for large-sample inference for weighted panels, while treating œâÃÇ and ŒªÃÇ as fixed.
Here, we focus on estimating VœÑ by applying the jackknife [Miller, 1974, Efron and Stein,
1981] to the weighted regression (2.3)‚Äîagain, with œâÃÇ and ŒªÃÇ treated as fixed. Following Bertrand,
Duflo, and Mullainathan [2004], we seek robustness to errors that may be correlated within rows,
and so we repeatedly run the regression (2.3) with one row i omitted at a time to get œÑÃÇ (‚àíi) , and
use the variation of these œÑÃÇ (‚àíi) to get an estimate the variance VœÑ of the original treatment effect
estimate œÑÃÇ . Although we do not do so here, one could also estimate VœÑ via other methods for
heteroskedasticity-consistent variance estimation [Efron, 1982, MacKinnon and White, 1985].
In terms of connections to the literature, we note that a qualitatively result was used by
Bonhomme and Manresa [2015] to provide large-sample inference for panels with grouped fixed
effects: They rely on clustering to discover groups, but then show that inference remains asymptotically valid while ignoring the effect of clustering. Meanwhile, Chernozhukov, Wuthrich, and
Zhu [2017] propose a method for inference in synthetic control problems with a single treated
unit that relies on the prediction residuals Yit ‚àí LÃÇit over the control units being representative
of the counterfactual untreated residuals Lit + Œµit ‚àí LÃÇit over the treated units.
Finally, we note that although the above discuss has focused on inference that is robust to
22

within-row correlations, our jackknife-based procedure can be flexibly adapted to reflect different
sampling assumptions. If we believe that the Œµit were all independent, we could also apply a
cell-wise jackknife (i.e., where the jackknife omits one cell rather than one row at a time),
potentially allowing for power gains when there are few treated units. Meanwhile, if we believe
the Œµit may be correlated within rows but that the noise process eventually mixes (i.e., there are
no long-range correlations), we could consider and intermediate solution that divides each row
into blocks and then applies a block-wise jackknife [Kunsch, 1989].

6

Empirical Evaluation

6.1

Placebo Evaluation: Predicting the Prevalence of Smoking

In one of the original studies on synthetic control methods, Abadie, Diamond, and Hainmueller
[2010] focus on estimating the causal effect of anti-smoking legislation in California (Proposition
99). As discussed above, when only a single cell (N, T ) is treated, synthetic control methods
can be understood as producing a prediction LÃÇN T of what the unit-N time-T outcome would
have been without treatment, and then estimating œÑÃÇ = YN T ‚àí LÃÇN T . This suggests a simple
placebo procedure for evaluating various synthetic control methods in a realistic environment:
If we run synthetic control methods while singling out as ‚Äútreated‚Äù a cell (n, t) that did not
actually receive treatment, we should expect LÃÇnt to be a good prediction of the realized outcome
Ynt . Here, we benchmark difference in differences, synthetic controls, and synthetic differences
in differences against each other by running such a placebo analysis, and comparing the errors
of each method in predicting Ynt .
The original dataset of Abadie, Diamond, and Hainmueller [2010] had observations for 39
states (including California) from 1970 through 2000, where California is treated from 1989
onwards. We follow Abadie, Diamond, and Hainmueller [2010] in using per capita smoking as
the outcome. Here, we focus only on time periods 1970-1988, in which none of the units were
treated, and seek to predict the outcome of a focal cell using all data from earlier years as well
as data from different states in the target year by running different methods with the focal cell
considered as ‚Äútreated‚Äù. For example, when predicting the outcome for Arizona in 1985, we run
the methods under comparison with the other 38 states used as the ‚Äúcontrol states‚Äù and the
years 1970-1984 used as the ‚Äúpre-treatment years‚Äù (and we do not used any data from 1986 or
23

50

‚óè

20
10

‚óè

‚óè
‚óè
‚óè
‚óè
‚óè ‚óè

5

synthetic control RMSE

‚óè

‚óè
‚óè

‚óè

‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè

‚óè

2

‚óè
‚óè

‚óè

‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè

‚óè

‚óè ‚óè

‚óè

2

4

6

8

synthetic diff‚àíin‚àídiff RMSE

Figure 1: Comparison of the per-state root-mean squared error for SDID and SC. California
is highlighted in blue.
later).
Given this setup, we make predictions for all states in years 1980-1988 (i.e., we run DID, SC
and SDID separately for each focal state-year pair), and calculate the square root of the average
squared error:
v
u 1988 
2
u1 X
RMSEi = t
Yi,t ‚àí LÃÇi,t ,
9 t=1980
for all 39 states. In this example, we use L2 -penalized SC weights

œâÃÇ sc = arg min
œâ‚ààW

Ô£±
Ô£≤

1
Ô£≥T ‚àí 1

T ‚àí1
X
t=1

YN t ‚àí

N
‚àí1
X

!2
œâi Yit

+ Œ∂ kœâk22

Ô£º
Ô£Ω

,

(6.1)

Ô£æ

i=1

where we set Œ∂ to be the average of (Yi,t+1 ‚àí Yi,t )2 over the pre-treatment data. For time weights
ŒªÃÇ, we use an analogously penalized version of the intercept weights Œªisc
t to deal with the trends
in smoking rates.
We report the results on the RMSE in Figure 1 for each state and the average over all 39
states; Table 3 in the Appendix has detailed results. We find that the SDID method does
substantially better than the SC and DID method in terms of predictive accuracy, with the
SC outperforming the DID method: in Figure 1 almost all the pairs of RMSE lie above the 45
24

degree line, showing that the average RMSE based on the SDID estimator is lower than that
based on the SC estimator, for almost every state. The median improvement of the state-wise
root-mean-squared error of SDID over the state-wise root-mean-squared error of SC is 15% (and
the corresponding improvement over DID is 50%).
We can gain further insight into the behavior of different methods by comparing the onestep-ahead predicted trajectories LÃÇi,t to the true ones Yi,t . We see that SC struggle when a
state doesn‚Äôt fit neatly within the convex hull of other states (e.g., in the case of Utah), whereas
difference-in-differences does poorly when the temporal pattern of a state doesn‚Äôt match the
average temporal pattern (e.g., in Alabama). Of course, it is unlikely that practitioners would
use SC to study a state that does not fit within the convex hull of other states, as is the case of
Utah here, as standard goodness of fit checks would flag SC as an inappropriate method to use
here. However, we find that SDID out-performs SC in states where the latter are appropriate
(such as California), and remain robust in cases where the latter are not (such as Utah).

6.2

Simulation Results: Point Estimation

In this section we assess the properties of the proposed SDID estimator relative to the DID
and SC estimators in finite samples using a simulation study. As in the above placebo study,
we consider a setting with no treatment effect, and run all methods as though a single unit in
cell (N, T ) had been treated; then, we measure the accuracy of LÃÇN T as an estimate for LN T .
In all our examples, the data is drawn as Yit ‚àº N (Lit , œÉ 2 ), independently for each (i, t) pair.
Meanwhile, the N √óT signal matrix L is low rank, L = U V > , where U ‚àà RN √óR and V ‚àà RT √óR
for a rank parameter R.
The key choice is in how we generate this low-rank matrix L. First, we consider a simulation
where the N -th row and the N -th column of L are ‚Äútypical‚Äù; formally, we generate L via an
exchangeable process, such that Uil ‚àº Exp(1) and Vtl ‚àº Exp(1) independently for each (i, l)
and (t, l). Second, we consider a case where the focal row and column are not ‚Äútypical‚Äù, and
p
in particular the rows and columns are not exchangeable. Here, we draw Uil ‚àº Pois( i/N ) for
p
each (i, l), and Vtl ‚àº Pois( t/T ) for each (t, l). Note that the N -th row and T -th column will
on average have relatively large observations.
In all our simulations, we use consider penalized SC weights as in (6.1), with Œ∂ set to the
sample variance of the Yit . Below, we first generate a random L, and then simulate Y 20 times
25

130
120
110
90

100

smoking [packs per capita]

120
115
110
105

smoking [packs per capita]

100
95
1980

1982

1984

1986

1988

1980

1982

year

1984

1986

1988

1986

1988

year

California

90
80

160

60

70

smoking [packs per capita]

240
220
200
180

smoking [packs per capita]

260

100

280

Alabama

1980

1982

1984

1986

1988

1980

year

1982

1984
year

New Hampshire

Utah

Figure 2: Predictions for per capita smoking rates for selected states, using as training data
all years prior to the year indicated on the x-axis. The true yearly per-capita smoking Yi,t is in
black. SDID estimates are in red. SC estimates are in blue. DID estimates are in teal.

26

given this L. This lets us separate the contributions of bias and variance to the error. We
report for the two designs, for different values of œÉ 2 and the rank R, and for different pairs
of (N, T ), the root-mean-squared-error and mean-absolute-bias for the three estimators, DID,
SC, and SDID. We report results in Tables 1 and 2. In the Appendix, we also show results for
unpenalized SC (Œ∂ = 0), in Tables 4 and 5. We find that in all cases the SDID estimator has
substantially better bias properties than the DID and SC estimators, and in most cases also has
better root-mean-squared-error.

6.3

Simulation Results: Confidence Intervals

Finally, we study the properties of confidence intervals derived via the weighted regression perspective of SDID. We work in the same data-generating distribution as for the ‚Äúnon-exchangeable‚Äù
example in Section 6.2, except now with multiple treated units. Units 1 : . . . , N0 are control
units, and units N0 + 1, . . . , N0 + N1 = N are treated from period T0 + 1 onwards. Writing Wit
for the treatment indicator, we draw data as

Yit = Lit + Wit œÑ + Œµit , Œµit ‚àº N 0, œÉ 2 .
We use weights œâÃÇi = 1/N1 for i = N0 + 1, .., N , and

sc
œâÃÇ1:N
0

Ô£±
T0
Ô£≤1 X
= arg min
Ô£≥ T0
t=1

1
N1

N
X
j=N0 +1

Yjt ‚àí

N0
X

!2
œâi Yit

Œ∂
kœâk22 : œâi ‚â• 0,
N1

+

i=1

N0
X
i=1

Ô£º
Ô£Ω
wi = 1 , (6.2)
Ô£æ

and pick ŒªÃÇ analogously. As above, we set Œ∂ to the sample variance of the Yit The, given these
weights, we estimate

œÑÃÇ = arg min

(
X

)
(Yit ‚àí ¬µ ‚àí Œ±i ‚àí Œ≤t ‚àí Wit œÑ )2 œâÃÇi ŒªÃÇt

.

(6.3)

i, t

We perform inference via heteroskedasticity-consistent standard error as provided in the R package sandwich [Zeileis, 2004]. We estimate variance via the jackknife [Miller, 1974], which corresponds to HC3 standard errors of MacKinnon and White [1985]. We run the weighted regression
as though œâÃÇi and ŒªÃÇt were deterministic and did not depend on the data.
27

N
50
50
50
50
50
50
50
50
200
200
200
200

T
50
50
50
50
200
200
200
200
200
200
200
200

root-mean sq. error mean absolute bias
œÉ rank DID SC SDID DID SC SDID
0.5
2
1.56 0.47 0.24
0.86 0.21 0.07
0.5
5
1.96 1.10 0.57
1.40 0.70 0.33
2
2
1.45 1.04 0.89
0.87 0.44 0.24
2
5
2.20 1.54 1.15
1.52 0.92 0.52
0.5
2
1.22 0.39 0.17
0.79 0.11 0.04
0.5
5
2.09 0.65 0.44
1.46 0.41 0.22
2
2
1.22 0.64 0.68
0.75 0.26 0.16
2
5
2.40 1.17 1.04
1.52 0.66 0.42
0.5
2
1.38 0.29 0.11
0.87 0.11 0.02
0.5
5
2.19 0.77 0.30
1.56 0.44 0.13
2
2
1.27 0.51 0.53
0.81 0.21 0.11
2
5
2.38 1.12 0.72
1.64 0.58 0.24

Table 1: Simulation study with an exchangeable distribution for L and penalized SC weights.
Results are aggregated over 400 draws of the low-rank L matrix and 25 draws of Y for each L
(for a total of 10,000 simulation replications).

N
50
50
50
50
50
50
50
50
200
200
200
200

T
50
50
50
50
200
200
200
200
200
200
200
200

root-mean sq. error mean absolute bias
œÉ rank DID SC SDID DID SC SDID
0.5
2
1.70 0.58 0.31
1.08 0.29 0.09
0.5
3
1.99 0.95 0.46
1.35 0.51 0.20
2
2
1.55 1.07 1.00
1.03 0.58 0.33
2
3
1.76 1.30 1.14
1.22 0.75 0.44
0.5
2
1.40 0.30 0.19
1.01 0.14 0.05
0.5
3
2.08 0.65 0.37
1.37 0.29 0.12
2
2
1.49 0.83 0.83
0.98 0.40 0.25
2
3
2.02 1.07 0.96
1.42 0.62 0.37
0.5
2
1.86 0.67 0.18
1.14 0.17 0.03
0.5
3
2.07 0.53 0.21
1.35 0.24 0.06
2
2
1.63 0.70 0.64
1.09 0.35 0.16
2
3
1.89 0.88 0.68
1.31 0.49 0.21

Table 2: Simulation study with an non-exchangeable distribution for L and penalized SC
weights. Results are aggregated over 400 draws of the low-rank L matrix and 25 draws of Y
for each L (for a total of 10,000 simulation replications).

28

3

‚óè

6

‚óè

‚óè

‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè

2
1
0

Sample Quantiles

‚óè

‚àí4

‚àí1
‚àí2
‚óè

‚àí3

2
0

Sample Quantiles

4

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè

‚àí2
‚àí4

‚óè

‚óè

‚óè
‚óè‚óè

‚àí2

0

2

4

‚óè

‚àí4

Theoretical Quantiles

standardized DID errors

‚àí2

0

2

4

Theoretical Quantiles

standardized SDID errors

d [œÑÃÇ ]1/2 , for the
Figure 3: Standard Gaussian QQ-plot of the standardized errors (œÑÃÇ ‚àí œÑ )/Var
independent design for both DID and SDID, aggregated across 10,000 simulation replications.
Points along the diagonal (dashed) would indicate perfectly calibrated Gaussian standard errors.
Points along a centered line with a slope shallower than 45 degrees indicate that confidence
intervals are conservative.
We generated data as in the non-exchangeable case above, with N = 100, N1 = 20, T =
120, T1 = 5, œÉ = 2, œÑ = 1, and rank set to 2. It appears that SDID confidence intervals
were well calibrated albeit slightly conservative: Nominal 95% confidence intervals achieved
98% coverage. The slight conservativeness may be due to the well-known mild upward bias of
jackknife variance estimates [Efron and Stein, 1981]. In contrast, a basic difference-in-differences
regression (6.3) but without weights œâÃÇ and ŒªÃÇ did poorly: Nominal 95% confidence intervals
achieved 82% coverage. Figure 3 shows a Gaussian QQ-plot of the standardized errors of both
DID and SDID, mirroring the observation that SDID confidence intervals are well calibrated
where DID ones are not.
To address a well-known critique of Bertrand et al. [2004] we also consider a design with
dependent errors. The data is generated in the same way as above, but now the errors are
correlated:
E[Œµit Œµil ] = œÅ|t‚àíl|
We set œÅ = 0.7 leaving all other parameters the same. To deal with the correlation in errors, we
29

‚óè

4
‚àí2

0

2

‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè

‚àí4

‚óè

‚àí4

‚óè

‚óè

Sample Quantiles

2
0
‚àí2
‚àí4

Sample Quantiles

4

‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè

‚óè

‚óè‚óè

‚óè

‚àí2

0

2

4

‚àí4

Theoretical Quantiles

standardized DID errors

‚àí2

0

2

4

Theoretical Quantiles

standardized SDID errors

d [œÑÃÇ ]1/2 , for the
Figure 4: Standard Gaussian QQ-plot of the standardized errors (œÑÃÇ ‚àí œÑ )/Var
correlated design for both DID and SDID, aggregated across 10,000 simulation replications.
Points along the diagonal (dashed) would indicate perfectly calibrated Gaussian standard errors.
estimate the variance using row-based jackknife. We run the weighted regression as though œâÃÇi
and ŒªÃÇt were deterministic and did not depend on the data.
Nominal 95% confidence intervals based on SDID now achieve 93% coverage, while those
based on simple DID estimator achieve 88%. Figure 4 shows a Gaussian QQ-plot of the standardized errors of both DID and SDID, indicating that again SDID is better calibrated than
DID.

7

Conclusion

We present a new estimator in a Synthetic Control setting, he synthetic difference in differences (SDID) estimator, which can be interpreted as a doubly weighted DID estimator. We find
that the new estimator has attractive double robustness properties compared to the SC and
DID estimators, both in simulations, in an application, and based on formal large N and large
T symptotic results. By putting the new estimator as well as the original SC estimator in a
weighted regression framework it allows us to connect the SC methodology to regression methods, which suggests alternative ways for accomodating time-invariant as well as time-varying
covariates, as well as generalizations from two-way fixed effect models to factor models.
30

References
Alberto Abadie and Javier Gardeazabal. The economic costs of conflict: A case study of the
basque country. American Economic Review, 93(-):113‚Äì132, 2003.
Alberto Abadie and Guido W Imbens. Large sample properties of matching estimators for
average treatment effects. Econometrica, 74(1):235‚Äì267, 2006.
Alberto Abadie and JeÃÅreÃÅmy L‚ÄôHour. A penalized synthetic control estimator for disaggregated
data, 2016.
Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for comparative case studies: Estimating the effect of California‚Äôs tobacco control program. Journal
of the American Statistical Association, 105(490):493‚Äì505, 2010.
Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Comparative politics and the synthetic
control method. American Journal of Political Science, pages 495‚Äì510, 2015.
Manuel Arellano. Panel data econometrics. Oxford university press, 2003.
Susan Athey and Guido W Imbens. Design-based analysis in difference-in-differences settings
with staggered adoption. Technical report, National Bureau of Economic Research, 2018.
Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi.
Matrix completion methods for causal panel data models. arXiv preprint arXiv:1710.10251,
2017.
Susan Athey, Guido W Imbens, and Stefan Wager. Approximate residual balancing: debiased
inference of average treatment effects in high dimensions. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 80(4):597‚Äì623, 2018.
Jushan Bai. Panel data models with interactive fixed effects. Econometrica, 77(4):1229‚Äì1279,
2009.
Jushan Bai and Serena Ng. Determining the number of factors in approximate factor models.
Econometrica, 70(1):191‚Äì221, 2002.

31

Alexandre Belloni, Victor Chernozhukov, and Christian Hansen. High-dimensional methods and
inference on structural and treatment effects. The Journal of Economic Perspectives, 28(2):
29‚Äì50, 2014.
Eli Ben-Michael, Avi Feller, and Jesse Rothstein. New perspectives on the synthetic control
method. Technical report, UC Berkeley, 2018.
Marianne Bertrand, Esther Duflo, and Sendhil Mullainathan. How much should we trust
differences-in-differences estimates?

The Quarterly journal of economics, 119(1):249‚Äì275,

2004.
SteÃÅphane Bonhomme and Elena Manresa. Grouped patterns of heterogeneity in panel data.
Econometrica, 83(3):1147‚Äì1184, 2015.
SteÃÅphane Bonhomme, Thibaut Lamadon, and Elena Manresa. Discretizing unobserved heterogeneity. Technical report, IFS Working Papers, 2017.
David Card. The impact of the mariel boatlift on the miami labor market. Industrial and Labor
Relation, 43(2):245‚Äì257, 1990.
Carlos Carvalho, Ricardo Masini, and Marcelo C Medeiros. Arco: an artificial counterfactual
approach for high-dimensional panel time-series data. Journal of Econometrics, 207(2):352‚Äì
380, 2018.
Victor Chernozhukov, Kaspar Wuthrich, and Yinchu Zhu. An exact and robust conformal
inference method for counterfactual and synthetic controls. arXiv preprint arXiv:1712.09089,
2017.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural
parameters. The Econometrics Journal, 21(1):C1‚ÄìC68, 2018a.
Victor Chernozhukov, Juan Carlos Escanciano, Hidehiko Ichimura, Whitney K Newey, and
M Robins. Locally robust semiparametric estimation. arXiv preprint arXiv:1608.00033, 2018b.
Victor Chernozhukov, Kaspar Wuthrich, and Yinchu Zhu. Inference on average treatment effects
in aggregate panel data settings. arXiv preprint arXiv:1812.10820, 2018c.
32

Nikolay Doudchenko and Guido W Imbens. Balancing, regression, difference-in-differences and
synthetic control methods: A synthesis. Technical report, National Bureau of Economic
Research, 2016.
Bradley Efron. The jackknife, the bootstrap, and other resampling plans, volume 38. Siam,
1982.
Bradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statistics,
pages 586‚Äì596, 1981.
Sergio Firpo and Vitor Possebom. Synthetic control method: Inference, sensitivity analysis and
confidence sets. Journal of Causal Inference, 6(2), 2018.
Bryan S Graham, Cristine Campos de Xavier Pinto, and Daniel Egel. Inverse probability tilting
for moment condition models with missing data. The Review of Economic Studies, 79(3):
1053‚Äì1079, 2012.
Jinyong Hahn and Ruoyao Shi. Synthetic control and inference. Available at UCLA, 2016.
Jens Hainmueller. Entropy balancing for causal effects: A multivariate reweighting method to
produce balanced samples in observational studies. Political Analysis, 20(1):25‚Äì46, 2012.
Christian B Hansen. Asymptotic properties of a robust variance matrix estimator for panel data
when t is large. Journal of Econometrics, 141(2):597‚Äì620, 2007.
David A Hirshberg and Stefan Wager. Augmented minimax linear estimation. arXiv preprint
arXiv:1712.00038, 2018.
Kosuke Imai and Marc Ratkovic. Covariate balancing propensity score. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 76(1):243‚Äì263, 2014.
Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical
Sciences. Cambridge University Press, 2015.
Hans R Kunsch. The jackknife and the bootstrap for general stationary observations. The
Annals of Statistics, pages 1217‚Äì1241, 1989.

33

Guillaume LecueÃÅ and Shahar Mendelson. Learning subgaussian classes: Upper and minimax
bounds. arXiv preprint arXiv:1305.4825, 2013.
Kathleen T Li and David R Bell. Estimation of average treatment effects with panel data:
Asymptotic theory and implementation. Journal of Econometrics, 197(1):65‚Äì75, 2017.
Kung-Yee Liang and Scott L Zeger. Longitudinal data analysis using generalized linear models.
Biometrika, 73(1):13‚Äì22, 1986.
Christopher Liaw, Abbas Mehrabian, Yaniv Plan, and Roman Vershynin. A simple tool for
bounding the deviation of random matrices on geometric sets. In Geometric aspects of
functional analysis, pages 277‚Äì299. Springer, 2017.
James G MacKinnon and Halbert White. Some heteroskedasticity-consistent covariance matrix
estimators with improved finite sample properties. Journal of econometrics, 29(3):305‚Äì325,
1985.
Shahar Mendelson. Learning without concentration. In Conference on Learning Theory, pages
25‚Äì39, 2014.
Rupert G Miller. The jackknife-a review. Biometrika, 61(1):1‚Äì15, 1974.
Whitney K Newey, Fushing Hsieh, and James M Robins. Twicing kernels and a small bias
property of semiparametric estimators. Econometrica, 72(3):947‚Äì962, 2004.
Juan Peypouquet. Convex optimization in normed spaces: theory, methods and examples.
Springer, 2015.
James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients
when some regressors are not always observed. Journal of the American statistical Association,
89(427):846‚Äì866, 1994.
Daniel O Scharfstein, Andrea Rotnitzky, and James M Robins. Adjusting for nonignorable
drop-out using semiparametric nonresponse models. Journal of the American Statistical
Association, 94(448):1096‚Äì1120, 1999.
James H Stock and Mark W Watson. Heteroskedasticity-robust standard errors for fixed effects
panel data regression. Econometrica, 76(1):155‚Äì174, 2008.
34

William F Trench. Asymptotic distribution of the spectra of a class of generalized kac‚Äìmurdock‚Äì
szegoÃà matrices. Linear algebra and its applications, 294(1-3):181‚Äì192, 1999.
Roman Vershynin. High-dimensional probability: An introduction with applications in data
science, volume 47. Cambridge University Press, 2018.
Yiqing Xu. Generalized synthetic control method: Causal inference with interactive fixed effects
models. Political Analysis, 25(1):57‚Äì76, 2017.
Achim Zeileis. Econometric computing with hc and hac covariance matrix estimators. Journal
of Statistical Software, Articles, 11(10):1‚Äì17, 2004. doi: 10.18637/jss.v011.i10. URL https:
//www.jstatsoft.org/v011/i10.
JoseÃÅ R Zubizarreta. Stable weights that balance covariates for estimation with incomplete
outcome data. Journal of the American Statistical Association, 110(511):910‚Äì922, 2015.

35

8

Appendix

Notation. Throughout the proofs section, we omit the ‚Äú:‚Äù subscript from œâ and Œª when there
is no risk of ambiguity. In addition, we write œâ? and Œª? with the same meaning as œâ ? and Œª?
where conventient. We will use c to denote a universal constant, which may differ in value in
each instance. Many of our bounds are phrased in terms of the gaussian width of a set S ‚äÜ Rn ,
w(S) = E sups‚ààS hg, si where g ‚àà Rn is a vector of iid standard gaussians, as well as the radius
rad(S) = sups‚ààS ktk and diameter diam(S) = sups,s0 ‚àà¬ß ks ‚àí s0 k. We will use results which, in
our references,
q may be phrased in terms of variants of gaussian width, Œ≥(S) = E sups‚ààS |hg, si|
and h(S) = E sups‚ààS hg, si2 , and will express them here without comment in terms of either
w(S) or w(S ‚àí S) for S ‚àí S := {s ‚àí s0 : s, s0 ‚àà S} using equivalences discussed in Vershynin
[2018, Section 7.6]. Unless otherwise specified, kvk will mean the euclidean norm kvk2 for a
vector v and kAk will mean the operator norm kAkop = supkvk2 ‚â§1 kAxk2 for a matrix A. The
L2 norm, subgaussian norm, and subexponential norms for a scalar random variable Z will
be written kZkL2 , kZkœà2 and kZkœà1 , and we extend them to random vectors Z by defining
kZkL2 = supkxk=1 k‚à†Z, xkL2 and the others analogously as in Vershynin [2018].

8.1

Proof of Theorem 2

When the fixed effects model is correctly specified, we can check that synthetic difference in
differences perfectly captures the signal for any set of weights, and the error depends only on
the noise Œµ:
bN T ‚àí LN T = œâÃÇ ¬∑ Œµ:T + ŒªÃÇ ¬∑ ŒµN : ‚àí œâÃÇ 0 Œµ:: ŒªÃÇ.
L
Next, by Assumption 3, we know that

œâÃÇ ¬∑ Œµ:T œâÃÇ ‚àº N 0, œÉ 2 kœâÃÇk22 ,
and so by the first part of (4.4) the term œâÃÇ ¬∑ Œµ:T converges in probability to 0; the same argument
also applies to ŒªÃÇ ¬∑ Œµn: . Finally, for the last term, we invoke Cauchy-Schwarz to check that
œâÃÇ 0 Œµ:: ŒªÃÇ ‚â§ kœâÃÇk2 kŒµ:: kop kŒªÃÇk2 = OP (kœâÃÇk2 kŒªÃÇk2

36

p
max{N, T }),

Alabama
Arkansas
California
Colorado
Connecticut
Delaware
Georgia
Idaho
Illinois
Indiana
Iowa
Kansas
Kentucky
Louisiana
Maine
Minnesota
Mississippi
Missouri
Montana
Nebraska
Nevada
New Hampshire
New Mexico
North Carolina
North Dakota
Ohio
Oklahoma
Pennsylvania
Rhode Island
South Carolina
South Dakota
Tennessee
Texas
Utah
Vermont
Virginia
West Virginia
Wisconsin
Wyoming

DID
12.95
16.24
8.79
7.18
6.25
3.89
12.68
7.60
2.40
6.31
4.45
6.29
9.24
5.42
4.25
6.43
8.09
5.98
6.98
2.84
27.34
42.52
1.75
30.35
6.98
9.59
8.11
8.55
6.58
8.74
3.44
17.22
7.93
4.26
6.49
2.18
4.34
5.57
12.27

SC SDID
3.41
2.46
5.03
2.81
3.37
1.81
4.66
3.81
2.79
2.40
5.26
3.04
3.61
2.42
2.55
2.24
3.07
3.08
4.36
3.46
4.77
5.12
3.92
3.59
18.52 4.62
2.71
2.48
5.01
5.62
3.56
3.72
2.31
1.88
2.14
1.82
4.31
3.41
1.31
1.40
8.10
7.90
48.37 8.72
2.38
2.65
9.96
5.10
5.37
4.15
2.58
1.33
4.88
4.27
2.47
2.32
6.90
6.73
2.69
2.24
2.28
2.41
5.94
3.15
4.21
3.58
23.59 3.89
3.85
4.05
2.51
2.39
4.13
3.42
3.36
3.30
8.15
6.87

Table 3: Root-mean squared error for one-step-ahead predictions made by difference in differences regression, SCs, and SDID. Results are averaged over the time period 1980-1988.
37

n
50
50
50
50
50
50
50
50
200
200
200
200

T
50
50
50
50
200
200
200
200
200
200
200
200

root-mean sq. error mean absolute bias
œÉ rank DID SC SDID DID SC SDID
0.5
2
1.56 0.32 0.33
0.86 0.09 0.05
0.5
5
1.96 0.82 0.47
1.40 0.38 0.14
2
2
1.45 1.15 1.20
0.87 0.39 0.25
2
5
2.20 1.51 1.42
1.52 0.73 0.43
0.5
2
1.22 0.38 0.27
0.79 0.07 0.04
0.5
5
2.09 0.50 0.42
1.46 0.19 0.09
2
2
1.22 0.84 0.98
0.75 0.25 0.18
2
5
2.40 1.22 1.24
1.52 0.54 0.35
0.5
2
1.38 0.27 0.21
0.87 0.06 0.04
0.5
5
2.19 0.58 0.30
1.56 0.19 0.05
2
2
1.27 0.63 0.79
0.81 0.18 0.14
2
5
2.38 1.10 0.96
1.64 0.45 0.21

Table 4: Simulation study with an exchangeable distribution for L and unpenalized SC
weights. Results are aggregated over 400 draws of the low-rank L matrix and 25 draws of Y for
each L (for a total of 10,000 simulation replications).

n
50
50
50
50
50
50
50
50
200
200
200
200

T
50
50
50
50
200
200
200
200
200
200
200
200

root-mean sq. error mean absolute bias
œÉ rank DID SC SDID DID SC SDID
0.5
2
1.70 0.47 0.36
1.08 0.16 0.07
0.5
3
1.99 0.80 0.46
1.35 0.32 0.12
2
2
1.55 1.15 1.26
1.03 0.51 0.32
2
3
1.76 1.36 1.38
1.22 0.65 0.41
0.5
2
1.40 0.30 0.28
1.01 0.09 0.05
0.5
3
2.08 0.57 0.40
1.37 0.17 0.08
2
2
1.49 0.97 1.06
0.98 0.37 0.25
2
3
2.02 1.16 1.17
1.42 0.56 0.34
0.5
2
1.86 0.61 0.24
1.14 0.12 0.04
0.5
3
2.07 0.42 0.27
1.35 0.13 0.05
2
2
1.63 0.75 0.84
1.09 0.30 0.17
2
3
1.89 0.89 0.88
1.31 0.41 0.20

Table 5: Simulation study with an non-exchangeable distribution for L and unpenalized
SC weights. Results are aggregated over 400 draws of the low-rank L matrix and 25 draws of
Y for each L (for a total of 10,000 simulation replications).

38



recalling that, under Assumption 1, it is known that E kŒµ:: k2op = O(max{N, T }).

8.2

Proof of Theorem 1

We start with the following high-level lemma.
Lemma 7. Suppose that Assumption 1 is satisfied, further assume that the following conditions
hold:
kœâ? k2 = o(1)
kŒª? k2 = o(1)
kŒªÃÇ ‚àí Œª? k2 = op (kŒª? k2 )

(8.1)

kœâÃÇ ‚àí œâ? k2 = op (kœâ? k2 )
p
kœâÃÇ ‚àí œâ? k1 = op (1/ log(T ))
p
kŒªÃÇ ‚àí Œª? k1 = op (1/ log(N ))
Then we have the following result:
œâÃÇ 0 Œµ:: ŒªÃÇ = op (max{kœâ? k2 , kŒª? k2 })

(8.2)

Proof. We decompose œâÃÇ T Œ£:: ŒªÃÇ into a sum of four terms Œæ1 + Œæ2 + Œæ3 + Œæ4 and bound each term:
œâÃÇ T Œµ:: ŒªÃÇ = œâ?0 Œµ:: Œª? + œâ?0 Œµ:: (ŒªÃÇ ‚àí Œª? ) + (œâÃÇ ‚àí œâ? )0 Œµ:: Œª + (œâÃÇ ‚àí œâ? )0 Œµ:: (ŒªÃÇ ‚àí Œª? ) (8.3)
Our goal is to show that these terms are negligible:
Œæk = op (max{kœâ? k2 , kŒª? k2 })

(8.4)

For the first term we get the following:
Œæ1 ‚àº N (0, kœâ? k22 kŒª? k22 ) ‚áí Œæ1 = Op (kœâ? k2 kŒª? k2 ) = op (max{kœâ? k2 , kŒª? k2 })

(8.5)

The second term Œæ2 is bounded, via HoÃàlder‚Äôs inequality, by kœâ?0 Œµ:: k‚àû kŒªÃÇ ‚àí Œª? k1 . The first factor
39

is the maximum of T ‚àí 1 independent mean-zero gaussians with variance œÉ 2 kœâk22 , which is
p
p
Op (kœâ? k2 log(T )), and the second is op (1/ log(T )) by assumption, so the product is op (kœâ? k2 ).
The third term Œæ3 is analogously op (kŒª? k2 ).
To bound the fourth term Œæ4 , we use Chevet‚Äôs inequality [Vershynin, 2018, Theorem 8.7.1],
E sup x0 Œµy ‚â§ rad(X) w(Y ) + w(X) rad(Y ).
x‚ààX,y‚ààY

In essence, this is a uniform version of the same HoÃàlder‚Äôs inequality bound, allowing us to bound
the simultaneous supremum over X and Y as if either x ‚àà X were a constant vector of length
rad(X) or y ‚àà Y were a constant vector of length rad(Y ). Here we can take X to be a set
p
of the form {x : kxk1 ‚â§ a/ log(T ), kxk2 ‚â§ bkœâ? k2 }, for a ‚Üí 0, which will contain œâÃÇ ‚àí œâ?
with high probability under our assumptions, and define Y analogously in terms of ŒªÃÇ and Œª? .
p
p
Then w(X) . a/ log(T ) ¬∑ log(T ) ‚Üí 0 and rad(X) . kœâ? k and analogously w(Y ) ‚Üí 0 and
rad(Y ) . kŒª? k, which shows that Œæ4 = op (max{kœâ? k2 , kŒª? k2 }).
We now move to prove the claimed result. Define deterministic weights:
œâi? = P

1 ({(Œ≤t ‚àí Œ≤T )2 ‚â§ cÃÉŒª })
1 ({(Œ±i ‚àí Œ±N )2 ‚â§ cÃÉœâ })
?
P
,
Œª
=
,
t
2
2
i6=N 1 ({(Œ±i ‚àí Œ±N ) ‚â§ cÃÉœâ })
t6=T 1 ({(Œ≤t ‚àí Œ≤T ) ‚â§ cÃÉŒª })

(8.6)

where cÃÉœâ = cœâ ‚àí œÉ 2 and cÃÉŒª = cŒª ‚àí œÉ 2 .
First we verify that conditions for Lemma 7 hold for œâÃÇ and œâ ? . Results for ŒªÃÇ and Œª? follow
in the same way. Define the following random variables:
K=

X

1



(Œ±i ‚àí Œ±N )2 ‚â§ cÃÉœâ



i6=N

X 
KÃÇ =
1
i6=N

1
kYi: ‚àí YN : k22 ‚â§ cœâ
T ‚àí1



(8.7)

By definition we have the following:
œâj‚àó =
œâÃÇj =

1{Œ¥j2 ‚â§ cÃÉœâ )}
K
2
1{Œ¥ÃÇj ‚â§ cœâ }

(8.8)

KÃÇ

40

where Œ¥ÃÇj2 =

1
T ‚àí1

kYi: ‚àí YN : k22 = Œ¥j2 + œÉ 2 + Œæj , where Œæj is a mean-zero random variable. Define

the following random variable:
l = kœâ ? ‚àí œâÃÇk0

(8.9)

By definition l is the sum of n ‚àí 1 i.i.d. binary terms thus:
l = Op (¬µN )
i
h
2
2
6 1{Œ¥ÃÇj ‚â§ cœâ }}
¬µN := (N ‚àí 1)E 1{1{Œ¥j ‚â§ cÃÉœâ )} =
Since

Œ¥ÃÇj2

=

Œ¥j2

2

+ œÉ + Œæj , where Œæj = Op


fŒ¥2 (cÃÉœâ )
¬µN = O (N ‚àí 1) ‚àö
T
Since fŒ¥2 (x) =

l = Op

‚àö
fŒ¥ ( x)
‚àö
,
x

N ‚àí1
‚àö
cÃÉœâ T



‚àö1
T



(8.10)

we get the following:


(8.11)

and using the fact that cÃÉœâ = o(1) we get:


(8.12)

By construction we have the following:


K = Op (N ‚àí 1)FŒ¥j2 (cÃÉœâ )

(8.13)

‚àö
)
‚àö
and since cÃÉœâ = o(1) and cÃÉœâ = aN,T log(N
we
have
K
=
O
((N
‚àí
1)
cÃÉœâ ) ‚Üí ‚àû. This implies the
p
T
following:
1
kœâ ? k2 = ‚àö = op (1)
K

(8.14)

We have the following relationship:
|K ‚àí KÃÇ| ‚â§ l


l
1
= Op ‚àö
= op (1)
K
T cÃÉœâ

(8.15)

41

that implies


KÃÇ
l
= Op 1 +
= Op (1 + op (1)) = Op (1)
K
K

(8.16)

As a result, we get that kœâÃÇk2 = Op (kœâ ? k2 ). Define the following weights (different normalization):
{Œ¥ÃÇj2 ‚â§ cœâ }
œâÃÉj =
K

(8.17)

We have bounds on the squared norms:
 
1
l
kœâÃÉ ‚àí
=
= op (kœâ ? k22 )
K K

2
 2
1
1
1
l
2
kœâÃÉ ‚àí œâÃÇk2 = KÃÇ
‚àí
‚â§
= op (kœâÃÉ ‚àí œâ ? k22 )
K KÃÇ
KÃÇ K
œâ ? k22

(8.18)

Finally, we have the following:
p
kœâÃÇ ‚àí œâ k2 kœâÃÇ ‚àí œâ ? k0 log(N ) = Op
?

As k¬∑k1 ‚â§ k¬∑k2



l
log(N )
K




= Op

log(N )
‚àö
cÃÉœâ T


= op (1)

(8.19)

p
k¬∑k0 , this implies that the conditions of Lemma 1 are satisfied. Thus, our

estimator has the following decomposition:
bN T ‚àí LN T = œâÃÇ ¬∑ Œµ:T + ŒªÃÇ ¬∑ ŒµN : ‚àí œâÃÇ 0 Œµ:: ŒªÃÇ
L
= œâ ? ¬∑ Œµ:T + Œª? ¬∑ ŒµN : + (œâÃÇ ‚àí œâ ? ) ¬∑ Œµ:T + (ŒªÃÇ ‚àí Œª? ) ¬∑ ŒµN : + op (max{kœâ ? k2 , kŒª? k2 })
= œâ ? ¬∑ Œµ:T + Œª ¬∑ ŒµN : + op (max{kœâ ? k2 , kŒª? k2 })
where the last equality uses the fact that kœâ ? ‚àí œâÃÇk2 = op (kœâ ? k2 ), kŒª? ‚àí ŒªÃÇk2 = op (kŒª? k2 ) and the
fact that œâÃÇ is independent of Œµ:T and ŒªÃÇ is independent of ŒµN : . This proves the result.

42

8.3

Generalizations of Theorem 3 and Lemma 4

In this section, we will replace Assumption 1 with the following generalization, which allows
for heteroskedastic and autocorrelated errors. In this setting, we consider the behavior of our
synthetic difference-in-difference estimator when we use least squares weights œâÃÇ, ŒªÃÇ subject to
arbitrary constraints.
Assumption 5. Y = L + Œµ is an N √ó T matrix where L is deterministic and E Œµ = 0; the rows
of Œµ are independent and subgaussian; and E Œµ0i: Œµi: = Œ£ for all i ‚â§ N ‚àí 1. Here subscripting by :
takes the rows or columns for i < N, j < T .
Theorem 8. Under Assumption 5, consider the least squares estimators
œâÃÇ = arg minkœâ 0 Y:: ‚àí YN : k22 and ŒªÃÇ = arg minkY:: Œª ‚àí Y:T k22
œâ‚àà‚Ñ¶

Œª‚ààŒõ

and the oracle estimators œâ? , Œª? defined analogously with L substituted for Y and define
Œ¥œâ = kLN : ‚àí œâ?0 L:: k;
Œ¥Œª = kL:T ‚àí L:: Œª? k;
Œ¥sdid = |LN T ‚àí (œâ?0 L:T + LN : Œª? ‚àí œâ?0 L:: Œª? )| .
Then, for rŒª defined in Lemma 9,


bN T ‚àí LN T
L



1/2
‚â§ diam(Œõ) Œ¥œâ + Op kŒ£k + w(‚Ñ¶) maxkŒµi: kœà2
i<N



1/2
+ diam(‚Ñ¶) Œ¥Œª + Op max Var[ŒµiT | Œµi: ] + w(Œõ) maxkŒµi: kœà2
i<N

i<N

+Œ¥sdid + Op (diam(‚Ñ¶)rŒª + kœâ?0 Œµ:: kœà2 w(Œõ) + [kŒµ:: Œª? kœà2 + kE[Œµ:T | Œµ:: ]kœà2 ] w(‚Ñ¶))
+ |Œµ0N : Œª? + œâ?0 Œµ:T ‚àí œâ?0 Œµ:: Œª? |
If the elements of Œµ:: are independent and identically distributed, we may substitute
min{diam(‚Ñ¶)rŒª , diam(Œõ)rœâ } for diam(‚Ñ¶)rŒª , where rœâ is defined analogously to rŒª , as the bound
established by Lemma 9 on k(œâÃÇ ‚àí œâ? )0 L:: k.

43

Lemma 9. Under Assumption 5, for any subset Œõ of RT ‚àí1 , the least squares estimator and
oracle least squares estimator
ŒªÃÇ = minkY:: Œª ‚àí Y:T k22 and Œª? = minkL:: Œª ‚àí L:T k22
Œª‚ààŒõ

Œª‚ààŒõ

satisfy the bound kL:: (ŒªÃÇ ‚àí Œª? )k2 = OP (rŒª ) where


rŒª = max kL:: Œª? ‚àí L:T k,
p
x approx-rank(L:: , x) for x = kŒµ:: Œª? kœà2 + kŒµ:T kœà2 ,
r
sup |Œ≥ÃÑ 0 Œ¥|,
Œ¥‚ààŒõ

r
‚àö ‚àö

4
diam(Œõ) max
T , N T log(T ) maxkŒµiT kœà2 kŒµij kœà2 ,
j

p
p
2
‚àí1
N kŒ£k diam(Œõ) + kŒµi: kœà2 kŒ£ k kŒ£k w(Œõ) .
Here Œ≥ÃÑ = (N ‚àí1)‚àí1

PN ‚àí1
i=1

E ŒµiT Œµi: ; w(Œõ) is the gaussian width of the set Œõ; and approx-rank(L:: , x)

is an approximation to the rank of L:: that ignores small nonzero singular values, defined

approx-rank(L:: , x) := min

Ô£±
Ô£≤
Ô£≥

Ô£∂Ô£º
sX
Ô£Ω
r ‚â• x‚àí1 min Ô£≠diam(Œõ)
s2k , sr+1 w(Œõ)Ô£∏
Ô£æ
Ô£´

r ‚àà 1, 2, . . .

k>r

in terms of the decreasing sequence of singular values s1 , s2 , . . . of L:: .

44

8.3.1

Proof of Theorem 8

Our estimator‚Äôs error is the difference between our estimator and the corresponding infeasible
estimator, plus the infeasible estimator‚Äôs error Œ¥sdid , i.e.
b N T ‚àí LN T
L
h
i
= YN : ŒªÃÇ + œâÃÇ 0 Y:T ‚àí œâÃÇ 0 Y:: ŒªÃÇ ‚àí [(YN : ‚àí ŒµN : )Œª? + œâ?0 (Y:T ‚àí Œµ:T ) ‚àí œâ?0 (Y:: ‚àí Œµ:: )Œª? ] + Œ¥sdid
h
i
0
0
0
0
= YN : (ŒªÃÇ ‚àí Œª? ) + (œâÃÇ ‚àí œâ? ) Y:T ‚àí (œâÃÇ ‚àí œâ? ) Y:: (ŒªÃÇ ‚àí Œª? ) + œâ? Y:: (ŒªÃÇ ‚àí Œª? ) + (œâÃÇ ‚àí œâ? ) Y:: Œª?
+ Œ¥sdid ‚àí Œµ0N : Œª? ‚àí œâ?0 Œµ:T + œâ?0 Œµ:: Œª?
= (YN : ‚àí œâ?0 Y:: )(ŒªÃÇ ‚àí Œª? ) + (œâÃÇ ‚àí œâ? )0 (Y:T ‚àí Y:: Œª? ) ‚àí (œâÃÇ ‚àí œâ? )0 Y:: (ŒªÃÇ ‚àí Œª? )
+ Œ¥sdid ‚àí Œµ0N : Œª? ‚àí œâ?0 Œµ:T + œâ?0 Œµ:: Œª?
= (L:N ‚àí œâ?0 L:: )(ŒªÃÇ ‚àí Œª? ) + (œâÃÇ ‚àí œâ? )0 (L:T ‚àí L:: Œª? ) + Œ¥sdid
+ (ŒµN : ‚àí œâ?0 Œµ:: )(ŒªÃÇ ‚àí Œª? ) + (œâÃÇ ‚àí œâ? )0 (Œµ:T ‚àí Œµ:: Œª? ) ‚àí (œâÃÇ ‚àí œâ? )0 Œµ:: (ŒªÃÇ ‚àí Œª? )
‚àí Œµ0N : Œª? ‚àí œâ?0 Œµ:T + œâ?0 Œµ:: Œª?
‚àí (œâÃÇ ‚àí œâ? )0 L:: (ŒªÃÇ ‚àí Œª? ).
We bound each line.
1. The first line is bounded by Œ¥œâ diam(Œõ) + Œ¥Œª diam(‚Ñ¶) + Œ¥sdid , which follows by applying
Cauchy-Schwarz to the first two terms.
2. The second line is






1/2

Op diam(Œõ) kŒ£k + w(‚Ñ¶) maxkŒµi: kœà2
i<N


1/2
+ diam(‚Ñ¶) max Var[ŒµiT | Œµi: ] + w(Œõ) maxkŒµi: kœà2
i<N

i<N

+kœâ?0 Œµ:: kœà2 w(Œõ) + [kŒµ:: Œª? kœà2 + kE[Œµ:T | Œµ:: ]kœà2 ] w(‚Ñ¶)
(a) The first term is the sum of two pieces, Œµ0N : (ŒªÃÇ ‚àí Œª? ) and ‚àíœâ?0 Œµ:: (ŒªÃÇ ‚àí Œª? ). The first
piece is Op (kŒ£k1/2 diam(Œõ)). Because the row ŒµN : is independent of the noise submatrices Œµ:: , Œµ:T that are used to define ŒªÃÇ, it has mean zero and variance bounded by
45

kŒ£k diam(Œõ)2 conditional on ŒªÃÇ. The second piece is Op (kœâ?0 Œµ:: kœà2 w(Œõ)), as the vector œâ?0 Œµ:: is subgaussian, and by Talagrand‚Äôs comparison inequality [Vershynin, 2018,
Corollary 8.6.3],
E sup |œâ?0 Œµ:: Œ¥| ‚â§ cK w(Œõ ‚àí Œª? );
Œ¥‚ààŒõ‚àíŒª?

K0 =

sup kœâ?0 Œµ:: (x ‚àí y)kœà2 /kx ‚àí yk ‚â§ kœâ?0 Œµ:: kœà2 .

x,y‚ààŒõ‚àíŒª?

(b) The second term is the sum of two pieces, (œâÃÇ ‚àí œâ? )0 Œµ:T and (œâÃÇ ‚àí œâ? )0 Œµ:: Œª? . The second
piece, by Talagrand‚Äôs comparison inequality as above, is Op (kŒµ:: Œª? kœà2 w(‚Ñ¶)). The
first piece is

Op diam(‚Ñ¶) max Var[ŒµiT | Œµi: ]

1/2

i<N


+ w(‚Ñ¶)kE[Œµ:T | Œµ:: ]kœà2

with terms bounding those in the decomposition
(œâÃÇ ‚àí œâ? )0 Œµ:T = (œâÃÇ ‚àí œâ? )0 (Œµ:T ‚àí E[Œµ:T | Œµ:: ]) + (œâÃÇ ‚àí œâ? ) E[Œµ:T | Œµ:: ].
The bound on the second of these terms follows from Talagrand‚Äôs comparison inequality as above, and the first of them has a conditional Chebyshev bound
P (|(œâÃÇ ‚àí œâ? )0 (Œµ:T ‚àí E[Œµ:T | Œµ:: ])| > t | Œµ:: , ŒµN : )
‚â§ t‚àí2

N
‚àí1
X

(œâÃÇ ‚àí œâ? )2i Var[ŒµiT | Œµ:: ] ‚â§ t‚àí2 diam(‚Ñ¶)2 max Var[ŒµiT | Œµi: ].
i<N

i=1

(c) The third term is Op (maxi<N kŒµi: kœà2 [diam(‚Ñ¶) w(Œõ) + diam(Œõ) w(‚Ñ¶)]). This follows
from Chevet‚Äôs inequality for random matrices with iid subgaussian rows,
E sup x0 Œµy ‚â§ c maxkŒµi: kœà2 [rad(X) w(Y ) + w(X) rad(Y )] .
x‚ààX,y‚ààY

i<N

The proof of Vershynin [2018, Theorem 8.7.1], which addresses the case of random
matrices with iid subgaussian elements, can be adapted for iid subgaussian rows by
applying Hoeffding‚Äôs inequality (i.e. Vershynin [2018, Proposition 2.6.1]) to row sums
46

rather than elementwise when bounding the increments of this subgaussian process.
3. The third line is included in the bound.
4. The fourth line is Op (diam(‚Ñ¶)rŒª ). This follows from the Cauchy-Schwarz bound kœâÃÇ ‚àí
œâ? kkL:: (ŒªÃÇ ‚àí Œª? )k and Lemma 9. If the elements of Œµ:: are iid, then Lemma 9 implies a
bound k(œâÃÇ ‚àí œâ? )0 L:: k ‚â§ rœâ , and we can also bound this term by k(œâÃÇ ‚àí œâ? )0 L:: kkŒªÃÇ ‚àí Œª? k, so
the fourth line will be Op (min{diam(‚Ñ¶)rŒª , diam(Œõ)rœâ }).
Collecting all of these results yields our claimed bound.
8.3.2

Proof of Lemma 9

To simplify our notation in this proof, we will write N and T for the dimensions of Y:: , which
are called N ‚àí 1 and T ‚àí 1 in the lemma statement.
Our proof is based on the well-known isomorphic bounds argument in empirical risk minimization [see e.g. LecueÃÅ and Mendelson, 2013, Mendelson, 2014].
0 ‚â• kY:: ŒªÃÇ ‚àí Y:T k22 ‚àí kY:: Œª? ‚àí Y:T k22
= kY:: ŒªÃÇk22 ‚àí kY:: Œª? k22 ‚àí 2Y:T0 Y:: (ŒªÃÇ ‚àí Œª? )
‚â• kY:: (ŒªÃÇ ‚àí Œª? )k22 + 2(Y:: Œª? ‚àí Y:T )0 Y:: (ŒªÃÇ ‚àí Œª? ) + 1{Œõ=conv(Œõ)} ¬∑ 2(L:T ‚àí L:: Œª? )0 L:: (ŒªÃÇ ‚àí Œª? )
‚â• kY:: (ŒªÃÇ ‚àí Œª? )k22 +2 (Y:: Œª? ‚àí Y:T )0 Y:: (ŒªÃÇ ‚àí Œª? ) ‚àí 1{Œõ=conv(Œõ)} (L:: Œª? ‚àí L:T )0 L:: (ŒªÃÇ ‚àí Œª? ) .
|
{z
}
|
{z
}
Q(ŒªÃÇ‚àíŒª? )
M (ŒªÃÇ‚àíŒª? )
(8.20)
Here the addition of the term 1{Œõ=conv(Œõ)} ¬∑2(L:T ‚àíL:: Œª? )0 L:: (ŒªÃÇ‚àíŒª? ) in the third line is justified by
its negativity. This is a consequence of the convexity of the set L:: Œõ when the term is nonzero:
L:T ‚àí L:: Œª? points ‚Äòoutward‚Äô from the projection of L:T onto L:: Œõ to L:T itself, whereas
L:: (ŒªÃÇ‚àíŒª? ) points ‚Äòinward‚Äô toward another element of L:: Œõ [see e.g. Peypouquet, 2015, Proposition
1.37].
Let Œõ ‚àí Œª? be the set of possible deviations from Œª? ; Œ¥ÃÇ = ŒªÃÇ ‚àí Œª? be the realized deviation; and
r2 (Œ¥) = kL:: Œ¥k2 . We will show that, with probability tending to one, (8.20) cannot be satisfied
if r(Œ¥ÃÇ) ‚â• r? . This implies our claimed bounds.
47

To do this, we will show that with probability tending to one, we have a uniform quadratic
lower bound on Q(Œ¥) and a corresponding upper bound on M (Œ¥ÃÇ) ,

inf

Q(Œ¥)
‚â• 1 ‚àí Œ∑,
r2 (Œ¥)

(8.21)

sup

|M (Œ¥)|
< (1 ‚àí Œ∑)/2.
r2 (Œ¥)

(8.22)

Œ¥‚ààŒõ‚àíŒª?
r(Œ¥)‚â•r?

Œ¥‚ààŒõ‚àíŒª?
r(Œ¥)‚â•r?

These bounds are implied by simpler bounds of the form
inf Q(Œ¥) ‚â• (1 ‚àí Œ∑)r?2 ,

(8.23)

sup |M (Œ¥)| < (1 ‚àí Œ∑)/2 ¬∑ r?2 .

(8.24)

Œ¥‚ààŒõ?

Œ¥‚ààŒõ?

where Œõ? = {Œ¥ ‚àà [0, 1](Œõ ‚àí Œª? ) : r(Œ¥) = r? }. To see this, consider Œ¥ ‚àà Œõ ‚àí Œª? with r(Œ¥) ‚â• r, and
observe that because Q is quadratic and L is linear and Œ¥r? /r(Œ¥) ‚àà Œõ? , (8.23) and (8.24) imply
that
Q(Œ¥) = (r(Œ¥)/r? )2 Q(Œ¥r? /r(Œ¥)) ‚â• (r(Œ¥)/r? )2 ¬∑ (1 ‚àí Œ∑)r?2 = (1 ‚àí Œ∑)r(Œ¥)2 ;
|M (Œ¥)| = (r(Œ¥)/r? ) |M (Œ¥r? /r(Œ¥))| ‚â§ (r(Œ¥)/r? ) ¬∑ (1 ‚àí Œ∑/2)r?2 ‚â§ (1 ‚àí Œ∑/2)r(Œ¥)2 .
We now move on to the core of our proof, which involves proving these bounds (8.23) and (8.24).
The lower bound (8.21). Much of our proof will rely on Œµ:: Œ¥ being small relative to L:: Œ¥.
For Œµ:: having independent rows with the same correlation structure, i.e. E Œµ0i: Œµi: = Œ£ with
kŒµi: kœà2 ‚â§ K, we can establish this using the matrix deviation inequality of Liaw et al. [2017],
E sup kŒµ:: Œ¥k ‚àí

p
‚àö ‚àö
N k Œ£Œ¥k ‚â§ ckŒ£‚àí1 k kŒ£kK 2 w(Œõ? ).

(8.25)

Œ¥‚ààŒõ?

For simplicity, we use Markov‚Äôs inequality rather than subgaussian concentration to derive a tail
bound from this, letting it hold on an event A.
By the triangle inequality, this controls the degree to which r? = kL:: Œ¥k can exceed kY:: Œ¥k,

48

as
kL:: Œ¥k ‚àí kY:: Œ¥k ‚â§ kL:: Œ¥ ‚àí Y:: Œ¥k = kŒµ:: Œ¥k.
In particular, every Œ¥ ‚àà Œõ? satisfying
‚àö

p
‚àö
N k Œ£Œ¥k + c(1 ‚àí P(A))‚àí1 K 2 kŒ£‚àí1 k kŒ£k w(Œõ? ) ‚â§ Œ∑/2 ¬∑ r?

, will also satisfy the bound kL:: Œ¥k ‚àí kY:: Œ¥k ‚â§ Œ∑/2 ¬∑ r? . For
r? ‚â• 2Œ∑ ‚àí1

p
p
N kŒ£k rad(Œõ? ) + c(1 ‚àí P(A))‚àí1 Œ∑ ‚àí1 K 2 kŒ£‚àí1 k kŒ£k w(Œõ? ),

(8.26)

this latter bound will be satisfied for all Œ¥ ‚àà Œõ? . And rearranging it gives (1‚àíŒ∑/2)kL:: Œ¥k ‚â§ kY:: Œ¥k,
which implies (8.23). Thus, (8.21) holds on the event A for r? above.
The upper bound (8.22).
M (Œ¥) = (Y:: Œª? ‚àí Y:T )0 Y:: Œ¥ ‚àí 1{Œõ=conv(Œõ)} (L:: Œª? ‚àí L:T )0 L:: Œ¥
= 1{Œõ6=conv(Œõ)} (L:: Œª? ‚àí L:T )0 L:: Œ¥

(8.27)

+ (L:: Œª? ‚àí L:T )0 Œµ:: Œ¥

(8.28)

+ Œª0? Œµ0:: L:: Œ¥

(8.29)

+ Œª0? Œµ0:: Œµ:: Œ¥

(8.30)

‚àí Œµ0:T L:: Œ¥.

(8.31)

‚àí Œµ0:T Œµ:: Œ¥.

(8.32)

The first term (8.27) is deterministic, and has the Cauchy-Schwarz bound
1{Œõ6=conv(Œõ)} kL:: Œª? ‚àí L:T kr? for Œ¥ ‚àà Œõ? .

49

The second term (8.28) has straightforward bound based Cauchy-Schwarz and (8.25). On A,
|(L:: Œª? ‚àí L:T )0 Œµ:: Œ¥| ‚â§ kL:: Œª? ‚àí L:T k ¬∑ (Œ∑/2)r? for Œ¥ ‚àà Œõ? .
Similarly, Cauchy-Schwarz, (8.25), and the analogous bound
E kŒµŒª? k ‚àí

‚àö

p
p
‚àö
N k Œ£Œª? k ‚â§ ckŒ£‚àí1 k kŒ£kK 2 w({0, Œª? }) ‚â§ ckŒ£‚àí1 k kŒ£kK 2 kŒª? k

(8.33)

suffice to bound (8.30). They imply that for Œ¥ ‚àà Œõ? , on the intersection of A and the analogous
event on which the bound above holds with probability P(A),
|Œª0? Œµ0:: Œµ:: Œ¥| ‚â§ kŒµ:: Œª? kkŒµ:: Œ¥k
h‚àö
i
p
‚â§
N kŒ£1/2 kkŒª? k + (1 ‚àí P(A))‚àí1 ckŒ£‚àí1 k kŒ£kK 2 kŒª? k [(Œ∑/2) ¬∑ r? ]
‚â§ (Œ∑/2)2 r?2 kŒª? k/ rad(Œõ? ).
The third term (8.29) is the supremum of the inner product of a subgaussian random vector
Œµ:: Œª? and a vector L:: Œ¥ in the intersection of the image of Œõ? under L:: and the k¬∑k2 ball of radius
r? . and via Talagrand‚Äôs comparison inequality [Vershynin, 2018, Corollary 8.6.3],
E sup |(Œµ:: Œª? )0 x| ‚â§ cK 0 w(L:: Œª? )
x‚ààL:: Œõ?

K0 =

sup k(Œµ:: Œª? )0 (x ‚àí y)kœà2 /kx ‚àí ykL2 ‚â§ kŒµ:: Œª? kœà2 .
x,y‚ààL:: Œõ?

To bound the gaussian width w(L:: Œõ? ), we split L:: into two pieces, a low rank approximation
P
LÃÉR defined as the sum of the first R terms in the singular value decomposition L:: = k œÉk uk vk0 ,
and the remainder. LÃÉR Œ¥ is contained in a R-dimensional ball of radius r? , so in terms of a
standard gaussian vector g
‚àö
w(L:: Œª? ) = E sup g 0 L:: Œ¥ ‚â§ E sup g 0 LÃÉR Œ¥ + E sup g 0 (L:: ‚àí LÃÉR )Œ¥ ‚â§ c Rr? + w((L:: ‚àí LÃÉR )Œõ? ).
Œ¥‚ààŒõ?

Œ¥‚ààŒõ?

Œ¥‚ààŒõ?

The fifth term (8.31) is analogous, with the difference that we substitute Œµ:T for Œµ:: Œª? , so in
the corresponding bound we have K 0 = kŒµ:T kœà2 .
50

To bound the sixth term, kŒµ0:T Œµ:: Œ¥k, we begin by characterizing the vector Œµ0:T Œµ:: . Because the
P
rows of Œµ are independent, Œµ0:T Œµ:j = i ŒµiT Œµij is a sum of independent subexponential random
variables with kŒµiT Œµij kœà1 ‚â§ kŒµiT kœà2 kŒµij kœà2 [Vershynin, 2018, Lemma 2.7.7]. In terms of the
P
averaged autocorrelation Œ≥ÃÑj = N ‚àí1 i EŒµiT Œµij , Œµ0:T Œµ:: = N Œ≥ÃÑ +Z where Zj = Œµ0:T Œµ:j ‚àíN Œ≥ÃÑj is a sum
of independent subexponential random variables with mean zero and kZi kœà1 ‚â§ ckŒµiT kœà2 kŒµij kœà2 .
By the Triangle inequality and Cauchy-Schwartz,
|Œµ0:T Œµ:: Œ¥| ‚â§ N |Œ≥ÃÑ 0 Œ¥| + |Z 0 Œ¥| ‚â§ N |Œ≥ÃÑ 0 Œ¥| + 2 rad(Œõ? )kZk,
and by Bernstein‚Äôs inequality [Vershynin, 2018, Theorem 2.8.1],

P(|Zj | ‚â• tj ) ‚â§ 2 exp ‚àíc min

t2j
tj
P
,
2
2
i kŒµiT kœà2 kŒµij kœà2 maxi kŒµiT kœà2 kŒµij kœà2

so we have a bound of the form 2 rad(Œõ? )kZk < Œær?2 by the union bound for

!!
,

2
j tj

P

‚â§ Œæ 2 r?4 /(4 rad(Œõ? )2 ).

Simply taking t2j = Œæ 2 r?4 /(4 rad(Œõ? )2 T ) yields
P(2 rad(Œõ? )kZk ‚â• Œær?2 )
‚â§2

X

exp ‚àíc min

j

‚â§ 2T exp ‚àíc min

Œæ 2 r?4
Œær?2
P
‚àö
,
rad(Œõ? )2 T i kŒµiT k2œà2 kŒµij k2œà2 rad(Œõ? ) T maxi kŒµiT kœà2 kŒµij kœà2

!!

Œæ 2 r?4
Œær?2
‚àö
,
rad(Œõ? )2 T N maxij kŒµiT k2œà2 kŒµij k2œà2 rad(Œõ? ) T maxij kŒµiT kœà2 kŒµij kœà2

!!

Putting everything together, our claims on M (Œ¥), as well as our claims on Q(Œ¥) from the

51

.

previous section, hold with high probability when Œ∑ ‚â§ min{1, 4 rad(Œõ? )/kŒª? k} and r? satisfies

r? ‚â• c max [1{Œõ6=conv(Œõ)} + Œ∑](1 ‚àí Œ∑)‚àí1 kL:: Œª? ‚àí L:T k,
r
h‚àö
i
‚àí1
(1 ‚àí Œ∑) [kŒµ:: Œª? kœà2 + kŒµ:T kœà2 ] Rr? + w((L:: ‚àí LÃÉR )Œõ) ,
r
(1 ‚àí Œ∑)‚àí1 N sup |Œ≥ÃÑ 0 Œ¥|,
Œ¥‚ààŒõ?

r
‚àö ‚àö

4
(1 ‚àí Œ∑)‚àí1 rad(Œõ? ) max
T , N T maxkŒµiT kœà2 kŒµij kœà2 log(T ),
ij

p
‚àö
‚àí1
‚àí1
2
Œ∑
N kŒ£k rad(Œõ? ) + maxkŒµi: kœà2 kŒ£ k kŒ£k w(Œõ? ) .
i

Here the constraint on Œ∑ comes from (8.30), the first line in r? comes from (8.27) and (8.28),
the second from (8.29) and (8.31), the third and fourth from (8.32), and the fifth incorporates
(8.26) from the lower bound section.
Simplifications This is a fixed point condition, as r? appears in the second line of the right
side explicitly in the third line and implicitly throughout, as Œõ? is a function of r? . To eliminate
the dependence of the right side on r? through Œõ? , we simply substitute for w(Œõ? ) and rad(Œõ? ) the
upper bounds w(Œõ) and diam(Œõ). This also allows us to drop the constraint Œ∑ ‚â§ 4 rad(Œõ? )/kŒª? k,
as because Œª? ‚àà Œõ, it becomes vacuous if we perform this substitution.4
This leaves us with an expression
q ‚àöon the right that depends on r? only through the second
line, which will have the form c x[ Rr? + w(R)] for x = (1 ‚àí Œ∑)‚àí1 [kŒµ:: Œª? kœà2 + kŒµ:T kœà2 ] and
w(R) = w((L:: ‚àí LÃÉR )Œõ). To obtain the claimed result, we simplify this fixed point condition to
p
‚àö
a bound, observing that r?2 ‚â• cx[ Rr? + w(R)] if r? ‚â• 2cx max(R, x‚àí1 w(R)). In addition, we
bound w(R) = w((L:: ‚àí LÃÉR )Œõ) by the minimum of kL:: ‚àí LÃÉR k w(Œõ) and the gaussian width of the
pP
2
ellipse with axes diam(Œõ)œÉR+1 , diam(Œõ)œÉR+2 , . . ., which is proportional to diam(Œõ)
k>R œÉk
[see e.g Vershynin, 2018, Section 7.6.1], and we minimize this bound over R.
4

To justify this, we can check that this constraint would not have arisen from our bound on (8.30) had we
made this substitution earlier.

52

8.4

Specializing Theorem 8 and Lemma 9

In this section, we will prove more concrete versions of the results of the previous section. This
includes Theorem 3 and Lemma 4 from Section 4, which make the assumption that Œµit ‚àº N (0, œÉ 2 )
is independent for each cell (i, t), and allowing us to dramatically simplify our bounds. We also
prove Theorem 5 from Section 4, a variant of Theorem 3 which characterizes the synthetic
control estimator under the same assumptions.
Proof of Lemma 4. In the bound of Lemma 9, we simplify several expressions under Assumpp
tion 1: kŒµ:: Œª? kœà2 = kŒµi: Œª? kœà2 = œÉkŒª? k, kŒµ:T kœà2 = kŒµiT kœà2 = œÉ, kŒµiT kœà2 kŒµij kœà2 = œÉ 2 , kŒ£k = œÉ,
p
kŒµi: k2œà2 kŒ£‚àí1 k kŒ£k = œÉ 2 œÉ ‚àí2 œÉ = œÉ, and Œ≥ÃÑ = 0. With these simplifications, as well as the bounds
p
p
‚àö
‚àö
2aœâ ‚â• diam(‚Ñ¶), 2aŒª ‚â• diam(Œõ), and min{ log(N ), aœâ N } & w(‚Ñ¶), min{ log(T ), aŒª T } &
w(Œõ), the bound of Lemma 9 reduces to that of Lemma 4.
Proof of Theorem 3. In the bound of Theorem 8 for iid noise, we substitute E[Œµ:T | Œµ:: ] =
0; œâÃÉ = œâÃÇ; kŒ£k1/2 = maxi<N kVar[ŒµiT | Œµi: k1/2 = œÉ; kŒµ:: Œª? kœà2 = œÉkŒª? k and kœâ?0 Œµ:: kœà2 =
p
‚àö
œÉkœâ? k; aœâ & diam(‚Ñ¶) + kœâ? k and aŒª & diam(Œõ) + kŒª? k; min{ log(N ), aœâ N } & w(‚Ñ¶) and
p
‚àö
min{ log(T ), aŒª T } & w(Œõ); and Œµ0N : Œª? + œâ?0 Œµ:T ‚àí œâ?0 Œµ:: Œª? = Op (œÉkŒª? k + œÉkœâ? k). We do
not have an Op (œÉkœâ? kkŒª? k) term in our bound corresponding to the third term œâ?0 Œµ:: Œª? in this
last expression because the bounds for the other terms suffice: kŒª? k ‚â§ 1 for Œª? ‚àà Œõ ‚äÜ L, so
œÉkœâ? kkŒª? k ‚â§ œÉkœâ? k.
Proof of Theorem 5. We can express our estimator as
œâÃÇ ¬∑ Y:T ‚àí LN T = (œâ?0 L:T ‚àí LN T ) + (œâÃÇ ‚àí œâ? )0 L:T + œâÃÇ 0 Œµ:T .
The first term above is simply Œ¥sc , while the last is Op (aœâ ), as it is gaussian with standard
deviation kœâÃÇk ‚â§ aœâ conditional on Œµ:: , ŒµN : . It remains to bound the middle term. To do so, note
that for any ŒªÃÉ,


(œâÃÇ ‚àí œâ? )0 L:T = (œâÃÇ ‚àí œâ? )0 L:: ŒªÃÉ + (œâÃÇ ‚àí œâ? )0 L:T ‚àí L:: ŒªÃÉ
‚â§ kL0:: (œâÃÇ ‚àí œâ? )k2 ŒªÃÉ + kœâÃÇ ‚àí œâ? k2 L:T ‚àí L:: ŒªÃÉ
2


= Op rœâ ŒªÃÉ + aœâ L:T ‚àí L:: ŒªÃÉ ,
2

53

where the last line follows from Lemma 4.

8.5

Proof of Theorem 6

We will now consider the problem of inference in a simple setting with N1 treated units and T1
treated time periods, in which all units with i > N0 start treatment at the same time T0 + 1.
Our estimator (4.1) is, in this setting, of essentially the same form as those we‚Äôve discussed
for a single treated unit and time period. Having observed such an N0 + N1 √ó T0 + T1 matrix
YÃÉ = LÃÉ + ŒµÃÉ, we define a N0 + 1 √ó T0 + 1 variant Y = L + Œµ in which treated units and rows are
averaged,
Ô£´
Y =Ô£≠

T1‚àí1

Ô£∂

P

YÃÉ1:N0 ,1:T0
t>T0 YÃÉ1:N0 ,t
Ô£∏.
P
P
N1‚àí1 i>N0 YÃÉi,1:T0 (N1 T1 )‚àí1 n>N0 ,t>T0 YÃÉit

(8.34)

In these terms, letting N = N0 + 1 and T = T0 + 1, œÑÃÇ = YN T ‚àí LÃÇN T . Thus, the problem
of estimating the averaged unobserved control potential outcome in this setting is essentially
the same as the problem we‚Äôve considered in the previous section, in which we had only one
unobserved potential outcome. It differs only in that this averaging results in heteroskedasticity
in the errors Œµ, making the elements of ŒµN : and Œµ:T small relative to those of Œµ:: . Furthermore,
because averaging drives ŒµN T to zero, it is possible for our estimator œÑÃÇ to be consistent in
this setting, whereas in the previous section the constant order variance of YN T made this
impossible. Our core result is that, under Assumption 4 and some restrictions on N1 and T1 , œÑÃÇ
is approximately normal with bias negligible relative to variance Var(œÑÃÇ ) ‚âà Var(ŒµN T ).
Our first step is to establish a formal equivalence between estimation of œÑÃÇ under Assumption 4
on YÃÉ and under a specialization of Assumption 5 on Y . This is straightforward.
Proposition 10. If YÃÉ satisfies Assumption 4, then Y defined in (8.34) satisfies Assumption 6
with œÉ 2 = œÉŒæ2 /(1 ‚àí œÅ2 ) and the estimator œÑÃÇ (4.1) based on YÃÉ is the same as the one based on Y .
Assumption 6. We have N = N0 + 1, T = T0 + 1 ‚Üí ‚àû, and there is a N √ó T deterministic
matrix L such that Yit = Lit + Wit œÑ + Œµit with Wit = 1 {i = N, t = T }, and the rows of Œµ are
independent gaussian vectors with, for i < N ,
1. Cov (Œµij , Œµik ) = œÉ 2 œÅ`‚àík for j ‚â§ k < T ,
54

2. Cov (Œµij , ŒµiT ) = T1‚àí1 œÉ 2
3. Var (ŒµiT ) = T1‚àí2

PT1

k=1

PT1 PT1
k=1

`=1

œÅT0 +k‚àíj = T1‚àí1 œÉ 2 œÅT0 +1‚àíj (1 ‚àí œÅT1 )/(1 ‚àí œÅ) for j < T ,

œÉ 2 œÅ|k‚àí`| ,

for œÅ ‚â• 0 and, for i = N , the corresponding terms are N1‚àí1 times these quantities.
Thus, as our estimator‚Äôs error is
œÑÃÇ ‚àí œÑ = (YN T ‚àí LN T ) + (LN T ‚àí LÃÇN T ) = ŒµN T + Op (LN T ‚àí LÃÇN T ),
a specialized version of Theorem 8 under Assumption 6 is sufficient to establish conditions under
which the second term is negligible relative to the first, i.e. conditions under the latter term is
op ((N1 T1 )‚àí1/2 ). Our general result, of which Theorem 6 is a corollary, follows.
Lemma 11. Under Assumption 6, consider the least squares estimators
œâÃÇ = arg minkœâ 0 Y:: ‚àí YN : k22 and ŒªÃÇ = arg minkY:: Œª ‚àí Y:T k22
œâ‚àà‚Ñ¶

Œª‚ààŒõ

and the oracle estimators œâ? , Œª? defined analogously with L substituted for Y and define
Œ¥œâ = kLN : ‚àí œâ?0 L:: k;
Œ¥Œª = kL:T ‚àí L:: Œª? k;
Œ¥sdid = |LN T ‚àí (œâ?0 L:T + LN : Œª? ‚àí œâ?0 L:: Œª? )| .
bN T ‚àí LN T =
If œÉ = O(1) and lim supN,T ‚Üí‚àû œÅ < 1, and we take diam(‚Ñ¶) = O(1), then L

55

op ((N1 T1 )‚àí1/2 ) if

N1 T1  min

1
2
Œ¥sdid

,

1
,
max {Œ¥Œª2 , w2 (Œõ)}
1
,
2
diam(Œõ) max {Œ¥œâ2 , w2 (‚Ñ¶)}

1
;
diam(‚Ñ¶)2 diam(Œõ)2 N0
1
1/2
 ‚àö
N1 T1 
;
2
diam(‚Ñ¶) diam(Œõ) max T0 , N0 T0 log(T0 )


1
T1
o .

n
,
N1  min
w(‚Ñ¶)2 diam(‚Ñ¶)2 approx-rank L , max kŒª k, T ‚àí1/2
diam(‚Ñ¶)2

::

?

1

This follows from Corollary 13 and Corollary 14 below by a straightforward calculation.
These corollaries are specializations of Lemma 9 and Theorem 8 respectively, which follow from
our more general results with a few calculations collect in Lemma 12 below.
Lemma 12. Under Assumption 6 for œÅ > 0, letting Œ£ be the covariance matrix E Œµi: Œµ0i: and Œ≥
be the vector of autocovariances E Œµ0:T Œµ:: ,
1. kŒ£k ‚â§ œÉ 2 1+œÅ
1‚àíœÅ
2. kŒ£‚àí1 k ‚â§ œÉ ‚àí2 1+œÅ
1‚àíœÅ
3. kŒ≥k ‚â§ T1‚àí1 œÉ 2 œÅ2 (1 ‚àí œÅ)‚àí1 (1 ‚àí œÅ2 )‚àí1/2
4. kŒµi: kœà2 = kŒ£k1/2 ‚â§

‚àö
2œÉ(1 ‚àí œÅ)‚àí1/2 .

5. kŒµ:: Œª? kœà2 = kŒ£k1/2 kŒª? k ‚â§

‚àö
2œÉ(1 ‚àí œÅ)‚àí1/2 kŒª? k

6. kœâ?0 Œµ:: kœà2 = œÉkœâ? k
7. kŒµ:T kœà2 = kŒµiT kœà2 ‚â§

‚àö

‚àí1/2

2T1

œÉ(1 ‚àí œÅ)‚àí1/2

8. kE[Œµ:T | Œµ:: ]kœà2 = kE[ŒµiT | Œµ:: ]kœà2 ‚â§ T1‚àí1 œÅ1/2 (1 ‚àí œÅ)‚àí1/2 œÉ
56

1/2

9. Var[ŒµiT | Œµi: ]
10. ŒµN : Œª? +

œâ?0 Œµ:

= Op

T‚àí



‚àí1/2
T1 œÉ(1

œâ?0 Œµ:: Œª?

= Op

‚àí1/2

‚àí œÅ)

h



‚àí1/2
N1 kŒª? k

+

‚àí1/2
T1 kœâ? k

i

‚àí1/2

+ kœâ? kkŒª? k œÉ(1 ‚àí œÅ)



.

Corollary 13. Under Assumption 6, for any subset Œõ of RT ‚àí1 , the least squares estimator and
oracle least squares estimator
ŒªÃÇ = minkY:: Œª ‚àí Y:T k22 and Œª? = minkL:: Œª ‚àí L:T k22
Œª‚ààŒõ

Œª‚ààŒõ

satisfy the bound kL:: (ŒªÃÇ ‚àí Œª? )k2 = OP (rŒª ) where, for approx-rank defined in Lemma 9,


rŒª = max kL:: Œª? ‚àí L:T k,
n
o
p
‚àí1/2
x approx-rank(L:: , x) for x = max kŒª? k, T1
œÉ(1 ‚àí œÅ)‚àí1/2 ,
‚àí1/2

diam1/2 (Œõ) ¬∑ œÉœÅ(1 ‚àí œÅ)‚àí1/2 (1 ‚àí œÅ2 )‚àí1/4 ,
r

p p
‚àí1/4
T1
diam(Œõ) max
T0 , 4 N0 T0 log(T0 ) ¬∑ œÉ(1 ‚àí œÅ)‚àí1/4 ,

T1

1/2

diam(Œõ) ¬∑ œÉ(1 ‚àí œÅ)‚àí1/2 ,

3/2 
1+œÅ
w(Œõ) ¬∑ œÉ
.
1‚àíœÅ

N0

Corollary 14. Under Assumption 6, consider the least squares estimators
œâÃÇ = arg minkœâ 0 Y:: ‚àí YN : k22 and ŒªÃÇ = arg minkY:: Œª ‚àí Y:T k22
œâ‚àà‚Ñ¶

Œª‚ààŒõ

and the oracle estimators œâ? , Œª? defined analogously with L substituted for Y and define
Œ¥œâ = kLN : ‚àí œâ?0 L:: k;
Œ¥Œª = kL:T ‚àí L:: Œª? k;
Œ¥sdid = |LN T ‚àí (œâ?0 L:T + LN : Œª? ‚àí œâ?0 L:: Œª? )| .

57

Then, for rŒª defined in Corollary 13,


bN T ‚àí LN T ‚â§ diam(Œõ) Œ¥œâ + Op œÉ(1 ‚àí œÅ)‚àí1/2 max {1, w(‚Ñ¶)}
L



+ diam(‚Ñ¶) Œ¥Œª + Op œÉ(1 ‚àí œÅ)‚àí1/2 max T1‚àí1 , w(Œõ)

+Œ¥sdid + Op diam(‚Ñ¶)rŒª + œÉ(1 ‚àí œÅ)‚àí1/2 œÅ1/2 T1‚àí1 w(‚Ñ¶) .

8.6

Proof of Lemma 12

Because all random variables involved are gaussian, the subgaussian norm k¬∑kœà2 is equivalent
to k¬∑kL2 . Furthermore, for a random vector v of identically distributed elements vi , kvkL2 =
pP
2
2
supkxk=1
i xi E vi = kvi kL2 .
1,2. Our bounds (i-ii) are determined by upper and lower bounds on the maximal and minimal
eigenvalues of the correlation matrix Œ£/œÉ 2 respectively, which for our AR(1) process are
no larger than

1+œÅ
1‚àíœÅ

and no smaller than

1‚àíœÅ
1+œÅ

respectively [see e.g. Trench, 1999, Section 1].

3.
v

u T0
T1 uX

1‚àíœÅ t
œÅ2(T0 +1‚àíj)
1‚àíœÅ
j=1
s
1 ‚àí œÅ2T0
1 ‚àí œÅT1
œÅ2
= T1‚àí1 œÉ 2 œÅ
1‚àíœÅ
1 ‚àí œÅ2

kŒ≥k = T1‚àí1 œÉœÅ

‚â§ T1‚àí1 œÉ 2 œÅ2 (1 ‚àí œÅ)‚àí1 (1 ‚àí œÅ2 )‚àí1/2 .
4,5,6. For any vector v, kŒµ:: vkœà2 = kŒµi: Œ£‚àí1/2 Œ£1/2 vkL2 = kŒ£1/2 vk ‚â§

‚àö

2œÉ(1 ‚àí œÅ)‚àí1/2 kvk, reducing

our problem to one about an identically distributed vector and in the last step using our
bound (i) and substituting 2 > 1 + œÅ. Similarly, ku0 Œµ:: kœà2 = ku0 Œµ:: Œ£‚àí1/2 Œ£kL2 ‚â§ kŒ£kkuk ‚â§
‚àö
2œÉ(1 ‚àí œÅ)‚àí1/2 kuk.

58

7. kŒµ:T kœà2 = kŒµiT kL2 , as it‚Äôs a vector of identically distributed gaussian elements, and
kŒµiT k2L2

=

=
‚â§

T1‚àí2

T1 X
T1
X

œÉ 2 œÅ|k‚àí`|

k=1 `=1
!
TX
1 ‚àí1
T1‚àí2 œÉ 2
2(T1 ‚àí s)œÅs ‚àí T1
s=0
TX
1 ‚àí1
‚àí2 2
œÅs ‚â§ 2T1‚àí1 œÉ 2 (1 ‚àí
T1 œÉ ¬∑ 2T1
s=0

œÅ)‚àí1 .

8. For theq
same reason, kE[Œµ:T | Œµ:: ]kœà2 = kE[ŒµiT | Œµi: ]kL2 , and kE[ŒµiT | Œµi: ]kL2 = kT1‚àí1
T1‚àí1

T1

œÉ ‚â§ T1‚àí1 œÅ1/2 (1 ‚àí œÅ)‚àí1/2 œÉ.
œÅ 1‚àíœÅ
1‚àíœÅ

9. E Var[ŒµiT | Œµi: ] ‚â§ E Œµ2iT ‚â§ 2T1‚àí1 œÉ 2 (1 ‚àí œÅ)‚àí1 .

10.
‚àö ‚àí1/2
kŒ£1/2 Œª? k ‚â§ 2N1 kŒª? kœÉ(1 ‚àí œÅ)‚àí1/2
‚àö ‚àí1/2
kœâ?0 ŒµÃÉ:T kL2 = kœâ? kkŒµÃÉiT kL2 ‚â§ 2T1 kœâ? kœÉ(1 ‚àí œÅ)‚àí1/2
‚àö
kœâ?0 ŒµÃÉ:: Œª? kL2 = kœâ? kkŒ£1/2 Œª? k ‚â§ 2kœâ? kkŒª? kœÉ(1 ‚àí œÅ)‚àí1/2 .
‚àí1/2

kŒµÃÉ0N : Œª? kL2 = N1

59

PT1

k=1

œÅk Œµi,T0 kL2 =

8.6.1

Proof of Lemma 11

Substituting the bound of Corollary 13 into that of Corollary 14,
bN T ‚àí LN T =Œ¥sdid +
L

Op
diam(Œõ) max {Œ¥œâ , w(‚Ñ¶)}
+ diam(‚Ñ¶) max {Œ¥Œª , w(Œõ)}
n
or

n
o
‚àí1/2
‚àí1/2
+ diam(‚Ñ¶) max kŒª? k, T1
approx-rank L:: , max kŒª? k, T1
‚àí1/2

+ diam(‚Ñ¶)T1
+

‚àí1/4
diam(‚Ñ¶)T1

diam1/2 (Œõ)
r


p p
4
diam(Œõ) max
T0 , N0 T0 log(T0 )

1/2

+ diam(‚Ñ¶)N0 diam(Œõ)

‚àí1
+ T1 w(‚Ñ¶) .
Several of the terms in this bound can be ignored, as they are bounded by others:
r

o

n
1/2
‚àí1/2
. diam(‚Ñ¶)N0 diam(Œõ);
approx-rank L:: , max kŒª? k, T1
r
o

n
‚àí1/2
‚àí1/2
‚àí1/2
1/2
diam(‚Ñ¶)T1
diam (Œõ) . diam(‚Ñ¶)T1
.
approx-rank L:: , max kŒª? k, T1
diam(‚Ñ¶)kŒª? k

Under the stated conditions on N1 and T1 , each of the remaining terms is o((N1 T1 )‚àí1/2 ).
1. The condition on N1 T1 in the lemma statement arises from terms in the expression above
with no factors of N1 , T1 . It is equivalent to the condition
‚àí1/2

(N1 T1 )


 max Œ¥sdid ,
diam(Œõ) max {Œ¥œâ , w(‚Ñ¶)} ,
diam(‚Ñ¶) max {Œ¥Œª , w(Œõ)} ,

1/2
diam(‚Ñ¶) diam(Œõ)N0
.

60

1/2

2. The condition on N1 T1
‚àí1/4

leading factor of T1

in the lemma statment arises from the term above with the

.
‚àí1/2

3. The condition on N1 arises from the terms above with leading factors of T1

61

and T1‚àí1 .

