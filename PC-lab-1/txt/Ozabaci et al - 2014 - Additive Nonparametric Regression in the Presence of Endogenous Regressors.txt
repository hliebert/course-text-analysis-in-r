This article was downloaded by: [McMaster University]
On: 06 March 2015, At: 10:23
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House,
37-41 Mortimer Street, London W1T 3JH, UK

Journal of Business & Economic Statistics
Publication details, including instructions for authors and subscription information:
http://www.tandfonline.com/loi/ubes20

Additive Nonparametric Regression in the Presence of
Endogenous Regressors
a

b

c

Deniz Ozabaci , Daniel J. Henderson & Liangjun Su
a

Department of Economics, University of Connecticut, Storrs, CT 06269 ()

b

Department of Economics, Finance and Legal Studies, University of Alabama, Tuscaloosa,
AL 35487 ()
c

School of Economics, Singapore Management University, Singapore 178903 ()
Accepted author version posted online: 07 May 2014.Published online: 28 Oct 2014.

Click for updates
To cite this article: Deniz Ozabaci, Daniel J. Henderson & Liangjun Su (2014) Additive Nonparametric Regression
in the Presence of Endogenous Regressors, Journal of Business & Economic Statistics, 32:4, 555-575, DOI:
10.1080/07350015.2014.917590
To link to this article: http://dx.doi.org/10.1080/07350015.2014.917590

PLEASE SCROLL DOWN FOR ARTICLE
Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained
in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no
representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the
Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and
are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and
should be independently verified with primary sources of information. Taylor and Francis shall not be liable for
any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever
or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of
the Content.
This article may be used for research, teaching, and private study purposes. Any substantial or systematic
reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any
form to anyone is expressly forbidden. Terms & Conditions of access and use can be found at http://
www.tandfonline.com/page/terms-and-conditions

Supplementary materials for this article are available online. Please go to http://tandfonline.com/r/JBES

Additive Nonparametric Regression in the
Presence of Endogenous Regressors
Deniz OZABACI
Department of Economics, University of Connecticut, Storrs, CT 06269 (ozabaci.deniz@gmail.com)

Daniel J. HENDERSON
Department of Economics, Finance and Legal Studies, University of Alabama, Tuscaloosa, AL 35487
(djhender@cba.ua.edu)

Liangjun SU

Downloaded by [McMaster University] at 10:23 06 March 2015

School of Economics, Singapore Management University, Singapore 178903 (ljsu@smu.edu.sg)
In this article we consider nonparametric estimation of a structural equation model under full additivity
constraint. We propose estimators for both the conditional mean and gradient which are consistent,
asymptotically normal, oracle efficient, and free from the curse of dimensionality. Monte Carlo simulations
support the asymptotic developments. We employ a partially linear extension of our model to study the
relationship between child care and cognitive outcomes. Some of our (average) results are consistent with
the literature (e.g., negative returns to child care when mothers have higher levels of education). However,
as our estimators allow for heterogeneity both across and within groups, we are able to contradict many
findings in the literature (e.g., we do not find any significant differences in returns between boys and girls
or for formal versus informal child care). Supplementary materials for this article are available online.
KEY WORDS: Additive regression; Endogeneity; Generated regressors; Oracle estimation; Structural
equation.

1.

INTRODUCTION

Nonparametric and semiparametric estimation of structural
equation models is becoming increasingly popular in the literature (e.g., Roehrig 1988; Vella 1991; Newey, Powell, and Vella
1999; Pinkse 2000; Ai and Chen 2003; Newey and Powell 2003;
Hall and Horowitz 2005; Su and Ullah 2008; Darolles et al.
2011; Chen and Pouzo 2012; Martins-Filho and Yao 2012; Gao
and Phillips 2013; Su, Murtazashvili, and Ullah 2013). In this
article, we are interested in improving efficiency by imposing
a full additivity constraint on each equation. Our starting point
is the triangular system in Newey, Powell, and Vella (1999).
While the assumptions of their model are relatively restrictive
as compared to other examples in the literature, their estimator is
typically easier to implement, which is useful for applied work.
While many existing estimators allow for full flexibility, they
also suffer from the curse of dimensionality.
To combat the curse, we impose an additivity constraint on
each stage and propose a three-step estimation procedure for our
additively separable nonparametric structural equation model.
We employ series/sieve estimators for our first two stages. The
first stage involves separate (additive) regressions of each endogenous regressor on each of the exogenous regressors to obtain consistent estimates of the residuals. These residuals are
used in our second-stage regression where we perform a single
(additive) regression of our response variable on each of the
endogenous regressors (not their predictions), the “included”
exogenous regressors and each of the residuals from the firststage regressions. Our final step (one-stage backfitting) involves
(univariate) local-linear kernel regressions to estimate the conditional mean and gradient of each of our additive components.

This process allows our final-stage estimators to be free from
the curse of dimensionality. Further, our estimators have the
oracle property. In other words, each additive component can
be estimated with the same asymptotic accuracy as if all the
other components in the regression model were known up to a
location parameter (see, e.g., Li and Racine 2007; Henderson
and Parmeter 2014, Horowitz 2014).
We prove that our conditional mean and gradient estimates
are consistent and asymptotically normal. We provide the uniform convergence rate for the additive components and their
gradients. Our theoretical findings show that our final-stage estimator has asymptotic bias and variance equivalent to those of a
single dimension nonparametric local-linear regression estimator. We further propose a partially linear extension of our model.
We argue that the parametric components can be estimated at
the parametric root-n rate and conclude that our estimates of
the additive components and associated gradients remain unaffected in the asymptotic sense. Finite sample results for each of
our proposed estimators are analyzed via a set of Monte Carlo
simulations and support the asymptotic developments.
To showcase our estimators with empirical data, we consider a
proper application relating child care use to cognitive outcomes

555

© 2014 American Statistical Association
Journal of Business & Economic Statistics
October 2014, Vol. 32, No. 4
DOI: 10.1080/07350015.2014.917590
Color versions of one or more of the figures in the article can be
found online at www.tandfonline.com/r/jbes.

Downloaded by [McMaster University] at 10:23 06 March 2015

556

Journal of Business & Economic Statistics, October 2014

for children (controlling for likely endogeneity). Specifically,
we use the data in Bernal and Keane (2011) to examine the
relationship between child test scores (our cognitive outcome)
from single mothers and cumulative child care (both formal
and informal). The extensive set of instrumental variables in the
dataset allows us to have a stronger set of instruments than what
is typically used in the literature and our more flexible (partially
linear) estimator leads to more insights as we can exploit the
heterogeneity present both between and within groups (e.g.,
male vs. female children).
Our empirical results show both similarities and differences
from the existing literature. When we look at the average values
of our estimates, we find similar results to those in Bernal and
Keane (2011). On average we find mostly positive returns (to test
scores) from marginal changes in income, mother’s education,
and AFQT score. However, the mean is but one point estimate.
When we check the distribution of the estimated returns, we see
that the main reason behind lower returns to child care use is the
amount of cumulative child care rather than the type. Specifically, we show that as the amount of child care use increases,
additional units of child care lead to even lower returns. Bernal
and Keane (2011) argued that those who use informal child care
(versus formal) and girls (versus boys) receive lower returns.
Our distributions of returns show no significant differences between these groups. We also find evidence of both positive and
negative returns to child care use. When we analyze the characteristics of the children in each group (positive vs. negative
returns to child care), we find that children with negative returns
are those whose mothers have higher levels education, experience, and AFQT scores. Conversely, those children with positive
returns typically have mother’s with lower levels of education,
experience, and AFQT scores.
The article is organized as follows. Section 2 describes our
methodology whereas Section 3 presents the asymptotic results.
Section 4 considers an extension to a partially linear model and
Section 5 examines the finite sample performance of our estimators via Monte Carlo simulations. Section 6 gives the empirical
application and the Section 7 concludes. All the proofs of the
main theorems are relegated to the appendix. Additional proofs
for the technical lemmas are provided in the online supplementary material.
Notation. For a real matrix A, we denote its transpose as
as A (≡ [tr(AA )]1/2 ), its spectral
A , its Frobenius norm
√
norm as Asp (≡ λmax (A A)), where tr(·) is the trace operator, ≡ means “is defined as” and λmax (·) denotes the largest
eigenvalue of a real symmetric matrix (similarly, λmin (·) denotes the smallest eigenvalue of a real symmetric matrix). Note
that the two norms are equal when A is a vector. For any function q(·) defined on the real line, we use q̇(·) and q̈(·) to deD
note its first and second derivatives, respectively. We use →
P
and → to denote convergence in distribution and probability,
respectively.

2.

METHODOLOGY

In this section, we introduce our model and then propose a
three-step estimation procedure that is a combination of both
series and kernel methods.

2.1 Model
We start with the basic set-up of Newey, Powell, and Vella
(1999). They considered a triangular system of the following
form
⎧
⎨ Y = g (X, Z1 ) + ε,
X = m (Z1 , Z2 ) + U, E (U|Z1 , Z2 ) = 0,
(2.1)
⎩
E (ε|Z1 , Z2 , U) = E (ε|U) ,
where X = (X1 , . . . , Xdx ) is a dx × 1 vector of endogenous regressors, Z1 = (Z11 , . . . , Z1d1 ) is a d1 × 1 vector of “included”
exogenous regressors, Z2 ≡ (Z21 , . . . , Z2d2 ) is a d2 × 1 vector
of “excluded” exogenous regressors, g(·, ·) denotes the true unknown structural function of interest, m ≡ (m1 , . . . , mdx ) is a
dx × 1 vector of smooth functions of the instruments Z1 and Z2
and ε and U ≡ (U1 , . . . , Udx ) are error terms. Newey, Powell,
and Vella (1999) are interested in estimating g(·, ·) consistently.
Newey, Powell, and Vella (1999) showed that g(·, ·) can be
identified up to an additive constant under the key identification
conditions that E(U|Z1 , Z2 ) = 0 and E(ε|Z1 , Z2 , U) = E(ε|U).
If these conditions hold, then
E (Y |X, Z1 , Z2 , U) = g (X, Z1 ) + E (ε|X, Z1 , Z2 , U)
= g (X, Z1 ) + E (ε|Z1 , Z2 , U)
= g (X, Z1 ) + E (ε|U) .

(2.2)

If U is observed, this is a standard additive nonparametric regression model. However, in practice, U is not observed and
it needs to be replaced by a consistent estimate. This motivated Su and Ullah (2008) to consider a three-stage procedure
to obtain consistent estimates of g(·, ·) via local-polynomial regressions. In the first-stage, they regressed X on (Z1 , Z2 ) via
local-polynomial regression and obtain the residuals 
U from
this first-stage reduced-form regression. In the second-stage,
they estimated E(Y |X, Z1 , U) via another local-polynomial reU. In the third-stage, they
gression by regressing Y on X, Z1 and 
obtained the estimates of g(x, z1 ) via the method of marginal
integration. Unlike previous works in the literature, including
Newey, Powell, and Vella (1999), Pinkse (2000), and Newey and
Powell (2003) that are based upon two-stage series approximations and only establish mean square and uniform convergence,
they establish the asymptotic distribution for their three-step
local-polynomial estimator.
There are two drawbacks associated with the estimator of
Su and Ullah (2008). First, it is subject to the notorious “curse
of dimensionality.” Without any extra restriction, the convergence rate of their second and third-stage estimators depend on
2dx + d1 and dx + d1 , respectively, which can be quite slow if
either dx or d1 is not small. As a result, their estimates may
perform badly even for moderately large sample sizes when
dx + d1 ≥ 3. Second, their estimator does not have the oracle
property which an optimal estimator of the additive component in a nonparametric regression model should exhibit. In this
article we try to address both issues.
To alleviate the curse of dimensionality problem, we propose
to impose some amount of structure on g(X, Z1 ), E(ε|U) and
ml (Z1 , Z2 ),where l = 1, . . . , dx . Specifically, we assume that
E(ε) = 0 and the above nonparametric objects have additive

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors

forms:

 
g (X, Z1 ) = μg + g1 (X1 ) + · · · + gdx Xdx + gdx +1 (Z11 )


+ · · · + gdx +d1 Z1d1 ,
 
E (ε|U) = με + gdx +d1 +1 (U1 ) + · · · + g2dx +d1 Udx , and


ml (Z1 , Z2 ) = μl + ml,1 (Z11 ) + · · · + ml,d1 Z1d1


+ ml,d1 +1 (Z21 ) + · · · + ml,d Z2d2 ,

where l = 1, . . . , dx and d = d1 + d2 . Consequently, we
have
 
E (Y |X, Z1 , Z2 , U) = μ + g1 (X1 ) + · · · + gdx Xdx


+ gdx +1 (Z11 ) + · · · + gdx +d1 Z1d1
 
+ gdx +d1 +1 (U1 ) + · · · + g2dx +d1 Udx

Downloaded by [McMaster University] at 10:23 06 March 2015

≡ ḡ (X, Z1 , U) ,

(2.3)

where μ = μg + με . Note that the gj (·)’s are not fully identified
without further restriction. Depending on the method, which is
used to estimate the additive components, different identification conditions can be imposed. For example, for the method
of marginal integration, a convenient set of identification conditions would be that each additive component (other than μ) in
(2.3) has expectation zero.
Horowitz (2014) reviewed methods for estimating nonparametric additive models, including the backfitting method, the
marginal integration method, the series method, and the mixture of a series method and a backfitting method to obtain
oracle efficiency. It is well known that it is more difficult to
study the asymptotic property of the backfitting estimator than
the marginal integration estimator, but the latter has a curse
of dimensionality problem if additivity is not imposed at the
outset of estimation as in conventional kernel methods. Other
problems that are associated with the marginal integration estimator include its lack of oracle property and its heavy computational burden. Kim, Linton, and Hengartner (1999) tried
to address the latter two problems by proposing a fast instrumental variable (IV) pilot estimator. However, they could not
avoid the curse of dimensionality problem. In fact, their IV
pilot estimator depends on the estimation of the density function of the regressors at all data points. In addition, their article
ignores the notorious boundary bias problem for kernel density estimates and because their IV pilot estimate is not uniformly consistent on the full support, they had to use a trimming
scheme to obtain the second-stage oracle estimator. To overcome
the curse of dimensionality problem, Horowitz and Mammen
(2004) proposed a two-step estimation procedure with series
estimation of the nonparametric additive components followed
by a backfitting step that turns the series estimates into kernel
estimates that are both oracle efficient and free of the curse of
dimensionality.
Below we follow the lead of Horowitz and Mammen (2004)
and propose a three-stage estimation procedure that is computationally efficient, oracle efficient, and fully overcomes the curse
of dimensionality, we shall adopt the following identification restrictions: gl (0) = gl (xl )|xl =0 = 0 for l = 1, . . . , 2dx + d1 , and
ml,k (0) = 0 for l = 1, . . . , dx and k = 1, 2, . . . , d. Similar identification conditions are also adopted in Li (2000). The differ-

557

ence between our models and theirs is that their model does not
allow for endogenous regressors. This complicates our problem
relative to theirs as the endogeneity requires us to replace the
unobserved errors in the second-stage with residuals. Hence, we
need to take care of the additional bias factor from the first-step.
Further, we also analyze the gradients of the additive nonparametric components.
2.2

Estimation

Given a random sample of n observations {Yi , Xi , Z1i ,
where
Xi = (X1i , . . . , Xdx i ) ,
Z1i =
Z2i }ni=1 ,

(Z11,i , . . . , Z1d1 ,i ) and Z2i = (Z21,i , . . . , Z2d2 ,i ) , we propose the following three-stage estimation procedure:
μl , {
ml,k (Z1k,i ), k = 1, . . . , d1 } and
1. For l = 1, . . . , dx , let 
{
ml,d1 +j (Z2j,i ), j = 1, . . . , d2 }, denote the series estimates
of μl , {ml,k (Z1k,i ), k = 1, . . . , d1 } and {ml,d1 +j (Z2j,i ), j =
1, . . . , d2 } in the nonparametric additive regression




Xli = μl + ml,1 Z11,i + · · · + ml,d1 Z1d1 ,i




+ ml,d1 +1 Z21,i + · · · + ml,d Z2d2 ,i + Uli .
li ≡ Xli − 
Let
U
μl − m
l,1 (Z11,i ) − · · · − m
l,d1 (Z1d1 ,i ) −
l,d (Z2d2 ,i )for l = 1, . . . , dx and
m
l,d1 +1 (Z21,i ) − · · · − m
i = 1, . . . , n.
2. Estimate μ, {gl (Xli ), l = 1, . . . , dx }, {gdx +j (Z1j,i ), j =
ki ), k = 1, . . . , dx }, in the fol1, . . . , d1 }, and {gdx +d1 +k (U
lowing additive regression model


Yi = μ + g1 (X1i ) + · · · + gdx Xdx i




+ gdx +1 Z11,i + · · · + gdx +d1 Z1d1 ,i
1i ) + · · · + g2dx +d1 (U
dx i ) + i
+ gdx +d1 +1 (U
by the series method. Denote the estimates as 
μ,
gdx +j (Z1j,i ), j = 1, . . . , d1 } and
{
gl (Xli ), l = 1, . . . , dx }, {
ki ), k = 1, . . . , dx }.
{
gdx +d1 +k (U
3. Estimate g1 (x1 ) and its first-order derivative by the
1i = Yi − 
μ−
g2 (X2i ) −
local-linear regression of Y
−
gdx +1 (Z11,i ) − · · · − 
gdx +d1 (Z1d1 ,i ) −
··· −
gdx (Xdx i )
1i ) − · · · − 
dx i ) on X1i . Estimates of

gdx +d1 +1 (U
g2dx +d1 (U
the other additive components in (2.3) and their first-order
derivatives are obtained analogously.
In relation to Horowitz and Mammen (2004), the above firststage is new as we have to replace the unobservable Uli by their
consistent estimates in the second-stage. In addition, Horowitz
and Mammen (2004) were only interested in estimation of the
nonparametric additive components themselves, while we are
also interested in estimating the first-order derivatives (gradients). Alternatively, we could follow Kim, Linton, and Hengartner (1999) and use the kernel estimator in the first two stages.
The oracle estimator of Kim, Linton, and Hengartner (1999)
has gained popularity in recent years. For example, Ozabaci
and Henderson (2012) obtained the gradients of their estimator
for the local-constant case and Martins-Filho and Yang (2007)
considered the local-linear version of the oracle estimator, both
assuming strictly exogenous regressors. However, as mentioned
above, using the kernel estimators in the first two stages here

558

Journal of Business & Economic Statistics, October 2014

has several disadvantages and does not avoid the curse of dimensionality problem.
For notational simplicity, let W = (X , Z1 , U ) and w =
 
(x , z1 , u) , where, for example, u = (u1 , . . . , udx ) denotes a realization of U. We shall use Z ≡ Z 1 ×Z 2 and W ≡ X × Z1 ×
U to denote the support of (Z1 , Z2 ) and W, respectively. Let
{pl (·), l = 1, 2, . . .} denote a sequence of basis functions. Let
κ1 = κ1 (n) and κ = κ(n) be some integers such that κ1 , κ → ∞
as n → ∞. Let pκ1 (v) ≡ [p1 (v), . . . .pκ1 (v)] . Define
 
P κ1 (z1 , z2 ) ≡ 1, pκ1 (z11 ) , . . . , p κ1 z1d1 ,
  
pκ1 (z21 ) , . . . , pκ1 z2d2
, and
 
κ (w) ≡ 1, pκ (x1 ) , . . . , p κ xdx , pκ (z11 ) , . . . ,

Downloaded by [McMaster University] at 10:23 06 March 2015

 
  
pκ1 z1d1 , pκ (u1 ) , . . . , p κ udx
.
For each (z1 , z2 ) ∈ Z, we approximate ml (z1 , z2 ) and ḡ(w) by
P κ1 (z1 , z2 ) α l and κ (w) β, respectively, for l = 1, . . . , dx ,
where α l ≡ (μl , α l,1 , . . . , α l,d ) and β = (μ, β 1 , . . . , β 2dx +d1 )
are (1 + dκ1 ) × 1 and (1 + (2dx + d1 )κ) × 1 vectors of unknown parameters to be estimated. Here, each α l,k , k =
1, . . . , d, is a κ1 × 1 vector and each β j ,j = 1, . . . , 2dx + d1 ,
is a κ × 1 vector. Let S1k and Sk denote κ1 × (1 + dκ1 ) and
κ × (1 + (2dx + d1 )κ) selection matrices, respectively, such
that S1k α l = α l,k and Sk β l = β l .
To obtain the first-stage estimators of the ml (·)’s, let

μl , 
α l,1 , . . . , 
α l,d ) be the solution to minαl n−1 ni=1
α l ≡ (
κ1
[Xli − P (Z1i , Z2i ) α l ]2 . The series estimator of ml (z) is given
by
αl
m
l (z1 , z2 ) = P κ1 (z1 , z2 ) 
= P κ1 (z1 , z2 )

−

n
−1

× n

P

κ1

(Z1i , Z2i ) P

κ1



(Z1i , Z2i )

i=1
n
−1

×n

P κ1 (Z1i , Z2i ) Xli ,
i=1

where A− denotes the Moore–Penrose generalized inverse
l (z1 , z2 ) =
of A. Note that we can write m
l (z1 , z2 ) as m
1
m
l,k (z1k ) + dj 2=1 m
l,d1 +j (z2j ), where m
l,k (z1k ) =

μl + dk=1
α l,k is a series estimator of ml,k (z1k ) for k = 1, . . . , d1
pκ1 (z1k )
α l,d1 +j is a series estimator of
and m
l,d1 +j (z2j ) = pκ1 (z2j )
ml,d1 +j (z2j ) for j = 1, . . . , d2 .
To obtain the second-stage estimators of the gl (·)’s,


let 
β ≡ (
μ, 
β 1, . . . , 
β 2dx +d1 ) be a solution to minβ n−1 ni=1
 
 i ) β]2 , where W
 i = (X , Z , 
[Yi − P κ (W
and 
Ui =
i
1i Ui )

dx i ) . The series estimator of ḡ(w) is given by
1i , . . . , U
(U

ḡ (w) = P κ (w) 
β=
μ+

dx

d1


gl (xl ) +
l=1


gdx +k (z1k )
k=1

Below we study the asymptotic properties of 
β and 
γ1 (x1 ).
3.

ASYMPTOTIC PROPERTIES

In this section we state two theorems that give the main results of the article. Even though several results are available
in the literature on nonparametric or semiparametric regressions with nonparametrically generated regressors (see, e.g.,
Mammen, Rothe, and Schienle 2012 and Hahn and Ridder 2013
for recent contributions), none of them can be directly applied
to our framework. In particular, Hahn and Ridder (2013) studied
the asymptotic distribution of three-step estimators of a finitedimensional parameter vector where the second-step consists
of one or more nonparametric generated regressions on a regressor that is estimated in the first-step. In sharp contrast, our
third-stage estimator is also a nonparametric estimator. Under
fairly general conditions, Mammen, Rothe, and Schienle (2012)
focused on two-stage nonparametric regression where the first
stage can be kernel or series estimation while the second stage
is local-linear estimation. In principle, we can treat our secondand third-stage estimation as their first- and second-stage estimation, respectively, and then apply their results to our case.
However, their results are built upon high-level assumptions and
are usually not optimal. For this reason, we derive the asymptotic
properties of our three-stage estimators under some primitive
conditions specified in the preceding section.
Let Yi , Wi ≡ (Xi , Z1i , Ui ) , Z2i and Uli to denote the ith
random observation of Y, W, Z2 , and Ul , respectively. Let ei ≡
Yi − ḡ(Xi , Z1i , Ui ) and i = κ (Wi ), and Q ≡ E[i i ].
The asymptotic properties of the second-stage series estimator

β are reported in the following theorem.
Theorem 3.1. Suppose that Assumptions A.1–A.5(i) in Appendix A hold. Then
n
n
−1 −1
−1
(i) 
β − β = Q−1
 n
i=1 i ei + Q n
i=1 i [ḡ(Xi ,
dx
n
−1
Z1i , Ui ) − i β] − Q−1
n

×
i

i=1
l=1 ġdx +d1 +l

(Uli )(Uli − Uli ) + Rn,β ;
(ii) 
β − β = OP (νn + ν 1n );
ḡ(w) − ḡ(w) = OP [ς0κ (νn + ν1n )];
where
(iii) supw∈W 
Rn,β  = τn OP (νn + ν1n ) and ν1n , νn and τn are defined
in Assumption A.5(i).

To appreciate the effect of the first-stage series estimation on
the second-stage series estimation, let β̄ denote a series estimator
of β by using Ui together with (Xi , Z1i ) as the regressors. Then
it is standard to show that
n

dx

+

third stage by using the kernel function K(·) and bandwidth
∗
11 , . . . , Y
1n ) , X1i
(x1 ) ≡ (1, X1i − x1 ) , X1 (x1 ) ≡
h. Let 
Y1 ≡ (Y
∗
∗

[X11 (x1 ), . . . , X1n (x1 )] and Kx1 ≡ diag(K1x1 , . . . , Knx1 ) where
Kix1 ≡ Kh (X1i − x1 ) and Kh (·) ≡ K(·/ h)/ h. Then

−1
Y1 .
γ1 (x1 ) = X1 (x1 ) Kx1 X1 (x1 )

X1 (x1 ) Kx1 


gdx +d1 +k (uj ).
j =1

γ1 (x1 ) ≡ [
g1 (x1 ),
Let γ1 (x1 ) ≡ [g1 (x1 ), ġ1 (x1 )] . We use 

ġ 1 (x1 )] to denote the local-linear estimate of γ1 (x1 ) in the

β̄ − β =

−1
Q−1
 n

i ei
i=1
n

−1
+ Q−1
 n
i=1



i ḡ (Xi , Z1i , Ui ) − i β + R̄n,β

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors

and β̄ − β = OP (νn ), where R̄n,β  = OP (κn−1/2 νn ) =
o(νn ). The third term on the right-hand side of the expression in
Theorem 3.1(i) signifies the asymptotically nonnegligible dominant effect of the first-stage estimation on the second-stage
estimation.
With Theorem 3.1, it is straightforward to show the asymptotic joint distribution of our three-stage estimators of g1 (x1 )
and its gradient.
Theorem 3.2. Let H ≡diag(1, h). Suppose that Assumptions
A.1–A.5 in Appendix A hold. Then
√
D
nhH [
γ1 (x1 ) − γ1 (x1 ) − b1 (x1 )] →
υ21 2
h g̈1 (x1 )
N(0, 1 (x1 )), where b1 (x1 ) ≡ ( 2
), 1 (x1 ) ≡
0
2
σ (x1 )/fX1 (x1 )
0
(
), σ 2 (x1 ) ≡
2
fX1 (x1 )]
0
υ22 σ 2 (x1 )/[υ21
E(ei2 | X1i = x1 ), fX1 (·) denotes theprobability density
function (PDF) of X1i , and υst ≡ v s K(v)t dv for s,
t = 0, 1, 2.
(ii) (Uniform consistency) Suppose that Q,e ≡ E(i i ei2 )
has
bounded
maximum
eigenvalue.
Then
γ1 (x1 ) − γ1 (x1 )] = OP ((nh/ log n)−1/2 +
supx1 ∈X1 H [
h2 ).

Downloaded by [McMaster University] at 10:23 06 March 2015

(i) (Normality)

Theorem 3.2(i) indicates that our three-step estimator of
γ1 (x1 ) = [g1 (x1 ), ġ1 (x1 )] has the asymptotic oracle property.
The asymptotic distribution of the local-linear estimator of
γ1 (x1 ) is not affected by random sampling errors in the first
two-stage estimators. In fact, the three-step estimator of γ1 (x1 )
has the same asymptotic distribution that we would have if the
other components in ḡ(x, z1 , u) were known and a local-linear
procedure is used to estimate γ1 (x1 ). Theorem 3.2(ii) gives the
uniform convergence rate for 
γ1 (x1 ). Similar properties can be
established for the local-linear estimators of other components
of ḡ(x, z1 , u). In addition, following the standard exercise in the
nonparametric kernel literature, we can also demonstrate that
these estimators are asymptotically independently distributed.
4.

PARTIALLY LINEAR ADDITIVE MODELS

In this section we consider a slight extension of the model
in (2.1) to the following partially linear functional coefficient
model
⎧
Y = g (X, Z1 ) + θ  V + ε,
⎪
⎪
⎨
X = m (Z1 , Z2 ) + V + U, E (U|Z1 , Z2 , V) = 0,
(4.1)
E (ε|Z1 , Z2 , U, V)
⎪
⎪
⎩
= E (ε|U) , E (ε) = 0,
where Y, X, Z1 , Z2 , Z, and ε are defined as above, V is a
k × 1 vector of exogenous variables, θ is a k × 1 parameter
vector and  = [ψ1 , . . . , ψd x ] is a dx × k matrix of parameters
in the reduced form regression for X. To avoid the curse of
dimensionality, we continue to assume that m(Z1 , Z2 ), g(X, Z1 ),
and E(ε|U) have the additive forms given in Section 2.1.
We remark that the results developed in previous sections
extend straightforwardly to the model specified in (4.1). Note

559

that
E (Y |X, Z1 , Z2 , U, V) = g (X, Z1 ) + E (ε|U) + θ  V
= ḡ(X, Z1 , U) + θ  V and


E X|Z1 , Z2 , V = m (Z1 , Z2 ) + V.

(4.2)
(4.3)

Given a random sample {(Yi , Xi , Z1i , Z2i , Vi ), i = 1, . . . , n},
we can continue to adopt the three-step procedure outlined in Section 2.2 to estimate the above model. First, we
choose (α l , ψl ) to minimize n−1 ni=1 [Xli − P κ1 (Z1i , Z2i ) α l −
l ) denote the solution. The series estimaαl , ψ
Vi ψl ]2 . Let (
l (z1 , z2 ) = P κ1 (z1 , z2 )
α l . Detor of ml (z1 , z2 ) is given by m

l Vi . Let 
l (Z1i , Z2i ) − ψ
Ui =
fine the residuals Uli = Xli − m
 
κ 
 i = (X , Z , 
dx i ) , W
1i , . . . , U
(U
i
1i Ui ) and P (Wi ) be defined as
before. Second, we choose (β, θ ) to minimize n−1 ni=1 [Yi −


 i ) β −Vi θ ]2 . Let 
β ≡ (
μ, 
β 1, . . . , 
β 2dx +d1 ) and 
θ denote
P κ (W

κ 




the solution. Define Y1i = Yi − β (−1) P (Wi ) − θ Vi where

β with its component 
β 1 being replaced
β (−1) is defined as 
by a κ × 1 vector of zeros. Third, we estimate g1 (x1 ) and its
1i on X1i via the localfirst-order derivative by regressing Y
linear procedure. Let 
γ1 (x1 ) denote the estimate of γ1 (x1 ) via
local-linear fitting.
It is well known that the finite dimensional parameter
vectors
√
ψl ’s and θ can be estimated at the parametric n-rate and the
appearance of the linear components in (4.1) will not affect the
asymptotic properties of 
β and 
γ1 (x1 ). To conserve space, we
do not repeat the arguments here.
5.

FINITE SAMPLE PROPERTIES

In this section we evaluate the finite sample properties of our
estimator by simulations. We first look at four data generating
processes (DGPs) to show the performance of our estimator.
We then consider higher dimensional data and compare our
estimator to a fully nonparametric alternative of Su and Ullah
(2008). We report the average bias, variance, and root mean
square error (RMSE) for the final-stage conditional mean and
gradients estimates across 1000 Monte Carlo simulations. We
consider three different sample sizes: 100, 200, and 400.
5.1

Baseline Simulations

We consider four different DGPs of structural equations between Y, X, Z (once for V in DGP 3), ε, and u. Unless we state
otherwise, Z1 and Z2 are independently distributed as uniform
from zero to one (U [0, 1]) and ε and u are independently distributed as Gaussian with mean zero and variance one (N (0, 1))
and are mutually independent of one another and of X and
(Z1 , Z2 ). For the first three DGPs, each error distribution is
assumed to be homoscedastic.
Our first DGP is our baseline model and is given as
Y = sin (X) + sin (Z1 ) + ε, and X = sin (Z1 ) + sin (Z2 ) + u.
Our second DGP considers a slightly more complicated firststage regression model as Z2 enters the reduced form of X via

560

Journal of Business & Economic Statistics, October 2014

Table 1. Monte Carlo simulations for the final stage conditional mean and gradient estimates
n = 100

Downloaded by [McMaster University] at 10:23 06 March 2015

Bias
DGP 1
DGP 2
DGP 3
DGP 4
Variance
DGP 1
DGP 2
DGP 3
DGP 4
RMSE
DGP 1
DGP 2
DGP 3
DGP 4

n = 200


g (·, ·)

∂
g (·,·)
∂x

∂
g (·,·)
∂z1

0.0522
−0.0649
0.0526
0.0515

−0.0611
0.0525
−0.0544
−0.0341

0.0526
0.1234
0.1170
0.1182
0.2468
0.3708
0.3589
0.3659

n = 400


g (·, ·)

∂
g (·,·)
∂x

∂
g (·,·)
∂z1

0.2847
−0.0346
−0.0019
0.0542

0.0419
−0.0621
0.0371
0.0412

−0.0586
0.0178
−0.0575
−0.0556

0.0092
−0.0483
−0.0024
0.0166

0.0879
0.2485
0.1530
0.1520

0.3017
0.7299
0.6956
0.6670

0.0316
0.1211
0.0703
0.0692

0.0593
0.2352
0.1054
0.1016

0.3699
0.6714
0.4947
0.5004

0.6608
0.9763
0.9839
0.9740

0.1870
0.3701
0.2779
0.2755

0.2938
0.6684
0.4143
0.3974

the PDF of a logistic distribution
Y = 0.25X2 + 0.5Z12 + ε,


g (·, ·)

∂
g (·,·)
∂x

∂
g (·,·)
∂z1

0.0297
−0.0434
0.0278
0.0285

−0.0358
0.0577
−0.0502
−0.0347

−0.0078
−0.0013
−0.0015
−0.0138

0.1672
0.6851
0.4047
0.3773

0.0195
0.0508
0.0443
0.0436

0.0419
0.2242
0.0686
0.0714

0.1120
0.3233
0.2732
0.2562

0.4880
0.9310
0.7609
0.7161

0.1465
0.2365
0.2199
0.2163

0.2405
0.6382
0.3235
0.3171

0.3785
0.6197
0.5865
0.5669

objective function
1
GCV (κ1 ) =
n

and

e−Z2
X = 0.20Z12 + 
2 + u.
1 + e−Z2
Our third DGP considers the partially linear extension where V1
and V2 are distributed via a binomial and Gaussian distribution,
respectively:
Y = sin (X) + sin (Z1 ) + 0.5V1 + V2 + ε,

and

X = sin (Z1 ) + sin (Z2 ) + u.
Finally, our fourth DGP is similar to the first, but allows
for heteroscedasticity (note that our theory allows for heteroscedasticity). Specifically, we allow the variance of ε to be
a function of Z1 and Z2 via 0.1 + 0.5Z12 + 0.5Z22 . Apparently,
dx = d1 = d2 = 1 in DGPs 1-4.
We estimate the structural function in three steps. In the first
two steps we use cubic B-splines for the sieve estimation and
in the third step we use local-linear kernel regression. For the
spline estimation, we specify the number of knots as 2n1/5 so
that κ1 = κ = 2n1/5 + 4, where · denotes the integer part
of ·. For the kernel regression, we need to choose both the kernel function K(·) and the bandwidth parameter h. We apply
the Gaussian kernel throughout
the simulations and applica√
tion: K(v) = exp(−v 2 /2)/ 2π . There are two standard ways
to choose the bandwidth. One is to apply Silverman’s rule of
thumb by setting h = 1.06sX n−1/5 , where sX denotes to sample
standard deviation of X; and the other is to consider leaveone-out least-squares cross-validation (LSCV). To save time on
computation, we consider Silverman’s rule-of-thumb choice of
bandwidth. In our application, we will use generalized crossvalidation (GCV) to choose the number of sieve basis terms in
each of the first two steps’ sieve estimation and LSCV to choose
the bandwidth h in the last step kernel estimation. For example,
in the first step, we choose κ1 to minimize the following GCV

n

2

κ1 (Zi ) / [1 − (κ1 /n)]2 ,
Xi − m

i=1

where Zi is a collection of all exogenous variables and m
κ1 (·) denotes the sieve estimation of E(Xi |Zi ) by imposing the additive
structure and using κ1 terms of cubic B-spline basis functions
to approximate each additive component. In the third step, we
choose h to minimize
LSCV (h) =

1
n

n

2

1i − 
g1,−i (X1i ) ,
Y

i=1

where 
g1,−i (X1i ) is the leave-one-out version of 
g1 (X1i ) defined
in Section 2.2.
The simulation results for the final-stage regressions can be
found in Table 1. Each of the results are as expected. The average
bias, variance and RMSE of each estimator decreases with the
sample size. The conditional mean is estimated more precisely
than its gradients. The estimators in the homoscedastic DGP
outperform those from the heteroscedastic DGP (DGP 1 vs. 4).
5.2 Higher Dimensional Performance
Now we look at the performance of our estimator with higher
dimensional data and compare it with that of a fully nonparametric alternative—Su and Ullah (2008). Unless stated otherwise, Z1 , Z2 , Z3 , Z4 , and Z5 are independently distributed as
U [0, 1] and ε and u are independently distributed as N (0, 1)
and are mutually independent of one another and of X and
(Z1 , Z2 , Z3 , Z4 , Z5 ). For DGPs 5-7, each error distribution is
assumed to be homoscedastic.
Our fifth DGP is a variant of our baseline model and is given
as
Y = sin (X) + sin (Z1 ) + sin (Z2 ) + sin (Z3 ) + sin (Z4 ) + ε,
X = sin (Z1 ) + sin (Z2 ) + sin (Z3 ) + sin (Z4 ) + sin (Z5 ) + u.

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors

561

Table 2. Monte Carlo simulations for higher dimensional data for the final stage conditional mean and gradient estimates
n = 100

Downloaded by [McMaster University] at 10:23 06 March 2015

Bias
DGP 5
DGP 6
DGP 7
DGP 8
Variance
DGP 5
DGP 6
DGP 7
DGP 8
RMSE
DGP 5
DGP 6
DGP 7
DGP 8

n = 200


g (·, ·)

∂
g (·,·)
∂x

∂
g (·,·)
∂z1

0.0468
−0.0820
0.0659
0.0492

0.0390
0.0022
0.0509
0.0470

0.0453
0.0128
0.0056
−0.0599

0.2510
0.2480
0.5752
0.3575

0.2551
0.2453
0.2977
0.3000

1.2890
1.2710
1.7930
1.7510

0.5091
0.5111
0.7680
0.6052

0.4364
0.4663
0.7292
0.8277

1.1820
1.1620
0.5137
1.4001

n = 400


g (·, ·)

∂
g (·,·)
∂x

∂
g (·,·)
∂z1

0.0420
−0.0659
0.0613
0.03897

0.0492
−0.0028
0.0350
0.0303

0.0032
0.0093
0.0190
0.0024

0.1225
0.1198
0.3355
0.1750

0.0897
0.0977
0.0994
0.1158

0.3577
0.3573
0.5691
0.4263

0.3145
0.3206
0.5456
0.5833

Our sixth DGP is specified as follows:
Y = 0.25X2 + 0.5Z12 + sin (Z2 ) + Z32 + 3Z42 + ε,
e−Z2
X = 0.20Z12 + 
2 + cos (Z2 ) + sin (Z3 )
1 + e−Z2
+ sin (Z4 ) + u.
Our seventh DGP considers the partially linear extension
where V1 and V2 are distributed via a binomial and Gaussian
distribution, respectively:
Y = sin (X) + sin (Z1 ) + sin (Z2 ) + sin (Z3 ) + sin (Z4 )
+ 0.5V1 + V2 + ε,
X = sin (Z1 ) + sin (Z2 ) + sin (Z3 ) + sin (Z4 ) + sin (Z5 ) + u.
Finally, our eighth DGP is similar to the fifth, but allows for
heteroscedasticity. Specifically, we allow the variance of ε to be


g (·, ·)

∂
g (·,·)
∂x

∂
g (·,·)
∂z1

0.0364
−0.0583
0.0472
0.0332

0.0329
0.0011
0.0281
0.0371

−0.0082
0.0024
−0.0254
−0.0081

0.5907
0.6142
0.7068
0.8258

0.0699
0.0685
0.1809
0.0985

0.0571
0.0583
0.0637
0.0776

0.3391
0.3386
0.3713
0.4766

0.7953
0.8058
0.3315
0.9460

0.2700
0.2710
0.4306
0.3195

0.2498
0.2528
0.3987
0.2870

0.5970
0.5948
0.5833
0.7138

a function of Z1 , Z2 , Z3 , Z4 , and Z5 via 0.1 + 0.5Z12 + 0.5Z22 +
Z32 + Z42 + Z52 . Apparently, dx = d2 = 1 and d1 = 4 in DGPs
5–8.
The simulation results for the final-stage regressions can be
found in Tables 2 and 3 for our proposed estimates and Su
and Ullah’s (2008) estimates, respectively. For Su and Ullah’s
estimates, we basically follow their suggestions to choose the
orders of local polynomial regression (3 in the first stage and 1 in
the second stage), kernel, and bandwidth, but use the technique
of Kim, Linton, and Hengartner (1999) in the third stage to
speed up the calculation. The findings for our estimator are
similar to those in Table 1. As for the comparison between the
two estimates, we see that the additive estimates, which exploit
the additive nature of the data, have smaller bias, variance, and
RMSE than Su and Ullah’s fully nonparametric estimates with
higher dimensional data. In particular, Su and Ullah’s estimates
are subject to the curse of dimensionality and tend to have very
large variance and RMSE even for moderate sample sizes (e.g.,

Table 3. Monte Carlo simulations for higher dimensional data for Su and Ullah’s (2008) final stage conditional mean and gradient estimates
n = 100

Bias
DGP 5
DGP 6
DGP 7
DGP 8
Variance
DGP 5
DGP 6
DGP 7
DGP 8
RMSE
DGP 5
DGP 6
DGP 7
DGP 8


g (·, ·)

∂
g (·,·)
∂x

0.4796
1.0310
0.4767
1.3720

−0.0122
0.0549
−0.0238
0.4493

0.8934
1.6010
0.9969
2.0390
1.0218
1.0990
1.1134
2.4940

n = 200

n = 400


g (·, ·)

∂
g (·,·)
∂x

∂
g (·,·)
∂z1


g (·, ·)

∂
g (·,·)
∂x

∂
g (·,·)
∂z1

0.2364
0.2030
0.2484
0.4159

0.3342
−0.9077
0.1881
0.7077

−0.0166
0.0649
0.0015
0.5225

0.2500
0.2400
0.0963
0.5809

0.2655
0.3724
0.2334
0.5766

−0.0227
−0.1362
0.0998
0.5119

0.0066
0.2916
1.0110
0.4634

1.2996
1.5890
1.2118
2.8980

4.9969
5.6349
4.9003
10.8100

0.8995
1.4980
0.5720
2.0640

1.1916
1.7370
0.7518
2.6460

4.3957
5.0254
3.2076
9.4325

0.8924
1.4130
0.5663
2.0050

1.0031
1.6297
0.7199
2.5940

2.0049
3.9871
2.9871
9.2876

1.3035
1.7050
1.2347
2.9580

5.0366
5.6563
4.9763
10.9200

0.9635
1.0130
0.6046
2.2019

1.1989
1.6930
0.7565
2.7030

4.4616
5.0655
3.2311
9.4439

0.9334
0.9725
0.5997
2.0970

1.1157
1.6354
0.7223
2.6510

2.0134
4.5161
3.1100
9.3057

∂
g (·,·)
∂z1

562

Journal of Business & Economic Statistics, October 2014

n = 400) as they need to estimate d1 + d2 = 5, 2dx + d1 = 6,
and dx + d1 = 5 dimensional nonparametric objects in DGPs
5, 6 and 8 in their first-, second-, and third-stage estimation,
respectively. In DGP 7, the linear components V1 and V2 in
the structural equation are also counted as a part of Z1 in Su
and Ullah’s procedure. As a result, d1 = 6 and even higher
dimensional nonparametric objects have to be estimated. We
also consider models with an even higher number of covariates
and the results are as expected: the variance and RMSE of Su and
Ullah’s estimates blow up quickly as the number of covariates
increase and those of ours are still well behaved.

Downloaded by [McMaster University] at 10:23 06 March 2015

6.

APPLICATION: CHILD CARE USE AND
TEST SCORES

It is generally accepted in the literature that early childhood
achievement is a strong predictor for success (better labor market outcomes) later in life (Keane and Wolpin 1997, 2001a,b;
Cameron and Heckman 1998; Bernal and Keane 2011). Thus,
researchers have focused on the determinants of childhood
achievement. Various models have been developed and most focus on cognitive ability as the outcome measure. In the present
context, we are concerned whether or not child care improves
or hurts a particular measure of cognitive ability, test scores.
Although this is an interesting question, previously there were
serious data limitations.
The two major limitations associated with cognitive ability
production functions in this context are sample selection bias
and endogeneity. Sample selection bias occurs when only mothers’ labor force participation is used in the analysis. This variable
implicitly assumes that it is a direct indicator of child care use.
The main problem here is that working mothers and nonworking
mothers may differ substantially in the cognitive ability production process and if only labor force participation is used, the
analysis is going to rule out “nonworking” mothers. Adding actual child care use can help take care of the selectivity problem
(see Bernal 2008; Bernal and Keane 2011).
The second issue is potential endogeneity of the child care
use variable. To the best of our knowledge, there are relatively
few papers in this literature that use instrumental variable estimation to solve the endogeneity problem and those that do
find no benefits to IV regression. Two possible reasons for this
are the use of restrictive methods (those that likely hide the
existing heterogeneity of mothers hinder the sources of potential endogeneity) and data limitations. The three papers that we
are aware of which use IV regressions are Blau and Grossberg
(1992), James-Burdumy (2005), and Bernal and Keane (2011).
Blau and Grossberg (1992) used maternal labor supply as an
indicator of child care use and analyzed children’s cognitive
development. They defined endogeneity via the participation
decision of mothers. They defined it as a comparison between
in home and market production. They stated that the employed
and unemployed mothers’ differences may create differences in
child quality production. Hence, they focused on the endogeneity of the mothers. They proposed an instrument for maternal
labor supply and concluded that there is no statistical heterogeneity between employed and unemployed mothers (and reject
the IV model). An issue with their paper (which they pointed
out), is the possibility of weak instruments. They also did not
have a detailed control for child care. James-Burgundy (2005)

focused on the same problem and used labor market conditions
for her fixed effects IV model. Potentially weak instruments
were again blamed for rejection of the IV model.
In response to the issues mentioned above, Bernal and Keane
(2011) obtained data on actual child care use (which helps correct for selectivity issues) as well as an extensive number of
instrumental variables (which helps correct for the weak instrumental variable issue). Further, they chose a larger age range
(compared to existing studies) for children in their application
(previous studies found stronger correlations for their target
ages). Also, they focused only on single mothers which arguably fits their set of instruments better. They concluded that
their IV regression performs well.
We start our analysis with Bernal and Keane’s (2011) data,
but consider a more flexible cognitive ability production function (g(·) ). Consider the following cognitive ability production
function
test = g(age, care, inc, nmchild, char, iabil) + ε,

(6.1)

where test is the logarithm of child test scores (our measure
of cognitive ability), age represents the child’s age, care (the
primary variable of interest) is cumulative child care, inc is
the logarithm of cumulative income since childbirth, char is
a vector of group characteristics of the child and the mother
(e.g., mother’s AFQT score), nmchild is the number of children
and iabil measures initial ability (e.g., birth weight). Equation
(6.1) is the baseline cognitive ability production equation (minus
any functional form assumptions) in Bernal and Keane (2011,
p. 474).
A primary concern of Equation (6.1) is that care, inc, and
nmchild may be correlated with the error term. Hence, we use
instruments to correct for this potential endogeneity. Following
Bernal and Keane (2011), we use local demand conditions and
welfare rules as instruments.
Our contribution here is to provide a more flexible version
of the cognitive ability production function proposed by Bernal
and Keane (2011). This allows us to obtain the effects of child
care for each child. Standard least-squares estimation methods
are best suited to data near the mean. Looking solely at the
mean may be misleading. Further, it is arguable that we are
more interested in the upper or lower tails of the distribution
of returns to child care. Our approach allows us to observe the
overall variation.
Here we will be using the partially linear additive nonparametric specification. In our first stage, we estimate three separate
regressions: one for each endogenous regressor (care, inc, and
nmchild). In Equation (4.1), these are given as the regressions
of X on Z1 , Z2 , and V. Specifically, our first-stage equations are
written as
care = mc1 (mafqt) + mc2 (med) + mc3 (mex)
+ mc4 (mage) + θc V + uc
inc = mi1 (mafqt) + mi2 (med) + mi3 (mex)
+ mi4 (mage) + θi V + ui
nmchild = mn1 (mafqt) + mn2 (med) + mn3 (mex)
+ mn4 (mage) + θn V + un

(6.2)

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors

where we allow the control variables of mother’s AFQT score
(mafqt), mother’s education (med), mother’s experience (mex),
and mother’s age (mage) to enter nonlinearly. The remaining
control variables as well as each of the instruments are contained
in V. Note that V includes interactions for each instrument with
mafqt and med. This results in a total of 99 regressors in each
first-stage regression (clearly indicating the need for a partially
linear model).
After obtaining consistent estimates of each of the residual
vectors from Equation (6.2), we run the second-stage model via
a nonparametric additively separable partially linear regression
of log test scores (test) on the endogenous variables (not their
predicted values), each of the residuals from the first-stage and
the remaining control variables as
test = g1 (mafqt) + g2 (med) + g3 (mex) + g4 (mage)

Downloaded by [McMaster University] at 10:23 06 March 2015

+ g5 (care) + g6 (inc) + g7 (nmchild) + g8 (
uc )
+ g9 (
ui ) + g10 (
un ) +  
V + ,

(6.3)

where 
V is the same (20) control variables included in V (and
does not include any of the instruments Z2 ) as well as linear
interactions between the control variables in the nonparametric
functions (mafqt, med, mex, mage, inc, and nmchild) and childcare (care). Estimation (of the additive components and their
gradients) in the final-stage follows from Section 2.2.
Regarding implementation, note that in the first two stages we
use cross-validation techniques to choose the number of knots
for our B-splines. In our final-stage estimates, we use crossvalidation techniques to determine the bandwidth in our kernel
function. R code which can be used to estimate our model is
available from the authors upon request.

6.1 Data
Our data come directly from Bernal and Keane (2011). The
data are extensive and we will attempt to summarize them in
a concise manner. Those interested in the specific details, are
referred to the excellent description in the aforementioned paper.
Their primary data source is the National Longitudinal Survey
of Youth of 1979 (NLSY79). The exact instruments and control
variables can be found in Tables 1 and 2 in Bernal and Keane
(2011).
As noted in the introduction, the dataset consists of single
mothers. Although this may seem like a data restriction at first,
it leads to stronger instruments. The main reason behind this
choice, as explained by Bernal and Keane (2011), is that single
mothers fit their set of instruments better. The primary instruments used here are welfare rules, which (as claimed by Bernal
and Keane 2011) give exogenous variation for single mothers.
The (1990s) welfare policy changes resulted in increased employment rates for single mothers, hence higher child care use.
We describe our variables for the 2454 observations in our sample in more detail below.
6.1.1 Endogenous Regressors. We consider three potentially endogenous variables (X): cumulative child care, cumulative income, and number of children. These are the left-handside variables in our first-stage equations. They are modeled in

563

an additively separable nonparametric fashion in the secondstage regression.
6.1.2 Instruments. We group our instruments (Z2 ) into
four categories: time limits, work requirements, earning disregards, and other policy variables and local demand conditions.
We briefly explain each of these categories and refer the reader
to a more in-depth description of the instruments in Bernal and
Keane (2011, pp. 466–469).
Time limits. We consider (time limits for) two programs
which aid in helping families with children: Aid to Families
with Dependent Children (AFDC) and Waivers and Temporary
Aid for Needy Families (TANF). Under AFDC, single mothers
with children under age 18 may be eligible to get help from the
program as long as they fit certain criteria that are typically set
by the state and program regulations. TANF on the other hand,
enables the states to set certain time limits on the benefits they
provide to eligible individuals. AFDC provides the benefits and
TANF creates the variability because states can set their own
limits. The limits are important for benefit receivers because an
eligible female may become ineligible by hitting the limit and
she may choose to save some of the eligibility for later use. We
include each of the eight (time limit) instruments proposed by
Bernal and Keane (2011).
Work requirements. TANF requires eligible females to return to work after a certain time, as set by the state, to be able
to remain eligible. These rules are state-dependent. While the
main required length for females is to start working within two
years, several states prefer to choose shorter time limits. Some
states lift this requirement for females with young children. Besides the variation amongst states, even within states there exists
variation. Here we include each of the nine (work requirement)
instruments.
Earning disregards. The AFDC and TANF benefits are adjusted by states depending upon the number of children and
earnings of the eligible females. While more children may lead
to greater benefits, more earnings may lower them. States set
the level for AFDC grants and adjust the amount of reduction
in benefits via TANF. Specifically, our first-stage regressions
include both the “flat amount of earnings disregarded in calculating the benefit amount” and the “benefit reduction rate”
instrumental variables.
Other policy variables and local demand conditions. Our remaining instruments are grouped in one generic category: other
policy variables and local demand conditions. Here we consider
two additional programs for families with young children. These
programs are Child Support Enforcement (CSE) and the Child
Care Development Fund (CCDF). Bernal and Keane (2011) reported CSE as a significant source of income for single mothers
via the 2002 Current Population Survey. CSE’s goal is to find
absent parents and establish relationships with their children.
CCDF on the other hand, is a subsidy-based program which
provides child care for low-income families. States are independent in designing their own programs and, hence, variation is
present.
In addition to the policy variables, earned income tax credit
(EITC), the unemployment rate, and hourly wage rate are listed
as instruments. EITC is a wage support program for lowincome families. This is a subsidy-based program and the subsidy amount varies with family size. The benefit levels are not

Downloaded by [McMaster University] at 10:23 06 March 2015

564

Journal of Business & Economic Statistics, October 2014

conditioned on actual family size since family size is endogenous. Our first-stage regressions include six instruments from
this category.
6.1.3 Control Variables. In addition to the instrumental
variables, we have 24 control variables (Z1 ) which show up
in each stage (4 nonlinearly and 20 linearly). These variables
primarily represent characteristics of the mothers and children.
These are each assumed to be exogenous. The four variables
that enter nonlinearly are the mother’s AFQT score, education
level, work experience, and age. We treat these nonparametrically as we believe their impacts vary with their levels and
we are unaware of an economic theory which states exactly
how they should enter. To understand the intuition behind this
specification, consider a model where these variables enter linearly. In that case, having linear schooling in a model would
imply that each additional year of schooling a mother gets will
lead to the same percentage change in the child’s test score. The
same will hold true for a linearly modeled AFQT score, age, or
experience. There is no reason to assume that this will be true.

Table 5. Final-stage gradient estimates for child care use for different
types of child care, gender, amount, and for different attributes of the
mother at various percentiles for the specific group with
corresponding wild bootstrapped standard errors

Formal
Informal
Female
Male
Above median
child care
Below median
child care
Education < 12
Education ≥ 12

6.2 Results

Experience < 5

There are a vast number of results that can be presented from
this procedure and data. We plan to limit our discussion to a few
important issues. First, we want to determine the strength of our
instruments. To determine how well the instruments predict the
endogenous regressors, we propose a F-type (wild) bootstrapbased test for joint significance of the instruments. Second, we
are interested in whether or not endogeneity exists. We check
for this by testing for joint significance of g8 (·), g9 (·), and g10 (·)
in Equation (6.3). Finally, we are interested in potential heterogeneity in the return to child care use. We accomplish this by
separating our observation-specific estimates among different
prespecified groups.
Our final-stage results will be given in three separate tables.
The first two tables will give the 10th, 25th, 50th, 75th, and
90th percentiles of the estimated returns and their related (wild)
bootstrapped standard errors. The first of those tables (Table 4)

Experience ≥ 5

Table 4. Final-stage gradient estimates for each of the nonlinear
variables at various percentiles with corresponding wild bootstrapped
standard errors

care
inc
nmchild
med
mex
mafqt
mage

10%

25%

50%

75%

90%

−0.0089
0.0034
−0.0415
0.0225
−0.0157
0.0077
−0.0058
0.0062
−0.0034
0.0031
−0.0012
0.0008
−0.0051
0.0028

−0.0061
0.0038
−0.0045
0.0258
−0.0157
0.0077
−0.0058
0.0062
−0.0022
0.0031
0.0000
0.0009
−0.0017
0.0024

−0.0005
0.0042
0.0250
0.0249
−0.0117
0.0046
−0.0046
0.0061
−0.0016
0.0025
0.0015
0.0009
0.0035
0.0024

0.0049
0.0027
0.0463
0.0329
−0.0021
0.0065
0.0037
0.0091
0.0003
0.0038
0.0034
0.0013
0.0051
0.0020

0.0079
0.0054
0.0741
0.0304
−0.0021
0.0065
0.0128
0.0080
0.0031
0.0038
0.0046
0.0019
0.0063
0.0037

No experience
Age < 23
Age > 23,
< 29
Age ≥ 30

10%

25%

50%

75%

90%

−0.0087
0.0052
−0.0090
0.0089
−0.0092
0.0038
−0.0089
0.0034
−0.0092
0.0038
−0.0061
0.0038
−0.0067
0.0037
−0.0090
0.0089
−0.0090
0.0036
−0.0087
0.0052
−0.0061
0.0038
−0.0087
0.0052
−0.0090
0.0089
−0.0090
0.0036

−0.0067
0.0037
−0.0061
0.0038
−0.0061
0.0038
−0.0067
0.0037
−0.0087
0.0052
−0.0010
0.0035
0.0014
0.0036
−0.0068
0.0032
−0.0068
0.0032
−0.0046
0.0038
−0.0007
0.0036
−0.0053
0.0036
−0.0061
0.0038
−0.0076
0.0034

−0.0016
0.0036
−0.0007
0.0036
−0.0005
0.0042
−0.0005
0.0042
−0.0054
0.0036
0.0013
0.0042
0.0028
0.0037
−0.0014
0.0036
−0.0014
0.0036
0.0011
0.0040
0.0028
0.0037
0.0007
0.0039
−0.0007
0.0053
−0.0016
0.0036

0.0035
0.0032
0.0028
0.0049
0.0057
0.0046
0.0036
0.0046
0.0028
0.0037
0.0076
0.0034
0.0076
0.0034
0.0028
0.0049
0.0028
0.0049
0.0065
0.0031
0.0076
0.0034
0.0063
0.0036
0.0049
0.0037
0.0028
0.0037

0.0065
0.0031
0.0067
0.0042
0.0079
0.0055
0.0079
0.0055
0.0049
0.0037
0.0218
0.0080
0.0218
0.0080
0.0067
0.0042
0.0067
0.0042
0.0218
0.0080
0.0218
0.0080
0.0139
0.0066
0.0079
0.0055
0.0076
0.0034

will look at the returns to (gradients of) the final-stage estimates
of each of the (excluding the residuals) nonlinear variables from
the second-stage regressions. The remaining tables (Tables 5
and 6) will decompose the gradients for the child care use
variable.
We also provide several figures of estimated densities and
distribution functions. Figures 1–3 give a set of density plots for
the estimated returns to child care use. This allows us to see the
overall distribution. We believe this is a more informative type
of analysis as compared to solely showing results at the mean or
Table 6. Median characteristics for groups with positive versus
negative child care use gradients
Attribute
Mother’s education
Mother’s experience
Mother’s AFQT
Mother’s age
Child’s age
Formal child care
Informal child care
Total child care
Number of children
Sample size

Positive

Negative

12
4
15
22
5.8
0
6
8.5
1
1141

12
6
19
24
5.8
0
9
11.5
1
1313

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors

565

40
0
−0.015

−0.010

−0.005

0.000

0.005

0.010

0.015

0.020

Childcare Gradient

Figure 1. Density of estimated returns to child care for those with only formal versus those with only informal child care.

instruments. To analyze this, we check the significance of our
instruments in each of our three first-stage regressions. Noting
that we have 99 regressors in each first-stage, the percentage of
significant instruments in each first-stage regression is roughly
one-half. This type of analysis, of course, is informal. Rather

female
male

30
20
10

Density

40

50

60

percentiles. Figure 4 looks at empirical cumulative distribution
functions (ECDF) for both positive and negative gradients with
respect to the amount of child care.
6.2.1 First- and Second-Stage Estimation. For our firststage regressions, our main focus is the performance of our

0

Downloaded by [McMaster University] at 10:23 06 March 2015

20

Density

60

formal
informal

−0.01

0.00

0.01

0.02

Childcare Gradient

Figure 2. Density of estimated returns to child care for male and female children.

566

Journal of Business & Economic Statistics, October 2014

60
0

−0.01

0.00

0.01

0.02

0.03

Childcare Gradient

Figure 3. Density of estimated returns to child care for those with above and below the median units of child care.

sum of squares between a restricted and unrestricted model.
Our restricted model assumes that each of our instruments have
coefficients equal to zero. The asymptotic distribution can be
obtained by multiplying the statistic by a constant, but it is well

negative
positive

0.2

0.4

ECDF

0.6

0.8

1.0

than relying on univariate-type significances, we prefer to perform formal tests to check for joint significance.
Here we perform a nonparametric F-type test, originally proposed by Ullah (1985). The test involves comparing the residual

0.0

Downloaded by [McMaster University] at 10:23 06 March 2015

20

40

Density

80

100

above.median
below.median

5

10

15

20

Childcare Gradient

Figure 4. Empirical cummulative distribution functions for the amount of child care for those with both positive and negative returns to child
care.

Downloaded by [McMaster University] at 10:23 06 March 2015

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors

known that using the asymptotic distribution is problematic in
practice. Instead, we use a wild bootstrap to determine the conclusion of our tests. For each first-stage regression we perform
a test where the null is that each instrument is irrelevant. In each
case our p-value is zero to at least four decimal places. Hence,
we argue (as did Bernal and Keane 2011) that our instruments
are relevant in our prediction of our endogenous regressors.
In the second-stage regression we are concerned with the
joint significance of each of the residuals from the first-stage
regressions. We perform a similar Ullah (1985)-type test as
above and reject the null that the three sets of residuals are
jointly insignificant with a p-value that is zero to four decimal
places. We conclude that endogeneity is likely present and thus
justify the use of our procedure.
6.2.2 Final-Stage Estimation. Here we are interested in
comparing our gradient estimate results to those of Bernal and
Keane (2011). Our average gradient estimates (Table 4) for
the nonlinear variables are often similar in magnitude and sign
to their results. We find mostly positive effects for mother’s
income, education, and AFQT scores. For our primary variable
of interest, the gradient on child care is also similar at the median
(noting that our median result is insignificant). To put this in
perspective, the median coefficient of −0.0005 is equivalent to
a 0.2% decrease in test scores for an additional year of child
care use (or 0.05% for an additional quarter). That being said,
each of these statements ignore the heterogeneity in the gradient
estimates allowed by our procedure.
When we look at the percentiles, we see both positive and
negative estimates. This is not possible with standard linear
models. We, therefore, want to determine the story behind these
variable returns. To do so we break the results up for different
child care type, amount of child care, and between gender. We
also examine whether heterogeneity is present among mothers
(level of education, experience, and age). Finally, we try to determine what attributes are common with those receiving positive
or negative returns to child care use.
Disaggregating the child care gradient. In the first few rows
of Table 5, we analyze the child care gradient with respect to
child care type, child gender, and amount of child care use. In
this table we report the percentiles for the estimated gradients
for the related groups and associated standard errors. We also
provide Figures 1–3 which show the overall variation for the
(selected) chosen pairs. Before we get into the details for different groups, we want to point out that many of the results are
insignificant. In fact, only 634 of the 2454 estimates are statistically significant at the 5% level. What this implies is that for a
large portion of the sample, an additional unit of child care will
have no impact on test scores. That being said, we find many
cases where it does matter and we will highlight the results
below.
Bernal and Keane (2011) found that only informal child care
(e.g., a grandparent) had significantly negative effects. Specifically, they found that an additional year of informal child care
led to a 2.6% reduction in test scores. To compare our results,
we separated the gradients on child care use between those who
received only formal or only informal child care. Although there
are some differences in our percentile estimates, Figure 1 shows
essentially no difference in the densities of estimates between
those who received only formal (e.g., a licensed child care facil-

567

ity) versus only informal child care. Bernal and Keane (2011)
also found differences in returns to child care between genders. Both Table 5 and Figure 2 show essentially no difference
between these two groups.
Where we do find a difference, is with respect to the amount
of child care. In Figure 3, we look at the estimated gradients of
child care use for those children who get below and above the
median total child care. We find that more child care use leads
to lower returns. In fact, we can see negative returns for those
children receiving relatively more child care which suggests
decreasing returns to child care use. We consider this finding
important since this shows us evidence to believe that the lower
returns may be associated with the amount of child care use
rather than type of child care or gender of the child.
In the remaining rows of Table 5, we separate our estimated
gradients on child care based on attributes of the mothers.
Specifically, we analyze the estimated returns for mothers of different levels of education, experience, and age. We see variation
in our estimates for mothers of different age groups. As mothers
get older, we see lower returns to child care use (more negative and significant estimates) as compared to those of younger
mothers.
We also see variation for different experience levels. Here
we find more negative (and significant) estimates for mothers
with more experience. On the other hand, for mothers with no
experience, we see much larger (and significant) returns to child
care.
Similarly, mothers with more education appear to have more
negative returns. Many of the percentiles are negative for mothers with 12 or more years of education. For mothers without a
high school diploma, we see positive effects for all but the lowest percentile. In other words, for less educated mothers, more
child care may actually improve their child’s test scores.
All these results show us two important things. First, there
is substantial heterogeneity in our returns to child care use that
cannot be captured with a single parameter estimate. Second,
the returns tend to be related to the amount of child care and the
quality of maternal time. For those mothers who have more education and experience, child care tends to hurt their child’s test
score. On the other hand, for those mothers with less education
and experience, our results tend to suggest that their children
may be better off with more child care.
Positive and negative returns. For most of the reported estimates, we tend to see both positive and negative returns. This is
perhaps a more important result than the lower and higher estimated returns. What this finding suggests is that there are some
children who benefit from additional child care and there are
some who are harmed. We hope to uncover who these children
are and hopefully, the drivers of such returns.
Table 6 separates the partial effects on child care by those
which are positive and negative. Perhaps the first point of interest
is that slightly more than half of the gradients are negative. If
we were to run a simple ordinary least-squares regression, this
would (back of the envelope) suggest a negative coefficient. This
is what is typically found in the literature.
As for the remaining values in the table, the rows represent
characteristics of interest and each number represents the median value for that characteristic for both children with positive
and negative returns. Many values are the same. For example,

Downloaded by [McMaster University] at 10:23 06 March 2015

568

Journal of Business & Economic Statistics, October 2014

mother’s education, child’s age, quarters of formal child care,
and number of children are the same at the median in each
group. However, we see that for negative returns that, (median)
mothers have more experience and are older. It is true that they
have more informal child care (which likely represents the result
of Bernal and Keane 2011), but it is also true that they have far
more quarters of child care at the median.
Again, these are only point estimates. If we were to plot the
ECDFs of types of child care for the groups with negative and
positive child care gradients, we would see that the amount of
both formal and informal child care use are higher for those with
negative returns. For example, Figure 4 plots the distribution
functions with respect to total child care use between those with
negative and those with positive returns to child care. Those
children with negative returns to child care, receive more child
care overall. We fail to reject the null of first-order dominance
via a Kolmogorov–Smirnov test with a p-value near unity. We
also find this same level of dominance when we look at formal
versus informal or male versus female. This is strong evidence
that it is the amount of child care and not necessarily the type
that matters.
7.

CONCLUSION

In this article, we develop oracle efficient estimators for additive nonparametric structural equation models. We show that
our estimators of the conditional mean and gradient are consistent, asymptotically normal, and free from the curse of dimensionality. The finite sample results support the asymptotic
development. We also consider a partially linear extension of
our model which we use in an empirical application relating
child care to test scores. In our application we find that the
amount of child care use and not the type, is primarily responsible for the sign of the returns. Given that our nonparametric
procedure will give us observation specific estimates, we are
able to uncover, that in addition to the amount of child care,
what attributes of mothers are related to different returns. We
find evidence that more educated, more experienced mothers
with higher test scores (themselves) are associated with lower
returns to child care for their children. On the other hand, less
educated, less experienced mothers with lower test scores’ (for
themselves) children often have positive returns to child care.
APPENDIX
In this appendix we first provide assumptions that are used to prove
the main results and then prove the main results in Section 3.

APPENDIX A: ASSUMPTIONS
A real-valued function q(·) on the real line is said to satisfy a Hölder
condition with exponent r ∈ [0, 1] if there is cq such that |q(v) − q(
v )|
≤ cq |v − 
v |r for all v and 
v on the support of q(·). q(·) is said to be
γ -smooth, γ = r + m, if it is m-times continuously differentiable on
U and its mth derivative, ∂ m q(·), satisfies a Hölder condition with exponent r. The γ -smooth class of functions are popular in econometrics
because a γ -smooth function can be approximated well by various
linear sieves; see, for example, Chen (2007). For any scalar function
q(·) on the real line that has r derivatives and support S, let |q(·)|r ≡
maxs≤r supv∈S |∂ s q(v)| . Let Xl and Ul denote the supports of Xl and Ul ,
respectively, for l = 1, . . . , dx . Let Zsk denote the support of Zsk for

k = 1, . . . , ds and s = 1, 2. Let QP P ≡ E[P κ1 (Z1 , Z2 )P κ1 (Z1 , Z2 ) ]
and QP P ,Ul = E[P κ1 (Z1 , Z2 )P κ1 (Z1 , Z2 ) Ul2 ] for l = 1, . . . , dx . Let
Zi ≡ (Z1i , Z2i ) .
We make the following assumptions:
Assumption A.1. (i) {Yi , Xi , Zi , i = 1, . . . , n} are an IID random
sample. (ii) The supports W and Z of Wi and Zi are compact. (iii) The
distributions of Wi and Zi are absolutely continuous with respect to
the Lebesgue measure.
Assumption A.2. (i) For every κ1 that is sufficiently large, there
exist c1 and c̄1 such that 0 < c1 ≤ λmin (QP P ) ≤ λmax (QP P ) ≤ c̄1 <
∞ and λmax (QP P ,Ul ) ≤ c̄1 < ∞ for l = 1, . . . , dx . (ii) For every
κ that is sufficiently large, there exist c2 and c̄2 such that 0 <
c2 ≤ λmin (Q ) ≤ λmax (Q ) ≤ c̄2 < ∞. (iii) The functions {ml,k (·),
l = 1, . . . , d, k = 1, . . . , d}, and {gj (·), j = 2dx + d1 } belong to
the class of γ -smooth functions with γ ≥ 2. (iv) There exist
−γ
α l,k ’s such that supz∈Z1k |ml,k (z) − p κ1 (z) α l,k | = O(κ1 ) for l =
κ1
1, . . . , dx and k = 1, . . . , d1 , supz∈Z2k |ml,d1 +k (z) − p (z) α l,d1 +k | =
−γ
O(κ1 ) for l = 1, . . . , dx and k = 1, . . . , d2 . (v) There exist β l ’s
such that supx∈Xl gl (x) − p κ (x) β l  = O(κ −γ ) for l = 1, . . . , dx ,
κ

−γ
) for k = 1, . . . , d1 and
sup
 z∈Z1l |gdz +k (·)κ − p (z) β dz+k | = O(κ
−γ
gdx +d +l (·) − p (·) β d +d +l  = O(κ ) for l = 1, . . . , dx . (vi) The
1
x
1
1
set of basis functions, {pj (·), j = 1, 2, . . .}, are twice continuously
differentiable everywhere on the support of Uli for l = 1, . . . , dx .
max1≤l≤dx max0≤s≤r supul ∈Ul ∂ s p κ (ul ) ≤ ςrκ for r = 0, 1, 2.
Assumption A.3. (i) The PDF of any two elements in Wi is
bounded, bounded away from zero, and twice continuously differentiable. (ii) Let σi2 ≡ σ 2 (Xi , Zi , Ui ) ≡ E(ei2 |Xi , Zi , Ui ) and Qsk,pp ≡
E[p κ1 (Zsk,i )p κ1 (Zsk,i ) σi2 ] for k = 1, . . . , ds and s = 1, 2. The largest
eigenvalue of Qsk,pp is bounded uniformly in κ1 .
Assumption A.4. The kernel function K(·) is a PDF that is symmetric, bounded and has compact support [−cK , cK ]. It satisfies the
Lipschitz condition |K(v1 ) − K(v2 )| ≤ CK |v1 − v2 | for all v1 , v2 ∈
[−cK , cK ].
Assumption A.5. (i) κ1 ≤ κ. As n → ∞, κ1 → ∞, κ 3 /n → 0,
2
,
and τn → c1 ∈ [0, ∞), where τn ≡ (κ 1/2 ς0κ + ς1κ )ν1n + ς0κ ς2κ ν1n
−γ
1/2
1/2
1/2
1/2
−γ
ν1n ≡ κ1 /n + κ1 , and νn ≡ κ /n + κ . (ii) As n → ∞,
h → 0, nh3 log n → ∞, nhκ −2γ → 0, τn ν1n = o(n−1/2 h−1/2 ), and
−γ
2
](νn + ν1n ) → 0.
[h1/2 ς1κ (1 + n1/2 κ1 ) +ς2κ n1/2 h1/2 ν1n
Assumptions A.1(i)–(ii) impose IID sampling and compactness on
the support of the exogenous independent variables. Either assumption can be relaxed at lengthy arguments; see, for example, Su and Jin
(2012) who allowed for weakly dependent data and infinite support for
their regressors. A1(iii) requires that the variables in Wi and Zi be continuously valued, which is standard in the literature on sieve estimation.
Assumptions A.2(i)–(ii) ensure the existence and nonsingularity of the
asymptotic covariance matrix of the first two-stage estimators. They are
standard in the literature; see, for example, Newey (1997), Li (2000),
and Horowitz and Mammen (2004). Note that all of these authors assume that the conditional variances of the error terms given the exogenous regressors are uniformly bounded, in which case the second part
of A2(i) becomes redundant. A2(iii) imposes smoothness conditions
on the relevant functions and A2(iv)–(v) quantifies the approximation
error for γ -smooth functions. These conditions are satisfied, for example, for polynomials, splines, and wavelets. A2(vi) is needed for the
application of Taylor expansions. It is well known that ςrκ = O(κ r+1/2 )
and O(κ 2r+1 ) for B-splines and power series, respectively (see Newey
1997). The rate at which splines uniformly approximate a function is
the same as that for power series, so that the uniform convergence
rate for splines is faster than power series. In addition, the low multicollinearity of B-splines and recursive formula for calculation also
leads to computational advantages (see chap. 19 of Powell 1981 and
chap. 4 of Schumaker 2007). For these reasons, B-splines are widely
used in the literature.
Assumptions A.3(i)–(ii) and A4 are needed for the establishment
of the asymptotic property of the third-stage estimators. A3(ii) is redundant under Assumption A.2(i) if one assumes that the conditional

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors

Downloaded by [McMaster University] at 10:23 06 March 2015

variances of ei ’s given (Xi , Zi , Ui ) are uniformly bounded. A4 is standard for local-linear regression (see Fan and Gijbels 1996 and Masry
1996). The compact support condition facilitates the demonstration of
the uniform convergence rate in Theorem 3.2 but can be removed at the
cost of some lengthy arguments (see, e.g., Hansen 2008). In particular,
the Gaussian kernel can be applied. Assumptions A.5(i)–(ii) specify
conditions on κ1 , κ, and h. Note that we allow the use of different
series approximation terms in the first- and second-stage estimation,
which allows us to see clearly the effect of the first-stage estimates
on the second-stage estimates. The first condition (namely, κ1 ≤ κ) in
A5(i) is needed for the proof of a technical lemma (see Lemma B.5(iii))
and it can be removed at the cost of some additional assumptions on
the basis functions. The terms that are associated with ν1n arise because of the use of the nonparametrically generated regressors in the
second-stage series estimation. The appearance of log n arises to establish uniform consistency results in Theorem 3.2 and it can be replaced
by 1 if we are only interested in the pointwise result. In the case where
2
).In
ςrκ = O(κ r+1/2 ) in Assumption A.2(vi), τn = O(κ 3/2 ν1n + κ 3 ν1n
practice, we recommend setting κ1 = κ. These restrictions, in conjunction with the condition γ ≥ 2, imply that the conditions in Assumption
A.5 can be greatly simplified as follows:
Assumption A.5 ∗ . (i) As n → ∞, κ → ∞, κ 4 /n → c1 ∈ [0, ∞).
(ii) As n → ∞, h → 0, nh3 log n → ∞, nhκ −2γ → 0, and n−1 hκ 5 →
0.

APPENDIX B: PROOFS OF THE RESULTS IN
SECTION 3
 i ), Qn,P P ≡ n−1 ni=1 Pi Pi ,
i ≡ κ (W
Pi ≡ P κ1 (Zi ), 
n
n  
−1

−1

Qn, ≡ n
i=1 i i , and Qn, ≡ n
i=1 i i . By Lemmas
n, are
B.1(ii) and (v) and Lemma B.4(iv), Qn,P P , Qn, , and Q
invertible with probability approaching 1 (w.p.a.1) so that in large
−
samples we can replace the generalized inverses Q−
n,P P , Qn, , and
−
−1
−1
−1


Qn, by Qn,P P , Qn, , and Qn, , respectively. We first state some
technical lemmas that are used in the proof of the main results in
Section 3. The proofs of all technical lemmas but Lemma B.6 are
given in the online supplementary materials.
Let

Lemma B.1. Suppose that Assumptions A.1 and A.2(i)–(ii) and (vi)
hold. Then
(i) Qn,P P − QP P 2 = OP (κ12 /n);
(ii) λmin (Qn,pp ) = λmin (QP P ) + oP (1) and λmax (Qn,pp ) =
λmax (QP P ) + oP (1);
−1
1/2
(iii) Q−1
);
n,P P − QP P sp = OP (κ1 /n
2
2
(iv) Qn, − Q  = OP (κ /n);
(v) λmin (Qn, ) = λmin (Q ) + oP (1) and λmax (Qn, ) =
λmax (Q ) + oP (1).
−1

n
i=1

Pi Uli
Lemma
B.2. Let
ξnl ≡ n
n−1 ni=1 Pi [ml (Zi ) − Pi α l ] for l = 1, . . . , dx .
Assumptions A.1–A.2 hold. Then

and
ζnl ≡
Suppose that

(i) ξnl 2 = OP (κ1 /n);
−2γ
(ii) ζnl 2 = OP (κ1 );
n
n
−1 −1
−1 −1
(iii) 
α l −α l = Qκ1 n
i=1 Pi Uli + Qκ1 n
i=1 Pi [ml (Zi ) −
−γ +1/2

where
rnl  = OP (κ1 /n + κ1
/n1/2 )and
Pi α l ] + rnl ;
l = 1, . . . , dx .
Lemma B.3. Suppose that Assumptions A.1–A.3 hold. Then for
l = 1, . . . , dx ,
2
li − Uli )2 [σi2 ]r = OP (ν1n
) for r = 0, 1;
n−1 ni=1 (U
r 2
li − Uli )2 i r = OP (ς0κ
ν1n ) for r = 1, 2;
n−1 ni=1 (U
2 2
li ) − p κ (Uli )2 = OP (ς1κ
n−1 ni=1 p κ (U
ν1n );
n
−1
κ 
κ

1/2
[p
(
U
)
−
p
(U
)]

=
O
(κ
ς0κ ν1n +
n
li
li
P
i
i=1
2
ς0κ ς2κ ν1n );
li ) − p κ (Uli )]ei  = OP (n−1/2 ς1κ ν1n ).
(v) n−1 ni=1 [p κ (U

(i)
(ii)
(iii)
(iv)

569

Lemma B.4. Suppose Assumptions A.1–A.3 hold. Then
(i)
(ii)
(iii)
(iv)
(v)
(vi)

2 2
i − i 2 = OP (ς1κ
n−1 ni=1 
ν1n );
n
−1


n
(

−

)

=
O
(κ 1/2 ς1κ ν1n );
i
i
sp
P
i
i=1
2
1/2

);
Qn, − Qn, sp = OP (κ ς0κ ν1n + ς0κ ς2κ ν1n
−1
−1
2
1/2

Qn, − Q sp = OP (κ ς0κ ν1n + ς0κ ς2κ ν1n );
i − i )ei  = OP (n−1/2 ς1κ ν1n );
n−1 ni=1 (
−γ
n
−1


n
i=1 (i − i )[ḡ(Xi , Z1i , Ui ) − i β] = OP (κ1 ς1κ ν1n ).

Lemma B.5. Let ξn ≡ n−1 ni=1 i ei and ζn ≡ n−1 ni=1
i [ḡ(Xi , Z1i , Ui ) − i β]. Suppose Assumptions A.1–A.3 hold. Then
(i) ξn  = OP (κ 1/2 /n1/2 );
(ii) ζn  = OP (κ −γ );
n
dx
−1
κ


(iii) Q−1
 n
l=1 ṗ (Uli ) β dx +d1 +l (Uli − Uli ) =
i=1 i
OP (ν1n ) for l = 1, . . . , dx .
Lemma B.6. Let c ≡ (c1 , c2 ) be an arbitrary 2 × 1 nonrandom vector such that c = 1. Suppose that Assumptions A.1–A.5 hold. Then
for l = 2, . . . , dx
∗
li −
(i) S2nl (x1 ) ≡ n−1/2 h1/2 ni=1 Kix1 c H −1 X1i
(x1 )(U
κ
1/2
1/2 −γ
in x1 ;
Uli )ṗ (Uli ) = h ς1κ OP (1 + n κ1 ) uniformly

∗
li − Uki )2 =
(ii) S2nl (x1 ) ≡ n−1/2 h1/2 ni=1 Kix1 c H −1 X1i
(x1 ) (U
2
) uniformly in x1 .
n1/2 h1/2 OP (ν1n
∗
Kix c H −1 X1i
(x1 )p κ (Xli ) and
η̄l (x1 ) ≡ E[ηnl (x1 )]. By straightforward moment calculations and
Chebyshev inequality, we have ηnl (x1 ) = η̄l (x1 ) + rηl (x1 ) where
rηl (x1 ) = OP (κ 1/2 n−1/2 h−1/2 ). In fact, supx1 ∈X1 rηl (x1 ) =
OP (κ 1/2 (nh/ log n)−1/2 ) with a simple application of Bernstein
inequality for independent observations; see, for example, Serfling
(1980, p. 95). As an aside, note that the proof of Lemma 7 in
Horowitz and Mammen (2004) contains various errors as they ignored
the fact that κ is diverging to infinity as n → ∞. Note that for
l = 2, . . . , dx ,

Proof. (i) Let ηnl (x1 ) ≡ n−1

n
i=1

η̄l (x1 )



∗
(x1 ) p κ (Xli )
= E Kh (X1i − x1 ) c H −1 X1i



= K (v) (c1 + c2 v) p κ (xl ) f1l x1 + h1/2 v, xl dvdxl

= c1


f1l (x1 , xl ) p κ (xl ) dxl + c1

K (v) [f1l (x1 + hv, xl )


− f1l (x1 , xl )] p κ (xl ) dxl + c2

K (v) v [f1l (x1 + hv, xl )

− f1l (x1 , xl )] dvp κ (xl ) dxl
≡ c1 η̄1l (x1 ) + c1 η̄2l (x1 ) + c2 η̄3l (x1 ) .
As in Horowitz and Mammen (2004, p. 2435), in view of the fact that
the components of η̄1l (x1 ) are the Fourier coefficients of a function that
is bounded uniformly over X1 , we have supx1 ∈X1 η̄1l (x1 )2 = O(1). In
addition, using Assumptions A.2(v) and A3(i), we can readily show that
supx1 ∈X1 η̄2l (x1 ) = OP (κ 1/2 h2 ) and supx1 ∈X1 η̄3l (x1 ) = OP (κ 1/2 h).
It follows that supx1 ∈X1 η̄l (x1 ) = OP (1 + κ 1/2 h) = OP (1) under Assumption A.5(ii) and supx1 ∈X1 ηnl (x1 ) = OP (1).
By Equation C.2 in the supplementary appendix, S1nl (x1 ) =
∗
− 5s=1 n−1/2 h1/2 ni=1 Kix1 c H −1 X1i
(x1 )ṗ κ (Uli )usl,i ≡ − 5s=1
1/2 1/2
S1nl,s (x1 ), say. Noting that S1nl,1 (x1 ) = n h ηnl (x1 )(
μl − μl ), we
have


μl − μl |
sup S1nl,1 (x1 ) ≤ n1/2 h1/2 sup ηnl (x1 ) |
x1 ∈X1

x1 ∈X1

=n

1/2 1/2

h



OP (1) OP n−1/2 = oP (1) .

570

Journal of Business & Economic Statistics, October 2014

1
Next, note that S1nl,2 (x1 ) = dk=1
S1nl,2k (x1 ) where S1nl,2k (x1 ) =
n
∗
−1/2 1/2
 −1
n
h
X1i (x1 )ṗ κ (Uli )p κ1 (Z1k,i ) S1k a1l . We
i=1 Kix1 c H
decompose S1nl,2k as follows:

matrix B.

S2nl,2k (x1 )



≤ n1/2 h1/2 tr S1k a1l a1l S1k vnlk (x1 )sp

S1nl,2k (x1 )
n

= n−1/2 h1/2

∗ κ
ṗ (Uli ) p κ1
Kix1 c H −1 X1i

= n1/2 h1/2 S1k a1l 2sp vnlk (x1 )sp

i=1



× Z1k,i S1k Q−1
n,P P ξnl
1/2 1/2
= n1/2 h1/2 ψnkl (x1 ) S1k Q−1
h ψnkl (x1 ) S1k
P P ξnl + n
 −1

−1
× Qn,P P − QP P ξnl

Downloaded by [McMaster University] at 10:23 06 March 2015

≡ S1nl,2k1 (x1 ) + S1nl,2k2 (x1 ) , say.
∗
(x1 )ṗ κ (Uli )p κ1 (Z1k,i ) . Let
where ψnkl (x1 ) ≡ n−1 ni=1 Kix1 c H −1 X1i
ψ̄kl (x1 ) ≡ E[ψnkl (x1 )]. As in the analysis of ηnl (x1 ), we can show that
supx1 ∈X1 ψ̄kl (x1 )sp = OP (ς1κ ) and supx1 ∈X1 ψnkl (x1 ) − ψ̄kl (x1 )sp ≡
OP ((κ1 κ log n/n)−1/2 ). It follows that supx1 ∈X1 ψnkl (x1 )sp = OP (ς1κ
+(κ1 κ log n/n)−1/2 ) = OP (ς1κ ) under Assumption A.5(i). Then following the analysis of Bnl,1 (x1 ) in the proof of Theorem 3.2,
we can show that S1nl,2k1 (x1 ) = OP (h1/2 ς1κ ) uniformly in x1 . In
addition,



sup S1nl,2k2 (x1 )

x1 ∈X1



−1 
sup ψnkl (x1 )sp S1k sp Q−1
n,P P − QP P sp ξnl 

≤ n1/2 h1/2
=n

1/2 1/2

h

x1 ∈X1

≤ n1/2 h1/2 S1k 2sp a1l 2sp vnlk (x1 )sp




= n1/2 h1/2 O (1) OP κ1 n−1 OP (1) = OP κ1 n−1/2 h1/2
uniformly in x1 .
It follows that supx1 ∈X1 S2nl,2 (x1 ) = OP (κ1 n−1/2 h1/2 ). Similarly, uniformly in x1

S2nl,4 (x1 )
≤ n1/2 h1/2 S1k 2sp a2l 2sp vnlk (x1 )sp
−2γ

= n1/2 h1/2 O (1) OP (κ1



−2γ
)OP (1) = OP n1/2 κ1 h1/2 .

By the same token, S2nl,3 (x1 ) = OP (κ1 n−1/2 h1/2 ) and S2nl,5 (x1 ) =
−2γ
OP (n1/2 κ1 h1/2 ) uniformly in x1 . Consequently, supx1 ∈X1 S2nl (x1 ) =
2
1/2 1/2
).

n h OP (ν1n

1/2
OP (ς1κ ) O (1) OP (κ1 n−1/2 )OP (κ1 n−1/2 )

Proof of Theorem 3.1. (i) Noting that Yi = ḡ(Xi , Z1i , Ui ) + ei =
i β + ei + [ḡ(Xi , Z1i , Ui ) − 
i β], we have


= OP (ς1κ κ1 n−1/2 h1/2 ).
3/2

It
follows
that
supx1 ∈X1 S1nl,2k (x1 ) = OP (h1/2 ς1κ ) +
3/2 −1/2 1/2
1/2
h ) = OP (h ς1κ ) under Assumption A.5(i).
OP (ς1κ κ1 n
Analogously,


sup S1nl,4 (x1 )


β −β
n

−1
−1
=Q
n, n

x1 ∈X1

−1
−1
 i Yi − β = Q

n, n

i=1
n

d1

n1/2 h1/2 sup ψnkl (x1 ) S1k sp a2l 

≤

x1 ∈X1

k=1

=n

1/2 1/2

h

×

−1
−1
i ei + Q

n, n

i=1

−γ
OP (ς1κ )OP (κ1 )

= OP (n

1/2 1/2

h

−γ
ς1κ κ1 ).

By the same token, we can show that supx1 ∈X1 S1nl,3 (x1 ) =
−γ
OP (h1/2 ς1κ ) and supx1 ∈X1 ||S1nl,5 (x1 )|| = OP (n1/2 h1/2 ς1κ κ1 ). It fol1/2
1/2 −γ
lows that supx1 ∈X1 S1n (x1 ) = h ς1κ OP (1 + n κ1 ).
(ii) By Equation C.2 in the supplementary appendix and Cauchy–
Schwarz
inequality,
S2nl (x1 ) ≤ 5 5s=1 n−1/2 h1/2 ni=1 Kix1 u2sl,i

  −1 ∗
c H X (x1 ) ≡ 5 5 S2nl,s , say. It is easy to show that
1i
s=1
supx1 ∈X1 S2nl,1 (x1 ) = OP (n−1/2 h1/2 ). Note that S2nl,2 (x1 ) = n−1/2 h1/2

  −1 ∗
n
1

X1i (x1 ) u22l,i ≤ d1 dk=1
S2nl,2k (x1 ), where
i=1 Kix1 c H

n



i ḡ (Xi , Z1i , Ui ) − 
i β


i=1

−1
−1
−1 −1
=Q
n, ξn + Qn, ζn + Qn, n

n



i  β
i i − 

i=1
−1
−1
+Q
n, n

n



i − i ei


i=1
−1
−1
+Q
n, n

n




i − i ḡ (Xi , Z1i , Ui ) − i β


i=1
−1
−1
−Q
n, n

n




i − i 
i − i  β


i=1
n

S2nl,2k (x1 ) = n−1/2 h1/2





∗
(x1 ) p κ1 Z1k,i
Kix1 c H −1 X1i

i=1

× S1k a1l a1l S1k p κ1



Z1k,i


= n1/2 h1/2 tr S1k a1l a1l S1k vnlk (x1 )




∗  κ1
p (Z1k,i )p κ1 (Z1k,i ) . As in
and vnlk (x1 ) ≡ n−1 ni=1 Kix1 c H −1 X1i
the analysis of ηnl (x1 ), we can show that supx1 ∈X1 vnlk (x1 )sp =
OP (1). By the fact tr(AB) ≤ λmax (A)tr(B) and Bsp = λmax (B)
for any symmetric matrix A and conformable positive-semidefinite

≡ b1n + b2n + b3n + b4n + b5n − b6n , say.
−1
−1
where r1n = (Q
Note that b1n = Q−1
 ξn + r1n ,
n, − Q )ξn
−1
−1
1/2

satisfies r1n  ≤ Qn, − Q sp ×ξn sp = OP [(κ ς0κ ν1n +
2
)κ 1/2 n−1/2 ] by Lemmas B.4(iv) and B.5(i). Similarly,
ς0κ ς2κ ν1n
−1
−1
b2n = Q−1
 ζn + r2n , where r2n = (Qn, − Q )ζn satisfies r2n  ≤
−1
−1
2
1/2
n, − Q ζn sp = OP [(κ ς0κ ν1n + ς0κ ς2κ ν1n
)κ −γ ] by LemQ
n
−1 −1
mas B.4(iv) and B.5(ii). Next, note that b3n = Q n
i=1 i (i −
n
−1
−1

−1

n, − Q )n
 i ) β + (Q



(
−

)
β
≡
b
+ b3n,2 . We
i
i
i
3n,1
i=1

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors

pansion and triangle inequality that

further decompose b3n,1 as follows:

b3n,1 =



dx

n
−1
−Q−1
 n

i
i=1

dx

i ṗ

l=1

i=1

dx

n

l=1



li − p (Uli ) β d +d +l
U
x
1

Uli†

κ



dx

≤
l=1

li
i ġdx +d1 +l (Uli ) Uli − U

=
l=1

×

n
−1
Q−1
 n

+
l=1





× ġdx +d1 +l Uli† − ġdx +d1 +l (Uli )
dx

li
Uli − U



≤
l=1

l=1

× ṗ κ Uli†





li − Uli 
β dx +d1 +l U



 −1  
Q
n,  
n−1
sp 

i

β dx +d1 +l − ġdx +d1 +l



× p κ Uli†



 

li
Uli†
Uli − U



dx
dx

dx

b3n,11l +

≡
l=1

≡

dx

b3n,12l +
l=1

b3n,13l , say,
l=1



b3n,13l 
 −1



≤ O κ −γ Q−1
 sp n



li 
i  Uli − U

n

i=1






−1

≤ O κ −γ Q−1
 sp n

1/2

n

i 2
i=1

×

n

n−1



li
Uli − U

2

1/2

i=1





= O κ −γ O (1) O κ 1/2 OP (ν1n ) = κ −γ +1/2 OP (ν1n ) .
By Lemma B.5(iii), b3n,11l  = OP (ν1n ) which dominates
−1
−1
both b3n,12l  and b3n,13l . Thus b3n,2  ≤ Q
n, − Q sp
2
1/2
OP (ν1n ) = OP [(κ ς0κ ν1n + ς0κ ς2κ ν1n )ν1n ].
It
follows
that
n
x
−1
li ) + b̄3n ,
b3n = dl=1
Q−1
× ġdx +d1 +l (Uli )(Uli − U
 n
i=1 i
2
)ν1n ]. By Lemmas
where b̄3n  = OP [(κ 1/2 ς0κ ν1n + ς0κ ς2κ ν1n
−1/2
ς1κ ν1n ) and
B.4(v)-(vi), b4n  = OP (n
b5n 



n
 −1  



 −1






≤ Qn, sp n
i − i ḡ (Xi , Z1i , Ui ) − i β 


i=1


= OP κ −γ ς1κ ν1n ,
−1
Q
n, sp

−1
Q
n,

Q−1
 sp

≤
−
+
where we use the fact that

=
o
(1)
+
O(1)
=
O
(1).
For
b
,
we
have
by
Taylor
exQ−1
sp
P
P
6n


n



 

i − i ġdx +d1 +l Uli†


i=1

dx

l=1


 −1  
Q
n,  
n−1
sp 


n

i=1

 
†

β dx +d1 +l − ġdx +d1 +l Uli



i − i





li − Uli 
U


dx

b6nl,1 +

b6nl,2 , say.

l=1

li and Uli . Noting that
lies between U
where Uli†
li − Uli |
|ġdx +d1 +l (Uli† ) − ġdx +d1 +l (Uli )| ≤ cġ |U
where
cġ =
max1≤l≤dx maxul ∈Ul ġdx +d1 +l (ul ) = O(1) by Assumptions A.1(ii) and
2
li )2 = ς0κ OP (ν1n
A2(iii), b3n,12l  ≤ cġ n−1 ni=1 i (Uli − U
) by
Lemma B.3(i). By Assumption A.2(ii), Cauchy–Schwarz inequality
and Lemma B.3(i)



i=1


li − Uli  +
× U

i=1







i − i p κ


n



n
−1
Q−1
 n

+

Uli†

dx



i=1



 −1 −1
Qn, n




i
i=1



i − i


n



dx





 −1 −1
Qn, n




 
li − p κ (Uli )  β d +d +l 
× pκ U

x
1



li
β dx +d1 +l Uli − U

i=1
dx

Downloaded by [McMaster University] at 10:23 06 March 2015

b6n 



κ



−1
Q−1
 n

=





n

=

p

κ

l=1

−1
Q−1
 n

571

l=1

By the triangle inequality, Lemmas B.3(i) and B.4(i),

1/2
n
 −1 

2
−1



i − i 
b6nl,1 ≤ cġ Q
n
n, sp

i=1


×

n
−1

n



li − Uli
U

2

1/2

i=1



2
.
= OP (1)OP (ς1κ ν1n ) OP (ν1n ) = OP ς1κ ν1n
2
) by AssumpSimilarly, we can show that b6nl,2 = κ −γ OP (ς1κ ν1n
tion A.2(v) and Lemmas B.3(i) and B.4(i). It follows that b6n  =
2
). Combining the above results yield the conclusion in (i).
OP (ς1κ ν1n
−1
1/2
/n1/2 ) and
(ii) Noting that Q−1
 ξn  ≤ Q sp ξn  = OP (κ
−1
−1
||Q ζn || ≤ Q sp ×ζn  = OP (κ −γ ) by Lemmas B.5(i)–(ii), the
result in part (ii) follows from part (i), Lemma B.4 and the fact that
Rn,β  = OP (ν1n ) under Assumption A.5(i)

ḡ(w)− ḡ(w)| =
(iii) By (ii) and Assumptions A.2(v), supw∈W 

supw∈W |(w) (
β − β) + [β  (w)
−
ḡ(w)]|
≤
sup
w∈W (w)β −




β + supw∈W β (w) − ḡ(w) = OP [ς0κ (νn + ν1n )] as the second

term is O(νn ).

Proof of Theorem 3.2. Let Y1i ≡ Yi − μ − g2 (X2i ) − · · · −
−gdx +1 (Z11,i ) − · · · − gdx +d1 (Z1d1 ,i ) − gdx +d1 +1 (U1i ) −
gdx (Xdx i )
· · · − g2dx +d1 (Udx i ) and Y1 ≡ (Y11 , . . . , Y1n ) . Using the notation
defined at the end of Section 2.2, we have
H
γ1 (x1 )

−1
= H −1 X1 (x1 ) Kx1 X1 (x1 ) H −1
× H −1 X1 (x1 ) Kx1 X1 (x1 ) Y1

−1
+ H −1 X1 (x1 ) Kx1 X1 (x1 ) H −1
× H −1 X1 (x1 ) Kx1 (
Y1 − Y1 )
≡ J1n (x1 ) + J2n (x1 ) , say.
By standard results in local-linear regressions (e.g., Masry
1996 and Hansen (2008)], n−1 H −1 ×X1 (x1 ) Kx1 X1 (x1 )H −1 =
1
0
fX1 (x1 )(  2
) + oP (1) uniformly in x1 , n1/2 h1/2
0 u K(u)du

572

Journal of Business & Economic Statistics, October 2014
D

×[J1n (x1 ) − b1 (x1 )] → N (0, 1 (x1 ))
and
supx1 ∈X1 J1n (x1 ) =
OP ((nh/ log n)−1/2 + h2 ), where b1 (x1 ) and 1 (x1 ) are defined
in Theorem 3.2. It suffices to prove the theorem by showing that
Y1 − Y1 ) = oP (1) uniformly in x1 (for part
n−1/2 h1/2 H −1 X1 (x1 ) Kx1 (
(i) of Theorem 3.2 we only need the pointwise result to hold).
We make the following decomposition: (n/ h)−1/2 H −1
∗
1i )
X1 (x1 )Kx1 (Y1 − 
Y1 ) = n−1/2 h1/2 ni=1
Kix1 H −1 X1i
(x1 )(Y1i − Y
dx
d1
dx
= An (x1 ) + l=2 Bnl (x1 ) + j =1 Cnj (x1 ) + l=1 Dnl (x1 ), where
An (x1 ) =

√



we have Bnl,11 (x1 ) = OP (h1/2 ) for each x1 ∈ X1 . Let

Then we can write η̄l (x1 ) Sl Q−1
η̆l (x1 ) ≡ Q−1
 Sl η̄l (x1 ).
 ξn
as
n−1 ni=1 η̆l (x1 ) i ei .
Noting
that
E[η̆l (x1 ) i ei ] =
0
and
E[η̆l (x1 ) i ei ]2
= η̆l (x1 ) E(i i ei2 )η̆l (x1 )
≤
−1 2
λmax (Q,e )Q sp supx1 ∈X1 η̄l (x1 ) = O(1), we can readily
divide Xl into intervals of appropriate length and apply Bernstein
−1/2
).
inequality to show that η̄l (x1 ) Sl Q−1
 ξn = OP ((n/ log n)
Consequently, supx1 ∈X1 Bnl,11 (x1 ) = n1/2 h1/2 OP ((n/ log n)−1/2 ) =
OP ((h/ log n)−1/2 ) = oP (1). For Bnl,12 (x1 ), we have by Lemma B.5(i)

n

n (
μ − μ) n−1 h1/2



sup Bnl,12 (x1 )

∗
(x1 ) ,
Kix1 H −1 X1i
i=1

x1 ∈X1






≤ n1/2 h1/2 sup rηl (x1 ) Sl sp Q−1
 sp ξn 

n

Bnl (x1 ) = n−1/2 h1/2

∗
(x1 ) [
gl (Xli ) − gl (Xli )] ,
Kix1 H −1 X1i

x1 ∈X1

i=1

=n

1/2 1/2

n
−1/2 1/2

Cnj (x1 ) = n

h

Kix1 H

−1

∗
X1i

h

OP (κ 1/2 (nh/ log n)−1/2 )O (1) OP (1) OP (κ 1/2 n−1/2 )

= OP (κ(n/ log n)−1/2 ) = oP (1) .

(x1 )

i=1

Downloaded by [McMaster University] at 10:23 06 March 2015






× 
gdx +j Z1j,i − gdx +j Z1j,i ,
n

Dnl (x1 ) = n−1/2 h1/2

∗
(x1 )
Kix1 H −1 X1i



It follows that supx1 ∈X1 Bnl,1 (x1 ) = oP (1). By Lemma B.5(ii) and Assumptions A2(ii) and (v) and A5,


sup Bnl,2 (x1 )

i=1



li ) − gdx +d1 +l (Uli ) .
× 
gdx +d1 +l (U

x1 ∈X1




≤ n1/2 h1/2 sup ηnl (x1 ) Q−1
 sp Sl sp ζn 

We prove the first part of the theorem by showing that (i1) An (x1 ) =
oP (1), (i2) Bnl (x1 ) = oP (1) for l = 2, . . . , dx , (i3) Cnj (x1 ) = oP (1) for
j = 1, . . . , d1 and (i4) Dnl (x1 ) = oP (1) for l = 1, . . . , dx ,all uniformly
in x1 .
√
n(
μ − μ) = OP (1) and
(i1) holds by noticing that
n
−1
−1 ∗
X1i (x1 ) = OP (1) uniformly in x1 . Let c ≡ (c1 , c2 )
n
i=1 Kix1 H
be an arbitrary 2 × 1 nonrandom vector such that c = 1. Recall that
∗
(x1 )p κ (Xli ). For (i2), we make the
ηnl (x1 ) ≡ n−1 ni=1 Kix c H −1 X1i
following decomposition

x1 ∈X1

= n h OP (1) O (1) O(1)OP (κ −γ ) = oP (1) ,


sup Bnl,4 (x1 )
1/2 1/2

x1 ∈X1



≤ n1/2 h1/2 sup ηnl (x1 ) Sl sp Rn,β 
x1 ∈X1



= n1/2 h1/2 OP (1) O (1) oP n−1/2 h−1/2 = oP (1) ,

c Bnl (x1 )
n

= n−1/2 h1/2



∗
(x1 ) p κ (Xli ) Sl 
β −β
Kix1 c H −1 X1i

and

i=1
n

+ n−1/2 h1/2



∗
(x1 ) p κ (Xli ) Sl β − gl (Xli )
Kix1 c H −1 X1i

i=1

=n

1/2 1/2

h

1/2 1/2
ηnl (x1 ) Sl Q−1
h ηnl (x1 ) Sl Q−1
 ξn + n
 ζn
dx

n

− n−1/2 h1/2 ηnl (x1 ) Sl Q−1


j
j =1



kj − Ukj
δkj U

k=1

+ n−1/2 h1/2 ηnl (x1 ) Sl Rn,β
n

+ n−1/2 h1/2



∗
(x1 ) p κ (Xli ) Sl β − gl (Xli )
Kix1 c H −1 X1i

i=1

≡ Bnl,1 (x1 ) + Bnl,2 (x1 ) − Bnl,3 (x1 ) + Bnl,4 (x1 ) + Bnl,5 (x1 ) ,
where recall δkj ≡ ṗ κ (Ukj ) β dx +d1 +k , ξn ≡ n−1 nj=1 j ej , and ζn ≡
n−1 nj=1 j [ḡ(Xj , Z1j , Uj ) −β  j ]. Let η̄l (x1 ) ≡ E[ηnl (x1 )] and
rηl (x1 ) = ηnl (x1 ) − η̄l (x1 ). By the proof of Lemma B.6(i), rηl (x1 ) =
OP (κ 1/2 (nh/ log n)−1/2 ), η̄l (x1 ) = OP (1 + κ 1/2 h), and ηnl (x1 ) =
OP (1) uniformly in x1 . Write Bnl,1 (x1 ) = n1/2 h1/2 η̄l (x1 ) Sl Q−1
 ξn +
n1/2 h1/2 rηl (x1 ) Sl Q−1
 ξn ≡ Bnl,11 (x1 ) + Bnl,12 (x1 ), say. Noting that
 2

(x1 )
E Bnl,11
 −1 

 2
= hη̄l (x1 ) Sl Q−1
 E j j ej P Sl η̄l (x1 )
 



≤ hλmax E j j ej2 [λmin (P )]−2 λmax Sl Sl η̄l (x1 )2
= hO (1) OP (1) OP (1) = OP (h) ,



sup Bnl,5 (x1 )

x1 ∈X1



≤ O κ −γ n1/2 h1/2 sup n−1
x1 ∈X1



= OP n

1/2 1/2 −γ

h

κ



n



∗
(x1 )
Kix1 c H −1 X1i

i=1

= oP (1) .

x
For Bnl,3 (x1 ), we have Bnl,3 (x1 ) = dk=1
Bnl,3k (x1 ) where
n
−1
−1/2 1/2

kj − Ukj ).
h ηnl (x1 ) × Sl Q j =1 j δkj (U
Bnl,3k (x1 ) = n
Using Equation C.2 in the supplementary appendix,
n
Bnl,3k (x1 ) = − 5s=1 n−1/2 h1/2 ηnl (x1 ) Sl Q−1
j δkj usk,j ≡

j =1
5
− s=1 Bnl,3ks (x1 ), say. First, noting that δkj is uniformly bounded,
we can show n−1 nj=1 j δkj sp = OP (1) using arguments similar
to those used in the proof of Lemma B.5(iii). It follows that



sup Bnl,3k1 (x1 )

x1 ∈X1

≤h

1/2



 −1   −1
sup ηnl (x1 ) Sl sp Q sp 
n
x1 ∈X1


×n

1/2

=h

1/2




j δkj 


j =1
n

sp

|μk − 
μk |

OP (1) O (1) O (1) OP (1) O (1) OP (1) = oP (1) .

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors




 (1)
(2)
It follows that supx1 ∈X1 Bnl,3k2
(x1 ) = oP (1). For Bnl,3k2
(x1 ), we have

(1)
(2)
Now notice that Bnl,3k2 (x1 ) = Bnl,3k2
(x1 ) + Bnl,3k2
(x1 ), where




 (2)
(x1 )
sup Bnl,3k2

(1)
(x1 )
Bnl,3k2
d1

n

−1/2 1/2

h

η̄l (x1 )



x1 ∈X1






≤ n1/2 h1/2 sup rηl (x1 ) Sl sp Q−1
 sp



δkj j p Z1m,j S1m a1k ,

n

=

Sl Q−1


κ1

x1 ∈X1

j =1

m=1

d1

(2)
(x1 )
Bnl,3k2
d1

tnkm sp S1m  sp a1k 

×


δkj j p κ1 Z1m,j S1m a1k .

n

n−1/2 h1/2 rηl (x1 ) Sl Q−1


=

573

m=1



= n1/2 h1/2 OP (κ 1/2 log n/n)−1/2 O (1) OP (1) O

j =1

m=1

Downloaded by [McMaster University] at 10:23 06 March 2015

1/2
× (1) OP (κ1 n−1/2 ) = oP (1) ,
n
−1
κ1
Let
ϕnlkm (x1 ) = η̄l (x1 ) Sl Q−1
 n
j =1 δkj j p (Z1m,j )
and ϕ̄lkm (x1 ) = E[ϕnlkm (x1 )]. Arguments like those used
to study ηnl (x1 ) in the proof of Lemma B.6(i) show
= O(η̄l (x1 )) = O(1 + κ 1/2 h) = O(1)
unthat
ϕ̄lkm (x1 )
der Assumption A.5(ii) and ||ϕnlkm (x1 )− E[ϕnlkm (x1 )]||
= η̄l (x1 )OP ((κ 1/2 log n/n)−1/2 ) = OP ((κ 1/2 log n/n)−1/2 ) uniformly in x1 . We further make the following decomposition:
n
(1)
1
κ1

Bnl,3k2
(x1 ) = dm=1
n−1/2 h1/2 η̄l (x1 ) Sl Q−1

j =1 δkj j p (Z1m,j )
(1,j )
3
−1
S1m Qn,P P ξnk = j =1 Bnl,3k2 (x1 ), where

where tnkm ≡ n−1 nj=1 δkj j p κ1 (Z1m,j ) , we use the fact that
tnkm sp = OP (1) by following similar arguments to those used in the
proof of Lemma B.5(iii) and noticing that δkj is uniformly bounded.
Consequently we have shown that supx1 ∈X1 |Bnl,3k2 (x1 )| = oP (1). Analogously,


sup Bnl,3k4 (x1 )

x1 ∈X1




≤ n1/2 h1/2 sup ηnl (x1 )sp Sl sp Q−1
 sp
x1 ∈X1

d1
(1,1)
Bnl,3k2

d1

n1/2 h1/2 ϕ̄lkm (x1 ) S1m Q−1
P P ξnk ,

(x1 ) =

tnkm sp S1m sp a2k 

×

m=1

m=1

d1

 −γ 
= n1/2 h1/2 OP (1)O (1) OP (1) OP (1) O (1) OP κ1

−1
n1/2 h1/2 ϕ̄lkm (x1 ) S1m (Q−1
n,P P − QP P )ξnk ,

(1,2)
(x1 ) =
Bnl,3k2

= oP (1) .

m=1
d1

n1/2 h1/2 rnlkm (x1 ) S1m Q−1
n,P P ξnk .

(1,3)
(x1 ) =
Bnl,3k2
m=1

Following
of Bnl,11 (x1 ), we can

 the analysis

 (1,1)
supx1 ∈X1 Bnl,3k2
(x1 ) = OP ((h/ log n)1/2 ). In addition,

show

that




 (1,2)
(x1 )
sup Bnl,3k2

x1 ∈X1

By the same token, we can show that Bnl,3k3 (x1 ) = oP (1)
and Bnl,3k (x1 ) = oP (1) uniformly in x1 . It follows that
supx1 ∈X1 Bnl,3k (x1 ) = oP (1) for k = 1, . . . , dx . Analogously,
we can show that (i3): supx1 ∈X1 Cnj (x1 ) = oP (1) for j = 1, . . . , d1 .
Now
we
show
(i4).
Observe
that
c Dnl (x1 ) =
n
−1/2 1/2
 −1 ∗

li )]
h
X1i (x1 )[
gdx +d1 +l (Uli ) −gdx +d1 +l
(U
n
i=1 Kix1 c H
n
−1/2 1/2
 −1 ∗

h
X1i (x1 ) [gdx +d1 +l (Uli ) − gdx +d1 +k (Uli )] ≡
+n
i=1 Kix1 c H
Dnl,1 (x1 ) + Dnl,2 (x1 ), say. In view of the fact that
li ) − gdx +d1 +l (U
li ) = p κ (U
li ) Sdx + d1 +k (
li )
β − β) + [p κ (U

gdx +d1 +l (U
3

β l −gdx +d1 +l (Uli )], we have Dnl,1 (x1 ) = j =1 Dnl,1j (x1 ), where

d1

≤ n1/2 h1/2 sup

x1 ∈X1 m=1

ϕ̄lkm (x1 ) S1m sp



−1 
× Q−1
n,P P − QP P sp ξnk 


1/2
= n1/2 h1/2 OP (1)O (1) OP κ1 n−1/2 OP (κ1 n−1/2 ) = oP (1) ,

Dnl,11 (x1 ) = n−1/2 h1/2
n

×
i=1

Dnl,12 (x1 ) = n−1/2 h1/2
n

×

and



∗
(x1 ) p κ (Uli ) Sdx +d1 +l 
Kix1 c H −1 X1i
β −β ,



∗
li ) − p κ (Uli ) 
(x1 ) p κ (U
Kix1 c H −1 X1i

i=1



β −β ,
× Sdx +d1 +l 




 (1,3)
(x1 )
sup Bnl,3k2

Dnl,13 (x1 ) = −n−1/2 h1/2

x1 ∈X1

d1

≤ n1/2 h1/2 sup

x1 ∈X1 m=1

=n

1/2 1/2

1/2




rnlkm (x1 ) S1m sp Q−1
n,P P sp ξnk  sp

h OP ((κ log n/n)


1/2
× κ1 n−1/2 = oP (1) .

n

×



∗
li ) − p κ (U
li ) β l .
(x1 ) gdx +d1 +l (U
Kix1 c H −1 X1i

i=1
−1/2

)O (1) OP (1) OP
Analogous to the analysis of Bnl,1 (x1 ), we can readily
show that supx1 ∈X1 Dnl,11 (x1 ) = oP (1). For Dnl,12 (x1 ), by Taylor

574

Journal of Business & Economic Statistics, October 2014

Econometrics (Bolton, NY), the annual meeting of the Midwest Econometric Group (Bloomington, IN), and the Western
Economic Association International annual conference (Seattle,
WA). L. Su acknowledges support from the Singapore Ministry
of Education for Academic Research Fund under grant number MOE2012-T2-2-021. The R code used in the article can be
obtained from the authors upon request.

expansion,
Dnl,12 (x1 )
n

∗
li − Uli )ṗ κ
(x1 ) (U
Kix1 c H −1 X1i

= n−1/2 h1/2
i=1



× (Uli ) 
β l −β l


+

n

1 −1/2 1/2
h
n
2

∗
li − Uli )2 p̈ κ
(x1 ) (U
Kix1 c H −1 X1i

i=1

  

β l −β l
× Uli‡ 

REFERENCES

1
≡ Dnl,121 (x1 ) + Dnl,122 (x1 ) , say,
2

Downloaded by [McMaster University] at 10:23 06 March 2015

li and Uli . By Theorem 3.1 and Lemmas B.6(i)where Uli‡ liesbetween U

−γ
(ii), supx1 ∈X1 Dnl,121 (x1 ) = h1/2 ς1κ OP (1 + n1/2 κ1 )OP (νn + ν1n ) =
oP (1) and


sup Dnl,122 (x1 )
x1 ∈X1



≤ ς2κ sup



n

n

−1/2 1/2



h

x1 ∈X1

Kix1 c H

−1

∗
X1i

li − Uli )2
(x1 ) (U

i=1



× 
β l −β l 

−2γ

= ς2κ n1/2 h1/2 OP (κ1 n−1 + κ1

)OP (νn + ν1n ) = oP (1) .

In
addition,
supx1 ∈X1 Dnl,13 (x1 ) ≤ n1/2 h1/2 O(κ −γ ) supx1 ∈X1
∗
1/2 1/2 −γ
n−1 ni=1 Kix1 H −1 X1i
(x1 ) = OP (n h κ ) = oP (1). It follows
that supx1 ∈X1 Dnl,1 (x1 ) = oP (1).
By Taylor expansion,
Dnl,2 (x1 )


∗
li − Uli
(x1 ) ġ (Uli ) U
Kix1 c H −1 X1i

n

= n−1/2 h1/2
i=1

n

+ n−1/2 h1/2

[Received June 2013. Revised April 2014.]

∗
(x1 ) g̈dx +d1 +l (Uli‡ )
Kix1 c H −1 X1i

i=1



li − Uli 2
× U
≡ Dnl,21 (x1 ) + Dnl,22 (x1 ) .
Arguments like
those used to study Bnl,3 (x1 ) show

Dnl,21 (x1 )
= oP (1).
that
sup
By
Lemma
B.6(ii),
x
∈X
 1 1 
cg̈ supx1 ∈X1 {n−1/2 h1/2 ni=1 Kix1
supx1 ∈X1 Dnl,22 (x2 ) ≤

  −1 ∗
2
c H X (x1 ) (U
li − Uki )2 } = n1/2 h1/2 OP (ν1n
) = oP (1), where
1i

cg̈ = supul ∈Ul g̈dx +d1 +l (ul ) = O(1).

SUPPLEMENTARY MATERIALS
The supplementary appendix gives the proofs of Lemmas
B.1–B.5 in Appendix B.
ACKNOWLEDGMENTS
The authors thank two anonymous referees, the Joint Editors Rong Chen, Peter Brummund, David Jacho-Chavez, Chris
Parmeter, and Anton Schick for useful comments and suggestions. They also thank participants in talks given at the State University of New York at Albany, the University of Alabama, the
University of North Carolina at Greensboro, New York Camp

Ai, C., and Chen, X. (2003), “Efficient Estimation of Models With Conditional
Moment Restrictions Containing Unknown Functions,” Econometrica, 71,
1795–1843. [555]
Bernal, R. (2008), “The Effect of Maternal Employment and Child Care on
Children’s Cognitive Development,” International Economic Review, 49,
1173–1209. [562]
Bernal, R., and Keane, M. P. (2011), “Child Care Choices and Children’s Cognitive Achievement: The Case of Single Mothers,” Journal of Labor Economics, 29, 459–512. [556,562,563,567]
Blau, F. D., and Grossberg, A. J. (1992), “Maternal Labor Supply and Children’s
Cognitive Development,” Review of Economics and Statistics, 74, 474–481.
[562]
Cameron, S. V., and Heckman, J. J. (1998), “Life Cycle Schooling and Dynamic
Selection Bias: Models and Evidence for Five Cohorts,” NBER Working
Paper6385, National Bureau of Economic Research, Inc. [562]
Chen, X. (2007), “Large Sample Sieve Estimation of Semi-Nonparametric
Models,” in Handbook of Econometrics (Vol. 6B), eds. J. J. Heckman
and E. Leamer, New York: Elsevier Science, (chap. 76), pp. 5549–5632.
[568]
Chen, X., and Pouzo, D. (2012), “Estimation of Nonparametric Conditional
Moment Models With Possibly Nonsmooth Generalized Residuals,” Econometrica, 80, 277–321. [555]
Darolles, S., Fan, Y., Florens, J. P., and Renault, E. (2011), “Nonparametric
Instrumental Regression,” Econometrica, 79, 1541–1565. [555]
Fan, J., and Gijbels, I. (1996), Local Polynomial Modelling and Its Applications,
London: Chapman & Hall. [569]
Gao, J., and Phillips, P. C. B. (2013), “Semiparametric Estimation in Triangular
System Equations With Nonstationarity,” Journal of Econometrics, 176,
59–79. [555]
Hahn, J., and Ridder, G. (2013), “Asymptotic Variance of Semiparametric Estimators With Generated Regressors,” Econometrica, 81, 315–340. [558]
Hall, P., and Horowitz, J. L. (2005), “Nonparametric Methods for Inference
in the Presence of Instrumental Variables,” The Annals of Statistics, 33,
2904–2929. [555]
Hansen, B. E. (2008), “Uniform Convergence Rates for Kernel Estimation With
Dependent Data,” Econometric Theory, 24, 726–748. [569,571]
Henderson, D. J., and Parmeter, C. F. (2014), Applied Nonparametric Econometrics, New York: Cambridge University Press. [555]
Horowitz, J. L. (2014), “Nonparametric Additive Models,” in The Oxford Handbook of Applied Nonparametric and Semiparametric Econometrics and
Statistics, eds. J. Racine, L. Su, and A. Ullah, Oxford: Oxford University
Press, pp. 129–148. [555,557]
Horowitz, J. L., and Mammen, E. (2004), “Nonparametric Estimation of an
Additive Model With a Link Function,” The Annals of Statistics, 32, 2412–
2443. [557,568,569]
James-Burdumy, S. (2005), “The Effect of Maternal Labor Force Participation on Child Development,” Journal of Labor Economics, 23, 177–211.
[562]
Keane, M. P., and Wolpin, K. I. (1997), “The Career Decisions of Young Men,”
Journal of Political Economy, 105, 473–522. [562]
Keane, M. P., and Wolpin, K. I. (2001a), “The Effect of Parental Transfers and
Borrowing Constraints on Educational Attainment,” International Economic
Review, 42, 1051–1103. [562]
——— (2001b), “Estimating Welfare Effects Consistent With Forward-Looking
Behavior,” Journal of Human Resources, 37, 600–622. [562]
Kim, W., Linton, O. B., and Hengartner, N. W. (1999), “A Computationally
Efficient Estimator for Additive Nonparametric Regression With Bootstrap
Confidence Intervals,” Journal of Computational and Graphical Statistics,
8, 278–297. [557,561]
Li, Q. (2000), “Efficient Estimation of Additive Partially Linear Models,” International Economic Review, 41, 1073–1092. [557,568]

Downloaded by [McMaster University] at 10:23 06 March 2015

Ozabaci, Henderson, and Su: Additive Nonparametric Regression in the Presence of Endogenous Regressors
Li, Q., and Racine, J. (2007), Nonparametric Econometrics: Theory and Practice, Princeton, NJ: Princeton University Press. [555]
Mammen, E., Rothe, C., and Schienle, M. (2012), “Nonparametric Regression
With Nonparametrically Generated Covariates,” The Annals of Statistics,
40, 1132–1170. [558]
Martins-Filho, C., and Yang, K. (2007), “Finite Sample Performance of KernelBased Regression Methods for Nonparametric Additive Models Under Common Bandwidth Selection Criterion,” Journal of Nonparametric Statistics,
19, 23–62. [557]
——— (2012), “Kernel-Based Estimation of Semiparametric Regression in
Triangular Systems,” Economics Letters, 115, 24–27. [555]
Masry, E. (1996), “Multivariate Local Polynomial Regression for Time Series:
Uniform Strong Consistency Rates,” Journal of Time Series Analysis, 17,
571–599. [569,571]
Newey, W. K. (1997), “Convergence Rates and Asymptotic Normality for Series Estimators,” Journal of Econometrics, 79, 147–168.
[568]
Newey, W. K., and Powell, J. L. (2003), “Instrumental Variable Estimation of
Nonparametric Models,” Econometrica, 71, 1565–1578. [555,556]
Newey, W. K., Powell, J. L., and Vella, F. (1999), “Nonparametric Estimation
of Triangular Simultaneous Equation Models,” Econometrica, 67, 565–603.
[555,556]
Ozabaci, D., and Henderson, D. J. (2012), “Gradients via Oracle Estimation
for Additive Nonparametric Regression With Application to Returns to
Schooling,” Working Paper, State University of New York at Binghamton.
[557]

575

Pinkse, J. (2000), “Nonparametric Two-Step Regression Estimation When Regressors and Error are Dependent,” Canadian Journal of Statistics, 28, 289–
300. [555,556]
Powell, M. J. D. (1981), Approximation Theory and Methods, Cambridge: Cambridge University Press. [568]
Roehrig, C. S. (1988), “Conditions for Identification in Nonparametric and
Parametric Models,” Econometrica, 56, 433–447. [555]
Schumaker, L. L. (2007), Spline Functions: Basic Theory (3rd ed.), Cambridge:
Cambridge University Press. [568]
Serfling, R. J. (1980), Approximation Theorems of Mathematical Statistics, New
York: Wiley. [569]
Su, L., and Jin, S. (2012), “Sieve Estimation of Panel Data Models With Cross
Section Dependence,” Journal of Econometrics, 169, 34–47. [568]
Su, L., Murtazashvili, I., and Ullah, A. (2013), “Local Linear GMM Estimation
of Functional Coefficient IV Models With Application to the Estimation of
Rate of Return to Schooling,” Journal of Business & Economic Statistics,
31, 184–207. [555]
Su, L., and Ullah, A. (2008), “Local Polynomial Estimation of Nonparametric
Simultaneous Equations Models,” Journal of Econometrics, 144, 193–218.
[555,556,559,561]
Ullah, A. (1985), “Specification Analysis of Econometric Models,” Journal of
Quantitative Economics, 1, 187–209. [566,567]
Vella, F. (1991), “A Simple Two-Step Estimator for Approximating Unknown
Functional Forms in Models With Endogenous Explanatory Variables,”
Working Paper 239, Department of Economics, Australian National University. [555]

