Learning When-to-Treat Policies

arXiv:1905.09751v3 [stat.ME] 30 Apr 2020

Xinkun Nie
xinkun@stanford.edu

Emma Brunskill
ebrun@cs.stanford.edu

Stefan Wager
swager@stanford.edu
Draft version May 2020
Abstract
Many applied decision-making problems have a dynamic component: The policymaker needs not only to choose whom to treat, but also when to start which treatment.
For example, a medical doctor may choose between postponing treatment (watchful
waiting) and prescribing one of several available treatments during the many visits
from a patient. We develop an “advantage doubly robust” estimator for learning such
dynamic treatment rules using observational data under the assumption of sequential
ignorability. We prove welfare regret bounds that generalize results for doubly robust
learning in the single-step setting, and show promising empirical performance in several different contexts. Our approach is practical for policy optimization, and does not
need any structural (e.g., Markovian) assumptions.

1

Introduction

The promise of personalized data-driven decision-making has led to a surge in interest in
methods that leverage observational data to help inform how and to whom interventions
should be applied (Athey and Wager, 2017; Bertsimas and Kallus, 2020; Dudı́k, Erhan,
Langford, and Li, 2014; Elmachtoub and Grigas, 2017; Kallus and Zhou, 2018; Kitagawa
and Tetenov, 2018; Manski, 2004; Swaminathan and Joachims, 2015; Zhang, Tsiatis, Davidian, Zhang, and Laber, 2012; Zhao, Zeng, Rush, and Kosorok, 2012). Any solution to
this policy learning problem needs to deal with numerous difficulties, including how to incorporate robustness to potential selection bias as well as fairness constraints articulated by
stakeholders, and there have been several notable advances that address these difficulties
over the past few years.
One limitation of this line of work, however, is that the results cited above all focus on a
static setting where a decision-maker only sees each subject once and immediately decides
how to treat the subject. In contrast, many problems of applied interest involve a dynamic
component whereby the decision-maker makes a series of decisions based on time-varying
covariates. In medicine, if a patient has a disease for which all known cures are invasive and
have serious side effects, their doctor may choose to monitor disease progression for some
time before prescribing one of these invasive treatments. As another example, a health
inspector needs to not only choose which restaurants to inspect, but also when to carry out
these inspections.

1

In this paper, we study the problem of learning dynamic when-to-treat policies, where
a decision-maker is only allowed to act once, but gets to choose both which action to take
and when to perform the action.1 This setting covers several application areas that have
recently been discussed in the literature, including when to start antiretroviral therapy
for HIV-positive patients to prevent AIDS while mitigating side effects (When To Start
Consortium, 2009), when to recommend mothers to stop breastfeeding to maximize infants’
health (Moodie, Platt, and Kramer, 2009), and when to to turn off ventilators for intensive
care patients to maximize health outcomes (Prasad et al., 2017).
The available literature has developed several methods for evaluating and learning dynamic treatment rules from prior data, with notable contributions from statistics and epidemiology communities including from Murphy (2003); Robins (2004); Murphy (2005); Luckett et al. (2019); Tsiatis et al. (2019); Zhang et al. (2013, 2018), Van der Laan and Rose
(2018, Chapter 4), and the batch reinforcement learning community such as Jiang and Li
(2016) and Thomas and Brunskill (2016). As discussed further below, these papers develop
general approaches that can be used with arbitrary dynamic treatment rules. Here, in contrast, we seek to exploit special structure of the when-to-treat problem to develop tailored
learning methods with desirable statistical and computational properties.
In developing our approach, we build on recent results on doubly robust static policy
learning (Athey and Wager, 2017; Zhou, Athey, and Wager, 2018), and show how they
can be adapted to our dynamic setting without making any structural (e.g., Markovian)
assumptions and without compromising computational performance. Throughout this paper, we assume sequential ignorability, meaning that any confounders that affect making a
treatment choice at time t have already been measured by time t. Sequential ignorability
is a widely used generalization of the classical ignorability assumption of Rosenbaum and
Rubin (1983) to the dynamic setting (Hernán, Brumback, and Robins, 2001; Murphy, 2003;
Robins, 1986, 2004). We develop methods that can leverage generic machine learning estimators of various nuisance components (e.g., the propensity of starting treatment in any
given state and time) for learning policies with strong utilitarian regret bounds that hold in
a nonparametric setting.
Our problem setting is closely related to batch reinforcement learning (Sutton and Barto,
2018). The types of guarantees we derive, however, are more closely related to results from
the static policy learning literature, in that we use tools from semiparametric statistics to
derive sharp regret bounds given only nonparametric assumptions. To our knowledge, the
reinforcement learning literature has not pursued nor obtained the type of results we achieve
here for off-policy policy learning in a nonparametric setting.
We also note work on optimal stopping motivated by the problem of when to buy or
sell an asset. This setting, however, is different from ours in that most of the literature on
optimal stopping either works with a known probabilistic model (Jacka, 1991; Van Moerbeke,
1976), or assumes that we can observe the price evolution of the asset whether or not we
purchase it (Goel, Dann, and Brunskill, 2017). In contrast, we work in a nonparametric
setting, and adopt a potential outcomes model in which we only get to observe outcomes
corresponding to the sequence of actions we choose to take (Imbens and Rubin, 2015; Robins,
1986). Rust (1987) considers the descriptive problem of fitting an optimal stopping model
to the behavior of a rational agent; this is different from the problem of learning a decision
rule that can be used to guide future decisions in this paper. We will review the related
literature in more detail in Section 3 after first presenting our method below.
1 We note that the policies of interest in this paper also include when-to-stop policies. By flipping the
treatment indicator, it is without loss of generality that we only consider when-to-treat policies.

2

2
2.1

Policy Learning under Sequential Ignorability
Setup and Notation

We work in the following statistical setting. We observe a set of i = 1, . . . , n independent
and identically distributed trajectories generated from some distribution P that describe the
evolution of subjects over T time steps. For each subject i, we observe a vector of states
S (i) ∈ S T and actions A(i) ∈ AT , as well as a final outcome Y (i) ∈ R.2 For each t = 1, . . . , T ,
(i)
(i)
St denotes the state of the subject at time t and At denotes the action taken. We write
the set of possible actions as A = {0, 1, · · · , K}, and let At = 0 denote no action (i.e., no
treatment assignment) at time t. We define the filtration F1 ⊆ F2 ⊆ · · · ⊆ FT +1 , where
Ft = σ (S1:t , A1:t−1 ) contains information available at time t for t = 1, . . . , T , and FT +1 =
σ (S1:T , A1:T , Y ) also has information on the final outcome. For notational convenience,
(i)
(i)
(i)
(i)
we denote St1 :t2 := {St1 , · · · , St2 }, and we similarly define At1 :t2 , and write the relevant

generalization of the propensity score as et,a (s1:t ) = P At = a S1:t = s1:t , A1:(t−1) = 0 .
We formulate causal effects in terms of potential outcomes (Neyman, 1923; Robins, 1986;
Rubin, 1974). For any set of actions a ∈ AT , we posit potential outcomes Y (i) (a1:T ) and
(i)
St (a1:(t−1) ) corresponding to the outcome and state values we would have obtained for
subject i had we assigned treatment sequence a. In order to identify causal effects, we
make the standard assumptions of sequential ignorability, consistency and overlap (Hernán,
Brumback, and Robins, 2001; Murphy, 2003; Robins, 1986, 2004).
Assumption 1 (Consistency of potential outcomes). Our observations are consistent with
(i)
(i)
(i)
(i)
potential outcomes, in the sense that Y (i) = Y (i) (A1:T ) and St = St (A1:(t−1) ).
Assumption 2 (Sequential Ignorability). Actions cannot respond to future information,
i.e., {Y (A1:(t−1) , at:T ), St0 (A1:(t−1) , at:(t0 −1) )}Tt0 =t+1 ⊥
⊥ At Ft for all t = 1, · · · , T .
Assumption 3 (Overlap). There are constants 0 < η, η0 < 1 such that, for all t = 1, · · · , T
and s1:t ∈ S t , the following hold: et,a (s1:t ) > η/T for all a ∈ A\{0} and et,0 (s1:t ) > 1−η0 /T .
A policy π is a function that, for each time t = 1, . . . , T , maps time-t observables to an
action: πt : S t × At−1 → A such that πt is Ft -measurable; then π := {πt }Tt=1 . Recall that
we focus on when-to-treat type rules, meaning that the decision-maker only gets to act once
by starting a non-0 treatment regime at the time of their choice. For example, if K = 3
and T = 5, then the decision-maker may choose for instance to start treatment option #2
at time t = 4, resulting in a trajectory A = (0, 0, 0, 2, 2).
When applying π on-policy, the behavior of π is fully characterized by the time at which
π chooses to act, denoted by τπ = inf {t : πt (·, ·) 6= 0}, and the treatment chosen, denoted
by Wπ = πτπ (·, ·). When π chooses to never initiate treatment, we use the convention that
τπ = T + 1 and Wπ = 0. Note that τπ and Wπ are both Fτπ -measurable.3 For completeness,
we also need to specify how π behaves off-policy, i.e., how π would prescribe treatment
along a trajectory whose past action or treatment sequence may disagree with π; and, in
this paper, we do so by assuming that π is regular in the sense of Definition 1 below.
Definition 1 (Regular policy). A regular when-to-treat policy π is determined by an Ft measurable stopping time τπ and an associated Fτπ -measurable decision variable Wπ ∈
2 We note that it is without loss of generality that we assume the outcome Y (i) is only observed at the end
of a trajectory, since intermediate outcomes/rewards can be incorporated as part of the state representation.
3 See also Athey and Imbens (2018) for a closely related discussion of potential outcomes in the context
of staggered adoption.

3

{1, . . . , K} as follows: For each time t = 1, . . . , T , if At−1 6= 0 then πt (S1:t , A1:(t−1) ) =
At−1 , else if t ≥ τπ then πt (S1:t , A1:(t−1) ) = Wπ , else πt (S1:t , A1:(t−1) ) = 0.
The main substance of Definition 1 is that we assume that if π suggests to start treatment
k at a given moment, it persists in this choice k even if we fail to start treatment immediately.
One notable limitation of this regularity condition is in the setting where some patients may
die or otherwise be unable to receive treatment.4 We discuss extensions of our approach
beyond regular policies in Section 5.
Following Murphy (2005), we let ft (St S1:(t−1) , A1:(t−1) ) be the conditional density for
state transitions at time t. We can write the distribution function for trajectories (s1:T , a1:T )
as
T


Y
 
f (s, a) = f (s1 ) P A1 s1
ft st s1:(t−1) , a1:(t−1) P At = at s1:t , a1:(t−1) ,

(1)

t=2

and denote the expectation with respect to this distribution as E. Similarly, the distribution
of a trajectory under policy π is
f (s, a; π) = f (s1 )1a1 =π1 (s1 )

T
Y

ft st s1:(t−1) , a1:(t−1)



1at =πt (s1:t ,a1:(t−1) ) ,

(2)

t=2

and we use Eπ to denote the expectation with respect to it. Define
Vπ := Eπ [Y ] = E [Y (π1 (S1 ), π2 (S1 , S2 (π1 (S1 )), π1 (S1 )) , . . .)]

(3)

to be the value of the policy π, i.e., the expected outcome Y with actions At chosen according
to π such that At = πt (S1:t , A1:(t−1) ) for all t = 1, . . . , T . We further define the conditional
value function
µπ,t (s1:t , a1:t−1 )


= Eπ Y S1:t = s1:t , A1:t−1 = a1:t−1


= E Y (a1:t−1 , πt (S1:t , A1:t−1 ), πt+1 (. . .), . . .) S1:t = s1:t , A1:t−1 = a1:t−1 ,

(4)

and the Q-function
Qπ,t (s1:t , a1:t )


= Eπ Y S1:t = s1:t , A1:t = a1:t


= E Y (a1:t , πt+1 (S1:t , St+1 (A1:t ), A1:t ), . . .) S1:t = s1:t , A1:t = a1:t .

(5)

For any class Π, we write the optimal value function as V ∗ = supπ∈Π Vπ , and define the
regret of a policy π ∈ Π as R(π) = V ∗ − Vπ (Manski, 2004). Given this setting, our goal
is to learn the best policy from a predefined policy class Π to minimize regret. Our main
result is a method for learning a policy π̂ ∈ Π along with a bound on its regret R(π̂).
4 For example, consider a case where π says we should have started treatment on day 5 but we didn’t
in fact start treatment then, i.e., A5 = 0, and then the patient dies on day 6. Here, realistically, π should
recognize that starting treatment is now impossible and prescribe π6 (S1:6 , A1:5 ) = 0; however, doing so
would be inconsistent with Definition 1.

4

2.2

Existing Methods

In the static setting, a popular approach to policy learning starts by first providing an estimator Vbπ for the value Vπ of each feasible policy π ∈ Π, and then sets π̂ = argmax{Vbπ : π ∈ Π}
(e.g., Athey and Wager, 2017; Kitagawa and Tetenov, 2018; Manski, 2004; Swaminathan
and Joachims, 2015; Zhang, Tsiatis, Davidian, Zhang, and Laber, 2012). At a high level
our goal is to pursue the same strategy, but now in a dynamic setting. The challenge is
then to find a robust estimator Vbπ that behaves well when optimized over a policy class Π
of interest—both statistically and computationally.
Perhaps the most straightforward approach to estimating Vπ starts from inverse propensity weighting as used in the context of marginal structural modeling (Robins, Hernán, and
Brumback, 2000; Precup, 2000). Given sequential ignorability, we can write inverse propen(i)
sity weights γt (π) for any policy π recursively as follows, resulting in a value estimate
n
o
(i)
(i)
(i)
(i)
n
γ
1
A
=
π(S
,
A
)
X
t
t−1
1:t
1:t−1
1
(i)
(i)
i,
(6)
γ (π)Y (i) , γt (π) = h
V̂πIPW =
(i)
(i)
(i)
(i)
(i)
n i=1 T
P A = π(S , A
) S ,A
t

1:t

1:t−1

1:t

1:t−1

 Pn
Pn
(i)
(i)
or the normalized alternative V̂πWIPW = i=1 γT (π)Y (i)
i=1 γT (π). The functional
IPW
form of V̂π
makes it feasible to optimize this value estimate over a pre-specified policy
class π ∈ Π (e.g., via a grid-search or mixed integer programming). By Assumptions 1, 2
and 3, IPW is consistent if the treatment probabilities are known a-priori, and by uniform
concentration arguments following Kitagawa and Tetenov
√ (2018), the regret of the policy π̂
learned by maximizing V̂πIPW over π ∈ Π decays as 1/ n if Π is not too large (e.g., if Π is
a VC-class).
While inverse propensity weighting is a simple and transparent approach to estimating Vπ , it has several limitations. In observational studies treatment probabilities need to
be estimated from data, and it is known that the variant of (6) with estimated weights
(i)
γ̂t (π) can perform poorly with even mild estimation error (see, e.g., Liu et al., 2018b).
Furthermore, for any policy π considered, the IPW value estimator only uses trajectories
that match the policy π exactly, which can make policy learning sample-inefficient. Finally,
the IPW estimator is known to be unstable when treatment propensities are small, and
this difficulty is exacerbated in the multi-period setting as the probability of observing any
specific trajectory decays. In the static (i.e., single time step) policy learning setting, related
considerations led several authors to recommend against inverse propensity weighted policy
learning and to develop new methods that were found to have stronger properties both in
theory and in practice (Athey and Wager, 2017; Dudı́k, Erhan, Langford, and Li, 2014;
Kallus, 2018; Zhang, Tsiatis, Davidian, Zhang, and Laber, 2012; Zhou, Mayer-Hamblett,
Khan, and Kosorok, 2017).
Another approach to estimating Vπ is using a doubly robust (DR) estimator as follows
(Jiang and Li, 2016; Thomas and Brunskill, 2016; Zhang, Tsiatis, Laber, and Davidian,
2013)
!
n
T 
 

X
X
1
(i)
(i)
(i)
(i)
(i)
DR
(i)
,
(7)
Vbπ =
γ̂T (π)Y −
γ̂t (π) − γ̂t−1 (π) µ̂π S1:t , A1:t−1
n i=1
t=1
where µ̂π (·) is an estimator of µπ (·), the expected value following policy π conditionally on
the history up to time t as defined in the previous subsection. This estimator generalizes
the well known augmented inverse propensity weighted estimator of Robins, Rotnitzky, and
5

Zhao (1994) beyond the static case. The doubly robust estimator (7) is consistent if either
the propensity weights {γ̂t (·)}Tt=1 or the conditional value estimates µ̂π (·) are consistent. For
a further discussion of doubly robust estimation under sequential ignorability, see Van der
Laan and Rose (2018, Chapter 4), Tsiatis, Davidian, Holloway, and Laber (2019), and
references therein.
From an optimization point of view, a limitation of (7) is that evaluating a given policy
π requires nuisance components estimates µ̂π (·) that are specific to the policy under consideration. This makes policy learning by optimizing VbπDR problematic for several reasons.
Computationally, maximizing VbπDR for all π in a non-trivial set Π would require solving a
multitude of non-parametric dynamic programming problems.
Perhaps even more importantly, statistically, standard regret bounds for policy learning
for single time step problems rely crucially on the fact that Vbπ is continuous in π in an
appropriate sense, meaning that two policies are taken to have similar values if they make
similar recommendations in almost all cases (see, e.g., Athey and Wager, 2017). But, if
µ̂π (·) is learned separately for each π, we have no reason to believe that two similar policies
would necessarily have similar value function estimates—unless one were to use specially
designed µ̂π (·) estimators.5

2.3

Advantage Doubly Robust Policy Learning

The goal of this paper is to develop a new method for policy learning that addresses the
shortcomings of both inverse propensity weighting and the doubly robust method discussed
above in the case of when-to-treat policies.6 Our main proposal, the Advantage Doubly
Robust (ADR) estimator, uses an outcome regression like the doubly robust estimator (7)
to stabilize and robustify its value estimates. However, unlike the estimator (7) which needs
to use different outcome regressions µ̂π (·) to evaluate each different policy π, ADR only
has “universal” nuisance components that do not depend on the policy being estimated,
leveraging the when-to-treat (or when-to-stop) structure of the domain. Throughout this
paper, we will find that this universality property enables us to both effectively optimize
our value estimates to learn policies and to prove robust utilitarian regret bounds.
The motivation for our approach starts from an “advantage decomposition” presented
below. First, define


µnow,k (s1:t , t) := E Y S1:t = s1:t , A1:t−1 = 0, At = k ,


(8)
µnext,k (s1:t , t) := E µnow,k (S1:t+1 , t + 1) S1:t = s1:t , A1:t = 0 ,
which measure the conditional value of a policy that starts treatment k either now or in
the next time period, given that we have not yet started any treatment. Note that, for any
when-to-treat policy π as considered in this paper, the expectations in (8) do not depend
on π because the conditioning specifies all actions from time t = 1 to T . Given policies
π, π 0 ∈ Π, define ∆(π, π 0 ) = Vπ − Vπ0 to be the difference in value of the two policies.
Denote the never-treating policy by 0. Then, a result from Kakade (2003, Chapter 5) and
Murphy (2005) yields the following.
5 One heuristic solution to this difficulty, proposed by Zhang et al. (2013), is to first derive a policy
estimate π̂ ∗ via, e.g., IPW or fitted-Q learning, and then to use value estimates µ̂π̂∗ (·) to evaluate all
policies π ∈ Π using (7). The advantage of this proposal is that learning by maximizing VbπDR becomes more
tractable, since one does not need to re-fit nuisance components in order to evaluate different policies.
6 We emphasize that the IPW and DR estimators discussed above can be used with general dynamic
policies; in contrast, our method can only be used for learning when-to-treat policies. Our proposed method
does not present an alternative to IPW or DR in the general case.

6

Lemma 1. Under Assumptions 1 and 2 let π be a regular when-to-treat policy in the sense
of Definition 1. Then
" T
#
X
∆(π, 0) = E0
µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t) ,
(9)
t=τπ

where E0 samples trajectories under a never-treating policy and, following Definition 1, τπ
is the time at which π starts treating and Wπ is the treatment chosen at that time.
Proof. Given our setup, Lemma 1 of Murphy (2005) implies that
" T
#
X
∆(π, 0) = −E0
Qπ,t (S1:t , A1:t ) − µπ,t (S1:t , A1:t−1 )
t=1

"
= −E0

T
X

1t≥τπ Qπ,t (S1:t , 01:t ) − µπ,t (S1:t , 01:(t−1) )

(10)

#


.

t=1

Because π is a regular when-to-stop policy, whenever t ≥ τπ , the policy π prescribes starting
treatment Wπ immediately if no other treatment has been started yet, i.e.,

1t≥τπ µπ,t (S1:t , 01:(t−1) ) = 1t≥τπ E Y (01:(t−1) , Wπ , Wπ , . . .) S1:t , A1:t−1 = 01:(t−1)
(a)

(b)

=



1t≥τπ E Y (01:(t−1) , Wπ , Wπ , . . .) S1:t , A1:t−1 = 01:(t−1) , At = Wπ


1t≥τπ E Y S1:t , A1:t−1 = 01:(t−1) , At = Wπ
= 1t≥τπ µnow,Wπ (S1:t , t),
(c)

=







where (a), (b) and (c) are by Definition 1, Assumption 2, and Assumption 1 respectively.
Furthermore, given our definition of regular policies, we know that if t ≥ τπ and At = 0,
then π will deterministically prescribe treatment Wπ and time t + 1 regardless of the state
St+1 , and so

1t≥τπ Qπ,t (S1:t , 01:t )
= 1t≥τπ

Z

(d)

1t≥τπ

Z

(e)

1t≥τπ

Z

=




Eπ Y S1:t+1 , A1:t = 01:t dFt+1 St+1 S1:t , A1:t = 01:t



E Y (01:t , Wπ , Wπ , . . .) S1:t+1 , A1:t = 01:t dFt+1 St+1 S1:t , A1:t = 01:t



E Y (01:t , Wπ , Wπ , . . .) S1:t+1 , A1:t = 01:t , At+1 = Wπ

× dFt+1 St+1 S1:t , A1:t = 01:t
Z



(f )
= 1t≥τπ
E Y S1:t+1 , A1:t = 01:t , At+1 = Wπ dFt+1 St+1 S1:t , A1:t = 01:t ,
=

(11)
where (d), (e) and (f) are by Definition 1, Assumption 2 and 1 respectively. The conclusion
(9) emerges by plugging these facts into (10).

7



In Lemma 1 the expectation is taken with respect to the never-treating policy 0. To
make this result usable in practice, the following lemma translates it in terms of expectations taken with respect to the sampling measure. Recall that the propensity of starting
history up to time t is denoted by et,a (s1:t ) =
 treatment a assuming a never-treating

P At = a S1:t = s1:t , A1:t−1 = 0 . The proof of Lemma 2, given in the appendix, follows
directly from a change of measure.
Lemma 2. In the setting of Lemma 1,
#
" T
X
1A1:t−1 =0
(µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t)) .
∆(π, 0) = E
1t≥τπ Qt−1
0
0
t0 =1 et ,0 (S1:t )
t=1

(12)

This representation (12) is at the core of our approach, as it decomposes the relative
value of any given policy π in comparison to that of the never-treating policy 0 into a sum
of local advantages. For any t, the local advantage
δlocal,k (s1:t , t) := µnow,k (s1:t , t) − µnext,k (s1:t , t)

(13)

is the relative advantage of starting treatment k at t versus at t + 1 given the the state
history s1:t . The upshot is that the specification of these local advantages does not depend
on which policy we are evaluating, so if we get a handle on quantities δlocal,k (s1:t , t) for all
s and t, we can use (12) to evaluate any policy π.
Note that the quantity defined in (13) can be seen as a specific treatment effect, namely
the effect of starting treatment k at time t versus t + 1 among all trajectories that were in
state s1:t at time t and started treatment k in either time t or t + 1. Given this observation,
we propose turning (12) into a feasible estimator by replacing all instances of the unknown
regression surfaces δlocal,k (s1:t , t) with doubly robust scores analogous to those used for
augmented inverse propensity weighting in the static case (Robins and Rotnitzky, 1995).
More specifically, we propose the following 3-step policy learning algorithm, outlined
as Algorithm 1. We call our approach the Advantage Doubly Robust (ADR) estimator,
because it replaces local advantages (13) with appropriate doubly robust scores (15) when
estimating ∆(π, 0). In the first estimation step in Algorithm 1, we employ cross-fitting
where we divide the data into Q folds, and only use the Q − 1 folds that a sample trajectory
does not belong to to learn the estimates of its nuisance components; we use superscript
−q(i) on a predictor to denote using trajectories of all folds excluding the fold that the i-th
trajectory belongs to in training a predictor.7
The main strength of this procedure relative to existing doubly robust approaches discussed above (Jiang and Li, 2016; Thomas and Brunskill, 2016; Zhang, Tsiatis, Laber, and
Davidian, 2013) is that ADR can evaluate any stopping policy using universal scores Ψ̂t,k (·)
that do not depend on π. This allows us to ensure smoothness criteria we use to provide
regret bounds for policy optimization. It also provides computational benefits for using the
ADR estimator for policy optimization: the specific policy π we are evaluating only enters
into (14) by specifying which doubly robust scores we should sum over. In particular, the
number of nuisance components we need to learn in the first step of the ADR procedure
scales linearly with the horizon T , but not with the complexity of the policy class Π.
7 The idea of cross-fitting has gained growing popularity recently to reduce the effect of own-observation
bias and to enable results on semiparametric rates of convergence using generic nuisance component estimates
(Athey and Wager, 2017; Chernozhukov et al., 2016a; Schick, 1986).

8

Algorithm 1: Adantage Doubly Robust (ADR) Estimator
1

2

Estimate the outcome models µnow,k (·), µnext,k (·), as well as treatment propensities
et,a (s1:t ) with cross fitting using any supervised learning method tuned for
prediction accuracy.
Given these nuisance component estimates, we construct value estimates

1A(i) =0
1 XX
(i)
ˆ
Ψ̂t,Wπ (S1:t )
∆(π,
0) =
1t≥τπ(i) Qt−1 1:t−1
−q(i)
(i)
n i=1 t=1
ê
(S
)
0
0
0
n

T

t =1 t ,0

(14)

1:t

for each policy π ∈ Π, where the relevant doubly robust score is
(i)

−q(i)

−q(i)

(i)

(i)

Ψ̂t,k (S1:t ) = µ̂now,k (S1:t , t) − µ̂next,k (S1:t , t)
−q(i)

+ 1A(i) =k

−q(i)

êt,k

t

−1

3

(i)

Y (i) − µ̂now,k (S1:t , t)
(i)

(S1:t )

(15)

−q(i)
(i)
Y − µ̂next,k (S1:t , t)
.
(i)
(i)
At =0 At+1 =k −q(i)
(i) −q(i)
(i)
êt,0 (S1:t )êt+1,k (S1:t+1 )
(i)

1

ˆ
0).
Learn the optimal policy by setting π̂ = argmaxπ∈Π ∆(π,

By constructing doubly robust scores Ψ̂t,k (·), the ADR estimator benefits from certain
robustness properties; however, it is not doubly robust in the usual sense, e.g., we do not
robustly correct for the change of measure used to get from the representation in Lemma 1
to the one in Lemma 2. We discuss the asymptotic behavior of our method in Section 4.
In our experiments, we learn all the nuisance components in the first step with nonparametric regression methods (e.g., boosting, lasso, a deep net, etc.), and then optimize for the
best in-class policy by performing a grid search over the parameters that define the policies
in a policy class of interest.
Remark 3. For the purpose of estimating µnext,k (s1:t , t), it is helpful to re-express it in
terms of conditional expectations. First, under Assumption 3, we can continue from (11)
and rewrite µnext,k (s1:t , t) via inverse-propensity weighting as


1At+1 =k
µnext,k (s1:t , t) = E
Y S1:t , A1:t = 01:t .
(16)
et+1,k (S1:t+1 )
Then, using Bayes’ rule, we can verify that


E Y /et+1,k (S1:t+1 ) S1:t = s1:t , A1:t = 0, At+1 = k
.
µnext,k (s1:t , t) = 
E 1/et+1,k (S1:t+1 ) S1:t = s1:t , A1:t = 0, At+1 = k

(17)

This last expression implies that we can consistently estimate µnext,k (·, t) via weighted
non-parametric regression of Y on S1:t on the set of observations with A1:t = 0 and
At+1 = k, with weights e−1
t+1,k (S1:t+1 ). In practice, this may yield more stable estimates of
µnext,k (s1:t , t) than an unweighted non-parametric regression with response 1At+1 =k /et+1,k (S1:t+1 ) Y .

9

3

Related Work

The problem of learning optimal dynamic sequential decision rules is also called learning
dynamic optimal regimes (Murphy, 2003; Robins, 2004), adaptive strategies (Lavori and
Dawson, 2000), or batch off-policy policy learning in the reinforcement learning (RL) literature (Sutton and Barto, 2018). There are a few predominant approaches: the G-estimation
procedure (Robins, 1989; Robins et al., 1992) learns the Structural Nested Mean Models
(SNMM) (Robins, 1994) which model the difference in the marginal outcome functions directly. See also Robins (2004); Moodie, Richardson, and Stephens (2007); Murphy (2003);
Vansteelandt and Joffe (2014) for a further discussion. Orellana et al. (2006) and van der
Laan and Petersen (2007) proposed the dynamic marginal structural models (MSM) to
model the marginal outcome function directly. Under MSM, Robins (1986) proposed using
the G-computation, which is a maximum likelihood approach for solving the MSM; Robins,
Hernán, and Brumback (2000) and Precup (2000) proposed using the inverse propensity
weighting (IPW) approach. Unlike G-estimation or G-computation, we focus on policy
learning instead of structural parameter estimation, and our proposed approach ADR is
more data efficient and robust compared to IPW. Finally, Q-learning8 (Watkins and Dayan,
1992) and the closely related fitted-Q iteration algorithm model the optimal marginal outcome Q functions directly and seek to evaluate the optimal policy by stagewise backwards
regression (Ernst et al., 2005; Murphy, 2005; Prasad et al., 2017; Zhang et al., 2018).
Our work focuses on finding the best in-class policy in a pre-defined policy class, whereas
fitted-Q iteration focuses on finding the best policy by learning Q functions associated with
the optimal policy, which might not fall into the predefined policy class. The two approaches
are complementary, and the ADR estimator shines when there are predefined structural
constraints on the policy class (e.g., for ease of interpretability, budget constraints, etc.).
We note that the fitted-Q iteration algorithm can be adapted to learn the value of an
arbitrary policy by learning the Q functions associated with this policy. However, this
makes optimization over a policy class intractable, as we would need to estimate separate Q
functions for each policy in the class. For more discussion on the comparison of the existing
approaches, see Chakraborty and Moodie (2013), Moodie et al. (2007), Robins et al. (2008),
Tsiatis et al. (2019), Van der Laan and Rose (2018) and Vansteelandt and Joffe (2014).
Considerable progress has been made in learning good models for the value functions
and combining them with propensity models in doubly robust forms. In reinforcement
learning, there has been extensive work focused on learning good models (Farajtabar et al.,
2018; Hanna et al., 2017; Liu et al., 2018b). Guo et al. (2017) focuse reducing the meansquared error in policy evaluation in long horizon settings. Ernst et al. (2005) and Ormoneit
and Sen (2002) study approximating the Bellman operator using empirical estimates with
kernel averagers, and Haskell et al. (2016) focuses on the case with discrete state spaces.
Recently Doroudi et al. (2017) has shown that learning high-quality and fair policy decisions
is nontrivial from inverse propensity weighting based policy evaluation methods. Adaptions
of actor-critic (Degris et al., 2012) and Gaussian processes (Schulam and Saria, 2017) have
been proposed for the off-policy setting as well. Finally, there has been a line of work
that builds doubly-robust estimators that combines the model based estimators with inverse
propensity weighting based estimates to improve robustness (Dudı́k, Langford, and Li, 2011;
Jiang and Li, 2016; Liu, Wang, Kosorok, Zhao, and Zeng, 2018c; Thomas and Brunskill,
8 The fitted-Q iteration algorithm in reinforcement learning is a batch algorithm, and is also called Qlearning in the causal inference and biostatistics literature. This is not to be confused with Q-learning in
the reinforcement learning literature, which is an online-version of the batch-mode fitted-Q algorithm.

10

2016; Zhang, Tsiatis, Laber, and Davidian, 2013; Zhao, Zeng, Laber, Song, Yuan, and
Kosorok, 2014). We note that the closest works to ours are Jiang and Li (2016), Thomas
and Brunskill (2016) and Zhang, Tsiatis, Laber, and Davidian (2013), with the key difference
that our proposal has universal scores and nuisance components for all policies in a policy
class, and thus is practical for policy optimization, whereas it is unclear yet if this is possible
for generic Markov Decision Process settings considered in these works.
Among prior work from the reinforcement learning community that directly tries to learn
an optimal value function and policy, formal bounds on the optimality of the resulting policy
tend to require that the true value function is realizable by the regressor function used to
model the value function in order to obtain good rates and consistent estimators (e.g., Chen
and Jiang, 2019; Le, Voloshin, and Yue, 2019; Munos and Szepesvári, 2008). Such results
also require a bound on the concentratability coefficient (Munos, 2003), which measures
the ratio of the state action distribution of a policy to the state-action distribution under
the behavior policy, for any behavior policy (e.g., Chen and Jiang, 2019; Le, Voloshin, and
Yue, 2019; Munos and Szepesvári, 2008)—this can be viewed as a similar analogue to the
overlap assumption as in Assumption 3. Recent work provides convergence guarantees for
batch policy gradient (Liu et al., 2019), but to our knowledge, there are no regret bounds
on batch direct policy search and optimization.
In the special case of offline policy learning in a single timestep where a policy only needs
to decide whether to treat a subject but not when, substantial progress has been made in how
to derive an optimal regret and performing optimization in finding the optimal policy (Athey
and Wager, 2017; Kallus and Zhou, 2018; Kitagawa and Tetenov, 2018; Swaminathan and
Joachims, 2015; Zhang, Tsiatis, Davidian, Zhang, and Laber, 2012; Zhao, Zeng, Rush, and
Kosorok, 2012; Zhou, Athey, and Wager, 2018; Zhou, Mayer-Hamblett, Khan, and Kosorok,
√
2017). In particular, Kitagawa and Tetenov (2018) establishes a lower bound Ω(1/ n)
on learning the regret in the case of binary treatment. Athey and Wager (2017) and Zhou
et al. (2018) show a matching upperbound assuming the nuisance components can be learned
at a much slower rate in the settings where the treatment consists of binary actions and
multiple actions respectively. Extending this line of result to sequential multi-step settings
is nontrivial for several reasons: First, it is unclear how to optimize efficiently across all
policies in a policy class, especially given the increasing complexity with long horizons.
Second, the regret results used in Athey and Wager (2017) and Zhou et al. (2018) rely on a
chaining argument in which the special form of the estimator ensures values of policies close
to each other is close. It is not obvious whether existing doubly-robust estimators in the
sequential settings (e.g., Thomas and Brunskill, 2016; Zhang, Tsiatis, Laber, and Davidian,
2013) have such a form.
We note that there is a vast literature in optimal stopping (Goel et al., 2017; Jacka,
1991; Mordecki, 2002; Van Moerbeke, 1976). In optimal stopping, the treatment choices are
binary, i.e., whether to stop or not, and the goal is to optimize for a policy for when to start
or stop a treatment. In our setup, we assume multiple treatment actions are allowed. Many
existing works in optimal stopping (e.g., in finance) focus on the setup where a generator
is available for the system dynamics, or the full potential outcomes are available in the
training data. In our setup, we assume neither, and the policies of interest only make
treatment decisions given data observed thus far.
The problem of learning optimal decision rules is also closely related to learning heterogeneous treatment effects (Athey and Imbens, 2016; Athey, Tibshirani, and Wager, 2019;
Chen, 2007; Künzel, Sekhon, Bickel, and Yu, 2019; Nie and Wager, 2017; Wager and Athey,
2018). In both problems, the goal is to learn individualized treatment effect and decision

11

rules, but the type of estimands differ in that instead of learning a nonparametric function
of the treatment effects, here we learn decision rules in a policy class.
Finally, we note that while we focus on the finite horizon setting, there is a large literature
in policy evaluation in the infinite horizon setting (see Antos et al. (2008a,b); Liu et al.
(2018a); Luckett et al. (2019); Munos and Szepesvári (2008) and references therein), and
in the online setting (see Shah and Xie (2018) and references therein). We also note recent
work of Kallus and Uehara (2019) on doubly robust methods for Markov decision processes.
These cases are considerably different from ours and are beyond the scope of this work.
In this paper, we focus on the problem of making treatment decisions once and for all
and with multiple actions at the decision point, which we note is a strict generalization
of the setting in Zhou et al. (2018). We propose an advantage doubly robust estimator
that draws upon the semiparametrics and orthogonal moments literature (Belloni et al.,
2014; Chernozhukov et al., 2016a; Newey, 1994; Robins and Rotnitzky, 1995; Scharfstein
et al., 1999). There is also a growing number of existing works that have applied orthogonal
moments construction to policy evaluation (Belloni et al., 2017; Chernozhukov et al., 2016b;
Kallus, 2018).
Finally, our proposed estimator heavily relies on an advantage decomposition in Murphy
(2005). Murphy (2005) focuses on the generalization error on a variant of Q-learning, and
we turn such a decomposition into a practical and efficient estimator for learning policy
values.

4

Asymptotics

In this section, we study large-sample behavior of the advantage doubly robust estimator
proposed in Section 2.3 for policy learning in when-to-treat settings over a class of policies Π.
It is now standard in the literature in static policy learning for policies over a single decision
to bound regret over the learned policy (e.g., Athey and Wager, 2017; Kitagawa and Tetenov,
2018; Manski, 2004; Swaminathan and Joachims, 2015). However, to our knowledge there
are no directly comparable results for the sequential decision process setting.
Following the literature on static policy learning, our main goal is to prove a bound on
the utilitarian regret R of the learned policy π̂, where
R (π̂) = sup {V (π) : π ∈ Π} − V (π̂) .

(18)

In order to do so, we follow the high-level proof strategy taken by Athey and Wager (2017) for
studying static doubly robust policy learning. We first consider the behavior of an “oracle”
learner who runs our procedure but with perfect estimates of the nuisance components
µnow,k (·), µnext,k (·), and et,k (·), then we couple the behavior of our feasible estimator that
uses estimated nuisance components with this oracle.
Following this outline, recall that our approach starts by estimating the policy value
difference ∆(π, 0) between deploying policy π and the never treating policy 0. The oracle
b
variant of our estimator ∆(π,
0) is then

1A(i) =0
1 XX
(i)
˜
1t≥τπ Qt−1 1:t−1
Ψ̃t,Wπ (S1:t ),
∆(π,
0) =
n i=1 t=1
0
0
t0 =1 et ,0 (S1:t )
n

T

12

(19)

where
(i)

(i)

(i)

Ψ̃t,k (S1:t ) = µnow,k (S1:t , t) − µnext,k (S1:t , t)
+ 1A(i) =k

(i)

Y (i) − µnow,k (S1:t , t)
(i)

(20)

et,k (S1:t )

t

− 1A(i) =0 1A(i)

t+1 =k

t

(i)

Y (i) − µnext,k (S1:t , t)
(i)

(i)

.

et,0 (S1:t )et+1,k (S1:t+1 )

We name (19) the oracle estimator since we assume µnow,k , µnext,k , et,k for t = 1, · · · , T
and k = 1, · · · , K take ground-truth values in (20).
Because the nuisance components in (19) are known a-priori, we can use a standard
central limit theorem argument to verify the following.
Lemma 4. Suppose that Assumptions 1 - 3 hold and that |Y | ≤ M almost surely for some
constant M , for a fixed policy π ∈ Π,
√
˜
n(∆(π,
0) − ∆(π, 0)) ⇒ N (0, Ωπ ),
" T
#
1A(i) =0
X
(21)
(i)
1:t−1
where Ωπ = Var
1t≥τπ(i) Qt−1
Ψ̃t,Wπ (S1:t ) .
(i)
0
t=1
t0 =1 et ,0 (S1:t0 )
Next, we show that the rate of convergence suggested by (21) is in fact uniform over the
whole class Π under appropriate bounded entropy conditions, thus enabling a regret bound
for the oracle learner that optimizes (19). To do so, we introduce some more notation. Let
H = {S1:T , A1:T } be the entire history of a trajectory. Any policy π that is regular in the
sense of Definition 1 can then be re-expressed as a mapping from H to a length KT + 1
vector of all zeros except for an indicator 1 at one position in the probability simplex, such
that9
(
vK(τπ −1)+Wπ if τπ ≤ T ,
(22)
π(H) =
vKT +1
else,
KT +1

where vm ∈ {0, 1}
is the indicator vector with the m-th position 1, and all others 0.
Given this form, we note that we can re-express (20) as
n

1X
˜
∆(π,
π0 ) =
hπ(H (i) ) − π 0 (H (i) ), Γ̃(i) i, where
n i=1
(i)
Γ̃K(t−1)+k

=

1A(i) 0

T
X
t0 =t

1:(t −1)

Qt0 −1

=0

(i)
00
t00 =1 et ,0 (S1:t00 )

(23)

(i)
Ψ̃t,k (S1:t0 )

(i)

for all 1 ≤ t ≤ T and 1 ≤ k ≤ K, and Γ̃KT +1 = 0.
Given these preliminaries, let the Hamming distance between any two policies π, π 0 be
n

dh (π, π 0 ) =

1X
1 (i) 0 (i) .
n i=1 π(H )6=π (H )

9 All

regular policies can be expressed in the form (22); however, we emphasize that the the converse is
not true: The form (22) does not ensure that π is Ft measurable.

13

Define the ε-Hamming covering number of Π as
o
o
n

n
H (1) , . . . , H (n) ,
Ndh (ε, Π) = sup Ndh ε, Π, H (1) , . . . , H (n)


where Ndh ε, Π, H (1) , · · · , H (n)
is the smallest number of policies π (1) , π (2) , . . . , ∈ Π
(i)
such that ∀π ∈ Π, ∃π such that dh (π, π (i) ) ≤ ε. In our formal results, we control the
complexity of the policy class Π in terms of its Hamming entropy.
Assumption 4. There exist constants C, D ≥ 0 and 0 < ω < 0.5 such that, for all
0 < ε < 1, Ndh (ε, Π) ≤ C exp(D( 1ε )ω ).
Whenever Assumption 4 holds, we can use the argument from Lemma 2 of Zhou et al.
(2018) to show the rate of convergence in (21) holds uniformly over the whole class Π for
˜
the oracle estimator ∆(π,
π 0 ). The bounds below depend on the complexity of the class Π
via
Z 1p
log Ndh (ε2 , Π)dε,
(24)
κ(Π) =
0

which is always finite under Assumption 4.
Example 2 (The class of linear thresholding policies). In the case of linear thresholding
policies with binary actions |A| = 2, i.e., {π ∈ Π : τπ = min(t : θ> S1:t > 0)} where
θ ∈ Rd , we note that by Haussler (1995), the covering number of a policy class for singlestep decision-making is bounded by NL1 (Pn ) (ε, Πt ) ≤ cV C(Πt ) expV C(Πt ) (1/ε)V C(Πt ) where
V C(Πt ) is the VC dimension of Πt , the linear thresholding policy class at time t, and c is
some numerical constant. Thus, with a different constant c, NL1 (Pn ) (ε, Πt ) ≤ cded (1/ε)d .
By taking a Cartesian product of the covering at each timestep and with a union bound
on the error incurred at each timestep, we achieve a strict upperbound on Ndh (ε, Π) <
cdT edT (T /ε)dT for a (again different) constant c, and so κ(Π) < cdT log(T ).
Lemma 5. Under Assumptions 1–4 and assuming |Y | ≤ M for some constant M almost
surely, for any δ, c > 0, there exists 0 < ε0 (δ, c) < ∞ and universal constants 0 < c1 , c2 < ∞
such that for all ε < ε0 (δ, c), if we collect at least n(ε, δ) samples, with
1
n(ε, δ) = 2
ε

s

√
c+

V

∗

c1 κ(Π) + c2 +

 !!2
1
2 log
,
δ

(25)

h
i
where V ∗ = supπ,π0 ∈Π E hπ(H (i) ) − π 0 (H (i) ), Γ̃(i) i2 , then, with probability at least 1 − 2δ,
sup

˜
∆(π,
π 0 ) − ∆(π, π 0 ) ≤ ε,

(26)

π,π 0 ∈Π

˜
and, moreover, letting π̃ = argmax{∆(π,
0) : π ∈ Π} be the policy learned by optimizing the
oracle objective (19), we have with probability at least 1 − 2δ, R(π̃) ≤ ε.
Our goal is to get a comparable regret bound using the feasible estimator from (14) in
Algorithm 1 that uses estimated nuisance components by coupling the feasible value estimates with the oracle ones. We establish our coupling result in terms of rates of convergence
on the nuisance components, as follows.

14

−q(i)

Assumption 5. We work with a sequence of problems and estimators such that µ̂now,k ,
−q(i)

−q(i)

µ̂next,k , êt,k

, satisfy for some universal constants Cµ , Ce , κµ , κe ,
sup E



k,t

sup E



k,t

−q(i)

−q(i)
µ̂next,k (S1:t , t)

(27)

2 
− µnext,k (S1:t , t)
≤ Cµ n−2κµ ,

(28)


sup E 
k,t

−q(i)

and furthermore êt,k

1
−q(i)
êt,k (S1:t )

2 

≤ Cµ n−2κµ ,

µ̂now,k (S1:t , t) − µnow,k (S1:t , t)

−

1
et,k (S1:t )

!2 
 ≤ Ce n−2κe ,

(29)

(S1:t ) is uniformly consistent,
sup
t, k, s∈S T

−q(i)

êt,k

(s1:t ) − et,k (s1:t ) →p 0.

(30)

Moreover, motivated by the observation that treatment effects are weak relative to
the available sample size in many problems of interest, we allow for problem sequences
where treatment effects can shrink with sample size n. In regimes where treatment effects
stay constant when the sample size grows, super-efficiency phenomena are unavoidable; see
Luedtke and Chambaz (2017) for a formal statement with static decision rules. To consider
other such settings, first recall the definition of δlocal,k defined in (13). We further define
+
δlocal,k
(S1:t+1 , t) = µnow,k (s1:t+1 , t + 1) − µnext,k (s1:t , t).10
Assumption 6. For some universal constants Cδ , κδ , Cγ , κγ ,


sup E δlocal,k (S1:t , t)2 ≤ Cδ n−2κδ ,

(31)

t,k

h
i
+
sup E δlocal,k
(S1:t+1 , t)2 ≤ Cγ n−2κγ .

(32)

t,k

Lemma 6. Suppose that Assumptions 1–6 hold and assume |Y | ≤ M for some constant
M almost surely. Then, for any δ > 0, there exists 0 < ε0 (δ, η, T ) < ∞ such that for all
ε < ε0 (δ, η, T ), with probability at least 1 − 3δ,
ˆ
˜
sup ∆(π,
π 0 ) − ∆(π,
π 0 ) ≤ ε,
π,π 0

provided we collect at least n0 (ε, δ) samples, where
n0 (ε, δ) = C(δ)KT 2 ε−1

1/ min{1/2+κe , 1/2+κµ , κe +κµ , κe +κδ , κe +κγ }

,

where C(δ) only depends on the constants used in Assumptions 3, 5 and 6.
Combining the above with Lemma 5, we immediately have the following finite-sample
bound for the regret on the feasible estimator.
10 In deterministic systems this difference is always identically zero, but in stochastic settings the two
quantities will generally be different. Intriguingly related quantities (the expected temporal-difference error,
and the variance of the value of next state) have been observed to play important roles in online reinforcement
learning regret bounds (see, e.g., Zanette and Brunskill, 2019) as well.

15

ˆ
Theorem 7. Let π̂ = argmax{∆(π,
0) : π ∈ Π} be the policy learned by optimizing the feasible objective (14). Suppose Assumptions 1–6 and assume |Y | ≤ M for some constant M
almost surely. Then, for any δ > 0, there exist 0 < ε0 (δ, η, T ) < ∞ such that the following
statement holds for all ε < ε0 (δ, η, T ): If we collect at least n(ε, δ) samples, with
s

 !!2

√
1
1
∗
(33)
, n0 (ε, δ) ,
c+ V
c1 κ(Π) + c2 + 2 log
n(ε, δ) = max
ε2
δ
h
i
V ∗ = supπ,π0 ∈Π E hπ(H (i) ) − π 0 (H (i) ), Γ̃(i) i2 , and n0 (ε, δ) as defined in Lemma 6, then
with probability at least 1 − 5δ
sup

ˆ
∆(π,
π 0 ) − ∆(π, π 0 ) ≤ 2ε,

π,π 0 ∈Π

and in particular R(π̂) ≤ 2ε.
We obtain the following corollary if we assume specific learning rates on the nuisance
components and the signal strength.
Corollary 8. Assume κµ > 0, κe > 0, κe + κµ > 12 , κe + κγ > 12 , κe + κδ > 12 . Suppose
Assumptions 1–6 hold and assume |Y | ≤ M for some constant M almost surely. Then,
for any δ > 0, there exist 0 < ε0 (δ, η, T ) < ∞ such that the following holds for all ε <
ε0 (δ, η, T ): If we collect at least n(ε, δ) samples, with
s
 !!2
√
1
1
,
(34)
n(ε, δ) = 2 c + V ∗ c1 κ(Π) + c2 + 2 log
ε
δ
h
i
and V ∗ = supπ,π0 ∈Π E hπ(H (i) ) − π 0 (H (i) ), Γ̃(i) i2 , then with probability at least 1 − 5δ
sup

ˆ
∆(π,
π 0 ) − ∆(π, π 0 ) ≤ 2ε,

π,π 0 ∈Π

and in particular R(π̂) ≤ 2ε.
Our result above can be interpreted in several different regimes. First, we note that we
can reach the optimal sample complexity n ∼ ε−2 if either (a) the treatment propensities
et,k are known and we can consistently estimate µnow,k and µnext,k ; or (b) the signal size
of the advantages is null (i.e., µnow,k (S1:t , t) − µnext,k (S1:t , t) = 0) or is weak (in the sense
that κδ > 0 and et,0 can be learned at a rate such that κδ + κe > 1/2, etc.), and similarly
the stochastic fluctuations are weak (in the sense that κγ > 0 and et,0 can be learned at a
rate such that κγ + κe > 1/2, etc.), and we can consistently estimate µnow,k , µnext,k and
et,k such that κe + κµ > 1/2.
Conversely, if the treatment effects are of a fixed size (i.e., κδ = 0), and we don’t know the
treatment propensities et,k a priori, then we pay a price for not being robust to the change of
measure from Lemma 1 to Lemma 2, and we no longer achieve the optimal rate. The terms
that hurt us are due to error terms that decay as n−(κe +κδ ) which arises from the interaction
of how we use inverse propensity weighting for the treatment starting probabilities and the
signal size of the advantages, and ones that decays as n−(κe +κγ ) which are similarly due
to stochastic fluctuations in the value of starting treatment. If advantages are small, this
won’t matter for smaller target error rates ε, but requires a bigger sample size when we aim
for very small ε.
16

5

ADR with a Terminal State

So far, we have focused our analysis on when-to-stop problems in settings characterized by
overlap (Assumption 3), i.e., where the sampling policy can start treatment in any state
with a non-zero policy, and have assumed that we want to learn a regular policy in the
sense of Definition 1, i.e., one that never stops prescribing treatment once it has started to
do so. In many applications of interest, however, a patient may enter a terminal state in
which treatment becomes impossible—for example, a patient may leave the study or die.
The existence of such a terminal state contradicts the assumptions made above: There is no
overlap in the terminal state (because treatment can never start there), and a policy that
respects the terminal state may not be regular (because the policy must stop prescribing
treatment once the patient enters the terminal state).
The goal of this section is to briefly discuss methodological extensions to ADR that are
required in the presence of a terminal state. Algorithm 2 provides pseudocode for our ADR
policy optimization approach with terminal states.
To do so, we start by adapting Definition 1 and Assumption 3 to this setting.
Definition 3 (Terminal state). A state Φ ∈ S is terminal if, whenever St = Φ , then also
St0 = Φ for all t0 > t. Furthermore, we assume that once a patient enters a terminal state,
we can assess their final outcome, i.e., there exists a set of known functions11 Ht such that
Y = Ht (S1:t ) whenever St+1 = Φ.
Definition 1b (Regular policy with terminal state). A regular when-to-treat policy π that
respects the terminal state Φ is determined by an Ft -measurable stopping time τπ and
an associated Fτπ -measurable decision variable Wπ ∈ {1, . . . , K} as follows:12 For each
time t = 1, . . . , T , if St = Φ then πt (S1:t , A1:(t−1) ) = 0. Otherwise, if At−1 6= 0 then
πt (S1:t , A1:(t−1) ) = At−1 , else if t ≥ τπ then πt (S1:t , A1:(t−1) ) = Wπ . If none of the above
conditions apply, then πt (S1:t , A1:(t−1) ) = 0.
Assumption 3b (Overlap with terminal state). There are constants η, η0 > 0 as well as
a terminal state Φ such that, for all t = 1, · · · , T and s1:t ∈ S t , the following hold. If
st = Φ, then et,a (s1:t ) = 0 for all a ∈ A \ {0} and et,0 (s1:t ) = 1; else, et,a (s1:t ) > η/T for all
a ∈ A \ {0} and et,0 (s1:t ) > 1 − η0 /T .
In the presence of a terminal state, the main modification we need to make to ADR is
that the conditional expectation µnext,k (s1:t , t) as defined in (8) no longer matches the Qfunction that arises in (10) in the proof of Lemma 1, and so we need to adapt our statement
of this result. The proof of the following lemma is included in the Appendix.
Lemma 9. Under Assumptions 1 and 2, let Φ be the terminal state, and let π be a regular
when-to-treat policy that respects Φ in the sense of Definition 1b. Then
" T
#
X

Φ
∆(π, 0) = E0
1St 6=Φ µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t) ,
(35)
t=τπ

example, if Y is survival time, then one can use Ht (S1:t ) = sup {t0 : St0 6= Φ, t0 ≤ t}.
that the policies specified here are still when-to-start policies, i.e., if they have actually started
treatment then they never stop (even if the patient enters a terminal state). One could also choose to make
π stop treatment once the patient enters a terminal state. However, from a statistical perspective, this
makes no difference: All that matters is that the standard of care is a deterministic function of state once
treatment has started.
11 For

12 Note

17

where E0 samples trajectories under a never-treating policy, τπ is the time at which π starts
treating and Wπ is the treatment chosen at that time, and
µΦ
next,k (S1:t , t)

 

= P St+1 6= Φ S1:t , A1:t = 0 E µnow,k (S1:t+1 , t + 1) S1:t , A1:t = 0, St+1 6= Φ (36)


+ P St+1 = Φ S1:t , A1:t = 0 Ht (S1:t ).
Next, the following result is a direct consequence of Lemma 9; its proof is a direct
analogue to that of Lemma 2 and thus omitted.
Lemma 10. In the setting of Lemma 9 and under Assumptions 1, 2 and 3b,
" T
#
X

1A1:t−1 =0
∆(π, 0) = E
1St 6=Φ Qt−1
µnow,Wπ (S1:t , t) − µΦ
.
next,Wπ (S1:t , t)
0
0
t0 =1 et ,0 (S1:t )
t=τπ

(37)

We detail a candidate estimator based on this result as Algorithm
2. For notational


S
,
A
=
0
,
convenience, we denote terminating
probabilities
by
ρ(S
)
=
P
S
=
Φ
1:t
1:t
1:t
t+1
h
i
1A

=k

and write U (S1:t .Φ) = E et+1,kt+1
(S1:t+1 ) Y S1:t , A1:t = 0, St+1 6= Φ . We note that the proposed ADR estimator extended to terminal states is robust towards errors in estimating the
regression outcome functions µnow,k and U (·, Φ)) but we do not correct for the estimation
bias in estimating the terminating probabilities ρ(·). We leave it to future work to develop
robust methods that are also robust against terminating probability estimates.
Finally, in analogy to (16), it is convenient to re-express µnext,k (S1:t .t)Φ via inversepropensity weighting for purpose of estimating it,
µΦ
next,k (S1:t , t)




1At+1 =k
= P St+1 6= Φ S1:t , A1:t = 0 E
Y S1:t , A1:t = 0, St+1 6= Φ
et+1,k (S1:t+1 )


+ P St+1 = Φ S1:t , A1:t = 0 Ht (S1:t ).

(38)

Furthermore, a weighted regression expression analogous to (17) also holds.

6

Experiments

In order to assess the practical performance of our proposed method, we consider two different simulation studies. In the first simulation, we consider the optimal stopping case
in which the treatment decision is binary and the treatment assignment propensities are
not known a-priori. In the second simulation, we want to learn when to start which treatment. There are multiple treatment options and patients can be censored due to death, and
the data are generated from a randomized control trial with known treatment assignment
propensities. This second study helps to capture settings motivated by clinical trials.
In both settings we consider linear thresholding policy rules for simplicity and due to
their interpretability. In our implementation, we use the normalized variant of the IPW
estimator V̂πWIPW as presented in Section 2.2. For the first setup that doesn’t involve
ˆ W in Step 2 of
survival censoring, we use a correspondingly normalized ADR estimator ∆

18

Algorithm 2: Advantage Doubly Robust Estimator with Terminal State
1

2

Estimate the outcome models µnow,k (·), U (·, Φ), terminating propensities ρ(·) as well
as treatment propensities et,a (s1:t ) with cross fitting using any supervised learning
method tuned for prediction accuracy.
Given these nuisance component estimates, we construct value estimates

1A(i) =0
1 XX
(i)
ˆ
Ψ̂Φ
∆(π,
0) =
1St 6=Φ 1t≥τπ(i) Qt−1 1:t−1
t,Wπ (S1:t )
−q(i)
(i)
n i=1 t=1
ê 0 (S 0 )
0
n

T

t =1 t ,0

(39)

1:t

for each policy π ∈ Π, where the relevant doubly robust score is
−q(i)

(i)

(i)

(i)

Φ
−q(i)
Ψ̂Φ
t,k (S1:t ) = µ̂now,k (S1:t , t) − µ̂next,k (S1:t , t)
−q(i)

−q(i)

êt,k

t

(i)

(40)
(i)

t+1 =k

t

(i)

(S1:t )

Y (i) − Û −q(i) (S1:t , Φ)

− 1A(i) =0 1A(i)

3

(i)

Y (i) − µ̂now,k (S1:t , t)

+ 1A(i) =k

−q(i)

êt,0

−q(i)

(i)

,

(i)

(S1:t )êt+1,k (S1:t+1 )

(i)

(i)

(i)

(i)

−q(i)
and µ̂Φ
= (1 − ρ̂−q(i) (S1:t ))Û −q(i) (S1:t , Φ) + ρ̂−q(i) (S1:t )Ht (S1:t )
next,k (S1:t , t)
ˆ
Learn the optimal policy by setting π̂ = argmaxπ∈Π ∆(π,
0).

Algorithm 1:

ˆ W (π, 0) =
∆

T
X

1A(i)

Pn

1:t−1



=0

i=1 Qt−1 ê−q(i) (S (i) )
1:t0
t0 =1 t0 ,0

−

T
X

1:t−1

1:t−1

=0

1A(i) =W



π

t

i=1 Qt−1 ê−q(i) (S (i) )ê−q(i) (S (i) )
1:t
t,Wπ
1:t0
t0 =1 t0 ,0

1A(i)

Pn

1:t−1

=0

1A(i) =W
t

π

i=1 Qt−1 ê−q(i) (S 0 )ê−q(i) (S (i) )
1:t
1:t
t,Wπ
t0 =1 t0 ,0

i=1 Qt

t0 =1

1:t
−q(i)

=Wπ
t+1
−q(i)

(i)

êt0 ,0 (S1:t0 )êt+1,Wπ (S1:t+1 )

1A(i) =0 1A(i)

=Wπ
1:t
t+1
−q(i)
−q(i)
(i)
ê
(S
)ê
(S1:t+1 )
0
0
0
1:t
t+1,W
t =1 t ,0
π

t=1 Q
t



(i)
1t≥τπ(i) Y (i) − µ̂−q(i)
now,Wπ (S1:t , t)

1t≥τπ(i) +

1A(i) =0 1A(i)

Pn

=0

i=1 Qt−1 ê−q(i) (S (i) )
1:t0
t0 =1 t0 ,0

1A(i)

Pn

T
X
t=1

1A(i)

Pn

t=1

+



−q(i)
(i)
−q(i)
(i)
1t≥τπ(i) µ̂now,W
(S1:t , t) − µ̂next,Wπ (S1:t , t)
π

Pn

1A(i)

1:t−1

=0

i=1 Qt−1 ê−q(i) (S 0 ) (1
1:t
t0 =1 t0 ,0

1t≥τπ(i)



− 1t≥τ (i) )
π


−q(i)
(i)
Y (i) − µ̂next,Wπ (S1:t , t)

1A(i)

1t≥τπ(i) + Qt−1

=0
1:t−1
−q(i)
ê
(S1:t0 )
0
0
t =1 t ,0

.
(1 − 1t≥τ (i) )
π

For simplicity, we will refer to them as the IPW (baseline) and ADR (our estimator) respectively in this section. We have a similar weighted form for the ADR estimator with terminal
states that we use in experiments in the second simulation study. We include that in the
Appendix.
In addition to IPW, we also consider fitted-Q iteration as a baseline method for policy
learning. The variant of fitted-Q iteration we implement follows the Batch Q-learning algorithm as described in (Murphy, 2005) for solving the optimal Q function at each timestep:

19

At each t = T, T − 1, · · · , 1, we solve13
Q̂∗t (·, ·) = argminQt

2
n 
1X
(i)
(i)
(i)
(i)
max Q̂∗t+1 (S1:(t+1) , {A1:t , at+1 }) − Qt (S1:t , A1:t ) ,
n i=1 at+1

(41)

where we let Q̂∗T +1 = Y (i) . We note that fitted-Q iteration is an iterative backwardsregression based algorithm targeted at learning the optimal unrestricted policy, whereas our
goal is to learn the best in-class policy given a user-defined policy class. However, while
fitted-Q aims to perform a different task then us, it is still of interest to compare the regret
achieved by both methods. We use the shorthand Q-Opt to refer to this method. Another
variant of the fitted-Q method evaluates a given policy π instead of learning the best policy
(see, e.g., Le et al., 2019). Instead of taking the max operator above, at+1 is chosen according
to the policy π. We call the latter fitted-Q for evaluation and use Q-Eval as a shorthand
accordingly. All experiments can be replicated here: https://github.com/xnie/adr.

6.1

Binay Treatment Choices in an Observational Study

Our first simulation is motivated by a setting where we track a health metric and get a
reward if the health metric is above a threshold at T = 10. The treatment provides a
positive nudge to the health metric at a cost. We start with treatment on, and need to
choose when to stop to minimize cost while trying to keep the health metric stay above the
threshold. The data generating process is as follows:


σ2
1
2
At ,
+ 1Xt <−0.5 Xt
X1 ∼ N (0, σ ), Xt+1 Xt , At ∼ 1Xt ≥−0.5 N Xt +
1 + e0.3Xt
2T
St Xt ∼ N (Xt , ν 2 ), Y = β 1ST +1 >0 −

T
1X
At ,
T t=1

with the stopping action At Xt ∼ Bernoulli(1−1/(1+e−(Xt −1.5) −e−(t−3) )). We note that
Y is the final outcome we’d like to maximize. We also do not assume Markovian structure
and only get to observe St , which is a noisy version of the underlying state Xt .
In our implementation, both the propensity and outcome regressions only use the current
state and action information as opposed to the full history even though the underlying
dynamic is not Markovian.14 We parameterize the policy class of interest by [θ1 , θ2 , θ3 ] and
define each policy to be a linear thresholding rule θ1 St ≥ θ2 t + θ3 such that whenever this
holds, we stop the treatment. We then perform a grid search over a range of values for the
policy parameters, with the grid specified in Appendix B.
For each of the parameter combinations, we run ADR and baseline IPW to estimate
the value of the corresponding policy. The average mean-squared error (MSE) of each of
the policy values across all policies in the policy class is then computed against an oracle
evaluation by using a Monte-Carlo rollout of the policy using the underlying transition
dynamics averaged across 20000 times. We vary β, σ, and the observation noise ν and
13 For this purpose, we estimate propensities and conditional response surfaces using regression forests as
implemented in grf (Athey et al., 2019). For tractability, we do not consider history when learning these
regression; rather, we only use current state as covariates in each time step.
14 In other words, neither out propensity models nor conditional response models are well specified because
we do not use covariates that capture lagged states. Thus, this setting can be seen as a test case for the
value of the robust scoring method in ADR.

20

−1

●
●

log(regret)

−3.5

●

●

●

●
●
●

●
●

−2

●

●
●

●

●

method
● ●

●

−4.0

●

●

log(mse)

−3.0

ADR

●

IPW

●

Q−Opt

●
●

method

●

● ●

−3

●

●
●

●

●
● ●

−5
1000

3000

10000

IPW

−4

●
●

−5.0
300

ADR

●
●

●

−4.5

●

30000

300

n

1000

3000

10000

30000

n

Figure 1: We compare the performance of ADR in comparison to IPW and Q-Opt using
σ = 1, β = 1 and ν = 0.5 in the binary treatment setup. We plot the regret (left figure)
relative to the best in-class policy and the average mean-squared error (right figure) of the
value estimates for policies in the same policy class across all policies (both in log-scale).
The shaded regions are standard error bars. In the mean-squared error (MSE) plot, the
MSE for each policy is computed against an oracle evaluation using Monte-Carlo rollouts
using the underlying transition dynamics averaged across 20000 runs. Both the regret and
MSE results are averaged acros 50 runs. The x-axis shows the number of offline trajectories
we generate in the observational data.
compute the regret and the mean-squared error of policy value estimates (averaged across
all policies in the policy class).
ADR shows a clear advantage in both regret and learning the correct value of policies
across varying values of σ, β and ν. We present the tables of raw results for in Table 1-2
in Appendix B. We present one representative illustration in Figure 1, where we have used
σ = 1, β = 1 and ν = 0.5. We compare the performance of ADR against IPW and Q-Opt
with varying numbers of offline trajectories. IPW and ADR first evaluate the values of the
policies in the policy class, and so we plot the MSE of their policy estimates averaged across
all policies in the policy class in the right plot; it is not applicable for Q-Opt which seeks to
learn the optimal policy directly.

6.2

Multiple Treatment Choices

In the second setup, we consider multiple treatment choices. Our design here is motivated
by a healthcare setting where, once a doctor starts treatment, they can choose between a
more effective but more invasive treatment with strong side effects, or a less effective but less
invasive treatment. More specifically, imagine a cancer patient’s state at time t is modeled
by Xt , Yt and Z, where Xt is the general health state, Yt is the state of a tumor, and Z
is not time-dependent but models the category of the patients for which lifespan differs. In
particular, if Z = 0, a patient always dies immediately; if Z = 1, a patient always survives
until the end of a trial; if Z = 2, the patient’s lifespan has a strong dependency on Yt ,
which we detail below. There are two treatment choices, one non-invasive (At = 1) and
one invasive (At = 2). The non-invasive option lessens the severity of the tumor, and the
invasive option completely removes the tumor, but exacerbates a patient’s general health
conditions. The final outcome is denoted by R, which is the lifetime of a patient, and we
seek a policy π that maximizes Eπ [R]. We consider horizon T = 10. The data generating

21

process is as follows:
X1 ∼ Exp(1)

Y1 ∼ 0.5Exp(3)

Z = 1 : Lt+1 = 0,

Z ∼ M ultinomial(0.3, 0.3, 0.4)

L1 = 1

Z = 2 : Lt+1 = 1

Z = 3 : Lt+1 = 0 if Lt = 0;

(42)

otherwise, Lt+1 ∼ Bernoulli(1Yt ≤5 exp(−0.02Yt ) + 15<Yt ≤14 exp(−0.06Yt ))
At = 0 : Xt+1 = |Xt + σt |

Yt+1 = |Yt + 0.5Xt + σt |

At = 1 : Xt+1 = |Xt + σt |

Yt+1 = |0.5Yt + σt |

At = 2 : Xt+1 = Xt + max(Xt2 , 1.5Xt ) + σt − Xt
Xt0 = max (0, min (Xmax , Xt + ν)) ,
R = min{t : Lt = 0} − 1,

Yt+1 = 0

Yt0 = max (0, min (Ymax , Yt + ν))

Xmax = 10,

Ymax = 16,

σt ∼ N (0, 0.25),

ν ∼ N (0, σ 2 )

where Lt is an indicator for whether the patient is alive at time t.
In this setting, the treatment assignment mechanism is based on sequential randomization in the data such that there are roughly equal number of trajectories that start treating
at each time with either treatment option. Note that the states we observe is Xt0 and Yt0 ,
which is the original states added with noise, making our setup non-Markovian. We consider the following linear thresholding class: θ1 Xt0 + θ2 Yt0 + θ3 t ≥ θ4 is the region in which
we start treatment. If in addition, θ5 Xt0 + θ6 Yt0 + θ7 t ≥ θ8 , we use the invasive treatment
and otherwise, use the non-invasive treatment. We search over the eight parameters in the
policy class with a grid search, with details in Appendix B.
We compare running the ADR policy optimization procedure (as shown in Section 2.3)
against IPW and Q-Opt. Like the binary-action setup, we again estimate the oracle value
of all policies in the policy class with Monte-Carlo rollouts averaged across 20000 times.
In Figure 2, we see that for both the best value learned and the average mean-squared
error, ADR outperforms IPW. We also include the complete set of results with varying noise
parameter σ in Table 3 in Appendix B. Interestingly, we see that in very large samples QOpt becomes competitive with ADR. One possible explanation for this is that ADR is only
allowed to use linear thresholding policies whereas Q-Opt learns over arbitrary policies—
and, in large samples, the increased expressivity of Q-Opt may become helpful.

6.3

Policy Learning vs Policy Evaluation

Throughout this paper, we have focused on ADR as a method for policy learning, and
have emphasized that ADR is well suited to policy learning by empirical maximization
because it can evaluate any policy in the policy class Π using a single set of universal scores
as in (14). In contrast, standard doubly robust methods like AIPW (7) require different
nuisance components to evaluate different policies, thus making them less readily applicable
to learning. That being said, it may still be of interest to compare ADR with AIPW for
the task of evaluating a single policy, and to see whether the form of ADR—optimized for
policy learning—sacrifices accuracy when used for evaluation.
To this end, we revisit the two simulation settings discussed above. However, instead
of trying to learn the best policy, we simply seek to evaluate how much the optimal policy
improves over a never-treating policy. For ADR and IPW, we use the same value estimates as
were maximized for policy learning. For AIPW, we use a weighted form of (7) as in Thomas
and Brunskill (2016), with value functions estimated by Q-Eval, i.e., by a backwards iteration
procedure analogous to (41) that is tailored to evaluating a specific policy as opposed to
22

−2.0

●

●

●
●
●

0

●

●
●

●

log(regret)

−2.5

●

●

●

method
●

●
−3.0

●

●

ADR

●

IPW

●

log(mse)

●

●

method

●
●
●

●

Q−Opt

●

−3.5

−2

−4

●

ADR

●

IPW

●

●
●
●

−4.0

●
●

−6
300

1000

3000

10000

300

n

1000

3000

10000

n

Figure 2: We compare the performance of ADR in comparison to IPW and Q-Opt in the
multiple treatment setup. The plot shows results for σ = 1. We plot the regret (left figure)
relative to the best in-class policy and the average mean-squared error (right figure) of the
value estimates for policies in the same policy class across all policies (both in log-scale). The
shaded regions In the mean-squared error (MSE) plot, the MSE for each policy is computed
against an oracle evaluation using Monte-Carlo rollouts under the underlying transition
dynamics averaged across 20000 runs. Both the regret and MSE results are averaged across
50 runs. The x-axis shows the number of offline trajectories we generate in the observational
data.

finding the best policy. Finally, we also consider Q-Eval on its own, by averaging across
the learned Q values in the initial state across the initial state distribution and the action
distribution that follows the policy of interest.
Overall, as seen in Figure 3, the robust methods—ADR and AIPW—substantially outperform both IPW and Q-Eval here, while AIPW is slightly more accurate that ADR. It
thus appears that if the only task of interest is to evaluate a pre-specified policy then AIPW
is a good method to start with. However, if there’s also a need to learn policies by empirical
maximization, ADR may present a valuable option.

6.4

Comparison between ADR and Fitted-Q Iteration for Policy
Optimization

We emphasize that fitted-Q iteration for policy optimization (Q-Opt) needs to recursively
solve a series of nonparametric regression problems. Q-Opt can be asymptotically biased
depending in part on whether the used function approximator can perfectly model the
underlying optimal value function. In general, the convergence properties and finite sample
performance of Q-Opt are not yet well understood and are an active area of research (see
e.g. Szepesvári and Munos (2005); Munos and Szepesvári (2008); Antos et al. (2008b);
Chen and Jiang (2019)). It is thus not unimaginable that, with very little data, Q-Opt
would regularize towards a decent model of the world which motivates reasonable decisions;
however, once we get more data and Q-Opt increases the complexity of its model fit, the

23

●

●
●

value

1.3

●
●
●

0.4

●
●

●
●

●

●

●

1.1

●

●
●

●
●
●

0.9

●
●

0.3

method

●

● ●

●

ADR

●

IPW

●

AIPW

●

Q−Eval

●

●
0.2

●

● ●

●
●

●
●
●

●

●

●
●

●

●

●
●
●
●

●

●

0.1

● ●

method

●

●

value

1.5

●

ADR

●

IPW

●

AIPW

●

Q−Eval

●

0.0
300

1000

3000

10000

30000

300

1000

n
●
−1

●
●

3000

10000

n
0

●
●

●

●
●

●

●

●

●

●

●
●

●
●

●

−2

●
●

−2

●

●

●

●
−3

●

●

●

●

●

ADR

●

IPW

●

AIPW

●

●
●

log(mse)

log(mse)

method
●

●

method

●
●
●
●

●

−4

●

Q−Eval

●

●

ADR

●

IPW

●

AIPW

●

Q−Eval

●

●
●

−4

●

●

●
●
●

−6

●

●

●
−5
300

1000

3000

10000

30000

300

n

1000

3000

10000

n

Figure 3: Comparison of ADR, IPW, AIPW, and Q-Eval for estimating the value improvement of the best in-class policy over the never stop policy. The left panel is in the setting
of Figure 1 for the binary-treatment setup while the right panel is in the setting of Figure
2 for the multiple-treatment setup. The top two figures compare the value estimates of the
optimal policy, with the black solid line denotes the true value improvement of the optimal
policy via Monte Carlo simulations over 20000 trials. The bottom two figures are the mean
squared errors (MSE) of the value estimates on learning the optimal policy. The results here
are averaged across 50 independent runs, and the shadeded regions denote sampling error.
resulting decisions get worse.
Finally, as discussed previously, Q-Opt seeks to learn the optimal nonparametric policy,
whereas ADR aims for the best in-class policy. As argued by, e.g., Athey and Wager
(2017) and Kitagawa and Tetenov (2018), learning policies that belong to a structured
class specified in advance is important in practice, as this allows stakeholders to enforce
constraints such as interpretability, implementability and resource use. To visualize this
point, recall that, in Section 6.1, we used ADR to learn over linear thresholding policies.
In Figure 4, we plot the policies learned with different methods for one realization of the
simulation in the binary simulation setup in Section 6.1. The left panel of Figure 4 shows, at
each time step, the value of the state St for any trajectory that has not stopped treatment
yet according to the policy learned by ADR (shown as a black line). The color coding
specifies the policy decision for each trajectory at the given time step.15 For comparison,
15 There are fewer trajectories plotted as we move along the time axis, because once a trajectory has
stopped treatment, it would always stop treatment and there will be no longer decisions made.

24

ADR

Q−Opt

5.0
4

2.5

policy

2

St

St

ADR
oracle
0.0

treatment

0

on
off

−2.5

−2

1

2

3

4

5

1

t

2

3

4

5

t

Figure 4: A single realization of the best policy learned in the binary action setup case as described
in Section 6.1, in the setting of ν = 0.5, β = 5, σ = 1. ADR and the oracle choose the best in-class
policy from the predefined linear policy class, whereas Q-Opt learns the value function via blackbox
regression methods and learns a policy that is not so easy to interpret. At each time step, we plot
the value of the state St from trajectories that have not stopped treating yet.

we also plot the best policy learned by the oracle using the dotted-dashed line. The right
panel of Figure 4 is generated the same way, but shows decisions made by the policy learned
using Q-Opt instead. Unlike ADR, which returns a linear policy, Q-Opt learns a policy with
a complicated functional form that is not so easy to interpret.
One might ask whether we could make Q-Opt interpretable by using linear regression
in the recursive step (41). Doing so, however, would void any nonparametric consistency
guaranteed for Q-Opt, and in particular would not recover best-in-class linear policies. The
problem is that Q-Opt conflates modeling and policy optimization, rather than separating
out these two steps like ADR; in contrast, we first model µnow and µnext using appropriately
flexible method and then choose policy π̂ in a separate optimization step where we can
enforce structure.

Acknowledgement
We are grateful for enlightening conversations with Susan Athey, Miguel Hernan, Michael
Kosorok, Percy Liang, Susan Murphy, Jamie Robins, Andrea Rotnitzky and Zhengyuan
Zhou, as well as for helpful comments and feedback from the Associate Editor and seminar
participants at several universities and workshops. XN acknowledges the partial support
from the Stanford Data Science Scholars program. EB acknowledges the partial support
of a NSF Career Award and a Siemens grant. SW acknowledges the partial support of
a Stanford Institute for Human-Centered Artificial Intelligence grant, a Facebook Faculty

25

Award, and National Science Foundation grant DMS-1916163.

References
A. Antos, C. Szepesvári, and R. Munos. Fitted q-iteration in continuous action-space mdps.
In Advances in neural information processing systems, pages 9–16, 2008a.
A. Antos, C. Szepesvári, and R. Munos. Learning near-optimal policies with bellmanresidual minimization based fitted policy iteration and a single sample path. Machine
Learning, 71(1):89–129, 2008b.
S. Athey and G. Imbens. Recursive partitioning for heterogeneous causal effects. Proceedings
of the National Academy of Sciences, 113(27):7353–7360, 2016.
S. Athey and G. Imbens. Design-based analysis in difference-in-differences settings with
staggered adoption. arXiv preprint arXiv:1808.05293, 2018.
S. Athey and S. Wager. Efficient policy learning. arXiv preprint arXiv:1702.02896, 2017.
S. Athey, J. Tibshirani, and S. Wager. Generalized random forests. The Annals of Statistics,
47(2):1148–1178, 2019.
A. Belloni, V. Chernozhukov, and C. Hansen. Inference on treatment effects after selection
among high-dimensional controls. The Review of Economic Studies, 81(2):608–650, 2014.
A. Belloni, V. Chernozhukov, I. Fernández-Val, and C. Hansen. Program evaluation with
high-dimensional data. Econometrica, 85(1):233–298, 2017.
D. Bertsimas and N. Kallus. From predictive to prescriptive analytics. Management Science,
66(3):1025–1044, 2020.
B. Chakraborty and E. Moodie. Statistical methods for dynamic treatment regimes. Springer,
2013.
J. Chen and N. Jiang. Information-theoretic considerations in batch reinforcement learning.
Proceedings of International Conference on Machine Learning, 2019.
X. Chen. Large sample sieve estimation of semi-nonparametric models. Handbook of econometrics, 6:5549–5632, 2007.
V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, and W. K. Newey.
Double machine learning for treatment and causal parameters. Technical report, cemmap
working paper, 2016a.
V. Chernozhukov, J. C. Escanciano, H. Ichimura, and W. K. Newey. Locally robust semiparametric estimation. arXiv preprint arXiv:1608.00033, 2016b.
T. Degris, M. White, and R. S. Sutton.
Off-policy actor-critic.
arXiv preprint
arXiv:1205.4839, 2012.
S. Doroudi, P. S. Thomas, and E. Brunskill. Importance sampling for fair policy selection.
Grantee Submission, 2017.
M. Dudı́k, J. Langford, and L. Li. Doubly robust policy evaluation and learning. In Proceedings of the 28th International Conference on Machine Learning, pages 1097–1104,
2011.
M. Dudı́k, D. Erhan, J. Langford, and L. Li. Doubly robust policy evaluation and optimization. Statistical Science, 29(4):485–511, 2014.
A. N. Elmachtoub and P. Grigas. Smart” predict, then optimize”. arXiv preprint
arXiv:1710.08005, 2017.
D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6(Apr):503–556, 2005.

26

M. Farajtabar, Y. Chow, and M. Ghavamzadeh. More robust doubly robust off-policy
evaluation. arXiv preprint arXiv:1802.03493, 2018.
K. Goel, C. Dann, and E. Brunskill. Sample efficient policy search for optimal stopping
domains. Proceedings of the 26th International Joint Conference on Artificial Intelligence,
pages 1711–1717, 2017.
Z. Guo, P. S. Thomas, and E. Brunskill. Using options and covariance testing for long horizon
off-policy policy evaluation. In Advances in Neural Information Processing Systems, pages
2492–2501, 2017.
J. P. Hanna, P. Stone, and S. Niekum. Bootstrapping with models: Confidence intervals for
off-policy evaluation. In Proceedings of the 16th Conference on Autonomous Agents and
MultiAgent Systems, pages 538–546. International Foundation for Autonomous Agents
and Multiagent Systems, 2017.
W. B. Haskell, R. Jain, and D. Kalathil. Empirical dynamic programming. Mathematics of
Operations Research, 41(2):402–429, 2016.
D. Haussler. Sphere packing numbers for subsets of the boolean n-cube with bounded
vapnik-chervonenkis dimension. Journal of Combinatorial Theory, Series A, 69(2):217–
232, 1995.
M. A. Hernán, B. Brumback, and J. M. Robins. Marginal structural models to estimate
the joint causal effect of nonrandomized treatments. Journal of the American Statistical
Association, 96(454):440–448, 2001.
G. W. Imbens and D. B. Rubin. Causal inference in statistics, social, and biomedical
sciences. Cambridge University Press, 2015.
S. . Jacka. Optimal stopping and the american put. Mathematical Finance, 1(2):1–14, 1991.
N. Jiang and L. Li. Doubly robust off-policy value evaluation for reinforcement learning. In
Proceedings of The 33rd International Conference on Machine Learning, Proceedings of
Machine Learning Research. PMLR, 2016.
S. M. Kakade. On the sample complexity of reinforcement learning. PhD thesis, 2003.
N. Kallus. Balanced policy evaluation and learning. In Advances in Neural Information
Processing Systems, pages 8909–8920, 2018.
N. Kallus and M. Uehara. Double reinforcement learning for efficient off-policy evaluation
in markov decision processes. arXiv preprint arXiv:1908.08526, 2019.
N. Kallus and A. Zhou. Confounding-robust policy improvement. In Advances in neural
information processing systems, pages 9269–9279, 2018.
T. Kitagawa and A. Tetenov. Who should be treated? empirical welfare maximization
methods for treatment choice. Econometrica, 86(2):591–616, 2018.
S. R. Künzel, J. S. Sekhon, P. J. Bickel, and B. Yu. Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the National Academy of
Sciences, 116(10):4156–4165, 2019.
J. Lafferty, H. Liu, and L. Wasserman. Concentration of measure, 2008.
P. W. Lavori and R. Dawson. A design for testing clinical strategies: biased adaptive withinsubject randomization. Journal of the Royal Statistical Society: Series A (Statistics in
Society), 163(1):29–38, 2000.
H. Le, C. Voloshin, and Y. Yue. Batch policy learning under constraints. In International
Conference on Machine Learning, pages 3703–3712, 2019.
Q. Liu, L. Li, Z. Tang, and D. Zhou. Breaking the curse of horizon: Infinite-horizon off-policy
estimation. In Advances in Neural Information Processing Systems, pages 5361–5371,
2018a.
Y. Liu, O. Gottesman, A. Raghu, M. Komorowski, A. Faisal, F. Doshi-Velez, and E. Brun-

27

skill. Representation balancing MDPs for off-policy policy evaluation. arXiv preprint
arXiv:1805.09044, 2018b.
Y. Liu, A. Swaminathan, A. Agarwal, and E. Brunskill. Off-policy policy gradient with
state distribution correction. Proceedings of Uncertainty in AI, 2019.
Y. Liu, Y. Wang, M. R. Kosorok, Y. Zhao, and D. Zeng. Augmented outcome-weighted
learning for estimating optimal dynamic treatment regimens. Statistics in medicine, 2018c.
D. J. Luckett, E. B. Laber, A. R. Kahkoska, D. M. Maahs, E. Mayer-Davis, and M. R.
Kosorok. Estimating dynamic treatment regimes in mobile health using v-learning. Journal of the American Statistical Association, (just-accepted):1–39, 2019.
A. Luedtke and A. Chambaz.
Faster rates for policy learning.
arXiv preprint
arXiv:1704.06431, 2017.
C. F. Manski. Statistical treatment rules for heterogeneous populations. Econometrica, 72
(4):1221–1246, 2004.
E. E. Moodie, T. S. Richardson, and D. A. Stephens. Demystifying optimal dynamic treatment regimes. Biometrics, 63(2):447–455, 2007.
E. E. Moodie, R. W. Platt, and M. S. Kramer. Estimating response-maximized decision
rules with applications to breastfeeding. Journal of the American Statistical Association,
104(485):155–165, 2009.
E. Mordecki. Optimal stopping and perpetual options for lévy processes. Finance and
Stochastics, 6(4):473–493, 2002.
R. Munos. Error bounds for approximate policy iteration. In ICML, volume 3, pages
560–567, 2003.
R. Munos and C. Szepesvári. Finite-time bounds for fitted value iteration. Journal of
Machine Learning Research, 9(May):815–857, 2008.
S. A. Murphy. Optimal dynamic treatment regimes. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 65(2):331–355, 2003.
S. A. Murphy. A generalization error for q-learning. Journal of Machine Learning Research,
6(Jul):1073–1097, 2005.
W. K. Newey. The asymptotic variance of semiparametric estimators. Econometrica: Journal of the Econometric Society, pages 1349–1382, 1994.
J. Neyman. Sur les applications de la théorie des probabilités aux experiences agricoles:
Essai des principes. Roczniki Nauk Rolniczych, 10:1–51, 1923.
X. Nie and S. Wager. Quasi-oracle estimation of heterogeneous treatment effects. arXiv
preprint arXiv:1712.04912, 2017.
L. Orellana, A. Rotnitzky, and J. Robins. Generalized marginal structural models for estimating optimal treatment regimes, 2006.
D. Ormoneit and Ś. Sen. Kernel-based reinforcement learning. Machine learning, 49(2-3):
161–178, 2002.
N. Prasad, L.-F. Cheng, C. Chivers, M. Draugelis, and B. E. Engelhardt. A reinforcement
learning approach to weaning of mechanical ventilation in intensive care units. Conference
on Uncertainty in Artificial Intelligence, 2017.
D. Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department
Faculty Publication Series, page 80, 2000.
J. Robins. A new approach to causal inference in mortality studies with a sustained exposure period: application to control of the healthy worker survivor effect. Mathematical
Modelling, 7(9-12):1393–1512, 1986.
J. Robins, L. Orellana, and A. Rotnitzky. Estimation and extrapolation of optimal treatment
and testing strategies. Statistics in medicine, 27(23):4678–4721, 2008.

28

J. M. Robins. The analysis of randomized and non-randomized aids treatment trials using a new approach to causal inference in longitudinal studies. Health service research
methodology: a focus on AIDS, pages 113–159, 1989.
J. M. Robins. Correcting for non-compliance in randomized trials using structural nested
mean models. Communications in Statistics-Theory and methods, 23(8):2379–2412, 1994.
J. M. Robins. Optimal structural nested models for optimal sequential decisions. In Proceedings of the second Seattle Symposium in Biostatistics, pages 189–326. Springer, 2004.
J. M. Robins and A. Rotnitzky. Semiparametric efficiency in multivariate regression models
with missing data. Journal of the American Statistical Association, 90(429):122–129,
1995.
J. M. Robins, D. Blevins, G. Ritter, and M. Wulfsohn. G-estimation of the effect of prophylaxis therapy for pneumocystis carinii pneumonia on the survival of aids patients.
Epidemiology, pages 319–336, 1992.
J. M. Robins, A. Rotnitzky, and L. P. Zhao. Estimation of regression coefficients when some
regressors are not always observed. Journal of the American statistical Association, 89
(427):846–866, 1994.
J. M. Robins, M. A. Hernán, and B. Brumback. Marginal structural models and causal
inference in epidemiology. Epidemiology, 11(5):551, 2000.
P. R. Rosenbaum and D. B. Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41–55, 1983.
D. B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized
studies. Journal of Educational Psychology, 66(5):688, 1974.
J. Rust. Optimal replacement of gmc bus engines: An empirical model of harold zurcher.
Econometrica: Journal of the Econometric Society, pages 999–1033, 1987.
D. O. Scharfstein, A. Rotnitzky, and J. M. Robins. Adjusting for nonignorable drop-out using semiparametric nonresponse models. Journal of the American Statistical Association,
94(448):1096–1120, 1999.
A. Schick. On asymptotically efficient estimation in semiparametric models. The Annals of
Statistics, pages 1139–1151, 1986.
P. Schulam and S. Saria. Reliable decision support using counterfactual models. In Advances
in Neural Information Processing Systems, pages 1697–1708, 2017.
D. Shah and Q. Xie. Q-learning with nearest neighbors. In Advances in Neural Information
Processing Systems, pages 3111–3121, 2018.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
A. Swaminathan and T. Joachims. Batch learning from logged bandit feedback through
counterfactual risk minimization. Journal of Machine Learning Research, 16:1731–1755,
2015.
C. Szepesvári and R. Munos. Finite time bounds for sampling based fitted value iteration.
In Proceedings of the 22nd international conference on Machine learning, pages 880–887,
2005.
P. Thomas and E. Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, pages 2139–2148, 2016.
A. A. Tsiatis, M. Davidian, S. T. Holloway, and E. B. Laber. Dynamic Treatment Regimes:
Statistical Methods for Precision Medicine. CRC Press, 2019.
M. J. van der Laan and M. L. Petersen. Causal effect models for realistic individualized
treatment and intention to treat rules. The international journal of biostatistics, 3(1),
2007.
M. J. Van der Laan and S. Rose. Targeted Learning in Data Science. Springer, 2018.

29

P. Van Moerbeke. On optimal stopping and free boundary problems. Archive for Rational
Mechanics and Analysis, 60(2):101–148, 1976.
S. Vansteelandt and M. Joffe. Structural nested models and g-estimation: the partially
realized promise. Statistical Science, 29(4):707–731, 2014.
S. Wager and S. Athey. Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, 113(523):1228–1242,
2018.
C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.
When To Start Consortium. Timing of initiation of antiretroviral therapy in aids-free hiv1-infected patients: a collaborative analysis of 18 hiv cohort studies. The Lancet, 373
(9672):1352–1363, 2009.
A. Zanette and E. Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on Machine Learning, pages 7304–7312, 2019.
B. Zhang, A. A. Tsiatis, M. Davidian, M. Zhang, and E. Laber. Estimating optimal treatment regimes from a classification perspective. Stat, 1(1):103–114, 2012.
B. Zhang, A. A. Tsiatis, E. B. Laber, and M. Davidian. Robust estimation of optimal
dynamic treatment regimes for sequential treatment decisions. Biometrika, 100(3):681–
694, 2013.
Y. Zhang, E. B. Laber, M. Davidian, and A. A. Tsiatis. Interpretable dynamic treatment
regimes. Journal of the American Statistical Association, 113(524):1541–1549, 2018.
Y.-Q. Zhao, D. Zeng, E. B. Laber, R. Song, M. Yuan, and M. R. Kosorok. Doubly robust
learning for estimating individualized treatment with censored data. Biometrika, 102(1):
151–168, 2014.
Y. Zhao, D. Zeng, A. J. Rush, and M. R. Kosorok. Estimating individualized treatment
rules using outcome weighted learning. Journal of the American Statistical Association,
107(499):1106–1118, 2012.
X. Zhou, N. Mayer-Hamblett, U. Khan, and M. R. Kosorok. Residual weighted learning for
estimating individualized treatment rules. Journal of the American Statistical Association,
112(517):169–187, 2017.
Z. Zhou, S. Athey, and S. Wager. Offline multi-action policy learning: Generalization and
optimization. arXiv preprint arXiv:1810.04778, 2018.

30

SUPPLEMENTARY MATERIAL

A

Proofs

Proof of Lemma 2. For a fixed t such that 1 ≤ t ≤ T ,
"
E

1A1:t−1 =0

1t≥τπ Qt−1

t0 =1 et0 ,0 (S1:t0 )

Z
=

(µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t))

1
1t≥τπ Qt−1A1:t−1 =0

0
0
t0 =1 et ,0 (S1:t )

T
Y

(µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t)) f (s1 )

ft st s1:(t−1) , a1:(t−1)

Z

Y
 t−1

et0 ,0 (S1:t0 )dS1:t dA1:t−1

t0 =1

t=2

=

#

1t≥τπ (µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t)) f (s1 )

T
Y

ft st s1:(t−1) , a1:(t−1)



t=2

1A1:t−1 =0 dS1:t dA1:t−1
= E0 [1t≥τπ (µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t))] .
h
i
˜
Proof of Lemma 4. First, we check that E ∆(π,
0) = ∆(π, 0). It is sufficient to check that
for each t such that 1 ≤ t ≤ T , the following holds.
#
"
E

1
1
1t≥τπ Qt−1 A1:t−1 =0 At =k

(i)
0
0
t0 =1 et ,0 (S1:t )et,k (S1:t )

" "
=E E
=E

(Y − µnow,k (S1:t , t))

1A1:t−1 =0 1At =k

##

1t≥τπ Qt−1

0
0
t0 =1 et ,0 (S1:t )et,k (S1:t )

(Y − µnow,k (S1:t , t)) S1:t

1t≥τπ E (Y − µnow,k (S1:t , t)) S1:t , A1:t−1 = 0, At = k







= 0,
and
"
E

1A1:t =0 1At+1 =k

1t≥τπ Qt

(i)
0
0
t0 =1 et ,0 (S1:t )et+1,k (S1:t+1 )

" "
E E

1t≥τπ Qt

"

"

E

#

1t≥τπ E

(Y − µnext,k (S1:t , t))

1A1:t =0 1At+1 =k

(i)
t0 =1 et0 ,0 (S1:t0 )et+1,k (S1:t+1 )

1At+1 =k
(i)

et+1,k (S1:t+1 )

##
(Y − µnext,k (S1:t , t)) S1:t
##

(Y − µnext,k (S1:t , t)) S1:t , A1:t = 0

= 0,
where the last equality follows from (16). Next, we check that the variance Ωπ < ∞. Note
that, given the assumption that |Y | is bounded and given overlap as in Assumption 3, we

31

1
have QT −1 e1 (S ) et,k (S
≤ T (1−η0 /T )−T /η ≤ T exp(2η0 )/η when T ≥ 2 and 0 < η0 < 1.
1:t )
t,0
1:t
t=1
We then conclude that Ωπ < ∞.
The desired result then follows from the LindebergLvy Central Limit Theorem.

Proof of Lemma 5. Given a policy class Π, letting π ∗ = argmaxπ∈Π Vπ . First, we note that
R(π̃) = Vπ∗ − Vπ̃
= ∆(π ∗ , π̃)
˜ ∗ , π̃) + ∆(π ∗ , π̃) − ∆(π
˜ ∗ , π̃)
= ∆(π
˜ ∗ , π̃)
≤ ∆(π ∗ , π̃) − ∆(π
≤ sup

˜
∆(π, π 0 ) − ∆(π,
π0 ) .

π,π 0 ∈Π

We note that although we work in the sequential policy learning setup, by redefining policy
˜ in (23) becomes the same as ∆
˜ in Zhou et al. (2018). Given Asπ as in (22), the form of ∆
sumptions 1–4, by directly following their Lemma 2, we have for any δ > 0, with probability
at least 1 − 2δ, there exists universal constants 0 < c1 , c2 < ∞ such that
!r
r


1
V∗
1
0
0
˜
π ) − ∆(π, π ) ≤ c1 κ(Π) + c2 + 2 log
sup ∆(π,
+o √
.
δ
n
n
π,π 0 ∈Π
Let the last term be cn . Thus for any c > 0, there existsN (c) such thatq
for all n>
qN (c),
√
1
V∗
cn < c/ n. Choose ε0 (c, δ) so small such that by letting c1 κ(Π) + c2 + 2 log δ
n +
√
c/ n < ε0 (δ, c), we have n > N (c). The result then follows immediately for all ε <
ε0 (δ, c).
We now state a simple result on the mean-squared error of noisy products that will be
useful in the proof of Lemma 6.
Proposition 11. Let Xt for t = 1, ..., T be a set of (not necessarily independent
or identi

cally distributed) random variables with |log(1 + Xt )| ≤ c almost surely and E log(1 + Xt )2 ≤
v 2 . Then,

!2 


 2T c
T
Y
e
− 2T c − 1
2
2


E
(1 + Xt ) − 1
≤ v 4 + 64T
(43)
4T 2 c2
t=1
for all
(
v 2 ≤ min 1,



4T 2



e2T c − 2T c − 1
4T 2 c2

−1 )
.

(44)

Proof. For convenience, let at = E [log(1 + Xt )] and Zt = log(1 + Xt ) − at . Then, writing
PT
a = t=1 at , we see that

!2 

T
2 
Y
PT
E
(1 + Xt ) − 1  = E ea+ t=1 Zt − 1
t=1

h PT
i
 h PT
i

2
≤ e2a E e2 t=1 Zt − 2ea + 1 = (ea − 1) + e2a E e2 t=1 Zt − 1 ,

32

where the inequality statement follows by Jensen’s inequality
of the expohP and convexity
i
T
2 2
nential
function.
By
Cauchy-Schwartz,
we
see
that
Var
Z
≤
T
v
,
and clearly
t
t=1
PT
≤
T
c.
Thus,
by
a
Bernstein-style
bound
on
its
moment-generating
function
(see,
Z
t
t=1
e.g., Lemma 7.26 of Lafferty et al. (2008)), we find that

 2T c

h PT
i
e
− 2T c − 1
2 t=1 Zt
2 2
0≤E e
− 1 ≤ exp 4T v
−1
4T 2 c2
 2T c

e
− 2T c − 1
≤ 8T 2 v 2
4T 2 c2
whenever (44) holds, since ex − 1 ≤ 2x for all 0 ≤ x ≤ 1. Meanwhile, we must have |a| ≤ v,
and so we conclude that (43) holds; here, we used the bounds (ea − 1)2 ≤ 4a2 and e2a ≤ 8
for all |a| ≤ 1.
Proof of Lemma 6. For simplicity, we will prove this result in a setting where the nuisance
components êt,k (s1:t ), µ̂now,k (s1:t ) and µ̂next,k (s1:t ) are trained on a separate development
set and can thus be considered as exogenous. Results in our setting of interest, i.e., with
cross-fitting, can then be derived analogously by applying the same argument multiple
times, with one fold as the focal fold and the other folds acting as the development set. See
Chernozhukov et al. (2016a) for further details, and the proof of Lemma 4 of Athey and
Wager (2017) for a concrete example of a proof following this strategy.
Let δ > 0 be pre-specified. By Assumption 5, there is an n(δ) for which

sup |êt,k (s1:t ) − et,k (s1:t )| : t ∈ 1, . . . , T, k ∈ 1, . . . , K, s ∈ S T

≤

1 − η0
2T

(45)

for all n ≥ n(δ), with probability at least 1 − δ. For the rest of this proof, we focus the event
under which (45) has occurred. Now, recalling notation from (19), etc., we have
˜
ˆ
∆(π,
π 0 ) − ∆(π,
π0 )
n

1A(i)

T

1 X X π,π0
=
u
n i=1 t=1 t,i
n
T
X
1 X π,π0
u
≤
n i=1 t,i
t=1
|

1:t−1 =0

(i)
0
t0 =1 et ,0 (S1:t0 )

Qt−1

1A(i)

1:t−1 =0

(i)
0
t0 =1 et ,0 (S1:t0 )

Qt−1

(i)
Ψ̃t,Wπ (S1:t )

(i)
Ψ̃t,Wπ (S1:t )

1A(i)

− Qt−1

1:t−1 =0

(i)
0
t0 =1 êt ,0 (S1:t0 )

1A(i)

− Qt−1

1:t−1 =0

(i)
0
t0 =1 êt ,0 (S1:t0 )

!
(i)
Ψ̂t,Wπ (S1:t )

!
(i)
Ψ̂t,Wπ (S1:t )

{z

}

errt

≤ T sup {errt : t = 1, . . . , T } ,
0

where uπ,π
t,i = 1t≥τπ − 1t≥τπ0 . Furthermore, we note that
n

1 X π,π0
errt ≤
u
n i=1 t,i
+

1A(i)

1A(i)

1:t−1 =0

(i)
0
t0 =1 et ,0 (S1:t0 )

Qt−1

− Qt−1

1:t−1 =0

(i)
0
t0 =1 êt ,0 (S1:t0 )

!
(i)

Ψ̃t,Wπ (S1:t )

n


1A(i) =0
1 X π,π0
(i)
(i)
ut,i Qt−1 1:t−1 (i)
Ψ̃t,Wπ (S1:t ) − Ψ̂t,Wπ (S1:t ) .
n i=1
êt0 ,0 (S 0 )
0
t =1

1:t

We now proceed to bound the two above summands separately; call them errt (1) and errt (2).
The first term measures our errors in estimating the propensity of starting treatment, while
33

the second measures errors in the advantage doubly robust scores. For the first term, we
further note that
!
n
1A(i) =0
1A(i) =0
1 X π,π0
1:t−1
1:t−1
u
errt (1) ≤
− Qt−1
Qt−1
(i)
(i)
n i=1 t,i
0
0
t0 =1 et ,0 (S1:t0 )
t0 =1 êt ,0 (S1:t0 )


(i)
(i)
× µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t)

1A(i)

n

1 X π,π0
+
u
n i=1 t,i
×

(i)
0
t0 =1 et ,0 (S1:t0 )

Qt−1

1A(i)
t =Wπ

1A(i)

1:t−1 =0

(i)
0
t0 =1 et ,0 (S1:t0 )

Qt−1

Y

1 (i)
1A(i)
t =0 A

t+1 =Wπ

1A(i)

n

1 X π,π0
u
+
n i=1 t,i
×

Y

!

− Qt−1

1:t−1 =0

!

(i)
0
t0 =1 êt ,0 (S1:t0 )
!
(i)
(i)
− µnow,Wπ (S1:t+1 , t + 1)
(i)
(i)
et,0 (S1:t )et+1,Wπ (S1:t+1 )

1A(i)

1:t−1 =0

(i)
0
t0 =1 et ,0 (S1:t0 )

− Qt−1

1:t−1 =0

!

(i)
0
t0 =1 êt ,0 (S1:t0 )
!
(i)
(i)
µnow,Wπ (S1:t+1 , t + 1) − µnext,Wπ (S1:t , t)
(i)
(i)
At =0 At+1 =Wπ
(i)
(i)
et,0 (S1:t )et+1,Wπ (S1:t+1 )

Qt−1

1

− Qt−1

1:t−1 =0

(i)
0
t0 =1 êt ,0 (S1:t0 )
!
(i)
(i)
− µnow,Wπ (S1:t , t)
(i)
et,Wπ (S1:t )

1A(i)

n

1 X π,π0
+
u
n i=1 t,i
×

1A(i)

1:t−1 =0

1

.

The first summand above can be bounded by Cauchy-Schwartz:
v
!2
u n
1A(i) =0
1A(i) =0
u1 X
1:t−1
1:t−1
t
... ≤
− Qt−1
Qt−1
(i)
(i)
n i=1
0 ,0 (S
0
e
)
0
0
t
1:t
t =1
t0 =1 êt ,0 (S1:t0 )
v
u n 
2
u1 X
(i)
(i)
×t
µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t) .
n i=1
(i)

(i)

The second moments of µnow,Wπ (S1:t , t) − µnext,Wπ (S1:t , t) are governed by Assumption 6,
so we know that the second term in the product is bounded on the order of n−κδ (with
constants that do not depend on the problem setting. Meanwhile,
n

1X
n i=1

1A(i)

1A(i)

1:t−1 =0

(i)
0
t0 =1 et ,0 (S1:t0 )

Qt−1

n

1X
≤
n i=1

− Qt−1

1

!2

1:t−1 =0

(i)
0
t0 =1 êt ,0 (S1:t0 )
!2
(i)
A1:t−1 =0

1−

(i)
0
t0 =1 et ,0 (S1:t0 )

Qt−1

(i)
0
t0 =1 et ,0 (S1:t0 )
Qt−1
(i)
0
t0 =1 êt ,0 (S1:t0 )

Qt−1

!2
Qt−1
n
(i)
0
e2η0 X
t0 =1 et ,0 (S1:t0 )
≤
1 − Qt−1
,
(i)
n i=1
êt0 ,0 (S 0 )
0
t =1

34

1:t

!2

where on the last line we used Assumption 3. Finally, to bound this last term, notice that
from (45) and Assumption 3, we have inf t,k êt,k (·) ≥ 1 − η02T+1 almost surely. We now aim
e (S1:t )
− 1, we have
to bound this last term with Proposition 11. By taking Xt := êt,0
t,0 (S1:t )


et,0 (S1:t ) − êt,0 (S1:t )
|log(Xt + 1)| = log 1 +
êt,0 (S1:t )
1 − η0
1 − η0
≤ max{log(1 +
), − log(1 −
)}
2T − η0 − 1
2T − η0 − 1
2 − 2η0
≤
2T − η0 − 1
almost surely for T ≥ 2, where we use the fact that log(1 + x) < 2x for x > 0 and
log(1 − x) > −2x for 0 < x < 0.5.
We also have




E (log(Xt + 1))2 = E (log et,0 (S1:t ) − log êt,0 (S1:t ))2
h
i
2
≤ E 1/ (min{1 − η0 /T, 1 − (η0 + 1)/(2T )}) (et,0 (S1:t ) − êt,0 (S1:t ))2
≤

(1 − η0 /T )2 (1 − (η0 + 1)/(2T ))2
(min{1 − η0 /T, 1 − (η0 + 1)/(2T )})

2 Ce n

−2κe

≤ Ce n−2κe ,
|a−b|
where we have used the fact that |log a − log b| ≤ min{a,b}
for a, b > 0 by concavity, and
Assumption 3 and 5, and assuming (45) holds.
Given the above, applying Proposition (11), we see that for n > n(η, T ) where (44)
would hold for n(η, T ), we have for some constant C,

!2 
Qt−1
(i)
0
e
(S
)
0 =1 t ,0
1:t0
 ≤ Cn−2κe T 2 .
(46)
E  1 − Qtt−1
(i)
0
t0 =1 êt ,0 (S1:t0 )

Thus, we conclude that the first summand in the decomposition of errt (1) is bounded on
the order of T n−(κe +κδ ) in probability whenever (45) holds.
Meanwhile, the second and third summands used to construct errt (1) multiply errors in
êt0 ,0 (S1:t0 ) with noise terms that are mean-zero conditionally on Ft and Ft+1 respectively
thanks to Assumption 2 (recall that we assume that êt0 ,0 (S1:t0 ) has been trained on a separate
development set and so the predictive surface êt0 ,0 (·) may be taken as exogenous). They
can thus be bounded to order n−(1/2+κe ) whenever êt0 ,0 (S1:t0 ) is consistent in squared-error
loss at rate n−κe ; see, e.g., the proof of Lemma 4 of Athey and Wager (2017) for details.
Finally, the fourth term can again be bounded using Cauchy-Schwartz analogously to the
(i)
(i)
first one, noting that µnow,Wπ (S1:t+1 , t + 1) − µnext,Wπ (S1:t , t) is bounded on the order of
−κγ
n
by Assumption 6.
Thus, we find that there exists a constant C1 (δ) that depends the constants in Assumptions 5 and 6, as well as a problem-specific constant n1 (δ, η, T ) such that
errt ≤ C1 (δ)T n−(κe +min{1/2,

κδ , κγ })

with probability at least 1 − δ, for all n ≥ n1 (δ, η, T ) and on the event (45).
35

We now move to bounding errt (2). To this end, we first note that

1

(i)

b π,π := uπ,π Q A1:t−1 =0
G
t,i
t,i
(i)
t−1
0
t0 =1 êt ,0 (S1:t0 )
0

0

is Ft -measurable (again, recall that all nuisance components are taken to be exogenous),
and that it is uniformly bounded on the event (45),
0

b π,π ≤ e1+η0 ,
G
t,i

(47)

by Assumption 3. Given these preliminaries, we can bound errt (2) by expanding out the
square as follows:
!
n

1A(i)
1 X b π,π0 
(i)
(i)
t =Wπ
errt (2) ≤
G
µnow,Wπ (S1:t , t) − µ̂now,Wπ (S1:t , t)
1−
(i)
n i=1 t,i
et,Wπ (S1:t )
!
n


1 X b π,π0
1
1
(i)
(i)
+
G
−
1 (i)
Y − µnow,Wπ (S1:t , t)
(i)
(i)
n i=1 t,i At =Wπ
êt,Wπ (S1:t ) et,Wπ (S1:t )
n


1 X b π,π0
(i)
(i)
Gt,i 1A(i) =Wπ µ̂now,Wπ (S1:t , t) − µnow,Wπ (S1:t , t)
+
t
n i=1
!
1
1
×
−
(i)
(i)
êt,Wπ (S1:t ) et,Wπ (S1:t )
!
n

1A(i)
(i)
1 X b π,π0 
(i)
(i)
t =0,At+1 =Wπ
G
1−
+
µnext,Wπ (S1:t , t) − µ̂next,Wπ (S1:t , t)
(i)
(i)
n i=1 t,i
et,0 (S1:t )et+1,Wπ (S1:t+1 )
n


1 X b π,π0
(i)
+
Gt,i 1A(i) =0,A(i) =Wπ Y (i) − µnext,Wπ (S1:t , t)
t
t+1
n i=1
!
1
1
×
−
(i)
(i)
(i)
(i)
êt,0 (S1:t )êt+1,Wπ (S1:t+1 ) et,0 (S1:t )et+1,Wπ (S1:t+1 )
n


1 X b π,π0
(i)
(i)
Gt,i 1A(i) =0,A(i) =Wπ µ̂next,Wπ (S1:t , t) − µnext,Wπ (S1:t , t)
+
t
t+1
n i=1
!
1
1
−
×
.
(i)
(i)
(i)
(i)
êt,0 (S1:t )êt+1,Wπ (S1:t+1 ) et,0 (S1:t )et+1,Wπ (S1:t+1 )
Here, using Assumption 2, we again see that the 1st, 2nd and 4th terms are sums of regression
error multiplied by conditionally mean-zero noise and so, as argued above, decay at rates
n−(1/2+κe ) or n−(1/2+κµ ) depending on the rate of convergence of the nuisance components.
Meanwhile, the 3rd and 6th terms can be bounded by Cauchy-Schwartz, which imply that
they decay on the order of n−(κe +κµ ) . The last remaining term to bound is the 5th summand

36

in the above bound for errt (2), which we can further decompose as
n

... ≤



1 X b π,π0
(i)
Gt,i 1A(i) =0,A(i) =Wπ Y (i) − µnow,Wπ (S1:t+1 , t + 1)
t
t+1
n i=1
1

×

1

!

−
(i)
(i)
(i)
(i)
êt,0 (S1:t )êt+1,Wπ (S1:t+1 ) et,0 (S1:t )et+1,Wπ (S1:t+1 )
n


1 X b π,π0
(i)
(i)
Gt,i 1A(i) =0,A(i) =Wπ µnow,Wπ (S1:t+1 , t + 1) − µnext,Wπ (S1:t , t)
+
t
t+1
n i=1
!
1
1
.
×
−
(i)
(i)
(i)
(i)
êt,0 (S1:t )êt+1,Wπ (S1:t+1 ) et,0 (S1:t )et+1,Wπ (S1:t+1 )
The first of these two terms can again be bounded via Assumption 2; meanwhile, the second
one can be bounded by Cauchy-Schwartz to order n−(κe +κγ ) using Assumptions 5 and 6.
Tallying up all our bounds, we have found that, there is a constant C(δ) depending
on δ and the constants used in Assumptions 3, 5 and 6 such that, and a problem-specific
threshold n(δ, η, T ) such that
˜
ˆ
∆(π,
π 0 ) − ∆(π,
π 0 ) ≤ C(δ)T 2 n− min{1/2+κe , 1/2+κµ , κe +κµ , κe +κδ , κe +κγ } ,
with probability at least 1 − 3δ, for all n ≥ n(δ, η, T ). The desired conclusion follows.
Proof of Lemma 9. Following the proof of Lemma 1, we obtain an analogue to (10):
" T
#
X

∆(π, 0) = −E0
1t≥τπ , St 6=Φ Qπ,t (S1:t , 01:t ) − µπ,t (S1:t , 01:(t−1) ) ,

(48)

t=1

and again find that µnow,k (·) can be used to express the terms 1t≥τπ , St 6=Φ µπ,t (S1:t , 01:(t−1) ).
However, given the possibility of terminal states, we now need to account for the possibility
that the patient may enter the set Φ at time St+1 when characterizing terms involving the
Q-function. To this end, define policy πknow as follows: πknow (S1:t , At−1 ) = 0 if St ∈ Φ; else,
πknow (S1:t , At−1 ) = At−1 if At−1 6= 0, and otherwise πknow (S1:t , At−1 ) = k. Notice that this
now
policy satisfies Definition 1b and that, for all t ≥ τπ , our policy of interest π matches πW
.
π
Thus, we see that
now ,t (S1:t , 01:t )
1t≥τπ , St 6=Φ Qπ,t (S1:t , 01:t ) = 1t≥τπ , St 6=Φ QπW

π
Φ
now Y
= 1t≥τπ , St 6=Φ EπW
S
,
A
=
0
1:t
1:t
1:t = 1t≥τπ , St 6=Φ µnext,Wπ (S1:t , t),
π

where the last statement follows by analogous derivations to those used in the proof of
Lemma 1 under Assumptions 1 and 2.

B

Simulation Details and Results

First, we write out the weighted formula for the ADR estimator with terminal states. We
use this form in the experiments for the multiple-action setup where censoring due to death
is involved.
37

ˆ W (π, 0)Φ
∆

=

T
X

1A(i)

Pn

1:t−1

1A(i)

Pn

t=1

−

T
X
t=1

=0

1:t−1

1A(i)

i=1

T
X
t=1

1t≥τπ(i) 1St(i) 6=Φ 

−q(i)

−q(i)

(i)

(i)

µ̂now,Wπ (S1:t ) − µ̂next,Wπ (S1:t )Φ



i=1 Qt−1 ê−q(i),0 (S (i) )
1:t0
t0 =1 t0

Pn
+

=0

i=1 Qt−1 ê−q(i) (S (i) )
1:t0
t0 =1 t0 ,0

1A(i)

Pn

1

(i)
=0 At =Wπ
1:t−1
Qt−1 −q(i) (i) −q(i) (i)
(S1:t0 )êt,Wπ (S1:t ))
ê
t0 =1 t0 ,0

1:t−1

=0

π

i=1 Qt−1 ê−q(i) (S (i) )ê−q(i) (S (i) )
1:t
t,Wπ
1:t0
t0 =1 t0 ,0

Pn

1A(i) =0 1A(i)

i=1 Qt

1:t
−q(i)

t+1
(i)

−

−q(i)
(i)
µ̂now,Wπ (S1:t )

1A(i)

1A(i) =W
t

1t≥τπ(i) 1St(i) 6=Φ Y

!
(i)

1t≥τπ(i) 1St(i) 6=Φ + Qt−1

=0
1:t−1
(i)
−q(i)
(S1:t0 )
ê
t0 =1 t0 ,0

1S(i)



1 − 1t≥τ (i) 1S (i) 6=Φ
π

t



(i)
1t≥τ (i) 1S (i) 6=Φ Y (i) − Û −q(i) (S1:t
, Φ)

6=Φ
=Wπ
t+1
−q(i)
(i)

ê
(S1:t0 )êt+1,Wπ (S1:t+1 )
t0 =1 t0 ,0
Pn
i=1 A

π

t

,

+B+C

where
A = Qt

1A(i) =0 1A(i)

t+1 =Wπ

1:t

1S (i) 6=Φ
t+1

−q(i)
(i)
−q(i)
(i)
t0 =1 êt0 ,0 (S1:t0 )êt+1,Wπ (S1:t+1 )

1A(i) =0 1S (i) =Φ

B = Qt

1:t

t+1

−q(i)

(i)

t0 =1 êt0 ,0 (S1:t0 )

1A(i)

C = Qt−1

1:t−1 =0

(i)
−q(i)
t0 =1 êt0 ,0 (S1:t0 )

1t≥τπ(i) 1St(i) 6=Φ ,

1t≥τπ(i) 1St(i) 6=Φ ,



1 − 1t≥τ (i) 1S (i) 6=Φ .
π

t

For the binary treatment choices setup as described in Section 6.1, we define a linear thresholding rule θ1 St ≥ θ2 t + θ3 such that whenever this holds, we stop the treatment. We search
over three classes of policies:
• Policies that always stop after some time t, corresponding to θ1 = 0, θ2 = −1, θ3 ∈
[1, 2, · · · , T + 1] where T = 10 is the horizon length of the study.
• Policies that always stop once the patient’s state St is above some threshold, corresponding to θ1 = 1, θ2 = 0, θ3 ∈ [−0.5, 0, 0.5, · · · , 4.5, 5].
• Policies that depend on both the time and the patient’s state St , corresponding to
θ1 = 1, θ2 ∈ [−1/4, −1/3, −1/2, −1, −2, −3, −4], θ3 ∈ [1, 2, · · · , 15].
For the multiple treatment choices setup as descibed in Section 6.2, we consider the
linear thresholding class: θ1 Xt0 + θ2 Yt0 + θ3 t ≥ θ4 is the region in which we start treatment.
If in addition, θ5 Xt0 + θ6 Yt0 + θ7 t ≥ θ8 , we use the invasive treatment and otherwise, use the
non-invasive treatment. We search over the following classes of policies:
• Policies that always start treating at sometime, and always assign the non-invasive
treatment option, corresponding to θ1 = 0, θ2 = 0, θ3 = 1, θ4 ∈ [1, 2, · · · , T + 1] where
T = 10 is the horizon length of the study, θ5 = 0, θ6 = 0, θ7 = 0, θ8 = 1

38



• Policies that always start treating at sometime, and always assign the invasive treatment option, corresponding to θ1 = 0, θ2 = 0, θ3 = 1, θ4 ∈ [1, 2, · · · , T + 1] where
T = 10 is the horizon length of the study, θ5 = 0, θ6 = 0, θ7 = 1, θ8 = 0
• Policies that depend on both the time and the two covariates in the form of θ1 ∈
[0.2, 0.7, 1, 3, 5], θ2 = 1, θ3 ∈ [0, 1], θ4 ∈ [1, 3, 5, 7, 9], θ5 ∈ [0.1, 0.35, 0.5, 1.5, 2.5], θ6 =
1, θ7 ∈ [0, 1], θ8 ∈ [1, 3, 5, 7, 9].
• Policies that depend on both the time and the two covariates in the form of θ1 ∈
[0.2, 0.7, 1, 3, 5], θ2 = 1, θ3 ∈ [0, 1], θ4 ∈ [1, 3, 5, 7, 9], θ5 = −0.5, θ6 = 1, θ7 ∈ [0, 1], θ8 ∈
[−5, −2, 1, 4].

39

n
250
250
250
250
500
500
500
500
1000
1000
1000
1000
5000
5000
5000
5000
10000
10000
10000
10000
20000
20000
20000
20000
30000
30000
30000
30000

ν
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5

β
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5

σ
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3

ADR
8.39e-01
9.11e-01
8.32e-01
9.18e-01
8.73e-01
9.23e-01
8.69e-01
9.23e-01
8.78e-01
9.25e-01
8.73e-01
9.27e-01
8.81e-01
9.25e-01
8.78e-01
9.27e-01
8.81e-01
9.25e-01
8.78e-01
9.27e-01
8.81e-01
9.25e-01
8.78e-01
9.27e-01
8.81e-01
9.25e-01
8.78e-01
9.27e-01

IPW
8.38e-01
8.25e-01
8.33e-01
8.61e-01
8.49e-01
8.66e-01
8.62e-01
8.77e-01
8.71e-01
8.75e-01
8.70e-01
8.88e-01
8.79e-01
9.11e-01
8.75e-01
9.13e-01
8.80e-01
9.20e-01
8.76e-01
9.19e-01
8.81e-01
9.21e-01
8.77e-01
9.21e-01
8.81e-01
9.24e-01
8.77e-01
9.25e-01

Q-Opt
8.53e-01
8.79e-01
8.02e-01
8.78e-01
8.71e-01
8.79e-01
8.11e-01
8.84e-01
8.74e-01
8.87e-01
8.16e-01
8.84e-01
8.79e-01
8.98e-01
8.30e-01
8.91e-01
8.80e-01
8.99e-01
8.33e-01
8.93e-01
8.80e-01
9.01e-01
8.36e-01
8.95e-01
8.80e-01
9.02e-01
8.36e-01
8.93e-01

Oracle
8.78e-01
9.25e-01
8.76e-01
9.27e-01
8.78e-01
9.25e-01
8.76e-01
9.27e-01
8.78e-01
9.25e-01
8.76e-01
9.27e-01
8.78e-01
9.25e-01
8.76e-01
9.27e-01
8.78e-01
9.25e-01
8.76e-01
9.27e-01
8.78e-01
9.25e-01
8.76e-01
9.27e-01
8.78e-01
9.25e-01
8.76e-01
9.27e-01

MSE:ADR
1.70e-02
2.42e-02
3.51e-02
2.51e-02
5.50e-03
5.76e-03
3.23e-02
1.44e-02
4.50e-03
6.67e-03
2.25e-02
8.94e-03
1.29e-03
3.20e-03
8.48e-03
6.35e-03
8.70e-04
2.26e-03
4.72e-03
5.77e-03
5.51e-04
1.56e-03
2.96e-03
4.57e-03
2.63e-04
1.46e-03
2.20e-03
3.75e-03

MSE:IPW
8.63e-02
6.63e-02
8.27e-02
6.85e-02
8.11e-02
6.60e-02
7.54e-02
6.07e-02
8.06e-02
5.76e-02
7.19e-02
5.64e-02
5.56e-02
4.40e-02
5.35e-02
3.63e-02
4.33e-02
4.26e-02
3.07e-02
3.27e-02
3.09e-02
3.37e-02
1.56e-02
3.48e-02
1.87e-02
2.83e-02
1.18e-02
2.98e-02

Table 1: Detailed numerical results in the binary-action setup with β = 0.5.
In the fifth to the eighth columns, we show the value of the best learned
policy using ADR, weighted IPW, and Q-Opt against the value of the oracle
(oracle) best policy in the prespecified policy class, with all value
estimates evaluated using a Monte-Carlo rollout with 20000 repeats. In the
right two columns, we show the mean-squared error of the value estimates
averaged across all policies in the policy class. Results are averaged
across 50 runs and rounded to two decimal places. Numbers listed are
accurate up to the second displaying digit due to sampling errors.

40

n
250
250
250
250
500
500
500
500
1000
1000
1000
1000
5000
5000
5000
5000
10000
10000
10000
10000
20000
20000
20000
20000
30000
30000
30000
30000

ν
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5
0
0
0.5
0.5

β
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

σ
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3

ADR
7.65e-01
8.16e-01
7.48e-01
8.18e-01
7.86e-01
8.39e-01
7.56e-01
8.37e-01
7.88e-01
8.42e-01
7.63e-01
8.42e-01
7.97e-01
8.46e-01
7.68e-01
8.49e-01
8.00e-01
8.46e-01
7.69e-01
8.49e-01
8.00e-01
8.46e-01
7.69e-01
8.49e-01
8.00e-01
8.46e-01
7.70e-01
8.49e-01

IPW
7.21e-01
7.11e-01
7.25e-01
7.15e-01
7.50e-01
7.46e-01
7.28e-01
7.52e-01
7.74e-01
7.81e-01
7.41e-01
7.81e-01
7.94e-01
8.32e-01
7.60e-01
8.19e-01
7.98e-01
8.40e-01
7.64e-01
8.33e-01
8.01e-01
8.42e-01
7.67e-01
8.42e-01
8.00e-01
8.43e-01
7.68e-01
8.44e-01

Q-Opt
8.21e-01
8.17e-01
7.38e-01
8.11e-01
8.36e-01
8.26e-01
7.47e-01
8.17e-01
8.46e-01
8.31e-01
7.49e-01
8.16e-01
8.54e-01
8.39e-01
7.53e-01
8.22e-01
8.54e-01
8.42e-01
7.52e-01
8.23e-01
8.54e-01
8.42e-01
7.54e-01
8.21e-01
8.54e-01
8.41e-01
7.54e-01
8.22e-01

Oracle
8.03e-01
8.46e-01
7.79e-01
8.49e-01
8.03e-01
8.46e-01
7.79e-01
8.49e-01
8.03e-01
8.46e-01
7.79e-01
8.49e-01
8.03e-01
8.46e-01
7.79e-01
8.49e-01
8.03e-01
8.46e-01
7.79e-01
8.49e-01
8.03e-01
8.46e-01
7.79e-01
8.49e-01
8.03e-01
8.46e-01
7.79e-01
8.49e-01

MSE:ADR
1.56e-02
3.20e-02
1.07e-01
6.97e-02
4.70e-03
1.27e-02
7.20e-02
1.98e-02
3.09e-03
9.20e-03
6.16e-02
2.87e-02
1.62e-03
4.92e-03
2.46e-02
1.37e-02
9.82e-04
4.53e-03
1.45e-02
1.10e-02
3.54e-04
3.23e-03
7.74e-03
1.01e-02
3.22e-04
2.85e-03
7.99e-03
9.88e-03

MSE:IPW
3.32e-01
2.43e-01
3.28e-01
2.55e-01
3.16e-01
2.05e-01
3.21e-01
2.31e-01
2.87e-01
2.12e-01
2.82e-01
2.03e-01
2.05e-01
1.42e-01
1.58e-01
1.39e-01
1.69e-01
1.52e-01
9.67e-02
1.53e-01
9.76e-02
1.26e-01
5.15e-02
1.42e-01
9.42e-02
1.19e-01
5.27e-02
1.11e-01

Table 2: Detailed numerical results in the binary-action setup with β = 1;
details see caption of Table 1.

41

n
250
250
250
500
500
500
1000
1000
1000
5000
5000
5000
10000
10000
10000
20000
20000
20000

σ
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1

ADR
1.65e-01
1.24e-01
1.17e-01
1.88e-01
1.72e-01
1.48e-01
1.98e-01
1.99e-01
1.83e-01
2.35e-01
2.21e-01
2.26e-01
2.49e-01
2.36e-01
2.27e-01
2.47e-01
2.45e-01
2.35e-01

IPW
1.40e-01
9.13e-02
8.94e-02
1.14e-01
1.13e-01
9.30e-02
1.52e-01
1.40e-01
1.54e-01
1.94e-01
2.09e-01
1.75e-01
2.19e-01
2.06e-01
1.88e-01
2.20e-01
2.08e-01
2.22e-01

Q-Opt
5.88e-02
5.85e-02
8.67e-02
1.08e-01
1.00e-01
1.04e-01
1.38e-01
1.15e-01
1.38e-01
2.23e-01
2.02e-01
1.97e-01
2.43e-01
2.22e-01
2.15e-01
2.62e-01
2.39e-01
2.35e-01

Oracle
2.65e-01
2.67e-01
2.54e-01
2.65e-01
2.67e-01
2.54e-01
2.65e-01
2.67e-01
2.54e-01
2.65e-01
2.67e-01
2.54e-01
2.65e-01
2.67e-01
2.54e-01
2.65e-01
2.67e-01
2.54e-01

MSE:ADR
1.79e-01
1.83e-01
1.97e-01
8.74e-02
8.30e-02
9.00e-02
3.55e-02
4.28e-02
4.26e-02
7.04e-03
7.47e-03
8.21e-03
3.36e-03
3.95e-03
4.68e-03
1.75e-03
2.02e-03
2.92e-03

MSE:IPW
1.35e+00
1.26e+00
1.38e+00
6.39e-01
6.55e-01
6.12e-01
2.78e-01
3.28e-01
3.65e-01
5.99e-02
6.36e-02
6.43e-02
3.04e-02
3.27e-02
3.14e-02
1.51e-02
1.47e-02
1.46e-02

Table 3:
Detailed numerical results in the multiple-action setup. In the
third to the sixth columns, we show the value of the best learned policy
using ADR, IPW, and Q-Opt against the value of the oracle best policy in
the prespecified policy class, with all value estimates evaluated using
a Monte-Carlo rollout with 20000 repeats. In the right two columns, we
show the mean-squared error of the value estimates averaged across all
policies in the policy class. Results are averaged across 50 runs and
rounded to two decimal places. Numbers listed are accurate up to the
second displaying digit due to sampling errors.

42

