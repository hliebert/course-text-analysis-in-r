CHAPTER 3

The Econometrics of Randomized
Experimentsa
S. Athey*, x, 1, G.W. Imbens*, x, 1
*Stanford University, Stanford, CA, United States
x
NBER (National Bureau of Economic Research), Cambridge, MA, United States
1
Corresponding authors: E-mail: athey@stanford.edu; imbens@stanford.edu

Contents
1. Introduction
2. Randomized Experiments and Validity
2.1 Randomized experiments versus observational studies
2.2 Internal validity
2.3 External validity
2.4 Finite population versus random sample from superpopulation
3. The Potential Outcome/Rubin Causal Model Framework for Causal Inference
3.1 Potential outcomes
3.2 A classiﬁcation of assignment mechanisms
3.2.1
3.2.2
3.2.3
3.2.4

Completely randomized experiments
Stratiﬁed randomized experiments
Paired randomized experiments
Clustered randomized experiments

4. The Analysis of Completely Randomized Experiments
4.1 Exact p-values for sharp null hypotheses
4.2 Randomization inference for average treatment effects
4.3 Quantile treatment effects
4.4 Covariates in completely randomized experiments
5. Randomization Inference and Regression Estimators
5.1 Regression estimators for average treatment effects
5.2 Regression estimators with additional covariates
6. The Analysis of Stratiﬁed and Paired Randomized Experiments
6.1 Stratiﬁed randomized experiments: analysis
6.2 Paired randomized experiments: analysis
7. The Design of Randomized Experiments and the Beneﬁts of Stratiﬁcation
7.1 Power calculations
7.2 Stratiﬁed randomized experiments: beneﬁts
7.3 Rerandomization
8. The Analysis of Clustered Randomized Experiments
8.1 The choice of estimand in clustered randomized experiments
8.2 Point estimation in clustered randomized experiments
a

75
78
78
79
79
80
81
81
83
84
84
84
84

85
86
88
90
92
94
95
97
99
100
101
102
102
105
108
109
110
112

We are grateful for comments by Esther Duﬂo.

Handbook of Economic Field Experiments, Volume 1
ISSN 2214-658X, http://dx.doi.org/10.1016/bs.hefe.2016.10.003

© 2017 Elsevier B.V.
All rights reserved.

73

74

Handbook of Field Experiments

8.3 Clustered sampling and completely randomized experiments
9. Noncompliance in Randomized Experiments
9.1 Intention-to-treat analyses
9.2 Local average treatment effects
9.3 Generalizing the local average treatment effect
9.4 Bounds
9.5 As-treated and per protocol analyses
10. Heterogenous Treatment Effects and Pretreatment Variables
10.1 Randomized experiments with pretreatment variables
10.2 Testing for treatment effect heterogeneity
10.3 Estimating the treatment effect heterogeneity
10.3.1
10.3.2
10.3.3
10.3.4
10.3.5

Data-driven subgroup analysis: recursive partitioning for treatment effects
Nonparametric estimation of treatment effect heterogeneity
Treatment effect heterogeneity using regularized regression
Comparison of methods
Relationship to optimal policy estimation

11. Experiments in Settings With Interactions
11.1 Empirical work on interactions
11.2 The analysis of randomized experiments with interactions in subpopulations
11.3 The analysis of randomized experiments with interactions in networks
12. Conclusion
References

113
114
115
117
118
120
121
122
123
123
124
126
127
129
130
130

131
132
133
133
135
135

Abstract
In this chapter, we present econometric and statistical methods for analyzing randomized
experiments. For basic experiments, we stress randomization-based inference as opposed to
sampling-based inference. In randomization-based inference, uncertainty in estimates arises naturally
from the random assignment of the treatments, rather than from hypothesized sampling from a large
population. We show how this perspective relates to regression analyses for randomized experiments.
We discuss the analyses of stratiﬁed, paired, and clustered randomized experiments, and we stress the
general efﬁciency gains from stratiﬁcation. We also discuss complications in randomized experiments
such as noncompliance. In the presence of noncompliance, we contrast intention-to-treat analyses
with instrumental variables analyses allowing for general treatment effect heterogeneity. We consider,
in detail, estimation and inference for heterogenous treatment effects in settings with (possibly many)
covariates. These methods allow researchers to explore heterogeneity by identifying subpopulations
with different treatment effects while maintaining the ability to construct valid conﬁdence intervals.
We also discuss optimal assignment to treatment based on covariates in such settings. Finally, we
discuss estimation and inference in experiments in settings with interactions between units, both in
general network settings and in settings where the population is partitioned into groups with all
interactions contained within these groups.

Keywords
Causality; Potential outcomes; Random assignment; Randomized experiments; Regression analyses

The Econometrics of Randomized Experiments

JEL Codes
C01; C13; C18; C21; C52; C54

1. INTRODUCTION
Randomized experiments have a long tradition in agricultural and biomedical settings. In
economics, they have a much shorter history. Although there have been notable experiments over the years, such as the RAND health care experiment (Manning et al., 1987;
see the general discussion in Rothstein and von Wachter, 2016), the negative income tax
experiments (e.g., Robins, 1985), as well as randomized experiments in laboratory
settings (Kagel et al., 1995), it is only recently that there has been a large number of
randomized ﬁeld experiments in economics, and development economics in particular.
See Duﬂo et al. (2006) for a survey. As digitization lowers the cost of conducting experiments, we may expect that their use may increase further in the near future. In this
chapter, we discuss some of the statistical methods that are important for the analysis
and design of randomized experiments.
Although randomized experiments avoid many of the challenges of observational studies
for causal inference, there remain a number of statistical issues to address in the design and
analysis of experiments. Even in the simplest case with observably homogenous, independent subjects, where the experiment is evaluated by comparing sample means for the treatment and control group, there are questions of how to conduct inference about the
treatment effect. When there are observable differences in characteristics among units,
questions arise about how best to design the experiment and how to account for imbalances
in characteristics between the treatment and control groups in analysis. In addition, it may be
desirable to understand how the results of an experiment would generalize to different settings. One approach to this is to estimate heterogeneity in treatment effects; another is to
reweigh units according to a target distribution of characteristics. Finally, statistical issues arise
when units are not independent, as when they are connected in a network. In this chapter,
we discuss a variety of methods for addressing these and other issues.
A major theme of the chapter is that we recommend using statistical methods that are
directly justiﬁed by randomization, in contrast to the more traditional sampling-based
approach that is commonly used in econometrics. In essence, the sampling-based
approach considers the treatment assignments to be ﬁxed, while the outcomes are
random. Inference is based on the idea that the subjects are a random sample from a
much larger population. In contrast, the randomization-based approach takes the
subject’s potential outcomes (that is, the outcomes they would have had in each possible
treatment regime) as ﬁxed, and considers the assignment of subjects to treatments as
random. Our focus on randomization follows the spirit of Freedman (2006, p. 691),
who wrote: “Experiments should be analyzed as experiments, not as observational
studies. A simple comparison of rates might be just the right tool, with little value added

75

76

Handbook of Field Experiments

by ‘sophisticated’ models.” Young (2016) has recently applied randomization-based
methods in development economics.
As an example of how the randomization-based approach matters in practice, we
show that methods that might seem natural to economists in the conventional sampling
paradigm (such as controlling for observable heterogeneity using a regression model)
require additional assumptions in order to be justiﬁed. Using the randomization-based
approach suggests alternative methods, such as placing the data into strata according to
covariates, analyzing the within-group experiments, and averaging the results. This is
directly justiﬁed by randomization of the treatment assignment, and does not require
any additional assumptions.
Our overall goal in this chapter is to collect in one place some of the most important
statistical methods for analyzing and designing randomized experiments. We will start by
discussing some general aspects of randomized experiments, and why they are widely
viewed as providing the most credible evidence on causal effects. We will then present
a brief introduction to causal inference based on the potential outcome perspective.
Next we discuss the analysis of the most basic of randomized experiments, what we call
completely randomized experiments where, out of a population of size N, a set of Nt units
are selected randomly to receive one treatment and the remaining Nc ¼ N  Nt are
assigned to the control group. We discuss estimation of, and inference for, average as
well as quantile treatment effects. Throughout, we stress randomization-based rather
than model-based inference as the basis of understanding inference in randomized experiments. We discuss how randomization-based methods relate to more commonly used
regression analyses, and why we think the emphasis on randomization-based inference is
important. We then discuss the design of experiments, ﬁrst considering power analyses
and then turning to the beneﬁts and costs of stratiﬁcation and pairwise randomization,
as well as the complications from rerandomization. We recommend using experimental
design rather than analysis to adjust for covariates’ differences in experiments. Speciﬁcally,
we recommend researchers to stratify the population into small strata and then randomize
within the strata and adjust the standard errors to capture the gains from the stratiﬁcation.
We argue that this approach is preferred to model-based analyses applied after the randomization to adjust for differences in covariates. However, there are limits on how small the
strata should be: we do not recommend to go as far as pairing the units, because it complicates the analysis due to the fact that variances cannot be estimated within pairs, whereas
they can within strata with at least two treated and two control units. We also discuss in
detail methods for estimating heterogenous treatment effects. We focus on methods that
allow the researcher to identify subpopulations with different average treatment effects,
as well as methods for estimating conditional average treatment effects. In both cases, these
methods allow the researcher to construct valid conﬁdence intervals.
This chapter draws from a variety of literature, including the statistical literature on
the analysis and design of experiments, e.g., Wu and Hamada (2009), Cox and Reid

The Econometrics of Randomized Experiments

(2000), Altman (1991), Cook and DeMets (2008), Kempthorne (1952, 1955), Cochran
and Cox (1957), Davies (1954), and Hinkelmann and Kempthorne (2005, 2008). We
also draw on the literature on causal inference, both in experimental and observational
settings, Rosenbaum (1995, 2002, 2009), Rubin (2006), Cox (1992), Morgan and
Winship (2007), Morton and Williams (2010), Lee (2005), and Imbens and Rubin
(2015). In the economics literature, we build on recent guides to practice in randomized
experiments in development economics, e.g., Duﬂo et al. (2006), Glennerster (2016),
and Glennerster and Takavarasha (2013) as well as the general empirical microliterature
(Angrist and Pischke, 2009).
There have been a variety of excellent surveys of methodology for experiments in recent
years. Compared to Duﬂo et al. (2006), Glennerster and Takavarasha (2013), Bertrand and
Duﬂo 2016, Banerjee and Duﬂo 2009, and Glennerster (2016), this chapter focuses more
on formal statistical methods and less on issues of implementation in the ﬁeld. Compared to
the statistics literature, we restrict our discussion largely to the case with a single binary
treatment. We also pay more attention to the complications arising from noncompliance,
clustered randomization, and the presence of interactions and spillovers. Relative to the
general causal literature, e.g., Rosenbaum (1995, 2009) and Imbens and Rubin (2015),
we do not discuss observational studies with unconfoundedness or selection-onobservables in depth, and focus more on complications in experimental settings.
This chapter is organized as follows. In Section 2 we discuss some general issues
related to randomized experiments, followed by a discussion of causality and the potential
outcome framework or Rubin Causal Model in Section 3. In Section 4 we discuss the
analysis of the simplest form of randomized experiments, completely randomized experiments using randomization inference. In Section 5 we extend the randomization
analyses to regression estimators for completely randomized experiments. Next, in
Section 6 we discuss more complicated designs, stratiﬁed and paired experiments that
have superior power properties compared to completely randomized experiments. In
Section 7 we discuss the power implications of stratiﬁcation and pairing. In Section 8
we discuss the complications arising from cluster-level randomization. We discuss how
the use of clustering required the researcher to make choices regarding the estimands.
We also focus on the choice concerning the unit of analysis, clusters or lower-level units.
We recommend in general to focus on cluster-level analyses as the primary analyses.
Section 9 contains a discussion of noncompliance to treatment assignment and its relation
to instrumental variables methods. In Section 10 we present some recent results for
analyzing heterogeneity in treatment effects. Finally, in Section 11 we discuss violations
of the no-interaction assumption, allowing outcomes for one unit to be affected by treatment assignments for other units. These interactions can take many forms, some through
clusters, and some through general networks. We show that it is possible to calculate
exact p-values for tests of null hypotheses of no interactions while allowing for direct
effects of the treatments. Section 12 concludes.

77

78

Handbook of Field Experiments

2. RANDOMIZED EXPERIMENTS AND VALIDITY
In this section we discuss some general issues related to the interpretation of analyses of
randomized experiments and their validity. Following Cochran (1972), we deﬁne
randomized experiments as settings where the assignment mechanism does not depend
on characteristics of the units, either observed or unobserved, and the researcher has
control over the assignments. In contrast, in observational studies (Rosenbaum, 1995;
Imbens and Rubin, 2015), the researcher does not have control over the assignment
mechanism, and the assignment mechanism may depend on observed and or unobserved
characteristics of the units in the study. In this section we discuss four speciﬁc issues. First,
we elaborate on the distinction between randomized experiments and observational
studies. Second, we discuss internal validity, and third, external validity. Finally, we
discuss the issues related to ﬁnite population versus inﬁnite superpopulation inference.

2.1 Randomized experiments versus observational studies
There is a long tradition in viewing randomized experiments as the most credible of
designs to obtain causal inferences. Freedman (2006) writes succinctly “Experiments offer
more reliable evidence on causation than observational studies.” On the other hand,
some researchers continue to be skeptical about the relative merits of randomized
experiments. For example, Deaton (2010, p. 426) argues, that “I argue that evidence
from randomized experiments has no special priority. . Randomized experiments
cannot automatically trump other evidence, they do not occupy any special place in
some hierarchy of evidence.” Our view aligns with that of Freedman and others who
view randomized experiments as playing a special role in causal inference. A randomized
experiment is unique in the control the researcher has over the assignment mechanism,
and by virtue of that control, selection bias in comparisons between treated and control
units can be eliminated. That does not mean that randomized experiments can answer all
causal questions. There are a number of reasons why randomized experiments may not be
suitable to answer particular questions.
First, consider a case where we are interested in the causal effect of a particular
intervention on a single unit: what would the outcome have been for a particular ﬁrm
in the absence of a merger compared to the outcome given the merger. In that case,
and similarly for many questions in macroeconomics, no randomized experiment will
provide us with the answer to the causal question. Once the interest is in an intervention
that can be applied repeatedly, however, it may be possible to conduct experiments, or
ﬁnd data from quasi-experiments, even in macroeconomics. Angrist and Kuersteiner
(2011), building on work by Romer and Romer (2004), use the potential outcome
framework to discuss causal analyses in a macroeconomic time series context. Second,
it may not be ethical to conduct an experiment. In educational settings, it is often impossible to withhold particular educational services to individuals in order to evaluate their
beneﬁts. In such cases, one may need to do observational studies of some kind, possibly
randomizing inducements to participate in the programs.

The Econometrics of Randomized Experiments

2.2 Internal validity
In a classic text, Shadish et al. (2002) discuss various aspects of the validity of studies of
causal effects. Here we focus on two of the most important ones, internal validity
and external validity. Shadish et al. (2002) deﬁne a study to have internal validity if
the observed covariance between a treatment and an outcome reﬂects “a causal
relationship . in which the variables were manipulated,” (p. 53). Internal validity refers
to the ability of a study to estimate causal effects within the study population. Shadish
et al. (2002) then continue to observe that “the (internal validity) problem is easily solved
in experiments because they force the manipulation of A to come before the measurement
of B.” Essentially they argue that well-executed randomized experiments by deﬁnition
have internal validity, and that the problem of internal validity is one that plagues only
observational studies or compromised random experiments. This is not necessarily true
in experimental settings where interference between units is a concern.

2.3 External validity
The second aspect of validity that Shadish et al. (2002) consider is that of external validity.
They write that “external validity concerns inferences about the extent to which a causal
relationship holds over variation in persons, settings, treatments, and outcomes.” (p. 83).
Thus, external validity is concerned with generalizing causal inferences, drawn for a
particular population and setting, to others, where these alternative settings could involve
different populations, different outcomes, or different contexts.
Shadish, Cook, and Campbell argue for the primacy of internal validity, and claim that
without internal validity, causal studies have little value. This echos Neyman’s comment
that without actual randomization a study would have little value, as well as Fisher’s
observation that randomization was what he called “the reasoned basis” for inference. It
stands in sharp contrast with a few researchers who have recently claimed that there is
no particular priority for internal validity over external validity (e.g., Manski, 2013).
The ﬁrst important point is that external validity cannot be guaranteed, neither in
randomized experiments, nor in observational studies. Formally, one major reason
for that in experiments involving human subjects is that one typically needs informed
consent: individuals typically need to agree to participate in the experiment. There is
nothing that will guarantee that subjects who agree to do so will be similar to
those that do not do so, and thus there is nothing that can guarantee that inferences
for populations that give informed consent will generalize to populations that do
not. See also the discussion in Glennerster (2016). This argument has been used to
question the value of randomized experiments. However, as Deaton (2010, p. 449)
notes, the same concern holds for nonexperimental studies: “RCTs, like nonexperimental results, cannot automatically be extrapolated outside the context in which
they were obtained.” There is nothing in nonexperimental methods that makes
them superior to randomized experiments with the same population and sample size
in this regard.

79

80

Handbook of Field Experiments

Fundamentally, most concerns with external validity are related to treatment effect
heterogeneity. Suppose one carries out a randomized experiment in setting A, where
the setting may be deﬁned in terms of geographic location, or time, or subpopulation.
What value have inferences about the causal effect in this location regarding the causal
effect in a second location, say setting B? Units in the two settings may differ in observed
or unobserved characteristics, or treatments may differ in some aspect. To assess these
issues, it is helpful to have causal studies, preferably randomized experiments, in multiple
settings. These settings should vary in terms of the distribution of characteristics of the
units, and possibly in terms of the speciﬁc nature of the treatments or the treatment
rate, in order to assess the credibility of generalizing to other settings. An interesting
case study is the effect of microﬁnance programs. Meager (2015) analyzes data from seven
randomized experiments, including six published in a special issue of the American
Economic Journal (Applied) in 2015, and ﬁnds remarkable consistency across these
studies.
Another approach is to speciﬁcally account for differences in the distributions of
characteristics across settings. Hotz et al. (2005) and Imbens (2010) set up a theoretical
framework where the differences in treatment effects between locations arise from differences in the distributions of characteristics of the units in the locations. Adjusting for these
differences in unit-level characteristics (by reweighting the units) enables the researcher
to compare the treatment effects in different locations. Allcott (2015) assess the ability of
similar unconfoundedness/selection-on-observable conditions to eliminate differences in
treatment effects between 111 energy conservation programs. Recently developed
methods for assessing the treatment effect heterogeneity with respect to observables,
reviewed below in Section 10, can in principle be used to ﬂexibly estimate and conduct
inference about treatment effects conditional on observables.
Finally, Bareinboim et al. (2013) develop graphical methods to deal with external
validity issues.

2.4 Finite population versus random sample from superpopulation
It is common in empirical analyses to view the sample analyzed as a sample drawn
randomly from a large, essentially inﬁnite superpopulation. Uncertainty is viewed as
arising from this sampling, with knowledge of the full population leading to full knowledge of the estimands. In some cases, however, this is an awkward perspective. In some of
these cases the researcher observes all units in the entire population, and sampling uncertainty is absent. In other cases, it is not clear what population the sample can be viewed as
being drawn from.
A key insight is that viewing the statistical problem as one of causal inference allows
one to interpret the uncertainty as meaningful without any sampling uncertainty. Instead
the uncertainty is viewed as arising from the unobserved (missing) potential outcomes:
we view some units in the population exposed to one level of the treatment, but do

The Econometrics of Randomized Experiments

not observe what would have happened to those units had they been exposed to another
treatment level, leaving some of the components of the estimands unobserved. Abadie
(2014) discuss these issues in detail.
In part of the discussion in this chapter, therefore, we view the sample at hand as the
full population of interest, following the approaches taken by Fisher (1925, 1935),
Neyman et al. (1935), and subsequently by Rubin (1974, 1978, 2007). The estimands
are deﬁned in terms of this ﬁnite population. However, these estimands depend on all
the potential outcomes, some observed and others not observed, and as a result we cannot
infer the exact values of the estimands even if all units in the population are observed.
Consider an experiment with 10 individuals, ﬁve randomly selected to receive a new
treatment, and the remaining ﬁve assigned to the control group. Even if this group of
10 individuals is the entire population of interest, observing realized outcomes for these
10 individuals will not allow us to derive the estimand, say the difference in the average
outcome if all individuals were treated and the average outcome if all 10 individual were
to receive the control treatment, without uncertainty. The uncertainty is coming from
the fact that for each individual we can only see one of the two relevant outcomes. In
many cases the variances associated with estimators based on random assignment of the
treatment will be similar to those calculated conventionally based on sampling uncertainty. In other cases the conventional sampling-based standard errors will be unnecessarily conservative. When covariates are close to uncorrelated with the treatment assignment
(as in a randomized experiment), the differences are likely to be modest. See Abadie et al.
(2014) for details.

3. THE POTENTIAL OUTCOME/RUBIN CAUSAL MODEL FRAMEWORK
FOR CAUSAL INFERENCE
The perspective on causality we take in this chapter is associated with the potential
outcome framework (for a textbook discussion see Imbens and Rubin, 2015). This
approach goes back to Fisher (1925) and Neyman (1923). The work by Rubin (1974,
1975, 1978) led Holland (1986) to label it the Rubin Causal Model (RCM).

3.1 Potential outcomes
This RCM or potential outcome setup has three key features. The ﬁrst is that it associates
causal effects with potential outcomes. For example, in a setting with a single unit (say an
individual), and a single binary treatment, say taking a drug or not, we associate two potential outcomes with this individual, one given the drug and one without the drug. The
causal effect is the comparison between these two potential outcomes. The problem, and
in fact what Holland (1986) in a widely quoted phrase called the “fundamental problem
of causal inference” (Holland, 1986, p. 947), is that we can observe at most one of these
potential outcomes, the one corresponding to the treatment received. In order for these

81

82

Handbook of Field Experiments

potential outcomes to be well deﬁned, we need to be able to think of a manipulation that
would have made it possible to observe the potential outcome that corresponds to the
treatment that was not received, which led Rubin to claim “no causation without manipulation” (Rubin, 1975, p. 238). Because for any single unit we can observe at most one of
the potential outcomes, we need to observe outcomes for multiple units. This is the second feature of the potential outcomes framework, the necessity of the presence of multiple units. By itself the presence of multiple units does not solve the problem because
with multiple units the number of distinct treatments increases: with N units and two
treatment levels for each unit there are 2N different values for the full vector of treatments, with any comparison between two of them a valid causal effect. However, in
many cases, we are willing to make assumptions that interactions between units is limited
so that we can draw causal inferences from comparisons between units. An extreme
version of this is the assumption that the treatment for one unit does not affect outcomes
for any other unit, so that there is no interference whatsoever. The third key feature of
the RCM is the central role of the assignment mechanism. Why did a unit receive the
treatment it did receive? Here randomized experiments occupy a special place in the
spectrum of causal studies: in a randomized experiment the assignment mechanism is a
known function of observed characteristics of the units in the study. The alternatives,
where parts of the assignment mechanism are unknown, and may possibly depend on unobserved characteristics (including the potential outcomes) of the units, are referred to as
observational studies (Cochran, 1972).
There are alternative approaches to causality. Most notably there has been much work
recently on causal graphs, summarized in the book by Pearl (2000, 2009). In this
approach, causal links are represented by arrows and conditional independencies are
captured by the absence of arrows. These methods have been found useful in studies
of identiﬁcation questions as well as for using data to discover causal relationships among
different variables. However, the claims in this literature (e.g., Pearl, 2000, 2009) that the
concept of a causal effect does not require the ability to at least conceptually manipulate
treatments remains controversial.
Let us now add some speciﬁcs to this discussion. Suppose we start with a single unit,
say “I”. Suppose we have a binary treatment, denoted by W ˛ {0,1} for this unit, which
may correspond to taking a drug or not. The two potential outcomes are Y(0), the
outcome for me if I do not take the drug, and Y(1), the outcome if I do take the
drug. The causal effect is a comparison of the two potential outcomes, say the difference,
Y(1)Y(0), or the ratio, Y(1)/Y(0). Once we assign the treatment, one of the potential
outcomes will be realized and possibly observed:

Y ð0Þ if W ¼ 0;
Y obs ¼ Y ðW Þ ¼
Y ð1Þ if W ¼ 1

The Econometrics of Randomized Experiments

We can only observe one of the potential outcomes, so drawing credible and precise
inferences about the causal effect, say the difference Y(1)  Y(0), is impossible without
additional assumptions or information. Now let us generalize to the setting with N units,
indexed by i ¼ 1,.,N. Each of the units can be exposed to the two treatments, no drug
or drug, with Wi denoting the treatment received for unit i. Let W be the N-vector of
assignments with typical element Wi. The problem is that in principle the potential outcomes can depend on the treatments for all units, so that for each unit we have 2N
different potential outcomes Yi(W). In many cases it is reasonable to assume that the
potential outcomes for unit i depend solely on the treatment received by unit i. This
is an important restriction on the potential outcomes, and one that is unrealistic in
many settings, with a classic example being that of vaccinations for infectious diseases.
For example, exposing some students to educational interventions may affect outcomes
for their class mates, or training some unemployed individuals may affect the labor market
prospects for other individuals in the labor market. We will discuss the complications
arising from interactions in Section 11. Note that the interactions can be a nuisance
for estimating the effects of interest, but they can also be the main focus.
If we are willing to make the no-interference assumption, or sutva (stable unit treatment value assumption, Rubin, 1978), we can index the potential outcomes by the own
treatment only, and write without ambiguity Yi(w), for w ¼ 0,1. For each of the N units
the realized outcome is now Yiobs ¼ Yi ðWi Þ. Now with some units exposed to the
active treatment and some exposed to the control treatment, there is some hope for
drawing causal inferences. In order to do so, we need to make assumptions about the
assignment mechanism. To be formal, let Y be the range of values for the potential outcomes, and let X be the range of values for the covariates or pretreatment variables. In
general, we write this as a function
p : f0; 1gN  Y2N  XN 1½0; 1;
so that pðWjYð0Þ; Yð1Þ; XÞ is the probability of the assignment vector W, as a function
of all the potential outcomes and covariates.
We limit the general class of assignment mechanism we consider. The most important
limitation is that for randomized experiments we disallow dependence on the potential
outcomes, and we assume that the functional form of the assignment mechanism is
known. Analyzing observational studies where the assignment mechanism depends in
potentially complicated ways on the potential outcomes is often a challenging task, typically relying on controversial assumptions.

3.2 A classiﬁcation of assignment mechanisms
Let us consider four assignment mechanisms that we will discuss in subsequent sections in
this chapter.

83

84

Handbook of Field Experiments

3.2.1 Completely randomized experiments
In completely randomized experiment, a ﬁxed number of units, say Nt, is drawn at
random from the population of N units to receive the active treatment, with the remaining Nc ¼ N  Nt assigned to the control group. It satisﬁes
 1
N
X
N
pðWjYð0Þ; Yð1Þ; XÞ ¼
for all W such that
Wi ¼ Nt :
Nt
i¼1
3.2.2 Stratiﬁed randomized experiments
The next two experimental designs, stratiﬁcation and pairing, are intended to improve
the efﬁciency of the design by disallowing assignments that are likely to be uninformative
about the treatment effects of interest. In a stratiﬁed randomized experiment, we ﬁrst
partition the population on the basis of covariate values into G strata. Formally, if the covariate space is X, we partition X into X1 ; .; XG , so that Wg Xg ¼ X, and Xj XXg0 ¼
0
B if g s g . Let Gig be an indicator for unit i belonging to stratum g, so that
Gig ¼ 1Xi ˛Xg . Let Ng be the number of units in stratum g. Then we ﬁx the number
of treated
P units in each stratum as Nt,g, so that the total number of treated units is
Nt ¼ G
g ¼ 1 Nt;g . The assignment probability is then
1
G 
N
Y
X
Nj
pðWjYð0Þ; Yð1Þ; XÞ ¼
; for all W such that cg
Gig $ Wi ¼ Nt;g :
Nt;g
g¼1

i¼1

This design rules out some assignments that are allowed in a completely randomized
design, with the hope that the assignment vectors that are disallowed are relatively uninformative compared to assignment vectors that are allowed, for example where all
men are in the treatment group and all women in the control group.
3.2.3 Paired randomized experiments
In a paired randomized experiment, we pair units together and randomize within the
pairs. We can think of this as an extreme case of stratiﬁcation where each stratum contains
exactly one treated unit and exactly one control unit. In that case, there are G ¼ N/2
strata, and Ng ¼ 2 and Nt,g ¼ 1 for all g. Then
 N=2
N
X
1
pðWjYð0Þ; Yð1Þ; XÞ ¼
for all W such that cg;
Gig $ Wi ¼ 1:
2
i¼1
3.2.4 Clustered randomized experiments
The last design we discuss is not intended to be more informative than a completely
randomized experiment with the same sample size. Rather it is a design that attempts
to avoid complications with local interactions at the unit level, as well as disallow

The Econometrics of Randomized Experiments

assignments that are may be relatively expensive in terms of data collection, and thus indirectly may attempt to increase the sample size to improve precision. In a clustered randomized experiment, as in a stratiﬁed randomized experiments, we start with a
partitioning of the covariate space. Now, however, instead of assigning treatments
randomly to units within a cluster (the same as the stratum in the stratiﬁed randomized
experiment), treatments are assigned randomly to entire clusters, with all units within a
cluster receiving the same level of the treatment.
This design may be motivated by concerns that there are interactions between units.
For example, for educational programs, it may be that exposing some children in a classroom to an intervention has spillover effects on children in the same classroom who were
not exposed to the intervention. For that reason, it may make sense to expose all children
in a classroom or school to the same treatment. Alternatively, it may be expensive to
randomize at the individual level compared to randomizing at the classroom or
geographic unit level.
Again, let Gig denote the indicator for unit i belonging to cluster g, with G the total
number of clusters. Although we may vary the probability of a cluster being assigned to
the treatment group, here we focus on the simplest case where Gt out of the G clusters
are selected randomly to be assigned to the treatment group. P
Thus, at the cluster level, we
have a completely randomized experiment. Let W g ¼
Wi =Ng be the average
i:Gig ¼ 1

value of Wi for units in cluster g, so that the clustering implies that W g ˛ f0; 1g.
More generally, one may vary the probability of being assigned to the treatment by
cluster, without requiring that all units in the same cluster having the same treatment,
although we do not consider that case here. Then
 1
G
;
pðWjYð0Þ; Yð1Þ; XÞ ¼
Gt
P
for all W such that if Gig ¼ Gi0 g ¼ 1, then Wi ¼ Wi0 , and G
g ¼ 1 W g ¼ Gt .

4. THE ANALYSIS OF COMPLETELY RANDOMIZED EXPERIMENTS
In this section, we discuss the analysis of the simplest form of randomized experiments,
completely randomized experiments. In this setting, we have a sample of N units, Nt of
whom are selected at random to receive the active treatment, and the remaining
Nc ¼ N  Nt of whom receive the control treatment. We consider four sets of analyses.
First, we study the calculation of exact p-values for sharp hypotheses, based on the work
by Fisher (1925, 1935). Second, we consider estimation of and inference for average
treatment effects, following the original work by Neyman (1928, 1990) and Neyman
et al. (1935). Third, we study the relation between the Neyman approach and linear
regression, showing how randomization justiﬁes conventional regression analyses.
Fourth, we look at quantile treatment effects. Central to our discussion is the view

85

86

Handbook of Field Experiments

of the potential outcomes as ﬁxed, leading to a focus on inference based on the randomization distribution, keeping ﬁxed the total number of units assigned to treatment and
control. We will sometimes view the sample as identical to the population of interest,
and sometimes as a random sample from an inﬁnitely sized population of interest.
Although randomization inference is still relatively uncommon in economics, we view
it as strongly preferable for data from randomized experiments. The properties of the
methods follow directly from the design, without reliance on auxiliary modeling assumptions that are typically more difﬁcult to assess.
Initially, we focus on the case without pretreatment variables. In Section 4.4, we
allow for the presence of covariates but maintain the focus on global targets such as
the average effect of the treatment. In Section 10, we explore the beneﬁts of observing
covariates that are not affected by the treatment, also known as pretreatment variables.
We will illustrate some of the discussions with analyses of an experimental evaluation
of a labor market program, ﬁrst analyzed by Lalonde (1986). The data set contains information on 445 individuals, 185 in the treatment group, and 260 in the control group.
The outcome is post-training earnings, and pretreatment variables include lagged earnings and individual characteristics.

4.1 Exact p-values for sharp null hypotheses
The ﬁrst analysis is based on Fisher’s (1925, 1935) work on exact p-values for sharp null
hypotheses. See, for recent discussions, Rosenbaum (1995), Gail et al. (1988), Imbens and
Rubin (2015), and in economics Young (2016). Fisher was interested in testing sharp null
hypotheses, that is, null hypotheses under which we can infer all the missing potential
outcomes from the observed ones. The leading null hypothesis in this class is the null hypothesis that the treatment has no effect whatsoever, as
H0 : Yi ð0Þ ¼ Yi ð1Þ

c i ¼ 1; .; N:

(1)

The implicit alternative hypothesis is that there is at least one unit i such that Yi(0) s
Yi(1). Other sharp null hypothesis corresponds to known constant treatment effects, but
in many cases these are less interesting and natural. However, in some cases, one can use
the exact p-values in settings without sharp null hypotheses by redeﬁning the experiment,
as shown by Athey et al. (2015) in the context of network experiments (see Section 11.3
for further discussion).
Given the sharp null hypothesis, we can infer all the missing potential outcomes. As a
result we can infer, for any statistic that is a function of Yobs, W, and X, the exact distribution of that statistic under the null hypothesis. So, suppose we choose as our statistic the
difference in means by treatment status, as


1 X obs
1 X obs
obs
T ave W; Yobs ; X ¼ Y obs
¼
Yi 
Y :
(2)
t  Yc
Nt i:W ¼1
Nc i:W ¼0 i
i

i

The Econometrics of Randomized Experiments

We can calculate the probability, over the randomization distribution, of the statistic
taking on a value as large, in absolute value, as the actual value given the actual treatment
assigned. This calculation gives us the p-value for this particular null hypothesis:

 



p ¼ pr T ave W; Yobs ; X   T ave Wobs ; Yobs ; X  :
(3)
Let us illustrate this using data from National Supported Work program, previously
analyzed by Lalonde (1986), Dehejia and Wahba (1999) and many others. The simple
difference in average posttreatment earnings between treated and control is 1.79 (in
thousands of dollars). To calculate the p-value associated with this difference of 1.79,
we reassign the treatment, keeping the number of treated and control units ﬁxed at
185 and 240, respectively. Given the reassigned treatment, we calculate what the value
of the statistic would have been. Although the observed outcomes do not change for any
unit under the null hypothesis, the value of the statistic changes because who is in the
treatment group and who is in the control group changes. Repeating this many times,
we calculate the fraction of reassignment vectors that leads to a statistic that is at least
as large as 1.79 in absolute value. The p-value associated with this statistic is 0.0044, suggesting we should clearly reject the null hypothesis that the program had no effect on
earnings whatsoever.
The main choice to be made in this procedure is the choice of statistic. A natural statistic is the one we choose in the illustration, the difference in means by treatment status.
Another attractive choice is the difference in means of the ranks by treatment status. Here
the outcomes are ﬁrst converted to ranks, normalized to have zero mean as
!
N
N
X
X
 obs

1
N þ1
Ri ¼ R i; Y1 ; .; YNobs ¼
1þ
1Yjobs <Yiobs þ
1Yjobs ¼Yiobs 
:
2
2
j¼1
j¼1
The term in the middle deals with the presence of ties in the data. For the Lalonde
data, this statistic leads to a p-value of 0.01. In this application the robustness to outliers
does not actually buy very much, and the presence of many zeros has a bigger impact on
the difference between the mean and rank statistics.
This transformation improves the power of the tests in settings with outliers and
thick-tailed distributions. It is less arbitrary than, for example, simply transforming the
outcome by taking logarithms, especially in settings where such transformations are
not feasible, for example, in settings with thick-tailed distribution and a mass point at
zero. There are some settings where the transformation to ranks does not work well.
An example would be a case with a large proportion of zeros, and a very thick-tailed distribution for the outcomes for units with nonzero outcomes.
In some cases the researcher has multiple outcomes. One can calculate exact p-values
for each of the outcomes, but obviously the probability that at least one of the p-values is

87

88

Handbook of Field Experiments

less than 0.05 even if the treatment has no effect on any of the outcomes is generally
larger than 0.05. There are two modiﬁcations one can implement to address this. The
simplest is to modify the test statistic to take account of all the outcomes. For example,
one could use an F-statistic, that is, a quadratic form in the difference in average outcomes
by treatment status, with the inverse of the covariance matrix in the middle. For that statistic, one can calculate the exact p-value under the null hypothesis that there is no effect
of the treatment whatsoever using the Fisher randomization distribution. See for example
Young (2016). Alternatively one can use adjustments to the p-values to take account of
the multiple testing. Traditionally such adjustments are based on Bonferroni bounds.
However, there are tighter bounds available, although they tend to be more conservative
than the exact Fisher p-values. See Romano et al. (2010) for a review of this literature.
Rosenbaum (1995) discusses estimators for treatment effects based on rank statistics, as
opposed to simply doing tests, following Hodges and Lehmann (1970) and Doksum
(1974). Speciﬁcally, he looks for values for the common treatment effect that set the
rank correlation between the residuals and the treatment equal to zero, leading to conﬁdence intervals based on inverting test statistics.

4.2 Randomization inference for average treatment effects
In this section, we continue the analysis of completely randomized experiments, taking
as ﬁxed the potential outcomes in the population. Here, we follow the line of research
that originates in the work by Neyman (1923, 1990) and Neyman et al. (1935).
Neyman was interested in estimating the average effect of the treatment for the sample
at hand as
s ¼

N
1 X
ðYi ð1Þ  Yi ð0ÞÞ ¼ Y ð1Þ  Y ð0Þ:
N i¼1

(4)

In addition, Neyman was interested in constructing conﬁdence intervals for such
average effects. Initially, we focus purely on the ﬁnite sample, with no assumptions on
any sampling that may have led to the particular sample at hand.
As an estimator, Neyman proposed the difference in average outcomes by treatment
status as
1 X obs
1 X obs
obs
bs ¼ Y obs
where Y obs
¼
Yi ; and Y obs
¼
Y : (5)
t  Yc ;
t
c
Nt i:W ¼1
Nc i:W ¼0 i
i

Deﬁning

8
Nc
>
>
<
Nt
N
Di ¼ Wi 
¼
>
N
N
>
: t
N

i

if Wi ¼ 1
if Wi ¼ 0;

The Econometrics of Randomized Experiments

so that E½Di  ¼ 0, we can write this estimator as


N
1 X
N
N
bs ¼ s þ
Di $
$ Yi ð1Þ þ
$ Yi ð0Þ :
N i¼1
Nt
Nc

(6)

Because all potential outcomes are ﬁxed, the only stochastic components are the Di,
and with E½Di  ¼ 0, the second term has expectation zero, which immediately implies
that this estimator is unbiased for the average treatment effect, s. A more tedious calculation (e.g., Imbens and Rubin, 2015), shows that the sampling variance of bs , over the
randomization distribution, is
Vðbs Þ ¼

Sc2 St2 Stc2
þ  ;
Nc Nt N

(7)

where Sc2 and St2 are the variances of Yi(0) and Yi(1) in the sample, deﬁned as
Sc2 ¼

N
1 X
2
ðYi ð0Þ  Y ð0ÞÞ ;
N  1 i¼1

and St2 ¼

N
1 X
2
ðYi ð1Þ  Y ð1ÞÞ ;
N  1 i¼1

and Stc2 is the sample variance of the unit-level treatment effects, deﬁned as
Stc2 ¼

N
1 X
2
ðYi ð1Þ  Yi ð0Þ  ðY ð1Þ  Y ð0ÞÞÞ :
N  1 i¼1

We can estimate the ﬁrst two terms as
X 
1
Yi ð0Þ  Y obs
s2c ¼
c
Nc  1 i:W ¼0

2

¼

i

X 
1
Y obs  Y obs
c
Nc  1 i:W ¼0 i

2

;

i

and
s2t ¼

X 
1
Yi ð1Þ  Y obs
t
Nt  1 i:W ¼1
i

2

¼

X 
1
Y obs  Y obs
t
Nt  1 i:W ¼1 i

2

:

i

These estimators are unbiased for the corresponding terms in the variance of bs . The
third term, Stc2 [the population variance of the unit-level treatment effects Yi(1)Yi(0)] is
generally impossible to estimate consistently because we never observe both Yi(1) and
Yi(0) for the same unit. We therefore have no direct observations on the variation in
the treatment effects across the population and cannot directly estimate Stc2 .
In practice, researchers therefore use the estimator for Vðbs Þ based on estimating the
ﬁrst two terms by s2c and s2t , and ignoring the third term as
2
2
b neyman ¼ sc þ st :
V
Nc Nt

(8)

89

90

Handbook of Field Experiments

This leads in general to an upwardly biased estimator for Vðbs Þ, and thus to conservative conﬁdence intervals. There are two important cases where the bias vanishes. First,
if the treatment effect is constant, the third term is zero, and so ignoring it is immaterial.
Second, if we view the sample at hand as a random sample from an inﬁnite population,
then Vðbs Þ is unbiased for the variance of bs viewed as an estimator of the population
average treatment effectP
E½Yi ð1Þ  Yi ð0Þ, rather than as an estimator of the sample
average treatment effect N
i ¼ 1 ðYi ð1Þ  Yi ð0ÞÞ=N (See Imbens and Rubin, 2015).
To construct conﬁdence intervals, we do need to make large sample approximations.
One way to do this is to assume that the sample can be viewed as a random sample from a
large population and use a standard central limit theorem for independent and identically
distributed random variables. An alternative is to make assumptions on the properties of
the sequence of [Yi(0),Yi(1)] so that one can use a Lindenberg-type central limit theorem
for independent, but not identically distributed, random variables for the second term in
Eq. (6). The main condition is that the sequence of averages of the squares of Yi(0) þ
Yi(1) does not diverge. The large sample approximations do play a very different role
though than in standard discussions with random sampling. Most importantly, the
estimand is deﬁned in terms of the ﬁnite sample, not in terms of the inﬁnite
superpopulation.
For the Lalonde data, the estimate and Neyman standard error, which are up to a
degrees-of-freedom adjustment equal to the White robust standard errors (White,
1980; Eicker, 1967; Huber, 1967; Eckles et al., 2014), are
bs ¼ 1:794 ð sbe 0:671Þ:
The p-value based on the normal approximation to the distribution of the t-statistic is
0.0076, compared to an exact p-value of 0.0044 based on the Fisher approach.

4.3 Quantile treatment effects
Much of the theoretical as well as the empirical literature on treatment effects has focused on
average causal effects. An exception is Firpo (2007). However, there are other causal effects
that might be of interest. Of particular interest are quantile treatment effects. These can be
used as a systematic way to uncover treatment effects that may be concentrated in tails of
the distribution of outcomes, or to estimate more robustly constant treatment effects in settings with thick-tailed distributions. For this case, there are no ﬁnite sample results in the
spirit of Neyman’s results for the average treatment effect, so we focus on the case where
the sample can be viewed as a random sample from an inﬁnite population.
In general, let qY(s) denote the s-th quantile of the distribution of the random variable
Y. Formally,
qY ðsÞ ¼ inf 1FY ðyÞs :
y

The Econometrics of Randomized Experiments

Now deﬁne the s-th quantile treatment effect as the difference in quantiles between
the Yi(1) and Yi(0) distributions:
ss ¼ qY ð1Þ ðsÞ  qY ð0Þ ðsÞ:

(9)

Such quantile treatment effects have been studied in Doksum (1974) and Lehman
(1974), and more recently in Abadie et al. (2002), Chernozhukov and Hansen (2005),
Firpo (2007), and Bitler et al. (2002).
Note that ss is a difference in quantiles, and in general, it is different from the quantile
of the differences, that is, the corresponding quantile of the unit-level treatment effects,
qY(1)Y(0)(s). Speciﬁcally, although the mean of the difference of Yi(0) and Yi(0) is equal to
the difference in the means of Yi(1) and Yi(0), in general, the median of the difference
Yi(1)Yi(0) is not equal to the difference in the medians of Yi(1) and Yi(0). There are
three important issues concerning the quantile of the treatment effects in relation to
the differences in quantiles. First, the two estimands, qY(1)(s)qY(0)(s) and qY(1)Y(0)(s),
are equal if there is perfect rank correlation between the two potential outcomes. In
that case,

Yi ð1Þ ¼ FY1ð1Þ FY ð0Þ ðYi ð0ÞÞ :
A special case of this is that where the treatment effect is additive and constant. This
assumption is implausible in many settings. However, in general, it has no testable
implications.
The second, related, issue is that in general the quantile of the unit-level treatment
effects, qY(1)Y(0)(s), is not identiﬁed. Even with large-scale experiments, we can only
infer the two marginal distributions of Yi(0) and Yi(1). Nothing about the joint distribution that cannot be expressed in terms of these two marginal distributions can be inferred
from the data.
A third issue is the question which of the two quantile treatment effects, the
difference in quantiles, qY(1)(s)qY(0)(s), or the quantile of the difference, qY(1)Y(0)(s),
is the more interesting object for policymakers. To discuss that question, it is useful
to think about the possible decisions faced by a policy maker. If a policy maker is
committed to making one of the two treatments universal and is deciding between
exposing all units to the control treatment or to the active treatment, the answer should
depend only on the two marginal distributions, and not on aspects of the joint
distribution that cannot be expressed in terms of the two marginal distributions. This
suggests that the difference in quantiles may be a more natural object to consider,
although there are some cases, such as legal settings, where unit-level treatment effects
are of primary interest.
For these reasons, we focus on the difference in quantiles, ss. Inspecting this estimand
for different values of s may reveal that a particular treatment affects the lower or upper

91

92

Handbook of Field Experiments

tail more than the center of the distribution. In addition, in cases where the average
effects of the treatment may be imprecisely estimated because of thick-tailed distributions, quantile treatment-effect estimates may be very informative.
Here, we estimate quantile effects for the Lalonde data for the quantiles 0.10, 0.25,
0.50, and 0.75. For each quantile, we estimate the average effect and calculate standard
errors using the bootstrap. We also use the difference in quantiles as a statistic in an exact
p-value calculation (Table 1).
The results for the exact tests are quite different from those based on estimating the
effects and calculating standard errors. The reason is that the quantile estimates are far
from normally distributed. Mainly because of the 30% zeros in the outcome distribution,
the distribution of the difference in the lower quantiles has a substantial point mass at
zero. Because of the substantial proportion of individuals with zero earnings, the bootstrap standard error for the 0.10 quantile is essentially zero.

4.4 Covariates in completely randomized experiments
In this section, we discuss some additional analyses that a researcher may wish to carry out
if covariates are recorded for each unit. Later, we discuss regression methods, but here we
discuss some general principles. We focus here on the case where the randomization took
place without taking into account the covariates. In fact, as we discuss in Section 3.2.2, if
one has covariates observed prior to the randomization, one should modify the design of
the experiment and carry out a stratiﬁed randomized experiment rather than as a
completely randomized experiment. If one has a well-conducted randomized experiment where the randomization did not take into account the covariates, one does not
need regressors in order to estimate average treatment effects. The simple difference in
obs
means by treatment status, bs ¼ Y obs
t  Y c , is unbiased for the average effect. So,
the question is what the role is of covariates. There are two principal roles.
First, incorporating covariates may make analyses more informative. For example,
one can construct test statistics in the Fisher exact p-value approach that may have
more power than statistics that do not depend on the covariates. Similarly, by estimating
average treatment effects within subpopulations, and then averaging up the estimates
appropriately, the results will be more precise if the covariates are sufﬁciently strongly
Table 1 Estimates of quantile treatment effects for Lalonde data
Quantile
est
Bootstrap s.e.

Exact p-value

0.10
0.25
0.50
0.75
0.90

1.000
0.003
0.189
0.029
0.071

0.00
0.49
1.04
2.34
2.78

(0.00)
(0.35)
(0.90)
(0.91)
(1.97)

The Econometrics of Randomized Experiments

correlated with the potential outcomes. There is potentially a small-sample cost to
ex-post adjustment. For example, if the covariates are independent of the potential
outcomes, this ex-post adjustment will lower precision slightly. In practice, the gains
in precision tend to be modest.
Second, if the randomization was compromised, adjusting for covariate differences
may remove biases. Even if the original randomization was done appropriately, this
may be relevant if there are missing data and the analysis uses only complete cases
where there is no guarantee of ex-ante comparability between treated and control
units.
To illustrate this, let us consider the Lalonde data, and focus on the indicator that the
lagged earnings are positive as a covariate. The overall estimate of the average treatment
effect is
bs ¼ 1:79

ð sbe ¼ 0:67Þ:

For the individuals with positive prior earnings the effect is


bs p ¼ 1:69 sbe p ¼ 1:31 :
For the individuals with zero prior earnings the effect is
bs z ¼ 1:71

ð sbe z ¼ 0:74Þ:

Combining the two estimates leads to
bs ¼ bp $ bs p þ ð1  bp Þ $ bs z ¼ 1:70

ð sbe ¼ 0:66Þ;

with a standard error that is barely smaller than that without adjusting for positive prior
earnings, 0.67.
The two arguments regarding the role of covariates in the analysis of randomized
experiments also raise the question whether there is any reason to compare the covariate
distributions by treatment status as part of the analysis. There are a couple of reasons
why such a comparison may be useful. If there is some distance between the agencies
carrying out the original randomization and the researcher analyzing the data, it may be
useful as a check on the validity of the randomization to assess whether there are any
differences in covariates. Second, even as the randomization was carried out appropriately, it may be informative to see whether any of the key covariates were by chance
relatively imbalanced between treatment and control groups, so that prior to seeing
the outcome data an analysis can be designed that addresses these presumably modest
imbalances. Third, if there is reason to believe that the sample to be analyzed is not
identical to the population that was randomized, possibly because of attrition, or
item nonresponse with incomplete observations dropped from the analysis, it is useful
to assess how big the imbalances are that resulted from the sample selection. Table 2
presents the differences in covariates for the experimental Lalonde data.

93

94

Handbook of Field Experiments

Table 2 Covariates in Lalonde data
Average
Covariate

Treated

Controls

Difference

s.e.

Exact p-value

African-American
Hispanic
Age
Education
Married
No-degree
Earnings 1974
Unemployed 1974
Earnings 1974
Unemployed 1975

0.84
0.06
25.8
10.3
0.19
0.71
2.10
0.71
1.53
0.60

0.83
0.11
25.0
10.1
0.15
0.84
2.11
0.75
1.27
0.69

0.02
0.05
0.8
0.3
0.045
0.13
0.01
0.04
0.27
0.09

(0.04)
(0.03)
(0.7)
(0.2)
(0.04)
(0.04)
(0.50)
(0.04)
(0.31)
(0.05)

0.700
0.089
0.268
0.139
0.368
0.002
0.983
0.329
0.387
0.069

We see that despite the randomization, there is substantial evidence that the proportion of individuals with a degree in the treatment group is lower than in the control
group. This conclusion survives adjusting for the multiplicity of testing.

5. RANDOMIZATION INFERENCE AND REGRESSION ESTIMATORS
In this section, we discuss regression and more generally modeling approaches to estimation
and inference in the context of completely randomized experiments. Although these
methods remain the most popular way of analyzing data from randomized experiments,
we suggest caution in using them. Some of these comments echo the concerns raised by
others. For example, in the abstract of Freedman (2008), he writes “Regression adjustments
are often made to experimental data. Since randomization does not justify the models,
almost anything can happen” (Freedman, 2008; abstract) and similar comments are made
by Deaton (2010), Young (2016), and Imbens and Rubin (2015). Regression methods
were not originally developed for analyzing data from randomized experiments, and the
attempts to ﬁt the appropriate analyses into the regression framework requires some subtleties. In particular, there is a disconnection between the way the conventional assumptions
in regression analyses are formulated and the implications of randomization. As a result, it is
easy for the researcher using regression methods to go beyond analyses that are justiﬁed by
randomization, and end up with analyses that rely on a difﬁcult-to-assess mix of randomization assumptions, modeling assumptions, and large sample approximations. This is particularly true once one uses nonlinear methods. See for additional discussions Lesaffre and
Senn (2003), Samii and Aronow (2012), Rosenbaum (2002), Lin (2013), Schochet
(2010), Bloniarz et al. (2015), Young (2016), Wager et al. (2016), and Senn (1994).
Ultimately, we recommend that researchers wishing to use regression or other
model-based methods, rather than the randomization-based methods we prefer, do so

The Econometrics of Randomized Experiments

with care. For example, using only indicator variables based on partitioning the covariate
space, rather than using multivalued variables as covariates in the regression function
preserves many of the ﬁnite sample properties that simple comparisons of means have,
and leads to regression estimates with clear interpretations. In addition, in many cases
the potential gains from regression adjustment can also be captured by careful ex-ante
design, that is, through stratiﬁed randomized experiments to be discussed in the next
section, without the potential costs associated with ex-post regression adjustment.

5.1 Regression estimators for average treatment effects
In ordinary least squares, one regresses the observed outcome Yiobs on the indicator for
the treatment, Wi, and a constant:
Yiobs ¼ a þ s $ Wi þ εi ;

(10)

where εi is an unobserved error term. The least squares estimator for s is based on
minimizing the sum of squared residuals over a and s,
b ols Þ ¼ arg min
ðbs ols ; a
s;a

with solution

N 
X
2
Yiobs  a  s $ Wi ;
i¼1


obs  obs
Y
i¼1 ðWi  W Þ $ Yi
PN
2
i¼1 ðWi  W Þ

PN
bs ols ¼

obs
¼ Y obs
t  Yc ;

and

b ols ¼ Y obs  bs ols $ W :
a
The least squares estimate of s is identical to the simple difference in means, so by the
Neyman results discussed in Section 4.2, the least squares estimator is unbiased for the
average causal effect. However, the assumptions that are typically used to justify linear
regression are substantially different from the randomization that justiﬁes Neyman’s
analysis. In addition, the unbiasedness claim in the Neyman analysis is conceptually
different from the one in conventional regression analysis: in the ﬁrst case the repeated
sampling paradigm keeps the potential outcomes ﬁxed and varies the assignments,
whereas in the latter the realized outcomes and assignments are ﬁxed but different units
with different residuals, but the same treatment status, are sampled. The assumptions typically used in regression analyses are that, in the inﬁnite population the sample was drawn
from, the error terms εi are independent of, or at least uncorrelated with, the treatment
indicator Wi. This assumption is difﬁcult to evaluate, as the interpretation of these residuals is rarely made explicit beyond a vague notion of capturing unobserved factors
affecting the outcomes of interest. Textbooks therefore often stress that regression

95

96

Handbook of Field Experiments

estimates measure only association between the two variables, and that causal interpretations are not in general warranted.
It is instructive to see the formal implications of the randomization for the properties
of the least squares estimator, and to see how the randomization relates to the standard
versions of the regression assumptions. To build this connection between the two
repeated sampling paradigms, it is very convenient to view the sample at hand as a
random sample from an inﬁnite population. This allows us to think of all the variables
as random variables, with moments deﬁned as population averages and with a distribution
induced by random sampling from this inﬁnite population. Deﬁne
s ¼ E½Yi ð1Þ  Yi ð0Þ;

and a ¼ E½Yi ð0Þ:

Then deﬁne the residual as
εi ¼ Yi ð0Þ  a þ Wi $ fYi ð1Þ  Yi ð0Þ  sg
¼ ð1  Wi Þ $ fYi ð0Þ  E½Yi ð0Þg þ Wi $ fYi ð1Þ  E½Yi ð1Þg:
This implies we can write the regression as in Eq. (10). Now the error term has a clear
meaning as the difference between potential outcomes and their population expectation,
rather than as the difference between the realized outcome and its conditional expectation given the treatment. Moreover, the independence of Wi and [Yi(0),Yi(1)], directly
implied by the randomization, now has implications for the properties of the error
term. Speciﬁcally, the randomization implies that the average residuals for treated and
control units are zero:
E½εi jWi ¼ 0 ¼ 0;

and E½εi jWi ¼ 1 ¼ 0:

Note that random assignment of the treatment does not imply that the error term is
independent of Wi. In fact, in general, there will be heteroskedasticity, and we need to
use the Eicker-Huber-White robust standard errors to get valid conﬁdence intervals.
It may appear that this is largely semantics, and that using regression methods here
makes no difference in practice. This is certainly true for estimation in this simple case
without covariates, but not necessarily for inference. The conventional least squares
approach suggests using the robust (Eicker-Huber-White) standard errors. Because the
general robust variance estimator has no natural degrees-of-freedom adjustment, these
standard robust variance estimators differ slightly from the Neyman unbiased variance
b neyman :
estimator V
2
2
b robust ¼ sc $ Nc  1 þ st $ Nt  1 :
V
Nc
Nt
Nc
Nt

(11)

The Eicker-Huber-White variance estimator is not unbiased, and in settings where one
of the treatment arms is rare, the difference may matter. For the Duﬂo-Hanna-Ryan data

The Econometrics of Randomized Experiments

on the effect of teacher presence on educational achievement (Duﬂo et al., 2012), this
leads to
b obs
¼ 0:5805 þ 0:2154  Wi ;
Y
i
ð0:0256Þ ð0:0308Þ
½0:0311
with the Eicker-Huber-White standard errors in parentheses and the Neyman standard
error in brackets. Because both subsample sizes are large enough (Nc ¼ 54 and
Nt ¼ 53), there is essentially no difference in the standard errors. However, if we
modify the sample so that there are Nc ¼ 54 control units but only Nt ¼ 4 treated
units, the standard errors are quite different, 0.1215 for the Eicker-Huber-White
standard errors, and 0.1400 for the Neyman standard errors.
Although there are reﬁnements of the general Eicker-Huber-White variance
estimator, there are none that are unbiased in general. The difference with the Neyman
variance estimator relies on the fact that the only regressor in the Neyman variance
estimator is a binary indicator. Moreover, the Neyman variance estimator, ﬁtting into
the classic Behrens-Fisher problem, suggests using a t-distribution rather than a normal
distribution with the degrees of freedom dependent on the size of the two treatment
groups. See Imbens and Kolesar (2016) and Young (2016) for recent discussions with
illustrations how the distribution of the covariates matters for the standard errors.

5.2 Regression estimators with additional covariates
Now let us turn to the case with additional covariates beyond the treatment indicator Wi,
with these additional covariates denoted by Xi. These additional covariates are not
affected by the treatment by deﬁnition, that is, they are pretreatment variables. Moreover, we assume here that these covariates did not affect the assignment, which we
continue to assume is completely random. It is the presence of these covariates that often
motivates using regression methods rather than simple differences by treatment status.
There are generally three motivations for including these covariates into the analysis.
First, they may improve the precision of the estimates. Second, they allow for estimation
of average effects for subpopulations and in general for assessments of heterogeneity in
treatment effects. Third, they may serve to remove biases in simple comparisons of means
if the randomization was not adequate. These are somewhat distinct, although related,
goals, however, and regression methods are not necessarily the optimal choice for any
of them. In general, again, we wish to caution against the routine way in which regression methods are often applied here.
There are two ways covariates are typically incorporated into the estimation strategy.
First, they can be included additively through the regression model
Yiobs ¼ a þ s $ Wi þ b0 X_ i þ εi :

(12)

97

98

Handbook of Field Experiments

Here X_ i ¼ Xi  X is the covariate measured in deviations from its mean. Using
deviations from means does not affect the point estimates of s or b, only that of the
intercept a, but this transformation of the covariates is convenient once we allow for
interactions. Estimating this regression function for the Duﬂo-Hanna-Ryan data changes
the point estimate of the average effect to 0.1921 and leaves the standard error unchanged
at 0.0298. The R-squared in the original regression was 0.3362, and the two additional
covariates increase this to 0.3596, which is not enough to make a difference in the standard error.
Second, we can allow for a model with a full set of interactions:
Yiobs ¼ a þ s $ Wi þ b0 X_ i þ g0 X_ i $ Wi þ εi :

(13)

In general the least squares estimates based on these regression functions are not
unbiased for the average treatment effects over the randomization distribution given
the ﬁnite population. There is one exception. If the covariates are all indicators and
they partition the population, and we estimate the model with a full set of interactions,
Eq. (13), then the least squares estimate of s is unbiased for the average treatment effect.
To see this, consider the simplest case with a single binary covariate. In that case, we can
think of average treatment effects sx for each value of x. We can also think of bs x estimated separately on the corresponding part of the subpopulation. If X is the average
value of Xi in the sample, then
bs ¼ bs 1 $ X þ bs 0 $ ð1  X Þ;

b ¼ bs 1  bs 0 :
and g

Below in Section 10.3.1, we discuss machine-learning methods for partitioning the
covariate space according to treatment effect heterogeneity; if we construct indicators
for the element of the partition derived according to an “honest causal tree” (Athey
and Imbens, 2016) and incorporate them into Eq. (13), then the resulting average
treatment effect (estimated on what Athey and Imbens (2016) refer to as the estimation
sample) is unbiased over the randomization distribution. This result extends conceptually
to the case where all regressors are indicators. In that case, all least squares estimates are
weighted averages of the within-cell estimated average effects.
If we are willing to make large sample approximations, we can also say something about
the case with multivalued covariates. In that case, bs is (asymptotically) unbiased for the
average treatment effect. Moreover, and this goes back to the ﬁrst motivation for including
covariates, the asymptotic variance for bs is less than that of the simple difference estimator
by a factor equal to 1  R2 from including the covariates relative to not including the
covariates. It is important that these two results do not rely on the regression model being
true in the sense that the conditional expectation of Yiobs is actually linear in the covariates
and the treatment indicator in the population. Because of the randomization, there is zero
correlation in the population between Wi and the covariates Xi, which is sufﬁcient for
the lack of bias from including or excluding the covariates. However, the large sample
approximation needs to be taken seriously here. If in fact the covariates have very skewed
distributions, the ﬁnite sample bias in the linear regression estimates may be substantial, as

The Econometrics of Randomized Experiments

Freedman (2008) points out. At the same time, the gain in precision is often modest as the
covariates often only have limited explanatory power.
The presence of nonzero values for g imply treatment effect heterogeneity.
However, the interpretation of the g depends on the actual functional form of the
conditional expectations. Only if the covariates partition the population do, these g
have a clear interpretation as differences in average treatment effects. For that reason,
it may be easier to convert the covariates into indicator variables. It is unlikely that the
goodness of ﬁt of the regression model is much affected by such transformations, and
both the interpretation and the ﬁnite sample unbiasedness would be improved by
following that approach.
For the Duﬂo et al. (2012) data,
b obs
¼ 0:59 þ 0:192  Wi þ 0:001  X1i  0:004  X2i  0:006  X1i  Wi þ 0:017  X2i  Wi
Y
i
ð0:02Þ ð0:03Þ
ð0:002Þ
ð0:01Þ
ð0:003Þ
ð0:01Þ

The inclusion of the two covariates with the full set of interactions does not affect the
point estimate of the average treatment effect, nor its standard error.
Alternatively, if we run the regression with an indicator for X1i > 37 (teacher score
greater than the median), we get
b obs
¼ 0:60 þ 0:188  Wi þ 0:10  1fX1i >37g
Y
i
ð0:02Þ ð0:03Þ
ð0:05Þ

 0:06  1fX1i >37g  Wi
ð0:06Þ

Now the coefﬁcient on the interaction is directly interpretable as an estimate of the
difference in the average effect for teachers with a score higher than 37 versus teachers
with a score less than or equal to 37. Ultimately, there is very little gain in precision in
the estimator for the average treatment effect. If one wants to interpret the estimates
of the coefﬁcients on the interactions in addition to the main effect, one should take
into account the multiplicity of comparisons and adjust the p-values accordingly. See,
for example, Benjamini and Hochberg (1995).

6. THE ANALYSIS OF STRATIFIED AND PAIRED RANDOMIZED
EXPERIMENTS
In this section, we discuss the analyses for two generalizations of completely randomized experiments. First, consider stratiﬁed randomized experiments. In that case the covariate space is
partitioned into a ﬁnite set. Within each of these subsets, a completely randomized experiment is carried out. In the extreme case where the partition is such that within each subset,
there are exactly two units, and the designs correspond to randomly assigning exactly one
of these two units to the treatment and the other to the control group we have a paired randomized experiment. Such experiments are more powerful than completely randomized experiments. Both these designs can be thought of as attempting to capture the gains from
adjusting from observable differences between units by design, rather than by model-based
analysis as in the previous section. As such, they capture the gains from ex-post regression

99

100

Handbook of Field Experiments

adjustment without the potential costs of linear regression, and therefore stratiﬁcation is
generally to be preferred over regression adjustment. In the current section, we discuss the
analyses of such experiments, and in Section 7.2 the design aspects.

6.1 Stratiﬁed randomized experiments: analysis
In a stratiﬁed randomized experiment the covariate space is partitioned into a ﬁnite set of
subsets. Within each of these subsets a completely randomized experiment is carried out,
after which the results are combined. If we analyze the experiment using Neyman’s
repeated sampling approach the analysis of stratiﬁed randomized experiments is straightforward. Suppose there are G strata within which we carry out a completely randomized
experiment, possibly with varying treatment probabilities. Let sg be the average causal effect of the treatment for all units within stratum g. Within this stratum, we can estimate
the average effect as the difference in average outcomes for treated and control units:
obs
bs g ¼ Y obs
t;g  Y c;g ;

and we can estimate the within-stratum variance, using the Neyman results, as
s2t;g
s2c;j
 
b
þ
;
V bs g ¼
Nt;g Nc;g
where the j-subscript indexes the stratum. We can then estimate the overall average effect
of the treatment by simply averaging the within-stratum estimates weighted by the
stratum share Ng/N as
 2
G
G
X
X
 
Ng
Ng
b
b
bs g $
bs ¼
; with estimated variance V strat ðbs Þ ¼
V bs g $
:
N
N
g¼1
g¼1
There is a special case that is of particular interest. Suppose the proportion of treated
units is the same in all strata. In that case the estimator for the average treatment effect is
equal to the difference in means by treatment status,
bs ¼

G
X
g¼1

bs g $

Ng
obs
¼ Y obs
t  Yc ;
N

which is the estimator we used for the completely randomized experiment. In general,
however, the variance based on the completely randomized experiment setup,
2
2
b neyman ¼ st þ sc ;
V
Nt Nc

will be conservative compared to the variance that takes into account the stratiﬁcation,
b strat ðbs Þ: the latter takes into account the precision gain from stratiﬁcation.
V

The Econometrics of Randomized Experiments

6.2 Paired randomized experiments: analysis
Now let us consider a paired randomized experiment. Starting with N units in our sample, N/2 pairs are constructed based on covariate values so that within the pairs the units
are more similar in terms of covariate values. Then, within each pair a single unit is chosen at random to receive the active treatment and the other unit is assigned to the control
group. The average treatment effect within the pair is estimated as the difference in
outcome for the treated unit and the control unit as
X
X
bs g ¼
Yiobs 
Yiobs :
i:Gig ¼1;Wi ¼1

i:Gig ¼1;Wi ¼0

The overall average effect is estimated as the average over the within-pair estimates as
N=2
1 X
obs
bs ¼
bs g ¼ Y obs
t  Yc :
N=2 g¼1

So far, this is similar to the analysis of a general stratiﬁed experiment, and conceptually
the two designs are closely related.
The complications arise when estimating
variance of this estimator, as an estimator
Pthe
N=2
of the average effect over the strata, s ¼ g ¼ 1 sg $ 2=N. In the stratiﬁed randomized
experiment case, we estimated the variance in two steps, ﬁrst estimating the within-stratum variance for stratum g as
s2t;g
s2c;g
 
b
V bs g ¼
þ
;
Nt;g Nc;g
followed by averaging this over the strata. However, this variance estimator requires at
least two treated and at least two control units in each stratum, and thus is not feasible
in the paired randomized experiment case with only one treated and one control unit
in each stratum or pair.
Instead, typically the following variance estimator is used:
b sÞ ¼
Vðb

N=2
X
2

1
bs g  bs ;
N=2 $ ðN=2  1Þ g¼1

(14)

over the pairs. This variance estimator is conservative if viewed as
the variance of the bs g P
N=2
an estimator of s ¼ g ¼ 1 sg $ 2=N. However, suppose we view the pairs as being
randomly drawn from a large superpopulation, with population average treatment
effect equal to s ¼ E½sg . Then the variance of bs , viewed as an estimator of s , can
b s Þ.
be estimated using Vðb

101

102

Handbook of Field Experiments

Because in this case the proportion of treated units is the same in each pair, namely
1/2, we can also use the variance based on analyzing this as a completely randomized
experiment, leading to
2
2
b neyman ¼ st þ sc :
V
Nt Nc

(15)

In general, this will be conservative, and more so than necessary.
Let us illustrate this with data from the Children’s Television Workshop experiment.
See Imbens and Rubin (2015) for details. There are eight pairs of classrooms in this experiment, with one classroom in each pair shown the Electric Company, a children’s television program. The outcome is a posttest score, leading to


bs ¼ 13:4;
sbe pair ¼ 4:6 ;
where the standard error is calculated as in Eq. (14), taking into account the paired design.
The variance estimate based on the interpretation as a completely randomized experiment
as in Eq. (15), rather than a paired experiment, is sbe neyman ¼ 7:8, almost twice the size.
There is a substantial gain from doing the paired randomized experiment in this case.

7. THE DESIGN OF RANDOMIZED EXPERIMENTS AND THE BENEFITS
OF STRATIFICATION
In this section, we discuss some issues related to the design of randomized experiments.
First, we discuss the basic power calculations for completely randomized experiments. Second, we discuss the beneﬁts of stratiﬁcation, and its limit, pairwise randomization, in terms
of the expected precision of the resulting experiments. Finally, we discuss issues related to
rerandomization if one feels the randomization did not produce the desired balance in
covariates between treatment and control groups. Ultimately, our recommendation is
that one should always stratify as much as possible, up to the point that each stratum contains at least two treated and two control units. Although there are in principle some beneﬁts in terms of expected precision to using paired designs rather than stratiﬁed designs with
two treated and two control units, these tend to be small and because there are some real
costs in terms of analyses we recommend the stratiﬁed rather than paired designs. If the
stratiﬁcation is done appropriately, there should be no need for rerandomization.

7.1 Power calculations
In this section, we look at some simple power calculations for randomized experiments.
These are intended to be carried out prior to any experiment, in order to assess whether
the proposed experiment has a reasonable chance of ﬁnding results of the size that one
might reasonably expect. These analyses depend on a number of inputs, and can focus
on various outputs. Here we largely focus on the formulation where the output is the

The Econometrics of Randomized Experiments

minimum sample size required to ﬁnd treatment effects of a prespeciﬁed size with a prespeciﬁed probability. Alternatively, one can also focus on the treatment size one would
be likely to ﬁnd given a particular sample size. For details on these and similar calculations
a standard reference is Cohen (1988). See also Murphy et al. (2014).
Let us consider a simple case where for a sample of size N, we would observe values
for an outcome for the N units, Y1obs ; .; YNobs , and a treatment indicator W1, ., WN.
We are interested in testing the hypothesis that the average treatment effect is zero as
H0 : E½Yi ð1Þ  Yi ð0Þ ¼ 0;
against the alternative that the average treatment effect differs from zero as
Ha : E½Yi ð1Þ  Yi ð0Þs0:
We restrict the size of the test, the probability of rejecting the null hypothesis when it
is in fact true, to be less than or equal to a. Often, following Fisher (1925), we choose
a ¼ 0.05 as the statistical signiﬁcance level. In addition, we want the power of the
test, the probability of rejecting the null when it is in fact false, to be at least equal to
b, in the case where the true average P
treatment effect is s ¼ E½Yi ð1Þ  Yi ð0Þ for
some prespeciﬁed value of s. Let g ¼
Wi =N be the proportion of treated units.
i

For simplicity, we assume that the conditional outcome variance in each treatment
arm is the same, s2 ¼ VðYi ð0ÞÞ ¼ VðYi ð1ÞÞ. We look for the minimum sample size
N ¼ Nc þ Nt, as a function of a, b, s, s2, and g.
To test the null hypothesis of no average treatment effect, we look at the T-statistic as
obs

obs

obs

obs

Yt  Yc
Yt  Yc
ﬃ:
T ¼ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
z pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 =N þ s2 =N
2
2
s
t
c
S Y Nt þ S Y Nc
We reject the null hypothesis of no difference if the absolute value of this t-statistic,
jT j, exceeds F1(1  a/2). Thus, if a ¼ 0.05, the threshold would be F1(1  a/2) ¼
1.96. We want the rejection probability to be at least b, given that the alternative hypothesis is true with the treatment effect equal to s. In general, the difference in means minus
the true treatment effect s, scaled by the standard error of that difference, has approximately a standard normal distribution as
obs
obs
Yt  Yc  s
ﬃ z N ð0; 1Þ:
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2 =Nt þ s2 =Nc

This implies that the t-statistic has an approximately normal distribution as
!
s
T z N pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ; 1 :
s2 =Nt þ s2 =Nc

103

104

Handbook of Field Experiments

Now, a simple calculation implies that the null hypothesis will be rejected with probability as
!


s
1
1
pr jT j > F ð1  a=2Þ zF  F ð1  a=2Þ þ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s 2 Nt þ s 2 Nc
!
s
þF  F1 ð1  a=2Þ  qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :
s2 Nt þ s2 Nc
The second term is small, so we ignore it. Thus we want the probability of the ﬁrst
term to be equal to b, which requires
!
s
1
b ¼ F  F ð1  a=2Þ þ pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ;
s2 =Nt þ s2 =Nc
leading to

pﬃﬃﬃﬃﬃpﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s N ðgð1  gÞÞ
:
F ðbÞ ¼  F ð1  a=2Þ þ
s
1

1

This leads to a required sample size as
 1
2
F ðbÞ þ F1 ð1  a=2Þ
N ¼
:
ðs2 =s2 Þ $ g $ ð1  gÞ

(16)

For example, let us consider a setting close to the Lalonde data. The standard deviation of the outcome is approximately 6, although that may have been difﬁcult to assess
before the experiment. Suppose we choose g ¼ 0.5 (equal sample sizes for treated and
controls, which is optimal in the case with homoskedasticity, and typically close to
optimal in other cases), a ¼ 0.05 (test at 0.05 level). Suppose also that we are looking
to be able to ﬁnd an effect of 1 (1000 dollars), which is a substantial amount given the
average preprogram earnings of these individuals, and that we choose b ¼ 0.8 (power
of 0.8). Then
2
2
 1
 1
F ðbÞ þ F1 ð1  a=2Þ
F ð0:8Þ þ F1 ð0:975Þ
N ¼
¼
¼ 1;126;
0:1672 $ 0:52
ðs2 s2Y Þ $ g $ ð1  gÞ
so that the minimum sample size is 1,126, with 563 treated and 563 controls. If the
effect we wish to have power of 0.8 for is 2, then the required sample size would
be substantially smaller, namely 282, split equally between 141 treated and 141
controls.

The Econometrics of Randomized Experiments

7.2 Stratiﬁed randomized experiments: beneﬁts
In this section, we discuss the beneﬁts of stratiﬁcation in randomized experiments. Mostly
this discussion is based on the special case where the ratio of the number of treated units
to the total number of units is the same in each stratum. In this case the intended beneﬁt
of the stratiﬁcation is to achieve balance in the covariates underlying the stratiﬁcation.
Suppose there are only two strata, containing, respectively, women and men. If the total
sample size is 100, with 50 women and 50 men, and there are 60 individuals to be
assigned to the treatment group and 40 to the control group, stratiﬁcation would ensure
that in the treatment group there are 30 women and 30 men, and the 20 of each sex in
the control group. This would avoid a situation where, by chance, there were 25 women
and 35 men in the treatment group, and 25 women and 15 men in the control group. If
the outcomes were substantially correlated with the sex of the individual, such a random
imbalance in the sex ratio in the two treatment groups would reduce the precision from
the experiment. Note that without stratiﬁcation, the experiment would still be valid, and,
for example, still lead to exact p-values. Stratifying does not remove any bias, it simply
leads more precise inferences than complete randomization.
Although it is well known that stratiﬁcation on covariates is beneﬁcial if based on
covariates that are strongly correlated with the outcomes, there appears to be confusion
in the literature concerning the beneﬁts of stratiﬁcation in small samples if this correlation
is weak. Bruhn and McKenzie (2009) document this in a survey of researchers in development economics, but the confusion is also apparent in the statistics literature. For
example, Snedecor and Cochran (1967,1989, p. 101) write:
If the criterion has no correlation with the response variable, a small loss in accuracy results from
the pairing due to the adjustment for degrees of freedom. A substantial loss may even occur if the
criterion is badly chosen so that member of a pair are negatively correlated.

Box et al. (2005, p. 93) also suggest that there is a tradeoff in terms of accuracy or variance in the decision to stratify, writing:
Thus you would gain from the paired design only if the reduction in variance from pairing outweighed the effect of the decrease in the number of degrees of freedom of the t distribution.

This is somewhat counterintuitive: if one stratiﬁes on a covariate that is independent
of all other variables, then stratiﬁcation is obviously equivalent to complete randomization. In the current section, we argue that this intuition is correct and that in fact there is
no tradeoff. We present formal results that show that in terms of expected-squared error,
stratiﬁcation (with the same treatment probabilities in each stratum) cannot be worse than
complete randomization, even in small samples, and even with little, or even no, correlation between covariates and outcomes. Ex ante, committing to stratiﬁcation can only
improve precision, not lower it. There are two important qualiﬁcations to this result.
First, ex post, given the joint distribution of the covariates in the sample, a particular

105

106

Handbook of Field Experiments

stratiﬁcation may be inferior to complete randomization. Second, the result requires that
the sample can be viewed as a (stratiﬁed) random sample from an inﬁnitely large population, with the expectation in the expected-squared-error taken over this population.
This requirement guarantees that outcomes within strata cannot be negatively correlated.
The lack of any ﬁnite sample cost to (ex-ante) stratiﬁcation in terms of expectedsquared-error contrasts with the potential cost of ex-post stratiﬁcation, or regression
adjustment. Ex-post adjustment for covariates through regression may increase the ﬁnite
sample variance, and in fact it will strictly increase the variance for any sample size, if the
covariates have no predictive power at all.
However, there is a cost to stratifying on a variable that has no association with the
potential outcomes. Although there is no cost to stratiﬁcation in terms of the variance,
there is a cost in terms of estimation of the variance. Because there are unbiased estimators
for the variance, it follows that if the variance given stratiﬁcation is less than or equal to
the variance without stratiﬁcation, it must be that the expectation of the estimated variance given stratiﬁcation is less than or equal to the expectation of the estimated variance
without stratiﬁcation. However, the estimator for the variance given stratiﬁcation typically has itself a larger variance, related to the degrees of freedom adjustment. In our
view, this should not be interpreted, however, as an argument against stratiﬁcation.
One can always use the variance that ignores the stratiﬁcation: this is conservative if
the stratiﬁcation did in fact reduce the variance. See Lynn and McCulloch (1992) for a
similar argument in the context of paired randomized experiments.
We state the formal argument for a simpliﬁed case where we have a single binary covariate, Xi ˛ {f,m} (females and males). We start with a large (inﬁnitely large) superpopulation that expectations and variances refer to. We will draw a sample of size N from
this population and then assign treatments to each unit. For simplicity, we assume that
N/4 is an integer. Each unit is characterized by a triple [Yi(0), Yi(1), Xi], where Xi is
a binary indicator. In the superpopulation Xi has a binomial distribution with support
{f, m} (females and males) with pr(Xi ¼ f ) ¼ 1/2. Let mft ¼ E½Yi ð1ÞjXi ¼ f ,
mfc ¼ E½Yi ð0ÞjXi ¼ f , mmt ¼ E½Yi ð1ÞjXi ¼ m, and mmc ¼ E½Yi ð0ÞjXi ¼ m, and
similarly for the variances.
We consider the following sampling scheme. We randomly sample N/2 units from
each of the two strata. Given a sample of X1,.,XN we consider two randomization
schemes. In the ﬁrst, we randomly select N/2 units out of the sample of N units to be
assigned to the treatment group. We refer to this as the completely randomized assignment, C. Second, we consider the following stratiﬁed randomization scheme, denoted by
S. For the stratiﬁed design randomly select N/4 from each stratum to be assigned to the
treatment, and assign the remainder to the control group. In both cases, we estimate the
average treatment effect as
obs
bs ¼ Y obs
t  Yc :

The Econometrics of Randomized Experiments

We consider the properties of this estimator over repeated randomizations, and
repeated random samples from the population. It follows trivially that under both designs,
the estimator is unbiased for the population average treatment effect under the randomization distribution. The differences in performance between the estimators and the designs are solely the result of differences in the variances. The exact variance for a
completely randomized experiment can be written as



2
2
1
1  2
VC ¼
þ
$ mfc  mmc þ mft  mmt
$ sft þ s2fc
4$N
N

1  2
$ smt þ s2mc :
þ
N
The variance for the corresponding stratiﬁed randomized experiment is

1  2
1  2
VS ¼
$ sft þ s2fc þ
$ smt þ s2mc :
N
N
Thus, the difference in the two variances is

1
VC  VS ¼
$
mfc  mmc
4$N

2


þ mft  mmt

2


 0:

Therefore, stratiﬁcation leads to variances that cannot be higher than those under a
completely randomized experiment. There can only be equality if neither of the potential outcomes is correlated with the covariate, and mfc ¼ mmc and mft ¼ mmt. This is the
main argument for our recommendation that one should always stratify.
The inability to rank the conditional variance is useful in understanding the Snedecor
and Cochran’s quote in the introduction. If the strata are deﬁned in terms of a continuous
covariate than in a particular sample, it is possible that stratiﬁcation leads to larger variances conditional on the covariate values (and in the special case of paired experiments,
to negative correlations within pairs). That is not possible on average, that is, over
repeated samples randomly drawn from large strata, rather than conditional on the covariate values in a single sample. As mentioned before, the large strata qualiﬁcation here is
important: if the strata we draw from are small, say litters of puppies, it may well be
that the within-stratum correlation is negative, but that is not possible if all the strata
are large: in that case the correlation has to be nonnegative.
Now let us consider two estimators for the variance. First deﬁne, for w ¼ c, t, and
x ¼ f, m,

X
2
1
s2xw ¼
and
Yiobs  Y obs
xw
Nxw  1 i:W ¼1 ;X ¼x
i
i
fw¼tg
X 
2
1
:
s2w ¼
Yiobs  Y obs
w
Nw  1 i:W ¼1
i

fw¼tg

107

108

Handbook of Field Experiments

The natural estimator for the variance under the completely randomized experiment is
2
2
b C ¼ sc þ st ;
V
Nc Nt

b C ¼ VC :
with E V

For a stratiﬁed randomized experiment the natural variance estimator, taking into account the stratiﬁcation, is
!
 2

2
2
s
s
N
Nm
smc
s2mt
f
fc
ft
b
b S ¼ VS :
VS ¼
þ
$
þ
$
þ
with E V
Nf þ Nm
Nfc Nft
Nf þ Nm
Nmc Nmt
b C . Nevertheless, in a particular sample, with values (Y, W, X),
b S   E½ V
Hence, E½ V
it may well be the case that the realized value of the completely randomized variance estib C ðY; W; XÞ is less than that of the stratiﬁed variance V
b S ðY; W; XÞ. To be more
mator V
speciﬁc, consider the case where the stratiﬁcation is not related to the potential outcomes
b S   E½ V
b C , but the
at all. In that case the two variances are identical in expectation, E½ V
b S is larger than the variance of V
b C , Vð V
b S Þ > Vð V
b C Þ. As a result the
variance of V
b S will be slightly lower than the power of a t-test based
power of a t-test based on V
b C . Nevertheless, in practice, we recommend to always stratify whenever possible.
on V

7.3 Rerandomization
Suppose one is conducting a randomized experiment. For the study population the
researcher has collected some background characteristics and has decided to assign the
units to the treatment or control group completely at random. Although this would in
general not be optimal, it may be that the researcher decided it was not worth the effort
investigating a better design, and just went ahead with the complete randomization.
Now, however, suppose that after the random assignment has been decided, but prior
to the actual implementation of the assignment, the researcher compares average pretreatment values by treatment status. In expectation, these should be identical for all
covariates, but obviously in reality these will differ somewhat. Now suppose that one
of the most important covariates does actually show a substantial difference between
the assigned treatment and control group. It need not be, although it may be statistically
signiﬁcant at conventional levels even if the randomization was done properly, simply
because there is a substantial number of covariates, or simply by chance. What should
one do in that case? More speciﬁcally, should one go back to the drawing board and
rerandomize the treatment assignment so that the important covariate is better balanced?
This question of rerandomization has received some attention in the empirical development literature. One paper that raised the question forcefully in this literature is

The Econometrics of Randomized Experiments

Bruhn and McKenzie (2009). Theoretical papers discussing some of the formal aspects are
Morgan and Rubin (2012) and Banerjee et al. (2016).
Here, we offer some comments. First of all, implicitly many designs for randomized
experiments can be thought of as based on rerandomization. Consider the case where the
population of N ¼ 100 individuals consists of 50 women and 50 men. Suppose we do a
completely randomized experiment, with 60 individuals to be assigned to the treatment
group and the remaining 40 assigned to the control group. Now suppose we reject and
rerandomize any randomization vector that does not correspond to 30 men and 30
women being assigned to the treatment group. Then, in an indirect manner, we end
up with a stratiﬁed randomized experiment that we know how to analyze, and that in
general offers better sampling properties in terms of variance. The point is that in this
case the rerandomization does not create any complications, although the appropriate
analysis given the rerandomization is different from the one based on ignoring the rerandomization. Speciﬁcally, p-values need to be adjusted for the rerandomization, although
ignoring the adjustment simply leads to conservative p-values. Both statements hold more
generally.
In order for the subsequent analysis to be able to take account of the rerandomization, however, the details of the rerandomization need to be spelled out. This is most
easily seen if we consider a Fisher exact p-value analysis. In order to calculate the exact
p-value, we need to know the exact distribution of the assignment vector. In the case of
possible rerandomization, we would therefore need to know exactly which assignment
vectors would be subject to rerandomization, and which would be viewed as acceptable. The actual criterion may be complicated, and involve calculation of t-statistics
for differences in average covariates between treatment groups, but it needs to be
completely spelled out in order for the exact p-value calculation to be feasible. Doing
this is ultimately equivalent to designing an experiment that guarantees more balance,
and it would most likely take a form close to that of a stratiﬁed randomized experiment.
We recommend simply taking care in the original design so that assignments that correspond to unacceptable balance are ruled out from the outset, rather than ruled out ex
post, which complicates inference.

8. THE ANALYSIS OF CLUSTERED RANDOMIZED EXPERIMENTS
In this section, we discuss clustered randomized experiments. Instead of assigning
treatments at the unit level, in this setting the population is ﬁrst partitioned into a number
of clusters. Then all units in a cluster are then assigned to the same treatment level.
Clusters may take the form of schools, where within a school district a number of schools
are randomly assigned to an educational intervention rather than individual students, or
villages, or states, or other geographical entities. For general discussions, see Donner
(1987), Gail et al. (1996), and Murray (2012).

109

110

Handbook of Field Experiments

Given a ﬁxed sample size, this design is in general not as efﬁcient as a completely randomized design or a stratiﬁed randomized design. The motivation for such clustered designs is different. One motivation is that in some cases there may be interference between
units at the unit level. If there is no interference between units in different clusters, then
the cluster-level randomization may allow for simple, no-interference type analyses,
whereas a unit-level analysis would require accounting for the within-cluster interactions. A second motivation is that in many cases it is easier to sample units at the cluster
level. For the same cost, or level of effort, it may be therefore be possible to collect data
on a larger number of units.
In practice, there are quite different settings where clustered randomization may take
place. In some cases the number of units per cluster is similar, for example in educational
settings where the clusters are classrooms. In other settings where clusters are geographical
units, e.g., states, or towns, there may be a substantive amount of variation in cluster size.
Although theoretically this does not make much of a difference, in practice it can affect
what effective strategies are available for dealing with the clustering. In the ﬁrst case, our
main recommendation is to include analyses that are based on the cluster as the unit of
analysis. Although more sophisticated analyses may be more informative than simple analyses using the clusters as units, it is rare that these differences in precision are substantial,
and a cluster-based analysis has the virtue of great transparency. Analyzing the data at the
unit level has the beneﬁt that one can directly take into account unit-level characteristics.
In practice, however, including unit-level characteristics generally improves precision by
a relatively modest amount compared to including cluster-averages as covariates in a cluster-level analysis, so our recommendation is to focus primarily on cluster-level analyses.
For the second case where there is substantial variation in cluster sizes, a key component
of our recommended strategy is to focus on analyses with cluster averages as the target in
addition to analysis with unit averages that may be the main target of interest. The former
may be much easier to estimate in settings with a substantial amount of heterogeneity in
cluster sizes.

8.1 The choice of estimand in clustered randomized experiments
As before, let G be the number of clusters, and let Gig ˛f0; 1g, i ¼P
1, ., N, g ¼ 1, .,G
denote the binary indicator that unit i belongs to cluster g. Ng ¼ N
i ¼ 1 Gig is the number of units in cluster g, so that Ng/N is the share of cluster g in the sample. Wi continues
to denote the treatment assignment for unit i, but now W g denotes the average value of
the treatment assignment for all units in cluster g, so that by the deﬁnition of clustered
randomized assignment, W g ˛f0; 1g. Let Gt be the number of treated clusters, and
Gc ¼ G  Gt the number of control clusters.

The Econometrics of Randomized Experiments

The ﬁrst issue in clustered randomized experiments is that there may be different estimands to consider. One natural estimand is the overall population average treatment
effect,
spop ¼

N
1 X
ðYi ð1Þ  Yi ð0ÞÞ;
N i¼1

where we average over all units in the population. A second estimand is the unweighted
average of the within-cluster average effects as
s

C

G
1 X
¼
sg ;
G g¼1

where sg ¼

1 X
ðYi ð1Þ  Yi ð0ÞÞ:
Ng i:G ¼1
ig

We can think of sC as a weighted average of the unit-level treatment effects, with the
weight for units in cluster g proportional to the inverse of the cluster sample size, 1/Ng.
Similarly, the population average treatment effect can be thought of as a weighted
average of the cluster-level average treatment effects with weights proportional to the
cluster sample size Ng.
There are two issues regarding the choice of estimand. One is which of the
estimands is of most substantive interest. In many cases, this will be the unweighted, population average treatment effect spop. A second issue is the ease and
informativeness of any analysis. Because the randomization is at the cluster level, simply
aggregating unit-level outcomes to cluster-level averages simpliﬁes the analysis substantially: all the methods developed for completely randomized experiments apply directly
to a cluster-level analysis for clustered randomized experiments. In addition, inferences
for sC are often much more precise than inferences for spop in cases where there are a
few large clusters and many small clusters. Consider an extreme case where there is one
extremely large cluster that in terms of size is larger than the other clusters combined.
Inference for spop in that case is difﬁcult because all the units in this mega cluster will
always be in the same treatment group. Inference for sC , on the other hand, may
well be precise. Moreover, if the substantive question is one of testing for the presence
of any treatment effect, answering this question by focusing on a statistic that averages
over clusters without weighting is just as valid as comparing weighted averages over
clusters.
In practice a researcher may therefore want to report analyses for spop in combination
with analyses for sC . In cases where spop is the estimand that is of most substantive interest, the more precise inferences for sC may complement the noisy analyses for the substantively more interesting spop.

111

112

Handbook of Field Experiments

8.2 Point estimation in clustered randomized experiments
Now let us consider the analysis of cluster randomized experiments. We focus on the case
where unit-level outcomes and possibly covariates are available. The ﬁrst choice facing
the researcher concerns the choice of the unit of analysis. One can analyze the data at
the unit level or at the cluster level. We ﬁrst do the latter, and then return to the former.
If we are interested in the average effect sC , we can directly use the methods for
completely randomized experiments discussed in Section 4. Let Y obs
g be the average of
the observed outcomes in cluster g. We can simply average the averages for the treated
and control clusters as
1 X obs
1 X obs
bs C ¼
Yg 
Yg :
Gt
Gc
g:W g ¼1

g:W g ¼0

The variance of this estimator can be estimated as
s2
s2
 
b bs C ¼ C;c þ C;t ;
V
Gc
Gt
where the variance for the averages of
s2C;c ¼

1
Gc  1

X
g:W g ¼0

0
@Y obs  1
g
Gt

X

12
obs
Y g0 A ;

g0 :W g0 ¼1

and similarly for s2C;c . We can also get the same estimates using regression methods for the
regression function as
obs

Yg

¼ a þ sC $ W g þ hg :

(17)

We can generalize the speciﬁcation of this regression function to include cluster-level
covariates, including cluster characteristics or averages of unit-level characteristics.
Using a unit-level analysis obtaining an estimate for sC is more complicated. Consider
the regression
Yiobs ¼ a þ s $ Wi þ εi :

(18)

We can estimate this regression function using weighted least squares with the
weight for unit i, belonging to cluster g(i), equal to 1/Ng(i) as in the Cox (1956) analysis
of weighted randomized experiments. This weighted least squares estimator is identical
to bs C .
Now consider the case where we are interested in spop. In that case, we can estimate
the regression function in Eq. (18) without any weights. Alternatively, we can get the
same numerical answer by estimating the regression (Eq. 17) at the cluster level with

The Econometrics of Randomized Experiments

weights proportional to the cluster sample sizes Ng. To get the variance for the estimator
for the population average treatment effect, we can use the unit-level regression, but we
need to take into account the clustering. We can do so using the robust clustering
b and bs be the least squares esstandard errors proposed by Liang and Zeger (1986). Let a
b  bs $ Wi be the residual.
timators for a and s based on Eq. (18), and let bε i ¼ Yiobs  a
Then the covariance matrix for ðb
a ; bs Þ can be estimated as
0
!1 X
N 
G
X
X  bε i 
1
Wi
@
Wi Wi2
Wi $ bε i
i¼1
g¼1 i:Cig ¼1
1
!1
N 
X  bε i 0
X
1
W
i
A

:
Wi Wi2
Wi $ bε i
i:Cig ¼1

i¼1

The key difference with the Eicker-Huber-White robust standard errors is that before
taking the outer product of the product of the residuals and the covariates they are
summed up within clusters. This cluster-robust variance estimator is implemented in
many regression software packages, sometimes with ad-hoc degrees of freedom
adjustments.
If we compare unit-level and cluster-level analyses in the form described so far, our
preference is for cluster-level analysis, as it is more transparent and more directly linked to
the randomization framework. However, unit-level analysis allows the analyst to impose
additional modeling assumptions; for example, a unit-level regression can incorporate
covariates and impose additional assumptions, such as restricting the effect of covariates
to be common across clusters. If justiﬁed, imposing such restrictions can increase
efﬁciency. One could accomplish the same goal by ﬁrst doing a unit-level regression
and constructing residuals for each unit, and then performing cluster-level analysis on
the residuals, but at that point the inference would become more complex and depart
from the pure randomization-based analysis, reducing the beneﬁts of a cluster-based
approach.

8.3 Clustered sampling and completely randomized experiments
A second issue related to clustering is that the original sample may have been obtained
through clustered sampling. This issue is discussed in more detail in Abadie et al.
(2016). Suppose we have a large population. The population is divided into G clusters,
as in the previous discussion. Instead of a random sample from this population, we ﬁrst
sample a number of clusters from the population of clusters. Within each of the sampled
clusters, we sample a ﬁxed fraction of the units within that cluster. Given our sample, we
conduct a completely randomized experiment, without regard to the cluster these units
belong to.

113

114

Handbook of Field Experiments

There is a subtle issue involved in deﬁning what the estimand is. The ﬁrst alternative is
to focus on the sample average treatment effect, that is, the average difference for the two
potential outcomes over all the units in the sample. A second alternative is to analyze the
population average treatment effect for all the units in the population, including those in
nonsampled clusters. For both alternatives, the simple difference in average outcomes by
treatment status is unbiased for the estimand.
Abadie et al. (2016) show that we are interested in the sample average treatment effect, we can ignore the clustering and use the conventional Neyman variance estimator
discussed in Section 4. In contrast, if we are interested in the population average treatment effect, we need to take into account the implications of the clustering sampling
design. We can adjust the standard errors for the clustered sampling by using the Liang
and Zeger (1986) clustered standard errors.

9. NONCOMPLIANCE IN RANDOMIZED EXPERIMENTS
Even if a randomized experiment is well designed, there may be complications in the
implementation. One of the most common of these complications is noncompliance.
Some units assigned to the treatment group may end up not taking the treatment, and
some units assigned to the treatment group may manage to acquire the active treatment.
If there are only violations of the treatment assignment of the ﬁrst type, we refer to it as
one-sided noncompliance. This may arise when individuals assigned to the control
groups can be effectively be embargoed from the active treatment. If some units assigned
to the control group do manage to receive the active treatment, we have two-sided
noncompliance.
The concern is that noncompliance is not random or accidental, but the result of systematic differences in behavior or characteristics between units. Units that are assigned to
the treatment but that choose not receive it may do so because they are different from the
units assigned to the treatment that do receive it. These differences may be associated with
the outcomes of interest, thereby invalidating simple comparisons of outcomes by treatment received. In other words, the randomization that validates comparisons by treatment
status does not validate comparisons by posttreatment variables such as the treatment
received. These issues come up both in randomized experiments as well as in observational
studies. The general term for these complications in the econometric literature is endogeneity of the receipt of treatment. Random assignment ensures that the assignment to treatment is exogenous, but it does not bear on the exogeneity of the receipt of treatment if the
receipt of treatment is different from the assignment to treatment.
In this chapter, we discuss three distinct approaches to dealing with noncompliance,
all of which are valid under fairly weak assumptions. First, one can ignore the actual
receipt of the treatment and focus on the causal effects of assignment to the treatment,
in an intention-to-treat analysis. Second, we can use instrumental variables methods to

The Econometrics of Randomized Experiments

estimate the local average treatment effect, the causal effect of the receipt of treatment for
the subpopulation of compliers. Third, we can use a partial identiﬁcation or bounds analysis to obtain the range of values for the average causal effect of the receipt of treatment
for the full population. Another approach, not further discussed here, is the randomization-based approach to instrumental variables developed in Imbens and Rosenbaum
(2005). There are also two types of analyses that require much stronger assumptions in
order to be valid. The ﬁrst of these is an as-treated analysis, where units are compared
by the treatment received; this relies on an unconfoundedness or selection-on-observables assumption. A second type of analysis is a per protocol analysis, where units are
dropped who do not receive the treatment they were assigned to.
We need some additional notation in this section. Let Zi ˛ {0,1} denote the
randomly assigned treatment. We generalize the notation for the treatment received,
to reﬂect its status as an (intermediate) outcome. Let Wi(z) ˛ {0,1} denote the potential
treatment outcome given assignment z, with Wiobs ¼ Wi ðZi Þ the realized value for the
treatment received. For the outcome of primary interest, there are different setups
possible. One approach, e.g., Angrist et al. (1996), is to let Yi(z, w) denote the potential
outcome corresponding to assignment z and treatment received w. Alternatively, we
~ i ðzÞ denoting the
could index the potential outcomes solely by the assignment, with Y
outcome corresponding to the treatment assigned to unit i. The two notations are closely
~ i ðzÞ ¼ Yi ðz; Wi ðzÞÞ. Here we mainly use the ﬁrst setup. The realized
related, with Y
obs
~ i ðZi Þ. To simplify notation, we index sample
outcome is Yi ¼ Yi ðZi ; Wi ðZi ÞÞ ¼ Y
sizes, averages, and variances by 0,1 when they are indexed by values of the assignment
Zi, and by c, t when they are indexed by values of the treatment received Wi. For
example, Y obs
0;t is the average of the observed outcome for units assigned to the control
group (Zi ¼ 0) but who received the active treatment (Wiobs ¼ 1).

9.1 Intention-to-treat analyses
In an intention-to-treat analysis, the receipt of treatment is ignored, and outcomes are
compared by the assignment to treatment (Imbens and Rubin, 2015; Fisher et al.,
1990). The intention-to-treat effect is the average effect of the assignment to treatment.
In terms of the notation introduced above, the estimand is
s

itt

N
1 X
¼
ðYi ð1; Wi ð1ÞÞ  Yi ð0; Wi ð0ÞÞÞ:
N i¼1

We can estimate this using the difference in averages of realized outcomes by treatment assignment as
1 X obs
obs
obs
bs itt ¼ Y obs

;
where
¼
Y
for z ¼ 0; 1:
Y
Y
1
0
z
Nz i:Z ¼z i
i

115

116

Handbook of Field Experiments

To construct valid conﬁdence intervals for sitt, we can use the standard methods discussed in Section 4.2. The exact variance for bs itt is
 
S2 S2 S2
V bs itt ¼ 0 þ 1  01 ;
N0 N1 N
where S02 and S12 are the variances of Yi[0, Wi(0)] and Yi[1, Wi(1)] in the sample, deﬁned as
S02 ¼

N
1 X
2
ðYi ð0; Wi ð0ÞÞ  Y ð0ÞÞ ; and
N  1 i¼1

S12 ¼

N
1 X
2
ðYi ð1; Wi ð1ÞÞ  Y ð1ÞÞ ;
N  1 i¼1

2 is the sample variance of the unit-level treatment effects, deﬁned as:
and S01
2
S01
¼

N
1 X
2
ðYi ð1; Wi ð1ÞÞ  Yi ð0; Wi ð0ÞÞ  ðY ð1Þ  Y ð0ÞÞÞ :
N  1 i¼1

We can estimate the ﬁrst two terms as
X 
1
s20 ¼
Y obs  Y obs
0
N0  1 i:Z ¼0 i

2

;

i

and
s21 ¼

X 
2
1
Yiobs  Y 1 :
N1  1 i:Z ¼1
i

2 is generally impossible to estimate
As discussed in Section 4.2, the third term, S01
consistently because we never observe both Yi[1, Wi(1)] and Yi[0, Wi(0)] for the same
unit. In practice, we therefore use the estimator for Vðbs itt Þ based on estimating the ﬁrst
two terms by s20 and s21 ; and ignoring the third term as
2
2
 
b bs itt ¼ s0 þ s1 :
V
N0
N1

This leads to valid conﬁdence intervals in large samples, justiﬁed by the randomization
and sutva without additional assumptions.
The main drawback associated with the intention-to-treat approach is that the corresponding estimand is typically not the object of primary interest. The researcher may
be interested in settings where the assignment mechanism may be different, and the incentives for individuals to take the treatment might change. For example, in medical drug
trials the compliance rate is often very different from what would happen if a drug is

The Econometrics of Randomized Experiments

released to the general population. In the trial phase individuals, knowing that the efﬁcacy of the drug has not been established may be more likely to stop adhering to the protocol. As a result the intention-to-treat effect would not provide much guidance to the
effects in the new setting. In other words, intention-to-treat effects may have poor
external validity. The presumption is that causal effects of the receipt of treatment are
more generalizable to other settings, though of course there is no formal result that proves
that this is so.

9.2 Local average treatment effects
An alternative approach that deals directly with the noncompliance is to use instrumental
variables methods and related methods based on principal stratiﬁcation (Frangakis and
Rubin, 2002; Barnard et al., 1998). Bloom (1984), Zelen (1979, 1990), Baker (2000),
Baker and Lindeman (1994), and Cuzick et al. (1997) contain early and independent
discussions of the instrumental variables approach, some in the special case of one-sided
noncompliance, and Imbens and Angrist (1994) and Angrist et al. (1996) develop the
general setup in the potential outcomes framework. See also Imbens and Rubin
(2015) and Lui (2011) for textbook discussions and Baker et al. (2016) for a biostatistical
perspective. The ﬁrst step is to consider the possible patterns of compliance behavior. Let
Ci ˛ {c, d, a, n} denote the compliance behavior, where
8
c if Wi ð0Þ ¼ 0; Wi ð1Þ ¼ 1;
>
>
<
d if Wi ð0Þ ¼ 1; Wi ð1Þ ¼ 0;
Ci ¼
a if Wi ð0Þ ¼ 1; Wi ð1Þ ¼ 1;
>
>
:
n if Wi ð0Þ ¼ 0; Wi ð1Þ ¼ 0;
where c stands for complier, d for deﬁer, n for never-taker, and a for always-taker. These
labels are just deﬁnitional, not requiring any assumptions.
Now we consider two key assumptions. The ﬁrst is monotonicity (Imbens and
Angrist, 1994), or no-deﬁance, which requires
Wi ð1Þ  Wi ð0Þ:
This rules out the presence of deﬁers, units that always (that is, whether assigned to
control or treatment) do the opposite of their assignment. In the setting we consider
in this chapter, where the instrument is the random assignment to treatment, this appears
a very plausible assumption: assigning someone to the active treatment increases the
incentive to take the active treatment, and it would appear unusual for there to be
many units who would respond to this increase in incentives by declining to take the
treatment where they would otherwise have done so. In other settings, monotonicity
may be a more controversial assumption. For example, in studies in criminal justice, researchers have used random assignment of cases to judges to identify the causal effect of
prison terms on recidivism (reference). In that case, even if one judge is more strict than

117

118

Handbook of Field Experiments

another in the sense that the ﬁrst judge has a higher rate of sentencing individuals to
prison terms, it is not necessarily the case that any individual who would be sentenced
to time in prison by the on-average more lenient judge would also be sentenced to prison
by the stricter judge.
The second key assumption is generally referred to as the exclusion restriction. It requires that there is no direct effect of the assignment on the outcome without passing
through the receipt of treatment. Formally, using the form used in Angrist et al. (1996),
Yi ðz; wÞ ¼ Yi ðz0 ; wÞ;

for all z; z0 ; w:

The key components on the assumption is that for never-takers,
Yi ð0; 0Þ ¼ Yi ð1; 0Þ;

and for always  takers Yi ð0; 1Þ ¼ Yi ð1; 1Þ:

For compliers and deﬁers, the assumption is essentially about the interpretation of the
causal effect of the assignment to treatment to the causal effect of the receipt of treatment.
The exclusion restriction is a strong one, and its plausibility needs to be argued on a caseby-case basis. It is not justiﬁed by, and in fact not related to, the random assignment.
Given the exclusion restriction, we can drop the dependence of the potential outcomes
on z, and simply write Yi(w), for w ¼ 0, 1.
Given the monotonicity assumption and the exclusion restriction, we can identify the
average causal effect of the receipt of treatment on the outcome, what is known as the
local average treatment effect (Imbens and Angrist, 1994) as
slate ¼ E½Yi ð1Þ  Yi ð0ÞjCi ¼ c ¼

E Yiobs jZi ¼ 1  E Yiobs jZi ¼ 0
:
E Wiobs jZi ¼ 1  E Wiobs jZi ¼ 0

Given the setting, it is clear that we cannot identify the average effect for alwaystakers or never-takers without additional assumptions: we do not observe outcomes
for always-takers without the receipt of treatment, and we do not observe outcomes
for never-takers given the receipt of treatment. As a result, we need assumptions to
extrapolate the treatment effects for compliers to other compliance groups in order to
identify the overall average treatment effect.

9.3 Generalizing the local average treatment effect
One major concern with the local average treatment effect is that it reﬂects only on a
subpopulation, the compliers. In many cases the researcher may be more interested in
the overall average effect of the treatment. Here, we discuss some supplementary analyses
that can be done to assess the generalizability of the local average treatment effect. This
section builds on the discussions in Angrist (2004), Hirano et al. (2000), Imbens and
Rubin (1997a,b), and Bertanha and Imbens (2014). The Bertanha and Imbens (2014)

The Econometrics of Randomized Experiments

discussion is primarily in the context of fuzzy regression discontinuity designs, but their
results apply directly to other instrumental variables settings.
We use the same setup as in the previous section, but explicitly allow for the presence of exogenous covariates Xi. Instead of using instrumental variables methods to estimate the local average treatment effect, an alternative approach is to adjust for
differences in the covariate to estimate the average effect of the treatment, assuming
unconfoundedness as
Wi tðYi ð0Þ; Yi ð1ÞÞjXi :
If this assumption is valid, we can estimate the average effect of the treatment, as well
as average effects for any subpopulation using an as-treated analysis. One natural analysis is
to compare the local average treatment effect to the covariate-adjusted difference by
treatment status. A formal comparison of the two estimates, in a linear model setting,
would be a Hausman test (Hausman, 1983). In the absence of covariates the Hausman
test would be testing the equality as

pa
$ E½Yi ð1ÞjGi ¼ a  E½Yi ð1ÞjGi ¼ c
pa þ pc $ p z

pn
¼
$ E½Yi ð0ÞjGi ¼ n  E½Yi ð0ÞjGi ¼ c ;
pn þ pc $ ð1  pz Þ
where pa, pc, and pn are the population shares of always-takers, compliers, and nevertakers, respectively. This equality is difﬁcult to interpret. A particular weighted average
of the difference between the expected outcomes given treatment for always-takers
and compliers is equal to a weighted average of the difference between the expected
outcomes without treatment for compliers and never-takers.
Compared to the Hausman test, a more natural and interpretable approach is to test
the equality of the unweighted differences, between always-takers and treated compliers
and never-takers and not-treated compliers as
E½Yi ð1ÞjGi ¼ a  E½Yi ð1ÞjGi ¼ c ¼ E½Yi ð0ÞjGi ¼ n  E½Yi ð0ÞjGi ¼ c;
as suggested in Angrist (2004).
Bertanha and Imbens suggest testing the pair of equalities, rather than just the difference, as
E½Yi ð1ÞjGi ¼ a  E½Yi ð1ÞjGi ¼ c ¼ 0;

and

E½Yi ð0ÞjGi ¼ n  E½Yi ð0ÞjGi ¼ c ¼ 0:
If this pair of equalities hold, possibly after adjusting for differences in the covariates, it
means that always-takers are comparable to compliers given the treatment, and nevertakers are comparable to compliers without the treatment. That would suggest that
always-takers without the treatment might also be comparable to compliers without

119

120

Handbook of Field Experiments

the treatment, and that never-takers with the treatment might be comparable to compliers with the treatment, although neither claim can be tested. If those equalities were
to hold, however, then the average effect for compliers, adjusted for covariates, can be
generalized to the entire population.

9.4 Bounds
To get estimates of, or do inference for, the average causal effect of the receipt of treatment in settings with noncompliance an alternative to making additional assumptions is
to focus on getting ranges of values for the estimand that are consistent with the data in a
bound or partial identiﬁcation approach in a line of research associated with Manski
(1990, 1996, 2003a,b, 2013).
The simplest approach without any additional assumptions recognizes that because of
the noncompliance the receipt of treatment is no longer exogenous. We can therefore
analyze this as an observational study without any assumptions on the assignment process.
Consider the average difference in potential outcomes if all units are assigned to the treatment versus no one is assigned to the treatment as
s ¼

N
1 X
ðYi ð1Þ  Yi ð0ÞÞ ¼ Y ð1Þ  Y ð0Þ:
N i¼1

To estimate this object, it is useful to look at both terms separately. The ﬁrst term is
Nt
Nc
1 X
Yi ð1Þ:
$ Y obs
$
Y ð1Þ ¼
t þ
N
N N i:W ¼0
i

The last term is what is causing the problems. The data are not directly informative
about this term. Let us look at the special case where the outcome is binary. In that case


Nt
Nc
obs Nt
obs
$ Yt ;
$ Yt þ
:
Y ð1Þ ˛
N
N
N
We can do the same thing for the second term in the estimand, leading to


Nt
Nc
Nt Nt
Nc Nc
obs
obs
obs
obs
s˛
$ Yt 
$ Yc  ;
$ Yt þ 
$ Yc :
N
N
N N
N N
This is not a very informative range. By construction, it always includes zero, so we
can never be sure that the treatment has any effect on the outcome of interest.
Next, let us consider how the bounds change when we add information in the form
of additional assumptions maintaining the binary outcome assumption. Under the full set
of instrumental variables assumptions, that is, the monotonicity assumption and the
exclusion restriction, we can tighten the bounds substantially. To derive the bounds,
and at the same time develop intuition for their value, it is useful to think of the average

The Econometrics of Randomized Experiments

treatment effect as the sum of the averages over the three compliance groups, compliers,
never-takers and always-takers, with shares equal to pc, pn, and pa, respectively. Under
monotonicity and the exclusion restriction, the average effect for compliers is identiﬁed.
For always-takers, we can identify E½Yi ð1ÞjCi ¼ a, but the data are uninformative about
E½Yi ð0ÞjCi ¼ a, so that the average effect for always-takers is bounded by
sa ˛ E Yiobs jZi ¼ 0; Wi ¼ 1  1; E Yiobs jZi ¼ 0; Wi ¼ 1 :
Similarly,
sn ˛  E Yiobs jZi ¼ 1; Wi ¼ 0 ; 1  E Yiobs jZi ¼ 1; Wi ¼ 0 :
Combining these leads to


s˛ pa $ E Yiobs jZi ¼ 0; Wi ¼ 1  1  pn $ E Yiobs jZi ¼ 1; Wi ¼ 0


þ E Yiobs jZi ¼ 1  E Yiobs jZi ¼ 1 ;


pa $ E Yiobs jZi ¼ 0; Wi ¼ 1 þ pn $ 1  E Yiobs jZi ¼ 1; Wi ¼ 0


þ E Yiobs jZi ¼ 1  E Yiobs jZi ¼ 1 :
Under these assumptions, these bounds are sharp (Balke and Pearl, 1997).

9.5 As-treated and per protocol analyses
There are two older methods that have sometimes been used to analyze experiments with
noncompliance that rely on strong assumptions, as-treated and per-protocol analyses.
See, for example, McNamee (2009) and Imbens and Rubin (2015). In an as-treated analysis, units are compared by the treatment received, rather than the treatment assigned,
essentially invoking an unconfoundedness assumption. Because it was the assignment
that was randomized, rather than the receipt of treatment, this is not justiﬁed by the
randomization. It is useful to consider in a setting where the instrumental variables assumptions, that is, the monotonicity assumption and the exclusion restriction, hold,
and assess what the as-treated analysis leads to.
The estimand in an as-treated analysis is
sat ¼ E Yiobs jWi ¼ 1  E Yiobs jWi ¼ 0 :
If the monotonicity assumption holds, the ﬁrst term is an average of outcomes given
treatment for always-takers and compliers. If the fraction of units with Zi ¼ 1 is equal to
pZ, then we can write the ﬁrst term as
E Yiobs jWi ¼ 1 ¼

pa
$ E½Yi ð1ÞjCi ¼ a
pa þ pc $ PZ
pc $ pZ
$ E½Yi ð1ÞjCi ¼ c:
þ
pa þ pc $ PZ

121

122

Handbook of Field Experiments

Similarly,
E Yiobs jWi ¼ 0 ¼

pn
$ E½Yi ð0ÞjCi ¼ n
pn þ pc $ ð1  PZ Þ
pc $ ð1  pZ Þ
þ
$ E½Yi ð0ÞjCi ¼ c:
pn þ pc $ ð1  PZ Þ

The difference is then
E Yiobs jWi ¼ 1  E Yiobs jWi ¼ 0
¼ E½Yi ð1Þ  Yi ð0ÞjCi ¼ c
pa
$ ðE½Yi ð1ÞjGi ¼ a  E½Yi ð1ÞjCi ¼ cÞ
þ
pa þ pc $ PZ
pn

$ ðE½Yi ð0ÞjGi ¼ n  E½Yi ð0ÞjCi ¼ cÞ:
pn þ pc $ ð1  PZ Þ
These last two terms in expression are compared to zero in a Hausman test for the
exogeneity of the treatment. The form is in general difﬁcult to interpret.
The second is a per protocol analysis, where units who do not comply with their
assigned treatment are simply dropped from the analysis. Again it is instructive to see
what this method is estimating under the monotonicity assumption and the exclusion restriction. In general,
spp ¼ E Yiobs jWi ¼ 1; Zi ¼ 1  E Yiobs jWi ¼ 0; Zi ¼ 0 :
Similar calculations as for the as-treated analysis show that given the monotoniticy
and the exclusion restriction, this is equal to
spp ¼ E½Yi ð1Þ  Yi ð0ÞjCi ¼ c þ


pa
$ ðE½Yi ð1ÞjGi ¼ a  E½Yi ð1ÞjCi ¼ cÞ
pa þ pc

pn
$ ðE½Yi ð0ÞjGi ¼ n  E½Yi ð0ÞjCi ¼ cÞ:
pn þ pc

This expression is again difﬁcult to interpret in general, and the analysis is not
recommended.

10. HETEROGENOUS TREATMENT EFFECTS AND PRETREATMENT
VARIABLES
Most of the literature has focused on estimating average treatment effects for the entire
sample or population. However, in many cases, researchers are also interested in the presence or absence of heterogeneity in treatment effects. There are different ways to study

The Econometrics of Randomized Experiments

such heterogeneity. Here, we discuss some approaches. Note that this is different from
the way covariates or pretreatment variables were used in Section 4.4, where the focus
remained on the overall average treatment effect and the presence of pretreatment variables served solely to improve precision of the estimators. In observational studies, covariates also serve to make the identifying assumptions more credible.
As discussed at the outset of this chapter, a key concern with randomized experiments
is external validity. If we apply the treatment in a different setting, will the effect be the
same? Although there are many factors that vary across settings, one common way that
settings differ is that the populations of individual units may be different. If these differences can be captured with observable pretreatment variables, then it is in principle
possible to address this element of external validity as in Hotz et al. (2005). In particular,
if we obtain an estimate of the treatment effect for each potential value of the covariate
vector x, then we can estimate average treatment effects in any population be accounting
for the differences in distributions. That is, given an estimate for
sðxÞ ¼ E½Yi ð1Þ  Yi ð0ÞjXi ¼ x, it is straightforward to estimate E½sðXi Þ if the distribution of Xi is known.

10.1 Randomized experiments with pretreatment variables
Traditionally, researchers speciﬁed particular subpopulations based on substantive interest, and estimated average treatment effects for those subpopulations, as well as tested
equality of treatment effects across these subpopulations. For example, one may be interested separately in the effect of an educational program on girls versus boys. In such cases
the analyses are straightforward. One can simply analyze the data separately by subpopulation using the methods developed in Section 6.1. In these cases there is often some
concern that the subpopulations were selected ex post, so that p-values are no longer
valid because of multiple testing concerns. For example, suppose one has a randomized
experiment, with a 100 independent binary pretreatment variables that are in fact unrelated to the treatments or the outcomes. One would expect that for ﬁve of them the tstatistic for testing the null hypothesis that the average treatment effect was different by
the value of that covariate was larger than two in absolute value, even though none of the
covariates are related to the treatment effect. Preanalysis plans (Casey et al., 2012; Olken,
2015) are one approach to alleviate such concerns; another is to correct for multiple
testing (List et al., 2016). Below, we describe some recently developed alternatives that
work not only when the number of covariates is small, but also when the number is large
relative to the sample size or the true underlying model of treatment effect heterogeneity
may be quite complex.

10.2 Testing for treatment effect heterogeneity
A second approach is to simply test for the presence of heterogeneity in the average treatment effect as a function of the covariates, sðxÞ ¼ E½Yi ð1Þ  Yi ð0ÞjXi ¼ x.

123

124

Handbook of Field Experiments

One type of test considers whether there is any evidence for observable heterogeneity. Crump et al. (2008) develop nonparametric tests for the null hypothesis as
H0 : sðxÞ ¼ s;

for all x ˛ X;

against the alternative
H0 : sðxÞssðx0 Þ;

for some x; x0 ˛ X:

The setup of Crump et al. uses a sequence of parametric approximations to the conditional expectation
E Yiobs jWi ¼ w; Xi ¼ x ¼ b00 hðxÞ $ ð1  wÞ þ b01 hðxÞ $ w;
for vector-valued functions h(x) and then tests the null hypothesis the equality b1 ¼ b0.
By increasing the dimension of h(x), with a suitable basis of functions, one can
nonparametrically test the null hypothesis that the average treatment effect s(x) is a
constant as a function of the covariates under the assumption of unconfoundedness,
which is implied by randomized assignment.
A researcher might also like to understand which, if any, covariates are associated with
treatment effect. A natural approach would be to evaluate heterogeneity with respect to
each covariate, one by one. For example, each covariate could be transformed into a binary indicator for whether the value of the covariate is above or below the median, and
then the researcher could test the hypothesis that the treatment effect is higher when the
covariate is high than when it is low. Conducting a large number of hypothesis tests raises
issues of multiple testing, and conﬁdence intervals should be corrected to account for this.
However, standard approaches (e.g., the Bonferroni correction) assume that each test is
independent, and thus may be overly conservative in an environment where many covariates are correlated with one another (which will imply that the test statistics are also
correlated with one another). List et al. (2016) propose a computationally feasible
approach to the multiple testing problem in this context. The approach uses bootstrapping, and it accounts for correlation among test statistics. One challenge with this
approach is that the researcher must prespecify the set of hypothesis tests to conduct;
thus, it is hard to explore all possible interactions among covariates and all possible
ways to discretize them. In the next section, we consider methods that explore more
complex forms of heterogeneity.

10.3 Estimating the treatment effect heterogeneity
There are several possible approaches for exploring the treatment effect heterogeneity.
The ﬁrst is to specify a parametric model of treatment effect heterogeneity (as in Eq.
13) and report the estimates. For example, one simple approach would be to specify a
regression of the outcome on an indicator for treatment status as well as interactions of

The Econometrics of Randomized Experiments

the indicator with the treatment indicator. With a small number of covariates relative to
the sample size, all linear interactions with the treatment indicator could be considered,
partially alleviating concerns about multiple testing. Below, we discuss generalizations of
this idea to regularized regression (e.g., LASSO) where a systematic method is used to
select covariates.
A second approach is to construct a fully nonparametric estimator for s(x). We will
develop this approach further below; with sufﬁciently large datasets and a relatively small
number of covariates, this approach can be effective, and recent work-building on techniques from machine learning (Wager and Athey, 2015) has lead to improvements in
how many covariates can be handled without sacriﬁcing the coverage of conﬁdence intervals. For the case where there may be many covariates relative to the sample size, a
third approach proposed by Athey and Imbens (2016) uses the data to select a set of subgroups (a “partition” of the covariate space) such that treatment effect heterogeneity
across subgroups is maximized in a particular sense.
Whether a fully nonparametric approach or an approach based on subgroups is
preferred may be partially determined by the constraints of the data; valid conﬁdence intervals may not be available (at least with existing methods) with too many covariates
relative to sample size. But even if both methods are potentially feasible, it may be desirable to learn about subgroups rather than a fully nonparametric estimate of s(x) if the results of the experiment will be used in a context where people with limited processing
capability/memory will make decisions based on the experiment. For example, doctors
might use a simple ﬂowchart to determine which patients should be prescribed a drug.
Results about subgroups may also be more easily interpretable by researchers.
Relative to testing all covariates one by one, an approach that selects a single partition
of the covariate space will not, in general, discover all heterogeneity that exists, since the
algorithm will focus on the covariates with the biggest impact to the exclusion of others.
In addition, in the process of constructing a partition, once we have divided the data into
two groups according to the value of one covariate, further divisions will be considered
on subsamples of the data, reducing the power available to test heterogeneity in additional covariates. Thus, constructing a single partition does not answer the question of
which covariates are associated with heterogeneity; rather, it identiﬁes a particular way
to divide the data into meaningful groups. If a researcher wanted to explore all covariates,
while maintaining a data-driven approach to how to discretize them, an approach would
be to construct distinct partitions that restrict attention to one covariate at the time. For
interactions, one could consider small subsets of covariates. If the results of such an exercise were reported in terms of which covariates are associated with signiﬁcant heterogeneity, multiple testing corrections would be warranted. The approach of List et al.
(2016) works for an arbitrary set of null hypotheses, so the researcher could generate a
long list of hypotheses using the causal tree approach restricted to different subsets of
covariates, and then test them with a correction for multiple testing. Since in datasets

125

126

Handbook of Field Experiments

with many covariates, there are often many ways to describe what are essentially the same
subgroups, we expect a lot of correlation in test statistics, reducing the magnitude of the
correction for multiple hypothesis testing.
We begin by describing the third approach, where we construct a partition of the covariate space, and then return to the second and ﬁrst approaches.
10.3.1 Data-driven subgroup analysis: recursive partitioning for treatment
effects
Athey and Imbens (2016) develop a method for exploring heterogeneity in treatment effects without having to prespecify the form of the heterogeneity, and without having to
worry about multiple testing. Their approach builds on “regression tree” or “recursive
partitioning” methods, where the sample is partitioned in a number of subgroups,
deﬁned by the region of the covariate space each unit belongs to. The data are used to
determine which partition produces subgroups that differ the most in terms of treatment
effects. The method avoids introducing biases in the estimated average treatment effects
and allows for valid conﬁdence intervals using “sample splitting,” or “honest” estimation.
The idea of sample splitting to control signiﬁcance levels goes back a long way in statistics;
see, e.g., Cox (1975), or for a more recent discussion, see Fithian et al. (2015). In a sample
splitting approach, in a ﬁrst step, one sample is used to select the partition, while in a second step, an independent sample is used to estimate treatment effects and construct conﬁdence intervals for each subgroup (separately) given the partition from the ﬁrst step. The
output of the method is a set of subgroups, selected to optimize for treatment effect heterogeneity (to minimize expected mean-squared error of treatment effects), together
with treatment effect estimates and standard errors for each subgroup.
Let us illustrate some of the issues in a simple case to develop more intuition. Suppose
we consider only a single split of the covariate space in a setting with a substantial number
of covariates. We specify a criterion that determines whether one split (that is, a combination of a choice of the covariate and a threshold) is better than another. We return to
the choice of criterion below. Given a criterion, we select the covariate and threshold
that maximize the criterion. If we estimate the average treatment effect on the two subsamples using the same sample, the fact that this particular split led to a high value of the
criterion would often imply that the average treatment effect estimate is biased. Athey
and Imbens (2016) therefore suggest, in what they call an honest approach, to estimate
the treatment effects on a separate sample. The implication is that the treatment effect
estimates are unbiased on the two subsamples, and the corresponding conﬁdence intervals
are valid, even in settings with a large number of pretreatment variables or covariates.
A key issue is the choice of criterion. In principle, one would like to split in order to
obtain more precise estimates of the average treatment effects. A complicating factor is
that the standard criterion for splitting optimized for prediction rely on observing the
outcome whose expectation one wants to estimate. That is not the case here because

The Econometrics of Randomized Experiments

the unit-level treatment effect is not observed. There have been various suggestions in
the literature to deal with this. One simple solution is to transform the outcome from
Yiobs to
Yi ¼ Yiobs $

Wi  p
:
p $ ð1  pÞ

This
transformed
outcome
has
the
property
that
E½Yi jXi ¼ x ¼ sðxÞ ¼ E½Yi ð1Þ  Yi ð0ÞjXi ¼ x so that standard methods for recursive partitioning based on prediction apply (see Weisburg and Pontes, 2015; Athey and
Imbens, 2016). Su et al. (2009) suggest using test statistics for the null hypothesis that the
average treatment effect in the two subsamples is equal to zero. Zeileis et al. (2008) suggest using model ﬁt, where the model corresponds to a linear regression model in the
partitions with an intercept and a binary treatment indicator. Athey and Imbens (2016)
show that neither of the two criteria is optimal, and derive a new criterion that focuses
directly on the expected squared error of the treatment effect estimator, and which turns
out to depend both on the t-statistic and on the ﬁt measures. The criterion is further
modiﬁed to anticipate honest estimation, that is, to anticipate that the treatment effects
will be reestimated on an independent sample after the subgroups are selected. This
modiﬁcation ends up penalizing the expected variance of subgroup estimates; for
example, if subgroups are too small, the variance of treatment effect estimates will be
large. It also rewards splits for covariates that explain outcomes but not treatment effect
heterogeneity, to the extent that controlling for such covariates enables a lower-variance
estimate of the treatment effect.
Other related approaches include Wager and Walther (2015), who discuss corrections
to conﬁdence intervals (widening the conﬁdence intervals by a factor) as an alternative to
sample splitting; however, since conﬁdence intervals need to be inﬂated fairly substantially, it is not clear whether there is a wide range of conditions where it improves on
sample splitting. Relative to the approach of List et al. (2016) discussed above, the
methods in this section focus on deriving a single partition, rather than considering heterogeneity one covariate at the time with prespeciﬁed discretizations of the covariates;
the approaches in this section will have the advantage of exploring interaction effects
and using the data to determine a meaningful partition in terms of mean-squared error
of treatment effects.
10.3.2 Nonparametric estimation of treatment effect heterogeneity
There are (at least) four possible goals for using nonparametric estimation to estimate heterogeneous treatment effects. The ﬁrst is descriptive: the researcher can gain insight about
what types of units have the highest and lowest treatment effects, as well as visualize
comparative statics results, all without imposing a prior restrictions. The second, discussed
earlier, is that the researcher wishes to estimate the impact of applying the treatment in a

127

128

Handbook of Field Experiments

setting with a different distribution of units. A third is that the researcher wishes to derive
a personalized policy recommendation. A fourth is that the researcher wishes to test hypotheses and construct conﬁdence intervals. If conﬁdence intervals are desired, the set of
potential methods is quite small. For optimal policy evaluation, a Bayesian framework
may have some advantages, since it is natural to incorporate the uncertainty and risk
involved in alternative policy assignments. For description or estimation where conﬁdence intervals are not important, there are a wide variety of approaches.
Classical nonparametric approaches to treatment effect heterogeneity would
include K-nearest neighbor matching and kernel estimation (H€ardle, 2002). In the
case of K-nearest neighbor matching, for any x, we can construct an estimate of the treatment effect at that x by averaging the outcomes of the K-nearest neighbors that were
treated, and subtracting the average outcomes of the K-nearest neighbors that were
control observations. Kernel estimation does something similar, but uses a smooth
weighting function rather than uniformly reweighting nearby neighbors and giving
0 weight to neighbors that are farther away. In both cases, distance is measured using
Euclidean distance for the covariate vector. These methods can work well and provide
satisfactory coverage of conﬁdence intervals with one or two covariates, but performance
deteriorates quickly after that. The output of the nonparametric estimator is a treatment
effect for an arbitrary x. The estimates generally must be further summarized or visualized
since the model produces a distinct prediction for each x.
A key problem with kernels and nearest neighbor matching is that all covariates are
treated symmetrically; if one unit is close to another in 20 dimensions, the units are probably not particularly similar in any given dimension. We would ideally like to prioritize
dimensions that are most important for heterogeneous treatment effects, as is done in
many machine learning methods, including the highly successful random forest algorithm. Unfortunately, many popular machine learning methods that use the data to select
covariates may be bias-dominated asymptotically (including the standard random forest).
Recently, Wager and Athey (2015) propose a modiﬁed version of the random forest algorithm that produces treatment effect estimates that can be shown to be asymptotically
normal and centered on the true value of the treatment effect, and they propose a consistent estimator for the asymptotic variance. The method averages over many “trees” of the
form developed in Athey and Imbens (2016); the trees differ from one another because
different subsamples are used for each tree, and in addition, there is some randomization
in the choice of which covariates to split on. Each tree is “honest,” in that, one subsample
is used to determine a partition and an independent subsample is used to estimate treatment effects within the leaves. Unlike the case of a single tree, no data are “wasted”
because each observation is used to determine the partition in some trees and used to estimate treatment effects in other trees, and subsampling is already an inherent part of the
method. The method can be understood as a generalization of kernels and nearest
neighbor-matching methods, in that the estimated treatment effect at x is the difference

The Econometrics of Randomized Experiments

between a weighted average of nearby treated units and nearby control units; but the
choice of what dimensions are important for measuring distance is determined by the
data. In simulations, this method can obtain nominal coverage with more covariates
than K-nearest neighbor matching or kernel methods, while simultaneously producing
much more accurate estimates of treatment effects. However, this method also eventually
becomes bias-dominated when the number of covariates grows. It is much more robust
to irrelevant covariates than kernels or nearest neighbor matching.
Another approach to the problem is to divide the training data by treatment status,
and apply supervised learning methods to each group separately. For example, Foster
et al. (2011) use random forests to estimate the effect of covariates on outcomes in treated
and control groups. They then take the difference in predictions as data and project treatment effects onto units’ attributes using regression or classiﬁcation trees. The approach of
Wager and Athey (2015) can potentially gain efﬁciency by directly estimating heterogeneity in causal effects, and further the off-the-shelf random forest estimator does not have
established statistical properties (so conﬁdence intervals are not available).
Taking the Bayesian perspective, Green and Kern (2012) and Hill (2011) have proposed the use of forest-based algorithms for estimating heterogeneous treatment effects.
These papers use the Bayesian additive regression tree method of Chipman et al. (2010),
and report posterior credible intervals obtained by Markov chain Monte Carlo (MCMC)
sampling based on a convenience prior. Although Bayesian regression trees are often
successful in practice, there are currently no results guaranteeing posterior concentration
around the true conditional mean function, or convergence of the MCMC sampler in
polynomial time. In a related paper, Taddy et al. (2015) use Bayesian nonparametric
methods with Dirichlet priors to ﬂexibly estimate the data-generating process, and
then project the estimates of heterogeneous treatment effects down onto the feature
space using regularization methods or regression trees to get low-dimensional summaries
of the heterogeneity; but again, asymptotic properties are unknown.
10.3.3 Treatment effect heterogeneity using regularized regression
Imai and Ratkovic (2013), Signovitch (2007), Tian et al. (2014), and Weisburg and
Pontes (2015) develop lasso-like methods for causal inference and treatment effect heterogeneity in a setting where there are potentially a large number of covariates, so that
regularization methods are used to discover which covariates are important. When the
treatment effect interactions of interest have low dimension (that is, a small number of
covariates have important interactions with the treatment), valid conﬁdence intervals
can be derived (without using sample splitting as described above); see, e.g., Chernozhukov et al. (2015) and references therein. These methods require that the true underlying
model is (at least approximately) “sparse”: the number of observations must be large relative to the number of covariates (and their interactions) that have an important effect on
the outcome and on treatment effect heterogeneity. Some of the methods

129

130

Handbook of Field Experiments

(e.g., Tian et al., 2014) propose modeling heterogeneity in the treatment and control
groups separately, and then taking the difference; this can be inefﬁcient if the covariates
that affect the level of outcomes are distinct from those that affect treatment effect heterogeneity. An alternative approach is to incorporate interactions of the treatment with
covariates as covariates, and then allow LASSO to select which covariates are important.
Interaction terms can be prioritized over terms that do not include treatment effect interactions through weighting.
10.3.4 Comparison of methods
Although the LASSO-based methods require more a-priori restrictions on sparsity than
the random forest methods, both types of methods will lose nominal coverage rates if the
models become too complex. The LASSO methods have some advantages with datasets
where there are linear or polynomial relationships between covariates and outcomes;
random forest methods do not parsimoniously estimate linear relationships and use
them for extrapolation, but are more localized. The random forest methods are well
designed to capture complex, multidimensional interactions among covariates, or highly
nonlinear interactions. LASSO has the advantage that the ﬁnal output is a regression,
which may be more familiar to researchers in some disciplines; however, it is important
to remember that the conditions the justify the standard errors are much more stringent
when the model selection was carried out on the same data that are used for estimation. If
valid conﬁdence intervals are the ﬁrst priority in an environment where the model is not
known to be sparse and there are many covariates, the recursive partitioning approach
provides conﬁdence intervals that do not deteriorate (at all) as the number of covariates
grow. What suffers, instead, is the mean-squared error of the predictions of treatment
effects.
Another point of comparison between the regression-based methods and tree-based
methods (including random forests) relates to our earlier discussions of randomizationbased inference versus sampling-based inference. Tree-based methods construct estimates by dividing the sample into subgroups and calculating sample averages within
the groups; thus, the estimates and associated inference can be justiﬁed by random
assignment of the treatment. In contrast, regression-based approaches require additional
assumptions.
10.3.5 Relationship to optimal policy estimation
The problem of estimating heterogeneous treatment effects is closely related to the
problem of estimating, as a function of the covariates, what the optimal policy is.
Heuristically, with a binary treatment, we would want to assign an individual with
covariates x to a treatment if s(x) > 0. However, the optimal policy literature addresses
additional issues that might arise when there are multiple potential treatments, as well as
when the loss function may be nonlinear (so that there is, for example, a mean-variance

The Econometrics of Randomized Experiments

tradeoff between different policies). More broadly, the criterion used in estimation may
be modiﬁed to account for the goal of policy estimation; when regularization approaches are used to penalize model complexity, the methods may deprioritize discovering heterogeneity that is not relevant for selecting an optimal policy. For example, if a
treatment clearly dominates another for some parts of the covariate space, understanding heterogeneity in the magnitude of the treatment’s advantage may not be important
in those regions.
Much of the policy estimation literature takes a Bayesian perspective; this allows the
researcher to evaluate welfare and to incorporate risk aversion in the loss function in an
environment where there is uncertainty about the effects of the policy.
In the machine learning literature, Beygelzimer and Langford (2009) and Dudik et al.
(2011) discuss procedures for transforming outcomes that enable off-the-shelf loss minimization methods to be used for optimal treatment policy estimation. In the econometrics literature, Graham et al. (2014), Dehejia (2005), Hirano and Porter (2009), Manski
(2003a,b), and Bhattacharya and Dupas (2012) estimate parametric or semiparametric
models for optimal policies, relying on regularization for covariate selection in the case
of Bhattacharya and Dupas (2012). See also Banerjee et al. (2016).

11. EXPERIMENTS IN SETTINGS WITH INTERACTIONS
In this section, we discuss the analysis of randomized experiments in settings with interference between units. Such interference may take different forms. There may be spillovers from the treatment assigned to one unit to other units. A classic example of that is
that of agricultural experiments where fertilizer applied to one plot of land may leach
over to other plots and thus affect outcomes in plots assigned to different treatments.
It may also take the form of active and deliberate interactions between individuals, for
example in educational settings, where exposing one student to a new program may
well affect the outcomes for students the ﬁrst student is friends with.
There are many different versions of these problems, and many different estimands.
An important theoretical paper in an observational setting is Manski (1993) who introduced terminology to distinguish among contextual effects, exogenous effects, and
endogenous effects. Contextual effects arise when individuals are exposed to similar environmental stimula as their peers. Exogenous effects refer to effects from ﬁxed characteristics of an individual’s peers. Endogenous effects in Manski’s terminology refer to direct
causal effects of the behavior of the peers of an individual.
The interactions may be a nuisance that affects the ability to do inference, with the
interest in the overall average effect, or the interactions may be of primary interest to
the researcher. This is an active area of research, with many different approaches, where
it not clear what will ultimately be the most useful results for empirical work. In fact,
some of the most interesting work has been empirical.

131

132

Handbook of Field Experiments

11.1 Empirical work on interactions
Here, we discuss some of the questions raised in empirical work on interactions. These
provide some of the background for the discussions of the theoretical work by suggesting
particular questions and settings where these questions are of interest. There are a number
of different settings. In some cases the peer group composition is randomized, and in
other cases treatments are randomized. An example of the ﬁrst case is Sacerdote (2001)
where individual students are randomly assigned to dorm rooms and thus matched to
a roommate. An example of the second is Duﬂo and Saez (2003) where individuals in
possibly endogenously formed groups are randomly assigned to treatments, with the
treatments clustered at the group level.
Miguel and Kremer (2004) were interested in the effects of deworming programs on
children’s educational outcomes. There are obviously direct effects of deworming on the
outcomes for individuals who are exposed to these programs, but just as in the case of
infectious diseases in general, there may be externalities for individuals not exposed to
the program if individuals they interact with are exposed. Miguel and Kremer ﬁnd evidence of substantial externalities.
Crepon et al. (2013) were interested in the effects of labor market-training programs.
They were concerned about interactions between individuals through the labor market.
Part of the effect of providing training to an unemployed individual may be that this individual becomes more attractive to an employer relative to an untrained individual. If,
however, the total number of vacancies is not affected by the presence of more trained
individuals, the overall effect may be zero even if the trained individuals are more likely
to be employed than the individuals in the control group. Crepon et al. studied this by
randomizing individuals to training programs in a number of labor markets. They varied
the marginal rate at which the individuals were assigned to the training program between
the labor markets. They then compared the difference in average outcomes by treatment
status within the labor markets, across the different labor markets. In the absence of interactions, here in the form of equilibrium effects, the average treatment effects should not
vary by the marginal treatment rate. Evidence that the average treatment effects were
higher when the marginal treatment rate was lower suggests that part of the treatment
effect was based on redistributing jobs from control individuals to trained individuals.
Sacerdote (2001) studies the effect of roommates on an individual’s behavior in college. He exploits the random assignment of incoming students to dorm rooms at Dartmouth (after taking account of some characteristics such as smoking behavior). The
treatment can be thought of here as having a roommate of a particular type, such as a
roommate with a relatively high or low level of high school achievement. If roommates
are randomly assigned, then ﬁnding that individuals with high achieving roommates have
outcomes that are systematically different from those of individuals with low achieving
roommates is evidence of causal interactions between the roommates.

The Econometrics of Randomized Experiments

Carrell et al. (2013) analyze data from the US Air Force Academy. They control the
assignment of incoming students to squadrons to manipulate the distribution of characteristics of fellow squadron students that an incoming student is faced with. They ﬁnd that
the outcomes for students vary systematically with this distribution of fellow student
characteristics, which is evidence of causal effects of interactions.

11.2 The analysis of randomized experiments with interactions in
subpopulations
One important special case of interference assumes the population can be partitioned into
groups or clusters, with the interactions limited to units within the same cluster. This is a
case studied by, among others, Manski (2013), Hudgens and Halloran (2008), and Liu
and Hudgens (2013). Ugander et al. (2013) discuss graph cutting methods in general
network settings to generate partitions of the basic network where such an assumption
holds, at least approximately. Hudgens and Halloran deﬁne in this setting direct, indirect,
total, and overall causal effects, and consider a two-stage randomized design where in the
ﬁrst stage clusters are selected randomly and in the second stage units within the clusters
are randomly assigned. The direct causal effect for a particular unit corresponds to the
difference between potential outcomes where only the treatment effect for that unit is
changed, and all other treatments are kept ﬁxed. Indirect effects correspond to causal effects of changes in the assignments for other units in the same group, keeping ﬁxed the
own assignment. The total effect combines the direct and indirect effects. Finally, the
overall effect in the Hudgens and Halloran framework is the average effect for a cluster
or group, compared to the baseline where the entire group receives the control
treatment.
Hudgens and Halloran also stress the widely used assumption that for unit i it matters
only what fraction of the other units in their group are treated, not the identity of the
treated units. Without such an assumption the proliferation of indirect treatment effects
makes it difﬁcult to obtain unbiased estimators for any of them. This assumption is often
made, sometimes implicitly, in empirical work in this area.
They consider designs where the marginal rate of treatment varies across groups. In
the ﬁrst stage of the assignment the groups are randomly assigned to different treatment rates, followed by a stage in which the units are randomly assigned to the
treatment.

11.3 The analysis of randomized experiments with interactions in
networks
Here, we look at a general network setting where the population of units is not necessarily partitioned into mutually exclusive groups. With N individuals in the population of
interest, we have a network characterized by an N  N adjacency matrix G, with
Gij ˛ {0, 1} a binary indicator for the event that units i and j are connected. The matrix

133

134

Handbook of Field Experiments

G is symmetric with all diagonal elements equal to zero. The question here is what we
can learn about presence of interaction effects by conducting a randomized experiment
on this single network, with a binary treatment. Unlike the Manski (1993) and Hudgens
and Halloran (2008) setting, we have only a single network, but the network is richer in
the sense that it need not be the case that friends of friends are also friends themselves. The
types of questions Athey et al. (2015) are interested in are, for example, whether there is
evidence that changing the treatment for friends affects an individual’s outcome, or
whether manipulating treatments for friends of an individual’s friends changes
their outcome. They do so by focusing on exact tests to avoid the reliance on large sample
approximations, which can be difﬁcult to derive in settings with a single network. (There
is not even a clear answer to the question of what it means for a network to grow in size;
specifying this would require the researcher to specify what it means for the network to
grow, in terms of new links for new units.)
Let us focus in this discussion on the two main hypotheses Athey et al. (2015)
consider. First, the null hypothesis of no interactions whatsoever, that is the null hypothesis that changing the treatment status for friends does not change an individual’s
outcome, also considered in Aronow (2012), and second, the null hypothesis that the
treatment of a friend of a friend does not have a causal effect on an individual’s outcome.
Like Liu and Hudgens (2013) and Athey et al. (2015), they consider randomization
inference.
We focus on the setting where in the population treatments are completely randomly
assigned. The network itself is analyzed as given. Initially let us focus on the null hypothesis of no interactions whatsoever. Athey et al. (2015) introduce the notion of an artiﬁcial
experiment. The idea is to select a number of units from the original population, whom
they call the focal units. Given these focal units, they deﬁne a test statistic in terms of the
outcomes for these focal units, say the correlation between outcomes and the fraction of
treated friends. They look at the distribution of this statistic, induced by randomizing the
treatments only for the nonfocal or auxiliary units. Under the null hypothesis of no treatment effects whatsoever, changing the treatment status for auxiliary units would not
change the value of the outcomes for the focal units.
For the second null hypothesis, that friends of friends have no effect, they again
consider a subset of the units to be focal. A second subset of units are termed “buffer”
units: these are the friends of the focal unit. If we allow that friends can have an impact
on focal units, then their treatments cannot be randomized in the artiﬁcial experiment
designed to test the impact of friends of friends. The complement of focal and buffer
units, termed the auxiliary units, are the units whose treatments are randomized in the
artiﬁcial experiment. The randomization distribution over the treatment assignments
of auxiliary units induces a distribution on the test statistic, and this approach thus enables
the researcher to test the hypothesis that friends of friends have no effect, without placing
any restrictions on direct effects or the effect of friends.

The Econometrics of Randomized Experiments

Athey et al. (2015) also consider richer hypotheses, such as hypotheses about what
types of link deﬁnitions correspond to meaningful peer effects; they propose a test of
the hypothesis that a sparser deﬁnition of the network is sufﬁcient to capture relationships
for which treating a friend inﬂuences a unit.
Aronow and Samii (2013) study estimation in this general network setting. They assume that there is a structure on the treatment effects so that only a limited number of
unit-level treatment assignments have a nonzero causal effect on the outcome for unit
i. The group structure that Hudgens and Halloran (2008) use is a special case where it
is only the treatments for units in the same group as unit i can have nonzero effects on
the outcome for unit i.

12. CONCLUSION
In this chapter, we discuss statistical methods for analyzing data from randomized experiments. We focus primarily on randomization-based, rather than model-based methods,
starting with classic methods developed by Fisher and Neyman, up to recent work on
noncompliance, clustering and methods for identifying treatment effect heterogeneity,
as well as experiments in settings with interference.

REFERENCES
Abadie, A., Angrist, J., Imbens, G., 2002. Instrumental variables estimates of the effect of subsidized training
on the quantiles of trainee earnings. Econometrica 70 (1), 91e117.
Abadie, A., Athey, S., Imbens, G., Wooldridge, J., 2014. Finite Population Causal Standard Errors. NBER
working paper 20325.
Abadie, A., Athey, S., Imbens, G., Wooldridge, J., 2016. Clustering as a Design Problem. Unpublished
working paper.
Allcott, H., 2015. Site selection bias in program evaluation. Q. J. Econ. 1117e1165.
Altman, D., 1991. Practical Statistics for Medical Research. Chapman and Hall/CRC.
Angrist, J., 2004. Treatment effect heterogeneity in theory and practice. Econ. J. 114 (494), C52eC83.
Angrist, J., Kuersteiner, G., 2011. Causal effects of monetary shocks: semiparametric conditional independence tests with a multinomial propensity score. Rev. Econ. Stat. 93 (3), 725e747.
Angrist, J., Pischke, S., 2009. Mostly Harmless Econometrics. Princeton University Press, Princeton, NJ.
Angrist, J., Imbens, G., Rubin, D., 1996. Identiﬁcation of causal effects using instrumental variables. J. Am.
Stat. Assoc. 91, 444e472.
Aronow, P., 2012. A general method for detecting interference between units in randomized experiments.
Sociol. Methods Res. 41 (1), 3e16.
Aronow, P., Samii, C., 2013. Estimating Average Causal Effects under Interference between Units. arXiv:
1305.6156(v1).
Athey, S., Eckles, D., Imbens, G., 2015. Exact P-values for Network Interference. NBER working paper
21313.
Athey, S., Imbens, G., 2016. Recursive partitioning for heterogeneous causal effects. arXiv:1504.01132
Proc. Nat. Acad. Sci. U.S.A. (Forthcoming).
Baker, S., 2000. Analyzing a randomized cancer prevention trial with a missing binary outcome, an auxiliary
variable, and all-or-none compliance. J. Am. Stat. Assoc. 95 (449), 43e50.
Baker, S.G., Kramer, B.S., Lindeman, K.S., 2016. Latent class instrumental variables: a clinical and biostatistical perspective. Stat. Med. 35 (1), 147e160.

135

136

Handbook of Field Experiments

Baker, S.G., Lindeman, K.S., 1994. The paired availability design: a proposal for evaluating epidural analgesia
during labor. Stat. Med. 13, 2269e2278.
Balke, A., Pearl, J., 1997. Bounds on treatment effects from studies with imperfect compliance. J. Am. Stat.
Assoc. 92 (439), 1171e1176.
Banerjee, A., Chassang, S., Snowberg, E., 2016. Decision theoretic approaches to experimental design and
external validity. In: Banerjee, Duﬂo (Eds.), Handbook of Development Economics. Elsevier, North
Holland.
Banerjee, A., Duﬂo, E., 2009. The experimental approach to development economics. Annu. Rev. Econ. 1,
151e178.
Bareinboim, E., Lee, S., Honavar, V., Pearl, J., 2013. Causal transportability from multiple environments
with limited experiments. Adv. Neural Inf. Process. Syst. 26 (NIPS Proceedings), 136e144.
Barnard, J., Du, J., Hill, J., Rubin, D., 1998. A broader template for analyzing broken randomized
experiments. Sociol. Methods Res. 27, 285e317.
Benjamini, Y., Hochberg, Y., 1995. Controlling the false discovery rate: a practical and powerful approach to
multiple testing. J. R. Stat. Soc. Series B (Methodological), 289e300.
Bertanha, M., Imbens, G., 2014. External Validity in Fuzzy Regression Discontinuity Designs. NBER
working paper 20773.
Bertrand, M., Duﬂo, E., 2016. Field Experiments on Discrimination. NBER working paper 22014.
Beygelzimer, A., Langford, J., 2009. The Offset Tree for Learning with Partial Labels. http://arxiv.org/pdf/
0812.4044v2.pdf.
Bhattacharya, D., Dupas, P., 2012. Inferring welfare maximizing treatment assignment under budget
constraints. J. Econ. 167 (1), 168e196.
Bitler, M., Gelbach, J., Hoynes, H., 2002. What mean impacts miss: distributional effects of welfare reform
experiments. Am. Econ. Rev. 96 (4), 988e1012.
Bloniarz, A., Liu, H., Zhang, C.H., Sekhon, J.S., Yu, B., 2015. Lasso adjustments of treatment effect
estimates in randomized experiments. arXiv preprint arXiv:1507.03652.
Bloom, H., 1984. Accounting for no-shows in experimental evaluation designs. Eval. Rev. 8, 225e246.
Box, G., Hunter, S., Hunter, W., 2005. Statistics for Experimenters: Design, Innovation and Discovery.
Wiley, New Jersey (Chapter 4 and 9).
Bruhn, M., McKenzie, D., 2009. In pursuit of balance: randomization in practice in development ﬁeld
experiments. Am. Econ. J. Appl. Econ. 1 (4), 200e232.
Carrell, S., Sacerdote, B., West, J., 2013. From natural variation to optimal policy? The importance of
endogenous peer group formation. Econometrica 81 (3), 855e882.
Casey, K., Glennerster, R., Miguel, E., 2012. Reshaping institutions: evidence on aid impacts using a preanalysis plan. Q. J. Econ. 1755e1812.
Chernozhukov, V., Hansen, C., 2005. An IV model of quantile treatment effects. Econometrica 73 (1),
245e261.
Chernozhukov, V., Hansen, C., Spindler, M., 2015. Post-selection and post-regularization inference in
linear models with many controls and instruments. Am. Econ. Rev. 105 (5), 486e490.
Chipman, H., George, E., McCulloch, R., 2010. BART: Bayesian additive regression trees. Ann. Appl. Stat.
4 (1), 266e298.
Cochran, W., 1972. Observational studies. In: Bancroft, T.A. (Ed.), Statistical Papers in Honor of George W.
Snedecor. Iowa State University Press, pp. 77e90. Reprinted in Observational Studies, 2015.
Cochran, W., Cox, G., 1957. Experimental Design, Wiley Classics Library.
Cohen, J., 1988. Statistical Power for the Behavioral Sciences, second ed.
Cook, T., DeMets, D., 2008. Introduction to Statistical Methods for Clinical Trials. Chapman and Hall/
CRC.
Cox, D., 1956. A note on weighted randomization. Ann. Math. Stat. 27 (4), 1144e1151.
Cox, D., 1975. A note on data-splitting for the evaluation of signiﬁcance levels. Biometrika 62 (2), 441e444.
Cox, D., 1992. Causality: some statistical aspects. J. R. Stat. Soc. Ser. A 155, 291e301.
Cox, D., Reid, N., 2000. The Theory of the Design of Experiments. Chapman and Hall/CRC, Boca
Raton, Florida.

The Econometrics of Randomized Experiments

Crepon, B., Duﬂo, E., Gurgand, M., Rathelot, R., Zamora, P., April 24, 2013. Do labor market policies
have displacement effects? Evidence from a clustered randomized experiment. Q. J. Econ. 128 (2),
531e580.
Crump, R., Hotz, V.J., Imbens, G., Mitnik, O., 2008. Nonparametric tests for treatment effect
heterogeneity. Rev. Econ. Stat. 90 (3), 389e405.
Cuzick, J., Edwards, R., Segnan, N., 1997. Adjusting for non-compliance and contamination in randomized
clinical trials. Stat. Med. 16, 1017e1039.
Davies, O., 1954. The Design and Analysis of Industrial Experiments. Oliver and Boyd, Edinburgh.
Deaton, A., 2010. Instruments, randomization, and learning about development. J. Econ. Lit. 424e455.
Dehejia, R., 2005. Program evaluation as a decision problem. J. Econ. 125 (1), 141e173.
Dehejia, R., Wahba, S., 1999. Causal effects in nonexperimental studies: reevaluating the evaluation of
training programs. J. Am. Stat. Assoc. 94, 1053e1062 (Chapters 8,11,16 and 17).
Donner, A., 1987. Statistical methodology for paired cluster designs. Am. J. Epidemiol. 126 (5), 972e979.
Doksum, K., 1974. Empirical probability plots and statistical inference for nonlinear models in the two-sample case. Ann. Stat. 2, 267e277.
Dudik, M., Langford, J., Li, L., 2011. Doubly robust policy evaluation and learning. In: Proceedings of the
28th International Conference on Machine Learning (ICML-11).
Duﬂo, E., Glennerster, R., Kremer, M., 2006. Using randomization in development economics research: a
toolkit. In: Handbook of Development Economics. Elsevier.
Duﬂo, E., Hanna, R., Ryan, S., 2012. Incentives work: getting teachers to come to school. Am. Econ. Rev.
102 (4), 1241e1278.
Duﬂo, E., Saez, E., 2003. The role of information and social interactions in retirement decisions: evidence
from a randomized experiment. Q. J. Econ. 815e842.
Eckles, D., Karrer, B., Ugander, J., 2014. Design and Analysis of Experiments in Networks: Reducing Bias
from Interference (Unpublished working paper).
Eicker, F., 1967. Limit theorems for regression with unequal and dependent errors. In: Proceedings of the
Fifth Berkeley Symposium on Mathematical Statistics and Probability, vol. 1. University of California
Press, Berkeley, pp. 59e82.
Firpo, S., 2007. Efﬁcient semiparametric estimation of quantile treatment effects. Econometrica 75 (1),
259e276.
Fisher, R.A., 1925. Statistical Methods for Research Workers, ﬁrst ed. Oliver and Boyd, London.
Fisher, R.A., 1935. Design of Experiments. Oliver and Boyd.
Fisher, L., et al., 1990. Intention-to-treat in clinical trials. In: Peace, K.E. (Ed.), Statistical Issues in Drug
Research and Development. Marcel Dekker, New York.
Fithian, W., Sun, C., Taylor, J., 2015. Optimal Inference after Model Selection. http://arxiv.org/abs/1410.
2597.
Foster, J., Taylor, J., Ruberg, S., 2011. Subgroup identiﬁcation from randomized clinical trial data. Stat.
Med. 30 (24), 2867e2880.
Frangakis, C., Rubin, D., 2002. Principal stratiﬁcation. Biometrics (1), 21e29.
Freedman, D., 2006. Statistical models for causality: what leverage do they provide. Eval. Rev. 30, 691e713.
Freedman, D., 2008. On regression adjustments to experimental data. Adv. Appl. Math. 30 (6), 180e193.
Gail, M., Tian, W., Piantadosi, S., 1988. Tests for no treatment effect in randomized clinical trials. Biometrika 75 (3), 57e64.
Gail, M., Mark, S., Carroll, R., Green, S., Pee, D., 1996. On design considerations and randomization-based
inference for community intervention trials. Stat. Med. 15, 1069e1092.
Glennerster, R., 2016. The practicalities of running randomized evaluations: partnerships, measurement,
ethics, and transparency. In: Banerjee, Duﬂo (Eds.), Handbook of Development Economics. Elsevier,
North Holland.
Glennerster, R., Takavarasha, K., 2013. Running Randomized Evaluations: A Practical Guide. Princeton
University Press.
Graham, B., Imbens, G., Ridder, G., 2014. Complementarity and aggregate implications of assortative
matching: a nonparametric analysis. Quant. Econ. 5 (1), 29e66.

137

138

Handbook of Field Experiments

Green, D., Kern, H., 2012. Detecting heterogeneous treatment effects in large-scale experiments using
Bayesian additive regression trees. Public Opin. Q. 76 (3), 491e511.
H€ardle, W., 2002. Applied Nonparametric Regression Analysis. Cambridge University Press.
Hausman, J.A., 1983. Speciﬁcation and estimation of simultaneous equation models. Handbook Econ 1,
391e448.
Hill, J., 2011. Bayesian nonparametric modeling for causal inference. J. Comput. Graph. Stat. 20 (1).
Hinkelmann, K., Kempthorne, O., 2008. Design and analysis of experiments. In: Introduction to Experimental Design, vol. 1. Wiley.
Hinkelmann, K., Kempthorne, O., 2005. Design and analysis of experiments. In: Advance Experimental
Design, vol. 2. Wiley.
Hirano, K., Imbens, G., Rubin, D., Zhou, A., 2000. Estimating the effect of ﬂu shots in a randomized
encouragement design. Biostatistics 1 (1), 69e88.
Hirano, K., Porter, J., 2009. Asymptotics for statistical treatment rules. Econometrica 77 (5), 1683e1701.
Hodges, J.L., Lehmann, E., 1970. Basic Concepts of Probability and Statistics, second ed. Holden-Day, San
Francisco.
Holland, P., 1986. Statistics and causal inference. J. Am. Stat. Assoc. 81, 945e970 with discussion.
Hotz, J., Imbens, G., Mortimer, J., 2005. Predicting the efﬁcacy of future training programs using past
experiences. J. Econ. 125, 241e270.
Huber, P., 1967. The behavior of maximum likelihood estimates under nonstandard conditions. In: Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, vol. 1. University
of California Press, Berkeley, pp. 221e233.
Hudgens, M., Halloran, M., 2008. Toward causal inference with interference. J. Am. Stat. Assoc. 103 (482),
832e842.
Imai, K., Ratkovic, M., 2013. Estimating treatment effect heterogeneity in randomized program evaluation.
Ann. Appl. Stat. 7 (1), 443e470.
Imbens, G., 2010. Better LATE than nothing: some comments on Deaton (2009) and Heckman and Urzua
(2009). J. Econ. Lit. 399e423.
Imbens, G., Angrist, J., 1994. Identiﬁcation and estimation of local average treatment effects. Econometrica
61 (2), 467e476.
Imbens, G.W., Kolesar, M., 2016. Robust standard errors in small samples: some practical advice full access.
Rev. Econ. Stat. 98 (4), 701e712.
Imbens, G., Rosenbaum, P., 2005. Randomization inference with an instrumental variable. J. R. Stat. Soc.
Ser. A 168 (1), 109e126.
Imbens, G., Rubin, D., 1997. Estimating outcome distributions for compliers in instrumental variable
models. Rev. Econ. Stud. 64 (3), 555e574.
Imbens, G., Rubin, D., 1997. Bayesian inference for causal effects in randomized experiments with
noncompliance. Ann. Stat. 25 (1), 305e327.
Imbens, G., Rubin, D., 2015. Causal Inference in Statistics, Social, and Biomedical Sciences: An
Introduction. Cambridge University Press.
Kagel, J.H., Roth, A.E., Hey, J.D., 1995. The Handbook of Experimental Economics. Princeton university
press, Princeton.
Kempthorne, O., 1952. The Design and Analysis of Experiments. Robert Krieger Publishing Company,
Malabar, Florida.
Kempthorne, O., 1955. The randomization theory of experimental inference. J. Am. Stat. Assoc. 50,
946e967.
Lalonde, R.J., 1986. Evaluating the econometric evaluations of training programs with experimental data.
Am. Econ. Rev. 76, 604e620.
Lee, M.-J., 2005. Micro-econometrics for Policy, Program, and Treatment Effects. Oxford University Press,
Oxford.
Lehman, E., 1974. Nonparametrics: Statistical Methods Based on Ranks. Holden-Day, San Francisco.
Lesaffre, E., Senn, S., 2003. A note on non-parametric ANCOVA for covariate adjustment in randomized
clinical trials. Stat. Med. 22, 3583e3596.

The Econometrics of Randomized Experiments

Liang, K., Zeger, S., 1986. Longitudinal data analysis using generalized linear models. Biometrika 73 (1),
13e22.
Lin, W., 2013. Agnostic notes on regression adjustments for experimental data: reexamining Freedman’s
critique. Ann. Appl. Stat. 7 (1), 295e318.
List, J., Shaikh, A., Xu, Y., 2016. Multiple Hypothesis Testing in Experimental Economics. NBER working
paper no. 21875.
Liu, L., Hudgens, M., 2013. Large sample randomization inference of causal effects in the presence of
interference. J. Am. Stat. Assoc. 288e301.
Lui, K., 2011. Binary Data Analysis of Randomized Clinical Trials with Noncompliance. Wiley. Statistics in
Practice.
Lynn, H., McCulloch, C., 1992. When does it pay to break the matches for analysis of a matched-pair design.
Biometrics 48, 397e409.
Manning, W., Newhouse, J., Duan, N., Keeler, E., Leibowitz, A., 1987. Health insurance and the demand
for medical care: evidence from a randomized experiment. Am. Econ. Rev. 77 (3), 251e277.
Manski, C., 1990. Nonparametric bounds on treatment effects. Am. Econ. Rev. Pap. Proc. 80, 319e323.
Manski, C., 1993. Identiﬁcation of endogenous ocial effects: the reﬂection problem. Rev. Econ. Stud. 60 (3),
531e542.
Manski, C., 1996. Learning about treatment effects from experiments with random assignment of treatments.
J. Hum. Resour. 31 (4), 709e773.
Manski, C., 2003a. Partial Identiﬁcation of Probability Distributions. Springer-Verlag, New York.
Manski, C., 2003b. Statistical treatment rules for heterogeneous populations. Econometrica 72 (4),
1221e1246.
Manski, C., 2013. Public Policy in an Uncertain World. Harvard University Press, Cambridge.
Meager, R., 2015. Understanding the Impact of Microcredit Expansions: A Bayesian Hierarchical Analysis of
7 Randomised Experiments. MIT, Department of Economics.
McNamee, R., 2009. Intention to treat, per protocol, as treated and instrumental variable estimators given
non-compliance and effect heterogeneity. Stat. Med. 28, 2639e2652.
Miguel, T., Kremer, M., 2004. Worms: identifying impacts on education and health in the presence of treatment externalities. Econometrica 72 (1), 159e217.
Morgan, K., Rubin, D., 2012. Rerandomization to improve covariate balance in experiments. Ann. Stat. 40
(2), 1263e1282.
Morgan, S., Winship, C., 2007. Counterfactuals and Causal Inference. Cambridge University Press,
Cambridge.
Morton, R., Williams, K., 2010. Experimental Political Science and the Study of Causality. Cambridge University Press, Cambridge, MA.
Murphy, K., Myors, B., Wollach, A., 2014. Statistical Power Analysis. Routledge.
Murray, B., 2012. Clustering: A Data Recovery Approach, second ed. Chapman and Hall.
Neyman, J., 1923, 1990. On the application of probability theory to agricultural experiments. Essay on principles. Section 9. translated in Stat. Sci. 5 (4), 465e480 (with discussion).
Neyman, J., Iwaskiewicz, K., Kolodziejczyk, St, 1935. Statistical problems in agricultural experimentation. J.
R. Stat. Soc. Ser. B 2, 107e180 (with discussion).
Olken, B., 2015. Promises and perils of pre-analysis plans. J. Econ. Perspect. 29 (3), 61e80.
Pearl, J., 2000, 2009. Causality: Models, Reasoning and Inference. Cambridge University Press, Cambridge.
Robins, P., 1985. A comparison of the labor supply ﬁndings from the four negative income tax experiments.
J. Hum. Resour. 20 (4), 567e582.
Romano, J., Shaikh, A., Wolf, M., 2010. Hypothesis testing in econometrics. Annu. Rev. Econ. 2, 75e104.
Romer, C.D., Romer, D.H., 2004. A new measure of monetary shocks: derivation and implications. Am.
Econ. Rev. 94 (4), 1055e1084.
Rosenbaum, P., 2009. Design of Observational Studies. Springer Verlag, New York.
Rosenbaum, P., 1995, 2002. Observational Studies. Springer Verlag, New York.
Rosenbaum, P., 2002. Covariance adjustment in randomized experiments and observational studies. Stat.
Sci. 17 (3), 286e304.

139

140

Handbook of Field Experiments

Rothstein, J., von Wachter, T., 2016. Social experiments in the labor market. In: Handbook of Experimental
Economics.
Rubin, D., 1974. Estimating causal effects of treatments in randomized and non-randomized studies. J. Educ.
Psychol. 66, 688e701.
Rubin, D., 1975. Bayesian inference for causality: the importance of randomization. In: Proceedings of the
Social Statistics Section of the American Statistical Association, pp. 233e239.
Rubin, D.B., 1978. Bayesian inference for causal effects: the role of randomization. Ann. Stat. 6, 34e58.
Rubin, D., 2006. Matched Sampling for Causal Effects. Cambridge University Press, Cambridge.
Rubin, D., 2007. The design versus the analysis of observational studies for causal effects: parallels with the
design of randomized trials. Stat. Med. 26 (1), 20e30.
Samii, C., Aronow, P., 2012. On equivalencies between design-based and regression-based variance
estimators for randomized experiments. Stat. Probab. Lett. 82, 365e370.
Sacerdote, B., 2001. Peer effects with random assignment: results for Dartmouth roommates. Q. J. Econ.
116 (2), 681e704.
Schochet, P., 2010. Is regression adjustment supported by the Neyman model for causal inference? J. Stat.
Plan. Inference 140, 246e259.
Senn, S., 1994. Testing for baseline balance in clinical trials. Stat. Med. 13, 1715e1726.
Shadish, W., Cook, T., Campbell, D., 2002. Experimental and Quasi-experimental Designs for Generalized
Causal Inference. Houghton Mifﬂin.
Signovitch, J., 2007. Identifying Informative Biological Markers in High-dimensional Genomic Data and
Clinical Trials (Ph.D. thesis). Department of Biostatistics, Harvard University.
Snedecor, G., Cochran, W., 1967. Statistical Methods. Iowa State University Press, Ames, Iowa.
Snedecor, G., Cochran, W., 1989. Statistical Methods. Iowa State University Press, Ames, Iowa.
Su, X., Tsai, C., Wang, H., Nickerson, D., Li, B., 2009. Subgroup analysis via recursive partitioning. J.
Mach. Learn. Res. 10, 141e158.
Taddy, M., Gardner, M., Chen, L., Draper, D., 2015. Heterogeneous Treatment Effects in Digital
Experimentation. Unpublished manuscript, arXiv:1412.8563.
Tian, L., Alizadeh, A., Gentles, A., Tibshirani, R., 2014. A simple method for estimating interactions
between a treatment and a large number of covariates. J. Am. Stat. Assoc. 109 (508), 1517e1532.
Ugander, J., Karrer, B., Backstrom, L., Kleinberg, J., 2013. Graph cluster randomization: network exposure
to multiple universes. In: Proceedings of KDD. ACM.
Wager, S., Athey, S., 2015. Estimation and Inference of Heterogeneous Treatment Effects using Random
Forests. arxiv.org:1510.04342.
Wager, S., Walther, G., 2015. Uniform Convergence of Random Forests via Adaptive Concentration.
arXiv:1503.06388.
Wager, S., Du, W., Taylor, J., Tibshirani, R.J., 2016. High-dimensional regression adjustments in
randomized experiments. Proc. Nat. Acad. Sci. 113 (45), 12673e12678.
Weisburg, H., Pontes, V., June 2015. Post hoc subgroups in clinical trials: anathema or analytics? Clin. Trials.
White, H., 1980. A heteroskedasticity-consistent covariance matrix estimator and a direct test for
heteroskedasticity. Econometrica 48, 817e838.
Wu, J., Hamada, M., 2009. Experiments, Planning, Analysis and Optimization, Wiley Series in Probability
and Statistics.
Young, A., 2016. Channelling Fisher: Randomization Tests and the Statistical Insigniﬁcance of Seemingly
Signiﬁcant Experimental Results. London School of Economics.
Zeileis, A., Hothorn, T., Hornik, K., 2008. Model-based recursive partitioning. J. Comput. Graph. Stat. 17
(2), 492e514.
Zelen, M., 1979. A new design for randomized clinical trials. N. Engl. J. Med. 300, 1242e1245.
Zelen, M., 1990. Randomized consent designs for clinical trials: an update. Stat. Med. 9, 645e656.

