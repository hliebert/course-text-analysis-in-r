Journal of Economic Literature 2009, 47:1, 5–86
http:www.aeaweb.org/articles.php?doi=10.1257/jel.47.1.5

Recent Developments in the
Econometrics of Program Evaluation
Guido W. Imbens and Jeffrey M. Wooldridge*
Many empirical questions in economics and other social sciences depend on causal
effects of programs or policies. In the last two decades, much research has been done
on the econometric and statistical analysis of such causal effects. This recent theoretical literature has built on, and combined features of, earlier work in both the statistics
and econometrics literatures. It has by now reached a level of maturity that makes
it an important tool in many areas of empirical research in economics, including
labor economics, public finance, development economics, industrial organization,
and other areas of empirical microeconomics. In this review, we discuss some of the
recent developments. We focus primarily on practical issues for empirical researchers, as well as provide a historical overview of the area and give references to more
technical research.

1.

Introduction

research in economics and suitable for a
review. In this article, we attempt to present such a review. We will focus on practical issues for empirical researchers, as well as
provide an historical overview of the area and
give references to more technical research.
This review complements and extends other
reviews and discussions, including those by
Richard Blundell and Monica Costa Dias
(2002), Guido W. Imbens (2004), and Joshua
D. Angrist and Alan B. Krueger (1999) and
the books by Paul R. Rosenbaum (1995),
Judea Pearl (2000), Myoung-Jae Lee (2005a),
Donald B. Rubin (2006), Marco Caliendo
(2006), Angrist and Jörn-Steffen Pischke
(2009), Howard S. Bloom (2005), Stephen
L. Morgan and Christopher Winship (2007),
Jeffrey M. Wooldridge (2002) and Imbens and
Rubin (forthcoming). In addition, the reviews
in James J. Heckman, Robert J. LaLonde,

M

any empirical questions in economics
and other social sciences depend on
causal effects of programs or policies. In the
last two decades, much research has been
done on the econometric and statistical analysis of such causal effects. This recent theoretical literature has built on, and combined
features of, earlier work in both the statistics
and econometrics literatures. It has by now
reached a level of maturity that makes it an
important tool in many areas of ­empirical
* Imbens: Harvard University and NBER. Wooldridge:
Michigan State University. Financial support for this
research was generously provided through NSF grants
SES 0136789, 0452590 and 08. We are grateful for comments by Esther Duflo, Caroline Hoxby, Roger Gordon,
Jonathan Beauchamp, Larry Katz, Eduardo Morales, and
two anonymous referees.

5

6

Journal of Economic Literature, Vol. XLVII (March 2009)

and Jeffrey A. Smith (1999), Heckman and
Edward Vytlacil (2007a, 2007b), and Jaap
H. Abbring and Heckman (2007) provide an
excellent overview of the important theoretical work by Heckman and his coauthors in
this area.
The central problem studied in this literature is that of evaluating the effect of the
exposure of a set of units to a program, or
treatment, on some outcome. In economic
studies, the units are typically economic
agents such as individuals, households, markets, firms, counties, states, or countries
but, in other disciplines where evaluation
methods are used, the units can be animals,
plots of land, or physical objects. The treatments can be job search assistance programs,
educational programs, vouchers, laws or
regulations, medical drugs, environmental
exposure, or technologies. A critical feature
is that, in principle, each unit can be exposed
to multiple levels of the treatment. Moreover,
this literature is focused on settings with
observations on units exposed, and not
exposed, to the treatment, with the evaluation based on comparisons of units exposed
and not exposed.1 For example, an individual
may enroll or not in a training program, or he
or she may receive or not receive a voucher,
or be subject to a particular regulation or
not. The object of interest is a comparison
of the two outcomes for the same unit when
exposed, and when not exposed, to the treatment. The problem is that we can at most
observe one of these outcomes because the
unit can be exposed to only one level of the
treatment. Paul W. Holland (1986) refers to
this as the fundamental problem of causal
inference. In order to evaluate the effect of
the treatment, we therefore always need to
compare distinct units receiving the different
levels of the treatment. Such a ­comparison
1 As oppposed to studies where the causal effect of
fundamentally new programs is predicted through direct
identification of preferences and production functions.

can involve different physical units or the
same physical unit at different times.
The problem of evaluating the effect of a
binary treatment or program is a well studied
problem with a long history in both econometrics and statistics. This is true both in
the theoretical literature as well as in the
more applied literature. The econometric
literature goes back to early work by Orley
Ashenfelter (1978) and subsequent work by
Ashenfelter and David Card (1985), Heckman
and Richard Robb (1985), LaLonde (1986),
Thomas Fraker and Rebecca Maynard
(1987), Card and Daniel G. Sullivan (1988),
and Charles F. Manski (1990). Motivated
primarily by applications to the evaluation of
labor market programs in observational settings, the focus in the econometric literature
is traditionally on endogeneity, or self-selection, issues. Individuals who choose to enroll
in a training program are by definition different from those who choose not to enroll.
These differences, if they influence the
response, may invalidate causal comparisons
of outcomes by treatment status, possibly
even after adjusting for observed covariates.
Consequently, many of the initial theoretical studies focused on the use of traditional
econometric methods for dealing with endogeneity, such as fixed effect methods from
panel data analyses, and instrumental variables methods. Subsequently, the econometrics literature has combined insights from
the semiparametric literature to develop new
estimators for a variety of settings, requiring fewer functional form and homogeneity
assumptions.
The statistics literature starts from a different perspective. This literature originates
in the analysis of randomized experiments by
Ronald A. Fisher (1935) and Jerzy SplawaNeyman (1990). From the early 1970s, Rubin
(1973a, 1973b, 1974, 1977, 1978), in a series
of papers, formulated the now dominant
approach to the analysis of causal effects in
observational studies. Rubin proposed the

Imbens and Wooldridge: Econometrics of Program Evaluation
interpretation of causal statements as comparisons of so-called potential outcomes:
pairs of outcomes defined for the same unit
given different levels of exposure to the treatment, with the ressearcher only observing
the potential outcome corresponding to the
level of the treatment received. Models are
developed for the pair of potential outcomes
rather than solely for the observed outcome.
Rubin’s formulation of the evaluation problem, or the problem of causal inference,
labeled the Rubin Causal Model (RCM) by
Holland (1986), is by now standard in both
the statistics and econometrics literature.
One of the attractions of the potential outcomes setup is that from the outset it allows
for general heterogeneity in the effects of the
treatment. Such heterogeneity is important
in practice, and it is important theoretically
as it is often the motivation for the endogeneity problems that concern economists. One
additional advantage of the potential outcome set up is that the parameters of interest
can be defined, and the assumptions stated,
without reference to particular statistical
models.
Of particular importance in Rubin’s
approach is the relationship between treatment assignment and the potential outcomes. The simplest case for analysis is when
assignment to treatment is randomized and,
thus, independent of covariates as well as the
potential outcomes. In such classical randomized experiments, it is straightforward
to obtain estimators for the average effect
of the treatment with attractive properties
under repeated sampling, e.g., the difference
in means by treatment status. Randomized
experiments have been used in some areas
in economics. In the 1970s, negative income
tax experiments received widespread attention. In the late 1980s, following an influential paper by LaLonde (1986) that concluded
econometric methods were unable to replicate experimental results, more emphasis
was put on experimental evaluations of labor

7

market programs, although more recently
this emphasis seems to have weakened a
bit. In the last couple of years, some of the
most interesting experiments have been
conducted in development economics (e.g.,
Edward Miguel and Michael Kremer 2004;
Esther Duflo 2001; Angrist, Eric Bettinger,
and Kremer 2006; Abhijit V. Banerjee
et al. 2007) and behavioral economics (e.g., Marianne Bertrand and Sendhil
Mullainathan 2004). Nevertheless, experimental evaluations remain relatively rare in
economics. More common is the case where
economists analyze data from observational
studies. Observational data generally create challenges in estimating causal effects
but, in one important special case, variously
referred to as unconfoundedness, exogeneity, ignorability, or selection on observables,
questions regarding identification and estimation of the policy effects are fairly well
understood. All these labels refer to some
form of the assumption that adjusting treatment and control groups for differences in
observed covariates, or pretreatment variables, remove all biases in comparisons
between treated and control units. This case
is of great practical relevance, with many
studies relying on some form of this assumption. The semiparametric efficiency bound
has been calculated for this case (Jinyong
Hahn 1998) and various semiparametric
estimators have been proposed (Hahn 1998;
Heckman, Hidehiko Ichimura, and Petra
E. Todd 1998; Keisuke Hirano, Imbens,
and Geert Ridder 2003; Xiaohong Chen,
Han Hong, and Alessandro Tarozzi 2008;
Imbens, Whitney K. Newey, and Ridder
2005; Alberto Abadie and Imbens 2006). We
discuss the current state of this literature,
and the practical recommendations coming
out of it, in detail in this review.
Without unconfoundedness, there is no
general approach to estimating treatment
effects. Various methods have been proposed
for special cases and, in this review, we

8

Journal of Economic Literature, Vol. XLVII (March 2009)

will discuss several of them. One approach
(Rosenbaum and Rubin 1983b; Rosenbaum
1995) consists of sensitivity analyses, where
robustness of estimates to specific limited
departures from unconfoundedness are
investigated. A second approach, developed
by Manski (1990, 2003, 2007), consists of
bounds analyses, where ranges of estimands
consistent with the data and the limited
assumptions the researcher is willing to make,
are derived and estimated. A third approach,
instrumental variables, relies on the presence of additional treatments, the so-called
instruments, that satisfy specific exogeneity
and exclusion restrictions. The formulation
of this method in the context of the potential
outcomes framework is presented in Imbens
and Angrist (1994) and Angrist, Imbens, and
Rubin (1996). A fourth approach applies to
settings where, in its pure form, overlap is
completely absent because the assignment
is a deterministic function of covariates, but
comparisons can be made exploiting continuity of average outcomes as a function of
covariates. This setting, known as the regression discontinuity design, has a long tradition
in statistics (see William R. Shadish, Thomas
D. Cook, and Donald T. Campbell 2002 and
Cook 2008 for historical perspectives), but
has recently been revived in the economics
literature through work by Wilbert van der
Klaauw (2002), Hahn, Todd, and van der
Klaauw (2001), David S. Lee (2001), and Jack
R. Porter (2003). Finally, a fifth approach,
referred to as difference-in-differences, relies
on the presence of additional data in the form
of samples of treated and control units before
and after the treatment. An early application is Ashenfelter and Card (1985). Recent
theoretical work includes Abadie (2005),
Bertrand, Duflo, and Mullainathan (2004),
Stephen G. Donald and Kevin Lang (2007),
and Susan Athey and Imbens (2006).
In this review, we will discuss in detail
some of the new methods that have been
developed in this literature. We will pay

­ articular attention to the practical issues
p
raised by the implementation of these methods. At this stage, the literature has matured
to the extent that it has much to offer the
empirical researcher. Although the evaluation problem is one where identification
problems are important, there is currently a
much better understanding of which assumptions are most useful, as well as a better set
of methods for inference given different sets
of assumptions.
Most of this review will be limited to settings with binary treatments. This is in keeping with the literature, which has largely
focused on binary treatment case. There are
some extensions of these methods to multivalued, and even continuous, treatments
(e.g., Imbens 2000; Michael Lechner 2001;
Lechner and Ruth Miquel 2005; Richard D.
Gill and James M. Robins 2001; Hirano and
Imbens 2004), and some of these extensions
will be discussed in the current review. But
the work in this area is ongoing, and much
remains to be done here.
The running example we will use throughout the paper is that of a job market training
program. Such programs have been among
the leading applications in the economics literature, starting with Ashenfelter (1978) and
including LaLonde (1986) as a particularly
influential study. In such settings, a number
of individuals do, or do not enroll in a training
program, with labor market outcomes, such
as yearly earnings or employment status, as
the main outcome of interest. An individual
not participating in the program may have
chosen not to do so, or may have been ineligible for various reasons. Understanding the
choices made, and constraints faced, by the
potential participants, is a crucial component
of any analysis. In addition to observing participation status and outcome measures, we
typically observe individual background characteristics, such as education levels and age,
as well as information regarding prior labor
market histories, such as earnings at various

Imbens and Wooldridge: Econometrics of Program Evaluation
levels of aggregation (e.g., yearly, quarterly, or
monthly). In addition, we may observe some
of the constraints faced by the individuals,
including measures used to determine eligibility, as well as measures of general labor
market conditions in the local labor markets
faced by potential participants.
2. The Rubin Causal Model: Potential
Outcomes, the Assignment Mechanism,
and Interactions
In this section, we describe the essential
elements of the modern approach to program
evaluation, based on the work by Rubin.
Suppose we wish to analyze a job training
program using observations on N individuals, indexed by i = 1, …, N. Some of these
individuals were enrolled in the training
program. Others were not enrolled, either
because they were ineligible or chose not to
enroll. We use the indicator Wi to indicate
whether individual i enrolled in the training
program, with Wi = 0 if individual i did not,
and Wi = 1 if individual i did, enroll in the
program. We use W to denote the N-vector
with i-th element equal to Wi, and N0 and N1
to denote the number of control and treated
units, respectively. For each unit, we also
observe a K-dimensional column vector of
covariates or pretreatment variables, Xi, with
X denoting the N × K matrix with i-th row
equal to X′i .
2.1 Potential Outcomes

The first element of the RCM is the notion
of potential outcomes. For individual i, for
i = 1, …, N, we postulate the existence of two
potential outcomes, denoted by Yi(0) and
Yi(1). The first, Yi(0), denotes the outcome that
would be realized by individual i if he or she
did not participate in the program. Similarly,
Yi(1) denotes the outcome that would be realized by individual i if he or she did participate in the program. Individual i can either
participate or not participate in the program,

9

but not both, and thus only one of these two
potential outcomes can be realized. Prior to
the assignment being determined, both are
potentially observable, hence the label potential outcomes. If individual i participates in
the program, Yi(1) will be realized and Yi(0)
will ex post be a counterfactual outcome. If,
on the other hand individual i does not participate in the program, Yi(0) will be realized
and Yi(1) will be the ex post counterfactual.
We will denote the realized outcome by Yi,
with Y the N-vector with i-th element equal
to Yi. The preceding discussion implies that
Yi = Yi(Wi) = Yi(0) (1 −Wi) + Yi(1) Wi
=

e

Yi(0) if Wi = 0,
Yi(1) if Wi = 1.

The potential outcomes are tied to the specific manipulation that would have made
one of them the realized outcome. The more
precise the specification of the manipulation,
the more well-defined the potential outcomes are.
This distinction between the pair of potential outcomes (Yi(0),Yi(1)) and the realized
outcome Yi is the hallmark of modern statistical and econometric analyses of treatment
effects. We offer some comments on it. The
potential outcomes framework has important
precursors in a variety of other settings. Most
directly, in the context of randomized experiments, the potential outcome framework was
introduced by Splawa-Neyman (1990) to
derive the properties of estimators and confidence intervals under repeated sampling.
The potential outcomes framework also
has important antecedents in econometrics.
Specifically, it is interesting to compare the
distinction between potential outcomes Yi(0)
and Yi(1) and the realized outcome Yi in
Rubin’s approach to Trygve Haavelmo’s (1943)
work on simultaneous equations ­models
(SEMs). Haavelmo discusses ­identification of

10

Journal of Economic Literature, Vol. XLVII (March 2009)

supply and demand models. He makes a distinction between “any imaginable price π” as
the argument in the demand and supply functions, qd(π) and qs(π), and the “actual price p,”
which is the observed equilibrium price satisfying qs( p) = qd( p). The supply and demand
functions play the same role as the potential
outcomes in Rubin’s approach, with the equilibrium price similar to the realized outcome.
Curiously, Haavelmo’s notational distinction
between equilibrium and potential prices has
gotten blurred in many textbook discussions of
simultaneous equations. In such discussions,
the starting point is often the general formulation YΓ + XB = U for N × M vectors of realized outcomes Y, N × L matrices of exogenous
covariates X, and an N × M matrix of unobserved components U. A nontrivial byproduct
of the potential outcomes approach is that it
forces users of SEMs to articulate what the
potential outcomes are, thereby leading to
better applications of SEMs. A related point is
made in Pearl (2000).
Another area where potential outcomes
are used explicitly is in the econometric
analyses of production functions. Similar to
the potential outcomes framework, a production function g(x, ε) describes production
levels that would be achieved for each value
of a vector of inputs, some observed (x) and
some unobserved (ε). Observed inputs may
be chosen partly as a function of (expected)
values of unobserved inputs. Only for the
level of inputs actually chosen do we observe
the level of the output. Potential outcomes
are also used explicitly in labor market settings by A. D. Roy (1951). Roy models individuals choosing from a set of occupations.
Individuals know what their earnings would
be in each of these occupations and choose
the occupation (treatment) that maximizes
their earnings. Here we see the explicit use
of the potential outcomes, combined with a
specific selection/assignment mechanism,
namely, choosing the treatment with the
highest potential outcome.

The potential outcomes framework has
a number of advantages over a framework
based directly on realized outcomes. The
first advantage of the potential outcome
framework is that it allows us to define causal
effects before specifying the assignment
mechanism, and without making functional
form or distributional assumptions. The most
common definition of the causal effect at the
unit level is as the difference Yi(1) − Yi(0),
but we may wish to look at ratios Yi(1)/Yi(0),
or other functions. Such definitions do not
require us to take a stand on whether the
effect is constant or varies across the population. Further, defining individual-specific
treatment effects using potential outcomes
does not require us to assume endogeneity or
exogeneity of the assignment mechanism. By
contrast, the causal effects are more difficult
to define in terms of the realized outcomes.
Often, researchers write down a regression
function Yi = α + τ · Wi + εi. This regression function is then interpreted as a structural equation, with τ as the causal effect.
Left unclear is whether the causal effect
is constant or not, and what the properties
of the unobserved component, εi, are. The
potential outcomes approach separates these
issues, and allows the researcher to first
define the causal effect of interest without
considering probabilistic properties of the
outcomes or assignment.
The second advantage of the potential outcome approach is that it links the
analysis of causal effects to explicit manipulations. Considering the two potential outcomes forces the researcher to think about
scenarios under which each outcome could
be observed, that is, to consider the kinds
of experiments that could reveal the causal
effects. Doing so clarifies the interpretation
of causal effects. For illustration, consider
a couple of recent examples from the economics literature. First, consider the causal
effects of gender or ethnicity on outcomes
of job applications. Simple comparisons of

Imbens and Wooldridge: Econometrics of Program Evaluation
e­ conomic outcomes by ethnicity are difficult to interpret. Are they the result of discrimination by employers, or are they the
result of differences between applicants,
possibly arising from discrimination at an
earlier stage of life? Now, one can obtain
unambiguous causal interpretations by linking comparisons to specific manipulations.
A recent example is the study by Bertrand
and Mullainathan (2004), who compare callback rates for job applications submitted
with names that suggest African-American
or Caucasian ethnicity. Their study has a
clear manipulation—a name change—and
therefore a clear causal effect. As a second example, consider some recent economic studies that have focused on causal
effects of individual characteristics such
as beauty (e.g., Daniel S. Hamermesh and
Jeff E. Biddle 1994) or height. Do the differences in earnings by ratings on a beauty
scale represent causal effects? One possible
interpretation is that they represent causal
effects of plastic surgery. Such a manipulation would make differences causal, but it
appears unclear whether cross-sectional
correlations between beauty and earnings
in a survey from the general population represent causal effects of plastic surgery.
A third advantage of the potential outcome
approach is that it separates the modeling
of the potential outcomes from that of the
assignment mechanism. Modeling the realized outcome is complicated by the fact that
it combines the potential outcomes and the
assignment mechanism. The researcher may
have very different sources of information to
bear on each. For example, in the labor market program example we can consider the
outcome, say, earnings, in the absence of the
program: Yi(0). We can model this in terms of
individual characteristics and labor market
histories. Similarly, we can model the outcome given enrollment in the program, again
conditional on individual characteristics and
labor market histories. Then finally we can

11

model the probability of enrolling in the program given the earnings in both treatment
arms conditional on individual characteristics. This sequential modeling will lead to a
model for the realized outcome, but it may
be easier than directly specifying a model for
the realized outcome.
A fourth advantage of the potential outcomes approach is that it allows us to formulate probabilistic assumptions in terms of
potentially observable variables, rather than
in terms of unobserved components. In this
approach, many of the critical assumptions
will be formulated as (conditional) independence assumptions involving the potential
outcomes. Assessing their validity requires
the researcher to consider the dependence
structure if all potential outcomes were
observed. By contrast, models in terms of
realized outcomes often formulate the critical assumptions in terms of errors in regression functions. To be specific, consider again
the regression function Yi = α + τ · Wi + εi.
Typically (conditional independence) assumptions are made on the relationship between εi
and Wi. Such assumptions implicitly bundle a
number of assumptions, including functionalform assumptions and substantive exogeneity
assumptions. This bundling makes the plausibility of these assumptions more difficult to
assess.
A fifth advantage of the potential outcome
approach is that it clarifies where the uncertainty in the estimators comes from. Even if
we observe the entire (finite) population (as
is increasingly common with the growing
availability of administrative data sets)—so
we can estimate population averages with no
uncertainty—causal effects will be uncertain
because for each unit at most one of the two
potential outcomes is observed. One may still
use super population arguments to justify
approximations to the finite sample distributions, but such arguments are not required to
motivate the existence of uncertainty about
the causal effect.

12

Journal of Economic Literature, Vol. XLVII (March 2009)

2.2 The Assignment Mechanism
The second ingredient of the RCM is the
assignment mechanism. This is defined as the
conditional probability of receiving the treatment, as a function of potential outcomes and
observed covariates. We distinguish three
classes of assignment mechanisms, in order of
increasing complexity of the required analysis.
The first class of assignment mechanisms is
that of randomized experiments. In randomized experiments, the probability of assignment to treatment does not vary with potential
outcomes, and is a known function of covariates. The leading case is that of a completely
randomized experiment where, in a population of N units, N1 < N randomly chosen units
are assigned to the treatment and the remaining N0 = N − N1 units are in the control group.
There are important variations on this example, such as pairwise randomization, where
initially units are matched in pairs, and in a
second stage one unit in each pair is randomly
assigned to the treatment. Another variant is a
general stratified experiment, where randomization takes place within a finite number of
strata. In any case, there are in practice few
experiments in economics, and most of those
are of the completely randomized experiment
variety, so we shall limit our discussion to this
type of experiment. It should be noted though
that if one has the opportunity to design a
randomized experiment, and if pretreatment
variables are available, stratified experiments
are at least as good as completely randomized
experiments, and typically better, in terms of
expected mean squared error, even in finite
samples. See Imbens et al. (2008) for more
details. The use of formal randomization
has become more widespread in the social
sciences in recent years, sometimes as a formal design for an evaluation and sometimes
as an acceptable way of allocating scarce
resources. The analysis of such experiments
is often straightforward. In practice, however,
researchers have typically limited themselves

to simple mean differences by assignment.
Such analyses are valid, but often they are not
the most powerful tools available to exploit
the randomization. We discuss the analysis
of randomized experiments, including more
powerful randomization-based methods for
inference, in section 4.
The second class of assignment mechanisms maintains the restriction that the
assignment probabilities do not depend on
the potential outcomes, or
Wi   ǁ   AYi(0), Yi(1)B | Xi,

where A   ǁ   B | C denotes conditional independence of A and B given C. However, in contrast
to randomized experiments, the assignment
probabilities are no longer assumed to be a
known function of the covariates. The precise form of this critical assumption, not tied
to functional form or distributional assumptions, was first presented in Rosenbaum
and Rubin (1983b). Following Rubin (1990)
we refer to this assignment mechanism as
unconfounded assignment. Somewhat confusingly, this assumption, or variations on it,
are in the literature also referred to by various other labels. These include selection on
observables,2 exogeneity,3 and conditional
2 Although Heckman, Ichimura, and Todd (1997, page
611) write that “In the language of Heckman and Robb
(1985), matching assumes that selection is on observables”
(their italics), the original definition in Heckman and Robb
(1985, page 163) is not equivalent to unconfoundedness.
In the context of a single cross-section version of their two
equation selection model, Y = Xi′β + Wi α + εi and Wi
= 1{Z i′ γ + ν i > 0}, they define selection bias to refer to
the case where E[εiWi ] ≠ 0, and selection-on-observables
to the case where selection bias is present and caused by
correlation between εi and Z i, rather than by correlation
between εi and ν i .
3 Although X is not exogenous for E[Y (1) − Y (0)],
i
i
i
according to the definitions in Robert F. Engle, David
F. Hendry and Jean-Francois Richard (1983), because
­k nowledge of its marginal distribution contains information about E[Yi(1) − Yi(0)], standard usage of the
term “exogenous” does appear to capture the notion of
unconfoundedness, e.g., Manski et al. (1992), and Imbens
(2004).

Imbens and Wooldridge: Econometrics of Program Evaluation
independence4. Although the analysis of data
with such assignment mechanisms is not as
straightforward as that of randomized experiments, there are now many practical methods available for this case. We review them
in section 5.
The third class of assignment mechanisms
contains all remaining assignment mechanisms with some dependence on potential
outcomes.5 Many of these create substantive
problems for the analysis, for which there is
no general solution. There are a number of
special cases that are by now relatively well
understood, and we discuss these in section 6.
The most prominent of these cases are instrumental variables, regression discontinuity, and
differences-in-differences. In addition, we
discuss two general methods that also relax
the unconfoundedness assumption but do not
replace it with additional assumptions. The
first relaxes the unconfoundedness assumption in a limited way and investigates the sensitivity of the estimates to such violations. The
second drops the unconfoundedness assumption entirely and establishes bounds on estimands of interest. The latter is associated with
the work by Manski (1990, 1995, 2007).
2.3 Interactions and General
Equilibrium Effects
In most of the literature, it is assumed that
treatments received by one unit do not affect
4 E.g., Lechner 2001; A. Colin Cameron and Pravin K.
Trivedi 2005.
5 This includes some mechanisms where the
dependence on potential outcomes does not create any
problems in the analyses. Most prominent in this category
are sequential assignment mechanisms. For example, one
could randomly assign the first ten units to the treatment
or control group with probability 1/2. From then on one
could skew the assignment probability to the treatment
with the most favorable outcomes so far. For example,
if the active treatment looks better than the control
treatment based on the first N units, then the (N + 1)th
unit is assigned to the active treatment with probability
0.8 and vice versa. Such assignment mechanisms are not
very common in economics settings, and we ignore them
in this discussion.

13

outcomes for another unit. Only the level of
the treatment applied to the specific individual is assumed to potentially affect outcomes
for that particular individual. In the statistics
literature, this assumption is referred to as
the Stable-Unit-Treatment-Value-Assumption
(Rubin 1978). In this paper, we mainly focus
on settings where this assumption is maintained. In the current section, we discuss
some of the literature motivated by concerns
about this assumption.
This lack-of-interaction assumption is very
plausible in many biomedical applications.
Whether one individual receives or does
not receive a new treatment for a stroke or
not is unlikely to have a substantial impact
on health outcomes for any other individual.
However, there are also many cases in which
such interactions are a major concern and the
assumption is not plausible. Even in the early
experimental literature, with applications
to the effect of various fertilizers on crop
yields, researchers were cognizant of potential problems with this assumption. In order
to minimize leaking of fertilizer applied to
one plot into an adjacent plot experimenters
used guard rows to physically separate the
plots that were assigned different fertilizers.
A different concern arises in epidemiological
applications when the focus is on treatments
such as vaccines for contagious diseases. In
that case, it is clear that the vaccination of
one unit can affect the outcomes of others in
their proximity, and such effects are a large
part of the focus of the evaluation.
In economic applications, interactions
between individuals are also a serious concern. It is clear that a labor market program
that affects the labor market outcomes for
one individual potentially has an effect on
the labor market outcomes for others. In a
world with a fixed number of jobs, a training program could only redistribute the jobs,
and ignoring this constraint on the number
of jobs by using a partial, instead of a general, equilibrium analysis could lead one to

14

Journal of Economic Literature, Vol. XLVII (March 2009)

erroneously conclude that extending the program to the entire population would raise
aggregate employment. Such concerns have
rarely been addressed in the recent program
evaluation literature. Exceptions include
Heckman, Lance Lochner, and Christopher
Taber (1999) who provide some simulation
evidence for the potential biases that may
result from ignoring these issues.
In practice these general equilibrium effects
may, or may not, be a serious problem. The
indirect effect on one individual of exposure
to the treatment of a few other units is likely to
be much smaller than the direct effect of the
exposure of the first unit itself. Hence, with
most labor market programs both small in
scope and with limited effects on the individual outcomes, it appears unlikely that general
equilibrium effects are substantial and they
can probably be ignored for most purposes.
One general solution to these problems is
to redefine the unit of interest. If the interactions between individuals are at an intermediate level, say a local labor market, or a
classroom, rather than global, one can analyze the data using the local labor market
or classroom as the unit and changing the
no-interaction assumption to require the
absence of interactions among local labor
markets or classrooms. Such aggregation is
likely to make the no-interaction assumption more plausible, albeit at the expense of
reduced precision.
An alternative solution is to directly model
the interactions. This involves specifying
which individuals interact with each other,
and possibly relative magnitudes of these
interactions. In some cases it may be plausible to assume that interactions are limited
to individuals within well-defined, possibly
overlapping groups, with the intensity of
the interactions equal within this group.
This would be the case in a world with a
fixed number of jobs in a local labor market.
Alternatively, it may be that interactions occur
in broader groups but decline in importance

depending on some distance metric, either
geographical distance or proximity in some
economic metric.
The most interesting literature in this area
views the interactions not as a nuisance but
as the primary object of interest. This literature, which includes models of social interactions and peer effects, has been growing
rapidly in the last decade, following the early
work by Manski (1993). See Manski (2000a)
and William Brock and Steven N. Durlauf
(2000) for recent surveys. Empirical work
includes Jeffrey R. Kling, Jeffrey B. Liebman,
and Katz (2007), who look at the effect of
households moving to neighborhoods with
higher average socioeconomic status; Bruce
I. Sacerdote (2001), who studies the effect
of college roommate behavior on a student’s
grades; Edward L. Glaeser, Sacerdote, and
Jose A. Scheinkman (1996), who study social
interactions in criminal behavior; Anne C.
Case and Lawrence F. Katz (1991), who look
at neighborhood effects on disadvantaged
youths; Bryan S. Graham (2008), who infers
interactions from the effect of class size on
the variation in grades; and Angrist and Lang
(2004), who study the effect of desegregation
programs on students’ grades. Many identification and inferential questions remain
unanswered in this literature.
3.

What Are We Interested In?
Estimands and Hypotheses

In this section, we discuss some of the
questions that researchers have asked in this
literature. A key feature of the current literature, and one that makes it more important to
be precise about the questions of interest, is
the accommodation of general ­heterogeneity
in treatment effects. In contrast, in many
early studies it was assumed that the effect
of a treatment was constant, implying that
the effect of various policies could be captured by a single parameter. The essentially
unlimited heterogeneity in the effects of the

Imbens and Wooldridge: Econometrics of Program Evaluation
treatment allowed for in the current literature implies that it is generally not possible
to capture the effects of all policies of interest in terms of a few summary statistics. In
practice researchers have reported estimates
of the effects of a few focal policies. In this
section we describe some of these estimands.
Most of these estimands are average treatment effects, either for the entire population
or for some subpopulation, although some
correspond to other features of the joint distribution of potential outcomes.
Most of the empirical literature has focused
on estimation. Much less attention has been
devoted to testing hypotheses regarding the
properties or presence of treatment effects.
Here we discuss null and alternative hypotheses that may be of interest in settings with
heterogeneous effects. Finally, we discuss
some of the recent literature on decisiontheoretic approaches to program evaluation
that ties estimands more closely to optimal
policies.
3.1 Average Treatment Effects
The econometric literature has largely
focused on average effects of the treatment.
The two most prominent average effects are
defined over an underlying population. In
cases where the entire population can be
sampled, population treatment effects rely on
the notion of a superpopulation, where the
current population that is available is viewed
as just one of many possibilities. In either
case, the the sample of size N is viewed as
a random sample from a large (super-)population, and interest is in the average effect
in the superpopulation.6 The most popular
treatment effect is the Population Average
Treatment Effect (PATE), the population
6 For simplicity, we restrict ourselves to random sampling. Some data sets are obtained by stratified sampling.
Most of the estimators we consider can be adjusted for
stratified sampling. See, for example, Wooldridge (1999,
2007) on inverse probability weighting of averages and
objective functions.

15

expectation of the unit-level causal effect,
Yi(1) − Yi(0):
τ PATE = E[Yi(1) − Yi(0)].
If the policy under consideration would
expose all units to the treatment or none at
all, this is the most relevant quantity. Another
popular estimand is the Population Average
Treatment effect on the Treated (PATT), the
average over the subpopulation of treated
units:
τ PATT = E[Yi(1) − Yi(0) | Wi = 1].
In many observational studies, τ PATT is a
more interesting estimand than the overall
average effect. As an example, consider the
case where a well defined population was
exposed to a treatment, say a job training
program. There may be various possibilities
for a comparison group, including subjects
drawn from public use data sets. In that case,
it is generally not interesting to consider the
effect of the program for the comparison
group: for many members of the comparison
group (e.g., individuals with stable, high-wage
jobs) it is difficult and uninteresting to imagine their being enrolled in the labor market
program. (Of course, the problem of averaging across units that are unlikely to receive
future treatments can be mitigated by more
carefully constructing the comparison group
to be more like the treatment group, making τ PATE a more meaningful parameter. See
the discussion below.) A second case where
τ PATT is the estimand of most interest is in
the setting of a voluntary program where
those not enrolled will never be required
to ­participate in the program. A specific
example is the effect of serving in the military where an interesting question concerns
the foregone earnings for those who served
(Angrist 1998).
In practice, there is typically little motivation presented for the focus on the overall

16

Journal of Economic Literature, Vol. XLVII (March 2009)

average effect or the average effect for the
treated. Take a job training program. The
overall average effect would be the parameter of interest if the policy under consideration is a mandatory exposure to the
treatment versus complete elimination. It
is rare that these are the alternatives, with
more typically exemptions granted to various
subpopulations. Similarly the average effect
for the treated would be informative about
the effect of entirely eliminating the current
program. More plausible regime changes
would correspond to a modest extension of
the program to other jurisdictions, or a contraction to a more narrow population.
A somewhat subtle issue is that we may
wish to separate the extrapolation from the
sample to the superpopulation from the
problem of inference for the sample at hand.
This suggests that, rather than focusing on
PATE or PATT, we might first focus on the
average causal effect conditional on the covariates in the sample,
N

​ 1  ​​∑​​​ E[Yi(1) − Yi(0) | Xi ],
τ CATE = __
N i=1
and, similarly, the average over the subsample of treated units:
τ CATT = ___
​  1  ​ ​∑ ​ ​​E[Yi(1) − Yi(0) | Xi ].
N1 i | Wi=1
If the effect of the treatment or intervention is constant (Yi(1) − Yi(0) = τ for some
constant τ), all four estimands, τ PATE, τ PATT,
τ CATE, and τ CATT, are obviously identical.
However, if there is heterogeneity in the
effect of the treatment, the estimands may
all be different. The difference between
τ PATE and τ CATE (and between τ PATT and
τ CATT ) is relatively subtle. Most estimators
that are attractive for the population treatment effect are also attractive for the corresponding conditional average treatment
effect, and vice versa. Therefore, we do not

have to be particularly concerned with the
distinction between the two estimands at the
estimation stage. However, there is an important difference between the population and
conditional estimands at the inference stage.
If there is heterogeneity in the effect of the
treatment, we can estimate the sample average treatment effect τ CATE more precisely
than the population average treatment effect
τ PATE. When one estimates the variance of an
estimator ​ τ​
ˆ —which can serve as an estimate
for τ PATE or τ CATE —one therefore needs to
be explicit about whether one is interested in
the variance relative to the population or to
the conditional average treatment effect. We
will return to this issue in section 5.
A more general class of estimands includes
average causal effects for subpopulations
and weighted average causal effects. Let 픸
be a subset of the covariate space 핏, and let
τ CATE,픸 denote the conditional average causal
effect for the subpopulation with Xi ∈ 픸:
1 ​ ​∑ ​​​E[Y (1) − Y (0) | X ],
τ CATE,픸 = ​ ___
i
i
i
N픸 i∶Xi∈픸
where N픸 is the number of units with Xi ∈ 픸.
Richard K. Crump et al. (2009) argue for
considering such estimands. Their argument is not based on the intrinsic interest of
these subpopulations. Rather, they show that
such estimands may be much easier to estimate than τ CATE (or τ CATT ). Instead of solely
reporting an imprecisely estimated average
effect for the overall population, they suggest it may be informative to also report
a precise estimate for the average effect of
some subpopulation. They then propose a
particular set 픸 for which the average effect
is most easily estimable. See section 5.10.2
for more details. The Crump et al. estimates
would not necessarily have as much external
validity as estimates for the overall population, but they may be much more informative
for the sample at hand. In any case, in many
instances the larger policy questions concern

Imbens and Wooldridge: Econometrics of Program Evaluation
extensions of the interventions or treatments
to other populations, so that external validity
may be elusive irrespective of the estimand.
In settings with selection on unobservables
the enumeration of the estimands of interest
becomes more complicated. A leading case
is instrumental variables. In the presence of
heterogeneity in the effect of the treatment
one can typically not identify the average
effect of the treatment even in the presence
of valid instruments. There are two new
approaches in the recent literature. One is to
focus on bounds for well-defined estimands
such as the average effect τ PATE or τ CATE.
Manski (1990, 2003) developed this approach
in a series of papers. An alternative is to focus
on estimands that can be identified under
weaker conditions than those required for the
average treatment effect. Imbens and Angrist
(1994) show that one can, under much weaker
conditions than required for identification of
τ PATE, identify the average effect for the subpopulation of units whose treatment status is
affected by the instrument. They refer to this
subpopulation as the compliers. This does not
directly fit into the classification above since
the subpopulation is not defined solely in
terms of covariates. We discuss this estimand
in more detail in section 6.3.
3.2 Quantile and Distributional Treatment
Effects and Other Estimands
An alternative class of estimands consists
of quantile treatment effects. These have
only recently been studied and applied in
the economics literature, although they were
introduced in the statistics literature in the
1970s. Kjell Doksum (1974) and Erich L.
Lehmann (1974) define
(1) 	

​−1 ​(q) − F
​ Y(0)
​−1 ​(q),
τq = ​FY(1)

as the q-th quantile treatment effect. There
are some important issues in interpreting
these quantile treatment effects. First, note
that these quantiles effects are defined as

17

differences between quantiles of the two
marginal potential outcome distributions,
and not as quantiles of the unit level effect,
(2) 	

τ​
​−1
​(q).
​ ˜ q = ​FY(1)−Y(0)

In general, the quantile of the difference, ​ τ​
˜ q, differs from the difference in the
quantiles, τq, unless there is perfect rank
correlation between the potential outcomes
Yi(0) and Yi(1) (the leading case of this is
the constant additive treatment effect).
The quantile treatment effects, τq, have
received much more attention, and in our
view rightly so, than the quantiles of the
treatment effect, τ​
​ ˜ q. There are two issues
regarding the choice between a focus on
the difference in quantiles versus quantiles
of the difference. The first issue is substantial. Suppose a policy maker is faced with
the choice of assigning all members of a
subpopulation, homogenous in covariates
Xi, to the treatment group, or assigning all
of them to the control group. The resulting outcome distribution is either f Y(0) ( y) or
f Y(1) (y), assuming the subpopulation is large.
Hence the choice should be governed by
preferences of the policymaker over these
distributions (which can often be summarized by differences in the quantiles), and
not depend on aspects of the joint distribution f Y(0),Y(1) ( y, z) that do not affect the
two marginal distributions. (See Heckman
and Smith 1997 for a somewhat different
view.) The second issue is statistical. In general the ​ τ​
˜ q are not (point-)identified without
assumptions on the rank correlation between
the potential outcomes, even with data from
a randomized experiment. In a randomized
experiment, one can identify f Y(0) ( y) and
f Y(1) (y) (and any functional thereof) but not
the joint distribution f Y(0),Y(1) ( y, z). Note that
this issue does not arise if we look at average
effects because the mean of the difference is
equal to the difference of the means: E[Yi(1)
− Yi(0)] = E[Yi(1)] − E[Yi(0)].

Journal of Economic Literature, Vol. XLVII (March 2009)

18

A complication facing researchers interested in quantile treatment effects is that
the difference in a marginal quantile, τq, is in
general not equal to the average difference
in the conditional quantiles, where the latter
are defined as
​−1 | X​(q | x) − F
​ Y(0)
​−1 | X​(q | x).
τq(x) = ​FY(1)
In other words, even if we succeed in estimating τq(x), we cannot simply average τq(Xi)
across i to consistently estimate τq. Marianne
Bitler, Jonah Gelbach, and Hilary Hoynes
(2006) estimate quantile treatment effects
in a randomized evaluation of a job training
program. Sergio Firpo (2007) develops methods for estimating τq in observational studies
given unconfoundedness. Abadie, Angrist,
and Imbens (2002) and Victor Chernozhukov
and Christian B. Hansen (2005) study quantile treatment effects in instrumental variables settings.
3.3 Testing
The literature on hypothesis testing in program evaluation is relatively limited. Most of
the testing in applied work has focused on
the null hypothesis that the average effect of
interest is zero. Because many of the commonly used estimators for average treatment
effects are asymptotically normally distributed with zero asymptotic bias, it follows
that standard confidence intervals (the point
estimate plus or minus a constant times the
standard error) can be used for testing such
hypotheses. However, there are other interesting hypotheses to consider.
One question of interest is whether there
is any effect of the program, that is whether
the distribution of Yi(1) differs from that of
Yi(0). This is equivalent to the hypothesis
that not just the mean, but all moments,
are identical in the two treatment groups.
Abadie (2002) studies such tests in the
­settings with ­randomized experiments as
well as settings with instrumental variables

using Kolmogorov-Smirnov type testing
procedures.
A second set of questions concerns treatment effect heterogeneity. Even if the average effect is zero, it may be important to
establish whether a targeted implementation of the intervention, with only those who
can expect to benefit from the intervention
assigned to it, could improve average outcomes. In addition, in cases where there is
not sufficient information to obtain precise inferences for the average causal effect
τ PATE, it may still be possible to establish
whether there are any subpopulations with an
average effect positive or different from zero,
or whether there are subpopulations with an
average effect exceeding some threshold. It
may also be interesting to test whether there
is any evidence of heterogeneity in the treatment effect by observable characteristics.
This bears heavily on the question whether
the estimands are useful for extrapolation to
other populations which may differ in terms
of some observable characteristics. Crump et
al. (2008) study these questions in settings
with unconfounded treatment assignment.
3.4 Decision-Theoretic Questions
Recently, a small but innovative literature
has started to move away from the focus
on summary statistics of the distribution of
treatment effects or potential outcomes to
directly address policies of interest. This is
very much a literature in progress. Manski
(2000b, 2001, 2002, 2004), Rajeev H. Dehejia
(2005b), and Hirano and Porter (2008) study
the problem faced by program administrators who can assign individuals to the active
treatment or to the control group. These
administrators have available two pieces of
information. First, covariate information for
these individuals, and second, information
about the efficacy of the treatment based on
a finite sample of other individuals for whom
both outcome and covariate information is
available. The administrator may care about

Imbens and Wooldridge: Econometrics of Program Evaluation
the entire distribution of outcomes, or solely
about average outcomes, and may also take
into account costs associated with participation. If the administrator knew exactly the
conditional distribution of the potential outcomes given the covariate information this
would be a simple problem: the administrator would simply compare the expected welfare for different rules and choose the one
with the highest value. However, the administrator does not have this knowledge and
needs to make a decision given uncertainty
about these distributions. In these settings, it
is clearly important that the statistical model
allows for heterogeneity in the treatment
effects.
Graham, Imbens, and Ridder (2006)
extend the type of problems studied in this
literature by incorporating resource constraints. They focus on problems that include
as a special case the problem of allocating a
fixed number of slots in a program to a set of
individuals on the basis of observable characteristics of these individuals given a random
sample of individuals for whom outcome and
covariate information is available.
4.

Randomized Experiments

Experimental evaluations have traditionally been rare in economics. In many cases
ethical considerations, as well as the reluctance of administrators to deny services to
randomly selected individuals after they
have been deemed eligible, have made it
difficult to get approval for, and implement,
randomized evaluations. Nevertheless, the
few experiments that have been conducted,
including some of the labor market training
programs, have generally been ­influential,
sometimes extremely so. More recently,
many exciting and thought-provoking experiments have been conducted in development
economics, raising new issues of design and
analysis (see Duflo, Rachel Glennerster, and
Kremer 2008 for a review).

19

With experimental data the statistical analysis is generally straightforward.
Differencing average outcomes by treatment
status or, equivalently, regressing the outcome on an intercept and an indicator for the
treatment, leads to an unbiased estimator for
the average effect of the treatment. Adding
covariates to the regression function typically
improves precision without jeopardizing consistency because the randomization implies
that in large samples the treatment indicator
and the covariates are independent. In practice, researchers have rarely gone beyond
basic regression methods. In principle,
however, there are additional methods that
can be useful in these settings. In section
4.2, we review one important experimental
technique, randomization-based inference,
including Fisher’s method for calculating
exact p-values, that deserves wider usage in
social sciences. See Rosenbaum (1995) for a
textbook discussion.
4.1 Randomized Experiments in Economics
Randomized experiments have a long
tradition in biostatistics. In this literature
they are often viewed as the only credible approach to establishing causality. For
example, the United States Food and Drug
Administration typically requires evidence
from randomized experiments in order to
approve new drugs and medical procedures.
A first comment concerns the fact that even
randomized experiments rely to some extent
on substantive knowledge. It is only once
the researcher is willing to limit interactions
between units that randomization can establish causal effects. In settings with potentially unrestricted interactions between
units, randomization by itself cannot solve
the ­identification problems required for
establishing causality. In biomedical settings,
where such interaction effects are often arguably absent, randomized experiments are
­therefore ­particularly attractive. Moreover,
in biomedical settings it is often possible to

20

Journal of Economic Literature, Vol. XLVII (March 2009)

keep the units ignorant of their treatment
status, further enhancing the interpretation
of the estimated effects as causal effects of
the treatment, and thus improving the external validity.
In the economics literature randomization
has played a much less prominent role. At various times social experiments have been conducted, but they have rarely been viewed as
the sole method for establishing causality, and
in fact they have sometimes been regarded
with some suspicion concerning the relevance of the results for policy purposes (e.g.,
Heckman and Smith 1995; see Gary Burtless
1995 for a more positive view of experiments
in social sciences). Part of this may be due to
the fact that for the treatments of interest to
economists, e.g., education and labor market programs, it is generally impossible to do
blind or double-blind experiments, creating
the possibility of placebo effects that compromise the internal validity of the estimates.
Nevertheless, this suspicion often downplays the fact that many of the concerns that
have been raised in the context of randomized experiments, including those related to
missing data, and external validity, are often
equally present in observational studies.
Among the early social experiments in economics were the negative income tax experiments in Seattle and Denver in the early
1970s, formally referred to as the Seattle and
Denver Income Maintenance Experiments
(SIME and DIME). In the 1980s, a number
of papers called into question the reliability of
econometric and statistical methods for estimating causal effects in observational studies.
In particular, LaLonde (1986) and Fraker and
Maynard (1987), using data from the National
Supported Work (NSW) programs, suggested
that widely used econometric methods were
unable to replicate the results from experimental evaluations. These influential conclusions encouraged government agencies to
insist on the inclusion of experimental evaluation components in job training programs.

Examples of such programs include the
Greater Avenues to INdependence (GAIN)
programs (e.g., James Riccio and Daniel
Friedlander 1992, the WIN programs (e.g.,
Judith M. Gueron and Edward Pauly 1991;
Friedlander and Gueron 1992; Friedlander
and Philip K. Robins 1995), the Self
Sufficiency Project in Canada (Card and Dean
R. Hyslop 2005, and Card and Robins 1996),
and the Statistical Assistance for Programme
Selection in Switzerland (Stefanie Behncke,
Markus Frölich, and Lechner 2006). Like
the NSW evaluation, these experiments have
been useful not merely in establishing the
effects of particular programs but also in providing fertile testing grounds for new statistical evaluations methods.
Recently there has been a large number of
exciting and innovative experiments, mainly
in development economics but also in others areas, including public finance (Duflo
and Emmanuel Saez 2003; Duflo et al.
2006; Raj Chetty, Adam Looney, and Kory
Kroft forthcoming). The experiments in
development economics include many educational experiments (e.g., T. Paul Schultz
2001; Orazio Attanasio, Costas Meghir,
and Ana Santiago 2005; Duflo and Rema
Hanna 2005; Banerjee et al. 2007; Duflo
2001; Miguel and Kremer 2004). Others
study topics as wide-ranging as corruption
(Benjamin A. Olken 2007; Claudio Ferraz
and Frederico Finan 2008) or gender issues
in politics (Raghabendra Chattopadhyay and
Duflo 2004). In a number of these experiments, economists have been involved from
the beginning in the design of the evaluations, leading to closer connections between
the substantive economic questions and the
design of the experiments, thus improving
the ability of these studies to lead to conclusive answers to interesting questions.
These experiments have also led to renewed
interest in questions of optimal design.
Some of these issues are discussed in Duflo,
Glennerster, and Kremer (2008), Miriam

Imbens and Wooldridge: Econometrics of Program Evaluation
Bruhn and David McKenzie (2008), and
Imbens et al. (2008).
4.2 Randomization-Based Inference and
Fisher’s Exact P-Values
Fisher (1935) was interested in calculating
p-values for hypotheses regarding the effect of
treatments. The aim is to provide exact inferences for a finite population of size N. This
finite population may be a random sample
from a large superpopulation, but that is not
exploited in the analysis. The inference is nonparametric in that it does not make functional
form assumptions regarding the effects; it is
exact in that it does not rely on large sample
approximations. In other words, the p-values
coming out of this analysis are exact and valid
irrespective of the sample size.
The most common null hypothesis in
Fisher’s framework is that of no effect of the
treatment for any unit in this population,
against the alternative that, at least for some
units, there is a non-zero effect:
H0 : Yi(0) = Yi(1), ∀i = 1, …, N,
against Ha : ∃i such that Yi(0) ≠ Yi(1).
It is not important that the null hypothesis
is that the effects are all zero. What is essential is that the null hypothesis is sharp, that
is, the null hypothesis specifies the value of
all unobserved potential outcomes for each
unit. A more general null hypothesis could
be that Yi(0) = Yi(1) + c for some prespecified c, or that Yi(0) = Yi(1) + ci for some set of
prespecified ci. Importantly, this framework
cannot accommodate null hypotheses such
as the average effect of the treatment is zero,
against the alternative hypothesis of a nonzero average effect, or
1  ​​∑​  ​Y A(1) − Y (0)B = 0,
H′0 : ​ __
i
N i i

1  ​​∑​  ​Y A(1) − Y (0)B ≠ 0.
against H′a : ​ __
i
N i i

21

Whether the null of no effect for any unit
versus the null of no effect on average is
more interesting was the subject of a testy
exchange between Fisher (who focused on
the first) and Neyman (who thought the latter was the interesting hypothesis, and who
stated that the first was only of academic
interest) in Splawa-Neyman (1990). Putting
the argument about its ultimate relevance
aside, Fisher’s test is a powerful tool for
establishing whether a treatment has any
effect. It is not essential in this framework
that the probabilities of assignment to the
treatment group are equal for all units. It is
crucial, however, that the probability of any
particular assignment vector is known. These
probabilities may differ by unit provided the
probabilities are known.
The implication of Fisher’s framework is
that, under the null hypothesis, we know the
exact value of all the missing potential outcomes. Thus there are no nuisance parameters under the null hypothesis. As a result,
we can deduce the distribution of any statistic, that is, any function of the realized values
​N ​, generated by the randomizaof (Yi, Wi​)i=1
tion. For example, suppose the statistic is
the average difference between
and
__ treated
__
Y ​
−
​
Y ​
,
where
​
control
outcomes,
T(W,
Y)
=
​
1
0
__
Y ​w = ​∑ i∶Wi=w​ ​ ​Yi /Nw , for w = 0, 1. Now suppose we had assigned a different set of units
to the treatment. Denote the vector
of alter    
˜ . Under
native treatment assignments by ​ W​
the null hypothesis we know all the potential
outcomes and thus we can deduce what the
value of the statistic would have been     
under
˜ , Y).
that alternative assignment, namely T(​ W​
We can infer the value of the statistic for all
possible values of the assignment vector W,
and since we know the distribution of W we
can deduce the distribution of T(W, Y). The
distribution generated by the randomization
of the treatment assignment is referred to as
the randomization distribution. The p-value
of the statistic is then calculated as the probability of a value for the statistic that is at

22

Journal of Economic Literature, Vol. XLVII (March 2009)

least as large, in absolute value, as that of the
observed statistic, T(W, Y).
In moderately large samples, it is typically not feasible to calculate the exact
p-values for these tests. In that case, one
can approximate the p-value by basing it on
a large number of draws from the randomization distribution. Here the approximation
error is of a very different nature than that
in typical large sample approximations: it is
controlled by the researcher, and if more
precision is desired one can simply increase
the number of draws from the randomization distribution.
In the form described above, with the
­statistic equal to the difference in averages
by treatment status, the results are typically
not that different from those using Wald
tests based on large sample normal approximations to the sampling
distribution
to the
__
__
difference in means ​Y ​1 − ​Y ​0, as long as the
sample size is moderately large. The Fisher
approach to calculating p-values is much
more interesting with other choices for
the statistic. For example, as advocated by
Rosenbaum in a series of papers (Rosenbaum
1984a, 1995), a generally attractive choice is
the difference in average ranks by treatment
status. First the outcome is converted into
ranks (typically with, in case of ties, all possible rank orderings averaged), and then the
test is applied using the average difference
in ranks by treatment status as the statistic.
The test is still exact, with its exact distribution under the null hypothesis known as
the Wilcoxon distribution. Naturally, the test
based on ranks is less sensitive to ­outliers
than the test based on the difference in
means.
If the focus is on establishing whether the
treatment has some effect on the outcomes,
rather than on estimating the average size
of the effect, such rank tests are much more
likely to provide informative conclusions
than standard Wald tests based differences
in averages by treatment status. To illustrate

this point, we took data from eight randomized evaluations of labor market programs.
Four of the programs are from the WIN
demonstration programs. The four evaluations took place in Arkansas, Baltimore, San
Diego, and Virginia. See Gueron and Pauly
(1991), Friedlander and Gueron (1992), David
Greenberg and Michael Wiseman (1992),
and Friedlander and Robins (1995) for more
detailed discussions of each of these evaluations. The second set of four programs is
from the GAIN programs in California. The
four locations are Alameda, Los Angeles,
Riverside, and San Diego. See Riccio and
Friedlander (1992), Riccio, Friedlander, and
Freedman (1994), and Dehejia (2003) for
more details on these programs and their
evaluations. In each location, we take as the
outcome total earnings for the first (GAIN)
or second (WIN) year following the program,
and we focus on the subsample of individuals
who had positive earnings at some point prior
to the program. We calculate three p-values
for each location. The first p-value is based
on the normal approximation to the t-statistic calculated as the difference in average
outcomes for treated and control individuals divided by the estimated standard error.
The second p-value is based on randomization inference using the difference in average outcomes by treatment status. And the
third p-value is based on the randomization
distribution using the difference in average
ranks by treatment status as the statistic. The
results are in table 1.
In all eight cases, the p-values based on
the t-test are very similar to those based
on randomization inference. This outcome
is not surprising given the reasonably large
sample sizes, ranging from 71 (Arkansas,
WIN) to 4,779 (San Diego, GAIN). However,
in a number of cases, the p-value for the
rank test is fairly different from that based
on the level difference. In both sets of four
locations there is one location where the
rank test suggests a clear rejection at the

Imbens and Wooldridge: Econometrics of Program Evaluation

23

Table 1

P-values for Fisher Exact Tests: Ranks versus Levels
Sample Size
Program

Location

GAIN
GAIN
GAIN
GAIN
WIN
WIN
WIN
WIN

Alameda
Los Angeles
Riverside
San Diego
Arkansas
Baltimore
San Diego
Virginia

p-values

Controls

Treated

t-test

FET (levels)

FET (ranks)

601
1400
1040
1154
37
260
257
154

597
2995
4405
6978
34
222
264
331

0.835
0.544
0.000
0.057
0.750
0.339
0.136
0.960

0.836
0.531
0.000
0.068
0.753
0.339
0.137
0.957

0.890
0.561
0.000
0.018
0.805
0.286
0.024
0.249

5 percent level whereas the level-based test
would suggest that the null hypothesis of no
effect should not be rejected at the 5 percent level. In the WIN (San Diego) evaluation, the p-value goes from 0.068 (levels) to
0.024 (ranks), and in the GAIN (San Diego)
evaluation, the p-value goes from 0.136 (levels) to 0.018 (ranks). It is not surprising that
the tests give different results. Earnings data
are very skewed. A large proportion of the
populations participating in these programs
have zero earnings even after conditioning
on positive past earnings, and the earnings
distribution for those with positive earnings
is skewed. In those cases, a rank-based test
is likely to have more power against alternatives that shift the distribution toward higher
earnings than tests based on the difference
in means.
As a general matter it would be useful in
randomized experiments to include such
results for rank-based p-values, as a generally
applicable way of establishing whether the
treatment has any effect. As with all omnibus
tests, one should use caution in interpreting
a rejection, as the test can pick up interesting
changes in the distribution (such as a mean
or median effect) but also less interesting
changes (such as higher moments about the
mean).

5.

Estimation and Inference under
Unconfoundedness

Methods for estimation of average treatment effects under unconfoundedness are
the most widely used in this literature. The
central paper in this literature, which introduces the key assumptions, is Rosenbaum
and Rubin (1983b), although the literature
goes further back (e.g., William G. Cochran
1968; Cochran and Rubin 1973; Rubin 1977).
Often the unconfoundedness assumption,
which requires that conditional on observed
covariates there are no unobserved factors
that are associated both with the assignment
and with the potential outcomes, is controversial. Nevertheless, in practice, where often
data have been collected in order to make this
assumption more plausible, there are many
cases where there is no clearly superior alternative, and the only alternative is to abandon
the attempt to get precise inferences. In this
section, we discuss some of these methods
and the issues related to them. A general
theme of this literature is that the concern is
more with biases than with efficiency.
Among the many recent economic applications relying on assumptions of this type
are Blundell et al. (2001), Angrist (1998),
Card and Hyslop (2005), Card and Brian P.

24

Journal of Economic Literature, Vol. XLVII (March 2009)

McCall (1996), V. Joseph Hotz, Imbens, and
Jacob A. Klerman (2006), Card and Phillip
B. Levine (1994), Card, Carlos Dobkin, and
Nicole Maestas (2004), Hotz, Imbens, and
Julie H. Mortimer (2005), Lechner (2002a),
Abadie and Javier Gardeazabal (2003), and
Bloom (2005).
This setting is closely related to that underlying standard multiple regression analysis
with a rich set of controls. See, for example,
Burt S. Barnow, Glend G. Cain, and Arthur
S. Goldberger (1980). Unconfoundedness
implies that we have a sufficiently rich set of
predictors for the treatment indicator, contained in the vector of covariates Xi, such
that adjusting for differences in these covariates leads to valid estimates of causal effects.
Combined with linearity assumptions of the
conditional expectations of the potential outcomes given covariates, the unconfoundedness
assumption justifies linear regression. But in
the last fifteen years the literature has moved
away from the earlier emphasis on regression
methods. The main reason is that, although
locally linearity of the regression functions
may be a reasonable approximation, in many
cases the estimated average treatment effects
based on regression methods can be severely
biased if the linear ­approximation is not accurate globally. To assess the potential problems
with (global) regression methods, it is useful
to report summary statistics of the covariates
by treatment status. In particular, one may
wish to report, for each covariate, the difference in averages by treatment status, scaled
by the square root of the sum of the variances, as a scale-free measure of the difference in ­distributions. To be specific, one may
wish to report the normalized difference
(3) 	

__

__

​ 0
​X ​1 − X ​
______
 ​,
Δ X = ​ ________
2
​ √​S0​ ​ ​+ ​S1​2​ ​ ​

where for w = 0, 1, ​Sw​2 ​​ = ∑
​ i∶Wi=w​ ​ ​(Xi −
__
X ​
​ w)2 /(Nw − 1), the sample variance of Xi

in the subsample with treatment WI = w.
Imbens and Rubin (forthcoming) suggest as a
rule of thumb that with a normalized difference exceeding one quarter, linear regression
methods tend to be sensitive to the specification. Note the difference with the often
reported t-statistic for the null hypothesis of
equal means,
(4) 	

__

__

​
​X ​1 − X ​
___________
  0    ​ .
T = ​ _____________
2
​ √​S0​ ​ ​/N0 + ​S1​2​ ​/  
N1 ​

The reason for focusing on the normalized
difference, (3), rather than on the ­t-statistic,
(4), as a measure of the degree of difficulty in
the statistical problem of adjusting for differences in covariates, comes from their relation
to the sample size. Clearly, simply increasing
the sample size does not make the problem
of inference for the average treatment effect
inherently more difficult. However, quadrupling the sample size leads, in expectation,
to a doubling of the t-statistic. In contrast,
increasing the sample size does not systematically affect the normalized difference. In
the landmark LaLonde (1986) paper the normalized difference in mean exceeds unity for
many of the covariates, immediately showing that standard regression methods are
unlikely to lead to credible results for those
data, even if one views unconfoundedness as
a reasonable assumption.
As a result of the concerns with the sensitivity of results based on linear regression methods to seemingly minor changes
in specification, the literature has moved to
more sophisticated methods for adjusting for
differences in covariates. Some of these more
sophisticated methods use the propensity
score—the conditional probability of receiving the treatment—in various ways. Others
rely on pairwise matching of treated units to
control units, using values of the covariates to
match. Although these estimators appear at
first sight to be quite different, many (­including

Imbens and Wooldridge: Econometrics of Program Evaluation
nonparametric versions of the regression estimators) in fact achieve the semiparametric
efficiency bound; thus, they would tend to be
similar in large samples. Choices among them
typically rely on small sample arguments,
which are rarely formalized, and which do not
uniformly favor one estimator over another.
Most estimators currently in use can be written as the difference of a weighted average of
the treated and control outcomes, with the
weights in both groups adding up to one:
N

ˆ =∑
​ ​​  ​λi · Yi,
 ​   τ​
i=1

with ∑
​ ​ ​λi = 1,
i∶Wi=1

​∑ ​ ​λi = −1.

i∶Wi=0

The estimators differ in the way the weights λi
depend on the full vector of assignments and
matrix of covariates (including those of other
units). For example, some estimators implicitly
allow the weights to be negative for the treated
units and positive for controls units, whereas
others do not. In addition, some depend on
essentially all other units whereas others
depend only on units with similar covariate
values. Nevertheless, despite the commonalities of the estimators and large sample equivalence results, in practice the performance of
the estimators can be quite different, particularly in terms of robustness and bias. Little
is known about finite sample properties. The
few simulation studies include Zhong Zhao
(2004), Frölich (2004a), and Matias Busso,
John DiNardo, and Justin McCrary (2008).
On a more positive note, some ­understanding
has been reached regarding the sensitivity of
specific estimators to particular configurations of the data, such as limited overlap in
covariate distributions. Currently, the best
practice is to combine linear regression with
either propensity score or matching methods
in ways that explicitly rely on local, rather than
global, linear approximations to the regression
functions.

25

An ongoing discussion concerns the role
of the propensity score, e(x) = pr(Wi = 1 | Xi
= x), introduced by Rosenbaum and Rubin
(1983b), and indeed whether there is any
role for this concept. See for recent contributions to this discussion Hahn (1998), Imbens
(2004), Angrist and Hahn (2004), Peter C.
Austin (2008a, 2008b), Dehejia (2005a),
Smith and Todd (2001, 2005), Heckman,
Ichimura, and Todd (1998), Frölich (2004a,
2004b), B. B. Hansen (2008), Jennifer Hill
(2008), Robins and Ya’acov Ritov (1997),
Rubin (1997, 2006), and Elizabeth A. Stuart
(2008).
In this section, we first discuss the key
assumptions underlying an analysis based on
unconfoundedness. We then review some of
the efficiency bound results for average treatment effects. Next, in sections 5.3 to 5.5, we
briefly review the basic methods relying on
regression, propensity score methods, and
matching. Although still fairly widely used,
we do not recommend these methods in practice. In sections 5.6 to 5.8, we discuss three
of the combination methods that we view as
more attractive and recommend in practice.
We discuss estimating variances in section
5.9. Next we discuss implications of lack of
overlap in the covariate distributions. In particular, we discuss two general ­methods for
constructing samples with improved covariate balance, both relying heavily on the propensity score. In section 5.11, we describe
methods that can be used to assess the plausibility of the unconfoundedness assumption,
even though this assumption is not directly
testable. We discuss methods for testing for
the presence of average treatment effects
and for the presence of treatment effect heterogeneity under unconfoundedness in section 5.12.
5.1 Identification
The key assumption is unconfoundedness, introduced by Rosenbaum and Rubin
(1983b),

26

Journal of Economic Literature, Vol. XLVII (March 2009)

Assumption 1 (Unconfoundedness)
Wi   ǁ   (Yi(0), Yi(1)) | Xi .
The unconfoundedness assumption is often
controversial, as it assumes that beyond the
observed covariates Xi there are no (unobserved) characteristics of the individual
associated both with the potential outcomes
and the treatment.7 Nevertheless, this kind
of assumption is used routinely in multiple
regression analysis. In fact, suppose we
assume that the treatment effect, τ, is constant, so that, for each random draw i, τ =
Yi(1) − Yi(0). Further, assume that Yi(0) = α
+ β′ Xi + εi, where εi = Yi(0) − E[Yi(0) | Xi ]
is the residual capturing the unobservables
affecting the response in the absence of
treatment. Then, with the observed outcome
defined as Yi = (1 − Wi) · Yi(0) + Wi · Yi(1),
we can write
Yi = α + τ · Wi + β′Xi + εi,
and unconfoundedness is equivalent to independence of εi and of Wi, conditional on Xi.
Imbens (2004) discusses some economic
models that imply unconfoundedness. These
models assume agents choose to participate in
a program if the benefits, equal to the difference in potential outcomes, exceed the costs
associated with participation. It is important
here that there is a distinction between the
objective of the participant (net benefits), and
the outcome that is the focus of the reseacher
(gross benefits). (See Athey and Scott Stern
1998 for some discussion.) Unconfoundedness
is implied by independence of the costs and
benefits, conditional on observed covariates.

7 Unconfoundedness generally fails if the covariates
themselves are affected by treatment. Wooldridge (2005)
provides a simple example where treatment is randomized with respect to the counterfactual outcomes but not
with respect to the covariates. Unconfoundedness is easily
shown to fail.

The second assumption used to identify
treatment effects is that for all possible values of the covariates, there are both treated
and control units.
Assumption 2 (Overlap)
0 < pr(Wi = 1 | Xi = x) < 1,

for all x.

We call this the overlap assumption as it
implies that the support of the conditional
distribution of Xi given Wi = 0 overlaps completely with that of the conditional distribution of Xi given Wi = 1.
​N ​ we
With a random sample (Wi, Xi​)i=1
can estimate the propensity score e(x)
= pr(Wi = 1 | X i = x), and this can provide
some guidance for determining whether
the overlap assumption holds. Of course
­common parametric models, such as probit
and logit, ensure that all estimated probabilities are strictly between zero and one,
and so examining the fitted probabilities
from such models can be misleading. We
discuss approaches for improving overlap
in 5.10.
The combination of unconfoundedness
and overlap was referred to by Rosenbaum
and Rubin’s (1983b) as strong ignorability.
There are various ways to establish identification of various average treatment effects
under strong ignorability. Perhaps the easiest is to note that τ(x) ≡ E[Yi(1) − Yi(0) | Xi
= x] is identified for x in the support of the
covariates:
(5) 	τ(x) = E[Yi(1) | Xi = x] − E[Yi(0) | Xi = x]
= E[Yi(1) | Wi = 1, Xi = x]
− E[Yi(0) | Wi = 0, Xi = x]
= E[Yi | Wi = 1, Xi = x]
− E[Yi | Wi = 0, Xi = x],

Imbens and Wooldridge: Econometrics of Program Evaluation
where the second equality follows by unconfoundedness: E[Yi(w) | Wi = w, Xi ] does not
depend on w. By the overlap assumption, we
can estimate both terms in the last line, and
therefore we can identify τ(x). Given that we
can identify τ(x) for all x, we can identify the
expected value across the population distribution of the covariates,
(6) 	

τ PATE = E[τ (Xi)],

as well as τ PATT and other estimands.
5.2 Efficiency Bounds

Before discussing specific estimation
methods, it is useful to see what we can learn
about the parameters of interest, given just
the strong ignorability of treatment assignment assumption, without functional form or
distributional assumptions. In order to do so,
we need some additional notation. Let ​σ0​2​ ​(x)
= 핍(Yi(0) | Xi = x) and ​σ1​2​ ​(x) = 핍(Yi(1) | Xi
= x) denote the conditional variances of the
potential outcomes given the covariates.
Hahn (1998) derives the __
lower bounds for
asymptotic variances of ​ √N ​-consistent estimators for τ PATE as
(7)

​σ​2​ ​(X )
​σ​12​ ​(Xi) _______
 ​+ ​  0 i  ​
핍PATE = E c​ _____
e(Xi)
1 − e(Xi)
+ (τ(Xi) − τ​)​ 2​d ,

where p = E[e(Xi)] is the unconditional treatment probability. Interestingly, this lower
bound holds irrespective of whether the
­propensity score is known or not. The form
of this variance bound is informative. It is no
surprise that τ PATE is more difficult to estimate the larger are the variances ​σ0​2​ ​(x) and​
σ​12​ ​(x). However, as shown by the presence of
the third term, it is also more difficult to estimate τ PATE, the more variation there is in the
average treatment effect conditional on the
covariates. If we focus instead on estimating τ CATE, the conditional average treatment

27

effect, the third term drops out, and the variance bound for τ CATE is
(8)

[

]

​σ​2​ ​(X )
​σ​2​ ​(X )
​  1 i ​+ _______
​  0 i  ​ ​.
핍CATE = E​ _____
e(Xi)
1 − e(Xi)

Still, the role of heterogeneity in the treatment effect is potentially important. Suppose
we actually had prior knowledge that the
average treatment effect conditional on the
covariates is constant, or τ(x) = τ PATE for all
x. Given this assumption, the model is closely
related to the partial linear model (Peter M.
Robinson 1988; James H. Stock 1989). Given
this prior knowledge, the variance bound is
(9) Vconst
−1

−1
​σ​12​ ​(Xi) _______
​σ​2​ ​(X )
   = aE c a​ _____
 ​ + ​  0 i  ​​b​  ​db
​ ​  .​
e(Xi)
1 − e(Xi)

This variance bound can be much lower
than (8) if there is variation in the propensity score. Knowledge of lack of variation in
the treatment effect can be very valuable, or,
conversely, allowing for general heterogeneity in the treatment effect can be expensive
in terms of precision.
In addition to the conditional variances of
the counterfactual outcomes, a third important determinant of the efficiency bound is
the propensity score. Because it enters into
(7) in the denominator, the presence of units
with the propensity score close to zero or one
will make it difficult to obtain precise estimates of the average effect of the treatment.
One approach to address this problem, developed by Crump et al. (2009) and discussed in
more detail in section 5.10, is to drop observations with the propensity score close to
zero and one, and focus on the average effect
of the treatment in the subpopulation with
propensity scores away from zero. Suppose
we focus on τ CATE,픸, the average of τ(Xi) for
Xi ∈ 픸. Then the variance bound is

Journal of Economic Literature, Vol. XLVII (March 2009)

28
(10) 	

1    ​
핍픸 = _________
​ 
pr(Xi ∈ 픸)

|

​σ​2​ ​(X )
​σ​12​ ​(Xi) ________
 ​ + ​  0 i  ​ (Xi) ∈ 픸 d ,
× E c​ _____
e(Xi)
1 − e (Xi)

By excluding from the set 픸 subsets of the
covariate space where the propensity score is
close to zero or one, we may be able to estimate τ CATE,픸 more precisely than τ CATE. (If
we are instead interested in τ CATT, we only
need to worry about covariate values where
e(x) is close to one.)
Having displayed these lower bounds on
variances for the average treatment effects,
a natural question is: Are there estimators
that achieve these lower bounds that do not
require parametric models or functional
form restrictions on either the conditional
means or the propensity score? The answer
in general is yes, and we now consider different classes of estimators in turn.
5.3 Regression Methods
To describe the general approach to
regression methods for estimating average
treatment effects, define μ 0(x) and μ1(x) to be
the two regression functions for the potential
outcomes:

and

μ 0(x) = E[Yi(0) | Xi = x]
μ1(x) = E[Yi(1) | Xi = x].

By definition, the average treatment effect
conditional on X = x is τ(x) = μ1(x) − μ 0(x).
As we discussed in the identification subsection, under the unconfoundedness assumption, μ 0(x) = E[Yi | Wi = 0, Xi = x] and μ1(x)
= E[Yi | Wi = 1, Xi = x], which means we can
estimate μ 0( · ) using regression methods for
the untreated subsample and μ1( · ) using
the treated subsample. Given consistent
μ​
​ ˆ 1( · ), a consistent esti­estimators ​ μ​
ˆ 0( · ) and    
mator for either τ PATE or τ CATE is

N

ˆ (X ) − ​    
ˆ 0(Xi)B.
ˆ reg = __
(11) 	​   τ​
​ 1  ​​∑​​​ A​    
μ​
μ​
N i=1 1 i

Given parametric models for μ 0( · ) and
μ1( · ), estimation and inference are straightforward.8 In the simplest case, we assume
each conditional mean can be expressed as
functions linear in parameters, say
(12) 	

μ 0(x) = α 0 + β′0(x − ψX),
μ1(x) = α1 + β′1 (x − ψX),

where we take deviations from the overall
population covariate mean ψX so that the
treatment effect is the difference in intercepts. (Naturally, as in any regression context,
we can replace x with general functions of x.)
Of course, we rarely know the ­population
mean of the covariates, so in estimation we
replace__ψX with the ­sample average across all
ˆ reg is simply
units, X ​
​ . Then ​   τ​
ˆ reg =    
α​
​ ˆ 1 −    
α​
​ ˆ 0 .
(13) 	​   τ​

This estimator is also obtained from the
coefficient on the treatment indicator Wi__in
the regression Yi on 1, Wi, Xi, Wi ·(Xi − ​X ​).
Standard errors can be obtained from standard least square regression output. (As
we show below, in the case of estimating
τ PATE, the usual standard error, whether
or not it is made robust to heteroskedastic__
ity, ignores the estimation error in ​X ​ as an
estimator of ψX; technically, the conventional
8 There is a somewhat subtle issue in estimating treatment effects from stratified samples or samples with
missing values of the covariates. If the missingness or
stratification are determined by outcomes on the covariates, Xi, and the conditional means are correctly specified,
then the missing data or stratification can be ignored for
the purposes of estimating the regression parameters; see,
for example, Wooldridge (1999, 2007). However, sample
selection or stratification based on Xi cannot be ignored in
estimating, say, τ PATE, because τ PATE equals the expected
difference in regression functions across the population
distribution of Xi. Therefore, consistent estimation of τ PATE
requires applying inverse probability weights or sampling
weights to the average in (11).

Imbens and Wooldridge: Econometrics of Program Evaluation
standard error is only valid for τ CATE and not
for τ PATE.)
  ˆ
A different representation of τ​
​  reg is useful
in order to illustrate some of the concerns
with regression estimators in this setting.
Suppose we do use the linear model in (12).
It can be shown that
__
__
   ˆ
N0
ˆ reg = Y ​
​ 1 − ​Y ​0 − a​ _______
 ​· ​ β​
(14) 	​   τ​
1
N0 + N1

__
   ˆ ′ __
N1
+ _______
​ 
 ​· ​ β​
​ 0).
0​b​ ​(​X ​1 − X ​
N0 + N1

To adjust for differences in covariates
between treated and control units,__the simple
__
​ 0, is
difference in average outcomes, ​Y ​1 − Y ​
adjusted
by
the
difference
in
average
covari__
__
ates, ​X ​1 − ​X ​0, multiplied by the weighted
­average
of    the
regression
coeffi   ˆ
ˆ in the two treatment regimes.
and
​ 
β​
cients ​ β​
0
1
This is a useful representation. It shows that
if the averages of the covariates in the two
treatment arms are very different, then the
adjustment to the simple mean difference can
be large. We can see that even more clearly
by inspecting the predicted outcome for the
treated units had they been subject to the control treatments:
   

__

   

__

__

ˆ [Y (1) | W = 0] = ​Y ​ + β​
​ ˆ 0′ (​X ​1 − ​X ​0).
	​ E​
i
i
0
   

The regression parameter β​
​ ˆ 0 is estimated
on the control sample, where__the average
of the covariates is equal to ​X ​0. It therefore likely provides a good approximation to
the conditional mean function around that
value. However, this estimated regression
function is then used to predict outcomes
in the treated sample, where
__ the average of
the covariates is equal to ​X ​1. If these covariate averages are very different, and thus
the regression model is used to predict outcomes far away from where the parameters
were estimated, the results can be sensitive to
minor changes in the specification. Unless the

29

linear approximation to the regression function is globally accurate, regression may lead
to severe biases. Another way of ­interpreting
this ­problem is as a multicollinearity problem. If the averages of the covariates in
the two treatment arms are very different,
the correlation between the covariates and
the treatment indicator is relatively high.
Although conventional least squares standard
errors take the degree of multicollinearity
into account, they do so conditional on the
specification of the regression function. Here
the concern is that any misspecification may
be exacerbated by the collinearity problem.
As noted in the introduction to section 5,
an easy way to establish the severity of this
problem
the normalized differ_______
__ is to__inspect
ences (​X ​1 − ​X ​0)/​ √​S0​2​ ​+ ​S1​2​ ​) ​.
In the case of the standard regression estimator it is straightforward to derive and to
estimate the variance when we view the estimator as an estimator of τ CATE. Assuming the
linear regression model is correctly specified,
we have
__

d
​ ˆ reg − τ CATE) →
(0, V0 + V1),
(15)	​ √N ​(  τ​

ˆ w − αw)2],
α​
where Vw = N · E [(​    

which can be obtained directly from standard
regression output. Estimating the variance
when we view the estimator as an estimator
of τ PATE requires adding a term capturing the
variation in the treatment effect conditional
on the covariates. The form is then
__

d
​ ˆ reg − τ CATE) →
(0, V0 + V1 + Vτ),
	​ √N ​(  τ​

where the third term in the normalized variance is
Vτ = (β1 − β 0)′
E[(Xi − E[Xi])(Xi − E[Xi])′](β1 − β 0),
which can be estimated as

Journal of Economic Literature, Vol. XLVII (March 2009)

30
   ˆ
   ˆ
   
​ ˆ τ = (​ β​
V​
​  0)′
1 − β​

__
__    
ˆ −    
​ )(Xi − X ​
​ )′(​ β​
​ ˆ 0).
β​
× __
​ 1  ​​∑​​​ (Xi − X ​
1
N i=1
N

In practice, this additional term is rarely
incorporated, and researcher instead report
the variance corresponding to τ CATE. In cases
where the slope coefficients do not differ
substantially across the two regimes—equivalently, the coefficients
on the interaction
__
terms Wi · (Xi − ​X ​) are “small”—this last
term is likely to be swamped by the variances
in (15).
In many cases, researchers have sought to
go beyond simple parametric models for the
regression functions. Two general directions
have been explored. The first relies on local
smoothing, and the second on increasingly
flexible global approximations. We discuss
both in turn.
Heckman, Ichimura, and Todd (1997) and
Heckman et al. (1998) consider local smoothing methods to estimate the two regression
functions. The first method they consider is
kernel regression. Given a kernel K( · ), and a
bandwidth h, the kernel estimator for μ w(x)
is
ˆ w(x) = ∑
μ​
​ ​ ​Yi · λi, with weight
 ​    
i∶Wi=w

/

Xi − x
Xi − x
 ​b ​∑ ​ ​K a​ _____
 ​b .
λi = K a​ _____
h
h
i∶Wi=w
Although the rate of convergence of the
kernel estimator to the regression function
is slower than the conventional parametric
rate N −1/2, the rate of convergence of the
implied estimator for the average treatment
effect,   τ​
​ ˆ reg in (11), is the regular parametric
rate under regularity conditions. These conditions include smoothness of the regression
functions and require the use of higher order
kernels (with the order of the kernel depending on the dimension of the covariates). In

practice, researchers have not used higher
order kernels and, with positive kernels, the
bias for kernel estimators is a more severe
problem than for the matching estimators
discussed in section 5.5.
Kernel regression of this type can be interpreted as locally fitting a constant regression
function. A general alternative is to fit locally
a polynomial regression function. The leading
case of this is local linear regression (J. Fan
and I. Gijbels 1996), applied to estimation
of average treatment effects by Heckman,
Ichimura, and Todd (1997) and
Heckman et
   ˆ
ˆ (x) and β​
α​
​  (x) as the local
al. (1998). Define ​    
least squares estimates, based on locally fitting a linear regression function:
N

ˆ (x)B = arg m
ˆ (x), ​ β​
α​
​  
in​∑
​ ​​ λi
A​    
   

α, β i=1

× AYi − α −β′(Xi − x)B 2,

with the same weights λi as in the standard
kernel estimator. The regression function at
ˆ (x) =    
μ​
α​
​ ˆ (x). In order
x is then estimated as ​    
to achieve convergence at the best possible rate for   τ​
​ ˆ reg, one needs to use higher
order kernels, although the order required
is less than that for the standard kernel
estimator.
For both the standard kernel estimator
and the local linear estimator an important
choice is that of the bandwidth h. In practice, researchers have used ad hoc methods
for bandwidth selection. Formal results on
bandwidth selection from the literature on
nonparametric regression are not directly
applicable. Those results are based on minimizing a global criterion such as the expected
value of the squared difference between the
estimated and true regression function, with
the expectation taken with respect to the
marginal distribution of the covariates. Thus,
they focus on estimating the regression function well everywhere. Here the focus is on
a particular scalar functional of the regression function, and it is not clear whether the

Imbens and Wooldridge: Econometrics of Program Evaluation
conventional methods for bandwidth choices
have good properties.
Although formal results are given for the
case with continuous regressors, modifications have been developed that allows for both
continuous and discrete covariates (Jeffrey S.
Racine and Qi Li 2004). All such methods
require choosing the degree of smoothing
(often known as bandwidths), and there has
not been much work on choosing bandwidths
for the particular problem of estimating average treatment effects where the parameter of
interest is effectively the average of a regression function, and not the entire function. See
Imbens (2004) for more discussion. Although
the estimators based on local smoothing have
not been shown to attain the variance efficiency bound, it is likely that they can be constructed to do so under sufficient smoothness
conditions.
An alternative to local smoothing methods are global smoothing methods, such as
series or sieve estimators. Such estimators
are parametric for a given sample size, with
the number of parameters and the flexibility of the model increasing with the sample
size. One attraction of such methods is that
often estimation and inference can proceed
as if the model is completely parametric.
The amount of smoothing is determined by
the number of terms in the series, and the
large-sample analysis is carried out with the
number of terms growing as a function of the
sample size. Again, little is known about how
to choose the number of terms when interest lies in average treatment effects. For the
­average treatment case, Hahn (1998), Imbens,
Newey, and Ridder (2005), Andrea Rotnitzky
and Robins (1995), and Chen, Hong, and
Tarozzi (2008) have developed estimators of
this type. Hahn shows that estimators in this
class can achieve the variance lower bounds
for estimating τ PATE. For a simple version of
such an estimator, suppose that Xi is a scalar. Then we can approximate μ w(x) by a K-th
order polynomial

31

K

μ w,K(x) = ∑
​ ​β w,k · xk.
k=0

We then estimate β w,k by least squares
regression, and estimate the average treatment effect using (11). This is a special case
of the estimator discussed in Imbens, Newey,
and Ridder (2005) and Chen, Hong, and
Tarozzi (2008), with formal results presented
for the case with general Xi. Imbens, Newey,
and Ridder (2005) also discuss methods for
choosing the number of terms in the series
based on expected squared error for the
average treatment effect.
If the outcome is binary or more generally
of a limited dependent variable form, a linear
series approximation to the regression function is not necessarily attractive. It is likely
that one can use increasingly flexible approximations based on models that exploit the
structure of the outcome data. For the case
with binary outcomes, Hirano, Imbens, and
Ridder (2003) show how using a polynomial
approximation to the log odds ratio leads to
an attractive estimator for the conditional
mean. See Chen (2007) for general discussion of such models. One can imagine that,
in cases with nonnegative response variables,
exponential regression functions, or those
derived from specific models, such as Tobit
(when the response can pile up at zero), combined with polynomial approximations in the
linear index function, might be useful.
Generally, methods based on global
approximations suffer from the same drawbacks as linear regression. If the covariate
distributions are substantially different in
both treatment groups, estimates based on
such methods rely, perhaps more than is
desired, on extrapolation. Using these methods in cases with substantial differences in
covariate distributions is therefore not recommended (except possibly in cases where
the sample has been trimmed so that the
covariates across the two treatment regimes
have considerable overlap).

Journal of Economic Literature, Vol. XLVII (March 2009)

32

Before we turn to propensity score methods,
we should comment on estimating the average
treatment effects on the treated, τ PATT and
  ˆ
(Xi) gets averaged across
τ CATT. In this case, ​ τ​ 
observations with Wi = 1, rather than across
μ​
the entire sample as in (11) Because    
​ ˆ 1(x) is
estimated on the treated subsample, in estimating PATT or CATT there is no problem
if μ1(x) is poorly estimated at covariate values
that are common in the control group but
scarce in the treatment group. But we must
have a good estimate of μ 0(x) at covariate values common in the treatment group, and this
is not ensured because we can only use the
μ​
control group to obtain    
​ ˆ 0(x). Nevertheless, in
many settings μ 0(x) can be estimated well over
the entire range of the covariates because the
control group often includes units that are similar to those in the treatment group. By contrast, often there are numerous control group
units—for example, high-income workers in
the context of a job training program—that
are quite different from any units in the treatment group, making the ATE parameters considerably more difficult to estimate than ATT
parameters. (Further, the ATT parameters are
more interesting from a policy perspective in
such cases, unless one redefines the population to exclude some units that are unlikely to
ever be in the treatment group.)
5.4 Methods Based on the Propensity Score
The first set of alternatives to regression estimators relies on estimates of the
­propensity score. These methods were introduced in Rosenbaum and Rubin (1983b).
An early economic discussion is in Card and
Sullivan (1988). Rosenbaum and Rubin show
that, under unconfoundedness, independence
of potential outcomes and treatment indicators also holds after conditioning solely on the
propensity score, e(x) = pr(Wi = 1 | Xi = x):
Wi   ǁ   AYi(0), Yi(1)B | Xi
⇒

Wi   ǁ   AYi(0), Yi(1)B | e(Xi).

The basic insight is that for any binary variable Wi, and any random vector Xi, it is true
(without assuming unconfoundedness) that
Wi   ǁ   Xi | e(Xi).
Hence, within subpopulations with the same
value for the propensity score, covariates are
independent of the treatment indicator and
thus cannot lead to biases (the same way in
a regression framework omitted variables that
are uncorrelated with included covariates do
not introduce bias). Since under unconfoundedness all biases can be removed by adjusting
for differences in covariates, this means that
within subpopulations homogenous in the
propensity score there are no biases in comparisons between treated and control units.
Given the Rosenbaum–Rubin result, it is
sufficient, under the maintained assumption of
unconfoundedness, to adjust solely for differences in the propensity score between treated
and control units. This result can be exploited
in a number of ways. Here we discuss three
of these that have been used in practice. The
first two of these methods exploit the fact
that the propensity score can be viewed as a
covariate that is sufficient to remove biases in
estimation of average treatment effects. For
this purpose, any one-to-one function of the
propensity score could also be used. The third
method further uses the fact that the propensity score is the conditional probability of
receiving the treatment.
The first method simply uses the propensity score in place of the covariates in
regression analysis. Define νw(e) = E[Yi | Wi
= w,e(Xi) = e]. Unconfoundedness in combination with the Rosenbaum–Rubin result
implies that νw(e) = E[Yi(w) | e(Xi) = e]. Then
we can estimate νw(e) very generally using
kernel or series estimation on the propensity
score, something which is greatly simplified by the fact that the propensity score is a
­scalar. Heckman, Ichimura, and Todd (1998)
consider local smoothers and Hahn (1998)

Imbens and Wooldridge: Econometrics of Program Evaluation
considers a series estimator. In either case
we have the consistent estimator
N

1  ​· ​∑​​ A​   ν​
ˆ (e(Xi)) −   
ˆ regprop = ​ __
ν​
​ ˆ 0(e(Xi))B ,
	​   τ​
N i=1 1

which is simply the average of the differences in predicted values for the treated and
untreated outcomes. Interestingly, Hahn
shows that, unlike when we use regression to
adjust for the full set of covariates, the series
regression estimator based on adjusting for
the known propensity score does not achieve
the efficiency bound.
Although methods of this type have been
used in practice, probably because of their
simplicity, regression on simple functions of
the propensity score is not recommended.
Because the propensity score does not have a
substantive meaning, it is difficult to motivate
a low order polynomial as a good approximation to the conditional expectation. For example, a linear model in the propensity score
is unlikely to provide a good approximation
to the conditional expectation: individuals
with propensity scores of 0.45 and 0.50 are
likely to be much more similar than individuals with propensity scores equal to 0.01 and
0.06. Moreover, no formal asymptotic properties have been derived for the case with
the propensity score unknown.
The second method, variously referred to
as blocking, subclassification, or ­stratification,
also adjusts for differences in the propensity
score in a way that can be interpreted as
regression, but in a more flexible manner.
Originally suggested by Rosenbaum and
Rubin (1983b), the idea is to partition the
sample into strata by (discretized) values of
the propensity score, and then analyze the
data within each stratum as if the propensity
score were constant and the data could be
interpreted as coming from a completely randomized experiment. This can be interpreted
as approximating the conditional mean of the
potential outcomes by a step function. To be
more precise, let 0 = c0 < c1 < c2 < … < cJ

33

= 1 be boundary values. Then define Bij,
for i = 1, … , N, and j = 1, … , J − 1, as the
indicators
  

Bij = e

if cj−1 ≤ e(Xi) < cj
otherwise

1
0

J−1

and BiJ = 1 − ​∑​Bij.
j=1

Now estimate within stratum j the average
treatment effect τj = E[Yi(1) − Yi(0) | Bij = 1]
as
__

__

ˆ j = Y ​
​ j1 − Y ​
​ j0
 ​   τ​

where

and

__

N

​Y ​jw = ___
​  1  ​ ∑
​ ​​B × Yi,
Njw i∶Wi=w ij
N

Njw = ∑
​ ​ ​Bij .
i∶Wi=w

If J is sufficiently large and the differences
cj − cj−1 small, there is little variation in the
propensity score within a stratum or block,
and one can analyze the data as if the propensity score is constant, and thus as if the data
within a block were generated by a completely
randomized experiment (with the assignment
probabilities constant within a stratum, but
varying between strata). The average treatment effect is then estimated as the weighted
average of the within-stratum estimates:
J
Nj0 + Nj1
  ˆ
_______
 ​b .
=
∑
​
​​   τ​
​ ˆ j · a​ 
	​ τ​
block
N
j=1

With J large, the implicit step function
approximation to the regression functions
νw(e) will be accurate. Cochran (1968) shows
in a Gaussian example that with five equalsized blocks the remaining bias is less than
5 percent of the bias in the simple difference between average outcomes among
treated and controls. Motivated by Cochran’s

34

Journal of Economic Literature, Vol. XLVII (March 2009)

c­ alculations, researchers have often used five
strata, although depending on the sample
size and the joint distribution of the data,
fewer or more blocks will generally lead to a
lower expected mean squared error.
The variance for this estimator is typically calculated conditional on the strata
indicators, and assuming random assignment
within the strata. That is, for stratum j, the
ˆ j, and its variance is estimated
estimator
is ​   τ​
   
   
   
ˆ
ˆ
ˆ , where
​  j0 + ​ V​
as ​ V​j = V​
j1
​Sjw
​2 ​​ 
   
ˆ = ​ ___
 ​, where
	​ V​
jw
Njw

__
2
​​ = ___
​  1  ​ ​∑ ​ ​(Yi − Y ​
​ jw)2
	​S​jw
Njw i∶Bij=1,Wi=w

The overall variance is then estimated as
J
Nj0 + Nj1 2
   
   
ˆ (  
ˆ + ​    
ˆ )· a​ 
_______
 ​​b​  ​.
τ​
​ ˆ block) = ∑
​ ​​ (​ V​
V​
	​ 핍​
0j
1j
N
j=1

This variance estimator is appropriate for
τ CATE, although it ignores biases arising from
variation in the propensity score within strata.
The third method exploiting the propensity
score is based on weighting. Recall that τ PATE
= E[Yi(1) − Yi(0)] = E[Yi(1)] − E[Yi(0)]. We
consider the two terms separately. Because
Wi · Yi = Wi · Yi(1), we have
Wi · Yi(1)
Wi · Yi
_____
_______
 ​d = E c​ 
 ​d
E c​ 
e(Xi)
e(Xi)

|

Wi · Yi(1)
_______
= E cE c​ 
 ​  Xi d d
e(Xi)

E(Wi | Xi) · E(Yi(1) | X)
= E ________________
c​ 
 ​
  
  d
e(Xi)

e(X ) · E(Yi(1) | Xi)
= E _____________
c​  i
 ​
   d
e(Xi)

= E [E(Yi(1) | Xi] = E[Yi(1)],

where the second and final inequalities follow by iterated expectations and the third
equality holds by unconfoundedness. The
implication is that weighting the treated
population by the inverse of the propensity
score recovers the expectation of the unconditional response under treatment. A similar
calculation shows E[((1 − Wi)Yi)/(1 − e(Xi))]
= E[Yi(0)], and together these imply
Wi · Yi _________
(1 − Wi) · Yi
_____
(16) 	τ PATE = E c​ 
 ​− ​ 
 ​d .
e(Xi)
1 − e(Xi)

Equation (16) suggests an obvious estimator
of τ PATE:
ˆ weight = __
​ 1  ​
(17) ​   τ​
N

N
Wi · Yi _________
(1 − Wi) · Yi
_____
×∑
​ ​​​ c​ 
 ​− ​ 
 ​d ,
e(X
)
1 − e(Xi)
i
i=1

which, as a sample average from a random
__
sample, is consistent for τ PATE and ​ √N ​
asymptotically normally distributed. The
estimator in (17) is essentially due to D. G.
Horvitz and D. J. Thompson (1952).9
In practice, (17) is not a feasible estimator because it depends on the propensity
score function e( · ), which is rarely known. A
­surprising result is that, even if we know the
ˆ weight does not achieve the
propensity score, ​   τ​
efficiency bound given in (7). It turns out to
be better, in terms of large sample efficiency,
to weight using the estimated rather than the
true propensity score. Hirano, Imbens, and
Ridder (2003) establish conditions under
which replacing e( · ) with a logistic sieve estimator results in a weighted propensity score
estimator that achieves the variance bound.
The estimator is practically simple to compute, as estimation of the propensity score
involves a straightforward logit estimation
9 Because the Horvitz–Thompson estimator is based on
sample averages, adjustments for stratified sampling are
straightforward if one is provided sampling weights.

Imbens and Wooldridge: Econometrics of Program Evaluation
involving flexible functions of the covariates.
Theoretically, the number of terms in the
approximation should increase with the sample size. In the second step, given the estimated propensity score   ​e ​ˆ(x), one estimates

/

N
W · Yi N ____
W
ˆ ipw = ∑
(18) ​   τ​
​ ​​ _____
​     i  ​ 
 ​∑​​  ​    i  ​−
ˆ
ˆ
​  (Xi) i=1 e​
​  (Xi)
i=1 e​

/

N
Wi
(1 − Wi) · Yi N _______
 ​.
 ​ ​∑​​ ​ 
​ 
​∑​​ _________
  ˆ
​ ˆ (Xi)
​  (Xi) i=1 1 −   e​
i=1 1 − e​

We refer to this as the inverse probability weighting (IPW) estimator. See Hirano,
Imbens, and Ridder (2003) for intuition as
to why estimating the propensity score leads
to a more efficient estimator, asymptotically,
than knowing the propensity score.
Ichimura and Oliver Linton (2005) studˆ ipw when   
e​
​ ˆ ( · ) is obtained via kernel
ied ​   τ​
regression, and they consider the problem of
optimal bandwidth choice when the object of
interest is τ PATE. More recently, Li, Racine,
and Wooldridge (forthcoming) consider
­kernel estimation for discrete as well as continuous covariates. The estimator proposed
by Li, Racine, and Wooldridge achieves the
variance lower bound. See Hirano, Imbens,
and Ridder (2003) and Wooldridge (2007)
for methods for estimating the variance for
these estimators.
Note that the blocking estimator can also
be interpreted as a weighting ­estimator.
Consider observations in block j. Within the
block, the Nj1 treated observations all get
equal weight 1/Nj1. In the estimator for the
overall average treatment effect, this block
​ ˆ
gets weight (Nj0 + Nj1)/N, so we can write   τ​
N
= ​∑ i=1​ ​ λi · Yi, where for treated observations
in block j the weight normalized by N is N · λi
= (Nj0 + Nj1)/Nj1), and for control observations it is N · λi = (Nj0 + Nj1)/Nj0). Implicitly
this estimator is based on an estimate of
the propensity score in block j equal to
Nj1/(Nj0 + Nj1). Compared to the IPW estimator, the propensity score is smoothed within

35

the block. This has the advantage of avoiding
particularly large weights, but comes at the
expense of introducing bias if the propensity
score is correctly specified.
A particular concern with IPW estimators
arises again when the covariate distributions
are substantially different for the two ­treatment
groups. That implies that the propensity score
gets close to zero or one for some values of the
covariates. Small or large values of the propensity score raises a number of issues. One
concern is that alternative parametric models
for the binary data, such as probit and logit
models that can provide similar approximations in terms of estimated probabilities over
the middle ranges of their arguments, tend to
be more different when the probabilities are
close to zero or one. Thus the choice of model
and specification becomes more important,
and it is often difficult to make well motivated
choices in treatment effect settings. A second
concern is that for units with propensity scores
close to zero or one, the weights can be large,
making those units particularly influential in
the ­estimates of the average treatment effects,
and thus making the estimator imprecise.
These concerns are less serious than those
regarding regression estimators because at
least the IPW estimates will accurately reflect
uncertainty. Still, these concerns make the
simple IPW estimators less attractive. (As
for regression cases, the problem can be less
severe for the ATT parameters because propensity score values close to zero play no role.
Problems for estimating ATT arise when some
units, as described by their observed covariates, are almost certain to receive treatment.)
5.5 Matching
Matching estimators impute the missing
potential outcomes using only the outcomes
of a few nearest neighbors of the opposite
treatment group. In that sense, matching is
similar to nonparametric kernel regression,
with the number of neighbors playing the role
of the bandwidth in the kernel ­regression. A

36

Journal of Economic Literature, Vol. XLVII (March 2009)

formal difference with kernel methods is that
the asymptotic distribution for matching estimators is derived conditional on the implicit
bandwidth, that is, the number of neighbors,
often fixed at a small number, e.g., one. Using
μ​
such asymptotics, the implicit estimate    
​ ˆ
(x)
is
(close
to)
unbiased,
but
not
consistent,
w
for μ w(x). In contrast, the kernel regression
estimators discussed in the previous section
ˆ w(x).
μ​
implied consistency of ​    
Matching estimators have the attractive
feature that the smoothing parameters are
easily interpretable. Given the matching
metric, the researcher only has to choose
the number of matches. Using only a single
match leads to the most credible inference
with the least bias, at the cost of sacrificing
some precision. This sits well with the focus
in the literature on reducing bias rather than
variance. It also can make the matching estimator easier to use than those estimators that
require more complex choices of smoothing
parameters, and this may be another explanation for its popularity.
Matching estimators have been widely
studied in practice and theory (e.g., X. Gu and
Rosenbaum 1993; Rosenbaum 1989, 1995,
2002; Rubin 1973b, 1979; Rubin and Neal
Thomas 1992a, 1992b, 1996, 2000; Heckman,
Ichimura, and Todd 1998; Dehejia and Sadek
Wahba 1999; Abadie and Imbens 2006;
Alexis Diamond and Jasjeet S. Sekhon 2008;
Sekhon forthcoming; Sekhon and Richard
Grieve 2008; Rosenbaum and Rubin 1985;
Stefano M. Iacus, Gary King, and Giuseppe
Porro 2008). Most often they have been
applied in settings where, (1) the interest is in
the average treatment effect for the treated,
and (2) there is a large reservoir of potential
controls, although recent work (Abadie and
Imbens 2006) shows that matching estimators can be modified to estimate the overall
average effect. The setting with many potential controls allows the researcher to match
each treated unit to one or more distinct
controls, hence the label “matching without

replacement.” Given the matched pairs, the
treatment effect within a pair is estimated
as the difference in outcomes, and the overall average as the average of the within-pair
difference. Exploiting the representation of
the estimator as a difference in two sample
means, inference is based on standard methods for differences in means or methods for
paired randomized experiments, ignoring
any remaining bias. Fully efficient matching
algorithms that take into account the effect
of a particular choice of match for treated
unit i on the pool of potential matches for
unit j are computationally cumbersome. In
practice, researchers use greedy algorithms
that sequentially match units. Most commonly the units are ordered by the value of
the propensity score with the highest propensity score units matched first. See Gu and
Rosenbaum (1993) and Rosenbaum (1995)
for discussions.
Abadie and Imbens (2006) study formal
asymptotic properties of matching estimators
in a different setting, where both treated and
control units are (potentially) matched and
matching is done with replacement. Code for
the Abadie–Imbens estimator is available in
Matlab and Stata (see Abadie et al. 2004).10
N ​,
Formally, given a sample, {(Yi, Xi, Wi​)} ​i=1
let ℓ1(i) be the nearest neighbor to i, that is,
ℓ1(i) is equal to the nonnegative integer j, for
j ∈ {1, … , N}, if Wj ≠ Wi, and
min ​ ǁXk − Xi ǁ.
ǁ Xj − Xi ǁ = ​    
k:Wk≠Wi

More generally, let ℓm(i) be the index that satisfies Wℓm(i) ≠ Wi and that is the m-th closest
to unit i:
​∑ ​ ​1 Eǁ Xl − Xi ǁ ≤ ǁ Xℓm(i) − Xi ǁ F = m,

l∶Wl≠Wi

10 See Sascha O. Becker and Andrea Ichino (2002) and
Edwin Leuven and Barbara Sianesi (2003) for alternative
Stata implementations of matching estimators.

Imbens and Wooldridge: Econometrics of Program Evaluation
where 1{ · } is the indicator function, equal to
one if the expression in brackets is true and
zero otherwise. In other words, ℓm(i) is the
index of the unit in the opposite treatment
group that is the m-th closest to unit i in
terms of the distance measure based on the
norm ǁ · ǁ. Let M(i) ⊂ {1, … , N} denote the
set of indices for the first M matches for unit
i: M(i) = {ℓ1(i), … , ℓM(i)}. Now impute the
missing potential outcomes as the average
of the
outcomes
for the matches, by defin   ˆ
   ˆ
(0)
and
Y​
​ 
(1)
as
ing ​ Y​
i
i
   
Y
Yˆ i(0) = e i

​  ​

1/M ∑
​ j∈M(i)​ ​ ​Yj

   
1/M ​∑ j∈M(i)​​  Yj
Yˆ i(1) = e

​  ​

Yi

if Wi = 0,
if Wi = 1,
if Wi = 0,
if Wi = 1,

The simple matching estimator discussed in
Abadie and Imbens is then
N

   ˆ
   ˆ
ˆ match = __
​ 1  ​​∑​​ A​ Y​
(19) 	​   τ​
i(1) − ​ Y​i(0)B.
N i=1

Abadie and Imbens show that the bias of
this estimator is of order O(N−1/K), where K
is the dimension of the covariates. Hence, if
one studies the asymptotic distribution
of the
__
estimator by normalizing by ​ √N ​ (as can be
justified by the fact that the variance of the
estimator is of order O(1/N)), the bias does
not disappear if the dimension of the covariates is equal to two, and will dominate the
large sample variance if K is at least three. To
put this result in perspective, it is useful to
relate it to bias properties of estimators based
on kernel regression. Kernel estimators can
be viewed as matching estimators where
all observations within some bandwidth hN
receive some weight. As the sample size N
increases, the bandwidth hN shrinks, but
sufficiently slow in order to ensure that the
number of units receiving non-zero weights
diverges. If all the weights are positive, the
bias for kernel estimators would generally be
worse. In order to achieve root-N consistency,

37

it is therefore critical that some weights are
negative through the device of higher order
kernels, with the exact order required dependent on the dimension of the covariates (see,
e.g., Heckman, Ichimura, and Todd 1998). In
practice, however, researchers have not used
higher order kernels, and so bias concerns
for nearest-neighbor matching estimators
are even more relevant for kernel matching
methods.
There are three caveats to the Abadie–
Imbens bias result. First, it is only the continuous covariates that should be counted in
the dimension of the covariates. With discrete covariates the matching will be exact
in large samples, and as a result such covariates do not contribute to the order of the
bias. Second, if one matches only the treated,
and the number of potential controls is much
larger than the number of treated units, one
can justify ignoring the bias by appealing to
an asymptotic sequence where the number
of potential controls increases faster with
the sample size than the number of treated
units. Specifically, if the number of controls,
N0, and the number of treated, N1, satisfy
​  ​ → 0, then the bias disappears__
in
N1/​N0​4/K
large samples after normalization by ​ √N1 ​.
Third, even though the order of the bias may
be high, the actual bias may still be small
if the coefficients in the leading term are
small. This is possible if the biases for different units are at least partially offsetting. For
example, the leading term in the bias relies
on the regression function being nonlinear,
and the density of the covariates having a
nonzero slope. If either the regression function is well approximated by a linear function, or the density is approximately flat, the
bias may be fairly limited.
Abadie and Imbens (2006) also show
that matching estimators are generally not
efficient. Even in the case where the bias
is of low enough order to be dominated by
the variance, the estimators do not reach
the efficiency bound given a fixed number

38

Journal of Economic Literature, Vol. XLVII (March 2009)

of matches. To reach the bound the number of matches would need to increase with
the sample size. If M → ∞, with M/N → 0,
then the matching estimator is essentially
like a nonparametric regression estimator. However, it is not clear that using an
approximation based on a sequence with
an increasing number of matches improves
the accuracy of the approximation. Given
that in an actual data set one uses a specific number of matches, M, it would appear
appropriate to calculate the asymptotic
variance conditional on that number, rather
than approximate the distribution as if this
number is large. Calculations in Abadie and
Imbens show that the efficiency loss from
even a very small number of matches is
quite modest, and so the concerns about the
inefficiency of matching estimators may not
be very relevant in practice. Little is known
about the optimal number of matches, or
about data-dependent ways of choosing it.
All of the distance metrics used in practice standardize the covariates in some
manner. Abadie and Imbens use a diagonal
matrix with each diagonal element equal to
the inverse of the corresponding covariate
variance. The most common metric is the
Mahalanobis metric, which is based on the
inverse of the full covariance matrix. Zhao
(2004), in an interesting discussion of the
choice of metrics, suggests some alternatives that depend on the correlation between
covariates, treatment assignment, and outcomes. So far there is little experience with
any metrics beyond inverse-of-the-variances
and the Mahalanobis metrics. Zhao (2004)
reports the results of some simulations using
his proposed metrics, finding no clear winner
given his specific design.
5.6 Combining Regression and Propensity
Score Weighting
In sections 5.3 and 5.4, we describe methods for estimating average causal effects
based on two strategies: the first is based

on estimating μ w(x) = E[Yi(w) | Xi = x] for
w = 0, 1 and averaging the difference as in
(11), and the second is based on estimating
the propensity score e(x) = pr(Wi = 1 | Xi = x)
and using that to weight the outcomes as in
(18). For each approach, we have discussed
estimators that achieve the asymptotic efficiency bound. If we have large sample sizes,
relative to the dimension of Xi, we might
think our nonparametric estimators of the
conditional means or propensity score are
sufficiently accurate to invoke the asymptotic
efficiency results described above.
In other cases, however, we might choose
flexible parametric models without being
confident that they necessarily approximate
the means or propensity score well. As we
discussed earlier, one reason for viewing estimators of conditional means or propensity
scores as flexible parametric models is that
it greatly simplifies standard error calculations for treatment effect estimates. In such
cases, one might want to adopt a strategy that
combines regression and propensity score
methods in order to achieve some robustness to misspecification of the parametric
models. It may be helpful to think about the
analogy to omitted variable bias. Suppose
we are interested in the coefficient on Wi in
the (long) linear regression of Yi on a constant, Wi and Xi. Suppose we omit Xi from
the long regression, and just run the short
regression of Yi on a constant and Wi. The
bias in the estimate from the short regression
is equal to the product of the coefficient on
Xi in the long regression, and the coefficient
on Xi in a regression of Wi on a constant and
Xi. Weighting can be interpreted as removing the correlation between Wi and Xi, and
regression as removing the direct effect of Xi.
Weighting therefore removes the bias from
omitting Xi from the regression. As a result,
combining regression and weighting can lead
to additional robustness by both removing
the correlation between the omitted covariates, and by reducing the ­correlation between

Imbens and Wooldridge: Econometrics of Program Evaluation
the omitted and included variables. This is
the idea behind the doubly-robust estimators developed in Robins and Rotnitzky
(1995), Robins, Rotnitzky and Lue Ping Zhao
(1995), and Mark J. van der Laan and Robins
(2003).
Suppose we model the two regression
func__
tions as μ w(x) = αw + β′w (x − ​X ​), for w = 0, 1
(where we abuse notation a bit and insert the
sample averages of the covariates for their population means). More generally, we may use a
nonlinear model for the conditional expectation, or just a more flexible linear approximation. Suppose we model the propensity score
as e(x) = p(x; γ), for example as p(x; γ) = exp(γ 0
+ x′γ1)/(1 + exp(γ 0 + x′γ1)). In the first step,
we estimate γ by maximum likelihood and
obtain the estimated propensity scores as   e​
​ ˆ
γ​
​ ˆ ). In the second step, we use lin(Xi) = p(x;    
ear regression, where we weight the objective function by the inverse probability of
treatment or non-treatment. Specifically, to
estimate (α 0, β 0) and (α1, β1), we would solve
the weighted least squares problems
__

​ ))2
(Y − α 0 − β′0 (Xi − X ​
 ​
​​  i
  
  ,
(20) ​min
   ​ ​∑ ​ _________________
α 0,β 0 i∶Wi=0
p(Xi;    
​ ˆ ))
γ​
and

__

​ ))2
(Y − α1 − β′1(Xi − X ​
​​  i
  
 ​
  ,
	​min
   ​ ​∑ ​ _________________
   
α1,β 1 i∶Wi=1
1 − p(Xi; γ​
​ ˆ ))
Given the estimated conditional mean functions, we estimate τ PATE, using the expresˆ reg =    
α​
​ ˆ 1 −    
α​
​ ˆ 0 as in equation (13).
sion for ​   τ​
But what is the motivation for weighting by
the inverse propensity score when we did
not use such weighting in section 5.3? The
motivation is the double robustness result
due to Robins and Rotnitzky (1995); see also
Daniel O. Scharfstein, Rotnitzky, and Robins
(1999).
First, suppose that the conditional expectation is indeed linear,
or E[Yi(w) | Xi = x]
__
​ ). Then, as discussed in
= αw + β′w (x − X ​
the treatment effect context by Wooldridge

39

(2007), weighting the objective function
by any nonnegative function of Xi does not
affect consistency of least squares.11 As a
result, even if the logit model for the propensity score is misspecified, the binary response
ˆ still has a well-defined probability
γ​
MLE ​    
limit, say γ *, and the IPW estimator that uses
ˆ ) for treated observations
γ​
weights 1/p(Xi; ​    
ˆ )) for control observations
γ​
and 1/(1 − p(Xi; ​    
is asymptotically equivalent to the estimator that uses weights based on γ *.12 It does
not matter that for some x, e(x) ≠ p(x; γ *).
This is the first part of the double robustness
result: if the parametric conditional means
for E[Y(w) | X = x] are correctly specified, the
model for the propensity score can be arbitrarily misspecified for the true propensity
score. Equation (20) still leads to a consistent
estimator for τ PATE.
When the conditional means are correctly
specified, weighting will generally hurt in
terms of asymptotic efficiency. The optimal
weight is the inverse of the variance, and
in general there is no reason to expect that
weighting the inverse of (one minus) the propensity score gives a good approximation to
that. Specifically, under homoskedasticity
of Yi(w) so that ​σ​w2 ​ ​ = ​σ​w2 ​(​  x), in the context of
least squares—the IPW estimator of (αw, β w)
is less efficient than the unweighted estimator; see Wooldridge (2007). The motivation
for propensity score weighting is different: it
offers a robustness advantage for estimating
τ PATE.
The second part of the double robustness
result assumes that the logit model (or an
alternative binary response model) is correctly specified for the propensity score, so
that e(x) = p(x; γ *), but allows the conditional mean functions to be misspecified.
11 More generally, it does not affect the consistency of
any quasi-likelihood method that is robust for estimating
the parameters of the conditional mean. These are likelihoods in the linear exponential family, as described in C.
Gourieroux, A. Monfort, and A. Trognon (1984a, 1984b).
12 See Wooldridge (2007).

40

Journal of Economic Literature, Vol. XLVII (March 2009)

ˆ w → E[Yi(w)],
The result is that in that case ​    
α​
ˆ 1 −    
ˆ = ​    
α​
α​
​ ˆ 0 → E[Yi(1)] − E[Yi(0)]
and thus ​   τ​
= τ PATE and the estimator is still consistent.
Let the weight for control observations be λi
= (1 − p(Xi; γ *))−1/​∑ j∶Wj=0​ ​ ​ (1 − p(Xj; γ *))−1.
ˆ 0 is
α​
Then the least squares estimator for ​    
N

ˆ0=∑
α​
​ ​​ (1 − Wi)  λi
(21) 	​    
i=1

   

__

× AYi − β​
​ ˆ 0′ (Xi − X ​
​ ) B.

Yi ]
The weights imply that E[(1 − Wi)λi__
= E[Yi(0)] __and E[(1 − Wi)λi(Xi − ​X ​)]
ˆ0 →
α​
= E[Xi − ​X ​] = 0, and as a result ​    
E[Yi(0)]. Similarly, the average of the predicted values for Yi(1) converges to E[Yi(1)],
α​
​ ˆ 1 −    
α​
​ ˆ 0
and so the resulting estimator   τ​
​ ˆ ipw =    
is consistent for τ PATE and τ CATE irrespective
of the shape of the regression functions. This
is the second part of the double robustness
part, at least for linear regression.
For certain kinds of responses, including
binary responses, fractional responses, and
count responses, linearity of E[Yi(w) | Xi =
x] is a poor assumption. Using linear conditional expectations for limited dependent
variables effectively abdicates the first part
of the double robustness result. Instead,
we should use coherent models of the conditional means, as well as a sensible model
for the propensity score, with the hope that
the mean functions, propensity score, or
both are correctly specified. Beyond specifying logically ­coherent for E[Yi(w) | Xi = x]
so that the first part of double robustness
has a chance, for the second part we need
to choose functional forms and estimators
with the following property: even when the
mean functions are misspecified, E[Yi(w)] =
, δ* )], where δ*w is the ­probability limit
E[μ(X
  ˆ i w
of δ​
​  w. Fortunately, for the common kinds of
limited dependent ­variables used in applications, such functional forms and estimators exist; see Wooldridge (2007) for further
discussion.

Once we estimate τ based on (20), how
should we obtain a standard error? The normalized variance still has the form V0 + V1,
ˆ w − μw)2]. One option is to
α​
where Vw = E[(​    
   
ˆ 0} as a weighted
exploit the representation
__of ​ α​
   ˆ
average of Yi + ​ β​0 (Xi − ​X ​), and use the naive
variance estimator based on weighted least
squares with known weights:
__
   ˆ
   
ˆ =∑
(22) ​ V​
​ ​ ​​λi​2​ ​· (Yi + β​
​  0′ (Xi − X ​
​ ) −    
α​
​ ˆ 0)2,
0
i∶Wi=0

and similar for V1. In general, we may again
want to adjust for the estimation of the
parameters in γ. See Wooldridge (2007) for
details.
Although combining weighting and regression is more attractive then either weighting
or regression on their own, it still requires at
least one of the two specifications to be accurate globally. It has been used regularly in
the epidemiology literature, partly through
the efforts of Robins and his coauthors, but
has not been widely used in the economics
literature.
5.7 Subclassification and Regression
We can also combine subclassification
with regression. The advantage relative to
weighting and regression is that we do not
use global approximations to the regression
function. The idea is that within stratum j,
we estimate the average treatment effect by
regressing the outcome on a constant, an
indicator for the treatment, and the covariates, instead of simply taking the difference
in averages by treatment status as in section
5.4. The latter can be viewed as a regression
estimate based on a regression with only an
intercept and the treatment indicator. The
further regression adjustment simply adds
(some of) the covariates to that regression.
The key difference with using regression in
the full sample is that, within a stratum, the
propensity score varies relatively little. As a

Imbens and Wooldridge: Econometrics of Program Evaluation
result, the covariate distributions are similar, and the regression function is not used to
extrapolate far out of sample.
To be precise, we estimate on the observations with Bi j = 1, the regression function
Yi = αj + τ j · Wi + β′j Xi + εi,
ˆj
by least squares, obtaining
the estimates ​   τ​
   
ˆ
X
from
and estimated variances V​
​  j. Dropping
__
__ i
​ j1 − Y ​
​ j0, which
this regression leads to   τ​
​ ˆ j = Y ​
is the blocking estimator we discussed in section 5.4. We average the estimated stratumspecific average treatment effects, weighted
by the relative stratum size:

Nj0 + Nj1
_______
ˆ =∑
 ​b ·   τ​
​ ˆ j ,
​ ​​  a​ 
	​   τ​
J

j=1

N

with estimated variance

Nj0 + Nj1
   
   
ˆ =∑
_______
	​ V​
 ​​b​  ​  · V​
​ ​​  ​a​ 
​ ˆ .
J

j=1

2

N

j

With a modest number of strata, this already
leads to an estimator that is considerably
more flexible and robust than either subclassification alone, or regression alone. It is probably one of the more attractive estimators in
practice. Imbens and Rubin (forthcoming)
suggest data-dependent methods for choosing the number of strata.
5.8 Matching and Regression
   ˆ
   ˆ
Once we have the N pairs (​ Y​
i(0), ​ Y​i(1)), the

simple matching estimator given in (19) averages the difference. This estimator may still
be biased due to discrepancies between the
covariates of the matched observations and
their matches. One can attempt to reduce
this bias by using regression methods. This
use of regression is very different from using
regression methods on the full sample.
Here the covariate distributions are likely
to be similar in the matched sample, and so

41

regression is not used to extrapolate far out
of sample.
The idea behind
the   regression adjustment
   ˆ
(0)
and
Y​
​ ˆ i(1) by
is to replace ​ Y​
i
   
Yˆ (0) =

​  i​

Yi

e	​ __1 ​​∑ j∈
M

if Wi = 0,
​
​ 
(
​
Y
+
β′
(X
−
X
))
if
Wi = 1,
j
0 i
j
M (i)

   
1
__
	​ 
ˆ (1) = M ​ ​∑ j∈M(i)​​ (Yj + β1(Xi − Xj)) if Wi = 0,
Y
eY
​  i​
if Wi = 1,
i

where the average of the matched outcomes
is adjusted by the difference in covariates
relative to the matched observation. The
only question left is how to estimate the
regression coefficients β 0 and β 1. For various methods, see D. Quade (1982), Rubin
(1979), and Abadie and Imbens (2006).
The methods differ in whether the difference in outcomes is modeled as linear in
the difference in covariates, or the original conditional outcome distributions are
approximated by linear regression functions, and on what sample the regression
functions are estimated.
Here is one simple regression adjustment.
To be clear, it is useful to introduce some
additional notation. Given the set of matching indices M(i), define
   
Xˆ (0) =

​  i​

Xi

e	​ __1 ​​∑ j∈
M

​ ​ ​Xj
M (i)

   
1
​​∑
​ ​ ​X
	​ __
Xˆ (1) = M j∈M(i) j

eX

​  i​

i

if Wi = 0,
if Wi = 1,
if Wi = 0,
if Wi = 1,

   ˆ
   ˆ
and let ​ β​
on a regression of ​ Y​
w be based
i(w)
   ˆ
on a constant and ​ X​(w):
i

ˆw
α​
​    
a  ​   ˆ  ​b =

​  w
β​

   

   

ˆ (w)
ˆ (w)′
−1
​ Y​
​ X​
i
1
i
  
    
    
   ˆ
   ˆ
   ˆ
   ˆ
a ​∑​​ a​    
a
b​
b
b.
ˆX​
  
​
​ 
 ​
​ 
 ​
​ 
 ​
​   
(w)
​ 
X​
(w)
​ Y​
​ 
X​
(w)​ 
X​
(w)′
i
i
i(w)
i
i
    
   
i=1
N

42

Journal of Economic Literature, Vol. XLVII (March 2009)

Like the combination of subclassification and
regression, this leads to relatively robust estimators. Abadie and Imbens (2008a) find that
the method works well in simulations based
on the LaLonde data.
5.9 A General Method for Estimating
Variances
For some of the estimators discussed in
the previous sections, particular variance
estimators have been used. Assuming that
a particular parametric model is valid, one
can typically use standard methods based on
likelihood theory or generalized method of
moments theory. Often, these methods rely
on consistent estimation of components of
the variance. Here we discuss two general
methods for estimating variances that apply
to all estimators.
The first approach is to use bootstrapping
(Bradley Efron and Robert J. Tibshirani 1993;
A. C. Davison and D. V. Hinkley 1997; Joel
L. Horowitz 2001). Bootstrapping has been
widely used in the treatment effects literature, as it is straightforward to implement. It
has rarely been formally justified, although in
many cases it is likely to be valid given that
many of the estimators are asymptotically
linear. However, in some cases it is known
that bootstrapping is not valid. Abadie and
Imbens (2008a) show that, for a fixed number of matches, bootstrapping is not valid for
matching estimators. It is likely that the problems that invalidate the bootstrap disappear
if the number of matches increases with the
sample size (thus, the bootstrap might be valid
for kernel estimators). Nevertheless, because
in practice researchers often use a small number of matches, or nonnegative kernels, it is
not clear whether the bootstrap is an effective
method for obtaining standard errors and constructing confidence intervals. In cases where
bootstrapping is not valid, often subsampling
(Dimitris N. Politis, Joseph P. Romano, and
Michael Wolf 1999) remains valid, but this
has not been applied in practice.

There is an alternative, general, method
for estimating variances of treatment effect
estimators, developed by Abadie and Imbens
(2006), that does not require additional nonparametric estimation. First, recall that most
estimators are of the form
N

  ˆ
τ​
​ ​​ ​λi · Yi, with ∑
​ ​ ​λi = 1, ∑
​ ​ ​λi = −1,
​  = ∑
i=1

i∶Wi=1

i∶Wi=0

with the weights λi generally functions of
all covariates and all treatment ­indicators.
Conditional on the covariates and the
­treatment indicators (and thus relative to
τ CATE), the variance of such an estimator is
N

ˆ | X1, … , X N, W1, … ,W N) = ∑
핍(​   τ​ 
​ ​​ ​​λ​i2​ ​· ​σW
​2 i​​ (Xi).
i=1

In order to use this representation, we need
estimates of ​σW
​2 i​​  (Xi), for all i. Fortunately,
these need not be consistent estimates, as
long as the estimation errors are not too
highly correlated so that the weighted average of the estimates is consistent for the
weighted average of the variances. This is
similar in the way robust (Huber-EickerWhite) standard errors allow for general
forms of heteroskedasticity without having to consistently estimate the conditional
variance function.
Abadie and Imbens (2006) suggested using
2
​​  (Xi). The idea
a matching estimator for ​σ​W
i
behind this matching variance estimator is that
if we can find two treated units with Xi = x,
σ​
​​ ˆ 1​2​ ​(x) = (Yi − Yj)2/2.
we can estimate ​σ1​2​ ​(x) as    
In general, it is difficult to find exact matches,
but, again, this is not necessary. Instead, one
uses the closest match within the set of units
with the same treatment status. Let ν(i) be
the unit closest to i, with the same treatment
indicator (Wν(i) = Wi ), so that
min ​ǁ Xj − Xi ǁ .
ǁ Xν(i) − Xi ǁ = ​   
j∶Wj=i

2
​ ​(Xi) as
Then we can estimate ​σ​W
i

Imbens and Wooldridge: Econometrics of Program Evaluation
ˆW
	​​    
σ​
​2 i​​ (Xi) = (Yi − Yν(i))2/2.

This way we can estimate ​σW
​2 i​​ (Xi) for all units.
Note that these are not consistent estimators
of the conditional variances. As the sample
size increases, the bias of these estimators will
disappear, just as we saw that the bias of the
matching estimator for the average treatment
effect disappears under similar conditions.
We then use these estimates of the conditional variance to estimate the variance of
the estimator:
N

  ˆ
2
ˆ (​ τ​
ˆW
)= ∑
​ ​​ ​λi​2​ ​·​​    
σ​​
​​ (Xi).
	​ V​
i
   

i=1

An extension to allow for clustering has
been developed by Samuel Hanson and Adi
Sunderam (2008).
5.10 Overlap in Covariate Distributions
In practice, a major concern in applying
methods under the assumption of unconfoundedness is lack of overlap in the covariate
distributions. In fact, once one is committed to
the unconfoundedness assumption, this may
well be the main problem facing the analyst.
The overlap issue was highlighted in papers
by Dehejia and Wahba (1999) and Heckman,
Ichimura, and Todd (1998). Dehejia and
Wahba reanalyzed data on a job training program originally analyzed by LaLonde (1986).
LaLonde (1986) had attempted to replicate
results from an ­experimental evaluation of a
job training program, the National Supported
Work (NSW) program, using a comparison
group constructed from two public use data
sets, the Panel Study of Income Dynamics
(PSID) and the Current Population Survey
(CPS). The NSW program targeted individuals who were disadvantaged with very
poor labor market histories. As a result, they
were very different from the raw comparison
groups constructed by LaLonde from the
CPS and PSID. LaLonde partially addressed
this problem by limiting his raw comparison
samples based on single covariate criteria (e.g.,

43

limiting it to individuals with zero earnings in
the year prior to the program). Dehejia and
Wahba looked at this problem more systematically and found that a major concern is the
lack of overlap in the covariate distributions.
Traditionally, overlap in the covariate distributions was assessed by looking at summary statistics of the covariate distributions
by treatment status. As discussed before in the
introduction to section 5, it is particularly useful to report differences in average covariates
normalized by the square root of the sum of
the within-treatment group variances. In table
2, we report, for the LaLonde data, averages
and standard deviations of the basic covariates,
and the normalized difference. For four out of
the ten covariates the means are more than
a standard deviation apart. This immediately
suggests that the technical task of adjusting
for differences in the covariates is a challenging one. Although reporting normalized differences in covariates by treatment status is a
sensible starting point, inspecting differences
one covariate at a time is not generally sufficient. Even if all these differences are small,
there may still be areas with limited overlap.
Formally, we are concerned with regions in the
covariate space where the density of covariates
in one treatment group is zero and the density
in the other treatment group is not. This corresponds to the propensity score being equal
to zero or one. Therefore, a more direct way of
assessing the overlap in covariate distributions
is to inspect histograms of the estimated propensity score by treatment status.
Once it has been established that overlap
is a concern, several strategies can be used.
We briefly discuss two of the earlier specific
suggestions, and then describe in more detail
two general methods. In practice, researchers
have often simply dropped observations with
propensity score close to zero or one, with the
actual cutoff value chosen in an ad hoc fashion.
Dehejia and Wahba (1999) focus on the average effect for the treated. After estimating
the propensity score, they find the ­smallest

Journal of Economic Literature, Vol. XLVII (March 2009)

44

Table 2

Balance Improvements in the Lalonde Data (Dehejia–Wahba Sample)

Covariate
Age
Education
Married
Nodegree
Black
Hispanic
Earn ’74
Earn ’74 positive
Earn ’75
Earn ’75 positive

CPS Controls

NSW Treated

(15992)

185

mean
33.23
12.03
0.71
0.30
0.07
0.07
14.02
0.88
13.65
0.89

(s.d.)
(11.05)
(2.87)
(0.45)
(0.46)
(0.26)
(0.26)
(9.57)
(0.32)
(9.27)
(0.31)

mean
25.82
10.35
0.19
0.71
0.84
0.06
2.10
0.29
1.53
0.40

value of the estimated propensity score
​ ˆ (Xi).
among the treated units, e1 = mini:Wi=1   e​
They then drop all control units with an
estimated propensity score lower than this
threshold e1. The idea behind this ­suggestion
is that control units with very low values for
the propensity score may be so different
from treated units that including them in the
analysis is likely to be ­counterproductive. (In
effect, the population over which the treatment effects are calculated is redefined.) A
concern is that the results may be sensitive
to the choice of specific threshold e1. If, for
example, one used as the threshold the K-th
order statistic of the estimated propensity
score among the treated (Lechner 2002a,
2002b), the results might change considerably. In the sixth column of table 2, we report
the normalized difference (normalized using
the same denominator equal to the square
root of the sum of the within treatment group
sample variances) after removing 9,891 (out
of a total 16,177) control observations whose
estimated propensity score was smaller than
the smallest value of the estimated propensity score among the treated, e1 = 0.00051.

(s.d.)
(7.16)
(2.01)
(0.39)
(0.46)
(0.36)
(0.24)
(4.89)
(0.46)
(3.22)
(0.49)

Normalized Difference Treated-Controls
All

ê (Xi ) ≥ e1

P-score

Maha

(16177)

(6286)

(370)

(370)

−0.08
−0.02
−0.01
0.08
−0.02
−0.02
−0.07
−0.07
−0.02
−0.09

−0.16
−0.09
−0.20
0.18
0.00
0.00
0.00
0.00
−0.01
0.00

−0.56
−0.48
−0.87
0.64
1.72
−0.04
−1.11
−1.05
−1.23
−0.84

−0.25
−0.30
−0.46
0.42
1.45
−0.22
−0.40
−0.72
−0.35
−0.54

This improves the covariate balance, but
many of the normalized differences are still
substantial.
Heckman, Ichimura, and Todd (1997) and
Heckman et al. (1998) develop a ­different
method. They focus on estimation of the set
where the density of the propensity score conditional on the treatment is bounded away from
zero for both treatment regimes. Specifically,
they first estimate the density functions f (e | W
= w), for w = 0, 1, nonparametrically.
  ˆ    They
ˆ (Xi) | Wi
then evaluate the estimated density f​
​  (​ e​
= 0) for all N values   Xi, and the same for
ˆ (Xi) | Wi = 1) for all
the estimated density f​
​ ˆ (​   e​
N values Xi. Given these 2N values they
calculate the 2N · q order statistic of
these 2N estimated   densities. Denote this
ˆ
order statistic by ​ f​
q. Then, for each unit
i,
they
compare
the
estimated
density
  ˆ   
  ˆ
  ˆ   
ˆ (Xi) | Wi = 0) to ​ f​
ˆ
(​ e​
,
and
​ 
f​
(​ 
e​
(
X
)
|
W
​ f​
q
i
i = 1)
  ˆ
.
If
either
of
those
estimated
densities
to ​ f​
q
is below the order statistic, the observation
gets dropped from the analysis. Smith and
Todd (2005) implement this method with
q = 0.02, but provide no motivation for the
choice of the threshold.

Imbens and Wooldridge: Econometrics of Program Evaluation
5.10.1 Matching to Improve Overlap in
Covariate Distributions
A systematic method for dropping control
units who are different from the treated units is
to construct a matched sample. This approach
has been pushed by Rubin in a series of studies, see Rubin (2006). It is designed for settings
where the interest is in the average effect for the
treated (e.g., as in the LaLonde application). It
relies on the control sample being larger than
the treated sample, and works especially well
when the control sample is much larger.
First, the treated observations are
ordered, typically by decreasing values of
the estimated propensity score, since treated
observations with high values of the propensity score are generally more difficult to
match. Then the first treated unit (e.g., the
one with the highest value for the estimated
propensity score) is matched to the nearest
control unit. Next, the second treated unit is
matched to the nearest control unit, excluding the control unit that was used as a match
for the first treated unit. Matching without
replacement all treated units in this manner
leads to a sample of 2 × N1 units, (where N1
is the size of the original treated subsample),
half of them treated and half of them control
units. Note that the matching is not necessarily used here as the final analysis. We do
not propose to estimate the average treatment effect for the treated by averaging the
differences within the pairs. Instead, this is
intended as a preliminary analysis, with the
goal being the construction of a sample with
more overlap. Given a more balanced sample,
one can use any of the previously discussed
methods for estimating the average effect of
the treatment, including regression, propensity score methods, or matching. Using those
methods on the balanced sample is likely to
reduce bias relative to using the simple difference in averages by treatment status.
The last two columns in table 2 report
the balance in the ten covariates after

45

c­ onstructing a matched sample in this fashion.
In both cases the treated units were matched
in reverse order of the estimated propensity score. The seventh column is based on
matching on the estimated propensity score,
and the last column is based on matching
on all the covariates, using the Mahalanobis
metric (the inverse of the covariance matrix
of the covariates). Matching, either on the
estimated propensity score or on the full set
of covariates dramatically improves the balance. Whereas before some of the covariates
differed by as much as 1.7 times a standard
deviation, now the normalized differences are
all less than one tenth of a standard deviation.
The remaining differences are not negligible,
however. For example, average differences in
1974 earnings are still on the order of $700,
which, given the experimental estimate from
the LaLonde (1986) paper of about $2,000,
is substantial. As a result, simple estimators
such as the average of the within-matchedpair differences are not likely to lead to credible estimates. Nevertheless, maintaining
unconfoundedness, this matched sample is
sufficiently well balanced that one may be
able to obtain credible and robust estimates
from it in a way that the original sample
would not allow.
5.10.2 Trimming to Improve Overlap in
Covariate Distributions
Matching with replacement does not work
if the estimand of interest is the overall average treatment effect. For that case Crump et
al. (2009) suggest an easily implementable
way of selecting the subpopulation with overlap, consistent with the current practice of
dropping observations with propensity score
values close to zero or one. Their method is
generally applicable and in particular does not
require that the control sample is larger than
the treated sample. They consider estimation
of the average treatment effect for the subpopulation with Xi ∈ 픸. They suggest choosing the set 픸 from the set of all ­subsets of the

Journal of Economic Literature, Vol. XLVII (March 2009)

46

covariate space to minimize the asymptotic
variance of the efficient estimator of the average treatment effect for that set. Under some
conditions (in particular homoskedasticity),
they show that the optimal set 픸* depends
only on the value of the propensity score. This
method suggests discarding observations with
a ­propensity score less than α away from the
two extremes, zero and one:
픸* = {x ∈ 핏 | α ≤ e(x) ≤ 1 − α},
where α satisifies a condition based on the
marginal distribution of the propensity
score:
1
________
 ​
​ 
α · (1 − α)

1    ​ ____________
1    ​
= 2 · E c​ ____________
​ 
e(X) · (1 − e(X)) e(X) · (1 − e(X))
1
 ​d .
< ​ ________
α · (1 − α)

|

Based on empirical examples and numerical
calculations with beta distributions for the
propensity score, Crump et al. (2009) suggest
that the rule-of-thumb fixing α at 0.10 gives
good results.
To illustrate this method, table 3 presents
summary statistics for data from Imbens,
Rubin and Sacerdote (2001) on lottery players, including “winners” who won big prizes,
and “losers” who did not. Even though winning the lottery is obviously random, variation in the number of tickets bought, and
nonresponse, creates imbalances in the covariate distributions. In the full sample (sample
size N = 496), some of the covariates differ by as much as 0.64 standard deviations.
Following the Crump et al. calculations leads
to a bound of 0.0914. Discarding the observations with an estimated propensity score
outside the interval [0.0914, 0.9086] leads
to a sample size 388. In this subsample, the
largest normalized difference is 0.35, about

half of what it is in the full sample, with this
improvement obtained by dropping approximately 20 percent of the original sample.
A potentially controversial feature of all
these methods is that they change what is
being estimated. Instead of estimating τ PATE,
the Crump et al. (2009) approach estimates
τ CATE,픸. This results in reduced external
validity, but it is likely to improve internal
validity.
5.11 Assessing the Unconfoundedness
Assumption
The unconfoundedness assumption used
in section 5 is not testable. It states that
the conditional distribution of the outcome
under the control given receipt of the active
treatment and covariates, is identical to the
distribution of the control outcome conditional on being in the control and covariates. A similar assumption is made for the
distribution of the treatment outcome. Yet
since the data are completely uninformative
about the distribution of Yi(0) for those who
received the active treatment and of Yi(1)
for those receiving the control, the data can
never reject the unconfoundedness assumption. Nevertheless, there are often indirect ways of assessing this assumption. The
most important of these were developed in
Rosenbaum (1987) and Heckman and Hotz
(1989). Both methods rely on testing the
null hypothesis that an average causal effect
is zero, where the particular average causal
effect is known to equal zero. If the testing
procedure rejects the null hypothesis, this is
interpreted as weakening the support for the
unconfoundedness assumption. These tests
can be divided into two groups.
The first set of tests focuses on estimating
the causal effect of a treatment that is known
not to have an effect. It relies on the presence
of two or more control groups (Rosenbaum
1987). Suppose one has two potential control
groups, for example eligible nonparticipants
and ineligibles, as in Heckman, Ichimura and

Imbens and Wooldridge: Econometrics of Program Evaluation

47

Table 3

Balance Improvements in the Lottery Data

Covariate
Year Won
Tickets Bought
Age
Male
Years of Schooling
Working Then
Earnings Year −6
Earnings Year −5
Earnings Year −4
Earnings Year −3
Earnings Year −2
Earnings Year −1
Pos Earnings Year −6
Pos Earnings Year −5
Pos Earnings Year −4
Pos Earnings Year −3
Pos Earnings Year −2
Pos Earnings Year −1

Losers

Winners

(259)

(237)

mean
1996.4
2.19
53.2
0.67
14.4
0.77
15.6
16.0
16.2
16.6
17.6
18.0
0.69
0.68
0.69
0.68
0.68
0.69

(s.d.)
(1.0)
(1.77)
(12.9)
(0.47)
(2.0)
(0.42)
(14.5)
(15.0)
(15.4)
(16.3)
(16.9)
(17.2)
(0.46)
(0.47)
(0.46)
(0.47)
(0.47)
(0.46)

All

mean
1996.1
4.57
47.0
0.58
13.0
0.80
12.0
12.1
12.0
12.8
13.5
14.5
0.70
0.74
0.73
0.73
0.74
0.74

Todd (1997). One can estimate a “pseudo”
average treatment effect by analyzing the
data from these two control groups as if one
of them is the treatment group. In that case,
the treatment effect is known to be zero
and statistical evidence of a non-zero effect
implies that at least one of the control groups
is invalid. Again, not rejecting the test does
not imply the unconfoundedness assumption
is valid (as both control groups could suffer
the same bias), but nonrejection in the case
where the two control groups could potentially have different biases makes it more
plausible that the unconfoundedness assumption holds. The key for the power of this test
is to have available control groups that are
likely to have different biases, if they have
any at all. Comparing ineligibles and eligible

Normalized Difference Treated-Controls

(s.d.)

(496)

(1.3)
(3.28)
(13.8)
(0.49)
(2.2)
(0.40)
(11.8)
(12.0)
(12.1)
(12.7)
(13.0)
(13.6)
(0.46)
(0.44)
(0.44)
(0.44)
(0.44)
(0.44)

−0.19
0.64
−0.33
−0.13
−0.50
0.06
−0.19
−0.20
−0.21
−0.18
−0.19
−0.16
0.02
0.10
0.07
0.09
0.10
0.07

0.0914 ≤ ê (Xi ) ≤ 0.9086
(388)
−0.13
0.33
−0.19
−0.09
−0.35
−0.02
−0.10
−0.12
−0.15
−0.14
−0.15
−0.14
0.05
0.07
0.02
0.02
0.04
0.02

nonparticipants as in Heckman, Ichimura
and Todd (1997) is a particularly attractive
comparison. Alternatively one may use geographically distinct comparison groups, for
example from areas bordering on different
sides of the treatment group.
To be more specific, let Gi be an indicator variable denoting the membership of the
group, taking on three values, Gi ∈ {−1, 0, 1}.
For units with Gi = −1 or 0, the treatment
indicator Wi is equal to 0:
Wi = e

0
1

if
if

Gi = −1, 0,
Gi = 1.

Unconfoundedness only requires that
(23)

Yi(0), Yi(1)

  ǁ   W
i

| X i,

48

Journal of Economic Literature, Vol. XLVII (March 2009)

and this is not testable. Instead we focus on
testing an implication of the stronger conditional independence relation
(24)

Yi(0), Yi(1)

  ǁ   G

i

| X i.

This independence condition implies (23),
but in contrast to that assumption, it also
implies testable restrictions. In particular, we
focus on the implication that
(25)

Yi(0)   ǁ   Gi | Xi, Gi ∈ {−1, 0}
⇔ Yi   ǁ   Gi | Xi, Gi ∈ {−1, 0},

because Gi ∈ {−1,0} implies that Yi = Yi(0).
Because condition (24) is slightly stronger than unconfoundedness, the question is
whether there are interesting settings where
the weaker condition of unconfoundedness
holds, but not the stronger condition. To discuss this question, it is useful to consider two
alternative conditional independence conditions, both of which are implied by (24):
(26) AYi(0), Yi(1)B   ǁ   Wi | Xi, Gi ∈ {−1, 1},
and

(27) AYi(0), Yi(1)B   ǁ   Wi | Xi, Gi ∈ {0, 1}.

If (26) holds, then we can estimate the average
causal effect by invoking the unconfoundedness assumption using only the first control
group. Similarly, if (27) holds, then we can
estimate the average causal effect by invoking the unconfoundedness assumption using
only the second control group. The point is
that it is difficult to envision a situation where
unconfoundedness based on the two comparison groups holds—that is, (23) holds—
but it does not hold using only one of the two
comparison groups at the time. In practice, it
seems likely that if unconfoundedness holds
then so would the ­stronger condition (24),
and we have the testable implication (25).

Next, we turn to implementation of the
tests. We can simply test whether there is a
difference in average values of Yi between
the two control groups, after adjusting for
differences in Xi. That is, we effectively test
whether
E CE[Yi | Gi = −1, Xi] − E[Yi | Gi = 0, Xi]D = 0.

More generally we may wish to test
   E CE[Yi | Gi = −1, Xi = x]

− E[Yi | Gi = 0, Xi = x]D = 0

for all x in the support of Xi using the methods discussed in Crump et al. (2008b). We
can also include transformations of the basic
outcomes in the procedure to test for difference in other aspects of the conditional
distributions.
A second set of tests of unconfoundedness focuses on estimating the causal effect
of the treatment on a variable known to be
unaffected by it, typically because its value
is determined prior to the treatment itself.
Such a variable can be time-invariant, but
the most interesting case is in considering
the treatment effect on a lagged outcome.
If it is not zero, this implies that the treated
observations are distinct from the controls;
namely that the distribution of Yi(0) for the
treated units is not comparable to the distribution of Yi(0) for the controls. If the treatment is instead zero, it is more plausible that
the unconfoundedness assumption holds. Of
course this does not directly test the unconfoundedness assumption; in this setting,
being able to reject the null of no effect does
not directly reflect on the hypothesis of interest, unconfoundedness. Nevertheless, if the
variables used in this proxy test are closely
related to the outcome of interest, the test
arguably has more power. For these tests it
is clearly helpful to have a number of lagged
outcomes.

Imbens and Wooldridge: Econometrics of Program Evaluation
First partition the vector of covariates Xi
into two parts, a (scalar) pseudo outcome,
denoted by ​X​ip​ ​, and the remainder, denoted
by ​X​ir​ ​, so that Xi = (​X​ip​ ​, ​Xi​r′​  ​)′. Now we will
assess whether the following conditional
independence relation holds:
(28)	​X​ip​ ​  ǁ   Wi | ​Xi​r​ ​.
The two issues are, first, the interpretation
of this condition and its relationship to the
unconfoundedness assumption, and second,
the implementation of the test.
The first issue concerns the link between
the conditional independence relation in (28)
and original unconfoundedness. This link, by
necessity, is indirect, as unconfoundedness
cannot be tested directly. Here we lay out the
arguments for the connection. First consider
a related condition:
(29)

Yi(0), Yi(1)   ǁ   Wi | ​Xi​r​ ​.

If this modified unconfoundedness condition
were to hold, one could use the adjustment
methods using only the subset of covariates ​X​ir​ ​. In practice, though not necessarily,
this is a stronger condition than the original
unconfoundedness condition which requires
conditioning on the full vector Xi. One has
to be careful here, because it is theoretically
possible that conditional on a subset of the
covariates unconfoundedness holds, but at
the same time unconfoundedness does not
hold conditional on the full set of covariates. In practice this situation is rare though.
For example, it is difficult to imagine in an
evaluation of a labor market program that
unconfoundedness would hold given age and
the level of education, but not if one additionally conditions on gender. Generally
making subpopulations more homogenous in
pretreatment variables tends to improve the
plausibility of unconfoundedness.
The modified unconfoundedness condition
(29) is not testable for the same reasons the

49

original unconfoundedness assumption is not
testable. Nevertheless, if one has a proxy for
either of the potential outcomes, and in particular a proxy that is observed irrespective
of the treatment status, one can test independence for that proxy variable. We use the
pseudo outcome ​X​ip​ ​as such a proxy variable.
That is, we view ​Xi​p​ ​ as a proxy for, say, Yi(0),
and assess (29) by testing (28).
The most convincing applications of these
assessments are settings where the two links
are plausible. One of the leading examples
is where Xi contains multiple lagged measures of the outcome. For example, in the
evaluation of the effect of a labor market
program on annual earnings, one might
have ­observations on earnings for, say, six
years prior to the program. Denote these
lagged outcomes by Yi,−1, … , Yi,−6, where
Yi,−1 is the most recent and Yi,−6 is the most
distant preprogram earnings measure. One
could implement the above ideas using earnings for the most recent preprogram year
Yi,−1 as the pseudo outcome ​Xi​p​ ​, so that the
vector of remaining pretreatment variables​
X​ir​ ​ would still include the five prior years of
preprogram earnings Yi,−2, … , Yi,−6 (ignoring additional pre-treatment variables). In
that case one might reasonably argue that if
unconfoundedness holds given six years of
preprogram earnings, it is plausible that it
would also hold given only five years of preprogram earnings. Moreover, under unconfoundedness Yi(c) is independent of Wi given
Yi,−1, … , Yi,−6, which would suggest that it is
plausible that Yi,−1 is independent of Wi given
Yi,−2, … , Yi,−6. Given those arguments, one
can plausibly assess unconfoundedness by
testing whether
Yi,−1   ǁ   Wi | Yi,−2, … , Yi,−6.
The implementation of the tests is the same
as in the first set of tests for assessing unconfoundedness. We can simply test whether
estimates of the average difference between

Journal of Economic Literature, Vol. XLVII (March 2009)

50

the groups adjusted for differences in ​X​ir​ ​are
zero, or test whether the average difference
is zero for all values of the covariates (e.g.,
Crump et al. 2008).
5.12 Testing
Most of the focus in the evaluation literature has been on estimating average treatment effects. Testing has largely been limited
to the null hypothesis that the average effect
is zero. In that case testing is straightforward
since many estimators exist for the average
treatment effect that are approximately normally distributed in large samples with zero
asymptotic bias. In addition there is some
testing based on the Fisher approach using
the randomization distribution. In many
cases, however, there are other null hypotheses of interest. Crump et al. (2008) develop
tests of the null hypotheses of zero average
effects conditional on the covariates, and of
a constant average effect conditional on the
covariates. Formally, in the first case the null
hypothesis
(30)

H0 : τ(x) = 0, ∀ x,

against the alternative hypothesis
Ha : τ(x) ≠ 0, for some x.
Recall that τ(x) = E[Yi(1) − Yi(0) | Xi = x] is
the average effect for the subpopulation with
covariate value x. The second hypothesis
studied by Crump et al. (2008) is
(31)

H0 : τ(x) = τ PATE, ∀ x,

against the alternative hypothesis
Ha : τ(x) ≠ τ PATE,

for some x.

Part of their motivation is that in many cases
there is substantive interest in whether the
program is beneficial for some groups, even

if on average it does not affect outcomes.13
They show that in some data sets they reject
the null hypothesis (30) even though they
cannot reject the null hypothesis of a zero
average effect.
Taking the motivation in Crump et al.
(2008) one step further, one may also be
interested in testing the null hypothesis that
the conditional distribution of Yi(0) given Xi
= x is the same as the conditional distribution of Yi(1) given Xi = x. Under the maintained hypothesis of unconfoundedness, this
is equivalent to testing the null hypothesis
that
H0 : Yi   ǁ   Wi | Xi,
against the alternative hypothesis that Yi is
not independent of Wi given Xi. Tests of this
type can be implemented using the methods
of Linton and Pedro Gozalo (2003). There
have been no applications of these tests in
the program evaluation literature.
5.13 Selection of Covariates
A very important set of decisions in
i­mplementing all of the methods described
in this section involves the choice of covariates to be included in the regression functions or the propensity score. Except for
warnings about including covariates that are
themselves influenced by the treatment (for
example, Heckman and Salvador NavarroLozano 2004; Wooldridge 2005), the literature has not been very helpful. Consequently,
researchers have just included all covariates
linearly, without much systematic effort to
find more compelling specifications. Most
of the technical results using nonparametric
methods include rates at which the ­smoothing
13 A second motivation is that it may be impossible to
obtain precise estimates for τ PATE even in cases where one
can convincingly reject some of the hypotheses regarding
τ(x).

Imbens and Wooldridge: Econometrics of Program Evaluation
parameters should change with the sample
size. For example, using regression estimators, one would have to choose the bandwidth
if using kernel estimators, or the number of
terms in the series if using series estimators.
The program evaluation literature does not
provide much guidance as to how to choose
these smoothing parameters in practice.
More generally, the nonparametric estimation literature has little to offer in this regard.
Most of the results in this literature offer
optimal choices for smoothing parameters if
the criterion is integrated squared error. In
the current setting the interest is in a scalar parameter, and the choice of smoothing
parameter that is optimal for the regression
function itself need not be close to optimal
for the average treatment effect.
Hirano and Imbens (2001) consider an
estimator that combines weighting with the
propensity score and regression. In their application they have a large number of covariates,
and they suggest deciding which ones to include
on the basis of t-statistics. They find that the
results are fairly insensitive to the actual cutoff
point if they use the weight/regression estimator, but find more sensitivity if they only use
weighting or regression. They do not provide
formal properties for these choices.
Ichimura and Linton (2005) consider
inverse probability weighting estimators and
analyze the formal problem of bandwidth
selection with the focus on the average treatment effect. Imbens, Newey and Ridder
(2005) look at series regression estimators
and analyze the choice of the number of
terms to be included, again with the objective
being the average treatment effect. Imbens
and Rubin (forthcoming) discuss some stepwise covariate selection methods for finding
a specification for the propensity score.
It is clear that more work needs to be
done in this area, both for the case where
the choice is which covariates to include
from a large set of potential covariates,
and in the case where the choice concerns

51

f­ unctional form and functions of a small set
of covariates.
6.

Selection on Unobservables

In this section we discuss a number of
methods that relax the pair of assumptions made in section 5. Unlike in the setting under unconfoundedness, there is not
a unified set of methods for this case. In
a number of special cases there are wellunderstood methods, but there are many
cases without clear recommendations. We
will highlight some of the controversies and
different approaches. First we discuss some
methods that simply drop the unconfoundedness assumption. Next, in section 6.2, we
discuss sensitivity analyses that relax the
unconfoundedness assumption in a more
limited manner. In section 6.3, we discuss
instrumental variables methods. Then, in
section 6.4 we discuss regression discontinuity designs, and in section 6.5 we discuss
difference-in-differences methods.
6.1 Bounds
In a series of papers and books, Manski
(1990, 1995, 2003, 2005, 2007) has
­developed a general framework for inference
in settings where the parameters of interest
are not identified. Manski’s key insight is that
even if in large samples one cannot infer the
exact value of the parameter, one may be
able to rule out some values that one could
not rule out a priori. Prior to Manski’s work,
researchers had typically dismissed models
that are not point-identified as not useful in
practice. This framework is not restricted to
causal settings, and the reader is referred to
Manski (2007) for a general discussion of the
approach. Here we limit the discussion to
program evaluation settings.
We start by discussing Manksi’s perspective in a very simple case. Suppose we
have no covariates and a binary outcome Yi
∈ {0, 1}. Let the goal be inference for the

Journal of Economic Literature, Vol. XLVII (March 2009)

52

­average effect in the population, τ PATE. We
can decompose the population average treatment effect as
τ PATE = E[Yi(1) | Wi = 1] · pr(Wi = 1)
+ E[Yi(1) | Wi = 0] · pr(Wi = 0)
− E[Yi(0) | Wi = 1] · pr(Wi = 1)
+ E[Yi(0) | Wi = 0] · pr(Wi = 0)].
Of the eight components of this expression, we can estimate six. The data contain no information about the remaining
two, E[Yi(1) | Wi = 0] and E[Yi(0) | Wi = 1].
Because the outcome is binary, and before
seeing any data, we can deduce that these
two ­conditional expectations must lie inside
the interval [0, 1], but we cannot say any more
without additional assumptions. This implies
that without additional assumptions we can
be sure that
τ PATE ∈ [τ l, τu],
where we can express the lower and upper
bound in terms of estimable quantities,
τ l = E[Yi(1) | Wi = 1] · pr(Wi = 1)
− pr(Wi = 1) − E[Yi(0) | Wi = 0]
× pr(Wi = 0),
and
τu = E[Yi(1) | Wi = 1] · pr(Wi = 1)
+ pr(Wi = 0) − E[Yi(0) | Wi = 0]
× pr(Wi = 0),
In other words, we can bound the average
treatment effect. In this example the bounds
are tight, meaning that without additional

assumptions we cannot rule out any value
inside the bounds. See Manski et al. (1992)
for an empirical example of these particular
bounds.
In this specific case the bounds are not
particularly informative. The width of the
bounds, the difference in τu − τ l, with τ l and
τu given above, is always equal to one, implying we can never rule out a zero average treatment effect. (In some sense this is obvious:
if we refrain from making any assumptions
regarding the treatment effects we cannot
rule out that the treatment effect is zero for
any unit.) In general, however, we can add
some assumptions, short of making the type
of assumption as strong as unconfoundedness
that gets us back to the point-identified case.
With such weaker assumptions we may be able
to tighten the bounds and obtain informative
results, without making the strong assumptions that strain credibility. The presence of
covariates increases the scope for additional
assumptions that may tighten the bounds.
Examples of such assumptions include those
in the spirit of instrumental variables, where
some covariates are known not to affect the
potential outcomes (e.g., Manski 2007), or
monotonicity assumptions where expected
outcomes are monotonically related to covariates or treatments (e.g., Manski and John
V. Pepper 2000). For an application of these
methods, see Hotz, Charles H. Mullin, and
Seth G. Sanders (1997). We return to some of
these settings in section 6.3.
This discussion has focused on identification and demonstrated what can be learned
in large samples. In practice these bounds
need to be estimated, which leads to additional uncertainty regarding the estimands.
A fast developing literature (e.g., Horowitz
and Manski 2000; Imbens and Manski 2004;
Chernozhukov, Hong, and Elie Tamer 2007;
Arie Beresteanu and Francesca Molinari
2006; Romano and Azeem M. Shaikh 2006a,
2006b; Ariel Pakes et al. 2006; Adam M.
Rosen 2006; Donald W. K. Andrews and

Imbens and Wooldridge: Econometrics of Program Evaluation
Gustavo Soares 2007; Ivan A. Canay 2007;
and Jörg Stoye 2007) discusses construction
of confidence intervals in general settings
with partial identification. One point of contention in this literature has been whether
the focus should be on confidence intervals
for the parameter of interest (τ PATE in this
case), or for the identified set. Imbens and
Manski (2004) develop confidence sets for
the parameter. In large samples, and at a
95 percent confidence level, the Imbens–
Manski confidence intervals amount to
taking the lower bound minus 1.645 times
the standard error of the lower bound and
the upper bound plus 1.645 times its standard error. The reason for using 1.645
rather than 1.96 is to take account of the
fact that, even in the limit, the width of the
confidence set will not shrink to zero, and
therefore one only needs to be concerned
with one-sided errors. Chernozhukov, Hong,
and Tamer (2007) focus on confidence sets
that include the entire partially identified
set itself with fixed probability. For a given
confidence level, the latter approach generally leads to larger confidence sets than the
Imbens–Manski approach. See also Romano
and Shaikh (2006a, 2006b) for subsampling
approaches to inference in these settings.
6.2 Sensitivity Analysis
Unconfoundedness has traditionally been
seen as an all or nothing assumption: either
it is satisfied and one proceeds accordingly using the methods appropriate under
unconfoundedness, such as matching, or
the assumption is deemed implausible and
one considers alternative methods. The latter include the bounds approach discussed
in section 6.1, as well as approaches relying
on alternative assumptions, such as instrumental variables, which will be discussed in
section 6.3. However, there is an important
alternative that has received much less attention in the economics literature. Instead of

53

completely relaxing the unconfoundedness
assumption, the idea is to relax it slightly.
More specifically, violations of unconfoundedness are interpreted as evidence of the
presence of unobserved covariates that are
correlated, both with the potential outcomes
and with the treatment indicator. The size of
bias these violations of unconfoundedness
can induce depends on the strength of these
correlations. Sensitivity analyses investigate
whether results obtained under the maintained assumption of unconfoundedness can
be changed substantially, or even overturned
entirely, by modest violations of the unconfoundedness assumption.
To be specific, consider a job training program with voluntary enrollment.
Suppose that we have monthly labor market
histories for a two year period prior to the
program. We may be concerned that individuals choosing to enroll in the program
are more motivated to find a job than those
that choose not to enroll in the program.
This unobserved motivation may be related
to subsequent earnings both in the presence
and in the absence of training. Conditioning
on the recent labor market histories of individuals may limit the bias associated with
this unobserved motivation, but it need not
eliminate it entirely. However, we may be
willing to limit how highly ­correlated unobserved motivation is with the enrollment
decision and the earnings outcomes in the
two regimes, conditional on the labor market histories. For example, if we compare
two individuals with the same labor market history for the last two years, e.g., not
employed the last six months and working
the eighteen months before, and both with
one two-year old child, it may be reasonable to assume that these cannot differ radically in their unobserved motivation given
that their recent labor market outcomes
have been so similar. The sensitivity analyses developed by Rosenbaum and Rubin
(1983a) formalize this idea and provides a

54

Journal of Economic Literature, Vol. XLVII (March 2009)

tool for making such assessments. Imbens
(2003) applies this sensitivity analysis to
data from labor market training programs.
The second approach is associated with
work by Rosenbaum (1995). Similar to the
Rosenbaum–Rubin approach Rosenbaum’s
method relies on an unobserved covariate
that generates the deviations from unconfoundedness. The analysis differs in that
sensitivity is measured using only the relation between the unobserved covariate and
the treatment assignment, with the focus
on the correlation required to overturn, or
change substantially, p-values of statistical
tests of no effect of the treatment.
6.2.1 The Rosenbaum–Rubin Approach to
Sensitivity Analysis
The starting point is that unconfoundedness is satisfied only conditional on the
observed covariates Xi and an unobserved
scalar covariate Ui:
Yi(0), Yi(1)   ǁ   Wi | Xi, Ui.
This set up in itself is not restrictive, although
once parametric assumptions are made the
assumption of a scalar unobserved covariate
Ui is restrictive.
Now consider both the conditional distribution of the potential outcomes given
observed and unobserved covariates and the
conditional probability of assignment given
observed and unobserved covariates. Rather
than attempting to estimate both these conditional distributions, the idea behind the
sensitivity analysis is to specify the form and
the amount of dependence of these conditional distributions on the unobserved covariate, and estimate only the dependence on
the observed covariate. Conditional on the
specification of the first part estimation of
the latter is typically straightforward. The
idea is then to vary the amount of dependence of the conditional distributions on the
unobserved covariate and assess how much

this changes the point estimate of the average treatment effect.
Typically the sensitivity analysis is done
in fully parametric settings, although
since the models can be arbitrarily flexible, this is not particularly restrictive.
Following Rosenbaum and Rubin (1983b),
we illustrate this approach in a setting
with binary outcomes. See Imbens (2003)
and Lee (2005b) for examples in economics. Rosenbaum and Rubin (1983a) fix the
marginal distribution of the unobserved
covariate to be binomial with p = pr(Ui =
1), and assume independence of Ui and Xi.
They specify a logistic distribution for the
treatment assignment:
   pr(Wi = 1 | Xi = x, Ui = u)

exp(α 0 + α′1 x + α 2 · u)
   
    ​.
= ____________________
​ 
1 + exp(α 0 + α′1 x + α 2 · u)

They also specify logistic regression functions for the two potential outcomes:
   pr(Yi(w) = 1 | Xi = x, Ui = u) =

exp(β w0 + β′w1 x + β w2 · u)
   
    ​.
	​ ______________________
1 + exp(β w0 + β′w1 x + β w2 · u)

For the subpopulation with Xi = x and Ui =
u, the average treatment effect is
   E[Yi(1) − Yi(0 | Xi = x, Ui = u] =

exp(β10 + β′11 x + β12 · u)
   
    ​
​ _____________________
1 + exp(β10 + β′11 x + β12 · u)

exp(β 00 + β′01 x + β 02 · u)
− ​ _____________________
   
    ​.
1 + exp(β 00 + β′01 x + β 02 · u)

The average treatment effect τ CATE can be
expressed in terms of the parameters of this
model and the distribution of the observable
covariates by averaging over Xi, and integrating out the unobserved covariate U:

Imbens and Wooldridge: Econometrics of Program Evaluation
τ CATE ≡ τ( p, α 2, β 02, β12, α 0, α1, β 00,
β 01, β10, β11)
N
exp(β10 + β′11 Xi + β12)
1  ​e ​∑​​ p a​ ____________________
   
    ​
= ​ __
N i=1
1 + exp(β10 + β′11 Xi + β12)

exp(β 00 + β′01 Xi + β 02)
− ​ ____________________
   
    ​b
1 + exp(β 00 + β′01 Xi + β 02)

exp(β10 + β′11 Xi)
+ (1 − p) a​ _______________
  
   ​
1 + exp(β10 + β′11 Xi)
exp(β 00 + β′01 Xi)
  
   ​b f .
− ​ _______________
1 + exp(β 00 + β′01 Xi)

We do not know the values of the parameters
( p, α, β), but the data are somewhat informative
about them. One conventional approach would
be to attempt to estimate all parameters, and
then use those estimates to obtain an estimate
for the average treatment effect. Given the
specific parametric model this may be possible, although in general this would be ­difficult
given the inclusion of unobserved covariates
in the basic model. A second approach, as discussed in section 6.1, is to derive bounds on
τ given the model and the data. A sensitivity
analysis offers a third approach.
The Rosenbaum–Rubin sensitivity analysis proceeds by dividing the parameters into
two sets. The first set includes the ­parameters
that would be set to boundary values under
unconfoundedness, (α 2, β 02, β12), plus the
parameter p capturing the marginal distribution of the unobserved covariate Ui. Together
we refer to these as the sensitivity parameters, θsens = ( p, α 2, β 02, β12). The second set
consists of the remaining parameters, θother
= (α 0, α1, β 00, β 01, β10, β11). The idea is that
θsens is difficult to estimate. Estimates of the
other parameters under unconfoundedness
could be obtained by fixing α 2 = β 02 = β12
= 0 and p at an arbitrary value. The data are
not directly informative about the effect of
an unobserved covariate in the absence of

55

f­ unctional form assumptions, and so attempts
to estimate θsens are therefore unlikely to be
effective. Given θsens, however, estimating the
remaining parameters is considerably easier.
In the second step the plan is therefore to
fix the first set of parameters and estimate
the others by maximum likelihood, and then
translate this into an estimate for τ. Thus, for
fixed θsens, we first estimate the remaining
parameters through maximum likelihood:
  

ˆ
​  
ax ​L(θother | θsens),
	​ θ​
other(θsens) = arg m
θother

where L( · ) is the logarithm of the likelihood
function. Then we consider the function
  

ˆ
τ(θsens) = τ(θsens, ​ θ​
other(θsens)),

Finally, in the third step, we consider the range
of values of the function τ(θsens) for a reasonable set of values for the sensitivity parameters
(θsens), and obtain a set of values for τ CATE.
The key question is how to choose the
set of reasonable values for the sensitivity ­parameters. If we do not wish to restrict
this set at all, we end up with unrestricted
bounds along the lines of section 6.1. The
power from the sensitivity approach comes
from the researcher’s willingness to put
real limits on the values of the sensitivity
parameters ( p, α 2, β 02, β12). Among these
parameters it is difficult to put real limits on
p, and typically it is fixed at 1/2, with little
sensitivity to its choice. The more interesting
parameters are (α 2, β 02, β12). Let us assume
that the effect of the unobserved covariate is
the same in both treatment arms, β 2 ≡ β 02
= β 21, so that there are only two parameters
left to fix, α 2 and β 2. Imbens (2003) suggests linking the parameters to the effects of
the observed covariates on assignment and
potential outcomes. Specifically he suggests
to calculate the partial correlations between
observed covariates and the treatment and
potential outcomes, and then as a benchmark look at the sensitivity to an unobserved

56

Journal of Economic Literature, Vol. XLVII (March 2009)

covariate that has ­partial correlations with
treatment and potential outcomes as high as
any of the observed covariates. For example,
Imbens considers, in the labor market training example, what the effect would be of
omitting unobserved motivation, if in fact
motivation had as much explanatory power
for future earnings and for treatment choice
as did earnings in the year prior to the training program. A bounds analysis, in contrast,
would implicitly allow unobserved motivation to completely determine both selection
into the program and future earnings. Even
though putting hard limits on the effect of
motivation on earnings and treatment choice
may be difficult, it may be reasonable to put
some limits on it, and the Rosenbaum–Rubin
sensitivity analysis provides a useful framework for doing so.
6.2.2 Rosenbaum’s Method for Sensitivity
Analysis
Rosenbaum (1995) developed a slightly
different approach. The advantage of his
approach is that it requires fewer tuning
parameters than the Rosenbaum–Rubin
approach. Specifically, it only requires the
researcher to consider the effect unobserved
confounders may have on the probability of
treatment assignment. Rosenbaum’s focus
is on the effect the presence of unobserved
covariates could have on the p-value for the
test of no effect of the treatment based on the
unconfoundedness assumption, in contrast to
the Rosenbaum–Rubin focus on point estimates for average treatment effects. Consider
two units i and j with the same value for the
covariates, xi = xj. If the unconfoundedness
assumption conditional on Xi holds, both units
must have the same probability of assignment
to the treatment, e(xi) = e(xj). Now suppose
unconfoundedness only holds conditional on
both Xi and a binary unobserved covariate
Ui. In that case the assignment probabilities
for these two units may differ. Rosenbaum

suggests bounding the ratio of the odds ratios
e(xi)/(1 − e(xi)) and e(xj)/(1 − e(xj)):
e(xi) · (1 − e(xj))
1/Γ ≤____________
​ 
  
   ​≤ Γ.
(1 − e(xi)) · e(xj)

If Γ = 1, we are back in the setting with
unconfoundedness. If we allow Γ = ∞, we
are not restricting the association between
the treatment indicator and the potential
outcomes. Rosenbaum investigates how
much the odds would have to be different in
order to substantially change the p-value. Or,
starting from the other side, he investigates
for fixed values of Γ what the implication is
on the p-value.
For example, suppose that a test of the
null hypothesis of no effect has a p-value of
0.0001 under the assumption of unconfoundedness. If the data suggest it would take the
presence of an unobserved covariate that
changes the odds of participation by a factor
ten in order to increase that p-value to 0.05,
then one would likely consider the result to
be very robust. If instead a small change in
the odds of participation, say with a value of
Γ = 1.5, would be sufficient for a change of
the p-value to 0.05, the study would be much
less robust.
6.3 Instrumental Variables
In this section, we review the recent literature on instrumental variables. We focus
on the part of the literature concerned with
­heterogenous effects. In the current section, we limit the discussion to the case with
a binary endogenous variable. The early
literature focused on identification of the
­population average treatment effect and the
average effect on the treated. Identification
of these estimands ran into serious problems once researchers wished to allow for
unrestricted heterogeneity in the effect of
the treatment. In an important early result,
Bloom (1984) showed that if eligibility for the
program is used as an instrument, then one

Imbens and Wooldridge: Econometrics of Program Evaluation
can identify the average effect of the treatment for those who received the treatment.
Key for the Bloom result is that the instrument changes the probability of receiving
the treatment to zero. In order to identify
the average effect on the overall population, the instrument would also need to shift
the probability of receiving the treatment
to one. This type of identification is sometimes referred to as identification at infinity
(Gary Chamberlain 1986; Heckman 1990) in
settings with a continuous instrument. The
practical usefulness of such identification
results is fairly limited outside of cases where
eligibility is randomized. Finding a credible
instrument is typically difficult enough, without also requiring that the instrument shifts
the probability of the treatment close to zero
and one. In fact, the focus of the current
literature on instruments that can credibly
be expected to satisfy exclusion restrictions
makes it even more difficult to find instruments that even approximately satisfy these
support conditions. Imbens and Angrist
(1994) got around this problem by changing
the focus to average effects for the subpopulation that is affected by the instrument.
Initially we focus on the case with a binary
instrument. This case provides some of the
clearest insight into the identification problems. In that case the identification at infinity arguments are obviously not satisfied and
so one cannot (point-)identify the population
average treatment effect.
6.3.1 A Binary Instrument
Imbens and Angrist adopt a potential outcome notation for the receipt of the ­treatment,
as well as for the outcome itself. Let Zi denote
the value of the instrument for individual i.
Let Wi(0) and Wi(1) denote the level of the
treatment received if the instrument takes on
the values 0 and 1 respectively. As before, let
Yi(0) and Yi(1) denote the potential values for
the outcome of interest. The observed treatment is, analogously to the relation between

57

the observed outcome Yi and the potential
outcomes Yi(0) and Yi(1), is
Wi = Wi (0) · (1 − Zi)
		
W (0)
+ Wi(1) · Zi = e i
Wi(1)

if Zi = 0
.
if Zi = 1

Exogeneity of the instrument is captured by
the assumption that all potential outcomes
are independent of the instrument, or
(Yi(0), Yi(1), Wi(0), Wi(1))   ǁ   Zi.

Formulating exogeneity in this way is attractive compared to conventional residualbased definitions, as it does not require the
researcher to specify a regression function in
order to define the residuals. This assumption captures two properties of the instrument. First, it captures random assignment
of the instrument so that causal effects of the
instrument on the outcome and treatment
received can be estimated consistently. This
part of the assumption, which is implied by
explicitly randomization of the instrument, as
for example in the seminal draft lottery study
by Angrist (1990), is not sufficient for causal
interpretations of instrumental variables
methods. The second part of the assumption
captures an exclusion restriction that there
is no direct effect of the instrument on the
outcome. This second part is captured by the
absence of z in the definition of the potential
outcome Yi(w). This part of the assumption is
not implied by randomization of the instrument and it has to be argued on a case by
case basis. See Angrist, Imbens, and Rubin
(1996) for more discussion on the distinction
between these two assumptions, and for a
formulation that separates them.
Imbens and Angrist introduce a new concept, the compliance type of an individual.
The type of an individual describes the level
of the treatment that an individual would
receive given each value of the instrument.

Journal of Economic Literature, Vol. XLVII (March 2009)

58

In other words, it is captured by the pair of
values (Wi(0), Wi(1)). With both the treatment and instrument binary, there are four
types of responses for the potential treatment. It is useful to define the compliance
types explicitly:

Ti =

e

never-taker
complier
defier
always-taker

if Wi(0) =
if Wi(0) =
if Wi(0) =
if Wi(0) =

Wi(1) = 0
0, Wi(1) = 1
1, Wi(1) = 0 .
Wi(1) = 1

The labels never-taker, complier, defier,
and always-taker (e.g., Angrist, Imbens, and
Rubin 1996) refer to the setting of a randomized experiment with noncompliance, where
the instrument is the (random) assignment
to the treatment and the endogenous regressor is an indicator for the actual receipt of
the treatment. Compliers are in that case
individuals who (always) comply with their
assignment, that is, take the treatment if
assigned to it and not take it if assigned to
the control group. One cannot infer from the
observed data (Zi, Wi, Yi) whether a particular
individual is a complier or not. It is important
not to confuse compliers (who comply with
their actual assignment and would have complied with the alternative assignment) with
individuals who are observed to comply with
their actual assignment: that is, individuals
who complied with the assignment they actually received, Zi = Wi. For such individuals
we do not know what they would have done
had their assignment been different, that is
we do not know the value of Wi(1 − Zi).
Imbens and Angrist then invoke an additional assumption they refer to as ­monotonicity.
Monotonicity requires that Wi(1) ≥ Wi(0) for
all individuals, or that increasing the level of
the instrument does not decrease the level
of the treatment. This assumption is equivalent to ruling out the presence of defiers, and
it is therefore sometimes referred to as the
“no-defiance” assumption (Alexander Balke
and Pearl 1994; Pearl 2000). Note that in the

Bloom set up with one-sided noncompliance
both always-takers and defiers are absent by
assumption.
Under these two assumptions, independence of all four potential outcomes
(Yi(0), Yi(1), Wi(0), Wi(1)) and the instrument
Zi, and monotonicity, Imbens and Angrist
show that one can identify the average
effect of the treatment for the subpopulation of compliers. Before going through their
argument, it is useful to see why we cannot
generally identify the average effect of the
treatment for others subpopulations. Clearly,
one cannot identify the average effect of the
treatment for never-takers because they are
never observed receiving the treatment, and
so E[Yi(1) | Ti = n] is not identified. Thus,
only compliers are observed in both treatment groups, so only for this group is there
any chance of identifying the average treatment effect. In order to understand the
positive component of the Imbens–Angrist
result, that we can identify the average effect
for compliers, it is useful to consider the
subpopulations defined by instrument and
treatment. Table 4 shows the information
we have about the individual’s type given
the ­monotonicity assumption. Consider individuals with (Zi = 1, Wi = 0). Because of
monotonicity such individuals can only be
never-takers. Similarly, individuals (Zi = 0,
Wi = 1) can only be always-takers. However,
consider individuals with (Zi = 0, Wi = 0).
Such individuals can be either compliers
or never-takers. We ­cannot infer the type
of such individuals from the observed data
alone. Similarly, individuals with (Zi = 1,
Wi = 1) can be either compliers or alwaystakers.
The intuition for the identification result
is as follows. The first step is to see that we
can infer the population proportions of the
three remaining subpopulations, nevertakers, always-takers and compliers (using
the fact that the monotonicity assumption
rules out the presence of defiers). Call these

Imbens and Wooldridge: Econometrics of Program Evaluation

59

Table 4

Type by Observed Variables
Zi

       Wi

0
1

­population shares Pt = pr(Ti = t), for t ∈
{n, a, c}. Consider the subpopulation with Zi
= 0. Within this subpopulation we observe
Wi = 1 only for always-takers. Hence the
conditional probability of Wi = 1 given Zi =
0 is equal to the population share of alwaystakers: Pa = pr(Wi = 1 | Zi = 0). Similarly, in
the subpopulation with Zi = 1 we observe Wi
= 0 only for never-takers. Hence the population share of never-takers is equal to the conditional probability of Wi = 0 given Zi = 1: Pn
= pr(Wi = 0 | Zi = 1). The population share
of compliers is then obtained by subtracting
the population shares of never-takers and
always-takers from one: Pc = 1 − Pn − Pa.
The second step uses the distribution of Yi
given (Zi, Wi). We can infer the distribution
of Yi | Wi = 0, Ti = n from the subpopulation with (Zi, Wi) = (1, 0) since all these
individuals are known to be never-takers.
Then we use the distribution of Yi | Zi = 0,
Wi = 0. This is a mixture of the distribution
of Yi | Wi = 0, Ti = n and the distribution of
Yi | Wi = 0, Ti = c, with mixture probabilities
equal to the relative population shares, Pn/
(Pc + Pn) and Pc/(Pc + Pn), respectively. Since
we already inferred the population shares
of the never-takers and compliers as well as
the ­distribution of Yi | Wi = 0, Ti = n, we can
back out of the conditional distribution of
Yi | Wi = 0, Ti = c. Similarly we can infer the
conditional distribution of Yi | Wi = 1, Ti = c.
The difference between the means of these
two conditional distributions is the Local
Average Treatment Effect (LATE), (Imbens
and Angrist, 1994):

0

1

Nevertaker/Complier
Alwaystaker

Nevertaker
Alwaystaker/Complier

τ LATE = E[Yi(1) − Yi(0) | Wi(0) = 0,
Wi(1) = 1]

= E[Yi(1) − Yi(0) | Ti = complier].
In practice one need not estimate the local
average treatment effect by decomposing the
mixture distributions directly. Imbens and
Angrist show that LATE equals the standard
instrumental variables estimand, the ratio of
the covariance of Yi and Zi and the covariance of Wi and Zi:
E[Yi | Zi = 1] − E[Yi | Zi = 0]
τ LATE = ______________________
​ 
   
    ​
E[Wi | Zi = 1] − E[Wi | Zi = 0]
E[Yi · (Zi − E[Zi])]
  
   ​,
= ______________
​ 
E[Wi · (Zi − E[Zi])]
which can be estimated using two-stageleast-squares. For applications using
­parametric models with covariate, see
Hirano et al. (2000) and Fabrizia Mealli et
al. (2004).
Earlier we argued that one cannot consistently estimate the average effect for
either never-takers or always-takers in
this setting. Nevertheless, we can still use
the bounds approach from Manski (1990,
1995) to bound the average effect for the
full population. To understand the nature
of the bound, it is useful to decompose the
average effect τ PATE by compliance type
(maintaining monotonicity, so there are no
defiers):

Journal of Economic Literature, Vol. XLVII (March 2009)

60

τ PATE = Pn · E[Yi(1) − Yi(0) | Ti = n]
+ Pa · E[Yi(1) − Yi(0) | Ti = a]
+ Pc · E[Yi(1) − Yi(0) | Ti = c].
The only quantities not consistently estimable are the average effects for never-takers
and always-takers. Even for those we have
some information. For example, we can write
E[Yi(1) − Yi(0) | Ti = n] = E[Yi(1) | Ti = n] −
E[Yi(0) | Ti = n]. The second term we can
estimate, and the data are completely uninformative about the first term. Hence, if there
are natural bounds on Yi(1) (for example, if
the outcome is binary), we can use that to
bound E[Yi(1) | Ti = n], and then in turn use
that to bound τ PATE. These bounds are tight.
See Manski (1990), Toru Kitagawa (2008),
and Balke and Pearl (1994).
6.3.2 Multivalued Instruments and
Weighted Local Average Treatment
Effects
The previous discussion was in terms of a
single binary instrument. In that case there is
no other average effect of the treatment that
can be estimated consistently other than the
local average treatment effect, τ LATE. With
a multivalued instrument, or with multiple
binary instruments (still maintaining the setting of a binary treatment—see for extensions
of the local average treatment effect concept to the multiple treatment case Angrist
and Imbens (1995) and Card (2001), we can
estimate a variety of local average treatment
effects. Let 핑 = {z1, … , zK} denote the set of
values for the instruments. Initially we take
the set of values to be finite. Then for each
pair (zk, zl) with pr(Wi = 1 | Zi = zk) > pr(Wi
= 1 | Zi = zl) one can define a local average
treatment effect:
τ LATE(zk, zl) =
   E[Yi(1) − Yi(0) | Wi(zl) = 0, Wi(zk) =1 ].

We can combine these to estimate any
weighted average of these local average treatment effects:
​ ​  ​λ k,l · τ LATE(zk, zl).
τ LATE,λ = ∑
k,l

Imbens and Angrist show that the standard
instrumental variables estimand, using g(Zi)
as an instrument for Wi, is equal to a particular weighted average:
E[Yi · (g(Zi) − E[g(Zi)])]
__________________
	​ 
   
   ​ = τ LATE,λ,
E[Wi · (g(Zi) − E[g(Zi)])]
for a particular set of nonnegative weights as
long as E[Wi | g(Zi) = g] increases in g.
Heckman and Vytlacil (2006) and
Heckman, Sergio Urzua, and Vytlacil (2006)
study the case with a continuous instrument.
They use an additive latent single index setup
where the treatment received is equal to
Wi = 1{h(Zi) + Vi ≥ 0},
where h( · ) is strictly monotonic, and the
latent type Vi is independent of Zi. In ­general,
in the presence of multiple instruments, this
latent single index framework imposes substantive restrictions.14 Without loss of generality we can take the marginal distribution
of Vi to be uniform. Given this framework,
Heckman, Urzua, and Vytlacil (2006) define
the marginal treatment effect as a function
of the latent type v of an individual,
τ MTE(v) = E[Yi(1) − Yi(0) | Vi = v].
In the single continuous instrument case,
τ MTE(v) is, under some differentiability and
invertibility conditions, equal to a limit of
local average treatment effects:

14 See Vytlacil (2002) for a discussion in the case with
binary instruments, where the latent index set up implies
no loss of generality.

Imbens and Wooldridge: Econometrics of Program Evaluation
τ MTE(v) = ​   
lim
​τ LATE(h−1(v), z).
−1
z↓h (v)

A parametric version of this concept goes
back to work by Anders Björklund and
Robert Moffitt (1987). All average treatment
effects, including the overall average effect,
the average effect for the treated, and any
local average treatment effect can now be
expressed in terms of integrals of this marginal treatment effect, as shown in Heckman
and Vytlacil (2005). For example, τ PATE =​
∫01​ ​ ​ τ MTE(v) dv. A complication in practice is
that not ­necessarily all the marginal treatment effects can be estimated. For example,
if the instrument is binary, Zi ∈ {0, 1}, then
for individuals with Vi < min(−h(0), −h(1)),
it follows that Wi = 0, and for these nevertakers we cannot estimate τ MTE(v). Any
average effect that requires averaging over
such values of v is therefore also not pointidentified. Moreover, average effects that can
be expressed as integrals of τ MTE(v) may be
identified even if some of the τ MTE(v) that
are being integrated over are not identified.
Again, in a binary instrument example with
pr(Wi = 1 | Zi = 1) = 1, and pr(Wi = 1 | Zi
= 0) = 0, the average treatment effect τ PATE
is identified, but τ MTE(v) is not identified for
any value of v.
6.4 Regression Discontinuity Designs
Regression discontinuity (RD) methods
have been around for a long time in the psychology and applied statistics ­literature, going
back to the early 1960s. For ­discussions and
references from this literature, see Donald L.
Thistlethwaite and Campbell (1960), William
M. K. Trochim (2001), Shadish, Cook, and
Campbell (2002), and Cook (2008). Except
for some important foundational work by
Goldberger (1972a, 1972b), it is only recently
that these methods have attracted much attention in the economics literature. For some of
the recent applications, see Van Der Klaauw
(2002, 2008a), Lee (2008), Angrist and
Victor Lavy (1999), DiNardo and Lee (2004),

61

Kenneth Y. Chay and Michael Greenstone
(2005), Card, Alexandre Mas, and Jesse
Rothstein (2007), Lee, Enrico Moretti, and
Matthew J. Butler (2004), Jens Ludwig and
Douglas L. Miller (2007), Patrick J. McEwan
and Joseph S. Shapiro (2008), Sandra E.
Black (1999), Susan Chen and van der Klaauw
(2008), Ginger Zhe Jin and Phillip Leslie
(2003), Thomas Lemieux and Kevin Milligan
(2008), Per Pettersson-Lidbom (2007, 2008),
and Pettersson-Lidbom and Björn Tyrefors
(2007). Key theoretical and conceptual
contributions include the interpretation of
estimates for fuzzy regression discontinuity designs allowing for general heterogeneity of treatment effects (Hahn, Todd, and
van der Klaauw 2001), adaptive estimation
methods (Yixiao Sun 2005), methods for
bandwidth selection tailored to the RD setting, (Ludwig and Miller 2005; Imbens and
Karthik Kalyanaraman 2008) and various
tests for discontinuities in means and distributions of nonaffected variables (Lee 2008;
McCrary 2008) and for misspecification
(Lee and Card 2008). For recent reviews in
the economics literature, see van der Klaauw
(2008b), Imbens and Lemieux (2008), and
Lee and Lemieux (2008).
The basic idea behind the RD design is that
assignment to the treatment is determined,
either completely or partly, by the value of
a predictor (the forcing variable Xi) being on
either side of a common threshold. This generates a discontinuity, sometimes of size one,
in the conditional probability of receiving
the treatment as a function of this particular
predictor. The forcing variable is often itself
associated with the potential outcomes, but
this association is assumed to be smooth. As
a result any discontinuity of the conditional
distribution of the outcome as a function of
this covariate at the threshold is interpreted
as evidence of a causal effect of the treatment.
The design often arises from administrative
decisions, where the incentives for individuals to participate in a program are rationed

62

Journal of Economic Literature, Vol. XLVII (March 2009)

for reasons of resource constraints, and clear
transparent rules, rather than discretion, by
administrators are used for the allocation of
these incentives.
It is useful to distinguish between two general settings, the sharp and the fuzzy regression discontinuity designs (e.g., Trochim
1984, 2001; Hahn, Todd, and van der Klaauw
2001; Imbens and Lemieux 2008; van der
Klaauw 2008b; Lee and Lemieux 2008).
6.4.1 The Sharp Regression Discontinuity
Design
In the sharp regression discontinuity (SRD)
design, the assignment Wi is a deterministic
function of one of the covariates, the forcing
(or treatment-determining) variable Xi:
Wi = 1[Xi ≥ c],
where 1[·] is the indicator function, equal to
one if the even in brackets is true and zero
otherwise. All units with a covariate value of
at least c are in the treatment group (and participation is mandatory for these individuals),
and all units with a covariate value less than
c are in the control group (members of this
group are not eligible for the treatment). In
the SRD design, we focus on estimation of
(32)

τ SRD = E[Yi(1) − Yi(0) | Xi = c].

(Naturally, if the treatment effect is constant,
then τ SRD = τ PATE.) Writing this expression
as E[Yi(1) | Xi = c] − E[Yi(0) | Xi = c], we
focus on identification of the two terms separately. By design there are no units with Xi
= c for whom we observe Yi(0). To estimate
E[Yi(w) | Xi = c] without making functional
form assumptions, we exploit the ­possibility
of observing units with covariate values arbitrarily close to c.15 In order to justify this
15 Although in principle the first term in the difference
in (32) would be straightforward to estimate if we actually
observe individuals with Xi = x, with continuous covariates

averaging we make a smoothness assumption that the two conditional expectations
E[Yi(w) | Xi = x], for w = 0, 1, are continuous
in x. Under this assumption, E[Yi(0) | Xi = c]
= limx↑c E[Yi(0) | Xi = x] = limx↑c E[Yi | Xi = x],
implying that
im​E[Yi | Xi = x] − ​l  
im​E[Yi | Xi = x],
τ SRD = l​  
x↑c

x↓c

where this expression uses the fact that Wi is
a deterministic function of Xi (a key feature
of the SRD). The statistical problem becomes
one of estimating a regression function nonparametrically at a boundary point. We discuss the statistical problem in more detail in
section 6.4.4.
6.4.2 The Fuzzy Regression Discontinuity
Design
In the fuzzy regression discontinuity (FRD)
design, the probability of receiving the treatment need not change from zero to one at the
threshold. Instead the design only requires a
discontinuity in the probability of assignment
to the treatment at the threshold:
	​lim
  ​pr(Wi = 1 | Xi = x)
x↓c

≠ l​  
im​pr(Wi = 1 | Xi = x).
x↑c

In practice, the discontinuity needs to be
sufficiently large that typically it can be seen
easily in simple graphical analyses. These
discontinuities can arise if incentives to participate in a program change discontinuously
at a threshold, without these incentives being
powerful enough to move all units from nonparticipation to participation.
In this design we look at the ratio of the
jump in the regression of the outcome on
the covariate to the jump in the regression
of the treatment indicator on the covariate

we also need to estimate this term by averaging over units
with covariate values close to c.

Imbens and Wooldridge: Econometrics of Program Evaluation
as an average causal effect of the treatment.
Formally, the functional of interest is
​l  
im​E[Yi | Xi = x] − ​l  
im​E[Yi | Xi = x]
x↑c
x↓c
​ 
    
    ​.
τ frd = __________________________
​l  
im​E[Wi | Xi = x] − ​l  
im​E[Wi | Xi = x]
x↓c

x↑c

Hahn, Todd, and van der Klaauw (2001)
exploit the instrumental variables connection to interpret the fuzzy regression
discontinuity design when the effect of
the treatment varies by unit. They define
complier to be units whose participation is
affected by the cutoff point. That is, a complier is someone with a value of the forcing
variable Xi close to c, and who would participate if c were chosen to be just below
Xi, and not participate if c were chosen to
be just above Xi. Hahn, Todd, and van der
Klaauw then exploit that structure to show
that in combination with a monotonicity
assumption,
τ frd = E[Yi(1) − Yi(0) | unit i is a complier
and Xi = c].

The estimand τ frd is an average effect of the
treatment, but only averaged for units with
Xi = c (by regression discontinuity), and only
for compliers (people who are affected by
the threshold). Clearly the analysis generally
does not have much external validity. It is
only valid for the subpopulation who is complier at the threshold, and it is only valid for
the subpopulation with Xi = c. Nevertheless,
the FRD analysis may do well in terms of
internal validity.
It is useful to compare the RD method in
this setting with standard methods based on
unconfoundedness. In contrast to the SRD
case, an unconfoundedness-based analysis
is possible in the FRD setting because some
treated observations will have Xi ≤ c, and
some control observations will have Xi ≥ c.
Ignoring the FRD setting—that is, ignoring the discontinuity in E[Wi | Xi = x] at x

63

= c—and acting as if unconfoundedness
holds, would lead to estimating the average treatment effect at Xi = c based on the
expression
τunconf = E[Yi | Xi = c, Wi = 1]
− E[Yi | Xi = c, Wi = 0],
which equals E[Yi(1) − Yi(0) | Xi = c] under
unconfoundedness. In fact, under unconfoundedness one can estimate the average
effect E[Yi(1) − Yi(0) | Xi = x] at values of
x different from c. However, an interesting
result is that if unconfoundedness holds,
the FRD also estimates E[Yi(1) − Yi(0) | Xi
= c], as long as the potential outcomes have
smooth expectations as a function of the
forcing variable around x = c. A special case
of this is discussed in Hahn, Todd, and van
der Klaauw (2001), who assume only that
treatment is unconfounded with respect to
the individual-specific gain. Therefore, in
principle, there are situations where even if
one believes that unconfoundedness holds,
one may wish to use the FRD approach.
In particular, even if we maintain unconfoundedness, a standard analysis based
on τunconf can be problematic because the
potential discontinuities in the regression
functions (at x = c) under the FRD design
invalidate the usual statistical methods that
treat the regression functions as continuous
at x = c.
Although unconfoundedness in the FRD
setting is possible, its failure makes it difficult to interpret τunconf. By contrast, provided
monotonicity holds, the FRD parameter,
τ frd, identifies the average treatment effect
for compliers at x = c. In other words,
approaches that exploit the FRD nature of
the design identify an (arguably) interesting
parameter both when unconfoundedness
holds and in a leading case (monotonicity)
when unconfoundedness fails.

Journal of Economic Literature, Vol. XLVII (March 2009)

64

6.4.3 Graphical Methods
Graphical analyses are typically an integral part of any RD analysis. RD designs
suggest that the effect of the treatment of
interest can be measured by the value of the
discontinuity in the conditional expectation
of the outcome as a function of the forcing
variable at a particular point. Inspecting the
estimated version of this conditional expectation is a simple yet powerful way to visualize the identification strategy. Moreover, to
assess the credibility of the RD strategy, it
can be useful to inspect additional graphs, as
discussed below in section 6.4.5. For strikingly clear examples of such plots, see Lee,
Moretti, and Butler (2004), Rafael Lalive
(2008), and Lee (2008).
The main plot in a SRD setting is a histogram-type estimate of the average value
of the outcome by the forcing variable. For
some binwidth h, and for some number of
bins K0 and K1 to the left and right of the
cutoff value, respectively, construct bins
(bk, bk+1], for k = 1, … , K = K0 + K1, where
bk = c − (K0 − k + 1) · h. Then calculate the
number of observations in each bin, and the
average outcome in the bin:
N

​ ​​ 1[bk ≤ Xi ≤ bk+1],
Nk = ∑

and credible estimates with statistically and
substantially significant magnitudes.
In addition to inspecting whether there
is a jump at this value of the covariate, one
should inspect the graph to see whether
there are any other jumps in the conditional
expectation of Yi given Xi that are comparable in size to, or larger than, the discontinuity
at the cutoff value. If so, and if one cannot
explain such jumps on substantive grounds, it
would call into question the interpretation of
the jump at the threshold as the causal effect
of the treatment.
In order to optimize the visual clarity it is
recommended to calculate averages that are
not smoothed across the cutoff point c. In
addition, it is recommended not to artificially
smooth on either side of the threshold in a
way that implies that the only discontinuity
in the estimated regression function is at c.
One should therefore use nonsmooth methods such as the histogram type estimators
described above rather than smooth methods such as kernel estimators.
In a FRD setting, one should also
calculate
__
​  1  ​· ​∑​​ Yi · 1[bk ≤ Xi ≤ bk+1],
	​W ​k = ___
Nk i=1
N

__

i=1

__
​  1  ​· ​∑​​ Yi · 1[bk ≤ Xi ≤ bk+1].
	​Y ​k = ___
Nk i=1
N

__

The key plot is that of the ​Y ​k, for k
= 1,    … , K against the mid point of the
˜ = (b + b )/2. The question is
bins, ​ b​
k
k
k+1
whether around the threshold c (by construction on the edge of one of the bins) there is
any evidence of a jump in the conditional
mean of the outcome. The formal statistical
analyses discussed below are essentially just
sophisticated versions of this, and if the basic
plot does not show any evidence of a discontinuity, there is relatively little chance that the
more sophisticated analyses will lead to robust

   

and plot the ​W ​k against the bin centers b​
​ ˜ k, in
the same way as described above.
6.4.4 Estimation and Inference
The object of interest in regression discontinuity designs is a difference in two regression functions at a particular point (in the
SRD case), and the ratio of two differences of
regression functions (in the FRD case). These
estimands are identified without functional
form assumptions, and in general one might
therefore like to use nonparametric regression methods that allow for flexible functional forms. Because we are interested in the
behavior of the regression functions around a
single value of the covariate, it is attractive

Imbens and Wooldridge: Econometrics of Program Evaluation
to use local smoothing methods such as kernel regression rather than global smoothing
methods such as sieves or series regression
because the latter will generally be sensitive to behavior of the regression function
away from the threshold. Local smoothing
methods are generally well understood (e.g.,
Charles J. Stone 1977; Herman J. Bierens
1987; Härdle 1990; Adrian Pagan and Aman
Ullah 1999). For a particular choice of the
kernel, K( · ), e.g., a rectangular kernel K(z) =
1[−h ≤ z ≤ h],
_ or a Gaussian kernel K(z) =
exp(−z2/2)/​ √( ​2π), the regression function
at x, m(x) = E[Yi | Xi = x] is estimated as
N

ˆ (x) = ∑
m​
​ ​​ Yi · λi,
	​     
i=1

X −x

i
KA ​ ____
​ B
h
with weights λi = ​ __________
  
    ​.
Xi − x
​ Ni=1​ ​ K A​ ____
∑
​ B
h

An important difference with the primary
focus in the nonparametric regression literature is that in the RD setting we are interested in the value of the regression functions
at boundary points. Standard kernel regression methods do not work well in such cases.
More attractive methods for this case are
local linear regression (Fan and Gijbels
1996; Porter 2003; Burkhardt Seifert and
Theo Gasser 1996, 2000; Ichimura and Todd
2007), where locally a linear regression function, rather than a constant regression function, is fitted. This leads to an estimator for
the regression function at x equal to
   

ˆ , β​
ˆ (x) =    
m​
α​
​ ˆ , where (​    
α​
​ ˆ )
​     
N

= arg m
​  
in​​∑​​ λi · (Yi − α − β · (Xi − x))2,
α,β i=1

with the same weights λi as before. In that
case the main remaining choice concerns
the bandwidth, denoted by h. Suppose one
uses a rectangular kernel, K(z) = 1[−h ≤ z
≤ h] (and typically the results are relatively
robust with respect to the choice of the ker-

65

nel). The choice of bandwidth then amounts
to to dropping all observations such that Xi ∉
[c − h, c + h]. The question becomes how to
choose the bandwidth h.
Most standard methods for choosing
bandwidths in nonparametric regression,
including both cross-validation and plug-in
methods, are based on criteria that integrate
the squared error over the entire distribution
m​
​ ˆ (z) − m(z))2 f X(z) dz.
of the covariates: ∫
​ z​  ​ ​(    
For our purposes this criterion does not
reflect the object of interest. We are specifically interested in the regression function at
a single point, moreover, this point is always
a boundary point. Thus we would like to
ˆ (c) − m(c))2] (using
m​
choose h to minimize E[(​     
the data with Xi ≤ c only, or using the data
with Xi ≥ c only). If the density of the forcing
variable is high at the threshold, a bandwidth
selection procedure based on global criteria
may lead to a bandwidth that is much larger
than is appropriate.
There are few attempts to formalize to
standardize the choice of a bandwidth for
such cases. Ludwig and Miller (2005) and
Imbens and Lemieux (2008) discuss some
cross-validation methods that target more
directly the object of interest in RD designs.
Assuming the density of Xi is continuous at c,
and that the conditional variance of Yi given
Xi is continuous and equal to σ 2 at Xi = c,
Imbens and Kalyanaraman (2009) show that
the optimal bandwidth depends on the second derivatives of the regression functions at
the threshold and has the form
hopt = N−1/5 · CK · σ 2

_
​ p1 ​+ ___
​ 1 −1 p​
_______________________
  
   
 ​​b​  ​,
× a ​ 
2
2
2
2
m
∂__
m
limx↓c (​ ∂__
2 ​(x)) + lim x↑c (​  2 ​(x))
1/5

∂x

∂x

where p is the fraction of observations with
Xi ≥ c, and CK is a constant that depends
on the kernel. For a rectangular kernel K(z)
= 1−h≤z≤h, the constant equals CK = 2.70.

66

Journal of Economic Literature, Vol. XLVII (March 2009)

Imbens and Kalyanaram propose and implement a plug in method for the bandwidth.16
If one uses a rectangular kernel, and given
a choice for the bandwidth, estimation for
the SRD and FRD designs can be based on
ordinary least squares and two stage least
squares, respectively. If the bandwidth goes
to zero sufficiently fast, so that the asymptotic bias can be ignored, one can also base
inference on these methods. (See HTV and
Imbens and Lemieux 2008.)
6.4.5 Specification Checks
There are two important concerns in the
application of RD designs, be they sharp
or fuzzy. These concerns can sometimes be
assuaged by investigating various implications of the identification argument underlying the regression discontinuity design.
A first concern about RD designs is the possibility of other changes at the same threshold
value of the covariate. For example, the same
age limit may affect eligibility for multiple
programs. If all the programs whose eligibility changes at the same cutoff value affect
the outcome of interest, an RD analysis may
mistakenly attribute the combined effect to
the treatment of interest. The second concern is that of manipulation by the individuals of the covariate value that underlies the
assignment mechanism. The latter is less of a
concern when the forcing variable is a fixed,
immutable characteristic of an individual
such as age. It is a particular concern when
eligibility criteria are known to potential participants and are based on variables that are
affected by individual choices. For example,
if eligibility for financial aid depends on test
scores that are graded by teachers who know
the cutoff values, there may be a tendency to
push grades high enough to make students
eligible. Alternatively if thresholds are known
to students they may take the test multiple
16 Code in Matlab and Stata for calculating the optimal
bandwidth is available on their website.

times in an attempt to raise their score above
the threshold.
There are two sets of specification checks
that researchers can typically perform to at
least partly assess the empirical relevance of
these concerns. Although the proposed procedures do not directly test null hypotheses that
are required for the RD approach to be valid,
it is typically difficult to argue for the validity
of the approach when these null hypotheses
do not hold. First, one may look for discontinuities in average value of the covariates
around the threshold. In most cases, the reason for the discontinuity in the probability of
the treatment does not suggest a discontinuity in the average value of covariates. Finding
a discontinuity in other covariates typically
casts doubt on the assumptions underlying the
RD design. Specifically, for covariates Zi, the
test would look at the difference
im​E[Zi | Xi = x] − ​l  
im​E[Zi | Xi = x].
τ Z = l​  
x↑c

x↓c

Second, McCrary (2008) suggests testing the
null hypothesis of continuity of the density
of the covariate that underlies the assignment at the threshold, against the alternative of a jump in the density function at that
point. A discontinuity in the density of this
covariate at the particular point where the
discontinuity in the conditional expectation
occurs is suggestive of violations of the no-­
manipulation assumption. Here the focus is
on the difference
im​f X(x) − l​  
im​f X (x).
τ f (x) = l​  
x↓c

x↑c

In both cases a substantially and statistically
significant difference in the left and right
limits suggest that there may be problems
with the RD approach. In practice, more useful than formal statistical tests are ­graphical
analyses of the type discussed in section
6.4.3 where histogram-type estimates of the
conditional expectation of E[Zi | Xi = x] and
of the marginal density f X(x) are graphed.

Imbens and Wooldridge: Econometrics of Program Evaluation
6.5 Difference-in-Differences Methods
Since the seminal work by Ashenfelter (1978)
and Ashenfelter and Card (1985), the use
of Difference-In-Differences (DID) methods has become widespread in empirical
economics. Influential applications include
Philip J. Cook and George Tauchen (1982,
1984), Card (1990), Bruce D. Meyer, W. Kip
Viscusi, and David L. Durbin (1995), Card
and Krueger (1993, 1994), Nada Eissa and
Liebman (1996), Blundell, Alan Duncan, and
Meghir (1998), and many others. The DID
approach is often associated with so-called
“natural experiments,” where policy changes
can be used to effectively define control and
treatment groups. See Angrist and Krueger
(1999), Angrist and Pischke (2009), and
Blundell and Thomas MaCurdy (1999) for
textbook discussions.
The simplest setting is one where outcomes are observed for units observed in
one of two groups, in one of two time periods. Only units in one of the two groups,
in the second time period, are exposed to
a treatment. There are no units exposed to
the treatment in the first period, and units
from the control group are never observed
to be exposed to the treatment. The average
gain over time in the non-exposed (control)
group is subtracted from the gain over time
in the exposed (treatment) group. This double differencing removes biases in second
period comparisons between the treatment
and control group that could be the result
from permanent differences between those
groups, as well as biases from comparisons over time in the treatment group that
could be the result of time trends unrelated
to the treatment. In general this allows for
the endogenous adoption of the new treatment (see Timothy Besley and Case 2000
and Athey and Imbens 2006). We discuss
here the conventional set up, and recent
work on inference (Bertrand, Duflo, and
Mullainathan 2004; Hansen 2007a, 2007b;

67

Donald and Lang 2007), as well as the recent
extensions by Athey and Imbens (2006) who
develop a functional form-free version of the
difference-in-differences methodology, and
Abadie, Diamond, and Jens Hainmueller
(2007), who develop a method for constructing an artificial control group from multiple
nonexposed groups.
6.5.1 Repeated Cross Sections
The standard model for the DID approach
is as follows. Individual i belongs to a group,
Gi ∈ {0, 1} (where group 1 is the treatment
group), and is observed in time period Ti ∈
{0, 1}. For i = 1, … , N, a random sample from
the population, individual i’s group identity
and time period can be treated as random
variables. In the standard DID model, we
can write the outcome for individual i in the
absence of the intervention, Yi(0) as
(33)

Yi(0) = α + β · Ti + γ · Gi + εi,

with unknown parameters α, β, and γ. We
ignore the potential presence of other covariates, which introduce no special complications. The second coefficient in this
specification, β, represents the time component common to both groups. The third
coefficient, γ, represents a group-specific,
time-invariant component. The fourth term,
εi, represents unobservable characteristics
of the individual. This term is assumed to
be independent of the group indicator and
have the same distribution over time, i.e.,
εi   ǁ   (Gi, Ti), and is normalized to have mean
zero.
An alternative set up leading to the same
estimator allows for a time-invariant individual-specific fixed effect, γ i, potentially correlated with Gi, and models Yi(0) as
(34)

Yi(0) = α + β · Ti + γ i + εi.

(See, e.g., Angrist and Krueger 1999.) This
generalization of the standard model does

Journal of Economic Literature, Vol. XLVII (March 2009)

68

not affect the standard DID estimand, and
it will be subsumed as a special case of the
model we propose.
The equation for the outcome without
the treatment is combined with an equation for the outcome given the treatment:
Yi(1) = Yi(0) + τ DID. The standard DID
estimand is under this model equal to
(35)

τ DID = E[Yi(1)] − E[Yi(0)]

6.5.2 Multiple Groups and Multiple Periods
With multiple time periods and multiple
groups we can use a natural extension of the
two-group two-time-period model for the
outcome in the absence of the intervention.
Let T and G denote the number of time periods and groups respectively. Then:
(36)

T

Yi(0) = α + ​∑​​ β t · 1[Ti = t]
t=1

= AE[Yi | Gi = 1, Ti = 1]

G

− E[Yi | Gi = 1, Ti = 0]B

− AE[Yi | Gi = 0, Ti = 1]

− E[Yi | Gi = 0, Ti = 0]B.

In other words, the population average
­difference over time in the control group
(Gi = 0) is subtracted from the population
average difference over time in the treatment
group (Gi = 1) to remove biases associated
with a common time trend unrelated to the
intervention.
We can estimate τ DID simply using least
squares methods on the regression function
for the observed outcome,
Yi = α + β1 · Ti + γ 1 · Gi + τ DID · Wi + εi,
where the treatment indicator Wi is equal to
the interaction of the group and time indicators, Ii = Ti · Gi. Thus the treatment effect
is estimated through the coefficient on the
interaction between the indicators for the
second time period and the treatment group.
This leads to
__

__

__

__

ˆ DID = (​Y ​11 − Y ​
​ 10) − (​Y ​01 − ​Y ​00),
	​   τ​
__

​ i | Gi=g, Ti=t​ ​ ​Yi/Ngt is the average
where Y ​
​ gt = ∑
outcome among units in group g and time
period t.

+∑
​ ​​γg · 1[Gi = g] + εi
g=1

with separate parameters for each group and
time period, γg and β t, for g = 1, … , G and t
= 1, … , T, where the initial time period coefficient and first group coefficient have implicitly been normalized to zero. This model is
then combined with the additive model for
the treatment effect, Yi(1) = Yi(0) + τ DID,
implying that the parameters of this model
can still be estimated by ordinary least
squares based on the regression function
T

​ ​​ β t · 1[Ti = t]
(37) Yi = α + ∑
t=1

G

+∑
​ ​​​γg · 1[Gi = g] + τ DID · Ii + εi,
g=1

where Ii is now an indicator for unit i being
in a group and time period that was exposed
to the treatment.
This model with more than two time
periods, or more than two groups, or both,
imposes testable restrictions on the data.
For example, if group g1 and g2 are both not
exposed to the treatment in periods t1 and t2,
under this model the double difference
__

__

__

__

​​ g​ 2,t1​) − (​​Y ​g​ 1,t2​− ​​Y ​g​ 1,t1​),
(​​Y ​g​ 2,t2​− Y ​
should estimate zero, which can be tested
using conventional methods—this possibility is exploited in the next subsection. In the
two-period, two-group setting there are no

Imbens and Wooldridge: Econometrics of Program Evaluation
testable restrictions on the four group/period
means.
6.5.3 Standard Errors in the Multiple
Group and Multiple Period Case
Recently there has been attention called
to the concern that ordinary least square
standard errors for the DID estimator may
not be accurate in the presence of correlations between outcomes within groups and
between time periods. This is a particular case of clustering where the regressor
of interest does not vary within clusters.
See Brent R. Moulton (1990), Moulton
and William C. Randolph (1989), and
Wooldridge (2002) for a general discussion. The specific problem has been analyzed recently by Donald and Lang (2007),
Bertrand, Duflo, and Mullainathan (2004),
and Hansen (2007a, 2007b).
The starting point of these analyses is a
particular structure on the error term εi:
εi = ​ηG​ i,Ti​+ ν i,
where ν i is an individual-level idiosyncratic
error term, and η gt is a group/time specific
component. The unit level error term ν i is
independent across all units, E[ν i · νj] = 0 if
i ≠ j and E[​νi​2​ ​] = ​σ​ν2​ ​. Now suppose we also
assume that η gt ~ (0, ​σ​η2​ ​), and all the η gt
are independent. Let us focus initially on
the two-group, two-time-period case. With a
large number
of units in each group and time
__
period, ​Y ​gt → α + β t + γg + 1g=1,t=1 · τ DID
+ η gt, so that
__
__
__
__
  ˆ
τ​
​ 00) → τ DID
​  DID = (​Y ​11 − ​Y ​10) − (​Y ​01 − Y ​

+ (η11 − η10) − (η 01 − η 00) ~ (τ DID, 4 · ​ση​2​ ​).
Thus, in this case with two groups and two
time periods, the conventional DID estimator is not consistent. In fact, no consistent estimator exists because there is no
way to eliminate the influence of the four

69

­unobserved components η gt. In this twogroup, two-time-period case the problem is
even worse than the absence of a consistent
estimator, because one cannot even establish whether there is a clustering problem:
the data are not informative about the value
of ​ση​2​ ​. If we have data from more than two
groups or from more than two time periods,
we can typically estimate ​σ​η2​ ​, and thus, at
least under the normality and independence
assumptions for η gt, construct confidence
intervals for τ DID. Consider, for example, the
case with three groups, and two time periods. If groups Gi = 0, 1 are__both not
__ treated
__
Y ​
−
Y ​
​ 10) − (​Y ​01
in the
second
period,
then
(​
11
__
− ​Y ​00) ~ (0, 4 · ​σ​η2​ ​), which can be used to
obtain an unbiased estimator for ​ση​2​ ​. See
Donald and Lang (2007) for details.
Bertrand, Duflo, and Mullainathan (2004)
and Hansen (2007a, 2007b) focus on the case
with multiple (more than two) time periods. In that case we may wish to relax the
assumption that the η gt are independent
over time. Note that with data from only two
time periods there is no information in the
data that allows one to establish the absence
of independence over time. The typical generalization is to allow for a autoregressive
structure on the η gt, for example,
η gt = α · η gt−1 + ωgt,
with a serially uncorrelated ωgt. More generally, with T time periods, one can allow for an
autoregressive process of order T − 2. Using
simulations and real data ­calculations based
on data for fifty states and multiple time
periods, Bertrand, Duflo, and Mullainathan
(2004) show that corrections to the conventional standard errors taking into account the
clustering and autoregressive structure make
a substantial difference. Hansen (2007a,
2007b) provides additional large sample
results under sequences where the number
of time periods increases with the sample
size.

Journal of Economic Literature, Vol. XLVII (March 2009)

70
6.5.4 Panel Data

Now suppose we have panel data, in the
two period, two group case. Here we have
N individuals, indexed i = 1, … , N, for whom
we observe (Gi, Yi0, Yi1, Xi0, Xi1), where Gi is,
as before, group membership, Xit is the covariate value for unit i at time t, and Yit is the
outcome for unit i at time t.
One option is to proceed with estimation
exactly as before, essentially ignoring the fact
that the observations in different time periods come from the same unit. We can now
interpret the estimator as the ordinary least
squares estimator based on the regression
function for the difference outcomes:
Yi1 − Yi0 = β + τ DID · Gi + εi,
which leads to the double difference
__
__as the
  ˆ
:
​ 
τ​
=
(​
Y ​
−
​
Y ​10) −
estimator
for
τ
DID
DID
11
__
__
(​Y ​01 − ​Y ​00). This estimator is identical to that
discussed in the context of repeated crosssections, and so does not exploit directly the
panel nature of the data.
A second, and very different, approach
with panel data, which does exploit the specific features of the panel data, would be to
assume unconfoundedness given lagged outcomes. Let us look at the differences between
these two approaches in a simple setting,
without covariates, and assuming linearity.
In that case the DID approach suggests the
regression of Yi1 − Yi0 on the group indicaˆ DID. The unconfoundedness
tor, leading to ​   τ​
assumption would suggest the regression of
the difference Yi1 − Yi0 on the group indicator and the lagged outcome Yi0:
Yi1 − Yi0 = β + τunconf · Gi + δ · Yi0 + εi.
While it appears that the analysis based
on unconfoundedness is necessarily less
restrictive because it allows a free coefficient in Yi0, this is not the case. The DID
assumption implies that adjusting for lagged

o­ utcomes actually compromises the comparison because Yi0 may in fact be correlated
with εi. In the end, the two approaches make
fundamentally different assumptions. One
needs to choose between them based on
substantive knowledge. When the estimated
coefficient on the lagged outcome is close
to zero, obviously there will be little difference between the point estimates. In addition, using the formula for omitted variable
bias in least squares estimation, the results
will be very similar if the average outcomes
in the treatment and control groups are similar in the first period. Finally, note that in
the repeated cross-section case the choice
between the DID and unconfoundedness
approaches did not arise because the unconfoundedness approach is not feasible: it is not
possible to adjust for lagged outcomes when
we do not have the same units available in
both periods.
As a practical matter, the DID approach
appears less attractive than the unconfoundedness-based approach in the context of panel
data. It is difficult to see how making treated
and control units comparable on lagged outcomes will make the causal interpretation of
their difference less credible, as suggested by
the DID assumptions.
6.5.5 The Changes-in-Changes Model
Now we return to the setting with two
groups, two time periods, and repeated crosssections. Athey and Imbens (2006) generalize the standard model in several ways. They
relax the additive linear model by assuming
that, in the absence of the intervention, the
outcomes satisfy
(38)

Yi(0) = h0(Ui, Ti),

with h0(u, t) increasing in u. The random variable Ui represents all unobservable characteristics of individual i, and (38) incorporates
the idea that the outcome of an individual
with Ui = u will be the same in a given time

Imbens and Wooldridge: Econometrics of Program Evaluation
period, irrespective of the group membership. The distribution of Ui is allowed to
vary across groups, but not over time within
groups, so that Ui   ǁ   Ti | Gi. Athey and Imbens
call the resulting model the changes-inchanges (CIC) model.
The standard DID model in (33) adds
three additional assumptions to the CIC
model, namely
(39)

Ui − E[Ui | Gi]   ǁ   Gi (additivity)

(40)

h0(u, t) = ϕ(u + δ · t),
(single index model)

for a strictly increasing function ϕ( · ), and
(41) 	 ϕ( · ) is the identity function.
(identity transformation).
In the CIC extension, the treatment
group’s distribution of unobservables may be
different from that of the control group in
arbitrary ways. In the absence of treatment,
all differences between the two groups can
be interpreted as coming from differences
in the conditional distribution of U given G.
The model further requires that the changes
over time in the distribution of each group’s
outcome (in the absence of treatment) arise
solely from the fact that h0(u, 0) differs from
h0(u, 1), that is, the relation between unobservables and outcomes changes over time.
Like the standard model, the Athey–Imbens
approach does not rely on tracking indivi­
duals over time. Although the distribution
of Ui is assumed not to change over time
within groups, the model does not make
any assumptions about whether a particular individual has the same realization Ui in
each period. Thus, the estimators derived by
Athey and Imbens will be the same whether
one observes a panel of individuals over time
or a repeated cross section. Just as in the
standard DID approach, if one only wishes
to estimate the effect of the intervention on

71

the treatment group, no assumptions are
required about how the intervention affects
outcomes.
The average effect of the treatment for
the second period treatment group is τcic
= E[Yi(1) − Yi(0) | Gi = 1, Ti = 1]. Because
the first term of this expression is equal to
E[Yi(1) | Gi = 1, Ti = 1] = E[Yi | Gi = 1, Ti =
1], it can be estimated directly from the data.
The difficulty is in estimating the second
term. Under the assumptions of monotonicity
of h0(u, t) in u, and conditional independence
of Ti and Ui given Gi, Athey and Imbens
show that in fact the full distribution of Y(0)
given Gi = Ti = 1 is identified through the
equality
​ ​Y10​(​FY​−1
​ ​(​FY​ 01​(y))),
(42)	​F​Y11​(y) = F
00
where ​F​Ygt​(y) denotes the distribution ­function
of Yi given Gi = g and Ti = t. The expected
outcome for the second period treatment
group under the control treatment is
−1
​ ​(F00(Yi10))].
E[Yi(0) | Gi = 1, Ti = 1] = E[​F​01

To analyze the counterfactual effect of the
intervention on the control group, Athey and
Imbens assume that, in the presence of the
intervention,
Yi(1) = h1(Ui, Ti)
for some function h1(u, t) that is increasing
in u. That is, the effect of the treatment at a
given time is the same for individuals with
the same Ui = u, irrespective of the group.
No further assumptions are required on
the functional form of h1, so the treatment
effect, equal to h1(u, 1) − h0(u, 1) for individuals with unobserved component u, can
differ across individuals. Because the distribution of the unobserved component U can
vary across groups, the average return to the
policy intervention can vary across groups as
well.

Journal of Economic Literature, Vol. XLVII (March 2009)

72

6.5.6 The Abadie–Diamond–Hainmueller
Artificial Control Group Approach
Abadie, Diamond, and Hainmueller
(2007) develop a very interesting alternative
approach to the setting with multiple control
groups. See also Abadie and Gardeazabal
(2003). Here we discuss a simple version of
their approach, with T + 1 time periods,
and G + 1 groups, one treated in the final
period, and G not treated in either period.
The Abadie–Diamond–Hainmueller idea is
to construct an artificial control group that
is more similar to the treatment group in the
initial period than any of the control groups
on their own. Let Gi = G denote the treated
group, and Gi = 0, … , G − 1 denote the G
control groups. The outcome for the final
period treatment group in the absence of the
treatment will be estimated as a weighted
average of period T outcomes in the G control groups,
   

ˆ [Y (0) | T = T, G = G] =
	​ E​
i
i
i
__

G−1

​ gT,
​∑ ​​ ​λ g · Y ​

g=0

G−1
with weights λ g satisfying ​∑ g=0​​ ​ λ g = 1, and
λ g ≥ 0. The weights are chosen to make the
weighted control group resemble the treatment group prior to the treatment. That is,
the weights λ g are chosen to minimize the
difference between the treatment group and
the weighted average of the control groups
prior to the treatment, namely,

ǁ

__

G−1

__

​ g0
	​Y ​G0 − ​∑ ​​λ g · Y ​
g=0

__

⋮

G−1

__

ǁ

	​Y ​G,T−1 − ​∑ ​​λ g · Y ​
​ g,T−1 ,
g=0

where ǁ · ǁ denotes a measure of distance.
One can also add group level covariates to
the criterion to determine the weights. These

group-level covariates may be averages of
individual level covariates, or quantiles of the
distribution of within group covariates. The
idea is that the future path of the artificial
control group, consisting of the λ-weighted
average of all the control groups, mimics the
path that would have been observed in the
treatment group in the absence of the treatment. Applications in Abadie, Diamond,
and Hainmueller (2007) to estimation of the
effect of smoking legislation in California and
the effect of reunification on West Germany
are very promising.
7.

Multivalued and Continuous
Treatments

Most of the recent econometric program
evaluation literature has focused on the case
with a binary treatment. As a result this case
is now understood much better than it was
a decade or two ago. However, much less is
known about settings with multivalued, discrete or continuous treatments. Such cases
are common in practice. Social programs are
rarely homogenous. Typically individuals are
assigned to various activities and regimes,
often sequentially, and tailored to their specific circumstances and characteristics.
To provide some insight into the issues
arising in settings with multivalued treatments we discuss in this review five separate cases. First, the simplest setting where
the treatment is discrete and one is willing
to assume unconfoundedness of the treatment assignment. In that case straightforward extensions of the binary treatment case
can be used to obtain estimates and inferences for causal effects. Second, we look at
the case with a continuous treatment under
unconfoundedness. In that case, the definition of the propensity score requires some
modification but many of the insights from
the binary treatment case still carry over.
Third, we look at the case where units can be
exposed to a sequence of binary treatments.

Imbens and Wooldridge: Econometrics of Program Evaluation
For example, an individual may remain in a
training program for a number of periods. In
each period the assignment to the program
is assumed to be unconfounded, given permanent characteristics and outcomes up to
that point. In the last two cases we briefly
discuss multivalued endogenous treatments.
In the fourth case, we look at settings with
a discrete multivalued treatment in the presence of endogeneity. We allow the treatment
to be continuous in the final case. The last
two cases tie in closely with the simultaneous equations literature, where, somewhat
separately from the program evaluation literature, there has been much recent work on
nonparametric identification and estimation.
Especially in the discrete case, many of the
results in this literature are negative in the
sense that, without unattractive restrictions
on heterogeneity or functional form, few
objects of interest are point-identified. Some
of the literature has turned toward establishing bounds. This is an area with much ongoing work and considerable scope for further
research.
7.1 Multivalued Discrete Treatments with
Unconfounded Treatment Assignment
If there are a few different levels of the
treatment, rather than just two, essentially all
of the methods discussed before go through
in the unconfoundedness case. Suppose, for
example, that the treatment can be one of three
levels, say Wi ∈ {0, 1, 2}. In order to estimate
the effect of treatment level 2 relative to treatment level 1, one can simply put aside the data
for units exposed to treatment level 0 if one
is willing to assume unconfoundedness. More
specifically, one can estimate the average outcome for each treatment level conditional on
the covariates, E[Yi(w) | Xi = x], using data on
units exposed to treatment level w, and average these over the (estimated)
marginal dis   ˆ
tribution of the covariates, F​
​  X(x). In practice,
the overlap assumption may more likely to be
violated with more than two treatments. For

73

example, with three ­treatments, it may be
that no units are exposed to treatment level 2
if Xi is in some subset of the covariate space.
The insights from the binary case directly
extend to this multiple (but few) treatment
case. If the number of treatments is relatively
large, one may wish to smooth across treatment levels in order to improve precision of
the inferences.
7.2 Continuous Treatments with
Unconfounded Treatment Assignment
In the case where the treatment taking
on many values, Imbens (2000), Lechner
(2001, 2004), Hirano and Imbens (2004),
and Carlos A. Flores (2005) extended some
of the propensity score methodology under
unconfoundedness. The key maintained
assumption is that adjusting for pre-treatment differences removes all biases, and thus
solves the problem of drawing causal inferences. This is formalized by using the concept of weak unconfoundedness, introduced
by Imbens (2000). Assignment to treatment
Wi is weakly unconfounded, given pre-treatment variables Xi, if
Wi   ǁ   Yi(w) | Xi,
for all w. Compare this to the stronger
assumption made by Rosenbaum and Rubin
(1983b) in the binary case:
Wi   ǁ   (Yi(0), Yi(1)) | Xi,
which requires the treatment Wi to be
independent of the entire set of potential
outcomes. Instead, weak unconfoundedness requires only pairwise independence
of the treatment with each of the potential
outcomes. A similar assumption is used in
Robins and Rotnitzky (1995). The definition
of weak unconfoundedness is also similar
to that of “missing at random” (Rubin 1976,
1987; Roderick J. A. Little and Rubin 1987)
in the missing data literature.

74

Journal of Economic Literature, Vol. XLVII (March 2009)

Although in substantive terms the weak
unconfoundedness assumption is not very
different from the assumption used by
Rosenbaum and Rubin (1983b), it is important
that one does not need the stronger assumption to validate estimation of the expected
value of Yi(w) by adjusting for Xi: under
weak unconfoundedness, we have E[Yi(w) | Xi]
= E[Yi(w) | Wi = w, Xi] = E[Yi | Wi = w, Xi],
and expected outcomes can then be estimated by averaging these conditional means:
E[Yi(w)] = E[E[Yi(w) | Xi]]. In practice, it can
be difficult to estimate E[Yi(w)] in this manner when the dimension of Xi is large, or if
w takes on many values, because the first
step requires estimation of the expectation
of Yi(w) given the treatment level and all pretreatment variables. It was this difficulty that
motivated Rosenbaum and Rubin (1983b) to
develop the propensity score methodology.
Imbens (2000) introduces the generalized propensity score for the multiple treatment case. It is the conditional probability of
receiving a particular level of the treatment
given the pretreatment variables:
r(w, x) ≡ pr(Wi = w | Xi = x).
In the continuous case, where, say, Wi
takes values in the unit interval, r (w, x)
= F W | X(w | x). Suppose assignment to treatment Wi is weakly unconfounded given pretreatment variables Xi. Then, by the same
argument as in the binary treatment case,
assignment is weakly unconfounded given
the generalized propensity score, as δ → 0,
1{w − δ ≤ Wi ≤ w + δ}   ǁ   Yi(w) | r(w, Xi),
for all w. This is the point where using the
weak form of the unconfoundedness assumption is important. There is, in general, no scalar function of the covariates such that the
level of the treatment Wi is independent of
the set of potential outcomes {Yi(w)}w∈[0,1],
unless additional structure is imposed on

the assignment mechanism; see for example,
Marshall M. Joffe and Rosenbaum (1999).
Because weak unconfoundedness given all
pretreatment variables implies weak unconfoundedness given the generalized propensity score, one can estimate average outcomes
by conditioning solely on the generalized
propensity score. If assignment to treatment
is weakly unconfounded given pretreatment
variables X, then two results follow. First, for
all w,
   β(w, r) ≡ E[Yi(w) | r(w, Xi) = r]
= E[Yi | Wi = w, r(Wi, Xi) = r],
which can be estimated using data on Yi, Wi,
and r (Wi, Xi). Second, the average outcome
given a particular level of the treatment,
E[Yi(w)], can be estimated by appropriately
averaging β(w, r):
E[Yi(w)] = E[β(w, r (w, Xi))].
As with the implementation of the binary
treatment propensity score methodology, the
implementation of the generalized propensity
score method consists of three steps. In the
first step the score r (w, x) is estimated. With
a binary treatment the standard approach
(Rosenbaum and Rubin 1984; Rosenbaum
1995) is to estimate the propensity score
using a logistic regression. More generally, if
the treatments correspond to ordered levels
of a treatment, such as the dose of a drug or
the time over which a treatment is applied,
one may wish to impose smoothness of the
score in w. For continuous Wi, Hirano and
Imbens (2004) use a lognormal distribution.
In the second step, the conditional expectation β(w, r) = E[Yi | Wi = w, r(Wi, Xi) = r] is
estimated. Again, the implementation may be
different in the case where the levels of the
treatment are qualitatively distinct than in
the case where smoothness of the conditional
expectation function in w is ­appropriate.

Imbens and Wooldridge: Econometrics of Program Evaluation
Here, some form of linear or nonlinear
regression may be used. In the third step the
average response at treatment level w is estimated as the average    of the estimated conditional expectation, β​
​ ˆ (w, r (w, Xi)), averaged
over the distribution of the pretreatment
variables, X1, … , X N. Note that to get the
average E[Yi(w)], the second argument in the
conditional expectation β(w, r) is evaluated at
r (w, Xi), not at r (Wi, Xi).
7.2.1 Dynamic Treatments with
Unconfounded Treatment Assignment
Multiple-valued treatments can arise
because at any point in time individuals
can be assigned to multiple different treatment arms, or because they can be assigned
sequentially to different treatments. Gill and
Robins (2001) analyze this case, where they
assume that at any point in time an unconfoundedness assumption holds. Lechner
and Miquel (2005) (see also Lechner 1999,
and Lechner, Miquel, and Conny Wunsch
2004) study a related case, where again a
sequential unconfoundedness assumption is
maintained to identify the average effects
of interest. Abbring and Gerard J. van den
Berg (2003) study settings with duration
data. These methods hold great promise but,
until now, there have been few substantive
applications.
7.3 Multivalued Discrete Endogenous
Treatments
In settings with general heterogeneity in
the effects of the treatment, the case with
more than two treatment levels is considerably more challenging than the binary case.
There are few studies investigating identification in these settings. Angrist and Imbens
(1995) and Angrist, Kathryn Graddy and
Imbens (2000) study the interpretation of
the standard instrumental variable estimand,
the ratio of the covariances of outcome and
instrument and treatment and instrument.
They show that in general, with a valid

75

instrument, the instrumental variables estimand can still be interpreted as an average
causal effect, but with a complicated weighting scheme. There are essentially two levels
of averaging going on. First, at each level
of the treatment we can only get the average effect of a unit increase in the treatment
for compliers at that level. In addition, there
is averaging over all levels of the treatment,
with the weights equal to the proportion of
compliers at that level.
Imbens (2007) studies, in more detail,
the case where the endogenous treatment
takes on three values and shows the limits to
identification in the case with heterogenous
treatment effects.
7.4 Continuous Endogenous Treatments
Perhaps surprisingly, there are many
more results for the case with continuous
endogenous treatments than for the discrete
case that do not impose restrictive assumptions. Much of the focus has been on triangular ­systems, with a single unobserved
component of the equation determining the
treatment:
Wi = h(Zi, η i),

where η i is scalar, and an essentially unrestricted outcome equation:
Yi = g(Wi, εi),

where εi may be a vector. Blundell and James
L. Powell (2003, 2004), Chernozhukov and
Hansen (2005), Imbens and Newey (forthcoming), and Andrew Chesher (2003) study
various versions of this setup. Imbens and
Newey (forthcoming) show that if h(z, η) is
strictly monotone in η, then one can identify average effects of the treatment subject
to support conditions on the instrument.
They suggest a control function approach
to estimation. First η is normalized to have
a uniform distribution on [0, 1] (e.g., Rosa
L. Matzkin 2003). Then η i is estimated

Journal of Economic Literature, Vol. XLVII (March 2009)

76
  

as   η​
​ ˆ i = F​
​ ˆ W | Z (Wi | Zi). In the second stage, Yi
​ ˆ i.
is regressed nonparametrically on Xi and   η​
Chesher (2003) studies local versions of this
problem.
When the treatment equation has an additive form, say Wi = h1(Zi) + η i, where η i is
independent of Zi, Blundell and Powell (2003,
2004) derive nonparametric control function
methods for estimating the average structural function, E[g(w, εi)]. The general   idea
ˆ i = Wi − h​
​ ˆ 1(Zi)
is to first obtain residuals, ​   η​
from a nonparametric regression. Next, a
​ ˆ i
nonparametric regression of Yi on Wi and   η​
is used to recover m(w, η) = E(Yi | Wi = w, η i
= η). Blundell and Powell show that the average structural function is generally identified
as E[m(w, η i)], which is easily estimated by
ˆ i across the sample.
averaging out ​   η​
8.

Conclusion

Over the last two decades, there has
been a proliferation of the literature on program evaluation. This includes ­theoretical
­econometrics work, as well as empirical
work. Important features of the modern literature are the convergence of the statistical
and econometric literatures, with the Rubin
potential outcomes framework now the dominant framework. The modern literature has
stressed the importance of relaxing functional form and distributional assumptions,
and has allowed for general heterogeneity in
the effects of the treatment. This has led to
renewed interest in identification questions,
leading to unusual and controversial estimands such as the local average treatment
effect (Imbens and Angrist 1994), as well
as to the literature on partial identification
(Manski 1990). It has also borrowed heavily from the semiparametric literature, using
both efficiency bound results (Hahn 1998)
and methods for inference based on series
and kernel estimation (Newey 1994a, 1994b).
It has by now matured to the point that it is
of great use for practitioners.

References
Abadie, Alberto. 2002. “Bootstrap Tests of Distributional Treatment Effects in Instrumental Variable
Models.” Journal of the American Statistical Association, 97(457): 284–92.
Abadie, Alberto. 2003. “Semiparametric Instrumental
Variable Estimation of Treatment Response Models.” Journal of Econometrics, 113(2): 231–63.
Abadie, Alberto. 2005. “Semiparametric Differencein-Differences Estimators.” Review of Economic
Studies, 72(1): 1–19.
Abadie, Alberto, Joshua D. Angrist, and Guido W.
Imbens. 2002. “Instrumental Variables Estimates of
the Effect of Subsidized Training on the Quantiles
of Trainee Earnings.” Econometrica, 70(1): 91–117.
Abadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2007. “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s
Tobacco Control Program.” National Bureau of Economic Research Working Paper 12831.
Abadie, Alberto, David Drukker, Jane Leber Herr, and
Guido W. Imbens. 2004. “Implementing Matching
Estimators for Average Treatment Effects in Stata.”
Stata Journal, 4(3): 290–311.
Abadie, Alberto, and Javier Gardeazabal. 2003. “The
Economic Costs of Conflict: A Case Study of the
Basque Country.” American Economic Review,
93(1): 113–32.
Abadie, Alberto, and Guido W. Imbens. 2006. “Large
Sample Properties of Matching Estimators for
Average Treatment Effects.” Econometrica, 74(1):
235–67.
Abadie, Alberto, and Guido W. Imbens. 2008a. “Bias
Corrected Matching Estimators for Average Treatment Effects.” Unpublished.
Abadie, Alberto, and Guido W. Imbens. Forthcoming.
“Estimation of the Conditional Variance in Paired
Experiments. Annales d’Economie et de Statistique.
Abadie, Alberto, and Guido W. Imbens.  2008b. “On
the Failure of the Bootstrap for Matching Estimators.” Econometrica, 76(6): 1537–57.
Abbring, Jaap H., and James J. Heckman. 2007.
“Econometric Evaluation of Social Programs, Part
III: Distributional Treatment Effects, Dynamic
Treatment Effects, Dynamic Discrete Choice, and
General Equilibrium Policy Evaluation.” In Handbook of Econometrics, Volume 6B, ed. James J.
Heckman and Edward E. Leamer, 5145–5303.
Amsterdam; New York and Oxford: Elsevier Science, North-Holland.
Abbring, Jaap H., and Gerard J. van den Berg. 2003.
“The Nonparametric Identification of Treatment
Effects in Duration Models.” Econometrica, 71(5):
1491–1517.
Andrews, Donald W. K., and Gustavo Soares. 2007.
“Inference for Parameters Defined By Moment
Inequalities Using Generalized Moment Selection.”
Cowles Foundation Discussion Paper 1631.
Angrist, Joshua D. 1990. “Lifetime Earnings and the
Vietnam Era Draft Lottery: Evidence from Social

Imbens and Wooldridge: Econometrics of Program Evaluation
Security Administrative Records.” American Economic Review, 80(3): 313–36.
Angrist, Joshua D. 1998. “Estimating the Labor Market
Impact of Voluntary Military Service Using Social
Security Data on Military Applicants.” Econometrica, 66(2): 249–88.
Angrist, Joshua D. 2004. “Treatment Effect Heterogeneity in Theory and Practice.” Economic Journal,
114(494): C52–83.
Angrist, Joshua D., Eric Bettinger, and Michael Kremer. 2006. “Long-Term Educational Consequences
of Secondary School Vouchers: Evidence from
Administrative Records in Colombia.” American
Economic Review, 96(3): 847–62.
Angrist, Joshua D., Kathryn Graddy, and Guido W.
Imbens. 2000. “The Interpretation of Instrumental Variables Estimators in Simultaneous Equations
Models with an Application to the Demand for Fish.”
Review of Economic Studies, 67(3): 499–527.
Angrist, Joshua D., and Jinyong Hahn. 2004. “When to
Control for Covariates? Panel Asymptotics for Estimates of Treatment Effects.” Review of Economics
and Statistics, 86(1): 58–72.
Angrist, Joshua D., and Guido W. Imbens. 1995. “TwoStage Least Squares Estimation of Average Causal
Effects in Models with Variable Treatment Intensity.” Journal of the American Statistical Association, 90(430): 431–42.
Angrist, Joshua D., Guido W. Imbens, and Donald B.
Rubin. 1996. “Identification of Causal Effects Using
Instrumental Variables.” Journal of the American
Statistical Association, 91(434): 444–55.
Angrist, Joshua D., and Alan B. Krueger. 1999. “Empirical Strategies in Labor Economics.” In Handbook of
Labor Economics, Volume 3A, ed. Orley Ashenfelter
and David Card, 1277–1366. Amsterdam; New York
and Oxford: Elsevier Science, North-Holland.
Angrist, Joshua D., and Kevin Lang. 2004. “Does
School Integration Generate Peer Effects? Evidence
from Boston’s Metco Program.” American Economic
Review, 94(5): 1613–34.
Angrist, Joshua D., and Victor Lavy. 1999. “Using Maimonides’ Rule to Estimate the Effect of Class Size
on Scholastic Achievement.” Quarterly Journal of
Economics, 114(2): 533–75.
Angrist, Joshua D., and Jörn-Steffen Pischke. 2009.
Mostly Harmless Econometrics: An Empiricist’s
Companion. Princeton: Princeton University Press.
Ashenfelter, Orley. 1978. “Estimating the Effect of
Training Programs on Earnings.” Review of Economics and Statistics, 6(1): 47–57.
Ashenfelter, Orley, and David Card. 1985. “Using the
Longitudinal Structure of Earnings to Estimate the
Effect of Training Programs.” Review of Economics
and Statistics, 67(4): 648–60.
Athey, Susan, and Guido W. Imbens. 2006. “Identification and Inference in Nonlinear Difference-in-Differences Models.” Econometrica, 74(2): 431–97.
Athey, Susan, and Scott Stern. 1998. “An Empirical
Framework for Testing Theories About Complimentarity in Organizational Design.” National Bureau of

77

Economic Research Working Paper 6600.
Attanasio, Orazio, Costas Meghir, and Ana Santiago.
2005. “Education Choices in Mexico: Using a Structural Model and a Randomized Experiment to Evaluate Progresa.” Institute for Fiscal Studies Centre
for the Evaluation of Development Policies Working
Paper EWP05/01.
Austin, Peter C. 2008a. “A Critical Appraisal of Propensity-Score Matching in the Medical Literature
between 1996 and 2003.” Statistics in Medicine,
27(12): 2037–49.
Austin, Peter C. 2008b. “Discussion of ‘A Critical
Appraisal of Propensity-Score Matching in the Medical Literature between 1996 and 2003’: Rejoinder.”
Statistics in Medicine, 27(12): 2066–69.
Balke, Alexander, and Judea Pearl. 1994. “Nonparametric Bounds of Causal Effects from Partial Compliance Data.” University of California Los Angeles
Cognitive Systems Laboratory Technical Report
R-199.
Banerjee, Abhijit V., Shawn Cole, Esther Duflo, and
Leigh Linden. 2007. “Remedying Education: Evidence from Two Randomized Experiments in India.”
Quarterly Journal of Economics, 122(3): 1235–64.
Barnow, Burt S., Glend G. Cain, and Arthur S. Goldberger. 1980. “Issues in the Analysis of Selectivity
Bias.” In Evaluation Studies, Volume 5, ed. Ernst W.
Stromsdorfer and George Farkas, 43–59. San Francisco: Sage.
Becker, Sascha O., and Andrea Ichino. 2002. “Estimation of Average Treatment Effects Based on Propensity Scores.” Stata Journal, 2(4): 358–77.
Behncke, Stefanie, Markus Frölich, and Michael Lechner. 2006. “Statistical Assistance for Programme
Selection—For a Better Targeting of Active Labour
Market Policies in Switzerland.” University of St.
Gallen Department of Economics Discussion Paper
2006-09.
Beresteanu, Arie, and Francesca Molinari. 2006.
“Asymptotic Properties for a Class of Partially Identified Models.” Institute for Fiscal Studies Centre
for Microdata Methods and Practice Working Paper
CWP10/06.
Bertrand, Marianne, Esther Duflo, and Sendhil Mullainathan. 2004. “How Much Should We Trust
Differences-in-Differences Estimates?” Quarterly
Journal of Economics, 119(1): 249–75.
Bertrand, Marianne, and Sendhil Mullainathan. 2004.
“Are Emily and Greg More Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review,
94(4): 991–1013.
Besley, Timothy, and Anne C. Case. 2000. “Unnatural Experiments? Estimating the Incidence of
Endogenous Policies.” Economic Journal, 110(467):
F672–94.
Bierens, Herman J. 1987. “Kernel Estimators of Regression Functions.” In Advances in Econometrics: Fifth
World Congress, Volume 1, ed. Truman F. Bewley, 99–144. Cambridge and New York: ­Cambridge
­University Press.

78

Journal of Economic Literature, Vol. XLVII (March 2009)

Bitler, Marianne, Jonah Gelbach, and Hilary Hoynes.
2006. “What Mean Impacts Miss: Distributional
Effects of Welfare Reform Experiments.” American
Economic Review, 96(4): 988–1012.
Björklund, Anders, and Robert Moffitt. 1987. “The
Estimation of Wage Gains and Welfare Gains in
Self-Selection.” Review of Economics and Statistics,
69(1): 42–49.
Black, Sandra E. 1999. “Do Better Schools Matter?
Parental Valuation of Elementary Education.” Quarterly Journal of Economics, 114(2): 577–99.
Bloom, Howard S. 1984. “Accounting for No-Shows
in Experimental Evaluation Designs.” Evaluation
Review, 8(2): 225–46.
Bloom, Howard S., ed. 2005. Learning More from
Social Experiments: Evolving Analytic Approaches.
New York: Russell Sage Foundation.
Blundell, Richard, and Monica Costa Dias. 2002.
“Alternative Approaches to Evaluation in Empirical
Microeconomics.” Institute for Fiscal Studies Centre for Microdata Methods and Practice Working
Paper CWP10/02.
Blundell, Richard, Monica Costa Dias, Costas Meghir,
and John Van Reenen. 2001. “Evaluating the
Employment Impact of a Mandatory Job Search
Assistance Program.” Institute for Fiscal Studies
Working Paper WP01/20.
Blundell, Richard, Alan Duncan, and Costas Meghir.
1998. “Estimating Labor Supply Responses Using
Tax Reforms.” Econometrica, 66(4): 827–61.
Blundell, Richard, Amanda Gosling, Hidehiko
Ichimura, and Costas Meghir. 2004. “Changes in the
Distribution of Male and Female Wages Accounting
for Employment Composition Using Bounds.” Institute for Fiscal Studies Working Paper W04/25.
Blundell, Richard, and Thomas MaCurdy. 1999. “Labor
Supply: A Review of Alternative Approaches.” In
Handbook of Labor Economics, Volume 3A, ed.
Orley Ashenfelter and David Card, 1559–1695.
Amsterdam; New York and Oxford: Elsevier Science, North-Holland.
Blundell, Richard, and James L. Powell. 2003. “Endogeneity in Nonparametric and Semiparametric
Regression Models.” In Advances in Economics and
Econometrics: Theory and Applications, Eighth
World Congress, Volume 2, ed. Mathias Dewatripont, Lars Peter Hansen, and Stephen J. Turnovsky,
312–57. Cambridge and New York: Cambridge University Press.
Blundell, Richard, and James L. Powell. 2004. “Endogeneity in Semiparametric Binary Response Models.” Review of Economic Studies, 71(3): 655–79.
Brock, William, and Steven N. Durlauf. 2000. “Interactions-Based Models.” National Bureau of Economic
Research Technical Working Paper 258.
Bruhn, Miriam, and David McKenzie. 2008. “In Pursuit of Balance: Randomization in Practice in Development Field Experiments.” World Bank Policy
Research Working Paper 4752.
Burtless, Gary. 1995. “The Case for Randomized Field
Trials in Economic and Policy Research.” Journal of

Economic Perspectives, 9(2): 63–84.
Busso, Matias, John DiNardo, and Justin McCrary.
2008. “Finite Sample Properties of Semiparametric Estimators of Average Treatment Effects.”
Unpublished.
Caliendo, Marco. 2006. Microeconometric Evaluation
of Labour Market Policies. Heidelberg: Springer,
Physica-Verlag.
Cameron, A. Colin, and Pravin K. Trivedi. 2005.
Microeconometrics: Methods and Applications.
Cambridge and New York: Cambridge University
Press.
Canay, Ivan A. 2007. “EL Inference for Partially Identified Models: Large Deviations Optimally and
Bootstrap Validity.” Unpublished.
Card, David. 1990. “The Impact of the Mariel Boatlift
on the Miami Labor Market.” Industrial and Labor
Relations Review, 43(2): 245–57.
Card, David. 2001. “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.” Econometrica, 69(5): 1127–60.
Card, David, Carlos Dobkin, and Nicole Maestas.
2004. “The Impact of Nearly Universal Insurance
Coverage on Health Care Utilization and Health:
Evidence from Medicare.” National Bureau of Economic Research Working Paper 10365.
Card, David, and Dean R. Hyslop. 2005. “Estimating
the Effects of a Time-Limited Earnings Subsidy for
Welfare-Leavers.” Econometrica, 73(6): 1723–70.
Card, David, and Alan B. Krueger. 1993. “Trends in
Relative Black–White Earnings Revisited.” American Economic Review, 83(2): 85–91.
Card, David, and Alan B. Krueger. 1994. “Minimum
Wages and Employment: A Case Study of the FastFood Industry in New Jersey and Pennsylvania.”
American Economic Review, 84(4): 772–93.
Card, David, and Phillip B. Levine. 1994. “Unemployment Insurance Taxes and the Cyclical and Seasonal
Properties of Unemployment.” Journal of Public
Economics, 53(1): 1–29.
Card, David, Alexandre Mas, and Jesse Rothstein.
2007. “Tipping and the Dynamics of Segregation.”
National Bureau of Economic Research Working
Paper 13052.
Card, David, and Brian P. McCall. 1996. “Is Workers’
Compensation Covering Uninsured Medical Costs?
Evidence from the ‘Monday Effect.’” Industrial and
Labor Relations Review, 49(4): 690–706.
Card, David, and Philip K. Robins. 1996. “Do Financial Incentives Encourage Welfare Recipients to
Work? Evidence from a Randomized Evaluation of
the Self-Sufficiency Project.” National Bureau of
Economic Research Working Paper 5701.
Card, David, and Daniel G. Sullivan. 1988. “Measuring the Effect of Subsidized Training Programs on
Movements In and Out of Employment.” Econometrica, 56(3): 497–530.
Case, Anne C., and Lawrence F. Katz. 1991. “The
Company You Keep: The Effects of Family and
Neighborhood on Disadvantaged Youths.” National
Bureau of Economic Research Working Paper 3705.

Imbens and Wooldridge: Econometrics of Program Evaluation
Chamberlain, Gary. 1986. “Asymptotic Efficiency in
Semi-parametric Models with Censoring.” Journal
of Econometrics, 32(2): 189–218.
Chattopadhyay, Raghabendra, and Esther Duflo.
2004. “Women as Policy Makers: Evidence from a
Randomized Policy Experiment in India.” Econometrica, 72(5): 1409–43.
Chay, Kenneth Y., and Michael Greenstone. 2005.
“Does Air Quality Matter? Evidence from the Housing Market.” Journal of Political Economy, 113(2):
376–424.
Chen, Susan, and Wilbert van der Klaauw. 2008. “The
Work Disincentive Effects of the Disability Insurance Program in the 1990s.” Journal of Econometrics, 142(2): 757–84.
Chen, Xiaohong. 2007. “Large Sample Sieve Estimation of Semi-nonparametric Models.” In Handbook
of Econometrics, Volume 6B, ed. James J. Heckman
and Edward E. Leamer, 5549–5632. Amsterdam
and Oxford: Elsevier, North-Holland.
Chen, Xiaohong, Han Hong, and Alessandro Tarozzi.
2008. “Semiparametric Efficiency in GMM Models with Auxiliary Data.” Annals of Statistics, 36(2):
808–43.
Chernozhukov, Victor, and Christian B. Hansen.
2005. “An IV Model of Quantile Treatment Effects.”
Econometrica, 73(1): 245–61.
Chernozhukov, Victor, Han Hong, and Elie Tamer.
2007. “Estimation and Confidence Regions for
Parameter Sets in Econometric Models.” Econometrica, 75(5): 1243–84.
Chesher, Andrew. 2003. “Identification in Nonseparable Models.” Econometrica, 71(5): 1405–41.
Chetty, Raj, Adam Looney, and Kory Kroft. Forthcoming. “Salience and Taxation: Theory and Evidence.”
American Economic Review.
Cochran, William G. 1968. “The Effectiveness of
Adjustment by Subclassification in Removing Bias in
Observational Studies.” Biometrics, 24(2): 295–314.
Cochran, William G., and Donald B. Rubin. 1973.
“Controlling Bias in Observational Studies: A
Review.” Sankhya, 35(4): 417–46.
Cook, Thomas D. 2008. “‘Waiting for Life to Arrive’:
A History of the Regression–Discontinuity Design
in Psychology, Statistics and Economics.” Journal of
Econometrics, 142(2): 636–54.
Cook, Philip J., and George Tauchen. 1982. “The Effect
of Liquor Taxes on Heavy Drinking.” Bell Journal of
Economics, 13(2): 379–90.
Cook, Philip J., and George Tauchen. 1984. “The Effect
of Minimum Drinking Age Legislation on Youthful
Auto Fatalities, 1970–1977.” Journal of Legal Studies, 13(1): 169–90.
Crump, Richard K., V. Joseph Hotz, Guido W. Imbens,
and Oscar A. Mitnik.  2009. “Dealing with Limited Overlap in Estimation of Average ­Treatment
Effects.” Biometrika, 96:187–99.
Crump, Richard K., V. Joseph Hotz, Guido W. Imbens,
and Oscar A. Mitnik. 2008. “Nonparametric Tests
for Treatment Effect Heterogeneity.” Review of
­Economics and Statistics, 90(3): 389–405.

79

Davison, A. C., and D. V. Hinkley. 1997. Bootstrap
Methods and Their Application. Cambridge and
New York: Cambridge University Press.
Dehejia, Rajeev H. 2003. “Was There a Riverside
Miracle? A Hierarchical Framework for Evaluating
Programs with Grouped Data.” Journal of Business
and Economic Statistics, 21(1): 1–11.
Dehejia, Rajeev H. 2005a. “Practical Propensity Score
Matching: A Reply to Smith and Todd.” Journal of
Econometrics, 125(1–2): 355–64.
Dehejia, Rajeev H. 2005b. “Program Evaluation as a
Decision Problem.” Journal of Econometrics, 125(1–
2): 141–73.
Dehejia, Rajeev H., and Sadek Wahba. 1999. “Causal
Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs.” Journal
of the American Statistical Association, 94(448):
1053–62.
Diamond, Alexis, and Jasjeet S. Sekhon. 2008. “Genetic
Matching for Estimating Causal Effects: A General
Multivariate Matching Method for Achieving Balance in Observational Studies.” Unpublished.
DiNardo, John, and David S. Lee. 2004. “Economic
Impacts of New Unionization on Private Sector
Employers: 1984–2001.” Quarterly Journal of Economics, 119(4): 1383–1441.
Doksum, Kjell. 1974. “Empirical Probability Plots
and Statistical Inference for Nonlinear Models in
the Two-Sample Case.” Annals of Statistics, 2(2):
267–77.
Donald, Stephen G., and Kevin Lang. 2007. “Inference
with Difference-in-Differences and Other Panel
Data.” Review of Economics and Statistics, 89(2):
221–33.
Duflo, Esther. 2001. “Schooling and Labor Market
Consequences of School Construction in Indonesia: Evidence from an Unusual Policy Experiment.”
American Economic Review, 91(4): 795–813.
Duflo, Esther, William Gale, Jeffrey B. Liebman, Peter
Orszag, and Emmanuel Saez. 2006. “Saving Incentives for Low- and Middle-Income Families: Evidence from a Field Experiment with H&R Block.”
Quarterly Journal of Economics, 121(4): 1311–46.
Duflo, Esther, Rachel Glennerster, and Michael Kremer. 2008. “Using Randomization in Development
Economics Research: A Toolkit.” In Handbook of
Development Economics, Volume 4, ed. T. Paul
Schultz and John Strauss, 3895–3962. Amsterdam
and Oxford: Elsevier, North-Holland.
Duflo, Esther, and Rema Hanna. 2005. “Monitoring Works: Getting Teachers to Come to School.”
National Bureau of Economic Research Working
Paper 11880.
Duflo, Esther, and Emmanuel Saez. 2003. “The Role
of Information and Social Interactions in Retirement Plan Decisions: Evidence from a Randomized Experiment.” Quarterly Journal of Economics,
118(3): 815–42.
Efron, Bradley, and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. New York and
­London: Chapman and Hall.

80

Journal of Economic Literature, Vol. XLVII (March 2009)

Eissa, Nada, and Jeffrey B. Liebman. 1996. “Labor
Supply Response to the Earned Income Tax Credit.”
Quarterly Journal of Economics, 111(2): 605–37.
Engle, Robert F., David F. Hendry, and Jean-Francois
Richard. 1983. “Exogeneity.” Econometrica, 51(2):
277–304.
Fan, J., and I. Gijbels. 1996. Local Polynomial Modelling and Its Applications. London: Chapman and
Hall.
Ferraz, Claudio, and Frederico Finan. 2008. “Exposing
Corrupt Politicians: The Effects of brazil’s Publicly
Released Audits on Electoral Outcomes.” Quarterly
Journal of Economics, 123(2): 703–45.
Firpo, Sergio. 2007. “Efficient Semiparametric Estimation of Quantile Treatment Effects.” Econometrica, 75(1): 259–76.
Fisher, Ronald A. 1935. The Design of Experiments,
First edition. London: Oliver and Boyd.
Flores, Carlos A. 2005. “Estimation of Dose-Response
Functions and Optimal Doses with a Continuous
Treatment.” Unpublished.
Fraker, Thomas, and Rebecca Maynard. 1987. “The
Adequacy of Comparison Group Designs for Evaluations of Employment-Related Programs.” Journal
of Human Resources, 22(2): 194–227.
Friedlander, Daniel, and Judith M. Gueron. 1992. “Are
High-Cost Services More Effective than Low-Cost
Services?” In Evaluating Welfare Training Programs, ed. Charles F. Manski and Irwin Garfinkel,
143–98. Cambridge and London: Harvard University Press.
Friedlander, Daniel, and Philip K. Robins. 1995.
“Evaluating Program Evaluations: New Evidence
on Commonly Used Nonexperimental Methods.”
American Economic Review, 85(4): 923–37.
Frölich, Markus. 2004a. “Finite-Sample Properties of
Propensity-Score Matching and Weighting Estimators.” Review of Economics and Statistics, 86(1):
77–90.
Frölich, Markus. 2004b. “A Note on the Role of the
Propensity Score for Estimating Average Treatment
Effects.” Econometric Reviews, 23(2): 167–74.
Gill, Richard D., and James M. Robins. 2001. “Causal
Inference for Complex Longitudinal Data: The
Continuous Case.” Annals of Statistics, 29(6):
1785–1811.
Glaeser, Edward L., Bruce Sacerdote, and Jose A.
Scheinkman. 1996. “Crime and Social Interactions.”
Quarterly Journal of Economics, 111(2): 507–48.
Goldberger, Arthur S. 1972a. “Selection Bias in Evaluating Treatment Effects: Some Formal Illustrations.”
Unpublished.
Goldberger, Arthur S. 1972b. “Selection Bias in Evaluating Treatment Effects: The Case of Interaction.”
Unpublished.
Gourieroux, C., A. Monfort, and A. Trognon. 1984a.
“Pseudo Maximum Likelihood Methods: Applications to Poisson Models.” Econometrica, 52(3):
701–20.
Gourieroux, C., A. Monfort, and A. Trognon. 1984b.
“Pseudo Maximum Likelihood Methods: Theory.”

Econometrica, 52(3): 681–700.
Graham, Bryan S. 2008. “Identifying Social Interactions through Conditional Variance Restrictions.”
Econometrica, 76(3): 643–60.
Graham, Bryan S., Guido W. Imbens, and Geert Ridder. 2006. “Complementarity and Aggregate Implications of Assortative Matching: A Nonparametric
Analysis.” Unpublished.
Greenberg, David, and Michael Wiseman. 1992. “What
Did the OBRA Demonstrations Do?” In Evaluating Welfare and Training Programs, ed. Charles F.
Manski and Irwin Garfinkel, 25–75. Cambridge and
London: Harvard University Press.
Gu, X., and Paul R. Rosenbaum. 1993. “Comparison
of Multivariate Matching Methods: Structures, Distances and Algorithms.” Journal of Computational
and Graphical Statistics, 2(4): 405–20.
Gueron, Judith M., and Edward Pauly. 1991. From Welfare to Work. New York: Russell Sage Foundation.
Haavelmo, Trygve. 1943. “The Statistical Implications
of a System of Simultaneous Equations.” Econometrica, 11(1): 1–12.
Hahn, Jinyong. 1998. “On the Role of the Propensity
Score in Efficient Semiparametric Estimation of
Average Treatment Effects.” Econometrica, 66(2):
315–31.
Hahn, Jinyong, Petra E. Todd, and Wilbert van der
Klaauw. 2001. “Identification and Estimation of
Treatment Effects with a Regression-Discontinuity
Design.” Econometrica, 69(1): 201–09.
Ham, John C., and Robert J. LaLonde. 1996. “The
Effect of Sample Selection and Initial Conditions
in Duration Models: Evidence from Experimental
Data on Training.” Econometrica, 64(1): 175–205.
Hamermesh, Daniel S., and Jeff E. Biddle. 1994.
“Beauty and the Labor Market.” American Economic Review, 84(5): 1174–94.
Hansen, B. B. 2008. “The Essential Role of Balance
Tests in Propensity-Matched Observational Studies:
Comments on ‘A Critical Appraisal of PropensityScore Matching in the Medical Literature between
1996 and 2003’ by Peter Austin.” Statistics in Medicine, 27(12): 2050–54.
Hansen, Christian B. 2007a. “Asymptotic Properties of
a Robust Variance Matrix Estimator for Panel Data
When T Is Large.” Journal of Econometrics, 141(2):
597–620.
Hansen, Christian B. 2007b. “Generalized Least
Squares Inference in Panel and Multilevel Models
with Serial Correlation and Fixed Effects.” Journal
of Econometrics, 140(2): 670–94.
Hanson, Samuel, and Adi Sunderam. 2008. “The Variance of Average Treatment Effect Estimators in the
Presence of Clustering.” Unpublished.
Hardle, Wolfgang. 1990. Applied Nonparametric
Regression. Cambridge; New York and Melboure:
Cambridge University Press.
Heckman, James J. 1990. “Varieties of Selection Bias.”
American Economic Review, 80(2): 313–18.
Heckman, James J., and V. Joseph Hotz. 1989.
­“Choosing among Alternative Nonexperimental

Imbens and Wooldridge: Econometrics of Program Evaluation
Methods for Estimating the Impact of Social Programs: The Case of Manpower Training.” Journal
of the American Statistical Association, 84(408):
862–74.
Heckman, James J., Hidehiko Ichimura, Jeffrey A.
Smith, and Petra E. Todd. 1998. “Characterizing
Selection Bias Using Experimental Data.” Econometrica, 66(5): 1017–98.
Heckman, James J., Hidehiko Ichimura, and Petra E.
Todd. 1997. “Matching as an Econometric Evaluation Estimator: Evidence from Evaluating a Job
Training Programme.” Review of Economic Studies,
64(4): 605–54.
Heckman, James J., Hidehiko Ichimura, and Petra E.
Todd. 1998. “Matching as an Econometric Evaluation Estimator.” Review of Economic Studies, 65(2):
261–94.
Heckman, James J., Robert J. Lalonde, and Jeffrey A.
Smith. 1999. “The Economics and Econometrics of
Active Labor Market Programs.” In Handbook of
Labor Economics, Volume 3A, ed. Orley Ashenfelter
and David Card, 1865–2097. Amsterdam; New York
and Oxford: Elsevier Science, North-Holland.
Heckman, James J., Lance Lochner, and Christopher
Taber. 1999. “Human Capital Formation and General Equilibrium Treatment Effects: A Study of Tax
and Tuition Policy.” Fiscal Studies, 20(1): 25–40.
Heckman, James J., and Salvador Navarro-Lozano.
2004. “Using Matching, Instrumental Variables, and
Control Functions to Estimate Economic Choice
Models.” Review of Economics and Statistics, 86(1):
30–57.
Heckman, James J., and Richard Robb Jr. 1985. “Alternative Methods for Evaluating the Impact of Interventions.” In Longitudinal Analysis of Labor Market
Data, ed. James J. Heckman and Burton Singer, 156245. Cambridge; New York and Sydney: Cambridge
University Press.
Heckman, James J., and Jeffrey A. Smith. 1995.
“Assessing the Case for Social Experiments.” Journal
of Economic Perspectives, 9(2): 85–110.
Heckman, James J., and Jeffrey A. Smith. 1997. “Making the Most Out of Programme Evaluations and
Social Experiments: Accounting for Heterogeneity
in Programme Impacts.” Review of Economic Studies, 64(4): 487–535.
Heckman, James J., Sergio Urzua, and Edward Vytlacil. 2006. “Understanding Instrumental Variables
in Models with Essential Heterogeneity.” Review of
Economics and Statistics, 88(3): 389–432.
Heckman, James J., and Edward Vytlacil. 2005. “Structural Equations, Treatment Effects, and Econometric Policy Evaluation.” Econometrica, 73(3):
669–738.
Heckman, James J., and Edward Vytlacil. 2007a.
“Econometric Evaluation of Social Programs, Part
I: Causal Models, Structural Models and Econometric Policy Evaluation.” In Handbook of Econometrics, Volume 6B, ed. James J. Heckman and Edward
E. Leamer, 4779–4874. Amsterdam and Oxford:
Elsevier, North-Holland.

81

Heckman, James J., and Edward Vytlacil. 2007b.
“Econometric Evaluation of Social Programs, Part
II: Using the Marginal Treatment Effect to Organize Alternative Econometric Estimators to Evaluate Social Programs, and to Forecast Their Effects
in New Environments.” In Handbook of Econometrics, Volume 6B, ed. James J. Heckman and Edward
E. Leamer, 4875–5143. Amsterdam and Oxford:
Elsevier, North-Holland.
Hill, Jennifer. 2008. “Discussion of Research Using
Propensity-Score Matching: Comments on ‘A Critical Appraisal of Propensity-Score Matching in the
Medical Literature between 1996 and 2003’ by Peter
Austin.” Statistics in Medicine, 27(12): 2055–61.
Hirano, Keisuke, and Guido W. Imbens. 2001. “Estimation of Causal Effects Using Propensity Score
Weighting: An Application to Data on Right Heart
Catheterization.” Health Services and Outcomes
Research Methodology, 2(3–4): 259–78.
Hirano, Keisuke, and Guido W. Imbens. 2004. “The
Propensity Score with Continuous Treatments.” In
Applied Bayesian Modeling and Causal Inference
from Incomplete-Data Perspectives, ed. Andrew
Gelman and Xiao-Li Meng, 73–84. Hoboken, N.J.:
Wiley.
Hirano, Keisuke, Guido W. Imbens, and Geert Ridder.
2003. “Efficient Estimation of Average Treatment
Effects Using the Estimated Propensity Score.”
Econometrica, 71(4): 1161–89.
Hirano, Keisuke, Guido W. Imbens, Donald B. Rubin,
and Xiao-Hua Zhou. 2000. “Assessing the Effect of
an Influenza Vaccine in an Encouragement Design.”
Biostatistics, 1(1): 69–88.
Hirano, Keisuke, and Jack R. Porter. 2008. “Asymptotics for Statistical Treatment Rules.” http://
www.u.arizona.edu/~hirano/hp3_2008_08_10.pdf.
Holland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association, 81(396): 945–60.
Horowitz, Joel L. 2001. “The Bootstrap.” In Handbook of Econometrics, Volume 5, ed. James J. Heckman and Edward Leamer, 3159–3228. Amsterdam;
London and New York: Elsevier Science, NorthHolland.
Horowitz, Joel L., and Charles F. Manski. 2000. “Nonparametric Analysis of Randomized Experiments
with Missing Covariate and Outcome Data.” Journal of the American Statistical Association, 95(449):
77–84.
Horvitz, D. G., and D. J. Thompson. 1952. “A Generalization of Sampling without Replacement from a
Finite Universe.” Journal of the American Statistical
Association, 47(260): 663–85.
Hotz, V. Joseph, Guido W. Imbens, and Jacob A. Klerman. 2006. “Evaluating the Differential Effects of
Alternative Welfare-to-Work Training Components:
A Reanalysis of the California GAIN Program.”
Journal of Labor Economics, 24(3): 521–66.
Hotz, V. Joseph, Guido W. Imbens, and Julie H. Mortimer. 2005. “Predicting the Efficacy of Future
Training Programs Using Past Experiences at Other

82

Journal of Economic Literature, Vol. XLVII (March 2009)

Locations.” Journal of Econometrics, 125(1–2):
241–70.
Hotz, V. Joseph, Charles H. Mullin, and Seth G. Sanders. 1997. “Bounding Causal Effects Using Data from
a Contaminated Natural Experiment: Analysing the
Effects of Teenage Childbearing.” Review of Economic Studies, 64(4): 575–603.
Iacus, Stefano M., Gary King, and Giuseppe Porro.
2008. “Matching for Causal Inference without Balance Checking.” Unpublished.
Ichimura, Hidehiko, and Oliver Linton. 2005. “Asymptotic Expansions for Some Semiparametric Program
Evaluation Estimators.” In Identification and Inference for Econometric Models: Essays in Honor of
Thomas Rothenberg, ed. Donald W. K. Andrews and
James H. Stock, 149–70. Cambridge and New York:
Cambridge University Press.
Ichimura, Hidehiko, and Petra E. Todd. 2007. “Implementing Nonparametric and Semiparametric Estimators.” In Handbook of Econometrics, Volume
6B, ed. James J. Heckman and Edward E. Leamer,
5369–5468. Amsterdam and Oxford: Elsevier,
North-Holland.
Imbens, Guido W. 2000. “The Role of the Propensity
Score in Estimating Dose-Response Functions.”
Biometrika, 87(3): 706–10.
Imbens, Guido W. 2003. “Sensitivity to Exogeneity
Assumptions in Program Evaluation.” American
Economic Review, 93(2): 126–32.
Imbens, Guido W. 2004. “Nonparametric Estimation
of Average Treatment Effects under Exogeneity: A
Review.” Review of Economics and Statistics, 86(1):
4–29.
Imbens, Guido W. 2007. “Non-additive Models with
Endogenous Regressors.” In Advances in Economics
and Econometrics: Theory and Applications, Ninth
World Congress, Volume 3, ed. Richard Blundell,
Whitney K. Newey, and Torsten Persson, 17–46.
Cambridge and New York: Cambridge University
Press.
Imbens, Guido W., and Joshua D. Angrist. 1994. “Identification and Estimation of Local Average Treatment Effects.” Econometrica, 62(2): 467–75.
Imbens, Guido W., and Karthik Kalyanaraman. 2009.
“Optimal Bandwidth Choice for the Regression Discontinuity Estimator.” National Bureau of Economic
Research Working Paper 14726.
Imbens, Guido W., Gary King, David McKenzie, and
Geert Ridder. 2008. “On the Benefits of Stratification in Randomized Experiments.” Unpublished.
Imbens, Guido W., and Thomas Lemieux. 2008.
“Regression Discontinuity Designs: A Guide to
Practice.” Journal of Econometrics, 142(2): 615–35.
Imbens, Guido W., and Charles F. Manski. 2004. “Confidence Intervals for Partially Identified Parameters.”
Econometrica, 72(6): 1845–57.
Imbens, Guido W., and Whitney K. Newey. Forthcoming. “Identification and Estimation of Triangular
Simultaneous Equations Models without Additivity.”
National Bureau of Economic Research Technical
Econometrica.

Imbens, Guido W., Whitney K. Newey, and Geert Ridder. 2005. “Mean-Squared-Error Calculations for
Average Treatment Effects.” Unpublished.
Imbens, Guido W., and Donald B. Rubin. 1997a.
“Bayesian Inference for Causal Effects in Randomized Experiments with Noncompliance.” Annals of
Statistics, 25(1): 305–27.
Imbens, Guido W., and Donald B. Rubin. 1997b.
“Estimating Outcome Distributions for Compliers
in Instrumental Variables Models.” Review of Economic Studies, 64(4): 555–74.
Imbens, Guido W., and Donald B. Rubin. Forthcoming. Causal Inference in Statistics and the Social
Sciences. Cambridge and New York: Cambridge
University Press.
Imbens, Guido W., Donald B. Rubin, and Bruce I. Sacerdote. 2001. “Estimating the Effect of Unearned
Income on Labor Earnings, Savings, and Consumption: Evidence from a Survey of Lottery Players.”
American Economic Review, 91(4): 778–94.
Jin, Ginger Zhe, and Phillip Leslie. 2003. “The Effect
of Information on Product Quality: Evidence from
Restaurant Hygiene Grade Cards.” Quarterly Journal of Economics, 118(2): 409–51.
Joffe, Marshall M., and Paul R. Rosenbaum. 1999.
“Invited Commentary: Propensity Scores.” American Journal of Epidemiology, 150(4): 327–33.
Kitagawa, Toru. 2008. “Identification Bounds for the
Local Average Treatment Effect.” Unpublished.
Kling, Jeffrey R., Jeffrey B. Liebman, and Lawrence
F. Katz. 2007. “Experimental Analysis of Neighborhood Effects.” Econometrica, 75(1): 83–119.
Lalive, Rafael. 2008. “How Do Extended Benefits
Affect Unemployment Duration? A Regression
Discontinuity Approach.” Journal of Econometrics,
142(2): 785–806.
LaLonde, Robert J. 1986. “Evaluating the Econometric Evaluations of Training Programs with Experimental Data.” American Economic Review, 76(4):
604–20.
Lechner, Michael. 1999. “Earnings and Employment
Effects of Continuous Off-the-Job Training in East
Germany after Unification.” Journal of Business and
Economic Statistics, 17(1): 74–90.
Lechner, Michael. 2001. “Identification and Estimation of Causal Effects of Multiple Treatments under
the Conditional Independence Assumption.” In
Econometric Evaluation of Labour Market Policies,
ed. Michael Lechner and Friedhelm Pfeiffer, 43–58.
Heidelberg and New York: Physica; Mannheim: Centre for European Economic Research.
Lechner, Michael. 2002a. “Program Heterogeneity
and Propensity Score Matching: An Application to
the Evaluation of Active Labor Market Policies.”
Review of Economics and Statistics, 84(2): 205–20.
Lechner, Michael. 2002b. “Some Practical Issues in
the Evaluation of Heterogeneous Labour Market
Programmes by Matching Methods.” Journal of the
Royal Statistical Society: Series A (Statistics in Society), 165(1): 59–82.
Lechner, Michael. 2004. “Sequential Matching

Imbens and Wooldridge: Econometrics of Program Evaluation
­ stimation of Dynamic Causal Models.” University
E
of St. Gallen Department of Economics Discussion
Paper 2004-06.
Lechner, Michael, and Ruth Miquel. 2005. “Identification of the Effects of Dynamic Treatments By
Sequential Conditional Independence Assumptions.” University of St. Gallen Department of Economics Discussion Paper 2005-17.
Lechner, Michael, Ruth Miquel, and Conny Wunsch.
2004. “Long-Run Effects of Public Sector Sponsored Training in West Germany.” Institute for the
Study of Labor Discussion Paper 1443.
Lee, David S. 2001. “The Electoral Advantage to
Incumbency and the Voters’ Valuation of Politicians’
Experience: A Regression Discontinuity Analysis of
Elections to the U.S. . . . ” National Bureau of Economic Research Working Paper 8441.
Lee, David S. 2008. “Randomized Experiments from
Non-random Selection in U.S. House Elections.”
Journal of Econometrics, 142(2): 675–97.
Lee, David S., and David Card. 2008. “Regression
Discontinuity Inference with Specification Error.”
Journal of Econometrics, 142(2): 655–74.
Lee, David S., and Thomas Lemieux. 2008. “Regression
Discontinuity Designs in Economics.” Unpublished.
Lee, David S., Enrico Moretti, and Matthew J. Butler.
2004. “Do Voters Affect or Elect Policies? Evidence
from the U.S. House.” Quarterly Journal of Economics, 119(3): 807–59.
Lee, Myoung-Jae. 2005a. Micro-Econometrics for
Policy, Program, and Treatment Effects. Oxford and
New York: Oxford University Press.
Lee, Myoung-Jae. 2005b. “Treatment Effect and Sensitivity Analysis for Self-Selected Treatment and
Selectively Observed Response.” Unpublished.
Lehmann, Erich L. 1974. Nonparametrics: Statistical
Methods Based on Ranks. San Francisco: HoldenDay.
Lemieux, Thomas, and Kevin Milligan. 2008. “Incentive Effects of Social Assistance: A Regression Discontinuity Approach.” Journal of Econometrics,
142(2): 807–28.
Leuven, Edwin, and Barabara Sianesi. 2003.
“PSMATCH2: Stata Module to Perform Full
Mahalanobis and Propensity Score Matching,
Common Support Graphing, and Covariate Imbalance Testing.” http://ideas.repec.org/c/boc/bocode/
s432001.html.
Li, Qi, Jeffrey S. Racine, and Jeffrey M. Wooldridge.
Forthcoming. “Efficient Estimaton of Average
Treatment Effects with Mixed Categorical and Continuous Data.” Journal of Business and Economic
Statistics.
Linton, Oliver, and Pedro Gozalo. 2003. “Conditional
Independence Restrictions: Testing and Estimation.” Unpublished.
Little, Roderick J. A., and Donald B. Rubin. 1987.
Statistical Analysis with Missing Data. New York:
Wiley.
Ludwig, Jens, and Douglas L. Miller. 2005. “Does
Head Start Improve Children’s Life Chances?

83

­ vidence from a Regression Discontinuity Design.”
E
National Bureau of Economic Research Working
Paper 11702.
Ludwig, Jens, and Douglas L. Miller. 2007. “Does
Head Start Improve Children’s Life Chances? Evidence from a Regression Discontinuity Design.”
Quarterly Journal of Economics, 122(1): 159–208.
Manski, Charles F. 1990. “Nonparametric Bounds on
Treatment Effects.” American Economic Review,
80(2): 319–23.
Manski, Charles F. 1993. “Identification of Endogenous
Social Effects: The Reflection Problem.” Review of
Economic Studies, 60(3): 531–42.
Manski, Charles F. 1995. Identification Problems in
the Social Sciences. Cambridge and London: Harvard University Press.
Manski, Charles F. 2000a. “Economic Analysis of
Social Interactions.” Journal of Economic Perspectives, 14(3): 115–36.
Manski, Charles F. 2000b. “Identification Problems
and Decisions under Ambiguity: Empirical Analysis
of Treatment Response and Normative Analysis of
Treatment Choice.” Journal of Econometrics, 95(2):
415–42.
Manski, Charles F. 2001. “Designing Programs for
Heterogeneous Populations: The Value of Covariate
Information.” American Economic Review, 91(2):
103–06.
Manski, Charles F. 2002. “Treatment Choice under
Ambiguity Induced By Inferential Problems.” Journal of Statistical Planning and Inference, 105(1):
67–82.
Manski, Charles F. 2003. Partial Identification of
Probabilities Distributions. New York and Heidelberg: Springer.
Manski, Charles F. 2004. “Statistical Treatment Rules
for Heterogeneous Populations.” Econometrica,
72(4): 1221–46.
Manski, Charles F. 2005. Social Choice with Partial
Knowledge of Treatment Response. Princeton and
Oxford: Princeton University Press.
Manski, Charles F. 2007. Identification for Prediction
and Decision. Cambridge and London: Harvard
University Press.
Manski, Charles F., and John V. Pepper. 2000. “Monotone Instrumental Variables: With an Application
to the Returns to Schooling.” Econometrica, 68(4):
997–1010.
Manski, Charles F., Gary D. Sandefur, Sara McLanahan, and Daniel Powers. 1992. “Alternative Estimates of the Effect of Family Structure during
Adolescence on High School Graduation.” Journal
of the American Statistical Association, 87(417):
25–37.
Matzkin, Rosa L. 2003. “Nonparametric Estimation
of Nonadditive Random Functions.” Econometrica,
71(5): 1339–75.
McCrary, Justin. 2008. “Manipulation of the Running
Variable in the Regression Discontinuity Design:
A Density Test.” Journal of Econometrics, 142(2):
698–714.

84

Journal of Economic Literature, Vol. XLVII (March 2009)

McEwan, Patrick J., and Joseph S. Shapiro. 2008. “The
Benefits of Delayed Primary School Enrollment:
Discontinuity Estimates Using Exact Birth Dates.”
Journal of Human Resources, 43(1): 1–29.
Mealli, Fabrizia, Guido W. Imbens, Salvatore Ferro,
and Annibale Biggeri. 2004. “Analyzing a Randomized Trial on Breast Self-Examination with Noncompliance and Missing Outcomes.” Biostatistics, 5(2):
207–22.
Meyer, Bruce D., W. Kip Viscusi, and David L. Durbin.
1995. “Workers’ Compensation and Injury Duration:
Evidence from a Natural Experiment.” American
Economic Review, 85(3): 322–40.
Miguel, Edward, and Michael Kremer. 2004. “Worms:
Identifying Impacts on Education and Health in the
Presence of Treatment Externalities.” Econometrica, 72(1): 159–217.
Morgan, Stephen L., and Christopher Winship. 2007.
Counterfactuals and Causal Inference: Methods and
Principles for Social Research. Cambridge and New
York: Cambridge University Press.
Moulton, Brent R. 1990. “An Illustration of a Pitfall
in Estimating the Effects of Aggregate Variables on
Micro Unit.” Review of Economics and Statistics,
72(2): 334–38.
Moulton, Brent R., and William C. Randolph. 1989.
“Alternative Tests of the Error Components Model.”
Econometrica, 57(3): 685–93.
Newey, Whitney K. 1994a. “Kernel Estimation of
Partial Means and a General Variance Estimator.”
Econometric Theory, 10(2): 233–53.
Newey, Whitney K. 1994b. “Series Estimation of
Regression Functionals.” Econometric Theory,
10(1): 1–28.
Olken, Benjamin A. 2007. “Monitoring Corruption:
Evidence from a Field Experiment in Indonesia.”
Journal of Political Economy, 115(2): 200–249.
Pagan, Adrian, and Aman Ullah. 1999. Nonparametric Econometrics. Cambridge; New York and Melbourne: Cambridge University Press.
Pakes, Ariel, Jack R. Porter, Kate Ho, and Joy Ishii.
2006. “Moment Inequalities and Their Application.”
Institute for Fiscal Studies Centre for Microdata
Methods and Practice Working Paper CWP16/07.
Pearl, Judea. 2000. Causality: Models, Reasoning, and
Inference. Cambridge; New York and Melbourne:
Cambridge University Press.
Pettersson-Lidbom, Per. 2007. “The Policy Consequences of Direct versus Representative Democracy:
A Regression-Discontinuity Approach.” Unpublished.
Pettersson-Libdom, Per. 2008. “Does the Size of the
Legislature Affect the Size of Government? Evidence
from Two Natural Experiments.” Unpublished.
Pettersson-Lidbom, Per, and Björn Tyrefors. 2007. “Do
Parties Matter for Economic Outcomes? A Regression-Discontinuity Approach.” Unpublished.
Politis, Dimitris N., Joseph P. Romano, and Michael
Wolf. 1999. Subsampling. New York: Springer,
Verlag
Porter, Jack R. 2003. “Estimation in the Regression
Discontinuity Model.” Unpublished.

Quade, D. 1982. “Nonparametric Analysis of Covariance By Matching.” Biometrics, 38(3): 597–611.
Racine, Jeffrey S., and Qi Li. 2004. “Nonparametric
Estimation of Regression Functions with Both Categorical and Continuous Data.” Journal of Econometrics, 119(1): 99–130.
Riccio, James, and Daniel Friedlander. 1992. GAIN:
Program Strategies, Participation Patterns, and
First-Year Impacts in Six Countries. New York:
Manpower Demonstration Research Corporation.
Riccio, James, Daniel Friedlander, and Stephen Freedman. 1994. GAIN: Benefits, Costs, and Three-Year
Impacts of a Welfare-to-Work Program. New York:
Manpower Demonstration Research Corporation.
Robins, James M., and Ya’acov Ritov. 1997. “Toward
a Curse of Dimensionality Appropriate (CODA)
Asymptotic Theory for Semi-parametric Models.”
Statistics in Medicine, 16(3): 285–319.
Robins, James M., and Andrea Rotnitzky. 1995. “Semiparametric Efficiency in Multivariate Regression
Models with Missing Data.” Journal of the American
Statistical Association, 90(429): 122–29.
Robins, James M., Andrea Rotnitzky, and Lue Ping
Zhao. 1995. “Analysis of Semiparametric Regression
Models for Repeated Outcomes in the Presence of
Missing Data.” Journal of the American Statistical
Association, 90(429): 106–21.
Robinson, Peter M. 1988. “Root-N-Consistent Semiparametric Regression.” Econometrica, 56(4):
931–54.
Romano, Joseph P., and Azeem M. Shaikh. 2006a.
“Inference for Identifiable Parameters in Partially
Identified Econometric Models.” Stanford University
Department of Statistics Technical Report 2006-9.
Romano, Joseph P., and Azeem M. Shaikh. 2006b.
“Inference for the Identified Set in Partially Identified Econometric Models.” Unpublished.
Rosen, Adam M. 2006. “Confidence Sets for Partially
Identified Parameters That Satisfy a Finite Number
of Moment Inequalities.” Institute for Fiscal Studies
Centre for Microdata Methods and Practice Working Paper CWP25/06.
Rosenbaum, Paul R. 1984a. “Conditional Permutation
Tests and the Propensity Score in Observational
Studies.” Journal of the American Statistical Association, 79(387): 565–74.
Rosenbaum, Paul R. 1984b. “The Consequences of
Adjustment for a Concomitant Variable That Has
Been Affected By the Treatment.” Journal of the
Royal Statistical Society: Series A (Statistics in Society), 147(5): 656–66.
Rosenbaum, Paul R. 1987. “The Role of a Second Control Group in an Observational Study.” Statistical
Science, 2(3): 292–306.
Rosenbaum, Paul R. 1989. “Optimal Matching for
Observational Studies.” Journal of the American
Statistical Association, 84(408): 1024–32.
Rosenbaum, Paul R. 1995. Observational Studies. New
York; Heidelberg and London: Springer.
Rosenbaum, Paul R. 2002. “Covariance Adjustment
in Randomized Experiments and Observational

Imbens and Wooldridge: Econometrics of Program Evaluation
­Studies.” Statistical Science, 17(3): 286–327.
Rosenbaum, Paul R., and Donald B. Rubin. 1983a.
“Assessing Sensitivity to an Unobserved Binary
Covariate in an Observational Study with Binary
Outcome.” Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 45(2): 212–18.
Rosenbaum, Paul R., and Donald B. Rubin. 1983b.
“The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika,
70(1): 41–55.
Rosenbaum, Paul R., and Donald B. Rubin. 1984.
“Reducing Bias in Observational Studies Using Subclassification on the Propensity Score.” Journal of the
American Statistical Association, 79(387): 516–24.
Rosenbaum, Paul R., and Donald B. Rubin. 1985.
“Constructing a Control Group Using Multivariate
Matched Sampling Methods That Incorporate the
Propensity Score.” American Statistician, 39(1):
33–38.
Rotnitzky, Andrea, and James M. Robins. 1995. “Semiparametric Regression Estimation in the Presence of
Dependent Censoring.” Biometrika, 82(4): 805–20.
Roy, A. D. 1951. “Some Thoughts on the Distribution of
Earnings.” Oxford Economic Papers, 3(2): 135–46.
Rubin, Donald B. 1973a. “Matching to Remove Bias in
Observational Studies.” Biometrics, 29(1): 159–83.
Rubin, Donald B. 1973b. “The Use of Matched Sampling and Regression Adjustment to Remove Bias in
Observational Studies.” Biometrics, 29(1): 184–203.
Rubin, Donald B. 1974. “Estimating Causal Effects
of Treatments in Randomized and Nonrandomized
Studies.” Journal of Educational Psychology, 66(5):
688–701.
Rubin, Donald B. 1976. “Inference and Missing Data.”
Biometrika, 63(3): 581–92.
Rubin, Donald B. 1977. “Assignment to Treatment
Group on the Basis of a Covariate.” Journal of Educational Statistics, 2(1): 1–26.
Rubin, Donald B. 1978. “Bayesian Inference for Causal
Effects: The Role of Randomization.” Annals of Statistics, 6(1): 34–58.
Rubin, Donald B. 1979. “Using Multivariate Matched
Sampling and Regression Adjustment to Control Bias
in Observational Studies.” Journal of the American
Statistical Association, 74(366): 318–28.
Rubin, Donald B. 1987. Multiple Imputation for Nonresponse in Surveys. New York: Wiley.
Rubin, Donald B. 1990. “Formal Mode of Statistical
Inference for Causal Effects.” Journal of Statistical
Planning and Inference, 25(3): 279–92.
Rubin, Donald B. 1997. “Estimating Causal Effects
from Large Data Sets Using Propensity Scores.”
Annals of Internal Medicine, 127(5 Part 2): 757–63.
Rubin, Donald B. 2006. Matched Sampling for Causal
Effects. Cambridge and New York: Cambridge University Press.
Rubin, Donald B., and Neal Thomas. 1992a. “Affinely
Invariant Matching Methods with Ellipsoidal Distributions.” Annals of Statistics, 20(2): 1079–93.
Rubin, Donald B., and Neal Thomas. 1992b.
­“Characterizing the Effect of Matching Using

85

­ inear ­Propensity Score Methods with Normal DisL
tributions.” Biometrika, 79(4): 797–809.
Rubin, Donald B., and Neal Thomas. 1996. “Matching
Using Estimated Propensity Scores: Relating Theory
to Practice.” Biometrics, 52(1): 249–64.
Rubin, Donald B., and Neal Thomas. 2000. “Combining Propensity Score Matching with Additional
Adjustments for Prognostic Covariates.” Journal
of the American Statistical Association, 95(450):
573–85.
Sacerdote, Bruce. 2001. “Peer Effects with Random
Assignment: Results for Dartmouth Roommates.”
Quarterly Journal of Economics, 116(2): 681–704.
Scharfstein, Daniel O, Andrea Rotnitzky, and James
M. Robins. 1999. “Adjusting for Nonignorable DropOut Using Semiparametric Nonresponse Models.”
Journal of the American Statistical Association,
94(448): 1096–1120.
Schultz, T. Paul. 2001. “School Subsidies for the Poor:
Evaluating the Mexican Progresa Poverty Program.”
Yale University Economic Growth Center Discussion Paper 834.
Seifert, Burkhardt, and Theo Gasser. 1996. “FiniteSample Variance of Local Polynomials: Analysis and
Solutions.” Journal of the American Statistical Association, 91(433): 267–75.
Seifert, Burkhardt, and Theo Gasser. 2000. “Data
Adaptive Ridging in Local Polynomial Regression.”
Journal of Computational and Graphical Statistics,
9(2): 338–60.
Sekhon, Jasjeet S. Forthcoming. “Multivariate and Propensity Score Matching Software with Automated
Balance Optimization: The Matching Package for
R.” Journal of Statistical Software.
Sekhon, Jasjeet S., and Richard Grieve. 2008. “A New
Non-parametric Matching Method for Bias Adjustment with Applications to Economic Evaluations.”
http://sekhon.berkeley.edu/papers/GeneticMatching_SekhonGrieve.pdf.
Shadish, William R., Thomas D. Cook, and Donald T.
Campbell. 2002. Experimental and Quasi-Experimental Designs for Generalized Causal Inference.
Boston: Houghton Mifflin.
Smith, Jeffrey A., and Petra E. Todd. 2001. “Reconciling Conflicting Evidence on the Performance of
Propensity-Score Matching Methods.” American
Economic Review, 91(2): 112–18.
Smith, Jeffrey A., and Petra E. Todd. 2005. “Does
Matching Overcome Lalonde’s Critique of Nonexperimental Estimators?” Journal of Econometrics,
125(1–2): 305–53.
Splawa-Neyman, Jerzy. 1990. “On the Application of
Probability Theory to Agricultural Experiments.
Essays on Principles. Section 9.” Statistical Science,
5(4): 465–72. (Orig. pub. 1923.)
Stock, James H. 1989. “Nonparametric Policy Analysis.” Journal of the American Statistical Association,
84(406): 567–75.
Stone, Charles J. 1977. “Consistent Nonparametric
Regression.” Annals of Statistics, 5(4): 595–620.
Stoye, Jörg. 2007. “More on Confidence Intervals for

86

Journal of Economic Literature, Vol. XLVII (March 2009)

Partially Identified Parameters.” Unpublished.
Stuart, Elizabeth A. 2008. “Developing Practical Recommendations for the Use of Propensity Scores: Discussion of ‘A Critical Appraisal of Propensity Score
Matching in the Medical Literature between 1996
and 2003’ by Peter Austin.” Statistics in Medicine,
27(12): 2062–65.
Sun, Yixiao. 2005. “Adaptive Estimation of the Regression Discontinuity Model.” Unpublished.
Thistlethwaite, Donald L., and Donald T. Campbell.
1960. “Regression-Discontinuity Analysis: An Alternative to the Ex Post Facto Experiment.” Journal of
Educational Psychology, 51(6): 309–17.
Trochim, William M. K. 1984. Research Design for
Program Evaluation: The Regression-Discontinuity Approach. Thousand Oaks, Calif.: Sage
Publications.
Trochim, William M. K. 2001. “Regression-Discontinuity Design.” In International Encyclopedia of the
Social and Behavioral Sciences, Volume 20, ed. Neil
J. Smelser and Paul B. Baltes, 12940–45. Oxford:
Elsevier Science.
Van der Klaauw, Wilbert. 2002. “Estimating the Effect
of Financial Aid Offers on College Enrollment: A
Regression-Discontinuity Approach.” International
Economic Review, 43(4): 1249–87.
Van der Klaauw, Wilbert. 2008a. “Breaking the Link
between Poverty and Low Student Achievement:

An Evaluation of Title I.” Journal of Econometrics,
142(2): 731–56.
Van der Klaauw, Wilbert. 2008b. “Regression-Discontinuity Analysis: A Survey of Recent Developments
in Economics.” Labour, 22(2): 219–45.
Van der Laan, Mark J., and James M. Robins. 2003.
Unified Methods for Censored Longitudinal Data
and Causality. New York: Springer, Physica-Verlag.
Vytlacil, Edward. 2002. “Independence, Monotonicity,
and Latent Index Models: An Equivalence Result.”
Econometrica, 70(1): 331–41.
Wooldridge, Jeffrey M. 1999. “Asymptotic Properties
of Weighted M-Estimators for Variable Probability
Samples.” Econometrica, 67(6): 1385–1406.
Wooldridge, Jeffrey M. 2002. Econometric Analysis of
Cross Section and Panel Data. Cambridge and London: MIT Press.
Wooldridge, Jeffrey M. 2005. “Violating Ignorability
of Treatment By Controlling for Too Many Factors.”
Econometric Theory, 21(5): 1026–28.
Wooldridge, Jeffrey M. 2007. “Inverse Probability
Weighted Estimation for General Missing Data
Problems.” Journal of Econometrics, 141(2):
1281–1301.
Zhao, Zhong. 2004. “Using Matching to Estimate
Treatment Effects: Data Requirements, Matching
Metrics, and Monte Carlo Evidence.” Review of
Economics and Statistics, 86(1): 91–107.

This article has been cited by:
1. Martin Petrick, Patrick Zier. 2012. Common Agricultural Policy effects on dynamic labour use in
agriculture. Food Policy 37:6, 671-678. [CrossRef]
2. David McKenzie. 2012. Beyond baseline and follow-up: The case for more T in experiments.
Journal of Development Economics 99:2, 210-221. [CrossRef]
3. José Miguel Benavente, Gustavo Crespi, Lucas Figal Garone, Alessandro Maffioli. 2012.
The impact of national research funds: A regression discontinuity approach to the Chilean
FONDECYT. Research Policy 41:8, 1461-1475. [CrossRef]
4. Robert Girtz. 2012. The Effects of Personality Traits on Wages: A Matching Approach. LABOUR
no-no. [CrossRef]
5. James I. Stewart. 2012. Migration to U.S. frontier cities and job opportunity, 1860–1880.
Explorations in Economic History 49:4, 528-542. [CrossRef]
6. Hyun Ah Kim, Yong-seong Kim, Myoung-jae Lee. 2012. Treatment effect analysis of early
reemployment bonus program: panel MLE and mode-based semiparametric estimator for interval
truncation. Portuguese Economic Journal . [CrossRef]
7. David T. Butry. 2012. Comparing the performance of residential fire sprinklers with other lifesafety technologies. Accident Analysis & Prevention 48, 480-494. [CrossRef]
8. Manuel Gomes, Richard Grieve, Richard Nixon, Edmond S.-W. Ng, James Carpenter, Simon G.
Thompson. 2012. METHODS FOR COVARIATE ADJUSTMENT IN COST-EFFECTIVENESS
ANALYSIS THAT USE CLUSTER RANDOMISED TRIALS. Health Economics 21:9,
1101-1118. [CrossRef]
9. Dirk Czarnitzki, Susanne Thorwarth. 2012. The Contribution of In-house and External Design
Activities to Product Market Performance. Journal of Product Innovation Management 29:5,
878-895. [CrossRef]
10. Nassul Ssentamu Kabunga, Thomas Dubois, Matin Qaim. 2012. Heterogeneous information
exposure and technology adoption: the case of tissue culture bananas in Kenya. Agricultural
Economics 43:5, 473-486. [CrossRef]
11. Cristina Borra, Maria Iacovou, Almudena Sevilla. 2012. The effect of breastfeeding on children's
cognitive and noncognitive development. Labour Economics 19:4, 496-515. [CrossRef]
12. Arne Feddersen, Wolfgang Maennig. 2012. Sectoral Labour Market Effects of the 2006 FIFA
World Cup. Labour Economics . [CrossRef]
13. Yu Ye, Jason C. Bond, Laura A. Schmidt, Nina Mulia, Tammy W. Tam. 2012. Toward a better
understanding of when to apply propensity scoring: a comparison with conventional regression in
ethnic disparities research. Annals of Epidemiology . [CrossRef]
14. Sanders Korenman, Kristin S. Abner, Robert Kaestner, Rachel A. Gordon. 2012. The Child and
Adult Care Food Program and the nutrition of preschoolers. Early Childhood Research Quarterly
. [CrossRef]
15. Rosario Crinò. 2012. Imported inputs and skill upgrading. Labour Economics . [CrossRef]
16. Daniel P. McMillen. 2012. Repeat Sales as a Matching Estimator. Real Estate Economics no-no.
[CrossRef]

17. Gustavo Canavire-Bacarreza, Merlin M. Hanauer. 2012. Estimating the Impacts of Bolivia’s
Protected Areas on Poverty. World Development . [CrossRef]
18. Bruno Martorano, Marco Sanfilippo. 2012. INNOVATIVE FEATURES IN POVERTY
REDUCTION PROGRAMMES: AN IMPACT EVALUATION OF CHILE SOLIDARIO ON
HOUSEHOLDS AND CHILDREN. Journal of International Development n/a-n/a. [CrossRef]
19. Kishore Gawande, Hank Jenkins-Smith, May Yuan. 2012. The long-run impact of nuclear waste
shipments on the property market: Evidence from a quasi-experiment. Journal of Environmental
Economics and Management . [CrossRef]
20. Harald Oberhofer. 2012. Employment Effects of Acquisitions: Evidence from Acquired European
Firms. Review of Industrial Organization . [CrossRef]
21. Hiroki Uematsu, Ashok K. Mishra. 2012. Organic farmers or conventional farmers: Where's the
money?. Ecological Economics 78, 55-62. [CrossRef]
22. Jeffrey L. Furman, Fiona Murray, Scott Stern. 2012. Growing Stem Cells: The Impact of Federal
Funding Policy on the U.S. Scientific Frontier. Journal of Policy Analysis and Management 31:3,
661-705. [CrossRef]
23. Patrick J. McEwan. 2012. Cost-effectiveness analysis of education and health interventions in
developing countries. Journal of Development Effectiveness 4:2, 189-213. [CrossRef]
24. François Claveau. 2012. The Russo–Williamson Theses in the social sciences: Causal inference
drawing on two types of evidence. Studies in History and Philosophy of Science Part C: Studies
in History and Philosophy of Biological and Biomedical Sciences . [CrossRef]
25. Hung-Hao Chang. 2012. Does the use of eco-labels affect income distribution and income
inequality of aquaculture producers in Taiwan?. Ecological Economics . [CrossRef]
26. Breno Sampaio. 2012. Identifying peer states for transportation policy analysis with an application
to New York's handheld cell phone ban. Transportmetrica 1-14. [CrossRef]
27. T. Randolph Beard, George S. Ford, Richard P. Saba, Richard A. Seals. 2012. Internet use and job
search. Telecommunications Policy 36:4, 260-273. [CrossRef]
28. Thomas K. Bauer, Stefan Bender, Alfredo R. Paloyo, Christoph M. Schmidt. 2012. Evaluating the
labor-market effects of compulsory military service. European Economic Review 56:4, 814-829.
[CrossRef]
29. Dirk Czarnitzki, Cindy Lopes-Bento. 2012. Value for money? New microeconometric evidence
on public R&D grants in Flanders. Research Policy . [CrossRef]
30. Daniel L. Millimet, Rusty Tchernis. 2012. Estimation of Treatment Effects without an Exclusion
Restriction: with an Application to the Analysis of the School Breakfast Program. Journal of
Applied Econometrics n/a-n/a. [CrossRef]
31. Ephraim Nkonya, Dayo Phillip, Tewodaj Mogues, John Pender, Edward Kato. 2012. Impacts of
Community-driven Development Programs on Income and Asset Acquisition in Africa: The Case
of Nigeria. World Development . [CrossRef]
32. Sarah Kuck Jalbert, William Rhodes. 2012. Reduced caseloads improve probation outcomes.
Journal of Crime and Justice 1-18. [CrossRef]
33. B Sampaio. 2012. To generalize or not to generalize? Comment on Robinson and Davies. Journal
of the Operational Research Society 63:4, 563-565. [CrossRef]

34. Barry T. Hirsch, Edward J. Schumacher. 2012. Underpaid or Overpaid? Wage Analysis for Nurses
Using Job and Worker Attributes. Southern Economic Journal 78:4, 1096-1119. [CrossRef]
35. Francisco Henríquez, Bernardo Lara, Alejandra Mizala, Andrea Repetto. 2012. Effective schools
do exist: low-income children's academic performance in Chile. Applied Economics Letters 19:5,
445-451. [CrossRef]
36. Vijesh V. Krishna, Matin Qaim. 2012. Bt cotton and sustainability of pesticide reductions in India.
Agricultural Systems 107, 47-55. [CrossRef]
37. RICHARD HARRIS, QIAN CHER LI, JOHN MOFFAT. 2012. THE IMPACT OF HIGHER
EDUCATION INSTITUTION-FIRM KNOWLEDGE LINKS ON ESTABLISHMENT-LEVEL
PRODUCTIVITY IN BRITISH REGIONS*. The Manchester School no-no. [CrossRef]
38. Noémi Kreif, Richard Grieve, M. Zia Sadique. 2012. STATISTICAL METHODS FOR
COST-EFFECTIVENESS ANALYSES THAT USE OBSERVATIONAL DATA: A CRITICAL
APPRAISAL TOOL AND REVIEW OF CURRENT PRACTICE. Health Economics n/a-n/a.
[CrossRef]
39. Julia Koschinsky. 2012. The case for spatial analysis in evaluation to reduce health inequities.
Evaluation and Program Planning . [CrossRef]
40. Christian Langpap, Joe Kerkvliet. 2012. Endangered species conservation on private land:
Assessing the effectiveness of habitat conservation plans. Journal of Environmental Economics
and Management . [CrossRef]
41. Myoung-Jae Lee. 2012. Semiparametric Estimators for Limited Dependent Variable (LDV)
Models with Endogenous Regressors. Econometric Reviews 31:2, 171-214. [CrossRef]
42. Astrid Kiil. 2012. Does employment-based private health insurance increase the use of covered
health care services? A matching estimator approach. International Journal of Health Care
Finance and Economics . [CrossRef]
43. Andreas Peichl, Nico Pestel, Sebastian Siegloch. 2012. The politicians’ wage gap: insights from
German members of parliament. Public Choice . [CrossRef]
44. Patrick J. Egan. 2012. Group Cohesion without Group Mobilization: The Case of Lesbians, Gays
and Bisexuals. British Journal of Political Science 1-20. [CrossRef]
45. Petra Moser, , Alessandra Voena. 2012. Compulsory Licensing: Evidence from the Trading with
the Enemy Act. American Economic Review 102:1, 396-427. [Abstract] [View PDF article] [PDF
with links]
46. Peter C. Rockers, Andrea B. Feigl, John-Arne Røttingen, Atle Fretheim, David de Ferranti, John N.
Lavis, Hans Olav Melberg, Till Bärnighausen. 2012. Study-design selection criteria in systematic
reviews of effectiveness of health systems interventions and reforms: A meta-review. Health
Policy . [CrossRef]
47. Ingeborg Waernbaum. 2012. Model misspecification and robustness in causal inference:
comparing matching with doubly robust estimation. Statistics in Medicine n/a-n/a. [CrossRef]
48. Enrico Beretta, Silvia Del Prete. 2012. Bank Acquisitions and Decentralization Choices. Economic
Notes 41:1-2, 27-57. [CrossRef]
49. Tania Barham. 2012. Enhancing Cognitive Functioning: Medium-Term Effects of a Health and
Family Planning Program in Matlab. American Economic Journal: Applied Economics 4:1,
245-273. [Abstract] [View PDF article] [PDF with links]

50. S. Benin, E. Nkonya, G. Okecho, J. Randriamamonjy, E. Kato, G. Lubade, M. Kyotalimye. 2012.
Impact of the National Agricultural Advisory Services (Naads) program of Uganda: Considering
Different Levels of Likely Contamination with the Treatment. American Journal of Agricultural
Economics 94:2, 386-392. [CrossRef]
51. P. M. Dontsop Nguezet, V. O. Okoruwa, A. I. Adeoti, K. O. Adenegan. 2012. Productivity Impact
Differential of Improved Rice Technology Adoption Among Rice Farming Households in Nigeria.
Journal of Crop Improvement 26:1, 1-21. [CrossRef]
52. Sea-Jin Chang, Jaiho Chung, Jon Jungbien Moon. 2012. When do wholly owned subsidiaries
perform better than joint ventures?. Strategic Management Journal n/a-n/a. [CrossRef]
53. C. N. Brinch, T. A. Galloway. 2011. Schooling in adolescence raises IQ scores. Proceedings of
the National Academy of Sciences . [CrossRef]
54. Fabrizia Mealli, Barbara Pacini, Donald B. RubinStatistical Inference for Causal Effects 171-192.
[CrossRef]
55. MARIA DE PAOLA, VINCENZO SCOPPA. 2011. THE EFFECTS OF CLASS SIZE ON
THE ACHIEVEMENT OF COLLEGE STUDENTS*. The Manchester School 79:6, 1061-1079.
[CrossRef]
56. Allyson Pollock, Azeem Majeed, Alison Macfarlane, Ian Greener, Graham Kirkwood, Howard
Mellett, Sylvia Godden, Sean Boyle, Carol Morelli, Petra Brhlikova. 2011. In defence of our
research on competition in England's National Health Service – Authors' reply. The Lancet
378:9809, 2065-2066. [CrossRef]
57. R. G. Fryer. 2011. Financial Incentives and Student Achievement: Evidence From Randomized
Trials. The Quarterly Journal of Economics . [CrossRef]
58. Beatrix Eugster, Rafael Lalive, Andreas Steinhauer, Josef Zweimüller. 2011. The Demand for
Social Insurance: Does Culture Matter?*. The Economic Journal 121:556, F413-F448. [CrossRef]
59. Thomas C. Buchmueller, , John DiNardo, , Robert G. Valletta. 2011. The Effect of an Employer
Health Insurance Mandate on Health Insurance Coverage and the Demand for Labor: Evidence
from Hawaii. American Economic Journal: Economic Policy 3:4, 25-51. [Abstract] [View PDF
article] [PDF with links]
60. Timothy Powell-Jackson, Kara Hanson. 2011. Financial incentives for maternal health: impact of
a national programme in nepal. Journal of Health Economics . [CrossRef]
61. Nicholas Bloom, Zack Cooper, Martin Gaynor, Stephen Gibbons, Simon Jones, Alistair McGuire,
Rodrigo Moreno-Serra, Carol Propper, John Van Reenen, Stephan Seiler. 2011. In defence of our
research on competition in England's National Health Service. The Lancet . [CrossRef]
62. Shelia R. Cotten, George Ford, Sherry Ford, Timothy M. Hale. 2011. Internet use and depression
among older adults. Computers in Human Behavior . [CrossRef]
63. WILLIAM NILSSON. 2011. HETEROGENEITY OR TRUE STATE DEPENDENCE IN
POVERTY: THE TALE TOLD BY TWINS. Review of Income and Wealth no-no. [CrossRef]
64. Hunt Allcott. 2011. Social norms and energy conservation. Journal of Public Economics 95:9-10,
1082-1095. [CrossRef]
65. Marco Di Cintio, Emanuele Grassi. 2011. Internal migration and wages of Italian university
graduates*. Papers in Regional Science no-no. [CrossRef]

66. Seung-Hyun Hong. 2011. MEASURING THE EFFECT OF NAPSTER ON RECORDED
MUSIC SALES: DIFFERENCE-IN-DIFFERENCES ESTIMATES UNDER COMPOSITIONAL
CHANGES. Journal of Applied Econometrics n/a-n/a. [CrossRef]
67. Kevin Gross, Jay A. Rosenheim. 2011. Quantifying secondary pest outbreaks in cotton and
their monetary cost with causal-inference statistics. Ecological Applications 21:7, 2770-2780.
[CrossRef]
68. Laure B. de Preux. 2011. Anticipatory ex ante moral hazard and the effect of medicare on
prevention. Health Economics 20:9, 1056-1072. [CrossRef]
69. François Claveau. 2011. Evidential variety as a source of credibility for causal inference: beyond
sharp designs and structural models. Journal of Economic Methodology 18:3, 233-253. [CrossRef]
70. Sébastien Massoni, Jean-Christophe Vergnaud. 2011. How to improve pupils’ literacy? A
cost-effectiveness analysis of a French educational project. Economics of Education Review .
[CrossRef]
71. Mark B. Stewart. 2011. Quantile estimates of counterfactual distribution shifts and the effect of
minimum wage increases on the wage distribution. Journal of the Royal Statistical Society: Series
A (Statistics in Society) no-no. [CrossRef]
72. P. J. Ferraro, M. M. Hanauer, K. R. E. Sims. 2011. Cozzarelli Prize Winner: Biodiversity
Conservation and Poverty Traps Special Feature: Conditions associated with protected area
success in conservation and poverty reduction. Proceedings of the National Academy of Sciences
108:34, 13913-13918. [CrossRef]
73. C. D. Meyerhoefer, M. Yang. 2011. The Relationship between Food Assistance and Health:
A Review of the Literature and Empirical Strategies for Identifying Program Effects. Applied
Economic Perspectives and Policy . [CrossRef]
74. Elisabetta Magnani, Rong Zhu. 2011. Gender wage differentials among Rural-Urban Migrants in
China. Regional Science and Urban Economics . [CrossRef]
75. Heiner Mikosch, Jan-Egbert Sturm. 2011. Has the EMU reduced wage growth and unemployment?
Testing a model of trade union behavior. European Journal of Political Economy . [CrossRef]
76. Oliviero Carboni. 2011. R&D subsidies and private R&D expenditures: evidence from Italian
manufacturing data. International Review of Applied Economics 25:4, 419-439. [CrossRef]
77. Bert Scholtens. 2011. The sustainability of green funds. Natural Resources Forum no-no.
[CrossRef]
78. Isabelle Martinez, Stéphanie Serve. 2011. The delisting decision: The case of buyout offer with
squeeze-out (BOSO). International Review of Law and Economics . [CrossRef]
79. Yongheng Deng, Daniel P. McMillen, Tien Foo Sing. 2011. Private residential price indices in
Singapore: A matching approach. Regional Science and Urban Economics . [CrossRef]
80. Erling Barth, Bernt Bratsberg, Torbjørn Haegeland, Oddbjørn Raaum. 2011. Performance Pay,
Union Bargaining and Within-Firm Wage Inequality*. Oxford Bulletin of Economics and Statistics
no-no. [CrossRef]
81. Anirban Basu, Daniel Polsky, Willard G. Manning. 2011. Estimating treatment effects on
healthcare costs under exogeneity: is there a ‘magic bullet’?. Health Services and Outcomes
Research Methodology . [CrossRef]

82. A. J. Reynolds, J. A. Temple, S.-R. Ou, I. A. Arteaga, B. A. B. White. 2011. School-Based Early
Childhood Education and Age-28 Well-Being: Effects by Timing, Dosage, and Subgroups. Science
. [CrossRef]
83. Frank D. Bean, Mark A. Leach, Susan K. Brown, James D. Bachmeier, John R Hipp. 2011. The
Educational Legacy of Unauthorized Migration: Comparisons Across U.S.-Immigrant Groups in
How Parents’ Status Affects Their Offspring1. International Migration Review 45:2, 348-385.
[CrossRef]
84. Jonathan R. Kesselman. 2011. Consumer Impacts of BC's Harmonized Sales Tax: Tax Grab or
Pass-Through?. Canadian Public Policy 1:-1, 139-162. [CrossRef]
85. Taro Esaka. 2011. Do hard pegs avoid currency crises? An evaluation using matching estimators.
Economics Letters . [CrossRef]
86. Benedito Cunguara, Ika Darnhofer. 2011. Assessing the impact of improved agricultural
technologies on household income in rural Mozambique. Food Policy 36:3, 378-390. [CrossRef]
87. Tarjei Havnes, Magne Mogstad. 2011. Money for nothing? Universal child care and maternal
employment. Journal of Public Economics . [CrossRef]
88. B. Cunguara, K. Moder. 2011. Is Agricultural Extension Helping the Poor? Evidence from Rural
Mozambique. Journal of African Economies . [CrossRef]
89. Luis H. B. Braido, Pedro Olinto, Helena Perrone. 2011. Gender Bias in Intrahousehold
Allocation: Evidence from an Unintentional Experiment. Review of Economics and Statistics
120224102436000. [CrossRef]
90. D. Newhouse, D. Suryadarma. 2011. The value of vocational education: High school type and
labor market outcomes in Indonesia. The World Bank Economic Review . [CrossRef]
91. DAVID DREYER LASSEN, SØREN SERRITZLEW. 2011. Jurisdiction Size and Local
Democracy: Evidence on Internal Political Efficacy from Large-scale Municipal Reform.
American Political Science Review 1-21. [CrossRef]
92. Yu Xiao, Jun Wan, Geoffrey J. D. Hewings. 2011. Flooding and the Midwest economy: assessing
the Midwest floods of 1993 and 2008. GeoJournal . [CrossRef]
93. Tarjei Havnes, , Magne Mogstad. 2011. No Child Left Behind: Subsidized Child Care and
Children's Long-Run Outcomes. American Economic Journal: Economic Policy 3:2, 97-129.
[Abstract] [View PDF article] [PDF with links]
94. Bryan S. Graham, , Keisuke Hirano. 2011. Robustness to Parametric Assumptions in Missing Data
Models. American Economic Review 101:3, 538-543. [Abstract] [View PDF article] [PDF with
links]
95. B. Cantillon, A. De Ridder, E. Vanhaecht, G. Verbist. 2011. (Un)desirable effects of output funding
for Flemish universities. Economics of Education Review . [CrossRef]
96. Pao-Li Chang, Myoung-Jae Lee. 2011. The WTO trade effect. Journal of International Economics
. [CrossRef]
97. Martin Huber, Michael Lechner, Conny Wunsch. 2011. Does leaving welfare improve health?
Evidence for Germany. Health Economics 20:4, 484-504. [CrossRef]
98. Jay A. Rosenheim, Soroush Parsa, Andrew A. Forbes, William A. Krimmel, Yao Hua Law, Michal
Segoli, Moran Segoli, Frances S. Sivakoff, Tania Zaviezo, Kevin Gross. 2011. Ecoinformatics

for Integrated Pest Management: Expanding the Applied Insect Ecologist's Tool-Kit. Journal of
Economic Entomology 104:2, 331-342. [CrossRef]
99. Nikolaus Graf, Helmut Hofer, Rudolf Winter-Ebmer. 2011. Labor supply effects of a subsidized
old-age part-time scheme in Austria. Zeitschrift für ArbeitsmarktForschung . [CrossRef]
100. Hilmar Schneider, Arne Uhlendorff, Klaus F. Zimmermann. 2011. Mit Workfare aus der
Sozialhilfe? Lehren aus einem Modellprojekt. Zeitschrift für ArbeitsmarktForschung . [CrossRef]
101. Lota D. Tamini. 2011. A nonparametric analysis of the impact of agri-environmental advisory
activities on best management practice adoption: A case study of Qu??bec. Ecological Economics
. [CrossRef]
102. Yu Xiao. 2011. LOCAL ECONOMIC IMPACTS OF NATURAL DISASTERS*. Journal of
Regional Science no-no. [CrossRef]
103. Dirk Czarnitzki, Petr Hanel, Julio Miguel Rosa. 2011. Evaluating the impact of R&D tax credits
on innovation: A microeconometric study on Canadian firms#. Research Policy 40:2, 217-229.
[CrossRef]
104. Martin Petrick, Patrick Zier. 2011. Regional employment impacts of Common Agricultural Policy
measures in Eastern Germany: a difference-in-differences approach. Agricultural Economics 42:2,
183-193. [CrossRef]
105. Eeshani Kandpal. 2011. Beyond Average Treatment Effects: Distribution of Child Nutrition
Outcomes and Program Placement in India’s ICDS. World Development . [CrossRef]
106. Gregory Leonard, G. Steven Olley. 2011. What Can Be Learned About the Competitive Effects of
Mergers from “Natural Experiments”?. International Journal of the Economics of Business 18:1,
103-107. [CrossRef]
107. Hendrik Jürges, Kerstin Schneider. 2011. Why Young Boys Stumble: Early Tracking, Age and
Gender Bias in the German School System. German Economic Review no-no. [CrossRef]
108. Christian Volpe Martincus, Jeronimo Carballo, Pablo Garcia. 2011. Public programmes to promote
firms' exports in developing countries: are there heterogeneous effects by size categories?. Applied
Economics 1-21. [CrossRef]
109. Tove Faber Frandsen, Jeppe Nicolaisen. 2011. Praise the bridge that carries you over: Testing
the flattery citation hypothesis. Journal of the American Society for Information Science and
Technology n/a-n/a. [CrossRef]
110. Alberto Abadie, Guido W. Imbens. 2011. Bias-Corrected Matching Estimators for Average
Treatment Effects. Journal of Business and Economic Statistics 29:1, 1-11. [CrossRef]
111. Providing Clean Water: Evidence from Randomized Evaluations 67-77. [CrossRef]
112. Bijan J. Borah, Marguerite E. Burns, Nilay D. Shah. 2011. Assessing the impact of high deductible
health plans on health-care utilization and cost: a changes-in-changes approach. Health Economics
n/a-n/a. [CrossRef]
113. Paul J. Ferraro, Merlin M. Hanauer. 2011. Protecting Ecosystems and Alleviating Poverty with
Parks and Reserves: ‘Win-Win’ or Tradeoffs?. Environmental and Resource Economics 48:2, 269.
[CrossRef]
114. Olli Ropponen. 2011. Reconciling the evidence of Card and Krueger (1994) and Neumark and
Wascher (2000). Journal of Applied Econometrics n/a-n/a. [CrossRef]

115. Annette Bergemann, Marco Caliendo, Gerard J. van den Berg, Klaus F. Zimmermann. 2011. The
threat effect of participation in active labor market programs on job search behavior of migrants
in Germany. International Journal of Manpower 32:7, 777-795. [CrossRef]
116. Zia Sadique, Richard Grieve, David A Harrison, Brian H Cuthbertson, Kathryn M Rowan. 2011.
Is Drotrecogin alfa (activated) for adults with severe sepsis, cost-effective in routine clinical
practice?. Critical Care 15:5, R228. [CrossRef]
117. Myoung-jae Lee, Sanghyeok LeeLikelihood-Based Estimators for Endogenous or Truncated
Samples in Standard Stratified Sampling 27, 63-91. [CrossRef]
118. Matias D. Cattaneo, Max H. FarrellEfficient Estimation of the Dose–Response Function Under
Ignorability Using Subclassification on the Covariates 27, 93-127. [CrossRef]
119. Daniel L. MillimetThe Elephant in the Corner: A Cautionary Tale about Measurement Error in
Treatment Effects Models 27, 1-39. [CrossRef]
120. Ian M. McCarthy, Rusty TchernisOn the Estimation of Selection Models when Participation is
Endogenous and Misclassified 27, 179-207. [CrossRef]
121. B. F. Arnold, R. S. Khush, P. Ramaswamy, A. G. London, P. Rajkumar, P. Ramaprabha, N.
Durairaj, A. E. Hubbard, K. Balakrishnan, J. M. Colford. 2010. Causal inference methods to study
nonrandomized, preexisting development interventions. Proceedings of the National Academy of
Sciences 107:52, 22605-22610. [CrossRef]
122. Daniel J. Graham, Kurt Van Dender. 2010. Estimating the agglomeration benefits of transport
investments: some tests for stability. Transportation . [CrossRef]
123. C. B. Barrett, M. R. Carter. 2010. The Power and Pitfalls of Experiments in Development
Economics: Some Non-random Reflections. Applied Economic Perspectives and Policy 32:4,
515-548. [CrossRef]
124. Wang-Sheng Lee, Michael B. Coelli. 2010. The Labour Market Effects of Vocational Education
and Training in Australia. Australian Economic Review 43:4, 389-408. [CrossRef]
125. James Fenske. 2010. THE CAUSAL HISTORY OF AFRICA: A RESPONSE TO HOPKINS.
Economic History of Developing Regions 25:2, 177-212. [CrossRef]
126. Arnab Acharya, Giulia Greco, Edoardo Masset. 2010. The economics approach to evaluation
of health interventions in developing countries through randomised field trial. Journal of
Development Effectiveness 2:4, 401-420. [CrossRef]
127. Christopher S. Armstrong, Wayne R. Guay, Joseph P. Weber. 2010. The role of information and
financial reporting in corporate governance and debt contracting#. Journal of Accounting and
Economics 50:2-3, 179-234. [CrossRef]
128. Carlos A. Flores, Alfonso Flores-Lagunes, Arturo Gonzalez, Todd C. Neumann. 2010. Estimating
the Effects of Length of Exposure to Instruction in a Training Program: The Case of Job Corps.
Review of Economics and Statistics 111102154542001. [CrossRef]
129. David Card, Jochen Kluve, Andrea Weber. 2010. Active Labour Market Policy Evaluations: A
Meta-Analysis*. The Economic Journal 120:548, F452-F477. [CrossRef]
130. ÁDÁM SZENTPÉTERI, ÁLMOS TELEGDY. 2010. POLITICAL SELECTION OF FIRMS
INTO PRIVATIZATION PROGRAMS. EVIDENCE FROM ROMANIAN COMPREHENSIVE
DATA. Economics & Politics 22:3, 298-328. [CrossRef]

131. Alain de Janvry, Elisabeth Sadoulet, Sofia Villas-Boas. 2010. Short on shots: Are calls for
cooperative restraint effective in managing a flu vaccines shortage?. Journal of Economic Behavior
& Organization 76:2, 209-224. [CrossRef]
132. Halbert White, Xun Lu. 2010. Causal Diagrams for Treatment Effect Estimation with Application
to Efficient Covariate Selection. Review of Economics and Statistics 110823094915005.
[CrossRef]
133. Judea Pearl. 2010. THE FOUNDATIONS OF CAUSAL INFERENCE. Sociological Methodology
40:1, 75-149. [CrossRef]
134. Jenny C. Aker. 2010. Information from Markets Near and Far: Mobile Phones and Agricultural
Markets in Niger. American Economic Journal: Applied Economics 2:3, 46-59. [Abstract] [View
PDF article] [PDF with links]
135. René Böheim, Andrea Weber. 2010. The Effects of Marginal Employment on Subsequent Labour
Market Outcomes. German Economic Review no-no. [CrossRef]
136. Tobias Wolbring. 2010. Weshalb die Separierung von Produktivitätseffekten und Diskriminierung
bei der studentischen Lehrveranstaltungsbewertung misslingt. KZfSS Kölner Zeitschrift für
Soziologie und Sozialpsychologie 62:2, 317-326. [CrossRef]
137. Markus Gangl. 2010. Causal Inference in Sociological Research. Annual Review of Sociology 36:1,
21-47. [CrossRef]
138. Guido W. Imbens. 2010. Better LATE Than Nothing: Some Comments on Deaton (2009) and
Heckman and Urzua (2009). Journal of Economic Literature 48:2, 399-423. [Abstract] [View PDF
article] [PDF with links]
139. James J. Heckman. 2010. Building Bridges between Structural and Program Evaluation
Approaches to Evaluating Policy. Journal of Economic Literature 48:2, 356-398. [Abstract] [View
PDF article] [PDF with links]
140. Joshua D. Angrist, , Jörn-Steffen Pischke, . 2010. The Credibility Revolution in Empirical
Economics: How Better Research Design is Taking the Con out of Econometrics. Journal of
Economic Perspectives 24:2, 3-30. [Abstract] [View PDF article] [PDF with links]
141. Peter Midmore, Mark D. Partridge, M. Rose Olfert, Kamar Ali. 2010. The Evaluation of Rural
Development Policy: Macro and Micro Perspectives
L’évaluation de la politique de développement rural: perspectives macro et microéconomiques
Die Evaluation der Politik zur Entwicklung des ländlichen Raums: Mikro- und Makroperspekti.
EuroChoices 9:1, 24-29. [CrossRef]
142. Matias D. Cattaneo. 2010. Efficient semiparametric estimation of multi-valued treatment effects
under ignorability#. Journal of Econometrics 155:2, 138-154. [CrossRef]
143. Runsheng Yin, Guiping Yin, Lanying Li. 2010. Assessing China’s Ecological Restoration
Programs: What’s Been Done and What Remains to Be Done?. Environmental Management 45:3,
442-453. [CrossRef]
144. Franklin E. Zimring, Jeffrey Fagan, David T. Johnson. 2010. Executions, Deterrence, and
Homicide: A Tale of Two Cities. Journal of Empirical Legal Studies 7:1, 1-29. [CrossRef]
145. Fabio Veras Soares, Rafael Perez Ribas, Guilherme Issamu Hirata. 2010. Impact evaluation of a
rural conditional cash transfer programme on outcomes beyond health and education. Journal of
Development Effectiveness 2:1, 138-157. [CrossRef]

146. Akhter Ali, Awudu Abdulai. 2010. The Adoption of Genetically Modified Cotton and Poverty
Reduction in Pakistan. Journal of Agricultural Economics 61:1, 175-192. [CrossRef]
147. Steven Koch, Olufunke Alaba. 2010. On health insurance and household decisions: A treatment
effect analysis#. Social Science & Medicine 70:2, 175-182. [CrossRef]
148. Joseph Farrell, Paul A. Pautler, Michael G. Vita. 2009. Economics at the FTC: Retrospective
Merger Analysis with a Focus on Hospitals. Review of Industrial Organization 35:4, 369-385.
[CrossRef]
149. Andreas Kuhn, Rafael Lalive, Josef Zweimüller. 2009. The public health costs of job loss. Journal
of Health Economics 28:6, 1099-1115. [CrossRef]
150. Onur Baser. 2009. Too Much Ado about Instrumental Variable Approach: Is the Cure Worse than
the Disease?. Value in Health 12:8, 1201-1209. [CrossRef]
151. Ori Heffetz, , Moses Shayo. 2009. How Large Are Non-Budget-Constraint Effects of Prices on
Demand?. American Economic Journal: Applied Economics 1:4, 170-199. [Abstract] [View PDF
article] [PDF with links]

