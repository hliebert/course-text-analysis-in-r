Comment: Boosting Algorithms: Regularization, Prediction and Model Fitting
Author(s): Andreas Buja, David Mease and Abraham J. Wyner
Source: Statistical Science, Vol. 22, No. 4 (Nov., 2007), pp. 506-512
Published by: Institute of Mathematical Statistics
Stable URL: https://www.jstor.org/stable/27645855
Accessed: 21-10-2019 14:59 UTC
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and
extend access to Statistical Science

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:07 UTC
All use subject to https://about.jstor.org/terms

Statistical Science
2007, Vol. 22, No. 4, 506-512

DOI: 10.1214/07-STS242B

Main article DOI: 10.1214/07-STS242
? Institute of Mathematical Statistics, 2007

Comment: Boosting Algorithms:
Regularizaron, Prediction and Model
Fitting
Andreas Buja, David Mease and Abraham J. Wyner

Abstract. The authors are doing the readers of Statistical Science a true
service with a well-written and up-to-date overview of boosting that origi
nated with the seminal algorithms of Freund and Schapire. Equally, we are
grateful for high-level software that will permit a larger readership to ex
periment with, or simply apply, boosting-inspired model fitting. The authors
show us a world of methodology that illustrates how a fundamental innova
tion can penetrate every nook and cranny of statistical thinking and practice.
They introduce the reader to one particular interpretation of boosting and
then give a display of its potential with extensions from classification (where
it all started) to least squares, exponential family models, survival analysis, to
base-learners other than trees such as smoothing splines, to degrees of free
dom and regularization, and to fascinating recent work in model selection.
The uninitiated reader will find that the authors did a nice job of present
ing a certain coherent and useful interpretation of boosting. The other reader,

though, who has watched the business of boosting for a while, may have
quibbles with the authors over details of the historic record and, more impor
tantly, over their optimism about the current state of theoretical knowledge.
In fact, as much as "the statistical view" has proven fruitful, it has also re
sulted in some ideas about why boosting works that may be misconceived,
and in some recommendations that may be misguided.

HISTORY OF "THE STATISTICAL VIEW" AND

take the late Leo Breiman as our guide, because learn
ing what he knew or did not know is instructive to this

FIRST QUESTIONS

day.
Only a decade ago Freund and Schapire (1997,

To get a sense of past history as well as of current
ignorance, we must go back to the roots of boosting,
which are in classification. On this way back, we will

page 119), defined boosting as "converting a 'weak'
PAC learning algorithm that performs just slightly bet
ter than random guessing into one with arbitrarily high

Andreas Buja is Liem Sioe Liong/First Pacific Company
Professor of Statistics, Statistics Department, The Wharton
School, University of Pennsylvania, Philadelphia,

accuracy." The assumptions underlying the quote im

ply that the classes are 100% separable and hence
that classification solves basically a geometric prob
lem. How else would one interpret "arbitrarily high
accuracy" other than implying a zero Bayes error?

Pennsylvania 19104-6340, USA (e-mail:
buja.at.wharton?gmail.com). David Mease is Assistant
Professor, Department of Marketing and Decision Sciences,
College of Business, San Jose State University, San Jose,

Professor, Statistics Department, The Wharton School,
University of Pennsylvania, Philadelphia, Pennsylvania

See Breiman's (1998, Appendix) patient but firm com
ments on this point. To a statistician the early literature
on boosting was an interesting mix of creativity, tech
nical bravado, and statistically unrealistic assumptions
inspired by the PAC learning framework. Yet, in as far

19104-6340, USA (e-mail: ajw@wharton.upenn.edu).

as machine learners relied on Vapnik's random sam

California 95192-0069, USA (e-mail:
mease_d@cob.sjsu.edu). Abraham J. Wyner is Associate

506
This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:07 UTC
All use subject to https://about.jstor.org/terms

COMMENT 507
pling assumption and his allowance for overlapping
classes, they had in hand the seeds for a fundamentally
statistical treatment of boosting, at least in theory.

By now, statistical views of boosting have existed for
a number of years, and they are mostly due to statisti

cians. One such view is due to Friedman, Hastie and
Tibshirani (2000) who propose that boosting is stage
wise additive model fitting. Equivalent to stagewise ad
ditive fitting is B?hlmann and Hothorn's notion of fit
ting by gradient descent in function space, theirs be
ing a more mathematical than statistical terminology.
B?hlmann and Hothorn attribute the view of boosting
as functional gradient descent (FGD) to Breiman, but
in this they are factually inaccurate. Of the two arti
cles they cite, "Arcing Classifiers" (Breiman, 1998) has
nothing to do with optimization. Here is Breiman's fa

mous praise of boosting algorithms as "the most ac
curate ... off-the-shelf classifiers on a wide variety of
data sets." The article is important, but not as an ances

tor of the "statistical view" of boosting as we will see
below. A better candidate is B?hlmann and Hothorn's

other reference, "Prediction Games and Arcing Al
gorithms" (Breiman, 1999). A closer reading shows,
however, that it is an ancestor, not a founder, of a sta
tistical view of boosting, even though here is the first
interpretation of AdaBoost as minimization of an expo
nential criterion. Borrowing from Freund and Schapire

(1996), Breiman's approach is not statistical but game
theoretic, hence he justifies fitting base learners not
with gradient descent but with the minimax theorem.

boosting as model fitting in the following sense: Boost
ing creates linear combinations of base learners (called

"weighted votes" in machine learning) that are esti
mates of half the logit of the underlying conditional

class probabilities, P(Y = \\x). In this view, boost
ing could suddenly be seen as class probability esti
mation in the conditional Bernoulli model, and con
sequently FHT's (2000) first order of business was to
create LogitBoost by replacing exponential loss with
the loss function that is natural to statisticians, the neg

ative log-likelihood of the Bernoulli model (= "log
loss"). FHT (2000) also replaced boosting's reweight
ing with the reweighting that statisticians have known
for decades, iteratively reweighted least squares, to im
plement Newton descent/Fisher scoring. In this clean
picture, AdaBoost estimates half the logit, LogitBoost
estimates the logit, both by stagewise fitting, but by
different approaches to the functional gradient that pro
duces the additive terms. Going yet further, Friedman
(2001, based on a 1999 report) discarded weighting al
together by approximating gradients with plain least
squares. These innovations had been absorbed as early

as 1999 by the newly minted Ph.D. Greg Ridgeway
(1999) who presented an excellent piece on "The State
of Boosting" that included a survey of these yet-to-be

published developments as well as his own work on
boosting for exponential family and survival regres
sion. Thus the new view of boosting as model fitting
developed in a short period between the middle of 1998
and early 1999 and bore fruit instantly before any of it
had appeared in print.

He stylizes the problem to selecting among finitely

It is Friedman's (2001) gradient boosting that

many fixed base learners, thereby removing the func
tional aspect. His calculations are on training samples,
not populations, and hence they never reveal what is

B?hlmann and Hothorn now call "the generic FGD
or boosting algorithm" (Section 2.1). This promotion
of one particular algorithm to a standard could give

being estimated. In his pre-2000 work one will find
neither the terms "functional" and "gradient" nor a
concept of boosting as model fitting and estimation.

These facts stand against Mason et al.'s (2000, Sec
tion 2.1) attribution of "gradient descent in function

space" to Breiman, against Breiman (2000a, 2004)
himself when he links FGD to Breiman (1999, 1997),
and now against B?hlmann and Hothorn.
For a statistical view of boosting, the dam really
broke in 1998 with a report by Friedman, Hastie
and Tibshirani (2000, based on a 1998 report; "FHT
(2000)" henceforth). Around that time, others had also
picked up on the exponential criterion and its mini
mization, including Mason et al. (2000) and Schapire
and Singer (1999), but it was FHT (2000) whose sim
ple population calculations established the meaning of

rise to misgivings among the originators of boosting

because the original discrete AdaBoost (Section 1.2)
is not even a special case of gradient boosting. There
exists, however, a version of gradient descent that con

tains AdaBoost as a special case: it is alluded to in
Section 2.1.1 and appears in Mason et al. (2000, Sec
tion 3), FHT (2000, Section 4.1) and Breiman (2000a;
2004, Sections 2.2, 4.1). Starting with the identity

-ot^p(F!-,/(XI)
+ fg(X?))
t=0j
= ?p,(r/,/(X/))g(Xf)
i
{p' = the partial w.r.t. the second argument), find steep
est descent directions by minimizing the right-hand ex
pression with regard to g(X). Minimization in this case

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:07 UTC
All use subject to https://about.jstor.org/terms

508 A. BUJA, D. MEASE AND A. J. WYNER
is not generally well defined, because it typically pro

duces ?oo unless the permissible directions g(X) are
bounded (Ridgeway, 2000). One way to bound g(X)
is by confining it to classifiers (g(X) e {?1, +1}), in
which case gradient descent on the exponential loss

function p = exp(?Y?f(Xi)) (F/ = ?1) yields dis

crete AdaBoost. Instead of bounding of g(X), Ridge
way (2000) pointed out that the above ill-posed gra
dient minimization could be regularized by adding a
quadratic penalty 2(g) = J2i g(Xi)2/2 to the right
hand side, only to arrive at a criterion that, after

quadratic completion, produces Friedman's (2001)
least squares gradient boosting:

j2{(-p(y^f(Xi)))-g(Xi))2.
i
We may wonder what, other than algebraic conve
nience, makes ^2? g(Xi)2/2 the penalty of choice.
A mild modification is g(g) = l/(2c) ?\ g(X/)2 with
c > 0 as a penalty parameter; quadratic completion re
sults in the least squares criterion

J^ii-cp'iYnfiX^-giXi))2,
i
which shows that for small c its minimization yields
Friedman's step size shrinkage. The choice

Qia^S'tYi,i f(Xi))g(Xi)2/2
has the particular justification that it provides a second
order approximation to the loss function, and hence its

minimization generates Newton descent/Fisher scor
ing as used in FHT's LogitBoost. For comparison,

gradient descent uses ?p'(F?,/(X?)) as the work
ing response in an unweighted least squares problem,

whereas Newton descent uses (?pf/p")(Yi,f(Xi))
as the working response in a weighted least squares
problem with weights p"(F?, f(X?)). In view of these
choices, we may ask B?hlmann and Hothorn whether
there are deeper reasons for their advocacy of Fried
man's gradient descent as the boosting standard. Fried

man's intended applications included L\- and Huber
M-estimation, in which case second derivatives are
not available. In many other cases, though, includ
ing exponential and logistic loss and the likelihood of
any exponential family model, second derivatives are

available, and we should expect some reasoning from
B?hlmann and Hothorn for abandoning entrenched sta
tistical practice.

LIMITATIONS OF "THE STATISTICAL VIEW" OF

BOOSTING

While the statistical view of boosting as model fit
ting is truly a breakthrough and has proven extremely
fruitful in spawning new boosting methodologies, one

should not ignore that it has also caused misconcep
tions, in particular in classification. For example, the
idea that boosting implicitly estimates conditional class
probabilities turns out to be wrong in practice. Both

AdaBoost and LogitBoost are primarily used for clas
sification, not class probability estimation, and in so far
as they produce successful classifiers in practice, they

also produce extremely overfitted estimates of con
ditional class probabilities, namely, values near zero
and 1. In other words, it would be a mistake to as
sume that in order to successfully classify, one should
look for accurate class probability estimates. Success
ful classification cannot be reduced to successful class
probability estimation, and some published theoretical

work is flawed because of doing just that. B?hlmann
and Hothorn allude to these problems in Section 1.3,
but they do not discuss them. It would be helpful if they

summarized for us the state of statistical theory in ex
plaining successful classification without committing
the fallacy of reducing it to successful class probability
estimation.

There have been some misunderstandings in the
literature about an alleged superiority of LogitBoost
over AdaBoost for class probability estimation. No
such thing can be asserted to date. Both produce
scores that are in theory estimates of P(Y = l\x)
when passed through an inverse link function. Both
could be used for class probability estimation if prop
erly regularized?at the cost of deteriorating classifi

cation performance. B?hlmann and Hothorn's list of
reasons for preferring log-loss over exponential loss
(Section 3.2.1) might cater to some of the more com

mon misconceptions: log-loss "(i) .. .yields probabil
ity estimates"?so does exponential loss; both do so in
theory but not in practice, unless either loss function is

suitably regularized; "(ii) it is a monotone loss function

of the margin"?so is exponential loss; "(iii) it grows
linearly as the margin... tends to ?oo, unlike the expo
nential loss"?true, but when they add "The third point
reflects a robustness aspect: it is similar to Huber's loss
function," they are overstepping the boundaries of to

day's knowledge. Do we know that there even exists
a robustness issue? Unlike quantitative responses, bi
nary responses have no problem of vertically outly
ing values. The stronger growth of the exponential loss

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:07 UTC
All use subject to https://about.jstor.org/terms

COMMENT 509
only implies greater penalties for strongly misclassified
cases, and why should this be detrimental? It appears
that there is currently no theory that allows us to rec

ommend log-loss over exponential loss or vice versa,
or to choose from the larger class of proper scoring
rules described by Buja et al. (2005). If B?hlmann and
Hothorn have a stronger argument to make, it would be

most welcome.

For our next point, we return to Breiman's (1998)
article because its main message is a heresy in light
of today's "statistical view" of boosting. He writes:
"The main effect of both bagging and arcing is to re

duce variance" (page 802; "arcing" = Breiman's term
for boosting). This was written before his discovery
of boosting's connection with exponential loss, from
a performance-oriented point of view informed by a
bias-variance decomposition he devised for classifica
tion. It was also before the advent of the "statistical
view" and its "low-variance principle," which explains
Breiman's use of the full CART algorithm as the base
learner, following earlier examples in machine learning
that used the full C4.5 algorithm.

Then Breiman (1999, page 1494) dramatically re
verses himself in response to learning that "Schapire

et al. (1997) [(1998)] gave examples of data where
two-node trees (stumps) had high bias and the main

effect of AdaBoost was to reduce the bias." This work
of Breiman's makes fascinating reading because of its

perplexed tone and its admission in the Conclusions
section (page 1506) that "the results leave us in a
quandary," and "the laboratory results for various arc
ing algorithms are excellent, but the theory is in dis
array." His important discovery that AdaBoost can be
interpreted as the minimizer of an exponential criterion
happens on the side line of an argument with Schapire
and Freund about the deficiencies of VC- and margin
based arguments for explaining boosting. Yet, there
after Breiman no longer cites his 1998 Annals article
in a substantive way, and he, too, submits to the idea

whereas current theories and the "statistical view" in

general obsess with bias. Against today's consensus
we need to draw attention again to the earlier Breiman

(1998) to remind us of his and others' favorable ex
periences with boosting of high-variance base learners

such as CART and C4.5. It was in the high-variance
case that Breiman issued his praise of boosting, and it
is this case that seems to be lacking theoretical expla
nation. Obviously, high-variance base learners cannot
be analyzed with a heuristic such as in B?hlmann and

Hothorn's Section 5.1 (from B?hlmann and Yu, 2003)
for L2 boosting which only transfers variability from
residuals to fits and never the other way round. Ideally,

we would have a single approach that automatically
reduces bias when necessary and variance when nec
essary. That such could be the case for some versions
of AdaBoost was still in the back of Breiman's mind,
and it is now explicitly asserted by Amit and Blanchard
(2001), not only for AdaBoost but for a large class of
ensemble methods. Is this a statistical jackpot, and we
are not realizing it because we are missing the theory
to comprehend it?
After his acquiescence to low-complexity base learn
ers and regularization, Breiman still uttered occasion

ally a discordant view, as in his work on random
forests (Breiman, 1999b, page 3) where he conjec
tured: "Adaboost has no random elements ... But just
as a deterministic random number generator can give
a good imitation of randomness, my belief is that in

its later stages Adaboost is emulating a random for
est." If his intuition is on target, then we may want
to focus on randomized versions of boosting for vari

ance reduction, both in theory and practice. On the
practical side, Friedman (2002, based on a report of
1999) took a leaf out of Breiman's book and found
that restricting boosting iterations to random subsam
ples improved performance in the vast majority of sce

narios he examined. The abstract of Friedman's ar

ticle ends on this note: "This randomized approach
also increases robustness against overcapacity of the
base learner," that is, against overfitting by a high

that the complexity of base learners needs to be con
trolled. Today we seem to be sworn in on base learners
that are weak in the sense of having low complexity,
high bias (for most data) and low variance, and accord
ingly B?hlmann and Hothorn exhort us to adopt the
"low-variance principle" (Section 4.4). What PAC the
ory used to call "weak learner" is now statistically re
interpreted as "low-variance learner." In this we miss

variance base learner. This simple yet powerful exten
sion of functional gradient descent is not mentioned
by B?hlmann and Hothorn. Yet, Breiman's and Fried
man's work seems to point to a statistical jackpot out
side the "statistical view."

out on the other possible cause of weakness, which
is high variance. As much as underfitting calls for

BOOSTING EXEMPLIFIED

bias reduction, overfitting calls for variance reduction.
Some varieties of boosting may be able to achieve both,

LIMITATIONS OF "THE STATISTICAL VIEW" OF
In the previous section we outlined limitations of
the prevalent "statistical view" of boosting by follow

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:07 UTC
All use subject to https://about.jstor.org/terms

510 A. BUJA, D. MEASE AND A. J. WYNER

ing some of boosting's history and pointing to mis
conceptions and blind spots in "the statistical view."
In this section we will sharpen our concerns based on
an article, "Evidence Contrary to the Statistical View
of Boosting," by two of us (Mease and Wyner, 2007,
"MW (2007)" henceforth), to appear in the Journal of
Machine Learning Research (JMLR). Understandably
this article was not known to B?hlmann and Hothorn

at the time when they wrote theirs, as we were not
aware of theirs when we wrote ours. Since these two
works represent two contemporary contesting views,
we feel it is of interest to discuss the relationship fur

ther. Specifically, in this section we will draw con
nections between statements made in B?hlmann and
Hothorn's article and evidence against these statements
presented in our JMLR article. In what follows, we pro
vide a list of five beliefs central to the statistical view of

boosting. For each of these, we cite specific statements
in the B?hlmann-Hothorn article that reflect these be

liefs. Then we briefly discuss empirical evidence pre
sented in our JMLR article that calls these beliefs into

stumps give a higher misclassification error (even with
the optimal stopping time), they also exhibit substantial

overfitting while the larger trees show no signs of over
fitting in the first 1000 iterations and lead to a much
smaller hold-out misclassification error.

Statistical Perspective on Boosting Belief #2: Early
Stopping Should Be Used to Prevent Overfitting
In Section 1.3 B?hlmann and Hothorn tell us that "it

is clear nowadays that AdaBoost and also other boost
ing algorithms are overfitting eventually, and early

stopping is necessary." This statement is extremely
broad and contradicts Breiman (2000b) who wrote,
based on empirical evidence, that "A crucial property
of AdaBoost is that it almost never overfits the data

no matter how many iterations it is run." The con
trast might suggest that in the seven years since, there

has been theory or further empirical evidence to ver
ify that overfitting will happen eventually in all of the
instances on which Breiman based his claim. No such
theory exists and empirical examples of overfitting are
rare, especially for relatively high-variance base learn

question. The discussion is now limited to two-class
classification where boosting's peculiarities are most

ers. Ironically, stumps with low variance seem to be

in focus. The algorithm we use is "discrete AdaBoost."

more prone to overfitting than base learners with high

Statistical Perspective on Boosting Belief #1:
Stumps Should Be Used for Additive Bayes

Decision Rules

In their Section 4.3 B?hlmann and Hothorn repro
duce the following argument from FHT (2000): "When
using stumps ... the boosting estimate will be an addi
tive model in the original predictor variables, because
every stump-estimate is a function of a single predictor
variable only. Similarly, boosting trees with (at most) d
terminal nodes results in a nonparametric model hav
ing at most interactions of order d ? 2. Therefore, if
we want to constrain the degree of interactions, we can
easily do this by constraining the (maximal) number of
nodes in the base procedure." In Section 4.4 they sug
gest to "choose the base procedure (having the desired
structure) with low variance at the price of larger esti
mation bias." As a consequence, if one decides that the
desired structure is an additive model, the best choice

for a base learner would be stumps. While this be
lief certainly is well accepted in the statistical commu
nity, practice suggests otherwise. It can easily be shown
through simulation that boosted stumps often perform
substantially worse than larger trees even when the true

variance. Also, some examples of overfitting in the
literature are quite artificial and often employ algo
rithms that bear little resemblance to the original Ad

aBoost algorithm. On the other hand, examples for
which overfitting is not observed are abundant, and a

number of such examples are given in our JMLR ar
ticle. If overfitting is judged with respect to misclas
sification error, not only does the empirical evidence
suggest early stopping is not necessary in most appli

cations of AdaBoost, but early stopping can degrade
performance. Another matter is overfitting in terms of
the conditional class probabilities as measured by the

surrogate loss function (exponential loss, negative log

likelihood, proper scoring rules in general; see Buja
et al., 2005). Class probabilities tend to overfit rapidly
and drastically, while hold-out misclassification errors
keep improving.

Statistical Perspective on Boosting Belief #3:
Shrinkage Should Be Used to Prevent Overfitting
Shrinkage in boosting is the practice of using a step

length factor smaller than 1. It is discussed in Sec
tion 2.1 where the authors write the following: "The
choice of the step-length factor v in step 4 is of minor

classification boundaries can be described by an addi

importance, as long as it is 'small' such as v = 0.1. A

tive function. A striking example is given in Section 3.1

smaller value of v typically requires a larger number
of boosting iterations and thus more computing time,

of our JMLR article. In this simulation not only do

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:07 UTC
All use subject to https://about.jstor.org/terms

COMMENT 511
while the predictive accuracy has been empirically
found to be potentially better and almost never worse

when choosing v 'sufficiently small' (e.g., v ? 0.1)."
With regard to AdaBoost, these statements are gener

for conditional class probabilities, the resulting classi
fiers would be entirely uncompetitive in terms of hold
out misclassification error. In our two JMLR articles

(Mease et al., 2007; MW, 2007) we provide a num

ally not true. In fact, not only does shrinkage often not

ber of examples in which the hold-out misclassifica

improve performance, it can lead to overfitting in cases
in which AdaBoost otherwise would not overfit. An ex

tion error decreases throughout while the hold-out bi
nomial log-likelihood and similar measures deteriorate

ample can be found in Section 3.7 of our JMLR article.

throughout. This would suggest that the "good stop

Statistical Perspective on Boosting Belief #4:
Boosting is Estimating Probabilities
In Section 3.1 B?hlmann and Hothorn present the
usual probability estimates for AdaBoost that emerge
from the "statistical view," mentioning that "the reason
for constructing these probability estimates is based on
the fact that boosting with a suitable stopping iteration
is consistent." While the "statistical view" of boosting
does in fact suggest this mapping produces estimates
of the class probabilities, they tend to produce uncom
petitive classification if stopped early, or else vastly
overfitted class probabilities if stopped late. We do cau
tion against their use in the article cited by the authors
(Mease, Wyner, Buja, 2007). In that article we further
show that simple approaches based on over- and under
sampling yield class probability estimates that perform
quite well. In MW (2007) we give a simple example for
which the true conditional probabilities of class 1 are
either 0.1 or 0.9, yet the probability estimates quickly
diverge to values smaller than 0.01 and larger than 0.99

well before the classification rule has approached its
optimum. This behavior is typical.

Statistical Perspective on Boosting Belief #5:

Regularization Should Be Based on the Loss

Function

In Section 5.4 the authors suggest one can "use in
formation criteria for estimating a good stopping iter
ation." One of these criteria suggested for the classifi

cation problem is an AIC- or BIC-penalized negative
binomial log-likelihood. A problem with B?hlmann
and Hothorn's presentation is that they do not explain
whether their recommendation is intended for estimat
ing conditional class probabilities or for classification.
In the case of classification, readers should be warned
that the recommendation will produce inferior perfor

mance for reasons explained earlier: Boosting itera
tions keep improving in terms of hold-out misclassi
fication error while class probabilities are being over

fitted beyond reason. While early stopping based on
penalized likelihoods might produce reasonable values

ping iteration" is the very first iteration, when in fact
for classification the best iteration is the last iteration

which is at least 800 in all examples.

WHAT IS THE ROLE OF THE SURROGATE LOSS

FUNCTION?

In this last section we wish to further muddy our

view of the role of surrogate loss functions as well
as the issues of step-size selection and early stopping.
Drawing on Wyner (2003), we consider a modification
of AdaBoost that doubles the step size relative to the
standard AdaBoost algorithm:

/\-errM\

~ ?g\ errW J'
The additional factor of 2 of course does not simply
double all the coefficients because it affects the re
weighting at each iteration: starting with the second
iteration, raw and modified AdaBoost will use differ
ent sets of weights, hence the fitted base learners will
differ.

As can be seen from the description of the AdaBoost

algorithm in B?hlmann and Hothorn's Section 1.2,
doubling the step size amounts to using the square
of the weight multiplier in each iteration. It is obvi
ous that the modified AdaBoost uses a more aggres
sive reweighting strategy because, relatively speak
ing, squaring makes small weights smaller and large
weights larger. Just the same, modified AdaBoost is a
reweighting algorithm that is very similar to the origi
nal AdaBoost, and it is not a priori clear which of the
two algorithms is going to be the more successful one.
It is obvious, however, that modified AdaBoost does

strange things in terms of the exponential loss. We
know that the original AdaBoost's step-size choice is
the minimizer in a line search of the exponential loss
in the direction of the fitted base learner. Doubling the

step size overshoots the line search by not descend
ing to the valley but re-ascending on the opposite slope
of the exponential loss function. Even more is known:
Wyner (2003) showed that the modified algorithm re
ascends in such a way that the exponential loss is the

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:07 UTC
All use subject to https://about.jstor.org/terms

512 A. BUJA, D. MEASE AND A. J. WYNER
same as in the previous iteration! In other words, the
value of the exponential loss remains constant across
iterations. Still more is known: it can be shown that
there does not exist any loss function for which modi
fied AdaBoost yields the minimizer of a line search.

Are we to conclude that modified AdaBoost must

perform badly? This could not be further from the

truth: with C4.5 as the base learner, misclassification
errors tend to approach zero quickly on the training
data and tend to decrease long thereafter on the hold
out data, just as in AdaBoost. As to the bottom line, the

modified algorithm is comparable to AdaBoost: hold
out misclassification errors after over 200 iterations
are not identical but similar on average to AdaBoost's

(Wyner, 2003, Figures 1-3). What is the final analy
sis of these facts? At a minimum, we can say that they
throw a monkey wrench into the tidy machinery of the
"statistical view of boosting."

CONCLUSIONS
There is something missing in the "statistical view

of boosting," and what is missing results in mis

Breiman, L. (1998). Arcing classifiers (with discussion). Ann.
Statist. 26 801-849. MR1635406
Breiman, L. (1999). Random forests?Random features. Techni
cal Report 567, Dept. Statistics, Univ. California. Available at
www.stat.berkeley.edu.
Breiman, L. (2000a). Some infinity theory for predictor ensem
bles. Technical Report 577, Dept. Statistics, Univ. California.
Available at www.stat.berkeley.edu.
Breiman, L. (2000b). Discussion of "Additive logistic regression:
A statistical view of boosting," by J. Friedman, T. Hastie and R.

Tibshirani. Ann. Statist. 28 374-377. MR1790002

Breiman, L. (2004). Population theory for boosting ensembles.
Ann. Statist. 32 1-11. MR2050998

Buja, A., Stuetzle, W. and Shen, Y. (2005). Loss func
tions for binary class probability estimation: Structure and
applications. Technical report, Univ. Washington. Avail
able at http://www.stat.washington.edu/wxs/Learning-papers/
paper-proper-scoring.pdf.

B?HLMANN, R and Yu, B. (2003). Boosting with the L2 loss:
Regression and classification. /. Amer. Statist. Assoc. 98 324

339. MR1995709

Friedman, J. (2001). Greedy function approximation: A gradient
boosting machine. Ann. Statist. 29 1189-1232. MR1873328

guided recommendations. By guiding us toward high

Friedman, J. H. (2002). Stochastic gradient boosting. Comput.
Statist. Data Anal. 38 367-378. MR1884869

bias/low-variance/low-complexity base learners for

Friedman, J. H., Hastie, T. and Tibshirani, R. (2000). Ad

boosting, the "view" misses out on the power of
boosting low-bias/high-variance/high-complexity base

learners such as C4.5 and CART. It was in this con

text that boosting had received its original praise in
the statistics world (Breiman, 1998). The situation in
which the "statistical view" finds itself is akin to the
joke in which a man looks for the lost key under the
street light even though he lost it in the dark. The "sta
tistical view" uses the ample light of traditional model
fitting that is based on predictors with weak explana
tory power. A contrasting view, pioneered by the earlier

Breiman as well as Amit and Geman (1997) and asso
ciated with the terms "bagging" and "random forests,"
assumes predictor sets so rich that they overfit and re

quire variance- instead of bias-reduction. Breiman's
(1998) early view was that boosting is like bagging,
only better, in its ability to reduce variance. By not ac
counting for variance reduction, the "statistical view"
guides us into a familiar corner where there is plenty
of light but where we might be missing out on more
powerful fitting technology.

REFERENCES
Amit, Y. and Blanchard, G. (2001). Multiple randomized clas
sifiers: MRCL. Technical report, Univ. Chicago.
Amit, Y. and Gem an, D. (1997). Shape quantization and recogni
tion with randomized trees. Neural Computation 9 1545-1588.

Breiman, L. (1997). Arcing the edge. Technical Report 486,
Dept. Statistics, Univ. California. Available at www.stat.
berkeley.edu.

ditive logistic regression: A statistical view of boosting (with
discussion). Ann. Statist. 38 367-378. MR1790002

Freund, Y. and Schapire, R. (1996). Experiments with a new
boosting algorithm. In Proceedings of the Thirteenth Interna

tional Conference on Machine Learning. Morgan Kaufmann,
San Francisco, CA.

Freund, Y. and Schapire, R. (1997). A decision-theoretic gen
eralization of on-line learning and an application to boosting.
J. Comput. System Sei, 55 119-139. MR1473055

Mason, L., Baxter, J., Bartlett, R and Frean, M. (2000).
Functional gradient techniques for combining hypotheses. In
Advances in Large Margin Classifiers (A. Smola, R Bartlett,
B. Sch?lkopf and D. Schuurmans, eds.). MIT Press, Cambridge.

MR1820960

Mease, D., Wyner, A. and Buja, A. (2007). Boosted classifica
tion trees and class probability/quantile estimation. /. Machine

Learning Research 8 409-439.
Mease, D. and Wyner, A. (2007). Evidence contrary to the sta
tistical view of boosting. J. Machine Learning Research. To ap

pear.
Ridgeway, G. (1999). The state of boosting. Comput. Sei. Statis
tics 31 172-181.
Ridgeway, G. (2000). Discussion of "Additive logistic regression:
A statistical view of boosting," by J. Friedman, T. Hastie and R.

Tibshirani. Ann. Statist. 28 393-400. MR 1790002

Schapire, R. E. and Singer, Y. (1999). Improved boosting al
gorithms using confidence-rated predictions. Machine Learning

37 297-226. MR1811573

WYNER, A. (2003). On boosting and the exponential loss. In Pro
ceedings of the Ninth International Workshop on Artificial In

telligence and Statistics.

This content downloaded from 206.253.207.235 on Mon, 21 Oct 2019 14:59:07 UTC
All use subject to https://about.jstor.org/terms

