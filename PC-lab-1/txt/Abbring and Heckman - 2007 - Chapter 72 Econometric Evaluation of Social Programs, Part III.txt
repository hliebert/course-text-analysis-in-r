Chapter 72

ECONOMETRIC EVALUATION OF SOCIAL PROGRAMS,
PART III: DISTRIBUTIONAL TREATMENT EFFECTS, DYNAMIC
TREATMENT EFFECTS, DYNAMIC DISCRETE CHOICE, AND
GENERAL EQUILIBRIUM POLICY EVALUATION*
JAAP H. ABBRING
Vrije Universiteit Amsterdam, The Netherlands
Tinbergen Institute, The Netherlands
JAMES J. HECKMAN
The University of Chicago, USA
American Bar Foundation, USA
University College Dublin, Ireland

Contents
Abstract
Keywords
1. Introduction
2. Identifying distributions of treatment effects
2.1.
2.2.
2.3.
2.4.
2.5.

The problem
Why bother identifying joint distributions?
Solutions
Bounds from classical probability inequalities
Solutions based on dependence assumptions
2.5.1. Solutions based on conditional independence or matching
2.5.2. The common coefficient approach
2.5.3. More general dependence assumptions
2.5.4. Constructing distributions from assuming independence of the gain from the base
2.5.5. Random coefficient regression approaches
2.6. Information from revealed preference

5148
5148
5149
5150
5151
5151
5153
5153
5158
5158
5158
5159
5162
5162
5163

* This research was supported by NSF: 9709873, 0099195, and SES-0241858 and NICHD: R01-HD32058,
and the American Bar Foundation. The views expressed in this chapter are those of the authors and not
necessarily those of funders listed here. We have benefited from discussions with Thomas Amorde, Flavio
Cunha, and Sergio Urzua, and detailed proofreading by Seong Moon, Rodrigo Pinto, Peter Savelyev, G. Adam
Savvas, John Trujillo, Semih Tumen, and Jordan Weil. Section 2 of this chapter is based, in part, on joint work
with Flavio Cunha and Salvador Navarro.

Handbook of Econometrics, Volume 6B
Copyright © 2007 Elsevier B.V. All rights reserved
DOI: 10.1016/S1573-4412(07)06072-2

5146

J.H. Abbring and J.J. Heckman
2.7. Using additional information
2.7.1. Some examples
2.7.2. Relationship to matching
2.7.3. Nonparametric extensions
2.8. General models
2.8.1. Steps 1 and 2: Solving the selection problem within each treatment state
2.8.2. Step 3: Constructing counterfactual distributions using factor models
2.9. Distinguishing ex ante from ex post returns
2.9.1. An approach based on factor structures

2.9.2. Operationalizing the method
2.9.3. The estimation of the components in the information set
2.9.4. Outcome and choice equations
2.10. Two empirical studies

3. Dynamic models
3.1. Policy evaluation and treatment effects
3.1.1. The evaluation problem
3.1.2. The treatment-effect approach
3.1.3. Dynamic policy evaluation
3.2. Dynamic treatment effects and sequential randomization
3.2.1. Dynamic treatment effects
3.2.2. Policy evaluation and dynamic discrete-choice analysis
3.2.3. The information structure of policies
3.2.4. Selection on unobservables
3.3. The event-history approach to policy analysis
3.3.1. Treatment effects in duration models
3.3.2. Treatment effects in more general event-history models
3.3.3. A structural perspective
3.4. Dynamic discrete choice and dynamic treatment effects
3.4.1. Semiparametric duration models and counterfactuals
3.4.2. A sequential structural model with option values
3.4.3. Identification at infinity
3.4.4. Comparing reduced form and structural models
3.4.5. A short survey of dynamic discrete-choice models
3.5. Summary of the state of the art in analyzing dynamic treatment effects

4. Accounting for general equilibrium, social interactions, and spillover effects
4.1. General equilibrium policy evaluation
4.2. General equilibrium approaches based on microdata
4.2.1. Subsequent research
4.2.2. Equilibrium search approaches
4.3. Analyses of displacement
4.4. Social interactions
4.5. Summary of general equilibrium approaches

5. Summary

5166
5168
5173
5173
5174
5175
5179
5181
5184
5187
5188
5189
5194
5209
5210
5210
5214
5215
5217
5217
5224
5227
5229
5230
5231
5237
5242
5243
5245
5258
5265
5266
5268
5273
5274
5275
5275
5281
5282
5282
5285
5285
5286

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5147

Appendix A: Deconvolution
Appendix B: Matzkin conditions and proof of Theorem 2

5286
5287
5287
5288
5290
5290
5294

B.1. The Matzkin conditions
B.2. Proof of Theorem 2

Appendix C: Proof of Theorem 4
Appendix D: Proof of a more general version of Theorem 4
References

5148

J.H. Abbring and J.J. Heckman

Abstract
This chapter develops three topics. (1) Identification of the distributions of treatment effects and the distributions of agent subjective evaluations of treatment effects. Methods
for identifying ex ante and ex post distributions are presented and empirical examples
are given. (2) Identification of dynamic treatment effects. The relationship between the
statistical literature on dynamic causal inference based on sequential-randomization and
the dynamic discrete-choice literature is exposited. The value of well posed economic
choice models for decision making under uncertainty in analyzing and interpreting
dynamic intervention studies is developed. A survey of the dynamic discrete-choice
literature is presented. (3) The key ideas and papers in the recent literature on general
equilibrium evaluations of social programs are summarized.

Keywords
distributions of treatment effects, dynamic treatment effects, dynamic discrete choice,
general equilibrium policy evaluation
JEL classification: C10, C23, C41, C50

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5149

1. Introduction
Part I of this Handbook contribution by Heckman and Vytlacil (Chapter 70) presents
a general framework for policy evaluation. Three distinct policy problems are analyzed: P-1 (Internal Validity)—evaluating the effects of a policy in place; P-2 (External
Validity)—forecasting the effect of a policy in place in a new environment, and P-3—
forecasting the effect of new policies never previously implemented. Among other
topics, Part I considers the analysis of distributions of treatment effects and distinguishes
private (subjective) valuations of programs from objective valuations. It also discusses
the dynamic revelation of information and the uncertainty facing agents. It makes a distinction between ex ante expectations of subjective and objective treatment effects and
ex post realizations of subjective and objective treatment effects. It presents a framework
for defining the option value of participating in social programs. The analysis there is
largely microeconomic in focus and does not consider the full general equilibrium impacts of policies.
Part II by Heckman and Vytlacil (Chapter 71) focuses primarily on methods for conducting ex post evaluations of policies in place (problem P-1), organizing our discussion
around the marginal treatment effect (MTE). Mean treatment effect parameters receive
the most attention. The methods exposited there can be used to identify marginal impact distributions for Y0 and Y1 separately. We show how to use the marginal treatment
effect to solve problems P-2 and P-3 in constructing ex post evaluations but we do not
consider general equilibrium policy analysis.
This chapter presents methods that implement the most innovative aspects of Part I.
It is organized in three sections. The first section analyzes methods for the identification
of distributions of treatment effects (Y1 − Y0 ) and not just the distribution of marginal
outcome distributions (or their means) for Y0 and Y1 separately. We first analyze ex post
realized distributions. A different way to say this is that we initially ignore uncertainty.
We then present methods for identifying ex ante distributions of treatment effects and
the information that agents act on when they make their treatment choices prior to the
realization of outcomes. Agent ex ante expectations are one form of subjective valuation. We present empirical examples based on the research of Carneiro, Hansen and
Heckman (2001, 2003), Cunha, Heckman and Navarro (2005, 2006) and Cunha and
Heckman (2007b, 2007c, 2008). This part of the chapter helps move the evaluation literature out of statistics and into economics. It presents methods for developing subjective
and objective distributions of outcomes.
In the second portion of this contribution, we build on the analysis in the first portion to consider dynamic treatment effects, where sequential revelation of information
plays a prominent role. We consider dynamic matching models introduced by Robins
(1997), Gill and Robins (2001) and Lok (2007), and applied in economics by Lechner
and Miquel (2002) and Fitzenberger, Osikominu and Völter (2006). We then consider
more economically motivated models based on continuous-time duration analysis [see
Abbring and Van den Berg (2003b)] and dynamic generalizations of the Roy model
[Heckman and Navarro (2007)]. We consider identification of mean treatment effects

5150

J.H. Abbring and J.J. Heckman

and joint distributions of both objective and subjective outcomes. In the third section of
the paper, we briefly consider general equilibrium policy evaluation for distributions of
outcomes. We now turn to identification of the distributions of treatment effects.
2. Identifying distributions of treatment effects
The fundamental problem of policy evaluation is that we cannot observe agents in more
than one possible state. Chapter 71 focused on various methods for identifying mean
outcomes and marginal distributions. Methods useful for identifying means apply in
a straightforward way to identification of quantiles of marginal distributions as well
as the full marginal distributions. In a two potential outcome world, we can identify
Pr(Y1  y | X) = E(1[Y1  y] | X) and Pr(Y0  y | X) = E(1[Y0  y] | X)
using the variety of methods summarized in that chapter. One can compare outcomes
at one quantile of Y1 with outcomes at a quantile of Y0 . See, e.g., Heckman, Smith and
Clements (1997) or Abadie, Angrist and Imbens (2002). However, these methods do not
in general identify the quantiles of the distribution of Y1 − Y0 .
The research reported here is based on work by Aakvik, Heckman and Vytlacil
(2005), Heckman and Smith (1998), Heckman, Smith and Clements (1997), Carneiro,
Hansen and Heckman (2001, 2003), Cunha, Heckman and Navarro (2005, 2006), and
Cunha and Heckman (2007b, 2007c, 2008). It moves beyond means as descriptions of
policy outcomes and considers joint counterfactual distributions of outcomes (for example, F (y1 , y0 ), gains, F (y1 − y0 ) or outcomes for participants F (y1 , y0 | D = 1)).
These are the ex post distributions realized after treatment is received. We also analyze
ex ante distributions, inferring the information available to agents when they make their
choices. From knowledge of the ex post joint distributions of counterfactual outcomes,
it is possible to determine the proportion of people who benefit or lose from treatment,
and hence ex post regret, the origin and destination outcomes of those who change status
because of treatment and the amount of gain (or loss) from various policies targeted to
persons at different deciles of an initial pre-policy income distribution.1 Using the joint
distribution of counterfactuals, it is possible to develop a more nuanced understanding
of the distributional impacts of public policies, and to move beyond comparisons of aggregate distributions induced by different policies to consider how people in different
portions of an initial distribution are affected by public policy.
Except in special cases, which we discuss in this portion of the chapter, the methods
discussed in Chapter 71 do not solve the fundamental problem of identifying the distribution of treatment effects, i.e., constructing the joint distribution of (Y0 , Y1 ) and of the
treatment effects Y1 − Y0 . This part of the Handbook reviews methods for constructing
or bounding these distributions. We now state precisely the problem analyzed in this
section.
1 It is also possible to generate all mean, median or other quantile gains to treatment, to identify all pairwise treatment effects in a multi-outcome setting, and to determine how much of the variability in returns
across persons comes from variability in the distributions of the outcome selected and how much comes from
variability in distributions for alternative opportunities.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5151

2.1. The problem
Consider a two-outcome model. The methods surveyed apply in a straightforward way
to models with more than two outcomes, as we demonstrate after analyzing the twooutcome case. For expositional convenience, we focus on scalar outcomes unless explicitly stated otherwise. We do not usually observe (Y0 , Y1 ) as a pair, but rather only
one coordinate and that subject to a selection bias. Thus the problem of recovering joint
distributions from cross-section data has two aspects. The first is the selection problem. From data on outcomes, F1 (y1 | D = 1, X), F0 (y0 | D = 0, X), under what
conditions can one recover F1 (y1 | X) and F0 (y0 | X), respectively? The second problem is how to construct the joint distribution of F (y0 , y1 | X) from the two marginal
distributions. We assume in this section that one of the methods for dealing with the selection problem discussed in Chapters 70 and 71 has been applied and the analyst knows
Pr(Y0  y0 | X) = F0 (y0 | X) and Pr(Y1  y1 | X) = F1 (y1 | X). The problem is to
construct Pr(Y0  y0 , Y1  y1 | X) = F (y0 , y1 | X). A related problem is how to construct the joint distribution of (Y0 , Y1 , D): F (y0 , y1 , d | X). We also consider methods
for bounding joint distributions. But first we answer the question, “Why bother”?
2.2. Why bother identifying joint distributions?
Given the intrinsic difficulty in identifying joint distributions of counterfactual outcomes, it is natural to ask, “why not settle for the marginals F0 (y0 | X) and F1 (y1 | X)?”
The methods surveyed in Chapter 71 afford identification of the marginal distributions.
Any method that can identify means or quantiles of distributions can be modified to
identify marginal distributions since E[1(Yj  yj ) | X] = Fj (yj | X), j = 0, 1.2
The literature on the measurement of economic inequality as surveyed by Foster and
Sen (1997) focuses on marginal distributions across different policy states. Invoking
the anonymity postulate, it does not keep track of individual fortunes across different
policy states. It does not decompose overall outcomes in each policy state, Y = DY1 +
(1 − D)Y0 , into their component parts Y1 , Y0 , attributable to treatment, and D due
to choice or assignment mechanisms. Thus, in comparing policies p and p ∈ P, it
compares the marginal distributions of
 p

p
Y p = D p Y1 + 1 − D p Y0 ,
where D p is the treatment choice indicator under policy p, and


 p
  p
Y p = D p Y1 + 1 − D p Y0
without seeking information on the subjective valuations of the policy change
or the
p
p
p
p
components of the treatment distributions under each policy (Y0 and Y1 ; Y0 and Y1 ).
2 Quantile methods [Chesher (2003), Koenker and Xiao (2002), Koenker (2005)] and many of the methods
surveyed in Chapter 73 (Matzkin) of this Handbook also recover these marginal distributions under appropriate assumptions.

5152

J.H. Abbring and J.J. Heckman


It compares FY p (y p | X) and FY p (y p | X) in making comparisons of welfare and
does not worry about the component distributions, subjective valuations of agents, or
any issues of self-selection.
Distinguishing the contributions of the component outcome distributions F0 (y0 ) and
F1 (y1 ) and the choice mechanisms is essential for understanding the channels through
which different policies operate [Carneiro, Hansen and Heckman (2001, 2003), and
Cunha and Heckman (2008)]. Throughout this section, we assume policy invariance for
outcomes, (PI-1) and (PI-2), in the notation of Chapter 70, unless otherwise noted.
Heckman, Smith and Clements (1997) and Heckman (1998) apply the concepts of
first and second order stochastic dominance used in the conventional inequality measurement literature to compare outcome distributions across treatment states within a
policy regime.3 The same methods can be used to compare treatment outcome distributions across policy states.4
Some economists appeal to classical welfare economics and classical decision theory
to argue that marginal distributions of treatment outcomes are all that is required to
implement the criteria used by these approaches. The argument is that under expected
utility maximization with information set I, the agent should be assigned to (choose)
treatment 1 if


E Υ (Y1 ) − Υ (Y0 ) | I  0,
where Υ is the preference function and I is the appropriate information set (that of
the social planner or the agent). To compute this expectation it is only necessary to
know F1 (y1 | I) and F0 (y0 | I), and not the full joint distribution F (y0 , y1 | I). For
many other criteria used in classical decision theory, marginal distributions are all that
is required.
As noted in Section 2.5 of Chapter 70, if one seeks to know the proportion of people
who benefit in terms of income from the program in gross terms (Pr(Y1  Y0 | I)), one
needs to know the joint distribution of (Y0 , Y1 ) given the appropriate information set.
Thus if one seeks to determine the proportion of agents who benefit from 1 compared
to 0, it is necessary to determine the joint distribution of (Y0 , Y1 ) unless information set
I is known to the econometrician and the agent uses the Roy model to make choices.
For the Roy model,
D = 1[Y1  Y0 ],
the probability of selecting treatment given the econometrician’s information set IE is
Pr(D = 1 | IE ) = Pr(Y1  Y0 | IE ).

3 Abadie (2002) develops standard errors for this method and presents additional references.
4 Under the conventional microeconometric partial equilibrium approach to policy evaluation surveyed in

Chapter 70, the marginal distributions of (Y0 , Y1 ) are invariant to the choice of the policy regime. This assumption is relaxed in our analysis of general equilibrium effects in Section 4.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5153

If there are no direct costs of participation, and agents participate in the program based
on self-selection under perfect certainty, and IE is both the econometrician’s and the
agent’s information set, then data on choices identify this proportion without need for
any further econometric analysis. See the discussion in Section 2.6 below. More generally, agents may use the generalized Roy model presented in Chapter 70, or some other
model, to make decisions, but the analyst seeks to know the proportion who gain ex post,
conditioning on a different information set. Individual choice data will not reveal this
probability if (a) agents do not use a Roy model formulated in ex post outcomes, (b) they
use a more general decision rule, or (c) the information set of the agent is different from
that of the econometrician. In these cases, further econometric analysis is required to
identify Pr(Y1  Y0 | I) for any particular information set.
Clearly the joint distribution of (Y0 , Y1 ) given I is required to compute the gain in
gross outcomes in general terms. In analyzing the option values of social programs and
the distribution of returns to schooling (e.g., (Y1 −Y0 )), in identifying dynamic discretechoice models (reviewed in Section 3), and in determining ex post regret, knowledge of
the full joint distribution of outcomes is required.
Section 2.10 presents examples of the richer, more nuanced, approach to policy evaluation that is possible when the analyst has access to the joint distribution of outcomes
across counterfactual states. We show how the tools presented in this section allow
economists to move beyond the limitations of the anonymity postulate to consider who
benefits and who loses from policy reforms. We present estimates of the proportion of
people who have ex post regret about their schooling choices and estimates of the ex
0
) which inherently require
ante and ex post distributions of returns to schooling ( Y1Y−Y
0
knowledge of the joint distribution of outcomes across states. We now turn to methods
for identifying or bounding joint distributions.
2.3. Solutions
There are two basic approaches in the literature to solving the problem of identifying
F (y0 , y1 | X): (A) solutions that postulate assumptions about dependence between
Y0 and Y1 and (B) solutions based on information from agent participation rules and
additional data on choice. Recently developed methods build on these two basic approaches and combine choice theory with supplementary data and assumptions about
the structure of dependence among model unobservables. We survey all of these methods. In addition to methods for exact identification, Fréchet bounds can be placed on
the joint distributions from knowledge of the marginals [see, e.g., Heckman and Smith
(1993, 1995), Heckman, Smith and Clements (1997), Manski (1997)]. We first consider
these bounds.
2.4. Bounds from classical probability inequalities
The problem of bounding an unknown joint distribution from known marginal distributions is a classical problem in mathematical statistics. Hoeffding (1940) and Fréchet

5154

J.H. Abbring and J.J. Heckman

(1951) demonstrate that the joint distribution is bounded by two functions of the marginal distributions. Their inequalities state that


max F0 (y0 | X) + F1 (y1 | X) − 1, 0  F (y0 , y1 | X)


 min F0 (y0 | X), F1 (y1 | X) .5
To simplify the notation, we keep conditioning on X implicit in the remainder of this
section. Rüschendorf (1981) establishes that these bounds are tight.6 Mardia (1970)
establishes that both the lower bound and the upper bound are proper probability distributions. At the upper bound, Y1 = F1−1 (F0 (Y0 )) is a non-decreasing deterministic
function of Y0 . At the lower bound, Y1 is a non-increasing deterministic function of Y0 :
Y1 = F1−1 (1 − F0 (Y0 )).
By a theorem of Cambanis, Simons and Stout (1976), if k(y1 , y0 ) is superadditive
(or subadditive), then extreme values of E(k(Y1 , Y0 )) are obtained from the upper and
lower bounding distributions.7,8 Since k(y1 , y0 ) = (y1 − E(Y1 ))(y0 − E(Y0 )) is superadditive, the maximum attainable product-moment correlation rY0 Y1 is obtained from
the upper bound distribution while the minimum attainable product moment correlation
is obtained at the lower bound distribution. Let  = Y1 − Y0 . It is possible to bound
Var() = (Var(Y1 ) + Var(Y0 ) − 2rY0 Y1 [Var(Y1 ) Var(Y0 )]1/2 ) with the minimum obtained from the Fréchet–Hoeffding upper bound.9 Checking whether the lower bound
of Var() is statistically significantly different from zero provides a test of whether or
not the data are consistent with the common effect model. For example, if Y1 − Y0 = β,
a constant, Var() = 0.
Tchen (1980) establishes that Kendall’s τ and Spearman’s ρ also attain their extreme values at the bounding distributions. The upper and lower bounding distributions
produce the cases of perfect positive dependence and perfect negative dependence, respectively. Often the bounds on the quantiles of the  distribution obtained from the
Fréchet–Hoeffding bounds are very wide.10 Table 1 presents the range of values of rY0 Y1 ,

5 King (1997) applies these inequalities to solve the problem of ecological correlation. These inequalities
are used in the missing data literature for contingency tables [see, e.g., Bishop, Fienberg and Holland (1975)].
6 An upper bound is “tight” if it is the smallest possible upper bound. A lower bound is tight if it is the
largest lower bound.
7 k is assumed to be Borel-measurable and right-continuous. k is strictly superadditive if y > y  and
1
1
y0 > y0 imply that k(y1 , y0 ) + k(y1 , y0 ) > k(y1 , y0 ) + k(y1 , y0 ). k is strictly subadditive if the final
inequality is reversed.
8 An interesting application of the analysis of Cambanis, Simons and Stout (1976) is to the assignment
problem studied by Koopmans and Beckmann (1957) and Becker (1974). If total output of a match k(y0 , y1 )
is superadditive, as it is in the Cobb–Douglas model (k(y0 , y1 ) = y0 y1 ), then the optimal sorting rule is
obtained by the upper bound of the Fréchet distribution.
9 Note that the maximum value of r
Y0 Y1 is obtained at the upper bound and that all other components of
the variance of  are obtained from the marginal distributions. Thus the minimum variance of  is obtained
from the Fréchet–Hoeffding upper bound distribution.
10 See the examples in Heckman and Smith (1993).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5155

Table 1
Characteristics of the distribution of impacts on earnings in the 18 months after random assignment
at the Fréchet–Hoeffding bounds (National JTPA Study 18 month impact sample: adult females)
Statistic

From lower
bound distribution

From upper
bound distribution

Impact standard deviation

14968.76
(211.08)
−0.760
(0.013)
−0.9776
(0.0016)

674.50
(137.53)
0.998
(0.001)
0.9867
(0.0013)

Outcome correlation
Spearman’s ρ

Notes: 1. These estimates were obtained using the empirical c.d.f.s calculated at 100 dollar earnings
intervals rather than using the percentiles of the two c.d.f.s.
2. Bootstrap standard errors in parentheses.
Source: Heckman, Smith and Clements (1997).

Spearman’s ρ and [Var()]1/2 for the Job Training Partnership Act (JTPA) data analyzed in Heckman, Smith and Clements (1997).11 The ranges are rather wide, but it is
interesting to observe that the bounds rule out the common effect model, as Var() is
bounded away from zero.
The Fréchet–Hoeffding bounds apply to all joint distributions.12 The outcome variables may be discrete, continuous or both discrete and continuous. It is fruitful to
consider the bounds for this model with binary outcomes to establish the variability
in the distribution of impacts for a discrete variable such as employment. For specificity, we analyze the employment data from the JTPA experiment reported in Heckman,
Smith and Clements (1997). The data are multinomial.13 Let (E, E) denote the event
“employed with treatment” and “employed without treatment” and let (E, N ) be the
event “employed with treatment, not employed without treatment.” Similarly, (N, E)
and (N, N ) refer respectively to cases where a person would not be employed if treated
but would be employed if not treated, and where a person would not be employed in
either case. The probabilities associated with these events are PEE , PEN , PN E and
PN N , respectively. This model can be written in the form of a contingency table. The
columns refer to employment and nonemployment in the untreated state. The rows refer
to employment and nonemployment in the treated state.

11 Heckman, Smith and Clements (1997) discuss the properties of the estimates of the standard errors reported

in Table 1. JTPA was a job training program in place in the US in the 1980s and 1990s.
12 Formulae for multivariate bounds are given in Tchen (1980) and Rüschendorf (1981).
13 The following formulation owes a lot to the missing cell literature in contingency table analysis. See, e.g.,

Bishop, Fienberg and Holland (1975).

5156

J.H. Abbring and J.J. Heckman

Untreated

Treated

E
N

E
PEE
PN E
P·E

N
PEN
PN N
P·N

PE·
PN·

If we observed the same person in both the treated and untreated states, we could fill
in the table and estimate the full distribution. With experimental data or data corrected
for selection using the methods discussed in Chapter 71, one can estimate the marginals
of the table parameters:
PE· = PEE + PEN

(employment proportion among the treated),

(2.1a)

P·E = PEE + PN E

(employment proportion among the untreated).

(2.1b)

The treatment effect is usually defined as
 = PEN − PN E .

(2.2)

This is the proportion of people who would switch from being nonemployed to being
employed as a result of treatment minus the proportion of persons who would switch
from being employed to not being employed as a result of treatment. Using (2.1a) and
(2.1b), we obtain the treatment effect as
 = PE· − P·E ,

(2.3)

so that  is identified by subtracting the proportion employed in the control group (P̂·E )
from the proportion employed in the treatment group (P̂E· ).
If we wish to decompose  into its two components, experimental data or selectioncorrected data do not in general give an exact answer. In terms of the contingency table
presented above, we know the row and column marginals but not the individual elements
in the table. The case in the 2 × 2 table corresponding to the common effect model for
continuous outcomes restricts the effect of the program on employment to be always
positive or always negative, so that either PEN or PN E = 0, respectively. Under such
assumptions, the model is fully identified. This is analogous to the continuous case
in which the common effect assumption, or more generally, an assumption of perfect
positive dependence, identifies the joint distribution of outcomes.
More generally, the Fréchet–Hoeffding bounds restrict the range of admissible values
for the cell probabilities. Their application in this case produces:
max[PE· + P·E − 1, 0]  PEE  min[PE· , P·E ],
max[PE· − P·E , 0]  PEN  min[PE· , 1 − P·E ],
max[−PE· + P·E , 0]  PN E  min[1 − PE· , P·E ],
max[1 − PE· − P·E , 0]  PN N  min[1 − PE· , 1 − P·E ].

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5157

Table 2
Fraction employed in the 16th, 17th or 18th month after random assignment and
Fréchet–Hoeffding bounds on the probabilities PN E and PEN (National JTPA
study 18 month impact sample: adult females)
Parameter

Estimate

Fraction employed in the treatment group

0.64
(0.01)
0.61
(0.01)
[0.03, 0.39]
(0.01), (0.01)
[0.00, 0.36]
(0.00), (0.01)

Fraction employed in the control group
Bounds on PEN
Bounds on PN E

Notes: 1. Employment percentages are based on self-reported employment in
months 16, 17 and 18 after random assignment. A person is coded as employed if the sum of their self-reported earnings over these three months is
positive.
2. Pij is the probability of having employment status i in the treated state and
employment status j in the untreated state, where i and j take on the values E
for employed and N for not employed. The Fréchet–Hoeffding bounds are given
in the text.
3. Standard errors are discussed in Heckman, Smith and Clements (1997).
Source: Heckman, Smith and Clements (1997).

Table 2, taken from the analysis of Heckman, Smith and Clements (1997), presents the
Fréchet–Hoeffding bounds for PN E and PEN from the national JTPA experiment—the
source of data for Table 1. The outcome variable is whether or not a person is employed
in the 16th, 17th or 18th month after random assignment. The bounds are very wide.
Even without taking into account sampling error, the experimental evidence for adult
females is consistent with a value of PN E ranging from 0.00 to 0.36. The range for
PEN is equally large. Thus as many as 39% and as few as 3% of adult females may
have had their employment status improved by participating in the training program.
As many as 36% and as few as 0% may have had their employment status harmed by
participating in the program. From (2.2), we know that the net difference PEN −PN E =
, so that high values of PEN are associated with high values of PN E . As few as 25%
[(0.64 − 0.39) × 100%] and as many as 61% of the women would have worked whether
or not they entered the program (PEE ∈ [0.25, 0.61]).
From the evidence presented in Table 2, one cannot distinguish two different stories. The first story is that the JTPA program benefits many people by facilitating their
employment but it also harms many people who would have worked if they had not
participated in the program. The second story is that the program benefits and harms

5158

J.H. Abbring and J.J. Heckman

few people.14 Heckman, Smith and Clements (1997) and Manski (1997, 2003) develop
these bounds further. We next consider methods to point identify the joint distributions
of outcomes. All entail using some auxiliary information.
2.5. Solutions based on dependence assumptions
A variety of approaches solve the problem of identifying the joint distribution of potential outcomes by making dependence assumptions connecting Y0 and Y1 . We review
some of the major approaches.
2.5.1. Solutions based on conditional independence or matching
An approach based on matching postulates access to variables Q that have the property
that conditional on Q, F0 (y0 | D = 0, X, Q) = F0 (y0 | X, Q) and F1 (y1 | D = 1,
X, Q) = F1 (y1 | X, Q). As discussed in Section 9 of Chapter 71, matching assumes
that conditional on observed variables, Q, there is no selection problem: (Y0 ⊥
⊥D |
X, Q) and (Y1 ⊥
⊥ D | X, Q). If it is further assumed that all of the dependence between
(Y0 , Y1 ) given X comes through Q, it follows that
F (y1 , y0 | X, Q) = F1 (y1 | X, Q)F0 (y0 | X, Q).
Using these results, it is possible to identify the joint distribution F (y0 , y1 | X) because

F (y0 , y1 | X) = F0 (y0 | X, Q)F1 (y1 | X, Q) dμ(Q | X),
where μ(Q | X) is the conditional distribution of Q given X. Under the assumption
that we observe X and Q, this conditional distribution can be constructed from data.
We obtain F0 (y0 | X, Q), F1 (y1 | X, Q) by matching. Thus we can construct the righthand side of the preceding expression. As noted in Chapter 71, matching makes the
strong assumption that conditional on (Q, X) the marginal return to treatment is the
same as the average return, although returns may differ by the level of Q and X.
2.5.2. The common coefficient approach
The traditional approach in economics to identifying joint distributions is to assume
that the joint distribution F (y0 , y1 | X) is a degenerate, one dimensional distribution.
Conditional on X, Y0 and Y1 are assumed to be deterministically related:
Y1 − Y0 = ,

(2.4)

14 Heckman, Smith and Clements (1997) show that conditioning on other background variables does not

reduce the intrinsic uncertainty in the data. Thus in both the discrete and continuous cases, the data from the
JTPA experiment are consistent with a wide variety of impact distributions.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5159

where  is a constant given X. It is the difference in means between Y1 and Y0 for
the selection corrected distribution.15 This approach assumes that treatment has the
same effect on everyone (with the same X), and that the effect is . Because (2.4)
implies a perfect ranking across quantiles of the outcome distributions Y0 and Y1 ,
 can be identified from the difference in the quantiles between Y0 and Y1 for any
quantile. Even if the means do not exist, one can still identify . From knowledge
of F0 (y0 | X) and F1 (y1 | X), one can identify the means and quantiles. Hence one can
identify .
2.5.3. More general dependence assumptions
Heckman, Smith and Clements (1997) and Heckman and Smith (1998) relax the common coefficient assumption by postulating perfect ranking in the positions of individuals
in the F1 (y1 | X) and F0 (y0 | X) distributions. The best in one distribution is the best
in the other. Assuming continuous and strictly increasing marginal distributions, they
postulate that quantiles are perfectly ranked so Y1 = F1−1 (F0 (Y0 )). This is the tight upper bound of the Fréchet bounds. An alternative assumption is that people are perfectly
inversely ranked so the best in one distribution is the worst in the other:


Y1 = F1−1 1 − F0 (Y0 ) .
This is the tight Fréchet lower bound.
One can associate quantiles across the marginal distributions more generally.
Heckman, Smith and Clements (1997) use Markov transition kernels that stochastically map quantiles of one distribution into quantiles of another. They define a pair of
Markov kernels M(y1 , y0 | X) and M̃(y0 , y1 | X) with the property that they map
marginals into marginals:

F1 (y1 | X) = M(y1 , y0 | X) dF0 (y0 | X),

F0 (y0 | X) = M̃(y0 , y1 | X) dF1 (y1 | X).
Allowing these kernels to be degenerate produces a variety of deterministic transformations, including the two previously presented, as special cases of a general
mapping. Different (M, M̃) pairs produce different joint distributions. These transformations supply the missing information needed to construct the joint distributions.16
15  may be a function of X.
16 For given marginal distributions F and F , we cannot independently pick M and M̃. Consistency requires
0
1

that

 y
0
−∞

M(y1 , y | X) dF0 (y | X) =

for all y0 , y1 .

 y
1
−∞

M̃(y0 , y | X) dF1 (y | X),

5160

J.H. Abbring and J.J. Heckman

A perfect ranking (or perfect inverse ranking) assumption generalizes the perfect
ranking, constant-shift assumptions implicit in the conventional literature. It allows analysts to apply conditional quantile methods to estimate the distributions of gains.17
However, it imposes a strong and arbitrary dependence across distributions. Lehmann
and D’Abrera (1975), Robins (1989, 1997), Koenker and Xiao (2002), and many others
maintain this assumption under the rubric of “rank invariance” in order to identify the
distribution of treatment effects.
Table 3 shows the percentiles of the earnings impact distribution (F (y1 − y0 )) for
females in the National JTPA experiment under various assumptions about dependence
Table 3
Percentiles of the impact distribution as ranking across distributions (τ ) varies based on random samples of
50 permutations with each value of τ (National JTPA study 18 month impact sample: adult females)
Measure of rank Minimum
correlation τ
1.00
0.95
0.90
0.70
0.50
0.30
0.00
−0.30
−0.50
−0.70
−0.90
−0.95
−1.00

0.00
(703.64)
−14504.00
(1150.01)
−18817.00
(1454.74)
−25255.00
(1279.50)
−28641.50
(1149.22)
−32621.00
(1843.48)
−44175.00
(2372.05)
−48606.00
(1281.80)
−48606.00
(1059.06)
−48606.00
(1059.06)
−48606.00
(1059.06)
−48606.00
(1059.06)
−48606.00
(1059.06)

5th
percentile

25th
percentile

0.00
(47.50)
0.00
(360.18)
−1168.00
(577.84)
−8089.50
(818.25)
−12037.00
(650.31)
−14855.50
(548.48)
−18098.50
(630.73)
−20566.00
(545.99)
−21348.00
(632.55)
−22350.00
(550.00)
−22350.00
(547.17)
−22350.00
(547.17)
−22350.00
(547.17)

572.00
(232.90)
125.50
(124.60)
0.00
(29.00)
−136.00
(260.00)
−1635.50
(314.39)
−3172.50
(304.62)
−6043.00
(300.47)
−8918.50
(286.92)
−9757.50
(351.55)
−10625.00
(371.38)
−11381.00
(403.30)
−11559.00
(404.67)
−11755.00
(411.83)

50th
percentile

75th
percentile

95th
percentile

Maximum

864.00
(269.26)
616.00
(280.19)
487.00
(265.71)
236.50
(227.38)
0.00
(83.16)
0.00
(37.96)
0.00
(163.17)
779.50
(268.02)
859.00
(315.37)
581.50
(309.84)
580.00
(346.12)
580.00
(366.37)
580.00
(389.51)

966.00
(305.74)
867.00
(272.60)
876.50
(282.77)
982.50
(255.78)
1362.50
(249.29)
4215.50
(244.67)
7388.50
(263.25)
9735.50
(300.59)
10550.50
(255.28)
11804.50
(246.58)
12545.00
(251.07)
12682.00
(255.97)
12791.00
(253.18)

2003.00
(543.03)
1415.50
(391.51)
2319.50
(410.27)
12158.50
(614.45)
16530.00
(329.44)
16889.00
(423.05)
19413.25
(423.63)
21093.25
(462.13)
22268.00
(435.78)
23351.00
(520.93)
23351.00
(341.41)
23351.00
(341.41)
23351.00
(341.41)

18550.00
(5280.67)
48543.50
(8836.49)
49262.00
(6227.38)
55169.50
(5819.28)
58472.00
(5538.14)
54381.00
(5592.86)
60599.00
(5401.02)
65675.00
(5381.91)
67156.00
(5309.90)
67156.00
(5309.90)
67156.00
(5309.90)
67156.00
(5309.90)
67156.00
(5309.90)

(continued on next page)

17 See, e.g., Heckman, Smith and Clements (1997).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5161

Table 3
(continued)
Notes: 1. This table shows selected percentiles of the empirical distribution of Y1 − Y0 under different assumptions about the dependence of Y1 and Y0 . The empirical distribution of Y1 − Y0 for each indicated value
of Kendall’s rank correlation τ is constructed by pairing the percentiles of the empirical distributions of Y0
and Y1 in a way consistent with the value of τ . There are J ! ways of pairing the J = 100 percentiles of both
marginal distributions, each corresponding to lining up the Y0 percentiles to one of the J ! permutations of the
Y1 percentiles. First, consider the two extreme cases, τ = 1 and τ = −1. If the percentiles of Y0 are assigned
to the corresponding percentile of Y1 , then the rank correlation τ between the percentiles among the resulting
J pairs equals 1. The difference between the percentile of Y1 and the associated percentile of Y0 in each pair
is the impact for that pair. Taken together, the J pairs’ impacts form the distribution of impacts for τ = 1. It
is the minimum, maximum and percentiles of this impact distribution that are reported in the first row of the
table. If the percentile comparisons are based on pairing the biggest in one distribution with the smallest in
the other distribution, then τ = −1. Computations for τ = −1 are reported in the table’s last row.
Intermediate values of τ are obtained by considering pairings of percentiles with a specified number of inversions in the ranks. An inversion is said to arise if, among two pairs of quantiles, a lower Y0 quantile is
matched with a higher Y1 quantile. For a given pairing of percentiles (permutation of the Y1 percentiles) the
total number of inversions is


(j )
(i)
η=
hij , hij = 1, Y1 > Y1 ,
0
j i<j
(j )

where Y1 is the percentile of Y1 associated with the j th percentile of Y0 . The value of η ranges from 0
(corresponding to perfect positive rank correlation) to 12 J (J −1) (perfect negative rank correlation). Kendall’s
rank correlation measure τ is
τ =1−

4η
,
J (J − 1)

where τ ∈ [−1, 1].

There are multiple pairings of percentiles consistent with each intermediate value of τ (number of inversions η), unlike in the cases of τ = 1 and τ = −1. Therefore, for intermediate values of τ the table reports
the mean of the indicated parameters of the impact distribution over a random sample of 50 pairings having
the indicated value of τ .
2. Bootstrap standard errors in parentheses.
Source: Heckman, Smith and Clements (1997).

between Y1 and Y0 . The experiment identified F1 (y1 ) and F0 (y0 ) separately. The table
reports selected percentiles of the estimated impact distributions for different assumed
levels of dependence, τ (Kendall’s rank correlation). As shown in the first footnote to
the table, τ = 1 corresponds to the Fréchet upper bound. τ = −1 corresponds to
the Fréchet lower bound. Since without further information in hand, the joint distribution is not identified, the data are consistent with all values of τ and so each row
of the table is a possible outcome distribution. Notice that the medians (50th percentile) are reasonable, but many percentiles are not. Heckman, Smith and Clements
(1997) suggest that prior information about plausible outcomes, possibly formalized
by a Bayesian analysis, can be used to pick reasonable values of τ . We next consider
alternative deconvolution assumptions that can be used to point identify the joint distributions.

5162

J.H. Abbring and J.J. Heckman

2.5.4. Constructing distributions from assuming independence of the gain from the
base
An alternative assumption about the dependence across outcomes is that Y1 = Y0 + ,
where , the treatment effect, is a random variable stochastically independent of Y0
given X, i.e.,
⊥  | X.
(CON-1) Y0 ⊥
This assumption states that the gain from participating in the program is independent of
the base Y0 . If we assume
(M-1) (Y0 , Y1 ) ⊥
⊥ D | X,
and (CON-1), we can identify F (y0 , y1 | X) from the cross-section outcome distributions of participants and nonparticipants and estimate the joint distribution by using
deconvolution.18 Methods for using this information are presented in Appendix A.
Horowitz and Markatou (1996) develop the asymptotic properties of convolution estimators with regression building on the work of Stefanski and Carroll (1991). Heckman,
Smith and Clements (1997) and Heckman and Smith (1998) use deconvolution to analyze the distribution of gains from the JTPA data. Neither (CON-1) nor (M-1) is an
attractive assumption from the point of view of economic choice models. (M-1) implies
that marginal entrants into a social program have the same return as average participants.
The assumption (CON-1) is not a prediction of general choice models.
2.5.5. Random coefficient regression approaches
In a regression setting in which means and variances are assumed to capture all of
the relevant information about the distributions of outcomes and treatment effects, the
convolution approach discussed in the preceding section is equivalent to the traditional
normal random coefficient model. Letting
Y1 = μ1 (X) + U1 ,

E(U1 | X) = 0,

Y0 = μ0 (X) + U0 ,

E(U0 | X) = 0,

this version of the model may be written as


Y = μ0 (X) + μ1 (X) − μ0 (X) + U1 − U0 D + U0
β(X)

18 Barros (1987) uses this assumption in the context of an analysis of selection bias.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5163



= μ0 (X) + μ1 (X) − μ0 (X) D + (U1 − U0 )D + U0
= μ0 (X) + β̄(X)D + υD + U0 ,

(2.5)

where in the notation of Chapter 71, β(X) is the treatment effect (= ), β̄(X) =
μ1 (X) − μ0 (X), and υ = U1 − U0 . From (M-1), (U0 , U1 ) ⊥
⊥ D | X.
Nonparametric regression methods may be used to recover μ0 (X) and μ1 (X) −
μ0 (X) or one may use ordinary parametric regression methods if one assumes that
μ1 (X) = Xβ1 and μ0 (X) = Xβ0 . Equation (2.5) is a components-of-variance model
and a test of (CON-1) given (M-1) is that
Var(Y | D = 1, X) = Var(Y0 +  | D = 1, X)
= Var(Y0 | X) + Var( | X)
 Var(Y | D = 0, X) = Var(Y0 | X).
Under standard conditions, each component of variance is identified and estimable from
the residuals obtained from the nonparametric regression of Y on D and X. Thus one
can jointly test a prediction of (CON-1) and (M-1) by checking these inequalities.
2.6. Information from revealed preference
An alternative approach, rooted more deeply in economics, uses information on agent
choices to recover the joint population distribution of potential outcomes.19 Unlike the
method of matching or the methods based on particular assumptions about dependence
between Y0 and Y1 , the method based on revealed preference capitalizes on a close
relationship between (Y0 , Y1 ) and decisions about program participation. Participation
includes voluntary entry into a program or attrition from it.
The prototypical framework is the Roy (1951) model extensively utilized in
Chapters 70 and 71. In that setup, as previously noted in Section 2.2,
D = 1[Y1  Y0 ].

(2.6)

If we postulate that the outcome equations can be written in a separable form, so that
Y1 = μ1 (X) + U1 ,

E(U1 | X) = 0,

Y0 = μ0 (X) + U0 ,

E(U0 | X) = 0,

then Pr(D = 1 | X) = Pr(Y1 − Y0  0 | X) = Pr(U1 − U0  −(μ1 (X) − μ0 (X))).
Heckman and Honoré (1990) demonstrate that if X ⊥
⊥ (U0 , U1 ), Var(U0 ) < ∞ and
Var(U1 ) < ∞, and (U0 , U1 ) are normal, the full model F (y0 , y1 , D | X) is identified even if we only observe Y0 or Y1 for any person and there are no regressors and
19 Heckman (1974a, 1974b) demonstrates how access to censored samples on hours of work, wages for

workers, and employment choices identifies the joint distribution of the value of nonmarket time and potential
market wages under a normality assumption. Heckman and Honoré (1990) consider nonparametric versions
of this model without labor supply.

5164

J.H. Abbring and J.J. Heckman

no exclusion restrictions. If instead of assuming normality, it is assumed that the support of (μ0 (X), μ1 (X)) contains the support of (U0 , U1 ), (μ0 (X), μ1 (X)) and the joint
distribution of (U0 , U1 ) are nonparametrically identified up to location normalizations.
The proof of this theorem due to Heckman and Honoré (1990) is a special case of the
general theorem proved in Appendix B of Chapter 70.
A crucial feature of the Roy model is that the decision to participate in the program
is made solely in terms of potential outcomes. No new unobserved variables enter the
model that do not also appear in the outcome equations (Y0 , Y1 ). We could augment
decision rule (2.6) to be D = 1[Y1 − Y0 − μC (Z)  0], where μC (Z) is the cost
of participation in the program and Z is observed, and still preserve the identifiability
of the Roy model. Provided that we measure Z and condition on it, and provided that
⊥ (X, Z), the model remains nonparametrically identified. The crucial prop(U0 , U1 ) ⊥
erty of the identification result is that no new unobservable enters the model through the
participation equation. However, if we add components of cost based on observables,
subjective valuations of gain (Y1 − Y0 − μC (Z)) no longer equal “objective” measures
(Y1 − Y0 ). This is the distinction between the generalized Roy model and the extended
Roy model extensively discussed in Chapter 71.
In the case of the Roy model, information about who participates in the program also
informs the analyst about the distribution of the value of the program to participants
F (y1 − y0 | Y1  Y0 , X). Thus, we acquire the distribution of implicit values of
the program for participants. In the Roy model, “objective” and “subjective” outcomes
coincide and agent’s choices are informative on the outcome not chosen.
For more general decision rules with additional sources of unobservables apart from
those arising from (Y0 , Y1 ), it is not generally possible to identify F (y0 , y1 ) from information on (Y, D, X, Z) without invoking additional assumptions. For the generalized
Roy model,
D = 1[Y1 − Y0 − C  0],
where, for example,
C = μC (Z) + UC .
Let UI = U1 − U0 − UC , I = Y1 − Y0 − C and μI (X, Z) = μ1 (X) − μ0 (X) − μC (Z).
Define P (X, Z) = Pr(D = 1 | X, Z). If UC is not perfectly predicted by (U0 , U1 ), then
we cannot, in general, estimate the joint distribution of (Y0 , Y1 , C) given (X, Z) or the
distribution of (U0 , U1 , UC ) from data on Y , D, X and Z.
However, under the conditions in Appendix B of Chapter 70, we can identify up to
an unknown scale for I , FY0 ,I (y0 , i | X, Z) and FY1 ,I (y1 , i | X, Z).20 The following
intuition motivates the conditions under which FY0 ,I (y0 , i | X, Z) is identified. A parallel argument holds for FY1 ,I (y1 , i | X, Z). First, under the conditions given in Cosslett
(1983), Manski (1988), Matzkin (1992) and Appendix B of Chapter 70, we can identify
20 In our application of that theorem, there are only two choices so S̄ = 2 in the notation of that theorem.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5165

μI (X,Z)
σUI

from Pr(D = 1 | X, Z) = Pr(μI (X, Z) + UI  0 | X, Z). σU2 I is the variance
of UI . We can also identify the distribution of σUUI . Second, from this information and
I
F0 (y0 | D = 0, X, Z) = Pr(Y0  y0 | μI (X, Z) + UI < 0, X, Z), we can form
F0 (y0 | D = 0, X, Z) Pr(D = 0 | X, Z) = Pr(Y0  y0 , I < 0 | X, Z).
The left-hand side of this expression is known (we observe Y0 when D = 0 and we
know the probability that D = 0 given X, Z). The right-hand side can be written as

UI
μI (X, Z) 
<−
Pr Y0  y0 ,
 X, Z .
σUI
σUI
In particular if μI (X, Z) can be made arbitrarily small (μI (X, Z) → −∞), for a
given X, we can recover the marginal distribution Y0 from which we can recover μ0 (X),
and hence the distribution of U0 .
From the definition of Y0 , U0 = Y0 − μ0 (X). We may write the preceding probability
as

UI
−μI (X, Z) 
<
Pr U0  y0 − μ0 (X),
 X, Z .
σUI
σUI
Note that the X and Z can be varied and y0 is a number. Thus, by varying the known y0
and μIσ(X,Z)
, we can trace out the joint distribution of (U0 , σUUI ). Thus we can recover
UI
I
the joint distribution of

μI (X, Z) + UI
(Y0 , I ) = μ0 (X) + U0 ,
.
σUI
Notice the three key ingredients required to recover the joint distribution:
(a) The independence between (U0 , UI ) and (X, Z).
arbitrarily small for a given X (so we
(b) The assumption that we can make μIσ(X,Z)
UI
get the marginal distribution of Y0 and hence μ0 (X)). As noted in Chapter 71, this
type of identification-at-infinity assumption plays a key role in the entire selection
and evaluation literature for identifying many important evaluation parameters,
such as the average treatment effect and treatment on the treated.
can be varied independently of μ0 (X). This enables
(c) The assumption that μIσ(X,Z)
U
I

us to trace out the joint distribution of (U0 , σUUI ).21
I

21 Another way to see how identification works is to note that from Cosslett (1983), Manski (1988), Matzkin

(1992) and ingredients (a) and (b), we can express
F0 (y0 | D = 0, X, Z) Pr(D = 0 | X, Z)
μ (X,Z)

as a function of μ0 (X) and Iσ
. The dependence on X and Z operating only through the indices
UI
μ (X,Z)
μ (X,Z)
μ0 (X) and Iσ
is called index sufficiency. Varying the μ0 (X) and Iσ
traces out the distribution
U

of (U0 , σ I ).
UI

UI

UI

5166

J.H. Abbring and J.J. Heckman

A parallel argument establishes identification of the distribution of (Y1 , I ) given X
and Z.
Identification of the Roy model follows from this analysis. Recall that the model assumes that UI = U1 − U0 so σU2 I = Var(U1 − U0 ). From the distributions of (Y0 , I )

0
and (Y1 , I ), given X and Z, we can recover the joint distributions of (U0 , U1σ−U
) and
U
I

0
) and hence the joint distribution of (U0 , U1 ). We can recover the joint dis(U1 , U1σ−U
UI
tribution of U1 − U0 even if μI (X, Z) = μ1 (X) − μ0 (X) as long as UC ≡ 0.

2.7. Using additional information
We have established that data from social experiments or observational data corrected
for selection do not in general identify joint distributions of potential outcomes. In the
special case of the Roy model, choice data supplemented with outcome data will identify the joint distribution. But this result is fragile. For more general choice criteria, we
cannot without further assumptions identify the joint distribution of potential outcomes.
Recent approaches build on these results to supplement choice models with dependence
assumptions to identify the joint distribution of (U0 , U1 ).
Aakvik, Heckman and Vytlacil (2005), Carneiro, Hansen and Heckman (2001, 2003),
Cunha, Heckman and Navarro (2005, 2006), and Cunha and Heckman (2007b, 2008)
use factor models to capture the dependence across the unobservables (U0 , U1 , UI )
and to supplement the information used in order to construct the joint distribution
of counterfactuals. Their approach is a version of the proxy/replacement function approach developed in Heckman and Robb (1985, 1986) that is discussed in Section 10
of Chapter 71 and in Chapter 73 (Matzkin) of this Handbook. It extends factor models
developed by Jöreskog and Goldberger (1975) and Jöreskog (1977) to restrict the dependence among the (U0 , U1 , UI ). A low dimensional set of random variables generates
the dependence across the outcome unobservables. Such dimension reduction coupled
with the use of choice data and additional measurements that proxy or replace the factors can provide enough information to identify the joint distributions of (Y0 , Y1 ) and
(Y0 , Y1 , D).
The factor models are built around a conditional-independence assumption. Conditional on the factors, outcomes and choice equations are independent. Thus the factor
models have a close affinity with matching except that they do not assume that the
analyst observes the factors and must instead integrate them out and identify their distribution.
To demonstrate how this approach works, assume separability between observables
and unobservables:
Y1 = μ1 (X) + U1 ,
Y0 = μ0 (X) + U0 .
Denote I as the latent variable generating treatment choices:
I = μI (Z) + UI ,

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5167

D = 1[I  0].
Allow any X to be in Z so the notation is general.
To understand this approach, it is convenient but not essential to assume that
(U0 , U1 , UI ) is normally distributed with mean zero and covariance matrix Σ. Normality plays no essential role in the analysis of this section. The key role is played by the
factor structure assumption introduced below. Assume access to data on (Y, D, X, Z).
We can identify F0 (y0 | D = 0, X, Z), F1 (y1 | D = 1, X, Z) and Pr(D = 1 | X, Z).
Under certain conditions presented in Appendix B, Chapter 70 and the preceding section, we can identify the distributions of (U0 , σUUI ) and (U1 , σUUI ) nonparametrically. We
I
I
can sometimes identify the scale on UI .
To restrict the dependence across the unobservables, we adopt a factor structure
model for the U0 , U1 , UI . Other restrictions across the unobservables are possible.
Models for a single factor are extensively developed by Jöreskog and Goldberger
(1975). Aakvik, Heckman and Vytlacil (2005) and Carneiro, Hansen and Heckman
(2001, 2003) extend their analysis to generate distributions of counterfactuals.
Initially assume a one-factor model where θ is a scalar factor (say unmeasured ability) that generates dependence across the unobservables assumed to be independent of
(X, Z):
U0 = α0 θ + ε0 ,
U1 = α1 θ + ε1 ,
UI = αUI θ + εUI ,
θ⊥
⊥ (ε0 , ε1 , εUI ),

(ε0 , ε1 , εUI ) are mutually independent.

We discuss methods for multiple factors in the next section. Assume that E(U0 ) = 0,
E(U1 ) = 0 and E(UI ) = 0. In addition, E(θ ) = 0. Thus E(ε0 ) = 0, E(ε1 ) = 0 and
E(εUI ) = 0. To set the scale of the unobserved factor, normalize one “loading” (coefficient on θ) to 1. Note that all the dependence in the unobservables across equations
arises from θ.
From the joint distributions of (U0 , σUUI ) and (U1 , σUUI ) we can identify
I
I

UI
α0 αUI 2
σ ,
=
Cov U0 ,
σUI
σUI θ

UI
α1 αUI 2
Cov U1 ,
σ ,
=
σUI
σUI θ
assuming that the covariances on the left-hand side exist. From the ratio of the second
covariance to the first we obtain αα10 . Thus we obtain the sign of the dependence between
U0 , U1 because
Cov(U0 , U1 ) = α0 α1 σθ2 .
From the ratio, we obtain α1 if we normalize α0 = 1. Without further information, we
cannot identify the variance of UI , σU2 I . We normalize it to 1. (Alternatively, we could

5168

J.H. Abbring and J.J. Heckman

normalize the variance of εUI to 1.) Below, we present a condition that sets the scale
of UI .
With additional information, one can identify the full joint distribution of (U0 , U1 , UI )
and hence can construct the joint distribution of potential outcomes. In this section, we
show this by a series of examples for a normal model. In a normal model, the joint
distribution of (Y0 , Y1 ) is determined (given X) if one can identify the variances of Y0
and Y1 and their covariance. We then show that normality plays no essential role in this
analysis. We first consider what can be identified from access to a proxy M for θ (e.g.,
a test score).
2.7.1. Some examples
E XAMPLE 1 (Access to a single proxy measure (e.g., a test score)). Assume access to
data on Y0 given D = 0, X, Z; to data on Y1 given D = 1, X, Z; and to data on D
given X, Z. Suppose that the analyst also has access to a proxy for θ . Denote the proxy
measure by M. In a schooling example, it could be a test score:
M = μM (X) + UM ,
where
UM = αM θ + εM ,
so
M = μM (X) + αM θ + εM ,
⊥ (ε0 , ε1 , εUI , θ,
where εM is independent of ε0 , ε1 , εUI and θ , as well as (X, Z) (εM ⊥
X, Z)). We can identify the mean μM (X) from observations on M and X. From this
additional information, we acquire three additional covariance terms, conditional on
X, Z, where we keep the conditioning implicit and define I as normalized by σUI :
Cov(Y1 , M) = α1 αM σθ2 ,
Cov(Y0 , M) = α0 αM σθ2 ,
αUI
Cov(I, M) =
αM σθ2 .22
σUI
Suppose that we normalize the loading on the proxy (or test score) to one (αM = 1). It
is no longer necessary to normalize α0 = 1 as in the preceding section. From the ratio
of the covariance of Y1 with I with the covariance of I with M, we obtain the right-hand

22 Conditioning on X, Z, we can remove the dependence of Y , Y , M and I on these variables and effectively
1 0

work with the residuals Y0 − μ0 (X) = U0 , Y1 − μ1 (X) = U1 , M − μM (X) = UM , I − μI (Z) = UI , where
we keep the scale on I implicit.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5169

side of
α1 αUI σθ2
Cov(Y1 , I )
= α1 ,
=
Cov(I, M)
αUI αM σθ2
because αM = 1 (normalization). From the discussion in the preceding section where
no proxy is assumed, we obtain α0 since
α1 αUI σθ2
Cov(Y1 , I )
α1
=
.
=
Cov(Y0 , I )
α0
α0 αUI σθ2
From knowledge of α1 and α0 and the normalization for αM , we obtain σθ2 from
Cov(Y1 , M) or Cov(Y0 , M). We obtain αUI (up to scale σUI ) from Cov(I, M) =
αUI αM σθ2
σUI

since we know αM (= 1) and σθ2 . The model is overidentified. We can identify
the scale of σUI by a standard argument from the discrete-choice literature. We review
this argument below.
Observe that if we write out the decision rule in terms of costs, we can characterize
the latent variable determining choices as:
I = Y1 − Y0 − C,
where C = μC (Z) + UC and UC = αC θ + εC , where εC is independent of θ and the
other ε’s. E(UC ) = 0 and UC is independent of (X, Z). Then, UI = U1 − U0 − UC and
αUI = α1 − α0 − αC ,
εUI = ε1 − ε0 − εC ,
Var(εUI ) = Var(ε1 ) + Var(ε0 ) + Var(εC ).
Identification of α0 , α1 and αUI implies identification of αC . Identification of the variance of εUI implies identification of the variance of εC since the variances of ε1 and ε0
are known.
Observe further that the scale σUI is identified if there are variables in X but not
in Z [see Heckman (1976, 1979), Heckman and Robb (1985, 1986), Willis and Rosen
(1979)].23 From the variance of M given X, we obtain Var(εM ) since we know Var(M)
2 σ 2:
(conditional on X) and we know αM
θ
2 2
σθ = σε2M .
Var(M) − αM

(Recall that we keep the conditioning on X implicit.) By similar reasoning, it is possible to identify Var(ε0 ), Var(ε1 ) and the fraction of Var(UI ) due to εUI . We can thus
23 The easiest case to understand is one where μ (Z) = Zγ , μ (X) = Xβ , μ (X) = Xβ and μ (Z, X) =
C
I
1
1 0
0
X(β1 − β0 ) − Zγ . We identify the coefficients of the index μI (Z, X) up to scale σUI , but we know β1 − β0

from the earnings functions. Thus if one X is not in Z and its associated coefficient is not zero, we can
identify σUI . See, e.g., Heckman (1976).

5170

J.H. Abbring and J.J. Heckman

construct the joint distribution of (Y0 , Y1 , C) and hence the joint distribution of (Y0 , Y1 )
since we identified μC (Z) and all of the factor loadings. Thus we can identify the objective outcome distribution for (Y0, Y1 ) and the subjective distribution for C as well as
their joint distribution (Y0 , Y1 , C).
We have assumed normality because it is convenient to do so. Carneiro, Hansen and
Heckman (2003), Cunha, Heckman and Navarro (2005, 2006) and Cunha and Heckman
(2008) show that it is possible to nonparametrically identify the distributions of θ, ε0 ,
ε1 , εUI and εM so our results do not hinge on arbitrary distributional assumptions as we
establish in the next section.
We next show by way of example that choice data are not strictly required to secure
identification of the joint distributions of counterfactuals. It is the extra information
joined with the factor restriction on the dependence that allows us to identify the joint
distribution of outcomes.
E XAMPLE 2 (Identification without choice data). This example builds on Example 1.
Let M be two dimensional so M = (M1 , M2 ), and M1 , M2 are indicators that depend
on θ and assume that they are both observed. In place of I from choice theory as in the
preceding section, we can work with a second indicator of θ, i.e., a second measurement M2 . Suppose that either by limit operations (P (X, Z) → 0 or P (X, Z) → 1
along certain sequences in its support) or some randomization we observe triplets
(Y0 , M1 , M2 ), (Y1 , M1 , M2 ) but not Y0 and Y1 together. We can still identify the joint
distribution of (Y0 , Y1 ).
Example 1 applies to this case with only trivial modifications. We can identify all
of the variances and covariances of the factor model as well as the factor loadings up
to one normalization. Thus we can identify the joint distribution of (Y0 , Y1 ). Since the
(M1 , M2 ) are assumed to be observed and their scale is known, we can identify the
variances of M1 and M2 directly. In this example, we do not need to use any of the
apparatus of discrete-choice theory except to govern the limit operations that control
for selection.
There are other ways to construct the joint distributions that do not require a proxy
M that may be extended to the model. Access to panel data on earnings affords identification. One way, that motivates our analysis of ex ante vs. ex post returns developed
later, is given next.
E XAMPLE 3 (Two (or more) periods of panel data on outcomes). Suppose that for each
person we have two periods of outcome data in one counterfactual state or the other.
Thus we observe (Y0,1 , Y0,2 ) or (Y1,1 , Y1,2 ) but never both pairs of vectors together for
the same person. We also observe choices. We assume that Yj,t = μj,t (X) + Uj,t ,
j = 0, 1, t = 1, 2, and write
U1,t = α1,t θ + ε1,t

and U0,t = α0,t θ + ε0,t

to obtain
Y1,t = μ1,t (X) + α1,t θ + ε1,t ,

t = 1, 2,

Ch. 72:

Econometric Evaluation of Social Programs, Part III

Y0,t = μ0,t (X) + α0,t θ + ε0,t ,

5171

t = 1, 2.

In the context of a schooling choice model as analyzed by Carneiro, Hansen and Heckman (2001, 2003) and Cunha, Heckman and Navarro (2005, 2006), if we assume that
the interest rate is zero and that agents maximize the present value of their income, the
index generating choices is
I = (Y1,2 + Y1,1 ) − (Y0,2 + Y0,1 ) − C,
D = 1[I  0],
where C was defined previously, and
I = μ1,1 (X) + μ1,2 (X) − μ0,1 (X) − μ0,2 (X) − μC (Z) + U1,1 + U1,2
− U0,1 − U0,2 − UC .
We assume no proxy—just two periods of panel data. The multiple periods of earnings
serve as the proxy.
Under normality, application of the standard normal selection model allows us to
identify μ1,t (X) for t = 1, 2; μ0,t (X) for t = 1, 2 and μ1,1 (X) + μ1,2 (X) − μ0,1 (X) −
μ0,2 (X) − μC (Z), the latter up to a scalar σUI where
UI = U1,1 + U1,2 − U0,1 − U0,2 − UC .
Following our discussion of Example 1, we can recover the scale σUI if there are variables in X that are not in Z such that (μ1,1 (X) + μ1,2 (X) − (μ0,1 (X) + μ0,2 (X)))
can be varied independently from μC (Z). To simplify the analysis, we assume that this
condition holds.24
From normality, we can recover the joint distributions of (I, Y1,1 , Y1,2 ) and
(I, Y0,1 , Y0,2 ) but not directly the joint distribution of (I, Y1,1 , Y1,2 , Y0,1 , Y0,2 ). Thus,
conditioning on X and Z, we can recover the joint distribution of (UI , U0,1 , U0,2 ) and
(UI , U1,1 , U1,2 ) but apparently not that of (UI , U0,1 , U0,2 , U1,1 , U1,2 ). However, under
our factor structure assumptions, this joint distribution can be recovered as we next
show.
From the available data, we can identify the following covariances:
Cov(UI , U1,2 ) = (α1,2 + α1,1 − α0,2 − α0,1 − αC )α1,2 σθ2 ,
Cov(UI , U1,1 ) = (α1,2 + α1,1 − α0,2 − α0,1 − αC )α1,1 σθ2 ,
Cov(UI , U0,1 ) = (α1,2 + α1,1 − α0,2 − α0,1 − αC )α0,1 σθ2 ,
Cov(UI , U0,2 ) = (α1,2 + α1,1 − α0,2 − α0,1 − αC )α0,2 σθ2 ,
Cov(U1,1 , U1,2 ) = α1,1 α1,2 σθ2 ,
Cov(U0,1 , U0,2 ) = α0,1 α0,2 σθ2 .
24 If not, then μ (Z), σ 2 and α are only identified up to normalizations.
C
C
ε
C

5172

J.H. Abbring and J.J. Heckman

If we normalize α0,1 = 1 (recall that one normalization is needed to set the scale
of θ ), we can form the ratios
Cov(UI , U1,2 )
= α1,2 ,
Cov(UI , U0,1 )
Cov(UI , U0,2 )
= α0,2 .
Cov(UI , U0,1 )

Cov(UI , U1,1 )
= α1,1 ,
Cov(UI , U0,1 )

From these coefficients and the remaining covariances, using Cov(U1,1 , U1,2 ) and/or
Cov(U0,1 , U0,2 ), we identify σθ2 . Thus if the factor loadings are nonzero, we can identify
σθ2 from two relationships, both of which are identified:
Cov(U1,1 , U1,2 )
= σθ2
α1,1 α1,2
and
Cov(U0,1 , U0,2 )
= σθ2 .
α0,1 α0,2
Since we know α1,1 α2,2 and α0,1 α0,2 , we can recover σθ2 from Cov(U1,1 , U1,2 ) and
Cov(U0,1 , U0,2 ). We can also recover αC since we know σθ2 , α1,2 + α1,1 − α0,2 − α0,1 −
αC , and α1,1 , α1,2 , α0,1 , α0,2 . We can form (conditional on X)
Cov(Y1,1 , Y0,1 ) = α1,1 α0,1 σθ2 ;
Cov(Y1,1 , Y0,2 ) = α1,1 α0,2 σθ2

Cov(Y1,2 , Y0,1 ) = α1,2 α0,1 σθ2 ;
and

Cov(Y1,2 , Y0,2 ) = α1,2 α0,2 σθ2 .

We can identify μC (Z) from the schooling choice equation since we know μ0,1 (X),
μ0,2 (X), μ1,1 (X), μ1,2 (X) and we have assumed that there are some Z not in X so that
σUI is identified. Thus we can identify the joint distribution of (Y0,1 , Y0,2 , Y1,1 , Y1,2 , C).
These examples extend to nonnormal and nonparametric models. The key idea to
constructing joint distributions of counterfactuals using the analysis of Cunha and Heckman (2008) and Cunha, Heckman and Navarro (2005, 2006) is not the factor structure
for unobservables although it is convenient. The crucial idea is the assumption that a
low dimensional set of random variables generates the dependence across outcomes.
Other low dimensional representations such as the ARMA model or the dynamic factor
structure model [see Sargent and Sims (1977)] can also be used. Cunha and Heckman (2007a) and Cunha, Heckman and Schennach (2007) extend factor models to more
general frameworks where the θ evolve over time as in state space models. The factor
structure model presented in this section is easy to exposit and has been used to estimate
joint distributions of counterfactuals. We present some examples in a later subsection.
That subsection reviews recent work that generalizes the analysis of this section to derive ex ante and ex post outcome distributions, and measure the fundamental uncertainty
facing agents in the labor market. With these methods it is possible to compute the distributions of both ex ante and ex post returns to treatments. Before presenting a more
general analysis, we relate factor models to matching models.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5173

2.7.2. Relationship to matching
If the analyst knew θ and could condition on it, the analyst would obtain the conditionalindependence assumption of matching, (M-1), in Chapter 71:
(U-1) (Y0 , Y1 ) ⊥
⊥ D | X, Z, θ .
This is also the general control function assumption (U-1) in Chapter 71.
The approach developed by Aakvik, Heckman and Vytlacil (2005), Carneiro, Hansen
and Heckman (2001, 2003), Cunha, Heckman and Navarro (2005, 2006), and Cunha
and Heckman (2007b, 2007c, 2008) extends matching and treats θ as an unobservable.
It uses proxies for θ and identifies the distribution of θ under the following assumption:
(U-2) θ ⊥
⊥ X, Z.
Thus the factor approach is a version of matching on unobservables, where the unobserved match variables are integrated out.
2.7.3. Nonparametric extensions
The analysis of the generalized Roy model developed in Appendix B of Chapter 70 establishes conditions under which it is possible to nonparametrically identify the joint
distribution of (Y0 , I, M) given X, Z and the joint distribution of (Y1 , I, M) given
X, Z, where we also allow the functions determining M to be nonparametrically determined.25 These conditions can be extended to provide identification of the distributions
of (Y0 , I, M) and (Y1 , I, M) where M is observed for all persons treated or not whereas
Y0 and Y1 are observed only if D = 0 or D = 1, respectively. The identification conditions are also easily extended to account for vector Y0 and Y1 (e.g., Y0 = (Y0,1 , Y0,2 ) and
Y1 = (Y1,1 , Y1,2 )) as our third example in Section 2.7.1 reveals. We present a general
theorem for the identification of state-contingent outcomes free of selection bias in the
next section and in Appendix B of this chapter. With the state-contingent distributions
nonparametrically identified, we can apply factor analysis to identify the factor loadings because we identify the required covariances as a by-product of our nonparametric
analysis.
With the αj (or αi,j ) in hand, we can nonparametrically identify the distribution of θ
and the εj (or εi,j ) for the different models assuming mutual independence between θ
and all of the components of εj (or εi,j ) using Kotlarski’s Theorem [Kotlarski (1967),
Prakasa-Rao (1992)]. That theorem states that, for any pair of random variables T1 , T2
generated by a common random variable θ , we can nonparametrically identify the distribution of θ and the associated components of errors: ε1 and ε2 . Stated precisely:
25 Recall that, depending on the assumptions discussed in Section 2.7.1, the scale of I may, or may not, be

identified.

5174

J.H. Abbring and J.J. Heckman

T HEOREM 1. If
T1 = θ + ε1
and
T2 = θ + ε2
and (θ, ε1 , ε2 ) are mutually independent, the means of all three generating random
variables are finite and are normalized to E(ε1 ) = E(ε2 ) = 0, and the random variables possess nonvanishing (a.e.) characteristic functions, then the densities
of (θ, ε1 , ε2 ), gθ (θ ), g1 (ε1 ), g2 (ε2 ), respectively, are identified.
P ROOF. See Kotlarski (1967). See also Prakasa-Rao (1992).



Applied to our context, consider the first two equations of a vector of indicators M
which are stochastically dependent only through θ . We write
M1 = λ1 θ + ε1 ,

where λ1 = 1,

M2 = λ2 θ + ε2 ,

where λ2 = 0.

By the preceding analysis, we can identify λ2 (subject to a normalization λ1 = 1) from
factor models. Thus we can rewrite these equations as
M1 = θ + ε1 ,
M2
= θ + ε2∗ ,
λ2
where ε2∗ = ε2 /λ2 . Applying Kotlarski’s Theorem, we can nonparametrically identify
the densities gθ (θ ), g1 (ε1 ) and g2 (ε2∗ ). Since we know λ2 , we can nonparametrically
identify g2 (ε2 ). Schennach (2004), Hu and Schennach (2006), and Cunha, Heckman
and Schennach (2007) weaken many of the strong independence conditions to mean
independence assumptions. Carneiro, Hansen and Heckman (2003) extend the analysis
of this section to the case of vector θ .
2.8. General models
The analysis of Carneiro, Hansen and Heckman (2003), Cunha, Heckman and Navarro
(2005, 2006) and Cunha and Heckman (2007c, 2008) generalizes the analysis of the preceding sections to consider vectors of outcomes (Y0 and Y1 ), vectors of measurements
(M) and more general choice equations. We summarize that work here. This analysis feeds directly into our analysis of dynamic treatment effects and dynamic discrete
choice presented in Section 3.
Our analysis has three components: (1) Identifying the choice of treatment equation
and hence evaluation of treatments as perceived by agents; (2) Identifying the joint
distributions of outcomes and measurements in each treatment state s, s = 1, . . . , S̄,

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5175

where S̄ is the number of treatment states; and (3) Identifying the joint distribution of
outcomes across treatment states. Only the third step requires a factor structure. Step 1 is
conventional nonparametric discrete-choice analysis. Step 2 solves the selection problem using nonparametric methods. Step 3 solves the evaluation problem using factor
models.
Conditions for nonparametric identification of discrete-choice models are presented
in Matzkin (1992, 1993, 1994) and in her contribution to this Handbook (Chapter 73).
Appendix B of Chapter 70 presents a nonparametric proof of identification of choice
equations as part of a nonparametric analysis of choice and outcome equations for a
general static discrete-choice model. Carneiro, Hansen and Heckman (2003) present a
parallel analysis for an ordered choice model.26 Heckman and Navarro (2007) present
an identification analysis that is used in this section and in Section 3. We now establish
an extension of the theorem proved in Appendix B of Chapter 70 to account for vectors
of outcomes and for associated vectors of measurements. This provides a solution to the
selection problem.
2.8.1. Steps 1 and 2: Solving the selection problem within each treatment state
Associated with each treatment s, s = 1, . . . , S̄, is a vector of outcomes of length Ā,

  



Y s, X, U (s) = Y 1, s, X, U (1, s) , . . . , Y a, s, X, U (a, s) , . . . ,


Y Ā, s, X, U (Ā, s) .
They depend on observables X and unobservables U (s) = (U (1, s), . . . , U (a, s), . . . ,
U (Ā, s)), where the observability distinction is made from the point of view of the
econometrician. The X may also have a- and s-specific subvectors, but for the sake of
notational simplicity we do not make this explicit. We can make the list of outcomes
s-dependent, but only at the cost of notational complexity. Elements of Y (s, X, U (s))
are outcomes associated with receiving treatment s. They are factual outcomes if treatment s is actually selected, which we denote by D(s) = 1. Outcomes corresponding to
treatments s  that are not selected—we denote this by D(s  ) = 0—are counterfactuals.
The outcome variables are not necessarily what the agent thinks will happen when he
or she chooses treatment s, but rather what actually happens. The treatments s may be
associated with stages that are not necessarily identical with real time events, although
this framework can be used in our analysis of dynamic choices evolving in real time
that is presented in Section 3.
Henceforth, whenever we have random variables with multiple arguments R0 (s,
Q0 , . . .) or R1 (a, s, Q0 , . . .) where the argument list begins with treatment state s or
both age a and state s (perhaps followed by other arguments Q0 , . . .), we will make
use of several condensed notations: (a) dropping the first argument as we collect the
26 Cunha, Heckman and Navarro (2007) present a nonparametric identification analysis of the ordered choice

model. They also establish that it imposes the absence of option values.

5176

J.H. Abbring and J.J. Heckman

components into vectors R0 (Q0 , . . .) or R1 (s, Q0 , . . .) of length S̄ or Ā, respectively,
and (b) going further in the case of R1 , dropping the s argument as we collect the
vectors R1 (s, Q0 , . . .) into a single S̄ × Ā array R1 (Q0 , . . .), but also (c) suppressing one or more of the other arguments and writing R1 (a, s) or R1 (a, s, Q0 ) instead
of R1 (a, s, Q0 , Q1 , . . .), etc. This notation is sufficiently rich to represent the life cycle
of outcomes for persons who receive treatment s. We use this notation in the remainder
of this section and in Section 3.
Following Carneiro, Hansen and Heckman (2003), the variables in Y (a, s, X, U (a, s))
may include discrete, continuous or mixed discrete-continuous components. For the
discrete or mixed discrete-continuous cases, we assume that latent continuous variables
cross thresholds to generate the discrete components. Durations can be generated by
latent index models associated with each outcome crossing thresholds analogous to the
model we develop in Section 3 below, in the discussion surrounding Equation (3.11). In
this framework, we can model the effect of attaining s years of schooling on durations
of unemployment or durations of employment.
We decompose Y (a, s) into continuous and discrete components:


Yc (a, s)
.
Y (a, s) =
Yd (a, s)
∗ (a, s).
Associated with the j th component of Yd (a, s), Yd,j (a, s) is a latent variable Yd,j
We define
 ∗

Yd,j (a, s) = 1 Yd,j
(a, s)  0 .27

From standard results in the discrete-choice literature, without additional information,
∗ (a, s) up to scale.
we can only know Yd,j
We assume an additively separable model for the continuous variables and latent
continuous indices. Making the X explicit, we write
Yc (a, s, X) = μc (a, s, X) + Uc (a, s),
Yd∗ (a, s, X) = μd (a, s, X) − Ud (a, s),
1  s  S̄, 1  a  Ā.
We array the Yc (a, s, X) into a matrix Yc (s, X) and the Yd∗ (a, s, X) into a matrix
Yd∗ (s, X). We decompose these vectors into components corresponding to the means
μc (s, X), μd (s, X) and the unobservables Uc (s), Ud (s). Thus
Yc (s, X) = μc (s, X) + Uc (s),
Yd∗ (s, X) = μd (s, X) − Ud (s).

27 Extensions to nonbinary discrete outcomes are straightforward. Thus we could entertain, at greater nota-

tional cost, a multinomial outcome model at each age a for each counterfactual state.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5177

Yd∗ (s, X) generates Yd (s, X). Using our condensed notation, we write
Yc (X) = μc (X) + Uc ,
Yd∗ (X) = μd (X) − Ud .
Following Carneiro, Hansen and Heckman (2003), Cunha, Heckman and Navarro
(2005, 2006) and Cunha and Heckman (2007b, 2007c, 2008), we may also have a
system of measurements with both discrete and continuous components. The measurements are not s-indexed. They are the same for each treatment state.28 We write the
equations for the measurements in an additively separable form, in a fashion comparable to those of the outcomes. The equations for the continuous measurements and latent
indices producing discrete measurements are
Mc (a, X) = μc,M (a, X) + Uc,M (a),
Md∗ (a, X) = μd,M (a, X) − Ud,M (a),
where the discrete variable corresponding to the j th index in Md∗ (a, X) is
 ∗

(a, X)  0 .
Md,j (a, X) = 1 Md,j
The measurements play the role of indicators unaffected by the process being studied. We array Mc (a, X) and Md∗ (a, X) into matrices Mc (X) and Md∗ (X). We array
μc,M (a, X), μd,M (a, X) into matrices μc,M (X) and μd,M (X). We array the corresponding unobservables into Uc,M and Ud,M . In this notation,
Mc (X) = μc,M (X) + Uc,M ,
Md∗ (X) = μd,M (X) − Ud,M .
In the notation of Appendix B of Chapter 70, write the utility valuation of treatment
state s as
R(s, Z) = μR (s, Z) − V (s),

s = 1, . . . , S̄.

Collect R(s, Z), s = 1, . . . , S̄, into a vector


R(Z) = R(1, Z), . . . , R(S̄, Z) .
Collect μR (s, Z), s = 1, . . . , S̄, into a vector


μR (Z) = μR (1, Z), . . . , μR (S̄, Z) .
Collect V (s), s = 1, . . . , S̄, into a vector


V = V (1), . . . , V (S̄) .
28 Thus measurements are not causally affected by treatment. Measurements that are causally affected by

treatment can be included in the model as outcomes using the analysis of Hansen, Heckman and Mullen
(2004).

5178

J.H. Abbring and J.J. Heckman

D(s) = 1 (state s is selected) if


s = argmax R(j, Z) .
j =1,...,S̄

Otherwise D(s) = 0.
S̄


D(j ) = 1.

j =1

Define



V s = V (s) − V (1), . . . , V (s) − V (S̄) ,


μsR (Z) = μR (s, Z) − μR (1, Z), . . . , μR (s, Z) − μR (S̄, Z) ,

s = 1, . . . , S̄.

These contrast vectors are standard in discrete-choice theory, where utilities in treatment
state s are compared with utilities in other treatment states. We assume that we have
access to a large i.i.d. sample from the distribution of (Yc , Yd , Mc , Md , {D(s)}S̄s=1 ).29
We now state a basic theorem that solves the selection problem (Step 2) for the general model of this section. We draw on the work of Matzkin (1992, 1993, 1994) and
Chapter 73 of this Handbook to provide a general characterization of nonparametric
functions and their identifiability. We define the Matzkin class of functions in Appendix B and use it in the next proof. They include all of the familiar linear-in-parameters
functional forms for discrete choice as well as a variety of other classes of functions
that can be identified under conditions specified in her papers.
T HEOREM 2. The joint distribution of (Uc (s), Ud (s), Uc,M , Ud,M , V s ) is identified
along with the functions (μc (s, X), μd (s, X), μc,M (X), μd,M (X), μsR (Z)) (the components of μd (s, X) and μd,M (X) over the supports admitted by the supports of the
errors) if, for s = 1, . . . , S̄,
(i) E[Uc (s)] = E[Uc,M ] = 0. (Uc (s), Ud (s), Uc,M , Ud,M , V s ) are continuous random variables with support (U c (s), Uc (s))×(U d (s), Ud (s))×(U c,M , Uc,M )×
(U d,M , Ud,M ) × Rs−1 . These conditions are assumed to apply within each
component of each subvector. The joint system is thus variation free for each
component with respect to every other component.
⊥ (X, Z).
(ii) (Uc (s), Ud (s), Uc,M , Ud,M , V s ) ⊥
(iii) Supp(μsR (Z), X) = Supp(μsR (Z)) × Supp(X).
(iv) Supp(μd (s, X), μd,M (X)) ⊇ Supp(Ud (s), Ud,M ).
(v) μc (s, X), μc,M (X) and μR (Z) are continuous functions. The components of
the μd (s, X) and μd,M (X) belong to the Matzkin class of functions given in
 with Z
 ⊆ Supp(Z) such that {μ1 (z);
Appendix B. μ1R (z) is known for z ∈ Z
R
 = RS̄−1 . μR (1, Z) is known.
z ∈ Z}
29 We can allow for dependence across individuals by invoking appropriate limit laws for dependent random

variables.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5179



P ROOF. See Appendix B.30

This proof presents conditions for producing a selection-bias free joint distribution
of (Yc (s, X), Yd (s, X), Mc (X), Md (X), V s ), s = 1, . . . , S̄ conditionally on X which
are the inputs for our factor analysis to which we now turn.
2.8.2. Step 3: Constructing counterfactual distributions using factor models
The analysis of the preceding section presented conditions under which subjective relative evaluations of treatment outcomes from choice functions and objective outcome
distributions in state s, s = 1, . . . , S̄, can be identified. Missing is an analysis of identification of joint outcome distributions. In this subsection, we generalize the analysis of
Section 2.7 to present conditions under which joint distributions can be identified in a
multifactor setting.
Theorem 2 gives conditions under which the distributions of (Uc (s), Ud (s), Uc,M ,
Ud,M , V s ), s = 1, . . . , S̄, are identified. If we factor analyze these errors, we can identify the joint distributions of these vectors across treatment states. We write in the case
of vector θ,

θ + εc (s),
Uc (s) = αc,s

θ + εd (s),
Ud (s) = αd,s

Uc,M = αc,M
θ + εc,M ,

Ud,M = αd,M
θ + εd,M ,

V s = αV s θ + εV (s),

(2.7)

or more compactly, using the notation


U (s) = Uc (s), Ud (s), Uc,M , Ud,M , V s ,


ε(s) = εc (s), εd (s), εc,M , εd,M , εV (s) ,
we may write the preceding system as a system of equations:
U (s) = Λ(s)θ + ε(s),

s = 1, . . . , S̄,

(2.8)

where the components of ε = (ε(1), . . . , ε(S̄)) are mutually independent and ε ⊥
⊥ θ.
The factor loadings may differ across treatment states. All of the dependence among
outcomes and measurements and the choice indicators {D(s)}S̄s=1 is generated by dependence on common factors θ. The outcome, choice, and measurement equations all
contribute to the U (s) and are a source of information on the distribution of θ .
The same principles guide identifiability in this system of equations as in the onefactor models analyzed in Section 2.7. With enough measurements, outcomes and
30 Matzkin (1993) presents alternative sets of conditions for identifiability of the choice model.

5180

J.H. Abbring and J.J. Heckman

choices relative to the dimensionality of θ, it is possible to identify the joint distribution of outcomes across counterfactual states.
Identification problems in factor analysis were first clearly stated by Anderson and
Rubin (1956). If, for example, there are L(s) components of U (s) and θ is K × 1,
ε(s) is L(s) × 1 and Λ(s) is L(s) × K. Even if the θi , i = 1, . . . , K, are mutually independent, the model of Equation (2.8) is underidentified. To see this, note that
Cov(U (s)) = Λ(s)Σθ Λ (s) + Dε(s) , where Σθ is a matrix of the variances of the factors, assumed to be diagonal in this example, Dε(s) is a diagonal matrix of the variances
of the uniquenesses.31 We have identified Cov(U (s)), the discrete components up to
scale, but we do not directly observe θ or ε(s). Any orthogonal transformation applied
to Λ(s) is consistent with the same Cov(U (s)).
Without restrictions on Λ(s), and on the dependence structure among the components of θ , identification of the model is not possible. Conventional factor-analytic
models make assumptions to identify parameters. The diagonals of Cov(U (s)) combine elements of Dε(s) with parameters from the rest of the model. Once those other
parameters are determined, the diagonals identify Dε(s) . Accordingly, one can only rely
on the L(s)(L(s) − 1)/2 non-diagonal elements to identify the K variances (assuming
θi ⊥
⊥ θj , ∀i = j ), and the L(s) × K factor loadings. Since the scale of each θi is arbitrary, one factor loading devoted to each factor must be normalized to set the scale.
Typically the normalization is unity. Accordingly, we require as a necessary condition
for identification of the variances and parameters of (2.8) for a given s



L(s)(L(s) − 1)
 L(s) × K − K +
2
Number of off-diagonal
covariance elements

⇐⇒

Number of unrestricted Λ

L(s)  2K + 1.

K.
Variances of θ

(2.9)

Anderson and Rubin (1956), Chamberlain (1975), Carneiro, Hansen and Heckman
(2003), Hansen, Heckman and Mullen (2004), Cunha, Heckman and Navarro (2005)
and Cunha, Heckman and Schennach (2007) present alternative normalizations and
identification assumptions for models with multiple factors. Carneiro, Hansen and
Heckman (2003) and Cunha, Heckman and Schennach (2006, 2007) use information
from higher moments to identify the model.32 Many of the identifying assumptions
in various empirical literatures such as the literature on earnings dynamics are motivated by appeals to empirical conventions and to economic theory [see, e.g., Cunha
and Heckman (2007b, 2007c, 2008)]. Case-specific analyses are necessary to provide
economically interpretable identifying assumptions. Access to measurements facilitates
this task.

31 The uniquenesses are the ε(s) in Equation (2.8).
32 See also Bonhomme and Robin (2004). Note that restrictions across the s-systems facilitate identification.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5181

2.9. Distinguishing ex ante from ex post returns
The analysis of the preceding sections presents tools for estimating joint distributions of
outcomes and subjective valuations of outcomes across counterfactual states. It is silent
about the information that agents possess about expected returns at the time they make
their program participation decisions. Uncertainty and the dynamics of information revelation are not systematically incorporated in the current literature on treatment effects.
As noted in Chapter 70 of this Handbook, anticipated (ex ante) returns may differ from
realized (ex post) returns and understanding these differences is important for computing the welfare gains to program participation, the regret that agents may experience
about participating or not participating in a program, and the option value of social programs. In addition, subjective evaluations are not a part of the literature on statistical
treatment effects.
In a medical trial [see, e.g., Chan and Hamilton (2006)], the patient will not only value
the medical treatment but he/she will also consider the medical benefits or costs (pain
and suffering) connected with the treatment. Agents may be pleasantly or unpleasantly
surprised by arrival of information during a course of therapy, and this information revision will affect choices of future treatment. Knowing agent preferences and perceptions
is helpful in determining compliance and patient welfare. In an analysis of job training
programs, agents may be disappointed, ex post about the treatment they have received
[Heckman and Smith (1998)].
Empirical analyses of the “returns to education” that have extensively used IV methods focus exclusively on the ex post returns to education rather than the ex ante returns
that motivate agent schooling decisions. As Hicks (1946, p. 179) puts it,
“Ex post calculations of capital accumulation have their place in economic and
statistical history; they are a useful measuring-rod for economic progress; but they
are of no use to theoretical economists who are trying to find out how the system
works, because they have no significance for conduct.”
This section presents some recent results on the identification of agent information
sets and ex ante and ex post distributions of outcomes. It builds on and synthesizes
work by Carneiro, Hansen and Heckman (2001, 2003), Cunha, Heckman and Navarro
(2005, 2006) and Cunha and Heckman (2007b, 2007c, 2008).
To motivate the main ideas underlying this approach, consider the problem of estimating the return to an activity. It could be schooling or the installation of a new technology.
The problem can be cast as a prototypical generalized Roy model with two sectors and
solutions to it apply to many related problems. Let D denote different choices. D = 0
denotes choice of sector 0 and D = 1 denotes choice of sector 1. In a schooling example
this could represent high school (D = 0) and college (D = 1). Each person chooses
to be in one or the other sector but cannot be in both. Let the two potential outcomes
be represented by the pair (Y0 , Y1 ), only one of which is observed by the analyst for
any agent. Denote by C the direct cost of choosing sector 1. In a schooling example

5182

J.H. Abbring and J.J. Heckman

these would include tuition and nonpecuniary costs of attending college expressed in
monetary values.
Y1 is the ex post present value of making the choice 1, discounted over horizon T̄ for
a person choosing at a fixed age, assumed for convenience to be one,
Y1 =

T̄

t=1

Y1,t
,
(1 + r)t−1

and Y0 is the ex post present value of making the choice 0 at age one,
Y0 =

T̄

t=1

Y0,t
,
(1 + r)t−1

where r is the one-period risk-free interest rate. Y1 and Y0 can be constructed from
time series of ex post potential outcome streams in the two states: (Y0,1 , . . . , Y0,T̄ ) and
(Y1,1 , . . . , Y1,T̄ ). A practical problem is that we only observe one or the other of these
streams for any person. This is the fundamental program evaluation problem. In addition, we observe these streams selectively, i.e., for those who chose D = 0 or D = 1,
respectively.
The variables Y1 , Y0 , and C are ex post realizations of returns and costs, respectively.
At the time agents make their choices, these random variables may only be partially
known to the agent. Using the information set notation introduced in Section 2.6 of
Chapter 70, let IA denote the information set of an agent at the time the choice is made,
which is time period t = 1 in our notation. Under a complete markets assumption with
all risks diversifiable (so that there is risk-neutral pricing) or under a perfect foresight
model with unrestricted borrowing or lending but full repayment, the decision rule governing sectoral choices at decision time 1 is

1, if E(Y1 − Y0 − C | IA )  0,
D=
(2.10)
0, otherwise.33
Under perfect foresight, the postulated information set would include Y1 , Y0 , and C.
Under either model of information, the decision rule is simple: one chooses sector 1 if
the expected gains from doing so are greater than or equal to the expected costs. Thus
under either set of assumptions, a separation theorem governs choices. Agents maximize
expected wealth independently of their consumption decisions over time.34

33 If there are aggregate sources of risk, full insurance would require a linear utility function.
34 The decision rule is more complicated in the absence of full risk diversifiability and depends on the curva-

ture of utility functions, the availability of markets to spread risk, and possibilities for storage. [See Heckman,
Lochner and Todd (2006) for a more extensive discussion.] In these more realistic economic settings, the
components of earnings and costs required to forecast the gain to the choice depend on higher moments than
the mean. In this section, we use a model with a simple market setting to motivate the identification analysis
of a more general environment analyzed elsewhere [Carneiro, Hansen and Heckman (2003)].

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5183

Suppose that we seek to determine IA . This is a difficult task. Typically we can
only partially identify IA and generate a list of candidate variables that belong to the
information set. We can usually only estimate the distributions of the unobservables
in IA (from the standpoint of the econometrician) and not individual realizations of the
unobservables.
Cunha, Heckman and Navarro (2005, 2006) and Cunha and Heckman (2007b, 2007c)
exploit covariances between choices and realized outcomes that arise under different information structures to test which information structure characterizes the data, building
on the analysis of Carneiro, Hansen and Heckman (2003). To see how the method works,
we simplify the exposition to a two-choice framework. In Section 3 of this contribution,
we extend this analysis to multiple choices in a dynamic setting.
Suppose, contrary to what is possible, that the analyst observes Y0 , Y1 , and C for each
person. Such information would come from an ideal data set in which the evaluation
problem is solved and we could observe two different lifetime outcome streams for the
same person as well as the costs they pay for choosing sector 1. From such information,
we could construct Y1 − Y0 − C. If we knew the information set IA of the agent that
governs choices, we could also construct E(Y1 − Y0 − C | IA ). Under the correct model
of expectations, we could form the residual
ζIA = (Y1 − Y0 − C) − E(Y1 − Y0 − C | IA ),
and from the ex ante choice decision, we could determine whether D depends on ζIA .
It should not if we have specified IA correctly.
A test for correct specification of candidate information set 
IA for an agent is a test
of whether D depends on ζIA , where
IA ).
ζIA = (Y1 − Y0 − C) − E(Y1 − Y0 − C | 
IA . A test of misspecification
More precisely, the information set is valid if D ⊥
⊥ ζIA | 

of IA is a test of whether the coefficient of ζIA in the choice equation is statistically
significantly different from zero.
More generally, 
IA is the correct information set if ζIA does not help to predict
schooling. One can search among candidate information sets 
IA to determine which
ones satisfy the requirement that the generated ζIA does not predict D and what components of Y1 −Y0 −C (and Y1 −Y0 ) are predictable at the age schooling decisions are made
for the specified information set. This procedure is motivated by a Sims (1972) version
of a Wiener–Granger causality test. There may be several information sets that satisfy
IA , ζIA should not cause (predict) schooling
this property.35 For a properly specified 
choices. The components of ζIA that are unpredictable are intrinsic components of uncertainty at the date the choice represented by D is made.
35 Thus different combinations of variables may contain the same information. The issue of the existence of a

smallest information set is a technical one concerning a minimum σ -algebra that satisfies the conditions used
to define IA .

5184

J.H. Abbring and J.J. Heckman

It is difficult to determine the exact content of IA known to each agent. If we could,
we would perfectly predict D given our decision rule. More realistically, we might find
variables that proxy IA or their distribution. This strategy is pursued in Cunha, Heckman
and Navarro (2005, 2006) for a two-choice model, and is generalized by Cunha and
Heckman (2007b) and Heckman and Navarro (2007). We now present an example of
this approach. We consider identification of information sets as well as identification of
the psychic costs of treatment.
2.9.1. An approach based on factor structures
Consider the following model for T̄ periods. Write outcomes in each counterfactual
state as
Y0,t = μ0,t (Xt ) + U0,t ,
Y1,t = μ1,t (Xt ) + U1,t ,

t = 1, . . . , T̄ .

We let costs of picking sector 1 be defined as
C = μC (Z) + UC .
Assume that the horizon of the agent ends at period T̄ .
Suppose that there exists a vector of mutually independent factors
θ = (θ1 , θ2 , . . . , θK ).
Under the factor assumption, the error term in outcomes in period t for an agent can be
represented in the following manner:
U0,t = α0,t θ + ε0,t ,
U1,t = α1,t θ + ε1,t ,
where α0,t and α1,t are now 1 × K vectors and θ is a K × 1 vector. The ε0,t , ε1,t , and
θ are mutually independent. We can also decompose the cost function C in a similar
fashion:
C = μC (Z) + αC θ + εC .
All of the statistical dependence across potential outcomes and costs is generated by
θ , X, and Z. Thus, if we could match on θ (as well as X and Z), we could use matching
to infer the distribution of counterfactuals and capture all of the dependence across the
counterfactual states through θ . Carneiro, Hansen and Heckman (2001, 2003), Cunha,
Heckman and Navarro (2005, 2006) and Cunha and Heckman (2007b, 2007c, 2008)
allow for the possibility that not all of the required elements of θ are observed.
The parameters αC and αs,t , for s = 0, 1, and t = 1, . . . , T̄ are the factor loadings.
εC is independent of the θ and the other ε components. In this notation, the choice

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5185

equation can be written as:
 T̄
 (μ1,t (Xt ) + α1,t θ + ε1,t ) − (μ0,t (Xt ) + α0,t θ + ε0,t )
D∗ = E
(1 + r)t−1
t=1
 


− μC (Z) + αC θ + εC  IA ,
D = 1 if D ∗  0;

D = 0 otherwise.

(2.11)

The first term in the summation inside the parentheses is discounted outcomes in state 1
minus discounted outcomes in state 0. The second term in the expression is the cost.
Equation (2.11) entails counterfactual comparisons. Even if the outcomes associated
with one choice are observed over the horizon using panel data, the outcomes in the
counterfactual state are not. After the choice is made, some components of the Xt ,
the θ , and the εt may be revealed (e.g., unemployment rates, macroshocks) to both the
observing economist and the agent, although different components may be revealed to
each and at different times.
Examining alternative information sets, one can determine which ones produce models for outcomes that fit the data best in terms of producing a model that predicts date
t = 1 choices and at the same time passes the test for misspecification of predicted
earnings and costs described in the previous subsection. Some components of the error terms of the outcome equations may be known or not known at the date schooling
choices are made. The unforecastable components are intrinsic uncertainty. The forecastable information is called heterogeneity.36
To formally characterize an empirical procedure to test for and measure the importance of uncertainty, it is useful to introduce some additional notation. Let  denote the
Hadamard product, a  b = (a1 b1 , . . . , aL bL ), for vectors a and b of length L. This is a
componentwise multiplication of vectors to produce a vector. Let κXt , t = 1, . . . , T̄ ,
κZ , κθ , κεt , κεC , denote coefficient vectors associated with the Xt , t = 1, . . . , T̄ ,
the Z, the θ, the ε1,t − ε0,t , and the εC , respectively. For a proposed information set

IA which may or may not be the true information set on which agents act, define the
proposed choice index D̃ ∗ in the following way. For simplicity write μ1,t (Xt ) = Xt β1,t ,
μ0,t (Xt ) = Xt β0,t , and μC (Z) = Zγ . Then
D̃ ∗ =

T̄
T̄


E(Xt | 
IA )
[Xt − E(Xt | 
IA )]
(β
−
β
)
+
(β1,t − β0,t )  κXt
1,t
0,t
t−1
t−1
(1 + r)
(1 + r)
t=1
t=1
 T̄

 (α1,t − α0,t )
+
− αC E(θ | 
IA )
(1 + r)t−1
t=1

36 The term ‘heterogeneity’ is somewhat unfortunate. This term includes trends common across all people

(e.g., macrotrends). The real distinction they are making is between components of realized outcomes forecastable by agents at the time they make their choices vs. components that are not forecastable.

5186

J.H. Abbring and J.J. Heckman


+



T̄



(α1,t − α0,t )
− αC  κθ θ − E(θ | 
IA )
t−1
(1 + r)
t=1

T̄
E(ε1,t − ε0,t | 
IA )  [(ε1,t − ε0,t ) − E(ε1,t − ε0,t | 
IA )]
+
κεt
(1 + r)t−1
(1 + r)t−1
t=1
t=1


− E(Z | 
IA )γ − Z − E(Z | 
IA ) γ  κZ − E(εC | 
IA )


IA ) κεC .
− εC − E(εC | 
(2.12)

+

T̄


Fit a choice model based on the proposed information set. Estimate the parameters
of the model including the κ parameters. The κ parameters will be estimated to be
nonzero in a choice equation if a proposed information set is not the actual information
set used by agents. This particular decomposition for D̃ ∗ assumes that agents know
the β, the γ , and the α.37 If this assumption is not correct, the presence of additional
unforecastable components due to unknown coefficients affects the interpretation of
the estimates. A test of no misspecification of information set 
IA is a joint test of the
hypothesis that the κ are all zero. That is, when 
IA = IA then the proposed choice
index D̃ ∗ = D ∗ . In a model with a correctly specified information set, the components
associated with zero κj are the unforecastable elements or the elements which, even if
known to the agent, are not acted on in making schooling choices.
To illustrate the method of Cunha, Heckman and Navarro (2005), assume that the Xt ,
the Z, the εC , the β1,t , β0,t , the α1,t , α0,t , and αC are known to the agent at the time
decisions about D are being made, and that the εj,t are unknown, and that the agents set
them at their mean values of zero. We can infer which components of the θ are known
and acted on in making decisions if we postulate that some components of θ are known
perfectly at date t = 1 while others are not known at all, and their forecast values have
mean zero given IA .
If there is an element of the vector θ , say θ2 (factor 2), that has nonzero loadings
(coefficients) in the choice equation and a nonzero loading on one or more potential
future outcomes, then one can say that at the time the choice is made, the agent knows
the unobservable captured by factor 2 that affects future outcomes. If θ2 does not enter
the choice equation but explains future outcomes, then θ2 is unknown (not predictable
by the agent) at the age decisions are made. An alternative interpretation is that the
second component of
 T̄

 (α1,t − α0,t )
− αC
(1 + r)t−1
t=1

is zero, i.e., that even if the component is known, it is not acted on. Analysts can only
test for what the agent knows and acts on.

37 Cunha, Heckman and Navarro (2005) and Cunha and Heckman (2007b) relax this assumption.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5187

One plausible scenario is that εC is known to the agent, since costs are assumed to be
incurred up front, but that the future ε1,t and ε0,t are not and have mean zero. If there
are components of the εj,t that are predictable at age t = 1, they will induce additional
dependence between D and future outcomes that will pick up additional factors beyond
those initially specified. The procedure can be generalized to consider all components of
the outcome equations. Using this procedure, the analyst can test the predictive power
of each subset of the possible information set at the date the decision is being made. The
approach allows the analyst to determine which components of θ and {ε0,t , ε1,t }T̄t=1 are
known and acted on at the time decisions are made.
Statistical decompositions do not tell us which components of error variance are
known at the time agents make their decisions. A model of expectations and choices
is needed. If some of the components of {ε0,t , ε1,t }T̄t=1 are known to the agent at the
date decisions are made and enter decision equation (2.11), then additional dependence between D and future Y1 − Y0 due to the {ε0,t , ε1,t }T̄t=1 , beyond that due to θ,
would be estimated. Our version of the Sims test can in principle detect these components.
It is helpful to contrast the dependence between D and future Y0,t , Y1,t arising from
θ and the dependence between D and the {ε0,t , ε1,t }T̄t=1 . Some of the θ in the ex post
outcomes equation may not appear in the choice equation. Under other information sets,
some additional dependence between D and {ε0,t , ε1,t }T̄t=1 may arise. The contrast between the sources generating realized outcomes and the sources generating dependence
between D and realized outcomes is the essential idea in inferring the information in
the agent’s information set when decisions are being made. The method can be generalized to deal with nonlinear preferences and imperfect market environments.38 We next
show how to operationalize this method and identify psychic costs and agent information sets. This econometric analysis is followed by some empirical applications of this
methodology.
2.9.2. Operationalizing the method
In order to see how to operationalize the method, we draw on the work of Cunha and
Heckman (2007b). Assume normality to simplify the analysis. The normality assumption plays no essential role in the analysis and is relaxed below. Our empirical examples
in fact show the estimated models to be highly nonnormal.
The key idea underlying this approach is to have more measurement, outcome and
choice equations than components in θ. These are the necessary conditions for identification encapsulated in inequality (2.9). Here we assume that we have multiple periods
of data on outcomes associated with each treatment state s, s = 1, . . . , S̄, as well
as measurement equations. We assume a two-factor example and show how to test

38 See Carneiro, Hansen and Heckman (2003), Cunha and Heckman (2007c) and the survey in Heckman,

Lochner and Todd (2006).

5188

J.H. Abbring and J.J. Heckman

whether factors that predict post-treatment earnings appear in the choice equation. For
specificity, one can think of the choice as schooling (high school vs. college), and the
outcomes as earnings.
2.9.3. The estimation of the components in the information set
We show how we can determine the unobservable components of the information set IA
of the agent at the time of the choice by exploring the convenient structure provided by
the factor models. Assume that X, Z, εC , and the factor loadings and parameters of cost
equations and outcome equations are in the information set IA . We can test for what
is in agent’s decision sets using the Sims test described in Section 2.9.1. To conserve
on notation, we define factor loadings on each factor in (2.12) using the condensed
expression
αk,D =

T̄

t=1

1
1+r

t−1
(αk,1,t − αk,0,t ) − αk,C

for k = 1, . . . , K.

(2.13)

Suppose that for a two-factor (K = 2) model, θ1 and θ2 are in the agent’s information
set IA but εs,t is not. If the null hypothesis that θ1 and θ2 are in IA is true, we may write
the choice index D ∗ as:
D ∗ = μD (X, Z) + α1,D θ1 + α2,D θ2 + εC .

(2.14)

The choice index is written in terms of structural parameters using (2.10). From our
analysis of Step 2, we can identify μD (X, Z) and βs,t for all s and t. Given observations on X and Z, we can obtain from data on outcomes, (Y, X, D, Z), the covariance
between the terms D ∗ − μD (X, Z) and Y1,1 − Xβ1,1 . Under the null hypothesis that θ1
and θ2 are both in the agents’ information sets, this covariance is equal to


Cov D ∗ − μD (X, Z), Y1,1 − μ1,1 (X) = α1,D α1,1,1 σθ21 + α2,D α2,1,1 σθ22 . (2.15)
We seek to test the null that θ1 and θ2 are in IA against alternative hypotheses. To
fix ideas, consider the alternative assumption that θ1 is in IA but θ2 is not, and maintain
that E[θ2 | IA ] = 0. If the alternative is valid, the choice index (2.14) may be written
as
D ∗ = μD (X, Z) + α1,D θ1 + εC .

(2.16)
D∗

In this case, the covariance between the terms
− μD (X, Z) and Y1,1 − μ1,1 (X)
satisfies


Cov D ∗ − μD (X, Z), Y1,1 − μ1,1 (X) = α1,D α1,1,1 σθ21 ,
(2.17)
and the difference between the choice generated by the null and the alternative hypotheses is the term α2,D α2,1,1 σθ22 that appears in (2.15) but not in (2.17). This insight allows
us to redefine the Sims test by generating parameters κθ1 and κθ2 to satisfy:

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5189



Cov D ∗ − μD (X, Z), Y1,1 − μ1,1 (X) − κθ1 α1,D α1,1,1 σθ21
− κθ2 α2,D α2,1,1 σθ22 = 0.
It is easy to see how we can rewrite the test in terms of κθ1 and κθ2 . We conclude that
agents know and act on the information contained in factors 1 and 2, so that θ1 and θ2
are in IA , if we reject both κθ1 = 0 and κθ2 = 0. Parallel tests can be conducted for
other components of realized earnings.
It remains to be shown that we can actually identify all of the parameters of the model,
in particular, the function μD (X, Z), the parameters β and α in the test and earnings
equations, the distribution of the factors, Fθ , as well as the distribution of idiosyncratic
components Fε in the measurement, outcomes and cost equations.
We start by analyzing the measurement equations which in the context of a schooling
choice problem could be test score equations. We assume that the measurement equations only depend on θ1 and not the other factors. In an analysis of college choices, test
scores are typically available for all agents before their decisions are made, and they
proxy ability. By assumption, there is no selection bias in observations on the measurement equations. We can identify the mean outcome equations μM,n (X), n = 1, . . . , N ,
where N is the number of measurements.
Given knowledge of these parameters, we can construct differences Mn − μM,n (X)
and compute the covariances, as in the case of three measurements:


Cov M1 − μM,1 (X), M2 − μM,2 (X) = α1M α2M σθ21 ,
(2.18)


M M 2
Cov M1 − μM,1 (X), M3 − μM,3 (X) = α1 α3 σθ1 ,
(2.19)


M M 2
Cov M2 − μM,2 (X), M3 − μM,3 (X) = α2 α3 σθ1 .
(2.20)
The left-hand sides of (2.18), (2.19), and (2.20) can be computed from sample moments.
The right-hand sides of (2.18), (2.19), and (2.20) are implications of the factor model,
assuming measurements are dependent only through θ1 . We need to normalize one of
the factor loadings. Let α1M = 1. If we take the ratio of (2.20) to (2.18), we identify α3M . Analogously, the ratio of (2.20) to (2.19) allows us to recover α2M . Given the
normalization of α1M = 1 and identification of α2M , we recover σθ21 from (2.18). Finally,
we can identify the variance of εkM from the variance of Mk − μM,k . Because the factor
θ1 and uniquenesses εk are independently normally distributed random variables, we
have identified their distribution. Normality plays no crucial role here. Our analysis
in Section 2.7.3 shows how this analysis can be made fully nonparametric under the
conditions of Theorem 1.
2.9.4. Outcome and choice equations
Establishing the identification of the joint distribution of outcomes requires more work
because of the evaluation problem. We only observe one stream of outcomes for each
agent, corresponding to outcomes associated with treatment D. It is at this stage of the
analysis that focusing the discussion on normally distributed factors and uniquenesses

5190

J.H. Abbring and J.J. Heckman

becomes helpful for understanding how identification can be secured. We can use the
closed-form solutions developed in the traditional econometric literature to reduce the
identification problem to the identification of a few parameters. However, the analysis
does not require normality.
All of the dependence among U0,t , U1,t , and UC is captured through the factors θ1
and θ2 . To establish identification most transparently, assume that they are normally
distributed with the following mean and covariance matrix:

  2

σθ1 0
θ1
0
∼N
,
.
θ2
0
0 σθ22
Because of the loadings α1,s,t , α2,s,t , α1,C , and α2,C the factors θ can affect U0,t , U1,t ,
and UC differently. By adopting the factor structure representation, we are not imposing,
for example, perfect ranking in the sense that the best in the distribution of earnings in
sector s at period t is the best (or the worst) in the distribution of earnings in sector
s  at period t  as in the models of rank invariance surveyed in Section 2.5. The joint
distribution of the earnings Y0,t , Y1,t conditional on X is:


Y0,t 
X
Y1,t
2
2
 μ (X)   α1,0,t
σθ2 +α2,0,t
σθ2 +σε2
α1,0,t α1,1,t σθ2 +α2,0,t α2,1,t σθ2 
0,t
0,t
1
2
1
2
.
∼ N μ (X) ,
(2.21)
2
2
2
2
2
2
2
1,t

α1,0,t α1,1,t σθ +α2,0,t α2,1,t σθ
1

 Y0,t 

2

α1,1,t σθ +α2,1,t σθ +σε
1

2

1,t

 Y0,t  

The joint distribution of Y1,t and Y  is fully determined by the means of each
1,t
vector, the variance matrix of each vector, and the covariance matrix
 



Y0,t  
Y0,t
,
Cov
X
Y1,t
Y1,t 


α1,0,t α1,0,t  σθ21 + α2,0,t α2,0,t  σθ22 α1,0,t α1,1,t  σθ21 + α2,0,t α2,1,t  σθ22
=
,
α1,0,t α1,1,t  σθ21 + α2,0,t α2,1,t  σθ22 α1,1,t α1,1,t  σθ21 + α2,1,t α2,1,t  σθ22
(2.22)
for all t = t  . If we determine the means and the covariances across all of
the t, t  under a normality assumption, we fully specify the joint distributions
of (Y0,1 , . . . , Y0,T̄ , Y1,1 , . . . , Y1,T̄ ) and (Y0,1 , . . . , Y0,T̄ , Y1,1 , . . . , Y1,T̄ , D ∗ ). As a result, identification of the joint distributions reduces to the identification of the functions
μ0,t (X), μ1,t (X), αk,s,t , αk,D , σεs,t , σεC (possibly up to scale) and σθ2j for s = 0, 1;
t = 1, . . . , T̄ and j = 1, 2, and k = 1, 2. This also entails identification of the distributions of θ1 and θ2 as well as the parameters associated with the choice equation. Using
the methods discussed in Sections 2.7 and 2.8, we can relax the normality assumption. The factor structure is essential to model the dependence across observations. The
factors can be nonnormal. We now show an example of how to secure identification.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5191

From the observed data and the factor structure it follows that
E(Y1,t | X, Z, D = 1) = μ1,t (X) + α1,1,t E[θ1 | X, Z, D = 1]
+ α2,1,t E[θ2 | X, Z, D = 1]
+ E[ε1,t | X, Z, D = 1].
(2.23)

1 t−1
) (Y1,t − Y0,t ) − C | IA )  0.
The event D = 1 is the event D ∗ = E( T̄t=1 ( 1+r
For simplicity, assume that r is known by the analyst. It can be identified along with the
other parameters.39
It is important to distinguish the role played by the factors θ from the role played
by the uniquenesses εs,t . We assume in this example that the εs,t are unknown to the
agent at the time choices are made. If not, those components would fail the Sims test
for their exclusion from the choice equation, and would be in agent information sets.
By definition, the terms that affect the covariance between future outcomes and choices
are what is in the information set at the time choices are made. Under our assumptions
and initial specification of the information set,
 T̄
 
t−1


1
(Y1,t − Y0,t ) − C  IA
E
1+r
t=1

= μD (X, Z) + α1,D θ1 + α2,D θ2 − εC .
Let VD be the linear combination of the three independent normal random variables in
the decision rule:
VD = α1,D θ1 + α2,D θ2 − εC .
2 σ 2 + α 2 σ 2 + σ 2 and
Then, VD ∼ N (0, σV2D ), with σV2D = α1,D
εc
θ1
2,D θ2

D=1

⇔

VD  −μD (X, Z).

(2.24)

We now use standard normal sample selection arguments to establish identifiability. If
we use representation (2.24) in place of D = 1 in Equation (2.23) and use the fact that
εs,t is independent of X, Z, and VD , it follows that


E(Y1,t | X, Z, D = 1) = μ1,t (X) + α1,1,t E θ1 | X, Z, VD  −μD (X, Z)


+ α2,1,t E θ2 | X, Z, VD  −μD (X, Z) .
(2.25)
Second, because θ1 , θ2 and VD are normal random variables, we can use the projection
property for normal random variables to break θj into statistically independent components predictable by VD and components that are not predictable:
θj =

Cov(θj , VD )
VD + νj
Var(VD )

for j = 1, 2,

39 See the discussion and references in Section 3.

(2.26)

5192

J.H. Abbring and J.J. Heckman

where νj is a mean zero, normal random variable independent of VD . Because
Cov(θ1 , VD ) = σθ21 α1,D and Cov(θ2 , VD ) = σθ22 α2,D , it follows that:
 σθ2 α1,D 


E θ1 | X, Z, VD  −μD (X, Z) = 1 2 E VD | X, Z, VD  −μD (X, Z) ,
σVD

 σθ2 α2,D 

E θ2 | X, Z, VD  −μD (X, Z) = 2 2 E VD | X, Z, VD  −μD (X, Z) .
σVD
From the standard normal selection formulae presented in Appendix C of Chapter 70,


φ μDσ(X,Z)


VD
E Y1,t | X, Z, VD  −μD (X, Z) = μ1,t (X) + π1,t  μ (X,Z)  ,
(2.27)
Φ DσV
D

where φ is the density, Φ is the cdf of the unit normal, and
π1,t =

Cov(U1,t , VD )

=

α1,D α1,1,t σθ21 + α2,D α2,1,t σθ22

.
1
σVD
(Var(VD )) 2
Following the same steps, we can derive a similar expression for mean observed earnings in sector “0”:


φ μDσ(X,Z)


VD
E Y0,t | X, Z, VD < −μD (X, Z) = μ0,t (X) − π0,t  μ (X,Z)
(2.28)
 .40
Φ − DσV
D

Standard arguments show that we can identify μ0,t (X), μ1,t (X), π0,t , and π1,t . Given
identification of βs,t for all s and t, we can construct the differences Ys,t − μs,t (X) and
compute the covariances:


Cov M1 − μM,1 (X), Y0,t − μ0,t (X) = α1,0,t σθ21 ,
(2.29)


2
Cov M1 − μM,1 (X), Y1,t − μ1,t (X) = α1,1,t σθ1 .
(2.30)
The left-hand sides of (2.29) and (2.30) are identified from sample moments. The righthand sides are implied by the factor model and the assumption that the measurements
depend only on factor 1. We determined σθ21 from the analysis of the test scores. From
Equations (2.29) and (2.30) we can recover α1,0,t and α1,1,t for all t. Note that we can
α
by computing the covariance:
also identify the σ1,C
VD

D ∗ − μD (X, Z)
Cov M1 − μM,1 (X),
σVD
T̄
1 t−1
(α1,1,t − α1,0,t ) − α1,C 2
t=1 ( 1+r )
=
(2.31)
σθ1 .
σVD
2
2
40 π
0,t = (α1,D α1,0,t σθ1 + α2,D α2,0,t σθ2 )/σVD .

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5193

Using (2.29) and (2.30), we can identify α1,1,t and α1,0,t for all t. The only remaining
α
term to be identified is the ratio σ1,C
, which we can obtain from the covariance equation
VD
(2.31).
With enough panel data on outcomes, we can also identify the parameters related to
factor θ2 , such as α2,s,t and σθ22 . To see this, first normalize α2,0,1 = 1 and compute the
covariances:


Cov Y0,1 − μ0,1 (X), Y0,2 − μ0,2 (X) − α1,0,1 α1,0,2 σθ21 = α2,0,2 σθ22 ,

D ∗ − μD (X, Z)
Cov Y0,1 − μ0,1 (X),
σVD

1 t−1
) (α1,1,t − α1,0,t ) − α1,C )
α1,0,1 σθ21 T̄t=1 (( 1+r
−
σVD

1 t−1
) (α2,1,t − α2,0,t ) − α2,C )
σθ22 T̄t=1 (( 1+r
=
,
σVD

D ∗ − μD (X, Z)
Cov Y0,2 − μ0,2 (X),
σVD

1 t−1
α1,0,2 σθ21 T̄t=1 (( 1+r
) (α1,1,t − α1,0,t ) − α1,C )
−
σVD

1 t−1
α2,0,2 σθ22 T̄t=1 (( 1+r
) (α2,1,t − α2,0,t ) − α2,C )
=
.
σVD

(2.32)

(2.33)

(2.34)

The left-hand sides of (2.32), (2.33), and (2.34) are identified from sample moments.
If we compute the ratio of (2.34) to (2.33) we can recover α2,0,2 . From (2.32), we can
recover σθ22 . From the covariances from the earnings associated with s = 1,


Cov Y1,1 − μ1,1 (X), Y1,2 − μ1,2 (X) − α1,1,1 α1,1,2 σθ21 = α2,1,1 α2,1,2 σθ22 , (2.35)

D ∗ − μD (X, Z)
Cov Y1,1 − μ1,1 (X),
σVD

1 t−1
) (α1,1,t − α1,0,t ) − α1,C )
α1,1,1 σθ21 T̄t=1 (( 1+r
−
σVD

1 t−1
) (α2,1,t − α2,0,t ) − α2,C )
α2,1,1 σθ22 T̄t=1 (( 1+r
=
,
(2.36)
σVD

5194

J.H. Abbring and J.J. Heckman

Cov Y1,2 − μ1,2 (X),
−
=

α1,1,2 σθ21

α2,1,2 σθ22

T̄

D ∗ − μD (X, Z)
σVD

1 t−1
(α1,1,t
t=1 (( 1+r )

T̄



− α1,0,t ) − α1,C )

σVD

1 t−1
(α2,1,t
t=1 (( 1+r )

σVD

− α2,0,t ) − α2,C )

.

(2.37)

Taking the ratios of (2.37) to (2.35) and (2.36) to (2.35) and assuming nonzero denominators, we obtain α2,1,2 and α2,1,1 respectively. Finally, we use the information in
Var(Y0,t | X, Z, D = 0) and Var(Y1,t | X, Z, D = 1) to compute σε20,t and σε21,t , respectively. Thus we can identify all of the elements that characterize the joint distribution as
specified in (2.21) and can construct the counterfactual joint distributions. Using the factor loadings identified within each treatment group, we can form the covariance (2.22)
and identify the joint distribution of (Y0,1 , . . . , Y0,T̄ , Y1,1 , . . . , Y1,T̄ ), and, in a similar
fashion, the joint distribution of (Y0,1 , . . . , Y0,T̄ , Y1,1 , . . . , Y1,T̄ , D ∗ ).
Our use of normality in this example is merely for expositional convenience. As
established in Section 2.7.3 and in Section 2.8, all we require is the factor structure
assumption (2.6). We can nonparametrically identify all means and distributions of unobservables as a consequence of Theorem 2. The covariances are a by-product of a
general nonparametric identification analysis. We next consider two applications of the
method. In the context of an analysis of college choice and earnings, they show examples of how to use panel data to identify agent information sets, regret, intrinsic
uncertainty and ex ante and ex post distributions, and the psychic costs facing agents at
the time they make their schooling decisions.
2.10. Two empirical studies
This subsection presents two applications of the factor methodology exposited in this
section. We draw on work by Cunha and Heckman (2007b, 2008). The computational
algorithms used to compute the estimates are described in Carneiro, Hansen and Heckman (2003), Cunha, Heckman and Navarro (2005, 2006) and Cunha and Heckman
(2007b, 2008). Geweke and Keane (2001) present relevant background on the Bayesian
computational methods used to produce the estimates reported here.
Using data from the National Longitudinal Sample of Youth (NLSY79) on lifetime
earnings, ability and college choices for white males, Cunha and Heckman (2007b)
estimate a six-factor model (K = 6). The θ are assumed to be mutually independent.
Agents are assumed to know εC , the coefficients of the factors and the regression coefficients, but not the ε’s in the earnings equation. They can update their expectation of θ
after choices are made, as in the normal model presented in the preceding section. The
θ are estimated as mixtures of normals and there is strong evidence that most of the
components are nonnormal. Using the Sims testing procedure described in Section 2.9,
Cunha and Heckman conclude that three factors (θ1 , θ2 , θ3 ) are in agents’ information
sets at the age college going decisions are made.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5195

Table 4
Ex ante conditional distributions for the NLSY79 (college earnings Y1 conditional on high school earnings Y0 )
High school

1
2
3
4
5
6
7
8
9
10

College
1

2

3

4

5

6

7

8

9

10

0.2995
0.2273
0.1532
0.1110
0.0748
0.0494
0.0306
0.0236
0.0264
0.0457

0.1685
0.2119
0.1840
0.1368
0.1100
0.0866
0.0582
0.0348
0.0262
0.0182

0.1114
0.1597
0.1656
0.1492
0.1244
0.1146
0.0904
0.0531
0.0316
0.0214

0.0789
0.1271
0.1472
0.1474
0.1413
0.1204
0.1094
0.0769
0.0459
0.0216

0.0570
0.0907
0.1146
0.1418
0.1459
0.1371
0.1264
0.0989
0.0651
0.0321

0.0413
0.0678
0.0914
0.1184
0.1403
0.1399
0.1436
0.1252
0.0929
0.0446

0.0393
0.0450
0.0642
0.0882
0.1172
0.1283
0.1506
0.1638
0.1308
0.0772

0.0431
0.0288
0.0434
0.0588
0.0836
0.1242
0.1430
0.1799
0.1784
0.1176

0.0471
0.0180
0.0230
0.0334
0.0462
0.0736
0.1064
0.1676
0.2431
0.2291

0.1137
0.0236
0.0132
0.0148
0.0162
0.0258
0.0414
0.0761
0.1594
0.3925

Notes: Pr(di < Y1 < di+1 | dj < Y0 < dj +1 , I) where di is the ith decile of the college lifetime ex ante
earnings distribution and dj is the j th decile of the high school ex ante lifetime earnings distribution. The
agent fixes unknown θ at their means. The information set includes {θ1 , θ2 , θ3 }. Correlation (Y1 , Y0 ) =
0.1666.
Source: Cunha and Heckman (2007b).

Table 4 presents the estimated ex ante conditional distributions of the college earnings
conditional on high school earnings in the overall population. They show a mild positive
correlation that is far from the perfect dependence across potential outcomes assumed
by the rank invariance approaches discussed in Section 2.5. Table 5 shows the ex post
joint distribution of college earnings after all components of θ and the ε are realized.41
The dependence across potential outcomes in the ex post distribution is stronger than
that in the ex ante distribution.
Table 6 documents that there are substantial unpredictable components in college Y1 ,
high school Y0 and the returns (Y1 − Y0 ) distributions after conditioning on X, Z at
the time agents make their schooling decisions. Figures 1–3 plot the distributions of
total residual and unforecastable components of Y1 − Y0 , Y1 and Y0 , respectively, where
forecasts are measured from the date college decisions are made (age 17). There are
substantial components of uncertainty that are distinct from variability observed in the
data. Ex post, many agents regret their choices (see Table 7). Only 3.1% of those who
attend college regret that decision, while 7.5% of those who do not proceed beyond high
school regret not attending college.42
41 It assumes that, ex post, agents perfectly observe all potential outcome streams. More realistically, agents

would only know one stream or the other.
42 This calculation is for a stationary environment and ignores the secular growth in the mean earning gap

between college and high school graduates that is documented by Katz and Autor (1999). Accounting for the
growth in this gap substantially reduces the regret of those going to college and raises the regret of those who
stopped at high school.

5196

J.H. Abbring and J.J. Heckman

Table 5
Ex post conditional distributions for the NLSY79 (college earnings Y1 conditional on high school earnings Y0 )
High school

1
2
3
4
5
6
7
8
9
10

College
1

2

3

4

5

6

7

8

9

10

0.2118
0.1684
0.1374
0.1080
0.0787
0.0656
0.0548
0.0428
0.0416
0.0386

0.1614
0.1777
0.1676
0.1336
0.1105
0.1028
0.0779
0.0507
0.0436
0.0204

0.1188
0.1557
0.1464
0.1433
0.1232
0.1149
0.0842
0.0741
0.0474
0.0269

0.0932
0.1213
0.1390
0.1378
0.1335
0.1201
0.1097
0.0880
0.0577
0.0292

0.0782
0.1038
0.1244
0.1213
0.1345
0.1276
0.1196
0.0994
0.0803
0.0339

0.0654
0.0862
0.0954
0.1115
0.1291
0.1330
0.1224
0.1224
0.1001
0.0520

0.0532
0.0640
0.0754
0.0980
0.1144
0.1250
0.1410
0.1410
0.1277
0.0704

0.0554
0.0516
0.0577
0.0746
0.0862
0.0998
0.1331
0.1585
0.1728
0.1155

0.0651
0.0417
0.0333
0.0475
0.0614
0.0823
0.1132
0.1539
0.1939
0.1945

0.0974
0.0296
0.0234
0.0243
0.0286
0.0288
0.0441
0.0693
0.1348
0.4186

Notes: Pr(di < Y1 < di+1 | dj < Y0 < dj +1 , I) where di is the ith decile of the college lifetime
ex post earnings distribution and dj is the j th decile of the high school ex post lifetime earnings distribution.
Individual fixes unknown θ at their means. The information set includes {θ1 , θ2 , θ3 , θ4 , θ5 , θ6 }. Correlation
(Y1 , Y0 ) = 0.2842.
Source: Cunha and Heckman (2007b).

Table 6
Uncertainty at age 17 about future returns

Total residual variance∗
Variance of unforecastable components∗

College

High school

Returns

709.7487
372.3509

507.2910
272.3596

906.0066
432.8733

Source: Cunha and Heckman (2007b).
∗ After conditioning on X, Z.

Table 7
Percentage that regret schooling choices
Percentage of high school graduates who regret not graduating from college
Percentage of college graduates who regret graduating from college

0.0749
0.0311

Notes: Ex post people know their “luck” components (i.e., the uncertain εs,t for each schooling group s for
all ages t on their earnings equations) when making their schooling decisions. These calculations are for
a stationary environment and ignore the growth in the mean of college distribution experienced in recent
decades.
Source: Cunha and Heckman (2007b).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5197

Figure 1. The densities of total residual vs. unforecastable components. Returns to college vs. high school
(NLSY79). In this figure, we plot the density of the total residual (the solid curve) against the density
of unforecastable components (the dashed curve) for the present value of returns to college from ages 22
to 41. The present value of returns to college is calculated using a 5% interest rate.
Source: Cunha and Heckman (2007b).

Figure 2. The densities of total residual vs. unforecastable components in present value of high school
earnings. In this figure, we plot the density of the total residual (the solid curve) against the density of unforecastable components (the dashed curve) for the present value of high school earnings from ages 22 to 41. The
present value of earnings is calculated using a 5% interest rate. Source: Cunha and Heckman (2007b).

5198

J.H. Abbring and J.J. Heckman

Figure 3. The densities of total residual vs. unforecastable components in present value of college earnings.
In this figure, we plot the density of the total residual (the solid curve) against the density of unforecastable
components (the dashed curve) for the present value of college earnings from ages 22 to 41. The present value
of earnings is calculated using a 5% interest rate. Source: Cunha and Heckman (2007b).

Selection on the first three factors is illustrated in Figures 4–6. Factor one is associated with ability as measured by a test score (θ1 in the examples of the previous
sections). The factors sort on the basis of schooling choices. Cunha and Heckman
(2007b) show that accounting for nonnormality of the factors is empirically important.
Table 8 presents the selection-corrected mean rates of return to 4 years of college. It
is close to 10% for college goers, 8.25% for those who choose to stop their education at
high school and 8.75% for those who are at the margin of indifference between attending high school and going to college. Matching assumption (M-1), which requires that
average returns equal marginal returns, is not supported by these estimates. For further
details on these estimates, see Cunha and Heckman (2007b).
To show the possibilities for a more nuanced approach to policy evaluation that is
possible, we draw on a second, earlier, paper by Cunha and Heckman (2008). While this
research is superceded by the richer empirical analysis in Cunha and Heckman (2007b),
it illustrates the potential of the method exposited in this chapter.43 They estimate a

43 Cunha and Heckman (2007b) use many more periods of panel data, have many more measurements and

estimate a six-factor model. Cunha and Heckman (2008) use many fewer periods, have a lower dimension
L(s) in the notation of condition (2.9) and determine that K = 2 fits the data they analyze.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5199

Figure 4. Densities of factor 1 by schooling level (NLSY79). The solid line plots the density of the factor for
high school graduates. The dashed line plots the density of the factor for college graduates. Source: Cunha
and Heckman (2007b).

Figure 5. Densities of factor 2 by schooling level (NLSY79). The solid line plots the density of the factor for
high school graduates. The dashed line plots the density of the factor for college graduates. Source: Cunha
and Heckman (2007b).

5200

J.H. Abbring and J.J. Heckman

Figure 6. Densities of factor 3 by schooling level (NLSY79). The solid line plots the density of the factor for
high school graduates. The dashed line plots the density of the factor for college graduates. Source: Cunha
and Heckman (2007b).
Table 8
Mean rates of return to college by schooling group (NLSY79)
Schooling group

Mean returns

Standard error

High school graduates
College graduates
Individuals at the margin

0.3095
0.3994
0.3511

0.0113
0.0129
0.0535

Source: Cunha and Heckman (2007b).

two-factor model. Figures 7 and 8 plot the densities of the present value of earnings and
the associated counterfactual distribution for college graduates (D = 1, Figure 7) and
0
) are plotted in
high school graduates (D = 0, Figure 8). Gross rates of return ( Y1Y−Y
0
Figure 9 for both high school and college graduates.
The overlap in the factual and counterfactual distributions for each schooling level is
substantial. The returns to college for high school graduates are substantial. One reason why such large monetary returns to college are not realized is shown in Figure 10
which plots the psychic costs (C) of attending college. Confirming earlier findings by
Carneiro, Hansen and Heckman (2003) and Cunha, Heckman and Navarro (2005), there
are substantial psychic costs of attending school for the high school graduates (see Figure 10).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5201

Figure 7. Density of present value of earnings in the college sector. Let Y1 denote present value of earnings
(discounted at a 3% interest rate) in the college sector. Let f1 (y1 ) denote its density function. The dashed line
plots the predicted Y1 density conditioned on choosing college, that is, f1 (y1 | D = 1), while the solid line
shows the counterfactual density function of Y0 for those agents that are actually college graduates, that is,
f0 (y0 | D = 1). Source: Cunha and Heckman (2008).

Figure 8. Density of present value of earnings in the high school sector. Let Y0 denote present value of
earnings (discounted at a 3% interest rate) in the high school sector. Let f0 (y0 ) denote its density function.
The solid curve plots the predicted Y0 density conditioned on choosing high school, that is, f0 (y0 | D = 0),
while the dashed line shows the counterfactual density function of Y1 for those agents that are high-school
graduates, that is, f1 (y1 | D = 0). Source: Cunha and Heckman (2008).

5202

J.H. Abbring and J.J. Heckman

Figure 9. Density of ex post returns to college by schooling level chosen. Let Y0 , Y1 denote the present value
of earnings in high school and college sectors, respectively. Define ex post returns to college as the ratio
R = (Y1 − Y0 )/Y0 . Let f (r) denote the density function of random variable R. The solid line is the density
of ex post returns to college for high school graduates, that is, f (r | D = 0). The dashed line is the density of
ex post returns to college for college graduates, that is, f (r | D = 1). Source: Cunha and Heckman (2008).

Figure 10. Density of monetary value of psychic cost both overall and by schooling level. In this figure we plot
the monetary value of psychic costs, which we denote by C. It is defined as: C = Zγ +θ1 α1,C +θ2 α2,C +εC .
The contribution of ability to the costs of attending college, in monetary value, is θ1 α1,C . Source: Cunha and
Heckman (2008).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5203

A comparison of Figures 9 and 10 is revealing. There are small differences in objective returns to college between those who go to college and those who do not. However,
the subjective returns (inclusive of psychic costs) are substantially different. This evidence of large subjective costs highlights the value of the econometric approach to the
evaluation of social programs, and the importance of the distinction between objective
and subjective outcomes in interpreting choices and outcomes.
As an example of the power of these methods to evaluate the consequences of policy
on income inequality, Cunha and Heckman (2008) analyze a cross-subsidized tuition
policy indexed by family income level. The traditional approach to policy evaluation
compares overall income distributions before and after a policy change is implemented.
Although this approach can be justified by certain axiomatic approaches [see, e.g.,
Foster and Sen (1997), and Cowell (2000)], it does not present a very accurate summary of the true distributional consequences of policies.
Cunha and Heckman (2008) construct joint distributions of outcomes within policy
regimes (treatment and no treatment or schooling and no schooling) and joint distributions of outcomes (Y = DY1 + (1 − D)Y0 ) across policy regimes. The policy they
analyze is as follows. A prospective student whose family income at age 17 is below the
mean is allowed to attend college free of charge. The policy is self financing within each
schooling cohort. To pay for this policy, persons attending college with family income
above the mean pay a tuition charge equal to the amount required to cover the costs of
the students from lower income families as well as their own.
Total tuition raised covers the cost Q of educating each student. Thus if there are
NP poor students and NR rich students, total costs are (NP + NR )Q. For the proposed
policy, the poor pay nothing. So each rich person is charged a tuition

NP
T =Q 1+
.
NR
To determine T , notice that there is a unique tuition level T such that

NP (T )
,
T =Q 1+
NR (T )
with NP (T ) and NR (T ) the numbers of poor and rich people attending college if the
rich pay a fee T . They iterate to find the unique self-financing T . Notice that NP (T ),
the number of poor people who attend college when tuition is zero, is the same for all
values of T (NP (T ) = NP (0) for all T ). NR is sensitive to the tuition level charged.
Figure 11 shows that the marginal distributions of overall income in both the prepolicy state and the post-policy state are essentially identical. Under the standard
anonymity postulate used to evaluate income distributions [see Foster and Sen (1997),
and Cowell (2000)], we would judge these two situations as equally good using Lorenz
measures or second order stochastic dominance. Cunha and Heckman (2008) move beyond anonymity and analyze the effect that the policy has on what Fields (2003) calls
“positional” mobility.

5204

J.H. Abbring and J.J. Heckman

Figure 11. Density of present value of lifetime earnings before and after implementing cross-subsidy policy.
Let Y A , Y B denote the observed present value of earnings pre and post policy, respectively. Define f (y A ),
g(y B ) as the marginal densities of present value of earnings pre and post policy. In this figure, we plot f (y A ),
g(y B ). Source: Cunha and Heckman (2007a).

Panel 1 of Table 9 presents this analysis by describing how the 9.2% of the people
who are affected by the policy move between deciles of the distribution of income.
These statistics describe movements from one income distribution in the initial regime
to another income distribution associated with the new regime. The policy affects more
people at the top deciles than at the lower deciles. Around half of the people affected
who start at the first decile remain at the first decile. People in the middle deciles
are spread both up and down and a large proportion of people in the upper deciles is
moved into a lower position (only sixteen percent of those starting on the top decile
(the first) remain there after the policy is implemented). Moving beyond the anonymity
postulate (which instructs us to examine only marginal distributions), we learn much
more about the effects of the policy on different groups by looking at joint distributions.
Thus far, we have focused on constructing and interpreting the joint distribution of
outcomes across the two policy regimes. If outcomes under both regimes are observed,
these comparisons could be made using panel data. No use of econometric analysis
would be necessary. However, the methods discussed by Cunha and Heckman (2007b)
will apply if either or both policy regimes are unobserved but are proposed. Taking
advantage of the fact that we can identify not only joint distributions of earnings over
policy regimes but also over counterfactual states within regimes we can learn a great

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5205

Table 9
Mobility of people affected by cross-subsidizing tuition
Fraction by
decile of
origin

Deciles of
origin

Probability of moving to a different decile of the lifetime earnings distribution
1

2

3

4

5

6

7

8

9

10

0.0076
0.0870
0.1888
0.1615
0.0571
0.0142
0.0420
0.1495
0.2733
0.0517

0.0012
0.0322
0.1387
0.2084
0.1411
0.0415
0.0082
0.0388
0.2302
0.2082

0.0000
0.0025
0.0409
0.1557
0.2456
0.1671
0.0348
0.0034
0.0447
0.3242

0.0000
0.0002
0.0034
0.0360
0.1396
0.2605
0.2346
0.0513
0.0014
0.2490

0.0000
0.0000
0.0000
0.0000
0.0055
0.0425
0.1827
0.3029
0.2459
0.1626

Panel 2
High school. Fraction of total population who switch from high school to college due to the policy: 0.0450
0.1014
1
0.4049 0.2618 0.1817 0.0958 0.0427 0.0112 0.0018 0.0000 0.0000
0.1282
2
0.0382 0.1220 0.2176 0.2325 0.2200 0.1210 0.0448 0.0035 0.0003
0.1372
3
0.0023 0.0188 0.0692 0.1536 0.2244 0.2701 0.1984 0.0584 0.0049
0.1370
4
0.0000 0.0016 0.0088 0.0368 0.1116 0.2417 0.3123 0.2332 0.0540
0.1288
5
0.0000 0.0000 0.0007 0.0052 0.0277 0.0903 0.2324 0.4047 0.2300
0.1125
6
0.0000 0.0000 0.0000 0.0004 0.0024 0.0151 0.0792 0.3209 0.5004
0.1019
7
0.0000 0.0000 0.0000 0.0000 0.0000 0.0009 0.0101 0.0761 0.5133
0.0798
8
0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0067 0.1440
0.0559
9
0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0032
0.0173
10
0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000

0.0000
0.0000
0.0000
0.0000
0.0090
0.0816
0.3997
0.8493
0.9968
1.0000

Panel 3
College. Fraction of total population who switch from college to high school due to the policy: 0.0473
0.0460
1
0.9066 0.0878 0.0056 0.0000 0.0000 0.0000 0.0000 0.0000
0.0477
2
0.6423 0.2972 0.0534 0.0062 0.0009 0.0000 0.0000 0.0000
0.0562
3
0.3763 0.4510 0.1501 0.0211 0.0015 0.0000 0.0000 0.0000
0.0649
4
0.1860 0.4648 0.2559 0.0868 0.0059 0.0007 0.0000 0.0000
0.0794
5
0.0753 0.3801 0.3518 0.1522 0.0347 0.0059 0.0000 0.0000
0.0985
6
0.0138 0.2001 0.3602 0.3064 0.1059 0.0133 0.0004 0.0000
0.1152
7
0.0011 0.0618 0.2598 0.3603 0.2337 0.0766 0.0066 0.0000
0.1371
8
0.0000 0.0071 0.0807 0.2744 0.3436 0.2323 0.0603 0.0015
0.1623
9
0.0000 0.0000 0.0073 0.0559 0.2084 0.3628 0.3056 0.0593
0.1926
10
0.0000 0.0000 0.0000 0.0002 0.0044 0.0561 0.2260 0.3519

0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0911

Panel 1
Overall. Fraction of total population who switch schooling levels: 0.0923
0.0730
1
0.5680 0.2052 0.1245 0.0647 0.0288
0.0869
2
0.2079 0.1712 0.1715 0.1690 0.1585
0.0957
3
0.1148 0.1489 0.0935 0.1137 0.1573
0.1001
4
0.0619 0.1557 0.0910 0.0534 0.0764
0.1035
5
0.0296 0.1495 0.1387 0.0630 0.0304
0.1053
6
0.0066 0.0959 0.1726 0.1471 0.0520
0.1087
7
0.0006 0.0336 0.1411 0.1956 0.1269
0.1092
8
0.0000 0.0046 0.0519 0.1765 0.2211
0.1104
9
0.0000 0.0000 0.0055 0.0421 0.1570
0.1071
10
0.0000 0.0000 0.0000 0.0002 0.0041

0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0008
0.2702

Notes: Cross subsidy consists in making tuition zero for people with family income below average and making
the budget balance by raising tuition for college students with family income above the average. For example,
we read from the first panel row 1, column 1 that 7.3% of the people who switch schooling levels come from
the lowest decile. Out of those, 56.8% are still in the first decile after the policy while 2.88% jump to the fifth
decile. Panel 2 has the same interpretation but it only looks at people who switch from high school to college
while panel 3 looks at individuals who switch from college to high school.
Source: Cunha and Heckman (2008).

5206

J.H. Abbring and J.J. Heckman
Table 10
Mobility of people affected by cross-subsidizing tuition (extracted from Table 9)

Fraction of the total population who switch schooling levels: 0.0923
Pre-policy choice
Fraction of high school graduates
Do not switch
High school

College

0.9197

Become college graduates
0.0803
Fraction of college graduates

Do not switch

Become high school graduates

0.8923

0.1077

Note: Cross subsidy consists of making tuition zero for people with family income below average and making
the budget balance by raising tuition for college students with family income above the average.
Source: Cunha and Heckman (2008).

deal more about the effects of this policy, whether or not policy regimes are observed.44
In this way, one solves problems P-1, P-2, and P-3 stated in Chapter 70.
Panels 2 and 3 of Table 9 reveal that not only 9.2% of the population is affected by
the policy, but that actually about half of them moved from high school into college
(4.5% of the population) and half moved from college into high school (4.7% percent
of the population). This translates into saying that, of those affected by the policy, 92%
of the high school graduates stay in high school in the post-policy regime while only
89% of college graduates stay put. (See Table 10.) Thus the policy is slightly biased
against college attendance. We can form the joint distributions of lifetime earnings by
initial schooling level. Figure 12 breaks out some of the evidence implicit in Table 9.
Panels 2 and 3 of Table 9 show that the policy affects very few high school graduates
at the top end of the income distribution (only 1.7% of those affected come from the
10th percentile) and a lot of college graduates in the same situation (19% of college
graduates affected come from the top decile). We can also see that the policy tends to
move high school graduates up in the income distribution and moves college graduates
down.
As another example of the generality of our method and the new insight into income
mobility induced by policy that it provides, we can determine where people come from
and where they end up at in the counterfactual distributions of earnings. Table 11 shows
where in the pre-policy distribution of high school earnings persons induced to go to
college come from and where in the post-policy distribution of college earnings they go
to. Most people stay in their decile or move to closely adjacent ones. Given that some
people benefit from the policy while others lose, it is not clear whether society as a
44 It is implausible that analysts would have panel data on policy regimes where under one regime a person

goes to school and under another he does not.

Ch. 72:
Econometric Evaluation of Social Programs, Part III
Figure 12. Fraction of people who switch schooling levels when tuition is cross subsidized by decile of origin from the lifetime earnings distribution. Cross
subsidy consists in making tuition zero for people with family income below average and making the budget balance by raising tuition for college students with
family income above the average. Source: Cunha and Heckman (2008).

5207

5208

J.H. Abbring and J.J. Heckman

Table 11
Mobility of people affected by cross-subsidizing tuition across counterfactual distributions
Panel 1
High school. Fraction of total population who switch from high school to college due to the policy: 0.0450
Fraction by
decile of
origin in the
pre-policy
high school
distribution

Deciles of
origin

0.0668
0.0813
0.0910
0.1000
0.1049
0.1060
0.1064
0.1118
0.1140
0.1176

1
2
3
4
5
6
7
8
9
10

Probability of moving to a different decile of the post-policy college lifetime
earnings distribution

1

2

3

4

5

6

7

8

9

10

0.8563
0.4046
0.1488
0.0401
0.0089
0.0004
0.0000
0.0000
0.0000
0.0000

0.1272
0.4112
0.3544
0.2343
0.0713
0.0202
0.0033
0.0004
0.0000
0.0000

0.0145
0.1491
0.3059
0.3096
0.2081
0.0950
0.0243
0.0016
0.0000
0.0000

0.0021
0.0296
0.1419
0.2490
0.3053
0.2155
0.0896
0.0159
0.0016
0.0000

0.0000
0.0055
0.0445
0.1234
0.2348
0.2761
0.1888
0.0630
0.0043
0.0000

0.0000
0.0000
0.0039
0.0379
0.1282
0.2416
0.3026
0.1690
0.0293
0.0000

0.0000
0.0000
0.0005
0.0053
0.0365
0.1273
0.2662
0.3220
0.1227
0.0027

0.0000
0.0000
0.0000
0.0004
0.0068
0.0239
0.1155
0.3228
0.3271
0.0333

0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0096
0.1024
0.4568
0.2626

0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0028
0.0582
0.7014

Panel 2
College. Fraction of total population who switch from college to high school due to the policy: 0.0473
Fraction by
decile of
origin in the
pre-policy
college
distribution

Deciles of
origin

0.1098
0.1059
0.1039
0.1016
0.1016
0.0983
0.0980
0.0956
0.0967
0.0885

1
2
3
4
5
6
7
8
9
10

Probability of moving to a different decile of the post-policy high school lifetime
earnings distribution

1

2

3

4

5

6

7

8

9

10

0.5505
0.1076
0.0180
0.0004
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000

0.2962
0.3257
0.1473
0.0355
0.0050
0.0000
0.0000
0.0000
0.0000
0.0000

0.1141
0.2937
0.2776
0.1535
0.0467
0.0091
0.0000
0.0000
0.0000
0.0000

0.0318
0.1789
0.2657
0.2349
0.1503
0.0513
0.0087
0.0004
0.0000
0.0000

0.0062
0.0716
0.1833
0.2866
0.2654
0.1678
0.0463
0.0044
0.0000
0.0000

0.0012
0.0204
0.0857
0.1890
0.2705
0.2683
0.1609
0.0430
0.0009
0.0000

0.0000
0.0016
0.0200
0.0847
0.1903
0.2972
0.3071
0.1560
0.0127
0.0000

0.0000
0.0004
0.0024
0.0150
0.0668
0.1786
0.3387
0.4020
0.1337
0.0034

0.0000
0.0000
0.0000
0.0004
0.0050
0.0276
0.1362
0.3617
0.5355
0.0915

0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0022
0.0324
0.3173
0.9051

Notes: Cross subsidy consists in making tuition zero for people with family income below average and making
the budget balance by raising tuition for college students with family income above the average. For example,
we read from the first panel row 1, column 1 that 6.68% of the people who switch from high school to college
come from the lowest decile of the pre-policy high school distribution. Out of those, 85.63% are still in the
first decile of the post-policy college earnings distribution after the policy is implemented while 1.45% “jump”
to the third decile. Panel 2 has the same interpretation but it only looks at people who switch from college to
high school.
Source: Cunha and Heckman (2008).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5209

Table 12
Voting outcome of proposing cross-subsidizing tuition
Fraction of the total population who switch schooling levels: 0.0923
920.55
Average pre-policy lifetime earnings∗
Average post-policy lifetime earnings∗
905.96
Fraction of the population who vote
Yes
0.0716
No
0.6152
Indifferent
0.3132
Note: Cross subsidy consists of making tuition zero for people with family income below average and making the budget balance by raising
tuition for college students with family income above the average.
Source: Cunha and Heckman (2008).
∗ In thousands of dollars.

whole values this policy positively or not. An advantage of examining the joint distribution of outcomes is that it allows us to calculate the effect that the policy has on welfare.
An individual’s relative utility is not only given by earnings but also by the monetary
value of psychic costs. We can predict how people would vote if the policy analyzed in
this section were proposed. Table 12 shows the result of such an exercise. The policy
lowers the mean earnings for people affected by it. Most people not indifferent to the
policy would vote against it.
We next turn to the development of models for the timing of treatment choice. The
models that distinguish ex ante from ex post outcomes discussed in this section of the
chapter have an implicit dynamics. Agents make decisions under one information set.
That information set is revised in light of subsequent flows of information. The outcomes realized after the choice is made will differ in general from the outcomes that are
anticipated. However, in this section, choices are one shot. While this framework advances models that ignore uncertainty, it does not capture the rich dynamics that comes
from updating information in real time. We next consider models that analyze the choice
of the timing of treatment and the consequences of the choices. Analyses of decisions
about the timing of dropping out of school, the timing of initiating or terminating a
medical treatment, when to end a period of unemployment, and the consequences of
such decisions raise new issues to which we now turn.
3. Dynamic models45
We now develop econometric and statistical models for the choice of timing of treatment and the consequences of alternative treatment times on subjective and objective
45 This section draws in part on Abbring and Heckman (2008) and the papers they cite.

5210

J.H. Abbring and J.J. Heckman

outcomes. The analysis presented in this section extends the analysis of multiple treatments and treatment choices presented in Chapter 71 by explicitly considering dynamics
and information updating. We first develop some main ideas in a framework with general dynamic treatments. We subsequently focus on the choice of the timing of a single
treatment which may have very different consequences when implemented in different periods. The same treatments administered at different times can be thought of as
different treatments. Thus, dropping out of school at grade 11 may have different consequences than dropping out at grade 10. Starting chemotherapy eight months after
diagnosis of the onset of cancer may have different consequences than chemotherapy
starting after one month. There is a close affinity between econometric models for
discrete choice and models for the analysis of the choice of treatment times which is
developed in this section.
The plan of this section is as follows. Section 3.1 briefly reviews the policy evaluation
problem extensively discussed by Heckman and Vytlacil in Chapter 70 and discusses
the treatment-effects approach to policy evaluation. It establishes the notation used in
the rest of this section. Section 3.2 reviews an approach to the analysis of dynamic treatment effects developed in statistics based on a sequential randomization assumption that
is popular in biostatistics [Robins (1997), Gill and Robins (2001), Lok (2007)] and has
been applied in economics [see Fitzenberger, Osikominu and Völter (2006) and Lechner
and Miquel (2002)]. This is a dynamic version of matching. We relate the assumptions justifying this approach to the assumptions underlying the econometric dynamic
discrete-choice literature based on Rust’s (1987) conditional-independence condition
which, as discussed in Section 3.4.5 below, is frequently invoked in the structural econometrics literature. We note the limitations of the dynamic matching treatment-effects
approach in accounting for dynamic information accumulation. In Sections 3.3 and 3.4,
we discuss two econometric approaches for the analysis of treatment times that allow for
nontrivial dynamic selection on unobservables. Section 3.3 discusses the continuoustime event-history approach to policy evaluation developed by Abbring and Van den
Berg (2003b, 2005) and Abbring (2008). Section 3.4 introduces an approach that builds
on and extends the discrete-time dynamic discrete-choice literature. Like the analysis
of Abbring and Van den Berg, it does not rely on the conditional-independence assumptions used in dynamic matching. This part of our survey is based on the work of
Heckman and Navarro (2007). The approach exposited in this section generalizes the
factor model approach exposited in Section 2 to a dynamic setting. The two complementary approaches surveyed in this section span the existing econometric literature on
dynamic treatment effects.
3.1. Policy evaluation and treatment effects
3.1.1. The evaluation problem
We review the evaluation problem discussed in Chapter 70 using a succinct notation
employed in the analysis of this section. Let Ω be the set of agent types. It is the sam-

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5211

ple space of a probability space (Ω, I, P), and all choices and outcomes are random
variables defined on this probability space. Each agent type ω ∈ Ω represents a single
agent in a particular state of nature. We could distinguish variation between agents from
within-agent randomness by taking Ω = J × Ω̃, with J the set of agents and Ω̃ the
set of possible states of nature. However, we do not make this distinction explicit in this
section, and often simply refer to agents instead of agent types.46
Consider a policy that targets the allocation of each agent in Ω to a single treatment from a set S. In the most basic binary version, S = {0, 1}, where “1” represents
“treatment”, such as a training program, and “0” some baseline, “control” program. Alternatively, S could take a continuum of values, e.g., R+ = [0, ∞), representing, e.g.,
unemployment benefit levels, or duration of time in a program.
A policy p = (a, τ ) ∈ A × T = P consists of a planner’s rule a : Ω → B for
allocating constraints and incentives to agents, and a rule τ : Ω × A → S that generates agent treatment choices for a given constraint allocation a. This framework allows
agent ω’s treatment choice to depend both on the constraint assignment mechanism a—
in particular, the distribution of the constraints in the population—and on the constraints
a(ω) ∈ B assigned to agent ω.47
The randomness in the planner’s constraint assignment a may reflect heterogeneity of
agents as observed by the planner, but it may also be due to explicit randomization. For
example, consider profiling on background characteristics of potential participants in
the assignment a to treatment eligibility. If the planner observes some background characteristics on individuals in the population of interest, she could choose eligibility status
to be a deterministic function of those characteristics and, possibly, some other random
variable under her control by randomization. This includes the special case in which
the planner randomizes persons into eligibility. We denote the information set generated by the variables observed by the planner when she assigns constraints, including
those generated through deliberate randomization, by IP .48 The planner’s information
set IP determines how precisely she can target agents ω when assigning constraints.
The variables in the information set fully determine the constraints assignment a.
Subsequent to the planner’s constraints assignment a, each agent ω chooses treatment τ (ω, a). We assume that agents know the constraint assignment mechanism a in
place. However, agents do not directly observe their types ω, but only observe realizations IA (ω) of some random variables IA . For given a ∈ A, agent ω’s treatment

46 For example, we could have Ω = [0, 1] indexing the population of agents, with P being Lebesgue measure

on [0, 1]. Alternatively, we could take Ω = [0, 1] × Ω̃ and have [0, 1] represent the population of agents and
Ω̃ states of nature.
47 In Chapter 70, the dependence of agent ω’s treatment choice τ on the constraints a(ω) was made explicit
by defining τ on Ω × A × B, and subsequently restricting τ to {(ω, a, b) ∈ Ω × A × B: a(ω) = b}.
Because the constraints b = a(ω) assigned are already encoded in a and ω, we can drop the constraints b
from τ assigned without loss of generality. In the dynamic context of this chapter, this convention simplifies
the discussion of dynamic information accumulation.
48 Formally, I is a sub-σ -algebra of I and a is assumed to be I -measurable.
P
P

5212

J.H. Abbring and J.J. Heckman

choice τ (ω, a) can only depend on ω through his observations IA (ω). Typically, IA (ω)
includes the variables used by the planner in determining a(ω), so that agents know
the constraints that they are facing. Other components of IA (ω) may be determinants
of preferences and outcomes. Variation in IA (ω) across ω may thus reflect preference
heterogeneity, heterogeneity in the assigned constraints, and heterogeneity in outcome
predictors. We use IA to denote the information set generated by IA .49 An agent’s information set IA determines how precisely the agent can tailor his treatment choice
to his type ω. For expositional convenience, we assume that agents know more when
choosing treatment than what the planner knows when assigning constraints, so that
IA ⊇ IP . One consequence is that agents observe the constraints a(ω) assigned to
them, as previously discussed. In turn, the econometrician may not have access to all of
the information that is used by the agents when they choose treatment.50 In this case,
IA ⊆ IE , where IE denotes the econometrician’s information set.
We define sp (ω) as the treatment selected by agent ω under policy p. With p =
(a, τ ), we have that sp (ω) = τ (ω, a). The random variable sp : Ω → S represents
the allocation of agents to treatments implied by policy p.51 Randomness in this allocation reflects both heterogeneity in the planner’s assignment of constraints and the
agent’s heterogeneous responses to this assignment. One extreme case arises if the planner assigns agents to treatment groups and agents perfectly comply, so that B = S and
sp (ω) = τ (ω, a) = a(ω) for all ω ∈ Ω. In this case, all variation of sp is due to heterogeneity in the constraints a(ω) across agents ω. At the other extreme, agents do not
respond at all to the incentives assigned by mechanisms in A, and τ (a, ω) = τ (a  , ω)
for all a, a  ∈ A and ω ∈ Ω. In general, there are policies that have a nontrivial (that
is, nondegenerate) constraint assignment a, where at least some agents respond to the
assigned constraints a in their treatment choice, τ (a, ω) = τ (a  , ω) for some a, a  ∈ A
and ω ∈ Ω.
We seek to evaluate a policy p in terms of some outcome Yp , for example, earnings.
For each p ∈ P, Yp is a random variable defined on the population Ω. We index
outcomes by a policy subscript in order to simplify the notation. To avoid notational
confusion, we will not use treatment subscripts in this section. The evaluation can focus
49 Formally, I is a sub-σ -algebra of I – the σ -algebra generated by I – and ω ∈ Ω → τ (ω, a) ∈ S
A
A
should be IA -measurable for all a ∈ A. The possibility that different agents have different information

sets is allowed for because a distinction between agents and states of nature is implicit. As suggested in the
introduction to this section, we can make it explicit by distinguishing a set J of agents and a set Ω̃ of states
of nature and writing Ω = J × Ω̃. For expositional convenience, let J be finite. We can model that agents
observe their identity j by assuming that the random variable JA on Ω that reveals their identity, that is
JA (j, ω̃) = j , is in their information set IA . If agents, in addition, observe some other random variable V
on Ω, then the information set IA generated by (JA , V ) can be interpreted as providing each agent j ∈ J with
perfect information about his identity j and with the agent-j -specific information about the state of nature ω̃
encoded in the random variable ω̃ → V (j, ω̃) on Ω̃.
50 See the discussion by Heckman and Vytlacil in Chapter 71, Sections 2 and 9, of their contribution to this
Handbook.
51 Formally, {s }
p p∈A×T is a stochastic process indexed by p.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5213

on objective outcomes Yp , on the subjective valuation R(Yp ) of Yp by the planner or
the agents, or on both types of outcomes. The evaluation can be performed relative
to a variety of information sets reflecting different actors (the agent, the planner and
the econometrician) and the arrival of information in different time periods. Thus, the
randomness of Yp may represent both (ex ante) heterogeneity among agents known to
the planner when constraints are assigned (that is, variables in IP ) and/or heterogeneity
known to the agents when they choose treatment (that is, information in IA ), as well
as (ex post) shocks that are not foreseen by the policy maker or by the agents. An
information-feasible (ex ante) policy evaluation by the planner would be based on some
criterion using the distribution of Yp conditional on IP . The econometrician can assist
the planner in computing this evaluation if the planner shares her ex ante information
and IP ⊆ IE . We discussed ex ante and ex post evaluations in Section 2 in the context
of a one shot model. It is also discussed in Chapter 70 of this Handbook. In this section,
we discuss information revelation and ex ante and ex post evaluations in a dynamic
setting.
Suppose that we have data on outcomes Yp0 under policy p0 with corresponding
treatment assignment sp0 . Consider an intervention that changes the policy from the
actual p0 to some counterfactual p  with associated treatments sp and outcomes Yp .
This could involve a change in the planner’s constraint assignment from a0 to a  for
given τ0 = τ  , a change in the agent choice rule from τ0 to τ  for given a0 = a  , or both.
The policy evaluation problem involves contrasting Yp and Yp0 or functions of these
outcomes. For example, if the outcome of interest is mean earnings, we might be interested in some weighted average of E[Yp − Yp0 | IP ], such as E[Yp − Yp0 ]. The
special case where S = {0, 1} and sp = a  = 0 generates the effect of abolishing the
program.52 Implementing such a policy requires that the planner be able to induce all
agents into the control group by assigning constraints a  = 0. In particular, as discussed
in Chapter 71, Section 10, this assumes that there are no substitute programs available
to agents that are outside the planner’s control.
For notational convenience, write S = sp0 for treatment assignment under the actual
policy p0 in place. Cross-sectional microdata typically provide a random sample from
the joint distribution of (Yp0 , S).53 Clearly, without further assumptions, such data do
not identify the effects of the policy shift from p0 to p  . This identification problem
becomes even more difficult if we do not seek to compare the counterfactual policy p 
with the actual policy p0 , but rather with another counterfactual policy p  that also has
never been observed. A leading example is the binary case in which 0 < Pr(S = 1) < 1,
but we seek to know the effects of sp = 0 (universal nonparticipation) and sp = 1
(universal treatment), where neither policy has ever been observed in place. As we have
52 Such a widespread policy would likely have general equilibrium effects. In this section, we will abstract

from these by invoking invariance assumptions (PI-1)–(PI-4) discussed in Chapter 70. Section 4 discusses
general equilibrium effects.
53 Notice that a random sample of outcomes under a policy may entail nonrandom selection of treatments as
individual agents select individual treatments given τ and the constraints they face assigned by a.

5214

J.H. Abbring and J.J. Heckman

stressed repeatedly in Chapters 70 and 71 of this Handbook, determining the average
treatment effect (ATE) is often a difficult task.
The standard microeconometric approach to the policy evaluation problem assumes
that the (subjective and objective) outcomes for any individual agent are the same across
all policy regimes for any particular treatment assigned to the individual [see, e.g.,
Heckman, LaLonde and Smith (1999)]. The invariance assumptions (PI-1)–(PI-4) that
justify this practice are presented in Chapter 70. They simplify the task of evaluating
policy p to determining (i) the assignment sp of treatments under policy p and (ii) treatment effects for individual outcomes. Even within this simplified framework, there are
still two difficult, and distinct, problems in identifying treatment effects on individual
outcomes:
(A) The Evaluation Problem: that we observe an agent in one treatment state and
seek to determine that agent’s outcomes in another state; and
(B) The Selection Problem: that the distributions of outcomes for the agents we observe in a given treatment state are not the marginal population distributions
that would be observed if agents were randomly assigned to the state.
The assignment mechanism sp of treatments under counterfactual policies p is straightforward in the case where the planner assigns agents to treatment groups and agents
fully comply, so that sp = a. More generally, an explicit model of agent treatment
choices is needed to derive sp for counterfactual policies p. An explicit model of agent
treatment choices can also be helpful in addressing the selection problem, and in identifying agent subjective valuations of outcomes. We now formalize the notation for the
treatment-effect approach that we will use in this section.
3.1.2. The treatment-effect approach
For each agent ω ∈ Ω, let y(s, X(ω), U (ω)) be the potential outcome when the agent
is assigned to treatment s ∈ S. Here, X and U are covariates that are not causally
affected by the treatment or the outcomes.54,55 In the language of Kalbfleisch and Prentice (1980) and Leamer (1985), we say that such covariates are “external” to the causal
model. X is observed by the econometrician (that is, in IE ) and U is not.
Recall that sp is the assignment of agents to treatments under policy p. For all policies p that we consider, the outcome Yp is linked to the potential outcomes by the
54 This is the “no feedback” condition (A-6) presented in Chapter 71. The condition requires that X and U

are the same fixing S = s for all s. See Haavelmo (1943), Pearl (2000), or the discussion in Chapter 70.
55 Note that this framework is rich enough to capture the case in which potential outcomes depend on

treatment-specific unobservables as in Sections 2 and 3.4, because these can be simply stacked in U and
subsequently selected by y. For example, in the case where S = {0, 1} we can write y(s, X, (U0 , U1 )) =
sy1 (X, U1 ) + (1 − s)y0 (X, U0 ) for some y0 and y1 . A specification without treatment-dependent unobservables is more tractable in the case of continuous treatments in Section 3.2 and, in particular, continuous
treatment times in Section 3.3.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5215

consistency condition Yp = y(sp , X, U ). This condition follows from the invariance
assumptions presented in Chapter 70. It embodies the assumption that an agent’s outcome only depends on the treatment assigned to the agent and not separately on the
mechanism used to assign treatments. This excludes (strategic) interactions between
agents and equilibrium effects of the policy.56 It ensures that we can specify individual outcomes y from participating in programs in S independently of the policy p and
treatment assignment sp . Economists say that y is autonomous, or structurally invariant with respect to the policy environment [see Frisch (1938), Hurwicz (1962), and our
discussion of structure and invariance in Chapter 70]. 57 With this notation in hand, we
now turn to the dynamic policy evaluation problem.
3.1.3. Dynamic policy evaluation
Interventions often have consequences that span over many periods. Policy interventions at different points in time can be expected to affect not only current outcomes,
but also outcomes at other points in time. The same policy implemented at different
time periods may have different consequences. Moreover, policy assignment rules often
have nontrivial dynamics. The assignment of programs at any point in time can be contingent on the available data on past program participation, intermediate outcomes and
covariates.
The dynamic policy evaluation problem can be formalized in a fashion similar to
the way we formalized the static problem in Chapter 70 and in Section 3.1.1. In this
subsection, we analyze a discrete-time finite-horizon model. We consider continuoustime models in Section 3.3. The possible treatment assignment times are 1, . . . , T̄ . We
do not restrict the set S of treatments. We allow the same treatment to be assigned on
multiple occasions. In general, the set of available treatments at each time t may depend
on time t and on the history of treatments, outcomes, and covariates. For expositional
convenience, we will only make this explicit in Sections 3.3 and 3.4, where we focus on
the timing of a single treatment.
We define a dynamic policy p = (a, τ ) ∈ A × T = P as a dynamic constraint
assignment rule a = {at }T̄t=1 with a dynamic treatment choice rule τ = {τt }T̄t=1 . At
each time t, the planner assigns constraints at (ω) to each agent ω ∈ Ω, using information in the time-t policy-p information set IP (t, p) ⊆ I. The planner’s information
set IP (t, p) could be based on covariates and random variables under the planner’s
control, as well as past choices and realized outcomes. We denote the sequence of planner’s information sets by IP (p) = {IP (t, p)}T̄t=1 . We assume that the planner does not

56 See Pearl (2000), Heckman (2005), or the discussion in Chapter 70.
57 See also Aldrich (1989) and Hendry and Morgan (1995). Rubin’s (1986) stable-unit-treatment-value as-

sumption is a version of the classical invariance assumptions of econometrics [see Abbring (2003), for
discussion of this point, and the discussion in Chapter 70].

5216

J.H. Abbring and J.J. Heckman

forget any information she once had, so that her information improves over time and
IP (t, p) ⊆ IP (t + 1, p) for all t.58
Each agent ω chooses treatment τt (ω, a) given their information about ω at time t under policy p and given the constraint assignment mechanism a ∈ A in place. We assume
that agents know the constraint assignment mechanism a in place. At time t, under policy p, agents infer their information about their type ω from random variables IA (t, p)
that may include preference components and determinants of constraints and future outcomes. IA (t, p) denotes the time-t policy-p information set generated by IA (t, p) and
IA (p) = {IA (t, p)}T̄t=1 . We assume that agents are increasingly informed as time goes
by, so that IA (t, p) ⊆ IA (t + 1, p).59 For expositional convenience, we also assume
that agents know more than the planner at each time t, so that IP (t, p) ⊆ IA (t, p).60
Because all determinants of past and current constraints are in the planner’s information set IP (t, p), this implies that agents observe (a1 (ω), . . . , at (ω)) at time t. Usually,
they do not observe all determinants of their future constraints (at+1 (ω), . . . , aT̄ (ω)).61
Thus, the treatment choices of the agents may be contingent on past and current constraints, their preferences, and on their predictions of future outcomes and constraints
given their information IA (t, p) and given the constraint assignment mechanism a in
place.
Extending the notation for the static case, we denote the assignment of agents to
treatment τt at time t implied by a policy p by the random variable sp (t) defined so
that sp (ω, t) = τt (ω, a). We use the shorthand spt for the vector (sp (1), . . . , sp (t)) of
treatments assigned up to and including time t under policy p, and write sp = spT̄ . The
assumptions made so far about the arrival of information imply that treatment assignment sp (t) can only depend on the information IA (t, p) available to agents at time t.62
Because past outcomes typically depend on the policy p, the planner’s information
IP (p) and the agents’ information IA (p) will generally depend on p as well. In the
treatment-effect framework that we develop in the next section, at each time t different
policies may have selected different elements in the set of potential outcomes in the past.
The different elements reveal different aspects of the unobservables underlying past and
future outcomes. We will make assumptions that limit the dependence of information
sets on policies in the context of the treatment-effects approach developed in the next
section.
Objective outcomes associated with policies p are expressed as a vector of timespecific outcomes Yp = (Yp (1), . . . , Yp (T̄ )). The components of this vector may
58 Formally, the information I (p) that accumulates for the planner under policy p is a filtration in I, and a
P

is a stochastic process that is adapted to IP (p).
59 Formally, the information I (p) that accumulates for the agents is a filtration in I.
A
60 If agents are strictly better informed, and I (t, p) ⊂ I (t, p), it is unlikely that the planner catches up
P
A
and learns the agent’s information with a delay (e.g., IA (t, p) ⊆ IP (t + 1, p)) unless agent’s choices and
outcomes reveal all their private information.
61 Formally, a , . . . , a are I (t, p)-measurable, but a
t
A
1
t+1 , . . . , aT̄ are not.
62 Formally, {s (t)}T̄ is a stochastic process that is adapted to I (p).
p
A
t=1

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5217

also be vectors. We denote the outcomes from time 1 to time t under policy p by
Ypt = (Yp (1), . . . , Yp (t)). We analyze both subjective and objective evaluations of policies in Section 3.4, where we consider more explicit economic models. Analogous to
our analysis of the static case, we cannot learn about the outcomes Yp that would arise
under a counterfactual policy p  from data on outcomes Yp0 and treatments sp0 = S
under a policy p0 = p  without imposing further structure on the problem.63 We follow
the approach exposited for the static case and assume policy invariance of individual
outcomes under a given treatment. These are the invariance assumptions (PI-1)–(PI-4)
presented in Chapter 70. They reduce the evaluation of a dynamic policy p to identifying (i) the dynamic assignment sp of treatments under policy p and (ii) the dynamic
treatment effects on individual outcomes. We focus our discussion on the fundamental
evaluation problem and the selection problem that haunt inference about treatment effects. In the remainder of the section, we review alternative approaches to identifying
dynamic treatment effects, and some approaches to modeling dynamic treatment choice.
We first analyze methods recently developed in statistics.
3.2. Dynamic treatment effects and sequential randomization
In a series of papers, Robins extends the static Neyman–Rubin model based on selection
on observables discussed in Chapter 71 to a dynamic setting [see, e.g., Robins (1997),
and the references therein]. He does not consider agent choice or subjective evaluations. Here, we review his extension, discuss its relationship to dynamic choice models
in econometrics, and assess its merits as a framework for economic policy analysis.
We follow the exposition of Gill and Robins (2001), but add some additional structure
to their basic framework to exposit the connection of their approach to the dynamic
approach pursued in econometrics.
3.2.1. Dynamic treatment effects
3.2.1.1. Dynamic treatment and dynamic outcomes To simplify the exposition, suppose that S is a finite discrete set.64 Recall that, at each time t and for given p,
treatment assignment sp (t) is a random variable that only depends on the agent’s information IA (t, p), which includes personal knowledge of preferences and determinants
of constraints and outcomes. To make this dependence explicit, suppose that external
covariates Z, observed by the econometrician (that is, variables in IE ), and unobserved
external covariates V1 that affect treatment assignment are revealed to the agents at
time 1. Then, at the start of each period t  2, past outcomes Yp (t − 1) corresponding
63 If outcomes under different policy regimes are informative about the same technology and preferences,

for example, then the analyst and the agent could learn about the ingredients that produce counterfactual
outcomes in all outcome states.
64 All of the results presented in this subsection extend to the case of continuous treatments. We will give
references to the appropriate literature in subsequent footnotes.

5218

J.H. Abbring and J.J. Heckman

to the outcomes realized under treatment assignment sp and external unobserved covariates Vt enter the agent’s information set.65 In this notation, IA (1, p) is the information
σ (Z, V1 ) conveyed to the agent by (Z, V1 ) and, for t  2, IA (t, p) = σ (Ypt−1 , Z, V t ),
with V t = (V1 , . . . , Vt ). In the notation of the previous subsection, IA (1, p) = (Z, V1 )
and, for t  2, IA (t, p) = (Ypt−1 , Z, V t ). Among the elements of IA (t, p) are the determinants of the constraints faced by the agent up to t, which may or may not be observed
by the econometrician.
We attach ex post potential outcomes Y (t, s) = yt (s, X, Ut ), t = 1, . . . , T̄ , to each
treatment sequence s = (s(1), . . . , s(T̄ )). Here, X is a vector of observed (by the econometrician) external covariates and Ut , t = 1, . . . , T̄ , are vectors of unobserved external
covariates. Some components of X and Ut may be in agent information sets. We denote
Y t (s) = (Y (1, s), . . . , Y (t, s)), Y (s) = Y T̄ (s), and U = (U1 , . . . , UT̄ ). As in the static
case, potential outcomes y are assumed to be invariant across policies p, which ensures
that Yp (t) = yt (sp , X, Ut ). In the remainder of this section, we keep the dependence of
outcomes on observed covariates X implicit and suppress all conditioning on X.
We assume no causal dependence of outcomes on future treatment:66
(NA) For all t  1, Y (t, s) = Y (t, s  ) for all s, s  such that s t = (s  )t ,
where s t = (s(1), . . . , s(t)) and (s  )t = (s  (1), . . . , s  (t)). Abbring and Van den Berg
(2003b) and Abbring (2003) define this as a “no-anticipation” condition. It requires
that outcomes at time t (and before) be the same across policies that allocate the same
treatment up to and including t, even if they allocate different treatments after t. In the
structural econometric models discussed in Sections 3.2.2 and 3.4 below, this condition
is trivially satisfied if all state variables relevant to outcomes at time t are included as
inputs in the outcome equations Y (t, s) = yt (s, Ut ), t = 1, . . . , T̄ .
Because Z and V1 are assumed to be externally determined, and therefore not affected by the policy p, the initial agent information set IA (1, p) = σ (Z, V1 ) does not
depend on p. Agent ω has the same initial data (Z(ω), V1 (ω)) about his type ω under
all policies p. Thus, IA (1, p) = IA (1, p  ) is a natural benchmark information set for
an ex ante comparison of outcomes at time 1 among different policies. For t  2, (NA)
implies that actual outcomes up to time t − 1 are equal between policies p and p ,
t−1 = s t−1 .
Ypt−1 = Ypt−1
 , if the treatment histories coincide up to time t − 1 so that sp
p
Together with the assumption that Z and V t are externally determined, it follows that
agents have the same time-t information set structure about ω under policies p and p ,

65 Note that any observed covariates that are dynamically revealed to the agents can be subsumed in the

outcomes.
66 For statistical inference from data on the distribution of (Y , S, Z), these equalities only need to hold on
p0
events {ω ∈ Ω: S t (ω) = s t }, t  1, respectively.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5219

t

t−1 = s t−1 .67,68 In
IA (t, p) = σ (Ypt−1 , Z, V t ) = σ (Ypt−1
 , Z, V ) = IA (t, p ), if sp
p
this context, IA (t, p) = IA (t, p  ) is a natural information set for an ex ante comparison of outcomes from time t onwards between any two policies p and p  such that
spt−1 = spt−1
 .
With this structure on the agent information sets in hand, it is instructive to review
the separate roles in determining treatment choice of information about ω and knowledge about the constraint assignment rule a. First, agent ω’s time-t treatment choice
sp (ω, t) = τt (ω, a) may depend on distributional properties of a, for example the share
of agents assigned to particular treatment sequences, and on the past and current constraints (a1 (ω), . . . , at (ω)) that were actually assigned to them. We have assumed both
to be known to the agent. Both may differ between policies, even if the agent information about ω is fixed across the policies. Second, agent ω’s time-t treatment choice may
depend on agent ω’s predictions of future constraints and outcomes. A forward-looking
agent ω will use observations of his covariates Z(ω) and V t (ω) and past outcomes
Ypt−1 (ω) to infer his type ω and subsequently predict future external determinants
(Ut (ω), . . . , UT̄ (ω)) of his outcomes and (Vt+1 (ω), . . . , VT̄ (ω)) of his constraints and
treatments. In turn, this information updating allows agent ω to predict his future potential outcomes (Y (t, s, ω), . . . , Y (T̄ , s, ω)) and, for a given policy regime p, his future
constraints (at+1 (ω), . . . , aT̄ (ω)), treatments (sp (t +1, ω), . . . , sp (T̄ , ω)), and realized
outcomes (Yp (t, ω), . . . , Yp (T̄ , ω)). Under different policies, the agent may gather different information on his type ω and therefore come up with different predictions of
the external determinants of his future potential outcomes and constraints. In addition,
even if the agent has the same time-t predictions of the external determinants of future
constraints and potential outcomes, he may translate these into different predictions of
future constraints and outcomes under different policies.
Assumption (NA) requires that current potential outcomes are not affected by future treatment. Justifying this assumption requires specification of agent information
about future treatment and agent behavior in response to that information. Such an interpretation requires that we formalize how information accumulates for agents across
 , . . . , s  ).
treatment sequences s and s  such that s t = (s  )t and (st+1 , . . . , sT̄ ) = (st+1
T̄


To this end, consider policies p and p such that sp = s and sp = s . These policies
produce the same treatment assignment up to time t, but are different in the future. We
have previously shown that, even though the time-t agent information about ω is the

t−1
t−1
67 If s t−1 (ω) = s t−1 (ω) only holds for ω in some subset Ω
p
t−1 ⊂ Ω of agents, then Yp (ω) = Yp (ω)
p

only for ω ∈ Ωt−1 , and information coincides between p and p only for agents in Ωt−1 . Formally, let
Ωt−1 be the set {ω ∈ Ω: spt−1 (ω) = spt−1
 (ω)} of agents that share the same treatment up to and including
time t − 1. Then, Ωt−1 is in the agent’s information set under both policies, Ωt−1 ∈ IA (t, p) ∩ IA (t, p  ).
Moreover, the partitioning of Ωt−1 implied by IA (t, p) and IA (t, p  ) is the same. To see this, note that the
collections of all sets in, respectively, IA (t, p) and IA (t, p  ) that are weakly included in Ωt−1 are identical
σ -algebras on Ωt−1 .
68 Notice that the realizations of the random variables Y t−1 , Z, V t may differ among agents.
p

5220

J.H. Abbring and J.J. Heckman

same under both policies, IA (t, p) = IA (t, p  ), agents may have different predictions
of future constraints, treatments and outcomes because the policies may differ in the future and agents know this. The policy-invariance conditions (PI-1)–(PI-4) of Chapter 70
ensure that time-t potential outcomes are nevertheless the same under each policy. This
requires that potential outcomes be determined externally, and are not affected by agent
actions in response to different predictions of future constraints, treatments and outcomes.
In general, different policies in P will produce different predictions of future constraints, treatment and outcomes. In the dynamic treatment-effects framework, this may
affect outcomes indirectly through agent treatment choices. If potential outcomes are
directly affected by agent’s forward-looking decisions, then the invariance conditions
(PI-1)–(PI-4) of Chapter 70 underlying the treatment-effects framework will be violated. Section 3.2.3 illustrates this issue, and the no-anticipation condition, with some
examples.
3.2.1.2. Identification of treatment effects Suppose that the econometrician has data
that allows her to estimate the joint distribution of (Yp0 , S, Z) of outcomes, treatments
and covariates under some policy p0 , where again S = sp0 . These data are not enough
to identify dynamic treatment effects.
To secure identification, Gill and Robins (2001) invoke a dynamic version of the
matching assumption (conditional independence) which relies on sequential randomization:69
(M-2) For all treatment sequences s and all t,

 

S(t) ⊥
⊥ Y (t, s), . . . , Y (T̄ , s) | Ypt−1
, S t−1 = s t−1 , Z ,
0
where the conditioning set (Yp00 , S 0 = s 0 , Z) for t = 1 should be simply stated as Z.
Equivalently,


S(t) ⊥
⊥ (Ut , . . . , UT̄ ) | Ypt−1
, S t−1 , Z
0
for all t without further restricting the data. Sequential randomization allows the Yp0 (t)
to be “dynamic confounders”—variables that are affected by past treatment and that
affect future treatment assignment.
The sequence of conditioning information sets appearing in the sequential randomization assumption, IE (1) = σ (Z) and, for t  2, IE (t) = σ (Ypt−1
, S t−1 , Z), is a filtration
0
IE of the econometrician’s information set σ (Yp0 , S, Z). Note that IE (t) ⊆ IA (t, p0 )
for each t. If treatment assignment is based on strictly more information than IE , so

69 Formally, we need to restrict attention to sequences s in the support of S. Throughout this section, we will

assume this and related support conditions hold.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5221

that agents know strictly more than the econometrician and act on their superior information, (M-2) is likely to fail if that extra information also affects outcomes. This point
is made in a static setting in Chapter 71.
Together with the no-anticipation condition (NA), which is a condition on outcomes
and distinct from (M-2), the dynamic potential-outcome model set up so far is a natural dynamic extension of the Neyman–Rubin model for a static (stratified) randomized
experiment.
Under assumption (M-2) that the actual treatment assignment S is sequentially randomized, we can sequentially identify the causal effects of treatment from the distribution of the data (Yp0 , S, Z) and construct the distribution of the potential outcomes Y (s)
for any treatment sequence s in the support of S.
Consider the case in which all variables are discrete. No-anticipation condition (NA)
ensures that potential outcomes for a treatment sequence s equal actual (under policy p0 ) outcomes up to time t − 1 for agents with treatment history s t−1 up to time
t − 1. Formally, Y t−1 (s) = Ypt−1
on the set {S t−1 = s t−1 }. Using this, sequential ran0
domization assumption (M-2) can be rephrased in terms of potential outcomes: for all s
and t,


 
S(t) ⊥
⊥ Y (t, s), . . . , Y (T̄ , s) | Y t−1 (s), S t−1 = s t−1 , Z .
In turn, this implies that, for all s and t,


Pr Y (t, s) = y(t) | Y t−1 (s) = y t−1 , S t = s t , Z


= Pr Y (t, s) = y(t) | Y t−1 (s) = y t−1 , Z ,

(3.1)

where y t−1 = (y(1), . . . , y(t − 1)) and y = y T̄ . From Bayes’ rule and (3.1), it follows
that


Pr Y (s) = y | Z


= Pr Y (1, s) = y(1) | Z

T̄



Pr Y (t, s) = y(t) | Y t−1 (s) = y t−1 , Z

t=2



= Pr Y (1, s) = y(1) | S(1) = s(1), Z
T̄

×



Pr Y (t, s) = y(t) | Y t−1 (s) = y t−1 , S t = s t , Z .

t=2

on {S t = s t },
Invoking (NA), in particular Y (t, s) = Yp0 (t) and Y t−1 (s) = Ypt−1
0
produces


Pr Y (s) = y | Z


= Pr Yp0 (1) = y(1) | S(1) = s(1), Z
T̄

×
t=2



Pr Yp0 (t) = y(t) | Ypt−1
= y t−1 , S t = s t , Z .
0

(3.2)

5222

J.H. Abbring and J.J. Heckman

This is a version of Robins’ (1997) “g-computation formula”.70,71 We can sequentially
identify each component on the left-hand side of the first expression, and hence identify the counterfactual distributions. This establishes identification of the distribution
of Y (s) by expressing it in terms of objects that can be identified from data. Identification is exact (or “tight”) in the sense that the identifying assumptions, no anticipation
and sequential randomization, do not restrict the factual data and are therefore not
testable [Gill and Robins (2001, Section 6)].72
E XAMPLE 4. Consider a two-period (T̄ = 2) version of the model in which agents
take either “treatment” (1) or “control” (0) in each period. Then, S(1) and S(2) have
values in S = {0, 1}. The potential outcomes in period t are Y (t, (0, 0)), Y (t, (0, 1)),
Y (t, (1, 0)) and Y (t, (1, 1)). For example, Y (2, (0, 0)) is the outcome in period 2 in the
case that the agent is assigned to the control group in each of the two periods. Using
Bayes’ rule, it follows that


Pr Y (s) = y | Z

 

= Pr Y (1, s) = y(1) | Z Pr Y (2, s) = y(2) | Y (1, s) = y(1), Z .
(3.3)
The g-computation approach to constructing Pr(Y (s) = y | Z) from data replaces the
two probabilities in the right-hand side with probabilities of the observed (by the econometrician) variables (Yp0 , S, Z). First, note that Pr(Y (1, s) = y(1) | Z) = Pr(Y (1, s) =
y(1) | S(1) = s(1), Z) by (M-2). Moreover, (NA) ensures that potential outcomes in
period 1 do not depend on the treatment status in period 2, so that




Pr Y (1, s) = y(1) | Z = Pr Yp0 = y(1) | S(1) = s(1), Z .
70 Gill and Robins (2001) present versions of (NA) and (M-2) for the case with more general distributions

of treatments, and prove a version of the g-computation formula for the general !case. For a random vector
X and a function f that is integrable with respect to the distribution of X, let x∈A f (x) Pr(X ∈ dx) =
E[f (X)1(X ∈ A)]. Then,





Pr Yp0 (T̄ ) ∈ dy(T̄ ) | YpT̄0−1 = y T̄ −1 , S T̄ = s T̄ , Z
Pr Y (s) ∈ A | Z =
y∈A

..
.


× Pr Yp0 (2) ∈ dy(2) | Yp0 (1) = y(1), S 2 = s 2 , Z


× Pr Yp0 (1) ∈ dy(1) | S(1) = s(1), Z ,
where A is a set of Y (s). The right-hand side of this expression is almost surely unique under regularity
conditions presented by Gill and Robins (2001).
71 An interesting special case arises if the outcomes are survival indicators, that is if Y (t) = 1 if the agent
p0
survives up to and including time t and Yp0 (t) = 0 otherwise, t  1. Then, no anticipation (NA) requires
that treatment after death does not affect survival, and the g-computation formula simplifies considerably
[Abbring (2003)].
72 Gill and Robins’ (2001) analysis only involves causal inference on a final outcome (i.e., our Y (s, T̄ )) and
does not invoke the no-anticipation condition. However, their proof directly applies to the case studied in this
chapter.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5223

Similarly, subsequently invoking (NA) and (M-2), then (M-2), and then (NA), gives


Pr Y (2, s) = y(2) | Y (1, s) = y(1), Z
 


by (NA) and (M-2)
= Pr Y (2, s) = y(2) | Yp0 (1), S(1) = s(1), Z
 


by (M-2)
= Pr Y (2, s) = y(2) | Yp0 (1), S = s, Z
 


= Pr Yp0 (2) = y(2) | Yp0 (1), S = s, Z . by (NA)
Substituting these equations into the right-hand side of (3.3) gives the g-computation
formula,




Pr Y (s) = y | Z = Pr Yp0 (1) = y(1) | S(1) = s(1), Z


× Pr Yp0 (2) = y(2) | Yp0 (1) = y(1), S = s, Z .
Note that the right-hand side does not generally reduce to Pr(Yp0 = y | S = s, Z). This
would require the stronger, static matching condition S ⊥
⊥ Y (s) | Z, which we have not
assumed here.
Matching on pre-treatment covariates is a special case of the g-computation approach.
Suppose that the entire treatment path is assigned independently of potential outcomes
given pre-treatment covariates Z or, more precisely, S ⊥
⊥ Y (s) | Z for all s. This implies
sequential randomization (M-2), and directly gives identification of the distributions
of Y (s) | Z and Y (s). The matching assumption imposes no restriction on the data since
Y (s) is only observed if S = s. The no-anticipation condition (NA) is not required for
identification in this special case because no conditioning on S t is required. Matching
on pre-treatment covariates is equivalent to matching in a static model. The distribution
of Y (s) | Z is identified without (NA), and assuming it to be true would impose testable
restrictions on the data. In particular, it would imply that treatment assignment cannot
be dependent on past outcomes given Z. The static matching assumption is not likely
to hold in applications where treatment is dynamically assigned based on information
on intermediate outcomes. This motivates an analysis based on the more subtle sequential randomization assumption. An alternative approach, developed in Section 3.4, is to
explicitly model and identify the evolution of the unobservables.
Gill and Robins claim that their sequential randomization and no-anticipation assumptions are “neutral”, “for free”, or “harmless”. As we will argue later, from an
economic perspective, some of the model assumptions, notably the no-anticipation assumption, can be interpreted as substantial behavioral/informational assumptions. For
example, Heckman and Vytlacil (2005, and Chapter 70 of this Handbook) and Heckman
and Navarro (2004) show how matching imposes the condition that marginal and average returns are equal. Because of these strong assumptions, econometricians sometimes
phrase their “neutrality” result more negatively as a nonidentification result [Abbring
and Van den Berg (2003b)], since it is possible that (M-2) and/or (NA) may not hold.

5224

J.H. Abbring and J.J. Heckman

3.2.2. Policy evaluation and dynamic discrete-choice analysis
3.2.2.1. The effects of policies Consider a counterfactual policy p  such that the corresponding allocation of treatments sp satisfies sequential randomization, as in (M-2):
(M-3) For all treatment sequences s and all t,

 

t−1
⊥ Y (t, s), . . . , Y (T̄ , s) | Ypt−1
= s t−1 , Z .
sp (t) ⊥
 , sp 
The treatment assignment rule sp is equivalent to what Gill and Robins (2001) call a
“randomized plan”. The outcome distribution under such a rule cannot be constructed
by integrating the distributions of {Y (s)} with respect to the distribution of sp , because
there may be feedback from intermediate outcomes into treatment assignment. Instead,
under the assumptions of the previous subsection and a support condition, we can use
a version of the g-computation formula for randomized plans given by Gill and Robins
to compute the distribution of outcomes under the policy p  :73
 

Pr(Yp = y | Z) =
Pr Yp0 (1) = y(1) | S(1) = s(1), Z
s∈S



× Pr sp (1) = s(1) | Z
T̄

 

Pr Yp0 (t) = y(t) | Ypt−1
= y t−1 , S t = s t , Z
0

×
t=2



t−1
= y t−1 , spt−1
, Z . (3.4)
× Pr sp (t) = s(t) | Ypt−1

 (1) = s

73 The corresponding formula for the case with general treatment distributions is


Pr(Yp ∈ A | Z) =


y∈A s∈S



Pr Yp0 (T̄ ) ∈ dy(T̄ ) | YpT̄0−1 = y T̄ −1 , S T̄ = s T̄ , Z



× Pr sp (T̄ ) ∈ ds(T̄ ) | YpT̄−1 = y T̄ −1 , spT̄−1 = s T̄ −1 , Z
..
.


× Pr Yp0 (2) ∈ dy(2) | Yp0 (1) = y(1), S(1) = s(1), Z


× Pr sp (2) ∈ ds(2) | Yp (1) = y(1), sp (1) = s(1), Z

 

× Pr Yp0 (1) ∈ dy(1) | S(1) = s(1), Z Pr sp (1) ∈ ds(1) | Z .
= y t−1 , spt−1
=
The support condition on sp requires that, for each t, the distribution of sp (t) | (Ypt−1


t−1
t−1
t−1
t−1
s
, Z = z) is absolutely continuous with respect to the distribution of S(t) | (Yp0 = y
,S
=
s t−1 , Z = z) for almost all (y t−1 , s t−1 , z) from the distribution of (Ypt−1
, S t−1 , Z).
0

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5225

In the special case of static matching on Z, so that sp ⊥
⊥ U | Z, this simplifies to
integrating the distribution of Yp0 | (S = s, Z) over the distribution of sp | Z:74

Pr(Yp = y | Z) =
Pr(Yp0 = y | S = s, Z) Pr(sp = s | Z).
s∈S

3.2.2.2. Policy choice and optimal policies We now consider the problem of choosing
a policy p that is optimal according to some criterion. This problem is both of normative
interest and of descriptive interest if actual policies are chosen to be optimal. We could,
for example, study the optimal assignment a  of constraints and incentives to agents.
Alternatively, we could assume that agents pick τ to maximize their utilities, and use
the methods discussed in this section to model τ .
Under the policy invariance assumptions that underlie the treatment-effects approach,
p only affects outcomes through its implied treatment allocation sp . Thus, the problem
of choosing an optimal policy boils down to choosing an optimal treatment allocation sp
under informational and other constraints specific to the problem at hand. For example,
suppose that the planner and the agents have the same information, IP (p) = IA (p), the
planner assigns eligibility to a program by a, and agents fully comply, so that B = S
and sp = a. Then, sp can be any rule from A and is adapted to IP (p) = IA (p).
For expositional convenience, we consider the optimal choice of a treatment assignment sp adapted to the agent’s information IA (p) constructed earlier. We will use the
word “agents” to refer to the decision maker in this problem, even though it can also
apply to the planner’s decision problem. An econometric approach to this problem is to
estimate explicit dynamic choice models with explicit choice-outcome relationships.
One emphasis in the literature is on Markovian discrete-choice models that satisfy
Rust’s (1987) conditional-independence assumption [see Rust (1994)]. Other assumptions are made in the literature and we exposit them in Section 3.4.
Here, we explore the use of Rust’s (1987) model as a model of treatment choice
in a dynamic treatment-effects setting. In particular, we make explicit the additional
structure that Rust’s model, and in particular his conditional-independence assumption,
imposes on Robins’ dynamic potential-outcomes model. We follow Rust (1987) and
focus on a finite treatment (control) space S. In the notation of our model, payoffs are
determined by the outcomes Yp , treatment choices sp , the “cost shocks” V , and the
covariates Z. Rust (1987) assumes that {Yp (t − 1), Vt , Z} is a controlled first-order
Markov process, with initial condition Yp (0) ≡ 0 and control sp .75 As before, Vt and Z
74 In the general case this condition becomes


Pr(Yp ∈ A | Z) =

s∈S

Pr(Yp0 ∈ A | S = s, Z) Pr(sp ∈ ds | Z).

75 Rust (1987) assumes an infinite-horizon, stationary environment. Here, we present a finite-horizon version

to facilitate a comparison with the dynamic potential-outcomes model and to link up with the analysis in
Section 3.4.

5226

J.H. Abbring and J.J. Heckman

are not causally affected by choices, but Yp (t) may causally depend on current and past
choices. The agents choose a treatment assignment rule sp that maximizes
 T̄


 




E
(3.5)
Υt Yp (t − 1), Vt , sp (t), Z + ΥT̄ +1 Yp (T̄ ), Z  IA (1) ,
t=1

for some (net and discounted) utility functions Υt and IA (1) = IA (1, p), which is
independent of p. ΥT̄ +1 {Yp (T̄ ), Z} is the terminal value. Under standard regularity
conditions on the utility functions, we can solve backward for the optimal policy sp .
Because of Rust’s Markov assumption, sp has a Markovian structure,
 


sp (t) ⊥
⊥ Ypt−2 , V t−1 | Yp (t − 1), Vt , Z ,
for t = 2, . . . , T̄ , and {Yp (t − 1), Vt , Z} is a first-order Markov process. Note that Z
enters the model as an observed (by the econometrician) factor that shifts net utility.
A key assumption embodied in the specification of (3.5) is time-separability of utility.
Rust (1987), in addition, imposes separability between observed and unobserved state
variables. This assumption plays no essential role in expositing the core ideas in Rust,
and we will not make it here.
Rust’s (1987) conditional-independence assumption imposes two key restrictions on
the decision problem. It is instructive to consider these restrictions in isolation from
Rust’s Markov restriction. We make the model’s causal structure explicit using the
potential-outcomes notation. Note that the model has a recursive causal structure—
the payoff-relevant state is controlled by current and past choices only—and satisfies
no-anticipation condition (NA). Setting Y (0, s) ≡ 0 for specificity, and ignoring the
Markov restriction, Rust’s conditional-independence assumption requires, in addition
to the assumption that there are no direct causal effects of choices on V , that


Y (s, t) ⊥
⊥ V t | Y t−1 (s), Z ,
(3.6)
 t

t
⊥ V | Y (s), Z
Vt+1 ⊥
(3.7)
for all s and t. As noted by Rust (1987, p. 1011) condition (3.6) ensures that the observed (by the econometrician) controlled state evolves independently of the unobserved
payoff-relevant variables. It is equivalent to [Florens and Mouchart (1982)]76
(M-4) [Y (s, t), . . . , Y (s, T̄ )] ⊥
⊥ V t | [Y t−1 (s), Z] for all t and s.
In turn, (M-4) implies (M-2) and is equivalent to the assumption that (M-3) holds for
all sp .77
76 Note that (3.6) is a Granger (1969) noncausality condition stating that, for all s and conditional on Z, V

does not cause Y (s).
77 If V has redundant components, that is components that do not nontrivially enter any assignment rule s ,
p

(M-4) imposes more structure, but structure that is irrelevant to the decision problem and its empirical analysis.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5227

Condition (3.7) excludes serial dependence of the unobserved payoff-relevant variables conditional on past outcomes. In contrast, Robins’ g-computation framework
allows for such serial dependence, provided that sequential randomization holds if serial dependence is present. For example, if V ⊥
⊥ U | Z, then (M-2) and its variants hold
without further assumptions on the time series structure of Vt .
The first-order Markov assumption imposes additional restrictions on potential outcomes. These restrictions are twofold. First, potential outcomes follow a first-order
Markov process. Second, s(t) only directly affects the Markov transition from Y (t, s) to
Y (t+1, s). This strengthens the no-anticipation assumption presented in Section 3.2.1.1.
The Markov assumption also requires that Vt+1 only depends on Y (s, t), and not on
Y t−1 (s), given Y (s, t).
In applications, we may assume that actual treatment assignment S solves the
Markovian decision problem. Together with specifications of Υt , this further restricts
the dynamic choice-outcome model. Alternatively, one could make other assumptions
on S and use (3.5) to define and find an optimal, and typically counterfactual, assignment rule sp .
Our analysis shows that the substantial econometric literature on the structural empirical analysis of Markovian decision problems under conditional independence can be
applied to policy evaluation under sequential randomization. Conversely, methods developed for potential-outcomes models with sequential randomization can be applied to
learn about aspects of dynamic discrete-choice models. Murphy (2003) develops methods to estimate an optimal treatment assignment rule using Robins’ dynamic potentialoutcomes model with sequential randomization (M-3).
3.2.3. The information structure of policies
One concern about methods for policy evaluation based on the potential-outcomes
model is that potential outcomes are sometimes reduced form representations of dynamic models of agent’s choices. A policy maker choosing optimal policies typically
faces a population of agents who act on the available information, and their actions in
turn affect potential outcomes. For example, in terms of the model of Section 3.2.2,
a policy may change financial incentives—the b ∈ B assigned through a could enter the
net utilities Υt —and leave it to the agents to control outcomes by choosing treatment.
In econometric policy evaluation, it is therefore important to carefully model the information IA that accumulates to the agents in different program states and under different
policies, separately from the policy maker’s information IP .
This can be contrasted with common practice in biostatistics. Statistical analyses of
the effects of drugs on health are usually concerned with the physician’s (planner’s)
information and decision problem. Gill and Robins’ (2001) sequential randomization
assumption, for example, is often justified by the assumption that physicians base their
treatment decisions on observable (by the analyst) information only. This literature,
however, often ignores the possibility that many variables known to the physician may

5228

J.H. Abbring and J.J. Heckman

not be known to the observing statistician and that the agents being given drugs alter the
protocols.
Potential outcomes will often depend on the agent’s information. Failure to correctly
model the information will often lead to violation of (NA) and failure of invariance. Potential outcomes may therefore not be valid inputs in a policy evaluation study. A naive
specification of potential outcomes would only index treatments by actual participation
in, e.g., job search assistance or training programs. Such a naive specification is incomplete in the context of economies inhabited by forward-looking agents who make
choices that affect outcomes. In specifying potential outcomes, we should not only consider the effects of actual program participation, but also the effects of the information
available to agents about the program and policy. We now illustrate this point.
E XAMPLE 5. Black et al. (2003) analyze the effect of compulsory training and employment services provided to unemployment insurance (UI) claimants in Kentucky on the
exit rate from UI and earnings. In the program they study, letters are sent out to notify
agents some time ahead whether they are selected to participate in the program. This
information is recorded in a database and available to them. They can analyze the letter
as part of a program that consists of information provision and subsequent participation
in training. The main empirical finding of their paper is that the threat of future mandatory training conveyed by the letters is more effective in increasing the UI exit rate than
training itself.
The data used by Black et al. (2003) are atypical of many economic data sets, because
the data collectors carefully record the information provided to agents. This allows
Black et al. to analyze the effects of the provision of information along with the effects
of actual program participation. In many econometric applications, the information on
the program under study is less rich. Data sets may provide information on actual participation in training programs and some background information on how the program is
administered. Typically, however, the data do not record all of the letters sent to agents
and do not record every phone conversation between administrators and agents. Then,
the econometrician needs to make assumptions on how this information accumulates
for agents. In many applications, knowledge of specific institutional mechanisms of assignment can be used to justify specific informational assumptions.
E XAMPLE 6. Abbring, Van den Berg and Van Ours (2005) analyze the effect of punitive
benefits reductions, or sanctions, on Dutch UI on re-employment rates. In the Netherlands, UI claimants have to comply with certain rules concerning search behavior and
registration. If a claimant violates these rules, a sanction may be applied. A sanction
is a punitive reduction in benefits for some period of time and may be accompanied
by increased levels of monitoring by the UI agency.78 Abbring, Van den Berg and Van
78 See Grubb (2000) for a review of sanction systems in the OECD.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5229

Ours (2005) use administrative data and know the re-employment duration, the duration at which a sanction is imposed if a sanction is imposed, and some background
characteristics for each UI case.
Without prior knowledge of the Dutch UI system, an analyst might make a variety of
informational assumptions. One extreme is that UI claimants know at the start of their
UI spells that their benefits will be reduced at some specific duration if they are still
claiming UI at that duration. This results in a UI system with entitlement periods that
are tailored to individual claimants and that are set and revealed at the start of the UI
spells. In this case, claimants will change their labor-market behavior from the start of
their UI spell in response to the future benefits reduction [e.g., Mortensen (1977)]. At
another extreme, claimants receive no prior signals of impending sanctions and there
are no anticipatory effects of actual benefits reductions. However, agents may still be
aware of the properties of the sanctions process and to some extent this will affect their
behavior. Abbring, Van den Berg and Van Ours (2005) analyze a search model with
these features. Abbring and Van den Berg (2003b) provide a structural example where
the data cannot distinguish between these two informational assumptions. We discuss
this example further in Section 3.3.1. Abbring, Van den Berg and Van Ours (2005)
use institutional background information to argue in favor of the second informational
assumption as the one that characterizes their data.
If data on information provision are not available and simplifying assumptions on
the program’s information structure cannot be justified, the analyst needs to model the
information that accumulates to agents as an unobserved determinant of outcomes. This
is the approach followed, and further discussed, in Section 3.4.
The information determining outcomes typically includes aspects of the policy. In Example 5, the letter announcing future training will be interpreted differently in different
policy environments. If agents are forward looking, the letter will be more informative
under a policy that specifies a strong relation between the letter and mandatory training
in the population than under a policy that allocates letters and training independently.
In Example 6, the policy is a monitoring regime. Potential outcomes are UI durations
under different sanction times. A change in monitoring policy changes the value of unemployment. In a job-search model with forward looking agents, agents will respond
by changing their search effort and reservation wage, and UI duration outcomes will
change. In either example, potential outcomes are not invariant to variation in the policy. In the terminology of Hurwicz (1962), the policy is not “structural” with regard
to potential outcomes and violates invariance assumptions (PI-1)–(PI-4) presented in
Chapter 70. One must control for the effects of agents’ information.
3.2.4. Selection on unobservables
In econometric program evaluations, (sequentially) randomized assignment is unlikely
to hold. We illustrate this in the models developed in Section 3.4. Observational data are
characterized by a lot of heterogeneity among agents, as documented by the empirical

5230

J.H. Abbring and J.J. Heckman

examples in Section 2 and in Heckman, LaLonde and Smith (1999). This heterogeneity
is unlikely to be fully captured by the observed variables in most data sets. In a dynamic
context, such unmeasured heterogeneity leads to violations of the assumptions of Gill
and Robins (2001) and Rust (1987) that choices represent a sequential randomization.
This is true even if the unmeasured variables only affect the availability of slots in
programs but not outcomes directly. If agents are rational, forward-looking and observe
at least some of the unmeasured variables that the econometrician does not, they will
typically respond to these variables through their choice of treatment and through their
investment behavior. In this case, the sequential randomization condition fails.
For the same reason, identification based on instrumental variables is relatively hard
to justify in dynamic models [Hansen and Sargent (1980), Rosenzweig and Wolpin
(2000), Abbring and Van den Berg (2005)]. If the candidate instruments only vary
across persons but not over time for the same person, then they are not likely to be valid
instruments because they affect expectations and future choices and may affect current
potential outcomes. Instead of using instrumental variables that vary only across persons, we require instruments based on unanticipated person-specific shocks that affect
treatment choices but not outcomes at each point in time. In the context of continuously
assigned treatments, the implied data requirements seem onerous. To achieve identification, Abbring and Van den Berg (2003b) focus on regressor variation rather than
exclusion restrictions in a sufficiently smooth model of continuous-time treatment effects. We discuss their analysis in Section 3.3. Heckman and Navarro (2007) show that
curvature conditions, not exclusion restrictions, that result in the same variables having different effects on choices and outcomes in different periods, are motivated by
economic theory and can be exploited to identify dynamic treatment effects in discrete
time without literally excluding any variables. We discuss their analysis in Section 3.4.
We now consider a formulation of the analysis in continuous time.
3.3. The event-history approach to policy analysis
The discrete-time models just discussed in Section 3.2 have an obvious limitation. Time
is continuous and many events are best described by a continuous-time model. There
is a rich field of continuous-time event-history analysis that has been adapted to conduct policy evaluation analysis.79 For example, the effects of training and counseling
on unemployment durations and job stability have been analyzed by applying eventhistory methods to data on individual labor-market and training histories [Ridder (1986),
Card and Sullivan (1988), Gritz (1993), Ham and LaLonde (1996), Eberwein, Ham and
LaLonde (1997), Bonnal, Fougère and Sérandon (1997)]. Similarly, the moral hazard
effects of unemployment insurance have been studied by analyzing the effects of timevarying benefits on labor-market transitions [e.g., Meyer (1990), Abbring, Van den Berg

79 Abbring and Van den Berg (2004) discuss the relation between the event-history approach to program

evaluation and more standard latent-variable and panel-data methods, with a focus on identification issues.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5231

and Van Ours (2005), Van den Berg, Van der Klaauw and Van Ours (2004)]. In fields
like epidemiology, the use of event-history models to analyze treatment effects is widespread [see, e.g., Andersen et al. (1993), Keiding (1999)].
The event-history approach to program evaluation is firmly rooted in the econometric literature on state dependence (lagged dependent variables) and heterogeneity
[Heckman and Borjas (1980), and Heckman (1981a)]. Event-history models along the
lines of Heckman and Singer (1984, 1986) are used to jointly model transitions into programs and transitions into outcome states. Causal effects of programs are modelled as
the dependence of individual transition rates on the individual history of program participation. Dynamic selection effects are modelled by allowing for dependent unobserved
heterogeneity in both the program and outcome transition rates.
Without restrictions on the class of models considered, true state dependence and
dynamic selection effects cannot be distinguished.80 Any history dependence of current transition rates can be explained both as true state dependence and as the result
of unobserved heterogeneity that simultaneously affects the history and current transitions. This is a dynamic manifestation of the problem of drawing causal inference from
observational data. In applied work, researchers avoid this problem by imposing additional structure. A typical, simple, example is a mixed semi-Markov model in which the
causal effects are restricted to program participation in the previous spell [e.g., Bonnal,
Fougère and Sérandon (1997), see Section 3.3.2]. There is a substantial literature on
the identifiability of state-dependence effects and heterogeneity in duration and eventhistory models that exploit such additional structure [see Heckman and Taber (1994),
and Van den Berg (2001), for reviews]. Here, we provide discussion of some canonical
cases.
3.3.1. Treatment effects in duration models
3.3.1.1. Dynamically assigned binary treatments and duration outcomes We first consider the simplest case of mutual dependence of events in continuous time, involving
only two binary events. This case is sufficiently rich to capture the effect of a dynamically assigned binary treatment on a duration outcome. Binary events in continuous
time can be fully characterized by the time at which they occur and a structural model
for their joint determination is a simultaneous-equations model for durations. We develop such a model along the lines of Abbring and Van den Berg (2003b). This model
is an extension, with general marginal distributions and general causal and spurious
dependence of the durations, of Freund’s (1961) bivariate exponential model.
Consider two continuously-distributed random durations Y and S. We refer to one of
the durations, S, as the time to treatment and to the other duration, Y , as the outcome
duration. Such an asymmetry arises naturally in many applications. For example, in
Abbring, Van den Berg and Van Ours’s (2005) study of unemployment insurance, the

80 See Heckman and Singer (1986).

5232

J.H. Abbring and J.J. Heckman

treatment is a punitive benefits reduction (sanction) and the outcome re-employment.
The re-employment process continues after imposition of a sanction, but the sanctions
process is terminated by re-employment. The current exposition, however, is symmetric
and unifies both cases. It applies to both the asymmetric setup of the sanctions example
and to applications in which both events may causally affect the other event.
Let Y (s) be the potential outcome duration that would prevail if the treatment time
is externally set to s. Similarly, let S(y) be the potential treatment time resulting from
setting the outcome duration to y. We assume that ex ante heterogeneity across agents
is fully captured by observed covariates X and unobserved covariates V , assumed to
be external and temporally invariant. Treatment causally affects the outcome duration
through its hazard rate. We denote the hazard rate of Y (s) at time t for an agent with
characteristics (X, V ) by θY (t | s, X, V ). Similarly, outcomes affect the treatment times
through its hazard θS (t | y, X, V ). Causal effects on hazard rates are produced by recursive economic models driven by point processes, such as search models. We provide
an example below, and further discussion in Section 3.3.3.
Without loss of generality, we partition V into (VS , VY ) and assume that θY (t |
s, X, V ) = θY (t | s, X, VY ) and θS (t | y, X, V ) = θS (t | y, X, VS ). Intuitively, VS and
VY are the unobservables affecting, respectively, treatment and outcome, and the joint
distribution of (VS , VY ) is unrestricted. In particular, VS and VY may have elements in
common.
! t The corresponding integrated hazard rates are! tdefined by ΘY (t | s, X, VY ) =
0 θY (u | s, X, VY ) du and ΘS (t | y, X, VS ) = 0 θS (u | y, X, VS ) du. For expositional convenience, we assume that these integrated hazards are strictly increasing in t.
We also assume that they diverge to ∞ as t → ∞, so that the duration distributions
are non-defective.81 Then, ΘY (Y (s) | s, X, VY ) and ΘS (S(y) | y, X, VS ) are unit exponential for all y, s ∈ R+ .82 This implies the following model of potential outcomes and
treatments,83
Y (s) = y(s, X, VY , εY ) and

S(y) = s(y, X, VS , εS ),

for some unit exponential random variables εY and εS that are independent of (X, V ),
y = ΘY−1 , and s = ΘS−1 .

81 Abbring and Van den Berg (2003b) allow for defective distributions, which often have structural interpre-

tations. For example, some women never have children and some workers will never leave a job. See Abbring
(2002) for discussion.
82 Let T | X be distributed with density f (t | X), non-defective cumulative distribution function F (t | X),
!
and hazard rate θ(t | X) = f (t | X)/[1 − F (t | X)]. Then, 0T θ(t | X) dt = − ln[1 − F (T | X)] is a unit
exponential random variable that is independent of X.
83 The causal hazard model only implies that the distributions of ε and ε are invariant across assigned
Y
S
treatments and outcomes, respectively; their realizations may not be. This is sufficient for the variation
of y(s, X, VY , εY ) with s and of s(y, X, VS , εS ) with y to have a causal interpretation. The further restriction
that the random variables εY and εS are invariant is made for simplicity, and is empirically innocuous. See
Abbring and Van den Berg (2003b) for details and Freedman (2004) for discussion.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5233

The exponential errors εY and εS embody the ex post shocks that are inherent to the
individual hazard processes, that is the randomness in the transition process after conditioning on covariates X and V and survival. We assume that εY ⊥
⊥ εS , so that {Y (s)} and
{S(y)} are only dependent through the observed and unobserved covariates (X, V ). This
conditional-independence assumption is weaker than the conditional-independence assumption underlying the analysis of Section 3.2 and used in matching, because it allows for conditioning on the invariant unobservables V . It shares this feature with the
discrete-time models developed in Section 3.4 and is a version of matching on unobserved variables discussed in Section 2.
We assume a version of the no-anticipation condition of Section 3.2.1: for all t ∈ R+ ,
θY (t | s, X, VY ) = θY (t | s  , X, VY )

and θS (t | y, X, VS ) = θS (t | y  , X, VS )

for all s, s  , y, y  ∈ [t, ∞). This excludes effects of anticipation of the treatment on
the outcome. Similarly, there can be no anticipation effects of future outcomes on the
treatment hazard.
E XAMPLE 7. Consider a standard search model describing the job search behavior of
an unemployed individual [e.g., Mortensen (1986)] with characteristics (X, V ). Job offers arrive at a rate λ > 0 and are random draws from a given distribution F . Both
λ and F may depend on (X, V ), but for notational simplicity we suppress all explicit
representations of conditioning on (X, V ) throughout this example. An offer is either
accepted or rejected. A rejected offer cannot be recalled at a later time. The individual initially receives a constant flow of unemployment-insurance benefits. However, the
individual faces the risk of a sanction—a permanent reduction of his benefits to some
lower, constant level—at some point during his unemployment spell. During the unemployment spell, sanctions arrive independently of the job-offer process at a constant
rate μ > 0. The individual cannot foresee the exact time a sanction is imposed, but he
knows the distribution of these times.84 The individual chooses a job-acceptance rule as
to maximize his expected discounted lifetime income. Under standard conditions, this
is a reservation-wage rule: at time t, the individual accepts each wage of w(t) or higher.
The corresponding re-employment hazard rate is λ(1 − F (w(t))). Apart from the sanction, which is not foreseen and arrives at a constant rate during the unemployment spell,
the model is stationary. This implies that the reservation wage is constant, say equal
to w0 , up to and including time s, jumps to some lower level w 1 < w0 at time s and
stays constant at w1 for the remainder of the unemployment spell if benefits would be
reduced at time s.
The model is a version of the simultaneous-equations model for durations. To see this,
let Y be the re-employment duration and S the sanction time. The potential-outcome

84 This is a rudimentary version of the search model with punitive benefits reductions, or sanctions, of

Abbring, Van den Berg and Van Ours (2005). The main difference is that in the present version of the model
the sanctions process cannot be controlled by the agent.

5234

J.H. Abbring and J.J. Heckman

hazards are
θY (t | s) =



λ0
λ1

if 0  t  s,
if t > s,

where λ0 = λ[1 − F (w0 )] and λ1 = λ[1 − F (w1 )], and clearly λ1  λ0 . Similarly,
the potential-treatment time hazards are θS (t | y) = μ if 0  t  y, and 0 otherwise.
Note that the no-anticipation condition follows naturally from the recursive structure of
the economic decision problem in this case in which we have properly accounted for all
relevant components of agent information sets. Furthermore, the assumed independence
of the job offer and sanction processes at the individual level for given (X, V ) implies
that εY ⊥
⊥ εS .
The actual outcome and treatment are related to the potential outcomes and treatments by S = S(Y ) and Y = Y (S). The no-anticipation assumption ensures that this
system has a unique solution (Y, S) by imposing a recursive structure on the underlying
transition processes. Without anticipation effects, current treatment and outcome hazards only depend on past outcome and treatment events, and the transition processes
evolve recursively [Abbring and Van den Berg (2003b)]. Together with a distribution
G(· | X) of V | X, this gives a nonparametric structural model of the distribution
of (Y, S) | X that embodies general simultaneous causal dependence of Y and S, dependence of (Y, X) on observed covariates X, and general dependence of the unobserved
errors VY and VS .
There are two reasons for imposing further restrictions on this model. First, it is
not identified from data on (Y, S, X). Take a version of the model with selection on
unobservables (VY ⊥
⊥
/ VS | X) and consider the distribution of (Y, S) | X generated by
this version of the model. Then, there exists an alternative version of the model that
satisfies both no-anticipation and VY ⊥
⊥ VS | X, and that generates the same distribution
of (Y, S) | X [Abbring and Van den Berg (2003b, Proposition 1)]. In other words,
for each version of the model with selection on unobservables and anticipation effects,
there is an observationally-equivalent model version that satisfies no-anticipation and
conditional randomization. This is a version of the nonidentification result discussed in
Section 3.2.1.
Second, even if we ensure nonparametric identification by assuming no-anticipation
and conditional randomization, we cannot learn about the agent-level causal effects embodied in y and s without imposing even further restrictions. At best, under regularity
conditions we can identify θY (t | s, X) = E[θY (t | s, X, VY ) | X, Y (s)  t] and
θS (t | y, X) = E[θS (t | y, X, VS ) | X, S(y)  t] from standard hazard regressions
[e.g., Andersen et al. (1993), Fleming and Harrington (1991)]. Thus we can identify the
distributions of Y (s) | X and S(y) | X, and therefore solve the selection problem if we
are only interested in these distributions. However, if we are also interested in the causal
effects on the corresponding hazard rates for given X, V , we face an additional dynamic
selection problem. The hazards of the identified distributions of Y (s) | X and S(y) | X
only condition on observed covariates X, and not on unobserved covariates V , and are

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5235

confounded with dynamic selection effects [Heckman and Borjas (1980), Heckman and
Singer (1986), Meyer (1996), Abbring and Van den Berg (2005)]. For example, the
difference between θY (t | s, X) and θY (t | s  , X) does not only reflect agent-level differences between θY (t | s, X, VY ) and θY (t | s  , X, VY ), but also differences in the
subpopulations of survivors {X, Y (s)  t} and {X, Y (s  )  t} on which the hazards are
computed.
In the next two subsections, we discuss what can be learned about treatment effects
in duration models under additional model restrictions. We take the no-anticipation assumption as fundamental. As explained in Section 3.2, this requires that we measure
and include in our model all relevant information needed to define potential outcomes.
However, we relax the randomization assumption. We first consider Abbring and Van
den Berg’s (2003b) analysis of identification without exclusion restrictions. They argue
that these results are useful, because exclusion restrictions are hard to justify in an inherently dynamic setting with forward-looking agents. Abbring and Van den Berg (2005)
further clarify this issue by studying inference for treatment effects in duration models
using a social experiment. We discuss what can be learned from such experiments at the
end of this section.
3.3.1.2. Identifiability without exclusion restrictions Abbring and Van den Berg consider an extension of the multivariate Mixed Proportional Hazard (MPH) model
[Lancaster (1979)] in which the hazard rates of Y (s) | (X, V ) and S(y) | (X, V ) are
given by

λY (t)φY (X)VY
if t  s,
θY (t | s, X, V ) =
(3.8)
and
λY (t)φY (X)δY (t, s, X)VY if t > s

if t  y,
λS (t)φS (X)VS
θS (t | y, X, V ) =
(3.9)
λS (t)φS (X)δS (t, y, X)VS if t > y,
respectively, and V = (VY , VS ) is distributed independently of X. The baseline hazards λY : R+ → (0, ∞) and λS : R+ → (0, ∞) capture duration
! t dependence of the
individual!transition rates. The integrated hazards are ΛY (t) = 0 λY (τ ) dτ < ∞ and
t
ΛS (t) = 0 λS (τ ) dτ < ∞ for all t ∈ R+ . The regressor functions φY : X → (0, ∞)
and φS : X → (0, ∞) are assumed to be continuous, with X ⊂ Rq the support of X.
In empirical work, these functions are frequently specified as φY (x) = exp(x  βY ) and
φS (x) = exp(x  βS ) for some parameter vectors βY and βS . We will not make such
parametric assumptions. Note that the fact that both regressor functions are defined on
the same domain X is not restrictive, because each function φY and φS can “select”
certain elements of X by being trivial functions of the other elements. In the parametric example, the vector βY would only have nonzero elements for those regressors that
matter to the outcome hazard. The functions δY and δS capture the causal effects. Note
that δY (t, s, X) only enters θY (t | s, X, V ) at durations t > s, so that the model satisfies
no anticipation of treatment. Similarly, it satisfies no anticipation of outcomes and has
a recursive causal structure as required by the no-anticipation assumption. If δY = 1,

5236

J.H. Abbring and J.J. Heckman

treatment is ineffective; if δY is larger than 1, it stochastically reduces the remaining
outcome duration.
Note that this model allows δY and δS to depend on elapsed duration t, past endogenous events, and the observed covariates X, but not on V . Abbring and Van den Berg
also consider an alternative model that allows δY and δS to depend on unobservables in
a general way, but not on past endogenous events.
Abbring and Van den Berg show that these models are nonparametrically identified
from single-spell data under the conditions for the identification of competing risks
models based on the multivariate MPH model given by Abbring and Van den Berg
(2003a). Among other conditions are the requirements that there is some independent
local variation of the regressor effects in both hazard rates and a finite-mean restriction
on V , and are standard in the analysis of multivariate MPH models. With multiple-spell
data, most of these assumptions, and the MPH structure, can be relaxed [Abbring and
Van den Berg (2003b)].
The models can be parameterized in a flexible way and estimated by maximum likelihood. Typical parameterizations involve linear-index structures for the regressor and
causal effects, a discrete distribution G, and piecewise-constant baseline hazards λS
and λY . Abbring and Van den Berg (2003c) develop a simple graphical method for inference on the sign of ln(δY ) in the absence of regressors. Abbring, Van den Berg and
Van Ours (2005) present an empirical application.
3.3.1.3. Inference based on instrumental variables The concerns expressed in Section 3.2.4 about the validity of exclusion restrictions in dynamic settings carry over to
event-history models.
E XAMPLE 8. A good illustration of this point is offered by the analysis of Eberwein,
Ham and LaLonde (1997), who study the effects of a training program on labor-market
transitions. Their data are particularly nice, as potential participants are randomized
into treatment and control groups at some baseline point in time. This allows them to
estimate the effect of intention to treat (with training) on subsequent labor-market transitions. This is directly relevant to policy evaluation in the case that the policy involves
changing training enrollment through offers of treatment which may or may not be accepted by agents.
However, Eberwein et al. are also interested in the effect of actual participation in the
training program on post-program labor-market transitions. This is a distinct problem,
because compliance with the intention-to-treat protocol is imperfect. Some agents in the
control group are able to enroll in substitute programs, and some agents in the treatment
group choose never to enroll in a program at all. Moreover, actual enrollment does not
take place at the baseline time, but is dispersed over time. Those in the treatment group
are more likely to enroll earlier. This fact, coupled with the initial randomization, suggests that the intention-to-treat indicator might be used as an instrument for identifying
the effect of program participation on employment and unemployment spells.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5237

The dynamic nature of enrollment into the training program, and the event-history
focus of the analysis complicate matters considerably. Standard instrumental-variables
methods cannot be directly applied. Instead, Eberwein et al. use a parametric duration
model for pre- and post-program outcomes that excludes the intention-to-treat indicator from directly determining outcomes. They specify a duration model for training
enrollment that includes an intention-to-treat indicator as an explanatory variable, and
specify a model for labor-market transitions that excludes the intention-to-treat indicator and imposes a no-anticipation condition on the effect of actual training participation
on labor-market transitions. Such a model is consistent with an environment in which
agents cannot perfectly foresee the actual training time they will be assigned and in
which they do not respond to information about this time revealed by their assignment to an intention-to-treat group. This is a strong assumption. In a search model
with forward-looking agents, for example, such information would typically affect the
ex ante values of unemployment and employment. Then, it would affect the labormarket transitions before actual training enrollment through changes in search efforts
and reservation wages, unless these are both assumed to be exogenous. An assumption
of perfect foresight on the part of the agents being studied only complicates matters
further.
Abbring and Van den Berg (2005) study what can be learned about dynamically assigned programs from social experiments if the intention-to-treat instrument cannot be
excluded from the outcome equation. They develop bounds, tests for unobserved heterogeneity, and point-identification results that extend those discussed in this section.85
3.3.2. Treatment effects in more general event-history models
It is instructive to place the causal duration models developed in Section 3.3.1 in the
more general setting of event-history models with state dependence and heterogeneity.
We do this following Abbring’s (2008) analysis of the mixed semi-Markov model.
3.3.2.1. The mixed semi-Markov event-history model The model is formulated in a
fashion analogous to the frameworks of Heckman and Singer (1986). The point of departure is a continuous-time stochastic process assuming values in a finite set S at each
point in time. We will interpret realizations of this process as agents’ event histories of
transitions between states in the state space S.
Suppose that event histories start at real-valued random times T0 in an S-valued random state S0 , and that subsequent transitions occur at random times T1 , T2 , . . . such
that T0 < T1 < T2 < · · ·. Let Sl be the random destination state of the transition at Tl .
Taking the sample paths of the event-history process to be right-continuous, we have
that Sl is the state occupied in the interval [Tl , Tl+1 ).
85 In the special case that a static treatment, or treatment plan, is assigned at the start of the spell, standard

instrumental-variables methods may be applied. See Abbring and Van den Berg (2005).

5238

J.H. Abbring and J.J. Heckman

Suppose that heterogeneity among agents is captured by vectors of time-constant
observed covariates X and unobserved covariates V .86 In this case, state dependence
in the event-history process for given individual characteristics X, V has a causal interpretation.87 We structure such state dependence by assuming that the event-history
process conditional on X, V is a time-homogeneous semi-Markov process. Conditional
on X, V the length of a spell in a state and the destination state of the transition
ending that spell depend only on the past through the current state. In our notation,
⊥ {(Ti , Si ), i = 0, . . . , l − 1} | Sl−1 , X, V , where Tl = Tl − Tl−1 is the
(Tl , Sl ) ⊥
length of spell l. Also, the distribution of (Tl , Sl ) | Sl−1 , X, V does not depend on l.
Note that, conditional on X, V , {Sl , l  0} is a time-homogeneous Markov chain under
these assumptions.
Nontrivial dynamic selection effects arise because V is not observed. The eventhistory process conditional on observed covariates X only is a mixed semi-Markov
process. If V affects the initial state S0 , or transitions from there, subpopulations of
agents in different states at some time t typically have different distributions of the unobserved characteristics V . Therefore, a comparison of the subsequent transitions in two
such subpopulations does not only reflect state dependence, but also sorting of agents
with different unobserved characteristics into the different states they occupy at time t.
We model {(Tl , Sl ), l  1} | T0 , S0 , X, V as a repeated competing risks model.
Due to the mixed semi-Markov assumption, the latent durations corresponding to transitions into the possible destination states in the lth spell only depend on the past through
the current state Sl−1 , conditional on X, V . This implies that we can fully specify the
repeated competing risks model by specifying a set of origin-destination-specific latent
durations, with corresponding transition rates. Let Tjlk denote the latent duration corresponding to the transition from state j to state k in spell l. We explicitly allow for
the possibility that transitions between certain (ordered) pairs of states may be impossible. To this end, define the correspondence Q : S → σ (S) assigning to each s ∈ S
the set of all destination states to which transitions are made from s with positive probability.88 Here, σ (S) is the set of all subsets of S (the “power set” of S). Then, the
length of spell l is given by Tl = mins∈Q(Sl−1 ) TSll−1 s , and the destination state by
Sl = arg mins∈Q(Sl−1 ) TSll−1 s .
We take the latent durations to be mutually independent, jointly independent from
T0 , S0 , and identically distributed across spells l, all conditional on X, V . This reflects

86 We restrict attention to time-invariant observed covariates for expositional convenience. The analysis can

easily be adapted to more general time-varying external covariates. Restricting attention to time-constant
regressors is a worst-case scenario for identification. External time variation in observed covariates aids identification [Heckman and Taber (1994)].
87 We could make this explicit by extending the potential-outcomes model of Section 3.3.1 to the general
event-history setup. However, this would add a lot of complexity, but little extra insight.
88 Throughout this section, we assume that Q is known. It is important to note, however, that Q can actually
be identified trivially in all cases considered.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5239

both the mixed semi-Markov assumption and the additional assumption that all dependence between the latent durations corresponding to the competing risks in a given spell
l is captured by the observed regressors X and the unobservables V . This is a standard
assumption in econometric duration analysis, which, with the semi-Markov assumption,
allows us to characterize the distribution of {(Tl , Sl ), l  1} | T0 , S0 , X, V by specifying origin-destination-specific hazards θj k (t | X, V ) for the marginal distributions
of Tjlk | X, V .
We assume that the hazards θj k (t | X, V ) are of the mixed proportional hazard (MPH)
type:89

λj k (t)φj k (X)Vj k if k ∈ Q(j ),
θj k (t | X, V ) =
(3.10)
0
otherwise.
The baseline hazards λj k : R+ → (0, ∞) have integrated hazards Λj k (t) =
!t
0 λj k (τ ) dτ < ∞ for all t ∈ R+ . The regressor functions φj k : X → (0, ∞) are
assumed to be continuous. Finally, the (0, ∞)-valued random variable Vj k is the scalar
component of V that affects the transition from state j to state k. Note that we allow
for general dependence between the components of V . This way, we can capture, for
example, that agents with lower re-employment rates have higher training enrollment
rates.
This model fully characterizes the distribution of the transitions {(Tl , Sl ), l  1}
conditional on the initial conditions T0 , S0 and the agents’ characteristics X, V . A complete model of the event histories {(Tl , Sl ), l  0} conditional on X, V would in
addition require a specification of the initial conditions T0 , S0 for given X, V . It is important to stress here that T0 , S0 are the initial conditions of the event-history process itself,
and should not be confused with the initial conditions in a particular sample (which we
will discuss below). In empirical work, interest in the dependence between start times
T0 and characteristics X, V is often limited to the observation that the distribution of
agent’s characteristics may vary over cohorts indexed by T0 . The choice of initial state
S0 may in general be of some interest, but is often trivial. For example, we could model
labor-market histories from the calendar time T0 at which agents turn 15 onwards. In
an economy with perfect compliance to a mandatory schooling up to age 15, the initial state S0 would be “(mandatory) schooling” for all. Therefore, we will not consider
a model of the event history’s initial conditions, but instead focus on the conditional
model of subsequent transition histories.
Because of the semi-Markov assumption, the distribution of {(Tl , Sl ), l  1} |
T0 , S0 , X, V only depends on S0 , and not T0 . Thus, T0 only affects observed event histories through cohort effects on the distribution of unobserved characteristics V . The
initial state S0 , on the other hand, may both have causal effects on subsequent transitions and be informative on the distribution of V . For expositional clarity, we assume

89 Proportionality can be relaxed if we have data on sufficiently long event-histories. See Honoré (1993) and

Abbring and Van den Berg (2003a, 2003b) for related arguments for various multi-spell duration models.

5240

J.H. Abbring and J.J. Heckman

that V ⊥
⊥ (T0 , S0 , X). This is true, for example, if all agents start in the same state, so
that S0 is degenerate, and V is independent of the start date T0 and the observed covariates X.
An econometric model for transition histories conditional on the observed covariates
X can be derived from the model of {(Tl , Sl ), l  1} | S0 , X, V by integrating out V .
The exact way this should be done depends on the sampling scheme used. Here, we
focus on sampling from the population of event-histories. We assume that we observe
the covariates X, the initial state S0 , and the first L transitions from there. Then, we can
model these transitions for given S0 , X by integrating the conditional model over the
distribution of V .
Abbring (2008) discusses more complex, and arguably more realistic, sampling
schemes. For example, when studying labor-market histories we may randomly sample
from the stock of the unemployed at a particular point in time. Because the unobserved
component V affects the probability of being unemployed at the sampling date, the distribution of V | X in the stock sample does not equal its population distribution. This is
again a dynamic version of the selection problem. Moreover, in this case we typically
do not observe an agent’s entire labor-market history from T0 onwards. Instead, we may
have data on the time spent in unemployment at the sampling date and on labor-market
transitions for some period after the sampling date. This “initial conditions problem”
complicates matters further [Heckman (1981b)].
In the next two subsections, we first discuss some examples of applications of the
model and then review a basic identification result for the simple sampling scheme
above.
3.3.2.2. Applications to program evaluation Several empirical papers study the effect
of a single treatment on some outcome duration or set of transitions. Two approaches
can be distinguished. In the first approach, the outcome and treatment processes are
explicitly and separately specified. The second approach distinguishes treatment as one
state within a single event-history model with state dependence.
The first approach is used in a variety of papers in labor economics. Eberwein, Ham
and LaLonde (1997) specify a model for labor-market transitions in which the transition intensities between various labor-market states (not including treatment) depend on
whether someone has been assigned to a training program in the past or not. Abbring,
Van den Berg and Van Ours (2005) and Van den Berg, Van der Klaauw and Van Ours
(2004) specify a model for re-employment durations in which the re-employment hazard
depends on whether a punitive benefits reduction has been imposed in the past. Similarly, Van den Berg, Holm and Van Ours (2002) analyze the duration up to transition
into medical trainee positions and the effect of an intermediate transition into a medical
assistant position (a “stepping-stone job”) on this duration. In all of these papers, the
outcome model is complemented with a hazard model for treatment choice.
These models fit into the framework of Section 3.3.1 or a multi-state extension
thereof. We can rephrase the class of models discussed in Section 3.3.1 in terms of
a simple event-history model with state dependence as follows. Distinguish three states,

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5241

untreated (O), treated (P ) and the exit state of interest (E), so that S = {O, P , E}.
All subjects start in O, so that S0 = O. Obviously, we do not want to allow for all
possible transitions between these three states. Instead, we restrict the correspondence
Q representing the possible transitions as follows:

{P , E}
s = O,
Q(s) = {E}
if s = P ,
∅
s = E.
State dependence of the transition rates into E captures treatment effects in the sense
of Section 3.3.1. Not all models in Abbring and Van den Berg (2003b) are included
in the semi-Markov setup discussed here. In particular, in this paper we do not allow
the transition rate from P to E to depend on the duration spent in O. This extension
with “lagged duration dependence” [Heckman and Borjas (1980)] would be required to
capture one variant of their model.
The model for transitions from “untreated” (O) is a competing risks model, with program enrollment (transition to P ) and employment (E) competing to end the untreated
spell. If the unobservable factor VOE that determines transitions to employment and the
unobservable factor VOP affecting program enrollment are dependent, then program
enrollment is selective in the sense that the initial distribution of VOE —and also typically that of VP E —among those who enroll at a given point in time does not equal its
distribution among survivors in O up to that time.90
The second approach is used by Gritz (1993) and Bonnal, Fougère and Sérandon
(1997), among others. Consider the following simplified setup. Suppose workers are
either employed (E), unemployed (O), or engaged in a training program (P ). We can
now specify a transition process among these three labor-market states in which a causal
effect of training on unemployment and employment durations is modeled as dependence of the various transition rates on the past occurrence of a training program in
the labor-market history. Bonnal, Fougère and Sérandon (1997) only have limited information on agents’ labor-market histories before the sample period. Partly to avoid
difficult initial conditions problems, they restrict attention to “first order lagged occurrence dependence” [Heckman and Borjas (1980)] by assuming that transition rates
only depend on the current and previous state occupied. Such a model is not directly covered by the semi-Markov model, but with a simple augmentation of the state
space it can be covered. In particular, we have to include lagged states in the state
space on which the transition process is defined. Because there is no lagged state in
the event-history’s first spell, initial states should be defined separately. So, instead
of just distinguishing states in S ∗ = {E, O, P }, we distinguish augmented states in
S = {(s, s  ) ∈ (S ∗ ∪ {I }) × S ∗ : s = s  }. Then, (I, s), s ∈ S ∗ , denote the initial
states, and (s, s  ) ∈ S the augmented state of an agent who is currently in s  and came
from s = s  . In order to preserve the interpretation of the model as a model of lagged
90 Note that, in addition, the survivors in O themselves are a selected subpopulation. Because V affects

survival in O, the distribution of V among survivors in O is not equal to its population distribution.

5242

J.H. Abbring and J.J. Heckman

occurrence dependence, we have to exclude certain transitions by specifying


Q(s, s  ) = (s  , s  ), s  ∈ S ∗ \ {s  } .
This excludes transitions to augmented states that are labeled with a lagged state different from the origin state. Also, it ensures that agents never return to an initial state.
For example, from the augmented state (O, P )—previously unemployed and currently
enrolled in a program—only transitions to augmented states (P , s  )—previously enrolled in a program and currently in s  —are possible. Moreover, it is not possible to
be currently employed and transiting to initially unemployed, (I, O). Rather, an employed person who loses her job would transit to (E, O)—currently unemployed and
previously employed.
The effects of, for example, training are now modeled as simple state-dependence
effects. For example, the effect of training on the transition rate from unemployment to
employment is simply the contrast between the individual transition rate from (E, O)
to (O, E) and the transition rate from (P , O) to (O, E). Dynamic selection into the
augmented states (E, O) and (P , O), as specified by the transition model, confounds
the empirical analysis of these training effects. Note that due to the fact that we have
restricted attention to first-order lagged occurrence dependence, there are no longer-run
effects of training on transition rates from unemployment to employment.
3.3.2.3. Identification without exclusion restrictions In this section, we sketch a basic
identification result for the following sampling scheme. Suppose that the economist
randomly samples from the population of event-histories, and that we observe the first L̄
transitions (including destinations) for each sampled event-history, with the possibility
that L̄ = ∞.91 Thus, we observe a random sample of {(Tl , Sl ), l ∈ {0, 1, . . . , L̄}},
and X.
First note that we can only identify the determinants of θj k for transitions (j, k) that
occur with positive probability among the first L̄ transitions. Moreover, without further
restrictions, we can only identify the joint distribution of a vector of unobservables
corresponding to (part of) a sequence of transitions that can be observed among the
first L̄ transitions.
With this qualification, identification can be proved by extending Abbring and Van
den Berg’s (2003a) analysis of the MPH competing risks model to the present setting.
This analysis assumes that transition rates have an MPH functional form. Identification
again requires specific moments of V to be finite, and independent local variation in the
regressor effects.
3.3.3. A structural perspective
Without further restrictions, the causal duration model of Section 3.3.1 is versatile. It
can be generated as the reduced form of a wide variety of continuous-time economic
91 Note that this assumes away econometric initial conditions problems of the type previously discussed.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5243

models driven by point processes. Leading examples are sequential job-search models
in which job-offer arrival rates, and other model parameters, depend on agent characteristics (X, V ) and policy interventions [see, e.g., Mortensen (1986), and Example 7].
The MPH restriction on this model, however, is hard to justify from economic theory. In particular, nonstationary job-search models often imply interactions between
duration and covariate effects; the MPH model only results under strong assumptions
[Heckman and Singer (1986), Van den Berg (2001)]. Similarly, an MPH structure is
hard to generate from models in which agents learn about their individual value of the
model’s structural parameters, that is about (X, V ), through Bayesian updating.
An alternative class of continuous-time models, not discussed in this chapter, specifies durations as the first time some Gaussian or more general process crosses a
threshold. Such models are closely related to a variety of dynamic economic models.
They have attracted recent attention in statistics [see, e.g., Aalen and Gjessing (2004)].
Abbring (2007) analyzes identifiability of “mixed hitting-time models”, continuoustime threshold-crossing models in which the parameters depend on observed and unobserved covariates, and discusses their link with optimizing models in economics. This
is a relatively new area of research, and a full development is beyond the scope of this
paper. It extends to a continuous-time framework the dynamic threshold-crossing model
developed in Heckman (1981a, 1981b) that is used in the next subsection of this chapter.
We now discuss a complementary discrete-time approach where it is possible to
make many important economic distinctions that are difficult to make in the setting of
continuous-time models and to avoid some difficult measure-theoretic problems. In the
structural version, it is possible to specify precisely agent information sets in a fashion
that is not possible in conventional duration models.
3.4. Dynamic discrete choice and dynamic treatment effects
Heckman and Navarro (2007) and Cunha, Heckman and Navarro (2007) present econometric models for analyzing time to treatment and the consequences of the choice of
a particular treatment time. Treatment may be a medical intervention, stopping schooling, opening a store, conducting an advertising campaign at a given date or renewing a
patent. Associated with each treatment time, there can be multiple outcomes. They can
include a vector of health status indicators and biomarkers; lifetime employment and
earnings consequences of stopping at a particular grade of schooling; the sales revenue
and profit generated from opening a store at a certain time; the revenues generated and
market penetration gained from an advertising campaign; or the value of exercising an
option at a given time. Heckman and Navarro (2007) unite and contribute to the literatures on dynamic discrete choice and dynamic treatment effects. For both classes of
models, they present semiparametric identification analyses. We summarize their work
in this section. It is a natural extension of the framework for counterfactual analysis
of multiple treatments developed in Section 2 to a dynamic setting. It is formulated in
discrete time, which facilitates the specification of richer unobserved and observed co-

5244

J.H. Abbring and J.J. Heckman

variate processes than those entertained in the continuous-time framework of Abbring
and Van den Berg (2003b).
Heckman and Navarro extend the literature on treatment effects to model choices
of treatment times and the consequences of choice and link the literature on treatment
effects to the literature on precisely formulated structural dynamic discrete-choice models generated from index models crossing thresholds. They show the value of precisely
formulated economic models in extracting the information sets of agents, in providing
model identification, in generating the standard treatment effects and in enforcing the
nonanticipating behavior condition (NA) discussed in Section 3.2.1.92
They establish the semiparametric identifiability of a class of dynamic discrete-choice
models for stopping times and associated outcomes in which agents sequentially update
the information on which they act. They also establish identifiability of a new class of
reduced form duration models that generalize conventional discrete-time duration models to produce frameworks with much richer time series properties for unobservables
and general time-varying observables and patterns of duration dependence than conventional duration models. Their analysis of identification of these generalized models
requires richer variation driven by observables than is needed in the analysis of the
more restrictive conventional models. However, it does not require conventional periodby-period exclusion restrictions, which are often difficult to justify. Instead, they rely
on curvature restrictions across the index functions generating the durations that can be
motivated by dynamic economic theory.93 Their methods can be applied to a variety of
outcome measures including durations.
The key to their ability to identify structural models is that they supplement information on stopping times or time to treatment with additional information on measured
consequences of choices of time to treatment as well as measurements. The dynamic
discrete-choice literature surveyed in Rust (1994) and Magnac and Thesmar (2002) focuses on discrete-choice processes with general preferences and state vector evolution
equations, typically Markovian in nature. Rust’s (1994) paper contains negative results on nonparametric identification of discrete-choice processes. Magnac and Thesmar
(2002) present some positive results on nonparametric identification if certain parameters or distributions of unobservables are assumed to be known. Heckman and Navarro
(2007) produce positive results on nonparametric identification of a class of dynamic
discrete-choice models based on expected income maximization developed in labor
economics by Flinn and Heckman (1982), Keane and Wolpin (1997) and Eckstein and
Wolpin (1999). These frameworks are dynamic versions of the Roy model. Heckman
and Navarro (2007) show how use of cross-equation restrictions joined with data on supplementary measurement systems can undo Rust’s nonidentification result. We exposit
92 Aakvik, Heckman and Vytlacil (2005), Heckman, Tobias and Vytlacil (2001, 2003), Carneiro, Hansen

and Heckman (2001, 2003) and Heckman and Vytlacil (2005) show how standard treatment effects can be
generated from structural models.
93 See Heckman and Honoré (1989) for examples of such an identification strategy in duration models. See
also Cameron and Heckman (1998).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5245

their work and the related literature in this section. With their structural framework,
they can distinguish objective outcomes from subjective outcomes (valuations by the
decision maker) in a dynamic setting. Applying their analysis to health economics, they
can identify the causal effects on health of a medical treatment as well as the associated
subjective pain and suffering of a treatment regime for the patient.94 Attrition decisions
also convey information about agent preferences about treatment.95
They do not rely on the assumption of conditional independence of unobservables with outcomes, given observables, that is used throughout much of the dynamic
discrete-choice literature and the dynamic treatment literature surveyed in Section 3.2.96
As noted in Section 3.1, sequential conditional-independence assumptions underlie recent work on reduced form dynamic treatment effects.97 The semiparametric analysis of
Heckman and Navarro (2007) based on factors generalizes matching to a dynamic setting. In their paper, some of the variables that would produce conditional independence
and would justify matching if they were observed, are treated as unobserved match variables. They are integrated out and their distributions are identified.98 They consider two
classes of models. We review both.
3.4.1. Semiparametric duration models and counterfactuals
Heckman and Navarro (2007), henceforth HN, develop a semiparametric index model
for dynamic discrete choices that extends conventional discrete time duration analysis. They separate out duration dependence from heterogeneity in a semiparametric
framework more general than conventional discrete-time duration models. They produce a new class of reduced form models for dynamic treatment effects by adjoining
time-to-treatment outcomes to the duration model. This analysis builds on Heckman
(1981a, 1981b, 1981c).
Their models are based on a latent variable for choice at time s,


I (s) = Ψ s, Z(s) − η(s),
where the Z(s) are observables and η(s) are unobservables from the point of view of
the econometrician. Treatments at different times may have different outcome consequences which they model after analyzing the time to treatment equation. Define D(s)
as an indicator of receipt of treatment at date s. Treatment is taken the first time I (s)

94 See Chan and Hamilton (2006) for a structural dynamic empirical analysis of this problem.
95 See Heckman and Smith (1998). Use of participation data to infer preferences about outcomes is developed

in Heckman (1974b).
96 See, e.g., Rust (1987), Manski (1993), Hotz and Miller (1993) and the papers cited in Rust (1994).
97 See, e.g., Gill and Robins (2001) and Lechner and Miquel (2002).
98 For estimates based on this idea see Carneiro, Hansen and Heckman (2003), Aakvik, Heckman and Vytlacil

(2005), Cunha and Heckman (2007b, 2008), Cunha, Heckman and Navarro (2005, 2006), and Heckman and
Navarro (2005).

5246

J.H. Abbring and J.J. Heckman

becomes positive. Thus,


D(s) = 1 I (s)  0, I (s − 1) < 0, . . . , I (1) < 0 ,
where the indicator function 1[·] takes the value of 1 if the term inside the braces is
true.99 They derive conditions for identifying a model with general forms of duration
dependence in the time to treatment equation using a large sample from the distribution
of (D, Z).
3.4.1.1. Single spell duration model Individuals are assumed to start spells in a given
(exogenously determined) state and to exit the state at the beginning of time period S.100
S is thus a random variable representing total completed spell length. Let D(s) = 1 if
the individual exits at time s, S = s, and D(s) = 0 otherwise. In an analysis of drug
treatments, S is the discrete-time period in the course of an illness at the beginning
of which the drug is administered. Let S̄ (< ∞) be the upper limit on the time the
agent being studied can be at risk for a treatment. It is possible in this example that
D(1) = 0, . . . , D(S̄) = 0, so that a patient never receives treatment. In a schooling
example, “treatment” is not schooling, but rather dropping out of schooling.101 In this
case, S̄ is an upper limit to the number of years of schooling, and D(S̄) = 1 if D(1) = 0,
. . . , D(S̄ − 1) = 0.
The duration model can be specified recursively in terms of the threshold-crossing
behavior of the sequence of underlying latent indices I (s). Recall that I (s) =
Ψ (s, Z(s)) − η(s), with Z(s) being the regressors that are observed by the analyst.
The Z(s) can include expectations of future outcomes given current information in
the case of models with forward-looking behavior. For a given stopping time s, let
D s = (D(1), . . . , D(s)) and designate by d(s) and d s values that D(s) and D s assume.
Thus, d(s) can be zero or one and d s is a sequence of s zeros or a sequence containing
s − 1 zeros and a single one. Denote a sequence of all zeros by (0), regardless of its
length. Then,


D(1) = 1 I (1)  0 and

1[I (s)  0] if D s−1 = (0),
D(s) =
(3.11)
s = 2, . . . , S̄.
0
otherwise,
For s = 2, . . . , S̄, the indicator 1[I (s)  0] is observed if and only if the agent is still at
risk of treatment, D s−1 = (0). To identify period s parameters from period s outcomes,
one must condition on all past outcomes and control for any selection effects.
99 This framework captures the essential feature of any stopping time model. For example, in a search model

with one wage offer per period, I (s) is the gap between market wages and reservation wages at time s. See,
e.g., Flinn and Heckman (1982). This framework can also approximate the explicit dynamic discrete-choice
model analyzed in Section 3.4.2.
100 Thus we abstract from the initial conditions problem discussed in Heckman (1981b).
101 In the drug treatment example, S may designate the time a treatment regime is completed.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5247

Let Z = (Z(1), . . . , Z(S̄)), and let η = (η(1), . . . , η(S̄)).102 Assume that Z is statistically independent of η. Heckman and Navarro (2007) assume that Ψ (s, Z(s)) =
Z(s)γs . We deal with a more general case. Ψ (Z) = (Ψ (1, Z(1)), . . . , Ψ (S̄, Z(S̄))). We
let Ψ denote the abstract parameter. Depending on the values assumed by Ψ (s, Z(s)),
one can generate very general forms of duration dependence that depend on the values
assumed by the Z(s). HN allow for period-specific effects of regressors on the latent
indices generating choices.
This model is the reduced form of a general dynamic discrete-choice model. Like
many reduced form models, the link to choice theory is not clearly specified. It is not a
conventional multinomial choice model in a static (perfect certainty) setting with associated outcomes.
3.4.1.2. Identification of duration models with general error structures and duration
dependence Heckman and Navarro (2007) establish semiparametric identification of
the model of Equation (3.11) assuming access to a large sample of i.i.d. (D, Z) observations. Let Z s = (Z(1), . . . , Z(s)). Data on (D, Z) directly identify the conditional
probability Pr(D(s) = d(s) | Z s , D s−1 = (0)) a.e. FZ s |D s−1 =(0) where FZ s |D s−1 =(0)
is the distribution of Z s conditional on previous choices D s−1 = (0). Assume that
(Ψ, Fη ) ∈ Φ × H, where Fη is the distribution of η and Φ × H is the parameter space. The goal is to establish conditions under which knowledge of Pr(D(s) =
d(s) | Z, D s−1 = (0)) a.e. FZ|D s−1 =(0) allows the analyst to identify a unique element of Φ × H. They use a limit strategy that allows them to recover the parameters by
conditioning on large values of the indices of the preceding choices. This identification
strategy is widely used in the analysis of discrete choice.103
They establish sufficient conditions for the identification of model (3.11). We prove
the following more general result:
T HEOREM 3. For the model defined by Equation (3.11), assume the following conditions:
(i) η ⊥
⊥ Z.
(ii) η is an absolutely continuous random variable on RS̄ with support
"S̄
s=1 (η(s), η̄(s)), where −∞  η(s) < η̄(s)  +∞ for all s = 1, . . . , S̄.
(iii) The Ψ (s, Z(s)) are members of the Matzkin class of functions defined in Appendix B.1, s = 1, . . . , S̄.

102 A special case of the general model arises when η(s) has a factor model representation as analyzed in
Section 2. We will use such a representation when we adjoin outcomes to treatment times later in this section.
103 See, e.g., Manski (1988), Heckman (1990), Heckman and Honoré (1989, 1990), Matzkin (1992, 1993),
Taber (2000), and Carneiro, Hansen and Heckman (2003). A version of the strategy of this proof was first used
in psychology where agent choice sets are eliminated by experimenter manipulation. The limit set argument
effectively uses regressors to reduce the choice set confronting agents. See Falmagne (1985) for a discussion
of models of choice in psychology.

5248

J.H. Abbring and J.J. Heckman

(iv) Supp(Ψ s−1 (Z), Z(s)) = Supp(Ψ s−1 (Z)) × Supp(Z(s)), s = 2, . . . , S̄.
(v) Supp(Ψ (Z)) ⊇ Supp(η).
Then Fη and Ψ (Z) are identified, where the Ψ (s, Z(s)), s = 1, . . . , S̄, are identified
over the relevant support admitted by (ii).
P ROOF. We sketch the proof for S̄ = 2. The result for general S̄ follows by a recursive
application of this argument. Consider the following three probabilities.
 Ψ (1,z(1))


(a) Pr D(1) = 1 | Z = z =
fη(1) (u) du.
η(1)



(b) Pr D(2) = 1, D(1) = 0 | Z = z
 Ψ (2,z(2))  η̄(1)
=
fη(1),η(2) (u1 , u2 ) du1 du2 .
η(2)

(c)

Ψ (1,z(1))



Pr D(2) = 0, D(1) = 0 | Z = z
 η̄(1)
 η̄(2)
=
fη(1),η(2) (u1 , u2 ) du1 du2 .
Ψ (2,z(2)) Ψ (1,z(1))

The left-hand sides are observed from data on those who stop in period 1 (a); those
who stop in period 2 (b); and those who terminate in the “0” state in period 2 (c). From
Matzkin (1992), under our conditions on the class of functions Φ, which are stronger
than hers, we can identify Ψ (1, z(1)) and Fη(1) from (a). Using (b), we can fix z(2)
1 , possibly dependent
and vary Ψ (1, z(1)). From (iv) and (v) there exists a limit set Z
on z(2), such that limz(1)→Z1 Ψ (1, z(1)) = η(1). Thus we can construct


Pr D(2) = 0 | Z = z =



η̄(2)

fη(2) (u2 ) du2
Ψ (2,z(2))

and identify Ψ (2, z(2)) and Fη(2) (η(2)). Using the Ψ (1, z(1)), Ψ (2, z(2)), one can trace
out the joint distribution Fη(1),η(2) over its support. Under the Matzkin conditions, identification is achieved on a nonnegligible set. The proof generalizes in a straightforward
way to general S̄.

Observe that if the η(s) are bounded by finite upper and lower limits, we can only
determine the Ψ (s, Z(s)) over the limits so defined. Consider the first step of the proof.
Under the Matzkin conditions, Fη(1) is known. From assumption (ii) we can determine



−1  
Ψ 1, z(1) = Fη(1)
Pr D(1) = 1 | Z = z ,
but only over the support (η(1), η̄(1)). If the support of η(1) is R, we determine Ψ (1, z(1)) for all z(1). Heckman and Navarro (2007) analyze the special case
Ψ (s, Z(s)) = Z(s)γs and invoke sequential rank conditions to identify γs , even over
limited supports. They also establish that the limit sets are nonnegligible in this case so

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5249

that standard definitions of identifiability [see, e.g., Matzkin (1992)] will be satisfied.104
s , s = 1, . . . , S̄, depends on the functional form speciConstruction of the limit set Z
fied for the Ψ (s, z(s)). For the linear-in-parameters case Ψ (s, z(s)) = Z(s)γs , they are
obtained by letting arguments get big or small. Matzkin (1992) shows how to establish
the limit sets for functions in her family of functions.
A version of Theorem 3 with Ψ (s, Z(s)) = Zs γs that allows dependence between
Z and ηs except for one component can be proved using the analysis of Lewbel (2000)
and Honoré and Lewbel (2002).105
The assumptions of Theorem 3 will be satisfied if there are transition-specific exclusion restrictions for Z with the required properties. As noted in Section 3.3, in models
with many periods, this may be a demanding requirement. Very often, the Z variables
are time invariant and so cannot be used as exclusion restrictions. Corollary 1 in HN,
for the special case Ψ (s, Z(s)) = Z(s)γs , tells us that the HN version of the model
can be identified even if there are no conventional exclusion restrictions and the Z(s)
are the same across all time periods if sufficient structure is placed on how the γs vary
with s. Variations in the values of γs across time periods arise naturally in finite-horizon
dynamic discrete-choice models where a shrinking horizon produces different effects
of the same variable in different periods. For example, in Wolpin’s (1987) analysis of a
search model, the value function depends on time and the derived decision rules weight
the same invariant characteristics differently in different periods. In a schooling model,
parental background and resources may affect education continuation decisions differently at different stages of the schooling decision. The model generating equation (3.11)
can be semiparametrically identified without transition-specific exclusions if the duration dependence is sufficiently general. For a proof, see Corollary 1 in Heckman and
Navarro (2007).
The conditions of Theorem 3 are somewhat similar to the conditions on the regressor
effects needed for identification of the continuous-time event-history models in Section 3.3. One difference is that the present analysis requires independent variation of
the regressor effects over the support of the distribution of the unobservables generating outcomes. The continuous-time analysis based on the functional form of the mixed
proportional hazard model (MPH) as analyzed by Abbring and Van den Berg (2003a)
only requires local independent variation.
Theorem 3 and Corollary 1 in HN have important consequences. The Ψ (s, Z(s)),
s = 1, . . . , S̄, can be interpreted as duration dependence parameters that are modified
by the Z(s) and that vary across the spell in a more general way than is permitted in
104 Heckman and Navarro (2007) prove their theorem for a model where D(s) = 1[I (s)  0] if D s−1 =
(0), s = 2, . . . , S̄. Our formulation of their result is consistent with the notation in this chapter.
105 HN discuss a version of such an extension at their website. Lewbel’s conditions are very strong. To account for general forms of dependence between Z and ηs requires modeling the exact form of the dependence.
Nonparametric solutions to this problem remain an open question in the literature on dynamic discrete choice.
One solution is to assume functional forms for the error terms, but in general, this is not enough to identify
the model without further restrictions imposed. See Heckman and Honoré (1990).

5250

J.H. Abbring and J.J. Heckman

mixed proportional hazards (MPH), generalized accelerated failure time (GAFT) models or standard discrete-time hazard models.106 Duration dependence in conventional
specifications of duration models is usually generated by variation in model intercepts.
The regressors are allowed to interact with the duration dependence parameters. In the
specifications justified by Theorem 3, the “heterogeneity” distribution Fη is identified
for a general model. No special “permanent-transitory” structure is required for the unobservables although that specification is traditional in duration analysis. Their explicit
treatment of the stochastic structure of the duration model is what allows HN to link in a
general way the unobservables generating the duration model to the unobservables generating the outcome equations that are introduced in the next section. Such an explicit
link is not currently available in the literature on continuous-time duration models for
treatment effects surveyed in Section 3.3, and is useful for modelling selection effects
in outcomes across different treatment times. Their outcomes can be both discrete and
continuous and are not restricted to be durations.
Under conditions given in Corollary 1 of HN, no period-specific exclusion conditions are required on the Z. Hansen and Sargent (1980) and Abbring and Van den Berg
(2003b) note that period-specific exclusions are not natural in reduced form duration
models designed to approximate forward-looking life cycle models. Agents make current decisions in light of their forecasts of future constraints and opportunities, and
if they forecast some components well, and they affect current decisions, then they
are in Z(s) in period s. Corollary 1 in HN establishes identification without such exclusions. HN adjoin a system of counterfactual outcomes to their model of time to
treatment to produce a model for dynamic counterfactuals. We summarize that work
next.
3.4.1.3. Reduced form dynamic treatment effects This section reviews a reduced form
approach to generating dynamic counterfactuals developed by HN. They apply and extend the analysis of Carneiro, Hansen and Heckman (2003) and Cunha, Heckman and
Navarro (2005, 2006) to generate ex post potential outcomes and their relationship with
the time to treatment indices I (s) analyzed in the preceding subsection. With reduced
form models, it is difficult to impose restrictions from economic theory or to make
distinctions between ex ante and ex post outcomes. In the structural model developed
below, these and other distinctions can be made easily.
The reduced form model’s specification closely follows the exposition of Section 2.8.1. Associated with each treatment s, s = 1, . . . , S̄, is a vector of T̄ outcomes,


Y s, X, U (s)
 





= Y 1, s, X, U (1, s) , . . . , Y t, s, X, U (t, s) , . . . , Y T̄ , s, X, U (T̄ , s) .
In this section, treatment time s is synonymous with treatment state s in Section 2.
Outcomes depend on covariates X and U (s) = (U (1, s), . . . , U (t, s), . . . , U (T̄ , s))
106 See Ridder (1990) for a discussion of these models.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5251

that are, respectively, observable and unobservable by the econometrician. Elements
of Y (s, X, U (s)) are outcomes associated with stopping or receiving treatment at the
beginning of period s. They are factual outcomes if treatment s is actually selected
(S = s and D(s) = 1). Outcomes corresponding to treatments s  that are not selected
(D(s  ) = 0) are counterfactuals. The outcomes associated with each treatment may be
different, and indeed the treatments administered at different times may be different.
The components Y (t, s, X, U (t, s)) of the vector Y (s, X, U (s)) can be interpreted
as the outcomes revealed at age t, t = 1, . . . , T̄ , and may themselves be vectors. The
reduced form approach presented in this section is not sufficiently rich to capture the notion that agents revise their anticipations of components of Y (s, X, U (s)), s = 1, . . . , S̄,
as they acquire information over time. This notion is systematically developed using the
structural model discussed below in Section 3.4.2.
The treatment “times” may be stages that are not necessarily connected with real
times. Thus s may be a schooling level. The correspondence between stages and times is
exact if each stage takes one period to complete. Our notation is more flexible, and time
and periods can be defined more generally. Our notation in this section accommodates
both cases.
In this section of the chapter, we use the condensed notation introduced in Section 2.8.1. This notation is sufficiently rich to represent the life cycle of outcomes for
persons who receive treatment at s. Thus, in a schooling example, the components of
this vector may include life cycle earnings, employment, and the like associated with a
person with characteristics X, U (s), s = 1, . . . , S̄, who completes s years of schooling
and then forever ceases schooling. It could include earnings while in school at some
level for persons who will eventually attain further schooling as well as post-school
earnings.
We measure age and treatment time on the same time scale, with origin 1, and let
T̄  S̄. Then, the Y (t, s, X, U (t, s)) for t < s are outcomes realized while the person
is in school at age t (s is the time the person will leave school; t is the current age)
and before “treatment” (stopping schooling) has occurred. When t  s, these are postschool outcomes for treatment with s years of schooling. In this case, t − s is years
of post-school experience. In the case of a drug trial, the Y (t, s, X, U (t, s)) for t < s
are measurements observed before the drug is taken at s and if t  s, they are the
post-treatment measurements.
Following Carneiro, Hansen and Heckman (2003) and our analysis in Section 2,
the variables in Y (t, s, X, U (t, s)) may include discrete, continuous or mixed discretecontinuous components. For the discrete or mixed discrete-continuous cases, HN assume that latent continuous variables cross thresholds to generate the discrete components. Durations can be generated by latent index models associated with each outcome
crossing thresholds analogous to the model presented in Equation (3.11). In this framework, for example, we can model the effect of attaining s years of schooling on durations
of unemployment or durations of employment.
The reduced form analysis in this section does not impose restrictions on the temporal (age) structure of outcomes across treatment times in constructing outcomes and

5252

J.H. Abbring and J.J. Heckman

specifying identifying assumptions. Each treatment time can have its own age path of
outcomes pre and post treatment. Outcomes prior to treatment and outcomes after treatment are treated symmetrically and both may be different for different treatment times.
In particular, HN can allow earnings at age t for people who receive treatment at some
future time s  to differ from earnings at age t for people who receive treatment at some
future time s  , min(s  , s  ) > t even after controlling for U and X.
This generality is in contrast with the analyses of Robins (1997) and Gill and Robins
(2001) discussed in Section 3.2 and the analysis of Abbring and Van den Berg (2003b)
discussed in Section 3.3. These analyses require exclusion of such anticipation effects
to secure identification, because their models attribute dependence of treatment on past
outcomes to selection effects. The sequential randomization assumption (M-2) underlying the work of Gill and Robins allows treatment decisions S(t) at time t to depend
on past outcomes Ypt−1
in a general way. Therefore, without additional restrictions, it is
0
not possible to also identify causal (anticipatory) effects of treatment S(t) on Ypt−1
. The
0
no-anticipation condition (NA) excludes such effects and secures identification in their
framework.107 It is essential for applying the conditional-independence assumptions in
deriving the g-computation formula.
HN’s very different approach to identification allows them to incorporate anticipation effects. As in their analysis of the duration model, they assume that there is an
exogenous source of independent variation of treatment decisions, independent of past
outcomes. Any variation in current outcomes with variation in future treatment decisions induced by this exogenous source cannot be due to selection effects (since they
explicitly control for the unobservables) and is interpreted as anticipatory effects of
treatment in their framework. However, their structural analysis naturally excludes such
effects (see Section 3.4.2 below). Therefore, a natural interpretation of the ability of HN
to identify anticipatory effects is that they have overidentifying restrictions that allow
them to test their model and, if necessary, relax their assumptions.
In a model with uncertainty, agents act on and value ex ante outcomes. The model
developed below in Section 3.4.2 distinguishes ex ante from ex post outcomes. The
107 The role of the no-anticipation assumption in Abbring and Van den Berg (2003b) is similar. However,

their main analysis assumes an asymmetric treatment-outcome setup in which treatment is not observed if
it takes place after the outcome transition. In that case, the treatment time is censored at the outcome time.
In this asymmetric setup, anticipatory effects of treatment on outcomes cannot be identified because the
econometrician cannot observe variation of outcome transitions with future treatment times. This point may
appear to be unrelated to the present discussion, but it is not. As was pointed out by Abbring and Van den
Berg (2003b), and in Section 3.3, the asymmetric Abbring and Van den Berg (2003b) model can be extended
to a fully symmetric bivariate duration model in which treatment hazards may be causally affected by the
past occurrence of an outcome event just like outcomes may be affected by past treatment events. This model
could be used to analyze data in which both treatment and outcome times are fully observed. In this symmetric
setup, any dependence in the data of the time-to-treatment hazard on past outcome events is interpreted as an
effect of outcomes on future treatment decisions, and not an anticipatory effect of treatment on past outcomes.
If one does not restrict the effects of outcomes on future treatment, without further restrictions, the data on
treatments occurring after the outcome event carry no information on anticipatory effects of treatment on
outcomes and they face an identification problem similar to that in the asymmetric case.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5253

model developed in this section cannot because, within it, it is difficult to specify the
information sets on which agents act or the mechanism by which agents forecast and
act on Y (s, X, U (s)) when they are making choices.
One justification for not making an ex ante–ex post distinction is that the agents being
modeled operate under perfect foresight even though econometricians do not observe all
of the information available to the agents. In this framework, the U (s), s = 1, . . . , S̄, are
an ingredient of the econometric model that accounts for the asymmetry of information
between the agent and the econometrician studying the agent.
Without imposing assumptions about the functional structure of the outcome equations, it is not possible to nonparametrically identify counterfactual outcome states
Y (s, X, U (s)) that have never been observed. Thus, in a schooling example, HN assume
that analysts observe life cycle outcomes for some persons for each stopping time (level
of final grade completion) and our notation reflects this.108 However, analysts do not
observe Y (s, X, U (s)) for all s for anyone. A person can have only one stopping time
(one completed schooling level). This observational limitation creates the “fundamental
problem of causal inference”.109
In addition to this problem, there is the standard selection problem that the
Y (s, X, U (s)) are only observed for persons who stop at s and not for a random sample of the population. The selected distribution may not accurately characterize the
population distribution of Y (s, X, U (s)) for persons selected at random. Note also that
without further structure, we can only identify treatment responses within a given policy environment. In another policy environment, where the rules governing selection
into treatment and/or the outcomes from treatment may be different, the same time to
treatment may be associated with entirely different responses.110 We now turn to the
HN analysis of identification of outcome and treatment time distributions.
3.4.1.4. Identification of outcome and treatment time distributions We assume access
to a large i.i.d. sample from the distribution of (S, Y (S, X, U (S)), X, Z), where S is the
stopping time, X are the variables determining outcomes and Z are the variables determining choices. We also know Pr(S = s | Z = z) for s = 1, . . . , S̄, from the data. For
expositional convenience, we first consider the case of scalar outcomes Y (S, X, U (S)).
An analysis for vector Y (S, X, U (S)) is presented in HN and is discussed below.
Consider the analysis of continuous outcomes. HN analyze more general cases. Their
results extend the analyses of Heckman and Honoré (1990), Heckman (1990) and
Carneiro, Hansen and Heckman (2003) by considering choices generated by a stopping

108 In practice, analysts can only observe a portion of the life cycle after treatment. See the discussion on

pooling data across samples in Cunha, Heckman and Navarro (2005) to replace missing life cycle data.
109 See Holland (1986) or Gill and Robins (2001).
110 This is the problem of general equilibrium effects, and leads to violation of the policy invariance conditions. See Heckman, Lochner and Taber (1998a), Heckman, LaLonde and Smith (1999) or Abbring and Van
den Berg (2003b) for discussion of this problem.

5254

J.H. Abbring and J.J. Heckman

time model. To simplify the notation in this section, assume that the scalar outcome associated with stopping at time s can be written as Y (s) = μ(s, X)+U (s), where Y (s) is
shorthand for Y (s, X, U (s)). Y (s) is observed only if D(s) = 1 where the D(s) are generated by the model analyzed in Theorem 3. Write I (s) = Ψ (s, Z(s)) − η(s). Assume
that the Ψ (s, Z(s)) belong to the Matzkin class of functions described in Appendix B.
We use the condensed representations I , Ψ (Z), η, Y , μ(X) and U as described in Section 2.8.1, and in the previous subsection.
Heckman and Navarro permit general stochastic dependence within the components
of U , within the components of η and across the two vectors. They assume that (X, Z)
are independent of (U, η). Each component of (U, η) has a zero mean. The joint distribution of (U, η) is assumed to be absolutely continuous.
With “sufficient variation” in the components of Ψ (Z), one can identify μ(s, X),
[Ψ (1, Z(1)), . . . , Ψ (s, Z(s))] and the joint distribution of U (s) and ηs . This enables
the analyst to identify average treatment effects across all stopping times, since one can
extract E(Y (s) − Y (s  ) | X = x) from the marginal distributions of Y (s), s = 1, . . . , S̄.
T HEOREM 4. Write Ψ s (Z) = (Ψ (1, Z(1), . . . , Ψ (s, Z(s))). Assume in addition to the
conditions in Theorem 3 that
(i) E[U (s)] = 0. (U (s), ηs ) are continuous random variables with support
Supp(U (s)) × Supp(ηs ) with upper and lower limits (U (s), η̄s ) and (U (s), ηs ),
respectively, s = 1, . . . , S̄. These conditions hold for each component of each
subvector. The joint system is thus variation free for each component with respect to every other component.
(ii) (U (s), ηs ) ⊥
⊥ (X, Z), s = 1, . . . , S̄ (independence).
(iii) μ(s, X) is a continuous function, s = 1, . . . , S̄.
(iv) Supp(Ψ (Z), X) = Supp(Ψ (Z)) × Supp(X).
Then one can identify μ(s, X), Ψ s (Z) Fηs ,U (s) , s = 1, . . . , S̄, where Ψ (Z) is identified
over the support admitted by condition (ii) of Theorem 3.
P ROOF. See Appendix C.



Appendix D, which extends Heckman and Navarro (2007), states and proves the more
general Theorem D.1 for vector outcomes and both discrete and continuous variables
that is parallel to the proof of Theorem 2 for the static model.
Theorem 4 does not identify the joint distribution of Y (1), . . . , Y (S̄) because analysts
observe only one of these outcomes for any person. Observe that exclusion restrictions
in the arguments of the choice of treatment equation are not required to identify the
counterfactuals. What is required is independent variation of arguments which might be
achieved by exclusion conditions but can be obtained by other functional restrictions
(see HN, Corollary 1, for example). One can identify the μ(s, X) (up to constants)
without the limit set argument. Thus one can identify certain features of the model
without using the limit set argument. See HN.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5255

The proof of Theorem 4 in Appendix C covers the case of vector Y (s, X, U (s)) where
each component is a continuous random variable. The analysis in Appendix D allows
for age-specific outcomes Y (t, s, X, U (t, s)), t = 1, . . . , T̄ , where Y can be a vector
of outcomes. In particular, HN can identify age-specific earnings flows associated with
multiple sources of income.
As a by-product of Theorem 4, one can construct various counterfactual distributions
of Y (s) for agents with index crossing histories such that D(s) = 0 (that is, for whom
Y (s) is not observed). Define B(s) = 1[I (s)  0], B s = (B(1), . . . , B(s)), and let bs
denote a vector of possible values of B s . D(s) was defined as B(s) if B s−1 = (0) and
0 otherwise. Theorem 4 gives conditions under which the counterfactual distribution
of Y (s) for those with D(s  ) = 1, s  = s, can be constructed. More generally, it can be
used to construct




Pr Y (s)  y(s) | B s = bs , X = x, Z = z








for all of the 2s possible sequences bs of B s outcomes up to s   s. If bs equals a


sequence of s  − 1 zeros followed by a one, then B s = bs corresponds to D(s  ) = 1.


The event B s = (0) corresponds to D s = (0), i.e., S > s  . For all other sequences



bs , B s = bs defines a subpopulation of the agents with D(s  ) = 1 for some s  < s 

and multiple index crossings. For example, B s = (0, 1, 0) corresponds to D(2) = 1
and I (3) < 0. This defines a subpopulation that takes treatment at time 2, but that
would not take treatment at time 3 if it would not have taken treatment at time 2.111
It is tempting to interpret such sequences with multiple crossings as corresponding to
multiple entry into and exit from treatment. However, this is inconsistent with the stopping time model (3.11), and would require extension of the model to deal with recurrent
treatment. Whether a threshold-crossing model corresponds to a structural model of
treatment choice is yet another issue, which is taken up in the next section and is also
addressed in Cunha, Heckman and Navarro (2007).
The counterfactuals that are identified by fixing different D(s  ) = 1 for different
treatment times s  in the general model of HN have an asymmetric aspect. HN can
generate Y (s) distributions for persons who are treated at s or before. Without further
structure, they cannot generate the distributions of these random variables for people
who receive treatment at times after s.
The source of this asymmetry is the generality of duration model (3.11). At each
stopping time s, HN acquire a new random variable η(s) which can have arbitrary dependence with Y (s) and Y (s  ) for all s and s  . From Theorem 4, HN can identify the
dependence between η(s  ) and Y (s) if s   s. They cannot identify the dependence
between η(s  ) and Y (s) for s  > s without imposing further structure on the unobservables.112 Thus one can identify the distribution of college outcomes for high school
graduates who do not go on to college and can compare these to outcomes for high
111 Cunha, Heckman and Navarro (2007) develop an ordered choice model with stochastic thresholds.
112 One possible structure is a factor model which is applied to this problem in the next section.

5256

J.H. Abbring and J.J. Heckman

school graduates, so they can identify the parameter “treatment on the untreated.” However, one cannot identify the distribution of high school outcomes for college graduates
(and hence treatment on the treated parameters) without imposing further structure.113
Since one can identify the marginal distributions under the conditions of Theorem 4,
one can identify pairwise average treatment effects for all s, s  .
It is interesting to contrast the model identified by Theorem 4 with a conventional static multinomial discrete-choice model with an associated system of counterfactuals, as
presented in Appendix B of Chapter 70 and analyzed in Section 2 of this chapter. Using
standard tools, it is possible to establish semiparametric identification of the conventional static model of discrete choice joined with counterfactuals and to identify all of
the standard mean counterfactuals. For that model there is a fixed set of unobservables
governing all choices of states. Thus the analyst does not acquire new unobservables
associated with each stopping time as occurs in a dynamic model. Selection effects for
Y (s) depend on the unobservables up to s but not later innovations. Selection effects
in a static discrete-choice model depend on a fixed set of unobservables for all outcomes. With suitable normalizations, HN identify the joint distributions of choices and
associated outcomes without the difficulties, just noted, that appear in the reduced form
dynamic model. HN develop models for discrete outcomes including duration models.
3.4.1.5. Using factor models to identify joint distributions of counterfactuals From
Theorem 4 and its generalizations reported in HN, one can identify joint distributions
of outcomes for each treatment time s and the index generating treatment times. One
cannot identify the joint distributions of outcomes across treatment times. Moreover, as
just discussed, one cannot, in general, identify treatment on the treated parameters.
As reviewed in Section 2, Aakvik, Heckman and Vytlacil (2005) and Carneiro,
Hansen and Heckman (2003) show how to use factor models to identify the joint distributions across treatment times and recover the standard treatment parameters. HN use
their approach to identify the joint distribution of Y = (Y (1), . . . , Y (S̄)).
The basic idea underlying this approach is to use joint distributions for outcomes
measured at each treatment time s along with the choice index to construct the joint
distribution of outcomes across treatment choices. To illustrate how to implement this
intuition, suppose that we augment Theorem 4 by appealing to Theorem 2 in Carneiro,
Hansen and Heckman (2003) or the extension of Theorem 4 proved in Appendix D to
identify the joint distribution of the vector of outcomes at each stopping time along with
I s = (I (1), . . . , I (s)) for each s. For each s, we may write


Y t, s, X, U (t, s) = μ(t, s, X) + U (t, s), t = 1, . . . , T̄ ,


I (s) = Ψ s, Z(s) − η(s).
113 In the schooling example, one can identify treatment on the treated for the final category S̄ since D S̄−1 =

(0) implies D(S̄) = 1. Thus at stage S̄ − 1, one can identify the distribution of Y (S̄ − 1) for persons for whom
D(0) = 0, . . . , D(S̄ − 1) = 0, D(S̄) = 1. Hence, if college is the terminal state, and high school the state
preceding college, one can identify the distribution of high school outcomes for college graduates.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5257

The scale of Ψ (s, Z(s)) is determined from the Matzkin (1994) conditions presented
in Appendix B. If we specify the Matzkin functions only up to scale, we determine the
functions up to scale and make a normalization. From Theorem 4, we can identify the
joint distribution of (η(1), . . . , η(s), U (1, s), . . . , U (T̄ , s)).
To review these concepts and their application to the model discussed in this section,
suppose that we adopt a one-factor model where θ is the factor. It has mean zero. The
errors can be represented by
η(s) = ϕs θ + εη(s) ,
U (t, s) = αt,s θ + εt,s ,

t = 1, . . . , T̄ , s = 1, . . . , S̄.

The θ are independent of all of the εη(s) , εt,s and the ε’s are mutually independent
mean zero disturbances. The ϕs and αt,s are factor loadings. Since θ is an unobservable,
its scale is unknown. One can set the scale of θ by normalizing one-factor loading,
say αT̄ ,S̄ = 1. From the joint distribution of (ηs , U (s)), one can identify σθ2 , αt,s , ϕs ,
t = 1, . . . , T̄ , for s = 1, . . . , S̄, using the same argument as presented in Section 2.8.
A sufficient condition is T̄  3, but this ignores possible additional information from
cross-system restrictions. From this information, one can form for t = t  or s = s  or
both,


Cov U (t, s), U (t  , s  ) = αt,s αt  ,s  σθ2 ,
even though the analyst does not observe outcomes for the same person at two different stopping times. In fact, one can construct the joint distribution of (U, η) =
(U (1), . . . , U (S̄), η). From this joint distribution, one can recover the standard mean
treatment effects as well as the joint distributions of the potential outcomes. One can
determine the percentage of participants at treatment time s who benefit from participation compared to what their outcomes would be at other treatment times. One can
perform a parallel analysis for models for discrete outcomes and durations. The analysis can be generalized to multiple factors in precisely the same way as described in
Section 2.8. Conventional factor analysis assumes that the unobservables are normally
distributed. Carneiro, Hansen and Heckman (2003) establish nonparametric identifiability of the θ’s and the ε’s and their analysis of nonparametric identifiability applies
here.
Theorem 4, strictly applied, actually produces only one scalar outcome along with
one or more choices for each stopping time, although the proof of the extended Theorem 4 in Appendix D is for a vector-outcome model with both discrete and continuous
outcomes.114 If vector outcomes are not available, access to a measurement system M
that assumes the same values for each stopping time can substitute for the need for vector outcomes for Y . Let Mj be the j th component of this measurement system. Write
Mj = μj,M (X) + Uj,M ,

j = 1, . . . , J,

where Uj,M are mean zero and independent of X.
114 HN analyze the vector-outcome case.

5258

J.H. Abbring and J.J. Heckman

Suppose that the Uj,M have a one-factor structure so Uj,M = αj,M θ + εj,M ,
j = 1, . . . , J , where the εj,M are mean zero, mutually independent random variables,
independent of the θ . Adjoining these measurements to the one outcome measure Y (s)
can substitute for the measurements of Y (t, s) used in the previous example. In an analysis of schooling, the Mj can be test scores that depend on ability θ. Ability is assumed
to affect outcomes Y (s) and the choice of treatment times indices.
To extend a point made in Section 2 to the framework for dynamic treatment
effects, the factor models implement a matching on unobservables assumption,
{Y (s)}S̄s=1 ⊥
⊥ S | X, Z, θ . HN allow for the θ to be unobserved variables and present
conditions under which their distributions can be identified.
3.4.1.6. Summary of the reduced form model A limitation of the reduced form approach pursued in this section is that, because the underlying model of choice is not
clearly specified, it is not possible without further structure to form, or even define, the
marginal treatment effect analyzed in Heckman and Vytlacil (1999, 2001, 2005, and
Chapters 70 and 71 in this Handbook) or Heckman, Urzua and Vytlacil (2006). The absence of well defined choice equations is problematic for the models analyzed thus far
in this section of our chapter, although it is typical of many statistical treatment effect
analyses.115 In this framework, it is not possible to distinguish objective outcomes from
subjective evaluations of outcomes, and to distinguish ex ante from ex post outcomes.
Another limitation of this analysis is its strong reliance on large support conditions on
the regressors coupled with independence assumptions. Independence can be relaxed
following Lewbel (2000) and Honoré and Lewbel (2002). The large support assumption
plays a fundamental role here and throughout the entire evaluation literature.
HN develop an explicit economic model for dynamic treatment effects that allows
analysts to make these and other distinctions. They extend the analysis presented in this
subsection to a more precisely formulated economic model. They explicitly allow for
agent updating of information sets. A well posed economic model enables economists to
evaluate policies in one environment and accurately project them to new environments
as well as to accurately forecast new policies never previously experienced. We now
turn to an analysis of a more fully articulated structural econometric model.
3.4.2. A sequential structural model with option values
This section analyzes the identifiability of a structural sequential optimal stopping time
model. HN use ingredients assembled in the previous sections to build an economically interpretable framework for analyzing dynamic treatment effects. For specificity,
Heckman and Navarro focus on a schooling model with associated earnings outcomes

115 Heckman (2005) and the analysis of Chapters 70 and 71 point out that one distinctive feature of the economic approach to program evaluation is the use of choice theory to define parameters and evaluate alternative
estimators.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5259

that is motivated by the research of Keane and Wolpin (1997) and Eckstein and Wolpin
(1999). They explicitly model costs and build a dynamic version of a Roy model. We
briefly survey the literature on dynamic discrete choice in Section 3.4.5 below.
In the model of this section, it is possible to interpret the literature on dynamic treatment effects within the context of an economic model; to allow for earnings while in
school as well as grade-specific tuition costs; to separately identify returns and costs;
to distinguish private evaluations from “objective” ex ante and ex post outcomes and to
identify persons at various margins of choice. In the context of medical economics, HN
consider how to identify the pain and suffering associated with a treatment as well as the
distribution of benefits from the intervention. They also model how anticipations about
potential future outcomes associated with various choices evolve over the life cycle as
sequential treatment choices are made.
In contrast to the analysis of Section 3.4.1, the identification proof for their dynamic
choice model works in reverse starting from the last period and sequentially proceeding
backward. This approach is required by the forward-looking nature of dynamic choice
analysis and makes an interesting contrast with the analysis of identification for the
reduced form models which proceeds forward from initial period values.
HN use limit set arguments to identify the parameters of outcome and measurement
systems for each stopping time s = 1, . . . , S̄, including means and joint distributions
of unobservables. These systems are identified without invoking any special assumptions about the structure of model unobservables. When they invoke factor structure
assumptions for the unobservables, they identify the factor loadings associated with
the measurements (as defined in Section 3.4.1) and outcomes. They also nonparametrically identify the distributions of the factors and the distributions of the innovations
to the factors. With the joint distributions of outcomes and measurements in hand for
each treatment time, HN can identify cost (and preference) information from choice
equations that depend on outcomes and costs (preferences). HN can also identify joint
distributions of outcomes across stopping times. Thus they can identify the proportion
of people who benefit from treatment. Their analysis generalizes the one shot decision models of Cunha and Heckman (2007b, 2008), Cunha, Heckman and Navarro
(2005, 2006) to a sequential setting.
All agents start with one year of schooling at age 1 and then sequentially choose, at
each subsequent age, whether to continue for another year in school. New information
arrives at each age. One of the benefits of staying in school is the arrival of new information about returns. Each year of schooling takes one year of age to complete. There is
no grade repetition. Once persons leave school, they never return.116 As a consequence,
an agent’s schooling level equals her age up to the time S  S̄ she leaves school. After that, ageing continues up to age T̄  S̄, but schooling does not. We again denote

116 It would be better to derive such stopping behavior as a feature of a more general model with possible recurrence of states. Cunha, Heckman and Navarro (2007) develop general conditions under which it is optimal
to stop and never return.

5260

J.H. Abbring and J.J. Heckman

Figure 13. Evolution of grades and age.

D(s) = 1(S = s) for all s ∈ {1, . . . , S̄}. Let δ(t) = 1 if a person has left school at or
before age t; δ(t) = 0 if a person is still in school. Figure 13 shows the evolution of age
and grades, and clarifies the notation used in this section.
A person’s earnings at age t depend on her current schooling level s and whether she
has left school on or before age t (δ(t) = 1) or not (δ(t) = 0). Thus,






Y t, s, δ(t), X = μ t, s, δ(t), X + U t, s, δ(t) .

(3.12)

Note that Y (t, s, 0, X) is only meaningfully defined if s = t, in which case it denotes
the earnings of a person as a student at age and schooling level s. More precisely,
Y (s, s, 0, X) denotes the earnings of an individual with characteristics X who is still
enrolled in school at age and schooling level s and goes on to complete at least s + 1
years of schooling. The fact that earnings in school depend only on the current schooling
level, and not on the final schooling level obtained, reflects the no-anticipation condition
(NA). U (t, s, δ(t)) is a mean zero shock that is unobserved by the econometrician but
may, or may not, be observed by the agent. Y (t, s, 1, X) is meaningfully defined only if
s  t, in which case it denotes the earnings at age t of an agent who has decided to stop
schooling at s.
The direct cost of remaining enrolled in school at age and schooling level s is




C s, X, Z(s) = Φ s, X, Z(s) + W (s)
where X and Z(s) are vectors of observed characteristics (from the point of view of the
econometrician) that affect costs at schooling level s, and W (s) are mean zero shocks
that are unobserved by the econometrician that may or may not be observed by the
agent. Costs are paid in the period before schooling is undertaken. The agent is assumed

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5261

to know the costs of making schooling decisions at each transition. The agent is also
assumed to know the X and Z = (Z(1), . . . , Z(S̄ − 1)) from age 1.117
The optimal schooling decision involves comparisons of the value of continuing in
school for another year and the value of leaving school forever at each age and schooling
level s ∈ {1, . . . , S̄ − 1}. We can solve for these values, and the optimal schooling
decision, by backward recursion.
The agent’s expected reward of stopping schooling forever at level and age s (i.e.,
receiving treatment s) is given by the expected present value of her remaining lifetime
earnings:
 T̄ −s
 
j


1
Y (s + j, s, 1, X)  Is ,
R(s, Is ) = E
(3.13)
1+r
j =0

where Is are the state variables generating the age-s-specific information set Is .118 They
include the schooling level attained at age s, the covariates X and Z, as well as all other
variables known to the agent and used in forecasting future variables. Assume a fixed,
nonstochastic, interest rate r.119 The continuation value at age and schooling level s
given information Is is denoted by K(s, Is ).
At S̄ −1, when an individual decides whether to stop or continue on to S̄, the expected
reward from remaining enrolled and continuing to S̄ (i.e., the continuation value) is the
earnings while in school less costs plus the expected discounted future return that arises
from completing S̄ years of schooling:


K(S̄ − 1, IS̄−1 ) = Y (S̄ − 1, S̄ − 1, 0, X) − C S̄ − 1, X, Z(S̄ − 1)


1
+
E R(S̄, IS̄ ) | IS̄−1
1+r
where C(S̄ − 1, X, Z(S̄ − 1)) is the direct cost of schooling for the transition to S̄. This
expression embodies the assumption that each year of school takes one year of age. IS̄−1
incorporates all of the information known to the agent.
The value of being in school just before deciding on continuation at age and schooling
level S̄ − 1 is the larger of the two expected rewards that arise from stopping at S̄ − 1
or continuing one more period to S̄:


V (S̄ − 1, IS̄−1 ) = max R(S̄ − 1, IS̄−1 ), K(S̄ − 1, IS̄−1 ) .
More generally, at age and schooling level s this value is

117 These assumptions can be relaxed and are made for convenience. See Carneiro, Hansen and Heckman
(2003), Cunha, Heckman and Navarro (2005) and Cunha and Heckman (2007b) for a discussion of selecting
variables in the agent’s information set.
118 We only consider the agent’s information set here, and drop the subscript A for notational convenience.
119 This assumption is relaxed in HN who present conditions under which r can be identified.

5262

J.H. Abbring and J.J. Heckman



V (s, Is ) = max R(s, Is ), K(s, Is )

#
Y (s, s, 0, X) − C(s, X, Z(s))
= max R(s, Is ),
.120
1
E(V (s + 1, Is+1 ) | Is )
+ 1+r
Following the exposition of the reduced form decision rule in Section 3.4.1, define the
decision rule in terms of a first passage of the “index” R(s, Is ) − K(s, Is ),

D(s) = 1 R(s, Is ) − K(s, Is )  0, R(s − 1, Is−1 ) − K(s − 1, Is−1 ) < 0, . . . ,

R(1, I1 ) − K(1, I1 ) < 0 .
An individual stops at the schooling level at the first age where this index becomes positive. From data on stopping times, one can nonparametrically identify the conditional
probability of stopping at s,


R(s, I ) − K(s, I )  0,

s
s

Pr(S = s | X, Z) = Pr R(s − 1, Is−1 ) − K(s − 1, Is−1 ) < 0, . . . ,  X, Z .

R(1, I1 ) − K(1, I1 ) < 0
HN use factor structure models based on the θ introduced in Section 3.4.1 to define
the information updating structure. Agents learn about different components of θ as they
evolve through life. The HN assumptions allow for the possibility that agents may know
some or all the elements of θ at a given age t regardless of whether or not they determine
earnings at or before age t. Once known, they are not forgotten. As agents accumulate
information, they revise their forecasts of their future earnings prospects at subsequent
stages of the decision process. This affects their decision rules and subsequent choices.
Thus HN allow for learning which can affect both pre-treatment outcomes and posttreatment outcomes.121,122 All dynamic discrete choice models make some assumptions
120 This model allows no recall and is clearly a simplification of a more general model of schooling with

option values. Instead of imposing the requirement that once a student drops out the student never returns, it
would be useful to derive this property as a feature of the economic environment and the characteristics of
individuals. Cunha, Heckman and Navarro (2007) develop such conditions. In a more general model, different
persons could drop out and return to school at different times as information sets are revised. This would create
further option value beyond the option value developed in the text that arises from the possibility that persons
who attain a given schooling level can attend the next schooling level in any future period. Implicit in this
analysis of option values is the additional assumption that persons must work at the highest level of education
for which they are trained. An alternative model allows individuals to work each period at the highest wage
across all levels of schooling that they have attained. Such a model may be too extreme because it ignores the
costs of switching jobs, especially at the higher educational levels where there may be a lot of job-specific
human capital for each schooling level. A model with these additional features is presented in Heckman,
Urzua and Yates (2007).
121 This type of learning about unobservables can be captured by HN’s reduced form model, but not by
Abbring and Van den Berg’s (2003b) single-spell mixed proportional hazards model. Their model does not
allow for time-varying unobservables. Abbring and Van den Berg develop a multiple-spell model that allows
for time-varying unobservables. Moreover, their nonparametric discussion of (NA) and randomization does
not exclude the sequential revelation to the agent of a general finite number of unobserved factors although
they do not systematically develop such a model.
122 It is fruitful to distinguish models with exogenous arrival of information (so that information arrives at
each age t independent of any actions taken by the agent) from information that arrives as a result of choices

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5263

about the updating of information and any rigorous identification analysis of this class
of models must test among competing specifications of information updating.
Variables unknown to the agent are integrated out by the agent in forming expectations over future outcomes. Variables known to the agent are treated as constants by
the agents. They are integrated out by the econometrician to control for heterogeneity.
These are separate operations except for special cases. In general, the econometrician
knows less than what the agent knows. The econometrician seeks to identify the distributions of the variables in the agent information sets that are used by the agents to form
their expectations as well as the distributions of variables known to the agent and treated
as certain quantities by the agent but not known by the econometrician. Determining
which elements belong in the agent’s information set can be done using the methods
exposited in Cunha, Heckman and Navarro (2005) and Cunha and Heckman (2007b)
who consider testing what components of X, Z, ε as well as θ are in the agent’s information set (see Section 2). We briefly discuss this issue at the end of the next section.123
HN establish semiparametric identification of the model assuming a given information
structure. Determining the appropriate information structure facing the agent and its
evolution is an essential aspect of identifying any dynamic discrete-choice model.
Observe that agents with the same information variables It at age t have the same
expectations of future returns, and the same continuation and stopping values. They
make the same investment choices. Persons with the same ex ante reward, state and
preference variables have the same ex ante distributions of stopping times. Ex post,
stopping times may differ among agents with identical ex ante information. Controlling
for It , future realizations of stopping times do not affect past rewards. This rules out
the problem that the future can cause the past, which may happen in HN’s reduced
form model. It enforces the (NA) condition of Abbring and Van den Berg. Failure to
accurately model It produces failure of (NA).
HN establish semiparametric identification of their model without period-by-period
exclusion restrictions. Their analysis extends Theorems 3 and 4 to an explicit choicetheoretic setting. They use limit set arguments to identify the joint distributions of
earnings (for each treatment time s across t) and any associated measurements that
do not depend on the stopping time chosen. For each stopping time, they construct the
means of earnings outcomes at each age and of the measurements and the joint distributions of the unobservables for earnings and measurements. Factor analyzing the joint
distributions of the unobservables, under conditions specified in Carneiro, Hansen and
Heckman (2003), they identify the factor loadings, and nonparametrically identify the
distributions of the factors and the independent components of the error terms in the
earnings and measurement equations. Armed with this knowledge, they use choice data
by the agent. The HN model is in the first class. The models of Miller (1984) or Pakes (1986) are in the second
class. See our discussion in Section 3.4.5.
123 The HN model of learning is clearly very barebones. Information arrives exogenously across ages. In the
factor model, all agents who advance to a stage get information about additional factors at that stage of their
life cycles but the realizations of the factors may differ across persons.

5264

J.H. Abbring and J.J. Heckman

to identify the distribution of the components of the cost functions that are not directly
observed. They construct the joint distributions of outcomes across stopping times. They
also present conditions under which the interest rate r is identified.
In their model, analysts can distinguish period by period ex ante expected returns
from ex post realizations by applying the analysis of Cunha, Heckman and Navarro
(2005). See the survey in Heckman, Lochner and Todd (2006) for a discussion of this
approach or recall our analysis in Section 2. Because they link choices to outcomes
through the factor structure assumption, they can also distinguish ex ante preference
or cost parameters from their ex post realizations. Ex ante, agents may not know some
components of θ. Ex post, they do. All of the information about future rewards and
returns is embodied in the information set It . Unless the time of treatment is known
with perfect certainty, it cannot cause outcomes prior to its realization.
The analysis of HN is predicated on specification of the agent’s information sets.
This information set should be carefully distinguished from that of the econometrician.
Cunha, Heckman and Navarro (2005) present methods for determining which components of future outcomes are in the information sets of agents at each age, It . If there
are components unknown to the agent at age t, under rational expectations, agents form
their value functions used to make schooling choices by integrating out the unknown
components using the distributions of the variables in their information sets. Components that are known to the agent are treated as constants by the individual in forming
the value function but as unknown variables by the econometrician and their distribution
is estimated. The true information set of the agent is determined from the set of possible
specifications of the information sets of agents by picking the specification that best fits
the data on choices and outcomes penalizing for parameter estimation. If neither the
agent nor the econometrician knows a variable, the econometrician identifies the determinants of the distribution of the unknown variables that is used by the agent to form
expectations. If the agent knows some variables, but the econometrician does not, the
econometrician seeks to identify the distribution of the variables, but the agent treats the
variables as known constants.
HN can identify all of the treatment parameters including pairwise ATE, the marginal
treatment effect (MTE) for each transition (obtained by finding mean outcomes for individuals indifferent between transitions), all of the treatment on the treated and treatment
on the untreated parameters and the population distribution of treatment effects by applying the analysis of Carneiro, Hansen and Heckman (2003) and Cunha, Heckman and
Navarro (2005) to this model. Their analysis can be generalized to cover the case where
there are vectors of contemporaneous outcome measures for different stopping times.
See HN for proofs and details.124

124 The same limitations regarding independence assumptions between the regressors and errors discussed
in the analysis of reduced forms apply to the structural model.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5265

Figure 14. Sample distribution of schooling attainment probabilities for males from the National Longitudinal
Survey of Youth. Source: Heckman, Stixrud and Urzua (2006).

3.4.3. Identification at infinity
Heckman and Navarro (2007), and many other researchers, rely on identification at infinity to obtain their main identification results. As noted in Chapter 71, identification at
infinity is required to identify the average treatment effect (ATE) using IV and control
function methods and in the reduced form discrete-time models developed in the previous subsections. While this approach is controversial, it is also testable. In any sample,
one can plot the distributions of the probability of each state (exit time) to determine if
the identification conditions are satisfied in any sample. Figure 14, taken from Heckman,
Stixrud and Urzua (2006), shows such plots for a six-state static schooling model that
they estimate. To identify the marginal outcome distributions for each state, the support
of the state probabilities should be the full unit interval. The identification at infinity
condition is clearly not satisfied in their data.125 Only the empirical distribution of the
state probability of graduating from a four year college comes even close to covering
the full unit interval. Thus, their empirical results rely on parametric assumptions, and
ATE and the marginal distributions of outcomes are nonparametrically nonidentified in
their data without invoking additional structure.
125 One can always argue that they are satisfied in an infinite sample that has not yet been realized. That
statement has no empirical content.

5266

J.H. Abbring and J.J. Heckman

3.4.4. Comparing reduced form and structural models
The reduced form model analyzed in Section 3.4.1 is typical of many reduced form
statistical approaches within which it is difficult to make important conceptual distinctions. Because agent choice equations are not modeled explicitly, it is hard to use such
frameworks to formally analyze the decision makers’ expectations, costs of treatment,
the arrival of information, the content of agent information sets and the consequences
of the arrival of information for decisions regarding time to treatment as well as outcomes. Key behavioral assumptions are buried in statistical assumptions. It is difficult
to distinguish ex post from ex ante valuations of outcomes in the reduced form models. Cunha, Heckman and Navarro (2005), Carneiro, Hansen and Heckman (2003) and
Cunha and Heckman (2007b, 2008) present analyses that distinguish ex ante anticipations from ex post realizations.126 In reduced form models, it is difficult to make the
distinction between private evaluations and preferences (e.g., “costs” as defined in this
section) from objective outcomes (the Y variables).
Statistical and reduced form econometric approaches to analyzing dynamic counterfactuals appeal to uncertainty to motivate the stochastic structure of models. They do
not explicitly characterize how agents respond to uncertainty or make treatment choices
based on the arrival of new information [see Robins (1989, 1997), Lok (2007), Gill
and Robins (2001), Abbring and Van den Berg (2003b), and Van der Laan and Robins
(2003)]. The structural approach surveyed in Section 3.4.2 and developed by HN allows
for a clear treatment of the arrival of information, agent expectations, and the effects of
new information on choice and its consequences. In an environment of imperfect certainty about the future, it rules out the possibility of the future causing the past once the
effects of agent information are controlled for.
The structural model developed by HN allows agents to learn about new factors
(components of θ ) as they proceed sequentially through their life cycles. It also allows
agents to learn about other components of the model [see Cunha, Heckman and Navarro
(2005)]. Agent anticipations of when they will stop and the consequences of alternative stopping times can be sequentially revised. Agent anticipated payoffs and stopping
times are sequentially revised as new information becomes available. The mechanism
by which agents revise their anticipations is modeled and identified. See Cunha, Heckman and Navarro (2005, 2006), Cunha and Heckman (2007b, 2008) and the discussion
in Section 2 for further discussion of these issues and Heckman, Lochner and Todd
(2006) for a partial survey of recent developments in the literature.
The clearest interpretation of the models in the statistical literature on dynamic treatment effects is as ex post selection-corrected analyses of distributions of events that
have occurred. In a model of perfect certainty, where ex post and ex ante choices and
outcomes are identical, the reduced form approach can be interpreted as approximating
clearly specified choice models. In a more general analysis with information arrival and

126 See the summary of this literature in Heckman, Lochner and Todd (2006).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5267

agent updating of information sets, the nature of the approximation is less clear cut.
Thus the current reduced form literature is unclear as to which agent decision-making
processes and information arrival assumptions justify the conditional sequential randomization assumptions widely used in the dynamic treatment effect literature [see, e.g.,
Gill and Robins (2001), Lechner and Miquel (2002), Lok (2007), Robins (1989, 1997),
Van der Laan and Robins (2003)]. Section 3.2.2 provides some insight by highlighting the connection to the conditional-independence assumption often employed in the
structural dynamic discrete-choice literature [see Rust (1987), and the survey in Rust
(1994)]. Reduced form approaches are not clear about the source of the unobservables
and their relationship with conditioning variables. It would be a valuable exercise to
exhibit which structural models are approximated by various reduced form models. In
the structural analysis, this specification emerges as part of the analysis, as our discussion of the stochastic properties of the unobservables presented in the preceding section
makes clear.
The HN analysis of both structural and reduced form models relies heavily on limit
set arguments. They solve the selection problem in limit sets. The dynamic matching
models of Gill and Robins (2001) and Lok (2007) solve the selection problem by invoking recursive conditional-independence assumptions. In the context of the models of
HN, they assume that the econometrician knows the θ or can eliminate the effect of θ on
estimates of the model by conditioning on a suitable set of variables. The HN analysis
entertains the possibility that analysts know substantially less than the agents they study.
It allows for some of the variables that would make matching valid to be unobservable.
As we have noted in early subsections, versions of recursive conditional-independence
assumptions are also used in the dynamic discrete-choice literature [see the survey in
Rust (1994)]. The HN factor models allow analysts to construct the joint distribution of
outcomes across stopping times. This feature is missing from the statistical treatment
effect literature.
Both HN’s structural and reduced form models of treatment choice are stopping time
models. Neither model allows for multiple entry into and exit from treatment, even
though agents in these models would like to reverse their treatment decisions for some
realizations of their index if this was not too costly (or, in the case of the reduced form
model, if the index thresholds for returning would not be too low).127 Cunha, Heckman
and Navarro (2007) derive conditions on structural stopping models from a more basic
model that entertains the possibility of return from dropout states but which nonetheless exhibits the stopping time property. The HN identification strategy relies on the
nonrecurrent nature of treatment. Their identification strategy of using limit sets can be
applied to a recurrent model provided that analysts confine attention to subsets of (X, Z)
such that in those subsets the probability of recurrence is zero.
127 Recall that treatment occurs if the index turns positive. If there are costs to reversing this decision, agents
would only reverse their decision if the index falls below some negative threshold. The stopping time assumption is equivalent to the assumption that the costs of reversal are prohibitively large, or that the corresponding
threshold is at the lower end of the support of the index.

5268

J.H. Abbring and J.J. Heckman

3.4.5. A short survey of dynamic discrete-choice models
Table 13 presents a brief summary of the models used to analyze dynamic discrete
choices. Rust (1994) presents a widely cited nonparametric nonidentification theorem
for dynamic discrete-choice models. It is important to note the restrictive nature of his
negative results. He analyzes a recurrent-state infinite-horizon model in a stationary
environment. He does not use any exclusion restrictions or cross outcome-choice restrictions. He uses a general utility function. He places no restrictions on period-specific
utility functions such as concavity or linearity nor does he specify restrictions connecting preferences and outcomes. One can break Rust’s nonidentification result with
additional information.
Magnac and Thesmar (2002) present an extended comment on Rust’s analysis including positive results for identification when the econometrician knows the distributions
of unobservables, assumes that unobservables enter period-specific utility functions in
an additively separable way and is willing to specify functional forms of utility functions or other ingredients of the model, as do Pakes (1986), Keane and Wolpin (1997),
Eckstein and Wolpin (1999), and Hotz and Miller (1988, 1993). Magnac and Thesmar
(2002) also consider the case where one state (choice) is absorbing [as do Hotz and
Miller (1993)] and where the value functions are known at the terminal age (T̄ ) [as do
Keane and Wolpin (1997) and Belzil and Hansen (2002)]. In HN, each treatment time is
an absorbing state. In a separate analysis, Magnac and Thesmar consider the case where
unobservables from the point of view of the econometrician are correlated over time (or
age t) and choices (s) under the assumption that the distribution of the unobservables is
known. They also consider the case where exclusion restrictions are available. Throughout their analysis, they maintain that the distribution of the unobservables is known both
by the agent and the econometrician.
HN provide a semiparametric identification of a finite-horizon finite-state model
with an absorbing state with semiparametric specifications of reward and cost functions.128 Given that rewards are in value units, the scale of their utility function is fixed.
Choices are not invariant to arbitrary affine transformations so that one source of nonidentifiability in Rust’s analysis is eliminated. They can identify the error distributions
nonparametrically given their factor structure. They do not have to assume either the
functional form of the unobservables or knowledge of the entire distribution of unobservables.
HN present a fully specified structural model of choices and outcomes motivated by,
but not identical to, the analyses of Keane and Wolpin (1994, 1997) and Eckstein and
Wolpin (1999). In their setups, outcome and cost functions are parametrically specified.
Their states are recurrent while those of HN are absorbing. In their model, once an
agent drops out of school, the agent does not return. In the Keane–Wolpin model, an

128 Although their main theorems are for additively separable reward and cost functions, it appears that
additive separability can be relaxed using the analysis of Matzkin (2003).

Ch. 72:

Use outcomes
Finite or
along with discrete infinite
choices?
horizon

Recurrent Stationary
states
environment

Temporal correlation
of unobserved shocks

Information updating

Nonparametric
or parametric
identification

Terminal Crossvalue
equation
assumed restrictions?1
to be
known

Flinn and Heckman
(1982)

Yes (wages)

Infinite

Yes

Yes

Temporal independence Arrival of independent
given heterogeneity
shocks

Nonparametric

No

Yes

Miller (1984)

Yes (wages)

Infinite

Yes

Yes

Bayesian normal
learning induces
dependence

Bayesian learning, arrival Parametric
of independent shocks

No

Yes

Pakes (1986)

No (use cost
data to identify
discrete choice)

Finite

No

No

AR-1 dependence on
unobservables

Arrival of independent
shocks

Yes

No

Wolpin (1984)

No

Finite

Yes

No

Wolpin (1987)

Yes

Finite

No

No

Temporal independence Temporal independence Parametric

Yes

No

Independent shocks

Arrival of independent
shocks

Parametric

No

Yes

Wolpin (1992)

Yes (wages)

Finite

Yes

No

Renewal process for
shocks; job-specific
shocks independent
across jobs

Arrival of independent
shocks (from new jobs)

Parametric

Yes

Yes

Rust (1987)

Yes3

Infinite

Yes

Yes

Shocks conditionally
Arrival of independent
independent given state shocks
variables

Parametric

No

No

Hotz and Miller (1993)

No

Infinite

Yes

Yes

Shocks conditionally
Synthetic cohort
independent given state assumption
variables

Parametric

Yes

No

Parametric2

Econometric Evaluation of Social Programs, Part III

Table 13
Comparisons among papers in the literature on dynamic discrete-choice models

(continued on next page)

5269

5270

Table 13
(continued)
Recurrent Stationary
states
environment

Temporal correlation
of unobserved shocks

Manski (1993)

No

Infinite

Yes

Yes

Keane and Wolpin
(1997)

Yes

Finite

Yes

Taber (2000)

No

Magnac and Thesmar
(2002)

Heckman and Navarro
(2007)

Nonparametric
or parametric
identification

Terminal Crossvalue
equation
assumed restrictions?1
to be
known

Shocks conditionally
Synthetic cohort
independent given state assumption
variables

Nonparametric

No

No

No

Shocks temporally
Shocks temporally
independent given initial independent
condition

Parametric

Yes

Yes

Finite
No
(2 periods)

No

General dependence

General dependence

Nonparametric

No

No

Yes3

Both finite Yes
and infinite

Yes

Conditional
independence given
state variables in main
case

Conditional dependence Conditional
nonparametric

No

No

Yes

Finite

No

General dependence
(updating)

Serially correlated
updating of states

No

Yes

No

1 Cross equation means restrictions used between outcome and choice equations.
2 Pakes and Simpson (1989) sketch a nonparametric proof of this model.
3 There is an associated state vector equation which can be interpreted as an outcome equation.

Information updating

Nonparametric

J.H. Abbring and J.J. Heckman

Use outcomes
Finite or
along with discrete infinite
choices?
horizon

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5271

agent who drops out can return. Keane and Wolpin do not establish identification of their
model whereas HN establish semiparametric identification of their model. They analyze
models with more general times series processes for unobservables. In both the HN and
Keane–Wolpin frameworks, agents learn about unobservables. In the Keane–Wolpin
framework, such learning is about temporally independent shocks that do not affect
agent expectations about returns relevant to possible future choices. The information
just affects the opportunity costs of current choices. In the HN framework, learning
affects agent expectations about future returns as well as opportunity costs.
The HN model extends previous work by Carneiro, Hansen and Heckman (2003)
and Cunha and Heckman (2007b, 2008), Cunha, Heckman and Navarro (2006, 2005)
by considering explicit multiperiod dynamic models with information updating. They
consider one-shot decision models with information updating and associated outcomes.
Their analysis is related to that of Taber (2000). Like Cameron and Heckman (1998),
both HN and Taber use identification-in-the-limit arguments.129 Taber considers identification of a two period model with a general utility function whereas in Section 3.4.2,
we discuss how HN consider identification of a specific form of the utility function (an
earnings function) for a multiperiod maximization problem. As in HN, Taber allows for
the sequential arrival of information. His analysis is based on conventional exclusion
restrictions, but the analysis of HN is not. They use outcome data in conjunction with
the discrete dynamic choice data to exploit cross-equation restrictions, whereas Taber
does not.
The HN treatment of serially correlated unobservables is more general than any discussion that appears in the current dynamic discrete choice and dynamic treatment
effect literature. They do not invoke the strong sequential conditional-independence assumptions used in the dynamic treatment effect literature in statistics [Gill and Robins
(2001), Lechner and Miquel (2002), Lok (2007), Robins (1989, 1997)], nor the closely
related conditional temporal independence of unobserved state variables given observed
state variables invoked by Rust (1987), Hotz and Miller (1988, 1993), Manski (1993)
and Magnac and Thesmar (2002) (in the first part of their paper) or the independence
assumptions invoked by Wolpin (1984).130 HN allow for more general time series dependence in the unobservables than is entertained by Pakes (1986), Keane and Wolpin
(1997) or Eckstein and Wolpin (1999).131
129 Pakes and Simpson (1989) sketch a proof of identification of a model of the option values of patents that
is based on limit sets for an option model.
130 Manski (1993) and Hotz and Miller (1993) use a synthetic cohort effect approach that assumes that young
agents will follow the transitions of contemporaneous older agents in making their life cycle decisions. The
synthetic cohort approach has been widely used in labor economics at least since Mincer (1974). Manski and
Hotz and Miller exclude any temporally dependent unobservables from their models. See Ghez and Becker
(1975), MaCurdy (1981) and Mincer (1974) for applications of the synthetic cohort approach. For empirical
evidence against the assumption that the earnings of older workers are a reliable guide to the earnings of
younger workers in models of earnings and schooling choices for recent cohorts of workers, see Heckman,
Lochner and Todd (2006).
131 Rust (1994) provides a clear statement of the stochastic assumptions underlying the dynamic discretechoice literature up to the date of his survey.

5272

J.H. Abbring and J.J. Heckman

Like Miller (1984) and Pakes (1986), HN explicitly model, identify and estimate
agent learning that affects expected future returns.132 Pakes and Miller assume functional forms for the distributions of the error process and for the serial correlation
pattern about information updating and time series dependence. The HN analysis of
the unobservables is nonparametric and they estimate, rather than impose, the stochastic structure of the information updating process.
Virtually all papers in the literature, including the HN analysis, invoke rational expectations. An exception is the analysis of Manski (1993) who replaces rational expectations with a synthetic cohort assumption that choices and outcomes of one group can
be observed (and acted on) by a younger group. This assumption is more plausible in
stationary environments and excludes any temporal dependence in unobservables. In
recent work, Manski (2004) advocates use of elicited expectations as an alternative to
the synthetic cohort approach.
While HN use rational expectations, they estimate, rather than impose the structure
of agent information sets. Miller (1984), Pakes (1986), Keane and Wolpin (1997), and
Eckstein and Wolpin (1999) assume that they know the law governing the evolution of
agent information up to unknown parameters.133 Following the procedure presented in
Cunha and Heckman (2007b, 2008), Cunha, Heckman and Navarro (2005, 2006) and
Navarro (2005), HN can test for which factors (θ ) appear in agent information sets at
different stages of the life cycle and they identify the distributions of the unobservables
nonparametrically.
The HN analysis of dynamic treatment effects is comparable, in some aspects, to the
recent continuous-time event-history approach of Abbring and Van den Berg (2003b)
previously analyzed. Those authors build a continuous time model of counterfactuals for outcomes that are durations. They model treatment assignment times using a
continuous-time duration model.
The HN analysis is in discrete time and builds on previous work by Heckman
(1981a, 1981c) on heterogeneity and state dependence that identifies the causal effect
of employment (or unemployment) on future employment (or unemployment).134 They
model time to treatment and associated vectors of outcome equations that may be discrete, continuous or mixed discrete-continuous. In a discrete-time setting, they are able
to generate a variety of distributions of counterfactuals and economically motivated parameters. They allow for heterogeneity in responses to treatment that has a general time
series structure.
As noted in Section 3.4.4, Abbring and Van den Berg (2003b) do not identify explicit agent information sets as HN do in their paper and as is done in Cunha, Heckman

132 As previously noted, the previous literature assumes learning only about current costs.
133 They specify a priori particular processes of information arrival as well as which components of the

unobservables agents know and act on, and which components they do not.
134 Heckman and Borjas (1980) investigate these issues in a continuous-time duration model. See also
Heckman and MaCurdy (1980).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5273

and Navarro (2005), and they do not model learning about future rewards. Their outcomes are restricted to be continuous-time durations. The HN framework is formulated
in discrete time, which facilitates the specification of richer unobserved and observed
covariate processes than those entertained in the continuous-time framework of Abbring
and Van den Berg (2003b). It is straightforward to attach a vector of treatment outcomes
in the HN model that includes continuous outcomes, discrete outcomes and durations
expressed as binary strings.135 At a practical level, the approach often can produce
very fine-grained descriptions of continuous-time phenomena by using models with
many finite periods. Clearly a synthesis of the event-history approach with the HN approach would be highly desirable. That would entail taking continuous-time limits of
the discrete-time models. It is a task that awaits completion.
Flinn and Heckman (1982) utilize information on stopping times and associated
wages to derive cross-equation restrictions to partially identify an equilibrium job search
model for a stationary economic environment where agents have an infinite horizon.
They establish that the model is nonparametrically nonidentified. Their analysis shows
that use of outcome data in conjunction with data on stopping times is not sufficient
to secure nonparametric identification of a dynamic discrete-choice model, even when
the reward function is linear in outcomes unlike the reward functions in Rust (1987)
and Magnac and Thesmar (2002). Parametric restrictions can break their nonidentification result. Abbring and Campbell (2005) exploit such restrictions, together with
cross-equation restrictions on stopping times and noisy outcome measures, to prove
identification of an infinite-horizon model of firm survival and growth with entrepreneurial learning. Alternatively, nonstationarity arising from finite horizons can break
their nonidentification result [see Wolpin (1987)]. The HN analysis exploits the finitehorizon backward-induction structure of our model in conjunction with outcome data to
secure identification and does not rely on arbitrary period by period exclusion restrictions. They substantially depart from the assumptions maintained in Rust’s nonidentification theorem (1994). They achieve identification by using cross-equation restrictions,
linearity of preferences and additional measurements, and exploiting the structure of
their finite-horizon nonrecurrent model. Nonstationarity of regressors greatly facilitates
identification by producing both exclusion and curvature restrictions which can substitute for standard exclusion restrictions.
3.5. Summary of the state of the art in analyzing dynamic treatment effects
This section has surveyed new methods for analyzing the dynamic effects of treatment. We have compared and contrasted the statistical dynamic treatment approach
based on sequential conditional-independence assumptions that generalize matching to
a dynamic panel setting to approaches developed in econometrics. We compared and

135 Abbring (2008) considers nonparametric identification of mixed semi-Markov event-history models that
extends his work with Van den Berg. See Section 3.3.

5274

J.H. Abbring and J.J. Heckman

contrasted a continuous-time event-history approach developed by Abbring and Van
den Berg (2003b) to discrete time reduced form and structural models developed by
Heckman and Navarro (2007), and Cunha, Heckman and Navarro (2005).

4. Accounting for general equilibrium, social interactions, and spillover effects
The treatment-control paradigm motivates the modern treatment effect literature. Outcomes of persons who are “treated” are compared to outcomes of those who are not.
The “untreated” are assumed to be completely unaffected by who else gets treatment.
This assumption is embodied in invariance assumptions (PI-2) and (PI-4) in Chapter 70.
In the “Rubin” model, (PI-2) is one component of his “SUTVA” assumption.136
In any social setting, this assumption is very strong, and many economists have built
models to account for various versions of social interactions and their consequences
for policy evaluation. The literature on general equilibrium policy analysis is vast and
the details of particular approaches are difficult to synthesize in a concise way. In this
section, we make a few general points and offer some examples where accounting for
general equilibrium effects has substantial consequences for the evaluation of public
policy. Note that there are also cases where accounting for general equilibrium has
little effect on policy evaluations. One cannot say that a full-fledged empirical general
equilibrium analysis is an essential component of every evaluation. However, ignoring
general equilibrium and social interactions can be perilous.
It is fruitful to distinguish interactions of agents through market mechanisms, captured by the literature on general equilibrium analysis, from social interactions. Social
interactions are a type of direct externality in which the actions of one agent directly
affect the actions (preferences, constraints, technology) of other agents.137 The former type of interaction is captured by general equilibrium models. The second type
of interaction is captured in the recent social interactions literature. Within the class of
equilibrium models where agents interact through markets, there is a full spectrum of
possible interactions from partial equilibrium models where agent interactions in some
markets are modeled, to full fledged general equilibrium models where all interactions
are modeled.
The social interactions literature is explicitly microeconomic in character, since it
focuses on the effects of individuals (or groups) on other individuals. The traditional
general equilibrium literature is macroeconomic in its focus and deals with aggregates.
A more recent version moves beyond the representative consumer paradigm and considers heterogeneity and the impact of policy on individuals. We first turn to versions of
the empirical general equilibrium literature.

136 Recall the discussion in Chapter 70, Section 4.4.
137 This distinction is captured in neoclassical general equilibrium models by the contrast between pecuniary

and nonpecuniary externalities.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5275

4.1. General equilibrium policy evaluation
There is a large literature on empirical general equilibrium models applied to trade, public finance, finance, macroeconomics, energy policy, industrial organization, and labor
economics. The essays in Kehoe, Srinivasan and Whalley (2005) present a rich collection of empirical general equilibrium models and references to a large body of related
work. Much of the traditional general equilibrium analysis analyzes representative models using aggregate data.
Lewis (1963) is an early study of the partial equilibrium spillover effects of unionism
on the wages of nonunion workers.138 Leading examples of empirical general equilibrium studies based on the representative consumer paradigm are Auerbach and Kotlikoff
(1987), Hansen and Sargent (1980), Huggett (1993), Jorgenson and Slesnick (1997),
Jorgenson and Yun (1990), Kehoe, Srinivasan and Whalley (2005), Krusell and Smith
(1998), Kydland and Prescott (1982), Shoven and Whalley (1977). There are many
other important studies and this list is intended to be illustrative, and not exhaustive.
Jorgenson and Slesnick (1997) give precise conditions for aggregation of microdata
into macro aggregates that can be used to identify clearly defined economic parameters
and policy criteria.
These models provide specific frameworks for analyzing policy interventions. Their
specificity is a source of controversy because so many components of the social system
need to be accounted for, and so often there is little professional consensus on these
components and their empirical importance. Being explicit has its virtues and stimulates
research promoting improved understanding of mechanisms and parameters. However,
rhetorically, this clarity can be counterproductive. By sweeping implicit assumptions
under the rug, the treatment effect literature appears to some to offer a universality and
generality that is absent from the general equilibrium approach, in which mechanisms
of causation and agent interaction are more clearly specified.
There is a large and often controversial literature about the sources of parameter
estimates for the representative agent models. The “calibration vs. estimation debate”
concerns the best way to secure parameters for these models [see Kydland and Prescott
(1996), Hansen and Heckman (1996), and Sims (1996)]. Dawkins, Srinivasan and Whalley (2001) present a useful guide to this literature. Browning, Hansen and Heckman
(1999) discuss the sources of the estimates for a variety of prototypical general equilibrium frameworks. In this section, we discuss the smaller body of literature that links
general equilibrium models to microdata to evaluate public policy.
4.2. General equilibrium approaches based on microdata
A recent example of general equilibrium analysis applied to policy problems is the study
of Heckman, Lochner and Taber (1998a, 1998b, 1998c), who consider the evaluation of
138 He does not consider the effect of unionism on product prices or other factor markets besides the labor
market.

5276

J.H. Abbring and J.J. Heckman

tuition subsidy programs in a general equilibrium model of human capital accumulation with both schooling and on the job training, and with heterogeneous skills in
which prices are flexible. We first discuss their model and then turn to other frameworks. Their model is an overlapping generations empirical general equilibrium model
with heterogeneous agents across and within generations which generalizes the analysis of Auerbach and Kotlikoff (1987) by introducing human capital and by synthesizing
micro- and macrodata analysis.
The standard microeconomic evaluation of tuition policy on schooling choices estimates the response of college enrollment to tuition variation using geographically
dispersed cross-sections of individuals facing different tuition rates. These estimates
are then used to determine how subsidies to tuition will raise college enrollment. The
impact of tuition policies on earnings are evaluated using a schooling–earnings relationship fit on pre-intervention data and do not account for the enrollment effects of the
taxes raised to finance the tuition subsidy. Kane (1994), Dynarski (2000), and Cameron
and Heckman (1998, 2001) exemplify this approach. This approach is neither partial
equilibrium or general equilibrium in character. It entirely ignores market interactions.
The danger in this widely used practice is that what is true for policies affecting a
small number of individuals, as studied by social experiments or as studied in the microeconomic “treatment effect” literature, may not be true for policies that affect the
economy at large. A national tuition-reduction policy may stimulate substantial college enrollment and will also likely reduce skill prices. However, agents who account
for these changes will not enroll in school at the levels calculated from conventional
procedures which ignore the impact of the induced enrollment on skill prices. As a result, standard policy evaluation practices are likely to be misleading about the effects
of tuition policy on schooling attainment and wage inequality. The empirical question is to determine the extent to which this is true. Heckman, Lochner and Taber
(1998a, 1998b, 1998c) show that conventional practices in the educational evaluation
literature lead to estimates of enrollment responses that are ten times larger than the
long-run general equilibrium effects, which account for the effect of policy on all factor
markets. They improve on current practice in the “treatment effects” literature by considering both the gross benefits of the program and the tax costs of financing the policy
as borne by different groups.
Evaluating the general equilibrium effects of a national tuition policy requires more
information than the tuition-enrollment parameter that is the centerpiece of the micro
policy analyses, which ignore any equilibrium effects. Policy proposals of all sorts
typically extrapolate well outside the range of known experience and ignore the effects of induced changes in skill quantities on skill prices. To improve on current
practice, Heckman, Lochner and Taber (1998a) use microdata to identify an empirically estimated rational expectations, perfect foresight overlapping-generations general
equilibrium framework for the pricing of heterogeneous skills and the adjustment of
capital. It is based on an empirically grounded theory of the supply of schooling and
post-school human capital, where different schooling levels represent different skills.
Individuals differ in their learning ability and in initial endowments of human capital.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5277

Household saving behavior generates the aggregate capital stock, and output is produced
by combining the stocks of different types of human capital with physical capital. Factor markets are competitive, and it is assumed that wages are set in flexible, competitive
markets. Their model explains the pattern of rising wage inequality experienced in the
United States in the past 30 years. They apply their framework to evaluate tuition policies that attempt to increase college enrollment.
They present two reasons why the “treatment effect” framework that ignores the general equilibrium effects of tuition policy is inadequate. First, the conventional treatment
parameters depend on who in the economy is “treated” and who is not. Second, these
parameters do not measure the full impact of the program. For example, increasing tuition subsidies may increase the earnings of uneducated individuals who do not take
advantage of the subsidy. They become more scarce after the policy is implemented.
The highly educated are taxed to pay for the subsidy, and depending on how taxes
are collected this may affect their investment behavior. In addition, more competitors
for educated workers enter the market as a result of the policy, and their earnings are
depressed. Conventional methods ignore the effect of the policy on nonparticipants operating through changes in equilibrium skill prices and on taxes. In order to account for
these effects, it is necessary to conduct a general equilibrium analysis.
The analysis of Heckman, Lochner and Taber (1998a, 1998b, 1998c) has important
implications for the widely-used difference-in-differences estimator. If the tuition subsidy changes the aggregate skill prices, the decisions of nonparticipants will be affected.
The “no treatment” benchmark group is affected by the policy and the difference-indifferences estimator does not identify the effect of the policy for anyone compared to
a no treatment state.
Using their estimated model, Heckman, Lochner and Taber (1998c) simulate the effects of a revenue-neutral $500 increase in college tuition subsidy on top of existing
programs that is financed by a proportional tax, on enrollment in college and wage inequality. They start from a baseline economy that describes the US in the mid-1980s
and that produces wage growth profiles and schooling enrollment and capital stock data
that match micro- and macroevidence. The microeconomic treatment effect literature
predicts an increase in college attendance of 5.3 percent. This analysis holds college
and high school wage rates fixed. This is the standard approach in the microeconomic
“treatment effect” literature.
When the policy is evaluated in a general equilibrium setting, the estimated effect
falls to 0.46 percent. Because the college–high school wage ratio falls as more individuals attend college, the returns to college are less than when the wage ratio is held
fixed. Rational agents understand this effect of the tuition policy on skill prices and adjust their college-going behavior accordingly. Policy analysis of the type offered in the
“treatment effect” literature ignores equilibrium price adjustment and the responses of
rational agents to the policies being evaluated. Their analysis shows substantial attenuation of the effects of tuition policy on capital and on the stocks of the different skills
in their model compared to a treatment effect model. They show that their results are
robust to a variety of specifications of the economic model.

5278

J.H. Abbring and J.J. Heckman

Table 14
Simulated effects of $5000 tuition subsidy on different groups. Steady state changes in present value of
lifetime wealth (in thousands of US dollars)
Group (proportion)1

High School–High School (0.528)
High School–College (0.025)
College–High School (0.003)
College–College (0.444)

After-tax
earnings using
base tax
(1)

After-tax
earnings

9.512
−4.231
−46.711
−7.654

Utility2

(2)

After-tax
earnings net of
tuition
(3)

(4)

−0.024
−13.446
57.139
−18.204

−0.024
1.529
−53.019
0.420

−0.024
1.411
−0.879
0.420

1 The groups correspond to each possible counterfactual. For example, the “High School–High School” group
consists of individuals who would not attend college in either steady state, and the “High School–College”
group would not attend college in the first steady state, but would in the second, etc.
2 Column (1) reports the after-tax present value of earnings in thousands of 1995 US dollars discounted using
the after-tax interest rate where the tax rate used for the second steady state is the base tax rate. Column (2)
adds the effect of taxes, column (3) adds the effect of tuition subsidies and column (4) includes the nonpecuniary costs of college in dollar terms.
Source: Heckman, Lochner and Taber (1998b).

They also analyze short run effects. When they simulate the model with rational expectations, the short-run college enrollment effects in response to the tuition policy
are also very small, as agents anticipate the effects of the policy on skill prices and
calculate that there is little gain from attending college at higher rates. Under myopic
expectations, the short-run enrollment effects are much closer to the estimated treatment effects. With learning on the part of agents, but not perfect foresight, there is still a
substantial gap between treatment and general equilibrium estimates. The sensitivity of
policy estimates to model specification is a source of concern and a stimulus to research.
The treatment effect literature ignores these issues.
Heckman, Lochner and Taber (1998a, 1998b, 1998c) also consider the impact of a
policy change on discounted earnings and utility and decompose the total effects into
benefits and costs, including tax costs for each group. Table 14 compares outcomes in
two steady states: (a) the benchmark steady state and (b) the steady state associated with
the new tuition policy.139 The row “High School–High School” reports the change in
a variety of outcome measures for those persons who would be in high school under
either the benchmark or new policy regime; the “High School–College” row reports the
change in the same measures for high school students in the benchmark state who are
induced to attend college by the new policy; the “College–High School” outcomes refer
139 Given that the estimated schooling response to a $500 subsidy is small, Heckman, Lochner and Taber
instead use a $5000 subsidy for the purpose of exploring general equilibrium effects on earnings. Current
college tuition subsidy levels are this high or higher at many colleges in the US.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5279

to those persons in college in the benchmark economy who only attend high school
after the policy; and so forth. Because agents choose sectors, there is spillover from one
sector to another.
By the measure of the present value of earnings, some of those induced to change
are worse off. Contrary to the monotonicity assumption built into the LATE parameter
discussed in Chapters 70 and 71, and defined in this context as the effect of the tuition
subsidy on the earnings of those induced by it to go to college, Heckman, Lochner and
Taber find that the tuition policy produces a two-way flow. Some people who would
have attended college in the benchmark regime no longer do so. The rest of society is
also affected by the policy—again, contrary to the implicit assumption built into LATE
that only those who change status are affected by the policy. People who would have
gone to college without the policy and continue to do so after the policy are financially
worse off for two reasons: (a) the price of their skill is depressed and (b) they must pay
higher taxes to finance the policy. However, they now receive a tuition subsidy and for
this reason, on net, they are better off both financially and in terms of utility. Those
who abstain from attending college in both steady states are worse off. They pay higher
taxes, and do not get the benefits of a college education. Those induced to attend college
by the policy are better off in terms of utility. Note that neither category of non-changers
is a natural benchmark for a difference-in-differences estimator. The movement in their
wages before and after the policy is due to the policy and cannot be attributed to a
benchmark “trend” that is independent of the policy.
Table 15 presents the impact of a $5000 tuition policy on the log earnings of individuals with ten years of work experience for different definitions of treatment effects.
The treatment effect version given in the first column holds skill prices constant at initial steady state values. The general equilibrium version given in the second column
allows prices to adjust when college enrollment varies. Consider four parameters initially defined in a partial equilibrium context. The average treatment effect is defined
for a randomly selected person in the population in the benchmark economy and asks
how that person would gain in wages by moving from high school to college. The parameter treatment on the treated is defined as the average gain over their non-college
alternative of those who attend college in the benchmark state. The parameter treatment on the untreated is defined as the average gain over their college wage received
by individuals who did not attend college in the benchmark state. The marginal treatment effect is defined for individuals who are indifferent between going to college or
not. This parameter is a limit version of the LATE parameter under the assumptions
presented in Chapter 71. Column (2) presents the general equilibrium version of treatment on the treated. It compares the earnings of college graduates in the benchmark
economy with what they would earn if no one went to college.140 The treatment on the
140 In the empirical general equilibrium model of Heckman, Lochner and Taber (1998a, 1998b, 1998c), the
Inada conditions for college and high school are not satisfied for the aggregate production function and the
marginal product of each skill group when none of it is utilized is a bounded number. If the Inada conditions
were satisfied, this counterfactual and the counterfactual treatment on the untreated would not be defined.

5280

J.H. Abbring and J.J. Heckman

Table 15
Treatment effect parameters: treatment effect and general equilibrium difference in log earnings, college graduates vs. high school graduates at 10 years of work experience
Parameter

Prices fixed1
(1)

Prices vary2
(2)

Fraction of sample3
(3)

Average treatment effect (ATE)
Treatment on treated (TT)
Treatment on untreated (TUT)
Marginal treatment effect (MTE)
LATE4 $5000 subsidy:
Partial equilibrium
GE (H.S. to College) (LATE)
GE (College to H.S.) (LATER)
GE net (TLATE)
LATE4 $500 subsidy:
Partial equilibrium
GE (H.S. to College) (LATE)
GE (College to H.S.) (LATER)
GE net (TLATE)

0.281
0.294
0.270
0.259

1.801
3.364
−1.225
0.259

100%
44.7%
55.3%
–

0.255
0.253
0.393
–

–
0.227
0.365
0.244

23.6%
2.48%
0.34%
2.82%

0.254
0.250
0.393
–

–
0.247
0.390
0.264

2.37%
0.24%
0.03%
0.27%

1 In column (1), prices are held constant at their initial steady state levels when wage differences are calculated.
2 In column (2), we allow prices to adjust in response to the change in schooling proportions when calculating

wage differences.
3 For each row, column (3) presents the fraction of the sample over which the parameter is defined.
4 The LATE group gives the effect on earnings for persons who would be induced to attend college by a tuition
change. In the case of GE, LATE measures the effect on individuals induced to attend college when skill prices
adjust in response to quantity movements among skill groups. The treatment effect LATE measures the effect
of the policy on those induced to attend college when skill prices are held at the benchmark level.
Source: Heckman, Lochner and Taber (1998b).

untreated parameter is defined analogously by comparing what high school graduates
in the benchmark economy would earn if everyone in the population were forced to go
to college. The average treatment effect compares the average earnings in a world in
which everyone attends college versus the earnings in a world in which nobody attends
college. Such dramatic policy shifts produce large estimated effects. In contrast, the
general equilibrium marginal treatment effect parameter considers the gain to attending
college for people on the margin of indifference between attending college and only
attending high school. In this case, as long as the mass of people in the indifference set
is negligible, the standard treatment effect and general equilibrium parameters are the
same.
The final set of parameters considered by Heckman, Lochner and Taber (1998b) are
versions of the LATE parameter. This parameter depends on the particular intervention
being studied and its magnitude. The standard version of LATE is defined on the outcomes of individuals induced to attend college, assuming that skill prices do not change.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5281

The general equilibrium version is defined for the individuals induced to attend college
when prices adjust in response to the policy. In this general equilibrium model, the two
LATE parameters are quite close to each other and are also close to the marginal treatment effect.141 General equilibrium effects change the group over which the parameter
is defined compared to the standard treatment effect case. For a $5000 subsidy, there are
substantial price effects and the standard treatment effect parameter differs substantially
from the general equilibrium version.
Heckman, Lochner and Taber (1998a, 1998b, 1998c) also present standard treatment effect and general equilibrium estimates for two extensions of the LATE concept:
LATER (the effect of the policy on those induced to attend only high school rather than
go to college)—Reverse LATE—and TLATE (the effect of the policy on all of those induced to change whichever direction they flow). LATER is larger than LATE, indicating
that those induced to drop out of college have larger gains from dropping out than those
induced to enter college have from entering. TLATE is a weighted average of LATE
and LATER with weights given by the relative proportion of people who switch in each
direction.
The general equilibrium impacts of tuition on college enrollment are an order of
magnitude smaller than those reported in the literature on microeconometric treatment
effects. The assumptions used to justify the LATE parameter in a microeconomic setting do not carry over to a general equilibrium framework. Policy changes, in general,
induce two-way flows and violate the monotonicity—or one-way flow—assumption of
LATE. Heckman, Lochner and Taber (1998b) extend the LATE concept to allow for
the two-way flows induced by the policies. They present a more comprehensive approach to program evaluation by considering both the tax and benefit consequences of
the program being evaluated and placing the analysis in a market setting. Their analysis
demonstrates the possibilities of the general equilibrium approach and the limitations
of the microeconomic “treatment effect” approach to policy evaluation.
4.2.1. Subsequent research
Subsequent research by Blundell et al. (2004), Duflo (2004), Lee (2005), and Lee and
Wolpin (2006) estimate—or estimate and calibrate—general equilibrium models for the
effects of policies on labor markets. Lee (2005) assumes that occupational groups are
perfect substitutes and that people can costlessly switch between skill categories. These
assumptions neutralize any general equilibrium effects. They are relaxed and shown to
be inconsistent with data from US labor markets in Lee and Wolpin (2006).
Lee and Wolpin (2006) assume adaptive expectations rather than rational expectations. Heckman, Lochner and Taber (1998a) establish the sensitivity of the policy
evaluations to specifications of expectations. Duflo (2004) demonstrates the importance

141 The latter is a consequence of the discrete-choice framework for schooling choices analyzed in the
Heckman, Lochner and Taber (1998b) model. See Chapter 71.

5282

J.H. Abbring and J.J. Heckman

of general equilibrium effects on wages for the evaluation of a large scale schooling
program in Indonesia. However, accounting for general equilibrium does not affect her
estimates of the rate of return of schooling.
4.2.2. Equilibrium search approaches
Equilibrium search models are another framework for studying market level interactions among agents. Search theory as developed by Mortensen and Pissarides (1994)
and Pissarides (2000) has begun to be tested on microdata [see Van den Berg (1999)]. It
accounts for direct and indirect effects without imposing full equilibrium price adjustment. Some versions of search theory allow for wage flexibility through a bargaining
mechanism while other approaches assume rigid wages. Search theory produces an explicit theory of unemployment. Davidson and Woodbury (1993) consider direct and
indirect effects of a bonus scheme to encourage unemployed workers to find jobs more
quickly using a Mortensen–Pissarides (1994) search model in which prices are fixed.
Their model is one of displacement with fixed prices.
More recent studies of equilibrium search models in which wages are set through
bargaining that have been used for policy analysis include papers by Lise, Seitz and
Smith (2005a, 2005b) and Albrecht, Van den Berg and Vroman (2005). Lise, Seitz and
Smith (2005a) present a careful synthesis of experimental and nonexperimental data
combining estimation and calibration. They provide evidence on labor-market feedback
effects associated with job subsidy schemes. In their analysis, accounting for general
equilibrium feedback reverses the cost–benefit evaluations of a job subsidy program in
Canada. Albrecht, Van den Berg and Vroman (2005) demonstrate important equilibrium
effects of an adult education program on employment and job vacancies, showing a skill
bias of the programs.
4.3. Analyses of displacement
Newly trained workers from a job training program may displace previously trained
workers if wages are inflexible, as they are in many European countries. For some
training programs in Europe, substantial displacement effects have been estimated
[Organization for Economic Cooperation and Development (1993), Calmfors (1994)].
If wages are flexible, the arrival of new trained workers to the market tends to lower the
wages of previously trained workers but does not displace any worker.
Even if the effect of treatment on the treated is positive, nonparticipants may be worse
off as a result of the program compared to what they would have experienced in a no
program state. Nonparticipants who are good substitutes for the new trainees are especially adversely affected. Complementary factors benefit. These spillover effects can
have important consequences for the interpretation of traditional evaluation parameters.
The benchmark “no treatment” state is affected by the program and invariance assumption (PI-2) presented in Chapter 70 is violated.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5283

To demonstrate these possibilities in a dramatic way, consider the effect of a wage
subsidy for employment in a labor market for low-skill workers. Assume that firms act
to minimize their costs of employment. Wage subsidies operate by taking nonemployed
persons and subsidizing their employment at firms. Firms who employ the workers receive the wage subsidy.
Many active labor-market policies have a substantial wage-subsidy component. Suppose that the reason for nonemployment of low-skill workers is that minimum wages are
set too high. This is the traditional justification for wage subsidies.142 If the number of
subsidized workers is less than the number of workers employed at the minimum wage,
a wage subsidy financed from lump sum taxes has no effect on total employment in the
low wage sector because the price of labor for the marginal worker hired by firms is
the minimum wage. It is the same before and after the subsidy program is put in place.
Thus the marginal worker is unsubsidized both before and after the subsidy program is
put in place.
The effects of the program are dramatic on the individuals who participate in it. Persons previously nonemployed become employed as firms seek workers who carry a
wage subsidy. Many previously-employed workers become nonemployed as their employment is not subsidized. There are no effects of the wage subsidy program on GDP
unless the taxes raised to finance the program have real effects on output. Yet there is
substantial redistribution of employment. Focusing solely on the effects of the program
on subsidized workers greatly overstates its beneficial impact on the economy at large.
In order to estimate the impact of the program on the overall economy, it is necessary to look at outcomes for both participants and nonparticipants. Only if the benefits
accruing to previously-nonemployed participants are adopted as the appropriate evaluation criterion would the effect of treatment on the treated be a parameter of interest.
Information on both participants and nonparticipants affected by the program is required
to estimate the net gain in earnings and employment resulting from the program.
In the case of a wage subsidy, comparing the earnings and employment of subsidized
participants during their subsidized period to their earnings and employment in the presubsidized period can be a very misleading estimator of the total impact of the program.
So is a cross-section comparison of participants and nonparticipants. In the example
of a subsidy in the presence of a minimum wage, the before–after estimate of the gain
exceeds the cross-section estimate unless the subsidy is extended to a group of nonemployed workers as large as the number employed at the minimum wage. For subsidy
coverage levels below this amount, some proportion of the unsubsidized employment is
paid the minimum wage. Under these circumstances, commonly-used evaluation estimators produce seriously misleading estimates of program impacts.
The following example clarifies and extends these points to examine the effect of
displacement on conventional estimators. Let N be the number of participants in the
low-wage labor market. Let NE be the number of persons employed at the minimum
142 See, e.g., Johnson (1979), or Johnson and Layard (1986).

5284

J.H. Abbring and J.J. Heckman

wage M and let NS be the number of persons subsidized. Subsidization operates solely
on persons who would otherwise have been nonemployed and had no earnings. Assume
NE > NS . Therefore, the subsidy has no effect on total employment in the market,
because the marginal cost of labor to a firm is still the minimum wage. Workers with
the subsidy are worth more to the firm by the amount of the subsidy S. Firms would be
willing to pay up to S + M per subsidized worker to attract them.
The estimated wage gain using a before–after comparison for subsidized participants
is:
Before–After:

( S + M) − (0) = S + M,
after

before

because all subsidized persons earn a zero wage prior to the subsidy. For them, the program is an unmixed blessing. The estimated wage gain using cross-section comparisons
of participants and nonparticipants is:

NE − NS
Cross-Section:
S+M −
M
×
N − NS
participant’s wage

=S+M

nonparticipant’s
wage

N − NE
N − NS



< S + M.

(<1)

Since NE > NS , the before–after estimator is larger than the cross-section estimator.
The widely used difference-in-differences estimator compares the before–after outcome
measure for participants to the before–after outcome measure for nonparticipants:

NE − NS
NE
Difference-in-Differences: (S + M − 0) − M
−
N − NS
N − NS

N
> S + M.
=S+M
N − NS
The gain estimated from the difference-in-differences estimator exceeds the gain estimated from the before–after estimator which in turn exceeds the gain estimated from the
cross-section estimator. The “no treatment” benchmark in the difference-in-differences
model is contaminated by treatment. The estimate of employment creation obtained
from the three estimators is obtained by setting M = 1 and S = 0 in the previous expressions. This converts those expressions into estimates of employment gains for the
different groups used in their definition.
None of these estimators produces a correct assessment of wage or employment gain
for the economy at large. Focusing only on direct participants causes analysts to lose
sight of overall program impacts. Only an aggregate analysis of the economy as a whole,
or random samples of the entire economy, would produce the correct assessment that
no wage increase or job creation is produced by the program. The problem of indirect

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5285

effects poses a major challenge to conventional micro methods used in evaluation research that focus on direct impacts instead of total impacts, and demonstrates the need
for program evaluations to utilize market-wide data and general equilibrium methods.
Calmfors (1994) presents a comprehensive review of the issues that arise in evaluating active labor-market programs and an exhaustive list of references on theoretical
and empirical work on this topic. He distinguishes a number of indirect effects including displacement effects (jobs created by one program at the expense of other jobs),
deadweight effects (subsidizing hiring that would have occurred in the absence of the
program), substitution effects (jobs created for a certain category of workers replace
jobs for other categories because relative wage costs have changed) and tax effects (the
effects of taxation required to finance the programs on the behavior of everyone in society). A central conclusion of this literature is that the estimates of program impact from
the microeconomic treatment effect literature provide incomplete information about the
full impacts of active labor-market programs. The effect of a program on participants
may be a poor approximation to the total effect of the program, as our simple example
has shown. Blundell et al. (2004) present evidence on substitution and displacement for
an English active labor-market program.
4.4. Social interactions
There is a growing empirical literature on social interactions. Brock and Durlauf (2001)
and Durlauf and Fafchamps (2005) present comprehensive surveys of the methods and
evidence from this emerging field. Instead of being market mediated, as in search and
general equilibrium models, the social interactions considered in this literature are at
the individual or group level which can include family interactions through transfers.
Linkages through family and other social interactions undermine the sharp treatmentcontrol separation assumed in the microeconomic treatment effect literature.
A recent paper by Angelucci and De Giorgi (2006) illustrates this possibility. They
analyze the effect of the Progressa program in Mexico on both treated and untreated
families. Progressa paid families to send their children to school. They present evidence that noneligible families received transfers from the eligible families and altered
their saving and consumption behavior. Thus, through the transfer mechanism, the “untreated” receive treatment. However, they show no general equilibrium effects of the
program on the product and labor markets that they study.
4.5. Summary of general equilibrium approaches
Many policies affect both “treatment” groups and indirectly affect “control” groups
through market and social interactions. Reliance on microeconomic treatment effect
approaches to evaluate such policies can produce potentially misleading estimates. The
analysis of Heckman, Lochner and Taber (1998a) and the later work by Albrecht, Van
den Berg and Vroman (2005), Blundell et al. (2004), Duflo (2004), Angelucci and De

5286

J.H. Abbring and J.J. Heckman

Giorgi (2006), Lee (2005), Lise, Seitz and Smith (2005a, 2005b), and Lee and Wolpin
(2006) indicate that ignoring indirect effects can produce misleading policy evaluations.
The cost of this enhanced knowledge is the difficulty in assembling all of the behavioral parameters required to conduct a general equilibrium evaluation. From a long run
standpoint, these costs are worth incurring. Once a solid knowledge base is put in place,
a more trustworthy framework for policy evaluation will be available, one that will offer
an economically-justified framework for accumulating evidence across studies and will
motivate empirical research by microeconomists to provide better empirical foundations
for general equilibrium policy analyses.

5. Summary
This chapter extends the traditional static ex post literature on mean treatment effects
to consider the identification of distributions of treatment effects, the identification of
ex ante and ex post distributions of treatment effects, the measurement of uncertainty
facing agents and the analysis of subjective valuations of programs. We also survey
methods for identifying dynamic treatment effects with information updating by agents,
using both explicitly formulated economic models and less explicit approaches. We
discuss general equilibrium policy evaluation and evaluation of models with social interactions.

Appendix A: Deconvolution
To see how to use (CON-1) and (M-1) to identify F (y0 , y1 | X), note that
Y = Y0 + D.
From FY (y | X, D = 0), we identify F0 (y0 | X) as a consequence of matching assumption (M-1). From FY (y | X, D = 1) we identify F1 (y1 | X) = FY0 + (y0 +  | X). If
Y0 and Y1 have densities, then, as a consequence of (CON-1) and (M-1), the densities
satisfy
f1 (y1 | X) = f ( | X) ∗ f0 (y0 | X)
where “∗” denotes convolution. The characteristic functions of Y0 , Y1 and  are related
in the following way:


 


E eiY1 | X = E ei | X E eiY0 | X .
Since we can identify F1 (y1 | X), we know its characteristic function. By a similar
argument, we can recover E(eiY0 | X). Thus, from
 E(eiY1 | X)

,
E ei | X =
E(eiY0 | X)

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5287

and by the inversion theorem,143 we can recover the density f ( | X). We know the
joint density
f,Y0 (, y0 | X) = f ( | X)f0 (y0 | X).
From the definition of , we obtain
f (y1 − y0 | X)f0 (y0 | X) = f (y1 , y0 | X).
Thus we can recover the full joint distribution of outcomes and the distribution of gains.
Under assumption (M-1), assumption (CON-1) is testable. The ratio of two characteristic functions is not necessarily a characteristic function. If it is not, the estimated
density f recovered from the ratio of the characteristic functions need not be positive
and the estimated variance of  can be negative.144

Appendix B: Matzkin conditions and proof of Theorem 2
We prove Theorem 2. We first present a review of the conditions Matzkin (1992) imposes for identification of nonparametric discrete choice models which are used in this
proof.
B.1. The Matzkin conditions
Consider a binary choice model, D = 1[ϕ(Z) > V ], where Z is observed and V is
unobserved. Let ϕ ∗ denote the true ϕ and let FV∗ denote the true cdf of V . Let Z ⊆ RK
denote the support of Z. Let H denote the set of monotone increasing functions from R
into [0, 1]. Assume:
(i) ϕ ∈ Φ, where Φ is a set of real valued, continuous functions defined over Z,
which is also assumed to be the domain of definition of ϕ, and the true function
 ⊆ Z such that (a) for all ϕ, ϕ  ∈ Φ, and all
is ϕ ∗ ∈ Φ. There exists a subset Z


z ∈ Z, ϕ(z) = ϕ (z), and (b) for all ϕ ∈ Φ and all t in the range space of ϕ ∗ (z)
 such that ϕ(z̃) = t. In addition, ϕ ∗ is strictly
for z ∈ Z, there exists a z̃ ∈ Z
increasing in the Kth coordinate of Z.
(ii) Z ⊥
⊥V.
(iii) The Kth component of Z possesses a Lebesgue density conditional on the other
components of Z.

143 See, e.g., Kendall and Stuart (1977).
144 For the ratio of characteristic functions, r(), to be a characteristic function, it must satisfy the require-

ment that r(0) = 1, that r() is continuous in  and r() is nonnegative definite. This identifying assumption
can be tested using the procedures developed in Heckman, Robb and Walker (1990).

5288

J.H. Abbring and J.J. Heckman

(iv) FV∗ is strictly increasing on the support of ϕ ∗ (Z). Matzkin (1992) notes that if
one assumes that V is absolutely continuous, and the other conditions hold, one
can relax the condition that ϕ ∗ is strictly increasing in one coordinate (listed
in (i)) and the requirement in (iii).
Then (ϕ ∗ , FV∗ ) is identified within Φ × H, where FV∗ is identified on the support of
ϕ ∗ (Z).
Matzkin establishes identifiability for the following alternative representations of
functional forms that satisfy condition (i) for exact identification for ϕ(Z).
1. ϕ(Z) = Zγ , γ  = 1 or γ1 = 1.
2. ϕ(z) is homogeneous of degree one and attains a given value α at z = z∗ (e.g.,
cost functions).
3. The ϕ(Z) are least concave functions that attain common values at two points in
their domain.
4. The ϕ(Z) are additively separable functions:
(a) Functions additively separable into a continuous monotone increasing function and a continuous monotone increasing function which is concave and
homogeneous of degree one;
(b) Functions additively separable into the value of one variable and a continuous,
monotone increasing function of the remaining variables;
(c) A set of functions additively separable in each argument [see Matzkin (1992,
Example 5, p. 255)].
We now prove Theorem 2.
B.2. Proof of Theorem 2
P ROOF. Proof of the identifiability of the joint distribution of V s and μsR (Z) follows
from Matzkin (1993), Theorem 2. See also the proof presented in Chapter 70 (Appendix B) of this Handbook. We condition on the event D(s) = 1. From the data on
Yc (s, X), Yd (s, X), Mc (X), Md (X) for D(s) = 1, and the treatment selection probabilities, we can construct the left-hand side of the following equation:



Yc (s, X)  yc , μd (s, X)  Ud (s), 
Pr
D(s) = 1, X = x, Z = z
Mc (X)  mc , μd,M (X)  Ud,M 


× Pr D(s) = 1 | X = x, Z = z
 yc −μc (s,x)  Ud (s)  mc −μc,M (x)  Ud,M  μR (s,z)−μR (1,z)
=
···


U c (s)

μd (s,x) U c,M

μR (s,z)−μR (S̄,z)
V s (S̄)

μd,M (x) V s (1)


fUc (s),Ud (s),Uc,M ,Ud,M ,V s uc (s), ud (s), uc,M , ud,M ,

v(s) − v(1), v(s) − v(S̄)

Ch. 72:

Econometric Evaluation of Social Programs, Part III





· d v(s) − v(S̄) · · · d v(s) − v(1) dud,M duc,M dud (s) duc (s).

5289

(B.1)

Parallel expressions can be derived for the other possible values of Md (X) and Yd (s, X).
We obtain the selection-bias free distribution of Yc (s, X), Yd (s, X), Mc (X), Md (X)
given X, Pr(Yc (s, X)  yc , Yd (s, X) = yd , Mc (X)  mc , Md (X) = md | X), from
Pr(Yc (s, X)  yc , Yd (s, X) = yd , Mc (X)  mc , D(s) = 1 | X, Z = z) for z → Zs ,
a limit set, possibly dependent on X, such that limz→Zs Pr(D(s) = 1 | X, Z = z) = 1.
This produces the μc (s, X), μc,M (X) directly and the μd (s, X), μd,M (X) using the
analysis of Matzkin (1992, 1993, 1994) for the class of Matzkin functions defined in
Appendix B.1. Varying the yc − μc (s, X), μd (s, X), mc − μc,M (X), μd,M (X), μsR (Z),
under the conditions of the theorem we can trace out the joint distribution of (Uc (s),

Ud (s), Uc,M , Ud,M , V s ) for each s = 1, . . . , S̄.
As a consequence of (ii), we can identify μc (s, X), μc,M (X) directly from the means
of the limit outcome distributions. We can thus identify all pairwise average treatment
effects




E Yc (s, X) | X = x − E Yc (s  , X) | X = x
for all s, s  and any other linear functionals derived from the distributions of the continuous variables defined at s and s  . Identification of the means and distributions of the
latent variables giving rise to the discrete outcomes is more subtle, but standard [see
Carneiro, Hansen and Heckman (2003)]. With one continuous regressor among the X,
one can identify the marginal distributions of the Ud (s) and the Ud,M . To identify the
joint distributions of Ud (s) and Ud,M one must use condition (iv) component by component.
Thus for system s, suppose that there are Nd,s discrete outcome components with
associated means μd,j (s, X) and error terms Ud,j (s), j = 1, . . . , Nd,s . As a consequence of condition (iv) of this theorem, Supp(μd (s, X)) ⊇ Supp(Ud (s)). We thus
can trace out the joint distribution of Ud (s) and identify it (up to scale if we specify the Matzkin class only up to scale). By a parallel argument for the measurements,
we can identify the joint distribution of Ud,M . Let Nd,M be the number of discrete
measurements. From condition (iv), we obtain Supp(μd,M (X)) ⊇ Supp(Ud,M ). Under these conditions, we can trace out the joint distribution of Ud,M and identify it (up
to scale for Matzkin class of functions specified up to scale) within the limit sets. In
the general case, we can vary each limit of the integral in (B.1) and similar integrals
for the other possible values of the discrete measurements and outcomes independently and trace out the full joint distribution of (Uc (s), Ud (s), Uc,M , Ud,M , V s ). For
further discussion, see the analysis in Carneiro, Hansen and Heckman (2003, Theorem 3).

5290

J.H. Abbring and J.J. Heckman

Appendix C: Proof of Theorem 4
P ROOF. From Theorem 3, we obtain identifiability of Ψ s (Z) and the joint distribution
of ηs . From the data on Y (s, X), for D(s) = 1, and from the time to treatment probabilities, we can construct the left-hand side of the following equation:



Pr Y (s, X)  y  D(s) = 1, X = x, Z = z


× Pr D(s) = 1 | X = x, Z = z
 y−μ(s,x)  Ψ (s,z(s))  η̄(s−1)
···
=


U (s)
η̄(1)

η(s)

Ψ (s−1,z(s−1))



fU (s),ηs u(s), η(1), . . . , η(s) dη(1) · · · dη(s) du(s).

(C.1)

Ψ (1,z(1))

Under assumption (iv), for all x ∈ Supp(X), we can vary the Ψ (j, Z(j )),
j = 1, . . . , s, and obtain a limit set Zs , possibly dependent on X, such that
limz→Zs Pr(D(s) = 1 | X = x, Z = z) = 1. We can identify the joint distribution
of Y (s, X), free of selection bias in this limit set for all s = 1, . . . , S̄. We know the limit
sets given the functional forms in Matzkin (1992, 1993, 1994) with the leading case
being Ψ (s, Z(s)) = Z(s)γs . From the analysis of Theorem 3, we achieve identifiability
on nonnegligible sets.
As a consequence of (ii), we can identify μ(s, X) directly from the means of the
limit outcome distributions. We can thus identify all pairwise average treatment effects
E(Y (s, X) | X = x)−E(Y (s  , X) | X = x) for all s, s  and any other linear functionals
derived from the distributions of the continuous variables defined at s and s  .
In the general case, we can vary each limit of the integral in (C.1) independently and
trace out the full joint distribution of (U (s), η(1), . . . , η(s)). For further discussion, see
the analysis in Carneiro, Hansen and Heckman (2003, Theorem 3). Note the close parallel to the proof of Theorem 2. The key difference between the two proofs is the choice
equation. In Theorem 2, the choice of treatment equation is a conventional multivariate
discrete-choice model. In Theorem 3, it is the reduced form dynamic model extensively
analyzed in Heckman and Navarro (2007).

Appendix D: Proof of a more general version of Theorem 4
This appendix states and proves a more general version of Theorem 4. Use Y (t, s) as
shorthand for Y (t, s, X, U (t, s)). Ignore (for notational simplicity) the mixed discretecontinuous outcome case. One can build that case from the continuous and discrete
cases and for the sake of brevity we do not analyze it here. We also do not analyze duration outcomes although it is straightforward to do so. Decompose Y (t, s) into discrete
and continuous components:


Yc (t, s)
Y (t, s) =
.
Yd (t, s)

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5291

∗ (t, s).
Associated with the j th component of Yd (t, s), Yd,j (t, s), is a latent variable Yd,j
Define, as in Theorem 2,
 ∗

Yd,j (t, s) = 1 Yd,j
(t, s)  0 .145

From standard results in the discrete-choice literature, without additional information,
∗ (t, s) up to scale.
one can only identify Yd,j
Assume an additively separable model for the continuous variables and latent continuous indices. Making the X explicit, we obtain
Yc (t, s, X) = μc (t, s, X) + Uc (t, s),
Yd∗ (t, s, X) = μd (t, s, X) − Ud (t, s),
1  s  S̄, 1  t  T̄ .
Array the Yc (t, s, X) into a matrix Yc (s, X) and the Yd∗ (t, s, X) into a matrix Yd∗ (s, X).
Decompose these vectors into components corresponding to the means μc (s, X),
μd (s, X) and the unobservables Uc (s), Ud (s). Thus
Yc (s, X) = μc (s, X) + Uc (s),
Yd∗ (s, X) = μd (s, X) − Ud (s).
Yd∗ (s, X) generates Yd (s, X). To simplify the notation, make use of the condensed forms
Yc (X), Yd∗ (X), μc (X), μd (X), Uc and Ud as described in the text. In this notation,
Yc (X) = μc (X) + Uc ,
Yd∗ (X) = μd (X) − Ud .
Following Carneiro, Hansen and Heckman (2003) and Cunha and Heckman (2007b,
2008), Cunha, Heckman and Navarro (2005, 2006), one may also have a system of
measurements with both discrete and continuous components. The measurements are
not s-indexed. They are the same for each stopping time. Write the equations for the
measurements in an additively separable form, in a fashion comparable to those of the
outcomes. The equations for the continuous measurements and latent indices producing
discrete measurements are
Mc (t, X) = μc,M (t, X) + Uc,M (t),
Md∗ (t, X) = μd,M (t, X) − Ud,M (t),
where the discrete variable corresponding to the j th index in Md∗ (t, X) is
 ∗

(t, X)  0 .
Md,j (t, X) = 1 Md,j
145 Extensions to nonbinary discrete outcomes are straightforward. Thus one could entertain, at greater notational cost, a multinomial outcome model at each age t for each counterfactual state, building on the analysis
of Appendix B in Chapter 70.

5292

J.H. Abbring and J.J. Heckman

The measurements play the role of indicators unaffected by the process being studied. We array Mc (t, X) and Md∗ (t, X) into matrices Mc (X) and Md∗ (X). We array
μc,M (t, X), μd,M (t, X) into matrices μc,M (X) and μd,M (X). We array the corresponding unobservables into Uc,M and Ud,M . Thus we write
Mc (X) = μc,M (X) + Uc,M ,
Md∗ (X) = μd,M (X) − Ud,M .
We use the notation of Section 3.4.1 to write I (s) = Ψ (s, Z(s)) − η(s) and collect
I (s), Ψ (s, Z(s)) and η(s) into vectors I , Ψ (Z), η. We define ηs = (η(1), . . . , η(s)) and
Ψ s (Z) = (Ψ (1, Z(1)), . . . , Ψ (s, Z(s))). Using this notation, we extend the analysis of
Carneiro, Hansen and Heckman (2003) to identify our model assuming that we have a
large i.i.d. sample from the distribution of (Yc , Yd , Mc , Md , I ).
T HEOREM D.1. Assuming the conditions of Theorem 3 hold, for s = 1, . . . , S̄, the joint
distribution of (Uc (s), Ud (s), Uc,M , Ud,M , ηs ) is identified along with the mean functions (μc (s, X), μd (s, X), μc,M (X), μd,M (X), Ψ s (Z)) (the components of μd (s, X)
and μd,M (X) over the supports admitted by the supports of the errors) if
(i) E[Uc (s)] = E[Uc,M ] = 0. (Uc (s), Ud (s), Uc,M , Ud,M , ηs ) are continuous
random variables with support: Supp(Uc (s)) × Supp(Ud (s)) × Supp(Uc,M ) ×
Supp(Ud,M ) × Supp(ηs ) with upper and lower limits (Uc (s), Ud (s), Uc,M ,
Ud,M , η̄s ) and (U c (s), U d (s), U c,M , U d,M , ηs ) respectively. These conditions
are assumed to apply within each component of each subvector. The joint system
is thus variation free for each component with respect to every other component.
⊥ (X, Z).
(ii) (Uc (s), Ud (s), Uc,M , Ud,M , ηs ) ⊥
(iii) Supp(Ψ (Z), X) = Supp(Ψ (Z)) × Supp(X).
(iv) Supp(μd (s, X), μd,M (X)) ⊇ Supp(Ud (s), Ud,M ).
(v) μc (s, X) and μc,M (X) are continuous functions. The components of the
μd (s, X) and μd,M (X) satisfy the Matzkin conditions developed in Appendix B.1.
P ROOF. We use the proof of Theorem 3 to identify Ψ s (Z) and the distributions of ηs ,
s = 1, . . . , S̄. From the data on Yc (s, X), Yd (s, X), Mc (X), Md (X) for D(s) = 1,
and from the time to treatment probabilities, we can construct the left-hand side of the
following equation:


Yc (s, X)  yc , μd (s, X)  Ud (s), 
D(s) = 1, X = x, Z = z
Pr
Mc (X)  mc , μd,M (X)  Ud,M 


× Pr D(s) = 1 | X = x, Z = z


Ch. 72:

Econometric Evaluation of Social Programs, Part III


=


yc −μc (s,x)  Ud (s)
U c (s)
η̄(1)

Ψ (1,z(1))



mc −μc,M (x)  Ud,M

μd (s,x) U c,M

5293



Ψ (s,z(s))  η̄(s−1)

μd,M (x) η(s)

···

Ψ (s−1,z(s−1))



fUc (s),Ud (s),Uc,M ,Ud,M ,ηs uc (s), ud (s), uc,M , ud,M , η(1), . . . , η(s)

· dη(1) · · · dη(s) dud,M duc,M dud (s) duc (s).

(D.1)

We can construct distributions for the other configurations of conditioning events defining the discrete dependent variables (i.e., μd (s, X) > Ud (s), μd,M (X) > Ud,M ;
μd (s, X) > Ud (s), μd,M (X) < Ud,M ; μd (s, X)  Ud (s), μd,M (X) > Ud,M ).
Under assumption (iii), for all x ∈ Supp(X), we can vary the Ψ (j, z(j )),
j = 1, . . . , s, and obtain a limit set Zs , possibly dependent on X, such that
limz→Zs Pr(D(s) = 1 | X = x, Z = z) = 1. We can use (D.1) and parallel distributions for the other configurations for the discrete dependent variables to identify
the joint distribution of Yc (s, X), Yd (s, X), Mc (X), Md (X) free of selection bias for all
s = 1, . . . , S̄ in these limit sets. We identify the parameters of Yd (s, X), s = 1, . . . , S̄,
and Md (X). We know the limit sets given the functional forms for the Ψ (s, Z(s)),
s = 1, . . . , S̄, presented in B.1 or in Matzkin (1992, 1993, 1994).
As a consequence of (ii), we can identify μc (s, X), μc,M (X) directly from the means
of the limit outcome distributions. We can thus identify all pairwise average treatment
effects




E Yc (s, X) | X = x − E Yc (s  , X) | X = x
for all s, s  and any other linear functionals derived from the distributions of the continuous variables defined at s and s  . Identification of the means and distributions of
the latent variables giving rise to the discrete outcomes is more subtle. The required
argument is standard. With one continuous regressor among the X, one can identify the
marginal distributions of the Ud (s) and the Ud,M (up to scale if the Matzkin functions
are only specified up to scale). To identify the joint distributions of Ud (s) and Ud,M ,
one can invoke (iv).
Thus for system s, suppose that there are Nd,s discrete outcome components with associated means μd,j (s, X) and error terms Ud,j (s), j = 1, . . . , Nd,s . As a consequence
of condition (iv) of this theorem, Supp(μd (s, X)) ⊇ Supp(Ud (s)). We thus can trace
out the joint distribution of Ud (s) and identify it (up to scale if we specify the Matzkin
class only up to scale). By a parallel argument for the measurements, we can identify
the joint distribution of Ud,M . Let Nd,M be the number of discrete measurements. From
condition (iv), we obtain Supp(μd,M (X)) ⊇ Supp(Ud,M ). Under these conditions, we
can trace out the joint distribution of Ud,M and identify it (up to scale for the Matzkin
class of functions specified up to scale) within the limit sets. From assumption (v), we
obtain identification on nonnegligible sets.
We can vary each limit of the integral in (D.1) independently and trace out the full
joint distribution of (Uc (s), Ud (s), Uc,M , Ud,M , η(1), . . . , η(s)) using the parameters
determined from the marginals. For further discussion, see the analysis in Carneiro,

5294

J.H. Abbring and J.J. Heckman

Hansen and Heckman (2003, Theorem 3). We obtain identifiability on nonnegligible
sets by combining the conditions in Theorem 3 with those in condition (v).

References
Aakvik, A., Heckman, J.J., Vytlacil, E.J. (2005). “Estimating treatment effects for discrete outcomes when
responses to treatment vary: An application to Norwegian vocational rehabilitation programs”. Journal of
Econometrics 125 (1–2), 15–51.
Aalen, O.O., Gjessing, H.K. (2004). “Survival models based on the Ornstein–Uhlenbeck process”. Lifetime
Data Analysis 10 (4), 407–423 (December).
Abadie, A. (2002). “Bootstrap tests of distributional treatment effects in instrumental variable models”. Journal of the American Statistical Association 97 (457), 284–292 (March).
Abadie, A., Angrist, J.D., Imbens, G. (2002). “Instrumental variables estimates of the effect of subsidized
training on the quantiles of trainee earnings”. Econometrica 70 (1), 91–117 (January).
Abbring, J.H. (2002). “Stayers versus defecting movers: A note on the identification of defective duration
models”. Economics Letters 74 (3), 327–331 (February).
Abbring, J.H. (2003). “Dynamic econometric program evaluation”. Discussion Paper 804. IZA, Bonn. Paper
prepared for the H. Theil Memorial Conference, Amsterdam, 16–18 August 2002.
Abbring, J.H. (2007). “Mixed hitting-time models”. Discussion Paper 2007-057/3. Tinbergen Institute, Amsterdam.
Abbring, J.H. (2008). “The event-history approach to program evaluation”. In: Millimet, D., Smith, J., Vytlacil, E. (Eds.), Modeling and Evaluating Treatment Effects in Econometrics. In: Advances in Econometrics, vol. 21. Elsevier Science, Oxford (forthcoming).
Abbring, J.H., Campbell, J.R. (2005). “A firm’s first year”. Discussion Paper 2005-046/3. Tinbergen Institute,
Amsterdam (May).
Abbring, J.H., Heckman, J.J. (2008). “Dynamic policy analysis”. In: Mátyás, L., Sevestre, P. (Eds.), The
Econometrics of Panel Data, third ed. Kluwer, Dordrecht (forthcoming).
Abbring, J.H., Van den Berg, G.J. (2003a). “The identifiability of the mixed proportional hazards competing
risks model”. Journal of the Royal Statistical Society, Series B 65 (3), 701–710 (September).
Abbring, J.H., Van den Berg, G.J. (2003b). “The nonparametric identification of treatment effects in duration
models”. Econometrica 71 (5), 1491–1517 (September).
Abbring, J.H., Van den Berg, G.J. (2003c). “A simple procedure for inference on treatment effects in duration
models”. Discussion paper 810. IZA, Bonn.
Abbring, J.H., Van den Berg, G.J. (2004). “Analyzing the effect of dynamically assigned treatments using
duration models, binary treatment models and panel data models”. Empirical Economics 29 (1), 5–20
(January).
Abbring, J.H., Van den Berg, G.J. (2005). “Social experiments and instrumental variables with duration outcomes”. Discussion Paper 2005-047/3. Tinbergen Institute, Amsterdam.
Abbring, J.H., Van den Berg, G.J., Van Ours, J.C. (2005). “The effect of unemployment insurance sanctions
on the transition rate from unemployment to employment”. Economic Journal 115 (505), 602–630 (July).
Albrecht, J., Van den Berg, G.J., Vroman, S. (2005). “The knowledge lift: The Swedish adult education
program that aimed to eliminate low worker skill levels”. Discussion Paper 1503. Institute for the Study
of Labor (IZA), Bonn (February).
Aldrich, J. (1989). “Autonomy”. Oxford Economic Papers 41 (1), 15–34 (January).
Andersen, P.K., Borgan, Ø., Gill, R., Keiding, N. (1993). Statistical Models Based on Counting Processes.
Springer-Verlag, New York.
Anderson, T., Rubin, H. (1956). “Statistical inference in factor analysis”. In: Neyman, J. (Ed.), Proceedings
of the Third Berkeley Symposium on Mathematical Statistics and Probability, 5. University of California
Press, Berkeley, pp. 111–150.

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5295

Angelucci, M., De Giorgi, G. (2006). “Indirect effects of an aid program: The case of Progresa and consumption”. Discussion Paper 1955. Institute for the Study of Labor (IZA) (January).
Auerbach, A.J., Kotlikoff, L.J. (1987). Dynamic Fiscal Policy. Cambridge University Press, Cambridge, New
York.
Barros, R.P. (1987). “Two essays on the nonparametric estimation of economic models with selectivity using
choice-based samples”. PhD thesis. University of Chicago.
Becker, G.S. (1974). “A theory of marriage: Part II”. Journal of Political Economy 82 (2, Part 2: Marriage,
Family Human Capital and Fertility), S11–S26 (March).
Belzil, C., Hansen, J. (2002). “Unobserved ability and the return to schooling”. Econometrica 70 (5), 2075–
2091 (September).
Bishop, Y.M., Fienberg, S.E., Holland, P.W. (1975). Discrete Multivariate Analysis: Theory and Practice. The
MIT Press, Cambridge, Massachusetts.
Black, D.A., Smith, J.A., Berger, M.C., Noel, B.J. (2003). “Is the threat of reemployment services more
effective than the services themselves? Evidence from random assignment in the UI system”. American
Economic Review 93 (4), 1313–1327 (September).
Blundell, R., Costa Dias, M., Meghir, C., Van Reenen, J. (2004). “Evaluating the employment effects of a
mandatory job search program”. Journal of the European Economic Association 2 (4), 569–606 (June).
Bonhomme, S., Robin, J.-M. (2004). “Nonparametric identification and estimation of independent factor models”. Unpublished working paper. Sorbonne, Paris.
Bonnal, L., Fougère, D., Sérandon, A. (1997). “Evaluating the impact of French employment policies on
individual labour market histories”. Review of Economic Studies 64 (4), 683–713 (October).
Brock, W.A., Durlauf, S.N. (2001). “Interactions-based models”. In: Heckman, J.J., Leamer, E. (Eds.), Handbook of Econometrics, vol. 5. North-Holland, New York, pp. 3463–3568.
Browning, M., Hansen, L.P., Heckman, J.J. (1999). “Micro data and general equilibrium models”. In: Taylor,
J.B., Woodford, M. (Eds.), Handbook of Macroeconomics, vol. 1A. Elsevier, pp. 543–633, Chapter 8
(December).
Calmfors, L. (1994). “Active labour market policy and unemployment – a framework for the analysis of
crucial design features”. OECD Economic Studies 22, 7–47 (Spring).
Cambanis, S., Simons, G., Stout, W. (1976). “Inequalities for e(k(x, y)) when the marginals are fixed”.
Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 36, 285–294.
Cameron, S.V., Heckman, J.J. (1998). “Life cycle schooling and dynamic selection bias: Models and evidence
for five cohorts of American males”. Journal of Political Economy 106 (2), 262–333 (April).
Cameron, S.V., Heckman, J.J. (2001). “The dynamics of educational attainment for black, Hispanic and white
males”. Journal of Political Economy 109 (3), 455–499 (June).
Card, D., Sullivan, D.G. (1988). “Measuring the effect of subsidized training programs on movements in and
out of employment”. Econometrica 56 (3), 497–530 (May).
Carneiro, P., Hansen, K., Heckman, J.J. (2001). “Removing the veil of ignorance in assessing the distributional
impacts of social policies”. Swedish Economic Policy Review 8 (2), 273–301 (Fall).
Carneiro, P., Hansen, K., Heckman, J.J. (2003). “Estimating distributions of treatment effects with an application to the returns to schooling and measurement of the effects of uncertainty on college choice”. 2001
Lawrence R. Klein Lecture. International Economic Review 44 (2), 361–422 (May).
Chamberlain, G. (1975). “Unobservables in econometric models”. PhD thesis. Harvard University.
Chan, T.Y., Hamilton, B.H. (2006). “Learning, private information and the economic evaluation of randomized experiments”. Journal of Political Economy 114 (6), 997–1040.
Chesher, A. (2003). “Identification in nonseparable models”. Econometrica 71 (5), 1405–1441 (September).
Cosslett, S.R. (1983). “Distribution-free maximum likelihood estimator of the binary choice model”. Econometrica 51 (3), 765–782 (May).
Cowell, F.A. (2000). “Measurement of inequality”. In: Atkinson, A., Bourguignon, F. (Eds.), Handbook of
Income Distribution, vol. 1. North-Holland, New York, pp. 87–166.
Cunha, F., Heckman, J.J. (2007a). “Formulating, identifying and estimating the technology of cognitive
and noncognitive skill formation”. Unpublished manuscript. University of Chicago, Department of Economics. Journal of Human Resources (forthcoming).

5296

J.H. Abbring and J.J. Heckman

Cunha, F., Heckman, J.J. (2007b). “The evolution of earnings risk in the US economy”. Presented at the 9th
World Congress of the Econometric Society, London.
Cunha, F., Heckman, J.J. (2007c). “Identifying and estimating the distributions of ex post and ex ante returns
to schooling: A survey of recent developments”. Labour Economics (forthcoming).
Cunha, F., Heckman, J.J. (2008). “A framework for the analysis of inequality”. Macroeconomic Dynamics
(forthcoming).
Cunha, F., Heckman, J.J., Navarro, S. (2005). “Separating uncertainty from heterogeneity in life cycle earnings, the 2004 Hicks lecture”. Oxford Economic Papers 57 (2), 191–261 (April).
Cunha, F., Heckman, J.J., Navarro, S. (2006). “Counterfactual analysis of inequality and social mobility”.
In: Morgan, S.L., Grusky, D.B., Fields, G.S. (Eds.), Mobility and Inequality: Frontiers of Research in
Sociology and Economics. Stanford University Press, Stanford, CA, pp. 290–348 (Chapter 4).
Cunha, F., Heckman, J.J., Navarro, S. (2007). “The identification and economic content of ordered choice
models with stochastic cutoffs”. International Economic Review (forthcoming, November).
Cunha, F., Heckman, J.J., Schennach, S.M. (2006). “Nonlinear factor analysis”. Unpublished manuscript.
University of Chicago, Department of Economics.
Cunha, F., Heckman, J.J., Schennach, S.M. (2007). “Estimating the technology of cognitive and noncognitive
skill formation”. Unpublished manuscript. University of Chicago, Department of Economics. Presented
at the Yale Conference on Macro and Labor Economics, May 5–7, 2006. Econometrica (under revision).
Davidson, C., Woodbury, S.A. (1993). “The displacement effect of reemployment bonus programs”. Journal
of Labor Economics 11 (4), 575–605 (October).
Dawkins, C., Srinivasan, T.N., Whalley, J. (2001). “Calibration”. In: Heckman, J.J., Leamer, E. (Eds.), Handbook of Econometrics. In: Handbooks in Economics, vol. 5. Elsevier Science, New York, pp. 3653–3703.
Duflo, E. (2004). “The medium run effects of educational expansion: Evidence from a large school construction program in Indonesia”. Journal of Development Economics 74 (1), 163–197 (June, special issue).
Durlauf, S., Fafchamps, M. (2005). “Social capital”. In: Durlauf, S., Aghion, P. (Eds.), Handbook of Growth
Economics. In: Handbooks in Economics, vol. 1B. Elsevier, pp. 1639–1699.
Dynarski, S.M. (2000). “Hope for whom? Financial aid for the middle class and its impact on college attendance”. National Tax Journal 53 (3, Part 2), 629–661 (September).
Eberwein, C., Ham, J.C., LaLonde, R.J. (1997). “The impact of being offered and receiving classroom training
on the employment histories of disadvantaged women: Evidence from experimental data”. Review of
Economic Studies 64 (4), 655–682 (October).
Eckstein, Z., Wolpin, K.I. (1999). “Why youths drop out of high school: The impact of preferences, opportunities and abilities”. Econometrica 67 (6), 1295–1339 (November).
Falmagne, J.-C. (1985). Elements of Psychophysical Theory. Oxford Psychology Series. Oxford University
Press, New York.
Fields, G.S. (2003). “Economic and social mobility really are multifaceted”. Paper presented at the Conference on Frontiers in Social and Economic Mobility. Cornell University, Ithaca, New York (March).
Fitzenberger, B., Osikominu, A., Völter, R. (2006). “Get training or wait? Long-run employment effects of
training programs for the unemployed in West Germany”. Technical Report 2121. IZA (Institute for the
Study of Labor) (May).
Fleming, T.R., Harrington, D.P. (1991). Counting Processes and Survival Analysis. Wiley, New York.
Flinn, C., Heckman, J.J. (1982). “New methods for analyzing structural models of labor force dynamics”.
Journal of Econometrics 18 (1), 115–168 (January).
Florens, J.-P., Mouchart, M. (1982). “A note on noncausality”. Econometrica 50 (3), 583–591 (May).
Foster, J.E., Sen, A.K. (1997). On Economic Inequality. Oxford University Press, New York.
Fréchet, M. (1951). “Sur les tableaux de corrélation dont les marges sont données”. Annales de l’Université
de Lyon A Series 3 14, 53–77.
Freedman, D.A. (2004). “On specifying graphical models for causation and the identification problem”. Evaluation Review 28 (4), 267–293 (August).
Freund, J.E. (1961). “A bivariate extension of the exponential distribution”. Journal of the American Statistical
Association 56 (296), 971–977 (December).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5297

Frisch, R. (1938). “Autonomy of economic relations”. Paper given at League of Nations. Reprinted in:
Hendry, D.F., Morgan, M.S. (Eds.), The Foundations of Econometric Analysis. Cambridge University
Press, 1995.
Geweke, J., Keane, M. (2001). “Computationally intensive methods for integration in econometrics”. In:
Heckman, J.J., Leamer, E. (Eds.), Handbook of Econometrics, vol. 5. North-Holland, New York,
pp. 3463–3568.
Ghez, G.R., Becker, G.S. (1975). The Allocation of Time and Goods over the Life Cycle. National Bureau of
Economic Research, New York.
Gill, R.D., Robins, J.M. (2001). “Causal inference for complex longitudinal data: The continuous case”. The
Annals of Statistics 29 (6), 1785–1811 (December).
Granger, C.W.J. (1969). “Investigating causal relations by econometric models and cross-spectral methods”.
Econometrica 37 (3), 424–438 (August).
Gritz, R.M. (1993). “The impact of training on the frequency and duration of employment”. Journal of Econometrics 57 (1–3), 21–51 (May–June).
Grubb, D. (2000). “Eligibility criteria for unemployment benefits”. In: Special Issue: Making Work Pay.
OECD Economic Studies 31, 147–184. OECD.
Haavelmo, T. (1943). “The statistical implications of a system of simultaneous equations”. Econometrica 11
(1), 1–12 (January).
Ham, J.C., LaLonde, R.J. (1996). “The effect of sample selection and initial conditions in duration models:
Evidence from experimental data on training”. Econometrica 64 (1), 175–205 (January).
Hansen, K.T., Heckman, J.J., Mullen, K.J. (2004). “The effect of schooling and ability on achievement test
scores”. Journal of Econometrics 121 (1–2), 39–98 (July–August).
Hansen, L.P., Heckman, J.J. (1996). “The empirical foundations of calibration”. Journal of Economic Perspectives 10 (1), 87–104 (Winter).
Hansen, L.P., Sargent, T.J. (1980). “Formulating and estimating dynamic linear rational expectations models”.
Journal of Economic Dynamics and Control 2 (1), 7–46 (February).
Heckman, J.J. (1974a). “Effects of child-care programs on women’s work effort”. Journal of Political Economy 82 (2), S136–S163 (March/April). Reprinted in: Schultz, T.W. (Ed.), Economics of the Family:
Marriage, Children and Human Capital. University of Chicago Press, 1974.
Heckman, J.J. (1974b). “Shadow prices, market wages and labor supply”. Econometrica 42 (4), 679–694
(July).
Heckman, J.J. (1976). “A life-cycle model of earnings, learning, and consumption”. In: Journal Special Issue:
Essays in Labor Economics in Honor of H. Gregg Lewis. Journal of Political Economy 84 (4, Part 2),
S11–S44 (August).
Heckman, J.J. (1979). “Sample selection bias as a specification error”. Econometrica 47 (1), 153–162 (January).
Heckman, J.J. (1981a). “Heterogeneity and state dependence”. In: Rosen, S. (Ed.), Studies in Labor Markets,
National Bureau of Economic Research. University of Chicago Press, pp. 91–139.
Heckman, J.J. (1981b). “The incidental parameters problem and the problem of initial conditions in estimating a discrete time-discrete data stochastic process and some Monte Carlo evidence”. In: Manski, C.,
McFadden, D. (Eds.), Structural Analysis of Discrete Data with Econometric Applications. MIT Press,
Cambridge, MA, pp. 179–185.
Heckman, J.J. (1981c). “Statistical models for discrete panel data”. In: Manski, C., McFadden, D. (Eds.),
Structural Analysis of Discrete Data with Econometric Applications. MIT Press, Cambridge, MA,
pp. 114–178.
Heckman, J.J. (1990). “Varieties of selection bias”. American Economic Review 80 (2), 313–318 (May).
Heckman, J.J. (1998). “The effects of government policies on human capital investment, unemployment and
earnings inequality”. In: Third Public GAAC Symposium: Labor Markets in the USA and Germany,
vol. 5. German–American Academic Council Foundation, Bonn, Germany.
Heckman, J.J. (2005). “The scientific model of causality”. Sociological Methodology 35 (1), 1–97 (August).

5298

J.H. Abbring and J.J. Heckman

Heckman, J.J., Borjas, G.J. (1980). “Does unemployment cause future unemployment? Definitions, questions
and answers from a continuous time model of heterogeneity and state dependence”. In: Special Issue on
Unemployment. Economica 47 (187), 247–283 (August).
Heckman, J.J., Honoré, B.E. (1989). “The identifiability of the competing risks model”. Biometrika 76 (2),
325–330 (June).
Heckman, J.J., Honoré, B.E. (1990). “The empirical content of the Roy model”. Econometrica 58 (5), 1121–
1149 (September).
Heckman, J.J., LaLonde, R.J., Smith, J.A. (1999). “The economics and econometrics of active labor market
programs”. In: Ashenfelter, O., Card, D. (Eds.), Handbook of Labor Economics, vol. 3A. North-Holland,
New York, pp. 1865–2097 (Chapter 31).
Heckman, J.J., Lochner, L.J., Taber, C. (1998a). “Explaining rising wage inequality: Explorations with a
dynamic general equilibrium model of labor earnings with heterogeneous agents”. Review of Economic
Dynamics 1 (1), 1–58 (January).
Heckman, J.J., Lochner, L.J., Taber, C. (1998b). “General-equilibrium treatment effects: A study of tuition
policy”. American Economic Review 88 (2), 381–386 (May).
Heckman, J.J., Lochner, L.J., Taber, C. (1998c). “Tax policy and human-capital formation”. American Economic Review 88 (2), 293–297 (May).
Heckman, J.J., Lochner, L.J., Todd, P.E. (2006). “Earnings equations and rates of return: The Mincer equation
and beyond”. In: Hanushek, E.A., Welch, F. (Eds.), Handbook of the Economics of Education. NorthHolland, Amsterdam, pp. 307–458.
Heckman, J.J., MaCurdy, T.E. (1980). “A life cycle model of female labour supply”. Review of Economic
Studies 47 (1), 47–74 (January).
Heckman, J.J., Navarro, S. (2004). “Using matching, instrumental variables and control functions to estimate
economic choice models”. Review of Economics and Statistics 86 (1), 30–57 (February).
Heckman, J.J., Navarro, S. (2005). “Empirical estimates of option values of education and information sets
in a dynamic sequential choice model”. Unpublished manuscript. University of Chicago, Department of
Economics.
Heckman, J.J., Navarro, S. (2007). “Dynamic discrete choice and dynamic treatment effects”. Journal of
Econometrics 136 (2), 341–396 (February).
Heckman, J.J., Robb, R. (1985). “Alternative methods for evaluating the impact of interventions”. In: Heckman, J., Singer, B. (Eds.), Longitudinal Analysis of Labor Market Data, vol. 10. Cambridge University
Press, New York, pp. 156–245.
Heckman, J.J., Robb, R. (1986). “Alternative methods for solving the problem of selection bias in evaluating
the impact of treatments on outcomes”. In: Wainer, H. (Ed.), Drawing Inferences from Self-Selected
Samples. Springer-Verlag, New York, pp. 63–107. Reprinted in: Lawrence Erlbaum Associates, Mahwah,
NJ, 2000.
Heckman, J.J., Robb, R., Walker, J.R. (1990). “Testing the mixture of exponentials hypothesis and estimating
the mixing distribution by the method of moments”. Journal of the American Statistical Association 85
(410), 582–589 (June).
Heckman, J.J., Singer, B.S. (1984). “Econometric duration analysis”. Journal of Econometrics 24 (1–2), 63–
132 (January–February).
Heckman, J.J., Singer, B.S. (1986). “Econometric analysis of longitudinal data”. In: Griliches, Z., Intriligator,
M.D. (Eds.), Handbook of Econometrics, vol. 3. North-Holland, pp. 1690–1763 (Chapter 29).
Heckman, J.J., Smith, J.A. (1993). “Assessing the case for randomized evaluation of social programs”. In:
Jensen, K., Madsen, P. (Eds.), Measuring Labour Market Measures: Evaluating the Effects of Active
Labour Market Policy Initiatives. Proceedings from the Danish Presidency Conference “Effects and
Measuring of Effects of Labour Market Policy Initiatives”. Denmark Ministry of Labour, Copenhagen,
pp. 35–95.
Heckman, J.J., Smith, J.A. (1995). “Assessing the case for social experiments”. Journal of Economic Perspectives 9 (2), 85–110 (Spring).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5299

Heckman, J.J., Smith, J.A. (1998). “Evaluating the welfare state”. In: Strom, S. (Ed.), Econometrics and
Economic Theory in the Twentieth Century: The Ragnar Frisch Centennial Symposium. Cambridge University Press, New York, pp. 241–318.
Heckman, J.J., Smith, J.A., Clements, N. (1997). “Making the most out of programme evaluations and social experiments: Accounting for heterogeneity in programme impacts”. Review of Economic Studies 64
(221), 487–536 (October).
Heckman, J.J., Stixrud, J., Urzua, S. (2006). “The effects of cognitive and noncognitive abilities on labor
market outcomes and social behavior”. Journal of Labor Economics 24 (3), 411–482 (July).
Heckman, J.J., Taber, C. (1994). “Econometric mixture models and more general models for unobservables
in duration analysis”. Statistical Methods in Medical Research 3 (3), 279–299.
Heckman, J.J., Tobias, J.L., Vytlacil, E.J. (2001). “Four parameters of interest in the evaluation of social
programs”. Southern Economic Journal 68 (2), 210–223 (October).
Heckman, J.J., Tobias, J.L., Vytlacil, E.J. (2003). “Simple estimators for treatment parameters in a latent
variable framework”. Review of Economics and Statistics 85 (3), 748–754 (August).
Heckman, J.J., Urzua, S., Vytlacil, E.J. (2006). “Understanding instrumental variables in models with essential heterogeneity”. Review of Economics and Statistics 88 (3), 389–432.
Heckman, J.J., Urzua, S., Yates, G. (2007). “The identification and estimation of option values in a model
with recurrent states”. Unpublished manuscript. University of Chicago, Department of Economics.
Heckman, J.J., Vytlacil, E.J. (1999). “Local instrumental variables and latent variable models for identifying and bounding treatment effects”. Proceedings of the National Academy of Sciences 96, 4730–4734
(April).
Heckman, J.J., Vytlacil, E.J. (2001). “Causal parameters, treatment effects and randomization”. Unpublished
manuscript. University of Chicago, Department of Economics.
Heckman, J.J., Vytlacil, E.J. (2005). “Structural equations, treatment effects and econometric policy evaluation”. Econometrica 73 (3), 669–738 (May).
Heckman, J.J., Vytlacil, E.J. (2007a). “Econometric evaluation of social programs, Part I: Causal models,
structural models and econometric policy evaluation”. In: Heckman, J., Leamer, E. (Eds.), Handbook of
Econometrics, vol. 6B. Elsevier, Amsterdam (Chapter 70 in this Handbook).
Heckman, J.J., Vytlacil, E.J. (2007b). “Econometric evaluation of social programs, Part II: Using the marginal treatment effect to organize alternative economic estimators to evaluate social programs, and to
forecast their effects in new environments”. In: Heckman, J., Leamer, E. (Eds.), Handbook of Econometrics, vol. 6B. Elsevier, Amsterdam (Chapter 71 in this Handbook).
Hendry, D.F., Morgan, M.S. (1995). The Foundations of Econometric Analysis. Cambridge University Press,
New York.
Hicks, J.R. (1946). Value and Capital: An Inquiry into Some Fundamental Principles of Economic Theory,
second ed. Clarendon Press, Oxford.
Hoeffding, W. (1940). “Masstabinvariante korrelationtheorie”. Schriften des Mathematischen Instituts und
des Instituts für Angewandte Mathematik und Universität Berlin 5, 197–233.
Holland, P.W. (1986). “Statistics and causal inference”. Journal of the American Statistical Association 81
(396), 945–960 (December).
Honoré, B.E. (1993). “Identification results for duration models with multiple spells”. Review of Economic
Studies 60 (1), 241–246 (January).
Honoré, B.E., Lewbel, A. (2002). “Semiparametric binary choice panel data models without strictly exogenous regressors”. Econometrica 70 (5), 2053–2063 (September).
Horowitz, J.L., Markatou, M. (1996). “Semiparametric estimation of regression models for panel data”. Review of Economic Studies 63 (1), 145–168 (January).
Hotz, V.J., Miller, R.A. (1988). “An empirical analysis of life cycle fertility and female labor supply”. Econometrica 56 (1), 91–118 (January).
Hotz, V.J., Miller, R.A. (1993). “Conditional choice probabilities and the estimation of dynamic models”.
Review of Economic Studies 60 (3), 497–529 (July).
Hu, Y., Schennach, S.M. (2006). “Identification and estimation of nonclassical nonlinear errors-in-variables
models with continuous distributions”. Working Paper. University of Chicago.

5300

J.H. Abbring and J.J. Heckman

Huggett, M. (1993). “The risk-free rate in heterogeneous-agent incomplete-insurance economies”. Journal of
Economic Dynamics and Control 17 (5–6), 953–969 (September–November).
Hurwicz, L. (1962). “On the structural form of interdependent systems”. In: Nagel, E., Suppes, P., Tarski, A.
(Eds.), Logic, Methodology and Philosophy of Science. Stanford University Press, pp. 232–239.
Johnson, G.E. (1979). “The labor market displacement effect in the analysis of the net impact of manpower
training programs”. In: Bloch, F. (Ed.), Evaluating Manpower Training Programs: Revisions of Papers
Originally Presented at the Conference on Evaluating Manpower Training Programs, Princeton University,
May 1976. JAI Press, Greenwich, CT.
Johnson, G.E., Layard, R. (1986). “The natural rate of unemployment: Explanation and policy”. In: Ashenfelter, O., Layard, R. (Eds.), Handbook of Labor Economics, vol. 2. North-Holland, New York, pp. 921–999.
Jöreskog, K.G. (1977). “Structural equations models in the social sciences: Specification, estimation and
testing”. In: Krishnaiah, P. (Ed.), Applications of Statistics. North-Holland, New York, pp. 265–287.
Jöreskog, K.G., Goldberger, A.S. (1975). “Estimation of a model with multiple indicators and multiple causes
of a single latent variable”. Journal of the American Statistical Association 70 (351), 631–639 (September).
Jorgenson, D.W., Slesnick, D.T. (1997). “General equilibrium analysis of economic policy”. In: Jorgenson,
D.W. (Ed.), Measuring Social Welfare. In: Welfare, vol. 2. MIT Press, Cambridge, MA, pp. 165–218.
Jorgenson, D.W., Yun, K.-Y. (1990). “Tax reform and U.S. economic growth”. Journal of Political Economy 98 (5), 151–193 (October).
Kalbfleisch, J.D., Prentice, R.L. (1980). The Statistical Analysis of Failure Time Data. Wiley, New York.
Kane, T.J. (1994). “College entry by blacks since 1970: The role of college costs, family background and the
returns to education”. Journal of Political Economy 102 (5), 878–911 (October).
Katz, L.F., Autor, D.H. (1999). “Changes in the wage structure and earnings inequality”. In: Ashenfelter,
O., Card, D. (Eds.), Handbook of Labor Economics, vol. 3. North-Holland, New York, pp. 1463–1555
(Chapter 25).
Keane, M.P., Wolpin, K.I. (1994). “The solution and estimation of discrete choice dynamic programming
models by simulation and interpolation: Monte Carlo evidence”. The Review of Economics and Statistics 76 (4), 648–672 (November).
Keane, M.P., Wolpin, K.I. (1997). “The career decisions of young men”. Journal of Political Economy 105
(3), 473–522 (June).
Kehoe, T.J., Srinivasan, T.N., Whalley, J. (2005). Frontiers in Applied General Equilibrium Modeling. Cambridge University Press, New York.
Keiding, N. (1999). “Event history analysis and inference from observational epidemiology”. Statistics in
Medicine 18 (17–18), 2353–2363 (September).
Kendall, M.G., Stuart, A. (1977). The Advanced Theory of Statistics, vol. 1, fourth ed. C. Griffen, London.
King, G. (1997). A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior from
Aggregate Data. Princeton University Press, Princeton, NJ.
Koenker, R. (2005). Quantile Regression. Cambridge University Press, New York.
Koenker, R., Xiao, Z. (2002). “Inference on the quantile regression process”. Econometrica 70 (4), 1583–1612
(July).
Koopmans, T.C., Beckmann, M. (1957). “Assignment problems and the location of economic activities”.
Econometrica 25 (1), 53–76 (January).
Kotlarski, I.I. (1967). “On characterizing the gamma and normal distribution”. Pacific Journal of Mathematics 20, 69–76.
Krusell, P., Smith, A.A. (1998). “Income and wealth heterogeneity in the macroeconomy”. Journal of Political
Economy 106 (5), 867–896 (October).
Kydland, F.E., Prescott, E.C. (1982). “Time to build and aggregate fluctuations”. Econometrica 50 (6), 1345–
1370 (November).
Kydland, F.E., Prescott, E.C. (1996). “The computational experiment: An econometric tool”. Journal of Economic Perspectives 10 (1), 69–85 (Winter).
Lancaster, T. (1979). “Econometric methods for the duration of unemployment”. Econometrica 47 (4), 939–
956 (July).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5301

Leamer, E.E. (1985). “Vector autoregressions for causal inference?”. Carnegie–Rochester Conference Series
on Public Policy 22, 255–303 (Spring).
Lechner, M., Miquel, R. (2002). “Identification of effects of dynamic treatments by sequential conditional
independence assumptions”. Discussion paper. University of St. Gallen, Department of Economics.
Lee, D. (2005). “An estimable dynamic general equilibrium model of work, schooling, and occupational
choice”. International Economic Review 46 (1), 1–34 (February).
Lee, D., Wolpin, K.I. (2006). “Intersectoral labor mobility and the growth of the service sector”. Econometrica 74 (1), 1–40 (January).
Lehmann, E.L., D’Abrera, H.J.M. (1975). Nonparametrics: Statistical Methods Based on Ranks. Holden–
Day, San Francisco.
Lewbel, A. (2000). “Semiparametric qualitative response model estimation with unknown heteroscedasticity
or instrumental variables”. Journal of Econometrics 97 (1), 145–177 (July).
Lewis, H.G. (1963). Unionism and Relative Wages in the United States: An Empirical Inquiry. University of
Chicago Press, Chicago.
Lise, J., Seitz, S., Smith, J. (2005a). “Equilibrium policy experiments and the evaluation of social programs”,
Working Paper 1076. Queen’s University, Department of Economics, Kingston, Ontario.
Lise, J., Seitz, S., Smith, J. (2005b). “Evaluating search and matching models using experimental data”.
Working Paper 1074. Queen’s University, Department of Economics, Kingston, Ontario.
Lok, J.J. (2007). “Statistical modelling of causal effects in continuous time”. Annals of Statistics (forthcoming).
MaCurdy, T.E. (1981). “An empirical model of labor supply in a life-cycle setting”. Journal of Political
Economy 89 (6), 1059–1085 (December).
Magnac, T., Thesmar, D. (2002). “Identifying dynamic discrete decision processes”. Econometrica 70 (2),
801–816 (March).
Manski, C.F. (1988). “Identification of binary response models”. Journal of the American Statistical Association 83 (403), 729–738 (September).
Manski, C.F. (1993). “Dynamic choice in social settings: Learning from the experiences of others”. Journal
of Econometrics 58 (1–2), 121–136 (July).
Manski, C.F. (1997). “The mixing problem in programme evaluation”. Review of Economic Studies 64 (4),
537–553 (October).
Manski, C.F. (2003). Partial Identification of Probability Distributions. Springer-Verlag, New York.
Manski, C.F. (2004). “Measuring expectations”. Econometrica 72 (5), 1329–1376 (September).
Mardia, K.V. (1970). Families of Bivariate Distributions. Griffin, London.
Matzkin, R.L. (1992). “Nonparametric and distribution-free estimation of the binary threshold crossing and
the binary choice models”. Econometrica 60 (2), 239–270 (March).
Matzkin, R.L. (1993). “Nonparametric identification and estimation of polychotomous choice models”. Journal of Econometrics 58 (1–2), 137–168 (July).
Matzkin, R.L. (1994). “Restrictions of economic theory in nonparametric methods”. In: Engle, R., McFadden,
D. (Eds.), Handbook of Econometrics, vol. 4. North-Holland, New York, pp. 2523–2558.
Matzkin, R.L. (2003). “Nonparametric estimation of nonadditive random functions”. Econometrica 71 (5),
1339–1375 (September).
Matzkin, R.L. (2007). “Nonparametric identification”. In: Heckman, J.J., Leamer, E. (Eds.), Handbook of
Econometrics, vol. 6B. Elsevier, Amsterdam (Chapter 73 in this Handbook).
Meyer, B.D. (1990). “Unemployment insurance and unemployment spells”. Econometrica 58 (4), 757–782.
(July).
Meyer, B.D. (1996). “What have we learned from the Illinois reemployment bonus experiment?”. Journal of
Labor Economics 14 (1), 26–51 (January).
Miller, R.A. (1984). “Job matching and occupational choice”. Journal of Political Economy 92 (6), 1086–
1120 (December).
Mincer, J. (1974). Schooling, Experience and Earnings. Columbia University Press for National Bureau of
Economic Research, New York.

5302

J.H. Abbring and J.J. Heckman

Mortensen, D.T. (1977). “Unemployment insurance and job search decisions”. Industrial and Labor Relations
Review 30 (4), 505–517 (July).
Mortensen, D.T. (1986). “Job search and labor market analysis”. In: Ashenfelter, O., Card, D. (Eds.), Handbook of Labor Economics. In: Handbooks in Economics, vol. 2. Elsevier Science, New York, pp. 849–919.
Mortensen, D.T., Pissarides, C.A. (1994). “Job creation and job destruction in the theory of unemployment”.
Review of Economic Studies 61 (3), 397–415 (July).
Murphy, S.A. (2003). “Optimal dynamic treatment regimes”. Journal of the Royal Statistical Society, Series
B 65 (2), 331–366 (May).
Navarro, S. (2005). “Understanding schooling: Using observed choices to infer agent’s information in a dynamic model of schooling choice when consumption allocation is subject to borrowing constraints”. PhD
Dissertation. University of Chicago, Chicago, IL.
Organization for Economic Cooperation and Development (1993). “Active labour market policies: Assessing
macroeconomic and microeconomic effects”. In: Employment Outlook. OECD, Paris, pp. 39–67.
Pakes, A. (1986). “Patents as options: Some estimates of the value of holding European patent stocks”. Econometrica 54 (4), 755–784 (July).
Pakes, A., Simpson, M. (1989). “Patent renewal data”. Brookings Papers on Economic Activity, 331–401
(special issue).
Pearl, J. (2000). Causality. Cambridge University Press, Cambridge, England.
Pissarides, C.A. (2000). Equilibrium Unemployment Theory. MIT Press, Cambridge, MA.
Prakasa-Rao, B.L.S. (1992). Identifiability in Stochastic Models: Characterization of Probability Distributions. Probability and Mathematical Statistics. Academic Press, Boston.
Ridder, G. (1986). “An event history approach to the evaluation of training, recruitment and employment
programmes”. Journal of Applied Econometrics 1 (2), 109–126 (April).
Ridder, G. (1990). “The non-parametric identification of generalized accelerated failure-time models”. Review of Economic Studies 57 (2), 167–181 (April).
Robins, J.M. (1989). “The analysis of randomized and non-randomized aids treatment trials using a new
approach to causal inference in longitudinal studies”. In: Sechrest, L., Freeman, H., Mulley, A. (Eds.),
Health Services Research Methodology: A Focus on AIDS. U.S. Department of Health and Human Services, National Center for Health Services Research and Health Care Technology Assessment, Rockville,
MD, pp. 113–159.
Robins, J.M. (1997). “Causal inference from complex longitudinal data”. In: Berkane, M. (Ed.), Latent Variable Modeling and Applications to Causality. In: Lecture Notes in Statistics. Springer-Verlag, New York,
pp. 69–117.
Rosenzweig, M.R., Wolpin, K.I. (2000). “Natural “natural experiments” in economics”. Journal of Economic
Literature 38 (4), 827–874 (December).
Roy, A. (1951). “Some thoughts on the distribution of earnings”. Oxford Economic Papers 3 (2), 135–146
(June).
Rubin, D.B. (1986). “Statistics and causal inference: Comment: Which ifs have causal answers”. Journal of
the American Statistical Association 81 (396), 961–962.
Rüschendorf, L. (1981). “Sharpness of Fréchet bounds”. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 41, 293–302.
Rust, J. (1987). “Optimal replacement of GMC bus engines: An empirical model of Harold Zurcher”. Econometrica 55 (5), 999–1033 (September).
Rust, J. (1994). “Structural estimation of Markov decision processes”. In: Engle, R., McFadden, D. (Eds.),
Handbook of Econometrics. North-Holland, New York, pp. 3081–3143.
Sargent, T.J., Sims, C.A. (1977). “Business cycle modeling without much a priori economic theory”. In:
New Methods in Business Cycle Research: Proceedings from a Conference. Federal Reserve Bank of
Minneapolis, Minneapolis.
Schennach, S.M. (2004). “Estimation of nonlinear models with measurement error”. Econometrica 72 (1),
33–75 (January).
Shoven, J.B., Whalley, J. (1977). “Equal yield tax alternatives: General equilibrium computational techniques”. Journal of Public Economics 8 (2), 211–224 (October).

Ch. 72:

Econometric Evaluation of Social Programs, Part III

5303

Sims, C.A. (1972). “Money, income and causality”. American Economic Review 62 (4), 540–552 (September).
Sims, C.A. (1996). “Macroeconomics and methodology”. Journal of Economic Perspectives 10 (1), 105–120.
Stefanski, L.A., Carroll, R.J. (1991). “Deconvolution-based score tests in measurement error models”. The
Annals of Statistics 19 (1), 249–259 (March).
Taber, C.R. (2000). “Semiparametric identification and heterogeneity in discrete choice dynamic programming models”. Journal of Econometrics 96 (2), 201–229 (June).
Tchen, A.H. (1980). “Inequalities for distributions with given marginals”. Annals of Probability 8 (4), 814–
827 (August).
Van den Berg, G.J. (1999). “Empirical inference with equilibrium search models of the labour market”. Economic Journal 109 (456), F283–F306 (June).
Van den Berg, G.J. (2001). “Duration models: Specification, identification and multiple durations”. In: Heckman, J.J., Leamer, E. (Eds.), Handbook of Econometrics. In: Handbooks in Economics, vol. 5. NorthHolland, New York, pp. 3381–3460.
Van den Berg, G.J., Holm, A., Van Ours, J.C. (2002). “Do stepping-stone jobs exist? Early career paths in the
medical profession”. Journal of Population Economics 15 (4), 647–665 (November).
Van den Berg, G.J., Van der Klaauw, B., Van Ours, J.C. (2004). “Punitive sanctions and the transition rate
from welfare to work”. Journal of Labor Economics 22 (1), 211–241 (January).
Van der Laan, M.J., Robins, J.M. (2003). Unified Methods for Censored Longitudinal Data and Causality.
Springer-Verlag, New York.
Willis, R.J., Rosen, S. (1979). “Education and self-selection”. Journal of Political Economy 87 (5, Part 2),
S7–S36 (October).
Wolpin, K.I. (1984). “An estimable dynamic stochastic model of fertility and child mortality”. Journal of
Political Economy 92 (5), 852–874 (October).
Wolpin, K.I. (1987). “Estimating a structural search model: The transition from school to work”. Econometrica 55 (4), 801–817 (July).
Wolpin, K.I. (1992). “The determinants of black–white differences in early employment careers: Search,
layoffs, quits, and endogenous wage growth”. Journal of Political Economy 100 (3), 535–560.

