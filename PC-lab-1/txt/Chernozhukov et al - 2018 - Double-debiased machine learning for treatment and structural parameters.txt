Econometrics Journal (2018), volume 21, pp. C1â€“C68.
doi: 10.1111/ectj.12097

Double/debiased machine learning for treatment
and structural parameters
V ICTOR C HERNOZHUKOV â€  , D ENIS C HETVERIKOV â€¡ , M ERT D EMIRER â€  ,
E STHER D UFL O â€  , C HRISTIAN H ANSEN Â§ , W HITNEY N EWEY â€ 
AND JAMES R OBINS 
â€ 

Massachusetts Institute of Technology, 50 Memorial Drive, Cambridge, MA 02139, USA .
E-mail: vchern@mit.edu, mdemirer@mit.edu, duflo@mit.edu, wnewey@mit.edu
â€¡

University of California Los Angeles, 315 Portola Plaza, Los Angeles, CA 90095, USA .
E-mail: chetverikov@econ.ucla.edu
Â§

University of Chicago, 5807 S. Woodlawn Ave., Chicago, IL 60637, USA .
E-mail: chansen1@chicagobooth.edu


Harvard University, 677 Huntington Avenue, Boston, MA 02115, USA .
E-mail: robins@hsph.harvard.edu

First version received: October 2016; final version accepted: June 2017
Summary We revisit the classic semi-parametric problem of inference on a low-dimensional
parameter Î¸0 in the presence of high-dimensional nuisance parameters Î·0 . We depart from the
classical setting by allowing for Î·0 to be so high-dimensional that the traditional assumptions
(e.g. Donsker properties) that limit complexity of the parameter space for this object break
down. To estimate Î·0 , we consider the use of statistical or machine learning (ML) methods,
which are particularly well suited to estimation in modern, very high-dimensional cases.
ML methods perform well by employing regularization to reduce variance and trading
off regularization bias with overfitting in practice. However, both regularization bias and
overfitting in estimating Î·0 cause a heavy bias in estimators of Î¸0 that are obtained by
naively plugging ML estimators of Î·0 into estimating equations for Î¸0 . This bias results in
the naive estimator failing to be N âˆ’1/2 consistent, where N is the sample size. We show that
the impact of regularization bias and overfitting on estimation of the parameter of interest Î¸0
can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal
moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate
Î¸0 ; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We
call the resulting set of methods double or debiased ML (DML). We verify that DML
delivers point estimators that concentrate in an N âˆ’1/2 -neighbourhood of the true parameter
values and are approximately unbiased and normally distributed, which allows construction
of valid confidence statements. The generic statistical theory of DML is elementary and
simultaneously relies on only weak theoretical requirements, which will admit the use of a
broad array of modern ML methods for estimating the nuisance parameters, such as random
forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of
these methods. We illustrate the general theory by applying it to provide theoretical properties
of the following: DML applied to learn the main regression parameter in a partially linear
regression model; DML applied to learn the coefficient on an endogenous variable in a
partially linear instrumental variables model; DML applied to learn the average treatment
effect and the average treatment effect on the treated under unconfoundedness; DML applied

C

2017 Royal Economic Society. Published by John Wiley & Sons Ltd, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA, 02148, USA.

C2

V. Chernozhukov et al.
to learn the local average treatment effect in an instrumental variables setting. In addition to
these theoretical applications, we also illustrate the use of DML in three empirical examples.

1. INTRODUCTION AND MOTIVATION
Motivation
We develop a series of simple results for obtaining root-N consistent estimation, where N is
the sample size, and valid inferential statements about a low-dimensional parameter of interest,
Î¸0 , in the presence of a high-dimensional or â€˜highly complexâ€™ nuisance parameter, Î·0 . The
parameter of interest will typically be a causal parameter or treatment effect parameter, and we
consider settings in which the nuisance parameter will be estimated using machine learning (ML)
methods, such as random forests, lasso or post-lasso, neural nets, boosted regression trees, and
various hybrids and ensembles of these methods. These ML methods are able to handle many
covariates and they provide natural estimators of nuisance parameters when these parameters are
highly complex. Here, highly complex formally means that the entropy of the parameter space
for the nuisance parameter is increasing with the sample size in a way that moves us outside the
traditional framework considered in the classical semi-parametric literature where the complexity
of the nuisance parameter space is taken to be sufficiently small. The main contribution of this
paper is to offer a general and simple procedure for estimating and to perform inference on Î¸0
that is formally valid in these highly complex settings.
E XAMPLE 1.1. (PARTIALLY L INEAR R EGRESSION) As a lead example, consider the following
partially linear regression (PLR) model as in Robinson (1988):
Y = DÎ¸0 + g0 (X) + U,
D = m0 (X) + V ,

E[U | X, D] = 0,

(1.1)

E[V | X] = 0.

(1.2)

Here, Y is the outcome variable, D is the policy/treatment variable of interest, vector
X = (X1 , . . . , Xp )
consists of other controls, and U and V are disturbances.1 The first equation is the main equation,
and Î¸0 is the main regression coefficient that we would like to infer. If D is exogenous conditional
on controls X, Î¸0 has the interpretation of the treatment effect parameter or â€˜liftâ€™ parameter in
business applications. The second equation keeps track of confounding, namely the dependence
of the treatment variable on controls. This equation is not of interest per se but it is important
for characterizing and removing regularization bias. The confounding factors X affect the policy
variable D via the function m0 (X) and the outcome variable via the function g0 (X). In many
applications, the dimension p of vector X is large relative to N . To capture the feature that p is not
vanishingly small relative to the sample size, modern analyses then model p as increasing with
the sample size, which causes traditional assumptions that limit the complexity of the parameter
space for the nuisance parameters Î·0 = (m0 , g0 ) to fail.
1 We consider the case where D is a scalar for simplicity. Extension to the case where D is a vector of fixed, finite
dimension is accomplished by introducing an equation such as (1.2) for each element of the vector.


C

2017 Royal Economic Society.

C3

Double/debiased machine learning

Regularization bias. A naive approach to estimation of Î¸0 using ML methods would be, for
example, to construct a sophisticated ML estimator D Î¸Ì‚0 + gÌ‚0 (X) for learning the regression
function DÎ¸0 + g0 (X).2 Suppose, for the sake of clarity, that we randomly split the sample into
two parts: a main part of size n, with observation numbers indexed by i âˆˆ I , and an auxiliary
part of size N âˆ’ n, with observations indexed by i âˆˆ I c . For simplicity, we take n = N/2 for the
moment and we turn to more general cases that cover unequal split-sizes, using more than one
split, and achieving the same efficiency as if the full sample were used for estimating Î¸0 in the
formal development in Section 3. Suppose gÌ‚0 is obtained using the auxiliary sample and that,
given this gÌ‚0 , the final estimate of Î¸0 is obtained using the main sample:
 1  âˆ’1 1 
D2
Di (Yi âˆ’ gÌ‚0 (Xi )).
(1.3)
Î¸Ì‚0 =
n iâˆˆI i
n iâˆˆI
âˆš
The estimator Î¸Ì‚0 will generally have a slower than 1/ n rate of convergence, namely,
âˆš
p
| n(Î¸Ì‚0 âˆ’ Î¸0 )| â†’ âˆž.

(1.4)

As detailed below, the driving force behind this â€˜inferiorâ€™ behaviour is the bias in learning g0 .
To heuristically illustrate the impact of the bias in learning g0 , we can decompose the scaled
estimation error in Î¸Ì‚0 as
 1  âˆ’1 1 
 1  âˆ’1 1 
âˆš
n(Î¸Ì‚0 âˆ’ Î¸0 ) =
Di2
D i Ui +
D2
Di (g0 (Xi ) âˆ’ gÌ‚0 (Xi )) .
âˆš
âˆš
n iâˆˆI
n iâˆˆI i
n iâˆˆI
n iâˆˆI


 


:=a

:=b

Â¯ for some .
Â¯ Term b
The first term is well behaved under mild conditions, obeying a  N(0, )
is the regularization bias term, which is not centred and diverges in general. Indeed, we have
1 
m0 (Xi )(g0 (Xi ) âˆ’ gÌ‚0 (Xi )) + oP (1)
b = (E[Di 2 ])âˆ’1 âˆš
n iâˆˆI
to the first order. Heuristically, b isâˆš the sum of n terms that do not have mean zero,
m0 (Xi )(g0 (Xi ) âˆ’ gÌ‚0 (Xi )), divided by n. These terms have non-zero mean because, in highdimensional or otherwise highly complex settings, we must employ regularized estimators â€“
such as lasso, ridge, boosting or penalized neural nets â€“ for informative learning to be feasible.
The regularization in these estimators keeps the variance of the estimator from exploding but
also necessarily induces substantive biases in the estimator gÌ‚0 of g0 . Specifically, the rate of
convergence of (the bias of) gÌ‚0 to g0 in the root-mean-squaredâˆšerror sense will typically be nâˆ’Ï•g
with Ï•g < 1/2. Hence, we expect b to be of stochastic order nnâˆ’Ï•g â†’ âˆž as Di is centred at
m0 (Xi ) = 0, which then implies (1.4).
Overcoming
construction
the effect of
we obtain VÌ‚

regularization biases using orthogonalization. Now consider a second
that employs an orthogonalized formulation obtained by directly partialling out
X from D to obtain the orthogonalized regressor V = D âˆ’ m0 (X). Specifically,
= D âˆ’ mÌ‚0 (X), where mÌ‚0 is an ML estimator of m0 obtained using the auxiliary

2 For instance, we could use lasso if we believe g is well approximated by a sparse linear combination of pre-specified
0
functions of X. In other settings, we could, for example, use iterative methods that alternate between random forests, for
estimating g0 , and least squares, for estimating Î¸0 .


C

2017 Royal Economic Society.

C4

V. Chernozhukov et al.

sample of observations. We are now solving an auxiliary prediction problem to estimate the
conditional mean of D given X, so we are doing â€˜double predictionâ€™ or â€˜double machine
learningâ€™.
After partialling the effect of X out from D and obtaining a preliminary estimate of g0 from
the auxiliary sample as before, we can formulate the following debiased ML (DML) estimator
for Î¸0 using the main sample of observations:3
âˆ’1 1 
1 
VÌ‚i Di
VÌ‚i (Yi âˆ’ gÌ‚0 (Xi )).
(1.5)
Î¸ÌŒ0 =
n iâˆˆI
n iâˆˆI
By approximately orthogonalizing D with respect to X and approximately removing the direct
effect of confounding by subtracting an estimate of g0 , Î¸ÌŒ0 removes the effect of regularization
bias that contaminates (1.3). The formulation of Î¸ÌŒ0 also provides direct links to both the classical
econometric literature, as the estimator can clearly be interpreted as a linear instrumental variable
(IV) estimator, and to the more recent literature on debiased lasso in the context where g0 is taken
to be well approximated by a sparse linear combination of pre-specified functions of X; see, e.g.,
Belloni et al. (2013, 2014a,b), Javanmard and Montanari (2014b), van de Geer et al. (2014) and
Zhang and Zhang (2014).4
To illustrate the benefits of the auxiliary prediction step and the estimation of Î¸0 with Î¸ÌŒ0 , we
sketch the properties of Î¸ÌŒ0 here. We can decompose the scaled estimation error of Î¸ÌŒ0 into three
components:
âˆš
n(Î¸ÌŒ0 âˆ’ Î¸0 ) = a âˆ— + bâˆ— + câˆ— .
The leading term, a âˆ— , will satisfy
1 
Vi Ui  N(0, )
a âˆ— = (E[V 2 ])âˆ’1 âˆš
n iâˆˆI
under mild conditions. The second term, bâˆ— , captures the impact of regularization bias in
estimating g0 and m0 . Specifically, we will have
1 
(mÌ‚0 (Xi ) âˆ’ m0 (Xi ))(gÌ‚0 (Xi ) âˆ’ g0 (Xi )),
bâˆ— = (E[V 2 ])âˆ’1 âˆš
n iâˆˆI
which now depends on the product of the estimation errors in mÌ‚0 and gÌ‚0 . Because this term
depends only on the product of the estimation errors, it can vanish
under a broad range of dataâˆš
generating processes. Indeed, this term is upper-bounded by nnâˆ’(Ï•m +Ï•g ) , where nâˆ’Ï•m and nâˆ’Ï•g
are respectively the rates of convergence of mÌ‚0 to m0 and gÌ‚0 to g0 ; this upper bound can clearly
vanish even though both m0 and g0 are estimated at relatively slow rates. Verifying that Î¸ÌŒ0 has
good properties then requires that the remainder term, câˆ— , is sufficiently well behaved. Sample

3

In Section 4, we also consider another debiased estimator, based on the partialling-out approach of Robinson (1988):
Î¸ÌŒ0 =

1
VÌ‚i VÌ‚i
n
iâˆˆI

âˆ’1

1
VÌ‚i (Yi âˆ’ Ë†0 (Xi )),
n

0 (X) = E[Y |X].

iâˆˆI

4 Each of these works differs in terms of detail but can be viewed through the lens of either debiasing or
orthogonalization to alleviate the impact of regularization bias on subsequent estimation and inference.


C

2017 Royal Economic Society.

Double/debiased machine learning

C5

Figure 1. Comparison of the conventional and double ML estimators. [Colour figure can be viewed at
wileyonlinelibrary.com]

splitting will play a key role in allowing us to guarantee that câˆ— = oP (1) under weak conditions
as outlined below and discussed in detail in Section 3.
Figure 1 provides a numerical illustration of the negative impact of regularization bias and
the benefit of orthogonalization. The left panel shows the behaviour of a conventional (nonorthogonal) ML estimator, Î¸Ì‚0 , in the partially linear model in a simple simulation experiment
where we learn g0 using a random forest. The g0 in this experiment is a very smooth function of a
small number of variables, so the experiment is seemingly favourable to the use of random forests
a priori. The histogram shows the simulated distribution of the centred estimator, Î¸Ì‚0 âˆ’ Î¸0 . The
estimator is badly biased, shifted much to the right relative to the true value Î¸0 . The distribution
of the estimator (approximated by the blue histogram) is substantively different from a normal
approximation (shown by the red curve) derived under the assumption that the bias is negligible.
The right panel shows the behaviour of the orthogonal, DML estimator, Î¸ÌŒ0 , in the partially linear
model in a simple experiment where we learn nuisance functions using random forests. Note
that the simulated data are exactly the same as those underlying in the left panel. The simulated
distribution of the centred estimator, Î¸ÌŒ0 âˆ’ Î¸0 (given by the blue histogram) illustrates that the
estimator is approximately unbiased, concentrates around Î¸0 , and is well approximated by the
normal approximation obtained in Section 3 (shown by the red curve).
The role of sample splitting in removing bias induced by overfitting. Our analysis makes use
of sample splitting, which plays a key role in establishing that remainder terms, such as câˆ— ,
vanish in probability. In the partially linear model, we find that the remainder câˆ— contains terms
such as
1 
Vi (gÌ‚0 (Xi ) âˆ’ g0 (Xi )),
âˆš
n iâˆˆI

(1.6)

âˆš
which involve 1/ n-normalized sums of products of structural unobservables from model (1.1)â€“
(1.2) with estimation errors in learning the nuisance functions g0 and m0 . The use of sample
splitting allows simple and tight control of such terms. To see this, assume that observations
are independent and recall that gÌ‚0 is estimated using only observations in the auxiliary sample.

C

2017 Royal Economic Society.

C6

V. Chernozhukov et al.

Then, conditioning on the auxiliary sample and recalling that E[Vi |Xi ] = 0, it is easy to verify
that term (1.6) has mean zero and variance of order
1
p
(gÌ‚0 (Xi ) âˆ’ g0 (Xi ))2 â†’ 0.
n iâˆˆI
Thus, the term (1.6) vanishes in probability by Chebyshevâ€™s inequality.
While sample splitting allows us to deal with remainder terms such as câˆ— , its direct application
does have the drawback that the estimator of the parameter of interest only makes use of the main
sample, which can result in a substantial loss of efficiency as we are only making use of a subset
of the available data. However, we can flip the role of the main and auxiliary samples to obtain
a second version of the estimator of the parameter of interest. By averaging the two resulting
estimators, we can regain full efficiency. Indeed, the two estimators will be approximately
independent, so simply averaging them offers an efficient procedure. We call this sample-splitting
procedure â€“ where we swap the roles of main and auxiliary samples to obtain multiple estimates
and then average the results â€“ â€˜cross-fittingâ€™. We formally define this procedure and discuss a
K-fold version of cross-fitting in Section 3.
Without sample splitting, terms such as (1.6) might not vanish and can lead to poor
performance of estimators of Î¸0 . The difficulty arises because model errors, such as Vi , and
estimation errors, such as gÌ‚0 (Xi ) âˆ’ g0 (Xi ), are generally related because the data for observation
i are used in forming the estimator gÌ‚0 . The association can then lead to poor performance of
an estimator of Î¸0 that makes use of gÌ‚0 as a plug-in estimator for g0 even when this estimator
converges at a very favourable rate, say N âˆ’1/2+ .
As an artificial but illustrative example of the problems that can result from overfitting, let
gÌ‚0 (Xi ) = g0 (Xi ) + (Yi âˆ’ g0 (Xi ))/N 1/2âˆ’ for any i in the sample used to form estimator gÌ‚0 , and
note that the second term provides a simple model that captures overfitting of the outcome
variable within the estimation sample. This estimator is excellent in terms of rates. If Ui and
Di are bounded, gÌ‚0 converges uniformly to g0 at the nearly parametric rate N âˆ’1/2+ . Despite this
fast rate of convergence, term câˆ— now explodes if we do not use sample splitting. For example,
suppose that the full sample is used to estimate both gÌ‚0 and Î¸ÌŒ0 . A simple calculation then reveals
that term câˆ— becomes
N
1 
Vi (gÌ‚0 (Xi ) âˆ’ g0 (Xi )) âˆ N  â†’ âˆž.
âˆš
N i=1

This bias due to overfitting is illustrated in the left panel of Figure 2. The histogram in the
figure gives a simulated distribution for the studentized Î¸ÌŒ resulting from using the full sample and
the contrived estimator gÌ‚(Xi ) given above. We can see that the histogram is shifted markedly to
the left demonstrating substantial bias resulting from overfitting. The right panel of Figure 2 also
illustrates that this bias is completely removed by sample splitting. The results in the right panel
of Figure 2 make use of the twofold cross-fitting procedure discussed above using the estimator
Î¸ÌŒ and the contrived estimator gÌ‚(Xi ) exactly as in the left panel. The difference is that gÌ‚(Xi ) is
formed in one half of the sample and then Î¸ÌŒ is estimated using the other half of the sample. This
procedure is then repeated, swapping the roles of the two samples, and the results are averaged.
We can see that the substantial bias from the full sample estimator has been removed and that the
spread of the histogram corresponding to the cross-fitting estimator is roughly the same as that
of the full sample estimator, clearly illustrating the bias-reduction property and efficiency of the
cross-fitting procedure.

C

2017 Royal Economic Society.

C7

Double/debiased machine learning

Figure 2. Comparison of full-sample and cross-fitting procedures. [Colour figure can be viewed at
wileyonlinelibrary.com]

A less contrived example that highlights the improvements brought by sample splitting is the
sparse high-dimensional IV model analysed in Belloni et al. (2012). Specifically, they consider
the IV model
Y = DÎ¸0 + ,
where E[|D] = 0 but instruments Z exist such that E[D|Z] is not a constant and E[|Z] =
0. Within this model, Belloni et al. (2012) focus on the problem of estimating the optimal
instrument, Î·0 (Z) = E[D|Z], using lasso-type methods. If Î·0 (Z) is approximately sparse in the
sense that only s terms of the dictionary of series transformations B(Z) = (B1 (Z), . . . , Bp (Z))
n to
are needed to approximate the function accurately, Belloni et al. (2012) require that s 2
establish their asymptotic results when sample splitting is not used. However, they show that
these results continue to hold under the much weaker requirement that s
n if one employs
sample splitting. We note that this example provides a prototypical example where Neyman
orthogonality holds and ML methods can usefully be adopted to aid in learning structural
parameters of interest. We also note that the weaker conditions required when using sample
splitting would also carry over to sparsity-based estimators in the partially linear model cited
above. We discuss this in more detail in Section 4.
While we find substantial appeal in using sample splitting, one can also use empirical
process methods to verify that biases introduced due to overfitting are negligible. For
example,
consider the problematic term in the partially linear model described previously,
âˆš
(1/ n) iâˆˆI Vi (gÌ‚0 (Xi ) âˆ’ g0 (Xi )). This term is clearly bounded by
1 
sup âˆš
Vi (g(Xi ) âˆ’ g0 (Xi )) ,
n iâˆˆI
gâˆˆGN

(1.7)

where GN is the smallest class of functions that contains estimators of g0 , gÌ‚, with high probability.
In conventional semi-parametric statistical and econometric analysis, the complexity of GN is
controlled by invoking Donsker conditions, which allow verification that terms such as (1.7)
vanish asymptotically. Importantly, Donsker conditions require that GN has bounded complexity,

C

2017 Royal Economic Society.

C8

V. Chernozhukov et al.

specifically a bounded entropy integral. Because of the latter property, Donsker conditions are
inappropriate in settings using ML methods where the dimension of X is modelled as increasing
with the sample size and estimators necessarily live in highly complex spaces. For example,
Donsker conditions rule out even the simplest linear parametric model with high-dimensional
regressors with parameter space given by the Euclidean ball with the unit radius:
GN = {x â†’ g(x) = x Î¸ ; Î¸ âˆˆ RpN : Î¸  â‰¤ 1}.
The entropy of this model, as measured by the logarithm of the covering number, grows at the
rate pN . Without invoking Donsker conditions, one can still show that terms such as (1.7) vanish
as long as the entropy of GN does not increase with N too rapidly. A fairly general treatment is
given by Belloni et al. (2017) who provide a set of conditions under which terms, such as câˆ— , can
vanish making use of the full sample. However, these conditions on the growth of entropy could
result in unnecessarily strong restrictions on model complexity, such as very strict requirements
on sparsity in the context of lasso estimation, as demonstrated in the IV example mentioned
above. Sample splitting allows one to obtain good results under very weak conditions.
Neyman orthogonality and moment conditions. Now we turn to a generalization of the
orthogonalization principle above. The first â€˜conventionalâ€™ estimator Î¸Ì‚0 given in (1.3) can be
viewed as a solution to estimating equations
1
Ï•(W ; Î¸Ì‚0 , gÌ‚0 ) = 0,
n iâˆˆI
where Ï• is a known score function and gÌ‚0 is the estimator of the nuisance parameter g0 . For
example, in the partially linear model above, the score function is Ï•(W ; Î¸, g) = (Y âˆ’ Î¸ D âˆ’
g(X))D. It is easy to see that this score function Ï• is sensitive to biased estimation of g.
Specifically, the Gateaux derivative operator with respect to g does not vanish:5
âˆ‚g E[Ï•(W ; Î¸0 , g0 )][g âˆ’ g0 ] = 0.
The proofs of the general results in Section 3 show that this termâ€™s vanishing is a key to
establishing good behaviour of an estimator for Î¸0 .
By contrast, the orthogonalized or double/debiased ML estimator Î¸ÌŒ0 given in (1.5) solves
1
Ïˆ(W ; Î¸ÌŒ0 , Î·Ì‚0 ) = 0,
n iâˆˆI
where Î·Ì‚0 is the estimator of the nuisance parameter Î·0 and Ïˆ is an orthogonalized or debiased
score function that satisfies the property that the Gateaux derivative operator with respect to Î·
vanishes when evaluated at the true parameter values:
âˆ‚Î· E[Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] = 0.

(1.8)

We refer to property (1.8) as Neyman orthogonality and to Ïˆ as the Neyman orthogonal score
function due to the fundamental contributions in Neyman (1959, 1979), where this notion was
introduced. Intuitively, the Neyman orthogonality condition means that the moment conditions
used to identify Î¸0 are locally insensitive to the value of the nuisance parameter, which allows one
5

See Section 2 for the definition of the Gateaux derivative operator.

C

2017 Royal Economic Society.

Double/debiased machine learning

C9

to plug in noisy estimates of these parameters without strongly violating the moment condition.
In the partially linear model (1.1)â€“(1.2), the estimator Î¸ÌŒ0 uses the score function Ïˆ(W ; Î¸, Î·) =
(Y âˆ’ DÎ± âˆ’ g(X))(D âˆ’ m(X)), with the nuisance parameter being Î· = (m, g). It is easy to see
that these score functions Ïˆ are not sensitive to biased estimation of Î·0 in the sense that (1.8)
holds. The proofs of the general results in Section 3 show that this property and sample splitting
are two generic keys that allow us to establish good behaviour of an estimator for Î¸0 .
Literature overview
Our paper builds upon two important bodies
âˆš of research within the semi-parametric literature.
The first is the literature on obtaining N-consistent and asymptotically normal estimates
of low-dimensional objects in the presence of high-dimensional or non-parametric nuisance
functions. The second is the literature on the use of sample splitting to relax entropy conditions.
We provide links to each of these bodies of literature in turn.
The problem we study is obviously âˆšrelated to the classical semi-parametric estimation
framework, which focuses on obtaining N-consistent and asymptotically normal estimates
for low-dimensional components with nuisance parameters estimated by conventional nonparametric estimators, such as kernels or series. See, for example, the work by Levit (1975),
Ibragimov and Hasminskii (1981), Bickel (1982), Robinson (1988), Newey (1990, 1994), van der
Vaart (1991), Andrews (1994a), Newey et al. (1998, 2004), Robins and Rotnitzky (1995), Linton
(1996), Bickel et al. (1998), Chen et al. (2003), van der Laan and Rose (2011) and Ai and Chen
(2012). Neyman orthogonality (1.8), introduced by Neyman (1959), plays a key role in optimal
testing theory and adaptive estimation, semi-parametric learning theory and econometrics, and,
more recently, targeted learning theory. For example, Andrews (1994a), Newey (1994) and
van der Vaart (1998) provide a general set of results on estimation of a low-dimensional
parameter Î¸0 in the presence of nuisance parameters Î·0 . Andrews (1994a) uses Neyman
orthogonality (1.8) and Donsker conditions to demonstrate the key equicontinuity condition

 p
1 
Ïˆ(Wi ; Î¸0 , Î·Ì‚) âˆ’ Ïˆ(w; Î¸0 , Î·Ì‚)dP(w) âˆ’ Ïˆ(Wi ; Î¸0 , Î·0 ) â†’ 0,
âˆš
n iâˆˆI
which reduces to (1.6) in the PLR model. Newey (1994) gives conditions on estimating
equations and nuisance function estimators so that nuisance function estimators do not affect the
limiting distribution of parameters of interest, providing a semi-parametric version of Neyman
orthogonality. van der Vaart (1998) discusses use of semi-parametrically efficient scores to
define estimators that solve estimating equations, setting averages of efficient scores to zero.
He also uses efficient scores to define k-step estimators, where a preliminary estimator is used
to estimate the efficient score and then updating is done to further improve estimation; see also
comments below on the use of sample splitting.
There is also a related targeted maximum likelihood learning approach, introduced in
Scharfstein et al. (1999) in the context of treatments effects analysis. This is substantially
generalized by van der Laan and Rubin (2006), who use maximum likelihood in a least favourable
direction and then perform one-step or k-step updates using the estimated scores, in an effort
to better estimate the target parameter.6 This procedure is like the least favourable direction
6 Targeted minimum loss estimation, which shares similar properties, is also discussed in, e.g., van der Laan and Rose
(2011) and van der Laan (2015).


C

2017 Royal Economic Society.

C10

V. Chernozhukov et al.

approach in semi-parametrics; see, for example, Severini and Wong (1992). The introduction
of the likelihood introduces major benefits, such as allowing simple and natural imposition of
constraints inherent in the data (e.g. support restrictions when the outcome is binary or censored)
and permitting the use of likelihood cross-validation to choose the nuisance parameter estimator.
This data adaptive choice of the nuisance parameter has been dubbed the â€˜super learnerâ€™ by
van der Laan et al. (2007). In subsequent work, van der Laan and Rose (2011) emphasize the use
of ML methods to estimate the nuisance parameters for use with the super learner. Much of this
work, including recent work such as Luedtke and van der Laan (2016), Toth and van der Laan
(2016) and Zheng et al. (2016), focuses on formal results under a Donsker condition, though the
use of sample splitting to relax these conditions has also been advocated in the targeted maximum
likelihood setting, as discussed below.
The Donsker condition is a powerful classical condition that allows rich structures for fixed
function classes G, but unfortunately it is unsuitable for high-dimensional settings. Examples
of function classes where a Donsker condition holds include functions of a single variable that
have total variation bounded by 1 and functions x â†’ f (x) that have r > dim(x)/2 uniformly
bounded derivatives. As a further example, functions composed from function classes with VC
dimensions bounded by p through a fixed number of algebraic and monotonic transforms are
Donsker. However, this property will no longer hold if we let dim(x) grow to infinity with the
sample size as this increase in dimension would require that the VC dimension also increases
with n. More generally, Donsker conditions are easily violated once dimensions become large.
A major point of departure of the present work from the classical literature on semi-parametric
estimation is its explicit focus on high-complexity/entropy cases. One way to analyse the problem
of estimation in high-entropy cases is to see to what degree equicontinuity results continue to hold
while allowing moderate growth of the complexity/entropy of GN . Examples of papers taking this
approach in approximately sparse settings are Belloni et al. (2014b, 2016, 2017), Chernozhukov
et al. (2015b), Javanmard and Montanari (2014a), van de Geer et al. (2014) and Zhang and Zhang
(2014). In all of these examples, entropy growth must be limited in what can be very restrictive
ways. The entropy conditions rule out the contrived overfitting example mentioned above, which
does approximate realistic examples, and might otherwise place severe restrictions on the model.
For example,
in Belloni et al. (2010, 2012), the optimal instrument needs to be sparse of order
âˆš
n.
s
A key device that we use to avoid strong entropy conditions is cross-fitting via sample
splitting. Cross-fitting is a practical, efficient form of data splitting. Importantly, its use here is not
simply as a device to make proofs elementary (which it does), but as a practical method to allow
us to overcome the overfitting/high-complexity phenomena that commonly arise in data analysis
based on highly adaptive ML methods. Our treatment builds upon the sample-splitting ideas
employed in Belloni et al. (2010, 2012) who considered sample splitting in a high-dimensional
sparse optimal IV model to weaken the sparsity condition mentioned in the previous paragraph
to s
n. This work, in turn, was inspired by Angrist and Krueger (1995). We also build on
Ayyagari (2010) and Robins et al. (2013), where ML methods and sample splitting were used in
the estimation of a partially linear model of the effects of pollution while controlling for several
covariates. We use the term cross-fitting to characterize our recommended procedure, partly
borrowing the jargon from Fan et al. (2012), who employed a slightly different form of sample
splitting to estimate the scale parameter in a high-dimensional sparse regression. Of course,
the use of sample splitting to relax entropy conditions has a long history in semi-parametric
estimation problems. For example, Bickel (1982) considered estimating nuisance functions using
a vanishing fraction of the sample, and these results were extended to sample splitting into two

C

2017 Royal Economic Society.

Double/debiased machine learning

C11

equal halves and discretization of the parameter space by Schick (1986). Similarly, van der
Vaart (1998) uses two-way sample splitting and discretization of the parameter space to give
weak conditions for k-step estimators using the efficient scores where sample splitting is used
to estimate the updates; see also Hubbard et al. (2016). Robins et al. (2008, 2017) use sample
splitting in the construction of higher-order influence function corrections in semi-parametric
estimation. Some recent work in the targeted maximum likelihood literature, e.g. Zheng and
van der Laan (2011), also notes the utility of sample splitting in the context of k-step updating,
though this sample splitting approach is different from the cross-fitting approach we pursue.
Plan of the paper. We organize the rest of the paper as follows. In Section 2, we formally
define Neyman orthogonality and provide a brief discussion that synthesizes various models
and frameworks that can be used to produce estimating equations satisfying this key condition.
In Section 3, we carefully define DML estimators and develop their general theory. We then
illustrate this general theory by applying it to provide theoretical results for using DML to
estimate and carry out inference for key parameters in the PLR model, and for using DML to
estimate and carry out inference for coefficients on endogenous variables in a partially linear
IV model in Section 4. In Section 5, we provide a further illustration of the general theory by
applying it to develop theoretical results for DML estimation and inference for average treatment
effects (ATEs) and average treatment effects on the treated (ATTEs) under unconfoundedness,
and for DML estimation of local average treatment effects (LATEs) in an IV context within the
potential outcomes framework; see Imbens and Rubin (2015). Finally, we apply DML in three
empirical illustrations in Section 6. In the Appendix, we define additional notation and present
proofs.
Notation. The symbols Pr and E denote probability and expectation operators with respect
to a generic probability measure that describes the law of the data. If we need to signify the
dependence on a probability measure P , we use P as a subscript in PrP and EP . We use
capital letters, such as W , to denote random elements and we use the corresponding lowercase
letters, such as w, to denote fixed values that these random elements can take. In what follows,
denote the Lq (P ) norm; for example, we denote f P ,q := f (W )P ,q :=
we
 use  qÂ· P ,q to 1/q
( |f (w)| dP (w)) . We use x to denote the transpose of a column vector x. For a differentiable
map x â†’ f (x), mapping Rd to Rk , we use âˆ‚x f to abbreviate the partial derivatives (âˆ‚/âˆ‚x )f ,
and we correspondingly use the expression âˆ‚x f (x0 ) to mean âˆ‚x f (x) |x=x0 , etc.

2. CONSTRUCTION OF NEYMAN ORTHOGONAL SCORE/MOMENT
FUNCTIONS
Here we formally introduce the model and we discuss several methods for generating orthogonal
scores in a wide variety of settings, including the classical Neyman construction. We also use
this as an opportunity to synthesize some recent developments in the literature.
2.1. Moment condition/estimating equation framework
We are interested in the true value Î¸0 of the low-dimensional target parameter Î¸ âˆˆ , where
a non-empty measurable subset of RdÎ¸ . We assume that Î¸0 satisfies the moment conditions
EP [Ïˆ(W ; Î¸0 , Î·0 )] = 0,

C

2017 Royal Economic Society.

is
(2.1)

C12

V. Chernozhukov et al.

where Ïˆ = (Ïˆ1 , . . . , ÏˆdÎ¸ ) is a vector of known score functions, W is a random element taking
values in a measurable space (W, AW ) with law determined by a probability measure P âˆˆ PN ,
and Î·0 is the true value of the nuisance parameter Î· âˆˆ T , where T is a convex subset of some
normed vector space with the norm denoted by  Â· T . We assume that the score functions Ïˆj :
W Ã— Ã— T â†’ R are measurable once we equip
and T with their Borel Ïƒ -fields, and we
from
the
distribution
of W is available for estimation and
assume that a random sample (Wi )N
i=1
inference.
As discussed in Section 1, we require the Neyman orthogonality condition for the score Ïˆ.
To introduce the condition, for T = {Î· âˆ’ Î·0 : Î· âˆˆ T } we define the pathwise (or the Gateaux)
derivative map Dr : T â†’ RdÎ¸ ,
Dr [Î· âˆ’ Î·0 ] := âˆ‚r {EP [Ïˆ(W ; Î¸0 , Î·0 + r(Î· âˆ’ Î·0 )]},

Î· âˆˆ T,

for all r âˆˆ [0, 1), which we assume to exist. For convenience, we also denote
âˆ‚Î· EP [Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] := D0 [Î· âˆ’ Î·0 ],

Î· âˆˆ T.

(2.2)

Note that Ïˆ(W ; Î¸0 , Î·0 + r(Î· âˆ’ Î·0 )) here is well defined because for all r âˆˆ [0, 1) and Î· âˆˆ T ,
Î·0 + r(Î· âˆ’ Î·0 ) = (1 âˆ’ r)Î·0 + rÎ· âˆˆ T ,
as T is a convex set. In addition, let TN âŠ‚ T be a nuisance realization set such that the estimators
Î·Ì‚0 of Î·0 specified below take values in this set with high probability. In practice, we typically
assume that TN is a properly shrinking neighbourhood of Î·0 . Note that TN âˆ’ Î·0 is the nuisance
deviation set, which contains deviations of Î·Ì‚0 from Î·0 , Î·Ì‚0 âˆ’ Î·0 , with high probability. The
Neyman orthogonality condition requires that the derivative in (2.2) vanishes for all Î· âˆˆ TN .
D EFINITION 2.1. (N EYMAN ORTHOGONALITY) The score Ïˆ = (Ïˆ1 , . . . , ÏˆdÎ¸ ) obeys the
orthogonality condition at (Î¸0 , Î·0 ) with respect to the nuisance realization set TN âŠ‚ T if (2.1)
holds and the pathwise derivative map Dr [Î· âˆ’ Î·0 ] exists for all r âˆˆ [0, 1) and Î· âˆˆ TN and
vanishes at r = 0; namely,
âˆ‚Î· EP [Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] = 0,

for all Î· âˆˆ TN .

(2.3)

We remark here that condition (2.3) holds with TN = T when Î· is a finite-dimensional vector
as long as âˆ‚Î· EP [Ïˆj (W ; Î¸0 , Î·0 )] = 0 for all j = 1, . . . , dÎ¸ , where âˆ‚Î· EP [Ïˆj (W ; Î¸0 , Î·0 )] denotes
the vector of partial derivatives of the function Î· â†’ EP [Ïˆj (W ; Î¸0 , Î·)] for Î· = Î·0 .
Sometimes it will also be helpful to use an approximate Neyman orthogonality condition as
opposed to the exact one given in Definition 2.1.
D EFINITION 2.2. (N EYMAN NEAR-ORTHOGONALITY) The score Ïˆ = (Ïˆ1 , . . . , ÏˆdÎ¸ ) obeys the
Î»N near-orthogonality condition at (Î¸0 , Î·0 ) with respect to the nuisance realization set TN âŠ‚ T
if (2.1) holds and the pathwise derivative map Dr [Î· âˆ’ Î·0 ] exists for all r âˆˆ [0, 1) and Î· âˆˆ TN
and is small at r = 0; namely,
âˆ‚Î· EP [Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] â‰¤ Î»N ,

for all Î· âˆˆ TN ,

(2.4)

where {Î»N }Nâ‰¥1 is a sequence of positive constants such that Î»N = o(N âˆ’1/2 ).

C

2017 Royal Economic Society.

C13

Double/debiased machine learning

2.2. Construction of Neyman orthogonal scores
If we start with a score Ï• that does not satisfy the orthogonality condition above, we first
transform it into a score Ïˆ that does. Here we outline several methods for doing so.
2.2.1. Neyman orthogonal scores for likelihood and other M-estimation problems with finitedimensional nuisance parameters. First, we describe the construction used by Neyman (1959)
to derive his celebrated orthogonal score and C(Î±)-statistic in a maximum likelihood setting.7
Such a construction also underlies the concept of local unbiasedness in the construction of
optimal tests in, e.g., Ferguson (1967), and it was extended to non-likelihood settings by
Wooldridge (1991). The discussion of Neymanâ€™s construction here draws on Chernozhukov et al.
(2015a).
To describe the construction, let Î¸ âˆˆ âŠ‚ RdÎ¸ and Î² âˆˆ B âŠ‚ RdÎ² , where B is a convex set,
be the target and the nuisance parameters, respectively. Further, suppose that the true parameter
values Î¸0 and Î²0 solve the optimization problem
max EP [(W ; Î¸, Î²)],

(2.5)

Î¸âˆˆ , Î²âˆˆB

where (W ; Î¸, Î²) is a known criterion function. For example, (W ; Î¸, Î²) can be the log-likelihood
function associated with observation W . More generally, we refer to (W ; Î¸, Î²) as the quasi-loglikelihood function. Then, under mild regularity conditions, Î¸0 and Î²0 satisfy
EP [âˆ‚Î¸ (W ; Î¸0 , Î²0 )] = 0,

EP [âˆ‚Î² (W ; Î¸0 , Î²0 )] = 0.

(2.6)

Note that the original score function Ï•(W ; Î¸, Î²) = âˆ‚Î¸ (W ; Î¸, Î²) for estimating Î¸0 will not
generally satisfy the orthogonality condition. Now consider the new score function, which we
refer to as the Neyman orthogonal score,
Ïˆ(W ; Î¸, Î·) = âˆ‚Î¸ (W ; Î¸, Î²) âˆ’ Î¼âˆ‚Î² (W ; Î¸, Î²),

(2.7)

where the nuisance parameter is
Î· = (Î² , vec(Î¼) ) âˆˆ T = B Ã— RdÎ¸ dÎ² âŠ‚ Rp ,

p = dÎ² + dÎ¸ dÎ² ,

and Î¼ is the dÎ¸ Ã— dÎ² orthogonalization parameter matrix whose true value Î¼0 solves the equation
JÎ¸Î² âˆ’ Î¼JÎ²Î² = 0

(2.8)

for
J =

JÎ¸Î¸
JÎ²Î¸

JÎ¸Î²
JÎ²Î²

= âˆ‚(Î¸ ,Î² ) EP [âˆ‚(Î¸ ,Î² ) (W ; Î¸, Î²)]|Î¸=Î¸0 ; Î²=Î²0 .

The true value of the nuisance parameter Î· is
Î·0 = (Î²0 , vec(Î¼0 ) ) ;

(2.9)

7 The C(Î±)-statistic, or the orthogonal score statistic, has been explicitly used for testing and estimation in highdimensional sparse models in Belloni et al. (2015).


C

2017 Royal Economic Society.

C14

V. Chernozhukov et al.

and when JÎ²Î² is invertible, (2.8) has the unique solution,
âˆ’1
.
Î¼0 = JÎ¸Î² JÎ²Î²

(2.10)

The following lemma shows that the score Ïˆ in (2.7) satisfies the Neyman orthogonality
condition.
L EMMA 2.1. (N EYMAN ORTHOGONAL SCORES FOR QUASI-LIKELIHOOD SETTINGS) If (2.6)
holds, J exists, and JÎ²Î² is invertible, then the score Ïˆ in (2.7) is Neyman orthogonal at (Î¸0 , Î·0 )
with respect to the nuisance realization set TN = T .
R EMARK 2.1. (A DDITIONAL NUISANCE PARAMETERS) Note that the orthogonal score Ïˆ in
(2.7) has nuisance parameters consisting of the elements of Î¼ in addition to the elements of Î²,
and Lemma 2.1 shows that Neyman orthogonality holds both with respect to Î² and with respect
to Î¼. We will find that Neyman orthogonal scores in other settings, including infinite-dimensional
ones, have a similar property.
R EMARK 2.2. (E FFICIENCY) Note that in this example, Î¼0 not only creates the necessary
orthogonality but also creates the efficient score for inference on the target parameter Î¸ when the
quasi-log-likelihood function is the true (possibly conditional) log-likelihood, as demonstrated
by Neyman (1959).
E XAMPLE 2.1. (H IGH-DIMENSIONAL LINEAR REGRESSION) As an application of the
construction above, consider the following linear predictive model,
Y = DÎ¸0 + X Î²0 + U,
D = X Î³0 + V ,

EP [U (X , D) ] = 0,

(2.11)

EP [V X] = 0,

(2.12)

where, for simplicity, we assume that Î¸0 is a scalar. The first equation here is the main predictive
model, and the second equation only plays a role in the construction of the Neyman orthogonal
scores. It is well known that Î¸0 and Î²0 in this model solve the optimization problem (2.5) with
(W ; Î¸, Î²) = âˆ’

(Y âˆ’ DÎ¸ âˆ’ X Î²)2
,
2

Î¸âˆˆ

= R, Î² âˆˆ B = RdÎ² ,

where we denote W = (Y, D, X ) . Hence, equations (2.6) hold with
âˆ‚Î¸ (W ; Î¸, Î²) = (Y âˆ’ DÎ¸ âˆ’ X Î²)D,

âˆ‚Î² (W ; Î¸, Î²) = (Y âˆ’ DÎ¸ âˆ’ X Î²)X,

and the matrix J satisfies
JÎ¸Î² = âˆ’EP [DX ],

JÎ²Î² = âˆ’EP [XX ].

The Neyman orthogonal score is then given by
Ïˆ(W ; Î¸, Î·) = (Y âˆ’ DÎ¸ âˆ’ X Î²)(D âˆ’ Î¼X);
Ïˆ(W ; Î¸0 , Î·0 ) = U (D âˆ’ Î¼0 X);

Î· = (Î² , vec(Î¼) ) ;

Î¼0 = EP [DX ](EP [XX ])âˆ’1 = Î³0 .

(2.13)

If the vector of covariates X here is high-dimensional but the vectors of parameters Î²0 and Î³0 are
approximately sparse, we can use 1 -penalized least-squares, 2 -boosting, or forward selection
methods to estimate Î²0 and Î³0 = Î¼0 , and hence Î¼0 = (Î²0 , vec(Î¼0 ) ) ; see references cited in
Section 1.

C

2017 Royal Economic Society.

C15

Double/debiased machine learning

If JÎ²Î² is not invertible, (2.8) typically has multiple solutions. In this case, it is convenient to
focus on a minimal norm solution,
Î¼0 = arg min Î¼ such that JÎ¸Î² âˆ’ Î¼JÎ²Î² q = 0
for a suitably chosen norm  Â· q on the space of dÎ¸ Ã— dÎ² matrices. With an eye on solving the
empirical version of this problem, we can also consider the relaxed version of this problem,
Î¼0 = arg min Î¼ such that JÎ¸Î² âˆ’ Î¼JÎ²Î² q â‰¤ rN

(2.14)

for some rN > 0 such that rN â†’ 0 as N â†’ âˆž. This relaxation is also helpful when JÎ²Î² is
invertible but ill-conditioned. The following lemma shows that using Î¼0 in (2.14) leads to
Neyman near-orthogonal scores. The proof of this lemma can be found in the Appendix.
L EMMA 2.2. (N EYMAN NEAR-ORTHOGONAL SCORES FOR QUASI-LIKELIHOOD SETTINGS) If
(2.6) holds, J exists, the solution of the optimization problem (2.14) exists, and Î¼0 is taken to
be this solution, then the score Ïˆ defined in (2.7) is Neyman Î»N near-orthogonal at (Î¸0 , Î·0 ) with
respect to the nuisance realization set TN = {Î² âˆˆ B : Î² âˆ’ Î²0 âˆ—q â‰¤ Î»N /rN } Ã— RdÎ¸ dÎ² , where the
norm  Â· âˆ—q on RdÎ² is defined by Î²âˆ—q = supA AÎ² with the supremum being taken over all
dÎ¸ Ã— dÎ² matrices A such that Aq â‰¤ 1.
E XAMPLE 2.1. (C ONTINUED) In the high-dimensional linear regression example above, the
relaxation (2.14) is helpful when JÎ²Î² = EP [XX ] is ill-conditioned. Specifically, if one suspects
that EP [XX ] is ill-conditioned, one can define Î¼0 as the solution to the following optimization
problem:
min Î¼ such that EP [DX ] âˆ’ Î¼EP [XX ]âˆž â‰¤ rN .

(2.15)

Lemma 2.2 then shows that using this Î¼0 leads to a score Ïˆ that obeys the Neyman nearorthogonality condition. Alternatively, one can define Î¼0 as the solution of the following closely
related optimization problem,
min(Î¼EP [XX ]Î¼ âˆ’ Î¼EP [DX] + rN Î¼1 ),
Î¼

whose solution also obeys EP [DX] âˆ’ Î¼EP [XX ]âˆž â‰¤ rN , which follows from the first-order
conditions. An empirical version of either problem leads to a Lasso-type estimator of the
regularized solution Î¼0 ; see Javanmard and Montanari (2014a).
R EMARK 2.3. (G IVING UP EFFICIENCY) Note that the regularized Î¼0 in (2.14) creates the
necessary near-orthogonality at the cost of giving up somewhat on the efficiency of the score
Ïˆ. At the same time, regularization may generate additional robustness gains as achieving full
efficiency by estimating Î¼0 in (2.10) may require stronger conditions.
R EMARK 2.4. (C ONCENTRATING-OUT APPROACH) The approach for constructing Neyman
orthogonal scores described above is closely related to the following concentrating-out approach,
which has been used, for example, in Newey (1994), to show Neyman orthogonality when Î² is
infinite dimensional. For all Î¸ âˆˆ , let Î²Î¸ be the solution of the following optimization problem:
max EP [(W ; Î¸, Î²)].
Î²âˆˆB

Under mild regularity conditions, Î²Î¸ satisfies
âˆ‚Î² EP [(W ; Î¸, Î²Î¸ )] = 0,

C

2017 Royal Economic Society.

for all Î¸ âˆˆ

.

(2.16)

C16

V. Chernozhukov et al.

Differentiating (2.16) with respect to Î¸ and interchanging the order of differentiation gives
0 = âˆ‚Î¸ âˆ‚Î² EP [(W ; Î¸, Î²Î¸ )] = âˆ‚Î² âˆ‚Î¸ EP [(W ; Î¸, Î²Î¸ )]
= âˆ‚Î² EP [âˆ‚Î¸ (W ; Î¸, Î²Î¸ ) + [âˆ‚Î¸ Î²Î¸ ] âˆ‚Î² (W ; Î¸, Î²Î¸ )]
= âˆ‚Î² EP [Ïˆ(W ; Î¸, Î², âˆ‚Î¸ Î²Î¸ )]|Î²=Î²Î¸ ,
where we denote
Ïˆ(W ; Î¸, Î², âˆ‚Î¸ Î²Î¸ ) := âˆ‚Î¸ (W ; Î¸, Î²) + [âˆ‚Î¸ Î²Î¸ ] âˆ‚Î² (W ; Î¸, Î²).
This vector of functions is a score with nuisance parameters Î· = (Î² , vec(âˆ‚Î¸ Î²Î¸ )) . As before,
additional nuisance parameters, âˆ‚Î¸ Î²Î¸ in this case, are introduced when the orthogonal score
is formed. Evaluating these equations at Î¸0 and Î²0 , it follows from the previous equation that
Ïˆ(W ; Î¸, Î², âˆ‚Î¸ Î²Î¸ ) is orthogonal with respect to Î² and from EP [âˆ‚Î² (W ; Î¸0 , Î²0 )] = 0 that we
have orthogonality with respect to âˆ‚Î¸ Î²Î¸ . Thus, maximizing the expected objective function
with respect to the nuisance parameters, plugging that maximum back in, and differentiating
with respect to the parameters of interest produces an orthogonal moment condition. See also
Section 2.2.3.
2.2.2. Neyman orthogonal scores in generalized method of moments problems. The
construction in the previous section gives a Neyman orthogonal score whenever the moment
conditions (2.6) hold, and, as discussed in Remark 2.2, the resulting score is efficient as long as
(W ; Î¸, Î²) is the log-likelihood function. The question, however, remains about constructing the
efficient score when (W ; Î¸, Î²) is not necessarily a log-likelihood function. In this section, we
answer this question and describe a generalized method of moments (GMM)-based method of
constructing an efficient and Neyman orthogonal score in this more general case. The discussion
here is related to Lee (2005), Bera et al. (2010) and Chernozhukov et al. (2015b).
Because GMM does not require that the moment conditions (2.6) are obtained from the firstorder conditions of the optimization problem (2.5), we use a different notation for the moment
conditions. Specifically, we consider parameters Î¸ âˆˆ âŠ‚ RdÎ¸ and Î² âˆˆ B âŠ‚ RdÎ² , where B is a
convex set, whose true values, Î¸0 and Î²0 , solve the moment conditions
EP [m(W ; Î¸0 , Î²0 )] = 0,

(2.17)

where m : W Ã— Ã— B â†’ R is a known vector-valued function, and dm â‰¥ dÎ¸ + dÎ² is the
number of moment conditions. In this case, a Neyman orthogonal score function is
dm

Ïˆ(W ; Î¸, Î·) = Î¼m(W ; Î¸, Î²),

(2.18)

where the nuisance parameter is
Î· = (Î² , vec(Î¼) ) âˆˆ T = B Ã— RdÎ¸ dm âŠ‚ Rp ,

p = dÎ² + dÎ¸ dm ,

and Î¼ is the dÎ¸ Ã— dm orthogonalization parameter matrix whose true value is
Î¼0 = (A âˆ’1 âˆ’ A âˆ’1 GÎ² (GÎ² âˆ’1 GÎ² )âˆ’1 GÎ² âˆ’1 ).
Here
GÎ³ = âˆ‚Î³ EP [m(W ; Î¸, Î²)]|Î³ =Î³0
= [âˆ‚Î¸ EP [m(W ; Î¸, Î²)], âˆ‚Î² EP [m(W ; Î¸, Î²)]]|Î³ =Î³0 =: [GÎ¸ , GÎ² ],

C

2017 Royal Economic Society.

Double/debiased machine learning

C17

for Î³ = (Î¸ , Î² ) and Î³0 = (Î¸0 , Î²0 ) , A is a dm Ã— dÎ¸ moment selection matrix,  is a dm Ã— dm
positive definite weighting matrix, and both A and  can be chosen arbitrarily. Note that setting
A = GÎ¸ and  = VarP (m(W ; Î¸0 , Î²0 )]) = EP [m(W ; Î¸0 , Î²0 )m(W ; Î¸0 , Î²0 ) ]
leads to the efficient score in the sense of yielding an estimator of Î¸0 having the smallest variance
in the class of GMM estimators (Hansen, 1982), and, in fact, to the semi-parametrically efficient
score; see Levit (1975), Nevelson (1977) and Chamberlain (1987). Let Î·0 = (Î²0 , vec(Î¼0 ) ) be
the true value of the nuisance parameter Î· = (Î² , vec(Î¼) ) . The following lemma shows that the
score Ïˆ in (2.18) satisfies the Neyman orthogonality condition.
L EMMA 2.3. (N EYMAN ORTHOGONAL SCORES FOR GMM SETTINGS) If (2.17) holds, GÎ³
exists and  is invertible, then the score Ïˆ in (2.18) is Neyman orthogonal at (Î¸0 , Î·0 ) with
respect to the nuisance realization set TN = T .
As in the quasi-likelihood case, we can also consider near-orthogonal scores. Specifically,
note that one of the orthogonality conditions that the score Ïˆ in (2.18) has to satisfy is that
Î¼0 GÎ² = 0, which can be rewritten as
A âˆ’1/2 (I âˆ’ L(L L)âˆ’1 L )L = 0,
where L = âˆ’1/2 GÎ² . Here, the part A âˆ’1/2 L(L L)âˆ’1 L can be expressed as Î³0 L , where Î³0 =
A âˆ’1/2 L(L L)âˆ’1 solves the optimization problem
min Î³ o such that A âˆ’1/2 L âˆ’ Î³ L Lâˆž = 0,
for a suitably chosen norm  Â· o . When L L is close to being singular, this problem can be
relaxed:
min Î³ o such that A âˆ’1/2 L âˆ’ Î³ L Lâˆž â‰¤ rN .

(2.19)

This relaxation leads to Neyman near-orthogonal scores.
L EMMA 2.4. (N EYMAN NEAR-ORTHOGONAL SCORES FOR GMM SETTINGS) In the set-up
above, with Î³0 denoting the solution of (2.19), we have for Î¼0 := A âˆ’1 âˆ’ Î³0 L âˆ’1/2 and Î·0 =
(Î²0 , vec(Î¼0 ) ) that Ïˆ defined in (2.18) is the Neyman Î»N near-orthogonal score at (Î¸0 , Î·0 ) with
respect to the nuisance realization set TN = {Î² âˆˆ B : Î² âˆ’ Î²0 1 â‰¤ Î»N /rN } Ã— RdÎ¸ dm .
2.2.3. Neyman orthogonal scores for likelihood and other M-estimation problems with
infinite-dimensional nuisance parameters. Here we show that the concentrating-out approach
described in Remark 2.4 for the case of finite-dimensional nuisance parameters can be extended
to the case of infinite-dimensional nuisance parameters. Let (W ; Î¸, Î²) be a known criterion
function, where Î¸ and Î² are the target and the nuisance parameters taking values in and B,
respectively, and let us assume that the true values of these parameters, Î¸0 and Î²0 , solve the
optimization problem (2.5). The function (W ; Î¸, Î²) is analogous to that discussed above but
now, instead of assuming that B is a (convex) subset of a finite-dimensional space, we assume
that B is some (convex) set of functions, so that Î² is the functional nuisance parameter. For
example, (W ; Î¸, Î²) could be a semi-parametric log-likelihood where Î² is the non-parametric
part of the model. More generally, (W ; Î¸, Î²) could be some other criterion function such as the
negative of a squared residual. Also, let
Î²Î¸ = arg max EP [(W ; Î¸, Î²)]
Î²âˆˆB


C

2017 Royal Economic Society.

(2.20)

C18

V. Chernozhukov et al.

be the concentrated-out non-parametric part of the model. Note that Î²Î¸ is a function-valued
function. Now consider the score function
Ïˆ(W ; Î¸, Î·) =
where the nuisance parameter is Î· :

d(W ; Î¸, Î·(Î¸ ))
,
dÎ¸

(2.21)

â†’ B, and its true value Î·0 is given by

Î·0 (Î¸ ) = Î²Î¸ ,

for all Î¸ âˆˆ

.

Here, the symbol d/dÎ¸ denotes the full derivative with respect to Î¸ , so that we differentiate with
respect to both Î¸ arguments in (W ; Î¸, Î·(Î¸ )). The following lemma shows that the score Ïˆ in
(2.21) satisfies the Neyman orthogonality condition.
L EMMA 2.5. (N EYMAN ORTHOGONAL SCORES VIA CONCENTRATING-OUT APPROACH)
Suppose that (2.5) holds, and let T be a convex set of functions mapping
into B such that
Î·0 âˆˆ T . Also, suppose that for each Î· âˆˆ T , the function Î¸ â†’ (W ; Î¸, Î·(Î¸ )) is continuously
differentiable almost surely. Then, under mild regularity conditions, the score Ïˆ in (2.21) is
Neyman orthogonal at (Î¸0 , Î·0 ) with respect to the nuisance realization set TN = T .
As an example, consider the partially linear model from Section 1. Let
1
(W ; Î¸, Î²) = âˆ’ (Y âˆ’ DÎ¸ âˆ’ Î²(X))2 ,
2
and let B be the set of functions of X with finite mean square. Then
(Î¸0 , Î²0 ) = arg max EP [(W ; Î¸, Î²)]
Î¸âˆˆ ,Î²âˆˆB

and
Î²Î¸ (X) = EP [Y âˆ’ DÎ¸ |X],

Î¸âˆˆ

.

Hence, (2.21) gives the following Neyman orthogonal score
1 d{Y âˆ’ DÎ¸ âˆ’ EP [Y âˆ’ DÎ¸ |X]}2
2
dÎ¸
= (D âˆ’ EP [D|X]) Ã— (Y âˆ’ EP [Y |X] âˆ’ (D âˆ’ EP [D|X])Î¸ )

Ïˆ(W ; Î¸, Î²Î¸ ) = âˆ’

= (D âˆ’ m0 (X)) Ã— (Y âˆ’ DÎ¸ âˆ’ g0 (X)),
which corresponds to the estimator Î¸0 described in Section 1 in (1.5).
It is important to note that the concentrating-out approach described here gives a Neyman
orthogonal score without requiring that (W ; Î¸, Î²) is the log-likelihood function. Except for the
technical conditions needed to ensure the existence of derivatives and their interchangeability,
the only condition that is required is that Î¸0 and Î²0 solve the optimization problem (2.5). If
(W ; Î¸, Î²) is the log-likelihood function, however, it follows from Newey (1994, p. 1359),
that the concentrating-out approach actually yields the efficient score. An alternative, but
closely related, approach to derive the efficient score in the likelihood setting would be to
apply Neymanâ€™s construction described above for a one-dimensional least favourable parametric
submodel; see Severini and Wong (1992) and Chapter 25 of van der Vaart (1998).


C

2017 Royal Economic Society.

Double/debiased machine learning

C19

R EMARK 2.5. (G ENERATING ORTHOGONAL SCORES BY VARYING B) When we calculate the
concentrated-out non-parametric part Î²Î¸ , we can use some other set of functions Ï’ instead of B
on the right-hand side of (2.20):
Î²Î¸ = arg max EP [(W ; Î¸, Î²)].
Î²âˆˆÏ’

By replacing B by Ï’, we can generate a different Neyman orthogonal score. Of course, this
replacement might also change the true value Î¸0 of the parameter of interest, which is an
important consideration for the selection of Ï’. For example, consider the partially linear model
and assume that X has two components, X1 and X2 . Now, consider what would happen if we
replaced B, which is the set of functions of X with finite mean square, by the set of functions Ï’
that is the mean square closure of functions that are additive in X1 and X2 :
Ï’ = {h(X1 ) + h(X2 )}.
Let EÌ„P denote the least-squares projection on Ï’. Then, applying the previous calculation with
EÌ„P replacing EP gives
Ïˆ(W ; Î¸, Î²Î¸ ) = (D âˆ’ EÌ„P [D|X]) Ã— (Y âˆ’ EÌ„P [Y |X] + (D âˆ’ EÌ„P [D|X])Î¸ ),
which provides an orthogonal score based on additive function of X1 and X2 . Here, it is important
to note that the solution to EP [Ïˆ(W, Î¸, Î²Î¸ )] = 0 will be the true Î¸0 only when the true function of
X in the partially linear model is additive. More generally, the solution of the moment condition
would be the coefficient of D in the least-squares projection of Y on functions of the form
DÎ¸ + h1 (X1 ) + h1 (X2 ). Note, though, that the corresponding score is orthogonal by virtue of
additivity being imposed in the estimation of EÌ„P [Y |X] and EÌ„P [D|X].
2.2.4. Neyman orthogonal scores for conditional moment restriction problems with infinitedimensional nuisance parameters. Next we consider the conditional moment restrictions
framework studied in Chamberlain (1992). To define the framework, let W , R and Z be random
vectors taking values in W âŠ‚ Rdw , R âŠ‚ Rdr and Z âŠ‚ Rdz , respectively. Assume that Z is a
subvector of R and R is a subvector of W , so that dz â‰¤ dr â‰¤ dw . Also, let Î¸ âˆˆ âŠ‚ RdÎ¸ be
a finite-dimensional parameter whose true value Î¸0 is of interest, and let h be a vector-valued
functional nuisance parameter taking values in a convex set of functions H mapping Z to Rdh ,
with the true value of h being h0 . The conditional moment restrictions framework assumes that
Î¸0 and h0 satisfy the moment conditions
EP [m(W ; Î¸0 , h0 (Z)) | R] = 0,

(2.22)

where m : W Ã— Ã— Rdh â†’ Rdm is a known vector-valued function. This framework is of
interest because it covers a rich variety of models without having to explicitly rely on the
likelihood formulation.
To build a Neyman orthogonal score Ïˆ(W ; Î¸, Î·) for estimating Î¸0 , consider the matrix-valued
functional parameter Î¼ : R â†’ RdÎ¸ Ã—dm whose true value is given by
Î¼0 (R) = A(R) (R)âˆ’1 âˆ’ G(Z)(R) (R)âˆ’1 ,


C

2017 Royal Economic Society.

(2.23)

C20

V. Chernozhukov et al.

where the moment selection matrix-valued function A : R â†’ Rdm Ã—dÎ¸ and the weighting positive
definite matrix-valued function  : R â†’ Rdm Ã—dm can be chosen arbitrarily, and the matrix-valued
functions  : R â†’ Rdm Ã—dÎ¸ and G : Z â†’ RdÎ¸ Ã—dm are given by
(R) = âˆ‚v EP [m(W ; Î¸0 , v) | R]|v=h0 (Z)

(2.24)

G(Z) = EP [A(R) (R)âˆ’1 (R) | Z] Ã— (EP [(R) (R)âˆ’1 (R) | Z])âˆ’1 .

(2.25)

and

Note that Î¼0 in (2.23) is well defined even though the right-hand side of (2.23) contains both R
and Z as Z is a subvector of R. Then a Neyman orthogonal score is
Ïˆ(W ; Î¸, Î·) = Î¼(R)m(W ; Î¸, h(Z)),

(2.26)

where the nuisance parameter is
Î· = (Î¼, h) âˆˆ T = L1 (R; RdÎ¸ Ã—dm ) Ã— H.
Here, L1 (R; RdÎ¸ Ã—dm ) is the vector space of matrix-valued functions f : R â†’ RdÎ¸ Ã—dm satisfying
EP [f (R)] < âˆž. Also, note that even though the matrix-valued functions A and  can be
chosen arbitrarily, setting
A(R) = âˆ‚Î¸ EP [m(W ; Î¸, h0 (Z)) | R]|Î¸=Î¸0

(2.27)

(R) = EP [m(W ; Î¸0 , h0 (Z))m(W ; Î¸0 , h0 (Z)) | R]

(2.28)

and

leads to an asymptotic variance equal to the semi-parametric bound of Chamberlain (1992).
Let Î·0 = (Î¼0 , h0 ) be the true value of the nuisance parameter Î· = (Î¼, h). The following lemma
shows that the score Ïˆ in (2.26) satisfies the Neyman orthogonality condition.
L EMMA 2.6. (N EYMAN O RTHOGONAL S CORES FOR C ONDITIONAL M OMENT S ETTINGS)
Suppose that (a) (2.22) holds, (b) the matrices EP [(R)4 ], EP [G(Z)4 ], EP [A(R)2 ]
and EP [(R)âˆ’2 ] are finite, and (c) for all h âˆˆ H, there exists a constant Ch > 0 such that
P r P (EP [m(W ; Î¸0 , h(Z)) | R] â‰¤ Ch ) = 1. Then the score Ïˆ in (2.26) is Neyman orthogonal
at (Î¸0 , Î·0 ) with respect to the nuisance realization set TN = T .
As an application of the conditional moment restrictions framework, let us derive Neyman
orthogonal scores in the PLR example using this framework. The PLR model (1.1) is
equivalent to
EP [Y âˆ’ DÎ¸0 âˆ’ g0 (X) | X, D] = 0,
which can be written in the form of the conditional moment restrictions framework (2.22) with
W = (Y, D, X ) , R = (D, X ) , Z = X, h(Z) = g(X) and m(W ; Î¸, v) = Y âˆ’ DÎ¸ âˆ’ v. Hence,
using (2.27) and (2.28) and denoting Ïƒ (D, X)2 = EP [U 2 | D, X] for U = Y âˆ’ DÎ¸0 âˆ’ g0 (X),
we can take
A(R) = âˆ’D,

(R) = EP [U 2 | D, X] = Ïƒ (D, X)2 .

With this choice of A(R) and (R), we have
 
  
âˆ’1
D
1
(R) = âˆ’1, G(Z) = EP
| X Ã— EP
|X
,
2
2
Ïƒ (D, X)
Ïƒ (D, X)

C

2017 Royal Economic Society.

C21

Double/debiased machine learning

and so (2.23) and (2.26) give
Ïˆ(W ; Î¸, Î·0 ) =

1
Ïƒ (D, X)2

Dâˆ’

EP [(D/Ïƒ (D, X)2 ) | X]
EP [(1/Ïƒ (D, X)2 ) | X]

Ã—(Y âˆ’ DÎ¸ âˆ’ g0 (X)).
By construction, the score Ïˆ above is efficient and Neyman orthogonal. Note, however, that
using this score would require estimating the heteroscedasticity function Ïƒ (D, X)2 , which would
require the imposition of some additional smoothness assumptions over this conditional variance
function. Instead, if are willing to give up on efficiency to gain some robustness, we can take
A(R) = âˆ’D,

(R) = 1;

in which case we have
(R) = âˆ’1,

G(Z) = EP [D | X].

Equations (2.23) and (2.26) then give
Ïˆ(W ; Î¸, Î·0 ) = (D âˆ’ EP [D | X]) Ã— (Y âˆ’ DÎ¸ âˆ’ g0 (X))
= (D âˆ’ m0 (X)) Ã— (Y âˆ’ DÎ¸ âˆ’ g0 (X)).
This score Ïˆ is Neyman orthogonal and corresponds to the estimator of Î¸0 described in the
Introduction in (1.5). Note, however, that this score Ïˆ is efficient only if Ïƒ (X, D) is a constant.
2.2.5. Neyman orthogonal scores and influence functions. Neyman orthogonality is a joint
property of the score Ïˆ(W ; Î¸, Î·), the true parameter value Î·0 , the parameter set T , and the
distribution of W . It is not determined by any particular model for the parameter Î¸ . Nevertheless,
it is possible to use semi-parametric efficiency calculations to construct the orthogonal score
from the original score as in Chernozhukov et al. (2016). Specifically, an orthogonal score can
be constructed by adding to the original score the influence function adjustment for estimation
of the nuisance functions that is analysed in Newey (1994). The resulting orthogonal score will
be the influence function of the limit of the average of the original score.
0 be a
To explain, consider the original score Ï•(W ; Î¸, Î²), where Î² is some function, and let Î²
non-parametric estimator of Î²0 , the true value of Î². Here, Î² is implicitly allowed to depend on Î¸ ,
though we suppress that dependence for notational convenience. The corresponding orthogonal
score can be formed when there is Ï†(W ; Î¸, Î·) such that

n
1
Ï•(w; Î¸0 , Î²Ì‚0 )dP(w) =
Ï†(Wi ; Î¸0 , Î·0 ) + oP (nâˆ’1/2 ),
(*)
n i=1
where Î· is a vector of nuisance functions that includes Î², and Ï†(W ; Î¸, Î·) is an adjustment for
the presence of the estimated function Î²Ì‚0 in the original score Ï•(W ; Î¸, Î²). The decomposition
(*) typically holds when Î²Ì‚ is either a kernel or a series estimator with a suitably chosen tuning
parameter. The Neyman orthogonal score is given by
Ïˆ(W ; Î¸, Î·) = Ï•(W ; Î¸, Î²) + Ï†(W ; Î¸, Î·).

(2.29)

Ï•(Wi ; Î¸0 , Î²Ì‚0 ), as analysed
Here, Ïˆ(W ; Î¸0 , Î·0 ) is the influence function of the limit of nâˆ’1
in Newey (1994), with the restriction EP [Ïˆ(W ; Î¸0 , Î·0 )] = 0 identifying Î¸0 .
n
i=1


C

2017 Royal Economic Society.

C22

V. Chernozhukov et al.

The form of the adjustment term Ï†(W ; Î¸, Î·) depends on the estimator Î²Ì‚0 and, of course,
on the form of Ï•(W ; Î¸, Î²). Such adjustment terms have been derived for various Î²Ì‚0 by Newey
(1994). Also, Ichimura and Newey (2015) show how the adjustment term can be computed from
the limit of a certain derivative. Any of these results can be applied to a particular starting score
Ï•(W ; Î¸, Î²) and estimator Î²Ì‚0 to obtain an orthogonal score.
For example, consider again the partially linear model with the original score
Ï•(W ; Î¸, Î²) = D(Y âˆ’ DÎ¸ âˆ’ g0 (X)).
Here, Î²Ì‚0 = gÌ‚0 is a non-parametric regression estimator. From Newey (1994), we know that we
obtain the influence function adjustment by taking the conditional expectation of the derivative
of the score with respect to g0 (x) (obtaining âˆ’m0 (X) = âˆ’EP [D|X]) and multiplying the result
by the non-parametric residual to obtain
Ï†(W, Î¸, Î·) = âˆ’m0 (X){Y âˆ’ DÎ¸ âˆ’ Î²(X, Î¸ )}.
The corresponding orthogonal score is then simply
Ïˆ(W ; Î¸, Î·) = {D âˆ’ m0 (X)}{Y âˆ’ DÎ¸ âˆ’ Î²(X, Î¸ )},
Î²0 (X, Î¸ ) = EP [Y âˆ’ DÎ¸ |X], m0 (X) = EP [D|X],
illustrating that an orthogonal score for the partially linear model can be derived from an
influence function adjustment.
Influence functions have been used to estimate functionals of non-parametric estimators
by Hasminskii and Ibragimov (1979) and Bickel and Ritov (1988). Newey et al. (1998, 2004)
showed that nâˆ’1/2 ni=1 Ïˆ(Wi ; Î¸0 , Î·Ì‚0 ) from (2.29) will have a second-order remainder in Î·Ì‚0 ,
which is the key asymptotic property of orthogonal scores. Orthogonality of influence functions
in semi-parametric models follows from van der Vaart (1991), as shown for higher-order
counterparts in Robins et al. (2008, 2017). Chernozhukov et al. (2016) point out that, in general,
an orthogonal score can be constructed from an original score and non-parametric estimator Î²Ì‚0
by adding to the original score the adjustment term for estimation of Î²0 as described above. This
construction provides a way of obtaining an orthogonal score from any initial score Ï•(W ; Î¸, Î²)
and non-parametric estimator Î²Ì‚0 .

3. DML: POST-REGULARIZED INFERENCE BASED ON
NEYMAN-ORTHOGONAL ESTIMATING EQUATIONS
3.1. Definition of DML and its basic properties
We assume that we have a sample (Wi )N
i=1 , modelled as independent and identically distributed
(i.i.d.) copies of W , whose law is determined by the probability measure P on W. Estimation
will be carried out using the finite-sample analogue of the estimating equation (2.1).
We assume that the true value Î·0 of the nuisance parameter Î· can be estimated by Î·Ì‚0 using a
part of the data (Wi )N
i=1 . Different structured assumptions on Î·0 allow us to use different machinelearning tools for estimating Î·0 , for example:
(1) approximate sparsity for Î·0 with respect to some dictionary calls for the use of forward
selection, lasso, post-lasso, 2 -boosting, or some other sparsity-based technique;

C

2017 Royal Economic Society.

C23

Double/debiased machine learning

(2) well-approximability of Î·0 by trees calls for the use of regression trees and random forests;
(3) well-approximability of Î·0 by sparse neural and deep neural nets calls for the use of 1 penalized neural and deep neural networks;
(4) well-approximability of Î·0 by at least one model mentioned in (1)â€“(3) above calls for the
use of an ensemble/aggregated method over the estimation methods mentioned in (1)â€“(3).
There are performance guarantees for most of these ML methods that make it possible to satisfy
the conditions stated below. Ensemble and aggregation methods ensure that the performance
guarantee is approximately no worse than the performance of the best method.
We assume that N is divisible by K in order to simplify the notation. The following algorithm
defines the simple cross-fitted DML as outlined in the Introduction.
D EFINITION 3.1. (DML 1) (a) Take a K-fold random partition (Ik )K
k=1 of observation indices
[N] = {1, . . . , N } such that the size of each fold Ik is n = N/K. Also, for each k âˆˆ [K] =
{1, . . . , K}, define Ikc := {1, . . . , N } \ Ik . (b) For each k âˆˆ [K], construct an ML estimator
Î·Ì‚0,k = Î·Ì‚0 ((Wi )iâˆˆIkc )
of Î·0 , where Î·Ì‚0,k is a random element in T , and where randomness depends only on the subset
of data indexed by Ikc . (c) For each k âˆˆ [K], construct the estimator Î¸ÌŒ0,k as the solution of the
following equation:
En,k [Ïˆ(W ; Î¸ÌŒ0,k , Î·Ì‚0,k ] = 0,

(3.1)

where Ïˆ is the Neyman orthogonal score, and En,k is the empirical expectation over the kth fold
of the data; that is, En,k [Ïˆ(W )] = nâˆ’1 iâˆˆIk Ïˆ(Wi ). If achievement of exact 0 is not possible,
define the estimator Î¸ÌŒ0,k of Î¸0 as an approximate N -solution:
En,k [Ïˆ(W ; Î¸ÌŒ0,k , Î·Ì‚0,k )] â‰¤ inf En,k [Ïˆ(W ; Î¸, Î·Ì‚0,k )] + N ,
Î¸âˆˆ

N = o(Î´N N âˆ’1/2 ),

(3.2)

where (Î´N )Nâ‰¥1 is some sequence of positive constants converging to zero. (4) Aggregate the
estimators:
Î¸Ìƒ0 =

K
1 
Î¸ÌŒ0,k .
K k=1

(3.3)

This approach generalizes the 50â€“50 cross-fitting method mentioned in the Introduction.
We now define a variation of this basic cross-fitting approach that may behave better in small
samples.
D EFINITION 3.2. (DML 2) (a) Take a K-fold random partition (Ik )K
k=1 of observation indices
[N] = {1, . . . , N } such that the size of each fold Ik is n = N/K. Also, for each k âˆˆ [K] =
{1, . . . , K}, define Ikc := {1, . . . , N } \ Ik . (b) For each k âˆˆ [K], construct an ML estimator
Î·Ì‚0,k = Î·Ì‚0 ((Wi )iâˆˆIkc )
of Î·0 , where Î·Ì‚0,k is a random element in T , and where randomness depends only on the subset of
data indexed by Ikc . (c) Construct the estimator Î¸Ìƒ0 as the solution to
K
1 
En,k [Ïˆ(W ; Î¸Ìƒ0 , Î·Ì‚0,k )] = 0,
K k=1

C

2017 Royal Economic Society.

(3.4)

C24

V. Chernozhukov et al.

where Ïˆ is the Neyman orthogonal score, and En,k is the empirical expectation over the kth fold
of the data; that is, En,k [Ïˆ(W )] = nâˆ’1 iâˆˆIk Ïˆ(Wi ). If achievement of exact 0 is not possible,
define the estimator Î¸Ìƒ0 of Î¸0 as an approximate N -solution,
K
K
1 
1 






En,k [Ïˆ(W ; Î¸Ìƒ0 , Î·Ì‚0,k )]] â‰¤ inf 
En,k [Ïˆ(W ; Î¸0 , Î·Ì‚0,k )]] + N ,

Î¸âˆˆ
K k=1
K k=1

(3.5)

for N = o(Î´N N âˆ’1/2 ), where (Î´N )Nâ‰¥1 is some sequence of positive constants converging to zero.
R EMARK 3.1. (R ECOMMENDATIONS) The choice of K has no asymptotic impact under our
conditions but, of course, the choice of K may matter in small samples. Intuitively, larger values
of K provide more observations in Ikc from which to estimate the high-dimensional nuisance
functions, which seems to be the more difficult part of the problem. We have found moderate
values of K, such as 4 or 5, to work better than K = 2 in a variety of empirical examples and in
simulations. Moreover, we generally recommend DML2 over DML1 though in some problems
such as estimation of ATE in the interactive model, which we discuss later, there is no difference
between the two approaches. In most other problems, DML2 is better behaved because the pooled
empirical Jacobian for the equation in (3.4) exhibits more stable behaviour than the separate
empirical Jacobians for the equation in (3.1).
3.2. Moment condition models with linear scores
We first consider the case of linear scores, where
Ïˆ(w; Î¸, Î·) = Ïˆ a (w; Î·)Î¸ + Ïˆ b (w; Î·),

for all w âˆˆ W,

Î¸âˆˆ

, Î· âˆˆ T.

(3.6)

Let c0 > 0, c1 > 0, s > 0 and q > 2 be some finite constants such that c0 â‰¤ c1 , and let {Î´N }Nâ‰¥1
and {N }Nâ‰¥1 be some sequences of positive constants converging to zero such that Î´N â‰¥ N âˆ’1/2 .
Also, let K â‰¥ 2 be some fixed integer, and let {PN }Nâ‰¥1 be some sequence of sets of probability
distributions P of W on W.
A SSUMPTION 3.1 (L INEAR SCORES WITH APPROXIMATE NEYMAN ORTHOGONALITY) For
all N â‰¥ 3 and P âˆˆ PN , the following conditions hold. (a) The true parameter value Î¸0 obeys
(2.1). (b) The score Ïˆ is linear in the sense of (3.6). (c) The map Î· â†’ EP [Ïˆ(W ; Î¸, Î·)] is twice
continuously Gateaux-differentiable on T . (d) The score Ïˆ obeys the Neyman orthogonality
or, more generally, the Neyman Î»N near-orthogonality condition at (Î¸0 , Î·0 ) with respect to the
nuisance realization set TN âŠ‚ T for




Î»N := sup âˆ‚Î· EP Ïˆ(W ; Î¸0 , Î·0 )[Î· âˆ’ Î·0 ] â‰¤ Î´N N âˆ’1/2 .
Î·âˆˆTN

(e) The identification condition holds; namely, the singular values of the matrix
J0 := EP [Ïˆ a (W ; Î·0 )]
are between c0 and c1 .
Assumption 3.1 requires scores to be Neyman orthogonal or near-orthogonal and imposes
mild smoothness requirements as well as the canonical identification condition.

C

2017 Royal Economic Society.

Double/debiased machine learning

C25

A SSUMPTION
ESTIMATORS )

3.2 (S CORE REGULARITY AND QUALITY OF NUISANCE PARAMETER
For all N â‰¥ 3 and P âˆˆ PN , the following conditions hold. (a) Given a random
subset I of [N] of size n = N/K, the nuisance parameter estimator Î·Ì‚0 = Î·Ì‚0 ((Wi )iâˆˆI c ) belongs to
the realization set TN with probability at least 1 âˆ’ N , where TN contains Î·0 and is constrained
by the next conditions. (b) The moment conditions hold:
mN := sup (EP [Ïˆ(W ; Î¸0 , Î·)q ])1/q â‰¤ c1 ;
Î·âˆˆTN

mN := sup (EP [Ïˆ a (W ; Î·)q ])1/q â‰¤ c1 .
Î·âˆˆTN

(c) The following conditions on the statistical rates rN , rN , and Î»N hold:
rN := sup EP [Ïˆ a (W ; Î·)] âˆ’ EP [Ïˆ a (W ; Î·0 )] â‰¤ Î´N ,
Î·âˆˆTN

rN := sup (EP [Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 ])1/2 â‰¤ Î´N ,
Î·âˆˆTN

Î»N :=

sup
râˆˆ(0,1),Î·âˆˆTN

âˆš
âˆ‚r2 EP [Ïˆ(W ; Î¸0 , Î·0 + r(Î· âˆ’ Î·0 ))] â‰¤ Î´N / N .

(d) The variance of the score Ïˆ is non-degenerate: All eigenvalues of the matrix
EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ]
are bounded from below by c0 .
Assumptions 3.2(a)â€“(c) state that the estimator of the nuisance parameter belongs to the
realization set TN âŠ‚ T , which is a shrinking neighbourhood of Î·0 , that contracts around Î·0 with
the rate determined by the â€˜statisticalâ€™ rates rN , rN and Î»N . These rates are not given in terms of
the norm  Â· T on T , but rather are the intrinsic rates that are most connected to the statistical
problem at hand. However, in smooth problems, as discussed below this translates, in the worst
cases, to the crude requirement that the nuisance parameters are estimated at the rate o(N âˆ’1/4 ).
The conditions in Assumption 3.2 embody refined requirements on the quality of nuisance
parameter estimators. In many applications, where (Î¸, Î·) â†’ Ïˆ(W ; Î¸, Î·) is smooth, we can bound
rN  ÎµN ,

rN  ÎµN ,

Î»N  ÎµN2 ,

(3.7)

where ÎµN is the upper bound on the rate of convergence of Î·Ì‚0 to Î·0 with respect to the norm
 Â· T =  Â· P ,2 :
Î·Ì‚0 âˆ’ Î·T  ÎµN .
Note that TN can be chosen as the set of Î· that is within a neighbourhood of size ÎµN of Î·0 ,
possibly with other restrictions, in this case. If only (3.7) holds, Assumption 3.2, particularly
Î»N = o(N âˆ’1/2 ), imposes the (crude) rate requirement
ÎµN = o(N âˆ’1/4 ).

(3.8)

This rate is achievable for many ML methods under structured assumptions on the nuisance
parameters. Among many others, see Bickel et al. (2009), BuÌˆhlmann and van de Geer (2011),
Belloni et al. (2011, 2012) and Belloni and Chernozhukov (2011, 2013) for 1 -penalized and
related methods in a variety of sparse models, Kozbur (2016) for forward selection in sparse

C

2017 Royal Economic Society.

C26

V. Chernozhukov et al.

models, Luo and Spindler (2016) for L2 -boosting in sparse linear models, Wager and Walther
(2016) for concentration results for a class of regression trees and random forests, and Chen and
White (1999) for a class of neural nets.
However, the presented conditions allow for more refined statements than (3.8). We note
that many important structured problems â€“ such as estimation of parameters in PLR models,
estimation of parameters in partially linear structural equation models, and estimation of ATEs
under unconfoundedness â€“ are such that some cross-derivatives vanish, allowing more refined
requirements than (3.8). This feature allows us to require much finer conditions on the quality of
the nuisance parameter estimators than the crude bound (3.8). For example, in many problems
Î»N = 0,

(3.9)

because the second derivatives vanish,
âˆ‚r2 EP [Ïˆ(W ; Î¸0 , Î·0 + r(Î· âˆ’ Î·0 ))] = 0.
This occurs in the following important examples:
(1) the optimal instrument problem (see Belloni et al., 2012);
(2) the PLR model when m0 (X) = 0 or is otherwise known (see Section 4);
(3) the treatment effect examples when the propensity score is known, which includes
randomized control trials as an important special case (see Section 5).
If both (3.7) and (3.9) hold, Assumption 3.2, particularly rN = o(1) and rN = o(1), imposes
the weakest possible rate requirement:
ÎµN = o(1).
We note that similar refined rates have appeared in the context of estimation of treatment effects
in high-dimensional settings under sparsity; see Farrell (2015) and Athey et al. (2016) and the
related discussion in Remark 5.2. Our refined rate results complement this work by applying to
a broad class of estimation contexts, including estimation of ATEs, and to a broad set of ML
estimators.
T HEOREM 3.1. (P ROPERTIES OF THE DML) Suppose that Assumptions 3.1 and 3.2 hold. In
addition, suppose âˆš
that Î´N â‰¥ N âˆ’1/2 for all N â‰¥ 1. Then the DML1 and DML2 estimators Î¸Ìƒ0
concentrate in a 1/ N neighbourhood of Î¸0 and are approximately linear and centred Gaussian,
âˆš

N
1 
N Ïƒ âˆ’1 (Î¸Ìƒ0 âˆ’ Î¸0 ) = âˆš
ÏˆÌ„(Wi ) + OP (ÏN )  N(0, Id ),
N i=1

(3.10)

uniformly over P âˆˆ PN , where the size of the remainder term obeys
ÏN := N âˆ’1/2 + rN + rN + N 1/2 Î»N + N 1/2 Î»N  Î´N .

(3.11)

Here, ÏˆÌ„(Â·) := âˆ’Ïƒ âˆ’1 J0âˆ’1 Ïˆ(Â·, Î¸0 , Î·0 ) is the influence function, and the approximate variance is
Ïƒ 2 := J0âˆ’1 EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ](J0âˆ’1 ) .
The result establishes that the estimator based on the orthogonal scores achieves the root-N
rate of convergence and is approximately normally distributed. It is noteworthy that this
convergence result, both the rate of concentration and the distributional approximation, holds

C

2017 Royal Economic Society.

Double/debiased machine learning

C27

uniformly with respect to P varying over an expanding class of probability measures PN . This
means that the convergence holds under any sequence of probability distributions (PN )Nâ‰¥1
with PN âˆˆ PN for each N, which in turn implies that the results are robust with respect to
perturbations of a given P along such sequences. The same property can be shown to fail for
methods not based on orthogonal scores.
T HEOREM 3.2. (VARIANCE ESTIMATOR FOR DML) Suppose that Assumptions 3.1 and 3.2 hold.
for all N â‰¥ 1. Consider the following estimator
In addition, suppose that Î´N â‰¥ N âˆ’[(1âˆ’2/q)âˆ§1/2]
âˆš
of the asymptotic variance matrix of N(Î¸Ìƒ0 âˆ’ Î¸0 ):
ÏƒÌ‚ 2 = JË†0âˆ’1

K
1 
En,k [Ïˆ(W ; Î¸Ìƒ0 , Î·Ì‚0,k )Ïˆ(W ; Î¸Ìƒ0 , Î·Ì‚0,k ) ](JË†0âˆ’1 ) ,
K k=1

where
K
1 
JË†0 =
En,k [Ïˆ a (W ; Î·Ì‚0,k )],
K k=1

and Î¸Ìƒ0 is either the DML1 or the DML2 estimator. This estimator concentrates around the true
variance matrix Ïƒ 2 ,
ÏƒÌ‚ 2 = Ïƒ 2 + OP (N ),

N := N âˆ’[(1âˆ’2/q)âˆ§1/2] + rN + rN  Î´N .

Moreover, ÏƒÌ‚ 2 can replace Ïƒ 2 in the statement of Theorem 3.1 with the size of the remainder term
updated as ÏN = N âˆ’[(1âˆ’2/q)âˆ§1/2] + rN + rN + N 1/2 Î»N + N 1/2 Î»N .
Theorems 3.1 and 3.2 can be used for standard construction of confidence regions, which are
uniformly valid over a large, interesting class of models:
C OROLLARY 3.1. (U NIFORMLY VALID CONFIDENCE BANDS) Under the conditions of Theorem
3.2, suppose we are interested in the scalar parameter  Î¸0 for some dÎ¸ Ã— 1 vector . Then the
confidence interval


CI :=  Î¸Ìƒ0 Â± âˆ’1 (1 âˆ’ Î±/2)  ÏƒÌ‚ 2 /N
obeys
sup |P r P ( Î¸0 âˆˆ CI) âˆ’ (1 âˆ’ Î±)| â†’ 0.

P âˆˆPN

Indeed, the above theorem implies that CI obeys PrPN ( Î¸0 âˆˆ CI) â†’ (1 âˆ’ Î±) under any
sequence {PN } âˆˆ PN , which implies that these claims hold uniformly in P âˆˆ PN . For example,
one may choose {PN } such that, for some N â†’ 0
sup |PrP ( Î¸0 âˆˆ CI) âˆ’ (1 âˆ’ Î±)| â‰¤ |PrPN ( Î¸0 âˆˆ CI) âˆ’ (1 âˆ’ Î±)| + N â†’ 0.

P âˆˆPN

Next we note that the estimators need not be semi-parametrically efficient, but under some
conditions they can be.
C OROLLARY 3.2. (C ASES WITH SEMI-PARAMETRIC EFFICIENCY) Under the conditions of
Theorem 3.1, if the score Ïˆ is efficient for estimating Î¸0 at a given P âˆˆ P âŠ‚ PN , in the semiparametric sense as defined in van der Vaart (1998), then the large sample variance Ïƒ02 of Î¸Ìƒ0
reaches the semi-parametric efficiency bound at this P relative to the model P.

C

2017 Royal Economic Society.

C28

V. Chernozhukov et al.

3.3. Models with non-linear scores
Let c0 > 0, c1 > 0, a > 1, v > 0, s > 0 and q > 2 be some finite constants, and let {Î´N }Nâ‰¥1 ,
{N }Nâ‰¥1 and {Ï„N }Nâ‰¥1 be some sequences of positive constants converging to zero. To derive the
properties of the DML estimator, we will use the following assumptions.
A SSUMPTION 3.3. (N ON-LINEAR MOMENT CONDITION PROBLEM WITH APPROXIMATE
â‰¥ 3 and P âˆˆ PN , the following conditions hold. (a) The
true parameter value Î¸0 obeys (2.1), and contains a ball of radius c1 N âˆ’1/2 log N centred at Î¸0 .
(b) The map (Î¸, Î·) â†’ EP [Ïˆ(W ; Î¸, Î·)] is twice continuously Gateaux-differentiable on Ã— T .
(c) For all Î¸ âˆˆ , the identification relation
NEYMAN ORTHOGONALITY ) For all N

2EP [Ïˆ(W ; Î¸, Î·0 )] â‰¥ J0 (Î¸ âˆ’ Î¸0 ) âˆ§ c0
is satisfied, for the Jacobian matrix
J0 := âˆ‚Î¸ {EP [Ïˆ(W ; Î¸, Î·0 )]}|Î¸=Î¸0
having singular values between c0 and c1 . (d) The score Ïˆ obeys the Neyman orthogonality or,
more generally, the Neyman near-orthogonality with Î»N = Î´N N âˆ’1/2 for the set TN âŠ‚ T .
Assumption 3.3 is mild and rather standard in moment condition problems. Assumption
3.3(a) requires Î¸0 to be sufficiently separated from the boundary of . Assumption 3.3(b)
only requires differentiability of the function (Î¸, Î·) â†’ EP [Ïˆ(W ; Î¸, Î·)] and does not require
differentiability of the function (Î¸, Î·) â†’ Ïˆ(W ; Î¸, Î·). Assumption 3.3(c) implies sufficient
identifiability of Î¸0 . Assumption 3.3(d) is the orthogonality condition that has already been
extensively discussed.
A SSUMPTION 3.4. (S CORE REGULARITY AND REQUIREMENTS ON THE QUALITY OF
ESTIMATION OF NUISANCE PARAMETERS ) Let K be a fixed integer. For all N â‰¥ 3 and P âˆˆ PN ,

the following conditions hold. (a) Given a random subset I of {1, . . . , N } of size n = N/K,
we have that the nuisance parameter estimator Î·Ì‚0 = Î·Ì‚0 ((Wi )iâˆˆI c ) belongs to the realization set
TN with probability at least 1 âˆ’ N , where TN contains Î·0 and is constrained by conditions
given below. (b) The parameter space
is bounded and for each Î· âˆˆ TN , the function class
F1,Î· = {Ïˆj (Â·, Î¸, Î·) : j = 1, . . . , dÎ¸ , Î¸ âˆˆ } is suitably measurable and its uniform covering
entropy obeys
sup log N(F1,Î· Q,2 , F1,Î· ,  Â· Q,2 ) â‰¤ v log(a/),

for all 0 <  â‰¤ 1,

(3.12)

Q

where F1,Î· is a measurable envelope for F1,Î· that satisfies F1,Î· P ,q â‰¤ c1 . (c) The following
conditions on the statistical rates rN , rN , and Î»N hold:
rN :=

EP [Ïˆ(W ; Î¸, Î·) âˆ’ EP [Ïˆ(W ; Î¸, Î·0 )] â‰¤ Î´N Ï„N ,

sup
Î·âˆˆTN ,Î¸âˆˆ

rN :=
Î»N :=

sup

Î·âˆˆTN ,Î¸âˆ’Î¸0 â‰¤Ï„N

sup

(EP [Ïˆ(W ; Î¸, Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 ])1/2 and rN log1/2 (1/rN ) â‰¤ Î´N ,

râˆˆ(0,1),Î·âˆˆTN ,Î¸âˆ’Î¸0 â‰¤Ï„N

âˆ‚r2 EP [Ïˆ(W ; Î¸0 , Î·0 + r(Î¸ âˆ’ Î¸0 ) + r(Î· âˆ’ Î·0 ))] â‰¤ Î´N N âˆ’1/2 .


C

2017 Royal Economic Society.

Double/debiased machine learning

C29

(d) The variance of the score is non-degenerate. All eigenvalues of the matrix
EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ]
are bounded from below by c0 .
Assumptions 3.3(a)â€“(c) state that the estimator of the nuisance parameter belongs to the
realization set TN âŠ‚ T , which is a shrinking neighbourhood of Î·0 that contracts at the statistical
rates rN , rN and Î»N . These rates are not given in terms of the norm  Â· T on T , but rather
are intrinsic rates that are connected to the statistical problem at hand. In smooth problems,
these conditions translate to the crude requirement that nuisance parameters are estimated at the
o(N âˆ’1/4 ) rate, as discussed previously in the case with linear scores. However, these conditions
can be refined as, for example, when Î»N = 0 or when some cross-derivatives vanish in Î»N ;
see the linear case in the previous subsection for further discussion. Suitable measurability and
pointwise entropy conditions, required in Assumption 3.4(b), are mild regularity conditions that
are satisfied in all practical cases. The assumption of a bounded parameter space in Assumption
3.4(b) is embedded in the entropy condition, but we state it separately for clarity. This assumption
was not needed in the linear case, and it can be removed in the non-linear case with the imposition
of more complicated Huber-like regularity conditions. Assumption 3.4(c) is a set of mild growth
conditions.
R EMARK 3.2. (R ATE REQUIREMENTS ON NUISANCE PARAMETER ESTIMATORS) Similar
to the discussion in the linear case, the conditions in Assumption 3.4 are very flexible
and embody refined requirements on the quality of the nuisance parameter estimators. The
conditions essentially reduce to the previous conditions in the linear case, with the exception
of compactness, which is imposed to make the conditions easy to verify in non-linear cases.
T HEOREM 3.3. (P ROPERTIES OF THE DML FOR NON-LINEAR SCORES) Suppose that
Assumptions 3.3 and 3.4 hold. In addition, suppose that Î´N â‰¥ N âˆ’1/2+1/q log N and that
N âˆ’1/2 log
âˆš N â‰¤ Ï„N â‰¤ Î´N for all N â‰¥ 1. Then the DML1 and DML2 estimators Î¸Ìƒ0 concentrate
in a 1/ N neighbourhood of Î¸0 , and are approximately linear and centred Gaussian,
âˆš

N
1 
N Ïƒ âˆ’1 (Î¸Ìƒ0 âˆ’ Î¸0 ) = âˆš
ÏˆÌ„(Wi ) + OP (ÏN )  N(0, I),
N i=1

uniformly over P âˆˆ PN , where the size of the remainder term obeys
ÏN := N âˆ’1/2+1/q log N + rN log1/2 (1/rN ) + N 1/2 Î»N + N 1/2 Î»N  Î´N ,
ÏˆÌ„(Â·) := âˆ’Ïƒ0âˆ’1 J0âˆ’1 Ïˆ(Â·, Î¸0 , Î·0 ) is the influence function, and the approximate variance is
Ïƒ 2 := J0âˆ’1 EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ](J0âˆ’1 ) .
Moreover, in the statement above, Ïƒ 2 can be replaced by a consistent estimator ÏƒÌ‚ 2 , obeying
ÏƒÌ‚ 2 = Ïƒ 2 + oP (N ) uniformly in P âˆˆ PN , with the size of the remainder term updated as ÏN =
ÏN + N . Furthermore, Corollaries 3.1 and 3.2 continue to hold under the conditions of this
theorem.
3.4. Finite-sample adjustments to incorporate uncertainty induced by sample splitting
The estimation technique developed in this paper relies on subsamples obtained by randomly
partitioning the sample: an auxiliary sample for estimating the nuisance functions and a main

C

2017 Royal Economic Society.

C30

V. Chernozhukov et al.

sample for estimating the parameter of interest. Although the specific sample partition has no
impact on estimation results asymptotically, the effect of the particular random split on the
estimate can be important in finite samples. To make the results more robust with respect to
the partitioning, we propose to repeat the DML estimator S times, obtaining the estimates
Î¸Ìƒ0s ,

s = 1, . . . , S.

Features of these estimates may then provide insight into the sensitivity of results to the sample
splitting, and we can report results that incorporate features of this set of estimates that should
be less driven by any particular sample-splitting realization.
D EFINITION 3.3. (I NCORPORATING THE IMPACT OF SAMPLE SPLITTING USING MEAN AND
For point estimation, we define

MEDIAN METHODS )

Î¸Ìƒ0mean =

S
1 s
Î¸Ìƒ
S s=1 0

or

Î¸Ìƒ0median = median{Î¸Ëœ0s }Ss=1 ,

where the median operation is applied coordinate-wise. To quantify and incorporate the variation
introduced by sample splitting, we consider variance estimators,
ÏƒÌ‚ 2,mean =

S

1 2
ÏƒÌ‚s + (Î¸Ì‚s âˆ’ Î¸Ìƒ mean )(Î¸Ì‚s âˆ’ Î¸Ìƒ mean ) ,
S s=1

(3.13)

and a more robust version,
ÏƒÌ‚ 2,median = median{ÏƒÌ‚s2 + ((Î¸Ì‚s âˆ’ Î¸Ìƒ median )(Î¸Ì‚s âˆ’ Î¸Ìƒ median ) }Ss=1 ,

(3.14)

where the median picks out the matrix with median operator norm, which preserve non-negative
definiteness.
We recommend using medians, reporting Î¸Ìƒ0median and ÏƒÌ‚ 2,median , as these quantities are more
robust to outliers.
C OROLLARY 3.3. If S is fixed, as N â†’ âˆž and maintaining either Assumptions 3.1 and 3.2 or
Assumptions 3.3 and 3.4 as appropriate, Î¸Ìƒ0mean and Î¸Ìƒ0median are first-order equivalent to Î¸Ìƒ0 and
obey the conclusions of Theorems 3.1 and 3.2 or of Theorem 3.3. Moreover, ÏƒÌ‚ 2,median and ÏƒÌ‚ 2,mean
can replace ÏƒÌ‚ in the statement of the appropriate theorems.
It would be interesting to investigate the behaviour under the regime where S â†’ âˆž as
N â†’ âˆž.

4. INFERENCE IN PARTIALLY LINEAR MODELS
4.1. Inference in partially linear regression models
Here we revisit the PLR model
Y = DÎ¸0 + g0 (X) + U,
D = m0 (X) + V ,

EP [U | X, D] = 0,

(4.1)

EP [V | X] = 0.

(4.2)

C

2017 Royal Economic Society.

C31

Double/debiased machine learning

The parameter of interest is the regression coefficient Î¸0 . If D is conditionally exogenous (as good
as randomly assigned conditional on covariates), then Î¸0 measures the average causal/treatment
effect of D on potential outcomes.
The first approach to inference on Î¸0 , which we described in the Introduction, is to employ
the DML method using the score function
Ïˆ(W ; Î¸, Î·) := {Y âˆ’ DÎ¸ âˆ’ g(X)}(D âˆ’ m(X)),

Î· = (g, m),

(4.3)

where W = (Y, D, X), and g and m are P -square-integrable functions mapping the support of
X to R. It is easy to see that Î¸0 satisfies the moment condition EP Ïˆ(W ; Î¸0 , Î·0 ) = 0, and also the
orthogonality condition âˆ‚Î· EP Ïˆ(W ; Î¸0 , Î·0 )[Î· âˆ’ Î·0 ] = 0 where Î·0 = (g0 , m0 ).
A second approach employs the Robinson-style â€˜partialling-outâ€™ score function
Ïˆ(W ; Î¸, Î·) := {Y âˆ’ (X) âˆ’ Î¸ (D âˆ’ m(X))}(D âˆ’ m(X)),

Î· = (, m),

(4.4)

where W = (Y, D, X) and  and m are P -square-integrable functions mapping the support of X
to R. This gives an alternative parametrization of the score function above, and using this score
is first-order equivalent to using the previous score. It is easy to see that Î¸0 satisfies the moment
condition EP Ïˆ(W ; Î¸0 , Î·0 ) = 0, and also the orthogonality condition âˆ‚Î· EP Ïˆ(W ; Î¸0 , Î·0 )[Î· âˆ’
Î·0 ] = 0, for Î·0 = (0 , m0 ), where 0 (X) = EP [Y |X].
In the partially linear model, the DML approach complements Belloni et al. (2013, 2014a,b,
2015), Zhang and Zhang (2014), van de Geer et al. (2014) and Javanmard and Montanari
(2014b), all of which consider estimation and inference for parameters within the partially
linear model using lasso-type methods without cross-fitting. By relying upon cross-fitting, the
DML approach allows for the use of a much broader collection of ML methods for estimating
the nuisance functions and also allows relaxation of sparsity conditions in the case where
lasso or other sparsity-based estimators are used. Both the DML approach and the approaches
taken in the aforementioned papers can be seen as heuristically debiasing the score function
(Y âˆ’ DÎ¸ âˆ’ g(X))D, which does not possess the orthogonality property unless m0 (X) = 0.
âˆž
Let (Î´N )âˆž
n=1 and (N )n=1 be sequences of positive constants approaching 0 as before. Also,
let c, C and q be fixed strictly positive constants such that q > 4, and let K â‰¥ 2 be a fixed
integer. Moreover, for any Î· = (1 , 2 ), where 1 is a function 1 and 2 are functions mapping
the support of X to R, denote Î·P ,q = 1 P ,q âˆ¨ 2 P ,q . For simplicity, assume that N/K is
an integer.
A SSUMPTION 4.1. (R EGULARITY CONDITIONS FOR PARTIALLY LINEAR REGRESSION
Let P be the collection of probability laws P for the triple W = (Y, D, X) such
that (a) (4.1) and (4.2) hold; (b) Y P ,q + DP ,q â‰¤ C; (c) U V P ,2 â‰¥ c2 and EP [V 2 ] â‰¥
c; (d) EP [U 2 | X]P ,âˆž â‰¤ C and EP [V 2 | X]P ,âˆž â‰¤ C; and (e) given a random subset
I of [N ] of size n = N/K, the nuisance parameter estimator Î·Ì‚0 = Î·Ì‚0 ((Wi )iâˆˆI c ) obeys the
following conditions for all n â‰¥ 1. With P -probability no less than 1 âˆ’ N , Î·Ì‚0 âˆ’ Î·0 P ,q â‰¤ C,
Î·Ì‚0 âˆ’ Î·0 P ,2 â‰¤ Î´N and (i) for the score Ïˆ in (4.3), where Î·Ì‚0 = (gÌ‚0 , mÌ‚0 ), mÌ‚0 âˆ’ m0 P ,2 Ã— gÌ‚0 âˆ’
g0 P ,2 â‰¤ Î´N N âˆ’1/2 , and (ii) for the score Ïˆ in (4.4), where Î·Ì‚0 = (Ë†0 , mÌ‚0 ), mÌ‚0 âˆ’ m0 P ,2 Ã—
(mÌ‚0 âˆ’ m0 P ,2 + Ë†0 âˆ’ 0 P ,2 ) â‰¤ Î´N N âˆ’1/2 .
MODEL )

R EMARK 4.1. (R ATE CONDITIONS FOR ESTIMATORS OF NUISANCE PARAMETERS) The only
non-primitive condition here is the assumption on the rate of estimating the nuisance parameters.
These rates of convergence are available for most often used ML methods and are case-specific,
so we do not restate conditions that are needed to reach these rates.

C

2017 Royal Economic Society.

C32

V. Chernozhukov et al.

The following theorem follows as a corollary to the results in Section 3 by verifying
Assumptions 3.1 and 3.2, and it will be proven as a special case of Theorem 4.2 below.
T HEOREM 4.1. (DML INFERENCE ON REGRESSION COEFFICIENTS IN THE PARTIALLY
Suppose that Assumption 4.1 holds. Then the DML1 and DML2
estimators Î¸Ìƒ0 constructed in Definitions 3.1 and 3.2 using the score in either (4.3) or (4.4) are
first-order equivalent and obey
âˆš
Ïƒ âˆ’1 N(Î¸Ìƒ0 âˆ’ Î¸0 )  N(0, 1),
LINEAR REGRESSION MODEL )

uniformly over P âˆˆ P, where Ïƒ 2 = (EP [V 2 ])âˆ’1 EP [V 2 U 2 ](EP [V 2 ])âˆ’1 . Moreover, the result
continues to hold if Ïƒ 2 is replaced by ÏƒÌ‚ 2 defined in Theorem 3.2. Consequently, confidence
regions based upon the DML estimators Î¸Ìƒ0 have uniform asymptotic validity:
âˆš
lim sup |P r P (Î¸0 âˆˆ [Î¸Ìƒ0 Â± âˆ’1 (1 âˆ’ Î±/2)ÏƒÌ‚ / N]) âˆ’ (1 âˆ’ Î±)| = 0.
Nâ†’âˆž P âˆˆP

R EMARK 4.2. (A SYMPTOTIC EFFICIENCY UNDER HOMOSCEDASTICITY) Under conditional
homoscedasticity, i.e. E[U 2 |Z] = E[U 2 ], the asymptotic variance Ïƒ 2 reduces to E[V 2 ]âˆ’1 E[U 2 ],
which is the semi-parametric efficiency bound for Î¸ .
R EMARK 4.3. (T IGHTNESS OF CONDITIONS UNDER CROSS-FITTING) The conditions in
Theorem 4.1 are sharp, though they are simplified for ease of presentation. The sharpness can be
understood by examining the case where the regression function g0 and the propensity function
N and s m
N. They are estimated by 1 -penalized
m0 are sparse with sparsity indices s g
and
mÌ‚
that
have
sparsity
indices
of
orders
s g and s m and converge to g0 and m0 at
estimators
gÌ‚
0
0
âˆš m
âˆš g
the rates s /N and s /N (ignoring logs). The rate conditions in Assumption 4.1 then require
(ignoring logs) that


s g /N s m /N
N âˆ’1/2 â‡” s g s m
N,
which is much weaker than the condition
(s g )2 + (s m )2

N

(ignoring logs) required without sample splitting. For example, if the propensity function m0 is
very sparse (low sm ), then the regression function is allowed to be quite dense (high sg ), and
vice versa. If the propensity function is known (s m = 0) or can be estimated at the N âˆ’1/2 rate,
then only consistency for gÌ‚0 is needed. Such comparisons also extend to approximately sparse
models.
4.2. Inference in partially linear IV models
Here we extend the PLR model studied in Section 4.1 to allow for IV identification. Specifically,
we consider the model
Y = DÎ¸0 + g0 (X) + U,
Z = m0 (X) + V ,

EP [U | X, Z] = 0,

(4.5)

EP [V | X] = 0,

(4.6)

where Z is the instrumental variable. As before, the parameter of interest is Î¸ and its true value
is Î¸0 . If Z = D, the model (4.5)â€“(4.6) coincides with (4.1)â€“(4.2) but is otherwise different.

C

2017 Royal Economic Society.

C33

Double/debiased machine learning

To estimate Î¸0 and to perform inference on it, we use the score
Ïˆ(W ; Î¸, Î·) := (Y âˆ’ DÎ¸ âˆ’ g(X))(Z âˆ’ m(X)),

Î· = (g, m),

(4.7)

where W = (Y, D, X, Z) and g and m are P -square-integrable functions mapping the support of
X to R. Alternatively, we can use the Robinson-style score
Ïˆ(W ; Î¸, Î·) := (Y âˆ’ (X) âˆ’ Î¸ (D âˆ’ r(X)))(Z âˆ’ m(X)),

Î· = (, m, r),

(4.8)

where W = (Y, D, X, Z) and , m and r are P -square-integrable functions mapping the support
of X to R. It is straightforward to verify that both scores satisfy the moment condition
EP [Ïˆ(W ; Î¸0 , Î·0 )] = 0 and also the orthogonality condition âˆ‚Î· EP [Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] = 0,
for Î·0 = (g0 , m0 ) in the former case and Î·0 = (0 , m0 , r0 ) for 0 and r0 defined by 0 (X) =
EP [Y | X] and r0 (X) = EP [D | X], respectively, in the latter case.8
Note that the score in (4.8) has a minor advantage over the score in (4.7) because all of its
nuisance parameters are conditional mean functions, which can be directly estimated by the ML
methods. If one prefers to use the score in (4.7), one has to construct an estimator of g0 first.
To do so, one can first obtain a DML estimator of Î¸0 based on the score in (4.8), say Î¸Ìƒ0 . Then,
using the fact that g0 (X) = EP [Y âˆ’ DÎ¸0 | X], one can construct an estimator gÌ‚0 by applying an
ML method to regress Y âˆ’ D Î¸Ìƒ0 on X. Alternatively, one can use assumption-specific methods to
directly estimate g0 , without using the score (4.8) first. For example, if g0 can be approximated
by a sparse linear combination of a large set of transformations of X, one can use the methods of
Gautier and Tsybakov (2014) to obtain an estimator of g0 .
âˆž
Let (Î´N )âˆž
n=1 and (N )n=1 be sequences of positive constants approaching 0 as before. Also,
let c, C and q be fixed strictly positive constants such that q > 4, and let K â‰¥ 2 be a fixed
integer. Moreover, for any Î· = (j )lj =1 mapping the support of X to Rl , denote Î·P ,q =
max1â‰¤j â‰¤l j P ,q . For simplicity, assume that N/K is an integer.
A SSUMPTION 4.2. (R EGULARITY CONDITIONS FOR PARTIALLY LINEAR IV MODEL) For all
probability laws P âˆˆ P for the quadruple W = (Y, D, X, Z) the following conditions hold:
(a) equations (4.5) and (4.6) hold; (b) Y P ,q + DP ,q + ZP ,q â‰¤ C; (c) U V P ,2 â‰¥ c2
and |EP [DV ]| â‰¥ c; (d) EP [U 2 | X]P ,âˆž â‰¤ C and EP [V 2 | X]P ,âˆž â‰¤ C; and (e) given a
random subset I of [N] of size n = N/K, the nuisance parameter estimator Î·Ì‚0 = Î·Ì‚0 ((Wi )iâˆˆI c )
obeys the following conditions. With P -probability no less than 1 âˆ’ N , Î·Ì‚0 âˆ’ Î·0 P ,q â‰¤ C,
Î·Ì‚0 âˆ’ Î·0 P ,2 â‰¤ Î´N and (i) for the score Ïˆ in (4.7), where Î·Ì‚0 = (gÌ‚0 , mÌ‚0 ), mÌ‚0 âˆ’ m0 P ,2 Ã— gÌ‚0 âˆ’
âˆ’1/2
Ë†
, and (ii) for
g
 the score Ïˆ in (4.8), where Î·Ì‚0 = (0 , mÌ‚0 , rÌ‚0 ), mÌ‚0 âˆ’ m0 P ,2 Ã—
0 P ,2 â‰¤ Î´N N
âˆ’1/2
rÌ‚0 âˆ’ r0 P ,2 + Ë†0 âˆ’ 0 P ,2 â‰¤ Î´N N
.
The following theorem follows as a corollary to the results in Section 3 by verifying
Assumptions 3.1 and 3.2.
T HEOREM 4.2. (DML INFERENCE ON REGRESSION COEFFICIENTS IN THE PARTIALLY
LINEAR IV MODEL ) Suppose that Assumption 4.2 holds. Then the DML1 and DML2 estimators
8 It is interesting to note that the methods for constructing Neyman orthogonal scores described in Section 2 may
give scores that are different from those in (4.7) and (4.8). For example, applying the method for conditional moment
restriction problems in Section 2.2.4 with (R) = 1 gives the score Ïˆ(W ; Î¸, Î·) = (Y âˆ’ DÎ¸ âˆ’ g(X))(r(Z, X) âˆ’ f (X)),
where the true values of r(Z, X) and f (X) are r0 (Z, X) = EP [D | Z, X] and f0 (X) = EP [D | X], respectively. It may
be interesting to compare properties of the DML estimators Î¸Ìƒ0 based on this score with those based on (4.7) and (4.8) in
future work.


C

2017 Royal Economic Society.

C34

V. Chernozhukov et al.

Î¸Ìƒ0 constructed in Definitions 3.1
âˆš and 3.2 using the score in either (4.7) or (4.8) are firstorder equivalent and obey Ïƒ âˆ’1 N(Î¸Ìƒ0 âˆ’ Î¸0 )  N(0, 1) uniformly over P âˆˆ P, where Ïƒ 2 =
(EP [DV ])âˆ’1 EP [V 2 U 2 ](EP [DV ])âˆ’1 . Moreover, the result continues to hold if Ïƒ 2 is replaced
by ÏƒÌ‚ 2 defined in Theorem 3.2. Consequently, confidence regions based upon the DML estimators
Î¸Ìƒ0 have uniform asymptotic validity:
âˆš
lim sup |P r P (Î¸0 âˆˆ [Î¸Ìƒ0 Â± âˆ’1 (1 âˆ’ Î±/2)ÏƒÌ‚ / N]) âˆ’ (1 âˆ’ Î±)| = 0.
Nâ†’âˆž P âˆˆP

5. INFERENCE ON TREATMENT EFFECTS IN THE INTERACTIVE MODEL
5.1. Inference on ATE and ATTE
In this section, we specialize the results of Section 3 to estimate treatment effects under the
unconfoundedness assumption of Rosenbaum and Rubin (1983). Within this setting, there is a
large classical literature focused on low-dimensional settings that provides methods for adjusting
for confounding variables including regression methods, propensity score adjustment methods,
matching methods, and â€˜doubly-robustâ€™ combinations of these methods; see, e.g. Robins and
Rotnitzky (1995), Hahn (1998), Hirano et al. (2003) and Abadie and Imbens (2006) as well as
the textbook overview provided in Imbens and Rubin (2015). In this section, we present results
that complement this important classic work as well as the rapidly expanding body of work on
estimation under unconfoundedness using ML methods; see, among others, Athey et al. (2016),
Belloni et al. (2014a, 2017), Farrell (2015) and Imai and Ratkovic (2013).
We specifically consider estimation of ATEs when treatment effects are fully heterogeneous
and the treatment variable is binary, D âˆˆ {0, 1}. We consider vectors (Y, D, X) such that
Y = g0 (D, X) + U,
D = m0 (X) + V ,

EP [U | X, D] = 0,

(5.1)

EP [V | X] = 0.

(5.2)

Because D is not additively separable, this model is more general than the partially linear model
for the case of binary D. A common target parameter of interest in this model is the ATE:9
Î¸0 = EP [g0 (1, X) âˆ’ g0 (0, X)].
Another common target parameter is the ATTE:
Î¸0 = EP [g0 (1, X) âˆ’ g0 (0, X)|D = 1].
The confounding factors X affect the policy variable via the propensity score m0 (X) and the
outcome variable via the function g0 (D, X). Both of these functions are unknown and potentially
complicated, and we can employ ML methods to learn them.

9 Without unconfoundedness/conditional exogeneity, these quantities measure association, and could be referred to as
average predictive effect (APE) and average predictive effect for the exposed (APEX). Inferential results for these objects
would follow immediately from Theorem 5.1.


C

2017 Royal Economic Society.

Double/debiased machine learning

C35

We proceed to set up moment conditions with scores obeying orthogonality conditions. For
estimation of the ATE, we employ
Ïˆ(W ; Î¸, Î·) := (g(1, X) âˆ’ g(0, X)) +

D(Y âˆ’ g(1, X)) (1 âˆ’ D)(Y âˆ’ g(0, X))
âˆ’
âˆ’ Î¸, (5.3)
m(X)
1 âˆ’ m(X)

where the nuisance parameter Î· = (g, m) consists of P -square-integrable functions g and m
mapping the support of (D, X) to R and the support of X to (Îµ, 1 âˆ’ Îµ), respectively, for some
Îµ âˆˆ (0, 1/2). The true value of Î· is Î·0 = (g0 , m0 ). This orthogonal moment condition is based on
the influence function for the mean for missing data from Robins and Rotnitzky (1995).
For estimation of the ATTE, we use the score
Ïˆ(W ; Î¸, Î·) =

D(Y âˆ’ g(X)) m(X)(1 âˆ’ D)(Y âˆ’ g(X)) DÎ¸
âˆ’
âˆ’
,
p
p(1 âˆ’ m(X))
p

(5.4)

where the nuisance parameter Î· = (g, m, p) consists of P -square-integrable functions g and m
mapping the support of X to R and to (Îµ, 1 âˆ’ Îµ), respectively, and a constant p âˆˆ (Îµ, 1 âˆ’ Îµ),
for some Îµ âˆˆ (0, 1/2). The true value of Î· is Î·0 = (g 0 , m0 , p0 ), where g 0 (X) = g0 (0, X) and
p0 = EP [D]. Note that estimating ATTE does not require estimating g0 (1, X). Note also that
because p is a constant, it does not affect the DML estimators Î¸Ìƒ0 based on the score Ïˆ in (5.4),
but having p simplifies the formula for the variance of Î¸Ìƒ0 .
Using their respective scores, it can be easily seen that true parameter values Î¸0 for ATE
and ATTE obey the moment condition EP [Ïˆ(W ; Î¸0 , Î·0 )] = 0, and also that the orthogonality
condition âˆ‚Î· EP [Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] = 0 holds.
âˆž
Let (Î´N )âˆž
n=1 and (N )n=1 be sequences of positive constants approaching 0. Also, let c, Îµ, C
and q be fixed strictly positive constants such that q > 2, and let K â‰¥ 2 be a fixed integer.
Moreover, for any Î· = (1 , . . . , l ), denote Î·P ,q = max1â‰¤j â‰¤l j P ,q . For simplicity, assume
that N/K is an integer.
A SSUMPTION 5.1. (R EGULARITY CONDITIONS FOR ATE AND ATTE ESTIMATION) For all
probability laws P âˆˆ P for the triple (Y, D, X) the following conditions hold: (a) equations
(5.1) and (5.2) hold, with D âˆˆ {0, 1}; (b) Y P ,q â‰¤ C; (c) P r P (Îµ â‰¤ m0 (X) â‰¤ 1 âˆ’ Îµ) = 1;
(d) U P ,2 â‰¥ c; (e) EP [U 2 | X]P ,âˆž â‰¤ C; and (f) given a random subset I of [N] of size
n = N/K, the nuisance parameter estimator Î·Ì‚0 = Î·Ì‚0 ((Wi )iâˆˆI c ) obeys the following conditions.
With P -probability no less than 1 âˆ’ N , Î·Ì‚0 âˆ’ Î·0 P ,q â‰¤ C, Î·Ì‚0 âˆ’ Î·0 P ,2 â‰¤ Î´N , mÌ‚0 âˆ’
1/2P ,âˆž â‰¤ 1/2 âˆ’ Îµ and (i) for the score Ïˆ in (5.3), where Î·Ì‚0 = (gÌ‚0 , mÌ‚0 ) and the target
parameter is ATE, mÌ‚0 âˆ’ m0 P ,2 Ã— gÌ‚0 âˆ’ g0 P ,2 â‰¤ Î´N N âˆ’1/2 , and (ii) for the score Ïˆ in (5.4),
where Î·Ì‚0 = (gÌ‚ 0 , mÌ‚0 , pÌ‚0 ) and the target parameter is ATTE, mÌ‚0 âˆ’ m0 P ,2 Ã— gÌ‚ 0 âˆ’ g 0 P ,2 â‰¤
Î´N N âˆ’1/2 .
R EMARK 5.1. The only non-primitive condition here is the assumption on the rate of estimating
the nuisance parameters. These rates of convergence are available for the most often used ML
methods and are case-specific, so we do not restate conditions that are needed to reach these
rates. The conditions are not the tightest possible, but offer a set of simple conditions under
which Theorem 5.1 follows as a special case of the general theorem provided in Section 3. We
could obtain more refined conditions by doing customized proofs.
The following theorem follows as a corollary to the results in Section 3 by verifying
Assumptions 3.1 and 3.2.

C

2017 Royal Economic Society.

C36

V. Chernozhukov et al.

T HEOREM 5.1. (DML INFERENCE ON ATE AND ATTE) Suppose that the target parameter is
either ATE, Î¸0 = EP [g0 (1, X) âˆ’ g0 (0, X)], and the score Ïˆ in (5.3) is used, or ATTE, Î¸0 =
EP [g0 (1, X) âˆ’ g0 (0, X) | D = 1], and the score Ïˆ in (5.4) is used. In addition, suppose that
Assumption 5.1 holds. Then the DML1 and DML2 estimators Î¸Ìƒ0 , constructed in Definitions 3.1
and 3.2, are first-order equivalent and obey
âˆš
(5.5)
Ïƒ âˆ’1 N(Î¸Ìƒ0 âˆ’ Î¸0 )  N(0, 1),
uniformly over P âˆˆ P, where Ïƒ 2 = EP [Ïˆ 2 (W ; Î¸0 , Î·0 )]. Moreover, the result continues to hold if
Ïƒ 2 is replaced by ÏƒÌ‚ 2 defined in Theorem 3.2. Consequently, confidence regions based upon the
DML estimators Î¸Ìƒ0 have uniform asymptotic validity:
âˆš
lim sup |P r P (Î¸0 âˆˆ [Î¸Ìƒ0 Â± âˆ’1 (1 âˆ’ Î±/2)ÏƒÌ‚ / N]) âˆ’ (1 âˆ’ Î±)| = 0.
Nâ†’âˆž P âˆˆP

The scores Ïˆ in (5.3) and (5.4) are efficient, so both estimators are asymptotically efficient,
reaching the semi-parametric efficiency bound of Hahn (1998).
R EMARK 5.2. (T IGHTNESS OF CONDITIONS) The conditions in Assumption 5.1 are sharp
though simplified for ease of presentation. The sharpness can be understood by examining the
case where the regression function g0 and the propensity function m0 are sparse with sparsity
N and s m
N . They are estimated by 1 -penalized estimators gÌ‚0 and
indices s g
âˆš g mÌ‚0 that
g
m
and
s
and
converge
to
g
and
m
at
the
rates
s /N and
have
sparsity
indices
of
orders
s
0
0
âˆš m
s /N (ignoring logs). Then the rate conditions in Assumption 5.1 require


s g /N s m /N
N âˆ’1/2 â‡” s g s m
N
N (ignoring logs)
(ignoring logs), which is much weaker than the condition (s g )2 + (s m )2
is
very
sparse, then the
required without sample splitting. For example, if the propensity
score
m
0
âˆš
regression function is allowed to be quite dense with s g > N, and vice versa. If the propensity
score is known (s m = 0), then only consistency for gÌ‚0 is needed. Such comparisons also extend to
approximately sparse models. We note that similar refined rates appeared in Farrell (2015), who
considers estimation of treatment effects in a setting where an approximately sparse model holds
for both the regression
âˆš and propensity score functions. In interesting related work, Athey et al.
(2016) show that N-consistent estimation of an ATE is possible under very weak conditions
on the propensity score â€“ allowing for the possibility that the propensity score may not
âˆš be
N.
consistently estimated â€“ under strong sparsity of the regression function such that sg
Thus, the approach taken in this context and by Athey et al. (2016) are complementary, and
either might be preferred, depending on whether or not the regression function can be estimated
extremely well based on a sparse method.
5.2. Inference on local average treatment effects
In this section, we consider estimation of LATEs with a binary treatment variable, D âˆˆ {0, 1},
and a binary instrument, Z âˆˆ {0, 1}.10 As before, Y denotes the outcome variable, and X is the
vector of covariates.
10 Similar results can be provided for the local average treatment effect on the treated (LATT) by adapting the following
arguments to use the orthogonal scores for the LATT; see, e.g. Belloni et al. (2017).


C

2017 Royal Economic Society.

C37

Double/debiased machine learning

Consider the functions Î¼0 , m0 and p0 , where Î¼0 maps the support of (Z, X) to R, and m0
and p0 , respectively, map the support of (Z, X) and X to (Îµ, 1 âˆ’ Îµ) for some Îµ âˆˆ (0, 1/2), such
that
Y = Î¼0 (Z, X) + U,

EP [U | Z, X] = 0,

(5.6)

D = m0 (Z, X) + V ,

EP [V | Z, X] = 0,

(5.7)

EP [Î¶ | X] = 0.

(5.8)

Z = p0 (X) + Î¶,
We are interested in estimating
Î¸0 =

EP [Î¼(1, X)] âˆ’ EP [Î¼(0, X)]
.
EP [m(1, X)] âˆ’ EP [m(0, X)]

Under the assumptions of Imbens and Angrist (1994) and FroÌˆlich (2007), Î¸0 is the LATE â€“ the
average treatment effect for compliers that are observations that would have D = 1 if Z were 1
and D = 0 if Z were 0. To estimate Î¸0 , we use the score
Ïˆ(W ; Î¸, Î·) := Î¼(1, X) âˆ’ Î¼(0, X) +


Z(Y âˆ’ Î¼(1, X)) (1 âˆ’ Z)(Y âˆ’ Î¼(1, X))
âˆ’
p(X)
1 âˆ’ p(X))

Z(D âˆ’ m(1, X)) (1 âˆ’ Z)(D âˆ’ m(0, X))
âˆ’
âˆ’ m(1, X) âˆ’ m(0, X) +
p(X)
1 âˆ’ p(X)


Ã— Î¸,

where W = (Y, D, X, Z) and the nuisance parameter Î· = (Î¼, m, p) consists of P -squareintegrable functions Î¼, m and p, with Î¼ mapping the support of (Z, X) to R and m and p,
respectively, mapping the support of (Z, X) and X to (Îµ, 1 âˆ’ Îµ) for some Îµ âˆˆ (0, 1/2). It is
easy to verify that this score satisfies the moment condition EP [Ïˆ(W ; Î¸0 , Î·0 )] = 0 and also the
orthogonality condition âˆ‚Î· EP [Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] = 0 for Î·0 = (Î¼0 , m0 , p0 ).
âˆž
Let (Î´N )âˆž
n=1 and (N )n=1 be sequences of positive constants approaching 0. Also, let c, C
and q be fixed strictly positive constants such that q > 4, and let K â‰¥ 2 be a fixed integer.
Moreover, for any Î· = (1 , 2 , 3 ), where 1 is a function mapping the support of (Z, X) to R
and 2 and 3 are functions respectively mapping the support of (Z, X) and X to (Îµ, 1 âˆ’ Îµ) for
some Îµ âˆˆ (0, 1/2), we denote Î·P ,q = 1 P ,q âˆ¨ 2 P ,2 âˆ¨ 3 P ,q . For simplicity, assume that
N/K is an integer.
A SSUMPTION 5.2. (R EGULARITY CONDITIONS FOR LATE ESTIMATION) For all probability
laws P âˆˆ P for the quadruple W = (Y, D, X, Z) the following conditions hold: (a) equations
(5.6)â€“(5.8) hold, with D âˆˆ {0, 1} and Z âˆˆ {0, 1}; (b) Y P ,q â‰¤ C; (c) P r P (Îµ â‰¤ p0 (X) â‰¤ 1 âˆ’
Îµ) = 1; (d) EP [m0 (1, X) âˆ’ m0 (0, X)] â‰¥ c, (e) U âˆ’ Î¸0 V P ,2 â‰¥ c; (f) EP [U 2 | X]P ,âˆž â‰¤ C;
and (g) given a random subset I of [N] of size n = N/K, the nuisance parameter estimator
Î·Ì‚0 = Î·Ì‚0 ((Wi )iâˆˆI c ) obeys the following conditions. With P -probability no less than 1 âˆ’ N ,
Î·Ì‚0 âˆ’ Î·0 P ,q â‰¤ C, Î·Ì‚0 âˆ’ Î·0 P ,2 â‰¤ Î´N , pÌ‚0 âˆ’ 1/2P ,âˆž â‰¤ 1/2 âˆ’ Îµ and pÌ‚0 âˆ’ p0 P ,2 Ã— (Î¼Ì‚0 âˆ’
Î¼0 P ,2 + mÌ‚0 âˆ’ m0 P ,2 ) â‰¤ Î´N N âˆ’1/2 .
The following theorem follows as a corollary to the results in Section 3 by verifying
Assumptions 3.1 and 3.2.

C

2017 Royal Economic Society.

C38

V. Chernozhukov et al.

T HEOREM 5.2. (DML INFERENCE ON LATE) Suppose that Assumption 5.2 holds. Then the
DML1 and DML2 estimators Î¸Ìƒ0 constructed in Definitions 3.1 and 3.2 and based on the score Ïˆ
above are first-order equivalent and obey
âˆš
Ïƒ âˆ’1 N(Î¸Ìƒ0 âˆ’ Î¸0 )  N(0, 1),
(5.9)
uniformly over P âˆˆ P, where Ïƒ 2 = (EP [m(1, X) âˆ’ m(0, X)])âˆ’2 EP [Ïˆ 2 (W ; Î¸0 , Î·0 )]. Moreover,
the result continues to hold if Ïƒ 2 is replaced by ÏƒÌ‚ 2 defined in Theorem 3.2. Consequently,
confidence regions based upon the DML estimators Î¸Ìƒ0 have uniform asymptotic validity:
âˆš
lim sup |P r P (Î¸0 âˆˆ [Î¸Ìƒ0 Â± âˆ’1 (1 âˆ’ Î±/2)ÏƒÌ‚ / N]) âˆ’ (1 âˆ’ Î±)| = 0.
Nâ†’âˆž P âˆˆP

6. EMPIRICAL EXAMPLES
To illustrate the methods developed in the preceding sections, we consider three empirical
examples. The first example reexamines the Pennsylvania Reemployment Bonus experiment,
which used a randomized control trial to investigate the incentive effect of unemployment
insurance. In the second, we use the DML method to estimate the effect of 401(k) eligibility,
the treatment variable, and 401(k) participation, a self-selected decision to receive the treatment
that we instrument for with assignment to the treatment state, on accumulated assets. In this
example, the treatment variable is not randomly assigned and we aim to eliminate the potential
biases due to the lack of random assignment by flexibly controlling for a rich set of variables. In
the third, we revisit the IV estimation by Acemoglu et al. (2001) of the effects of institutions on
economic growth by estimating a partially linear IV model.
6.1. Effect of unemployment insurance bonus on unemployment duration
In this example, we reanalyse the Pennsylvania Reemployment Bonus experiment, which was
conducted by the US Department of Labor in the 1980s to test the incentive effects of alternative
compensation schemes for unemployment insurance (UI). This experiment has been previously
studied by Bilias (2000) and Bilias and Koenker (2002). In these experiments, UI claimants were
randomly assigned either to a control group or to one of five treatment groups.11 In the control
group, the standard rules of the UI system applied. Individuals in the treatment groups were
offered a cash bonus if they found a job within some pre-specified period of time (qualification
period), provided that the job was retained for a specified duration. The treatments differed in the
level of the bonus, the length of the qualification period, and whether the bonus was declining
over time in the qualification period; see Bilias and Koenker (2002) for further details.
In our empirical example, we focus only on the most generous compensation scheme,
treatment 4, and we drop all individuals who received other treatments. In this treatment,
the bonus amount is high and the qualification period is long compared to other treatments,
and claimants are eligible to enroll in a workshop. Our treatment variable, D, is an indicator
variable for being assigned treatment 4, and the outcome variable, Y , is the log of duration of
unemployment for the UI claimants. The vector of covariates, X, consists of age group dummies,

11

There are six treatment groups in the experiments. Following Bilias (2000). we merge groups 4 and 6.

C

2017 Royal Economic Society.

Double/debiased machine learning

C39

gender, race, the number of dependents, quarter of the experiment, location within the state,
existence of recall expectations and type of occupation.
We report results based on five simple methods for estimating the nuisance functions used
in forming the orthogonal estimating equations. We consider three tree-based methods, labelled
â€˜Random forestâ€™, â€˜Reg. treeâ€™ and â€˜Boostingâ€™, one 1 -penalization based method, labelled â€˜Lassoâ€™
and a neural network method, labelled â€˜Neural networkâ€™. For Reg. tree, we fit a single CART tree
to estimate each nuisance function with penalty parameter chosen by tenfold cross-validation.
The results in the Random forest column are obtained by estimating each nuisance function with
a random forest that averages over 1000 trees. The results in Boosting are obtained using boosted
regression trees with regularization parameters chosen by tenfold cross-validation. To estimate
the nuisance functions using the neural networks, we use two neurons and a decay parameter
of 0.02, and we set activation function as logistic for classification problems and as linear for
regression problems.12 Lasso estimates an 1 -penalized linear regression model using the datadriven penalty parameter selection rule developed in Belloni et al. (2012). For Lasso, we use a
set of 96 potential control variables formed from the raw set of covariates and all second-order
terms (i.e. all squares and first-order interactions). For the remaining methods, we use the raw
set of covariates as features.
We also consider two hybrid methods labelled â€˜Ensembleâ€™ and â€˜Bestâ€™. Ensemble optimally
combines four of the ML methods listed above by estimating the nuisance functions as weighted
averages of estimates from Lasso, Boosting, Random forest and Neural network. The weights
are restricted to sum to one and are chosen so that the weighted average of these methods
gives the lowest average mean squared out-of-sample prediction error estimated using fivefold
cross-validation. The final column in Table 1 (Best) reports results that combine the methods
in a different way. After obtaining estimates from the five simple methods and Ensemble, we
select the best methods for estimating each nuisance function based on the average out-of-sample
prediction performance for the target variable associated with each nuisance function obtained
from each of the previously described approaches. As a result, the reported estimate in the last
column uses different ML methods to estimate different nuisance functions. Note that if a single
method outperformed all the others in terms of prediction accuracy for all nuisance functions,
the estimate in the Best column would be identical to the estimate reported under that method.
Table 1 presents DML2 estimates of the ATE on unemployment duration using the median
method described in Section 3.4. We report results for the heterogeneous effect model in Panel
A and for the partially linear model in Panel B. Because the treatment is randomly assigned,
we use the fraction of treated as the estimator of the propensity score in forming the orthogonal
estimating equations.13 For both the partially linear model and the interactive model, we report
estimates obtained using twofold cross-fitting and fivefold cross-fitting. All results are based
on taking 100 different sample splits. We summarize results across the sample splits using the
median method. For comparison, we report two different standard errors: in brackets, we report
the median standard errors from across the 100 splits; in parentheses, we report standard errors
adjusted for variability across the sample splits using the median method in parentheses.

12 We also experimented with Deep Learning methods from which we obtained similar results for some tuning
parameters. However, we ran into stability and computational issues and we have chosen not to report these results
in the empirical section.
13 We also estimated the effects using non-parametric estimates of the conditional propensity score obtained from the
ML procedures given in the column labels. As expected due to randomization, the results are similar to those provided
in Table 1 and are not reported for brevity.


C

2017 Royal Economic Society.

C40

V. Chernozhukov et al.
Table 1. Estimated effect of cash bonus on unemployment duration.
Lasso

Reg. tree

Random forest

Boosting

Neural network

Ensemble

Best

âˆ’0.079
[0.036]
(0.036)

âˆ’0.073
[0.036]
(0.036)

âˆ’0.079
[0.036]
(0.036)

âˆ’0.078
[0.036]
(0.036)

âˆ’0.074
[0.036]
(0.036)

âˆ’0.077
[0.035]
(0.036)

âˆ’0.073
[0.036]
(0.036)

âˆ’0.078
[0.036]
(0.036)

âˆ’0.077
[0.036]
(0.036)

Panel B: partially linear regression model
ATE
âˆ’0.080
âˆ’0.084
âˆ’0.077
(twofold)
[0.036]
[0.036]
[0.035]
(0.036)
(0.036)
(0.037)

âˆ’0.076
[0.035]
(0.036)

âˆ’0.074
[0.035]
(0.036)

âˆ’0.075
[0.035]
(0.036)

âˆ’0.075
[0.035]
(0.036)

âˆ’0.074
[0.035]
(0.035)

âˆ’0.073
[0.035]
(0.036)

âˆ’0.075
[0.035]
(0.035)

âˆ’0.074
[0.035]
(0.035)

Panel A: interactive regression model
ATE
âˆ’0.081
âˆ’0.084
âˆ’0.074
(twofold)
[0.036]
[0.036]
[0.036]
(0.036)
(0.036)
(0.036)
ATE
(fivefold)

ATE
(fivefold)

âˆ’0.081
[0.036]
(0.036)

âˆ’0.080
[0.036]
(0.036)

âˆ’0.085
[0.036]
(0.037)

âˆ’0.084
[0.036]
(0.037)

âˆ’0.077
[0.035]
(0.036)

Note: Estimated ATE and standard errors from a linear model (Panel B) and heterogeneous effect model (Panel A) based
on orthogonal estimating equations. Column labels denote the method used to estimate nuisance functions. Results are
based on 100 splits with point estimates calculated the median method. The median standard errors across the splits
are reported in brackets and standard errors calculated using the median method to adjust for variation across splits are
provided in parentheses. Further details about the methods are provided in the main text.

The estimation results are consistent with the findings of previous studies that have analysed
the Pennsylvania Bonus Experiment. The ATE on unemployment duration is negative and
significant across all estimation methods at the 5% level regardless of the standard error estimator
used. Interestingly, we see that there is no practical difference across the two different standard
errors in this example.

6.2. Effect of 401(k) eligibility and participation on net financial assets
The key problem in determining the effect of 401(k) eligibility is that working for a firm that
offers access to a 401(k) plan is not randomly assigned. To overcome the lack of random
assignment, we follow the strategy developed in Poterba et al. (1994a,b). In these papers, the
authors use data from the 1991 Survey of Income and Program Participation and they argue that
eligibility for enrolling in a 401(k) plan in these data can be taken as exogenous after conditioning
on a few observables â€“ of which the most important for their argument is income. The basic idea
of their argument is that, at least around the time 401(k) initially became available, people were
unlikely to be basing their employment decisions on whether an employer offered a 401(k) but
would instead focus on income and other aspects of the job. Following this argument, whether
one is eligible for a 401(k) may then be taken as exogenous after appropriately conditioning on
income and other control variables related to job choice.
A key component of the argument underlying the exogeneity of 401(k) eligibility is that
eligibility can only be taken as exogenous after conditioning on income and other variables
related to job choice that might correlate with whether a firm offers a 401(k). Poterba et al.
(1994a,b) and many subsequent papers adopt this argument but control only linearly for a small

C

2017 Royal Economic Society.

C41

Double/debiased machine learning
Table 2. Estimated effect of 401(k) eligibility on net financial assets.
Lasso

Reg. tree

Random forest

Boosting

Neural network

Ensemble

Best

7770
[1276]
(1363)

7806
[1159]
(1202)

7764
[1328]
(1468)

7702
[1149]
(1170)

7546
[1360]
(1533)

8105
[1242]
(1299)

7713
[1155]
(1177)

7788
[1238]
(1293)

7839
[1134]
(1148)

7753
[1237]
(1294)

Panel B: partially linear regression model
ATE
7717
8709
9116
(twofold) [1346]
[1363]
[1302]
(1749)
(1427)
(1377)

8759
[1339]
(1382)

8950
[1335]
(1408)

9010
[1309]
(1344)

9125
[1304]
(1357)

9110
[1314]
(1328)

9038
[1322]
(1355)

9166
[1299]
(1310)

9215
[1294]
(1312)

Panel A: interactive regression model
ATE
6830
7713
(twofold) [1282]
[1208]
(1530)
(1271)
ATE
(fivefold)

ATE
(fivefold)

7170
[1201]
(1398)

8187
[1298]
(1558)

7993
[1198]
(1236)

8871
[1358]
(1418)

9247
[1295]
(1328)

Note: Estimated ATE and standard errors from a linear model (Panel B) and heterogeneous effect model (Panel A) based
on orthogonal estimating equations. Column labels denote the method used to estimate nuisance functions. Results are
based on 100 splits with point estimates calculated the median method. The median standard errors across the splits
are reported in brackets and standard errors calculated using the median method to adjust for variation across splits are
provided in parentheses. Further details about the methods are provided in the main text.

number of terms. One might wonder whether such specifications are able to adequately control
for income and other related confounds. At the same time, the power to learn about treatment
effects decreases as one allows more flexible models. The principled use of flexible ML tools
offers one resolution to this tension. The results presented below thus complement previous
results that rely on the assumption that confounding effects can adequately be controlled for
by a small number of variables chosen ex ante by the researcher.
In the example in this paper, we use the same data as in Chernozhukov and Hansen (2004). We
use net financial assets â€“ defined as the sum of IRA balances, 401(k) balances, checking accounts,
US saving bonds, other interest-earning accounts in banks and other financial institutions, other
interest-earning assets (such as bonds held personally), stocks, and mutual funds less nonmortgage debt â€“ as the outcome variable, Y , in our analysis. Our treatment variable, D, is an
indicator for being eligible to enroll in a 401(k) plan. The vector of raw covariates, X, consists
of age, income, family size, years of education, a married indicator, a two-earner status indicator,
a defined benefit pension status indicator, an IRA participation indicator, and a home-ownership
indicator.
In Table 2, we report DML2 estimates of ATE of 401(k) eligibility on net financial assets
both in the partially linear model as in (1.1) and allowing for heterogeneous treatment effects
using the interactive model outlined in Section 5.1. To reduce the disproportionate impact of
extreme propensity score weights in the interactive model, we trim the propensity scores at 0.01
and 0.99. We present two sets of results based on sample splitting as discussed in Section 3
using twofold cross-fitting and fivefold cross-fitting. As in the previous section, we consider 100
different sample partitions and summarize the results across different sample splits using the
median method. For comparison, we report two different standard errors: in brackets, we report
the median standard errors from across the 100 splits; in parentheses, we report standard errors

C

2017 Royal Economic Society.

C42

V. Chernozhukov et al.
Table 3. Estimated effect of 401(k) participation on net financial assets.
Lasso

Reg. tree

Random forest

Boosting

Neural network

Ensemble

Best

LATE
(twofold)

8978
[2192]
(3014)

11073
[1749]
(1849)

11384
[1832]
(1993)

11329
[1666]
(1718)

11094
[1903]
(2098)

11119
[1653]
(1689)

10952
[1657]
(1699)

LATE
(fivefold)

8944
[2259]
(3307)

11459
[1717]
(1786)

11764
[1788]
(1893)

11133
[1661]
(1710)

11186
[1795]
(1890)

11173
[1641]
(1678)

11113
[1645]
(1675)

Note: Estimated LATE based on orthogonal estimating equations. Column labels denote the method used to estimate
nuisance functions. Results are based on 100 splits with point estimates calculated the median method. The median
standard errors across the splits are reported in brackets and standard errors calculated using the median method to adjust
for variation across splits are provided in parentheses. Further details about the methods are provided in the main text.

adjusted for variability across the sample splits using the median method. We consider the same
methods with the same tuning choices for estimating the nuisance functions as in the previous
example, with one exception, and so we do not repeat details for brevity. The one exception is
that we implement neural networks with eight neurons and a decay parameter of 0.01 in this
example.
Turning to the results, it is first worth noting that the estimated ATE of 401(k) eligibility
on net financial assets is $19,559 with an estimated standard error of 1413 when no control
variables are used. Of course, this number is not a valid estimate of the causal effect of 401(k)
eligibility on financial assets if there are neglected confounding variables as suggested by Poterba
et al. (1994a,b). When we turn to the estimates that flexibly account for confounding reported
in Table 2, we see that they are substantially attenuated relative to this baseline that does not
account for confounding, suggesting much smaller causal effects of 401(k) eligibility on financial
asset holdings. It is interesting and reassuring that the results obtained from the different flexible
methods are broadly consistent with each other. This similarity is consistent with the theory
that suggests that results obtained through the use of orthogonal estimating equations and any
sensible method of estimating the necessary nuisance functions should be similar. Finally, it is
interesting that these results are also broadly consistent with those reported in the original work
of Poterba et al. (1994a,b), who used a simple intuitively motivated functional form, suggesting
that this intuitive choice was sufficiently flexible to capture much of the confounding variation in
this example.
As a further illustration, we also report the LATE in this example where we take the
endogenous treatment variable to be participating in a 401(k) plan. Even after controlling for
features related to job choice, it seems likely that the actual choice of whether to participate in
an offered plan would be endogenous. Of course, we can use eligibility for a 401(k) plan as an
instrument for participation in a 401(k) plan under the conditions that were used to justify the
exogeneity of eligibility for a 401(k) plan provided above in the discussion of estimation of the
ATE of 401(k) eligibility.
We report DML2 results of estimating the LATE of 401(k) participation using 401(k)
eligibility as an instrument in Table 3. We employ the procedure outlined in Section 5.2 using the
same ML estimators to estimate the quantities used to form the orthogonal estimating equation
as we employed to estimate the ATE of 401(k) eligibility outlined previously, so we omit the
details for brevity. Looking at the results, we see that the estimated causal effect of 401(k)
participation on net financial assets is uniformly positive and statistically significant across all of

C

2017 Royal Economic Society.

Double/debiased machine learning

C43

the considered methods. As when looking at the ATE of 401(k) eligibility, it is reassuring that the
results obtained from the different flexible methods are broadly consistent with each other. It is
also interesting that the results based on flexible ML methods are broadly consistent with, though
somewhat attenuated relative to, those obtained by applying the same specification for controls
as used in Poterba et al. (1994a,b) and using a linear IV model, which returns an estimated effect
of participation of $13,102 with estimated standard error of (1922). The mild attenuation may
suggest that the simple intuitive control specification used in the original baseline specification
is too simplistic.
Looking at Tables 2 and 3, there are other interesting observations that can provide useful
insights into understanding the finite sample properties of the DML estimation method. First,
the standard errors of the estimates obtained using fivefold cross-fitting are lower than those
obtained from twofold cross-fitting for all methods across all cases. This fact suggests that having
more observations in the auxiliary sample may be desirable. Specifically, the fivefold cross-fitting
estimates use more observations to learn the nuisance functions than twofold cross-fitting and
thus are likely learn them more precisely. This increase in precision in learning the nuisance
functions may then translate into more precisely estimated parameters of interest. While intuitive,
we note that this statement does not seem to be generalizable, in that there does not appear to be a
general relationship between the number of folds in cross-fitting and the precision of the estimate
of the parameter of interest (see the next example). Secondly, we also see that the standard errors
of the Lasso estimates after adjusting for variation due to sample splitting are noticeably larger
than the standard errors coming from the other ML methods. We believe that this is due to the
fact that the out-of-sample prediction errors from a linear model tend to be larger when there is a
need to extrapolate. In our framework, if the main sample includes observations that are outside
of the range of the observations in the auxiliary sample, the model has to extrapolate to those
observations. The fact that the standard errors are lower in fivefold cross-fitting than in twofold
cross-fitting for the Lasso estimations also supports this hypothesis, because the higher number
of observations in the auxiliary sample reduces the degree of extrapolation. We also see that there
is a noticeable increase in the standard errors that account for variability due to sample splitting
relative to the simple unadjusted standard errors in this case, though these differences do not
qualitatively change the results.

6.3. Effect of institutions on economic growth
To demonstrate DML estimation of partially linear structural equation models with instrumental
variables, we consider estimation of the effect of institutions on aggregate output following
the work of Acemoglu et al. (2001, hereafter AJR). Estimating the effect of institutions on
output is complicated by the clear potential for simultaneity between institutions and output.
Specifically, better institutions may lead to higher incomes, but higher incomes may also lead
to the development of better institutions. To help overcome this simultaneity, AJR use mortality
rates for early European settlers as an instrument for institution quality. The validity of this
instrument hinges on the arguments that settlers set up better institutions in places where they
are more likely to establish long-term settlements, that where they are likely to settle for the
long term is related to settler mortality at the time of initial colonization, and that institutions
are highly persistent. The exclusion restriction for the instrumental variable is then motivated by
the argument that GDP, while persistent, is unlikely to be strongly influenced by mortality in the
previous century, or earlier, except through institutions.

C

2017 Royal Economic Society.

C44

V. Chernozhukov et al.
Table 4. Estimated effect of institutions on output.
Lasso

Reg. tree

Random forest

Boosting

Neural network

Ensemble

Best

Twofold

0.85
[0.28]
(0.22)

0.81
[0.42]
(0.29)

0.84
[0.38]
(0.30)

0.77
[0.33]
(0.27)

0.94
[0.32]
(0.28)

0.80
[0.35]
(0.30)

0.83
[0.34]
(0.29)

Fivefold

0.77
[0.24]
(0.17)

0.95
[0.46]
(0.45)

0.90
[0.41]
(0.40)

0.73
[0.33]
(0.27)

1.00
[0.33]
(0.30)

0.83
[0.37]
(0.34)

0.88
[0.41]
(0.39)

Note: Estimated coefficient from a linear IV model based on orthogonal estimating equations. Column labels denote the
method used to estimate nuisance functions. Results are based on 100 splits with point estimates calculated the median
method. The median standard errors across the splits are reported in brackets and standard errors calculated using the
median method to adjust for variation across splits are provided in parentheses. Further details about the methods are
provided in the main text.

In their paper, AJR note that their IV strategy will be invalidated if other factors are also
highly persistent and related to the development of institutions within a country and to the
countryâ€™s GDP. A leading candidate for such a factor, as they discuss, is geography. AJR address
this by assuming that the confounding effect of geography is adequately captured by a linear term
in distance from the equator and a set of continent dummy variables. Using DML allows us to
relax this assumption and to replace it by a weaker assumption that geography can be sufficiently
controlled by an unknown function of distance from the equator and continent dummies, which
can be learned by ML methods.
We use the same set of 64 country-level observations as AJR. The data set contains
measurements of GDP, settler morality, an index that measures protection against expropriation
risk and geographic information. The outcome variable, Y , is the logarithm of GDP per capita
and the endogenous explanatory variable, D, is a measure of the strength of individual property
rights that is used as a proxy for the strength of institutions. To deal with endogeneity, we use
an instrumental variable Z, which is mortality rates for early European settlers. Our raw set of
control variables, X, include distance from the equator and dummy variables for Africa, Asia,
North America and South America.
We report results from applying DML2 following the procedure outlined in Section 4.2
in Table 4. The considered ML methods and tuning parameters are the same as the previous
examples except for the Ensemble method, from which we exclude Neural network, as the
small sample size causes stability problems in training the neural network. We use the raw
set of covariates and all second-order terms when doing Lasso estimation, and we simply use
the raw set of covariates in the remaining methods. As in the previous examples, we consider
100 different sample splits and report the median estimates of the coefficient and two different
standard error estimates. In brackets, we report the median standard errors from across the 100
splits, and we report standard errors adjusted for variability across the sample splits using the
median method in parentheses. Finally, we report results from both twofold cross-fitting and
fivefold cross-fitting as in the other examples.
In this example, we see uniformly large and positive point estimates across all procedures
considered, and estimated effects are statistically significant at the 5% level. As in the second
example, we see that adjusting for variability across sample splits leads to noticeable increases in
estimated standard errors but does not result in qualitatively different conclusions. Interestingly,
we see that the estimated standard errors based on fivefold cross-fitting are larger than those

C

2017 Royal Economic Society.

Double/debiased machine learning

C45

based on twofold cross-fitting in all procedures except lasso, which differs from the finding in
the 401(k) example. Further understanding these differences and the impact of the number of
folds on inference for objects of interest seems to be an interesting question for future research.
Finally, although the estimated coefficients are somewhat smaller than the baseline estimates
reported in AJR â€“ an estimated coefficient of 1.10 with estimated standard error of 0.46 (see
AJR, Table 4, Panel A, column 7) â€“ the results are qualitatively similar, indicating a strong and
positive effect of institutions on output.
6.4. Comments on empirical results
Before closing this section, we want to emphasize some important conclusions that can be drawn
from these empirical examples. First, the choice of the ML method used in estimating nuisance
functions does not substantively change the conclusion in any of the examples, and we have
obtained broadly consistent results regardless of which method we employ. The robustness of
the results to the different methods is implied by the theory assuming that all of the employed
methods are able to deliver sufficiently high-quality approximations to the underlying nuisance
functions. Secondly, the incorporation of uncertainty due to sample splitting using the median
method increases the standard errors relative to a baseline that does not account for this
uncertainty, though these differences do not alter the main results in any of the examples. This
lack of variation suggests that the parameter estimates are robust to the particular sample split
used in the estimation in these examples.

ACKNOWLEDGEMENTS
We would like to acknowledge research support from the National Science Foundation. We
also thank participants of the MIT Stochastics and Statistics seminar, the Kansas Econometrics
conference, the Royal Economic Society Annual Conference, the Hannan Lecture at the
Australasian Econometric Society meeting, the Econometric Theory lecture at the EC2 meetings
2016 in Toulouse, the CORE 50th Anniversary Conference, the Beckerâ€“Friedman Institute
Conference on Machine Learning and Economics, the INET conferences on Big Data at
the University of Southern California in Los Angeles, the World Congress of Probability
and Statistics 2016, the Joint Statistical Meetings 2016, the New England Day of Statistics
Conference, CEMMAPâ€™s Masterclass on Causal Machine Learning, and St Gallenâ€™s summer
school on Big Data, for many useful comments and questions. We would like to thank Susan
Athey, Peter Aronow, Jin Hahn, Guido Imbens, Mark van der Laan and Matt Taddy for
constructive comments. We thank Peter Aronow for pointing us to the literature on targeted
learning on which we build, along with prior works of Neyman, Bickel, and the many other
contributions to semi-parametric learning theory.

REFERENCES
Abadie, A. and G. W. Imbens (2006). Large sample properties of matching estimators for average treatment
effects. Econometrica 74, 235â€“67.
Acemoglu, D., S. Johnson and J. A. Robinson (2001). The colonial origins of comparative development: an
empirical investigation. American Economic Review 91 (5), 1369â€“401 (AJR).

C

2017 Royal Economic Society.

C46

V. Chernozhukov et al.

Ai, C. and X. Chen (2012). The semi-parametric efficiency bound for models of sequential moment
restrictions containing unknown functions. Journal of Econometrics 170, 442â€“57.
Andrews, D. W. K. (1994a). Asymptotics for semi-parametric econometric models via stochastic
equicontinuity. Econometrica 62, 43â€“72.
Andrews, D. W. K. (1994b). Empirical process methods in econometrics. In R. F. Engle and D. L. McFadden
(Eds.), Handbook of Econometrics, Volume IV, Chapter 37, 2247â€“94. Amsterdam: Elsevier.
Angrist, J. D. and A. B. Krueger (1995). Split-sample instrumental variables estimates of the return to
schooling. Journal of Business and Economic Statistics 13, 225â€“35.
Athey, S., G. Imbens, and S. Wager (2016). Approximate residual balancing: de-biased inference of average
treatment effects in high-dimensions. Preprint (arXiv:1604.07125v3).
Ayyagari, R. (2010). Applications of influence functions to semi-parametric regression models. PhD Thesis,
Harvard School of Public Health, Harvard University.
Belloni, A. and V. Chernozhukov (2011). 1 -penalized quantile regression for high dimensional sparse
models. Annals of Statistics 39, 82â€“130.
Belloni, A. and V. Chernozhukov (2013). Least squares after model selection in high-dimensional sparse
models. Bernoulli 19, 521â€“47.
Belloni, A., V. Chernozhukov and C. Hansen (2010). Lasso methods for Gaussian instrumental variables
models. Preprint (arXiv:1012.1297).
Belloni, A., V. Chernozhukov and L. Wang (2011). Square-root-lasso: pivotal recovery of sparse signals via
conic programming. Biometrika 98, 791â€“806.
Belloni, A., D. Chen, V. Chernozhukov and C. Hansen (2012). Sparse models and methods for optimal
instruments with an application to eminent domain. Econometrica 80, 2369â€“429.
Belloni, A., V. Chernozhukov and C. Hansen (2013). Inference for high-dimensional sparse econometric
models. In D. Acemoglu, M. Arellano and E. Dekel (Eds.), Advances in Economics and Econometrics:
Tenth World Congress of Econometric Society, Volume III, 245â€“95. Cambridge: Cambridge University
Press.
Belloni, A., V. Chernozhukov and C. Hansen (2014a). Inference on treatment effects after selection amongst
high-dimensional controls. Review of Economic Studies 81, 608â€“50.
Belloni, A., V. Chernozhukov and L. Wang (2014b). Pivotal estimation via square-root Lasso in
nonparametric regression. Annals of Statistics 42, 757â€“88.
Belloni, A., V. Chernozhukov and K. Kato (2015). Uniform post selection inference for LAD regression
models and other z-estimators. Biometrika 102, 77â€“94.
Belloni, A., V. Chernozhukov and Y. Wei (2016). Post-selection inference for generalized linear models
with many controls. Journal of Business and Economic Statistics 34, 606â€“19.
Belloni, A., V. Chernozhukov, I. FernaÌndez-Val and C. Hansen (2017). Program evaluation with highdimensional data. Econometrica 85, 233â€“98.
Bera, A., G. Montes-Rojas and W. Sosa-Escudero (2010). General specification testing with locally
misspecified models. Econometric Theory 26, 1838â€“45.
Bickel, P. J. (1982). On adaptive estimation. Annals of Statistics 10, 647â€“71.
Bickel, P. and Y. Ritov (1988). Estimating integrated squared density derivatives. Sankhya A-50,
381â€“93.
Bickel, P. J., C. A. J. Klaassen, Y. Ritov and J. A. Wellner (1998). Efficient and Adaptive Estimation for
Semi-Parametric Models. Berlin: Springer.
Bickel, P. J., Y. Ritov and A. Tsybakov (2009). Simultaneous analysis of Lasso and Dantzig selector. Annals
of Statistics 37, 1705â€“32.
Bilias, Y. (2000). Sequential testing of duration data: the case of the Pennsylvania â€˜reemployment bonusâ€™
experiment. Journal of Applied Econometrics 15, 575â€“94.

C

2017 Royal Economic Society.

Double/debiased machine learning

C47

Bilias, Y. and R. Koenker (2002). Quantile regression for duration data: a reappraisal of the Pennsylvania
reemployment bonus experiments. In B. Fitzenberger, R. Koenker and J. A. Machado (Eds.), Studies in
Empirical Economics: Economic Applications of Quantile Regression, 199â€“220. Heidelberg: PhysicaVerlag.
BuÌˆhlmann, P. and S. van de Geer (2011). Statistics for High-Dimensional Data, Springer Series in Statistics.
Berlin: Springer.
Chamberlain, G. (1987). Asymptotic efficiency in estimation with conditional moment restrictions. Journal
of Econometrics 34, 305â€“34.
Chamberlain, G. (1992). Efficiency bounds for semi-parametric regression. Econometrica 60, 567â€“96.
Chen, X. and H. White (1999). Improved rates and asymptotic normality for nonparametric neural network
estimators. IEEE Transactions on Information Theory 45, 682â€“91.
Chen, X., O. Linton and I. van Keilegom (2003). Estimation of semi-parametric models when the criterion
function is not smooth. Econometrica 71, 1591â€“608.
Chernozhukov, V. and C. Hansen (2004). The effects of 401 (k) participation on the wealth distribution: an
instrumental quantile regression analysis. Review of Economics and Statistics 86, 735â€“51.
Chernozhukov, V., D. Chetverikov and K. Kato (2014). Gaussian approximation of suprema of empirical
processes. Annals of Statistics 42, 1564â€“97.
Chernozhukov, V., J. Escanciano, H. Ichimura, W. Newey and J. Robins (2016). Locally robust semiparametric estimation. Preprint (arXiv:1608.00033).
Chernozhukov, V., C. Hansen and M. Spindler (2015a). Post-selection and post-regularization inference
in linear models with very many controls and instruments. Americal Economic Review: Papers and
Proceedings 105, 486â€“90.
Chernozhukov, V., C. Hansen and M. Spindler (2015b). Valid post-selection and post-regularization
inference: an elementary, general approach. Annual Review of Economics 7, 649â€“88.
DasGupta, A. (2008). Asymptotic Theory of Statistics and Probability, Springer Texts in Statistics. Berlin:
Springer.
Fan, J., S. Guo and K. Yu (2012). Variance estimation using refitted cross-validation in ultrahigh
dimensional regression. Journal of the Royal Statistical Society, Series B 74, 37â€“65.
Farrell, M. (2015). Robust inference on average treatment effects with possibly more covariates than
observations. Journal of Econometrics 174, 1â€“23.
Ferguson, T. (1967). Mathematical Statistics: A Decision Theoretic Approach. New York, NY: Academic
Press.
FroÌˆlich, M. (2007). Nonparametric IV estimation of local average treatment effects with covariates. Journal
of Econometrics 139, 35â€“75.
Gautier, E. and A. Tsybakov (2014). High-dimensional instrumental variables regression and confidence
sets. Preprint (arXiv:1105.2454v4).
Hahn, J. (1998). On the role of the propensity score in efficient semi-parametric estimation of average
treatment effects. Econometrica 66, 315â€“31.
Hansen, L. (1982). Large sample properties of generalized method of moments estimators. Econometrica
50, 1029â€“54.
Hasminskii, R. and I. Ibragimov (1979). On the nonparametric estimation of functionals. In P. Mandl and
M. HusÌŒkovaÌ (Eds.), Proceedings of the Second Prague Symposium on Asymptotic Statistics, 41â€“51.
Amsterdam: North-Holland.
Hirano, K., G. W. Imbens and G. Ridder (2003). Efficient estimation of average treatment effects using the
estimated propensity score. Econometrica 71, 1161â€“89.
Hubbard, A. E., S. Kherad-Pajouh and M. J. van der Laan (2016). Statistical inference for data adaptive
target parameters. International Journal of Biostatistics 12, 3â€“19.

C

2017 Royal Economic Society.

C48

V. Chernozhukov et al.

Ibragimov, I. A. and R. Z. Hasminskii (1981). Statistical Estimation: Asymptotic Theory. New York, NY:
Springer-Verlag.
Ichimura, H. and W. Newey (2015). The influence function of semi-parametric estimators. Preprint
(arXiv:1508.01378).
Imai, K. and M. Ratkovic (2013). Estimating treatment effect heterogeneity in randomized program
evaluation. Annals of Applied Statistics 7, 443â€“70.
Imbens, G. and J. Angrist (1994). Identification and estimation of local average treatment effects.
Econometrica 62, 467â€“475.
Imbens, G. W. and D. B. Rubin (2015). Causal Inference for Statistics, Social, and Biomedical Sciences:
An Introduction. Cambridge: Cambridge University Press.
Javanmard, A. and A. Montanari (2014a). Confidence intervals and hypothesis testing for high-dimensional
regression. Journal of Machine Learning Research 15, 2869â€“909.
Javanmard, A. and A. Montanari (2014b). Hypothesis testing in high-dimensional regression under the
Gaussian random design model: asymptotic theory. IEEE Transactions on Information Theory 60, 6522â€“
54.
Kozbur, D. (2016). Testing-based forward model selection. Preprint (arXiv:1512.02666).
Lee, L. (2005). A c(Î±)-type gradient test in the GMM approach. Working paper, Ohio State University.
Levit, B. Y. (1975). On the efficiency of a class of nonparametric estimates. Theory of Probability and Its
Applications 20, 723â€“40.
Linton, O. (1996). Edgeworth approximation for MINPIN estimators in semi-parametric regression models.
Econometric Theory 12, 30â€“60.
Luedtke, A. R. and M. J. van der Laan (2016). Optimal individualized treatments in resource-limited
settings. International Journal of Biostatistics 12, 283â€“303.
Luo, Y. and M. Spindler (2016). High-dimensional l2 boosting: rate of convergence. Preprint
(arXiv:1602.08927).
Nevelson, M. (1977). On one informational lower bound. Problemy Peredachi Informatsii 13, 26â€“31.
Newey, W. (1990). Semi-parametric efficiency bounds. Journal of Applied Econometrics 5, 99â€“135.
Newey, W. (1994). The asymptotic variance of semi-parametric estimators. Econometrica 62, 1349â€“82.
Newey, W. K., F. Hsieh and J. Robins (1998). Undersmoothing and bias corrected functional estimation.
Working paper, MIT Economics Department (http://economics.mit.edu/files/11219).
Newey, W. K., F. Hsieh and J. M. Robins (2004). Twicing kernels and a small bias property of semiparametric estimators. Econometrica 72, 947â€“62.
Neyman, J. (1959). Optimal asymptotic tests of composite statistical hypotheses. In U. Grenander (Ed.),
Probability and Statistics, 416â€“44. New York, NY: Wiley.
Neyman, J. (1979). c(Î±) tests and their use. Sankhya, 1â€“21.
Poterba, J. M., S. F. Venti and D. A. Wise (1994a). 401(k) plans and tax-deferred savings. In D. Wise (Ed.),
Studies in the Economics of Aging, 105â€“42. Chicago, IL: University of Chicago Press.
Poterba, J. M., S. F. Venti, and D. A. Wise (1994b). Do 401(k) contributions crowd out other personal
saving? Journal of Public Economics 58, 1â€“32.
Robins, J. and A. Rotnitzky (1995). Semi-parametric efficiency in multivariate regression models with
missing data. Journal of the American Statistical Association 90, 122â€“29.
Robins, J., L. Li, E. Tchetgen and A. van der Vaart (2008). Higher order influence functions and minimax
estimation of nonlinear functionals. In D. Nolan and T. Speed (Eds.), Probability and Statistics: Essays
in Honor of David A. Freedman, 335â€“421. Beachwood, OH: Institute of Mathematical Statistics.
Robins, J., P. Zhang, R. Ayyagari, R. Logan, E. Tchetgen, L. Li, A. Lumley and A. van der Vaart (2013).
New statistical approaches to semi-parametric regression with application to air pollution research.
Research Report 175, Health Effects Institute.

C

2017 Royal Economic Society.

Double/debiased machine learning

C49

Robins, J., L. Li, R. Mukherjee, E. Tchetgen and A. van der Vaart (2017). Minimax estimation of a
functional on a structured high dimensional model. Forthcoming in Annals of Statistics.
Robinson, P. M. (1988). Root-N -consistent semi-parametric regression. Econometrica 56, 931â€“54.
Rosenbaum, P. R. and D. B. Rubin (1983). The central role of the propensity score in observational studies
for causal effects. Biometrika 70, 41â€“55.
Scharfstein, D. O., A. Rotnitzky and J. M. Robins (1999). Rejoinder to â€œadjusting for non-ignorable dropout using semi-parametric non-response modelsâ€. Journal of the American Statistical Association 94,
1135â€“46.
Schick, A. (1986). On asymptotically efficient estimation in semi-parametric models. Annals of Statistics
14, 1139â€“51.
Severini, T. A. and W. H. Wong (1992). Profile likelihood and conditionally parametric models. Annals of
Statistics 20, 1768â€“802.
Toth, B. and M. J. van der Laan (2016). TMLE for marginal structural models based on an instrument.
Working Paper 350, UC Berkeley Division of Biostatistics Working Paper Series.
van de, Geer, S., P. BuÌˆhlmann, Y. Ritov and R. Dezeure (2014). On asymptotically optimal confidence
regions and tests for high-dimensional models. Annals of Statistics 42, 1166â€“202.
van der Laan, M. J. (2015). A generally efficient targeted minimum loss based estimator. Working Paper
343, UC Berkeley Division of Biostatistics Working Paper Series.
van der Laan, M. J. and S. Rose (2011). Targeted Learning: Causal Inference for Observational and
Experimental Data. Berlin: Springer.
van der Laan, M. and D. Rubin (2006). Targeted maximum likelihood learning. Working Paper 213, UC
Berkeley Division of Biostatistics Working Paper Series.
van der Laan, M. J., E. C. Polley and A. E. Hubbard (2007). Super learner. Statistical Applications in
Genetics and Molecular Biology 6.
van der Vaart, A. W. (1991). On differentiable functionals. Annals of Statistics 19, 178â€“204.
van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge: Cambridge University Press.
Wager, S. and G. Walther (2016). Adaptive concentration of regression trees, with application to random
forests. Preprint (arXiv:1503.06388).
Wooldridge, J. (1991). Specification testing and quasi-maximum-likelihood estimation. Journal of
Econometrics 48, 29â€“55.
Zhang, C. and S. Zhang (2014). Confidence intervals for low-dimensional parameters with highdimensional data. Journal of the Royal Statistical Society, Series B 76, 217â€“42.
Zheng, W. and M. J. van der Laan (2011). Cross-validated targeted minimum-loss-based estimation. In M.
J. van der Laan and S. Rose (Eds.), Targeted Learning, 459â€“74. Berlin: Springer.
Zheng, W., Z. Luo, and M. J. van der Laan (2016). Marginal structural models with counterfactual effect
modifiers. Working Paper 348, UC Berkeley Division of Biostatistics Working Paper Series.

APPENDIX: PROOFS OF RESULTS
In this appendix, we use C to denote a strictly positive constant that is independent of n and P âˆˆ PN . The
value of C may change at each appearance. Also, the notation aN  bN means that aN â‰¤ CbN for all n and
some C. The notation aN  bN means that bN  aN . Moreover, the notation aN = o(1) means that there
exists a sequence (bN )nâ‰¥1 of positive numbers such that (a) |aN | â‰¤ bN for all n, (b) bN is independent of
P âˆˆ PN for all n and (c) bN â†’ 0 as n â†’ âˆž. Finally, the notation aN = OP (bN ) means that for all  > 0,
there exists C such that P r P (aN > CbN ) â‰¤ 1 âˆ’  for all n. Using this notation allows us to avoid repeating
â€˜uniformly over P âˆˆ PN â€™ many times in the proofs.

C

2017 Royal Economic Society.

C50

V. Chernozhukov et al.

Define the empirical process Gn (Ïˆ(W )) as a linear operator acting on measurable functions Ïˆ : W â†’
R such that ÏˆP ,2 < âˆž via

1 
Gn (Ïˆ(W )) := Gn,I (Ïˆ(W )) := âˆš
Ïˆ(Wi ) âˆ’ Ïˆ(w)dP(w).
n iâˆˆI
Analogously, we defined the empirical expectation as
En [Ïˆ(W )] := En,I [Ïˆ(W )] :=

1
Ïˆ(Wi ).
n iâˆˆI

The following lemma is useful particularly in the sample-splitting contexts.
L EMMA 6.1. (C ONDITIONAL CONVERGENCE IMPLIES UNCONDITIONAL) Let {Xm } and {Ym } be
sequences of random vectors. (a) If, for m â†’ 0, Pr(Xm  > m | Ym ) â†’Pr 0, then Pr(Xm  > m ) â†’ 0. In
particular, this occurs if E[Xm q /mq | Ym ] â†’Pr 0 for some q â‰¥ 1, by Markovâ€™s inequality. (b) Let {Am } be
a sequence of positive constants. If Xm  = OP (Am ) conditional on Ym , namely, that for any m â†’ âˆž,
P r(Xm  > m Am | Ym ) â†’Pr 0, then Xm  = OP (Am ) unconditionally, namely, that for any m â†’ âˆž,
Pr(Xm  > m Am ) â†’ 0.
Proof: (a) For any  > 0 P r(Xm  > m ) â‰¤ E[P r(Xm  > m | Ym )] â†’ 0, as the sequence {P r(Xm  >
m | Ym )} is uniformly integrable. To show the second part note that P r(Xm  > m | Ym ) â‰¤
p
E[Xm q /mq | Ym ] âˆ¨ 1 â†’ 0 by Markovâ€™s inequality. (b) This follows from (a).

Let (Wi )ni=1 be a sequence of independent copies of a random element W taking values in a measurable
space (W, AW ) according to a probability law P . Let F be a set of suitably measurable functions f : W â†’
R, equipped with a measurable envelope F : W â†’ R.
L EMMA 6.2. (M AXIMAL INEQUALITY, CHERNOZHUKOV ET AL. (2014)) Work with the set-up above.
Suppose that F â‰¥ supf âˆˆF |f | is a measurable envelope for F with F P ,q < âˆž for some q â‰¥ 2. Let M =
maxiâ‰¤n F (Wi ) and Ïƒ 2 > 0 be any positive constant such that supf âˆˆF f 2P ,2 â‰¤ Ïƒ 2 â‰¤ F 2P ,2 . Suppose that
there exist constants a â‰¥ e and v â‰¥ 1 such that
log sup N (F Q,2 , F,  Â· Q,2 ) â‰¤ v log(a/), 0 <  â‰¤ 1.
Q

Then

âŽž
âŽ›
 aF   vM
 aF  
P
,2
P
,2
P
,2
âŽ ,
+ âˆš
EP [Gn F ] â‰¤ K âŽ vÏƒ 2 log
log
Ïƒ
Ïƒ
n

where K is an absolute constant.
Moreover, for every t â‰¥ 1, with probability
> 1 âˆ’ t âˆ’q/2 , Gn F â‰¤


âˆš
âˆ’1/2
âˆ’1 âˆ’1/2
MP ,q ) t + Î± n
MP ,2 t , âˆ€Î± > 0, where K(q) > 0 is a
(1 + Î±)EP [Gn F ] + K(q) (Ïƒ + n
constant depending only on q. In particular, setting a â‰¥ n and t = log n, with probability > 1 âˆ’ c(log n)âˆ’1 ,
âŽž
âŽ› 
 aF   vM
 aF  
P ,2
P ,q
P ,2
âŽ ,
+ âˆš
(A.1)
Gn F â‰¤ K(q, c)âŽÏƒ v log
log
Ïƒ
Ïƒ
n
where MP ,q â‰¤ n1/q F P ,q and K(q, c) > 0 is a constant depending only on q and c.
Proof of Lemma 2.1: Because J exists and JÎ²Î² is invertible, (2.8) has the unique solution Î¼0 given in
(2.10), and so we have by (2.6) that E[Ïˆ(W ; Î¸0 , Î·0 )] = 0 for Î·0 given in (2.9). Moreover,


âˆ‚Î· EP Ïˆ(W ; Î¸0 , Î·0 ) = [JÎ¸Î² âˆ’ Î¼0 JÎ²Î² ], E[âˆ‚Î² (W ; Î¸0 , Î²0 )] âŠ— IdÎ¸ Ã—dÎ¸ = 0,

C

2017 Royal Economic Society.

C51

Double/debiased machine learning

where IdÎ¸ Ã—dÎ¸ is the dÎ¸ Ã— dÎ¸ identity matrix and âŠ— is the Kronecker product. Hence, the asserted claim holds
by the remark after Definition 2.1.

Proof of Lemma 2.2: The proof is similar to that of Lemma 2.1, except that now we have to verify (2.4)
intead of (2.3). To do so, take any Î² âˆˆ B such that Î² âˆ’ Î²0 âˆ—q â‰¤ Î»N /rN and any dÎ¸ Ã— dÎ² matrix Î¼. Denote
Î· = (Î² , vec(Î¼) ) . Then
âˆ‚Î· EP Ïˆ(W, Î¸0 , Î·0 )[Î· âˆ’ Î·0 ] = (JÎ¸Î² âˆ’ Î¼0 JÎ²Î² )(Î² âˆ’ Î²0 )
â‰¤ JÎ¸Î² âˆ’ Î¼0 JÎ²Î² q Ã— Î² âˆ’ Î²0 âˆ—q â‰¤ rn Ã— (Î»N /rN ) = Î»N .


This completes the proof of the lemma.
Proof of Lemma 2.3: The proof is similar to that of Lemma 2.1, except that now we have
âˆ‚Î· EP Ïˆ(W, Î¸0 , Î·0 ) = [Î¼0 GÎ² , EP m(W, Î¸0 , Î²0 ) âŠ— IdÎ¸ Ã—dÎ¸ ] = 0,
where IdÎ¸ Ã—dÎ¸ is the dÎ¸ Ã— dÎ¸ identity matrix and âŠ— is the Kronecker product.



Proof of Lemma 2.4: The proof is similar to that of Lemma 2.2, except that now for any Î² âˆˆ B such that
Î² âˆ’ Î²0 1 â‰¤ Î»N / rN , any dÎ¸ Ã— k matrix Î¼, and Î· = (Î² , vec(Î¼) ) , we have
âˆ‚Î· EP Ïˆ(W, Î¸0 , Î·0 )[Î· âˆ’ Î·0 ] = Î¼0 GÎ² (Î² âˆ’ Î²0 )
â‰¤ A âˆ’1/2 L âˆ’ Î³0 L Lâˆž Ã— Î² âˆ’ Î²0 1
â‰¤ rn Ã— (Î»N / rN ) = Î»N .

Proof of Lemma 2.5: Take any Î· âˆˆ T , and consider the function
Q(W ; Î¸, r) := (W ; Î¸, Î·0 (Î¸ ) + r(Î·(Î¸ ) âˆ’ Î·0 (Î¸ ))),

Î¸âˆˆ

, r âˆˆ [0, 1].

Then
Ïˆ(W ; Î¸, Î·0 + r(Î· âˆ’ Î·0 )) = âˆ‚Î¸ Q(W ; Î¸, r),
and so
âˆ‚r EP [Ïˆ(W ; Î¸, Î·0 + r(Î· âˆ’ Î·0 ))] = âˆ‚r EP [âˆ‚Î¸ Q(W ; Î¸, r)]
= âˆ‚r âˆ‚Î¸ EP [Q(W ; Î¸, r)] = âˆ‚Î¸ âˆ‚r EP [Q(W ; Î¸, r)]
= âˆ‚Î¸ âˆ‚r EP [(W ; Î¸, Î·0 (Î¸ ) + r(Î·(Î¸ ) âˆ’ Î·0 (Î¸ )))].

(A.2)

Hence,
âˆ‚r EP [Ïˆ(W ; Î¸, Î·0 + r(Î· âˆ’ Î·0 ))]|r=0 = 0
because
âˆ‚r EP [(W ; Î¸, Î·0 (Î¸ ) + r(Î·(Î¸) âˆ’ Î·0 (Î¸ )))]|r=0 = 0,

for all Î¸ âˆˆ

,

as Î·0 (Î¸ ) = Î²Î¸ solves the optimization problem
max EP [(W ; Î¸, Î²)],
Î²âˆˆB

for all Î¸ âˆˆ

.

Here, the regularity conditions are needed to make sure that we can interchange EP and âˆ‚Î¸ and also âˆ‚Î¸ and

âˆ‚r in (A.2).

C

2017 Royal Economic Society.

C52

V. Chernozhukov et al.

Proof of Lemma 2.6: First, we demonstrate that Î¼0 âˆˆ L1 (R; RdÎ¸ Ã—dm ). Indeed,
EP [Î¼0 (R)] â‰¤ EP [A(R) (R)âˆ’1 ] + EP [G(Z)(R)(R)âˆ’1 ]
â‰¤ EP [A(R) Ã— (R)âˆ’1 ] + EP [G(Z) Ã— (R) Ã— (R)âˆ’1 ]
â‰¤ (EP [A(R)2 ] Ã— EP [(R)âˆ’2 ])1/2
+ (EP [G(Z)2 Ã— (R)2 ] Ã— EP [(R)âˆ’2 ])1/2 ,
which is finite by assumptions of the lemma as
EP [G(Z)2 Ã— (R)2 ] â‰¤ (EP [G(Z)4 ] Ã— EP [(R)4 ])1/2 < âˆž.
Next, we demonstrate that
EP [Ïˆ(W, Î¸0 , Î·)] < âˆž

for all Î· âˆˆ T .

Indeed, for all Î· âˆˆ T , there exist Î¼ âˆˆ L1 (R; RdÎ¸ Ã—dm ) and h âˆˆ H such that Î· = (Î¼, h), and so
EP [Ïˆ(W, Î¸0 , Î·)] = EP [Î¼(X)m(W, Î¸0 , h(Z))]
â‰¤ EP [Î¼(R) Ã— m(W, Î¸0 , h(Z))]
= EP [Î¼(R) Ã— EP [m(W, Î¸0 , h(Z)) | R]] â‰¤ Ch E[Î¼(R)],
which is finite by assumptions of the lemma. Further, (2.1) holds because
EP [Ïˆ(W, Î¸0 , Î·0 )] = EP [Î¼0 (R)m(W, Î¸0 , h0 (Z))]
= EP [Î¼0 (R)EP [m(W, Î¸0 , h0 (Z)) | R]] = 0,

(A.3)

where the last equality follows from (2.22).
Finally, we demonstrate that (2.3) holds. To do so, take any Î· = (Î¼, h) âˆˆ TN = T . Then
EP [Ïˆ(W, Î¸0 , Î·0 + r(Î· âˆ’ Î·0 )]
= EP [(Î¼0 (R) + r(Î¼(R) âˆ’ Î¼0 (R)))m(W, Î¸0 , h0 (Z) + r(h(Z) âˆ’ h0 (Z)))],
and so
âˆ‚Î· EP [Ïˆ(W, Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] = I1 + I2 ,
where
I1 = EP [(Î¼(R) âˆ’ Î¼0 (R))m(W, Î¸0 , h0 (Z)],
I2 = EP [Î¼0 (R)âˆ‚v m(W, Î¸0 , v)|v=h0 (Z) (h(Z) âˆ’ h0 (Z))].
Here, I1 = 0 by the same argument as that in (A.3) and I2 = 0 because
I2 = EP [Î¼0 (R)EP [âˆ‚v m(W, Î¸0 , v)|v=h0 (Z) | X](h(Z) âˆ’ h0 (Z))]
= EP [Î¼0 (R)(X)(h(Z) âˆ’ h0 (Z))] = EP [EP [Î¼0 (R)(R) | Z](h(Z) âˆ’ h0 (Z))] = 0


C

2017 Royal Economic Society.

Double/debiased machine learning

C53

because
EP [Î¼0 (R)(X) | Z] = EP [A(R) (R)âˆ’1 (R) | Z] âˆ’ EP [G(Z)(R) (R)âˆ’1 (R) | Z]
= EP [A(R) (R)âˆ’1 (R) | Z] âˆ’ G(Z)EP [(R) (R)âˆ’1 (R) | Z]
= EP [A(R) (R)âˆ’1 (R) | Z] âˆ’ EP [A(R) (R)âˆ’1 (R) | Z]
âˆ’1

Ã— EP [(R) (R)âˆ’1 (R) | Z]
Ã— EP [(R) (R)âˆ’1 (R) | Z]
= EP [A(R) (R)âˆ’1 (R) | Z] âˆ’ EP [A(R) (R)âˆ’1 (R) | Z] = 0.

Proof of Theorem 3.1 (DML2 case) : To start with, note that (3.11) follows immediately from the
assumptions. Hence, it suffices to show that (3.10) holds uniformly over P âˆˆ PN .
Fix any sequence {PN }Nâ‰¥1 such that PN âˆˆ PN for all N â‰¥ 1. Because this sequence is chosen arbitrarily,
to show that (3.10) holds uniformly over P âˆˆ PN , it suffices to show that
âˆš

N
1 
NÏƒ âˆ’1 (Î¸Ìƒ0 âˆ’ Î¸0 ) = âˆš
ÏˆÌ„(Wi ) + OPN (ÏN )  N (0, Id ).
N i=1

(A.4)

To do so, we proceed in five steps. Step 1 shows the main argument, and Steps 2â€“5 present auxiliary
calculations. In the proof, it will be convenient to denote by EN the event that Î·Ì‚0,k âˆˆ TN for all k âˆˆ [K].
Note that by Assumption 3.2 and the union bound, P r PN (EN ) â‰¥ 1 âˆ’ Kn = 1 âˆ’ o(1) as n = o(1).
S TEP 1. Denote
K
1 
JË†0 :=
En,k [Ïˆ a (W ; Î·Ì‚0,k )],
K k=1

RN,2 :=

RN,1 := JË†0 âˆ’ J0 ,

K
N
1 
1 
En,k [Ïˆ(W ; Î¸0 , Î·Ì‚0,k )] âˆ’
Ïˆ(Wi ; Î¸0 , Î·0 ).
K k=1
N i=1

In Steps 2â€“5 respectively, we will show that
RN,1  = OPN (N âˆ’1/2 + rN ),

(A.5)

RN,2  = OPN (N âˆ’1/2 rN + Î»N + Î»N ),

(A.6)

N


 âˆ’1/2 

Ïˆ(Wi ; Î¸0 , Î·0 ) = OPN (1),
N

(A.7)

i=1

Ïƒ âˆ’1  = OPN (1).

(A.8)

Because N âˆ’1/2 + rN â‰¤ ÏN = o(1) and all singular values of J0 are bounded below from zero by
Assumption 3.1, it follows from (A.5) that with PN -probability 1 âˆ’ o(1), all singular values of JË†0
are bounded below from zero as well. Therefore, with the same PN -probability,
K
1 
En,k [Ïˆ b (W ; Î·Ì‚0,k )]
Î¸Ìƒ0 = âˆ’JË†0âˆ’1
K k=1

C

2017 Royal Economic Society.

C54

V. Chernozhukov et al.
and
âˆš

âˆš
N (Î¸Ìƒ0 âˆ’ Î¸0 ) = âˆ’ N JË†0âˆ’1

K
1 
En,k [Ïˆ b (W ; Î·Ì‚0,k )] + JË†0 Î¸0
K k=1

K
âˆš
1 
= âˆ’ N JË†0âˆ’1
En,k [Ïˆ(W ; Î¸0 , Î·Ì‚0,k )]
K k=1
âˆ’1

= âˆ’ J0 + RN,1

Ã—

N
âˆš
1 
Ïˆ(Wi ; Î¸0 , Î·0 ) + N RN,2 .
âˆš
N i=1

(A.9)

In addition, given that
(J0 + RN,1 )âˆ’1 âˆ’ J0âˆ’1 = (J0 + RN,1 )âˆ’1 (J0 âˆ’ (J0 + RN,1 ))J0âˆ’1
= âˆ’(J0 + RN,1 )âˆ’1 RN,1 J0âˆ’1 ,
it follows from (A.5) that
(J0 + RN,1 )âˆ’1 âˆ’ J0âˆ’1  â‰¤ (J0 + RN,1 )âˆ’1  Ã— RN,1  Ã— J0âˆ’1 

Moreover, because rN +

âˆš

= OPN (1)OPN (N âˆ’1/2 + rN )OPN (1) = OPN (N âˆ’1/2 + rN ).

(A.10)

N(Î»N + Î»N ) â‰¤ ÏN = o(1), it follows from (A.6) and (A.7) that

N
N

  1 
 1 
âˆš
âˆš

 

Ïˆ(Wi ; Î¸0 , Î·0 ) + N RN,2  â‰¤  âˆš
Ïˆ(Wi ; Î¸0 , Î·0 ) +  N RN,2 
âˆš
N i=1
N i=1

= OPN (1) + oPN (1) = OPN (1).

(A.11)

Combining (A.10) and (A.11) gives
N


 1 
âˆš


Ïˆ(Wi ; Î¸0 , Î·0 ) + NRN,2 
((J0 + RN,1 )âˆ’1 âˆ’ J0âˆ’1 ) Ã— âˆš
N i=1
N

  1 

âˆš

 

â‰¤ (J0 + RN,1 )âˆ’1 âˆ’ J0âˆ’1  Ã—  âˆš
Ïˆ(Wi ; Î¸0 , Î·0 ) + NRN,2 
N i=1

= OPN (N âˆ’1/2 + rN ).
Now, substituting the last bound into (A.9) yields
âˆš

N

 1 
âˆš
N(Î¸Ìƒ0 âˆ’ Î¸0 ) = âˆ’J0âˆ’1 Ã— âˆš
Ïˆ(Wi ; Î¸0 , Î·0 ) + NRN,2 + OPN (N âˆ’1/2 + rN )
N i=1
N
1 
Ïˆ(Wi ; Î¸0 , Î·0 ) + OPN (ÏN ),
= âˆ’J0âˆ’1 Ã— âˆš
N i=1

where in the second line we used (A.6) and the definition of ÏN . Combining this with (A.8) gives
âˆš

N
1 
N Ïƒ âˆ’1 (Î¸Ìƒ0 âˆ’ Î¸0 ) = âˆš
ÏˆÌ„(Wi ) + OPN (ÏN )
N i=1

(A.12)


C

2017 Royal Economic Society.

C55

Double/debiased machine learning

by the definition of ÏˆÌ„ given in the statement of the theorem. In turn, because ÏN = o(1), combining
(A.12) with the Lindebergâ€“Feller central limit theorem (CLT) and the Cramerâ€“Wold device yields
(A.4). To complete the proof of the theorem, it remains to establish the bounds (A.5)â€“(A.8). We
do so in the following four steps.
S TEP 2. In this step, we establish (A.5). Because K is a fixed integer, which is independent of N ,
it suffices to show that for any k âˆˆ [K],
En,k [Ïˆ a (W ; Î·Ì‚0,k )] âˆ’ EPN [Ïˆ a (W ; Î·0 )] = OPN (N âˆ’1/2 + rN ).

(A.13)

To do so, fix any k âˆˆ [K] and observe that by the triangle inequality,
En,k [Ïˆ a (W ; Î·Ì‚0,k )] âˆ’ EPN [Ïˆ a (W ; Î·0 )] â‰¤ I1,k + I2,k ,

(A.14)

where
I1,k := En,k [Ïˆ a (W ; Î·Ì‚0,k )] âˆ’ EPN [Ïˆ a (W ; Î·Ì‚0,k ) | (Wi )iâˆˆIkc ],
I2,k := EPN [Ïˆ a (W ; Î·Ì‚0,k ) | (Wi )iâˆˆIkc ] âˆ’ EPN [Ïˆ a (W ; Î·0 )].
To bound I2,k , note that on the event EN , which holds with PN -probability 1 âˆ’ o(1),
I2,k â‰¤ sup EPN [Ïˆ a (W ; Î·)] âˆ’ EPN [Ïˆ a (W ; Î·0 )] = rN ,
Î·âˆˆTN

and so I2,k = OPN (rN ). To bound I1,k , note that conditional on (Wi )iâˆˆIkc , the estimator Î·Ì‚0,k is nonstochastic, and so on the event EN ,
2
| (Wi )iâˆˆIkc ] â‰¤ nâˆ’1 EPN [Ïˆ a (W ; Î·Ì‚0,k )2 | (Wi )iâˆˆIkc ]
EPN [I1,k

â‰¤ sup nâˆ’1 EPN [Ïˆ a (W ; Î·)2 ] â‰¤ c12 /n,
Î·âˆˆTN

where the last inequality holds by Assumption 3.2. Hence, I1,k = OPN (N âˆ’1/2 ) by Lemma 6.1.
Combining the bounds I1,k = OPN (N âˆ’1/2 ) and I2,k = OPN (rN ) with (A.14) gives (A.13).
S TEP 3. In this step, we establish (A.6). This is the step where we invoke the Neyman orthogonality
(or near-orthogonality) condition. Again, because K is a fixed integer, which is independent of N ,
it suffices to show that for any k âˆˆ [K],
En,k [Ïˆ(W ; Î¸0 , Î·Ì‚0,k )] âˆ’

1
Ïˆ(Wi ; Î¸0 , Î·0 ) = OPN (N âˆ’1/2 rN + Î»N + Î»N ).
n iâˆˆI

(A.15)

k

To do so, fix any k âˆˆ [K] and introduce the following additional empirical process notation,

1 
Ï†(Wi ) âˆ’ Ï†(w)dPN ,
Gn,k [Ï†(W )] = âˆš
n iâˆˆI
k

where Ï† is any PN -integrable function on W. Then observe that by the triangle inequality,
 I +I

1
3,k
4,k


Ïˆ(Wi ; Î¸0 , Î·0 ) â‰¤
,
âˆš
En,k [Ïˆ(W ; Î¸0 , Î·Ì‚0,k )] âˆ’
n iâˆˆI
n
k

where
I3,k := Gn,k [Ïˆ(W ; Î¸0 , Î·Ì‚0,k )] âˆ’ Gn,k [Ïˆ(W ; Î¸0 , Î·0 )],
âˆš
I4,k := nEPN [Ïˆ(W ; Î¸0 , Î·Ì‚0,k ) | (Wi )iâˆˆIkc ] âˆ’ EPN [Ïˆ(W ; Î¸0 , Î·0 )].

C

2017 Royal Economic Society.

(A.16)

C56

V. Chernozhukov et al.
To bound I3,k , note that, as above, conditional on (Wi )iâˆˆIkc , the estimator Î·Ì‚0,k is non-stochastic, and
so on the event EN ,
2
| (Wi )iâˆˆIkc ] = EPN [Ïˆ(W ; Î¸0 , Î·Ì‚0,k ) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 | (Wi )iâˆˆIkc ]
EPN [I3,k

â‰¤ sup EPN [Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 | (Wi )iâˆˆIkc ]
Î·âˆˆTN

â‰¤ sup EPN [Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 ] = (rN )2
Î·âˆˆTN

by the definition of rN in Assumption 3.2. Hence, I3,k = OPN (rN ) by Lemma 6.1. To bound I4,k ,
introduce the function
fk (r) := EPN [Ïˆ(W ; Î¸0 , Î·0 + r(Î·Ì‚0,k âˆ’ Î·0 )) | (Wi )iâˆˆIkc ] âˆ’ EPN [Ïˆ(W ; Î¸0 , Î·0 )],

r âˆˆ [0, 1].

Then, by Taylor expansion,
fk (1) = fk (0) + fk (0) + fk (rÌƒ)/2,

for some rÌƒ âˆˆ (0, 1).

However, fk (0) = 0 because
EPN [Ïˆ(W ; Î¸0 , Î·0 ) | (Wi )iâˆˆIkc ] = EPN [Ïˆ(W ; Î¸0 , Î·0 )].
In addition, on the event EN , by the Neyman Î»N near-orthogonality condition imposed in
Assumption 3.1,
fk (0) = âˆ‚Î· EPN Ïˆ(W ; Î¸0 , Î·0 )[Î·Ì‚0,k âˆ’ Î·0 ] â‰¤ Î»N .
Moreover, on the event EN ,
fk (rÌƒ) â‰¤ sup fk (r) â‰¤ Î»N
râˆˆ(0,1)

by the definition Î»N in Assumption 3.2. Hence,
âˆš
âˆš
I4,k = nfk (1) = OPN ( n(Î»N + Î»N )).
Combining the bounds on I3,k and I4,k with (A.16) and using the fact that nâˆ’1 = O(N âˆ’1 ) gives
(A.15).
S TEP 4. To establish (A.7), note that
N
2 
 1 




Ïˆ(Wi ; Î¸0 , Î·0 ) = EPN Ïˆ(W ; Î¸0 , Î·0 )2 â‰¤ c12
EPN  âˆš
N i=1

by Assumption 3.2. Combining this with the Markov inequality gives (A.7).
S TEP 5. Here we establish (A.8). Note that all eigenvalues of the matrix
Ïƒ 2 = J0âˆ’1 EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ](J0âˆ’1 )
are bounded from below by c0 /c12 because all singular values of J0 are bounded from above by c1
by Assumption 3.1 and all eigenvalues of EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ] are bounded from below
by c0 by Assumption 3.2. Hence, given that Ïƒ âˆ’1  is the largest eigenvalue of Ïƒ âˆ’1 , it follows that
âˆš

Ïƒ âˆ’1  = c1 / c0 . This gives (A.8) and completes the proof of the theorem.
Proof of Theorem 3.1 (DML1 case): As in the case of the DML2 version, note that (3.11) follows
immediately from the assumptions, and so it suffices to show that (3.10) holds uniformly over P âˆˆ PN .

C

2017 Royal Economic Society.

Double/debiased machine learning

C57

Fix any sequence {PN }Nâ‰¥1 such that PN âˆˆ PN for all N â‰¥ 1. Because this sequence is chosen arbitrarily,
to show that (3.10) holds uniformly over P âˆˆ PN , it suffices to show that
âˆš

N
1 
NÏƒ âˆ’1 (Î¸Ìƒ0 âˆ’ Î¸0 ) = âˆš
ÏˆÌ„(Wi ) + OPN (ÏN )  N (0, Id ).
N i=1

(A.17)

To do so, for all k âˆˆ [K], denote
RN,1,k := JË†0,k âˆ’ J0 ,

JË†0,k := En,k [Ïˆ a (W ; Î·Ì‚0,k )],

RN,2,k := En,k [Ïˆ(W ; Î¸0 , Î·Ì‚0,k )] âˆ’

1
Ïˆ(Wi ; Î¸0 , Î·0 ).
n iâˆˆI
k

Because K is a fixed integer, which is independent of n, it follows by the same arguments as those in Steps
2â€“5 in the Proof of Theorem 3.1 (DML2 case) that
max RN,1,k  = OPN (N âˆ’1/2 + rN ),

(A.18)

max RN,2,k  = OPN (N âˆ’1/2 rN + Î»N + Î»N ),

(A.19)

kâˆˆ[K]

kâˆˆ[K]

max nâˆ’1/2

kâˆˆ[K]

iâˆˆIk

Ïˆ(Wi ; Î¸0 , Î·0 ) = OPN (1),

(A.20)

Ïƒ âˆ’1  = OPN (1).

(A.21)

âˆ’1/2

Because N
+ rN â‰¤ ÏN = o(1) and all singular values of J0 are bounded below from zero by Assumption
3.1, it follows from (A.18) that for all k âˆˆ [K], with PN -probability 1 âˆ’ o(1), all singular values of JË†0,k are
bounded below from zero, and so with the same PN -probability,
âˆ’1
Î¸ÌŒ0,k = âˆ’JË†0,k
En,k [Ïˆ b (W ; Î·Ì‚0,k )].

Hence, by the same arguments as those in Step 1 in the Proof of Theorem 3.1 (DML2 case), it follows from
the bounds (A.18)â€“(A.21) that for all k âˆˆ [K],
âˆš âˆ’1
1 
nÏƒ (Î¸ÌŒ0,k âˆ’ Î¸0 ) = âˆš
ÏˆÌ„(Wi ) + OPN (ÏN ).
n iâˆˆI
k

Therefore,
âˆš

N Ïƒ âˆ’1 (Î¸Ìƒ0 âˆ’ Î¸0 ) =

âˆš

N Ïƒ âˆ’1

K
N
1 

1 
Î¸ÌŒ0,k âˆ’ Î¸0 = âˆš
ÏˆÌ„(Wi ) + OPN (ÏN ).
K k=1
N i=1

(A.22)

In turn, because ÏN = o(1), combining (A.22) with the Lindebergâ€“Feller CLT and the Cramerâ€“Wold device
yields (A.17) and completes the proof of the theorem.

Proof of Theorem 3.2: In this proof, all bounds hold uniformly in P âˆˆ PN for N â‰¥ 3, and we do not
repeat this qualification throughout. Also, the second asserted claim follows immediately from the first one
and Theorem 3.1. Hence, it suffices to prove the first asserted claim.
In the Proof of Theorem 3.1 (DML2 case), we established that JË†0 âˆ’ J0  = OP (rN + N âˆ’1/2 ). Hence,
because J0âˆ’1  â‰¤ c0âˆ’1 by Assumption 3.1 and
EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ] â‰¤ EP [Ïˆ(W ; Î¸0 , Î·0 )2 ] â‰¤ c12
by Assumption 3.2, it suffices to show that
K

1 


En,k [Ïˆ(W ; Î¸Ìƒ0 , Î·Ì‚0,k )Ïˆ(W ; Î¸Ìƒ0 , Î·Ì‚0,k ) ] âˆ’ EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ] = OP (N ).

K k=1

C

2017 Royal Economic Society.

C58

V. Chernozhukov et al.

Moreover, as both K and dÎ¸ , the dimension of Ïˆ, are fixed integers, which are independent of N , the last
bound will follow if we show that for all k âˆˆ [K] and all j, k âˆˆ [dÎ¸ ],
Ikjl := |En,k [Ïˆj (W ; Î¸Ìƒ0 , Î·Ì‚0,k )Ïˆl (W ; Î¸Ìƒ0 , Î·Ì‚0,k )] âˆ’ EP [Ïˆj (W ; Î¸0 , Î·0 )Ïˆl (W ; Î¸0 , Î·0 )]|
satisfies
Ikjl = OP (N ).

(A.23)

To do so, observe that by the triangle inequality,
Ikjl â‰¤ Ikjl,1 + Ikjl,2 ,

(A.24)

where
Ikjl,1 := |En,k [Ïˆj (W ; Î¸Ìƒ0 , Î·Ì‚0,k )Ïˆl (W ; Î¸Ìƒ0 , Î·Ì‚0,k )] âˆ’ En,k [Ïˆj (W ; Î¸0 , Î·0 )Ïˆl (W ; Î¸0 , Î·0 )]|,
Ikjl,2 := |En,k [Ïˆj (W ; Î¸0 , Î·0 )Ïˆl (W ; Î¸0 , Î·0 )] âˆ’ EP [Ïˆj (W ; Î¸0 , Î·0 )Ïˆl (W ; Î¸0 , Î·0 )]|.
We bound Ikjl,2 first. If q â‰¥ 4, then


2
EP [Ikjl,2
] â‰¤ nâˆ’1 EP (Ïˆj (W ; Î¸0 , Î·0 )Ïˆl (W ; Î¸0 , Î·0 ))2

1/2
â‰¤ nâˆ’1 EP [Ïˆj4 (W ; Î¸0 , Î·0 )] Ã— EP [Ïˆl4 (W ; Î¸0 , Î·0 )]
â‰¤ nâˆ’1 EP [Ïˆ(W ; Î¸0 , Î·0 )4 ] â‰¤ c14 ,
where the second line holds by the HoÌˆlder inequality, and the third by Assumption 3.2. Hence, Ikjl,2 =
OP (N âˆ’1/2 ). If q âˆˆ (2, 4), we apply the following von Bahrâ€“Esseen inequality with p = q/2: if X1 , . . . , Xn
are independent random variables with mean zero, then for any p âˆˆ [1, 2],
E

n

i=1

p

Xi

n

1
â‰¤ 2âˆ’
E[|Xi |p ];
n i=1

see DasGupta (2008, p. 650). This gives
EP [Ikj l,2 ]  nâˆ’q/2+1 EP [(Ïˆj (W ; Î¸0 , Î·0 )Ïˆl (W ; Î¸0 , Î·0 ))q/2 ]
q/2

â‰¤ nâˆ’q/2+1 EP [Ïˆ(W ; Î¸0 , Î·0 )q ]  nâˆ’q/2+1
by Assumption 3.2. Hence, Ikjl,2 = OP (N 2/qâˆ’1 ). We conclude that
Ikjl,2 = OP (N âˆ’[(1âˆ’2/q)âˆ§(1/2)] ).

(A.25)

Next, we bound Ikjl,1 . To do so, observe that for any numbers a, b, Î´a and Î´b such that |a| âˆ¨ |b| â‰¤ c and
|Î´a| âˆ¨ |Î´b| â‰¤ r, we have
|(a + Î´a)(b + Î´b) âˆ’ ab| â‰¤ 2r(c + r).
Denoting
Ïˆhi := Ïˆh (Wi ; Î¸0 , Î·0 ) and ÏˆÌ‚hi := Ïˆh (Wi ; Î¸Ìƒ0 , Î·Ì‚0,k ), for (h, i) âˆˆ {j, l} Ã— Ik ,
and applying the inequality above with a := Ïˆji , b := Ïˆli , a + Î´a := ÏˆÌ‚ji , b + Î´b := ÏˆÌ‚li , r := |ÏˆÌ‚ji âˆ’ Ïˆji | âˆ¨
|ÏˆÌ‚li âˆ’ Ïˆli | and c := |Ïˆji | âˆ¨ |Ïˆli | gives

C

2017 Royal Economic Society.

C59

Double/debiased machine learning

Ikjl,1 =

1
1
|ÏˆÌ‚ji ÏˆÌ‚li âˆ’ Ïˆji Ïˆli |
ÏˆÌ‚ji ÏˆÌ‚li âˆ’ Ïˆji Ïˆli â‰¤
n iâˆˆI
n iâˆˆI
k

â‰¤

k

2
(|ÏˆÌ‚ji âˆ’ Ïˆji | âˆ¨ |ÏˆÌ‚li âˆ’ Ïˆli |) Ã— (|Ïˆji | âˆ¨ |Ïˆli | + |ÏˆÌ‚ji âˆ’ Ïˆji | âˆ¨ |ÏˆÌ‚li âˆ’ Ïˆli |)
n iâˆˆI
k

2
(|ÏˆÌ‚ji âˆ’ Ïˆji |2 âˆ¨ |ÏˆÌ‚li âˆ’ Ïˆli |2 )
n iâˆˆI

â‰¤

1/2

k

2
(|Ïˆji | âˆ¨ |Ïˆli | + |ÏˆÌ‚ji âˆ’ Ïˆji | âˆ¨ |ÏˆÌ‚li âˆ’ Ïˆli |)2
n iâˆˆI

Ã—

1/2

.

k

In addition, the expression in the last line above is bounded by
2
|Ïˆji |2 âˆ¨ |Ïˆli |2
n iâˆˆI

1/2

+

k

2
|ÏˆÌ‚ji âˆ’ Ïˆji |2 âˆ¨ |ÏˆÌ‚li âˆ’ Ïˆli |2
n iâˆˆI

1/2

,

k

and so
2
 RN Ã—
Ikjl,1

1
Ïˆ(Wi ; Î¸0 , Î·0 )2 + RN ,
n iâˆˆI
k

where
RN :=

1
Ïˆ(Wi ; Î¸Ìƒ0 , Î·Ì‚0,k ) âˆ’ Ïˆ(Wi ; Î¸0 , Î·0 )2 .
n iâˆˆI
k

Moreover,
1
Ïˆ(Wi ; Î¸0 , Î·0 )2 = OP (1),
n iâˆˆI
k

by the Markov inequality because
 

1
2
EP
Ïˆ(Wi ; Î¸0 , Î·0 ) = EP [Ïˆ(W ; Î¸0 , Î·0 2 ] â‰¤ c12
n iâˆˆI
k

by Assumption 3.2. It remains to bound RN . We have
RN 

1 a
1
Ïˆ (Wi ; Î·Ì‚0,k )(Î¸Ìƒ0 âˆ’ Î¸0 )2 +
Ïˆ(Wi ; Î¸0 , Î·Ì‚0,k ) âˆ’ Ïˆ(Wi ; Î¸0 , Î·0 )2 .
n iâˆˆI
n iâˆˆI
k

(A.26)

k

The first term on the right-hand side of (A.26) is bounded from above by
1 a
Ïˆ (Wi ; Î·Ì‚0,k )2 Ã— Î¸Ìƒ0 âˆ’ Î¸0 2 = OP (1) Ã— OP (N âˆ’1 ) = OP (N âˆ’1 ),
n iâˆˆI
k

and the conditional expectation of the second term given (Wi )iâˆˆIkc on the event that Î·Ì‚0,k âˆˆ TN is equal to
EP [Ïˆ(W ; Î¸0 , Î·Ì‚0,k ) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 | (Wi )iâˆˆIkc ]
â‰¤ sup EP [Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 | (Wi )iâˆˆIkc ] = (rN )2 .
Î·âˆˆTN


C

2017 Royal Economic Society.

C60

V. Chernozhukov et al.

Because the event that Î·Ì‚0,k âˆˆ TN holds with probability 1 âˆ’ N = 1 âˆ’ o(1), it follows that RN =
OP (N âˆ’1 + (rN )2 ), and so
Ikjl,1 = OP (N âˆ’1/2 + rN ).

(A.27)

Combining the bounds (A.25) and (A.27) with (A.24) gives (A.23) and completes the proof of the
theorem.

Proof of Theorem 3.3: We only consider the case of the DML1 estimator and note that the DML2 estimator
can be treated similarly.
With the help of Lemma 6.3, which establishes approximate linearity of the subsample DML estimators
Î¸ÌŒ0,k and is presented below, the proof is the same as that in the linear case presented in the Proof of
Theorem 3.1 (DML1 case).
L EMMA 6.3. (L INEARIZATION FOR SUBSAMPLE DML IN NON-LINEAR PROBLEMS) Under the conditions
of Theorem 3.3, for any k = 1, . . . , K, the estimator Î¸ÌŒ0 = Î¸ÌŒ0,k defined by (3.2) obeys
âˆš

1 
nÏƒ0âˆ’1 (Î¸ÌŒ0 âˆ’ Î¸0 ) = âˆš
ÏˆÌ„(Wi ) + OP (Ïn )
n iâˆˆI

(A.28)

uniformly over P âˆˆ PN , where Ïn = nâˆ’1/2 + rN + rN + n1/2 Î»N + n1/2 Î»N  Î´N and where ÏˆÌ„(Â·) :=
âˆ’Ïƒ0âˆ’1 J0âˆ’1 Ïˆ(Â·, Î¸0 , Î·0 ).
Proof: Fix any k = 1, . . . , K and any sequence {PN }Nâ‰¥1 such that PN âˆˆ PN for all N â‰¥ 1. To prove the
asserted claim, it suffices to show that the estimator Î¸ÌŒ0 = Î¸ÌŒ0,k satisfies (A.28) with P replaced by PN . To do
so, we split the proof into four steps. In the proof, we use En , Gn , I and Î·Ì‚0 instead of En,k , Gn,k , Ik and Î·Ì‚0,k ,
respectively.
S TEP 1. (Preliminary rate result) We claim that with PN -probability 1 âˆ’ o(1),
Î¸ÌŒ0 âˆ’ Î¸0  â‰¤ Ï„N .

(A.29)

To show this claim, note that the definition of Î¸ÌŒ0 implies that
En [Ïˆ(W ; Î¸ÌŒ0 , Î·Ì‚0 )] â‰¤ En [Ïˆ(W ; Î¸0 , Î·Ì‚0 )] + N ,
which in turn implies via the triangle inequality that, with PN -probability 1 âˆ’ o(1),
EPN [Ïˆ(W ; Î¸, Î·0 )]|Î¸=Î¸ÌŒ0  â‰¤ N + 2I1 + 2I2 ,

(A.30)

where
I1 :=

sup
Î¸âˆˆ ,Î·âˆˆTN

EPN [Ïˆ(W ; Î¸, Î·)] âˆ’ EPN [Ïˆ(W ; Î¸, Î·0 )],

I2 := max sup En [Ïˆ(W ; Î¸, Î·)] âˆ’ EPN [Ïˆ(W ; Î¸, Î·)].
Î·âˆˆ{Î·0 ,Î·Ì‚0 } Î¸âˆˆ

Here N = o(Ï„N ) because N = o(Î´N N âˆ’1/2 ), Î´N = o(1) and Ï„N â‰¥ c0 N âˆ’1/2 log n. Also, I1 = rN â‰¤
Î´N Ï„N = o(Ï„N ) by Assumption 3.4(c). Moreover, applying Lemma 6.2 to the function class F1,Î· for
Î· = Î·0 and Î· = Î·Ì‚0 defined in Assumption 3.4, conditional on (Wi )iâˆˆI c and I c , so that Î·Ì‚0 is fixed
after conditioning, shows that with PN -probability 1 âˆ’ o(1),
I2  N âˆ’1/2 (1 + N âˆ’1/2+1/q log n)  N âˆ’1/2 = o(Ï„N ).
Hence, it follows from (A.30) and Assumption 3.3 that with PN -probability 1 âˆ’ o(1),
J0 (Î¸ÌŒ0 âˆ’ Î¸0 ) âˆ§ c0 â‰¤ EPN [Ïˆ(W ; Î¸, Î·0 )]|Î¸=Î¸ÌŒ0  = o(Ï„N ).

C

(A.31)

2017 Royal Economic Society.

C61

Double/debiased machine learning

Combining this bound with the fact that the singular values of J0 are bounded away from zero,
which holds by Assumption 3.3, gives the claim of this step.
S TEP 2. (Linearization) Here we prove the claim of the lemma. First, by definition of Î¸ÌŒ0 , we have
âˆš
âˆš
âˆš
nEn [Ïˆ(W ; Î¸ÌŒ0 , Î·Ì‚0 )] â‰¤ inf nEn [Ïˆ(W ; Î¸, Î·Ì‚0 )] + N n.

(A.32)

Î¸âˆˆ

Also, it will be shown in Step 4 that
âˆš
I3 := inf nEn [Ïˆ(W ; Î¸, Î·Ì‚0 )]
Î¸âˆˆ

âˆš
âˆš
= OPN (nâˆ’1/2+1/q log n + rN log1/2 (1/rN ) + Î»N n + Î»N n).

(A.33)

Moreover, for any Î¸ âˆˆ and Î· âˆˆ TN , we have
âˆš
âˆš
nEn [Ïˆ(W ; Î¸, Î·)] = nEn [Ïˆ(W ; Î¸0 , Î·0 )] + Gn [Ïˆ(W ; Î¸, Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )]
âˆš
+ n(EPN [Ïˆ(W ; Î¸, Î·)),

(A.34)

where we are using the fact that EPN [Ïˆ(W ; Î¸0 , Î·0 )]= 0. Finally, by Taylor expansion of the
function r â†’ EPN [Ïˆ(W ; Î¸0 + r(Î¸ âˆ’ Î¸0 ), Î·0 + r(Î· âˆ’ Î·0 ))], which vanishes at r = 0,
EPN [Ïˆ(W ; Î¸, Î·)] = J0 (Î¸ âˆ’ Î¸0 ) + âˆ‚Î· EPN Ïˆ(W ; Î¸0 , Î·0 )[Î· âˆ’ Î·0 ]
 1
2âˆ’1 âˆ‚r2 EPN [W ; Î¸0 + r(Î¸ âˆ’ Î¸0 ), Î·0 + r(Î· âˆ’ Î·0 )]dr.
+

(A.35)

0

Therefore, because Î¸ÌŒ0 âˆ’ Î¸0  â‰¤ Ï„N and Î· âˆˆ TN with PN -probability 1 âˆ’ o(1), and because by
Neyman Î»N -near orthogonality,
âˆ‚Î· EPN [Ïˆ(W ; Î¸0 , Î·0 )][Î·Ì‚0 âˆ’ Î·0 ] â‰¤ Î»N ,
applying (A.34) with Î¸ = Î¸ÌŒ0 and Î· = Î·Ì‚0 , we have with PN -probability 1 âˆ’ o(1),

âˆš
âˆš
âˆš 


nEn [Ïˆ(W ; Î¸0 , Î·0 )] + J0 (Î¸ÌŒ0 âˆ’ Î¸0 ) â‰¤ Î»N n + N n + I3 + I4 + I5 ,
where by Assumption 3.4,
I4 :=

âˆš

n

sup

Î¸âˆ’Î¸0 â‰¤Ï„N ,Î·âˆˆTN





1


âˆš

2âˆ’1 âˆ‚r2 EPN [W ; Î¸0 + r(Î¸ âˆ’ Î¸0 ), Î·0 + r(Î· âˆ’ Î·0 )]dr  â‰¤ Î»N n,

0

and by Step 3 below, with PN -probability 1 âˆ’ o(1),
I5 :=

sup

Î¸âˆ’Î¸0 â‰¤Ï„N

Gn (Ïˆ(W ; Î¸, Î·Ì‚0 ) âˆ’ Ïˆ(W ; Î¸0 , Î·0 ))

â‰¤ rN log1/2 (1/ rN ) + nâˆ’1/2+1/q log n.

(A.36)

Therefore, because all singular values of J0 are bounded below from zero by Assumption 3.3(d),
it follows that
âˆš
âˆš
J0âˆ’1 nEn [Ïˆ(W ; Î¸0 , Î·0 )] + n(Î¸ÌŒ0 âˆ’ Î¸0 )
âˆš
= OPN (nâˆ’1/2+1/q log n + rN log1/2 (1/rN ) + (N + Î»N + Î»N ) n.
âˆ’1/2

(under the
The asserted claim now follows by multiplying both parts of the display by 0
norm on the left-hand side) and noting that singular values of 0 are bounded below from zero by
Assumptions 3.3 and 3.4.

C

2017 Royal Economic Society.

C62

V. Chernozhukov et al.
S TEP 3. Here we derive a bound on I5 in (A.36). We have
I5  sup |Gn (f )|,

F2 = {Ïˆj (Â·, Î¸, Î·Ì‚0 ) âˆ’ Ïˆj (Â·, Î¸0 , Î·0 ) : j = 1, . . . , dÎ¸ , Î¸ âˆ’ Î¸0  â‰¤ Ï„n }.

f âˆˆF2

To bound supf âˆˆF2 |Gn (f )|, we apply Lemma 6.2 conditional on (Wi )iâˆˆI c and I c so that Î·Ì‚0 can be
treated as fixed. Observe that with PN -probability 1 âˆ’ o(1), supf âˆˆF2 f PN ,2  rN where we used
Assumption 3.4. Thus, an application of Lemma 6.2 to the empirical process {Gn (f ), f âˆˆ F2 }
with an envelope F2 = F1,Î·Ì‚0 + F1,Î·0 and Ïƒ = CrN for sufficiently large constant C conditional on
(Wi )iâˆˆI c and I c yields that with PN -probability 1 âˆ’ o(1),
sup |Gn (f )|  rN log1/2 (1/ rN ) + nâˆ’1/2+1/q log n.

(A.37)

f âˆˆF2

This follows because F2 P ,q = F1,Î·Ì‚0 + F1,Î·0 P ,q â‰¤ 2C1 by Assumption 3.4(b) and the triangle
inequality, and
log sup N (F2 Q,2 , F2 ,  Â· Q,2 ) â‰¤ 2v log(2a/ ),

for all 0 <  â‰¤ 1,

Q

because F2 âŠ‚ F1,Î·Ì‚0 âˆ’ F1,Î·0 for F1,Î· defined in Assumption 3.4(b), and
log sup N (F1,Î·Ì‚0 + F1,Î·0 Q,2 , F1,Î·Ì‚0 âˆ’ F1,Î·0 ,  Â· Q,2 )
Q

â‰¤ log sup N ((/2)F1,Î·Ì‚0 Q,2 , F1,Î·Ì‚0 ,  Â· Q,2 )
Q

+ log sup N ((/2)F1,Î·0 Q,2 , F1,Î·0 ,  Â· Q,2 )
Q

by the proof of Theorem 3 in Andrews (1994b). The claim of this step follows.
âˆ’1
S TEP 4. Here we
âˆš derive a bound on I3 inâˆš(A.33). Let Î¸Ì„0 = Î¸0 âˆ’ J0 En [Ïˆ(W ; Î¸0 , Î·0 )]. Then Î¸Ì„0 âˆ’
Î¸0  = OPN (1/ n) = oPN (Ï„n ) as EPN [ nEn [Ïˆ(W ; Î¸0 , Î·0 )]] is bounded and the singular values
of J0 are bounded below from zero by Assumption 3.3(d). Therefore, Î¸Ì„0 âˆˆ with PN -probability
1 âˆ’ o(1) by Assumption 3.3(a). Hence, with the same probability,
âˆš
âˆš
inf nEn [Ïˆ(W ; Î¸, Î·Ì‚0 )] â‰¤ nEn [Ïˆ(W ; Î¸Ì„0 , Î·Ì‚0 )],
Î¸âˆˆ

and so it suffices to show that with PN -probability 1 âˆ’ o(1),
âˆš
âˆš
âˆš
nEn [Ïˆ(W ; Î¸Ì„0 , Î·Ì‚0 )] = O(nâˆ’1/2+1/q log n + rN log1/2 (1/rN ) + Î»N n + Î»N n).
Î·0 into (A.34) and use the Taylor expansion in (A.35). This
To prove it, substitute Î¸ = Î¸Ì„0 and Î· = 
shows that with PN -probability 1 âˆ’ o(1),
âˆš
âˆš
âˆš
nEn [Ïˆ(W ; Î¸Ì„0 , Î·Ì‚0 )] â‰¤ nEn [Ïˆ(W ; Î¸0 , Î·0 )] + J0 (Î¸Ì„0 âˆ’ Î¸0 ) + Î»N n + I4 + I5
âˆš
= Î»N n + I4 + I5 .
Combining this with the bounds on I4 and I5 derived above gives the claim of this step and
completes the proof of the lemma.

Proof of Theorems 4.1 and 4.2: Because Theorem 4.1 is a special case of Theorem 4.2 (with Z = D), it
suffices to prove the latter. Also, we only consider the DML estimators based on the score (4.7) and note
that the estimators based on the score (4.8) can be treated similarly.
Observe that the score Ïˆ in (4.7) is linear in Î¸ :
Ïˆ(W ; Î¸, Î·) = (Y âˆ’ DÎ¸ âˆ’ g(X))(Z âˆ’ m(X)) = Ïˆ a (W ; Î·)Î¸ + Ïˆ b (W ; Î·);
Ïˆ a (W ; Î·) = D(m(X) âˆ’ Z),

Ïˆ b (W ; Î·) = (Y âˆ’ g(X))(Z âˆ’ m(X)).

C

2017 Royal Economic Society.

Double/debiased machine learning

C63

Therefore, all asserted claims of Theorem 4.2 follow from Theorems 3.1 and 3.2 and Corollary 3.1 as
long as we can verify Assumptions 3.1 and 3.2, which we do here. We do so with TN being the set of all
Î· = (g, m) consisting of P -square-integrable functions g and m such that
Î· âˆ’ Î·0 P ,q â‰¤ C,
Î· âˆ’ Î·0 P ,2 â‰¤ Î´N ,
m âˆ’ m0 P ,2 Ã— g âˆ’ g0 P ,2 â‰¤ Î´N N âˆ’1/2 .
Also, we replace the
âˆš constant q and the sequence (Î´N )Nâ‰¥1 in Assumptions 3.1 and 3.2 by q/2 and (Î´N )Nâ‰¥1
with Î´N = (C + 2 C + 2)(Î´N âˆ¨ N âˆ’((1âˆ’4/q)âˆ§(1/2)) ) for all N (recall that we assume that q > 4, and the
analysis in Section 3 only requires that q > 2; also, Î´N satisfies Î´N â‰¥ N âˆ’1/2 , which is required in Theorems
3.1 and 3.2). We proceed in five steps. All bounds in the proof hold uniformly over P âˆˆ P but we omit this
qualifier for brevity).
S TEP 1. We first verify Neyman orthogonality. We have that EP [Ïˆ(W ; Î¸0 , Î·0 )] = 0 by definition
of Î¸0 of Î·0 . Also, for any Î· = (g, m) âˆˆ TN , the Gateaux derivative in the direction Î· âˆ’ Î·0 = (g âˆ’
g0 , m âˆ’ m0 ) is given by
âˆ‚Î· EP [Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] = EP [(g(X) âˆ’ g0 (X))(m0 (X) âˆ’ Z)]
+EP [(m0 (X) âˆ’ m(X))(Y âˆ’ DÎ¸0 âˆ’ g0 (X))] = 0,
by the law of iterated expectations, as V = Z âˆ’ m0 (X) and U = (Y âˆ’ DÎ¸0 âˆ’ g0 (X)) obey
EP [V |X] = 0 and EP [U |Z, X] = 0. This gives Assumption 3.1(d) with Î»N = 0.
S TEP 2. Note that
|J0 | = |EP [Ïˆ a (W ; Î·0 )]| = |EP [D(m0 (X) âˆ’ Z)]| = |EP [DV ]| â‰¥ c > 0
by Assumption 4.2(c). In addition,
|EP [Ïˆ a (W ; Î·0 )]| = |EP [D(m0 (X) âˆ’ Z)]| â‰¤ DP ,2 m0 (X)P ,2 + DP ,2 ZP ,2
â‰¤ 2DP ,2 ZP ,2 â‰¤ 2DP ,q ZP ,q â‰¤ 2C 2
by the triangle inequality, the HoÌˆlder inequality, the Jensen inequality and Assumption 4.2(b). This
gives Assumption 3.1(e). Hence, given that Assumptions 3.1(a)â€“(c) hold trivially, Steps 1 and 2
together show that all conditions of Assumption 3.1 hold.
S TEP 3. Note that Assumption 3.2(a) holds by construction of the set TN and Assumption 4.2(e).
Also, note that Ïˆ(W ; Î¸0 , Î·0 ) = U V , and so
EP [Ïˆ(W ; Î¸0 , Î·0 )Ïˆ(W ; Î¸0 , Î·0 ) ] = EP [U 2 V 2 ] â‰¥ c4 > 0,
by Assumption 4.2(c), which gives Assumption 3.2(d).
S TEP 4. Here we verify Assumption 3.2(b). For any Î· = (g, m) âˆˆ TN , we have
(EP [Ïˆ a (W ; Î·)q/2 ])2/q = Ïˆ a (W ; Î·)P ,q/2 = D(m(X) âˆ’ Z)P ,q/2
â‰¤ D(m(X) âˆ’ m0 (X))P ,q/2 + Dm0 (X)P ,q/2 + DZP ,q/2
â‰¤ DP ,q m(X) âˆ’ m0 (X)P ,q + DP ,q m0 (X)P ,q + DP ,q ZP ,q
â‰¤ CDP ,q + 2DP ,q ZP ,q â‰¤ 3C 2
by Assumption 4.2(b), which gives the bound on mN in Assumption 3.2(b). Also, because
|EP [(D âˆ’ r0 (X))(Z âˆ’ m0 (X))]| = |EP [DV ]| â‰¥ c

C

2017 Royal Economic Society.

C64

V. Chernozhukov et al.
by Assumption 4.2(c), it follows that Î¸0 satisfies
|Î¸0 | =

|EP [(Y âˆ’ 0 (X))(Z âˆ’ m0 (X))]|
|EP [(D âˆ’ r0 (X))(Z âˆ’ m0 (X))]|

â‰¤ câˆ’1 (Y P ,2 + 0 (X)P ,2 )(ZP ,2 + m0 (X)P ,2 )
â‰¤ 4câˆ’1 Y P ,2 ZP ,2 â‰¤ 4C 2 /c.
Hence,
(EP [Ïˆ(W ; Î¸0 , Î·)q/2 ])2/q = Ïˆ(W ; Î¸0 , Î·)P ,q/2
= (Y âˆ’ DÎ¸0 âˆ’ g(X))(Z âˆ’ m(X))P ,q/2
â‰¤ U (Z âˆ’ m(X))P ,q/2 + (g(X) âˆ’ g0 (X))(Z âˆ’ m(X))P ,q/2
â‰¤ U P ,q Z âˆ’ m(X)P ,q + g(X) âˆ’ g0 (X)P ,q Z âˆ’ m(X)P ,q
â‰¤ (U P ,q + C)Z âˆ’ m(X)P ,q
â‰¤ (Y âˆ’ DÎ¸0 P ,q + g0 (X)P ,q + C)
Ã—(ZP ,q + m0 (X)P ,q + m(X) âˆ’ m0 (X)P ,q )
â‰¤ (2Y âˆ’ DÎ¸0 P ,q + C)(2ZP ,q + C)
â‰¤ (2Y P ,q + 2DP ,q |Î¸0 | + C)(2ZP ,q + C)
â‰¤ 3C(3C + 8C 3 /c),
where we used the fact that because g0 (X) = EP [Y âˆ’ DÎ¸0 | X], g0 (X)P ,q â‰¤ Y âˆ’ DÎ¸0 P ,q by
the Jensen inequality. This gives the bound on mN in Assumption 3.2(b). Hence, Assumption
3.2(b) holds.
S TEP 5. Finally, we verify Assumption 3.2(c). For any Î· = (g, m) âˆˆ TN , we have
EP [Ïˆ a (W ; Î·)] âˆ’ EP [Ïˆ a (W ; Î·0 )] = |EP [Ïˆ a (W ; Î·) âˆ’ Ïˆ a (W ; Î·0 )]|
= |EP [D(m(X) âˆ’ m0 (X))]|
â‰¤ DP ,2 m(X) âˆ’ m0 (X)P ,2 â‰¤ CÎ´N â‰¤ Î´N ,
which gives the bound on rN in Assumption 3.2(c). Further,
(EP [Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 ])1/2 = Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )P ,2
= (U + g0 (X) âˆ’ g(X))(Z âˆ’ m(X)) âˆ’ U (Z âˆ’ m0 (X))P ,2
â‰¤ U (m(X) âˆ’ m0 (X))P ,2 + (g(X) âˆ’ g0 (X))(Z âˆ’ m(X))P ,2
âˆš
â‰¤ Cm(X) âˆ’ m0 (X)P ,2 + V (g(X) âˆ’ g0 (X))P ,2
+(g(X) âˆ’ g0 (X))(m(X) âˆ’ m0 (X))P ,2
âˆš
âˆš
â‰¤ Cm(X) âˆ’ m0 (X)P ,2 + Cg(X) âˆ’ g0 (X)P ,2
+g(X) âˆ’ g0 (X)P ,2 Ã— m(X) âˆ’ m0 (X)P ,2
âˆš
âˆš
â‰¤ (2 C + N âˆ’1/2 )Î´N â‰¤ (2 C + 1)Î´N â‰¤ Î´N ,

C

2017 Royal Economic Society.

C65

Double/debiased machine learning
which gives the bound on rN in Assumption 3.2(c). Finally, let
f (r) := EP [Ïˆ(W ; Î¸0 , Î·0 + r(Î· âˆ’ Î·0 )],

r âˆˆ (0, 1).

Then, for any r âˆˆ (0, 1),
f (r) = EP [(U âˆ’ r(g(X) âˆ’ g0 (X)))(V âˆ’ r(m(X) âˆ’ m0 (X)))],
and so
âˆ‚f (r) = âˆ’EP [(g(X) âˆ’ g0 (X))(V âˆ’ r(m(X) âˆ’ m0 (X)))]
âˆ’EP [(U âˆ’ r(g(X) âˆ’ g0 (X)))(m(X) âˆ’ m0 (X))],
âˆ‚ f (r) = 2EP [(g(X) âˆ’ g0 (X))(m(X) âˆ’ m0 (X))].
2

Hence,
|âˆ‚ 2 f (r)| â‰¤ 2g(X) âˆ’ g0 (X)P ,2 Ã— m(X) âˆ’ m0 (X)P ,2 â‰¤ 2Î´N N âˆ’1/2 â‰¤ Î´N N âˆ’1/2 ,
which gives the bound on Î»N in Assumption 3.2(c). Thus, all conditions of Assumptions 3.1 are
verified. This completes the proof.

Proof of Theorems 5.1 and 5.2: The proof of Theorem 5.2 is similar to that of Theorem 5.1 and is therefore
omitted. In turn, regarding Theorem 5.1, we show the proof for the case of ATE and note that the proof for
the case of ATTE is similar.
Observe that the score Ïˆ in (5.3) is linear in Î¸:
Ïˆ(W ; Î¸, Î·) = Ïˆ a (W ; Î·)Î¸ + Ïˆ b (W ; Î·),
Ïˆ b (W ; Î·) = (g(1, X) âˆ’ g(0, X)) +

Ïˆ a (W ; Î·) = âˆ’1,

D(Y âˆ’ g(1, X)) (1 âˆ’ D)(Y âˆ’ g(0, X))
âˆ’
.
m(X)
1 âˆ’ m(X)

Therefore, all asserted claims of Theorem 5.1 follow from Theorems 3.1 and 3.2 and Corollary 3.1 as
long as we can verify Assumptions 3.1 and 3.2, which we do here. We do so with TN being the set of all
Î· = (g, m) consisting of P -square-integrable functions g and m such that
Î· âˆ’ Î·0 P ,q â‰¤ C,
Î· âˆ’ Î·0 P ,2 â‰¤ Î´N ,
m âˆ’ 1/2P ,âˆž â‰¤ 1/2 âˆ’ Îµ;
m âˆ’ m0 P ,2 Ã— g âˆ’ g0 P ,2 â‰¤ Î´N N âˆ’1/2 .
Also, we replace the sequence (Î´N )Nâ‰¥1 in Assumptions 3.1 and 3.2 by (Î´N )Nâ‰¥1 with Î´N = CÎµ (Î´N âˆ¨
N âˆ’((1âˆ’4/q)âˆ§(1/2)) ) for all N , where CÎµ is a sufficiently large constant that depends only on Îµ and C (note
that Î´N satisfies Î´N â‰¥ N âˆ’((1âˆ’4/q)âˆ§(1/2)) , which is required in Theorems 3.1 and 3.2). We proceed in five steps.
All bounds in the proof hold uniformly over P âˆˆ P but we omit this qualifier for brevity.
S TEP 1. We first verify Neyman orthogonality. We have that E[Ïˆ(W ; Î¸0 , Î·0 )] = 0 by definition
of Î¸0 and Î·0 . Also, for any Î· = (g, m) âˆˆ TN , the Gateaux derivative in the direction Î· âˆ’ Î·0 =
(g âˆ’ g0 , m âˆ’ m0 ) is given by
âˆ‚Î· EP [Ïˆ(W ; Î¸0 , Î·0 )][Î· âˆ’ Î·0 ] = EP [g(1, X) âˆ’ g0 (1, X)] âˆ’ EP [g(0, X) âˆ’ g0 (0, X)]
 D(g(1, X) âˆ’ g (1, X)) 
 (1 âˆ’ D)(g(0, X) âˆ’ g (0, X)) 
0
0
+ EP
âˆ’EP
m0 (X)
1 âˆ’ m0 (X)

C

2017 Royal Economic Society.

C66

V. Chernozhukov et al.

âˆ’EP
âˆ’EP

 D(Y âˆ’ g (1, X))(m(X) âˆ’ m (X)) 
0
0
m20 (X)
 (1 âˆ’ D)(Y âˆ’ g (0, X))(m(X) âˆ’ m (X)) 
0

0

(1 âˆ’ m0 (X))2

,

which is 0 by the law of iterated expectations, as
EP [D | X] = m0 (X),
EP [D(Y âˆ’ g0 (1, X)) | X] = 0,

EP [1 âˆ’ D | X] = 1 âˆ’ m0 (X),
EP [(1 âˆ’ D)(Y âˆ’ g0 (0, X)) | X] = 0.

This gives Assumption 3.1(d) with Î»N = 0.
S TEP 2. Note that J0 = âˆ’1, and so Assumption 3.1(e) holds trivially. Hence, given that
Assumptions 3.1(a)â€“(c) hold trivially as well, Steps 1 and 2 together show that all conditions
of Assumption 3.1 hold.
S TEP 3. Note that Assumption 3.2(a) holds by construction of the set TN and Assumption 5.1(f).
Also,
EP [Ïˆ 2 (W ; Î¸0 , Î·0 )] = EP [EP [Ïˆ 2 (W ; Î¸0 , Î·0 ) | X]]
= EP EP [(g0 (1, X) âˆ’ g0 (0, X) âˆ’ Î¸0 )2 | X]

 D(Y âˆ’ g (1, X)) (1 âˆ’ D)(Y âˆ’ g (0, X)) 2

0
0
âˆ’
+ EP
|X
m0 (X)
1 âˆ’ m0 (X)
 D(Y âˆ’ g (1, X)) (1 âˆ’ D)(Y âˆ’ g (0, X)) 2 
0
0
âˆ’
â‰¥ EP
m0 (X)
1 âˆ’ m0 (X)
 D 2 (Y âˆ’ g (1, X))2
(1 âˆ’ D)2 (Y âˆ’ g0 (0, X))2 
0
= EP
+
2
m0 (X)
(1 âˆ’ m0 (X))2
â‰¥

1
EP [D 2 (Y âˆ’ g0 (1, X))2 + (1 âˆ’ D)2 (Y âˆ’ g0 (0, X))2 ]
(1 âˆ’ Îµ)2

=

1
c2
1
EP [DU 2 + (1 âˆ’ D)U 2 ] =
EP [U 2 ] â‰¥
.
2
2
(1 âˆ’ Îµ)
(1 âˆ’ Îµ)
(1 âˆ’ Îµ)2

This gives Assumption 3.2(d).
S TEP 4. Here we verify Assumption 3.2(b). We have
g0 (D, X)P ,q = (EP [|g0 (D, X)|q ])1/q
â‰¥ (EP [|g0 (1, X)|q P r P (D = 1 | X) + |g0 (0, X)|q P r P (D = 0 | X)])1/q
â‰¥ Îµ1/q (EP [|g0 (1, X)|q ] + EP [|g0 (0, X)|q ])1/q
â‰¥ Îµ1/q (EP [|g0 (1, X)|q ] âˆ¨ EP [|g0 (0, X)|q ])1/q
â‰¥ Îµ1/q (g0 (1, X)P ,q âˆ¨ g0 (0, X)P ,q ),
where in the third line, we used the facts that PrP (D = 1 | X) = m0 (X) â‰¥ Îµ and P r P (D = 0 |
X) = 1 âˆ’ m0 (X) â‰¥ Îµ. Hence, given that g0 (D, X)P ,q â‰¤ Y P ,q â‰¤ C by the Jensen inequality
and Assumption 5.1(b), it follows that
g0 (1, X)P ,q â‰¤ C/Îµ1/q

and

g0 (0, X)P ,q â‰¤ C/Îµ1/q .

and

g(0, X) âˆ’ g0 (0, X)P ,q â‰¤ C/Îµ1/q

Similarly, for any Î· âˆˆ (g, m) âˆˆ TN ,
g(1, X) âˆ’ g0 (1, X)P ,q â‰¤ C/Îµ1/q


C

2017 Royal Economic Society.

Double/debiased machine learning

C67

as g(D, X) âˆ’ g0 (D, X)P ,q â‰¤ C. In addition,
|Î¸0 | = |EP [g0 (1, X) âˆ’ g0 (0, X)]| â‰¤ g0 (1, X)P ,2 + g0 (0, X)P ,2 â‰¤ 2C/Îµ1/q .
Therefore, for any Î· = (g, m) âˆˆ TN , we have
(EP [|Ïˆ(W ; Î¸0 , Î·)|q ])1/q = Ïˆ(W ; Î¸0 , Î·)P ,q


â‰¤ (1 + Îµâˆ’1 ) g(1, X)P ,q + g(0, X)P ,q + 2Y P ,q /Îµ + |Î¸0 |


â‰¤ (1 + Îµâˆ’1 ) g(1, X) âˆ’ g0 (1, X)P ,q + g(0, X) âˆ’ g0 (0, X)P ,q


+(1 + Îµâˆ’1 ) g0 (1, X)P ,q + g0 (0, X)P ,q + 2C/Îµ + 2C/Îµ1/q
â‰¤ 4C(1 + Îµâˆ’1 )/Îµ1/q + 2C/Îµ + 2C/Îµ1/q .
This gives the bound on mN in Assumption 3.2(b). Also, we have
(EP [|Ïˆ a (W ; Î·)|q ])1/q = 1.
This gives the bound on mN in Assumption 3.2(b). Hence, Assumption 3.2(b) holds.
S TEP 5. Finally, we verify Assumption 3.2(c). For any Î· = (g, m) âˆˆ TN , we have
EP [Ïˆ a (W ; Î·) âˆ’ Ïˆ a (W ; Î·0 )] = |1 âˆ’ 1| = 0 â‰¤ Î´N ,
which gives the bound on rN in Assumption 3.2(c). Further, by the triangle inequality,
(EP [Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 ])1/2 = Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 ; Î·0 )P ,2
â‰¤ I1 + I2 + I3 ,
where
I1 := g(1, X) âˆ’ g0 (1, X)P ,2 + g(0, X) âˆ’ g0 (0, X)P ,2 ,
 D(Y âˆ’ g(1, X)) D(Y âˆ’ g (1, X)) 
0


âˆ’
I2 := 
 ,
P ,2
m(X)
m0 (X)
 (1 âˆ’ D)(Y âˆ’ g(0, X)) (1 âˆ’ D)(Y âˆ’ g (0, X)) 
0


âˆ’
I3 := 
 .
P ,2
1 âˆ’ m(X)
1 âˆ’ m0 (X)
To bound I1 , note that by the same argument as that used in Step 4,
g(1, X) âˆ’ g0 (1, X)P ,2 â‰¤ Î´N /Îµ1/2

and

g(0, X) âˆ’ g0 (0, X)P ,2 â‰¤ Î´N /Îµ1/2 , (A.38)

and so I1 â‰¤ 2Î´N /Îµ1/2 . To bound I2 , we have
I2 â‰¤ Îµâˆ’2 Dm0 (X)(Y âˆ’ g(1, X)) âˆ’ Dm(X)(Y âˆ’ g0 (1, X))P ,2
â‰¤ Îµâˆ’2 m0 (X)(g0 (1, X) + U âˆ’ g(1, X)) âˆ’ m(X)U P ,2
â‰¤ Îµâˆ’2 (m0 (X)(g(1, X) âˆ’ g0 (1, X))P ,2 + (m(X) âˆ’ m0 (X))U P ,2 )
âˆš
â‰¤ Îµâˆ’2 (g(1, X) âˆ’ g0 (1, X)P ,2 + Cm(X) âˆ’ m0 (X)P ,2 )
âˆš
â‰¤ Îµâˆ’2 (Îµâˆ’1/2 + C)Î´N .
Here, the first inequality follows from the bounds Îµ â‰¤ m0 (X) â‰¤ 1 âˆ’ Îµ and Îµ â‰¤ m(X) â‰¤ 1 âˆ’ Îµ,
the second from the facts that D âˆˆ {0, 1} and for D = 1, Y = g0 (1, X) + U , the third from the

C

2017 Royal Economic Society.

C68

V. Chernozhukov et al.
2
triangle inequality, the fourth from the facts
âˆš that m0 (X) â‰¤ 1 and EP [U | X] â‰¤ C, and the fifth
âˆ’2 âˆ’1/2
+ C)Î´N . Combining these inequalities shows that
from (A.38). Similarly, I3 â‰¤ Îµ (Îµ
âˆš
(EP [Ïˆ(W ; Î¸0 , Î·) âˆ’ Ïˆ(W ; Î¸0 , Î·0 )2 ])1/2 â‰¤ 2(Îµâˆ’1/2 + Îµâˆ’5/2 + CÎµâˆ’2 )Î´N â‰¤ Î´N ,
âˆš
as long as CÎµ in the definition of Î´N satisfies CÎµ â‰¥ 2(Îµâˆ’1/2 + Îµâˆ’5/2 + CÎµâˆ’2 ). This gives the bound
on rN in Assumption 3.2(c).
Finally, let

f (r) := EP [Ïˆ(W ; Î¸0 , Î·0 + r(Î· âˆ’ Î·0 ))],

r âˆˆ (0, 1).

Then for any r âˆˆ (0, 1),
 D(g(1, X) âˆ’ g (1, X))(m(X) âˆ’ m (X)) 
0
0
âˆ‚ 2 f (r) = EP
(m0 (X) + r(m(X) âˆ’ m0 (X)))2
 (1 âˆ’ D)(g(0, X) âˆ’ g (0, X))(m(X) âˆ’ m (X)) 
0
0
+EP
(1 âˆ’ m0 (X) âˆ’ r(m(X) âˆ’ m0 (X)))2
 (g(1, X) âˆ’ g (1, X))(m(X) âˆ’ m (X)) 
0
0
+EP
(m0 (X) + r(m(X) âˆ’ m0 (X)))2
 D(Y âˆ’ g (1, X) âˆ’ r(g(1, X) âˆ’ g (1, X)))(m(X) âˆ’ m (X))2 
0
0
0
+2EP
(m0 (X) + r(m(X) âˆ’ m0 (X)))3
 (g(0, X) âˆ’ g (0, X))(m(X) âˆ’ m (X)) 
0
0
+EP
(1 âˆ’ m0 (X) âˆ’ r(m(X) âˆ’ m0 (X)))2
 (1 âˆ’ D)(Y âˆ’ g (0, X) âˆ’ r(g(0, X) âˆ’ g (0, X)))(m(X) âˆ’ m (X))2 
0
0
0
,
âˆ’2EP
(1 âˆ’ m0 (X) âˆ’ r(m(X) âˆ’ m0 (X)))3
and so, given that
D(Y âˆ’ g0 (1, X)) = DU,
EP [U | D, X] = 0,

(1 âˆ’ D)(Y âˆ’ g0 (0, X)) = (1 âˆ’ D)U,
|m(X) âˆ’ m0 (X)| â‰¤ 2,

it follows that for some constant CÎµ that depends only on Îµ and C,
|âˆ‚ 2 f (r)| â‰¤ CÎµ m âˆ’ m0 P ,2 Ã— g âˆ’ g0 P ,2 â‰¤ Î´N N âˆ’1/2 ,
as long as the constant CÎµ in the definition of Î´N satisfies CÎµ â‰¥ CÎµ . This gives the bound on Î»N
in Assumption 3.2(c). Thus, all conditions of Assumptions 3.1 are verified. This completes the
proof.


SUPPORTING INFORMATION
Additional Supporting Information may be found in the online version of this article at the
publisherâ€™s website:
Replication files


C

2017 Royal Economic Society.

