R
Foundations and Trends
in
Econometrics
Vol. 4, No. 3 (2010) 165–224
c 2011 M. Lechner

DOI: 10.1561/0800000014

The Estimation of Causal Eﬀects by
Diﬀerence-in-Diﬀerence Methods
By Michael Lechner

Contents
1 Introduction

167

2 The History of DiD

170

3 Models, Eﬀects, and Identiﬁcation

174

3.1
3.2

174
175

Notation and Eﬀects
Identiﬁcation

4 Some Issues Concerning Estimation

204

4.1
4.2

204
204

Parametric Models
Semiparametric and Nonparametric Models

5 Some DID-speciﬁc Issues About Inference

208

6 Further Issues

210

6.1
6.2

Very Small Number of Treated and Control
Units/Interventions at the Aggregate Level
Additional Periods

210
211

7 Conclusions

214

A Technical Appendix

216

A.1 Propensity Score Property
A.2 Independence of Diﬀerences of Potential Outcomes Over
Time and Treatment

216
217

References

219

R
Foundations and Trends
in
Econometrics
Vol. 4, No. 3 (2010) 165–224
c 2011 M. Lechner

DOI: 10.1561/0800000014

The Estimation of Causal Eﬀects by
Diﬀerence-in-Diﬀerence Methods∗
Michael Lechner
Professor of Econometrics, Swiss Institute for Empirical Economic Research
(SEW), University of St. Gallen, Varnbüelstrasse 14, CH-9000 St. Gallen,
Switzerland, Michael.Lechner@unisg.ch

Abstract
This survey gives a brief overview of the literature on the diﬀerence-indiﬀerence (DiD) estimation strategy and discusses major issues using
a treatment eﬀects perspective. In this sense, this survey gives a somewhat diﬀerent view on DiD than the standard textbook discussion of
the DiD model, but it will not be as complete as the latter. It contains
some extensions of the literature, for example, a discussion of, and suggestions for nonlinear DiD estimators as well as DiD estimators based
on propensity-score type matching methods.
*I

am a Research Fellow of CEPR, London, CESifo, Munich, IAB, Nuremberg, IZA, Bonn,
and PSI, London. This project received ﬁnancial support from the St. Gallen Research
Centre of Aging, Welfare, and Labour Market Analysis (SCALA). The paper was presented at the econometrics breakfast at the University of St. Gallen and beneﬁted from
comments by seminar participants. I thank Christina Felfe, Alex Lefter, Conny Wunsch
as well as Hans Fricke and Susanne Zopf for carefully reading a previous draft of this
manuscript, Patrick Puhani for an interesting discussion on some issues related to limited
dependent outcome variables, and Jeﬀ Smith for additional useful references. The comments of a referee of this journal helped to improve the paper in several dimensions. The
usual disclaimer applies.

Keywords: Causal inference, counterfactual analysis, before-aftertreatment-control design, control group design with pretest
and posttest.
JEL Codes: C21, C23, C31, C33

1
Introduction

The Diﬀerence-in-Diﬀerence (DiD) approach is a research design for
estimating causal eﬀects. It is popular in empirical economics, for example, to estimate the eﬀects of certain policy interventions and policy
changes that do not aﬀect everybody at the same time and in the
same way. It is used in other social sciences as well.1 DiD could be
an attractive choice when using research designs based on controlling
for confounding variables or using instrumental variables is deemed
unsuitable, and at the same time, pre-treatment information is available.2 The DiD design is usually based on comparing de facto four
diﬀerent groups of objects. Three of these groups are not aﬀected by
the treatment. In many applications, “time” is an important variable to
1 In

other social sciences the DiD approach is also denoted as “untreated control group
design with independent pretest and posttest samples” or “control group design with
pretest and posttest.” See, for example, Cook and Campbell (1979), Rosenbaum (2001),
and Shadish et al. (2002) for further references.
2 Following the literature, the event for which we want to estimate the causal eﬀect is called
the treatment. The outcome denotes the variable that will be used to measure the eﬀect
of the treatment. Outcomes that would be realised if a speciﬁc treatment has, or would
have been applied, are called potential outcomes. A variable is called confounding if it is
related to the treatment and the potential outcomes. A variable is called an instrument if
it inﬂuences the treatment but not the potential outcomes.

167

168

Introduction

distinguish the groups.3 Besides the group which already received the
treatment (post-treatment treated), these groups are the treated prior
to their treatment (pre-treatment treated), the nontreated in the period
before the treatment occurs to the treated (pre-treatment nontreated),
and the nontreated in the current period (post-treatment nontreated).4
The idea of this empirical strategy is that if the two treated and the
two nontreated groups are subject to the same time trends, and if the
treatment has had no eﬀect in the pre-treatment period, then an estimate of the “eﬀect” of the treatment in a period in which it is known
to have none, can be used to remove the eﬀect of confounding factors
to which a comparison of post-treatment outcomes of treated and nontreated may be subject to. This is to say that we use the mean changes
of the outcome variables for the nontreated over time and add them to
the mean level of the outcome variable for the treated prior to treatment to obtain the mean outcome the treated would have experienced
if they had not been subjected to the treatment.
This survey presents a brief overview of the literature on the
diﬀerence-in-diﬀerence estimation strategy and discusses major issues
mainly using a treatment eﬀect perspective (and language) that allows,
in our opinion, more general considerations than the classical regression
formulation that still dominates the applied work. In this sense, this
survey might give a somewhat diﬀerent perspective than the standard
text book discussion of the DiD design, but it will not be as complete
as the latter. Thus, this review is more of a complement than a substitute to the excellent text type discussions of the DiD approach that
are already available in the literature (e.g., Angrist and Pischke, 2009;
Blundell and Costa Dias, 2009; Imbens and Wooldridge, 2009).
This review focuses on the case of only two diﬀerences although
the basic ideas of DiD estimation could be extended to more than

3 As

the concept of time is only used to deﬁne a group that is similar to the treated group
with respect to relevant unobservable variables and whose members have not (yet) participated, any other characteristic may be used instead of time as well, as long as the formal
conditions given below are fulﬁlled.
4 When a data set is available in which everybody is observed in all periods, there will be
just two groups with outcomes measured before and after the treatment.

169
two dimensions to create diﬀerence-in-diﬀerence-in-diﬀerence-in- . . .
estimators.5 However, the basic ideas of the approach of taking multiple diﬀerences are already apparent with two dimensions. Thus, we
refrain from addressing these higher dimensions to keep the discussion
as focused as possible.
The outline of this survey is as follows: The next section gives a
historical perspective and discusses some interesting applications. Section 3, which is the main part of this survey, discusses identiﬁcation
issues at length. Section 4 concerns DiD speciﬁc issues related to estimation, including a discussion of propensity score matching estimation
of DiD models. Section 5 discusses some speciﬁc issues related to inference, and Section 6 considers important practical extensions to the
basic approach. Section 7 concludes. Some short proofs are relegated
to a technical appendix.

5 For

example, Yelowitz (1995) analyzes the eﬀects of losing public health insurance on labor
market decisions in the United States by using Medicaid eligibility that varies over time,
state and age (of the child in the household). Another example for a triple diﬀerence is
the paper by Ravallion et al. (2005) who analyze the eﬀects of a social programme based
on a comparison of participants with nonparticipants and ex-participants.

2
The History of DiD

The method of DiD estimation is a well established econometric tool
and, although there are a couple of open issues, the main components
of this approach are well understood.1 The ﬁrst scientiﬁc study using
explicitly a DiD approach known to the author of this survey is the
study by Snow (1854).2 Snow (1855) was interested in the question
whether cholera was transmitted by (bad) air or (bad) water. He used a
change in the water supply in one district of London, namely the switch
from polluted water taken from the Themes in the centre of London
to a supply of cleaner water taken upriver. The basic idea of his study
is described by a quote from the introduction to (the reprint of) his
book by Ralph R. Frerichs: “ . . . Another investigation cited in his book
which drew praise for Snow was his recognition and analysis of a natural
1 Expositions

of this approach at an advanced textbook level are provided for example by
Meyer (1995), Angrist and Krueger (1999), Heckman et al. (1999), Angrist and Pischke
(2009), Blundell and Costa Dias (2009), and Imbens and Wooldridge (2009). For one of
the rather rare treatments of this topic in the statistics literature see Rosenbaum (2001).
2 It is also mentioned by Angrist and Pischke (2009). Besides this approach, Snow also used
“before-after” type of methods when analyzing a cholera outbreak related to one particular
water street-pump in Broad Street, London. See the short article by Snow (1854) himself
or the interesting account given by Lai (2011).

170

171
experiment involving two London water companies, one polluted with
cholera and the other not. He demonstrated that persons who received
contaminated water from the main river in London had much higher
death rates due to cholera. Most clever was his study of persons living
in certain neighborhoods supplied by both water companies, but who
did not know the source of their water. He used a simple salt test to
identify the water company supplying each home. This reduced misclassiﬁcation of exposure, and provided him with convincing evidence
of the link between impure water and disease.” Obviously, using close
neighborhoods is clever, as they are probably exposed to similar air
quality. It is worth adding that Snow also had data on death rates in
those neighborhoods prior to the switch of water supply. He used them
to correct his estimates for other features of these neighborhoods that
could have also lead to diﬀerential death rates. In that way, the ﬁrst
DiD estimate had an important impact for public health in a scientiﬁc
as well as in a very practical, life-saving way. Probably the most important reason for this impact was the high credibility of Snow’s (1855)
clever research design.
Later on, the DiD type of approach became relevant also for
other ﬁelds, like psychology, for example, Rose (1952) investigated the
eﬀects of a regime of ‘mandatory mediation’ on work stoppages by
a DiD design. The following quote from his article reveals the key
issues: “To test the eﬀectiveness of mandatory mediation in preventing work stoppages, it is necessary to make two simultaneous comparisons: (1) comparisons of states with the law to states without the law;
(2) comparisons of the former states before and after the law is put into
operation. The ﬁrst comparison can be achieved by taking percentages
of each of the three states to the total United States, for the measures
used. The second comparison can be achieved by setting the date of
the passage of the law at zero for each of the states. Figure 1 indicates
both comparisons simultaneously . . . .” (Rose, 1952, p. 191).
In economics, the basic idea of the DiD approach appeared early.
Obenauer and von der Nienburg (1915) analyzed the impact of a minimum wage by using an introduction of the minimum wage (in the retail
industry) in the state of Oregon that, for a particular group of employees, led to higher wage rates in Portland, the largest city, compared

172

The History of DiD

to the rest of the state. Therefore, the authors documented the levels
of various outcome variables for the diﬀerent groups of employees in
Portland before and after the introduction of the minimum wage and
compared the respective changes to those computed for Salem, which
is also located in Oregon and thought to be comparable to Portland.
Another early application in economics has been conducted by
Lester (1946). He was concerned with the eﬀects of wages on employment. He based his analysis on a survey of ﬁrms that had operations in
both the northern and the southern US states. His idea was to compare
employment levels, before and after various minimum wage rises, of
groups of ﬁrms with low average wages to groups of ﬁrms with higher
wage levels. The wage bills of the latter were naturally only mildly
aﬀected, if at all, by the rise in the minimum wage.
An important aspect of DiD estimation highlighted by these early
applications is that it does not require high powered computational
eﬀort to compute the basic DiD estimates, at least as long as further
covariates are not needed and no complicated inference methods are
used. This simplicity certainly makes some of the intuitive appeal of
DiD (and is also responsible for some of its weaknesses that will be
discussed below).
Over time the ﬁeld of economics developed a literature that, like
Rose (1952), uses changes in state laws and regulations to deﬁne pretreatment periods (prior to the introduction of the policy) and unaffected comparison groups (states having a diﬀerent policy than the one
of interest). One early example is the analysis of the price elasticity of
liquor sales which has been conducted by Simon (1966): “The essence
of the method is to examine the “before” and “after” sales of a given
state, sandwiched around a price change and standardized with the
sales ﬁgures of states that did not have a price change. The standardizing removes year-to-year ﬂuctuations and the trend. We then pool the
results of as many quasi-experimental “trial” events as are available.”
(Simon, 1966, p. 196).3 The question of the price reaction of liquor
3 Note

that in those times liquor prices were ﬁxed by the states. It is also interesting that
Simon (1966) relates this type of approach to the experimental literature, a relation that is
still frequently used: “This investigation uses a method that has features of both the cross
section and the time series. Though it has not been used by economists, to my knowledge,

173
sales has also been addressed in another important study by Cook and
Tauchen (1982) exploiting the state variation of the exercise tax for
liquor with a DiD approach.
Later on, DiD designs have been used to address many other important policy issues, like the eﬀects of minimum wages on employment
(e.g., Card and Krueger, 1994), the eﬀects of training and other active
labor market programmes for unemployed on labor market outcomes
(e.g., Ashenfelter, 1978; Ashenfelter and Card, 1985; Heckman and
Robb, 1986; Heckman and Hotz, 1989; Heckman et al., 1998; Blundell et al., 2004), the eﬀect of immigration on the local labor market
(e.g., Card, 1990), or the analysis of labor supply (e.g., Blundell et al.,
1998).
There is also a considerable literature analyzing various types of
labor market regulations with DiD designs. Meyer et al. (1995) consider
the eﬀect of workers injury compensation on injury related absenteeism.
Waldfogel (1998) looks at maternity leave regulation. Acemoglu and
Angrist (2001) investigate the eﬀects of the American with Disabilities
Act, and Besley and Burgess (2004) consider the impact of more or less
worker friendly labor regulations on growth in a developing country
(India), to mention only some important examples.4

it is very similar to designs used for experiments in psychology and the natural sciences
and to sociological paradigms. Because this method is not an experiment, though similar
to one, we call it the “quasi-experimental” method.” (Simon, 1966, p. 195). Economists
also use the term of a natural experiment (e.g., Meyer, 1995).
4 DiD estimates may also be the starting point (ﬁrst stage) of instrumental variable estimation strategies, like in the analysis of an Indonesian school construction programme by
Duﬂo (2001). These types of extensions of the DiD approach are however not the focus of
this partial survey.

3
Models, Eﬀects, and Identiﬁcation

3.1

Notation and Eﬀects

We start with a simple set-up to show the main ideas and problems of
DiD. The treatment variable, denoted by D, is binary, i.e., d ∈ {0, 1}.1
We have measurements of the various variables at most in two time periods, T , t ∈ {0, 1}. Period zero indicates a time period before the treatment (pre-treatment period) and period one indicates a time period
after the treatment took place (post-treatment period). Assuming that
the treatment happens between the two periods means that every member of the population is untreated in the pre-treatment period. We are
interested in discovering the mean eﬀect of switching D from zero to
one on some outcome variables. Therefore, we deﬁne “potential” outcome variables indexed by the potential states of the treatment, so
that Ytd denotes the outcome that would be realized for a speciﬁc value
of d in period t. The outcome that is realized (and thus observable) is
denoted by Y (not indexed by d). Finally, denote some further observable variables by X. They are assumed not to vary over time. Later on,
1 We

use the convention that capital letters denote random variables and small letters denote
speciﬁc values or realizations.

174

3.2 Identiﬁcation

175

several of the restrictions implied by this framework, like time constant
X and observing only two periods, will be relaxed. However, imposing them initially helps exposing the main ideas without unnecessary
complications.
Having deﬁned the notation, the mean treatment eﬀects can be
derived. In line with the literature on causal inference (see Imbens and
Wooldridge (2009), for a recent and comprehensive survey) we would
like to consider eﬀects for the treated, the nontreated, and the population at large, separately. However, it will be shown below that the latter
two are only identiﬁed under considerably stronger DiD assumptions
that are not attractive in typical DiD applications. Hence, their identiﬁcation is not an important topic in this survey. The average treatment
eﬀect on the treated in period t is deﬁned in the usual way:
ATET t = E(Yt1 − Yt0 |D = 1)






= E E(Yt1 − Yt0 |X = x, D = 1) |D = 1



θt (x)

= EX|D=1 θt (x).

(3.1)

ATETt denotes the so-called average treatment eﬀect on the treated
and θt (x) are the corresponding eﬀects in the respective subpopulations
deﬁned by the value of X being x.2

3.2

Identiﬁcation

Although the DiD approach is frequently used within the linear regression model, we start by studying the properties of this approach in a
nonparametric framework, which is common in the econometric literature on causal inference. Among many other virtues, it has the important advantage that the treatment eﬀects are naturally allowed to be
heterogeneous across the members of the population.
2 Recently,

Bonhomme and Sauder (2011) extended the DiD logic to the distribution of the
outcome variables using characteristic functions. Hence, their approach can be used to
recover quantile treatment eﬀects as well. For the sake of simplicity, the rest of this survey
sticks to mean eﬀects, however.

176

Models, Eﬀects, and Identiﬁcation

3.2.1

Identifying Assumptions in the Standard
Nonparametric Model

As mentioned before, the main idea of the DiD identiﬁcation strategy
is to compute the diﬀerence of the mean outcomes of treated and controls after the treatment and subtract the outcome diﬀerence that had
been there already before the treatment had any eﬀect (conditional
on a given value of x). If the assumptions formulated below hold, this
strategy will indeed identify a mean causal eﬀect.
To motivate the assumptions in this section, we use an
empirical example from the literature on active labor
market policy evaluation. For the sake of this example,
suppose that we are interested in estimating the earnings eﬀect of participation in a training programme for
the unemployed based on micro data containing information on the periods before and after training as well
as on programme participants and nonparticipants.
The ﬁrst assumption implies that one, and only one, of the potential
outcomes is indeed observable for every member of the population.
This assumption, sometimes called the observation rule, follows from
the so-called Stable Unit Treatment Value assumption (SUTVA; Rubin
(1977)). Importantly, it implies that the treatments are completely
represented and, in particular, that there are no relevant interactions
between the members of the population.
Yt = dYt1 + (1 − d)Yt0 ,

∀ t ∈ {0, 1}.

(SUTVA)

If SUTVA is violated, we observe neither of the two potential outcomes and conventional microeconometric evaluation strategies break
down.3
Example for a violation of SUTVA: If the training programme is very large, it may change the equilibrium
3 Manski

(2011) calls this assumption individualistic treatment response and analyzes identiﬁcation if it does not hold. Miguel and Kremer (2004) is an early study analyzing this
phenomenon of spill-over eﬀects or treatment externalities. They point out that in such
cases conducting experiments based on group randomization may be more adequate than
randomizing individuals.

3.2 Identiﬁcation

177

wages in the labor market by inﬂuencing skill-speciﬁc
demand and supply relations. For example, while some
unemployed are participating in the training courses, it
will be easier for the nonparticipants to ﬁnd a job than
without the existence of the programme. However, after
the programme, nonparticipants with skills comparable
to those obtained in the training programme will have
more diﬃculties in ﬁnding a job because the supply in
this skill group is now larger compared to the hypothetical world without the training programme. Thus, the
nonparticipants’ outcome is not the same outcome as
the one they would have experienced in a world without
the programme. Therefore, SUTVA is violated. Clearly,
SUTVA is more relevant for period one than for period
zero, but even in period zero it may play a role. An
example for this is that individuals anticipating their
future programme participation reduce their job search
eﬀorts. Thus, it is easier for the searching future nonparticipants to ﬁnd a job.
The next assumption concerns the conditioning variables X because
the main behavioral assumptions are supposed to hold conditional on
some covariates X. To make sure that this conditioning does not destroy
identiﬁcation, it is assumed that the components of X are not inﬂuenced by the treatment. This assumption is called EXOG (exogeneity)
and formalized applying the potential outcome notation to the control
variables, X d 4 :
X 1 = X 0 = X,

∀ x ∈ χ.

(EXOG)

χ denotes the subspace of X that is of interest to the researcher.
Examples for violations of EXOG: It is particularly
likely that variables that are measured after the treatment is known (like post-treatment job satisfaction)
4 Lechner

(2008a) shows that this assumption is too strong as one needs only to rule out
that any inﬂuence of D on X does not aﬀect the potential outcomes. Nevertheless, we
keep it here for convenience.

178

Models, Eﬀects, and Identiﬁcation

may be inﬂuenced by the treatment. Measuring variables prior to the treatment, however, does not automatically ensure exogeneity: individuals may anticipate
the treatment and change behavior accordingly. If they
do this in any way that impacts also the outcome variables, endogeneity of such control variables is a problem. Note that given our overall set of assumptions,
variables that cannot change over time are exogenous
by construction, because we consider a time varying
treatment. The problem with conditioning on a variable
inﬂuenced by the treatment can easily be seen with an
extreme example. Suppose the observed outcome variable Y1 would be included among the control variables,
then by construction θ1 (y1 ) = 0. In other words, conditioning on an endogenous variable is like estimating
only that part of the causal eﬀect that is not already
captured by the particular endogenous variable.
While SUTVA and EXOG are standard assumptions in microeconometric causal studies, the assumption that in the pre-treatment period
the treatment had no eﬀect on the pre-treatment population (NEPT)
is speciﬁc to the DiD.
θ0 (x) = 0;

∀ x ∈ χ.

(N EP T )

NEPT also rules out behavioral changes of the treated that inﬂuence
their pre-treatment outcome in anticipation of a future treatment.5
Example: This is very similar to the exogeneity condition but now applied to the pre-treatment outcomes
instead of the covariates. In the training example when
5 Note

that the observation rule (SUTVA) does not exclude the possibility that the treatment has an eﬀect before it starts (anticipation), because we observe the treatment outcome of the treated before and after the treatment. Hence, in this review we separate
SUTVA from the assumption that the treatment has no eﬀect in period zero. Some reviews
combine these assumptions by deﬁning the observation rule in a way such that in period
zero we always observe the nonparticipation outcome. In that case, explicitly assuming
NEPT is redundant as NEPT is already implied by this type of observation rule.

3.2 Identiﬁcation

179

the outcome of interest is unemployment, NEPT would
be violated if individuals decided not to search for a job
because they know (or plausibly anticipate in a way not
captured by X) that they will participate in an attractive training programme.
Next, we state the deﬁning assumptions for the DiD approach,
namely the “common trend” (CT) and “bias stability” (BS) assumptions. The common trend assumption is given by the following
expression:
E(Y10 |X = x, D = 1) − E(Y00 |X = x, D = 1)
= E(Y10 |X = x, D = 0) − E(Y00 |X = x, D = 0)
= E(Y10 |X = x) − E(Y00 |X = x);

(CT )

∀ x ∈ χ.

This assumption states that the diﬀerences in the expected potential
nontreatment outcomes over time (conditional on X) are unrelated to
belonging to the treated or control group in the post-treatment period.
This is the key assumption of the DiD approach. It implies that if the
treated had not been subjected to the treatment, both subpopulations
deﬁned by D = 1 and D = 0 would have experienced the same time
trends conditional on X. Thus, this also implies that the covariates
X should be selected such that they capture all variables that would
lead to diﬀerential time trends (in other words, select control variables
for which the time trends of the nonparticipation outcome diﬀer for
diﬀerent values of X, and at the same time for which the distribution of X diﬀers between the treated and the controls). The common
trend assumption already gives the intuition of the identiﬁcation proof
below. As the nontreatment potential outcomes share the same trend
for treated and nontreated, any deviation of the trend of the observed
outcomes of the treated from the trend of the observed outcomes of the
nontreated will be directly attributed to the eﬀect of the treatment and
not to diﬀerences in other characteristics of the treatment and control
group.
Example for a violation of the common trend assumption: Suppose that unemployed individuals from shrinking sectors are particularly likely to be admitted into

180

Models, Eﬀects, and Identiﬁcation

the training programme. Thus, unemployed who worked
in such declining sectors are overrepresented in the
group of programme participants. As these workers possess sector speciﬁc skills, the reemployment chances of
unemployed from declining sectors (sectors that continuously reduce their demand for labor) are likely
to deteriorate faster than the reemployment chances
of unemployed searching jobs in sectors in which the
demand for labor increases over time. Since the respective shares of these types of unemployed diﬀer in the
treated and control groups, the common trend assumption is violated unconditionally. However, it may hold
if the sector of the last employment is used as a control
variable.
As another example, it has been observed by Ashenfelter (1978) that trainees from public training programmes suﬀer a larger drop in earnings prior to
training than nontrainees. Suppose these drops are due
to negative “idiosyncratic temporary shocks.” The temporary nature of these shocks implies that individuals
who received the shock will recover faster than other
individuals once the eﬀect of the shock disappears. This
reaction directly violates the common trend assumption (see, for example, the exposition of this problem in
Heckman et al. (1999), or in Blundell and Costa Dias
(2009)).
Alternatively one can see the intuition behind the DiD approach by
considering the possibility of estimating the eﬀects of D in both periods
while (falsely) pretending that a selection-on-observables assumption
would be correct conditional on X. If NEPT is true, then a nonzero
eﬀect in the estimation of the eﬀects of D on Y0 (in the pre-treatment
period) implies that the estimator is biased and inconsistent and the
selection-on-observables assumption is implausible. If (and only if) this
bias is constant over time, it can be used to correct the estimate of
the eﬀect of D on Y1 , i.e., in the post-treatment period, which is the

3.2 Identiﬁcation

181

eﬀect we are interested in (e.g., Heckman et al., 1998). Therefore, the
assumption corresponding to this intuition may be called “constant
bias” assumption (CB) and is formalized by:
E[Y00 |X = x, D = 1] − E[Y00 |X = x, D = 0] [= Bias 0 (x)]
= E[Y10 |X = x, D = 1] − E[Y10 |X = x, D = 0] [= Bias 1 (x)],

∀ x ∈ χ.
(3.2)

By simple rewriting of the CT and CB assumptions we see that
they are identical:
Bias 1 (x) − Bias 0 (x)
= [E(Y10 |X = x, D = 1) − E(Y10 |X = x, D = 0)]
− [E(Y00 |X = x, D = 1) − E(Y00 |X = x, D = 0)]
= [E(Y10 |X = x, D = 1) − E(Y00 |X = x, D = 1)]
− [E(Y10 |X = x, D = 0) − E(Y00 |X = x, D = 0)].

(3.3)

From these assumptions it is obvious that identiﬁcation relies on the
counterfactual diﬀerence E(Y10 |X = x, D = 1) − E(Y00 |X = x, D = 1)
being identical to the observable diﬀerence E(Y1 |X = x, D = 0)−
E(Y0 |X = x, D = 0). Therefore, it is necessary that observations with
characteristics x exist in all four subsamples. This is guaranteed by the
so-called common support assumption:
P[T D = 1|X = x, (T, D) ∈ {(t, d), (1, 1)}] < 1;
∀(t, d) ∈ {(0, 1), (0, 0), (1, 0)};

∀x ∈ χ.

(COSU )

Example of a violation of COSU: If participation in
a training programme was compulsory for unemployed
below 25, and unemployed below 25 years were subject
to a diﬀerent trend than unemployed above 25 years
(so that this agecut-oﬀ is required as conditioning variable to make the common trend assumption plausible),
then the common support assumption would be violated
because there would not be any nonparticipants of age
25 or younger.

182

Models, Eﬀects, and Identiﬁcation

This assumption is formulated in terms of observable quantities
and is thus testable. All the other (identifying) assumptions mentioned
above are formulated in terms of unobservable random variables and
are thus not testable. If common support did not hold for all values
of X, a common practice would be to redeﬁne the population for which
we estimate the average treatment eﬀects of interest to those treated
types, deﬁned by the values of X(χ), that are observable in all four
subpopulations. Alternatively, one may has to be satisﬁed with partial
identiﬁcation of the original parameter.6
3.2.2

Proof of Identiﬁcation of the Average
Eﬀect on the Treated

Although the proof of identiﬁcation is straightforward and available in
the literature, because of its instructive nature it is repeated below.
First, note that once the conditional-on-X eﬀects, θ1 (x), are identiﬁed for all relevant values of x, ATET 1 is identiﬁed as well (ATET 0 is
zero because of the NEPT assumption). This property holds because
of the common support assumption implying that X has support in all
four subpopulations deﬁned by the diﬀerent values of D and T . Hence,
the identiﬁcation proof shows identiﬁcation of θ1 (x) only.
θ1 (x) = E(Y11 − Y10 |X = x, D = 1)
SUTVA

=

E(Y1 |X = x, D = 1) −E(Y10 |X = x, D = 1);



identiﬁed
CT

E(Y10 |X = x, D = 1) = E(Y10 |X = x, D = 0)
− E(Y00 |X = x, D = 0) + E(Y00 |X = x, D = 1)
SU T V A

=

E(Y1 |X = x, D = 0) − E(Y0 |X = x, D = 0)



identiﬁed

+ E(Y00 |X = x, D = 1);
6 See

Lechner (2008b) for such a bounding strategy in the case of matching. This strategy
could be directly transferred to the context of DiD estimation.

3.2 Identiﬁcation

183

N EP T

E(Y00 |X = x, D = 1) = E(Y01 |X = x, D = 1)
SU T V A

=

E(Y0 |X = x, D = 1) .



identiﬁed

Putting all pieces together, we get:
θ1 (x) = [E(Y1 |X = x, D = 1) − E(Y0 |X = x, D = 1)]
− [E(Y1 |X = x, D = 0) − E(Y0 |X = x, D = 0)].
Since θ1 (x) is a function of random variables for which realizations
are observable, it is identiﬁed. Aggregating the conditional eﬀects with
respect to the appropriate distribution of X in the group of the treated
in the post-treatment period leads to the desired average treatment
eﬀect on the treated.
The interpretation of the identiﬁcation of the counterfactual nontreatment outcome is obvious: We use the pre-treatment outcome of
the participants to infer the level of the nontreatment outcome and
then infer the change of that potential outcome that would occur from
period zero to period one from the change we actually observe for the
nonparticipants.
Note that the assumptions imposed above rule out that the composition of the group of nontreated is aﬀected by the treatment outcomes
(this is mentioned, for example, by Angrist and Pischke (2009), as one
of the common pitfalls with DiD estimation in practice).

3.2.3

Identiﬁcation of the Average Eﬀects on the
Population in General and the Nontreated

To be able to identify the average eﬀect for the nontreated as well,
ATENT t = E(Yt1 − Yt0 |D = 0), and thus also to be able to identify
the mean eﬀect for the population, ATE t = E(Yt1 − Yt0 ) = ATENT t ×
P (D = 0) + ATET t × P (D = 1), it is required to express another
counterfactual, namely E(Y11 |X = x, D = 0), in terms of observables
using DiD type of assumptions. In this case, the common trend assumption would have to involve the potential outcomes when treated and

184

Models, Eﬀects, and Identiﬁcation

could be formalized in the following form:
E(Y11 |X = x, D = 1) − E(Y01 |X = x, D = 1)
= E(Y11 |X = x, D = 0) − E(Y01 |X = x, D = 0) ∀ x ∈ χ.
This (technical) condition could of course be assumed (together
with some further generalisations of the assumptions made in Section 3.2.1). However, to use the same ideas as for the average treatment
eﬀect on the treated, we would need a subpopulation that is treated in
period 0 and somehow become untreated later on. Correctly speaking,
“becoming untreated” means that the eﬀect of the treatment vanishes
in period one. Such a scenario is unlikely to be plausible in most
economic applications. Thus, empirical papers using DiD almost
always do neither attempt to identify the ATE nor to identify ATET
and ATENT together.
3.2.4

The Scale Dependence of the Identifying Assumptions

As mentioned before, it has been observed by several authors, e.g.,
Meyer et al. (1995), that the identifying assumptions in the DiD framework are scale dependent, i.e., if they hold for the level of Y , they may
not hold for monotone transformations of Y . In other words, the way
how we measure and transform the outcome variable is relevant for the
plausibility of the identifying assumptions, even without postulating
any parametric model for the relation of confounders and treatment to
the outcomes. This is a feature that is not shared by other (nonparametric) identiﬁcation strategies like instrumental variables or matching.
Thus, we should call the DiD design a semiparametric identiﬁcation
strategy in contrast to the nonparametric identiﬁcation strategies.
This distinction can easily be seen by considering the following
example. Suppose that the potential nontreatment outcomes are lognormally distributed and that covariates play no role. Parameterize the
log-normal distribution in one of the following ways: (i) ln Yt0 |X = x,
D = d ∼ N (0, 2d + 2t); (ii) ln Yt0 |X = x, D = d, T = t ∼ N (d + t, 2); or
(iii) ln Yt0 |X = x, D = d, T = t ∼ N (d, d + 1), ∀d, t ∈ {0, 1}. In the ﬁrst
case, the log of the potential outcome has mean zero and is heteroscedastic. In the second case, it is homoscedastic, but its mean

3.2 Identiﬁcation

185

depends on group membership and time period. In the third example we shut down the trend and consider the stationary case. Consider
two choices of scale of Y (for example earnings) that are popular for
continuous variables: the level of the outcome variable (Y 0 ) as well as a
log transformation (ln Y 0 ). Next, we check whether the CT assumption
holds in these settings7 :
Case 1: ln Yt0 |D = d ∼ N (0, 2d + 2t)
E(ln Y10 |D = 1) − E(ln Y00 |D = 1) = 0,

 



0

0

E(ln Y10 |D


0) + E(ln Y00 |D

=



 



0

E(Y10 |D






0

=

1) − E(Y00 |D

=

0) + E(Y00 |D

e2

E(Y10 |D


= 0) = 0;


e1






= 1) = e(e − 1),






e1



e0 =1

= 0) = e − 1.


For Case 1, the common trend assumption holds for the logs but
not for the levels. Next, we consider Case 2:
Case 2: ln Yt0 |D = d ∼ N (d + t, 2)
E(ln Y10 |D = 1) − E(ln Y00 |D = 1) = 1,

 



2

1

E(ln Y10 |D




=

0) + E(ln Y00 |D
 



1

E(Y10 |D






0

=

1) − E(Y00 |D

=

0) + E(Y00 |D

e3

E(Y10 |D

e2

= 0) = 1;









e2


e1

= 1) = e2 (e − 1),

= 0) = e(e − 1).


Again, the common trend assumption holds for the log-speciﬁcation,
but not for the level-speciﬁcation. Another feature of the functional form
7 Note

2

2

2

that ln Y ∼ N (µ, σ 2 ) ⇒ Y ∼ N (eµ+0.5σ , (eσ − 1)(e2µ+σ )).

186

Models, Eﬀects, and Identiﬁcation

dependence is also apparent in the second example: if the conditional
mean would be t times d instead of t + d, then the common trend assumption would be violated for the log as well as for the level speciﬁcation.
Case 3: ln Yt0 |D = d ∼ N (2d, 2d + 2).
This case is indeed trivial. As neither the mean nor the variance
changes over time, the common trend assumption is automatically fulﬁlled for all transformations of Y .
This dependence of the validity of the identifying assumption on
the scale of measurement of the outcome variable is a disadvantage
of DiD, because the credibility of the “common trend” or “constant
bias” assumptions becomes functional form dependent. Identiﬁcation
by functional form is less attractive as the researcher very seldom has
access to credible information about the appropriate functional forms.
There is, however, another way of viewing this problem of functional
form (or scale of measurement) dependence: The appropriate functional
form of the outcome variable should follow from the parameter the
researcher is interested in. However, even when a sensible functional
form can be derived from the parameters of interest, it remains a credibility issue. Why should the CT or CB assumptions be plausible for
that particular choice of scaling, while they may be violated for other
choices, which might be equally plausible a priori?
3.2.5

The Changes-in-Changes Model by Athey
and Imbens (2006)

The functional form dependence explored in the previous section is the
starting point for the so-called “changes-in-changes model” that has
been proposed by Athey and Imbens (2006). The goal of their paper is
to state a set of DiD-like assumptions that are not scale dependent.
The CiC (“Changes-in-Changes”) model proposed by Athey and
Imbens (2006) assigns the idea of the DiD model to the distribution of the potential outcomes. The idea is to compare the cumulative distribution functions (cdfs) of the outcomes in the four groups.
Then the diﬀerence of the cdfs of the treated and the nontreated
group in the pre-programme period is used to predict the hypothetical nontreatment cumulative distribution function of the treated group

3.2 Identiﬁcation

187

in the post-treatment period if they were not treated. Comparing this
predicted cdfs to the observed cdfs of the treated in the post-treatment
period gives the desired eﬀect.
The key diﬀerence to the standard DiD approach is that the assumptions made as well as the information exploited for identiﬁcation and estimation comes from the whole outcome distribution and not just from
the ﬁrst moments. Estimation is based on estimating cdfs as well as their
inverses for each group deﬁned by treatment and time conditional on X
and predicting the counterfactual outcomes based on those functionals.
Although the√estimation problem is straightforward in principle (the
authors provide N -consistent and asymptotically normal estimators),
considerable problems may appear in practice either if the outcome
variables are not continuous or the types of individuals are too heterogeneous (based on the diﬀerent relevant values of x). In the ﬁrst
case, the problem is that the inversion of a cdf of a discrete random
variable is not unique. Therefore, only bounds of the true eﬀects are
identiﬁed (they are given in the paper). In the second case, when control variables are present, estimation either has to be performed within
cells deﬁned by the discrete values of those variables, or appropriate
smoothing techniques have to be used in the case of continuous variables (or discrete variables with many diﬀerent possible values). Both
issues are only of limited concern asymptotically. However, given the
usual dimensions of the control variables and sample sizes in applications, the curse of dimensionality could be a major concern for applied
researchers who may need to control for many variables to make the
common trend assumption suﬃciently credible. Perhaps for this reason
the number of empirical applications of this approach is limited so far.
The only published application known to the author (other than the
one provided by Athey and Imbens in their seminal paper) is the one
by Havnes and Mogstad (2010) who analyze the eﬀects of child care in
Norway. They are not only interested in mean impacts but also in the
eﬀects on the quantiles of the earnings distribution.
3.2.6

The Role of Covariates

As mentioned above, we need to control for precisely those exogenous
variables that lead to diﬀerential trends and that are not inﬂuenced

188

Models, Eﬀects, and Identiﬁcation

by the treatment. Including further control variables has positive and
negative aspects, even when these additional variables do not lead to
a violation of the DiD assumptions postulated above. On the positive
side, it could help to detect eﬀect heterogeneity that may be of substantive interest to the researcher (e.g., estimating the eﬀects for men and
women separately although men and women experience the same trends
for their potential outcomes). On the negative side, every additional
variable makes the common support assumption more diﬃcult to fulﬁl.
So far we discussed the case of time constant covariates. In many
studies, though, measurements in diﬀerent time periods may be available. In particular, in a repeated cross-section setting, it is likely that
the only available measurement of some covariates was taken at the
same time as the measurement of the outcome variables. The particular
concern here is the post-treatment period because time varying trendconfounding variables measured after the treatment are more likely to
be inﬂuenced by it. Thus, the exogeneity condition might be violated.
In this case, controlling for such time varying covariates leads to biased
estimates. Generally, time varying covariates are no problem if they
are exogenous. Indeed, if they are exogenous they may even be better
suited to remove trend-confounding than covariates that do not vary
over time (for example using age instead of birth year may be a superior choice to remove trend-confounding in some applications). If these
variables are endogenous and if anticipation eﬀects play no role, then
using pre-treatment measurements whenever available may be the best
empirical strategy.
Now, we change the perspective somewhat and look at the type
of circumstances under which an unobservable variable that inﬂuences
the potential outcomes can be safely ignored when estimating a DiD
model. To see this, we consider the impact of the excluded (perhaps
unobservable) time constant variable U in the simplest case without
covariates. Obviously, we need to require CT to hold unconditionally,
because U is unobserved. To see its role on the unconditional common
trend assumption, we use iterated expectations:
CT : EU |D=1 [E(Y10 |D = 1, U = u) − E(Y00 |D = 1, U = u)]
= EU |D=0 [E(Y10 |D = 0, U = u) − E(Y00 |D = 0, U = u)]. (3.4)

3.2 Identiﬁcation

189

Clearly, if the common trend assumption holds conditional on U (if
not, then there is no reason to consider using U as an additional control
variable), and if the distribution of U does not depend on the treatment status, then U can be safely ignored in the estimation without
violating CT.
Next, consider the case in which CT again holds conditional on U ,
but the distribution of U diﬀers for diﬀerent values of D. In this case,
if U cannot be used as covariate, further assumptions are required. One
such assumption is that the eﬀect of the unobservable variable on the
potential outcomes is constant over time (but may vary across treatment status). The following separable structure has such a property:
EU |D=d E(Yt0 |D = d, U = u) = E(Yt0 |D = d) + EU |D=d f d (U ),

(3.5)

where f d (U ) denotes some arbitrary function of U that may depend
on the selection into group d but not on time. By taking diﬀerences
over time, the term EU |D=d f d (U ) disappears. Thus, we can allow for
selection into treatment based on unobservable variables that also inﬂuence potential outcomes as long as their impact is constant over time.
This feature is the reason why in a linear parametric setting with panel
data DiD estimation and ﬁxed-eﬀects estimation is indeed very similar
and sometimes identical (see for example the discussion in Angrist and
Pischke (2009)).
3.2.7

The Relation of the DiD Assumptions to the
Matching Assumptions

Returning to the case of time constant covariates, next we consider
the relation of DiD to another closely related identiﬁcation and estimation approach, namely matching. An assumption that identiﬁes ATET
is that the expectation of the respective potential outcome does not
depend on the treatment status conditional on the covariates:
E(Y10 |X = x, D = 1) = E(Y10 |X = x, D = 0).

(3.6)

This assumption is implied by the set of assumptions that characterizes the matching approach to the identiﬁcation of causal eﬀects
(these assumptions are labelled as conditional independence, selection

190

Models, Eﬀects, and Identiﬁcation

on observables, and unconfoundedness assumptions). It is in fact
related to the DiD assumption that presumes that the diﬀerence of the
expectations of the potential outcomes over time does not depend on
the treatment status. Despite their similarity, neither of the assumption nests the other: the DiD assumption allows for some selection on
unobservables, which is ruled out in matching, while matching makes
no assumptions about the pre-treatment periods, which is required
in DiD. In applications based on matching estimation the conditional
independence assumption is frequently strengthened by assuming that
not only the mean, but the distribution of the potential outcomes
conditional on covariates is independent of the treatment status. This
stronger assumption has the virtue that it identiﬁes not only the
counterfactual expectations but the full counterfactual distribution.
Furthermore, it makes the identiﬁcation invariant to the chosen scaling
of Y . Although the same approach can be chosen in a DiD framework,
namely assuming that the diﬀerence of the potential outcomes over
time is independent of the treatment, Appendix A.2 shows that there
are no comparable gains for DiD estimation as there are for matching.
Hence, the “independence of the diﬀerences of the potential outcomes
over time” assumption is not that attractive because compared to
the common trend in means assumption it is more restrictive without
identifying further interesting quantities.8
3.2.8

Panel Data

So far in this exposition, it was not discussed whether the same individuals are observed in the pre- and post-treatment periods, because
all identiﬁcation results that are valid for repeated cross-sections also
hold for panel data. So even though individual pre-treatment–posttreatment diﬀerences of outcome variables can be computed with panel
data, the nature of the estimator is the same but its precision may
change.
One consequence of basing the estimator on individual diﬀerences
over time is that all inﬂuences of time constant confounding factors
8 See,

however, the alternative assumptions imposed by Athey and Imbens (2006) and Bonhomme and Sauder (2011) that identify distributional eﬀects.

3.2 Identiﬁcation

191

that are additively separable from the remaining part of the conditional
expectations of the potential outcomes are removed by the DiD-type
of diﬀerencing, as shown in the previous section. Therefore, it is not
surprising that adding ﬁxed individual eﬀects instead of the treatment
group dummy d in the regression formulation below (and all time constant covariates X), will lead to the same estimand (e.g., Angrist and
Pischke, 2009).
Furthermore, from the point of view of identiﬁcation, a substantial
advantage of panel data is that matching estimation based on conditioning on pre-treatment outcomes is feasible as well. This is an important issue because it appears to be a natural requirement for a “good”
comparison group to have similar pre-treatment means of the outcome
variables. This is not possible with repeated cross-sections since we
do not observe pre-treatment outcomes from the same individuals but
only from some group that is similar to the individuals obtaining the
treatment in terms of other observable characteristics X.
The corresponding matching-type assumption when lagged outcome
variables are available can be expressed as follows:
E(Y10 |Y0 = y0 , X = x, D = 1) = E(Y10 |Y0 = y0 , X = x, D = 0).

(3.7)

Imbens and Wooldridge (2009) observe that the common trend
assumption and this matching-type assumption impose diﬀerent identifying restrictions on the data which are not nested and must be rationalized based on substantive knowledge about the selection process,
i.e., only one of them can be true. Angrist and Krueger (1999) elaborate on this issue on the basis of regression models and come to the
same conclusions. The advantage of the DiD method is that it allows
for time constant confounding unobservables while requiring common
trends, whereas matching does not require common trends but assumes
that conditional on pre-treatment outcomes confounding unobservables
are irrelevant. Of course, one may argue that conditioning on the past
outcome variables already controls for the part of the unobservables
that manifested itself in the lagged outcome variables.
One may try to combine the good features of both methods by including pre-treatment outcomes among the covariates in a DiD framework.
This is however identical to matching: Taking the diﬀerence while keeping

192

Models, Eﬀects, and Identiﬁcation

the pre-treatment part of that diﬀerence constant at the individual level
in any comparison (i.e., the treated and matched control observations
have the same pre-treatment level) is equivalent to just ignoring the difference in DiD and to focus on the post-treatment variables alone. Thus,
such a procedure implicitly requires the matching assumptions.9 In other
words, assuming common trends conditional on the start of the trend
(which means it has to be the same starting point for treated and controls)
is practically identical to assuming no confounding (i.e., that the matching assumptions hold) conditional on past outcomes.
Thus, Imbens and Wooldridge’s (2009, p. 70) conclusion about the
usefulness of DiD in panel data compared to matching is negative: “As
a practical matter, the DiD approach appears less attractive than the
unconfoundedness-based approach in the context of panel data. It is
diﬃcult to see how making treated and control units comparable on
lagged outcomes will make the causal interpretation of their diﬀerence
less credible, as suggested by the DID assumptions.” However, a recent
paper by Chabé-Ferret (2010) gives several examples in which a DiD
strategy leads to a consistent estimator while matching conditional on
past outcomes may be biased. He also shows calibrations based on real
data suggesting that this bias may not be small.
3.2.9

The Regression Formulation

3.2.9.1

Derivation of the linear speciﬁcation

Most of the empirical applications employing the DiD identiﬁcation
strategy so far used the linear model. The linear regression formulation
can be motivated by the following assumptions about the conditional
expectations for the potential outcomes:
E(Yt1 |X = x, D = d) = α + tδ 1 + dγ + xβ + txλ1 + dxπ;
E(Yt0 |X = x, D = d) = α + tδ 0 + dγ + xβ + txλ0 + dxπ;
∀d ∈ {0, 1}, ∀x ∈ χ.
9 Although

(3.8)

the confounding individual eﬀect has been removed by taking diﬀerences, conditioning on the pre-treatment levels is like conditioning on it again and thus may induce a
correlation with D. In other words, if the DiD assumptions hold unconditionally on the pretreatment outcome, they are likely to be violated conditional on pre-treatment outcomes.

3.2 Identiﬁcation

193

The speciﬁcation is ﬂexible to some degree as it includes several
interaction terms between the control variables and group membership.
However, it does not include interactions between time and treatment
status as these interactions would violate the common trend assumption. This exclusion restriction gives rise to an interpretation of DiD
estimation as conditional (on X, D, and T ) IV estimation with the
interaction term D T acting as an instrument (with perfect compliance). This interpretation again points to the essentially parametric
nature of this approach as there cannot be independent variation of
this instrument given its components D and T . This is only possible if
a (semi-) parametric model will be speciﬁed in which the eﬀects of D1
and T are separable.
Note that some of the coeﬃcients do not vary across potential outcomes as a simple way to ensure that the treatment has no eﬀect in
period zero.10
The next step is to show that this speciﬁcation indeed fulﬁls the
common trend assumption:
E(Y10 |X = x, D = 1) − E(Y00 |X = x, D = 1)
= α + δ 0 + γ + xβ + xλ0 + xπ − α − γ − xβ − xπ = δ 0 + xλ0 ;
E(Y10 |X = x, D = 0) − E(Y00 |X = x, D = 0)
= α + δ 0 + xβ + xλ0 − α − xβ = δ 0 + xλ0 .
Again, these derivations clarify that having diﬀerential trends for
the diﬀerent potential outcomes is no problem as long as the trends do
not depend on the treatment status.
Starting from the speciﬁcations for the potential outcomes, the
eﬀects can be expressed in terms of their regression coeﬃcients:
θ1 (x) = E(Y1 |X = x, D = 1) − E(Y0 |X = x, D = 1)
− [E(Y1 |X = x, D = 0) − E(Y0 |X = x, D = 0)]
= (α + δ 1 + γ + xβ + xλ1 + xπ) − (α + γ + xβ + xπ)



δ 1 +xλ1
˜

10 In

˜

˜

˜

˜

a more general model, we would have E[Ytd |X = x, D = d] = αd + tδ d + dγ d + xβ d +
˜
˜
txλd + dxπ d and E[Y01 − Y00 |X = x, D = d] = (α1 − α0 ) + d(γ 1 − γ 0 ) + x[(β 1 − β 0 ) +
!

d(π 1 − π 0 )] = 0.

194

Models, Eﬀects, and Identiﬁcation

− [(α + δ 0 + xβ + xλ0 ) − (α + xβ)]



δ 0 +xλ0

= (δ 1 − δ 0 ) +x (λ1 − λ0 ) = δ + xλ.
  
  
δ

λ

Therefore, the task of regression estimation is to obtain consistent estimates of δ and λ. To do so, we derive the regression model for the
observed outcome variable.
The observation rule (SUTVA), Yt = dYt1 + (1 − d)Yt0 , leads to the
following speciﬁcation for the observed outcome:
E(Yt |X = x, D = d)
= E[dYt1 + (1 − d)Yt0 |X = x, D = d]
= dE(Yt1 |X = x, D = d) + (1 − d)E(Yt0 |X = x, D = d)
= d(α + tδ 1 + γ + xβ + txλ1 + xπ) + (1 − d)(α + tδ 0 + xβ + txλ0 )
= α + tδ 0 + xβ + txλ0
+ d(α + tδ 1 + γ + xβ + txλ1 + xπ − α − tδ 0 − xβ − txλ0 )


= α + tδ 0 + xβ + txλ0 + d γ + xπ + t (δ 1 − δ 0 ) +tx (λ1 − λ0 )
  
  
δ
0

0

= α + tδ + dγ + xβ + txλ + dxπ + dtδ + dtxλ;

λ

∀ d ∈ {0, 1}, x ∈ χ.

From these derivations, we see that a regression with group and
time dummies (so-called main eﬀects) plus the various interaction
terms identify the causal eﬀects. In such a regression, the coeﬃcients
of the interaction terms between time and treatment group capture the
eﬀects. It is rather a common practice to assume that the coeﬃcient
λ is zero, implying that the interaction of group and time with the
control variables disappears.
3.2.9.2

Advantages of the regression formulation

The advantage of the regression formulation of the DiD identiﬁcation
and estimation problem is the easiness of obtaining the ﬁnal estimates
and their standard errors (although even in this simple case there are

3.2 Identiﬁcation

195

some DiD speciﬁc inference problems that were discovered recently;
they are mentioned below). Furthermore, we can easily extend the
model to cover more periods and more treatments, including continuous treatments, and add additional covariates without much further
computational eﬀort.
3.2.9.3

Disadvantages of the regression formulation

There are also disadvantages of this regression-based approach to DiD.
They concern (i) the eﬀect heterogeneity that is allowed for, (ii) the way
how control variables are included, and (iii) the possibility of arriving at
estimates that are not plausible. It is important to note that those issues
only appear if covariates are included. If covariates are not included,
then estimation of the eﬀects in DiD designs using the linear regression
framework described above is fully nonparametric (in the sense that
it does not impose any further assumptions than the ones needed for
identiﬁcation, which were discussed in the previous section).
Let us consider the potential issues (restrictions implied by the
regression framework) in turn. First, for the issue of eﬀect heterogeneity consider the simpler case in which λ equals 0. If there is indeed
any eﬀect heterogeneity, it means that the true coeﬃcient δ is random
instead of being a constant. Since the regression estimation essentially
assumes a nonstochastic coeﬃcient, the stochastic deviation that cannot be captured in the regression becomes part of the error term. This
may not be harmful, if the heterogeneity is pure random noise uncorrelated with all variables included in the regression or if the model is fully
saturated in the controls variables (see Angrist and Pischke, 2009). The
regression coeﬃcient still captures the average eﬀect. However, if the
heterogeneity is related to those variables and the model is not fully
saturated in the covariates, then for example δ estimated by OLS is
inconsistent (and asymptotically biased) for the ATET.
Second, including the control variables in a linear fashion implies
the assumption of common trends conditional on the linear index, X β,
which is more restrictive than assuming common trends conditional
on X. Any deviation from the linear index is again absorbed in the
regression error term and may invalidate the estimates.

196

Models, Eﬀects, and Identiﬁcation

Finally, if the outcomes have limited support, such as a binary
variable, there is no guarantee that the predicted expected potential outcomes will respect this support condition. The latter is one of the reasons
why in practice linear models are rarely used in these cases and why nonlinear models, like logit or probit models for binary outcome variables, are
usually preferred. However, these nonlinear models come with their particular problems in a DiD setup, as will be explained below.
3.2.9.4

Nonlinear models with the standard
common trend assumption

There are many types of outcome variables for which it is common practice to use nonlinear models instead of linear ones, because they provide
a better approximation of the statistical properties of such random variables. Popular examples are probit, logit, tobit, count data models, and
duration models. The general arguments in favor of such models do not,
however, carry over to eﬀects identiﬁed by the DiD assumptions. When
applying nonlinear models in a DiD framework, researchers typically
use a linear index structure together with a nonlinear link function
(e.g., Hunt, 1995; Gruber and Poterba, 1994). The linear index structure is speciﬁed as if it would be a speciﬁcation for the linear regression
model. Then, the model is estimated and the estimated coeﬃcients, or
(average) marginal eﬀects, are interpreted causally. However, while the
linear regression can be derived from the DiD assumptions together
with some restrictions on functional forms, such rationalization is generally not possible for nonlinear models. To see this more clearly, let us
consider a nonlinear regression model in the same fashion as we have
analyzed the corresponding linear model.
We start with a “natural” nonlinear model with a linear index structure which is transformed by a link function, G(·), to yield the conditional expectation of the potential outcome. In the case of the probit
model, this link function would, for example, be the cdf of the standard
normal distribution:
E(Yt1 |X = x, D = d) = G(α + tδ 1 + dγ + xβ + txλ1 + dxπ);
E(Yt0 |X = x, D = d) = G(α + tδ 0 + dγ + xβ + txλ0 + dxπ);
∀ d ∈ {0, 1}, ∀ x ∈ χ.

(3.9)

3.2 Identiﬁcation

197

This speciﬁcation resembles the linear model with the exception
of the addition of the link function. It fulﬁls the NEPT assumption.
Using the observation rule and performing the same transformation
as for the linear model (inside the G(·)-function), we obtain the following speciﬁcation for the observable outcomes that can be used for
the econometric estimation of the model parameters (again, using the
notation as introduced for the linear model above):
E(Yt |X = x, D = d)
= E[dYt1 + (1 − d)Yt0 |X = x, D = d]
= dE(Yt1 |X = x, D = d) + (1 − d)E(Yt0 |X = x, D = d)
= G[d(α + tδ 1 + γ + xβ + txλ1 + xπ)
+ (1 − d)(α + tδ 0 + xβ + txλ0 )]
= ...
= G(α + tδ 0 + dγ + xβ + txλ0 + dxπ + dtδ + dtxλ)
∀ d ∈ {0, 1}, ∀x ∈ χ. (3.10)
This equation, sometimes speciﬁed with fewer interaction terms, is
the basis for the empirical analysis in papers using (standard) nonlinear
diﬀerence-in-diﬀerences.11
Of course, as for the linear model, we need to check whether such a
speciﬁcation indeed fulﬁls the common trend assumption:
˜

˜

E(Y1d |X = x, D = 1) − E(Y0d |X = x, D = 1)
˜

˜

= G(α + δ d + γ + x(β + λd + π)) − G(α + γ + x(β + π));
˜

˜

E(Y1d |X = x, D = 0) − E(Y0d |X = x, D = 0)
˜

˜

= G(α + δ d + x(β + λd )) − G(α + xβ);

∀d˜ ∈ {0, 1}.

It may or may not come as a surprise, but the intuitive speciﬁcation does not fulﬁl the common trend assumption. The common trend
assumption relies on diﬀerencing out speciﬁc terms of the unobservable
potential outcome, which does not happen in this nonlinear speciﬁcation. Indeed, the common trend assumption would only be respected
if γ and π would be zero. However, these are exactly those coeﬃcients
11 For

diﬀerent ways to estimate “causal” parameters from these models, see the papers by
Ai and Norton (2003) and Puhani (2008).

198

Models, Eﬀects, and Identiﬁcation

that capture the group speciﬁc diﬀerences. Whereas the linear speciﬁcation requires the group speciﬁc diﬀerences to be time constant,
the nonlinear speciﬁcation requires them to be absent. Of course, this
property of this nonlinear speciﬁcation removes the attractive feature
that DiD allows for some selection on unobservable group and individual speciﬁc diﬀerences. Thus, we conclude that estimating a DiD model
with the standard speciﬁcation of a nonlinear model would usually lead
to an inconsistent estimator if the standard common trend assumption
is upheld.
In other words, if the standard DiD assumptions hold, this nonlinear
model does not exploit them (it will usually violate them). Therefore,
estimation based on this model does not identify the causal eﬀect of D
on Y . Let us generalize the above model by introducing group speciﬁc
coeﬃcients:
E(Yt1 |X = x, D = d) = G(αd + tδ 1,d + xβ d + txλ1,d );
E(Yt0 |X = x, D = d) = G(αd + tδ 0,d + xβ d + txλ0,d );

∀ d ∈ {0, 1}.
(3.11)

Note that the terms dγ d and dxπ d are absorbed by the group speciﬁc
constant and slope (αd and xβ d ). For the common trend assumption,
we then obtain:
˜

˜

E(Y1d |X = x, D = 1) − E(Y0d |X = x, D = 1)
˜

˜

= G[α1 + δ d,1 + x(β 1 + λd,1 )] − G(α1 + xβ 1 );
˜

˜

E(Y1d |X = x, D = 0) − E(Y0d |X = x, D = 0)
˜

˜

= G[α0 + δ d,0 + x(β 0 + λd,0 )] − G(α0 + xβ 0 );

∀ d˜ ∈ {0, 1}.

This model fulﬁls the common trend assumption under a set
˜
˜
of restrictions on the coeﬃcients (e.g., α1 + δ d,1 = α0 + δ d,0 , α1 =
˜
˜
˜
˜
˜
˜
α0 , β 1 + λd,1 = β 0 + λd,0 , β 1 = β 0 , so that λd,1 = λd,0 = λd and δ d,1 =
˜
˜
δ d,0 = δ d ). However, those restrictions imply, as before, that there are
no group eﬀects and thus this parameterization for the potential outcomes is not attractive either. To summarize, these “standard” nonlinear parametric speciﬁcations of the potential outcomes and the derived
observed outcomes are not attractive, because they fulﬁl the DiD

3.2 Identiﬁcation

199

assumptions only under additional constraints which are usually not
attractive in typical applications.12
Since the simple way of using standard parametric models does
not work in the nonlinear case when identiﬁcation is achieved by the
DiD assumptions, what are the alternatives? One alternative is to
use a linear speciﬁcation despite its problematic features for outcome
variables with bounded support. A second alternative is to use nonlinear parametric approximations to predict the four components of the
conditional-on-X eﬀects, E(Yt |X = x, D = d), t, d ∈ {0, 1} in a parsimonious way, and then average those conditional-on-X eﬀects according to the desired distribution of confounders to obtain estimates for
the treated population. For example, with a binary outcome variable
we may want to estimate a probit model in all three subsamples and
obtain the following estimator for the average treatment eﬀect on the
treated:
1 =
ATET

N

di ti {[y1i − Φ(xi ϕ̂01 )] − [Φ(xi ϕ̂10 ) − Φ(xi ϕ̂00 )]},

(3.12)

i=1

where ϕ̂dt denotes a vector of coeﬃcients (including a constant) estimated using a probit model with dependent variable, Yt , in the subsample deﬁned by group d in period t. Φ(a) denotes the cdf of the
standard normal distribution evaluated at a. Note that one could predict the outcome for the treated in the post-treatment periods as well,
but the average of such a prediction should be close to the mean of the
outcome (it would be identical if a logit model estimated by maximum
likelihood is used).
Estimating three (or four) probit models is of course similar to estimating a model in the overall sample (or the three or four subsamples)
12 This

problem has already been observed by Meyer (1995, p. 155), who explains it in the
following way: “ . . . if the mean of the outcome variable is very diﬀerent in the treatment
and control group, then [comparing expectations of means in the four groups] could not
be an appropriate model both in levels and in means . . . This problem occurs because
nonlinear transformations of the dependent variable imply diﬀerent marginal eﬀects on
the dependent variable at diﬀerent levels of the dependent variable. Thus, time could
not have an eﬀect of the exact same magnitude in both treatment and control groups in
both a linear and a logarithmic speciﬁcation.” A similar observation has been made by
Heckman (1996) in his discussion of a paper by Eissa (1996).

200

Models, Eﬀects, and Identiﬁcation

which is fully interacted with respect to t and d. In practice, one may
wish to estimate a more parsimonious speciﬁcation by omitting some
of those interaction terms.
Although this approach may work well in practice (however, there
seems to be no applied literature using such a speciﬁcation in this way),
a drawback of using these approximations is that we cannot recover the
exact functional speciﬁcations of the mean potential outcomes, which
makes it harder to understand the underlying restrictions that come
from the required functional form assumptions.
3.2.9.5

Nonlinear models with a modiﬁed
common trend assumption

In the previous section, it was shown that commonly used nonlinear
models violate the common trend assumption. Here, we show that,
indeed, for certain types of outcome variables the nature of these
outcome variables makes it hard to believe from the outset that common trends may prevail at all. To see this problem using an example,
assume that a binary variable for a particular group of nontreated in
the post-treatment period has a mean of 0.9. Suppose further that the
gap between the treated and nontreated groups prior to treatment is
0.2 in favor of the treated. In this case, adjusting for common trends
would lead to an expected nontreatment outcome for the treated of 1.1,
which would be outside the support of the outcome variable. Thus, in
this example the common trend assumption must be violated.13
Following ideas similar to those of Blundell and Costa Dias (2009), we
now explore the potential of a diﬀerent way to specify identifying assumptions that resemble the key ideas of DiD estimation, but may appear to
be more plausible than the common trend assumption for variables with
bounded support and other cases in which the original DiD assumption
appears implausible.14 The following exposition is based on the concept
of a latent dependent variable. Such variables ﬁgure very prominently
in microeconometrics to link standard econometric linear models to discrete, censored or truncated dependent variables.
13 I

thank Patrick Puhani for a very interesting discussion on this subject.
and Costa Dias (2009) use speciﬁcations based on various error terms that lead to
the same results as the more direct approach followed here.

14 Blundell

3.2 Identiﬁcation

201

Concretely, assume that the conditional expectation of the observable outcome variable, Y , is related to the conditional expectation of a
latent outcome variable, Y ∗ , in the following way:
E(Yt0 |X = x, D1 = d) = H[E(Yt0∗ |X = x, D = d)];
∀ d, t ∈ {0, 1}, ∀ x ∈ χ.

(3.13)

The function H(·) is assumed to be strictly monotonously increasing
and invertible. The inverted function is denoted by H(·)−1 . Therefore,
we get:
E(Yt0∗ |X = x, D = d) = H −1 [E(Yt0 |X = x, D = d)];
∀ d, t ∈ {0, 1}, ∀ x ∈ χ.
The function H(·) plays the role of a typical link function that
appears in probit, logit, tobit, and many other nonlinear models. For
example, in the probit model H(·) is the cdf of the standard normal
distribution. Now, let us assume common trends at the level of the
expectations of the latent nontreatment outcome variables:
E(Y10∗ |X = x, D = 1) − E(Y00∗ |X = x, D = 1)
= E(Y10∗ |X = x, D = 0) − E(Y00∗ |X = x, D = 0)
= E(Y10∗ |X = x) − E(Y00∗ |X = x),

(CT ∗)

∀ x ∈ χ.

Clearly, whether this assumption is plausible or not depends on the
particular parameterization of the model which critically involves the
H(·) function. Even more important is that sometimes a substantive
meaning can be given to the latent outcome variable, like an utility or
an earnings potential, for example, which can then be used as the basis
for judging the credibility of this assumption.
Using (NEPT) and the modiﬁed common trend assumption, we can
show that the usual eﬀects are identiﬁed:
E(Y 0∗ |X = x, D = 1)


 1
H −1 [E(Y10 |X=x,D=1)]

= E(Y10∗ |X = x, D = 0) − E(Y00∗ |X = x, D = 0)

 



H −1 [E(Y10 |X=x,D=0)]

+ E(Y00∗ |X = x, D = 1)



H −1 [E(Y00 |X=x,D=1)]

H −1 [E(Y00 |X=x,D=0)]

202

Models, Eﬀects, and Identiﬁcation

= H −1 [E(Y10 |X = x, D = 0)] − H −1 [E(Y00 |X = x, D = 0)]

 



H −1 [E(Y1 |X=x,D=0)]

H −1 [E(Y0 |X=x,D=0)]

+ H −1 [E(Y00 |X = x, D = 1)]



H −1 [E(Y0 |X=x,D=1)]

=H

−1

[E(Y1 |X = x, D = 0)] − H −1 [E(Y0 |X = x, D = 0)]

+ H −1 [E(Y0 |X = x, D = 1)].
Therefore, we can express the missing counterfactual E(Y10 |X =
x, D = 1) as
E(Y10 |X = x, D = 1) = H[E(Y1∗0 |X = x, D = 1)]
= H{H −1 [E(Y1 |X = x, D = 0)]
− H −1 [E(Y0 |X = x, D = 0)]
+ H −1 [E(Y0 |X = x, D = 1)]}.
Thus, ATET1 is identiﬁed.15
As an example consider a binary outcome variable analyzed with
a probit model in its linear index form with subsample speciﬁc coeﬃcients. In this case, H(·) will be the cdf of the standard normal distribution, Φ(·), and H −1 (·) will be the respective inverse, Φ−1 (·), which
exists and is unique in this case. Such a probit model is deﬁned as:
E(Yt |X = x, D = d) = Φ(xϕdt ).

(3.14)

This expressions leads to the ﬁnal result:
E(Y10 |X = x, D = 1) = Φ{Φ−1 [Φ(xϕ01 )] − Φ−1 [Φ(xϕ00 )] + Φ−1 [Φ(xϕ10 )]}
= Φ(xϕ01 − xϕ00 + xϕ10 ).
Letting ϕ̂01 , ϕ̂00 , and ϕ̂10 be consistent estimates of the unknown coefﬁcients leads to the following expression for the missing mean potential
outcome and thus the average treatment eﬀect on the treated can be
consistently estimated by:
1 = 1
ATET
N1
15 Note

N

N

di ti [y1i − Φ(xϕ̂01 − xϕ̂00 + xϕ̂10 )],
i=1

N1 =

d i ti .
i=1

that when leaving out the covariates, this is exactly the expression derived by
Blundell and Costa Dias (2009, p. 586).

3.2 Identiﬁcation

203

It is obvious that this expression is diﬀerent from the one given above
that was also based on the probit model. Although both examples
are based on a probit model, the former assumes common trends
for the expected potential outcomes, whereas the latter assumes common trends for a nonlinear transformation of the expected outcomes.
As already seen before, DiD is functional form dependent. Therefore,
such transformations matter and lead to diﬀerent results.

4
Some Issues Concerning Estimation

4.1

Parametric Models

So far, most empirical studies relying on a DiD approach are using
parametric models. In this case, estimation is usually simple, at least
as long as the model that is estimated is either a linear or a standard
nonlinear model. Of course, if the model is based on a DiD assumption
imposed on some complex transformation as discussed in the previous
section on nonlinear models, it may be that even estimating a parametric model may be demanding and subject to substantial problems
(like excessive computation time, convergence problems, or a nonunique
objective function, etc.). However, there is a large literature in microeconometrics regarding these issues and their possible solutions. The
fact that there is a type of DiD assumption involved does not create
additional problems. Even in the parametric case, there may be DiDspeciﬁc problems with inference from the standard estimators that will,
however, be brieﬂy addressed in the section about inference below.

4.2

Semiparametric and Nonparametric Models

When no covariates are present, estimation can still be based on the
regression formulation without being restrictive in any sense. Thus,
204

4.2 Semiparametric and Nonparametric Models

205

standard linear estimators, like OLS, can be used and will have desirable properties. OLS based on a constant, group and time dummies,
as well as their interaction only is identical to the typical DiD comparison of the four sample means. When covariates are included and the
chosen speciﬁcation is not rich enough to lead to a saturated model
(usually because the number of observations is too small within each
cell to allow for a complete set of interactions), the consistency of OLS
is not guaranteed. It may depend on the validity of the functional form
assumptions. Generally, parametric regression is restrictive because it
depends on a linear index function to capture the inﬂuence of covariates
on the outcomes and it restricts eﬀect heterogeneity (see the in-depth
discussion of the properties of linear estimation in such settings by
Angrist and Pischke (2009)). Whether the resulting bias is a serious
one or only a small one, is usually impossible to judge in an application without comparing the parametric results to results obtained by
more ﬂexible semi- or nonparametric models.
The issue of allowing covariates to enter the estimator in a ﬂexible
way can, however, be tackled similarly to the approaches taken by the
semiparametric matching literature. To see this point, start by rewriting the average treatment eﬀect on the treated in the following way:
ATET 1 = EX|D=1 θ1 (x)
= E(Y1 |D = 1) − EX|D=1 E(Y1 |X = x, D = 0)
− [EX|D=1 E(Y0 |X = x, D = 1)
− EX|D=1 E(Y0 |X = x, D = 0)].

(4.1)

The ﬁrst observation is that in order to estimate ATET 1 consistently, a consistent estimate of the conditional on X eﬀect, θ1 (x), is
not necessary. Any estimate that will consistently estimate ATET 1 will
have to reweight the outcomes observed in the three “counterfactual”
subpopulations (deﬁned by T and D) such that these weights applied
to the covariates will make the covariate distribution in the particular
subpopulation identical to the one observed in the target population
(T = 1, D = 1).
The second observation is that the structure of the estimand is very
similar to the one for matching estimation for which a large literature

206

Some Issues Concerning Estimation

exists (see for example the survey by Imbens (2004)). The diﬀerence is
that instead of adjusting a covariate distribution in just the one subsample of nontreated, within a two-period DiD framework three covariate
distributions have to be adjusted because there are three nontreated
groups. In other words, the combination of three consistent matching
estimators in the respective subsamples will lead to a consistent estimator of the population eﬀects (if there are additional time periods available, the number of matching estimations increases by two for every
additional period). It is also worth noting that the so-called propensity
score properties ﬁrst shown by Rosenbaum and Rubin (1983) for the
case of matching can be applied here as well:
EX|D=1 E(Yt |X = x, D = d)
= Ep(X,t,d)|D=1 E[Yt |p(X, t, d) = p(x, t, d), D = d];
p(X, t, d) := P (T D = 1|X = x, (T, D) ∈ {(t, d), (1, 1)}].

(4.2)

The common support assumption (COSU) ensures that these probabilities, called propensity scores in the matching literature, are positive.
This general property is in principle already contained in the results
derived by Rosenbaum and Rubin (1983). The proof for the DiD case
is sketched in Appendix A.1.
So far studies using semiparametric or nonparametric covariate
adjustments in DiD estimation are rare. The ﬁrst paper to observe that
DiD estimators can be based on matching seems to be Heckman et al.
(1997). They base their estimators on local linear kernel estimators, as
do Bergemann et al. (2009).
Acemoglu and Angrist (2001), whose empirical analysis is based on
a very large sample and a fairly small number of discrete covariates,
compute the respective means within the cells deﬁned by the covariates
and then weight the resulting diﬀerences with the probability of the
particular cell in the population.
Eichler and Lechner (2002) use simple propensity score matching
methods while Blundell et al. (2004) use a time speciﬁc and crosssection speciﬁc propensity score in their matching methods (see also
Blundell and Costa Dias (2009)). Ravallion et al. (2005) use propensity
score matching as well to purge their estimates of diﬀerences coming

4.2 Semiparametric and Nonparametric Models

207

from diﬀerent distributions of trend confounders in the various subsamples.
Abadie (2005) proposes estimators based on weighting the outcomes
by the propensity scores combined with linear projections. He also provides distribution theory for the case of using a nonparametric estimator for the propensity score. Finally, in an empirical application Bühler
et al. (2011) also use weighting on the propensity score. They propose
to estimate diﬀerent scores for each of the three comparisons involved.
It is worth pointing out that all matching estimators considered in the
literature can be used in this DiD setting as well. The paper by Huber
et al. (2010) give a comprehensive account of such estimators and their
relative performance.

5
Some DID-speciﬁc Issues About Inference

Recently, there has been a renewed discussion about how to conduct inference in a (usually parametric) DiD setting. These papers
are concerned with potential correlations of uncertainty over time as
well as within groups (in particular with panel data), for example
exhibited by group-time speciﬁc error terms. Note that if we have
period/group-speciﬁc randomness (e.g., group-time speciﬁc individual
random eﬀects),1 then with a ﬁnite number of periods and groups, no
consistent estimator can exist, because within group averaging cannot eliminate such variability. The only way to reduce this type of
uncertainty is to have more periods and more (nontreatment) groups.
The issue of correlations over time of the units in the diﬀerent groups
(like states before and after a policy change) as well as their crosssectional correlation within groups has been analysed within a regression framework by Bertrand et al. (2004), Donald and Lang (2007),
Hansen (2007a,b) allowing for multiple periods and groups. These
papers conclude that inference may be substantially aﬀected by such
1 Of

course, (additive separable) treatment group speciﬁc uncertainty that is constant over
time, as well as time speciﬁc uncertainty that is constant over groups, is diﬀerenced out
by taking the diﬀerences of the four group means.

208

209
cluster eﬀects (see also the informative discussion about these issues
within a regression-type framework in Imbens and Wooldridge (2009,
p. 69)). Conley and Taber (2011) approach this problem somewhat differently. They consider the case of a small number of treated groups
and an expanding number of control groups and develop asymptotic
distribution theory for test statistics under this scenario. Although in
this case DiD estimators for the eﬀect of D may be inconsistent (but
unbiased), they develop asymptotic standard tests that can be used for
inference in this setting.
Despite, or because of, the recent ﬁndings on problems with standard inference in DiD estimation, how to best conduct inference very
much depends on how uncertainty is speciﬁed. Generally, how to do
inference remains an open issue in many DiD settings.

6
Further Issues

6.1

Very Small Number of Treated and Control
Units/Interventions at the Aggregate Level

There may be a problem for identiﬁcation, estimation, and inference
(see previous section) if, for example, one region is subject to a treatment in period t (i.e., all units are subject to the new regime), whereas
a couple of other regions are not subject to it. First, note that this
may create some problem for identiﬁcation if the other regions have
diﬀerent time trends of the potential outcomes compared to the treatment region. Abadie et al. (2007) suggest treating this problem essentially as a matching problem. They propose using a weighted average of
all the control regions with weights chosen such that the weighted mean
of the covariates in the control regions is equal, or at least as similar as
possible, to the observed values in the treatment region after the treatment. Furthermore, the authors also argue that even in such aggregate
studies, one should account for uncertainty (one may argue that one
observes the population and, thus, there is no need to take sampling
uncertainty into account) and provide some inference procedures that
210

6.2 Additional Periods

211

(i) do not rely on asymptotic theory and are valid in small samples,
and (ii) take account of other uncertainty than sampling uncertainty.1

6.2

Additional Periods

For most of the paper the simplest case of just two periods has been
considered. This is suﬃcient to achieve identiﬁcation of the usual quantities of interest. Now, we consider the value added by having more
(post- and/or pre-treatment) periods available. It is assumed that in
all pre-treatment periods the (future) treatment group is identiﬁed and
that for all pre- and post-treatment periods the same covariates (that
are not inﬂuenced by the future or past treatments) are observed.
Before expanding on the advantages of having more post- and pretreatment periods, note that they are only useful if the common trend
assumptions holds for all pre- and post-treatment comparisons. This
assumption may become particularly implausible, or particular plausible, if the periods are further away from each other, depending on the
particular scenario. Although derived in a fairly restrictive framework,
the paper by Chabé-Ferret (2010) points to cases when, for example,
an estimator using pre-post-treatment pairs with equal time distance
to the treatment is consistent while using just the most recent pretreatment period leads to an inconsistent estimator. Unfortunately, he
also shows examples in which the opposite is true. Apparently, more
research is required on this important topic.

6.2.1

Dynamics of the Eﬀects

Suppose we have additional post-treatment knowledge. If the common trend assumptions are valid for all comparisons with the pretreatment period, then the additional information allows discovering
eﬀect dynamics and testing for eﬀects being stable over time. This type
of dynamics may be important information from a policy perspective.
1 The

already mentioned paper by Donald and Lang (2007) is also particularly interested in
the case of a small number of treated and controls, while the study by Conley and Taber
(2011) mentioned above provides an alternative analysis that is geared to the intermediate
case of a larger number of nontreated and a small number of treated.

212

Further Issues

If, however, it is known that the true eﬀects are constant over
time, performing DiD estimations based on using diﬀerent single posttreatment periods allows to test the plausibility of the identifying
assumption, because if the true eﬀects are the same over time and if the
identifying assumptions hold for all post-treatment periods, then the
estimates from these pair-wise comparisons should be the same as well.
6.2.2

Checking the Credibility of the Identifying
Assumptions

The last remarks in the previous section also apply to the case of just
one post-treatment period and many pre-treatment periods. Again, if
(and only if) the identiﬁcation condition is valid for all those periods,
then the choice of the pre-treatment period used should not systematically change the estimates.
So-called “placebo experiments” are also possible to test overidentifying assumptions and make the common trend assumption more plausible. Suppose for example that we have several pre-treatment periods.
In this case, we could pretend that actually the treatment happened
earlier and then measure the outcome after the pretended treatment
but before the treatment actually happened. If we ﬁnd an eﬀect of this
artiﬁcial treatment it could have either of the following two reasons:
(i) The treatment is anticipated and therefore has an eﬀect even before
it starts. This (at least) raises some concerns about when to measure
covariates which are not supposed to be inﬂuenced by the treatment.
(ii) If we can rule out anticipation, this eﬀect of the placebo treatment becomes a speciﬁcation test for the common trend assumption,
because any estimated nonzero eﬀect would have to be interpreted as
selection bias and thus would cast serious doubts on the validity of the
identifying assumptions.2
6.2.3

Eﬃciency

Apparently, since more periods may generate overidentifying assumptions, such assumptions can be used to obtain more eﬃcient estimators.
2 See

also Angrist and Pischke (2009) as well as Autor (2003) of how to exploit the timing
information in a regression DiD framework.

6.2 Additional Periods

213

If the identifying assumptions hold, it must be more eﬃcient to use this
additional information. Suppose we have Υ additional pre-treatment
periods indexed by τ with valid common trend assumptions. In this
case, we can rewrite the estimand in the following way:
0

0

ATET 1 = Diﬀ (1) −

wτ Diﬀ (τ );
τ =−Υ

wτ > 0,

wτ = 1;
τ =−Υ

Diﬀ (t) = EX|D=1 E(Yt |X = x, D = 1) − EX|D=1 E(Yt |X = x, D = 0).
The question now is how to choose the (Υ − 1) free weights wτ . An
obvious way would be to require that they minimize the variance of
the overall estimator. The variance of the estimators for Diﬀ (t) can be
derived using the usual methods. Intuitively, the smaller the variance
of Yt and the larger the sample size, the more precise this period estimator will be. Furthermore, the more similar the distribution of X in
the subsamples in period τ is compared to the target distribution of
X in T = 1, D = d, the more precise we expect the estimator to be.
Furthermore, this requires an estimator of the covariances of Diﬀ (t)
and Diﬀ (t ), which may be diﬃcult to derive for those semiparametric
estimators for which the bootstrap is not a valid variance estimator.3

3 Note

that even in the case of repeated cross-sections with independent observations the
estimators of Diﬀ(t) may not be independent for diﬀerent periods, because all periods use
the empirical distribution of X, or of some function of X like the propensity score, to
estimate Diﬀ(t). Thus, the two estimates could be correlated.

7
Conclusions

The DiD design for empirical analysis of causal eﬀects has a long history in and outside econometrics. Nowadays, it is certainly one of the
most heavily used empirical research designs to estimate the eﬀects
of policy changes or interventions in empirical microeconomics. It has
the advantage that the basic idea is very intuitive and thus easy to
understand for an audience with limited econometric education. Compared for example to matching methods it has the further advantage
that there is no need to control for all confounding variables. This
means that it can accommodate a certain degree of selectivity based
on unobservables correlated with treatment and outcome variables. Its
key identifying assumption is the common trend assumption that must
hold either unconditionally or conditionally on some observables (that
are not inﬂuenced by the treatment). If the latter is the case, standard
DiD estimation can be combined fruitfully with matching estimation
techniques to accommodate such covariates in a ﬂexible way. For that
latter case, this paper propose some new propensity score based matching techniques that extend the proposals made by Abadie (2005) and
Heckman et al. (1998) to a setting more similar to “standard” matching
estimation.
214

215
Unfortunately, like any empirical research design it has also severe
disadvantages. First of all, the common trend assumption is functional form, or scale-of-measurement, dependent. For example, if there
are common trends in the logs of an outcome variable, then in all
other than some exceptional and uninteresting cases the common trend
assumption will not be valid for the untransformed levels of the outcome variable. This makes it much harder to justify the common trend
assumption from substantive knowledge about selection and outcomes
than for empirical research designs that lead to nonparametric identiﬁcation, like matching or instrumental variables. A related problem
is that the common trend assumption usually cannot be true for variables with bounded support. A potential remedy in this case may be to
impose the common trend assumption in some latent model. However,
justifying a common trend assumption for some latent objects is most
likely to be even more diﬃcult to justify in a credible way. It remains
to be explored whether the more complex changes-in-changes model by
Athey and Imbens (2006) will mediate these problems to some extent.
Finally, depending on the structure of uncertainty, inference may be
tricky to impossible in the two-periods-two-groups case. Having many
(similar) periods and many (similar) groups (of nontreated) seems to
be important, as it allows (i) more precise estimation; (ii) testing for
the common trend assumption; (iii) and more reliable inference.

A
Technical Appendix

A.1

Propensity Score Property

The propensity score properties discussed in the main part of the text
states that it does not matter whether reweighting the outcome variables in a speciﬁc subsample is with respect to the distribution of X
or with respect to the distribution of the propensity score. Of course,
this is not novel as it is already implied by the results in Rosenbaum
and Rubin (1983). Therefore, we will only sketch the proof in a general
way.
Denote the two subpopulations of interest as those characterized
by the value of the binary random variable W . We want to reweight
the outcomes in subpopulation W = 0 according to the distribution of
X in subpopulation characterized by W = 1, i.e., the target is to get
EX|Z=1 E(Y |X = x, Z = 0). The claim is that this can also be done by
the propensity score, deﬁned as p(x) = P (W = 1|X). Thus, the following property has to be shown to be valid:
!

EX|Z=1 E(Y |X = x, Z = 0) = Ep(X)|Z=1 E(Y |p(X) = p(x), Z = 0).

216

A.2 Potential Outcomes Over Time and Treatment

217

A sketch of the proof is as follows:
Ep(X)|Z=1 E(Y |p(X) = p(x), Z = 0)
= Ep(X)|Z=1 EX|p(X),Z=0 E(Y |X = x, p(X) = p(x), Z = 0)
= Ep(X)|Z=1 EX|p(X),Z=0 E(Y |X = x, Z = 0)
= Ep(X)|Z=1 EX|p(X),Z=1 E(Y |X = x, Z = 0)
= EX|Z=1 E(Y |X = x, Z = 0).
The ﬁrst line of the above expression uses iterated expectations,
the second line employs the fact that X is ﬁner than p(x), so that
E(Y |X = x, p(X) = p(x), Z = 0) =?. The third line is based on the balancing score property derived in Rosenbaum and Rubin (1983) stating
that X and Z are independent conditional on P (Z = 1|X), and the last
line exploits Bayes’ Law and again the fact that X is ﬁner than p(X),
(fX|p(X),Z=1 (x)fp(X),Z=1 (x) = fX,p(X)|Z=1 (x, p(x)) = fX|Z=1 (x)).

A.2

Independence of Diﬀerences of Potential Outcomes
Over Time and Treatment

An alternative to the common trend assumptions in means is to assume
that the diﬀerences of the potential outcomes conditional on covariates
are independent of D.
F (Y10 − Y00 |X = x, D = 0) = F (Y10 − Y00 |X = x, D = 1);
In the matching literature it would be more common to use the
following notation:
(Y10 − Y00 )

D|X = x.

A B|C = c means that A is independent of B conditional on C being
equal to c. Next, we prove identiﬁcation in a similar way as before:
F (Y10 − Y00 |X = x, D = 1)
= F (Y10 − Y00 |X = x, D = 0) = F (Y1 − Y0 |X = x, D = 0)
if Y00 = Y01 · · · ⇒ F (Y10 − Y0 |X = x, D = 1)
= F (Y1 − Y0 |X = x, D = 0).

218

Technical Appendix

It appears that no further simpliﬁcation of this expression is
possible. Thus contrary to the matching assumptions, DiD does not
identify the counterfactual distribution. It is probably easiest to see
this result for the variance. The above assumption implies that changes
in the variances of the diﬀerences over time do not depend on the treatment status:
Var(Y10 − Y00 |X = x, D = 1) = Var(Y10 − Y00 |X = x, D = 0)
= Var(Y1 − Y0 |X = x, D = 0).
From the standard deﬁnition of the variance of a diﬀerence, we get the
following expression:
Var(Y10 − Y00 |X = x, D = 1) = Var(Y10 |X = x, D = 1)
+ Var(Y00 |X = x, D = 1)
− 2Covar(Y10 , Y00 |X = x, D = 1).
Putting those two equations together, we obtain the ﬁnal expression
for the variance of the counterfactual outcome:
Var(Y10 |X = x, D = 1)
= Var(Y1 − Y0 |X = x, D = 0) − Var(Y00 |X = x, D = 1)
+ 2Covar(Y10 , Y00 |X = x, D = 1)
= Var(Y1 − Y0 |X = x, D = 0) − Var(Y0 |X = x, D = 1)






identiﬁed with panel data if Y00 =Y01

identiﬁed if Y00 =Y01

+ 2 Covar(Y10 , Y0 |X = x, D = 1) .



not identiﬁed

Clearly, the covariance term is not identiﬁed and thus eﬀects on the
variance are not identiﬁed. This holds whether panel data are available
or not.
However, due to the linearity of the expectations operator, mean
eﬀects are identiﬁed because this common trend assumption implies
common trends in means as well (if the means exist), which is enough
to identify mean average eﬀects:
F (Y10 − Y00 |X = x, D = 1) = F (Y10 − Y00 |X = x, D = 0)
⇒ E(Y10 − Y00 |X = x, D = 1) = E(Y10 − Y00 |X = x, D = 0).

References

Abadie, A. (2005), ‘Semiparametric diﬀerence-in-diﬀerence estimators’.
Review of Economic Studies 72, 1–19.
Abadie, A., A. Diamond, and J. Hainmüller (2007), ‘Synthetic control methods for comparative case studies: Estimating the eﬀect
of California’s tobacco control program’. NBER technical working
paper 335.
Acemoglu, D. and J. D. Angrist (2001), ‘Consequences of employment
protection? The case of the Americans with disabilities act’. Journal
of Political Economy 109, 915–957.
Ai, D. and E. C. Norton (2003), ‘Interaction terms in logit and probit
models’. Economics Letters 80, 123–129.
Angrist, J. D. and A. B. Krueger (1999), ‘Empirical strategies in labor
economics’. In: O. Ashenfelter and D. Card (eds.): Handbook of Labor
Economics, vol. III A, Ch 23. pp. 1277–1366.
Angrist, J. D. and J.-S. Pischke (2009), Mostly Harmless Econometrics.
New York: Princeton University Press.
Ashenfelter, O. (1978), ‘Estimating the eﬀect of training programs
on earnings’. The Review of Economics and Statistics 60(1),
47–57.

219

220

References

Ashenfelter, O. and D. Card (1985), ‘Using the longitudinal structure
of earnings to estimate the eﬀect of training programs’. The Review
of Economics and Statistics 67, 648–660.
Athey, S. and G. W. Imbens (2006), ‘Identiﬁcation and inference in nonlinear diﬀerence-in-diﬀerence models’. Econometrica 74,
431–497.
Autor, D. H. (2003), ‘Outsourcing at will: The contribution of unjust
dismissal doctrine to the growth of employment outsourcing’. Journal
of Labor Economics 21, 1–42.
Bergemann, A., B. Fitzenberger, and S. Speckesser (2009), ‘Evaluating
the dynamic employment eﬀects of training programs in east Germany using conditional diﬀerence-in-diﬀerences’. Journal of Applied
Econometrics 24, 797–823.
Bertrand, M., E. Duﬂo, and S. Mullainathan (2004), ‘How much should
we trust diﬀerences-in-diﬀerences estimates’. Quarterly Journal of
Economics pp. 249–275.
Besley, T. and R. Burgess (2004), ‘Can labor regulation hinder
economic performance? Evidence from India’. Quarterly Journal of
Economics pp. 91–134.
Blundell, R. and M. Costa Dias (2009), ‘Alternative approaches to evaluation in empirical microeconomics’. Journal of Human Resources
44, 565–640.
Blundell, R., A. Duncan, and C. Meghir (1998), ‘Estimating labor supply responses using tax reforms’. Econometrica 66, 827–861.
Blundell, R., C. Meghir, M. Costa Dias, and J. van Reenen
(2004), ‘Evaluating the employment impact of a mandatory job
search program’. Journal of the European Economic Association 2,
569–606.
Bonhomme, S. and U. Sauder (2011), ‘Recovering distributions in
diﬀerence-in-diﬀerences models: A comparison of selective and comprehensive schooling’. The Review of Economics and Statistics 93,
479–494.
Bühler, S., M. Helm, and M. Lechner (2011), ‘Trade liberalization
and growth: Plant-level evidence from Switzerland’. Discussion paper
2011-33, Department of Economics, University of St. Gallen.
Card, D. (1990), ‘The impact of the mariel boatlift on the miami labor
market’. Industrial and Labor Relations Review 43(2), 245–257.

References

221

Card, D. and A. B. Krueger (1994), ‘Minimum wages and employment:
A case study of the fast-food industry in New Jersey and Pennsylvania’. The American Economic Review 84, 772–793.
Chabé-Ferret, S. (2010), To Control or Not to Control? Bias of Simple
Matching vs Diﬀerence-In-Diﬀerence Matching in a Dynamic Framework. mimeo.
Conley, T. and C. Taber (2011), ‘Inference with “diﬀerence in diﬀerences” with a small number of policy changes’. Review of Economics
and Statistics 93, 113–125.
Cook, P. J. and G. Tauchen (1982), ‘The eﬀect of liquor taxes on heavy
drinking’. Bell Journal of Economics 13, 379–390.
Cook, T. D. and D. T. Campbell (1979), Quasi-Experimentation.
Boston: Houghton Miﬄin.
Donald, S. G. and K. Lang (2007), ‘Inference with diﬀerence-indiﬀerences and other panel data’. Review of Economics and Statistics
89, 221–233.
Duﬂo, E. (2001), ‘Schooling and labor market consequences of school
construction in Indonesia: Evidence from an unusual policy experiment’. American Economic Review 91, 795–813.
Eichler, M. and M. Lechner (2002), ‘An evaluation of public employment programmes in the east German state of Sachsen-Anhalt’.
Labour Economics: An International Journal 9, 143–186.
Eissa, N. (1996), ‘Labor supply and the economic recovery act of 1981’.
In: M. Feldstein and J. Poterba (eds.): Empirical Foundations of
Household Taxation. pp. 5–38.
Gruber, J. and J. Poterba (1994), ‘Tax incentives and the decision to
purchase health insurance: Evidence from the self- employed’. The
Quarterly Journal of Economics 109, 701–733.
Hansen, C. B. (2007a), ‘Asymptotic properties of a robust variance
matrix estimator for panel data when T is large’. Journal of Econometrics 141, 597–620.
Hansen, C. B. (2007b), ‘Generalized least squares inference in panel and
multilevel models with serial correlation and ﬁxed eﬀects’. Journal
of Econometrics 140, 670–694.
Havnes, T. and M. Mogstad (2010), ‘Is universal child care leveling
the playing ﬁeld? Evidence from a nonlinear diﬀerence-in-diﬀerence
approach’. IZA discussion paper 4478.

222

References

Heckman, J. J. (1996), ‘Comment on eissa: Labor supply and the economic recovery act of 1981’. In: M. Feldstein and J. Poterba (eds.):
Empirical Foundations of Household Taxation. pp. 5–38.
Heckman, J. J. and V. J. Hotz (1989), ‘Choosing among alternative
nonexperimental methods for estimating the impact of social programs: The case of manpower training’. Journal of the American
Statistical Association 84, 862–880.
Heckman, J. J., H. Ichimura, J. Smith, and P. Todd (1998), ‘Characterizing selection bias using experimental data’. Econometrica 66,
1017–1098.
Heckman, J. J., R. LaLonde, and J. Smith (1999), ‘The economics and
econometrics of active labor market programs’. In: O. Ashenfelter
and D. Card (eds.): Handbook of Labour Economics, vol. 3. Amsterdam: North-Holland, pp. 1865–2097.
Heckman, J. J. and R. Robb (1986), ‘Alternative methods for solving
the problem of selection bias in evaluating the impact of treatments
on outcomes’. In: H. Wainer (ed.): Drawing Inferences from SelfSelected Samples. pp. 63–113.
Huber, M., M. Lechner, and C. Wunsch (2010), ‘How to control for
many covariates? Reliable estimators based on the propensity score’.
Discussion paper 2010-30, Department of Economics, University of
St. Gallen.
Hunt, J. (1995), ‘The eﬀect of unemployment compensation on unemployment duration in Germany’. Journal of Labor Economics 13,
88–120.
Imbens, G. W. and J. M. Wooldridge (2009), ‘Recent developments
in the econometrics of program evaluation’. Journal of Economic
Literature 47, 5–86.
Lai, A. (2011), ‘London cholera and the blind-spot of an epidemiology
theory’. Signiﬁcance pp. 82–85.
Lechner, M. (2008a), ‘A note on endogenous control variables in evaluation studies’. Statistics and Probability Letters 78, 190–195.
Lechner, M. (2008b), ‘A note on the common support problem in
applied evaluation studies’. Annales d’Économie et de Statistique
91–92, 217–234.

References

223

Lester, R. A. (1946), ‘Shortcomings of marginal analysis for the wageemployment problems’. American Economic Review 36, 63–82.
Manski, C. F. (2011), ‘Identiﬁcation of treatment response with social
interactions’. Department of Economics and Institute for Policy
Research, Northwestern University, mimeo.
Meyer, B. D. (1995), ‘Natural and quasi-experiments in economics’.
Journal of Business & Economic Statistics 13, 151–161.
Meyer, B. D., W. K. Viscusi, and D. L. Durbin (1995), ‘Workers’ compensation and injury duration: Evidence from a natural experiment’.
American Economic Review 85(3), 322–340.
Miguel, E. and M. Kremer (2004), ‘Worms: Identifying impacts on education and health in the presence of treatment externalities’. Econometrica 72, 159–217.
Obenauer, M. and B. von der Nienburg (1915), ‘Eﬀect of minimumwage determinations in oregon’. Bulletin of the U.S. Bureau of Labor
Statistics, 176, Washington, D.C.: U.S. Government Printing Oﬃce.
Puhani, P. (2008), ‘The treatment eﬀect, the cross diﬀerence, and the
interaction term in nonlinear “diﬀerence-in-diﬀerence” models’. IZA
Discussion paper 3478, revised November 2008.
Ravallion, M., E. Galasso, T. Lazo, and E. Philipp (2005), ‘What can
ex-participants reveal about a program’s impact’. Journal of Human
Ressources 40, 208–230.
Rose, A. M. (1952), ‘Needed research on the mediation of labour disputes’. Personal Psychology 5, 187–200.
Rosenbaum, P. (2001), ‘Stability in the absence of treatment’. Journal
of the American Statistical Association 96, 210–219.
Rosenbaum, P. R. and D. B. Rubin (1983), ‘The central role of
the propensity score in observational studies for causal eﬀects’.
Biometrika 70, 41–50.
Rubin, D. B. (1977), ‘Assignment to treatment group on the basis of a
covariate’. Journal of Educational Statistics 2, 1–26.
Shadish, W. R., T. D. Cook, and D. T. Campbell (2002), Experimental
and Quasi-Experimental Designs for Generalized Causal Inference.
Boston: Houghton-Miﬄin.
Simon, J. L. (1966), ‘The price elasticity of liquor in the U.S. and a
simple method of determination’. Econometrica 34, 193–205.

224

References

Snow, J. (1854), ‘The cholera near golden square, and at deptford’.
Medical Times and Gazette 9, 321–322.
Snow, J. (1855), On the Mode of Communication of Cholera. 2nd edition. London: John Churchill.
Waldfogel, J. (1998), ‘The family gap for young women in the United
States and Britain: Can maternity leave make a diﬀerence’. Journal
of Labor Economics 16, 505–545.
Yelowitz, A. S. (1995), ‘The medicaid notch, labor supply, and welfare participation: Evidence from eligibility expansions’. Quarterly
Journal of Economics 110, 909–939.

