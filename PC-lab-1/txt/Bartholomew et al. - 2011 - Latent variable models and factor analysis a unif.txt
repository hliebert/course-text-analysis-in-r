P1: TIX/XYZ
JWST070-FM

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

Latent Variable Models
and Factor Analysis

P1: TIX/XYZ
JWST070-FM

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART and SAMUEL S. WILKS
Editors
David J. Balding, Noel A.C. Cressie, Garrett M. Fitzmaurice, Harvey Goldstein,
Geert Molenberghs, David W. Scott, Adrian F.M. Smith, Ruey S. Tsay,
Sanford Weisberg
Editors Emeriti
Vic Barnett, Ralph A. Bradley, J. Stuart Hunter, J.B. Kadane, David G. Kendall,
Jozef L. Teugels
A complete list of the titles in this series can be found on
http://www.wiley.com/WileyCDA/Section/id-300611.html.

P1: TIX/XYZ
JWST070-FM

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

Latent Variable Models and
Factor Analysis
A Unified Approach

3rd Edition

David Bartholomew

r

Martin Knott

r

Irini Moustaki

London School of Economics and Political Science, UK

A John Wiley & Sons, Ltd., Publication

P1: TIX/XYZ
JWST070-FM

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

This edition first published 2011
© 2011 John Wiley & Sons, Ltd
Registered office
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United
Kingdom
For details of our global editorial offices, for customer services and for information about how to apply
for permission to reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identified as the author of this work has been asserted in accordance with the
Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise,
except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of
the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may
not be available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand
names and product names used in this book are trade names, service marks, trademarks or registered
trademarks of their respective owners. The publisher is not associated with any product or vendor
mentioned in this book. This publication is designed to provide accurate and authoritative information in
regard to the subject matter covered. It is sold on the understanding that the publisher is not engaged in
rendering professional services. If professional advice or other expert assistance is required, the services
of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Bartholomew, David J.
Latent variable models and factor analysis : a unified approach. – 3rd ed. / David Bartholomew,
Martin Knott, Irini Moustaki.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-470-97192-5 (cloth)
1. Latent variables. 2. Latent structure analysis. 3. Factor analysis. I. Knott, M. (Martin)
II. Moustaki, Irini. III. Title.
QA278.6.B37 2011
519.5 35–dc22
2011007711
A catalogue record for this book is available from the British Library.
Print ISBN: 978-0-470-97192-5
ePDF ISBN: 978-1-119-97059-0
oBook ISBN: 978-1-119-97058-3
ePub ISBN: 978-1-119-97370-6
Mobi ISBN: 978-1-119-97371-3
Set in 10/12pt Times by Aptara Inc., New Delhi, India.

P1: TIX/XYZ
JWST070-FM

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

Contents
Preface

xi

Acknowledgements

xv

1.4
1.5
1.6
1.7
1.8
1.9

Basic ideas and examples
The statistical problem
The basic idea
Two examples
1.3.1 Binary manifest variables and a single binary latent variable
1.3.2 A model based on normal distributions
A broader theoretical view
Illustration of an alternative approach
An overview of special cases
Principal components
The historical context
Closely related fields in statistics

1
1
3
4
4
6
6
8
10
11
12
17

2
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
2.10
2.11
2.12
2.13
2.14
2.15
2.16
2.17

The general linear latent variable model
Introduction
The model
Some properties of the model
A special case
The sufficiency principle
Principal special cases
Latent variable models with non-linear terms
Fitting the models
Fitting by maximum likelihood
Fitting by Bayesian methods
Rotation
Interpretation
Sampling error of parameter estimates
The prior distribution
Posterior analysis
A further note on the prior
Psychometric inference

19
19
19
20
21
22
24
25
27
29
30
33
35
38
39
41
43
44

1
1.1
1.2
1.3

P1: TIX/XYZ
JWST070-FM

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

vi

CONTENTS

3
3.1
3.2
3.3
3.4
3.5
3.6
3.7

The normal linear factor model
The model
Some distributional properties
Constraints on the model
Maximum likelihood estimation
Maximum likelihood estimation by the E-M algorithm
Sampling variation of estimators
Goodness of fit and choice of q
3.7.1 Model selection criteria
Fitting without normality assumptions: least squares methods
Other methods of fitting
Approximate methods for estimating 
Goodness of fit and choice of q for least squares methods
Further estimation issues
3.12.1 Consistency
3.12.2 Scale-invariant estimation
3.12.3 Heywood cases
Rotation and related matters
3.13.1 Orthogonal rotation
3.13.2 Oblique rotation
3.13.3 Related matters
Posterior analysis: the normal case
Posterior analysis: least squares
Posterior analysis: a reliability approach
Examples

47
47
48
50
50
53
55
58
58
59
61
62
63
64
64
65
67
69
69
70
70
71
72
74
74

Binary data: latent trait models
Preliminaries
The logit/normal model
The probit/normal model
The equivalence of the response function and underlying variable
approaches
Fitting the logit/normal model: the E-M algorithm
4.5.1 Fitting the probit/normal model
4.5.2 Other methods for approximating the integral
Sampling properties of the maximum likelihood estimators
Approximate maximum likelihood estimators
Generalised least squares methods
Goodness of fit
Posterior analysis
Fitting the logit/normal and probit/normal models: Markov chain
Monte Carlo
4.11.1 Gibbs sampling
4.11.2 Metropolis–Hastings

83
83
84
86

3.8
3.9
3.10
3.11
3.12

3.13

3.14
3.15
3.16
3.17
4
4.1
4.2
4.3
4.4
4.5

4.6
4.7
4.8
4.9
4.10
4.11

88
90
93
93
94
95
96
97
100
102
102
105

P1: TIX/XYZ
JWST070-FM

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

CONTENTS

vii

4.11.3 Choosing prior distributions
4.11.4 Convergence diagnostics in MCMC
Divergence of the estimation algorithm
Examples

108
108
109
109

Polytomous data: latent trait models
Introduction
A response function model based on the sufficiency principle
Parameter interpretation
Rotation
Maximum likelihood estimation of the polytomous logit model
An approximation to the likelihood
5.6.1 One factor
5.6.2 More than one factor
Binary data as a special case
Ordering of categories
5.8.1 A response function model for ordinal variables
5.8.2 Maximum likelihood estimation of the model with ordinal
variables
5.8.3 The partial credit model
5.8.4 An underlying variable model
An alternative underlying variable model
Posterior analysis
Further observations
Examples of the analysis of polytomous data using the logit model

119
119
120
124
124
125
126
127
130
134
136
136

Latent class models
Introduction
The latent class model with binary manifest variables
The latent class model for binary data as a latent trait model
K latent classes within the GLLVM
Maximum likelihood estimation
Standard errors
Posterior analysis of the latent class model with binary manifest
variables
6.8
Goodness of fit
6.9
Examples for binary data
6.10 Latent class models with unordered polytomous manifest variables
6.11 Latent class models with ordered polytomous manifest variables
6.12 Maximum likelihood estimation
6.12.1 Allocation of individuals to latent classes
6.13 Examples for unordered polytomous data
6.14 Identifiability
6.15 Starting values

157
157
158
159
161
162
164

4.12
4.13
5
5.1
5.2
5.3
5.4
5.5
5.6

5.7
5.8

5.9
5.10
5.11
5.12
6
6.1
6.2
6.3
6.4
6.5
6.6
6.7

138
140
140
144
147
148
149

166
167
167
170
171
172
174
174
178
180

P1: TIX/XYZ
JWST070-FM

viii

6.16

6.17
6.18

7
7.1
7.2
7.3

7.4

7.5
7.6
7.7
7.8
7.9
8
8.1
8.2
8.3
8.4
8.5
8.6

8.7
8.8
8.9
8.10
8.11

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

CONTENTS

Latent class models with metrical manifest variables
6.16.1 Maximum likelihood estimation
6.16.2 Other methods
6.16.3 Allocation to categories
Models with ordered latent classes
Hybrid models
6.18.1 Hybrid model with binary manifest variables
6.18.2 Maximum likelihood estimation

180
181
182
185
185
186
186
187

Models and methods for manifest variables of mixed type
Introduction
Principal results
Other members of the exponential family
7.3.1 The binomial distribution
7.3.2 The Poisson distribution
7.3.3 The gamma distribution
Maximum likelihood estimation
7.4.1 Bernoulli manifest variables
7.4.2 Normal manifest variables
7.4.3 A general E-M approach to solving the likelihood equations
7.4.4 Interpretation of latent variables
Sampling properties and goodness of fit
Mixed latent class models
Posterior analysis
Examples
Ordered categorical variables and other generalisations

191
191
192
193
193
194
194
195
196
197
199
200
201
202
203
204
208

Relationships between latent variables
Scope
Correlated latent variables
Procrustes methods
Sources of prior knowledge
Linear structural relations models
The LISREL model
8.6.1 The structural model
8.6.2 The measurement model
8.6.3 The model as a whole
Adequacy of a structural equation model
Structural relationships in a general setting
Generalisations of the LISREL model
Examples of models which are indistinguishable
Implications for analysis

213
213
213
215
215
216
218
218
219
219
221
222
223
224
227

P1: TIX/XYZ
JWST070-FM

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:30

Printer Name: Yet to Come

CONTENTS

9
9.1
9.2

9.3
9.4
9.5
9.6
9.7

ix

Related techniques for investigating dependency
Introduction
Principal components analysis
9.2.1 A distributional treatment
9.2.2 A sample-based treatment
9.2.3 Unordered categorical data
9.2.4 Ordered categorical data
An alternative to the normal factor model
Replacing latent variables by linear functions of the manifest
variables
Estimation of correlations and regressions between latent variables
Q-Methodology
Concluding reflections of the role of latent variables in statistical
modelling

229
229
229
229
233
235
236
236

Software appendix

247

References

249

Author index

265

Subject index

271

238
240
242
244

P1: TIX/XYZ
P2: ABC
JWST070-Preface
JWST070-Bartholomew

May 9, 2011

13:58

Printer Name: Yet to Come

Preface
It is more than 20 years since the first edition of this book appeared in 1987, and
its subject, like statistics as a whole, has changed radically in that period. By far
the greatest impact has been made by advances in computing. In 1987 adequate
implementation of most latent variable methods, even the well-established factor
analysis, was guided more by computational feasibility than by theoretical optimality.
What was true of factor analysis was even more true of the assortment of other
latent variable techniques, which were then seen as unconnected and very specific
to different applications. The development of new models was seriously inhibited
by the insuperable computational problems which they would have posed. This new
edition aims to take full account of these changes.
The Griffin series of monographs, then edited by Alan Stuart, was designed to
consolidate the literature of promising new developments into short books. Knowing
that one of us (DJB) was attempting to develop and unify latent variable modelling
from a statistical point of view, he proposed what appeared in 1987 as Volume 40
in the Griffin series. Ten years later the series had been absorbed into the Kendall
Library of Statistics monographs designed to complement the evergreen volumes
of Kendall and Stuart’s Advanced Theory of Statistics. Latent Variable Models and
Factor Analysis took its place as Volume 7 in that series in 1999. This second edition
was somewhat different in character from its predecessor, and a second author (MK)
brought his particular expertise into the project. After a further decade that book was
in urgent need of revision, and this could only be done adequately by recruiting a
third author (IM) who is actively involved at the frontiers of contemporary research.
Throughout its long history the principal aim has remained unchanged and it is worth
quoting at some length from the Preface of the second edition:
the prime object of the book remains the same – that is, to provide a
unified and coherent treatment of the field from a statistical perspective.
This is achieved by setting up a sufficiently general framework to enable
us to derive the commonly used models, and many more as special cases.
The starting point is that all variables, manifest and latent, continuous
or categorical, are treated as random variables. The subsequent analysis
is then done wholly within the realm of the probability calculus and the
theory of statistical inference.
The subtitle, added in this edition, merely serves to emphasise, rather than modify its
original purpose.

P1: TIX/XYZ
P2: ABC
JWST070-Preface
JWST070-Bartholomew

xii

May 9, 2011

13:58

Printer Name: Yet to Come

PREFACE

Chapter 1 covers the same ground as before, but the order of the material has
been changed. The aim of the revision is to provide a more natural progression of
ideas from the most elementary to the more advanced.
Chapters 2 and 3, as before, are the heart of the book. Chapter 2 provides an
overall treatment of the basic model together with an account of general questions of
inference relating to it. It introduces what we call the general linear latent variable
model (GLLVM) from which almost all of the models considered later in the book
are derived as special cases. An important new feature is an introductory account of
Markov chain Monte Carlo (MCMC) methods for parameter estimation. These are
a good example of the computer-intensive methods which the growth in the power
of computers has made possible. In principle, these methods are now capable of
handling any of the models in this book and a general introduction is given in this
chapter, leaving more detailed treatment until later.
In Chapter 3 the general model is specialised to the normal linear factor model.
This includes traditional factor analysis, which is probably the most thoroughly
studied and widely applied latent variable model. Little directly relevant research
has appeared since the second edition, but our treatment has been revised and this
chapter will serve as a source for the basic theory, much of which is now embodied
in computer software.
Latent trait models are widely used, especially in educational testing, but they
have a far wider field of application, as the examples in Chapter 4 show. The chapter
begins with two versions of the model and then discusses the statistical methods
available for their implementation. Although the traditional estimation methods,
based on likelihood, are efficient and are present in the standard software, we have
also taken the opportunity to demonstrate the MCMC method in some detail in a
situation where it can easily be compared with established methods. There is no
intention here to suggest that its use is limited to such relatively simple examples.
On the contrary, this example is designed to illustrate the potential of the MCMC
method in a broader context.
Chapters 5 and 7 extend the ideas into newer areas, particularly where ordered
categorical variables are involved. A number of the models appeared for the first time
in earlier editions. This work has been consolidated here and, now that computing
is no longer a barrier, they should find wider application. Latent class models are
often seen as among the simpler latent variable models, and in the first edition they
appeared much earlier in the book. Here they appear in Chapter 6 where it can be
seen more easily, perhaps, how they fit in to the broader scheme.
Chapter 8, on relationships between latent variables, has been supplemented by
an account of methods of estimation and goodness-of-fit in the LISREL model,
but otherwise is unchanged, apart from the transfer to Chapter 9 of some material
noted below.
Chapter 9 is entirely new except for the inclusion of a little material from the old
Chapter 8 which now fits more naturally in its new setting. It draws attention to a
number of methods, especially principal components analysis, which serve much the
same purpose as latent variable models but come from a different statistical tradition.

P1: TIX/XYZ
P2: ABC
JWST070-Preface
JWST070-Bartholomew

May 9, 2011

13:58

Printer Name: Yet to Come

PREFACE

xiii

The examples are an important part of the text. They are intended not only to
illustrate the mechanics of putting the theory into practice but they also bring to light
many subtleties which are not immediately evident from the formal derivations. This
is especially important in latent variable modelling where questions of interpretation
need to be explored in numerical terms for their full implications to be appreciated.
Many of the original examples have been retained because, although the data on
which they are based are now necessarily older, it is the point that the examples make
which is important. Where we felt that these could not be bettered, they have been
retained. But, in some cases, we have replaced original examples and added new ones
where we felt that an improvement could be made. However, all the original examples
have been recalculated using the newer software described in the Appendix.
There was a website linked to the second edition which has been discontinued.
There are two reasons for this. First, we have provided an appendix to this book which
gives details of the more comprehensive software that is currently available: the new
appendix has removed the need for the individual programs provided on the original
website. Secondly, it is now much easier to find numerical examples on which the
methods can be tried out. One convenient source is in Bartholomew et al. (2008) and
its associated website, where there are extensive data sets and some of the methods
are described in a form more suitable for users.

P1: TIX/XYZ
JWST070-Ack

P2: ABC
JWST070-Bartholomew

May 9, 2011

14:0

Printer Name: Yet to Come

Acknowledgements
Alan Stuart died in 1998, but his encouragement and support in getting the first edition
off the ground, when latent variable models were often viewed by statisticians with
suspicion, if not hostility, still leave the statistical community in his debt.
Much of the earlier editions remains, as does our debt to those who contributed
to them: Lilian de Menezes, Panagiota Tzamourani, Stephen Wood, Teresa Albanese
and Brian Shea, all once at the London School of Economics. Fiona Steele read a
draft of the new Chapter 9 and her comments have materially helped the exposition.
The anonymous advice garnered by our publisher, John Wiley, for this edition
was invaluable both in encouraging us to proceed and in defining the changes and
additions we have made.
We extensively used the IRTPRO software for producing output for the factor
analysis model for categorical variables. The authors of the software, Li Cai, Stephen
du Toit and David Thissen, have kindly provided us with a free version of the software,
and Li Cai in particular helped us resolve any software-related questions. We would
also like to thank Jay Magidson and Jeroen Vermunt for their help with Latent Gold
and Albert Maydeu-Olivares for sharing with us the UK data on Eysenck’s Personality
Questionnaire–Revised.
The material relating to Sir Godfrey Thomson’s work in Chapter 9 was covered in
much greater detail in a research project at the University of Edinburgh in which one of
us (DJB) was a principal investigator. References to relevant publications arising from
the project are included here. This project was financed as part of research supported
by the Economic and Social Research Council, grant no. RES-000-23-1246.
David J. Bartholomew
Martin Knott
Irini Moustaki
London School of Economics and Political Science
January 2011

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

1

Basic ideas and examples
1.1

The statistical problem

Latent variable models provide an important tool for the analysis of multivariate
data. They offer a conceptual framework within which many disparate methods can
be unified and a base from which new methods can be developed. A statistical
model specifies the joint distribution of a set of random variables and it becomes
a latent variable model when some of these variables – the latent variables – are
unobservable. In a formal sense, therefore, there is nothing special about a latent
variable model. The usual apparatus of model-based inference applies, in principle,
to all models regardless of their type. The interesting questions concern why latent
variables should be introduced into a model in the first place and how their presence
contributes to scientific investigation.
One reason, common to many techniques of multivariate analysis, is to reduce
dimensionality. If, in some sense, the information contained in the interrelationships
of many variables can be conveyed, to a good approximation, in a much smaller
set, our ability to ‘see’ the structure in the data will be much improved. This is the
idea which lies behind much of factor analysis and the newer applications of linear
structural models. Large-scale statistical enquiries, such as social surveys, generate
much more information than can be easily absorbed without drastic summarisation.
For example, the questionnaire used in a sample survey may have 50 or 100 questions
and replies may be received from 1000 respondents. Elementary statistical methods
help to summarise the data by looking at the frequency distributions of responses
to individual questions or pairs of questions and by providing summary measures
such as percentages and correlation coefficients. However, with so many variables it
may still be difficult to see any pattern in their interrelationships. The fact that our
ability to visualise relationships is limited to two or three dimensions places us under
strong pressure to reduce the dimensionality of the data in a manner which preserves
as much of the structure as possible. The reasonableness of such a course is often
Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

1

P1: TIX/XYZ
JWST070-01

2

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

evident from the fact that many questions overlap in the sense that they seem to be
getting at the same thing. For example, one’s views about the desirability of private
health care and of tax levels for high earners might both be regarded as a reflection
of a basic political position. Indeed, many enquiries are designed to probe such basic
attitudes from a variety of angles. The question is then one of how to condense the
many variables with which we start into a much smaller number of indices with
as little loss of information as possible. Latent variable models provide one way of
doing this.
A second reason is that latent quantities figure prominently in many fields to which
statistical methods are applied. This is especially true of the social sciences. A cursory
inspection of the literature of social research or of public discussion in newspapers or
on television will show that much of it centres on entities which are handled as if they
were measurable quantities but for which no measuring instrument exists. Business
confidence, for example, is spoken of as though it were a real variable, changes in
which affect share prices or the value of the currency. Yet business confidence is an
ill-defined concept which may be regarded as a convenient shorthand for a whole
complex of beliefs and attitudes. The same is true of quality of life, conservatism,
and general intelligence. It is virtually impossible to theorise about social phenomena
without invoking such hypothetical variables. If such reasoning is to be expressed in
the language of mathematics and thus made rigorous, some way must be found of
representing such ‘quantities’ by numbers. The statistician’s problem is to establish
a theoretical framework within which this can be done. In practice one chooses a
variety of indicators which can be measured, such as answers to a set of yes/no
questions, and then attempts to extract what is common to them.
In both approaches we arrive at the point where a number of variables have to
be summarised. The theoretical approach differs from the pragmatic in that in the
former a pre-existing theory directs the search and provides some means of judging the
plausibility of any measures which result. We have already spoken of these measures
as indices or hypothetical variables. The usual terminology is latent variables or
factors. The term factor is so vague as to be almost meaningless, but it is so firmly
entrenched in this context that it would be fruitless to try to dislodge it now. We
prefer to speak of latent variables since this accurately conveys the idea of something
underlying what is observed. However, there is an important distinction to be drawn.
In some applications, especially in economics, a latent variable may be real in the
sense that it could, in principle at least, be measured. For example, personal wealth is
a reasonably well-defined concept which could be expressed in monetary terms, but
in practice we may not be able or willing to measure it. Nevertheless we may wish to
include it as an explanatory variable in economic models and therefore there is a need
to construct some proxy for it from more accessible variables. There will be room
for argument about how best to do this, but wide agreement on the existence of the
latent variable. In most social applications the latent variables do not have this status.
Business confidence is not something which exists in the sense that personal wealth
does. It is a summarising concept which comes prior to the indicators of it which
we measure. Much of the philosophical debate which takes place on latent variable
models centres on reification; that is, on speaking as though such things as quality

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

BASIC IDEAS AND EXAMPLES

3

of life and business confidence were real entities in the sense that length and weight
are. However, the usefulness and validity of the methods to be described in this book
do not depend primarily on whether one adopts a realist or an instrumentalist view
of latent variables. Whether one regards the latent variables as existing in some real
world or merely as a means of thinking economically about complex relationships,
it is possible to use the methods for prediction or establishing relationships as if the
theory were dealing with real entities. In fact, as we shall see, some methods, which
appear to be purely empirical, lead their users to behave as if they had adopted a
latent variable model. We shall return to the question of interpreting latent variables
at the end of Chapter 9. In the meantime we note that an interesting discussion of the
meaning of a latent variable can be found in Sobel (1994).

1.2

The basic idea

We begin with a very simple example which will be familiar to anyone who has met the
notion of spurious correlation in elementary statistics. It concerns the interpretation
of a 2 × 2 contingency table. Suppose that we are presented with Table 1.1. Leaving
aside questions of statistical significance, the table exhibits an association between
the two variables. If A was being a heavy smoker and B was having lung cancer
someone might object that the association was spurious and that it was attributable
to some third factor C with which A and B were both associated – such as living in
an urban environment. If we go on to look at the association between A and B in the
presence and absence of C we might obtain data as set out in Table 1.2. The original
association has now vanished and we therefore conclude that the underlying variable
C was wholly responsible for it. Although the correlation between the manifest
variables might be described as spurious, it is here seen as pointing to an underlying
latent variable whose influence we wish to determine.
Even in the absence of any suggestion about C it would still be pertinent to ask
whether the original table could be decomposed into two tables exhibiting independence. If so, we might then look at the members of each subgroup to see if they
had anything in common, such as most of one group living in an urban environment.
The idea can be extended to a p-way table and again we can enquire whether it
can be decomposed into sub-tables in which the variables are independent. If this
were possible there would be grounds for supposing that there was some latent categorisation which fully explained the original association. The discovery of such a
Table 1.1 A familiar example.

B
B̄

A

Ā

Total

350
150

200
300

550
450

500

500

1000

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

4

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 1.2 Effect of a hidden factor.
C

B
B̄

C̄

A

Ā

Total

A

Ā

Total

320
80

80
20

400
100

30
70

120
280

150
350

400

100

500

100

400

500

decomposition would amount to having found a latent categorical variable for which
conditional independence held. The validity of the search does not require the assumption that the goal will be reached. In a similar fashion we can see how two
categorical variables might be rendered independent by conditioning on a third continuous latent variable. We now illustrate these rather abstract ideas by showing how
they arise with two of the best-known latent variable models.

1.3
1.3.1

Two examples
Binary manifest variables and a single binary
latent variable

We now take the argument one step further by introducing a probability model for
binary data. In order to do this we shall need to anticipate some of the notation required
for the more general treatment given below. Thus suppose there are p binary variables,
rather than two as in the last example. Let these be denoted by x1 , x2 , . . . , x p with
xi = 0 or 1 for all i. Let us consider whether the mutual association of these variables
could be accounted for by a single binary variable y. In other words, is it possible
to divide the population into two parts so that the xs are mutually independent in
each group? It is convenient to label the two hypothetical groups 1 and 0 (as with the
xs, any other labelling would serve equally well). The prior distribution of y will be
denoted h(y), and this may be written
h(1) = P{y = 1} = η

and

h(0) = 1 − h(1).

(1.1)

The conditional distribution of xi given y will be that of a Bernoulli random variable
written
P{xi | y} = πixyi (1 − πi y )1−xi

(xi , y = 0, 1),

(1.2)

where πi y is the probability that xi = 1 when the latent class is y. Notice that in this
simple case the form of the distributions h and P{xi | y} is not in question; it is only
their parameters, η, {πi0 } and {πi1 } which are unspecified by the model.

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

BASIC IDEAS AND EXAMPLES

5

For this model
f (x) = η

p


πi1xi (1 − πi1 )1−xi + (1 − η)

i=1

p


πi0xi (1 − πi0 )1−xi .

(1.3)

i=1

To test whether such a decomposition is adequate we would fit the probability distribution of (1.3) to the observed frequency distribution of x-vectors and apply a
goodness-of-fit test. As we shall see later, the parameters of (1.3) can be estimated by
maximum likelihood. If the fit were not adequate we might go on to consider three
or more latent classes or, perhaps, to allow y to vary continuously.
If the fit were satisfactory we might wish to have a rule for allocating individuals
to one or other of the latent classes on the basis of their x-vector. For this we need
the posterior distribution
h(1 | x) = P{y = 1 | x}
 p

 x
πi1i (1 − πi1 )1−xi
=η
f (x)
i=1




1−η
=1
1+
exp
η

p

xi ln
i=1

πi0
1 − πi0
+ (1 − xi ) ln
πi1
1 − πi1

.
(1.4)

Clearly individuals cannot be allocated with certainty, but if estimates of the parameters are available an allocation can be made on the basis of which group is more
probable. Thus we could allocate to group 1 if
h(1 | x) > h(0 | x),
that is, if
p

X =

xi {logit πi1 − logit πi0 }
i=1
p

>

ln{(1 − πi0 )/(1 − πi1 )} − logit η.

(1.5)

i=1

where logit u = ln{u/(1 − u)}. An interesting feature of this result is that the rule for
discrimination depends on the xs in a linear fashion. Here, this is a direct consequence
of the fact that the posterior distribution of (1.4) depends on x only through the linear
combination which we may denote by X. In that sense X contains all the relevant
information in the data about the latent variable. This is not peculiar to this example
but will turn out to be a key idea which is at the heart of the theoretical treatment of
Chapter 2.

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

6

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

It is worth emphasising again that much of the arbitrariness in the general approach
with which we started has been avoided by fixing the number of latent classes and
hence the form of the distribution h. There might, of course, be some prior grounds
for expecting two latent groups, but nothing is lost by the assumption because, if it
fails, we can go on to try more.

1.3.2

A model based on normal distributions

When x consists of metrical variables the writing down of a model is a little less
straightforward. As before, we might postulate two latent classes and then we
should have
f (x) = η

p


gi (xi | y = 1) + (1 − η)

i=1

p


gi (xi | y = 0),

(1.6)

i=1

where gi (xi | .) denotes the conditional density of xi given the particular value of y.
However, we are now faced with the choice of conditional distributions for xi . There is
now no natural choice as there was when the xs were binary. We could, of course, make
a plausible guess, fit the resulting model and try to justify our choice retrospectively
by a goodness-of-fit test. Thus if a normal conditional distribution seemed reasonable
we could proceed along the same lines as in Section 1.3.1. Models constructed in this
way will be discussed in Chapter 6.

1.4

A broader theoretical view

Having introduced the basic idea of a latent variable model, we are now ready to
move on to the general case where the latent variables may not be determined in
either number or form. As our primary concern is with the logic of the general model
we shall treat all variables as continuous, but this is purely for notational simplicity
and does not affect the key idea.
There are two sorts of variables to be considered and they will be distinguished as
follows. Variables which can be directly observed, also known as manifest variables,
will be denoted by x. A collection of p manifest variables will be distinguished
by subscripts and written as a column vector x = (x1 , x2 , . . . , x p ) . In the interests
of notational economy we shall not usually distinguish between random variables
and the values which they take. When necessary, the distinction will be effected by
the addition of a second subscript, thus xih will be the observed value of random
variable xi for the hth sample member and xh will be that member’s x-vector. The
corresponding notation for latent variables will be y and q, and such variables will
form the column vector y. In practice we shall be concerned with the case where q
is much smaller than p. Since both manifest and latent variables, by definition, vary
from one individual to another they are represented in the theory by random variables.
The relationships between them must therefore be expressed in terms of probability
distributions, so, for example, after the xs have been observed the information we

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

BASIC IDEAS AND EXAMPLES

7

have about y is contained in its conditional distribution given x. Although we are
expressing the theory in terms of continuous variables, the modifications required for
the other combinations of Table 1.3 on page 11 are straightforward and do not bear
upon the main points to be made.
As only x can be observed, any inference must be based on their joint distribution
whose density may be expressed as
f (x) =

h(y)g(x | y)dy,

(1.7)

Ry

where h(y) is the prior distribution of y, g(x | y) is the conditional distribution of x
given y and Ry is the range space of y (this will usually be omitted). Our main interest
is in what can be known about y after x has been observed. This information is wholly
conveyed by the conditional density h(y | x), deduced from Bayes’ theorem,
h(y | x) = h(y)g(x | y)/ f (x).

(1.8)

We use h for both the prior distribution and the conditional distribution, but which
one is meant is always clear from the notation. The nature of the problem we face is
now clear. In order to find h(y | x) we need to know both h and g, but all that we can
estimate is f . It is obvious that h and g are not uniquely determined by (1.7) and thus,
at this level of generality, we cannot obtain a complete specification of h(y | x). For
further progress to be made we must place some further restriction on the classes of
functions to be considered. In fact (1.7) and (1.8) do not specify a model, they are
merely different ways of expressing the fact that x and y are random variables that
are mutually dependent on one another. No other assumption is involved. However,
rather more is implied in our discussion than we have yet brought out. If the xs are
each related to one or more of the ys then there will be correlations among the xs.
Thus if x1 and x2 both depend on y1 we may expect the common influence of y1 to
induce a correlation between x1 and x2 . Conversely if x1 and x2 were uncorrelated
there would be no grounds for supposing that they had anything in common. Taking
this one step further, if x1 and x2 are uncorrelated when y1 is held fixed we may infer
that no other y is needed to account for their relationship since the existence of such
a y would induce a correlation even if y1 were fixed.
In general we are saying that if the dependencies among the xs are induced by a
set of latent variables y then when all ys are accounted for, the xs will be independent
if all the ys are held fixed. If this were not so the set of ys would not be complete and
we should have to add at least one more. Thus q must be chosen so that
g(x | y) =

p


gi (xi | y).

(1.9)

i=1

This is often spoken of as the assumption (or axiom) of conditional (or local) independence. But it is misleading to think of it as an assumption of the kind that could
be tested empirically because there is no way in which y can be fixed and therefore

P1: TIX/XYZ
JWST070-01

8

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

no way in which the independence can be tested. It is better regarded as a definition
of what we mean when we say that the set of latent variables y is complete. In other
words, that y is sufficient to explain the dependencies among the xs. We are asking
whether f (x) admits the representation
f (x) =

h(y)

p


gi (xi | y)dy,

(1.10)

i=1

for some q, h and {gi }. In practice we are interested in whether (1.10) is an adequate
representation for some small value of q. The dependence of (1.10) on q is concealed
by the notation and is thus easily overlooked. We do not assume that (1.10) holds;
a key part of our analysis is directed to discovering the smallest q for which such a
representation is adequate.
The treatment we have just given is both general and abstract. In the following
chapter we shall propose a family of conditional distributions to take the place of
gi (xi | y) which will meet most practical needs. However, there are several points
which the foregoing treatment makes very clear which have often been overlooked
when we come down to particulars. For example, once f (x) is known, or estimated,
we are not free to choose h and g independently. Our choice is constrained by the need
for (1.7) to be satisfied. Thus if we want to talk of ‘estimating’ the prior distribution
h(y), as is sometimes done, such an estimate will be constrained by the choice already
made for g. Similarly, any attempt to locate individuals in the latent space using the
conditional distribution h(y | x) must recognise the essential arbitrariness of the prior
distribution h(y). As we shall see, this indeterminacy is central to understanding what
is often called the factor scores problem.
A more subtle point concerns the interpretation of the prior distribution, h(y)
itself (we temporarily restrict the discussion to the one-dimensional case). The latent
variable is essentially a construct and therefore there is no need for it to exist in the
ordinary sense of that word. Consequently, its distribution does not exist either and it
is therefore meaningless to speak of estimating it!

1.5

Illustration of an alternative approach

In the foregoing development we constructed the model from its basic ingredients,
finally arriving at the joint distribution which we can actually observe. We might try
to go in the other direction starting from what we observe, namely f (x), and deducing
what the ingredients would have to be if f (x) is to be the end-product. Suppose, for
example, our sample of xs could be regarded as coming from a multivariate normal
distribution with mean vector μ and non-singular covariance matrix . We might
then ask whether the multivariate normal distribution admits a representation of the
form (1.10) and, if so, whether it is unique. It is easy to find one such representation
using standard results of distribution theory. Suppose, for example, that
y ∼ Nq (0, I)

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

BASIC IDEAS AND EXAMPLES

9

and
x | y ∼ N p (μ + y, ),

(1.11)

where  is a p × q matrix of coefficients and  is a diagonal matrix of variances. It
then follows that
x ∼ N p (μ,  + ),

(1.12)

which is of the required form. Note that although this representation works for
all q ≤ p there is no implication in general that a  and  can be found such
that  +  is equal to the given . Every model specified by (1.11) leads to a
multivariate normal x, but if q < p the converse is not true. The point of the argument
is to show that the model (1.11) is worth entertaining if the xs have a multivariate
normal distribution.
The posterior distribution of y is easily obtained by standard methods and it, too,
turns out to be normal. Thus


y | x ∼ Nq   −1 (x − μ), (  −1  + I)−1 ,

(1.13)

where  =  +  and q < p. The mean of this distribution might then be used
to predict y for a given x and the precision of the predictions would be given by the
elements of the covariance matrix.
Unfortunately the decomposition of (1.11) is not unique as we now show. In the
traditional approach to factor analysis this feature is met in connection with rotation,
but the point is a general one which applies whether or not x is normal.
Suppose that y is continuous and that we make a one-to-one transformation of the
factor space from y to v. This will have no effect on f (x) since it is merely a change
of variable in the integral (1.7), but both of the functions h and g will be changed. In
the case of h the form of the prior distribution will, in general, be different, and in
the case of g there will be a change, for example, in the regression of x on the latent
variables. It is thus clear that there is no unique way of expressing f (x) as in (1.10)
and therefore no empirical means of distinguishing among the possibilities. We are
thus not entitled to draw conclusions from any analysis which would be vitiated by
a transformation of the latent space. However, there may be some representations
which are easier to interpret than others. We note in the present case, from (1.11),
that the regression of xi on y is linear and this enables us to interpret the elements
of  as weights determining the effect of each y on a particular xi . Any non-linear
transformation of y would destroy this relationship.
Another way of looking at the matter is to argue that the indeterminacy of h leaves
us free to adopt a metric for y such that h has some convenient form. A normal scale
is familiar so we might require each yj to have a standard normal distribution. If, as
a further convenience, we make the ys independent as in (1.11) then the form of gi
is uniquely determined and we would then note that we had the additional benefit of

P1: TIX/XYZ
JWST070-01

10

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

linear regressions. This choice is essentially a matter of calibration; we are deciding
on the properties we wish our scale to have.
In general, if we find the representation (1.10) is possible, we may fix either h or
{gi }; in the normal case either approach leads us to (1.11).
If x is normal there is an important transformation which leaves the form of
h unchanged and which thus still leaves a degree of arbitrariness about {gi }. This
is the rotation which we referred to above. Suppose q ≥ 2; then the orthogonal
transformation v = My (M M = I) gives
v ∼ Nq (0, I),
which is the same distribution as y had. The conditional distribution is now
x | v ∼ N p (μ + M v, ),

(1.14)

so that a model with weights  is indistinguishable from one with weights M .
The effect of orthogonally transforming the latent space is thus exactly the same as
transforming the weight matrix. The joint distribution of x is, of course, unaffected
by this. In the one case the covariance matrix is  +  and in the other it is
M M + , and these are equal because M M = I.
The indeterminacy of the factor model revealed by this analysis has advantages
and disadvantages. So far as determining q, the dimensionality of the latent space,
is concerned, there is no problem. But from a purely statistical point of view the
arbitrariness is unsatisfactory and in Chapter 3 we shall consider how it might be
removed. However, there may be practical advantages in allowing the analyst freedom
to choose from among a set of transformations that which has most substantive
meaning. This too is a matter to which we shall return.
The reader already familiar with factor analysis will have recognised many of
the formulae in this section, even though the setting may be unfamiliar. The usual
treatment, to which we shall come later, starts with the linear regression implicit
in (1.11) and then adds the distributional assumptions in a convenient but more or
less arbitrary way. In particular, the essential role of the conditional independence
postulate is thereby obscured. The advantage of starting with the distribution of x is
that it leads to the usual model in a more compelling way but, at the same time, makes
the essential arbitrariness of some of the usual assumptions clearer. We shall return to
these points in Chapter 2 where we shall see that the present approach lends itself more
readily to generalisation when the rather special properties of normal distributions
which make the usual linear model the natural one are no longer available.

1.6

An overview of special cases

One of the main purposes of this book is to present a unified account of latent variable
models in which existing methods take their place within a single broad framework.

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

BASIC IDEAS AND EXAMPLES

11

Table 1.3 Classification of latent variable methods.
Manifest variables
Metrical

Categorical

Metrical

Factor analysis

Latent trait analysis

Categorical

Latent profile analysis

Latent class analysis

Latent
variables

This framework can be conveniently set out in tabular form as in Table 1.3. The
techniques mentioned there will be defined in later chapters.
It is common to classify the level of measurement of variables as nominal, ordinal,
interval or ratio. For our purposes it is convenient to adopt a twofold classification:
metrical and categorical. Metrical variables have realised values in the set of real
numbers and may be discrete or continuous. Categorical variables assign individuals
to one of a set of categories. They may be unordered or ordered; ordering commonly arises when the categories have been formed by grouping metrical variables.
The two-way classification in Table 1.3 shows how the commonly used techniques
are related.
It is perfectly feasible to mix types of variables – both manifest and latent. A
model including both continuous and categorical xs has been given by Moustaki
(1996) for continuous ys and by Moustaki and Papageorgiou (2004) for categorical
ys , and these are described in Chapters 6 and 7. When latent variables are of mixed
type we obtain what we shall later call hybrid models.

1.7

Principal components

We remarked above that the representation
 =  + 

(1.15)

is always possible when q = p. This follows from the fact that  is a symmetric
matrix and so can be expressed as
 = M M,
where  is a diagonal matrix whose elements are the eigenvalues, {δi }, of  and
M is an orthogonal matrix whose columns are the corresponding eigenvectors of .
Consequently, if we choose
 = M 1/2

and

 = 0,

P1: TIX/XYZ
JWST070-01

12

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

then (1.15) follows. The conditional distribution of x given y in (1.11) is now
degenerate with all the probability concentrated at the mean given by
x = μ + y = μ + M 1/2 y,

(1.16)

with x having been expressed exactly as a linear combination of independent standard
normal variables. The variables
y∗ = 1/2 y = M(x − μ)

(1.17)

are known as the principal components. We shall return to the subject of principal
components in Chapter 9 where other approaches will also be presented.

1.8

The historical context

Latent variable models and factor analysis are among the oldest multivariate methods,
but their origin and development lie almost entirely outside of mainstream statistics.
For this reason topics such as latent class analysis and factor analysis had separate
origins and have remained almost totally isolated from one another. This means that
they have been regarded as separate fields with virtually no cross-fertilisation. The
distinctive feature of the treatment of latent variable models given in this book is
that all are presented as special cases of the family described in Section 1.4 and
Chapter 2. The statistical approach to such matters, which we have followed, is to
begin with a probability model and then use standard statistical ideas to develop the
theory. This approach was foreshadowed in Anderson (1959) and taken up again in
Fielding (1977), but the first explicit statement, of which we are aware, of the general
approach of Section 1.4 was given in Bartholomew (1980) where it was applied to
categorical data. The general class of models derived from the exponential family,
which underlies the general linear latent variable model of Chapter 2, was proposed
in Bartholomew (1984). This, in turn, led to the first edition of the present book
(Bartholomew 1987) which demonstrated that most existing latent variable models,
and some new ones, could be subsumed under the general framework. The practical
potential of this idea has been worked out for an even wider field of applications in
Skrondal and Rabe-Hesketh (2004).
All of this means that the extensive literature, extending over almost a century,
presents a disjointed picture both notationally and conceptually. It may, therefore,
help the reader if we identify some of the key elements in their historical context. This
task is made the more necessary by the fact that much of the development has been
the work of psychologists and sociologists whose technical language often appears
unfamiliar or even meaningless to statisticians. (A good example of this is provided
by Maraun (1996) and the ensuing discussion.)
The origins of latent variable modelling can be traced back to the early part of
the last century in psychology, notably in the study of human abilities. The origins
of factor analysis, in particular, are usually attributed to Spearman (1904). However,

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

BASIC IDEAS AND EXAMPLES

13

the key idea was already present in Galton (1888) as the following quotation, drawn
to our attention by John Aldrich of the University of Southampton, shows:
Two variable organs are said to be co-related when the variation of the
one is accompanied on the average by more or less variation of the
other, and in the same direction. . . . It is easy to see that co-relation
must be the consequence of the variations of the two organs being partly
due to common causes. If they were wholly due to common causes,
the co-relation would be perfect, as is approximately the case with the
symmetrically disposed parts of the body.
Whether or not Spearman knew of this must remain a matter for speculation, but
an account of Spearman’s innovative role in the development of the subject can be
found in Bartholomew (1995b). Spearman was concerned with the fact that people,
especially children, who performed well in one test of mental ability also tended to do
well in others. This led to the idea that all individuals’ scores were manifestations of
some underlying general ability which might be called general ability or g. However,
the scores on different items were certainly not perfectly correlated, and this was
explained by invoking factors specific to each item to account for the variation in
performance from one item to another. The common and specific factors were thus
supposed to combine in some way to produce the actual performance. This idea
was expressed in what was originally known as Spearman’s two-factor model. He
supposed that any test score could be expressed as the sum of two parts – or factors
as he would have called them – thus:
Score = common factor + specific factor.

(1.18)

The common factor was named g by Spearman and it appears that he regarded it as a
characteristic of the brain, though he deliberately did not call it intelligence in order
to avoid the contemporary connotations of that term. If (1.18) were true it could be
deduced that the correlation matrix would have a particularly simple form which can
be characterised in a variety of ways. One of the most useful was based on the fact
that the tetrad differences were all zero. If a correlation matrix has typical element
ρij , the tetrad differences are given by
ρij ρi+h, j+k − ρi, j+k ρi+h, j = 0,
for all integer values of i, j, h, k within the bounds of the matrix.
It was noted quite early by Thomson (1916) that although the two-factor theory
was a sufficient condition for the tetrad differences to be zero, it was not necessary.
Thomson proposed what has become known as his bonds model as an alternative,
and we shall return to this in Chapter 9.
At the time the two-factor theory was proposed the idea of a probability model was
not current among either statisticians or psychologists. However, to clothe Spearman’s
idea in modern dress we must translate (1.18) into a statement about the distribution

P1: TIX/XYZ
JWST070-01

14

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

of random variables. It was probably Bartlett who first recognised that the ‘specific
factor’ of (1.18) had all the characteristics of an error term as used in statistics. From
there it is a short step to write (1.18) in the form
xi = μi + λi y + ei ,

(1.19)

where xi is a random variable representing the score on test i, y the common factor
and ei the factor specific to item i. (The notation has been chosen to conform to that
used in the early part of this chapter.) If y and the es are mutually independent the
correlation coefficient of xi and xj will have the form
ρij = ai a j ,

(1.20)

which is the zero tetrad condition in another guise. A a rough test of the plausibility
of the model can be made by inspection of the correlation matrix. It follows from
(1.20) that
ρih /ρij = ah /a j .

(1.21)

If we form the ratios on the left-hand side for any pair of columns of the matrix they
should be constant across rows. Another way of exhibiting the pattern in the matrix
is by observing that if ρij = 0 then
ρih ρhj /ρij = ah2 ,

(1.22)

for all pairs i and j. Since
ah2 = λ2h var(y)/{λ2h var(y) + var(eh )},
it is thus possible to estimate the ratio λ2h var(y)/var(eh ) and so determine the relative
contributions of the general and specific factors to the observed score.
If the correlation matrix fails to exhibit the simple pattern required by (1.20) it
is natural to add further general factors to the model in the hope of reproducing the
observed pattern more exactly.
Within psychology there was a new impetus in the 1930s from Thurstone and his
associates in Chicago. He advocated the desirability of obtaining solutions possessing
what he called ‘simple structure’, which meant replacing Spearman’s single general
factor by a cluster of distinct but possibly correlated factors representing different
abilities. The common or general factor of earlier work was thus seen as a sort of
average of the ‘real’ factors and thus of little substantive meaning. Much of the work
in this tradition was published in the then new journal Psychometrika, the fiftieth
anniversary of which was the occasion of a historical review by Mulaik (1986).
Although this was primarily concerned with the subject as it has developed in the
pages of that journal there are indications of what was happening on a broader front.

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

BASIC IDEAS AND EXAMPLES

15

Thurstone’s writings are now mainly of historical interest, but his book Multiple
Factor Analysis (Thurstone 1947) is still worth reading for its insight into the essential
character and purpose of factor analysis – something which easily becomes overlaid
by technical details. The same is true of Thomson’s The Factorial Analysis of Human
Ability. The first edition of this book appeared in 1939 and the fifth and final edition
in 1951 with a reprint in 1956; this anticipates many topics which have occupied
post-war theorists.
A second fiftieth anniversary, this time of the British Journal of Mathematical and
Statistical Psychology in 1997, was also marked by a special issue which contains
much relevant historical material. Within the psychological tradition there has been
a steady stream of texts at all levels, including Harman (1976), Mulaik (1972) and
McDonald (1985). Mulaik gives a comprehensive statement of the theory as it then
existed, interspersed with illuminating historical material. It is still a useful source
of many of the basic algebraic derivations. The book by Harman, which ran to three
editions, lays considerable emphasis on statistical methods and computational matters
but with less mathematical apparatus than Mulaik or McDonald. However, Mulaik
(2009a) is a second edition of the earlier book and brings the subject up to date from
the psychological perspective.
A third anniversary occurred in 2004, this time marking 100 years since the
publication of Spearman’s path-breaking paper of 1904. This was marked by a conference at the Thurstone Laboratory at the University of North Carolina at Chapel
Hill in 2004, the 16 contributions to which were published in Cudeck and MacCallum
(2007). These represent a diverse set of perspectives looking forward as well as back.
A major treatment of an entirely different kind can be found in the much older
book by Cattell (1978) who is at pains to emphasise the limitations of a purely
statistical approach to factor analysis as against its use as part of scientific method.
His exposition is made without any explicit use of models. It is worth adding that
factor analysis has been widely used, mainly by psychologists, for purposes for which
it was not intended. We shall note in Chapter 9 its use instead of cluster analysis and
multidimensional scaling in Q-analysis. Hotelling (1957) also drew attention to other
problems for which factor analysis had been used inappropriately.
The present book, like the first and second editions (Bartholomew 1987;
Bartholomew and Knott 1999), lies at the opposite pole to Cattell in that it gives
priority to modelling. A model serves to clarify the conceptual basis of the subject
and provides a framework for analysis and interpretation. In a subject which has
been criticised as arbitrary and hence too subjective, it is especially necessary to
clarify in as rigourous a way as possible what is being assumed and what can be
legitimately inferred. As already noted, the general approach used here goes back to
Anderson (1959), but his work pre-dated the computer era and the time was not ripe
for its exploitation.
The first major book-length treatment of factor analysis within the statistical
tradition was that of Lawley and Maxwell (1971). The first edition appeared in 1963
and the second, larger, edition followed in 1971; this remains a valuable source of
results on the normal theory factor model – especially in the area of estimation and
hypothesis testing. Basilevsky (1994) is a more recent comprehensive theoretical

P1: TIX/XYZ
JWST070-01

16

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

treatment, though Factor Analysis in its title is used with a broader connotation
than usual. Factor analysis in the sense used here is largely confined to two or
three chapters.
More recently the focus of research has shifted to linear structural relations
models. These originated with Jöreskog (1970) and may be regarded as a generalisation of the factor model. They incorporate not only the basic factor model but
also linear relationships among the ys (or factors). Our general framework can be
extended to include such models and an introductory account is given in Chapter 8.
A key reference is Bollen (1989), but work in this area is focused on three major software packages, LISREL (Jöreskog and Sörbom 2006), Mplus (Muthén and Muthén
2010) and EQS (Bentler 2008). All these authors have made many fundamental
contributions to the whole field of latent variable modelling and a selection of their
relevant publications is included in the References.
Statisticians have often preferred principal components analysis to factor analysis
and some have seen little value in the latter (see, for example, Chatfield and Collins
(1980), Hills (1977) and Seber (1984)). The idea of this technique goes back to
Pearson (1901) but it was developed as a multivariate technique by Hotelling (1933).
Although, as we shall see, it is quite distinct from factor analysis in that it does not
depend on any probability model, the close affinities between the two techniques
outlined above mean that they are often confused.
Latent structure analysis has its origins in sociology and concerns models in which
the latent variables are categorical. Its development has, until recently, been entirely
separate from factor analysis. It originated with Lazarsfeld as a tool for sociological
analysis and was expounded in the book by Lazarsfeld and Henry (1968). Here again
modern computing facilities have greatly extended the applicability of the methods.
A more up-to-date account is in Everitt (1984) and Heinen (1996). Langeheine
and Rost (1988) and Rost and Langeheine (1997) contain many examples of more
recent work.
A third, distinct, strand which has contributed to the current range of models comes
from educational testing. The manifest variables in this case are usually categorical,
often indicating whether an individual got an item right or wrong in a test. It is often
assumed that there is a single continuous latent variable corresponding to an ability
of some kind, and the practical objective is to locate individuals on some suitable
scale. The contribution of Birnbaum (1968) was a major step forward in this field,
which is now the scene of substantial research activity and some controversy. A good
introductory account is given by Hambleton et al. (1991). Much of the controversy has
centred upon the so-called Rasch model (see, for example, Rasch (1960), Andersen
(1980b) or Bartholomew (1996)) which has many appealing statistical properties.
However, a feature of much of this work on latent trait models, as they are called,
which tends to obscure their connection with the general family set out in Section 1.4,
is that the latent traits are not treated as random variables. Instead a parameter is
introduced to represent each individual’s position on the latent scale. Such a model
may be termed a fixed effects model by analogy with usage in other branches of
statistics. Its use would be appropriate if we were interested only in the abilities of
the particular individual in the sample and not in the distribution of ability in any

P1: TIX/XYZ
JWST070-01

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

BASIC IDEAS AND EXAMPLES

17

population from which they might have been drawn. It is not easy to find practical
examples where this would be the case, and for this reason such models will receive
only passing mention in the rest of the book. However, some of the later work on the
Rasch model (for example, Andersen (1973, 1977) and Andersen and Madsen (1977))
treats ‘ability’ as a latent variable and thus falls within our territory. In Chapters 2
and 4, in particular, we shall have occasion to note the simplifications that result from
specialisation to the Rasch model.
Fixed effects models have also been considered in factor analysis by Whittle
(1953) and Anderson and Rubin (1956), for example, but have not attracted much
practical interest. One of their chief disadvantages is that the number of parameters goes up in proportion to the sample size and this creates problems with the
behaviour of maximum likelihood estimators. However, there are circumstances in
which such methods are relatively simple and can be made to yield estimates of the
item parameters which are virtually the same as those derived from a random effects
model. They thus have a certain practical interest but in spite of a voluminous and
often polemical literature they are, from our standpoint, outside the mainstream of
theoretical development.
There is clearly a need for this whole area to be part of current statistical theory
and practice. The combination of a unified theoretical treatment of the whole field
with the flexibility and power of current computing facilities offers ample scope for
new developments in many directions. Some of the gaps in the theory, which were
particularly obvious when the first edition of this book (Bartholomew 1987) appeared,
have been filled largely by the advent of more adequate computing facilities. But much
more remains to be done, especially in introducing to new audiences the unifying
ideas on which this book is based.

1.9

Closely related fields in statistics

Latent variables have a long history in statistics, but their ubiquity has been partly
obscured by variations in terminology. Lee and Nelder (2009), for example, mention
random effects, latent processes, factor, missing data, unobserved future observations
and potential outcomes, among others. Even where there is no specific name, the idea
is present. For instance, the latent class model is an example of a finite mixture model.
There are many such statistical problems where an observed frequency distribution is
thought to be composed of a known or unknown number of samples from populations
with different distributions. A general treatment of such problems is given in
Titterington et al. (1985). Mixture models have also been used as a basis for cluster
analysis. In fact any analysis based on a latent variable model which assumes that the
latent space is categorical can be regarded as a method of cluster analysis.
The idea may be illustrated further by the example of accident proneness which
has received much attention in the statistical literature. If all members of a large
population have the same small risk of an accident, which is constant over time, then
the number of accidents per unit time will have a Poisson distribution. Often it is
found that the actual frequency distribution is over-dispersed. This can be explained

P1: TIX/XYZ
JWST070-01

18

P2: ABC
JWST070-Bartholomew

May 9, 2011

21:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

by supposing that the Poisson rate parameter is a random variable. Proneness is a
construct which cannot be directly observed and so, in our terminology, is a latent
variable. It is usual to assume that the rate has a gamma distribution and this leads to
a negative binomial distribution for the number of accidents.
More recently, latent variables have been used under the name hidden variables
especially in the context of discrete time series where Markov chains are used. This
term may have been borrowed from quantum mechanics where it is used to refer
to unobserved variables which can be invoked, by some, to explain the otherwise
random character of quantum behaviour. If the movement of a system among a finite
set of states is described by a Markov chain it may happen that the states of the chain
cannot be observed directly, but only via manifest indicators. The aim will be to
make inferences about the transition matrix of the underlying chain. An introduction
to such models is given in Zucchini and MacDonald (2009). This work extends the
scope of latent variable modelling into the domain of time series which should prove
a fertile field for new developments.
Another long-standing use of latent variables, though under another name, arises
in many applications in econometrics and related fields. Econometric models often
allow for what is termed unobserved heterogeneity. Discussion of this will be found in
the contributions of Chamberlain, Hoem, Trydman and Singer and Tuma to Heckman
and Singer (1985) where further references will be found. Another source is Tuma
and Hannan (1984).
It is clear that not all the types of latent variable mentioned have the same scientific
status. Missing values, for example, are observable in principle. They may even have
been observed and then lost or mislaid. With finite mixtures the position is more
equivocal. We may know that we have a mixture but do not have the means of
identifying the underlying populations. On the other hand, we may be exploring the
data to see whether the sample could have arisen from a mixture. The latent variables
with which we are concerned in this book might be better described as hypothetical
because, like intelligence, they are constructed for a particular purpose, which is to
provide a parsimonious and meaningful description of the data. However, we shall
follow well-established custom by using the term latent.

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

2

The general linear latent
variable model
2.1

Introduction

The principal aim of this book is to give a unified treatment of latent variable models.
This is to be achieved by deriving them as special cases of what we shall call the
general linear latent variable model (GLLVM). In a later section we shall show that
there are rather compelling reasons for choosing this general model, but first we
introduce it informally and use it to motivate the general treatment which follows.
Having introduced the model and justified our choice we shall then use it to establish
a number of properties shared by all members of the family. This is not only more
economical but it gives a greater insight into the strengths and weaknesses of all
latent variable models.

2.2

The model

We saw in Chapter 1 that a latent variable model consists of two parts. First there
is the prior distribution of the latent variables represented in (1.10) by the density
function h(y). This was seen to be essentially arbitrary and, as we shall argue later,
its choice is largely a matter of convention. In any event, most of the results of this
chapter do not depend on the choice of h(y) which, for most purposes, is therefore
irrelevant. The second element in the model is the set of conditional distributions of
the manifest variables {xi } given the latent variables. In (1.10) these were denoted by
gi (xi | y) (i = 1, 2, . . . , p) where the subscript i on g reminds us that the form of the
distribution can vary with i.
A convenient family of distributions which turns out to have many useful properties in other branches of statistics is the one-parameter exponential family. If we
Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

19

P1: TIX/XYZ
JWST070-02

20

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

suppose that the latent variables combine to determine the value of the parameter
then we may write
gi (xi | θi ) = Fi (xi )G i (θi ) exp(θi u i (xi ))

(i = 1, 2, . . . , p),

(2.1)

where θi is some function of y. The simplest assumption about the form of this
function is to suppose that it is a linear function, in which case we have
θi = αi0 + αi1 y1 + αi2 y2 + · · · + αiq yq

(i = 1, 2, . . . , p).

(2.2)

This is the general linear latent variable model. The term linear refers to its linearity
in the αs for reasons which will appear shortly.
The similarity between this model and the so-called generalised linear model
(GLIM) used in statistics will be immediately apparent. The chief difference is that
here we have a set of xs rather than a single dependent variable and here the ys are
unobservable. In view of this connection it may seem somewhat perverse of us to
have used x to denote the dependent variable and y the independent (or regressor)
variables when the usual convention is to reverse their roles. The reason for this will
become apparent when we come to discuss the posterior analysis of the model. In
that context we shall be predicting the ys given the xs, in which case our notation
conforms to the standard usage.
The exponential family of (2.1) includes the normal, gamma and Bernoulli distributions as special cases. If we allow xi and θ to be vector-valued it also includes
the multinomial distribution which we shall need for our treatment of categorical
variables. However, we continue for the moment with the scalar version in order not
to obscure the essential simplicity of the model by mathematical complications.

2.3

Some properties of the model

We noted in Chapter 1 that all the information about the latent variables in the vector
x is conveyed by the posterior distribution with density
h(y | x) = h(y)

p



gi (xi | y)

f (x).

(2.3)

i=1

Substituting for gi (xi | y) as given by (2.1), we find
h(y)
h(y | x) =  p 
i=1

∝ h(y)


p
Fi (xi )G i (θi ) exp i=1 θi u i (xi )
p
h(y) [Fi (xi )G i (θi )] exp i=1 θi u i (xi )dy

 p

p

i=1

i=1

q

yj X j,

G i (θi ) exp
j=1

(2.4)

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

21

p

where X j = i=1 αi j u i (xi ) ( j = 1, 2, . . . , q). The first important thing to notice here
is that the posterior distribution of y depends on x only through the q-dimensional
vector X = (X 1 , X 2 , . . . , X q ). This has dimension precisely equal to that of y. In the
Bayesian sense X is a minimal sufficient statistic for y. It represents a reduction in
dimensionality from the p-vector x to the q-vector X. We shall call these sufficient
statistics components. They are not principal components but they will play a similar role in our treatment to principal components which they include as a special
(degenerate) case of one of our models. The second important thing to notice is
that the reduction effected by (2.4) does not depend on the prior distribution of the
latent variables. This is reassuring in view of the arbitrariness in the choice of h(y)
and it demonstrates that the reduction of dimensionality is achieved, without loss of
information, and without any reference to the distribution of the latent variables.

2.4

A special case

Although we shall consider various special cases in detail later in this chapter and in
other chapters, it may help to fix ideas if we give a simple example now. Suppose
that xi is a Bernoulli random variable arising from putting a proposition to people and
inviting them to agree or disagree. If the answers are coded 1 (agree) or 0 (disagree)
the conditional distribution of xi may be written
g(xi | y) = πixi (1 − πi )1−xi (xi = 0 or 1)
= (1 − πi ) exp{xi logit(πi )},
where πi is the probability of agreeing. Comparing this with (2.1), we see
that θi = logit πi = ln{πi /(1 − πi )}, G i (θi ) = 1 − πi and u i (xi ) = xi . The GLLVM
is thus
q

logit(πi ) = αi0 +

αi j y j ,

(2.5)

j=1

p
and the components are given by X j = i=1 αi j xi . The logit here is known as the
link function. In the special case q = 1 this reduces to the familiar logistic latent trait
model with response function
πi (y) =

1
1+

e−αi0 −αi1 y

.

The model tells us that if we have a collection of scores {xi } all conforming to
this model then, if there is a single latent dimension underlying them, all that we
can learn about that latentvariable is summed up in the linear combination of the
p
responses given by X = i=1 αi1 xi . This is a remarkable result which is not true
for an alternative model which is often used in this context and which we discuss in
Section 4.3.

P1: TIX/XYZ
JWST070-02

22

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

This example also serves to highlight other questions which still have to be
answered. The parameters {αi j } are, of course, unknown, and will have to be estimated
from the data. Any method, and maximum likelihood in particular, will depend on
the joint distribution of the xs. Equation (1.10) shows that this does depend on the
prior distribution. Similarly, q is unknown and will have to be estimated. Although X
is sufficient for y, the theory so far tells us nothing about its relationship with y. If,
for example, one individual scores higher than another on X, what can we say about
their ranking on the y scale?
A final point to note is that there is nothing in the theory which requires y to be
a continuous variable. To take an extreme case, suppose q = 1 and that y is binary,
taking one of the two values (0, 1). We would then have a simple example of a latent
class model with two latent classes. Having constrained y to just two values in this
way the prior distribution is no longer completely arbitrary but must consist of two
probability masses located at y = 0 and y = 1. All that remains arbitrary in this
case is the location and scale of y. This illustrates in a simple manner the intimate
relationship which exists between the prior distribution and the linear model.

2.5

The sufficiency principle

The attractive feature of the GLLVM is that it leads to a q-dimensional sufficient
statistic which, in a precise sense, can be used as a proxy for the unobservable y.
Our choice of this model is thus justified retrospectively, but it is not clear from our
analysis whether this is the only family of models which admits this reduction. We
now show that there is a larger class of models which lead to a minimal sufficient
statistic. In determining this class we shall provide a more satisfactory basis for using
the GLLVM.
Let us begin with the idea of dimension reduction. If we wish to construct a
model with q latent variables the best we can hope for is to find q functions of x,
X 1 , X 2 , . . . , X q , say, such that the conditional distribution given X does not depend
on y. Then X is a minimal sufficient statistic. The first question to ask, therefore,
is whether there is a class of conditional distributions gi (xi | y) (i = 1, 2, . . . , p)
for which such a minimal sufficient set exists. The answer has been provided by
Barankin and Maitra (1963, Theorems 5.1, 5.2 and 5.3). They give necessary and
sufficient conditions which amount to the requirement that, subject to weak regularity
conditions, at least p − q of the gi shall be of exponential type with
q

gi (xi | y) = Fi (xi )G i (y) exp

u i j (xi )φ j (y).

(2.6)

j=1

This is more general than (2.1). For instance, taking all gi to be of the form (2.6), the
posterior distribution is easily found to be
q
p
h(y) i=1 [Fi (xi )G i (y)] exp j=1 X j φ j (y)
p
q
,
(2.7)
h(y | x) = 
h(y) i=1 [Fi (xi )G i (y)] exp j=1 X j φ j (y)dy

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

23

p

where X j = i=1 u i j (xi ) ( j = 1, 2, . . . , q), yielding X as the minimal sufficient
statistic. It should be noted that the functions {u i j (x)} may involve unknown parameters and then, in speaking of Xj as a statistic, we are extending the conventional
usage according to which sufficient statistics are functions of random variables only.
In effect, we are treating any such parameters as known. The term ‘sufficient’ has
exactly the meaning we wish to convey and so we will use it in this extended sense.
Special cases of our general result when the xs are binary go back at least to
Birnbaum (1968). From our perspective his object was to determine the class of
models for which the minimal sufficient statistic was a linear combination of the
manifest variables. Andersen (1973) similarly asked what class of model yields
a statistic which is parameter-free. In our notation this means that the functions
{u i j (xi )} of (2.7) involve no unknown parameters. The answer to this question led to
the Rasch model for which the sufficient statistic is the total score. Later Andersen
(1977) extended the result to polytomous xs.
The GLLVM of (2.2) arises as a special case when u i j (xi ) = αi j u i (xi ), in which
case what is often called the natural parameter of the exponential family is a linear
function of the latent variables. In order to see what further scope the model offers,
consider the following simple example. Let q = 2 and suppose that
xi | y ∼ N (y1 , y2 ).
This may be written in the form (2.6) by setting

Fi (xi ) = 1,
u i1 (xi ) = xi ,

−1/2

G i (y) = y2

u 2i (xi ) = xi2 ,

exp −

φ1 (y) = y1 /y2 ,

1 y12
,
2 y2
φ2 (y) = −1/(2y2 ).

The two sufficient statistics are
p

X1 =

p

xi ,
i=1

X2 =

xi2 .
i=1

The more general model therefore allows us to have the two parameters of the
normal distribution depending simultaneously on different latent variables. Given
the difficulties of handling one-parameter models which we shall encounter, there
seem likely to be severe practical problems in implementing the many-parameter
generalisations permitted by the more general result of (2.6). For the remainder of
the book we shall therefore confine the discussion to the general linear latent variable
model of (2.2).

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

24

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

2.6

Principal special cases

The bulk of the work on latent variable models is confined to a few special cases
of the GLLVM. It is convenient to introduce them at this point in order to make
the link between the general theory of this chapter and the detailed treatment to be
given in subsequent chapters. The classification of models in Table 1.3 provides the
starting point.
The latent trait model has already been illustrated in Section 2.4. It was obtained
by supposing that each of the xs was a Bernoulli random variable for which the
probability of a positive response depended on a single latent variable. In practice
this latent variable is often supposed to represent a human ability or attitude. The
sufficiency principle shows that to achieve the full dimensional reduction, the logit of
the response probability must be a linear function of the latent variable. The traditional
model generalises immediately to one with q latent variables when we have
q

logit πi (y) = αi0 +

αi j y j .

(2.8)

j=1

This model is the subject of Chapter 4 where it will be compared with other models
which do not achieve the reduction in dimensionality without loss of information.
If xi is a categorical variable with more than two categories a further generalisation
is required. This will be given in Chapter 5 and is achieved by allowing xi to be a
vector-valued indicator variable. Thus, if there are ci categories, xi would consist of
ci − 1 zeros and a single 1 occurring in the position corresponding to the category in
which the individual falls.
The latent class model can be thought of as a special kind of latent trait model in
which the latent variable is categorical. As shown in Section 2.4, if we have a single
binary latent variable, we arrive at a two-class latent class model. By allowing y to
be a vector-valued indicator variable we can introduce more latent classes into the
model. Latent class models will be treated in Chapter 6.
The remaining two cells of the Table 1.3 relate to models in which the xs are
continuous. The distinction between a latent profile and a factor model lies in the
form of the latent variable. The latent profile model can be thought of as a factor
model with a discrete (multivariate) prior distribution. (As we shall see later in Section
6.16.2, this fact leads to serious problems of identifiability.) For this reason we shall
not give an extended treatment to latent profile models.
Many variants of the factor model are possible. The one in common use is
discussed in Chapter 3 and is known as the normal linear factor model. We need
to take some care in deriving it from the GLLVM because the normal distribution
which is used for the conditional distribution is a two-parameter family and it can be
parameterised to give two distinct models.
We suppose first that
xi | y ∼ N (μi , σi2 )

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

25

or
1

1
gi (xi | y) = √
exp −
2
2π σi



xi2
μi2
2xi μi
−
+
σi2
σi2
σi2


.

(2.9)

If σi is known this may be written in the required form by setting
θi = μi /σi , u i (xi ) = xi /σi ,

G i (θi ) = e

− 12

μi2
σi2

1
1 xi2
.
Fi (xi ) = √
exp −
2 σi2
2π σi

The GLLVM is then
μi
= αi0 +
σi

p

αi j y j

(i = 1, 2, . . . , p),

j=1

according to which the latent variables are supposed to influence the mean only and
in an additive fashion. If the ys are assumed to be normal this is the standard linear
normal model referred to above. The sufficient statistics are
p

Xj =

αi j
i=1

xi
σi

( j = 1, 2, . . . , q).

An alternative model arises if we treat μi as known. The density of (2.9) may
then be written in the exponential form with
1
θi = 2 ,
σi

1
u i (xi ) = − (xi − μi )2 ,
2

√
θi
G i (θi ) = √ ,
2π

Fi (xi ) = constant.

The GLLVM is then a linear model for the inverse of the variance (the precision) and
the sufficient statistics are
Xj = −

1
2

p

αi j (xi − μi )2

( j = 1, 2, . . . , q).

(2.10)

i=1

This model would enable us to investigate the effect of latent variables on the variance
rather than the mean. The link function would have to be chosen so that θi was always
positive. No study of models of this kind appears to have been made but it serves as a
further example to show the range of models encompassed by our general approach.

2.7

Latent variable models with non-linear terms

It is worth pointing out that since nothing has been assumed so far about the prior
distribution there is nothing to prevent some of the latent variables being functionally

P1: TIX/XYZ
JWST070-02

26

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

related (polynomial terms of latent variables or interactions among latent variables).
This fact enables us to include a variety of models which are non-linear in the ys,
in particular polynomial or interaction models of the kind discussed by McDonald
(1962, 1967a,b,c, 1985) and Etezadi-Amoli and McDonald (1983). The examples
given are the typical middle item of an attitude scale and variables that measure
the intensity of attitude that are known to have a U-shape relation to anxiety. They
were viewed from the present angle in Bartholomew (1985) and Bartholomew and
McDonald (1986) but without taking full advantage of the flexibility which the present
approach offers, as the work by Rizopoulos and Moustaki (2008) shows. Kenny
and Judd (1984) also modelled latent variable interaction terms under the structural
equation modelling (SEM) framework. In their formulation, products of the observed
variables are used as indicators for the non-linear terms of the model. Their paper
led to many methodological papers discussing several aspects of non-linear latent
variable modelling mainly under the SEM approach – in particular, Jöreskog and
Yang (1996), Yang Jonsson (1997), Arminger and Muthén (1998), Bollen (1995),
Bollen and Paxton (1998) and Wall and Amemiya (2000, 2001). Bayesian estimation
methods have also been proposed by Zhu and Lee (1999), Arminger and Muthén
(1998), Lee and Zhu (2002), Lee and Song (2004a,b) and Song and Lee (2004, 2006).
Applications of latent variable models with non-linear terms in different disciplines
such as marketing, social psychology and political theory can be found in Schumacker
and Marcoulides (1998) and references therein.
Bartlett (1953), McDonald (1983), Etezadi-Amoli and McDonald (1983) and
Bartholomew (1985) have suggested graphical ways (bivariate plots of factor scores
and components) of investigating the necessity of non-linear terms. In addition,
Bartlett (1953) pointed out that the inclusion of the interaction between two latent
variables in the linear factor analysis model will produce the same correlation properties of the manifest variables as if a third genuine factor had been included. We
illustrate the point by considering a one-factor model with σi2 known in which
xi | y ∼ N (μi + αi y + βi y 2 , σi2 )

(i = 1, 2, . . . , p).

So far as finding the sufficient statistics is concerned, this is equivalent to a two-factor
model with latent variables y1 = y and y2 = y 2 . The components are then
p

X1 =

αi xi /σi
i=1

and
p

X2 =

βi xi /σi .
i=1

A two-dimensional statistic is needed to fix the path of the quadratic mean in the
one-dimensional latent space.

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

27

The GLLVM can be written to include non-linear terms in its parameter θi :
θi = αi0 + α i G(y)

(i = 1, . . . , p),

(2.11)

where G(y) is a vector function of y that can allow for any relationship between
the manifest and latent variables with an associated item-specific parameter vector
α i . For example, for the model with two latent variables and an interaction term,
G  (y) = (y1 , y2 , y1 y2 ). Although non-linear terms increase the complexity of the
exponential parameter θ , their inclusion might be necessary to describe more complex
relationships between the latent and observed variables. The current formulation has
certain numerical advantages. First, the model is based on conditional distributions
of observed variables given the latent variables that keep their form within the known
distributions of the exponential family of (2.1). Second, in the case of a two-factor
model with non-linear terms, the marginal distribution of x is computed using a double
integral instead of a higher-order integration, while allowing for more complex latent
structures compared with a two-factor model. The model with non-linear terms can
be fitted with maximum likelihood where the likelihood function is given in (2.19).
Regarding the estimation of standard errors, it has been recognised (Busemeyer
and Jones 1983) that the inclusion of non-linear terms could lead to potential
underestimation. Rizopoulos and Moustaki (2008) propose a robust estimation for
the standard errors of 
α based on the sandwich estimator (see also White (1982) for
more details):

α )K (
α )H −1 (
α ),
var(
α ) = C(
α ) = H −1 (

(2.12)

where H (
α ) is the observed information matrix evaluated at the maximum likelihood
estimates, and
K (
α ) = D(
α )D(
α )T ,
with D(
α ) denoting the score vector evaluated at the maximum likelihood estimates.
At this point the reader who is interested in a particular model can proceed to
the appropriate chapter. However, since many of the questions which will arise to do
with fitting, rotation and so on are common to most models they will be investigated
in the remainder of this chapter, which will thus serve also as a point of reference for
later use.

2.8

Fitting the models

Fitting the models includes estimating the parameters and testing goodness of fit.
For this purpose some assumptions must be made about the prior distribution h(y).
However, considerable progress can be made with some of the models using methods
which may be described as distribution-free.

P1: TIX/XYZ
JWST070-02

28

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

We note first that, without loss of generality, the ys may be assumed to have zero
means and unit standard deviations. Any shift in location can be absorbed into the
constant, αi0 , and any change of scale into the αi j ( j > 0). For the present we assume
the ys to be independent because this has advantages when we come to the matter of
interpretation.
The distribution-free method depends on the following result. Suppose that we
can find a transformation ξ (xi ) of xi such that
q

E{ξ (xi ) | y} = θi = αi0 +

αi j y j

(i = 1, 2, . . . , p).

j=1

Then
cov{ξ (xu ), ξ (xv )} = E [E {ξ (xu ) ξ (xv ) | y}] − Eξ (xu )Eξ (xv ) (u = v)
q

αu j αv j

=
j=1

and
q

var(ξ (xi )) =

αi2j + E var{ξ (xi ) | y}.

(2.13)

j=1

In matrix notation we may thus express the covariance matrix as
D(ξ ) = AA + V,

(2.14)

where A = {αi j } and V is a diagonal matrix of variances given by (2.13).
If this is possible, we can estimate the parameters by minimising the distance
between the theoretical covariance matrix, given by (2.14), and the sample matrix
estimated from the data matrix. Methods for doing this are available and will be
reviewed in the discussion of the normal linear model in Chapters 3 and 8.
The generality of this method depends, of course, on whether a suitable transformation can be found. In the case of the linear factor model where xi ∼ N (μi , σi2 ) it
follows at once that E(xi | y) = μi = θi , so that no transformation is needed.
If the conditional distribution of xi has the gamma form with
gi (xi | θi ) = θiνi xiνi −1 exp{−xi θi }/ (νi ),

(2.15)

then, provided that νi > 1, E(1/xi ) = θi and so we proceed to use the reciprocals of
the xs. (See Section 7.3.3 for further treatment of the gamma distribution.)
Even if there is no such transformation, as when νi = 1 in the foregoing example,
it may still be possible to find an approximation. One method of doing so utilises the

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

29

fact that for any distribution of exponential type
E{u i (xi ) | θi } = −

d
ln G i (θi ) = T (θi ),
dθi

(2.16)

say. This suggests trying the transformation
ξ (xi ) = T −1 {u i (xi )},
or some modification of it.
As an illustration, consider the case when xi is a Poisson variable with mean
mi (see also Section 7.3.2). The natural parameter is θi = ln m i , u i (xi ) = xi and
T (θi ) = exp θi , which suggests the transformation ξ (xi ) = ln xi . However, ln xi has
infinite expectation, but if we take
ξ (xi ) = ln(xi + c)
to avoid this, we find that with c = 12 ,

1 
y = θi + O(m i−2 ).
E ln xi +
2 


(2.17)

Thus, provided that the means are large enough, we could fit the model by least
squares using the covariances of the transformed variables.
A similar argument for binomial random variables when xi has parameters ni and
pi yields the transformation

ξ (xi ) = log

xi +



1
2

n i − xi −

,

1
2

(2.18)

which requires ni to be reasonably large. A very important special case which cannot
be handled by this means occurs when n i = 1 for all i. This requires the special
treatment given in Chapter 4.

2.9

Fitting by maximum likelihood

All models can, in principle, be fitted by maximum likelihood. This method also
provides asymptotic standard errors of the parameter estimates and a global test of
goodness of fit. The likelihood function is formed from the joint distribution of the
xs which, for the one-parameter exponential family given in (2.1), is

f (x) =

h(y)

p

i=1



p

Fi (xi )G i (θi ) exp


u i (xi )θi dy,

i=1

(2.19)

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

30

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

q
where θi = αi0 + j αi j y j (i = 1, 2, . . . , p). When dealing with individual special
cases we shall discover that the choice of h(y) seems to have very little effect on
the parameter estimates. This suggests that the choice is not critical. The following
heuristic argument provides some further support for this.
The marginal distribution of any particular xi may be written
f (xi ) = Fi (xi )E{G i (θi ) exp(u i (xi )θi )},
where the expectation is taken with respect to the distribution of θi which, of course,
is a linear function of y. Hence if q is not too small θi will be approximately normal,
by the central limit theorem, whatever the distribution of y. Thus the marginal distributions will be close to what they would be if the ys were normal. The near normality
of the univariate margins does not, of course, guarantee the approximate multivariate
normality of x but one would expect them to be close.

2.10

Fitting by Bayesian methods

In a similar manner, all models can be fitted using Bayesian estimation methods.
Bayesian inference is based on the posterior distribution of the vector of all random parameters conditionally on the data. First and second moments or percentiles
of the posterior distribution are used for computing point estimates and their corresponding standard deviations. Bayesian methods typically require computer-intensive
techniques that can transform a mathematically intractable problem of finding the
posterior distribution into a computationally feasible one. For parameter spaces of
high dimension, Bayesian inference uses methods such as Markov chain Monte Carlo
(MCMC). Monte Carlo integration is a long-established technique for numerically
approximating high-dimensional integrals by drawing values from a desired probability distribution. MCMC methods are more sophisticated methods for achieving
the same goal by constructing chains of those sampled values. One can then obtain
estimates as expectations, or modes, calculated from their empirical distribution.
The basic idea depends upon our being able to find a Markov chain defined on the
set of states, comprising all possible values of the unknown quantities, which has
as its steady-state distribution the desired posterior distribution. A single run gives
one sample value; repeated simulations will give a sample for the desired distribution. The key question is how to choose transition probabilities from the chain
(the kernel).
More specifically, MCMC methods are iterative algorithms that aim to construct
a Markov chain that can be used as a sample from a desired probability distribution.
The term ‘Markov chain’ is due to the fact that results from the current iteration
are used for the next iteration, resulting in a chain. MCMC methods avoid the
complexity of computing multiple integrals often required in the computation of
posterior distributions by taking draws directly from that distribution function. They
are approximate methods that might require a large number of steps before the
values sampled are from the desired distribution. MCMC methods can be seen as

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

31

an alternative to marginal maximum likelihood estimation shortly to be discussed in
Chapters 3 to 7. Marginal maximum likelihood estimation and MCMC methods have
been compared and found to give close if not identical parameter estimates (Kim
2001; Moustaki and Knott 2005; Wollack et al. 2002).
MCMC methods, such as the Metropolis–Hastings algorithm and its special case,
Gibbs sampling, (Gelfand and Smith 1990) have become popular for estimating the
parameters of latent variable models mainly because they simplify the estimation of
complex models; see, for example Tanner and Wong (1987), Albert (1992), Albert and
Chib (1993), Baker (1998), Johnson and Albert (1999), Patz and Junker (1999a,b),
Dunson (2000, 2003), Béguin and Glas (2001), Fox and Glas (2001) and Shi and
Lee (1998). Lee and Song (2003), Lee (2007) and Palomo et al. (2007) have also
proposed Bayesian estimation methods for structural equation models for continuous
and categorical responses. Lopez and West (2004) also looked at the problem of
estimating the number of factors in the classical factor analysis model using reversible
jump MCMC algorithms. Earlier, researchers such as Birnbaum (1969) and Owen
(1975) proposed Bayes estimates of ability parameters in an item response model
under the assumptions that the item parameters are known.
In a Bayesian estimation model, parameters are not considered fixed but are
stochastic random variables and so probability statements are made about those
parameters. The model is defined as the joint distribution over all unobserved
(parameters) and observed quantities (data). The way information is obtained about
the unobserved quantities is through the posterior distribution of the unobserved
quantities conditional on the data. The marginalisation of the posterior distribution
with respect to individual parameters of interest is done through Gibbs sampling
(Gelfand and Smith 1990) or the Metropolis–Hastings algorithm. We will consider
both those sampling techniques for factor analysis models. Albert (1992) applied
Gibbs sampling for obtaining parameter estimates of the two-parameter probit/normal
model, whereas Patz and Junker (1999b) used the Metropolis–Hastings algorithm
for estimating a family of item response theory models. Gibbs sampling is easier to
implement when one can obtain closed-form expressions for the conditional densities
of the model parameters. This is feasible in the probit/normal model as shown
in Albert (1992) but not in other item response theory models. To overcome the
limitation of Gibbs sampling one can use the Metropolis–Hastings algorithm within
Gibbs sampling. The Metropolis–Hastings algorithm introduces an efficient acceptance/rejection sampling at each Gibbs step that helps to sample from the ‘complete’
conditional densities.
Bayesian estimation has two main advantages. First, it provides information for
the whole distribution of the model parameters and latent variables; in particular,
the standard errors of the parameters. Secondly, it is appropriate when one wants
to incorporate prior information about the parameters (e.g. imposing limits on the
range of values for the factor loadings or for the variances of the error term in an
attempt to avoid Heywood cases; see also Section 3.12.3). However, the implications of rotation (see Section 2.11) and label switching have not been thoroughly
investigated under the Bayesian framework and workable solutions still need to be
given. As with rotation, any random permutation of the set of factors leaves the joint

P1: TIX/XYZ
JWST070-02

32

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

distribution of the observed variables unchanged, which often leads to multimodal
posterior distributions for the parameters of interest. To our knowledge no
research has been published that addresses label switching in latent variable models. For the classical factor analysis model, restrictions similar to those applied for
model identifiability have usually been adopted as a practice for avoiding label
switching.
A sketch of estimation in the Bayesian framework
Let us denote by v = (y , α  ) the vector with all the unknown parameters including
now the vector of latent variables, where the vector α contains all the parameters that
appear in the parameter θi of (2.2) for all i. The joint posterior distribution of the
parameter vector v is
h(v | x) = 

g(x | v)ψ(v)
∝ g(x | v)ψ(v),
g(x | v)ψ(v)dv

(2.20)

which is the likelihood of the data multiplied by the prior distribution (ψ(v)) of all
model parameters including the latent variables and divided by a normalising constant.
Calculating the normalising constant requires multidimensional integration that can
be very heavy computationally, if not infeasible. As will be explained in Chapter 4,
MCMC techniques make use of the joint posterior distribution possible without
having necessarily to compute the normalising constant. We shall
qtake the form of
g(x | v) from the exponential family with parameter θi = αi0 + j=1 αi j y j already
defined in (2.1) and (2.2) respectively. The main steps of the Bayesian approach are
as follows:
1. Inference is based on the posterior distribution, h(v | x), of the unknown
parameters, α and y, conditional on the observations x. Depending on the
model fitted, the form of the posterior distribution can be very complex.
2. The mean, mode or any other percentile vector, and the standard deviation of
the posterior distribution and h(v | x) can be used as an estimator of v and its
corresponding standard error.
3. In general, we may use the posterior mean E(ψ(v) | x) as a point
 estimate of a
function of the parameters ψ(v), where E(ψ(v) | x) = · · · ψ(v)h(v | x)dv.
4. Analytic evaluation of the above expectation is impossible. Alternatives
include numerical evaluation, analytic approximations and Monte Carlo
integration.
Markov chain Monte Carlo
To avoid the integration required in the posterior expectation, Monte Carlo integration
is used to approximate that integral by a sample average of quantities calculated by
the method. R samples are drawn from the posterior distribution of all the unknown

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

33

parameters, h(v | x). Then the expectation over the posterior is approximated by
the average
1
R

R

ψ(v(r ) | x).
r =1

The samples drawn from the posterior distribution do not have to be independent.
Samples are drawn from the posterior distribution through a Markov chain with
h(v | x) as its stationary distribution. Algorithms such as the Gibbs sampler and the
Metropolis–Hastings algorithm are used to get the unique stationary distribution.
We start by defining a Markov chain M0 , M1 , . . . with states Mk = (vk ) =
k
(y , α k ). We proceed by simulating states from the Markov chain, and those simulated states will be used to make inferences about the parameters (y, α). Under
some regularity conditions and as the number of draws k increases, the distribution
of state Mk will converge to the chain’s stationary distribution π (y, α). The aim is
to define the state Mk in such a way that π (y, α) will be the desired distribution
h(y, α | x). The stationary distribution π (y, α) satisfies

t[(y0 , α 0 ), (y1 , α 1 )]π (y0 , α 0 )d(y0 , α 0 ) = π (y1 , α 1 ),

(2.21)

where t[(y0 , α 0 ), (y1 , α 1 )] is the transition kernel defined as
P(Mk+1 = (y1 , α 1 ) | Mk = (y0 , α 0 )).
The success of the whole process depends on the choice of the transition kernel.
There is always a ‘burn-in’ period where the first k observations are thrown away
and the remaining observations are used to make inference since they will be, in
theory, draws from the desired posterior distribution h(y, α | x). The choice of the
transition kernel depends on the sampling scheme used, and this will be illustrated in
Chapter 4.

2.11

Rotation

If y is continuous, no latent variable model is unique. By this we mean that there
are other models which lead to exactly the same joint distribution of the observable
variables and hence are empirically indistinguishable from the one in question. This
is easily seen by noting that any transformation of the latent variable in (1.7) leaves
f (x) unchanged. However, in general, such a transformation changes both the prior
distribution and the conditional distribution. In the special case of the GLLVM
the natural parameter of the conditional distribution continues to be linear in the
parameters but not in the latent variables.
It is important to appreciate the full implications of this simple fact, so we pursue
it with the logit model for binary data before proceeding with the main topic of this

P1: TIX/XYZ
JWST070-02

34

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

section. For the case of a single latent variable we have
logit πi (y) = αi0 + αi1 y

(i = 1, 2, . . . , p).

(2.22)

Suppose we now transform to z = H (y), where H is the cumulative distribution
function of the prior distribution. Then instead of a model with prior h(y) and
response function given by (2.22) we have one with a uniform prior on (0, 1) and
response function
logit πi (z) = αi0 + αi1 H −1 (z) (i = 1, 2, . . . , p).

(2.23)

Both models lead to exactly the same distribution of the xs. The second version is of
some independent interest but is only one of infinitely many which can be generated
in this manner. One other, very simple, transformation is to take z = −y. This has an
effect equivalent to changing the sign of αi1 , thus drawing our attention to the general
fact that the signs of the αs can be changed without affecting the joint distribution.
This example makes it clear that the prior and the response function have to be
considered as a pair. For any change by transformation we make in one, there is a
compensating change we can make in the other which leaves the joint distribution
unchanged. It is in this sense that the prior is arbitrary. It means, for example, that it
is meaningless to talk about estimating one member of the pair without first fixing
the other. This argument applies equally to any member of the GLLVM where the
natural parameter takes over the role of the response function. It also applies however
many latent variables there may be.
In general, transformations may exist which leave the form of the prior distribution
and of the natural parameter unchanged. In the example given above this would be
the case for the transformation z = −y if the prior were symmetrical. This example
is, in fact, a very special case of an orthogonal rotation which we now consider.
If we start with the GLLVM given by
θ = Ay

(2.24)

and make the non-singular transformation z = My then
θ = AM−1 z.

(2.25)

z ∼ N (0, MM ).

(2.26)

If y ∼ N (0, I) then

If the transformation is orthogonal MM = I and hence y and z have the same joint
distribution, as we noted in Section 1.5. We thus have two indistinguishable models
with the same prior but different θ s, both linear in the latent variables. This means
that, with a multivariate normal prior, we cannot distinguish either conceptually or

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

35

empirically between a GLLVM with loading matrix A and one with AM (noting
that M = M−1 for an orthogonal transformation). This result depends upon the
assumption of a normal prior. Any orthogonal transformation takes the standard
normal prior in which the ys are independent into a transformed prior of exactly
the same form. The normal prior is the only form for which this is true. This is
a consequence of what is sometimes called Maxwell’s theorem (see, for example,
(Feller 1966, corollary on p. 77)). In the bivariate case this states that if the unrotated
and rotated ys are both to be independent they must also be normal. Normality and
independence thus go together. If this is to be true for all ys in the multivariate case
it must be true for every pair. Thus the normal prior is special in the sense that it is
necessary to guarantee invariance under rotation.
Viewed geometrically, the transformation is an orthogonal rotation in the latent
variable space and so the generation of alternative solutions by this means is known as
rotation. Rotation assumes a very important role when we come to the interpretation
of latent variables and it has played a central part in the classical treatment of
the normal factor model. The foregoing discussion shows that rotation is a much
more general concept, and we shall see it at work in relation to other models in
later chapters.
There are two further points to be added. If, instead of assuming y to be standard normal, we supposed y ∼ N (0, ) then any non-singular transformation, not
necessarily orthogonal, would have yielded
z ∼ N (0, MM );

(2.27)

that is, the transformed prior would have belonged to the same family as the initial
prior. This is an example of an oblique rotation which, as we shall see later in
Chapter 3, leads to a wider range of options for interpretation.
Secondly, we note that the equivalence of the initial and rotated priors depends
on the assumed normality of y. If that assumption is not made the two models will
be distinguishable conceptually if not empirically. Even so, important features of the
distribution may be unaffected – in particular, the covariance structure of the xs.

2.12

Interpretation

Interpretation is concerned with naming the latent variables – that is, with linking
them with theoretical constructs in the substantive field to which the data relate.
There are two, closely related, approaches to this question which can be used with
any member of the GLLVM family. They can be elaborated in various ways to exploit
the characteristics of particular models, but here we shall confine ourselves to those
aspects which are common to all.
The first approach is along the lines of traditional factor analysis in terms of the
αi j which in this context are known as the factor loadings. A large value of αi j means
that the jth factor exerts a big influence on the ith manifest variable. By identifying

P1: TIX/XYZ
JWST070-02

36

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

the set of manifest variables for which yj is an important determinant we might hope
to find an interpretation for yj . In essence we ask what it is that these variables have in
common. It often happens that the dominant latent variable has a large influence on
all manifest variables, in which case it is sometimes called a general factor. In tests
of ability, for example, general ability is indicated by a tendency to score similarly
on all items.
The idea of ‘looking for what is common’ to a set of manifest variables is central
to interpretation. We begin with defining the notion of simple structure for a loading
matrix because this is particularly easy to interpret. Then we consider how such a
pattern might be achieved by rotation.
As an example, consider the following matrix of loadings:
⎡
+
⎢+
⎢
⎢+
⎢
⎢.
⎢
⎢.
A=⎢
⎢.
⎢
⎢.
⎢
⎢.
⎢
⎣.
.

.
.
.
+
+
+
+
.
.
.

⎤
.
.⎥
⎥
.⎥
⎥
.⎥
⎥
.⎥
⎥
.⎥
⎥
.⎥
⎥
+⎥
⎥
+⎦
+

(2.28)

where + denotes a large positive loading and · a small loading. Presented with such
a pattern, we would seek to identify y1 with what is the common influence on x1 ,
x2 and x3 ; y2 with the common influence on x4 , x5 , x6 , x7 ; and y3 with the common
influence on x8 , x9 and x10 . If such a pattern occurred we would have partitioned the
manifest variables into three non-overlapping groups each of which was indicative
of a different latent variable. The matrix is then said to have simple structure.
In practice, of course, it would be rare to find simple structure in such a clear-cut
form, but given that there is an unlimited number of rotations to choose from it might
be possible to find one which provides a reasonable approximation. A number of
algorithms for this purpose are in common use in factor analysis, and we shall meet
some of them in Chapter 3. For the moment we shall give a simple method which
will often serve the purpose.
If we had a matrix of the form A in which the small loadings were all precisely
zero it would follow that A A was diagonal. Given an arbitrary A, we now show how
to find a rotation such that the transformed matrix satisfies this diagonality condition.
Any p × q loading matrix can be expressed in the form
A = MDN ,

(2.29)

where D is a q × q diagonal matrix of singular values, M is a p × q matrix with
orthogonal columns and N is a q × q orthogonal matrix. This is known as the singular

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

37

value decomposition of A (we shall meet this again in Chapter 9). The matrix Ã = MD
is an orthogonal rotation of A (since it is obtained by post-multiplying Ã by the
orthogonal matrix N ). It also has the diagonality property because
Ã Ã = DM MD = D2 ,
since M M = I. An alternative means of finding Ã from A is provided by the fact that
the elements of D2 are the non-zero eigenvalues values of A A and M consists of the
associated eigenvectors. Facilities for making this calculation are readily available in
many software packages, for instance R. It should be noted that what we have shown
is that we can always find a rotation which makes Ã Ã diagonal and that this is also
a property of any loading matrix which has simple structure. It does not follow that
the transformed matrix will necessarily have simple structure. However, if it does our
problem is solved.
The second approach to interpretation starts with the components and is similar
in spirit to the usual method of interpreting principal components. It was presented
in the first edition (Bartholomew 1987) as a way of interpreting factors in the normal linear factor model, but it is quite general. We have shown that the GLLVM
leads to sufficient statistics (components) which contain all the information about
the latent variables. When interpreting the components we are then, in a sense, interpreting the corresponding latent variables. For the GLLVM family of models the jth
component is
p

Xj =

αi j u i (xi ).

(2.30)

i=1

The question we now address is what Xj measures. If u i (xi ) ∝ xi , as for the logit
model for binary data and the normal factor model, the component is simply a linear
combination of the xs. The interpretation of Xj – and hence yj – then depends on the
signs and magnitudes of the αs. If αi j is large and positive then the larger xi , the larger
Xj – and conversely if αi j is large and negative. If A had simple structure Xj would
be essentially a weighted average of a subset of the variables. Such an average is a
natural way of measuring what they have in common.
The idea of rotation carries over to the interpretation of the Xs. The set of sufficient
statistics X is not unique. Any one-to-one transformation is also sufficient and this
includes, of course, orthogonal linear transformations. We could therefore search for
simple structure via this route using the same method as before.
In essence then, we see that the two approaches are equivalent, though the
connection is a little less direct if u i (xi ) is not proportional to xi . The advantage of the
approach based on X is that it can legitimately be regarded as a sensible measure in
its own right. Sums or weighted averages of manifest variables have often been used
as measures without any attempt to validate them by reference to a probability model.
Users who might be cautious about adopting the full latent variable framework

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

38

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

might nevertheless be prepared to adopt component-based measures on purely
pragmatic grounds.

2.13

Sampling error of parameter estimates

Since the interpretation of the latent variables depends on the estimated parameter
values it is important to know how precisely they have been estimated. For most practical purposes it will suffice to have estimates of their standard errors. The traditional
approach, at least for maximum likelihood estimates, is to obtain asymptotic standard
errors from the second derivatives evaluated at the maximum of the likelihood. An
alternative is to use repeated sampling methods such as the bootstrap. Until recently,
the latter method has been prohibitively expensive in computing time but this is
changing rapidly and we anticipate that this will become the preferred method. We
shall deal with the technical details when we come to discuss individual models in
later chapters. However, there is one conceptual issue which is common to all models
of the family we are considering.
If there are two or more latent variables we have seen that there is no unique set of
estimates. Rather, there is a set of values which may be obtained from one another by
orthogonal rotation as described above. This requires us to think carefully about what
we mean when we ask for an estimate of sampling variability. Suppose we were to
fix the parameter values in the model and to generate a random sample of data. This
data set would then yield a set of estimates in the parameter space all having the same
likelihood. Repeated sampling in this manner would therefore produce a collection of
such sets and our problem would be to describe their collective variation. Variation of
points in a parameter space can be summarised in terms of variances and covariances
but it is not clear how best to describe the variation of sets.
Two avenues appear to be open. One is to choose some typical member of the set
and to describe the variation of the set in terms of the variation of that member. This
can conveniently be done by specifying constraints satisfied by only one member of
the set. In the GLLVM this can be achieved by requiring the matrix A A to be diagonal,
and fixing the sign for one loading for each factor. As we have seen, such solutions may
facilitate the interpretation. An alternative is to specify a maximisation procedure and
regard the member of the optimal set to which it leads as the representative member.
In either case we can estimate the standard errors of other members of the set using
the linear relationships which link all members of the optimal set.
The second avenue is to describe sampling variation in terms of parameters which
are invariant over the optimal set. For example, in the general model the elements
of the matrix AA are invariant under orthogonal rotation. Any functions of these
elements will share the same property. Although standard errors of such parameters
will tell us something about how well the model has been estimated, they may not be
very helpful in the matter of interpretation. This is not necessarily so, as we shall see
when we consider the normal linear factor model in more detail in Chapter 3. In that
case the parameters, known as communalities, are invariant under rotation and, as we
shall see, they give us useful information about the importance of individual variables.

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

2.14

39

The prior distribution

We have already noted that for many purposes the form of the prior distribution is
irrelevant. Nevertheless there are two purposes for which it is necessary to make
some assumption about the prior. The first is for estimating the parameters of the
model by a method such as maximum likelihood which depends on f (x). The second
is for finding the posterior distribution h(y | x) and quantities derived from it such as
E(y | x) and var(y | x).
We noted in Section 2.8 that, without loss of generality, we may fix the mean and
variance of the prior. Thus if in the model
θi = αi0 + αi1 y1 + αi2 y2 + · · ·

(i = 1, 2, . . . , p),

(2.31)

we make the transformation
z j = aj yj + bj,
the model becomes


b1
b2
αi1 z 1
αi2 z 1
θi = αi0 − αi1 − αi2 · · · +
+
+ ···.
a1
a2
a1
a2

(2.32)

This is statistically indistinguishable from (2.31) and shows that changes in the
location or scale of y can be absorbed into the αs. As a matter of convention we
shall usually suppose the ys to have zero mean and unit variance. It is, therefore, only
through the effect of its shape that the prior can influence the parameter estimates. We
have already seen in Section 2.8 that there are methods of fitting which depend only
on the second moment properties and these are therefore independent of the prior
distribution. This suggests that the effect of the shape of the prior on the estimates
might be slight, and we shall meet empirical evidence for this later.
The most obvious way to deal with the problem of the prior is to treat it as
unknown and then to estimate it. This can only be attempted if we first fix the form
of θ , otherwise we run into the inherent indeterminacy discussed in Section 2.11. A
natural choice is to suppose that θ is linear in the ys, as well as the αs. In that case
we can, in principle, estimate the prior either parametrically, by choosing a family
of distributions and estimating their shape parameters, or non-parametrically. In the
latter case – see, for instance, Tzamourani and Knott (2002) – we would approximate
h(y) by a discrete distribution defined on a grid of points. Experience with estimating
mixing distributions in general statistical practice suggests that it may be difficult to
do this with adequate precision. Difficulty in estimating the prior is a consequence of
its weak influence on the joint distribution of x noted above. For estimation purposes
at least, there appears to be little at stake in the precise choice of h(y) and the
conventional choice of a standard normal prior is both convenient and innocuous.
We shall consider analysis based on the posterior distribution in the following
section where it will appear that the effect of the prior is again slight. However, it is

P1: TIX/XYZ
JWST070-02

40

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

worth asking first whether there is anything implicit in our specification of the model
which might constrain our choice. There are two points to be made. First, the range
of y is not, in general, arbitrary. For example, in the logit model with a single y,
logit πi (y) = αi0 + αi1 y,

(2.33)

y must range over the interval (−∞, +∞) if πi (y) is to span the interval (0, 1). If we
wish all possible values of πi (y) to be admissible, the range of y must be unrestricted.
If the left-hand side of (2.33) were to be a quantity like a rate or a variance, as in the
case of the gamma distribution, the range of y would have to be constrained to make
the right-hand side non-negative.
A second, more powerful, constraint arises from our discussion of rotation. For
it to be invariant under rotation we saw that the prior distribution had to be standard
normal. Thus if we wish to have the rotational properties that are linked to the normal
prior we are constrained in our choice to that prior.
Finally, there are a number of loosely related considerations of a more subjective
kind which bear upon the question. In some fields, notably ability and attitude testing,
the intuitive idea of the underlying scale seems to carry with it some sense of how
individuals should be spaced along it. Heinen (1996, p. 105), for example, remarks
that the normal prior in attitudinal data ‘is often not very appropriate with regard to
the population distribution’. Goldstein (1980) has made a similar point in relation to
ability measurement in proposing the complementary log function in place of the logit
in (2.33). Since such judgements cannot be substantiated empirically they must be
understood as something implicit in the meaning of the terms ‘attitude’ or ‘ability’ to
which the theory is required to give expression. We note that any monotonic stretching
or shrinking of the scale is allowable within the framework we have adopted. In fact,
as we shall see in the next section, only an ordinal scaling can be justified empirically.
A related point concerns the question of what it is that the prior distribution
physically represents. Hitherto we have implicitly regarded it as the frequency distribution of the latent variable in some population from which the individuals under
consideration have been sampled. The ‘ability’ or ‘attitude’ is thus regarded as a fixed
‘property’ of the individual: h(y) describes how it varies in the population. Heinen
(1996, p. 167) calls this the random score or structural model. Adopting a specific
form for h(y), such as the standard normal, then amounts to an agreement to measure
y on a scale which renders its distribution normal. Assuming that y is everywhere
continuous, this can always be done.
The whole of the argument thus far is predicated on the assumption that we
are dealing with a single population. All inferences we make are relative to that
population. If we wish to make comparisons between populations a new set of
questions arise. Suppose, for example, that we had two populations with normal
priors but widely different locations. We could estimate αs for samples from each
population separately and scale individuals but each scaling will be with reference to
its own population: it will tell us nothing about differences between populations. We
take up this point in Section 2.16.

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

41

An alternative way of regarding h(y) is to use what Holland (1990b) has called
the stochastic subject rationale. This approach treats all individuals as inherently the
same and regards y as a random response to the stimulus of the test item or question.
It is latent because it cannot be observed directly but only indirectly through an
associated set of variables, x. Interest is focused on y, rather than x, because it is, in
some sense, considered to be more fundamental. On this interpretation we would be
likely to have little interest in knowing what y was on any particular occasion but
more in the form of its distribution.

2.15

Posterior analysis

We use this term to refer to any analysis relating to the distribution of y after x has
been observed. It is concerned with making statements about an individual’s latent
location given their x. All of the relevant information needed for this is contained in
the posterior distribution h(y | x). In factor analysis what we call posterior analysis
has traditionally included the ‘problem of factor scores’ which is concerned with how
to find an ‘estimate’ of y for a given x. The literature on this topic is extensive and
controversial as the recent debate on the subject (Maraun 1996) bears witness. In our
view, a great deal of needless confusion has arisen which is avoided by adopting the
general approach to latent variable modelling used in this book.
For the latent class model where y is a finite set of points representing the c latent
classes, inference about y given x is a straightforward application of Bayes’ theorem.
If the prior probabilities are {η j } then the posterior probability that an individual with
observed x belongs to class j is

h( j | x) = η j

p



gi (xi | j)

f (x)

( j = 0, 1, 2, . . . , c − 1).

(2.34)

i=1

A special case was given in (1.4). Individuals can be allocated to the class for which
the estimate of this probability is greatest. We have seen that if {gi (xi | j)} is a member
of the exponential family, the posterior probability will depend on x only through a
linear function which therefore acts as a linear discriminant function.
If the latent space is continuous we face the problem that the posterior distribution
is proportional to the unknown h(y). We can, of course, circumvent the difficulty
by making an assumption about θ , but this merely shifts the arbitrariness from one
function to another. However, it turns out that a surprising amount can be said without
making any arbitrary assumption about h(y). This is made possible by the following
theorem, where we assume that the latent variable is arranged so that in (2.7) we have
φ j (y) = y j .
Theorem 2.15.1 The vector E[y | x] is the gradient of a convex function of the
component vector X.

P1: TIX/XYZ
JWST070-02

42

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Proof. From (2.7) it is easy to see that the moment generating function of h(y | x) is
given by
My | x (t) = My | 0 (X − X0 + t)/My | 0 (X − X0 ),
where X0 is the component vector for X = 0. So, on taking logarithms and differentiating with respect to t, we have


∂
∂
K y | 0 (X − X0 + t) =
K y | 0 (X − X0 ),
E(y | x) =
∂t
∂X
t=0

2

∂2
 = ∂
K
(X
−
X
+
t)
K y | 0 (X − X0 ),
cov(y | x) =
y|0
0


∂t∂t
∂X∂X
t=0
where K y | x (t) is the cumulant generating function of the posterior distribution.
Since the covariance matrix is non-negative definite, it is clear that E(y | x) is the
gradient of a convex function of the component vector X.
It follows immediately that E(y j | x) is non-decreasing in Xj , if all other components are held fixed. A slightly stronger conclusion is that if two individuals have x-vectors x1 , and x2 , and component vectors X1 , X2 , then the vectors
E(y | x1 ) − E(y | x2 ) and X1 − X2 make an acute angle with each other, that is,
[E(y | x1 ) − E(y | x2 )] [E(y | x1 ) − E(y | x2 )] ≥ 0.
These results linking the rankings of individuals on the y scale and the scale based
on X are valid whatever the distribution of y.
The results above are weaker than those given in the second edition (Bartholomew
and Knott 1999), which Jouni Kuha (personal communication) pointed out were
too strong.
Similar results in a special case were given by Knott and Albanese (1993) and
slightly extended in Bartholomew (1996). They are related to the Dutch identity
(Holland 1990a). Another slight extension of the work by Albanese and Knott gives
information about the form of the relationship between X and E(y | x). Using the
moment generating function, it can be shown that if the posterior distribution is
normal when X = 0 then it is normal for all X. The relevance of this result for our
present purposes is that posterior distributions often turn out to be close to normal.
and we conjecture that this is generally true as p → ∞. Given normality of the
posterior distribution, it follows that
E(y | x) = E(y | 0) + cov(y | 0) (X − X0 ).

(2.35)

This result helps to explain why the empirical relationship between X and E(y | X)
often turns out to be nearly linear – a point first noted by Upton (1980).
These results underpin the use of X for factor scores. Not only does it arise
naturally from our general approach but it has a strong intuitive appeal in its own right.

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

43

The weak dependence of the posterior distribution on the prior has close affinities
with Savage’s principle of precise measurement in Bayesian inference. Roughly
speaking, this says that as the sample size increases the dependence of the posterior
distribution on the prior diminishes. The situation here is a little more complicated but
what we would like to be true is that a similar result hold as p, the number of variables,
increases. The variables are not identically distributed and the idea of sampling
variables from an infinite sequence is not entirely straightforward. Nevertheless, it
should be possible to establish an extended principle of precise measurement, and
the notion of ‘consistency at large’ in Schneeweiss (1993) is a step in that direction.
See also Chang and Stout (1993).

2.16

A further note on the prior

We have emphasised the unimportance of the prior distribution for most of the things
we wish to do. In particular, we have argued that the choice of a standard normal
prior is a matter of convention and is adopted largely as a matter of convenience. This
gives rise to no problems as long as we are concerned with a single population. The
αs are, in effect, calibrated with respect to a standardised prior for that population
and the choice of the normal form is then tantamount to adopting a particular scaling.
The trouble with this approach arises when a second population comes into the
picture. For definiteness let us discuss the problem as it would arise in the context of
ability testing. Suppose that we estimate the item response curve of the logit model
for a given population by drawing a sample from it. Each item will thus have its
own difficulty and discrimination values. Scale values for any individual can then be
predicted on the basis of their αs and this will place them at some point on a standard
normal scale.
Suppose next that we draw a sample from a second population (in practice this
might be the same population at a later time or a different population, country say, at
the same time). Any scale values we calculate for this second sample will relate to this
second population. Since our methods automatically transform the population mean
to be zero (and the standard deviation 1) we are thus precluded from making any
comparisons between populations. Yet this is often exactly what we wish to do. We
may wish to know, for example, whether the general level of ability (the population
mean) has increased over time. Is there any way we can get at this information?
One method is to argue as follows. The difficulty and discrimination of an item
are intrinsic properties of that item (albeit calibrated with respect to a particular population). If these same items are administered to members of the second population
their difficulty and discriminating ability should be the same. Hence any change observed must be attributable to population differences. The first population is assumed
to have mean 0 and standard deviation 1: let the corresponding values for the second
population be μ and σ . Let the model for the first population be
logit πi (y) = αi0 + αi1 y

(2.36)

logit πi (z) = αi0 + αi1 z

(2.37)

and that for the second

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

44

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

with z = (y + μ)σ . The second model may be written in terms of y as
logit πi (y) = (αi0 + μσ αi1 ) + αi1 σ y.

(2.38)

If we assume that αi0 and αi1 have the same values in the second population as the
first, the only unknowns in the second case would be μ and σ , which could then be
estimated from the likelihood. This would give us estimates of the mean and standard
deviation of the second population in units of the first. A rather more sophisticated
estimation procedure would be to pool the likelihoods and estimate the αs, μ and σ
simultaneously. However, here we are concerned with what is possible in principle
and not with the technical details.
The problem simplifies considerably if the {αi1 }s can be assumed to be equal, in
which case we have the random effects Rasch model. In this case the common value
of {αi1 } (i = 1, . . . , p) can be absorbed into σ in (2.37). This special case is discussed by Andersen (1980a) who gave a maximum likelihood method of estimating
the means and variances of the group distributions. The method was illustrated on
examples. The relevant likelihood is simplified by the fact that, in this case, the sufficient statistic is the sum of the xi , and this takes the integer values 0, 1, . . . , p.
Andersen’s method requires the αi0 s to be known (or estimated independently
with sufficient accuracy). Such known values must, of course be relative to some
standard population.
A second approach would be to consider the mixture of the two populations as
a single population. In fitting the logit model to the combined samples we would be
adopting a different reference population and our estimates of {αi0 } and {αi1 } would
not be the same as before, even though they related to the same items. Any difference between the two populations would now be evidenced by the way in which the
predicted scale values for the individuals from the two samples were distributed with
respect to one another. For example, if there had been an increase in the population
mean from the first to the second occasion, we would expect the members of the
second sample to tend to have higher factor scores than those of the first sample.
This method is essentially a special case of what, in factor analysis, is called multigroup factor analysis; see Basilevsky (1994, Chapters 5 and 6) for further details
and references. These methods could easily be extended to the GLLVM, as shown
in Bock and Moustaki (2007), and this is a potentially fruitful area for future research. However, it is important not to lose sight of the implied assumption about the
prior distribution.

2.17

Psychometric inference

Our approach to inference, here and in subsequent chapters, follows the standard lines
of statistical inference, but we digress at this point to draw attention to another kind
of inference which has sometimes been thought to be relevant in some psychological
situations. This is known as psychometric inference. Psychometric inference is best
explained by reference to the practical problem which gave rise to it.

P1: TIX/XYZ
JWST070-02

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

THE GENERAL LINEAR LATENT VARIABLE MODEL

45

It appears to have originated in educational testing where a sample of children
take a series of tests in order to evaluate an ability of some kind. Frank Lord at
the Educational Testing Centre in Princeton pioneered this work in the 1960s, and
he referred to it as ‘item selection’. This term indicates more precisely the problem
that testers faced. In a typical testing situation n individuals take p test items. The n
individuals will be a sample (usually random) of the population of individuals to which
one wishes to generalise the result of the test. Standard statistical inference provides
the tool for doing this. But if the items are also a selection from the set of all possible
test items, we may well wish to generalise the result to apply also to the population
of items from which they were selected. This requires psychometric inference.
Workers in educational testing wanted their conclusions to relate not only to all
children but also to the whole population of test items which might have been used.
The basic approach has been to ask whether the results from the sample of items
actually used can be generalised to the whole population of items. (Hence what we
are about to describe is part of what has been called generalisability theory.) The idea
is as follows. If all the items are very similar (in difficulty or whatever distinguishes
them from one another) then each will lead to very similar responses – two identical
items would presumably lead to identical scores. If we can show that we would have
got very similar results, whatever sample of items had been selected, then we can
legitimately generalise the result to the whole population of test items. This can be
judged by constructing some measure of the similarity of the sets of scores resulting
from each item.
Such a measure is provided immediately by noting that the set-up is essentially
the same as that encountered in the one-way analysis of a random effects model in
the analysis of variance. The ‘groups’ of the analysis of variance are the test items,
which we suppose have been selected from some population of items. Psychometric
inference is based on a comparison of the ‘between items’ sum of squares and the
‘between persons within items’ sum of squares. The ratio of the expected mean
squares under the standard model will be unity if the items show no variation and will
increase as that variation increases. Cronbach (1951) proposed the complement of this
ratio as an index of generalisability and called it coefficient alpha. Conventionally, it
is sometimes suggested that alpha should be greater than 0.7, say, for generalisation to
be permissible, but this value is not chosen with reference to any sampling criterion. It
should be emphasised that the rationale for Cronbach’s alpha involves the assumption
that items have been selected at random, and in many applications this will not be
valid. Educational testing is a partial exception, as a simple example will show. If one
wished to test the ability of children to add up three 2-digit numbers the population
size of such sums is finite but large. In such a case it would be possible to select
items at random. Even if we excluded very simple cases there will be something
approaching a million possible sums and no child could be expected to attempt that
number in a single sitting.
In a factor analysis context, this approach requires us to estimate the factor
structure in such a way that it is as close as possible to what we would have found if
we had the full (possibly infinite) set of manifest variables available to us. For this
purpose Kaiser and Caffrey (1965) used Cronbach’s coefficient as a basis for what

P1: TIX/XYZ
JWST070-02

46

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:18

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

they called alpha factor analysis. Cronbach’s alpha is essentially based on a ratio of
variances and is very similar to newer measures of reliability; the link is discussed
in Bartholomew and Schuessler (1991). Since it turns out that the generalisability
coefficient is 1 for a one-factor model, the approach only comes into play if there are
two or more factors.
The idea of sampling variables (items in the above context) has arisen several
times in connection with latent variable modelling. Apart from the closely related
topic of alpha factor analysis, which we briefly touch on again in Chapter 3, it has
arisen in the debate on factor scores where Williams (1978), for example, has defined
an infinite sequence of random variables, known as the domain, in order to study
the behaviour of factor score ‘estimates’ as the number of elements in the domain
increases without limit. Schneeweiss (1993) introduced the concept of ‘consistency
at large’ to study the limiting behaviour of estimation procedures in infinite domains,
and the idea is also implicit in Q-analysis to which we come in Chapter 9.
However, the great drawback of psychometric inference, as currently understood,
is that it lacks an adequate theoretical probability framework. A promising new
development, which is not open to this criticism, is foreshadowed in de Boeck (2008).
Although his approach is directed primarily at the educational testing situation, it
offers the prospect of wider applicability. The essential idea is to allow the parameters
of the response function (see Chapter 4) to be random variables instead of being fixed.
This new source of variation might arise because the items are selected from some
population or because the items yield an outcome which is inherently uncertain. If this
idea proves fruitful, the general approach adopted in this book could be generalized
to accommodate it. For the present we emphasise that our general approach does not
depend in any way on any manifest variables (xs) which have not been observed. The
posterior distribution of y, which encapsulates all that the available xs have to tell
us about y, depends only on the conditional distribution of x given y and the prior
distribution of y.

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

3

The normal linear factor model
3.1

The model

We have already introduced the normal linear factor (NLFM) model in Chapter 1 as
an example of our general approach. Its main properties arise as special cases of the
GLLVM given in Chapter 2. The emphasis in this chapter is on the implementation
of the model and, especially, its interpretation.
The NLFM is the oldest and most widely used latent variable model. The
analyses based on the model may be motivated in various ways and not all of them
require the normality assumption. We shall therefore include sections at appropriate
points which show what can be done if we do not assume normality. The distinctive
feature of our presentation is that its development flows naturally from the framework
provided in Chapter 2.
The NLFM assumes that
x | y ∼ N p (μ + y, )

(3.1)

y ∼ Nq (0, I),

(3.2)

and

where  is a p × p diagonal matrix of specific variances and  is a p × q matrix of
factor loadings. We have already noted that there is no loss of generality in assuming
zero means and unit variances for the ys. For some purposes, to be considered later,
we may wish to allow correlations among the ys, but for the moment this is excluded.
Note that there are two assumptions of normality involved.

Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

47

P1: TIX/XYZ
JWST070-03

48

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

An equivalent, and more common, way of writing the model is as a linear equation
in normal random variables. Thus
x = μ + y + e,

(3.3)

where y ∼ Nq (0, I), e ∼ N p (0, ) and y is independent of e.
The difference between the two versions lies in the manner in which the manifest
variables are supposed to have been generated. In the first version this takes place
in two stages. First we select the individual from the population; the values of x
for that individual are then sampled from the conditional distribution of x. In the
second version y and e are selected simultaneously and independently and are then
combined according to (3.3) to give x. Whether we write the model in terms of
probability distributions, as in (3.1) and (3.2), or as an equation in random variables,
as in (3.3), is perhaps more a matter of taste than of substance. However, the blurring
of the distinction between random variables and their realised values which (3.3)
encourages can lead to confusion when we come to the question of what can be said
about y after x has been observed.
Since the conditional distribution of x in (3.1) belongs to the exponential family
there is a sufficient statistic which we gave by way of example in Section 2.6. In
matrix notation it is
X =   −1 x,

(3.4)

in the parameterisation of the present section.

3.2

Some distributional properties

The two main properties have already been given in (1.12) and (1.13), namely
x ∼ N p (μ,  + )

(3.5)

y | x ∼ Nq ( ( + )−1 (x − μ), (  −1  + I)−1 ).

(3.6)

and

The first result is required for fitting the model by maximum likelihood; the second
for making inferences about the latent variables on the basis of the observed xs.
The link between the conditional expectation of y and the components given in
(3.4) is established by noting that
  −1 ( + ) = (I + ) ,

(3.7)

where  =   −1 . From this it follows that
 ( + )−1 = (I + )−1   −1

(3.8)

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

49

and hence that
E(y | x) = (I + )−1 (X − E(X)).

(3.9)

The matrix  plays a key role in what follows. If it is diagonal we see that, given x,
the ys are independent and each posterior mean is proportional to the corresponding
component. Other consequences will emerge below.
It follows from (3.5) that
var(xi ) =

q


λi2j + ψi

(i = 1, 2, . . . , p).

(3.10)

j=1


The variance of xi is thus composed of two parts. The first, λi2j , arises from what is
common to all xs. For this reason it is known as the communality. The complementary
part, ψi , is the variance specific to that particular xi .
Other distributional properties, which are useful for interpreting the model, concern the relationship between the factors on the one hand and the observables, x and
X, on the other.
(a) The covariance between the manifest variables and the factor is given by


E(x − μ)y = E E(x − μ)y | y


= E E{(x − μ) | y}y
= E(yy ) = .
The factor loadings can therefore be interpreted as covariances between individual
manifest variables and factors. The correlations are given by
{diag }−1/2 .
(b) The covariance between the components and the factors is
E(X − E(X))y =   −1 E[(x − μ)y ]
=   −1  = .
If  is diagonal there are no cross-correlations between components and factors.
For the correlations we require the covariance matrix of X which is


var(X) = E   −1 (x − μ)(x − μ)  −1 
=   −1 ( + ) −1 
=  2 + .

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

50

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

The required correlation matrix is thus
{diag( 2 + )}−1/2 .
−1/2
, where  j
So if  is diagonal the correlation between Xj and yj is (1 +  −1
j )
is the jth diagonal element of . The larger  j , the larger will the correlation be.
Recalling the role which  plays in the posterior distribution of y, we can say
that the smaller the variance of yj (i.e. the more precisely it is determined by the
data) the larger will  j be and hence the more closely related will Xj and yj be.

3.3

Constraints on the model

There are circumstances in which we may wish to place constraints on the parameters.
For example, we have seen above that if the matrix  =   −1  is diagonal the
ys will be independent a posteriori and the relationship between the ys and the
components is particularly simple. This may help to facilitate the interpretation of
the model. This particular constraint also removes the freedom to arbitrarily rotate 
(unless some elements of  are equal).
In confirmatory factor analysis, to which we come in Chapter 8, there may be
grounds for specifying the pattern of the elements in . This usually takes the form
of setting the values of certain λs equal to zero. This too will usually remove the
rotational freedom.
A third type of constraint arises from the arbitrariness of the scale of the manifest
variables in many applications, especially in social sciences. Any change of scale in
an x will be reflected in the covariances and hence in the parameter estimates and their
interpretation. We therefore need a parameterisation which is unaffected by arbitrary
changes of scale. This can be done by expressing the xs in units of their standard
deviation. The effect of this is to make all the diagonal elements of  equal to 1, that is,
q


λi2j + i = 1

(i = 1, 2, . . . , p).

j=1

The implications of this constraint for maximum likelihood estimation will be
explained in Section 3.12.2.

3.4

Maximum likelihood estimation

Estimation by maximum likelihood is not easy. The treatment below follows somewhat the same lines as Lawley and Maxwell (1971), with some help from matrix
differentiation results in Magnus and Neudecker (1988). There is an attempt to fill
some of the gaps in previously available treatments, including that of the second
edition of the present volume (Bartholomew and Knott 1999), by including Heywood
cases, and being more careful about the choice of stationary values. The derivation
here is, as far as we know, different from any already published.

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

51

If x ∼ N p (μ, ) then the likelihood function for observations x1 , . . . , xn may be
written
n
l() = constant + [ln |  −1 | − trace[ −1 S]]
(3.11)
2
n
(xi − μ)(xi − μ) /n. The first step is to maximise with respect to
where S = i=1
μ. This is a standard problem and it is easily shown that μ̂ = x̄. Henceforth we shall
suppose this estimate to have been substituted into S. The novelty in the second stage
is that we wish to maximise with respect to  and , where
 =  + 

(3.12)

and where  is a p × q matrix of rank q, and  is diagonal.
It is convenient to use matrix differentiation results; see, for instance, Rao (1973),
Magnus and Neudecker (1988). One can safely ignore the symmetry constraints. We
have
n
dl() = [−trace( −1 d +  −1 (d) −1 S)]
(3.13)
2
n
(3.14)
= − trace[( −1 −  −1 S −1 )d].
2
From (3.14)
n
dl() = − trace[( −1 −  −1 S −1 )((d) + (d) )]
2
= −n trace[ ( −1 −  −1 S −1 )d].

(3.15)
(3.16)

So the stationarity conditions for  can be written
( −1 −  −1 S −1 ) = 0.

(3.17)

n
dl() = − trace[( −1 −  −1 S −1 )d],
2

(3.18)

Similarly, one can see that

so that stationarity conditions for  are
diag( −1 −  −1 S −1 ) = 0,

(3.19)

where diag A is the matrix with diagonal elements as for A and zeros elsewhere. Preand post-multiplying the matrices in (3.19) by the diagonal matrix  =  − 
and using (3.17) gives the stationarity conditions for  in the form
diag( − S) = 0.

(3.20)

Now we follow Lawley and Maxwell in spirit, but unlike them allow  to be singular
to accommodate the Heywood cases that arise in practice (see Section 3.12.3). The

P1: TIX/XYZ
JWST070-03

52

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

form of the stationary values for  can be made more transparent. Pre-multiplying
(3.17) by S−1/2 S−1  leads to
[S−1/2 S−1/2 − I]S−1/2  = 0.

(3.21)

So at stationary values of , the columns of S−1/2  are eigenvectors of S−1/2 S−1/2
with eigenvalues 1. As might be expected, one can rotate the columns of  by postmultiplying by an orthogonal matrix, and (3.12) is still satisfied. Replacing  by
 +  gives
[S−1/2 S−1/2 ]S−1/2  = S−1/2 [I −  S−1 ].

(3.22)

So, if the columns of  are scaled to make  S−1  a diagonal matrix, one can
see that, at stationary values of , we then have S−1/2  with columns which are
eigenvectors of S−1/2 S−1/2 with eigenvalues no greater than 1. It is always possible
to choose  so that there are q eigenvalues no greater than 1. The scaling is always
possible. It is easy to verify directly that if V is a p × q set of orthogonal eigenvectors
of S−1/2 S−1/2 with eigenvalues (all no greater than 1) on the diagonal of the q × q
matrix , one can take
 = S1/2 V(I − )1/2 .

(3.23)

It is interesting to note that  from (3.23) cannot be arbitrarily rotated while still
keeping  S−1  diagonal. A rotation is chosen by the way in which we solve the
likelihood equations.
The choice of a particular stationary value for  can be made looking to see
which one maximises the likelihood. To maximise the log-likelihood is to minimise
− ln |  −1 S | + trace[ −1 S] = − ln | S1/2  −1 S1/2 | + trace[S1/2  −1 S1/2 ].
(3.24)
So, if θi are the eigenvalues of S−1/2 S−1/2 , one must minimise
n

[− ln θi + 1/θi ].

(3.25)

i=1

As θ varies, − ln θ + 1/θ can be seen to take a minimum value of 1 when θ = 1.
For any stationary value of , q of the {θi } are equal to 1, and the other p − q of the
{θi } are also eigenvalues of S−1/2 S−1/2 , and are less than 1. So the log-likelihood is
maximised by choosing the columns of S−1/2  to be the eigenvectors corresponding
to the q smallest of the eigenvalues of S−1/2 S−1/2 .
Having found , one can use (3.20) to find a new value for  and then iterate.
As Jöreskog (1967) first showed, the hard part of maximum likelihood estimation is
the determination of , while given  the maximisation over  is easy. One can see
from the presentation above that the maximum likelihood estimation of  chooses
−1/2
S−1/2 minimise
it
so that the largest p − q eigenvalues {θi }, all less than 1, of S
(− ln θi + 1/θi ).

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

53

It is interesting to see what happens if there is a Heywood case (see also
Section 3.12.3). The simplest way this arises is if the maximum likelihood estimate of
one of the diagonal elements of , say the first one, is zero. Then S−1/2 S−1/2 is singular, so one of the {θi } is zero. The eigenvector associated with that zero eigenvalue
will therefore be one of the columns of the maximum likelihood estimate of S−1/2 .
It is easy to check directly that a suitable column of  is s111 S1 , where S1 is the first
column of S, and s11 is its first element. The maximum likelihood estimates for the
remaining factors, from (3.22), are found by using the sub-matrix of S−1/2 obtained
by omitting its first row and first column. That is the same as using the conditional
covariance matrix for variables 2, . . . , p given variable 1. Jöreskog (1967) has this
result, but no obvious way of integrating it into his analytic treatment of maximum
likelihood estimation.

3.5

Maximum likelihood estimation by the
E-M algorithm

Another way of carrying out the maximum likelihood estimation is to use the E-M
method, which is an iterative technique well suited to maximum likelihood estimation
for models where there is missing information. The missing pieces of information
here are the values of the latent variables. The E-M approach was introduced in
Dempster et al. (1977), and applied directly to the normal factor analysis model in
Rubin and Thayer (1982).
Starting with some initial values for the parameters, the procedure is to write
down the joint likelihood of (xi , yi ) for i = 1, . . . , n. The log-likelihood is replaced
by its expected value (E-step) conditional on the xi . The expected values are worked
out with the current values of the parameters at that iteration.
This modified likelihood is maximised (M-step) to give new values for the parameters, and the whole procedure iterated until convergence. Although convergence to
the global maximum is not guaranteed, the marginal likelihood of xi for i = 1, . . . , n
will never decrease with each change in the parameters. This follows from use of
the maximising property of the M-step, and a well-known information theory inequality on the conditional distributions of y given x; see Dempster et al. (1977,
equation (3.10)).
In practice it is easier to set the conditional expected value of the score function
from the joint likelihood of (xi , yi ) given xi equal to zero. Notice that this is in fact
the score function from the marginal likelihood of the xi (McLachlan and Krishnan
1997, (3.42), for instance).
The log-likelihood of the (xi , yi ) for i = 1, . . . , n is
 n


n
n
−1 1

constant − log |  | − trace 
(xi − μ − yi )(xi − μ − yi )
2
2
n i=1
n
1
n
− trace
(yi yi ),
2
n i=1

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

54

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

which gives a score function for μ,
n −1 (x̄ − μ − ȳ),

(3.26)

n −1 (Sx y − μȳ − Syy ),

(3.27)

for ,

and for  the diagonal elements of
n
n
−  −1 +  −1 [Sx x − μx̄ − x̄μ − Sx y  − Syx
2
2
 
+ μȳ  + ȳμ + μμ + Syy  ] −1 ,

(3.28)

where
Sx x =

n
1 
xi xi .
n i=1

To find the conditional expected values of the score functions, it is enough here to do
so for the sufficient statistics:
1
yi ,
n
1 
xi yi ,
=
n
1 
yi yi .
=
n

ȳ =
Sx y
Syy
Now,

E[ȳ | xi , i = 1, . . . , n] =   −1 (x̄ − μ) = ȳˆ ,

E[Sx y

(3.29)

n
1
| xi , i = 1, . . . , n] =
xi E[yi | xi ]
n i=1

n
1
=
xi (  −1 (xi − μ))
n i=1

= (Sx x − x̄μ ) −1  = Ŝx y ,

(3.30)

E[Syy | xi , i = 1, . . . , n] = (Iq +   −1 )−1 +   −1 (Sx x − μx̄ − x̄μ + μμ ) −1 
= Ŝyy .

(3.31)

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

55

These are calculated replacing  and  by their current estimates and then substituted
into the score functions of (3.26), (3.27), (3.28) above. Setting the score functions
equal to zero and solving,
μ̂ = x̄ − ȳˆ ,
ˆ = (Ŝ − x̄ȳˆ  )(Ŝ − ȳˆ ȳˆ  )−1 ,

xy
yy

ˆ Ŝ yx
ˆ−
ˆ = diag(Sx x − μ̂x̄ − x̄μ̂ − Ŝx y 




ˆ

ˆ μ̂ + μ̂μ̂ + 
ˆ Ŝ 
ˆ ).
+μ̂ŷ  + ŷ
yy

(3.32)
(3.33)

(3.34)

The procedure is iterated until it converges. It is easier, as Rubin and Thayer noticed,
to leave out μ, which has maximum likelihood estimator x̄. The natural way to allow
for this would seem to be to substitute μ = x̄ into (3.29), (3.30) and (3.31) to give
ȳˆ = 0,
= Sx x  −1 ,

Ŝx y
Ŝyy

= (Iq +   −1 )−1 +   −1 Sx x  −1 .

These are then used in (3.33) and (3.34) to provide new updating equations. The
ˆ that results from this approach is not that obtained by Rubin
updating equation for 
ˆ into the updating
and Thayer who, in effect, substitute the result of updating 
ˆ
equation for .

3.6

Sampling variation of estimators

The lack of information about the sampling behaviour of parameter estimates in
the factor model when used in exploratory mode is a serious defect of the standard
software packages. Standard errors, whether asymptotic or exact, are only crude
indicators of uncertainty in problems with many parameters, especially if they turn
out to be highly correlated. Nevertheless, without some indication of the precision of
estimators the interpretation of factors, which depends critically on those values, must
be suspect. The need for such information was recognised from an early stage, as the
Cudeck and O’Dell (1994) account shows, and the lack of adequate theory has led
to the adoption of a variety of rules of thumb which bear no relation to the sampling
variability of the estimators themselves. For example, it is common to regard factor
loadings greater than 0.3 as ‘significant’ and it is not unusual to find published tables
of loadings with the smaller loadings deleted. This may make the interpretation easier
but it owes nothing to the statistical significance of the loadings themselves.
In the absence of exact sampling theory we can have recourse to the asymptotic
theory of maximum likelihood. For the normal factor model this was first provided by
Lawley (1967) for the case of unstandardised variables and extended to standardised

P1: TIX/XYZ
JWST070-03

56

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

variables by Lawley and Maxwell (1971) who also gave a numerical illustration.
Subsequently, Jennrich and Thayer (1973) provided a small correction to Lawley’s
results. As we pointed out in Section 2.11, the estimation process yields a set of
estimates rather than a single point in the parameter space. What the Lawley (1967)
method did was to give the sampling behaviour of the estimator given by the standard
maximum likelihood routine, namely that for which   −1  is diagonal. Estimated
standard errors for the loadings of rotated factors can be obtained by the usual ‘delta’
methods, and this was done for orthogonal rotation by Archer and Jennrich (1973)
and for oblique rotations by Jennrich (1973). An account of the history of the subject
is given in Cudeck and O’Dell (1994) who go on to extend the theory in various
ways, including simultaneous inference for sets of parameters based on confidence
intervals. Further results can be found in Ogasawara (1998).
The heavy computations required may have inhibited the implementation of these
methods in a routine fashion but they are well within the scope of modern desktop
computers. The Mplus software (Muthén and Muthén 2010) and the CEFA computer
program (Browne et al. 1998) fit the NLFM and give standard deviations for many
forms of rotated loadings, whether standardised or not.
However, one does not know how large the sample size need be before the
approximations provided by the asymptotic results are adequate in practice. Results
reported by de Menezes (1999), and discussed in Chapter 6 below for the latent class
model, show that the asymptotic theory may provide a very poor approximation even
if the sample size is of the order of 1000. Similar investigations for factor analysis
seem to be lacking but, given the complexities of multi-parameter models, much
more comparative information is needed before the asymptotic results can be used
with confidence.
An alternative approach to studying sampling behaviour is through resampling
or simulation methods of the bootstrap or jackknife variety. These too have a long
history in factor analysis but have not been widely used because of the amount of
computing needed. However, the situation is rapidly changing and it is now possible
to contemplate using the approach with complex models and large sample sizes.
Tucker et al. (1969) carried out a detailed simulation study and Seber (1984) reported
a series of simulation studies on samples of size 50 by Francis (1974).
Pennell (1972) used the jackknife to find confidence intervals for factor loadings.
In the first edition (Bartholomew 1987) we reported an application of the bootstrap by
Chatterjee (1984). He used data taken from Johnson and Wichern (1982) concerning
seven variables measured on 50 randomly chosen salesmen. Three variables were
measures of sales performance and four were from tests of aptitude. The method of
fitting was the principal factor method with the ψs set equal to zero (i.e. a principal
components analysis). One would have preferred a method which allowed estimation
of the ψs, but with only two or three factors turning out to be significant the results
are not likely to be seriously affected and the computational ease of this method
commended it for this particular study. Chatterjee settled the question of how many
repeated samples to draw empirically, and it appeared that 300 gave reasonable
stability. Some of the results are given in Table 3.1 for factor loadings (standardised
λs) for the first three factors.

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

57

Table 3.1 Estimated loadings and their standard deviations for Johnson and
Wichern’s (1982) data.
Original estimates
Var. Fac. 1
1
2
3
4
5
6
7

0.973
0.943
0.945
0.660
0.783
0.649
0.914

Bootstrap estimates on 300 samples

Fac. 2

Fac. 3

Fac. 1

−0.110
0.029
0.010
0.646
0.286
−0.620
−0.193

0.054
0.312
−0.144
−0.318
−0.005
−0.427
0.306

0.972
0.945
0.943
0.657
0.774
0.644
0.916

s.d.

Fac. 2

s.d.

Fac. 3

s.d.

0.006 −0.096 0.055
0.038 0.052
0.013
0.010 0.091
0.192 0.158
0.020
0.014 0.079
0.076 0.168
0.095
0.577 0.276 −0.179 0.254
0.064
0.268 0.165
0.041 0.373
0.093 −0.516 0.358 −0.266 0.229
0.017 −0.185 0.091
0.197 0.173

Table 3.1 shows that the first factor is well determined but that we should be
wary of attributing much significance to the other two. None of the loadings for
factor 3 differ from zero by much more than their standard deviation, and only two
or three of those for factor 2 come anywhere near significance. An advantage of
the bootstrap method is that we can also look at the frequency distributions of the
estimators. Chatterjee (1984) gives a number of examples, of which that given in
Table 3.2 is particularly instructive. We notice that the large standard deviation is due
to the extreme skewness of the distribution. Six percent of the samples actually gave
negative loadings (if all loadings on a factor were negative the signs should have been
reversed) but even on the positive half of the scale the scatter is considerable.
Although this example is very limited in both scope and method, it provides a
warning against taking estimated loadings at their face value for sample sizes as
small as 50. The results reported by Seber (1984) and others support this. Much more
work is needed to extend and consolidate our limited knowledge in this important
area. Results reported in later chapters for categorical variables strongly suggest that
much larger samples (say, 500 or more) are needed if parameters are to be estimated
with precision. The lack of reproducibility of the results of factor analysis, which has
somewhat tarnished its image among practitioners, doubtless owes much to the use
of inadequate sample sizes.
A much simpler approach conceptually, which can be used if the sample is large
enough, is known as cross-validation. By splitting the sample randomly into two (or
more) equal parts and fitting the model to each part, some limited idea can be gained
about the stability of the estimates. An illustration for the latent class model, due to
Pickering and Forbes (1984), is reported in Chapter 6.
Table 3.2 Frequency distribution of factor 2 loading for variable 4 (300 samples).
Mid-point
of interval 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 −0.1 −0.2 −0.3 −0.4 −0.5 −0.6 −0.7
Frequency

7

43 111

52

40

22

4

2

1 0

0

3

1

6

2

5

1

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

58

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

3.7

Goodness of fit and choice of q

If q is specified a priori, the goodness of fit of the factor model can be judged using
the likelihood ratio statistic for testing the hypothesis  =  +  (H0 ) against the
alternative that  is unconstrained (H1 ). The statistic is then
−1

ˆ | + trace 
ˆ S − log | S | − p}
− 2{L(H0 ) − L(H1 )} = n{log | 
−1
ˆ S − log | 
ˆ −1 S | − p}
= n{trace 
(3.35)
ˆ
ˆ+
ˆ =
ˆ is the estimated covariance matrix. If  > 0 this statistic is
where 
asymptotically distributed as χ 2 with degrees of freedom

1
1
1
ν = p( p + 1) − pq + p − q(q − 1) = {( p − q)2 − ( p + q)}.
2
2
2

(3.36)

This is the difference between the number of parameters in  and the number of free
parameters on the null hypothesis. Bartlett (1950) showed that the approximation can
be improved by replacing n in (3.35) by n − 1 − 16 (2 p + 5) − 23 q. The behaviour of
the test when n is small was investigated by Geweke and Singleton (1980), whose
results suggest that it is adequate for n as low as 30 with one or two factors. However,
Schönemann (1981) argues that this is too optimistic because they used untypical
examples with high communalities.
Since q is not usually specified in advance the test is often made the basis of a
procedure for choosing the best value. Starting with q = 1, we then take successive
values in turn until the fit of the model is judged to be adequate. Viewed as a testing
procedure this is not strictly valid because it does not adjust the significance levels
to allow for the sequential character of the test. It rather depends on regarding the
p-value of the test as a measure of the adequacy of the model.

3.7.1

Model selection criteria

The trouble with a procedure of this kind is that the larger we make q the better the fit,
but it provides us with no criterion for judging when to stop. An alternative approach
is provided by Akaike’s information criterion for model selection. The situation is
that we have a set of linear models indexed by q and a selection has to be made.
Akaike (1983) proposed that method for use in factor analysis and showed that it
required q to be chosen to make
AIC = −2L + 2ν
a minimum, where ν is the number of free parameters in the model, and L is the
maximised value of the log-likelihood function for the estimated model. Note that
L and ν are both functions of q. The criterion can be justified in a variety of ways
but, in essence, it effects a trade-off between the bias introduced by fitting the wrong
number of factors and the precision with which the parameters are estimated—as q

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

59

is increased the bias decreases but the error increases. An alternative version, due to
Schwarz (1978), also known as the Bayesian information criterion (BIC), replaces
the term 2ν by ν ln n. A further criterion based on the residuals of the fitted correlation
matrix is proposed by Bozdogan and Ramirez (1986), who also report a comparison
of all three criteria using Monte Carlo methods applied to models used in the study of
Francis (1974). All methods perform reasonably well, but for the examples considered
no method is uniformly best. These model selection ideas show considerable promise
and their performance over a wide range of models merits further investigation.
Other criteria for selecting q which do not involve distributional assumptions will
be mentioned in Section 3.11.

3.8

Fitting without normality assumptions:
least squares methods

If nothing is assumed about the distributions of y and e, it remains true that the
covariance matrix predicted by the model is given by  =  + . We could then
aim to estimate  and  in such a way that  was as close to S, the sample covariance
matrix, as is possible. For this we need some scalar measure of distance between  and
S which must then be minimised with respect to the parameters. The distance function
(, S) = −trace  −1 S + log |  −1 S |

(3.37)

which arose in the course of maximum likelihood estimation is one possibility. It
will only yield maximum likelihood estimators if the normal assumptions hold, but
if (3.37) were regarded as a reasonable way of measuring distance the estimators
would be justified on a much broader basis.
There are many other possible measures of distance which could be used in place
of (3.37). It is natural to turn to least squares ideas. A simple unweighted least squares
criterion would be

1

=

p
p


(siu − σiu )2 = trace(S − )2 .

(3.38)

i=1 u=1

Another, suggested by the role played by the matrix  −1/2 S −1/2 in the Lawley and
Maxwell approach to maximum likelihood estimation, is
2

= trace{[ −1/2 (S − ) −1/2 ]2 } = trace{(S − ) −1 }2 .

(3.39)

These are both special cases of a general class of measures stemming from
fundamental work by Browne (1982, 1984) (see also Tanaka and Huba (1985))
which may be written
= trace{(S − )V}2 .

(3.40)

P1: TIX/XYZ
JWST070-03

60

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

These arise from a generalised least squares approach which allows the deviations
{si j − σi j } to be weighted in various ways. We shall not develop these ideas here,
but reference to the authors mentioned above will show that these methods lead to
robust methods of estimation under a range of distributional assumptions. The case
V = S−1 was investigated by Jöreskog and Goldberger (1972); see also Anderson
(1984, Section 14.3.4). The attraction of 1 and 2 is that, like maximum likelihood,
the optimisation requires the solution of an eigenvalue problem. In the case of 1 the
function to be minimised may be written

1

=

p
p



⎛
⎝siu − δiu ψi −

i=1 u=1

q


⎞2
λi j λu j ⎠ .

j=1

Differentiating with respect to λr s ,
∂ 1
∂λr s

⎧
⎫
p
p
q
⎨ 
⎬


=4 −
(sri − δir ψi )λis +
λis
λi j λr j
⎩
⎭
i=1

i=1

j=1

(r = 1, 2, . . . , p; s = 1, 2, . . . , q)

(3.41)

or
∂ 1
= 4{( ) − (S − )},
∂
which gives the estimating equations
(S − ) = ( ).

(3.42)

Differentiating with respect to ψr ,
⎛
⎞
q

∂ 1
= −2 ⎝srr − ψr −
λr2j ⎠
∂ψr
j=1
or
diag

∂ 1
= −diag S +  + diag  ,
∂

leading to
 = diag (S −  ).

(3.43)

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

61

These estimating equations are similar to those obtained for maximum likelihood
estimation in (3.14) and (3.17) and are solved in a similar manner. Suppose first that
 is known and that S −  is positive definite. Then (3.42) will be satisfied if:
1. the columns of  consist of any q eigenvectors of S − ;
2.   is a diagonal matrix with elements equal to the eigenvalues of S − 
associated with the vectors in .
Thus if we have a starting value for , the solution of (3.42) will yield a first
approximation to  which can then be inserted in (3.43) to give a second estimate
of , and then the cycle can be continued until convergence occurs. The question of
which eigenvectors of S −  are to be included in  can be answered as follows:
1

= trace(S −  −  )2
= trace(S − )2 − 2trace(S − ) + trace( )2
= trace(S − )2 − trace( )2 ,

using (3.42).
Now  has ( p − q) zero eigenvalues because it is of rank q. The others are
also eigenvalues of S −  since if we replace S −  in (3.42) by  the equation
is obviously satisfied. Let the eigenvalues which the two matrices have in common
be θ1 , θ2 , . . . , θq and the remaining eigenvalues of S −  be θq+1 , . . . , θ p ; then

1

=

p

i=1

θi2 −

q

i=1

θi2 =

p


θi2 .

(3.44)

i=q+1

For this to be a minimum, θq+1 , . . . , θ p must be the smallest eigenvalues and hence
 must consist of the eigenvectors associated with the q largest eigenvalues.
In this method  is chosen so that S −  is positive definite and so that the sum
of squares of the p − q smallest eigenvalues of S −  is as small as possible.

3.9

Other methods of fitting

The foregoing method is known as the principal factor (or principal axis) method
because of its similarity to principal components analysis to which it is equivalent
if  = 0. If we use the correlation matrix R instead of S the estimates obtained
for the scale-invariant parameters ∗ and  ∗ defined in Section 3.12.2 will not be
identical to those arrived at by first using S and then transforming as they were with
the maximum likelihood method.
Estimation for 2 using (3.39) proceeds in an exactly similar manner. In fact,
since
 −1/2  −1/2 = ∗ ∗ + I = ( −1/2 )( −1/2 ) + I,

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

62

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

all we have to do is to replace  by ∗ =  −1/2  in (3.42) and S −  by S∗ − I.
The estimating equation for ∗ is then identical to that for maximum likelihood.
Rather surprisingly, it turns out that the differing distance functions both lead to the
eigenvalues and vectors of S∗ . However, this is true only for fixed . If we bring 
into the picture its partial derivatives will be different from those in the maximum
likelihood case and, in fact, a good deal more complicated. With this method the
choice of  is made so that  −1/2 S −1/2 has eigenvalues all no less than 1 and so
that the sum of the squared differences from 1 of the p − q smallest eigenvalues is as
small as possible.
For
given by (3.40) with V = S−1 the same equation for  for fixed  is
obtained, but the (implicit) estimating equation for  is
diag S−1 {( + ) − S}S−1 = diag 0;
see Anderson (1984). Here the sum of the squared differences from 1 of the reciprocals
of the p − q smallest eigenvalues of  −1/2 S −1/2 is minimised.
We have noted that if  were known, these methods would be much simpler,
being straightforward eigenproblems. Since  only enters into the diagonal elements
of , there is a prospect of avoiding the difficulties by eliminating these terms from
1 . We would then minimise


1

=

p
p



⎛
⎝siu −

i=1, i=u u=1

q


⎞2
λi j λu j ⎠ .

(3.45)

j=1

This approach is described in Harman and Jones (1966) and is usually known as the
‘minres’ method. Various methods of obtaining estimates have been given by Comrey
(1962), Comrey and Ahumada (1964), Okamoto and Ihara (1983) and Zegers and
ten Berge (1983). Unfortunately the omission of the diagonal terms destroys the
structure which led to the easily solved eigenequations. Given that the minres method
is unweighted and that the iterative methods for the -family are well within the
scope of modern computers, the method offers few advantages and is not included in
modern computer packages.
There are yet other methods of estimating the normal linear factor model. Knott
(2005) has a contribution to, and references for, the minimum trace method.

3.10

Approximate methods for estimating 

The iterative methods require a starting value for , and although one can use an
arbitrary value such as  = diag S there are approximations which will reduce the
number of iterations required. Apart from their practical value they give some insight
into the interpretation of the analysis. We have a non-negative definite matrix
−

(3.46)

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

63

and so, assuming now that  is invertible,
 −1 −  −1

(3.47)

is non-negative definite, (see, for instance, Rao (1973, p. 70, Exercise 9)), from which
it follows that
σ ii ≤ ψr−1 .

(3.48)

Since σ ii can be estimated from the inverse of S, we can estimate an upper bound for
ψi and this may be used as a starting value for the iteration.
This result may be given another interpretation: sii , which we would use as an
approximation, may be expressed using standard regression results as
s ii = sii (1 − Ri2 ),

(3.49)

where Ri2 is the multiple correlation coefficient of xi regressed on the remaining xs.
Now
sii (1 − Ri2 ) = var(xi | x1 , x2 , . . . , xi−1 , xi+1 , . . . , x p )
and
ψi = var(xi | y).
We would expect the latter to be smaller than the former because y is not precisely
determined by any finite set of xs (see (3.6)). A simpler but less precise bound for
ψ follows from the fact that Ri2 ≥ max j=i ri2j , where rij is the correlation between xi
and xj . Hence an estimated upper bound for ψi is sii (1 − max ri2j ).


ˆ ≈ diag S−1 −1 derived from (3.48) is the basis of image
The approximation 
factor analysis. This was proposed by Guttman (1953); see also Jöreskog (1969).
Like other methods of fitting the NLFM which seem to have fallen into disuse, it
offered a relatively simple method of fitting at a time when computing facilities were
very modest. Treating  as if it were known and proportional to (diag S−1 )−1 means
that the fitting can be carried out by the principal axis method without iteration. An
ˆ but
iterative version can be used in which  is updated using the latest values of ,
Basilevsky (1994, Section 5.33) has shown that this leads to inconsistent estimators.
With modern computing facilities there is no longer any need for such methods.

3.11

Goodness of fit and choice of q for least
squares methods

Little appears to be known about the sampling behaviour of the methods of fitting
discussed in Sections 3.8–3.10, but there is an important result due to Amemiya and
Anderson (1985) which shows that the limiting χ 2 distribution of the goodness-of-fit

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

64

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

statistic of (3.40) is valid under very general circumstances. They show that if the
elements of e are independent and if y and e have finite second moments then (3.35)
calculated using the maximum likelihood estimators has the same distribution as in
the normal case. This result also holds for another goodness-of-fit statistic which is
sometimes used, namely
1
ˆ 
ˆ −1 }2
n trace{(S − )
2
which, in the normal case, is asymptotically equivalent to (3.35). This is a further
reason for using the maximum likelihood method of fitting, whether or not one makes
the normality assumptions.
One consequence of these results is that we can use the same methods for choosing
q as were proposed for the maximum likelihood method in Section 3.7. There are
also two other methods which do not depend on distributional assumptions. Both
are based on the eigenvalues of the sample correlation matrix and the role which
they have in principal components analysis. The Kaiser–Guttman criterion chooses
q equal to the number of eigenvalues greater than unity. The rationale is that the
average contribution of a manifest variable to the total variation is 1, and that a
principal component which did not contribute at least as much variation as a single
variable represents no advantage. The carry-over of this argument from principal
components to factor analysis rests on the similarity between the two techniques
noted in Chapters 1 and 9. Simulation results obtained by Fachel (1986) suggest that
if p is large this method is likely to overestimate q.
The second method, due to Cattell, is known as the ‘scree test’. If the eigenvalues
are plotted against their rank order they will lie on a decreasing curve. One then
looks for an ‘elbow’ in the curve, as this would indicate the point at which the
further addition of factors shows diminishing returns in terms of variation explained.
A simulation study by Hakistian et al. (1982) comparing these two methods with the
use of the likelihood ratio statistic does not lead to clear-cut conclusions.

3.12

Further estimation issues

3.12.1

Consistency

In the discussion of estimation methods we have tacitly assumed that the parameters
could be consistently estimated. A necessary condition for this to be possible is
that there shall be at least as many sample statistics as there are parameters to be
estimated. The number of parameters in a q-factor model is pq + p, but in order
to obtain a unique solution we have to impose 12 q(q − 1) constraints (usually by
requiring the off-diagonal elements of   −1  to be zero). The number of free
parameters is then
1
pq + p − q(q − 1).
2

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

65

The sample covariance matrix S has 12 p( p + 1) distinct elements, so for consistent
estimation to be possible we must have
1
1
1
p( p + 1) − pq − p + q(q − 1) = [( p − q)2 − ( p + q)] ≥ 0.
2
2
2

(3.50)

Equation (3.50) implies that there is an upper bound to the number of factors which
can be fitted which is given by
q≤

1
{2 p + 1 − (8 p + 1)1/2 }.
2

(3.51)

The condition in (3.50) is not sufficient because it does not guarantee that the estimates of the ψi will be non-negative. Building on earlier work by Anderson and
Rubin (1956), Kano (1983, 1986a,b) has provided general conditions under which
maximum likelihood and generalised least squares estimators are consistent. However, in Kano (1986a) he provides an example of a model which does not admit any
consistent estimator. The question of consistency when p is not fixed is investigated in
Kano (1986b).

3.12.2

Scale-invariant estimation

In practice the factor model is almost always fitted using the sample correlation matrix
rather than the covariance matrix. The reason usually given for this is that the scaling
adopted for the manifest variables is often arbitrary, especially in social science
applications. Using correlations rather than covariances amounts to standardising the
xs using their sample standard deviations, and this ensures that changing the scale of
the xs has no effect on the analysis. However, the joint distribution of the correlation
coefficients is not the same as that of the covariances and so it is not obvious that
the estimators obtained in the way described above will be true maximum likelihood
estimators or that the asymptotic goodness-of-fit test will be valid. A good deal
of confusion has surrounded this topic so we shall approach the matter from first
principles.
We start from the proposition that if the units of measurement of the xs are
arbitrary then there can be no scientific interest in any aspect of the model which
depends on the choice of units. The effect of a scale change on the model of (3.3)
can be seen by transforming to x∗ = Cx, where C is a diagonal matrix with positive
elements. Expressed in terms of x∗ , the model becomes
x∗ = Cμ + Cy + Ce,
with
var(x∗ ) = C C + CC.

(3.52)

P1: TIX/XYZ
JWST070-03

66

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

In general, the parameters  and  do not therefore meet our requirements since
their estimated values and the interpretation to be put upon them will depend on the
scales of the xs. Suppose, however, that we choose C = (diag )−1/2 so that each x
is divided by its theoretical standard deviation; then we may write (3.52) as
x∗ = μ∗ + ∗ y + e∗ ,

(3.53)

with var(x∗ ) = ∗ ∗ +  ∗ , where ∗ = C and  ∗ = CC.
Clearly, any change in the scale of x now has no effect on the value of x∗ and
hence the parameter estimates obtained for ∗ and  ∗ will be independent of the
scaling of x. These parameters will be said to be scale-invariant and it is on them
that our interest will be centred. We shall also refer to ∗ and  ∗ as the parameters
of the model in standard form.
The important difference between this approach and that starting with the sample
correlations is that in the latter case the xs would be standardised using the sample
standard deviations. Their distribution would not then be normal, and that would be
inconsistent with the assumptions we have made about the form and distribution of
the right-hand side of (3.3).
To estimate ∗ and  ∗ one could first estimate  and  for any arbitrary scaling
of the xs using the sample covariance matrix and then transform them by
ˆ ∗ = (diag )
ˆ
ˆ −1/2 


and

ˆ −1/2 (diag
ˆ −1/2 .
ˆ ∗ = (diag )
ˆ
)


However, it turns out that the usual practice of treating the correlation matrix as if
it were the covariance matrix does yield the maximum likelihood estimators of the
scale-invariant parameters. This was shown by Krane and McDonald (1978) and an
outline of the justification is as follows.
We reparameterise the likelihood by writing T = C−1/2 C−1/2 , where C =
diag . From (3.11) we then have
− 2L/n = constant + log | C1/2 TC1/2 | + trace(C1/2 TC1/2 )−1 S
= constant + log | C | + log | T | + trace C−1/2 T−1 C−1/2 S
= constant + log | C | + log | T | + trace T−1 C−1/2 SC−1/2 . (3.54)
The matrix T involves only the parameters ∗ , since its diagonal elements are now
unity. The expression in (3.54) may be maximised in two stages, first with respect
to C and then with respect to T. Krane and McDonald showed, as one might have
anticipated, that C = diag S, so at the second stage the quantity to be maximised is
log | T | + trace T−1 R,
where R = (diag S)−1/2 S(diag S)−1/2 which is exactly what we would have done if we
had treated the sample correlation matrix as a covariance matrix. It is easily verified

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

67

that the log-likelihood ratio statistic is the same whichever parameterisation is used,
so the procedure for estimation and testing goodness of fit is fully justified.

3.12.3

Heywood cases

All the methods of estimating the parameters of the factor model involve minimising
a measure of the distance between S and . However, the parameter space is restricted
by the condition  ≥ 0 so the usual procedure of setting all the partial derivatives
equal to zero may yield a solution with a negative ψ. In such cases the minimum we
seek will lie on a boundary of the admissible region at a point where one or more
of the ψs is zero. When this happens we have what is called a Heywood case, after
Heywood (1931). In practice this will be recognised either by the appearance of a
negative estimate or by the convergence of an estimate to zero, depending on the
algorithm used.
There is no inconsistency in the occurrence of a zero residual variance and, taken
at its face value, it would simply mean that the variation of the manifest variable in
question was wholly explained by the latent variables. In practice this rarely seems
plausible and the rather frequent occurrence of Heywood cases has caused a good
deal of unease among practitioners. Taken with the fact that it is known empirically
that zero estimates can easily arise when the true parameter values are not zero, there
is good reason for not taking such estimates at their face value.
There is a good deal of evidence, mainly of an empirical kind, about the circumstances under which Heywood cases are likely to occur. Much of this is based
on simulation studies where there is no question of the model itself being invalid.
The results are widely scattered in the literature but we have drawn heavily in what
follows on van Driel (1978), Anderson and Gerbing (1984), Boomsma (1985) and
Fachel (1986).
If a Heywood case arises when the data conform to the linear factor model it will
probably be the result of sampling error. A key factor is therefore sample size. For a
model with positive ψs the probability of a Heywood case tends to zero as n tends
to infinity. We are therefore dealing with a small-sample phenomenon. The risk of
a Heywood case depends on other factors mentioned below, but as a rough guide it
appears that it is high with sample sizes of 100 or less and low with samples of 500
or more.
For a given sample size the risk decreases as p, the number of variables, increases.
Obviously the risk will also be greater if one or more ψs is very small, but this is not
something which would be known in advance. Nevertheless, one can obtain some
clues from the data. For example, in Section 3.10 and the remarks that followed, we
saw that an estimated upper bound for ψ could be obtained and, in particular, that ψi
would be small if xi was highly correlated with any other variable. The presence of
one or more high correlations is therefore indicative of a potential Heywood case.
One of the commonest cause of Heywood cases is the attempt to extract more
factors than are present. This is readily demonstrated by simulation but might have
been anticipated on the grounds that artificially inflating the communality forces the
residuals towards zero.

P1: TIX/XYZ
JWST070-03

68

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Once the causes of Heywood cases are understood, the ways of dealing with
them become clearer. At the stage of designing an enquiry one should aim for a large
sample with a good number of variables. But in selecting variables it is important
to avoid introducing new variables which add little to those already there. This will
merely create high correlations without contributing significantly to the information
about the latent variables.
At the analysis stage the options are more limited. Over-factoring can be avoided
by paying careful attention to the various criteria suggested in Section 3.7. If a highly
correlated pair of variables appears to be the cause, one of them can be dropped with
little loss. However, it does not follow that dropping a variable which is implicated
in a Heywood case is always advisable. If it arises because of a single small residual
variance we should then be omitting one of the more valuable variables which was
a relatively pure indicator of the latent variables. In any case, experience shows
that such a course often leads to a Heywood case involving another variable, which
defeats the object.
If the foregoing precautions fail there are at least three courses still open:
1. We can use a Bayesian approach like that suggested by Martin and McDonald
(1975). Here we maximise not the likelihood but the posterior density, using a
prior distribution for the ψs which assigns zero probability to negative values.
Martin and McDonald propose a form which is both tractable and plausible in
that it implies a distribution which is almost uniform except that it decreases to
zero at the point ψi = 0. Lee (1981) also investigated the form of the posterior
density under different informative prior distributions, some of which have
been designed to deal with Heywood cases. Furthermore, a simulation-based
Bayesian estimation approach discussed in Sections 2.10 and 4.11 has been
also used by Shi and Lee (1998) for the factor analysis model for continuous
variables that can also handle categorical variables.
2. We may stop the iteration at some arbitrarily small value of ψi such as 0.05 or
0.01. In effect, this is a special case of course of action 1 with a uniform prior
on the interval ( , 1) where is the chosen cut-off point. It may be justified on
the grounds that the likelihood (or other distance measure) will be very close
to its optimum and all that matters for purposes of interpretation is to know
that ψi is ‘small’.
3. A third method which shows promise but needs further investigation rests on
the following simple idea. If a variable xi has a small ψi we could increase the
latter by adding to xi an independent random variable with known variance
σ 2 . Denoting the new variable by xi , its variance would then be
var(xi ) =

q


λi2j + ψi + σ 2 ,

j=1

but the covariances would be unchanged.
If xi were used instead of xi in estimating the parameters, we would obtain
an estimate of ψi + σ 2 . Knowing σ 2 , we could then obtain an estimate of ψi

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

69

by subtraction. If this still led to a negative ψi the procedure would need to
be repeated with a larger value of σ 2 . It is not, in fact, necessary to add the
artificial variable to each value of xi ; we could simply add σ 2 to the appropriate
diagonal element of S.
If the analysis is carried out on the correlation matrix the effect of replacing
xi by xi is to multiply the off-diagonal elements in the ith row of the sample
correlation matrix by (1 + σ 2 )−1/2 . The relationship between the parameters
of the original model and the modified one is then given by
λi∗j = λi j (1 + σ 2 )1/2 ,

ψi∗ = 1 − (1 + σ 2 )

q


λi2j ,

(3.55)

j=1

where the asterisk denotes the modified loading. The result holds for any value
of σ 2 but it seems reasonable to choose a value just large enough to avoid the
occurrence of a Heywood case. In practice σ 2 = 1 seems to be the right order
of magnitude. It should be noted that the results on the standard errors of the
estimates and on goodness of fit will be invalidated by this device.

3.13

Rotation and related matters

In Section 2.11 we introduced the idea of orthogonal and oblique rotations in the factor
space. These transformations have the property that they leave the joint distribution
of the manifest variables unchanged and so the fit of the model is unaffected. A choice
of rotation thus has to be made on non-statistical grounds. We saw that patterns of
loadings exhibiting simple structure were relatively easy to interpret. The rotations
available in the major software packages for the NLFM all embody some algorithm
for getting as close as possible to simple structure.
If only two factors have been fitted the position will usually be clear from a plot
of the loadings. In the space of the factor loadings one is looking to position the axes
so that the points lie close to one or other axis. This is often all that is necessary, and
we shall illustrate the method on the examples in Section 3.17.

3.13.1

Orthogonal rotation

One characteristic of simple structure is that the loadings on any factor are either zero
or large. Since the squares of standardised loadings lie between 0 and 1, an algorithm
which seeks to maximise their variance will move them towards opposite ends of
the range. This is the varimax approach which was proposed by Kaiser (1958). In an
alternative version λi2j is replaced by λi2j /(1 − i ) (i = 1, 2, . . . , p) in the criterion
for maximisation. The maximisation is achieved iteratively, and details are given in
Magnus and Neudecker (1988).
Another way of describing simple structure is in terms of the variance of the
squares of the factor loadings across the rows of  rather than down the columns.
This idea is implemented in the quartimax method. It would, of course, be possible
to combine the characteristics of varimax and quartimax in a single criterion.

P1: TIX/XYZ
JWST070-03

70

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

3.13.2

Oblique rotation

An oblique rotation offers a better chance of finding simple structure, but at the price
of complicating the interpretation. In practical terms there is no obvious reason why
factors of substantive interest should be uncorrelated as orthogonality implies. For
example, if one is seeking to identify two factors called verbal ability and numerical
ability in an educational test, there is every reason to expect them to be correlated
and, if so, they will not be uncovered by a search restricted to orthogonal factors.
Algorithms similar to those in the orthogonal case, but without the constraint of
orthogonality, are available. Details of the options available may be found in the
specialist literature, including Tabachnick and Fidell (1996) and McDonald (1985).
We shall use the OBLIMIN routine provided by Mplus (Muthén and Muthén 2010).
When the factors are uncorrelated we saw that the standardised factor loadings
could be interpreted as the correlation coefficients of the manifest variables and
the factors. Under oblique rotation this is no longer true. The calculation in (a) of
Section 3.2 now gives
E(x − μ)y = ,

(3.56)

where  is the correlation matrix of the ys. The matrix of correlations is sometimes
called the structure loading matrix to distinguish it from the pattern loading matrix .

3.13.3

Related matters

There are other methods of fitting the factor model which are best regarded as ways
of selecting a particular rotation. One of these is given by Rao (1955). This approach
views the problem in the context of canonical correlation, where the object is to
find linear combinations of one group of variables which are maximally correlated
with linear combinations of a second group. In factor analysis the xs form one group
and the ys the other. Using the methods of canonical correlation, we find that linear
combination of the xs and of the ys which are most highly correlated. Then we find
two further linear combinations, uncorrelated with the corresponding member of the
first pair, and so on. This procedure leads to estimates of  and  which satisfy the
maximum likelihood equations and also the constraint that   −1  be diagonal. In
effect, therefore, it is selecting the rotation with this property and, in the process,
providing another reason for choosing that particular solution.
Alpha factor analysis can be regarded in the same light. This was proposed by
Kaiser and Caffrey (1965) and based on the psychometric notion of generalisability
which we considered in Section 2.17. It is based on the eigendecomposition of the
correlation matrix of the common parts of the manifest variable. Thus for the model
x = μ + y + e,

(3.57)

c = μ + y is the vector of the common parts of x. Its correlation matrix is
R∗ = (I − )−1/2 (R − )(I − )−1/2 ,
where R is the correlation matrix of the xs.

(3.58)

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

71

The standard least squares fit of principal axis factoring then yields
ˆ = (I − )1/2 M,


(3.59)

where 2 is the diagonal matrix containing the q largest eigenvalues of R̂∗ and M the
matrix of associated eigenvectors. The unknown  can be estimated by an iterative
procedure.
The solution is thus the rotation which maximises the generalisability as measured by the correlations of the common parts. Given the questionable nature of the
psychometric measure of generalisability, other methods of selecting rotations are to
be preferred. An alternative version of alpha factor analysis was proposed by Bentler
(1968) which turned out to be equivalent to Rao’s method.
The software packages CEFA (Browne et al. 1998) and Mplus (Muthén and
Muthén 2010) allow a choice between very many different sorts of rotations and
standardisations.

3.14

Posterior analysis: the normal case

Having determined the dimension of the factor space and established that the model
is a reasonable fit, we may wish to assign scale values to individuals in the latent
dimensions. We may need these for selection purposes as, for example, when we
wish to choose those with the highest ability. Or we may wish to use scale values as
an input to some further analysis to see how ability is related to some other variable.
It is sometimes argued that this latter need can best be met within the framework of
a structural equation model (see Chapter 8) where the role of the latent variables is
implicit and does not need to be made explicit. In Chapter 8 we shall advance reasons
for treating this recommendation with great caution. For the present we simply note
that this does not eliminate the need for factor scores.
Within the framework we have adopted and set out in Chapter 2, the so-called
factor scores problem is solved by the posterior distribution of y given x. Scale values
for the dimensions of y can be obtained by using some measure of location of the
posterior distribution such as E(y | x). The precision of any predicted variable can
be measured by the posterior variance. This procedure was set out in general terms
in Chapter 2, where it was noted that the posterior expectation was close to a linear
function of the sufficient statistic. We have already given the posterior distribution
for the normal case in (3.6) (and also in (1.13)) which shows that the relationship
for the normal linear latent variable model is precisely linear. Whether we use the
expectation or the component is therefore of no significance since one is a linear
function of the other.
The posterior covariance matrix of y | x was given in (3.6) in the form (I + )−1 .
It is much easier to interpret the posterior analysis when  is diagonal because this
makes the ys independent.
The traditional way of approaching the factor scores problem has been to start
with (3.3). This is an equation in random variables, but realised values of these

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

72

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

random variables will satisfy the same equation. However, the only realised values
we are able to observe are the xs. This leaves the ys and the es undetermined. The set
of equations cannot, therefore, be solved for the ys alone. For this reason the factor
scores are said to be indeterminate and various devices have been adopted to get
best-fitting values, in some sense, for y.
The case for adopting the posterior approach was argued in Bartholomew (1981)
but had been anticipated by Dolby (1976). The ‘indeterminacy’ is reflected in the fact
that y remains a random variable after x has been observed though its variance will
have been reduced by the knowledge of x. The choice between the posterior and the
traditional approach remains a matter of lively debate, as reference to Maraun (1996)
and the ensuing discussion shows. In purely practical terms there is little at stake and
we shall confine our treatment to the posterior method. (It may be noted that it is only
in relation to the linear factor model that this controversy arises. With other latent
variable models, in particular the latent class model, the posterior approach has been
used without question.)
The greatest apparent drawback of the posterior method is that the scale values
depend on the choice of prior distribution. However, this is an inevitable consequence
of the essential indeterminacy of the prior. Theorem 2.15.1 shows that, using the
component scores, no higher level of scaling than ordinal is possible. Any assumption
about the form of the prior distribution is therefore a matter of convention. By
choosing a standard normal prior we are implicitly choosing to measure the latent
variable on a scale which renders its distribution normal. The predicted value of y for
a given individual is therefore with reference to that prior scaling.

3.15

Posterior analysis: least squares

If we are unwilling to make assumptions about the forms of the distributions in the
linear model we can still aim to find functions of the xs which are, in some sense, as
near as possible to the ys. In the case of the linear model it is sufficient to consider
only linear functions, as we now show. Let us consider the q linear functions X
obtained by pre-multiplying x − μ by a q × p matrix of the form (B)−1 B. Then
from (3.3),
X = (B)−1 B(x − μ) = y + (B)−1 Be,

(3.60)

and hence we may write
X j = yj + u j

( j = 1, 2, . . . , q).

(3.61)

We could then regard Xj as a factor score for yj since uj is a random error independent
of yj . Since the transformation in (3.60) involves the arbitrary matrix B we shall seek
to choose the transformation so that var(u j ) is as small as possible for each j. In this
way Xj is made as close as possible to yj in a mean square sense.

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

73

It can easily be shown that choosing (B)−1 B = C =  −1   −1 leads to the
smallest possible values for the variances of the uj . These smallest values are the
diagonal elements of  −1 . One way to see this is to show that for fixed a,
var a u = a CC a ≥ a  −1 a.
Since C = I,
a  −1 a = a C 1/2  −1/2  −1 a.
The Cauchy inequality gives
(a  −1 a)2 ≤ (a C 1/2  1/2 C a)(a  −1   −1/2  −1/2  −1 a)
= (a CC a)(a  −1 a).
So,
a  −1 a ≤ a CC a = var(a u).
We have
X =  −1   −1 (x − μ) and

var(u) =  −1 .

(3.62)

These are known as Bartlett’s scores (Bartlett 1937), and the advantage often claimed
for them is that E(X | y) = y, as (3.61) shows. However, an expectation of x conditional on y is hardly relevant when it is x that is known and y that is to be predicted.
Of more interest is the comparison with (3.9). The only difference is that (I + )−1
in (3.9) is replaced by  −1 in (3.62). If  is diagonal the effect of this difference is
merely to introduce different scaling of the Xs which is of little practical significance.
The distribution-free argument can thus be viewed as establishing the result of (3.9)
on a broader basis. However, a slightly different approach leads to scores which are
identical to those of (3.9), as we now show.
In this method, due to Thomson (1951), we choose X = C(x − μ), where C is a
q × p matrix, and try to make Xj to as close to yj as possible in the sense of minimising
φ j = E(X j − y j )2

(3.63)

where the expectation is with respect to both y and x.
The choice C =   −1 leads to the smallest φ j because for a fixed vector a,

a (X − y) has variance
a (CC − C −  C + I)

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

74

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

which may be written
a [(C −   −1 )(C −   −1 ) + I −   −1 ]a.
This is clearly no less than
a [I −   −1 ]a
and that lower bound is achieved for
C =   −1 = (I + )−1   −1 .

(3.64)

When defined in this way the scores are known as ‘regression’ scores. In this case
it is not true that E(X | y) = y, but for the reason given above this is not a relevant
objection.

3.16

Posterior analysis: a reliability approach

An entirely different approach to determining factor scores was proposed by Knott
and Bartholomew (1993) (see also Bartholomew and Knott (1993)). This starts from
the reliability of the score and aims to choose a score for which the reliability is a
maximum. Let φ(x) denote any function of the xs which is proposed as a score for the
latent variable y. Suppose we were able to make two independent determinations of
φ(x), φ(x1 ) and φ(x2 ) say, for all members of a population. Then the better the function
φ, the closer we would expect φ(x1 ) and φ(x2 ) to be. Closeness can be measured by
the correlation of φ(x1 ) and φ(x2 ) – known as the test–retest correlation. Knott and
Bartholomew showed that the function h which maximised this correlation satisfies
E{φ(x1 ) | x2 } = λφ(x2 ).

(3.65)

In the case of the NLFM it turns out that the function φ is identical to the components
and hence proportional to the conditional expectation E(y | x). For other models this
is not necessarily so though it appears, on the limited evidence available, that the
components are close to optimal. This approach therefore provides an interesting
further justification for the existing methods rather than a different type of score.
For a fuller account of the use of reliability in this context, see Bartholomew (1996,
Section 1.2).

3.17

Examples

We illustrate the application of the foregoing theory on two examples. The first
is small-scale and is designed to show some of the options available in a typical
computer package. The second is on a larger scale and is more typical of examples

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

75

encountered in the applied literature. Both examples exhibit features which are quite
common but seldom remarked upon in textbook treatments.
Example 3.17.1 In the first case the data are taken from a study by Smith and Stanley
(1983) on the relationship between reaction times and intelligence test scores. Here
we consider only the factor analysis of the ability variables. Scores were available
for 112 individuals on the following six variables:
1. a non-verbal measure of general intelligence (Spearman’s g) using Cattell’s
culture-fair test;
2. picture completion test;
3. block design;
4. mazes
5. reading comprehension;
6. vocabulary.
Full details may be found in the original paper. The correlation coefficients, covariances and variances, supplied by the authors, are set out in Table 3.3. The data were
first analysed using the maximum likelihood routine with the correlation matrix input.
Table 3.3 Correlation coefficients (right upper) and variances and covariances (left
lower) for Smith and Stanley’s (1983) data.

1
2
3
4
5
6

1

2

3

4

5

6

26.641
5.991
33.520
6.023
20.755
29.701

0.466
6.700
18.137
1.782
4.936
7.204

0.552
0.572
149.831
19.424
31.430
50.753

0.340
0.193
0.445
12.711
4.757
9.075

0.576
0.263
0.354
0.184
52.604
66.762

0.510
0.239
0.356
0.219
0.794
135.292

Since the xs have no natural common scale it is sensible to base the interpretation
on the standardised loadings and communalities, which are given in Table 3.4 for a
two-factor fit. We discuss the reasons for choosing two factors later.
The interpretation of these results is not immediately clear. The communalities
vary a good deal. In the case of reading comprehension, block design and vocabulary
the high figure means that a large part of their variation is accounted for by the two
factors. The other three variables, especially mazes, have smaller communalities and
so are much poorer indicators of the factors.
The particularly high communality of 0.96 for reading comprehension indicates
that we are close to a Heywood case. This output was obtained from Mplus (Muthén
and Muthén 2010).

P1: TIX/XYZ
JWST070-03

76

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 3.4 Standardised loadings, with standard errors in
brackets, and communalities of a two-factor model estimated
by maximum likelihood for Smith and Stanley’s (1983) data.
λ̂i1

λ̂i2

1 − ψ̂i

0.53 (0.09)
0.19 (0.09)
0.25 (0.08)
0.13 (0.09)
0.97 (0.07)
0.79 (0.07)

0.52 (0.09)
0.61 (0.08)
0.85 (0.07)
0.46 (0.09)
0.13 (0.05)
0.18 (0.07)

0.54
0.41
0.78
0.23
0.96
0.66

Variable (i)
1
2
3
4
5
6

One has to deduce from the factor loadings what these two factors might be. The
loadings in the first column are all positive, indicating that the first factor contributes
to all variables, and this might tentatively be identified with some general ability. In
the second column the loadings indicate that the second factor is largely concerned
with the first four items. These are all non-verbal items, and so this seems to point to
a non-verbal dimension to ability.
It might be that the interpretation would become clearer if we look at rotations.
Some idea of what to try can be gained from a plot of the loadings, and this is given
in Figure 3.1.
1.0

0.8

*
Oblimin rotation

0.6

*
λi2 0.4

*

*

0.2

0.0

*
*

–0.2
–0.2

0.0

0.2

0.4
λi1

0.6

0.8

1.0

Figure 3.1 Factor loadings given in Table 3.4. The axes for OBLIMIN rotation are
also shown.

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

77

The aim of rotation, we recall, is to achieve ‘simple structure’. In geometrical
terms this means seeking a rotation which makes the points lie as nearly as possible
along the two axes. It is clear from the figure that this cannot be achieved with an
orthogonal rotation. We could certainly rotate to make items 1, 2 and 3 lie very close
to one axis, but then the remaining points would be nowhere near the other. Equally
an axis passing close to 6 and 5 would leave the remaining points farther away from
the other axis than before rotation.
The figure shows that something close to simple structure could be obtained by an
oblique rotation, and one resulting from the OBLIMIN option of Mplus is illustrated.
This still leaves item 1 with a substantial loading on both dimensions, but this is not
surprising given that it is itself a composite measure. The rotated loadings are given
in Table 3.5.
Table 3.5 Factor loadings for the OBLIMIN rotation, with
standard errors in brackets, for Smith and Stanley’s (1983)
data (the communalities are unchanged by rotation and so
are omitted).
Variable (i)
1
2
3
4
5
6

λ̂i1

λ̂i2

0.38 (0.10)
−0.01 (0.08)
−0.03 (0.03)
−0.02 (0.10)
1.00 (0.07)
0.78 (0.09)

0.48 (0.10)
0.65 (0.09)
0.90 (0.07)
0.49 (0.10)
−0.04 (0.02)
0.06 (0.08)

Leaving aside item 1 for the moment, we have factor 1 loading almost entirely
on the verbal items, 5 and 6, and factor 2 loading on the remaining non-verbal items.
This strongly suggests two dimensions of ability – one verbal and one non-verbal.
We would expect the measure of general intelligence to contribute to both factors
but more strongly, perhaps, to the non-verbal dimension because of the non-verbal
character of that test.
We might thus conclude that the analysis identifies two dimensions of ability, one
verbal and one non-verbal, but that these are not uncorrelated (because the rotation
is oblique). The package gives an estimate of the correlation between the two factors
of 0.462 with an estimated standard error of 0.086.
In view of this analysis we might expect the two-factor structure to be clearer if
we omitted item 1 from the analysis, thus leaving items which fall clearly into verbal
and non-verbal categories.
A repeat of maximum likelihood estimation for the last five items reveals a
Heywood case. Mplus produces an unsatisfactory solution with a very large negative
variance for item 2. In effect, we have a Heywood case with a communality of 1 for
item 2. An R routine has been specifically written to cope with Heywood cases based

P1: TIX/XYZ
JWST070-03

78

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

on the maximum likelihood estimation presented in Section 3.4, the results of which
are given in Table 3.6.
Table 3.6 Factor loadings and communalities
of a two-factor model for Smith and Stanley’s
(1983) data (item 1 omitted).
Variable (i)
2
3
4
5
6

λ̂i1

λ̂i2

1 − ψ̂i

−0.57
−1.00
−0.45
−0.35
−0.36

0.05
0.00
0.06
0.78
0.86

0.33
1.00
0.20
0.73
0.87

Heywood cases are not uncommon with small sample sizes of a hundred or so
as we have already remarked. However, taking the loadings at their face values, the
interpretation is essentially the same as before. The verbal ability factor emerges
clearly in the second column whereas the first factor loads more heavily on the
non-verbal items, with the dominating item being block design.
Given the small sample size, one should not push the interpretation beyond
claiming some evidence for verbal and non-verbal dimensions of ability. If we wished
to scale individuals on any of the dimensions we have uncovered, we would need
the coefficients of the factor scores (or components), and these are provided by the
standard packages. However, those coefficients can be very unstable in small samples
because they involve the factor ψ −1/2 . If ψ̂i is small, and subject to a large sampling
error, the weight of xi will be large and uncertain. If a Heywood case occurs, as in
Table 3.6, the estimate becomes infinite. Posterior analysis for this example is thus
not worthwhile.
All of the foregoing analysis presupposes that a model with two factors provides a
satisfactory fit and that two is the best number of factors. In this case the default value
provided by Mplus was two and we set this question on one side in order to proceed
directly to questions of interpretation. We now return to the question of goodness
of fit.
For the example with all six items the overall global test of goodness of fit yields:
for the one-factor model, χ 2 = 78.95 with 9 degrees of freedom (P = 0.000); and
for the two-factor model, χ 2 = 6.36 with 4 degrees of freedom (P = 0.174). There
is little point in going beyond two factors because when q = 3 the model is only just
identifiable with the number of parameters equal to the number of statistics. In any
case the fit with two factors is good.
If the fit is not good, a comparison of the observed and predicted correlation (or
covariance) matrix may help to identify the source of the deviation. Table 3.7 gives
for the present example the differences between the observed and fitted correlation
matrices. It appears that, with the possible exception of items 2 and 4, the 2-factor
model predicts the observed correlation matrix very closely.

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

79

Table 3.7 Differences between the observed and fitted
correlation matrices for Smith and Stanley’s (1983) data.
Variable

1

2

3

4

5

6

1
2
3
4
5
6

0
—
—
—
—
—

0.05
0
—
—
—
—

−0.02
0.01
0
—
—
—

0.03
−0.11
0.02
0
—
—

0.00
0.00
0.00
0.00
0
—

0.00
−0.02
0.00
0.03
0.00
0

Example 3.17.2 The second example is taken from Harman (1976) and is based on
eight physical variables measured on 305 individuals. The eight variables are: height,
arm span, length of forearm, length of lower leg, weight, bitrochanteric diameter, chest
girth and chest width. The first four variables were assumed to measure ‘lankiness’
and the last four ‘stockiness’. Table 3.8 gives the sample correlation matrix of the
eight variables.
Table 3.8 Sample correlation matrix for the physical data.
Variable
1
2
3
4
5
6
7
8

1

2

3

4

5

6

7

8

1.000
0.846
0.805
0.859
0.473
0.398
0.301
0.382

1.000
0.881
0.826
0.376
0.326
0.277
0.415

1.000
0.801
0.380
0.319
0.237
0.345

1.000
0.436
0.329
0.327
0.365

1.000
0.762
0.730
0.629

1.000
0.583
0.577

1.000
0.539

1.000

Inspection of the correlation matrix shows a quite clear grouping of the first
four variables and the last four. We would therefore expect to find that a two-factor
solution adequately explains the correlation matrix. We first address the question of
the choice of q, the number of factors. Inspection of the scree plot of the eigenvalues
in Figure 3.2 shows that there are two eigenvalues greater than 1. If we look at the
goodness-of-fit test and the AIC in Table 3.9 the position is less clear. The χ 2 test
is highly significant for q ≤ 3 and the AIC shows no sign of reaching a minimum
value by q = 4. However, the normality assumption on which the χ 2 test is based is
suspect, and we note that, whether the factors lower in the order are significant or
not, they individually contribute very little to the overall variation in scores. When
q = 5 no solution could be obtained due to convergence problems, indicating that the
factoring has been taken too far. We shall, therefore, concentrate on the two-factor
model for purposes of interpretation.

P1: TIX/XYZ
JWST070-03

80

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS
5

Eigenvalue

4

3

2

1

0
1

2

3

4

5
Index

6

7

8

Figure 3.2 Scree plot for physical data.

Table 3.9 χ 2 and AIC for physical data.
χ2

df

P

AIC

713.7
88.6
23.9
0.84

20
13
7
2

0.000
0.000
0.001
0.658

6254.5
5643.3
5590.6
5577.6

Factors
1
2
3
4

Table 3.10 Two-factor solution (unrotated and geomin rotated), with standard errors
in brackets, for physical data.
Unrotated
Variable
1
2
3
4
5
6
7
8

Factor 1
0.885
0.939
0.907
0.876
0.304
0.254
0.192
0.325

(0.013)
(0.009)
(0.011)
(0.014)
(0.041)
(0.044)
(0.045)
(0.046)

Rotated

Factor 2
0.219
0.108
0.109
0.184
0.905
0.756
0.739
0.598

(0.024)
(0.020)
(0.022)
(0.025)
(0.021)
(0.027)
(0.028)
(0.037)

Factor 1
0.855
0.951
0.917
0.858
0.003
0.002
−0.057
0.133

(0.020)
(0.013)
(0.012)
(0.021)
(0.019)
(0.026)
(0.043)
(0.047)

Factor 2
0.115
−0.015
−0.010
0.078
0.953
0.797
0.787
0.612

(0.032)
(0.022)
(0.011)
(0.034)
(0.019)
(0.027)
(0.033)
(0.040)

P1: TIX/XYZ
JWST070-03

P2: ABC
JWST070-Bartholomew

May 9, 2011

22:34

Printer Name: Yet to Come

THE NORMAL LINEAR FACTOR MODEL

81

The parameter estimates obtained by maximum likelihood are given in Table 3.10,
with standard errors in brackets, both for the unrotated and the rotated solution. The
geomin oblique rotation, available in Mplus (Muthén and Muthén 2010), (Jennrich
2006) is used here. Both the unrotated and the oblique solution produced interpretable
factors. However, the rotated solution produced a simple structure. In order to identify
what the two factors are measuring, we have to ask what distinguishes the items with
the higher loading from those with the lower. This, of course, requires some expertise
in the field but the task might not be very difficult here because of the nature of the
items. The loadings printed in bold type for factor 1 constitute a group which load
heavily on that factor. On the second factor it is the complementary set of items which
have the high loadings. The high loadings on the first four items are associated with
length and height, and therefore factor 1 is related to aspects of physique that have
to do with tallness and thinness. The high loadings on the last four items are related
to weight, diameter and width, and so factor 2 measures the stockiness aspects of the
physique. The two factors are significantly correlated (r = 0.436, (0.049)) where the
standard error is given in brackets.

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

4

Binary data: latent
trait models
4.1

Preliminaries

Binary responses are extremely common, especially in the social sciences.
Individuals can be classified according to whether or not they belong to a trade union
or take holidays abroad. They can be recorded as agreeing or disagreeing with some
proposition or as getting some item in an educational test right or wrong. Such binary
variables are often supposed to be indicators of more fundamental attitudes or abilities,
and it is in these circumstances that latent variable modelling is relevant. Even when
the observed responses fall into more than two categories, it is often useful to conflate
them into two categories. Models for polytomous data are discussed in Chapter 5.
In this chapter we shall consider only the case where the latent variables are
continuous. Because the greatest stimulus for the development of models in this
area has come from the field of educational testing, where the latent variables are
conceived of as ‘traits’, it is usual to speak of latent trait models. This contrasts with
latent class models where the latent space is treated as categorical. Such models are
the subject of Chapter 6.
If we code the possible responses as 0 or 1, the response pattern for any sample
member consists of a string of zeros and ones. If there are p items the number of
distinct response patterns is 2 p . The most economical way of setting out the data
for a sample of size n will depend on the relative magnitudes of n and 2 p . If p is as
large as 50, for example, the possible number of response patterns is approximately
1.1 × 1015 . In such a case only a tiny fraction of possible patterns will occur and
the chance of any one being repeated is negligible. One would then simply list the
response patterns in some convenient order. This has been done with the BSA sexual
attitudes data used in Chapter 6, for example.
Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

83

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

84

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

If 2 p is not too large the data may be set out as a frequency distribution over
response patterns listed in a systematic order. Examples are given in Tables 4.2 and
4.4 which appear in Section 4.13. The first, in Table 4.2, is taken from educational
testing and relates to the Law School Admission Test (Section VI). It is included here
because it has been analysed many times before and so allows us to compare the newer
approaches with earlier work. The second, in Table 4.4, is taken from Schuessler’s
Social Life Feelings Scales (for some background see Schuessler (1982); Krebs and
Schuessler (1987)). The scale was labelled ‘economic self-determination’ and was
based on five items. It is immediately clear that, in both data sets, the different
response patterns occur with very unequal frequencies. The main kind of question to
be addressed in this chapter is how far these patterns can be explained by supposing
that all items depend on one or more latent variables.
The relative frequencies from Tables 4.2 and 4.4 are the empirical counterparts of
the joint probability distribution f (x) which assigns probabilities to the 2 p response
patterns x. For some purposes, especially when testing goodness of fit, it is useful
to specify the joint distribution in terms of a set of marginal probabilities. Thus the
univariate distributions f (xi ) (i = 1, 2, . . . , p) are the first-order marginal distributions; f (xi , x j ) (i, j = 1, 2, . . . , p) the second-order margins and so on. Suppose we
consider the following set of marginal probabilities:
P{xi = 1}

(i = 1, 2, . . . , p),

P{xi = 1, x j = 1} (i, j = 1, 2, . . . , p (i = j)),
P{xi = 1, x j = 1, xk = 1} (i, j, k = 1, 2, . . . , p (i, j, k not equal)),

(4.1)

...,
P{x1 = 1, x2 = 1, . . . , x p = 1}.
The total number of probabilities in this set is
   
 
p
p
p
+
+ ··· +
= 2 p − 1.
1
2
p
Taking account of the fact that the 2 p response probabilities f (x) sum to one, there
are just sufficient marginal probabilities in the above set to allow the use of an
inclusion–exclusion formula to uniquely determine the complete joint distribution.
Either set thus provides a full specification of the joint distribution. The second-order
margins, in particular, tell us about the pairwise associations between the xs, and
the success with which these are predicted by a model will be an important part of
assessing its goodness of fit.

4.2

The logit/normal model

If the dependence among the xs is wholly explained by a vector y of latent variables
they may be regarded as mutually independent random variables with
P{xi = 1 | y} = πi (y) (xi = 0, 1; i = 1, 2, . . . , p),

(4.2)

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

85

where πi (y) is called the response function. Since it is a probability, 0 ≤ πi (y) ≤ 1.
In test theory, where y is usually a scalar, πi (y) is known as the item response function
or item characteristic curve. In that context y represents an ability of some kind, in
which case one would expect πi (y) to be a monotonic function.
The conditional distribution of xi given y is thus
gi (xi | y) = {πi (y)}xi {1 − πi (y)}1−xi

(xi = 0, 1; i = 1, 2, . . . , p).

(4.3)

We have met this distribution in Section 2.1 and, because it is a member of the
exponential family, the general linear latent variable model takes the form
logit πi (y) = αi0 +

q


αi j y j

(i = 1, 2, . . . , p).

(4.4)

j=1

If we complete the specification of the model by assuming that the prior distribution of
y is standard normal with correlation matrix I, then we have the logit/normal model.
Figure 4.1 gives three examples showing the shape of the logit/normal response
function when q = 1.
The parameter αi0 is sometimes called the ‘intercept’ because of its role in the
linear plot of logit πi (y) against y. An alternative parameterisation uses
πi (0) = πi =

1.0
Probability of positive response

P1: TIX/XYZ
JWST070-04

1
.
1 + e−αi0

(4.5)

α10 = 0, α11 = 1.5
α10 = 0.5, α11 = 0.5
α10 = –0.25, α11 = 0.25

0.8

0.6

0.4

0.2

0.0
−3

−2

−1

0
Latent variable y

1

Figure 4.1 Several logistic response functions.

2

3

P1: TIX/XYZ
JWST070-04

86

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

This is the probability of a positive response from an individual with y = 0 – that is,
for someone at the median point of the latent scale.
The parameter αi1 governs the steepness of the curve; in educational testing it
is known as the discrimination parameter because the bigger αi1 the easier it will
be to discriminate between a pair of individuals a given distance apart on the latent
scale. This is because the greater the difference between the value of πi (y) for the
two individuals, the more likely it is that they will give a different response.
If the αi1 are equal the model reduces to the random effects Rasch model. The
equality of the αi1 s implies equal discriminating power, which might be regarded
as a desirable feature of a test on educational grounds. It certainly leads to greater
simplification both conceptually and numerically. The fact that the minimal sufficient
statistic becomes the total score supports the common practice of scaling individuals
on that basis. However, we prefer to base our analysis on the more general case
because, in our experience, it is unusual to find equal αi1 outside the educational field.
In the general case the αi j ( j ≥ 1) are effectively the same as factor loadings
and may thus be interpreted according to the general principles laid out in Section
2.12. Later, in Section 7.8, we shall introduce a standardisation which makes the link
with factor analysis even closer. For the moment we show how the special features of
binary data allow us to approach the interpretation of the loadings from another angle.
From the general theory of the GLLVM we have that the components are

Xj =

p


αi j xi

( j = 1, 2, . . . , q).

(4.6)

i=1

Since xi is either 1 or 0, Xj is simply the sum of those αi j for which xi = 1. For
example, if an individual has the response pattern 0111010 then
X j = 0 + α2 j + α3 j + α4 j + 0 + α6 j + 0.

(4.7)

In effect, therefore, we get the same score for Xj as if, for the jth latent dimension,
we had assigned the score αi j to a positive response on item i – that is, if we had used
the scoring (0, αi j ) rather than (0, 1). There will, of course, be a different scoring for
each of the q dimensions.

4.3

The probit/normal model

An alternative model for binary responses replaces the logit function on the left-hand
side of (4.4) by the inverse of the normal distribution function,
−1 (πi (y)) = αi0 +

q

j=1

αi j y j

(i = 1, 2, . . . , p).

(4.8)

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

87

Since
√
.
logit(u) = (π/ 3)−1 (u),

(4.9)

it is clear that the two models are very close. It follows from (4.9) that the values
of the αs for the probit/normal model
√ will be less than those of the logit/normal
model by a factor of approximately 3/π . In practice, therefore, the two models are
virtually equivalent, though the probit/normal model lacks the sufficiency property
of the linear component X.
The motivation for the introduction of the probit/normal model lies in its relationship to the standard normal linear model of factor analysis. This makes it possible
to transfer much of the methodology and modes of interpretation which are already
well established for that model.
The probit/normal model is an example of what we shall call the underlying
variable approach to modelling categorical data. When applied to the case of
binary manifest variables it supposes that, underlying the ith indicator, there is a
continuous variable, say ξi . The observed binary variable, xi , is then an indicator of
whether or not ξi is above or below some threshold level τi . That is, we suppose (for
i = 1, 2, . . . , p) that

xi =

1 if ξi ≤ τi ,
0 otherwise.

(4.10)

The idea is then to set up a latent variable model for the continuous variable ξ and
to find a way of fitting it using the partial information provided by the indicator
variables, x. In particular, if we assume ξ to be generated by a standard normal factor
model, we shall have
ξ = μ + y + e.

(4.11)

If we can find a way of estimating the parameters, the interpretation will then be
exactly as in factor analysis. Before pursuing this matter, we need to consider whether
postulating an underlying variable in this manner is substantively meaningful. In
some cases it undoubtedly is. Indeed, the dichotomies may have been arrived at in
just that way as, for example, when incomes are recorded as being below or above
a tax threshold. Again, if a person is asked whether or not they are in favour of some
proposition they may be imagined to have a quantifiable degree of enthusiasm which
is imperfectly revealed by their response to a question of the agree/disagree variety.
But there are other cases where the notion is less appealing. The question asked may
be a simple matter of fact such as ‘Were you born in this country?’ In this case it
is difficult to conceive of any relevant underlying variable. However, from a purely
practical point of view, this is immaterial because there is an equivalence between
underlying variable and response function models which we now demonstrate. As
we shall see in Chapter 5, this breaks down if the indicators are polytomous nominal.

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

88

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

4.4

The equivalence of the response function and
underlying variable approaches

To show that the two approaches are equivalent we must show that the joint probability
distributions of x to which they lead are the same. For if this is so then no empirical
information can distinguish between them. Instead of working with the probability
function f (x) we use the equivalent set of marginal probabilities {P{xi = 1, x j =
1, . . .}} defined by (4.1). Let S denote any non-empty subset of {1, 2, . . . , p}; then


  ∞  



P
(xi = 1) = . . . P
(xi = 1 | y) h(y)dy
i∈S


=

i∈S

−∞
∞ 

...
−∞

P{xi = 1 | y}h(y) dy

(4.12)

i∈S

because of the conditional independence of the xs. In the response function approach
P{xi = 1 | y} is the response function and h(y) is the prior distribution of y. To show
the equivalence with the underlying variable approach we have to show in this case
that the left-hand probability of (4.12) admits the same representation. Thus





(xi = 1) = P
(ξi ≤ τi )
P


i∈S


=

i∈S
∞ 

...
−∞



P
(ξi ≤ τi ) y h(y)dy.


(4.13)

i∈S

When y is fixed the ξ s are independent (because the es are independent) and therefore


  ∞ 

P
(xi = 1) = . . .
i∈S

−∞

P{ξi ≤ τi | y}h(y)dy.

(4.14)

i∈S

Now
⎫
⎬
P{ξi ≤ τi | y} = P μi + ei +
λi j y j ≤ τi y
⎭
⎩
j=1


q
τi − μi − j=1 λi j y j
ei
=P
y
≤
1/2
1/2
ψi
ψi


q
τi − μi − j=1 λi j y j
=R
,
1/2
ψi
⎧
⎨

q


(4.15)

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

89

1/2

where R(·) is the distribution function of ei /ψi (note that in the factor model this
does not depend on i). For the response function model,
⎛
P{xi = 1 | y} = G ⎝αi0 +

q


⎞
αi j y j ⎠ ,

(4.16)

j=1

where G is temporarily used to denote the inverse logit or probit function. Equivalence
1/2
between the two models therefore exists if G ≡ R and if αi0 = (τi − μi )/ψi and
1/2

αi j = −λi j /ψi

(i = 1, 2, . . . , p).

(4.17)

Thus for any model of the one kind we can find a model of the other which is
empirically indistinguishable from it. For example, if G −1 is the logit function,
G(u) = (1 + e−u )−1 = R(ei /ψi ).
1/2

The standard logistic random variable with distribution function G(u) has mean
zero and variance π 2 /3; ei therefore has a logistic distribution with mean zero and
variance ψi π 2 /3.
The equivalence of the two methods has also been noted by, for example, Takane
and de Leeuw (1987) and by Moran in unpublished work.
An important corollary of the foregoing analysis is that the parameters {τi },
{μi } and {λi j } of the underlying variable model are not individually estimable. The
1/2
likelihood is a function of the parameters {(τi − μi )/ψi }. This is a reflection of the
fact that there can be no information about the standard deviation of the underlying
ξ s in the dichotomized data. In practice, when working with continuous variables,
we usually throw this information away by first standardising the xs; that is, we
assume that
var(ξi ) = 1 =

q


λi2j + ψi .

j=1

The loadings that we estimate are then


1−

λi j


2
j λi j

1/2 =

λi j
1/2

ψi

(i, j = 1, 2, . . . , p; i = j).

For later reference we emphasise that member of the underlying variable family
which is equivalent to the logit model. In that case G −1 is the logit function, so
G(u) =

1
.
1 + e−u

P1: TIX/XYZ
JWST070-04

90

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

This is the distribution function of a symmetrically distributed random variable with
1/2
zero mean and variance π 2 /3. In the underlying variable model ei /ψi thus has to
have a distribution of this form, which implies that ei has a logistic distribution with
zero mean and variance ψi π 2 /3 for all i.

4.5

Fitting the logit/normal model: the E-M algorithm

The parameters of the logit/normal model can be estimated in various ways. Many
of the earlier methods, some of which were given in the first edition (Bartholomew
1987), were devised more with an eye to ease of calculation than efficiency. Now one
can use efficient methods like maximum likelihood or generalised least squares. In this
section we give the E-M algorithm, and in Section 4.8 we outline a method based on
generalised least squares. The E-M method was first used by Bock and Aitkin (1981).
We shall set out the method for the case of a single latent variable. This is
notationally simpler and can be extended to the general case almost immediately.
The approach also provides the basis for the general method applicable to a wider
class of distributions to be given in Chapter 7.
Dempster et al. (1977) developed the iterative E-M method for maximum likelihood estimation when there are missing observations. In favourable applications
of the E-M method, estimation is easy when there are no observations missing, but
difficult when there is averaging over the distribution of the missing observations.
The missing observations are the values of the latent variables yh . If those were
known, the logit/normal model would be p logistic response models for the xi h for
the explanatory variables yh , and easy to estimate by maximum likelihood.
The E-M algorithm starts from the log-likelihood Lc for the complete data
(xi h , yh ) (i = 1, . . . , p, h = 1, . . . , n), where yh is the value of the latent variable for
individual h:
Lc =

n


ln f (xh , yh ) =

h=1

 p
n


h=1


ln g(xi h | yh ) + ln h(yh ) .

(4.18)

i=1

The E-M algorithm alternates E-steps, which evaluate E[L c | x1 , . . . , xn ], and
M-steps, which maximise E[L c | x1 , . . . , xn ] over the parameters. The expectation
of the E-step uses current estimates for the parameters, and the M-step finds new
estimates for the parameters that achieve the maximisation. Since h(y) does not depend on the parameters, there is no need to use the complete data likelihood; we can
replace Lc by
L=

n


ln g(xh | yh ) =

h=1

and at each E-step find E[L | x].

 p
n


h=1

i=1


ln g(xi h | yh ) ,

(4.19)

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

91

From Section 2.4,
g(xi h | yh ) = (1 − πi (yh )) exp{xi h (αi0 + αi1 yh )},
and so

L=

n



 p
n


ln g(xh | yh ) =
(ln(1 − πi (yh )) + xi h (αi0 + αi1 yh ))

h=1

=

 n
p


i=1

h=1

i=1

ln(1 − πi (yh )) + αi0

h=1

n


xi h + αi1

h=1

n



xi h yh .

h=1

The E-step is therefore

E[L | x] =

 n
p


i=1

E(ln(1 − πi (yh )) | x1 , . . . , xn )

h=1
n


+ αi0
=

xi h + αi1

h=1
 n
p



n



xi h E(yh | x1 , . . . , xn )

h=1

E(ln(1 − πi (yh )) | xh ) + αi0

i=1

h=1

n


xi h + αi1

h=1

n



xi h E(yh | xh ) .

h=1

For the M-step one can maximise separately, for each item i,
n


E(ln(1 − πi (y)) | xh ) + αi0

h=1

n


xi h + αi1

h=1

n


xi h E(y | xh ).

(4.20)

h=1

One can write (4.20) as
 
ln(1 − πi (y))

n


h(y | xh ) + αi0

h=1

n


xi h + αi1

h=1

n



xi h h(y | xh ) dy,

h=1

or, simplifying the notation, as

 
n

ln(1 − πi (y))N (y) + αi0
xi h + αi1 Ri (y) dy,
h=1

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

92

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

where
Ri (y) =
N (y) =

n

h=1
n


xi h h(y | xh ),
h(y | xh ).

(4.21)

h=1

So the M-step is the same as maximum likelihood estimation in a continuous analogue
of logistic regression with Ri (y) positive responses out of N (y) trials at explanatory
variable value y.
Bock and Aitkin (1981) used Gauss–Hermite quadrature to approximate h(y | x h ),
which would work best if that distribution were standard normal. Suppose that the
Gauss–Hermite quadrature uses the discrete distribution at values y 1 , . . . , y k with
probabilities h(y 1 ), . . . , h(y k ). Then approximating the integrals in (4.21) gives
rit =
nt =

n

h=1
n


xi h h(y t | xh ),

(4.22)

h(y t | xh ).

(4.23)

h=1

This leads to the M-step used by Bock and Aitkin, who used maximum likelihood
estimation for a logistic regression with responses rit from nt trials at explanatory
variable values yt for t = 1, . . . , k.
h(yt | xh ) is the
It may be helpful to give an interpretation of rit and nt . The quantity
probability that an individual with response vector xh is located at yt ; kh=1 h(yt | xh )
is thus the expected number of individuals at yt . By a similar argument rit is the
expected number of those predicted to be at yt who will respond positively.
Note that E[L | x] is the log-likelihood of the observed data, and can be
directly maximised, without fixing the parameters in the conditional expectation.
That can allow an interplay between the E-M algorithm and a maximisation of the
observed-data log-likelihood. Newton–Raphson algorithms may be used for direct
maximisation. They are known for being fast but sensitive to initial values. On
the contrary, the E-M is not sensitive to the initial values but convergence can be
slow in high-dimensional problems. Combination of the two methods can give an
improvement.
As noted above, the simple form of the equations in the estimation part of the
algorithm depends on choosing the logit function on the left-hand side of (4.4).
However, the same idea can be used for other members of the general family, and it
was for the probit/normal version that Bock and Aitkin (1981) developed their method.
It should be noted that in both versions it is only necessary to evaluate the
posterior probabilities and expectations for those response patterns that actually
occur. Furthermore, these quantities will be the same for all sample members having
the same response pattern. This is of particular importance when p is large.

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

4.5.1

93

Fitting the probit/normal model

The model can be fitted by the E-M algorithm using essentially the method
described above.
A simple, though less efficient, method is to use the fact that the correlation matrix
of the ξ s can be estimated from the tetrachoric correlations obtained from the xs.
The tetrachoric correlation is that value of the normal bivariate correlation ρi j which
makes the volumes under the normal surface in the four quadrants formed by the
thresholds equal to those in the fourfold association table between xi and xj . If the
tetrachoric coefficients are treated as if they were product-moment correlations, then
the parameters can be estimated using a standard method of fitting the factor model.
More efficient methods have been given based on generalised least squares and
are discussed in Section 4.8.

4.5.2

Other methods for approximating the integral

Gauss–Hermite quadrature as described above is not always the most accurate of the
many ways of approximating the integral required in the calculation of the posterior
distribution. Other methods, which are particularly valuable for large q, are adaptive quadrature (Rabe-Hesketh et al. 2002; Schilling and Bock 2005), Monte Carlo
methods (Sammel et al. 1997) and Laplace approximations (Huber et al. 2004).
Adaptive quadrature is an improved version of the Gauss–Hermite quadrature
described above. The values y 1 , . . . , y k and probabilities h(y 1 ), . . . , h(y k ) are chosen
so that the normal distributions used match the location and dispersion of the posterior
distribution h(y | xh ) for each response pattern xh during every E-step.
Schilling and Bock (2005) show that when there are several factors, this method of
numerical integration can produce fast and accurate solutions using k as low as 2 for
each factor. Assuming that h(y | xh ) can be approximated by a multivariate normal
distribution N (u, ), Schilling and Bock suggest that in the posterior distribution
y are transformed to y∗ , which have approximately uncorrelated standard normal
distributions. One way to do this is to use
y∗ = L (y − μ),
where L is the lower triangle Cholesky factor of  = LL . Then for each dimension of y∗ the values and probabilities for the adaptive quadrature are those for
Gauss–Hermite quadrature. Approximate averaging over the distribution h(y | xh ) is
done with a sum over the values for the y∗ weighted by their probabilities, using
y = Ly∗ + μ, and multiplying by the determinant of L. Schilling and Bock use the
mode, and the inverse information matrix computed at the mode, of h(y | xh ) for μ
and , respectively.
The Monte Carlo method known in the literature as the Monte Carlo E-M (MCEM)
algorithm works by generating a large sample of y(1), . . . , y(R) values from h(y | xh ),
which distribution is not usually available in a closed form. Expectation over the latter
is replaced by averaging over the large sample.

P1: TIX/XYZ
JWST070-04

94

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

A large number of draws R are required to obtain precise estimates, which
slows down convergence of the E-M algorithm. Meng and Schilling (1996) proposed drawing from the posterior distribution h(y | xh ) by using a Gibbs sampler (see
Section 4.11.1). They showed that, by retaining every fifth value generated, the Gibbs
sampler can produce nearly independent samples from the posterior distribution. A
Monte Carlo integration technique has also been successfully applied to structural
equation models (see, for example, Lee and Song (2004a); Lee and Zhu (2002)).

4.6

Sampling properties of the maximum
likelihood estimators

The determination of exact standard errors for the parameter estimates does not appear
to be possible. The usual way to deal with this situation for maximum likelihood and
generalised least squares estimation is to compute the asymptotic variance–covariance
matrix using the information matrix. Even this proves to be impracticable for large
p, and so we shall give a more tractable alternative.
Asymptotically the sampling variances and covariances of the maximum likelihood estimates of the αs are given by the elements of the inverse of the information
matrix evaluated at the solution point. Thus if we have a set of parameters β then


∂2 L
,
{var(β̂)}−1 = E −
∂βi ∂β j β=β̂
in which

∂2 L
=
∂βi ∂β j
h=1
n



1 ∂2 f
1 ∂f ∂f
− 2
f ∂βi ∂β j
f ∂βi ∂β j


(4.24)

where f ≡ f (xh ). On taking the expectation the first term vanishes leaving
{var(β̂)}

−1



1 ∂f ∂f
=n E 2
f ∂βi ∂β j


.

(4.25)

β=β̂

In our case x is a score pattern taking 2 p different values and the expectation in (4.25)
is thus

all x

1 ∂ f (x) ∂ f (x)
.
f (x) ∂βi ∂β j

(4.26)

If p is small it is feasible to evaluate this sum for all i and j and then to invert the
resulting matrix. However, if p is larger the computer resources required to compute
(4.26) become prohibitive. Moreover, many of the probabilities f (xh ) will become
so small that the computation of 1/ f (xh ) will cause overflow on most computing

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

95

machines. In these circumstances an approximation can be obtained by replacing
the expectation of the information matrix by its observed value. This requires the
computation of (4.24) and the calculation of a generalised inverse of the resulting
matrix. Since the first term has expectation zero, a further approximation may be
obtained from the generalised inverse

∗

var (β̂) =

n

h=1

∂ f (xh ) ∂ f (xh )
1
2
f (xh ) ∂βi
∂β j

−1
.

(4.27)

The dimension of this matrix is p + qp ( p αi0 s and p αi j s for the q values of j). The
number of distinct terms in the sum of (4.27) will usually be less than n since more
than one individual may have the same score pattern.
The approximation given by (4.27) appears to be good for standard errors but
less reliable for covariances. Some comparative figures for the case of a single latent
variable are given in Table 4.1 for standard errors, and these are typical of the
differences found in other cases. It therefore seems acceptable to use the simpler
version from (4.27) which, in any case, tends to err on the safe side.
In the case of two or more latent variables the computation of standard errors
requires more care. The position is essentially the same as for the normal linear model
set out in Section 3.6 because of the rotational indeterminacy. If we impose sufficient
constraints on the parameters to ensure that there is a unique maximum, then we
can use the standard asymptotic approach as for the normal latent variable model.
Without such constraints the maximum is not unique and the information matrix will
be singular. This may not be true for the approximation of (4.27), but it is not clear
that this then has any meaning.
We showed in Section 2.12 how to transform any solution for the maximum
likelihood estimates into one satisfying some desired orthogonality condition. In
principle, therefore, we could use the asymptotic theory to find standard errors for the
rotated solution. Standard errors could be estimated by simulation, as in the one-factor
case, but this is currently very time-consuming.

4.7

Approximate maximum likelihood estimators

In certain circumstances it is possible to approximate the likelihood function in a
manner which allows us to obtain the maximum likelihood estimators explicitly. As
the result for binary data is a special case of a more general result for polytomous data
to be given in Section 5.6, we merely quote the result here. The method starts with the
matrix of phi-coefficients between all pairs of the p variables. (The phi-coefficient is
the product-moment correlation of the binary variables.) We then find the eigenvectors
associated with all eigenvalues greater than one. Let g be the normalised vector
associated with the largest eigenvalue; then there is a vector α = (α11 , α21 , . . . , α p1 )

P1: TIX/XYZ
JWST070-04

96

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

given by
α ∝ Q̂−1/2 g,

(4.28)

where Q̂ is a diagonal matrix with ith diagonal element equal to π̂i (1 − π̂i ), which
maximises the approximate likelihood. The constant of proportionality is determined
in a manner to be explained later. The vector associated with the second largest eigenvalue provides a set of estimators for a second factor, and so on for all eigenvalues
exceeding one. There is an obvious similarity between this method and a principal
components analysis of the phi-coefficient matrix. However, the methods give different loadings because of the matrix Q̂ and the constant of proportionality in (4.28).
Like all of the approximate methods, this is concerned only with the αi j for j ≥ 1;
the αi0 or πi can easily be estimated approximately using a method given in the first
edition (Bartholomew 1987, pp. 118–119) as
1/2



3
α̂i0 = 1 +
π

2
j≥1 αi j

−1 (Pi ),

(4.29)

where Pi is the proportion with the score 1 on item i.

4.8

Generalised least squares methods

Before the introduction of the E-M algorithm for obtaining maximum likelihood
estimators, two other methods were devised for circumventing the heavy calculations
required for full maximum likelihood. These were based on weighted least squares,
and, as they have the advantage of being applicable when there are several latent variables, we now give a brief outline of them. They were proposed for the probit/normal
model, but the idea could readily be extended to the other members of the family.
Both methods start with the presumption that most of the relevant information
in the sample data is contained in the first- and second-order margins. A heuristic
justification for this is obtained by considering the underlying variable model. If the ξ s
are known, then we know that the sample covariance matrix is sufficient for the model
parameters, and these require only a knowledge of the bivariate distributions. One
would expect the bivariate distributions of the indicator variables, x, to contain almost
all of the information about the underlying bivariate distributions. This is supported
by an approximation to the likelihood function, given below, which depends only
on the first- and second-order margins. Retrospectively, of course, one can see that
the sampling errors of the least squares estimators are virtually the same as those
obtained by maximum likelihood.
The Christoffersson (1975) method was to choose parameter estimates which
minimise the distance, in a least squares sense, between the observed and expected
first- and second-order marginal proportions. Let P̂i be the observed proportion who
respond positively on variable i, and P̂i j the proportion for variables i and j, with Pi
and Pij being the corresponding population values. The parameters are then estimated

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

97

by minimising
SS = (P̂ − P)  −1
p (P̂ − P),

(4.30)

where P = (P1 , P2 , . . . , Pp , P12 , P13 , . . . , Pp, p−1 ) (of dimension p + 12 p( p − 1))
and  p is the covariance matrix of P. Christoffersson (1975) showed that a consistent
estimator of  p could be obtained and hence that efficient estimators could be found.
( p involves third- and fourth-order marginal proportions.)
Although this method is much faster than full maximum likelihood, Muthén
(1978) showed that a further improvement was possible. At each stage of the iterative
minimisation of (4.30) one has to evaluate the integrals

P(τi ) =

τi
−∞


φ(u) du, P(τi , τ j ) =

τi
−∞



τj
−∞

φ(u 1 , u 2 , : li j )du 1 du 2 ,

(4.31)

where φ is the standard univariate (or bivariate) normal density function. Muthén’s
method was to invert equations (4.31) to give
τi = −1 (P(τi )), li j = f (P(τi , τ j ), P(τi ), P(τ j ))

(i, j = 1, 2, . . . , p; i = j),
(4.32)

and then to fit the model by minimising the weighted squared distance between the
parameters of (4.32) and their sample estimates. To obtain the weights, Muthén used
a Taylor expansion expressing the estimator t̂ of t = (τi , . . . , τ p , l12 , . . . , l p−1, p ) in
the form
t̂ = t + δ
and showing that the covariance matrix of δ could be consistently estimated. This
method avoids the repeated integration involved in Christoffersson’s approach while
retaining the same asymptotic efficiency. The sample estimators of {lˆi j } are, of course,
the tetrachoric correlations, so the method is analogous to the weighted least squares
method for normal models. The difference lies in the use of tetrachoric correlations,
with consequent differences in the weighting.

4.9

Goodness of fit

The theoretical hypothesis is H0 : π = π (α), where π is the true probability and
π(α) is the probability computed from the model. In practice, the sample proportions
P are compared with the estimated ones which are π (α̂), where α̂ can be either
the maximum likelihood estimator or any other. If n is large compared with 2 p , the
expected frequency for each response pattern is likely to be large enough to carry out
a valid chi-squared (χ 2 ) or log-likelihood ratio (G2 ) test to compare the observed and
expected frequencies. Up to a point, a few small expected frequencies can be dealt

P1: TIX/XYZ
JWST070-04

98

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

with by pooling response patterns so that all are greater than 5, say. In the unpooled
case the number of degrees of freedom will be 2 p − p(q + 1) − 1. As 2 p becomes
larger the amount of pooling needed may be such as to reduce the degrees of freedom
to zero, at which point no test can be done. It is common to have values of p ≥ 10,
large enough to make this a common problem. We can think of the data as set out
in a 2 p contingency table and so our problem is an example of how to test goodness
of fit in sparse contingency tables. We shall meet the problem again in relation to
the latent class model in Chapter 6. The problem has been investigated in that case
by Collins et al. (1993) and, in a more general context, by Langeheine et al. (1996).
The specific case of the fit of the latent trait model is treated in Bartholomew and
Tzamourani (1999). The main conclusion of this work is that the best way to do a
global test of fit is to generate the empirical sampling distribution of the statistic (χ 2
or G2 ) using the parametric bootstrap method. This proceeds as follows:
1. Fit the desired model in the usual way.
2. Generate a random sample (of the same size) from the population in which
the parameter values are equal to those estimated for the actual sample.
3. Fit the model in each case and compute the chosen test of fit.
4. Compare the actual value of the statistic with the bootstrap sampling
distribution.
The number of bootstrap samples needs to be large enough to give a reasonable
estimate of the sampling distribution, but the time needed to do the calculation will
increase proportionally.
A global test is a useful first step but, on its own, it may fail to reveal much about
the nature of any departure from the model. In particular, it may happen that just two or
three items are responsible for a poor fit because, perhaps, they also depend on some
factor peculiar only to them. The effect of this may be diffused across all response
patterns and so be difficult to detect by looking at the individual contributions to χ 2 .
Therefore, we recommend carrying out supplementary tests designed to detect the
effects of such inappropriate items.
These tests are based on residuals calculated from the marginal frequencies of various orders. In Section 4.1 we showed that the set of response patterns was equivalent
to a set of marginal probabilities. The first-order margins, P{xi = 1} (i = 1, 2, . . . , p)
contain no information about the dependencies among the xs. The higher-order margins do contain that information, beginning with the simple pairwise associations
derivable from probabilities of the form P{xi = 1, x j = 1}. A model which successfully predicts these associations may be useful even if it fails to capture the
higher-order dependencies. We note that we can obtain all other pairwise probabilities using results such as P{xi = 0, x j = 1} = P{x j = 1} − P{xi = 1, x j = 1}.
Similarly, all second-order marginal probabilities for xi , x j , xk can be derived from
P{xi = 1, x j = 1, xk = 1} and the lower-order margins. In practice the second- and
third-order margins usually suffice.

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

99

If O denotes the observed frequency for any marginal probability and E the
corresponding expected frequency, we define the residual as R = (O − E)2 /E. (Note
that such a residual is not one of the terms making up the global χ 2 test.) Large values
of R for the second-order margins will identify pairs of xs which are more (or less)
strongly associated than the model predicts. Such an excess of association might point
to some extraneous factor affecting only those two variables. In the binary case, Reiser
and VandenBerg (1994) and Bartholomew et al. (2008) used residuals extensively
for identifying pairs and triplets of items where the fit was poor. However, only
recently have proper goodness-of-fit test statistics based on the lower-order observed
and expected margins probabilities been developed. These are also known as limitedinformation goodness-of-fit tests (Reiser 1996; Bartholomew and Leung 2002; Cai
et al. 2006; Maydeu-Olivares and Joe 2005).
Let us denote by P = (P1 , . . . , P2 p ) the vector of sample (observed) proportions
for all possible distinct response patterns with a corresponding vector of estimated
probabilities under the model by π(α̂) = (π1 (α̂), . . . , π2 p (α̂)). In general, the fit of
a model is judged by how close the estimated proportions π(α̂) are to the sample
proportions P. One can obtain the lower-order probabilities by pre-multiplying the
overall probabilities by an indicator matrix.
A test statistic based on the lower-order residuals is
n(Pl − π l (α̂))  l−1 (α̂)(Pl − π l (α̂)),

(4.33)

where l indicates residuals obtained either from lower-order margins such as univariate (l = 1) or bivariate (l = 2) or up to the lth margins, and Pl and π l (α̂) are
the sample and estimated proportions under the model respectively for the lth-order
margins. The covariance matrix  l (α̂) can be obtained from
 l = l − l I −1 l ,
evaluated at α̂, where
l =

∂π l (α)
,
∂α

√
I is the Fisher information matrix and l is the covariance matrix of n(Pl − π l ).
Using known results from multivariate theory and under the null hypothesis (H0 :
π = π(α)), (4.33) follows a χ 2 distribution with degrees of freedom equal to the
rank of the asymptotic variance–covariance matrix given by  l . Maydeu-Olivares
and Joe (2005) suggested using the Moore–Penrose inverse of the covariance matrix;
however, they noted that its calculation might not be always feasible due to the
presence of eigenvalues close to zero. To overcome this problem an alternative test
statistic has been proposed by Maydeu-Olivares and Joe (2005) given by
n(Pl − π l (α̂)) M̂l (Pl − π l (α̂)),

(4.34)

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

100

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

where
ˆ lc ([
ˆ lc ] 
ˆ lc ]
ˆ l
ˆ lc )−1 [
M̂l (α̂) = 
ˆ lc is a full-rank orthogonal complement to 
ˆ l . Under the null hypothesis, (4.34)
and 
follows a χ 2 distribution with degrees of freedom equal to the number of sample
moments up to order l minus the total number of free parameters.
A closely related problem to that discussed here has been the subject of a good
deal of research in item response theory. This is concerned with testing for unidimensionality. If one wishes to construct a unidimensional scale using a latent variable
model, one needs to know whether a single latent variable is sufficient to explain
the distribution of response patterns. Here we are looking for a particular kind of
departure from the logit/normal model. Therefore it is relevant to know the best
way of detecting the effect of additional latent variables. The approach initiated by
Holland (1981) and continuing with Rosenbaum (1984) and Holland and Rosenbaum
(1985) is concerned with the restraints which unidimensionality imposes on the joint
distribution f (x). In particular, these authors arrive at a requirement of conditional
association. Roughly speaking, this says that the pairwise associations, conditional
on all other variables, must be positive. Looking at these conditional associations
is somewhat similar to inspection of the residuals. A different approach to testing
unidimensionality is found in Stout (1987, 1990). He bases a test on the proportion
of positive responses and then extends the idea to what he terms essential unidimensionality. This condition allows minor variations from strict unidimensionality.
Junker (1993) reviews these approaches and establishes the links between them.
An important feature of these methods is that they are non-parametric in the sense
that they make no assumption about the form of the item response functions or of
the prior distribution. They are not, therefore, tests of the logit/normal model but the
information they give is relevant to that question.

4.10

Posterior analysis

We have given some general results applicable to all members of the GLLVM family
of models in Section 2.15. Here we illustrate them for the case of binary data and
add one or two further results. In principle, all the information about y is contained
in the posterior distribution h(y | x). In the case of the logit/normal model we have
seen that this distribution depends on x only through the q-variate sufficient statistic
X = Ax, where A = {αi j }. Much of the interest in posterior analysis lies in its use
for scaling, and this aspect has been especially prominent in the measurement of
abilities. In that context one is usually dealing with a single latent dimension, and for
the remainder of this section we shall concentrate on that case. Some of the results
can easily be extended to the case of q latent variables. We quoted the result of Knott
and Albanese (1993) that if the posterior distribution was normal for one response
pattern, say (0, 0, . . . , 0), then it would be normal for all response patterns. In that
event, they showed that the posterior mean was a linear function of X. Chang and
Stout (1993) subsequently showed that the posterior distribution tends to normality as

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

101

p, the number of items, increases without limit. In practice the normal approximation
and the consequent linearity of the posterior mean in X appear to be good if p is quite
small. This can be checked in the case of Example 4.13.2 below.
The posterior variance, or standard deviation, of y is a useful measure of the
precision of the predicted scale point. In the context of test theory this is usually known
as reliability since it has to do with the reproducibility of a measurement. The posterior
variance serves this purpose. By convention we assume the prior variance to be unity.
After we have observed x we shall be less uncertain about the location of the subject
on the scale of y, and this will be reflected in the smaller value obtained for var(y | X ).
If this turns out to be around 0.35, as in Example 4.13.2, then we can say that 65%
of the original variation in y has been removed by the information contained in X.
The traditional measure of reliability has been based on the variance of X given
y, and not the other way round as proposed here. Since y is never known, the average
has been taken with respect to y. Thus the coefficient is
r =1−

E var(X | y)
.
var(X )

(4.35)

This tells us by how much we would expect the variance of X to be reduced by
knowledge of y. This coefficient is also equivalent to the test–retest correlation (see,
for example, Bartholomew and Schuessler (1991)). If we were to take a random
sample of individuals and obtain two independent measurements of X on each, then
r is the correlation between the first and second determinations. A reliable method
of scaling should yield a close correspondence between the two measures. The use
of (4.35) for determining the reliability of measures derived from the logit/normal
model was illustrated by Bartholomew and Schuessler (1991). A disadvantage of r
is that it is based on an average variance, and this variance might depend strongly
on y. More importantly, it inverts the natural way of regarding X and y. When X has
been observed and y is unknown, it is more natural to condition on X rather than
to consider the variation of something which is known conditional on something
which is not.
One can thus define an average posterior variance in an analogous way, giving
r∗ = 1 −

E var(y | X )
,
var(y)

(4.36)

as was done in Bartholomew (1995a) (see also Bartholomew (1996, section 2.2)). In
that paper a common approximation to both r and r ∗ was obtained for the case where
xi has a conditional exponential family distribution. This implies that r and r ∗ are
approximately equal, and numerical estimates of both show that this is usually
the case.
However, once the relevance of var(y | X ) is accepted, averaging over X seems
unnecessary. It is more useful to attach the appropriate conditional variance (or
standard deviation) to each prediction. In actual fact the dependence of var(y | X ) on
X is usually quite slight. Such departure as there is from constancy occurs for the
extreme response patterns where most responses are the same.

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

102

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

4.11

Fitting the logit/normal and probit/normal
models: Markov chain Monte Carlo

The main features of Bayesian estimation methods and, in particular, the MCMC
methodology are outlined in Section 2.9. As already mentioned in Chapter 2,
Bayesian inference is based on the posterior distribution of the vector of all random parameters conditionally on the data, and MCMC is primarily used for helping
to make draws from that posterior distribution. MCMC methods, such as the
Metropolis–Hastings algorithm and its special case, Gibbs sampling (Gelfand and
Smith 1990), can be used to estimate the parameters of latent variable models.
In particular, Metropolis–Hastings within Gibbs sampling is used for estimating the
logit/normal model in (4.4), whereas Gibbs sampling can be used for the probit/normal
model in (4.8).

4.11.1

Gibbs sampling

Gibbs sampling (Geman and Geman 1984; Gelfand and Smith 1990), is a way of
sampling from complex joint posterior distributions. Consider the posterior distribution h(v | x), where it will be convenient here to consider partitioning the vector of
unknown parameters v into two components, v1 = y and v2 = α. Other partitions
might be more convenient for other problems. When it is computationally difficult to
take draws from the joint posterior of all parameters, it might be easier to simulate
from the full conditional distributions, here h(y1 | α 0 , x) and h(α 1 | y1 , x). Gibbs sampling produces a sequence of iterations v0 , v1 , . . . , vk that form a Markov chain that
eventually converges to its stationary distribution, which is the posterior distribution.
It can be summarised in the following two steps:
1. Start with initial guesses of y0 , α 0 .
2. Then simulate in the following order:
(a) draw y1 from h(y | α 0 , x),
(b) draw α 1 from h(α | y1 , x),
where the conditional distributions are given by
h(y | α, x) =
h(α | y, x) =

g(x | y, α)h(y, α)
,
g(x | y, α)h(y, α)dy
g(x | y, α)h(y, α)
.
g(x | y, α)h(y, α)dα

(4.37)
(4.38)

Step 2 is iterated t times generating the sample (vt = vt1 , vt2 ). For suitably large t,
the joint distribution of vt will be close to the joint distribution of v. So that will
be considered as one simulated value from the posterior distribution of v. Note that
all the draws up to t are discarded (burn-in period) and t should be large enough to

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

103

guarantee convergence. Repeating the above whole process s times gives a sample
of size s from the posterior distribution. That sample can be used to approximate the
mean and variance of the posterior distribution of each model parameter.
Note that each distribution used at step 2 is called a full conditional distribution;
it is multivariate and, depending on the number of items and number of sample
members, can be heavily multidimensional. That problem can be easily solved by
arranging the blocks of parameters to have smaller dimensions. In latent trait models,
one might block the parameters (α i , i = 1, . . . , p) separately for each item (p blocks),
and further form n blocks, one for each subject’s latent vector (yh , h = 1, . . . , n). In
that case, the vector of parameters v consists of n + 2 p components and the number
of transitions required under step 2 will be equal to n+p. The elements of each of
those components will not then usually be bigger than three or four dimensions.
Gibbs sampling is particularly useful when the full conditional distributions given
in (4.37) and (4.38) exist in closed form. However, the normalising constants that
appear in the denominator of the posterior distributions are not often easy to compute.
Methods based on data augmentation, rejection sampling and Metropolis–Hastings
have been used with Gibbs sampling to simplify or avoid the computation of the
normalising constants. Albert (1992) proposed a data augmentation scheme within
Gibbs for estimating the parameters of the probit/normal of (4.8). The data augmentation scheme borrows the idea of the underlying variable approach discussed
in Section 4.3. The observed binary variable, xi , is assumed to be a manifestation
of an underlying continuous variable ξi . Suppose that when the underlying variable
ξi is positive the observed binary response is 1, and 0 otherwise. Furthermore, suppose that the underlying variable ξ conditional on the latent variable is normally
distributed with mean αi0 + αi1 yh and variance 1. That leads to the known linear
factor model,
ξi h = αi0 + αi1 yh + i h

(i = 1, . . . , p; h = 1, . . . , n),

(4.39)

where the error term i h ∼ N (0, 1). We would like to draw samples from the
joint posterior
h(ξ , y, α | x) ∝ g(x | ξ )h(ξ | y, α)h(y)(α),

(4.40)

but instead we draw samples from the full conditional posteriors, h(ξ | y, α, x),
h(y | ξ , α, x) and h(α | ξ , y, x), where for the one-factor model the vector ξ is of
dimension n × p, the vector y is of dimension n × 1 and the vector of model parameters α is of dimension p × 2. Prior information needs to be provided for all model
parameters. Usually, yh ∼ N (0, 1), αi1 ∼ N(0,∞) (μai1 , Sα2i1 ) and αi0 ∼ N (μai0 , Sα2i0 ),
where (μai1 , μai0 ) and (Sα2i1 , Sα2i0 ) are means and variances respectively chosen by the
user. The steps of the Gibbs sampling at a given step t − 1 are as follows:
1. Suppose that at iteration t − 1 the values of all model parameters, including
latent and underlying variables, are ξ (t−1) , y(t−1) , α (t−1) .

P1: TIX/XYZ
JWST070-04

104

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

2. Then simulate in the following order:
(t−1)
(a) Draw ξi(t)
, α (t−1) , x). The random variables ξi h are
h from h(ξ | y
assumed to be independent conditional on the latent variable and model
parameters, and to have a truncated normal distribution with mean
αi0 + αi0 yh and variance 1. The distribution is truncated to the right at
0 when the observed binary response is 0 and to the left at 0 when the
observed response is 1.

(b) Conditional on current values of the underlying variables and item parameters, the model in (4.39) becomes
(t−1)
(t−1)
ξi(t)
= αi1
yh + i h
h − αi0

(i = 1, . . . , p; h = 1, . . . , n),

with unknown parameters the latent variables yh . We continue by drawing
yh(t) from h(yh | ξ (t) , α (t−1) , x), which turns out to be normally distributed
with mean
p
(t)
(t−1) (t−1)
)αi1
i=1 (ξi h − αi0
p
2(t−1)
+1
i=1 αi1
and variance
1

p

2(t−1)
i=1 αi1

+1

.

(c) Finally, conditional on current values of the underlying variables and latent
variables, (4.39) becomes
(t−1)
(t−1) (t)
ξi(t)
+ αi1
yh + i h
h = αi0

(i = 1, . . . , p; h = 1, . . . , n),

with unknown item parameters. That model resembles the linear regres(t)
(t)
sion model. The draws (αi0
, αi1
) come from the posterior distribution
of the vector of item parameters (αi0 , αi1 ), conditional on the latent and
underlying variables that can be shown to be normal with mean
"−1 !
"
!
X ξ +  −1 [μai1 , μai0 ]
X X +  −1
and covariance matrix
!
"−1
X X +  −1
,
where





−1

Sα2i1
=
0

0
Sα2i0



and X is the covariate vector with elements (yh , 1).

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

4.11.2

105

Metropolis–Hastings

A method that altogether avoids the computation of the normalising constant is the
Metropolis–Hastings (M-H) algorithm (Metropolis et al. 1953; Hastings 1970). For
the M-H algorithm at each step, the next state is chosen by sampling a candidate from
a proposal distribution q(·) that can be of any form. Independently of the form of the
proposal distribution, the chain will converge to the desired distribution π (y, α).
The transition kernel for the M-H algorithm is given by
t[(y0 , α 0 ), (y1 , α 1 )] = q[(y0 , α 0 ), (y1 , α 1 )]a[(y0 , α 0 ), (y1 , α 1 )] + I [(y1 , α 1 )



0
0
∗
∗
0
0
0
0
∗
∗
∗
∗
= (y , α )] 1 − q[(y , α ), (y , α )]a[(y , α ), (y , α )]d(y , α ) , (4.41)
where I (·) is an indicator function taking the value 1 when the statement in brackets
is true and 0 otherwise. The first component of (4.41) occurs when the candidate state
(y1 , α 1 ) has been accepted and the second term occurs from a rejection probability
for all possible candidates (y∗ , α ∗ ). The proposal distribution q is a function of the
current state of the Markov chain (y0 , α 0 ) and the candidate state (y∗ , α ∗ ), and a is the
acceptance probability that determines whether the candidate state will be accepted
as the next state of the chain, defined as

π (y∗ , α ∗ )q[(y∗ , α ∗ ), (y0 , α 0 )]
.
a[(y , α ), (y , α )] = min 1,
π (y0 , α 0 )q[(y0 , α 0 ), (y∗ , α ∗ )]
0

0

∗

∗



(4.42)

The normalising constant cancels out in the computation of the ratio of the π (·)
densities of (4.42).
For a given value (y0 , α 0 ), the proposal distribution q[(y0 , α 0 ), (y∗ , α ∗ )] is a
density function of (y1 , α 1 ). If the candidate is accepted, then (y1 , α 1 ) = (y∗ , α ∗ ),
otherwise we generate another candidate. One can show that under general conditions the sequence (y0 , α 0 ), (y1 , α 1 ), (y2 , α 2 ), . . . is a Markov chain with stationary
distribution π (y, α).
In theory one can choose any proposal distribution. Depending on the choice of
the proposal distribution q, the algorithm might converge to its stationary distribution
after very few or very many steps. Usually, known distributions such as the uniform,
beta, gamma, normal or t distribution are chosen. Common forms of the transition
kernel are the independence chain and the random walk chain. For the independence
chain, the proposal distribution q[(y0 , α 0 ), (y1 , α 1 )] does not depend on the current
state (y0 , α 0 ); it will work effectively when the proposal is close to the target stationary
distribution π since the acceptance probability will be close to 1 and accept nearly all
candidates. In contrast, in the random walk chain the proposal distribution depends
on the current state of the chain. The candidate value is considered to be the current
state plus some noise that might be coming from a known distribution such as the
normal with some variance σ 2 .
The number of iterations until convergence or, in other words, the number of
candidate states being rejected will increase with the number of parameters.

P1: TIX/XYZ
JWST070-04

106

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

To avoid the slow convergence of the M-H algorithm and the problem of
computing the normalising constant required in the Gibbs sampling, one can
use both algorithms together. Patz and Junker (1999b) discuss rejection sampling
techniques, such as the M-H, within Gibbs sampling. In essence, an M-H sampling
scheme is used within the Gibbs for drawing samples from the complete conditionals,
which need only be defined up to a normalising constant. M-H avoids the calculation
of the normalising constants by performing draws using rejection sampling.
We now provide the steps of an M-H within Gibbs sampling algorithm for
estimating the parameters of the logit/normal model of (4.4). For binary response
the complete conditionals required for Gibbs sampling are not in closed form, and
so M-H will be used to avoid computations of the normalising constants. To proceed
we make the following assumptions:
1. All model parameters, including the latent variable, are assumed to be a priori
independent,
p

n

h(y, α) =

h(yh )
h=1

h(αi0 )h(αi1 ).
i=1

2. Prior distributions are defined for all the random components of the model as
follows: yh ∼ N (0, σ y2 ), h = 1, . . . , n; α0i ∼ N (0, σα2i0 ) and αi1 ∼ N (0, σα2i1 ),
i = 1, . . . , p. Often a log-normal is assumed for the αi1 parameters to guarantee all positive loadings. Alternatively, α i ∼ N (0,  αi ), where  αi is the
variance–covariance matrix of the parameters related to item i. To simplify
things, assume that all variances are the same across items, giving σα2i0 = σα20
and σα2i1 = σα21 .
3. The draws will be done separately for each yh , h = 1, . . . , n, and for each
vector of parameters α i = (αi0 , αi1 ), i = 1, . . . , p. We need two proposal
distributions, namely, q y = (yh∗ , yhk−1 ) and qα = (α i∗ , α ik−1 ). Furthermore, as
stated earlier, proposal distributions are taken to have known forms. Here,
the normal is assumed: q y = (yh∗ , yhk−1 ) ∼ N (yhk−1 , σ y2 ), h = 1, . . . , n, and
qα = (α i∗ , α ik−1 ) ∼ N (α ik−1 , σα2 ), i = 1, . . . , p, with means equal to the value
at the current state k − 1.
4. We assume symmetrical proposal distributions and therefore the proposal distribution need not be included in the calculation of the acceptance probabilities.
The details of an M-H within Gibbs sampling algorithm for estimating the parameters
of the two-parameter logistic model are given below. For iteration k, do the following:
1. Draw yk from the complete conditional, h(y | α k−1 , x). Since the complete
conditional is not given in a closed form we use the M-H algorithm:
(a) Draw yh∗ from N (yhk−1 , σ y2 ) independently for each h = 1, . . . , n.

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

107

(b) Compute the acceptance probability a,

a[yhk−1 , yh∗ ]

= min 1,

g(xh | yh∗ , α k−1 )

h(yh∗ , α k−1 )



g(xh | yhk−1 , α k−1 ) h(yhk−1 , α k−1 )

#p
k−1
∗
) h(yh∗ )
i=1 g(x i h | yh , α i
= min 1, # p
,
k−1 k−1
) h(yhk−1 )
i=1 g(x i h | yh α i


(4.43)

where
g(xi h | yh∗ , α ik−1 ) = [πi (yh∗ , α ik−1 )]xi h [1 − πi (yh∗ , α ik−1 )](1−xi h )
and
h(yh∗ )
h(yhk−1 )

=

exp(− 2σ1 2 (yh∗ )2 )
y

exp(− 2σ1 2 (yhk−1 )2 )

.

y

2. Draw α ik from the complete conditional, h(α i | y k , x). Again here, the complete
conditionals are not given in a closed form and therefore the M-H algorithm
is used:
(a) Draw α0i∗ from N (α0ik−1 , σα20 ), i = 1, . . . , p.
(b) Draw α1i∗ from N (α1ik−1 , σα21 ), i = 1, . . . , p.
(c) Compute the acceptance probability a for each item i,

ai [α ik−1 , α i∗ ]

= min 1,

g(xh | y k , α i∗ )

h(y k , α i∗ )



g(xh | y k , α ik−1 ) h(y k , α ik−1 )


#n
k
∗
h(α i∗ )
h=1 g(x i h | yh , α i )
= min 1, #n
,
k−1
k
) h(α ik−1 )
h=1 g(x i h | yh , α i

(4.44)

where
g(xi h | yhk , α k−1 ) = [πi (yhk , α k−1 )]xi h [1 − πi (yhk , α k−1 )](1−xi h )
and
h(α i∗ )
h(α ik−1 )

=

exp(− 2σ1 2 (α0i∗ )2 ) exp(− 2σ1 2 (α1i∗ )2 )
α

exp(− 2σ1 2 (α0ik−1 )2 )
α

α

exp(− 2σ1 2 (α1ik−1 )2 )
α

.

More details on the M-H within Gibbs for latent variable models can be found in Patz
(1996) and Patz and Junker (1999b).

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

108

4.11.3

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Choosing prior distributions

For all the parameters defined in v we usually assume independent prior distributions.
Diffuse or vague priors are needed in most applications, so that the likelihood is
emphasised rather than the prior, unless one would like to impose some restrictions
on the parameters such as positive loadings or values within a specific range. The
latent variables y are a random sample from a known population, usually independent
standard normal distributions.

4.11.4

Convergence diagnostics in MCMC

A main concern in MCMC estimation methods is to assess whether and at what
stage the distribution of the parameter values produced by the Markov chain may be
considered to be from the stationary distribution of the chain, which is the posterior
distribution of the parameters given the data. Most of the work developed in this
direction is based on diagnostic tools that rely for those conclusions on the MCMC
output. The most popular diagnostic tools are that of Heidelberger and Welch (1983),
Geweke (1992), Raftery and Lewis (1992b) and Gelman and Rubin (1992). Their
popularity greatly depends on their availability in freely distributed software such as
BUGS (Spiegelhalter et al. 1996). For a review and comparison of all the currently
available diagnostic tools one should read Cowles and Carlin (1996) and Brooks and
Roberts (1998). We briefly discuss a few of those diagnostics.
Geweke’s (1992) criterion compares the mean of the parameter values from two
sections of the Markov chain using a z test. The criterion uses two sections of the
Markov chain. The first section contains, for example, the first 10% of the iterations
and the second section contains the last 50% of the iterations. If the whole chain is
stationary then the mean of the parameter values in the early stage and in the late
stage should be similar. The test is based on a z-score that compares the difference
in the two means divided by the asymptotic standard error of the difference. As the
chain length tends to infinity the distribution of z tends towards standard normal.
Absolute values of z greater than 2 indicate that the chain has not converged. In the
case of non-convergence, the first 10% is discarded and Geweke’s diagnostic score
is recomputed to compare the first 20% with the last 50%.
The Raftery and Lewis (1992b) diagnostic tool aims to check the accuracy of
the estimation of quantiles and computes the number of iterations required in order
to achieve a certain level of accuracy in estimating quantiles. A statistic computed
within that criterion is the dependence factor, computed as I = N /Nmin , where N
is the maximum number of iterations needed to achieve convergence and Nmin is
the minimum number. The dependence factor measures the increase in the number
of iterations needed to reach convergence due to dependence between the samples
in the chain. Values of I greater than 1.0 indicate high within-chain correlations
and probable convergence failure. Raftery and Lewis (1992a) suggest that I ≥ 5.0
indicates that a reparameterisation of the model might be needed.
Heidelberger and Welch (1983) developed a convergence test that uses the
Cramér–von Mises statistic to test the null hypothesis that the sampled values (either
the whole chain, or after discarding some percentage of the values of the chain) come

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

109

from a stationary distribution. Gelman and Rubin (1992) monitor many parallel chains
that are initialised with different starting values. Convergence is diagnosed when the
chains have ‘forgotten’ their initial values, and the output from all chains is the same.

4.12

Divergence of the estimation algorithm

As with the normal model (see Section 3.12.3), it sometimes happens that the iterative
estimation routine fails to give a sensible result. In the normal case this happened
when one of the residual variances converged to zero – known as a Heywood case. The
analogous situation with the present model occurs when the iterative estimation procedure fails to converge. This can happen when one of the αs appears to be increasing
without limit. Experience shows that this situation is particularly prone to occur:
(a) when the sample size is small – a few hundred or less;
(b) when the number of variables is small;
(c) when the αi j (i ≥ 1) are very unequal.
In practice this behaviour is less serious than might appear at first sight since,
though the estimate may become very large, the likelihood changes very little and
the fit of the model is hardly affected. The situation can be dealt with most simply
by adopting a cut-off value of 10, say, which terminates the algorithm if any estimate
reaches this value. Knott and Albanese (1992) investigated the situation in greater detail and pointed out that large estimated values may arise because the true α is, in fact,
infinite. This means that the response function degenerates into a step function with
all those above some critical value of y responding with probability 1 and those below
with probability 0. This is not an unreasonable supposition, and we shall see in Chapter 6 that latent class models involve an implicit assumption of precisely this kind.
In addition to the above, in a Bayesian estimation framework, rotational indeterminacy often leads to computational problems such as non-convergence or label
switching among the factors in the multidimensional case. Lopez and West (2004)
suggested constraints for the classical factor analysis model, but no systematic work
has been done for the factor analysis model for categorical manifest variables. Reparameterisations of the model are in some cases necessary to avoid lack of convergence
when large discrimination parameters occur.

4.13

Examples

We give three examples chosen to illustrate many of the uses of the logit/normal
model. The first, based on data from the Law School Admission Test, is a classical
example of the investigation of latent traits in educational testing. The second is taken
from a study of attitude scaling where the items are the responses to questions in a
survey. The third example, taken from a fertility survey in Bangladesh, is more typical of the use of the model in the general exploration of relationships between

P1: TIX/XYZ
JWST070-04

110

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

binary variables in multivariate analysis. Some of the results are set out in the
following tables, but much more detail will be found in the output files produced
by the IRTPRO software (Cai et al. 2011). In our examples, we have not discussed
questions of missing values in the data. All of the data sets are presented in complete
form. In some cases this has been achieved by deleting cases or imputing values for
missing observations. Missing values could be included in the analysis by adding
a response propensity latent dimension that is correlated with the attitudinal latent
variables (O’Muircheartaigh and Moustaki 1999; Moustaki and Knott 2000b; Glas
and Pimental 2008). Such matters are very important in practice but are not relevant
to our more limited objective here, which is to illustrate the theory of this chapter.
There are many examples in the literature of fitting latent trait models to data on tests
of ability. Attempts to use the models in the manner of exploratory factor analysis
are more recent and less common. Those given here provide an introduction to what
is possible.
Example 4.13.1 The data in Table 4.2 relate to Section VI of the Law School
Admission Test and have appeared in the latent trait literature on many occasions (for
example, in Muthén (1978)). The maximum likelihood estimates of the parameter are
given in the second column of Table 4.1 and the fitted frequencies in Table 4.2. The
latter are close to the observed frequencies and the chi-squared goodness-of-fit test
gives χ 2 = 18.14 on 21 degrees of freedom (P = 0.6412). We may thus conclude
that the associations among the five items are adequately explained by their common
dependence on a single latent variable which, in this case, may be identified with the
underlying ability which the test is designed to measure.
In Table 4.1 we have also given three estimates of the standard errors. The results
appear to be fairly typical for data sets of this size. The asymptotic estimates derived
Table 4.1 Parameter estimates and standard errors for the logit/normal model fitted
to the Law School Admission (VI) data.
Maximum
likelihood
estimator

Asymptotic
by (4.25)

standard error
by (4.27)

Standard error
estimated by 1000
bootstrap samples

α̂11
α̂21
α̂31
α̂41
α̂51

0.83
0.72
0.89
0.69
0.66

0.25
0.17
0.20
0.17
0.19

0.26
0.19
0.23
0.19
0.21

0.29
0.22
0.31
0.20
0.22

π̂1
π̂2
π̂3
π̂4
π̂5

0.94
0.73
0.56
0.78
0.89

0.01
0.02
0.02
0.01
0.01

0.01
0.02
0.02
0.02
0.01

0.01
0.02
0.02
0.02
0.01

Parameter

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

111

Table 4.2 Frequencies and scores for the logit/normal model fitted to the Law School
Admission Test (VI) data.
Response
pattern

Frequency

Expected
frequency

Total
score

Component
score

00000

3

2.3

0

0

00001
00010
01000
10000
00100

6
2
1
10
1

5.9
2.6
1.8
9.5
.7

1
1
1
1
1

0.66
0.69
0.72
0.83
0.89

00011
01001
01010
10001
10010
00101
11000
00110
01100
10100

11
8
0
29
14
1
16
3
0
3

8.9
6.4
2.9
34.6
15.6
2.6
11.3
1.2
.9
4.7

2
2
2
2
2
2
2
2
2
2

1.35
1.38
1.41
1.48
1.51
1.55
1.55
1.58
1.61
1.71

01011
10011
11001
00111
11010
01101
01110
10101
10110
11100

16
81
56
4
21
3
2
28
15
11

13.6
76.6
56.1
6.0
25.7
4.4
2.0
25.0
11.5
8.4

3
3
3
3
3
3
3
3
3
3

2.07
2.17
2.21
2.24
2.24
2.27
2.30
2.37
2.40
2.44

11011
01111
10111
11101
11110

173
15
80
61
28

173.3
13.9
83.5
62.5
29.1

4
4
4
4
4

2.89
2.96
3.06
3.10
3.13

11111

298

296.7

5

3.78

Total

1000

1000.0

—

—

P1: TIX/XYZ
JWST070-04

112

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

from the inverse of the information matrix of (4.25) are close to but smaller than the
approximation proposed in (4.27). This suggests that the approximation is adequate
and will, if anything, err on the safe side. The bootstrap estimates suggest that the
asymptotic theory tends to underestimate the true standard error though not, in this
case, by very much. Our experience shows that this is often the case and it arises
from the fact that the actual sampling distribution tends to be positively skewed with
a few unusually large estimates.
The estimates of the π s show that the items are of varying difficulty, and are
relatively easy in the sense that the median individual stands a better than even
chance of getting any item correct. We note from the standard errors that the π s
are estimated with little error. For scaling purposes it is the relative values of the αs
which matter and, having regard to the large standard errors, they are obviously close.
Little would be lost by weighting them equally and simply using the
 total score. This
is shown in Table 4.2 alongside the component score given by
α̂i1 xi . It is clear
that the two scalings are consistent. The component score allows some distinctions
to be made among those with the same total score, but it is doubtful whether much
significance should be attributed to them.
Example 4.13.2 This example is taken from a major study of ‘social life feelings’
reported in Schuessler (1982) and Krebs and Schuessler (1987). The aim was to
establish scales of measurement comparable to those used in ability testing. Further analyses of these data will be found in Bartholomew and Schuessler (1991),
Bartholomew et al. (1993, 1997) and de Menezes and Bartholomew (1996). Here we
take scale 10, called ‘economic self-determination’. We use the ‘intercultural version’
derived from a sample of 1490 Germans. Yes or no responses were obtained to the
following five questions.
1. Anyone can raise his standard of living if he is willing to work at it.
2. Our country has too many poor people who can do little to raise their standard
of living.
3. Individuals are poor because of the lack of effort on their part.
4. Poor people could improve their lot if they tried.
5. Most people have a good deal of freedom in deciding how to live.
The frequency distribution of response patterns is given in Table 4.4. Table 4.3 gives
the maximum likelihood with the E-M parameter estimates and their asymptotic
standard errors as well as the MCMC estimates and standard deviations obtained
from WinBUGS (Spiegelhalter et al. 1996) after 10 000 iterations. WinBUGS uses
Gibbs sampling within M-H. There is little to choose between the two sorts of
estimates. For the E-M solution, we have given both the α̂i0 and the π̂i0 , but with
standard errors for the α̂i0 only. The π̂ vary much more in this example, as one
might expect. The typical respondent is unlikely to agree that anyone can raise their
standard of living (question 1) but is much more likely to believe that individuals are

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

113

Table 4.3 Parameter estimates and standard errors for the logit/normal model fitted
to Social Life Feelings scale 10 with the E-M and MCMC algorithms.
E-M
Item
1
2
3
4
5

MCMC

α̂i0

π̂i0

α̂i1

α̂i0

α̂i1

−2.35 (0.13)
0.80 (0.06)
0.99 (0.09)
−0.67 (0.12)
−1.10 (0.07)

0.09
0.69
0.73
0.34
0.25

1.20 (0.15)
0.71 (0.09)
1.53 (0.17)
2.55 (0.39)
0.92 (0.10)

−2.37 (0.14)
0.79 (0.06)
0.99 (0.09)
−0.74 (0.20)
−1.10 (0.07)

1.21 (0.15)
0.70 (0.09)
1.52 (0.17)
2.86 (0.81)
0.92 (0.11)

poor because of their own lack of effort (question 3). The α̂s also show considerable
variation, with questions 3 and 4 having the greatest discriminating power.
Goodness of fit as measured by χ 2 (= 38.92) or G2 (= 39.09), both on 21
.
degrees of freedom, indicates a rather poor fit (P = 0.01). This should be interpreted
with some caution for two reasons. First, Bartholomew and Tzamourani (1999) have
shown that both χ 2 and G2 tend to overestimate significance in this context. Secondly,
none of the marginal residuals is large. Almost all of the pairwise residuals are less
than 1, and the largest is only 2.7. The three-way margins likewise yield no residual
for the response (1, 1, 1) greater than 2.20, so the two- and three-way associations are
well predicted by the model. This seems sufficient grounds for believing that a single
latent dimension provides an adequate explanation of the dependencies among the
item responses. However, the limited-information goodness-of-fit test based on the
one- and two-way margins as measured by expression (4.34) is 15.71 on 5 degrees
of freedom, indicating a poor fit (P = 0.0077).
The remaining columns of Table 4.4 are concerned with posterior analysis, which
is relevant if we wish to construct a scale of economic self-determination. In this
example the rankings given by the total score and the component score are not so
close. For example, the response pattern 00010 has a component score which is
substantially higher than the other cases where there was only one positive response.
This is because item 4 has a much higher weight than the others, and so a single
positive response on that item carries more weight than, say, two positive responses
on items 1 and 2. Other inversions in the rankings of the two measures can be
explained similarly.
The values in the column headed ‘posterior mean’ (E(y | X )) give the predicted
positions on a standard normal scale for each of the response patterns. In Section 4.10
we quoted results about the approximate linear relationship between X and E(y | X ),
which can be confirmed for this example by plotting one against the other. Apart from
the extremes, the linearity is almost exact. The posterior standard deviation, given
in the final column, is remarkably constant over all response patterns. The average
variance with respect to the distribution of X will be close to the test–retest correlation
of (4.35), but this adds nothing to what we can learn from inspection of the individual
values. With a typical value of σ (y | X ) around 0.60 there would be considerable

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

114

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 4.4 Frequencies and scores for the logit/normal model fitted to Social Life
Feelings scale 10.
Response
Total Component Posterior
Posterior
pattern
Observed Expected score
score
mean standard deviation
00000
01000
00001
10000
00100
01001
11000
10001
01100
00101
00010
10100
11001
01101
01010
11100
00011
10101
10010
00110
01011
11101
11010
10011
01110
00111
10110
11011
01111
11110
10111
11111

156
174
26
8
127
35
8
2
208
26
14
4
2
65
36
19
9
4
1
66
13
10
5
3
195
16
18
3
129
31
9
68

162.0
174.0
22.2
5.2
122.3
31.9
8.1
1.1
208.2
30.2
16.9
8.3
2.3
65.3
37.6
19.3
5.8
3.0
1.8
56.5
16.2
8.8
5.3
0.9
182.7
31.8
11.2
3.4
130.9
49.9
9.6
56.7

0
1
1
1
1
2
2
2
2
2
1
2
3
3
2
3
2
3
2
2
3
4
3
3
3
3
3
4
4
4
4
5

0.00
0.71
0.92
1.20
1.53
1.64
1.91
2.12
2.24
2.45
2.55
2.73
2.83
3.17
3.26
3.44
3.47
3.65
3.74
4.08
4.19
4.36
4.46
4.67
4.79
5.00
5.27
5.38
5.72
5.99
6.20
6.91

−1.19
−0.85
−0.76
−0.64
−0.50
−0.46
−0.36
−0.28
−0.23
−0.16
−0.12
−0.06
−0.02
0.09
0.12
0.18
0.19
0.25
0.29
0.40
0.44
0.50
0.53
0.61
0.65
0.73
0.83
0.88
1.01
1.12
1.21
1.54

0.72
0.67
0.66
0.64
0.63
0.62
0.61
0.61
0.60
0.60
0.59
0.59
0.59
0.59
0.58
0.58
0.58
0.58
0.58
0.59
0.59
0.59
0.59
0.60
0.60
0.61
0.62
0.62
0.64
0.65
0.66
0.70

uncertainty about the location of any individual on the latent scale. Put another way,
the observation of the score pattern x reduces the variance in our knowledge of the
location of the individual from 1 to 0.62 = 0.36.
Example 4.13.3 The third example analyses the UK data on Eysenck’s Personality
Questionnaire–Revised (EPQ-R); see Eysenck et al. (1985). The sample consists of

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

115

610 males. The EPQ-R is a widely used personality inventory that consists of three
scales, each one measuring a broad personality trait: Extraversion (E, 23 items),
Neuroticism or Emotionality (N, 24 items), and Psychoticism or Tough-Mindness
(P, 32 items). We analyse here the revised P-scale on Psychoticism. The 32 items can
be found in Eysenck et al. (1985). The revised P-scale was a result of criticisms of
the psychometric properties of the original scale related to low reliability, low range
of scoring and grossly skewed score distributions. Furthermore, the Psychoticism
scale had been constructed to be unidimensional. The AIC and BIC favour the twofactor model (see Table 4.5). We also looked at the standardised χ 2 local dependence
(LD) index (Chen and Thissen 1997) both for the one- and two-factor solutions.

Table 4.5 Standardised χ 2 local dependence index for item
pairs greater than 3, and AIC and BIC for the one- and twofactor logit/normal model fitted to the Psychoticism scale.
Item pairs
(10, 4)
(12, 9)
(13, 4)
(13, 10)
(13, 12)
(15, 2)
(18, 15)
(21, 19)
(21, 20)
(22, 19)
(22, 21)
(23, 15)
(26, 7)
(26, 9)
(28, 14)
(29, 4)
(29, 10)
(29, 13)
(29, 17)
(30, 2)
(30, 20)
(30, 21)
(32, 12)
(32, 18)
AIC
BIC

Index one-factor
16.4
8.5
16.1
15.3
3.0
4.0
3.7
6.7
7.4
4.8
73.0
3.8
7.4
14.7
30.3
6.2
6.9
32.1
16.3
10.3
15.2
5.2
3.0
21.9
18 243.77
18 526.23

Index two-factor
4.9

3.8
4.2

4.2
7.3
11.5
29.3

6.1
10.7
7.5
6.9

21.5
17 739.10
18 354.79

P1: TIX/XYZ
JWST070-04

116

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 4.6 Unstandardised and standardised parameter estimates with standard errors
in brackets for the one-factor logit/normal model, Psychoticism scale.
Item
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

α̂i0 (s.e.)

π̂i0

α̂i1 (s.e.)

stα̂i1

−1.69 (0.14)
1.12 (0.10)
−3.93 (0.36)
−1.65 (0.13)
−2.36 (0.19)
0.53 (0.09)
−1.70 (0.13)
−0.05 (0.09)
−2.47 (0.16)
−2.13 (0.15)
−0.57 (0.09)
−2.36 (0.20)
−1.46 (0.12)
−1.84 (0.16)
−0.69 (0.09)
−1.19 (0.11)
−0.90 (0.09)
−3.67 (0.34)
−1.09 (0.10)
−0.21 (0.09)
−1.41 (0.12)
−1.93 (0.15)
−2.11 (0.14)
−1.63 (0.13)
−1.48 (0.11)
−2.65 (0.18)
−2.23 (0.18)
−1.40 (0.13)
−0.72 (0.09)
0.02 (0.09)
−1.95 (0.15)
−2.76 (0.21)

0.16
0.75
0.02
0.16
0.09
0.63
0.15
0.49
0.08
0.11
0.36
0.09
0.19
0.14
0.33
0.23
0.29
0.02
0.25
0.45
0.20
0.13
0.11
0.16
0.19
0.07
0.10
0.20
0.33
0.50
0.12
0.06

1.13 (0.16)
0.84 (0.13)
1.24 (0.30)
0.80 (0.15)
1.36 (0.21)
0.93 (0.13)
0.76 (0.14)
0.63 (0.11)
0.53 (0.18)
0.87 (0.17)
0.87 (0.12)
1.54 (0.22)
0.74 (0.14)
1.25 (0.18)
0.53 (0.11)
0.78 (0.13)
0.50 (0.11)
1.45 (0.29)
0.58 (0.12)
0.99 (0.13)
0.85 (0.14)
1.11 (0.17)
0.67 (0.16)
1.02 (0.16)
0.52 (0.13)
0.65 (0.19)
1.32 (0.20)
1.19 (0.16)
0.65 (0.12)
0.92 (0.13)
0.94 (0.16)
0.99 (0.21)

0.75
0.64
0.78
0.62
0.81
0.68
0.61
0.53
0.47
0.66
0.66
0.84
0.59
0.78
0.47
0.62
0.45
0.82
0.50
0.70
0.65
0.74
0.56
0.71
0.46
0.54
0.80
0.77
0.54
0.68
0.68
0.70

The standardised χ 2 statistic compares the observed and expected frequencies in the
two-way marginal tables for pairs of items. The standardised
√ version of LD produced
by the IRTPRO program (Cai et al. 2011) is (χ 2 − d f )/ 2d f , where d f = 1 in the
binary case. Table 4.5 gives the standardised χ 2 LD index for those item pairs that
exhibit positive dependencies that have not been well explained by the one- and twofactor models. Positive values indicate that the observed covariation between pairs of

P1: TIX/XYZ
JWST070-04

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:7

Printer Name: Yet to Come

BINARY DATA: LATENT TRAIT MODELS

117

items exceeds that predicted by the fitted model. Therefore an additional factor might
be needed to accommodate those dependencies. Values greater than 3 are reported
here. There are few bad fits in both the one- and two-factor models. Certainly, the
two-factor model is shown to be an improvement. However, there only a few large
values and no obvious clustering of offensive items can be identified. Although the
goodness of fit measures suggest that a two-factor solution is a better fit, it has been
difficult to produce an interpretable two-factor solution even after an orthogonal or
oblique rotation.
We give in Table 4.6 the unstandardised and standardised loadings for the onefactor logit/normal model. The standardised loadings are obtained from equation
(7.52). All factor loadings are statistically significant and show moderate to strong
positive correlations between the 32 items and the underlying construct. More analysis
might be necessary to improve the 32-item scale. However, Eysenck et al. (1985)
point out in their discussion that the Psychoticism scale accounts for several different
aspects such as hostility, cruelty, lack of empathy, and non-conformism, whereas the
rest of the scales constructed for Extraversion, Neuroticism, and Social Desirability
are much more homogeneous in what they try to measure.

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

5

Polytomous data: latent
trait models
5.1

Introduction

In conceptual terms it seems a small step from problems with binary manifest data to
polytomous data, but this is deceptive. In the binary case we identified two approaches
to the problem. The first, via the sufficiency principle, led to the logit model in
which the response function turned out to be a logistic curve. The second started by
supposing that the binary observations were formed by dichotomising an underlying
continuous variable and then assuming that these underlying variables were generated
by a normal linear factor model. The two models arrived at from these very different
starting points proved to be equivalent. Which one we chose to adopt was therefore
largely a non-empirical matter. In the polytomous case this equivalence breaks down.
The natural extension of the binary underlying variable model is not equivalent to the
response function model derived from the sufficiency argument. There is, however,
another model based on the idea of underlying variables to which it is equivalent. This
alerts us to the need to consider the choice of model more carefully and to determine
the circumstances under which different choices might be appropriate.
There are also new practical questions which arise with polytomous data. In most
models the number of parameters increases in line with the number of categories, and
this can lead to problems of estimation and inference. Even with binary data we found
that standard errors could be quite large and that there might be problems with the
convergence of the E-M algorithm. These features are magnified when we move to
polytomous data, as is the problem of sparseness when testing goodness of fit. More
research is necessary on some of these matters and, for that reason, there will be a
somewhat greater emphasis in this chapter on the theoretical aspects of the models.

Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

119

P1: TIX/XYZ
JWST070-05

120

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

We shall give a number of examples to illustrate the different model strategies for
polytomous variables.

5.2

A response function model based on the
sufficiency principle

The general approach to choosing factor models set out in Chapter 2 yields a family
of models for polytomous data which generalise those for binary data in Chapter 4.
We first need to extend the notation used there. Let ci denote the number of categories
of variable i which are labelled 0, 1, . . . , ci − 1 (i = 1, 2, . . . , p) and indexed by s.
The indicator variable xi is now replaced by a vector-valued indicator function with
its sth element defined by

1 if the response falls in category s,
xi (s) =
0 otherwise.

(5.1)


The ci -vector with these elements is denoted by xi and, obviously, s xi (s) = 1.
The full response
pattern for an individual is denoted by x = (x1 , x2 , . . . , xp ), of

dimension i ci . Note that if we specialise to the binary case by putting ci = 2 for
all i, this notation does not coincide exactly with that in Chapter 4. The two-element
vector xi was there represented by the single element which here is denoted by xi (1).
Since xi (0) + xi (1) = 1, it is sufficient in the binary case to record only xi (1) and to
delete alternate elements in x; in the general case we could, in a similar way, delete
xi (0) for each i. However, the symmetry which results from retaining the redundancy
outweighs any advantage of the more parsimonious description.
The single response function πi (y) is now replaced by a set of functions defined by
P{xi (s) = 1 | y} = πis (y)

(s = 0, 1, . . . , ci − 1; i = 1, 2, . . . , p).

(5.2)


The requirement that s πis (y) = 1 again means that there is some redundancy in
the notation. We may now suppose the conditional probability function of xi given
y to be multinomial so that
gi (xi | y) =

c
i −1

{πis (y)}xi (s)

(i = 1, 2, . . . , p).

(5.3)

s=0

The posterior density function of y is then given by
 p c −1

i

xi (s)
h(y | x) = h(y)
{πis (y)}
/ f (x)
i=1 s=0

= h(y) exp

 p c −1
i

i=1 s=0


xi (s) ln πis (y) / f (x).

(5.4)

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

121

In Chapter 2 we introduced the GLLVM for the model in which the conditional
distribution of a manifest variable, xi , had a distribution in the exponential family
with canonical parameter θi . Both xi and θi were there supposed to be scalars. In
the present case, both are vectors but the arguments presented in Sections 2.2 and
2.3 generalise almost immediately to provide a vector-valued GLLVM. In fact, if
xi and θi in (2.1) and the αs in (2.2) are vectors, the sufficiency property and the
linearity of the components still hold.
 In applying this result to polytomous responses
we must allow for the fact that s πis (y) = 1. We did this in the binary case by
implicitly reparameterising the problem, replacing πi (y) and 1 − πi (y) (here denoted
by πi0 (y) and πi1 (y)) by logit πi (y) and {1 − πi (y)} respectively. The new parameter
θi = logit πi (y) was then equated to a linear combination of the latent variables. In
the general case the equivalent operation involves writing (1.9) in the form
gi (xi | y) =

c
i −1

{πi0 (y)}xi (s) {πis (y)/πi0 (y)}xi (s)

s=0

= πi0 (y) exp
= πi0 (y) exp

c
i −1

xi (s) ln {πis (y)/πi0 (y)}
s=0
θ i xi ,

(5.5)

where
θ i



πi1 (y)
πi2 (y)
πi(ci −1) (y)
.
= 0, ln
, ln
, . . . , ln
πi0 (y)
πi0 (y)
πi0 (y)

(5.6)

Which category is labelled 0 is, of course, quite arbitrary and any one may be chosen.
The appropriate model may now be written
θ i = α i0 + α i1 y1 + α i2 y2 + · · · + α iq yq ,

(5.7)

where α i j = (αi j (0) = 0, αi j (1), . . . , αi j (ci − 1)) for j = 0, 1, . . . , q. It is easily
shown that the components are given by
Xj =

p ci −1



αi j (s)xi (s)

(5.8)

i=1 s=0

or
X = Ax,
say. For complete consistency with the binary case we should have chosen the last
probability, πci −1 (y), as the divisor in (5.6) but it is notationally simpler to take the
first. We shall refer to category 0 as the reference category and continue to refer to
the elements of θi as logits and the model as the logit model.

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

122

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

The requirement that



s

πis (y) = 1, together with (5.5)and (5.6), implies that
exp αi0 (s) +

πis (y) = 
ci −1
r =0

q

αi j (s)y j
q
exp αi0 (r ) + j=1 αi j (r )y j
j=1

(5.9)

for i = 1, 2, . . . , p and s = 0, 1, . . . , ci − 1.
Figure 5.1 illustrates the forms of the response functions for the set of categories
on a given manifest variable for a single latent variable measured on the y scale. As
we move from left to right along the latent dimension the probability of a positive
response is decreasing in some categories and increasing in others. The model thus
determines an ordering of the categories such that the higher a subject is on the latent
scale the greater is the tendency for the response to fall in the higher categories. This
behaviour is plausible in many contexts. It would be appropriate, for example, if
the categorisation were arrived at by grouping individuals according to an imperfect
perception of their place on the scale. In educational testing it would be plausible if
the possible answers were of varying degrees of ‘rightness’. Thissen and Steinberg
(1984, 1986) argue that the model would not apply if one response is correct and
the others equally wrong. However, this can be accommodated as a limiting case.
Nevertheless, the point made in connection with binary models bears repetition:
one should not assume that the possession of attractive mathematical properties will

Probability of category response

1.0

αi0 (0)
αi0 (1)
αi0 (2)
αi0 (3)

0.8

=
=
=
=

0, αi1 (0)
0, αi1 (1)
0, αi1 (2)
0, αi1 (3)

=
=
=
=

0
1
2
4

0.6

0.4

0.2

0.0
-3

-2

-1

0

Latent variable y

1

Figure 5.1 Probabilities of response categories.

2

3

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

123

guarantee the applicability of a model in any particular circumstances. Thissen and
Steinberg (1984) discuss alternative models.
In (5.9), we have, as in the binary case of Chapter 4, αi j (0) = 0. The logit model
for polytomous data is thus

ln

πis (y)
πi0 (y)


= αi0 (s) +

q


αi j (s)y j

(s = 0, 1, 2, . . . , ci − 1; i = 1, 2, . . . , p).

j=1

(5.10)

We shall assume that the ys are mutually independent with standard normal distributions. Under this assumption we have a logit/normal model. It no longer makes
sense to talk of a probit/normal model since there is no simple comparable way of
introducing a probit function on the left-hand side. The logit model was first proposed
by Bock (1972), though with a rather different rationale.
An interesting property of the polytomous logit model arises as follows. Suppose
we consider the conditional probability that a positive response is in category s given
that it is in r or s. This is clearly πis (y)/{πis (y) + πir (y)} = πi,s | s,r (y), say. A simple
manipulation then reveals that
logit πi,s | s,r (y) = {αi0 (s) − αi0 (r )} +

q

{αi j (s) − αi j (r )}y j .

(5.11)

j=1

In other words, if we reduce the data to binary form by extracting only those responses
where the positive response is in r or s, then we have the standard binary logit model.
This property provides an alternative route to the polytomous logit model. If we wish
(5.11) to hold for any pair of categories, then the model (5.10) must hold for them
all. This was the approach adopted by Thissen and Steinberg (1984, 1986), who used
the term divide-by-total to refer to this model.
An alternative parameterisation which is useful later and which facilitates the
interpretation of the αi0 (s) is obtained as follows. If we put y j = 0 for all j, we obtain
the response probability for the ‘median’ individual. Let this be denoted by
exp αi0 (s)
πis = ci −1
.
r =0 exp αi0 (r )

(5.12)

Remembering that αi0 (0) = 0,
πis =

exp αi0 (s)
ci −1
1 + r =1 exp αi0 (r )

(s = 1, 2, . . . , ci − 1).

(5.13)

Substituting into (5.9) then gives
πis (y) = ci −1
r =0

q

αi j (s)y j
q
πir exp j=1 αi j (r )y j

πis exp

j=1

(5.14)

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

124

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

or, from (5.10) ,
ln{πis (y)/πi0 (y)} = ln{πis /πi0 } +

q


αi j (s)y j

(s = 1, 2, . . . , ci − 1).

(5.15)

j=1

This is an alternative way of defining the polytomous logit model. When, later, we
refer loosely to the ‘αs’ we shall mean the αs of (5.14) or (5.15) which are coefficients
of ys. The other parameters will be referred to, collectively, as the π s.

5.3

Parameter interpretation

The interpretation of the π s is clear from their definition as ‘median’ response
probabilities. The αs may be interpreted in ways similar to those of the binary case,
but now we need to consider them, separately, as functions of s and i.
First, the discriminating power of the latent variable yj is indicated by the spread
of the αi j (s) considered as a function of s. A large spread produces larger differences
between the corresponding response probabilities and so a better chance of discriminating between individuals a given distance apart on the y scale on the evidence of xi .
Secondly, the αs are also weights in the components of (5.8). Here we are looking
at the relative influence which each manifest variable has in determining the value of
the component. A variable will thus be an important determinant if all of the αi j (s) for
given i are large. That is, it is the average level of the αs rather than their dispersion
which counts.
The distinction between these two aspects disappears when we specialise to the
binary case. The fact that αi (0) = 0 means that the dispersion and the mean of αi (0)
and αi (1) are essentially the same thing.
The third way of looking at the αs is as category scores. If we think of αi j (s)
as a score associated with category s of variable i then Xj is the total score for an
individual. This interpretation, already introduced in Section 4.2, will be particularly
relevant when we come to consider the relationship with correspondence analysis.
All of these interpretations can be given a new aspect in relation to an equivalent
model based on underlying variables described in Section 5.9.

5.4

Rotation

In Chapter 4 we favoured the choice of the standard normal distribution for y on the
grounds that it left the likelihood unchanged under orthogonal transformation of the
αs. The same advantage is gained in the polytomous case. To show this we redefine
A to be the matrix
A = (α 1 , α 2 , . . . , α q ),
where α j = (α1 j (0), α1 j (1), . . . , α1 j (c1 − 1), α2 j (0), . . . , α2 j (c2 − 1), . . . , α pj (c p −
1)). If we now make an orthogonal transformation to
A∗ = AM,

(5.16)

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

125



where M is q × q with MM = I, then j αi j (s)y j = Ai (s)y, where Ai (s) is the row
of A beginning with the element αi1 (s). As before,
Ai (s)y = Ai∗ (s)M−1 y
and the previous argument can be repeated making the transformation y∗ = M−1 y to
show that the likelihood is unchanged.

5.5

Maximum likelihood estimation of the polytomous
logit model

The E-M algorithm given in Section 4.5 for binary data is easily generalised to the
polytomous case as follows. As before, we work with only one factor to simplify the
notation. Then (4.19) becomes
L=

n


ln g(xh | yh ) =

h=1

 p
n


h=1


ln g(xi h | yh ) ,

(5.17)

i=1

where at each E-step we have to find E[L | x]. Substituting (5.5) in (5.17), we have

L=

n


ln g(xh | yh ) =

h=1

=

 n
p


i=1

 p
n


h=1

ln πi0 (yh ) +

h=1

ln πi0 (yh ) +

i=1

c
i −1
s=0

c
i −1


xi h (s)(αi0 (s) + αi1 (s)yh )

s=0

αi0 (s)

n


xi h (s) + αi1 (s)

h=1

n



.

xi h (s)yh

h=1

The E-step is therefore

E[L | x] =

 n
p


i=1

+

E(ln πi0 (yh ) | x1 , . . . , xn )

h=1

c
i −1

αi0 (s)

s=0

=

 n
p


i=1

+

n


xi h (s) + αi1 (s)

h=1

n



xi h (s)E(yh | x1 , . . . , xn )

h=1

E(ln πi0 (yh ) | xh )

h=1

c
i −1
s=0

αi0 (s)

n

h=1

xi h (s) + αi1 (s)

n

h=1


xi h (s)E(yh | xh )

.

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

126

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

For the M-step one can maximise separately for each item i and each category s,
n


E(ln πi0 (y) | xh ) + αi0 (s)

h=1

n


xi h (s) + αi1 (s)

h=1

n


xi h (s)E(y | xh ).

(5.18)

h=1

So, referring to Section 4.5, the M-step is the same as maximum likelihood estimation
in a continuous analogue of logistic regression.
As in Section 4.5, we approximate the averages in (5.18) with Gauss–Hermite
quadrature, giving
n 
k


xi h (s) ln πi0 (y t )h(y t | xh ) + αi0 (s)

h=1 t=1

+ αi1 (s)
=

xi h (s)

h=1
k

t=1

k


n


yt

n


xi h (s)h(y t | xh )

h=1

n t ln πi0 (y t ) + αi0 (s)

t=1

n


xi h (s) + αi1 (s)

h=1

k


y t rit (s),

(5.19)

t=1

where
rit (s) =

n


xi h (s)h(y t | xh )

(s = 1, 2, . . . , ci − 1; i = 1, 2, . . . , p)

(5.20)

h=1

and
nt =

n


h(y t | xh ).

(5.21)

h=1

This leads to the M-step. For a given value of s, these equations are essentially the
same as those of (4.22) and (4.23) where, again, rit (s) and nt can be interpreted
as pseudo-observed and expected frequencies. The simple form of the equations
in the estimation process is because the logit link has been used on the left-hand
side of (5.10).
At this juncture in the binary case we noted the similarity with dosage response
analysis and hence that the conditional maximum likelihood estimators could be
obtained by an iterative weighted least squares analysis. The same approach can be
used here.

5.6

An approximation to the likelihood

We shall now derive an approximation to the likelihood function for the logit
model which can easily be maximised with respect to the parameters, thus yielding

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

127

approximate estimators. The reason for doing this is not to provide an easier way of
finding maximum likelihood estimators but to bring out a link with correspondence
analysis. It turns out that the approximations are rather poor if judged by the absolute
differences. However, it is the profile of the set of weights which counts (i.e. their
relative values) and in this respect the approximations appear to be good. This fact
is illustrated by the example to be given in Table 5.2. The exact and approximate
estimates are not precisely proportional but are near enough to lead to essentially the
same interpretation. The result is particularly interesting because of the equivalence
of the approximate estimates and the category scores of correspondence analysis to
which we shall draw attention at the end of Section 5.7.

5.6.1

One factor

We treat this case first in order to establish the method and then, more briefly, extend
the results to cover any q. The joint probability function of x for the logit model is
1
f (x) = √
2π

+∞
−∞



p c −1

i

1
exp − y 2 +
xi (r ) ln πir (y)
2
i=1 r =0


dy.

(5.22)

Using the version of πir (y) given in (5.14), (5.22) becomes

f (x) = exp

p ci −1




xi (r ) ln πis

i=1 r =0



+∞

1
× √
2π

−∞

p


c
i −1

−

ln



p c −1

i

1
exp − y 2 + y
xi (r )αi (r )
2
i=1 r =0
 

πir exp αi (r )y

dy .

(5.23)

r =0

i=1

We begin with the αi (r ) which appear only in the second factor of (5.23). The
method is first to approximate this factor as described below and then to obtain estimators by maximising the relevant part of the likelihood. We replace the last term in the
exponent by its Taylor expansion taken as far as terms of the second degree in α. Thus
p


ln

c
i −1

i=1

1
+ y2
2

p ci −1

πir exp αi (r )y

. 
=y
πir αi (r )

r =0

⎧
i −1
 ⎨c

i=1 r =0

p

i=1

⎩

r =0

πir αi2 (r ) −

c
i −1
r =0

2

πir αi (r )

⎫
⎬
⎭

.

(5.24)

P1: TIX/XYZ
JWST070-05

128

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Recalling that the probability is unaffected by translations of the αi (r ), there is
no loss of generality if we impose the constraints
c
i −1

πir αi (r ) = 0

(i = 1, 2, . . . , p).

(5.25)

r =0

The right-hand side of (5.24) then simplifies to
p c −1

i
1 2
1
y
πir αi2 (r ) = y 2 α  Pα,
2 i=1 r =0
2

(5.26)

where α  = (α1 (0), α1 (1), . . . , α1 (c1 − 1), α2 (0), . . . , α p (c p − 1)) and P is the diagonal matrix with elements πir listed along the diagonal in dictionary order. Denoting
the second factor of (5.23) by I, we now have


1 2
2
exp − y + Y y − ay dy
2
−∞


1 Y2
−1/2
= (2a + 1)
,
exp
2 2a + 1

1
I = √
(2π )

+∞

(5.27)

 p  −1
where a = 12 α  Pα and Y = i=1 rci=0
xi (r )αi (r ) = x α.
That part of the likelihood in which the αs occur may now be written


l = (2a + 1)

−1/2


n
1  Yh2
exp
,
2 h=1 2a + 1

(5.28)

where Yh is the value of Y for the hth sample member. Now
n

h=1

Yh2 =

p
p ci −1 ci −1
n 




xi h (r )x j h (s)αi (r )α j (s)

h=1 i=1 j=1 r =0 s=0
 

= α X Xα.

(5.29)

The hth row of the matrix X consists of the set of indicator variables for the hth sample
member as defined in Section 5.2. Since each member falls into exactly one category
on each variable, each row of X sums to p. The column sums of X give the numbers of
sample members
into
n falling
 each
ncategory on each variable. The diagonal elements
2
of X X are
h=1 x i h (s) =
h=1 x i h (s) and are thus the same as the column
sums. The off-diagonal elements give the two-way marginal frequencies; thus
n

h=1

xi h (r )x j h (s)

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

129

is the number of individuals in category r of variable i and category s of variable
j. X X thus contains the first- and second-order marginal frequencies, and since our
approximate likelihood depends on the data only through this matrix we may deduce
that the estimates to be derived will also be functions of first- and second-order
marginal frequencies only. Furthermore, this would also be true if the exponent in
the probability function were approximated by any other quadratic function of α –
for example, a Taylor expansion about some point other than zero.
The relevant part of the log-likelihood may now be written
1
n
L = − ln(α  Pα + 1) + α  X Xα/(α  Pα + 1)
2
2

(5.30)

which has to be maximised subject to the constraints of (5.25). These may be written
α  P∗ = 0, wherethe ith column of P∗ has elements πi0 , πi1 , . . . , πici −1 in rows

i 0 <i ci 0 + 1 to
i 0 ≤i ci 0 and zeros elsewhere (i = 1, 2, . . . , p).
We maximise (5.30) in three stages. First choose the π s, then make α  X Xα
as large as possible for given π s and fixed α  Pα, and then choose the best value
for α  Pα.
To choose the best π s we use only the first factor from (5.23), ignoring terms of
second order in α. That factor is a multinomial probability, so the maximising π s are
π̂is =

n


xi h (s)/n

(i = 1, 2, . . . , p; s = 0, 1, . . . , ci − 1)

h=1

or
π̂ = X 1/n,

(5.31)

where 1 is an n-vector of 1s and π is the vector of the πis in dictionary order. The π s
are thus estimated by corresponding marginal proportions. We assume for the rest of
the maximisation that π is always replaced by π̂ , but do not use the ‘hat’ notation.
The second stage of the maximisation is to find the best value for α  X Xα for
fixed α  Pα. It is easier to use β, defined by
β = P1/2 α.

(5.32)

α  X Xα = β  P−1/2 X XP−1/2 β.

(5.33)

Then

We maximise the right-hand side of (5.33) over all choices of β such that (P∗ ) α =
(P∗ ) P−1/2 β = 0. This is a simple eigenvalue problem. It is happily the case that the
constraints span a space that is spanned also by p eigenvectors of P−1/2 X XP−1/2 . To
maximise the quadratic form it is enough to find the eigenvector not in this spanning
set that has the largest eigenvector. The constraints themselves are not eigenvectors,

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

130

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

but one can rotate them to obtain eigenvectors. For H a p × p orthogonal matrix with
all elements in its first column equal, the columns of P−1/2 P∗ H are eigenvectors of
P−1/2 X XP−1/2 . This is because, for 1n an n-vectors of 1s,
P−1/2 X XP−1/2 P−1/2 P∗ H = P−1/2 X XP−1 P∗ H
= P−1/2 X X(1 ci ⊗ I p )H
= P−1/2 X 1n 1p H
= nP−1/2 π 1p H,

(5.34)

where (5.34) follows from (5.31). Since 1 p is orthogonal to all but the first column
of H, (5.34)can be written
√
P−1/2 X XP−1/2 P−1/2 P∗ H = nP−1/2 π ( p, 0, . . . , 0)
√
= P−1/2 (n pπ , 0, . . . , 0)
⎡
np 0 . . .
⎢
0 0 ...
= P−1/2 P∗ H⎢
⎣...
0 0 ...

⎤
0
0⎥
⎥.
⎦
0

(5.35)

So we have identified p eigenvectors of P−1/2 X XP−1/2 , the first being π , which by
the Perron–Frobenius theorem has the maximum eigenvalue of size np. The other
p − 1 eigenvectors have eigenvalues 0. To maximise β  P−1/2 X XP−1/2 β subject to
(P∗ ) P−1/2 β = 0 is the same as maximising subject to H (P∗ ) P−1/2 β = 0, and this is
achieved by selecting β as an eigenvector β̂ of P−1/2 X XP−1/2 attached to its second

largest eigenvalue, say λ2 . Then the maximum value of β  P−1/2 X XP−1/2 β is λ2 β̂ β̂
which, using (5.32), can be written as, say, λ2 α̂  Pα̂.
Substituting this maximum value back in the likelihood, it remains to carry out
the third stage of the maximisation over the choice of size for α̂  Pα̂ (we can choose
an eigenvector α̂ of any length). The relevant part of the log-likelihood may now be
written, putting α̂  Pα̂ = δ,
1
n
L = − ln(δ + 1) + λ2 δ/(δ + 1).
2
2

(5.36)

Assuming that λ2 > 0, a weak assumption, the likelihood in (5.36) is easily shown
to be maximised for δ = max(λ2 /n − 1, 0).

5.6.2

More than one factor

The treatment of the general case follows the same lines, so we shall treat it more
briefly by concentrating on the points at which it differs from the case q = 1. The

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

131

second factor in (5.23) becomes
+∞

(2π )−q/2

−∞

⎡

q
q
p
c
i −1

1 2 
yj −
yjYj −
ln
πir
2 j=1
−∞
j=1
i=1
r =0
⎧
⎫⎤
q
⎨
⎬
× exp
αi j (r )y j ⎦ dy.
(5.37)
⎩
⎭

...

∞

exp ⎣−

j=1

The approximation now consists of replacing the last term in the exponent by a
quadratic form derived from the Taylor expansion, giving
p


ln

c
i −1

i=1

⎧
q
⎨

πir exp

⎩

r =0

αi j (r )y j

j=1

⎫
⎬
⎭

.
= y By + b y,

(5.38)

where b is a q-vector with

bj = −

p ci −1



πir αi j (r ) ( j = 1, 2, . . . , q)

(5.39)

i=1 r =0

and B is a q × q matrix with elements bjk given by
2b jk =

c −1
p
i


i=1

πir αi j (r )αik (r ) −

r =0

c
i −1

πir αi j (r )

c
i −1

r =0


πir αik (r ) .

(5.40)

r =0

If, as before, we centre the αs for each variable at zero, then
c
i −1

πir αi j (r ) = 0

(i = 1, 2, . . . , p; j = 1, 2, . . . , q)

r =0

and
b = 0 and

2B = A PA,

where A = (α 1 , α 2 , . . . , α q ), α q being the vector of αs for the jth latent variable.
The exponent in (5.37) can thus be written as
1 
{y y − 2Y y + y A PAy},
2

P1: TIX/XYZ
JWST070-05

132

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

where Y = (Y1 , Y2 , . . . , Yq ). Completing the square, the integral reduces to


| I + A PA |

−1/2




1  
−1
exp
Y (A PA + I) Y .
2

(5.41)

Noting that Y = A x and that
Y (A PA + I)Y = trace{(A PA + I)−1 YY },
the relevant part of the log-likelihood becomes
n
1
L = − ln | A PA + I | + trace{(A PA + I)−1 A X XA}.
2
2

(5.42)

Our approximate estimators are obtained by maximising this expression with respect
to A. However, a new feature is that there is no unique maximum, for if we make an
orthogonal transformation to A∗ = AN (N N = I), the value of L is unchanged. The
proof of this result depends on the two following well-known theorems:
(a) The determinant of a product of two square matrices equals the product of the
determinants.
(b) trace N ZN = trace Z, where N is orthogonal.
Taking the two parts of (5.42) in turn,
ln | (A∗ ) PA∗ + I | = ln | N A PAN + I | = ln | N (A PA + I)N |
= ln | N N | + ln | A PA + I | = ln | A PA + I | (5.43)
and
trace[{(A∗ ) PA∗ + I}−1 (A∗ ) X XA∗ ] = trace{(N A PAN + I)−1 N A X XAN}
= trace[{N (A PA + I)−1 N}−1 N A X XAN]
= trace[N (A PA + I)−1 NN A X XAN]
= trace{(A PA + I)−1 A X XA}.
(5.44)
To obtain a unique solution we must therefore impose sufficient constraints; other
solutions can then be obtained by orthogonal rotation.
Since A PA is a symmetric positive definite matrix, it can be written in the form
MM , where  is a diagonal matrix of its eigenvalues and M is an orthogonal
matrix composed of the eigenvectors. It follows that
 = (AM) P(AM)

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

133

and therefore that the matrix A PA can be diagonalised by the transformation
A∗ = AM. We may thus obtain the unique maximiser of L, except for sign changes
over whole columns of A and reorderings of those columns, when A PA is diagonal,
and then derive all others by orthogonal transformation. Since the determinant of a
diagonal matrix is the product of its diagonal elements, (5.42) may now be written

L=

q




j=1

α j X Xα j
n
− ln{α j Pα j + 1} +
2
1 + α j Pα j


.

(5.45)

Note that the jth diagonal element of A PA is only equal to α j Pα j when the former
is diagonal, as we are assuming. If (5.45) is now compared with (5.30) it will be seen
that each term in the sum is of the same form as in the previous case. L in (5.45) is
thus maximised if each term individually is maximised, and this is a problem we have
already solved. It is thus clear that the vectors α j will all be eigenvectors of P−1 X X.
Let α 1 , α 2 , . . . , α q be any q such eigenvectors; then according to (5.34),
L=−


q 
δj
δj
n
ln −
+1 .
2 j=1
n
n

(5.46)

It follows that δ1 , δ2 , . . . , δ j should be the largest j eigenvalues and that we may
extract factors as long as θ j ≥ n. This restriction imposes a limit on the number of
factors that can be extracted.
An interesting property of this solution is that, to the same degree of approximation, the components are uncorrelated. Thus the covariance matrix of Y is
cov(Y) = A E(xx − ExEx )A.
Conditioning on y, we have Ex = Eπ (y) and Exx = Eπ (y)π  (y), where
π  (y) = (πi0 (y), . . . , πici −1 (y), . . . , π pc p −1 (y)).
Expanding the (i, r )th element of this vector as far as terms of the first degree in the
αs,
⎡
πir (y) = πir ⎣1 −

= πir

⎧
⎨
⎩

q

j=1

1+

q

j=1


y j αi j (r ) +
⎫
⎬

y j αi j (r ) .
⎭

c
i −1
r =0

⎤
πir αi j (r ) ⎦

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

134

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Hence
πir (y) − Eπir (y) = πir

q


y j αi j (r )

j=1

and
cov{πir (y), πks (y)} = πir πks

q


αi j (r )αk j (s).

j=1

Expressed in matrix notation,
cov(x) = PA(PA)
and therefore,
cov(Y) = A PA(PA) A = (A PA)2 .

(5.47)

It thus follows that if we have constrained A PA to be diagonal then, to a first
approximation, the components will be uncorrelated. There are interesting parallels
here with results for the normal linear model.
The estimation of the π s requires no new theory and we thus proceed as before,
using the estimators of (5.31).

5.7

Binary data as a special case

We now have to justify the results given in Section 4.7, and this is done by putting
ci = 2, for all i, in the formulae of this chapter. The vector α is then given by
α  = (α1 (0), α1 (1), α2 (0), α2 (1), . . . , α p (0), α p (1)).
The elements of the vector have been constrained so that
πi0 αi (0) + πi1 αi (1) = 0

(i = 1, 2, . . . , p).

(5.48)

In the binary case we estimated a p-vector of parameters, also called α, given by α  =
(α1 , α2 , . . . , α p ); αi was the score associated with category 1 of each variable – the
score for the first category was zero. Since both approaches are essentially estimating
the difference between the scores for each variable, the relationship between the two
sets of αs is that
αi = αi (1) − αi (0),

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

135

or, using (5.48), that
αi (0) = −πi1 αi

αi (1) = πi0 αi

and

(i = 1, 2, . . . , p).

Since, in the binary case, πi1 = πi and πi0 = 1 − πi we may write
⎡

−π1

⎢
⎢ 1 − π1
⎢
⎢
⎢ 0
⎢
⎢
α=⎢ 0
⎢
⎢ .
⎢ ..
⎢
⎢ .
⎣ ..
0

0

...

0

...

−π2

...

1 − π2

...

0
..
.

...

0
..
.
..
.
..
.
..
.

0

...

−π p
1 − πp

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥ ∗
⎥ α = Sα ∗ ,
⎥
⎥
⎥
⎥
⎥
⎦

(5.49)

say, where, temporarily, (α ∗ ) = (α1 , α2 , . . . , α p ). Substituting (5.49) into (5.32)
yields
P̂−1 X XSα ∗ = (n + μ)Sα ∗ .

(5.50)

We convert this into another eigenequation in two steps. First pre-multiply both sides
by S P̂, giving
S X XSα ∗ = (n + μ)S P̂Sα ∗ .

(5.51)

Next pre-multiply by (S P̂S)−1 , so that
(S P̂S)−1 S X XSα ∗ = (n + μ)α ∗ .

(5.52)

This is the required equation and it remains to simplify the left-hand side. As before,
we replace the π s by their estimators given by (5.31). Direct multiplication gives
Ŝ P̂Ŝ = diag {π̂i (1 − π̂i )} = Q̂,
say, and
Ŝ X XŜ =

n

ˆ
(xh − π̂ ) (xh − π̂ ) = n ,
h=1

say, where xh = (x1h , x2h , . . . , x ph ) = (x1h (1), x2h (1), . . . , x ph (1)). Equation (5.52)
may thus be written
ˆ ∗ = (n + μ)Q̂1/2 α ∗ .
Q̂−1/2 n α

(5.53)

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

136

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

If we make the further transformation β = Q1/2 α ∗ , the equation becomes
ˆ Q̂−1/2 β = 1 +
Q̂−1/2 

μ!
β
n

(5.54)

and the left-hand matrix will be recognised as the ‘phi-coefficient’ matrix for the
binary variables. It follows that the βs are the principal components derived from
the phi-coefficient matrix. The αs are thus not principal component weights because
of the weightings {π̂i (1 − π̂i )}−1/2 arising from multiplication by Q−1/2 . Since the
αs derived by this means are also the scores that would be derived from a multiple
correspondence analysis, this result also shows the connection of that technique with
principal components.

5.8

Ordering of categories

Nothing done so far requires or implies any ordering of the categories. Having fitted
a model we could, of course, relabel the categories so that their estimated scores
were ranked in increasing or decreasing order. The analysis might then be seen as a
way of uncovering an underlying metric. Conversely, if we believed the categories
to be ordered a priori we could impose the order restrictions on the parameters and
estimate the αs subject to these constraints. However, the basis for any supposed
prior ordering would be likely to rest on the belief that there was some variable
underlying the categories. If so it would seem more sensible to make such underlying
variables explicit in the model as in the underlying variable binary model. There are,
in fact, two such models which we could use. One turns out in some circumstances
to be equivalent to the response function model discussed in Sections 5.1 and 5.2
but does not lead to a natural ordering of the categories. The other is the obvious
generalisation of the underlying variable model for binary data. This is equivalent
to a response function model, but not the one given in Section 5.2. We treat each of
them, in turn, taking the latter first, in Sections 5.8.1 and 5.8.4.

5.8.1

A response function model for ordinal variables

A review of the most widely used latent variable models for ordinal variables can
be found in van der Linden and Hambleton (1997). There are models that can accommodate ordered categories such as the graded response model generalised from
either the probit/normal model or the logit/normal model for binary responses and
the partial credit model generalised from the Rasch model or logit/normal model.
Samejima (1969) formulated a latent variable model for graded scores as a generalisation of the probit/normal model or logit/normal model. Her unidimensional graded
response model was a starting point for later research by Muraki (1990) and Muraki
and Carlson (1995) on maximum likelihood estimation for graded response models
and multidimensional models. Moustaki (2000) discusses the graded response model
within the framework of exponential family distributions.

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

137

As before, the ci ordered categories have, for item i, response category
probabilities πis (y) (s = 1, . . . , ci ). To preserve the ordinality property of an item we
model the cumulative response function
link[is (y)] = αi0(s) −

q


αi j y j

(s = 1, . . . , ci − 1; i = 1, . . . , p), (5.55)

j=1

where is (y) is the cumulative probability of a response in category s or lower of
item xi , written
is (y) = πi1 (y) + πi2 (y) + · · · + πis (y).
To simplify notation we just write is in the following.
The link function can be the logit, the complementary log-log function, the
inverse normal function, the inverse Cauchy, or the log-log function. All those link
functions are monotonically increasing functions that map (0, 1) onto (−∞, ∞).
The parameters αi0(s) are referred to as ‘cut-points’ on the logistic, probit or other
scale, where αi0(1) ≤ αi0(2) ≤ · · · ≤ αi0,(ci ) = +∞. The negative sign in front of the
slope parameter is used to indicate that as y increases the response on the observed
item xi is more likely to fall at the high end of the scale. Note that all category
response probabilities have the same discriminating power. The logit link leads to the
proportional odds model,
"
ln

#
q

is (y)
= αi0(s) −
αi j y j
1 − is (y)
j=1

(s = 1, . . . , ci − 1).

(5.56)

From (5.56) we see that the difference between corresponding cumulative logits is
independent of the category involved. As in the unordered case of (5.3), we take the
conditional probability function of xi given y to be multinomial so that
g(xi | y) =

ci


πis (y)xi (s)

s=1
ci

=
(is − i,s−1 )xi (s) ,

(5.57)

s=1

where xi (s) = 1 if a randomly selected individual responds into category s of the ith
item and xi (s) = 0 otherwise. We can write (5.57)as
g(xi | y) =

c
i −1 
s=1

is
i,s+1

xi∗ (s) 

i,s+1 − is
i,s+1

xi∗ (s+1)−xi∗ (s)

,

(5.58)

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

138

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

where xi∗ (s) = 1 if a randomly selected individual responds into category s or a lower
one of the ith item and xi∗ (s) = 0 otherwise. Taking logarithms through (5.58),

ln g(xi | y) =

c
i −1 "

xi∗ (s) ln

s=1

=

c
i −1

is
i,s+1
− xi∗ (s + 1) ln
i,s+1 − is
i,s+1 − is

[xi∗ (s)θis (y) − xi∗ (s + 1)b(θis (y))],

#

(5.59)

s=1

where
θis (y) = ln

is
i,s+1 − is

(s = 1, . . . , ci − 1)

(5.60)

and
b(θis (y)) = ln

5.8.2

i,s+1
= ln{1 + exp(θis (y))} (s = 1, . . . , ci − 1).
i,s+1 − is

(5.61)

Maximum likelihood estimation of the model with
ordinal variables

The E-M algorithm used in the binary case in Section 4.5 and for the unordered
polytomous case in Section 5.5 can be also applied here. The estimation is presented for a single latent variable, but a generalisation to the multidimensional
case is straightforward. For a random sample of size n the complete log-likelihood
is written
Lc =
=

n


ln f (xh , yh )

h=1
 p
n


h=1


ln g(xi h | yh ) + ln h(yh )

(5.62)

i=1

Substituting (5.59) in (5.62), we have

L=

n

h=1

ln g(xh | yh ) =

 p c −1
n
i


h=1

i=1 s=1


[xi∗ (s)θis (yh ) − xi∗ (s + 1)b(θis (yh ))] .

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

139

The E-step is therefore
E[L | x] =

 n
p ci −1

 
$
i=1 s=1

h=1

i=1 s=1

h=1

xi∗ (s)E(θis (yh ) | x1 , . . . , xn )

%&
− xi∗ (s + 1)E(b(θis (yh )) | x1 , . . . , xn )

 n
p ci −1

 
$ ∗
%
∗
xi (s)E(θis (yh ) | xh ) − xi (s + 1)E(b(θis (yh )) | xh ) .
=
As in the unordered case, for the M-step, one can maximise separately for each item
i and each category s the quantity
n

$

%
xi∗ (s)E(θis (yh ) | xh ) − xi∗ (s + 1)E(b(θis (yh )) | xh ) .

(5.63)

h=1

As in Section 5.5, we approximate the averages in (5.63) with Gauss–Hermite quadrature, giving
=

n 
k


xi∗h (s)θis (y t )h(y t | xh ) −

h=1 t=1

=

k


xi∗h (s + 1)b(θis (y t ))h(y t | xh )

h=1 t=1

rit (s)θis (y t ) −

t=1

n 
k


k


rit (s + 1)b(θis (y t )),

(5.64)

t=1

where
rit (s) =

n


xi∗h (s)h(y t | xh )

(s = 1, 2, . . . , ci − 1; i = 1, 2, . . . , p).

(5.65)

h=1

Other approximations to the averages, as discussed in Section 4.5.2, can be used here
too. The derivatives required in the M-step for the model in (5.56) are
∂θis (y)
(1 − is )i,s+1
,
=
∂αi0(s)
(i,s+1 − is )
∂b(θis (y))
is (1 − is )
,
=
∂αi0(s)
(i,s+1 − is )
∂θis (y)
= −yi,s+1 (s = 1, . . . , ci − 1)
∂αi1
and
∂b(θis (y))
= −yis
∂αi1

(s = 1, . . . , ci − 1).

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

140

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

5.8.3

The partial credit model

A simpler version of the proportional odds model is the partial credit model (Masters
1982) which assumes that the respondent processes the item task in successive steps
with probability πis of success in making the step from s to s + 1, where πi1 = 1. The
process terminates when the first failure occurs or when the step s is equal to the last
category of the item ci . The partial credit model is a generalisation of the Rasch model
(Rasch 1961) for ordinal responses that assumes one parameter for individuals and
one parameter for items. We will outline the main features of the partial credit model
here. The main focus is on comparing adjacent categories, and we shall leave out any
other probabilities. Therefore any parameters of the model are defined with respect
to two adjacent categories rather than taking into account all categories of the item.
We let πis | s−1 (y) be the conditional probability of selecting category s rather than
s − 1, and that conditional probability is expected to increase monotonically with
the latent variable. The conditional probability for a randomly selected individual is
modelled as
πis | s−1 (y) =

exp(y − αi0,s )
πis (y)
=
πi,s−1 (y) + πis (y)
1 + exp(y − αi0,s )

(s = 1, . . . , ci ).

(5.66)

Assuming that each individual is assigned only one score on item i from (5.66), it
follows that the response category probability

πis (y) = 
mi

exp

k=1

s

exp

l=1 (y − αi0,l )
!.
k
(y
−
α
)
i0,l
l=1

(5.67)

Conditional, joint and marginal maximum likelihood estimation methods can be
used for estimating the parameters of the partial credit model (Masters and Wright
1997). A generalised partial credit model that allows for a different slope for each
item has been developed by Muraki (1992).

5.8.4

An underlying variable model

For the binary case we imagined that the data had arisen by recording whether or
not underlying variables had values above or below fixed thresholds. The obvious
generalisation is to suppose that the range of each underlying variable is divided
into non-overlapping intervals and that the manifest categorical variables record into
which interval the individual falls. Thus we suppose that underlying each set of
categories there is a continuous random variable ξ whose variation is explained by
the standard linear factor model
ξ =μ+

y + e.

(5.68)

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

141

On the ith dimension there is a sequence of thresholds τi1 , τi2 , . . . , τi,ci −1 with
associated indicator variables

1 if τis ≤ ξi < τi,s+1 (s = 0, 1, . . . , ci − 1),
xi (s) =
0 otherwise,
where we define τi0 = −∞, τici = ∞. The aim is thus to make inferences about the
parameters and fit of the model (5.68) on the evidence of a sample of observations
on x. We assume that the underlying bivariate distributions are normal and hence
we can fit the model provided we can estimate the correlation coefficients. Estimates
of these coefficients are known as polychoric correlations. Their computation from
ordered contingency tables with more than two categories is more difficult than for
2 × 2 tables, but a maximum likelihood method is given in Olsson (1979) and an
algorithm based on the polychoric series in Martinson and Hadman (1975). Jöreskog
(1994) extends this work to include the asymptotic standard errors.
The model just described is not equivalent to the response function model of
Section 5.2 unless, of course, we have only two categories on each dimension. This
is apparent from the fact that the categories are necessarily ordered in the underlying
variable model, whereas in the response function model they are not. However, the
response function model of Section 5.8.1 can be made equivalent. By the result of
Section 4.4 this is equivalent to a underlying variable model with
αi0(s) =

τi,s−1 − μi
1/2
ψi

,

αi j =

λi j
1/2

ψi

(s = 1, 2, . . . , ci − 1; i = 1, 2, . . . , p)
(5.69)

and the weighted error distribution having a logistic distribution. The threshold
parameters {τis } are thus related to the αi0(s) (or π s) and the factor loadings to the αs.
This model does not arise from the sufficiency principle, and this means that we
no longer have the same range of interpretations for the parameters. The parameter
πis is still the probability that the median individual falls in category s on variable i,
but the αs cannot now be interpreted as category scores or as weights in a component.
This is because the posterior density of y is no longer dependent on a linear function
of the manifest variables. To see this we write
πis (y) = is (y) − i,s+1 (y)
⎧
⎛
⎞⎫−1
q
⎨
⎬

= 1 + exp ⎝−αi0(s) +
αi j y j ⎠
⎩
⎭
⎧
⎨

⎛

j=1

− 1 + exp ⎝−αi0(s+1) +
⎩

q

j=1

⎞⎫−1
⎬
αi j y j ⎠
.
⎭

When this is substituted into (5.4) it does not yield a set of linear components.

P1: TIX/XYZ
JWST070-05

142

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Various model specifications under the underlying variable model
The simplest method of fitting this model is to utilise the link with the underlying
factor model. If we can estimate the correlation coefficients of the underlying
variables from the observed categorical variables, any factor analysis program will
do the rest. This method is not fully efficient, and it will not provide asymptotic
standard errors for the estimators. Nevertheless, once the correlation matrix has been
estimated, the whole analysis is moved onto familiar territory. We will explore three
different methods that utilise different parts of the data, ranging from full information
maximum likelihood to limited-information maximum likelihood methods.
Since only ordinal information is available about ξi , the mean and variance
of ξi are not identified and are therefore set to zero and one, respectively. Other
parameterisations are possible; see, for example, Lee et al. (1990a). Utilising the
assumptions of the classical factor analysis model, it is further assumed that ys and
es are independent and normally distributed with y j ∼ N (0, 1) and ei ∼ N (0, ψi ).
with
Furthermore, it follows that ξ1 , . . . , ξ p has a multivariate normal distribution
q
zero means, unit variances and correlation matrix P = (ρi j ), where ρi j = l=1 λil λ jl .
The parameters of the model are the thresholds τis (i = 1, 2, . . . , p; s =
1, 2, . . . , ci − 1) and the factor loadings λi j (i = 1, 2, . . . , p; j = 1, 2, . . . , q). From
the multivariate normality of ξ1 , . . . , ξ p , it follows that the probability of a general
p-dimensional response pattern is
Pr(x1 = s1 , . . . , x p = s p ) =

τ1,s1
τi,s1 −1

···

τ p,s p
τ p,s p −1

φ p (u 1 , . . . u p | P)du 1 · · · du p , (5.70)

where φ p (u 1 , u 2 , . . . u p | P) is a p-dimensional normal density function with zero
means, unit variances and correlation matrix P. It is apparent that full information
maximum likelihood estimation under the multivariate underlying variable approach
requires the evaluation of the p-dimensional integral in (5.70) for each response
pattern in the sample at several points of the parameter space. It is obvious that this
approach is not feasible for more than few variables. Alternative methods are required
within the underlying variable approach. Approaches that do not specify a model for
the complete p-dimensional response pattern but instead make use of the univariate
and bivariate marginals have been developed by Jöreskog (1990, 1994) and in a more
general setting by Muthén (1984) and Muthén and Satorra (1995). Those methods
have been implemented in LISREL (Jöreskog and Sörbom 2006) and Mplus (Muthén
and Muthén 2010). In the first step, the thresholds are estimated from the univariate
margins of the observed variables. In the second step, the polychoric correlations are
estimated from the bivariate margins of the observed variables for given thresholds.
In the third step, the factor model is estimated from the polychoric correlations by
weighted least squares using a weight matrix which is the inverse of an estimate
of the asymptotic covariance matrix of the polychoric correlations. The asymptotic
covariance matrix is often unstable in small samples, particularly if there are zero or
small frequencies in the bivariate margins. The link between the parameters of the
standard factor model and those of the underlying variable model is given in (5.69).

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

143

Jöreskog and Moustaki (2001) explored a composite maximum likelihood
approach that estimates the thresholds and the factor loadings in one single step
from the univariate and bivariate margins without the use of a weight matrix. This
approach only requires bivariate normality of all pairs of underlying response
variables and uses only the data in the univariate and bivariate margins to estimate
the model. The estimation method has been implemented in LISREL (Jöreskog and
Sörbom 2006). From the multivariate normality of ξ1 , . . . , ξ p , it follows immediately
that the probability πis of a response in category s on variable i is
πis (ai ) =

τis
τi,s−1

φ(u)du,

(5.71)

where φ(u) is the standard normal density function and ai is a vector of model
parameters associated with item i, and the probability πi j,sh of a response in category
s on variable i and a response in category h on variable j is
πi j,sh (ai ) =

τis

τ jh

(i)
τi,s−1

τ j,h−1

φ2 (u, v | ρi j )dudv,

(5.72)

where φ2 (u, v | ρ) is the density function of the standardised bivariate normal distribution with correlation ρ. Their proposal is to maximise the sum of all univariate and
bivariate log-likelihoods
L=

p
ci


i=1 s=1

pis ln πis (ai ) +

cj
p i−1 ci



pi j,sh ln πi j,sh (ai ),

(5.73)

i=2 j=1 s=1 h=1

where pis is the sample proportion of category s in the univariate margin of variable i,
pi j,sh is the sample proportion of category s and h in the bivariate margin of variables
i and j, and πis (ai ) and πi j,sh (ai ) are given by (5.71) and (5.72), respectively. The
integrals in (5.71) and (5.72) can be computed directly from the univariate and
bivariate distribution functions. Only data in the univariate and bivariate margins are
used. This approach is quite feasible in that it can handle a large number of variables
as well as a large number of factors.
The underlying variable model has fewer parameters than the response function
model because the αi j ( j > 0) do not involve the argument s which indexes the
categories. This approach was developed by Lee et al. (1990b, 1992), who also give
various simplifications based on a two-stage procedure in Lee et al. (1995). These
authors consider the problem in the context of the broader structural equations model,
and they allow mixtures of continuous and categorical variables among the manifest
variables. We return to both of these generalisations in Chapters 7 and 8.
As we have already shown, a promising new approach to maximising complicated likelihood functions is provided by the Gibbs sampler algorithm. This was
demonstrated for Lee, Poon and Bentler’s problem by Shi and Lee (1998).

P1: TIX/XYZ
JWST070-05

144

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

A third approach is to exploit the connection with the logit/normal model for
binary data expressed in (5.56). By reducing each variable to a dichotomy, we can
estimate all of the αi j and those αi0 (s) corresponding to the point at which the
dichotomies have been made. Questions then arise about the best way of making the
reduction to two categories, and this requires further investigation, but one would
expect that one would aim to get a near 50–50 split. The other values of the αi0 (s)
could be estimated by making the split at all other possible points, though this would
be tedious if there were large numbers of categories. A simpler alternative would
be first to estimate the αi j , as described above, and then to equate the observed and
expected marginal proportions. This gives the estimating equations
Eπis (y) = n is /n

(i = 1, 2, . . . , p; s = 0, 1, . . . , ci − 1),

where nis is the number of sample members responding in category s of variable i.
A review of full and limited-information maximum likelihood methods for fitting
a latent variable model to ordinal variables can be found in Jöreskog and Moustaki
(2001).

5.9

An alternative underlying variable model

Another way of relating categorical responses to underlying variables has been
developed for modelling choice behaviour in economics (see, for example,
McFadden (1974, 1982)). This approach is of equal relevance whether one is choosing
among a set of alternatives in a multiple choice question or among candidates in an
election. The model was introduced into latent variable modelling in the first edition
of this book (Bartholomew 1987) and developed in a broader context by Arminger
and Küsters (1988).
In this model a random variable is associated with each category, and the category
into which an individual falls is determined by which of these underlying variables
turns out to be largest. Thus, suppose that there are variables ξi0 , ξi1 , . . . , ξi,ci −1
associated with variable i. The realised values of these variables, for any i, can
be thought of as measures of the relative ‘attractiveness’ of the categories to that
individual. The largest ξ thus determines which category ‘wins’ in the competition
for the individual’s vote, purchasing decision or whatever. If we postulate that a
number of latent variables together with a random ‘error’ contribute to the final
attractiveness, we may suppose that
ξ i = μi +

iy

+ ei

(i = 1, 2, . . . , p),

(5.74)

with the usual distributional assumptions about the random variables. Here ξ i =
(ξi0 , ξi1 , . . . , ξi,ci −1 ), μi = (μi0 , μi1 , . . . , μi,ci −1 ) and i = {λi j (s)} is a ci × q
matrix of factor loadings. We thus have a model in which a common set of
latent variables (factors) account for the associations between observed categorical
variables.

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

145

The parameters μi may be interpreted as measuring the average attractiveness of
the categories of variable i. The term i y shows how the q latent variables influence
their attractiveness, and ei represents the unexplained variation. All that we can
observe is which of the elements in ξi is the greatest, and, using this information,
we have to devise methods for estimating the parameters of the model. To do this by
maximum likelihood we have to specify the form of the distributions of the random
variables involved. As in earlier models of the form (5.74) that we have considered,
this choice is not likely to be critical since there will be a central limit effect inducing
normality in the ξ s. We might therefore be content with any reasonable specification
which renders the subsequent analysis tractable.
There is one such specification which makes this underlying variable model
equivalent to the logit response function model of Section 5.2. This requires us
to assume that each residual element in (5.74) has the same Type I extreme-value
distribution; that is,
P{eis ≤ u} = exp(− exp(−u)),

for all i and s.

(5.75)

The significant part of this assumption is the independence of i and s, implying that
the extraneous sources of variation act on the attractiveness of all categories for all
variables equally.
We now demonstrate the equivalence referred to by deriving the response function
πis (y) for this model as follows:
πis (y) = P{individual falls into category s of variable i | y}
= P{ξis ≥ ξir , r = 0, 1, . . . , ci − 1 | y}
=
=

+∞

−∞
+∞
−∞

P{ξis ∈ (u, u + du) ξir ≤ u; r = 0, 1, . . . , ci − 1 | y} du
P{ξis ∈ (u, u + du) | y}

c
i −1

P{ξir ≤ u | y}du.

(5.76)

r =0
r =s

The last step follows from the independence of the ξ s when y is fixed. Now
⎫
+ ⎬
+
P{ξir ≤ u | y} = P μir +
λi j (r )y j + eir ≤ u +y
⎭
⎩
j=1
⎧
⎫
q
⎨
+ ⎬

+
= P eir ≤ u − μir −
λi j (r )y j +y
⎩
⎭
j=1
⎧
⎛
⎞⎫
q
⎨
⎬

= exp − exp ⎝−u + μir +
λi j (r )y j ⎠
⎩
⎭
⎧
⎨

q


j=1

(5.77)

P1: TIX/XYZ
JWST070-05

146

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

by (5.75). The density function is obtained by differentiation, and when this is
substituted into (5.76) we obtain
⎧ ⎛
⎞⎫
q
⎨
⎬

exp − ⎝u − μis −
λi j (s)y j ⎠
πis (y) =
⎩
⎭
−∞
j=1
⎧ ⎛
⎡
⎞⎫⎤
q
c
i −1
⎨
⎬

× exp ⎣−
exp − ⎝u − μir −
λi j (r )y j ⎠ ⎦ du
⎩
⎭
r =0
j=1
⎛
⎞
q

= exp ⎝μis +
λi j (s)y j ⎠
+∞

j=1

×

+∞
−∞

⎧
⎨

exp − u + e−u
⎩

c
i −1
r =0

⎛
exp ⎝μir +

q

j=1

⎞⎫
⎬
λi j (r )y j ⎠ du.
⎭

(5.78)

The integral may be evaluated by substituting v = e−u , giving

πis (y) = 

exp μis +

ci −1
r =0

q

exp μir +

j=1

λi j (s)y j

q
j=1

!

λi j (r )y j

! (s = 0, 1, . . . , ci − 1; i = 1, 2, . . . , p).
(5.79)

Comparing this with (5.9), we see that the two models are identical because αi0 (s) ≡
μis and αi j (s) ≡ λi j (s).
If we wish, therefore, the parameters can be interpreted as required by the ‘choice’
model. Whether or not this model provides a useful interpretation will depend on what
the variables represent in a particular practical situation. If, for example, respondents
to a questionnaire are asked which party they would vote for in an election, it
seems reasonable to suppose that this would depend partly on their position on a
variety of attitude scales (the ys) common to all individuals and partly on their own
idiosyncrasies (the es). The same would be true for many other categorical variables
arising from the expression of attitudes, preferences or opinions. The argument seems
less convincing for biographical variables such as country of birth or level of education
attained. However, the fact that the parameters can also be given the more neutral
interpretations of Section 5.3 means that nothing vital depends on the choice.
The appearance of the extreme-value distribution in the error term where we might
have expected a distribution such as the normal or logistic is unusual and not easy
to interpret. The Type I distribution is positively skewed and rather like a log-normal
distribution in shape. Fortunately, as noted in Section 5.9 above, its influence on the
form of the distribution of ξ is not likely to be large, so it should not be critical for
the form of the response function.

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

147

The identity between the αs and the λs implies that the origin of the λs is arbitrary
for given i and j. This could be deduced directly from the underlying variable model.
Here it means that, without loss of generality, we could regard the factors y as having
no effect on the attractiveness of one arbitrarily selected category on each manifest
dimension.
An interesting feature emerges if we specialise to the binary case with ci = 2
for all i. We know that our model is then equivalent to the response function model
which, in turn, is equivalent to the underlying variable model of Section 4.3. It should
then follow that the underlying variable model and the present one are equivalent in
the binary case. That this is so is easily demonstrated. From (5.74),
ξi0 − ξi1 = (μi0 − μi1 ) + {λi (0) − λi (1)}y
= ei0 − ei1 (i = 1, 2, . . . , p),
where λi (s) = {λi j (s)}, s = 0, 1; the probability of a response in category 1, given y,
is then
P{ξi1 − ξi0 ≥ 0 | y} = P{ei1 − ei0 ≥ μi0 − μi1 + {λi (0) − λi (1)}y | y}.

(5.80)

For the underlying variable model of Chapter 4 the corresponding probability is
⎧
⎨ e
τi − μi −
i
P{ξi ≥ τi | y} = P
1 ≥
1/2
⎩ψ 2
ψi
i

where i is the ith row of
be chosen so that

⎫
+ ⎬
+
y
i +
+y⎭ ,

(5.81)

. For the two to be the same the parameter values must
−1/2

μi1 − μi2 = (τi − μi )ψi

and

λi (1) − λi (0) =

−1/2
,
i ψi

−1/2

must be the same as that of ei1 − ei0 . This last
and the distribution of ei ψi
requirement is easily verified, since if ei0 and ei1 have independent Type I extremevalue distributions their difference has a logistic distribution with variance π 2 /3.
−1/2
must have if it is to be
This is precisely the form which the distribution of ei ψi
equivalent to the response function model. The two underlying variable models thus
coincide for binary data.

5.10

Posterior analysis

There are no essentially new issues arising in the posterior analysis of polytomous
models. Those models which are based on the sufficiency principle yield linear
components which can be used to rank individuals on the latent scale as before. Some
examples are given below. The posterior expectations and variances can be computed
from the posterior distribution as given in (5.4).

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

148

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

In the case of the underlying variable model, methods based on the E-M algorithm
have been provided by Shi and Lee (1997a,b). The underlying variable model is
basically an NLFM and hence the posterior distribution, when expressed in terms of
the underlying variables, is multivariate normal. For such a distribution the mean and
the mode coincide and Shi and Lee’s method is to find the mode of that distribution.

5.11

Further observations

There is now a fairly complete theory of latent trait models available for polytomous
data, both nominal and ordinal, but caution must be exercised in applying it. Much
less practical experience of these methods is available than for those for binary data.
The very large number of parameters involved, especially in the nominal model,
magnifies the problems we have already noted with binary models. For example,
there may be problems with the convergence of the estimators, and their estimated
asymptotic standard errors may be very large and, hence, unreliable. Again, the
problems of testing goodness of fit arising from the sparseness,of the contingency
p
table are potentially more serious. The number of cells is now i=1 ci , which may
p
be substantially larger than 2 . In the binary case we saw that this problem could
be alleviated by pooling response patterns to avoid very small expected frequencies.
With polytomous data there is a second option. This is to combine categories. In the
social sciences, especially, this can often be done in a substantively meaningful way.
Categories of the Likert variety often consist of five or seven possible responses,
but one finds that the bulk of observations fall into only two or three of them. Little
information is then lost by amalgamating these little used categories with larger ones.
Of course, even if this procedure is taken to the limit in which all variables become
dichotomies, sparseness may remain. In that case, resort must be had to pooling
response patterns and/or using the Monte Carlo test recommended for use in these
circumstances in the case of binary data. Section 4.9 discusses various methods for
model testing that can be applied to the polytomous case. In the polytomous case
as in the binary case, tests that are based on residuals calculated from the marginal
frequencies of various orders developed for multidimensional contingency tables
(Maydeu-Olivares and Joe 2006) can be used.
An important practical question concerns the choice of model. Should we use the
logit or probit response function model (divide-by-total) or the underlying variable
(difference) model in any particular case? The underlying variable model has received
most attention in the literature primarily, one suspects, because it fits easily into the
familiar framework of factor analysis. However, it is clear that there are circumstances
in which it is not appropriate. If there were an underlying variable, the categories
formed from it would necessarily be ordered. Without some a priori ordering it
would therefore be pointless to consider such a model. Even where ordering occurs,
as when respondents in a social survey are asked to express a strength of opinion,
there is likely to be a ‘don’t know’ category which does not fit into the ordering
of the remaining categories. In such cases, the response function model may be
more appropriate.

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

149

The response function model can always be used and it provides a means of
checking any supposed ordering. We saw that the αi j (s) could be viewed as category
scores and their ordering should correspond to the a priori ordering of the categories.
A more sophisticated way of accommodating ordered categories into the response
function model mentioned in Section 5.8 would be to maximise the likelihood subject to order restrictions on the category scores using the techniques of monotonic
regression. This would be particularly useful if only some of the manifest variables
had ordered categories. This avenue remains to be explored.
The choice between the two kinds of model in practice is likely to depend on a
blend of substantive plausibility and goodness of fit. A good fit is a necessary attribute
of a good model, but it may not be sufficient.
One of the most intriguing things revealed by our analysis of the response function
model is the connection with multiple correspondence analysis (see also Chapter 9).
Both techniques can be viewed as methods for assigning scores to the categories of
a multi-way contingency table. The link between the sets of scores obtained from
the two approaches was established by our demonstration that the correspondence
analysis scores were first-order approximations to the αi j (s) of the response function
model. In Example 5.12.1 below, the correlation coefficient between the two sets of
scores was 0.97, and our limited experience in other cases suggests that this may not
be untypical. Multiple correspondence analysis is very easy to carry out as it only
involves the eigenanalysis of a matrix containing the two-way margins.
The link with correspondence analysis suggests a further way of using the
latent trait model. Correspondence analysis has traditionally been concerned
with the two-way contingency tables, sometimes with many categories on each
dimension. Provided that there are enough cells, it would be possible to fit a
latent trait model as a way of explaining or measuring the association between
only two variables. This too is a matter for further research.

5.12

Examples of the analysis of polytomous data
using the logit model

Example 5.12.1 This first example comes from the results of a staff assessment
exercise in a large public organisation. A five-point ordered scale was used to assess
397 managers on 13 aspects of their work. Three of the variables are used in this
illustration. The higher categories were rarely used, so some amalgamation was
carried out. On variable 1, categories 4 and 5 were amalgamated, and on variables 2
and 3, categories 4, 5 and 6. The resulting data constitute a 4 × 3 × 3 contingency
table. The frequencies are set out in Table 5.1, where they are listed in order of
posterior mean. We shall fit a one-factor response function model to see whether the
three categorical variables can be replaced by a single underlying latent variable and,
if so, how each of the observed variables contributes to the latent scale.
The calculations were done with the IRTPRO program, (Cai et al. 2011) and all
entries have been rounded for ease of presentation. In order to economise on space,

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

150

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 5.1 The fit and scores obtained by fitting the logit/normal model to staff
assessment data.
Response
pattern
111
211
311
112
212
121
131
221
312
231
321
122
331
132
222
232
322
332
123
422
133
223
432
233
323
333
423
433

Frequency

Expected
frequency

E(y | x)

Standard
deviation

1
7
1
2
13
3
1
10
5
5
3
12
1
4
64
36
38
31
1
4
1
37
3
23
34
41
5
11

2.3
6.4
1.2
2.8
11.6
2.0
0.7
9.9
3.4
3.9
3.6
9.1
1.7
4.3
68.6
38.1
40.9
26.8
2.1
3.3
1.6
29.0
2.7
26.8
35.6
40.1
6.8
9.9

−2.17
−1.92
−1.64
−1.61
−1.37
−1.34
−1.14
−1.09
−1.09
−0.88
−0.79
−0.76
−0.57
−0.54
−0.48
−0.26
−0.17
0.06
0.13
0.21
0.36
0.42
0.44
0.66
0.76
1.02
1.20
1.49

0.61
0.59
0.59
0.59
0.59
0.60
0.60
0.61
0.61
0.62
0.62
0.62
0.63
0.63
0.63
0.63
0.64
0.64
0.64
0.64
0.65
0.65
0.65
0.66
0.67
0.69
0.71
0.74

χ 2 = 11.44 on 21 degrees of freedom, P = 0.9537.

results are presented only for response patterns which have a non-zero observed
frequency.
It is clear that the one-factor model provides an excellent fit, so the observed
response patterns are entirely consistent with a single underlying dimension which
one might describe as success in the job.
One can further elucidate the relationship between the latent variable and the
observed variables by looking at the parameter estimates given in Table 5.2. We
notice first that the standard errors of the αs are very large, so if we interpret the
αs as category scores they must be regarded as very imprecisely determined on a

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

151

Table 5.2 Maximum likelihood estimates and standard errors for the logit/normal
model fitted to staff assessment data.
Category (s)

α̂i (s)

s.e.α̂i (s)

Approx αi (s)

π̂is

s.e.(π̂is )

1

0
1
2
3

0
0.70
1.49
2.41

—
0.43
0.51
0.80

0
0.76
1.33
1.79

0.05
0.52
0.40
0.03

—
0.03
0.03
0.02

2

0
1
2

0
2.37
2.93

—
1.51
1.53

0
1.22
1.68

0.02
0.59
0.40

—
0.03
0.03

3

0
1
2

0
1.57
3.77

—
0.76
1.58

0
1.00
1.68

0.02
0.67
0.31

—
0.10
0.09

Variable(i)

sample of this size. The categories were ordered, and it is reassuring to note that for
each variable the αs form a monotonic sequence. In Section 5.3 we noted several
ways in which the αs could be interpreted. Viewing them as weights in the formation
of the components, we see that variable 3 carries most weight, and this accounts
for the preponderance of response patterns in the highest category of this variable
at the upper end of the scale of Table 5.1. Similarly, variable 2 is more important
than variable 1. Interpreted as discrimination parameters, it appears that variable 3,
with the largest spread of scores, is the best single variable for distinguishing the
overall quality of individuals. Looking at the αs as category scores, the categories of
variables 1 and 3 are roughly equally spaced, whereas for variable 2, categories 2 and
3 are very close together and might be amalgamated with little loss of information.
The π s are much more precisely estimated. Recall that πis is the probability
of the ‘median individual’ falling into category s on variable i. As one might have
anticipated, such an individual will most likely be placed in the middle category.
Table 5.2 also gives the approximate maximum likelihood estimates for the αs
obtained by the method described in Section 5.6; they are also used as starting values
for the iterative procedure used to obtain the estimates. They are equivalent to the
scores obtained by multiple correspondence analysis and therefore of independent
interest. At first sight the approximation is very poor; the true estimates exceed
the starting values by a factor of roughly 2, and this seems to be a typical result.
However, as we observed earlier, it is the profile of the scores which is important for
interpretation. In every case the approximate scores place the categories in the same
order and at roughly the same spacing. (It is this fact which makes them good starting
values for iteration, because it appears to be important to have the right ordering to get
rapid convergence.) One way of judging the accuracy of the approximate estimates
is to compute the approximate components and to see how the rankings assigned to
response patterns compare. If this is done for the data of this example, we noted above

P1: TIX/XYZ
JWST070-05

152

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

that the ranking assigned to response patterns, using the final and initial estimates in
constructing the components, has correlation coefficient 0.97. In this sense therefore
the approximate estimates give results which differ little from the exact estimates.
Example 5.12.2 It often happens that a category such as ‘don’t know’ is included
among a set of possible answers to a question in a survey. For example, in a simple
case there might be three answers: yes, no and don’t know.
If we were to use an underlying variable model, we would have to decide on
an ordering of the responses and, in particular, whether ‘don’t know’ should be the
middle category. One advantage of the response function model is that the data are
allowed to speak for themselves by determining the category scores. Our second
example illustrates this using data from Leimu (1983).
The study was of a sample of 469 employees, obtained in 1975, from a small
industry in Finland. There are three variables defined by the following questions.
1. Was there any alternative choice of job when coming to your present job?
(0 = no, 1 = don’t know, 2 = yes)
2. Is the job permanent? (0 = very unsure or quite unsure, 1 = don’t know,
2 = quite sure or very sure)
3. Were you unemployed in the last three years? (0 = no, 1 = yes)
The results of fitting the one-factor response function model are given in
Tables 5.3 and 5.4 in the usual format.
Although the standard errors of the αs are large, their ordering across the categories strongly suggests that the ‘don’t know’ response is correctly placed in the
middle position on each variable. The π s are more precisely estimated. Had the ‘yes’
response on variable 3 been coded 0, the α̂s would have had the same signs on all
variables and, in retrospect, this would have been more natural. It is clear from the
goodness-of-fit test that the response patterns are satisfactorily explained by a single
latent dimension. The component score may thus be thought of as an index which
ranks employees according to the ease with which they can find secure employment.
A person who ranks high will be someone who has been unemployed, had little
choice of job in the past and no secure future in the present job. A person with the
complementary attributes will be at the opposite end of the scale.
The foregoing examples involved only three manifest variables and one latent
variable. This was sufficient to illustrate the main features of the model, but it falls
short of what is often required in practice. Further examples with two latent variables
and many more manifest variables can be found in Leung (1992). One of these relates
to a survey of secondary school pupils in Hong Kong and uses 29 items on democratic
belief. The responses were on a five-point scale: strongly agree, agree, marginally
agree, disagree and strongly disagree. It is common practice to use equally spaced
scores for such categories and then to analyse the data as though those scores were
continuous variables. Leung treats them as unordered categories and uses the model

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

153

Table 5.3 Observed and expected frequencies and component scores for Leimu’s
(1983) employment data.
Response pattern

Frequency

Expected frequency

Component score

220
120
020
210
110
221
200
010
121
100
021
000
211
111
201
011
101
001

145
54
72
33
22
17
24
14
9
9
11
21
7
6
6
7
2
10

145.3
57.4
67.5
33.6
15.7
16.6
23.1
21.6
8.5
11.9
12.7
17.9
6.9
4.1
6.6
7.1
4.4
8.2

−1.78
−1.46
−1.17
−1.06
−0.74
−0.65
−0.62
−0.44
−0.33
−0.30
−0.04
0.00
0.07
0.39
0.51
0.69
0.83
1.13

Total

469

469.10

χ 2 = 7.55 on 6 degrees of freedom.

Table 5.4 Parameter estimates of the response function model for Leimu’s (1983)
employment data.
Category (s)

α̂i (s)

s.e.

π̂is

s.e.

1

0
1
2

0
−0.30
−0.62

—
0.30
0.27

.28
0.22
0.50

—
0.02
0.03

2

0
1
2

0
−0.44
−1.67

—
0.46
0.61

0.12
0.19
0.69

—
0.02
0.03

3

0
1

0
1.13

—
0.56

0.88
0.12

—
0.04

Variable (i)

P1: TIX/XYZ
JWST070-05

154

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

of Section 5.2 to estimate the category scores. This allows the data to speak for
themselves. The way is now open to use models which are designed for categorical
data rather than, as in the past, to adapt continuous models purely because of the
computational advantages which they offer.
Example 5.12.3 We now turn to an example based on a data set that has been frequently analysed with factor models. It is a US sample of the political action survey.
The data was made available by the Zentralarchiv für Empirische Sozialforschung,
University of Cologne (Barnes and Kaase 1979). The sample consists of 1554 respondents (after listwise deletion) who were asked their opinions on six variables with four
response categories each ranging from ‘agree strongly’ to ‘disagree strongly’. These
six items have been used in many surveys as indicators of the theoretical construct
political efficacy. The variables are:
1. People like me have no say in what the government does.
2. Voting is the only way that people like me can have any say about how the
government runs things.
3. Sometimes politics and government seem so complicated that a person like
me cannot really understand what is going on.
4. I don’t think that public officials care much about what people like me think.
5. Generally speaking, those we elect to Parliament lose touch with the people
pretty quickly.
6. Parties are only interested in people’s votes but not in their opinions.
The six variables have been thoroughly analysed in Jöreskog and Moustaki (2001)
using different factor analysis models for ordinal responses. Using different measures
of fit and goodness of fit statistics, they found that the one-factor solution was not
adequate, and we therefore show here the results for the two-factor model under three
different model specifications – the proportional odds model (5.56), the probit/normal
model obtained from (5.55) with a probit link (inverse of the distribution function
of the normal) and the parameter estimates obtained when univariate and bivariate
information is used (5.73). The first two methods are full maximum likelihood, while
the third is a limited-information estimation method. LISREL (Jöreskog and Sörbom
2006) provides estimates from the maximisation of the sum of the univariate and
bivariate likelihoods. Constraints are imposed to allow for comparisons among the
three model solutions. We fix the second loading of the first observed variable to
zero. Table 5.5 gives the estimated standardised parameters. The three methods show
similar patterns. The loadings for the proportional odds model tend to be higher than
the loadings obtained under the other two methods.
Example 5.12.4 We conclude this chapter with an example based on a survey of 301
university students (204 women and 97 men) on body satisfaction using seven items

P1: TIX/XYZ
JWST070-05

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

POLYTOMOUS DATA: LATENT TRAIT MODELS

155

Table 5.5 Standardised parameter estimates for the logit/normal, the probit/normal
and the underlying bivariate model to political efficacy data.
Logit/normal

Underlying
bivariate

Probit/normal

Item

α̂i1

α̂i2

α̂i1

α̂i2

λ̂i1

λ̂i2

1
2
3
4
5
6

0.88
0.70
0.75
0.85
0.68
0.72

0.00
−0.13
0.14
0.43
0.63
0.61

0.75
0.44
0.65
0.81
0.54
0.60

0.00
−0.03
0.05
0.38
0.62
0.65

0.72
0.46
0.52
0.75
0.57
0.62

0.00
−0.07
0.12
0.38
0.58
0.58

selected from the Body Esteem Scale (Franzoi and Shield 1984). Students were asked
how satisfied they were with certain parts of their bodies – in particular, thighs, body
build, buttocks, hips, legs, figure and weight. Responses were given on a five-point
scale (1 = very dissatisfied; 2 = moderately dissatisfied; 3 = slightly satisfied; 4 =
moderately satisfied; and 5 = very satisfied). The data are analysed again in Chapter
6 using a latent class model. The proportional odds model of (5.56) is fitted to the
seven items. The analysis is mainly exploratory. The AIC and BIC model selection
criteria favoured the two-factor solution (see Table 5.6). The upper triangle of Table
5.7 gives a standardised χ 2 local dependence (LD) index (Chen and Thissen 1997)
for the one-factor solution, while the lower triangle gives the residuals from the twofactor solution. For each pair of variables a sum of χ 2 residuals across all response
categories is calculated in order to produce the LD index. The standardised version
√
of LD produced by the IRTPRO program (Cai et al. 2011) is (χ 2 − d f )/ 2d f ,
where d f = (r − 1)(c − 1) and r and c are the number of categories for the row and
column variables respectively. There are still some positive dependencies that have
not been well explained by the introduction of the second factor. Table 5.8 gives
the unstandardised and standardised loadings. The two-factor solution suggests two
groups of items. Thighs, buttocks, hips and legs seem to be highly correlated with
factor 2, while body build, figure and weight are highly correlated with factor 1.
Although one might have expected the seven items to form a one-dimensional scale
this is not the case. Analysing both the females and males together without accounting
for gender in the analysis is another possible reason for expecting complications with
Table 5.6 AIC and BIC for the body satisfaction data.

AIC
BIC

One-factor

Two-factor

4734.70
4864.45

4608.58
4764.25

P1: TIX/XYZ
JWST070-05

156

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:35

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 5.7 Standardised χ 2 statistic for one-factor model (upper triangle) and twofactor model (lower triangle) for the body satisfaction data.
Item
1
2
3
4
5
6
7

1
2.4
5.2
2.6
1.6
−0.4
1.2

2

3

4

5

6

7

6.2

4.6
4.2

6.2
8.7
6.6

2.0
6.7
3.8
9.3

0.4
13.9
3.0
2.5
1.4

1.4
4.1
1.1
3.5
1.0
4.0

4.7
7.0
4.9
3.2
4.3

6.6
3.8
2.6
0.7

9.5
2.9
3.3

1.6
0.8

2.8

Table 5.8 Unstandardised and standardised parameter estimates with standard errors
in brackets for the proportional odds model for the body satisfaction data.
Item
1
2
3
4
5
6
7

α̂i1

α̂i2

λ̂i1

λ̂i2

2.60 (0.62)
3.52 (0.56)
1.62 (0.27)
2.19 (0.34)
1.83 (0.31)
6.91 (0.39)
1.75 (0.21)

5.34 (0.50)
1.40 (0.37)
2.22 (0.26)
2.90 (0.35)
2.54 (0.30)
2.44 (0.74)
1.14 (0.21)

0.42 (0.15)
0.85 (0.08)
0.50 (0.12)
0.55 (0.12)
0.51 (0.12)
0.92 (0.06)
0.65 (0.09)

0.86 (0.07)
0.34 (0.15)
0.69 (0.09)
0.72 (0.09)
0.71 (0.09)
0.32 (0.16)
0.42 (0.12)

the analysis. Gender could be included in the analysis as a covariate directly affecting
the seven items together with the latent variables. In order to accommodate covariate
effects we need to add the covariates in the right-hand side of (5.56). The latent class
analysis performed in Chapter 6 indicates the need for five classes to explain the
dependencies for the female sample only.

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

6

Latent class models
6.1

Introduction

In Chapter 1 we used a model with two latent classes to illustrate our general approach.
In this chapter we shall extend that model in three principal directions by allowing:
(a) more than two latent classes;
(b) the manifest variables to be polytomous, ordered or unordered;
(c) the manifest variables to be metrical.
The first systematic treatment of latent class models was by Lazarsfeld and Henry
(1968). They tended to emphasise the differences between factor analysis and latent
structure analysis, remarking (p. 6) that ‘Latent structure analysis is in some ways
similar and in many very basic ways different from factor analysis’. This judgement,
which perhaps had greater justification 40 years ago, may have inhibited the development of the common approach on which this book is based. Ten years later Goodman
(1978, Part Four) gave many interesting developments, though the connections with
other latent variable methods tend to be obscured by his choice of notation. Everitt
(1984) gave a good elementary introduction to the topic and since then there has
been a rapid growth in applications. The two volumes of conference proceedings
(Langeheine and Rost 1988; Rost and Langeheine 1997) provide a focal point for
much of this work. Heinen (1996) links latent class models with latent trait models
and with log-linear models (see also Haberman (1979) and Formann (1992)).
It is worth pausing to consider the circumstances under which it might be substantively plausible to treat the latent space as categorical. We might distinguish those
cases where there is prior evidence or theory which leads us to expect there to be latent
categories, from those where it would be practically useful if such a representation
proved to be possible.
Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

157

P1: TIX/XYZ
JWST070-06

158

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

An example of the former is provided by criterion-referenced testing in education.
Here the focus is on whether subjects have acquired the mastery of a concept or
skill. If it were possible to design test items which infallibly discriminated between
‘masters’ and ‘non-masters’ there would be no problem. But if masters sometimes
make mistakes, and non-masters sometimes get items correct by accident or guessing,
it will not be possible to allocate an individual with certainty to either class on the basis
of their responses. To meet this situation, Macready and Dayton (1977) and others
have proposed that allocation be based on a latent class model. Here, theory predicts
that there will be two classes, and it seems reasonable to suppose that members of
any one class will have the same fixed probabilities of getting each item correct. An
example of such a data set is given later in Table 6.3.
An example of the second kind is provided by a study of nearly 50 000 Scottish
births carried out by Pickering and Forbes (1984). Eleven categorical variables were
recorded for each baby: eight were binary, concerning for example whether the baby
had convulsions or jaundice, and three, including birthweight, had more than two
categories. Here, the aim was to identify classes of abnormal birth which, it was
hoped, might show some geographical variation. If this proved to be the case it would
then be possible to target resources more effectively. The expectation was that there
would be one (large) normal class and an unknown number of abnormal classes. The
analysis here was exploratory, in the sense that it was not known in advance whether
classes existed or how many there might be. The ‘mastery’ model, on the other hand,
involves a confirmatory approach, though if we failed to find two classes, we might
go on to see if there were more.

6.2

The latent class model with binary
manifest variables

The extension of the two-class model to a K-class model is almost immediate. If the
model is used in exploratory mode, K will be a parameter to be determined, otherwise
it will be given.
Let πi j be the probability of a positive response on variable i for a person in
category j (i = 1, 2, . . . , p; j = 0, 1, . . . , K − 1) and let η j be the prior probability
 −1
that a randomly chosen individual is in class j ( Kj=0
η j = 1). For the case of K
classes (1.3) becomes
f (x) =

K
−1

j=0

ηj

p


πixji (1 − πi j )1−xi .

(6.1)

i=1

The posterior probability that an individual with response vector x belongs to category
j is thus
h( j | x) = η j

p

i=1

πixji (1 − πi j )1−xi / f (x)

( j = 0, 1, . . . , K − 1).

(6.2)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

159

We can use (6.2) to construct an allocation rule according to which an individual
is placed in the class for which the posterior probability is greatest. The principal
statistical problem is thus the estimation of the parameters and testing goodness
of fit. On the substantive side the main problem is to identify the latent classes –
that is, to interpret them in terms which make practical sense. In this connection
it is worth noting that our model includes the case where there are two or more
cross-classified latent categorical variables. For example, if there were two binary
cross-classified latent categorical variables, their joint distribution would involve four
classes representing the possible combinations of two factors at two levels. If, in such
a case, we wished to require the latent variables to be independent, this would impose
a constraint on the ηs.

6.3

The latent class model for binary data
as a latent trait model

The latent class model is a special case of the latent trait model in which the prior
distribution consists of discrete probability masses. The conditional distribution of
xi given y is identical for both models. Given an individual’s latent location, xi is
assumed to be a Bernoulli random variable. All of the general results which do not
depend on the prior distribution will therefore apply equally to the latent trait and the
latent class model. In particular, the form of the sufficient statistic will be a linear
combination of the xs, as we shall show in Section 6.4 below. Since the two models
differ only in the way they treat the prior distribution, and since the prior has relatively
little influence on the fit of the model, especially when p is large, we may anticipate
that it may not be easy to distinguish empirically between the two families. We can
get further insight into this relationship by considering the correspondence in more
detail, beginning with the case K = 2.
If we start with the logit/normal model with a single latent variable we have
logit πi (y) = αi0 + αi1 y

(i = 1, 2, . . . , p),

(6.3)

where
y has zero mean
√
√ and unit standard deviation. Suppose that y takes the two values
(1 − η)/η and − η/(1 − η) with probabilities η and 1 − η, respectively. It follows
that E(y) = 0 and var(y) = 1 as required. We then have a latent class model with

logit πi0 = αi0 − αi1

η
1−η

(6.4)

1−η
.
η

(6.5)

and

logit πi1 = αi0 + αi1

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

160

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

This differs from the two-class model defined above only in the way we√
designate the
classes and in the parameterisation. For class 1, corresponding to y = (1 − η)/η,
πi1 =



1



1

1 + exp −αi0 − αi1



η
1−η

,

and for class 0,
πi0 =

1 + exp −αi0 + αi1



1−η
η

.

These equations relate the class-specific response probabilities of the latent class
model to the intercept and slope parameters of the latent trait model. If, for example,
we had estimated πi0 , πi1 and η using the theory for the latent class model, we
could express them as latent trait parameters. They will not, of course, be equal to
the estimates of αi0 and αi1 obtained from IRTPRO (Cai et al. 2011) because that
program assumes a normal prior distribution. How close they turn out to be will give
some indication of how sensitive the estimates are to the choice of prior distribution.
We have used this method for the Law School Admission Test data given in
Table 4.2. The results are set out in Table 6.1.
The two sets of estimates are remarkably close, especially in the case of the
αi0 . The first set of estimates uses the normal prior and the second the two-point
distribution appropriate for the latent class model. We can interpret this result either
by saying that the prior distribution has little effect or that it is difficult to distinguish
empirically between the latent trait and latent class models. The reader can confirm
the latter interpretation by using the Latent GOLD program (Vermunt and Magidson
2000) to fit a two-class model to the Law School Admission Test data. The predicted
Table 6.1 Parameter estimates for the two-class model fitted to the Law School
Admission Test VI data together with a comparison of the estimates of {αi0 } and
{αi1 } obtained from (6.4) and (6.6) and from the logit/normal model (from Table 4.1).
Logit/normal
estimates

Latent class
estimates

i

π̂i0

π̂i1

α̂i0

α̂i1

α̂i0

α̂i1

1
2
3
4
5

0.852
0.528
0.301
0.611
0.776

0.965
0.812
0.693
0.849
0.924

2.77
0.99
0.25
1.28
2.05

0.83
0.72
0.89
0.69
0.66

2.75
0.97
0.22
1.27
2.03

0.74
0.64
0.79
0.61
0.59

η̂ = 0.638

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

161

frequencies are as close to the observed, as in the case of the latent trait model. The
χ 2 = 19.42 and the log-likelihood ratio G 2 = 22.76 on 20 degrees of freedom, with
P = 0.49 and 0.30 respectively, indicate a good fit.
There is a second way in which a two-class model can be expressed as a latent
trait model. In this case we retain the standard normal prior but choose the item
response function to be
πi (y) =

πi0
πi1

if y ≤ y0 ,
if y > y0 ,

where πi1 > πi0 (i = 1, 2, . . . , p). All individuals with y ≤ y0 will thus have the
same response probability and the proportion of the population in this class will be
(y0 ) = η, say. The difference between the latent class model formulated in this way
and the logit/normal model is in the form of the response function. This example
illustrates very clearly the intimate relationship between the prior and the response
function. We have two substantively different hypotheses which give rise to the same
model. In one the prior is normal with a step-function as its item response curve.
In the other the prior is a two-point distribution with an arbitrary response function
constrained only by the values it takes at two values of y.
The step-function approach can easily be extended to models with more than
two latent classes by increasing the number of segments into which the latent scale
is divided.

6.4

K latent classes within the GLLVM

We now return to the representation in (6.4) and (6.5) of the two-class model as a
latent trait model and extend it to the general case of K classes. It has to be shown
that the model as defined in Section 6.2 can be written in the form (4.3) for some
q. This was easy to do when K = 2 because we could immediately see how to
construct a single binary variable with zero mean and unit standard deviation so
that the response probabilities coincided. In the general case we proceed as follows.
Let y be an indicator vector showing into which class an individual falls. Thus, for
j =
0, . . . , K − 1, y j = 1 if the individual is in class j and zero otherwise. It follows
that y j = 1. So far as the formal treatment is concerned, we can treat y just like any
vector of latent variables because the general theory places no restrictions whatsoever
on the form of their distribution. This would lead to a GLLVM of the form
logit πi (y) =

K
−1


πi j y j

(i = 1, 2, . . . p).

(6.6)

j=0

The prior distribution h(y) is highly degenerate. For j = 0, . . . , K − 1 it puts probability η j at the point where y j = 1 and all other ys are 0.

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

162

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

This form defines a parameterisation of the latent class model which is essentially
the log-linear representation discussed by Haberman (1979). A discussion of various
parameterisations of this kind is given by Heinen (1996, Chapter 2).

6.5

Maximum likelihood estimation

The log-likelihood function from (6.1) is complicated, but it can be maximised using
standard optimisation routines. This might be done as in McHugh (1956, 1958) using
the standard Newton–Raphson technique. However, as with many other latent variable
models, an easier method which enables larger problems to be tackled is offered by
the E-M algorithm. The standard reference is Dempster et al. (1977), supplemented
by Wu (1983), but a version suitable for the latent class model was also given by
Goodman (1978). Here we shall develop the method from first principles in a manner
which gives some insight into the nature of the estimators.
From (6.1) we find the log-likelihood for a random sample of size n to be

L=

n

h=1

ln

⎧
−1
⎨ K
⎩

ηj

j=0

This has to be maximised subject to
of



p


x

πi ji h (1 − πi j )1−xi h

i=1

⎫
⎬
⎭

.

(6.7)

η j = 1, so we find the unrestrained maximum

φ = L +θ

K
−1


ηj,

j=0

where θ is an undetermined multiplier. Finding partial derivatives, we have

p
n


∂φ
xi h
1−xi h
=
πi j (1 − πi j )
/ f (xh ) + θ
∂η j
h=1 i=1
=

( j = 0, 1, . . . , K − 1)

n

{g(xh | j)/ f (xh )} + θ,

(6.8)

h=1

where g(xh | j) is the joint probability of xh for an individual in class j.
Also,

∂φ
∂
=
ηj
g(xh | j)/ f (xh )
∂πi j
∂π
ij
h=1
n

(i = 1, 2, . . . , p; j = 0, 1, . . . , K − 1).

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

163

Now

∂g(xh | j)
∂
=
exp
{xi h ln πi j + (1 − xi h ) ln(1 − πi j )}
∂πi j
∂πi j
i=1


xi h
(1 − xi h )
= g(xh | j)
−
πi j
(1 − πi j )
= (xi h − πi j )g(xh | j)/πi j (1 − πi j ).
p

(6.9)

Therefore
n

∂φ
= {η j /πi j (1 − πi j )}
(xi h − πi j )g(xh | j)/ f (xh ).
∂πi j
h=1

(6.10)

The resulting equations can be simplified by expressing (6.8) and (6.10) in terms of
the posterior probabilities {h( j | x)}. By Bayes’ theorem,
h( j | xh ) = η j g(xh | j)/ f (xh ).

(6.11)

Substituting in (6.8) and setting equal to zero, we find
n


h( j | xh ) = −θ η j .

h=1

Summing both sides over j and using
estimating equation is
η̂ j =

n




h( j | xh )/n

η j = 1 gives θ = −n, and hence the first

( j = 0, 1, . . . , K − 1).

(6.12)

h=1

The second is
n

(xi h − πi j )h( j | xh )/πi j (1 − πi j ) = 0,
h=1

whence
n
h=1 x i h h( j | xh )
π̂i j = 
n
h=1 h( j | xh )
n

=
xi h h( j | xh )/n η̂ j
h=1

(i = 1, 2, . . . , p; j = 0, 1, . . . , K − 1).

(6.13)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

164

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Although these equations have a simple form it must be remembered that h( j | xh ) is
a complicated function of {η j } and {πi j } given by
ηj

p

h( j | xh ) =  K −1
k=0

x

πi ji h (1 − πi j )1−xi h
.
p
x
η j i=1 πiki h (1 − πik )1−xi h
i=1

(6.14)

However, if h( j | xh ) were known it would be easy to solve (6.12) and (6.13) for {η j }
and {πi j }. The E-M algorithm takes advantage of this fact, proceeding in a ‘zig-zag’
fashion as follows:
1. Choose an initial set of posterior probabilities {h( j | xh )}.
2. Use (6.12) and (6.13) to obtain a first approximation to {η̂ j } and {π̂i j }.
3. Substitute these estimates into (6.14) to obtain improved estimates of
{h( j | xh )}.
4. Return to step 2 to obtain second approximations to the parameters and continue the cycle until convergence is attained.
The solution reached will be a local maximum (or saddle point). It is known that
models of this kind may have multiple maxima and the risk of this appears to increase
as K, the number of classes, increases and to decrease with increasing sample size.
Aitkin et al. (1981) provided an illustration of multiple maxima arising with only
three latent classes. By using different starting values one can guard against the risk
of mistaking a local for a global maximum, but if such multiple maxima do occur it
is not clear what interpretation should be placed on the different sets of latent classes
implied by the various local maxima.
A reasonable way of starting the iteration
is to allocate
individuals, arbitrarily, to

p
–
that
is, to take h( j | xh ) = 1
x
latent classes on the basis of their total score
i
i=1
if xh is allocated to class j and h( j | xh ) = 0 otherwise. Although the method may take
a very large number of iterations to converge, the steps are simple and fast, so the total
computing time is unlikely to be excessive. As well as providing parameter estimates,
the method provides the posterior probabilities that each individual belongs to a given
latent class. It does not provide the second derivatives needed for the calculation of
standard errors, but these can easily be found and evaluated at the solution point, as
we now see. The program Latent GOLD (Vermunt and Magidson 2000) implements
the theory given above.

6.6

Standard errors

 −1
Since the ηs are constrained to sum to one, we eliminate η0 using η0 = 1 − Kj=1
ηj,
and then second derivatives and cross-derivatives of L can be expressed in terms of

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

165

the posterior distribution as follows:
n

∂2 L
=−
{h( j | xh ) − h(0 | xh )}{h(k | xh ) − h(0 | xh )}/η j ηk ,
∂η j ∂ηk
h=1

(6.15)

n

∂2 L
=
(xi h − πi j )(xlh − πlk )h( j | xh )
∂πi j ∂πlk
h=1

× {δ jk (1 − δil ) − h(k | xh )}/πi j (1 − πi j )πlk (1 − πlk ),

(6.16)

where

δr s =

1 if r = s,
0 otherwise,


∂2 L
=
(xi h − πi j )h( j | xh ){δ jk − h(k | xh )}
∂η j ∂πik
h=1
n

( j, k = 0, 1, 2, . . . , K − 1; i, l = 1, 2, . . . , p).

(6.17)

The asymptotic variance–covariance matrix of the estimates is then the inverse of the
expectation of the (K ( p + 1) − 1) × (K ( p + 1) − 1) matrix of the negatives of the
derivatives set out above. The exact computation of the expected values involves
summation over the 2 p possible score patterns x. This is feasible if p is small, but with
large p the number of terms becomes extremely large and the magnitude of each term
so small that accurate calculation becomes impossible. In this case the expectation
can be approximated by taking the inverse of the observed second derivative matrix.
It was shown by de Menezes (1999) that the asymptotic standard errors may be
very poor approximations for sample sizes which are commonly encountered. This
arises especially, but not exclusively, when the parameter estimates are close to the
boundary values of 0 and 1. For this reason commercial software for latent class
analysis usually provides the facility for estimating standard errors by the parametric
bootstrap method. An example is given in Table 6.2 for the Law School Admission
Test data where the asymptotic standard errors are compared with estimates based on
5000 bootstrap samples.
The asymptotic standard error of η̂ is 0.141 and the bootstrap estimate is 0.129.
In this example the asymptotic values underestimate the true values for most of the
parameters as estimated by the bootstrap. This discrepancy may be partly due to the
skewness of the sampling distribution which gives rise to a few extreme estimates
but, as de Menezes showed, the position is far from clear and caution needs to
be exercised.

P1: TIX/XYZ
JWST070-06

166

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 6.2 Comparison of asymptotic standard errors and bootstrap estimates for the
Law School Admission Test data (5000 replications).
π̂i0

π̂i1

i

Asymptotic

Bootstrap

Asymptotic

Bootstrap

1
2
3
4
5

0.039
0.068
0.092
0.063
0.044

0.043
0.084
0.093
0.071
0.051

0.015
0.043
0.053
0.035
0.024

0.016
0.041
0.058
0.035
0.023

6.7

Posterior analysis of the latent class model with
binary manifest variables

Posterior analysis in the case of a latent class model is concerned with what we
can say about an individual’s class membership after their response pattern has been
observed. All the information relevant to this question is contained in the posterior
probability distribution h( j | x). The link with the latent trait model is made more
explicit by noting that
E(y | x) = (h(0 | x), h(1 | x), . . . , h(K − 1 | x)).
In practice the class to which an individual is most likely to belong could be discovered
by inspection of this set of conditional posterior probabilities. Equivalently we can
do it by reference to the components. Thus, class j is more likely than class k if
h( j | x)/ h(k | x) > 1, which is true if

(η j /ηk ) exp

p



xi ln πi j + (1 − xi ) ln(1 − πi j )



i=1

− {xi ln πik + (1 − xi ) ln(1 − πik )}] > 1.
This inequality is equivalent to
p


xi logit πi j +

i=1

+

p


ln(1 − πi j ) + ln η j >

i=1
p

i=1

ln(1 − πik ) + ln ηk .

p


xi logit πik

i=1

(6.18)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

167

When K = 2 this reduces to the inequality of (1.5). The first term on each side
of the inequality is simply a component, but Theorem 2.15.1 is not applicable to
comparisons across latent variables.

6.8

Goodness of fit

There is very little to be added to what was said in Section 4.9 for the latent trait
model. All of the methods described there apply equally to the latent class model.
In fact some of the relevant research quoted there was directed towards the latent
class model. The calculation of the number of degrees of freedom for a model with
K classes is
d f = 2 p − 1 − pK (response probabilities) − (K − 1)
= 2 p − K ( p + 1).

6.9

Examples for binary data

Example 6.9.1 Having fitted a model we will normally wish to consider what can
be learned from the classification at which we have arrived. In the case of Macready
and Dayton’s mastery model we would expect to find two classes with one having
the characteristics we would expect of ‘masters’ and the other of ‘non-masters’. The
results of the analysis for the example from Macready and Dayton (1977) are given
in Tables 6.3 and 6.4.
It is immediately clear from Table 6.3 that the fit is extremely good. The χ 2 = 9.79
and the G 2 = 9.08 on 6 degrees of freedom, with P = 0.13 and 0.17 respectively,
also indicate a very good fit. The posterior analysis shown in the last column reveals
an interesting distribution of probabilities. For many response patterns, for example
1111, 1011 and 0110, we would be extremely confident in allocating individuals
to the ‘master’ category. Similarly, we would have little hesitation in classifying
someone responding 0000 as a non-master, and scarcely more if it was 1000. There
are several response patterns, however, which are ambiguous. These are 0100, 0010
and 0001, where the posterior probability is close to a 12 . There are only 11 individuals
in this situation out of 142, so the method has been successful in allocating the great
majority of individuals to their respective groups. A little further light can be shed on
these conclusions by inspecting the parameter estimates in Table 6.4.
Members of class 0 have a small estimated probability of responding correctly to
any item; these are clearly the ‘non-masters’. Those in class 1 have, in each case, a
much higher probability of answering correctly; these are the ‘masters’. The standard
errors are quite large, but not sufficiently so to put this interpretation at risk. The
uncertainty about those who get only one item correct arises from the need to balance
the very small probability of a non-master getting that item correct, against the high
probability that a non-master would get the other three wrong. In other words, such
a response pattern shows a somewhat inconsistent performance, and it is therefore

P1: TIX/XYZ
JWST070-06

168

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 6.3 Macready and Dayton’s (1977) data with posterior probabilities of belonging to the mastery state.
Response pattern
1111
1110
1101
1100
1011
1010
1001
1000
0111
0110
0101
0100
0011
0010
0001
0000

Frequency

Expected frequency

Pr{master | x}

15
7
23
7
1
3
6
13
4
2
5
6
4
1
4
41

15.0
6.1
19.7
8.9
4.1
1.9
6.1
13.1
4.8
2.0
6.5
5.9
1.4
1.6
4.4
41.3

1.00
1.00
1.00
0.90
1.00
0.85
0.87
0.16
1.00
0.95
0.96
0.43
0.95
0.33
0.38
0.02

142

142

reassuring that there are so few responses of that kind. The response pattern 1000
is less surprising because, with a probability of 0.22, it is much more likely that a
non-master will get item 1 correct.
Example 6.9.2 For our second example we turn to a case, discussed in de Menezes
and Bartholomew (1996), where there was no prior theory to suggest whether there
might be any latent classes at all or, if there were, how many. Fuller details are given
in the above paper, including a discussion of how the data were reduced to binary
form. The data are taken from the British Social Attitudes survey 1990 and relate to

Table 6.4 Parameter estimates for the fit of Macready and Dayton’s (1977) model
to the data of Table 6.3 (standard errors in brackets).
j

π̂1 j

π̂2 j

π̂3 j

π̂4 j

η̂

0

0.22
(0.07)
0.76
(0.06)

0.08
(0.06)
0.79
(0.06)

0.03
(0.04)
0.43
(0.06)

0.06
(0.05)
0.71
(0.06)

0.43
(0.06)
0.57
(0.06)

1

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

169

1077 respondents who provided views on the following ten issues relating to sexual
attitudes:
1. Should divorce be easier here?
2. Do you support the law against sexual discrimination?
3. View on pre-marital sex: not at all wrong, . . . , always wrong.
4. View on extra-marital sex: not at all wrong, . . . , always wrong.
5. View on sexual relationship between individuals of the same sex: not at all
wrong, . . . , always wrong.
6. Should gays teach in schools?
7. Should gays teach in higher education?
8. Should gays hold public positions?
9. Should a female homosexual couple be allowed to adopt children?
10. Should a male homosexual couple be allowed to adopt children?
The parameter estimates are given in Table 6.5 for a two-, three- and four-class model.
The fits in the first two cases were poor, but they are included to show how
the latent class structure evolves as further classes are added. The two-class model
produces a pattern not dissimilar in some respects to that in the ‘mastery’ model.
Members of the second class have a higher probability of responding positively to
every item than those of the first class. This suggests a permissive/non-permissive
dichotomy in the population into groups of roughly equal size.
Table 6.5 Parameter estimates π̂i j for the British Social Attitudes 1990 survey.
Item

Two classes

Three classes

Four classes

1
2
3
4
5
6
7
8
9
10

0.14
0.76
0.64
0.09
0.08
0.01
0.08
0.23
0.07
0.02

0.18
0.88
0.88
0.17
0.49
0.92
0.99
0.94
0.30
0.19

0.13
0.76
0.64
0.09
0.08
0.01
0.06
0.21
0.07
0.03

0.09
0.87
0.86
0.13
0.38
0.87
0.98
0.91
0.10
0.00

0.21
0.92
0.96
0.30
0.82
0.97
1.00
0.99
0.98
0.83

0.14
0.77
0.63
0.08
0.07
0.01
0.06
0.21
0.04
0.00

0.07
0.60
0.87
0.27
0.59
0.00
0.18
0.30
0.99
0.91

0.10
0.87
0.87
0.13
0.38
0.87
0.98
0.91
0.10
0.00

0.21
0.93
0.96
0.31
0.83
1.00
1.00
1.00
0.98
0.84

η̂ j

0.49

0.51

0.47

0.41

0.11

0.46

0.02

0.42

0.10

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

170

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 6.6 Two-class model fitted to items 5–10
in the British Social Attitudes 1990 survey.
Item

π̂i1

π̂i2

5
6
7
8
9
10

0.08
0.01
0.09
0.23
0.07
0.02

0.49
0.92
0.99
0.94
0.30
0.19

η̂ j

0.49

0.51

When we add a third class, the new first class is almost the same in size and
response probabilities as before, but the second has split into two parts. These differ
most strikingly on the last two items concerning adoption. The third group, comprising
11% of the population, are permissive on all issues. The middle group are only
marginally less permissive on the first eight items, but are clearly distinguished by
their negative views on adoption.
The final and best-fitting model has four latent classes. The non-permissive class
remains almost unchanged, but more subtle differences emerge among the permissives. The third class is almost identical to the second class of the previous model; its
members are permissive on everything except adoption. The new element is a very
small group, comprising 1% of the population, whose distinguishing feature is very
positive support for homosexual adoption, combined with a strong negative attitude
to items 6, 7 and 8, concerning teaching and holding public positions.
This provides a coherent and plausible interpretation of the situation but the
increasing uncertainty of the parameter estimates as the number of classes increases
should be borne in mind.
In a further attempt to confirm these conclusions, de Menezes and Bartholomew
(1996), repeated the analysis using items 5–10 only: the results are given in Table 6.6.
These appeared to be the items on which views were most sharply divided. This
produced a result which was identical to the relevant part of the two-class analysis
for the full set of items, though the fit, in this case, was much better.

6.10

Latent class models with unordered polytomous
manifest variables

The extension of the foregoing theory to polytomous data is straightforward, though
applications have been less common. Some theory of estimation was given by Goodman (1978), and we shall describe applications by Pickering and Forbes (1984) and
Clogg (1979), the latter reanalysed by Masters (1985).

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

171

When there are more than two categories the indicator variable xi used previously
is replaced by a vector xi with ci elements xi (s) defined by

1
xi (s) =
0
Note that


s

if the response is in category s of variable i,
otherwise (s = 0, 1, . . . , ci − 1).

xi (s) = 1. The complete response vector for an individual is then written
x = (x1 , x2 , . . . , xp ).

The conditional response probabilities are defined by
πi j (s) = P{Response of an individual in class j is in category s on variable i}.
The joint probability function of x is then
f (x) =

K
−1

j=0

ηj

p ci −1



{πi j (s)}xi (s) .

(6.19)

i=1 s=0

The posterior distribution is
h( j | x) = η j

p ci −1



{πi j (s)}xi (s) / f (x).

(6.20)

i=1 s=0

The model for polytomous manifest variables can be reparameterised in the loglinear form in exactly the same way as the model for binary data. In place of the logit
relationship given in (6.6) we need the multivariate logits given in (5.10) and (5.15).
On the right-hand sides of the latter equations we replace the latent vector y by the
indicator vector z showing into which class the individual falls.

6.11

Latent class models with ordered polytomous
manifest variables

We have seen that a latent class model is, essentially, a latent variable model whose
prior distribution consists of probability masses on a finite set of points. From this it
follows that methods for dealing with ordered categories already discussed in Chapter
5 for that model can be used here also. As already defined in Section 6.10, πi j (s) is
the conditional probability of a response of an individual in category s on variable
i given the latent class j. To preserve the ordinality property of the variable xi , we
model the cumulative probability of responding in category s or above on variable i
for a member of class j,
i j (s)

= πi j (s) + πi j (s + 1) + · · · + πi j (ci − 1)

(i = 1, . . . , p; j = 1, . . . , K ).

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

172

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

One would then estimate the i j (s) from which the πi j (s) could be obtained by
differencing. The joint probability function of x is then

f (x) =

K
−1

j=0

ηj

c
i −1

πi j (s)

xi (s)

=

K
−1


s=0

ηj

j=0

c
i −1

{

i j (s)

−

i j (s

+ 1)}xi (s) ,

(6.21)

s=0

where xi (s) = 1 if a randomly selected object belongs into category s of the ith
variable and xi (s) = 0 otherwise.

6.12

Maximum likelihood estimation

As with binary data, the log-likelihood may be written
L=

n


ln f (xh ),

h=1

but the maximisation
must now be effected under two sets of constraints. The original

constraint η j = 1 remains, but we must now impose
c
i −1

πi j (s) = 1 (i = 1, 2, . . . , p).

(6.22)

s=0

This did not arise in the binary case because we used (6.22) to eliminate one of the
two probabilities for each dimension. The function now to be maximised is thus

φ = L +θ

K
−1


ηj +

j=0

p
K
−1 


βi j

j=0 i=1

c
i −1

πi j (s),

(6.23)

s=0

where θ and {βi j } are undetermined multipliers. The partial derivatives with respect
to {η j } are

∂φ
=
g(xh | j)/ f (xh ) + θ
∂η j
h=1

(6.24)

n
1
η̂ j =
h( j | xh ).
n h=1

(6.25)

n

leading, as before, to

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

173

Similarly,
 ∂g(xh | j)
∂L
=
ηj
∂πi j (s)
∂πi j (s)
h=1
n


f (xh ).

Now
p ci −1


∂g(xh | j)
∂
xi h (s) ln πi j (s)
=
exp
∂πi j (s)
∂πi j (s)
i=1 s=0

= g(xh | j)xi h (s)/πi j (s),

(6.26)

giving
 g(xh | j) xi h (s)
∂φ
= ηj
+ βi j
∂πi j (s)
f (xh ) πi j (s)
h=1
n

=

n


h( j | xh )xi h (s)/πi j (s) + βi j .

(6.27)

h=1

Setting the right-hand side equal to zero yields
0=

n


h( j | xh )xi h (s) + πi j (s)βi j .

(6.28)

h=1

Summing both sides over s,
0=

n


h( j | xh ) + βi j

h=1

or

βi j = −

n


h( j | xh ).

h=1

Substitution into (6.28) finally gives the estimating equations
n
h( j | xh )xi h (s)
n
π̂i j (s) = h=1
h=1 h( j | xh )
n
=

h=1

h( j | xh )xi h (s)
(i = 1, 2, . . . , p; j = 0, 1, . . . , K − 1; s = 0, . . . , ci − 1).
n η̂ j
(6.29)

In the ordered polytomous case (6.29) can be also written
n

xi h(s) h( j | xh )
n η̂ j
(i = 1, 2, . . . , p; j = 0, 1, . . . , K − 1; s = 0, . . . , ci − 1).

ˆ i j(s) = ˆ i j(s+1) +

h=1

(6.30)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

174

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Equations (6.25) and (6.29) or (6.30) may then be solved as in the binary E-M
algorithm. We first choose starting values for {ĥ( j | xh )} and then obtain first
approximations to {η̂ j } and {π̂i j (s)}. These are then used to improve the estimates of
{ĥ( j | xh )} and so on.

6.12.1

Allocation of individuals to latent classes

Reference to (6.20) shows that
h( j | xh )/ h(k | xh ) = (η j /ηk ) exp

p ci −1



xi (s) ln πi j (s)/πik (s).

i=1 s=0

Discrimination is thus based on comparing the linear functions
Xj =

p ci −1



xi (s) ln πi j (s)

( j = 0, 1, . . . , K − 1).

i=1 s=0

Thus class j is preferred to class k if
X j − X k > ln(ηk /η j ).

(6.31)

This implies that we allocate to the class for which
X j + ln η j
is greatest.

6.13

Examples for unordered polytomous data

Example 6.13.1 We return to the classification of Scottish infants investigated by
Pickering and Forbes (1984). This involved an unusually large sample size (45 426
usable cases relating to the year 1980) which permitted some interesting additions
to the usual analysis. There were 11 variables, of which 8 were binary; they are
listed in Table 6.7, which also gives the parameter estimates obtained using the E-M
algorithm.
It is only necessary to give the estimates for ci − 1 categories of each variable
because the response probabilities for each variable sum to 1. Although the value of
χ 2 continues to decrease as the number of classes fitted increases, multiple maxima
began to appear when four classes were fitted. The decision to present results up
to the four-class model was based on the interpretability of the results rather than
on statistical criteria. However, with such a large sample it was possible to test the
stability of the results by dividing the sample in half at random and refitting the
model to each sub-sample. The authors showed that the parameter estimates obtained

2
2
2
2
2
3

η̂ j
0.92

0.08

0.06
0.58
0.03
0.30
0.05
0.34
0.61

0.12
0.19
0.17
0.10

0.92

0.00
0.28
0.00
0.01
0.00
0.83
0.03

0.01
0.08
0.02
0.00

0.03

0.17
0.67
0.07
0.60
0.13
0.09
0.79

0.26
0.26
0.33
0.25

0.18
0.26
0.21
0.19

0.05

0.00
0.49
0.00
0.10
0.00
0.53
0.45

0.01
0.13
0.04
0.00

0.79
0.09
0.01
0.62

0.89

0.00
0.28
0.00
0.01
0.00
0.83
0.03

0.00
0.07
0.01
0.00

0.00
0.00
0.00
0.07

0.03

0.20
0.71
0.07
0.67
0.15
0.04
0.82

0.21
0.25
0.29
0.29

0.20
0.32
0.25
0.21

0.05

0.00
0.49
0.00
0.10
0.00
0.50
0.46

0.01
0.13
0.03
0.00

0.78
0.09
0.01
0.62

0.04

0.00
0.32
0.01
0.05
0.00
0.84
0.14

0.32
0.31
0.52
0.01

0.03
0.00
0.00
0.10

12:39

0.00
0.28
0.00
0.01
0.00
0.84
0.03

present
Present
Present
Present
Present
4–10 days
>11 days

2
3

0.00
0.00
0.00
0.07

4 Classes

May 10, 2011

2

0.01
0.08
0.02
0.00

<7
Intermediate
By intubation
Present

2

Birthweight
for Gestation age
Apgar at 5 min
Resuscitation

0.48
0.15
0.08
0.43

3 Classes

P2: ABC
JWST070-Bartholomew

Assisted ventilation
after 30 mins
Recurrent apnoea
Jaundice
Convulsions
In tube feeding
Dead at discharge
Age at discharge

0.01
0.00
0.00
0.07

2001–2500 g
1501–2000 g
≤1500 g
< 10 th percentile

4

2 Classes

Birthweight

Levels

ci

Variable

Table 6.7 Parameter estimates (π̂s) for the Pickering and Forbes (1984) data.

P1: TIX/XYZ
JWST070-06
Printer Name: Yet to Come

LATENT CLASS MODELS
175

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

176

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

were virtually the same. The four-class solution distinguishes between a large class
of normal healthy infants (class I) and three additional classes of infants whose births
are abnormal in some way. Two classes (III and IV) represent moderately ill infants
requiring special neonatal care and the third (II) represents severely ill infants with
low birthweight.
Using these results one can predict the prevalence of each class and study their
geographical distribution.
Example 6.13.2 The second example is taken from Clogg (1979) who fitted a threeclass latent variable model to data from the 1975 US General Household Survey with
a sample size of 1472. The data were reanalysed by Masters (1985) using a latent
trait model.
The data concern three questions about degree of satisfaction with family (F),
hobbies (H) and residence (R). Answers to each question were classified as low,
medium or high. It is reasonable to suppose that if individuals tend to give the same
reply to each question then the distribution might be explained by a model with three
classes, and this is what Clogg fitted. The method used was maximum likelihood
using Goodman’s E-M routine. The parameter estimates are given in Table 6.8 and
the observed and expected frequencies in Table 6.9.
The goodness of fit may be judged by the value of χ 2 = 2.36 which, with 27 −
18 − 2 − 1 = 6 degrees of freedom, indicates an extremely good fit.
We note from the η̂s in Table 6.8 that the first class is very small, but the pattern
of the π s confirms our expectation that there might be a tendency to give similar
answers to all questions. Thus those in class I tend to have low satisfaction, those in
class II somewhat higher satisfaction and those in class III highest of all. Indeed, the
choice of questions with three alternatives may well have divided the respondents
into three classes by requiring them to think about life satisfaction in this way. To
Table 6.8 Parameter estimates (π̂s) of three-class latent variable models for life
satisfaction data.
Class
Family

Hobbies

Residence

I

II

III

low
medium
high
low
medium
high
low
medium
high

0.48
0.28
0.24
0.85
0.00
0.15
0.53
0.31
0.16

0.06
0.36
0.58
0.18
0.50
0.32
0.21
0.53
0.26

0.03
0.02
0.95
0.07
0.14
0.79
0.06
0.22
0.72

η̂ j

0.04

0.41

0.55

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

177

Table 6.9 Observed and expected frequencies for life satisfaction data (n = 1472).
Response
pattern
F

H

R

Observed
frequency

Expected
frequency

0
0
0
1
1
1
2
2
2
0
0
0
1
1
1

0
0
0
0
0
0
0
0
0
1
1
1
1
1
1

0
1
2
0
1
2
0
1
2
0
1
2
0
1
2

15
11
7
16
26
12
23
49
54
3
12
5
23
58
31

14.1
11.3
6.8
15.6
25.3
13.2
22.8
48.9
57.0
4.0
10.3
7.2
23.0
58.1
29.9

Response
pattern
F

H

R

Observed
frequency

Expected
frequency

2
2
2
0
0
0
1
1
1
2
2
2

1
1
1
2
2
2
2
2
2
2
2
2

0
1
2
0
1
2
0
1
2
0
1
2

45
117
126
5
14
16
18
38
27
64
191
466

43.3
116.4
123.0
5.7
11.7
17.5
16.6
40.4
27.8
61.2
193.6
466.9

1472

1472.0

Total

that extent the excellent fit may be, in part, an artefact, even though responses to all
questions reflect a basic degree of satisfaction.
Individuals may be allocated to classes using the estimated parameters and the
allocation rule of Section 6.12.1. The result is that those with response patterns 000,
001 and 002 go into class I, those with response patterns 222, 221, 220, 212, 202
and 022 into class III, and the remainder into class II. Membership of class I is thus
determined by low satisfaction with family and hobbies; the make-up of class III is
less transparent, but high satisfaction in at least two areas is the main qualification.
Example 6.13.3 In this third example we return to the survey of 301 university
students (204 female and 97 men) on body satisfaction using seven items selected
from the Body Esteem Scale (Franzoi and Shield 1984) discussed in Example 5.12.4.
Students were asked how satisfied they were with certain parts of their bodies – in
particular, thighs, body build, buttocks, hips, legs, figure and weight. Responses were
given on a five-point scale (1 = very dissatisfied; 2 = moderately dissatisfied; 3 =
slightly satisfied; 4 = moderately satisfied; and 5 = very satisfied). We present here
the analysis only for the female part of the sample. Table 6.10 gives the BIC for five
different models. According to the BIC, the five-class model is marginally preferred
to the six-class model.
Table 6.11 gives the estimated category probabilities for the five-class solution
obtained with Latent GOLD (Vermunt and Magidson 2000). Nearly 9% of the female
students are dissatisfied with all seven parts of their body (class 4), followed by 19.6%

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

178

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 6.10 Body satisfaction: log-likelihood, BIC and number of parameters.
Model

Log-likelihood

BIC

Number of parameters

2-Class
3-Class
4-Class
5-Class
6-Class

−1747.5965
−1643.7688
−1593.8066
−1555.1662
−1534.3650

3686.6454
3521.5349
3464.1554
3429.4195
3430.3622

36
44
52
60
68

of students in class 3 who are mainly dissatisfied but some slightly satisfied. Only
5.8% are allocated in class 5, which shows high satisfaction with all parts of the body,
followed by 33% in class 1 with students who are at least slightly satisfied. Finally,
33% are students in the middle range of the satisfaction scale (class 2). Attention
should be drawn to the extreme sparseness of the data due to the large number of
response categories and the relatively small sample size. Model selection criteria
favoured the five-class solution over less classes, but a more thorough investigation
of residuals is needed to identify possible outliers. The seven items have been also
analysed using marginal models that look not at the correlations among the seven
items but on the marginal proportions (Bergsma et al. 2009).

6.14

Identifiability

As with all models involving many parameters, there is the possibility that the latent
class model may not be identifiable. That is, there may be more than one point in the
parameter space which yields the same likelihood. We can approach this question by
probability
thinking of the data set out in a c1 × c2 × · · · × c p contingency table. The
p
for the cell designated by x is then given by (6.19). There are only i=1 ci − 1
independent probabilities because of the requirement that they sum to 1. These cell
probabilities are functions of the model parameters which appear on the right-hand
side of (6.19). If there are more model parameters than there are independent cell
probabilities then, clearly, there will be many sets of model parameters leading to
the same set of f (x)s and hence to the same likelihood. The number of parameters
in the model is
K


p

i=1

(ci − 1) + K 
− 1 .





πs

ηs

The model will, therefore, certainly be unidentifiable if
p

i=1

p

ci − 1 < K
(ci − 1) + K − 1.

(6.32)

i=1

However, this condition is not sufficient because the parameters are probabilities and
thus are subject to constraints on their ranges.

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

Table 6.11 Body satisfaction: estimated category and class probabilities.
Item
Thighs
1
2
3
4
5
Body build
1
2
3
4
5
Buttocks
1
2
3
4
5
Hips
1
2
3
4
5
Legs
1
2
3
4
5
Figure
1
2
3
4
5
Weight
1
2
3
4
5
η̂ j

Class 1

Class 2

Class 3

Class 4

Class 5

0.0000
0.0157
0.2883
0.6769
0.0191

0.0088
0.3815
0.5191
0.0904
0.0002

0.1422
0.7355
0.1197
0.0025
0.0000

0.8251
0.1737
0.0012
0.0000
0.0000

0.0000
0.0000
0.0066
0.5135
0.4799

0.0000
0.0095
0.1704
0.6902
0.1299

0.0001
0.0546
0.3627
0.5446
0.0380

0.0101
0.5189
0.4005
0.0698
0.0006

0.3690
0.6154
0.0155
0.0001
0.0000

0.0000
0.0000
0.0005
0.1011
0.8984

0.0003
0.0370
0.3341
0.5240
0.1045

0.0128
0.2846
0.5264
0.1692
0.0069

0.1331
0.6143
0.2366
0.0158
0.0001

0.7734
0.2213
0.0053
0.0000
0.0000

0.0000
0.0014
0.0624
0.4755
0.4606

0.0000
0.0166
0.2805
0.6162
0.0868

0.0017
0.1654
0.5702
0.2554
0.0073

0.1049
0.7184
0.1714
0.0053
0.0000

0.8794
0.1200
0.0006
0.0000
0.0000

0.0000
0.0001
0.0118
0.3439
0.6443

0.0001
0.0253
0.2396
0.6131
0.1219

0.0090
0.3410
0.4700
0.1750
0.0051

0.0391
0.5891
0.3233
0.0479
0.0006

0.8309
0.1678
0.0012
0.0000
0.0000

0.0000
0.0010
0.0429
0.5013
0.4547

0.0000
0.0043
0.1251
0.8102
0.0604

0.0012
0.0815
0.4241
0.4869
0.0064

0.0538
0.5169
0.3706
0.0586
0.0001

0.5476
0.4272
0.0249
0.0003
0.0000

0.0000
0.0000
0.0000
0.0321
0.9679

0.0011
0.0239
0.2021
0.5372
0.2358

0.0236
0.1600
0.4211
0.3479
0.0475

0.1682
0.3864
0.3445
0.0964
0.0045

0.4275
0.4055
0.1493
0.0173
0.0003

0.0001
0.0033
0.0687
0.4472
0.4808

0.3316

0.3269

0.1959

0.0875

0.0581

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

180

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

There appear to be no known global conditions for identifiability, but Goodman
(1974) was able to make progress with local identifiability. This means that in a small
neighbourhood the maximum of the likelihood is unique. Goodman’s condition is
based on the fact that the maximum is unique if the transformation from the cell
probabilities to the parameters is one-to-one in a small neighbourhood. This can
be tested by finding the Jacobian of the transformation. The matrix
 p of the partial
derivatives
of
the
f
(x)s
with
respect
to
the
model
parameters
has
i=1 ci − 1 rows
p
and K { i=1 (ci − 1) + 1} columns. The model is locally identifiable if the rank of
the Jacobian is equal to the number of columns.
In practice the questions of identifiability and precision become intertwined.
Roughly speaking, unidentifiability is a limiting case of decreasing precision of
estimation. High asymptotic errors imply that the likelihood is fairly flat in the neighbourhood of the maximum – unidentifiability that it is precisely flat. Unidentifiability
is therefore unlikely to be a problem if we restrict ourselves to models which can be
estimated with reasonable precision. In our experience this means models with no
more than three or four classes.

6.15

Starting values

The initial allocation of objects into classes can be done both randomly and by using
hierarchical cluster analysis or k-means cluster analysis. Many different starting values need to be used before choosing a final model to avoid the problem of likelihood
convergence into a local maximum. That phenomenon often occurs in latent class
models (finite mixture models) especially when the number of variables is large and
the sample size is small (Wedel and DeSarbo 1995). Searching for a global maximum
could increase the computational time needed to estimate a latent class model significantly. Methodology that allows the estimation of the number of classes together
with the model parameters is desirable to avoid refitting the model too many times.

6.16

Latent class models with metrical
manifest variables

At the beginning of Section 1.3.2 we gave an example of a latent class model with
metrical data and we noted that the use of such a model would require the choice
of a suitable form for the conditional distribution of {xi }. When dealing with binary
or polytomous data the binomial and multinomial distributions were the obvious
choice, but with metrical data there may be some difficulty in knowing what to
assume. Some information can be gleaned from the marginal distributions, as we
note below. In this section we are concerned with a model where the joint distribution
of the xs has the form
f (x) =

K
−1

j=0

ηj

p

i=1

gi (xi | j)

(6.33)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

181

for some K, where gi (xi | j) is the conditional distribution of xi for members of class
j. Such models have been termed latent profile models.
In the absence of information on the form of gi (xi | j), some guidance may be
obtained from an inspection of the marginal distributions given by
f (xi ) =

K
−1


η j gi (xi | j)

(i = 1, 2, . . . , p).

(6.34)

j=0

A bimodal distribution would suggest a two-class normal mixture, whereas an extremely skew distribution would argue against a normal mixture of any number of
components. At best we are only likely to obtain very crude information in this way,
but it should be possible to avoid grossly inappropriate assumptions. In favourable
circumstances it may be possible to use the marginal distributions by methods given
in Everitt and Hand (1981), but this would not solve our present problem unless the ηs
were constrained to be the same for each margin. We shall describe two approaches
to fitting the model of (6.33), the second of which throws some interesting light on
the interpretation of covariance structures.

6.16.1

Maximum likelihood estimation

The same approach can be used as with categorical manifest variables, though the
details depend on our choice of {gi (xi | j)}. The equations obtained by setting the
derivatives of L with respect to η equal to zero are the same for all choices including
the distributions used for categorical data and thus are
η̂ j =

n


h( j | xh )/n

( j = 0, 1, . . . , K − 1).

(6.35)

h=1

Suppose we now take
gi (xi | j) = g(xi | θi j );
then
L=

n

h=1

ln

K
−1

j=0

ηj

p


g(xi h | θi j )

(6.36)

i=1

and
 ηj 
∂ log g(xi h | θi j )
∂L
=
g(xr h | θr j )
.
∂θi j
f
(x
)
∂θi j
h r =1
h=1
n

p

(6.37)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

182

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Setting these derivatives equal to zero and solving for {θi j }, we shall have equations
of the form
θi j = ψ(xh , η j )

(i = 1, 2, . . . , p; j = 0, 1, . . . , K − 1).

(6.38)

The E-M procedure can then be used, as before, by alternating between (6.35) and
(6.38). The second derivatives can be used to obtain asymptotic standard errors.
Equations (6.38) take a particularly simple form if g(xi | θi j ) is a member of the
exponential family and especially if it is normal with mean θi j and unit variance. We
then find
∂ log g(xi h | θi j )
= xi h − θi j
∂θi j
and hence that
n


h( j | xh )(xi h − θi j ) = 0

h=1

or
n
h=1 x i h h( j | xh )
θ̂i j = 
.
n
h=1 h( j | xh )

(6.39)

Equations (6.39) and (6.35) then lend themselves to a straightforward application of
the E-M algorithm.
Furthermore, if one assumes that g(xi | θi j , σi2 ) is normal with mean θi j and
variance σi2 the same in all classes then

σ̂i2 =

n K
−1



(xi h − θ̂i j )2 h( j | xh )/n.

h=1 j=0

6.16.2

Other methods

The original method of estimation proposed for the latent profile model by Lazarsfeld
and Henry (1968) involved fitting by the method of moments. In this case one writes
down sufficient moment, and cross-moment, equations to determine the unknown
parameters. As a method of estimation this has been superseded by the maximum
likelihood approach, but it brings out an important link with the factor analysis model
which is not otherwise evident.

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

183

Let μi ( j) be the mean of xi for members of latent class j and let σi2 ( j) be its
variance. Then
E(xi ) =

K
−1



ηj


···

xi

j=0

E(xi2 ) =

K
−1


p


gi (xi | j) dx =

i=1

K
−1


η j μi ( j),

j=0

η j {σi2 ( j) + μi2 ( j)},

j=0

E(xi xk ) =

K
−1


η j μi ( j)μk ( j) (i, k = 1, 2, . . . , p; i = j).

(6.40)

j=0

We then have
var(xi ) =

K
−1


η j σi2 ( j)

j=0

cov(xi , xk ) =

K
−1


+

K
−1

j=0

η j μi ( j)μk ( j) −

j=0

=

K
−1


η j {μi ( j) − μ̄i }2
⎧
−1
⎨ K
⎩

η j μi ( j)

j=0

η j {μi ( j) − μ̄i }{μk ( j) − μ̄k }

(i = 1, 2, . . . , p),

⎫2
⎬
⎭
(i, k = 1, 2, . . . , p; i = k),

j=0

(6.41)
where μ̄i =

 K −1
j=0

η j μi ( j). The covariance matrix may thus be written
cov(x) = LL + ,

where  is now a diagonal matrix with (i, i)th element
element lij of L is given by
li j =

√
η j {μi ( j) − μ̄i }.

(6.42)
 K −1
j=0

η j σi2 ( j) and the
(6.43)

Thus cov(x) is of exactly the same form as the covariance matrix for the linear
factor model with a normally distributed latent variable, but there is one important
difference. The columns of L are linearly dependent because
K
−1


√
η j li j = 0

(i = 1, 2, . . . , p)

j=0

from the definition of μ̄i . For this reason any attempt to estimate L and  by fitting
the sample covariance matrix to (6.42) by standard methods (for which see Chapter 3)

P1: TIX/XYZ
JWST070-06

184

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

would fail. However, there exists a p × (K − 1) matrix  with linearly independent
columns such that LL =  . We can then write
cov(x) =  + ,

(6.44)

which makes the correspondence complete.
This result is very important because it shows that, at the level of second moments,
the latent class model with K classes is indistinguishable from a factor model with
K − 1 factors. Thus if we have successfully fitted a linear factor model to a given
covariance matrix there is no guarantee that the latent space is continuous. It could
equally well be categorical. This was one reason why we emphasised the desirability
of examining the marginal distributions since multimodality there would be one way
of distinguishing the two models. Unfortunately mixtures of distributions do not
always exhibit distinct modes, so an inspection of the margins may leave the matter
unresolved.
The analysis given above has been taken further by Molenaar and von Eye (1994).
They show that the converse is also true, namely that if a covariance matrix has arisen
from a linear factor model it is always possible to find a latent class model with the
same covariance structure. They have also shown that the indeterminacy associated
with the factor model that allows rotations in the factor space also carries over to the
latent class model, and have provided the means to find rotated solutions. This means
that any set of classes we identify by a latent class analysis is not unique.
The result of (6.44) suggests that we might be able to use software for fitting
factor models to fit latent profile models. However, knowledge of  and  is not
sufficient to determine the parameters of the latent profile model, as (6.43) shows.
Molenaar and von Eye (1994) have, however, shown that it is possible to construct
constrained factor models, having the same covariance structure as a latent profile
model, for which the prior distribution {η j } and the conditional means {μi ( j)} (but
not the variances) can be identified. For practical purposes, maximum likelihood is to
be preferred as a method of estimation. The real importance of the moment results is
that they illustrate the intrinsic difficulty of obtaining precise knowledge about latent
variables.
A full analysis of the relationship between the moment structure of the latent profile and factor models would take us too far afield, but it is instructive and informative
to look at the link between  and L when K = 2. We note first that
μi (1) − μ̄i = (1 − η1 ){μi (1) − μi (2)},
μi (2) − μ̄i = −η1 {μi (1) − μi (2)}.
L is thus a matrix with ith row equal to
√
√
[ η1 (1 − η1 ){μi (1) − μi (2)}, − η2 η1 {μi (1) − μi (2)}].

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

185

The (i, k)th element of LL is thus
η1 (1 − η1 ){μi (1) − μi (2)}{μk (1) − μk (2)},
and this is identical to the corresponding element in  when  is a p × 1 matrix
with ith element
η1 (1 − η1 ){μi (1) − μi (2)}.

(6.45)

The same result could have been obtained by starting with the linear model of (1.11),
setting q = 1 and allowing y to have the Bernoulli distribution with P{y = 1} = η1 .
Looking at the problem this way shows why the case K > 2 is less simple since then
we need a vector-valued indicator for y.

6.16.3

Allocation to categories

If gi (xi | j) is a member of the exponential family for all i then it follows from the
general theory that the posterior distribution, and hence the allocations rule, will be
based on a linear function of the xs. In particular, if xi given j is normal with mean
μi ( j) and unit standard deviation then
g(x | j) = (2π )

− 12 p


p
1
2
exp −
(xi − μi ( j)) .
2 i=1

Category j is more probable than category k if
η j g(x | j) > ηk g(x | k),
which holds if
p

i=1

6.17


1 2
1 2
μi ( j) + ln η j >
μi (k) −
μ (k) + ln ηk .
2 i=1
2 i=1 i
i=1
p

μi ( j)xi −

p

p

Models with ordered latent classes

In some circumstances we may expect the latent classes to be ordered. If, for example,
they represent political beliefs we might suppose that they are ordered on a left/right
scale. In that case the probability of giving a positive response to a binary item might
be expected to change monotonically as we move along the sequence of classes. A
model with ordered classes represents an intermediate stage between the latent trait
model, in which the latent space is a continuum, and the latent class model. Models
of this kind were proposed by Croon (1990) who gave an algorithm for fitting and a
numerical example. Croon also considers the more general case in which the manifest
variables are polytomous and ordered. There is then a choice of ways for specifying

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

186

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

the ordering. Croon’s method involves requiring that the monotonicity property holds
for all possible ways of dichotomising the ordered manifest variables.
Given the difficulty of distinguishing a latent class model from a latent trait
model, which we have already noted, one would expect it to be even more difficult
to distinguish either from an intermediate form. This point should be borne in mind
when using the model with ordered latent categories.

6.18

Hybrid models

Table 1.3 can be extended to allow for mixed categorical and metrical latent variables.
The purpose of this might be to identify latent sub-populations and to measure latent
constructs within each such latent class. To illustrate the value of such models we
shall describe a typical example. A two-parameter latent trait model assumes that the
item difficulty and item discrimination parameters are the same for all population
members. This is often a plausible assumption, but not always. For example, the item
parameters may take different values in different subgroups of the population. In
particular, a majority of respondents might give responses according to their ability
whereas others might use another strategy such as random guessing. These different
strategies could not be identified from the respondents considered as a whole but
only within the latent classes. In some cases it might be possible to identify classes
in advance using covariates such as gender or age; otherwise the classes could be
latent. A hybrid model allows us to fit latent trait models within the unobserved latent
classes. This is a useful way of extending the traditional approach which has many
new applications (see, for example, Mislevy and Verhelst (1990); Muthén (2008);
Muthén and Asparouhov (2006): Rost (1990)). Such models can take into account
various types of population heterogeneity arising from different response strategies
and so resolve the lack of local (or conditional) independence in latent classes by
imposing further structure.

6.18.1

Hybrid model with binary manifest variables

We discuss in this section a latent class model with K classes with a two-parameter
item response theory model fitted in each class. Other models can be also fitted
in a class, or a class can be chosen to be without any further structure, capturing
for example aberrant response patterns or outliers. Let πi j (y) be the conditional
probability of a positive response on variable i for a person in category j (i =
1, 2, . . . , p; j = 0, 1, . . . , K − 1) with latent variable value
!y. Let η j be" the prior
K −1
probability that a randomly chosen individual is in class j
j=0 η j = 1 and h(y)
be the prior distribution of the latent variable. For the case of K classes and one latent
variable y,

f (x) =

h(y)
Ry

K
−1

j=0

ηj

p

{πi j (y)}xi {1 − πi j (y)}1−xi dy.
i=1

(6.46)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

187

The posterior probability that an individual with response vector x belongs to category
j and is at the latent position y is thus
h( j, y | x) = h(y)η j

p

{πi j (y)}xi {1 − πi j (y)}1−xi / f (x) ( j = 0, 1, . . . , K − 1).
i=1

(6.47)
Let us assume that in each class the logit/normal model with a single latent variable
is fitted,
logit πi j (y) = αi0, j + αi1, j y

(i = 1, 2, . . . , p; j = 0, 1, . . . , K − 1),

(6.48)

where y has standard normal distribution.

6.18.2

Maximum likelihood estimation

The log-likelihood for a random sample of size n is
⎧
⎫
p
n
K
−1
⎨
⎬



L=
ln
h(y)
ηj
{πi j (y)}xi h {1 − πi j (y)}1−xi h dy .
⎩ Ry
⎭
h=1

j=0

(6.49)

i=1

The integral in (6.49) can be approximated using the Gauss–Hermite method already
discussed in Section 4.5. Suppose that the Gauss–Hermite quadrature uses the discrete
distribution at values y 1 , . . . , y T with probabilities h(y 1 ), . . . , h(y T ). Then
⎧
⎫
p
n
T
K
−1
⎨
⎬



L=
ln
h(y t )
ηj
{πi j (y t )}xi h {1 − πi j (y t )}1−xi h .
(6.50)
⎩
⎭
h=1

t=1

j=0

i=1

As before, this has to be maximised subject to
strained maximum of
φ = L +θ

K
−1




η j = 1, so we find the unre-

ηj,

j=0

where θ is an undetermined multiplier. Finding partial derivatives, we have

p
n 
T


∂φ
t
t xi h
t 1−xi h
=
h(y )
{πi j (y )} {1 − πi j (y )}
/ f (xh ) + θ
∂η j
h=1 t=1
i=1
( j = 0, 1, . . . , K − 1)
=

n 
T

h=1 t=1

{h(y t )g(xh | j, y t )/ f (xh )} + θ,

(6.51)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

188

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

where g(xh | j, y t ) is the joint probability of xh for an individual in class j with latent
ability yt .
Also,

∂φ
∂g(xh | j, y t )
= ηj
h(y t )
∂αil, j
∂αil, j
h=1 t=1
n

T


f (xh )

(i = 1, 2, . . . , p; j = 0, 1, . . . , K − 1; l = 0, 1),
giving
n
T


∂φ
= ηj
ylt h(y t )
(xi h − πi j (y t ))g(xh | j, y t )/ f (xh )
∂αil, j
t=1
h=1

(l = 0, 1). (6.52)

The resulting equations can be simplified by expressing (6.51) and (6.52) in terms of
the posterior probabilities {h( j, y t | x)}. Substituting in (6.51) and equating to zero,
we find
n 
T


h( j, y t | xh ) = −θ η j .

h=1 t=1

Summing both sides over j and y t and using
first estimating equation (6.51) is
η̂ j =

n 
T


h( j, y t | xh )/n



η j = 1 gives θ = −n, and hence the

( j = 0, 1, . . . , K − 1).

(6.53)

h=1 t=1

The second estimating equation (6.52) becomes
T 
n


ylt {xi h − πi j (y t )}h( j, y t | xh ) = 0 (l = 0, 1)

t=1 h=1

or
T


ylt {ri, jt − n jt πi j (y t )} = 0 (l = 0, 1),

t=1

where
ri, jt =

n


xi h h( j, y t | xh ),

h=1

n jt =

n

h=1

h( j, y t | xh )

(6.54)

P1: TIX/XYZ
JWST070-06

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:39

Printer Name: Yet to Come

LATENT CLASS MODELS

189

and
η j h(y t )g(xh | j, y t )
h( j, y t | xh ) = T  K −1
.
t
t
t=1
k=0 η j h(y )g(xh | j, y )

(6.55)

As in Section 4.5, it may be helpful to give an interpretation of ri, jt and njt . The
that an individual with response vector xh
quantity h( j, y t | xh ) is the probability

is located in class j and at yt ; nh=1 h( j, y t | xh ) is thus the expected number of
individuals in class j and yt . By a similar argument, ri, jt is the expected number of
those predicted to be in class j and yt who will respond positively.
An E-M algorithm is used again here to obtain maximum likelihood estimates
using the following steps
1. Choose starting values for all model parameters.
2. Compute the values ri, jt , njt and h( j, y t | xh ) .
3. Obtain improved estimates of the αs by solving (6.54) for l = 0, 1 and i =
1, . . . , p, treating ri, jt and njt as given numbers, and for the ηs by solving
(6.53).
4. Obtain improved estimates of {h( j, y t | xh )} from (6.55).
5. Return to step 2 and continue until convergence is attained.

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

7

Models and methods for
manifest variables of
mixed type
7.1

Introduction

So far we have supposed that all manifest variables were of the same type – that is,
were all continuous or all categorical with the same distribution. In the former case
their conditional distributions were assumed to be normal; and in the latter Bernoulli
for binary responses, or multinomial for polytomous responses. In many fields of
application, especially with data arising from sample surveys, both types of variables
often occur together. The purpose of this chapter is to show how such problems can
easily be handled within the framework of the GLLVM, as set out in Chapter 2.
There is nothing in the general treatment given there which requires the xs to have
distributions of the same form, provided that they are all members of the exponential
family. As a by-product we shall obtain new results for cases where all the variables
are of the same type but have distributions other than those already covered.
The traditional way of handling problems with manifest variables of mixed type
has been to convert the problem into one where the variables are of the same type.
One way of doing this is by categorising the continuous variables. At the price of
some loss of information the problem can then be handled by established methods
for categorical variables. The more usual course has been to resort to the device of
introducing underlying variables. All categorical variables are thereby regarded as
incompletely observed continuous variables which are supposed to conform to the
normal linear factor model. The correlations required for fitting such models are then
estimated as tetrachoric, polychoric or polyserial correlations according to the type
of variable involved. This approach was pioneered by Muthén (1984) and has been
Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

191

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

192

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

implemented in the Mplus program (Muthén and Muthén 2010). A similar approach
is available in the LISREL package (Jöreskog and Sörbom 2006) within the broader
framework of linear structural relations modelling (see Chapter 8). A comprehensive theoretical treatment of such models, where the manifest variables are of mixed
type, is given by Arminger and Küsters (1988). We shall return to this approach in
Section 7.9 in order to compare it with our treatment based on the GLLVM. The latter
has been developed by Moustaki (1996) for mixtures of continuous and binary variables and, more generally, in Moustaki and Knott (2000a). A very similar treatment
has been given by Sammel et al. (1997).

7.2

Principal results

Suppose the manifest variables are of k types, with p1 of the first type, p2 of the second
and so on ( p1 + p2 + · · · + pk = p). The vector of manifest variables can then be
partitioned, according to type, as x = (x1 , x2 , . . . , xk ), where xi = (xi1 , xi2 , . . . , xi pi )
(i = 1, 2, . . . , k). All elements of xi belong to the same member of the exponential
family. In the general formulation of the model in (2.1) and (2.2) we allowed each
manifest variable to have a different distribution, as indicated by the suffix of g in
(2.1). In the present case, the first p1 elements of x share the same form of distribution
and hence the functions Fi , G i and ui in (2.1) will be common to all members of that
subset. The parameter θi will, however, be influenced differently by each member.
The same is true for the remaining subsets of x. For our present purpose it is therefore
convenient to rewrite the model of (2.1) and (2.2) as
gi (xil | θil ) = Fi (xil )G i (θil ) exp(θil u i (xil ))

(7.1)

θil = αil0 + αil1 y1 + αil2 y2 + · · · + αil yq ,

(7.2)

and

where, in both cases, i = 1, 2, . . . , k and l = 1, 2, . . . , pi . When expressed in this
form (2.4) becomes
h(y | x) ∝ h(y)

pi
k 


G i (θi ) exp

q


i=1 l=1

yj X j,

(7.3)

j=1

where
Xj =

pi
k 


αilj u i (xil ).

(7.4)

i=1 l=1

X = (X 1 , X 2 , . . . , X q ) is again the Bayesian minimal sufficient statistic or component. For those distributions for which ui is a linear function the components will

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

193

be linear functions of the manifest variables. They may, therefore, be interpreted in
essentially the same way as when the xs were all of the same type.
Although the foregoing argument has been conducted as though the latent variables were continuous, the same result follows if some or all of the latent variables are
categorical, as we saw in Chapter 6. In principle, therefore, no new theory is required
for manifest variables of mixed type. It emerges as a special case of the GLLVM.
There are, however, new technical difficulties concerned with fitting the models and
with judging their goodness of fit. Many of these problems remain unsolved, but
methods are available for some of the commonly occurring problems and these will
be reviewed in the remainder of the chapter. We shall conclude by setting our results
in the rather broader context mentioned in Section 7.1.

7.3

Other members of the exponential family

Hitherto we have worked only with normal, Bernoulli and multinomial distributions
for the conditional distributions of manifest variables. Before proceeding we shall
therefore extend the repertoire to include the binomial, Poisson and gamma distributions which are all easily derived from the GLLVM. The results for the gamma
distribution are needed for Example 7.8.2.

7.3.1

The binomial distribution

This can be dealt with using the results for the Bernoulli distribution because a
binomial random variable is the sum of independent Bernoulli random variables.
However, a direct approach starts with
gi (xi | y) =

 
n
{πi (y)}xi {1 − πi (y)}n−xi
xi

(xi = 0, 1, 2, . . . , n),

(7.5)

where n is the numbers of ‘trials’. Writing this in the form
gi (xi | y) =

 
n
{1 − πi (y)}n exp{xi logit πi (y)},
xi

(7.6)

the linear model becomes
logit πi (y) = αi0 +

q


αi j y j

(7.7)

j=1

exactly as for the Bernoulli case. The components are then
Xj =

p

i=1

αi j xi .

(7.8)

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

194

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

7.3.2

The Poisson distribution

If
gi (xi | y) =

(μi (y))xi −μi (y)
e
xi !

(xi = 0, 1, 2, . . .),

(7.9)

the standard approach yields
ln μi (y) = αi0 +

q


αi j y j

(7.10)

j=1

and
Xj =

q


αi j xi ,

(7.11)

j=1

a result which we might have anticipated from the results for the binomial distribution.

7.3.3

The gamma distribution

Like the normal distribution, the gamma distribution depends on two parameters,
and different models arise according to how we parameterise the model and which
parameter is made to depend on the latent variables. In the case of the normal
distribution we assumed that only the location of the conditional distribution depended
on the latent variables. If we are to consider mixtures of normal and gamma variates,
say, it is natural to treat each type of variable in the same way, but it must be
remembered that other options are possible. Here, therefore, we shall parameterise
the distribution so that the mean is one of the parameters and it will be the mean
which depends on the latent variables. Thus suppose
1
gi (xi | y) =
(νi )



νi
μi (y)

νi

xiνi −1 e−νi xi /μi (y) ,

(7.12)

where μi (y) is the mean and νi is a nuisance parameter determining the shape of the
distribution. For this distribution

1
= αi0 +
αi j y j
μi (y)
j=1
q

θi (y) =

(7.13)

and the components are therefore
Xj =

p

i=1

αi j νi xi .

(7.14)

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

195

It should be noted that, since a gamma variate is non-negative, μi (y) must also be
non-negative. If we are considering several types of manifest variables we shall not
wish to impose constraints on the form or range of the prior distribution since we
shall want it to serve for all such variables. The model proposed will therefore only
be reasonable if αi0 is sufficiently large to make μi (y) positive with high probability.
Returning to (7.4) in Section 7.2, we can see immediately that if we have p1
variables of one type, p2 of another and so on, the components will simply be the
sum of the components for the constituent parts – one for each type. The maximum
likelihood estimators will follow by constructing the likelihood from the appropriate
number of constituents of each type, as illustrated in the examples which follow.

7.4

Maximum likelihood estimation

We have provided several methods for finding maximum likelihood estimators when
all manifest variables are of the same type. Now we shall develop a general approach
for the GLLVM when the individual xs may all have different distributions within the
exponential family. This will cover, as a special case, the problem when the manifest
variables are grouped according to type. The basic idea is the same as that which was
used for the latent trait and latent class models in Chapters 4–6. The method lends
itself to implementation using the E-M approach but can also be the starting point for
other approaches.
As in Chapter 2, we now suppose that
gi (xi | θi ) = Fi (xi )G i (θi ) exp{θi u i (xi )},

(7.15)

where
θi = αi0 y0 + αi1 y1 + αi2 y2 + · · · + αiq yq

(i = 1, 2, . . . , p)

(7.16)

and y0 ≡ 1.
We consider the problem of estimating the αs, any nuisance parameters being
assumed known at this stage. If we have a random sample of size n, the log-likelihood
may be written

L=

n


ln f (x )

=1

=

n

=1


ln

h(y)

p

i=1

Fi (xi )G i (θi ) exp

 p



θi u i (xi ) dy,

i=1

the integral here and below being over the range space of y.

(7.17)

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

196

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

The partial derivatives of L with respect to the αs are then given by
 1
∂L
=
∂αi j
f (x )
=1
n



p


∂
Fi (xi )
h(y)
∂α
ij
i=1


exp

p



θi u i (xi ) + ln G i (θi )

dy

i=1

(i = 1, 2, . . . , p; j = 1, 2, . . . , q).
The partial derivatives, within the integral, may be evaluated by first differentiating
with respect to θi to give
 p


∂θi
d ln G i (θi )
(θi u i (xi ) + ln G i (θi ))
u i (xi ) +
exp
.
dθi
∂α
ij
i=1
Hence

∂L
=
∂αi j
=1
n


h(y | x ) u i (xi ) +

d ln G i (θi )
dθi

y j dy

(7.18)

(i = 1, 2, . . . , p; j = 0, 1, 2, . . . , q),
where h(y | x ) is the posterior distribution of y given x , y0 = 1.
Interchanging the order of summation and integration and setting the partial
derivatives equal to zero, we obtain the basic estimating equations

yj

n



u i (xi )h(y | x )dy =

=1

d ln G i (θi ) 
yj
h(y | x )dy
dθi
=1
n

(7.19)

(i = 1, 2, . . . , p; j = 0, 1, . . . , q).

If a maximum of the likelihood exists, it will satisfy these equations. Estimation for
all models covered in this book, belonging to the exponential family, can be tackled
using the appropriate special case of these equations. If there happen to be additional
unknown (nuisance) parameters, each of these will give rise to a further estimating
equation.

7.4.1

Bernoulli manifest variables

In order to establish the link between the general case and the special cases we have
already met, we look briefly at the cases of Bernoulli and normal manifest variables.
When the xs are all binary random variables with a conditional Bernoulli distribution, we saw in Section 2.4 that
θi = logit πi (y),

G i (θi ) = 1 − πi (y) = 1/(1 + eθi )

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

197

and u i (xi ) = xi . Consequently,
−

1
d ln G i (θi )
=
= πi (y).
dθi
1 + e−θi

(7.20)

Substitution into (7.19) gives
n


xi E(y | x ) =

=1

n


E(y j πi (y) | x ),

(7.21)

=1

which is essentially the same as (4.20) after it has been differentiated with respect to
model parameters and in which the integrals for the expectations have been approximated by sums.

7.4.2

Normal manifest variables

If the conditional distributions of the xs are normal with means {μi } and standard

deviations {σi }, we showed in Section 2.6 that θi = μi /σi , G i (θi ) = exp − 12 θi2 and
u i (xi ) = xi /σi , (i = 1, 2, . . . , p). In this case
d ln G i (θi )
= −θi
dθi

(i = 1, 2, . . . , p).

(7.22)

To facilitate comparison with our earlier results for the normal linear model we write
(7.19) in matrix notation after substituting −θi for d ln G i (θi )/dθi , writing ψi = σi2
and putting u i (xi ) = xi /σi . This gives
n


E(y | x )x ψ −1/2 =

=1

n


E(yθ  | x )

=1

=
=

n

=1
n


E(yy A0 | x )
{ var(y | x ) + E(y | x )E(y | x )}A0 ,

(7.23)

=1

where A0 = (α 0 , A) with
α 0 = (α01 , α02 , . . . , α0 p )

and

A = {αi j } (i = 1, 2, . . . ; j = 1, 2, . . . , q).

For this model, the conditional expectations in (7.23) have already been obtained in
(1.13) and (3.6). On the left-hand side, we have that

1
,
E(y | x ) =
  −1 (x − x̄)


(7.24)

P1: TIX/XYZ
JWST070-07

198

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

where
we have replaced the μ of (3.6) by its maximum likelihood estimator, x̄ =
1 n
=1 x , and
n

var(y | x ) =


0
0
.
0 (I + )−1

(7.25)

Substituting (7.24) and (7.25) into (7.23) and carrying out the summation over l, we
find that the left-hand side becomes


x̄
n
ψ −1/2 ,
  −1 Sx
where Sx is the sample covariance matrix of the xs, given by
Sx =

n
1
(x − x̄)(x − x̄) .
n =1

On the right-hand side we obtain

n

0
0

0
(I + )




+

1
0
 −1
0   Sx  −1 



A0 .

The resulting estimation equations may therefore be written
x̄ −1/2 = α 0

(7.26)

  −1 Sx  −1/2 = (I + )−1 A +   −1 Sx  −1 A .

(7.27)

and

To complete the derivation we replace A in (7.27) by the
expression in 
equivalent
q
and . In the standard normal model E(xi | y) = μi + j=1 λi j y j and the GLLVM
formulation is
⎧
⎫
q
⎨
⎬

E(xi | y) = σi αi0 +
αi j y j .
(7.28)
⎩
⎭
j=1

Therefore, A ≡  −1/2 . Substituting into (7.27) and rearranging gives
(I + )  −1 Sx  −1/2 =   −1/2 + (I + )  −1 Sx  −1   −1/2 .
We showed in (3.8) that
  −1 = (I + )−1   −1

(7.29)

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

199

and, therefore, the estimating equations become
  −1 Sx  −1/2 =   −1/2 +   −1 Sx  −1 (I + )−1   − 2
1

(7.30)

or
A V = A + A VA(I + )−1 A ,

(7.31)

where V =  −1/2 Sx  −1/2 . Transposing and rearranging, we find
(I − A(I + )−1 A )VA = A.

(7.32)

This may be simplified further as follows. Pre-multiplying both sides by A and noting
that A A = ,
A VA + (I + )−1 (A VA) = 

(7.33)

A VA = (I + ).

(7.34)

or

Substituting back into the transpose of (7.31), we have
VA = A{I + (I + )−1 (I + )} = A(I + )

(7.35)

( −1/2 Sx  −1/2 )( −1/2 ) = ( −1/2 )(I + ).

(7.36)

or

This is the equation arrived at in standard treatments of the normal linear model.
Here we have derived it as a special case of the general exponential family model.
An interesting by-product of this result is that we can use the general E-M approach
described below as an alternative estimation method for the normal linear model.

7.4.3

A general E-M approach to solving the likelihood
equations

We can rewrite (7.19) in a form which makes the equations amenable to solution by
the E-M method and which also gives some insight into their interpretation. Let
ny =

n

=1

h(y | x )

(7.37)

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

200

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

and
riy =

n


u i (xi )h(y | x ).

(7.38)

=1

These are generalisations of (4.23) and (4.22). If y has a discrete distribution, n y may
be interpreted as the expected number of sample members located at y, given the data.
If y is continuous, it is the density function of the expectation. In either event we may
think of it, loosely, as specifying the distribution of individuals in the latent space.
n If u i (xi ) is regarded as a score attached to individual  on variable i then
=1 u i (x i ) is the total score associated with variable i. The quantity riy then specifies how that total score is distributed across the latent space. In this notation the
estimation equations may be written



y j riy dy = −

y j ny

d ln G i (θi )
dy
dθi

(i = 1, 2, . . . , p : j = 1, 2, . . . , q).
(7.39)

The notation riy and n y has been chosen to bring out the similarity between the general
equations and those in the special case of the latent trait model discussed in Chapter
4. The E-M algorithm used there applies here also. Its essential steps are as follows:
1. Choose initial values of {riy } and n y .
2. Solve (7.39) for {αi j }.
3. Use these estimates to give new values of riy and n y .
4. Repeat the cycle until convergence is achieved.
In practice the integrals in (7.39) are approximated by sums over a suitable grid
of points in the y-space. The choice of starting values for {riy } and n y is made by
specifying a discrete version of h(y | x ).
The algorithm takes an especially simple form when all of the manifest variables
are conditionally normal. For, in that case, (7.23) is linear in the αs and so step 2
involves the solution of a set of linear simultaneous equations. In Section 3.5 we gave
a different E-M algorithm for fitting the NLFM.

7.4.4

Interpretation of latent variables

In Chapter 2 we described three ways in which the question of interpretation – or
naming – of the latent variables could be approached. All of these led to comparisons
among the αs and, in particular, on their relative values. When the manifest variables
are of different types some care is needed because the interpretation depends on the
scaling of the xs. A Bernoulli variable takes only values 0 and 1 whereas a normal
variable varies continuously with an unknown standard deviation which has to be

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

201

estimated. In order to interpret the latent variables we shall therefore have to ensure
that the αs are calibrated so that they may be meaningfully compared across variable
types. This may be done in a variety of ways, but our aim will be to adopt a parameterisation which keeps the interpretation as close as possible to the familiar methods
of traditional factor analysis. Since these questions are best addressed in relation to
particular applications, we shall take them up again in Examples 7.8.1 and 7.8.2.

7.5

Sampling properties and goodness of fit

The sampling properties of the parameter estimates can be found in the mixed case
by exactly the same method as when all of the manifest variables have the same
distribution. Thus if β is the vector of all parameters to be estimated in a particular
model, the asymptotic variances and covariances are given by the elements of the
inverse of the information matrix. As before, this may be approximated by

I (β̂) =

n

h=1

1
∂ f (xh ) ∂ f (xh )
2
{ f (xh )} ∂β j
∂βh

−1
.

(7.40)

β=β ∗

Similar problems to those encountered before concerning the accuracy of the approximation will arise if some of the parameters take extreme values. It is therefore
preferable to supplement or replace the asymptotic sampling theory by bootstrap
calculations.
No global methods exist for testing the goodness of fit of the mixed model. When
all manifest variables were discrete, we based such a test on the observed and expected
values of the frequencies of the score patterns – that is, we compared the observed
with the expected joint frequency distributions. In the case of normal, continuous
variables, the closeness of the observed and expected distributions was judged by
the respective covariances. With other types of continuous variable we would have
to look for similar relevant parameters of their joint distribution. If the manifest
variables are of mixed type, it is not immediately clear how the overall fit should be
judged, and further research is needed. Nevertheless, it is possible to make partial
judgements of fit by utilising the notion of residuals used for this purpose in earlier
chapters. Thus, for example, we can compare the observed and expected frequencies
for two- or three-way margins of the categorical variables. Similarly, we can compare
observed and expected correlation matrices for any normal continuous variables.
If our main concern is with model selection, as when we wish to choose the
appropriate number of latent variables to include in the model, a goodness-of-fit test
is not the most appropriate instrument. For that we need a model selection criterion
such as the AIC proposed by Akaike (1983, 1987) or the BIC proposed by Schwarz
(1978). The AIC takes account of the maximised value of the likelihood and the
number of parameters estimated, while the BIC also takes account of the sample size.
Those criteria, first given in Section 3.7.1, can be applied regardless of the type of

P1: TIX/XYZ
JWST070-07

202

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

variable and are given by
AIC = −2(max log-likelihood) + 2(number of parameters).
and
BIC = −2(max log-likelihood) + ln(n)(number of parameters).
The model with the smallest value of AIC or BIC is judged to be the most appropriate.
Other variants of this approach are available. They are reviewed and compared in
Sclove (1987).

7.6

Mixed latent class models

As noted in Section 7.1, the treatment which we have given in the foregoing sections is
sufficiently general to cover latent class models. In the case of just two latent classes,
all that is necessary is to have a single binary latent variable. More latent classes can
be accommodated by the device described in Section 6.4. Here we shall illustrate the
method when the manifest variables are a mixture of normal and binary variables.
In the case of two types of variable the joint distribution may be written
f (x) =

K
−1


η j g(x1 | j) g(x2 | j),

(7.41)

j=0

where x1 is the vector of normal variables, say, x2 is that of the binary variables and
K is the number of classes. Since the normal distribution depends on two parameters,
the mean and the variance, we must specify how these parameters depend on the
latent class. In this example we shall suppose that the mean depends on the latent
class but that the variance is constant across classes. Thus we suppose that
x1j ∼ N (μj , σ2 )

( j = 0, 1, . . . , K − 1;  = 1, 2, . . . , p1 ).

(7.42)

For the binary variables we assume
P(x2j = 1 | j) = πj

( j = 0, 1, . . . , K − 1;  = 1, 2, . . . , p2 ).

(7.43)

The log-likelihood is then
L=

n


ln f (xh )

h=1

=

n

h=1



ln

K
−1

j=0

ηj

p
i

=1

−1/2
(2π )−1/2 σ exp


p2 


x2h
πj (1 − πj )1−x2h .
×
=1

1
− 2 (x1h − μj )2
2σ



(7.44)

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

203

Finding the partial derivatives with respect to the unknown parameters then gives the
following estimating equations:
η̂ j =
π̂j =
μ̂j =
σ̂2 =

n

h=1
n


h( j | x1h , x2h )/n,

(7.45)

x2h h( j | x1h , x2h )/n η̂ j ,

(7.46)

h=1
n


x1h h( j | x1h , x2h )/n η̂ j ,
h=1
n  K −1
2
h=1
j=0 (x 1h − μj ) h( j | x1h ,
n  K −1
h=1
j=0 h( j | x1h , x2h )

(7.47)
x2h )

,

(7.48)

where
h( j | x1h , x2h ) ∝ η j g(x1h | j)g(x2h | j)
is the posterior probability that an individual with response pattern xh = (x1h , x2h )
is in class j ( j = 0, 1, . . . , K − 1). These equations can be solved in the usual way
as follows:
1. Choose initial values for the posterior probabilities.
2. Obtain first approximations for η̂ j , π̂j , μ̂j and σ̂2 .
3. Use these to re-estimate the posterior probabilities.
4. Repeat the cycle until convergence is achieved.
See Example 7.8.3 below.

7.7

Posterior analysis

Posterior analysis flows from the conditional distribution of y given x which, for the
case of variables of mixed type, is given by (7.3) and (7.4). It is clear from (7.4) that
the jth component Xj is simply the sum of the components for the constituent subsets
of variables of each type. Thus, for example, if we have normal and binary manifest
variables the components will be
Xj =

p1

λj
=1

ψ

x1 +

p2


αj x2 .

(7.49)

=1

In the case of a single latent variable, Theorem 2.15.1 applies. This shows that the
component ranks individuals in the same order as does the conditional expectation

P1: TIX/XYZ
JWST070-07

204

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

E(y | x). For most practical purposes the components will serve. However, if we need
the expectations or the posterior variances it will be necessary to compute them by
numerical integration starting with the posterior distribution of (7.3).
For the latent class model, posterior analysis involves the determination of the
posterior probabilities of falling in the latent classes. These, too, will be functions of
the components.

7.8

Examples

Several examples where the manifest variables are mixtures of binary and continuous (treated as normal) variables are given in Moustaki (1996) and Moustaki and
Papageorgiou (2004). Further examples covering a wider range of types of variables,
including multinomial and gamma variates, will be found in Moustaki and Knott
(2000a). Here we use two of these examples to illustrate the use and interpretation
of the models of this chapter. For our purposes it is not necessary to enquire into
questions such as the number and choice of items or the justification of the distributional assumptions. More detail will be found in the original papers. Here we shall
concentrate on the fitting and interpretation of the models.
Example 7.8.1 The origin of these data come from a study of the phenomenon of
‘flashbulb’ memories reported in Wright et al. (1994). In the present example 485
individuals were asked about what they could remember of the Hillsborough football
disaster on 15 April 1989. There were five questions, the first two of which were as
follows.
Q.1 Taking your answer from this list, how clear is your recollection of the event?
Cannot remember it. (1)
Vague. (2)
Fairly clear. (3)
Clear. (4)
Completely clear. (5)
Q.2 Thinking back to when you first heard about the disaster, can you remember –
just answer yes or no.
Where you were.
Who you were with.
How you heard about it.
What you were doing.
Answers to the four items in Question 2 were coded 1 for yes and 0 for no. Answers to
Question 1 were scored on an equally spaced scale as indicated above. The distribution

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

205

Table 7.1 Hillsborough disaster: estimates for the single latent trait model.
Variable(x1 )



μ̂

λ̂1

ψ̂

λ̂∗1

Vividness of recollection

1

3.19 (0.05)

0.56 (0.05)

0.75 (0.06)

0.54

α̂0

α̂1

π̂

∗
α̂1

4.41 (1.54)
2.91 (0.73)
5.55 (0.83)
1.39 (0.19)

6.45 (2.33)
5.29 (1.31)
3.24 (0.61)
2.03 (0.28)

0.99
0.95
0.99
0.80

0.988
0.983
0.955
0.897

Variable(x2 )
Where you were
Who you were with
How you heard about it
What you were doing

1
2
3
4

was roughly normal, so the problem was treated as one with four binary variables
and one normal variable.
In fitting a latent variable model we are asking whether the variation between
individuals can be explained by the supposition that they are located in a small
number of latent dimensions or classes. Moustaki (1996) investigated the fit of a
latent trait model with one and two dimensions and a latent class model with two
classes. The two-dimensional model was marginally poorer than the one-dimensional
model, and we only report the latter. The parameter estimates are given in Tables 7.1
and 7.2. Asymptotic standard errors are given in brackets in Table 7.1.
The interpretation of the latent class model fitted in Table 7.2 is straightforward.
In the first class individuals have a high probability of being able to remember the
circumstances in which they heard the news, whereas those in the second have, with
the partial exception of x3 , a much lower probability. Those in the first class score
higher on the x1 scale (mean 3.44) than those in the second (mean 2.53). This is
consistent with the interpretation suggested by the x2 -variables, though one might
perhaps have expected a bigger difference in the means given the very clear separation
in the π s. A difference of barely one unit on a scale from 1 to 5 and a standard deviation
Table 7.2 Hillsborough disaster: estimates for a latent class model with two classes.
Variable(x1 )



μ̂10

μ̂11

σ̂12

Vividness of recollection

1

3.44

2.53

0.90

π̂0

π̂1

0.979
0.933
0.995
0.863

0.141
0.090
0.774
0.274

0.724

0.276

Variable (x2 )
Where you were
Who you were with
How you heard about it
What you were doing
η̂ j

1
2
3
4

P1: TIX/XYZ
JWST070-07

206

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

√
of 0.95 ( 0.9) suggests that the reported vividness of the recollection correlates rather
weakly with the class to which the individual belongs.
The interpretation of the latent trait model needs some care. If we look first at
the part concerning the variables x2 we find the coefficients {α1 } are typical of what
we would expect where there is a dominant latent variable with which each item is
strongly correlated. Further, the π s show that the median individual on the latent scale
is very likely to remember all the circumstances. The complications arise when we
attempt to link the results for the continuous variable with those for the remainder. The
question is how to calibrate the two types of loading. In particular, does λ̂1 = 0.56
represent a bigger ‘loading’ than, say, α̂21 = 5.29? To answer such questions we turn
back to the approaches to interpretation given in Chapter 2 (especially Section 2.12).
One is via the components. For this example the component is
X = 0.75x1 + 6.45x21 + 5.29x22 + 3.24x23 + 2.03x24 .
The importance of variables is judged by the impact which they have on the value of X.
For example, the presence or absence of x22 (which is a binary variable) changes X by
an amount 5.29. If x1 changes from 1, at one end of its range, to 5, X will change by an
amount (4 × 0.56)/0.75=2.99. A change of 1 unit in the value of x1 produces a change
of 0.75 in X. It seems, therefore, that x1 is certainly less important than x22 and, by the
same token, than any of the other binary variables. This is consistent with the results
of the latent class analysis. A second approach to interpretation, used previously
for the normal model, is to look at the correlations between the manifest and latent
variables. For the continuous part of this model the correlation between x1 and y is
λ̂11
= 0.54,
λ̂∗1 = 
2
λ̂11 + ψ̂1

(7.50)

as given in the last column of Table 7.1. One could calculate the correlation between
each of the binary variables and the latent variable, but this is not strictly comparable
with a product-moment correlation between two continuous variables. A better
method is to use the equivalence of the binary model and an underlying variable
model established in equation (4.31). The correlation between the underlying
1/2
variable and the latent variable is then obtained by replacing λi j by αi j ψi . The
standardised version of αi j thus becomes
1/2



αi j ψi
αi2j ψi

+ ψi

=

αi j
αi2j

+1

= αi∗j ,

(7.51)

say. In the general case, when there are q latent variables,
αi∗j = 
q

αi j

j=1

αi2j + 1

.

(7.52)

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

207

The estimated standardised αs are given in the last column of Table 7.1. They show
a very strong correlation with the latent variables – substantially greater than that for
∗
= 0.54.
the variable x1 represented by the loading α̂11
Taking the two analyses together it is clear that the variation in the ability of
individuals to recall salient facts about the circumstances in which they heard of the
Hillsborough disaster is satisfactorily explained by the hypothesis of a single latent
variable. It is equally clear that the binary variables are much better indicators of that
latent dimension than the subjectively assessed vividness of that memory.
Example 7.8.2 This example is taken from Moustaki and Knott (1997) and is chosen
to illustrate the use of the model to construct a measurement scale from variables
of mixed type. The items are from the Enquête sur la Consommation 1990 Survey
of the Office Fédéral de la Statistique. Information was collected from a survey of
1963 households. After removing cases and variables judged to be inappropriate, the
sample used consisted of 923 households with five variables as follows:
1. Presence of a dishwasher (1/0) [DishWasher]
2. Presence of a video recorder (1/0) [Video]
3. Presence of a car (1/0) [Car]
4. Equivalent food expenditures [FoodExp]
5. Equivalent expenditures for clothing [ClothExp]
The first three items are binary and items 4, 5 are metrical. Judged by their
frequency distributions it seemed reasonable to treat item 4 as normally distributed
and item 5 as gamma distributed. Both of the metrical variables were standardised so
that item 4 had mean zero and variance one and item 5 variance one. The one-factor
model gave a good fit to the binary variables taken alone, judging from the two- and
three-way margins. The AIC was 16 546 for the one-factor model and 16 544 for the
two-factor model.
The parameter estimates for the full model are given in Table 7.3. From the last
column of the table we see that the median individual is very likely to have a car
(π̂3 = 0.91) and less likely to have a dish washer (π̂3 = 0.40) and video recorder
(π̂3 = 0.38).
As an index of welfare we might then use the component given by
X = 1.49x1 + 0.74x2 + 1.72x3 + 0.67x4 + 0.46x5
where the xs are listed in the same order as in Table 7.3.
Example 7.8.3 We conclude this section with an example taken from Moustaki and
Papageorgiou (2004). It is an application of latent class analysis for mixed binary and
continuous data in archaeometry. The 73 sample terracotta objects are majolica vases
and floor tiles, manufactured between the sixteenth and eighteenth centuries. Most

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

208

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 7.3 Measurement of welfare: estimates and standard errors for the one-factor
latent trait model.
Binary
DishWasher
Video
Car
Normal
FoodExp
Gamma
ClothExp

α̂i0

α̂i1

−0.43 (0.07)
−0.51 (0.05)
2.30 (0.15)

1.49 (0.15)
0.74 (0.08)
1.72 (0.18)

α̂i0

α̂i1

ˆi


0.50 (0.03)

0.75 (0.03)

α̂i0

α̂i1

ν̂i−1

−0.74 (0.02)

0.18 (0.01)

0.39 (0.01)

0.00 (0.02)

π̂i
0.40
0.38
0.91

of them can be found in the Museo Regionale della Ceramica of Caltagirone. There
are 21 metric variables that provide chemical information and 12 binary variables
from the petrological analysis. A latent class model is fitted to the 33 variables. The
investigation of univariate, bivariate and trivariate chi-squared residuals suggested
that a three-class solution is adequate: 33% of the objects fall in class I, 37% in
class II and 30% in class III. The estimates for the conditional class probabilities and
the parameters of the metric variables, with standard errors in brackets, are given in
Tables 7.4 and 7.5 respectively. More specifically, Table 7.4 shows that variables 4
and 8 do not discriminate between the three classes whereas variables 3 and 7 do not
discriminate between classes 1 and 3.
The estimated posterior class-membership probability was greater than 0.99 for
each object. Therefore, the allocation into classes is almost deterministic. The classification obtained was the same as the grouping with respect to the origin of the objects
(class I contains ceramics from Napoli, class II contains ceramics from Caltagirone
and class III from Palermo).

7.9

Ordered categorical variables and other
generalisations

All of the models considered in this chapter emerge as special cases of the sufficiency
principle. However, we have already noted in Chapters 5 and 6 that there is an alternative way of modelling ordered categorical variables which does not fall within this
framework. In that approach we adopted a logit model for the cumulative probability
that the response occurred in a given category or one which ranked higher. This was
shown to be equivalent to an underlying variable model in which the categories were
formed by grouping a continuous distribution. This, in turn, offered a simple, but

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

209

Table 7.4 Terracotta data: estimated conditional probabilities, π̂i j , for the binary
variables and class probabilities, η̂ j , with standard errors in brackets, for the threeclass model.
Variables

Class I π̂i1

Class II π̂i2

Class III π̂i3

1
2
3
4
5
6
7
8
9
10
11
12

0.04 (0.04)
0.66 (0.19)
1.00 (0.01)
1.00 (0.06)
0.50 (0.10)
0.08 (0.06)
1.00 (0.01)
0.92 (0.06)
0.17 (0.08)
0.12 (0.07)
0.46 (0.10)
0.50 (0.10)

0.11 (0.06)
0.59 (0.09)
0.44 (0.09)
0.85 (0.07)
0.08 (0.05)
0.11 (0.06)
0.19 (0.08)
0.93 (0.05)
0.07 (0.05)
0.18 (0.07)
0.59 (0.09)
0.81 (0.07)

0.18 (0.08)
0.00 (0.02)
1.00 (0.01)
1.00 (0.00)
0.00 (0.01)
0.27 (0.09)
1.00 (0.01)
1.00 (0.00)
0.82 (0.08)
0.05 (0.05)
0.64 (0.11)
0.77 (0.09)

η̂ j

0.33 (0.05)

0.37 (0.06)

0.30 (0.05)

not fully efficient, method of estimating the parameters by estimating first the correlations between the underlying variables. If ordered categorical variables are mixed
with normal variables, the same approach can be used except that different kinds
of correlations will be involved. There will be polychoric correlations for pairs of
categorical variables, polyserial correlations where one member is categorical and the
other continuous, and product-moment correlations for pairs of continuous variables.
For full maximum likelihood, the form of the likelihood will be exactly the same as
for the unordered model derived from the sufficiency principle, but the π s will now
be given by the expression in (5.79).
All of these models, together with those treated earlier in the chapter, are included within the general framework provided by Arminger and Küsters (1988).
Their treatment starts, like this book and earlier related work, from the basic idea that
all variables, latent and manifest, are random variables. Whether or not such variables
are continuous, categorical or whatever, is then a secondary matter which is irrelevant
to the essential structure of the model which is expressed in the various conditional
distributions. Once this framework is accepted, there is no problem about having
manifest variables of mixed type. Arminger and Küsters (1988) treat the estimation
problem at a sufficient level of generality to cover all the problems of this chapter
and many more. The lack of software needed to implement this approach may have
inhibited the wider recognition of its generality.
Arminger and Küsters’s model goes beyond the subject of this chapter in several
respects. It provides for relationships between the latent variables to which we come
in the next chapter. It also allows for other exogenous manifest variables which are
assumed to act causally on the latent variables. Of more relevance to the content

P1: TIX/XYZ
JWST070-07

210

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

Table 7.5 Terracotta data: estimated within-class means and variances for the metric
variables, with standard errors in brackets, for the three-class model.
Variables
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

Class I μ̂i1

Class II μ̂i2

Class III μ̂i3

σ̂i2

0.67 (0.20)
−0.72 (0.06)
−0.29 (0.12)
−0.74 (0.09)
0.59 (0.15)
−0.36 (0.12)
0.31 (0.34)
0.54 (0.17)
0.24 (0.17)
−0.59 (0.06)
−0.55 (0.09)
−0.78 (0.09)
0.87 (0.15)
0.14 (0.17)
0.36 (0.15)
0.46 (0.23)
−0.52 (0.23)
0.14 (0.23)
−0.07 (0.33)
−0.67 (0.09)
−0.37 (0.14)

0.05 (0.13)
1.04 (0.13)
−0.41 (0.17)
−0.05 (0.14)
−0.25 (0.21)
1.06 (0.11)
−0.14 (0.02)
0.26 (0.12)
0.73 (0.10)
−0.03 (0.16)
−0.44 (0.14)
1.17 (0.10)
−0.40 (0.05)
0.30 (0.18)
−0.38 (0.19)
−0.01 (0.17)
0.60 (0.11)
0.58 (0.13)
0.18 (0.08)
0.65 (0.20)
0.47 (0.23)

−0.80 (0.17)
−0.49 (0.15)
0.82 (0.21)
0.87 (0.22)
−0.34 (0.18)
−0.90 (0.08)
−0.16 (0.03)
−0.90 (0.19)
−1.16 (0.08)
0.68 (0.26)
1.14 (0.16)
−0.59 (0.06)
−0.46 (0.25)
−0.53 (0.21)
0.08 (0.22)
−0.49 (0.14)
−0.17 (0.18)
−0.86 (0.09)
−0.15 (0.06)
−0.07 (0.18)
−0.17 (0.17)

0.64
0.34
0.70
0.58
0.81
0.29
0.94
0.63
0.37
0.73
0.42
0.17
0.61
0.86
0.89
0.85
0.76
0.64
0.97
0.68
0.85

of the present chapter, they consider another type of manifest variable obtained by
censoring a continuous variable. Thus suppose a variable x is observed only if its
value is greater than some threshold τ ; otherwise we only know that it is in the
interval (−∞, τ ). Such a variable might be described as partly continuous and partly
categorical. If we define a new variable
x∗ =

x
τ

if x > τ ,
if x ≤ τ ,

(7.53)

the transformation from x to x ∗ is called the tobit relation. For further details, see
Amemiya (1984).
There is a way of dealing with ordered categorical variables within the framework
of the sufficiency principle which merits further investigation. We saw in Chapter 5
that the multinomial model involved parameters which can be regarded as category
scores (the αs of (5.20)). An ordering of the categories implies an ordering of the αs,
and thus we could use this ordering information by maximising the likelihood subject
to the appropriate inequalities on the αs. This is more general than the cumulative

P1: TIX/XYZ
JWST070-07

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:42

Printer Name: Yet to Come

MANIFEST VARIABLES OF MIXED TYPE

211

response probability model because it allows us to impose partial orders. For example,
if we have ordered responses such as strongly agree, agree, disagree, strongly disagree
together with don’t know, we may not know where to place don’t know in the ranking.
We can, therefore, order the four ranked categories and leave the fitting procedure to
determine where to place the don’t know category as in Example 5.12.2.
The maximisation of a likelihood subject to order restraints is a problem in
isotonic regression, but general solutions for functions such as the likelihoods arising
here do not appear to be available. However, there is one case in which the solution
is immediate. If we maximise without restraints and find that the resulting parameter
estimates satisfy the required order restrictions, then the solutions of the restrained
and unrestrained problems are identical. Modest inversions of the ordering, within
the limits of sampling variation, might be regarded as consistent with the predicted
order. This is clearly a matter where further research is needed.

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

8

Relationships between
latent variables
8.1

Scope

In this chapter we focus on two closely related topics which, in the factor analysis
tradition, are usually referred to as confirmatory factor analysis and linear structural
relations modelling (LISREL). Some work has been done in this area for categorical
or non-normal continuous manifest variables, but we shall concentrate mainly on the
normal model. Nevertheless we shall set the results in the broader framework of the
GLLVM partly to provide a springboard for future research and partly to give greater
insight into the scope and limitations of the models. We shall make no attempt to
provide a comprehensive account of linear structural relations modelling which has
been the subject of several books (Bollen 1989; Bollen and Long 1993; Cuttance
and Ecob 1987; Kaplan 2009; Lee 2007; Loehlin 1989; Mulaik 2009b; Saris and
Stronkhorst 1984) and a number of widely used software packages such as LISREL
(Jöreskog and Sörbom 2006), Mplus (Muthén and Muthén 2010), AMOS (Arbuckle
2006) and EQS (Bentler 2008). Instead our aim will be to locate these methods within
the framework we have adopted in the earlier part of this book. In this way we shall be
able to identify serious issues of identifiability which place limits on the usefulness
of the methods.

8.2

Correlated latent variables

In formulating the GLLVM we supposed the latent variables to be standard normal
and uncorrelated. The normality, we emphasised, was a matter of convention chosen
because of its mathematical convenience. The justification for this lay in the fact

Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

213

P1: TIX/XYZ
JWST070-08

214

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

that the form of the prior (latent) distribution was indeterminate unless arbitrary
constraints were imposed on the conditional distributions of the manifest variables.
More informally, if the latent variable is a construct then we are at liberty to ‘construct’
it to have any distribution we please.
The assumption that the latent variables be uncorrelated might seem less easy
to defend since there seem to be no prior reasons to expect substantively interesting
latent variables to be independent. However, independence of the latent variables
was simply a convenient starting point since we found that oblique rotations in the
factor space generated other models which did have correlated factors. The reason for
not introducing correlated factors in the first place was simply that their correlation
matrix would not have been identified. Different factor structures having different
correlation structures would have yielded exactly the same likelihood. In practice,
therefore, the correlation structure was ‘estimated’ in such a case by choosing that
rotation which seemed most meaningful in substantive terms. There could be no
empirical ground for preferring one rotation to another.
There are, however, circumstances in which there is prior knowledge, or a hypothesis we wish to explore, about the factor structure, which removes the indeterminacy
and so makes it possible to estimate the correlations between the factors as well as
their loadings. This arises when there are constraints on the factor loadings. Most
commonly these are in the form of loadings assumed to be zero but could be in
the form of equalities among subsets of loadings. If the loadings matrix  of the
GLLVM contains zeros in particular positions then they will not, in general, persist
under linear transformations. If the only admissible solutions are those with zeros in
these positions then rotation will destroy that structure.
In these circumstances it may be possible to estimate the correlation matrix
of the factors in addition to the non-zero loadings. We then replace the standard
normal prior distribution with a standard multivariate normal distribution with correlation matrix . In the case of the normal linear factor model the covariance matrix
then becomes
 =  + .

(8.1)

The sufficient condition for the model to be identified is that the number of unknown
parameters on the right-hand side of (8.1) be no larger than 12 p( p − 1) + p, which is
the number of sample variances and covariances. If f is the number of free parameters
in , then on the right-hand side of (8.1) there are
1
f + q(q − 1) + p
2
parameters and this must not exceed 12 p( p + 1).
Software packages, such as LISREL and Mplus, provide maximum likelihood
estimates of the parameters together with standard errors and various other quantities
relevant to inference about the model.

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

RELATIONSHIPS BETWEEN LATENT VARIABLES

8.3

215

Procrustes methods

A variation on the above approach is first to fit an unrestricted model and then to
estimate a rotation matrix M such that the pattern of the matrix of the rotated loadings
is as close as possible to that desired. The correlation matrix for the factors obtained
by this method will then be  = MM . This procedure is very similar to that used in
the search for simple structure when we look for a rotation yielding a loadings matrix
with only one non-zero element in each row. The difference is that in confirmatory
mode the zeros are specified in advance whereas in the search for simple structure
any such solution is acceptable. The name ‘Procrustes’ is taken from the figure in
Greek mythology who adjusted the lengths of his victims to fit his bed. The problem
is a long-standing one to which contributions have been made by Green (1952), Cliff
(1966), Schönemann (1966), Lawley and Maxwell (1971, Sections 6.5 and 6.6) and
Brokken (1983). The Procrustes approach gives a better fit at the price of a poorer
reproduction of the desired pattern of loadings. Current practice uses the readily
available software for estimating the confirmatory factor analysis model with fixed
zeros, as in Section 8.2, rather than Procrustes methods.

8.4

Sources of prior knowledge

There are a number of different circumstances in which prior knowledge about the
structure of the loading matrix may be available, and these give rise to different forms
of analysis.
The first arises when an exploratory analysis shows some loadings to be close
to zero. We may then suspect that those loadings should be exactly zero and wish
to test that hypothesis using a new set of data. A zero coefficient means that the
latent variable to which it is attached exerts no influence on the manifest variable
corresponding to that row of the loading matrix. This may be a plausible hypothesis
because we often find sets of variables for which a one-factor model provides a good
fit. If some or all of these variables were to be embedded in a larger set depending
on other factors we would not expect those items to be influenced by any other factor
introduced in this fashion.
Another circumstance where we may wish to impose constraints is where there
is a dominant general factor. It often happens that the loadings of the first factor
turn out to be large and roughly equal. One might then wish to test the hypothesis
that they were precisely equal. This situation arises with the logit/normal model of
Chapter 4. If, in that case, αi1 = α1 (i = 1, 2, . . . , p) we obtain the random effects
Rasch model. The first component is then simply the sum of the binary indicators.
This has a simple direct interpretation as the ‘number of correct answers’ (in the
educational test context) and this model permits a stronger form of measurement (see
Bartholomew (1996), Section 8.5).
In essence we are seeking empirical justification for using a simpler model by
looking at sub-models nested within a larger family. If there are at least two factors

P1: TIX/XYZ
JWST070-08

216

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

involved, the simplification resulting from constraining the loadings matrix may
enable us to estimate the correlation between the factors as we saw above.

8.5

Linear structural relations models

Another circumstance in which we may wish to estimate the correlation matrix of the
factors is when we are interested in elucidating causal relationships between latent
variables. Such analyses for manifest variables have a long history going back to
Wright (1921) and have been given a new impetus by the development of so-called
graphical models; see, for example, Whittaker (1990) and Cox and Wermuth (1996).
It is a natural extension of such models to suppose that the causal relationships exist
between the latent variables, rather than the manifest variables. As we shall see in
Section 8.6, linear relationships among the latent variables determine the correlation
matrix. Conversely, if we can estimate the correlation matrix we may hope to infer
what linear structural equations have given rise to it.
We begin with a special, but important, case of the general model. Suppose
there are q latent variables y = (y1 , y2 , . . . , yq ). For yj we have a set of indicators
xj = (x j1 , x j2 , . . . , x j p j ), where pj is the number of indicators for yj . If we assume
that the indicators and latent variables are related in the manner of a standard normal
factor model, we may write
xj = μj + λj yj + ej

( j = 1, 2, . . . , q),

(8.2)

where λ j is a column vector of loadings. These equations may be collected together
and written as
x = μ + y + e,

(8.3)

where the vectors x j , μ j and e j are stacked vertically to give x, μ and e respectively.
The matrix  then has the form
⎡
λ1
⎢0
⎢
⎢
=⎢0
⎢ ..
⎣.

0
λ2
0
..
.

0
0
λ3
..
.

...
...
...

0

0

0

...

⎤
0
0⎥
⎥
0⎥
⎥,
.. ⎥
.⎦
λq

(8.4)

which corresponds to what, in another context, we have called simple structure.
Provided that the estimation problem is identifiable we can now proceed to
estimate the parameters, including ,
qthe correlation matrix of the ys. The number of free parameters in  is f = j=1 p j = p and so the problem will not be

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

RELATIONSHIPS BETWEEN LATENT VARIABLES

217

identifiable if
1
1
2 p + q(q − 1) > p( p + 1)
2
2
or q(q − 1) > p( p − 3). If q = 4, for example, p must be as least equal to 6 which
implies an average of at least 2 12 indicators per latent variable.
The foregoing merely tells us something about the intercorrelations of the latent
variables. We may wish to go further and explore the causal relationships which gave
rise to those correlations. Patterns of such relationships are conveniently expressed
as path diagrams, of which the following is a very simple example:

y1
y3
y2
This says that y1 and y2 are both influenced by y3 but are not directly related to one
another. There will be, of course, a correlation between y1 and y2 resulting from their
common dependence on y3 , but there is no causal link. If we are prepared to make an
assumption about the relationships in the path diagram we can link their parameters
to the correlation matrix of the ys and so have a means of estimating them.
Suppose that we assume that y1 and y2 have a simple linear regression on y3
so that
y1 = a1 + b1 y3 + 1

y2 = a2 + b2 y3 + 2 ,

and

with the usual distributional assumptions about the error terms. We are not now free
to specify the means and variances of all the ys but only that of y3 . If we suppose
y3 ∼ N (0, 1) then
E(y1 ) = a1 ,
var(y1 ) =
corr(y1 , y3 ) =
corr(y1 , y2 ) =

b12

+ δ1 ,

b1
b12

+ δ1
b1 b2

,

E(y2 ) = a2 ,
var(y2 ) = b22 + δ2 ,
corr(y2 , y3 ) =

b12 + δ1 b22 + δ2

b2
b22

+ δ2

,

(8.5)

= corr(y1 , y3 )corr(y2 , y3 ),

where δ1 and δ2 are the variances of 1 and 2 , respectively. If we can estimate the
correlation matrix of y then we can estimate the standardised regression coefficients

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

218

May 10, 2011

12:44

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

bi / bi2 + δi (i = 1, 2) of the structural relationships. Further, we can check whether
the last relationship of (8.5) holds and so obtain some idea of the plausibility of
the model.
In rudimentary form, this simple example illustrates the idea behind linear structural relationship modelling and its connection with confirmatory factor analysis.
Before moving on to give a general treatment we make three observations.
First, a distinction has to be made between y3 on the one hand and y1 and y2 on
the other. We are free to choose the distribution of y3 and, in particular, its mean and
variance. There is no such freedom with y1 and y2 because, once y3 is specified, the
distributions of y1 and y2 are determined by the regression assumption. In general,
the free latent variables, whose distributions are arbitrary, are called independent
or exogenous. The remainder are called dependent or endogenous because their
distributions are determined by those of the exogenous variables and the structure of
the system.
Second, since the structural model concerns the latent variables only it can, in
principle, be made the basis of a model for any kind of indicators, whether normal,
categorical or mixed. The structural element makes its presence felt through the
matrix  which is a parameter of the distribution of the ys, not the xs. Essentially
there are two models – the structural model concerning the ys and the measurement
model which relates the xs to the ys.
Third, the model can be described in terms of prior and conditional distributions
in exactly the same way as the models considered in the earlier chapters.

8.6

The LISREL model

We now generalise the simple example of the previous section to the general linear
structural relations model as originally formulated by Jöreskog (see, for example,
Jöreskog (1977) and Jöreskog and Sörbom (1977), which also gives references to
earlier work). In the following section we shall set the model in a yet more general
framework which allows a wider choice of measurement models. In this section the
measurement part of the model is the normal linear factor model.

8.6.1

The structural model

We shall adopt a non-standard notation in order to emphasise the links with our
treatment of earlier models. This is crucial for the important conclusions we wish
to draw later about serious problems of model identification. The latent variables
are divided into two groups – the independent and the dependent variables. The
independent variables are denoted by y1 = (y11 , y12 , . . . , y1q1 ) and the dependent by
y2 = (y21 , y22 , . . . , y2q2 ), with q1 + q2 = q and y = (y1 , y2 ). The structural part of
the model is then
By2 = Gy1 + ζ ,

(8.6)

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

RELATIONSHIPS BETWEEN LATENT VARIABLES

219

with E(y2 ) = 0, E(y1 ) = 0 and E(ζ ) = 0. It is further assumed that ζ is uncorrelated
with y1 and that B is non-singular. The error term ζ has a diagonal covariance matrix
denoted by ζ . The model is said to be recursive if the elements of y2 can be arranged
as a sequence (usually temporal) so that each y2 depends on y1 and ζ , but only on
those other y2 j which precede it in the sequence.

8.6.2

The measurement model

This is the standard factor model x = μ + y + e, but with a special structure. It is
assumed that the xs can be partitioned into two parts x = (x1 , x2 ) of dimensions p1
and p2 respectively ( p1 + p2 = p) such that the members of x1 depend only on those
latent variables in y1 , and those in x2 on those in y2 . In other words, for measurement
purposes, the independent and dependent variables have no common indicators. This
being so, we can write
=

x1
0

0
.
x2

(8.7)

We can similarly partition the error term into two parts, e = (e1 , e2 ), with diagonal
covariance matrices  e1 and  e2 respectively.

8.6.3

The model as a whole

Under these assumptions the covariance matrix of x turns out to be
=

x1 y1 x1 +  1

x1 y1 G B −1 x2

x2 y1 G B −1 x1

x2 (B−1 G y1 GB −1 + B−1 ζ B −1 )x2 +  2

,
(8.8)

where y1 is the covariance matrix of y1 . The matrix (8.8) is obtained by first showing
that , the covariance matrix of the ys, is given by
=

y1

y1 G B −1

y1 G B −1

B−1 G y1 GB −1 + B−1 ζ B −1

(8.9)

and then substituting this into
 =  + .

(8.10)

The parameters are estimated by minimising some measure of the distance
between  as given by (8.10) and the sample covariance matrix S. Unweighted
least squares (ULS), generalised least squares (GLS), generally weighted least

P1: TIX/XYZ
JWST070-08

220

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

squares (WLS), and maximum likelihood (ML) are some of the methods used for
estimating structural equation models (see also Bollen (1989), chapters 4 and 9).
More specifically, the ULS estimator minimises the function
FULS =

1
trace[(S − )2 ].
2

This is a minimisation of the sum of squares of the differences between each element
of the sample covariance matrix and the covariance matrix under the specified model.
The main advantage of the ULS estimation is that it leads to consistent estimates without requiring any distributional assumptions for the data. However, the ULS method
is not invariant to scale transformations and the estimates obtained are different when
a covariance or a correlation matrix is used.
The GLS estimator minimises the function
FGLS =

1
trace[{S−1/2 (S − )S−1/2 }2 ].
2

This criterion minimises a sum of squares of weighted differences between each
element of the sample covariance matrix and the covariance matrix under the
specified model.
The WLS estimator minimises
FWLS = (s − σ ) W−1 (s − σ ),
where W−1 is a p × ( p + 1)/2 positive definite weight matrix, and s, σ are vectors
of the elements in the lower triangle, including the diagonal elements, of S, 
respectively. Any positive definite W will produce consistent estimates which are
not necessarily efficient. To obtain estimates with the same optimal properties as ML
estimates (asymptotic multivariate normality and asymptotic efficiency), choose W
as a consistent estimator of the covariance matrix of s. It is possible to choose W so
that the WLS estimate is the same as the ULS or GLS estimators.
The ML estimate maximises the likelihood or, in the context of the fit
function, minimises
FML = ln |  | + trace(S −1 ) − ln | S | − p.
All the above estimators are consistent, but only the WLS and ML can be asymptotically efficient with asymptotic distributions which are multivariate normal to allow
significance testing.
It is usual in practice to work with the standardised variables, and  then becomes the correlation matrix. The fitting can be done by using LISREL or the other
covariance structure analysis programs mentioned earlier.
If the indicator variables are categorical or non-normal the measurement model
must be replaced by one of the other exponential family models as appropriate.

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

RELATIONSHIPS BETWEEN LATENT VARIABLES

221

No software is currently available for such extensions, but there is no problem,
in principle, about providing it. However, in view of the serious questions of
identifiability which we shall raise in the following sections it may be better to
explore alternative approaches of the kind to be discussed in Section 9.4.

8.7

Adequacy of a structural equation model

One way of testing the adequacy of the specified model is to use the value of
the fit function evaluated at the final estimates. More specifically, in large samples
(n − 1)FGLS and (n − 1)FML have an asymptotic chi-squared distribution when the
model is correct with degrees of freedom equal to 12 p( p + 1) − t, where t is the total
number of free parameters being estimated and p is the total number of observed
variables. For the GLS fit function there is the requirement that the weight matrix be
a consistent estimator of the sample covariance matrix. For the ML fit function the
requirement for the chi-squared test statistic is multivariate normality of the observed
variables when a covariance matrix is being analysed. The chi-squared test statistics
are very sensitive to the sample size and to deviations from normality.
Because of the limitation of the chi-squared statistic mentioned above, alternative
standardised forms of the chi-squared statistic have been proposed in the literature
which are less sensitive to the sample size and to the assumption that the model holds
exactly in the population. Those statistics are known as fit indices. The best-known
fit indices are discussed below. Fit indices are based on the sample and estimated
(reproduced) covariance matrices. There is a bewildering array of such indices. We
enumerate below those that are more robust.
1. The root mean square error of approximation (RMSEA) takes into account
the fact that the model might hold approximately rather than exactly in the
population. Browne and Cudeck (1993) suggested that values smaller than
0.05 indicate a close fit. A value greater than 0.1 indicates a poor fit. A p-value
and a confidence interval can be computed.
2. The comparative fit index (CFI) takes values between 0 and 1. CFI values
close to 1 indicate a very good fit. The CFI computes the relative improvement
in the overall fit of the fitted model compared to a null model; this is usually
the independence model (where the observed variables are assumed to be
uncorrelated).
3. A similar measure to the CFI is the relative fit index (RFI). Values close to 1
indicate a very good fit.
4. The standardised root mean square residual (SRMR) is a standardised
summary of the average covariance residuals. Covariance residuals are
discrepancies between the sample covariance matrix and the reproduced
covariance matrix estimated under the model, assuming that the model is
correct. The smaller the SRMR is, the better the fit. A value less than 0.08
indicates a good fit.

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

222

May 10, 2011

12:44

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

5. For the Tucker–Lewis or non-normed fit index (NNFI) a value of 1 indicates
a very good fit, while a value greater than 1 might indicate overfitting. Values
smaller than 0.9 indicate a poor fit.
6. For Bollen’s incremental fit index (IFI) a value of 1 again indicates a very good
fit, while a value greater than 1 might indicate overfitting. Values smaller than
0.9 indicate a poor fit. The IFI is reported to be less variable than the NNFI.
The CFI, NNFI and IFI are relative fit indices. They compare the chi-squared statistic
of the fitted model with the chi-squared of a baseline model, usually the independence
model (the degrees of freedom of each model are taken into account). Indices such as
the RMSEA are calculated using the chi-squared statistics and the degrees of freedom
of the fitted model.

8.8

Structural relationships in a general setting

It is interesting and instructive to look at the general question of relationships between
latent variables from the perspective taken in Chapter 2. We shall, therefore, retrace
our steps and consider the topics of the earlier part of this chapter from that point
of view.
The fundamental idea is that if the dependencies among a set of manifest variables
x can be explained by their common dependence on a set of latent variables y, then
their joint distribution admits the representation
p

f (x) =

gi (xi | y)dy.

h(y)

(8.11)

i=1

The impossibility of estimating h(y), for example, then followed from the fact that
f (x) remains unchanged by transformations of y. However, by restricting the conditional distributions {gi (xi | y)} to be members of an exponential family we were able
to find functions of x which were sufficient for y independently of any assumptions
about the prior distribution of y. We shall return to the relevance of this fact later.
First we look at how the questions about relationships between latent variables appear
when translated into the general framework.
Simple structure means that each x is dependent on no more than one y. We can
then partition the x vector according to which latent variable the members of the
groups thus formed belong to. Suppose that there are qj xs which depend on yj and
that the xs are so ordered that those depending on y1 come first, those depending on
y2 come second and so on. The product term in (8.11) then decomposes into
q

Qj

j=1

i=Q j−1 +1

gi (xi | y j ),

(8.12)

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

RELATIONSHIPS BETWEEN LATENT VARIABLES

223

where Q j = q0 + q1 + · · · + q j and Q 0 ≡ 0. If the ys are independent we then have
f (x) = f (x1 ) f (x2 ) · · · f (xq ), which reduces the problem to q separate one-factor
models, as it should.
If y is assumed to have a standard normal multivariate distribution with correlation
matrix , then  can be estimated by maximum likelihood. The assumption of simple
structure somewhat simplifies the likelihood but, in particular cases, there may be
considerable further simplifications as we shall see.

8.9

Generalisations of the LISREL model

The core of the LISREL model is the relationship between the dependent (endogenous) and the independent (exogenous) latent variables. This specifies the distribution
of y2 given y1 which, in its most general form, may be written h(y2 | y1 ). We thus
replace h(y) in (8.11) by h(y1 ) h(y2 | y1 ).
The measurement part of the model supposes that the indicators divide into two
groups – those that depend on y1 and those that depend on y2 . This means that the
product in (8.11) factorises into two parts:
p

p1

gi (xi | y) =
i=1

p

gi (xi | y1 )

gi (xi | y2 ).

(8.13)

i= p1 +1

i=1

This implies that f (x) admits the representation
p1

f (x) =

h(y1 )h(y2 | y1 )

p

gi (xi | y1 )
i=1

gi (xi | y2 ) dy.

(8.14)

i= p1 +1

If all the distributions {gi } are members of the exponential family, the first product
within the integral will give rise to q1 statistics sufficient for y1 and the second product
to q2 which are sufficient for y2 . The parameters which appear in the products in the
integrand relate to the measurement part of the model. Our earlier work suggests that
these latter estimates will not depend critically on the choice of prior. However, this
fact has an unfortunate consequence for our present purposes. We have already seen
that the inherent arbitrariness of the prior means that we can say very little about
its form. If, as we showed in the case of the latent class and latent trait models, the
difference between a two-point and a normal prior is scarcely distinguishable, what
hope is there for establishing the form of the relationships between a set of latent
variables? The structural part of the present model is contained in the conditional
distribution h(y2 | y1 ), and especially in E(y2 | y1 ). Transformations of y1 and y2 which
leave f (x) unchanged could do considerable violence to the supposed form of the
relationships between the ys. It is important to emphasise that the lack of identifiability
we are talking about here is not concerned with parameter values, in the usual sense,
but with different models which have the same empirical consequences.

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

224

May 10, 2011

12:44

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

This simple analysis shows the LISREL model of Section 8.6 in a different light.
It demonstrates that, even if a linear model fits the data, there may be other models
which fit them equally well. We now illustrate this point by means of examples.

8.10

Examples of models which are indistinguishable

We shall construct two models, in which all the latent variables are categorical, whose
covariance structure is identical to a given LISREL model. In order to do this we
shall represent the categorical variables by indicator variables taking the values 0 and
1. In the measurement part of the model we shall suppose that the manifest variables
depend linearly on these indicators. The covariances of the manifest variables will
then be functions of the covariances of the indicators. Two models will then be
indistinguishable if their latent indicators have the same covariance matrix. We do
not, therefore, need to specify the details of the measurement sub-model; all that
is needed is the assumption that the manifest variables are continuous with linear
regressions on the indicators of the latent variables. The alternative models are thus
specified by the conditional distribution of y2 on y1 . Both examples are adapted from
Bartholomew (1993).
Model I is a standard LISREL model with two independent latent variables, y11
and y12 , and one dependent latent variable, y2 . The structural sub-model is
y2 = G 1 y11 + G 2 y12 + ζ,

(8.15)

where ζ has mean zero, variance , and is uncorrelated with y11 and y12 ; y11 and y12
are standard normal with correlation coefficient 12 .
Model II has one independent latent variable, y1 , and one dependent variable,
y2 . The dependent variable, y2 , is binary with outcomes coded 0 and 1; y1 has three
categories which are coded by a bivariate indicator u = (u 1 , u 2 ) taking the values
(1, 0), (0, 1) and (0, 0). The joint distribution of y2 and u, specified by the marginal
distribution and the conditional distribution of y2 given u (the structural model), is
set out below:
y2 \ u
1
0

(1, 0)

(0, 1)

(0, 0)

p1 π1
p1 (1 − π1 )

p2 π2
p2 (1 − π2 )

p3 π3
p3 (1 − π3 )

p1

p2

p3

π̄
1 − π̄
1

From this table we can find the covariance matrix of (y2 , u 1 , u 2 ). To facilitate comparison with model I, we shall first standardise u1 and u2 since the LISREL formulation
assumes the independent latent variables to be standardised.
Model III is the same as model II except that the trichotomous independent latent
variable is replaced by two binary variables, y11 and y12 . The joint distribution is
again specified by the marginal distribution of the independent variables and the

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

RELATIONSHIPS BETWEEN LATENT VARIABLES

225

conditional distribution of the dependent variable, as follows:
P{y11 = i, y12 = j} = pi j

(i, j = 1, 2)

and
P{y2 = 1 | y11 = i, y12 = j} = πi j

(i, j = 1, 2).

The joint distribution is then as follows:
y2 \ (y11 , y12 )
1
0

(1, 1)

(1, 0)

(0, 1)

(0, 0)

π11 p11
(1 − π11 ) p11

π10 p10
(1 − π10 ) p10

π01 p01
(1 − π01 ) p01

π00 p00
(1 − π00 ) p00

p11

p10

p01

p00

π̄
1 − π̄
1

Again we shall standardise the independent variables before computing the
covariance matrix.
The three covariance matrices which result are as follows: for model I,
⎡

1

⎢
⎣

12

G2 + G1

G2 + G1

12

12

1
G1 + G2

⎤
12

⎥
G 1 + G 2 12
⎦;
2
2
G 1 + G 2 + G 1 G 2 12 + 

12

for model II,
⎡
1

⎢
⎢
⎢ 
⎢
p1 p2
⎢−
⎢
(1
−
p
1 )(1 − p2 )
⎢
⎢

⎣
p2
(π2 − π̄ )
1 − p2


−

p1 p2
(1 − p1 )(1 − p2 )
1


(π1 − π̄ )

p1
1 − p1



⎤
p2
1 − p2 ⎥
⎥
⎥

p1 ⎥
⎥;
(π1 − π̄ )
1 − p1 ⎥
⎥
⎥
⎦
π̄ (1 − π̄ )

(π2 − π̄ )

and for model III,
⎡
1

⎢
⎢
⎢
⎢
p11 − p.1 p1.
⎢√
⎢ p (1 − p ) p (1 − p )
.1
.1 1.
1.
⎢
⎢

⎣
p.1
(π̄.1 − π̄ )
1 − p.1

√

p11 − p.1 p1.
p.1 (1 − p.1 ) p1. (1 − p1. )
1

(π̄1. − π̄ )

p1.
1 − p1.



⎤
p.1
1 − p.1 ⎥
⎥
⎥

p1. ⎥
⎥,
(π̄1. − π̄ )
1 − p1. ⎥
⎥
⎥
⎦
π̄ (1 − π̄ )

(π̄.1 − π̄ )

P1: TIX/XYZ
JWST070-08

226

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

where p1. = p10 + p11 , p.1 = p01 + p11 , π̄1. = ( p11 π11 + p10 π10 )/ p1. and similarly
for π̄.1 .
The matrix for model I involves four unknowns, that for model II has five and that
for model III has seven. One might, therefore, expect to be able to find many examples
of models II and III having the same covariance matrix as model I. However, there
are constraints imposed by the fact that the defining probabilities of models II and
III must lie in the unit interval and, in model II, the covariance of u1 and u2 must
be negative. Nevertheless it is still easy to find models of all three types which yield
identical matrices. For example if, for model I,
G 1 = 0.189,

G 2 = 0.094,

12

= −0.500,

 = 0.204,

for model II,
p1 = p2 = p3 = 0.333,

π1 = 0.6,

π2 = 0.4,

π3 = 0.2,

and for model III,
p11 = p00 = 0.125,
π11 = 0.700,

π10 = 0.489,

p10 = p01 = 0.375,
π01 = 0.300,

π00 = 0.132,

we find in each case,
⎡
0.240
cov(y) = ⎣0.142
0

0.142
1.000
−0.500

⎤
0
−0.500⎦ .
1.000

These examples are a simple extension of the idea used in Section 6.14 showing
how latent profile models and linear factor models could have identical covariance
structures.
As a result of a LISREL analysis we can therefore say whether or not a model
with a specified linear latent structure is consistent with the data, but we cannot go on
to infer that it is the only latent structure model having that property. Unless we have
further non-empirical information favouring the linear model, no substantive claims
for it can be justified. Just as the latent variables themselves are constructs, so also is
the model connecting them.
The ready availability of good software for LISREL models has made it very easy
to carry out analyses, but this has not been matched by a corresponding concern with
the interpretation which the results will bear. However, there have been a number of
moves to provide alternative methods which allow a rather more critical approach to
the data. These are reviewed in Section 9.5.

P1: TIX/XYZ
JWST070-08

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

RELATIONSHIPS BETWEEN LATENT VARIABLES

8.11

227

Implications for analysis

This entire chapter has been concerned with estimating the covariance matrix of a
set of latent variables, or of parameters which are functions of these covariances.
Almost all of the discussion has been concerned with the normal factor model and
its generalisations, although we have shown that the extension to all models derived
from the exponential family is almost immediate. In the case of confirmatory factor
analysis the theory required is a straightforward development of that already given
for the normal factor model for which good software is available.
When we move on to LISREL models which specify a linear structure for the
latent variables, serious problems emerge. These arise because the prior distribution
of the latent variables, which incorporates the structural part of the model, is poorly
determined by the data. In addition to the possible lack of identifiability of the linear
model itself, we demonstrated that distinct models involving different numbers of
categorical latent variables could give rise to exactly the same covariance structure.
For these reasons, any inference made from a structural model rests heavily on the
substantive assumptions made about the nature of the latent variables and their relationships. The essence of the structural part of these assumptions is often expressed in
terms of a path diagram of postulated causal linkages. These may place quite severe
constraints on the form of the conditional distribution h(y2 | y1 ) of (8.14) and hence
reduce the degree of arbitrariness. Nevertheless it remains unclear how far the key
structural relationships would survive arbitrary transformations of the latent space.
The global character of the analysis, in which the measurement and structural models
are conflated, makes it difficult to diagnose the source of any misfit.
A partial way out of this difficulty is offered by the proposal to separate the
two stages of the analysis. First we scale the indicator variables for each latent
variable, using appropriate measurement models, and then check their validity before
proceeding to the second stage. We use the score thus derived as the raw material for
the structural analysis. The fact that our general approach yields scores (components)
which are sufficient for the latent variables means that no information about the latter
is lost in the process.
There are two problems with this course of action. One is that the correlations among the components always underestimate the correlations among the latent
variables. However, this underestimation is slight if a reasonable large number of
indicators are used for each latent variable. In any case, adjustments can be made to
correct the underestimation. The estimation of regression coefficients can, as Croon
and Bolck (1997) have shown, be made to a good approximation by using the Bartlett
scores which are scaled components. This form of analysis needs further investigation, but appears to us to offer a better way forward.
The second difficulty is that we cannot even be sure that our individual measurement models are genuinely scaling a continuous latent variable. We saw in Chapter 6
how difficult it was to discriminate between a continuous normal prior and one with
probability masses concentrated at a few points.
This last point suggests abandoning the structural part of the model altogether.
After all, as Sobel (1994) pertinently points out, it is not clear how a mere construct

P1: TIX/XYZ
JWST070-08

228

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:44

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

can cause anything. Once the measurement sub-models have yielded their sufficient
statistics, we can treat these statistics as variables in their own right. They are usually
linear functions of the indicators and, as such, it is often possible to name what they
are measuring in exactly the same manner as we do with principal components. It
is then reasonable to make these quantities the basis of a structural path analysis
without any reference to an underlying latent variable model. Such a model is,
after all, no more than a construct and it is of no substantive concern whether or
not regressions and correlations between components correctly estimate its purely
hypothetical parameters. When we add to the argument the fact that working with
components enables us to explore the form of the structural relations, the case against
the traditional LISREL analysis is formidable.
It might appear that our treatment of structural relationships has been inconsistent
with our use of posterior analysis in earlier chapters. There we argued that, after the
xs have been observed, inferences about y should be made conditional on x. Thus
we were led to construct measures of y from the posterior distribution h(y | x) – in
particular, its expectation E(y | x). If this is characterised as a Bayesian approach
then what we have been using in this chapter is a frequentist approach. We have been
treating components as functions of x and determining their properties by reference
to the unconditional distribution of x.
A seemingly more consistent approach to, for example, the regression of y2
on y1 discussed in Section 9.5 would have been to consider E(y2 | y1 , x). This is the
regression of y2 on y1 , after x has been observed. However, this is not what is required
in the present instance. We used E(y | x) to scale an individual for whom the indicator
x has been recorded. In the problems of this chapter we are not interested in what
can be said about the relationship between y1 and y2 for any particular individual.
We want to know about the relationship for the whole population of individuals, and
so the distribution unconditioned by x is relevant. That is, we are interested in the
structure of the prior distribution h(y), as we noted in Section 8.9.
We conclude with the observation that, in the model-based approach to statistical
methodology, everything is conditional on the model being true. When we come to
models for relationships between latent variables we have reached a point where so
much has to be assumed that one might justly conclude that the limits of scientific
usefulness have been reached, if not exceeded.

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

9

Related techniques for
investigating dependency
9.1

Introduction

One of the main purposes of a latent variable model is to explain the dependencies
among a set of manifest variables. It does this by showing that if the latent variables are
held fixed, then the manifest variables are conditionally independent. There are other
methods which have the same aim, without (explicitly) introducing latent variables.
In this chapter we review a number of them and compare them withe corresponding
latent variable models. We begin with principal components analysis.

9.2
9.2.1

Principal components analysis
A distributional treatment

In this section we shall introduce several ways of looking at principal components
analysis (PCA). We begin with one which facilitates the comparison with factor
analysis, with which it is sometimes confused. In fact, PCA is sometimes regarded as
one method of factor analysis and it is so treated in, for example, the SPSS software.
PCA was introduced by Hotelling (1933) who appears to have regarded it as an
alternative to factor analysis, but it was quickly absorbed into the armoury of factor
analysts among whom it was sometimes known as ‘Hotelling’s method’. One of the
purposes of this section is to show its relationship to factor analysis, as we have
developed it in this book, and to identify the circumstances under which the two
methods are very similar. Latent variable models achieve their aim in two stages:
first, they introduce independent, or at worst, highly structured random variables;
secondly, they attempt to interpret them individually. Independent random variables
Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

229

P1: TIX/XYZ
JWST070-09

230

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

are easier to interpret than dependent variables, and it is there that the object of
the analysis lies. Throughout this section, unless otherwise stated, we shall assume
manifest variables to be standardised with zero mean and unit standard deviation.
According to the normal linear factor model, the joint distribution of the manifest
variables is multivariate normal. This will be our starting point for the development
of PCA.
A set of correlated normal variables can be linearly transformed into a set of
independent normal variables in a routine way as follows. If the manifest variables
are listed in a vector x of dimension p then we may choose
y = Ax.

(9.1)

We require the ys to be uncorrelated, which means that cov(y) must be diagonal –
that is,
cov(y) = E(yy ) = E(Axx A )

(9.2)

is diagonal. If, as before, we denote the covariance matrix of x by , then A must be
such that AA is diagonal. Since  is a symmetric non-negative matrix it may be
expressed in the form
 = M M,

(9.3)

where M is an orthogonal matrix of eigenvectors and  is a diagonal matrix of eigenvalues which are all non-negative. If, therefore, we choose M = A,  is diagonalised.
The covariance matrix of y will therefore be
cov(y) = .

(9.4)

We may list the variables in y so that the eigenvalues are in decreasing order of magnitude. The ys are called the principal components, so by making this transformation we
have arrived at a new set of independent normal variables whose relative importance
is reflected in their associated variances given, for yi , by δi (i = 1, 2 . . . p).
The principal components are given by
y = Mx.

(9.5)

To make the comparison with factor analysis we need to express x in terms of y,
which is easily done. Because M is an orthogonal matrix its inverse is the same as its
transpose. Thus
x = M y.
This is still not in a form suitable for our purposes because in the factor model the
factors, corresponding to the principal components, all have unit variance. Thus, to

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

INVESTIGATING DEPENDENCY

231

facilitate the comparison we need to make a further transformation, scaling the ys so
that each has unit variance. This is achieved by pre-multiplying y by −1/2 . So if
z = −1/2 y, we shall have
x = M 1/2 z = Bz.
Suppose we now arbitrarily partition z into two parts such that zq consists of the first
q elements of z and z p−q the remainder. Similarly, we partition B so that Bq consists
of the first q columns of B and B p−q the remainder. We may then write
x = Bq zq + e∗ ,

where e∗ = B p−q z p−q .

(9.6)

The extent to which (9.6) agrees with (3.3) depends on the comparison of e of that
equation and e∗ of (9.6). It is clear that e∗ is independent of Bq zq because there are no
zs in common. However, the elements of e∗ are not, in general, independent because
they are all functions of z p−q . If the first few principal components account for a high
proportion of the total variation, the error terms will be small and there will be little
difference between principal components and the corresponding factor model. This
fact justifies the common use of PCA to determine the value of q in factor analysis.
The number of eigenvalues judged to be ‘large’ will correspond approximately with
the number of factors needed.
There is another way of demonstrating the similarity of factor analysis and PCA
which arises out of older approaches to factor analysis which were not model-based.
This is on the same lines as the methods of fitting without normality assumptions
which we used in Chapter 3. The idea there was to make the actual and theoretical
values of the correlation matrix as close as possible. According to the factor model
we are looking for a matrix  such that
R −  =  ,

(9.7)

where R is the correlation matrix of the xs and  is the diagonal matrix of residual
variances. If  is known, the equation is satisfied exactly because R −  is symmetrical, semi-definite and so can be expressed exactly in the required form. In the case
of  = 0 we are back to principal components. As we have already seen, methods of
factor analysis often estimate the loadings for a fixed value of  and, from that point
on, the fitting can be regarded as carrying out a PCA on the matrix R − . PCA can
then be regarded as one of a family of methods which chooses the communalities
(the complements of the ψs) to be unity.
To complete our discussion of the relationship between PCA and factor analysis
we show that, in certain circumstances, PCA provides the solution of the estimation
problem for a particular member of the normal linear factor model. This member,
which is known as the isotropic normal model, has ψi = ψ for i = 1, . . . , p. This
particular model appears to have been first considered in this connection by Lawley
(1953) and Anderson and Rubin (1956). A further account is given in Basilevsky

P1: TIX/XYZ
JWST070-09

232

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

(1994, pp. 361–362), and the whole problem has been considered in a broader context
by Tipping and Bishop (1999).
As in our earlier treatment of the normal linear factor model with p variables and
q factors, we are considering the model having covariance structure
 =  + ψI.

(9.8)

We first establish a relationship between the eigenvalues and vectors of  and  .
The eigenvalues and vectors of  are given by
( − δI)u = 0.

(9.9)

Substituting for , this equation is equivalent to
( + ψI − δI)u = [ − (δ − ψ)I]u.

(9.10)

It is clear from (9.9) and (9.10) that  and  have the same eigenvectors and that
the eigenvalues of  are less than those of  by the amount ψ. However, because 
and  are both positive definite matrices, their eigenvalues must be non-negative.
This requirement is not met if, for any eigenvalue, δ < ψ. Hence the equivalence
required by (9.10) implies that δi = ψ for all those eigenvalues of  which are less
than or equal to ψ. This means that the p−q smallest eigenvalues of  must be equal.
It follows that we may write
⎛

(δ1 − ψ)1/2
⎜
0
⎜
.
 = U⎜
⎜
⎝
.
0

0
(δ2 − ψ)1/2
.
.
0

0
0
.
.
.

.
.
.
.
.

⎞
0
⎟
0
⎟
⎟,
.
⎟
⎠
.
(δq − ψ)1/2

(9.11)

where U is the p × q matrix of eigenvectors associated with the q largest eigenvalues
of  and δq+1 = δq+2 = · · · = δ p = ψ.
In practice, we shall not know . The sample covariance provides an estimate,
of course, but it is highly unlikely that its p−q eigenvalues will be exactly equal.
However, the maximum likelihood estimator of this common value, which is equal to
ψ, can easily be determined, as shown, for example, in Tipping and Bishop (1999).
It turns out that
p
ˆ
i=q+1 δi
,
(9.12)
ψ̂ =
p−q
where δˆi is the ith eigenvalue of the observed covariance matrix.
We are thus able to estimate the parameters of the linear normal isotropic model
from the eigendecomposition of the covariance matrix and without any iteration. If
we happened to encounter a sample p × p covariance matrix whose p−q smallest

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

INVESTIGATING DEPENDENCY

233

eigenvalues were roughly equal, then the principal components analysis of that matrix
would be essentially the same as that for the corresponding isotropic normal factor
model with q factors.
It is worth mentioning that the foregoing analysis treats q, the number of factors,
as fixed. The larger q becomes, the greater will be the likelihood until we reach the
limiting case when q = p. At that point the factor model becomes degenerate and it
and PCA coincide.
Our main interest here has been in showing the close link between PCA and
factor analysis. The emphasis of Tipping and Bishop (1999) was rather different.
They wished to show that PCA could be regarded as a model-based technique and
that methods already available for factor analysis could be used to obtain principal
components. One might pause to consider why this should be desirable since determining principal components is, if anything, rather simpler than the iterative methods
usually needed for factor analysis. The reason is that methods are available for
model-based techniques which are useful in applications which lie beyond the range
of the problems considered here. For example, there are ways of dealing with missing
values which presuppose a probability model, and the authors’ interest in image compression and visualisation involving complex projection methods requires multiple
PCA models. It is when moving in such directions that the benefits of a model-based
formulation are realised.

9.2.2

A sample-based treatment

The foregoing treatment was designed to show the close link between PCA and factor
analysis at a distributional level. This is not the usual approach, but it may be linked to
the following sample-based treatment as follows. In this case we start with a sample
of n multivariate observations as set out in the matrix below. However, the prime
object now is not to make inferences about the population from which the sample
been drawn. If it was we could, of course, estimate the correlation matrix and from it
the coefficients of the components, and then make inferences about their values, but
our main concern now is with the structure of the data.
We suppose that a random sample of n observations has been obtained on p
variables. These may be set out in an n × p data matrix,
⎛

x11
⎜ x21
⎜
X=⎜
⎜ .
⎝ .
xn1

x12
x22
.
.
xn2

.
.
.
.
.

.
.
.
.
.

⎞
x1 p
x2 p ⎟
⎟
. ⎟
⎟.
. ⎠
xnp .

(9.13)

The rows may be regarded as the coordinates of n points in p-dimensional space,
and from that perspective PCA is concerned with the dimensionality of the space
occupied by the points. More exactly, we ask what is the smallest dimensional space
into which the points can be adequately fitted.

P1: TIX/XYZ
JWST070-09

234

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

The procedure is first to find a linear combination of the xs which accounts for as
much of the total variation of the xs as possible. This is the first principal component.
The second is a linear combination of the xs which is uncorrelated with the first
component and accounts for as much of the remaining variance as possible. The
extraction proceeds in this fashion, forming at each stage a linear combination which
is uncorrelated with all its predecessors and accounts for as much of the remaining
variance as possible.
By basing PCA on correlations we have, implicitly, measured all variables in units
of their standard deviations. It would be perfectly possible to carry out the analysis
with different units of measurement for each variable basing the analysis on the
covariance matrix. However, it is important to recognise that the resulting principal
components will depend on the particular units used. The technique is, therefore,
only likely to be practically useful when the units are arbitrary and so can reasonably
be scaled by the standard deviations.
Example 9.2.1 We use Example 3.17.2 to make a comparison between solutions
produced by factor analysis and principal component analysis. A PCA was carried
out on the correlation matrix given in Table 3.8 and produced the eigenvalues shown
in Table 9.1. Only the first two eigenvalues are greater than 1 and together they
explain 80.5% of the total variance of the manifest variables. This is also shown in
the scree plot in Figure 3.2. When the factor analysis model was fitted we also adopted
a two-factor solution as being the most interpretable for the correlation among the
eight variables. The first two component loadings and the factor loadings from the
two-factor analysis model already given in Table 3.10 are presented in Table 9.2.
The loadings of the first component are all large and positive, measuring a general
state of physique. The second component is a contrast between the first four items
(lankiness) and the last four (stockiness). Since the aim is to compare the PCA and
factor analysis solutions, we orthogonally rotate both solutions to achieve a match.
The rotated solutions are very close to each other, indicating that PCA and factor
analysis can produce in some cases very close loadings.

Table 9.1 Physical data: variance explained by each
principal component.
Component
1
2
3
4
5
6
7
8

Variance

%

Cumulative %

4.67
1.77
0.48
0.42
0.23
0.19
0.14
0.10

58.41
22.14
6.01
5.27
2.92
2.33
1.72
1.21

58.41
80.55
86.56
91.83
94.74
97.08
98.79
100.00

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

INVESTIGATING DEPENDENCY

235

Table 9.2 Physical data: unrotated and matched PCA and factor analysis (FA)
solutions.
Unrotated solution
PCA
Variable
1
2
3
4
5
6
7
8

9.2.3

Matched solution

FA

PCA

FA

α̂1

α̂2

λ̂1

λ̂2

α̂1∗

α̂2∗

λ̂∗1

λ̂∗2

0.859
0.842
0.813
0.840
0.758
0.674
0.617
0.671

−0.372
−0.441
−0.459
−0.395
0.525
0.533
0.580
0.418

0.885
0.939
0.907
0.876
0.304
0.254
0.192
0.325

0.219
0.108
0.109
0.184
0.905
0.756
0.739
0.598

−0.864
−0.848
−0.820
−0.846
−0.750
−0.666
−0.608
−0.665

−0.359
−0.429
−0.447
−0.383
0.536
0.543
0.589
0.428

−0.856
−0.838
−0.812
−0.829
−0.759
−0.634
−0.573
−0.604

−0.315
−0.437
−0.418
−0.339
0.579
0.484
0.504
0.313

Unordered categorical data

There is yet another way of looking at PCA which helps us to see its link with the
latent variable models for categorical data discussed in Chapter 5. This starts by
considering how the idea of PCA might be carried over when the xs are categorical.
This approach starts from what is known as the singular value decomposition of the
data matrix X of standardised observations which we have taken as being n × p and
having rank p – this may be expressed in the form
X = U1/2 V ,

(9.14)

where 1/2 is a positive definite diagonal matrix whose elements are known as the
singular values of the n × p matrix X; U and the p × p matrix V have orthogonal
columns. It follows that
(1/n)X X = (1/n)VV .

(9.15)

From this expression it is clear that the singular values are the square roots of
the eigenvalues of the correlation matrix of the xs which, in turn, are the variances of the principal components. The matrix V is the matrix of eigenvectors of
the covariance matrix, and we have seen that these provide the coefficients in the
principal components.
If the xs are categorical, we can still write the data matrix in a form similar to that
in (9.13), but the columns have to be treated differently as in Chapter 5. The value xij
is replaced by an indicator vector with one element for each category. In this vector a
1 indicates the category into which the individual falls and the other elements are 0.
The relationship between PCA and the singular value decomposition worked
because the xs were standardised so that the matrix (1/n)X X was the correlation

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

236

May 10, 2011

12:47

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

matrix. If the matrix is constructed from categorical data we first standardise X for
scale by dividing each column by the corresponding diagonal element in (1/n)X X,
then standardise each column of X by its mean. Now X X becomes a standardised
‘Burt’ matrix of all the two-way contingency tables and provides one way of carrying
out multiple correspondence analysis; see Greenacre (1984, p. 140).
It thus appears that the latent variable model of Chapter 5 stands in a similar relationship to multiple correspondence as does normal linear factor analysis to PCA.
However, the connection goes deeper. In Sections 5.6 and 5.7 we derived approximations to the maximum likelihood estimators for the polytomous logit model, and these
turned out to be the same as the multiple correspondence scores – as we noted for the
case of binary data at the end of Section 5.7. We know of no work on the closeness
of this approximation. If it turned out that the approximation was very good, there
would be no need for the much more difficult maximum likelihood estimation for the
polytomous ordered model. In any event the link at a conceptual level is remarkable.

9.2.4

Ordered categorical data

Multiple correspondence analysis makes no use of any ordering of the categories.
After the event, of course, the scores allocated to the categories would indicate an
ordering and those allocated to the principal dimension might reveal an unsuspected
ordering. However, when the ordering is available a priori one would hope to make
explicit use of that information in the analysis exactly as we did in Section 5.8.4. By
assuming, for example, that there was a normal distribution underlying the categories
one could estimate the correlation matrix using, say, the mid-points of each group.
From that stage it would be possible to carry out a principal components analysis
and so to make some inference about the dimensionality of the data. However, to
construct the actual principal components the best we could do would be to take these
mid-points, substituting them into the formulae for the components. This could give
no more than a coarse approximation, and one might well prefer to use the full latent
variable model.
An alternative approach to finding principal components for categorical variables
is described in Linting et al. (2007). This is actually described as a method of nonlinear PCA but it repeatedly assigns scores to categories and then performs a standard
PCA, treating the scores as if they were values of a variable. These scores may be
chosen in variety of ways according to the assumptions one wishes to make about
the variables which are supposed to underlie the categories. In essence, however, the
method involves a principal components analysis of the assigned scores.

9.3

An alternative to the normal factor model

A key idea underlying all of the methods in this book, and reiterated at the beginning
of this chapter, is that the interdependence of a set of variables may be explained by
supposing them to depend on latent variables. This was demonstrated by showing that,
if the latent variables were held fixed, the interdependence of the manifest variables

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

INVESTIGATING DEPENDENCY

237

vanished. It is then tempting to regard the latent variables as real, in some sense, and
to suppose that the causal relationships with the manifest variables are also real. This
is not a legitimate conclusion because there may be other models which may have
exactly the same observable consequences. We now describe one such model which
is almost as old as factor analysis itself but is still little known.
The origin of factor analysis is usually dated to a seminal paper by Spearman
(1904). He proposed and championed what was described as the ‘two-factor’ model
which supposed that an individual’s score on a test item was made up of two parts –
a common part which affected all individuals and a specific part peculiar to that individual. He described the common part as general ability, usually denoted, following
Spearman, by g. Expressed in modern terms, this becomes what we would now call
the normal linear one-factor model. Spearman’s specific factor has now become the
error term of the standard model. Thurstone and others subsequently generalised this
and it is now subsumed into the normal linear factor model. There is little doubt that
Spearman regarded g as measuring some real characteristic of the brain; initially he
referred to this as ‘mental energy’, but later he was less specific about what it might
be. Nevertheless it continues to lie at the foundation of much work in the intelligence
field (see, for example, Jensen (1998)).
A decade later Godfrey Thomson challenged Spearman’s interpretation,
(Thomson 1916) and the debate between them continued, inconclusively, for 30 years.
Thomson’s approach was to propose another model which predicted exactly the same
correlation matrix but implied no general factor. He thereby intended to show that
if at least two models predicted the same correlation matrix then neither could be
interpreted as being the ‘truth’. Thomson was handicapped by the fact that he relied,
initially, on sampling experiments conducted with dice and, latterly, on an incomplete
mathematical argument. The valid point which he was making seems to have been
missed by subsequent workers who, without exception, followed Spearman. Thomson
never claimed that Spearman’s model was wrong, merely that it was not unique.
Initially, Thomson spoke of his sampling theory, but we shall use the term he
introduced subsequently, referring to it as the bonds model. The essence of the model
is as follows:
1. The relevant part of the brain involved in answering test items consists of N
bonds. (Thomson did not know what a bond might be, though he often spoke
of neural arcs.)
2. When an item is attempted a sample of bonds is selected at random (the harder
the item the more bonds will be needed). A bond which is selected may be
described as active or switched on.
3. Each bond contributes the same (expected) amount to the score.
Since Thomson’s death, the bonds model seems to have been ignored until a
version, in modern dress, was given in Bartholomew (2007). It may be written
xi = ai1 ei1 + ai2 ei2 + · · · + ai N ei N ,

(9.16)

P1: TIX/XYZ
JWST070-09

238

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

where ei j ∼ N (μ, σi2 ), Pr(ai j = 1) = πi and Pr(ai j = 0) = 1 − πi (i = 1, . . . , p).
The es and as are mutually independent. In the same paper it was shown that the
correlation matrix of the xs was of exactly the same form as produced by Spearman’s
g-model.
This result can be generalised by supposing that each test score is arrived at by
summing the outcome of k independent ‘passes’. In practice each pass is supposed
to focus on a different aspect of the item – its verbal or spatial aspects, for example.
The final score is then the sum of the scores for the individual passes:
xi = xi1 + xi2 + · · · xik

(i = 1, 2, . . . p).

(9.17)

In this formula a second subscript is added to the xs to denote the pass. If the passes
are independent it is easy to determine the correlation structure. The first step is to
calculate the relevant expectations conditional on the bonds which are selected and
then take expectations over selections. This shows that the covariances are identical
to those for the normal linear factor model. This was done in Bartholomew et al.
(2009), for the case of k passes. If πi j is the probability that any bond is selected for
the ith item on the jth pass, the correlation coefficient of xi and xj is
k
corr(xi , x j ) =

k

r =1

r =1

k

=
r =1

⎛

πri πr j
k

πri

r =1

πri
⎝
k

r =1 πri

πr j

⎞

πr j
k

⎠

r =1 πr j

k

=

λri λr j

(i, j = 1, 2, . . . , p).

(9.18)

r =1

This has exactly the same form as the correlation for the linear factor model originating
with Spearman, as we intended to show. Using this formula, we can easily move from
the π s of the bonds model to the λs of the factor model. However, if we move in
the opposite direction, it may happen that some of the π s turn out to be negative. In
such cases there is no bonds model equivalent to the factor model, but it may still be
possible to achieve positivity by rotating the factor solution.

9.4

Replacing latent variables by linear functions of
the manifest variables

In Chapter 8 we moved away from the determination of a set of latent variables, which
could be regarded as explaining the correlations among a set of manifest variables,
to the study of the relationships among the latent variables themselves. The principal

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

INVESTIGATING DEPENDENCY

239

tool, which is widely used for this purpose, is structural equation modelling which
we considered briefly in Chapter 8. In particular, we were interested in exploring
causal relationships when there was a temporal ordering of the latent variables. In
this section we explore other ways of achieving much the same end without explicitly
introducing latent variables.
Earlier in this chapter we discussed PCA, which is a way of determining linear
combinations of manifest variables which can be given a meaning in their own right.
It is natural to wonder whether it might be better to investigate causal relationships
between such linear combinations rather than the unobservable latent variables which
they purport to indicate. There is a ready-made technique in path analysis which
aims to map the pattern of causal relationships between a set of individual manifest
variables. In a similar manner we might treat the aforementioned linear combinations
in the same way. Some such attempts have been made, and it is instructive to consider
them here.
One of the earliest attempts is known as soft modelling or partial least squares
(PLS). This was proposed by Wold (1975, 1980, 1982). A recent, extended, treatment
is given in Esposito Vinzi et al. (2010). This approach makes no use of a probability
model, though it is very similar to LISREL in the way it conceptualises the problem.
What are called ‘blocks’ of manifest variables are selected from which a linear
estimate of each underlying latent variable is to be ‘estimated’. These estimates are
then used to explore the linear relationships between the latent variables which they
represent. The procedure is iterative. At one stage weights are estimated for the
manifest variables within each block and, at the next, regression analysis is used on
the measures thus formed. The method does not appear to have undergone further
theoretical development, but has commended itself as a practical tool. A useful
summary is given by Schneeweiss (1993), whose main purpose is to show that under
certain circumstances the PLS method and the LISREL model give very similar
results. Since the PLS method does not depend on a model there is no way of testing
its adequacy and, like LISREL methods, it provides a global analysis in which the
measures and their relationships are estimated simultaneously and interdependently.
It does not, therefore meet the requirement of greater transparency.
A number of writers have expressed unease about LISREL analysis and have
sought some means of making the implications more transparent. An obvious route
is to separate the measurement and the structural modelling aspects. One could then
examine the adequacy of the measurement model before going on to look at the relationships between the latent variables. Strategies of this kind appear to have been first
proposed by Burt (1973, 1976) and taken up by Hunter and Gerbing (1982), Anderson
and Gerbing (1988, 1992) and Lance et al. (1988). The idea here is to go one step
further than Wold by replacing the latent variables by linear functions suggested by
a probability model in which the variables are assumed to have normal distributions.
The question is then whether an analysis carried out using these functions instead
of the latent variables themselves is satisfactory. Since the analysis depends on the
correlation coefficients, some insight can be gained by comparing the correlation
coefficients between the latent variables with those of their manifest substitutes.

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

240

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

9.5

Estimation of correlations and regressions
between latent variables

We can illustrate the position by considering just two latent variables, y1 and y2 . Let
us suppose that we have two random variables, y1 and y2 , such that
E(y1 ) = 0,

var(y1 ) = 1,

E(y2 | y1 ) = by1 ,

var(y2 | y1 ) = σ 2 .

(9.19)

Then it follows that the correlation of y1 and y2 is
ρ(y1 , y2 ) = b/ b2 + σ 2 .

(9.20)

The proposal we are considering is to replace y1 and y2 by their sufficient statistics
(components) X 1 and X 2 , say. The question then to be answered is whether the
regression coefficient of X 2 on X 1 and the correlation coefficient of X 1 and X 2 are
sufficiently close to the corresponding coefficients for y1 and y2 .
Let us suppose that we have a group of indicators x1 = (x11 , x12 , . . . , x1 p1 ) for y1
and x2 = (x21 , x22 , . . . , x2 p2 ) for y2 , with p1 + p2 = p. The corresponding sufficient
statistics (components) are, from (3.4), given by


X=

X1
X2

λ1
=
0


0
 −1 x,
λ2

(9.21)

where x = (x1 , x2 ). The covariance matrix of X is
cov(X) =  + ,

(9.22)

where  is the covariance matrix of y and


 =

−1


λ1
=
0

0
λ2


 −1
1


λ1
−1
0
2
0

0


0
,
λ2

where we have partitioned  to match the partitioning of  and x. We then obtain

=

1

0



0

,

2

where
pi
i

= λi  i−1 λi =
j=1

λi2j
j

(i = 1, 2).

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

INVESTIGATING DEPENDENCY

241

It then follows that
cov(X 1 , X 2 ) =

1 2

12 ,

var(X i ) =

2
i

ii

+

i

(i = 1, 2),

and hence that
1 2

ρ(X 1 , X 2 ) =

2
1



2
1

1

and

2

+

1

1

2 2
2 (b



1

=
1

+ σ 2) +
√
2
2
2 ρ(y1 , y2 ) b + σ

1

=

Since

+

12

2 2
2 (b

+1

2

2

+ σ 2) +

2
2
2 (b + σ )
(b2 + σ 2 ) +

2

1

ρ(y1 , y2 ).

(9.23)

are both positive,
ρ(X 1 , X 2 ) < ρ(y1 , y2 ).

The correlation between the components thus underestimates the true correlation
between the latent variables. This is a generalisation of a term known among psychologists as attenuation. The latter refers to the fact that, if two variables are both
measured with error, the true correlation will be underestimated by their estimated
correlation. That situation is, essentially, a special case of the result derived here with
p1 = p2 = 1.
The regression coefficient of X 2 on X 1 is
cov(X 1 , X 2 )
2
=
var(X 1 )
1+

12
1

=

2b

1+

.

(9.24)

1

In general, therefore, the regression coefficients will not be the same.
Two questions now arise. How serious are the discrepancies revealed by this
analysis? Can they be corrected? The difference between the two
√ correlation coefficients depends on 1 and 2 . If both are large, the factors
1 /( 1 + 1) and
2 + σ 2 )/( (b2 + σ 2 ) +
(b
)
will
be
close
to
one
and
the
discrepancy
will be
2
2
2
negligible. This will happen if p1 and p2 are large (i.e. there is a large number of
One can easily
indicators), if the λs are large and, especially, if any of the ψi are small. √
correct for attenuation by scaling ρ(X 1 , X 2 ) by the estimated values of 1 /( 1 + 1)
2
2
2
2
and
2 (b + σ )/( 2 (b + σ ) + 2 ).
The regression coefficient is scale-dependent, and it is thus possible that the
estimation of b y2 | y1 can be improved by applying a scaling factor to the components.
This point was investigated by Croon and Bolck (1997) who suggested that each
component should be multiplied by a scalar c such that
E(ci X i | yi ) = yi

(i = 1, 2).

P1: TIX/XYZ
JWST070-09

242

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

This requires that
E(ci λi  i−1 xi | yi ) = ci λi  i−1 λi yi = yi
or that ci = 1/ i . If we choose this scaling then the regression coefficient of the
second scaled component on the first is
bc2 X 2 | c1 X 1 =

c2
c1

2
1

+1

1

b=
1

+1

b.

Under the conditions outlined above the regression coefficient of the scaled components will be almost equal to, but slightly less than, the true regression coefficient.
Components scaled in this way are the Bartlett scores mentioned in Section 3.15.
This analysis suggests that there is not likely to be a serious discrepancy between
LISREL and the more ad hoc method outlined here if there is a large number of
indicators for each latent variable and if the ψs are small – in other words, if the latent
variables are well determined by the associated manifest variables. In quantitative
terms one must obviously look at the estimated parameter values. However, the
ubiquity of easily used software for fitting linear structural equation models appears
to have inhibited further exploration of this alternative.

9.6

Q-Methodology

This is the name given to the analysis in which the rows and columns of the data
matrix are first transposed. The key to understanding what Q-analysis all about is
to focus on the meaning of the correlation coefficients themselves. In order to make
the point more clearly we will use terminology from educational testing. Thus in
the matrix of (9.13) the columns would represent test items and the rows individuals
or subjects; the entries would be test scores. The ordinary correlation coefficient
between the test scores for, say, a pair of individuals is a measure of the similarity of
those two individuals. Similarly, if we take the correlation of two test items across all
individuals we get a measure of how close those test items are with respect to that set
of individuals. R-analysis is then concerned with the relationship between variables
and Q-analysis with the relationships between persons.
In principle, one can carry out either PCA or factor analysis starting from the P
matrix or the Q matrix. Thomson (1951, Part V), traces the origin of this idea as far
back as 1912, but Stephenson was undoubtedly the chief architect and advocate of
the method. His first contribution was in Stephenson (1935).
There was a good deal of controversy in the early days, particularly between
Stephenson on the one hand and Burt (1973) on the other. Burt regarded R- and
Q-methodology as equivalent ways of extracting the basic factorial information
from the correlation matrix, whereas Stephenson saw them as quite different. As we
shall shortly see, Stephenson was essentially correct and Thomson (1951), although
more circumspect, clearly understood the weakness of Burt’s position. We shall not

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

INVESTIGATING DEPENDENCY

243

pursue the history of this controversy but place the problem in the context of modern
factor analysis.
Consider the matrix X X, where X is the data matrix defined in (9.13). This is
symmetric and of dimension p × p. It will thus have p non-negative eigenvalues and
associated eigenvectors satisfying
X Xu = λu.

(9.25)

Next, suppose we pre-multiply each side of this equation by X. We shall then have
(XX )(Xu) = λ(Xu).

(9.26)

This shows that X X and its transpose, XX , have the same positive eigenvalues,
and eigenvectors equal to u and Xu respectively. There is thus a duality in the
eigensolutions of the two matrices which means that if we have the solution of one
we can easily find that of the other.
If we regard the decomposition represented by (9.25) as the R-analysis of the
matrix X X and that by (9.26) as the Q-analysis then it is clear that the two are
equivalent. The question is whether this duality carries over to the covariance or
correlation matrices computed from X and X . Burt assumed that it did, at least
approximately, but Thomson saw difficulties and produced numerical examples to
make his point (Thomson 1951, Chapter XVII).
The equivalence which Burt looked for exists only under very special circumstances. These occur when X is doubly centred. This means that the row sums of X
and the column sums are zero. In this case (1/n)X X is the covariance matrix of X
and (1/n)XX is the covariance matrix of X . In this special case a PCA in R-mode is
equivalent to a PCA in Q-mode. One might then expect that the corresponding factor
analyses would be very similar and hence that any factors uncovered in the two cases
would be essentially the same. It is easy to make the column sums, say, of X zero
by subtracting the column mean from each element, but attempting to do the same
simultaneously for the rows tends to destroy the associations which it is the purpose
of the analysis to uncover.
Although we have shown that the R- and Q-versions of the analysis are not
equivalent in general, there is nothing to prevent us from forming the correlation
matrix of the transposed matrix and asking how the results of a factor or principal
components analysis might be interpreted. This is exactly what has become known
as Q-factor analysis.
It must be recognised straight away that there can be no probability model behind
Q-analysis. Anything we do will be purely descriptive. There is, in fact, an obvious
and better way to analyse a matrix of similarities. This is provided by non-metric
multidimensional scaling. Such an analysis would locate each variable in a space of
small dimension in which distances apart reflected the similarities as measured, for
example by the ‘between-persons’ correlation coefficients. This method is illustrated
in Bartholomew et al. (2008). However, in the 1920s when Q-analysis was first
mooted no such method was available and the natural thing seemed to factor-analyse

P1: TIX/XYZ
JWST070-09

244

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

LATENT VARIABLE MODELS AND FACTOR ANALYSIS

the transposed matrix and classify the variables into groups by means of rotations to
something like ‘simple structure’. Factor analysis was, in effect, being used as a form
of cluster analysis because it was the only technique then available.
Subsequently, Q-methodology became detached from both psychology and statistics and exists as a separate entity with its own journal, conferences and so on. The
journal in which relevant publications occur is called Operant Subjectivitym and
Stephenson’s ideas are now propagated by the International Society for the Scientific
Study Of Subjectivity. It is, perhaps, appropriate to observe that although the original
link with factor analysis has become tenuous, if not tendentious, the existence of this
field of study bears testimony to fertile ground which factor analysis has provided for
the generation of new methods of statistical analysis.

9.7

Concluding reflections of the role of latent
variables in statistical modelling

We conclude the chapter and the book with some general remarks about latent
variable models. Some of the foregoing material may have led the reader to wonder
whether it is really necessary to introduce latent variables. For if we can achieve
much the same objectives without introducing latent variables at all, a case has to
be made for what may seem the unnecessary complication of introducing additional
variables into the model. Arguments can be put on either side but they must,
inevitably, involve substantive questions of meaningfulness and usefulness. Viewed
purely theoretically, latent variable models, like any other probability models, are
mathematical statements about probability distributions, Theoretical deductions can
be made from them about other distributions in a routine fashion. It seems to us that
any arguments, which we do not intend to pursue in detail here, must reckon with the
following points.
First, much of science, especially social science, deals with quantities which
are essentially latent. We use the word ‘quantities’ because they occur, typically,
in statements which imply a magnitude, if only in the sense of an ordering. Thus,
psychologists speak about abilities as something varying from one individual to
another and about which one can at least make statements of relative magnitude. The
same is true of attitudes which can exist in varying degrees. Sociologists speak of
one political party being more right wing than another, even though the degree of
conservatism possessed by a party is not something which can be directly measured.
Instead they will have to depend on various indicators which are believed to be related
to conservatism. There is, therefore, a close correspondence between the way that
social scientists think and the latent variable models constructed to give expression to
that thinking. It is no accident that factor analysis was invented by Charles Spearman, a
psychologist, and developed almost exclusively by psychologists. Similarly, although
structural equation modelling was first introduced in Biometrika by Karl Jöreskog,
a statistician, most of the subsequent development has been in the hands of social
scientists. In a sense, the process of modelling mirrors very closely the way we

P1: TIX/XYZ
JWST070-09

P2: ABC
JWST070-Bartholomew

May 10, 2011

12:47

Printer Name: Yet to Come

INVESTIGATING DEPENDENCY

245

think about relationships, which makes no fundamental distinction between latent
and manifest variables.
Secondly, the essential idea behind our analysis is that we stand between the data,
which are already available, and the latent variables, which can never be observed. It
is thus appropriate that the analysis should be based on distributions which look at the
latent variables conditional on the manifest variables – the data. This procedure neatly
circumvents one of the problems which statisticians have rarely noticed but which
has exercised a good many psychologists. This arises from the fact that the manifest
variables are seldom samples drawn randomly from any population of variables. The
inferences derived in this book are all made conditional on those particular variables
which have been used. They therefore do not depend on how the variables were
obtained. In practice the questions used in a sample survey will be those which the
surveyors have though relevant or practical. They will not have been selected from
some population of possible questions – at least not in any formal manner. Psychologists have attempted to formalise this by introducing what they call the domain, and
Williams (1978), for example, has attempted to carry out a formal analysis, using
this concept, to place the notion of a factor score on a formal footing. By making the
inferences conditional on the data we may hope to obtain relevant information about
the latent variables which is valid whatever manifest variables happen to be to hand.
The dimensionality of the manifest sample space is then irrelevant.
If, for example, we were to use PCA instead of factor analysis, the first principal
component might commend itself as a suitable proxy for something like general
intelligence. But we would get a different linear combination of manifest variables if
we had used a different set of manifest variables. It is difficult to see how one could
be preferred to another. It is worth remembering that the choice of a linear function
for the components was, itself, an arbitrary choice motivated more by convenience
than optimality in any sense. In the general approach to latent variable modelling
which we have adopted, linearity arises naturally from the very general model we
have proposed.
Thirdly, the reality of latent variables has no bearing upon the validity of the
methods described in this book. In a purely formal sense, latent variables are treated
just like manifest variables. In effect, they may be regarded as variables on which
all observations are missing. From a practical point of view, however, their role is
radically different. They are introduced because the scientific model we are trying
to determine requires their presence. In other words, their meaning derives from the
scientific interpretation we put upon them. This does not mean that they are real in
the sense that something like the price of potatoes in a particular wholesale market is
real – that is, it is independently verifiable by different observers. It is worth adding
that we might have a similar discussion about whether the parameters which appear
in statistical models are real. They are unobservable just like latent variables. Indeed,
in a fully Bayesian treatment, both parameters and latent variables would be treated
as random variables.
No statistical model is real in that sense, nor is it intended to be. Rather, it aims
to capture part of reality to a sufficient degree to give genuine insight and to provide
a tool for changing things. Latent variables models do just that.

P1: TIX/XYZ
JWST070-APP

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:49

Printer Name: Yet to Come

Software appendix
Many of the standard packages such as SPSS and S-Plus, but also more specialised
software such as LISREL, GLLAMM in STATA, and CEFA, can be used for some
of the analyses which we have performed but occasionally we have used and reported
the results of using packages such as IRTPRO, Latent GOLD, Mplus and WinBUGS.
We provide here a short description of the version and characteristics of each of these.

r IRTPRO (Item Response Theory for Patient-Reported Outcomes) is distributed
by Scientific Software International, Inc. (available from August 2011). The
program fits various unidimensional and multidimensional confirmatory and
exploratory factor analysis models for binary, nominal, ordinal, continuous
and mixed observed variables. The estimation is based on the E-M algorithm
with fixed or adaptive quadrature points. Other estimation methods such as
the Metropolis–Hastings Robbins–Monro algorithm (Cai 2010) are available.
IRTPRO 2 was used in Examples 4.13.1–4.13.3 and 5.12.1–5.12.4.

r Latent GOLD 4.5 is a latent class and finite mixture program. Latent GOLD

can fit latent class and latent trait models for any type of observed variables
as well as models with mixed latent variables (hybrid models). A log-linear
model representation is also used. The program is distributed by Statistical
Innovations. Latent GOLD was used in Examples 6.9.1, 6.9.2, 6.13.3 and 7.8.3.

r Mplus 5 is a general structural equation modelling program that can analyse

any combination of observed variables. It runs exploratory and confirmatory
factor analysis and performs all the analysis available in a structural equation
modelling framework both for cross-sectional and longitudinal data. Mplus
was used in Examples 3.17.1 and 3.17.2.

r WinBUGS 1.4 is a freely available program which can be used for fitting
complex statistical models using Markov chain Monte Carlo (MCMC) methods
(‘BUGS’ stands for Bayesian inference Using Gibbs Sampling). The program
produces estimates, standard deviations as well as monitoring and convergence
diagnostics plots and statistics. WinBUGS 1.4 was used in Example 4.13.2.
The program can be downloaded from http://www.mrc-bsu.cam.ac.uk/bugs/
Finally, R packages for fitting various item response theory models have been developed and can be found at http://cran.r-project.org/web/views/Psychometrics.html.
Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

247

P1: TIX/XYZ
JWST070-REF

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

References
Aitkin M, Anderson D and Hinde J 1981 Statistical modelling of data on teaching styles.
Journal of the Royal Statistical Society A 144, 419–461.
Akaike H 1983 Information measures and model selection. Bulletin of the International Statistical Institute 50, 277–290.
Akaike H 1987 Factor analysis and AIC. Psychometrika 52, 317–332.
Albert JH 1992 Bayesian estimation of normal ogive item response curves using Gibbs sampling. Journal of Educational Statistics 17, 251–269.
Albert JH and Chib S 1993 Bayesian analysis of binary and polychotomous response data.
Journal of the American Statistical Association 88, 669–679.
Amemiya T 1984 Tobit models: a survey. Journal of Econometrics 24, 3–61.
Amemiya Y and Anderson TW 1985 Asymptotic chi-square tests for a large class of factor
analysis models. Technical Report 13, Stanford University.
Andersen EB 1973 Conditional inference and multiple choice questionnaires. British Journal
of Mathematical and Statistical Psychology 26, 31–44.
Andersen EB 1977 Sufficient statistics and latent trait models. Psychometrika 42(1), 69–81.
Andersen EB 1980a Comparing latent distributions. Psychometrika 45(1), 121–134.
Andersen EB 1980b Discrete Statistical Models with Social Science Applications. NorthHolland, Amsterdam.
Andersen EB and Madsen M 1977 Estimating the parameters of the latent population distribution. Psychometrika 42(3), 357–374.
Anderson JC and Gerbing DW 1984 Statistical inference in factor analysis. Psychometrika 49,
155–173.
Anderson JC and Gerbing DW 1988 Structural equation modelling in practice: a review and
recommended two-step approach. Psychological Bulletin 103, 411–423.
Anderson JC and Gerbing DW 1992 Assumptions and comparative strength of the two-step
approach: comment on Furnell & Yi. Sociological Methods and Research 20, 321–333.
Anderson TW 1959 Some scaling models and estimation procedures in the latent class model
In Probability and Statistics (ed. Grenander U) John Wiley & Sons, Inc. New York pp. 9–38.
Anderson TW 1984 An Introduction to Multivariate Statistical Analysis 2nd edn. John Wiley &
Sons, Inc., New York.
Anderson TW and Rubin H 1956 Statistical inference in factor analysis. Third Berkeley Symposium of Mathematical Statistics and Probability (ed. Neyman J) University of California
Press, Berkeley vol. 5, pp. 111–150.
Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

249

P1: TIX/XYZ
JWST070-REF

250

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

Arbuckle JL 2006 AMOS 7.0 – Programming: Reference Guide. Amos Development
Corporation.
Archer CO and Jennrich RI 1973 Standard errors for rotated factor loadings. Psychometrika
38, 581–592.
Arminger G and Küsters U 1988 Latent trait models with indicators of mixed measurement
level In Latent Trait and Latent Class Models (ed. Langeheine R and Rost J) Plenum Press
New York.
Arminger G and Muthén B 1998 A Bayesian approach to nonlinear latent variables models using Gibbs sampler and the Metropolis-Hastings algorithm. Psychometrika 63,
271–300.
Baker FB 1998 An investigation of the item parameter recovery characteristics of a Gibbs
sampling procedure. Applied Psychological Measurement 22, 153–169.
Barankin EW and Maitra EP 1963 Generalisation of the Fisher-Darmois-Koopman-Pitman
theorem on sufficient statistics. Sankhyā A 25, 217–244.
Barnes S and Kaase M 1979 Political Action: Mass Participation in Five Western Democracies.
Sage Publications, Beverly Hills, CA.
Bartholomew DJ 1980 Factor analysis for categorical data. Journal of the Royal Statistical
Society B 42(3), 293–321.
Bartholomew DJ 1981 Posterior analysis of the factor model. British Journal of Mathematical
and Statistical Psychology 43, 93–99.
Bartholomew DJ 1984 The foundations of factor analysis. Biometrika 71, 221–232.
Bartholomew DJ 1985 Foundations of factor analysis: Some practical implications. British
Journal of Mathematical and Statistical Psychology 38, 1–10.
Bartholomew DJ 1987 Latent Variable Models and Factor Analysis vol. 40 of Griffin’s Statistical Monographs and Courses. Charles Griffin, London.
Bartholomew DJ 1993 Estimating relationships between latent variables. Sankhyā 55(3),
409–419.
Bartholomew DJ 1995a Social measurement: A statistical perspective In Applied Statistics –
Recent Developments (ed. Frohn J, Gather U, Stute W and Thöni H) Vandenhoeck &
Ruprecht Göttingen.
Bartholomew DJ 1995b Spearman and the origin and development of factor analysis. British
Journal of Mathematical and Statistical Psychology 48, 211–220.
Bartholomew DJ 1996 The Statistical Approach to Social Measurement. Academic Press, San
Diego, CA.
Bartholomew DJ 2007 Three faces of factor analysis In Factor Analysis at 100: Historical
Developments and Future Directions (ed. Cudeck R and MacCallum R) Lawrence Erlbaum
Associates Mahwah, NJ pp. 9–21.
Bartholomew DJ and Knott M 1993 Scale construction by maximising reliability In Statistical
Modelling and Latent Variables (ed. Haagen K, Bartholomew DJ and Deistler M) North
Holland Amsterdam pp. 17–26.
Bartholomew DJ and Knott M 1999 Latent Variable Models and Factor Analysis vol. 7 of
Kendall Library of Statistics 2nd edn. Arnold, London.
Bartholomew DJ and Leung SO 2002 A goodness of fit test for sparse 2p contingency tables.
British Journal of Mathematical and Statistical Psychology 55, 1–15.

P1: TIX/XYZ
JWST070-REF

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

251

Bartholomew DJ and McDonald RP 1986 The foundations of factor analysis: a further
comment. British Journal of Mathematical and Statistical Psychology 39, 228–229.
Bartholomew DJ and Schuessler KF 1991 Reliability of attitude scores based on a latent trait
model. Sociological Methodology 21, 97–123.
Bartholomew DJ and Tzamourani P 1999 The goodness-of-fit of latent trait models in attitude
measurement. Sociological Methods and Research 27, 525–546.
Bartholomew DJ, Bassin EL and Schuessler KF 1993 Properties of a latent trait reliability
coefficient. Sociological Methods & Research 22(2), 163–192.
Bartholomew DJ, de Menezes L and Tzamourani P 1997 Latent trait and latent class models
applied to survey data In Application of Latent Trait and Latent Class Models in the Social
Sciences (ed. Rost J and Langeheine R) Waxmann Münster.
Bartholomew DJ, Deary IJ and Lawn M 2009 A new lease of life for Thomson’s bonds model
of intelligence. Psychological Review 116, 567–579.
Bartholomew DJ, Steele F, Moustaki I and Galbraith J 2008 Analysis of Multivariate Social
Science Data 2nd edn. CRC Press, Boca Raton, FL.
Bartlett MS 1937 The statistical conception of mental factors. British Journal of Psychology
28, 97–104.
Bartlett MS 1950 Tests of significance in factor analysis. British Journal of Psychology (Statistical Section) 3, 77–85.
Bartlett MS 1953 Factor analysis in psychology as a statistician sees it Uppsala Symposium of
Psychology and Factor Analysis Almqvist & Wicksell Uppsala pp. 23–34.
Basilevsky A 1994 Statistical Factor Analysis and Related Methods. Theory and Applications Wiley Series in Probability and Mathematical Statistics. John Wiley & Sons, Inc.,
New York.
Béguin AA and Glas CAW 2001 MCMC estimation and some model-fit analysis of multidimensional IRT models. Psychometrika 66(4), 541–562.
Bentler PM 1968 Alpha-maximized factor analysis (alphamax): its relation to alpha and canonical factor analysis. Psychometrika 33, 335–345.
Bentler PM 2008 EQS 6: Structural Equations Program Manual. Multivariate Software,
Encino, CA.
Bergsma W, Croon M and Hagenaars JA 2009 Marginal Models. Springer.
Birnbaum A 1968 Some latent trait models and their use in inferring an examinee’s ability
In Statistical Theories of Mental Test Scores (ed. Lord FM and Novick MR) Reading, MA
pp. 425–435.
Birnbaum A 1969 Statistical theory for logistic mental test models with a prior distribution of
ability. Journal of Mathematical Psychology 6, 258–276.
Bock RD 1972 Estimating item parameters and latent ability when responses are scored in two
or more nominal categories. Psychometrika 37(1), 29–51.
Bock RD and Aitkin M 1981 Marginal maximum likelihood estimation of item parameters:
application of an EM algorithm. Psychometrika 46(4), 443–459.
Bock RD and Moustaki I 2007 Item response theory in a general framework In Psychometrics
(ed. Rao CR and Sinharay S) Elsevier Boston.
Bollen KA 1989 Structural Equations with Latent Variables. John Wiley & Sons, Inc.,
New York.

P1: TIX/XYZ
JWST070-REF

252

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

Bollen KA 1995 Structural equation models that are nonlinear in latent variables: A least
squares estimator In Sociological Methodology (ed. Marsden P) Blackwell Cambridge,
MA.
Bollen KA and Long JS 1993 Testing Structural Equation Models. Sage Publications, Newbury
Park, CA.
Bollen KA and Paxton P 1998 Two-stage least squares estimation of interaction effects In
Interaction and Nonlinear Effects in Structural Equation Models (ed. Schumacker R and
Marcoulides G) Lawrence Erlbaum Associates Mahwah, NJ pp. 125–151.
Boomsma A 1985 Nonconvergence, improper solutions, and starting values in LISREL maximum likelihood estimation. Psychometrika 50(2), 229–242.
Bozdogan H and Ramirez DE 1986 Model selection approach to the factor model problem,
parameter parsimony, and choosing the number of factors. Research report, University of
Virginia, Department of Mathematics.
Brokken FB 1983 Orthogonal Procrustes rotation maximising congruence. Psychometrika 48,
343–349.
Brooks SP and Roberts GO 1998 Convergence assessment techniques for Markov chain Monte
Carlo. Statistics and Computing 8, 319–335.
Browne MW 1982 Covariance structures In Topics in Applied Multivariate Analysis
(ed. Hawkins DM) Cambridge University Press Cambridge pp. 72–141.
Browne MW 1984 Asymptotically distribution-free methods for the analysis of covariance
structures. British Journal of Mathematical and Statistical Psychology 37, 62–83.
Browne MW and Cudeck R 1993 Alternative ways of assessing model fit In Testing Structural
Equation Models (ed. Bollen KL and Long JS) Sage Newbury Park, CA.
Browne MW, Cudeck R, Tateneni K and Mels G 1998 CEFA: Comprehensive exploratory factor analysis. Technical report, Ohio State University. Software and documentation available
at http://faculty.psy.ohio-state.edu/browne/software.php.
Burt RS 1973 Confirmatory factor analytic structures and the theory of the construction process.
Sociological Methods and Research 2, 131–190.
Burt RS 1976 Interpretational confounding of unobserved variables in structural equation
models. Sociological Methods and Research 5, 3–52.
Busemeyer J and Jones L 1983 Analysis of multiplicative combination rules when the causal
variables are measured with error. Psychological Bulletin 93, 549–562.
Cai L 2010 High-dimensional exploratory item factor analysis by a Metropolis-Hastings
Robbins-Monro algorithm. Psychometrika 75, 33–57.
Cai L, du Toit SHC and Thissen D 2011 IRTPRO: Flexible, multidimensional, multiple categorical IRT modeling. Scientific Software International, Chicago.
Cai L, Maydeu-Olivares A, Coffman DL and Thissen D 2006 Limited information goodness of fit testing of item response theory models for sparse 2p tables. British Journal of
Mathematical and Statistical Psychology 59, 173–194.
Cattell RB 1978 The Scientific Use of Factor Analysis in Behavioral and Life Sciences. Plenum
Press, New York.
Chang HH and Stout W 1993 The asymptotic posterior normality of the latent trait in an IRT
model. Psychometrika 58(1), 37–52.
Chatfield C and Collins C 1980 An Introduction to Multivariate Analysis. Chapman & Hall,
London.

P1: TIX/XYZ
JWST070-REF

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

253

Chatterjee S 1984 Variance estimation in factor analysis: an application of the bootstrap. British
Journal of Mathematical and Statistical Psychology 37, 252–262.
Chen WH and Thissen D 1997 Local dependence indexes for item pairs using item response
theory. Journal of Educational and Behavioral Statistics 22, 265–289.
Christoffersson A 1975 Factor analysis of dichotomized variables. Psychometrika 40(1),
5–32.
Cliff N 1966 Orthogonal rotation to congruence. Psychometrika 31, 33–42.
Clogg CC 1979 Some latent structure models for the analysis of Likert-type data. Social
Science Research 8, 287–301.
Collins LM, Fidler PL, Wugalter SE and Long JD 1993 Goodness-of-fit testing for latent class
models. Multivariate Behavioral Research 28(33), 375–389.
Comrey AL 1962 The minimum residual method of factor analysis. Psychology Reports 11,
15–18.
Comrey AL and Ahumada A 1964 An improved procedure and program for minimum residual
factor analysis. Psychology Reports 15, 91–96.
Cowles MK and Carlin BP 1996 Markov chain Monte Carlo convergence diagnostics: a
comparative review. Journal of the American Statistical Association 91, 883–904.
Cox DR and Wermuth N 1996 Multivariate Dependencies Monographs on Statistics and
Applied Probability 2nd edn. Chapman & Hall, London.
Cronbach LJ 1951 Coefficient alpha and the internal structure of tests. Psychometrika 48,
99–111.
Croon M 1990 Latent class analysis with ordered latent classes. British Journal of Mathematical
and Statistical Psychology 43, 171–192.
Croon M and Bolck A 1997 On the use of factor scores in structural equations models.
Technical Report 97.10.102/7, Work and Organization Research Centre, Tilburg University.
WORC Paper.
Cudeck R and MacCallum RC (eds) 2007 Factor Analysis at 100: Historical Developments
and Future Directions. Lawrence Erlbaum Associates, Mahwah, NJ.
Cudeck R and O’Dell LL 1994 Applications of standard error estimates in unrestricted factor
analysis: significance tests for factor loadings and correlations. Psychological Bulletin 115,
475–487.
Cuttance P and Ecob R (eds) 1987 Structural Modelling by Example. Cambridge University
Press, Cambridge.
de Boeck P 2008 Random item IRT models. Psychometrika 73, 533–559.
de Menezes LM 1999 On fitting latent class models for binary data: the estimation of standard
errors. British Journal of Mathematical and Statistical Psychology 52, 149–168.
de Menezes LM and Bartholomew DJ 1996 New developments in latent structure analysis
applied to social attitudes. Journal of the Royal Statistical Society A 159, 213–224.
Dempster AP, Laird NM and Rubin DB 1977 Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society B 39, 1–38.
Dolby GR 1976 Structural relations and factor analysis. Mathematical Scientist Supplement
No. 1 pp. 25–29. Abstracts from the First Conference of the CSIRO Division of Mathematics
and Statistics.
Dunson DB 2000 Bayesian latent variable models for clustered mixed outcomes. Journal of
the Royal Statistical Society B 62, 355–366.

P1: TIX/XYZ
JWST070-REF

254

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

Dunson DB 2003 Dynamic latent trait models for multidimensional longitudinal data. Journal
of the American Statistical Association 98, 555–563.
Esposito Vinzi V, Chin W, Henseler J and Wang H 2010 Handbook of Partial Least Squares:
Concepts, Methods and Applications in Marketing and Related Areas Handbook of Computational Statistics. Springer, New York.
Etezadi-Amoli J and McDonald RP 1983 A second generation nonlinear factor analysis.
Psychometrika 48(3), 315–342.
Everitt BS 1984 An Introduction to Latent Variable Models. Chapman & Hall, London.
Everitt BS and Hand DJ 1981 Finite Mixture Distributions. Chapman & Hall, London.
Eysenck SBG, Eysenck HJ and Barrett PT 1985 A revised version of the Psychoticism scale.
Personality and Individual Differences 6, 21–29.
Fachel GMG 1986 The C-type Distribution as an Underlying Model for Categorical Data and
its Use in Factor Analysis PhD thesis University of London.
Feller W 1966 An Introduction to Probability Theory and Its Applications vol. 2. John Wiley
and Sons, Inc., New York.
Fielding A 1977 Latent structure models In The Analysis of Survey Data: Exploring Data Structures (ed. O’Muircheartaigh CA and Payne C) vol. 1. John Wiley & Sons, Ltd. Chichester
pp. 125–157.
Formann AK 1992 Linear logistic latent class analysis for polytomous data. Journal of the
American Statistical Association 87, 476–486.
Fox JP and Glas CAW 2001 Bayesian estimation of a multilevel IRT model using Gibbs
sampling. Psychometrika 66(2), 271–288.
Francis I 1974 Factor analysis: fact or fabrication. Mathematical Chronicle 3, 9–44. Invited address delivered at the Seventh New Zealand Mathematics Colloquium held at Christchurch.
Franzoi SL and Shield SA 1984 The Body Esteem Scale: Multidimensional structure and sex
differences in a college population. Journal of Personality Assessment 48, 173–178.
Galton F 1888 Co-relations and their measurement. Proceedings of the Royal Society 45,
135–145.
Gelfand AE and Smith AFM 1990 Sampling-based approaches to calculating marginal densities. Journal of the American Statistical Association 85, 398–409.
Gelman A and Rubin DB 1992 Inference from iterative simulation using multiple sequences.
Statistical Science 7, 457–511.
Geman S and Geman D 1984 Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence 6,
721–741.
Geweke JF 1992 Evaluating the accuracy of sampling-based approaches to calculating posterior
moments In Bayesian Statistics 4 (ed. Bernardo JM, Berger JO, Dawid AP and Smith AFM)
Clarendon Press Oxford pp. 169–188.
Geweke JF and Singleton KJ 1980 Interpreting the likelihood ratio statistic in factor models
when sample size is small. Journal of the American Statistical Association 75, 133–137.
Glas CAW and Pimental JL 2008 Modeling nonignorable missing data in speeded tests.
Educational and Psychological Measurement 68, 907–922.
Goldstein H 1980 Dimensionality, bias, independence and measurement scale problems in
latent trait test score models. British Journal of Mathematical and Statistical Psychology
33, 234–246.

P1: TIX/XYZ
JWST070-REF

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

255

Goodman LA 1974 Explanatory latent structure analysis using both identifiable and
unidentifiable models. Biometrika 61, 215–231.
Goodman LA 1978 Analyzing Qualitative/Categorical Data. Abt Books, Cambridge, MA.
Edited by J. Magisdon.
Green BF 1952 The orthogonal approximation of an oblique structure in factor analysis.
Psychometrika 17, 429–440.
Greenacre MJ 1984 Theory and Applications of Correspondence Analysis. Academic Press,
London.
Guttman L 1953 Image theory for the structure of quantitative variables. Psychometrika 18,
277–296.
Haberman SJ 1979 Analysis of Qualitative Data, Volume 2, New Developments. Academic
Press, New York.
Hakistian AR, Rogers WT and Cattell RB 1982 The behaviour of number-of-factors rules with
simulated data. Multivariate Behavioural Research 17, 193–219.
Hambleton RK, Swaminathan H and Rogers HJ 1991 Fundamentals of Item Response Theory.
Sage Publications, Newbury Park, CA.
Harman HH 1976 Modern Factor Analysis 3rd edn. University of Chicago Press, Chicago.
Harman HH and Jones WH 1966 Factor analysis by minimising residuals (Minres). Psychometrika 31, 351–368.
Hastings WK 1970 Monte Carlo sampling methods using Markov chains and their applications.
Biometrika 57, 97–109.
Heckman JJ and Singer B (eds) 1985 Longitudinal Analysis of Labor Market Data. Cambridge
University Press, Cambridge.
Heidelberger P and Welch P 1983 Simulation run length control in the presence of an initial
transient. Operations Research 31, 1109–1144.
Heinen T 1996 Latent Class and Discrete Latent Trait Models Similarities and Differences
Advanced Quantitative Techniques in the Social Sciences. Sage Publications, Thousand
Oaks, CA.
Heywood HB 1931 On finite sequences of real numbers. Proceedings of the Royal Society A
134, 486–510.
Hills M 1977 Review of ‘An Introduction to Multivariate Techniques for Social and Behavioural
Sciences’ by S. Bennett and D. Bowers. Applied Statistics 26(3), 339–340.
Holland PW 1981 When are item response models consistent with observed data? Psychometrika 46, 79–92.
Holland PW 1990a The Dutch identity: a new tool for the study of item response models.
Psychometrika 55, 5–18.
Holland PW 1990b On the sampling theory foundations of item response theory models.
Psychometrika 55, 577–601.
Holland PW and Rosenbaum PR 1985 Conditional association and unidimensionality
in monotone latent variable models. Technical report, Educational Testing Service,
Princeton, NJ.
Hotelling H 1933 Analysis of a complex of statistical variables into principal components.
Journal of Educational Psychology 24, 417–441.
Hotelling H 1957 The relation of the newer multivariate methods to factor analysis. British
Journal of Statistical Psychology 10, 69–79.

P1: TIX/XYZ
JWST070-REF

256

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

Huber P, Ronchetti E and Victoria-Feser MP 2004 Estimation of generalized linear latent
variable models. Journal of the Royal Statistical Society B 66, 893–908.
Hunter JE and Gerbing DW 1982 Unidimensional measurement, second order factor analysis,
and causal models. Research in Organizational Behavior 4, 267–320.
Jennrich RI 1973 Standard errors for obliquely rotated factor loadings. Psychometrika 38,
593–604.
Jennrich RI 2006 Rotation to simple loadings using component loss functions: The oblique
case. Psychometrika 71, 173–191.
Jennrich RI and Thayer DT 1973 A note on Lawley’s formulas for standard errors in maximum
likelihood factor analysis. Psychometrika 38, 571–580.
Jensen AR 1998 The g Factor: The Science of Mental Ability. Praeger, Westport, CT.
Johnson RA and Wichern DW 1982 Applied Multivariate Analysis. Prentice Hall, Englewood
Cliffs, NJ.
Johnson VE and Albert JH 1999 Ordinal Data Modeling. Springer-Verlag, New York.
Jöreskog KG 1967 Some contributions to maximum likelihood factor analysis. Psychometrika
32, 443–482.
Jöreskog KG 1969 Efficient estimation in image factor analysis. Psychometrika 34, 51–75.
Jöreskog KG 1970 A general method for analysis of covariance structures. Biometrika 57,
239–251.
Jöreskog KG 1977 Factor analysis by least squares and maximum likelihood methods In
Statistical Methods for Digital Computers (ed. Enslein K, Ralston W and Wilf HS) vol. 3
John Wiley & Sons, Inc. New York.
Jöreskog KG 1990 New developments in LISREL: analysis of ordinal variables using
polychoric correlations and weighted least squares. Quality and Quantity 24, 387–
404.
Jöreskog KG 1994 On the estimation of polychoric correlations and their asymptotic covariance
matrix. Psychometrika 59, 381–389.
Jöreskog KG and Goldberger AS 1972 Factor analysis by generalized least squares. Psychometrika 37, 243–259.
Jöreskog KG and Moustaki I 2001 Factor analysis of ordinal variables: a comparison of three
approaches. Multivariate Behavioral Research 36, 347–387.
Jöreskog KG and Sörbom D 1977 Statistical models and methods for analysis of longitudinal
data In Latent Variables and Socio-economic Models (ed. Aigner DJ and Goldberger AS)
North-Holland Amsterdam.
Jöreskog KG and Sörbom D 2006 LISREL 8.8 for Windows. Scientific Software International,
Lincolnwood, IL. Computer software.
Jöreskog KG and Yang F 1996 Nonlinear structural equation models: The Kenny-Judd model
with interaction effects In Advanced Structural Equation Modeling: Issues and Techniques (ed. Marcoulides G and Schumacker R) Lawrence Erlbaum Associates Mahwah,
NJ pp. 57–88.
Junker BW 1993 Conditional association, essential independence and monotone unidimensional item response models. Annals of Statistics 21, 1359–1378.
Kaiser HF 1958 The varimax criterion for analytic rotation in factor analysis. Psychometrika
23, 187–200.
Kaiser HF and Caffrey J 1965 Alpha factor analysis. Psychometrika 30(1), 1–14.

P1: TIX/XYZ
JWST070-REF

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

257

Kano Y 1983 Consistency of estimators in factor analysis. Journal of the Japanese Statistical
Society 13, 137–174.
Kano Y 1986a Conditions on consistency of estimators in covariance structure model. Journal
of the Japanese Statistical Society 16, 75–80.
Kano Y 1986b Consistency conditions on the least squares estimators in a single common
factor analysis model. Annals of the Institute of Statistical Mathematics, A 39, 57–68.
Kaplan D 2009 Structural Equation Modeling: Foundations and Extensions 2nd edn. Sage,
London.
Kenny DA and Judd CM 1984 Estimating the non-linear and interactive effects of latent
variables. Psychological Bulletin 96, 201–210.
Kim SH 2001 An evaluation of a Markov chain Monte Carlo method for the Rasch model.
Applied Psyhological Measurement 25(2), 163–176.
Knott M 2005 A measure of independence for a multivariate normal distribution and some
connections with factor analysis. Journal of Multivariate Analysis 96(2), 374–383.
Knott M and Albanese MT 1992 Large values for maximum likelihood estimates of parameters
in latent variable models Unpublished.
Knott M and Albanese MT 1993 Conditional distributions of a latent variable and scoring for
binary data. Revista Brasileira de Probabilidade e Estatı́stica 6, 171–188.
Knott M and Bartholomew DJ 1993 Scoring binary responses for maximum reliability. Psychometrika 58, 331–338.
Krane WR and McDonald RP 1978 Scale invariance and the factor analysis of correlation
matrices. British Journal of Mathematical and Statistical Psychology 31, 218–228.
Krebs D and Schuessler KF 1987 Soziale Empfindungen: ein interkultureller Skalenvergleich
bei Deutschen und Amerikanern (Social Attitudes: an intercultural Scale Comparison for
Germans and Americans) Monographien : Sozialwissenschaftliche Methoden. Campus
Verlag, Frankfurt/Main.
Lance CE, Cornwell JM and Mulaik SA 1988 Limited information parameter estimates for
latent or mixed manifest and latent variable models. Multivariate Behavioral Research 23,
171–187.
Langeheine R and Rost J 1988 Latent Trait and Latent Class Models. Plenum Press, New York.
Langeheine R, Pannekoek J and van de Pol F 1996 Bootstrapping goodness-of-fit measures in
categorical data analysis. Sociological Methods and Research 24(4), 492–516.
Lawley DN 1953 A Modified Method of Estimation in Factor Analysis and Some Large
Sample Results number 3 in Nordisk Psychologi’s Monograph Series, Copenhagen: Ejnar
Mundsgaards.
Lawley DN 1967 Some new results in maximum likelihood factor analysis. Proceedings of the
Royal Society of Edinburgh A 67, 256–264.
Lawley DN and Maxwell AE 1971 Factor Analysis as a Statistical Method 2nd edn. Butterworth, London.
Lazarsfeld PF and Henry NW 1968 Latent Structure Analysis. Houghton-Mifflin, New York.
Lee SY 1981 A Bayesian approach to confirmatory factor analysis. Psychometrika 46, 153–160.
Lee SY 2007 Structural Equation Modelling: A Bayesian Approach. John Wiley & Sons, Inc.,
New York.
Lee SY and Song XY 2003 Bayesian analysis of structural equation models with dichotomous
variables. Statistics in Medicine 22, 3073–3088.

P1: TIX/XYZ
JWST070-REF

258

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

Lee SY and Song XY 2004a Bayesian model comparison of nonlinear latent variable models
with missing continuous and ordinal categorical data. British Journal of Mathematical and
Statistical Psychology 57, 131–150.
Lee SY and Song XY 2004b Maximum likelihood analysis of a generalized latent variable
model with hierarchical mixed data. Biometrics 60, 624–636.
Lee SY and Zhu H 2002 Maximum likelihood estimation of nonlinear structural equation
models. Psychometrika 67, 189–210.
Lee SY, Poon WY and Bentler PM 1990a Full maximum likelihood analysis of structural equation models with polytomous variables. Statistics and Probability letters 9,
91–97.
Lee SY, Poon WY and Bentler PM 1990b A three-stage estimation procedure for structural
equation models with polytomous variables. Psychometrika 55, 45–51.
Lee SY, Poon WY and Bentler PM 1992 Structural equation models with continuous and
polytomous variables. Psychometrika 57, 89–105.
Lee SY, Poon WY and Bentler PM 1995 A two-stage estimation of structural equation models
with continuous and polytomous variables. British Journal of Mathematical and Statistical
Psychology 48, 339–358.
Lee Y and Nelder JA 2009 Likelihood inference for models with unobservables: another view.
Statistical Science 24(3), 255–269.
Leimu H 1983 Työntekijäin työasema ja työpaikkaliikkuvuus erikokoisissa teollisuusyrityksissä. (Blue collar work position and workplace mobility in small and large industrial firms.
I. Theoretical points of departure and comparison of workers in small and large industrial
firms.). Technical Report C40, University of Turku, Turku.
Leung SO 1992 Estimation and application of latent variable models in categorical data
analysis. British Journal of Mathematical and Statistical Psychology 45, 311–328.
Linting M, Meulman JJ, Groenen PJF and Van der Kooij JJ 2007 Nonlinear principal components analysis: Introduction and application. Psychological Methods 12, 336–358.
Loehlin JC 1989 Latent Variable Models: An Introduction to Factor, Path, and Structural
Analysis 2nd edn. Lawrence Erlbaum Associates, Hillsdale, NJ.
Lopez HF and West M 2004 Bayesian model assessment in factor analysis. Statistica Sinica
14, 41–67.
Macready GB and Dayton CM 1977 The use of probabilistic models in the assessment of
mastery. Journal of Educational Statistics 2(2), 99–120.
Magnus JR and Neudecker H 1988 Matrix Calculus with Applications in Statistics and Econometrics Wiley Series in Probability and Mathematical Statistics. John Wiley & Sons, Inc.,
New York.
Maraun MD 1996 Metaphor taken as math: Indeterminacy in the factor analysis model.
Multivariate Behavioral Research 31, 517–538.
Martin JK and McDonald RP 1975 Bayesian estimation in unrestricted factor analysis; a
treatment for Heywood cases. Psychometrika 40, 505–517.
Martinson EO and Hadman MA 1975 Calculation of the polychoric estimate of correlation in
contingency tables. Applied Statistics 24, 272–278.
Masters GM 1985 A comparison of latent trait and latent class analysis of Likert-type data.
Psychometrika 50, 69–82.
Masters GN 1982 A Rasch models for partial credit scoring. Psychometrika 47, 149–174.

P1: TIX/XYZ
JWST070-REF

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

259

Masters GN and Wright BD 1997 The partial credit model In Handbook of Modern Item
Response Theory (ed. van der Linden W and Hambleton RK) Springer New York
pp. 351–367.
Maydeu-Olivares A and Joe H 2005 Limited- and full-information estimation and goodness-offit testing in 2n contingency tables: A unified framework. Journal of the American Statistical
Association 100(471), 1009–1020.
Maydeu-Olivares A and Joe H 2006 Limited information goodness-of-fit testing in multidimensional contingency tables. Psychometrika 71, 713–732.
McDonald RP 1962 A note on the derivation of the general latent class model. Psychometrika
27(2), 203–206.
McDonald RP 1967a Factor interaction in nonlinear factor analysis. British Journal of Mathematical and Statistical Psychology 32(20), 205–215.
McDonald RP 1967b Non-linear factor analysis number 15 in Psychometric Monographs.
Psychometric Society.
McDonald RP 1967c Numerical methods for polynomial models in nonlinear factor analysis.
Psychometrika 32, 77–112.
McDonald RP 1983 Alternative weights and invariant parameters in optimal scaling. Psychometrika 48(3), 377–391.
McDonald RP 1985 Factor Analysis and Related Methods. Lawrence Erlbaum Associates,
Hillsdale, NJ.
McFadden D 1974 Conditional logit analysis of qualitative choice behaviour In Frontiers in
Econometrics (ed. Zarembka P) Academic Press New York pp. 105–142.
McFadden D 1982 Qualitative response models In Advances in Econometrics (ed. Hildenbrand
W) Cambridge University Press Cambridge pp. 1–37.
McHugh RB 1956 Efficient estimation and local identification in latent class analysis. Psychometrika 21(4), 331–347.
McHugh RB 1958 Note on ‘efficient estimation and local identification in latent class analysis’.
Psychometrika 23(3), 273–274.
McLachlan GJ and Krishnan J 1997 The EM Algorithm and Extensions. John Wiley & Sons,
Inc., New York.
Meng XL and Schilling S 1996 Fitting full-information item factor models and an empirical
investigation of bridge sampling. Journal of the American Statistical Association 91(435),
1254–1267.
Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH and Teller E 1953 Equations of
state calculations by fast computing machine. Journal of Chemical Physics 21, 1087–1091.
Mislevy RJ and Verhelst N 1990 Modeling item responses when different subjects employ
different solution strategies. Psychometrika 55, 195–215.
Molenaar PCW and von Eye A 1994 On the arbitrary nature of latent variables In Latent
Variables Analysis (ed. von Eye A and Clogg CC) Sage London pp. 226–242.
Moustaki I 1996 A latent trait and a latent class model for mixed observed variables. British
Journal of Mathematical and Statistical Psychology 49, 313–334.
Moustaki I 2000 A latent variable model for ordinal variables. Applied Psychological Measurement 24, 211–223.
Moustaki I and Knott M 1997 Generalized latent trait models. Statistics Research Report 36,
London School of Economics and Political Science, Statistics Department.

P1: TIX/XYZ
JWST070-REF

260

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

Moustaki I and Knott M 2000a Generalized latent trait models. Psychometrika 65, 391–411.
Moustaki I and Knott M 2000b Weighting for item non-response in attitude scales by using
latent variable models with covariates. Journal of the Royal Statistical Society, Series A 163,
445–459.
Moustaki I and Knott M 2005 Computational aspects of the E-M and Bayesian estimation in
latent variable models In New Developments in Categorical Data Analysis for the Social
and Behavioral Sciences (ed. van der Ark L, Croon M and Sijtsma K) Lawrence Erlbaum
Associates, Mahwah, NJ pp. 103–124.
Moustaki I and Papageorgiou I 2004 Latent class models for mixed outcomes with applications
in archaeometry. Computational Statistics & Data Analysis 48, 659–675.
Mulaik SA 1972 The Foundations of Factor Analysis. McGraw-Hill, New York.
Mulaik SA 1986 Factor analysis and Psychometrika: major developments. Psychometrika
51(1), 23–33.
Mulaik SA 2009a Foundations of Factor Analysis 2nd edn. CRC Press, Boca Raton, FL.
Mulaik SA 2009b Linear Causal Modeling with Structural Equations. CRC Press, Boca Raton,
FL.
Muraki E 1990 Fitting a polytomous item response model to Likert-type data. Applied Psychological Measurement 14, 59–71.
Muraki E 1992 A generalized partial credit model: application of an EM algorithm. Applied
Psychological Measurement 16, 159–176.
Muraki E and Carlson E 1995 Full-information factor analysis for polytomous item responses.
Applied Psychological Measurement 19, 73–90.
Muthén B 1978 Contributions to factor analysis of dichotomous variables. Psychometrika
43(4), 551–560.
Muthén B 1984 A general structural model with dichotomous, ordered categorical and continuous latent variable indicators. Psychometrika 49(1), 115–132.
Muthén B 2008 Latent variable hybrids In Advances in Latent Variable Mixture Models
(ed. Hancock G and Samuelsen K) Information Age Charlotte, NC pp. 1–24.
Muthén B and Asparouhov T 2006 Item response mixture modeling: Application to tobacco
dependence criteria. Addictive Behaviors 31, 1050–1066.
Muthén B and Satorra A 1995 Technical aspects of Muthén’s LISCOMP approach to estimation
of latent variable relations with a comprehensive measurement model. Psychometrika 60,
489–503.
Muthén L and Muthén B 2010 Mplus User’s Guide 6th edn. Muthén and Muthén, Los Angeles.
Ogasawara H 1998 Standard errors for rotation matrices with an application to the promax
solution. British Journal of Mathematical and Statistical Psychology 51, 163–178.
Okamoto M and Ihara M 1983 A new algorithm for least-squares solution in factor analysis.
Psychometrika 48, 597–605.
Olsson U 1979 On the robustness of factor analysis against crude classification of the observations. Multivariate Behavioral Research 14, 485–500.
O’Muircheartaigh C and Moustaki I 1999 Symmetric pattern models: a latent variable approach
to item non-response in attitude scales. Journal of the Royal Statistical Society, Series A
162, 177–194.
Owen RJ 1975 A Bayesian sequential procedure for quantal response in the context of adaptive
testing. Journal of the American Statistical Association 70, 351–356.

P1: TIX/XYZ
JWST070-REF

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

261

Palomo J, Dunson DB and Bollen K 2007 Bayesian structural equation modeling In Handbook
of Latent Variable and Related Models (ed. Lee SY) Handbook of Computing and Statistics
with Applications Elsevier Amsterdam.
Patz RJ 1996 Markov Chain Monte Carlo Methods for Item Response Theory Models with
Applications for the National Assessment of Educational Progress PhD thesis Carnegie
Mellon University.
Patz RJ and Junker BW 1999a Applications and extensions of MCMC in IRT: multiple item
types, missing data, and rated responses. Journal of Educational and Behavioral Statistics
24, 342–366.
Patz RJ and Junker BW 1999b A straightforward aproach to Markov chain Monte Carlo
methods for item response models. Journal of Educational and Behavioral Statistics 24,
146–178.
Pearson K 1901 On lines and planes of closest fit to a system of points in space. Philosophical
Magazine (6) 2, 557–572.
Pennell R 1972 Routinely computable confidence intervals for factor loadings using the ‘jackknife’. British Journal of Mathematical and Statistical Psychology 25, 107–114.
Pickering RM and Forbes JF 1984 A classification of Scottish infants using latent class analysis.
Statistics in Medicine 3, 249–259.
Rabe-Hesketh S, Skrondal A and Pickles A 2002 Reliable estimation of generalized linear
mixed models using adaptive quadrature. Stata Journal 2(1), 1–21.
Raftery AL and Lewis S 1992a Comment: One long run with diagnostics: Implementation
strategies for Markov chain Monte Carlo. Statistical Science 7, 493–497.
Raftery AL and Lewis SM 1992b How many iterations in the Gibbs sampler? In Bayesian
Statistics 4 (ed. Bernardo JM, Berger, J. O.and Dawid AP and Smith AFM) Clarendon Press
Oxford pp. 763–773.
Rao CR 1955 Estimation and tests of significance in factor analysis. Psychometrika 20,
93–111.
Rao CR 1973 Linear Statistical Inference and Its Applications 2nd edn. John Wiley & Sons,
Inc., New York.
Rasch G 1960 Probabilistic Models for Some Intelligence and Attainment Tests. Pædagogiske
Institut, Copenhagen.
Rasch G 1961 On general laws and the meaning of measurement in psychology In
Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability
(ed. Neyman J), vol. 4, pp. 321–334. University of California Press, Berkeley.
Reiser M 1996 Analysis of residuals for the multinomial item response model. Psychometrika
61, 509–528.
Reiser M and VandenBerg M 1994 Validity of the chi-square test in dichotomous variable
factor analysis when expected frequencies are small. British Journal of Mathematical and
Statistical Psychology 47, 85–107.
Rizopoulos D and Moustaki I 2008 Generalized latent variable models with non-linear effects.
British Journal of Mathematical and Statistical Psychology 24, 415–438.
Rosenbaum PR 1984 Testing the conditional independence and monotonicity assumptions of
item response theory. Psychometrika 49(3), 425–435.
Rost J 1990 Rasch models in latent classes: An integration of two approaches to item analysis.
Applied Psychological Measurement 14(3), 271–282.

P1: TIX/XYZ
JWST070-REF

262

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

Rost J and Langeheine R (eds) 1997 Applications of Latent Trait and Latent Class Models in
the Social Sciences. Waxmann, Münster.
Rubin DB and Thayer DT 1982 EM algorithms for ML factor analysis. Psychometrika 47,
69–76.
Samejima F 1969 Estimation of latent ability using a response pattern of graded scores.
Psychometrika, Monograph Supplement No. 17.
Sammel RD, Ryan LM and Legler JM 1997 Latent variable models for mixed discrete and
continuous outcomes. Journal of the Royal Statistical Society, B 59, 667–678.
Saris WE and Stronkhorst LH 1984 Causal Modelling in Non-experimental Research: An
Introduction to the LISREL Approach. Sociometric Research Foundation, Amsterdam.
Schilling S and Bock RD 2005 High-dimensional maximum marginal likelihood item factor
analysis by adaptive quadrature. Psychometrika 70, 533–555.
Schneeweiss H 1993 Consistency at large in models with latent variables In Statistical Modelling and Latent Variables (ed. Haagen K, Bartholomew DJ and Deistler M) North Holland
Amsterdam pp. 299–320.
Schönemann PH 1966 A generalized solution of the orthogonal Procrustes problem. Psychometrika 31, 1–10.
Schönemann PH 1981 Power as a function of communality in factor analysis. Bulletin of the
Psychometric Society 17(1), 57–60.
Schuessler KF 1982 Measuring Social Life Feelings Jossey-Bass Social and Behavioral Science
Series. Jossey-Bass, San Francisco.
Schumacker R and Marcoulides G (eds) 1998 Interaction and Nonlinear Effects in Structural
Equation Modeling. Lawrence Erlbaum Associates, Mahwah, NJ.
Schwarz G 1978 Estimating the dimension of a model. Annals of Statistics 6, 461–464.
Sclove S 1987 Application of model-selection criteria to some problems of multivariate analysis. Psychometrika 52, 333–343.
Seber GAF 1984 Multivariate Observations. John Wiley & Sons, Inc., New York.
Shi JQ and Lee SY 1997a A Bayesian estimator of factor score in confirmatory factor model
with polytomous, censored or truncated data. Psychometrika 62, 29–50.
Shi JQ and Lee SY 1997b Estimation of factor scores with polytomous data by the E-M
algorithm. British Journal of Mathematical and Statistical Psychology 50, 215–226.
Shi JQ and Lee SY 1998 Bayesian sampling-based approach for factor analysis model with
continuous and polytomous data. British Journal of Mathematical and Statistical Psychology
51, 233–252.
Skrondal A and Rabe-Hesketh S 2004 Generalized Latent Variable Modeling: Multilevel,
Longitudinal, and Structural Equation Models. Chapman & Hall/CRC, Boca Raton, FL.
Smith GA and Stanley G 1983 Clocking g: Relating intelligence and measures of timed
performance. Intelligence 7, 353–368.
Sobel ME 1994 Causal inference in latent variable models In Latent Variables Analysis
(ed. van Eye A and Clogg CC) Sage Publications Thousand Oaks, CA pp. 3–35.
Song XY and Lee SY 2004 Bayesian analysis of two-level nonlinear structural equation models
with continuous and polytomous data. British Journal of Mathematical and Statistical
Psychology 57, 29–52.
Song XY and Lee SY 2006 Bayesian analysis of structural equation models with nonlinear
covariates and latent variables. Multivariate Behavioral Research 41, 337–365.

P1: TIX/XYZ
JWST070-REF

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

263

Spearman C 1904 General intelligence, objectively determined and measured. American
Journal of Psychology 15, 201–293.
Spiegelhalter D, Thomas A, Best N and Gilks W 1996 BUGS: Bayesian inference using Gibbs
sampling. Technical report, MRC Biostatistics Unit, Institute of Public Health, Cambridge.
Stephenson W 1935 Correlating persons instead of tests. Character and personality 4, 17–24.
Stout W 1987 A nonparametric approach for assessing latent trait unidimensionality. Psychometrika 52, 589–617.
Stout W 1990 A new item response theory modeling approach with applications to unidimensionality assessment and ability estimation. Psychometrika 55, 293–325.
Tabachnick BG and Fidell LS 1996 Using Multivariate Statistics 3rd edn. Harper & Row,
New York.
Takane Y and de Leeuw J 1987 On the relationship between item response theory and factor
analysis of discretized variables. Psychometrika 52, 393–408.
Tanaka JS and Huba GJ 1985 A fit index for covariance structure models under arbitrary GLS.
British Journal of Mathematical and Statistical Psychology 38, 197–201.
Tanner MA and Wong WH 1987 The calculation of posterior distributions by data augmentation. Journal of the American Statistical Association 82, 528–540.
Thissen D and Steinberg L 1984 A response model for multiple choice items. Psychometrika
49, 501–519.
Thissen D and Steinberg L 1986 A taxonomy of item response models. Psychometrika 51,
567–577.
Thomson GH 1916 A hierarchy without a general factor. British Journal of Psychology 8,
271–281.
Thomson GH 1951 The Factorial Analysis of Human Ability 5th edn. University of London
Press, London.
Thurstone LL 1947 Multiple Factor Analysis. University of Chicago Press, Chicago.
Tipping ME and Bishop CM 1999 Probabilistic principal component analysis. Journal of the
Royal Statistical Society B 21(3), 611–622.
Titterington DM, Smith AFM and Makov UE 1985 Statistical Analysis of Finite Mixture
Distributions. John Wiley & Sons, Ltd, Chichester.
Tucker LR, Koopman RF and Linn RL 1969 Evaluation of factor analytic research procedures
by means of simulated correlation matrices. Psychometrika 34, 421–459.
Tuma NB and Hannan MT 1984 Social Dynamics, Models and Methods. Academic Press, San
Diego, CA.
Tzamourani P and Knott M 2002 Fully semiparametric estimation of the two-parameter latent
trait model for binary data In Latent Variable and Latent Structure Models (ed. Marcoulides
GA and Moustaki I) Lawrence Erlbaum Associates Mahwah NJ pp. 63–84.
Upton GJ 1980 Contribution to the discussion of Bartholomew (1980). Journal of the Royal
Statistical Society, B 42, 316.
van der Linden W and Hambleton RK (eds) 1997 Handbook of Modern Item Response Theory.
Springer, New York.
van Driel OP 1978 On various causes of improper solutions in maximum likelihood factor
analysis. Psychometrika 43, 225–243.
Vermunt JK and Magidson J 2000 Latent GOLD 4.5 User’s Guide. Statistical Innovations,
Belmont, MA.

P1: TIX/XYZ
JWST070-REF

264

P2: ABC
JWST070-Bartholomew

May 9, 2011

13:52

Printer Name: Yet to Come

REFERENCES

Wall M and Amemiya Y 2000 Estimation of polynomial structural equation models. Journal
of the American Statistical Association 95, 929–940.
Wall M and Amemiya Y 2001 Generalized appended product indicator procedure for nonlinear
structural equation analysis. Journal of Educational and Behavioural Statistics 26, 1–29.
Wedel M and DeSarbo WS 1995 A mixture likelihood approach for generalized linear models.
Journal of Classification 12, 21–55.
White H 1982 Maximum likelihood estimation of misspecified models. Econometrics 50,
1–25.
Whittaker J 1990 Graphical Models in Applied Multivariate Analysis. John Wiley & Sons,
Inc., New York.
Whittle P 1953 On principal components and least square methods of factor analysis. Skandinavisk Aktuarietidskrift 35, 223–239.
Williams JS 1978 A definition for the common factor analysis model and the elimination of
problems of factor score indeterminacy. Psychometrika 43, 293–306.
Wold H 1975 Soft modelling by latent variables: the non-linear iterative partial least squares
(NIPALS) approach In Perspectives in Probability and Statistics: Papers in honour of M. S.
Bartlett (ed. Gani J) Academic Press London pp. 117–142.
Wold H 1980 Model construction and evaluation when theoretical knowledge is scarce: theory
and application of partial least squares In Evaluation of Econometric Models (ed. Kmenta
J and Ramsey JB) Academic Press London pp. 47–74.
Wold H 1982 Soft modelling: the basic design and some extensions In Systems under Indirect
Observation: Causality, Structure, Prediction, Part II (ed. Jöreskog K and Wold H) North
Holland Amsterdam.
Wollack JA, Bolt DM, Cohen AS and Lee YS 2002 Recovery of item parameters in the nominal
response model: A comparison of marginal maximum likelihood estimation and Markov
chain Monte Carlo estimation. Applied Psychological Measurement 26(3), 339–352.
Wright DB, Gaskell G and O’Muircheartaigh C 1994 Flashbulb memory assumptions: Using
national surveys to explore cognitive phenomena. Technical Report 11, London School of
Economics Methodology Institute.
Wright S 1921 Correlation and causation. Journal of Agricultural Research 20, 557–585.
Wu CF 1983 On the convergence properties of the EM algorithm. Annals of Statistics 11,
95–103.
Yang Jonsson F 1997 Simulation Studies of the Kenny-Judd Model PhD thesis Uppsala University.
Zegers FE and ten Berge JMF 1983 A fast and simple computational method of minimum
residual factor analysis. Multivariate Behavioral Research 18, 331–340.
Zhu H and Lee SY 1999 Statistical analysis of nonlinear factor analysis models. British Journal
of Mathematical and Statistical Psychology 52, 225–242.
Zucchini W and MacDonald IL 2009 Hidden Markov Models for Time Series: An Introduction
Using R Monographs on Statistics and Applied Probability. Chapman & Hall/CRC, Boca
Raton, FL.

P1: TIX/XYZ
P2: ABC
JWST070-AUIND
JWST070-Bartholomew

May 9, 2011

13:56

Printer Name: Yet to Come

Author index
Ahumada, A., 62
Aitkin, M., 90, 92, 164
Akaike, H., 58, 201
Albanese, M.T., 42, 100, 109
Albert, J.H., 31, 103
Amemiya, Y., 26, 63
Amemiya, T., 210
Andersen, E.B., 16, 17, 23, 44
Anderson, T.W., 12, 15, 17, 60, 62, 63,
65, 67, 164, 231, 239
Arbuckle, J.L., 213
Archer, C.O., 56
Arminger, G., 26, 144, 192, 209
Asparouhov, T., 186

Bolck, A., 227, 241
Bollen, K.A., 16, 26, 31, 213, 220
Bolt, D.M., 31
Boomsma, A., 67
Bozdogan, H., 59
Brokken, F.B., 215
Brooks, S.P., 108
Browne, M.W., 56, 59, 71, 221
Burt, R.S., 239, 242
Busemeyer, J., 27

Baker, F.B., 31
Barankin, E.W., 22
Barnes, S., 154
Barrett, P.T., 114, 115
Bartholomew, D.J., ii, 12, 13, 16, 26, 42,
46, 72, 74, 98, 99, 101, 112, 113, 168,
170, 215, 224, 237, 238, 243
Bartlett, M.S., 26, 58, 73
Basilevsky, A., 15, 44, 63, 232
Bassin, E.L., 112
Beguin, A.A., 31
Bentler, P.M., 16, 71, 142, 143, 213
Bergsma, W., 178
Best, N., 108, 112
Birnbaum, A., 16, 23, 31
Bishop, C.M., 232, 233
Bock, R.D., 44, 90, 92, 93, 123

Caffrey, J., 45, 70
Cai, L., 99, 110, 116, 149, 155, 160
Carlin, B.P., 108
Carlson, E., 136
Cattell, R.B., 15, 64
Chamberlain, G., 18
Chang, H.H., 43, 100
Chatfield, C., 16
Chatterjee, S., 56
Chen, W.H., 115, 155
Chib, S., 31
Chin, W., 239
Christoffersson, A., 96, 97
Cliff, N., 215
Clogg, C.C., 170, 176
Coffman, D.L., 99
Cohen, A.S., 31
Collins, L.M., 16, 98
Comrey, A.L., 62
Cornwell, J.M., 239
Cowles, M.K., 108

Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

265

P1: TIX/XYZ
P2: ABC
JWST070-AUIND
JWST070-Bartholomew

266

May 9, 2011

13:56

Printer Name: Yet to Come

AUTHOR INDEX

Cox, D.R., 216
Cronbach, L.J., 45
Croon, M., 178, 185, 227, 241
Cudeck, R., 15, 55, 56, 71, 221
Cuttance, P., 213
Dayton, C.M., 158, 167
de Leeuw, J., 89
de Menezes, L., 56, 112, 165, 168, 170
Deary, I.J., 238
Dempster, A.P., 53, 90, 162
DeSarbo, W.S., 180
Dolby, G.R., 72
du Toit, S.H.C., 110, 116, 149, 155, 160
Dunson, D.B., 31
Ecob, R., 213
Esposito Vinzi, V., 239
Etezadi-Amoli, J., 26
Everitt, B.S., 16, 157, 181
Eysenck, H.J., 114, 115
Eysenck, S.B.G., 114, 115
Fachel, G.M.G., 64, 67
Fidell, L.S., 70
Fidler, P.L., 98
Fielding, A., 12
Forbes, J.F., 57, 158, 170, 174
Formann, A.K., 157
Fox, J.P., 31
Francis, I., 56, 59
Franzoi, S.L., 155, 177
Galbraith, J., ii, 99, 243
Galton, F., 13
Gaskell, G., 204
Gelfand, A.E., 31, 102
Gelman, A., 108, 109
Geman, D., 102
Geman, S., 102
Gerbing, D.W., 67, 239
Geweke, J.F., 58, 108
Gilks, W., 108, 112
Glas, C.A.W., 31, 110
Goldberger, A.S., 60

Goldstein, H., 40
Goodman, L.A., 157, 162, 170, 180
Green, B.F., 215
Greenacre, M.J., 236
Groenen, P.J.F., 236
Guttman, L., 63
Haberman, S.J., 157, 162
Hadman, M.A., 141
Hagenaars, J.A., 178
Hakistian, A.R., 64
Hambleton, R.K., 16, 136
Hand, D.J., 181
Hannan, M.T., 18
Harman, H.H., 15, 62, 79
Hastings, W.K., 105
Heckman, J.J., 18
Heidelberger, P., 108
Heinen, T., 16, 40, 157, 162
Henry, N.W., 16, 157, 182
Henseler, J., 239
Heywood, H.B., 67
Hills, M., 16
Hinde, J., 164
Hoem, J.M., 18
Holland, P.W., 41, 42, 100
Hotelling, H., 15, 16, 229
Huba, G.J., 59
Huber, P., 93
Hunter, J.E., 239
Ihara, M., 62
Jöreskog, K.G., 16, 52, 53, 60, 63,
141–144, 154, 192, 213, 218
Jennrich, R.I., 56, 81
Jensen, A.R., 237
Joe, H., 99, 148
Johnson, R.A., 31, 56
Jones, W.H., 27, 62
Judd, C.M., 26
Junker, B.W., 31, 100, 106, 107
Kaase, M., 154
Kaiser, H.F., 45, 69, 70

P1: TIX/XYZ
P2: ABC
JWST070-AUIND
JWST070-Bartholomew

May 9, 2011

13:56

Printer Name: Yet to Come

AUTHOR INDEX

Kano, Y., 65
Kaplan, D., 213
Kenny, D.A., 26
Kim, S.H., 31
Knott, M., 31, 39, 42, 62, 74, 100, 109,
110, 192, 204
Koopman, R.F., 56
Krane, W.R., 66
Krebs, D., 84, 112
Krishnan, J., 53
Kuha, J., 42
Küsters, U., 144, 192
Laird, N.M., 53, 90, 162
Lance, C.E., 239
Langeheine, R., 16, 98, 157
Lawley, D.N., 15, 50, 51, 55, 56, 59,
215, 231
Lawn, M., 238
Lazarsfeld, P.F., 16, 157, 182
Lee, Y., 17, 26, 31, 68, 94, 142, 143,
148
Lee, S.-Y., 213
Legler, J.M., 93, 192
Leimu, H., 152
Leung, S.O., 99, 152
Lewis, S., 108
Linn, R.L., 56
Linting, M., 236
Loehlin, J.C., 213
Long, J., 98
Lopez, J., 31, 109
MacCullum, R., 15
MacDonald, I.L., 18
Macready, G.B., 158, 167
Madsen, M., 17
Magidson, J., 160, 164, 177
Magnus, J.R., 50, 51, 69
Maitra, E.P., 22
Makov, U.E., 17
Maraun, M.D., 12, 41, 72
Marcoulides, G., 26
Martin, J.K., 68
Martinson, E.O., 141

267

Masters, G.M., 140, 170, 176
Maxwell, A.E., 15, 50, 51, 56, 59,
215
Maydeu-Olivares, A., 99, 148
McDonald, R.P., 15, 26, 66, 68, 70
McFadden, D., 144
McHugh, R.B., 162
McLachlan, G.J., 53
Mels, G., 56, 71
Meng, X.L., 94
Metropolis, N., 105
Meulman, J.J., 236
Mislevy, R.J., 186
Molenaar, P.C.W., 184
Moran, P.A., 89
Moustaki, I., ii, 11, 26, 27, 31, 44, 99,
110, 136, 143, 144, 154, 192, 204,
205, 207, 243
Mulaik, S.A., 14, 15, 213, 239
Muraki, E., 136, 140
Muthén, B., 16, 26, 56, 70, 71, 75, 81,
97, 110, 142, 186, 191, 213
Muthén, L., 16, 56, 70, 71, 75, 81, 142,
213
Nelder, J.A., 17
Neudecker, H., 50, 51, 69
O’Dell, L.L., 55, 56
O’Muircheartaigh, C., 110, 204
Ogasawara, H., 56
Okamoto, M., 62
Olsson, U., 141
Owen, R.J., 31
Palomo, J., 31
Pannekoek, J., 98
Papageorgiou, I., 11, 207
Patz, R.J., 31, 106, 107
Paxton, P., 26
Pearson, K., 16
Pennell, R., 56
Pickering, R.M., 57, 158, 170, 174
Pimental, J.L., 110
Poon, W.-Y., 142, 143

P1: TIX/XYZ
P2: ABC
JWST070-AUIND
JWST070-Bartholomew

268

May 9, 2011

13:56

Printer Name: Yet to Come

AUTHOR INDEX

Rabe-Hesketh, S., 12, 93
Raftery, A.L., 108
Ramirez, D.E., 59
Rao, C.R., 51, 63, 70
Rasch, G., 140
Reiser, M., 99
Rizopoulos, D., 26, 27
Roberts, G.O., 108
Rogers, W.T., 16, 64
Ronchetti, E., 93
Rosenbaum, P.R., 100
Rosenbluth, A.W., 105
Rosenbluth, M.N., 105
Rost, J., 16, 157, 186
Rubin, D.B., 17, 53, 65, 90, 108, 109,
162, 231
Ryan, L., 93, 192
Sörbom, D., 142, 143, 154, 192, 218
Samejima, F., 136
Sammel, R.D., 93, 192
Saris, W.E., 213
Satorra, A., 142
Savage, L.J., 43
Schönemann, P.H., 58, 215
Schilling, S., 93, 94
Schneeweiss, H., 43, 46, 239
Schuessler, K.F., 46, 84, 101, 112
Schumacker, R., 26
Schwarz, G., 59, 201
Sclove, S., 202
Scott Long, J., 213
Seber, G.A.F., 16, 56, 57
Shi, J.-Q., 31, 68, 143, 148
Shield, S.A., 155, 177
Singer, B., 18
Singleton, K.J., 58
Skrondal, A., 12, 93
Smith, G.A., 17, 31, 75, 102
Smith, A.F.M., 102
Sobel, M.E., 3, 227
Song, X.-Y., 26, 31, 94
Spearman, C., 12, 237
Spiegelhalter, D., 108, 112
Stanley, G., 75

Steele, F., ii, 99, 243
Steinberg, L., 122, 123
Stephenson, W., 242
Stout, W., 43, 100
Stronkhorst, L.H., 213
Swaminathan, H., 16
Tabachnick, B.G., 70
Takane, Y., 89
Tanaka, J.S., 59
Tanner, M.A., 31
Tateneni, K., 56, 71
Teller, A.H., 105
Teller, E., 105
ten Berge, J.M.F., 62
Thayer, D.T., 53, 56
Thissen, D., 99, 110, 115, 116, 122,
123, 149, 155, 160
Thomas, A., 108, 112
Thomson, G.H., 13, 15, 73, 237,
242, 243
Thurstone, L.L., 14, 15, 237
Tipping, M.E., 232, 233
Titterington, D.M., 17
Tucker, L.R., 56
Tuma, N.B., 18
Tzamourani, P., 39, 98, 112, 113
Upton, G.J., 42
van de Pol, F., 98
Van der Kooij, J.J., 236
van der Linden, W., 136
van Driel, O.P., 67
Vandenberg, M., 99
Verhelst, N., 186
Vermunt, J.K., 160, 164, 177
Victoria-Feser, M.P., 93
von Eye, A., 184
Wall, M., 26
Wang, H., 239
Wedel, M., 180
Welch, P., 108
Wermuth, N., 216

P1: TIX/XYZ
P2: ABC
JWST070-AUIND
JWST070-Bartholomew

May 9, 2011

13:56

Printer Name: Yet to Come

AUTHOR INDEX

West, M., 31, 109
White, H., 27
Whittaker, J., 216
Whittle, P., 17
Wichern, D.W., 56
Williams, J.S., 46, 245
Wold, H., 239
Wollack, J.A., 31
Wong, W.H., 31
Wright, B.D., 140

Wright, D.B., 204
Wright, S., 216
Wu, C .F., 162
Wugalter, S.E., 98
Yang, F., 26
Zegers, F.E., 62
Zhu, H., 26, 94
Zucchini, W., 18

269

P1: TIX/XYZ
JWST070-IND

P2: ABC
JWST070-Bartholomew

May 9, 2011

11:43

Printer Name: Yet to Come

Subject index
ability testing, 112
acceptance/rejection sampling, 31
accident proneness, 17
adaptive quadrature, 93
Akaike’s information criterion (AIC),
58, 79, 201, 207
alpha factor analysis, 70
AMOS, 213
assumption (or axiom) of conditional
(or local) independence, 7
asymptotic standard errors, 29, 38, 141,
148, 165, 166, 182, 205
asymptotic theory of maximum
likelihood, 55
asymptotic variance–covariance matrix,
94, 99, 142, 165
attenuation, 241
attitude scaling, 109
average posterior variance, 101

Bernoulli random variable, 4, 21, 24,
196
binary data
examples, 167
latent trait models, 83
special case, 134
binary response, 33, 83, 86, 87, 95, 100
binomial distribution, 193
body satisfaction, 154, 177
bonds model, 13, 237, 238
bootstrap, 38, 56, 57, 112, 201
bootstrap samples, 98
BUGS, 108, 112, 247
Burt matrix, 236
business confidence, 2
canonical correlation, 70
categorical responses, 144
categorical variables, 236
category scores, 124, 127, 141, 149,
150, 152, 154
Cauchy inequality, 73
causal relationships, 216, 239
CEFA, 56, 71, 247
chi-squared test, 97, 110, 221
choice model, 146
cluster analysis, 244
coefficient alpha, 45
common factor, 13
communality, 49, 75, 77
comparative fit index (CFI), 221
complementary log link function, 40

Bartlett scores, 73, 227, 242
Bayes estimates, 31
Bayes’ theorem, 7, 41, 163
Bayesian approach, 32, 68
Bayesian estimation, 26, 30, 31,
102
Bayesian inference, 30, 43, 102
Bayesian information criterion (BIC),
59, 201
Bayesian minimal sufficient statistic,
192
Bernoulli distribution, 20, 193, 196

Latent Variable Models and Factor Analysis: A Unified Approach, Third Edition.
David Bartholomew, Martin Knott and Irini Moustaki.
© 2011 John Wiley & Sons, Ltd. Published 2011 by John Wiley & Sons, Ltd.

271

P1: TIX/XYZ
JWST070-IND

272

P2: ABC
JWST070-Bartholomew

May 9, 2011

11:43

Printer Name: Yet to Come

SUBJECT INDEX

component score, 112, 113, 152, 153
components, 21, 26, 37, 41, 42, 49, 86,
121, 151, 166, 167, 193–195, 203,
227, 228, 240, 241
conditional density, 6, 7
conditional distributions, 8, 19, 22, 27
conditional expectation, 203
conditional probability, 120, 123, 137,
140
confidence intervals, 56
confirmatory factor analysis, 50, 213,
218
confirmatory mode, 215
conservatism, 2
consistency, 64
continuous variable, 87
convergence, 92, 94
convergence diagnostics, 108
correlated latent variables, 213
correlation coefficient, 141, 149, 152
correlation matrix, 13, 14, 50, 59, 61,
65, 66, 69, 70, 79, 85, 93
correspondence analysis, 124, 127,
149, 151
covariance matrix, 28, 29, 35, 42, 49,
58, 59, 65, 66, 97, 99, 133, 214,
219, 224, 227, 230, 232, 234, 235,
240, 243
covariance structure, 224
Cramér–von Mises statistic, 108
Cronbach’s alpha, 45
cross-validation, 57
cumulant generating function, 42
cumulative probability, 171, 208
cumulative response function, 137
data augmentation, 103
density function, 146
dependent latent variables, 218, 219,
223–225
diagonal elements, 128
diagonal matrix of variances, 9
diagonality condition, 36
difficulty parameter, 43
dimension reduction, 1, 22

dimensionality, 1
discrete (multivariate) prior
distribution, 24
discrimination parameter, 43, 86
distribution function, 86, 89, 90
distribution theory, 8
distribution-free, 27, 28
divergence of estimation algorithm, 109
E-M algorithm, 53, 90, 92, 93, 112,
119, 125, 138, 148, 162, 164, 174,
182, 189, 195, 199, 200
econometric models, 18
educational testing, 16, 83, 84, 86, 109
eigenvalues, 130, 132
eigenvectors, 130, 132
employment data, 153
endogenous latent variables, 218, 223
EQS, 16, 213
equivalence, 87, 88, 127, 145
exogenous latent variables, 218, 223
exponential
parameter, 27
exponential family, 20, 23, 27, 29, 32,
41, 48, 85, 101, 121, 136, 185,
191, 192, 195, 196, 199, 220, 222,
223, 227
one-parameter, 19, 29
exponential type, 22
extreme-value distribution, 145–147
Eysenck’s Personality Questionnaire,
114
factor
general, 14
loadings, 86
specific, 14
factor analysis, 1, 9, 10, 12
factor loadings, 31, 35, 47, 49, 55, 56,
69, 70, 76, 117
factor model, 10, 13, 16, 24
factor scores, 41, 42, 44, 46, 71, 72, 74
factor space, 69, 71
factors, 2
finite mixture model, 17

P1: TIX/XYZ
JWST070-IND

P2: ABC
JWST070-Bartholomew

May 9, 2011

11:43

Printer Name: Yet to Come

SUBJECT INDEX

fit indices, 221
frequency distribution, 84, 112
gamma distribution, 18, 20, 105, 194
Gauss–Hermite quadrature, 92, 93,
126, 139, 187
general intelligence, 2
general linear latent variable model
(GLLVM), 19–21, 23–25, 27,
33–35, 37, 38, 44, 86, 100, 121,
161, 191–193, 195, 198, 213
fitting by maximum likelihood, 29
fitting the models, 27
interpretation, 35
properties, 20
sampling error, 38
special cases, 21, 22, 24
vector-valued, 121
generalisability, 70
generalisability coefficient, 46
generalised least squares, 60, 65, 90,
93, 94, 96, 219
generalised linear model, 20
generalised partial credit model, 140
generally weighted least squares, 220
Gibbs sampler algorithm, 33, 94, 143
Gibbs sampling, 31, 102, 103, 106, 112
GLLAMM, 247
goodness of fit, 58, 63
goodness-of-fit test, 5, 6, 27, 29, 65, 67,
69, 78, 79, 84, 97–99, 113, 117,
148, 149, 152, 154, 167, 176, 201
graded response model, 136
graphical models, 216
Heywood cases, 31, 50, 51, 53, 67, 69,
75, 77, 109
hidden factor, 4
hidden variables, 18
Hillsborough football disaster, 204
Hotelling’s method, 229
hybrid models, 11, 186
identifiability, 178, 180, 223, 227
image factor analysis, 63

273

incremental fit index (IFI), 222
independence chain, 105
independent latent variables, 218, 219,
223–225
index of generalisability, 45
indistinguishable models, 224
inference, 32, 33, 40, 41
information matrix, 27, 94
interaction, 26
intercept, 85
intercultural version, 112
IRTPRO, 110, 116, 149, 155, 160, 247
isotonic regression, 211
isotropic normal model, 231
item characteristic curve, 85
item response curve, 43, 161
item response function, 85, 100, 161
iterative weighted least squares
analysis, 126
jackknife, 56
Jacobian, 180
joint distribution, 84, 100, 102, 222,
224, 225
joint probability, 162, 188
joint probability function, 171, 172
Kaiser–Guttman criterion, 64
label switching, 31, 32, 109
Laplace approximations, 93
latent class analysis, 12
latent class model, 17, 22, 24, 41, 83,
98, 109, 157, 223
allocation of individuals, 174
for binary data as a latent trait model,
159
ordered, 171
unordered variables, 170
with binary manifest variables, 158
Latent GOLD, 160, 164, 177, 247
latent profile models, 24, 181, 184, 226
latent structure analysis, 16, 157
latent trait models, 16, 24, 98, 149, 157,
159–161, 166, 176, 186, 200, 223

P1: TIX/XYZ
JWST070-IND

274

P2: ABC
JWST070-Bartholomew

May 9, 2011

11:43

Printer Name: Yet to Come

SUBJECT INDEX

latent variable methods
broader theoretical view, 6
classification, 11, 24
closely related fields, 17
historical context, 12
statistical problem, 1
latent variables, 2, 200
intercorrelations, 217
relationships between, 213
Law School Admission Test, 84,
109–111, 160, 165
least squares
posterior analysis, 72
least squares methods, 59
choice of q, 63
life satisfaction data, 176, 177
likelihood function, 27, 29
approximation, 95, 96
limited-information estimation method,
154
limited-information goodness-of-fit
test, 99, 113
limited-information maximum
likelihood estimation, 142
linear factor models, 226
linear normal isotropic model, 232
linear structural relations modelling
(LISREL), 16, 154, 192, 213, 214,
239, 242, 247
link function, 21, 25
LISREL model, 218, 224, 226, 227
generalisations, 223
measurement model, 219
structural model, 218
loading matrix, 35–37
log-likelihood, 129, 130, 138, 162, 172,
187
bivariate, 143
univariate, 143
log-likelihood ratio statistic, 67, 97
logistic distribution, 141, 147
logistic response functions, 85
logit model, 119, 121
polytomous data, 123, 124
logit response function model, 145

logit/normal model, 84, 87, 90,
100–102, 109, 123, 136, 144, 159,
161, 187, 215
manifest variables, 3, 11
of mixed type, 191
marginal frequencies, 128, 129, 148
marginal probabilities, 84, 88, 98
Markov chain, 18, 30, 33, 102, 105, 108
Markov chain Monte Carlo (MCMC),
30–32, 102, 108
mastery model, 167
maximum likelihood
composite, 143
maximum likelihood estimation, 22,
27, 29, 39, 44, 48, 50, 52, 53, 59,
61, 64, 65, 77, 81, 90, 92, 94, 96,
126, 140, 142, 151, 162, 181, 184,
187, 195, 214, 220, 223
binary data, 172
marginal, 31
polytomous logit model, 125, 236
with ordinal variables, 138
maximum likelihood estimators, 17,
126
approximate, 95, 127
sampling properties, 94
maximum likelihood method, 141
measurement model, 218, 223, 227,
239
metrical variables, 6, 11
Metropolis–Hastings algorithm (M-H),
31, 33, 102, 103, 105, 106, 112
minimum trace, 62
minres method, 62
missing values, 110
mixed latent class models, 202
moment generating function, 42
Monte Carlo E-M (MCEM), 93
Monte Carlo integration, 32, 94
Monte Carlo methods, 93
Mplus, 16, 56, 70, 71, 75, 77, 78, 81,
192, 213, 214, 247
multi-group factor analysis, 44
multinomial distribution, 193

P1: TIX/XYZ
JWST070-IND

P2: ABC
JWST070-Bartholomew

May 9, 2011

11:43

Printer Name: Yet to Come

SUBJECT INDEX

multinomial probability, 129
multiple correspondence analysis, 136,
149, 236
multiple correspondence scores, 236
multivariate data analysis, 1
multivariate normal distribution, 8, 9
natural parameter, 23
Newton–Raphson, 92
nominal model, 148
non-linear models, 25–27
non-linear principal components
analysis, 236
non-metric multidimensional scaling,
243
non-normed fit index (NNFI), 222
normal distributions, 6, 10
normal linear factor model (NLFM),
24, 38, 47, 87, 119, 134
approximate methods for
estimating , 62
constraints, 50
distributional properties, 48
fitting without normality
assumptions, 59
normal random variables, 48, 196
normalising constant, 32, 103, 105, 106
nuisance parameter, 194–196
oblique rotations, 35, 56, 69, 70, 77,
214
geomin, 81
OBLIMIN, 70, 77
one-dimensional latent space, 26
one-factor latent trait model, 208
one-factor logit/normal model, 117
one-factor model, 26, 46, 103, 150
one-to-one transformation, 9, 37
operant subjectivity, 244
ordered categorical data, 236
ordered categorical variables, 208
ordered latent classes, 185
ordering of categories, 136
orthogonal rotation, 34, 38, 56, 69, 132
orthogonal transformation, 10, 124, 132

275

pairwise residuals, 113
parametric bootstrap method, 98, 165
partial credit model, 140
partial least squares (PLS), 239
path analysis, 239
pattern loading matrix, 70
Perron–Frobenius theorem, 130
phi-coefficient matrix, 95, 96, 136
physical data, 234
physical variables, 79
PLS, 239
Poisson distribution, 17, 194
Poisson variable, 29
political action survey, 154
polychoric correlations, 141, 142, 191,
209
polyserial correlations, 191, 209
polytomous data
analysis using logit model, 149
latent trait models, 119
more than one factor, 130
one factor, 127
unordered, 170, 174
polytomous logit model
maximum likelihood estimation,
125
positive response, 86, 122
posterior analysis, 20, 41, 100, 113,
147, 203, 228
least squares, 72
reliability, 74
the normal case, 71
with binary manifest variables,
166
with metrical manifest variables,
180
with unordered manifest variables,
171
posterior covariance matrix, 71
posterior density, 141
posterior density function, 120
posterior distribution, 5, 9, 20–22,
30–33, 39, 42, 43, 100, 102
posterior mean, 100, 101
posterior probability, 41

P1: TIX/XYZ
JWST070-IND

276

P2: ABC
JWST070-Bartholomew

May 9, 2011

11:43

Printer Name: Yet to Come

SUBJECT INDEX

posterior variance, 71, 101
principal axis method, 63
principal components, 11, 12, 21, 96,
136, 228, 230
principal components analysis (PCA),
16, 56, 61, 64, 229, 233, 235, 236,
239, 242
principal factor method, 56, 61
principle of precise measurement, 43
prior distribution, 9, 19, 22, 32, 33, 39,
43, 44, 85, 88, 100, 108, 159
prior information, 31
probit link, 154
probit/normal model, 31, 86, 93, 96,
102, 103, 123, 136, 154
Procrustes methods, 215
proportional odds model, 137, 140,
154
psychometric inference, 44
Psychometrika, 14
Q-analysis, 15, 46, 242, 243
Q-methodology, 242
quality of life, 2
quartimax, 69
R-methodology, 242
random effects Rasch model, 44, 86
random score, 40
random variables, 6
random walk chain, 105
Rasch model, 136, 140, 215
reference category, 121
regression coefficients, 217, 227
regression scores, 74
reification, 2
rejection probability, 105
rejection sampling, 103, 106
relative fit index, 221
reliability, 74, 101
reproduced covariance matrix, 221
residuals, 201
response
binomial, 29
gamma, 28

response function, 21, 34, 46, 85, 87,
88, 109, 120, 122, 161
response function model, 136, 141,
143, 148, 149
response patterns, 83, 84, 92, 98, 99
pooling, 148
reversible jump MCMC, 31
root mean square error of
approximation (RMSEA), 221
rotated factors, 56
rotated solution, 95, 184
rotation, 9, 31, 33, 35, 69, 124, 214
see also oblique and orthogonal
rotational indeterminacy, 109
S-Plus, 247
saddle point, 164
sample correlation matrix, 64, 66,
69
sample covariance matrix, 96, 198,
219–221
sample size, 56
sample standard deviations, 65
sampling behaviour, 55
sampling error of parameter estimates,
38
sampling properties
of maximum likelihood estimators,
201
sampling variables, 43, 46
sampling variation of estimators, 55
sandwich estimator, 27
scale-invariant estimators, 66
scaled components, 242
score function, 53–55
Scottish infants, 174
scree plot, 79
scree test, 64
shape parameters, 39
simple structure, 36, 37
simulated states, 33
single binary latent variable, 4
single latent variable, 122, 138
singular value decomposition, 37
small industry data, 152

P1: TIX/XYZ
JWST070-IND

P2: ABC
JWST070-Bartholomew

May 9, 2011

11:43

Printer Name: Yet to Come

SUBJECT INDEX

Smith and Stanley data, 75
social life feelings, 84, 112
social surveys, 1
specific factor, 13, 237
specific variances, 47
SPSS, 229, 247
staff assessment data, 149
standard deviations, 56
standard errors, 27, 55, 56, 94, 110,
112, 119, 150, 164, 214
standard form, 66
standard linear factor model, 140
standard normal distribution, 9
standardised χ 2 local dependence (LD)
index, 115, 155
standardised loadings, 69, 75
standardised root mean square residual
(SRMR), 221
STATA, 247
stationarity conditions, 51
stationary distribution, 33
statistical inference, 44
stochastic subject rationale, 41
structural equation modelling, 239
structural model, 40, 218, 227
structural path analysis, 228
structural relationships, 218, 222
structure loading, 70
sufficiency, 87, 121
sufficiency principle, 22, 119, 141,
208–210
sufficient statistic, 25, 26, 37, 48, 54,
100, 159, 228, 240
q-dimensional, 22
minimal, 21–23

277

Taylor expansion, 127, 129, 131
terracotta data, 207
test–retest correlation, 101, 113
tetrachoric correlations, 93, 97, 191
tetrad differences, 13
thresholds, 140
tobit relation, 210
transition kernel, 33, 105
transition probabilities, 30
two-factor model, 26, 27, 79, 154
two-factor solution, 155
Type I distribution, 146
underlying factor model, 142
underlying latent variable model, 228
underlying variable, 96, 191
underlying variable approach
and response function, 88
modelling categorical data, 87
underlying variable model, 89, 90, 119,
136, 140–143, 145, 147, 148, 206,
208
alternative, 144
binary, 136
unidimensionality, 100
univariate distributions, 84
unobserved heterogeneity, 18
unordered categorical data, 235
unweighted least squares, 219
varimax, 69
weighted least squares, 142
welfare measurement, 207
WinBUGS, 247

