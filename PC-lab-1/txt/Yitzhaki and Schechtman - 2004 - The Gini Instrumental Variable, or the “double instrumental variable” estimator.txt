METRON - International Journal of Statistics
2004, vol. LXII, n. 3, pp. 287-313

SHLOMO YITZHAKI – EDNA SCHECHTMAN

The Gini Instrumental Variable,
or the “double instrumental variable”
estimator
Summary - This paper puts OLS and Gini regression in a common framework by
showing that the coefficients can be interpreted as weighted averages of slopes between adjacent observations, where weights are derived from the Absolute Lorenz
Curve of the independent variable. Instrumental Variable (IV) estimators, under both
approaches, are also put in a common framework, and viewed as weighted sums of the
same slopes, with weights derived from Absolute Concentration Curve of the instrument with respect to the independent variable. This interpretation enables derivation
of sufficient conditions for monotonic transformations to change the signs of the instrumental variables’ estimators. These conditions inform the user how robust the
conclusion with respect to the sign of the estimate really is. It is also shown that
Gini-IV is less sensitive to outliers and monotonic transformations than OLS-IV, it
has built-in test for examining the validity of IV, and it can be used to test the sensitivity of IV estimator to OLS regression method.
The estimation is not based on a specific set of assumptions. However, different
assumptions lead to proper choices of weights. In this sense, it follows the spirit of
Gini (1957) of analyzing the implication of the use of different weighting schemes.
Key Words - Instrumental Variable; Ordinary least squares; Gini’s mean difference;
Gini regression.

1. Introduction
The method of Instrumental Variables (IV) is widely used to estimate
parameters when (some of) the regressors are endogenous. Its popularity has
recently increased as a result of its application to the evaluation of the impact
of social programs (Angrist, 1990, and others). Recent investigations of its
properties include, among others, Angrist, Imbens and Rubin (1996), Angrist
and Evans (1998), Bound et al. (1995) and Heckman (1995), who pointed out
its main advantages and drawbacks.
Received March 2004 and revised September 2004.

288

SHLOMO YITZHAKI – EDNA SCHECHTMAN

In this paper we compare the properties of the IV estimation method(1 ) under two alternative metrics. The standard IV estimator is based on a minimization of a quadratic function of the errors, as in Ordinary Least Squares (OLS).
We shall refer to this as OIV. In theory, one could also conceive of using
an IV method under another metric such as the expected absolute difference
between two random draws of the errors(2 ). We shall refer to this estimator as the GIV estimator, because it is based on using IV in a regression
method based on Gini Mean Difference (GMD)(3 ). The main purpose of this
paper is to put OLS and Gini regression in a common framework by showing that the regression coefficients can be interpreted as weighted averages
of slopes between adjacent observations. We note that we do not make any
specific assumptions, but rather show that the different choices of weights depend on the assumptions made. For example, if the model is linear, with
the commonly used assumptions, then both methods (Gini and OLS) produce
the same estimates. Otherwise, the estimators are still weighted averages of
slopes, but the weights arising from the two methods will be different. We
consider this approach as an application of the methodologies proposed by
Corrado Gini (1957).
In addition, this paper shows that the GIV estimator is less sensitive to extreme observations and to violation of the linearity assumption than the standard
IV estimator (OIV). This is a direct result of stronger robustness of Gini-based
estimators relative to estimators based on the minimization of a sum of squared
errors (OLS estimators). The GIV estimator can be used either as an independent or “stand alone” estimator or as an additional estimator for evaluating the
sensitivity of the IV method to the OLS metric.
Whether or not an alternative estimation method is needed depends crucially
on the answer to the following question: can two investigators who use the
same variables, the same estimation method and the same data set possibly
reach contradictory conclusions with respect to the sign (or magnitude) of the
main coefficient? Note the qualification “the same data set”, which means that
the contradictory signs of the coefficient may continue to appear even if the
entire population is used in the estimation procedure. This paper argues that
if investigators are allowed free choice of monotonic transformations of the
variables [i.e., one uses x and the other uses log(x)] the answer to the above
question is in the affirmative. Clearly, this opens the way to data-manipulation
and therefore, an additional instrumental variable method, which is less sensitive
to monotonic transformations, is welcome.
(1 ) We will refer to an estimation method as an IV method if a variable Z , not included in the original
model, is used to identify the model.
(2 ) A third option is the LAD method, which is based on a minimization of absolute deviation of the
errors from the regression line (Bassett and Koenker (1978)).
(3 ) Abadie, Angrist and Imbens (1998) apply the IV method in a quantile regression.

The Gini Instrumental Variable, or the “double instrumental variable” estimator

289

In a recent paper, Schechtman and Yitzhaki (2000) propose using the Gini
regression as an alternative to OLS. The Gini regression is similar to OLS,
except that Gini Mean Difference is used as a measure of variability. This
change in the variability measure (i.e., the metric) leads to robust estimators
and provides a way to derive built-in linearity tests.
In this paper we introduce the GIV and show that it is similar in its
presentation to the standard IV estimator (OIV), except that the former is more
robust than the latter. It will also be shown that GIV can be interpreted as
applying the OIV procedure twice, hence the title of the paper. GIV estimators
rely only on the rank of the instrumental variable and therefore are not sensitive
to monotonic transformations or to extreme observations of the instrument.
It turns out that the comparison between the two estimators can be done
under a unified approach, and new insights on the structure of OIV and GIV
can be gained when the analysis is carried out using Absolute Concentration
Curves (ACC)(4 ).
The structure of the paper is as follows: the second section presents the
main properties of the ACC used in this paper. Section 3 presents the OLS
and the OIV estimators as functionals of the ACC. Specifically, it is shown that
OIV and OLS are weighted sums of the slopes of the regression curve and that
the weights are based on ACC curves. Section 4 presents the Gini regression
coefficient and the GIV as functionals of the same ACCs as the OLS. Therefore,
the choice of an instrumental variable and the regression technique can be
interpreted as choosing a weighting scheme with which to weigh a given set of
slopes. The analysis is restricted to a simple regression framework. Section 5
presents an example to illustrate the difference between the two approaches,
Section 6 comments on an extension to the multiple regression framework,
while Section 7 concludes.
2. Preliminaries: the absolute concentration curve
The concentration curve is mainly used in the field of income distribution
to portray the impact of taxes on income distribution (Kakwani, 1977, 1980;
Lambert, 1993; Suits, 1977; and Yitzhaki and Slemrod, 1991)(5 ). Normally,
the horizontal axis would portray the poorest p percent of the population while
the vertical axis would present the share of total expenditure on a consumption
item spent by the poorest p percent. The absolute concentration curve (ACC)
differs from the concentration curve by presenting the cumulative consumption
(4 ) Blitz and Brittain (1964) used them to define the Gini correlations. Schechtman and Yitzhaki (1987,
1999) investigated the properties of Gini correlation. Yitzhaki (2003) surveys the properties of the
GMD, Gini correlations, and relates them to the variance and Concentration Curves. See also Yitzhaki
and Olkin (1988, 1991).
(5 ) See also Iyengar (1960), who uses it for estimating income elasticities.

290

SHLOMO YITZHAKI – EDNA SCHECHTMAN

(rather than the cumulative share of consumption) of the poorest p percent on
the vertical axis. In this paper the ACC is not restricted to a particular variable.
The definitions follow the terminology in Yitzhaki and Olkin (1988, 1991).
The following notation is used. The expected values are µ X and µY .
The conditional density function is f Y |X and the conditional expectation is
g(x) = µY.X ≡ E{Y |X = x}. It is assumed that all densities are continuous
and differentiable, and all second moments exist.
Definition 2.1. The absolute concentration curve (ACC) of Y with respect to X ,
AY.X ( p), is implicitly defined by the relationship
AY.X ( p) =
where X p is defined by
p=



Xp

−∞



Xp

−∞

g(t)d FX (t) ,

(2.1)

d FX (t) .

(2.2)

The special case A X.X ( p) is referred to as the absolute Lorenz curve (ALC)(6 ).
Definition 2.2. The Line of Independence (LOI) is the line connecting (0, 0)
with (1, µY ). Let L Y.X ( p) = µY p denote the LOI of Y with respect to X .
Figure 1 presents a typical ACC curve and LOI. The solid curve is the
absolute concentration curve of Y with respect to X and the dashed line is
LOI.
Cumulative
value of Y

D

µY

E

C

F

+
0

B

1

FX

Cumulative
dis tribution of X

A

Figure 1.

To apply the concentration curve to OLS parameters, it is convenient to redefine
the concentration curve and the LOI as functions of the variate, X . In this
(6 ) Shorrocks (1983) calls this curve the Generalized Lorenz Curve. Gastwirth (1971) presents a
formal definition of the Lorenz curve.

The Gini Instrumental Variable, or the “double instrumental variable” estimator

291

case, we denote the ACC by V and define it as:
VY.X (x) =



x

−∞

g(t)d Fx (t) .

(2.3)

The LOI simply changes to L Y.X ( p(x)) = µY FX (x). Note, however, that it is
no longer a straight line. In terms of figure 1, the only difference between
equations (2.1) and (2.3) is that the horizontal axis is changed from p = FX (x)
to X .
The absolute concentration curve has the following properties (proofs are
scattered in Yitzhaki and Olkin (1991), Yitzhaki (1990, 1996, 1998)).
(a) The ACC passes through the points (0, 0) and (1, µY ).
(b) The derivative of the ACC with respect to p is g(x p ) = E Y {Y | X = x P }.
Consequently, AY.X ( p) is increasing if and only if g(x p ) = E Y {Y |X =
x P } > 0. (Hereafter we will write g(x) instead of g(x p ).)
(c) The ACC is convex (concave, straight line) if and only if ∂g(x)/∂ x > 0.
(∂g(x)/∂ x < 0, ∂g(x)/∂ x = 0.) A X.X ( p) is always convex.
(d) If Y and X are independent, the ACC coincides with the LOI.
(e)
(e.1) The area between the LOI and the ACC is equal to COV(Y, FX (X )).
That is,
COV(Y, FX (X )) =



0

1

{µY p − AY.X ( p)}dp .

(2.4)

The term COV(Y, FX (X )), i.e., the covariance between a random variable and the cumulative distribution of another variable, is the equivalent of the regular covariance when Gini mean difference (GMD) is
used as a measure of variability. COV(X, FX (X )) is one fourth of
GMD).
(e.2) The area between the shifted LOI and the shifted ACC is equal to
COV(Y, X ):
COV(Y, X ) =



∞

−∞

{µY FX (t) − VY.X (t)}dt .

(2.5)

Note that VY.X (x) is the transformed ACC, while µY FX (t) is the transformed LOI. The variance of X is the area between the (transformed)
LOI and the (transformed) ACC, denoted by VX.X (Yitzhaki, 1998).
(f) The ACC is above the LOI for all p if and only if COV(Y, T (X )) < 0 for
all continuous differentiable monotonically increasing functions T (X ). (The
ACC is below the LOI iff the covariance is positive.) This property is a
modification of Grether (1974). It implies that whenever the ACC intersects
the LOI one can divide the data into two sections, conditional on the values
of X . In one section COV(Y, X ) < 0 and in the other COV(Y, X ) > 0. By

292

SHLOMO YITZHAKI – EDNA SCHECHTMAN

applying a monotonic transformation to X , the investigator can change the
magnitude of the covariances in these sections, thereby affecting the sign of
the overall covariance. Yitzhaki (1990) derives the conditions under which
it is possible to change the sign of a regression coefficient by applying a
monotonic transformation to one of the variables. Note, however, that if
AY.X and L Y.X intersect, it does not necessarily imply that A X.Y and L X.Y
intersect. The ACC in figure 1 intersects the LOI at C.
(g) If Y and X follow a bivariate normal distribution, AY.X and L Y.X do not
intersect. Thus, a monotonic transformation cannot change the sign of the
covariance.
Additional properties of ACC are presented in Yitzhaki and Olkin (1988).
The estimators of L Y.X and AY.X are based on concomitants Y ’s of X (that
is, the observations of Y ’s, after ordering them by their respective (ordered) X
values)(7 ). That is:
L̂ Y.X ( p) = p ȳn

(2.6)

where p = i/n and ȳn is the mean of Y . The empirical ACC at i is:

ÂY.X ( p) =

i
1
yj = p ȳi ,
n j=1

(2.7)

where ȳi is the mean of the first i observations of Y , ordered according to
their respective (ordered) X values. The vertical difference between the LOI
and ACC at i is:
L̂ Y.X ( p) − ÂY.X ( p) = p( ȳn − ȳi ) ,

(2.8)

where p = i/n.
Finally, there are several theorems concerning the convergence of empirical
concentration curves to the population curves (Goldie, 1977; Bishop, Formby
and Thistle, 1992) and several large sample tests concerning the intersection
of concentration curves (Eubank, Schechtman and Yitzhaki, 1993; Beach and
Davidson, 1983; Bishop, Chow and Formby, 1994; Davidson and Doclus, 1997;
and Nygård and Sandström, 1981). Moreover, additional literature, which seems
to approach convergence issues of empirical Lorenz curves from an additional
angle is emerging (Davydov and Egorov (2000a, 2000b), Rao and Zhao (1995,
1996)). However, convergence issues are beyond the scope of this paper.
(7 ) See Barnett, Green and Robinson (1976) for properties of concomitants. Concomitants are also
called induced order statistics (see, for example, Davydov and Egorov (2000a)).

The Gini Instrumental Variable, or the “double instrumental variable” estimator

293

3. The Ordinary Least Squares (OLS)-based estimators
The OLS simple regression coefficient can be viewed as a weighted average of the slopes between each pair of adjacent observations in the sample
(Yitzhaki, 1996). The weighting scheme depends solely on the properties of
the distribution of the independent variable. This section discusses and reinterprets the OLS weighting scheme in order to apply the same approach to an IV
estimation method. We shall see that both OLS and OIV are weighted sums
of the same slopes, and that they only differ in the weighting scheme used.
By investigating the weighting schemes used in specific models one can check
whether the identifying assumptions used are supported by the data.
The presentation of the weighting scheme is done in a simple regression
framework. The extension for multiple regression is deferred to Section 6.
Let Y , X , and Z be the dependent, independent, and instrumental variables,
respectively. It is assumed that all variables are continuous with finite moments.
Let Y ≡ g(X ) + ε, where g(x) = E{Y | X = x}, and ε and X are assumed to
be independent. Then,
g  (x) =

∂ E{Y |X = x}
= β(x) ,
∂x

(3.1)

where β(x) is the slope of the regression curve at x.
The investigator assumes that the following linear model holds in the population:
Y = α + βX + ε .
(3.2)
That is, it is assumed that β(x) = β.
We turn now to alternative representation of β in the population and the
sample. Let yi , xi , z i (i = 1, . . . , n) be the observations of the dependent,
independent, and instrumental variables, respectively. Observations are ranked
by an increasing order of X (the independent variable). Let
xi = xi+1 − xi (xi > 0)(8 )
z i = z i+1 − z i
bi = (yi+1 − yi )/(xi+1 − xi ) (i = 1, . . . , n − 1) ,
be the differences in the independent and the instrumental variables and the
slope (e.g., the marginal propensity to spend) defined by adjacent observations
of X . To simplify the notation, the term COV(X, Y ) is used to represent the
parameter in the population while cov(x, y) represents the equivalent term in
the sample.
(8 ) The assumption xi > 0 simplifies the presentation; omitting it requires that all observations with
the same X be aggregated, a procedure that complicates the presentation without adding insight.

294

SHLOMO YITZHAKI – EDNA SCHECHTMAN

Proposition 1 is needed for interpreting alternative methods of estimation
when using alternative weighting schemes of the slopes. Propositions 2 and 3
were presented in Yitzhaki (1996) (along with their proofs) and are given here
only for completeness and for reinterpretation.
Proposition 1. Let E ∗ (Y | X ) = α + β X denote the best linear predictor of Y
given X . Then, any linear estimator of β can be expressed as a weighted sum of
slopes, defined by adjacent observations.


Proof. Let b = ci yi be a linear estimator. Write yi = y1 + i−1
j=1 x j b j and
insert it into the definition of the estimator. Adding the requirement that b is
not affected by shifting all y’s by a constant completes the proof.
Propositions 2 and 3 characterize the weights for the OLS.
Proposition 2. Let E ∗ (Y | X ) = α + β X denote the best linear predictor of Y
given X . Then β is a weighted average of slopes of the regression curve:
βOLS =
where w(x) > 0 and





w(x)g  (x)d x ,

(3.3)

w(x)d x = 1. The weights are:
w(x) =

1
[µ X FX (x) − VX.X (x)] ,
σ X2

(3.4)

where VX.X is defined in (2.3) and σ X2 is the variance of X .
Proposition 3 is identical to Proposition 2, except that it is applied to the
sample.
Proposition 3. The OLS estimator of the slope of the regression curve β is a
weighted average of slopes defined by adjacent observations. That is,
bOLS =

n−1


vi bi ,

(3.5)

i=1



n−1
where vi > 0, i=1
vi = 1.
The weights are given by




vi =

n−1


i(n − j)x j +

j=i

n−1

k=1




n−1

j=k

i−1




j (n − i)x j  xi

j=1

k(n − j)x j +

k−1

j=1



j (n − k)x j  xk

.

(3.6)

The Gini Instrumental Variable, or the “double instrumental variable” estimator

295

It proves convenient to represent the weights by the ACC. In this case the
weight can be expressed as:
vi =

i (x̄n − x̄i )xi
,
n
σ̂x2

(3.7)

where x̄i is the mean of the i smallest observations of X while σ̂x2 is the
estimate of the variance of X . Note that in terms of figure 1, the weight is
the vertical distance between LOI and Absolute Lorenz Curve multiplied by
x and divided by the whole area (which is represented by the variance in the
denominator). The proof appears in Yitzhaki (1996). Since it is required for
other proofs, it is replicated in Appendix A, (a).
The two components of equation (3.5) are the slopes bi and the weights vi .
The weight vi is actually the contribution of section xi to the variance of X (9 ).
It can be shown that the weighting scheme in equation (3.6) is a symmetric
function of the deviation of the rank of X from the rank of the median of X (in
the sample) and is quadratic in x. On the other hand, equation (3.7) describes
the weighting scheme as the vertical distance between the LOI and the Absolute
Lorenz Curve, multiplied by x. We intend to show that (i) both the OLS
and OIV estimators are weighted sums of the same regression slopes, (ii) both
weighting schemes have the same structure, i.e., they are based on concentration
curves of the independent variable, and (iii) they only differ in the concentration
curve used to derive the weights: the OLS is based on a modification of the
Absolute Lorenz Curve (thus, the weighting scheme is a decomposition of
COV(X, X )), while the OIV is based on the concentration curve of Z with
respect to X (thus, it is based on a decomposition of COV(X, Z )).
The IV population’s parameter, βOIV , is defined as:
βOIV =

COV(Y, Z )
.
COV(X, Z )

(3.8)

For Z to qualify as an instrumental variable it is assumed that ε in equation (3.2)
and Z are independent. The IV estimator is based on using sample’s values.
That is:
cov(y, z)
.
(3.9)
bOIV =
cov(x, z)
Proposition 4 is the IV equivalent of Proposition 2.
Proposition 4. Given the model in (3.2), the OIV parameter defined in (3.8) is a
weighted sum of slopes of the regression curve g  (x). That is:
βOIV =



w(x, z)g  (x)d x ,

(9 ) To see this, note that from (3.7) σ̂x2 is the normalizing factor in (3.5).

(3.10)

296

SHLOMO YITZHAKI – EDNA SCHECHTMAN

where the weights w(x, z) represent
the contribution of each segment d x to the

covariance COV(X, Z ). Thus, w(x, z)d x = 1, and
w(x, z) =

1
[µ Z FX (x) − VZ ,X (x)] .
COV(Z , X )

(3.11)

The proof appears in Appendix A, (b). Note, however, that for OIV to be
a weighted sum of the true impact of X on Y (i.e., of β(X ) in (3.1)), it must
be assumed that Z and ε are independent.
Proposition 5 is identical to Proposition 4, except that it is applied to the
sample.
Proposition 5. The OIV estimator of the slope of the regression coefficient β is a
weighted sum of slopes defined by adjacent observations of X . That is,
bOIV =

n−1


vi bi ,

(3.12)

i=1



n−1
vi = 1.
where i=1
The weights are given by




vi =

n−1


i(n − j) Z j +

j=i

n−1

k=1




n−1

j=k

i−1




j (n − i) Z j   X i

j=1

k(n − j) Z j +

k−1




.

(3.13)

j (n − k) Z j   X k

j=1

Proof. The proof is identical to the proof of Proposition 3. Note, however,
that the denominator is equal to cov(z, x) and that the weight is therefore the
contribution of each section xi to cov(z, x). Note also that the weights are
not restricted to be positive here.
As in the case of Proposition 3, the weights can be expressed in terms of
the vertical distance between the LOI and the Absolute Concentration Curve.
That is,
i (z̄ n − z̄ i )xi
.
(3.14)
vi =
n cov(z, x)
The proof of the last step is given in Appendix A, (c).
Several implications follow from Propositions 2-5. Both OLS and OIV
estimators are weighted sums of bi ; both weighting schemes are based on the
vertical distance of LOI and ACC. The only difference between the schemes is
in the ACC used – the OLS weighting scheme relies on the Absolute Lorenz

The Gini Instrumental Variable, or the “double instrumental variable” estimator

297

Curve of X , which describes the contribution of each segment of X to the
variance of X , while the weights in the OIV estimators are based on the ACC
of Z with respect to X , which describes the contribution of each segment of X
to COV(X, Z )(10 ).
Weights that are based on the variance are non-negative, while those based
on the covariance can be both positive and negative. An estimator with positive weights can have totally different properties from an estimator with a
combination of positive and negative weights: in the former case, the estimator is a convex combination of slopes while in the latter case it is not. To
see the implications, note that in the former case the estimate is bounded by
Min(b1 , . . . , bn−1 ) and Max(b1 , . . . , bn−1 ), whereas in the latter case the estimate is not bounded and can fall outside the range of the observed slopes(11 ).
(See the example in Section 5.)
In some sense, negative weights cancel out positive weights, thereby in
effect reducing the effective size of the sample. Imagine a case in which
98 percent of the slopes are equal to b1 and only 2 percent are equal to b2 .
Assuming also that the sum of the weights of the b1 slopes equals to zero, we
get that the IV method estimates the slope to be equal to b2 although 98 percent
of the observations form a perfect line with b1 slope. Because of the opposite
signs of the weights, the estimate is actually determined by the remaining two
percent of the sample. The point that even large samples may be insufficient
for the IV method, has been raised by Bound et al. (1995).
Note that the weighting scheme is composed of weights with mixed signs
if and only if L Z .X and A Z .X intersect. Property (f) of the ACC indicates that
this condition is identical to the condition of whether there exists a monotonic
transformation of X that can change the sign of b X.Z , the regression coefficient
in the first-stage regression. If the ACC and the LOI intersect, one can split
the data into two sets, composed of all observations below (or above) the
intersection. Then, cov(x, z) in the two sets have different signs. A monotonic
transformation can change the magnitude of the two covariances and therefore,
can change the sign of the regression coefficient between X and Z . This implies
that an additional property is required from a good instrument: it should have
monotonic relationship with the independent variable. The test whether this
property exists is based on the concentration curve. A failure of the instrument
to have positive weights only, implies that one can change the sign of the OIV
estimator by applying a monotonic transformation.
An obvious reason for an instrument Z to have a low correlation with X
is sampling variability; i.e; the random deviation of the estimate from the pop(10 ) Proposition 4 gives an intuitive explanation to Imbens and Angrist (1994), who showed that βOIV
can be interpreted as a local average treatment effect specific to the instrument, z.
(11 ) There are some additional restrictions on the weights. For example, it can be shown that the
maximum weight is bounded from above by the absolute deviation of Z .

298

SHLOMO YITZHAKI – EDNA SCHECHTMAN

ulation parameter. Ignoring sampling variability, there can be two additional
reasons for an instrument Z to have a low correlation with X . One possibility
is that although the population’s concentration curve of Z with respect to X
is located on one side of the LOI, it is close to it. This means that although
the expected values of each weight in the weighting scheme of the slopes of
the regression between Z and X are all positive, they tend to be close to zero.
Hence the correlation between the two variables is weak. Another possibility
for low correlation between Z and X , is when the concentration curve of Z
with respect to X (or X with respect to Z ) intersects with the LOI, although
there can be sections where it is far away from the LOI, as illustrated in figure 1. This means that the conditional correlation, conditional on the segment
of X we are looking at, changes signs. This means that there are segments of
the range of X , where the correlation is positive and large, (when the curve
is below the LOI – between 0 and C in figure 1), and at the same time,
there are other segments where the correlation is negative and large (when the
curve is above the LOI, between C and E in figure 1). This case will be
characterized by having positive and negative weights. The former type of an
instrument should be preferred to the latter because by increasing the sample,
the weakness of the former disappears while the latter will continue to be a
weak instrument even for large samples(12 ). In other words, a weak correlation
between the instrument and the independent variable can cause the weighting
scheme to be with mixed signs. However, if the appropriate concentration
curve in the population does not intersect with LOI, then a sufficiently large
sample can mitigate the impact of weak correlation as is the case discussed
in Bound et al (1995) and Staiger and Stock (1997). On the other hand, the
weakness of the instrument that we stress in this paper is caused by the nonmonotonic relationship between the instrument and the independent variable. If
the concentration curve in the population intersects with LOI, then this weakness holds in the population and therefore, will not disappear even if we rely
on the entire population. It implies that the investigator can force the data
to deliver the sign of the coefficient he wishes to get. The non-monotonic
relationship is identified by the properties of the ACC of Z with respect
to X (13 ).
Using the property that both estimators are weighted sums of the same
slopes, we can use (3.7) and (3.14) to express the difference between OIV and
OLS explicitly, as in (3.15). This procedure enables us to trace the sources of
the difference between the two estimates. Equation (3.15) details the effect of
(12 ) Property (g) of the ACC implies that if X and Z are drawn from a bivariate normal distribution
then this is a sufficient condition for the weights to converge to positive values in large samples.
(13 ) Note that some kind of non-monotonicity can be accepted. If the ACC is concave in some sections
and convex in others but does not cross the LOI, then the conditional correlation over those segments
can be negative or positive, but the weighting scheme does not change its sign.

The Gini Instrumental Variable, or the “double instrumental variable” estimator

299

applying an instrumental variable on the weighting scheme:
bOIV − bOLS =

n−1
(z̄ n − z̄ i )
1 
− b Z .X vi bi ,
b Z .X i=1 (x̄n − x̄i )

(3.15)

where b Z .X is the OLS estimator of the slope of the regression of Z on X
and νi are the OLS weights of (3.7). The weight attached to bi is changed
according to the term in the brackets which, in turn, is the difference between
the LOI and the ACC of Z with respect to X divided by the difference between
the LOI and ALC of X . If this ratio is greater (smaller) than the slope of
the regression of Z on X , the weight attached to bi is proportionally increased
(decreased).
As can be seen from (3.15), the impact of using IV on the estimate is
channeled in three possible ways. Consider the case where bOIV < bOLS and
recall that the weights, vi , are all positive and add up to 1. The difference
between the estimates can be caused solely by the terms inside the brackets
and the sign and magnitude of vi bi that accompany them. The change in the
estimate can be caused by (a) a decline in the terms in the brackets of large
vi bi and an increase in the terms in the bracket of small vi bi , (b) a decrease
in the terms in the brackets of both small and large vi bi , or (c) an increase
in both. (Cases (b) and (c) are possible since some terms in the bracket
are positive while others are negative. The decrease/increase is in terms of
absolute value.) We would argue that (a) represents a good instrument while (b)
and (c) represent bad ones. Therefore, the decomposition of the sources of the
change in the estimate enables testing whether the identifying assumptions, i.e.,
the assumptions used to construct the model, are supported by the data. For
example, one possible reason for using an IV is that economic theory leads
us to suspect that X is positively correlated with the error term, and therefore
the OLS estimate is biased upward. Divide the set of slopes {bi } into two
groups: the above-average (bad) group and the below-average (good) group.
We refer to the sets as good and bad because chances are that high values of
slopes will be more contaminated than low values of slopes from the positive
spurious correlation. If so, one can test whether the decline in the estimate
due to the use of an IV is caused by a decline in the weights of bad slopes or
by an increase in the weights of good slopes. Increasing the weights of good
slopes together with a decline of the weights of bad slopes (case (a) above)
should indicate consistency with the economic model while diminishing (case
(b)) or increasing (case (c)) both the weights of good and bad slopes should be
viewed as data-manipulation and/or a search for spurious correlation. Note that
the sum of the weights equals one. Therefore, if all the weights are positive,
an increase in the weights of one group implies a decrease in the weights of
the other. Therefore, an instrument that produces only positive weights will

300

SHLOMO YITZHAKI – EDNA SCHECHTMAN

never be found as spurious, so that the suggested test is actually a test on
the properties of the ACC. This particular issue, which is model specific, goes
beyond the scope of the present paper.
Finally, note that the OLS weighting scheme continues to serve under
the OIV regime. Hence the OIV estimator, like the OLS, is also sensitive to
extreme observations.
Additional implications of the difference between the weighting schemes
are discussed after we introduce the Gini IV estimator.

4. The Gini-Based estimators
Gini’s Mean Difference (GMD) is a measure of variability that has been
known for almost a century. Two regressions can be viewed as based on GMD.
One is based on minimization of the GMD of the error term and is similar
to Least Absolute Deviation Regression (Bassett and Koenker, 1978). The
other regression is based on a weighted average of slopes between adjacent
observations; as such it is non-parametric and does not require the assumption
of a linear model. The estimators of the parameters in the second regression are
similar in structure to OLS estimators; the only difference is that each variance
is substituted by the appropriate GMD and each covariance is substituted by
an appropriate Gini covariance. (See Olkin and Yitzhaki (1992) for the simple
regression case, and Schechtman and Yitzhaki (2000) for multiple regression.)
The discussion of the IV approach in a Gini regression framework is limited
to the former type of regression.
The aim of this section is to present the Gini estimator of the regression
coefficient as a weighted average of slopes defined by adjacent observations,
in a way similar to the OLS estimates. All weighting schemes are based on
the vertical distance between an LOI and ACC. The only difference is that the
ACCs used for OLS are based on the cumulative value of the variate, while
the ACCs used in the GMD approach are based on the cumulative value of the
cumulative distribution. As a result, the GMD estimator is more robust than
the OLS estimator, because the weighting scheme is a function of x (rather
than of (x)2 ). Similarly, the GIV is based on the ACC of the cumulative
distribution of the instrumental variable, which means that GIV is more robust
than OIV because GIV, unlike OIV, is not sensitive to monotonic transformations
of the instrument. Alternatively, one can use GIV to check how robust OIV
estimators are to a small perturbation in the weighting scheme.
The Gini non-parametric regression coefficient in a simple regression framework is defined as:
COV(Y, FX (X ))
,
(4.1)
βG =
COV(X, FX (X ))

The Gini Instrumental Variable, or the “double instrumental variable” estimator

301

where F() denotes the cumulative distribution function. In the sample, F(x)
is estimated by the rank of X . That is,
bG =

cov(y, r (x))
.
cov(x, r (x))

(4.2)

Note that the Gini regression coefficient can be viewed as an OIV estimator,
with r (x) serving as the instrumental variable(14 ). The large sample properties of (4.2) are discussed in Olkin and Yitzhaki (1992) and Schechtman and
Yitzhaki (2000).
As in the case of OLS, the instrumental variable under the Gini framework
should be independent of Y | X . Then, the instrumental variable estimator (and
the population’s parameter) in the Gini regression framework can be defined in
a way that resembles the OLS definition:
βGIV =

βG,Y.Z
COV(Y, F(Z ))
=
.
COV(X, F(Z ))
βG,X.Z

(4.3)

The estimator bGIV is identical to the population’s parameter except that samples’ values substitute for the population parameters. Its derivation is identical
to the derivation of an IV estimator under the OLS framework. Comparison
of (3.8) and (4.3) reveals that the difference between OIV and GIV regressions
is that GIV relies solely on the ranks of the instrumental variable, while OIV
relies on the variate itself. Like the OLS and OIV estimators, the GMD and
GIV regression coefficients are weighted averages (sums) of slopes between adjacent observations. The only difference is in the weighting scheme. Formally,
in the GMD framework, the weights in the population are
w(x) = 

[1 − FX (x)]FX (x)d x
∞

−∞

[1 − FX (t)]FX (t)dt

,

(4.4)

where w(x) replaces w(x) of (3.3) in Proposition 2. w(x) can also be presented as
1
{0.5FX (x) − VFX (X ).X (x)} .
(4.5)
w(x) =
COV(X, F(X ))
Note that the denominator in (4.5) is a quarter of the GMD of X . Hence,
(4.5) represents the contribution of each section’s d x to the GMD of X .
(14 ) Durbin (1954) suggested this estimator as a way of handling errors-in-variables. However, he
did not notice the GMD in his estimator.

302

SHLOMO YITZHAKI – EDNA SCHECHTMAN

In the sample,
vi =

(n − i)ixi
n−1


(4.6)

[(n − k)k]xk

k=1

replaces vi of (3.6) in Proposition 3.
The Gini instrumental variable can also be written as a weighted sum of
slopes of the original regression curve, and the similarity to OIV holds here
too, where w(x, z) of (3.11) is replaced by
w(x, z) =

1
[0.5FX (x) − VFZ (Z ).X (x)] .
COV(X, F(Z ))

(4.7)

In the sample, the weights vi of (3.13) are replaced by
vi =

n+1
pxi
− r̄iz ,
z
n cov(x, r )
2

(4.8)

where r̄iz is the average rank of the i observations of Z , ordered in an increasing
order of x (i.e., rank concomitants).
Proof. The proofs are variations of a proof in Yitzhaki (1996). Sketches of the
proofs are given in Appendix B, (a,b).
The statements above show that GIV is identical in structure to OIV, where
instrumental variables are replaced by their ranks. Therefore, in the GIV framework, an investigator cannot affect the estimate by using a monotonic transformation of the instrumental variable. As for other properties, it is easy to
see that GIV and OIV weights have the same signs, so there is no difference
between the two with respect to transformations of X . Therefore, the same
condition that identifies a spurious instrument under OLS will do so under the
GMD framework. However, since GIV weights are based on x while OLS
weights are based on (x)2 , we expect the former to be more robust.
To sum up the properties of GIV estimators:
(a) A monotonic transformation of Z does not affect the GIV estimate, unlike
the case of OIV estimates, where a monotonic transformation of Z does
have an effect and may even change its sign. In this sense, the GIV
method reduces the possibility of data-manipulation. However, as a result
of this property, one cannot use two IVs that are monotonic transformation
of each other (having Spearman’s correlation coefficient of one), because
the ranks will be identical which will result in multi-co-linearity.
(b) The GIV attaches less weight to extreme observations than does OIV.
Therefore, it is more robust to outliers than OIV.

The Gini Instrumental Variable, or the “double instrumental variable” estimator

303

(c) The GIV can be written explicitly, and relies on the same terminology as
the OIV.
(d) The GIV can be used as a sensitivity test for OIV. Presumably, a minor change, such as slightly altering the metric of variability, should not
drastically affect the estimates.
(e) As shown in Schechtman and Yitzhaki (2000), all estimators used in GMD
regression are based on U-statistics. Therefore, they are consistent and their
sampling distributions converge (after proper standardization) to normal.
(f) The estimation can be done with the same software used for OIV. The
only component that requires special programming is the estimation of the
standard errors. Yitzhaki (1991) offers a simple method for calculating
jackknife variance estimators for estimators based on the Gini method.
Summing up the intuitive idea that leads to the GIV, it is worth to recall Angrist
and Evans explanation for the IV method: “The IV method attributes any effect
of Z i on Yi , to the effect of Z i on X i ”. (1998, p. 458.) The Gini regression
can be interpreted as attributing any effect of rank of X i on Yi , to the effect
of rank of X i on X i . In the GIV, the IV method is used twice so that the final
result is to attribute any effect of change in rank of Z i on Yi , to the effect of
change in rank of Z i on X i . This property reduces the sensitivity to Z . The
double use of the IV method when using GIV explains the title of the paper.

5. An example
The aim of the following example is to illustrate the conditions under
which the IV method will fail to produce a reasonable estimate of the slope.
The artificial data set is composed of four observations and three variables,
X , Y and Z , which represent the independent, dependent and instrumental
variables, respectively. Table 1 presents the data together with the cumulative
distributions of X and Z .

Table 1. The Data.
X

Y

Z

FX

FZ

1
2
3
4

0
−1
0
1

9
0
8
7

0.25
0.5
0.75
1

1
0.25
0.75
0.5

304

SHLOMO YITZHAKI – EDNA SCHECHTMAN

The variance-covariance matrix is given in Table 2.
Table 2. The Variance-Covariance Matrix

X
Y
Z

X

Y

Z

FX

1.25

0.5
0.5

0.25
1.75
12.5

1.25
0.5

FZ
−0.5
0.25

As shown, the covariance between X and Z is positive and equals 0.25,
which implies a correlation coefficient of 0.06.
Table 3 presents the weighting schemes according to the different methods
and the slopes between adjacent observations.
Table 3: Weighting Schemes and Slopes.
I

WOLS

WG(1)

WOIV

WGIV

bi

1
2
3

0.3
0.4
0.3

0.3
0.4
0.3

−3
3
1

0.75
0
0.25

−1
1
1

(1)
Since the distance between adjacent observations of X is constant, the OLS and the Gini regression weighting schemes are identical.

As can be seen from the first column of Table 1, the distance between
adjacent observations of X equals a constant (one), which causes the OLS and
Gini estimators of the slope of the regression curve to be equal.
Thus, bOLS = bG = 0.4. On the other hand, the IV estimates differ:
bOIV = 7, while bGIV = −0.5.
As seen in the last column of Table 3, the slopes are either 1 or −1,
which means that the OIV estimator is far above the maximum slope observed
in the data. This result is explained by the large negative weight attached to
the negative slope between the first and second observations, which translates
into a positive contribution to the estimated slope.
Applying monotonic transformations to Z would leave the GIV estimates
unaffected, but may affect the OIV estimate through a change in its weighting
scheme, and may even change its sign. The effect of applying a transformation
to Z on the estimate depends on its effect on the absolute values of the
negative and positive weights. Whether a transformation reduces or increases
the absolute value of the weight depends on whether its derivative in the
relevant range is greater or smaller than one. This fact, that a monotonic
transformation of the instrument can change the sign of the regression coefficient

The Gini Instrumental Variable, or the “double instrumental variable” estimator

305

is illustrated in our example: one can view the use of F(z) in GIV, as using
a monotonic transformation of Z . This means that GIV is the OIV of the
monotonic transformation, so that bOIV = 7 while bGIV = bOIV (of F(z)) = −0.5.

6. Some comments on multiple regression
The semi-nonparametric version of the multiple Gini regression has already been derived (Schechtman and Yitzhaki, 2000), where it is shown that
the distribution of the estimators converges to the normal distribution, after
proper standardization. Like the OLS, there is an explicit formulation for the
estimators: the Gini estimator is identical to the OLS in structure, while every variance is substituted by GMD and every covariance is substituted by
an appropriate Gini covariance(15 ). Similarly, GIV is obtained from OIV by
replacing Z by its cumulative distribution function.
However, to infer the impact of the application of monotonic transformations in a multiple regression framework is much more complicated than in the
simple regression case. The reason for this complication is that a monotonic
transformation of an independent variable i, in an OLS and Gini settings, may
affect the signs of all the covariances with other variables. To see this, note
that in the simple regression case, a monotonic transformation can change the
sign of the covariance if the underlying concentration curve intersects the LOI.
However, in doing so in a multiple regression framework, it may also affect the
magnitude (or the signs) of the variances and the covariances with other independent variables. In a multiple regression setting it is cumbersome to detect
and test the occurrence of the necessary conditions for a possibility of a sign
change in a coefficient. This is so because one has to plot the concentration
curve for each covariance and detect intersections with the LOI, which would
require a complicated method of simultaneous confidence intervals (Richmond,
1982). On the other hand, using the GIV procedure as a sensitivity test is quite
easy, and provided the results are similar to the OIV estimate, it can increase
confidence in the reliability of the estimates.

7. Concluding remarks
The paper shows that the regression coefficients in OLS, IV-in-OLS, Gini
regression and IV-in-Gini regression can all be expressed as weighted sum
of slopes of adjacent observations. The weights in the case of the OLS and
(15 ) Note that between every two random variables, X and Y , two Gini covariances can be defined:
COV(Y, F(X )) and COV(X, F(Y )). Hence, one has to be careful in choosing the appropriate covariance.

306

SHLOMO YITZHAKI – EDNA SCHECHTMAN

Gini regressions are based on the properties of the Absolute Lorenz Curve
of the independent variable, while the weights in the case of IV, under both
methods, are based on the properties of the Absolute Concentration Curve of the
instrument with respect to the independent variable. The Absolute Lorenz Curve
cannot intersect the LOI, which means that under OLS and Gini regressions
all weights are positive. On the other hand, the ACC can intersect the LOI,
causing the weights of IV regression, under both methods, to be with both
negative and positive signs. This means that a monotonic transformation of the
instrument can change the sign of the regression coefficient. In such cases the
estimator may be inconsistent.
One recommendation is to plot the concentration curve of the instrument
with respect to the independent variable, in order to see whether a sign change
in the weighting scheme can occur. This recommendation is a bit complicated
to apply in a multiple regression framework, and more work in the extension
to multiple regression is still needed.
An implication of the analysis presented in the paper is that one can interpret switching from one method to the other as a decision to change the
weighting scheme of the slopes of the regression curve. Switching from a
weighting scheme with positive weights to a weighting scheme with both negative and positive weights should be reported because it changes the properties
of the estimation procedure.
A new direction for further research is the comparison of the efficiency of
the different methods, and finding out the relative advantages of the different
methods. The convergence theorems of Davydov and Egorov (2000a, 2000b)
seem to be a promising direction in the investigation of the properties of
the concentration curves, i.e., the weighting schemes of the different types of
regressions.

Appendix A
(a) Proof that the discrete weighting scheme (3.6) is identical to (3.7).
Let us ignore the denominator, which is a normalizing factor. The numerator wn i is:
wn i =

n−1


i(n − j)x j +

j=i

i−1


j (n − i)x j

j=1

(A.1)
=i

n−1

j=i

(n − j)x j + (n − i)

i−1

j=1

jx j .

The Gini Instrumental Variable, or the “double instrumental variable” estimator

307

Using x j = x j+1 − x j , we see that the two components of (A.1) are:
n−1


(n − j)x j =

j=i

n


x j − (n − i)xi

(A.2)

j=i+1

and
i−1


jx j = (i − 1)xi −

j=1

i−1


xj .

(A.3)

j=1

Combining (A.2) and (A.3) with (A.1) and canceling terms yields:
wn i = i

n


x j − i(n − i)xi + (n − i)(i − 1)xi − (n − i)

j=i+1

=i

n

j=1

xj − n

i−1


xj

j=1
i


(A.4)

x j = n p(x̄n − x̄i ) .
2

j=1

The first term in (A.4) is the empirical cumulative distribution of X multiplied
by mean X ; the second term is the value of the concentration curve at that
value of X .
The formula for the denominator follows similar steps and was proved in
Yitzhaki (1996).
(b) A Proof of Proposition 4
Proposition 4 interprets the OIV as a weighted sum of slopes of the regression curve. The proof is constructed in two steps: first, the OIV regression
parameter is shown to be a weighted sum of slopes of the regression curve,
and then the weights are constructed.
Step 1: OIV is a weighted sum of slopes of the regression curve
We start with the numerator of the OLS estimator, COV(Y, Z ).
(Y1 , Z 1 , X 1 ), (Y2 , Z 2 , X 2 ) be i.i.d. trivariate random variables. Then:

Let

2 COV(Y, Z ) = E Y 1 E Y 2 E Z 1 E Z 2 [(Z 1 − Z 2 )(Y1 − Y2 )]
= EY 1 E Z 1 EY 2 E Z 2

Y1 − Y2
(Z 1 − Z 2 )(X 1 − X 2 )
X1 − X2



,

(A.5)

where the second expression is derived by dividing and multiplying by X 1 −
X 2 (16 ). The first two terms can be interpreted as the weighting scheme while
the third is the slope of the regression curve. To see this, note that:
2 COV(X, Z ) = E X 1 E Z 1 E X 2 E Z 2 [(Z 1 − Z 2 )(X 1 − X 2 )] ,
(16 ) Since X is a continuous variable, the probability that X 1 = X 2 equals zero.

(A.6)

308

SHLOMO YITZHAKI – EDNA SCHECHTMAN

which is also the denominator in the OIV estimator. Using (3.1), and for given
X 1 and X 2 , the right hand part of (A.5) can be viewed as a weighted average
of slopes:
 Max(X 1 ,X 2 )
1
Y1 − Y2
=
g  (x)d x .
(A.7)
X1 − X2
|X 1 − X 2 | min(X 1 ,X 2 )
Inserting (A.7) into (A.5) we can rewrite (A.5) as:


1
2 COV(Y, Z ) = E (Z 1 − Z 2 )(X 1 − X 2 )
|X 1 − X 2 |



Max(X 1 ,X 2 )

min(X 1 ,X 2 )




g (x)d x . (A.8)

Dividing (A.8) by (A.6) completes the presentation of OIV as a weighted
sum of slopes of the original regression curve g(x). Note, however, that
for βOLS = βOIV it must be assumed that Z and the error term in (3.1) are
independent.
Step 2: Deriving the weighting scheme
By using property (e.2) of the concentration curve we can write:
COV(Z , X ) =



∞

−∞

[µ Z FX (x) − VZ .X (x)]d x ,

(A.9)

where VZ .X (x) is the (transformed) Absolute Concentration Curve of Z with
respect to X . According to (A.6) the sum of the weights, COV(X, Z ), is given
by (A.9). (Note that (A.9) is the denominator, i.e., the normalizing factor of
the IV parameter.)
(c) A proof that (3.13) equals (3.14)
Assume that observations are ordered in an increasing order of X , and
again the denominator is ignored. The numerator, (up to xi ), vn i is:
vn i =

n−1


i(n − j)z j +

j=i

=i

n−1


i−1


j (n − i)z j

j=1

(n − j)z j + (n − i)

j=i

i−1


(A.10)
jz j .

j=1

Using z j = z j+1 − z j (17 ), we get that the two components of (A.10) are:
n−1

j=i

(n − j)z j =

n


z j − (n − 1)z i

j=i+1

(17 ) Since the observations are arranged in an increasing order of X , z can be negative.

(A.11)

309

The Gini Instrumental Variable, or the “double instrumental variable” estimator

and
i−1


jz j = (i − 1)z i −

j=1

i−1


zj .

(A.12)

j=1

Combining (A.11) and (A.12) with (A.10) and canceling terms we get:
vn i = i

n


z j − i(n − i)z i + (n − i)(i − 1)z i − (n − i)

j=i+1

=i

n

j=1

zj − n

i−1


zj

j=1
i


(A.13)

z j = n p(z̄ n − z̄ i ) .
2

j=1

The first term in (A.13) is the estimate LOI Z .X while the second term is the
estimate of A Z .X .
Appendix B
A proof of the properties of GIV
(a) The GIV estimator is a weighted sum of slopes of the regression curve
E{Y |X = x}.
It is convenient to present the numerator of (4.1) as:
COV(Y, FX (X )) =



{[FX (x)][1 − FX (x)]g  (x)d x ,

(B.1)

where g(x) = E Y (Y | X = x) and g  (x) is the derivative with respect to x.
To derive (B.1) note that
COV{Y, FX (X )} = E X E Y {(Y − µY )(FX (X ) − 1/2)}
= E X {(FX (X ) − 1/2)g(X )}
=



(FX (x) − 1/2)g(x) f X (x)d x .

Using integration by parts with u(x) = g(x) and
v  (x) = [FX (x) − 1/2] f X (x);

v(x) = 1/2(1 − FX (x) − [1 − FX (x)]2 )

yields:
2 COV(Y, FX (X )) = ([1 − FX (x)] − [1 − FX (x)]2 )g(x)|∞
−∞
−



[1 − FX (x)]FX (x)g  (x)d x .

310

SHLOMO YITZHAKI – EDNA SCHECHTMAN

For a bounded g(x), the first term is equal to zero. Hence
2 COV(Y, FX (X )) =



(1 − FX (x))FX (x)g  (x)d x .

Applying the same procedure to the denominator with g  (x) ≡ 1 completes the
proof for (4.4).
(b) A proof of (4.8)
Let us start with the sample version of the numerator of (4.3) (the denominator can be handled in a similar way). Note that the observations are
arranged in a non-decreasing order of X .
The numerator is:
cov(y, r z ) = (1/2)

n
n 


(yi − yj )(riz − r jz ) ,

(B.2)

i=1 j=1

with riz being the rank of the concomitant z of observation xi . Decomposing
the elements to slopes we get:
n max(i,
n 

j)−1 max(i,
j)−1

cov(y, r ) =
z

rsz xk bk ,

(B.3)

i=1 j=1 s=min(i, j) k=min(i, j)
z
− rsz is the change in the rank of z when x changes from s
where rsz = rs+1
to s + 1.
Rearranging terms, we get:

cov(y, r z ) =


n−1 
n−1

i=1



i(n − j)riz +

i−1


j=i

j (n − i)r jz

j=1





xi bi .

(B.4)

Now we can follow the steps from (A.10) to (A.13):
vn i =

n−1


i(n − j)r jz +

j=i

=i

n−1


i−1


j (n − i)r jz

j=1

(n − j)r jz + (n − i)

j=i

i−1


(A.10 )
jr jz .

j=1

Using z j = z j+1 − z j (18 ), we get that the two components of (A.10 ) are:
n−1

j=i

(n − j)r jz =

n


r jz − (n − i)riz

j=i+1

(18 ) Since the observations are arranged in an increasing order of X , z can be negative.

(A.11 )

311

The Gini Instrumental Variable, or the “double instrumental variable” estimator

and
i−1


jr jz = (i − 1)riz −

j=1

i−1


(A.12 )

r jz .

j=1

Combining (A.11 ) and (A.12 ) with (A.10 ) and canceling terms we get:
vn i = i

n


r jz − i(n − i)riz + (n − i)(i − 1)riz − (n − i)

j=i+1

=i

n

j=1

r jz − n

i−1

j=1

i

j=1

r jz = n

i
n

n


r jz − n

j=1

i


riz
(A.13 )

r jz .

j=1

The first term in (A.13 ) is the empirical cumulative distribution of X multiplied
by mean r z (mean r z equals one half by construction), while the second term
is the value of the concentration curve of rank Z with respect to X at that
value of the cumulative distribution.

Acknowledgments
We would like to thank Josh Angrist, John Bound, David Genesove, Saul Lach, Vadim Marmer,
Ingram Olkin, Emanuel Parzen and Haim Shalit for helpful discussions, and the Editor in Chief and
two anonymous referees for helpful comments.

REFERENCES
Abadie, A., Angrist, J., and Imbens, G. W. (1998) Instrumental variable estimation of quantile
treatment effects, NBER, Technical Working Paper, 229.
Angrist, J. (1990) Life time earnings and the Vietnam era draft lottery: evidence from social security
administration records, American Economic Review, 80, 313–335.
Angrist, J. and Evans, W. G. (1998) Children and their parents’ labor supply: evidence from
exogeneous variation in family size, American Economic Review, 88, 450–77.
Angrist, J., Imbens, G., and Rubin, D. (1996) Identification of casual effects using instrumental
variables, Journal of the American Statistical Association, 91, 444–472.
Barnett, V., Green, P. J., and Robinson, A. (1976) Concomitants and Correlation Estimates,
Biometrika, 63, 323–328.
Bassett, G. Jr. and Koenker, R. (1978) Asymptotic theory of least absolute error regression,
Journal of the American Statistical Association, 73, 618–622.
Beach, C. and Davidson, R. (1983) Distribution-free statistical inference with Lorenz curves and
income shares, Review of Economic Studies, 50, 723–725.
Bishop, J. A., Formby, J. P., and Thistle, P. D. (1992) Convergence of the south and non-south
income distributions, 1969-1979, American Economic Review, 82, 262–72.

312

SHLOMO YITZHAKI – EDNA SCHECHTMAN

Bishop, J. A., Chow, K. V., and Formby, J. P. (1994) Testing for marginal changes in income
distributions with Lorenz and concentration curves, International Economic Review, 35,
479–488.
Blitz, R. C. and Brittain, J. A. (1964) An extension of the Lorenz diagram to the correlation of
two variables, Metron, 23, 137–143.
Bound, J., Jaeger, D. A., and Baker, R. M. (1995) Problems with instrumental variables estimations when the correlation between the instruments and the endogenous explanatory variable
is weak, Journal of the American Statistical Association, 73, 618–622.
Davidson, R. and Duclos, J. Y. (1997) Statistical inference for the measurement of the incidence
of taxes and transfers, Econometrica, 65, 1453–1465.
Davydov, Y. and Egorov, V. (2000a) Functional limit theorems for induced order statistics, Mathematical Methods of Statistics, 9, 297–313.
Davydov, Y. and Egorov, V. (2000b) Functional limit theorems for induced order statistics of a
sample from a domain of attraction of α-stable law, αε(0, 2), In: Asymptotics in Statistics and
Probability. Papers in Honor of George Gregory Roussas (M. L. Puri eds.), VSP, Netherland.
Burbin, J. (1954) Errors in variables, Review of International Statistical Institute, 23–32.
Eubank, R., Schechtman, E., and Yitzhaki, S. (1993) A test for second order dominance, Communications in Statistics, Theory and Methods, 61, 1893–1905.
Gastwirth, J. L. (1971) A general definition of the Lorenz curve, Econometrica, 39, 1037–1039.
Gini, C. (1957) Le Medie, Unione Tipografico-Editrice Torinese, Milano.
Goldie, C. M. (1977) Convergence theorems for empirical Lorenz curves and their inverses, Advances
in Applied Probability, 9, 765–791.
Grether, D. M. (1974) Correlations with ordinal data, Journal of Econometrics, 2, 241–246.
Hoeffding, W. (1948) A class of statistics with asymptotically normal distribution, Annals of Mathematical Statistics, 19, 293–325.
Heckman, J. J. (1995) Instrumental variables: a cautionary tale, NBER Working Paper # 185,
September.
Imbens, G. W. and Angrist, J. D. (1994) Identification and estimation of local average treatment
effects, Econometrica, 62, 467–76.
Iyengar, S. N. (1960) On a method of computing Engel elasticities from concentration curves,
Econometrica, 28, 882–891.
Kakwani, N. C. (1977) Applications of Lorenz curves in economic analysis, Econometrica, 45,
719–727.
Kakwani, N. C. (1980) Income inequality and poverty, New York, Oxford University Press.
Lambert, P. J. (1993) The distribution and redistribution of income, 2nd ed., Manchester, Manchester
University Press.
Nygård, F. and Sandström, A. (1981) Measuring income inequality, Stockholm, Almqvist &
Wiksell International.
Olkin, I. and Yitzhaki, S. (1992) Gini regression analysis, International Statistical Review, 60,
185–196.
Rao, C. R. and Zhao, L. C. (1995) Convergence theorems for empirical cumulative quantile
regression functions, Mathematical Methods of Statistics, 4, 81–91.
Rao, C. R. and Zhao, L. C. (1996) Law of iterated logarithm for empirical cumulative quantile
regression functions, Statistica Sinica, 6, 693–702.
Richmond, J. (1982) A general method for constructing simultaneous confidence intervals, Journal
of the American Statistical Association, 77, 455–460.

The Gini Instrumental Variable, or the “double instrumental variable” estimator

313

Shorrocks, A. J. (1983) Ranking income distributions, Economica, 50, 3–17.
Schechtman, E. and Yitzhaki, S. (1987) A measure of association based on Gini’s mean difference,
Communications in Statistics, Theory and Methods, 16, 207–231.
Schechtman, E. and Yitzhaki, S. (1999) On the proper bounds of the Gini correlation, Economics
Letters, 63, 133–138.
Schechtman, E. and Yitzhaki, S. (2000) Multiple Gini regressions, Mimeograph.
Staiger, D. and Stock, J. H. (1997) Instrumental variables regression with weak instruments,
Econometrica, 65, 557–586.
Suits, D. B. (1977) Measurement of tax progressivity, American Economic Review, 67, 747–752.
Yitzhaki, S. (1990) On the sensitivity of a regression coefficient to monotonic transformations,
Econometric Theory, 6, 165–169.
Yitzhaki, S. (1991) Calculating jackknife variance estimators for parameters of the Gini method,
Journal of Business & Economic Statistics, 9, 235–239.
Yitzhaki, S. (1996) On using linear regression in welfare economics, Journal of Business & Economic
Statistics, 14, 478–486.
Yitzhaki, S. (1998) More than a dozen alternative ways of spelling Gini, Research on Economic
Inequality, 8, 13–30.
Yitzhaki, S. (2003) Gini’s mean difference: a superior measure of variability for non-normal distributions, Metron, LXI, 285–316.
Yitzhaki, S. and Olkin, I. (1988) Concentration curves, Working Paper No. 288, Dept. of Economics, Hebrew University.
Yitzhaki, S. and Olkin, I. (1991) Concentration indices and concentration curves, In: Stochastic
Orders and Decisions under Risk (K. Mosler and M. Scarsini eds.), Hayward, CA., Institute
of Mathematical Statistics, Lecture-Notes Monograph Series, 19, 380–392.
Yitzhaki, S. and Slemrod, J. (1991) Welfare dominance: an application to commodity taxation,
American Economic Review, 81, 480–496.

SHLOMO YITZHAKI
Dept. of Economics
The Hebrew University of Jerusalem
91905 Jerusalem (Israel)
msruhama@pluto.mscc.huji.ac.il

EDNA SCHECHTMAN
Dept. of Industrial Engineering
Ben-Gurion University
Beer-Sheva (Israel)
ednas@bgumail.bgu.ac.il

