CHAPTER 9

Field Experiments on Voter
Mobilization: An Overview of a
Burgeoning Literature
A.S. Gerber*, 1, D.P. Greenx, 1
*Yale University, New Haven, CT, United States
x
Columbia University, New York, NY, United States
1
Corresponding authors: E-mail: alan.gerber@yale.edu; dpg2110@columbia.edu

Contents
1. Intellectual Context for Emergence of Field Experiments in Political Science
1.1 The development of ﬁeld experimentation in political science
2. How Do Experiments Address the Problems in the Prior Voter Turnout Research?
3. An Overview of the Experimental Literature on Voter Mobilization
3.1 Modes of contact
3.1.1
3.1.2
3.1.3
3.1.4
3.1.5

Canvassing studies
Commercial phone banks
Mailings
Other modes of communication
Conclusion

4. The Effect of Messaging
4.1 Self-prophecy and implementation intentions
4.1.1
4.1.2
4.1.3
4.1.4

Social pressure, pride, and shame
Gratitude
Descriptive norms
Discussion

4.2 Voter mobilization outside the US
4.3 Downstream effects
4.4 Future directions
References

397
397
402
406
406
407
410
412
415
416

417
422
424
428
429
430

430
431
432
434

Abstract
This essay reviews the ways in which ﬁeld experiments have been used to study political participation.
We begin by charting the intellectual history of ﬁeld experimentation in political science. We explain
why the advent of ﬁeld experimentation in recent years represents an important advance over
previous work, which relied principally on nonexperimental survey research. Our review of the

Handbook of Economic Field Experiments, Volume 1
ISSN 2214-658X, http://dx.doi.org/10.1016/bs.hefe.2016.09.002

© 2017 Elsevier B.V.
All rights reserved.

395

396

Handbook of Field Experiments

experimental literature on political participation focuses on two broad research domains related to
voter mobilization: the effects of different modes of communication (e.g., face-to-face conversations,
phone calls, and mail) and the effects of different messages (e.g., those that stress social norms, express
gratitude, or urge the expression of implementation intentions). In the ﬁnal section, we discuss some
open questions and new ways that ﬁeld experiments may illuminate the study of voter turnout and
political behavior more generally.

Keywords
Field experiments; Political participation; Voter mobilization; Voting behavior

JEL Codes
C93; D72

In recent years the focus of empirical work in political science has begun to shift from
description to an increasing emphasis on the credible estimation of causal effects. A
key feature of this change has been the growing prominence of experimental methods,
and especially ﬁeld experiments.
In this chapter we review the use of ﬁeld experiments to study political participation. Although several important experiments address political phenomena other
than voter participation (Bergan, 2009; Broockman and Butler, 2015; Butler and
Nickerson, 2011; Broockman, 2013, 2014; Grose, 2014; Kalla and Broockman
2016), the literature measuring the effect of various interventions on voter turnout is
the largest and most fully developed, and it provides a good illustration of how the
use of ﬁeld experiments in political science has proceeded. From an initial focus on
the relative effects of different modes of communication, scholars began to explore
how theoretical insights from social psychology and behavioral economics might be
used to craft messages and how voter mobilization experiments could be employed
to test the real world effects of theoretical claims. The existence of a large number of
experimental turnout studies was essential, because it provided the background against
which unusual and important results could be easily discerned.
We begin by describing the intellectual context of the modern emergence of ﬁeld
experiments to study voter turnout. We discuss the state of the literature on campaign
effects and voter mobilization around the time of the reintroduction of ﬁeld experimentation to study political behavior. We discuss some of the methodological reasons why
this change represents an important advance over previous work. Our literature reviews
focus on two broad areas of research: the effects of different modes of communication
(face-to-face conversations, phone calls, and mail) and the effects of different messages.
In the ﬁnal section we discuss some open questions and new directions for applications
of ﬁeld experiments to the application of ﬁeld experiments to voter turnout and the study
of political behavior more generally.

Field Experiments on Voter Mobilization

1. INTELLECTUAL CONTEXT FOR EMERGENCE OF FIELD
EXPERIMENTS IN POLITICAL SCIENCE
1.1 The development of ﬁeld experimentation in political science
The ﬁrst political-science ﬁeld experiments were conducted by Harold Gosnell in the
1920s. Gosnell, one of the foremost empirical political scientists of the ﬁrst half of the
20th century, showed an early appreciation for the challenge of identifying the effects
of voter mobilization efforts. He notes that the fall of 1924 featured a great deal of
get-out-the-vote (GOTV) activity, including a National League of Women Voters’
door-to-door canvassing effort and a campaign by two million Boy Scouts to remind
citizens of their duty to vote. However, he recognized that any correlation between
turnout and mobilization activity cannot be taken to demonstrate that a causal relationship exists. Foreshadowing the concerns about causal identiﬁcation that now suffuse
work on voter turnout, Gosnell writes:
What was the net effect of all this publicity regarding the election? Did a higher proportion of the
eligible voters take part in the electoral process? The only candid answer to these questions is that
we do not know. It is true that in some states a larger portion of the adult citizens voted in 1924
than in 1920, but what part of this increase, if any, can be traced to a single factor like the getout-the-vote movement?
Gosnell (1927, p. 2)

Gosnell took up this challenge and conducted the earliest ﬁeld studies of voter mobilization. He investigated the effects of GOTV mailings on turnout in the presidential
election of 1924 and the 1925 Chicago mayoral election (Gosnell, 1927). Although it
remains unclear whether Gosnell employed random assignment of the GOTV treatment
in his study, other aspects of his research protocol such as measurement of outcomes using
the administrative voter records have become familiar features in the modern experimental literature.1 Three decades after Gosnell, Eldersveld (1956) conducted a series of
randomized ﬁeld experiments to measure the effects of different modes of campaign contact on voter turnout. Eldersveld assigned treatments at the household level and, using
post-election administrative records, measured the effect of mail, phone, and canvassing
on voter turnout in Ann Arbor, Michigan. While these early experiments have many features of contemporary work, the studies were seldom cited and had little effect on the
trajectory of subsequent research. In the decades after Eldersveld, ﬁeld experimentation
was treated as an unusual curio and, when the method was considered at all, it was dismissed as impractical or of limited application. Although lab and survey experiments
gained popularity during the 1980s and 1990s, experiments in naturalistic settings

1

Gosnell canvassed some Chicago neighborhoods and assembled a collection of matched pairs of streets. He selected
one of the pair to get the treatment, but it is not clear what method Gosnell used to decide which of the pair was to be
treated.

397

398

Handbook of Field Experiments

remained rare; no ﬁeld experiment on any subject was published in a major political
science journal during the 1990s.
The modern tradition of political-science ﬁeld experimentation began with a series of
experimental studies of campaign activity (Gerber and Green, 2000; Gerber et al., 2001).
The turn to ﬁeld experiments can be understood in part as a response to persistent methodological concerns regarding the then-dominant approaches employed in important
political behavior literature. To provide an appreciation for the context in which ﬁeld
experimentation developed in political science, we brieﬂy review the state of the literature on campaign effects at the time of the authors’ 1998 ﬁeld experiment on voter
turnout in New Haven. Although this literature includes some of the very best empirical
studies of their time, the work suffered from important methodological weaknesses and
often produced sharply conﬂicting results. The appeal of ﬁeld experiments stems in part
from its ability to address many of the deﬁciencies in the prior literature.
At the time of the New Haven ﬁeld experiments, the literature that attempted to
measure the effect of campaign spending on election outcomes included perhaps a dozen
major studies using a variety of empirical strategies. With few exceptions, one common
feature of this literature was that the studies did not examine the effect of particular
campaign activities but rather explored the correlation between reported campaign
spending [as compiled by the Federal Election Commission (FEC)] and candidate vote
shares.2 The pioneering work employing the newly available FEC data was conducted
by Jacobson, who estimated spending effects by regressing election outcomes on incumbent and challenger spending levels (Jacobson, 1978, 1985, 1990, 1998). A key assumption of this approach is that spending levels do not adjust to unmeasured aspects of the
political context. Intuition, however, suggests that incumbents tend to increase their
spending when facing a tough race. This concern was heightened by the major ﬁnding
of this line of work that incumbent spending frequently had a negative relationship with
incumbent vote share. There were two major responses to the threat of bias. First, some
studies proposed instrumental variables for candidates spending levels (Green and Krasno,
1988; Gerber, 1998). Second, it was proposed that omitted variables regarding election
conditions could be eliminated through a panel approach. Levitt (1994) examined the
subset of races that involved the same candidates facing each other in the same district
on more than one occasion. Using the subset of races involving repeat-pairs, Levitt
measured the relationship between the change in vote share and the change in spending
levels, producing estimates that were unrelated to differences in candidate or district attributes that might be lurking in the error term of a cross-sectional regression.

2

There were some exceptions, for example, Ansolabehere and Gerber (1994).

Field Experiments on Voter Mobilization

Using the results from several of the leading studies, we can calculate the implied cost
per vote.3 Table 1 shows that the results produced by alternative estimation strategies are
dramatically different. The estimated cost of moving the vote margin by a single vote
ranges from as little as 20 dollars to as much as 500 (Gerber, 2004). This range seems
to span all plausible estimates. Further, it is not clear which study ought to be believed,
as each relies on assumptions that, while plausible, are far from airtight. The dramatically
inconsistent results, and the sensitivity of the estimates to modeling assumptions, suggest
the usefulness of attempting a fresh approach to measuring campaign effects.
The turn to experiments represents one such attempt. The campaign-spending literature attempts to draw conclusions about the effectiveness of spending using overall
campaign spending as the independent variable. However, overall spending is the sum
of spending on a variety of different activities. Thus, it might be possible to gain insight
into the effect of spending overall by measuring the effectiveness of spending on particular
components of campaigns, such as voter mobilization efforts. This suggests the usefulness of
obtaining a ballpark estimate of the cost of inducing a supporter to cast a ballot. As the literature on campaign-spending effects developed, a parallel literature examining the effects of
campaign mobilization was developing as well. This literature progressed on an independent track and, despite its relevance, no connection was drawn to the aggregate spending
literature. What did the observational and experimental works on voter mobilization say
about the votes that could be produced through voter mobilization efforts?
Prior to the 1998 New Haven experiments, a small ﬁeld experimental literature
addressed the effects of campaign activity on voter turnout. Table 2 lists and summarizes
the results of these studies. Gosnell’s study in the 1920s was by far the largest. Gosnell
measured the effect of a nonpartisan mail campaign in Chicago’s 1924 and 1925 elections.
Eldersveld followed three decades later with studies of the effect of different modes of
contact on turnout levels. He examined the effect of voter mobilization in a pair of local
Table 1 Approximate cost of adding one vote to candidate vote margin
Incumbent
Challenger

Jacobson (1985)
Green and Krasno (1988)
Levitt (1994)
Erikson and Palfrey (2000)

$278/vote
$22/vote
$540/vote
$68/vote

$18/vote
$19/vote
$162/vote
$35/vote

2015 dollars. Calculations are based on 190,000 votes cast in a typical House district. For House
elections, this implies that a 1% boost in the incumbent’s share of the vote increases the incumbent’s
vote margin by 3800 votes.
Adapted from Gerber, A.S., 2004. Does campaign spending work?: Field experiments provide evidence
and suggest new theory. Am. Behav. Sci. 47, 541e574.

3

The cost-per-vote in this context refers to the cost of reducing the vote margin by one vote. In the context of the
turnout literature described below, cost-per-vote refers to the cost of mobilizing one additional voter.

399

400

Handbook of Field Experiments

Table 2 Voter mobilization experiments prior to 1998 New Haven experiment

Study

Date

Election

Place

Gosnell
(1927)

1924

Presidential

Chicago

Gosnell
(1927)

1925

Mayoral

Chicago

Eldersveld
(1956)

1953

Municipal

Ann Arbor

Eldersveld
(1956)

Miller et al.
(1981)

Adams and
Smith
(1980)
Greenwald
et al.
(1987)

1954

1980

Municipal

Primary

Ann Arbor

Carbondale,
IL

1979

Special city
council

Washington,
DC

1984

Presidential

Columbus,
Ohio

No. of subjects
(including
control group)

3969
registered
voters
3676
registered
voters
41 registered
voters
43 registered
voters
276 registered
voters
268 registered
voters
220 registered
voters
79 registered
voters
80 registered
voters
81 registered
voters
2650
registered
voters
60 registered
voters

Treatment

Effects
on
turnout
(%)a

Mail

þ1

Mail

þ9

Canvass

þ42

Mail

þ26

Canvass

þ20

Mail

þ4

Phone

þ18

Canvass

þ21

Mail

þ19

Phone

þ15

Phone

þ9

Phone

þ23

a
These are the effects reported in the tables of these research reports. They have not been adjusted for contact rates. In
Eldersveld’s 1953 experiment, subjects were those who opposed or had no opinion about charter reform. In 1954,
subjects were those who had voted in national but not local elections. The Greenwald et al. results are those for which
Greenwald and colleagues count as voting those who in a follow up call say they voted outside the jurisdiction of the
study. Alternative treatments of these cases has no material effect on the results. Note that this table includes only studies
that use random experimental design [or (possibly) near-random, in the case of Gosnell (1927)].
Adapted from Gerber, A.S., Green, D.P., Nickerson, D.W., 2001. Testing for publication bias in political science. Polit.
Anal. 9, 385e392.

elections in Ann Arbor. Greenwald et al. (1987) investigated the psychological hypothesis
that predicting one’s behavior had a causal effect on future action. They constructed a
brief series of questions that had the effect of inducing treated subjects, a random subset
of 32 treated subjects drawn from a collection 60 Ohio State undergraduates, to state that

Field Experiments on Voter Mobilization

they intended to vote in the next day’s 1984 presidential election. They measured the
effect of this treatment on their subsequent turnout and found the intervention produced
more than a 20 percentage point boost in turnout. In 1980, Adams and Smith measured
the effect of a 30 s phone call on turnout and vote choice in a District of Columbia special
election. In the same year, Miller et al. (1981) measured the turnout effects of door-todoor canvassing, phone calls, and direct mail on randomly targeted voters in a Carbondale, Illinois primary election.
Summarizing the early experiment literature, prior to 1998, there were a few studies
conducted over many decades and across a range of political contexts. Nevertheless,
when the small literature is viewed as a whole, a few conclusions emerge. First, it appears
that campaign interventions are highly effective. Short phone calls produce turnout increases of 10 or even 20 percentage points. According to these studies, visits from canvassers or even a single letter also tend to produce effects of this same magnitude.
These are very large estimated effects; to put this treatment effect into context, the falloff
in turnout between a presidential election and a midterm election is about 10 percentage
points. Second, these large treatment effects are observed in both general elections, such
as the 1984 presidential election, and less high proﬁle contests. Third, treatment effects
show no tendency to decrease over time.
Another important and related line of research employed laboratory experiments to
assess the effect of campaign activity. A leading example of this work is the inﬂuential
study by Ansolabehere and Iyengar (1996), who brought subjects into a laboratory setting
designed to mimic a typical living room and measured the effect of political advertisements inserted into mock newscasts. They found that advertisements that attacked the
opposing candidate reduced the likelihood that subjects, when interviewed later, said
they would vote, an effect that was especially strong among independent voters. Like
ﬁeld experiments, these studies use random assignment to estimate the causal effect of
campaign communications. However, it is hard to translate the results from the laboratory experiments into quantitative estimates of the impact of actual campaign activity on
actual voter turnout. Despite the researchers’ best effort to simulate the typical viewer
experience and measure outcomes reliably, the context in which subjects receive the
treatment and express outcomes (intention to vote) differs from natural settings in so
many ways, both obvious and subtle, that it is unclear how the lab result indicates either
the magnitude or even the direction of the campaign effects being studied.4

4

As Gerber (2011) notes, it is not necessarily the case that estimates obtained in the lab indicate the direction of effects in
ﬁeld settings. There are often plausible arguments for why a lab effect might go in the opposite direction from the real
world effect. One major difference between the lab and ﬁeld is that in the real world individuals have additional
choices and exposures. Applying this to laboratory studies of negative campaigning, outside the lab individuals may be
inspired by a negative advertisement to seek additional information about the claim or to pay more attention to
campaign related stimuli, leading to greater interest and higher participation levels.

401

402

Handbook of Field Experiments

In contrast to the occasional experimental study, the vast majority of work on campaigns and turnout was (and is) observational. During the 1990s, the most inﬂuential
scholarship on the causes of turnout were studies that measured the relationship between
voter turnout and voter demographics, attitudes, and reported campaign contacts using
survey data. Research by Rosenstone and Hansen (1993) is an exemplar of this line of
work. Their book was extremely inﬂuential and remains a standard reference (its Google
scholar citation count exceeded 3700 as of the start of 2016). The book is cited by all
turnout scholars, and the research design they employ is still common in current research.
The American National Election Study is a federally funded biennial survey research
project that began in 1952 and continues to this day. Questions about voter turnout
and campaign contact have been asked since the earliest surveys, and Rosenstone and
Hansen use the American National Election Studies (ANES) to measure the effect of reported campaign contacts on various measures of participation. Using estimates from a
pooled cross-sectional analysis of ANES data, they calculate the incremental contribution
of many different factors, including campaign contacts, on reported participation in presidential and midterm years (see Tables 5.1 and 5.2 in Rosenstone and Hansen, 1993).
They ﬁnd that the estimated effect of campaign contact on reported voter turnout is
approximately a 10 percentage point increase in turnout probability.
The 10 percentage point turnout boost from campaign contact found by Rosenstone
and Hansen is similar in magnitude to the effects estimated by many of the early ﬁeld experiments. However, despite this agreement, there are grounds for skepticism. As we
point out in the next section, respondents’ exposure to campaign contact is neither
randomly assigned nor accurately measured. The move to ﬁeld experiments in the late
1990s was motivated in part by concern about the potential bias in the dominant survey-based research tradition.

2. HOW DO EXPERIMENTS ADDRESS THE PROBLEMS IN THE PRIOR
VOTER TURNOUT RESEARCH?
In this section we present a basic framework for deﬁning causal effects and apply the
framework to explain how ﬁeld experiments eliminate some of the key sources of bias
in observational studies. To ﬁx ideas, we will use the classic Rosenstone and Hansen
(1993) survey analysis as a running example. In Rosenstone and Hansen, some respondents report that they are “treated” (contacted by a campaign) and some report that
they are “untreated” (not contacted by the campaign). The key challenge in estimating
the treatment effect of campaign contact on those who are truly contacted is that the analyst must use available data to construct an estimate of a counterfactual quantity, the
turnout rate of the contacted in the event they had not been treated. We express this
challenge using potential outcomes notation (Rubin, 1978). For each individual i, let
Yi0 be the outcome if i does not receive the treatment (in this example, contact by the

Field Experiments on Voter Mobilization

mobilization effort), and Yi1 be the outcome if i receives the treatment. The treatment
effect for individual i is deﬁned as
si ¼ Yi1  Yi0 :

(1)

We deﬁne the treatment effect for individual i as the difference between the outcome
for i in the two possible, but mutually exclusive, states of the world: one in which i is
treated, and another in which i is not. Moving from a single individual, the average treatment effect for the treated (ATT) is deﬁned as
ATT ¼ Eðsi jTi ¼ 1Þ ¼ EðYi1 jTi ¼ 1Þ  EðYi0 jTi ¼ 1Þ;

(2)

where the E( ) operator stands for a group average and Ti ¼ 1 when a person is treated.
The quantity Yi1jTi ¼ 1 is the posttreatment outcome for those who are actually treated,
and Yi0jTi ¼ 1 is the outcome that would have been observed for the treated had they, in
fact, not been treated.
In Rosenstone and Hansen, as in the rest of the nonexperimental literature, the comparison group for the treated are subjects who are untreated. When covariate adjustment
is used, the comparison group is the set of subjects who are untreated but resemble the
treated with respect to their background attributes. This approach is susceptible to selection bias when the potential outcomes among the untreated are systematically different
from those of the treated. Stated formally, in expectation the observational comparison of
the treated and the untreated estimates yields
EðYi1 jTi ¼ 1Þ  EðYi0 jTi ¼ 0Þ ¼ ½EðYi1 jTi ¼ 1Þ  EðYi0 jTi ¼ 1Þ
þ ½EðYi0 jTi ¼ 1Þ  EðYi0 jTi ¼ 0Þ
¼ ATT þ Selection Bias:

(3)

Under what conditions does the selection bias term disappear? The critical assumption
for identiﬁcation of the average treatment on treated in observational work is thatd
controlling for covariates (whether through regression or through matching),
E(Yi0jTi ¼ 1) ¼ E(Yi0jTi ¼ 0), that is, apart from their exposure to the treatmentdthe
treated and untreated group outcomes are on average the same in the untreated state.
In the absence of some unusual as-if random circumstance by which some units came
to be treated and other remained untreated, this assumption is not credible. Consider
the case at hand, estimating the effect of campaigning on voter turnout. Campaigns typically have extensive information available about a jurisdiction’s voters based on both
administrative records of voter turnout and demographics along with insider information
about individuals and neighborhoods. This information, which may not be fully available
to the data analyst, is typically used in campaign targeting strategies. Campaigns
commonly target those who have shown a tendency to participate, and this characteristic
is, from the standpoint of the analyst, an omitted variable. The ANES, for example, does

403

404

Handbook of Field Experiments

not record respondents’ vote history, although voter ﬁles available to campaigns do
contain this information. Second, previous turnout records are highly predictive of the
outcome variable, turnout. Therefore, E(Yi0jTi ¼ 1) may be substantially higher than
E(Yi0jTi ¼ 0). Although in this case it is possible to guess the direction of the bias, analysts
rarely have a ﬁrm basis to speculate about the magnitude of the bias, and so it is not
possible to correct the estimates.5
Beyond selection bias, ﬁeld experiments mitigate a variety of other common methodological concerns regarding observational studies of political behavior. In observational studies the researcher controls neither the treatment assignment nor the design
of the treatment. At the most basic level, a key feature of ﬁeld experimentation is
that the researcher controls the assignment to treatment and therefore knows which
subjects are assigned to treatment and control conditions. Observational studies often
attempt to measure whether an individual is treated or not, but survey measures may
be unreliable. Commonly, whether a subject is treated or not relies on the subject’s
self-report (of campaign contact, of advertising exposure, of media usage, etc.).
Consider again the example of attempts to measure the effects of campaign mobilization
on voter turnout. In this literature, contact is self-reported, and misreporting leads to a
treatment group that is a mixture of the treated and untreated. If this misreporting is
random misclassiﬁcation, the estimated average treatment effects will be attenuated,
but if those who misreport campaign contact tend to be the more politically engaged,
this nonrandom measurement error may exaggerate the effects of campaign contacts.
This bias will be heightened when, as is often the case, the subject’s turnout is itself
based on self-report. There is empirical evidence of both substantial misreporting and
a positive correlation between misreporting campaign exposure and misreporting having voted (Vavreck, 2007; Gerber and Doherty, 2009). It should be noted that although
from time to time previous observational work has employed validated vote (the ANES
used public voting records to add this variable into the survey datasets for the years
1964, 1972, 1974, 1976, 1978, 1980, 1984, 1986, 1988, and 19906), one of the important innovations brought about by the advent of ﬁeld experimentation in this area is that
it has become common for studies of political behavior to use administrative data rather
than self-reports.
A further problem that is avoided by ﬁeld experiments is ambiguity about what intervention is being assessed. Turning again to the case of the voter mobilization research, the
ANES item used for campaign contact in the Rosenstone and Hansen study asks

5

6

Further, when “correcting” for bias, this uncertainty about the size of bias is not contained in the reported standard
errors and, unlike sampling variability, it remains undiminished as the sample size increases (Gerber et al., 2004). The
conventional measures of coefﬁcient uncertainty in observational research thereby underestimate the true level of
uncertainty, especially in cases where the sample size is large.
See http://www.electionstudies.org/overview/dataqual.htm.

Field Experiments on Voter Mobilization

respondents: “Did anyone from one of the political parties call you up or come around
and talk to you about the campaign?” Taken literally, this question asks the respondents
about partisan phone or face-to-face contact leading to a conversation about the
campaign, which omits all campaign contact through mail, all contact about political issues outside the campaign, and possibly all manner of nonpartisan contact urging turnout.
It is unclear whether survey respondents attend to these nuances when answering the
question, which only deepens the ambiguity surrounding the treatment effect that survey-based regressions are estimating.
In experimental analysis, it is now standard to account for noncompliance. In the
context of voter mobilization, noncompliance most commonly occurs when individuals
who were assigned to the treatment group remain untreated. The rate at which failure to
treat occurs varies across modes of contact, the intensity of the effort to contact, the difﬁculty of contact, and attributes of the subjects and context. Noncompliance arises for a
variety of reasons, such as the subject relocating, not answering the door or phone when
the campaign attempts contact, or the campaign running out of resources before attempting to contact all subjects assigned to be treated. The failure to treat is immediately
apparent in ﬁeld experiments, and the observed difference in average outcomes for the
treatment and control groups is adjusted for the proportion of the treatment group contacted to estimate the average treatment effect among compliers (Angrist et al., 1996),
which is the same as the average effect of the treatment on the treated when experiments
encounter one-sided noncompliance.
Properly accounting for noncompliance in voter mobilization experiments is an
innovation of recent work, as experimental studies prior to 1998 either dropped the untreated subjects in the treatment group from the analysis or reclassiﬁed them as control
group observations (Adams and Smith, 1980; Eldersveld, 1956) or made no mention
of the issue (Miller et al., 1981). Such approaches produce biased estimates of the effect
of the treatment on the treated if those who cannot be contacted in the treatment group
have a different average untreated potential outcome than the entire pool of subjects.
Because failure to treat may stem from factors related to propensity to turnout, such as
recently relocating, being out of town around election day, being busy or anti-social,
or any of a number of other possibilities, noncompliance is unlikely to be ignorable.
In studies of GOTV phone calls, those who are hard to contact often prove to be
much less likely to vote than the average subject (Gerber and Green, 2005; Arceneaux
et al. 2006). In observational studies, those whom the campaign cannot reach will
tend to report that they were untreated and will therefore be grouped with those the
campaign did not attempt to reach. Thus, in addition to selection bias due to the
campaign targeting, there is also bias due to the campaign’s failure to treat some of its
targets.
In sum, ﬁeld experiments have at least three important advantages over survey-based
observational studies of voter turnout. Random assignment of the treatment eliminates

405

406

Handbook of Field Experiments

the threat of selection bias. Direct manipulation of the treatment also allows researchers to
have more control over what the treatment is and to more accurately ascertain whether
subjects received it. The use of administrative data to measure outcomes helps ensure
symmetry between those assigned to the treatment and control groups. A commonly
noted limitation of ﬁeld experiments is that they seldom encompass a random sample
of a national electorate (but see Fieldhouse et al., 2013), which raises the question of
whether experimental results generalize across subjects, treatments, and contexts. One
way to address this concern is through extensive replication of experiments, a practice
that has become common in voter mobilization research. Indeed, one of the distinctive
features of the experimental literature on voter mobilization is the large and ever growing
number of studies that replicate and extend existing research. The next section describes
the evolution of the experimental literature, which now encompasses studies conducted
in Europe, Asia, and Latin America.

3. AN OVERVIEW OF THE EXPERIMENTAL LITERATURE ON VOTER
MOBILIZATION7
The modern voter mobilization literature can be divided into two main classes of studies.
The early work focused on the relative effectiveness of different modes of contact. This
focus was in part inspired by a concern that the shift from the more personal campaigning
of a previous era to modern campaigns conducted through mailings and television were
contributing to a decline in turnout. Although some studies introduced experimental
variation in message content, this was not the major focus of the research. A second
line of research aimed to measure the effect of alternative messages employed in the communications. Often inspired by psychological theories or political folk wisdom, these
studies examined how the impact of the communication changed according to the words
and images used in the campaign material. For some inﬂuential theories in social psychology, this literature, although published largely in political science journals, provides some
of the most telling empirical evidence.

3.1 Modes of contact
The New Haven 1998 study examined the relative effectiveness of three common
campaign tactics: door-to-door canvassing, calls from commercial phone banks, and
direct mail. The study found that face-to-face canvassing produced an 8 percentage point
increase in turnout among those contacted, each piece of mail raised turnout by half a
percentage point in households receiving the mail (the number of mailings varied
from 0 to 3), and a phone call produced no increase in turnout. A substantial follow
7

This section is adapted from Green and Gerber (2015).

Field Experiments on Voter Mobilization

up literature measured the effect of each of these three modes of communication across a
range of contexts and extended this line of research to include GOTV appeals communicated via television, radio, and social media.
3.1.1 Canvassing studies
After the New Haven Study, basic questions of generalizability abounded. Would
canvassing work elsewhere? Would it work in competitive as well as uncompetitive
municipal races? We ﬁrst summarize studies that, like the New Haven Study, canvassed
using nonpartisan GOTV appeals. In 2001 a multisite evaluation was carried out in six
cities: Bridgeport, Columbus, Detroit, Minneapolis, Raleigh, and St. Paul. Baseline
turnout rates in the control groups varied considerably across sites, from 8.2% to
43.3%. Despite the varying electoral and demographic contexts, results were no more
variable than one would expect by chance. In all six sites, turnout was higher in the
assigned treatment group than the control group, although the increase was negligible
in one site. Analyzing the data for the six sites with a single regression model raised
turnout among those contacted by 7.1 percentage points with a standard error of 2.2 percentage points (Green et al., 2003).
Another mobilization experiment conducted in 2001 extended previous work in
three important directions (Michelson, 2003). First, the canvassing effort achieved a
remarkable 75%-contact rate. Second, it showed how mobilization works in a rural
setting. The study took place in a low-turnout municipal election in a largely Latino California farming community. Third, it varied the campaign message between appeals that
stressed either civic duty, ethnic solidarity (for Latino voters), or community solidarity
(for non-Latino voters). Regardless of the message used, the team of Latino canvassers
proved highly effective at mobilizing Latino voters. For all Latinos, turnout increased
from 13.8% (N ¼ 298) to 18.5% (N ¼ 466). For non-Latinos, turnout increased from
25.7% (N ¼ 758) to 28.2% (N ¼ 1243). Canvassers contacted 73% of Latinos and 78%
of non-Latinos. The scripts were not signiﬁcantly different in terms of the effectiveness
with which they mobilized voters.
Again examining the effects of alternative messages in addition to the effects of Latino
and non-Latino canvassers, Herbert Villa and Melissa Michelson (2003) focused on a
sample of voters under the age of 26, encouraging them to vote in the 2002 state and
federal elections. Turnout among Latino subjects rose from 7.2% (N ¼ 1384) to 9.3%
(N ¼ 1507), and among non-Latino subjects it rose from 8.9% (N ¼ 1438) to 10.0%
(N ¼ 1455). The contact rates were 51% and 39%, respectively. Again, Michelson and
Villa found no evidence that the content of the canvassing script made an appreciable difference. Michelson returned to Fresno in 2003, using students from her classes to conduct
an experiment on the differential effects of partisan and nonpartisan appeals. Like the
Bennion study of the 2002 midterm election, which also used students canvassing as
part of a course assignment, this study found weak treatment effects (Bennion, 2005).

407

408

Handbook of Field Experiments

Overall, the control group (N ¼ 2672) turned out at a rate of 15.2%, compared to 14.9%
in the treatment group (N ¼ 3371), which was contacted at a rate of 34%.
Unlike other studies of door-to-door canvassing, Nickerson (2008) used a placebo
control design. Half of those contacted were urged to recycle; the other half, to vote
in the 2002 primary elections held in Denver and Minneapolis. Turnout increased
from 47.7% (N ¼ 279) to 56.3% (N ¼ 283) among those urged to vote. Since by design
the contact rate was 100%, the study had reasonable statistical power despite the small
sample size. Perhaps the most interesting aspect of this experiment was Nickerson’s
demonstration that turnout among housemates of persons in the treatment group was
signiﬁcantly higher than turnout among housemates of those in the control group, suggesting that the mobilizing effects of a face-to-face conversation with canvassers may have
been transmitted to other members of the household.
In 2004, Carrie LeVan (2016) organized a nonpartisan canvassing campaign aimed
at mobilizing voters in low-turnout, low-income, and largely Latino precincts in
Bakersﬁeld, California. The study comprised 727 voters, 423 of whom lived in households that were assigned to the treatment group. The contact rate among those assigned
to the treatment group was 50%. The study found strong canvassing effects. Among
voters living in one-person households, for example, turnout was 41.0% in the control
group and 54.5% in the treatment group. Gregg Murray and Richard Matland (2012)
also conducted a canvassing study in a largely Latino area, Brownsville, Texas. Turnout
among the 3844 individuals assigned to the control group was 33.3%, compared to
34.9% among the 7580 assigned to the canvassing group, of whom 22% were actually
contacted.
Lisa García Bedolla and Melissa Michelson (2012) collaborated with several nonpartisan groups participating in the California Votes Initiative, which sought to mobilize
low-propensity voters in a series of elections from 2006 through 2008. The effort is noteworthy because of the number of organizations that conducted door-to-door outreach,
the range of ethnic groups that were targeted, and the range of electoral contexts during
which canvassing took place. In all, 117 distinct experiments were conducted. Although
the authors note that many of the participating organizations contacted voters primarily
to spread the word about the organization’s activities or administer issue surveys rather
than to engage in voter mobilization (p. 127), the treatment voted at a higher rate
than the control group in 77 of these experiments, which would occur by chance
with p < .001.
By comparison to partisan canvassing, which tends to occur on a vast scale in closely
contested states in presidential elections, nonpartisan canvassing is relatively rare. However, since partisan campaigns always have the option of using nonpartisan appeals to
mobilize their partisan supporters, experimental evaluations of nonpartisan canvassing
are potentially informative even to campaigns that seek to advocate on behalf of a candidate or ballot measure. Nevertheless, the question arises as to whether the results would

Field Experiments on Voter Mobilization

differ if canvassers attempted to urge voters to support a particular candidate or cause.
Although no experiments have attempted a head-to-head comparison between nonpartisan and advocacy appeals, a series of advocacy experiments suggest that such canvassing
may produce widely varying effects.
Two experiments conducted in 2003 gave early indications that advocacy campaigns could be quite effective in mobilizing voters. In Kansas City, the ACORN organization canvassed extensively in predominantly African American precincts. Its aim
was to identify and mobilize those supportive of a ballot measure designed to preserve
local bus service. Unlike most other canvassing experiments, this one was randomized at
the level of the precinct, with 14 assigned to the treatment group and 14 to the control
group. Among voters assigned to control precincts (N ¼ 4779), turnout was 29.1%,
compared to 33.5% in the treatment group, 62.7% of whom were contacted
(Arceneaux, 2005). At roughly the same time, ACORN canvassed in Phoenix on
behalf of a ballot measure to determine the future of the county hospital (Villa and
Michelson, 2003). ACORN conducted two rounds of canvassing, the ﬁrst to identify
voters sympathetic to the ballot measure and a second to urge supportive voters to vote.
The canvassing effort targeted voters with Latino surnames who had voted in at least
one of the previous four elections. ACORN made multiple attempts to contact voters
(including making a small number of phone calls), the result being that 71% of those
living in one-voter households were contacted at least once. This ﬁgure rose to 80%
among two-voter households. This mobilization campaign had a powerful effect on
turnout. Among one-person households, turnout rose from 7.4% in the control group
(N ¼ 473) to 15.9% in the treatment group (N ¼ 2666). Among two-person households, turnout rose from 6.9% in the control group (N ¼ 72) to 21.0% in the treatment
group (N ¼ 2550).
On the other hand, advocacy campaigns have been known to produce disappointing
results. Strategic Concepts in Organizing and Policy Education (SCOPE) in Los
Angeles canvassed in opposition to the “three-strikes” statewide ballot measure but
generated no apparent turnout effect (Arceneaux and Nickerson, 2009). Gray and Potter
(2007) found weak mobilization effects in a small canvassing experiment on behalf of a
candidate for local magistrate. In their study of canvassing on behalf of a local candidate,
Barton et al. (2012) ﬁnd an unexpectedly negative effect on turnout. Larger candidate
advocacy experiments show positive effects, although the treatment-on-treated estimates
are smaller than those obtained in the ACORN studies. A sizable experiment on behalf of
a Democratic gubernatorial candidate in 2005 generated a treatment-on-treated estimate
of 3.5 (SE ¼ 2.4), and a series of experiments in 2014 on behalf of state legislative candidates in Republican primary runoff elections generated a treatment-on-treated estimate
of 3.1 (SE ¼ 1.8).

409

410

Handbook of Field Experiments

3.1.2 Commercial phone banks
In 1998 the authors conducted two nonpartisan campaigns using a single commercial
phone bank (Gerber and Green, 2000, 2001). The smaller of the two campaigns was conducted in New Haven; a larger study was conducted in neighboring West Haven. In
both cities, the elections were rather quiet affairs, with relatively little campaign activity.
In both experiments, the group receiving phone calls voted at rates that were no greater
than the rates of the control group receiving no calls. None of the three scriptsdone
stressing civic duty, another, neighborhood solidarity, and a third, the possibility of
deciding a close electiondhad any appreciable impact.
In order to assess whether these results were speciﬁc to the context or the calling
house, we replicated the 1998 experiments on a grand scale in 2002 (Arceneaux et al.,
2006). Congressional districts in Iowa and Michigan were divided into two categories,
depending on whether they featured competitive or uncompetitive races. Within each
category, 15,000 randomly selected individuals at distinct addresses were called by one
of two commercial phone banks, each delivering the same nonpartisan message. Thus,
60,000 people in all were called in the treatment group, and more than 1 million names
were placed in the control group. In the 2002 study, the treatment effects were just barely
on the positive side of zero, implying that these phone banks mobilized one additional
voter for every 280 people they spoke with. Another massive study in Illinois, which
called voters before the 2004 November election using a similar nonpartisan script, found
somewhat larger effects (Arceneaux et al., 2010). This time one vote was generated per
55 completed calls. However, this study is counterbalanced by a pair of large nonpartisan
experiments in North Carolina and Missouri, which found conventional calls to have
meager effects, just one vote generated per 500 contacts (Ha and Karlan, 2009).
Calls that advocate on behalf of a candidate or ballot measure have been found to produce similarly weak average treatment effects among compliers. Close to 30,000 calls
(about half resulting in successful contact) were made by a commercial phone center
on behalf of a ballot measure in a San Francisco municipal election. Consistent with other
ﬁndings concerning the delivery of brief scripts by commercial phone banks, one vote
was produced for every 200 successful contacts (McNulty, 2005). Similar results were
found in a relatively small study of a 2002-gubernatorial primary (Cardy, 2005). A
much larger experiment conducted by the 2006 general elections also found weak effects,
regardless of whether these calls were made using nonpartisan messages or messages advocating support for a minimum wage measure (Mann, 2008). A head-to-head experimental comparison between partisan and nonpartisan scripts indicated that neither had
an appreciable effect on turnout (Panagopoulos, 2008).
Several scholars have investigated the hypothesis that the effectiveness of these calls
hinges on the manner in which the scripts are delivered. Commercial vendors are paid
according to the number of targets they reach, not the number of votes they generate.
The callers, who can forge through 50 or so completed calls per hour, behave much

Field Experiments on Voter Mobilization

as one would expect given the incentives of piecework and the eagerness of supervisors to
move on to the next calling campaign.
In 2002 David Nickerson evaluated a youth-oriented voter mobilization campaign in
which a commercial phone bank was paid top dollar to deliver its GOTV appeal in a
chatty and unhurried manner. The script required the reader to pause for questions
and to invite respondents to visit a website in order to learn more about their polling
location. A good deal of coaching ensured that this appeal was read at the proper speed.
Between one and four calls were made to randomly selected subgroups of young people
over the four-week period leading up to election day. The phone bank kept records of
each person they contacted, so that when respondents were contacted a second time, the
script took notice of the fact that the previous conversation was being resumed. The calls
produced a substantial and statistically signiﬁcant increase in voter turnout in the target
group, but only among those called during the ﬁnal week of the campaign. In other
words, calls made during the ﬁrst three weeks of a month-long GOTV campaign had
no apparent effect on voter turnout. Calls made during the last week produced one
vote for every 20 contacts (Nickerson, 2007). This ﬁnding set in motion a series of experiments designed to sort out whether the strong effects reﬂect timing, the use of
repeated calls, or the conversational style in which the scripts were delivered.
As to the timing and sequencing of calls from commercial phone banks, a large study
conducted across battleground and non-battleground states in the weeks leading up to
the 2008 presidential election found that neither ﬁrst round nor second round calls by
themselves boosted turnout but that turnout rose signiﬁcantly when voters who in round
1 said they planned to vote were later called back and asked whether they could still be
counted on to vote. This ﬁnding echoes the unusually strong effects found in four
follow-up call experiments conducted by volunteer phone banks (Michelson et al.,
2009). However, this effect did not replicate in a large commercial phone bank experiment in 2014, which found follow-up calls to have much weak effects (Gerber et al.,
2016).
These results tentatively suggest that the active ingredients in a successful call are the
scripts and the manner in which they are delivered. This scripts hypothesis was tested in
prior to the presidential election of 2004 with calls directed at residents of a battleground and non-battleground state (Ha and Karlan, 2009). A large phone bank
deployed three kinds of nonpartisan scripts: a standard script akin to the ones used
above; a longer, chattier script in which people were asked whether they knew their
polling location, which was provided on request; and a still longer script in which people were encouraged both to vote and to mobilize their friends and neighbors to vote.
The results are suggestive, if a bit puzzling. As expected, the standard script had weak
effects, raising turnout by just 1.2 percentage points among those contacted. Also as expected, the medium script had a fairly large effect, producing a complier average casual
effect (CACE) estimate of 3.4 percentage points. This statistically signiﬁcant increase

411

412

Handbook of Field Experiments

implies that one vote was generated for every 30 completed calls. The puzzling result is
the fact that the chatty recruit-your-friends script had an unexpectedly weak effect, as
one vote per 69 completed calls.
The call-quality hypothesis was tested in 2010 in a head-to-head competition among
different phone banks (Mann and Klofstad, 2015). On the high side of the quality spectrum were phone banks that specialized in fundraising or political calls; on the low side
were phone banks whose business consisted of a wide array of nonpolitical as well as political clients. Mann and Klofstad reason that ﬁrms on the low end of the quality spectrum
are incentivized to push through a high volume of calls in a mechanical fashion, whereas
the focus and reputation of the high-quality ﬁrms required them to recruit and retain callers with a knack for political persuasion. Each of the four phone banks called more than
100,000 voters across several states. All the phone banks used the same “chatty” script,
which blended several of the ideas discussed in Section 4: gratitude, implementation intentions, and positive descriptive norms. Consistent with the quality hypothesis, the two
low-quality phone banks generated weak results, raising turnout among those they spoke
with by just 0.2 percentage points. By contrast, the two high-quality phone banks raised
turnout among those they reached by 0.9 and 1.4 percentage points. Although the highquality phone banks proved far less effective than the average volunteer phone bank or
the vaunted high-quality phone bank in the Nickerson study, they were signiﬁcantly
more effective than the low-quality phone banks. (Ironically, the lower quality phone
banks also reported a higher rate of contacts, which meant that they ended up being
more expensive on a cost-per-vote basis.) Given the immense size of this experiment
and the tight controls that the authors imposed on the scripts used by the different phone
banks, this study offers the most convincing evidence to date about the importance of
that intangible ingredient, quality.
3.1.3 Mailings
We begin our summary of the direct mail literature by focusing on “standard” nonpartisan appeals, deferring the discussion of mailings that exert social pressure and other psychological tactics until Section 4. During the four weeks leading up to the 1998 election,
we conducted an experiment in which registered voters in New Haven received one,
two, or three pieces of nonpartisan direct mail. Each batch of mail reﬂected one of three
themes: the need to do one’s civic duty, the responsibility to stand up for one’s neighborhood so that politicians will take an interest in its problems, or the importance of voting
in a close election. Turnout in the control group, which received no mail, phone calls, or
door-to-door canvassing, was 42.2% (N ¼ 11,596). Turnout was 42.6% (N ¼ 2550)
among those receiving one mailer, 43.3% (N ¼ 2699) among those receiving two,
and 44.6% (N ¼ 2527) among those receiving three. For the sample as a whole
(N ¼ 31,098), regression estimates that controlled for the effects of phone and doorto-door canvassing put the effects of each additional mailer at 0.5 percentage point

Field Experiments on Voter Mobilization

(SE ¼ 0.3), which was narrowly signiﬁcant at the 0.05 level using a one-tailed test. No
signiﬁcant differences were found among the three messages.
In New Haven’s 1999 mayoral election, nonpartisan mailings patterned after the civic
duty and close election mailings used in the 1998 earlier study were sent to a random
sample of the 1998 voter list. The innovation of this study was to send up to eight mailings in order to assess diminishing returns. The close election message had no effect (the
election was not remotely close), but the civic duty message performed on par with the
1998 results. The results suggest that returns from mailings begin to diminish after six
mailings per household.
Given these encouraging initial results, a series of subsequent experiments tested the
effectiveness of nonpartisan mailings as a means of encouraging turnout among ethnic
minorities. In a ﬁeld experiment conducted before the 2002 election, Janelle Wong
(2005) classiﬁed Los Angeles County voters by last name into one of several Asian American groups: Chinese, Filipino, Indian, Japanese, and Korean. Chinese Americans were
sent one piece of bilingual nonpartisan direct mail encouraging them to vote. Other
ethnic groups were sent one piece of direct mail in English. Among Chinese Americans,
turnout in the control group was 29.0% (2924); the treatment group turned out at a rate
of 31.7% (1137). Among other Asian groups, the control group voted at a rate of 38.5%
(N ¼ 5802), compared with the treatment group rate of 39.4% (N ¼ 2095). Also in the
2002 election, a much larger multisite experiment sought to mobilize Latino voters in Los
Angeles County, Orange County (California), Houston, New Mexico, and Colorado
(Ramirez, 2005). The content of the bilingual mailers was developed in collaboration
with consultants using focus groups. The number of mailers varied across sites from
two to four. Despite the high quality of the printing and graphics, they were found to
have weak turnout effects.
Several other scholars have attempted to gauge whether ethnic communities can be
mobilized using direct mail in 2004. Trivedi (2005) tested alternative nonpartisan messages and graphic themes designed to mobilize Indian American voters living in New
York City. Her postcards conveyed ethnic, pan-ethnic, or civic duty appeals, but no message stood out as particularly effective. Richard Matland and Gregg Murray (2012) conducted a nonpartisan mail campaign in largely Latino Brownsville, Texas. Households
were randomly assigned a postcard with one of two messages. One emphasized greater
power for Latinos if they became more politically active and voted. The other emphasized civic duty and the closeness of the election as the reasons for the recipients should
go to the polls and vote in the upcoming presidential election. Both found weak effects.
Bedolla and Michelson (2012) conducted 38 direct mail experiments in California
from 2006 through 2008 in an effort to mobilize minority voters using a combination
of generic and ethnic appeals. They found weak effects overall, with 19 of the 38 experiments producing positive estimates. Neither voter guides nor hand-written postcards
seemed to boost turnout.

413

414

Handbook of Field Experiments

Two further strands of the nonpartisan mail literature deserve mention. The ﬁrst is the
simple reminder that an election is imminent. This tactic has repeatedly been shown to
have negligible effects and has come to be used as a placebo condition in several experiments (Panagopoulos, 2014, 2013, 2011). Another tactic is to pique voters’ interest in an
election by calling attention to certain ballot measures. One such experiment sent a single
mailing to registered voters both across Florida and speciﬁcally in Leon County (Barabas
et al., 2010). Each mailing alerted voters to the importance of one ballot measure. The
authors compared the mobilization effects of these mailings to those of a generic GOTV
mailing and found small differences in effects across different issues or appeals.
The literature gauging the turnout effects of advocacy mailings is essentially a string of
null ﬁndings. The ﬁrst large-scale experiments were conducted in 1999 in state legislative
and municipal elections on behalf of Democratic candidates (Gerber et al., 2003). The state
legislative experiments divided the target population into “prime” Democrats (those with a
high propensity to vote), “nonprime” Democrats and Independents, and a random sample
of the list of registered voters. The mailings boosted turnout among prime Democrats, but
not among other Democrats. Turnout in the random sample rose with the number of
mailings, but the effects were small given the number of mailings sent to each household.
Combining all of the New Jersey samples suggests that mail did not signiﬁcantly increase
voter turnout. Some slight evidence for demobilization may be found in the negatively
toned mayoral campaign, which sent nine mailings to each household.
Another early study evaluated the mobilizing effects of advocacy mail from an abortion-rights interest group, which backed a pro-choice candidate in a gubernatorial primary campaign (Cardy, 2005). The group targeted strongly pro-choice voters whose
stances had been previously identiﬁed by the phone interviews. The treatment group
(N ¼ 1974) received ﬁve mailings that were printed in full color on glossy paper and
mailed between 19 and 6 days before the election. Turnout in the control group
(N ¼ 2008) was slightly higher than in the treatment group. Other small studies produced results that, on average, suggest little effect on turnout (Cho et al., 2006; Gray
and Potter, 2007; Niven, 2006). This conclusion was bolstered by a massive study that
sent up to nine pieces of mail on behalf of a Democratic gubernatorial candidate in
2005, as well as a large test on behalf of Republican state legislative candidates in 2014
that sent up to 12 mailers (Cubbison, 2015). The lack of effect is not altogether surprising
given that the mailers focused on issues and candidates rather than turnout. Nevertheless,
the ﬁndings drive home the point that advocacy communications per se do little to stimulate voter turnout.8

8

Somewhere between nonpartisan mail and advocacy mail are mailers from advocacy groups that target ideologically
allied voters but appeal to them using nonpartisan language. See, for example, Mann (2008). These studies tend to
produce effects that are somewhere between the noneffects of partisan mail and the weak effects of nonpartisan mail.

Field Experiments on Voter Mobilization

3.1.4 Other modes of communication
Compared to the extensive experimental literature on canvassing, phone banks, and
direct mail, the literature on other modes of communication looks relatively thin.
Some of the most robust ﬁndings concern the weak effects of some widely used tactics.
For example, David Nickerson (2007) reports the results of 13 experiments in which
almost a quarter of a million peopledcollege students, registered voters who did not
opt out of email communication, or visitors to websites who agreed to be reminded
to votedwere urged via email to vote in an upcoming election. These nonpartisan
appeals produced negligible effects, even when 20% or more of the recipients opened
the GOTV email on an HTML-compatible browser. Malhotra et al. (2012) found
that small but statistically signiﬁcant effects were found when emails were sent out by
the registrar of voters, but identical emails sent by a nonpartisan group has no effect. Alissa
Stollwerk collaborated with the Democratic National Committee to assess the effects of
three emails encouraging voter turnout in support of the Democratic mayoral candidate
in the 2005 New York City general election. The emails were sent in the late afternoon
on election eve, on the morning of Election Day, and during the midafternoon of Election Day. The subject lines referred to voting, and the text of the email itself implored
Democrats to “vote to ensure that our representatives protect the values and beliefs
that we all treasure.” Of the 41,900 people in the treatment group, 13% opened at least
one of the emails. The partisan reminders, however, had no effect on voter turnout.
Among the 41,900 people in the treatment group, the turnout rate was 58.7%. Among
the 10,513 in the control group, turnout was 59.7%. When Stollwerk replicated this
study in the days leading up to the 2013 mayoral election, she found positive but insigniﬁcant effects (Stollwerk, 2015). Overall, it appears that GOTV email does little to raise
turnout.
Another growing literature evaluates the effects of messages conveyed via social
media, such as Facebook. The most well-known study was conducted by a team of
academics and Facebook researchers. Prior to the 2010 midterm election, Bond et al.
(2012) randomly assigned millions of Facebook users to one of three conditions. The
ﬁrst was a control group that received no encouragement to vote. The second group
received an information treatment that consisted of several elements: users were shown
a banner at the top of their news feed announcing that “Today is Election Day,”
encouraged to indicate whether they voted by clicking an “I Voted” graphic, provided
a link to locate their polling place, and presented with a counter tabulating the cumulative vote count among Facebook users. The third group received a social treatment:
they received the same encouragement as the information group and were also shown
the faces of up to six friends who had voted along with a count of friends who had
voted. Data gleaned from Facebook users’ personal proﬁles allowed the research
team to assess actual turnout for about one in 10 subjects, which nevertheless left
approximately 60,000 subjects in the control and information conditions and millions

415

416

Handbook of Field Experiments

in the social condition. Two key ﬁndings emerged. The ﬁrst is that the information
treatment had precisely zero effect on turnout. This ﬁnding reafﬁrms the ﬁnding
from direct mail experiments suggesting that reminders to vote have little effect on
turnout. The second is that the social treatment increased turnout by 0.39 percentage
points, which is small but statistically signiﬁcant. Turnout in the social group is significantly higher than in the control condition or the information condition. Evidently,
the active ingredient in social condition is the presentation of friends’ turnout, a theme
that foreshadows the results discussed in Section 4.1.1.
The idea of putting an “I-voted” widget on Facebook user’s news feeds is a creative
one, but this intervention is not something that those outside Facebook are at liberty to
do, even for a fee. The fallback position is to buy ads on Facebook. In a pair of large-scale
experiments, Collins et al. (2014) tested whether “Rock The Vote” ads placed in the
news feed in fact raised turnout. In 2012, they assigned approximately 365,000 people
to an untreated control group and another 365,000 to a treatment group that received
encouragements to vote via sidebar ads and in their news feeds (the latter were actually
delivered to 41% of the assigned treatment group). These encouragements showed, for
example, the number of days left before the election and a display of friends who “liked”
this countdown. Because “Rock The Vote” enjoyed a positive image among the subjects
who received its message, having helped many of them register to vote, it was a credible
source of information and encouragement. However, voter turnout records later
revealed that the treatment and control groups voted at identical rates, 56.5%. The
following year, a follow-up experiment using the same design was conducted in 14 states
where November elections were being held. Roughly 46,500 voters were assigned to an
untreated control and a like number to the treatment group exposed to “Rock The
Vote” advertising. This time, a slightly higher proportion of the assigned treatment
group, 54%, received ads embedded in their news feeds. Turnout, however, was
14.6% in the control group and 14.0% in the treatment group. In both elections, Facebook ads proved ineffective on increasing turnout.
The experimental literature on email and social media, while disappointing to those
who hoped that votes could be mobilized on a grand scale at low marginal cost, is theoretically informative. Evidently, a stream of reminders to vote are ineffective, even when
they come from credible sources (e.g., a civic group on whose website one registered to
vote or opted in for voting reminders). There is some evidence that more personalized
peer-to-peer interaction via social media may foster an interest in politics and increase
turnout (Teresi and Michelson, 2015). Further testing is needed to assess whether new
media stimulate turnout to the extent that they mimic direct personal interaction.
3.1.5 Conclusion
We conclude this section by taking note of one of the striking features of the experimental literature on voter turnoutdthe sheer volume of similar studies employing

Field Experiments on Voter Mobilization

each of the modes of contact. To illustrate this, consider the example of GOTV mailings.
Table 3, which is adapted from our recent book reviewing the experimental literature
(Green and Gerber, 2015), collects the results of 85 distinct studies conducted between
1998 and 2014.9 The table reports the average treatment effect for each study, and includes information about the political context (e.g., general election, primary election,
etc.), date, location, and content of the mailing (e.g., mail that supported a candidate
or a cause, mail that employed strategies to exert social pressure).
These studies can be used to explore how treatment effects vary with differences in subject pools, election contexts, messages scripts, and other details of the experiment. Table 4
shows the results of several meta-analyses performed using Table 3 data and presents the
results for the overall effect of mailings and for some subsets of studies formed by partitioning the studies by the message scripts that appeared on the mailing. Pooling all mail studies
together shows that sending a piece of mail to a voter increases the subject’s turnout rate by
about 3/4 of a percentage point. Further, there is some evidence that the content of the
mailings inﬂuences the size of the treatment effect. Messages that exert social pressure
are substantially more effective than the typical nonpartisan GOTV message, for example,
and pooling across the social pressure studies shown in Table 3 produces a treatment effect
estimate of a 2.3 percentage point increase in voting rates. Two things should be kept in
mind when interpreting the results from these meta-analyses. First, there is substantial heterogeneity in both the estimated treatment effects and the experimental conditions within
the groupings used to form the sets of studies included in each of the rows of Table 4. Second, because there are relatively few studies that conduct a “horserace” between messages
in which scripts vary but the other experimental conditions are held constant, it is possible
that some of the observed differences in message effectiveness are due to variation in conditions other than the message, a possibility highlighted by the variability of the treatment
effects across studies that use similar messaging approaches.

4. THE EFFECT OF MESSAGING
We next review studies that assess the effectiveness of alternative messages. The New
Haven study varied message as well as mode of contact. The study tested the effect of
including three different messages based on the calculus of voting and folk theories about
campaign messaging: pictures and text that urged voting on the grounds that it is a civic
duty, that one’s vote might be pivotal in deciding a close race, and that one’s neighborhood beneﬁts from higher turnout and the attention that attracts among elected ofﬁcials.
There were some differences in the estimated effects, but these fell short of statistical
9

These 85 table entries are produced from an analysis of 220 distinct treatment and control comparisons. See Green and
Gerber 2015, Table B-1 for details of how study results were condensed for this table.

417

418

Handbook of Field Experiments

Table 3 Results of direct mailing experiments in the United States from 1998 to 2014
Social
Estimated
pressure (S)
turnout
or gratitude
effect per
Advocacy
(G)
mailer
SE
used
Context
Study

1998G
1999G
1999G
2000G
2002G
2002G
2002M
2002P
2002P
2002S
2003M
2004G
2004G
2004G
2005G
2006G
2006G
2006G
2006G
2006G
2006G
2006G
2006P
2006P
2006P
2006P
2007G
2007G
2007G
2007G
2007G
2008G

Gerber & GreendNew
Haven
Gerber & GreendNew
Haven
Gerber et al.dConnecticut
and New Jersey
GreendNAACP
RamirezdNALEO
WongdLos Angeles county
GillespiedNewark
CardydPennsylvania
GerberdPennsylvania
GillespiedNewark
NivendWest Palm Beach
AnonymousdMinnesota
Matland &
MurraydBrownsville
TrivedidQueens county
AnonymousdVirginia
Barabas et al.dFlorida
Bedolla &
MichelsondAPALC
Bedolla &
MichelsondOCAPICA
Bedolla &
MichelsondPICO
Gray & PotterdFranklin
county
ManndMissouri
AnonymousdMaryland
Bedolla &
MichelsondAPALC
Bedolla &
MichelsondPICO
Gerber et al.dMichigan
Gerber et al.dMichigan
Gerber et al.dMichigan
Gerber et al.dMichigan
ManndKentucky
PanagopoulosdGilroy
PanagopoulosdIowa and
Michigan
Keane &
NickersondColorado

0.51

0.3

0.30

0.18

0.01

0.09

X

0.02
0.05
1.3
1.1
0.23
0.05
1.6
1.42
0.86
2.94

0.46
0.07
1
2.5
0.50
0.31
2
2.07
0.74
1.09

X

1.13
0.2
0.25
1.15

1.67
0.05
0.62
0.53

0.45

0.79

3.17

0.97

2.92

2.73

X

0.06
0.41
0.01

0.04
0.32
0.34

X
X

1.09

0.82

1.8
5.23
1.78
5.15
2.73
0.3
2.20

0.3
0.17
0.87
0.46
0.20
1.4
0.84

0.67

0.29

X
X
X
X

X

S*
S
S*
S
S
S

Field Experiments on Voter Mobilization

Table 3 Results of direct mailing experiments in the United States from 1998 to 2014dcont'd
Social
Estimated
pressure (S)
turnout
or gratitude
effect per
Advocacy
(G)
mailer
SE
used
Context
Study

2008G
2008G
2008G
2008G
2008G
2008G
2008P
2008 PP
2008 PP
2008 PP
2009G
2009G
2009G
2009G
2009S
2009S
2009S
2009S
2010G
2010G
2010G
2010G
2010G
2010G
2010G
2010G
2010M
2010P
2010P
2011G
2011G
2011G
2011G

NickersondAPIA Vote
NickersondFRESC
NickersondLatina Initiative
NickersondNCL
NickersondVoto Latino
Rogers &
MiddletondOregon
EnosdLos Angeles county
Barabas et al.dFlorida
Nickerson & WhitedNorth
Carolina
Nickerson & WhitedNorth
Carolina
Larimer & CondondCedar
falls
ManndHouston
PanagopoulosdNew Jersey
PanagopoulosdNew Jersey
Abrajano &
PanagopoulosdQueens
ManndHouston
PanagopoulosdStaten Island
Sinclair et al.dChicago
AnonymousdNevada
Barton et al.dunknown state
BryantdSan Francisco
Gerber et al.dConnecticut
Gerber et al.dConnecticut
Mann & MayhewdIdaho,
Md., N.C., and Ohio
Murray & MatlanddTexas
and Wisconsin
Murray & MatlanddTexas
and Wisconsin
PanagopoulosdLancaster
Binder et al.dSan
Bernardino county
PanagopoulosdGeorgia
Mann & KalladMaine
PanagopoulosdLexington
Panagopoulos
et al.dHawthorne
Panagopoulos
et al.dHawthorne

1.2
0.2
0.23
1.47
0.59
0.03

0.6
0.7
0.26
0.63
0.33
0.48

2.05
2.73
0.8

1.13
0.62
0.7

0.96

0.26

S

0.74

2.38

S

1.2
2.5
2
1.10

0.6
0.5
0.5
0.40

G
G
S
S

1.1
2
4.4
0.15
2.23
1.75
2.00
0.39
2.00

0.5
0.98
0.6
0.45
1.65
1.99
0.53
0.64
0.42

G
G
S
S

1.75

0.66

1.46

0.66

1.08
0.11

0.96
0.50

2.5
2.40
0.97
0.40

0.5
0.58
0.75
0.71

G

2.17

0.58

S

X

X
X

S

X

Continued

419

420

Handbook of Field Experiments

Table 3 Results of direct mailing experiments in the United States from 1998 to 2014dcont'd
Social
Estimated
pressure (S)
turnout
or gratitude
effect per
Advocacy
(G)
mailer
SE
used
Context
Study

2011M
2011M
2011S
2011S
2012G
2012G
2012G
2012M
2012P
2012P
2012P
2012R
2012R
2013G
2013G

2013M
2014G
2014G
2014P
2014P

PanagopoulosdKey West
PanagopoulosdKey West
ManndNevada
PanagopoulosdCharlestown
Citrin et al.dVirginia and
Tennessee
Doherty &
Adlerdbattleground state
Levine & ManndGeorgia
and Ohio
PanagopoulosdVirginia
Condon et al.dIowa
Condon et al.dIowa
Condon et al.dIowa
Gerber et al.dWisconsin
Rogers et al.dWisconsin
BiggersdVirginia
Matland and
MurraydMinn., Ohio,
Tex., and Va.
Matland & MurraydEl Paso
Broockman &
GreendCalifornia
CubbisondNorth Carolina
Green et al.dTexas
Hill & KousserdCalifornia

1.1
0.05
0.85
0.30
0.74

0.5
0.35
0.28
0.53
0.41

0.05

0.20

0.24

0.30

0.03
2.85
0.4
2.7
1.1
1.05
0.11
0.41

0.62
0.64
0.9
0.9
0.7
0.27
0.18
0.32

0.12
0.35

0.39
0.13

0.12
0.12
0.49

0.07
0.53
0.08

S
G

X
G

S
X

S

X
X
X

Context refers to the election year and type, where G, general; M, municipal; P, primary; PP, presidential primary; R,
runoff; S, special election; SE, standard error. Advocacy refers to appeals that urge support for candidates or causes.
Social pressure refers to appeals that emphasize compliance with the social norm of civic participation. Social pressure
entries marked with an asterisk forcefully assert the norm of voting but do not tell recipients that whether they vote is a
matter of public record. Gratitude mailers thank recipients for voting in a prior election or for their past involvement in
elections.
When a given study involved multiple pieces of mail or varying quantities of mail, regression was used to estimate the
per-mailer turnout effect. The number of signiﬁcant digits in the table may vary depending on how the studies’ authors
reported their results. When a given author or authors report multiple mail tests by the same organization in the same
election, we calculated the overall estimated effect and standard error by taking the precision-weighted average, which
is equivalent to a ﬁxed effects meta-analysis.
APALC, Asian Paciﬁc American Legal Center; APIA, Asian and Paciﬁc Islander American Vote; FRESC, Front
Range Economic Strategy Center; NAACP, National Association for the Advancement of Colored People; NALEO,
National Association of Latino Elected Ofﬁcials; NCL, National Council of La Raza; OCAPICA, Orange County
Asian and Paciﬁc Islander Community Alliance; PICO, People Improving Communities through Organizing.
This table uses information from Green, D.P., Gerber, A.S., 2015. Get Out The Vote: How to Increase Voter Turnout,
third ed. Brookings Institution Press (Table B-1).

Field Experiments on Voter Mobilization

Table 4 Meta-analysis of direct mail experiments in the United States, 1998e2014
Type of direct mail
Estimate
95% Conﬁdent interval
No. of studies

Advocacy (excludes social pressure)
Nonadvocacy (excludes social
pressure)
Social pressure
All

0.010
0.523

(0.101, 0.120)
(0.299, 0.748)

19
51

2.280
0.759

(1.259, 3.301)
(0.530, 0.988)

15
85

Results obtained using the metan command in Stata 12, with the random effects option. Estimates are in percentage
points. Advocacy includes mailings that urge support for ballot issues or candidates. Social pressure refers to mailings
that emphasize compliance with the social norm of civic participation and present recipients with information about
their record of voting in past elections. The nonadvocacy category includes mailings the express gratitude for past
turnout or stress the norm of voting but do not present or refer to past or future turnout records. Excluding these
studies reduces the estimate to 0.366, with a conﬁdence interval ranging from 0.136 to 0.596.

signiﬁcance, and it appeared that messaging effects were, if present, relatively modest. A
large number of subsequent ﬁeld experiments investigated message effects and some approaches, especially those that employ a treatment that is designed to induce social pressure to participate, have shown large and reproducible increases in turnout.
Here we focus on studies in which the messaging is closely related to or explicitly
inspired by leading social psychological theories and for which there is a sufﬁciently large
literature to get a sense for the robustness of the ﬁndings.10 It is useful to compare the
mechanisms that might be at work in these psychological approaches to the more standard elements emphasized in the classic accounts of rational participation.
When voting is analyzed from the standpoint of rational decision theory, an individual votes if pB > C, where p is the probability the vote changes the outcome (one vote
makes or breaks a tie), B is the private beneﬁt to the individual from the preferred candidate winning, and C is the cost of voting. This is the decision theoretic account, since in
this account the “pivot probability” is a belief and there is no effort to justify it as the
endogenous outcome of game among voters. Because the empirical probability of being
pivotal in a large election is miniscule, elections that attract tens of millions of voters
represent an anomaly. As long as there is even a modest amount of noise regarding
turnout, for any symmetric rule mapping voter costs and beneﬁts into voting, as a theoretical matter the chances that the election in a large electorate will be an exact tie (or
within one vote) is essentially zero, which leads to a zero expected return for participation. To account for substantial turnout rates, the basic theory was expanded to include
an explicit term for the beneﬁts from voting: pB þ D > C, where D stands for a sense of
civic duty (Riker and Ordeshook, 1968). Some of the messaging strategies can be

10

Rogers et al. (2013) provide an argument for why voter mobilization ﬁeld experiments are an excellent environment
to test social psychology theories and provide a description of some early ﬁndings.

421

422

Handbook of Field Experiments

relatively easily incorporated into the standard decision theoretic framework for rational
participation or modest extensions of it. Messages employed might affect citizens’ beliefs
about the components of the formula or the weight should be placed on them.
An alternative source of theoretical inspiration is social psychology, which emphasizes
the ways in which behavior may be induced by raising the salience of certain ideas and
norms. For example, it has been argued that behaviors such as obtaining an immunization
become more likely when people think about how where and when they would be
immunized (Milkman et al., 2011). As described below, similar approaches have been
used to mobilize voters. One might express this hypothesis using the language of
pB þ D > C by arguing that rehearsing the steps by which one will cast a ballot reduces
the cognitive costs encompassed by C. Similarly, it may be argued that one of the beneﬁts
of voting (D) is that it raises one’s esteem in the eyes of others, who look down on those
who do not perform this civic obligation. We next consider experimental tests of these
propositions.

4.1 Self-prophecy and implementation intentions
Both the theory of “self-prophecy” (Greenwald et al., 1987) and the theory of “implementation intentions” (Gollwitzer, 1999) hypothesize that the trajectory of an individual’s
behavior can be altered by inducing the individual to state that he or she will take a certain
action. We will discuss each of these theories and their application to voter turnout.
The notion of self-prophecy is inspired by the idea that some kinds of prediction errors may be self-correcting (Sherman, 1980). There are many things that a person feels he
or she ought to do but, for some reason, the individual’s actions do not match his or her
putative goals. Contributing something to charity, getting more exercise in the coming
year, and voting in the next election would be examples of such aspirations. When asked
to predict whether they expect to undertake the desirable action, people frequently say
they will. According to Sherman (1980) and subsequent authors, inducing individuals to
predict their behavior produces “self-erasing error” or an example of “self-prophecy,” as
the prediction itself induces a sense of obligation to follow through, which then leads to a
higher level of adherence to the predicted course of action. Applying this argument to
voter mobilization suggests that merely by asking individuals if they expected to vote,
a question that is overwhelmingly answered in the afﬁrmative, one can raise turnout.11
The “self-prophecy effect” was ﬁrst applied to voting behavior by Greenwald and
colleagues. Prior to the 1984 presidential election, several dozen college students were
phoned and asked some questions about the upcoming election. They found that the
11

It is possible that such a question could also serve as a reminder to vote, but, as noted in our earlier discussion of
reminder phone calls and email, there is ample evidence that reminding people that an election is coming has
negligible effects on turnout. Simple reminders are often used as the placebo condition in messaging studies involving
direct mail.

Field Experiments on Voter Mobilization

incremental effect adding an item that asked subjects to predict their participation was a
stunning 23 percentage point increase in the voting rate (Greenwald et al., 1987).12 Subsequent studies were much less supportive. When the same setup was repeated by the
original authors in a 1986 senate election and a 1987 state primary, they found no effect.
Studies of self-prophecy by other scholars have found treatment effects similar to those
produced by a typical commercial turnout phone call (on the order of a 1% turnout increase). In a replication study approximately 10 times the size of the original Greenwald
study, Smith et al. (2003) organized a phone bank to call registered voters in advance of
the 2000 presidential primary. They compared the turnout of subjects asked if they knew
where and when to vote with those asked these questions and whether they expected to
vote on Tuesday; the incremental effect of the self-prophecy treatment was 0.1 percentage points. Dustin Cho (2008) replicated this experiment at a larger scale during
the 2008 presidential primary and found a 2.2 percentage point turnout increase from
the self-prophecy treatment. A large study by Nickerson and Rogers (2010), also conducted during the 2008 presidential primary, found a 2 percentage point effect. Although
the effect of self-prophecy each of these three follow up studies was not signiﬁcant, pooling these ﬁndings together suggests that self-prophecy might produce a small boost in
turnout, although nothing close to the ﬁnding reported in the sentinel study.
In addition to the studies that directly test self-prophecy, several studies provide indirect evidence about self-prophecy’s effectiveness. In some studies the question about
vote prediction is just one component of the treatment. Christopher Mann (2005) studied the effect of being administered a multiquestion preelection survey that included
questions about turnout intention. He found that those registered voters assigned to
be asked about their vote intentions and other political attitudes by major news media
pollsters voted at the same rate as the randomly selected control group who were not
called for the survey. A recent study by Green et al. (2015) found that canvassing door
to door with a script that merely asked residents how they intend to vote produced
no increase in turnout. Commercial phone banks often conclude their GOTV appeals
with the query “can I count on you to vote?” As noted above, the overall turnout effect
of these calls is small, typically ﬁnding less than a 1 percentage point increase in turnout.
A theory closely related to self-prophecy is the “implementation-intentions” hypothesis, which posits that there is a weak but consequential cognitive barrier between an individual’s goals and taking the actions needed to accomplish those goals. According to
this theory, getting a person to state the goal and then elaborate the steps necessary to
achieve the goal makes accomplishing the goal more likely. The exercise of elaboration
makes the steps required more salient and illuminates the path for successful goal-oriented

12

There are some studies of self-prophecy in other domains. For example, Morwitz et al. (1993) detect an effect of
asking people about their plans to buy a car on subsequent car purchases.

423

424

Handbook of Field Experiments

action. A messaging strategy based on this theory has been applied to voter turnout by
supplementing the self-prophecy item (do you expect to vote?) with follow-up questions
about what subjects need to do to achieve their (now stated) intention to vote.
An early effort to test implementation intentions in a ﬁeld setting was Nickerson and
Rogers (2010). Their study asked subjects if they intended to vote and then, for a subset
of those subjects, proceeded to a series of questions about actions related to casting a ballot. Those who stated they planned to vote were asked: Around what time do you expect
you will head to the polls on Tuesday? Where do you expect to be coming from when
you head to the polls on Tuesday? What do you think you will be doing before you head
out to the polls?13 Nickerson and Rogers report that the implementation intentions script
(which combines a standard GOTV message, an inquiry about intention to vote, and the
implementation intentions questions) boosted turnout by 4.1 percentage points, and that
the incremental effect of the three questions implementation battery was 2.1 percentage
points.14
Several other studies have investigated the effect of elaborating a voting plan. These
include Dustin Cho (2008) and Gerber et al. (2015), who found an implementation intentions phone call script to have negligible effects, and Rogers and Ternovski (2015),
who tested a version of implementation intentions using a mailing and found a statistically
signiﬁcant 0.5 percentage-point effect from a single mailer.15 Overall, it appears that
scripts that evoke self-prophecy and implementation intentions may nudge turnout upward, but the effects tend to be much smaller than suggested by the sentinel studies.
4.1.1 Social pressure, pride, and shame
If we restrict ourselves to a theoretical model that focuses exclusively on the pivot probability, the beneﬁts from being decisive, and the costs of participation, it is impossible to
produce a robust explanation for the observed high levels of turnout in mass elections.
One response to this gap between prediction and model is to extend the set of considerations used by the voter to evaluate whether to vote. An example of this approach is
Coate and Conlin (2004), in which two groups of strategic voters incorporate a group
identity and then adhere (for unmodeled psychological or social reasons) to a behavioral
rule that maximizes group welfare. The particular voting rule (the cut point for the cost of
13
14

15

Slightly over 85% of subjects said that they planned to vote in the upcoming election.
Further exploration of treatment effect by Nickerson and Rogers revealed that, unexpectedly, the effect of
implementation was concentrated among those who resided in households with one eligible voter, for whom the
overall effect of the implementation script was 9.1 percentage points and the incremental effect of the implementation questions in particular was 8 percentage points. The authors speculated that this ﬁnding was consistent
with the idea that implementation planning was unnecessary for those living in multiple voter households because,
due to the relative centrality of politics in these households and other aspects of the multiresident social context, these
individuals were more likely to already have a voting plan.
The Rogers and Ternovski mailing also included a “gratitude treatment,” a message strategy described below.

Field Experiments on Voter Mobilization

voting that separates voters from nonvoters) that is considered normatively desirable for
each group emerges as an equilibrium outcome. How these implied norms of proper
voter behavior for members of each groupdrules that make sense for the group but
are not rational for the individualdare enforced is either a psychological or social matter.
A complementary but alternative strategy has been to examine the norms that support
voting directly and study how these norms are enforced. Survey evidence indicates that
voting behavior appears to be embedded in a set of social norms that support voting. Simple reﬂection suggests that there is some social dimension to voting, but how important is
this consideration? It may be that people merely pay lip service to ideas of voting and civic
duty, but perhaps the norms regarding voting are more deeply rooted. If individuals are
susceptible to feelings of pride and shame regarding their voting behavior, interventions
that heighten these feelings may affect turnout. Conversely, if heightening social pressure
leads to a large change in turnout, this lends plausibility to social pressure as a mechanism
that is working to produce the observed levels of mass participation.
Experiments have explored the effectiveness of “social pressure,” that is, strategies crafted
to tap into the basic human drive to win praise or avoid scolding. Social pressure is exerted by
praising those who uphold a social norm or by chastising those who violate them. The level
of social pressure exerted can be varied through variation in the intensity of the message or
through disclosure of the individual’s level of compliance with the norm. In the voter mobilization literature, social pressure messages typically involve three components: exhorting the
receiver to comply with the social norm, stating that the receiver’s behavior will be monitored, and warning that the receiver’s compliance may be disclosed to others.
In a large experiment conducted in a primary election in Michigan in 2006, Gerber
et al. (2008) evaluated a set of four mailers that conveyed varying doses of social pressure.
The ﬁrst mailer employed a hectoring tone to encourage citizens to do their civic duty
and vote. The second mailer added to this message an element of surveillance by telling
people that they were part of an academic study and that their turnout in the upcoming
election would be monitored. The third mailing, labeled the “self” mailer, included information from the voter ﬁles listing the voting behavior of household members in recent
elections and contained a promise to send an updated mailing after the election reporting
whether the listed individuals voted or not. Finally, the “Neighbors” mailing increased
the social pressure by including the turnout history of the household as well as that of
the neighbors on the recipient’s block. Thus, the four mailings represented a (steep)
gradation in social pressure.
The results show a very strong effect of social pressure on voter turnout. Bear in mind
that a typical nonpartisan GOTV mailing raises turnout by half a percentage point. The
ﬁrst mailing, a forceful civic duty appeal, raised turnout by 1.8 percentage points, while
the “self” mailing raised turnout by 4.9 percentage points. The “neighbors” mailing
produced a remarkable 8.1 percentage point boost in turnout. These quantities are all

425

426

Handbook of Field Experiments

distinguishable from zero and each other, since the treatment groups each comprised
20,000 households and the control group comprised 100,000 households.
Follow up studies have conﬁrmed the basic contours of these results.16 The main
effort has been to replicate and extend the “self” mailing. The social pressure mailings,
especially those that confronted voters with their voting record and those of their neighbors (the highly effective “Neighbors” mailing), provoked outrage among some recipients (Murray and Matland, 2014), prompting a search for messaging strategies that
produced the turnout effect without as much agitation. These efforts led to a few
different approaches. First, there was an attempt to build on the “self” treatment, a strong
message that produced an outsized increase in vote but only a modest level of resistance.
Table 5 shows the results of several studies testing messages employing the “self”
approach. Pooling the results of these studies, which were conducted across a variety
of political contexts, shows that the “self” mailing is a powerful treatment, with especially
strong effects in low-to-medium salience elections. The results across these studies are
similar to the 16% boost (þ4.9 from a base of 29.7 in the control group) observed in
the 2006 Michigan primary election study. In addition to the higher base rate of voting
in a general election, the cases in which the mailing had weaker effects may also be related
to the mechanism of the social pressure treatment; the Texas and Wisconsin studies by
Matland and Murray used a version of the mailer that did not scold voters for failing
to vote.17
A second strategy is to employ social norms to praise rather than scold. Panagopoulos
(2013) used this approach and encouraged subjects to join an “honor roll” of perfect
voters. A collection of voters identiﬁed as African American, Hispanic, or unmarried
women were randomly assigned to receive a mailing that presented the perfect voting
history of 10 neighbors. The text included this language:
There is no action more important to our democracy than going to the polls to vote. That’s why Our
Community Votes, a non-proﬁt organization that encourages voting, is recognizing citizens in your
neighborhood who have perfect voting records in general elections over the past four years.
These neighbors deserve our recognition and congratulations for doing their civic duty and making

16

17

Although the neighbors’ mailing has been used from time to time in campaigns, to date, there is only one academic
follow-up to the Michigan neighbors’ mailing. In the very high turnout of Wisconsin governor election, a neighbors’
mailing produced a one-percentage point increase overall and a 3 percentage point increase among those whose base
turnout rate was 30%, a subgroup with participation levels more similar to the Michigan subjects than the overall
Wisconsin subject pool (Rogers et al. 2015).
No turnout gains were produced by a partisan version of the self mailing, in which subjects were presented with their
turnout record and told that it was important for Democrats and Independents to vote because of the negative effects
of having Republicans in power (Schwenzfeier, 2014). This may be related to the mechanism thought to be at work.
A partisan message is typical in politics and does not cause the subject to reﬂect on civic duty and the social implications of participation. Nonpartisan mailers that threaten to shame/praise nonvoters/voters by putting their
names in a local newspaper seem to produce large effects, although these experiments are somewhat underpowered
(Panagopoulos, 2010).

Field Experiments on Voter Mobilization

Table 5 The effects of the self mailer on voter turnout across multiple studies

Study

Election type

Setting

Control

Self

Percentage
increase
in turnout

1b

2006 August
primary
2007 Municipal
2007 Gubernatorial
general (previous
nonvoters)
2007 Gubernatorial
general (previous
voters)
2009 Municipal
special
2010 General
2010 General
2011 Municipal

Michigan

29.7 (191,243)

34.5 (38,218)

16%a

Michigan
Kentucky

27.7 (772,479)
6.8 (19,561)

32.4 (27,609)
8.9 (13,689)

17%a
31%a

Kentucky

13.2 (25,037)

16.3 (17,731)

23%a

New York
city
Texas
Wisconsin
California

3.2 (3445)

4.2 (3486)

36%a

40.5 (63,531)
49.0 (43,797)
10.6 (13,482)

43.1 (1200)
50.8 (801)
12.0 (1000)

6%
4%
13%

2c
3d
3d
4e
5f
5f
6g

Statistically signiﬁcant at p < .01, one-tailed test. This Table is adapted from Table 11-1, Get out the Vote, Green and
Gerber (2015).
Gerber, A.S., Green, D.P., Larimer, C.W., 2008. Social pressure and voter turnout: evidence from a large-scale ﬁeld
experiment. Am. Polit. Sci. Rev. 102 (1), 33e48.
c
Gerber, A.S., Green, D.P., Larimer, C.W., 2010. An experiment testing the relative effectiveness of encouraging voter
participation by inducing feelings of pride or shame. Polit. Behav. 32, 409e422.
d
Mann, C.B., 2010. Is there backlash to social pressure? A large-scale ﬁeld experiment on voter mobilization. Polit.
Behav. 32, 387e407.
e
Abrajano, M., Panagopoulos, C., July 2011. Does language matter? The impact of Spanish versus English-language
GOTV efforts on latino turnout. Am. Polit. Res. 39, 643e663.
f
Murray, G.R., Matland, R.E., 2014. Mobilization effects using mail: social pressure, descriptive norms, and timing.
Polit. Res. Q. 67, 304e319. The table reports only the results of the Self mailer with no additional information about
the voting rate of the community.
g
Panagopoulos, C., Larimer, C.W., Condon, M., 2014. Social pressure, descriptive norms, and voter mobilization.
Polit. Behav. 36, 451e469.
a

b

their voices heard. And with New Jersey’s election for governor taking place on November 3rd, we
hope that you will go to the polls and join your neighborhood’s Civic Honor Roll of perfect voters.
Voting records show that you voted in the presidential election of 2008 but not in the 2005 election
for governor. Voting records are public information, so people know when you voted, but never how
you voted. By voting on November 3rd, you will join the following voters as perfect voters.
Panagopoulos (2013, p. 275)

This approach, an extensive modiﬁcation of the self-message, raised turnout signiﬁcantly, albeit less than the original self-mailer; turnout rose by 2.3 percentage points
among African American and Hispanic subjects and by 1.3 percentage points among
women.

427

428

Handbook of Field Experiments

A third variation includes language that hints at the possibility that the subjects might
be contacted after the election and asked to explain their participation or failure to participate. An example of this approach is Rogers and Ternovski’s (2015) large-scale study of
turnout on the 2010 midterm election, which included a box in the corner stating that
“You may be called after the election to discuss your experience at the polls.” They ﬁnd
that the incremental effect of adding this to the mailing was a statistically signiﬁcant
quarter-percentage point increase in turnout.18
4.1.2 Gratitude
Gratitude is thought by some to have evolutionary roots and to have developed in a
manner to facilitate social exchange and reciprocity (Trivers, 1971). Drawing on the
extensive and growing literature on the power of gratitude and the reciprocity caused
by expressions of gratitude (McCullough et al. (2008), Bernstein and Simmons (1974),
Clark et al. (1988), Rind and Bordia (1995)), Panagopolous (2011) proposed a voter
mobilization message in which the subject is thanked for prior participation. Part of
the motivation for the gratitude mailing was to explore a method of making the self
mailing, which has the subject’s vote history as a centerpiece, more palatable. Thanking
the voter provides an explanation for why the subject’s vote history has been looked up
and presented.
Panagopolous tested the gratitude mailing in three very different elections: a 2009
special election in Staten Island, New York, the November 2009 Governor’s Election
in New Jersey, and a 2010 Georgia Primary. He found sizeable effects for the gratitude
mailings, with a turnout boost of 2.4 percentage points in Staten Island, a 2.5 percentage
point increase in New Jersey, and a 2.4% increase in Georgia. The effects of the gratitude
mailing were approximately two-thirds as large as the self mailer. An unexpected feature
of this trio of studies emerged in the Georgia study, where Panagopolous included two
additional treatment arms: (1) a mailing in which the vote history was discussed but there
was no mention of ofﬁcial records of voter turnout and (2) a mailing that included just a
generic expression of gratitude for the subjects attention to politics, but did not mention
anything about the individual or their voting record. The key portion of the basic gratitude message was:
THANK YOU FOR VOTING!
We realize voting takes time and effort.
Ofﬁcial voter records indicate that you voted in the last midterm election in November 2006, and
we just wanted to say “thank you.”

18

Another ﬁeld experiment that reports the results of an intervention that includes a similar message (“researchers will
contact you within three weeks of the Election to conduct a survey on your voter participation”) is DellaVigna et al.
(2014).

Field Experiments on Voter Mobilization

Our democracy depends on people like you exercising their right to vote. We appreciate the fact
that you made it a priority to cast a ballot.
We also remind you that the primary elections in Georgia will take place on
Tuesday, July 20, 2010. You are eligible to vote.

The version that makes no mention of ofﬁcial records is identical except the sentence
about ofﬁcial records is excluded. The text for the generic gratitude treatment is:
THANK YOU!
Our democracy depends on people like you paying attention to politics and getting involved in
the political process. We appreciate the fact that you make this a priority.
We also remind you that the primary elections in Georgia will take place on
Tuesday, July 20, 2010. You are eligible to vote.

These three arms ﬁelded in Georgia were approximately equally effective, producing
turnout increases of over 2 percentage points. Remarkably, the point estimate for the
generic expression of gratitude was a 3.1 percentage point turnout increase, implying
that the gratitude mailer is not simply a veiled self mailer but rather taps into a distinct
set of psychological mechanisms. More research is needed to verify this potentially
important discovery and to assess whether GOTV messages delivered in person or by
phone are enhanced by expressions of gratitude.
4.1.3 Descriptive norms
In contrast to prescriptive norms, which assert that people ought to vote, descriptive
norms center on what others do, with the implication you should do likewise. For
example, the statement “Everyone else is voting, and you should, too” suggests that
you should conform to others’ example, either because others know best or because there
are personal advantages to going along with the crowd. Conversely, a statement of the
form “Turnout is low, so we hope that you will vote” sends a mixed message; voting
is encouraged, but the descriptive norm seems to militate in favor of not voting.
In comparison to the literature on prescriptive norms, the literature on descriptive
norms rests on fewer studies, and the experiments tend to be smaller in size. An early
study by Gerber and Rogers (2009) showed that voting intentions are affected by information about whether turnout is likely to be high or low. Subsequent studies have
gauged the effects of such information on subjects’ actual turnout. Panagopoulos et al.
(2014) presented voters in a 2011 municipal election with either a standard mailer or
a self mailer. Each type of mailer was distributed with different variants. In the high
turnout condition, the mailer included the wording “THE MAJORITY OF YOUR
NEIGHBORS DO THEIR CIVIC DUTY. DO YOURS TOO.” Following this statement, individuals were told “TURNOUT IN YOUR COMMUNITY: 70%” in

429

430

Handbook of Field Experiments

reference to turnout in the 2008 general election. In the low turnout condition, the
wording was reversed: “THE MAJORITY OF YOUR NEIGHBORS DO NOT
DO THEIR CIVIC DUTY. BUT YOU SHOULD DO YOURS.” Following this
statement, individuals were told “TURNOUT IN YOUR COMMUNITY: 35%” in
reference to turnout in the 2006 election. In the self condition, wording with either
the high or low norm boosted turnout slightly but not signiﬁcantly; estimated effects
were essentially zero in the standard condition. Another study by Murray and Matland
(2014) presented parallel experiments conducted in Lubbock, Texas and Kenosha, Wisconsin. Standard or self-mailers sent to subjects in the low descriptive norm condition
included the following passage:
In the Lubbock city elections earlier this year, voter turnout was around 10%, among the lowest
levels recorded in the past twenty years. While there are many opportunities to participate, millions of people in Texas never take advantage of these opportunities. Many experts are discouraged by how few voters they expect for the upcoming election. We encourage you to buck this
trend among your fellow Lubbock citizens and vote on Tuesday, November 2nd.

By contrast, the high descriptive norm language expressed optimism:
In the General Election in Lubbock in 2008, voter turnout was over 70% of registered voters and
among the highest levels recorded in the past twenty years. Throughout the country there has
been a surge in voter participation. Many experts are encouraged by this trend and are expecting
another large turnout in the upcoming election. We encourage you to join your fellow Lubbock
citizens and vote on Tuesday, November 2nd.

Again, the results were ambiguous. In Lubbock, the mailers were equally effective
regardless of whether they conveyed high or low norms or none at all. In Kenosha,
the high-norm language boosted turnout signiﬁcantly, whereas the low-norm language
had no effect. Although larger replication studies are needed in order to estimate these
effects with more precision, it appears that descriptive norms exert weaker effects than
prescriptive norms.
4.1.4 Discussion
Our summary of the literature has highlighted a number of empirical regularities. One is
that encouragements to vote tend to be more effective when delivered in person than via
direct mail or email. Another is that advocacy messages that give voters reasons to support
or oppose a given candidate or cause tend not to increase turnout. Yet another is that
messages that forcefully assert the social norm of civic participation are often highly effective at stimulating turnout, especially in low salience elections.

4.2 Voter mobilization outside the US
Although these conclusions emerge from a robust experimental literature, the studies
described above were all conducted in the context of American elections, which leaves

Field Experiments on Voter Mobilization

open the question of whether the results hold outside the United States. The last decade has
seen a steady increase in the number of GOTV experiments conducted in other countries.
One of the earliest large-scale experiments assessed the effects of nonpartisan phone calls and
canvassing in the United Kingdom (John and Brannan, 2008), and several follow-up studies
have extended this experimental work to nationally representative samples (Fieldhouse
et al., 2013) and to partisan campaigns (Foos and John, 2016). Within the domain of
nonpartisan campaigns, these studies conﬁrm the effectiveness of personal GOTV tactics
and, if anything, suggest that volunteer phone banks work especially well in the UK,
where landlines are less overburdened by telemarketing calls. On the other hand, impersonal tactics such as direct mail have been found to be effective, too, both in the UK
(Fieldhouse et al., 2013) and Ireland (Regan, 2013), again perhaps due to the lower volume
of commercial junk mail in those countries. Interestingly, partisan canvassing and phone
calls have produced mixed results, with studies in the UK, France, and Spain ﬁnding no
increase in turnout (Foos and John, 2016; Pons, 2014; Ramiro et al., 2012) or heterogeneous effects that are positive only among supporters (Foos and de Rooij, 2013).
Although studies conducted outside the United States have the potential to shed light
on the interaction between interventions and electoral context, the lack of individuallevel administrative data on voter turnout often presents an impediment to ﬁeld experimental research. One response has been to conduct an experiment in a single precinct,
stationing poll workers to observe who votes in that location, as Guan and Green (2006)
did when studying door-to-door canvassing among students in a Chinese university. Occasionally, research collaboration with government agencies gives scholars access to
extraordinarily rich data on both turnout outcomes and the social attributes of the study
participants. For example, in their study of voter mobilization in Denmark, Bhatti et al.
(2016) had access to detailed family data linking parents and offspring, enabling the
research team to assess whether the text messages they sent to young voters affected
turnout among family members and housemates. Another approach is to randomize at
the polling station or city level, a research strategy that has been used in Brazil (De
Figueiredo et al., 2011), Italy (Kendall et al., 2013), and Mexico (Chong et al., 2015).
Although this type of experimental design tends to be less powerful than one based on
individual assignment, it allows the researcher to estimate treatment effects on both
turnout and vote share. Some of the most interesting studies are those that show how
persuasive campaign messages affect vote share even when they do not affect turnout
(Pons and Liegey, 2013), a ﬁnding reminiscent of analogous experiments in high-salience
US elections (Rogers and Middleton, 2015).

4.3 Downstream effects
One of the most interesting ﬁndings to emerge from GOTV research in the US and UK
is that voter mobilization campaigns have enduring effects. The New Haven residents

431

432

Handbook of Field Experiments

who were randomly assigned to receive direct mail or face-to-face canvassing in 1998
were more likely to vote in both the election held in November 1998 and the mayoral
election held in November 1999. This type of persistent effect has since been replicated
repeatedly (Coppock and Green 2016). For example, voters assigned to receive mailings
in the Michigan social pressure experiment not only voted at higher rates in the 2006
August primary; they were also signiﬁcantly more likely to vote in August primaries in
2008, 2010, and 2012. The self mailing generated approximately 1850 votes in August
2006, plus an additional 900 votes over the next three August primaries. This pattern
of over time-persistence holds for other large social pressure studies (Davenport et al.,
2010; Rogers et al., 2015), for nonpartisan efforts to mobilize ethnic minority voters
in California (Bedolla and Michelson, 2012), and for GOTV efforts in Britain (Cutts
et al., 2009).
The enduring impact of voter mobilization is subject to multiple interpretations. One
interpretation is that voting is a habit-forming activity. Someone who votes in this election is more likely to vote in the next election. Someone who skips an election is less
likely to vote in the future. America’s low turnout rates may reﬂect the fact that we
have the most frequent elections on earth. One might liken sleepy municipal elections
to gateway drugs; by enticing so many people to abstain from voting, they weaken voting
habits. Another interpretation is that voting in the initial election attracts the attention of
political campaigns, which direct extra attention to recent voters, thereby promoting
their continuing participation. The one study to track campaign activitydusing contact
records from the campaigns themselvesdfound that those assigned to the treatment
group prior to a spring election were more likely to receive mail but no more likely
to receive phone calls or personal visits prior to the fall general election (Rogers et al.,
2015). Still another interpretation is that mobilization effects endure because subjects
continue to remember the communication that mobilized them initially, a hypothesis
that has some plausibility when the initial mobilization takes the form of strongly worded
social pressure mailers.

4.4 Future directions
Much of the existing experimental work on stimulating voter turnout is inspired by theoretical accounts of why people vote or psychological theories of how individuals might be
persuaded to take an action. These frameworks make directional predictions about the
effects of interventions, but there is rarely any effort to estimate parameters of the subjects’ utility function. An important avenue for future research is to use ﬁeld experiments
to estimate parameters in explicit structural models. An example of this work is Della
Vigna et al. (2014), which incorporates a “social-image” motivation for voting into
the subject’s utility function and designs a set of experiments that identiﬁes the monetary
value of voting in order to avoid having to say you failed to vote (lying is costly). Based on

Field Experiments on Voter Mobilization

their experimental results and some assumptions (including an evidence-based assumed
cost to subjects of lying), Della Vigna et al. estimate that the monetary cost of admitting
failure to vote is between $5 and $15 for the 2010 congressional election, a plausible estimate given the observed level of turnout and the modest time and effort cost of voting.
A noteworthy feature of Della Vigna et al. is that, as a side beneﬁt to their search for
interventions designed to estimate model parameters, the authors conduct novel experiments that are interesting in their own rights. Misreporting of voting is a common source
of measurement error in surveys. Della Vigna et al. examine the effect of providing the
subject an incentive to tell the interviewer she did not vote; a random subset of survey
respondents are told after 2 min of a 10-min survey that if they answer that they did
not vote in the recent congressional election, the survey will end rather than continue
for eight more minutes. Della Vigna et al. ﬁnd that providing the incentive to say you
did not vote has a small, statistically insigniﬁcant effect on reported voting by respondents
who had voted, but nonvoters are substantially more likely to admit (that is, report truthfully) having not voted. Thus, very little misreporting of turnout is induced by the incentive, while the net degree of misreporting is substantially reduced in the incentive
condition.
Another path for further research is to explicitly consider the implications of the
accumulating corpus of academic work on voter mobilization for the “applied work”
done by campaigns and elections. Most directly, there is the question of how campaign
activity might be optimized given the experimental evidence on the relative effectiveness of alternative communications’ strategies or the differences in treatment response
across individuals. Imai and Strauss (2011), continuing a line of work pioneered by
Kramer (1966), consider the question of crafting the optimal GOTV campaign.
They use data from existing experiments to estimate treatment effect heterogeneity
and then compare the relative effectiveness of strategies that begin by targeting the individuals who are expected to have the largest turnout response versus a strategy that
assumes zero-treatment effect heterogeneity. They ﬁnd that there are often large expected gains from incorporating treatment-effect heterogeneity into the prioritization
of mobilization targets.
A second question (perhaps better labeled a puzzle) regarding the relationship between the experimental work and real world campaign activity is to understand how
accumulating experimental evidence affects industry practice. What accounts for the
continued reliance by candidates and parties on methods with little evidence to support
their use, despite the fact that elections have important stakes and are often sharply
competitive? Some scholars have argued that the continued ubiquity of techniques
experimentally demonstrated to produce small returns for large expenditures (early TV
advertising, for example) stems from the ﬁnancial windfalls the spending produces for
campaign consultants (Sheingate, 2016). This explanation, while somewhat persuasive,
seems at best incomplete, because all marketers, political or otherwise, would be

433

434

Handbook of Field Experiments

interested in selling worthless things at a high price, but this is probably not often a sustainable business model.

REFERENCES
Adams, W.C., Smith, D.J., 1980. Effects of telephone canvassing on turnout and preferences: a ﬁeld
experiment. Public Opin. Q. 44, 389e395.
Angrist, J.D., Imbens, G., Rubin, D.B., 1996. Identiﬁcation of causal effects using instrumental variables. J.
Am. Stat. Assoc. 91, 444e472.
Ansolabehere, S.D., Gerber, A.S., 1994. The mismeasure of campaign spending: evidence from the 1990 US
House elections. J. Polit. 56, 1106e1118.
Ansolabehere, S.D., Iyengar, S., 1996. Going Negative: How Political Advertising Divides and Shrinks the
American Electorate. The Free Press, New York.
Arceneaux, K., 2005. Using cluster randomized ﬁeld experiments to study voting behavior. Ann. Am. Acad.
Polit. Soc. Sci. 601 (1), 169e179.
Arceneaux, K., Gerber, A.S., Green, D.P., 2006. Comparing experimental and matching methods using a
large-scale voter mobilization experiment. Polit. Anal. 14, 1e36.
Arceneaux, K., Nickerson, D., 2009. Who is mobilized to vote? A re-analysis of eleven randomized ﬁeld
experiments. Am. J. Polit. Sci. 53, 1e16.
Arceneaux, K., Gerber, A.S., Green, D.P., 2010. A cautionary note on the use of matching to estimate causal
effects: an empirical example comparing matching estimates to an experimental benchmark. Sociol.
Methods Res. 39, 256e282.
Barabas, J., Barrilleaux, C., Scheller, D., 2010. Ballot Initiative Knowledge and Voter Turnout: Evidence
From Field Experiments and National Surveys. Florida State University (unpublished manuscript).
Barton, J., Castillo, M., Petrie, R., 2012. Going Negative: The Persuasive Effect of Tone and Information on
Campaign Fundraising and Voter Turnout. No. 1037 (unpublised manuscript).
Bedolla, L.G., Michelson, M.R., 2012. Mobilizing Inclusion: Transforming the Electorate Through GetOut-the-Vote Campaigns. Yale University Press.
Bennion, E.A., 2005. Caught in the ground wars: mobilizing voters during a competitive congressional
campaign. Ann. Am. Acad. Polit. Soc. Sci. 601 (1), 123e141.
Bergan, D.E., 2009. Does grassroots lobbying work?: A ﬁeld experiment measuring the effects of an e-mail
lobbying campaign on legislative behavior. Am. Polit. Res. 37, 327e352.
Bernstein, D.M., Simmons, R.G., 1974. The adolescent kidney donor: the right to give. Am. J. Psychiatry
131.
Bhatti, et al., 2016. http://cvap.polsci.ku.dk/publikationer/arbejdspapirer/2015/SMS_spillover.pdf.
Bond, R.M., Fariss, C.J., Jones, J.J., Kramer, A.D.I., Marlow, C., Settle, J.E., Fowler, J.H., 2012. A 61million-person experiment in social inﬂuence and political mobilization. Nature 489 (7415),
295e298.
Broockman, D.E., 2013. Black politicians are more intrinsically motivated to advance blacks’ interests: a ﬁeld
experiment manipulating political incentives. Am. J. Polit. Sci. 57 (3), 521e536.
Broockman, D.E., 2014. Mobilizing candidates: political actors strategically shape the candidate pool with
personal appeals. J. Exp. Polit. Sci. 1 (2), 104e119.
Broockman, D.E., Butler, D.M., 2015. The causal effects of elite position-taking on voter attitudes: ﬁeld
experiments with elite communication. Am. J. Polit. Sci. http://onlinelibrary.wiley.com/doi/10.
1111/ajps.12243/epdf.
Butler, D.M., Nickerson, D.W., 2011. Can learning constituency opinion affect how legislators vote? Results from a ﬁeld experiment. Q. J. Polit. Sci. 6 (1), 55e83. http://dx.doi.org/10.1561/100.00011019.
Cardy, E.A., 2005. An experimental ﬁeld study of the GOTV and persuasion effects of partisan direct mail
and phone calls. Ann. Am. Acad. Polit. Soc. Sci. 601 (1), 28e40.
Cho, D., 2008. Acting on the Intent to Vote: A Voter Turnout Experiment. Available at: SSRN 1402025.
Yale University (unpublished manuscript).

Field Experiments on Voter Mobilization

Cho, W.K.T., Gimpel, J.G., Dyck, J.J., 2006. Residential concentration, political socialization, and voter
turnout. J. Polit. 68 (1), 156e167.
Chong, A., Ana, L., Karlan, D., Wantchekon, L., 2015. Does corruption information inspire the ﬁght or
quash the hope? A ﬁeld experiment in Mexico on voter turnout, choice, and party identiﬁcation. J. Polit.
77 (1), 55e71.
Clark, H.B., Northrop, J.T., Barkshire, C.T., 1988. The effects of contingent thank-you notes on case Managers’visiting residential clients. Educ. Treat. Child. 45e51.
Coate, S., Conlin, M., 2004. A group rule-utilitarian approach to voter turnout: theory and evidence. Am.
Econ. Rev. 94 (5), 1476e1504.
Collins, K., Keane, L., Kalla, J., 2014. Youth voter mobilization through online advertising: evidence from
two GOTV ﬁeld experiments. In: Paper Presented at the Annual Meeting of the American Political Science Association, Washington, DC (unpublished manuscript).
Coppock, A., Green, D.P., 2016. Is voting habit forming? New evidence from experiments and regression
discontinuities. Am. J. Polit. Sci. 60 (4), 1044e1062.
Cubbison, W., 2015. The marginal effects of direct mail on vote choice. In: Paper Presented at the Annual
Meeting of the Midwest Political Science Association, Chicago, IL (unpublished manuscript).
Cutts, D., Fieldhouse, E., John, P., 2009. Is voting habit forming? The longitudinal impact of a GOTV
campaign in the UK. J. Elections Public Opin. Parties 19 (3), 251e263.
Davenport, T.C., Gerber, A.S., Green, D.P., 2010. Field experiments and the study of political behavior. In:
Leighley, J.E. (Ed.), The Oxford Handbook of American Elections and Political Behavior. Oxford
University Press, New York.
De Figueiredo, M.F.P., Daniel Hidalgo, F., Kasahara, Y., 2011. When Do Voters Punish Corrupt Politicians? Experimental Evidence From Brazil. University of California Berkeley (unpublished manuscript).
Della Vigna, S., List, J.A., Malmendier, U., Rao, G., 2014. Voting to Tell Others. NBER Working Paper
No. 19832 (unpublished manuscript).
Eldersveld, S.J., 1956. Experimental propaganda techniques and voting behavior. Am. Polit. Sci. Rev. 50,
154e165.
Erikson, R.S., Palfrey, T.R., 2000. Equilibria in campaign spending games: theory and data. Am. Polit. Sci.
Rev. 94, 595e609.
Fieldhouse, E., Cutts, D., Widdop, P., John, P., 2013. Do impersonal mobilisation methods work? Evidence
from a nationwide get-out-the-vote experiment in England. Elect. Stud. 32 (1), 113e123.
Foos, F., de Rooij, E., 2013. Does Candidate Party Afﬁliation Affect Turnout? University of Zurich (unpublished manuscript).
Foos, F., John, P., 2016. Parties are no civic charities: voter contact and the changing partisan composition of
the electorate. Polit. Sci. Res. Methods. http://dx.doi.org/10.7910/DVN/EWISS3 (forthcoming).
Gerber, A.S., 1998. Estimating the effect of campaign spending on senate election outcomes using instrumental variables. Am. Polit. Sci. Rev. 92, 401e411.
Gerber, A.S., 2004. Does campaign spending work?: Field experiments provide evidence and suggest new
theory. Am. Behav. Sci. 47, 541e574.
Gerber, A.S., 2011. New directions in the study of voter mobilization: combining psychology and ﬁeld
experimentation. In: Gerken, H.K., Charles, G.U.E., Kang, M.S. (Eds.), Race, Reform. Cambridge
University Press.
Gerber, A.S., Doherty, D., 2009. Can Campaign Effects Be Accurately Measured Using Surveys?: Evidence
From a Field Experiment. Yale University (unpublished manuscript).
Gerber, A.S., Green, D.P., 2000. The effects of canvassing, direct mail, and telephone contact on voter
turnout: a ﬁeld experiment. Am. Polit. Sci. Rev. 94, 653e663.
Gerber, A.S., Green, D.P., 2001. Do phone calls increase voter turnout? A ﬁeld experiment. Public Opin. Q.
65, 75e85.
Gerber, A.S., Green, D.P., September 2005. Do phone calls increase voter turnout? An update (with Green).
Ann. Acad. Polit. Soc. Sci. 601.
Gerber, A.S., Green, D.P., Green, M., 2003. Partisan mail and voter turnout: results from randomized ﬁeld
experiments. Elect. Stud. 22 (4), 563e579.

435

436

Handbook of Field Experiments

Gerber, A.S., Green, D.P., Kaplan, E.H., 2004. The illusion of learning from observational research. In:
Shapiro, I., Smith, R., Massoud, T. (Eds.), Problems and Methods in the Study of Politics. Cambridge
University Press, New York.
Gerber, A.S., Green, D.P., Larimer, C.W., 2008. Social pressure and voter turnout: evidence from a largescale ﬁeld experiment. Am. Polit. Sci. Rev. 102, 33e48.
Gerber, A.S., Green, D.P., Nickerson, D.W., 2001. Testing for publication bias in political science. Polit.
Anal. 9, 385e392.
Gerber, A.S., Hill, S.J., Huber, G.A., 2015. Small cues and large effect: the results from a collection of simultaneous ﬁeld experiments. In: Paper Presented at the Annual Meeting of the Midwest Political Science
Association, Chicago, IL (unpublished manuscript).
Gerber, A.S., Huber, G.A., Fang, A.H., Reardon, C.E., 2016. When Does Increasing Mobilization Effort
Increase Turnout? New Theory and Evidence from a Field Experiment on Reminder Calls. Institution
for Social and Policy Studies, Yale University (unpublished manuscript).
Gerber, A.S., Rogers, T., 2009. Descriptive social norms and motivation to vote: everybody’s voting and so
should you. J. Polit. 71 (01), 178e191.
Gollwitzer, P.M., 1999. Implementation intentions: strong effects of simple plans. Am. Psychol. 54 (7), 493.
Gosnell, H.F., 1927. Getting-Out-the-Vote: An Experiment in the Stimulation of Voting. University of
Chicago Press, Chicago.
Gray, J., Potter, P., 2007. Does signaling matter in elections? Evidence from a ﬁeld experiment. In: Paper
Presented at the Annual Meeting of the American Political Science Association (unpublished
manuscript).
Green, D.P., Gerber, A.S., 2015. Get Out the Vote: How to Increase Voter Turnout. Brookings Institution
Press, Washington, DC.
Green, D.P., Gerber, A.S., Nickerson, D.W., 2003. Getting out the vote in local elections: results from six
door-to-door canvassing experiments. J. Polit. 65 (4), 1083e1096.
Green, D.P., Krasno, J.S., 1988. Salvation for the spendthrift incumbent: reestimating the effects of campaign
spending in house elections. Am. J. Polit. Sci. 32, 884e907.
Green, D.P., Zelizer, A., Kirby, D., 2015. Testing the Effecst of Mail, Phone, and Canvassing Treatments in
Partisan Primary Runoff Elections. Columbia University (unpublished manuscript).
Greenwald, A.G., Carnot, C.G., Beach, R., Young, B., 1987. Increasing voting behavior by asking people if
they expect to vote. J. Appl. Psychol. 72 (2), 315.
Grose, C.R., 2014. Field experimental work on political institutions. Annu. Rev. Polit. Sci. 17.
Guan, M., Green, D.P., 2006. Non-coercive mobilization in state-controlled elections: an experimental
study in Beijing. Comp. Polit. Stud. 39, 1175e1193.
Ha, S.E., Karlan, D.S., 2009. Get-out-the-vote phone calls does quality matter? Am. Polit. Res. 37 (2),
353e369.
Imai, K., Strauss, A., 2011. Estimation of heterogeneous treatment effects from randomized experiments,
with application to the optimal planning of the get-out-the-vote campaign. Polit. Anal. 19, 1e19.
Jacobson, G.C., 1978. The effects of campaign spending in congressional elections. Am. Polit. Sci. Rev. 72,
469e491.
Jacobson, G.C., 1985. Money and votes reconsidered: congressional elections, 1972e1982. Public Choice
47, 7e62.
Jacobson, G.C., 1990. The effects of campaign spending in house elections: new evidence for old arguments.
Am. J. Polit. Sci. 34, 334e362.
Jacobson, G.C., 1998. The Politics of Congressional Elections. Longman, New York.
John, P., Brannan, T., 2008. How different are telephoning and canvassing? Results from a ‘get out the vote’
ﬁeld experiment in the British 2005 general election. Br. J. Polit. Sci. 38, 565e574.
Kalla, Broockman, 2016. http://onlinelibrary.wiley.com/store/10.1111/ajps.12180/asset/ajps12180.pdf;
jsessionid¼822F8F1CAE0F9A97C646C147CD02C675.f03t01?v¼1&t¼itfpo37d&s¼51a51e9cae44
80b5d78bf9141dec01497ef187de.
Kendall, C., Nannicini, T., Trebbi, F., 2013. How Do Voters Respond to Information? Evidence From a
Randomized Campaign. No. w18986. National Bureau of Economic Research (unpublished
manuscript).

Field Experiments on Voter Mobilization

Kramer, G.H., 1966. A decision theoretic analysis of a problem in political campaigning. In: Bernd, J.L. (Ed.),
Mathematical Applications in Political Science, vol. 2. Southern Methodist University Press, Dallas,
Texas, pp. 137e160.
LeVan, C., 2016. The Neighbor Effect: Spillover Effects of an Experimental Intervention to Increase
Turnout Amongst Voters in Low-Income Neighborhoods. University of California, Los Angeles (unpublished manuscript).
Levitt, S.D., 1994. Using repeat challengers to estimate the effect of campaign spending on election outcomes in the US House. J. Polit. Econ. 102, 777e798.
Malhotra, N., Michelson, M.R., Valenzuela, A.A., 2012. Emails from ofﬁcial sources can increase turnout.
Q. J. Polit. Sci. 7 (3), 321e332.
Mann, C.B., 2005. Unintentional voter mobilization: does participation in preelection surveys increase voter
turnout? Ann. Am. Acad. Polit. Soc. Sci. 601 (1), 155e168.
Mann, C., 2008. Field Experimentation in Political Communication for Mobilization (Ph.D. dissertation).
Yale University, Department of Political Science.
Mann, C.B., Klofstad, C.A., 2015. The role of call quality in voter mobilization: implications for electoral
outcomes and experimental design. Polit. Behav. 37 (1), 135e154.
McCullough, M.E., Kimeldorf, M.B., Cohen, A.D., 2008. An adaptation for altruism the social causes, social
effects, and social evolution of gratitude. Curr. Dir. Psychol. Sci. 17 (4), 281e285.
McNulty, J.E., 2005. Phone-based GOTVdWhat’s on the line? Field experiments with varied partisan
components, 2002e2003. Ann. Am. Acad. Polit. Soc. Sci. 601 (1), 41e65.
Michelson, M.R., 2003. Getting out the latino vote: how door-to-door canvassing inﬂuences voter turnout
in rural central California. Polit. Behav. 25, 247e263.
Michelson, M.R., Bedolla, L.G., McConnell, M.A., 2009. Heeding the call: the effect of targeted tworound phonebanks on voter turnout. J. Polit. 71, 1549e1563.
Miller, R.E., Bositis, D.A., Baer, D.L., 1981. Stimulating voter turnout in a primary: ﬁeld experiment with a
precinct committeeman. Int. Polit. Sci. Rev. 2, 445e460.
Milkman, K.L., Beshears, J., Choi, J.J., Laibson, D., Madrian, B.C., 2011. Using implementation intentions
prompts to enhance inﬂuenza vaccination rates. Proc. Natl. Acad. Sci. 108 (26), 10415e10420.
Morwitz, V.G., Johnson, E., Schmittlein, D., 1993. Does measuring intent change behavior? J. Consum.
Res. 46e61.
Murray, G.R., Matland, R.E., 2014. Mobilization effects using mail social pressure, descriptive norms, and
timing. Polit. Res. Q. 67 (2), 304e319.
Nickerson, D.W., 2007. Quality is job one: volunteer and professional phone calls. Am. J. Polit. Sci. 51 (2),
269e282.
Nickerson, D.W., 2008. Is voting contagious? Evidence from two ﬁeld experiments. Am. Polit. Sci. Rev.
102, 49e57.
Nickerson, D.W., Rogers, T., 2010. Do you have a voting plan? Implementation intentions, voter turnout,
and organic plan making. Psychol. Sci. 21 (2), 194e199.
Niven, D., 2006. A ﬁeld experiment on the effects of negative campaign mail on voter turnout in a municipal
election. Polit. Res. Q. 59 (2), 203e210.
Panagopoulos, C., 2008. Partisan and nonpartisan message content and voter mobilization: ﬁeld experimental evidence. Polit. Res. Q. 62.
Panagopoulos, C., 2010. Affect, social pressure and prosocial motivation: ﬁeld experimental evidence
of the mobilizing effects of pride, shame and publicizing voting behavior. Polit. Behav. 32 (3),
369e386.
Panagopoulos, C., 2011. Social pressure, surveillance and community size: evidence from ﬁeld experiments
on voter turnout. Elect. Stud. 30 (2), 353e357.
Panagopoulos, C., 2013. Positive social pressure and prosocial motivation: evidence from a large-scale ﬁeld
experiment on voter mobilization. Polit. Psychol. 34 (2), 265e275.
Panagopoulos, C., 2014. Raising hope: hope inducement and voter turnout. Basic Appl. Soc. Psychol. 36
(6), 494e501.
Panagopoulos, C., Larimer, C.W., Condon, M., 2014. Social pressure, descriptive norms, and voter
mobilization. Polit. Behav. 36 (2), 451e469.

437

438

Handbook of Field Experiments

Pons, V., 2014. Does Door-to-Door Canvassing Affect Vote Shares? Evidence From a Countrywide Field
Experiment in France. Harvard University (unpublished manuscript).
Pons, V., Liegey, G., 2013. Increasing the electoral participation of immigrants. Experimental evidence from
France. Massachusetts Institute of Technology (unpublished manuscript).
Ramirez, R., 2005. Giving voice to Latino voters: a ﬁeld experiment on the effectiveness of a national
nonpartisan mobilization effort. Ann. Am. Acad. Polit. Soc. Sci. 601 (1), 66e84.
Ramiro, L., Morales, L., Jimenez Buedo, M., 2012. Assessing the electoral payoffs of partisan mobilization. A
ﬁeld experimental study of the 2011 Spanish local elections. In: Paper Presented at the Annual Meeting
of the International Political Science Association.
Regan, J., 2013. The Effects of Direct Mail on Voter Turnout: A Randomized Field Experiment. University
of Birmingham Department of Economics (unpublished manuscript).
Riker, W.H., Ordeshook, P.C., 1968. A theory of the calculus of voting. Am. Polit. Sci. Rev. 62 (01),
25e42.
Rind, B., Bordia, P., 1995. Effect of server’s “thank you” and personalization on restaurant tipping. J. Appl.
Soc. Psychol. 25 (9), 745e751.
Rogers, T., Fox, C.R., Gerber, A.S., 2013. Rethinking Why People Vote. In: The Behavioral Foundations
of Public Policy, vol. 91.
Rogers, T., Green, D.P., Ternovski, J., Ferrerosa-Young, C., 2015. Social Pressure and Voting: A Field
Experiment Conducted in a High-Salience Election. Harvard University (unpublished manuscript).
Rogers, T., Middleton, J., 2015. Are ballot initiative outcomes inﬂuenced by the campaigns of independent
groups? A precinct-randomized ﬁeld experiment showing that they are. Polit. Behav. 37 (3), 567e593.
Rogers, T., Ternovski, J., 2015. ‘We May Ask if Your Voted’: Accountability and a Behavior’s Importance
to the Self (unpublished manuscript).
Rosenstone, S.J., Hansen, J.M., 1993. Mobilization, Participation, and Democracy in America. MacMillan,
New York.
Rubin, D.B., 1978. Bayesian inference for causal effects: the role of randomization. Ann. Stat. 6, 34e58.
Schwenzfeier, M., 2014. When Social Pressure Fails: Evidence From Two Direct Mail Experiments. College
of William & Mary Undergraduate Honors Theses. Paper 69 (unpublished manuscript).
Sheingate, A., 2016. The Rise of Political Consulting and the Transformation of American Democracy.
Oxford University Press.
Sherman, S.J., 1980. On the self-erasing nature of errors of prediction. J. Personal. Soc. Psychol. 39 (2), 211.
Smith, J.K., Gerber, A.S., Orlich, A., 2003. Self-prophecy effects and voter turnout: an experimental
replication. Polit. Psychol. 24 (3), 593e604.
Stollwerk, A., 2015. Does Partisan E-mail Affect Voter Turnout? An Examination of Two Field Experiments in New York City. Columbia University, Department of Political Science (unpublished
manuscript).
Teresi, H., Michelson, M.R., 2015. Wired to mobilize: the effect of social networking messages on voter
turnout. Soc. Sci. J. 52 (2), 195e204.
Trivers, R.L., 1971. The evolution of reciprocal altruism. Q. Rev. Biol. 35e57.
Trivedi, N., 2005. The effect of identity-based GOTV direct mail appeals on the turnout of Indian
Americans. Ann. Am. Acad. Polit. Soc. Sci. 601 (1), 115e122.
Vavreck, L., 2007. The exaggerated effects of advertising on turnout: the dangers of self-reports. Q. J. Polit.
Sci. 2, 287e305.
Villa Jr., H., Michelson, M., 2003. Mobilizing the Latino Youth Vote. The Field Experiments Website, No.
00311.
Wong, J.S., 2005. Mobilizing Asian American voters: a ﬁeld experiment. Ann. Am. Acad. Polit. Soc. Sci. 601
(1), 102e114.

