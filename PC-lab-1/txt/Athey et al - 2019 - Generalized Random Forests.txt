The Annals of Statistics
2019, Vol. 47, No. 2, 1148â€“1178
https://doi.org/10.1214/18-AOS1709
Â© Institute of Mathematical Statistics, 2019

GENERALIZED RANDOM FORESTS
B Y S USAN ATHEYâˆ— , J ULIE T IBSHIRANIâ€ 

AND

S TEFAN WAGERâˆ—

Stanford Universityâˆ— and Elasticsearch BVâ€ 
We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001)
5â€“32]) that can be used to fit any quantity of interest identified as the solution
to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby
training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive
weighting function derived from a forest designed to express heterogeneity
in the specified quantity of interest. We propose a flexible, computationally
efficient algorithm for growing generalized random forests, develop a large
sample theory for our method showing that our estimates are consistent and
asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop
new methods for three statistical tasks: nonparametric quantile regression,
conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf
for R and C++, is available from CRAN.

1. Introduction. Random forests, introduced by Breiman (2001), are a
widely used algorithm for statistical learning. Statisticians usually study random forests as a practical method for nonparametric conditional mean estimation: Given a data-generating distribution for (Xi , Yi ) âˆˆ X Ã— R, forests are used
to estimate Î¼(x) = E[Yi | Xi = x]. Several theoretical results are available on the
asymptotic behavior of such forest-based estimates Î¼Ì‚(x), including consistency
[Arlot and Genuer (2014), Biau (2012), Biau, Devroye and Lugosi (2008), Denil,
Matheson and De Freitas (2014), Lin and Jeon (2006), Wager and Walther (2015),
Scornet, Biau and Vert (2015)], second-order asymptotics [Mentch and Hooker
(2016)] and confidence intervals [Wager and Athey (2018)].
This paper extends Breimanâ€™s random forests into a flexible method for estimating any quantity Î¸ (x) identified via local moment conditions. Specifically, given
data (Xi , Oi ) âˆˆ X Ã— O , we seek forest-based estimates of Î¸ (x) defined by a local
estimating equation of the form
(1)





E ÏˆÎ¸ (x),Î½(x) (Oi ) | Xi = x = 0 for all x âˆˆ X ,

Received July 2017; revised April 2018.
MSC2010 subject classifications. 62G05.
Key words and phrases. Asymptotic theory, causal inference, instrumental variable.

1148

GENERALIZED RANDOM FORESTS

1149

where Ïˆ(Â·) is some scoring function and Î½(x) is an optional nuisance parameter. This setup encompasses several key statistical problems. For example,
if we model the distribution of Oi conditionally on Xi as having a density
fÎ¸ (x),Î½(x) (Â·) then, under standard regularity conditions, the moment condition (1)
with ÏˆÎ¸ (x),Î½(x) (O) = âˆ‡ log(fÎ¸ (x),Î½(x) (Oi )) identifies the local maximum likelihood parameters (Î¸ (x), Î½(x)). More generally, we can use moment conditions of
the form (1) to identify conditional means, quantiles, average partial effects, etc.,
and to develop robust regression procedures via Huberization. Our main substantive application of generalized random forests involves heterogeneous treatment
effect estimation with instrumental variables.
Our aim is to build a family of nonparametric estimators that inherit the desirable empirical properties of regression forestsâ€”such as stability, ease of use
and flexible adaptation to different functional forms as in, for example, Biau and
Scornet (2016) or Varian (2014)â€”but can be used in the wide range of statistical
settings characterized by (1) in addition to standard conditional mean estimation.
This paper addresses the resulting conceptual and methodological challenges and
establishes formal asymptotic results.
Regression forests are typically understood as ensemble methods, that is, forest
predictions Î¼Ì‚(x)
are written as the average of B noisy tree-based predictors Î¼Ì‚b (x),
B
âˆ’1
Î¼Ì‚(x) = B
b=1 Î¼Ì‚b (x); and, because individual trees Î¼Ì‚b (x) have low bias but
high variance, such averaging meaningfully stabilizes predictions [BÃ¼hlmann and
Yu (2002), Scornet, Biau and Vert (2015)]. However, noisy solutions to moment
equations as in (1) are generally biased, and averaging would do nothing to alleviate the bias.
To avoid this issue, we cast forests as a type of adaptive locally weighted estimators that first use a forest to calculate a weighted set of neighbors for each
test point x, and then solve a plug-in version of the estimating equation (1) using
these neighbors. Section 2.1 gives a detailed treatment of this perspective. This
locally weighting view of random forests was previously advocated by Hothorn
et al. (2004) in the context of survival analysis and by Meinshausen (2006) for
quantile regression, and also underlies theoretical analyses of regression forests
[e.g., Lin and Jeon (2006)]. For conditional mean estimation, the averaging and
weighting views of forests are equivalent; however, once we move to more general
settings, the weighting-based perspective proves substantially more effective, and
also brings forests closer to the literature on local maximum likelihood estimation
[Fan and Gijbels (1996), Loader (1999), Newey (1994a), Stone (1977), Tibshirani
and Hastie (1987)].
A second challenge in generalizing forest-based methods is that their success
hinges on whether the adaptive neighborhood function obtained via partitioning
adequately captures the heterogeneity in the underlying function Î¸ (x) we want to
estimate. Even within the same class of statistical tasks, different types of questions
can require different neighborhood functions. For example, suppose that two scientists are studying the effects of a new medical treatment: One is looking at how

1150

S. ATHEY, J. TIBSHIRANI AND S. WAGER

the treatment affects long-term survival, and the other at its effect on the length
of hospital stays. It is plausible that the treatment heterogeneity in each setting
would be based on disparate covariates, for example, a patientâ€™s smoking habits
for long-term survival, and the location and size of the hospital for the length of
stay.
Thus, each time we apply random forests to a new scientific task, it is important
to use rules for recursive partitioning that are able to detect and highlight heterogeneity in the signal the researcher is interested in. In prior work, such problemspecific rules have largely been designed on a case by case basis. Although the
CART rules of Breiman et al. (1984) have long been popular for classification
and regression tasks, there has been a steady stream of papers proposing new
splitting rules for other problems, including Athey and Imbens (2016) and Su et
al. (2009) for treatment effect estimation, Beygelzimer and Langford (2009) and
Kallus (2017) for personalized policy allocation and Gordon and Olshen (1985),
LeBlanc and Crowley (1992), Molinaro, Dudoit and van der Laan (2004) as well
as several others for survival analysis. Zeileis, Hothorn and Hornik (2008) propose
a method for constructing a single tree for general maximum likelihood problems,
where splitting is based on hypothesis tests for improvements in goodness-of-fit.
In contrast, we seek a unified, general framework for computationally efficient
problem-specific splitting rules, optimized for the primary objective of capturing heterogeneity in a key parameter of interest. In the spirit of gradient boosting [Friedman (2001)], our recursive partitioning method begins by computing a
linear, gradient-based approximation to the nonlinear estimating equation we are
trying to solve, and then uses this approximation to specify the tree-split point. Algorithmically, our procedure reduces to iteratively applying a labeling step where
we generate pseudo-outcomes by computing gradients using parameters estimated
in the parent node, and a regression step where we pass this labeled data to a standard CART regression routine. Thus, we can make use of pre-existing, optimized
tree software to execute the regression step, and obtain high quality neighborhood
functions while only using computational resources comparable to those required
by standard CART algorithms. In line with this approach, our generalized random
forest software package builds on the carefully optimized ranger implementation of regression forest splitting rules [Wright and Ziegler (2017)].
Moment conditions of the form (1) typically arise in scientific applications
where rigorous statistical inference is required. The bulk of this paper is devoted
to a theoretical analysis of generalized random forests, and to establishing asymptotic consistency and Gaussianity of the resulting estimates Î¸Ì‚ (x). We also develop
methodology for asymptotic confidence intervals. Our analysis is motivated by
classical results for local estimating equations, in particular Newey (1994a), paired
with machinery from Wager and Athey (2018) to address the adaptivity of the random forest weighting function.
The resulting framework presents a flexible method for nonparametric statistical estimation and inference with formal asymptotic guarantees. In this paper,

GENERALIZED RANDOM FORESTS

1151

we develop applications to quantile regression, conditional average partial effect
estimation and heterogeneous treatment effect estimation with instrumental variables; however, there are many other popular statistical models that fit directly into
our framework, including panel regression, Huberized robust regression, models
of consumer choice, etc. In order to fit any of these models with generalized random forests, the analyst simply needs to provide the problem-specific routines to
calculate gradients of the moment conditions evaluated at different observations
in the dataset for the â€œlabelâ€ step of our algorithm. Moreover, we emphasize that
our method is in fact a proper generalization of regression forests: If we apply our
framework to build a forest-based method for local least-squares regression, we
exactly recover a regression forest. A high-performance software implementation
of generalized random forests, grf for R and C++, is available from CRAN.
1.1. Related work. The idea of local maximum likelihood (and local generalized method of moments) estimation has a long history, including Fan, Farmen
and Gijbels (1998), Newey (1994a), Staniswalis (1989), Stone (1977), Tibshirani
and Hastie (1987) and Lewbel (2007). In economics, popular applications of these
techniques include multinomial choice modeling in a panel data setting [e.g.,
HonorÃ© and Kyriazidou (2000)] and instrumental variables regression [e.g., Su,
Murtazashvili and Ullah (2013)]. The core idea is that when estimating parameters at a particular value of covariates, a kernel weighting function is used to place
more weight on nearby observations in the covariate space. A challenge facing this
approach is that if the covariate space has more than two or three dimensions, performance can suffer due to the â€œcurse of dimensionalityâ€ [e.g., Robins and Ritov
(1997)].
Our paper replaces the kernel weighting with forest-based weights, that is,
weights derived from the fraction of trees in which an observation appears in the
same leaf as the target value of the covariate vector. The original random forest algorithm for nonparametric classification and regression was proposed by Breiman
(2001), building on insights from the ensemble learning literature [Amit and Geman (1997), Breiman (1996), Dietterich (2000), Ho (1998)]. The perspective we
take on random forests as a form of adaptive nearest neighbor estimation, however,
most closely builds on the proposals of Hothorn et al. (2004) and Meinshausen
(2006) for forest-based survival analysis and quantile regression. This adaptive
nearest neighborsâ€™ perspective also underlies several statistical analyses of random
forests, including Arlot and Genuer (2014), Biau and Devroye (2010) and Lin and
Jeon (2006).
Our gradient-based splitting scheme draws heavily from a long tradition in the
statistics and econometrics literatures of using gradient-based test statistics to detect change points in likelihood models [Andrews (1993), Hansen (1992), Hjort
and Koning (2002), Nyblom (1989), Zeileis (2005), Zeileis and Hornik (2007),
Ploberger and KrÃ¤mer (1992)]. In particular, Zeileis, Hothorn and Hornik (2008)

1152

S. ATHEY, J. TIBSHIRANI AND S. WAGER

consider the use of such methods for model-based recursive partitioning. Our problem setting differs from the above in that we are not focused on running a hypothesis test, but rather seek an adaptive nearest neighbor weighting that is as sensitive as
possible to heterogeneity in our parameter of interest; we then rely on the random
forest resampling mechanism to achieve statistical stability [Mentch and Hooker
(2016), Scornet, Biau and Vert (2015), Wager and Athey (2018)]. In this sense, our
approach is related to gradient boosting [Friedman (2001)], which uses gradientbased approximations to guide a greedy, nonparametric regression procedure.
Our asymptotic theory relates to an extensive recent literature on the statistics of random forests, most of which focuses on the regression case [Arlot and
Genuer (2014), Biau (2012), Biau, Devroye and Lugosi (2008), Biau and Scornet
(2016), BÃ¼hlmann and Yu (2002), Denil, Matheson and De Freitas (2014), Geurts,
Ernst and Wehenkel (2006), Ishwaran and Kogalur (2010), Lin and Jeon (2006),
Meinshausen (2006), Mentch and Hooker (2016), Scornet, Biau and Vert (2015),
Sexton and Laake (2009), Wager and Athey (2018), Wager and Walther (2015),
Zhu, Zeng and Kosorok (2015)]. Our present paper complements this body of work
by showing how methods developed to study regression forests can also be used
understand estimated solutions to local moment equations obtained via generalized
random forests.
2. Generalized random forests. In standard classification or regression
forests as proposed by Breiman (2001), the prediction for a particular test point x
is determined by averaging predictions across an ensemble of different trees [Amit
and Geman (1997), Breiman (1996), Dietterich (2000), Ho (1998)]. Individual
trees are grown by greedy recursive partitioning, that is, we recursively add axisaligned splits to the tree, where each split it chosen to maximize the improvement
to model fit [Breiman et al. (1984)]. The trees are randomized using bootstrap (or
subsample) aggregation, whereby each tree is grown on a different random subset
of the training data, and random split selection that restricts the variables available
at each step of the algorithm. For an introductory overview of random forests, we
recommend the chapter of Hastie, Tibshirani and Friedman (2009) dedicated to the
method. As discussed below, in generalizing random forests, we preserve several
core elements of Breimanâ€™s forestsâ€”including recursive partitioning, subsampling
and random split selectionâ€”but we abandon the idea that our final estimate is obtained by averaging estimates from each member of an ensemble. Treating forests
as a type of adaptive nearest neighbor estimator is much more amenable to statistical extensions.
2.1. Forest-based local estimation. Suppose that we have n independent and
identically distributed samples, indexed i = 1, . . . , n. For each sample, we have
access to an observable quantity Oi that encodes information relevant to estimating Î¸ (Â·), along with a set of auxiliary covariates Xi . In the case of nonparametric
regression, this observable just consists of an outcome Oi = {Yi } with Yi âˆˆ R;

1153

GENERALIZED RANDOM FORESTS

in general, it may contain richer information. In the case of treatment effect estimation with exogenous treatment assignment, Oi = {Yi , Wi } also includes the
treatment assignment Wi . Given this type of data, our goal is to estimate solutions
to local estimation equations of the form E[ÏˆÎ¸ (x),Î½(x) (Oi ) | Xi = x] = 0 for all
âˆˆ X , where Î¸ (x) is the parameter we care about and Î½(x) is an optional nuisance
parameter.
One approach to estimating such functions Î¸ (x) is to first define some kind of
similarity weights Î±i (x) that measure the relevance of the ith training example to
fitting Î¸ (Â·) at x, and then fit the target of interest via an empirical version of the
estimating equation [Fan, Farmen and Gijbels (1998), Newey (1994a), Staniswalis
(1989), Stone (1977), Tibshirani and Hastie (1987)]:
(2)


 n




Î¸Ì‚ (x), Î½Ì‚(x) âˆˆ argmin  Î±i (x)ÏˆÎ¸,Î½ (Oi )


Î¸,Î½





.

2

i=1

When the
above expression has a unique root, we can simply say that (Î¸Ì‚ (x), Î½Ì‚(x))
n
solves i=1 Î±i (x)ÏˆÎ¸Ì‚ (x),Î½Ì‚(x) (Oi ) = 0. The weights Î±i (x) used to specify the above
solution to the heterogeneous estimating equation are traditionally obtained via a
deterministic kernel function, perhaps with an adaptively chosen bandwidth parameter [Hastie, Tibshirani and Friedman (2009)]. Although methods of the above
kind often work well in low dimensions, they are sensitive to the curse of dimensionality.
Here, we seek to use forest-based algorithms to adaptively learn better, problemspecific, weights Î±i (x) that can be used in conjunction with (2). As in Hothorn et
al. (2004) and Meinshausen (2006), our generalized random forests obtain such
weights by averaging neighborhoods implicitly produced by different trees. First,
we grow a set of B trees indexed by b = 1, . . . , B and, for each such tree, define
Lb (x) as the set of training examples falling in the same â€œleafâ€ as x. The weights
Î±i (x) then capture the frequency with which the ith training example falls into the
same leaf as x:
(3)

Î±bi (x) =

1({Xi âˆˆ Lb (x)})
,
|Lb (x)|

Î±i (x) =

1
B

B

Î±bi (x).
b=1

These weights sum to 1, and define the forest-based adaptive neighborhood of x;
see Figure 1 for an illustration of this weighting function.
There are some subtleties in how the sets Lb (x) are definedâ€”in particular, as
discussed in Section 2.4, our construction will rely on both subsampling and a
specific form of sample-splitting to achieve consistencyâ€”but at a high level the
estimates Î¸Ì‚ (x) produced by a generalized random forests are simply obtained by
solving (2) with weights (3).
Finally, for the special case of regression trees, our weighting-based definition of a random forest is equivalent to the standard â€œaverage of treesâ€ perspective taken in Breiman (2001): If we estimate the conditional mean function

1154

S. ATHEY, J. TIBSHIRANI AND S. WAGER

F IG . 1. Illustration of the random forest weighting function. Each tree starts by giving equal (positive) weight to the training examples in the same leaf as our test point x of interest, and zero weight
to all the other training examples. Then the forest averages all these tree-based weightings, and
effectively measures how often each training example falls into the same leaf as x.

Î¼(x) = E[Yi | Xi = x], as identified in (1) using ÏˆÎ¼(x) (Yi ) = Yi âˆ’ Î¼(x), then we


1 B
see that ni=1 B1  B
b=1 Î±bi (x)(Yi âˆ’ Î¼Ì‚(x)) = 0 if and only if Î¼Ì‚(x) = B
b=1 Î¼Ì‚b (x),
where Î¼Ì‚b (x) = {i:Xi âˆˆLb (x)} Yi /|Lb (x)| is the prediction made by a single CART
regression tree.
2.2. Splitting to maximize heterogeneity. We seek trees that, when combined
into a forest, induce weights Î±i (x) that lead to good estimates of Î¸ (x). The
main difference between random forests relative to other nonparametric regression
techniques is their use of recursive partitioning on subsamples to generate these
weights Î±i (x). Motivated by the empirical success of regression forests across
several application areas, our approach mimics the algorithm of Breiman (2001)
as closely as possible, while tailoring our splitting scheme to focus on heterogeneity in the target functional Î¸ (x).
Just like in Breimanâ€™s forests, our search for good splits proceeds greedily, that
is, we seek splits that immediately improve the quality of the tree fit as much as
possible. Every split starts with a parent node P âŠ† X ; given a sample of data J ,
we define (Î¸Ì‚P , Î½Ì‚P )(J ) as the solution to the estimating equation, as follows (we
suppress dependence on J when unambiguous):
(4)



(Î¸Ì‚P , Î½Ì‚P )(J ) âˆˆ argmin 
Î¸,Î½

{iâˆˆJ :Xi âˆˆP }



ÏˆÎ¸,Î½ (Oi )

2

.

1155

GENERALIZED RANDOM FORESTS

We would like to divide P into two children C1 , C2 âŠ† X using an axis-aligned cut
such as to improve the accuracy of our Î¸ -estimates as much
as possible; formally,

we seek to minimize err(C1 , C2 ) defined as err(C1 , C2 ) = j =1,2 P[X âˆˆ Cj | X âˆˆ
P ]E[(Î¸Ì‚Cj (J ) âˆ’ Î¸ (X))2 | X âˆˆ Cj ], where Î¸Ì‚Cj (J ) are fit over children Cj in analogy to (4), and expectations are taken over both the randomness in Î¸Ì‚Cj (J ) and a
new test point X.
Many standard regression tree implementations, such as CART [Breiman et al.
(1984)], choose their splits by simply minimizing the in-sample prediction error
of the node, which corresponds to err(C1 , C2 ) with plug-in estimators from the
training sample. In the case of estimating the effect of a binary treatment, Athey
and Imbens (2016) study sample-splitting trees, and propose an unbiased, modelfree estimate of err(C1 , C2 ) using an overfitting penalty in the spirit of Mallows
(1973). In our setting, however, this kind of direct loss minimization is not an
option: If Î¸ (x) is only identified through a moment condition, then we do not in
general have access to unbiased, model-free estimates of the criterion err(C1 , C2 ).
To address this issue, we rely on the following more abstract characterization of
our target criterion.
P ROPOSITION 1. Suppose that basic assumptions detailed in Section 3 hold,
and that the parent node P has a radius smaller than r for some value r > 0. We
write nP = |{i âˆˆ J : Xi âˆˆ P }| for the number of observations in the parent and
nCj for the number of observations in each child, and define
(5)





(C1 , C2 ) := nC1 nC2 /n2P Î¸Ì‚C1 (J ) âˆ’ Î¸Ì‚C2 (J ) 2 ,

where Î¸Ì‚C1 and Î¸Ì‚C2 are solutions to the estimating equation computed in the children, following (4). Then, treating the child nodes C1 and C2 as well as the corresponding counts nC1 and nC2 as fixed, and assuming that nC1 , nC2  r âˆ’2 , we have
err(C1 , C2 ) = K(P ) âˆ’ E[(C1 , C2 )] + o(r 2 ) where K(P ) is a deterministic term
that measures the purity of the parent node that does not depend on how the parent
is split, and the o-term incorporates terms that depend on sampling variance.
Motivated by this observation, we consider splits that make the above criterion (5) large. A special case of the above idea also underlies the splitting
rule for treatment effect estimation proposed by Athey and Imbens (2016). At a
high level, we can think of this -criterion as favoring splits that increase the heterogeneity of the in-sample Î¸ -estimates as fast as possible. The dominant bias term
in err(C1 , C2 ) is due to the sampling variance of regression trees, and is the same
term that appears in the analysis of Athey and Imbens (2016). Including this error
term in the splitting criterion may stabilize the construction of the tree, and further it can prevent the splitting criterion from favoring splits that make the model
difficult to estimate.

1156

S. ATHEY, J. TIBSHIRANI AND S. WAGER

2.3. The gradient tree algorithm. The above discussion provides conceptual guidance on how to pick good splits. But actually optimizing the criterion
(C1 , C2 ) over all possible axis-aligned splits while explicitly solving for Î¸Ì‚C1
and Î¸Ì‚C2 in each candidate child using an analogue to (4) may be quite expensive computationally. To avoid this issue, we instead optimize an approximate
criterion (C1 , C2 ) built using gradient-based approximations for Î¸Ì‚C1 and Î¸Ì‚C2 .
For each child C, we use Î¸ÌƒC â‰ˆ Î¸Ì‚C as follows: We first compute AP as any consistent estimate for the gradient of the expectation of the Ïˆ-function, that is,
âˆ‡E[ÏˆÎ¸Ì‚P ,Î½Ì‚P (Oi ) | Xi âˆˆ P ], and then set
(6)

1
Î¾  Aâˆ’1
P ÏˆÎ¸Ì‚P ,Î½Ì‚P (Oi ),
|{i : Xi âˆˆ C}| {i:X âˆˆC}

Î¸ÌƒC = Î¸Ì‚P âˆ’

i

where Î¸Ì‚P and Î½Ì‚P are obtained by solving (4) once in the parent node, and Î¾ is a
vector that picks out the Î¸ -coordinate from the (Î¸, Î½) vector. When the Ïˆ-function
itself is continuously differentiable, we use
(7)

AP =

1
âˆ‡ÏˆÎ¸Ì‚P ,Î½Ì‚P (Oi ),
|{i : Xi âˆˆ P }| {i:X âˆˆP }
i

and the quantity Î¾  Aâˆ’1
P ÏˆÎ¸Ì‚P ,Î½Ì‚p (Oi ) corresponds to the influence function of the ith
observation for computing Î¸Ì‚P in the parent. Cases where Ïˆ is nondifferentiable,
for example, with quantile regression, require more care.
Algorithmically, our recursive partitioning scheme now reduces to alternatively
applying the following two steps. First, in a labeling step, we compute Î¸Ì‚P , Î½Ì‚P ,
and the derivative matrix Aâˆ’1
P on the parent data as in (4), and use them to get
pseudo-outcomes
(8)

Ïi = âˆ’Î¾  Aâˆ’1
P ÏˆÎ¸Ì‚P ,Î½Ì‚P (Oi ) âˆˆ R.

Next, in a regression step, we run a standard CART regression split on the pseudooutcomes Ïi . Specifically, we split P into two axis-aligned children C1 and C2
such as to maximize the criterion
2

(9)

1
(C1 , C2 ) =
|{i : Xi âˆˆ Cj }|
j =1



2

Ïi

.

{i:Xi âˆˆCj }

Once we have executed the regression step, we relabel observations in each child
by solving the estimating equation, and continue on recursively.
For intuition, it is helpful to examine the simplest case of least-squares regression, that is, with ÏˆÎ¸ (x) (Y ) = Y âˆ’ Î¸ (x). Here, the labeling step (8) does not change
anythingâ€”we get Ïi = Yi âˆ’ Yp , where Yp is the mean outcome in the parentâ€”
while the second step maximizing (9) corresponds to the usual way of making
splits as in Breiman (2001). Thus, the special structure of the type of problem we

GENERALIZED RANDOM FORESTS

1157

are trying to solve is encoded in (8), while the second scanning step is a universal
step shared across all different types of forests.
We expect this approach to provide more consistent computational performance
than optimizing (5) at each split directly. When growing a tree, the computation is
typically dominated by the split-selection step, and so it is critical for this step to
be implemented as efficiently as possible [conversely, the labeling step (8) is only
solved once per node, and so is less performance sensitive]. From this perspective,
using a regression splitting criterion as in (9) is very desirable, as it is possible
to evaluate all possible split points along a given feature with only a single pass
over the data in the parent node (by representing the criterion in terms of cumulative sums). In contrast, directly optimizing the original criterion (5) may require
solving intricate optimization problems for each possible candidate split.
This type of gradient-based approximation also underlies other popular statistical algorithms, including gradient boosting [Friedman (2001)] and the modelbased recursive partitioning algorithm of Zeileis, Hothorn and Hornik (2008).
Conceptually, tree splitting bears some connection to change-point detection if we
imagine tree splits as occurring at detected change-points in Î¸ (x); and, from this
perspective, our approach is closely related to standard techniques for momentbased change-point detection [Andrews (1993), Hansen (1992), Hjort and Koning
(2002), Nyblom (1989), Zeileis (2005), Zeileis and Hornik (2007), Ploberger and
KrÃ¤mer (1992)].
In our context, we can verify that the error from using the approximate criterion
(9) instead of the exact -criterion (5) is within the tolerance used to motivate the
-criterion in Proposition 1, thus suggesting that our use of (6) to guide splitting
may not result in too much inefficiency. Note that consistent estimates of AP can in
general be derived directly via, for example, (7), without relying on Proposition 2.
P ROPOSITION 2. Under the conditions of Proposition 1, if |AP âˆ’
âˆ‡E[ÏˆÎ¸Ì‚P ,Î½Ì‚P (Oi ) | Xi âˆˆ P ]| â†’P 0, that is, AP is consistent, then (C1 , C2 )
and (C1 , C2 ) are approximately equivalent, in that (C1 , C2 ) = (C1 , C2 ) +
oP (max{r 2 , 1/nC1 , 1/nC2 }).
2.4. Building a forest with theoretical guarantees. Now, given a practical
splitting scheme for growing individual trees, we want to grow a forest that allows for consistent estimation of Î¸ (x) using (2) paired with the forest weights (3).
We expect each tree to provide small, relevant neighborhoods for x that give us
noisy estimates of Î¸ (x); then we may hope that forest-based aggregation will provide a single larger but still relevant neighborhood for x that yields stable estimates
Î¸Ì‚ (x).
To ensure good statistical behavior, we rely on two conceptual ideas that have
proven to be successful in the literature on forest-based least-squares regression:
Training trees on subsamples of the training data [Mentch and Hooker (2016),

1158

S. ATHEY, J. TIBSHIRANI AND S. WAGER

Algorithm 1 Generalized random forest with honesty and subsampling
All tuning parameters are prespecified, including the number of trees B and the sub-sampling
s rate used in S UBSAMPLE. This function is implemented in the package grf for R and C++.
1: procedure G ENERALIZED R ANDOM F OREST(set of examples S , test point x)
2:
weight vector Î± â† Z EROS(|S |)
3:
for b = 1 to total number of trees B do
4:
set of examples I â† S UBSAMPLE(S , s)
5:
sets of examples J1 , J2 â† S PLIT S AMPLE(I )
6:
tree T â† G RADIENT T REE(J1 , X )
See Algorithm 2.
7:
N â†N EIGHBORS(x, T , J2 )
Returns those elements of J2 that fall
into the same leaf as x in the tree T .
8:
for all example e âˆˆ N do
9:
Î±[e] += 1/|N |
10:
output Î¸Ì‚ (x), the solution to (2) with weights Î±/B
The function Z EROS creates a vector of zeros of length |S |; S UBSAMPLE draws a subsample of
size s from S without replacement; and S PLIT S AMPLE randomly divides a set into two evenlysized, nonoverlapping halves. The step (2) can be solved using any numerical estimator. Our
implementation grf provides an explicit plug-in point where a user can write a solver for (2)
appropriate for their Ïˆ -function. X is the domain of the Xi . In our analysis, we consider a
restricted class of generalized random forests satisfying Specification 1.

Scornet, Biau and Vert (2015), Wager and Athey (2018)], and a subsample splitting technique that we call honesty [Biau (2012), Denil, Matheson and De Freitas
(2014), Wager and Athey (2018)]. Our final algorithm for forest-based solutions
to heterogeneous estimating equations is given as Algorithm 1; we refer to Section 2.4 of Wager and Athey (2018) for a more in-depth discussion of honesty in
the context of forests. As shown in Section 3, assuming regularity conditions, the
estimates Î¸Ì‚ (x) obtained using a generalized random forest as described in Algorithm 1 are consistent for Î¸ (x). Moreover, given appropriate subsampling rates, we
establish asymptotic normality of the resulting forest estimates Î¸Ì‚ (x).
3. Asymptotic analysis. We now turn to a formal characterization of generalized random forests, with the aim of establishing asymptotic Gaussianity of the
Î¸Ì‚(x), and of providing tools for statistical inference about Î¸ (x). We first list assumptions underlying our theoretical results. Throughout, the covariate space and
the parameter space are both subsets of Euclidean space; specifically, X = [0, 1]p
and (Î¸, Î½) âˆˆ B âŠ‚ Rk for some p, k > 0, where B is a compact subset of Rk . Moreover, we assume that the features X have a density that is bounded away from 0
and âˆ; as argued in, for example, Wager and Walther (2015), this is equivalent to
imposing a weak dependence condition on the individual features (Xi )j because
trees and forests are invariant to monotone rescaling of the features. All proofs are
in the supplement [Athey, Tibshirani and Wager (2018)].

GENERALIZED RANDOM FORESTS

1159

Algorithm 2 Gradient tree
Gradient trees are grown as subroutines of a generalized random forest.
1: procedure G RADIENT T REE(set of examples J , domain X )
2:
node P0 â† C REATE N ODE(J , X )
3:
queue Q â† I NITIALIZE Q UEUE(P0 )
4:
while N OT N ULL(node P â† P OP(Q)) do
5:
(Î¸Ì‚P , Î½Ì‚P , AP ) â† S OLVE E STIMATING E QUATION(P )
Computes (4) and (7).
6:
vector RP â† G ET P SEUDO O UTCOMES(Î¸Ì‚P , Î½Ì‚P , AP )
Applies (8) over P .
7:
split â† M AKE C ART S PLIT(P , RP )
Optimizes (9).
8:
if S PLIT S UCCEEDED( ) then
9:
S ET C HILDREN(P , G ET L EFT C HILD( ), G ET R IGHT C HILD( ))
10:
A DD T O Q UEUE(Q, G ET L EFT C HILD( ))
11:
A DD T O Q UEUE(Q, G ET R IGHT C HILD( ))
12:
output tree with root node P0
The function call I NITIALIZE Q UEUE initializes a queue with a single element; P OP returns
and removes the oldest element of a queue Q, unless Q is empty in which case it returns null.
M AKE C ART S PLIT runs a CART split on the pseudo-outcomes, and either returns two child
nodes or a failure message that no legal split is possible.

Some practically interesting cases, such as quantile regression, involve discontinuous score functions Ïˆ, which makes the analysis more intricate. Here, we follow standard practice, and assume that the expected score function,
(10)





MÎ¸,Î½ (x) := E ÏˆÎ¸,Î½ (O) | X = x ,

varies smoothly in the parameters, even though Ïˆ itself may be discontinuous. For
example, with quantile regression ÏˆÎ¸ (Y ) = 1({Y > Î¸ }) âˆ’ (1 âˆ’ q) is discontinuous
in q, but MÎ¸ (x) = P[Y > Î¸ | X = x] âˆ’ (1 âˆ’ q) will be smooth whenever Y | X = x
has a smooth density.
A SSUMPTION 1 (Lipschitz x-signal). For fixed values of (Î¸, Î½), we assume
that MÎ¸,Î½ (x) as defined in (10) is Lipschitz continuous in x.
A SSUMPTION 2 (Smooth identification). When x is fixed, we assume that the
M-function is twice continuously differentiable in (Î¸, Î½) with a uniformly bounded
second derivative, and that V (x) := VÎ¸ (x),Î½(x) (x) is invertible for all x âˆˆ X , with
VÎ¸,Î½ (x) := âˆ‚/âˆ‚(Î¸, Î½)MÎ¸,Î½ (x)|Î¸ (x),Î½(x) .
Our next two assumptions control regularity properties of the Ïˆ-function itself.
Assumption 3 holds trivially when Ïˆ itself is Lipschitz in (Î¸, Î½) (in fact, having
Ïˆ be 0.5-HÃ¶lder would be enough), while Assumption 4 is used to show that a
certain empirical process is Donsker. Examples are given at the end of this section.

1160

S. ATHEY, J. TIBSHIRANI AND S. WAGER

A SSUMPTION 3 (Lipschitz (Î¸, Î½)-variogram). The score functions ÏˆÎ¸,Î½ (Oi )
have a continuous covariance structure. Writing Î³ for the worst-case variogram
and  Â· F for the Frobenius norm, then for some L > 0,
Î³
(11)
Î³

    
Î¸
Î¸

Î½

,

Î½



    
Î¸
Î¸

Î½

,

Î½

 
 Î¸

â‰¤ L 

Î½

âˆ’

  
Î¸ 
Î½ 





for all (Î¸, Î½), Î¸  , Î½  ,
2



 
:= sup Var ÏˆÎ¸,Î½ (Oi ) âˆ’ ÏˆÎ¸  ,Î½  (Oi ) | Xi = x F .
xâˆˆX

A SSUMPTION 4 (Regularity of Ïˆ).
The Ïˆ-functions can be written as
ÏˆÎ¸,Î½ (O) = Î»(Î¸, Î½; Oi ) + Î¶Î¸,Î½ (g(Oi )), such that Î» is Lipschitz-continuous in (Î¸, Î½),
g : {Oi } â†’ R is a univariate summary of Oi , and Î¶Î¸,Î½ : R â†’ R is any family of
monotone and bounded functions.
A SSUMPTION
5 (Existence of solutions). We assume that, for any weights Î±i

with Î±i = 1, the estimating equation (2) returns
a minimizer (Î¸Ì‚ , Î½Ì‚) that at least

approximately solves the estimating equation:  ni=1 Î±i ÏˆÎ¸Ì‚,Î½Ì‚ (Oi )2 â‰¤ C max{Î±i },
for some constant C â‰¥ 0.
All the previous assumptions only deal with local properties of the estimating
equation, and can be used to control the behavior of (Î¸Ì‚ (x), Î½Ì‚(x)) in a small neighborhood of the population parameter value (Î¸ (x), Î½(x)). Now, to make any use of
these assumptions, we first need to verify that (Î¸Ì‚ (x), Î½Ì‚(x)) be consistent. Here,
we use the following assumption to guarantee consistency; this setup is general
enough to cover both instrumental variables regression and quantile regression.
A SSUMPTION 6 (Convexity). The score function ÏˆÎ¸,Î½ (Oi ) is a negative subgradient of a convex function, and the expected score MÎ¸,Î½ (Xi ) is the negative
gradient of a strongly convex function.
Finally, our consistency and Gaussianty results require using some specific settings for the trees from Algorithm 1. In particular, we require that all trees be
honest and regular in the sense of Wager and Athey (2018), as follows. In order
to satisfy the minimum split probability condition below, our implementation relies on the device of Denil, Matheson and De Freitas (2014), whereby the number
splitting variables considered at each step of the algorithm is random; specifically,
we try min{max{Poisson(m), 1}, p} variables at each step, where m > 0 is a tuning
parameter.
S PECIFICATION 1. All trees are symmetric, in that their output is invariant to
permuting the indices of training examples; make balanced splits, in the sense that
every split puts at least a fraction Ï‰ of the observations in the parent node into
each child, for some Ï‰ > 0; and are randomized in such a way that, at every split,

GENERALIZED RANDOM FORESTS

1161

the probability that the tree splits on the j th feature is bounded from below by
some Ï€ > 0. The forest is honest and built via subsampling with subsample size s
satisfying s/n â†’ 0 and s â†’ âˆ, as described in Section 2.4.
For generality, we set up Assumptions 1â€“6 in an abstract way. We end this section by showing that, in the context of our main problems of interest requiring
Assumptions 1â€“6 is not particularly stringent. Further examples that satisfy the
above assumptions will be discussed in Sections 6 and 7.
E XAMPLE 1 (Least squares regression). In the case of least-squares regression, that is, ÏˆÎ¸ (Yi ) = Yi âˆ’ Î¸ , Assumptions 2â€“6 hold immediately from the definition of Ïˆ. In particular, V = 1 in Assumption 2, Î³ (Î¸, Î¸  ) = 0 in Assumption 3,
d
Ïˆ itself is Lipschitz for Assumption 4, and ÏˆÎ¸ (y) = âˆ’ dÎ¸
(y âˆ’ Î¸ )2 /2 for Assumption 6. Meanwhile, Assumption 1 simply means that the conditional mean function
E[Yi | Xi = x] must be Lipschitz in x; this is a standard assumption in the literature
on regression forests.
E XAMPLE 2 (Quantile regression). For quantile regression, we have ÏˆÎ¸ (Yi ) =
q âˆ’ 1({Yi â‰¤ Î¸}) and MÎ¸ (x) = q âˆ’ Fx (Î¸ ), where Fx (Â·) denotes the cumulative distribution function of Yi given Xi = x. Assumption 1 is equivalent to assuming
that the conditional exceedance probabilities P[Yi > y | Xi = x] be Lipschitzcontinuous in x for all y âˆˆ R, while Assumption 2 holds if the conditional density fx (y) has a continuous uniformly bounded first derivative, and is bounded
away from 0 at the quantile of interest y = Fxâˆ’1 (q). Assumption 3 holds if fx (y)
is uniformly bounded from above [specifically, Î³ (Î¸, Î¸  ) â‰¤ maxx {fx (y)}|Î¸ âˆ’ Î¸  |],
Assumption 4 holds because Ïˆ is monotone and Oi = Yi is univariate, Assumpd
tion 5 is immediate, and Assumption 6 holds because âˆ’ dÎ¸
MÎ¸ (x) = fx (Î¸ ) > 0 and
ÏˆÎ¸ (Yi ) is the negative subgradient of a V-shaped function with elbow at Yi .
3.1. A central limit theorem for generalized random forests. Given these assumptions, we are now ready to provide an asymptotic characterization of generalized random forests. In doing so, we note that existing asymptotic analyses of
regression forests, including Mentch and Hooker (2016), Scornet, Biau and Vert
(2015) and Wager and Athey (2018), were built around the fact that regression
forests are averages of regression trees grown over sub-samples, and can thus be
analyzed as U -statistics [Hoeffding (1948)]. Unlike regression forest predictions,
however, the parameter estimates Î¸Ì‚ (x) from generalized random forests are not
averages of estimates made by different trees; instead, we obtain Î¸Ì‚ (x) by solving a
single weighted moment equation as in (2). Thus, existing proof strategies do not
apply in our setting.
We tackle this problem using the method of influence functions as described
by Hampel (1974); in particular, we are motivated by the analysis of Newey

1162

S. ATHEY, J. TIBSHIRANI AND S. WAGER

(1994a). The core idea of these methods is to first derive a sharp, linearized approximation to the local estimator Î¸Ì‚ (x), and then to analyze the linear approximation instead. In our setup, the influence function heuristic motivates a natural approximation Î¸Ìƒ âˆ— (x) to Î¸Ì‚ (x) as follows. Let Ïiâˆ— (x) denote the influence
function of the ith observation with respect to the true parameter value Î¸ (x),
Ïiâˆ— (x) := âˆ’Î¾  V (x)âˆ’1 ÏˆÎ¸ (x),Î½(x) (Oi ). These quantities are closely related to the
pseudo-outcomes (8) used in our gradient tree splitting rule; the main difference
is that, here, the Ïiâˆ— (x) depend on the unknown true parameter values at x and are
thus inaccessible in practice. We use the âˆ—-superscript to remind ourselves of this
fact.
Then, given any set of forest weights Î±i (x) used to define the generalized random forest estimate Î¸Ì‚ (x) by solving (2), we can also define a pseudo-forest
(12)

âˆ—

n

Î¸Ìƒ (x) := Î¸ (x) +

Î±i (x)Ïiâˆ— (x),

i=1

which we will use as an approximation for Î¸Ì‚ (x). We note that, formally, this
pseudo-forest estimate Î¸Ìƒ âˆ— (x) is equivalent to the output of an (infeasible) regression forest with weights Î±i (x) and outcomes Î¸ (x) + Ïiâˆ— (x).
The upshot of this approximation is that, unlike Î¸Ì‚ (x), the pseudo-forest Î¸Ìƒ âˆ— (x) is
a U -statistic. Because Î¸Ìƒ âˆ— (x) is a linear function of the pseudo-outcomes Ïiâˆ— (x), we

âˆ—
can write it as an average of pseudo-tree predictions Î¸Ìƒ âˆ— (x) = B1 B
b=1 Î¸Ìƒb (x) with

n
Î¸Ìƒbâˆ— (x) = i=1 Î±ib (x)(Î¸ (x) + Ïiâˆ— (x)). Then, because each individual pseudo-tree
prediction Î¸Ìƒbâˆ— (x) is trained on a size-s subsample of the training data drawn without
replacement (see Section 2.4), Î¸Ìƒ âˆ— (x) is an infinite-order U -statistic whose order
corresponds to the subsample size, and so the arguments of Mentch and Hooker
(2016) or Wager and Athey (2018) can be used to study the averaged estimator
Î¸Ìƒ âˆ— (x) using results about U -statistics [Efron and Stein (1981), Hoeffding (1948)].
Following this proof strategy, the key difficulty is in showing that our influencebased statistic Î¸Ìƒ âˆ— (x) is in fact a good approximation for Î¸Ì‚ (x). To do so, we start by
establishing consistency of Î¸Ì‚(x) for Î¸ (x) given our assumptions; we note that this
is the only point in the paper where we use the fact that Ïˆ is the negative gradient
of a convex loss as in Assumption 6.
T HEOREM 3. Given Assumptions 1â€“6, estimates (Î¸Ì‚ (x), Î½Ì‚(x)) from a forest
satisfying Specification 1 converge in probability to (Î¸ (x), Î½(x)).
Building on this consistency result, we obtain a coupling of the desired type in
Lemma 4, the main technical contribution of this paper. We note that separating
the analysis of moment estimators into a local approximation argument that hinges
on consistency and a separate result that establishes consistency is standard; see,
for example, Chapter 5.3 of van der Vaart (1998). The remainder of our analysis

1163

GENERALIZED RANDOM FORESTS

assumes that trees are grown on subsamples of size s scaling as s = nÎ² for some
Î²min < Î² < 1, with
(13)







 



Î²min := 1 âˆ’ 1 + Ï€ âˆ’1 log Ï‰âˆ’1 / log (1 âˆ’ Ï‰)âˆ’1

âˆ’1

< Î² < 1,

where Ï€ and Ï‰ are as in Specification 1. This scaling guarantees that the errors of
forests are variance-dominated.
L EMMA 4. Given Assumptions 1â€“5, and a forest trained according to Specification 1 with (13), suppose that the generalized random forest estimator Î¸Ì‚ (x) is
consistent for Î¸ (x). Then Î¸Ì‚(x) and Î¸Ìƒ âˆ— (x) are coupled at the following rate, where
s, Ï€ and Ï‰ are as in Specification 1:


(14)



âˆ’1

 1 


n âˆ—
s
âˆ’ Ï€ log((1âˆ’Ï‰) )
Î¸Ìƒ (x) âˆ’ Î¸Ì‚(x) = OP max s 2 log(Ï‰âˆ’1 ) ,
s
n

6

.

Given this coupling result, it now remains to study the asymptotics of Î¸Ìƒ âˆ— (x). In
doing so, we reiterate that Î¸Ìƒ âˆ— (x) is exactly the output of an infeasible regression
forest trained on outcomes Î¸ (x) + Ïiâˆ— (x). Thus, the results of Wager and Athey
(2018) apply directly to this object, and can be used to establish its Gaussianity.
That we cannot actually compute Î¸Ìƒ âˆ— (x) does not hinder an application of their
results. Pursuing this approach, we find that given (13), Î¸Ìƒ âˆ— (x) and Î¸Ì‚(x) are both
asymptotically normal. By extending the same argument, we could also show that
the nuisance parameter estimates Î½Ì‚(x) are consistent and asymptotically normal;
however, we caution that the tree splits are not necessarily targeted to expressing
heterogeneity in Î½(x), and so the resulting Î½Ì‚(x) may not be particularly accurate
in finite samples.
T HEOREM 5. Suppose Assumptions 1â€“6 and a forest trained according to
Specification 1 with trees are grown on subsamples of size s = nÎ² satisfying (13).
Finally, suppose that Var[Ïiâˆ— (x) | Xi = x] > 0. Then there is a sequence Ïƒn (x) for
which (Î¸Ì‚n (x) âˆ’ Î¸ (x))/Ïƒn (x) â‡’ N (0, 1) and Ïƒn2 (x) = polylog(n/s)âˆ’1 s/n, where
polylog(n/s) is a function that is bounded away from 0 and increases at most
polynomially with the log-inverse sampling ratio log(n/s).
4. Confidence intervals via the delta method. Theorem 5 can also be
used for statistical inference about Î¸ (x). Given any consistent estimator ÏƒÌ‚n (x)/
Ïƒn (x) â†’p 1 of the noise scale of Î¸Ì‚n (x), Theorem 5 can be paired with Slutskyâ€™s
lemma to verify that limnâ†’âˆ E[Î¸ (x) âˆˆ (Î¸Ì‚n (x) Â± âˆ’1 (1 âˆ’ Î±/2)ÏƒÌ‚n (x))] = Î±. Thus,
in order to build asymptotically valid confidence intervals for Î¸ (x) centered on
Î¸Ì‚ (x), it suffices to derive an estimator for Ïƒn (x).
In order to do so, we again leverage coupling with our approximating pseudoforest Î¸Ìƒ âˆ— (x). In particular, the proof of Theorem 5 implies that Var[Î¸Ìƒ âˆ— (x)]/

1164

S. ATHEY, J. TIBSHIRANI AND S. WAGER

Ïƒn2 (x) â†’p 1, and so it again suffices to study Î¸Ìƒ âˆ— (x). Moreover, from the definition of Î¸Ìƒ âˆ— (x), we directly see that




Var Î¸Ìƒ âˆ— (x) = Î¾  V (x)âˆ’1 Hn x; Î¸ (x), Î½(x) V (x)âˆ’1

(15)

n



Î¾,

where Hn (x; Î¸, Î½) = Var i=1 Î±i (x)ÏˆÎ¸,Î½ (Oi ). Thus, we propose building Gaussian confidence intervals using


n (x)âˆ’1  Î¾,
n (x) V
ÏƒÌ‚n2 (x) := Î¾  Vn (x)âˆ’1 H

(16)

n (x) are consistent estimators for the quantities in (15).
where Vn (x) and H
The first quantity V (x) is a problem specific curvature parameter, and is not
directly linked to forest-based methods. It is the same quantity that is needed to estimate variance of classical local maximum likelihood methods following Newey
(1994a); for example, for the instrumental variables problem described in Section 7,


E[Zi Wi | Xi = x] E[Zi | Xi = x]
(17)
V (x) =
,
1
E[Wi | Xi = x]

while for quantile regression, V (x) = fx (Î¸ (x)). In both cases, several different
strategies are available for estimating this term. In the case of instrumental variables forests, we suggest estimating the entries of (17) using (honest and regular)
regression forests.
The more interesting term is the inner variance term Hn (x; Î¸ (x), Î½(x)).
To
n study this quantity, we note that the forest score (Î¸ (x), Î½(x)) =
i=1 Î±i (x)ÏˆÎ¸ (x),Î½(x) (Oi ) is again formally equivalent to the output of a regression forest with weights Î±i (x), this time with effective outcomes ÏˆÎ¸ (x),Î½(x) (Oi ).
A number of proposals have emerged for estimating the variance of a regression
forest, including work by Sexton and Laake (2009), Mentch and Hooker (2016)
and Wager, Hastie and Efron (2014); and, in principle, any of these methods could
be adapted to estimate the variance of . The only difficulty is that  depends
on the true parameter values (Î¸ (x), Î½(x)), and so cannot directly be accessed in
practice. Here, we present results based on a variant of the bootstrap of little bags
algorithm (or noisy bootstrap) proposed by Sexton and Laake (2009). As a side
benefit, we also obtain the first consistency guarantees for this method for any
type of forest, including regression forests.
4.1. Consistency of the bootstrap of little bags. To motivate the bootstrap of
little bags, we first note that building confidence intervals via half-samplingâ€”
whereby we evaluate an estimator on random halves of the training data to estimate
its sampling errorâ€”is closely related to the bootstrap [Efron (1982)] (throughout
this section, we assume that s â‰¤ n/2). In our context, the ideal half-sampling
HS (x) defined as
estimator would be H
n


(18)

n
n/2

âˆ’1









2

H Î¸Ì‚ (x), Î½Ì‚(x) âˆ’  Î¸Ì‚(x), Î½Ì‚(x)

{H:|H|= n2 }

,

GENERALIZED RANDOM FORESTS

1165

where H denotes a version of  computed only using all the possible trees that
only rely on data from the half sample H âŠ‚ {1, . . . , n} (specifically, in terms of
Algorithm 1, we only use trees whose full I -subsample is contained in H). If we
HS (x), results from Efron (1982) suggest that it would be a good
could evaluate H
n
variance estimator for , but doing so is effectively impossible computationally
as it would require growing very many forests.
Following Sexton and Laake (2009), however, we can efficiently approximate
HS (x) at almost no computational cost if we are willing to slightly modify our
H
n
subsampling scheme. To do so, let  â‰¥ 2 denote a little bag size and assume, for
simplicity, that B is an integer multiple of it. Then we grow our forest as follows:
First, draw g = 1, . . . , B/ random half-samples Hg âŠ‚ {1, . . . , n} of size n/2,
and then generate the subsamples Ib used to build the forest in Algorithm 1 such
that Ib âŠ† Hb/ for each b = 1, . . . , B. In other words, we now generate our forest
using little bags of  trees, where all the trees in a given bag only use data from the
same half-sample. Sexton and Laake (2009) discuss optimal choices of  for minimizing Monte Carlo error, and show that they depend on the ratio of the sampling
variance of a single tree to that of the full forest.
HS (x) using a
The upshot of this construction is that we can now identify H
n
simple variance decomposition. Writing b for a version of  computed only
HS (x) can be expressed in terms of the
using the bth tree, we can verify that H
n
â€œbetween groupsâ€ and â€œwithin groupâ€ variance terms,


Ess

1 
b âˆ’ 
 b=1

2 



HS (x) +
=H
n



1
1 
1 
b âˆ’
b
Ess
âˆ’1
 b=1
 b=1

2 

,

where Ess denotes expectations over the subsampling mechanism while holding
the data fixed. We define our feasible boostrap of little bags variance estimator
BLB (x) via a version of the above ANOVA decomposition that uses empirical
H
n
moments and note that, given a large enough number of trees B, this converges to
the ideal half-sampling estimator.
The result below verifies that, under the conditions of Theorem 5, the optiHS (x) with plug-in values for (Î¸Ì‚ (x), Î½Ì‚(x)) as in
mal half-sampling estimator H
n
(18) consistently estimates the sampling variance of (Î¸ (x), Î½(x)). We have alBLB (x) will match
ready seen above that the computationally feasible estimator H
n
HS
 (x) whenever B is large enough and so, given any consistent estimator V
n (x)
H
n
for V (x), we find that the confidence intervals built using (16) will be asymptotically valid.
HS (x) is consistent,
T HEOREM 6. Given the conditions of Theorem 5, H
n
HS (x) âˆ’ Hn (x; Î¸ (x), Î½(x))F /Hn (x; Î¸ (x), Î½(x))F â†’p 0. Moreover, given
H
n
any consistent Vn (x) estimator for V (x) such that V(x) âˆ’ V (x)F â†’p 0, Gaussian confidence intervals built using (16) will asymptotically have nominal coverage.

1166

S. ATHEY, J. TIBSHIRANI AND S. WAGER

One challenge with the empirical moment estimator based on the above is that,
BLB (x) may be negative. In our software,
if B is small, the variance estimates H
n
we avoid this problem by using a Bayesian analysis of variance following, for
HS (x) over
example, Gelman et al. (2004), with an improper uniform prior for H
n
[0, âˆ). When B is large enough, this distinction washes out.
5. Application: Quantile regression forests. Our first application of generalized random forests is to the classical problem of nonparametric quantile regression. This problem has also been considered in detail by Meinshausen (2006),
who proposed a consistent forest-based quantile regression algorithm; his method
also fits into the paradigm of solving estimating equations (2) using random forest
weights (3). However, unlike us, Meinshausen (2006) does not propose a splitting
rule that is tailored to the quantile regression context, and instead builds his forests
using plain CART regression splits. Thus, a comparison of our method with that of
Meinshausen (2006) provides a perfect opportunity for evaluating the value of our
proposed method for constructing forest-based weights Î±i (x) that are specifically
designed to express heterogeneity in conditional quantiles.
Recall that, in the language of estimating equations, the qth quantile Î¸q (x) of
the distribution of Y conditionally on X = x is identified via (1), using the moment function ÏˆÎ¸ (Yi ) = q1({Yi > Î¸ }) âˆ’ (1 âˆ’ q)1({Yi â‰¤ Î¸}). Plugging this moment
function into our splitting scheme (8) gives us pseudo-outcomes Ïi = 1({Yi >
Î¸Ì‚q,P (Xi ) }), where Î¸Ì‚q,P (Xi ) is the qth quantile of the parent node P (Xi ) containing Xi , up to a scaling and recentering that do not affect the subsequent regression
split on these pseudo-outcomes. In other words, gradient-based quantile regression
trees try to separate observations that fall above the qth quantile of the parent from
those below it.
We compare our method to that of Meinshausen (2006) in Figure 2. In the left
panel, we have a mean shift in the distribution of Yi conditional on Xi at (Xi )1 = 0,
and both methods are able to pick it up as expected. However, in the right panel,
the mean of Y given X is constant, but there is a scale shift at (Xi )1 = 0. Here,
our method still performs well, as our splitting rule targets changes in the quantiles of the Y -distribution. However, the method of Meinshausen (2006) breaks
down completely, as it relies on CART regression splits that are only sensitive
to changes in the conditional mean of Y given X. We also note that generalized random forests produce somewhat smoother sample paths than the method of
Meinshausen (2006); this is due to our use of honesty as described in Section 2.4.
If we run generalized random forests without honesty, then our method still correctly identifies the jumps at x = 0, but has sample paths that oscillate locally just
as much as the baseline method. The purpose of this example is not to claim that
our variant of quantile regression forests built using gradient trees is always superior to the method of Meinshausen (2006) that uses regression-based splitting
to obtain the weights Î±i (x); rather, we found that, our splitting rule is specifically
sensitive to quantile shifts in a way that regression splits are notâ€”and, moreover,

GENERALIZED RANDOM FORESTS

1167

F IG . 2. Comparison of quantile regression using generalized random forests and the quantregForest package of Meinshausen (2006). In both cases, we have n = 2000 independent and identically distributed examples where Xi is uniformly distributed over [âˆ’1, 1]p with p = 40, and Yi is
Gaussian conditionally on (Xi )1 : In the left panel, Yi | Xi âˆ¼ N (0.8 Â· 1({(Xi )1 > 0}), 1), while in the
right panel Yi | Xi âˆ¼ N (0, (1 + 1({(Xi )1 > 0}))2 ). The other 39 covariates are noise. We estimate
the quantiles at q = 0.1, 0.5, 0.9.

deriving our splitting rule was fully automatic given the generalized random forest
formalism.
In several applications, we want to estimate multiple quantiles at the same time.
For example, in Figure 2, we estimate at q = 0.1, 0.5, 0.9. Estimating different
forests for each quantile separately would be undesirable for many reasons: It
would be computationally expensive, and there is a risk that quantile estimates
might cross in finite samples due to statistical noise. Thus, we need to build a forest using a splitting scheme that is sensitive to changes at any of our quantiles of
interests. Here, we use a simple heuristic inspired by our relabeling transformation. Given a set of quantiles of interest q1 < Â· Â· Â· < qk , we first evaluate all these
quantiles Î¸Ì‚q1 ,P (Xi ) â‰¤ Â· Â· Â· â‰¤ Î¸Ì‚qk ,P (Xi ) in the parent node, and label ith point by the
interval [Î¸Ì‚qj âˆ’1 ,P (Xi ) , Î¸Ì‚qj ,P (Xi ) ) it falls into. Then we choose the split point using a
multiclass classification rule that classifies each observation into one of the intervals.
6. Application: Estimating conditional average partial effects. Next, we
consider conditional average partial effect estimation under exogeneity; procedurally, the statistical task is equivalent to solving linear regression problems conditionally on features. Suppose that we observe samples (Xi , Yi , Wi ) âˆˆ X Ã— R Ã— Rq ,
and posit a random effects model Yi = Wi Â· bi + Îµi , Î²(x) = E[bi | Xi = x]. Our
goal is to estimate Î¸ (x) = Î¾ Â· Î²(x) for some contrast Î¾ âˆˆ Rp . If Wi âˆˆ {0, 1} is a
treatment assignment, then Î²(x) corresponds to the conditional average treatment
effect.

1168

S. ATHEY, J. TIBSHIRANI AND S. WAGER

In order for the average effect Î²(x) to be identified, we need to make certain
distributional assumptions. Here, we assume that the Wi are exogenous, that is, independent of the unobservables conditionally on Xi : {bi , Îµi } âŠ¥
âŠ¥ Wi | Xi . If Wi is a
binary treatment, this condition is equivalent to the unconfoundedness assumption
used to motivate propensity score methods [Rosenbaum and Rubin (1983)]. When
exogeneity does not hold, more sophisticated identification strategies are needed
(see following section).
6.1. Growing a forest. Our parameter of interest Î¸ (x) = Î¾ Â· Î²(x) is identified by (1) with ÏˆÎ²(x),c(x) (Yi , Wi ) = (Yi âˆ’ Î²(x) Â· Wi âˆ’ c(x))(1 Wi ) where
c(x) is an intercept term; this can also be written more explicitly as Î¸ (x) =
Î¾  Var Wi | Xi = x âˆ’1 Cov[Wi , Yi | Xi = x]. Given forest weights Î±i (x) as in (2),
the induced estimator Î¸Ì‚ (x) for Î¸ (x) is
(19)

Î¸Ì‚ (x) = Î¾




 n

âŠ—2

âˆ’1 n

Î±i (x)(Wi âˆ’ WÎ± )
i=1

Î±i (x)(Wi âˆ’ WÎ± )(Yi âˆ’ YÎ± ),
i=1



where WÎ± = Î±i (x)Wi and YÎ± = Î±i (x)Yi , and we write v âŠ—2 = vv  .
Generalized random forests provide us with a quasi-automatic framework for
getting the weights Î±i (x) needed in (19); all that needs to be done is to compute
the pseudo-outcomes Ïi from (8) used for recursive partitioning. We use (7) and,
for every parent P and each observation i with Xi âˆˆ P set




Ïi = Î¾  Aâˆ’1
P (Wi âˆ’ WP ) Yi âˆ’ YP âˆ’ (Wi âˆ’ WP )Î²Ì‚P ,
(20)

AP =

1
(W âˆ’ WP )âŠ—2 ,
|{i : Xi âˆˆ P }| {i:X âˆˆP }
i

where now WP and YP stand for averages taken over the parent P , and Î²Ì‚P is the
least-squares regression solution of Yi on Wi in the parent. Note that the matrix
inverse Aâˆ’1
P only needs to be evaluated once per parent node.
Checking the conditions required in Section 3, note that Assumption 1 holds
whenever the functions E[Yi | Xi = x], E[Wi | Xi = x], Cov[Yi , Wi | Xi = x]
and Var Wi | Xi = x are all Lipschitz in x, Assumption 2 holds provided that
Var Wi | Xi = x in invertible, while Assumptions 3â€“6 hold by construction. Thus,
Theorem 5 in fact applies in this setting.
6.1.1. Local centering. The above construction allows for asymptotically
valid inference for Î¸ (x), but the performance of the forests can in practice be improved by first regressing out the effect of the features Xi on all the outcomes
separately. Writing y(x) = E[Yi | X = x] and w(x) = E[Wi | X = x] for the conditional marginal expectations of Yi and Wi respectively, define centered outcomes
i = Wi âˆ’ wÌ‚ (âˆ’i) (Xi ), where yÌ‚ (âˆ’1) (Xi ), etc., are leaveYi = Yi âˆ’ yÌ‚ (âˆ’i) (Xi ) and W
one-out estimates of the marginal expectations, computed without using the ith

1169

GENERALIZED RANDOM FORESTS

i }n instead of
observation. We then run a forest using centered outcomes {Yi , W
i=1
n
the original {Yi , Wi }i=1 .
In order to justify this transformation, we note if there is any set S âŠ† X over
which Î²(x) is constant [and so Î¸ (x) is also constant], the following expression
also identifies Î¸ (x) for any x âˆˆ S :

(21)







 

Î¸ (x) = Î¾  Var Wi âˆ’ E[Wi | Xi ] | Xi âˆˆ S

âˆ’1





Ã— Cov Wi âˆ’ E[Wi | Xi ] , Yi âˆ’ E[Yi | Xi ] | Xi âˆˆ S .

Thus, if we locally center the Yi and the Wi before running our forest, the estimator
(19) has the potential to be robust to confounding effects even when the weights
Î±i (x) are not sharply concentrated around x. Similar orthogonalization ideas have
proven to be useful in many statistical contexts [e.g., Chernozhukov et al. (2018),
Newey (1994b), Neyman (1979)]; in particular, Robinson (1988) showed that if
we have access to a neighborhood S over which Î²(x) = Î²S is constant, then the
moment condition (21) induces a semiparametrically efficient estimator for Î¸S =
Î¾ Â· Î²S .
We note that if we ran a forest with any deterministic centering scheme, that is,
we used Yi = Yi âˆ’ yÌ‚(Xi ) for any Lipschitz function yÌ‚(Xi ) that does not depend
on the data, etc., then the theory developed in Section 3 would allow for valid inference about Î¸ (x) [in particular, we do not need to assume consistency of yÌ‚(Xi )].
Moreover, we could also emulate this result by using a form of k-fold cross-fitting
[Chernozhukov et al. (2018), Schick (1986)]. In the context of forests, it is much
more practical to carry out residualization via leave-one-out prediction than via
k-fold cross-fitting, because leave-one-out prediction in forests is computationally
cheap [Breiman (2001)]; however, a practitioner wanting to use results that are
precisely covered by theory may prefer to use cross-fitting for centering.
6.2. Example: Causal forests. When Wi âˆˆ {0, 1} is a binary treatment assignment, the present setup is equivalent to the standard problem of heterogeneous
treatment effect estimation under unconfoundedness. Heterogeneous treatment effect estimation via tree-based methods has received considerable attention in the
recent literature: Athey and Imbens (2016) and Su et al. (2009) develop tree-based
methods for subgroup analysis, Hill (2011) studies treatment effect estimation via
Bayesian additive regression trees [Chipman, George and McCulloch (2010)], and
Wager and Athey (2018) propose a causal forest procedure that is very nearly a
special case of our generalized random forests. The main interest of our method
is in how it can handle situations for which no comparable methods exist, such
as instrumental variables regression as discussed below. Here, however, we briefly
discuss how some concepts developed as a part of our more general approach directly improve the performance of causal forests.
The closest method to ours is Procedure 1 of Wager and Athey (2018), which is
almost equivalent to a generalized random forest without centering, the only substantive differences being that they split using the exact loss criterion (5) rather

1170

S. ATHEY, J. TIBSHIRANI AND S. WAGER

than our gradient-based loss criterion (9), and let each tree compute its own treatment effect estimate rather than using the weighting scheme from Section 2.1
(these methods are exactly equivalent for regression forests, but not for causal
forests). Wager and Athey (2018) also consider a second approach, Procedure 2,
that obtains its neighborhood function via a classification forest on the treatment
assignments Wi .
A weakness of the methods in Wager and Athey (2018), as they note in their
discussion, is that these two procedures have different strengthsâ€”Procedure 1 is
more sensitive to changes in the treatment effect function, while Procedure 2 is
more robust to confoundingâ€”but the hard coded nature of these methods made
it difficult to reconcile their relative advantages. Conversely, given the framing of
generalized random forests via estimating equations, it is â€œobviousâ€ that we can
leverage best practices from the literature on estimating equations and orthogonalize our moment conditions by regressing out the main effect of Xi on Wi and Yi
as in Robinson (1988).
To illustrate the value of orthogonalization, we revisit a simulation of Wager and
Athey (2018) where Xi âˆ¼ U ([0, 1]p ), Wi | Xi âˆ¼ Bernoulli(e(Xi )), and Yi | Xi ,
Wi âˆ¼ N (m(Xi ) + (Wi âˆ’ 0.5)Ï„ (Xi ), 1). The authors consider two different simulation settings: One with no confounding, m(x) = 0 and e(x) = 0.5, but with
treatment heterogeneity Ï„ (x) = Ï‚ (x1 )Ï‚(x2 ), Ï‚ (u) = 1 + 1/(1 + eâˆ’20(uâˆ’1/3) ), and
second with no treatment effect, Ï„ (x) = 0, but with confounding, e(x) = 14 (1 +
Î²2,4 (x3 )), m(x) = 2x3 âˆ’ 1, where Î²a,b is the Î²-density with shape parameters a
and b. We also consider a third setting with both heterogeneity and confounding,
that combines Ï„ (Â·) from the first setting with m(Â·) and e(Â·) from the second. For the
first setting, Wager and Athey (2018) used their Procedure 1, whereas for the second they used Procedure 2, while noting that it is unfortunate that the practitioner
is forced to choose one procedure or the other.
Results presented in Table 1 are reassuring, suggesting that generalized random
forests with centering do well under both settings, and can better handle the case
with both confounding and treatment heterogeneity than either of the other two
procedures. In contrast, Procedure 1 of Wager and Athey does poorly with pure
confounding, whereas Procedure 2 of Wager and Athey is good in the pure confounding setting, but does poorly with strong heterogeneity; this is as expected,
noting the design of both methods.
7. Application: Instrumental variables regression. In many applications,
we want to measure the causal effect of an intervention on an outcome, all while
recognizing that the intervention and the outcome may also be tied together
through noncausal pathways, thus ruling out the exogeneity assumption made
above. One approach in this situation is to rely on instrumental variables (IV) regression, where we find an auxiliary source of randomness that can be used to
identify causal effects.

1171

GENERALIZED RANDOM FORESTS

TABLE 1
Mean squared error of various â€œcausal forestâ€ methods, that estimate heterogeneous treatment
effects under unconfoundedness using forests. We compare our generalized random forests with and
without local centering (C. GRF and GRF) to Procedures 1 and 2 of Wager and Athey (2018), WA-1
and WA-2. All forests have B = 2000 trees, and results are aggregated over 60 simulation
replications with 1000 test points each. The mean-squared errors numbers are multiplied by 10 for
readability
Heterog.

p

n

WA-1

WA-2

GRF

C. GRF

No
No
No
No

Yes
Yes
Yes
Yes

10
10
20
20

800
1600
800
1600

1.37
0.63
2.05
0.71

6.48
6.23
8.02
7.61

0.85
0.58
0.92
0.52

0.87
0.59
0.93
0.52

Yes
Yes
Yes
Yes

No
No
No
No

10
10
20
20

800
1600
800
1600

0.81
0.68
0.90
0.77

0.16
0.10
0.13
0.09

1.12
0.80
1.17
0.95

0.27
0.20
0.17
0.11

Yes
Yes
Yes
Yes

Yes
Yes
Yes
Yes

10
10
20
20

800
1600
800
1600

4.51
2.45
5.93
3.54

7.67
7.94
8.68
8.61

1.92
1.51
1.92
1.55

0.91
0.62
0.93
0.57

Conf.

For example, suppose we want to measure the causal effect of child rearing on
a motherâ€™s labor-force participation. It is well known that, in the United States,
mothers with more children are less likely to work. But how much of this link is
causal, that is, some mothers work less because they are busy raising children, and
how much of it is merely due to confounding factors, for example, some mothers
have preferences that both lead them to raise more children and be less likely
to participate in the labor force? Understanding these effects may be helpful in
predicting the value of programs like subsidized daycare that assist mothersâ€™ labor
force participation while they have young children.
To study this question, Angrist and Evans (1998) found a source of auxiliary
randomness that can be used to distinguish causal versus correlational effects:
They found that, in the United States, parents who already have two children of
mixed sexes, that is, one boy and one girl, will have fewer kids in the future than
parents whose first two children were of the same sex. Assuming that the sexes of
the first, that is, two children in a family are effectively random, this observed preference for having children of both sexes provides an exogenous source of variation
in family size that can be used to identify causal effects: If the mixed sex indicator
is unrelated to the motherâ€™s propensity to work for a fixed number of children, then
the effect of the mixed sex indicator on the observed propensity to work can be attributed to its effect on family size. The instrumental variable estimator normalizes
this effect by the effect of mixed sex on family size, so that the normalized estimate is a consistent estimate of the treatment effect of family size on work. Other

1172

S. ATHEY, J. TIBSHIRANI AND S. WAGER

classical uses of instrumental variables regression include measuring the impact
of military service on lifetime income by using the Vietnam draft lottery as an
instrument [Angrist (1990)], and measuring the extent to which 401(k) savings
programs crowd out other savings, using eligibility for 401(k) savings programs as
an instrument [Abadie (2003), Poterba, Venti and Wise (1996)].
7.1. A forest for instrumental variables regression. Classical instrumental
variables regression seeks a global understanding of the treatment effect, for example, on average over the whole U.S. population, does having more children reduce
the labor force participation of women? Here, we use forests to estimate heterogeneous treatment effects: We might ask how the causal effect of child rearing varies
with a motherâ€™s age and socioeconomic status.
We observe i = 1, . . . , n independent and identically distributed subjects, each
of whom has features Xi âˆˆ X , an outcome Yi âˆˆ R, a received treatment Wi âˆˆ {0, 1}
and an instrument Zi âˆˆ {0, 1}. We believe that the outcomes Yi and received treatment Wi are related via a structural model Yi = Î¼(Xi ) + Ï„ (Xi )Wi + Îµi , where
Ï„ (Xi ) is understood to be the causal effect of Wi on Yi , and Îµi is a noise term that
may be positively correlated with Wi . Because Îµi is correlated with Wi , standard
regression analyses will not in general be consistent for Ï„ (Xi ), and we need to use
the instrument Zi . If Zi is independent of Îµi conditionally on Xi then, provided
that Zi has an influence on the received treatment Wi , that is, that the covariance
of Zi and Wi conditionally on Xi = x is nonzero, the treatment effect Ï„ (x) is
identified via Ï„ (x) = Cov[Yi , Zi | Xi = x]/ Cov[Wi , Zi | Xi = x]. We can then estimate Ï„ (x) by via moment functions E[Zi (Yi âˆ’ Wi Ï„ (x) âˆ’ Î¼(x)) | Xi = x] = 0
and E[Yi âˆ’ Wi Ï„ (x) âˆ’ Î¼(x) | Xi = x] = 0, where the intercept Î¼(x) is a nuisance
parameter. If we are not willing to assume that every individual i with features
Xi = x has the same treatment effect Ï„ (x), then heterogeneous instrumental variables regression allows us to estimate a (conditional) local average treatment effect
[Abadie (2003), Imbens and Angrist (1994)].
We then use our formalism to derive a forest that is targeted towards estimating
causal effects identified via conditional two-stage least squares. Gradient-based
labeling (8) yields pseudo-outcomes for every parent node P and each observation
i with Xi âˆˆ P , Ïi = (Zi âˆ’ ZP )((Yi âˆ’ YP ) âˆ’ (Wi âˆ’ WP )Ï„Ì‚P ), where YP , WP , ZP
are moments in the parent node, and Ï„Ì‚P is a solution to the estimating equation
in the parent. Given these pseudo-outcomes, the tree executes a CART regression
split on the Ïi as usual. Finally, we obtain personalized treatment effect estimates
Ï„Ì‚ (x) by solving (2) with forest weights (3).
To verify that Theorem 5 holds in this setting, we note that Assumption 1
holds whenever the conditional moment functions E[Wi | Xi = x], E[Yi | Xi = x],
E[Zi | Xi = x], E[Wi Zi | Xi = x] and E[Yi Zi | Xi = x] are all Lipschitz continuous in x, while Assumption 2 holds whenever the instrument is correlated with
received treatment (i.e., the instrument is valid). Assumptions 3â€“6 hold thanks to
the definition of Ïˆ.

GENERALIZED RANDOM FORESTS

1173

As in Section 6.1.1, we center our procedure using the transformation of
Robinson (1988), and regress out the marginal effects of Xi first. Writing y(x) =
E[Yi | X = x], w(x) = E[Wi | X = x], and z(x) = E[Zi | X = x], we compute
conditionally centered outcomes by leave-one-out estimation Yi = Yi âˆ’ yÌ‚ (âˆ’i) (Xi ),
i = Wi âˆ’ wÌ‚ (âˆ’i) (Xi ) and Zi = Zi âˆ’ zÌ‚(âˆ’i) (Xi ), and then run the full instrumental
W
i , Zi }n . We recommend working
variables forest using centered outcomes {Yi , W
i=1
with centered outcomes by default, and we do so in our simulations. Our package grf provides the option of making this transformation automatically, where
yÌ‚ (âˆ’i) (Xi ), wÌ‚(âˆ’i) (Xi ) and zÌ‚(âˆ’i) (Xi ) are first estimated using 3 separate regression
forests.
There is a rich literature on nonparametric instrumental variables regression.
The above approach generalizes classical approaches based on kernels or series estimation [Abadie (2003), Su, Murtazashvili and Ullah (2013), Wooldridge (2010)].
In other threads, Darolles et al. (2011) and Newey and Powell (2003) study instrumental variables models that generalize the conditionally linear treatment model
and allowing for nonlinear effects, and Hartford et al. (2017) develop deep learning tools. Belloni et al. (2012) consider working with high-dimensional instruments Zi .
The supplement [Athey, Tibshirani and Wager (2018)] has a simulation study
for IV forests, comparing them to nearest-neighbor and series regression. We find
our method to perform well relative to these baselines, and centering to be helpful.
We also evaluate coverage of the bootstrap of little bag confidence intervals.
7.2. The effect of child rearing on labor-force participation. We now revisit
our motivating example discussed at the beginning of Section 7. We follow Angrist
and Evans (1998) in constructing our dataset, and study a sample of n = 334,535
married mothers with at least 2 children (1980 census data), based on the following quantities: The outcome Yi is whether the mother did not work in the year
preceding the census, the received treatment Wi is whether the mother had 3 or
more children at census time, and the instrument Zi measures whether or not the
motherâ€™s first two children were of different sexes. Based on this data, Angrist and
Evans (1998) estimated the local average treatment effect of having a third child

Z] = 1.6 Â· 10âˆ’2 ,
among mothers with at least two children. In our sample, Cov[W,
âˆ’3

while Cov[Y, Z] = 2.1 Â· 10 , leading to a 95% confidence interval for the local
average treatment effect Ï„ âˆˆ (0.14 Â± 0.054) using the R function ivreg [Kleiber
and Zeileis (2008)]. Thus, it appears that having a third child reduces womenâ€™s
labor force participation on average in the US. Angrist and Evans (1998) conduct
extensive sensitivity analysis to corroborate the plausibility of this identification
strategy.
We seek to extend this analysis by fitting heterogeneity on several covariates,
including the motherâ€™s age at the birth of her first child, her age at census time, her
years of education and her race (black, hispanic, other), as well as the fatherâ€™s income. Formally, our analysis identifies a conditional local average treatment effect
Ï„ (x) [Abadie (2003), Imbens and Angrist (1994)].

1174

S. ATHEY, J. TIBSHIRANI AND S. WAGER

F IG . 3. Generalized random forest estimates (along with pointwise 95% confidence intervals) for
the causal effect of having a third child on the probability that a mother works for pay, as identified
by the same sex instrument of Angrist and Evans (1998); a positive treatment effect means that the
treatment reduces the probability that the mother works. We vary the motherâ€™s age at first birth and
the fatherâ€™s income; other covariates are set to their median values in the above plots. The forest
was grown with a subsample fraction s/n = 0.05, a minimum leaf size k = 800 and consists of
B = 100,000 trees.

Results from a generalized random forest analysis are presented in Figure 3.
These results suggest that the observed treatment effect is driven by mothers whose
husbands have a lower income. Such an effect would be intuitively easy to justify:
It seems plausible that mothers with wealthier husbands can afford to hire help in
raising their children, and so can choose whether or not to work based on other
considerations. That being said, we caution that the fatherâ€™s income was measured
in the census, so there is potentially an endogeneity problem: Perhaps a motherâ€™s
choice not to work after having a third child enables the husband to earn more.
Ideally, we would have wanted to measure the husbandâ€™s income at the time of the
second childâ€™s birth, but we do not have access to this measurement in the present
data. Moreover, the confidence intervals in Figure 3 are rather wide, attesting to
the importance of formal asymptotic theory when using forest-based methods for
instrumental variables regression.
8. Discussion. We introduced generalized random forests as a versatile
method for adaptive, local estimation in a wide variety of statistical models. We
discussed our method in the contexts of quantile regression and heterogeneous
treatment effect estimation, and our approach also applies to a wide variety of
other settings, such as demand estimation or panel data analysis. Our software,
grf, is implemented in a modular way that should enable users to implement
splitting rules motivated by new statistical questions.

GENERALIZED RANDOM FORESTS

1175

Many of the remaining challenges with generalized random forests are closely
related to those with standard nonparametric methods for local likelihood estimation. In particular, as discussed above, our confidence interval construction relies
on undersmoothing to get valid asymptotic coverage (without undersmoothing, the
confidence intervals account for sampling variability of the forest, but do not capture bias). Developing a principled way to bias-correct our confidence intervals,
and thus avoid the need for undersmoothing, would be of considerable interest both
conceptually and in practice. Moreover, again like standard methods, forests can
exhibit edge effects whereby the slope of our estimates Î¸Ì‚(x) may taper off as we
approach the edge of X -space, even when the true function Î¸ (x) keeps changing.
Finding an elegant way to deal with such edge effects could improve the quality of
the confidence intervals provided by generalized random forests.
Acknowledgment. We are grateful to Jerry Friedman for first recommending we take a closer look at splitting rules for quantile regression forests, to Will
Fithian for drawing our attention to connections between our early ideas and gradient boosting, to Guido Imbens for suggesting the local centering scheme in Section 6.1.1, and to colleagues, seminar participants, the Associate Editor and three
anonymous referees for helpful suggestions.
SUPPLEMENTARY MATERIAL
Supplement to â€œGeneralized random forestsâ€ (DOI: 10.1214/18-AOS1709
SUPP; .pdf). The supplement ontains proofs of technical results, as well as a simulation study for instrumental variables regression with forests.
REFERENCES
A BADIE , A. (2003). Semiparametric instrumental variable estimation of treatment response models.
J. Econometrics 113 231â€“263. MR1960380
A MIT, Y. and G EMAN , D. (1997). Shape quantization and recognition with randomized trees. Neural
Comput. 9 1545â€“1588.
A NDREWS , D. W. K. (1993). Tests for parameter instability and structural change with unknown
change point. Econometrica 61 821â€“856. MR1231678
A NGRIST, J. D. (1990). Lifetime earnings and the Vietnam era draft lottery: Evidence from social
security administrative records. AER 313â€“336.
A NGRIST, J. D. and E VANS , W. N. (1998). Children and their parentsâ€™ labor supply: Evidence from
exogenous variation in family size. AER 450â€“477.
A RLOT, S. and G ENUER , R. (2014). Analysis of purely random forests bias. ArXiv preprint. Available at arXiv:1407.3939.
ATHEY, S. and I MBENS , G. (2016). Recursive partitioning for heterogeneous causal effects. Proc.
Natl. Acad. Sci. USA 113 7353â€“7360. MR3531135
ATHEY, S., T IBSHIRANI , J. and WAGER , S. (2018). Supplement to â€œGeneralized random forests.â€
DOI:10.1214/18-AOS1709SUPP.
B ELLONI , A., C HEN , D., C HERNOZHUKOV, V. and H ANSEN , C. (2012). Sparse models and methods for optimal instruments with an application to eminent domain. Econometrica 80 2369â€“2429.
MR3001131

1176

S. ATHEY, J. TIBSHIRANI AND S. WAGER

B EYGELZIMER , A. and L ANGFORD , J. (2009). The offset tree for learning with partial labels. In
Proceedings of KDD 129â€“138. ACM.
B IAU , G. (2012). Analysis of a random forests model. J. Mach. Learn. Res. 13 1063â€“1095.
MR2930634
B IAU , G. and D EVROYE , L. (2010). On the layered nearest neighbour estimate, the bagged nearest
neighbour estimate and the random forest method in regression and classification. J. Multivariate
Anal. 101 2499â€“2518. MR2719877
B IAU , G., D EVROYE , L. and L UGOSI , G. (2008). Consistency of random forests and other averaging
classifiers. J. Mach. Learn. Res. 9 2015â€“2033. MR2447310
B IAU , G. and S CORNET, E. (2016). A random forest guided tour. TEST 25 197â€“227. MR3493512
B REIMAN , L. (1996). Bagging predictors. Mach. Learn. 24 123â€“140.
B REIMAN , L. (2001). Random forests. Mach. Learn. 45 5â€“32.
B REIMAN , L., F RIEDMAN , J. H., O LSHEN , R. A. and S TONE , C. J. (1984). Classification and
Regression Trees. Wadsworth Advanced Books and Software, Belmont, CA. MR0726392
B ÃœHLMANN , P. and Y U , B. (2002). Analyzing bagging. Ann. Statist. 30 927â€“961. MR1926165
C HERNOZHUKOV, V., C HETVERIKOV, D., D EMIRER , M., D UFLO , E., H ANSEN , C., N EWEY, W.
and ROBINS , J. (2018). Double/debiased machine learning for treatment and structural parameters. Econom. J. 21 C1â€“C68.
C HIPMAN , H. A., G EORGE , E. I. and M C C ULLOCH , R. E. (2010). BART: Bayesian additive regression trees. Ann. Appl. Stat. 4 266â€“298. MR2758172
DAROLLES , S., FAN , Y., F LORENS , J. P. and R ENAULT, E. (2011). Nonparametric instrumental
regression. Econometrica 79 1541â€“1565. MR2883763
D ENIL , M., M ATHESON , D. and D E F REITAS , N. (2014). Narrowing the Gap: Random forests in
theory and in practice. In Proceedings of ICML 665â€“673.
D IETTERICH , T. G. (2000). An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Mach. Learn. 40 139â€“157.
E FRON , B. (1982). The Jackknife, the Bootstrap and Other Resampling Plans. CBMS-NSF Regional
Conference Series in Applied Mathematics 38. Society for Industrial and Applied Mathematics
(SIAM), Philadelphia, PA. MR0659849
E FRON , B. and S TEIN , C. (1981). The jackknife estimate of variance. Ann. Statist. 9 586â€“596.
MR0615434
FAN , J., FARMEN , M. and G IJBELS , I. (1998). Local maximum likelihood estimation and inference.
J. R. Stat. Soc. Ser. B. Stat. Methodol. 60 591â€“608. MR1626013
FAN , J. and G IJBELS , I. (1996). Local Polynomial Modelling and Its Applications. Monographs on
Statistics and Applied Probability 66. Chapman & Hall, London. MR1383587
F RIEDMAN , J. H. (2001). Greedy function approximation: A gradient boosting machine. Ann.
Statist. 29 1189â€“1232. MR1873328
G ELMAN , A., C ARLIN , J. B., S TERN , H. S. and RUBIN , D. B. (2004). Bayesian Data Analysis,
2nd ed. Chapman & Hall/CRC, Boca Raton, FL. MR2027492
G EURTS , P., E RNST, D. and W EHENKEL , L. (2006). Extremely randomized trees. Mach. Learn. 63
3â€“42.
G ORDON , L. and O LSHEN , R. A. (1985). Tree-structured survival analysis. Cancer Treat. Rep. 69
1065â€“1069.
H AMPEL , F. R. (1974). The influence curve and its role in robust estimation. J. Amer. Statist. Assoc.
69 383â€“393. MR0362657
H ANSEN , B. E. (1992). Testing for parameter instability in linear models. J. Policy Model. 14 517â€“
533.
H ARTFORD , J., L EWIS , G., L EYTON -B ROWN , K. and TADDY, M. (2017). Deep IV: A flexible
approach for counterfactual prediction. In Proceedings of ICML 1414â€“1423.
H ASTIE , T., T IBSHIRANI , R. and F RIEDMAN , J. (2009). The Elements of Statistical Learning, 2nd
ed. Springer, New York. MR2722294

GENERALIZED RANDOM FORESTS

1177

H ILL , J. L. (2011). Bayesian nonparametric modeling for causal inference. J. Comput. Graph.
Statist. 20 217â€“240. MR2816546
H JORT, N. L. and KONING , A. (2002). Tests for constancy of model parameters over time. J. Nonparametr. Stat. 14 113â€“132. MR1905588
H O , T. K. (1998). The random subspace method for constructing decision forests. IEEE Trans.
Pattern Anal. Mach. Intell. 20 832â€“844.
H OEFFDING , W. (1948). A class of statistics with asymptotically normal distribution. Ann. Math.
Stat. 19 293â€“325. MR0026294
H ONORÃ‰ , B. E. and K YRIAZIDOU , E. (2000). Panel data discrete choice models with lagged dependent variables. Econometrica 68 839â€“874. MR1771585
H OTHORN , T., L AUSEN , B., B ENNER , A. and R ADESPIEL -T RÃ–GER , M. (2004). Bagging survival
trees. Stat. Med. 23 77â€“91.
I MBENS , G. W. and A NGRIST, J. D. (1994). Identification and estimation of local average treatment
effects. Econometrica 62 467â€“475.
I SHWARAN , H. and KOGALUR , U. B. (2010). Consistency of random survival forests. Statist.
Probab. Lett. 80 1056â€“1064. MR2651045
K ALLUS , N. (2017). Recursive Partitioning for Personalization using Observational Data. In Proceedings of ICML. 1789â€“1798.
K LEIBER , C. and Z EILEIS , A. (2008). Applied Econometrics with R. Springer Science & Business
Media.
L E B LANC , M. and C ROWLEY, J. (1992). Relative risk trees for censored survival data. Biometrics
411â€“425.
L EWBEL , A. (2007). A local generalized method of moments estimator. Econom. Lett. 94 124â€“128.
MR2287404
L IN , Y. and J EON , Y. (2006). Random forests and adaptive nearest neighbors. J. Amer. Statist. Assoc.
101 578â€“590. MR2256176
L OADER , C. (1999). Local Regression and Likelihood. Springer, New York. MR1704236
M ALLOWS , C. L. (1973). Some comments on Cp. Technometrics 15 661â€“675.
M EINSHAUSEN , N. (2006). Quantile regression forests. J. Mach. Learn. Res. 7 983â€“999.
MR2274394
M ENTCH , L. and H OOKER , G. (2016). Quantifying uncertainty in random forests via confidence
intervals and hypothesis tests. J. Mach. Learn. Res. 17 26. MR3491120
M OLINARO , A. M., D UDOIT, S. and VAN DER L AAN , M. J. (2004). Tree-based multivariate regression and density estimation with right-censored data. J. Multivariate Anal. 90 154â€“177.
MR2064940
N EWEY, W. K. (1994a). Kernel estimation of partial means and a general variance estimator. Econometric Theory 10 233â€“253. MR1293201
N EWEY, W. K. (1994b). The asymptotic variance of semiparametric estimators. Econometrica 62
1349â€“1382. MR1303237
N EWEY, W. K. and P OWELL , J. L. (2003). Instrumental variable estimation of nonparametric models. Econometrica 71 1565â€“1578. MR2000257
N EYMAN , J. (1979). C(Î±) tests and their use. Sankhya, Ser. A 41 1â€“21. MR0615037
N YBLOM , J. (1989). Testing for the constancy of parameters over time. J. Amer. Statist. Assoc. 84
223â€“230. MR0999682
P LOBERGER , W. and K RÃ„MER , W. (1992). The CUSUM test with OLS residuals. Econometrica 60
271â€“285. MR1162619
P OTERBA , J. M., V ENTI , S. F. and W ISE , D. A. (1996). How retirement saving programs increase
saving. J. Electron. Publ. 10 91â€“112.
ROBINS , J. M. and R ITOV, Y. (1997). Toward a curse of dimensionality appropriate (CODA) asymptotic theory for semi-parametric models. Stat. Med. 16.

1178

S. ATHEY, J. TIBSHIRANI AND S. WAGER

ROBINSON , P. M. (1988). Root-N -consistent semiparametric regression. Econometrica 56 931â€“954.
MR0951762
ROSENBAUM , P. R. and RUBIN , D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika 70 41â€“55. MR0742974
S CHICK , A. (1986). On asymptotically efficient estimation in semiparametric models. Ann. Statist.
14 1139â€“1151. MR0856811
S CORNET, E., B IAU , G. and V ERT, J.-P. (2015). Consistency of random forests. Ann. Statist. 43
1716â€“1741. MR3357876
S EXTON , J. and L AAKE , P. (2009). Standard errors for bagged and random forest estimators. Comput. Statist. Data Anal. 53 801â€“811. MR2654590
S TANISWALIS , J. G. (1989). The kernel estimate of a regression function in likelihood-based models.
J. Amer. Statist. Assoc. 84 276â€“283. MR0999689
S TONE , C. J. (1977). Consistent nonparametric regression. Ann. Statist. 5 595â€“645. MR0443204
S U , L., M URTAZASHVILI , I. and U LLAH , A. (2013). Local linear GMM estimation of functional
coefficient IV models with an application to estimating the rate of return to schooling. J. Bus.
Econom. Statist. 31 184â€“207. MR3055331
S U , X., T SAI , C.-L., WANG , H., N ICKERSON , D. M. and L I , B. (2009). Subgroup analysis via
recursive partitioning. J. Mach. Learn. Res. 10 141â€“158.
T IBSHIRANI , R. and H ASTIE , T. (1987). Local likelihood estimation. J. Amer. Statist. Assoc. 82
559â€“567. MR0898359
VAN DER VAART, A. W. (1998). Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics 3. Cambridge Univ. Press, Cambridge. MR1652247
VARIAN , H. R. (2014). Big data: New tricks for econometrics. J. Electron. Publ. 28 3â€“27.
WAGER , S. and ATHEY, S. (2018). Estimation and inference of heterogeneous treatment effects
using random forests. J. Amer. Statist. Assoc. 113 1228â€“1242.
WAGER , S., H ASTIE , T. and E FRON , B. (2014). Confidence intervals for random forests: The jackknife and the infinitesimal jackknife. J. Mach. Learn. Res. 15 1625â€“1651. MR3225243
WAGER , S. and WALTHER , G. (2015). Adaptive concentration of regression trees, with application
to random forests. ArXiv preprint. Available at arXiv:1503.06388.
W OOLDRIDGE , J. M. (2010). Econometric Analysis of Cross Section and Panel Data, 2nd ed. MIT
Press, Cambridge, MA. MR2768559
W RIGHT, M. N. and Z IEGLER , A. (2017). ranger: A fast implementation of random forests for high
dimensional data in C++ and R. J. Stat. Softw. 77 1â€“17.
Z EILEIS , A. (2005). A unified approach to structural change tests based on ML scores, F statistics,
and OLS residuals. Econometric Rev. 24 445â€“466. MR2208835
Z EILEIS , A. and H ORNIK , K. (2007). Generalized M-fluctuation tests for parameter instability. Stat.
Neerl. 61 488â€“508. MR2351461
Z EILEIS , A., H OTHORN , T. and H ORNIK , K. (2008). Model-based recursive partitioning. J. Comput.
Graph. Statist. 17 492â€“514. MR2439970
Z HU , R., Z ENG , D. and KOSOROK , M. R. (2015). Reinforcement learning trees. J. Amer. Statist.
Assoc. 110 1770â€“1784. MR3449072
S. ATHEY
S. WAGER
S TANFORD G RADUATE S CHOOL OF B USINESS
S TANFORD U NIVERSITY
655 K NIGHT WAY
S TANFORD , C ALIFORNIA 94305
USA
E- MAIL : athey@stanford.edu
swager@stanford.edu

J. T IBSHIRANI
E LASTICSEARCH BV
800 W EST E L C AMINO R EAL
S UITE 350
M OUNTAIN V IEW, C ALIFORNIA 94040
USA
E- MAIL : julietibs@gmail.com

