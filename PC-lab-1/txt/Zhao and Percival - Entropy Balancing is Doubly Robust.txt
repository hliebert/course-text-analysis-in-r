ENTROPY BALANCING IS DOUBLY ROBUST

arXiv:1501.03571v3 [stat.ME] 11 Feb 2017

QINGYUAN ZHAO
Department of Statistics, Wharton School, University of Pennsylvania

DANIEL PERCIVAL
Google Inc.
Abstract. Covariate balance is a conventional key diagnostic for methods
used estimating causal effects from observational studies. Recently, there is an
emerging interest in directly incorporating covariate balance in the estimation.
We study a recently proposed entropy maximization method called Entropy
Balancing (EB), which exactly matches the covariate moments for the different experimental groups in its optimization problem. We show EB is doubly
robust with respect to linear outcome regression and logistic propensity score
regression, and it reaches the asymptotic semiparametric variance bound when
both regressions are correctly specified. This is surprising to us because there
is no attempt to model the outcome or the treatment assignment in the original proposal of EB. Our theoretical results and simulations suggest that EB
is a very appealing alternative to the conventional weighting estimators that
estimate the propensity score by maximum likelihood.

1. Introduction
Consider a typical setting of observational study that two conditions (â€œtreatmentâ€ and â€œcontrolâ€) are not randomly assigned to the units. Deriving a causal
conclusion from such observational data is essentially difficult because the treatment
exposure may be related to some covariates that are also related to the outcome.
In this case, those covariates may be imbalanced between the treatment groups and
the naive mean causal effect estimator can be severely biased.
To adjust for the covariate imbalance, the seminal work of Rosenbaum and
Rubin (1983) points out the essential role of propensity score, the probability of
exposure to treatment conditional on observed covariates. This quantity, rarely
known in an observation study, may be estimated from the data. Based on the
E-mail addresses: qyzhao@wharton.upenn.edu, dancsi@google.com.
Date: February 14, 2017.
Key words and phrases. Causal Inference, Double Robustness, Exponential Tilting, Convex
Optimization, Survey Sampling.
This work is completed when Qingyuan Zhao is a Ph.D. student at the Department of Statistics,
Stanford University. We would like to thank Jens Hainmueller, Trevor Hastie, Hera He, Bill
Heavlin, Diane Lambert, Daryl Pregibon, Jean Steiner and one anonymous reviewer for their
helpful comments.
1

2

ENTROPY BALANCING

estimated propensity score, many statistical methods are proposed to estimate the
mean causal effect. The most popular approaches are matching (e.g. Rosenbaum
and Rubin, 1985; Abadie and Imbens, 2006), stratification (e.g. Rosenbaum and
Rubin, 1984), and weighting (e.g. Robins et al., 1994; Hirano and Imbens, 2001).
Theoretically, propensity score weighting is the most attractive among these methods. Hirano et al. (2003) show that nonparametric propensity score weighting can
achieve the semiparametric efficiency bound for the estimation of mean causal effect derived by Hahn (1998). Another desirable property is double robustness.
The pioneering work of Robins et al. (1994) augments propensity score weighting
by an outcome regression model. The resulting estimator has the so-called double
robustness property:
Property 1. If either the propensity score model or the outcome regression model
is correctly specified, the mean causal effect estimator is statistically consistent.
In practice, the success of any propensity score method hinges on the quality of
the estimated propensity score. The weighting methods are usually more sensitive
to model misspecification than matching and stratification, and furthermore, this
bias can even be amplified by a doubly robust estimator, brought to attention
by Kang and Schafer (2007). In order to avoid model misspecification, applied
researchers usually increase the complexity of the propensity score model until a
sufficiently balanced solution is found. This cyclical process of modeling propensity
score and checking covariate balance is criticized as the â€œpropensity score tautologyâ€
by Imai et al. (2008) and, moreover, has no guarantee of finding a satisfactory
solution eventually.
Recently, there is an emerging interest, particularly among applied researchers,
in directly incorporating covariate balance in the estimation procedure, so there is
no need to check covariate balance repeatedly (e.g. Graham et al., 2012; Diamond
and Sekhon, 2013; Imai and Ratkovic, 2014; Zubizarreta, 2015). In this paper, we
study a method of this kind called Entropy Balancing (hereafter EB) proposed in
Hainmueller (2011). In a nutshell, EB solves an (convex) entropy maximization
problem under the constraint of exact balance of covariate moments. Due to its
easy interpretation and fast computation, EB has already gained some popularity in
applied fields (Marcus, 2013; Ferwerda, 2014). However, little do we known about
the theoretical properties of EB. The original proposal in Hainmueller (2011) did
not give a condition such that EB is guaranteed to give a consistent estimate of the
mean causal effect.
In this paper, we shall show EB is indeed a very appealing propensity score
weighting method. We find EB simultaneously fits a logistic regression model for
the propensity score and a linear regression model for the outcome. The linear
predictors of these regression models are the covariate moments being balanced.
We shall prove EB is doubly robust (Property 1), in the sense that if at least
one of the two models are correctly specified, EB is consistent for the Population
Average Treatment effect for the Treated (PATT), a common quantity of interest
in causal inference and survey sampling. Moreover, EB is sample bounded (Tan,
2010), meaning the PATT estimator is always within the range of the observed outcomes, and it is semiparametrically efficient if both models are correctly specified.
Lastly, The two linear models have an exact correspondence to the primal and dual
optimization problem used to solve EB, revealing an interesting connection between
doubly robust estimation and convex optimization.

ENTROPY BALANCING

Propensity Score
Modeling

Dual problem

Primal problem
Covariate Balance

3

Outcome Regression
(Empirical Calibration)

Double Robustness

Figure 1. The role of covariate balance in doubly robust estimation. Dashed arrows: conventional procedure to achieve double
robustness. Solid arrows: double robustness of Entropy Balancing
via covariate balance.

Our discoveries can be summarized in the diagram in Figure 1. Conventionally,
the recipe given by Robins and his coauthors is to fit separate models for propensity
score and outcome regression and then combine them by a doubly robust estimator
(see e.g. Robins et al., 1994; Lunceford and Davidian, 2004; Bang and Robins, 2005;
Kang and Schafer, 2007). In contrast, Entropy Balancing achieves this goal through
enforcing covariate balance. The primal optimization problem of EB amounts to an
empirical calibration estimator (Deville and SaÌˆrndal, 1992; SaÌˆrndal and LundstroÌˆm,
2005), which is widely popular in survey sampling but perhaps not sufficiently
recognized in causal inference (Chan et al., 2015). The balancing constraints in this
optimization problem result in unbiasedness of the PATT estimator under linear
outcome regression model. The dual optimization problem of EB is fitting a logistic
propensity score model with a loss function different from the negative binomial
likelihood. The Fisher-consistency of this loss function (also called proper scoring
rule in statistical decision theory, see e.g. Gneiting and Raftery (2007)) ensures the
other half of double robustnessâ€”consistency under correctly specified propensity
score model. Since EB essentially just uses a different loss function, other types of
propensity score models, for example the generalized additive models (Hastie and
Tibshirani, 1990), can also easily be fitted. A forthcoming article by Zhao (2016)
offers more discussion and extension to other weighted average treatment effects.
2. Setting
First, we fix some notations for the causal inference problem considered in this
paper. We follow the potential outcome language of Neyman (1923) and Rubin
(1974). In this causal model, each unit i is associated with a pair of potential
outcomes: the response Yi (1) that is realized if Ti = 1 (treated), and another
response Yi (0) realized if Ti = 0 (control). We assume the observational units are
independent and identically distributed samples from a population, for which we
wish to infer the treatmentâ€™s effect. The main obstacle is that only one potential
outcome is observed: Yi = Ti Yi (1) âˆ’ (1 âˆ’ Ti )Yi (0), which is commonly known as
the â€œfundamental problem of causal inferenceâ€ (Holland, 1986).
In this paper we focus on estimating the Population Average Treatment effect
on the Treated (PATT):
(1)

âˆ†

Î³ = E[Y (1)|T = 1] âˆ’ E[Y (0)|T = 1] = Âµ(1|1) âˆ’ Âµ(0|1).

4

ENTROPY BALANCING

The counterfactual mean Âµ(0|1) = E[Y (0)|T = 1] also naturally occurs in survey
sampling with missing data (Deville and SaÌˆrndal, 1992; SaÌˆrndal and LundstroÌˆm,
2005) by viewing Y (0) as the only outcome of interest (so T = 1 stands for nonresponse).
Along with the treatment exposure Ti and outcome Yi , each unit i is usually
associated with a set of covariates denoted by Xi measured prior to the treatment
assignment. In a typical observational study, both treatment assignment and outcome may be related to the covariates, which can cause serious confounding bias.
The seminal work by Rosenbaum and Rubin (1983) suggest that it is possible to
correct the confounding bias under the following two assumptions:
Assumption 1 (strong ignorability). (Y (0), Y (1)) âŠ¥ T | X.
Assumption 2 (overlap). 0 < P(T = 1|X) < 1.
Intuitively, the first assumption says that the observed covariates contain all
the information that may cause the selection bias, i.e. there is no unmeasured
confounding variable, and the second assumption ensures that the bias-correction
information is available across the entire domain of X.
Since the covariates X contain all the information of confounding bias, it is
important to understand the relationship between T, Y and X. Under Assumption
1 (strong ignorability), the joint distribution of (X, Y, T ) is determined by the
marginal distribution of X and two conditional distributions given X. The first
conditional distribution e(X) = P (T = 1|X) is often called the propensity score
and plays a central role in causal inference (Rosenbaum and Rubin, 1983). The
second conditional distribution is the density of Y (0) and Y (1) given X. Since we
only consider the mean causal effect in this paper, it suffices to study the mean
regression functions g0 (X) = E[Y (0)|X] and g1 (X) = E[Y (1)|X].
To estimate the PATT defined in (1), a conventional weighting estimator based
on the propensity score is the inverse probability weighting (IPW) defined as

(2)

Î³Ì‚ IPW =

X 1
X
eÌ‚(Xi )(1 âˆ’ eÌ‚(Xi ))âˆ’1
P
Yi âˆ’
Y.
âˆ’1 i
n1
Ti =0 eÌ‚(Xi )(1 âˆ’ eÌ‚(Xi ))

Ti =1

Ti =0

P
Here Ti =t is a short-hand notation of summation over all units i such that Ti = t.
This will be repeatedly used throughout this paper. In (2), the control units are
weighted proportionally to eÌ‚(Xi )(1 âˆ’ eÌ‚(Xi ))âˆ’1 to resemble the full population. The
most popular choice of propensity score model
Pp is the logistic regression, where
logit(e(x)) = log[e(x)/(1âˆ’e(x))] is modeled by j=1 Î¸j cj (x) and cj (x) are functions
of the covariates.

3. Entropy Balancing
Entropy Balancing (EB) is an alternative weighting method proposed by Hainmueller (2011) to estimate PATT. EB operates by maximizing the entropy of the

ENTROPY BALANCING

5

weights under some pre-specified balancing constraints:
X
maximize âˆ’
wi log wi
w

Ti =0

subject to
(3)

X

wi cj (Xi ) = cÌ„j (1) =

Ti =0

X

1 X
cj (Xi ), j = 1, . . . , p,
n1
Ti =1

wi = 1,

Ti =0

wi > 0, i = 1, . . . , n.
P
Hainmueller (2011) proposes to use the weighted average Ti =0 wiEB Yi to estimate
the counterfactual mean E[Y (0)|T = 1]. This gives the Entropy Balancing estimator of PATT
X
X Yi
âˆ’
wiEB Yi .
(4)
Î³Ì‚ EB =
n1
Ti =0

Ti =1

{cj (Â·)}pj=1

The functions
in (3) are called moment functions of the covariates.
They can be any transformation of X, not necessarily polynomial functions. We
use c(X) and cÌ„(1) to stand for the vector of cj (X) and cÌ„j (1), j = 1, . . . , p. We shall
see the functions {cj (Â·)}pj=1 indeed serve as the linear predictors in the propensity
score model and the outcome regression model, although at this point it is not even
clear that EB attempts to fit any model.
First, we give some heuristics that allows us to view EB as a propensity score
weighting method. Since EB seeks to empirically match the control and treatment
covariate distributions, we connect EB with density estimation. Let m(x) be the
density function of the covariates X for the control population. The minimum
relative entropy principle estimates the density of the treatment population by
(5)

maximize H(mÌƒkm)
mÌƒ

subject to EmÌƒ [c(X)] = cÌ„(1),

where H(mÌƒkm) = EmÌƒ [log(mÌƒ(X)/m(X))] is the relative entropy between mÌƒ and m.
As an estimate of the distribution of the treatment group, the optimal mÌƒ of (5)
is the â€œclosestâ€ to the control distribution among all distributions satisfying the
moment constraints. Now let w(x) = [P(T = 1) Â· mÌƒ(x)]/[P(T = 0) Â· m(x)] be the
population version of the inverse probability weights in (2). Applying a change of
measure, we can rewrite (5) as an optimization problem over w(x):
(6)

maximize Em [w(X) log w(X)]
w

subject to Em [w(X)c(X)] = cÌ„(1).

The EB optimization problem (3) is the finite sample version of (6), where the
population distribution m is replaced by the empirical distribution of the control
units.
Using the Lagrangian multipliers, one can show the solution of (5) belongs to
the family of exponential titled distributions of m (Cover and Thomas, 2012):
mÎ¸ (x) = m(x) exp(Î¸T c(x) âˆ’ Ïˆ(Î¸)).
Here, Ïˆ(Î¸) is the moment generating function of this exponential family. Consequently, the solution of the population EB (6) is
e(x)
P(T = 1|X = x)
=
= w(x) = exp(Î± + Î¸T c(x)),
1 âˆ’ e(x)
P(T = 0|X = x)

6

ENTROPY BALANCING

where Î± = log(P(T = 1)/P(T = 0)). This is exactly the logistic regression model
with predictors c(x).
Notice that EB is different from the maximum likelihood fit of the logistic regression. The dual optimization problem of (3) is
ï£«
ï£¶
X

p
p
X
X
(7)
minimize log ï£­
exp
Î¸j cj (Xi ) ï£¸ âˆ’
Î¸j cÌ„j (1),
Î¸

Ti =0

j=1

j=1

whereas the maximum likelihood solves
ï£«
ï£¶


p
n
X
X
(8)
minimize
log ï£­1 + exp âˆ’ (2Ti âˆ’ 1)
Î¸j cj (Xi ) ï£¸ .
Î¸

i=1

j=1

It is apparent from (7) and (8) that EB and maximum likelihood use different loss
functions. As a remark, the estimating equations defined by (7) are used to augment
the estimating equations defined by (8) in the covariate balancing propensity score
(CBPS) approach of Imai and Ratkovic (2014). We will compare the empirical
performance of these methods in Section 5.
The optimization problem (7) is strictly convex and the unique solution Î¸Ì‚EB can
be efficiently computed by Newton method. The EB weights (solution to the primal
problem (3)) are given by the Karush-Kuhn-Tucker (KKT) conditions: for any i
such that Ti = 0,
P

p
EB
Î¸Ì‚
c
(X
)
exp
j
i
j
j=1
P
.
(9)
wiEB = P
p
EB c (X )
exp
Î¸Ì‚
p
i
Ti =0
j=1 j
As a final remark, Entropy Balancing bridges two existing approaches of estimating the mean causal effect:
(1) The calibration estimator that is very popular in survey sampling (Deville
and SaÌˆrndal, 1992; SaÌˆrndal and LundstroÌˆm, 2005; Chan et al., 2015);
(2) The empirical likelihood approach that significantly advances the theory of
doubly robust estimation in observation study (Wang and Rao, 2002; Tan,
2006; Qin and Zhang, 2007; Tan, 2010).
EB is a special casePof these two approaches. The main distinction is that it uses the
n
Shannon entropy i=1 wi log wi as the discrepancy function, resulting in an easyto-solve convex optimization. Due to its easy interpretation, Entropy Balancing
has already gained some ground in practice (e.g. Marcus, 2013; Ferwerda, 2014).
4. Properties of Entropy Balancing
We give some theoretical guarantees of Entropy Balancing to justify its usage in
real applications. The following is the main theorem of this paper, which shows EB
is doubly robust even though its original form (3) does not contain a propensity
score model or a outcome regression model.
Theorem 1. Let Assumption 1 (strong ignorability) and Assumption 2 (overlap)
be given. Additionally, assume the expectation of c(x) exists and Var(Y (0)) < âˆ.
Then Entropy Balancing is doubly robust (Property 1) in the sense that
(1) If logit(e(x)) or g0 (x) is linear in cj (x), j = 1, . . . , R, then Î³Ì‚ EB is statistically consistent.

ENTROPY BALANCING

7

(2) Moreover, if logit(e(x)), g0 (x) and g1 (x) are all linear in cj (x), j = 1, . . . , R,
then Î³Ì‚ EB reaches the semiparametric variance bound of Î³ derived in Hahn
(1998, Theorem 1) with unknown propensity score.
We give two proofs of the first claim in Theorem 1. The first proof reveals an
interesting connection between the primal-dual optimization problems (3) and (7)
and the statistical property, double robustness, which motivates the interpretation
in Figure 1. The second proof uses a stabilization trick in Robins et al. (2007).
First proof (sketch). The consistency under the linear model of logit(P(T = 1|X))
is a consequence of the dual optimization problem (7). See Section 3 for a heuristic
justification via the minimum relative entropy principle and Appendix A for a
rigorous proof by using the M-estimation theory.
The consistency
P under the linear model of Y (0) can be proved by expanding
E[Y (0)|X] and Ti =0 wi Yi . Here we provide an indirect proof by showing that augmenting EB with a linear outcome regression does not change the estimator. Given
an estimated propensity score model eÌ‚(x), the corresponding weights eÌ‚(x)/(1âˆ’eÌ‚(x))
for the control units, and an estimated outcome regression model gÌ‚0 (x), a doubly
robust estimator of PATT is given by
(10)

Î³Ì‚ DR =

X eÌ‚(Xi )
X 1
(Yi âˆ’ gÌ‚0 (Xi )).
(Yi âˆ’ gÌ‚0 (Xi )) âˆ’
n1
1 âˆ’ eÌ‚(Xi )
Ti =0

Ti =1

This estimator satisfies Property 1, i.e. if eÌ‚(x) â†’ e(x) or gÌ‚0 (x) â†’ g(x), then Î³Ì‚ DR is
statistically consistent for Î³. To see this, in the case that gÌ‚0 (x) â†’ g0 (x), the first
sum in (10) is consistent for Î³ and the second sum in (10) has mean going to 0 as
n â†’ âˆ. In the case where gÌ‚0 (x) 6â†’ g0 (x) but eÌ‚(x) â†’ e(x), the second sum in (10)
is consistent for the bias of the first sum (as an estimator of Î³).
When the estimated propensity score model eÌ‚(x) is obtained by the EB dual
Pp
problem (7) and the estimated outcome regression model is gÌ‚0 (x) = j=1 Î²Ì‚j cj (x),
we have
X
1 X
Î³Ì‚ DR âˆ’ Î³Ì‚ EB =
wiEB gÌ‚0 (Xi ) âˆ’
gÌ‚0 (Xi )
n1
Ti =0

=

X

Ti =0

Ti =0

=

p
X
j=1

p
1 XX
Î²Ì‚j cj (Xi )
n1
j=1
Ti =1 j=1
!
X
1 X
EB
cj (Xi )
wi cj (Xi ) âˆ’
n1

wiEB

Î²Ì‚j

p
X

Ti =0

Î²Ì‚j cj (Xi ) âˆ’

Ti =1

= 0.
Therefore by enforcing covariate balancing constraints, EB implicitly fits a linear
outcome regression model and is consistent for Î³ under this model.

Second proof. This proof is pointed out by an anonymous reviewer. In a discussion
of Kang and Schafer (2007), Robins et al. (2007) show that one can stabilize the
standard doubly robust estimator in a number of ways. Specifically, one trick
suggested by Robins et al. (2007, Section 4.1.2) is to estimate the propensity score,

8

ENTROPY BALANCING

say eÌƒ(x), by the following estimating equation

n 
X
Ti
(1 âˆ’ Ti )eÌƒ(Xi )/(1 âˆ’ eÌƒ(Xi ))
Pn
âˆ’ Pn
gÌ‚0 (Xi ) = 0.
(11)
i=1 (1 âˆ’ Ti )eÌƒ(Xi )/(1 âˆ’ eÌƒ(Xi ))
i=1 Ti
i=1
Then one can estimate PATT by the IPW estimator (2) by replacing eÌ‚(Xi ) with
eÌƒ(Xi ). This estimator is sample bounded (the estimator is always within the range
of observed values of Y ) and doubly robust with respect to the parametric specifications of eÌƒ(x) = eÌƒ(x; Î¸) and gÌ‚0 (x) = gÌ‚0 (x; Î²). The only problem with (11) is it
may not have a unique solution. However, when logit(e(x)) and g0 (x) are assumed
linear in c(x), (11) corresponds to the first order condition of the EB dual problem
(7). Since (7) is strictly convex, it has an unique solution and eÌƒ(X; Î¸) is the same
as the EB estimate eÌ‚(X; Î¸). As a consequence, Î³Ì‚ EB is also doubly robust.

To prove the second claim in Theorem 1, we compute the asymptotic variance
of Î³Ì‚ EB using the M-estimation theory. To state our results, we need to introduce
four differently weighted covariance-like functions for two random vectors a1 and
a2 of length p:
Ha1 ,a2 = Cov(a1 , a2 |T = 1),


e(X)
(a1 âˆ’ E[a1 |T = 1])(a2 âˆ’ E[a2 |T = 1])T T = 1 ,
Ga1 ,a2 = E
1 âˆ’ e(X)
Ka1 ,a2 = E[(1 âˆ’ e(X))a1 aT2 |T = 1],
Kam1 ,a2 = E[(1 âˆ’ e(X))a1 (a2 âˆ’ E[a2 |T = 1])T |T = 1].
It is obvious that H â‰¥ K and usually G â‰¥ H. To make the notation more
concise, c(X) will be abbreviated as c and Y (0) as 0 in subscripts. For example,
Hc,0 = Hc(X),Y (0) , Gc,1 = Gc(X),Y (1) and Kc = Kc(X),c(X) .
Theorem 2. Assume the logistic regression model of propensity score is correct,
i.e. logit(P(T = 1|X)) is a linear combination of {cj (X)}pj=1 . Let Ï€ = P(T = 1),
d

d

then we have Î³Ì‚ EB â†’ N(Î³, V EB /n) and Î³Ì‚ IPW â†’ N(Î³, V IPW /n) where


T
(12) V EB = Ï€ âˆ’1 Â· H1 + G0 âˆ’ Hc,0
Hcâˆ’1 2Gc,0 âˆ’ Hc,0 âˆ’ Gc Hcâˆ’1 Hc,0 + 2Hc,1 ,


T
m
m
(13) V IPW = Ï€ âˆ’1 Â· H1 + G0 âˆ’ Hc,0
Kcâˆ’1 Hc,0 âˆ’ 2Kc,0
+ 2Kc,1
.
The proof of Theorem 2 is given in Appendix A. The H, G and K matrices in
Theorem 2 can be estimated from the observed data, yielding approximate sampling
variances for Î³Ì‚ EB and Î³Ì‚ IPW . Alternatively, variance estimates may be obtained via
the empirical sandwich method (e.g. Stefanski and Boos, 2002). In practice (particularly in simulations where we compare to a known truth), we find that the
empirical sandwich method is more stable than the plug-in method, which is consistent with the suggestion in Lunceford and Davidian (2004) for PATE estimators.
To complete the proof of the second claim in Theorem 1, we compare these
variances with the semiparametric variance bound of Î³ with unknown e(X) derived
by Hahn (1998, Theorem 1):


1
e(X)2
Var(Y (0)|X) + e(X)(g1 (X) âˆ’ g0 (X) âˆ’ Î³)2
V âˆ— = 2 E e(X)Var(Y (1)|X) +
Ï€
1 âˆ’ e(X)
After some algebra, one can express V âˆ— in terms of HÂ·,Â· and GÂ·,Â· defined above:
V âˆ— = Ï€ âˆ’1 Â· {H1 + G0 âˆ’ 2Hg0 ,g1 âˆ’ Gg0 + Hg0 } .

ENTROPY BALANCING

9

Now assume logit(P(T = 1|X)) = Î¸T c(X) and E[Y (t)|X] = Î²(t)T c(X), t = 0, 1, it
is easy to verify that
Hc,t = Cov(c(X), Y (t)|T = 1)
= Cov(c(X), Î²(t)T c(X)|T = 1)
= Hc Î²(t), for t = 0, 1.
Similarly, Gc,t = Gc Î²(t), t = 0, 1. From here it is easy to check V EB and V âˆ— are the
same. Since Entropy Balancing reaches the efficiency bound in this case, obviously
V EB < V IPW when both models are linear.
If logit(P(T = 1|X)) = Î¸T c(X) is true but E[Y (t)|X] = Î²(t)T c(X) is not true for
some t = 0, 1, there is no guarantee that EB has the smaller asymptotic variance.
In practice, the features c(X) in the outcome regression models are almost always
correlated with Y . This correlation compensates the slight efficiency loss of not
maximizing the likelihood function in logistic regression. As a consequence, the
variance V EB in (12) is usually smaller than V IPW in (13). This efficiency advantage
of EB over IPW is verified in the next section using simulations.
5. Simulations
5.1. Kang-Schafer Example. We use the simulation example in Kang and Schafer
(2007) to compare EB weighting with IPW (after maximum likelihood logistic regression) and the over-identified Covariate Balancing Propensity Score (CBPS) proposed by Imai and Ratkovic (2014). The simulated data consist of {Xi , Zi , Ti , Yi }, i =
1, . . . , n}. Xi and Ti are always observed, Yi is observed only if Ti = 1, and Zi is
never observed. To generate this data set, Xi is distributed as N(0, I4 ), Zi is computed by first applying the following transformation:
Zi1 = exp(Xi1 /2),
Zi2 = Xi2 /(1 + exp(Xi1 )) + 10,
Zi3 = (Xi1 Xi3 + 0.6)3 ,
Zi4 = (Xi2 + Xi4 + 20)2 .
Next we normalize each column such that Zi has mean 0 and standard deviation 1.
In one setting, Yi is generated by Yi = 210 + 27.4Xi1 + 13.7Xi2 + 13.7Xi3 +
13.7Xi4 + i , i âˆ¼ N(0, 1) and the true propensity scores are ei = expit(âˆ’Xi1 +
0.5Xi2 âˆ’ 0.25Xi3 âˆ’ 0.1Xi4 ). In this case, both Y and T can be correctly modeled
by (generalized) linear model of the observed covariates X.
In the other settings, at least one of the propensity score model and the outcome
regression model is incorrect. In order to achieve this, the data generating process
described above is altered such that Y or T (or both) is linear in the unobserved Z
instead of the observed X, though the parameters are kept the same.
For each setting (4 in total), we generated 1000 simulated data sets of size
n = 200 and 1000, then apply various methods discussed earlier including
(1) IPW, CBPS: the IPW estimator in (2) with propensity score estimated by
logistic regression or CBPS (since the estimand is overall mean, we use the
CBPS weights tailored for estimating PATE);
(2) EB: the Entropy Balancing estimator (the EB weights are used to estimate
the unobserved mean E[Y |T = 0]);

10

ENTROPY BALANCING

(3) IPW+DR, CBPS+DR: the doubly robust estimator in (10) with propensity
score estimated by logistic regression or CBPS.
The simulation results are presented in Figures 2 and 3. Figures 2a and 3a
show the covariate imbalance before adjustment in terms of standardized difference.
Figures 2b and 3b show the mean estimates given by the five different methods.
First, notice that the doubly robust estimator â€œIPW+DRâ€ performs poorly when
both models are misspecified (bottom-right panel in Figures 2b and 3b). In fact, all
the three doubly robust methods are worse than just using IPW. Second, the three
doubly robust estimators have exactly the same variance if the Y model is correct
(top two panels in Figures 2b and 3b). It seems that how one fits the propensity
score model has no impact on the final estimate. This is related to the observation
in Kang and Schafer (2007) that, in this example, the plain OLS estimate of Y
actually outperforms any method involving the propensity score model. Discussion
articles such as Robins et al. (2007) and Ridgeway and McCaffrey (2007) find this
phenomenon very uncommon in practice and is most likely due to the estimated
inverse probability weights are highly variable, which is a bad setting for doubly
robust estimators.
Regarding Entropy Balancing (EB), we find that:
(1) If both T and Y models are misspecified, EB has smaller bias than the
conventional â€œIPW+DRâ€ or â€œCBPS+DRâ€. So EB seems to be less affected
by such unfavorable setting.
(2) When T model is correct but Y model is wrong (bottom-left panel in Figures
2 and 3), EB has the smallest variance among all estimators. This supports
the conclusion of our efficiency comparison of IPW and EB in Section 4.
Finally notice that the same simulation setting is used in Tan (2010) to study
the performance of a number of doubly robust estimators. The reader can compare
the Figures 2 and 3 with the results there. The performance of Entropy Balancing
is comparable to the best estimator in Tan (2010).
5.2. Lunceford-Davidian Example. We provide another simulation example by
Lunceford and Davidian (2004) to verify claims in Theorems 1 and 2. In this
simulation, the data still consist of {(Xi , Zi , Ti , Yi ), i = 1, . . . , n}, but all of them
are observed. Both Xi and Zi are three dimensional vectors. The propensity score
is only related to X through:
logit(P(Ti = 1)) = Î²0 +

X

Î²j Xij .

j=1

Note the above does not involve elements of Zi . The response Y is generated
according to
Yi = Î½0 +

3
X
j=1

Î½j Xij + Î½4 Ti +

3
X

Î¾j Zij + i ; i âˆ¼ N(0, 1).

j=1

The parameters here are set to be Î½ = (0, âˆ’1, 1, âˆ’1, 2)T , and Î² is set as:
Î² no = (0, 0, 0, 0)T ,
Î² moderate = (0, 0.3, âˆ’0.3, 0.3)T , or
Î² strong = (0, 0.6, âˆ’0.6, 0.6)T .

ENTROPY BALANCING

(a) Covariate imbalance before adjustment.

(b) Mean estimates. The methods are: Inverse Propensity Weighting
(IPW), Covariate Balancing Propensity Score (CBPS), Entropy Balancing (EB), and doubly robust versions of the first two (IPW+DR,
CBPS+DR). Target mean is 210 and is marked as a black horizontal
line to compare the biases. Numbers printed at Y = 230 are the sample
standard deviations to compare efficiency.

Figure 2. Kang-Schafer example: sample size n = 200. Both
propensity score model and outcome regression model can be correct or incorrect, so there are four scenarios in total. We generate
1000 simulations in each scenario.

11

12

ENTROPY BALANCING

(a) Covariate imbalance before adjustment.

(b) Mean estimates. The methods are: Inverse Propensity Weighting
(IPW), Covariate Balancing Propensity Score (CBPS), Entropy Balancing (EB), and doubly robust versions of the first two (IPW+DR,
CBPS+DR). Target mean is 210 and is marked as a black horizontal
line to compare the biases. Numbers printed at Y = 230 are the sample
standard deviations to compare efficiency.

Figure 3. Kang-Schafer example: sample size n = 1000. Both
propensity score model and outcome regression model can be correct or incorrect, so there are four scenarios in total. We generate
1000 simulations in each scenario.

ENTROPY BALANCING

13

The choice of Î² depends on the level of association of T and X. Î¾ is based on a
similar choice on the level of association of Y and Z:
Î¾ no = (0, 0, 0)T ,
Î¾ moderate = (âˆ’0.5, 0.5, 0.5)T , or
Î¾ strong = (âˆ’1, 1, 1)T .
The joint distribution of (Xi , Zi ) is specified by taking Xi3 âˆ¼ Bernoulli(0.2) and
then generate Zi3 as Bernoulli with
P (Zi3 = 1|Xi3 ) = 0.75Xi3 + 0.25(1 âˆ’ Xi3 ).
Conditional on Xi3 , (Xi1 , Zi1 , Xi2 , Zi2 ) is then generated as multivariate normal
N(aXi3 , BXi3 ), where a1 = (1, 1, âˆ’1, âˆ’1)T , a0 = (âˆ’1, âˆ’1, 1, 1)T and
ï£«
ï£¶
1
0.5 âˆ’0.5 âˆ’0.5
ï£¬ 0.5
1
âˆ’0.5 âˆ’0.5ï£·
ï£·.
B0 = B1 = ï£¬
ï£­âˆ’0.5 âˆ’0.5
1
0.5 ï£¸
âˆ’0.5 âˆ’0.5 0.5
1
Figure 4 shows the covariate imbalance in the three settings before adjustment.

Figure 4. Lunceford-Davidian Example: covariate imbalance before adjustment.
The data generating model implies that the true PATT is Î³ = 2. Since the
outcome Y depends on both X and Z, we always fit a full linear model of Y using
X and Z, if such model is needed. T only depends on X, so it is not necessary to
include Z in propensity score modeling. However, as pointed out by Lunceford and
Davidian (2004, Sec. 3.3), it is actually beneficial to â€œovermodelâ€ the propensity
score by including Z in the model. Here we will try both possibilities, the â€œfullâ€
modeling of T using both X and Z, and the â€œpartialâ€ modeling of T using only X.

14

ENTROPY BALANCING

Since the estimand is PATT in this case, we use the over-identified CBPS weights
tailored for estimating PATT.

Figure 5. Results of the Lunceford-Davidian example (full
propensity score modeling). The propensity score model and outcome regression model, if applies, are always correctly specified,
but the level of association between T or Y with X or Z could be
different, ended up with 9 different scenarios. X are confounding
covariates and Z only affects the outcome. We generate 1000 simulations of 1000 in each scenario and apply five different estimators.
The true PATT is 2 and is marked as a black horizontal line to
compare the biases of the methods. Numbers printed at Y = 5 are
the sample standard deviation of each method, in order to compare
their efficiency.
We generated 1000 simulated data sets and the results are shown in Figure 5
for â€œfullâ€ propensity score modeling and Figure 6 for â€œpartialâ€ propensity score
modeling. We make the following comments about these two plots:

ENTROPY BALANCING

15

Figure 6. Results of the Lunceford-Davidian example (partial
propensity score modeling). The settings are exactly the same as
Figure 5 except the methods here donâ€™t use Z in their propensity
score models.

(1) IPW and all other estimators are always consistent, no matter what level
of association is specified. This is because the propensity score model is
always correctly specified.
(2) When using the â€œfullâ€ propensity score modeling, all doubly robust estimators (EB, IPW+DR, CBPS+DR and EB+DR) have almost the same
sample variance. This is because all of them are asymptotically efficient.
(3) CBPS, to our surprise, does not perform very well in this simulation. It has
smaller variance than IPW but this comes with the price of some bias. If
we use the partial propensity score model (only involve X, Figure 6), this
bias is smaller but still not negligible. While it is not clear what causes
this bias, one possible reason is that the optimization problem of CBPS
is nonconvex, so the local solution which is used to construct Î³ estimator

16

ENTROPY BALANCING

could be far from the global solution. Another possibility is that CBPS
uses GMM or Empirical Likelihood to combine likelihood with imbalance
penalty, which is less efficient than maximum likelihood directly. Thus,
although the estimator is asymptotically unbiased, the convergence spend
to the true Î³ is quite slower than IPW. CBPS combined with outcome
regression (CB+DR) fixes the bias and inefficiency issue occurred in CBPS
without outcome regression.
(4) EB, in contrast, performs quite well in this simulation. It has relatively
small variance, particularly if we use the â€œfullâ€ model in which both X and
Z are balanced.
(5) The difference between EB and EB+DR is that while EB only balances
â€œpartialâ€ or â€œfullâ€ covariates, EB+DR additionally combines a outcome
linear regression model on all the covariates. As shown in the first proof
of Theorem 1, when the â€œfullâ€ covariates are used, EB is exactly the same
as EB+DR. We can observe this from Figure 5. When EB only balances
â€œpartialâ€ covariates, the two methods are different and indeed EB+DR is
more efficient in Figure 6 since it fits the correct Y model.
(6) Using the â€œfullâ€ propensity score model improves the efficiency of pure
weighting estimators (IPW, CBPS and EB) a lot, but has very little impact
on estimators that involves an outcome regression model (IPW+DR and
CBPS+DR) compared to â€partialâ€ propensity score modeling. Although
EB could be viewed as fitting a outcome model implicitly, the â€partialâ€ EB
estimator only uses X in the outcome model, that is precisely the reason
why it is not efficient. Thus there are both robustness and efficiency reasons
that one should include all relevant covariates in EB, even if the covariates
affect only one of T and Y .
In summary, EB outperforms IPW in all the simulations, making it an appealing
alternative to the conventional propensity score weighting methods.
Appendix A. Theoretical proofs
We first describe the conditions under which the EB problem (3) admits a solution. The existence of wEB depends on the solvability of the moment matching
constraints
X
X
(14)
wi cj (Xi ) = cÌ„j (1), j = 1, . . . , p, w > 0,
wi = 1.
Ti =0

Ti =0

As one may expect, this is closely related to the existence condition of maximum
likelihood estimate of logistic regression (Silvapulle, 1981; Albert and Andersen,
1984). An easy way to obtain such condition is through the dual problem of (8)
maximize
w

(15)

subject to

âˆ’

n
X

[wi log wi + (1 âˆ’ wi ) log(1 âˆ’ wi )]

i=1

X
Ti =0

wi cj (Xi ) =

X

wi cj (Xi ), j = 1, . . . , p,

Ti =1

0 < wi < 1, i = 1, . . . , n.
Thus, the existence of Î¸Ì‚MLE is equivalent to the solvability of the constraints in
(15), which is the overlap condition first given by Silvapulle (1981).

ENTROPY BALANCING

17

Intuitively, in the space of c(X), the solvability of (14) or the existence of wEB
means there is no hyperplane separating {c(Xi )}Ti =0 and cÌ„(1). In contrast, the
solvability of (15) or the existence of wMLE means there is no hyperplane separating {c(Xi )}Ti =0 and {c(Xi )}Ti =1 . Hence the existence of EB requires a stronger
condition than the logistic regression MLE.
The next proposition suggests that the existence of wEB and hence wMLE is
guaranteed by Assumption 2 (overlap) with high probability.
Proposition 1. Suppose Assumption 2 (overlap) is satisfied and
expectation
Pthe
n
of c(X) exist, then P(wEB exists) â†’ 1 as n â†’ âˆ. Furthermore, i=1 (wiEB )2 â†’ 0
in probability as n â†’ âˆ.
Proof. Since the expectation of c(X) exist, the weak law of large number says
p
cÌ„(1) â†’ cÌ„âˆ— (1) = E[c(X)|T = 1]. Therefore
Lemma 1. For any  > 0, P(kcÌ„(1) âˆ’ cÌ„âˆ— (1)kâˆ â‰¥ ) â†’ 0 as n â†’ âˆ.
Now condition on kcÌ„(1) âˆ’ cÌ„âˆ— (1)kâˆ â‰¥ , i.e. cÌ„(1) is in the box of side length 2
centered at cÌ„âˆ— (1), we
P want to prove that
P with probability going to 1 there exists w
such that wi > 0, Ti =0 wi = 1 and Ti =0 wi c(Xi ) = cÌ„(1). Equivalently, this is
saying the convex hull generated by {c(Xi )}Ti =0 contains cÌ„(1). We indeed prove a
stronger result:
Lemma 2. With probability going to 1 the convex hull generated by {c(Xi )}Ti =0
contains the box B (cÌ„âˆ— (1)) = {c(x) : kc(x) âˆ’ cÌ„âˆ— (1)kâˆ â‰¤ } for some  > 0.
Proposition 1 follow immediately from Lemma 1 and Lemma 2. Now we prove
Lemma 2. Denote the sample space of X by â„¦(X). Assumption 2 (overlap) implies
cÌ„âˆ— (1) hence B (cÌ„âˆ— (1)) is in the interior of the convex hull of â„¦(X) for sufficiently
small . Let Ri , i = 1, . . . , 3p , be the 3p boxes centered at cÌ„âˆ— (1) + 23 b, where b âˆˆ Rp
is a vector that each entry can be âˆ’1, 0, or 1. It is easy to check that the sets Ri are
p
disjoint and the convex hull of {xi }3i=1 contains B (cÌ„âˆ— (1)) if xi âˆˆ Ri , i = 1, . . . , 3p .
Since 0 < P (T = 0|X) < 1, Ï = mini P(X âˆˆ Ri |T = 0) > 0. This implies
p

p

P(âˆƒXi âˆˆ Ri and Ti = 0, âˆ€i = 1, . . . , 3 ) â‰¥ 1 âˆ’
(16)

3
X

P(X 6âˆˆ Ri |T = 0)n

i=1

â‰¥ 1 âˆ’ 3p (1 âˆ’ Ï)n
â†’1

as n â†’ âˆ. This proves the lemma because the event in the left hand side implies
the convex hull generated by {c(Xi )}Ti =0 contains the desired box. Note that (16)
also tells us how many samples we actually need to ensure the existence of wEB .
Indeed if n â‰¥ Ïâˆ’1 (p log 2 + log Î´ âˆ’1 ) â‰¥ log(1âˆ’Ï) (Î´2âˆ’p ), then the probability in (16)
is greater than 1 âˆ’ Î´. Usually we expect Î´ = O(3âˆ’p ). If this is the case, the number
of samples needed is n = O(p Â· 3p )1.
P
p
Now we turn to the second claim of the proposition, i.e. Ti =0 wi2 â†’ 0. To prove
this, we only need to find a sequence (with respect to growing n) of feasible solutions
to (3) such that maxi wi â†’ 0. This is not hard to show, because the probability in
(16) is exponentially decaying as n increases. We can pick n1 â‰¥ N (Î´, p, Ï) such that
1Note that this naive rate can actually be greatly improved by Wendelâ€™s theorem in geometric
probability theory.

18

ENTROPY BALANCING

1
contains B (cÌ„âˆ— (1)) is at least 1âˆ’Î´, then
the probability of the convex hull of {xi }ni=1
ni+1
i
contains B (cÌ„âˆ— (1))
pick ni+1 â‰¥ ni + 3 N (Î´, p, Ï) so the convex hull of {xi }i=n
i +1
ni+1
i
with probability at least 1 âˆ’ 3 Î´. This means for each {xi }i=ni +1 , i = 0, 1, . . .,
Pni+1
ni+1
such that i=n
wÌƒi xi = cÌ„(1). Now suppose
we have a set of weights {wÌƒi }i=n
i +1
i +1
nk â‰¤ n < nk+1 , the choice wi = wÌƒi /k if i â‰¤ nk and wi = 0 if i > nk satisfies
the
and maxi wi â‰¤ k. As n â†’ âˆ, this implies maxi wi â†’ 0 and hence
P constraints
2

i wi â†’ 0 with probability tending to 1.

Now we turn to the main theorem of the paper (Theorem 1). The first claim in
Theorem 1 follows immediately from the following lemma:
Lemma 3. Under the assumptions in Theorem 1 and suppose logit(P [T = 1|X]) =
Pp
EB p
âˆ—
â†’ Î¸âˆ— . As a consequence,
j=1 Î¸j cj (X), then as n â†’ âˆ, Î¸Ì‚
"
#
X
p
EB
E
wi Yi â†’ E[Y (0)|T = 1].
Ti =0

Proof. The proof is a standard application of M-estimation (more precisely Zestimation) theory. We will follow the estimating equations approach described
in (Stefanski and Boos, 2002) to derive consistency of Î¸Ì‚EB . First we note the first
order optimality condition of (7) is
(17)

n
X

(1 âˆ’ Ti )e

Pp
k=1

Î¸k ck (Xi )

(cj (Xi ) âˆ’ cÌ„j (1)) = 0, j = 1, . . . , R.

i=1

We can rewrite (17) as estimating equations.PLet Ï†j (X, T ; m) = T (cj (X)âˆ’mj ), j =
p
1, . . . , R and Ïˆj (X, T ; Î¸, m) = (1 âˆ’ T ) exp{ k=1 Î¸k ck (X)}(cj (X) âˆ’ mj ), then (17)
is equivalent to
n
X
Ï†j (Xi , Ti ; m) = 0, j = 1, . . . , R,
(18)

i=1
n
X

Ïˆj (Xi , Ti ; Î¸, m) = 0, j = 1, . . . , R.

i=1

Since Ï†(Â·) and Ïˆ(Â·) are all smooth functions of Î¸ and m, all we need to verify is
that mâˆ—j = E[cj (X)|T = 1] and Î¸âˆ— is the unique solution to the population version
of (18). It is obvious that mâˆ— is the solution to E[Ï†j (X, T ; m)] = 0, j = 1, . . . , R.
Now take conditional expectation of Ïˆj given X:
Pp

E[Ïˆj (X, T ; Î¸, mâˆ— ) | X] = (1 âˆ’ e(X))e

Pp

=

k=1

(cj (X) âˆ’ mâˆ—j )
!

Î¸k ck (X)

âˆ—

e k=1 Î¸k ck (X)
Pp
1âˆ’
âˆ—
1 + e k=1 Î¸k ck (X)

e

Pp
k=1

Î¸k ck (X)

Pp

(cj (X) âˆ’ mâˆ—j )

e k=1 Î¸k ck (X)
Pp
=
(cj (X) âˆ’ E[cj (X)|T = 1]).
âˆ—
1 + e k=1 Î¸k ck (X)
The only way to make E[Ïˆj (X, T ; Î¸Ì‚, mâˆ— )] = 0 is to have
Pp

e k=1 Î¸Ì‚k ck (X)
Pp
= const Â· P(T = 1|X),
âˆ—
1 + e k=1 Î¸k ck (X)
i.e. Î¸Ì‚ = Î¸âˆ— . This proves the consistency of Î¸Ì‚EB .

ENTROPY BALANCING

19

The consistency of Î³Ì‚ EB is proved by noticing
Pp
exp( j=1 Î¸Ì‚jEB cj (Xi ))
P (Ti = 1|Xi )
p
EB
â†’
wi = P
,
Pp
EB
1 âˆ’ P (Ti = 1|Xi )
Ti =0 exp(
j=1 Î¸Ì‚j cj (Xi ))
which is the IPW-NR weight defined in (2).



The second claim is a corollary of Theorem 2, which is proved below. For simplicity we denote Î¾ = (mT , Î¸T , Âµ(1|1), Î³)TPand the true parameter as Î¾ âˆ— . Throughout
p
this section we assume logit(e(X)) = j=1 Î¸jâˆ— cj (X). Denote cÌƒ(X) = c(X) âˆ’ cÌ„âˆ— (1),
P
p
eâˆ— (X) = e(X; Î¸âˆ— ), lâˆ— (X) = exp{ j=1 Î¸jâˆ— cj (X)} = eâˆ— (X)/(1 âˆ’ eâˆ— (X)). Let
Ï†j (X, T ; m) = T (cj (X) âˆ’ mj ), j = 1, . . . , p,
Ïˆj (X, T ; Î¸, m) = (1 âˆ’ T )e

Pp
k=1

Î¸k ck (X)

(cj (X) âˆ’ mj ), j = 1, . . . , p,

Ï•1|1 (X, T, Y ; Âµ(1|1)) = T (Y âˆ’ Âµ(1|1)),
Ï•(X, T, Y ; Î¸, Âµ(1|1), Î³) = (1 âˆ’ T )e

Pp

j=1

Î¸j cj (X)

(Y + Î³ âˆ’ Âµ(1|1)),

and Î¶(X, T, Y ; m, Î¸, Âµ(1|1), Î³) = (Ï†T , Ïˆ T , Ï•1|1 , Ï•)T be all the estimating equations.
The Entropy Balancing estimator Î³Ì‚ EB is the solution to
n

(19)

1X
Î¶(Xi , Ti , Yi ; m, Î¸, Âµ(1|1), Î³) = 0.
n i=1

There are two forms of â€œinformationâ€ matrix that need to be computed. The
first is


âˆ‚
AEB (Î¾ âˆ— ) = E âˆ’ T Î¶(X, T, Y ; Î¾ âˆ— )
âˆ‚Î¾
 
 
 

 
âˆ‚
âˆ‚
âˆ‚
âˆ‚
âˆ—
âˆ—
âˆ—
âˆ—
Î¶(Î¾
)
E
âˆ’
Î¶(Î¾
)
E
âˆ’
Î¶(Î¾
)
E
âˆ’
Î¶(Î¾
)
= E âˆ’
âˆ‚mT
âˆ‚Î¸T
âˆ‚Âµ(1|1)
âˆ‚Î³
ï£®ï£«
ï£¶ï£¹
T Â·IR
0
0
0
âˆ—
âˆ—
âˆ—
T
ï£¯ï£¬(1âˆ’T )l (X)Â·IR âˆ’(1âˆ’T )l (X)(c(X)âˆ’cÌ„ (1))c(X)
ï£·ï£º
0
0
ï£¬
ï£·ï£º
= Eï£¯
ï£°ï£­
ï£¸ï£»
0T
0T
T
0
âˆ—
âˆ—
T
âˆ—
âˆ—
0
âˆ’(1âˆ’T )l (X)(Y (0)âˆ’Âµ (0|1))c(X) (1âˆ’T )l (X) âˆ’(1âˆ’T )l (X)
ï£«
ï£¶
IR
0
0 0
ï£¬ IR
âˆ’Cov[c(X)|T = 1]
0 0 ï£·
ï£·.
= Ï€Â· ï£¬
ï£­0T
0T
1 0 ï£¸
0 âˆ’Cov(Y (0), c(X)|T = 1) 1 âˆ’1

A very useful identity in the computation of the expectation is
E[f (X, Y )|T = 1] = Ï€ âˆ’1 E[e(X)f (X, Y )]


P(T = 0)
e(x)
=
Â·E
f (X, Y ) T = 0 .
Ï€
1 âˆ’ e(X)
The second information matrix is the covariance of Î¶(X, T, Y ; Î¾ âˆ— ). Denote YÌƒ (t) =
Y (t) âˆ’ Âµâˆ— (t|1), t = 0, 1
B EB (Î¾ âˆ— ) = E[Î¶(X, Y, T ; Î¾ âˆ— )Î¶(X, Y, T ; Î¾ âˆ— )T ]
ï£®ï£«
ï£¶ï£¹
T cÌƒ(X)cÌƒ(X)T
0
T YÌƒ (1)cÌƒ(X)
0
âˆ—
2
T
âˆ—
2
ï£¯ï£¬
ï£º
0
(1âˆ’T )l (X) cÌƒ(X)cÌƒ(X)
0
(1âˆ’T )l (X) YÌƒ (0)cÌƒ(X)ï£·
ï£¬
ï£·ï£º .
= Eï£¯
ï£°ï£­T YÌƒ (1)cÌƒ(X)T
ï£¸ï£»
0T
T YÌƒ 2 (1)
0
0
(1âˆ’T )lâˆ— (X)2 YÌƒ (0)cÌƒ(X)T
0
(1âˆ’T )lâˆ— (X)2 YÌƒ 2 (0)

20

ENTROPY BALANCING

The asymptotic distribution of Î³Ì‚ EB is N(Î³, V EB (Î¾ âˆ— )/n) where V EB (Î¾ âˆ— ) is the
bottom right entry of AEB (Î¾ âˆ— )âˆ’1 B EB (Î¾ âˆ— )AEB (Î¾ âˆ— )âˆ’T . Letâ€™s denote
Ha1 ,a2 = Cov(a1 , a2 |T = 1),

T
Ga1 ,a2 = E lâˆ— (X)(a1 âˆ’ E[a1 |T = 1])(a2 âˆ’ E[a2 |T = 1])T |T = 1 ,
and Ha = Ha,a , Ga = Ga,a . So
ï£«

IR
ï£¬
IR
AEB (Î¾ âˆ— ) = Ï€ Â· ï£¬
ï£­0T
0
ï£«
ï£¬
AEB (Î¾ âˆ— )âˆ’1 = Ï€ âˆ’1 Â· ï£¬
ï£­

0
âˆ’Hc(X)
0T
âˆ’HY (0),c(X)

IR
âˆ’1
Hc(X)
0T

âˆ’1
T
âˆ’Hc(X),Y
(0) Hc(X)

ï£¶
0 0
0 0ï£·
ï£·,
1 0ï£¸
1 âˆ’1

0
âˆ’1
âˆ’Hc(X)
T
0
âˆ’1
T
Hc(X),Y
(0) Hc(X)

ï£¶
0 0
0 0ï£·
ï£·,
1 0ï£¸
1 âˆ’1

and
ï£«

Hc(X)
ï£¬
0
EB âˆ—
B (Î¾ ) = Ï€ Â· ï£¬
ï£­HY (1),c(X)
0

0
Gc(X)
0T
GTY (0),c(X)

Hc(X),Y (1)
0
HY (1)
0

0

ï£¶

Gc(X),Y (0) ï£·
ï£·.
ï£¸
0
GY (0)

Thus
 T âˆ’1

V EB = Ï€ âˆ’1 Â· Hc,0
Hc Hc,0 + Gc Hcâˆ’1 Hc,0 âˆ’ 2Gc,0 âˆ’ 2Hc,1 + H1 + G0 .
It would be interesting to compare V EB (Î¾ âˆ— ) with V IPW (Î¾ âˆ— ), the asymptotic variance of Î³Ì‚ IPW . The IPW PATT estimator (2) is equivalent to solving the following
estimating equations

n 
X
1
P
Ti âˆ’
cj (Xi ) = 0, r = 1, . . . , R,
p
1 + eâˆ’ k=1 Î¸k ck (Xi )
i=1
n

1X
Ï•1|1 (Xi , Ti , Yi ; Î¸, Âµ(1|1), Î³) = 0,
n i=1
n

1X
Ï•(Xi , Ti , Yi ; Î¸, Âµ(1|1), Î³) = 0.
n i=1
If we call Ka1 ,a2 = E[(1 âˆ’ e(X))a1 aT2 |T = 1], we have
ï£®ï£« âˆ—
ï£¶ï£¹
e (X)(1âˆ’eâˆ— (X))c(X)c(X)T
0
0
T
ï£°
ï£­
ï£¸ï£»
0
T
0
A
(Î¾ ) = E
âˆ’(1âˆ’T )lâˆ— (X)YÌƒ (0)c(X)T
(1âˆ’T )lâˆ— (X) âˆ’(1âˆ’T )lâˆ— (X)
ï£«
ï£¶
Kc(X)
0 0
= Ï€Â· ï£­
0T
1 0 ï£¸.
âˆ’HY (0),c(X) 1 âˆ’1
ï£«
ï£¶
âˆ’1
Kc(X)
0 0
ï£·
IPW âˆ— âˆ’1
âˆ’1 ï£¬
T
A
(Î¾ )
= Ï€ Â·ï£­
0
1 0 ï£¸.
âˆ’1
âˆ’HY (0),c(X) Kc(X) 1 âˆ’1
IPW

âˆ—

ENTROPY BALANCING

21

Let q âˆ— (X) = eâˆ— (X)lâˆ— (X),
ï£¶ï£¹
(T âˆ’eâˆ— (X))2 c(X)c(X)T T (T âˆ’eâˆ— (X))YÌƒ (1)c(X) âˆ’(1âˆ’T )q âˆ— (X)YÌƒ (0)c(X)
âˆ—
T
2
ï£¸ï£»
(Î¾ ) = E ï£°ï£­T (T âˆ’e (X))YÌƒ (1)c(X)
T YÌƒ (1)
0
âˆ’(1âˆ’T )q âˆ— (X)YÌƒ (0)c(X)
0
(1âˆ’T )lâˆ— (X)2 YÌƒ 2 (0)
ï£«
ï£¶
Kc(X)
Kc(X),YÌƒ (1) Kc(X),YÌƒ (0) âˆ’Hc(X),Y (0)
T
ï£¬
ï£·
K
HY (1)
0
= Ï€Â· ï£­
ï£¸.
ï£®ï£«

B

IPW

âˆ—

KT

c(X),YÌƒ (1)
T
âˆ’Hc(X),Y
(0)

c(X),YÌƒ (0)

0

GY (0)

V IPW can thus be computed consequently and the details are omitted.
References
Abadie, A. and G. W. Imbens (2006). Large sample properties of matching estimators for average treatment effects. Econometrica 74 (1), 235â€“267.
Albert, A. and J. A. Andersen (1984). On the existence of maximum likelihood
estimates in logistic regression models. Biometrika 71 (1), 1â€“10.
Bang, H. and J. M. Robins (2005). Doubly robust estimation in missing data and
causal inference models. Biometrics 61 (4), 962â€“973.
Chan, K. C. G., S. C. P. Yam, and Z. Zhang (2015). Globally efficient nonparametric inference of average treatment effects by empirical balancing calibration
weighting. Journal of Royal Statistical Society, Series B (Methodology) to appear.
Cover, T. M. and J. A. Thomas (2012). Elements of information theory. John
Wiley & Sons.
Deville, J.-C. and C.-E. SaÌˆrndal (1992). Calibration estimators in survey sampling.
Journal of the American Statistical Association 87 (418), 376â€“382.
Diamond, A. and J. S. Sekhon (2013). Genetic matching for estimating causal
effects: A general multivariate matching method for achieving balance in observational studies. Review of Economics and Statistics 95 (3), 932â€“945.
Ferwerda, J. (2014). Electoral consequences of declining participation: A natural
experiment in austria. Electoral Studies 35 (0), 242â€“252.
Gneiting, T. and A. E. Raftery (2007). Strictly proper scoring rules, prediction, and
estimation. Journal of the American Statistical Association 102 (477), 359â€“378.
Graham, B. S., C. C. D. X. Pinto, and D. Egel (2012). Inverse probability tilting for moment condition models with missing data. The Review of Economic
Studies 79 (3), 1053â€“1079.
Hahn, J. (1998). On the role of the propensity score in efficient semiparametric
estimation of average treatment effects. Econometrica 66 (2), 315â€“332.
Hainmueller, J. (2011). Entropy balancing for causal effects: A multivariate
reweighting method to produce balanced samples in observational studies. Political Analysis 20, 25â€“46.
Hastie, T. J. and R. J. Tibshirani (1990). Generalized additive models, Volume 43.
CRC Press.
Hirano, K. and G. Imbens (2001). Estimation of causal effects using propensity
score weighting: An application to data on right heart catheterization. Health
Services and Outcomes Research Methodology 2, 259â€“278.
Hirano, K., G. W. Imbens, and G. Ridder (2003). Efficient estimation of average
treatment effects using the estimated propensity score. Econometrica 71 (4),
1161â€“1189.
Holland, P. W. (1986). Statistics and causal inference. Journal of the American
Statistical Association 81, 945â€“960.

22

ENTROPY BALANCING

Imai, K., G. King, and E. A. Stuart (2008). Misunderstandings between experimentalists and observationalists about causal inference. Journal of the Royal
Statistical Society: Series A (Statistics in Society) 171 (2), 481â€“502.
Imai, K. and M. Ratkovic (2014). Covariate balancing propensity score. Journal of
the Royal Statistical Society: Series B (Statistical Methodology) 76 (1), 243â€“263.
Kang, J. D. and J. L. Schafer (2007). Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete
data. Statistical Science 22 (4), 523â€“539.
Lunceford, J. K. and M. Davidian (2004). Stratification and weighting via the
propensity score in estimation of causal treatment effects: a comparative study.
Statistics in Medicine 23 (19), 2937â€“2960.
Marcus, J. (2013). The effect of unemployment on the mental health of spouses
evidence from plant closures in germany. Journal of Health Economics 32 (3),
546â€“558.
Neyman, J. (1923). Sur les applications de la thar des probabilities aux experiences
agaricales: Essay des principle. excerpts reprinted (1990) in english. Statistical
Science 5, 463â€“472.
Qin, J. and B. Zhang (2007). Empirical-likelihood-based inference in missing response problems and its application in observational studies. Journal of the Royal
Statistical Society: Series B (Statistical Methodology) 69 (1), 101â€“122.
Ridgeway, G. and D. F. McCaffrey (2007). Comment: Demystifying double robustness: A comparison of alternative strategies for estimating a population mean
from incomplete data. Statistical Science 22 (4), 540â€“543.
Robins, J., M. Sued, Q. Lei-Gomez, and A. Rotnitzky (2007). Comment: Performance of double-robust estimators when inverse probability weights are highly
variable. Statistical Science 22 (4), 544â€“559.
Robins, J. M., A. Rotnitzky, and L. Zhao (1994). Estimation of regression coefficients when some regressors are not always observed. Journal of the American
Statistical Association 89, 846â€“866.
Rosenbaum, P. and D. Rubin (1983). The central role of the propensity score in
observational studies for causal effects. Biometrika 70 (1), 41â€“55.
Rosenbaum, P. and D. Rubin (1984). Reducing bias in observational studies using
subclassification on the propensity score. Journal of the American Statistical
Association 79, 516â€“524.
Rosenbaum, P. R. and D. B. Rubin (1985). Constructing a control group using
multivariate matched sampling methods that incorporate the propensity score.
The American Statistician 39 (1), 33â€“38.
Rubin, D. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology 66 (5), 688â€“701.
SaÌˆrndal, C.-E. and S. LundstroÌˆm (2005). Estimation in surveys with nonresponse.
John Wiley & Sons.
Silvapulle, M. J. (1981). On the existence of maximum likelihood estimators for
the binomial response models. Journal of the Royal Statistical Society. Series B
(Methodological) 43 (3), 310â€“313.
Stefanski, L. A. and D. D. Boos (2002). The Calculus of M-Estimation. The
American Statistician 56 (1), 29â€“38.
Tan, Z. (2006). A distributional approach for causal inference using propensity
scores. Journal of the American Statistical Association 101, 1619â€“1637.

ENTROPY BALANCING

23

Tan, Z. (2010). Bounded, efficient and doubly robust estimation with inverse weighting. Biometrika 97 (3), 661â€“682.
Wang, Q. and J. N. K. Rao (2002). Empirical likelihood-based inference under
imputation for missing response data. The Annals of Statistics 30 (3), 896â€“924.
Zhao, Q. (2016). Covariate balancing propensity score by tailored loss functions.
Zubizarreta, J. R. (2015). Stable weights that balance covariates for estimation
with incomplete outcome data. Journal of the American Statistical Association 110 (511), 910â€“922.

