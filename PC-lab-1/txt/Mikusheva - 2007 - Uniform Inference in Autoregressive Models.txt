Econometrica, Vol. 75, No. 5 (September, 2007), 1411â€“1452

UNIFORM INFERENCE IN AUTOREGRESSIVE MODELS
BY ANNA MIKUSHEVA1
The purpose of this paper is to provide theoretical justification for some existing
methods for constructing confidence intervals for the sum of coefficients in autoregressive models. We show that the methods of Stock (1991), Andrews (1993), and
Hansen (1999) provide asymptotically valid confidence intervals, whereas the subsampling method of Romano and Wolf (2001) does not. In addition, we generalize the three
valid methods to a larger class of statistics. We also clarify the difference between uniform and pointwise asymptotic approximations, and show that a pointwise convergence
of coverage probabilities for all values of the parameter does not guarantee the validity
of the confidence set.
KEYWORDS: Autoregressive process, confidence set, local-to-unity asymptotics, uniform convergence.

1. INTRODUCTION
OVER THE PAST FIFTEEN YEARS, there has been a considerable amount of theoretical and applied work on the problem of constructing a confidence interval
for the autoregressive coefficient in an autoregression of order 1 (AR(1)) or,
more generally, for the sum of the coefficients in an autoregression of order p
(AR(p)). From an empirical perspective, the problem is important because the
sum of the AR coefficients measures the persistence of a shock to a process.
Some recent empirical papers involving confidence intervals for the autoregressive coefficient Ï (or sum of the coefficients) include Murray and Papell
(2002), Imbs, Mumtaz, Ravn, and Rey (2005) (exchange rate dynamics), Rapach and Wohar (2004) (real interest rates), and Oâ€™Reilly and Whelan (2005)
(inflation). From a theoretical perspective, the problem is of interest because
the finite-sample distribution of the ordinary least squares (OLS) estimator of
the AR(1) coefficient is biased, its limiting distribution (and its rate) changes
near Ï = 1, and there is no known pivotal statistic.
Several methods have been proposed for constructing confidence intervals
for Ï. Stock (1991) proposed inverting the Dickeyâ€“Fuller (1979) t-statistic
when Ï is in a 1/T neighborhood of 1 (local to unity). Andrews (1993) suggested inverting the finite-sample distribution of the OLS estimator of Ï under
the assumption of normality. Hansen (1999) introduced the â€œgrid bootstrapâ€
method, which is conceptually similar to Andrewsâ€™ method except that Hansen
1
I would like to thank Graham Elliott, Bruce Hansen, Rustam Ibragimov, and Allie Schwartz
for comments and suggestions. Suggestions from four anonymous referees and a co-editor greatly
improved the paper. I am grateful to Marcelo Moreira for numerous discussions, his enthusiasm,
and his support. I am deeply indebted to Jim Stock for posing the question, and for his help,
support, and encouragement. I would like to thank Aleksander Bulinski for introducing me to
the theory of stochastic processes and Skorokhodâ€™s embedding. I thank Donald Andrews and
Patrik Guggenberger for finding some technical difficulties in an earlier version of the paper. All
remaining errors are mine.

1411

1412

ANNA MIKUSHEVA

inverted the t-statistic and used the bootstrap to approximate the exact distribution on a grid of values of Ï. Romano and Wolf (2001) proposed a subsampling method, in which the distribution of the statistics in subsamples is used to
approximate the sampling distribution. Yet, as these authors recognized, all of
these methods have limitations, and none of the existing proofs actually proves
that the confidence intervals are asymptotically correct under standard weak
assumptions on the errors.2
In this paper we focus on confidence sets that in large samples have correct
coverage uniformly over the parameter space. We refer to these as valid confidence intervals. For a confidence set C to be asymptotically valid, the condition
limT â†’âˆ infÏ PÏ (Ï âˆˆ C) â‰¥ 1 âˆ’ Î± must hold, where the infimum is taken over the
parameter space (for example, |Ï| â‰¤ 1). That is, the notion of â€œvalidityâ€ in this
paper is closely related to the concept of â€œglobal uniform validity.â€
The distinction between uniform validity and pointwise validity (that is,
infÏ limT â†’âˆ PÏ (Ï âˆˆ C) â‰¥ 1 âˆ’ Î±) is not of practical importance in many econometric applications, but it is here. For example, consider the pretest confidence
set, constructed as the OLS estimator Â± 196 standard errors if the Dickeyâ€“
Fuller t-statistic is less than âˆ’ ln T and which equals 1 otherwise. This set satisfies the pointwise convergence criterion but not the uniform criterion because
the procedure places point mass on 1 for all values of Ï in a 1/T neighborhood
of 1. For this set, the asymptotic coverage rate is actually zero, that is, there are
(sequences of) parameter values for which the probability of being in the set
tends to zero, no matter how large the sample size. Lack of uniformity is one
way to understand the poor performance of standard confidence intervals in
the AR(1) model discussed by Nankervis and Savin (1985, 1988) and Rayner
(1990).
The purpose of this paper is to prove the uniform validity or invalidity of
a variety of methods for the construction of confidence intervals for Ï. The
proofs use recently developed tools involving the strong invariance principal
and stochastic process theory. We show that the methods of Stock (1991),
Andrews (1993), and Hansen (1999), when based on inverting the t-statistic,
provide asymptotically valid confidence intervals in the uniform sense stated
above, but the subsampling method of Romano and Wolf (2001) does not (even
though it possesses pointwise validity). In addition to these main results, we
generalize the three valid methods to a larger class of statistics.
The paper proceeds as follows. The next section introduces the concept of
uniform asymptotic approximation and relates it to the construction of asymptotically correct confidence sets. It also sets up the theoretical framework for
2
Through simulations, Andrews (1993) showed that the method seems to be robust to nonnormal errors, but did not prove that it can be used in a general AR model. Stock (1991) proved the
validity of his method in the local-to-unity neighborhood, but there is no proof that this method
can be used when Ï is fixed and less than 1. Hansen (1999) proved that his grid bootstrap provides
pointwise asymptotically correct approximations under both classical and local-to-unity asymptotics. Romano and Wolf (2001) proved pointwise validity for |Ï| < 1 and for Ï = 1.

INFERENCE IN AUTOREGRESSIVE MODELS

1413

proving the validity of the three methods. Sections 3â€“6 consider the AR(1)
model with an intercept and/or a linear time trend. Section 3 provides large
sample justification for the method proposed by Andrews (1993). Section 4
proves that the local-to-unity asymptotic approach (Stock (1991)) provides a
uniform approximation even for values of the AR coefficient far from the unit
root. Section 5 gives a theoretical justification of Hansenâ€™s (1999) grid bootstrap. In Section 6, we show why the Romano and Wolf (2001) subsampling
method should not be used for making inferences in an AR model. Section 7
extends the results to AR(p) processes with at most one root close to the unit
circle. The Appendix contains proofs of results from Sections 2â€“6.
Since the proofs are long and technically involved, we place some details
of the proofs and the proofs of the results from Section 7 in the online supplementary appendix (Mikusheva (2007)), which also can be found on the authorâ€™s web site.3 The supplementary appendix also contains a simulation study
that assesses the finite-sample properties of the discussed procedures.
2. UNIFORM APPROXIMATION
Assume that we have a sample Y = (y1      yT ) of size T from an AR(1)
process with an intercept:
(1)

yj = c + xj 

xj = Ïxjâˆ’1 + Îµj 

j = 1     T

x0 = 0

The autoregressive coefficient Ï can take on any value in the open interval
Î˜ = (âˆ’1 1). Model (1) is often used in practice to describe the behavior of
inflation or the logarithm of exchange rate. In Section 7, we show that the
results can be extended to more general autoregressive processes. We make
the following assumptions about the error terms:
ASSUMPTIONS A: Let (Îµj  Fj ) be a martingale difference sequence with
E(Îµj2 |Fjâˆ’1 ) = 1 and supj E(|Îµj |r |Fjâˆ’1 ) < âˆ almost surely for some 2 < r â‰¤ 4.
We are interested in constructing a confidence set for the parameter Ï. Below is the classical definition of a confidence set (Lehmann (1997, p. 90)).
DEFINITION: A subset C(Y ) of the parameter space Î˜ is said to be a confidence set at a confidence level 1 âˆ’ Î± if infÏâˆˆÎ˜ PÏ {Ï âˆˆ C(Y )} â‰¥ 1 âˆ’ Î±.
DEFINITION: A subset C(Y ) of the parameter space Î˜ is said to be an asymptotic confidence set at a confidence level 1 âˆ’ Î± (or is said to have a uniform
asymptotic coverage probability 1 âˆ’ Î±) if
(2)
3

lim inf inf PÏ {Ï âˆˆ C(Y )} â‰¥ 1 âˆ’ Î±
T â†’âˆ ÏâˆˆÎ˜

Web address http://econ-www.mit.edu/faculty/amikushe/paper-1/appendix.pdf.

1414

ANNA MIKUSHEVA

The requirement of uniform convergence (2) is much stronger than a requirement of pointwise convergence of coverage probabilities,
(3)

lim PÏ {Ï âˆˆ C(Y )} â‰¥ 1 âˆ’ Î± for every Ï âˆˆ Î˜

T â†’âˆ

The convergence (3) says that for every value of the parameter space and
for any given accuracy, we can find a large enough sample size to provide the
required accuracy of coverage at this value. However, convergence at some
values of the parameter can be much slower than at others. Condition (3) does
not guarantee that there is a sample size that provides the required accuracy
for all values of the parameter. That is, even for a huge sample size, we might
find a part of the parameter space where the required accuracy has not been
achieved. Since a priori we cannot guarantee that our parameter does not belong to this part of the parameter space, we are always at risk of having poor
coverage probability.
This paper deals with methods based on the classical idea of inverting tests
(Lehmann (1997, p. 90)). Let A(Ï0 ) be an acceptance region of an asymptotic level-Î± test for testing H0 : Ï = Ï0 . A set C(Y ) is constructed as a set of
parameter values for which the corresponding simple hypothesis is accepted:
C(Y ) = {Ï : Y âˆˆ A(Ï)}.
Let the testing procedure for a test of the hypothesis H0 : Ï = Ï0 be based
on a test statistic Ï•(Y T Ï0 ), and critical values c1 (T Ï0 ) and c2 (T Ï0 ). A set
C(Y ) is defined as
(4)

C(Y ) = {Ï âˆˆ Î˜ : c1 (T Ï) â‰¤ Ï•(Y T Ï) â‰¤ c2 (T Ï)}

We state all results for two-tailed tests, but they are equally applicable for onetailed tests.
Let FTÏ (x) = PÏ {Ï•(Y T Ï) â‰¤ x} be a distribution function of the statistic Ï•(Y T Ï) given that the true AR parameter is equal to Ï. Let qÎ±F (T Ï)
denote an Î±-quantile of the distribution FTÏ (x), that is, FTÏ (qÎ±F (T Ï)) = Î±.
F
F
(T Ï) and c2 (T Ï) = q1âˆ’Î±/2
(T Ï), then a set C(Y ) defined
If c1 (T Ï) = qÎ±/2
by (4) is a confidence set at the confidence level 1 âˆ’ Î±. In practice, the finitesample distribution FTÏ (x) is usually unknown. However, if there is a family
of distributions that provides a uniform asymptotic approximation of FTÏ (x),
then it can be used to construct an asymptotic confidence set.
LEMMA 1: Let GTÏ (x) be a family of distribution functions uniformly approximating the family of distributions FTÏ (x) as T increases:
lim sup sup |FTÏ (x) âˆ’ GTÏ (x)| = 0

T â†’âˆ ÏâˆˆÎ˜

x

INFERENCE IN AUTOREGRESSIVE MODELS

1415

G
Suppose, that a set C(Y ) is defined by (4) with c1 (T Ï) = qÎ±/2
(T Ï) and
G
G
c2 (T Ï) = q1âˆ’Î±/2 (T Ï), where qÎ± (T Ï) is the Î± quantile of the distribution
GTÏ (x). Then C(Y ) is an asymptotic confidence set at the confidence level
1 âˆ’ Î±.

REMARK 1: Having a uniformly approximating family of distributions is a
sufficient, but not necessary, condition for constructing an asymptotic confidence set. The real line R is an asymptotic confidence set at any level. However, this set is useless from the practical point of view, since it has zero power.
The confidence set constructed in Lemma 1 using a uniform approximation is
not conservative. In particular,
lim inf PÏ {Ï âˆˆ C(Y )} = lim sup PÏ {Ï âˆˆ C(Y )} = 1 âˆ’ Î±

T â†’âˆ ÏâˆˆÎ˜

T â†’âˆ ÏâˆˆÎ˜

The main goal of this paper is to prove that the three methods widely used
in practiceâ€”Andrewsâ€™ parametric grid bootstrap, Stockâ€™s method based on the
local-to-unity asymptotic approach, and Hansenâ€™s grid bootstrapâ€”provide asymptotic confidence sets via constructing uniformly approximating families of
distributions. The rest of the section describes a joint theoretical framework
for the proofs of all three methods.
2.1. Class of Test Statistics

T
Let yjÂµ be the demeaned process of yj , that is, yjÂµ = yj âˆ’ T1 i=1 yiâˆ’1 . We consider a wide class of test statistics based on a pair of statistics
(S(T Ï) R(T Ï))


T
T


1
1
Âµ
Âµ
yjâˆ’1 (yj âˆ’ Ïyjâˆ’1 )
(y )2 
= 
g(T Ï) j=1 jâˆ’1
g(T Ï) j=1
where g(T Ï) is a normalization function. We define g(T Ï) =
T
Âµ
EÏ ( j=1 (yjâˆ’1
)2 ). We should note that statistics (S R) are invariant with respect to values of c.
Most of the results of the paper also hold for slightly explosive processes. Let
us introduce a sequence of sets Î˜T = [âˆ’1 âˆ’ Î¸/T 1 + Î¸/T ] of possible values
of the AR coefficient when a sample size equals T , here Î¸ > 0.
DEFINITION: Let H be the class of functions Ï†(s r T Ï) : R Ã— R+ Ã— N Ã—
Î˜1 â†’ R that satisfy two conditions:
(1) For every C > 0 there exist constants MC and T1 such that for all s s1 âˆˆ
R, r > C, r1 > C, T > T1 , and Ï âˆˆ Î˜T , we have |Ï†(s r T Ï) âˆ’ Ï†(s1  r1  T Ï)| <
MC (|s âˆ’ s1 | + |r âˆ’ r1 |).

1416

ANNA MIKUSHEVA

(2) For every 0 < C1 < C2 < âˆ there exists a constant A > 0 such that
âˆ‚Ï†(s r T Ï)/âˆ‚s > A for all T , Ï âˆˆ Î˜T , and C1 < r < C2 .
DEFINITION: The class H of test statistics under consideration is given by the
set H = {Ï•(Y T Ï) = Ï†(S(T Ï) R(T Ï) T Ï) : Ï†(s r T Ï) âˆˆ H}.
The class H is quite wide. For instance, it includes the conventional t-statistic
t = âˆšSR and an appropriately normalized OLS estimate of the autoregressive

ÏOLS âˆ’ Ï) = RS .
coefficient g(T Ï)(
Selecting the proper test statistic is a difficult task. More powerful testing
procedures tend to produce more accurate confidence sets. It is well known
that even in model (1) without an intercept and with normal errors, the uniformly most powerful test for the simple hypothesis H0 : Ï = Ï0 does not exist
(Dufour and King (1991), Elliott, Rothenberg, and Stock (1996)). The class
H contains all test statistics used to create the power envelope considered in
Elliott, Rothenberg, and Stock (1996). Our class of test statistics allows different test statistics for testing different values of Ï. The idea goes back to Elliott
and Stock (2001), who suggested inverting a sequence of point optimal tests.
They showed that the confidence intervals constructed by inverting a sequence
of point optimal tests have power properties quite similar to inverting near
optimal tests for a unit root.
2.2. Stationary and Near Unity Regions
Let a set C(Y ) be defined by (4), where the test statistic is given by
Ï•(Y T Ï) = Ï†(S R T Ï) and the critical value functions c1 (T Ï) and
c2 (T Ï) are calculated using one of the three methods mentioned above. In
all three methods c1 (T Ï) and c2 (T Ï) are quantiles of the distribution of
an approximating statistic Ï†(S1 (T Ï) R1 (T Ï) T Ï). In Andrewsâ€™ method,
the approximating pair of statistics (S1 (T Ï) R1 (T Ï)) = (S N  RN ) is calculated using an AR(1) process with normal errors. In Stockâ€™s method we calculate the limiting distribution (S c  Rc ) of the pair (S(T Ï) R(T Ï)) when
the limit is taken along a sequence of models with ÏT = exp{c/T }. Then we
use (S1 (T Ï) R1 (T Ï)) = (S c(TÏ)  Rc(TÏ) ) as the approximating pair. Hansenâ€™s
grid bootstrap approximates the distribution of (S R) by the distribution of a
pair of bootstrapped statistics (S âˆ—  Râˆ— ).
We want to show that the distribution of the statistic Ï•1 (T Ï) = Ï†(S1  R1 
T Ï) approximates the unknown finite-sample distribution of the statistic
Ï•(T Ï) = Ï†(S R T Ï) uniformly over Ï:


(5)
lim sup supP{Ï•(T Ï) < x} âˆ’ P{Ï•1 (T Ï) < x} = 0
T â†’âˆ ÏâˆˆÎ˜

T

x

There are two different asymptotic approaches developed for autoregressive
processes. These approaches describe strikingly different asymptotic behavior
depending on how close the parameter Ï is to the unit root.

INFERENCE IN AUTOREGRESSIVE MODELS

1417

The classical approach is based on the central limit theorem and the law of
large numbers. If |Ï| < 1 is fixed, then

T
T
1 âˆ’ Ï2 
1 âˆ’ Ï2  2
(6)
xjâˆ’1 Îµj â‡’ N(0 1) and
xjâˆ’1 â†’p 1
T
T
j=1
j=1
as T â†’ âˆ
Park (2003) and Giraitis and Phillips (2006) generalized this result for sequences of processes for which the autoregressive coefficient ÏT converges to
the unit root as the sample size increases with speed slower than 1/T .
The second asymptotic approach, local-to-unity asymptotics, was developed
by Bobkoski (1983), Cavanagh (1985), Chan and Wei (1987), and Phillips
(1987). If the autoregressive coefficient is local to unity, that is, is defined by a
sequence ÏT = 1 + c/T for some fixed c â‰¤ 0, then as the sample size increases,
we have the convergence


T
T
1
1
1
1  2
(7)
xjâˆ’1 Îµj  2
xjâˆ’1 â‡’
Jc (x) dw(x)
Jc2 (x) dx 
T j=1
T j=1
0
0
where the process Jc is an Ornsteinâ€“Uhlenbeck process defined by Jc (x) =
x (xâˆ’y)c
e
dw(y) and w(y) is a Brownian motion.
0
Along with the different behaviors of the autoregressive coefficient, both
asymptotics require different normalizations of âˆš
the sums. The classical asymptotic approach employs the normalization (1/ T  1/T ), whereas the local
2
to unity
 asymptotic approach uses (1/T 1/T ). The proposed normalization
(1/ g(T Ï) 1/(g(T Ï))) joins the two in the same framework. For any fixed
âˆš
|Ï| < 1, our normalization is asymptotically proportional to (1/ T  1/T ), and
for ÏT = 1 + c/T , it is asymptotically proportional to (1/T 1/T 2 ). For fixed
|Ï| < 1, the statistic S(T Ï) has an asymptotically standard normal distribution
and R(T Ï) converges to 1 in probability. Along the sequence ÏT = 1 + c/T
the statistics (S(T ÏT ) R(T ÏT )) converge weakly to a pair of nonnormal distributions.
In order to receive the uniform approximation (5), we divide the parameter space Î˜T into two overlapping regions, the â€œstationaryâ€ and the â€œnear
unityâ€ regions. The stationary region is separated from the unit root by a neighborhood that is contracting at a speed slower than 1/T . The near unity area
shrinks toward the unit root at an even slower speed.
The pair (S R) and an approximating pair (S1  R1 ) follow the classical asymptotic behavior in the stationary region. Namely, statistics S and S1 weakly
converge to the standard normal distribution, whereas statistics R and R1 converge in probability to 1. The convergence in both cases is uniform over the
stationary region.

1418

ANNA MIKUSHEVA

Approximating in the near unity region is a more delicate task. We will be
able to construct pairs (S R) and (S1  R1 ) on a common probability space in
such a way that the distance between them converges to zero in probability
uniformly over the near unity region.
The general framework of the proofs of uniformity for the three methods is
formalized in Lemma 2:
LEMMA 2: Let (S(T Ï) R(T Ï)) and (S1 (T Ï) R1 (T Ï)) be two pairs
of random functions. Assume that there exists a sequence of overlapping sets
AT and BT such that AT âˆª BT = Î˜T . Let the following conditions be satisfied:
1. We can define variables (S(T Ï) R(T Ï)) and (S1 (T Ï) R1 (T Ï)) on a
common probability space in such a way that for every Îµ > 0,

lim sup P |S(T Ï) âˆ’ S1 (T Ï)| + |R(T Ï) âˆ’ R1 (T Ï)| > Îµ = 0
T â†’âˆ ÏâˆˆA

T

2. There exists a continuous distribution function F(x) that does not depend
on either T or Ï, such that S and S1 both converge in distribution to F(x)
uniformly over BT :


lim sup supP{S(T Ï) < x} âˆ’ F(x)
T â†’âˆ ÏâˆˆB

T

x



= lim sup supP{S1 (T Ï) < x} âˆ’ F(x)
T â†’âˆ ÏâˆˆB

T

x

= 0
3. As the sample size increases, R and R1 both converge uniformly over BT to
the same constant K that does not depend on Ï:


lim sup P |R(T Ï) âˆ’ K| > Îµ = lim sup P |R1 (T Ï) âˆ’ K| > Îµ
T â†’âˆ ÏâˆˆB

T

T â†’âˆ ÏâˆˆB

T

=0

âˆ€Îµ > 0

4. For every Îµ > 0, there exists C > 0 such that supT supÏâˆˆÎ˜T P{R1 (T Ï) <
C} < Îµ; that is, R1 is separated from zero uniformly over Î˜T . We also assume that ER1 = ER = K.
5. The pair of variables (S1  R1 ) possesses a continuous distribution uniformly
over Ï in the following way: for every Îµ > 0 there exists a constant M > 0
such that for all Î´1 < Îµ Î´2 < Îµ, |b âˆ’ K| > 2Îµ and all Ï âˆˆ Î˜T and T , we have

PÏ (S1 (T Ï) R1 (T Ï)) âˆˆ [a âˆ’ Î´1  a + Î´1 ] Ã— [b âˆ’ Î´2  b + Î´2 ]
â‰¤ MÎ´1 Î´2 
PÏ {S1 (T Ï) âˆˆ [a âˆ’ Î´1  a + Î´1 ]} â‰¤ MÎ´1 
Then for every statistic Ï•(T Ï) = Ï†(S R T Ï) âˆˆ H its distribution is uniformly approximated by the distribution of Ï•1 (T Ï) = Ï†(S1  R1  T Ï). Thus, con-

INFERENCE IN AUTOREGRESSIVE MODELS

1419

vergence (5) holds. If c1 (T Ï) and c2 (T Ï) are the quantiles of the distribution of
Ï•1 (T Ï), the set C(Y ) defined by (4) is an asymptotic confidence set.
The approximation in the near unity region is stated in condition 1. The asymptotic behavior of the pairs (S R) and (S1  R1 ) in the stationary region is
described in conditions 2 and 3. The statistic R is allowed to appear in the
denominator
of a test statistic. For example, the t-statistic can be written as
âˆš
t = S/ R. Condition 4 is imposed in order to guarantee that R1 is uniformly
bounded. Condition 5 is technical. It requires the distribution of S1 to be uniformly continuous and the joint distribution of (S1  R1 ) to be uniformly continuous in the area where R1 is separated from its stationary limit K.
2.3. Estimation of Variance
Now we relax the assumption of the previous sections that the conditional
variance of errors is known.
 = (
yT ) be a sample from an AR(1) process defined by an equaLet Y
y1     
tion
(8)


yj = c + 
xj 


xj = Ï
xjâˆ’1 +
Îµj 

j = 0     T


x0 = 0

Error terms satisfy a set of Assumptions A1 stated below.
Îµj  Fj ) be a martingale difference sequence with
ASSUMPTIONS A1: Let (
Îµj |r |Fjâˆ’1 ) < âˆ almost surely for some 2 < r â‰¤ 4.
E(
Îµj2 |Fjâˆ’1 ) = Ïƒ 2 and supj E(|
Note that if the variance of error terms Ïƒ 2 is known, then the process
yj /Ïƒ is a process described by (1) with errors satisfying the set of Assumpyj = 
tions A.
Âµ
yjâˆ’1
yjÂµ âˆ’ 
ÏOLS
be the OLS residuals. Let us define an estimator of Ïƒ 2
Let 
ej = 
T 2
ej . Despite the fact that
to be a sample variance of OLS residuals: 
Ïƒ 2 = T1 j=1 
the estimator Ï
OLS of the AR coefficient is biased toward zero, the estimator 
Ïƒ2
of error term variance is uniformly consistent.
Let us define statistics


T
T


1
1
Âµ
Âµ
2



yjâˆ’1 Îµj 
(
y ) 
(S R) = 
g(T Ï)
Ïƒ 2 j=1 jâˆ’1
g(T Ï)
Ïƒ 2 j=1
LEMMA 3: Let us consider a model (8) with error terms satisfying the set of AsÏƒ 2 /Ïƒ 2 âˆ’ 1| > Îµ} = 0 for every Îµ > 0. Any
sumptions A1. Then limT â†’âˆ supÏâˆˆÎ˜T P{|
 T Ï) = Ï†(
 T Ï), where Ï† belongs to the class H, is uniformly
statistic Ï•(Y
S R
approximated by the corresponding statistic Ï†(S R T Ï), where the pair (S R)
yj /Ïƒ with the unit variance of error terms.
is defined for the process yj = 
The proof of Lemma 3 is in the supplementary appendix.

1420

ANNA MIKUSHEVA

3. VALIDITY OF ANDREWSâ€™ METHOD
This section proves the validity of the method proposed by Andrews (1993).
Let us consider an AR(1) model with normal errors:
(9)

zj = Ïzjâˆ’1 + ej 

ej âˆ¼ iid N(0 1)

z0 = 0

The finite-sample distribution of the pair of statistics

(10)

1


g(T Ï)

T

j=1


T

1
Âµ
2
e
(z ) = (S N  RN )
g(T Ï) j=1 jâˆ’1

Âµ
jâˆ’1 j

z

is fully defined for every T and Ï, and can be calculated by numerical integration or by simulations. The distribution of the statistic Ï•1 = Ï†(S N  RN  T Ï) is
also fully defined and can be simulated.
Assume that we have a sample Y = (y1      yT ) from process (1), with independent standard normal error terms Îµt . Then set C(Y ), defined by Equation (4), where c1 (T Ï) and c2 (T Ï) equal the Î±/2 and 1 âˆ’ Î±/2 quantiles of
the finite-sample distribution of the statistic Ï•1 , is a confidence set for the parameter Ï at confidence level 1 âˆ’ Î±. Andrews (1993) proposed the procedure
described above for the test statistic equal to the OLS estimator of Ï, but the
procedure can be generalized for any Ï• âˆˆ H.
The described procedure is exact only for the AR(1) model with normal
errors. Andrews (1993) performed simulations showing that the method is robust to nonnormal errors. We prove that the method produces asymptotically
uniform confidence sets when applied to model (1) without normality assumptions.
THEOREM 1: Let Y be a sample from an AR(1) process with an intercept
defined by (1) with error terms satisfying the set of Assumptions A. We consider a
test statistic Ï•(Y T Ï) = Ï†(S R T Ï) belonging to the class H. Let C(Y ) be a
set defined by Equation (4) with c1 (T Ï) and c2 (T Ï) being the Î±/2 and 1 âˆ’ Î±/2
quantiles of the finite-sample distribution of the statistic Ï•1 = Ï†(S N  RN  T Ï),
where (S N  RN ) are statistics defined by (10) for model (9) with normal errors.
Then C(Y ) has a uniform asymptotic coverage probability equal to 1 âˆ’ Î±.
REMARK 2: The statement of Theorem 1 can be extended to an AR(1)
process with a linear trend, yj = a + bj + xj  where xj is defined in (1). Let
yjÏ„ be the detrended process of yj , that is,
T
(yi âˆ’ y)i
T +1

jâˆ’
yjÏ„ = yj âˆ’ y âˆ’ Ti=1
T +1 2
2
i=1 (i âˆ’ 2 )

1421

INFERENCE IN AUTOREGRESSIVE MODELS

We consider a pair of statistics
(S Ï„ (T Ï) RÏ„ (T Ï))


T
T


1
1
Ï„
yjâˆ’1
(yj âˆ’ Ïyjâˆ’1 ) Ï„
(y Ï„ )2 
= 
g (T Ï) j=1 jâˆ’1
gÏ„ (T Ï) j=1
T
Ï„
where gÏ„ (T Ï) = E( j=1 (yjâˆ’1
)2 ). Then if Assumptions A are satisfied, the
finite-sample distribution of the statistic Ï• = Ï†(S Ï„  RÏ„  T Ï) is uniformly approximated by the finite-sample distribution of the statistic Ï•1 = Ï†(S Ï„N  RÏ„N 
T Ï). Here (S Ï„N  RÏ„N ) is a pair of corresponding detrended statistics in a
model with normal errors.
The proof of Theorem 1 follows the plan proposed in Lemma 2. Let
(11)

AT = {Ï âˆˆ Î˜T : |1 âˆ’ Ï|T Î± < 1 or |1 + Ï|T Î± < 1}

for some 0 < Î± < 1 be a near unity region. Let the stationary region be defined
by the set
(12)

BT = {Ï âˆˆ Î˜T : âˆ’ÏT â‰¤ Ï â‰¤ ÏT }

where

ÏT = 1 âˆ’

log(T )

T

The sets AT and BT are overlapping and cover the whole Î˜T .
Giraitis and Phillips (2006) showed that the convergence in (6) holds uniformly over BT . Conditions 2 and 3 of Lemma 2 are direct corollaries from
Lemmas 2.1 and 2.2 in Giraitis and Phillips (2006). The fact that the statistic
RN is uniformly separated from zero (condition 4) follows from Theorem 2 in
SzÃ©kely and Bakirov (2003).
Our main efforts are devoted to checking condition 1. Our proof uses the
strong invariance principle. We define statistics (S R) and (S N  RN ) on a common probability space in such a way that the distance between them converges
to zero in probability uniformly over set AT .
Let (Îµj  Fj ) be a martingale difference sequence of error terms satisfying the
j
set of Assumptions A. We consider partial
sums
S
j =
i=1 Îµi and the normalâˆš
ized partial sum process Î·T (t) = (1/ T )S[tT ] . Using Skorohodâ€™s embedding
scheme, we can enlarge the initial probability space and construct a sequence
of Brownian motions wT on it in such a way that for every Îµ > 0, we have
(13)

sup |Î·T (t) âˆ’ wT (t)| = o(T âˆ’1/2+1/r+Îµ ) a.s.
0â‰¤tâ‰¤1

For more details, please refer to Lemma S2 in the supplementary appendix.
We should note that since r > 2, the distance between the processes in (13)
converges to zero with the speed of T raised to a negative power. The normalized process of partial sums for variables with a finite moment of higher order
can be better approximated by a Brownian motion.

1422

ANNA MIKUSHEVA

âˆš
Let us define error terms by the equality eTj / T = wT (j/T ) âˆ’ wT ((j âˆ’
1)/T ). The error terms eTj are constructed on the same probability space as
Îµj and have the standard normal distribution.
In the following analysis, we work only with positive AR coefficients Ï âˆˆ A+T .
The proof for negative AR coefficients is similar. Let us define zTj (Ï) =
ÏzTjâˆ’1 (Ï) + eTj . Then for every Ï, the distribution of {zj }Tj=1 is the same as
the distribution of {zTj (Ï)}Tj=1 . In what follows we ignore the difference between them. The lemma below shows that many statistics for the constructed
processes will be uniformly close to one another.
LEMMA 4: For every Îµ > 0âˆšwe have: âˆš
(a) supÏâˆˆÎ˜+ supj=1T |xj / T âˆ’ zj / T | = o(T âˆ’1/2+1/r+Îµ ) a.s.
T
âˆš
(b) supÏâˆˆÎ˜+ supj=1T |xj / T | = O(1) a.s.
âˆš T
âˆš T T
(c) |(1/ T ) j=1 Î·T (j/T )Îµj âˆ’ (1/ T ) j=1 wT (j/T )eTj | = o(T âˆ’1/2+1/r+Îµ )
a.s.
T
T
1
(d) supÏâˆˆÎ˜+ (1âˆ’Ï)T
| 1 j=1 xjâˆ’1 Îµj âˆ’ T1 j=1 zjâˆ’1 eTj | = o(T âˆ’1/2+1/r+Îµ ) a.s.
+1 T
T
T
T 2
(e) supÏâˆˆÎ˜+ |(1/T 2 ) j=1 x2jâˆ’1 âˆ’ (1/T 2 ) j=1 zjâˆ’1
| = o(T âˆ’1/2+1/r+Îµ ) a.s.
T


T
T
(f) supÏâˆˆÎ˜+ |(1/T 3/2+k ) j=1 xjâˆ’1 j k âˆ’ (1/T 3/2+k ) j=1 zjâˆ’1 j k | = o(T âˆ’1/2+1/r+Îµ )
T
a.s.
(g) supÏâˆˆA+ |S(T Ï) âˆ’ S N (T Ï)| = o(T 3/2+1/râˆ’2Î±+Îµ ) a.s.
T
(h) supÏâˆˆA+ |R(T Ï) âˆ’ RN (T Ï)| = o(T 1/2+1/râˆ’Î±+Îµ ) a.s.
T

Statements (g) and (h) of Lemma 4 imply the validity of condition 1 of
Lemma 2 for the pairs (S R) and (S N  RN ).
We should note that statements (a)â€“(f) hold for the whole parameter space,
whereas (g) and (h) are stated for a local neighborhood of Ï = 1 only.
Parts (a)â€“(f) use the local-to-unity normalization, which is too strong for our
purposes. In (g) and (h) we received the right normalization at the price of a
lower convergence speed. Since r > 2, there is always some 0 < Î± < 1 such that
we have T in a negative power on the right-hand side of statements (g) and (h).
4. VALIDITY OF STOCKâ€™S METHOD
Stock (1991) proposed to construct a confidence set for the largest autoregressive root by using local-to-unity asymptotic approximation. He used a
t-statistic for testing H0 : Ï = 1. However, this test statistic, unlike the t-statistic
for testing the true Ï, does not belong to the class H. Hansen (1999) showed in
simulations that the version of the procedure proposed by Stock (1991) breaks
down for values of Ï far from the unit root. In this section, we prove the validity of a modification of the method proposed by Stock (1991) for constructing
confidence intervals with the help of local-to-unity asymptotics.

INFERENCE IN AUTOREGRESSIVE MODELS

1423

It is well known from Bobkoski (1983), Cavanagh (1985), Chan and Wei
(1987), Phillips (1987), and Stock (1991) that the asymptotic behavior of a
t-statistic when the AR coefficient is local to unity is completely different from
that for a fixed |Ï| < 1. If the autoregressive coefficient is local to unity, that
is, is defined by a sequence ÏT = exp{c/T } for some fixed c < 0, then as the
sample size increases, we have convergence (7).
Phillips (1987) proved that
(14)



1

âˆ’2c

1

Jc2 (x) dx â‡’ (N(0 1) 1)

Jc (x) dw(x) (âˆ’2c)
0

0

as c â†’ âˆ’âˆ
That is, we receive a classical normal approximation as a limiting case when c
tends to negative infinity.
Let us consider a pair of statistics
(S c  Rc ) = 

1

1
g(c)
1

JcÂµ (x) dw(x)
0

1
g(c)

1

(JcÂµ (x))2 dx 
0

1

where JcÂµ (x) = Jc (x) âˆ’ 0 Jc (r) dr and g(c) = E 0 (JcÂµ (x))2 dx. We also define
a function c(T Ï) = T log(Ï). Stockâ€™s method suggests constructing an asymptotic confidence set as defined in (4) with c1 (T Ï) and c2 (T Ï) being Î±/2 and
1 âˆ’ Î±/2 quantiles of the distribution of the statistic Ï•1 = Ï†(S c(TÏ)  Rc(TÏ)  T Ï).
An advantage of Stockâ€™s method is that the critical values depend on the one
dimensional local parameter c and can be tabulated for commonly used levels
of confidence and commonly used statistics.
Based on the construction, the set C(Y ) has correct local-to-unity asymptotic coverage. Namely,
lim PÏ=exp{c/T } {Ï âˆˆ C(Y )} = 1 âˆ’ Î± âˆ€c â‰¤ 0

T â†’âˆ

The convergence (14) suggests that the method may work well for the values of
the parameter Ï in the stationary region. However, until now there has been no
proof of the uniform validity of Stockâ€™s method. We present this proof below.
THEOREM 2: Let Y be an AR(1) process with an intercept defined by
model (1) with error terms satisfying the set of Assumptions A. Assume that the
statistic Ï•(Y T Ï) = Ï†(S R T Ï) belongs to the class H. Let C(Y ) be a set defined by Equation (4) with c1 (T Ï) and c2 (T Ï) being Î±/2 and 1 âˆ’ Î±/2 quantiles
of the distribution of the statistic Ï•1 = Ï†(S c(TÏ)  Rc(TÏ)  T Ï). Then the set C(Y )
has an (uniform) asymptotic coverage probability 1 âˆ’ Î±.

1424

ANNA MIKUSHEVA

REMARK 3: The statement of Theorem 2 can be extended to an AR(1)
process with a linear trend. Let
(S Ï„c  RÏ„c ) = 

1

1
gÏ„ (c)

JcÏ„ (x) dw(x)
0

1

1
Ï„
g (c)

1

(JcÏ„ (x))2 dx 
0

1

where JcÏ„ (x) = Jc (x) âˆ’ 0 (4 âˆ’ 6r)Jc (r) dr âˆ’ x 0 (12r âˆ’ 6)Jc (r) dr, gÏ„ (c) =
1
E 0 (JcÏ„ (x))2 dx. If Assumptions A are satisfied, the finite-sample distribution
of the statistic Ï• = Ï†(S Ï„  RÏ„  T Ï) is uniformly approximated by the distribution of the statistic Ï•1 = Ï†(S Ï„c(TÏ)  RÏ„c(TÏ)  T Ï).
As we already proved in Theorem 1, the distribution of the statistic
Ï†(S N  RN  T Ï) provides a uniform approximation for the distribution of the
statistic Ï†(S R T Ï). In order to prove Theorem 2, it is enough to show that
the distribution of the variable Ï†(S c(TÏ)  Rc(TÏ)  T Ï) uniformly approximates
the distribution of Ï†(S N  RN  T Ï). It is easy to check all the conditions of
Lemma 2 applied to pairs (S c(TÏ)  Rc(TÏ) ) and (S N  RN ). The only nontrivial
part is the validity of condition 1, which is discussed below.
Let us consider a standard Brownian
motion w(t) and define the normal erâˆš
ror terms by the equality eTj / T = w(j/T ) âˆ’ w((j âˆ’ 1)/T ). Then the AR(1)
process zTt (Ï) generated by eTj has the form
zTj (Ï)  jâˆ’i
i
w
Ï
=
âˆš
T
T
j=0
j

âˆ’w

iâˆ’1
T

j/T

=

elog(Ï)(jâˆ’[T s]âˆ’1) dw(s)
0

Many statistics of interest can be represented as stochastic integrals. For example,


T


1
g(T Ï)

t

1

zjâˆ’1 ej =

f1 (t s T Ï) dw(s) dw(t)
0

j=1

0


where f1 (t s T Ï) = T/( g(T Ï))elog(Ï)([T t]âˆ’[T s]âˆ’1) I{s â‰¤ [T t]/T }. Its local-tounity analogs has a similar form,


1

1
g(c(T Ï))

t

1

Jc (t) dw(t) =
0

f2 (t s T Ï) dw(s) dw(t)
0

0


where f2 (t s T Ï) = (1/ g(c(T Ï)))elog(Ï)T (tâˆ’s) .
The lemma below says that the described statistics of interest are uniformly
close to each other in the L2 metric:
LEMMA 5: Let a set AT be defined in (11). Then we have:

INFERENCE IN AUTOREGRESSIVE MODELS

(a) limT â†’âˆ supÏâˆˆAT E( âˆš

T

1
g(TÏ)

1
(b) limT â†’âˆ supÏâˆˆAT E( g(TÏ)

T

j=1

zjâˆ’1 ej âˆ’ âˆš

(c) limT â†’âˆ supÏâˆˆAT E( âˆš

1
1
c
g(c(TÏ)) 0
1
1
2
c
g(c(TÏ)) 0
1
1
jâˆ’1
c
g(c(TÏ)) 0

2
zjâˆ’1
âˆ’
T
j=1 z

j=1

1
âˆš
g(TÏ) T
N
c(TÏ) 2

1425

J (t) dw(t))2 = 0.

(J (t)) dt)2 = 0.

âˆ’âˆš

J (t) dt)2 = 0.

) = 0.
(d) limT â†’âˆ supÏâˆˆAT E(S âˆ’ S
(e) limT â†’âˆ supÏâˆˆAT E(RN âˆ’ Rc(TÏ) )2 = 0.
5. VALIDITY OF HANSENâ€™S METHOD
The grid bootstrap was proposed by Hansen (1999) for AR(p) processes.
This section considers a special case for the AR(1) model. The discussion of
the general case will be presented in Section 7.
Let us consider a bootstrapped sample
âˆ—
ytâˆ— = Ïytâˆ’1
+ eâˆ—t 

y0âˆ— = 0

eâˆ—t âˆ¼ iid FT 

where FT is a distribution function that can depend on Y . Let a pair of statistics
(S âˆ— (T Ï) Râˆ— (T Ï)) be defined by


T
T


1
1
Âµâˆ—
Âµâˆ—
ytâˆ’1 eâˆ—t 
(y )2 
(S âˆ—  Râˆ— ) = 
g(T Ï) t=1 tâˆ’1
g(T Ï) t=1
The grid bootstrap set C(Y ) is described by Equation (4), where Ï• =
Ï†(S R T Ï), with c1 (T Ï) and c2 (T Ï) being Î±/2 and 1 âˆ’ Î±/2 quantiles of
the distribution of the statistic Ï•1 = Ï†(S âˆ—  Râˆ—  T Ï).
Hansen (1999) proposed that Ï• be a t-statistic and FT be the cumulative
distribution function of the residuals. He proved that the distribution of Ï•1
provides an asymptotic approximation of the distribution of Ï• for any fixed
|Ï| < 1 and along a sequence of models with the local-to-unity AR coefficient
ÏT = exp{c/T }. We prove that the grid bootstrap provides uniform approximation and constructs asymptotic confidence sets. We also generalize the procedure in two ways. First of all, we can use any test statistic from the class H.
Second, we allow for different specifications of the distribution function FT . In
particular, we can consider a parametric grid bootstrap, a nonparametric error
based grid bootstrap, and a nonparametric residual based grid bootstrap.
The sample Y is fully defined by the realized error terms Î£T = {Îµj }Tj=1 and
the unknown true AR coefficient Ï. Assume that we are testing the hypothesis
H0 : Ï = Ï0 . Suppose that the distribution FT = FT (Y Ï0 ) can depend on the
sample Y and the null value Ï0 . Thus we can consider FT = FT (Î£T  Ï Ï0 ) as
being a function of the realized error terms, the unknown true coefficient Ï,
and the null value Ï0 .
DEFINITION: Let Lr (K M Î¸) be a class of sequences of distributions FT
satisfying the following three conditions:

1426

ANNA MIKUSHEVA

(1) Âµ1 (FT ) = 0;
(2) Âµ2 (FT ) = ÏƒT2 , where |ÏƒT2 âˆ’ 1| â‰¤ MT âˆ’Î¸ ;
(3) supT |Âµ|r (FT ) < K.
Here Âµj (F) is the jth central moment of distribution F and |Âµ|j (F) is the jth
absolute moment of distribution F .
THEOREM 3: Let Y be an AR(1) process defined by Equation (1) with error
terms satisfying the set of Assumptions A. Assume that the statistic Ï•(Y T Ï) =
Ï†(S R T Ï) belongs to the class H. Then the following three statements hold:
(1) limT â†’âˆ supÏâˆˆÎ˜T supFT âˆˆLr (KMÎ¸) supx |PÏ {Ï• < x} âˆ’ PÏâˆ— {Ï•1 < x}| = 0.
(2) If, for almost all realizations of error terms Î£ = {Îµ1      Îµj    }, there exist constants K(Î£) > 0 M(Î£) > 0, and Î¸ > 0 such that for all Ï âˆˆ Î˜T we have
FT (Î£T  Ï Ï) âˆˆ Lr (K M Î¸), then
lim sup sup |PÏ {Ï• < x} âˆ’ PÏâˆ— {Ï•1 < x|Î£T }| = 0

T â†’âˆ ÏâˆˆÎ˜

T

a.s.

x

That is, the bootstrap provides a uniform asymptotic approximation for almost all
realizations of error terms.
(3) Let the assumption from the second statement be satisfied. Let C(Y ) be a
set defined by Equation (4) with c1 (T Ï|Y ) and c2 (T Ï|Y ) being Î±/2 and 1âˆ’Î±/2
quantiles of the distribution of the statistic Ï•1 = Ï†(S âˆ—  Râˆ— ) given the realization
of Y . Then the set C(Y ) has asymptotic coverage probability 1 âˆ’ Î±.
In the rest of the section we discuss different choices of the bootstrap error
distribution FT . If FT is taken from a parametric family, then the bootstrap is
called parametric. We should note that Andrewsâ€™ (1993) method is a version of
the parametric grid bootstrap.
There are at least two ways to perform nonparametric grid bootstrap. The
most intuitive one is to resample bootstrap errors from the residuals of the
regression model (1). That is, let {
ej }Tj=1 be residuals based on the sample Y :

T
Âµ

ej = yj âˆ’ T1 i=1 yi âˆ’ 
Ïyjâˆ’1 . The residual based bootstrap obtains error terms
T
ej â‰¤ x}.
by resampling from {
ej }Tj=1 with repetition. That is, FTres (x) = T1 j=1 I{
The distribution function FTres depends on the sample Y , but does not depend
on the null hypothesis tested.
The second way to perform nonparametric grid bootstrap is to impose the
null while finding the error terms. Suppose that we are testing the null hypothesis H0 : Ï = Ï0 . Let us generate the sequence of error terms under the
null ej (Ï0 ) = yj âˆ’ Ï0 yjâˆ’1 . Note that if the null is true, then we have the true
ej (Ï0 ) =
realization of unknown errors ej (Ï0 ) = Îµj . We recenter the errors 
T
1
ej (Ï0 ) âˆ’ T i=1 ei (Ï0 ) and resample bootstrap errors from the centered errors:
T
ej (Ï0 ) â‰¤ x}. We call this form of bootstrap error based.
FTerr (x Ï0 ) = T1 j=1 I{
The distribution produced depends on the null value tested and on the sample
FTerr (x|Y Ï0 ) = FTerr (x|Î£T  Ï Ï0 ).

INFERENCE IN AUTOREGRESSIVE MODELS

1427

The lemma below states that the two nonparametric bootstrap procedures
produce asymptotic confidence sets.
LEMMA 6: Assume that {yt }Tt=1 is a sample from an AR(1) process defined by (1) with errors satisfying the set of Assumptions A. Let FTres (x|Î£T  Ï)
be an empirical distribution function for the residual based bootstrap and let
FTerr (x|Î£T  Ï Ï0 ) be an empirical distribution function for the error based bootstrap. Then for every realization of errors Î£, there exist constants K(Î£) > 0,
M(Î£) > 0, and Î¸ > 0 such that for all Ï âˆˆ Î˜T we have FTres (x|Î£ Ï) âˆˆ Lr (K M Î¸)
and FTerr (x|Î£ Ï Ï) âˆˆ Lr (K M Î¸).
REMARK 4: Results in this section also hold for an AR(1) process with a
linear trend if all statistics are calculated for the detrended process in place of
the demeaned.
6. WHY THE SUBSAMPLING PROCEDURE FAILS
In order to construct a uniformly asymptotically valid confidence set, it is
sufficient to have a uniform asymptotic approximation. The subsampling procedure proposed by Romano and Wolf (2001) is aimed at constructing asymptotic confidence sets for the AR coefficient. They proved that the procedure is
pointwise asymptotically correct. However, the sets provided by subsampling
are not uniformly asymptotically correct. We are able to construct a sequence
of AR(1) models with the AR coefficient depending on the sample size, such
that the coverage probability of the set constructed by subsampling converges
to a number lower than the declared coverage probability.
Let us consider a sample {zj }Tj=1 from the AR(1) process with intercept a and
with independent and identically distribution normal innovations. If |Ï| < 1,
the initial variable z0 is normally distributed with mean a/(1 âˆ’ Ï) and variance
1/(1 âˆ’ Ï2 ). When Ï = 1, the initial value is an arbitrary constant. We base our
inferences on the t-statistic t(T Ï) calculated from the sample of size T :
T
Âµ
(T ) âˆ’ Ï
Ï
j=1 ej zjâˆ’1
= 

t(T Ï) =
T
Ïƒ(
Ï(T ))
(z Âµ )2
j=1

jâˆ’1

Let b = bT be a block size when the sample size is equal to T . We consider a
subsample of size b starting from the observation j, that is, {zj  zj+1      zj+bâˆ’1 },
and calculate the estimate Ï
j (b) and the t-statistic using 
Ï(T ) as the null
value:

tj (b) =

(T )
Ï
j (b) âˆ’ Ï
Ïƒ(
Ïj (b))

1428

ANNA MIKUSHEVA

Romano and Wolf (2001) argued that the unknown distribution of the tstatistic t(T Ï) could be well approximated by the empirical distribution funcT âˆ’b+1
1
tion LTb (x) = T âˆ’b+1
I{
tj (b) â‰¤ x}. Let qÎ±L (T b) be the Î± quantile of the
j=1
distribution LTb (x). Then


L
L
C(T b) = 
Ï(T ) âˆ’ q1âˆ’Î±/2
(T b)Ïƒ(
Ï(T )) 
Ï(T ) âˆ’ qÎ±/2
(T b)Ïƒ(
Ï(T ))
is the proposed equitailed confidence interval. Romano and Wolf (2001)
proved that if bT â†’ âˆ and bT /T â†’ 0 as T â†’ âˆ, then for every Ï âˆˆ [0 1],
the coverage probability of the interval C(T b) converges to 1 âˆ’ Î± as the sample size increases.
Below we prove that subsampling is not a uniform procedure.
THEOREM 4: Let bT be a sequence of natural numbers such that bT â†’ âˆ and
bT /T â†’ 0 as T â†’ âˆ. For any c < 0, set ÏT = 1 + c/bT . Then
lim PÏT {ÏT âˆˆ C(T bT )} < 1 âˆ’ Î±

T â†’âˆ

COROLLARY 1: The interval constructed by using subsampling is not an asymptotically uniform confidence set for the unrestricted parameter space Î˜ = (0 1).
Romano and Wolf (2001) motivated their procedure by pointing out that
the subsamples are generated from the same population as the whole sample,
and as a result the autoregressive coefficients for the sample and the subsamples are the same. This fact, according to them, should make quantiles of the
distribution LTb (x) close to the quantiles of the unknown distribution of the
t-statistic t(T Ï).
However, the quality of approximations depends not only on the value of
the autoregressive parameter Ï, but also on the sample size. Park (2003) noted
that the bigger the sample you have is, the wider is the range of Ï for which the
normal approximation works well. The main idea of the proof of Theorem 4
lies in constructing a sequence of coefficients ÏT that slowly converge to the
unit root, such that the original sample size T is large enough and the limiting
normal approximation is achieved, whereas the size of subsamples bT is small
and should be handled by the local-to-unity asymptotic approach.
One may be uncomfortable with this counterexample, which involves choosing Ï in response to the block size. In practice, the block size is often datadriven. For example, we can choose the block size on the basis of the estimated persistence Ï
(T ). This is the so-called block size calibration method
suggested by Romano and Wolf (2001, Section 5.2). If the confidence interval is constructed by inverting hypothesis tests, we can choose different block
sizes for different null hypotheses. In either case, the econometrician has the
opportunity to choose bT in response to the estimated or hypothesized value

INFERENCE IN AUTOREGRESSIVE MODELS

1429

of Ï. However, even this flexibility in choosing the block size does not save the
method.
Assume that we can choose the block size such that bminT â‰¤ bT (Ï) â‰¤ bmaxT
with bminT â†’ âˆ and bmaxT /T â†’ 0. Let us consider ÏT = 1 + c/bmaxT and bT =
bT (ÏT ). Since (1 âˆ’ ÏT )T â†’ âˆ, the distribution of the test statistic calculated
for the whole sample could be handled by the classical asymptotics. Let (1 âˆ’
ÏTn )bTn â†’ Î³ be a converging subsequence of the sequence {(1âˆ’ÏT )bT }âˆ
T =1 . The
distribution of the subsampled test statistics along this subsequence converges
to the local-to-unity limiting distribution with the local-to-unity parameter Î³,
which is closer to 0 than c is. As a result, it is easy to show that
lim sup PÏT {ÏT âˆˆ C(T bT )} < 1 âˆ’ Î±
T â†’âˆ

It is evident from the proof that similar results could be obtained for more
general AR(1) models with a linear trend.4
We should note that the result of Theorem 4 is true only for equitailed
subsampling intervals. Symmetric subsampling intervals would have asymptotically uniformly correct coverage, but will be asymptotically uniformly conservative, that is, the limit of the maximal coverage is higher than the declared
level (see Andrews and Guggenberger (2007a)).
6.1. Small Sample Performance
In this subsection, we assess the extent to which the asymptotic results established in the paper are reflected in finite samples.
Romano and Wolf (2001) provided some Monte Carlo simulations supporting subsampling. The main drawback of their results is that they considered a very restricted set of values of the AR coefficient, in particular,
Ï âˆˆ {1 099 095 09 06} They found that the subsampling works well for
Ï = 1 and values of Ï very close to the unit root. Our asymptotic results predict
that the subsampling intervals would have good coverage for the unit root, but
undercoverage for intermediate values of Ï, that is, for values that could be
considered â€œstationaryâ€ for the whole sample, but â€œclose to the unit rootâ€ for
subsamples. Unfortunately, Romano and Wolf (2001) performed simulations
for only one such value of the AR coefficient Ï = 06 (they used sample sizes
T = 120 and T = 240). For Ï = 06, their 95% equitailed confidence intervals
have a coverage probability of 77%, which is even worse than the coverage of
the interval based on the normal approximation at this point.
4
Recently, Andrews and Guggenberger (2007b) independently showed that the subsampling
may fail to provide asymptotically correct tests when used in a model where the limiting distribution of a test statistic is discontinuous in the true parameter. They considered inferences about
autoregressive coefficients as one example.

1430

ANNA MIKUSHEVA

FIGURE 1.â€”Coverage of the equitailed interval constructed using local-to-unity asymptotics
(Stock (1991)) and subsampling intervals with nominal level 95% for an AR(1) model with a
linear time trend and normal errors (sample size = 120; number of simulations = 5,000).

Figure 1 shows finite-sample coverage of equitailed subsampling confidence
intervals for a wide range of the AR coefficient in an AR(1) model with a
linear time trend and normal errors. The sample size considered is T = 120.
Subsampling intervals are constructed for block sizes b = 5 8 12 26 (the grid
suggested in Romano and Wolf (2001)).
As expected, subsampling intervals exhibit undercoverage for all block sizes
for quite a wide range of Ï. However, the extent of the problem is not as extreme as predicted by the asymptotic results of Andrews and Guggenberger
(2007a). Results of additional simulations that can be found in the supplementary appendix show that the properties of subsampling intervals worsen as the
sample size increases.
The method using local-to-unity asymptotics performs consistently well. In
simulations, we constructed three intervals advertised in this paper (Andrewsâ€™,
Stockâ€™s, and Hansenâ€™s). They all have coverage that lies within simulation accuracy from the declared level over the whole parameter space. We depict only
one of them, since all three lines are essentially indistinguishable. A more extensive simulation study can be found in the supplementary appendix or on the
authorâ€™s web page.

INFERENCE IN AUTOREGRESSIVE MODELS

1431

7. AR(p) MODELS
This section extends the methods discussed in the previous sections to more
empirically relevant AR(p) models. The proofs of all results from Section 7
have been placed in the supplementary appendix.
In this section we consider an AR(p) model with at most one root close to
the unit circle. That is, we restrict all other roots to lie outside a circle strictly
wider than the unit circle. Our aim is to make asymptotically uniformly correct inferences about the persistence of the series. A long discussion about the
choice of a persistence measure is given by Andrews and Chen (1994). They
provided arguments in favor of using the sum of the AR coefficients as opposed to the largest root. We concentrate our attention on the sum of the AR
coefficients.
Let us consider an AR(p) model in augmented Dickeyâ€“Fuller (ADF) form,

(15)

yt = Ïytâˆ’1 +

pâˆ’1


Î±j ytâˆ’j + Îµt 

j=1

where error terms satisfy Assumptions B.
ASSUMPTIONS B: Let {Îµt }âˆ
t=1 be independent and identically distributed error
terms with zero mean EÎµt = 0, unit variance EÎµt2 = 1, and a finite fourth moment
EÎµt4 < âˆ.
The process (15) can be described by the equation a(L)yt = Îµt , where
pâˆ’1
a(L) = 1 âˆ’ ÏL âˆ’ j=1 Î±j (1 âˆ’ L)Lj . Let us represent the polynomial as a(L) =
(1 âˆ’ Âµ1 L) Â· Â· Â· (1 âˆ’ Âµp L), where |Âµ1 | â‰¤ |Âµ2 | â‰¤ Â· Â· Â· â‰¤ |Âµp | < 1. Let us fix 0 < Î´ < 1.
For every Ï âˆˆ (0 1), we define a set RÏ to be a set of all possible values of the
nuisance parameter Î± = (Î±1      Î±pâˆ’1 ) for which |Âµpâˆ’1 | < Î´. It is easy to see the
relationship between the sum of the AR coefficients, Ï, and the inverse roots
p
{Âµi }i=1 : 1 âˆ’ Ï = (1 âˆ’ Âµ1 ) Â· Â· Â· (1 âˆ’ Âµp ). The case when Ï is close to 1 corresponds
to Âµp being close to 1. If Ï = 1, the process (15) has a unit root.
The main aim of this section is to construct an asymptotically uniformly correct confidence set for the parameter Ï. The procedure should work uniformly
well for strictly stationary cases as well as in the situations when Ï is arbitrarily
close to 1. As before, the construction of a confidence set involves inverting a
sequence of tests H0 : Ï = Ï0 .
We should note that a vector Î± = (Î±1      Î±pâˆ’1 ) is a nuisance parameter for
the hypothesis H0 : Ï = Ï0 . To test that the sum of the AR coefficients is equal
to Ï0 , we calculate the conventional t-statistic t(Ï0  Y ) for this hypothesis in
the regression model (15). We also calculate 
Î±(Ï0 ), an estimate of the nui-

1432

ANNA MIKUSHEVA

sance parameter Î±, as the OLS estimator in the regression model with the null
hypothesis imposed:
(16)

yt âˆ’ Ï0 ytâˆ’1 =

pâˆ’1


Î±j ytâˆ’j + Îµt 

j=1

That is, we regress yt âˆ’ Ï0 ytâˆ’1 on ytâˆ’1      ytâˆ’p+1 . Then we compare the calÎ±(Ï0 )), deculated t-statistic t(Ï0  Y ) with a critical value function q(Ï0  T
pending on the tested value Ï0 of the parameter of interest, on the estimated
nuisance parameter, and on the sample size.
The confidence set for the parameter Ï is constructed as a set of values for
which the corresponding hypothesis is accepted:

(17)
Î±(Ï0 )) â‰¤ t(Ï0  Y ) â‰¤ q2 (Ï0  T
Î±(Ï0 )) 
C(y1      yT ) = Ï0 : q1 (Ï0  T
We consider two sets of critical value functions: the one obtained by parametric grid bootstrap, which is a generalization of Andrewsâ€™ (1993) method,
and those obtained by Hansenâ€™s (1999) nonparametric grid bootstrap. In the
parametric grid bootstrap, the critical value functions are quantiles of the distribution of the t-statistic t(Ï0  Z) in the model
(18)

zt = Ï0 ztâˆ’1 +

pâˆ’1



Î±j (Ï0 )ztâˆ’j + et 

j=1

with errors et being independently normally distributed. In the nonparametric
grid bootstrap, we simulate critical value functions as quantiles of the distribution of the t-statistic in model (18) with independent and identically distributed
error terms distributed according to a distribution function FT . Below we prove
the uniform asymptotic validity of both procedures.
7.1. Parametric Grid Bootstrap
When we have an AR(1) process with normal errors, the parametric grid
bootstrap (Andrewsâ€™ method) provides an exact confidence interval for the autoregressive coefficient Ï. The approximating distributions in AR(p) models
employ the estimates of the nuisance parameter, rather than the true value
of the nuisance parameter. As a result, the generalization of the method to
AR(p) is not an exact method even if the error terms are normally distributed.
We prove that the parametric grid bootstrap provides a uniform approximation
of the unknown distribution of the t-statistic in an AR(p) model with normal
errors as long as the estimate of the nuisance parameter is uniformly consistent.

INFERENCE IN AUTOREGRESSIVE MODELS

1433

Let statistics S and R be defined by
(S(Y Ï Î± T ) R(Y Ï Î± T ))


 Îµ G(Ï Î±)âˆ’1/2 Y
Y
G(Ï Î±)âˆ’1/2 
= G(Ï Î±)âˆ’1/2 Y
 = (Y
 Y
 ) , Îµ = (Îµ1      ÎµT ) ,
t = (ytâˆ’1  ytâˆ’1      ytâˆ’p+1 ), Y
where Y
1
T
T
T
T
and G(Ï Î±) = diag( t=1 Var(yt ) t=1 Var(yt )     t=1 Var(yt )). Then
the t-statistic for testing the hypothesis that the sum of AR coefficients is equal
to Ï is
t(Y Ï Î± T ) =

l1 Râˆ’1 (Y Ï Î± T )S(Y Ï Î± T )


l1 Râˆ’1 (Y Ï Î± T )l1

where l1 = (1 0     0).
LEMMA 7: Let us have two AR(p) processes: the process Y = (y1      yT )
defined by (15) and the process Z = (z1      zT ) defined by zt = Ïztâˆ’1 +
pâˆ’1
Î±j ztâˆ’j + Îµt . Assume that error terms Îµj are the same for both processes
j=1 
and have independent standard normal distribution. Assume that the estimate 
Î±
uniformly converges to Î± as the sample size increases,
(19)

lim sup sup PÏ { Î± âˆ’ 
Î± > } = 0 for every  > 0

T â†’âˆ Ïâˆˆ[01) Î±âˆˆR

Ï

where Î± âˆ’ Î² = maxi |Î±i âˆ’ Î²i |. Then


Î± T )| >  = 0
lim sup sup PÏ |t(y Ï Î± T ) âˆ’ t(z Ï

T â†’âˆ Ïâˆˆ[01) Î±âˆˆR

Ï

for every  > 0
Hansen (1999) suggested estimating the nuisance parameters by the OLS,
imposing the null. Lemma 8 states that the proposed estimates are uniformly
consistent.
LEMMA 8: Assume that we have an AR(p) process defined by Equation (15)
with error terms satisfying the set of Assumptions B. Let us define Yt (Ï) =
Î± be an OLS estimate in the reyt âˆ’ Ïytâˆ’1 and Xt = (ytâˆ’1      ytâˆ’p+1 ). Let 
gression of Yt (Ï) on Xt . Then 
Î± is a uniformly consistent estimate of Î±, that is,
convergence (19) holds.
To prove that the parametric grid bootstrap is an asymptotically uniformly
valid procedure for constructing confidence sets in models with nonnormal errors, we employ the same idea as in Section 2. We divide the set of values
of Ï into two overlapping subsets. One of the two subsets is increasing, while
the second is contracting toward the unit root with a speed slower than 1/T .

1434

ANNA MIKUSHEVA

The standard normal distribution provides a uniform approximation of the unknown distribution of the t-statistic over the first subset. We are able to construct two AR(p) processes with the same AR coefficients (one with normal
errors, the other with errors Îµj ) on a common probability space in such a way
that the t-statistics for both processes are close to each other uniformly over
the near unity set. As a result, the distribution of the t-statistic in an AR(p)
model is uniformly approximated by the distribution of the t-statistic in an
AR(p) model with the same AR coefficients but with normal errors. The validity of the parametric bootstrap procedure is stated in the theorem below.
THEOREM 5: Assume that Y = (y1      yT ) is a sample from an AR(p)
process defined by Equation (15) with error terms satisfying the set of Assumptions B. Let Z = (z1      zT ) be an AR(p) process with normal errors defined
by Equation (18), where 
Î±(Ï) is the OLS estimates in a regression model (16).
Then the distribution of the t-statistic based on the sample Y can be uniformly
approximated by the distribution of t-statistic based on the process Z:


Î±(Ï) T ) > x 
lim sup sup supP{t(Y Ï Î± T ) > x} âˆ’ P t(Z Ï
T â†’âˆ Ïâˆˆ(01) Î±âˆˆR

Ï

x

= 0
As a result, the set defined by (17) with qi (Ï T
Î±(Ï)), i = 1 2, being quantiles of
the distribution of t(Z Ï
Î±(Ï) T ), is a uniform asymptotic confidence set for Ï.
7.2. Nonparametric Grid Bootstrap
The nonparametric grid bootstrap procedure approximates the unknown
distribution of the t-statistic t(Y Ï Î± T ) by the distribution of the t-statistic
t(Z Ï
Î±(Ï) T ), where Z is an AR(p) process defined by (18) with error terms
having distribution FT . Let FT be the empirical distribution function FTerr (Â·) of
the residuals from regression (16). Then FT (Î£ Ï0  Ï Î±) depends on the realization of error terms of the process yt , on the true coefficients Ï and Î±, and
on the null value Ï0 tested.
THEOREM 6: Assume that Y is a sample from an AR(p) process defined by
Equation (15) with error terms satisfying the set of Assumptions B. Let zt be an
AR(p) process defined by Equation (18), where 
Î±(Ï) is the OLS estimates in a
regression model (16). Assume that the errors et of the process zt are independent
and identically distribution with the distribution function FT . Then the following
three statements hold:
(a) limT â†’âˆ supÏâˆˆ(01) supÎ±âˆˆRÏ supFT âˆˆLr (KMÎ¸) supx |P{t(Y Ï Î± T ) > x} âˆ’
P{t(Z Ï
Î± T ) > x}| = 0.
(b) If for almost all realizations of error terms Î£ = {Îµ1      Îµj    }, there
exist constants K(Î£) > 0 M(Î£) > 0, and Î¸ > 0 such that FT (Î£ Ï Ï Î±) âˆˆ

INFERENCE IN AUTOREGRESSIVE MODELS

1435

L4 (K M Î¸) for all Ï âˆˆ Î˜T , then


Î± T ) > x|Î£}
lim sup sup supPÏ {t(Y Ï Î± T ) > x} âˆ’ PÏâˆ— {t(Z Ï
T â†’âˆ Ïâˆˆ(01) Î±âˆˆR

Ï

=0

x

a.s.

That is, the bootstrap provides a uniform asymptotic approximation of the distribution of the t-statistic for almost all realizations of error terms.
(c) Let C(Y ) be a set defined by Equation (17) with qi (Ï T
Î±(Ï)) =
qi (Ï T
Î±(Ï)|Y ) i = 1 2, being quantiles of the distribution of the statistic
t(Z Ï
Î± T ), where the bootstrapped errors et have the distribution function FTerr .
Then the set C(Y ) is an asymptotic confidence set.
8. CONCLUSION
In this paper, I emphasize the difference between pointwise and uniform approximations. A pointwise approximation is not a strict enough condition for
constructing an asymptotic confidence set, since it allows the convergence of
the coverage probabilities to be extremely slow for some values of Ï. A uniform
asymptotic approximation guarantees that we can achieve any accuracy uniformly over all possible values of Ï, as long as the sample size is large enough.
Thus, having a uniform approximation of the unknown distribution of a test
statistic always allows us to construct asymptotically valid confidence sets.
However, there still exists a common misleading belief in the literature that
in order to construct a confidence set, it is enough to check the validity of the
procedure at each value of the parameter separately. Partially, this belief can
be explained by the observation that the distinction between pointwise and
uniform convergence is not important in many econometric applications (but
it is here). We show the insufficiency of pointwise approximation by proving
that Romano and Wolfâ€™s subsampling intervals are not asymptotic confidence
sets, even though they are pointwise asymptotically correct.
This paper also fills a gap in the literature by proving the uniform validity of
the three most used methods of constructing confidence sets for the persistence
parameter in autoregressive models: Stockâ€™s local-to-unity method, Andrewsâ€™
parametric grid bootstrap, and Hansenâ€™s grid bootstrap.
Dept. of Economics, Massachusetts Institute of Technology, 50 Memorial Drive,
Cambridge, MA 02142, U.S.A.; amikushe@mit.edu.
Manuscript received January, 2006; final revision received February, 2007.

APPENDIX: PROOFS
This appendix provides proofs of the theorems and lemmas stated in Sections 2â€“6. Proofs of the results from Section 7 are in the supplementary appendix.

1436

ANNA MIKUSHEVA

PROOF OF LEMMA 1: We have
G
G
(T Ï)) âˆ’ FTÏ (qÎ±/2
(T Ï))
PÏ {Ï âˆˆ C(Y )} = FTÏ (q1âˆ’Î±/2
G
G
(T Ï)) âˆ’ GTÏ (qÎ±/2
(T Ï))
â‰¥ GTÏ (q1âˆ’Î±/2

âˆ’ 2 sup |FTÏ (x) âˆ’ GTÏ (x)|
x

= 1 âˆ’ Î± âˆ’ 2 sup |FTÏ (x) âˆ’ GTÏ (x)|
x

As a result,
lim inf PÏ {Ï âˆˆ C(Y )} â‰¥ 1 âˆ’ Î± âˆ’ 2 lim sup sup |FTÏ (x) âˆ’ GTÏ (x)|

T â†’âˆ ÏâˆˆÎ˜

T â†’âˆ ÏâˆˆÎ˜

x

= 1 âˆ’ Î±

Q.E.D.

PROOF OF LEMMA 2: We have
lim sup sup |PÏ {Ï• < x} âˆ’ PÏ {Ï•1 < x}|

T â†’âˆ ÏâˆˆÎ˜

T

x

â‰¤ lim sup sup |PÏ {Ï• < x} âˆ’ PÏ {Ï•1 < x}|
T â†’âˆ ÏâˆˆA

x

T

+ lim sup sup |PÏ {Ï• < x} âˆ’ PÏ {Ï•1 < x}|
T â†’âˆ ÏâˆˆB

T

x

Let Ï âˆˆ AT . Then for the pairs (S R) and (S1  R1 ) on a common probability
space, we have

P |Ï†(S1  R1  T Ï) âˆ’ Ï†(S R T Ï)| > Îµ


Îµ

â‰¤ P{R1 < C} + P{R < C} + P |S âˆ’ S1 | + |R âˆ’ R1 | >
MC
Here we use the fact that Ï† âˆˆ H. Condition 4 of Lemma 2 allows one to choose
C > 0 such that supÏâˆˆÎ˜T P{R1 < C} is small enough. Since P{R < C} < P{R1 <
2C} + P{|R âˆ’ R1 | > C}, then due to conditions 1 and 4 of Lemma 2, we can
make supÏâˆˆAT P{R < C} small enough. According to condition 1, for a fixed C
we can find T1 such that supÏâˆˆAT P{|S âˆ’ S1 | + |R âˆ’ R1 | > Îµ/MC } becomes small
for T > T1 . As a result, the sequences of variables Ï•1 = Ï†(S1  R1  T Ï) and
Ï• = Ï†(S R T Ï) converge to each other in probability uniformly over AT :

(20)
lim sup P |Ï•1 (T Ï) âˆ’ Ï•(T Ï)| > Îµ = 0
T â†’âˆ ÏâˆˆA

T

Note that
P{Ï•1 < x âˆ’ Îµ} âˆ’ P{|Ï• âˆ’ Ï•1 | > Îµ} â‰¤ P{Ï• < x}
â‰¤ P{Ï•1 < x + Îµ} + P{|Ï• âˆ’ Ï•1 | > Îµ}

INFERENCE IN AUTOREGRESSIVE MODELS

1437

If the distribution of variable Ï•1 is uniformly continuous,
(21)

lim sup sup sup P{x âˆ’ Îµ < Ï•1 (T Ï) < x + Îµ} â†’ 0 as Îµ â†’ 0
T â†’âˆ ÏâˆˆÎ˜T

x

then statement (20) implies the closeness of distributions:
(22)

lim sup sup |PÏ {Ï• < x} âˆ’ PÏ {Ï•1 < x}| = 0

T â†’âˆ ÏâˆˆA

T

x

The statement that the distribution of variable Ï•1 is uniformly continuous follows from the definition of the class H and conditions 4 and 5 of Lemma 2.
Namely, for every 0 < C1 < C2 and Î´ > 0,
(23)

P{x âˆ’ Îµ < Ï†(S1  R1  T Ï) < x + Îµ}
â‰¤ P{R1 < C1 or R1 > C2 }
+ P{x âˆ’ Îµ < Ï†(S1  R1  T Ï) < x + Îµ
|R1 âˆ’ K| < Î´ C1 < R1 < C2 }
+ P{x âˆ’ Îµ < Ï†(S1  R1  T Ï) < x + Îµ
|R1 âˆ’ K| > Î´ C1 < R1 < C2 }

Condition 4 and Chebyshevâ€™s inequality imply P{R1 > C2 } â‰¤ K/C2 . From this
and condition 4 it follows that the first summand of (23) can be made small
enough by choosing large C2 and small C1 .
According to the definition of H, |Ï†(S1  R1  T Ï) âˆ’ Ï†(S1  K T Ï)| â‰¤
MC1 |R1 âˆ’ K|:
P{x âˆ’ Îµ < Ï†(S1  R1  T Ï) < x + Îµ |R1 âˆ’ K| < Î´ C1 < R1 < C2 }
â‰¤ P{x âˆ’ Îµ âˆ’ MC1 Î´ < Ï†(S1  K T Ï) < x + Îµ + MC1 Î´}
Let us have x âˆ’ Îµ âˆ’ MC1 Î´ < Ï†(y K T Ï) < x + Îµ + MC1 Î´ for some y.
Since âˆ‚Ï†(s r T Ï)/âˆ‚s > A > 0 (see the definition of H), we have Ï†(y +
 K T Ï) > Ï†(y K T Ï) + A and Ï†(y âˆ’  K T Ï) < Ï†(y K T Ï) âˆ’ A
for all  > 0. Thus,
P{x âˆ’ Îµ âˆ’ MC1 Î´ < Ï†(S1  K T Ï) < x + Îµ + MC1 Î´}


Îµ + MC1 Î´
Îµ + MC1 Î´
Îµ + MC1 Î´
< S1 < y + 2

â‰¤P y âˆ’2
â‰¤ 2M
A
A
A
Here the last inequality follows from condition 5 of Lemma 2. As a result, the
second term in (23) can be made small by choosing small enough Îµ and Î´.
Now we consider the last term in (23). Let y = y(r1 ) be a value such that
x âˆ’ Îµ < Ï†(y r1  T Ï) < x + Îµ for some C1 < r1 < C2 . By using the same rea-

1438

ANNA MIKUSHEVA

soning as above, we get the ordering of events {xâˆ’Îµ < Ï†(S1  r1 +Î´ T Ï) < x +
Îµ} âŠ† {x âˆ’ Îµ âˆ’ MC1 Î´ < Ï†(S1  r1  T Ï) < x + Îµ + MC1 Î´} âŠ† {y âˆ’ 2(Îµ + MC1 Î´)/A <
S1 < y + 2(Îµ âˆ’ MC1 Î´)/A}. By using continuity of the distribution of (S1  R1 )
(condition 5 of Lemma 2), we get that
P{x âˆ’ Îµ < Ï†(S1  R1  T Ï) < x + Îµ |R1 âˆ’ K| > Î´ C1 < R1 < C2 }

P{x âˆ’ Îµ < Ï†(S1  R1  T Ï) < x + Îµ R1 âˆˆ [xk âˆ’ Î´ xk + Î´]}
â‰¤
k

 
Îµ + MC1 Î´
Îµ âˆ’ MC1 Î´
< S1 < y(xk ) + 2

P y(xk ) âˆ’ 2
â‰¤
A
A
k

R1 âˆˆ [xk âˆ’ Î´ xk + Î´]
â‰¤


k

M2

Îµ + MC1 Î´
Îµ + MC1 Î´
Î´â‰¤M
(C2 âˆ’ C1 ) â‰¤ const(Îµ + MC1 Î´)
A
A

where we divided a set {|R1 âˆ’ K| > Î´ C1 < R1 < C2 } on intervals of length 2Î´.
By choosing small Îµ and Î´, we can make the last term of (23) arbitrary small.
This ends the proof of (21) and as a result, we have the uniform closeness of
distributions (22).
Now let us consider Ï âˆˆ BT :
P{Ï†(S R T Ï) < x}
â‰¤ P{Ï†(S R T Ï) < x |R âˆ’ K| < Îµ} + P{|R âˆ’ K| > Îµ}
â‰¤ P{Ï†(S K T Ï) < x + MC Îµ} + P{|R âˆ’ K| > Îµ}
Similarly,
P{Ï†(S R T Ï) < x âˆ’ MC Îµ}
â‰¤ P{Ï†(S K T Ï) < x} + P{|R âˆ’ K| > Îµ}
As a result,
(24)

P{Ï†(S K T Ï) < x âˆ’ MC Îµ} âˆ’ P{|R âˆ’ K| > Îµ}
â‰¤ P{Ï†(S R T Ï) < x}
â‰¤ P{Ï†(S K T Ï) < x + MC Îµ} + P{|R âˆ’ K| > Îµ}

According to condition 3 of the lemma, for any Îµ > 0 we can make P{|R âˆ’ K| >
Îµ} arbitrary small uniformly over Ï âˆˆ BT . Function s â†’ Ï†(s K T Ï) is a con-

1439

INFERENCE IN AUTOREGRESSIVE MODELS

tinuous function uniformly with respect to (T Ï) (definition of H). Condition 2
and the continuous mapping theorem imply that


(25)
lim sup supP{Ï†(S K T Ï) < x} âˆ’ P{Ï†(Î¾ K T Ï) < x} = 0
T â†’âˆ ÏâˆˆB

x

T

where Î¾ has distribution F(x). Equations (24) and (25) imply that for any Îµ > 0
there is T1 such that for all T > T1 ,
(26)

P{Ï†(Î¾ K T Ï) < x âˆ’ Îµ} âˆ’ Îµ â‰¤ P{Ï†(S R T Ï) < x}
â‰¤ P{Ï†(Î¾ K T Ï) < x + Îµ} + Îµ

Since Ï†(Â· K T Ï) is a continuous function and F(x) is a continuous cumulative distribution function,
P{x âˆ’ Îµ < Ï†(Î¾ K T Ï) < x + Îµ} â†’ 0

as

Îµ â†’ 0

As a result, (26) implies


lim sup supP{Ï†(S R T Ï) < x} âˆ’ P{Ï†(Î¾ K T Ï) < x} = 0
T â†’âˆ ÏâˆˆB

x

T

The same statement is true for S1 . It gives us
lim sup sup |P{Ï• < x} âˆ’ P{Ï•1 < x}| = 0

T â†’âˆ ÏâˆˆB

x

T

Q.E.D.

and completes the proof.

LEMMA 9â€”A corollary of Theorem 2.18 from Hall and Heyde (1980): Let
Î¾j be a martingale difference sequence with E|Î¾j |Î² < âˆ for some 1 < Î² < 2. Then
for every Îµ > 0,
lim n

âˆ’1/Î²âˆ’Îµ

nâ†’âˆ

n


Î¾j = 0 a.s.

j=1

PROOF OF LEMMA 4: We start with (a). It is easy to see that

xj
i
Ïjâˆ’i Î·T
âˆš =
T
T
i=1
j

=

j

(Ïjâˆ’i âˆ’ Ïjâˆ’iâˆ’1 )Î·T
i=1

= âˆ’(1 âˆ’ Ï)

j

i=1

iâˆ’1
T

âˆ’ Î·T

Ïjâˆ’iâˆ’1 Î·T

i
T
i
T

+ Î·T

+ Î·T

j
T

âˆ’ Ïj Î·T

j

T

0
T

1440

ANNA MIKUSHEVA

âˆš
j
A similar expression is true for zj : zj / T = âˆ’(1 âˆ’ Ï) i=1 Ïjâˆ’iâˆ’1 wT (i/T ) +
wT (j/T ). So


 xj
zj 

sup sup âˆš âˆ’ âˆš 
j
T
T
ÏâˆˆÎ˜+
T
â‰¤ sup sup(1 âˆ’ Ï)
ÏâˆˆÎ˜+
T

j

j


Ï

i=1



Î·T i

T

j 
T 


jâˆ’iâˆ’1

âˆ’ wT


i 
T 



j
+ supÎ·T
âˆ’ wT
T
j

j

|1 âˆ’ Ï|
â‰¤ sup
Ïjâˆ’i + 1 sup |Î·T (t) âˆ’ wT (t)|
sup
Ï
+
j
0â‰¤tâ‰¤1
ÏâˆˆÎ˜
i=1
T

= o(T âˆ’1/2+1/r+Îµ )
(b) We have




j

 xj 
|1 âˆ’ Ï|
jâˆ’i


Ï + 1 sup |w(t)|
sup
sup sup  âˆš  â‰¤ sup
Ï j=1T i=1
T
j=1T
0â‰¤tâ‰¤1
ÏâˆˆÎ˜+
ÏâˆˆÎ˜+
T
T
â‰¤ 2 sup |wT (t)|
0â‰¤tâ‰¤1

For (c), we note that
T
T

1 
Îµj
j
Îµ1
Îµj
Î·T
Îµj =
âˆš
âˆš + Â·Â·Â· + âˆš
âˆš
T
T j=1
T
T
T
j=1
 T
2
T
1  2
1
1  Îµj
+
(Îµj âˆ’ 1) +
=
âˆš
2 j=1 T
2T j=1
2
T
1
1  2
1
= (Î·T (1))2 +
(Îµj âˆ’ 1) + 
2
2T j=1
2

According to Lemma 9,

T
j=1

2
(ÎµTj
âˆ’ 1) = o(T 2/r+Îµ ) a.s. Consequently,

T
1 
1
1
j
Îµj = (Î·T (1))2 + + o(T 2/râˆ’1+Îµ )
Î·T
âˆš
T
2
2
T j=1

INFERENCE IN AUTOREGRESSIVE MODELS

1441

By similar arguments,
T
1 
1
1
j
wT
eTj = (wT (1))2 + + o(T 2/râˆ’1+Îµ )
âˆš
T
2
2
T j=1

As a result,


T
T

 1 
1 
j
j


Î·T
wT
Îµj âˆ’ âˆš
eTj 
âˆš

 T
T
T
T
j=1
j=1
1
sup (Î·T (t) âˆ’ wT (t))2 + o(T 2/râˆ’1+Îµ )
2 0â‰¤tâ‰¤1


â‰¤ sup |Î·T (t) âˆ’ w(t)| sup |Î·T (t)| + sup |w(t)| + o(T 2/râˆ’1+Îµ )

â‰¤

0â‰¤tâ‰¤1

0â‰¤tâ‰¤1

0â‰¤tâ‰¤1

= o(T âˆ’1/2+1/r+Îµ ) + o(T 2/râˆ’1+Îµ ) = o(T âˆ’1/2+1/r+Îµ )
(d) The formula for discrete integration by parts gives us
T
T

xjâˆ’1
j
xj
1
xjâˆ’1 Îµj =
Î·T
âˆš âˆ’âˆš
T j=1
T
T
T
j=1
T
T

zj
zjâˆ’1
1
j
zjâˆ’1 eTj =
wT
âˆš âˆ’âˆš
T j=1
T
T
T
j=1

xT
+ âˆš Î·T (1)
T
zT
+ âˆš wT (1)
T

âˆš
âˆš
âˆš
âˆš
We note that xjâˆ’1 / T âˆ’ xj / T = (1 âˆ’ Ï)xjâˆ’1 / T âˆ’ Îµj / T . As a result,
T
T
1
1
xjâˆ’1 Îµj âˆ’
zjâˆ’1 eTj
T j=1
T j=1

= (1 âˆ’ Ï)

âˆ’

 xjâˆ’1
j
âˆš Î·T
T
T
j

zjâˆ’1
j
âˆ’ âˆš wT
T
T

T
T
1 
1 
j
j
Îµj âˆ’ âˆš
eTj
Î·T
wT
âˆš
T
T
T j=1
T j=1

xT
zT
+ âˆš Î·T (1) âˆ’ âˆš wT (1) 
T
T



1442

ANNA MIKUSHEVA

By applying (a) and (b) it is easy to see that


 xjâˆ’1
j
j 
zjâˆ’1

sup  âˆš Î·T
âˆ’ âˆš wT
T
T 
T
T
ÏâˆˆÎ˜+
T


 xjâˆ’1 zjâˆ’1 

â‰¤ sup sup âˆš âˆ’ âˆš  sup |wT (t)|
j
T
T 0â‰¤tâ‰¤1
ÏâˆˆÎ˜+
T


 xjâˆ’1 

+ sup |wT (t) âˆ’ Î·T (t)| sup sup âˆš 
j
0â‰¤tâ‰¤1
T
ÏâˆˆÎ˜+
T
= o(T âˆ’1/2+1/r+Îµ )
As a result, from (c) we have


T
T
1 

1
1


xjâˆ’1 Îµj âˆ’
zjâˆ’1 eTj  = o(T âˆ’1/2+1/r+Îµ )
sup



T j=1
ÏâˆˆÎ˜+ (1 âˆ’ Ï)T + 1 T j=1
T

Statements (e) and (f) can be obtained from (a) and (b):


T
T
 1 
1  2 

2
xjâˆ’1 âˆ’ 2
z 
 2
T
T j=1 jâˆ’1 
j=1






 xj
 xj 
 zj 
zj 




â‰¤ sup âˆš âˆ’ âˆš  sup âˆš  + sup âˆš  = o(T âˆ’1/2+1/r+Îµ )
j
j
j
T
T
T
T


T
T

 1 
1 


xjâˆ’1 j k âˆ’ 3/2+k
zjâˆ’1 j k 
 3/2+k

T
T
j=1

j=1



T
 xj
zj  1  k
â‰¤ sup âˆš âˆ’ âˆš  k+1
j = o(T âˆ’1/2+1/r+Îµ )
T
j
T
T
j=1
To check statements (g) and (h), we use statements (d), (e), and (f):


T
T
 1 


1


Âµ
Âµ
sup  2
(yjâˆ’1
)2 âˆ’ 2
(zjâˆ’1
)2 

T
ÏâˆˆÎ˜T  T
j=1
j=1


T
T
 1 
1  2 

2
= sup  2
xjâˆ’1 âˆ’ 2
zjâˆ’1 

T
ÏâˆˆÎ˜T  T
j=1

j=1


2 
2 
T
T
 1 

1 


+ sup 
x
âˆ’
z

jâˆ’1
jâˆ’1
3/2
3/2


T
T
ÏâˆˆÎ˜T
j=1

j=1

1443

INFERENCE IN AUTOREGRESSIVE MODELS

= o(T âˆ’1/2+1/r+Îµ ) a.s.


T
T
1 
1
1  Âµ 

Âµ
yjâˆ’1 Îµj âˆ’
z ej 
sup

T j=1 jâˆ’1 
ÏâˆˆÎ˜T (1 + Ï)T + 1  T j=1
=

1
(1 + Ï)T + 1





T
T

 1 
1 


Î·
w
x
(1)
âˆ’
z
(1)
Ã— sup 

jâˆ’1
T
jâˆ’1
T
3/2
3/2


T
T
ÏâˆˆÎ˜T
j=1
j=1


T
T
1 

1
1


+
xjâˆ’1 Îµj âˆ’
zjâˆ’1 ej 
sup 

(1 + Ï)T + 1 ÏâˆˆÎ˜T  T j=1
T j=1

= o(T âˆ’1/2+1/r+Îµ )

a.s.

We receive (g) and (h) from the two convergence statements above by
noticing that supÏâˆˆA+ (T 2 /(g(T Ï))) = O(T 1âˆ’Î± ) This completes the proof of
T
Lemma 4.
Q.E.D.
LEMMA 10 â€”Corollary to Theorem 2 in SzÃ©kely and Bakirov (2003): For
every Îµ > 0 there exists C > 0 such that


T

1
Âµ
2
(z ) < C < Îµ
sup sup PÏ
g(T Ï) t=1 tâˆ’1
T ÏâˆˆÎ˜
That is, the statistic RN is uniformly separated from 0.
PROOF OF THEOREM 1: One can check that the conditions of Lemma 2
are satisfied for sets AT and BT defined by (11) and (12) for 34 + 2r1 < Î± < 1.
Condition 1 follows from (g) and (h) of Lemma 4. Condition 4 is checked in
Lemma 10.
Below we check conditions 2 and 3. It is easy to see that
S(T Ï)

=

T
2
(1 âˆ’ Ï )g(T Ï)



T
1 âˆ’ Ï2 
xtâˆ’1 Îµt âˆ’
T
t=1

R(T Ï)
T
1
T
1 âˆ’ Ï2  2
xtâˆ’1 âˆ’
=
Â·
2
(1 âˆ’ Ï )g(T Ï)
T
T
t=1







T


1
g(T Ï)

1
g(T Ï)


xtâˆ’1 Îµ

t=1

T

t=1

2
xtâˆ’1



1444

ANNA MIKUSHEVA

Giraitis and Phillips (2006, Lemmas 2.1 and 2.2) proved that the following
statements about convergence hold uniformly over BT :

 

T


1 âˆ’ Ï2 


xjâˆ’1 Îµj â‰¤ x âˆ’ (x) = 0
lim sup supP
T â†’âˆ ÏâˆˆB


T
x
T
j=1



T
 1 âˆ’ Ï2 



lim sup P 
x2jâˆ’1 âˆ’ 1 >  = 0
T â†’âˆ ÏâˆˆB


T
T
j=1


T
Let us note that Îµ â†’p 0. The fact that the term (1/ g(T Ï)) t=1 xtâˆ’1 is
bounded in probability uniformly over BT can be shown by checking that its
second moment is uniformly bounded. One also can check that




T

âˆ’ 1 = 0
lim sup 
2
T â†’âˆ ÏâˆˆB (1 âˆ’ Ï )g(T Ï)
T

Combining all the facts mentioned above, conditions 2 and 3 of Lemma 2 are
satisfied.
Q.E.D.
PROOF OF THEOREM 2: According to Theorem 1 distribution of Ï†(S R
T Ï) is uniformly approximated by the distribution of Ï†(S N  RN  T Ï). As a
result, it is enough to check the conditions of Lemma 2 for two pairs of statistics
(S1  R1 ) = (S N  RN ) and (S R) = (S c(TÏ)  Rc(TÏ) ), and sets AT and BT defined
by (11) and (12). Condition 1 follows from Lemma 5. Condition 4 has been
checked in Lemma 10.
We check that conditions 2 and 3 are satisfied. By simple calculations we
1
determine that limcâ†’âˆ’âˆ | âˆ’ 2cg(c) âˆ’ 1| = 0 and limcâ†’âˆ’âˆ (âˆ’2c)E( 0 Jc (r) dr)2 =
0. As a result, convergence (14) implies that as c â†’ âˆ’âˆ,

S =
c


1
âˆ’2c
âˆ’2cg(c)

1


Jc (x) dw(x) âˆ’ w(1) âˆ’2c

1

Jc (r) dr

0

0

â‡’ N(0 1)
1
(âˆ’2c)
R =
âˆ’2cg(c)

1

c


1
âˆ’2c
J (x) dx âˆ’
âˆ’2cg(c)

0

2

1

2
c

Jc (r) dr
0

â†’p 1
Since limT â†’âˆ maxÏâˆˆBT c(T Ï) = âˆ’âˆ, conditions 2 and 3 are satisfied for the
Q.E.D.
pair (S c(TÏ)  Rc(TÏ) ).

1445

INFERENCE IN AUTOREGRESSIVE MODELS

PROOF
we have

OF

LEMMA 5: (a) From the isomorphic property of Itoâ€™s integrals,


E 

T


1
g(T Ï)

zjâˆ’1 ej âˆ’ 

j=1

g(c(T Ï))

2

1

1

Jc (t) dw(t)
0

t

1

=

(f1 (t s T Ï) âˆ’ f2 (t s T Ï))2 ds dt
0

0


Let us introduce functions
f3 (t s T Ï) = T/( g(T Ï))elog(Ï)([T t]âˆ’[T s]âˆ’1) and

f4 (t s T Ï) = T/( g(T Ï))elog(Ï)T (tâˆ’s) :
t

1

(f1 âˆ’ f2 )2 ds dt
0

0
t

1

((f1 âˆ’ f3 )2 + (f3 âˆ’ f4 )2 + (f4 âˆ’ f2 )2 ) ds dt

â‰¤2
0

0

It is easy to see that
t

1

(f1 âˆ’ f3 )2 ds dt

lim sup

T â†’âˆ ÏâˆˆA

0

T

0

= lim sup
T â†’âˆ ÏâˆˆA

T

= lim sup
T â†’âˆ ÏâˆˆA

T

T2
g(T Ï)

1

t

e2 log(Ï)([T t]âˆ’[T s]âˆ’1) ds dt
0

[T t]/T

T
Ïâˆ’2 = 0
g(T Ï)

For the second term, we have
t

1

(f3 âˆ’ f4 )2 ds dt
0

0

=
â‰¤

T2
g(T Ï)

1
0

T
g(T Ï)

0

 log(Ï)([T t]âˆ’[T s]âˆ’1)
2
âˆ’ elog(Ï)T (tâˆ’s) ds dt
e

t



log(Ï)([T t] âˆ’ [T s] âˆ’ 1) âˆ’ log(Ï)T (t âˆ’ s)2

0
1

2

t

0

Ã— e2 log(Ï)([T t]âˆ’[T s]âˆ’1) ds dt
â‰¤ 2 log2 (Ï)

1446

ANNA MIKUSHEVA

We used the inequality |eâˆ’a âˆ’ eâˆ’b | â‰¤ |a âˆ’ b|eâˆ’a that holds for 0 < a < b. As a
result,
t

1

lim sup

T â†’âˆ ÏâˆˆA

0

T

(f3 âˆ’ f4 )2 ds dt â‰¤ lim T âˆ’2Î± = 0
T â†’âˆ

0

Finally, by simple calculation we can as certain that

 2

 T g(c(T Ï))

âˆ’ 1 = 0
lim sup 
T â†’âˆ ÏâˆˆA
g(T Ï)
T
1

t

As a result, limT â†’âˆ supÏâˆˆAT 0 0 (f4 âˆ’ f2 )2 ds dt = 0. This completes the proof
of (a).
For (b), we note that
2

T
1

1
1
z2 âˆ’
(Jc (t))2 dt
E
g(T Ï) j=1 jâˆ’1 g(c(T Ï)) 0
1



0

0



(f1 + f2 ) dWs

0

0

1

4

1

(f1 âˆ’ f2 ) dWs

E
0

2

dt

2

1

(f1 âˆ’ f2 ) dWs
0



f2 (t s) dw(s)
0

2

1

â‰¤E

2

1

âˆ’

f1 (t s) dw(s)
1

â‰¤

2

1

=E



1

dt
4

1

(f1 + f2 ) dWs

dt E

0

0

dt

0

From Theorem 4 in Chapter 2 of Skorokhod (1965), we get
4

1

(f1 (t s) âˆ’ f2 (t s)) dWs

E

1

â‰¤ 36

0

(f1 (t s) âˆ’ f2 (t s))4 ds
0

and
4

1

(f1 (t s) + f2 (t s)) dWs

E
0

1

â‰¤ 36

(f1 (t s) + f2 (t s))4 ds
0

It is easy to check that
1

1

(f1 (t s) + f2 (t s))4 ds dt = O(1) as T â†’ âˆ

sup
ÏâˆˆAT

0

0
1

1

The proof that limT â†’âˆ supÏâˆˆAT 0 0 (f1 (t s) âˆ’ f2 (t s))4 ds dt = 0 is completely
analogous to that of part (a). This finishes the proof of part (b).

1447

INFERENCE IN AUTOREGRESSIVE MODELS

(c) We have


1

E âˆš 
T g(T Ï)

T


zjâˆ’1 âˆ’ 

j=1

2

1

1

Jc(TÏ) (t) dt

g(c(T Ï))

0

t

1

=E

f1 (s t T Ï) dw(s) dt
0

0
2

t

1

âˆ’

f2 (s t T Ï) dw(s) dt
0

1

1

2

1

f1 (s t) dt âˆ’

=
s

0
1

dt

0

f2 (s t) dt

ds

s

1

â‰¤

(f1 âˆ’ f2 )2 ds dt
0

0

â†’0
uniformly over AT as T â†’ âˆ.
(d) and (e). By simple algebraic manipulation, we get
S =
Sc = 

T


1

N

g(T Ï)


zjâˆ’1 ej âˆ’

j=1

1 
ejâˆ’1
T 1/2 j=1
T

1

1

Jc (t) dw(t) âˆ’ w(1) 

g(c)

0



1

âˆš

g(T Ï) T

T



zjâˆ’1 

j=1

1

1
g(c)

Jc (t) dt
0

and

1
z2 âˆ’
R =
g(T Ï) j=1 jâˆ’1
T




N

1
Rc =
g(c)

Jc2 (t) dt âˆ’
0

g(T Ï)T 1/2



1

T


1



zjâˆ’1



j=1

2

1

1
g(c)

2

Jc (t) dt



0

Thus, statements (a), (b), and (c) of Lemma 5 imply statements (d)
and (e).
Q.E.D.
LEMMA 11: Let {ÎµTj ; j = 1     T ; T âˆˆ N} be a triangular array of random
variables, such that for every T , variables {ÎµTj }Tj=1 are independent and identically
distributed with distribution FT . Assume that yTj = ÏyTjâˆ’1 + ÎµTj . Then, for any

1448

ANNA MIKUSHEVA

sequence ÏT such that T (1 âˆ’ ÏT ) â†’ âˆ, we have

 

T



1


sup
sup supP 
yTjâˆ’1 ÎµTj < x âˆ’ (x)
lim
T â†’âˆ F âˆˆL (KMÎ¸) |Ï|â‰¤Ï


g(T
Ï)
x
r
T
T
j=1

=0
and, for every  > 0,



T
 1




2
lim
sup
sup P 
yTjâˆ’1 âˆ’ 1 >  = 0
T â†’âˆ F âˆˆL (KMÎ¸) |Ï|â‰¤Ï
 g(T Ï)

r
T
T
j=1



T



1


lim
sup
sup P  
yTjâˆ’1  >  = 0
âˆš
T â†’âˆ F âˆˆL (KMÎ¸) |Ï|â‰¤Ï
 g(T Ï) T

r
T
T
j=1
LEMMA 12: Let {ÎµTj ; j = 1     T ; T âˆˆ N} be a triangular array of random
variables, such that for every T , the variables {ÎµTj }Tj=1 are independent and identically distribution with cumulative distribution function FT âˆˆ Lr (K M Î¸). Then
[T t]
we can construct a process Î·T (t) = âˆš1T j=1 ÎµTj and Brownian motions wT on a
common probability space in such a way that for every Îµ > 0, we have
lim

sup

T â†’âˆ F âˆˆL (KMÎ¸)
r
T



P sup |Î·T (t) âˆ’ wT (t)| > ÎµT âˆ’Î´ = 0
0â‰¤tâ‰¤1

for some Î´ > 0.
Proofs of Lemmas 11 and 12 (which are restated as Lemma S5 and S6, respectively) can be found in the supplementary appendix.
PROOF OF THEOREM 3: (1) Given the validity of Theorem 1, it is enough
to prove that the bootstrapped statistics are uniformly approximated by statistics in a model with normal errors. We follow the framework proposed in
Lemma 2. We check the conditions of Lemma 2 for two pairs of statistics
(S1  R1 ) = (S N  RN ) and (S R) = (S âˆ—  Râˆ— ), and sets AT and BT defined by (11)
and (12). Condition 1 of Lemma 2 follows from Lemma 12 and reasoning completely parallel to that in Lemma 4, changing the speed of convergence from
âˆ’1/2 + 1/r to âˆ’Î´. Conditions 2 and 3 of Lemma 2 follow from Lemma 11.
Conditions 4 and 5 were checked in the proof of Theorem 1.
Statement (2) of Theorem 3 trivially follows from statement (1).
(3) From the definition of the grid bootstrap set, we have that Ï âˆˆ C(Y )
if and only if PÏâˆ— {Ï†(S1âˆ—  Râˆ—1  T Ï) > Ï†(S R T Ï)|FT } > Î±/2 and PÏâˆ— {Ï†(S1âˆ—  Râˆ—1 

1449

INFERENCE IN AUTOREGRESSIVE MODELS

T Ï) < Ï†(S R T Ï)|FT } > Î±/2. It is easy to see that
sup PÏ PÏâˆ— {Ï†(S1âˆ—  Râˆ—1  T Ï) > Ï†(S R T Ï)|FT } < Î±/2
ÏâˆˆÎ˜

â‰¤ sup PÏ F(Ï†(S R T Ï) T Ï) < Î±/2 + Îµ





ÏâˆˆÎ˜



+ sup PÏ sup supPÏ {Ï†(S R T Ï) < x}
ÏâˆˆÎ˜

ÏâˆˆÎ˜

x



âˆ’ PÏâˆ— {Ï†(S1âˆ—  Râˆ—1  T Ï) < x|Î£T } > Îµ 

where F(x T Ï) = PÏ {Ï†(S R T Ï) < x}. The second term goes to zero for
every Îµ > 0. The random variable F(Ï†(S R T Ï) T Ï) has a uniform distribution over [0 1], that is,

sup PÏ F(Ï†(S R T Ï) T Ï) < Î±/2 + Îµ = Î±/2 + Îµ
ÏâˆˆÎ˜

As a result, for every Îµ > 0, we have limT â†’âˆ infÏâˆˆÎ˜ PÏ {Ï âˆˆ Î˜T } â‰¥ 1 âˆ’ (Î± + 2Îµ),
so the coverage probability of C(Y ) converges to 1 âˆ’ Î±.
Q.E.D.
PROOF OF LEMMA 6: First of all, we check that the residual based bootstrap
produces FT that belongs to the Lr (K M Î¸) class. The first condition of the
class is trivially satisfied. For the third condition, we have
T
T
T
1 Âµr
1
1 r
|
et | â‰¤ Cr
|Îµt | + Cr
|
et âˆ’ ÎµtÂµ |r 
T t=1
T t=1
T t=1

T
T
Âµ
Âµ
Âµ
/ i=1 (yiâˆ’1
)2 )yjâˆ’1
. Let us consider each term sepwhere 
ej âˆ’ ÎµjÂµ = ( i=1 Îµi yiâˆ’1
arately. The first term is bounded almost surely due to the strong law of large
numbers. For the second term, we note that for every Îµ > 0, we have
 T
r T
Âµ
T
1  j=1 Îµj yjâˆ’1   Âµ r
1
Âµ r
|
ej âˆ’ Îµj | =  T
|yjâˆ’1 |

Âµ
T j=1
T  j=1 (yjâˆ’1
)2  j=1
r  T
 T
r/2
Âµ
1  j=1 Îµj yjâˆ’1   Âµ 2
â‰¤  T
|ytâˆ’1 |

Âµ
T  j=1 (yjâˆ’1
)2  t=1
 1 T

Âµ r
âˆš
Îµj yjâˆ’1
j=1
g(TÏ)
1
âˆ’1+Îµ
= 
)
r/2 = op (T
T
1
T
(y Âµ )2
g(TÏ)

j=1

jâˆ’1

1450

ANNA MIKUSHEVA

Now we check the second condition for the residual based bootstrap,
T
1 2

e âˆ’1=
T j=1 j



 1 T
Âµ 2

âˆš
T
j=1 Îµj yjâˆ’1
g(TÏ)
1 Âµ 2
1
(Îµ ) âˆ’ 1 + 3

T
Âµ
1
2
T j=1 j
T g(TÏ)
j=1 (yjâˆ’1 )

that converges almost surely to zero with a nontrivial speed since E|Îµj |r < âˆ
for r > 2.
For the error based bootstrap, the errors under the null are et (Ï) = Îµt â€”the
true errors. Then all conditions of the Lr (K M Î¸) class can be gotten directly
from the strong law of large numbers.
Q.E.D.
PROOF OF THEOREM 4: Let us fix a negative number c and define ÏT =
1 + c/bT . Then T (1 âˆ’ ÏT ) = âˆ’(c/bT )T â†’ âˆ. According to Park (2003) and
Giraitis and Phillips (2006), we have the convergence


lim supPÏT {t(T ÏT ) < x} âˆ’ (x) = 0

T â†’âˆ

x

The t-statistics calculated for the sample that has only bT observations follow
the local-to-unity asymptotics with the local parameter c. In particular, from
Lemma S13 of the supplementary appendix, we have

 1 Âµ


Kc (x) dw(x)


0
t1 (b) < x} âˆ’ P 
< x  = 0
lim supPÏT {
T â†’âˆ x 

1
Âµ
(Kc (x))2 dx
0
âˆš
where KcÂµ (t) is the demeaned process Kc (s) = Jc (s) + (ecs / âˆ’2c)Î¾ and Î¾ âˆ¼
N(0 1) is independent of w. The difference between Jc and Kc is due to the
nonzero initial value of z0 ; see more on this in Elliott (1999) and Elliott and
Stock (2001).
According to Lemma S14 of the supplementary appendix we, have

 1 Âµ


K
(s)
dw(s)


c
< x  = 0 in probability
lim supLTb (x) âˆ’ P 0
T â†’âˆ x 

1
(KcÂµ (s))2 ds
0
Since the cumulative distribution function for the local-to-unity limit is a continuous function, the convergence above gives us the convergence of the quantiles
qÎ±L (T b) âˆ’ qÎ±c â†’ 0

in probability

1451

INFERENCE IN AUTOREGRESSIVE MODELS
1

Here qÎ±c denotes the Î± quantile of the distribution of ( 0 KcÂµ (x) dw(x))/

1
( 0 (KcÂµ (x))2 dx). Finally, as shown in Section S6 of the supplementary appendix,
c
c
lim PÏT {ÏT âˆˆ C(T bT )} = (q1âˆ’Î±/2
) âˆ’ (qÎ±/2
) < 1 âˆ’ Î±

T â†’âˆ

Q.E.D.

REFERENCES
ANDREWS, D. W. K. (1993): â€œExactly Median-Unbiased Estimation of First Order Autoregressive/Unit Root Models,â€ Econometrica, 61, 139â€“165. [1411-1413,1420,1426,1432]
ANDREWS, D. W. K., AND H.-Y. CHEN (1994): â€œApproximately Median-Unbiased Estimation of
Autoregressive Models,â€ Journal of Business & Economic Statistics, 12, 187â€“202. [1431]
ANDREWS, D. W. K., AND P. GUGGENBERGER (2007a): â€œHybrid and Size-Corrected Subsample
Methods,â€ Discussion Paper 1606, Cowles Foundation. [1429,1430]
(2007b): â€œThe Limit of Finite Sample Size and a Problem with Subsampling,â€ Discussion
Paper 1605, Cowles Foundation. [1429]
BOBKOSKI, M. J. (1983): â€œHypothesis Testing in Nonstationary Time Series,â€ Unpublished Ph.D.
Thesis, Department of Statistics, University of Wisconsin, Madison. [1417,1423]
CAVANAGH, C. (1985): â€œRoots Local to Unity,â€ Unpublished Manuscript, Harvard University.
[1417,1423]
CHAN, N. H., AND C. Z. WEI (1987): â€œAsymptotic Inference for Nearly Nonstationary AR(1)
Processes,â€ The Annals of Statistics, 15, 1050â€“1063. [1417,1423]
DICKEY, D. A., AND W. A. FULLER (1979): â€œDistribution of the Autoregressive Time Series with
a Unit Root,â€ Journal of the American Statistical Society, Ser. B, 50, 338â€“354. [1411]
DUFOUR, J.-M., AND M. L. KING (1991): â€œOptimal Invariant Tests for the Autocorrelation Coefficient in Linear Regressions with Stationary or Nonstationary AR(1) Errors,â€ Journal of
Econometrics, 47, 115â€“143. [1416]
ELLIOTT, G. (1999): â€œEfficient Tests for an Autoregressive Unit Root when the Initial Observation
Is Drawn from Its Unconditional Distribution,â€ International Economic Review, 40, 767â€“783.
[1450]
ELLIOTT, G., T. J. ROTHENBERG, AND J. H. STOCK (1996): â€œEfficient Tests for an Autoregressive
Unit Root,â€ Econometrica, 64, 813â€“836. [1416]
ELLIOTT, G., AND J. H. STOCK (2001): â€œConfidence Intervals for Autoregressive Coefficients
Near One,â€ Journal of Econometrics, 103, 155â€“181. [1416,1450]
HALL, P., AND C. C. HEYDE (1980): Martingale Limit Theory and Its Application. New York: Academic Press. [1439]
HANSEN, B. E. (1999): â€œThe Grid Bootstrap and the Autoregressive Model,â€ Review of Economics
and Statistics, 81, 594â€“607. [1411-1413,1422,1425,1432,1433]
GIRAITIS, L., AND P. C. B. PHILLIPS (2006): â€œUniform Limit Theory for Stationary Autoregression,â€ Journal of Time Series Analysis, 27, 51â€“60. [1417,1421,1444,1450]
IMBS, J., H. MUMTAZ, M. O. RAVN, AND H. REY (2005): â€œPPP Strikes Back: Aggregation and the
Real Exchange Rate,â€ Quarterly Journal of Economics, 120, 1â€“43. [1411]
LEHMANN, E. L. (1997): Testing Statistical Hypothesis. New York: Springer-Verlag. [1413,1414]
MIKUSHEVA, A. (2007): â€œSupplement to â€˜Uniform Inference in Autoregressive Modelsâ€™,â€
Econometrica Supplementary Material, 75, http://www.econometricsociety.org/ecta/supmat/
6254_proofs.pdf. [1413]
MURRAY, CH. J., AND D. H. PAPELL (2002): â€œThe Purchasing Power Parity Persistence Paradigm,â€ Journal of International Economics, 56, 1â€“19. [1411]
NANKERVIS, J. C., AND N. E. SAVIN (1985): â€œTesting the Autoregressive Parameter with the
t-Statistic,â€ Journal of Econometrics, 27, 143â€“161. [1412]

1452

ANNA MIKUSHEVA

(1988): â€œThe Studentâ€™s t Approximation in a Stationary First Order Autoregressive
Model,â€ Econometrica, 56, 119â€“145. [1412]
Oâ€™REILLY, G., AND K. WHELAN (2005): â€œHas Euro-Area Inflation Persistence Changed Over
Time?â€ Review of Economics and Statistics, 87, 709â€“720. [1411]
PARK, J. Y. (2003): â€œWeak Unit Roots,â€ Unpublished Manuscript, Rice University. [1417,1428,
1450]
PHILLIPS, P. C. B. (1987): â€œToward a Unified Asymptotic Theory for Autoregression,â€ Biometrika,
74, 535â€“547. [1417,1423]
RAPACH, D. E., AND M. E. WOHAR (2004): â€œThe Persistence in International Real Interest
Rates,â€ International Journal of Finance and Economics, 9, 339â€“346. [1411]
RAYNER, R. K. (1990): â€œBootstrapping p-Values and Power in the First-Order Autoregression:
A Monte Carlo Investigation,â€ Journal of Business & Economic Statistic, 8, 251â€“263. [1412]
ROMANO, J. P., AND M. WOLF (2001): â€œSubsampling Intervals in Autoregressive Models with
Linear Time Trend,â€ Econometrica, 69, 1283â€“1314. [1412,1413,1427-1430]
SKOROKHOD, A. V. (1965): Studies in the Theory of Random Processes. Reading, MA: Addisonâ€“
Wesley. [1446]
STOCK, J. (1991): â€œConfidence Intervals for the Largest Autoregressive Root in US Macroeconomic Time Series,â€ Journal of Monetary Economics, 28, 435â€“459. [1411-1413,1422,1423,1430]
SZÃ‰KELY, G. J., AND N. K. BAKIROV (2003): â€œExtremal Probabilities for Gaussian Quadratic
Forms,â€ Probability Theory and Related Fields, 126, 184â€“202. [1421,1443]

