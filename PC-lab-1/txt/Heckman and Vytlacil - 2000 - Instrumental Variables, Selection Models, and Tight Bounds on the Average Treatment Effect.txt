 

&+) )   

   
  


  
      


  !"#$#
%$#&' $()* ,-.
%""*/001 ' (0** 02,-.
 

    
32-2%4"" 54
1 $( 2,367
4(4",222



  
 !"## $$
" $ $$ 
% &'& $(" $ $)* + $ 
%  $$ $ ,  % $,(   %- $ &  )
. /&/) 0&$$- /)1, ,)',,2*)!   $ %+$ 
+   2 &3   + ,  & $  *% ,, $ , $2.
$  2*$   )


           
      
       !   " 
#$   %  & ' # ()*
 (+++
, # -)+ ./
$%#%
  ' ' 0'     !    ''          
1 ''               23**+ 3**.4 !     
       

  '         2 4

     ''      0          "  23***
(+++ 4 !             0'    '   '   
    !   0           ' "         
  "                         ' ' !
"     ' !  !    '  "        % !
 5 234      "          '   '      
 6 2(4      ' "       "     '   
    ' 

     
7'    
8  "  -  
33(9   )*WK 
-   , 9+9/:
 #$
=;  <  

!   " 
7'    
8  "  -  
33(9   )*WK 
-   , 9+9/:
;"  <  

1 Introduction
A basic problem in evaluating social programs is that we do not observe the same individual in
both the treated and untreated state at the same time. A variety of econometric assumptions are
invoked to undo the consequences of this missing data. The traditional approach to this problem
is to invoke suÃ†cient assumptions about outcome equations, treatment selection equations, and
their interrelationship to point identify the treatment parameters. A more recent approach to
identi cation of treatment e ects is to conduct sensitivity or bounding analyses to present ranges
of estimates for estimated treatment parameters.
This paper exposits and relates two distinct approaches to bounding the average treatment
e ect. One approach, based on instrumental variables, is due to Manski (1990, 1994), who derives
tight bounds on the average treatment e ect under a mean independence form of the instrumental
variables (IV) condition.1 The second approach, based on latent index models, is due to Heckman
and Vytlacil (1999,2000a), who derive bounds on the average treatment e ect that exploit the
assumption of a nonparametric selection model with an exclusion restriction. Their conditions
imply the instrumental variable condition studied by Manski, so that their conditions are stronger
than the Manski conditions. In this paper, we study the relationship between the two sets of
bounds implied by these alternative conditions. We show that: (1) the Heckman and Vytlacil
1 Manski

also refers to this condition as a level-set restriction. See Robins (1989) and Balke and Pearl (1997) for
bounds that exploit a statistical independence version of the instrumental variables assumption. See Manski and
Pepper (2000) for bounds that exploit a weakened version of the instrumental variables assumption. Heckman,
Smith and Clements (1997) consider bounds on the distribution of treatment e ects in a randomized experiment.
See Heckman and Vytlacil (2000b) for a discussion of alternative approaches to the evaluation of treatment e ects,
including a survey of the bounding literature.

3

bounds are tight given their assumption of a nonparametric selection model; (2) the Manski
bounds simplify to the Heckman and Vytlacil bounds under the nonparametric selection model
assumption.
This paper is organized in the following way. In Section 2, we introduce notation and the basic
framework. We review the Manski IV bounds in Section 3, and review the Heckman and Vytlacil
nonparametric selection model bounds in Section 4. In Section 5, we show that the Heckman
and Vytlacil bounds are tight under the nonparametric selection model assumption. We compare
the Manski bounds to the Heckman and Vytlacil bounds in Section 6, and show that the Manski
bounds simplify to the Heckman and Vytlacil bounds under the nonparametric selection model
assumption. The paper concludes in Section 7 by relating the analysis of this paper to the analysis
of Balke and Pearl (1997).

2 Switching Regression Framework
For each person i, we observe (Y ; D ; W ), where Y is the outcome variable, D is an indicator
i

i

i

i

i

variable for receipt of treatment, and W is a vector of covariates. We assume that the outcome
i

variable is generated by a switching regression,

Y = D Y1 + (1
i

i

i

D )Y0 ,
i

i

where Y0 is the potential outcome if the individual does not receive treatment and Y1 is the
i

i

potential outcome if the individual does receive treatment. Y1 is observed if D = 1 but not
i

i

otherwise; Y0 is observed if D = 0 but not otherwise. We assume access to an i.i.d. sample,
i

i

4

and henceforth supress the i subscript. For any random variable A, we will use

A to denote the

support of A, a to denote a potential realization of A, and F to denote the distribution function
A

for A. In this paper, we will maintain the assumption that the outcome variables are bounded
with probability one:
For j = 0; 1, and for a.e. w 2 W , there exists y ; y

Assumption B.

Pr(y

l
w;j

l

u

w;j

w;j

2 < such that:

 Y  y jW = w) = 1.
j

u

w;j

In this paper, we examine bounds on the average treatment e ect (ATE), de ned for w 2 W
as2

E (Y1

Y0 jW = w):

By the law of iterated expectations:

Y0 jW = w)

E (Y1




= Pr[D = 1jW = w]E (Y1jW = w; D = 1) + Pr[D = 0jW = w]E (Y1 jW = w; D = 0)




Pr[D = 1jW = w]E (Y0jW = w; D = 1) + Pr[D = 0jW = w]E (Y0 jW = w; D = 0) :

The central identi cation problem in recovering this parameter from observational samples is that
we do not observe Y0 for individuals with D = 1, and we do not observe Y1 for individuals with
potential parameter of interest is the e ect of treatment on the treated, E (Y1 Y0 jW = w; D =
1): Heckman and Vytlacil (1999,2000a) construct bounds for the treatment on the treated parameter given the
nonparametric selection model assumption. Manski's analysis can be easily extended to this parameter as well.
One can extend the results of this paper to show that the Heckman and Vytlacil bounds on treatment on the
treated are tight given the assumption of a nonparametric selection model, and to show that the Manski bounds
adapted to the treatment on the treated parameter simplify to the Heckman and Vytlacil bounds on the treatment
on the treated parameter under the assumption of a nonparametric selection model.
2 Another

5

D = 0. Thus, we can identify Pr[D = 1jW = w], E (Y1 jW = w; D = 1), and E (Y0 jW = w; D = 0),
but cannot identify the counterfactual means E (Y1 jW = w; D = 0) or E (Y0 jW = w; D = 1).
Assumption (B) immediately implies that E (Y1 jW = w; D = 0) and E (Y0 jW = w; D = 1)
are bounded, and thus we can follow Manski (1989) and Robins (1989) in bounding the ATE
parameter as follows,3

B

L
w

 E (Y

1

Y0 jW = w)  B ;
U

w

where


B

L
w

=



Pr[D = 1jW = w]E (Y1jD = 1; W = w) + Pr[D = 0jW = w]y

l
w;1





Pr[D = 0jW = w]E (Y0 jD = 0; W = w) + Pr[D = 1jW = w]y

u
w;0



B

U
w

=

Pr[D = 1jW = w]E (Y1jD = 1; W = w) + Pr[D = 0jW = w]y


u
w;1



Pr[D = 0jW = w]E (Y0 jD = 0; W = w) + Pr[D = 1jW = w]y


l
w;0

:

For every value in the interval [B ; B ]; one can trivially construct a distribution of (Y1 ; Y0 ; D; W )
L

u

w

w

which is consistent with the observed distribution of (Y; D; W ) and such that the average treatment e ect equals the speci ed value. Thus, every point in the interval [B ; B ] must be contained
L
w

u
w

in any bounds on the average treatment e ect, and thus these bounds are tight under the given
3 Smith

and Welch (1986) construct analogous bounds on E (Y1 ) using the law of iterated expectations and the
restriction that 21 E (Y1 jW = w; D = 1)  E (Y1 jW = w; D = 0)  E (Y1 jW = w; D = 1); where the lower bound is
assumed to be known a priori.

6

information structure. Note that the width of the bounds is
Pr[D = 1jW = w](y

y 0) + (1

u

l

w;0

w;

Pr[D = 1jW = w])(y

u

w;1

y 1):
l

w;

Note that the width of the bounds depends only on Pr[D = 1jW = w] and y ; y ; j = 0; 1.
u
w;j

l
w;j

3 Bounds Under an IV Condition
We rst review the analysis of Manski (1990).4 Partition W as W = [X; Z ], where Z denotes
the instrument(s). He considers identi cation or bounding of the average treatment e ect under
a mean-independence form of the IV assumption:
Assumption IV.

E (Y jX; Z ) = E (Y jX ) for j = 0; 1.
j

j

Note that Assumption IV immediately implies that the average treatment e ect depends only on

X , E (Y1

Y0 jX = x; Z = z ) = E (Y1

Y0 jX = x). Let Z denote the support of Z conditional
x

on X = x. Let

P (z; x) = Pr[D = 1jZ = z; X = x]:
4 See

also Manski (1994) for a further development of these bounds.

7

Using the law of iterated expectations and the assumption that E (Y1 jX; Z ) = E (Y1 jX ), for any

x in the support of X and z 2 Z ,
x

P (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1

P (z; x))y(
l

x;z );1

 E (Y jX = x)  P (z; x)E (Y jD = 1; X = x; Z = z) + (1
1

1

P (z; x))y(

u
x;z );1

:

Since these bounds hold for all z 2 Z , we have
x

sup fP (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1

z

P (z; x))y(
l

x;z );1

2Zx

 E (Y jX = x) 

g

inf fP (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1

1

z

2Zx

P (z; x))y(

u
x;z );1

g:

Following the parallel argument for E (Y0 jX = x), Manski derives the the following sharp bounds
on the average treatment e ect under the mean independence assumption:

I

L

x

 E (Y

1

Y0 jX = x)  I ;
U

x

with

I = sup fP (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1
L

x

z

2Zx

inf f(1

z

2Zx

P (z; x))y(
l

x;z );1

g

P (z; x))(E (Y0 jD = 0; X = x; Z = z ) + P (z; x)y(

u
x;z );0

8

g:

I = inf fP (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1
U

x

z

2Zx

sup f(1

z

Let

P

2Zx

P (z; x))y(

u

x;z );1

g

P (z; x))(E (Y0 jD = 0; X = x; Z = z ) + P (z; x)y(
l

x;z );0

g;

denote the support of P (Z; X ) conditional on X = x. Let p = sup P and p = inf P :
u

x

The width of the bounds is I

U
x

l

x

x

x

x

I , a complicated expression to evaluate. Note that the above
L

x

bounds exactly identify the average treatment e ect if I = I . A trivial modi cation of Corollary
U

L

x

x

1 and Corollary 2 of Proposition 6 of Manski (1994) shows that, under assumptions (B) and (IV),
(i) p

u
x



(ii) If Y

1
2

and p

l
x



1
2

is a necessary condition for I = I .

?? DjX , then p

L
x

u
x

U
x

= 1, p = 0 is a necessary and suÃ†cient condition for I = I :
l

L

U

x

x

x

Note that it is neither necessary nor suÃ†cient for P (z; x) to be a nontrivial function of z for
these bounds to improve upon the [B ; B ] bounds of Section 2. Evaluating the bounds and the
L

U

w

w

width of the bounds for a given x requires knowledge of P (z; x); E (Y1 jD = 1; X = x; Z = z ),

E (Y0 jD = 1; X = x; Z = z ), and y(
l

x;z );j

; y(

u
x;z );j

, j = 0; 1, for each z 2 Z .
x

4 Bounds Under the Nonparametric Selection Model
We now review the analysis of Heckman and Vytlacil (1999,2000a). They use a nonparametric selection model to identify or bound the average treatment e ect, where the nonparametric
selection model is de ned through the following assumption:
Assumption S

D = 1[(Z; X )  U ], with Z ?? (U; Y0; Y1 )jX:
9

This is clearly a stronger assumption than Assumption IV because of the treatment assignment
rule, because of the independence (rather than mean independence) between Z and (Y0 ; Y1 ) given

X , and because of the assumed independence between U and Z given X . Without loss of gener-

 P (Z; X )jZ = z; X =

ality, they impose the normalization that (z; x) = P (z; x) so that Pr[U

x] = P (z; x): Note that Z

?? (Y ; Y )jX immediately implies that the average treatment e
0

1

Y0 jX = x; Z = z ) = E (Y1

depends only on X , E (Y1

Y0 jX = x), and that y(

k
x;z );j

=y

k
x;j

ect
for

j = 0; 1, k = u; l.
Note that DY = DY1 is an observed random variable, and thus for any x 2 Supp(X ), p 2 P ,
x

we identify the expectation of DY1 given X = x; P (Z; X ) = p,




E DY1 X = x; P (Z; X ) = p

= E (Y1 jX = x; P (Z; X ) = p; D = 1)p
= E (Y1 jX = x; P (Z; X ) = p; P (Z; X )  U )p

(1)

= E (Y1 jX = x; p  U )p
Z

=
where the third equality follows from Z

p

E (Y0 jX = x; U = u)dF

j

U X

0

(ujx);

?? (U; Y ; Y ) j X; and the fourth equality follows from
0

1

the law of iterated expectations. By similar reasoning,


E (1



D)Y0 X = x; P (Z; X ) = p =

Z 1

E (Y0 jX = x; U = u)dF

j

U X

p

(ujx):

(2)

We can evaluate (1) at p = p and evaluate (2) at p = p . The distribution of (D; Y; X; Z ) contains
no information on

R1
pu
x

u

l

x

x

E (Y1 jX = x; U = u)dF

j

U X

10

(ujx) and

R

l
px

0

E (Y0 jX = x; U = u)dF

j

U X

(ujx),

but we can bound these quantities:
(1

p )y
u
x

py
l

x

l

x;1

l

x;0



R1



R pl

pu
x

E (Y1 jX = x; U = u)dF

E (Y0 jX = x; U = u)dF

x

j

(ujx)



(1

j

(ujx)



p y 0;

U X

U X

0

where we use the fact that Pr[U > p jX = x] = 1
u
x

p , and Pr[U
u
x

p )y
u
x

l

u

x

x;

 p jX

u

x;1

(3)

= x] = p . Since

l

l

x

x

Z ?? (Y0 ; Y1 ) j X; it follows that E (Y1 Y0 jX = x; Z = z ) = E (Y1 Y0 jX = x). These inequalities
Y0 jX = x) as in the following way:

allow Heckman and Vytlacil to bound E (Y1

S

L

x

 E (Y

Y0 jX = x)  S ;
u

1

x

where


S

L

x



= p E (Y1 jX = x; P (Z; X ) = p ; D = 1) + (1
u

u

x

x

p )y
u

l

x

x;1





p ) E (Y0 jX = x; P (Z; X ) = p ; D = 0)

(1

l

l

x

x



S

U
x

p y 0;
l

u

x

x;



= p E (Y1 jX = x; P (Z; X ) = p ; D = 1) + (1
u

u

x

x

p )y



u

u

x

x;1



p ) E (Y0 jX = x; P (Z; X ) = p ; D = 0)

(1

l

l

x

x

The width of the bounds is

S

U

x

S = (1
L

x

p )(y
u
x

u

x;1

11

y 1 ) + p (y
l

l

x;

x

u

x;0

y 0):
l

x;

p y 0:
l

l

x

x;

Trivially, p = 1, p = 0 is necessary and suÃ†cient for S = S .5 Note that it is both necessary
u

l

L

U

x

x

x

x

and suÃ†cient for P (z; x) to be a nontrivial function of z for these bounds to improve upon the
[B ; B ] bounds of Section 2. Evaluating the width of the bounds for a given x requires knowledge
L

U

w

w

only of p ; p , and y ; y , j = 0; 1. The only additional information required to evaluate the
l

u

l

u

x

x

x;j

x;j

bounds for a given x is E (Y0 jX = x; P (Z; X ) = p ; D = 0) and E (Y1 jX = x; P (Z; X ) = p ; D =
l

u

x

x

1): The simpler structure for the Heckman-Vytlacil bounds compared to the Manski bounds is a
consequence of the selection model structure imposed by Heckman and Vytlacil.

5 Tight Bounds
We now show that the Heckman and Vytlacil bounds are tight given the assumption that the
outcomes are bounded (Assumption B) and the nonparametric selection model (Assumption S).
Theorem 1 Impose the nonparametric selection model, Assumption S, and impose that the out-

come variables are bounded, Assumption B. Then the Heckman-Vytlacil bounds on ATE are tight.

Proof.

The logic of the proof is as follows. We show that the Heckman-Vytlacil bounds are tight
by showing that for any point s

2 [S

L
x

; S ], there exists a distribution with the following
U
x

properties: (i) the distribution is consistent with the observed data; (ii) the distribution is
consistent with all of the Heckman-Vytlacil assumptions; and (iii) E (Y1
5 That

(1990).

u

px

Y0 jX ) evaluated

= 1, plx = 0 is suÃ†cient for point identi cation of the average treatment e ects is shown by Heckman

12

under the distribution equals s. Thus, the point s must be contained in any bounds on
the average treatment e ect. Since this holds for every s

2

[S ; S ], we have that the
L

U

x

x

interval [S ; S ] must be contained in any bounds on the the average treatment e ect, and
L

U

x

x

thus [S ; S ] are tight bounds on the average treatment e ect. We prove the existence of
L

U

x

x

such a distribution by constructing one that conforms to conditions (i)-(iii) for any given

s 2 [S ; S ].
L

U

x

x

For any random variable A, let F 0 denote the \true" CDF of A, and let F 0 j (jb) denote
A

A B

the true CDF of A conditional on B = b. Let s denote any given element of [S ; S ]. Note
L

U

x

x

that any element s 2 [S ; S ] can be written as
L

U

x

x





s = p E (Y1 jX = x; P (Z; X ) = p ; D = 1) + (1
u

u

x

x

p )q 1
u
x

x



p ) E (Y0 jX = x; P (Z; X ) = p ; D = 0)

(1

for some q 0 ; q 1 s.t. y
x

x

l

x

q y
j

u

x

x



l

l

x

x

, j = 0; 1.

For (u; x) 2 Supp(U; X ), de ne

F

j

Y1 U;X

F

j

Y0 U;X

(y1ju; x) =

(y0ju; x) =

8
>
>
>
<

F 01 j
Y

U;X

>
>
>
:

(y1 ju; x) if u  p

u
x

1[y1  q 1 ]

8
>
>
>
<

x

F 00 j

>
>
>
:

Y

U;X

u
x

(y0 ju; x) if u  p

l
x

1[y0  q 0 ]

13

if u > p

x

if u < p :
l

x

p q0
l

x

x

De ne

F

Y0 ;Y1 ;U;X;Z

(y0 ; y1 ; u; x; z )
Z Z

=

0

u

F

j

Y0 U;X

(y0 jt ; t )F 1 j
u

x

Y



U;X

(y1 jt ; t )dF j (t jt )
u

x

0

U X

u

x

 1[t  x; t  z]dF
x

Where F 0

X;Z

z

0

X;Z

(t ; t ):
x

z

and F 0 j are the \true" distributions of (X; Z ) and of U conditional on X .
U X

Note that F is a proper CDF and that F is a distribution satisfying the conditions that

Y1 , Y0 are bounded conditional on X , and satisfying the property that Z is independent of
(Y0 ; Y1; U ) conditional on X .
By construction, F

X;Z;U

(x; z; u) = F 0

X;Z;U

addition, using the fact that F 1 j
Y

F

j

Y1 X;Z;D

1
(y1 jx; z; 1) =
P (z; x)

U;X

Z

(x; z; u) so that F

(y1 ju; x) = F 01 j

P (z;x)

Y

F 01 j
Y

0

U;X

X;Z;D

(x; z; d) = F 0

j

Y0 X;Z;D

(y1 ju; x) for u  p , we have
x

(y1 ju; x)dF 0 j (ujx) = F 01 j
U X

(y0 jx; z; 0) = F 00 j
Y

14

(x; z; d). In

u

U;X

for (x; z ) 2 Supp(X; Z jD = 1). By a parallel argument,

F

X;Z;D

X;Z;D

(y0 jx; z; 0)

Y

X;Z;D

(y1 jx; z; 1)

for (x; z ) 2 Supp(X; Z jD = 0). Combining these results, we have

F

Y ;X;Z;D

where Y = DY1 + (1

Y0 jX ) =

Y ;X;Z;D

(y; x; z; d);

D)Y0 . Thus, F is observationally equivalent to the true F 0 .
Y0 under F equals the given point s 2 [S ; S ]:

The expected value of Y1

E (Y1

(y; x; z; d) = F 0

Z Z

U

x



y1 dF

j

Y1 U;X

(y1 ju; x) dF 0 j (ujx)
U X

Z Z



y0 dF

j

Y0 U;X

Z Z

= Pr[U

L

x

p ]
u
x

0

Pr[U > p ]

(y0 ju; x) dF 0 j (ujx)
U X

u
px



y1 dF

Z Z 1

l

x

plx

0

j

(y1 ju; x) dF 0 j (ujx) + Pr[U > p ]q 1
u

Y1 U;X

U X

x



y0 dF 00 j
Y

U;X

(y0 ju; x) dF 0 (u)
U

= p E (Y1 jX; P (Z ) = p ; D = 1) + (1
u

u

x

x

p E (Y0 jX; P (Z ) = p ; D = 0)
l

l

x

x

Pr[U

 p ]q
l

0

x

x

x

p )q 1
u
x

x

p q0
l

x

x

= s:

Since the expected value of Y1 Y0 under F equals s, and since F satis es all of the required
properties of the nonparametric selection model and is observationally equivalent to the true

F 0 , we have that the point s must be contained in any bounds on the average treatment
e ect. Since this holds for any point s 2 [S ; S ], we have that every point in [S ; S ] must
L

U

L

U

x

x

x

x

be contained in any bounds on the average treatment e ect, and thus the bounds [S ; S ]
are tight.
15

L

U

x

x

6 Comparing the Bounds
We now compare the Heckman and Vytlacil bounds that exploit the nonparametric selection model
to the Manski bounds that exploit an instrumental variables assumption. The nonparametric
selection model of Heckman and Vytlacil implies the mean independence conditions of Manski,
so that Manksi's bounds hold under the Heckman and Vytlacil conditions. We now show that,
under the nonparametric selection model, the Manski bounds simplify to the simpler form of the
Heckman and Vytlacil bounds.
Theorem 2 Impose the nonparametric selection model, Assumption S, and impose that the out-

come variables are bounded, Assumption B. The Manski mean-independence bounds coincide with

the Heckman-Vytlacil bounds.

Proof.

We rst show that the rst term of the Heckman-Vytlacil upper bound on Y1 coincides
with the rst term of the Manski upper bound on Y1 :
inf fP (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1

z

2Zx

= p E (Y1 jD = 1; X = x; P (Z ) = p ) + (1
u

u

x

x

Note that Z

?? (Y ; Y ) j X implies that y
0

1

P (z; x))y(

u

x;z );1

p )y 1:

u

(x;z );1

16

g

u

u

x

x;

= y 1. Fix any x 2 Supp(X ) and x any
u

x;

z2Z .
x





p E (Y1 jD = 1; X = x; P (Z; X ) = p ) + (1
u

u

x

x



Z

=

u
px

0

E (Y1 jX = x; U = u)dF

Z

P (z;x)

E (Y1 jX = x; U = u)dF

j

U X

=



E (Y1 jX = x; U = u)dF

j

U X

Z

=

u
px

P (z;x)
u
px

P (z;x)





(ujx)

E (Y1 jX = x; U = u)

y

x;1

dF

(p
j

x;1

p )y
u

u

x

x;1

(ujx) + (1


u

P (z; x))y

U X

u



(ujx) + (1

j

U X

u

x;1

x

P (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1

0

Z

p )y
u

u
x



P (z; x))y

u

P (z; x))y

x;1

u
x;1

(ujx)

0:

Since this inequality holds for any z 2 Z , we have
x

p E (Y1 jD = 1; X = x; P (Z; X ) = p ) + (1
u

u

x

x



p )y
u

u

x

x;1

inf fP (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1

z

2Zx

Using the fact that E (Y1 jX = x; U = u)

y

u
x;1

P (z; x))y

u

x;1

g

is bounded and the de nition of p , we have
u
x

that

p E (Y1 jD = 1; X = x; P (Z; X ) = p ) + (1
u

u

x

x



p )y
u

u

x

x;1

inf fP (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1

z

2Zx

17

P (z; x))y

u
x;1

g

and thus

p E (Y1 jD = 1; X = x; P (Z; X ) = p ) + (1
u

u

x

x

p )y
u
x

u

x;1

= inf fP (z; x)E (Y1 jD = 1; X = x; Z = z ) + (1
z

2Zx

P (z; x))y

u
x;1

g:

By the parallel argument, all other terms of the two sets of bounds coincide.
Thus, under the assumption of a nonparametric selection model, the Manski bounds simplify
to the same form as the Heckman and Vytlacil bounds. This result is related to Corollary 2
of Proposition 6 of Manski (1994), which shows the same simpli cation of the bounds under the
strong assumption that the treatment choice is exogenous so that Y

?? DjX . Note that the Manski

bounds do not simplify if one does not impose additional restrictions. One can easily construct
examples where the Manski bounds do not simplify when the mean independence condition holds
but not the nonparametric selection model does not hold.
Somewhat suprisingly, the assumption of a nonparametric selection model does not narrow the
bounds compared to what is produced from the weaker mean-independence assumption. However,
imposing the nonparametric selection model substantially simpli es the tight mean-independence
bounds. Note that this simpli cation implies the following results for the tight mean-independence
bounds under the nonparametric selection model:
1. p = 1, p = 0 is necessary and suÃ†cient for point identi cation.
u

l

x

x

2. It is both necessary and suÃ†cient for P (z; x) to be a nontrivial function of z for the bounds
18

to improve upon the bounds that only impose that the outcome is bounded, [B ; B ].
L

U

w

w

3. Evaluating the width of the bounds for a given x requires knowledge only of p ; p , and
l

u

x

x

y ; y , j = 0; 1.
l

u

x;j

x;j

4. Evaluating the bounds for a given x requires knowledge only of p ; p , y ; y , j = 0; 1,
l

u

l

u

x

x

x;j

x;j

E (Y0 jX = x; P (Z; X ) = p ; D = 0) and E (Y1 jX = x; P (Z; X ) = p ; D = 1):
l
x

u
x

In each case, the result does not hold in general if the nonparametric selection model is not
imposed.

7 Applications to Other Bounds
Our results can be related to the analysis of Balke and Pearl (1997). For the case where Y and

Z are binary, Balke and Pearl consider bounds that impose the same statistical independence
condition as used by Imbens and Angrist (1994):
(Y1 ; Y0 ; D0 ; D1 ) ?? Z jX
where D denotes the counterfactual choice that would have been observed if Z had been externally
z

set to z . Note that this independence condition strengthens the Manski assumptions not only by
imposing statistical independence of potential outcomes from Z , instead of mean-independence
from Z , but also by imposing independence of the counterfactual choices from Z . When Z and

Y are binary, Balke and Pearl show that the sharp bounds under their statistical independence
19

condition are narrower in general than the Manski bounds, although their bounds and the Manski
bounds coincide for some distributions of the observed data. In the context of binary Z and

Y , Balke and Pearl discuss the Imbens and Angrist monotonicity condition: either D1
everywhere or D1

D

0



D0

everywhere. They show that this assumption imposes constraints on

the observed data which imply that their bounds and the Manski mean-independence bounds
coincide.6
As demonstrated by Vytlacil (2000), imposing nonparametric selection model (Assumption S)
is equivalent to imposing the independence and monotonicity conditions of Imbens and Angrist.
The Heckman and Vytlacil analysis imposes the nonparametric selection model. Thus, for the
nonparametric selection model, we have from the analysis of Balke and Pearl that the tight bounds
when Y and Z are binary are the Manski mean-independence bounds. Thus, the analysis of this
paper can be seen as an extension of the Balke and Pearl analysis of the special case of binary Y
and Z under the independence and monotonicity conditions. They show that the tight bounds for
binary Y and Z under the independence and monotonicity conditions coincide with the Manski
mean-independence bounds. Our analysis shows that under the independence and monotonicity
conditions, the tight bounds for Y and Z with any support coincide with the Manski meanindependence bounds while having a much simpler and more readily implemented form than the
Manski mean-independence bounds.
6 Robins

(1989) also constructs the same bounds under the same conditions for the case of Z and Y binary, but
he does not prove that the bounds are tight.

20

References
[1] Balke, A., and Pearl, J., 1997, \Bounds on Treatment E ects From Studies with Imperfect
Compliance," Journal

, 1171-1176.

of the American Statistical Association, 92

[2] Heckman, J., 1990, \Varieties of Selection Bias,"

,

, 313-318.

American Economic Review 80

[3] Heckman, J., J. Smith, and N. Clements, 1997, \Making the Most Out of Programme Evaluations and Social Experiments: Accounting for Hetergeneity in Programme Impacts," Review
of Economic Studies

; 64(4):487-535.

[4] Heckman, J., and E. Vytlacil, 1999, \Local Instrumental Variables and Latent Variable Models for Identifying and Bounding Treatment E ects,"

Proceedings of the National Academy

, 4730-4734.

of Sciences, 96

[5]

, 2000a, \Local Instrumental Variables" with J. Heckman, forthcoming in C. Hsiao, K.
Morimune, and J. Powell, eds.,

Nonlinear Statistical Inference: Essays in Honor of Takeshi

, (Cambridge University Press: Cambridge).

Amemiya

[6]

, 2000b, \Econometric Evaluation of Social Programs," in J. Heckman and E. Leamer,
eds., Handbook of

, (North-Holland: Amsterdam), forthcoming.

Econometrics, Volume 5

[7] Imbens, G., and J. Angrist, 1994, \Identi cation and Estimation of Local Average Treatment
E ects", Econometrica, 62, 467-476.

21

[8] Manski, C., 1989, \Anatomy of the Selection Problem,"

,

Journal of Human Resources

,

24

343-360.
[9]

, 1990, \Nonparametric Bounds on Treatment E ects,"
Papers and Proceedings, 80

[10]

American Economic Review,

, 319-323.

, 1994, \The Selection Problem," in C. Sims, ed., Advances in Econometrics:

Sixth World

, (Cambridge: Cambridge University Press), 143-170.

Congress

[11] Manski, C. and J. Pepper, 2000, \Monotone Instrumental Variables: With an Application
to the Returns to Schooling," Econometrica, forthcoming.
[12] Robins, J., 1989, \The Analysis of Randomized and Non-randomized AIDS Treatment Trials
Using a New Approach to Causal Inference in Longitudinal Studies", in L. Sechrest, H.
Freeman and A. Mulley, eds., Health service Research Methodology:

A Focus on AIDS

(U.S.

Public Health Service, Washington, DC), 113-159.
[13] Smith, J. and F. Welch, 1986,
Blacks

Closing The Gap:

Forty Years of Economic Progress for

, (Rand Corporation, Santa Monica, CA).

[14] Vytlacil, E., 2000, \Independence, Monotonicity, and Latent Variable Models: An Equivalence Result," working paper, University of Chicago.

22

